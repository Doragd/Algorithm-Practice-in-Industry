# EMNLP2022

## 会议论文列表

本会议共有 1483 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022](https://aclanthology.org/volumes/2022.findings-emnlp/) |  | 0 |  | Yoav Goldberg, Zornitsa Kozareva, Yue Zhang |  |
| 2 |  |  [LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.1) |  | 0 | Recently, deep learning models have made great progress in MWP solving on answer accuracy. However, they are uninterpretable since they mainly rely on shallow heuristics to achieve high performance without understanding and reasoning the grounded math logic. To address this issue and make a step... | Zhicheng Yang, Jinghui Qin, Jiaqi Chen, Liang Lin, Xiaodan Liang |  |
| 3 |  |  [Commonsense Knowledge Salience Evaluation with a Benchmark Dataset in E-commerce](https://doi.org/10.18653/v1/2022.findings-emnlp.2) |  | 0 | In e-commerce, the salience of commonsense knowledge (CSK) is beneficial for widespread applications such as product search and recommendation. For example, when users search for “running” in e-commerce, they would like to find products highly related to running, such as “running shoes” rather than... | Yincen Qu, Ningyu Zhang, Hui Chen, Zelin Dai, Chengming Wang, Xiaoyu Wang, Qiang Chen, Huajun Chen |  |
| 4 |  |  [Automatic Rule Induction for Efficient Semi-Supervised Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.3) |  | 0 | Semi-supervised learning has shown promise in allowing NLP models to generalize from small amounts of labeled data. Meanwhile, pretrained transformer models act as black-box correlation engines that are difficult to explain and sometimes behave unreliably. In this paper, we propose tackling both of... | Reid Pryzant, Ziyi Yang, Yichong Xu, Chenguang Zhu, Michael Zeng |  |
| 5 |  |  [Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion](https://doi.org/10.18653/v1/2022.findings-emnlp.4) |  | 0 | Transformer-based pre-trained models like BERT have achieved great progress on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also shown general benefits in multiple NLP tasks. However, how to efficiently integrate dependency prior structure into pre-trained models to better... | Jian Song, Di Liang, Rumei Li, Yuntao Li, Sirui Wang, Minlong Peng, Wei Wu, Yongxin Yu |  |
| 6 |  |  [Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT](https://doi.org/10.18653/v1/2022.findings-emnlp.5) |  | 0 | We combine the capacity of sparsely gated Mixture-of-Experts (MoE) with the speed and stability of linear, mixing transformations to design the Sparse Mixer encoder model. Sparse Mixer slightly outperforms BERT on GLUE and SuperGLUE, but more importantly trains 65% faster and runs inference 61%... | James LeeThorp, Joshua Ainslie |  |
| 7 |  |  [KE-GCL: Knowledge Enhanced Graph Contrastive Learning for Commonsense Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.6) |  | 0 | Commonsense question answering (CQA) aims to choose the correct answers for commonsense questions. Most existing works focus on extracting and reasoning over external knowledge graphs (KG). However, the noise in KG prevents these models from learning effective representations. In this paper, we... | Lihui Zhang, Ruifan Li |  |
| 8 |  |  [Acceptability Judgements via Examining the Topology of Attention Maps](https://doi.org/10.18653/v1/2022.findings-emnlp.7) |  | 0 | The role of the attention mechanism in encoding linguistic knowledge has received special interest in NLP. However, the ability of the attention heads to judge the grammatical acceptability of a sentence has been underexplored. This paper approaches the paradigm of acceptability judgments with... | Daniil Cherniavskii, Eduard Tulchinskii, Vladislav Mikhailov, Irina Proskurina, Laida Kushnareva, Ekaterina Artemova, Serguei Barannikov, Irina Piontkovskaya, Dmitri Piontkovski, Evgeny Burnaev |  |
| 9 |  |  [Clip-Tuning: Towards Derivative-free Prompt Learning with a Mixture of Rewards](https://doi.org/10.18653/v1/2022.findings-emnlp.8) |  | 0 | Derivative-free prompt learning has emerged as a lightweight alternative to prompt tuning, which only requires model inference to optimize the prompts. However, existing work did not take full advantage of the over-parameterized characteristics of large pre-trained language models (PLMs). In this... | Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang |  |
| 10 |  |  [Soft-Labeled Contrastive Pre-Training for Function-Level Code Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.9) |  | 0 | Code contrastive pre-training has recently achieved significant progress on code-related tasks. In this paper, we present SCodeR, a Soft-labeled contrastive pre-training framework with two positive sample construction methods to learn functional-level Code Representation. Considering the relevance... | Xiaonan Li, Daya Guo, Yeyun Gong, Yun Lin, Yelong Shen, Xipeng Qiu, Daxin Jiang, Weizhu Chen, Nan Duan |  |
| 11 |  |  [Conditioned Masked Language and Image Modeling for Image-Text Dense Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.10) |  | 0 | Image-text retrieval is a fundamental cross-modal task that takes image/text as a query to retrieve relevant data of another type. The large-scale two-stream pre-trained models like CLIP have achieved tremendous success in this area. They embed the images and texts into instance representations... | Ziyang Luo, Yadong Xi, Rongsheng Zhang, GongZheng Li, Zeng Zhao, Jing Ma |  |
| 12 |  |  [Does Simultaneous Speech Translation need Simultaneous Models?](https://doi.org/10.18653/v1/2022.findings-emnlp.11) |  | 0 | In simultaneous speech translation (SimulST), finding the best trade-off between high output quality and low latency is a challenging task. To meet the latency constraints posed by different application scenarios, multiple dedicated SimulST models are usually trained and maintained, generating high... | Sara Papi, Marco Gaido, Matteo Negri, Marco Turchi |  |
| 13 |  |  [Utilizing Language-Image Pretraining for Efficient and Robust Bilingual Word Alignment](https://doi.org/10.18653/v1/2022.findings-emnlp.12) |  | 0 | Word translation without parallel corpora has become feasible, rivaling the performance of supervised methods. Recent findings have shown the improvement in accuracy and robustness of unsupervised word translation (UWT) by utilizing visual observations, which are universal representations across... | Tuan Dinh, Jyyong Sohn, Shashank Rajput, Timothy Ossowski, Yifei Ming, Junjie Hu, Dimitris S. Papailiopoulos, Kangwook Lee |  |
| 14 |  |  [Grape: Knowledge Graph Enhanced Passage Reader for Open-domain Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.13) |  | 0 | A common thread of open-domain question answering (QA) models employs a retriever-reader pipeline that first retrieves a handful of relevant passages from Wikipedia and then peruses the passages to produce an answer. However, even state-of-the-art readers fail to capture the complex relationships... | Mingxuan Ju, Wenhao Yu, Tong Zhao, Chuxu Zhang, Yanfang Ye |  |
| 15 |  |  [NarraSum: A Large-Scale Dataset for Abstractive Narrative Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.14) |  | 0 | Narrative summarization aims to produce a distilled version of a narrative to describe its most salient events and characters. Writing a summary for a narrative is challenging as it requires an understanding of event causality and character behaviors. To encourage research in this direction, we... | Chao Zhao, Faeze Brahman, Kaiqiang Song, Wenlin Yao, Dian Yu, Snigdha Chaturvedi |  |
| 16 |  |  [NMTScore: A Multilingual Analysis of Translation-based Text Similarity Measures](https://doi.org/10.18653/v1/2022.findings-emnlp.15) |  | 0 | Being able to rank the similarity of short text segments is an interesting bonus feature of neural machine translation. Translation-based similarity measures include direct and pivot translation probability, as well as translation cross-likelihood, which has not been studied so far. We analyze... | Jannis Vamvas, Rico Sennrich |  |
| 17 |  |  [Language Models Understand Us, Poorly](https://doi.org/10.18653/v1/2022.findings-emnlp.16) |  | 0 | Some claim language models understand us. Others won’t hear it. To clarify, I investigate three views of human language understanding: as-mapping, as-reliability and as-representation. I argue that while behavioral reliability is necessary for understanding, internal representations are sufficient;... | Jared Moore |  |
| 18 |  |  [Dialogue Meaning Representation for Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.17) |  | 0 | Dialogue meaning representation formulates natural language utterance semantics in their conversational context in an explicit and machine-readable form. Previous work typically follows the intent-slot framework, which is easy for annotation yet limited in scalability for complex linguistic... | Xiangkun Hu, Junqi Dai, Hang Yan, Yi Zhang, Qipeng Guo, Xipeng Qiu, Zheng Zhang |  |
| 19 |  |  [Learning from the Dictionary: Heterogeneous Knowledge Guided Fine-tuning for Chinese Spell Checking](https://doi.org/10.18653/v1/2022.findings-emnlp.18) |  | 0 | Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling errors. Recent researches start from the pretrained knowledge of language models and take multimodal information into CSC models to improve the performance. However, they overlook the rich knowledge in the dictionary, the... | Yinghui Li, Shirong Ma, Qingyu Zhou, Zhongli Li, Yangning Li, Shulin Huang, Ruiyang Liu, Chao Li, Yunbo Cao, Haitao Zheng |  |
| 20 |  |  [Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?](https://doi.org/10.18653/v1/2022.findings-emnlp.19) |  | 0 | Despite their recent popularity and well-known advantages, dense retrievers still lag behind sparse methods such as BM25 in their ability to reliably match salient phrases and rare entities in the query and to generalize to out-of-domain data. It has been argued that this is an inherent limitation... | Xilun Chen, Kushal Lakhotia, Barlas Oguz, Anchit Gupta, Patrick S. H. Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, Wentau Yih |  |
| 21 |  |  [SMARTAVE: Structured Multimodal Transformer for Product Attribute Value Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.20) |  | 0 | Automatic product attribute value extraction refers to the task of identifying values of an attribute from the product information. Product attributes are essential in improving online shopping experience for customers. Most existing methods focus on extracting attribute values from product title... | Qifan Wang, Li Yang, Jingang Wang, Jitin Krishnan, Bo Dai, Sinong Wang, Zenglin Xu, Madian Khabsa, Hao Ma |  |
| 22 |  |  [When Language Model Meets Private Library](https://doi.org/10.18653/v1/2022.findings-emnlp.21) |  | 0 | With the rapid development of pre-training techniques, a number of language models have been pre-trained on large-scale code corpora and perform well in code generation. In this paper, we investigate how to equip pre-trained language models with the ability of code generation for private libraries.... | Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, JianGuang Lou |  |
| 23 |  |  [Cross-Domain Sentiment Classification using Semantic Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.22) |  | 0 | Previous studies on cross-domain sentiment classification depend on the pivot features or utilize the target data for representation learning, which ignore the semantic relevance between different domains. To this end, we exploit Abstract Meaning Representation (AMR) to help with cross-domain... | Shichen Li, Zhongqing Wang, Xiaotong Jiang, Guodong Zhou |  |
| 24 |  |  [Yes-Yes-Yes: Proactive Data Collection for ACL Rolling Review and Beyond](https://doi.org/10.18653/v1/2022.findings-emnlp.23) |  | 0 | The shift towards publicly available text sources has enabled language processing at unprecedented scale, yet leaves under-serviced the domains where public and openly licensed data is scarce. Proactively collecting text data for research is a viable strategy to address this scarcity, but lacks... | Nils Dycke, Ilia Kuznetsov, Iryna Gurevych |  |
| 25 |  |  [AssistSR: Task-oriented Video Segment Retrieval for Personal AI Assistant](https://doi.org/10.18653/v1/2022.findings-emnlp.24) |  | 0 | It is still a pipe dream that personal AI assistants on the phone and AR glasses can assist our daily life in addressing our questions like “how to adjust the date for this watch?” and “how to set its heating duration? (while pointing at an oven)”. The queries used in conventional tasks (i.e. Video... | Weixian Lei, Difei Gao, Yuxuan Wang, Dongxing Mao, Zihan Liang, Lingmin Ran, Mike Zheng Shou |  |
| 26 |  |  [Dim-Krum: Backdoor-Resistant Federated Learning for NLP with Dimension-wise Krum-Based Aggregation](https://doi.org/10.18653/v1/2022.findings-emnlp.25) |  | 0 | Despite the potential of federated learning, it is known to be vulnerable to backdoor attacks. Many robust federated aggregation methods are proposed to reduce the potential backdoor risk. However, they are mainly validated in the CV field. In this paper, we find that NLP backdoors are hard to... | Zhiyuan Zhang, Qi Su, Xu Sun |  |
| 27 |  |  [Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.26) |  | 0 | Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks. In Natural Language Processing (NLP), DNNs are often backdoored during the fine-tuning process of a large-scale Pre-trained Language Model (PLM) with poisoned samples. Although the clean weights of PLMs are readily... | Zhiyuan Zhang, Lingjuan Lyu, Xingjun Ma, Chenguang Wang, Xu Sun |  |
| 28 |  |  [Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion](https://doi.org/10.18653/v1/2022.findings-emnlp.27) |  | 0 | Language models (LMs) have recently been shown to generate more factual responses by employing modularity (Zhou et al., 2022) in combination with retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et al. (2021) to include internet search as a module. Our SeeKeR (Search... | Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, Jason Weston |  |
| 29 |  |  [Stretching Sentence-pair NLI Models to Reason over Long Documents and Clusters](https://doi.org/10.18653/v1/2022.findings-emnlp.28) |  | 0 | Natural Language Inference (NLI) has been extensively studied by the NLP community as a framework for estimating the semantic relation between sentence pairs. While early work identified certain biases in NLI models, recent advancements in modeling and datasets demonstrated promising performance.In... | Tal Schuster, Sihao Chen, Senaka Buthpitiya, Alex Fabrikant, Donald Metzler |  |
| 30 |  |  [Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study](https://doi.org/10.18653/v1/2022.findings-emnlp.29) |  | 0 | This paper presents an empirical study to build relation extraction systems in low-resource settings. Based upon recent pre-trained language models, we comprehensively investigate three schemes to evaluate the performance in low-resource settings: (i) different types of prompt-based methods with... | Xin Xu, Xiang Chen, Ningyu Zhang, Xin Xie, Xi Chen, Huajun Chen |  |
| 31 |  |  [CLLE: A Benchmark for Continual Language Learning Evaluation in Multilingual Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.30) |  | 0 | Continual Language Learning (CLL) in multilingual translation is inevitable when new languages are required to be translated. Due to the lack of unified and generalized benchmarks, the evaluation of existing methods is greatly influenced by experimental design which usually has a big gap from the... | Han Zhang, Sheng Zhang, Yang Xiang, Bin Liang, Jinsong Su, Zhongjian Miao, Hui Wang, Ruifeng Xu |  |
| 32 |  |  [Lexicon-Enhanced Self-Supervised Training for Multilingual Dense Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.31) |  | 0 | Recent multilingual pre-trained models have shown better performance in various multilingual tasks. However, these models perform poorly on multilingual retrieval tasks due to lacking multilingual training data. In this paper, we propose to mine and generate self-supervised training data based on a... | Houxing Ren, Linjun Shou, Jian Pei, Ning Wu, Ming Gong, Daxin Jiang |  |
| 33 |  |  [Improve Interpretability of Neural Networks via Sparse Contrastive Coding](https://doi.org/10.18653/v1/2022.findings-emnlp.32) |  | 0 | Although explainable artificial intelligence (XAI) has achieved remarkable developments in recent years, there are few efforts have been devoted to the following problems, namely, i) how to develop an explainable method that could explain the black-box in a model-agnostic way? and ii) how to... | Junhong Liu, Yijie Lin, Liang Jiang, Jia Liu, Zujie Wen, Xi Peng |  |
| 34 |  |  [LEMON: Language-Based Environment Manipulation via Execution-Guided Pre-training](https://doi.org/10.18653/v1/2022.findings-emnlp.33) |  | 0 | Language-based environment manipulation requires agents to manipulate the environment following natural language instructions, which is challenging due to the huge space of the environments.To address this challenge, various approaches have been proposed in recent work. Although these approaches... | Qi Shi, Qian Liu, Bei Chen, Yu Zhang, Ting Liu, JianGuang Lou |  |
| 35 |  |  [CROP: Zero-shot Cross-lingual Named Entity Recognition with Multilingual Labeled Sequence Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.34) |  | 0 | Named entity recognition (NER) suffers from the scarcity of annotated training data, especially for low-resource languages without labeled data. Cross-lingual NER has been proposed to alleviate this issue by transferring knowledge from high-resource languages to low-resource languages via aligned... | Jian Yang, Shaohan Huang, Shuming Ma, Yuwei Yin, Li Dong, Dongdong Zhang, Hongcheng Guo, Zhoujun Li, Furu Wei |  |
| 36 |  |  [Handling and Presenting Harmful Text in NLP Research](https://doi.org/10.18653/v1/2022.findings-emnlp.35) |  | 0 | Text data can pose a risk of harm. However, the risks are not fully understood, and how to handle, present, and discuss harmful text in a safe way remains an unresolved issue in the NLP community. We provide an analytical framework categorising harms on three axes: (1) the harm type (e.g.,... | Hannah Kirk, Abeba Birhane, Bertie Vidgen, Leon Derczynski |  |
| 37 |  |  [Multimodal Contrastive Learning via Uni-Modal Coding and Cross-Modal Prediction for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2022.findings-emnlp.36) |  | 0 | Multimodal representation learning is a challenging task in which previous work mostly focus on either uni-modality pre-training or cross-modality fusion. In fact, we regard modeling multimodal representation as building a skyscraper, where laying stable foundation and designing the main structure... | Ronghao Lin, Haifeng Hu |  |
| 38 |  |  [Towards Unified Prompt Tuning for Few-shot Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.37) |  | 0 | Prompt-based fine-tuning has boosted the performance of Pre-trained Language Models (PLMs) on few-shot text classification by employing task-specific prompts. Yet, PLMs are unfamiliar with prompt-style expressions during pre-training, which limits the few-shot learning performance on downstream... | Jianing Wang, Chengyu Wang, Fuli Luo, Chuanqi Tan, Minghui Qiu, Fei Yang, Qiuhui Shi, Songfang Huang, Ming Gao |  |
| 39 |  |  [Can language models learn from explanations in context?](https://doi.org/10.18653/v1/2022.findings-emnlp.38) |  | 0 | Language Models (LMs) can perform new tasks by adapting to a few in-context examples. For humans, explanations that connect examples to task principles can improve learning. We therefore investigate whether explanations of few-shot examples can help LMs. We annotate questions from 40 challenging... | Andrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y. Chan, Kory W. Mathewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane Wang, Felix Hill |  |
| 40 |  |  [GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.39) |  | 0 | Recently, retrieval models based on dense representations are dominant in passage retrieval tasks, due to their outstanding ability in terms of capturing semantics of input text compared to the traditional sparse vector space models. A common practice of dense retrieval models is to exploit a... | Jiduan Liu, Jiahao Liu, Yang Yang, Jingang Wang, Wei Wu, Dongyan Zhao, Rui Yan |  |
| 41 |  |  [Linguistic Rules-Based Corpus Generation for Native Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2022.findings-emnlp.40) |  | 0 | Chinese Grammatical Error Correction (CGEC) is both a challenging NLP task and a common application in human daily life. Recently, many data-driven approaches are proposed for the development of CGEC research. However, there are two major limitations in the CGEC field: First, the lack of... | Shirong Ma, Yinghui Li, Rongyi Sun, Qingyu Zhou, Shulin Huang, Ding Zhang, Yangning Li, Ruiyang Liu, Zhongli Li, Yunbo Cao, Haitao Zheng, Ying Shen |  |
| 42 |  |  [Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding](https://doi.org/10.18653/v1/2022.findings-emnlp.41) |  | 0 | Temporal sentence grounding (TSG) aims to identify the temporal boundary of a specific segment from an untrimmed video by a sentence query. All existing works first utilize a sparse sampling strategy to extract a fixed number of video frames and then interact them with query for reasoning.However,... | Jiahao Zhu, Daizong Liu, Pan Zhou, Xing Di, Yu Cheng, Song Yang, Wenzheng Xu, Zichuan Xu, Yao Wan, Lichao Sun, Zeyu Xiong |  |
| 43 |  |  [System 1 + System 2 = Better World: Neural-Symbolic Chain of Logic Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.42) |  | 0 | Logical reasoning is a challenge for many current NLP neural network models since it requires more than the ability of learning informative representations from data. Inspired by the Dual Process Theory in cognitive science — which proposes that human cognition process involves two stages: an... | Wenyue Hua, Yongfeng Zhang |  |
| 44 |  |  [Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation](https://doi.org/10.18653/v1/2022.findings-emnlp.43) |  | 0 | Federated learning (FL) can be essential in knowledge representation, reasoning, and data mining applications over multi-source knowledge graphs (KGs). A recent study FedE first proposes an FL framework that shares entity embeddings of KGs across all clients. However, entity embedding sharing from... | Kai Zhang, Yu Wang, Hongyi Wang, Lifu Huang, Carl Yang, Xun Chen, Lichao Sun |  |
| 45 |  |  [TextHacker: Learning based Hybrid Local Search Algorithm for Text Hard-label Adversarial Attack](https://doi.org/10.18653/v1/2022.findings-emnlp.44) |  | 0 | Existing textual adversarial attacks usually utilize the gradient or prediction confidence to generate adversarial examples, making it hard to be deployed in real-world applications. To this end, we consider a rarely investigated but more rigorous setting, namely hard-label attack, in which the... | Zhen Yu, Xiaosen Wang, Wanxiang Che, Kun He |  |
| 46 |  |  [Visualizing the Obvious: A Concreteness-based Ensemble Model for Noun Property Prediction](https://doi.org/10.18653/v1/2022.findings-emnlp.45) |  | 0 | Neural language models encode rich knowledge about entities and their relationships which can be extracted from their representations using probing. Common properties of nouns (e.g., red strawberries, small ant) are, however, more challenging to extract compared to other types of knowledge because... | Yue Yang, Artemis Panagopoulou, Marianna Apidianaki, Mark Yatskar, Chris CallisonBurch |  |
| 47 |  |  [It's Better to Teach Fishing than Giving a Fish: An Auto-Augmented Structure-aware Generative Model for Metaphor Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.46) |  | 0 | Metaphor Detection aims to identify the metaphorical meaning of words in the sentence. Most existing work is discriminant models, which use the contextual semantic information extracted by transformers for classifications directly. Due to insufficient training data and corresponding paraphrases,... | Huawen Feng, Qianli Ma |  |
| 48 |  |  [Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks](https://doi.org/10.18653/v1/2022.findings-emnlp.47) |  | 0 | Natural language processing (NLP) models are known to be vulnerable to backdoor attacks, which poses a newly arisen threat to NLP models. Prior online backdoor defense methods for NLP models only focus on the anomalies at either the input or output level, still suffering from fragility to adaptive... | Sishuo Chen, Wenkai Yang, Zhiyuan Zhang, Xiaohan Bi, Xu Sun |  |
| 49 |  |  [Diving Deep into Modes of Fact Hallucinations in Dialogue Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.48) |  | 0 | Knowledge Graph(KG) grounded conversations often use large pre-trained models and usually suffer from fact hallucination. Frequently entities with no references in knowledge sources and conversation history are introduced into responses, thus hindering the flow of the conversation—existing work... | Souvik Das, Sougata Saha, Rohini K. Srihari |  |
| 50 |  |  [Representation Learning for Resource-Constrained Keyphrase Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.49) |  | 0 | State-of-the-art keyphrase generation methods generally depend on large annotated datasets, limiting their performance in domains with limited annotated data. To overcome this challenge, we design a data-oriented approach that first identifies salient information using retrieval-based corpus-level... | Di Wu, Wasi Uddin Ahmad, Sunipa Dev, KaiWei Chang |  |
| 51 |  |  [Systematicity in GPT-3's Interpretation of Novel English Noun Compounds](https://doi.org/10.18653/v1/2022.findings-emnlp.50) |  | 0 | Levin et al. (2019) show experimentally that the interpretations of novel English noun compounds (e.g., stew skillet), while not fully compositional, are highly predictable based on whether the modifier and head refer to artifacts or natural kinds. Is the large language model GPT-3 governed by the... | Siyan Li, Riley Carlson, Christopher Potts |  |
| 52 |  |  [CARE: Causality Reasoning for Empathetic Responses by Conditional Graph Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.51) |  | 0 | Recent approaches to empathetic response generation incorporate emotion causalities to enhance comprehension of both the user’s feelings and experiences. However, these approaches suffer from two critical issues. First, they only consider causalities between the user’s emotion and the user’s... | Jiashuo Wang, Yi Cheng, Wenjie Li |  |
| 53 |  |  [TransAdv: A Translation-based Adversarial Learning Framework for Zero-Resource Cross-Lingual Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.52) |  | 0 | Zero-Resource Cross-Lingual Named Entity Recognition aims at training an NER model of the target language using only labeled source language data and unlabeled target language data. Existing methods are mainly divided into three categories: model transfer based, data transfer based and knowledge... | Yichun Zhao, Jintao Du, Gongshen Liu, Huijia Zhu |  |
| 54 |  |  [BARLE: Background-Aware Representation Learning for Background Shift Out-of-Distribution Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.53) |  | 0 | Machine learning models often suffer from a performance drop when they are applied to out-of-distribution (OOD) samples, i.e., those drawn far away from the training data distribution. Existing OOD detection work mostly focuses on identifying semantic-shift OOD samples, e.g., instances from unseen... | Hanyu Duan, Yi Yang, Ahmed Abbasi, Kar Yan Tam |  |
| 55 |  |  [What Language Model to Train if You Have One Million GPU Hours?](https://doi.org/10.18653/v1/2022.findings-emnlp.54) |  | 0 | The crystallization of modeling methods around the Transformer architecture has been a boon for practitioners. Simple, well-motivated architectural variations can transfer across tasks and scale, increasing the impact of modeling research. However, with the emergence of state-of-the-art 100B+... | Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M. Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, Iz Beltagy |  |
| 56 |  |  [Enhancing Out-of-Distribution Detection in Natural Language Understanding via Implicit Layer Ensemble](https://doi.org/10.18653/v1/2022.findings-emnlp.55) |  | 0 | Out-of-distribution (OOD) detection aims to discern outliers from the intended data distribution, which is crucial to maintaining high reliability and a good user experience.Most recent studies in OOD detection utilize the information from a single representation that resides in the penultimate... | Hyunsoo Cho, Choonghyun Park, Jaewook Kang, Kang Min Yoo, Taeuk Kim, Sanggoo Lee |  |
| 57 |  |  [Contrastive Demonstration Tuning for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.56) |  | 0 | Pretrained language models can be effectively stimulated by textual prompts or demonstrations, especially in low-data scenarios. Recent works have focused on automatically searching discrete or continuous prompts or optimized verbalizers, yet studies for the demonstration are still limited.... | Xiaozhuan Liang, Ningyu Zhang, Siyuan Cheng, Zhenru Zhang, Chuanqi Tan, Huajun Chen |  |
| 58 |  |  [Detect-Localize-Repair: A Unified Framework for Learning to Debug with CodeT5](https://doi.org/10.18653/v1/2022.findings-emnlp.57) |  | 0 | Automated software debugging is a crucial task for improving the productivity of software developers. Many neural-based techniques have been proven effective for debugging-related tasks such as bug localization and program repair (or bug fixing). However, these techniques often focus only on either... | Nghi Bui, Yue Wang, Steven C. H. Hoi |  |
| 59 |  |  [Influence Functions for Sequence Tagging Models](https://doi.org/10.18653/v1/2022.findings-emnlp.58) |  | 0 | Many standard tasks in NLP (e.g., Named Entity Recognition, Part-of-Speech tagging, and Semantic Role Labeling) are naturally framed as sequence tagging problems. However, there has been comparatively little work on interpretability methods for sequence tagging models. In this paper, we extend... | Sarthak Jain, Varun Manjunatha, Byron C. Wallace, Ani Nenkova |  |
| 60 |  |  [Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.59) |  | 0 | Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with... | Yasaman Razeghi, Robert L. Logan IV, Matt Gardner, Sameer Singh |  |
| 61 |  |  [Syntactic and Semantic Uniformity for Semantic Parsing and Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.60) |  | 0 | This paper proposes a data representation framework for semantic parsing and task-oriented dialogue systems, aiming to achieve a uniform representation for syntactically and semantically diverse machine-readable formats.Current NLP systems heavily rely on adapting pre-trained language models to... | Bowen Chen, Yusuke Miyao |  |
| 62 |  |  [Knowledge-Rich Self-Supervision for Biomedical Entity Linking](https://doi.org/10.18653/v1/2022.findings-emnlp.61) |  | 0 | Entity linking faces significant challenges such as prolific variations and prevalent ambiguities, especially in high-value domains with myriad entities. Standard classification approaches suffer from the annotation bottleneck and cannot effectively handle unseen entities. Zero-shot entity linking... | Sheng Zhang, Hao Cheng, Shikhar Vashishth, Cliff Wong, Jinfeng Xiao, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon |  |
| 63 |  |  [ARTIST: A Transformer-based Chinese Text-to-Image Synthesizer Digesting Linguistic and World Knowledge](https://doi.org/10.18653/v1/2022.findings-emnlp.62) |  | 0 | Text-to-Image Synthesis (TIS) is a popular task to convert natural language texts into realistic images. Recently, transformer-based TIS models (such as DALL-E) have been proposed using the encoder-decoder architectures. Yet, these billion-scale TIS models are difficult to tune and deploy in... | Tingting Liu, Chengyu Wang, Xiangru Zhu, Lei Li, Minghui Qiu, Jun Huang, Ming Gao, Yanghua Xiao |  |
| 64 |  |  [From Spelling to Grammar: A New Framework for Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2022.findings-emnlp.63) |  | 0 | Chinese Grammatical Error Correction (CGEC) aims to generate a correct sentence from an erroneous sequence, where different kinds of errors are mixed. This paper divides the CGEC task into two steps, namely spelling error correction and grammatical error correction. We firstly propose a novel... | Xiuyu Wu, Yunfang Wu |  |
| 65 |  |  [Language Models Are Poor Learners of Directional Inference](https://doi.org/10.18653/v1/2022.findings-emnlp.64) |  | 0 | We examine LMs’ competence of directional predicate entailments by supervised fine-tuning with prompts. Our analysis shows that contrary to their apparent success on standard NLI, LMs show limited ability to learn such directional inference; moreover, existing datasets fail to test directionality,... | Tianyi Li, Mohammad Javad Hosseini, Sabine Weber, Mark Steedman |  |
| 66 |  |  [Wish I Can Feel What You Feel: A Neural Approach for Empathetic Response Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.65) |  | 0 | Expressing empathy is important in everyday conversations, and exploring how empathy arises is crucial in automatic response generation. Most previous approaches consider only a single factor that affects empathy. However, in practice, empathy generation and expression is a very complex and dynamic... | Yangbin Chen, Chunfeng Liang |  |
| 67 |  |  [Measuring and Improving Semantic Diversity of Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.66) |  | 0 | Response diversity has become an important criterion for evaluating the quality of open-domain dialogue generation models. However, current evaluation metrics for response diversity often fail to capture the semantic diversity of generated responses, as they mainly consider lexical aspects of the... | Seungju Han, Beomsu Kim, Buru Chang |  |
| 68 |  |  [Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training](https://doi.org/10.18653/v1/2022.findings-emnlp.67) |  | 0 | Visual question answering (VQA) is a hallmark of vision and language reasoningand a challenging task under the zero-shot setting.We propose Plug-and-Play VQA (PNP-VQA),a modular framework for zero-shot VQA.In contrast to most existing works, which require substantial adaptation of pretrained... | Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, Steven C. H. Hoi |  |
| 69 |  |  [TSGP: Two-Stage Generative Prompting for Unsupervised Commonsense Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.68) |  | 0 | Without training on labeled task data, unsupervised commonsense question answering seems challenging since it requires commonsense knowledge beyond the context of questions. Previous methods typically retrieved from traditional knowledge bases or used pre-trained language models (PrLMs) to generate... | Yueqing Sun, Yu Zhang, Le Qi, Qi Shi |  |
| 70 |  |  [Subword-Delimited Downsampling for Better Character-Level Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.69) |  | 0 | Subword-level models have been the dominant paradigm in NLP. However, character-level models have the benefit of seeing each character individually, providing the model with more detailed information that ultimately could lead to better models. Recent works have shown character-level models to be... | Lukas Edman, Antonio Toral, Gertjan van Noord |  |
| 71 |  |  [Autoregressive Structured Prediction with Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.70) |  | 0 | Recent years have seen a paradigm shift in NLP towards using pretrained language models (PLM) for a wide range of tasks. However, there are many difficult design decisions to represent structures (e.g. tagged text, coreference chains) in a way such that they can be captured by PLMs. Prior work on... | Tianyu Liu, Yuchen Eleanor Jiang, Nicholas Monath, Ryan Cotterell, Mrinmaya Sachan |  |
| 72 |  |  [XDoc: Unified Pre-training for Cross-Format Document Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.71) |  | 0 | The surge of pre-training has witnessed the rapid development of document understanding recently. Pre-training and fine-tuning framework has been effectively used to tackle texts in various formats, including plain texts, document texts, and web texts. Despite achieving promising performance,... | Jingye Chen, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei |  |
| 73 |  |  [A Few More Examples May Be Worth Billions of Parameters](https://doi.org/10.18653/v1/2022.findings-emnlp.72) |  | 0 | We investigate the dynamics of increasing the number of model parameters versus the number of labeled examples across a wide variety of tasks. Our exploration reveals that while scaling parameters consistently yields performance improvements, the contribution of additional examples highly depends... | Yuval Kirstain, Patrick Lewis, Sebastian Riedel, Omer Levy |  |
| 74 |  |  [MCP: Self-supervised Pre-training for Personalized Chatbots with Multi-level Contrastive Sampling](https://doi.org/10.18653/v1/2022.findings-emnlp.73) |  | 0 | Personalized chatbots focus on endowing the chatbots with a consistent personality to behave like real users and further act as personal assistants. Previous studies have explored generating implicit user profiles from the user’s dialogue history for building personalized chatbots. However, these... | Zhaoheng Huang, Zhicheng Dou, Yutao Zhu, Zhengyi Ma |  |
| 75 |  |  [ExpertPLM: Pre-training Expert Representation for Expert Finding](https://doi.org/10.18653/v1/2022.findings-emnlp.74) |  | 0 | Expert Finding is an important task in Community Question Answering (CQA) platforms, which could help route questions to potential users to answer. The key is to learn representations of experts based on their historical answered questions accurately. In this paper, inspired by the strong text... | Qiyao Peng, Hongtao Liu |  |
| 76 |  |  [You Truly Understand What I Need : Intellectual and Friendly Dialog Agents grounding Persona and Knowledge](https://doi.org/10.18653/v1/2022.findings-emnlp.75) |  | 0 | To build a conversational agent that interacts fluently with humans, previous studies blend knowledge or personal profile into the pre-trained language model. However, the model that considers knowledge and persona at the same time is still limited, leading to hallucination and a passive way of... | Jungwoo Lim, Myunghoon Kang, Yuna Hur, Seung Won Jeong, Jinsung Kim, Yoonna Jang, Dongyub Lee, Hyesung Ji, DongHoon Shin, Seungryong Kim, Heuiseok Lim |  |
| 77 |  |  [Faithful to the Document or to the World? Mitigating Hallucinations via Entity-Linked Knowledge in Abstractive Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.76) |  | 0 | Existing abstractive summarization systems are hampered by content hallucinations in which models generate text that is not directly inferable from the source alone. Annotations from prior work have shown that some of these hallucinations, while being ‘unfaithful’ to the source, are nonetheless... | Yue Dong, John Wieting, Pat Verga |  |
| 78 |  |  [RL with KL penalties is better viewed as Bayesian inference](https://doi.org/10.18653/v1/2022.findings-emnlp.77) |  | 0 | Reinforcement learning (RL) is frequently employed in fine-tuning large language models (LMs), such as GPT-3, to penalize them for undesirable features of generated sequences, such as offensiveness, social bias, harmfulness or falsehood. The RL formulation involves treating the LM as a policy and... | Tomasz Korbak, Ethan Perez, Christopher L. Buckley |  |
| 79 |  |  [Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.78) |  | 0 | With the recent success of dense retrieval methods based on bi-encoders, studies have applied this approach to various interesting downstream retrieval tasks with good efficiency and in-domain effectiveness.Recently, we have also seen the presence of dense retrieval models in Math Information... | Wei Zhong, JhengHong Yang, Yuqing Xie, Jimmy Lin |  |
| 80 |  |  [Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem](https://doi.org/10.18653/v1/2022.findings-emnlp.79) |  | 0 | Math word problem solver requires both precise relation reasoning about quantities in the text and reliable generation for the diverse equation. Current sequence-to-tree or relation extraction methods regard this only from a fixed view, struggling to simultaneously handle complex semantics and... | Wenqi Zhang, Yongliang Shen, Yanna Ma, Xiaoxia Cheng, Zeqi Tan, Qingpeng Nong, Weiming Lu |  |
| 81 |  |  [Few-shot initializing of Active Learner via Meta-Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.80) |  | 0 | Despite the important evolutions in few-shot and zero-shot learning techniques, domain specific applications still require expert knowledge and significant effort in annotating and labeling a large volume of unstructured textual data. To mitigate this problem, active learning, and meta-learning... | Zi Long Zhu, Vikrant Yadav, Zubair Afzal, George Tsatsaronis |  |
| 82 |  |  [Bootstrapping meaning through listening: Unsupervised learning of spoken sentence embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.81) |  | 0 | Inducing semantic representations directly from speech signals is a highly challenging task but has many useful applications in speech mining and spoken language understanding. This study tackles the unsupervised learning of semantic representations for spoken utterances. Through converting speech... | Jian Zhu, Zuoyu Tian, Yadong Liu, Cong Zhang, ChiaWen Lo |  |
| 83 |  |  [Progressive Sentiment Analysis for Code-Switched Text Data](https://doi.org/10.18653/v1/2022.findings-emnlp.82) |  | 0 | Multilingual transformer language models have recently attracted much attention from researchers and are used in cross-lingual transfer learning for many NLP tasks such as text classification and named entity recognition.However, similar methods for transfer learning from monolingual text to... | Sudhanshu Ranjan, Dheeraj Mekala, Jingbo Shang |  |
| 84 |  |  [Knowledge Stimulated Contrastive Prompting for Low-Resource Stance Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.83) |  | 0 | Stance Detection Task (SDT) aims at identifying the stance of the sentence towards a specific target and is usually modeled as a classification problem. Backgound knowledge is often necessary for stance detection with respect to a specific target, especially when there is no target explicitly... | Kai Zheng, Qingfeng Sun, Yaming Yang, Fei Xu |  |
| 85 |  |  [WSpeller: Robust Word Segmentation for Enhancing Chinese Spelling Check](https://doi.org/10.18653/v1/2022.findings-emnlp.84) |  | 0 | Chinese spelling check (CSC) detects and corrects spelling errors in Chinese texts. Previous approaches have combined character-level phonetic and graphic information, ignoring the importance of segment-level information. According to our pilot study, spelling errors are always associated with... | Fangfang Li, Youran Shan, Junwen Duan, Xingliang Mao, Minlie Huang |  |
| 86 |  |  [Extracting Trigger-sharing Events via an Event Matrix](https://doi.org/10.18653/v1/2022.findings-emnlp.85) |  | 0 | A growing interest emerges in event extraction which aims to extract multiple events with triggers and arguments. Previous methods mitigate the problem of multiple events extraction by predicting the arguments conditioned on the event trigger and event type, assuming that these arguments belong to... | Jun Xu, Weidi Xu, Mengshu Sun, Taifeng Wang, Wei Chu |  |
| 87 |  |  [TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.86) |  | 0 | Knowledge graph embedding (KGE) aims to learn continuous vector representations of relations and entities in knowledge graph (KG). Recently, transition-based KGE methods have become popular and achieved promising performance. However, scoring patterns like TransE are not suitable for complex... | Xuanyu Zhang, Qing Yang, Dongliang Xu |  |
| 88 |  |  [Sequential Topic Selection Model with Latent Variable for Topic-Grounded Dialogue](https://doi.org/10.18653/v1/2022.findings-emnlp.87) |  | 0 | Recently, topic-grounded dialogue system has attracted significant attention due to its effectiveness in predicting the next topic to yield better responses via the historical context and given topic sequence. However, almost all existing topic prediction solutions focus on only the current... | Xiaofei Wen, Wei Wei, XianLing Mao |  |
| 89 |  |  [Robust Task-Oriented Dialogue Generation with Contrastive Pre-training and Adversarial Filtering](https://doi.org/10.18653/v1/2022.findings-emnlp.88) |  | 0 | Data artifacts incentivize machine learning models to learn non-transferable generalizations by taking advantage of shortcuts in the data, andthere is growing evidence that data artifacts play a role for the strong results that deep learning models achieve in recent natural language processing... | Shiquan Yang, Xinting Huang, Jey Han Lau, Sarah M. Erfani |  |
| 90 |  |  [STAR: SQL Guided Pre-Training for Context-dependent Text-to-SQL Parsing](https://doi.org/10.18653/v1/2022.findings-emnlp.89) |  | 0 | In this paper, we propose a novel SQL guided pre-training framework STAR for context-dependent text-to-SQL parsing, which leverages contextual information to enrich natural language (NL) utterance and table schema representations for text-to-SQL conversations. Concretely, we propose two novel... | Zefeng Cai, Xiangyu Li, Binyuan Hui, Min Yang, Bowen Li, Binhua Li, Zheng Cao, Weijie Li, Fei Huang, Luo Si, Yongbin Li |  |
| 91 |  |  [Is MultiWOZ a Solved Task? An Interactive TOD Evaluation Framework with User Simulator](https://doi.org/10.18653/v1/2022.findings-emnlp.90) |  | 0 | Task-Oriented Dialogue (TOD) systems are drawing more and more attention in recent studies.Current methods focus on constructing pre-trained models or fine-tuning strategies while the evaluation of TOD is limited by a policy mismatch problem.That is, during evaluation, the user utterances are from... | Qinyuan Cheng, Linyang Li, Guofeng Quan, Feng Gao, Xiaofeng Mou, Xipeng Qiu |  |
| 92 |  |  [Translating Hanja Historical Documents to Contemporary Korean and English](https://doi.org/10.18653/v1/2022.findings-emnlp.91) |  | 0 | The Annals of Joseon Dynasty (AJD) contain the daily records of the Kings of Joseon, the 500-year kingdom preceding the modern nation of Korea.The Annals were originally written in an archaic Korean writing system, ‘Hanja’, and were translated into Korean from 1968 to 1993.The resulting translation... | Juhee Son, Jiho Jin, Haneul Yoo, JinYeong Bak, Kyunghyun Cho, Alice Oh |  |
| 93 |  |  [Exploring Compositional Image Retrieval with Hybrid Compositional Learning and Heuristic Negative Mining](https://doi.org/10.18653/v1/2022.findings-emnlp.92) |  | 0 | Compositional image retrieval (CIR) is a challenging retrieval task, where the query is composed of a reference image and a modification text, and the target is another image reflecting the modification to the reference image. Due to the great success of the pre-trained vision-and-language model... | Chao Wang, Ehsan Nezhadarya, Tanmana Sadhu, Shengdong Zhang |  |
| 94 |  |  [Outlier Dimensions that Disrupt Transformers are Driven by Frequency](https://doi.org/10.18653/v1/2022.findings-emnlp.93) |  | 0 | While Transformer-based language models are generally very robust to pruning, there is the recently discovered outlier phenomenon: disabling only 48 out of 110M parameters in BERT-base drops its performance by nearly 30% on MNLI. We replicate the original evidence for the outlier phenomenon and we... | Giovanni Puccetti, Anna Rogers, Aleksandr Drozd, Felice Dell'Orletta |  |
| 95 |  |  [MiST: a Large-Scale Annotated Resource and Neural Models for Functions of Modal Verbs in English Scientific Text](https://doi.org/10.18653/v1/2022.findings-emnlp.94) |  | 0 | Modal verbs (e.g., can, should or must) occur highly frequently in scientific articles. Decoding their function is not straightforward: they are often used for hedging, but they may also denote abilities and restrictions. Understanding their meaning is important for accurate information extraction... | Sophie Henning, Nicole Macher, Stefan Grünewald, Annemarie Friedrich |  |
| 96 |  |  [Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts](https://doi.org/10.18653/v1/2022.findings-emnlp.95) |  | 0 | Prompt tuning is a parameter-efficient tuning (PETuning) method for utilizing pre-trained models (PTMs) that simply prepends a soft prompt to the input and only optimizes the prompt to adapt PTMs to downstream tasks. Although it is parameter- and deployment-efficient, its performance still lags... | Xiangyang Liu, Tianxiang Sun, Xuanjing Huang, Xipeng Qiu |  |
| 97 |  |  [MICO: A Multi-alternative Contrastive Learning Framework for Commonsense Knowledge Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.96) |  | 0 | Commonsense reasoning tasks such as commonsense knowledge graph completion and commonsense question answering require powerful representation learning. In this paper, we propose to learn commonsense knowledge representation by MICO, a Multi-alternative contrastIve learning framework on COmmonsense... | Ying Su, Zihao Wang, Tianqing Fang, Hongming Zhang, Yangqiu Song, Tong Zhang |  |
| 98 |  |  [Leveraging Only the Category Name for Aspect Detection through Prompt-based Constrained Clustering](https://doi.org/10.18653/v1/2022.findings-emnlp.97) |  | 0 | Aspect category detection (ACD) aims to automatically identify user-concerned aspects from online reviews, which is of great value for evaluating the fine-grained performance of a product. The most recent solutions tackle this problem via weakly supervised methods, achieving remarkable improvement... | Yazheng Li, Pengyun Wang, Yasheng Wang, Yong Dai, Yadao Wang, Lujia Pan, Zenglin Xu |  |
| 99 |  |  [Controllable Factuality in Document-Grounded Dialog Systems Using a Noisy Channel Model](https://doi.org/10.18653/v1/2022.findings-emnlp.98) |  | 0 | In this work, we present a model for document-grounded response generation in dialog that is decomposed into two components according to Bayes’ theorem.One component is a traditional ungrounded response generation model and the other component models the reconstruction of the grounding document... | Nico Daheim, David Thulke, Christian Dugast, Hermann Ney |  |
| 100 |  |  [Transformer Language Models without Positional Encodings Still Learn Positional Information](https://doi.org/10.18653/v1/2022.findings-emnlp.99) |  | 0 | Causal transformer language models (LMs), such as GPT-3, typically require some form of positional encoding, such as positional embeddings. However, we show that LMs without any explicit positional encoding are still competitive with standard models and that this phenomenon is robust across... | Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, Omer Levy |  |
| 101 |  |  [Beyond Model Interpretability: On the Faithfulness and Adversarial Robustness of Contrastive Textual Explanations](https://doi.org/10.18653/v1/2022.findings-emnlp.100) |  | 0 | Contrastive explanation methods go beyond transparency and address the contrastive aspect of explanations. Such explanations are emerging as an attractive option to provide actionable change to scenarios adversely impacted by classifiers’ decisions. However, their extension to textual data is... | Julia El Zini, Mariette Awad |  |
| 102 |  |  [How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers](https://doi.org/10.18653/v1/2022.findings-emnlp.101) |  | 0 | The attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language... | Michael Hassid, Hao Peng, Daniel Rotem, Jungo Kasai, Ivan Montero, Noah A. Smith, Roy Schwartz |  |
| 103 |  |  [What Has Been Enhanced in my Knowledge-Enhanced Language Model?](https://doi.org/10.18653/v1/2022.findings-emnlp.102) |  | 0 | A number of knowledge integration (KI) methods have recently been proposed to incorporate external knowledge into pretrained language models (LMs). Even though knowledge-enhanced LMs (KELMs) outperform base LMs on knowledge-intensive tasks, the inner-workings of these KI methods are not... | Yifan Hou, Guoji Fu, Mrinmaya Sachan |  |
| 104 |  |  [Towards Generalized Open Information Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.103) |  | 0 | Open Information Extraction (OpenIE) facilitates the open-domain discovery of textual facts. However, the prevailing solutions evaluate OpenIE models on in-domain test sets aside from the training corpus, which certainly violates the initial task principle of domain-independence. In this paper, we... |  |  |
| 105 |  |  [BioLORD: Learning Ontological Representations from Definitions for Biomedical Concepts and their Textual Descriptions](https://doi.org/10.18653/v1/2022.findings-emnlp.104) |  | 0 | This work introduces BioLORD, a new pre-training strategy for producing meaningful representations for clinical sentences and biomedical concepts. State-of-the-art methodologies operate by maximizing the similarity in representation of names referring to the same concept, and preventing collapse... | François Remy, Kris Demuynck, Thomas Demeester |  |
| 106 |  |  [Improving the Extraction of Supertags for Constituency Parsing with Linear Context-Free Rewriting Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.105) |  | 0 | In parsing phrase structures, supertagging achieves a symbiosis between the interpretability of formal grammars and the accuracy and speed of more recent neural models.The approach was only recently transferred to parsing discontinuous constituency structures with linear context-free rewriting... | Thomas Ruprecht |  |
| 107 |  |  [Mask More and Mask Later: Efficient Pre-training of Masked Language Models by Disentangling the [MASK] Token](https://doi.org/10.18653/v1/2022.findings-emnlp.106) |  | 0 | The pre-training of masked language models (MLMs) consumes massive computation to achieve good results on downstream NLP tasks, resulting in a large carbon footprint. In the vanilla MLM, the virtual tokens, [MASK]s, act as placeholders and gather the contextualized information from unmasked tokens... | Baohao Liao, David Thulke, Sanjika Hewavitharana, Hermann Ney, Christof Monz |  |
| 108 |  |  [SMSMix: Sense-Maintained Sentence Mixup for Word Sense Disambiguation](https://doi.org/10.18653/v1/2022.findings-emnlp.107) |  | 0 | Word Sense Disambiguation (WSD) is an NLP task aimed at determining the correct sense of a word in a sentence from discrete sense choices. Although current systems have attained unprecedented performances for such tasks, the nonuniform distribution of word senses during training generally results... | Hee Suk Yoon, Eunseop Yoon, John B. Harvill, Sunjae Yoon, Mark HasegawaJohnson, Chang Dong Yoo |  |
| 109 |  |  [On the Effectiveness of Automated Metrics for Text Generation Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.108) |  | 0 | A major challenge in the field of Text Generation is evaluation, because we lack a sound theory that can be leveraged to extract guidelines for evaluation campaigns. In this work, we propose a first step towards such a theory that incorporates different sources of uncertainty, such as imperfect... | Pius von Däniken, Jan Deriu, Don Tuggener, Mark Cieliebak |  |
| 110 |  |  [Residual Learning of Neural Text Generation with n-gram Language Model](https://doi.org/10.18653/v1/2022.findings-emnlp.109) |  | 0 | N-gram language models (LM) has been largely superseded by neural LMs as the latter exhibits better performance. However, we find that n-gram models can achieve satisfactory performance on a large proportion of testing cases, indicating they have already captured abundant knowledge of the language... | Huayang Li, Deng Cai, Jin Xu, Taro Watanabe |  |
| 111 |  |  [DiffG-RL: Leveraging Difference between Environment State and Common Sense](https://doi.org/10.18653/v1/2022.findings-emnlp.110) |  | 0 | Taking into account background knowledge as the context has always been an important part of solving tasks that involve natural language. One representative example of such tasks is text-based games, where players need to make decisions based on both description text previously shown in the game,... | Tsunehiko Tanaka, Daiki Kimura, Michiaki Tatsubori |  |
| 112 |  |  [Unsupervised Syntactically Controlled Paraphrase Generation with Abstract Meaning Representations](https://doi.org/10.18653/v1/2022.findings-emnlp.111) |  | 0 | Syntactically controlled paraphrase generation has become an emerging research direction in recent years. Most existing approaches require annotated paraphrase pairs for training and are thus costly to extend to new domains. Unsupervised approaches, on the other hand, do not need paraphrase pairs... | KuanHao Huang, Varun Iyer, Anoop Kumar, Sriram Venkatapathy, KaiWei Chang, Aram Galstyan |  |
| 113 |  |  [Can AMR Assist Legal and Logical Reasoning?](https://doi.org/10.18653/v1/2022.findings-emnlp.112) |  | 0 | Abstract Meaning Representation (AMR) has been shown to be useful for many downstream tasks. In this work, we explore the use of AMR for legal and logical reasoning. Specifically, we investigate if AMR can help capture logical relationships on multiple choice question answering (MCQA) tasks. We... | Nikolaus Schrack, Ruixiang Cui, Hugo López, Daniel Hershcovich |  |
| 114 |  |  [Data Selection Curriculum for Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.113) |  | 0 | Neural Machine Translation (NMT) models are typically trained on heterogeneous data that are concatenated and randomly shuffled. However, not all of the training data are equally useful to the model. Curriculum training aims to present the data to the NMT models in a meaningful order. In this work,... | Tasnim Mohiuddin, Philipp Koehn, Vishrav Chaudhary, James Cross, Shruti Bhosale, Shafiq R. Joty |  |
| 115 |  |  [Text Editing as Imitation Game](https://doi.org/10.18653/v1/2022.findings-emnlp.114) |  | 0 | Text editing, such as grammatical error correction, arises naturally from imperfect textual data. Recent works frame text editing as a multi-round sequence tagging task, where operations – such as insertion and substitution – are represented as a sequence of tags. While achieving good results, this... | Ning Shi, Bin Tang, Bo Yuan, Longtao Huang, Yewen Pu, Jie Fu, Zhouhan Lin |  |
| 116 |  |  [Seeded Hierarchical Clustering for Expert-Crafted Taxonomies](https://doi.org/10.18653/v1/2022.findings-emnlp.115) |  | 0 | Practitioners from many disciplines (e.g., political science) use expert-crafted taxonomies to make sense of large, unlabeled corpora. In this work, we study Seeded Hierarchical Clustering (SHC): the task of automatically fitting unlabeled data to such taxonomies using a small set of labeled... | Anish Saha, Amith Ananthram, Emily Allaway, Heng Ji, Kathleen R. McKeown |  |
| 117 |  |  [Knowledge Graph Generation From Text](https://doi.org/10.18653/v1/2022.findings-emnlp.116) |  | 0 | In this work we propose a novel end-to-end multi-stage Knowledge Graph (KG) generation system from textual inputs, separating the overall process into two stages. The graph nodes are generated first using pretrained language model, followed by a simple edge construction head, enabling efficient KG... | Igor Melnyk, Pierre L. Dognin, Payel Das |  |
| 118 |  |  [DialogueGAT: A Graph Attention Network for Financial Risk Prediction by Modeling the Dialogues in Earnings Conference Calls](https://doi.org/10.18653/v1/2022.findings-emnlp.117) |  | 0 | Financial risk prediction is an essential task for risk management in capital markets. While traditional prediction models are built based on the hard information of numerical data, recent studies have shown that the soft information of verbal cues in earnings conference calls is significant for... | Yunxin Sang, Yang Bao |  |
| 119 |  |  [Investigating Ensemble Methods for Model Robustness Improvement of Text Classifiers](https://doi.org/10.18653/v1/2022.findings-emnlp.118) |  | 0 | Large pre-trained language models have shown remarkable performance over the past few years. These models, however, sometimes learn superficial features from the dataset and cannot generalize to the distributions that are dissimilar to the training scenario. There have been several approaches... | Jieyu Zhao, Xuezhi Wang, Yao Qin, Jilin Chen, KaiWei Chang |  |
| 120 |  |  [Adaptive Ranking-based Sample Selection for Weakly Supervised Class-imbalanced Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.119) |  | 0 | To obtain a large amount of training labels inexpensively, researchers have recently adopted the weak supervision (WS) paradigm, which leverages labeling rules to synthesize training labels rather than using individual annotations to achieve competitive results for natural language processing (NLP)... | Linxin Song, Jieyu Zhang, Tianxiang Yang, Masayuki Goto |  |
| 121 |  |  [ComFact: A Benchmark for Linking Contextual Commonsense Knowledge](https://doi.org/10.18653/v1/2022.findings-emnlp.120) |  | 0 | Understanding rich narratives, such as dialogues and stories, often requires natural language processing systems to access relevant knowledge from commonsense knowledge graphs. However, these systems typically retrieve facts from KGs using simple heuristics that disregard the complex challenges of... | Silin Gao, Jena D. Hwang, Saya Kanno, Hiromi Wakaki, Yuki Mitsufuji, Antoine Bosselut |  |
| 122 |  |  [Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.121) |  | 0 | How to usefully encode compositional task structure has long been a core challenge in AI. Recent work in chain of thought prompting has shown that for very large neural language models (LMs), explicitly demonstrating the inferential steps involved in a target task may improve performance over... | Victor S. Bursztyn, David Demeter, Doug Downey, Larry Birnbaum |  |
| 123 |  |  [Topic Taxonomy Expansion via Hierarchy-Aware Topic Phrase Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.122) |  | 0 | Topic taxonomies display hierarchical topic structures of a text corpus and provide topical knowledge to enhance various NLP applications. To dynamically incorporate new topic information, several recent studies have tried to expand (or complete) a topic taxonomy by inserting emerging topics... | Dongha Lee, Jiaming Shen, Seonghyeon Lee, Susik Yoon, Hwanjo Yu, Jiawei Han |  |
| 124 |  |  [Language as a fingerprint: Self-supervised learning of user encodings using transformers](https://doi.org/10.18653/v1/2022.findings-emnlp.123) |  | 0 | The way we talk carries information about who we are. Demographics, personality, clinical conditions, political preferences influence what we speak about and how, suggesting that many individual attributes could be inferred from adequate encodings of linguistic behavior. Conversely, conditioning... | Roberta Rocca, Tal Yarkoni |  |
| 125 |  |  [Hyperdecoders: Instance-specific decoders for multi-task NLP](https://doi.org/10.18653/v1/2022.findings-emnlp.124) |  | 0 | We investigate input-conditioned hypernetworks for multi-tasking in NLP, generating parameter-efficient adaptations for a decoder using a hypernetwork conditioned on the output of an encoder. This approach produces a unique decoder adaptation for every input instance, allowing the network a larger... | Hamish Ivison, Matthew E. Peters |  |
| 126 |  |  [Evaluating the Faithfulness of Importance Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining](https://doi.org/10.18653/v1/2022.findings-emnlp.125) |  | 0 | To explain NLP models a popular approach is to use importance measures, such as attention, which inform input tokens are important for making a prediction. However, an open question is how well these explanations accurately reflect a model’s logic, a property called faithfulness. To answer this... | Andreas Madsen, Nicholas Meade, Vaibhav Adlakha, Siva Reddy |  |
| 127 |  |  [Towards Explaining Subjective Ground of Individuals on Social Media](https://doi.org/10.18653/v1/2022.findings-emnlp.126) |  | 0 | Large-scale language models have been reducing the gap between machines and humans in understanding the real world, yet understanding an individual’s theory of mind and behavior from text is far from being resolved. This research proposes a neural model—Subjective Ground Attention—that learns... | Younghun Lee, Dan Goldwasser |  |
| 128 |  |  [Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot ICD Coding](https://doi.org/10.18653/v1/2022.findings-emnlp.127) |  | 0 | Automatic International Classification of Diseases (ICD) coding aims to assign multiple ICD codes to a medical note with average length of 3,000+ tokens. This task is challenging due to a high-dimensional space of multi-label assignment (tens of thousands of ICD codes) and the long-tail challenge:... | Zhichao Yang, Shufan Wang, Bhanu Pratap Singh Rawat, Avijit Mitra, Hong Yu |  |
| 129 |  |  [Do Language Models Understand Measurements?](https://doi.org/10.18653/v1/2022.findings-emnlp.128) |  | 0 | Recent success of pre-trained language models (PLMs) has stimulated interest in their ability to understand and work with numbers. Yet, the numerical reasoning over measurements has not been formally studied despite their importance. In this study, we show that PLMs lack the capability required for... | Sungjin Park, Seungwoo Ryu, Edward Choi |  |
| 130 |  |  [Reconciliation of Pre-trained Models and Prototypical Neural Networks in Few-shot Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.129) |  | 0 | Incorporating large-scale pre-trained models with the prototypical neural networks is a de-facto paradigm in few-shot named entity recognition. Existing methods, unfortunately, are not aware of the fact that embeddings from pre-trained models contain a prominently large amount of information... | Youcheng Huang, Wenqiang Lei, Jie Fu, Jiancheng Lv |  |
| 131 |  |  [HCL-TAT: A Hybrid Contrastive Learning Method for Few-shot Event Detection with Task-Adaptive Threshold](https://doi.org/10.18653/v1/2022.findings-emnlp.130) |  | 0 | Event detection has been suffering from constantly emerging event types with lack of sufficient data. Existing works formulate the new problem as few-shot event detection (FSED), and employ two-stage or unified models based on meta-learning to address the problem. However, these methods fall far... | Ruihan Zhang, Wei Wei, XianLing Mao, Rui Fang, Dangyang Chen |  |
| 132 |  |  [Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots](https://doi.org/10.18653/v1/2022.findings-emnlp.131) |  | 0 | This paper introduces Doc2Bot, a novel dataset for building machines that help users seek information via conversations. This is of particular interest for companies and organizations that own a large number of manuals or instruction books. Despite its potential, the nature of our task poses... | Haomin Fu, Yeqin Zhang, Haiyang Yu, Jian Sun, Fei Huang, Luo Si, Yongbin Li, CamTu Nguyen |  |
| 133 |  |  [DualNER: A Dual-Teaching framework for Zero-shot Cross-lingual Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.132) |  | 0 | We present DualNER, a simple and effective framework to make full use of both annotated source language corpus and unlabeled target language text for zero-shot cross-lingual named entity recognition (NER). In particular, we combine two complementary learning paradigms of NER, i.e., sequence... | Jiali Zeng, Yufan Jiang, Yongjing Yin, Xu Wang, Binghuai Lin, Yunbo Cao |  |
| 134 |  |  [Knowledge-augmented Self-training of A Question Rewriter for Conversational Knowledge Base Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.133) |  | 0 | The recent rise of conversational applications such as online customer service systems and intelligent personal assistants has promoted the development of conversational knowledge base question answering (ConvKBQA). Different from the traditional single-turn KBQA, ConvKBQA usually explores... | Xirui Ke, Jing Zhang, Xin Lv, Yiqi Xu, Shulin Cao, Cuiping Li, Hong Chen, Juanzi Li |  |
| 135 |  |  [Extractive Summarization of Legal Decisions using Multi-task Learning and Maximal Marginal Relevance](https://doi.org/10.18653/v1/2022.findings-emnlp.134) |  | 0 | Summarizing legal decisions requires the expertise of law practitioners, which is both time- and cost-intensive. This paper presents techniques for extractive summarization of legal decisions in a low-resource setting using limited expert annotated data. We test a set of models that locate relevant... | Abhishek Agarwal, Shanshan Xu, Matthias Grabmair |  |
| 136 |  |  [MovieUN: A Dataset for Movie Understanding and Narrating](https://doi.org/10.18653/v1/2022.findings-emnlp.135) |  | 0 | Automatic movie narration generation and narration grounding are very important to provide a true movie experience for the blind and visually impaired. To tell the movie story well, it is necessary to mention plot-related details (such as character names) and keep the narrations in a plot coherent.... | Qi Zhang, Zihao Yue, Anwen Hu, Ziheng Wang, Qin Jin |  |
| 137 |  |  [ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.136) |  | 0 | Data-to-text generation is challenging due to the great variety of the input data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse predicates). Recent end-to-end neural methods thus require substantial training examples to learn to disambiguate and describe the data. Yet,... | Jiannan Xiang, Zhengzhong Liu, Yucheng Zhou, Eric P. Xing, Zhiting Hu |  |
| 138 |  |  [FCGEC: Fine-Grained Corpus for Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2022.findings-emnlp.137) |  | 0 | Grammatical Error Correction (GEC) has been broadly applied in automatic correction and proofreading system recently. However, it is still immature in Chinese GEC due to limited high-quality data from native speakers in terms of category and scale. In this paper, we present FCGEC, a fine-grained... | Lvxiaowei Xu, Jianwang Wu, Jiawei Peng, Jiayu Fu, Ming Cai |  |
| 139 |  |  [Audience-Centric Natural Language Generation via Style Infusion](https://doi.org/10.18653/v1/2022.findings-emnlp.138) |  | 0 | Adopting contextually appropriate, audience-tailored linguistic styles is critical to the success of user-centric language generation systems (e.g., chatbots, computer-aided writing, dialog systems). While existing approaches demonstrate text style transfer (TST) with large volumes of parallel or... | Samraj Moorjani, Adit Krishnan, Hari Sundaram, Ewa Maslowska, Aravind Sankar |  |
| 140 |  |  [DocFin: Multimodal Financial Prediction and Bias Mitigation using Semi-structured Documents](https://doi.org/10.18653/v1/2022.findings-emnlp.139) |  | 0 | Financial prediction is complex due to the stochastic nature of the stock market. Semi-structured financial documents present comprehensive financial data in tabular formats, such as earnings, profit-loss statements, and balance sheets, and can often contain rich technical analysis along with a... | Puneet Mathur, Mihir Goyal, Ramit Sawhney, Ritik Mathur, Jochen L. Leidner, Franck Dernoncourt, Dinesh Manocha |  |
| 141 |  |  [Not Just Plain Text! Fuel Document-Level Relation Extraction with Explicit Syntax Refinement and Subsentence Modeling](https://doi.org/10.18653/v1/2022.findings-emnlp.140) |  | 0 | Document-level relation extraction (DocRE) aims to identify semantic labels among entities within a single document. One major challenge of DocRE is to dig decisive details regarding a specific entity pair from long text. However, in many cases, only a fraction of text carries required information,... | Zhichao Duan, Xiuxing Li, Zhenyu Li, Zhuo Wang, Jianyong Wang |  |
| 142 |  |  [Self-supervised Rewiring of Pre-trained Speech Encoders: Towards Faster Fine-tuning with Less Labels in Speech Processing](https://doi.org/10.18653/v1/2022.findings-emnlp.141) |  | 0 | Pre-trained speech Transformers have facilitated great success across various speech processing tasks. However, fine-tuning these encoders for downstream tasks require sufficiently large training data to converge or to achieve state-of-the-art. In text domain this has been partly attributed to... | Hao Yang, Jinming Zhao, Gholamreza Haffari, Ehsan Shareghi |  |
| 143 |  |  [RedApt: An Adaptor for wav2vec 2 EncodingFaster and Smaller Speech Translation without Quality Compromise](https://doi.org/10.18653/v1/2022.findings-emnlp.142) |  | 0 | Pre-trained speech Transformers in speech translation (ST) have facilitated state-of-the-art (SotA) results; yet, using such encoders is computationally expensive. To improve this, we present a novel Reducer Adaptor block, RedApt, that could be seamlessly integrated within any Transformer-based... | Jinming Zhao, Hao Yang, Gholamreza Haffari, Ehsan Shareghi |  |
| 144 |  |  [How sensitive are translation systems to extra contexts? Mitigating gender bias in Neural Machine Translation models through relevant contexts](https://doi.org/10.18653/v1/2022.findings-emnlp.143) |  | 0 | Neural Machine Translation systems built on top of Transformer-based architectures are routinely improving the state-of-the-art in translation quality according to word-overlap metrics. However, a growing number of studies also highlight the inherent gender bias that these models incorporate during... | Shanya Sharma, Manan Dey, Koustuv Sinha |  |
| 145 |  |  [P\textM²\textF²N: Patient Multi-view Multi-modal Feature Fusion Networks for Clinical Outcome Prediction](https://doi.org/10.18653/v1/2022.findings-emnlp.144) |  | 0 | Clinical outcome prediction is critical to the condition prediction of patients and management of hospital capacities. There are two kinds of medical data, including time series signals recorded by various devices and clinical notes in electronic health records (EHR), which are used for two common... | Ying Zhang, Baohang Zhou, Kehui Song, Xuhui Sui, Guoqing Zhao, Ning Jiang, Xiaojie Yuan |  |
| 146 |  |  [Long Text and Multi-Table Summarization: Dataset and Method](https://doi.org/10.18653/v1/2022.findings-emnlp.145) |  | 0 | Automatic document summarization aims to produce a concise summary covering the input document’s salient information. Within a report document, the salient information can be scattered in the textual and non-textual content. However, existing document summarization datasets and methods usually... | Shuaiqi Liu, Jiannong Cao, Ruosong Yang, Zhiyuan Wen |  |
| 147 |  |  [MatRank: Text Re-ranking by Latent Preference Matrix](https://doi.org/10.18653/v1/2022.findings-emnlp.146) |  | 0 | Text ranking plays a key role in providing content that best answers user queries. It is usually divided into two sub-tasks to perform efficient information retrieval given a query: text retrieval and text re-ranking. Recent research on pretrained language models (PLM) has demonstrated efficiency... | Jinwen Luo, Jiuding Yang, Weidong Guo, Chenglin Li, Di Niu, Yu Xu |  |
| 148 |  |  [Can Language Models Serve as Temporal Knowledge Bases?](https://doi.org/10.18653/v1/2022.findings-emnlp.147) |  | 0 | Recent progress regarding the use of language models (LMs) as knowledge bases (KBs) has shown that language models can act as structured knowledge bases for storing relational facts. However, most existing works only considered the LM-as-KB paradigm in a static setting, which ignores the analysis... | Ruilin Zhao, Feng Zhao, Guandong Xu, Sixiao Zhang, Hai Jin |  |
| 149 |  |  [Are Large Pre-Trained Language Models Leaking Your Personal Information?](https://doi.org/10.18653/v1/2022.findings-emnlp.148) |  | 0 | Are Large Pre-Trained Language Models Leaking Your Personal Information? In this paper, we analyze whether Pre-Trained Language Models (PLMs) are prone to leaking personal information. Specifically, we query PLMs for email addresses with contexts of the email address or prompts containing the... | Jie Huang, Hanyin Shao, Kevin ChenChuan Chang |  |
| 150 |  |  [Self-Distillation with Meta Learning for Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.findings-emnlp.149) |  | 0 | In this paper, we propose a self-distillation framework with meta learning (MetaSD) for knowledge graph completion with dynamic pruning, which aims to learn compressed graph embeddings and tackle the long-tail samples. Specifically, we first propose a dynamic pruning technique to obtain a small... | Yunshui Li, Junhao Liu, Min Yang, Chengming Li |  |
| 151 |  |  [CQR-SQL: Conversational Question Reformulation Enhanced Context-Dependent Text-to-SQL Parsers](https://doi.org/10.18653/v1/2022.findings-emnlp.150) |  | 0 | Context-dependent text-to-SQL is the task of translating multi-turn questions into database-related SQL queries. Existing methods typically focus on making full use of history context or previously predicted SQL for currently SQL parsing, while neglecting to explicitly comprehend the schema and... | Dongling Xiao, Linzheng Chai, QianWen Zhang, Zhao Yan, Zhoujun Li, Yunbo Cao |  |
| 152 |  |  [Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document](https://doi.org/10.18653/v1/2022.findings-emnlp.151) |  | 0 | Given the recent proliferation of false claims online, there has been a lot of manual fact-checking effort. As this is very time-consuming, human fact-checkers can benefit from tools that can support them and make them more efficient. Here, we focus on building a system that could provide such... | Shaden Shaar, Nikola Georgiev, Firoj Alam, Giovanni Da San Martino, Aisha Mohamed, Preslav Nakov |  |
| 153 |  |  [No Word Embedding Model Is Perfect: Evaluating the Representation Accuracy for Social Bias in the Media](https://doi.org/10.18653/v1/2022.findings-emnlp.152) |  | 0 | News articles both shape and reflect public opinion across the political spectrum. Analyzing them for social bias can thus provide valuable insights, such as prevailing stereotypes in society and the media, which are often adopted by NLP models trained on respective data. Recent work has relied on... | Maximilian Spliethöver, Maximilian Keiff, Henning Wachsmuth |  |
| 154 |  |  [Scientific and Creative Analogies in Pretrained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.153) |  | 0 | This paper examines the encoding of analogy in large-scale pretrained language models, such as BERT and GPT-2. Existing analogy datasets typically focus on a limited set of analogical relations, with a high similarity of the two domains between which the analogy holds. As a more realistic setup, we... | Tamara Czinczoll, Helen Yannakoudakis, Pushkar Mishra, Ekaterina Shutova |  |
| 155 |  |  [Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages](https://doi.org/10.18653/v1/2022.findings-emnlp.154) |  | 0 | Scaling multilingual representation learning beyond the hundred most frequent languages is challenging, in particular to cover the long tail of low-resource languages. We move away from the popular one-for-all multilingual models and focus on training multiple language (family) specific... | Kevin Heffernan, Onur Çelebi, Holger Schwenk |  |
| 156 |  |  [Towards Generalizable and Robust Text-to-SQL Parsing](https://doi.org/10.18653/v1/2022.findings-emnlp.155) |  | 0 | Text-to-SQL parsing tackles the problem of mapping natural language questions to executable SQL queries. In practice, text-to-SQL parsers often encounter various challenging scenarios, requiring them to be generalizable and robust. While most existing work addresses a particular generalization or... | Chang Gao, Bowen Li, Wenxuan Zhang, Wai Lam, Binhua Li, Fei Huang, Luo Si, Yongbin Li |  |
| 157 |  |  [EdiT5: Semi-Autoregressive Text Editing with T5 Warm-Start](https://doi.org/10.18653/v1/2022.findings-emnlp.156) |  | 0 | We present EdiT5 - a novel semi-autoregressive text-editing approach designed to combine the strengths of non-autoregressive text-editing and autoregressive decoding. EdiT5 is faster at inference times than conventional sequence-to-sequence (seq2seq) models, while being capable of modeling flexible... | Jonathan Mallinson, Jakub Adámek, Eric Malmi, Aliaksei Severyn |  |
| 158 |  |  [A Critical Reflection and Forward Perspective on Empathy and Natural Language Processing](https://doi.org/10.18653/v1/2022.findings-emnlp.157) |  | 0 | We review the state of research on empathy in natural language processing and identify the following issues: (1) empathy definitions are absent or abstract, which (2) leads to low construct validity and reproducibility. Moreover, (3) emotional empathy is overemphasized, skewing our focus to a... | Allison Lahnala, Charles Welch, David Jurgens, Lucie Flek |  |
| 159 |  |  [A Neural-Symbolic Approach to Natural Language Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.158) |  | 0 | Deep neural networks, empowered by pre-trained language models, have achieved remarkable results in natural language understanding (NLU) tasks. However, their performances can drastically deteriorate when logical reasoning is needed. This is because NLU in principle depends on not only analogical... | Zhixuan Liu, Zihao Wang, Yuan Lin, Hang Li |  |
| 160 |  |  [Social-aware Sparse Attention Network for Session-based Social Recommendation](https://doi.org/10.18653/v1/2022.findings-emnlp.159) |  | 0 | Session-based Social Recommendation (SSR) aims to use users’ social networks and historical sessions to provide more personalized recommendations for the current session.Unfortunately, existing SSR methods have two limitations.First, they do not screen users’ useless social relationships and noisy... | Kai Ouyang, Xianghong Xu, Chen Tang, Wang Chen, Haitao Zheng |  |
| 161 |  |  [SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters](https://doi.org/10.18653/v1/2022.findings-emnlp.160) |  | 0 | Adapter Tuning, which freezes the pretrained language models (PLMs) and only fine-tunes a few extra modules, becomes an appealing efficient alternative to the full model fine-tuning. Although computationally efficient, the recent Adapters often increase parameters (e.g. bottleneck dimension) for... | Shwai He, Liang Ding, Daize Dong, Jeremy Zhang, Dacheng Tao |  |
| 162 |  |  [Measurement Extraction with Natural Language Processing: A Review](https://doi.org/10.18653/v1/2022.findings-emnlp.161) |  | 0 | Quantitative data is important in many domains. Information extraction methods draw structured data from documents. However, the extraction of quantities and their contexts has received little attention in the history of information extraction. In this review, an overview of prior work on... | Jan Göpfert, Patrick Kuckertz, Jann M. Weinand, Leander Kotzur, Detlef Stolten |  |
| 163 |  |  [Summarizing Procedural Text: Data and Approach](https://doi.org/10.18653/v1/2022.findings-emnlp.162) |  | 0 | Procedural text is a widely used genre that contains many steps of instructions of how to cook a dish or how to conduct a chemical experiment and analyze the procedural text has become a popular task in the NLP field. Since the procedural text can be very long and contains many details, summarizing... | Shen Gao, Haotong Zhang, Xiuying Chen, Rui Yan, Dongyan Zhao |  |
| 164 |  |  [Snapshot-Guided Domain Adaptation for ELECTRA](https://doi.org/10.18653/v1/2022.findings-emnlp.163) |  | 0 | Discriminative pre-trained language models, such as ELECTRA, have achieved promising performances in a variety of general tasks. However, these generic pre-trained models struggle to capture domain-specific knowledge of domain-related tasks. In this work, we propose a novel domain-adaptation method... | Daixuan Cheng, Shaohan Huang, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Furu Wei, Denvy Deng, Qi Zhang |  |
| 165 |  |  [Exploiting Labeled and Unlabeled Data via Transformer Fine-tuning for Peer-Review Score Prediction](https://doi.org/10.18653/v1/2022.findings-emnlp.164) |  | 0 | Automatic Peer-review Aspect Score Prediction (PASP) of academic papers can be a helpful assistant tool for both reviewers and authors. Most existing works on PASP utilize supervised learning techniques. However, the limited number of peer-review data deteriorates the performance of PASP. This... | Panitan Muangkammuen, Fumiyo Fukumoto, Jiyi Li, Yoshimi Suzuki |  |
| 166 |  |  [HARALD: Augmenting Hate Speech Data Sets with Real Data](https://doi.org/10.18653/v1/2022.findings-emnlp.165) |  | 0 | The successful completion of the hate speech detection task hinges upon the availability of rich and variable labeled data, which is hard to obtain. In this work, we present a new approach for data augmentation that uses as input real unlabelled data, which is carefully selected from online... | Ilan Tal, Dan Vilenchik |  |
| 167 |  |  [Wait-info Policy: Balancing Source and Target at Information Level for Simultaneous Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.166) |  | 0 | Simultaneous machine translation (SiMT) outputs the translation while receiving the source inputs, and hence needs to balance the received source information and translated target information to make a reasonable decision between waiting for inputs or outputting translation. Previous methods always... | Shaolei Zhang, Shoutao Guo, Yang Feng |  |
| 168 |  |  [Turning Fixed to Adaptive: Integrating Post-Evaluation into Simultaneous Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.167) |  | 0 | Simultaneous machine translation (SiMT) starts its translation before reading the whole source sentence and employs either fixed or adaptive policy to generate the target sentence. Compared to the fixed policy, the adaptive policy achieves better latency-quality tradeoffs by adopting a flexible... | Shoutao Guo, Shaolei Zhang, Yang Feng |  |
| 169 |  |  [Alleviating Sparsity of Open Knowledge Graphs with Ternary Contrastive Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.168) |  | 0 | Sparsity of formal knowledge and roughness of non-ontological construction make sparsity problem particularly prominent in Open Knowledge Graphs (OpenKGs). Due to sparse links, learning effective representation for few-shot entities becomes difficult. We hypothesize that by introducing negative... | Qian Li, Shafiq R. Joty, Daling Wang, Shi Feng, Yifei Zhang |  |
| 170 |  |  [Using Developer Discussions to Guide Fixing Bugs in Software](https://doi.org/10.18653/v1/2022.findings-emnlp.169) |  | 0 | Automatically fixing software bugs is a challenging task. While recent work showed that natural language context is useful in guiding bug-fixing models, the approach required prompting developers to provide this context, which was simulated through commit messages written after the bug-fixing code... | Sheena Panthaplackel, Milos Gligoric, Junyi Jessy Li, Raymond J. Mooney |  |
| 171 |  |  [AutoCAD: Automatically Generate Counterfactuals for Mitigating Shortcut Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.170) |  | 0 | Recent studies have shown the impressive efficacy of counterfactually augmented data (CAD) for reducing NLU models’ reliance on spurious features and improving their generalizability. However, current methods still heavily rely on human efforts or task-specific designs to generate counterfactuals,... | Jiaxin Wen, Yeshuang Zhu, Jinchao Zhang, Jie Zhou, Minlie Huang |  |
| 172 |  |  [A Multi-Modal Knowledge Graph for Classical Chinese Poetry](https://doi.org/10.18653/v1/2022.findings-emnlp.171) |  | 0 | Classical Chinese poetry has a long history and is a precious cultural heritage of humankind. Displaying the classical Chinese poetry in a visual way, helps to cross cultural barriers in different countries, making it enjoyable for all the people. In this paper, we construct a multi-modal knowledge... | Yuqing Li, Yuxin Zhang, Bin Wu, JiRong Wen, Ruihua Song, Ting Bai |  |
| 173 |  |  [Assessing Non-autoregressive Alignment in Neural Machine Translation via Word Reordering](https://doi.org/10.18653/v1/2022.findings-emnlp.172) |  | 0 | Recent work on non-autoregressive neural machine translation (NAT) that leverages alignment information to explicitly reduce the modality of target distribution has reported comparable performance with counterparts that tackle multi-modality problem by implicitly modeling dependencies.... | ChunHin Tse, Ester Leung, William K. Cheung |  |
| 174 |  |  [Syntax-guided Localized Self-attention by Constituency Syntactic Distance](https://doi.org/10.18653/v1/2022.findings-emnlp.173) |  | 0 | Recent works have revealed that Transformers are implicitly learning the syntactic information in its lower layers from data, albeit is highly dependent on the quality and scale of the training data. However, learning syntactic information from data is not necessary if we can leverage an external... | Shengyuan Hou, Jushi Kai, Haotian Xue, Bingyu Zhu, Bo Yuan, Longtao Huang, Xinbing Wang, Zhouhan Lin |  |
| 175 |  |  [CodeExp: Explanatory Code Document Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.174) |  | 0 | Developing models that can automatically generate detailed code explanation can greatly benefit software maintenance and programming education. However, existing code-to-text generation models often produce only high-level summaries of code that do not capture implementation-level choices essential... | Haotian Cui, Chenglong Wang, Junjie Huang, Jeevana Priya Inala, Todd Mytkowicz, Bo Wang, Jianfeng Gao, Nan Duan |  |
| 176 |  |  [PAUQ: Text-to-SQL in Russian](https://doi.org/10.18653/v1/2022.findings-emnlp.175) |  | 0 | Semantic parsing is an important task that allows to democratize human-computer interaction. One of the most popular text-to-SQL datasets with complex and diverse natural language (NL) questions and SQL queries is Spider. We construct and complement a Spider dataset for Russian, thus creating the... | Daria Bakshandaeva, Oleg Somov, Ekaterina Dmitrieva, Vera Davydova, Elena Tutubalina |  |
| 177 |  |  [Event-Centric Question Answering via Contrastive Learning and Invertible Event Transformation](https://doi.org/10.18653/v1/2022.findings-emnlp.176) |  | 0 | Human reading comprehension often requires reasoning of event semantic relations in narratives, represented by Event-centric Question-Answering (QA). To address event-centric QA, we propose a novel QA model with contrastive learning and invertible event transformation, call TranCLR. Our proposed... | Junru Lu, Xingwei Tan, Gabriele Pergola, Lin Gui, Yulan He |  |
| 178 |  |  [Label-Driven Denoising Framework for Multi-Label Few-Shot Aspect Category Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.177) |  | 0 | Multi-Label Few-Shot Aspect Category Detection (FS-ACD) is a new sub-task of aspect-based sentiment analysis, which aims to detect aspect categories accurately with limited training instances. Recently, dominant works use the prototypical network to accomplish this task, and employ the attention... | Fei Zhao, Yuchen Shen, Zhen Wu, Xinyu Dai |  |
| 179 |  |  [Visual Named Entity Linking: A New Dataset and A Baseline](https://doi.org/10.18653/v1/2022.findings-emnlp.178) |  | 0 | Visual Entity Linking (VEL) is a task to link regions of images with their corresponding entities in Knowledge Bases (KBs), which is beneficial for many computer vision tasks such as image retrieval, image caption, and visual question answering. While existing tasks in VEL either rely on textual... | Wen Sun, Yixing Fan, Jiafeng Guo, Ruqing Zhang, Xueqi Cheng |  |
| 180 |  |  [MAGMA - Multimodal Augmentation of Generative Models through Adapter-based Finetuning](https://doi.org/10.18653/v1/2022.findings-emnlp.179) |  | 0 | Large-scale pretraining is fast becoming the norm in Vision-Language (VL) modeling. However, prevailing VL approaches are limited by the requirement for labeled data and the use of complex multi-step pretraining objectives. We present MAGMA - a simple method for augmenting generative language... | Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, Anette Frank |  |
| 181 |  |  [Towards Tracing Knowledge in Language Models Back to the Training Data](https://doi.org/10.18653/v1/2022.findings-emnlp.180) |  | 0 | Language models (LMs) have been shown to memorize a great deal of factual knowledge contained in their training data. But when an LM generates an assertion, it is often difficult to determine where it learned this information and whether it is true. In this paper, we propose the problem of fact... | Ekin Akyürek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, Kelvin Guu |  |
| 182 |  |  [ReaRev: Adaptive Reasoning for Question Answering over Knowledge Graphs](https://doi.org/10.18653/v1/2022.findings-emnlp.181) |  | 0 | Knowledge Graph Question Answering (KGQA) involves retrieving entities as answers from a Knowledge Graph (KG) using natural language queries. The challenge is to learn to reason over question-relevant KG facts that traverse KG entities and lead to the question answers. To facilitate reasoning, the... | Costas Mavromatis, George Karypis |  |
| 183 |  |  [Understanding Social Media Cross-Modality Discourse in Linguistic Space](https://doi.org/10.18653/v1/2022.findings-emnlp.182) |  | 0 | The multimedia communications with texts and images are popular on social media. However, limited studies concern how images are structured with texts to form coherent meanings in human cognition. To fill in the gap, we present a novel concept of cross-modality discourse, reflecting how human... | Chunpu Xu, Hanzhuo Tan, Jing Li, Piji Li |  |
| 184 |  |  [TAPE: Assessing Few-shot Russian Language Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.183) |  | 0 | Recent advances in zero-shot and few-shot learning have shown promise for a scope of research and practical purposes. However, this fast-growing area lacks standardized evaluation suites for non-English languages, hindering progress outside the Anglo-centric paradigm. To address this line of... | Ekaterina Taktasheva, Alena Fenogenova, Denis Shevelev, Nadezhda Katricheva, Maria Tikhonova, Albina Akhmetgareeva, Oleg Zinkevich, Anastasiia Bashmakova, Svetlana Iordanskaia, Valentina Kurenshchikova, Alena Spiridonova, Ekaterina Artemova, Tatiana Shavrina, Vladislav Mikhailov |  |
| 185 |  |  [A Hierarchical N-Gram Framework for Zero-Shot Link Prediction](https://doi.org/10.18653/v1/2022.findings-emnlp.184) |  | 0 | Knowledge graphs typically contain a large number of entities but often cover only a fraction of all relations between them (i.e., incompleteness). Zero-shot link prediction (ZSLP) is a popular way to tackle the problem by automatically identifying unobserved relations between entities. Most recent... | Mingchen Li, Junfan Chen, Samuel Mensah, Nikolaos Aletras, Xiulong Yang, Yang Ye |  |
| 186 |  |  [Quadapter: Adapter for GPT-2 Quantization](https://doi.org/10.18653/v1/2022.findings-emnlp.185) |  | 0 | Transformer language models such as GPT-2 are difficult to quantize because of outliers in the activations leading to a large quantization error. To adapt to the error, one must use quantization-aware training, which entails a fine-tuning process based on the dataset and the training pipeline... | Minseop Park, Jaeseong You, Markus Nagel, Simyung Chang |  |
| 187 |  |  [BanglaRQA: A Benchmark Dataset for Under-resourced Bangla Language Reading Comprehension-based Question Answering with Diverse Question-Answer Types](https://doi.org/10.18653/v1/2022.findings-emnlp.186) |  | 0 | High-resource languages, such as English, have access to a plethora of datasets with various question-answer types resembling real-world reading comprehension. However, there is a severe lack of diverse and comprehensive question-answering datasets in under-resourced languages like Bangla. The ones... | Syed Mohammed Sartaj Ekram, Adham Arik Rahman, Md. Sajid Altaf, Mohammed Saidul Islam, Mehrab Mustafy Rahman, Md Mezbaur Rahman, Md Azam Hossain, Abu Raihan Mostofa Kamal |  |
| 188 |  |  [Chaining Simultaneous Thoughts for Numerical Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.187) |  | 0 | Given that rich information is hidden behind ubiquitous numbers in text, numerical reasoning over text should be an essential skill of AI systems. To derive precise equations to solve numerical reasoning problems, previous work focused on modeling the structures of equations, and has proposed... | Zhihong Shao, Fei Huang, Minlie Huang |  |
| 189 |  |  [Inferring Implicit Relations in Complex Questions with Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.188) |  | 0 | A prominent challenge for modern language understanding systems is the ability to answer implicit reasoning questions, where the required reasoning steps for answering the question are not mentioned in the text explicitly. In this work, we investigate why current models struggle with implicit... | Uri Katz, Mor Geva, Jonathan Berant |  |
| 190 |  |  [Eliciting and Understanding Cross-task Skills with Task-level Mixture-of-Experts](https://doi.org/10.18653/v1/2022.findings-emnlp.189) |  | 0 | Recent works suggest that transformer models are capable of multi-tasking on diverse NLP tasks and adapt to new tasks efficiently. However, the potential of these multi-task models may be limited as they use the same set of parameters for all tasks. In contrast, humans tackle tasks in a more... | Qinyuan Ye, Juan Zha, Xiang Ren |  |
| 191 |  |  [On the Curious Case of l2 norm of Sense Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.190) |  | 0 | We show that the l2 norm of a static sense embedding encodes information related to the frequency of that sense in the training corpus used to learn the sense embeddings. This finding can be seen as an extension of a previously known relationship for word embeddings to sense embeddings. Our... | Yi Zhou, Danushka Bollegala |  |
| 192 |  |  [Partially-Random Initialization: A Smoking Gun for Binarization Hypothesis of BERT](https://doi.org/10.18653/v1/2022.findings-emnlp.191) |  | 0 | In the past few years, pre-trained BERT has become one of the most popular deep-learning language models due to their remarkable performance in natural language processing (NLP) tasks. However, the superior performance of BERT comes at the cost of high computational and memory complexity, hindering... | Arash Ardakani |  |
| 193 |  |  [Prompt Consistency for Zero-Shot Task Generalization](https://doi.org/10.18653/v1/2022.findings-emnlp.192) |  | 0 | One of the most impressive results of recent NLP history is the ability of pre-trained language models to solve new tasks in a zero-shot setting. To achieve this, NLP tasks are framed as natural language prompts, generating a response indicating the predicted output. Nonetheless, the performance in... | Chunting Zhou, Junxian He, Xuezhe Ma, Taylor BergKirkpatrick, Graham Neubig |  |
| 194 |  |  [In-Context Learning for Few-Shot Dialogue State Tracking](https://doi.org/10.18653/v1/2022.findings-emnlp.193) |  | 0 | Collecting and annotating task-oriented dialogues is time-consuming and costly. Thus, zero and few shot learning for dialogue tasks presents an exciting opportunity. In this work, we propose an in-context (IC) learning framework for zero-shot and few-shot learning dialogue state tracking (DST),... | Yushi Hu, ChiaHsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, Mari Ostendorf |  |
| 195 |  |  [On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization](https://doi.org/10.18653/v1/2022.findings-emnlp.194) |  | 0 | Combining the visual modality with pretrained language models has been surprisingly effective for simple descriptive tasks such as image captioning. More general text generation however remains elusive. We take a step back and ask: How do these models work for more complex generative tasks, i.e.... | Shruti Palaskar, Akshita Bhagia, Yonatan Bisk, Florian Metze, Alan W. Black, Ana Marasovic |  |
| 196 |  |  [The challenges of temporal alignment on Twitter during crises](https://doi.org/10.18653/v1/2022.findings-emnlp.195) |  | 0 | Language use changes over time, and this impacts the effectiveness of NLP systems. This phenomenon is even more prevalent in social media data during crisis events where meaning and frequency of word usage may change over the course of days. Contextual language models fail to adapt temporally,... | Aniket Pramanick, Tilman Beck, Kevin Stowe, Iryna Gurevych |  |
| 197 |  |  [Experimental Standards for Deep Learning in Natural Language Processing Research](https://doi.org/10.18653/v1/2022.findings-emnlp.196) |  | 0 | The field of Deep Learning (DL) has undergone explosive growth during the last decade, with a substantial impact on Natural Language Processing (NLP) as well. Yet, compared to more established disciplines, a lack of common experimental standards remains an open challenge to the field at large.... | Dennis Ulmer, Elisa Bassignana, Max MüllerEberstein, Daniel Varab, Mike Zhang, Rob van der Goot, Christian Hardmeier, Barbara Plank |  |
| 198 |  |  [Few-Shot Anaphora Resolution in Scientific Protocols via Mixtures of In-Context Experts](https://doi.org/10.18653/v1/2022.findings-emnlp.197) |  | 0 | Anaphora resolution is an important task for information extraction across a range of languages, text genres, and domains, motivating the need for methods that do not require large annotated datasets. In-context learning has emerged as a promising approach, yet there are a number of challenges in... | Nghia T. Le, Fan Bai, Alan Ritter |  |
| 199 |  |  [Exploring Predictive Uncertainty and Calibration in NLP: A Study on the Impact of Method & Data Scarcity](https://doi.org/10.18653/v1/2022.findings-emnlp.198) |  | 0 | We investigate the problem of determining the predictive confidence (or, conversely, uncertainty) of a neural classifier through the lens of low-resource languages. By training models on sub-sampled datasets in three different languages, we assess the quality of estimates from a wide array of... | Dennis Ulmer, Jes Frellsen, Christian Hardmeier |  |
| 200 |  |  [Conditional Supervised Contrastive Learning for Fair Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.199) |  | 0 | Contrastive representation learning has gained much attention due to its superior performance in learning representations from both image and sequential data. However, the learned representations could potentially lead to performance disparities in downstream tasks, such as increased silencing of... | Jianfeng Chi, William Shand, Yaodong Yu, KaiWei Chang, Han Zhao, Yuan Tian |  |
| 201 |  |  [SpaBERT: A Pretrained Language Model from Geographic Data for Geo-Entity Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.200) |  | 0 | Named geographic entities (geo-entities for short) are the building blocks of many geographic datasets. Characterizing geo-entities is integral to various application domains, such as geo-intelligence and map comprehension, while a key challenge is to capture the spatial-varying context of an... | Zekun Li, Jina Kim, YaoYi Chiang, Muhao Chen |  |
| 202 |  |  [Self-training with Two-phase Self-augmentation for Few-shot Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.201) |  | 0 | In task-oriented dialogue systems, response generation from meaning representations (MRs) often suffers from limited training examples, due to the high cost of annotating MR-to-Text pairs. Previous works on self-training leverage fine-tuned conversational models to automatically generate... | Wanyu Du, Hanjie Chen, Yangfeng Ji |  |
| 203 |  |  [Is NLP Ready for Standardization?](https://doi.org/10.18653/v1/2022.findings-emnlp.202) |  | 0 | While standardization is a well-established activity in other scientific fields such as telecommunications, networks or multimedia, in the field of AI and more specifically NLP it is still at its dawn. In this paper, we explore how various aspects of NLP (evaluation, data, tasks...) lack standards... | Lauriane Aufrant |  |
| 204 |  |  [Probing for Incremental Parse States in Autoregressive Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.203) |  | 0 | Next-word predictions from autoregressive neural language models show remarkable sensitivity to syntax. This work evaluates the extent to which this behavior arises as a result of a learned ability to maintain implicit representations of incremental syntactic structures. We extend work in syntactic... | Tiwalayo Eisape, Vineet Gangireddy, Roger Levy, Yoon Kim |  |
| 205 |  |  [Re-Examining Calibration: The Case of Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.204) |  | 0 | For users to trust model predictions, they need to understand model outputs, particularly their confidence — calibration aims to adjust (calibrate) models’ confidence to match expected accuracy. We argue that the traditional calibration evaluation does not promote effective calibrations: for... | Chenglei Si, Chen Zhao, Sewon Min, Jordan L. BoydGraber |  |
| 206 |  |  [Accelerating Learned Sparse Indexes Via Term Impact Decomposition](https://doi.org/10.18653/v1/2022.findings-emnlp.205) |  | 0 | Novel inverted index-based learned sparse ranking models provide more effective, but less efficient, retrieval performance compared to traditional ranking models like BM25. In this paper, we introduce a technique we call postings clipping to improve the query efficiency of learned representations.... | Joel Mackenzie, Antonio Mallia, Alistair Moffat, Matthias Petri |  |
| 207 |  |  [Do Text-to-Text Multi-Task Learners Suffer from Task Conflict?](https://doi.org/10.18653/v1/2022.findings-emnlp.206) |  | 0 | Traditional multi-task learning architectures learn a single model across multiple tasks through a shared encoder followed by task-specific decoders. Learning these models often requires specialized training algorithms that address task-conflict in the shared parameter updates, which otherwise can... | David Mueller, Nicholas Andrews, Mark Dredze |  |
| 208 |  |  [MANTa: Efficient Gradient-Based Tokenization for End-to-End Robust Language Modeling](https://doi.org/10.18653/v1/2022.findings-emnlp.207) |  | 0 | Static subword tokenization algorithms have been an essential component of recent works on language modeling. However, their static nature results in important flaws that degrade the models’ downstream performance and robustness. In this work, we propose MANTa, a Module for Adaptive Neural... | Nathan Godey, Roman Castagné, Éric de la Clergerie, Benoît Sagot |  |
| 209 |  |  [Towards Intelligent Clinically-Informed Language Analyses of People with Bipolar Disorder and Schizophrenia](https://doi.org/10.18653/v1/2022.findings-emnlp.208) |  | 0 | NLP offers a myriad of opportunities to support mental health research. However, prior work has almost exclusively focused on social media data, for which diagnoses are difficult or impossible to validate. We present a first-of-its-kind dataset of manually transcribed interactions with people... | Ankit Aich, Avery Quynh, Varsha D. Badal, Amy E. Pinkham, Philip D. Harvey, Colin A. Depp, Natalie Parde |  |
| 210 |  |  [Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes](https://doi.org/10.18653/v1/2022.findings-emnlp.209) |  | 0 | Multi-hop Question Answering (QA) is a challenging task since it requires an accurate aggregation of information from multiple context paragraphs and a thorough understanding of the underlying reasoning chains. Recent work in multi-hop QA has shown that performance can be boosted by first... | Kaige Xie, Sarah Wiegreffe, Mark O. Riedl |  |
| 211 |  |  [CheckHARD: Checking Hard Labels for Adversarial Text Detection, Prediction Correction, and Perturbed Word Suggestion](https://doi.org/10.18653/v1/2022.findings-emnlp.210) |  | 0 | An adversarial attack generates harmful text that fools a target model. More dangerously, this text is unrecognizable by humans. Existing work detects adversarial text and corrects a target’s prediction by identifying perturbed words and changing them into their synonyms, but many benign words are... | HoangQuoc NguyenSon, Huy Quang Ung, Seira Hidano, Kazuhide Fukushima, Shinsaku Kiyomoto |  |
| 212 |  |  [Mitigating Covertly Unsafe Text within Natural Language Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.211) |  | 0 | An increasingly prevalent problem for intelligent technologies is text safety, as uncontrolled systems may generate recommendations to their users that lead to injury or life-threatening consequences. However, the degree of explicitness of a generated statement that can cause physical harm varies.... | Alex Mei, Anisha Kabir, Sharon Levy, Melanie Subbiah, Emily Allaway, John Judge, Desmond Patton, Bruce Bimber, Kathleen R. McKeown, William Yang Wang |  |
| 213 |  |  ["I Know Who You Are": Character-Based Features for Conversational Humor Recognition in Chinese](https://doi.org/10.18653/v1/2022.findings-emnlp.212) |  | 0 | Humor plays an important role in our daily life, as it is an essential and fascinating element in the communication between persons. Therefore, how to recognize punchlines from the dialogue, i.e. conversational humor recognition, has attracted much interest of computational linguistics communities.... | Wenbo Shang, Jiangjiang Zhao, Zezhong Wang, Binyang Li, Fangchun Yang, KamFai Wong |  |
| 214 |  |  [DebiasGAN: Eliminating Position Bias in News Recommendation with Adversarial Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.213) |  | 0 | Click behaviors are widely used for learning news recommendation models, but they are heavily affected by the biases brought by the news display positions. It is important to remove position biases to train unbiased recommendation model and capture unbiased user interest. In this paper, we propose... | Chuhan Wu, Fangzhao Wu, Xiangnan He, Yongfeng Huang |  |
| 215 |  |  [Generating Multiple-Length Summaries via Reinforcement Learning for Unsupervised Sentence Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.214) |  | 0 | Sentence summarization shortens given texts while maintaining core contents of the texts. Unsupervised approaches have been studied to summarize texts without ground-truth summaries. However, recent unsupervised models are extractive, which remove words from texts and thus they are less flexible... | Dongmin Hyun, Xiting Wang, Chanyoung Park, Xing Xie, Hwanjo Yu |  |
| 216 |  |  [Multilingual Sentence Transformer as A Multilingual Word Aligner](https://doi.org/10.18653/v1/2022.findings-emnlp.215) |  | 0 | Multilingual pretrained language models (mPLMs) have shown their effectiveness in multilingual word alignment induction. However, these methods usually start from mBERT or XLM-R. In this paper, we investigate whether multilingual sentence Transformer LaBSE is a strong multilingual word aligner.... | Weikang Wang, Guanhua Chen, Hanqing Wang, Yue Han, Yun Chen |  |
| 217 |  |  [CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.216) |  | 0 | Counterfactual data augmentation (CDA) – i.e., adding minimally perturbed inputs during training – helps reduce model reliance on spurious correlations and improves generalization to out-of-distribution (OOD) data. Prior work on generating counterfactuals only considered restricted classes of... | Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, Luke Zettlemoyer |  |
| 218 |  |  [Conversation Disentanglement with Bi-Level Contrastive Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.217) |  | 0 | Conversation disentanglement aims to group utterances into detached sessions, which is a fundamental task in processing multi-party conversations. Existing methods have two main drawbacks. First, they overemphasize pairwise utterance relations but pay inadequate attention to the... | Chengyu Huang, Zheng Zhang, Hao Fei, Lizi Liao |  |
| 219 |  |  [You can't pick your neighbors, or can you? When and How to Rely on Retrieval in the kNN-LM](https://doi.org/10.18653/v1/2022.findings-emnlp.218) |  | 0 | Retrieval-enhanced language models (LMs), which condition their predictions on text retrieved from large external datastores, have recently shown significant perplexity improvements compared to standard LMs. One such approach, the kNN-LM, interpolates any existing LM’s predictions with the output... | Andrew Drozdov, Shufan Wang, Razieh Rahimi, Andrew McCallum, Hamed Zamani, Mohit Iyyer |  |
| 220 |  |  [StuBot: Learning by Teaching a Conversational Agent Through Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.findings-emnlp.219) |  | 0 | This paper proposes StuBot, a text-based conversational agent that provides adaptive feedback for learning by teaching. StuBot first asks the users to teach the learning content by summarizing and explaining it in their own words. After the users inputted the explanation text for teaching, StuBot... | Nayoung Jin, Hana Lee |  |
| 221 |  |  [Improved Universal Sentence Embeddings with Prompt-based Contrastive Learning and Energy-based Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.220) |  | 0 | Contrastive learning has been demonstrated to be effective in enhancing pre-trained language models (PLMs) to derive superior universal sentence embeddings. However, existing contrastive methods still have two limitations. Firstly, previous works may acquire poor performance under domain shift... | Yuxin Jiang, Linhan Zhang, Wei Wang |  |
| 222 |  |  [RaP: Redundancy-aware Video-language Pre-training for Text-Video Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.221) |  | 0 | Video language pre-training methods have mainly adopted sparse sampling techniques to alleviate the temporal redundancy of videos. Though effective, sparse sampling still suffers inter-modal redundancy: visual redundancy and textual redundancy. Compared with highly generalized text, sparsely... | Xing Wu, Chaochen Gao, Zijia Lin, Zhongyuan Wang, Jizhong Han, Songlin Hu |  |
| 223 |  |  [FCGCL: Fine- and Coarse-Granularity Contrastive Learning for Speech Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.222) |  | 0 | It is notoriously difficult to implement end-to-end speech translation (E2E-ST) model because of the task complexity and data scarcity. Existing techniques often attempt to carry out implicit knowledge transfer from machine translation (MT) to ST model by imposing various constraints. However, in... | Hao Zhang, Nianwen Si, Yaqi Chen, Zhen Li, Tong Niu, Xukui Yang, Dan Qu |  |
| 224 |  |  [InfoCSE: Information-aggregated Contrastive Learning of Sentence Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.223) |  | 0 | Contrastive learning has been extensively studied in sentence embedding learning, which assumes that the embeddings of different views of the same sentence are closer. The constraint brought by this assumption is weak, and a good sentence representation should also be able to reconstruct the... | Xing Wu, Chaochen Gao, Zijia Lin, Jizhong Han, Zhongyuan Wang, Songlin Hu |  |
| 225 |  |  [Benchmarking Language Models for Code Syntax Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.224) |  | 0 | Pre-trained language models have demonstrated impressive performance in both natural language processing and program understanding, which represent the input as a token sequence without explicitly modeling its structure. Some prior works show that pre-trained language models can capture the... | Da Shen, Xinyun Chen, Chenguang Wang, Koushik Sen, Dawn Song |  |
| 226 |  |  [Learning When and What to Quote: A Quotation Recommender System with Mutual Promotion of Recommendation and Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.225) |  | 0 | This work extends the current quotation recommendation task to a more realistic quotation recommender system that learns to predict when to quote and what to quote jointly. The system consists of three modules (tasks), a prediction module to predict whether to quote given conversation contexts, a... | Lingzhi Wang, Xingshan Zeng, KamFai Wong |  |
| 227 |  |  [Think Beyond Words: Exploring Context-Relevant Visual Commonsense for Diverse Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.226) |  | 0 | Commonsense knowledge has been widely considered for building intelligent open-domain dialogue agents, aiming to generate meaningful and diverse responses. Previous works in this field usually lack the ability to effectively obtain and utilize auxiliary commonsense from the external visual world.... | Yiting Liu, Liang Li, Beichen Zhang, Qingming Huang |  |
| 228 |  |  [Gender Bias in Meta-Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.227) |  | 0 | Different methods have been proposed to develop meta-embeddings from a given set of source embeddings. However, the source embeddings can contain unfair gender-related biases, and how these influence the meta-embeddings has not been studied yet.We study the gender bias in meta-embeddings created... | Masahiro Kaneko, Danushka Bollegala, Naoaki Okazaki |  |
| 229 |  |  [Third-Party Aligner for Neural Word Alignments](https://doi.org/10.18653/v1/2022.findings-emnlp.228) |  | 0 | Word alignment is to find translationally equivalent words between source and target sentences. Previous work has demonstrated that self-training can achieve competitive word alignment results. In this paper, we propose to use word alignments generated by a third-party word aligner to supervise the... | Jinpeng Zhang, Chuanqi Dong, Xiangyu Duan, Yuqi Zhang, Min Zhang |  |
| 230 |  |  [QaDialMoE: Question-answering Dialogue based Fact Verification with Mixture of Experts](https://doi.org/10.18653/v1/2022.findings-emnlp.229) |  | 0 | Fact verification is an essential tool to mitigate the spread of false information online, which has gained a widespread attention recently. However, a fact verification in the question-answering dialogue is still underexplored. In this paper, we propose a neural network based approach called... | Longzheng Wang, Peng Zhang, Xiaoyu Lu, Lei Zhang, Chaoyang Yan, Chuang Zhang |  |
| 231 |  |  [Multimodal Knowledge Learning for Named Entity Disambiguation](https://doi.org/10.18653/v1/2022.findings-emnlp.230) |  | 0 | With the popularity of online social media, massive-scale multimodal information has brought new challenges to traditional Named Entity Disambiguation (NED) tasks. Recently, Multimodal Named Entity Disambiguation (MNED) has been proposed to link ambiguous mentions with the textual and visual... | Dongjie Zhang, Longtao Huang |  |
| 232 |  |  [Generative Prompt Tuning for Relation Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.231) |  | 0 | Using prompts to explore the knowledge contained within pre-trained language models for downstream tasks has now become an active topic. Current prompt tuning methods mostly convert the downstream tasks to masked language modeling problems by adding cloze-style phrases and mapping all labels to... | Jiale Han, Shuai Zhao, Bo Cheng, Shengkun Ma, Wei Lu |  |
| 233 |  |  [Formulating Few-shot Fine-tuning Towards Language Model Pre-training: A Pilot Study on Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.232) |  | 0 | Fine-tuning pre-trained language models is a common practice in building NLP models for various tasks, including the case with less supervision. We argue that under the few-shot setting, formulating fine-tuning closer to the pre-training objective shall be able to unleash more benefits from the... | Zihan Wang, Kewen Zhao, Zilong Wang, Jingbo Shang |  |
| 234 |  |  [Masked Language Models Know Which are Popular: A Simple Ranking Strategy for Commonsense Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.233) |  | 0 | We propose a simple ranking strategy to solve a generative commonsense question answering (QA) problem. Compared with multiple-choice QA, it is challenging because the answers to a question are not unique and they are supposed to be popular and diverse. Our strategy exploits the dataset itself and... | Xuan Luo, Chuang Fan, Yice Zhang, Wanguo Jiang, Bing Qin, Ruifeng Xu |  |
| 235 |  |  [DialogUSR: Complex Dialogue Utterance Splitting and Reformulation for Multiple Intent Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.234) |  | 0 | While interacting with chatbots, users may elicit multiple intents in a single dialogue utterance. Instead of training a dedicated multi-intent detection model, we propose DialogUSR, a dialogue utterance splitting and reformulation task that first splits multi-intent user query into several... | Haoran Meng, Zheng Xin, Tianyu Liu, Zizhen Wang, He Feng, Binghuai Lin, Xuemin Zhao, Yunbo Cao, Zhifang Sui |  |
| 236 |  |  [Low-resource Interactive Active Labeling for Fine-tuning Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.235) |  | 0 | Recently, active learning (AL) methods have been used to effectively fine-tune pre-trained language models for various NLP tasks such as sentiment analysis and document classification. However, given the task of fine-tuning language models, understanding the impact of different aspects on AL... | Seiji Maekawa, Dan Zhang, Hannah Kim, Sajjadur Rahman, Estevam Hruschka |  |
| 237 |  |  [Getting the Most out of Simile Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.236) |  | 0 | Simile recognition involves two subtasks: simile sentence classification that discriminates whether a sentence contains simile, and simile component extraction that locates the corresponding objects (i.e., tenors and vehicles).Recent work ignores features other than surface strings and suffers from... | Xiaoyue Wang, Linfeng Song, Xin Liu, Chulun Zhou, Hualin Zeng, Jinsong Su |  |
| 238 |  |  [A Unified Framework for Pun Generation with Humor Principles](https://doi.org/10.18653/v1/2022.findings-emnlp.237) |  | 0 | We propose a unified framework to generate both homophonic and homographic puns to resolve the split-up in existing works. Specifically, we incorporate three linguistic attributes of puns to the language models: ambiguity, distinctiveness, and surprise. Our framework consists of three parts: 1) a... | Yufei Tian, Divyanshu Sheth, Nanyun Peng |  |
| 239 |  |  [Improving English-Arabic Transliteration with Phonemic Memories](https://doi.org/10.18653/v1/2022.findings-emnlp.238) |  | 0 | Transliteration is an important task in natural language processing (NLP) which aims to convert a name in the source language to the target language without changing its pronunciation. Particularly, transliteration from English to Arabic is highly needed in many applications, especially in... | Yuanhe Tian, Renze Lou, Xiangyu Pang, Lianxi Wang, Shengyi Jiang, Yan Song |  |
| 240 |  |  [Mix-and-Match: Scalable Dialog Response Retrieval using Gaussian Mixture Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.239) |  | 0 | Embedding-based approaches for dialog response retrieval embed the context-response pairs as points in the embedding space. These approaches are scalable, but fail to account for the complex, many-to-many relationships that exist between context-response pairs. On the other end of the spectrum,... | Gaurav Pandey, Danish Contractor, Sachindra Joshi |  |
| 241 |  |  [AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.240) |  | 0 | There are growing interests in adapting large-scale language models using parameter-efficient fine-tuning methods. However, accelerating the model itself and achieving better inference efficiency through model compression has not been thoroughly explored yet.Model compression could provide the... | Se Jung Kwon, Jeonghoon Kim, Jeongin Bae, Kang Min Yoo, JinHwa Kim, Baeseong Park, Byeongwook Kim, JungWoo Ha, Nako Sung, Dongsoo Lee |  |
| 242 |  |  [Learning Invariant Representation Improves Robustness for MRC Models](https://doi.org/10.18653/v1/2022.findings-emnlp.241) |  | 0 | The prosperity of Pretrained Language Models(PLM) has greatly promoted the development of Machine Reading Comprehension (MRC). However, these models are vulnerable and not robust to adversarial examples. In this paper, we propose Stable and Contrastive Question Answering (SCQA) to improve... | Yu Hai, Liang Wen, Haoran Meng, Tianyu Liu, Houfeng Wang |  |
| 243 |  |  [ER-Test: Evaluating Explanation Regularization Methods for Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.242) |  | 0 | By explaining how humans would solve a given task, human rationales can provide strong learning signal for neural language models (NLMs). Explanation regularization (ER) aims to improve NLM generalization by pushing the NLM’s machine rationales (Which input tokens did the NLM focus on?) to align... | Brihi Joshi, Aaron Chan, Ziyi Liu, Shaoliang Nie, Maziar Sanjabi, Hamed Firooz, Xiang Ren |  |
| 244 |  |  [Learning Cooperative Interactions for Multi-Overlap Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.243) |  | 0 | Aspect sentiment triplet extraction (ASTE) is an essential task, which aims to extract triplets(aspect, opinion, sentiment). However, overlapped triplets, especially multi-overlap triplets,make ASTE a challenge. Most existing methods suffer from multi-overlap triplets becausethey focus on the... | Shiman Zhao, Wei Chen, Tengjiao Wang |  |
| 245 |  |  [Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Parameter-Efficient Tuning](https://doi.org/10.18653/v1/2022.findings-emnlp.244) |  | 0 | Delta tuning (DET, also known as parameter-efficient tuning) is deemed as the new paradigm for using pre-trained language models (PLMs). Up to now, various DETs with distinct design elements have been proposed, achieving performance on par with fine-tuning. However, the mechanisms behind the above... | Jing Yi, Weize Chen, Yujia Qin, Yankai Lin, Ning Ding, Xu Han, Zhiyuan Liu, Maosong Sun, Jie Zhou |  |
| 246 |  |  [Explainable Slot Type Attentions to Improve Joint Intent Detection and Slot Filling](https://doi.org/10.18653/v1/2022.findings-emnlp.245) |  | 0 | Joint intent detection and slot filling is a key research topic in natural language understanding (NLU). Existing joint intent and slot filling systems analyze and compute features collectively for all slot types, and importantly, have no way to explain the slot filling model decisions. In this... | Kalpa Gunaratna, Vijay Srinivasan, Akhila Yerukola, Hongxia Jin |  |
| 247 |  |  [PseudoReasoner: Leveraging Pseudo Labels for Commonsense Knowledge Base Population](https://doi.org/10.18653/v1/2022.findings-emnlp.246) |  | 0 | Commonsense Knowledge Base (CSKB) Population aims at reasoning over unseen entities and assertions on CSKBs, and is an important yet hard commonsense reasoning task. One challenge is that it requires out-of-domain generalization ability as the source CSKB for training is of a relatively smaller... | Tianqing Fang, Quyet V. Do, Hongming Zhang, Yangqiu Song, Ginny Y. Wong, Simon See |  |
| 248 |  |  [History-Aware Hierarchical Transformer for Multi-session Open-domain Dialogue System](https://doi.org/10.18653/v1/2022.findings-emnlp.247) |  | 0 | With the evolution of pre-trained language models, current open-domain dialogue systems have achieved great progress in conducting one-session conversations. In contrast, Multi-Session Conversation (MSC), which consists of multiple sessions over a long term with the same user, is... | Tong Zhang, Yong Liu, Boyang Li, Zhiwei Zeng, Pengwei Wang, Yuan You, Chunyan Miao, Lizhen Cui |  |
| 249 |  |  [Guiding Abstractive Dialogue Summarization with Content Planning](https://doi.org/10.18653/v1/2022.findings-emnlp.248) |  | 0 | Abstractive dialogue summarization has recently been receiving more attention. We propose a coarse-to-fine model for generating abstractive dialogue summaries, and introduce a fact-aware reinforcement learning (RL) objective that improves the fact consistency between the dialogue and the generated... | Ye Wang, Xiaojun Wan, Zhiping Cai |  |
| 250 |  |  [Truncation Sampling as Language Model Desmoothing](https://doi.org/10.18653/v1/2022.findings-emnlp.249) |  | 0 | Long samples of text from neural language models can be of poor quality. Truncation sampling algorithms–like top-p or top-k—address this by setting some words’ probabilities to zero at each step. This work investigates why these methods are important, and how to improve them. We propose thinking of... | John Hewitt, Christopher D. Manning, Percy Liang |  |
| 251 |  |  [Knowledge-grounded Dialog State Tracking](https://doi.org/10.18653/v1/2022.findings-emnlp.250) |  | 0 | Knowledge (including structured knowledge such as schema and ontology and unstructured knowledge such as web corpus) is a critical part of dialog understanding, especially for unseen tasks and domains. Traditionally, such domain-specific knowledge is encoded implicitly into model parameters for the... | Dian Yu, Mingqiu Wang, Yuan Cao, Laurent El Shafey, Izhak Shafran, Hagen Soltau |  |
| 252 |  |  [Context-aware Information-theoretic Causal De-biasing for Interactive Sequence Labeling](https://doi.org/10.18653/v1/2022.findings-emnlp.251) |  | 0 | Supervised training of existing deep learning models for sequence labeling relies on large scale labeled datasets. Such datasets are generally created with crowd-source labeling. However, crowd-source labeling for tasks of sequence labeling can be expensive and time-consuming. Further, crowd-source... | Junda Wu, Rui Wang, Tong Yu, Ruiyi Zhang, Handong Zhao, Shuai Li, Ricardo Henao, Ani Nenkova |  |
| 253 |  |  [Simple but Challenging: Natural Language Inference Models Fail on Simple Sentences](https://doi.org/10.18653/v1/2022.findings-emnlp.252) |  | 0 | Natural language inference (NLI) is a task to infer the relationship between a premise and a hypothesis (e.g., entailment, neutral, or contradiction), and transformer-based models perform well on current NLI datasets such as MNLI and SNLI. Nevertheless, given the linguistic complexity of the... | Cheng Luo, Wei Liu, Jieyu Lin, Jiajie Zou, Ming Xiang, Nai Ding |  |
| 254 |  |  [DORE: Document Ordered Relation Extraction based on Generative Framework](https://doi.org/10.18653/v1/2022.findings-emnlp.253) |  | 0 | In recent years, there is a surge of generation-based information extraction work, which allows a more direct use of pre-trained language models and efficiently captures output dependencies. However, previous generative methods using lexical representation do not naturally fit document-level... | Qipeng Guo, Yuqing Yang, Hang Yan, Xipeng Qiu, Zheng Zhang |  |
| 255 |  |  [Explicit Role Interaction Network for Event Argument Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.254) |  | 0 | Event argument extraction is a challenging subtask of event extraction, aiming to identify and assign roles to arguments under a certain event. Existing methods extract arguments of each role independently, ignoring the relationship between different roles. Such an approach hinders the model from... | Nan Ding, Chunming Hu, Kai Sun, Samuel Mensah, Richong Zhang |  |
| 256 |  |  [Few-Shot Out-of-Domain Transfer Learning of Natural Language Explanations in a Label-Abundant Setup](https://doi.org/10.18653/v1/2022.findings-emnlp.255) |  | 0 | Training a model to provide natural language explanations (NLEs) for its predictions usually requires the acquisition of task-specific NLEs, which is time- and resource-consuming. A potential solution is the few-shot out-of-domain transfer of NLEs from a parent task with many NLEs to a child... | Yordan Yordanov, Vid Kocijan, Thomas Lukasiewicz, OanaMaria Camburu |  |
| 257 |  |  [RoChBert: Towards Robust BERT Fine-tuning for Chinese](https://doi.org/10.18653/v1/2022.findings-emnlp.256) |  | 0 | Despite of the superb performance on a wide range of tasks, pre-trained language models (e.g., BERT) have been proved vulnerable to adversarial texts. In this paper, we present RoChBERT, a framework to build more Robust BERT-based models by utilizing a more comprehensive adversarial graph to fuse... | Zihan Zhang, Jinfeng Li, Ning Shi, Bo Yuan, Xiangyu Liu, Rong Zhang, Hui Xue, Donghong Sun, Chao Zhang |  |
| 258 |  |  [Lexical Entailment with Hierarchy Representations by Deep Metric Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.257) |  | 0 | In this paper, we introduce a novel method for lexical entailment tasks, which detects a hyponym-hypernym relation among words. Existing lexical entailment studies are lacking in generalization performance, as they cannot be applied to words that are not included in the training dataset. Moreover,... | Naomi Sato, Masaru Isonuma, Kimitaka Asatani, Shoya Ishizuka, Aori Shimizu, Ichiro Sakata |  |
| 259 |  |  [Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation](https://doi.org/10.18653/v1/2022.findings-emnlp.258) |  | 0 | Prompt tuning, or the conditioning of a frozen pretrained language model (PLM) with soft prompts learned from data, has demonstrated impressive performance on a wide range of NLP tasks. However, prompt tuning requires a large training dataset to be effective and is outperformed by finetuning the... | Xu Guo, Boyang Li, Han Yu |  |
| 260 |  |  [McPhraSy: Multi-Context Phrase Similarity and Clustering](https://doi.org/10.18653/v1/2022.findings-emnlp.259) |  | 0 | Phrase similarity is a key component of many NLP applications. Current phrase similarity methods focus on embedding the phrase itself and use the phrase context only during training of the pretrained model. To better leverage the information in the context, we propose McPhraSy (Multi-context Phrase... | Amir David Nissan Cohen, Hila Gonen, Ori Shapira, Ran Levy, Yoav Goldberg |  |
| 261 |  |  [CANarEx: Contextually Aware Narrative Extraction for Semantically Rich Text-as-data Applications](https://doi.org/10.18653/v1/2022.findings-emnlp.260) |  | 0 | Narrative modelling is an area of active research, motivated by the acknowledgement of narratives as drivers of societal decision making. These research efforts conceptualize narratives as connected entity chains, and modeling typically focuses on the identification of entities and their... | Nandini Anantharama, Simon D. Angus, Lachlan O'Neill |  |
| 262 |  |  [Narrate Dialogues for Better Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.261) |  | 0 | Dialogue summarization models aim to generate a concise and accurate summary for multi-party dialogue. The complexity of dialogue, including coreference, dialogue acts, and inter-speaker interactions bring unique challenges to dialogue summarization. Most recent neural models achieve state-of-art... | Ruochen Xu, Chenguang Zhu, Michael Zeng |  |
| 263 |  |  [Towards Identifying Social Bias in Dialog Systems: Framework, Dataset, and Benchmark](https://doi.org/10.18653/v1/2022.findings-emnlp.262) |  | 0 | Among all the safety concerns that hinder the deployment of open-domain dialog systems (e.g., offensive languages, biases, and toxic behaviors), social bias presents an insidious challenge. Addressing this challenge requires rigorous analyses and normative reasoning. In this paper, we focus our... | Jingyan Zhou, Jiawen Deng, Fei Mi, Yitong Li, Yasheng Wang, Minlie Huang, Xin Jiang, Qun Liu, Helen Meng |  |
| 264 |  |  [CrossRE: A Cross-Domain Dataset for Relation Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.263) |  | 0 | Relation Extraction (RE) has attracted increasing attention, but current RE evaluation is limited to in-domain evaluation setups. Little is known on how well a RE system fares in challenging, but realistic out-of-distribution evaluation setups. To address this gap, we propose CrossRE, a new,... | Elisa Bassignana, Barbara Plank |  |
| 265 |  |  [Probing Structural Knowledge from Pre-trained Language Model for Argumentation Relation Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.264) |  | 0 | Extracting fine-grained structural information between argumentation component (AC) pairs is essential for argumentation relation classification (ARC). However, most previous studies attempt to model the relationship between AC pairs using AC level similarity or semantically relevant features. They... | Yang Sun, Bin Liang, Jianzhu Bao, Min Yang, Ruifeng Xu |  |
| 266 |  |  [LogicNMR: Probing the Non-monotonic Reasoning Ability of Pre-trained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.265) |  | 0 | The logical reasoning capabilities of pre-trained language models have recently received much attention. As one of the vital reasoning paradigms, non-monotonic reasoning refers to the fact that conclusions may be invalidated with new information. Existing work has constructed a non-monotonic... | Yeliang Xiu, Zhanhao Xiao, Yongmei Liu |  |
| 267 |  |  [Cheater's Bowl: Human vs. Computer Search Strategies for Open-Domain QA](https://doi.org/10.18653/v1/2022.findings-emnlp.266) |  | 0 | For humans and computers, the first step in answering an open-domain question is retrieving a set of relevant documents from a large corpus. However, the strategies that computers use fundamentally differ from those of humans. To better understand these differences, we design a gamified interface... | Wanrong He, Andrew Mao, Jordan L. BoydGraber |  |
| 268 |  |  [FRSUM: Towards Faithful Abstractive Summarization via Enhancing Factual Robustness](https://doi.org/10.18653/v1/2022.findings-emnlp.267) |  | 0 | Despite being able to generate fluent and grammatical text, current Seq2Seq summarization models still suffering from the unfaithful generation problem.In this paper, we study the faithfulness of existing systems from a new perspective of factual robustness which is the ability to correctly... | Wenhao Wu, Wei Li, Jiachen Liu, Xinyan Xiao, Ziqiang Cao, Sujian Li, Hua Wu |  |
| 269 |  |  [PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.268) |  | 0 | Formal verse poetry imposes strict constraints on the meter and rhyme scheme of poems. Most prior work on generating this type of poetry uses existing poems for supervision, which are difficult to obtain for most languages and poetic forms. In this work, we propose an unsupervised approach to... | Aitor Ormazabal, Mikel Artetxe, Manex Agirrezabal, Aitor Soroa, Eneko Agirre |  |
| 270 |  |  [ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback](https://doi.org/10.18653/v1/2022.findings-emnlp.269) |  | 0 | Recently, dataset-generation-based zero-shot learning has shown promising results by training a task-specific model with a dataset synthesized from large pre-trained language models (PLMs). The final task-specific model often achieves compatible or even better performance than PLMs under the... | Jiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong |  |
| 271 |  |  [Constructing Highly Inductive Contexts for Dialogue Safety through Controllable Reverse Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.270) |  | 0 | Large pretrained language models can easily produce toxic or biased content, which is prohibitive for practical use. In order to detect such toxic generations, existing methods rely on templates, real-world data extraction, crowdsourcing workers or automatic generation to construct adversarial... | Zhexin Zhang, Jiale Cheng, Hao Sun, Jiawen Deng, Fei Mi, Yasheng Wang, Lifeng Shang, Minlie Huang |  |
| 272 |  |  [Language Prior Is Not the Only Shortcut: A Benchmark for Shortcut Learning in VQA](https://doi.org/10.18653/v1/2022.findings-emnlp.271) |  | 0 | Visual Question Answering (VQA) models are prone to learn the shortcut solution formed by dataset biases rather than the intended solution. To evaluate the VQA models’ reasoning ability beyond shortcut learning, the VQA-CP v2 dataset introduces a distribution shift between the training and test set... | Qingyi Si, Fandong Meng, Mingyu Zheng, Zheng Lin, Yuanxin Liu, Peng Fu, Yanan Cao, Weiping Wang, Jie Zhou |  |
| 273 |  |  [Bridging the Training-Inference Gap for Dense Phrase Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.272) |  | 0 | Building dense retrievers requires a series of standard procedures, including training and validating neural models and creating indexes for efficient search. However, these procedures are often misaligned in that training objectives do not exactly reflect the retrieval scenario at inference time.... | Gyuwan Kim, Jinhyuk Lee, Barlas Oguz, Wenhan Xiong, Yizhe Zhang, Yashar Mehdad, William Yang Wang |  |
| 274 |  |  [Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources](https://doi.org/10.18653/v1/2022.findings-emnlp.273) |  | 0 | While the NLP community is generally aware of resource disparities among languages, we lack research that quantifies the extent and types of such disparity. Prior surveys estimating the availability of resources based on the number of datasets can be misleading as dataset quality varies: many... | Xinyan Yu, Trina Chatterjee, Akari Asai, Junjie Hu, Eunsol Choi |  |
| 275 |  |  [ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.274) |  | 0 | Recent years have witnessed the rise and success of pre-training techniques in visually-rich document understanding. However, most existing methods lack the systematic mining and utilization of layout-centered knowledge, leading to sub-optimal performances. In this paper, we propose ERNIE-Layout, a... | Qiming Peng, Yinxu Pan, Wenjin Wang, Bin Luo, Zhenyu Zhang, Zhengjie Huang, Yuhui Cao, Weichong Yin, Yongfeng Chen, Yin Zhang, Shikun Feng, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang |  |
| 276 |  |  [Do Charge Prediction Models Learn Legal Theory?](https://doi.org/10.18653/v1/2022.findings-emnlp.275) |  | 0 | The charge prediction task aims to predict the charge for a case given its fact description. Recent models have already achieved impressive accuracy in this task, however, little is understood about the mechanisms they use to perform the judgment.For practical applications, a charge prediction... | Zhenwei An, Quzhe Huang, Cong Jiang, Yansong Feng, Dongyan Zhao |  |
| 277 |  |  [Keep Me Updated! Memory Management in Long-term Conversations](https://doi.org/10.18653/v1/2022.findings-emnlp.276) |  | 0 | Remembering important information from the past and continuing to talk about it in the present are crucial in long-term conversations. However, previous literature does not deal with cases where the memorized information is outdated, which may cause confusion in later conversations. To address this... | Sanghwan Bae, DongHyun Kwak, Soyoung Kang, Min Young Lee, Sungdong Kim, Yuin Jeong, Hyeri Kim, SangWoo Lee, WooMyoung Park, Nako Sung |  |
| 278 |  |  [A Unified Dialogue User Simulator for Few-shot Data Augmentation](https://doi.org/10.18653/v1/2022.findings-emnlp.277) |  | 0 | Pre-trained language models have shown superior performance in task-oriented dialogues. However, existing datasets are on limited scales, which cannot support large-scale pre-training. Fortunately, various data augmentation methods have been developed to augment large-scale task-oriented dialogue... | Dazhen Wan, Zheng Zhang, Qi Zhu, Lizi Liao, Minlie Huang |  |
| 279 |  |  [An Error-Guided Correction Model for Chinese Spelling Error Correction](https://doi.org/10.18653/v1/2022.findings-emnlp.278) |  | 0 | Although existing neural network approaches have achieved great progress on Chinese spelling correction, there is still room to improve. The model is required to avoid over-correction and to distinguish a correct token from its phonological and visual similar ones. In this paper, we propose an... | Rui Sun, Xiuyu Wu, Yunfang Wu |  |
| 280 |  |  [Describing Sets of Images with Textual-PCA](https://doi.org/10.18653/v1/2022.findings-emnlp.279) |  | 0 | We seek to semantically describe a set of images, capturing both the attributes of single images and the variations within the set. Our procedure is analogous to Principle Component Analysis, in which the role of projection vectors is replaced with generated phrases. First, a centroid phrase that... | Oded Hupert, Idan Schwartz, Lior Wolf |  |
| 281 |  |  [Learning to Model Editing Processes](https://doi.org/10.18653/v1/2022.findings-emnlp.280) |  | 0 | Most existing sequence generation models produce outputs in one pass, usually left-to-right. However, this is in contrast with a more natural approach that humans use in generating content; iterative refinement and editing. Recent work has introduced edit-based models for various tasks (such as... | Machel Reid, Graham Neubig |  |
| 282 |  |  [PALT: Parameter-Lite Transfer of Language Models for Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.findings-emnlp.281) |  | 0 | This paper presents a parameter-lite transfer learning approach of pretrained language models (LM) for knowledge graph (KG) completion. Instead of finetuning, which modifies all LM parameters, we only tune a few new parameters while keeping the original LM parameters fixed. We establish this via... | Jianhao Shen, Chenguang Wang, Ye Yuan, Jiawei Han, Heng Ji, Koushik Sen, Ming Zhang, Dawn Song |  |
| 283 |  |  [Prompt-based Connective Prediction Method for Fine-grained Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.282) |  | 0 | Due to the absence of connectives, implicit discourse relation recognition (IDRR) is still a challenging and crucial task in discourse analysis. Most of the current work adopted multitask learning to aid IDRR through explicit discourse relation recognition (EDRR) or utilized dependencies between... | Hao Zhou, Man Lan, Yuanbin Wu, Yuefeng Chen, Meirong Ma |  |
| 284 |  |  [On Utilizing Constituent Language Resources to Improve Downstream Tasks in Hinglish](https://doi.org/10.18653/v1/2022.findings-emnlp.283) |  | 0 | Performance of downstream NLP tasks on code-switched Hindi-English (aka ) continues to remain a significant challenge. Intuitively, Hindi and English corpora should aid improve task performance on Hinglish. We show that meta-learning framework can effectively utilize the the labelled resources of... | Vishwajeet Kumar, Rudra Murthy, Tejas I. Dhamecha |  |
| 285 |  |  [SYGMA: A System for Generalizable and Modular Question Answering Over Knowledge Bases](https://doi.org/10.18653/v1/2022.findings-emnlp.284) |  | 0 | Knowledge Base Question Answering (KBQA) involving complex reasoning is emerging as an important research direction. However, most KBQA systems struggle with generalizability, particularly on two dimensions: (a) across multiple knowledge bases, where existing KBQA approaches are typically tuned to... | Sumit Neelam, Udit Sharma, Hima Karanam, Shajith Ikbal, Pavan Kapanipathi, Ibrahim Abdelaziz, Nandana Mihindukulasooriya, YoungSuk Lee, Santosh K. Srivastava, Cezar Pendus, Saswati Dana, Dinesh Garg, Achille Fokoue, G. P. Shrivatsa Bhargav, Dinesh Khandelwal, Srinivas Ravishankar, Sairam Gurajada, Maria Chang, Rosario UcedaSosa, Salim Roukos, Alexander Gray, Guilherme Lima, Ryan Riegel, Francois P. S. Luus, L. Venkata Subramaniam |  |
| 286 |  |  [Instance-Guided Prompt Learning for Few-Shot Text Matching](https://doi.org/10.18653/v1/2022.findings-emnlp.285) |  | 0 | Few-shot text matching is a more practical technique in natural language processing (NLP) to determine whether two texts are semantically identical. They primarily design patterns to reformulate text matching into a pre-trained task with uniform prompts across all instances. But they fail to take... | Jia Du, Xuanyu Zhang, Siyi Wang, Kai Wang, Yanquan Zhou, Lei Li, Qing Yang, Dongliang Xu |  |
| 287 |  |  [M3: Multi-level dataset for Multi-document summarisation of Medical studies](https://doi.org/10.18653/v1/2022.findings-emnlp.286) |  | 0 | We present M3 (Multi-level dataset for Multi-document summarisation of Medical studies), a benchmark dataset for evaluating the quality of summarisation systems in the biomedical domain. The dataset contains sets of multiple input documents and target summaries of three levels of complexity:... | Yulia Otmakhova, Karin Verspoor, Timothy Baldwin, Antonio JimenoYepes, Jey Han Lau |  |
| 288 |  |  [Adapters for Enhanced Modeling of Multilingual Knowledge and Text](https://doi.org/10.18653/v1/2022.findings-emnlp.287) |  | 0 | Large language models appear to learn facts from the large text corpora they are trained on. Such facts are encoded implicitly within their many parameters, making it difficult to verify or manipulate what knowledge has been learned. Language models have recently been extended to multilingual... | Yifan Hou, Wenxiang Jiao, Meizhen Liu, Carl Allen, Zhaopeng Tu, Mrinmaya Sachan |  |
| 289 |  |  [SepLL: Separating Latent Class Labels from Weak Supervision Noise](https://doi.org/10.18653/v1/2022.findings-emnlp.288) |  | 0 | In the weakly supervised learning paradigm, labeling functions automatically assign heuristic, often noisy, labels to data samples. In this work, we provide a method for learning from weak labels by separating two types of complementary information associated with the labeling functions:... | Andreas Stephan, Vasiliki Kougia, Benjamin Roth |  |
| 290 |  |  [Probing Relational Knowledge in Language Models via Word Analogies](https://doi.org/10.18653/v1/2022.findings-emnlp.289) |  | 0 | Understanding relational knowledge plays an integral part in natural language comprehension. When it comes to pre-trained language models (PLM), prior work has been focusing on probing relational knowledge this by filling the blanks in pre-defined prompts such as “The capital of France is —".... | Kiamehr Rezaee, José CamachoCollados |  |
| 291 |  |  [Semi-Supervised Lifelong Language Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.290) |  | 0 | Lifelong learning aims to accumulate knowledge and alleviate catastrophic forgetting when learning tasks sequentially. However, existing lifelong language learning methods only focus on the supervised learning setting. Unlabeled data, which can be easily accessed in real-world scenarios, are... | Yingxiu Zhao, Yinhe Zheng, Bowen Yu, Zhiliang Tian, Dongkyu Lee, Jian Sun, Yongbin Li, Nevin L. Zhang |  |
| 292 |  |  [Parameter-free Automatically Prompting: A Latent Pseudo Label Mapping Model for Prompt-based Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.291) |  | 0 | Prompt-based learning has achieved excellent performance in few-shot learning by mapping the outputs of the pre-trained language model to the labels with the help of a label mapping component. Existing manual label mapping (MLM) methods achieve good results but heavily rely on expensive human... | Jirui Qi, Richong Zhang, Junfan Chen, Jaein Kim, Yongyi Mao |  |
| 293 |  |  [Exploring Logographic Image for Chinese Aspect-based Sentiment Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.292) |  | 0 | In logographic languages like Chinese, word meanings are constructed using specific character formations, which can help to disambiguate word senses and are beneficial for sentiment classification. However, such knowledge is rarely explored in previous sentiment analysis methods. In this paper, we... | Xiabing Zhou, Renjie Feng, Xiaotong Jiang, Zhongqing Wang |  |
| 294 |  |  [On the Role of Bidirectionality in Language Model Pre-Training](https://doi.org/10.18653/v1/2022.findings-emnlp.293) |  | 0 | Prior work on language model pre-training has explored different architectures and learning objectives, but differences in data, hyperparameters and evaluation make a principled comparison difficult. In this work, we focus on bidirectionality as a key factor that differentiates existing approaches,... | Mikel Artetxe, Jingfei Du, Naman Goyal, Luke Zettlemoyer, Veselin Stoyanov |  |
| 295 |  |  [You Are What You Talk About: Inducing Evaluative Topics for Personality Analysis](https://doi.org/10.18653/v1/2022.findings-emnlp.294) |  | 0 | Expressing attitude or stance toward entities and concepts is an integral part of human behavior and personality. Recently, evaluative language data has become more accessible with social media’s rapid growth, enabling large-scale opinion analysis. However, surprisingly little research examines the... | Josip Jukic, Iva Vukojevic, Jan Snajder |  |
| 296 |  |  [CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure](https://doi.org/10.18653/v1/2022.findings-emnlp.295) |  | 0 | Code pre-trained models (CodePTMs) have recently demonstrated significant success in code intelligence. To interpret these models, some probing methods have been applied. However, these methods fail to consider the inherent characteristics of codes. In this paper, to address the problem, we propose... | Nuo Chen, Qiushi Sun, Renyu Zhu, Xiang Li, Xuesong Lu, Ming Gao |  |
| 297 |  |  [Learning to Revise References for Faithful Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.296) |  | 0 | In real-world scenarios with naturally occurring datasets, reference summaries are noisy and may contain information that cannot be inferred from the source text. On large news corpora, removing low quality samples has been shown to reduce model hallucinations. Yet, for smaller, and/or noisier... | Griffin Adams, HanChin Shing, Qing Sun, Christopher Winestock, Kathleen R. McKeown, Noémie Elhadad |  |
| 298 |  |  [Towards Intention Understanding in Suicidal Risk Assessment with Natural Language Processing](https://doi.org/10.18653/v1/2022.findings-emnlp.297) |  | 0 | Recent applications of natural language processing techniques to suicidal ideation detection and risk assessment frame the detection or assessment task as a text classification problem. Recent advances have developed many models, especially deep learning models, to boost predictive... | Shaoxiong Ji |  |
| 299 |  |  [On the Impact of Temporal Concept Drift on Model Explanations](https://doi.org/10.18653/v1/2022.findings-emnlp.298) |  | 0 | Explanation faithfulness of model predictions in natural language processing is typically evaluated on held-out data from the same temporal distribution as the training data (i.e. synchronous settings). While model performance often deteriorates due to temporal variation (i.e. temporal concept... | Zhixue Zhao, George Chrysostomou, Kalina Bontcheva, Nikolaos Aletras |  |
| 300 |  |  [Text-Only Training for Image Captioning using Noise-Injected CLIP](https://doi.org/10.18653/v1/2022.findings-emnlp.299) |  | 0 | We consider the task of image-captioning using only the CLIP model and additional text data at training time and no additional captioned images. Our approach relies on the fact that CLIP is trained to make visual and textual embeddings similar. Therefore, we only need to learn how to translate CLIP... | David Nukrai, Ron Mokady, Amir Globerson |  |
| 301 |  |  [Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.300) |  | 0 | Fine-tuning large pretrained language models on a limited training corpus usually suffers from poor generalization. Prior works show that the recently-proposed sharpness-aware minimization (SAM) optimization method can improve the model generalization. However, SAM adds a perturbation to each model... | Qihuang Zhong, Liang Ding, Li Shen, Peng Mi, Juhua Liu, Bo Du, Dacheng Tao |  |
| 302 |  |  [TINA: Textual Inference with Negation Augmentation](https://doi.org/10.18653/v1/2022.findings-emnlp.301) |  | 0 | Transformer-based language models achieve state-of-the-art results on several natural language processing tasks. One of these is textual entailment, i.e., the task of determining whether a premise logically entails a hypothesis. However, the models perform poorly on this task when the examples... | Chadi Helwe, Simon Coumes, Chloé Clavel, Fabian M. Suchanek |  |
| 303 |  |  [Improving Bilingual Lexicon Induction with Cross-Encoder Reranking](https://doi.org/10.18653/v1/2022.findings-emnlp.302) |  | 0 | Bilingual lexicon induction (BLI) with limited bilingual supervision is a crucial yet challenging task in multilingual NLP. Current state-of-the-art BLI methods rely on the induction of cross-lingual word embeddings (CLWEs) to capture cross-lingual word similarities; such CLWEs are obtained... | Yaoyiran Li, Fangyu Liu, Ivan Vulic, Anna Korhonen |  |
| 304 |  |  [Mixed-modality Representation Learning and Pre-training for Joint Table-and-Text Retrieval in OpenQA](https://doi.org/10.18653/v1/2022.findings-emnlp.303) |  | 0 | Retrieving evidences from tabular and textual resources is essential for open-domain question answering (OpenQA), which provides more comprehensive information. However, training an effective dense table-text retriever is difficult due to the challenges of table-text discrepancy and data sparsity... | Junjie Huang, Wanjun Zhong, Qian Liu, Ming Gong, Daxin Jiang, Nan Duan |  |
| 305 |  |  [The Effects of Corpus Choice and Morphosyntax on Multilingual Space Induction](https://doi.org/10.18653/v1/2022.findings-emnlp.304) |  | 0 | In an effort to study the inductive biases of language models, numerous studies have attempted to use linguistically motivated tasks as a proxy of sorts, wherein performance on these tasks would imply an inductive bias towards a specific linguistic phenomenon. In this study, we attempt to analyse... | Vinit Ravishankar, Joakim Nivre |  |
| 306 |  |  [Modeling Complex Dialogue Mappings via Sentence Semantic Segmentation Guided Conditional Variational Auto-Encoder](https://doi.org/10.18653/v1/2022.findings-emnlp.305) |  | 0 | Complex dialogue mappings (CDM), including one-to-many and many-to-one mappings, tend to make dialogue models generate incoherent or dull responses, and modeling these mappings remains a huge challenge for neural dialogue systems. To alleviate these problems, methods like introducing external... | Bin Sun, Shaoxiong Feng, Yiwei Li, Weichao Wang, Fei Mi, Yitong Li, Kan Li |  |
| 307 |  |  [Graph Embeddings for Argumentation Quality Assessment](https://doi.org/10.18653/v1/2022.findings-emnlp.306) |  | 0 | Argumentation is used by people both internally, by evaluating arguments and counterarguments to make sense of a situation and take a decision, and externally, e.g., in a debate, by exchanging arguments to reach an agreement or to promote an individual position. In this context, the assessment of... | Santiago Marro, Elena Cabrio, Serena Villata |  |
| 308 |  |  [SMiLE: Schema-augmented Multi-level Contrastive Learning for Knowledge Graph Link Prediction](https://doi.org/10.18653/v1/2022.findings-emnlp.307) |  | 0 | Link prediction is the task of inferring missing links between entities in knowledge graphs. Embedding-based methods have shown effectiveness in addressing this problem by modeling relational patterns in triples. However, the link prediction task often requires contextual information in entity... | Miao Peng, Ben Liu, Qianqian Xie, Wenjie Xu, Hua Wang, Min Peng |  |
| 309 |  |  [Multilingual Multimodal Learning with Machine Translated Text](https://doi.org/10.18653/v1/2022.findings-emnlp.308) |  | 0 | Most vision-and-language pretraining research focuses on English tasks. However, the creation of multilingual multimodal evaluation datasets (e.g. Multi30K, xGQA, XVNLI, and MaRVL) poses a new challenge in finding high-quality training data that is both multilingual and multimodal. In this paper,... | Chen Qiu, Dan Oneata, Emanuele Bugliarello, Stella Frank, Desmond Elliott |  |
| 310 |  |  [Learning From the Source Document: Unsupervised Abstractive Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.309) |  | 0 | Most of the state-of-the-art methods for abstractive text summarization are under supervised learning settings, while heavily relying on high-quality and large-scale parallel corpora. In this paper, we remove the need for reference summaries and present an unsupervised learning method SCR... | Haojie Zhuang, Wei Emma Zhang, Jian Yang, Congbo Ma, Yutong Qu, Quan Z. Sheng |  |
| 311 |  |  [How to Do Things without Words: Modeling Semantic Drift of Emoji](https://doi.org/10.18653/v1/2022.findings-emnlp.310) |  | 0 | Emoji have become a significant part of our informal textual communication. Previous work, addressing the societal and linguistic functions of emoji, overlooked the relation between the semantics and the visual variations of the symbols. In this paper we model and analyze the semantic drift of... | Eyal Arviv, Oren Tsur |  |
| 312 |  |  [Mind Your Bias: A Critical Review of Bias Detection Methods for Contextual Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.311) |  | 0 | The awareness and mitigation of biases are of fundamental importance for the fair and transparent use of contextual language models, yet they crucially depend on the accurate detection of biases as a precursor. Consequently, numerous bias detection methods have been proposed, which vary in their... | Silke Husse, Andreas Spitz |  |
| 313 |  |  [ZeroPrompt: Scaling Prompt-Based Pretraining to 1, 000 Tasks Improves Zero-Shot Generalization](https://doi.org/10.18653/v1/2022.findings-emnlp.312) |  | 0 | We propose a multitask pretraining approach ZeroPrompt for zero-shot generalization, focusing on task scaling and zero-shot prompting.While previous models are trained on only a few dozen tasks, we scale to 1,000 tasks for the first time using real-world data. This leads to a crucial discovery that... | Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, Zhilin Yang |  |
| 314 |  |  [Semantic Role Labeling Meets Definition Modeling: Using Natural Language to Describe Predicate-Argument Structures](https://doi.org/10.18653/v1/2022.findings-emnlp.313) |  | 0 | One of the common traits of past and present approaches for Semantic Role Labeling (SRL) is that they rely upon discrete labels drawn from a predefined linguistic inventory to classify predicate senses and their arguments.However, we argue this need not be the case. In this paper, we present an... | Simone Conia, Edoardo Barba, Alessandro Scirè, Roberto Navigli |  |
| 315 |  |  [Is anisotropy really the cause of BERT embeddings not being semantic?](https://doi.org/10.18653/v1/2022.findings-emnlp.314) |  | 0 | In this paper we conduct a set of experiments aimed to improve our understanding of the lack of semantic isometry in BERT, i.e. the lack of correspondence between the embedding and meaning spaces of its contextualized word representations. Our empirical results show that, contrary to popular... | Alejandro Fuster Baggetto, Víctor Fresno |  |
| 316 |  |  [m⌃4 Adapter: Multilingual Multi-Domain Adaptation for Machine Translation with a Meta-Adapter](https://doi.org/10.18653/v1/2022.findings-emnlp.315) |  | 0 | Multilingual neural machine translation models (MNMT) yield state-of-the-art performance when evaluated on data from a domain and language pair seen at training time. However, when a MNMT model is used to translate under domain shift or to a new language pair, performance drops dramatically. We... | Wen Lai, Alexandra Chronopoulou, Alexander Fraser |  |
| 317 |  |  [Textual Enhanced Contrastive Learning for Solving Math Word Problems](https://doi.org/10.18653/v1/2022.findings-emnlp.316) |  | 0 | Solving math word problems is the task that analyses the relation of quantities e and requires an accurate understanding of contextual natural language information. Recent studies show that current models rely on shallow heuristics to predict solutions and could be easily misled by small textual... | Yibin Shen, Qianying Liu, Zhuoyuan Mao, Fei Cheng, Sadao Kurohashi |  |
| 318 |  |  [What Do Compressed Multilingual Machine Translation Models Forget?](https://doi.org/10.18653/v1/2022.findings-emnlp.317) |  | 0 | Recently, very large pre-trained models achieve state-of-the-art results in various natural language processing (NLP) tasks, but their size makes it more challenging to apply them in resource-constrained environments. Compression techniques allow to drastically reduce the size of the models and... | Alireza Mohammadshahi, Vassilina Nikoulina, Alexandre Berard, Caroline Brun, James Henderson, Laurent Besacier |  |
| 319 |  |  [Controllable Dialogue Simulation with In-context Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.318) |  | 0 | Building dialogue systems requires a large corpus of annotated dialogues. Such datasets are usually created via crowdsourcing, which is expensive and time-consuming. In this paper, we propose Dialogic, a novel dialogue simulation method based on large language model in-context learning to automate... | Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, Xifeng Yan |  |
| 320 |  |  [Improving the Factual Correctness of Radiology Report Generation with Semantic Rewards](https://doi.org/10.18653/v1/2022.findings-emnlp.319) |  | 0 | Neural image-to-text radiology report generation systems offer the potential to improve radiology reporting by reducing the repetitive process of report drafting and identifying possible medical errors. These systems have achieved promising performance as measured by widely used NLG metrics such as... | JeanBenoit Delbrouck, Pierre J. Chambon, Christian Bluethgen, Emily Bao Tsai, Omar Almusa, Curtis P. Langlotz |  |
| 321 |  |  [Recursive Neural Networks with Bottlenecks Diagnose (Non-)Compositionality](https://doi.org/10.18653/v1/2022.findings-emnlp.320) |  | 0 | A recent line of work in NLP focuses on the (dis)ability of models to generalise compositionally for artificial languages.However, when considering natural language tasks, the data involved is not strictly, or locally, compositional.Quantifying the compositionality of data is a challenging task,... | Verna Dankers, Ivan Titov |  |
| 322 |  |  [HumSet: Dataset of Multilingual Information Extraction and Classification for Humanitarian Crises Response](https://doi.org/10.18653/v1/2022.findings-emnlp.321) |  | 0 | Timely and effective response to humanitarian crises requires quick and accurate analysis of large amounts of text data – a process that can highly benefit from expert-assisted NLP systems trained on validated and annotated data in the humanitarian response domain. To enable creation of such NLP... | Selim Fekih, Nicolò Tamagnone, Benjamin Minixhofer, Ranjan Shrestha, Ximena Contla, Ewan Oglethorpe, Navid Rekabsaz |  |
| 323 |  |  [Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.322) |  | 0 | Non-autoregressive models achieve significant decoding speedup in neural machine translation but lack the ability to capture sequential dependency. Directed Acyclic Transformer (DA-Transformer) was recently proposed to model sequential dependency with a directed acyclic graph. Consequently, it has... | Chenze Shao, Zhengrui Ma, Yang Feng |  |
| 324 |  |  [Lexical Generalization Improves with Larger Models and Longer Training](https://doi.org/10.18653/v1/2022.findings-emnlp.323) |  | 0 | While fine-tuned language models perform well on many language tasks, they were also shown to rely on superficial surface features such as lexical overlap. Excessive utilization of such heuristics can lead to failure on challenging inputs. We analyze the use of lexical overlap heuristics in natural... | Elron Bandel, Yoav Goldberg, Yanai Elazar |  |
| 325 |  |  [Realistic Data Augmentation Framework for Enhancing Tabular Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.324) |  | 0 | Existing approaches to constructing training data for Natural Language Inference (NLI) tasks, such as for semi-structured table reasoning, are either via crowdsourcing or fully automatic methods. However, the former is expensive and time consuming and thus limits scale, and the latter often... | Dibyakanti Kumar, Vivek Gupta, Soumya Sharma, Shuo Zhang |  |
| 326 |  |  [Inducing Generalizable and Interpretable Lexica](https://doi.org/10.18653/v1/2022.findings-emnlp.325) |  | 0 | Lexica – words and associated scores – are widely used as simple, interpretable, generalizable language features to predict sentiment, emotions, mental health, and personality. They also provide insight into the psychological features behind those moods and traits. Such lexica, historically created... | Yilin Geng, Zetian Wu, Roshan Santhosh, Tejas Srivastava, Lyle H. Ungar, João Sedoc |  |
| 327 |  |  [The Curious Case of Absolute Position Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.326) |  | 0 | Transformer language models encode the notion of word order using positional information. Most commonly, this positional information is represented by absolute position embeddings (APEs), that are learned from the pretraining data. However, in natural language, it is not absolute position that... | Koustuv Sinha, Amirhossein Kazemnejad, Siva Reddy, Joelle Pineau, Dieuwke Hupkes, Adina Williams |  |
| 328 |  |  [Goal-oriented Vision-and-Dialog Navigation via Reinforcement Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.327) |  | 0 | Vision-and-dialog navigation is a recent benchmark for evaluating the AI capabilities of perception, interaction, and decision making. While existing methods developed for this benchmark have demonstrated great successes, they mostly rely on large datasets, where data collection can be a challenge,... | Yan Cao, Keting Lu, David DeFazio, Shiqi Zhang |  |
| 329 |  |  [Leveraging Data Recasting to Enhance Tabular Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.328) |  | 0 | Creating challenging tabular inference data is essential for learning complex reasoning. Prior work has mostly relied on two data generation strategies. The first is human annotation, which yields linguistically diverse data but is difficult to scale. The second category for creation is synthetic... | Aashna Jena, Vivek Gupta, Manish Shrivastava, Julian Martin Eisenschlos |  |
| 330 |  |  [Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again](https://doi.org/10.18653/v1/2022.findings-emnlp.329) |  | 0 | Large pre-trained language models (PLMs) such as GPT-3 have shown strong in-context learning capabilities, which are highly appealing for domains such as biomedicine that feature high and diverse demands of language technologies but also high data annotation costs. In this paper, we present the... | Bernal Jimenez Gutierrez, Nikolas McNeal, Clayton Washington, You Chen, Lang Li, Huan Sun, Yu Su |  |
| 331 |  |  [Attention weights accurately predict language representations in the brain](https://doi.org/10.18653/v1/2022.findings-emnlp.330) |  | 0 | In Transformer-based language models (LMs) the attention mechanism converts token embeddings into contextual embeddings that incorporate information from neighboring words. The resulting contextual hidden state embeddings have enabled highly accurate models of brain responses, suggesting that the... | Mathis Lamarre, Catherine Chen, Fatma Deniz |  |
| 332 |  |  [Improving HowNet-Based Chinese Word Sense Disambiguation with Translations](https://doi.org/10.18653/v1/2022.findings-emnlp.331) |  | 0 | Word sense disambiguation (WSD) is the task of identifying the intended sense of a word in context. While prior work on unsupervised WSD has leveraged lexical knowledge bases, such as WordNet and BabelNet, these resources have proven to be less effective for Chinese. Instead, the most widely used... | Xiang Zhang, Bradley Hauer, Grzegorz Kondrak |  |
| 333 |  |  [Mask-then-Fill: A Flexible and Effective Data Augmentation Framework for Event Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.332) |  | 0 | We present Mask-then-Fill, a flexible and effective data augmentation framework for event extraction. Our approach allows for more flexible manipulation of text and thus can generate more diverse data while keeping the original event structure unchanged as much as possible. Specifically, it first... | Jun Gao, Changlong Yu, Wei Wang, Huan Zhao, Ruifeng Xu |  |
| 334 |  |  [MOBA-E2C: Generating MOBA Game Commentaries via Capturing Highlight Events from the Meta-Data](https://doi.org/10.18653/v1/2022.findings-emnlp.333) |  | 0 | MOBA (Multiplayer Online Battle Arena) games such as Dota2 are currently one of the most popular e-sports gaming genres. Following professional commentaries is a great way to understand and enjoy a MOBA game. However, massive game competitions lack commentaries because of the shortage of... | Dawei Zhang, Sixing Wu, Yao Guo, Xiangqun Chen |  |
| 335 |  |  [Enhancing Automatic Readability Assessment with Pre-training and Soft Labels for Ordinal Regression](https://doi.org/10.18653/v1/2022.findings-emnlp.334) |  | 0 | The readability assessment task aims to assign a difficulty grade to a text. While neural models have recently demonstrated impressive performance, most do not exploit the ordinal nature of the difficulty grades, and make little effort for model initialization to facilitate fine-tuning. We address... | Jinshan Zeng, Yudong Xie, Xianglong Yu, John Lee, DingXuan Zhou |  |
| 336 |  |  [Opening up Minds with Argumentative Dialogues](https://doi.org/10.18653/v1/2022.findings-emnlp.335) |  | 0 | Recent research on argumentative dialogues has focused on persuading people to take some action, changing their stance on the topic of discussion, or winning debates. In this work, we focus on argumentative dialogues that aim to open up (rather than change) people’s minds to help them become more... | Youmna Farag, Charlotte O. Brand, Jacopo Amidei, Paul Piwek, Tom Stafford, Svetlana Stoyanchev, Andreas Vlachos |  |
| 337 |  |  [You Are My Type! Type Embeddings for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.336) |  | 0 | One reason for the positive impact of Pre-trained Language Models (PLMs) in NLP tasks is their ability to encode semantic types, such as ‘European City’ or ‘Woman’. While previous work has analyzed such information in the context of interpretability, it is not clear how to use types to steer the... | Mohammed Saeed, Paolo Papotti |  |
| 338 |  |  [Generating Textual Adversaries with Minimal Perturbation](https://doi.org/10.18653/v1/2022.findings-emnlp.337) |  | 0 | Many word-level adversarial attack approaches for textual data have been proposed in recent studies. However, due to the massive search space consisting of combinations of candidate words, the existing approaches face the problem of preserving the semantics of texts when crafting adversarial... | Xingyi Zhao, Lu Zhang, Depeng Xu, Shuhan Yuan |  |
| 339 |  |  [SensePOLAR: Word sense aware interpretability for pre-trained contextual word embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.338) |  | 0 | Adding interpretability to word embeddings represents an area of active research in textrepresentation. Recent work has explored the potential of embedding words via so-called polardimensions (e.g. good vs. bad, correct vs. wrong). Examples of such recent approachesinclude SemAxis, POLAR,... | Jan Engler, Sandipan Sikdar, Marlene Lutz, Markus Strohmaier |  |
| 340 |  |  [Contextualizing Language Models for Norms Diverging from Social Majority](https://doi.org/10.18653/v1/2022.findings-emnlp.339) |  | 0 | To comprehensibly contextualize decisions, artificial systems in social situations need a high degree of awareness of the rules of conduct of human behavior. Especially transformer-based language models have recently been shown to exhibit some such awareness. But what if norms in some social... | Niklas Kiehne, Hermann Kroll, WolfTilo Balke |  |
| 341 |  |  [Empathetic Dialogue Generation via Sensitive Emotion Recognition and Sensible Knowledge Selection](https://doi.org/10.18653/v1/2022.findings-emnlp.340) |  | 0 | Empathy, which is widely used in psychological counseling, is a key trait of everyday human conversations. Equipped with commonsense knowledge, current approaches to empathetic response generation focus on capturing implicit emotion within dialogue context, where the emotions are treated as a... | Lanrui Wang, Jiangnan Li, Zheng Lin, Fandong Meng, Chenxu Yang, Weiping Wang, Jie Zhou |  |
| 342 |  |  [Joint Multilingual Knowledge Graph Completion and Alignment](https://doi.org/10.18653/v1/2022.findings-emnlp.341) |  | 0 | Knowledge graph (KG) alignment and completion are usually treated as two independent tasks. While recent work has leveraged entity and relation alignments from multiple KGs, such as alignments between multilingual KGs with common entities and relations, a deeper understanding of the ways in which... | Vinh Tong, Dat Quoc Nguyen, Trung Thanh Huynh, Tam Thanh Nguyen, Quoc Viet Hung Nguyen, Mathias Niepert |  |
| 343 |  |  [A Framework for Automatic Generation of Spoken Question-Answering Data](https://doi.org/10.18653/v1/2022.findings-emnlp.342) |  | 0 | This paper describes a framework to automatically generate a spoken question answering (QA) dataset. The framework consists of a question generation (QG) module to generate questions automatically from given text documents, a text-to-speech (TTS) module to convert the text documents into spoken... | Merve Ünlü Menevse, Yusufcan Manav, Ebru Arisoy, Arzucan Özgür |  |
| 344 |  |  [Readability Controllable Biomedical Document Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.343) |  | 0 | Different from general documents, it is recognised that the ease with which people can understand a biomedical text is eminently varied, owing to the highly technical nature of biomedical documents and the variance of readers’ domain knowledge. However, existing biomedical document summarization... | Zheheng Luo, Qianqian Xie, Sophia Ananiadou |  |
| 345 |  |  [Beyond Additive Fusion: Learning Non-Additive Multimodal Interactions](https://doi.org/10.18653/v1/2022.findings-emnlp.344) |  | 0 | Multimodal fusion addresses the problem of analyzing spoken words in the multimodal context, including visual expressions and prosodic cues. Even when multimodal models lead to performance improvements, it is often unclear whether bimodal and trimodal interactions are learned or whether modalities... | Torsten Wörtwein, Lisa Sheeber, Nicholas B. Allen, Jeffrey F. Cohn, LouisPhilippe Morency |  |
| 346 |  |  [Generalization Differences between End-to-End and Neuro-Symbolic Vision-Language Reasoning Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.345) |  | 0 | For vision-and-language reasoning tasks, both fully connectionist, end-to-end methods and hybrid, neuro-symbolic methods have achieved high in-distribution performance. In which out-of-distribution settings does each paradigm excel? We investigate this question on both single-image and multi-image... | Wang Zhu, Jesse Thomason, Robin Jia |  |
| 347 |  |  [Learning to Model Multimodal Semantic Alignment for Story Visualization](https://doi.org/10.18653/v1/2022.findings-emnlp.346) |  | 0 | Story visualization aims to generate a sequence of images to narrate each sentence in a multi-sentence story, where the images should be realistic and keep global consistency across dynamic scenes and characters. Current works face the problem of semantic misalignment because of their fixed... | Bowen Li, Thomas Lukasiewicz |  |
| 348 |  |  [SciFact-Open: Towards open-domain scientific claim verification](https://doi.org/10.18653/v1/2022.findings-emnlp.347) |  | 0 | While research on scientific claim verification has led to the development of powerful systems that appear to approach human performance, these approaches have yet to be tested in a realistic setting against large corpora of scientific literature. Moving to this open-domain evaluation setting,... | David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, Hannaneh Hajishirzi |  |
| 349 |  |  [COMET-QE and Active Learning for Low-Resource Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.348) |  | 0 | Active learning aims to deliver maximum benefit when resources are scarce. We use COMET-QE, a reference-free evaluation metric, to select sentences for low-resource neural machine translation. Using Swahili, Kinyarwanda and Spanish for our experiments, we show that COMET-QE significantly... | Everlyn Chimoto, Bruce A. Bassett |  |
| 350 |  |  [MedicalSum: A Guided Clinical Abstractive Summarization Model for Generating Medical Reports from Patient-Doctor Conversations](https://doi.org/10.18653/v1/2022.findings-emnlp.349) |  | 0 | We introduce MedicalSum, a transformer-based sequence-to-sequence architecture for summarizing medical conversations by integrating medical domain knowledge from the Unified Medical Language System (UMLS). The novel knowledge augmentation is performed in three ways: (i) introducing a guidance... | George Michalopoulos, Kyle Williams, Gagandeep Singh, Thomas Lin |  |
| 351 |  |  [Leveraging Training Dynamics and Self-Training for Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.350) |  | 0 | The effectiveness of pre-trained language models in downstream tasks is highly dependent on the amount of labeled data available for training. Semi-supervised learning (SSL) is a promising technique that has seen wide attention recently due to its effectiveness in improving deep learning models... | Tiberiu Sosea, Cornelia Caragea |  |
| 352 |  |  [Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference](https://doi.org/10.18653/v1/2022.findings-emnlp.351) |  | 0 | Natural Language Inference (NLI) or Recognizing Textual Entailment (RTE) aims at predicting the relation between a pair of sentences (premise and hypothesis) as entailment, contradiction or semantic independence. Although deep learning models have shown promising performance for NLI in recent... | Mobashir Sadat, Cornelia Caragea |  |
| 353 |  |  [Unsupervised Text Deidentification](https://doi.org/10.18653/v1/2022.findings-emnlp.352) |  | 0 | Deidentification seeks to anonymize textual data prior to distribution. Automatic deidentification primarily uses supervised named entity recognition from human-labeled data points. We propose an unsupervised deidentification method that masks words that leak personally-identifying information. The... | John X. Morris, Justin T. Chiu, Ramin Zabih, Alexander M. Rush |  |
| 354 |  |  [Federated Continual Learning for Text Classification via Selective Inter-client Transfer](https://doi.org/10.18653/v1/2022.findings-emnlp.353) |  | 0 | In this work, we combine the two paradigms: Federated Learning (FL) and Continual Learning (CL) for text classification task in cloud-edge continuum. The objective of Federated Continual Learning (FCL) is to improve deep learning models over life time at each client by (relevant and efficient)... | Yatin Chaudhary, Pranav Rai, Matthias Schubert, Hinrich Schütze, Pankaj Gupta |  |
| 355 |  |  [DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents](https://doi.org/10.18653/v1/2022.findings-emnlp.354) |  | 0 | In the real world, autonomous driving agents navigate in highly dynamic environments full of unexpected situations where pre-trained models are unreliable. In these situations, what is immediately available to vehicles is often only human operators. Empowering autonomous driving agents with the... | Ziqiao Ma, Benjamin VanDerPloeg, CristianPaul Bara, Yidong Huang, EuiIn Kim, Felix Gervits, Matthew Marge, Joyce Chai |  |
| 356 |  |  [He Said, She Said: Style Transfer for Shifting the Perspective of Dialogues](https://doi.org/10.18653/v1/2022.findings-emnlp.355) |  | 0 | In this work, we define a new style transfer task: perspective shift, which reframes a dialouge from informal first person to a formal third person rephrasing of the text. This task requires challenging coreference resolution, emotion attribution, and interpretation of informal text. We explore... | Amanda Bertsch, Graham Neubig, Matthew R. Gormley |  |
| 357 |  |  [Dynamic Augmentation Data Selection for Few-shot Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.356) |  | 0 | Data augmentation has been a popular method for fine-tuning pre-trained language models to increase model robustness and performance. With augmentation data coming from modifying gold train data (in-sample augmentation) or being harvested from general domain unlabeled data (out-of-sample... | Guangliang Liu, Lifeng Jin, Owen Yuan, Jiayu Zhou |  |
| 358 |  |  [KPDROP: Improving Absent Keyphrase Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.357) |  | 0 | Keyphrase generation is the task of generating phrases (keyphrases) that summarize the main topics of a given document. Keyphrases can be either present or absent from the given document. While the extraction of present keyphrases has received much attention in the past, only recently a stronger... | Jishnu Ray Chowdhury, Seoyeon Park, Tuhin Kundu, Cornelia Caragea |  |
| 359 |  |  [Natural Language Deduction through Search over Statement Compositions](https://doi.org/10.18653/v1/2022.findings-emnlp.358) |  | 0 | In settings from fact-checking to question answering, we frequently want to know whether a collection of evidence (premises) entails a hypothesis. Existing methods primarily focus on the end-to-end discriminative version of this task, but less work has treated the generative version in which a... | Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, Greg Durrett |  |
| 360 |  |  [EnDex: Evaluation of Dialogue Engagingness at Scale](https://doi.org/10.18653/v1/2022.findings-emnlp.359) |  | 0 | We propose EnDex, the first human-reaction based model to evaluate dialogue engagingness. EnDex is trained on 80k Reddit-based Engagement Dataset (RED) curated using a novel distant-supervision framework. Engagingness is a key measure that captures high-level quality of AI dialogue systems and... | Guangxuan Xu, Ruibo Liu, Fabrice HarelCanada, Nischal Reddy Chandra, Nanyun Peng |  |
| 361 |  |  [LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.360) |  | 0 | Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance... | Dheeraj Mekala, Chengyu Dong, Jingbo Shang |  |
| 362 |  |  [Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models](https://doi.org/10.18653/v1/2022.findings-emnlp.361) |  | 0 | Model compression by way of parameter pruning, quantization, or distillation has recently gained popularity as an approach for reducing the computational requirements of modern deep neural network models for NLP. Inspired by prior works suggesting a connection between simpler, more generalizable... | Clara Na, Sanket Vaibhav Mehta, Emma Strubell |  |
| 363 |  |  [Structural Contrastive Representation Learning for Zero-shot Multi-label Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.362) |  | 0 | Zero-shot multi-label text classification (ZMTC) is a fundamental task in natural language processing with applications in the cold start problem of recommendation systems. Ideally, one would learn an expressive representation of both input text and label features so that ZMTC is transformed into a... | Tianyi Zhang, Zhaozhuo Xu, Tharun Medini, Anshumali Shrivastava |  |
| 364 |  |  [Improving Generalization of Pre-trained Language Models via Stochastic Weight Averaging](https://doi.org/10.18653/v1/2022.findings-emnlp.363) |  | 0 | Knowledge Distillation (KD) is a commonly used technique for improving the generalization of compact Pre-trained Language Models (PLMs) on downstream tasks. However, such methods impose the additional burden of training a separate teacher model for every new dataset.Alternatively, one may directly... | Peng Lu, Ivan Kobyzev, Mehdi Rezagholizadeh, Ahmad Rashid, Ali Ghodsi, Philippe Langlais |  |
| 365 |  |  [Learn What Is Possible, Then Choose What Is Best: Disentangling One-To-Many Relations in Language Through Text-based Games](https://doi.org/10.18653/v1/2022.findings-emnlp.364) |  | 0 | Language models pre-trained on large self-supervised corpora, followed by task-specific fine-tuning has become the dominant paradigm in NLP. These pre-training datasets often have a one-to-many structure—e.g. in dialogue there are many valid responses for a given context. However, only some of... | Benjamin Towle, Ke Zhou |  |
| 366 |  |  [Structurally Diverse Sampling for Sample-Efficient Training and Comprehensive Evaluation](https://doi.org/10.18653/v1/2022.findings-emnlp.365) |  | 0 | A growing body of research has demonstrated the inability of NLP models to generalize compositionally and has tried to alleviate it through specialized architectures, training schemes, and data augmentation, among other approaches. In this work, we study a different approach: training on instances... | Shivanshu Gupta, Sameer Singh, Matt Gardner |  |
| 367 |  |  [Unsupervised Multi-Granularity Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.366) |  | 0 | Text summarization is a user-preference based task, i.e., for one document, users often have different priorities for the summary. As a key aspect of customization in summarization, granularity is used to measure the semantic coverage between the summary and source document. However, developing... | Ming Zhong, Yang Liu, Suyu Ge, Yuning Mao, Yizhu Jiao, Xingxing Zhang, Yichong Xu, Chenguang Zhu, Michael Zeng, Jiawei Han |  |
| 368 |  |  [HeLo: Learning-Free Lookahead Decoding for Conversation Infilling](https://doi.org/10.18653/v1/2022.findings-emnlp.367) |  | 0 | We propose Heuristic Guided Lookahead Decoding (HeLo), a novel decoding strategy for conversation infilling. Conversation infilling aims to generate a seamless bridge of utterances connecting a given pair of source and target utterances. HeLo does not require fine-tuning or extra models – only the... | Ivan Lee, Taylor BergKirkpatrick |  |
| 369 |  |  [Invernet: An Inversion Attack Framework to Infer Fine-Tuning Datasets through Word Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.368) |  | 0 | Word embedding aims to learn the dense representation of words and has become a regular input preparation in many NLP tasks. Due to the data and computation intensive nature of learning embeddings from scratch, a more affordable way is to borrow the pretrained embedding available in public and... | Ishrak Hayet, Zijun Yao, Bo Luo |  |
| 370 |  |  [LawngNLI: A Long-Premise Benchmark for In-Domain Generalization from Short to Long Contexts and for Implication-Based Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.369) |  | 0 | Natural language inference has trended toward studying contexts beyond the sentence level. An important application area is law: past cases often do not foretell how they apply to new situations and implications must be inferred. This paper introduces LawngNLI, constructed from U.S. legal opinions... | William Bruno, Dan Roth |  |
| 371 |  |  [Distillation-Resistant Watermarking for Model Protection in NLP](https://doi.org/10.18653/v1/2022.findings-emnlp.370) |  | 0 | How can we protect the intellectual property of trained NLP models? Modern NLP models are prone to stealing by querying and distilling from their publicly exposed APIs. However, existing protection methods such as watermarking only work for images but are not applicable to text. We propose... | Xuandong Zhao, Lei Li, YuXiang Wang |  |
| 372 |  |  [NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation](https://doi.org/10.18653/v1/2022.findings-emnlp.371) |  | 0 | While counterfactual data augmentation offers a promising step towards robust generalization in natural language processing, producing a set of counterfactuals that offer valuable inductive bias for models remains a challenge. Most existing approaches for producing counterfactuals, manual or... | Phillip Howard, Gadi Singer, Vasudev Lal, Yejin Choi, Swabha Swayamdipta |  |
| 373 |  |  [Don't Just Clean It, Proxy Clean It: Mitigating Bias by Proxy in Pre-Trained Models](https://doi.org/10.18653/v1/2022.findings-emnlp.372) |  | 0 | Transformer-based pre-trained models are known to encode societal biases not only in their contextual representations, but also in downstream predictions when fine-tuned on task-specific data.We present D-Bias, an approach that selectively eliminates stereotypical associations (e.g, co-occurrence... | Swetasudha Panda, Ari Kobren, Michael L. Wick, Qinlan Shen |  |
| 374 |  |  [The Undesirable Dependence on Frequency of Gender Bias Metrics Based on Word Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.373) |  | 0 | Numerous works use word embedding-based metrics to quantify societal biases and stereotypes in texts. Recent studies have found that word embeddings can capture semantic similarity but may be affected by word frequency. In this work we study the effect of frequency when measuring female vs. male... | Francisco Valentini, Germán Rosati, Diego Fernández Slezak, Edgar Altszyler |  |
| 375 |  |  [BioNLI: Generating a Biomedical NLI Dataset Using Lexico-semantic Constraints for Adversarial Examples](https://doi.org/10.18653/v1/2022.findings-emnlp.374) |  | 0 | Natural language inference (NLI) is critical in many domains requiring complex decision-making, such as the biomedical domain. We introduce a novel semi-supervised procedure that bootstraps biomedical NLI datasets from positive entailment examples present in abstracts of biomedical publications. We... | Mohaddeseh Bastan, Mihai Surdeanu, Niranjan Balasubramanian |  |
| 376 |  |  [Self-supervised Cross-modal Pretraining for Speech Emotion Recognition and Sentiment Analysis](https://doi.org/10.18653/v1/2022.findings-emnlp.375) |  | 0 | Multimodal speech emotion recognition (SER) and sentiment analysis (SA) are important techniques for human-computer interaction. Most existing multimodal approaches utilize either shallow cross-modal fusion of pretrained features, or deep cross-modal fusion with raw features. Recently, attempts... | IekHeng Chu, Ziyi Chen, Xinlu Yu, Mei Han, Jing Xiao, Peng Chang |  |
| 377 |  |  [Multimodal Conversation Modelling for Topic Derailment Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.376) |  | 0 | Conversations on social media tend to go off-topic and turn into different and sometimes toxic exchanges. Previous work focuses on analysing textual dialogues that have derailed into toxic content, but the range of derailment types is much broader, including spam or bot content, tangential... | Zhenhao Li, Marek Rei, Lucia Specia |  |
| 378 |  |  [Active Learning for Abstractive Text Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.377) |  | 0 | Construction of human-curated annotated datasets for abstractive text summarization (ATS) is very time-consuming and expensive because creating each instance requires a human annotator to read a long document and compose a shorter summary that would preserve the key information relayed by the... | Akim Tsvigun, Ivan Lysenko, Danila Sedashov, Ivan Lazichny, Eldar Damirov, Vladimir Karlov, Artemy Belousov, Leonid Sanochkin, Maxim Panov, Alexander Panchenko, Mikhail Burtsev, Artem Shelmanov |  |
| 379 |  |  [Finding Memo: Extractive Memorization in Constrained Sequence Generation Tasks](https://doi.org/10.18653/v1/2022.findings-emnlp.378) |  | 0 | Memorization presents a challenge for several constrained Natural Language Generation (NLG) tasks such as Neural Machine Translation (NMT), wherein the proclivity of neural models to memorize noisy and atypical samples reacts adversely with the noisy (web crawled) datasets. However, previous... | Vikas Raunak, Arul Menezes |  |
| 380 |  |  [SALTED: A Framework for SAlient Long-tail Translation Error Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.379) |  | 0 | Traditional machine translation (MT) metrics provide an average measure of translation quality that is insensitive to the long tail of behavioral problems. Examples include translation of numbers, physical units, dropped content and hallucinations. These errors, which occur rarely and unpredictably... | Vikas Raunak, Matt Post, Arul Menezes |  |
| 381 |  |  [Discord Questions: A Computational Approach To Diversity Analysis in News Coverage](https://doi.org/10.18653/v1/2022.findings-emnlp.380) |  | 0 | There are many potential benefits to news readers accessing diverse sources. Modern news aggregators do the hard work of organizing the news, offering readers a plethora of source options, but choosing which source to read remains challenging.We propose a new framework to assist readers in... | Philippe Laban, ChienSheng Wu, Lidiya Murakhovs'ka, Xiang 'Anthony' Chen, Caiming Xiong |  |
| 382 |  |  [FocusQA: Open-Domain Question Answering with a Context in Focus](https://doi.org/10.18653/v1/2022.findings-emnlp.381) |  | 0 | We introduce question answering with a cotext in focus, a task that simulates a free interaction with a QA system. The user reads on a screen some information about a topic, and they can follow-up with questions that can be either related or not to the topic; and the answer can be found in the... | Gianni Barlacchi, Ivano Lauriola, Alessandro Moschitti, Marco Del Tredici, Xiaoyu Shen, Thuy Vu, Bill Byrne, Adrià de Gispert |  |
| 383 |  |  [Challenges and Opportunities in Information Manipulation Detection: An Examination of Wartime Russian Media](https://doi.org/10.18653/v1/2022.findings-emnlp.382) |  | 0 | NLP research on public opinion manipulation campaigns has primarily focused on detecting overt strategies such as fake news and disinformation. However, information manipulation in the ongoing Russia-Ukraine war exemplifies how governments and media also employ more nuanced strategies. We release a... | Chan Young Park, Julia Mendelsohn, Anjalie Field, Yulia Tsvetkov |  |
| 384 |  |  [Disentangling Task Relations for Few-shot Text Classification via Self-Supervised Hierarchical Task Clustering](https://doi.org/10.18653/v1/2022.findings-emnlp.383) |  | 0 | Few-Shot Text Classification (FSTC) imitates humans to learn a new text classifier efficiently with only few examples, by leveraging prior knowledge from historical tasks. However, most prior works assume that all the tasks are sampled from a single data source, which cannot adapt to real-world... | Juan Zha, Zheng Li, Ying Wei, Yu Zhang |  |
| 385 |  |  [XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing](https://doi.org/10.18653/v1/2022.findings-emnlp.384) |  | 0 | In-context learning using large language models has recently shown surprising results for semantic parsing tasks such as Text-to-SQL translation.Prompting GPT-3 or Codex using several examples of question-SQL pairs can produce excellent results, comparable to state-of-the-art finetuning-based... | Peng Shi, Rui Zhang, He Bai, Jimmy Lin |  |
| 386 |  |  [Continuation KD: Improved Knowledge Distillation through the Lens of Continuation Optimization](https://doi.org/10.18653/v1/2022.findings-emnlp.385) |  | 0 | Knowledge Distillation (KD) has been extensively used for natural language understanding (NLU) tasks to improve a small model’s (a student) generalization by transferring the knowledge from a larger model (a teacher). Although KD methods achieve state-of-the-art performance in numerous settings,... | Aref Jafari, Ivan Kobyzev, Mehdi Rezagholizadeh, Pascal Poupart, Ali Ghodsi |  |
| 387 |  |  [Detecting Dementia from Long Neuropsychological Interviews](https://doi.org/10.18653/v1/2022.findings-emnlp.386) |  | 0 | Neuropsychological exams are commonly used to diagnose various kinds of cognitive impairment. They typically involve a trained examiner who conducts a series of cognitive tests with a subject. In recent years, there has been growing interest in developing machine learning methods to extract speech... | Nauman Dawalatabad, Yuan Gong, Sameer Khurana, Rhoda Au, James R. Glass |  |
| 388 |  |  [Sarcasm Detection is Way Too Easy! An Empirical Comparison of Human and Machine Sarcasm Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.387) |  | 0 | Recently, author-annotated sarcasm datasets, which focus on intended, rather than perceived sarcasm, have been introduced. Although datasets collected using first-party annotation have important benefits, there is no comparison of human and machine performance on these new datasets. In this paper,... | Ibrahim Abu Farha, Steven R. Wilson, Silviu Oprea, Walid Magdy |  |
| 389 |  |  [Cross-lingual Text-to-SQL Semantic Parsing with Representation Mixup](https://doi.org/10.18653/v1/2022.findings-emnlp.388) |  | 0 | We focus on the cross-lingual Text-to-SQL semantic parsing task,where the parsers are expected to generate SQL for non-English utterances based on English database schemas.Intuitively, English translation as side information is an effective way to bridge the language gap,but noise introduced by the... | Peng Shi, Linfeng Song, Lifeng Jin, Haitao Mi, He Bai, Jimmy Lin, Dong Yu |  |
| 390 |  |  [JamPatoisNLI: A Jamaican Patois Natural Language Inference Dataset](https://doi.org/10.18653/v1/2022.findings-emnlp.389) |  | 0 | JamPatoisNLI provides the first dataset for natural language inference in a creole language, Jamaican Patois.Many of the most-spoken low-resource languages are creoles. These languages commonly have a lexicon derived from a major world language and a distinctive grammar reflecting the languages of... | RuthAnn Armstrong, John Hewitt, Christopher D. Manning |  |
| 391 |  |  [Are Neural Topic Models Broken?](https://doi.org/10.18653/v1/2022.findings-emnlp.390) |  | 0 | Recently, the relationship between automated and human evaluation of topic models has been called into question. Method developers have staked the efficacy of new topic model variants on automated measures, and their failure to approximate human preferences places these models on uncertain ground.... | Alexander Miserlis Hoyle, Rupak Sarkar, Pranav Goel, Philip Resnik |  |
| 392 |  |  [Know Thy Strengths: Comprehensive Dialogue State Tracking Diagnostics](https://doi.org/10.18653/v1/2022.findings-emnlp.391) |  | 0 | Recent works that revealed the vulnerability of dialogue state tracking (DST) models to distributional shifts have made holistic comparisons on robustness and qualitative analyses increasingly important for understanding their relative performance. We present our findings from standardized and... | Hyundong Cho, Chinnadhurai Sankar, Christopher Lin, Kaushik Ram Sadagopan, Shahin Shayandeh, Asli Celikyilmaz, Jonathan May, Ahmad Beirami |  |
| 393 |  |  [Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge](https://doi.org/10.18653/v1/2022.findings-emnlp.392) |  | 0 | We propose a novel open-domain question answering (ODQA) framework for answering single/multi-hop questions across heterogeneous knowledge sources.The key novelty of our method is the introduction of the intermediary modules into the current retriever-reader pipeline.Unlike previous methods that... | Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg, Jianfeng Gao |  |
| 394 |  |  [Detecting Languages Unintelligible to Multilingual Models through Local Structure Probes](https://doi.org/10.18653/v1/2022.findings-emnlp.393) |  | 0 | Providing better language tools for low-resource and endangered languages is imperative for equitable growth.Recent progress with massively multilingual pretrained models has proven surprisingly effective at performing zero-shot transfer to a wide variety of languages.However, this transfer is not... | Louis Clouâtre, Prasanna Parthasarathi, Amal Zouaq, Sarath Chandar |  |
| 395 |  |  [Cards Against AI: Predicting Humor in a Fill-in-the-blank Party Game](https://doi.org/10.18653/v1/2022.findings-emnlp.394) |  | 0 | Humor is an inherently social phenomenon, with humorous utterances shaped by what is socially and culturally accepted. Understanding humor is an important NLP challenge, with many applications to human-computer interactions. In this work we explore humor in the context of Cards Against Humanity – a... | Dan Ofer, Dafna Shahaf |  |
| 396 |  |  [Open-Vocabulary Argument Role Prediction For Event Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.395) |  | 0 | The argument role in event extraction refers to the relation between an event and an argument participating in it. Despite the great progress in event extraction, existing studies still depend on roles pre-defined by domain experts. These studies expose obvious weakness when extending to emerging... | Yizhu Jiao, Sha Li, Yiqing Xie, Ming Zhong, Heng Ji, Jiawei Han |  |
| 397 |  |  [Token-level Sequence Labeling for Spoken Language Understanding using Compositional End-to-End Models](https://doi.org/10.18653/v1/2022.findings-emnlp.396) |  | 0 | End-to-end spoken language understanding (SLU) systems are gaining popularity over cascaded approaches due to their simplicity and ability to avoid error propagation. However, these systems model sequence labeling as a sequence prediction task causing a divergence from its well-established... | Siddhant Arora, Siddharth Dalmia, Brian Yan, Florian Metze, Alan W. Black, Shinji Watanabe |  |
| 398 |  |  [Baked-in State Probing](https://doi.org/10.18653/v1/2022.findings-emnlp.397) |  | 0 | Neural language models have been analyzed for their linguistic and extra-linguistic knowledge via probing. Of particular interest has been the following question: how much can a language model trained only on form learn about meaning? Recent work has demonstrated via probing classifiers that in the... | Shubham Toshniwal, Sam Wiseman, Karen Livescu, Kevin Gimpel |  |
| 399 |  |  [ClinicalT5: A Generative Language Model for Clinical Text](https://doi.org/10.18653/v1/2022.findings-emnlp.398) |  | 0 | In the past few years, large pre-trained language models (PLMs) have been widely adopted in different areas and have made fundamental improvements over a variety of downstream tasks in natural language processing (NLP). Meanwhile, domain-specific variants of PLMs are being proposed to address the... | Qiuhao Lu, Dejing Dou, Thien Huu Nguyen |  |
| 400 |  |  [Find Someone Who: Visual Commonsense Understanding in Human-Centric Grounding](https://doi.org/10.18653/v1/2022.findings-emnlp.399) |  | 0 | From a visual scene containing multiple people, human is able to distinguish each individual given the context descriptions about what happened before, their mental/physical states or intentions, etc. Above ability heavily relies on human-centric commonsense knowledge and reasoning. For example, if... | Haoxuan You, Rui Sun, Zhecan Wang, KaiWei Chang, ShihFu Chang |  |
| 401 |  |  [CrisisLTLSum: A Benchmark for Local Crisis Event Timeline Extraction and Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.400) |  | 0 | Social media has increasingly played a key role in emergency response: first responders can use public posts to better react to ongoing crisis events and deploy the necessary resources where they are most needed. Timeline extraction and abstractive summarization are critical technical tasks to... | Hossein Rajaby Faghihi, Bashar Alhafni, Ke Zhang, Shihao Ran, Joel R. Tetreault, Alejandro Jaimes |  |
| 402 |  |  [Prompt-Tuning Can Be Much Better Than Fine-Tuning on Cross-lingual Understanding With Multilingual Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.401) |  | 0 | Pre-trained multilingual language models show significant performance gains for zero-shot cross-lingual model transfer on a wide range of natural language understanding (NLU) tasks. Previously, for zero-shot cross-lingual evaluation, pre-trained models are only fine-tuned on English data and tested... | Lifu Tu, Caiming Xiong, Yingbo Zhou |  |
| 403 |  |  [BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model](https://doi.org/10.18653/v1/2022.findings-emnlp.402) |  | 0 | This paper presents BERT-CTC, a novel formulation of end-to-end speech recognition that adapts BERT for connectionist temporal classification (CTC). Our formulation relaxes the conditional independence assumptions used in conventional CTC and incorporates linguistic knowledge through the explicit... | Yosuke Higuchi, Brian Yan, Siddhant Arora, Tetsuji Ogawa, Tetsunori Kobayashi, Shinji Watanabe |  |
| 404 |  |  [EtriCA: Event-Triggered Context-Aware Story Generation Augmented by Cross Attention](https://doi.org/10.18653/v1/2022.findings-emnlp.403) |  | 0 | One of the key challenges of automatic story generation is how to generate a long narrative that can maintain fluency, relevance, and coherence. Despite recent progress, current story generation systems still face the challenge of how to effectively capture contextual and event features, which has... | Chen Tang, Chenghua Lin, Henglin Huang, Frank Guerin, Zhihao Zhang |  |
| 405 |  |  [LADIS: Language Disentanglement for 3D Shape Editing](https://doi.org/10.18653/v1/2022.findings-emnlp.404) |  | 0 | Natural language interaction is a promising direction for democratizing 3D shape design. However, existing methods for text-driven 3D shape editing face challenges in producing decoupled, local edits to 3D shapes. We address this problem by learning disentangled latent representations that ground... | Ian Huang, Panos Achlioptas, Tianyi Zhang, Sergey Tulyakov, Minhyuk Sung, Leonidas J. Guibas |  |
| 406 |  |  [Effective Pretraining Objectives for Transformer-based Autoencoders](https://doi.org/10.18653/v1/2022.findings-emnlp.405) |  | 0 | In this paper, we study trade-offs between efficiency, cost and accuracy when pre-training Transformer encoders with different pre-training objectives. For this purpose, we analyze features of common objectives and combine them to create new effective pre-training approaches. Specifically, we... | Luca Di Liello, Matteo Gabburo, Alessandro Moschitti |  |
| 407 |  |  [Language Model Detoxification in Dialogue with Contextualized Stance Control](https://doi.org/10.18653/v1/2022.findings-emnlp.406) |  | 0 | To reduce the toxic degeneration in a pretrained Language Model (LM), previous work on Language Model detoxification has focused on reducing the toxicity of the generation itself (self-toxicity) without consideration of the context. As a result, a type of implicit offensive language where the... | Jing Qian, Xifeng Yan |  |
| 408 |  |  [Multilingual SubEvent Relation Extraction: A Novel Dataset and Structure Induction Method](https://doi.org/10.18653/v1/2022.findings-emnlp.407) |  | 0 | Subevent Relation Extraction (SRE) is a task in Information Extraction that aims to recognize spatial and temporal containment relations between event mentions in text. Recent methods have utilized pre-trained language models to represent input texts for SRE. However, a key issue in existing SRE... | Viet Dac Lai, Hieu Man, Linh Ngo Van, Franck Dernoncourt, Thien Huu Nguyen |  |
| 409 |  |  [A Two-Stage Approach towards Generalization in Knowledge Base Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.408) |  | 0 | Most existing approaches for Knowledge Base Question Answering (KBQA) focus on a specific underlying knowledge base either because of inherent assumptions in the approach, or because evaluating it on a different knowledge base requires non-trivial changes. However, many popular knowledge bases... | Srinivas Ravishankar, Dung Thai, Ibrahim Abdelaziz, Nandana Mihindukulasooriya, Tahira Naseem, Pavan Kapanipathi, Gaetano Rossiello, Achille Fokoue |  |
| 410 |  |  [Few-Shot (Dis)Agreement Identification in Online Discussions with Regularized and Augmented Meta-Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.409) |  | 0 | Online discussions are abundant with opinions towards a common topic, and identifying (dis)agreement between a pair of comments enables many opinion mining applications. Realizing the increasing needs to analyze opinions for emergent new topics that however tend to lack annotations, we present the... | Yuanyuan Lei, Ruihong Huang |  |
| 411 |  |  [Data Cartography for Low-Resource Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.410) |  | 0 | While collecting or generating more parallel data is necessary to improve machine translation (MT) in low-resource settings, we lack an understanding of how the limited amounts of existing data are actually used to help guide the collection of further resources. In this paper, we apply data... | Aquia Richburg, Marine Carpuat |  |
| 412 |  |  [Augmenting Multi-Turn Text-to-SQL Datasets with Self-Play](https://doi.org/10.18653/v1/2022.findings-emnlp.411) |  | 0 | The task of context-dependent text-to-SQL aims to convert multi-turn user utterances to formal SQL queries. This is a challenging task due to both the scarcity of training data from which to learn complex contextual dependencies and to generalize to unseen databases. In this paper we explore... | Qi Liu, Zihuiwen Ye, Tao Yu, Linfeng Song, Phil Blunsom |  |
| 413 |  |  [Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.412) |  | 0 | We explore the idea of compressing the prompts used to condition language models, and show that compressed prompts can retain a substantive amount of information about the original prompt. For severely compressed prompts, while fine-grained information is lost, abstract information and general... | David Wingate, Mohammad Shoeybi, Taylor Sorensen |  |
| 414 |  |  [NaturalAdversaries: Can Naturalistic Adversaries Be as Effective as Artificial Adversaries?](https://doi.org/10.18653/v1/2022.findings-emnlp.413) |  | 0 | While a substantial body of prior work has explored adversarial example generation for natural language understanding tasks, these examples are often unrealistic and diverge from the real-world data distributions. In this work, we introduce a two-stage adversarial example generation framework... | Saadia Gabriel, Hamid Palangi, Yejin Choi |  |
| 415 |  |  [Multi-Path Transformer is Better: A Case Study on Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.414) |  | 0 | For years the model performance in machine learning obeyed a power-law relationship with the model size. For the consideration of parameter efficiency, recent studies focus on increasing model depth rather than width to achieve better performance. In this paper, we study how model width affects the... | Ye Lin, Shuhan Zhou, Yanyang Li, Anxiang Ma, Tong Xiao, Jingbo Zhu |  |
| 416 |  |  [Unsupervised Learning of Hierarchical Conversation Structure](https://doi.org/10.18653/v1/2022.findings-emnlp.415) |  | 0 | Human conversations can evolve in many different ways, creating challenges for automatic understanding and summarization. Goal-oriented conversations often have meaningful sub-dialogue structure, but it can be highly domain-dependent. This work introduces an unsupervised approach to learning... | BoRu Lu, Yushi Hu, Hao Cheng, Noah A. Smith, Mari Ostendorf |  |
| 417 |  |  [Task Compass: Scaling Multi-task Pre-training with Task Prefix](https://doi.org/10.18653/v1/2022.findings-emnlp.416) |  | 0 | Leveraging task-aware annotated data as supervised signals to assist with self-supervised learning on large-scale unlabeled data has become a new trend in pre-training language models. Existing studies show that multi-task learning with large-scale supervised tasks suffers from negative effects... | Zhuosheng Zhang, Shuohang Wang, Yichong Xu, Yuwei Fang, Wenhao Yu, Yang Liu, Hai Zhao, Chenguang Zhu, Michael Zeng |  |
| 418 |  |  [Sharpness-Aware Minimization with Dynamic Reweighting](https://doi.org/10.18653/v1/2022.findings-emnlp.417) |  | 0 | Deep neural networks are often overparameterized and may not easily achieve model generalization. Adversarial training has shown effectiveness in improving generalization by regularizing the change of loss on top of adversarially chosen perturbations. The recently proposed sharpness-aware... | Wenxuan Zhou, Fangyu Liu, Huan Zhang, Muhao Chen |  |
| 419 |  |  [Predicting Long-Term Citations from Short-Term Linguistic Influence](https://doi.org/10.18653/v1/2022.findings-emnlp.418) |  | 0 | A standard measure of the influence of a research paper is the number of times it is cited. However, papers may be cited for many reasons, and citation count is not informative about the extent to which a paper affected the content of subsequent publications. We therefore propose a novel method to... | Sandeep Soni, David Bamman, Jacob Eisenstein |  |
| 420 |  |  [Joint Audio/Text Training for Transformer Rescorer of Streaming Speech Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.419) |  | 0 | Recently, there has been an increasing interest in two-pass streaming end-to-end speech recognition (ASR) that incorporates a 2nd-pass rescoring model on top of the conventional 1st-pass streaming ASR model to improve recognition accuracy while keeping latency low. One of the latest 2nd-pass... | Suyoun Kim, Ke Li, Lucas Kabela, Ron Huang, Jiedan Zhu, Ozlem Kalinli, Duc Le |  |
| 421 |  |  [TyDiP: A Dataset for Politeness Classification in Nine Typologically Diverse Languages](https://doi.org/10.18653/v1/2022.findings-emnlp.420) |  | 0 | We study politeness phenomena in nine typologically diverse languages. Politeness is an important facet of communication and is sometimes argued to be cultural-specific, yet existing computational linguistic study is limited to English. We create TyDiP, a dataset containing three-way politeness... | Anirudh Srinivasan, Eunsol Choi |  |
| 422 |  |  [Probing Cross-modal Semantics Alignment Capability from the Textual Perspective](https://doi.org/10.18653/v1/2022.findings-emnlp.421) |  | 0 | In recent years, vision and language pre-training (VLP) models have advanced the state-of-the-art results in a variety of cross-modal downstream tasks. Aligning cross-modal semantics is claimed to be one of the essential capabilities of VLP models. However, it still remains unclear about the inner... | Zheng Ma, Shi Zong, Mianzhi Pan, Jianbing Zhang, Shujian Huang, Xinyu Dai, Jiajun Chen |  |
| 423 |  |  [Hidden State Variability of Pretrained Language Models Can Guide Computation Reduction for Transfer Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.422) |  | 0 | While transferring a pretrained language model, common approaches conventionally attach their task-specific classifiers to the top layer and adapt all the pretrained layers. We investigate whether one could make a task-specific selection on which subset of the layers to adapt and where to place the... | Shuo Xie, Jiahao Qiu, Ankita Pasad, Li Du, Qing Qu, Hongyuan Mei |  |
| 424 |  |  [Language Models as Agent Models](https://doi.org/10.18653/v1/2022.findings-emnlp.423) |  | 0 | Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in the outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them—a fact... | Jacob Andreas |  |
| 425 |  |  [Combinatory Grammar Tells Underlying Relevance among Entities](https://doi.org/10.18653/v1/2022.findings-emnlp.424) |  | 0 | Relation extraction (RE) is an important task in natural language processing which aims to annotate the relation between two given entities, which requires a deep understanding of the running text. To import model performance, existing approaches leverage syntactic information to facilitate the... | Yuanhe Tian, Yan Song |  |
| 426 |  |  [Leveraging Open Data and Task Augmentation to Automated Behavioral Coding of Psychotherapy Conversations in Low-Resource Scenarios](https://doi.org/10.18653/v1/2022.findings-emnlp.425) |  | 0 | In psychotherapy interactions, the quality of a session is assessed by codifying the communicative behaviors of participants during the conversation through manual observation and annotation. Developing computational approaches for automated behavioral coding can reduce the burden on human coders... | Zhuohao Chen, Nikolaos Flemotomos, Zac E. Imel, David C. Atkins, Shrikanth Narayanan |  |
| 427 |  |  [Learning to Detect Noisy Labels Using Model-Based Features](https://doi.org/10.18653/v1/2022.findings-emnlp.426) |  | 0 | Label noise is ubiquitous in various machine learning scenarios such as self-labeling with model predictions and erroneous data annotation. Many existing approaches are based on heuristics such as sample losses, which might not be flexible enough to achieve optimal solutions. Meta learning based... | Zhihao Wang, Zongyu Lin, Junjie Wen, Xianxin Chen, Peiqi Liu, Guidong Zheng, Yujun Chen, Zhilin Yang |  |
| 428 |  |  [Keyphrase Generation Beyond the Boundaries of Title and Abstract](https://doi.org/10.18653/v1/2022.findings-emnlp.427) |  | 0 | Keyphrase generation aims at generating important phrases (keyphrases) that best describe a given document. In scholarly domains, current approaches have largely used only the title and abstract of the articles to generate keyphrases. In this paper, we comprehensively explore whether the... | Krishna Garg, Jishnu Ray Chowdhury, Cornelia Caragea |  |
| 429 |  |  [Composition, Attention, or Both?](https://doi.org/10.18653/v1/2022.findings-emnlp.428) |  | 0 | In this paper, we propose a novel architecture called Composition Attention Grammars (CAGs) that recursively compose subtrees into a single vector representation with a composition function, and selectively attend to previous structural information with a self-attention mechanism. We investigate... | Ryo Yoshida, Yohei Oseki |  |
| 430 |  |  [CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model](https://doi.org/10.18653/v1/2022.findings-emnlp.429) |  | 0 | Manually designing cloze test consumes enormous time and efforts. The major challenge lies in wrong option (distractor) selection. Having carefully-design distractors improves the effectiveness of learner ability assessment. As a result, the idea of automatically generating cloze distractor is... | ShangHsuan Chiang, SsuCheng Wang, YaoChung Fan |  |
| 431 |  |  [G3: Geolocation via Guidebook Grounding](https://doi.org/10.18653/v1/2022.findings-emnlp.430) |  | 0 | We demonstrate how language can improve geolocation: the task of predicting the location where an image was taken. Here we study explicit knowledge from human-written guidebooks that describe the salient and class-discriminative visual features humans use for geolocation. We propose the task of... | Grace Luo, Giscard Biamby, Trevor Darrell, Daniel Fried, Anna Rohrbach |  |
| 432 |  |  [Controlling Bias Exposure for Fair Interpretable Predictions](https://doi.org/10.18653/v1/2022.findings-emnlp.431) |  | 0 | Recent work on reducing bias in NLP models usually focuses on protecting or isolating information related to a sensitive attribute (like gender or race). However, when sensitive information is semantically entangled with the task information of the input, e.g., gender information is predictive for... | Zexue He, Yu Wang, Julian J. McAuley, Bodhisattwa Prasad Majumder |  |
| 433 |  |  [Investigating the Benefits of Free-Form Rationales](https://doi.org/10.18653/v1/2022.findings-emnlp.432) |  | 0 | Free-form rationales aim to aid model interpretability by supplying the background knowledge that can help understand model decisions. Crowdsourced rationales are provided for commonsense QA instances in popular datasets such as CoS-E and ECQA, but their utility remains under-investigated. We... | Jiao Sun, Swabha Swayamdipta, Jonathan May, Xuezhe Ma |  |
| 434 |  |  [Data-Efficient Concept Extraction from Pre-trained Language Models for Commonsense Explanation Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.433) |  | 0 | Predicting the key explanation concept is essential for generating commonsense explanations. This paper introduces a method to predict the concept from pre-trained language models for commonsense explanation generation. Our experiment found that adopting a language model as the concept extractor... | Yanbo Fang, Yongfeng Zhang |  |
| 435 |  |  [Unsupervised Domain Adaptation for Joint Information Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.434) |  | 0 | Joint Information Extraction (JIE) aims to jointly solve multiple tasks in the Information Extraction pipeline (e.g., entity mention, event trigger, relation, and event argument extraction). Due to their ability to leverage task dependencies and avoid error propagation, JIE models have presented... | Nghia Trung Ngo, Bonan Min, Thien Huu Nguyen |  |
| 436 |  |  [Foiling Training-Time Attacks on Neural Machine Translation Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.435) |  | 0 | Neural machine translation (NMT) systems are vulnerable to backdoor attacks, whereby an attacker injects poisoned samples into training such that a trained model produces malicious translations. Nevertheless, there is little research on defending against such backdoor attacks in NMT. In this paper,... | Jun Wang, Xuanli He, Benjamin I. P. Rubinstein, Trevor Cohn |  |
| 437 |  |  [Learning Action-Effect Dynamics for Hypothetical Vision-Language Reasoning Task](https://doi.org/10.18653/v1/2022.findings-emnlp.436) |  | 0 | ‘Actions’ play a vital role in how humans interact with the world. Thus, autonomous agents that would assist us in everyday tasks also require the capability to perform ‘Reasoning about Actions & Change’ (RAC). This has been an important research direction in Artificial Intelligence (AI) in... | Shailaja Keyur Sampat, Pratyay Banerjee, Yezhou Yang, Chitta Baral |  |
| 438 |  |  [Named Entity and Relation Extraction with Multi-Modal Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.437) |  | 0 | Multi-modal named entity recognition (NER) and relation extraction (RE) aim to leverage relevant image information to improve the performance of NER and RE. Most existing efforts largely focused on directly extracting potentially useful information from images (such as pixel-level features,... | Xinyu Wang, Jiong Cai, Yong Jiang, Pengjun Xie, Kewei Tu, Wei Lu |  |
| 439 |  |  [Calibrating Factual Knowledge in Pretrained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.438) |  | 0 | Previous literature has proved that Pretrained Language Models (PLMs) can store factual knowledge. However, we find that facts stored in the PLMs are not always correct. It motivates us to explore a fundamental question: How do we calibrate factual knowledge in PLMs without re-training from... | Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui, Lei Li |  |
| 440 |  |  [MCPG: A Flexible Multi-Level Controllable Framework for Unsupervised Paraphrase Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.439) |  | 0 | We present MCPG: a simple and effectiveapproach for controllable unsupervised paraphrase generation, which is also flexible toadapt to specific domains without extra training. MCPG is controllable in different levels: local lexicons, global semantics, and universal styles. The unsupervised paradigm... | Yi Chen, Haiyun Jiang, Lemao Liu, Rui Wang, Shuming Shi, Ruifeng Xu |  |
| 441 |  |  [WordTies: Measuring Word Associations in Language Models via Constrained Sampling](https://doi.org/10.18653/v1/2022.findings-emnlp.440) |  | 0 | Word associations are widely used in psychology to provide insights on how humans perceive and understand concepts. Comparing word associations in language models (LMs) to those generated by human subjects can serve as a proxy to uncover embedded lexical and commonsense knowledge in language... | Peiran Yao, Tobias Renwick, Denilson Barbosa |  |
| 442 |  |  [Exploring The Landscape of Distributional Robustness for Question Answering Models](https://doi.org/10.18653/v1/2022.findings-emnlp.441) |  | 0 | We conduct a large empirical evaluation to investigate the landscape of distributional robustness in question answering. Our investigation spans over 350 models and 16 question answering datasets, including a diverse set of architectures, model sizes, and adaptation methods (e.g., fine-tuning,... | Anas Awadalla, Mitchell Wortsman, Gabriel Ilharco, Sewon Min, Ian Magnusson, Hannaneh Hajishirzi, Ludwig Schmidt |  |
| 443 |  |  [Collaborative Reasoning on Multi-Modal Semantic Graphs for Video-Grounded Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.442) |  | 0 | We study video-grounded dialogue generation, where a response is generated based on the dialogue context and the associated video. The primary challenges of this task lie in (1) the difficulty of integrating video data into pre-trained language models (PLMs) which presents obstacles to exploiting... | Xueliang Zhao, Yuxuan Wang, Chongyang Tao, Chenshuo Wang, Dongyan Zhao |  |
| 444 |  |  [Partitioned Gradient Matching-based Data Subset Selection for Compute-Efficient Robust ASR Training](https://doi.org/10.18653/v1/2022.findings-emnlp.443) |  | 0 | Training state-of-the-art ASR systems such as RNN-T often has a high associated financial and environmental cost. Training with a subset of training data could mitigate this problem if the subset selected could achieve on-par performance with training with the entire dataset. Although there are... | Ashish R. Mittal, Durga Sivasubramanian, Rishabh K. Iyer, Preethi Jyothi, Ganesh Ramakrishnan |  |
| 445 |  |  [Adaptive Graph Convolutional Network for Knowledge Graph Entity Alignment](https://doi.org/10.18653/v1/2022.findings-emnlp.444) |  | 0 | Entity alignment (EA) aims to identify equivalent entities from different Knowledge Graphs (KGs), which is a fundamental task for integrating KGs. Throughout its development, Graph Convolutional Network (GCN) has become one of the mainstream methods for EA. These GCN-based methods learn the... | Renbo Zhu, Xukun Luo, Meng Ma, Ping Wang |  |
| 446 |  |  [Towards Robust NLG Bias Evaluation with Syntactically-diverse Prompts](https://doi.org/10.18653/v1/2022.findings-emnlp.445) |  | 0 | We present a robust methodology for evaluating biases in natural language generation(NLG) systems. Previous works use fixed hand-crafted prefix templates with mentions of various demographic groups to prompt models to generate continuations for bias analysis. These fixed prefix templates could... | Arshiya Aggarwal, Jiao Sun, Nanyun Peng |  |
| 447 |  |  [PcMSP: A Dataset for Scientific Action Graphs Extraction from Polycrystalline Materials Synthesis Procedure Text](https://doi.org/10.18653/v1/2022.findings-emnlp.446) |  | 0 | Scientific action graphs extraction from materials synthesis procedures is important for reproducible research, machine automation, and material prediction. But the lack of annotated data has hindered progress in this field. We demonstrate an effort to annotate Polycrystalline Materials Synthesis... | Xianjun Yang, Ya Zhuo, Julia Zuo, Xinlu Zhang, Stephen D. Wilson, Linda R. Petzold |  |
| 448 |  |  [Validity Assessment of Legal Will Statements as Natural Language Inference](https://doi.org/10.18653/v1/2022.findings-emnlp.447) |  | 0 | This work introduces a natural language inference (NLI) dataset that focuses on the validity of statements in legal wills. This dataset is unique because: (a) each entailment decision requires three inputs: the statement from the will, the law, and the conditions that hold at the time of the... | Alice Saebom Kwak, Jacob O. Israelsen, Clayton T. Morrison, Derek E. Bambauer, Mihai Surdeanu |  |
| 449 |  |  [AdaPrompt: Adaptive Model Training for Prompt-based NLP](https://doi.org/10.18653/v1/2022.findings-emnlp.448) |  | 0 | Prompt-based learning, with its capability to tackle zero-shot and few-shot NLP tasks, has gained much attention in the community.The main idea is to bridge the gap between NLP downstream tasks and language modeling (LM), by mapping these tasks into natural language prompts, which are then filled... | Yulong Chen, Yang Liu, Li Dong, Shuohang Wang, Chenguang Zhu, Michael Zeng, Yue Zhang |  |
| 450 |  |  [Code Generation From Flowcharts with Texts: A Benchmark Dataset and An Approach](https://doi.org/10.18653/v1/2022.findings-emnlp.449) |  | 0 | Currently, researchers focus on generating codes from the requirement documents. However, current approaches still perform poorly on some requirements needing complex problem-solving skills. In reality, to tackle such complex requirements, instead of directly translating requirement documents into... | Zejie Liu, Xiaoyu Hu, Deyu Zhou, Lin Li, Xu Zhang, Yanzheng Xiang |  |
| 451 |  |  [Focus! Relevant and Sufficient Context Selection for News Image Captioning](https://doi.org/10.18653/v1/2022.findings-emnlp.450) |  | 0 | News Image Captioning requires describing an image by leveraging additional context derived from a news article. Previous works only coarsely leverage the article to extract the necessary context, which makes it challenging for models to identify relevant events and named entities. In our paper, we... | Mingyang Zhou, Grace Luo, Anna Rohrbach, Zhou Yu |  |
| 452 |  |  [Generative Aspect-Based Sentiment Analysis with Contrastive Learning and Expressive Structure](https://doi.org/10.18653/v1/2022.findings-emnlp.451) |  | 0 | Generative models have demonstrated impressive results on Aspect-based Sentiment Analysis (ABSA) tasks, particularly for the emerging task of extracting Aspect-Category-Opinion-Sentiment (ACOS) quadruples. However, these models struggle with implicit sentiment expressions, which are commonly... | Joseph Peper, Lu Wang |  |
| 453 |  |  [Semantic Dependency Parsing with Edge GNNs](https://doi.org/10.18653/v1/2022.findings-emnlp.452) |  | 0 | Second-order neural parsers have obtained high accuracy in semantic dependency parsing. Inspired by the factor graph representation of second-order parsing, we propose edge graph neural networks (E-GNNs). In an E-GNN, each node corresponds to a dependency edge, and the neighbors are defined in... | Songlin Yang, Kewei Tu |  |
| 454 |  |  [Explore Unsupervised Structures in Pretrained Models for Relation Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.453) |  | 0 | Syntactic trees have been widely applied in relation extraction (RE). However, since parsing qualities are not stable on different text domains and a pre-defined grammar may not well fit the target relation schema, the introduction of syntactic structures sometimes fails to improve RE performances... | Xi Yang, Tao Ji, Yuanbin Wu |  |
| 455 |  |  [Identifying Human Strategies for Generating Word-Level Adversarial Examples](https://doi.org/10.18653/v1/2022.findings-emnlp.454) |  | 0 | Adversarial examples in NLP are receiving increasing research attention. One line of investigation is the generation of word-level adversarial examples against fine-tuned Transformer models that preserve naturalness and grammaticality. Previous work found that human- and machine-generated... | Maximilian Mozes, Bennett Kleinberg, Lewis D. Griffin |  |
| 456 |  |  [Refinement Matters: Textual Description Needs to be Refined for Zero-shot Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.455) |  | 0 | Zero-Shot Learning (ZSL) has shown great promise at the intersection of vision and language, and generative methods for ZSL are predominant owing to their efficiency. Moreover, textual description or attribute plays a critical role in transferring knowledge from the seen to unseen classes in ZSL.... | Chandan Gautam, Sethupathy Parameswaran, Vinay Verma, Suresh Sundaram, Savitha Ramasamy |  |
| 457 |  |  [SAT: Improving Semi-Supervised Text Classification with Simple Instance-Adaptive Self-Training](https://doi.org/10.18653/v1/2022.findings-emnlp.456) |  | 0 | Self-training methods have been explored in recent years and have exhibited great performance in improving semi-supervised learning. This work presents a simple instance-adaptive self-training method (SAT) for semi-supervised text classification. SAT first generates two augmented views for each... | Hui Chen, Wei Han, Soujanya Poria |  |
| 458 |  |  [Answer Quality Aware Aggregation for Extractive QA Crowdsourcing](https://doi.org/10.18653/v1/2022.findings-emnlp.457) |  | 0 | Quality control is essential for creating extractive question answering (EQA) datasets via crowdsourcing. Aggregation across answers, i.e. word spans within passages annotated, by different crowd workers is one major focus for ensuring its quality. However, crowd workers cannot reach a consensus on... | Peide Zhu, Zhen Wang, Claudia Hauff, Jie Yang, Avishek Anand |  |
| 459 |  |  [Search to Pass Messages for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.findings-emnlp.458) |  | 0 | Completing missing facts is a fundamental task for temporal knowledge graphs (TKGs).Recently, graph neural network (GNN) based methods, which can simultaneously explore topological and temporal information, have become the state-of-the-art (SOTA) to complete TKGs. However, these studies are based... | Zhen Wang, Haotong Du, Quanming Yao, Xuelong Li |  |
| 460 |  |  [Code Vulnerability Detection via Nearest Neighbor Mechanism](https://doi.org/10.18653/v1/2022.findings-emnlp.459) |  | 0 | Code vulnerability detection is a fundamental and challenging task in the software security field. Existing research works aim to learn semantic information from the source code by utilizing NLP technologies. However, in vulnerability detection tasks, some vulnerable samples are very similar to... | Qianjin Du, Xiaohui Kuang, Gang Zhao |  |
| 461 |  |  [Robust Question Answering against Distribution Shifts with Test-Time Adaption: An Empirical Study](https://doi.org/10.18653/v1/2022.findings-emnlp.460) |  | 0 | A deployed question answering (QA) model can easily fail when the test data has a distribution shift compared to the training data. Robustness tuning (RT) methods have been widely studied to enhance model robustness against distribution shifts before model deployment. However, can we improve a... | Hai Ye, Yuyang Ding, Juntao Li, Hwee Tou Ng |  |
| 462 |  |  [ParaMac: A General Unsupervised Paraphrase Generation Framework Leveraging Semantic Constraints and Diversifying Mechanisms](https://doi.org/10.18653/v1/2022.findings-emnlp.461) |  | 0 | Paraphrase generation reflects the ability to understand the meaning from the language surface form and rephrase it to other expressions. Recent paraphrase generation works have paid attention to unsupervised approaches based on Pre-trained Language Models (PLMs) to avoid heavy reliance on parallel... | Jinxin Liu, Jiaxin Shi, Ji Qi, Lei Hou, Juanzi Li, Qi Tian |  |
| 463 |  |  [Semi-supervised New Slot Discovery with Incremental Clustering](https://doi.org/10.18653/v1/2022.findings-emnlp.462) |  | 0 | Discovering new slots is critical to the success of dialogue systems. Most existing methods rely on automatic slot induction in unsupervised fashion or perform domain adaptation across zero or few-shot scenarios. They have difficulties in providing high-quality supervised signals to learn... | Yuxia Wu, Lizi Liao, Xueming Qian, TatSeng Chua |  |
| 464 |  |  [Con-NAT: Contrastive Non-autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.463) |  | 0 | Inspired by the success of contrastive learning in natural language processing, we incorporate contrastive learning into the conditional masked language model which is extensively used in non-autoregressive neural machine translation (NAT). Accordingly, we propose a Contrastive Non-autoregressive... | Hao Cheng, Zhihua Zhang |  |
| 465 |  |  [Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection](https://doi.org/10.18653/v1/2022.findings-emnlp.464) |  | 0 | Knowledge distillation addresses the problem of transferring knowledge from a teacher model to a student model.In this process, we typically have multiple types of knowledge extracted from the teacher model.The problem is to make full use of them to train the student model.Our preliminary study... | Chenglong Wang, Yi Lu, Yongyu Mu, Yimin Hu, Tong Xiao, Jingbo Zhu |  |
| 466 |  |  [Syntactically Robust Training on Partially-Observed Data for Open Information Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.465) |  | 0 | Open Information Extraction models have shown promising results with sufficient supervision. However, these models face a fundamental challenge that the syntactic distribution of training data is partially observable in comparison to the real world. In this paper, we propose a syntactically robust... | Ji Qi, Yuxiang Chen, Lei Hou, Juanzi Li, Bin Xu |  |
| 467 |  |  [A Benchmark and Dataset for Post-OCR text correction in Sanskrit](https://doi.org/10.18653/v1/2022.findings-emnlp.466) |  | 0 | Sanskrit is a classical language with about 30 million extant manuscripts fit for digitisation, available in written, printed or scanned-image forms. However, it is still considered to be a low-resource language when it comes to available digital resources. In this work, we release a post-OCR text... | Ayush Maheshwari, Nikhil Singh, Amrith Krishna, Ganesh Ramakrishnan |  |
| 468 |  |  [Knowledge-Enhanced Self-Supervised Prototypical Network for Few-Shot Event Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.467) |  | 0 | Prototypical network based joint methods have attracted much attention in few-shot event detection, which carry out event detection in a unified sequence tagging framework. However, these methods suffer from the inaccurate prototype representation problem, due to two main reasons: the number of... | Kailin Zhao, Xiaolong Jin, Long Bai, Jiafeng Guo, Xueqi Cheng |  |
| 469 |  |  [VarMAE: Pre-training of Variational Masked Autoencoder for Domain-adaptive Language Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.468) |  | 0 | Pre-trained language models have been widely applied to standard benchmarks. Due to the flexibility of natural language, the available resources in a certain domain can be restricted to support obtaining precise representation. To address this issue, we propose a novel Transformer-based language... | Dou Hu, Xiaolong Hou, Xiyang Du, Mengyuan Zhou, Lianxin Jiang, Yang Mo, Xiaofeng Shi |  |
| 470 |  |  [Exploring Methods for Building Dialects-Mandarin Code-Mixing Corpora: A Case Study in Taiwanese Hokkien](https://doi.org/10.18653/v1/2022.findings-emnlp.469) |  | 0 | In natural language processing (NLP), code-mixing (CM) is a challenging task, especially when the mixed languages include dialects. In Southeast Asian countries such as Singapore, Indonesia, and Malaysia, Hokkien-Mandarin is the most widespread code-mixed language pair among Chinese immigrants, and... | SinEn Lu, BoHan Lu, ChaoYi Lu, Richard TzongHan Tsai |  |
| 471 |  |  [Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in Transformer-Based Variational AutoEncoder for Diverse Text Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.470) |  | 0 | Variational Auto-Encoder (VAE) has been widely adopted in text generation. Among many variants, recurrent VAE learns token-wise latent variables with each conditioned on the preceding ones, which captures sequential variability better in the era of RNN. However, it is unclear how to incorporate... | Jinyi Hu, Xiaoyuan Yi, Wenhao Li, Maosong Sun, Xing Xie |  |
| 472 |  |  [Tweet Based Reach Aware Temporal Attention Network for NFT Valuation](https://doi.org/10.18653/v1/2022.findings-emnlp.471) |  | 0 | Non-Fungible Tokens (NFTs) are a relatively unexplored class of assets. Designing strategies to forecast NFT trends is an intricate task due to its extremely volatile nature. The market is largely driven by public sentiment and “hype”, which in turn has a high correlation with conversations taking... | Ramit Sawhney, Megh Thakkar, Ritesh Soun, Atula Tejaswi Neerkaje, Vasu Sharma, Dipanwita Guhathakurta, Sudheer Chava |  |
| 473 |  |  [Entity Embedding Completion for Wide-Coverage Entity Disambiguation](https://doi.org/10.18653/v1/2022.findings-emnlp.472) |  | 0 | Entity disambiguation (ED) is typically solved by learning to classify a given mention into one of the entities in the model’s entity vocabulary by referring to their embeddings. However, this approach cannot address mentions of entities that are not covered by the entity vocabulary. Aiming to... | Daisuke Oba, Ikuya Yamada, Naoki Yoshinaga, Masashi Toyoda |  |
| 474 |  |  [Entity-level Interaction via Heterogeneous Graph for Multimodal Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.473) |  | 0 | Multimodal Named Entity Recognition (MNER) faces two specific challenges: 1) How to capture useful entity-related visual information. 2) How to alleviate the interference of visual noise. Previous works have gained progress by improving interacting mechanisms or seeking for better visual features.... | Gang Zhao, Guanting Dong, Yidong Shi, Haolong Yan, Weiran Xu, Si Li |  |
| 475 |  |  [Status Biases in Deliberation Online: Evidence from a Randomized Experiment on ChangeMyView](https://doi.org/10.18653/v1/2022.findings-emnlp.474) |  | 0 | Status is widely used to incentivize user engagement online. However, visible status indicators could inadvertently bias online deliberation to favor high-status users. In this work, we design and deploy a randomized experiment on the ChangeMyView platform to quantify status biases in deliberation... | Emaad Manzoor, Yohan Jo, Alan Montgomery |  |
| 476 |  |  [Empathetic and Emotionally Positive Conversation Systems with an Emotion-specific Query-Response Memory](https://doi.org/10.18653/v1/2022.findings-emnlp.475) |  | 0 | Emotional conversation systems generate responses for the input queries considering the speaker’s emotions in a conversation. Existing emotional conversation systems output emotional responses according to either a given emotion or the user’s emotion reflected in the input queries. Following a... | Zhiliang Tian, Yinliang Wang, Yiping Song, Chi Zhang, Dongkyu Lee, Yingxiu Zhao, Dongsheng Li, Nevin L. Zhang |  |
| 477 |  |  [Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision](https://doi.org/10.18653/v1/2022.findings-emnlp.476) |  | 0 | Clinical trials are essential for drug development but are extremely expensive and time-consuming to conduct. It is beneficial to study similar historical trials when designing a clinical trial. However, lengthy trial documents and lack of labeled data make trial similarity search difficult. We... | Zifeng Wang, Jimeng Sun |  |
| 478 |  |  [From Mimicking to Integrating: Knowledge Integration for Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.477) |  | 0 | Investigating better ways to reuse the released pre-trained language models (PLMs) can significantly reduce the computational cost and the potential environmental side-effects. This paper explores a novel PLM reuse paradigm, Knowledge Integration (KI). Without human annotations available, KI aims... | Lei Li, Yankai Lin, Xuancheng Ren, Guangxiang Zhao, Peng Li, Jie Zhou, Xu Sun |  |
| 479 |  |  [Model and Data Transfer for Cross-Lingual Sequence Labelling in Zero-Resource Settings](https://doi.org/10.18653/v1/2022.findings-emnlp.478) |  | 0 | Zero-resource cross-lingual transfer approaches aim to apply supervised modelsfrom a source language to unlabelled target languages. In this paper we performan in-depth study of the two main techniques employed so far for cross-lingualzero-resource sequence labelling, based either on data or model... | Iker GarcíaFerrero, Rodrigo Agerri, German Rigau |  |
| 480 |  |  [Early Guessing for Dialect Identification](https://doi.org/10.18653/v1/2022.findings-emnlp.479) |  | 0 | This paper deals with the problem of incre-mental dialect identification. Our goal is toreliably determine the dialect before the fullutterance is given as input. The major partof the previous research on dialect identification has been model-centric, focusing on performance. We address a new... | Vani Kanjirangat, Tanja Samardzic, Fabio Rinaldi, Ljiljana Dolamic |  |
| 481 |  |  [R-AT: Regularized Adversarial Training for Natural Language Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.480) |  | 0 | Currently, adversarial training has become a popular and powerful regularization method in the natural language domain. In this paper, we Regularized Adversarial Training (R-AT) via dropout, which forces the output probability distributions of different sub-models generated by dropout to be... | Shiwen Ni, Jiawen Li, HungYu Kao |  |
| 482 |  |  [Multi-View Active Learning for Short Text Classification in User-Generated Data](https://doi.org/10.18653/v1/2022.findings-emnlp.481) |  | 0 | Mining user-generated data often suffers from the lack of enough labeled data, short document lengths, and the informal user language. In this paper, we propose a novel active learning model to overcome these obstacles in the tasks tailored for query phrases–e.g., detecting positive reports of... | Payam Karisani, Negin Karisani, Li Xiong |  |
| 483 |  |  [Forging Multiple Training Objectives for Pre-trained Language Models via Meta-Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.482) |  | 0 | Multiple pre-training objectives fill the vacancy of the understanding capability of single-objective language modeling, which serves the ultimate purpose of pre-trained language models (PrLMs), generalizing well on a mass of scenarios. However, learning multiple training objectives in a single... | Hongqiu Wu, Ruixue Ding, Hai Zhao, Boli Chen, Pengjun Xie, Fei Huang, Min Zhang |  |
| 484 |  |  [ConGen: Unsupervised Control and Generalization Distillation For Sentence Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.483) |  | 0 | Sentence representations are essential in many NLP tasks operating at the sentence level.Recently, research attention has shifted towards learning how to represent sentences without any annotations, i.e., unsupervised representation learning. Despite the benefit of training without supervised data,... | Peerat Limkonchotiwat, Wuttikorn Ponwitayarat, Lalita Lowphansirikul, Ekapol Chuangsuwanich, Sarana Nutanong |  |
| 485 |  |  [Large-Scale Differentially Private BERT](https://doi.org/10.18653/v1/2022.findings-emnlp.484) |  | 0 | In this work, we study the large-scale pretraining of BERT-Large (Devlin et al., 2019) with differentially private SGD (DP-SGD). We show that combined with a careful implementation, scaling up the batch size to millions (i.e., mega-batches) improves the utility of the DP-SGD step for BERT; we also... | Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, Pasin Manurangsi |  |
| 486 |  |  [Improving Zero-Shot Multilingual Translation with Universal Representations and Cross-Mapping](https://doi.org/10.18653/v1/2022.findings-emnlp.485) |  | 0 | The many-to-many multilingual neural machine translation can translate between language pairs unseen during training, i.e., zero-shot translation. Improving zero-shot translation requires the model to learn universal representations and cross-mapping relationships to transfer the knowledge learned... | Shuhao Gu, Yang Feng |  |
| 487 |  |  [Controllable Fake Document Infilling for Cyber Deception](https://doi.org/10.18653/v1/2022.findings-emnlp.486) |  | 0 | Recent works in cyber deception study how to deter malicious intrusion by generating multiple fake versions of a critical document to impose costs on adversaries who need to identify the correct information. However, existing approaches are context-agnostic, resulting in sub-optimal and unvaried... | Yibo Hu, Yu Lin, Erick Skorupa Parolin, Latifur Khan, Kevin W. Hamlen |  |
| 488 |  |  [Weakly Supervised Headline Dependency Parsing](https://doi.org/10.18653/v1/2022.findings-emnlp.487) |  | 0 | English news headlines form a register with unique syntactic properties that have been documented in linguistics literature since the 1930s. However, headlines have received surprisingly little attention from the NLP syntactic parsing community. We aim to bridge this gap by providing the first news... | Adrian Benton, Tianze Shi, Ozan Irsoy, Igor Malioutov |  |
| 489 |  |  [BOOKSUM: A Collection of Datasets for Long-form Narrative Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.488) |  | 0 | The majority of existing text summarization datasets include short-form source documents that lack long-range causal and temporal dependencies, and often contain strong layout and stylistic biases. While relevant, such datasets will offer limited challenges for future text summarization systems. We... | Wojciech Kryscinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, Dragomir Radev |  |
| 490 |  |  [Not All Errors are Equal: Learning Text Generation Metrics using Stratified Error Synthesis](https://doi.org/10.18653/v1/2022.findings-emnlp.489) |  | 0 | Is it possible to build a general and automatic natural language generation (NLG) evaluation metric? Existing learned metrics either perform unsatisfactorily or are restricted to tasks where large human rating data is already available. We introduce SESCORE, a model-based metric that is highly... | Wenda Xu, YiLin Tuan, Yujie Lu, Michael Saxon, Lei Li, William Yang Wang |  |
| 491 |  |  [Summarization as Indirect Supervision for Relation Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.490) |  | 0 | Relation extraction (RE) models have been challenged by their reliance on training data with expensive annotations. Considering that summarization tasks aim at acquiring concise expressions of synoptical information from the longer context, these tasks naturally align with the objective of RE,... | Keming Lu, IHung Hsu, Wenxuan Zhou, Mingyu Derek Ma, Muhao Chen |  |
| 492 |  |  [DIGAT: Modeling News Recommendation with Dual-Graph Interaction](https://doi.org/10.18653/v1/2022.findings-emnlp.491) |  | 0 | News recommendation (NR) is essential for online news services. Existing NR methods typically adopt a news-user representation learning framework, facing two potential limitations. First, in news encoder, single candidate news encoding suffers from an insufficient semantic information problem.... | Zhiming Mao, Jian Li, Hongru Wang, Xingshan Zeng, KamFai Wong |  |
| 493 |  |  [SMASH: Improving SMAll Language Models' Few-SHot Ability with Prompt-Based Distillation](https://doi.org/10.18653/v1/2022.findings-emnlp.492) |  | 0 | Large-scale language models coupled with prompts have shown remarkable performance on few-shot learning. However, through systematic experiments, we find that the few-shot performance of small language models is poor, and using prompts on them brings fewer improvements than on larger ones. In this... | Yueqian Wang, Chang Liu, Kai Chen, Xi Wang, Dongyan Zhao |  |
| 494 |  |  [Consecutive Question Generation via Dynamic Multitask Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.493) |  | 0 | In this paper, we propose the task of consecutive question generation (CQG), which generates a set of logically related question-answer pairs to understand a whole passage, with a comprehensive consideration of the aspects including accuracy, coverage, and informativeness.To achieve this, we first... | Yunji Li, Sujian Li, Xing Shi |  |
| 495 |  |  [Subword Segmental Language Modelling for Nguni Languages](https://doi.org/10.18653/v1/2022.findings-emnlp.494) |  | 0 | Subwords have become the standard units of text in NLP, enabling efficient open-vocabulary models. With algorithms like byte-pair encoding (BPE), subword segmentation is viewed as a preprocessing step applied to the corpus before training. This can lead to sub-optimal segmentations for low-resource... | Francois Meyer, Jan Buys |  |
| 496 |  |  [Towards Robust Visual Question Answering: Making the Most of Biased Samples via Contrastive Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.495) |  | 0 | Models for Visual Question Answering (VQA) often rely on the spurious correlations, i.e., the language priors, that appear in the biased samples of training set, which make them brittle against the out-of-distribution (OOD) test data. Recent methods have achieved promising progress in overcoming... | Qingyi Si, Yuanxin Liu, Fandong Meng, Zheng Lin, Peng Fu, Yanan Cao, Weiping Wang, Jie Zhou |  |
| 497 |  |  [P3LM: Probabilistically Permuted Prophet Language Modeling for Generative Pre-Training](https://doi.org/10.18653/v1/2022.findings-emnlp.496) |  | 0 | Conventional autoregressive left-to-right (L2R) sequence generation faces two issues during decoding: limited to unidirectional target sequence modeling, and constrained on strong local dependencies.To address the aforementioned problem, we propose P3LM, a probabilistically permuted prophet... | Junwei Bao, Yifan Wang, Ying Jiangyong, Yeyun Gong, Jing Zhao, Youzheng Wu, Xiaodong He |  |
| 498 |  |  [Holistic Sentence Embeddings for Better Out-of-Distribution Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.497) |  | 0 | Detecting out-of-distribution (OOD) instances is significant for the safe deployment of NLP models. Among recent textual OOD detection works based on pretrained language models (PLMs), distance-based methods have shown superior performance. However, they estimate sample distance scores in the... | Sishuo Chen, Xiaohan Bi, Rundong Gao, Xu Sun |  |
| 499 |  |  [MuGER2: Multi-Granularity Evidence Retrieval and Reasoning for Hybrid Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.498) |  | 0 | Hybrid question answering (HQA) aims to answer questions over heterogeneous data, including tables and passages linked to table cells. The heterogeneous data can provide different granularity evidence to HQA models, e.t., column, row, cell, and link. Conventional HQA models usually retrieve coarse-... | Yingyao Wang, Junwei Bao, Chaoqun Duan, Youzheng Wu, Xiaodong He, Tiejun Zhao |  |
| 500 |  |  [EntityCS: Improving Zero-Shot Cross-lingual Transfer with Entity-Centric Code Switching](https://doi.org/10.18653/v1/2022.findings-emnlp.499) |  | 0 | Accurate alignment between languages is fundamental for improving cross-lingual pre-trained language models (XLMs). Motivated by the natural phenomenon of code-switching (CS) in multilingual speakers, CS has been used as an effective data augmentation method that offers language alignment at word-... | Chenxi Whitehouse, Fenia Christopoulou, Ignacio Iacobacci |  |
| 501 |  |  [MBTI Personality Prediction for Fictional Characters Using Movie Scripts](https://doi.org/10.18653/v1/2022.findings-emnlp.500) |  | 0 | An NLP model that understands stories should be able to understand the characters in them. To support the development of neural models for this purpose, we construct a benchmark, Story2Personality. The task is to predict a movie character’s MBTI or Big 5 personality types based on the narratives of... | Yisi Sang, Xiangyang Mou, Mo Yu, Dakuo Wang, Jing Li, Jeffrey M. Stanton |  |
| 502 |  |  [A Simple and Strong Baseline for End-to-End Neural RST-style Discourse Parsing](https://doi.org/10.18653/v1/2022.findings-emnlp.501) |  | 0 | To promote and further develop RST-style discourse parsing models, we need a strong baseline that can be regarded as a reference for reporting reliable experimental results. This paper explores a strong baseline by integrating existing simple parsing strategies, top-down and bottom-up, with various... | Naoki Kobayashi, Tsutomu Hirao, Hidetaka Kamigaito, Manabu Okumura, Masaaki Nagata |  |
| 503 |  |  [Probing for Constituency Structure in Neural Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.502) |  | 0 | In this paper, we investigate to which extent contextual neural language models (LMs) implicitly learn syntactic structure. More concretely, we focus on constituent structure as represented in the Penn Treebank (PTB). Using standard probing techniques based on diagnostic classifiers, we assess the... | David Arps, Younes Samih, Laura Kallmeyer, Hassan Sajjad |  |
| 504 |  |  [Table-To-Text generation and pre-training with TabT5](https://doi.org/10.18653/v1/2022.findings-emnlp.503) |  | 0 | Encoder-only transformer models have been successfully applied to different table understanding tasks, as in TAPAS. A major limitation of these architectures is that they are constrained to classification-like tasks such as cell selection or entailment detection. We present TabT5, an... | Ewa Andrejczuk, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Yasemin Altun |  |
| 505 |  |  [A POMDP Dialogue Policy with 3-way Grounding and Adaptive Sensing for Learning through Communication](https://doi.org/10.18653/v1/2022.findings-emnlp.504) |  | 0 | Agents to assist with rescue, surgery, and similar activities could collaborate better with humans if they could learn new strategic behaviors through communication. We introduce a novel POMDP dialogue policy for learning from people. The policy has 3-way grounding of language in the shared... | Maryam Zare, Alan R. Wagner, Rebecca J. Passonneau |  |
| 506 |  |  [PaCo: Preconditions Attributed to Commonsense Knowledge](https://doi.org/10.18653/v1/2022.findings-emnlp.505) |  | 0 | Humans can seamlessly reason with circumstantial preconditions of commonsense knowledge. We understand that a glass is used for drinking water, unless the glass is broken or the water is toxic. Despite state-of-the-art (SOTA) language models’ (LMs) impressive performance on inferring commonsense... | Ehsan Qasemi, Filip Ilievski, Muhao Chen, Pedro A. Szekely |  |
| 507 |  |  [Improving Few-Shot Domain Transfer for Named Entity Disambiguation with Pattern Exploitation](https://doi.org/10.18653/v1/2022.findings-emnlp.506) |  | 0 | Named entity disambiguation (NED) is a critical subtask of entity linking, which seeks to connect knowledge base entities with textual mentions of those entities. Naturally, the performance of a model depends on the domain it was trained on; thus, reducing the amount of data required to train... | Philip Blair, Kfir Bar |  |
| 508 |  |  [Capturing Topic Framing via Masked Language Modeling](https://doi.org/10.18653/v1/2022.findings-emnlp.507) |  | 0 | Differential framing of issues can lead to divergent world views on important issues. This is especially true in domains where the information presented can reach a large audience, such as traditional and social media. Scalable and reliable measurement of such differential framing is an important... | Xiaobo Guo, Weicheng Ma, Soroush Vosoughi |  |
| 509 |  |  [WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation](https://doi.org/10.18653/v1/2022.findings-emnlp.508) |  | 0 | A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel approach for dataset creation based on worker and AI collaboration, which brings together the... | Alisa Liu, Swabha Swayamdipta, Noah A. Smith, Yejin Choi |  |
| 510 |  |  [Sequentially Controlled Text Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.509) |  | 0 | While GPT-2 generates sentences that are remarkably human-like, longer documents can ramble and do not follow human-like writing structure. We study the problem of imposing structure on long-range text. We propose a novel controlled text generation task, sequentially controlled text generation, and... | Alexander Spangher, Yao Ming, Xinyu Hua, Nanyun Peng |  |
| 511 |  |  [Revisiting the Roles of "Text" in Text Games](https://doi.org/10.18653/v1/2022.findings-emnlp.510) |  | 0 | Text games present opportunities for natural language understanding (NLU) methods to tackle reinforcement learning (RL) challenges. However, recent work has questioned the necessity of NLU by showing random text hashes could perform decently. In this paper, we pursue a fine-grained investigation... | Yi Gu, Shunyu Yao, Chuang Gan, Josh Tenenbaum, Mo Yu |  |
| 512 |  |  [FPT: Improving Prompt Tuning Efficiency via Progressive Training](https://doi.org/10.18653/v1/2022.findings-emnlp.511) |  | 0 | Recently, prompt tuning (PT) has gained increasing attention as a parameter-efficient way of tuning pre-trained language models (PLMs). Despite extensively reducing the number of tunable parameters and achieving satisfying performance, PT is training-inefficient due to its slow convergence. To... | Yufei Huang, Yujia Qin, Huadong Wang, Yichun Yin, Maosong Sun, Zhiyuan Liu, Qun Liu |  |
| 513 |  |  [Prompt-learning for Fine-grained Entity Typing](https://doi.org/10.18653/v1/2022.findings-emnlp.512) |  | 0 | As an effective approach to adapting pre-trained language models (PLMs) for specific tasks, prompt-learning has recently attracted much attention from researchers. By using cloze-style language prompts to stimulate the versatile knowledge of PLMs, prompt-learning can achieve promising results on a... | Ning Ding, Yulin Chen, Xu Han, Guangwei Xu, Xiaobin Wang, Pengjun Xie, Haitao Zheng, Zhiyuan Liu, Juanzi Li, HongGee Kim |  |
| 514 |  |  [TransLIST: A Transformer-Based Linguistically Informed Sanskrit Tokenizer](https://doi.org/10.18653/v1/2022.findings-emnlp.513) |  | 0 | Sanskrit Word Segmentation (SWS) is essential in making digitized texts available and in deploying downstream tasks. It is, however, non-trivial because of the sandhi phenomenon that modifies the characters at the word boundaries, and needs special treatment. Existing lexicon driven approaches for... | Jivnesh Sandhan, Rathin Singha, Narein Rao, Suvendu Samanta, Laxmidhar Behera, Pawan Goyal |  |
| 515 |  |  [Fair NLP Models with Differentially Private Text Encoders](https://doi.org/10.18653/v1/2022.findings-emnlp.514) |  | 0 | Encoded text representations often capture sensitive attributes about individuals (e.g., race or gender), which raise privacy concerns and can make downstream models unfair to certain groups. In this work, we propose FEDERATE, an approach that combines ideas from differential privacy and... | Gaurav Maheshwari, Pascal Denis, Mikaela Keller, Aurélien Bellet |  |
| 516 |  |  [Modeling Context With Linear Attention for Scalable Document-Level Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.515) |  | 0 | Document-level machine translation leverages inter-sentence dependencies to produce more coherent and consistent translations. However, these models, predominantly based on transformers, are difficult to scale to long documents as their attention layers have quadratic complexity in the sequence... | Zhaofeng Wu, Hao Peng, Nikolaos Pappas, Noah A. Smith |  |
| 517 |  |  [What do Large Language Models Learn beyond Language?](https://doi.org/10.18653/v1/2022.findings-emnlp.516) |  | 0 | Large language models (LMs) have rapidly become a mainstay in Natural Language Processing. These models are known to acquire rich linguistic knowledge from training on large amounts of text. In this paper, we investigate if pre-training on text also confers these models with helpful ‘inductive... | Avinash Madasu, Shashank Srivastava |  |
| 518 |  |  [CONSISTENT: Open-Ended Question Generation From News Articles](https://doi.org/10.18653/v1/2022.findings-emnlp.517) |  | 0 | Recent work on question generation has largely focused on factoid questions such as who, what,where, when about basic facts. Generating open-ended why, how, what, etc. questions thatrequire long-form answers have proven more difficult. To facilitate the generation of openended questions, we propose... | Tuhin Chakrabarty, Justin Lewis, Smaranda Muresan |  |
| 519 |  |  [Efficient (Soft) Q-Learning for Text Generation with Limited Good Data](https://doi.org/10.18653/v1/2022.findings-emnlp.518) |  | 0 | Maximum likelihood estimation (MLE) is the predominant algorithm for training text generation models. This paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models.... | Han Guo, Bowen Tan, Zhengzhong Liu, Eric P. Xing, Zhiting Hu |  |
| 520 |  |  [Lexi: Self-Supervised Learning of the UI Language](https://doi.org/10.18653/v1/2022.findings-emnlp.519) |  | 0 | Humans can learn to operate the user interface (UI) of an application by reading an instruction manual or how-to guide. Along with text, these resources include visual content such as UI screenshots and images of application icons referenced in the text. We explore how to leverage this data to... | Pratyay Banerjee, Shweti Mahajan, Kushal Arora, Chitta Baral, Oriana Riva |  |
| 521 |  |  [Inferring the Reader: Guiding Automated Story Generation with Commonsense Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.520) |  | 0 | Transformer-based language model approaches to automated story generation currently provide state-of-the-art results. However, they still suffer from plot incoherence when generatingnarratives over time, and critically lack basiccommonsense reasoning. Furthermore, existing methods generally focus... | Xiangyu Peng, Siyan Li, Sarah Wiegreffe, Mark O. Riedl |  |
| 522 |  |  [How to Stop an Avalanche? JoDeM: Joint Decision Making through Compare and Contrast for Dialog State Tracking](https://doi.org/10.18653/v1/2022.findings-emnlp.521) |  | 0 | Dialog state tracking (DST) is a core component in task-oriented dialog systems. Existing state-of-the-art DST model incorporates insight and intuition from the human experience into design of supplementary labels, which greatly assisted the training process of turn-by-turn DST model. Though the... | Haoming Wang, Wang Xin |  |
| 523 |  |  [Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding](https://doi.org/10.18653/v1/2022.findings-emnlp.522) |  | 0 | Contrastive learning has become a new paradigm for unsupervised sentence embeddings.Previous studies focus on instance-wise contrastive learning, attempting to construct positive pairs with textual data augmentation. In this paper, we propose a novel Contrastive learning method with Prompt-derived... | Jiali Zeng, Yongjing Yin, Yufan Jiang, Shuangzhi Wu, Yunbo Cao |  |
| 524 |  |  [Weight Perturbation as Defense against Adversarial Word Substitutions](https://doi.org/10.18653/v1/2022.findings-emnlp.523) |  | 0 | The existence and pervasiveness of textual adversarial examples have raised serious concerns to security-critical applications. Many methods have been developed to defend against adversarial attacks for neural natural language processing (NLP) models.Adversarial training is one of the most... | Jianhan Xu, Linyang Li, Jiping Zhang, Xiaoqing Zheng, KaiWei Chang, ChoJui Hsieh, Xuanjing Huang |  |
| 525 |  |  [CORT: A New Baseline for Comparative Opinion Classification by Dual Prompts](https://doi.org/10.18653/v1/2022.findings-emnlp.524) |  | 0 | Comparative opinion is a common linguistic phenomenon. The opinion is expressed by comparing multiple targets on a shared aspect, e.g., “camera A is better than camera B in picture quality”. Among the various subtasks in opinion mining, comparative opinion classification is relatively less studied.... | Yequan Wang, Hengran Zhang, Aixin Sun, Xuying Meng |  |
| 526 |  |  [APEACH: Attacking Pejorative Expressions with Analysis on Crowd-Generated Hate Speech Evaluation Datasets](https://doi.org/10.18653/v1/2022.findings-emnlp.525) |  | 0 | In hate speech detection, developing training and evaluation datasets across various domains is the critical issue. Whereas, major approaches crawl social media texts and hire crowd-workers to annotate the data. Following this convention often restricts the scope of pejorative expressions to a... | Kichang Yang, Wonjun Jang, WonIk Cho |  |
| 527 |  |  [Guiding Neural Story Generation with Reader Models](https://doi.org/10.18653/v1/2022.findings-emnlp.526) |  | 0 | Automated storytelling has long captured the attention of researchers for the ubiquity of narratives in everyday life. However, it is challenging to maintain coherence and stay on-topictoward a specific ending when generating narratives with neural language models. In this paper, we introduce Story... | Xiangyu Peng, Kaige Xie, Amal Alabdulkarim, Harshith Kayam, Samihan Dani, Mark O. Riedl |  |
| 528 |  |  [Reason first, then respond: Modular Generation for Knowledge-infused Dialogue](https://doi.org/10.18653/v1/2022.findings-emnlp.527) |  | 0 | Large language models can produce fluent dialogue but often hallucinate factual inaccuracies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simultaneously. In this work, we... | Leonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, Jason Weston |  |
| 529 |  |  [Adapting Multilingual Models for Code-Mixed Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.528) |  | 0 | The scarcity of gold standard code-mixed to pure language parallel data makes it difficult to train translation models reliably.Prior work has addressed the paucity of parallel data with data augmentation techniques.Such methods rely heavily on external resources making systems difficult to train... | Aditya Vavre, Abhirut Gupta, Sunita Sarawagi |  |
| 530 |  |  [LPC: A Logits and Parameter Calibration Framework for Continual Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.529) |  | 0 | When we execute the typical fine-tuning paradigm on continuously sequential tasks, the model will suffer from the catastrophic forgetting problem (i.e., the model tends to adjust old parameters according to the new knowledge, which leads to the loss of previously acquired concepts). People proposed... | Xiaodi Li, Zhuoyi Wang, Dingcheng Li, Latifur Khan, Bhavani Thuraisingham |  |
| 531 |  |  [SlovakBERT: Slovak Masked Language Model](https://doi.org/10.18653/v1/2022.findings-emnlp.530) |  | 0 | We introduce a new Slovak masked language model called SlovakBERT. This is to our best knowledge the first paper discussing Slovak transformers-based language models. We evaluate our model on several NLP tasks and achieve state-of-the-art results. This evaluation is likewise the first attempt to... | Matús Pikuliak, Stefan Grivalsky, Martin Konopka, Miroslav Blsták, Martin Tamajka, Viktor Bachratý, Marián Simko, Pavol Balázik, Michal Trnka, Filip Uhlárik |  |
| 532 |  |  [Efficient Zero-shot Event Extraction with Context-Definition Alignment](https://doi.org/10.18653/v1/2022.findings-emnlp.531) |  | 0 | Event extraction (EE) is the task of identifying interested event mentions from text.Conventional efforts mainly focus on the supervised setting. However, these supervised models cannot generalize to event types out of the pre-defined ontology. To fill this gap, many efforts have been devoted to... | Hongming Zhang, Wenlin Yao, Dong Yu |  |
| 533 |  |  [Logical Fallacy Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.532) |  | 0 | Reasoning is central to human intelligence. However, fallacious arguments are common, and some exacerbate problems such as spreading misinformation about climate change. In this paper, we propose the task of logical fallacy detection, and provide a new dataset (Logic) of logical fallacies generally... | Zhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, Zhiheng Lyu, Mrinmaya Sachan, Rada Mihalcea, Bernhard Schölkopf |  |
| 534 |  |  [Topic-Aware Response Generation in Task-Oriented Dialogue with Unstructured Knowledge Access](https://doi.org/10.18653/v1/2022.findings-emnlp.533) |  | 0 | To alleviate the problem of structured databases’ limited coverage, recent task-oriented dialogue systems incorporate external unstructured knowledge to guide the generation of system responses. However, these usually use word or sentence level similarities to detect the relevant knowledge context,... | Yue Feng, Gerasimos Lampouras, Ignacio Iacobacci |  |
| 535 |  |  [Revisiting Transformer-based Models for Long Document Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.534) |  | 0 | The recent literature in text classification is biased towards short text sequences (e.g., sentences or paragraphs). In real-world applications, multi-page multi-paragraph documents are common and they cannot be efficiently encoded by vanilla Transformer-based models. We compare different... | Xiang Dai, Ilias Chalkidis, Sune Darkner, Desmond Elliott |  |
| 536 |  |  [Time-aware Prompting for Text Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.535) |  | 0 | In this paper, we study the effects of incorporating timestamps, such as document creation dates, into generation systems. Two types of time-aware prompts are investigated: (1) textual prompts that encode document timestamps in natural language sentences; and (2) linear prompts that convert... | Shuyang Cao, Lu Wang |  |
| 537 |  |  [Improving Scheduled Sampling with Elastic Weight Consolidation for Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.536) |  | 0 | Despite strong performance in many sequence-to-sequence tasks, autoregressive models trained with maximum likelihood estimation suffer from exposure bias, i.e. the discrepancy between the ground-truth prefixes used during training and the model-generated prefixes used at inference time. Scheduled... | Michalis Korakakis, Andreas Vlachos |  |
| 538 |  |  [Ensemble Transformer for Efficient and Accurate Ranking Tasks: an Application to Question Answering Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.537) |  | 0 | Large transformer models can highly improve Answer Sentence Selection (AS2) tasks, but their high computational costs prevent their use in many real-world applications. In this paper, we explore the following research question: How can we make the AS2 models more accurate without significantly... | Yoshitomo Matsubara, Luca Soldaini, Eric Lind, Alessandro Moschitti |  |
| 539 |  |  [Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis](https://doi.org/10.18653/v1/2022.findings-emnlp.538) |  | 0 | Pre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration... | Yuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, LouisPhilippe Morency |  |
| 540 |  |  [How to Represent Context Better? An Empirical Study on Context Modeling for Multi-turn Response Selection](https://doi.org/10.18653/v1/2022.findings-emnlp.539) |  | 0 | Building retrieval-based dialogue models that can predict appropriate responses based on the understanding of multi-turn context messages is a challenging problem. Early models usually concatenate all utterances or independently encode each dialogue turn, which may lead to an inadequate... | Jiazhan Feng, Chongyang Tao, Chang Liu, Rui Yan, Dongyan Zhao |  |
| 541 |  |  [CHIA: CHoosing Instances to Annotate for Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.540) |  | 0 | Neural machine translation (MT) systems have been shown to perform poorly on low-resource language pairs, for which large-scale parallel data is unavailable. Making the data annotation process faster and cheaper is therefore important to ensure equitable access to MT systems. To make optimal use of... | Rajat Bhatnagar, Ananya Ganesh, Katharina Kann |  |
| 542 |  |  [Guiding Neural Machine Translation with Semantic Kernels](https://doi.org/10.18653/v1/2022.findings-emnlp.541) |  | 0 | Machine Translation task has made great progress with the help of auto-regressive decoding paradigm and Transformer architecture. In this paradigm, though the encoder can obtain global source representations, the decoder can only use translation history to determine the current word. Previous... | Ping Guo, Yue Hu, Xiangpeng Wei, Yubing Ren, Yunpeng Li, Luxi Xing, Yuqiang Xie |  |
| 543 |  |  [HiSMatch: Historical Structure Matching based Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.542) |  | 0 | A Temporal Knowledge Graph (TKG) is a sequence of KGs with respective timestamps, which adopts quadruples in the form of (subject, relation, object, timestamp) to describe dynamic facts. TKG reasoning has facilitated many real-world applications via answering such queries as (query entity, query... | Zixuan Li, Zhongni Hou, Saiping Guan, Xiaolong Jin, Weihua Peng, Long Bai, Yajuan Lyu, Wei Li, Jiafeng Guo, Xueqi Cheng |  |
| 544 |  |  [Dependency Parsing via Sequence Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.543) |  | 0 | Dependency parsing aims to extract syntactic dependency structure or semantic dependency structure for sentences.Existing methods for dependency parsing include transition-based method, graph-based method and sequence-to-sequence method.These methods obtain excellent performance and we notice them... | Boda Lin, Zijun Yao, Jiaxin Shi, Shulin Cao, Binghao Tang, Si Li, Yong Luo, Juanzi Li, Lei Hou |  |
| 545 |  |  [Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments](https://doi.org/10.18653/v1/2022.findings-emnlp.544) |  | 0 | Neural scaling laws define a predictable relationship between a model’s parameter count and its performance after training in the form of a power law. However, most research to date has not explicitly investigated whether scaling laws can be used to accelerate model development. In this work, we... | Maor Ivgi, Yair Carmon, Jonathan Berant |  |
| 546 |  |  [Analyzing the Limits of Self-Supervision in Handling Bias in Language](https://doi.org/10.18653/v1/2022.findings-emnlp.545) |  | 0 | Prompting inputs with natural language task descriptions has emerged as a popular mechanism to elicit reasonably accurate outputs from large-scale generative language models with little to no in-context supervision. This also helps gain insight into how well language models capture the semantics of... | Lisa Bauer, Karthik Gopalakrishnan, Spandana Gella, Yang Liu, Mohit Bansal, Dilek HakkaniTur |  |
| 547 |  |  [Multiple Instance Learning for Offensive Language Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.546) |  | 0 | Automatic offensive language detection has become a crucial issue in recent years. Existing researches on this topic are usually based on a large amount of data annotated at sentence level to train a robust model. However, sentence-level annotations are expensive in practice as the scenario... | Jiexi Liu, Dehan Kong, Longtao Huang, Dinghui Mao, Hui Xue |  |
| 548 |  |  [Grounded Keys-to-Text Generation: Towards Factual Open-Ended Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.547) |  | 0 | Large pre-trained language models have recently enabled open-ended generation frameworks (e.g., prompt-to-text NLG) to tackle a variety of tasks going beyond the traditional data-to-text generation. While this framework is more general, it is under-specified and often leads to a lack of... | Faeze Brahman, Baolin Peng, Michel Galley, Sudha Rao, Bill Dolan, Snigdha Chaturvedi, Jianfeng Gao |  |
| 549 |  |  [CogKTR: A Knowledge-Enhanced Text Representation Toolkit for Natural Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-demos.1) |  | 0 | As the first step of modern natural language processing, text representation encodes discrete texts as continuous embeddings. Pre-trained language models (PLMs) have demonstrated strong ability in text representation and significantly promoted the development of natural language understanding... | Zhuoran Jin, Tianyi Men, Hongbang Yuan, Yuyang Zhou, Pengfei Cao, Yubo Chen, Zhipeng Xue, Kang Liu, Jun Zhao |  |
| 550 |  |  [LM-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models](https://doi.org/10.18653/v1/2022.emnlp-demos.2) |  | 0 | The opaque nature and unexplained behavior of transformer-based language models (LMs) have spurred a wide interest in interpreting their predictions. However, current interpretation methods mostly focus on probing models from outside, executing behavioral tests, and analyzing salience input... | Mor Geva, Avi Caciularu, Guy Dar, Paul Roit, Shoval Sadde, Micah Shlain, Bar Tamir, Yoav Goldberg |  |
| 551 |  |  [EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-demos.3) |  | 0 | Pre-Trained Models (PTMs) have reshaped the development of Natural Language Processing (NLP) and achieved significant improvement in various benchmarks. Yet, it is not easy for industrial practitioners to obtain high-performing PTM-based models without a large amount of labeled training data and... | Chengyu Wang, Minghui Qiu, Taolin Zhang, Tingting Liu, Lei Li, Jianing Wang, Ming Wang, Jun Huang, Wei Lin |  |
| 552 |  |  [An Explainable Toolbox for Evaluating Pre-trained Vision-Language Models](https://doi.org/10.18653/v1/2022.emnlp-demos.4) |  | 0 | We introduce VL-CheckList, a toolbox for evaluating Vision-Language Pretraining (VLP) models, including the preliminary datasets that deepen the image-texting ability of a VLP model. Most existing VLP works evaluated their systems by comparing the fine-tuned downstream task performance. However,... | Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, Jianwei Yin |  |
| 553 |  |  [TweetNLP: Cutting-Edge Natural Language Processing for Social Media](https://doi.org/10.18653/v1/2022.emnlp-demos.5) |  | 0 | In this paper we present TweetNLP, an integrated platform for Natural Language Processing (NLP) in social media. TweetNLP supports a diverse set of NLP tasks, including generic focus areas such as sentiment analysis and named entity recognition, as well as social media-specific tasks such as emoji... | José CamachoCollados, Kiamehr Rezaee, Talayeh Riahi, Asahi Ushio, Daniel Loureiro, Dimosthenis Antypas, Joanne Boisson, Luis Espinosa Anke, Fangyu Liu, Eugenio Martínez Cámara |  |
| 554 |  |  [JoeyS2T: Minimalistic Speech-to-Text Modeling with JoeyNMT](https://doi.org/10.18653/v1/2022.emnlp-demos.6) |  | 0 | JoeyS2T is a JoeyNMT extension for speech-to-text tasks such as automatic speech recognition and end-to-end speech translation. It inherits the core philosophy of JoeyNMT, a minimalist NMT toolkit built on PyTorch, seeking simplicity and accessibility. JoeyS2T’s workflow is self-contained, starting... | Mayumi Ohta, Julia Kreutzer, Stefan Riezler |  |
| 555 |  |  [FairLib: A Unified Framework for Assessing and Improving Fairness](https://doi.org/10.18653/v1/2022.emnlp-demos.7) |  | 0 | This paper presents FairLib, an open-source python library for assessing and improving model fairness. It provides a systematic framework for quickly accessing benchmark datasets, reproducing existing debiasing baseline models, developing new methods, evaluating models with different metrics, and... | Xudong Han, Aili Shen, Yitong Li, Lea Frermann, Timothy Baldwin, Trevor Cohn |  |
| 556 |  |  [ELEVANT: A Fully Automatic Fine-Grained Entity Linking Evaluation and Analysis Tool](https://doi.org/10.18653/v1/2022.emnlp-demos.8) |  | 0 | We present Elevant, a tool for the fully automatic fine-grained evaluation of a set of entity linkers on a set of benchmarks. Elevant provides an automatic breakdown of the performance by various error categories and by entity type. Elevant also provides a rich and compact, yet very intuitive and... | Hannah Bast, Matthias Hertel, Natalie Prange |  |
| 557 |  |  [A Pipeline for Generating, Annotating and Employing Synthetic Data for Real World Question Answering](https://doi.org/10.18653/v1/2022.emnlp-demos.9) |  | 0 | Question Answering (QA) is a growing area of research, often used to facilitate the extraction of information from within documents. State-of-the-art QA models are usually pre-trained on domain-general corpora like Wikipedia and thus tend to struggle on out-of-domain documents without fine-tuning.... | Matthew Maufe, James Ravenscroft, Rob Procter, Maria Liakata |  |
| 558 |  |  [DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population](https://doi.org/10.18653/v1/2022.emnlp-demos.10) |  | 0 | We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in the knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation... | Ningyu Zhang, Xin Xu, Liankuan Tao, Haiyang Yu, Hongbin Ye, Shuofei Qiao, Xin Xie, Xiang Chen, Zhoubo Li, Lei Li |  |
| 559 |  |  [AnEMIC: A Framework for Benchmarking ICD Coding Models](https://doi.org/10.18653/v1/2022.emnlp-demos.11) |  | 0 | Diagnostic coding, or ICD coding, is the task of assigning diagnosis codes defined by the ICD (International Classification of Diseases) standard to patient visits based on clinical notes. The current process of manual ICD coding is time-consuming and often error-prone, which suggests the need for... | Juyong Kim, Abheesht Sharma, Suhas Shanbhogue, Jeremy C. Weiss, Pradeep Ravikumar |  |
| 560 |  |  [SPEAR : Semi-supervised Data Programming in Python](https://doi.org/10.18653/v1/2022.emnlp-demos.12) |  | 0 | We present SPEAR, an open-source python library for data programming with semi supervision. The package implements several recent data programming approaches including facility to programmatically label and build training data. SPEAR facilitates weak supervision in the form of heuristics (or rules)... | Guttu Sai Abhishek, Harshad Ingole, Parth Laturia, Vineeth Dorna, Ayush Maheshwari, Ganesh Ramakrishnan, Rishabh K. Iyer |  |
| 561 |  |  [Evaluate & Evaluation on the Hub: Better Best Practices for Data and Model Measurements](https://doi.org/10.18653/v1/2022.emnlp-demos.13) |  | 0 | Evaluation is a key part of machine learning (ML), yet there is a lack of support and tooling to enable its informed and systematic practice. We introduce Evaluate and Evaluation on the Hub—a set of tools to facilitate the evaluation of models and datasets in ML. Evaluate is a library to support... | Leandro von Werra, Lewis Tunstall, Abhishek Thakur, Sasha Luccioni, Tristan Thrush, Aleksandra Piktus, Felix Marty, Nazneen Rajani, Victor Mustar, Helen Ngo |  |
| 562 |  |  [KeywordScape: Visual Document Exploration using Contextualized Keyword Embeddings](https://doi.org/10.18653/v1/2022.emnlp-demos.14) |  | 0 | Although contextualized word embeddings have led to great improvements in automatic language understanding, their potential for practical applications in document exploration and visualization has been little explored. Common visualization techniques used for, e.g., model analysis usually provide... | Henrik Voigt, Monique Meuschke, Sina Zarrieß, Kai Lawonn |  |
| 563 |  |  [MedConQA: Medical Conversational Question Answering System based on Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-demos.15) |  | 0 | The medical conversational system can relieve doctors’ burden and improve healthcare efficiency, especially during the COVID-19 pandemic. However, the existing medical dialogue systems have the problems of weak scalability, insufficient knowledge, and poor controllability. Thus, we propose a... | Fei Xia, Bin Li, Yixuan Weng, Shizhu He, Kang Liu, Bin Sun, Shutao Li, Jun Zhao |  |
| 564 |  |  [Label Sleuth: From Unlabeled Text to a Classifier in a Few Hours](https://doi.org/10.18653/v1/2022.emnlp-demos.16) |  | 0 | Label Sleuth is an open source platform for building text classifiers which does not require coding skills nor machine learning knowledge.- Project website: [https://www.label-sleuth.org/](https://www.label-sleuth.org/)- Link to screencast video:... | Eyal Shnarch, Alon Halfon, Ariel Gera, Marina Danilevsky, Yannis Katsis, Leshem Choshen, Martín Santillán Cooper, Dina Epelboim, Zheng Zhang, Dakuo Wang |  |
| 565 |  |  [AGReE: A system for generating Automated Grammar Reading Exercises](https://doi.org/10.18653/v1/2022.emnlp-demos.17) |  | 0 | We describe the AGReE system, which takes user-submitted passages as input and automatically generates grammar practice exercises that can be completed while reading. Multiple-choice practice items are generated for a variety of different grammar constructs: punctuation, articles, conjunctions,... | Sophia Chan, Swapna Somasundaran, Debanjan Ghosh, Mengxuan Zhao |  |
| 566 |  |  [BotSIM: An End-to-End Bot Simulation Framework for Commercial Task-Oriented Dialog Systems](https://doi.org/10.18653/v1/2022.emnlp-demos.18) |  | 0 | We present BotSIM, a data-efficient end-to-end Bot SIMulation framework for commercial task-oriented dialog (TOD) systems. BotSIM consists of three major components: 1) a Generator that can infer semantic-level dialog acts and entities from bot definitions and generate user queries via model-based... | Guangsen Wang, Samson Tan, Shafiq R. Joty, Gang Wu, Jimmy Au, Steven C. H. Hoi |  |
| 567 |  |  [DeepGen: Diverse Search Ad Generation and Real-Time Customization](https://doi.org/10.18653/v1/2022.emnlp-demos.19) |  | 0 | Demo: https://youtu.be/WQLL93TPB-cAbstract:We present DeepGen, a system deployed at web scale for automatically creating sponsored search advertisements (ads) for BingAds customers. We leverage state-of-the-art natural language generation (NLG) models to generate fluent ads from advertiser’s web... | Konstantin Golobokov, Junyi Chai, Victor Ye Dong, Mandy Gu, Bingyu Chi, Jie Cao, Yulan Yan, Yi Liu |  |
| 568 |  |  [ACCoRD: A Multi-Document Approach to Generating Diverse Descriptions of Scientific Concepts](https://doi.org/10.18653/v1/2022.emnlp-demos.20) |  | 0 | Systems that automatically define unfamiliar terms hold the promise of improving the accessibility of scientific texts, especially for readers who may lack prerequisite background knowledge. However, current systems assume a single “best” description per concept, which fails to account for the many... | Sonia K. Murthy, Kyle Lo, Daniel King, Chandra Bhagavatula, Bailey Kuehl, Sophie Johnson, Jonathan Borchardt, Daniel S. Weld, Tom Hope, Doug Downey |  |
| 569 |  |  [Automatic Comment Generation for Chinese Student Narrative Essays](https://doi.org/10.18653/v1/2022.emnlp-demos.21) |  | 0 | Automatic essay evaluation can help reduce teachers’ workload and enable students to refine their works rapidly. Previous studies focus mainly on giving discrete scores for either the holistic quality orseveral distinct traits. However, real-world teachers usually provide detailed comments in... | Zhexin Zhang, Jian Guan, Guowei Xu, Yixiang Tian, Minlie Huang |  |
| 570 |  |  [MIC: A Multi-task Interactive Curation Tool](https://doi.org/10.18653/v1/2022.emnlp-demos.22) |  | 0 | This paper introduces MIC, a Multi-task Interactive Curation tool, a human-machine collaborative curation tool for multiple NLP tasks. The tool aims to borrow recent advances in literature to solve pain-points in real NLP tasks. Firstly, it supports multiple projects with multiple users which... | Shi Yu, Mingfeng Yang, Jerrod Parker, Stephen Brock |  |
| 571 |  |  [SUMMARY WORKBENCH: Unifying Application and Evaluation of Text Summarization Models](https://doi.org/10.18653/v1/2022.emnlp-demos.23) |  | 0 | This paper presents Summary Workbench, a new tool for developing and evaluating text summarization models. New models and evaluation measures can be easily integrated as Docker-based plugins, allowing to examine the quality of their summaries against any input and to evaluate them using various... | Shahbaz Syed, Dominik Schwabe, Martin Potthast |  |
| 572 |  |  [Arabic Word-level Readability Visualization for Assisted Text Simplification](https://doi.org/10.18653/v1/2022.emnlp-demos.24) |  | 0 | This demo paper presents a Google Docs add-on for automatic Arabic word-level readability visualization. The add-on includes a lemmatization component that is connected to a five-level readability lexicon and Arabic WordNet-based substitution suggestions. The add-on can be used for assessing the... | Reem Hazim, Hind Saddiki, Bashar Alhafni, Muhamed AlKhalil, Nizar Habash |  |
| 573 |  |  [LogiTorch: A PyTorch-based library for logical reasoning on natural language](https://doi.org/10.18653/v1/2022.emnlp-demos.25) |  | 0 | Logical reasoning on natural language is one of the most challenging tasks for deep learning models. There has been an increasing interest in developing new benchmarks to evaluate the reasoning capabilities of language models such as BERT. In parallel, new models based on transformers have emerged... | Chadi Helwe, Chloé Clavel, Fabian M. Suchanek |  |
| 574 |  |  [stopes - Modular Machine Translation Pipelines](https://doi.org/10.18653/v1/2022.emnlp-demos.26) |  | 0 | Neural machine translation, as other natural language deep learning applications, is hungry for data. As research evolves, the data pipelines supporting that research evolve too, oftentimes re-implementing the same core components. Despite the potential of modular codebases, researchers have but... | Pierre Andrews, Guillaume Wenzek, Kevin Heffernan, Onur Çelebi, Anna Y. Sun, Ammar Kamran, Yingzhe Guo, Alexandre Mourachko, Holger Schwenk, Angela Fan |  |
| 575 |  |  [GEMv2: Multilingual NLG Benchmarking in a Single Line of Code](https://doi.org/10.18653/v1/2022.emnlp-demos.27) |  | 0 | Evaluations in machine learning rarely use the latest metrics, datasets, or human evaluation in favor of remaining compatible with prior work. The compatibility, often facilitated through leaderboards, thus leads to outdated but standardized evaluation practices. We pose that the standardization is... | Sebastian Gehrmann, Abhik Bhattacharjee, Abinaya Mahendiran, Alex Wang, Alexandros Papangelis, Aman Madaan, Angelina McMillanMajor, Anna Shvets, Ashish Upadhyay, Bernd Bohnet |  |
| 576 |  |  [KGI: An Integrated Framework for Knowledge Intensive Language Tasks](https://doi.org/10.18653/v1/2022.emnlp-demos.28) |  | 0 | In this paper, we present a system to showcase the capabilities of the latest state-of-the-art retrieval augmented generation models trained on knowledge-intensive language tasks, such as slot filling, open domain question answering, dialogue, and fact-checking. Moreover, given a user query, we... | Md. Faisal Mahbub Chowdhury, Michael R. Glass, Gaetano Rossiello, Alfio Gliozzo, Nandana Mihindukulasooriya |  |
| 577 |  |  [Twitter-Demographer: A Flow-based Tool to Enrich Twitter Data](https://doi.org/10.18653/v1/2022.emnlp-demos.29) |  | 0 | Twitter data have become essential to Natural Language Processing (NLP) and social science research, driving various scientific discoveries in recent years. However, the textual data alone are often not enough to conduct studies: especially, social scientists need more variables to perform their... | Federico Bianchi, Vincenzo Cutrona, Dirk Hovy |  |
| 578 |  |  [Azimuth: Systematic Error Analysis for Text Classification](https://doi.org/10.18653/v1/2022.emnlp-demos.30) |  | 0 | We present Azimuth, an open-source and easy-to-use tool to perform error analysis for text classification. Compared to other stages of the ML development cycle, such as model training and hyper-parameter tuning, the process and tooling for the error analysis stage are less mature. However, this... | Gabrielle Gauthier Melançon, Orlando Marquez Ayala, Lindsay Brin, Chris Tyler, Frederic BranchaudCharron, Joseph Marinier, Karine Grande, Di Le |  |
| 579 |  |  [SynKB: Semantic Search for Synthetic Procedures](https://doi.org/10.18653/v1/2022.emnlp-demos.31) |  | 0 | In this paper we present SynKB, an open-source, automatically extracted knowledge base of chemical synthesis protocols. Similar to proprietary chemistry databases such as Reaxsys, SynKB allows chemists to retrieve structured knowledge about synthetic procedures. By taking advantage of recent... | Fan Bai, Alan Ritter, Peter B. Madrid, Dayne Freitag, John Niekrasz |  |
| 580 |  |  [Camelira: An Arabic Multi-Dialect Morphological Disambiguator](https://doi.org/10.18653/v1/2022.emnlp-demos.32) |  | 0 | We present Camelira, a web-based Arabic multi-dialect morphological disambiguation tool that covers four major variants of Arabic: Modern Standard Arabic, Egyptian, Gulf, and Levantine.Camelira offers a user-friendly web interface that allows researchers and language learners to explore various... | Ossama Obeid, Go Inoue, Nizar Habash |  |
| 581 |  |  [POTATO: The Portable Text Annotation Tool](https://doi.org/10.18653/v1/2022.emnlp-demos.33) |  | 0 | We present POTATO, the Portable text annotation tool, a free, fully open-sourced annotation system that 1) supports labeling many types of text and multimodal data; 2) offers easy-to-configure features to maximize the productivity of both deployers and annotators (convenient templates for common... | Jiaxin Pei, Aparna Ananthasubramaniam, Xingyao Wang, Naitian Zhou, Apostolos Dedeloudis, Jackson Sargent, David Jurgens |  |
| 582 |  |  [KGxBoard: Explainable and Interactive Leaderboard for Evaluation of Knowledge Graph Completion Models](https://doi.org/10.18653/v1/2022.emnlp-demos.34) |  | 0 | Knowledge Graphs (KGs) store information in the form of (head, predicate, tail)-triples. To augment KGs with new knowledge, researchers proposed models for KG Completion (KGC) tasks such as link prediction; i.e., answering (h; p; ?) or (?; p; t) queries. Such models are usually evaluated with... | Haris Widjaja, Kiril Gashteovski, Wiem Ben Rim, Pengfei Liu, Christopher Malon, Daniel Ruffinelli, Carolin Lawrence, Graham Neubig |  |
| 583 |  |  [FALTE: A Toolkit for Fine-grained Annotation for Long Text Evaluation](https://doi.org/10.18653/v1/2022.emnlp-demos.35) |  | 0 | A growing swath of NLP research is tackling problems related to generating long text, including tasks such as open-ended story generation, summarization, dialogue, and more. However, we currently lack appropriate tools to evaluate these long outputs of generation models: classic automatic metrics... | Tanya Goyal, Junyi Jessy Li, Greg Durrett |  |
| 584 |  |  [SEAL: Interactive Tool for Systematic Error Analysis and Labeling](https://doi.org/10.18653/v1/2022.emnlp-demos.36) |  | 0 | With the advent of Transformers, large language models (LLMs) have saturated well-known NLP benchmarks and leaderboards with high aggregate performance. However, many times these models systematically fail on tail data or rare groups not obvious in aggregate evaluation. Identifying such problematic... | Nazneen Rajani, Weixin Liang, Lingjiao Chen, Margaret Mitchell, James Zou |  |
| 585 |  |  [Hands-On Interactive Neuro-Symbolic NLP with DRaiL](https://doi.org/10.18653/v1/2022.emnlp-demos.37) |  | 0 | We recently introduced DRaiL, a declarative neural-symbolic modeling framework designed to support a wide variety of NLP scenarios. In this paper, we enhance DRaiL with an easy to use Python interface, equipped with methods to define, modify and augment DRaiL models interactively, as well as with... | Maria Leonor Pacheco, Shamik Roy, Dan Goldwasser |  |
| 586 |  |  [Paraphrastic Representations at Scale](https://doi.org/10.18653/v1/2022.emnlp-demos.38) |  | 0 | We present a system that allows users to train their own state-of-the-art paraphrastic sentence representations in a variety of languages. We release trained models for English, Arabic, German, Spanish, French, Russian, Turkish, and Chinese. We train these models on large amounts of data, achieving... | John Wieting, Kevin Gimpel, Graham Neubig, Taylor BergKirkpatrick |  |
| 587 |  |  [Snoopy: An Online Interface for Exploring the Effect of Pretraining Term Frequencies on Few-Shot LM Performance](https://doi.org/10.18653/v1/2022.emnlp-demos.39) |  | 0 | Current evaluation schemes for large language models often fail to consider the impact of the overlap between pretraining corpus and test data on model performance statistics. Snoopy is an online interface that allows researchers to study this impact in few-shot learning settings. Our demo provides... | Yasaman Razeghi, Raja Sekhar Reddy Mekala, Robert L. Logan IV, Matt Gardner, Sameer Singh |  |
| 588 |  |  [BMCook: A Task-agnostic Compression Toolkit for Big Models](https://doi.org/10.18653/v1/2022.emnlp-demos.40) |  | 0 | Recently, pre-trained language models (PLMs) have achieved great success on various NLP tasks and have shown a trend of exponential growth in model size. To alleviate the unaffordable computational costs brought by the size growth, model compression has been widely explored. Existing efforts have... | Zhengyan Zhang, Baitao Gong, Yingfa Chen, Xu Han, Guoyang Zeng, Weilin Zhao, Yanxu Chen, Zhiyuan Liu, Maosong Sun |  |
| 589 |  |  [ALToolbox: A Set of Tools for Active Learning Annotation of Natural Language Texts](https://doi.org/10.18653/v1/2022.emnlp-demos.41) |  | 0 | We present ALToolbox – an open-source framework for active learning (AL) annotation in natural language processing. Currently, the framework supports text classification, sequence tagging, and seq2seq tasks. Besides state-of-the-art query strategies, ALToolbox provides a set of tools that help to... | Akim Tsvigun, Leonid Sanochkin, Daniil Larionov, Gleb Kuzmin, Artem Vazhentsev, Ivan Lazichny, Nikita Khromov, Danil Kireev, Aleksandr Rubashevskii, Olga Shahmatova |  |
| 590 |  |  [TextBox 2.0: A Text Generation Library with Pre-trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-demos.42) |  | 0 | To facilitate research on text generation, this paper presents a comprehensive and unified library, TextBox 2.0, focusing on the use of pre-trained language models (PLMs). To be comprehensive, our library covers 13 common text generation tasks and their corresponding 83 datasets and further... | Tianyi Tang, Junyi Li, Zhipeng Chen, Yiwen Hu, Zhuohao Yu, Wenxun Dai, Wayne Xin Zhao, JianYun Nie, JiRong Wen |  |
| 591 |  |  [Unsupervised Term Extraction for Highly Technical Domains](https://doi.org/10.18653/v1/2022.emnlp-industry.1) |  | 0 | Term extraction is an information extraction task at the root of knowledge discovery platforms. Developing term extractors that are able to generalize across very diverse and potentially highly technical domains is challenging, as annotations for domains requiring in-depth expertise are scarce and... | Francesco Fusco, Peter W. J. Staar, Diego Antognini |  |
| 592 |  |  [DynaMaR: Dynamic Prompt with Mask Token Representation](https://doi.org/10.18653/v1/2022.emnlp-industry.2) |  | 0 | Recent research has shown that large language models pretrained using unsupervised approaches can achieve significant performance improvement on many downstream tasks. Typically when adapting these language models to downstream tasks, like a classification or regression task, we employ a... | Xiaodi Sun, Sunny Rajagopalan, Priyanka Nigam, Weiyi Lu, Yi Xu, Iman Keivanloo, Belinda Zeng, Trishul Chilimbi |  |
| 593 |  |  [A Hybrid Approach to Cross-lingual Product Review Summarization](https://doi.org/10.18653/v1/2022.emnlp-industry.3) |  | 0 | We present a hybrid approach for product review summarization which consists of: (i) an unsupervised extractive step to extract the most important sentences out of all the reviews, and (ii) a supervised abstractive step to summarize the extracted sentences into a coherent short summary. This... | Saleh Soltan, Victor Soto, Ke Tran, Wael Hamza |  |
| 594 |  |  [Augmenting Operations Research with Auto-Formulation of Optimization Models From Problem Descriptions](https://doi.org/10.18653/v1/2022.emnlp-industry.4) |  | 0 | We describe an augmented intelligence system for simplifying and enhancing the modeling experience for operations research. Using this system, the user receives a suggested formulation of an optimization problem based on its description. To facilitate this process, we build an intuitive user... | Rindra Ramamonjison, Haley Li, Timothy T. L. Yu, Shiqi He, Vishnu Rengan, Amin BanitalebiDehkordi, Zirui Zhou, Yong Zhang |  |
| 595 |  |  [Knowledge Distillation based Contextual Relevance Matching for E-commerce Product Search](https://doi.org/10.18653/v1/2022.emnlp-industry.5) |  | 0 | Online relevance matching is an essential task of e-commerce product search to boost the utility of search engines and ensure a smooth user experience. Previous work adopts either classical relevance matching models or Transformer-style models to address it. However, they ignore the inherent... | Ziyang Liu, Chaokun Wang, Hao Feng, Lingfei Wu, Liqun Yang |  |
| 596 |  |  [Accelerating the Discovery of Semantic Associations from Medical Literature: Mining Relations Between Diseases and Symptoms](https://doi.org/10.18653/v1/2022.emnlp-industry.6) |  | 0 | Medical literature is a vast and constantly expanding source of information about diseases, their diagnoses and treatments. One of the ways to extract insights from this type of data is through mining association rules between such entities. However, existing solutions do not take into account the... | Alberto Purpura, Francesca Bonin, Joao H. BettencourtSilva |  |
| 597 |  |  [PENTATRON: PErsonalized coNText-Aware Transformer for Retrieval-based cOnversational uNderstanding](https://doi.org/10.18653/v1/2022.emnlp-industry.7) |  | 0 | Conversational understanding is an integral part of modern intelligent devices. In a large fraction of the global traffic from customers using smart digital assistants, frictions in dialogues may be attributed to incorrect understanding of the entities in a customer’s query due to factors including... | UmaNaresh Niranjan, Ziyan Jiang, Ankit Ankit, Sungjin Lee, Jie Hao, Xing Fan, Chenlei Guo |  |
| 598 |  |  [Machine translation impact in E-commerce multilingual search](https://doi.org/10.18653/v1/2022.emnlp-industry.8) |  | 0 | Previous work suggests that performance of cross-lingual information retrieval correlates highly with the quality of Machine Translation. However, there may be a threshold beyond which improving query translation quality yields little or no benefit to further improve the retrieval performance. This... | Bryan Zhang, Amita Misra |  |
| 599 |  |  [Ask-and-Verify: Span Candidate Generation and Verification for Attribute Value Extraction](https://doi.org/10.18653/v1/2022.emnlp-industry.9) |  | 0 | The product attribute value extraction (AVE) task aims to capture key factual information from product profiles, and is useful for several downstream applications in e-Commerce platforms. Previous contributions usually formulate this task using sequence labeling or reading comprehension... | Yifan Ding, Yan Liang, Nasser Zalmout, Xian Li, Christan Grant, Tim Weninger |  |
| 600 |  |  [Consultation Checklists: Standardising the Human Evaluation of Medical Note Generation](https://doi.org/10.18653/v1/2022.emnlp-industry.10) |  | 0 | Evaluating automatically generated text is generally hard due to the inherently subjective nature of many aspects of the output quality. This difficulty is compounded in automatic consultation note generation by differing opinions between medical experts both about which patient statements should... | Aleksandar Savkov, Francesco Moramarco, Alex PapadopoulosKorfiatis, Mark Perera, Anya Belz, Ehud Reiter |  |
| 601 |  |  [Towards Need-Based Spoken Language Understanding Model Updates: What Have We Learned?](https://doi.org/10.18653/v1/2022.emnlp-industry.11) |  | 0 | In productionized machine learning systems, online model performance is known to deteriorate over time when there is a distributional drift between offline training and online application data. As a remedy, models are typically retrained at fixed time intervals, implying high computational and... | Quynh Do, Judith Gaspers, Daniil Sorokin, Patrick Lehnen |  |
| 602 |  |  [Knowledge Distillation Transfer Sets and their Impact on Downstream NLU Tasks](https://doi.org/10.18653/v1/2022.emnlp-industry.12) |  | 0 | Teacher-student knowledge distillation is a popular technique for compressing today’s prevailing large language models into manageable sizes that fit low-latency downstream applications. Both the teacher and the choice of transfer set used for distillation are crucial ingredients in creating a high... | Charith Peris, Lizhen Tan, Thomas Gueudré, Turan Gojayev, Pan Wei, Gokmen Oz |  |
| 603 |  |  [Exploiting In-Domain Bilingual Corpora for Zero-Shot Transfer Learning in NLU of Intra-Sentential Code-Switching Chatbot Interactions](https://doi.org/10.18653/v1/2022.emnlp-industry.13) |  | 0 | Code-switching (CS) is a very common phenomenon in regions with various co-existing languages. Since CS is such a frequent habit in informal communications, both spoken and written, it also arises naturally in Human-Machine Interactions. Therefore, in order for natural language understanding (NLU)... | Maia Aguirre, Manex Serras, Laura GarcíaSardiña, Jacobo LopezFernandez, Ariane Méndez, Arantza del Pozo |  |
| 604 |  |  [Calibrating Imbalanced Classifiers with Focal Loss: An Empirical Study](https://doi.org/10.18653/v1/2022.emnlp-industry.14) |  | 0 | Imbalanced data distribution is a practical and common challenge in building production-level machine learning (ML) models in industry, where data usually exhibits long-tail distributions. For instance, in virtual AI Assistants, such as Google Assistant, Amazon Alexa and Apple Siri, the “play... | Cheng Wang, Jorge Balazs, György Szarvas, Patrick Ernst, Lahari Poddar, Pavel Danchenko |  |
| 605 |  |  [Unsupervised training data re-weighting for natural language understanding with local distribution approximation](https://doi.org/10.18653/v1/2022.emnlp-industry.15) |  | 0 | One of the major challenges of training Natural Language Understanding (NLU) production models lies in the discrepancy between the distributions of the offline training data and of the online live data, due to, e.g., biased sampling scheme, cyclic seasonality shifts, annotated training data coming... | Jose Garrido Ramas, DieuThu Le, Bei Chen, Manoj Kumar, Kay Rottmann |  |
| 606 |  |  [Cross-Encoder Data Annotation for Bi-Encoder Based Product Matching](https://doi.org/10.18653/v1/2022.emnlp-industry.16) |  | 0 | Matching a seller listed item to an appropriate product is an important step for an e-commerce platform. With the recent advancement in deep learning, there are different encoder based approaches being proposed as solution. When textual data for two products are available, cross-encoder approaches... | Justin Chiu, Keiji Shinzato |  |
| 607 |  |  [Deploying a Retrieval based Response Model for Task Oriented Dialogues](https://doi.org/10.18653/v1/2022.emnlp-industry.17) |  | 0 | Task-oriented dialogue systems in industry settings need to have high conversational capability, be easily adaptable to changing situations and conform to business constraints. This paper describes a 3-step procedure to develop a conversational model that satisfies these criteria and can... | Lahari Poddar, György Szarvas, Cheng Wang, Jorge Balazs, Pavel Danchenko, Patrick Ernst |  |
| 608 |  |  [Tackling Temporal Questions in Natural Language Interface to Databases](https://doi.org/10.18653/v1/2022.emnlp-industry.18) |  | 0 | Temporal aspect is one of the most challenging areas in Natural Language Interface to Databases (NLIDB). This paper addresses and examines how temporal questions being studied and supported by the research community at both levels: popular annotated dataset (e.g. Spider) and recent advanced models.... | Ngoc Phuoc An Vo, Octavian Popescu, Irene Manotas, Vadim Sheinin |  |
| 609 |  |  [Multi-Tenant Optimization For Few-Shot Task-Oriented FAQ Retrieval](https://doi.org/10.18653/v1/2022.emnlp-industry.19) |  | 0 | Business-specific Frequently Asked Questions (FAQ) retrieval in task-oriented dialog systems poses unique challenges vis à vis community based FAQs. Each FAQ question represents an intent which is usually an umbrella term for many related user queries. We evaluate performance for such Business FAQs... | Asha Vishwanathan, Rajeev Unnikrishnan Warrier, Gautham Vadakkekara Suresh, Chandra Shekhar Kandpal |  |
| 610 |  |  [Iterative Stratified Testing and Measurement for Automated Model Updates](https://doi.org/10.18653/v1/2022.emnlp-industry.20) |  | 0 | Automating updates to machine learning systems is an important but understudied challenge in AutoML. The high model variance of many cutting-edge deep learning architectures means that retraining a model provides no guarantee of accurate inference on all sample types. To address this concern, we... | Elizabeth Dekeyser, Nicholas Comment, Shermin Pei, Rajat Kumar, Shruti Rai, Fengtao Wu, Lisa A. Haverty, Kanna Shimizu |  |
| 611 |  |  [SLATE: A Sequence Labeling Approach for Task Extraction from Free-form Inked Content](https://doi.org/10.18653/v1/2022.emnlp-industry.21) |  | 0 | We present SLATE, a sequence labeling approach for extracting tasks from free-form content such as digitally handwritten (or “inked”) notes on a virtual whiteboard. Our approach allows us to create a single, low-latency model to simultaneously perform sentence segmentation and classification of... | Apurva Gandhi, Ryan Serrao, Biyi Fang, Gilbert Antonius, Jenna Hong, Tra My Nguyen, Sheng Yi, Ehi Nosakhare, Irene Shaffer, Soundararajan Srinivasan |  |
| 612 |  |  [Gaining Insights into Unrecognized User Utterances in Task-Oriented Dialog Systems](https://doi.org/10.18653/v1/2022.emnlp-industry.22) |  | 0 | The rapidly growing market demand for automatic dialogue agents capable of goal-oriented behavior has caused many tech-industry leaders to invest considerable efforts into task-oriented dialog systems. The success of these systems is highly dependent on the accuracy of their intent identification –... | Ella Rabinovich, Matan Vetzler, David Boaz, Vineet Kumar, Gaurav Pandey, Ateret AnabyTavor |  |
| 613 |  |  [CoCoID: Learning Contrastive Representations and Compact Clusters for Semi-Supervised Intent Discovery](https://doi.org/10.18653/v1/2022.emnlp-industry.23) |  | 0 | Intent discovery is to mine new intents from user utterances, which are not present in the set of manually predefined intents. Previous approaches to intent discovery usually automatically cluster novel intents with prior knowledge from intent-labeled data in a semi-supervised way. In this paper,... | Qian Cao, Deyi Xiong, Qinlong Wang, Xia Peng |  |
| 614 |  |  [Tractable & Coherent Multi-Document Summarization: Discrete Optimization of Multiple Neural Modeling Streams via Integer Linear Programming](https://doi.org/10.18653/v1/2022.emnlp-industry.24) |  | 0 | One key challenge in multi-document summarization is the generated summary is often less coherent compared to single document summarization due to the larger heterogeneity of the input source content. In this work, we propose a generic framework to jointly consider coherence and informativeness in... | Litton J. Kurisinkel, Nancy Chen |  |
| 615 |  |  [Grafting Pre-trained Models for Multimodal Headline Generation](https://doi.org/10.18653/v1/2022.emnlp-industry.25) |  | 0 | Multimodal headline utilizes both video frames and transcripts to generate the natural language title of the videos. Due to a lack of large-scale, manually annotated data, the task of annotating grounded headlines for video is labor intensive and impractical. Previous researches on pre-trained... | Lingfeng Qiao, Chen Wu, Ye Liu, Haoyuan Peng, Di Yin, Bo Ren |  |
| 616 |  |  [Semi-supervised Adversarial Text Generation based on Seq2Seq models](https://doi.org/10.18653/v1/2022.emnlp-industry.26) |  | 0 | To improve deep learning models’ robustness, adversarial training has been frequently used in computer vision with satisfying results. However, adversarial perturbation on text have turned out to be more challenging due to the discrete nature of text. The generated adversarial text might not sound... | Hieu Le, DieuThu Le, Verena Weber, Chris Church, Kay Rottmann, Melanie Bradford, Peter Chin |  |
| 617 |  |  [Is it out yet? Automatic Future Product Releases Extraction from Web Data](https://doi.org/10.18653/v1/2022.emnlp-industry.27) |  | 0 | Identifying the release of new products and their predicted demand in advance is highly valuable for E-Commerce marketplaces and retailers. The information of an upcoming product release is used for inventory management, marketing campaigns and pre-order suggestions. Often, the announcement of an... | Gilad Fuchs, Ido BenShaul, Matan Mandelbrod |  |
| 618 |  |  [Automatic Scene-based Topic Channel Construction System for E-Commerce](https://doi.org/10.18653/v1/2022.emnlp-industry.28) |  | 0 | Scene marketing that well demonstrates user interests within a certain scenario has proved effective for offline shopping. To conduct scene marketing for e-commerce platforms, this work presents a novel product form, scene-based topic channel which typically consists of a list of diverse products... | Peng Lin, Yanyan Zou, Lingfei Wu, Mian Ma, Zhuoye Ding, Bo Long |  |
| 619 |  |  [SpeechNet: Weakly Supervised, End-to-End Speech Recognition at Industrial Scale](https://doi.org/10.18653/v1/2022.emnlp-industry.29) |  | 0 | End-to-end automatic speech recognition systems represent the state of the art, but they rely on thousands of hours of manually annotated speech for training, as well as heavyweight computation for inference. Of course, this impedes commercialization since most companies lack vast human and... | Raphael Tang, Karun Kumar, Gefei Yang, Akshat Pandey, Yajie Mao, Vladislav Belyaev, Madhuri Emmadi, G. Craig Murray, Ferhan Ture, Jimmy Lin |  |
| 620 |  |  [Controlled Language Generation for Language Learning Items](https://doi.org/10.18653/v1/2022.emnlp-industry.30) |  | 0 | This work aims to employ natural language generation (NLG) to rapidly generate items for English language learning applications: this requires both language models capable of generating fluent, high-quality English, and to control the output of the generation to match the requirements of the... | Kevin Stowe, Debanjan Ghosh, Mengxuan Zhao |  |
| 621 |  |  [Improving Text-to-SQL Semantic Parsing with Fine-grained Query Understanding](https://doi.org/10.18653/v1/2022.emnlp-industry.31) |  | 0 | Most recent research on Text-to-SQL semantic parsing relies on either parser itself or simple heuristic based approach to understand natural language query (NLQ). When synthesizing a SQL query, there is no explicit semantic information of NLQ available to the parser which leads to undesirable... | Jun Wang, Patrick Ng, Alexander Hanbo Li, Jiarong Jiang, Zhiguo Wang, Bing Xiang, Ramesh Nallapati, Sudipta Sengupta |  |
| 622 |  |  [Unsupervised Dense Retrieval for Scientific Articles](https://doi.org/10.18653/v1/2022.emnlp-industry.32) |  | 0 | In this work, we build a dense retrieval based semantic search engine on scientific articles from Elsevier. The major challenge is that there is no labeled data for training and testing. We apply a state-of-the-art unsupervised dense retrieval model called Generative Pseudo Labeling that generates... | Dan Li, Vikrant Yadav, Zubair Afzal, George Tsatsaronis |  |
| 623 |  |  [Learning Geolocations for Cold-Start and Hard-to-Resolve Addresses via Deep Metric Learning](https://doi.org/10.18653/v1/2022.emnlp-industry.33) |  | 0 | With evergrowing digital adoption in the society and increasing demand for businesses to deliver to customers doorstep, the last mile hop of transportation planning poses unique challenges in emerging geographies with unstructured addresses. One of the crucial inputs to facilitate effective... | Govind, Saurabh Sohoney |  |
| 624 |  |  [Meta-learning Pathologies from Radiology Reports using Variance Aware Prototypical Networks](https://doi.org/10.18653/v1/2022.emnlp-industry.34) |  | 0 | Large pretrained Transformer-based language models like BERT and GPT have changed the landscape of Natural Language Processing (NLP). However, fine tuning such models still requires a large number of training examples for each target task, thus annotating multiple datasets and training these models... | Arijit Sehanobish, Kawshik Kannan, Nabila Abraham, Anasuya Das, Benjamin Odry |  |
| 625 |  |  [Named Entity Recognition in Industrial Tables using Tabular Language Models](https://doi.org/10.18653/v1/2022.emnlp-industry.35) |  | 0 | Specialized transformer-based models for encoding tabular data have gained interest in academia. Although tabular data is omnipresent in industry, applications of table transformers are still missing. In this paper, we study how these models can be applied to an industrial Named Entity Recognition... | Aneta Koleva, Martin Ringsquandl, Mark Buckley, Rakebul Hasan, Volker Tresp |  |
| 626 |  |  [Reinforced Question Rewriting for Conversational Question Answering](https://doi.org/10.18653/v1/2022.emnlp-industry.36) |  | 0 | Conversational Question Answering (CQA) aims to answer questions contained within dialogues, which are not easily interpretable without context. Developing a model to rewrite conversational questions into self-contained ones is an emerging solution in industry settings as it allows using existing... | Zhiyu Chen, Jie Zhao, Anjie Fang, Besnik Fetahu, Oleg Rokhlenko, Shervin Malmasi |  |
| 627 |  |  [Improving Large-Scale Conversational Assistants using Model Interpretation based Training Sample Selection](https://doi.org/10.18653/v1/2022.emnlp-industry.37) |  | 0 | This paper presents an approach to identify samples from live traffic where the customer implicitly communicated satisfaction with Alexa’s responses, by leveraging interpretations of model behavior. Such customer signals are noisy and adding a large number of samples from live traffic to training... | Stefan Schroedl, Manoj Kumar, Kiana Hajebi, Morteza Ziyadi, Sriram Venkatapathy, Anil Ramakrishna, Rahul Gupta, Pradeep Natarajan |  |
| 628 |  |  [Improving Precancerous Case Characterization via Transformer-based Ensemble Learning](https://doi.org/10.18653/v1/2022.emnlp-industry.38) |  | 0 | The application of natural language processing (NLP) to cancer pathology reports has been focused on detecting cancer cases, largely ignoring precancerous cases. Improving the characterization of precancerous adenomas assists in developing diagnostic tests for early cancer detection and prevention,... | Yizhen Zhong, Jiajie Xiao, Thomas Vetterli, Mahan Matin, Ellen Loo, Jimmy Lin, Richard Bourgon, Ofer Shapira |  |
| 629 |  |  [Developing Prefix-Tuning Models for Hierarchical Text Classification](https://doi.org/10.18653/v1/2022.emnlp-industry.39) |  | 0 | Hierarchical text classification (HTC) is a key problem and task in many industrial applications, which aims to predict labels organized in a hierarchy for given input text. For example, HTC can group the descriptions of online products into a taxonomy or organizing customer reviews into a... | Lei Chen, Hou Wei Chou, Xiaodan Zhu |  |
| 630 |  |  [PAIGE: Personalized Adaptive Interactions Graph Encoder for Query Rewriting in Dialogue Systems](https://doi.org/10.18653/v1/2022.emnlp-industry.40) |  | 0 | Unexpected responses or repeated clarification questions from conversational agents detract from the users’ experience with technology meant to streamline their daily tasks. To reduce these frictions, Query Rewriting (QR) techniques replace transcripts of faulty queries with alternatives that lead... | Daniel Bis, Saurabh Gupta, Jie Hao, Xing Fan, Chenlei Guo |  |
| 631 |  |  [Fast Vocabulary Transfer for Language Model Compression](https://doi.org/10.18653/v1/2022.emnlp-industry.41) |  | 0 | Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer... | Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, Paolo Torroni |  |
| 632 |  |  [Multimodal Context Carryover](https://doi.org/10.18653/v1/2022.emnlp-industry.42) |  | 0 | Multi-modality support has become an integral part of creating a seamless user experience with modern voice assistants with smart displays. Users refer to images, video thumbnails, or the accompanying text descriptions on the screen through voice communication with AI powered devices. This raises... | Prashan Wanigasekara, Nalin Gupta, Fan Yang, Emre Barut, Zeynab Raeesy, Kechen Qin, Stephen Rawls, Xinyue Liu, Chengwei Su, Spurthi Sandiri |  |
| 633 |  |  [Distilling Multilingual Transformers into CNNs for Scalable Intent Classification](https://doi.org/10.18653/v1/2022.emnlp-industry.43) |  | 0 | We describe an application of Knowledge Distillation used to distill and deploy multilingual Transformer models for voice assistants, enabling text classification for customers globally. Transformers have set new state-of-the-art results for tasks like intent classification, and multilingual models... | Besnik Fetahu, Akash Veeragouni, Oleg Rokhlenko, Shervin Malmasi |  |
| 634 |  |  [Bringing the State-of-the-Art to Customers: A Neural Agent Assistant Framework for Customer Service Support](https://doi.org/10.18653/v1/2022.emnlp-industry.44) |  | 0 | Building Agent Assistants that can help improve customer service support requires inputs from industry users and their customers, as well as knowledge about state-of-the-art Natural Language Processing (NLP) technology. We combine expertise from academia and industry to bridge the gap and build... | Stephen Obadinma, Faiza Khan Khattak, Shirley Wang, Tania Sidhorn, Elaine Lau, Sean Robertson, Jingcheng Niu, Winnie Au, Alif Munim, Karthik Raja K. Bhaskar |  |
| 635 |  |  [Zero-Shot Dynamic Quantization for Transformer Inference](https://doi.org/10.18653/v1/2022.emnlp-industry.45) |  | 0 | We introduce a novel run-time method for significantly reducing the accuracy loss associated with quantizing BERT-like models to 8-bit integers. Existing methods for quantizing models either modify the training procedure, or they require an additional calibration step to adjust parameters that also... | Yousef ElKurdi, Jerry Quinn, Avi Sil |  |
| 636 |  |  [Fact Checking Machine Generated Text with Dependency Trees](https://doi.org/10.18653/v1/2022.emnlp-industry.46) |  | 0 | Factual and logical errors made by Natural Language Generation (NLG) systems limit their applicability in many settings. We study this problem in a conversational search and recommendation setting, and observe that we can often make two simplifying assumptions in this domain: (i) there exists a... | Alex Estes, Nikhita Vedula, Marcus D. Collins, Matt Cecil, Oleg Rokhlenko |  |
| 637 |  |  [Prototype-Representations for Training Data Filtering in Weakly-Supervised Information Extraction](https://doi.org/10.18653/v1/2022.emnlp-industry.47) |  | 0 | The availability of high quality training data is still a bottleneck for the practical utilization of information extraction models, despite the breakthroughs in zero and few-shot learning techniques. This is further exacerbated for industry applications, where new tasks, domains, and specific use... | Nasser Zalmout, Xian Li |  |
| 638 |  |  [CGF: Constrained Generation Framework for Query Rewriting in Conversational AI](https://doi.org/10.18653/v1/2022.emnlp-industry.48) |  | 0 | In conversational AI agents, Query Rewriting (QR) plays a crucial role in reducing user frictions and satisfying their daily demands. User frictions are caused by various reasons, such as errors in the conversational AI system, users’ accent or their abridged language. In this work, we present a... | Jie Hao, Yang Liu, Xing Fan, Saurabh Gupta, Saleh Soltan, Rakesh Chada, Pradeep Natarajan, Chenlei Guo, Gökhan Tür |  |
| 639 |  |  [Entity-level Sentiment Analysis in Contact Center Telephone Conversations](https://doi.org/10.18653/v1/2022.emnlp-industry.49) |  | 0 | Entity-level sentiment analysis predicts the sentiment about entities mentioned in a given text. It is very useful in a business context to understand user emotions towards certain entities, such as products or companies. In this paper, we demonstrate how we developed an entity-level sentiment... | XueYong Fu, Cheng Chen, Md. Tahmid Rahman Laskar, Shayna Gardiner, Pooja Hiranandani, Shashi Bhushan TN |  |
| 640 |  |  [QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation](https://doi.org/10.18653/v1/2022.emnlp-industry.50) |  | 0 | Large Language Models (LLMs) have shown impressive results on a variety of text understanding tasks. Search queries though pose a unique challenge, given their short-length and lack of nuance or context. Complicated feature engineering efforts do not always lead to downstream improvements as their... | Krishna Srinivasan, Karthik Raman, Anupam Samanta, Lingrui Liao, Luca Bertelli, Michael Bendersky |  |
| 641 |  |  [Distinguish Sense from Nonsense: Out-of-Scope Detection for Virtual Assistants](https://doi.org/10.18653/v1/2022.emnlp-industry.51) |  | 0 | Out of Scope (OOS) detection in Conversational AI solutions enables a chatbot to handle a conversation gracefully when it is unable to make sense of the end-user query. Accurately tagging a query as out-of-domain is particularly hard in scenarios when the chatbot is not equipped to handle a topic... | Cheng Qian, Haode Qi, Gengyu Wang, Ladislav Kunc, Saloni Potdar |  |
| 642 |  |  [PLATO-Ad: A Unified Advertisement Text Generation Framework with Multi-Task Prompt Learning](https://doi.org/10.18653/v1/2022.emnlp-industry.52) |  | 0 | Online advertisement text generation aims at generating attractive and persuasive text ads to appeal to users clicking ads or purchasing products. While pretraining-based models have achieved remarkable success in generating high-quality text ads, some challenges still remain, such as ad generation... | Zeyang Lei, Chao Zhang, Xinchao Xu, Wenquan Wu, ZhengYu Niu, Hua Wu, Haifeng Wang, Yi Yang, Shuanglong Li |  |
| 643 |  |  [Dense Feature Memory Augmented Transformers for COVID-19 Vaccination Search Classification](https://doi.org/10.18653/v1/2022.emnlp-industry.53) |  | 0 | With the devastating outbreak of COVID-19, vaccines are one of the crucial lines of defense against mass infection in this global pandemic. Given the protection they provide, vaccines are becoming mandatory in certain social and professional settings. This paper presents a classification model for... | Jai Gupta, Yi Tay, Chaitanya Kamath, Vinh Tran, Donald Metzler, Shailesh Bavadekar, Mimi Sun, Evgeniy Gabrilovich |  |
| 644 |  |  [Full-Stack Information Extraction System for Cybersecurity Intelligence](https://doi.org/10.18653/v1/2022.emnlp-industry.54) |  | 0 | Due to rapidly growing cyber-attacks and security vulnerabilities, many reports on cyber-threat intelligence (CTI) are being published daily. While these reports can help security analysts to understand on-going cyber threats,the overwhelming amount of information makes it difficult to digest the... | Youngja Park, Taesung Lee |  |
| 645 |  |  [Deploying Unified BERT Moderation Model for E-Commerce Reviews](https://doi.org/10.18653/v1/2022.emnlp-industry.55) |  | 0 | Moderation of user-generated e-commerce content has become crucial due to the large and diverse user base on the platforms. Product reviews and ratings have become an integral part of the shopping experience to build trust among users. Due to the high volume of reviews generated on a vast catalog... | Ravindra Nayak, Nikesh Garera |  |
| 646 |  |  [SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval](https://doi.org/10.18653/v1/2022.emnlp-industry.56) |  | 0 | Sampling proper negatives from a large document pool is vital to effectively train a dense retrieval model. However, existing negative sampling strategies suffer from the uninformative or false negative problem. In this work, we empirically show that according to the measured relevance scores, the... | Kun Zhou, Yeyun Gong, Xiao Liu, Wayne Xin Zhao, Yelong Shen, Anlei Dong, Jingwen Lu, Rangan Majumder, JiRong Wen, Nan Duan |  |
| 647 |  |  [Revisiting and Advancing Chinese Natural Language Understanding with Accelerated Heterogeneous Knowledge Pre-training](https://doi.org/10.18653/v1/2022.emnlp-industry.57) |  | 0 | Recently, knowledge-enhanced pre-trained language models (KEPLMs) improve context-aware representations via learning from structured relations in knowledge bases, and/or linguistic knowledge from syntactic or dependency analysis. Unlike English, there is a lack of high-performing open-source... | Taolin Zhang, Junwei Dong, Jianing Wang, Chengyu Wang, Ang Wang, Yinghui Liu, Jun Huang, Yong Li, Xiaofeng He |  |
| 648 |  |  [A Stacking-based Efficient Method for Toxic Language Detection on Live Streaming Chat](https://doi.org/10.18653/v1/2022.emnlp-industry.58) |  | 0 | In a live streaming chat on a video streaming service, it is crucial to filter out toxic comments with online processing to prevent users from reading comments in real-time. However, recent toxic language detection methods rely on deep learning methods, which can not be scalable considering... | Yuto Oikawa, Yuki Nakayama, Koji Murakami |  |
| 649 |  |  [End-to-End Speech to Intent Prediction to improve E-commerce Customer Support Voicebot in Hindi and English](https://doi.org/10.18653/v1/2022.emnlp-industry.59) |  | 0 | Automation of on-call customer support relies heavily on accurate and efficient speech-to-intent (S2I) systems. Building such systems using multi-component pipelines can pose various challenges because they require large annotated datasets, have higher latency, and have complex deployment. These... | Abhinav Goyal, Anupam Singh, Nikesh Garera |  |
| 650 |  |  [PILE: Pairwise Iterative Logits Ensemble for Multi-Teacher Labeled Distillation](https://doi.org/10.18653/v1/2022.emnlp-industry.60) |  | 0 | Pre-trained language models have become a crucial part of ranking systems and achieved very impressive effects recently. To maintain high performance while keeping efficient computations, knowledge distillation is widely used. In this paper, we focus on two key questions in knowledge distillation... | Lianshang Cai, Linhao Zhang, Dehong Ma, Jun Fan, Daiting Shi, Yi Wu, Zhicong Cheng, Simiu Gu, Dawei Yin |  |
| 651 |  |  [A Comprehensive Evaluation of Biomedical Entity-centric Search](https://doi.org/10.18653/v1/2022.emnlp-industry.61) |  | 0 | Biomedical information retrieval has often been studied as a task of detecting whether a system correctly detects entity spans and links these entities to concepts from a given terminology. Most academic research has focused on evaluation of named entity recognition (NER) and entity linking (EL)... | Elena Tutubalina, Zulfat Miftahutdinov, Vladimir Muravlev, Anastasia Shneyderman |  |
| 652 |  |  [Domain Adaptation of Machine Translation with Crowdworkers](https://doi.org/10.18653/v1/2022.emnlp-industry.62) |  | 0 | Although a machine translation model trained with a large in-domain parallel corpus achieves remarkable results, it still works poorly when no in-domain data are available. This situation restricts the applicability of machine translation when the target domain’s data are limited. However, there is... | Makoto Morishita, Jun Suzuki, Masaaki Nagata |  |
| 653 |  |  [Biomedical NER for the Enterprise with Distillated BERN2 and the Kazu Framework](https://doi.org/10.18653/v1/2022.emnlp-industry.63) |  | 0 | In order to assist the drug discovery/development process, pharmaceutical companies often apply biomedical NER and linking techniques over internal and public corpora. Decades of study of the field of BioNLP has produced a plethora of algorithms, systems and datasets. However, our experience has... | Wonjin Yoon, Richard Jackson, Elliot Ford, Vladimir Poroshin, Jaewoo Kang |  |
| 654 |  |  [Large-scale Machine Translation for Indian Languages in E-commerce under Low Resource Constraints](https://doi.org/10.18653/v1/2022.emnlp-industry.64) |  | 0 | The democratization of e-commerce platforms has moved an increasingly diversified Indian user base to shop online. We have deployed reliable and precise large-scale Machine Translation systems for several Indian regional languages in this work. Building such systems is a challenge because of the... | Amey Patil, Nikesh Garera |  |
| 655 |  |  [Topic Modeling by Clustering Language Model Embeddings: Human Validation on an Industry Dataset](https://doi.org/10.18653/v1/2022.emnlp-industry.65) |  | 0 | Topic models are powerful tools to get an overview of large collections of text data, a situation that is prevalent in industry applications. A rising trend within topic modeling is to directly cluster dimension-reduced embeddings created with pretrained language models. It is difficult to evaluate... | Anton Eklund, Mona Forsman |  |
| 656 |  |  [Generative Knowledge Graph Construction: A Review](https://doi.org/10.18653/v1/2022.emnlp-main.1) |  | 0 | Generative Knowledge Graph Construction (KGC) refers to those methods that leverage the sequence-to-sequence framework for building knowledge graphs, which is flexible and can be adapted to widespread tasks. In this study, we summarize the recent compelling progress in generative knowledge graph... | Hongbin Ye, Ningyu Zhang, Hui Chen, Huajun Chen |  |
| 657 |  |  [CDConv: A Benchmark for Contradiction Detection in Chinese Conversations](https://doi.org/10.18653/v1/2022.emnlp-main.2) |  | 0 | Dialogue contradiction is a critical issue in open-domain dialogue systems. The contextualization nature of conversations makes dialogue contradiction detection rather challenging. In this work, we propose a benchmark for Contradiction Detection in Chinese Conversations, namely CDConv. It contains... | Chujie Zheng, Jinfeng Zhou, Yinhe Zheng, Libiao Peng, Zhen Guo, Wenquan Wu, ZhengYu Niu, Hua Wu, Minlie Huang |  |
| 658 |  |  [Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space](https://doi.org/10.18653/v1/2022.emnlp-main.3) |  | 0 | Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the... | Mor Geva, Avi Caciularu, Kevin Ro Wang, Yoav Goldberg |  |
| 659 |  |  [Learning to Generate Question by Asking Question: A Primal-Dual Approach with Uncommon Word Generation](https://doi.org/10.18653/v1/2022.emnlp-main.4) |  | 0 | Automatic question generation (AQG) is the task of generating a question from a given passage and an answer. Most existing AQG methods aim at encoding the passage and the answer to generate the question. However, limited work has focused on modeling the correlation between the target answer and the... | Qifan Wang, Li Yang, Xiaojun Quan, Fuli Feng, Dongfang Liu, Zenglin Xu, Sinong Wang, Hao Ma |  |
| 660 |  |  [Graph-based Model Generation for Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.5) |  | 0 | Few-shot relation extraction (FSRE) has been a challenging problem since it only has a handful of training instances. Existing models follow a ‘one-for-all’ scheme where one general large model performs all individual N-way-K-shot tasks in FSRE, which prevents the model from achieving the optimal... | Wanli Li, Tieyun Qian |  |
| 661 |  |  [Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling](https://doi.org/10.18653/v1/2022.emnlp-main.6) |  | 0 | Recent advances in federated learning have demonstrated its promising capability to learn on decentralized datasets. However, a considerable amount of work has raised concerns due to the potential risks of adversaries participating in the framework to poison the global model for an adversarial... | KiYoon Yoo, Nojun Kwak |  |
| 662 |  |  [Generating Natural Language Proofs with Verifier-Guided Search](https://doi.org/10.18653/v1/2022.emnlp-main.7) |  | 0 | Reasoning over natural language is a challenging problem in NLP. In this work, we focus on proof generation: Given a hypothesis and a set of supporting facts, the model generates a proof tree indicating how to derive the hypothesis from supporting facts. Compared to generating the entire proof in... | Kaiyu Yang, Jia Deng, Danqi Chen |  |
| 663 |  |  [Toward Unifying Text Segmentation and Long Document Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.8) |  | 0 | Text segmentation is important for signaling a document’s structure. Without segmenting a long document into topically coherent sections, it is difficult for readers to comprehend the text, let alone find important information. The problem is only exacerbated by a lack of segmentation in... | Sangwoo Cho, Kaiqiang Song, Xiaoyang Wang, Fei Liu, Dong Yu |  |
| 664 |  |  [The Geometry of Multilingual Language Model Representations](https://doi.org/10.18653/v1/2022.emnlp-main.9) |  | 0 | We assess how multilingual language models maintain a shared multilingual representation space while still encoding language-sensitive information in each language. Using XLM-R as a case study, we show that languages occupy similar linear subspaces after mean-centering, evaluated based on causal... | Tyler A. Chang, Zhuowen Tu, Benjamin K. Bergen |  |
| 665 |  |  [Improving Complex Knowledge Base Question Answering via Question-to-Action and Question-to-Question Alignment](https://doi.org/10.18653/v1/2022.emnlp-main.10) |  | 0 | Complex knowledge base question answering can be achieved by converting questions into sequences of predefined actions. However, there is a significant semantic and structural gap between natural language and action sequences, which makes this conversion difficult. In this paper, we introduce an... | Yechun Tang, Xiaoxia Cheng, Weiming Lu |  |
| 666 |  |  [PAIR: Prompt-Aware margIn Ranking for Counselor Reflection Scoring in Motivational Interviewing](https://doi.org/10.18653/v1/2022.emnlp-main.11) |  | 0 | Counselor reflection is a core verbal skill used by mental health counselors to express understanding and affirmation of the client’s experience and concerns. In this paper, we propose a system for the analysis of counselor reflections. Specifically, our system takes as input one dialog turn... | Do June Min, Verónica PérezRosas, Kenneth Resnicow, Rada Mihalcea |  |
| 667 |  |  [Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.12) |  | 0 | Recent graph-based models for joint multiple intent detection and slot filling have obtained promising results through modeling the guidance from the prediction of intents to the decoding of slot filling.However, existing methods (1) only model the unidirectional guidance from intent to slot; (2)... | Bowen Xing, Ivor W. Tsang |  |
| 668 |  |  [The Importance of Being Parameters: An Intra-Distillation Method for Serious Gains](https://doi.org/10.18653/v1/2022.emnlp-main.13) |  | 0 | Recent model pruning methods have demonstrated the ability to remove redundant parameters without sacrificing model performance. Common methods remove redundant parameters according to the parameter sensitivity, a gradient-based measure reflecting the contribution of the parameters. In this paper,... | Haoran Xu, Philipp Koehn, Kenton Murray |  |
| 669 |  |  [Interpreting Language Models with Contrastive Explanations](https://doi.org/10.18653/v1/2022.emnlp-main.14) |  | 0 | Model interpretability methods are often used to explain NLP model decisions on tasks such as text classification, where the output space is relatively small. However, when applied to language generation, where the output space often consists of tens of thousands of tokens, these methods are unable... | Kayo Yin, Graham Neubig |  |
| 670 |  |  [RankGen: Improving Text Generation with Large Ranking Models](https://doi.org/10.18653/v1/2022.emnlp-main.15) |  | 0 | Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues we present RankGen, a 1.2B parameter... | Kalpesh Krishna, Yapei Chang, John Wieting, Mohit Iyyer |  |
| 671 |  |  [Learning a Grammar Inducer from Massive Uncurated Instructional Videos](https://doi.org/10.18653/v1/2022.emnlp-main.16) |  | 0 | Video-aided grammar induction aims to leverage video information for finding more accurate syntactic grammars for accompanying text. While previous work focuses on building systems for inducing grammars on text that are well-aligned with video content, we investigate the scenario, in which text and... | Songyang Zhang, Linfeng Song, Lifeng Jin, Haitao Mi, Kun Xu, Dong Yu, Jiebo Luo |  |
| 672 |  |  [Normalized Contrastive Learning for Text-Video Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.17) |  | 0 | Cross-modal contrastive learning has led the recent advances in multimodal retrieval with its simplicity and effectiveness. In this work, however, we reveal that cross-modal contrastive learning suffers from incorrect normalization of the sum retrieval probabilities of each text or video instance.... | Yookoon Park, Mahmoud Azab, Seungwhan Moon, Bo Xiong, Florian Metze, Gourab Kundu, Kirmani Ahmed |  |
| 673 |  |  [Estimating Soft Labels for Out-of-Domain Intent Detection](https://doi.org/10.18653/v1/2022.emnlp-main.18) |  | 0 | Out-of-Domain (OOD) intent detection is important for practical dialog systems. To alleviate the issue of lacking OOD training samples, some works propose synthesizing pseudo OOD samples and directly assigning one-hot OOD labels to these pseudo samples. However, these one-hot labels introduce... | Hao Lang, Yinhe Zheng, Jian Sun, Fei Huang, Luo Si, Yongbin Li |  |
| 674 |  |  [Multi-VQG: Generating Engaging Questions for Multiple Images](https://doi.org/10.18653/v1/2022.emnlp-main.19) |  | 0 | Generating engaging content has drawn much recent attention in the NLP community. Asking questions is a natural way to respond to photos and promote awareness. However, most answers to questions in traditional question-answering (QA) datasets are factoids, which reduce individuals’ willingness to... | MinHsuan Yeh, Vincent Chen, TingHao (Kenneth) Huang, LunWei Ku |  |
| 675 |  |  [Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.20) |  | 0 | The predictions of question answering (QA) systems are typically evaluated against manually annotated finite sets of one or more answers. This leads to a coverage limitation that results in underestimating the true performance of systems, and is typically addressed by extending over exact match... | Jannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Börschinger, Tal Schuster |  |
| 676 |  |  [Non-Parametric Domain Adaptation for End-to-End Speech Translation](https://doi.org/10.18653/v1/2022.emnlp-main.21) |  | 0 | The end-to-end speech translation (E2E-ST) has received increasing attention due to the potential of its less error propagation, lower latency and fewer parameters. However, the effectiveness of neural-based approaches to this task is severely limited by the available training corpus, especially... | Yichao Du, Weizhi Wang, Zhirui Zhang, Boxing Chen, Tong Xu, Jun Xie, Enhong Chen |  |
| 677 |  |  [Prompting for Multimodal Hateful Meme Classification](https://doi.org/10.18653/v1/2022.emnlp-main.22) |  | 0 | Hateful meme classification is a challenging multimodal task that requires complex reasoning and contextual background knowledge. Ideally, we could leverage an explicit external knowledge base to supplement contextual and cultural information in hateful memes. However, there is no known explicit... | Rui Cao, Roy KaWei Lee, WenHaw Chong, Jing Jiang |  |
| 678 |  |  [Certified Error Control of Candidate Set Pruning for Two-Stage Relevance Ranking](https://doi.org/10.18653/v1/2022.emnlp-main.23) |  | 0 | In information retrieval (IR), candidate set pruning has been commonly used to speed up two-stage relevance ranking. However, such an approach lacks accurate error control and often trades accuracy against computational efficiency in an empirical fashion, missing theoretical guarantees. In this... | Minghan Li, Xinyu Zhang, Ji Xin, Hongyang Zhang, Jimmy Lin |  |
| 679 |  |  [Linearizing Transformer with Key-Value Memory](https://doi.org/10.18653/v1/2022.emnlp-main.24) |  | 0 | Efficient transformer variants with linear time complexity have been developed to mitigate the quadratic computational overhead of the vanilla transformer. Among them are low-rank projection methods such as Linformer and kernel-based Transformers. Despite their unique merits, they usually suffer... | Yizhe Zhang, Deng Cai |  |
| 680 |  |  [Robustness of Fusion-based Multimodal Classifiers to Cross-Modal Content Dilutions](https://doi.org/10.18653/v1/2022.emnlp-main.25) |  | 0 | As multimodal learning finds applications in a wide variety of high-stakes societal tasks, investigating their robustness becomes important. Existing work has focused on understanding the robustness of vision-and-language models to imperceptible variations on benchmark tasks. In this work, we... | Gaurav Verma, Vishwa Vinay, Ryan A. Rossi, Srijan Kumar |  |
| 681 |  |  [Translation between Molecules and Natural Language](https://doi.org/10.18653/v1/2022.emnlp-main.26) |  | 0 | We present MolT5 - a self-supervised learning framework for pretraining models on a vast amount of unlabeled natural language text and molecule strings. MolT5 allows for new, useful, and challenging analogs of traditional vision-language tasks, such as molecule captioning and text-based de novo... | Carl Edwards, Tuan Manh Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, Heng Ji |  |
| 682 |  |  [What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment](https://doi.org/10.18653/v1/2022.emnlp-main.27) |  | 0 | The instruction learning paradigm—where a model learns to perform new tasks from task descriptions alone—has become popular in research on general-purpose models. The capabilities of large transformer models as instruction learners, however, remain poorly understood. We use a controlled synthetic... | Matthew Finlayson, Kyle Richardson, Ashish Sabharwal, Peter Clark |  |
| 683 |  |  [Sentence-Incremental Neural Coreference Resolution](https://doi.org/10.18653/v1/2022.emnlp-main.28) |  | 0 | We propose a sentence-incremental neural coreference resolution system which incrementally builds clusters after marking mention boundaries in a shift-reduce method. The system is aimed at bridging two recent approaches at coreference resolution: (1) state-of-the-art non-incremental models that... | Matt Grenander, Shay B. Cohen, Mark Steedman |  |
| 684 |  |  [SNaC: Coherence Error Detection for Narrative Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.29) |  | 0 | Progress in summarizing long texts is inhibited by the lack of appropriate evaluation frameworks. A long summary that appropriately covers the facets of that text must also present a coherent narrative, but current automatic and human evaluation methods fail to identify gaps in coherence. In this... | Tanya Goyal, Junyi Jessy Li, Greg Durrett |  |
| 685 |  |  [HydraSum: Disentangling Style Features in Text Summarization with Multi-Decoder Models](https://doi.org/10.18653/v1/2022.emnlp-main.30) |  | 0 | Summarization systems make numerous “decisions” about summary properties during inference, e.g. degree of copying, specificity and length of outputs, etc. However, these are implicitly encoded within model parameters and specific styles cannot be enforced. To address this, we introduce HydraSum, a... | Tanya Goyal, Nazneen Rajani, Wenhao Liu, Wojciech Kryscinski |  |
| 686 |  |  [A Good Neighbor, A Found Treasure: Mining Treasured Neighbors for Knowledge Graph Entity Typing](https://doi.org/10.18653/v1/2022.emnlp-main.31) |  | 0 | The task of knowledge graph entity typing (KGET) aims to infer the missing types for entities in knowledge graphs. Some pioneering work has proved that neighbor information is very important for the task. However, existing methods only leverage the one-hop neighbor information of the central... | Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao |  |
| 687 |  |  [Guiding Neural Entity Alignment with Compatibility](https://doi.org/10.18653/v1/2022.emnlp-main.32) |  | 0 | Entity Alignment (EA) aims to find equivalent entities between two Knowledge Graphs (KGs). While numerous neural EA models have been devised, they are mainly learned using labelled data only. In this work, we argue that different entities within one KG should have compatible counterparts in the... | Bing Liu, Harrisen Scells, Wen Hua, Guido Zuccon, Genghong Zhao, Xia Zhang |  |
| 688 |  |  [InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning](https://doi.org/10.18653/v1/2022.emnlp-main.33) |  | 0 | Instruction tuning is an emergent paradigm in NLP wherein natural language instructions are leveraged with language models to induce zero-shot performance on unseen tasks. Dialogue is an especially interesting area in which to explore instruction tuning because dialogue systems perform multiple... | Prakhar Gupta, Cathy Jiao, YiTing Yeh, Shikib Mehri, Maxine Eskénazi, Jeffrey P. Bigham |  |
| 689 |  |  [Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling](https://doi.org/10.18653/v1/2022.emnlp-main.34) |  | 0 | Boundary information is critical for various Chinese language processing tasks, such as word segmentation, part-of-speech tagging, and named entity recognition. Previous studies usually resorted to the use of a high-quality external lexicon, where lexicon items can offer explicit boundary... | Peijie Jiang, Dingkun Long, Yanzhao Zhang, Pengjun Xie, Meishan Zhang, Min Zhang |  |
| 690 |  |  [RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder](https://doi.org/10.18653/v1/2022.emnlp-main.35) |  | 0 | Despite pre-training’s progress in many important NLP tasks, it remains to explore effective pre-training strategies for dense retrieval. In this paper, we propose RetroMAE, a new retrieval oriented pre-training paradigm based on Masked Auto-Encoder (MAE). RetroMAE is highlighted by three critical... | Shitao Xiao, Zheng Liu, Yingxia Shao, Zhao Cao |  |
| 691 |  |  [Aligning Recommendation and Conversation via Dual Imitation](https://doi.org/10.18653/v1/2022.emnlp-main.36) |  | 0 | Human conversations of recommendation naturally involve the shift of interests which can align the recommendation actions and conversation process to make accurate recommendations with rich explanations. However, existing conversational recommendation systems (CRS) ignore the advantage of user... | Jinfeng Zhou, Bo Wang, Minlie Huang, Dongming Zhao, Kun Huang, Ruifang He, Yuexian Hou |  |
| 692 |  |  [QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance](https://doi.org/10.18653/v1/2022.emnlp-main.37) |  | 0 | Existing metrics for assessing question generation not only require costly human reference but also fail to take into account the input context of generation, rendering the lack of deep understanding of the relevance between the generated questions and input contexts. As a result, they may wrongly... | Xiaoqiang Wang, Bang Liu, Siliang Tang, Lingfei Wu |  |
| 693 |  |  [Abstract Visual Reasoning with Tangram Shapes](https://doi.org/10.18653/v1/2022.emnlp-main.38) |  | 0 | We introduce KiloGram, a resource for studying abstract visual reasoning in humans and machines. Drawing on the history of tangram puzzles as stimuli in cognitive science, we build a richly annotated dataset that, with >1k distinct stimuli, is orders of magnitude larger and more diverse than prior... | Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert D. Hawkins, Yoav Artzi |  |
| 694 |  |  [UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.39) |  | 0 | Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different... | Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, ChienSheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, Tao Yu |  |
| 695 |  |  [Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models](https://doi.org/10.18653/v1/2022.emnlp-main.40) |  | 0 | Traditional (fickle) adversarial examples involve finding a small perturbation that does not change an input’s true label but confuses the classifier into outputting a different prediction. Conversely, obstinate adversarial examples occur when an adversary finds a small perturbation that preserves... | Hannah Chen, Yangfeng Ji, David E. Evans |  |
| 696 |  |  [When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks](https://doi.org/10.18653/v1/2022.emnlp-main.41) |  | 0 | Humans can reason compositionally whilst grounding language utterances to the real world. Recent benchmarks like ReaSCAN (Wu et al., 2021) use navigation tasks grounded in a grid world to assess whether neural models exhibit similar capabilities. In this work, we present a simple transformer-based... | Ankur Sikarwar, Arkil Patel, Navin Goyal |  |
| 697 |  |  [Generative Language Models for Paragraph-Level Question Generation](https://doi.org/10.18653/v1/2022.emnlp-main.42) |  | 0 | Powerful generative models have led to recent progress in question generation (QG). However, it is difficult to measure advances in QG research since there are no standardized resources that allow a uniform comparison among approaches. In this paper, we introduce QG-Bench, a multilingual and... | Asahi Ushio, Fernando AlvaManchego, José CamachoCollados |  |
| 698 |  |  [A Unified Encoder-Decoder Framework with Entity Memory](https://doi.org/10.18653/v1/2022.emnlp-main.43) |  | 0 | Entities, as important carriers of real-world knowledge, play a key role in many NLP tasks.We focus on incorporating entity knowledge into an encoder-decoder framework for informative text generation. Existing approaches tried to index, retrieve, and read external documents as evidence, but they... | Zhihan Zhang, Wenhao Yu, Chenguang Zhu, Meng Jiang |  |
| 699 |  |  [Segmenting Numerical Substitution Ciphers](https://doi.org/10.18653/v1/2022.emnlp-main.44) |  | 0 | Deciphering historical substitution ciphers is a challenging problem. Example problems that have been previously studied include detecting cipher type, detecting plaintext language, and acquiring the substitution key for segmented ciphers. However, attacking unsegmented ciphers is still a... | Nada Aldarrab, Jonathan May |  |
| 700 |  |  [Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.45) |  | 0 | Research in massively multilingual image captioning has been severely hampered by a lack of high-quality evaluation datasets. In this paper we present the Crossmodal-3600 dataset (XM3600 in short), a geographically diverse set of 3600 images annotated with human-generated reference captions in 36... | Ashish V. Thapliyal, Jordi PontTuset, Xi Chen, Radu Soricut |  |
| 701 |  |  [ReSel: N-ary Relation Extraction from Scientific Text and Tables by Learning to Retrieve and Select](https://doi.org/10.18653/v1/2022.emnlp-main.46) |  | 0 | We study the problem of extracting N-ary relation tuples from scientific articles. This task is challenging because the target knowledge tuples can reside in multiple parts and modalities of the document. Our proposed method ReSel decomposes this task into a two-stage procedure that first retrieves... | Yuchen Zhuang, Yinghao Li, Junyang Zhang, Yue Yu, Yingjun Mou, Xiang Chen, Le Song, Chao Zhang |  |
| 702 |  |  [GammaE: Gamma Embeddings for Logical Queries on Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.47) |  | 0 | Embedding knowledge graphs (KGs) for multi-hop logical reasoning is a challenging problem due to massive and complicated structures in many KGs. Recently, many promising works projected entities and queries into a geometric space to efficiently find answers. However, it remains challenging to model... | Dong Yang, Peijun Qing, Yang Li, Haonan Lu, Xiaodong Lin |  |
| 703 |  |  [Reasoning Like Program Executors](https://doi.org/10.18653/v1/2022.emnlp-main.48) |  | 0 | Reasoning over natural language is a long-standing goal for the research community. However, studies have shown that existing language models are inadequate in reasoning. To address the issue, we present POET, a novel reasoning pre-training paradigm. Through pre-training language models with... | Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Qiang Fu, Yan Gao, JianGuang Lou, Weizhu Chen |  |
| 704 |  |  [SEM-F1: an Automatic Way for Semantic Evaluation of Multi-Narrative Overlap Summaries at Scale](https://doi.org/10.18653/v1/2022.emnlp-main.49) |  | 0 | Recent work has introduced an important yet relatively under-explored NLP task called Semantic Overlap Summarization (SOS) that entails generating a summary from multiple alternative narratives which conveys the common information provided by those narratives. Previous work also published a... | Naman Bansal, Mousumi Akter, Shubhra Kanti Karmaker Santu |  |
| 705 |  |  [Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning](https://doi.org/10.18653/v1/2022.emnlp-main.50) |  | 0 | Prefix-tuning, or more generally continuous prompt tuning, has become an essential paradigm of parameter-efficient transfer learning. Using a large pre-trained language model (PLM), prefix-tuning can obtain strong performance by training only a small portion of parameters. In this paper, we propose... | Yifan Chen, Devamanyu Hazarika, Mahdi Namazifar, Yang Liu, Di Jin, Dilek HakkaniTur |  |
| 706 |  |  [DocInfer: Document-level Natural Language Inference using Optimal Evidence Selection](https://doi.org/10.18653/v1/2022.emnlp-main.51) |  | 0 | We present DocInfer - a novel, end-to-end Document-level Natural Language Inference model that builds a hierarchical document graph enriched through inter-sentence relations (topical, entity-based, concept-based), performs paragraph pruning using the novel SubGraph Pooling layer, followed by... | Puneet Mathur, Gautam Kunapuli, Riyaz A. Bhat, Manish Shrivastava, Dinesh Manocha, Maneesh Singh |  |
| 707 |  |  [LightEA: A Scalable, Robust, and Interpretable Entity Alignment Framework via Three-view Label Propagation](https://doi.org/10.18653/v1/2022.emnlp-main.52) |  | 0 | Entity Alignment (EA) aims to find equivalent entity pairs between KGs, which is the core step to bridging and integrating multi-source KGs. In this paper, we argue that existing complex EA methods inevitably inherit the inborn defects from their neural network lineage: poor interpretability and... | Xin Mao, Wenting Wang, Yuanbin Wu, Man Lan |  |
| 708 |  |  [Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.53) |  | 0 | Commonsense generation aims to generate a realistic sentence describing a daily scene under the given concepts, which is very challenging, since it requires models to have relational reasoning and compositional generalization capabilities. Previous work focuses on retrieving prototype sentences for... | Xingwei He, Yeyun Gong, ALong Jin, Weizhen Qi, Hang Zhang, Jian Jiao, Bartuer Zhou, Biao Cheng, SiuMing Yiu, Nan Duan |  |
| 709 |  |  [Efficient Document Retrieval by End-to-End Refining and Quantizing BERT Embedding with Contrastive Product Quantization](https://doi.org/10.18653/v1/2022.emnlp-main.54) |  | 0 | Efficient document retrieval heavily relies on the technique of semantic hashing, which learns a binary code for every document and employs Hamming distance to evaluate document distances. However, existing semantic hashing methods are mostly established on outdated TFIDF features, which obviously... | Zexuan Qiu, Qinliang Su, Jianxing Yu, Shijing Si |  |
| 710 |  |  [Curriculum Knowledge Distillation for Emoji-supervised Cross-lingual Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.55) |  | 0 | Existing sentiment analysis models have achieved great advances with the help of sufficient sentiment annotations. Unfortunately, many languages do not have sufficient sentiment corpus. To this end, recent studies have proposed cross-lingual sentiment analysis to transfer sentiment analysis models... | Jianyang Zhang, Tao Liang, Mingyang Wan, Guowu Yang, Fengmao Lv |  |
| 711 |  |  [Correctable-DST: Mitigating Historical Context Mismatch between Training and Inference for Improved Dialogue State Tracking](https://doi.org/10.18653/v1/2022.emnlp-main.56) |  | 0 | Recently proposed dialogue state tracking (DST) approaches predict the dialogue state of a target turn sequentially based on the previous dialogue state. During the training time, the ground-truth previous dialogue state is utilized as the historical context. However, only the previously predicted... | Hongyan Xie, Haoxiang Su, Shuangyong Song, Hao Huang, Bo Zou, Kun Deng, Jianghua Lin, Zhihui Zhang, Xiaodong He |  |
| 712 |  |  [DropMix: A Textual Data Augmentation Combining Dropout with Mixup](https://doi.org/10.18653/v1/2022.emnlp-main.57) |  | 0 | Overfitting is a notorious problem when there is insufficient data to train deep neural networks in machine learning tasks. Data augmentation regularization methods such as Dropout, Mixup, and their enhanced variants are effective and prevalent, and achieve promising performance to overcome... | Fanshuang Kong, Richong Zhang, Xiaohui Guo, Samuel Mensah, Yongyi Mao |  |
| 713 |  |  [Cross-document Event Coreference Search: Task, Dataset and Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.58) |  | 0 | The task of Cross-document Coreference Resolution has been traditionally formulated as requiring to identify all coreference links across a given set of documents. We propose an appealing, and often more applicable, complementary set up for the task – Cross-document Coreference Search, focusing in... | Alon Eirew, Avi Caciularu, Ido Dagan |  |
| 714 |  |  [VIRT: Improving Representation-based Text Matching via Virtual Interaction](https://doi.org/10.18653/v1/2022.emnlp-main.59) |  | 0 | Text matching is a fundamental research problem in natural language understanding. Interaction-based approaches treat the text pair as a single sequence and encode it through cross encoders, while representation-based models encode the text pair independently with siamese or dual encoders.... | Dan Li, Yang Yang, Hongyin Tang, Jiahao Liu, Qifan Wang, Jingang Wang, Tong Xu, Wei Wu, Enhong Chen |  |
| 715 |  |  [MAVEN-ERE: A Unified Large-scale Dataset for Event Coreference, Temporal, Causal, and Subevent Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.60) |  | 0 | The diverse relationships among real-world events, including coreference, temporal, causal, and subevent relations, are fundamental to understanding natural languages. However, two drawbacks of existing datasets limit event relation extraction (ERE) tasks: (1) Small scale. Due to the annotation... | Xiaozhi Wang, Yulin Chen, Ning Ding, Hao Peng, Zimu Wang, Yankai Lin, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu, Peng Li, Jie Zhou |  |
| 716 |  |  [Entity Extraction in Low Resource Domains with Selective Pre-training of Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.61) |  | 0 | Transformer-based language models trained on large natural language corpora have been very useful in downstream entity extraction tasks. However, they often result in poor performances when applied to domains that are different from those they are pretrained on. Continued pretraining using... | Aniruddha Mahapatra, Sharmila Reddy Nangi, Aparna Garimella, Anandhavelu Natarajan |  |
| 717 |  |  [How Large Language Models are Transforming Machine-Paraphrase Plagiarism](https://doi.org/10.18653/v1/2022.emnlp-main.62) |  | 0 | The recent success of large language models for text generation poses a severe threat to academic integrity, as plagiarists can generate realistic paraphrases indistinguishable from original work.However, the role of large autoregressive models in generating machine-paraphrased plagiarism and their... | Jan Philip Wahle, Terry Ruas, Frederic Kirstein, Bela Gipp |  |
| 718 |  |  [M2D2: A Massively Multi-Domain Language Modeling Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.63) |  | 0 | We present M2D2, a fine-grained, massively multi-domain corpus for studying domain adaptation in language models (LMs). M2D2 consists of 8.5B tokens and spans 145 domains extracted from Wikipedia and Semantic Scholar. Using ontologies derived from Wikipedia and ArXiv categories, we organize the... | Machel Reid, Victor Zhong, Suchin Gururangan, Luke Zettlemoyer |  |
| 719 |  |  ["Will You Find These Shortcuts?" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.64) |  | 0 | Feature attribution a.k.a. input salience methods which assign an importance score to a feature are abundant but may produce surprisingly different results for the same model on the same input. While differences are expected if disparate definitions of importance are assumed, most methods claim to... | Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, Katja Filippova |  |
| 720 |  |  [Information-Transport-based Policy for Simultaneous Translation](https://doi.org/10.18653/v1/2022.emnlp-main.65) |  | 0 | Simultaneous translation (ST) outputs translation while receiving the source inputs, and hence requires a policy to determine whether to translate a target token or wait for the next source token. The major challenge of ST is that each target token can only be translated based on the current... | Shaolei Zhang, Yang Feng |  |
| 721 |  |  [Learning to Adapt to Low-Resource Paraphrase Generation](https://doi.org/10.18653/v1/2022.emnlp-main.66) |  | 0 | Paraphrase generation is a longstanding NLP task and achieves great success with the aid of large corpora. However, transferring a paraphrasing model to another domain encounters the problem of domain shifting especially when the data is sparse. At the same time, widely using large pre-trained... | Zhigen Li, Yanmeng Wang, Rizhao Fan, Ye Wang, Jianfeng Li, Shaojun Wang |  |
| 722 |  |  [A Distributional Lens for Multi-Aspect Controllable Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.67) |  | 0 | Multi-aspect controllable text generation is a more challenging and practical task than single-aspect control. Existing methods achieve complex multi-aspect control by fusing multiple controllers learned from single-aspect, but suffer from attribute degeneration caused by the mutual interference of... | Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Lingyuan Zhang, Heng Gong, Bing Qin |  |
| 723 |  |  [ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.68) |  | 0 | We study the text generation task under the approach of pre-trained language models (PLMs). Typically, an auto-regressive (AR) method is adopted for generating texts in a token-by-token manner. Despite many advantages of AR generation, it usually suffers from inefficient inference. Therefore,... | Junyi Li, Tianyi Tang, Wayne Xin Zhao, JianYun Nie, JiRong Wen |  |
| 724 |  |  [Multilingual Relation Classification via Efficient and Effective Prompting](https://doi.org/10.18653/v1/2022.emnlp-main.69) |  | 0 | Prompting pre-trained language models has achieved impressive performance on various NLP tasks, especially in low data regimes. Despite the success of prompting in monolingual settings, applying prompt-based methods in multilingual scenarios has been limited to a narrow set of tasks, due to the... | Yuxuan Chen, David Harbecke, Leonhard Hennig |  |
| 725 |  |  [Topic-Regularized Authorship Representation Learning](https://doi.org/10.18653/v1/2022.emnlp-main.70) |  | 0 | Authorship attribution is a task that aims to identify the author of a given piece of writing. We aim to develop a generalized solution that can handle a large number of texts from authors and topics unavailable in training data. Previous studies have proposed strategies to address only either... | Jitkapat Sawatphol, Nonthakit Chaiwong, Can Udomcharoenchaikit, Sarana Nutanong |  |
| 726 |  |  [Fine-grained Contrastive Learning for Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.71) |  | 0 | Recent relation extraction (RE) works have shown encouraging improvements by conducting contrastive learning on silver labels generated by distant supervision before fine-tuning on gold labels. Existing methods typically assume all these silver labels are accurate and treat them equally; however,... | William Hogan, Jiacheng Li, Jingbo Shang |  |
| 727 |  |  [Curriculum Prompt Learning with Self-Training for Abstractive Dialogue Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.72) |  | 0 | Succinctly summarizing dialogue is a task of growing interest, but inherent challenges, such as insufficient training data and low information density impede our ability to train abstractive models. In this work, we propose a novel curriculum-based prompt learning method with self-training to... | Changqun Li, Linlin Wang, Xin Lin, Gerard de Melo, Liang He |  |
| 728 |  |  [Zero-Shot Text Classification with Self-Training](https://doi.org/10.18653/v1/2022.emnlp-main.73) |  | 0 | Recent advances in large pretrained language models have increased attention to zero-shot text classification. In particular, models finetuned on natural language inference datasets have been widely adopted as zero-shot classifiers due to their promising results and off-the-shelf availability.... | Ariel Gera, Alon Halfon, Eyal Shnarch, Yotam Perlitz, Liat EinDor, Noam Slonim |  |
| 729 |  |  [Deconfounding Legal Judgment Prediction for European Court of Human Rights Cases Towards Better Alignment with Experts](https://doi.org/10.18653/v1/2022.emnlp-main.74) |  | 0 | This work demonstrates that Legal Judgement Prediction systems without expert-informed adjustments can be vulnerable to shallow, distracting surface signals that arise from corpus construction, case distribution, and confounding factors. To mitigate this, we use domain expertise to strategically... | Tokala Yaswanth Sri Sai Santosh, Shanshan Xu, Oana Ichim, Matthias Grabmair |  |
| 730 |  |  [SQuALITY: Building a Long-Document Summarization Dataset the Hard Way](https://doi.org/10.18653/v1/2022.emnlp-main.75) |  | 0 | Summarization datasets are often assembled either by scraping naturally occurring public-domain summaries—which are nearly always in difficult-to-work-with technical domains—or by using approximate heuristics to extract them from everyday text—which frequently yields unfaithful summaries. In this... | Alex Wang, Richard Yuanzhe Pang, Angelica Chen, Jason Phang, Samuel R. Bowman |  |
| 731 |  |  [MetaASSIST: Robust Dialogue State Tracking with Meta Learning](https://doi.org/10.18653/v1/2022.emnlp-main.76) |  | 0 | Existing dialogue datasets contain lots of noise in their state annotations. Such noise can hurt model training and ultimately lead to poor generalization performance. A general framework named ASSIST has recently been proposed to train robust dialogue state tracking (DST) models. It introduces an... | Fanghua Ye, Xi Wang, Jie Huang, Shenghui Li, Samuel Stern, Emine Yilmaz |  |
| 732 |  |  [Multilingual Machine Translation with Hyper-Adapters](https://doi.org/10.18653/v1/2022.emnlp-main.77) |  | 0 | Multilingual machine translation suffers from negative interference across languages. A common solution is to relax parameter sharing with language-specific modules like adapters. However, adapters of related languages are unable to transfer information, and their total number of parameters becomes... | Christos Baziotis, Mikel Artetxe, James Cross, Shruti Bhosale |  |
| 733 |  |  [Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination](https://doi.org/10.18653/v1/2022.emnlp-main.78) |  | 0 | Large-scale pretrained language models have made significant advances in solving downstream language understanding tasks. However, they generally suffer from reporting bias, the phenomenon describing the lack of explicit commonsense knowledge in written text, e.g., ”an orange is orange”. To... | Yue Yang, Wenlin Yao, Hongming Zhang, Xiaoyang Wang, Dong Yu, Jianshu Chen |  |
| 734 |  |  [Using Commonsense Knowledge to Answer Why-Questions](https://doi.org/10.18653/v1/2022.emnlp-main.79) |  | 0 | Answering questions in narratives about why events happened often requires commonsense knowledge external to the text. What aspects of this knowledge are available in large language models? What aspects can be made accessible via external commonsense resources? We study these questions in the... | Yash Kumar Lal, Niket Tandon, Tanvi Aggarwal, Horace Liu, Nathanael Chambers, Raymond J. Mooney, Niranjan Balasubramanian |  |
| 735 |  |  [Affective Idiosyncratic Responses to Music](https://doi.org/10.18653/v1/2022.emnlp-main.80) |  | 0 | Affective responses to music are highly personal. Despite consensus that idiosyncratic factors play a key role in regulating how listeners emotionally respond to music, precisely measuring the marginal effects of these variables has proved challenging. To address this gap, we develop computational... | Sky CHWang, Evan Li, Oliver Li, Smaranda Muresan, Zhou Yu |  |
| 736 |  |  [Successive Prompting for Decomposing Complex Questions](https://doi.org/10.18653/v1/2022.emnlp-main.81) |  | 0 | Answering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output... | Dheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner |  |
| 737 |  |  [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://doi.org/10.18653/v1/2022.emnlp-main.82) |  | 0 | Pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which... | Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi |  |
| 738 |  |  [DANLI: Deliberative Agent for Following Natural Language Instructions](https://doi.org/10.18653/v1/2022.emnlp-main.83) |  | 0 | Recent years have seen an increasing amount of work on embodied AI agents that can perform tasks by following human language instructions. However, most of these agents are reactive, meaning that they simply learn and imitate behaviors encountered in the training data. These reactive agents are... | Yichi Zhang, Jianing Yang, Jiayi Pan, Shane Storks, Nikhil Devraj, Ziqiao Ma, Keunwoo Peter Yu, Yuwei Bao, Joyce Chai |  |
| 739 |  |  [Tracing Semantic Variation in Slang](https://doi.org/10.18653/v1/2022.emnlp-main.84) |  | 0 | The meaning of a slang term can vary in different communities. However, slang semantic variation is not well understood and under-explored in the natural language processing of slang. One existing view argues that slang semantic variation is driven by culture-dependent communicative needs. An... | Zhewei Sun, Yang Xu |  |
| 740 |  |  [Fine-grained Category Discovery under Coarse-grained supervision with Hierarchical Weighted Self-contrastive Learning](https://doi.org/10.18653/v1/2022.emnlp-main.85) |  | 0 | Novel category discovery aims at adapting models trained on known categories to novel categories. Previous works only focus on the scenario where known and novel categories are of the same granularity.In this paper, we investigate a new practical scenario called Fine-grained Category Discovery... | Wenbin An, Feng Tian, Ping Chen, Siliang Tang, Qinghua Zheng, Qianying Wang |  |
| 741 |  |  [PLM-based World Models for Text-based Games](https://doi.org/10.18653/v1/2022.emnlp-main.86) |  | 0 | World models have improved the ability of reinforcement learning agents to operate in a sample efficient manner, by being trained to predict plausible changes in the underlying environment. As the core tasks of world models are future prediction and commonsense understanding, our claim is that... | Minsoo Kim, YeonJoon Jung, Dohyeon Lee, Seungwon Hwang |  |
| 742 |  |  [Prompt-Based Meta-Learning For Few-shot Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.87) |  | 0 | Few-shot Text Classification predicts the semantic label of a given text with a handful of supporting instances. Current meta-learning methods have achieved satisfying results in various few-shot situations. Still, they often require a large amount of data to construct many few-shot tasks for... | Haoxing Zhang, Xiaofeng Zhang, Haibo Huang, Lei Yu |  |
| 743 |  |  [How well can Text-to-Image Generative Models understand Ethical Natural Language Interventions?](https://doi.org/10.18653/v1/2022.emnlp-main.88) |  | 0 | Text-to-image generative models have achieved unprecedented success in generating high-quality images based on natural language descriptions. However, it is shown that these models tend to favor specific social groups when prompted with neutral text descriptions (e.g., ‘a photo of a lawyer’).... | Hritik Bansal, Da Yin, Masoud Monajatipoor, KaiWei Chang |  |
| 744 |  |  [Geographic Citation Gaps in NLP Research](https://doi.org/10.18653/v1/2022.emnlp-main.89) |  | 0 | In a fair world, people have equitable opportunities to education, to conduct scientific research, to publish, and to get credit for their work, regardless of where they live. However, it is common knowledge among researchers that a vast number of papers accepted at top NLP venues come from a... | Mukund Rungta, Janvijay Singh, Saif M. Mohammad, Diyi Yang |  |
| 745 |  |  [Language Models of Code are Few-Shot Commonsense Learners](https://doi.org/10.18653/v1/2022.emnlp-main.90) |  | 0 | We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph.To employ large language models (LMs) for this task, existing approaches ‘serialize’ the output graph as a flat list of nodes and... | Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig |  |
| 746 |  |  [Numerical Optimizations for Weighted Low-rank Estimation on Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.91) |  | 0 | Singular value decomposition (SVD) is one of the most popular compression methods that approximate a target matrix with smaller matrices. However, standard SVD treats the parameters within the matrix with equal importance, which is a simple but unrealistic assumption. The parameters of a trained... | Ting Hua, YenChang Hsu, Felicity Wang, Qian Lou, Yilin Shen, Hongxia Jin |  |
| 747 |  |  [Generative Multi-hop Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.92) |  | 0 | A common practice for text retrieval is to use an encoder to map the documents and the query to a common vector space and perform a nearest neighbor search (NNS); multi-hop retrieval also often adopts the same paradigm, usually with a modification of iteratively reformulating the query vector so... | Hyunji Lee, Sohee Yang, Hanseok Oh, Minjoon Seo |  |
| 748 |  |  [Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.93) |  | 0 | Image-to-text tasks such as open-ended image captioning and controllable image description have received extensive attention for decades. Here we advance this line of work further, presenting Visual Spatial Description (VSD), a new perspective for image-to-text toward spatial semantics. Given an... | Yu Zhao, Jianguo Wei, Zhichao Lin, Yueheng Sun, Meishan Zhang, Min Zhang |  |
| 749 |  |  [M3: A Multi-View Fusion and Multi-Decoding Network for Multi-Document Reading Comprehension](https://doi.org/10.18653/v1/2022.emnlp-main.94) |  | 0 | Multi-document reading comprehension task requires collecting evidences from different documents for answering questions. Previous research works either use the extractive modeling method to naively integrate the scores from different documents on the encoder side or use the generative modeling... | Liang Wen, Houfeng Wang, Yingwei Luo, Xiaolin Wang |  |
| 750 |  |  [COCO-DR: Combating the Distribution Shift in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning](https://doi.org/10.18653/v1/2022.emnlp-main.95) |  | 0 | We present a new zero-shot dense retrieval (ZeroDR) method, COCO-DR, to improve the generalization ability of dense retrieval by combating the distribution shifts between source training tasks and target scenarios. To mitigate the impact of document differences, COCO-DR continues pretraining the... | Yue Yu, Chenyan Xiong, Si Sun, Chao Zhang, Arnold Overwijk |  |
| 751 |  |  [Language Model Pre-Training with Sparse Latent Typing](https://doi.org/10.18653/v1/2022.emnlp-main.96) |  | 0 | Modern large-scale Pre-trained Language Models (PLMs) have achieved tremendous success on a wide range of downstream tasks. However, most of the LM pre-training objectives only focus on text reconstruction, but have not sought to learn latent-level interpretable representations of sentences. In... | Liliang Ren, Zixuan Zhang, Han Wang, Clare R. Voss, ChengXiang Zhai, Heng Ji |  |
| 752 |  |  [On the Transformation of Latent Space in Fine-Tuned NLP Models](https://doi.org/10.18653/v1/2022.emnlp-main.97) |  | 0 | We study the evolution of latent space in fine-tuned NLP models. Different from the commonly used probing-framework, we opt for an unsupervised method to analyze representations. More specifically, we discover latent concepts in the representational space using hierarchical clustering. We then use... | Nadir Durrani, Hassan Sajjad, Fahim Dalvi, Firoj Alam |  |
| 753 |  |  [Watch the Neighbors: A Unified K-Nearest Neighbor Contrastive Learning Framework for OOD Intent Discovery](https://doi.org/10.18653/v1/2022.emnlp-main.98) |  | 0 | Discovering out-of-domain (OOD) intent is important for developing new skills in task-oriented dialogue systems. The key challenges lie in how to transfer prior in-domain (IND) knowledge to OOD clustering, as well as jointly learn OOD representations and cluster assignments. Previous methods suffer... | Yutao Mou, Keqing He, Pei Wang, Yanan Wu, Jingang Wang, Wei Wu, Weiran Xu |  |
| 754 |  |  [Extracted BERT Model Leaks More Information than You Think!](https://doi.org/10.18653/v1/2022.emnlp-main.99) |  | 0 | The collection and availability of big data, combined with advances in pre-trained models (e.g. BERT), have revolutionized the predictive performance of natural language processing tasks. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned... | Xuanli He, Lingjuan Lyu, Chen Chen, Qiongkai Xu |  |
| 755 |  |  [Do Vision-and-Language Transformers Learn Grounded Predicate-Noun Dependencies?](https://doi.org/10.18653/v1/2022.emnlp-main.100) |  | 0 | Recent advances in vision-and-language modeling have seen the development of Transformer architectures that achieve remarkable performance on multimodal reasoning tasks.Yet, the exact capabilities of these black-box models are still poorly understood. While much of previous work has focused on... | Mitja Nikolaus, Emmanuelle Salin, Stéphane Ayache, Abdellah Fourtassi, Benoît Favre |  |
| 756 |  |  [A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference](https://doi.org/10.18653/v1/2022.emnlp-main.101) |  | 0 | Most evaluations of attribution methods focus on the English language. In this work, we present a multilingual approach for evaluating attribution methods for the Natural Language Inference (NLI) task in terms of faithfulness and plausibility.First, we introduce a novel cross-lingual strategy to... | Kerem Zaman, Yonatan Belinkov |  |
| 757 |  |  [Graph-Based Multilingual Label Propagation for Low-Resource Part-of-Speech Tagging](https://doi.org/10.18653/v1/2022.emnlp-main.102) |  | 0 | Part-of-Speech (POS) tagging is an important component of the NLP pipeline, but many low-resource languages lack labeled data for training. An established method for training a POS tagger in such a scenario is to create a labeled training set by transferring from high-resource languages. In this... | Ayyoob Imani, Silvia Severini, Masoud Jalili Sabet, François Yvon, Hinrich Schütze |  |
| 758 |  |  [SubeventWriter: Iterative Sub-event Sequence Generation with Coherence Controller](https://doi.org/10.18653/v1/2022.emnlp-main.103) |  | 0 | In this paper, we propose a new task of sub-event generation for an unseen process to evaluate the understanding of the coherence of sub-event actions and objects. To solve the problem, we design SubeventWriter, a sub-event sequence generation framework with a coherence controller. Given an unseen... | Zhaowei Wang, Hongming Zhang, Tianqing Fang, Yangqiu Song, Ginny Y. Wong, Simon See |  |
| 759 |  |  [Infinite SCAN: An Infinite Model of Diachronic Semantic Change](https://doi.org/10.18653/v1/2022.emnlp-main.104) |  | 0 | In this study, we propose a Bayesian model that can jointly estimate the number of senses of words and their changes through time.The model combines a dynamic topic model on Gaussian Markov random fields with a logistic stick-breaking process that realizes Dirichlet process. In the experiments, we... | Seiichi Inoue, Mamoru Komachi, Toshinobu Ogiso, Hiroya Takamura, Daichi Mochihashi |  |
| 760 |  |  [Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization](https://doi.org/10.18653/v1/2022.emnlp-main.105) |  | 0 | Training language models to learn from human instructions for zero-shot cross-task generalization has attracted much attention in NLP communities. Recently, instruction tuning (IT), which fine-tunes a pre-trained language model on a massive collection of tasks described via human-craft... | Yuxian Gu, Pei Ke, Xiaoyan Zhu, Minlie Huang |  |
| 761 |  |  [Counterfactual Data Augmentation via Perspective Transition for Open-Domain Dialogues](https://doi.org/10.18653/v1/2022.emnlp-main.106) |  | 0 | The construction of open-domain dialogue systems requires high-quality dialogue datasets. The dialogue data admits a wide variety of responses for a given dialogue history, especially responses with different semantics. However, collecting high-quality such a dataset in most scenarios is... | Jiao Ou, Jinchao Zhang, Yang Feng, Jie Zhou |  |
| 762 |  |  [SQUIRE: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.107) |  | 0 | Multi-hop knowledge graph (KG) reasoning has been widely studied in recent years to provide interpretable predictions on missing links with evidential paths. Most previous works use reinforcement learning (RL) based methods that learn to navigate the path towards the target entity. However, these... | Yushi Bai, Xin Lv, Juanzi Li, Lei Hou, Yincen Qu, Zelin Dai, Feiyu Xiong |  |
| 763 |  |  [SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training](https://doi.org/10.18653/v1/2022.emnlp-main.108) |  | 0 | The rapid development of single-modal pre-training has prompted researchers to pay more attention to cross-modal pre-training methods. In this paper, we propose a unified-modal speech-unit-text pre-training model, SpeechUT, to connect the representations of a speech encoder and a text decoder with... | Ziqiang Zhang, Long Zhou, Junyi Ao, Shujie Liu, Lirong Dai, Jinyu Li, Furu Wei |  |
| 764 |  |  [Learning Label Modular Prompts for Text Classification in the Wild](https://doi.org/10.18653/v1/2022.emnlp-main.109) |  | 0 | Machine learning models usually assume i.i.d data during training and testing, but data and tasks in real world often change over time. To emulate the transient nature of real world, we propose a challenging but practical task: text classification in-the-wild, which introduces different... | Hailin Chen, Amrita Saha, Shafiq R. Joty, Steven C. H. Hoi |  |
| 765 |  |  [Unbiased and Efficient Sampling of Dependency Trees](https://doi.org/10.18653/v1/2022.emnlp-main.110) |  | 0 | Most computational models of dependency syntax consist of distributions over spanning trees. However, the majority of dependency treebanks require that every valid dependency tree has a single edge coming out of the ROOT node, a constraint that is not part of the definition of spanning trees. For... | Milos Stanojevic |  |
| 766 |  |  [Continual Learning of Neural Machine Translation within Low Forgetting Risk Regions](https://doi.org/10.18653/v1/2022.emnlp-main.111) |  | 0 | This paper considers continual learning of large-scale pretrained neural machine translation model without accessing the previous training data or introducing model separation. We argue that the widely used regularization-based methods, which perform multi-objective learning with an auxiliary loss,... | Shuhao Gu, Bojie Hu, Yang Feng |  |
| 767 |  |  [COST-EFF: Collaborative Optimization of Spatial and Temporal Efficiency with Slenderized Multi-exit Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.112) |  | 0 | Transformer-based pre-trained language models (PLMs) mostly suffer from excessive overhead despite their advanced capacity. For resource-constrained devices, there is an urgent need for a spatially and temporally efficient model which retains the major capacity of PLMs. However, existing statically... | Bowen Shen, Zheng Lin, Yuanxin Liu, Zhengxiao Liu, Lei Wang, Weiping Wang |  |
| 768 |  |  [Rescue Implicit and Long-tail Cases: Nearest Neighbor Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.113) |  | 0 | Relation extraction (RE) has achieved remarkable progress with the help of pre-trained language models. However, existing RE models are usually incapable of handling two situations: implicit expressions and long-tail relation types, caused by language complexity and data sparsity. In this paper, we... | Zhen Wan, Qianying Liu, Zhuoyuan Mao, Fei Cheng, Sadao Kurohashi, Jiwei Li |  |
| 769 |  |  [StoryER: Automatic Story Evaluation via Ranking, Rating and Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.114) |  | 0 | Existing automatic story evaluation methods place a premium on story lexical level coherence, deviating from human preference.We go beyond this limitation by considering a novel Story Evaluation method that mimics human preference when judging a story, namely StoryER, which consists of three... | Hong Chen, Duc Minh Vo, Hiroya Takamura, Yusuke Miyao, Hideki Nakayama |  |
| 770 |  |  [Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference](https://doi.org/10.18653/v1/2022.emnlp-main.115) |  | 0 | While large pre-trained language models are powerful, their predictions often lack logical consistency across test inputs. For example, a state-of-the-art Macaw question-answering (QA) model answers <i>Yes</i> to <i>Is a sparrow a bird?</i> and <i>Does a bird have feet?</i> but answers <i>No</i> to... | Eric Mitchell, Joseph J. Noh, Siyan Li, William S. Armstrong, Ananth Agarwal, Patrick Liu, Chelsea Finn, Christopher D. Manning |  |
| 771 |  |  [Robustness of Demonstration-based Learning Under Limited Data Scenario](https://doi.org/10.18653/v1/2022.emnlp-main.116) |  | 0 | Demonstration-based learning has shown great potential in stimulating pretrained language models’ ability under limited data scenario. Simply augmenting the input with some demonstrations can significantly improve performance on few-shot NER. However, why such demonstrations are beneficial for the... | Hongxin Zhang, Yanzhe Zhang, Ruiyi Zhang, Diyi Yang |  |
| 772 |  |  [Modeling Information Change in Science Communication with Semantically Matched Paraphrases](https://doi.org/10.18653/v1/2022.emnlp-main.117) |  | 0 | Whether the media faithfully communicate scientific information has long been a core issue to the science community. Automatically identifying paraphrased scientific findings could enable large-scale tracking and analysis of information changes in the science communication process, but this... | Dustin Wright, Jiaxin Pei, David Jurgens, Isabelle Augenstein |  |
| 773 |  |  [Word Order Matters When You Increase Masking](https://doi.org/10.18653/v1/2022.emnlp-main.118) |  | 0 | Word order, an essential property of natural languages, is injected in Transformer-based neural language models using position encoding. However, recent experiments have shown that explicit position encoding is not always useful, since some models without such feature managed to achieve... | Karim Lasri, Alessandro Lenci, Thierry Poibeau |  |
| 774 |  |  [An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.119) |  | 0 | Large language models are shown to present privacy risks through memorization of training data, andseveral recent works have studied such risks for the pre-training phase. Little attention, however, has been given to the fine-tuning phase and it is not well understood how different fine-tuning... | Fatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David E. Evans, Taylor BergKirkpatrick |  |
| 775 |  |  [Style Transfer as Data Augmentation: A Case Study on Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.120) |  | 0 | In this work, we take the named entity recognition task in the English language as a case study and explore style transfer as a data augmentation method to increase the size and diversity of training data in low-resource scenarios. We propose a new method to effectively transform the text from a... | Shuguang Chen, Leonardo Neves, Thamar Solorio |  |
| 776 |  |  [Linguistic Corpus Annotation for Automatic Text Simplification Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.121) |  | 0 | Evaluating automatic text simplification (ATS) systems is a difficult task that is either performed by automatic metrics or user-based evaluations. However, from a linguistic point-of-view, it is not always clear on what bases these evaluations operate. In this paper, we propose annotations of the... | Rémi Cardon, Adrien Bibal, Rodrigo Wilkens, David Alfter, Magali Norré, Adeline Müller, Patrick Watrin, Thomas François |  |
| 777 |  |  [Semantic Framework based Query Generation for Temporal Question Answering over Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.122) |  | 0 | Answering factual questions with temporal intent over knowledge graphs (temporal KGQA) attracts rising attention in recent years.In the generation of temporal queries, existing KGQA methods ignore the fact that some intrinsic connections between events can make them temporally related, which may... | Wentao Ding, Hao Chen, Huayu Li, Yuzhong Qu |  |
| 778 |  |  [There Is No Standard Answer: Knowledge-Grounded Dialogue Generation with Adversarial Activated Multi-Reference Learning](https://doi.org/10.18653/v1/2022.emnlp-main.123) |  | 0 | Knowledge-grounded dialogue (KGC) shows excellent potential to deliver an engaging and informative response. However, existing approaches emphasize selecting one golden knowledge given a particular dialogue context, overlooking the one-to-many phenomenon in dialogue. As a result, existing paradigm... | Xueliang Zhao, Tingchen Fu, Chongyang Tao, Rui Yan |  |
| 779 |  |  [Stop Measuring Calibration When Humans Disagree](https://doi.org/10.18653/v1/2022.emnlp-main.124) |  | 0 | Calibration is a popular framework to evaluate whether a classifier knows when it does not know - i.e., its predictive probabilities are a good indication of how likely a prediction is to be correct. Correctness is commonly estimated against the human majority class. Recently, calibration to human... | Joris Baan, Wilker Aziz, Barbara Plank, Raquel Fernández |  |
| 780 |  |  [Improving compositional generalization for multi-step quantitative reasoning in question answering](https://doi.org/10.18653/v1/2022.emnlp-main.125) |  | 0 | Quantitative reasoning is an important aspect of question answering, especially when numeric and verbal cues interact to indicate sophisticated, multi-step programs. In this paper, we demonstrate how modeling the compositional nature of quantitative text can enhance the performance and robustness... | Armineh Nourbakhsh, Cathy Jiao, Sameena Shah, Carolyn P. Rosé |  |
| 781 |  |  [A Comprehensive Comparison of Neural Networks as Cognitive Models of Inflection](https://doi.org/10.18653/v1/2022.emnlp-main.126) |  | 0 | Neural networks have long been at the center of a debate around the cognitive mechanism by which humans process inflectional morphology. This debate has gravitated into NLP by way of the question: Are neural networks a feasible account for human behavior in morphological inflection?We address that... | Adam Wiemerslage, Shiran Dudy, Katharina Kann |  |
| 782 |  |  [Can Visual Context Improve Automatic Speech Recognition for an Embodied Agent?](https://doi.org/10.18653/v1/2022.emnlp-main.127) |  | 0 | The usage of automatic speech recognition (ASR) systems are becoming omnipresent ranging from personal assistant to chatbots, home, and industrial automation systems, etc. Modern robots are also equipped with ASR capabilities for interacting with humans as speech is the most natural interaction... | Pradip Pramanick, Chayan Sarkar |  |
| 783 |  |  [AfroLID: A Neural Language Identification Tool for African Languages](https://doi.org/10.18653/v1/2022.emnlp-main.128) |  | 0 | Language identification (LID) is a crucial precursor for NLP, especially for mining web data. Problematically, most of the world’s 7000+ languages today are not covered by LID technologies. We address this pressing issue for Africa by introducing AfroLID, a neural LID toolkit for 517 African... | Ife Adebara, AbdelRahim A. Elmadany, Muhammad AbdulMageed, Alcides Alcoba Inciarte |  |
| 784 |  |  [EvEntS ReaLM: Event Reasoning of Entity States via Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.129) |  | 0 | This paper investigates models of event implications. Specifically, how well models predict entity state-changes, by targeting their understanding of physical attributes. Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our... | Evangelia Spiliopoulou, Artidoro Pagnoni, Yonatan Bisk, Eduard H. Hovy |  |
| 785 |  |  [Large language models are few-shot clinical information extractors](https://doi.org/10.18653/v1/2022.emnlp-main.130) |  | 0 | A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such... | Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, David A. Sontag |  |
| 786 |  |  [Towards a Unified Multi-Dimensional Evaluator for Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.131) |  | 0 | Multi-dimensional evaluation is the dominant paradigm for human evaluation in Natural Language Generation (NLG), i.e., evaluating the generated text from multiple explainable dimensions, such as coherence and fluency. However, automatic evaluation in NLG is still dominated by similarity-based... | Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, Jiawei Han |  |
| 787 |  |  [GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.132) |  | 0 | Recent work has shown that Pre-trained Language Models (PLMs) store the relational knowledge learned from data and utilize it for performing downstream tasks. However, commonsense knowledge across different regions may vary. For instance, the color of bridal dress is white in American weddings... | Da Yin, Hritik Bansal, Masoud Monajatipoor, Liunian Harold Li, KaiWei Chang |  |
| 788 |  |  [The (Undesired) Attenuation of Human Biases by Multilinguality](https://doi.org/10.18653/v1/2022.emnlp-main.133) |  | 0 | Some human preferences are universal. The odor of vanilla is perceived as pleasant all around the world. We expect neural models trained on human texts to exhibit these kind of preferences, i.e. biases, but we show that this is not always the case. We explore 16 static and contextual embedding... | Cristina EspañaBonet, Alberto BarrónCedeño |  |
| 789 |  |  [Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.134) |  | 0 | Our goal is a question-answering (QA) system that can show how its answers are implied by its own internal beliefs via a systematic chain of reasoning. Such a capability would allow better understanding of why a model produced the answer it did. Our approach is to recursively combine a trained... | Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark |  |
| 790 |  |  [Near-Negative Distinction: Giving a Second Life to Human Evaluation Datasets](https://doi.org/10.18653/v1/2022.emnlp-main.135) |  | 0 | Precisely assessing the progress in natural language generation (NLG) tasks is challenging, and human evaluation to establish a preference in a model’s output over another is often necessary.However, human evaluation is usually costly, difficult to reproduce, and non-reusable.In this paper, we... | Philippe Laban, ChienSheng Wu, Wenhao Liu, Caiming Xiong |  |
| 791 |  |  [ToKen: Task Decomposition and Knowledge Infusion for Few-Shot Hate Speech Detection](https://doi.org/10.18653/v1/2022.emnlp-main.136) |  | 0 | Hate speech detection is complex; it relies on commonsense reasoning, knowledge of stereotypes, and an understanding of social nuance that differs from one culture to the next. It is also difficult to collect a large-scale hate speech annotated dataset. In this work, we frame this problem as a... | Badr AlKhamissi, Faisal Ladhak, Srini Iyer, Veselin Stoyanov, Zornitsa Kozareva, Xian Li, Pascale Fung, Lambert Mathias, Asli Celikyilmaz, Mona T. Diab |  |
| 792 |  |  [Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations](https://doi.org/10.18653/v1/2022.emnlp-main.137) |  | 0 | Recent work on explainable NLP has shown that few-shot prompting can enable large pre-trained language models (LLMs) to generate grammatical and factual natural language explanations for data labels. In this work, we study the connection between explainability and sample hardness by investigating... | Swarnadeep Saha, Peter Hase, Nazneen Rajani, Mohit Bansal |  |
| 793 |  |  [Stanceosaurus: Classifying Stance Towards Multicultural Misinformation](https://doi.org/10.18653/v1/2022.emnlp-main.138) |  | 0 | We present Stanceosaurus, a new corpus of 28,033 tweets in English, Hindi and Arabic annotated with stance towards 250 misinformation claims. As far as we are aware, it is the largest corpus annotated with stance towards misinformation claims. The claims in Stanceosaurus originate from 15... | Jonathan Zheng, Ashutosh Baheti, Tarek Naous, Wei Xu, Alan Ritter |  |
| 794 |  |  [Gendered Mental Health Stigma in Masked Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.139) |  | 0 | Mental health stigma prevents many individuals from receiving the appropriate care, and social psychology studies have shown that mental health tends to be overlooked in men. In this work, we investigate gendered mental health stigma in masked language models. In doing so, we operationalize mental... | Inna W. Lin, Lucille Njoo, Anjalie Field, Ashish Sharma, Katharina Reinecke, Tim Althoff, Yulia Tsvetkov |  |
| 795 |  |  [Efficient Nearest Neighbor Search for Cross-Encoder Models using Matrix Factorization](https://doi.org/10.18653/v1/2022.emnlp-main.140) |  | 0 | Efficient k-nearest neighbor search is a fundamental task, foundational for many problems in NLP. When the similarity is measured by dot-product between dual-encoder vectors or L2-distance, there already exist many scalable and efficient search methods. But not so when similarity is measured by... | Nishant Yadav, Nicholas Monath, Rico Angell, Manzil Zaheer, Andrew McCallum |  |
| 796 |  |  [Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.141) |  | 0 | We propose a method for arbitrary textual style transfer (TST)—the task of transforming a text into any given style—utilizing general-purpose pre-trained language models. Our method, Prompt-and-Rerank, is based on a mathematical formulation of the TST task, decomposing it into three constituent... | Mirac Suzgun, Luke MelasKyriazi, Dan Jurafsky |  |
| 797 |  |  [Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts](https://doi.org/10.18653/v1/2022.emnlp-main.142) |  | 0 | Explicit decomposition modeling, which involves breaking down complex tasks into more straightforward and often more interpretable sub-tasks, has long been a central theme in developing robust and interpretable NLU systems. However, despite the many datasets and resources built as part of this... | Ben Zhou, Kyle Richardson, Xiaodong Yu, Dan Roth |  |
| 798 |  |  [Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality](https://doi.org/10.18653/v1/2022.emnlp-main.143) |  | 0 | Recent visuolinguistic pre-trained models show promising progress on various end tasks such as image retrieval and video captioning. Yet, they fail miserably on the recently proposed Winoground dataset, which challenges models to match paired images and English captions, with items constructed to... | Anuj Diwan, Layne Berry, Eunsol Choi, David Harwath, Kyle Mahowald |  |
| 799 |  |  [Gradient-based Constrained Sampling from Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.144) |  | 0 | Large pretrained language models are successful at generating fluent text but are notoriously hard to controllably sample from. In this work, we study constrained sampling from such language models, i.e., generating text that satisfies user-defined constraints, while maintaining fluency and model’s... | Sachin Kumar, Biswajit Paria, Yulia Tsvetkov |  |
| 800 |  |  [TaCube: Pre-computing Data Cubes for Answering Numerical-Reasoning Questions over Tabular Data](https://doi.org/10.18653/v1/2022.emnlp-main.145) |  | 0 | Existing auto-regressive pre-trained language models (PLMs) like T5 and BART, have been well applied to table question answering by UNIFIEDSKG and TAPEX, respectively, and demonstrated state-of-the-art results on multiple benchmarks. However, auto-regressive PLMs are challenged by recent emerging... | Fan Zhou, Mengkang Hu, Haoyu Dong, Zhoujun Cheng, Fan Cheng, Shi Han, Dongmei Zhang |  |
| 801 |  |  [Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence](https://doi.org/10.18653/v1/2022.emnlp-main.146) |  | 0 | Question answering models can use rich knowledge sources — up to one hundred retrieved passages and parametric knowledge in the large-scale language model (LM). Prior work assumes information in such knowledge sources is consistent with each other, paying little attention to how models blend... | HungTing Chen, Michael J. Q. Zhang, Eunsol Choi |  |
| 802 |  |  [QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation](https://doi.org/10.18653/v1/2022.emnlp-main.147) |  | 0 | Question answering (QA) has recently shown impressive results for answering questions from customized domains. Yet, a common challenge is to adapt QA models to an unseen target domain. In this paper, we propose a novel self-supervised framework called QADA for QA domain adaptation. QADA introduces... | Zhenrui Yue, Huimin Zeng, Bernhard Kratzwald, Stefan Feuerriegel, Dong Wang |  |
| 803 |  |  [When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain](https://doi.org/10.18653/v1/2022.emnlp-main.148) |  | 0 | Pre-trained language models have shown impressive performance on a variety of tasks and domains. Previous research on financial language models usually employs a generic training scheme to train standard model architectures, without completely leveraging the richness of the financial data. We... | Raj Sanjay Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Natraj Raman, Charese Smiley, Jiaao Chen, Diyi Yang |  |
| 804 |  |  [Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer](https://doi.org/10.18653/v1/2022.emnlp-main.149) |  | 0 | Systems for knowledge-intensive tasks such as open-domain question answering (QA) usually consist of two stages: efficient retrieval of relevant documents from a large corpus and detailed reading of the selected documents. This is usually done through two separate models, a retriever that encodes... | Zhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun Araki, Haibo Ding, Jamie Callan, Graham Neubig |  |
| 805 |  |  [Reproducibility in Computational Linguistics: Is Source Code Enough?](https://doi.org/10.18653/v1/2022.emnlp-main.150) |  | 0 | The availability of source code has been put forward as one of the most critical factors for improving the reproducibility of scientific research. This work studies trends in source code availability at major computational linguistics conferences, namely, ACL, EMNLP, LREC, NAACL, and COLING. We... | Mohammad Arvan, Luís Pina, Natalie Parde |  |
| 806 |  |  [Generating Information-Seeking Conversations from Unlabeled Documents](https://doi.org/10.18653/v1/2022.emnlp-main.151) |  | 0 | Synthesizing datasets for conversational question answering (CQA) from unlabeled documents remains challenging due to its interactive nature.Moreover, while modeling information needs is an essential key, only few studies have discussed it.In this paper, we introduce a novel framework,... | Gangwoo Kim, Sungdong Kim, Kang Min Yoo, Jaewoo Kang |  |
| 807 |  |  [Distill The Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.152) |  | 0 | Past works on multimodal machine translation (MMT) elevate bilingual setup by incorporating additional aligned vision information.However, an image-must requirement of the multimodal dataset largely hinders MMT’s development — namely that it demands an aligned form of [image, source text, target... | Ru Peng, Yawen Zeng, Jake Zhao |  |
| 808 |  |  [A Multifaceted Framework to Evaluate Evasion, Content Preservation, and Misattribution in Authorship Obfuscation Techniques](https://doi.org/10.18653/v1/2022.emnlp-main.153) |  | 0 | Authorship obfuscation techniques have commonly been evaluated based on their ability to hide the author’s identity (evasion) while preserving the content of the original text. However, to avoid overstating the systems’ effectiveness, evasion detection must be evaluated using competitive... | Malik H. Altakrori, Thomas Scialom, Benjamin C. M. Fung, Jackie Chi Kit Cheung |  |
| 809 |  |  [SafeText: A Benchmark for Exploring Physical Safety in Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.154) |  | 0 | Understanding what constitutes safe text is an important issue in natural language processing and can often prevent the deployment of models deemed harmful and unsafe. One such type of safety that has been scarcely studied is commonsense physical safety, i.e. text that is not explicitly violent and... | Sharon Levy, Emily Allaway, Melanie Subbiah, Lydia B. Chilton, Desmond Patton, Kathleen R. McKeown, William Yang Wang |  |
| 810 |  |  [Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations](https://doi.org/10.18653/v1/2022.emnlp-main.155) |  | 0 | Despite recent explosion of interests in in-context learning, the underlying mechanism and the precise impact of the quality of demonstrations remain elusive.Intuitively, ground-truth labels should have as much impact in in-context learning (ICL) as supervised learning, but recent work reported... | Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, SangWoo Lee, Sanggoo Lee, Taeuk Kim |  |
| 811 |  |  [D4: a Chinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat](https://doi.org/10.18653/v1/2022.emnlp-main.156) |  | 0 | In a depression-diagnosis-directed clinical session, doctors initiate a conversation with ample emotional support that guides the patients to expose their symptoms based on clinical diagnosis criteria. Such a dialogue system is distinguished from existing single-purpose human-machine dialog... | Binwei Yao, Chao Shi, Likai Zou, Lingfeng Dai, Mengyue Wu, Lu Chen, Zhen Wang, Kai Yu |  |
| 812 |  |  [Exploiting domain-slot related keywords description for Few-Shot Cross-Domain Dialogue State Tracking](https://doi.org/10.18653/v1/2022.emnlp-main.157) |  | 0 | Collecting dialogue data with domain-slot-value labels for dialogue state tracking (DST) could be a costly process. In this paper, we propose a novel framework based on domain-slot related description to tackle the challenge of few-shot cross-domain DST. Specifically, we design an extraction module... | QiXiang Gao, Guanting Dong, Yutao Mou, Liwen Wang, Chen Zeng, Daichi Guo, Mingyang Sun, Weiran Xu |  |
| 813 |  |  [CoCoa: An Encoder-Decoder Model for Controllable Code-switched Generation](https://doi.org/10.18653/v1/2022.emnlp-main.158) |  | 0 | Code-switching has seen growing interest in recent years as an important multilingual NLP phenomenon. Generating code-switched text for data augmentation has been sufficiently well-explored. However, there is no prior work on generating code-switched text with fine-grained control on the degree of... | Sneha Mondal, Ritika, Shreya Pathak, Preethi Jyothi, Aravindan Raghuveer |  |
| 814 |  |  [Towards Climate Awareness in NLP Research](https://doi.org/10.18653/v1/2022.emnlp-main.159) |  | 0 | The climate impact of AI, and NLP research in particular, has become a serious issue given the enormous amount of energy that is increasingly being used for training and running computational models. Consequently, increasing focus is placed on efficient NLP. However, this important initiative lacks... | Daniel Hershcovich, Nicolas Webersinke, Mathias Kraus, Julia Anna Bingler, Markus Leippold |  |
| 815 |  |  [Navigating Connected Memories with a Task-oriented Dialog System](https://doi.org/10.18653/v1/2022.emnlp-main.160) |  | 0 | Recent years have seen an increasing trend in the volume of personal media captured by users, thanks to the advent of smartphones and smart glasses, resulting in large media collections. Despite conversation being an intuitive human-computer interface, current efforts focus mostly on single-shot... | Satwik Kottur, Seungwhan Moon, Alborz Geramifard, Babak Damavandi |  |
| 816 |  |  [Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.161) |  | 0 | Pre-trained language models (LMs), such as BERT (Devlin et al., 2018) and its variants, have led to significant improvements on various NLP tasks in past years. However, a theoretical framework for studying their relationships is still missing. In this paper, we fill this gap by investigating the... | Hao Zhang |  |
| 817 |  |  [SynGEC: Syntax-Enhanced Grammatical Error Correction with a Tailored GEC-Oriented Parser](https://doi.org/10.18653/v1/2022.emnlp-main.162) |  | 0 | This work proposes a syntax-enhanced grammatical error correction (GEC) approach named SynGEC that effectively incorporates dependency syntactic information into the encoder part of GEC models. The key challenge for this idea is that off-the-shelf parsers are unreliable when processing... | Yue Zhang, Bo Zhang, Zhenghua Li, Zuyi Bao, Chen Li, Min Zhang |  |
| 818 |  |  [Varifocal Question Generation for Fact-checking](https://doi.org/10.18653/v1/2022.emnlp-main.163) |  | 0 | Fact-checking requires retrieving evidence related to a claim under investigation. The task can be formulated as question generation based on a claim, followed by question answering.However, recent question generation approaches assume that the answer is known and typically contained in a passage... | Nedjma Ousidhoum, Zhangdie Yuan, Andreas Vlachos |  |
| 819 |  |  [Bilingual Lexicon Induction for Low-Resource Languages using Graph Matching via Optimal Transport](https://doi.org/10.18653/v1/2022.emnlp-main.164) |  | 0 | Bilingual lexicons form a critical component of various natural language processing applications, including unsupervised and semisupervised machine translation and crosslingual information retrieval. In this work, we improve bilingual lexicon induction performance across 40 language pairs with a... | Kelly Marchisio, Ali SaadEldin, Kevin Duh, Carey E. Priebe, Philipp Koehn |  |
| 820 |  |  [Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection](https://doi.org/10.18653/v1/2022.emnlp-main.165) |  | 0 | Language models increasingly rely on massive web crawls for diverse text data. However, these sources are rife with undesirable content. As such, resources like Wikipedia, books, and news often serve as anchors for automatically selecting web text most suitable for language modeling, a process... | Suchin Gururangan, Dallas Card, Sarah K. Dreier, Emily K. Gade, Leroy Z. Wang, Zeyu Wang, Luke Zettlemoyer, Noah A. Smith |  |
| 821 |  |  [ConReader: Exploring Implicit Relations in Contracts for Contract Clause Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.166) |  | 0 | We study automatic Contract Clause Extraction (CCE) by modeling implicit relations in legal contracts. Existing CCE methods mostly treat contracts as plain text, creating a substantial barrier to understanding contracts of high complexity. In this work, we first comprehensively analyze the... | Weiwen Xu, Yang Deng, Wenqiang Lei, Wenlong Zhao, TatSeng Chua, Wai Lam |  |
| 822 |  |  [Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU](https://doi.org/10.18653/v1/2022.emnlp-main.167) |  | 0 | Curriculum Learning (CL) is a technique of training models via ranking examples in a typically increasing difficulty trend with the aim of accelerating convergence and improving generalisability. Current approaches for Natural Language Understanding (NLU) tasks use CL to improve in-distribution... | Fenia Christopoulou, Gerasimos Lampouras, Ignacio Iacobacci |  |
| 823 |  |  [Revisiting Parameter-Efficient Tuning: Are We Really There Yet?](https://doi.org/10.18653/v1/2022.emnlp-main.168) |  | 0 | Parameter-Efficient Tuning (PETuning) methods have been deemed by many as the new paradigm for using pretrained language models (PLMs). By tuning just a fraction amount of parameters comparing to full model finetuning, PETuning methods claim to have achieved performance on par with or even better... | Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, Shangsong Liang |  |
| 824 |  |  [Transfer Learning from Semantic Role Labeling to Event Argument Extraction with Template-based Slot Querying](https://doi.org/10.18653/v1/2022.emnlp-main.169) |  | 0 | In this work, we investigate transfer learning from semantic role labeling (SRL) to event argument extraction (EAE), considering their similar argument structures. We view the extraction task as a role querying problem, unifying various methods into a single framework. There are key discrepancies... | Zhisong Zhang, Emma Strubell, Eduard H. Hovy |  |
| 825 |  |  [Calibrating Zero-shot Cross-lingual (Un-)structured Predictions](https://doi.org/10.18653/v1/2022.emnlp-main.170) |  | 0 | We investigate model calibration in the setting of zero-shot cross-lingual transfer with large-scale pre-trained language models. The level of model calibration is an important metric for evaluating the trustworthiness of predictive models. There exists an essential need for model calibration when... | Zhengping Jiang, Anqi Liu, Benjamin Van Durme |  |
| 826 |  |  [PRINCE: Prefix-Masked Decoding for Knowledge Enhanced Sequence-to-Sequence Pre-Training](https://doi.org/10.18653/v1/2022.emnlp-main.171) |  | 0 | Pre-trained Language Models (PLMs) have shown effectiveness in various Natural Language Processing (NLP) tasks. Denoising autoencoder is one of the most successful pre-training frameworks, learning to recompose the original text given a noise-corrupted one. The existing studies mainly focus on... | Song Xu, Haoran Li, Peng Yuan, Youzheng Wu, Xiaodong He |  |
| 827 |  |  [How Far are We from Robust Long Abstractive Summarization?](https://doi.org/10.18653/v1/2022.emnlp-main.172) |  | 0 | Abstractive summarization has made tremendous progress in recent years. In this work, we perform fine-grained human annotations to evaluate long document abstractive summarization systems (i.e., models and metrics) with the aim of implementing them to generate reliable summaries. For long document... | Huan Yee Koh, Jiaxin Ju, He Zhang, Ming Liu, Shirui Pan |  |
| 828 |  |  [Measuring Context-Word Biases in Lexical Semantic Datasets](https://doi.org/10.18653/v1/2022.emnlp-main.173) |  | 0 | State-of-the-art pretrained contextualized models (PCM) eg. BERT use tasks such as WiC and WSD to evaluate their word-in-context representations. This inherently assumes that performance in these tasks reflect how well a model represents the coupled word and context semantics. We question this... | Qianchu Liu, Diana McCarthy, Anna Korhonen |  |
| 829 |  |  [Iteratively Prompt Pre-trained Language Models for Chain of Thought](https://doi.org/10.18653/v1/2022.emnlp-main.174) |  | 0 | While Pre-trained Language Models (PLMs) internalize a great amount of world knowledge, they have been shown incapable of recalling these knowledge to solve tasks requiring complex & multi-step reasoning. Similar to how humans develop a “chain of thought” for these tasks, how can we equip PLMs with... | Boshi Wang, Xiang Deng, Huan Sun |  |
| 830 |  |  [Unobserved Local Structures Make Compositional Generalization Hard](https://doi.org/10.18653/v1/2022.emnlp-main.175) |  | 0 | While recent work has shown that sequence-to-sequence models struggle to generalize to new compositions (termed compositional generalization), little is known on what makes compositional generalization hard on a particular test instance. In this work, we investigate the factors that make... | Ben Bogin, Shivanshu Gupta, Jonathan Berant |  |
| 831 |  |  [Mitigating Data Sparsity for Short Text Topic Modeling by Topic-Semantic Contrastive Learning](https://doi.org/10.18653/v1/2022.emnlp-main.176) |  | 0 | To overcome the data sparsity issue in short text topic modeling, existing methods commonly rely on data augmentation or the data characteristic of short texts to introduce more word co-occurrence information. However, most of them do not make full use of the augmented data or the data... | Xiaobao Wu, Anh Tuan Luu, Xinshuai Dong |  |
| 832 |  |  [Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.177) |  | 0 | Multi-turn dialogue modeling as a challenging branch of natural language understanding (NLU), aims to build representations for machines to understand human dialogues, which provides a solid foundation for multiple downstream tasks. Recent studies of dialogue modeling commonly employ pre-trained... | Yiyang Li, Hai Zhao, Zhuosheng Zhang |  |
| 833 |  |  [Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates](https://doi.org/10.18653/v1/2022.emnlp-main.178) |  | 0 | Calibration strengthens the trustworthiness of black-box models by producing better accurate confidence estimates on given examples. However, little is known about if model explanations can help confidence calibration. Intuitively, humans look at important features attributions and decide whether... | Dongfang Li, Baotian Hu, Qingcai Chen |  |
| 834 |  |  [Non-Autoregressive Neural Machine Translation: A Call for Clarity](https://doi.org/10.18653/v1/2022.emnlp-main.179) |  | 0 | Non-autoregressive approaches aim to improve the inference speed of translation models by only requiring a single forward pass to generate the output sequence instead of iteratively producing each predicted token. Consequently, their translation quality still tends to be inferior to their... | Robin M. Schmidt, Telmo Pires, Stephan Peitz, Jonas Lööf |  |
| 835 |  |  [RED-ACE: Robust Error Detection for ASR using Confidence Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.180) |  | 0 | ASR Error Detection (AED) models aim to post-process the output of Automatic Speech Recognition (ASR) systems, in order to detect transcription errors. Modern approaches usually use text-based input, comprised solely of the ASR transcription hypothesis, disregarding additional signals from the ASR... | Zorik Gekhman, Dina Zverinski, Jonathan Mallinson, Genady Beryozkin |  |
| 836 |  |  [Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation](https://doi.org/10.18653/v1/2022.emnlp-main.181) |  | 0 | Chart-based models have shown great potential in unsupervised grammar induction, running recursively and hierarchically, but requiring O(n³) time-complexity. The Recursive Transformer based on Differentiable Trees (R2D2) makes it possible to scale to large language model pretraining even with a... | Xiang Hu, Haitao Mi, Liang Li, Gerard de Melo |  |
| 837 |  |  [A Localized Geometric Method to Match Knowledge in Low-dimensional Hyperbolic Space](https://doi.org/10.18653/v1/2022.emnlp-main.182) |  | 0 | Matching equivalent entities across Knowledge graphs is a pivotal step for knowledge fusion. Previous approaches usually study the problem in Euclidean space. However, recent works have shown that hyperbolic space has a higher capacity than Euclidean space and hyperbolic embedding can represent the... | Bo Hui, Tian Xia, WeiShinn Ku |  |
| 838 |  |  [Memory-assisted prompt editing to improve GPT-3 after deployment](https://doi.org/10.18653/v1/2022.emnlp-main.183) |  | 0 | Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret “What word is similar to good?” to mean a homophone, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with... | Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang |  |
| 839 |  |  [LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.184) |  | 0 | Multimodal Machine Translation (MMT) focuses on enhancing text-only translation with visual features, which has attracted considerable attention from both natural language processing and computer vision communities. Recent advances still struggle to train a separate model for each language pair,... | Hongcheng Guo, Jiaheng Liu, Haoyang Huang, Jian Yang, Zhoujun Li, Dongdong Zhang, Zheng Cui |  |
| 840 |  |  [PromptEHR: Conditional Electronic Healthcare Records Generation with Prompt Learning](https://doi.org/10.18653/v1/2022.emnlp-main.185) |  | 0 | Accessing longitudinal multimodal Electronic Healthcare Records (EHRs) is challenging due to privacy concerns, which hinders the use of ML for healthcare applications. Synthetic EHRs generation bypasses the need to share sensitive real patient records. However, existing methods generate... | Zifeng Wang, Jimeng Sun |  |
| 841 |  |  [ROSE: Robust Selective Fine-tuning for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.186) |  | 0 | Even though the large-scale language models have achieved excellent performances, they suffer from various adversarial attacks.A large body of defense methods has been proposed. However, they are still limited due to redundant attack search spaces and the inability to defend against various types... | Lan Jiang, Hao Zhou, Yankai Lin, Peng Li, Jie Zhou, Rui Jiang |  |
| 842 |  |  [CodeRetriever: A Large Scale Contrastive Pre-Training Method for Code Search](https://doi.org/10.18653/v1/2022.emnlp-main.187) |  | 0 | In this paper, we propose the CodeRetriever model, which learns the function-level code semantic representations through large-scale code-text contrastive pre-training. We adopt two contrastive learning schemes in CodeRetriever: unimodal contrastive learning and bimodal contrastive learning. For... | Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang, Bolun Yao, Weizhen Qi, Daxin Jiang, Weizhu Chen, Nan Duan |  |
| 843 |  |  [Open-Topic False Information Detection on Social Networks with Contrastive Adversarial Learning](https://doi.org/10.18653/v1/2022.emnlp-main.188) |  | 0 | Current works about false information detection based on conversation graphs on social networks focus primarily on two research streams from the standpoint of topic distribution: in-topic and cross-topic techniques, which assume that the data topic distribution is identical or cross, respectively.... | Guanghui Ma, Chunming Hu, Ling Ge, Hong Zhang |  |
| 844 |  |  [Mitigating Inconsistencies in Multimodal Sentiment Analysis under Uncertain Missing Modalities](https://doi.org/10.18653/v1/2022.emnlp-main.189) |  | 0 | For the missing modality problem in Multimodal Sentiment Analysis (MSA), the inconsistency phenomenon occurs when the sentiment changes due to the absence of a modality. The absent modality that determines the overall semantic can be considered as a key missing modality. However, previous works all... | Jiandian Zeng, Jiantao Zhou, Tianyi Liu |  |
| 845 |  |  [ConvTrans: Transforming Web Search Sessions for Conversational Dense Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.190) |  | 0 | Conversational search provides users with a natural and convenient new search experience. Recently, conversational dense retrieval has shown to be a promising technique for realizing conversational search. However, as conversational search systems have not been widely deployed, it is hard to get... | Kelong Mao, Zhicheng Dou, Hongjin Qian, Fengran Mo, Xiaohua Cheng, Zhao Cao |  |
| 846 |  |  [MUSIED: A Benchmark for Event Detection from Multi-Source Heterogeneous Informal Texts](https://doi.org/10.18653/v1/2022.emnlp-main.191) |  | 0 | Event detection (ED) identifies and classifies event triggers from unstructured texts, serving as a fundamental task for information extraction. Despite the remarkable progress achieved in the past several years, most research efforts focus on detecting events from formal texts (e.g., news... | Xiangyu Xi, Jianwei Lv, Shuaipeng Liu, Wei Ye, Fan Yang, Guanglu Wan |  |
| 847 |  |  [Reproducibility Issues for BERT-based Evaluation Metrics](https://doi.org/10.18653/v1/2022.emnlp-main.192) |  | 0 | Reproducibility is of utmost concern in machine learning and natural language processing (NLP). In the field of natural language generation (especially machine translation), the seminal paper of Post (2018) has pointed out problems of reproducibility of the dominant metric, BLEU, at the time of... | Yanran Chen, Jonas Belouadi, Steffen Eger |  |
| 848 |  |  [Improving Multi-task Stance Detection with Multi-task Interaction Network](https://doi.org/10.18653/v1/2022.emnlp-main.193) |  | 0 | Stance detection aims to identify people’s standpoints expressed in the text towards a target, which can provide powerful information for various downstream tasks.Recent studies have proposed multi-task learning models that introduce sentiment information to boost stance detection.However, they... | Heyan Chai, Siyu Tang, Jinhao Cui, Ye Ding, Binxing Fang, Qing Liao |  |
| 849 |  |  [Neural-based Mixture Probabilistic Query Embedding for Answering FOL queries on Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.194) |  | 0 | Query embedding (QE)—which aims to embed entities and first-order logical (FOL) queries in a vector space, has shown great power in answering FOL queries on knowledge graphs (KGs). Existing QE methods divide a complex query into a sequence of mini-queries according to its computation graph and... | Xiao Long, Liansheng Zhuang, Aodi Li, Shafei Wang, Houqiang Li |  |
| 850 |  |  [Improving Multi-turn Emotional Support Dialogue Generation with Lookahead Strategy Planning](https://doi.org/10.18653/v1/2022.emnlp-main.195) |  | 0 | Providing Emotional Support (ES) to soothe people in emotional distress is an essential capability in social interactions. Most existing researches on building ES conversation systems only considered single-turn interactions with users, which was over-simplified. In comparison, multi-turn ES... | Yi Cheng, Wenge Liu, Wenjie Li, Jiashuo Wang, Ruihui Zhao, Bang Liu, Xiaodan Liang, Yefeng Zheng |  |
| 851 |  |  [Conformal Predictor for Improving Zero-Shot Text Classification Efficiency](https://doi.org/10.18653/v1/2022.emnlp-main.196) |  | 0 | Pre-trained language models (PLMs) have been shown effective for zero-shot (0shot) text classification. 0shot models based on natural language inference (NLI) and next sentence prediction (NSP) employ cross-encoder architecture and infer by making a forward pass through the model for each... | Prafulla Kumar Choubey, Yu Bai, ChienSheng Wu, Wenhao Liu, Nazneen Rajani |  |
| 852 |  |  [Effective and Efficient Query-aware Snippet Extraction for Web Search](https://doi.org/10.18653/v1/2022.emnlp-main.197) |  | 0 | Query-aware webpage snippet extraction is widely used in search engines to help users better understand the content of the returned webpages before clicking. The extracted snippet is expected to summarize the webpage in the context of the input query. Existing snippet extraction methods mainly rely... | Jingwei Yi, Fangzhao Wu, Chuhan Wu, Xiaolong Huang, Binxing Jiao, Guangzhong Sun, Xing Xie |  |
| 853 |  |  [You Only Need One Model for Open-domain Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.198) |  | 0 | Recent approaches to Open-domain Question Answering refer to an external knowledge base using a retriever model, optionally rerank passages with a separate reranker model and generate an answer using another reader model. Despite performing related tasks, the models have separate parameters and are... | Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paranjape, Christopher D. Manning, KyoungGu Woo |  |
| 854 |  |  [Generative Entity Typing with Curriculum Learning](https://doi.org/10.18653/v1/2022.emnlp-main.199) |  | 0 | Entity typing aims to assign types to the entity mentions in given texts. The traditional classification-based entity typing paradigm has two unignorable drawbacks: 1) it fails to assign an entity to the types beyond the predefined type set, and 2) it can hardly handle few-shot and zero-shot... | Siyu Yuan, Deqing Yang, Jiaqing Liang, Zhixu Li, Jinxi Liu, Jingyue Huang, Yanghua Xiao |  |
| 855 |  |  [SetGNER: General Named Entity Recognition as Entity Set Generation](https://doi.org/10.18653/v1/2022.emnlp-main.200) |  | 0 | Recently, joint recognition of flat, nested and discontinuous entities has received increasing attention. Motivated by the observation that the target output of NER is essentially a set of sequences, we propose a novel entity set generation framework for general NER scenes in this paper. Different... | Yuxin He, Buzhou Tang |  |
| 856 |  |  [Opinion Summarization by Weak-Supervision from Mix-structured Data](https://doi.org/10.18653/v1/2022.emnlp-main.201) |  | 0 | Opinion summarization of multiple reviews suffers from the lack of reference summaries for training.Most previous approaches construct multiple reviews and their summary based on textual similarities between reviews,resulting in information mismatch between the review input and the summary. In this... | Yizhu Liu, Qi Jia, Kenny Q. Zhu |  |
| 857 |  |  [Multi-level Distillation of Semantic Knowledge for Pre-training Multilingual Language Model](https://doi.org/10.18653/v1/2022.emnlp-main.202) |  | 0 | Pre-trained multilingual language models play an important role in cross-lingual natural language understanding tasks. However, existing methods did not focus on learning the semantic structure of representation, and thus could not optimize their performance. In this paper, we propose Multi-level... | Mingqi Li, Fei Ding, Dan Zhang, Long Cheng, Hongxin Hu, Feng Luo |  |
| 858 |  |  [Empowering Dual-Encoder with Query Generator for Cross-Lingual Dense Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.203) |  | 0 | In monolingual dense retrieval, lots of works focus on how to distill knowledge from cross-encoder re-ranker to dual-encoder retriever and these methods achieve better performance due to the effectiveness of cross-encoder re-ranker. However, we find that the performance of the cross-encoder... | Houxing Ren, Linjun Shou, Ning Wu, Ming Gong, Daxin Jiang |  |
| 859 |  |  [R2F: A General Retrieval, Reading and Fusion Framework for Document-level Natural Language Inference](https://doi.org/10.18653/v1/2022.emnlp-main.204) |  | 0 | Document-level natural language inference (DOCNLI) is a new challenging task in natural language processing, aiming at judging the entailment relationship between a pair of hypothesis and premise documents. Current datasets and baselines largely follow sentence-level settings, but fail to address... | Hao Wang, Yixin Cao, Yangguang Li, Zhen Huang, Kun Wang, Jing Shao |  |
| 860 |  |  [Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-main.205) |  | 0 | There is a growing body of work in recent years to develop pre-trained language models (PLMs) for the Arabic language. This work addresses two major problems in existing Arabic PLMs that limit the progress of the Arabic NLU and NLG fields. First, existing Arabic PLMs are not well-explored and their... | Abbas Ghaddar, Yimeng Wu, Sunyam Bagga, Ahmad Rashid, Khalil Bibi, Mehdi Rezagholizadeh, Chao Xing, Yasheng Wang, Xinyu Duan, Zhefeng Wang, Baoxing Huai, Xin Jiang, Qun Liu, Philippe Langlais |  |
| 861 |  |  [KECP: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.206) |  | 0 | Extractive Question Answering (EQA) is one of the most essential tasks in Machine Reading Comprehension (MRC), which can be solved by fine-tuning the span selecting heads of Pre-trained Language Models (PLMs). However, most existing approaches for MRC may perform poorly in the few-shot learning... | Jianing Wang, Chengyu Wang, Minghui Qiu, Qiuhui Shi, Hongbin Wang, Jun Huang, Ming Gao |  |
| 862 |  |  [Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.207) |  | 0 | Knowledge-enhanced Pre-trained Language Model (PLM) has recently received significant attention, which aims to incorporate factual knowledge into PLMs. However, most existing methods modify the internal structures of fixed types of PLMs by stacking complicated modules, and introduce redundant and... | Jianing Wang, Wenkang Huang, Minghui Qiu, Qiuhui Shi, Hongbin Wang, Xiang Li, Ming Gao |  |
| 863 |  |  [On the Evaluation Metrics for Paraphrase Generation](https://doi.org/10.18653/v1/2022.emnlp-main.208) |  | 0 | In this paper we revisit automatic metrics for paraphrase evaluation and obtain two findings that disobey conventional wisdom: (1) Reference-free metrics achieve better performance than their reference-based counterparts. (2) Most commonly used metrics do not align well with human... | Lingfeng Shen, Lemao Liu, Haiyun Jiang, Shuming Shi |  |
| 864 |  |  [Curriculum Learning Meets Weakly Supervised Multimodal Correlation Learning](https://doi.org/10.18653/v1/2022.emnlp-main.209) |  | 0 | In the field of multimodal sentiment analysis (MSA), a few studies have leveraged the inherent modality correlation information stored in samples for self-supervised learning. However, they feed the training pairs in a random order without consideration of difficulty. Without human annotation, the... | Sijie Mai, Ya Sun, Haifeng Hu |  |
| 865 |  |  [Rethinking Positional Encoding in Tree Transformer for Code Representation](https://doi.org/10.18653/v1/2022.emnlp-main.210) |  | 0 | Transformers are now widely used in code representation, and several recent works further develop tree Transformers to capture the syntactic structure in source code. Specifically, novel tree positional encodings have been proposed to incorporate inductive bias into Transformer.In this work, we... | Han Peng, Ge Li, Yunfei Zhao, Zhi Jin |  |
| 866 |  |  [RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL](https://doi.org/10.18653/v1/2022.emnlp-main.211) |  | 0 | Relational structures such as schema linking and schema encoding have been validated as a key component to qualitatively translating natural language into SQL queries. However, introducing these structural relations comes with prices: they often result in a specialized model structure, which... | Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, Zhouhan Lin |  |
| 867 |  |  [COM-MRC: A COntext-Masked Machine Reading Comprehension Framework for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.212) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) aims to extract sentiment triplets from sentences, which was recently formalized as an effective machine reading comprehension (MRC) based framework. However, when facing multiple aspect terms, the MRC-based methods could fail due to the interference from... | Zepeng Zhai, Hao Chen, Fangxiang Feng, Ruifan Li, Xiaojie Wang |  |
| 868 |  |  [CEM: Machine-Human Chatting Handoff via Causal-Enhance Module](https://doi.org/10.18653/v1/2022.emnlp-main.213) |  | 0 | Aiming to ensure chatbot quality by predicting chatbot failure and enabling human-agent collaboration, Machine-Human Chatting Handoff (MHCH) has attracted lots of attention from both industry and academia in recent years. However, most existing methods mainly focus on the dialogue context or assist... | ShanShan Zhong, Jinghui Qin, Zhongzhan Huang, Daifeng Li |  |
| 869 |  |  [Nearest Neighbor Zero-Shot Inference](https://doi.org/10.18653/v1/2022.emnlp-main.214) |  | 0 | Retrieval-augmented language models (LMs) use non-parametric memory to substantially outperform their non-retrieval counterparts on perplexity-based evaluations, but it is an open question whether they achieve similar gains in few- and zero-shot end-task accuracy. We extensively study one such... | Weijia Shi, Julian Michael, Suchin Gururangan, Luke Zettlemoyer |  |
| 870 |  |  [Robots-Dont-Cry: Understanding Falsely Anthropomorphic Utterances in Dialog Systems](https://doi.org/10.18653/v1/2022.emnlp-main.215) |  | 0 | Dialog systems are often designed or trained to output human-like responses. However, some responses may be impossible for a machine to truthfully say (e.g. “that movie made me cry”). Highly anthropomorphic responses might make users uncomfortable or implicitly deceive them into thinking they are... | David Gros, Yu Li, Zhou Yu |  |
| 871 |  |  [A Joint Learning Framework for Restaurant Survival Prediction and Explanation](https://doi.org/10.18653/v1/2022.emnlp-main.216) |  | 0 | The bloom of the Internet and the recent breakthroughs in deep learning techniques open a new door to AI for E-commence, with a trend of evolving from using a few financial factors such as liquidity and profitability to using more advanced AI techniques to process complex and multi-modal data. In... | Xin Li, Xiaojie Zhang, Peng Jia, Rui Mao, MingYang Zhou, Xing Xie, Hao Liao |  |
| 872 |  |  [Making Pretrained Language Models Good Long-tailed Learners](https://doi.org/10.18653/v1/2022.emnlp-main.217) |  | 0 | Prompt-tuning has shown appealing performance in few-shot classification by virtue of its capability in effectively exploiting pre-trained knowledge. This motivates us to check the hypothesis that prompt-tuning is also a promising choice for long-tailed classification, since the tail classes are... | Chen Zhang, Lei Ren, Jingang Wang, Wei Wu, Dawei Song |  |
| 873 |  |  [UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression](https://doi.org/10.18653/v1/2022.emnlp-main.218) |  | 0 | Geometry problem solving is a well-recognized testbed for evaluating the high-level multi-modal reasoning capability of deep models. In most existing works, two main geometry problems: calculation and proving, are usually treated as two specific tasks, hindering a deep model to unify its reasoning... | Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, Xiaodan Liang |  |
| 874 |  |  [Face-Sensitive Image-to-Emotional-Text Cross-modal Translation for Multimodal Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.219) |  | 0 | Aspect-level multimodal sentiment analysis, which aims to identify the sentiment of the target aspect from multimodal data, recently has attracted extensive attention in the community of multimedia and natural language processing. Despite the recent success in textual aspect-based sentiment... | Hao Yang, Yanyan Zhao, Bing Qin |  |
| 875 |  |  [FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.220) |  | 0 | Recent model-based reference-free metrics for open-domain dialogue evaluation exhibit promising correlations with human judgment. However, they either perform turn-level evaluation or look at a single dialogue quality dimension. One would expect a good evaluation metric to assess multiple quality... | Chen Zhang, Luis Fernando D'Haro, Qiquan Zhang, Thomas Friedrichs, Haizhou Li |  |
| 876 |  |  [Sentence Representation Learning with Generative Objective rather than Contrastive Objective](https://doi.org/10.18653/v1/2022.emnlp-main.221) |  | 0 | Though offering amazing contextualized token-level representations, current pre-trained language models take less attention on accurately acquiring sentence-level representation during their self-supervised pre-training. However, contrastive objectives which dominate the current sentence... | Bohong Wu, Hai Zhao |  |
| 877 |  |  [RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning](https://doi.org/10.18653/v1/2022.emnlp-main.222) |  | 0 | Prompting has shown impressive success in enabling large pre-trained language models (LMs) to perform diverse NLP tasks, especially with only few downstream data. Automatically finding the optimal prompt for each task, however, is challenging. Most existing work resorts to tuning \*soft\* prompts... | Mingkai Deng, Jianyu Wang, ChengPing Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, Zhiting Hu |  |
| 878 |  |  [DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.223) |  | 0 | Prompt learning with immensely large Casual Language Models (CLMs) has been shown promising for attribute-controllable text generation (CTG). However, vanilla prompt tuning tends to imitate training corpus characteristics beyond the control attributes, resulting in a poor generalization ability.... | Hanqing Zhang, Dawei Song |  |
| 879 |  |  [CPL: Counterfactual Prompt Learning for Vision and Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.224) |  | 0 | Prompt tuning is a new few-shot transfer learning technique that only tunes the learnable prompt for pre-trained vision and language models such as CLIP. However, existing prompt tuning methods tend to learn spurious or entangled representations, which leads to poor generalization to unseen... | Xuehai He, Diji Yang, Weixi Feng, TsuJui Fu, Arjun R. Akula, Varun Jampani, Pradyumna Narayana, Sugato Basu, William Yang Wang, Xin Wang |  |
| 880 |  |  [Red Teaming Language Models with Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.225) |  | 0 | Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of... | Ethan Perez, Saffron Huang, H. Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, Geoffrey Irving |  |
| 881 |  |  [CapOnImage: Context-driven Dense-Captioning on Image](https://doi.org/10.18653/v1/2022.emnlp-main.226) |  | 0 | Existing image captioning systems are dedicated to generating narrative captions for images, which are spatially detached from theimage in presentation. However, texts can also be used as decorations on the image to highlight the key points and increase theattractiveness of images. In this work, we... | Yiqi Gao, Xinglin Hou, Yuanmeng Zhang, Tiezheng Ge, Yuning Jiang, Peng Wang |  |
| 882 |  |  [SpanProto: A Two-stage Span-based Prototypical Network for Few-shot Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.227) |  | 0 | Few-shot Named Entity Recognition (NER) aims to identify named entities with very little annotated data. Previous methods solve this problem based on token-wise classification, which ignores the information of entity boundaries, and inevitably the performance is affected by the massive non-entity... | Jianing Wang, Chengyu Wang, Chuanqi Tan, Minghui Qiu, Songfang Huang, Jun Huang, Ming Gao |  |
| 883 |  |  [Discovering Differences in the Representation of People using Contextualized Semantic Axes](https://doi.org/10.18653/v1/2022.emnlp-main.228) |  | 0 | A common paradigm for identifying semantic differences across social and temporal contexts is the use of static word embeddings and their distances. In particular, past work has compared embeddings against “semantic axes” that represent two opposing concepts. We extend this paradigm to BERT... | Li Lucy, Divya Tadimeti, David Bamman |  |
| 884 |  |  [Generating Literal and Implied Subquestions to Fact-check Complex Claims](https://doi.org/10.18653/v1/2022.emnlp-main.229) |  | 0 | Verifying political claims is a challenging task, as politicians can use various tactics to subtly misrepresent the facts for their agenda. Existing automatic fact-checking systems fall short here, and their predictions like “half-true” are not very useful in isolation, since it is unclear which... | Jifan Chen, Aniruddh Sriram, Eunsol Choi, Greg Durrett |  |
| 885 |  |  [Machine Translation Robustness to Natural Asemantic Variation](https://doi.org/10.18653/v1/2022.emnlp-main.230) |  | 0 | Current Machine Translation (MT) models still struggle with more challenging input, such as noisy data and tail-end words and phrases. Several works have addressed this robustness issue by identifying specific categories of noise and variation then tuning models to perform better on them. An... | Jacob Bremerman, Xiang Ren, Jonathan May |  |
| 886 |  |  [Natural Language to Code Translation with Execution](https://doi.org/10.18653/v1/2022.emnlp-main.231) |  | 0 | Generative models of code, pretrained on large corpora of programs, have shown great success in translating natural language to code (Chen et al., 2021; Austin et al., 2021; Li et al., 2022, inter alia). While these models do not explicitly incorporate program semantics (i.e., execution results)... | Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, Sida I. Wang |  |
| 887 |  |  [Life is a Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes](https://doi.org/10.18653/v1/2022.emnlp-main.232) |  | 0 | Analogy-making gives rise to reasoning, abstraction, flexible categorization and counterfactual inference – abilities lacking in even the best AI systems today. Much research has suggested that analogies are key to non-brittle systems that can adapt to new domains. Despite their importance,... | Oren Sultan, Dafna Shahaf |  |
| 888 |  |  [Language Contamination Helps Explains the Cross-lingual Capabilities of English Pretrained Models](https://doi.org/10.18653/v1/2022.emnlp-main.233) |  | 0 | English pretrained language models, which make up the backbone of many modern NLP systems, require huge amounts of unlabeled training data. These models are generally presented as being trained only on English text but have been found to transfer surprisingly well to other languages. We investigate... | Terra Blevins, Luke Zettlemoyer |  |
| 889 |  |  [Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.234) |  | 0 | The emergent cross-lingual transfer seen in multilingual pretrained models has sparked significant interest in studying their behavior. However, because these analyses have focused on fully trained multilingual models, little is known about the dynamics of the multilingual pretraining process. We... | Terra Blevins, Hila Gonen, Luke Zettlemoyer |  |
| 890 |  |  [Neural Machine Translation with Contrastive Translation Memories](https://doi.org/10.18653/v1/2022.emnlp-main.235) |  | 0 | Retrieval-augmented Neural Machine Translation models have been successful in many translation scenarios. Different from previous works that make use of mutually similar but redundant translation memories (TMs), we propose a new retrieval-augmented NMT to model contrastively retrieved translation... | Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, Rui Yan |  |
| 891 |  |  [Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.236) |  | 0 | Continual Learning for Named Entity Recognition (CL-NER) aims to learn a growing number of entity types over time from a stream of data. However, simply learning Other-Class in the same way as new entity types amplifies the catastrophic forgetting and leads to a substantial performance drop. The... | Junhao Zheng, Zhanxian Liang, Haibin Chen, Qianli Ma |  |
| 892 |  |  [Exploring the Secrets Behind the Learning Difficulty of Meaning Representations for Semantic Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.237) |  | 0 | Previous research has shown that the design of Meaning Representation (MR) greatly influences the final model performance of a neural semantic parser. Therefore, designing a good MR is a long-term goal for semantic parsing. However, it is still an art as there is no quantitative indicator that can... | Zhenwen Li, Jiaqi Guo, Qian Liu, JianGuang Lou, Tao Xie |  |
| 893 |  |  [That's the Wrong Lung! Evaluating and Improving the Interpretability of Unsupervised Multimodal Encoders for Medical Data](https://doi.org/10.18653/v1/2022.emnlp-main.238) |  | 0 | Pretraining multimodal models on Electronic Health Records (EHRs) provides a means of learning representations that can transfer to downstream tasks with minimal supervision. Recent multimodal models induce soft local alignments between image regions and sentences. This is of particular interest in... | Denis Jered McInerney, Geoffrey S. Young, JanWillem van de Meent, Byron C. Wallace |  |
| 894 |  |  [Unsupervised Tokenization Learning](https://doi.org/10.18653/v1/2022.emnlp-main.239) |  | 0 | In the presented study, we discover that the so-called “transition freedom” metric appears superior for unsupervised tokenization purposes in comparison to statistical metrics such as mutual information and conditional probability, providing F-measure scores in range from 0.71 to 1.0 across... | Anton Kolonin, Vignav Ramesh |  |
| 895 |  |  [A Template-based Method for Constrained Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.240) |  | 0 | Machine translation systems are expected to cope with various types of constraints in many practical scenarios. While neural machine translation (NMT) has achieved strong performance in unconstrained cases, it is non-trivial to impose pre-specified constraints into the translation process of NMT... | Shuo Wang, Peng Li, Zhixing Tan, Zhaopeng Tu, Maosong Sun, Yang Liu |  |
| 896 |  |  [PATS: Sensitivity-aware Noisy Learning for Pretrained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.241) |  | 0 | A wide range of NLP tasks benefit from the fine-tuning of pretrained language models (PLMs). However, a number of redundant parameters which contribute less to the downstream task are observed in a directly fine-tuned model. We consider the gap between pretraining and downstream tasks hinders the... | Yupeng Zhang, Hongzhi Zhang, Sirui Wang, Wei Wu, Zhoujun Li |  |
| 897 |  |  [Towards Reinterpreting Neural Topic Models via Composite Activations](https://doi.org/10.18653/v1/2022.emnlp-main.242) |  | 0 | Most Neural Topic Models (NTM) use a variational auto-encoder framework producing K topics limited to the size of the encoder’s output. These topics are interpreted through the selection of the top activated words via the weights or reconstructed vector of the decoder that are directly connected to... | Jia Peng Lim, Hady W. Lauw |  |
| 898 |  |  [Few-shot Query-Focused Summarization with Prefix-Merging](https://doi.org/10.18653/v1/2022.emnlp-main.243) |  | 0 | Query-focused summarization has been considered as an important extension for text summarization. It aims to generate a concise highlight for a given query. Different from text summarization, query-focused summarization has long been plagued by the problem of lacking high-quality large-scale... | Ruifeng Yuan, Zili Wang, Ziqiang Cao, Wenjie Li |  |
| 899 |  |  [Cross-Align: Modeling Deep Cross-lingual Interactions for Word Alignment](https://doi.org/10.18653/v1/2022.emnlp-main.244) |  | 0 | Word alignment which aims to extract lexicon translation equivalents between source and target sentences, serves as a fundamental tool for natural language processing. Recent studies in this area have yielded substantial improvements by generating alignments from contextualized embeddings of the... | Siyu Lai, Zhen Yang, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou |  |
| 900 |  |  [BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.245) |  | 0 | Automatic evaluation metrics are crucial to the development of generative systems. In recent years, pre-trained language model (PLM) based metrics, such as BERTScore, have been commonly adopted in various generation tasks. However, it has been demonstrated that PLMs encode a range of stereotypical... | Tianxiang Sun, Junliang He, Xipeng Qiu, Xuanjing Huang |  |
| 901 |  |  [HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.246) |  | 0 | Hierarchical text classification (HTC) is a challenging subtask of multi-label classification due to its complex label hierarchy.Recently, the pretrained language models (PLM)have been widely adopted in HTC through a fine-tuning paradigm. However, in this paradigm, there exists a huge gap between... | Zihan Wang, Peiyi Wang, Tianyu Liu, Binghuai Lin, Yunbo Cao, Zhifang Sui, Houfeng Wang |  |
| 902 |  |  [Not to Overfit or Underfit the Source Domains? An Empirical Study of Domain Generalization in Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.247) |  | 0 | Machine learning models are prone to overfitting their training (source) domains, which is commonly believed to be the reason why they falter in novel target domains. Here we examine the contrasting view that multi-source domain generalization (DG) is first and foremost a problem of mitigating... | Md. Arafat Sultan, Avi Sil, Radu Florian |  |
| 903 |  |  [Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs](https://doi.org/10.18653/v1/2022.emnlp-main.248) |  | 0 | Social intelligence and Theory of Mind (TOM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allows humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social... | Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Choi |  |
| 904 |  |  [Improving Passage Retrieval with Zero-Shot Question Generation](https://doi.org/10.18653/v1/2022.emnlp-main.249) |  | 0 | We propose a simple and effective re-ranking method for improving passage retrieval in open question answering. The re-ranker re-scores retrieved passages with a zero-shot question generation model, which uses a pre-trained language model to compute the probability of the input question conditioned... | Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wentau Yih, Joelle Pineau, Luke Zettlemoyer |  |
| 905 |  |  [Summarizing Community-based Question-Answer Pairs](https://doi.org/10.18653/v1/2022.emnlp-main.250) |  | 0 | Community-based Question Answering (CQA), which allows users to acquire their desired information, has increasingly become an essential component of online services in various domains such as E-commerce, travel, and dining. However, an overwhelming number of CQA pairs makes it difficult for users... | TingYao Hsu, Yoshi Suhara, Xiaolan Wang |  |
| 906 |  |  [Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models](https://doi.org/10.18653/v1/2022.emnlp-main.251) |  | 0 | Current Natural Language Inference (NLI) models achieve impressive results, sometimes outperforming humans when evaluating on in-distribution test sets. However, as these models are known to learn from annotation artefacts and dataset biases, it is unclear to what extent the models are learning the... | Joe Stacey, Pasquale Minervini, Haim Dubossarsky, Marek Rei |  |
| 907 |  |  [How to disagree well: Investigating the dispute tactics used on Wikipedia](https://doi.org/10.18653/v1/2022.emnlp-main.252) |  | 0 | Disagreements are frequently studied from the perspective of either detecting toxicity or analysing argument structure. We propose a framework of dispute tactics which unifies these two perspectives, as well as other dialogue acts which play a role in resolving disputes, such as asking questions... | Christine de Kock, Andreas Vlachos |  |
| 908 |  |  [Chapter Ordering in Novels](https://doi.org/10.18653/v1/2022.emnlp-main.253) |  | 0 | Understanding narrative flow and text coherence in long-form documents (novels) remains an open problem in NLP.To gain insight, we explore the task of chapter ordering, reconstructing the original order of chapters in novel given a random permutation of the text. This can be seen as extending the... | Allen Kim, Steven Skiena |  |
| 909 |  |  [Open-ended Knowledge Tracing for Computer Science Education](https://doi.org/10.18653/v1/2022.emnlp-main.254) |  | 0 | In educational applications, knowledge tracing refers to the problem of estimating students’ time-varying concept/skill mastery level from their past responses to questions and predicting their future performance.One key limitation of most existing knowledge tracing methods is that they treat... | Naiming Liu, Zichao Wang, Richard G. Baraniuk, Andrew S. Lan |  |
| 910 |  |  [Logical Neural Networks for Knowledge Base Completion with Embeddings & Rules](https://doi.org/10.18653/v1/2022.emnlp-main.255) |  | 0 | Knowledge base completion (KBC) has benefitted greatly by learning explainable rules in an human-interpretable dialect such as first-order logic. Rule-based KBC has so far, mainly focussed on learning one of two types of rules: conjunction-of-disjunctions and disjunction-of-conjunctions. We... | Prithviraj Sen, Breno W. S. R. de Carvalho, Ibrahim Abdelaziz, Pavan Kapanipathi, Salim Roukos, Alexander G. Gray |  |
| 911 |  |  [MedCLIP: Contrastive Learning from Unpaired Medical Images and Text](https://doi.org/10.18653/v1/2022.emnlp-main.256) |  | 0 | Existing vision-text contrastive learning like CLIP aims to match the paired image and caption embeddings while pushing others apart, which improves representation transferability and supports zero-shot prediction. However, medical image-text datasets are orders of magnitude below the general... | Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, Jimeng Sun |  |
| 912 |  |  [GA-SAM: Gradient-Strength based Adaptive Sharpness-Aware Minimization for Improved Generalization](https://doi.org/10.18653/v1/2022.emnlp-main.257) |  | 0 | Recently, Sharpness-Aware Minimization (SAM) algorithm has shown state-of-the-art generalization abilities in vision tasks. It demonstrates that flat minima tend to imply better generalization abilities. However, it has some difficulty implying SAM to some natural language tasks, especially to... | Zhiyuan Zhang, Ruixuan Luo, Qi Su, Xu Sun |  |
| 913 |  |  [Sparse Teachers Can Be Dense with Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.258) |  | 0 | Recent advances in distilling pretrained language models have discovered that, besides the expressiveness of knowledge, the student-friendliness should be taken into consideration to realize a truly knowledgeable teacher. Based on a pilot study, we find that over-parameterized teachers can produce... | Yi Yang, Chen Zhang, Dawei Song |  |
| 914 |  |  [BBTv2: Towards a Gradient-Free Future with Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.259) |  | 0 | Most downstream adaptation methods tune all or part of the parameters of pre-trained models (PTMs) through gradient descent, where the tuning cost increases linearly with the growth of the model size.By contrast, gradient-free methods only require the forward computation of the PTM to tune the... | Tianxiang Sun, Zhengfu He, Hong Qian, Yunhua Zhou, Xuanjing Huang, Xipeng Qiu |  |
| 915 |  |  [Passage-Mask: A Learnable Regularization Strategy for Retriever-Reader Models](https://doi.org/10.18653/v1/2022.emnlp-main.260) |  | 0 | Retriever-reader models achieve competitive performance across many different NLP tasks such as open question answering and dialogue conversations. In this work, we notice these models easily overfit the top-rank retrieval passages and standard training fails to reason over the entire retrieval... | Shujian Zhang, Chengyue Gong, Xingchao Liu |  |
| 916 |  |  [Mixed-effects transformers for hierarchical adaptation](https://doi.org/10.18653/v1/2022.emnlp-main.261) |  | 0 | Language differs dramatically from context to context. To some degree, large language models like GPT-3 account for such variation by conditioning on strings of initial input text, or prompts. However, prompting can be ineffective when contexts are sparse, out-of-sample, or extra-textual. In this... | Julia White, Noah D. Goodman, Robert X. D. Hawkins |  |
| 917 |  |  [On Measuring the Intrinsic Few-Shot Hardness of Datasets](https://doi.org/10.18653/v1/2022.emnlp-main.262) |  | 0 | While advances in pre-training have led to dramatic improvements in few-shot learning of NLP tasks, there is limited understanding of what drives successful few-shot adaptation in datasets. In particular, given a new dataset and a pre-trained model, what properties of the dataset make it few-shot... | Xinran Zhao, Shikhar Murty, Christopher D. Manning |  |
| 918 |  |  [Group is better than individual: Exploiting Label Topologies and Label Relations for Joint Multiple Intent Detection and Slot Filling](https://doi.org/10.18653/v1/2022.emnlp-main.263) |  | 0 | Recent joint multiple intent detection and slot filling models employ label embeddings to achieve the semantics-label interactions.However, they treat all labels and label embeddings as uncorrelated individuals, ignoring the dependencies among them. Besides, they conduct the decoding for the two... | Bowen Xing, Ivor W. Tsang |  |
| 919 |  |  [An Empirical Study on Finding Spans](https://doi.org/10.18653/v1/2022.emnlp-main.264) |  | 0 | We present an empirical study on methods for span finding, the selection of consecutive tokens in text for some downstream tasks. We focus on approaches that can be employed in training end-to-end information extraction systems, and find there is no definitive solution without considering task... | Weiwei Gu, Boyuan Zheng, Yunmo Chen, Tongfei Chen, Benjamin Van Durme |  |
| 920 |  |  [MGDoc: Pre-training with Multi-granular Hierarchy for Document Image Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.265) |  | 0 | Document images are a ubiquitous source of data where the text is organized in a complex hierarchical structure ranging from fine granularity (e.g., words), medium granularity (e.g., regions such as paragraphs or figures), to coarse granularity (e.g., the whole page). The spatial hierarchical... | Zilong Wang, Jiuxiang Gu, Chris Tensmeyer, Nikolaos Barmpalios, Ani Nenkova, Tong Sun, Jingbo Shang, Vlad I. Morariu |  |
| 921 |  |  [Understanding Jargon: Combining Extraction and Generation for Definition Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.266) |  | 0 | Can machines know what twin prime is? From the composition of this phrase, machines may guess twin prime is a certain kind of prime, but it is still difficult to deduce exactly what twin stands for without additional knowledge. Here, twin prime is a jargon - a specialized term used by experts in a... | Jie Huang, Hanyin Shao, Kevin ChenChuan Chang, Jinjun Xiong, WenMei Hwu |  |
| 922 |  |  [ProsocialDialog: A Prosocial Backbone for Conversational Agents](https://doi.org/10.18653/v1/2022.emnlp-main.267) |  | 0 | Most existing dialogue systems fail to respond properly to potentially unsafe user utterances by either ignoring or passively agreeing with them. To address this issue, we introduce ProsocialDialog, the first large-scale multi-turn dialogue dataset to teach conversational agents to respond to... | Hyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing Lu, Daniel Khashabi, Gunhee Kim, Yejin Choi, Maarten Sap |  |
| 923 |  |  [Exploiting Global and Local Hierarchies for Hierarchical Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.268) |  | 0 | Hierarchical text classification aims to leverage label hierarchy in multi-label text classification. Existing methods encode label hierarchy in a global view, where label hierarchy is treated as the static hierarchical structure containing all labels. Since global hierarchy is static and... | Ting Jiang, Deqing Wang, Leilei Sun, Zhongzhi Chen, Fuzhen Zhuang, Qinghong Yang |  |
| 924 |  |  [Semantic-aware Contrastive Learning for More Accurate Semantic Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.269) |  | 0 | Since the meaning representations are detailed and accurate annotations which express fine-grained sequence-level semtantics, it is usually hard to train discriminative semantic parsers via Maximum Likelihood Estimation (MLE) in an autoregressive fashion. In this paper, we propose a semantic-aware... | Shan Wu, Chunlei Xin, Bo Chen, Xianpei Han, Le Sun |  |
| 925 |  |  [Scientific Paper Extractive Summarization Enhanced by Citation Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.270) |  | 0 | In a citation graph, adjacent paper nodes share related scientific terms and topics. The graph thus conveys unique structure information of document-level relatedness that can be utilized in the paper summarization task, for exploring beyond the intra-document information.In this work, we focus on... | Xiuying Chen, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, Xiangliang Zhang |  |
| 926 |  |  [Hardness-guided domain adaptation to recognise biomedical named entities under low-resource scenarios](https://doi.org/10.18653/v1/2022.emnlp-main.271) |  | 0 | Domain adaptation is an effective solution to data scarcity in low-resource scenarios. However, when applied to token-level tasks such as bioNER, domain adaptation methods often suffer from the challenging linguistic characteristics that clinical narratives possess, which leads to unsatsifactory... | Ngoc Dang Nguyen, Lan Du, Wray L. Buntine, Changyou Chen, Richard Beare |  |
| 927 |  |  [Syntactic Multi-view Learning for Open Information Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.272) |  | 0 | Open Information Extraction (OpenIE) aims to extract relational tuples from open-domain sentences. Traditional rule-based or statistical models were developed based on syntactic structure of sentence, identified by syntactic parsers. However, previous neural OpenIE models under-explored the useful... | Kuicai Dong, Aixin Sun, JungJae Kim, Xiaoli Li |  |
| 928 |  |  [TRIPS: Efficient Vision-and-Language Pre-training with Text-Relevant Image Patch Selection](https://doi.org/10.18653/v1/2022.emnlp-main.273) |  | 0 | Vision Transformers (ViTs) have been widely used in large-scale Vision and Language Pre-training (VLP) models. Though previous VLP works have proved the effectiveness of ViTs, they still suffer from computational efficiency brought by the long visual sequence. To tackle this problem, in this paper,... | Chaoya Jiang, Haiyang Xu, Chenliang Li, Ming Yan, Wei Ye, Shikun Zhang, Bin Bi, Songfang Huang |  |
| 929 |  |  [CGoDial: A Large-Scale Benchmark for Chinese Goal-oriented Dialog Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.274) |  | 0 | Practical dialog systems need to deal with various knowledge sources, noisy user expressions, and the shortage of annotated data. To better solve the above problems, we propose CGoDial, a new challenging and comprehensive Chinese benchmark for multi-domain Goal-oriented Dialog evaluation. It... | Yinpei Dai, Wanwei He, Bowen Li, Yuchuan Wu, Zheng Cao, Zhongqi An, Jian Sun, Yongbin Li |  |
| 930 |  |  [Kernel-Whitening: Overcome Dataset Bias with Isotropic Sentence Embedding](https://doi.org/10.18653/v1/2022.emnlp-main.275) |  | 0 | Dataset bias has attracted increasing attention recently for its detrimental effect on the generalization ability of fine-tuned models. The current mainstream solution is designing an additional shallow model to pre-identify biased instances. However, such two-stage methods scale up the... | Songyang Gao, Shihan Dou, Qi Zhang, Xuanjing Huang |  |
| 931 |  |  [A Unified Positive-Unlabeled Learning Framework for Document-Level Relation Extraction with Different Levels of Labeling](https://doi.org/10.18653/v1/2022.emnlp-main.276) |  | 0 | Document-level relation extraction (RE) aims to identify relations between entities across multiple sentences. Most previous methods focused on document-level RE under full supervision. However, in real-world scenario, it is expensive and difficult to completely label all relations in a document... | Ye Wang, Xinxin Liu, Wenxin Hu, Tao Zhang |  |
| 932 |  |  [Automatic Generation of Socratic Subquestions for Teaching Math Word Problems](https://doi.org/10.18653/v1/2022.emnlp-main.277) |  | 0 | Socratic questioning is an educational method that allows students to discover answers to complex problems by asking them a series of thoughtful questions. Generation of didactically sound questions is challenging, requiring understanding of the reasoning process involved in the problem. We... | Kumar Shridhar, Jakub Macina, Mennatallah ElAssady, Tanmay Sinha, Manu Kapur, Mrinmaya Sachan |  |
| 933 |  |  [Mixture of Attention Heads: Selecting Attention Heads Per Token](https://doi.org/10.18653/v1/2022.emnlp-main.278) |  | 0 | Mixture-of-Experts (MoE) networks have been proposed as an efficient way to scale up model capacity and implement conditional computing. However, the study of MoE components mostly focused on the feedforward layer in Transformer architecture. This paper proposes the Mixture of Attention Heads... | Xiaofeng Zhang, Yikang Shen, Zeyu Huang, Jie Zhou, Wenge Rong, Zhang Xiong |  |
| 934 |  |  [The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.279) |  | 0 | In this paper, we consider the problem of sparsifying BERT models, which are a key building block for natural language processing, in order to reduce their storage and computational cost. We introduce the Optimal BERT Surgeon (oBERT), an efficient and accurate pruning method based on approximate... | Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, Dan Alistarh |  |
| 935 |  |  [Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue](https://doi.org/10.18653/v1/2022.emnlp-main.280) |  | 0 | Video-grounded Dialogue (VGD) aims to decode an answer sentence to a question regarding a given video and dialogue context. Despite the recent success of multi-modal reasoning to generate answer sentences, existing dialogue systems still suffer from a text hallucination problem, which denotes... | Sunjae Yoon, Eunseop Yoon, Hee Suk Yoon, Junyeong Kim, Chang Dong Yoo |  |
| 936 |  |  [DSM: Question Generation over Knowledge Base via Modeling Diverse Subgraphs with Meta-learner](https://doi.org/10.18653/v1/2022.emnlp-main.281) |  | 0 | Existing methods on knowledge base question generation (KBQG) learn a one-size-fits-all model by training together all subgraphs without distinguishing the diverse semantics of subgraphs. In this work, we show that making use of the past experience on semantically similar subgraphs can reduce the... | Shasha Guo, Jing Zhang, Yanling Wang, Qianyi Zhang, Cuiping Li, Hong Chen |  |
| 937 |  |  [RelU-Net: Syntax-aware Graph U-Net for Relational Triple Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.282) |  | 0 | Relational triple extraction is a critical task for natural language processing. Existing methods mainly focused on capturing semantic information, but suffered from ignoring the syntactic structure of the sentence, which is proved in the relation classification task to contain rich relational... | Yunqi Zhang, Yubo Chen, Yongfeng Huang |  |
| 938 |  |  [Evidence \textgreater Intuition: Transferability Estimation for Encoder Selection](https://doi.org/10.18653/v1/2022.emnlp-main.283) |  | 0 | With the increase in availability of large pre-trained language models (LMs) in Natural Language Processing (NLP), it becomes critical to assess their fit for a specific target task a priori—as fine-tuning the entire space of available LMs is computationally prohibitive and unsustainable. However,... | Elisa Bassignana, Max MüllerEberstein, Mike Zhang, Barbara Plank |  |
| 939 |  |  [Chunk-based Nearest Neighbor Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.284) |  | 0 | Semi-parametric models, which augment generation with retrieval, have led to impressive results in language modeling and machine translation, due to their ability to retrieve fine-grained information from a datastore of examples. One of the most prominent approaches, kNN-MT, exhibits strong domain... | Pedro Henrique Martins, Zita Marinho, André F. T. Martins |  |
| 940 |  |  [FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.285) |  | 0 | Generative models have recently started to outperform extractive models in Open Domain Question Answering, largely by leveraging their decoder to attend over multiple encoded passages and combining their information. However, generative models tend to be larger than extractive models due to the... | Akhil Kedia, Mohd Abbas Zaidi, Haejun Lee |  |
| 941 |  |  [Inductive Relation Prediction with Logical Reasoning Using Contrastive Representations](https://doi.org/10.18653/v1/2022.emnlp-main.286) |  | 0 | Relation prediction in knowledge graphs (KGs) aims at predicting missing relations in incomplete triples, whereas the dominant embedding paradigm has a restriction on handling unseen entities during testing. In the real-world scenario, the inductive setting is more common because entities in the... | Yudai Pan, Jun Liu, Lingling Zhang, Tianzhe Zhao, Qika Lin, Xin Hu, Qianying Wang |  |
| 942 |  |  [Improving Chinese Spelling Check by Character Pronunciation Prediction: The Effects of Adaptivity and Granularity](https://doi.org/10.18653/v1/2022.emnlp-main.287) |  | 0 | Chinese spelling check (CSC) is a fundamental NLP task that detects and corrects spelling errors in Chinese texts. As most of these spelling errors are caused by phonetic similarity, effectively modeling the pronunciation of Chinese characters is a key factor for CSC. In this paper, we consider... | Jiahao Li, Quan Wang, Zhendong Mao, Junbo Guo, Yanyan Yang, Yongdong Zhang |  |
| 943 |  |  [MT-GenEval: A Counterfactual and Contextual Dataset for Evaluating Gender Accuracy in Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.288) |  | 0 | As generic machine translation (MT) quality has improved, the need for targeted benchmarks that explore fine-grained aspects of quality has increased. In particular, gender accuracy in translation can have implications in terms of output fluency, translation accuracy, and ethics. In this paper, we... | Anna Currey, Maria Nadejde, Raghavendra Reddy Pappagari, Mia Mayer, Stanislas Lauly, Xing Niu, Benjamin Hsu, Georgiana Dinu |  |
| 944 |  |  [A Span-level Bidirectional Network for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.289) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) is a new fine-grained sentiment analysis task that aims to extract triplets of aspect terms, sentiments, and opinion terms from review sentences. Recently, span-level models achieve gratifying results on ASTE task by taking advantage of the predictions of... | Yuqi Chen, Keming Chen, Xian Sun, Zequn Zhang |  |
| 945 |  |  [On the Calibration of Massively Multilingual Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.290) |  | 0 | Massively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how... | Kabir Ahuja, Sunayana Sitaram, Sandipan Dandapat, Monojit Choudhury |  |
| 946 |  |  [Momentum Contrastive Pre-training for Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.291) |  | 0 | Existing pre-training methods for extractive Question Answering (QA) generate cloze-like queries different from natural questions in syntax structure, which could overfit pre-trained models to simple keyword matching. In order to address this problem, we propose a novel Momentum Contrastive... | Minda Hu, Muzhi Li, Yasheng Wang, Irwin King |  |
| 947 |  |  [A Second Wave of UD Hebrew Treebanking and Cross-Domain Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.292) |  | 0 | Foundational Hebrew NLP tasks such as segmentation, tagging and parsing, have relied to date on various versions of the Hebrew Treebank (HTB, Sima’an et al. 2001). However, the data in HTB, a single-source newswire corpus, is now over 30 years old, and does not cover many aspects of contemporary... | Amir Zeldes, Nick Howell, Noam Ordan, Yifat Ben Moshe |  |
| 948 |  |  [Finding Dataset Shortcuts with Grammar Induction](https://doi.org/10.18653/v1/2022.emnlp-main.293) |  | 0 | Many NLP datasets have been found to contain shortcuts: simple decision rules that achieve surprisingly high accuracy. However, it is difficult to discover shortcuts automatically. Prior work on automatic shortcut detection has focused on enumerating features like unigrams or bigrams, which can... | Dan Friedman, Alexander Wettig, Danqi Chen |  |
| 949 |  |  [Retrieval Augmentation for Commonsense Reasoning: A Unified Approach](https://doi.org/10.18653/v1/2022.emnlp-main.294) |  | 0 | A common thread of retrieval-augmented methods in the existing literature focuses on retrieving encyclopedic knowledge, such as Wikipedia, which facilitates well-defined entity and relation spaces that can be modeled. However, applying such methods to commonsense reasoning tasks faces two unique... | Wenhao Yu, Chenguang Zhu, Zhihan Zhang, Shuohang Wang, Zhuosheng Zhang, Yuwei Fang, Meng Jiang |  |
| 950 |  |  [Open World Classification with Adaptive Negative Samples](https://doi.org/10.18653/v1/2022.emnlp-main.295) |  | 0 | Open world classification is a task in natural language processing with key practical relevance and impact.Since the open or unknown category data only manifests in the inference phase, finding a model with a suitable decision boundary accommodating for the identification of known classes and... | Ke Bai, Guoyin Wang, Jiwei Li, Sunghyun Park, Sungjin Lee, Puyang Xu, Ricardo Henao, Lawrence Carin |  |
| 951 |  |  [Re3: Generating Longer Stories With Recursive Reprompting and Revision](https://doi.org/10.18653/v1/2022.emnlp-main.296) |  | 0 | We consider the problem of automatically generating longer stories of over two thousand words. Compared to prior work on shorter stories, long-range plot coherence and relevance are more central challenges here. We propose the Recursive Reprompting and Revision framework (Re3) to address these... | Kevin Yang, Yuandong Tian, Nanyun Peng, Dan Klein |  |
| 952 |  |  [Does Joint Training Really Help Cascaded Speech Translation?](https://doi.org/10.18653/v1/2022.emnlp-main.297) |  | 0 | Currently, in speech translation, the straightforward approach - cascading a recognition system with a translation system - delivers state-of-the-art results.However, fundamental challenges such as error propagation from the automatic speech recognition system still remain.To mitigate these... | Viet Anh Khoa Tran, David Thulke, Yingbo Gao, Christian Herold, Hermann Ney |  |
| 953 |  |  [MasakhaNER 2.0: Africa-centric Transfer Learning for Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.298) |  | 0 | African languages are spoken by over a billion people, but they are under-represented in NLP research and development. Multiple challenges exist, including the limited availability of annotated training and evaluation datasets as well as the lack of understanding of which settings, languages, and... | David Ifeoluwa Adelani, Graham Neubig, Sebastian Ruder, Shruti Rijhwani, Michael Beukman, Chester PalenMichel, Constantine Lignos, Jesujoba O. Alabi, Shamsuddeen Hassan Muhammad, Peter Nabende, Cheikh M. Bamba Dione, Andiswa Bukula, Rooweither Mabuya, Bonaventure F. P. Dossou, Blessing K. Sibanda, Happy Buzaaba, Jonathan Mukiibi, Godson Kalipe, Derguene Mbaye, Amelia V. Taylor, Fatoumata Ouoba Kabore, Chris Chinenye Emezue, Aremu Anuoluwapo, Perez Ogayo, Catherine Gitau, Edwin MunkohBuabeng, Victoire Memdjokam Koagne, Allahsera Auguste Tapo, Tebogo Macucwa, Vukosi Marivate, Elvis Mboning, Tajuddeen Gwadabe, Tosin P. Adewumi, Orevaoghene Ahia, Joyce NakatumbaNabende, Neo L. Mokono, Ignatius Ezeani, Chiamaka Chukwuneke, Mofetoluwa Adeyemi, Gilles Hacheme, Idris Abdulmumin, Odunayo Ogundepo, Oreen Yousuf, Tatiana Moteu Ngoli, Dietrich Klakow |  |
| 954 |  |  [Ethics consideration sections in natural language processing papers](https://doi.org/10.18653/v1/2022.emnlp-main.299) |  | 0 | In this paper, we present the results of a manual classification of all ethical consideration sections for ACL 2021. We also compare how many papers had an ethics consideration section per track and per world region in ACL 2021. We classified papers according to the ethical issues covered (research... | Luciana Benotti, Patrick Blackburn |  |
| 955 |  |  [Continued Pretraining for Better Zero- and Few-Shot Promptability](https://doi.org/10.18653/v1/2022.emnlp-main.300) |  | 0 | Recently introduced language model prompting methods can achieve high accuracy in zero- and few-shot settings while requiring few to no learned task-specific parameters. Nevertheless, these methods still often trail behind full model finetuning. In this work, we investigate if a dedicated continued... | Zhaofeng Wu, Robert L. Logan IV, Pete Walsh, Akshita Bhagia, Dirk Groeneveld, Sameer Singh, Iz Beltagy |  |
| 956 |  |  [Less is More: Summary of Long Instructions is Better for Program Synthesis](https://doi.org/10.18653/v1/2022.emnlp-main.301) |  | 0 | Despite the success of large pre-trained language models (LMs) such as Codex, they show below-par performance on the larger and more complicated programming related questions. We show that LMs benefit from the summarized version of complicated questions. Our findings show that superfluous... | Kirby Kuznia, Swaroop Mishra, Mihir Parmar, Chitta Baral |  |
| 957 |  |  [Is a Question Decomposition Unit All We Need?](https://doi.org/10.18653/v1/2022.emnlp-main.302) |  | 0 | Large Language Models (LMs) have achieved state-of-the-art performance on many Natural Language Processing (NLP) benchmarks. With the growing number of new benchmarks, we build bigger and more complex LMs. However, building new LMs may not be an ideal option owing to the cost, time and... | Pruthvi Patel, Swaroop Mishra, Mihir Parmar, Chitta Baral |  |
| 958 |  |  [Discourse-Aware Soft Prompting for Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.303) |  | 0 | Current efficient fine-tuning methods(e.g., adapters, prefix-tuning, etc.) have optimized conditional text generation via training a small set of extra parameters of the neural language model, while freezing the rest for efficiency. While showing strong performance on some generation tasks, they... | Marjan Ghazvininejad, Vladimir Karpukhin, Vera Gor, Asli Celikyilmaz |  |
| 959 |  |  [ExPUNations: Augmenting Puns with Keywords and Explanations](https://doi.org/10.18653/v1/2022.emnlp-main.304) |  | 0 | The tasks of humor understanding and generation are challenging and subjective even for humans, requiring commonsense and real-world knowledge to master. Puns, in particular, add the challenge of fusing that knowledge with the ability to interpret lexical-semantic ambiguity. In this paper, we... | Jiao Sun, Anjali NarayanChen, Shereen Oraby, Alessandra Cervone, Tagyoung Chung, Jing Huang, Yang Liu, Nanyun Peng |  |
| 960 |  |  [SLING: Sino Linguistic Evaluation of Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.305) |  | 0 | To understand what kinds of linguistic knowledge are encoded by pretrained Chinese language models (LMs), we introduce the benchmark of Sino LINGuistics (SLING), which consists of 38K minimal sentence pairs in Mandarin Chinese grouped into 9 high-level linguistic phenomena. Each pair demonstrates... | Yixiao Song, Kalpesh Krishna, Rajesh Bhatt, Mohit Iyyer |  |
| 961 |  |  [Context-Situated Pun Generation](https://doi.org/10.18653/v1/2022.emnlp-main.306) |  | 0 | Previous work on pun generation commonly begins with a given pun word (a pair of homophones for heterographic pun generation and a polyseme for homographic pun generation) and seeks to generate an appropriate pun. While this may enable efficient pun generation, we believe that a pun is most... | Jiao Sun, Anjali NarayanChen, Shereen Oraby, Shuyang Gao, Tagyoung Chung, Jing Huang, Yang Liu, Nanyun Peng |  |
| 962 |  |  [Retrieval-Augmented Generative Question Answering for Event Argument Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.307) |  | 0 | Event argument extraction has long been studied as a sequential prediction problem with extractive-based methods, tackling each argument in isolation. Although recent work proposes generation-based methods to capture cross-argument dependency, they require generating and post-processing a... | Xinya Du, Heng Ji |  |
| 963 |  |  [Concadia: Towards Image-Based Text Generation with a Purpose](https://doi.org/10.18653/v1/2022.emnlp-main.308) |  | 0 | Current deep learning models often achieve excellent results on benchmark image-to-text datasets but fail to generate texts that are useful in practice. We argue that to close this gap, it is vital to distinguish descriptions from captions based on their distinct communicative roles. Descriptions... | Elisa Kreiss, Fei Fang, Noah D. Goodman, Christopher Potts |  |
| 964 |  |  [Context Matters for Image Descriptions for Accessibility: Challenges for Referenceless Evaluation Metrics](https://doi.org/10.18653/v1/2022.emnlp-main.309) |  | 0 | Few images on the Web receive alt-text descriptions that would make them accessible to blind and low vision (BLV) users. Image-based NLG systems have progressed to the point where they can begin to address this persistent societal problem, but these systems will not be fully successful unless we... | Elisa Kreiss, Cynthia L. Bennett, Shayan Hooshmand, Eric Zelikman, Meredith Ringel Morris, Christopher Potts |  |
| 965 |  |  [MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure](https://doi.org/10.18653/v1/2022.emnlp-main.310) |  | 0 | In this paper, we propose a comprehensive benchmark to investigate models’ logical reasoning capabilities in complex real-life scenarios. Current explanation datasets often employ synthetic data with simple reasoning structures. Therefore, it cannot express more complex reasoning processes, such as... | Yinya Huang, Hongming Zhang, Ruixin Hong, Xiaodan Liang, Changshui Zhang, Dong Yu |  |
| 966 |  |  [Explicit Query Rewriting for Conversational Dense Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.311) |  | 0 | In a conversational search scenario, a query might be context-dependent because some words are referred to previous expressions or omitted. Previous works tackle the issue by either reformulating the query into a self-contained query (query rewriting) or learning a contextualized query embedding... | Hongjin Qian, Zhicheng Dou |  |
| 967 |  |  [Efficient Nearest Neighbor Emotion Classification with BERT-whitening](https://doi.org/10.18653/v1/2022.emnlp-main.312) |  | 0 | Retrieval-based methods have been proven effective in many NLP tasks. Previous methods use representations from the pre-trained model for similarity search directly. However, the sentence representations from the pre-trained model like BERT perform poorly in retrieving semantically similar... | Wenbiao Yin, Lin Shang |  |
| 968 |  |  [FastClass: A Time-Efficient Approach to Weakly-Supervised Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.313) |  | 0 | Weakly-supervised text classification aims to train a classifier using only class descriptions and unlabeled data. Recent research shows that keyword-driven methods can achieve state-of-the-art performance on various tasks. However, these methods not only rely on carefully-crafted class... | Tingyu Xia, Yue Wang, Yuan Tian, Yi Chang |  |
| 969 |  |  [Neural-Symbolic Inference for Robust Autoregressive Graph Parsing via Compositional Uncertainty Quantification](https://doi.org/10.18653/v1/2022.emnlp-main.314) |  | 0 | Pre-trained seq2seq models excel at graph semantic parsing with rich annotated data, but generalize worse to out-of-distribution (OOD) and long-tail examples. In comparison, symbolic parsers under-perform on population-level metrics, but exhibit unique strength in OOD and tail generalization. In... | Zi Lin, Jeremiah Z. Liu, Jingbo Shang |  |
| 970 |  |  [A Speaker-Aware Co-Attention Framework for Medical Dialogue Information Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.315) |  | 0 | With the development of medical digitization, the extraction and structuring of Electronic Medical Records (EMRs) have become challenging but fundamental tasks. How to accurately and automatically extract structured information from medical dialogues is especially difficult because the information... | Yuan Xia, Zhenhui Shi, Jingbo Zhou, Jiayu Xu, Chao Lu, Yehui Yang, Lei Wang, Haifeng Huang, Xia Zhang, Junwei Liu |  |
| 971 |  |  [Towards Interactivity and Interpretability: A Rationale-based Legal Judgment Prediction Framework](https://doi.org/10.18653/v1/2022.emnlp-main.316) |  | 0 | Legal judgment prediction (LJP) is a fundamental task in legal AI, which aims to assist the judge to hear the case and determine the judgment. The legal judgment usually consists of the law article, charge, and term of penalty. In the real trial scenario, the judge usually makes the decision... | Yiquan Wu, Yifei Liu, Weiming Lu, Yating Zhang, Jun Feng, Changlong Sun, Fei Wu, Kun Kuang |  |
| 972 |  |  [RelCLIP: Adapting Language-Image Pretraining for Visual Relationship Detection via Relational Contrastive Learning](https://doi.org/10.18653/v1/2022.emnlp-main.317) |  | 0 | Conventional visual relationship detection models only use the numeric ids of relation labels for training, but ignore the semantic correlation between the labels, which leads to severe training biases and harms the generalization ability of representations. In this paper, we introduce compact... | Yi Zhu, Zhaoqing Zhu, Bingqian Lin, Xiaodan Liang, Feng Zhao, Jianzhuang Liu |  |
| 973 |  |  [Candidate Soups: Fusing Candidate Results Improves Translation Quality for Non-Autoregressive Translation](https://doi.org/10.18653/v1/2022.emnlp-main.318) |  | 0 | Non-autoregressive translation (NAT) model achieves a much faster inference speed than the autoregressive translation (AT) model because it can simultaneously predict all tokens during inference. However, its translation quality suffers from degradation compared to AT. And existing NAT methods only... | Huanran Zheng, Wei Zhu, Pengfei Wang, Xiaoling Wang |  |
| 974 |  |  [Evaluating Parameter Efficient Learning for Generation](https://doi.org/10.18653/v1/2022.emnlp-main.319) |  | 0 | Parameter efficient learning methods (PERMs)have recently gained significant attention asthey provide an efficient way for pre-trainedlanguage models (PLMs) to adapt to a downstream task. However, these conclusions aremostly drawn from in-domain evaluations overthe full training set. In this paper,... | Peng Xu, Mostofa Patwary, Shrimai Prabhumoye, Virginia Adams, Ryan Prenger, Wei Ping, Nayeon Lee, Mohammad Shoeybi, Bryan Catanzaro |  |
| 975 |  |  [McQueen: a Benchmark for Multimodal Conversational Query Rewrite](https://doi.org/10.18653/v1/2022.emnlp-main.320) |  | 0 | The task of query rewrite aims to convert an in-context query to its fully-specified version where ellipsis and coreference are completed and referred-back according to the history context. Although much progress has been made, less efforts have been paid to real scenario conversations that involve... | Yifei Yuan, Chen Shi, Runze Wang, Liyi Chen, Feijun Jiang, Yuan You, Wai Lam |  |
| 976 |  |  [Self-supervised Graph Masking Pre-training for Graph-to-Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.321) |  | 0 | Large-scale pre-trained language models (PLMs) have advanced Graph-to-Text (G2T) generation by processing the linearised version of a graph. However, the linearisation is known to ignore the structural information. Additionally, PLMs are typically pre-trained on free text which introduces domain... | Jiuzhou Han, Ehsan Shareghi |  |
| 977 |  |  [Improving Stability of Fine-Tuning Pretrained Language Models via Component-Wise Gradient Norm Clipping](https://doi.org/10.18653/v1/2022.emnlp-main.322) |  | 0 | Fine-tuning over large pretrained language models (PLMs) has established many state-of-the-art results. Despite its superior performance, such fine-tuning can be unstable, resulting in significant variance in performance and potential risks for practical applications. Previous works have attributed... | Chenghao Yang, Xuezhe Ma |  |
| 978 |  |  [Differentially Private Language Models for Secure Data Sharing](https://doi.org/10.18653/v1/2022.emnlp-main.323) |  | 0 | To protect the privacy of individuals whose data is being shared, it is of high importance to develop methods allowing researchers and companies to release textual data while providing formal privacy guarantees to its originators. In the field of NLP, substantial efforts have been directed at... | Justus Mattern, Zhijing Jin, Benjamin Weggenmann, Bernhard Schölkopf, Mrinmaya Sachan |  |
| 979 |  |  [Conditional set generation using Seq2seq models](https://doi.org/10.18653/v1/2022.emnlp-main.324) |  | 0 | Conditional set generation learns a mapping from an input sequence of tokens to a set. Several NLP tasks, such as entity typing and dialogue emotion tagging, are instances of set generation. Seq2Seq models are a popular choice to model set generation but they treat a set as a sequence and do not... | Aman Madaan, Dheeraj Rajagopal, Niket Tandon, Yiming Yang, Antoine Bosselut |  |
| 980 |  |  [Analyzing and Evaluating Faithfulness in Dialogue Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.325) |  | 0 | Dialogue summarization is abstractive in nature, making it suffer from factual errors. The factual correctness of summaries has the highest priority before practical applications. Many efforts have been made to improve faithfulness in text summarization. However, there is a lack of systematic study... | Bin Wang, Chen Zhang, Yan Zhang, Yiming Chen, Haizhou Li |  |
| 981 |  |  [Twist Decoding: Diverse Generators Guide Each Other](https://doi.org/10.18653/v1/2022.emnlp-main.326) |  | 0 | Many language generation models are now available for a wide range of generation tasks, including machine translation and summarization. Combining such diverse models may lead to further progress, but ensembling generation models is challenging during inference: conventional ensembling methods... | Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Hao Peng, Ximing Lu, Dragomir Radev, Yejin Choi, Noah A. Smith |  |
| 982 |  |  [Exploring Representation-level Augmentation for Code Search](https://doi.org/10.18653/v1/2022.emnlp-main.327) |  | 0 | Code search, which aims at retrieving the most relevant code fragment for a given natural language query, is a common activity in software development practice. Recently, contrastive learning is widely used in code search research, where many data augmentation approaches for source code (e.g.,... | Haochen Li, Chunyan Miao, Cyril Leung, Yanxian Huang, Yuan Huang, Hongyu Zhang, Yanlin Wang |  |
| 983 |  |  [Learning Semantic Textual Similarity via Topic-informed Discrete Latent Variables](https://doi.org/10.18653/v1/2022.emnlp-main.328) |  | 0 | Recently, discrete latent variable models have received a surge of interest in both Natural Language Processing (NLP) and Computer Vision (CV), attributed to their comparable performance to the continuous counterparts in representation learning, while being more interpretable in their predictions.... | Erxin Yu, Lan Du, Yuan Jin, Zhepei Wei, Yi Chang |  |
| 984 |  |  [STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension](https://doi.org/10.18653/v1/2022.emnlp-main.329) |  | 0 | Abstractive dialogue summarization has long been viewed as an important standalone task in natural language processing, but no previous work has explored the possibility of whether abstractive dialogue summarization can also be used as a means to boost an NLP system’s performance on other important... | Borui Wang, Chengcheng Feng, Arjun Nair, Madelyn Mao, Jai Desai, Asli Celikyilmaz, Haoran Li, Yashar Mehdad, Dragomir Radev |  |
| 985 |  |  [Competency-Aware Neural Machine Translation: Can Machine Translation Know its Own Translation Quality?](https://doi.org/10.18653/v1/2022.emnlp-main.330) |  | 0 | Neural machine translation (NMT) is often criticized for failures that happenwithout awareness. The lack of competency awareness makes NMT untrustworthy. This is in sharp contrast to human translators who give feedback or conduct further investigations whenever they are in doubt about predictions.... | Pei Zhang, Baosong Yang, Haoran Wei, Dayiheng Liu, Kai Fan, Luo Si, Jun Xie |  |
| 986 |  |  [PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training](https://doi.org/10.18653/v1/2022.emnlp-main.331) |  | 0 | Fact verification has attracted a lot of attention recently, e.g., in journalism, marketing, and policymaking, as misinformation and dis- information can sway one’s opinion and affect one’s actions. While fact-checking is a hard task in general, in many cases, false statements can be easily... | Zihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiaoman Zhao, Xiaoyong Du |  |
| 987 |  |  [Sentiment-Aware Word and Sentence Level Pre-training for Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.332) |  | 0 | Most existing pre-trained language representation models (PLMs) are sub-optimal in sentiment analysis tasks, as they capture the sentiment information from word-level while under-considering sentence-level information. In this paper, we propose SentiWSP, a novel Sentiment-aware pre-trained language... | Shuai Fan, Chen Lin, Haonan Li, Zhenghao Lin, Jinsong Su, Hang Zhang, Yeyun Gong, Jian Guo, Nan Duan |  |
| 988 |  |  [Towards Multi-Modal Sarcasm Detection via Hierarchical Congruity Modeling with Knowledge Enhancement](https://doi.org/10.18653/v1/2022.emnlp-main.333) |  | 0 | Sarcasm is a linguistic phenomenon indicating a discrepancy between literal meanings and implied intentions. Due to its sophisticated nature, it is usually difficult to be detected from the text itself. As a result, multi-modal sarcasm detection has received more and more attention in both academia... | Hui Liu, Wenya Wang, Haoliang Li |  |
| 989 |  |  [Efficiently Tuned Parameters Are Task Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.334) |  | 0 | Intermediate-task transfer can benefit a wide range of NLP tasks with properly selected source datasets. However, it is computationally infeasible to experiment with all intermediate transfer combinations, making choosing a useful source task a challenging problem. In this paper, we anticipate that... | Wangchunshu Zhou, Canwen Xu, Julian J. McAuley |  |
| 990 |  |  [COPEN: Probing Conceptual Knowledge in Pre-trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.335) |  | 0 | Conceptual knowledge is fundamental to human cognition and knowledge bases. However, existing knowledge probing works only focus on evaluating factual knowledge of pre-trained language models (PLMs) and ignore conceptual knowledge. Since conceptual knowledge often appears as implicit commonsense... | Hao Peng, Xiaozhi Wang, Shengding Hu, Hailong Jin, Lei Hou, Juanzi Li, Zhiyuan Liu, Qun Liu |  |
| 991 |  |  [Capturing Global Structural Information in Long Document Question Answering with Compressive Graph Selector Network](https://doi.org/10.18653/v1/2022.emnlp-main.336) |  | 0 | Long document question answering is a challenging task due to its demands for complex reasoning over long text. Previous works usually take long documents as non-structured flat texts or only consider the local structure in long documents. However, these methods usually ignore the global structure... | Yuxiang Nie, Heyan Huang, Wei Wei, XianLing Mao |  |
| 992 |  |  [Structural generalization is hard for sequence-to-sequence models](https://doi.org/10.18653/v1/2022.emnlp-main.337) |  | 0 | Sequence-to-sequence (seq2seq) models have been successful across many NLP tasks,including ones that require predicting linguistic structure. However, recent work on compositional generalization has shown that seq2seq models achieve very low accuracy in generalizing to linguistic structures that... | Yuekun Yao, Alexander Koller |  |
| 993 |  |  [Contrastive Learning enhanced Author-Style Headline Generation](https://doi.org/10.18653/v1/2022.emnlp-main.338) |  | 0 | Headline generation is a task of generating an appropriate headline for a given article, which can be further used for machine-aided writing or enhancing the click-through ratio. Current works only use the article itself in the generation, but have not taken the writing style of headlines into... | Hui Liu, Weidong Guo, Yige Chen, Xiangyang Li |  |
| 994 |  |  [Multi-Granularity Optimization for Non-Autoregressive Translation](https://doi.org/10.18653/v1/2022.emnlp-main.339) |  | 0 | Despite low latency, non-autoregressive machine translation (NAT) suffers severe performance deterioration due to the naive independence assumption. This assumption is further strengthened by cross-entropy loss, which encourages a strict match between the hypothesis and the reference token by... | Yafu Li, Leyang Cui, Yongjing Yin, Yue Zhang |  |
| 995 |  |  [Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.340) |  | 0 | How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types,... | Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, Xudong Shen |  |
| 996 |  |  [MetaFill: Text Infilling for Meta-Path Generation on Heterogeneous Information Networks](https://doi.org/10.18653/v1/2022.emnlp-main.341) |  | 0 | Heterogeneous information network (HIN) is essential to study complicated networks containing multiple edge types and node types. Meta-path, a sequence of node types and edge types, is the core technique to embed HINs. Since manually curating meta-paths is time-consuming, there is a pressing need... | Zequn Liu, Kefei Duan, Junwei Yang, Hanwen Xu, Ming Zhang, Sheng Wang |  |
| 997 |  |  [DRLK: Dynamic Hierarchical Reasoning with Language Model and Knowledge Graph for Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.342) |  | 0 | In recent years, Graph Neural Network (GNN) approaches with enhanced knowledge graphs (KG) perform well in question answering (QA) tasks. One critical challenge is how to effectively utilize interactions between the QA context and KG. However, existing work only adopts the identical QA context... | Miao Zhang, Rufeng Dai, Ming Dong, Tingting He |  |
| 998 |  |  [AEG: Argumentative Essay Generation via A Dual-Decoder Model with Content Planning](https://doi.org/10.18653/v1/2022.emnlp-main.343) |  | 0 | Argument generation is an important but challenging task in computational argumentation.Existing studies have mainly focused on generating individual short arguments, while research on generating long and coherent argumentative essays is still under-explored.In this paper, we propose a new task,... | Jianzhu Bao, Yasheng Wang, Yitong Li, Fei Mi, Ruifeng Xu |  |
| 999 |  |  [BotsTalk: Machine-sourced Framework for Automatic Curation of Large-scale Multi-skill Dialogue Datasets](https://doi.org/10.18653/v1/2022.emnlp-main.344) |  | 0 | To build open-domain chatbots that are able to use diverse communicative skills, we propose a novel framework BotsTalk, where multiple agents grounded to the specific target skills participate in a conversation to automatically annotate multi-skill dialogues. We further present Blended Skill... | Minju Kim, Chaehyeong Kim, Yongho Song, Seungwon Hwang, Jinyoung Yeo |  |
| 1000 |  |  [Wider & Closer: Mixture of Short-channel Distillers for Zero-shot Cross-lingual Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.345) |  | 0 | Zero-shot cross-lingual named entity recognition (NER) aims at transferring knowledge from annotated and rich-resource data in source languages to unlabeled and lean-resource data in target languages. Existing mainstream methods based on the teacher-student distillation framework ignore the rich... | JunYu Ma, Beiduo Chen, JiaChen Gu, Zhenhua Ling, Wu Guo, Quan Liu, Zhigang Chen, Cong Liu |  |
| 1001 |  |  [An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.346) |  | 0 | Access to external knowledge is essential for many natural language processing tasks, such as question answering and dialogue. Existing methods often rely on a parametric model that stores knowledge in its parameters, or use a retrieval-augmented model that has access to an external knowledge... | Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel |  |
| 1002 |  |  [Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation](https://doi.org/10.18653/v1/2022.emnlp-main.347) |  | 0 | Capturing emotions within a conversation plays an essential role in modern dialogue systems. However, the weak correlation between emotions and semantics brings many challenges to emotion recognition in conversation (ERC). Even semantically similar utterances, the emotion may vary drastically... | Xiaohui Song, Longtao Huang, Hui Xue, Songlin Hu |  |
| 1003 |  |  [RuCoLA: Russian Corpus of Linguistic Acceptability](https://doi.org/10.18653/v1/2022.emnlp-main.348) |  | 0 | Linguistic acceptability (LA) attracts the attention of the research community due to its many uses, such as testing the grammatical knowledge of language models and filtering implausible texts with acceptability classifiers.However, the application scope of LA in languages other than English is... | Vladislav Mikhailov, Tatiana Shamardina, Max Ryabinin, Alena Pestova, Ivan Smurov, Ekaterina Artemova |  |
| 1004 |  |  [Complex Hyperbolic Knowledge Graph Embeddings with Fast Fourier Transform](https://doi.org/10.18653/v1/2022.emnlp-main.349) |  | 0 | The choice of geometric space for knowledge graph (KG) embeddings can have significant effects on the performance of KG completion tasks. The hyperbolic geometry has been shown to capture the hierarchical patterns due to its tree-like metrics, which addressed the limitations of the Euclidean... | Huiru Xiao, Xin Liu, Yangqiu Song, Ginny Y. Wong, Simon See |  |
| 1005 |  |  [Towards Knowledge-Intensive Text-to-SQL Semantic Parsing with Formulaic Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.350) |  | 0 | In this paper, we study the problem of knowledge-intensive text-to-SQL, in which domain knowledge is necessary to parse expert questions into SQL queries over domain-specific tables. We formalize this scenario by building a new benchmark KnowSQL consisting of domain-specific questions covering... | Longxu Dou, Yan Gao, Xuqi Liu, Mingyang Pan, Dingzirui Wang, Wanxiang Che, Dechen Zhan, MinYen Kan, JianGuang Lou |  |
| 1006 |  |  [Should We Ban English NLP for a Year?](https://doi.org/10.18653/v1/2022.emnlp-main.351) |  | 0 | Around two thirds of NLP research at top venues is devoted exclusively to developing technology for speakers of English, most speech data comes from young urban speakers, and most texts used to train language models come from male writers. These biases feed into consumer technologies to widen... | Anders Søgaard |  |
| 1007 |  |  [LittleBird: Efficient Faster & Longer Transformer for Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.352) |  | 0 | BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a limitation dealing with long inputs due to its attention mechanism. Longformer, ETC and BigBird addressed this issue and effectively solved the quadratic dependency problem.However we find that these models are not... | Minchul Lee, Kijong Han, Myeong Cheol Shin |  |
| 1008 |  |  [WeTS: A Benchmark for Translation Suggestion](https://doi.org/10.18653/v1/2022.emnlp-main.353) |  | 0 | Translation suggestion (TS), which provides alternatives for specific words or phrases given the entire documents generated by machine translation (MT), has been proven to play a significant role in post-editing (PE). There are two main pitfalls for existing researches in this line. First, most... | Zhen Yang, Fandong Meng, Yingxue Zhang, Ernan Li, Jie Zhou |  |
| 1009 |  |  [Discrete Cross-Modal Alignment Enables Zero-Shot Speech Translation](https://doi.org/10.18653/v1/2022.emnlp-main.354) |  | 0 | End-to-end Speech Translation (ST) aims at translating the source language speech into target language text without generating the intermediate transcriptions. However, the training of end-to-end methods relies on parallel ST data, which are difficult and expensive to obtain. Fortunately, the... | Chen Wang, Yuchen Liu, Boxing Chen, Jiajun Zhang, Wei Luo, Zhongqiang Huang, Chengqing Zong |  |
| 1010 |  |  [Abstractive Summarization Guided by Latent Hierarchical Document Structure](https://doi.org/10.18653/v1/2022.emnlp-main.355) |  | 0 | Sequential abstractive neural summarizers often do not use the underlying structure in the input article or dependencies between the input sentences. This structure is essential to integrate and consolidate information from different parts of the text. To address this shortcoming, we propose a... | Yifu Qiu, Shay B. Cohen |  |
| 1011 |  |  [Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.356) |  | 0 | Multi-hop Question Answering is an agent task for testing the reasoning ability. With the development of pre-trained models, the implicit reasoning ability has been surprisingly improved and can even surpass human performance. However, the nature of the black box hinders the construction of... | Jianguo Mao, Wenbin Jiang, Xiangdong Wang, Hong Liu, Yu Xia, Yajuan Lyu, Qiaoqiao She |  |
| 1012 |  |  [DuReader-Retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine](https://doi.org/10.18653/v1/2022.emnlp-main.357) |  | 0 | In this paper, we present DuReader-retrieval, a large-scale Chinese dataset for passage retrieval. DuReader-retrieval contains more than 90K queries and over 8M unique passages from a commercial search engine. To alleviate the shortcomings of other datasets and ensure the quality of our benchmark,... | Yifu Qiu, Hongyu Li, Yingqi Qu, Ying Chen, Qiaoqiao She, Jing Liu, Hua Wu, Haifeng Wang |  |
| 1013 |  |  [Pair-Based Joint Encoding with Relational Graph Convolutional Networks for Emotion-Cause Pair Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.358) |  | 0 | Emotion-cause pair extraction (ECPE) aims to extract emotion clauses and corresponding cause clauses, which have recently received growing attention. Previous methods sequentially encode features with a specified order. They first encode the emotion and cause features for clause extraction and then... | Junlong Liu, Xichen Shang, Qianli Ma |  |
| 1014 |  |  [Affective Knowledge Enhanced Multiple-Graph Fusion Networks for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.359) |  | 0 | Aspect-based sentiment analysis aims to identify sentiment polarity of social media users toward different aspects. Most recent methods adopt the aspect-centric latent tree to connect aspects and their corresponding opinion words, thinking that would facilitate establishing the relationship between... | Siyu Tang, Heyan Chai, Ziyi Yao, Ye Ding, Cuiyun Gao, Binxing Fang, Qing Liao |  |
| 1015 |  |  [IndicNLG Benchmark: Multilingual Datasets for Diverse NLG Tasks in Indic Languages](https://doi.org/10.18653/v1/2022.emnlp-main.360) |  | 0 | Natural Language Generation (NLG) for non-English languages is hampered by the scarcity of datasets in these languages. We present the IndicNLG Benchmark, a collection of datasets for benchmarking NLG for 11 Indic languages. We focus on five diverse tasks, namely, biography generation using... | Aman Kumar, Himani Shrotriya, Prachi Sahu, Amogh Mishra, Raj Dabre, Ratish Puduppully, Anoop Kunchukuttan, Mitesh M. Khapra, Pratyush Kumar |  |
| 1016 |  |  [Improving Machine Translation with Phrase Pair Injection and Corpus Filtering](https://doi.org/10.18653/v1/2022.emnlp-main.361) |  | 0 | In this paper, we show that the combination of Phrase Pair Injection and Corpus Filtering boosts the performance of Neural Machine Translation (NMT) systems. We extract parallel phrases and sentences from the pseudo-parallel corpus and augment it with the parallel corpus to train the NMT models.... | Akshay Batheja, Pushpak Bhattacharyya |  |
| 1017 |  |  [An Anchor-based Relative Position Embedding Method for Cross-Modal Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.362) |  | 0 | Position Embedding (PE) is essential for transformer to capture the sequence ordering of input tokens. Despite its general effectiveness verified in Natural Language Processing (NLP) and Computer Vision (CV), its application in cross-modal tasks remains unexplored and suffers from two challenges:... | Ya Wang, Xingwu Sun, Fengzong Lian, Zhanhui Kang, Chengzhong Xu |  |
| 1018 |  |  [Norm-based Noisy Corpora Filtering and Refurbishing in Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.363) |  | 0 | Recent advances in neural machine translation depend on massive parallel corpora, which are collected from any open source without much guarantee of quality. It stresses the need for noisy corpora filtering, but existing methods are insufficient to solve this issue. They spend much time ensembling... | Yu Lu, Jiajun Zhang |  |
| 1019 |  |  [TeleMelody: Lyric-to-Melody Generation with a Template-Based Two-Stage Method](https://doi.org/10.18653/v1/2022.emnlp-main.364) |  | 0 | Lyric-to-melody generation is an important task in automatic songwriting. Previous lyric-to-melody generation systems usually adopt end-to-end models that directly generate melodies from lyrics, which suffer from several issues: 1) lack of paired lyric-melody training data; 2) lack of control on... | Zeqian Ju, Peiling Lu, Xu Tan, Rui Wang, Chen Zhang, Songruoyao Wu, Kejun Zhang, XiangYang Li, Tao Qin, TieYan Liu |  |
| 1020 |  |  [SEEN: Structured Event Enhancement Network for Explainable Need Detection of Information Recall Assistance](https://doi.org/10.18653/v1/2022.emnlp-main.365) |  | 0 | When recalling life experiences, people often forget or confuse life events, which necessitates information recall services. Previous work on information recall focuses on providing such assistance reactively, i.e., by retrieving the life event of a given query. Proactively detecting the need for... | YouEn Lin, AnZi Yen, HenHsen Huang, HsinHsi Chen |  |
| 1021 |  |  [Rethinking Style Transformer with Energy-based Interpretation: Adversarial Unsupervised Style Transfer using a Pretrained Model](https://doi.org/10.18653/v1/2022.emnlp-main.366) |  | 0 | Style control, content preservation, and fluency determine the quality of text style transfer models. To train on a nonparallel corpus, several existing approaches aim to deceive the style discriminator with an adversarial loss. However, adversarial training significantly degrades fluency compared... | Hojun Cho, Dohee Kim, Seungwoo Ryu, ChaeHun Park, Hyungjong Noh, JeongIn Hwang, Minseok Choi, Edward Choi, Jaegul Choo |  |
| 1022 |  |  [Towards Robust k-Nearest-Neighbor Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.367) |  | 0 | k-Nearest-Neighbor Machine Translation (kNN-MT) becomes an important research direction of NMT in recent years. Its main idea is to retrieve useful key-value pairs from an additional datastore to modify translations without updating the NMT model. However, the underlying retrieved noisy pairs will... | Hui Jiang, Ziyao Lu, Fandong Meng, Chulun Zhou, Jie Zhou, Degen Huang, Jinsong Su |  |
| 1023 |  |  [Tiny-NewsRec: Effective and Efficient PLM-based News Recommendation](https://doi.org/10.18653/v1/2022.emnlp-main.368) |  | 0 | News recommendation is a widely adopted technique to provide personalized news feeds for the user. Recently, pre-trained language models (PLMs) have demonstrated the great capability of natural language understanding and benefited news recommendation via improving news modeling. However, most... | Yang Yu, Fangzhao Wu, Chuhan Wu, Jingwei Yi, Qi Liu |  |
| 1024 |  |  [TABS: Efficient Textual Adversarial Attack for Pre-trained NL Code Model Using Semantic Beam Search](https://doi.org/10.18653/v1/2022.emnlp-main.369) |  | 0 | As pre-trained models have shown successful performance in program language processing as well as natural language processing, adversarial attacks on these models also attract attention.However, previous works on black-box adversarial attacks generated adversarial examples in a very inefficient way... | YunSeok Choi, Hyojun Kim, JeeHyong Lee |  |
| 1025 |  |  [Investigating the Robustness of Natural Language Generation from Logical Forms via Counterfactual Samples](https://doi.org/10.18653/v1/2022.emnlp-main.370) |  | 0 | The aim of Logic2Text is to generate controllable and faithful texts conditioned on tables and logical forms, which not only requires a deep understanding of the tables and logical forms, but also warrants symbolic reasoning over the tables according to the logical forms. State-of-the-art methods... | Chengyuan Liu, Leilei Gan, Kun Kuang, Fei Wu |  |
| 1026 |  |  [Helping the Weak Makes You Strong: Simple Multi-Task Learning Improves Non-Autoregressive Translators](https://doi.org/10.18653/v1/2022.emnlp-main.371) |  | 0 | Recently, non-autoregressive (NAR) neural machine translation models have received increasing attention due to their efficient parallel decoding.However, the probabilistic framework of NAR models necessitates conditional independence assumption on target sequences, falling short of characterizing... | Xinyou Wang, Zaixiang Zheng, Shujian Huang |  |
| 1027 |  |  [RACE: Retrieval-augmented Commit Message Generation](https://doi.org/10.18653/v1/2022.emnlp-main.372) |  | 0 | Commit messages are important for software development and maintenance. Many neural network-based approaches have been proposed and shown promising results on automatic commit message generation. However, the generated commit messages could be repetitive or redundant. In this paper, we propose... | Ensheng Shi, Yanlin Wang, Wei Tao, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Hongbin Sun |  |
| 1028 |  |  [PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.373) |  | 0 | Logical table-to-text generation is a task that involves generating logically faithful sentences from tables, which requires models to derive logical-level facts from table records via logical inference. It raises a new challenge on the logical-level content planning of table-to-text models.... | Ao Liu, Haoyu Dong, Naoaki Okazaki, Shi Han, Dongmei Zhang |  |
| 1029 |  |  [GHAN: Graph-Based Hierarchical Aggregation Network for Text-Video Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.374) |  | 0 | Text-video retrieval focuses on two aspects: cross-modality interaction and video-language encoding. Currently, the mainstream approach is to train a joint embedding space for multimodal interactions. However, there are structural and semantic differences between text and video, making this... | Yahan Yu, Bojie Hu, Yu Li |  |
| 1030 |  |  [MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text](https://doi.org/10.18653/v1/2022.emnlp-main.375) |  | 0 | While language Models store a massive amount of world knowledge implicitly in their parameters, even very large models often fail to encode information about rare entities and events, while incurring huge computational costs. Recently, retrieval-augmented models, such as REALM, RAG, and RETRO, have... | Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen |  |
| 1031 |  |  [PHEE: A Dataset for Pharmacovigilance Event Extraction from Text](https://doi.org/10.18653/v1/2022.emnlp-main.376) |  | 0 | The primary goal of drug safety researchers and regulators is to promptly identify adverse drug reactions. Doing so may in turn prevent or reduce the harm to patients and ultimately improve public health. Evaluating and monitoring drug safety (i.e., pharmacovigilance) involves analyzing an ever... | Zhaoyue Sun, Jiazheng Li, Gabriele Pergola, Byron C. Wallace, Bino John, Nigel Greene, Joseph Kim, Yulan He |  |
| 1032 |  |  [OTSeq2Set: An Optimal Transport Enhanced Sequence-to-Set Model for Extreme Multi-label Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.377) |  | 0 | Extreme multi-label text classification (XMTC) is the task of finding the most relevant subset labels from an extremely large-scale label collection. Recently, some deep learning models have achieved state-of-the-art results in XMTC tasks. These models commonly predict scores for all labels by a... | Jie Cao, Yin Zhang |  |
| 1033 |  |  [SimQA: Detecting Simultaneous MT Errors through Word-by-Word Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.378) |  | 0 | Detractors of neural machine translation admit that while its translations are fluent, it sometimes gets key facts wrong. This is particularly important in simultaneous interpretation where translations have to be provided as fast as possible: before a sentence is complete. Yet, evaluations of... | HyoJung Han, Marine Carpuat, Jordan L. BoydGraber |  |
| 1034 |  |  [Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations](https://doi.org/10.18653/v1/2022.emnlp-main.379) |  | 0 | Large pretrained multilingual language models (ML-LMs) have shown remarkable capabilities of zero-shot cross-lingual transfer, without direct cross-lingual supervision. While these results are promising, follow-up works found that, within the multilingual embedding spaces, there exists strong... | Zhihui Xie, Handong Zhao, Tong Yu, Shuai Li |  |
| 1035 |  |  [Rethinking the Authorship Verification Experimental Setups](https://doi.org/10.18653/v1/2022.emnlp-main.380) |  | 0 | One of the main drivers of the recent advances in authorship verification is the PAN large-scale authorship dataset. Despite generating significant progress in the field, inconsistent performance differences between the closed and open test sets have been reported. To this end, we improve the... | Florin Brad, Andrei Manolache, Elena Burceanu, Antonio Barbalau, Radu Tudor Ionescu, Marius Popescu |  |
| 1036 |  |  [Borrowing Human Senses: Comment-Aware Self-Training for Social Media Multimodal Classification](https://doi.org/10.18653/v1/2022.emnlp-main.381) |  | 0 | Social media is daily creating massive multimedia content with paired image and text, presenting the pressing need to automate the vision and language understanding for various multimodal classification tasks. Compared to the commonly researched visual-lingual data, social media posts tend to... | Chunpu Xu, Jing Li |  |
| 1037 |  |  [Training Language Models with Memory Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.382) |  | 0 | Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language... | Zexuan Zhong, Tao Lei, Danqi Chen |  |
| 1038 |  |  [Data-Efficient Strategies for Expanding Hate Speech Detection into Under-Resourced Languages](https://doi.org/10.18653/v1/2022.emnlp-main.383) |  | 0 | Hate speech is a global phenomenon, but most hate speech datasets so far focus on English-language content. This hinders the development of more effective hate speech detection models in hundreds of languages spoken by billions across the world. More data is needed, but annotating hateful content... | Paul Röttger, Debora Nozza, Federico Bianchi, Dirk Hovy |  |
| 1039 |  |  [Dimension Reduction for Efficient Dense Retrieval via Conditional Autoencoder](https://doi.org/10.18653/v1/2022.emnlp-main.384) |  | 0 | Dense retrievers encode queries and documents and map them in an embedding space using pre-trained language models. These embeddings need to be high-dimensional to fit training signals and guarantee the retrieval effectiveness of dense retrievers. However, these high-dimensional embeddings lead to... | Zhenghao Liu, Han Zhang, Chenyan Xiong, Zhiyuan Liu, Yu Gu, Xiaohua Li |  |
| 1040 |  |  [Controlled Text Reduction](https://doi.org/10.18653/v1/2022.emnlp-main.385) |  | 0 | Producing a reduced version of a source text, as in generic or focused summarization, inherently involves two distinct subtasks: deciding on targeted content and generating a coherent text conveying it. While some popular approaches address summarization as a single end-to-end task, prominent works... | Aviv Slobodkin, Paul Roit, Eran Hirsch, Ori Ernst, Ido Dagan |  |
| 1041 |  |  [Questioning the Validity of Summarization Datasets and Improving Their Factual Consistency](https://doi.org/10.18653/v1/2022.emnlp-main.386) |  | 0 | The topic of summarization evaluation has recently attracted a surge of attention due to the rapid development of abstractive summarization systems. However, the formulation of the task is rather ambiguous, neither the linguistic nor the natural language processing communities have succeeded in... | Yanzhu Guo, Chloé Clavel, Moussa Kamal Eddine, Michalis Vazirgiannis |  |
| 1042 |  |  [Invariant Language Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.387) |  | 0 | Modern pretrained language models are critical components of NLP pipelines. Yet, they suffer from spurious correlations, poor out-of-domain generalization, and biases.Inspired by recent progress in causal machine learning, in particular the invariant risk minimization (IRM) paradigm, we propose... | Maxime Peyrard, Sarvjeet Singh Ghotra, Martin Josifoski, Vidhan Agarwal, Barun Patra, Dean Carignan, Emre Kiciman, Saurabh Tiwary, Robert West |  |
| 1043 |  |  [AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning](https://doi.org/10.18653/v1/2022.emnlp-main.388) |  | 0 | Standard fine-tuning of large pre-trained language models (PLMs) for downstream tasks requires updating hundreds of millions to billions of parameters, and storing a large copy of the PLM weights for every task resulting in increased cost for storing, sharing and serving the models. To address... | Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, Jianfeng Gao |  |
| 1044 |  |  [How "Multi" is Multi-Document Summarization?](https://doi.org/10.18653/v1/2022.emnlp-main.389) |  | 0 | The task of multi-document summarization (MDS) aims at models that, given multiple documents as input, are able to generate a summary that combines disperse information, originally spread __across__ these documents. Accordingly, it is expected that both reference summaries in MDS datasets, as well... | Ruben Wolhandler, Arie Cattan, Ori Ernst, Ido Dagan |  |
| 1045 |  |  [BioReader: a Retrieval-Enhanced Text-to-Text Transformer for Biomedical Literature](https://doi.org/10.18653/v1/2022.emnlp-main.390) |  | 0 | The latest batch of research has equipped language models with the ability to attend over relevant and factual information from non-parametric external sources, drawing a complementary path to architectural scaling. Besides mastering language, exploiting and contextualizing the latent world... | Giacomo Frisoni, Miki Mizutani, Gianluca Moro, Lorenzo Valgimigli |  |
| 1046 |  |  [T-Modules: Translation Modules for Zero-Shot Cross-Modal Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.391) |  | 0 | We present a new approach to perform zero-shot cross-modal transfer between speech and text for translation tasks. Multilingual speech and text are encoded in a joint fixed-size representation space. Then, we compare different approaches to decode these multimodal and multilingual fixed-size... | PaulAmbroise Duquenne, Hongyu Gong, Benoît Sagot, Holger Schwenk |  |
| 1047 |  |  [LILA: A Unified Benchmark for Mathematical Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.392) |  | 0 | Mathematical reasoning skills are essential for general-purpose intelligentsystems to perform tasks from grocery shopping to climate modeling.Towards evaluating and improving AI systems in this domain, we proposeLILA, a unified mathematical reasoning benchmark consisting of 23 diversetasks along... | Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin Kalyan |  |
| 1048 |  |  [Leveraging Affirmative Interpretations from Negation Improves Natural Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.393) |  | 0 | Negation poses a challenge in many natural language understanding tasks. Inspired by the fact that understanding a negated statement often requires humans to infer affirmative interpretations, in this paper we show that doing so benefits models for three natural language understanding tasks. We... | Md Mosharaf Hossain, Eduardo Blanco |  |
| 1049 |  |  [GraphQ IR: Unifying the Semantic Parsing of Graph Query Languages with One Intermediate Representation](https://doi.org/10.18653/v1/2022.emnlp-main.394) |  | 0 | Subject to the huge semantic gap between natural and formal languages, neural semantic parsing is typically bottlenecked by its complexity of dealing with both input semantics and output syntax. Recent works have proposed several forms of supplementary supervision but none is generalized across... | Lunyiu Nie, Shulin Cao, Jiaxin Shi, Jiuding Sun, Qi Tian, Lei Hou, Juanzi Li, Jidong Zhai |  |
| 1050 |  |  [InforMask: Unsupervised Informative Masking for Language Model Pretraining](https://doi.org/10.18653/v1/2022.emnlp-main.395) |  | 0 | Masked language modeling is widely used for pretraining large language models for natural language understanding (NLU). However, random masking is suboptimal, allocating an equal masking rate for all tokens. In this paper, we propose InforMask, a new unsupervised masking strategy for training... | Nafis Sadeq, Canwen Xu, Julian J. McAuley |  |
| 1051 |  |  [CTRLsum: Towards Generic Controllable Text Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.396) |  | 0 | Current summarization systems yield generic summaries that are disconnected from users’ preferences and expectations. To address this limitation, we present CTRLsum, a generic framework to control generated summaries through a set of keywords. During training keywords are extracted automatically... | Junxian He, Wojciech Kryscinski, Bryan McCann, Nazneen Rajani, Caiming Xiong |  |
| 1052 |  |  [Missing Counter-Evidence Renders NLP Fact-Checking Unrealistic for Misinformation](https://doi.org/10.18653/v1/2022.emnlp-main.397) |  | 0 | Misinformation emerges in times of uncertainty when credible information is limited. This is challenging for NLP-based fact-checking as it relies on counter-evidence, which may not yet be available. Despite increasing interest in automatic fact-checking, it is still unclear if automated approaches... | Max Glockner, Yufang Hou, Iryna Gurevych |  |
| 1053 |  |  [A Framework for Adapting Pre-Trained Language Models to Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.emnlp-main.398) |  | 0 | Recent work has demonstrated that entity representations can be extracted from pre-trained language models to develop knowledge graph completion models that are more robust to the naturally occurring sparsity found in knowledge graphs. In this work, we conduct a comprehensive exploration of how to... | Justin Lovelace, Carolyn P. Rosé |  |
| 1054 |  |  [Mutual Information Alleviates Hallucinations in Abstractive Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.399) |  | 0 | Despite significant progress in the quality of language generated from abstractive summarization models, these models still exhibit the tendency to hallucinate, i.e., output content not supported by the source document. A number of works have tried to fix—or at least uncover the source of—the... | Liam van der Poel, Ryan Cotterell, Clara Meister |  |
| 1055 |  |  [Toward the Limitation of Code-Switching in Cross-Lingual Transfer](https://doi.org/10.18653/v1/2022.emnlp-main.400) |  | 0 | Multilingual pretrained models have shown strong cross-lingual transfer ability. Some works used code-switching sentences, which consist of tokens from multiple languages, to enhance the cross-lingual representation further, and have shown success in many zero-shot cross-lingual tasks. However,... | Yukun Feng, Feng Li, Philipp Koehn |  |
| 1056 |  |  [Syntactically Rich Discriminative Training: An Effective Method for Open Information Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.401) |  | 0 | Open information extraction (OIE) is the task of extracting facts "(Subject, Relation, Object)” from natural language text. We propose several new methods for training neural OIE models in this paper. First, we propose a novel method for computing syntactically rich text embeddings using the... | Frank Mtumbuka, Thomas Lukasiewicz |  |
| 1057 |  |  [Transformer-based Entity Typing in Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.402) |  | 0 | We investigate the knowledge graph entity typing task which aims at inferring plausible entity types. In this paper, we propose a novel Transformer-based Entity Typing (TET) approach, effectively encoding the content of neighbours of an entity by means of a transformer mechanism. More precisely,... | Zhiwei Hu, Víctor GutiérrezBasulto, Zhiliang Xiang, Ru Li, Jeff Z. Pan |  |
| 1058 |  |  [NewsClaims: A New Benchmark for Claim Detection from News with Attribute Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.403) |  | 0 | Claim detection and verification are crucial for news understanding and have emerged as promising technologies for mitigating misinformation and disinformation in the news. However, most existing work has focused on claim sentence analysis while overlooking additional crucial attributes (e.g., the... | Revanth Gangi Reddy, Sai Chetan Chinthakindi, Zhenhailong Wang, Yi R. Fung, Kathryn Conger, Ahmed Elsayed, Martha Palmer, Preslav Nakov, Eduard H. Hovy, Kevin Small, Heng Ji |  |
| 1059 |  |  [IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces](https://doi.org/10.18653/v1/2022.emnlp-main.404) |  | 0 | The ability to extract high-quality translation dictionaries from monolingual word embedding spaces depends critically on the geometric similarity of the spaces—their degree of “isomorphism.” We address the root-cause of faulty cross-lingual mapping: that word embedding training resulted in the... | Kelly Marchisio, Neha Verma, Kevin Duh, Philipp Koehn |  |
| 1060 |  |  [Adversarial Concept Erasure in Kernel Space](https://doi.org/10.18653/v1/2022.emnlp-main.405) |  | 0 | The representation space of neural models for textual data emerges in an unsupervised manner during training. Understanding how human-interpretable concepts, such as gender, are encoded in these representations would improve the ability of users to control the content of these representations and... | Shauli Ravfogel, Francisco Vargas, Yoav Goldberg, Ryan Cotterell |  |
| 1061 |  |  [The Authenticity Gap in Human Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.406) |  | 0 | Human ratings are the gold standard in NLG evaluation. The standard protocol is to collect ratings of generated text, average across annotators, and rank NLG systems by their average scores. However, little consideration has been given as to whether this approach faithfully captures human... | Kawin Ethayarajh, Dan Jurafsky |  |
| 1062 |  |  [BERT in Plutarch's Shadows](https://doi.org/10.18653/v1/2022.emnlp-main.407) |  | 0 | The extensive surviving corpus of the ancient scholar Plutarch of Chaeronea (ca. 45-120 CE) also contains several texts which, according to current scholarly opinion, did not originate with him and are therefore attributed to an anonymous author Pseudo-Plutarch. These include, in particular, the... | Ivan P. Yamshchikov, Alexey Tikhonov, Yorgos Pantis, Charlotte Schubert, Jürgen Jost |  |
| 1063 |  |  [Leveraging Locality in Abstractive Text Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.408) |  | 0 | Neural attention models have achieved significant improvements on many natural language processing tasks. However, the quadratic memory complexity of the self-attention module with respect to the input length hinders their applications in long text summarization. Instead of designing more efficient... | Yixin Liu, Ansong Ni, Linyong Nan, Budhaditya Deb, Chenguang Zhu, Ahmed Hassan Awadallah, Dragomir Radev |  |
| 1064 |  |  [Salience Allocation as Guidance for Abstractive Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.409) |  | 0 | Abstractive summarization models typically learn to capture the salient information from scratch implicitly.Recent literature adds extractive summaries as guidance for abstractive summarization models to provide hints of salient content and achieves better performance.However, extractive summaries... | Fei Wang, Kaiqiang Song, Hongming Zhang, Lifeng Jin, Sangwoo Cho, Wenlin Yao, Xiaoyang Wang, Muhao Chen, Dong Yu |  |
| 1065 |  |  [Fine-tuned Language Models are Continual Learners](https://doi.org/10.18653/v1/2022.emnlp-main.410) |  | 0 | Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions and that models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models even though... | Thomas Scialom, Tuhin Chakrabarty, Smaranda Muresan |  |
| 1066 |  |  [Natural Logic-guided Autoregressive Multi-hop Document Retrieval for Fact Verification](https://doi.org/10.18653/v1/2022.emnlp-main.411) |  | 0 | A key component of fact verification is the evidence retrieval, often from multiple documents. Recent approaches use dense representations and condition the retrieval of each document on the previously retrieved ones. The latter step is performed over all the documents in the collection, requiring... | Rami Aly, Andreas Vlachos |  |
| 1067 |  |  [AX-MABSA: A Framework for Extremely Weakly Supervised Multi-label Aspect Based Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.412) |  | 0 | Aspect Based Sentiment Analysis is a dominant research area with potential applications in social media analytics, business, finance, and health. Prior works in this area are primarily based on supervised methods, with a few techniques using weak supervision limited to predicting a single aspect... | Sabyasachi Kamila, Walid Magdy, Sourav Dutta, MingXue Wang |  |
| 1068 |  |  [Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.413) |  | 0 | Recent research shows synthetic data as a source of supervision helps pretrained language models (PLM) transfer learning to new target tasks/domains. However, this idea is less explored for spatial language. We provide two new data resources on multiple spatial language processing tasks. The first... | Roshanak Mirzaee, Parisa Kordjamshidi |  |
| 1069 |  |  [A Survey of Active Learning for Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-main.414) |  | 0 | In this work, we provide a literature review of active learning (AL) for its applications in natural language processing (NLP). In addition to a fine-grained categorization of query strategies, we also investigate several other important aspects of applying AL to NLP problems. These include AL for... | Zhisong Zhang, Emma Strubell, Eduard H. Hovy |  |
| 1070 |  |  [Bernice: A Multilingual Pre-trained Encoder for Twitter](https://doi.org/10.18653/v1/2022.emnlp-main.415) |  | 0 | The language of Twitter differs significantly from that of other domains commonly included in large language model training. While tweets are typically multilingual and contain informal language, including emoji and hashtags, most pre-trained language models for Twitter are either monolingual,... | Alexandra DeLucia, Shijie Wu, Aaron Mueller, Carlos Alejandro Aguirre, Philip Resnik, Mark Dredze |  |
| 1071 |  |  [CEFR-Based Sentence Difficulty Annotation and Assessment](https://doi.org/10.18653/v1/2022.emnlp-main.416) |  | 0 | Controllable text simplification is a crucial assistive technique for language learning and teaching. One of the primary factors hindering its advancement is the lack of a corpus annotated with sentence difficulty levels based on language ability descriptions. To address this problem, we created... | Yuki Arase, Satoru Uchida, Tomoyuki Kajiwara |  |
| 1072 |  |  [Simple Questions Generate Named Entity Recognition Datasets](https://doi.org/10.18653/v1/2022.emnlp-main.417) |  | 0 | Recent named entity recognition (NER) models often rely on human-annotated datasets requiring the vast engagement of professional knowledge on the target domain and entities. This work introduces an ask-to-generate approach, which automatically generates NER datasets by asking simple natural... | Hyunjae Kim, Jaehyo Yoo, Seunghyun Yoon, Jinhyuk Lee, Jaewoo Kang |  |
| 1073 |  |  [TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.418) |  | 0 | Language Models (LMs) become outdated as the world changes; they often fail to perform tasks requiring recent factual information which was absent or different during training, a phenomenon called temporal misalignment. This is especially a challenging problem because the research community still... | Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Minjoon Seo |  |
| 1074 |  |  [Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.419) |  | 0 | Recently, prompt-tuning has attracted growing interests in event argument extraction (EAE). However, the existing prompt-tuning methods have not achieved satisfactory performance due to the lack of consideration of entity information. In this paper, we propose a bi-directional iterative... | Lu Dai, Bang Wang, Wei Xiang, Yijun Mo |  |
| 1075 |  |  [Learning Robust Representations for Continual Relation Extraction via Adversarial Class Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.420) |  | 0 | Continual relation extraction (CRE) aims to continually learn new relations from a class-incremental data stream. CRE model usually suffers from catastrophic forgetting problem, i.e., the performance of old relations seriously degrades when the model learns new relations. Most previous work... | Peiyi Wang, Yifan Song, Tianyu Liu, Binghuai Lin, Yunbo Cao, Sujian Li, Zhifang Sui |  |
| 1076 |  |  [ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.421) |  | 0 | With the recent advance in large pre-trained language models, researchers have achieved record performances in NLP tasks that mostly focus on language pattern matching. The community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning... | Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, William Yang Wang |  |
| 1077 |  |  [A Span-based Multimodal Variational Autoencoder for Semi-supervised Multimodal Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.422) |  | 0 | Multimodal named entity recognition (MNER) on social media is a challenging task which aims to extract named entities in free text and incorporate images to classify them into user-defined types. However, the annotation for named entities on social media demands a mount of human efforts. The... | Baohang Zhou, Ying Zhang, Kehui Song, Wenya Guo, Guoqing Zhao, Hongbin Wang, Xiaojie Yuan |  |
| 1078 |  |  [R-TeaFor: Regularized Teacher-Forcing for Abstractive Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.423) |  | 0 | Teacher-forcing is widely used in training sequence generation models to improve sampling efficiency and to stabilize training. However, teacher-forcing is vulnerable to the exposure bias problem. Previous works have attempted to address exposure bias by modifying the training data to simulate... | GuanYu Lin, PuJen Cheng |  |
| 1079 |  |  [Modeling Consistency Preference via Lexical Chains for Document-level Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.424) |  | 0 | In this paper we aim to relieve the issue of lexical translation inconsistency for document-level neural machine translation (NMT) by modeling consistency preference for lexical chains, which consist of repeated words in a source-side document and provide a representation of the lexical consistency... | Xinglin Lyu, Junhui Li, Shimin Tao, Hao Yang, Ying Qin, Min Zhang |  |
| 1080 |  |  [Just Fine-tune Twice: Selective Differential Privacy for Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.425) |  | 0 | Protecting large language models from privacy leakage is becoming increasingly crucial with their wide adoption in real-world products. Yet applying \*differential privacy\* (DP), a canonical notion with provable privacy guarantees for machine learning models, to those models remains challenging... | Weiyan Shi, Ryan Shea, Si Chen, Chiyuan Zhang, Ruoxi Jia, Zhou Yu |  |
| 1081 |  |  [Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents](https://doi.org/10.18653/v1/2022.emnlp-main.426) |  | 0 | We argue that disentangling content selection from the budget used to cover salient content improves the performance and applicability of abstractive summarizers. Our method, FactorSum, does this disentanglement by factorizing summarization into two steps through an energy function: (1) generation... | Marcio Fonseca, Yftah Ziser, Shay B. Cohen |  |
| 1082 |  |  [Open-Domain Sign Language Translation Learned from Online Video](https://doi.org/10.18653/v1/2022.emnlp-main.427) |  | 0 | Existing work on sign language translation – that is, translation from sign language videos into sentences in a written language – has focused mainly on (1) data collected in a controlled environment or (2) data in a specific domain, which limits the applicability to real-world settings. In this... | Bowen Shi, Diane Brentari, Gregory Shakhnarovich, Karen Livescu |  |
| 1083 |  |  [Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change](https://doi.org/10.18653/v1/2022.emnlp-main.428) |  | 0 | Recent research has revealed that neural language models at scale suffer from poor temporal generalization capability, i.e., language model pre-trained on static data from past years performs worse over time on emerging data. Existing methods mainly perform continual training to mitigate such a... | Zhaochen Su, Zecheng Tang, Xinyan Guan, Lijun Wu, Min Zhang, Juntao Li |  |
| 1084 |  |  [ULN: Towards Underspecified Vision-and-Language Navigation](https://doi.org/10.18653/v1/2022.emnlp-main.429) |  | 0 | Vision-and-Language Navigation (VLN) is a task to guide an embodied agent moving to a target position using language instructions. Despite the significant performance improvement, the wide use of fine-grained instructions fails to characterize more practical linguistic variations in reality. To... | Weixi Feng, TsuJui Fu, Yujie Lu, William Yang Wang |  |
| 1085 |  |  [Federated Model Decomposition with Private Vocabulary for Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.430) |  | 0 | With the necessity of privacy protection, it becomes increasingly vital to train deep neural models in a federated learning manner for natural language processing (NLP) tasks. However, recent studies show eavesdroppers (i.e., dishonest servers) can still reconstruct the private input in federated... | Zhuo Zhang, Xiangjing Hu, Lizhen Qu, Qifan Wang, Zenglin Xu |  |
| 1086 |  |  [ReCo: Reliable Causal Chain Reasoning via Structural Causal Recurrent Neural Networks](https://doi.org/10.18653/v1/2022.emnlp-main.431) |  | 0 | Causal chain reasoning (CCR) is an essential ability for many decision-making AI systems, which requires the model to build reliable causal chains by connecting causal pairs. However, CCR suffers from two main transitive problems: threshold effect and scene drift. In other words, the causal pairs... | Kai Xiong, Xiao Ding, Zhongyang Li, Li Du, Ting Liu, Bing Qin, Yi Zheng, Baoxing Huai |  |
| 1087 |  |  [Video Question Answering: Datasets, Algorithms and Challenges](https://doi.org/10.18653/v1/2022.emnlp-main.432) |  | 0 | This survey aims to sort out the recent advances in video question answering (VideoQA) and point towards future directions. We firstly categorize the datasets into 1) normal VideoQA, multi-modal VideoQA and knowledge-based VideoQA, according to the modalities invoked in the question-answer pairs,... | Yaoyao Zhong, Wei Ji, Junbin Xiao, Yicong Li, Weihong Deng, TatSeng Chua |  |
| 1088 |  |  [Retrofitting Multilingual Sentence Embeddings with Abstract Meaning Representation](https://doi.org/10.18653/v1/2022.emnlp-main.433) |  | 0 | We introduce a new method to improve existing multilingual sentence embeddings with Abstract Meaning Representation (AMR). Compared with the original textual input, AMR is a structured semantic representation that presents the core concepts and relations in a sentence explicitly and unambiguously.... | Deng Cai, Xin Li, Jackie ChunSing Ho, Lidong Bing, Wai Lam |  |
| 1089 |  |  [Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.434) |  | 0 | Existing research generally treats Chinese character as a minimum unit for representation. However, such Chinese character representation will suffer two bottlenecks: 1) Learning bottleneck, the learning cannot benefit from its rich internal features (e.g., radicals and strokes); and 2) Parameter... | Zhijun Wang, Xuebo Liu, Min Zhang |  |
| 1090 |  |  [Boundary-Driven Table-Filling for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.435) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) aims to extract the aspect terms along with the corresponding opinion terms and the expressed sentiments in the review, which is an important task in sentiment analysis. Previous research efforts generally address the ASTE task in an end-to-end fashion... | Yice Zhang, Yifan Yang, Yihui Li, Bin Liang, Shiwei Chen, Yixue Dang, Min Yang, Ruifeng Xu |  |
| 1091 |  |  [Attention and Edge-Label Guided Graph Convolutional Networks for Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.436) |  | 0 | It has been shown that named entity recognition (NER) could benefit from incorporating the long-distance structured information captured by dependency trees. However, dependency trees built by tools usually have a certain percentage of errors. Under such circumstances, how to better use relevant... | Renjie Zhou, Zhongyi Xie, Jian Wan, Jilin Zhang, Yong Liao, Qiang Liu |  |
| 1092 |  |  [Title2Event: Benchmarking Open Event Extraction with a Large-scale Chinese Title Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.437) |  | 0 | Event extraction (EE) is crucial to downstream tasks such as new aggregation and event knowledge graph construction. Most existing EE datasets manually define fixed event types and design specific schema for each of them, failing to cover diverse events emerging from the online text. Moreover, news... | Haolin Deng, Yanan Zhang, Yangfan Zhang, Wangyang Ying, Changlong Yu, Jun Gao, Wei Wang, Xiaoling Bai, Nan Yang, Jin Ma, Xiang Chen, Tianhua Zhou |  |
| 1093 |  |  [Cascading Biases: Investigating the Effect of Heuristic Annotation Strategies on Data and Models](https://doi.org/10.18653/v1/2022.emnlp-main.438) |  | 0 | Cognitive psychologists have documented that humans use cognitive heuristics, or mental shortcuts, to make quick decisions while expending less effort. While performing annotation work on crowdsourcing platforms, we hypothesize that such heuristic use among annotators cascades on to data quality... | Chaitanya Malaviya, Sudeep Bhatia, Mark Yatskar |  |
| 1094 |  |  [Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts](https://doi.org/10.18653/v1/2022.emnlp-main.439) |  | 0 | Question-answering datasets require a broad set of reasoning skills. We show how to use question decompositions to teach language models these broad reasoning skills in a robust fashion. Specifically, we use widely available QDMR representations to programmatically create hard-to-cheat synthetic... | Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal |  |
| 1095 |  |  [ADDMU: Detection of Far-Boundary Adversarial Examples with Data and Model Uncertainty Estimation](https://doi.org/10.18653/v1/2022.emnlp-main.440) |  | 0 | Adversarial Examples Detection (AED) is a crucial defense technique against adversarial attacks and has drawn increasing attention from the Natural Language Processing (NLP) community. Despite the surge of new AED methods, our studies show that existing methods heavily rely on a shortcut to achieve... | Fan Yin, Yao Li, ChoJui Hsieh, KaiWei Chang |  |
| 1096 |  |  [G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.441) |  | 0 | General pre-trained language models (PLMs), such as BERT, have achieved remarkable performance on various NLP tasks. Recently, domain-specific PLMs have been proposed to boost the task performance of specific domains (e.g., biomedical and computer science) by continuing to pre-train general PLMs... | Zhongwei Wan, Yichun Yin, Wei Zhang, Jiaxin Shi, Lifeng Shang, Guangyong Chen, Xin Jiang, Qun Liu |  |
| 1097 |  |  [Towards Unifying Reference Expression Generation and Comprehension](https://doi.org/10.18653/v1/2022.emnlp-main.442) |  | 0 | Reference Expression Generation (REG) and Comprehension (REC) are two highly correlated tasks. Modeling REG and REC simultaneously for utilizing the relation between them is a promising way to improve both. However, the problem of distinct inputs, as well as building connections between them in a... | Duo Zheng, Tao Kong, Ya Jing, Jiaan Wang, Xiaojie Wang |  |
| 1098 |  |  [Textual Manifold-based Defense Against Natural Language Adversarial Examples](https://doi.org/10.18653/v1/2022.emnlp-main.443) |  | 0 | Despite the recent success of large pretrained language models in NLP, they are susceptible to adversarial examples. Concurrently, several studies on adversarial images have observed an intriguing property: the adversarial images tend to leave the low-dimensional natural data manifold. In this... | Dang Nguyen Minh, Anh Tuan Luu |  |
| 1099 |  |  [Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters](https://doi.org/10.18653/v1/2022.emnlp-main.444) |  | 0 | Adapter-tuning is a paradigm that transfers a pretrained language model to downstream tasks by adding and tuning a small number of new parameters. Previously proposed adapter architectures are all feed-forward neural networks. In this paper, we investigate the effectiveness of using... | Hongyu Zhao, Hao Tan, Hongyuan Mei |  |
| 1100 |  |  [Reduce Catastrophic Forgetting of Dense Retrieval Training with Teleportation Negatives](https://doi.org/10.18653/v1/2022.emnlp-main.445) |  | 0 | In this paper, we investigate the instability in the standard dense retrieval training, which iterates between model training and hard negative selection using the being-trained model. We show the catastrophic forgetting phenomena behind the training instability, where models learn and forget... | Si Sun, Chenyan Xiong, Yue Yu, Arnold Overwijk, Zhiyuan Liu, Jie Bao |  |
| 1101 |  |  [ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts](https://doi.org/10.18653/v1/2022.emnlp-main.446) |  | 0 | This work introduces a new multi-task, parameter-efficient language model (LM) tuning method that learns to transfer knowledge across different tasks via a mixture of soft prompts—small prefix embedding vectors pre-trained for different tasks. Our method, called ATTEMPT (ATTEntional Mixtures of... | Akari Asai, Mohammadreza Salehi, Matthew E. Peters, Hannaneh Hajishirzi |  |
| 1102 |  |  [Exploration of the Usage of Color Terms by Color-blind Participants in Online Discussion Platforms](https://doi.org/10.18653/v1/2022.emnlp-main.447) |  | 0 | Prominent questions about the role of sensory vs. linguistic input in the way we acquire and use language have been extensively studied in the psycholinguistic literature. However, the relative effect of various factors in a person’s overall experience on their linguistic system remains unclear. We... | Ella Rabinovich, Boaz Carmeli |  |
| 1103 |  |  [DEER: Descriptive Knowledge Graph for Explaining Entity Relationships](https://doi.org/10.18653/v1/2022.emnlp-main.448) |  | 0 | We propose DEER (Descriptive Knowledge Graph for Explaining Entity Relationships) - an open and informative form of modeling entity relationships. In DEER, relationships between entities are represented by free-text relation descriptions. For instance, the relationship between entities of machine... | Jie Huang, Kerui Zhu, Kevin ChenChuan Chang, Jinjun Xiong, WenMei Hwu |  |
| 1104 |  |  [META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI](https://doi.org/10.18653/v1/2022.emnlp-main.449) |  | 0 | Task-oriented dialogue (TOD) systems have been widely used by mobile phone intelligent assistants to accomplish tasks such as calendar scheduling or hotel reservation. Current TOD systems usually focus on multi-turn text/speech interaction, then they would call back-end APIs designed for TODs to... | Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, Kai Yu |  |
| 1105 |  |  [Understanding and Improving Knowledge Distillation for Quantization Aware Training of Large Transformer Encoders](https://doi.org/10.18653/v1/2022.emnlp-main.450) |  | 0 | Knowledge distillation (KD) has been a ubiquitous method for model compression to strengthen the capability of a lightweight model with the transferred knowledge from the teacher. In particular, KD has been employed in quantization-aware training (QAT) of Transformer encoders like BERT to improve... | Minsoo Kim, Sihwa Lee, Sukjin Hong, DuSeong Chang, Jungwook Choi |  |
| 1106 |  |  [Exploring Mode Connectivity for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.451) |  | 0 | Recent years have witnessed the prevalent application of pre-trained language models (PLMs) in NLP. From the perspective of parameter space, PLMs provide generic initialization, starting from which high-performance minima could be found. Although plenty of works have studied how to effectively and... | Yujia Qin, Cheng Qian, Jing Yi, Weize Chen, Yankai Lin, Xu Han, Zhiyuan Liu, Maosong Sun, Jie Zhou |  |
| 1107 |  |  [Synergy with Translation Artifacts for Training and Inference in Multilingual Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.452) |  | 0 | Translation has played a crucial role in improving the performance on multilingual tasks: (1) to generate the target language data from the source language data for training and (2) to generate the source language data from the target language data for inference. However, prior works have not... | Jaehoon Oh, Jongwoo Ko, SeYoung Yun |  |
| 1108 |  |  [Increasing Visual Awareness in Multimodal Neural Machine Translation from an Information Theoretic Perspective](https://doi.org/10.18653/v1/2022.emnlp-main.453) |  | 0 | Multimodal machine translation (MMT) aims to improve translation quality by equipping the source sentence with its corresponding image. Despite the promising performance, MMT models still suffer the problem of input degradation: models focus more on textual information while visual information is... | Baijun Ji, Tong Zhang, Yicheng Zou, Bojie Hu, Si Shen |  |
| 1109 |  |  [Improving Event Coreference Resolution Using Document-level and Topic-level Information](https://doi.org/10.18653/v1/2022.emnlp-main.454) |  | 0 | Event coreference resolution (ECR) aims to cluster event mentions that refer to the same real-world events. Deep learning methods have achieved SOTA results on the ECR task. However, due to the encoding length limitation, previous methods either adopt classical pairwise models based on... | Sheng Xu, Peifeng Li, Qiaoming Zhu |  |
| 1110 |  |  [Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.455) |  | 0 | Prompt Tuning has been largely successful as a parameter-efficient method of conditioning large-scale pre-trained language models to perform downstream tasks. Thus far, soft prompt tuning learns a fixed set of task-specific continuous vectors, i.e., soft tokens that remain static across the task... | Rishabh Bhardwaj, Amrita Saha, Steven C. H. Hoi, Soujanya Poria |  |
| 1111 |  |  [Boosting Natural Language Generation from Instructions with Meta-Learning](https://doi.org/10.18653/v1/2022.emnlp-main.456) |  | 0 | Recent work has shown that language models (LMs) trained with multi-task instructional learning (MTIL) can solve diverse NLP tasks in zero- and few-shot settings with improved performance compared to prompt tuning. MTIL illustrates that LMs can extract and use information about the task from... | Budhaditya Deb, Ahmed Hassan Awadallah, Guoqing Zheng |  |
| 1112 |  |  [Topical Segmentation of Spoken Narratives: A Test Case on Holocaust Survivor Testimonies](https://doi.org/10.18653/v1/2022.emnlp-main.457) |  | 0 | The task of topical segmentation is well studied, but previous work has mostly addressed it in the context of structured, well-defined segments, such as segmentation into paragraphs, chapters, or segmenting text that originated from multiple sources. We tackle the task of segmenting running... | Eitan Wagner, Renana Keydar, Amit Pinchevski, Omri Abend |  |
| 1113 |  |  [Unifying the Convergences in Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.458) |  | 0 | Although all-in-one-model multilingual neural machine translation (MNMT) has achieved remarkable progress, the convergence inconsistency in the joint training is ignored, i.e., different language pairs reaching convergence in different epochs. This leads to the trained MNMT model over-fitting... | YiChong Huang, Xiaocheng Feng, Xinwei Geng, Bing Qin |  |
| 1114 |  |  [Modeling Label Correlations for Ultra-Fine Entity Typing with Neural Pairwise Conditional Random Field](https://doi.org/10.18653/v1/2022.emnlp-main.459) |  | 0 | Ultra-fine entity typing (UFET) aims to predict a wide range of type phrases that correctly describe the categories of a given entity mention in a sentence. Most recent works infer each entity type independently, ignoring the correlations between types, e.g., when an entity is inferred as a... | Chengyue Jiang, Yong Jiang, Weiqi Wu, Pengjun Xie, Kewei Tu |  |
| 1115 |  |  [Help me write a Poem - Instruction Tuning as a Vehicle for Collaborative Poetry Writing](https://doi.org/10.18653/v1/2022.emnlp-main.460) |  | 0 | Recent work in training large language models (LLMs) to follow natural language instructions has opened up exciting opportunities for natural language interface design. Building on the prior success of large language models in the realm of computer assisted creativity, in this work, we present... | Tuhin Chakrabarty, Vishakh Padmakumar, He He |  |
| 1116 |  |  [Open Relation and Event Type Discovery with Type Abstraction](https://doi.org/10.18653/v1/2022.emnlp-main.461) |  | 0 | Conventional “closed-world” information extraction (IE) approaches rely on human ontologies to define the scope for extraction. As a result, such approaches fall short when applied to new domains. This calls for systems that can automatically infer new types from given corpora, a task which we... | Sha Li, Heng Ji, Jiawei Han |  |
| 1117 |  |  [Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples](https://doi.org/10.18653/v1/2022.emnlp-main.462) |  | 0 | Knowledge-enhanced language representation learning has shown promising results across various knowledge-intensive NLP tasks. However, prior methods are limited in efficient utilization of multilingual knowledge graph (KG) data for language model (LM) pretraining. They often train LMs with KGs in... | Linlin Liu, Xin Li, Ruidan He, Lidong Bing, Shafiq R. Joty, Luo Si |  |
| 1118 |  |  [Revisiting Grammatical Error Correction Evaluation and Beyond](https://doi.org/10.18653/v1/2022.emnlp-main.463) |  | 0 | Pretraining-based (PT-based) automatic evaluation metrics (e.g., BERTScore and BARTScore) have been widely used in several sentence generation tasks (e.g., machine translation and text summarization) due to their better correlation with human judgments over traditional overlap-based methods.... | Peiyuan Gong, Xuebo Liu, Heyan Huang, Min Zhang |  |
| 1119 |  |  [R2D2: Robust Data-to-Text with Replacement Detection](https://doi.org/10.18653/v1/2022.emnlp-main.464) |  | 0 | Unfaithful text generation is a common problem for text generation systems. In the case of Data-to-Text (D2T) systems, the factuality of the generated text is particularly crucial for any real-world applications. We introduce R2D2, a training framework that addresses unfaithful Data-to-Text... | Linyong Nan, Lorenzo Jaime Yu Flores, Yilun Zhao, Yixin Liu, Luke Benson, Weijin Zou, Dragomir Radev |  |
| 1120 |  |  [IDK-MRC: Unanswerable Questions for Indonesian Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.emnlp-main.465) |  | 0 | Machine Reading Comprehension (MRC) has become one of the essential tasks in Natural Language Understanding (NLU) as it is often included in several NLU benchmarks (Liang et al., 2020; Wilie et al., 2020). However, most MRC datasets only have answerable question type, overlooking the importance of... | Rifki Afina Putri, Alice Oh |  |
| 1121 |  |  [XLM-D: Decorate Cross-lingual Pre-training Model as Non-Autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.466) |  | 0 | Pre-training language models have achieved thriving success in numerous natural language understanding and autoregressive generation tasks, but non-autoregressive generation in applications such as machine translation has not sufficiently benefited from the pre-training paradigm. In this work, we... | Yong Wang, Shilin He, Guanhua Chen, Yun Chen, Daxin Jiang |  |
| 1122 |  |  [Cross-stitching Text and Knowledge Graph Encoders for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.467) |  | 0 | Bi-encoder architectures for distantly-supervised relation extraction are designed to make use of the complementary information found in text and knowledge graphs (KG).However, current architectures suffer from two drawbacks. They either do not allow any sharing between the text encoder and the KG... | Qin Dai, Benjamin Heinzerling, Kentaro Inui |  |
| 1123 |  |  [Assist Non-native Viewers: Multimodal Cross-Lingual Summarization for How2 Videos](https://doi.org/10.18653/v1/2022.emnlp-main.468) |  | 0 | Multimodal summarization for videos aims to generate summaries from multi-source information (videos, audio transcripts), which has achieved promising progress. However, existing works are restricted to monolingual video scenarios, ignoring the demands of non-native video viewers to understand the... | Nayu Liu, Kaiwen Wei, Xian Sun, Hongfeng Yu, Fanglong Yao, Li Jin, Zhi Guo, Guangluan Xu |  |
| 1124 |  |  [PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance](https://doi.org/10.18653/v1/2022.emnlp-main.469) |  | 0 | To facilitate conversational question answering (CQA) over hybrid contexts in finance, we present a new dataset, named PACIFIC. Compared with existing CQA datasets, PACIFIC exhibits three key features: (i) proactivity, (ii) numerical reasoning, and (iii) hybrid context of tables and text. A new... | Yang Deng, Wenqiang Lei, Wenxuan Zhang, Wai Lam, TatSeng Chua |  |
| 1125 |  |  [Generative Data Augmentation with Contrastive Learning for Zero-Shot Stance Detection](https://doi.org/10.18653/v1/2022.emnlp-main.470) |  | 0 | Stance detection aims to identify whether the author of an opinionated text is in favor of, against, or neutral towards a given target. Remarkable success has been achieved when sufficient labeled training data is available. However, it is labor-intensive to annotate sufficient data and train the... | Yang Li, Jiawei Yuan |  |
| 1126 |  |  [Better Few-Shot Relation Extraction with Label Prompt Dropout](https://doi.org/10.18653/v1/2022.emnlp-main.471) |  | 0 | Few-shot relation extraction aims to learn to identify the relation between two entities based on very limited training examples. Recent efforts found that textual labels (i.e., relation names and relation descriptions) could be extremely useful for learning class representations, which will... | Peiyuan Zhang, Wei Lu |  |
| 1127 |  |  [Break it Down into BTS: Basic, Tiniest Subword Units for Korean](https://doi.org/10.18653/v1/2022.emnlp-main.472) |  | 0 | We introduce Basic, Tiniest Subword (BTS) units for the Korean language, which are inspired by the invention principle of Hangeul, the Korean writing system. Instead of relying on 51 Korean consonant and vowel letters, we form the letters from BTS units by adding strokes or combining them. To... | Nayeon Kim, JunHyung Park, JoonYoung Choi, Eojin Jeon, Youjin Kang, SangKeun Lee |  |
| 1128 |  |  [The Devil in Linear Transformer](https://doi.org/10.18653/v1/2022.emnlp-main.473) |  | 0 | Linear transformers aim to reduce the quadratic space-time complexity of vanilla transformers. However, they usually suffer from degraded performances on various tasks and corpus. In this paper, we examine existing kernel-based linear transformers and identify two key issues that lead to such... | Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, Yiran Zhong |  |
| 1129 |  |  [Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective](https://doi.org/10.18653/v1/2022.emnlp-main.474) |  | 0 | We propose a new paradigm for zero-shot learners that is format agnostic, i.e., it is compatible with any format and applicable to a list of language tasks, such as text classification, commonsense reasoning, coreference resolution, and sentiment analysis. Zero-shot learning aims to train a model... | Ping Yang, Junjie Wang, Ruyi Gan, Xinyu Zhu, Lin Zhang, Ziwei Wu, Xinyu Gao, Jiaxing Zhang, Tetsuya Sakai |  |
| 1130 |  |  [Hypoformer: Hybrid Decomposition Transformer for Edge-friendly Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.475) |  | 0 | Transformer has been demonstrated effective in Neural Machine Translation (NMT). However, it is memory-consuming and time-consuming in edge devices, resulting in some difficulties for real-time feedback. To compress and accelerate Transformer, we propose a Hybrid Tensor-Train (HTT) decomposition,... | Sunzhu Li, Peng Zhang, Guobing Gan, Xiuqing Lv, Benyou Wang, Victor Junqiu Wei, Xin Jiang |  |
| 1131 |  |  [FigMemes: A Dataset for Figurative Language Identification in Politically-Opinionated Memes](https://doi.org/10.18653/v1/2022.emnlp-main.476) |  | 0 | Real-world politically-opinionated memes often rely on figurative language to cloak propaganda and radical ideas to help them spread. It is not only a scientific challenge to develop machine learning models to recognize them in memes, but also sociologically beneficial to understand hidden meanings... | Chen Liu, Gregor Geigle, Robin Krebs, Iryna Gurevych |  |
| 1132 |  |  [UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.477) |  | 0 | Relational triple extraction is challenging for its difficulty in capturing rich correlations between entities and relations. Existing works suffer from 1) heterogeneous representations of entities and relations, and 2) heterogeneous modeling of entity-entity interactions and entity-relation... | Wei Tang, Benfeng Xu, Yuyue Zhao, Zhendong Mao, Yifeng Liu, Yong Liao, Haiyong Xie |  |
| 1133 |  |  [X-FACTOR: A Cross-metric Evaluation of Factual Correctness in Abstractive Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.478) |  | 0 | Abstractive summarization models often produce factually inconsistent summaries that are not supported by the original article. Recently, a number of fact-consistent evaluation techniques have been proposed to address this issue; however, a detailed analysis of how these metrics agree with one... | Subhajit Chaudhury, Sarathkrishna Swaminathan, R. Chulaka Gunasekara, Maxwell Crouse, Srinivas Ravishankar, Daiki Kimura, Keerthiram Murugesan, Ramón Fernandez Astudillo, Tahira Naseem, Pavan Kapanipathi, Alexander Gray |  |
| 1134 |  |  [ParaTag: A Dataset of Paraphrase Tagging for Fine-Grained Labels, NLG Evaluation, and Data Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.479) |  | 0 | Paraphrase identification has been formulated as a binary classification task to decide whether two sentences hold a paraphrase relationship. Existing paraphrase datasets only annotate a binary label for each sentence pair. However, after a systematical analysis of existing paraphrase datasets, we... | Shuohang Wang, Ruochen Xu, Yang Liu, Chenguang Zhu, Michael Zeng |  |
| 1135 |  |  [Factual Accuracy is not Enough: Planning Consistent Description Order for Radiology Report Generation](https://doi.org/10.18653/v1/2022.emnlp-main.480) |  | 0 | Radiology report generation systems have the potential to reduce the workload of radiologists by automatically describing the findings in medical images.To broaden the application of the report generation system, the system should generate reports that are not only factually accurate but also... | Toru Nishino, Yasuhide Miura, Tomoki Taniguchi, Tomoko Ohkuma, Yuki Suzuki, Shoji Kido, Noriyuki Tomiyama |  |
| 1136 |  |  [FLUTE: Figurative Language Understanding through Textual Explanations](https://doi.org/10.18653/v1/2022.emnlp-main.481) |  | 0 | Figurative language understanding has been recently framed as a recognizing textual entailment (RTE) task (a.k.a. natural language inference (NLI)). However, similar to classical RTE/NLI datasets they suffer from spurious correlations and annotation artifacts. To tackle this problem, work on NLI... | Tuhin Chakrabarty, Arkadiy Saakyan, Debanjan Ghosh, Smaranda Muresan |  |
| 1137 |  |  [Precisely the Point: Adversarial Augmentations for Faithful and Informative Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.482) |  | 0 | Though model robustness has been extensively studied in language understanding, the robustness of Seq2Seq generation remains understudied.In this paper, we conduct the first quantitative analysis on the robustness of pre-trained Seq2Seq models. We find that even current SOTA pre-trained Seq2Seq... | Wenhao Wu, Wei Li, Jiachen Liu, Xinyan Xiao, Sujian Li, Yajuan Lyu |  |
| 1138 |  |  [RLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees](https://doi.org/10.18653/v1/2022.emnlp-main.483) |  | 0 | Interpreting the reasoning process from questions to answers poses a challenge in approaching explainable QA. A recently proposed structured reasoning format, entailment tree, manages to offer explicit logical deductions with entailment steps in a tree structure. To generate entailment trees, prior... | Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang |  |
| 1139 |  |  [Let the CAT out of the bag: Contrastive Attributed explanations for Text](https://doi.org/10.18653/v1/2022.emnlp-main.484) |  | 0 | Contrastive explanations for understanding the behavior of black box models has gained a lot of attention recently as they provide potential for recourse. In this paper, we propose a method Contrastive Attributed explanations for Text (CAT) which provides contrastive explanations for natural... | Saneem A. Chemmengath, Amar Prakash Azad, Ronny Luss, Amit Dhurandhar |  |
| 1140 |  |  [monoQA: Multi-Task Learning of Reranking and Answer Extraction for Open-Retrieval Conversational Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.485) |  | 0 | To address the Conversational Question Answering (ORConvQA) task, previous work has considered an effective three-stage architecture, consisting of a retriever, a reranker, and a reader to extract the answers. In order to effectively answer the users’ questions, a number of existing approaches have... | Sarawoot Kongyoung, Craig Macdonald, Iadh Ounis |  |
| 1141 |  |  [Composing Ci with Reinforced Non-autoregressive Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.486) |  | 0 | Composing Ci (also widely known as Song Ci), a special type of classical Chinese poetry, requires to follow particular format once their tune patterns are given. To automatically generate a well-formed Ci, text generation systems should strictly take into account pre-defined rigid formats (e.g.,... | Yan Song |  |
| 1142 |  |  [MetaTKG: Learning Evolutionary Meta-Knowledge for Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.487) |  | 0 | Reasoning over Temporal Knowledge Graphs (TKGs) aims to predict future facts based on given history. One of the key challenges for prediction is to learn the evolution of facts. Most existing works focus on exploring evolutionary information in history to obtain effective temporal embeddings for... | Yuwei Xia, Mengqi Zhang, Qiang Liu, Shu Wu, Xiaoyu Zhang |  |
| 1143 |  |  [mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections](https://doi.org/10.18653/v1/2022.emnlp-main.488) |  | 0 | Large-scale pre-trained foundation models have been an emerging paradigm for building artificial intelligence (AI) systems, which can be quickly adapted to a wide range of downstream tasks. This paper presents mPLUG, a new vision-language foundation model for both cross-modal understanding and... | Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, He Chen, Guohai Xu, Zheng Cao, Ji Zhang, Songfang Huang, Fei Huang, Jingren Zhou, Luo Si |  |
| 1144 |  |  [Q-TOD: A Query-driven Task-oriented Dialogue System](https://doi.org/10.18653/v1/2022.emnlp-main.489) |  | 0 | Existing pipelined task-oriented dialogue systems usually have difficulties adapting to unseen domains, whereas end-to-end systems are plagued by large-scale knowledge bases in practice. In this paper, we introduce a novel query-driven task-oriented dialogue system, namely Q-TOD. The essential... | Xin Tian, Yingzhan Lin, Mengfei Song, Siqi Bao, Fan Wang, Huang He, Shuqi Sun, Hua Wu |  |
| 1145 |  |  [Dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.490) |  | 0 | In this paper, we introduce the task of learning unsupervised dialogue embeddings.Trivial approaches such as combining pre-trained word or sentence embeddings and encoding through pre-trained language models (PLMs) have been shown to be feasible for this task.However, these approaches typically... | Che Liu, Rui Wang, Junfeng Jiang, Yongbin Li, Fei Huang |  |
| 1146 |  |  [WR-One2Set: Towards Well-Calibrated Keyphrase Generation](https://doi.org/10.18653/v1/2022.emnlp-main.491) |  | 0 | Keyphrase generation aims to automatically generate short phrases summarizing an input document. The recently emerged ONE2SET paradigm (Ye et al., 2021) generates keyphrases as a set and has achieved competitive performance. Nevertheless, we observe serious calibration errors outputted by ONE2SET,... | Binbin Xie, Xiangpeng Wei, Baosong Yang, Huan Lin, Jun Xie, Xiaoli Wang, Min Zhang, Jinsong Su |  |
| 1147 |  |  [Eeny, meeny, miny, moe. How to choose data for morphological inflection](https://doi.org/10.18653/v1/2022.emnlp-main.492) |  | 0 | Data scarcity is a widespread problem for numerous natural language processing (NLP) tasks within low-resource languages. Within morphology, the labour-intensive task of tagging/glossing data is a serious bottleneck for both NLP and fieldwork. Active learning (AL) aims to reduce the cost of data... | Saliha Muradoglu, Mans Hulden |  |
| 1148 |  |  [An Adaptive Logical Rule Embedding Model for Inductive Reasoning over Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.493) |  | 0 | Temporal knowledge graphs (TKGs) extrapolation reasoning predicts future events based on historical information, which has great research significance and broad application value. Existing methods can be divided into embedding-based methods and logical rule-based methods. Embedding-based methods... | Xin Mei, Libin Yang, Xiaoyan Cai, Zuowei Jiang |  |
| 1149 |  |  [UniNL: Aligning Representation Learning with Scoring Function for OOD Detection via Unified Neighborhood Learning](https://doi.org/10.18653/v1/2022.emnlp-main.494) |  | 0 | Detecting out-of-domain (OOD) intents from user queries is essential for avoiding wrong operations in task-oriented dialogue systems. The key challenge is how to distinguish in-domain (IND) and OOD intents. Previous methods ignore the alignment between representation learning and scoring function,... | Yutao Mou, Pei Wang, Keqing He, Yanan Wu, Jingang Wang, Wei Wu, Weiran Xu |  |
| 1150 |  |  [Open-domain Video Commentary Generation](https://doi.org/10.18653/v1/2022.emnlp-main.495) |  | 0 | Live commentary plays an important role in sports broadcasts and video games, making spectators more excited and immersed. In this context, though approaches for automatically generating such commentary have been proposed in the past, they have been generally concerned with specific fields, where... | Edison MarreseTaylor, Yumi Hamazono, Tatsuya Ishigaki, Goran Topic, Yusuke Miyao, Ichiro Kobayashi, Hiroya Takamura |  |
| 1151 |  |  [One size does not fit all: Investigating strategies for differentially-private learning across NLP tasks](https://doi.org/10.18653/v1/2022.emnlp-main.496) |  | 0 | Preserving privacy in contemporary NLP models allows us to work with sensitive data, but unfortunately comes at a price. We know that stricter privacy guarantees in differentially-private stochastic gradient descent (DP-SGD) generally degrade model performance. However, previous research on the... | Manuel Senge, Timour Igamberdiev, Ivan Habernal |  |
| 1152 |  |  [Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario](https://doi.org/10.18653/v1/2022.emnlp-main.497) |  | 0 | People can acquire knowledge in an unsupervised manner by reading, and compose the knowledge to make novel combinations. In this paper, we investigate whether pretrained language models can perform compositional generalization in a realistic setting: recipe generation. We design the counterfactual... | Xiao Liu, Yansong Feng, Jizhi Tang, Chengang Hu, Dongyan Zhao |  |
| 1153 |  |  [Tutoring Helps Students Learn Better: Improving Knowledge Distillation for BERT with Tutor Network](https://doi.org/10.18653/v1/2022.emnlp-main.498) |  | 0 | Pre-trained language models have achieved remarkable successes in natural language processing tasks, coming at the cost of increasing model size. To address this issue, knowledge distillation (KD) has been widely applied to compress language models. However, typical KD approaches for language... | Junho Kim, JunHyung Park, Mingyu Lee, WingLam Mok, JoonYoung Choi, SangKeun Lee |  |
| 1154 |  |  [Does Corpus Quality Really Matter for Low-Resource Languages?](https://doi.org/10.18653/v1/2022.emnlp-main.499) |  | 0 | The vast majority of non-English corpora are derived from automatically filtered versions of CommonCrawl. While prior work has identified major issues on the quality of these datasets (Kreutzer et al., 2021), it is not clear how this impacts downstream performance. Taking representation learning in... | Mikel Artetxe, Itziar Aldabe, Rodrigo Agerri, Olatz PerezdeViñaspre, Aitor Soroa |  |
| 1155 |  |  [Unifying Data Perspectivism and Personalization: An Application to Social Norms](https://doi.org/10.18653/v1/2022.emnlp-main.500) |  | 0 | Instead of using a single ground truth for language processing tasks, several recent studies have examined how to represent and predict the labels of the set of annotators. However, often little or no information about annotators is known, or the set of annotators is small. In this work, we examine... | Joan Plepi, Béla Neuendorf, Lucie Flek, Charles Welch |  |
| 1156 |  |  [Does Self-Rationalization Improve Robustness to Spurious Correlations?](https://doi.org/10.18653/v1/2022.emnlp-main.501) |  | 0 | Rationalization is fundamental to human reasoning and learning. NLP models trained to produce rationales along with predictions, called self-rationalization models, have been investigated for their interpretability and utility to end-users. However, the extent to which training with human-written... | Alexis Ross, Matthew E. Peters, Ana Marasovic |  |
| 1157 |  |  [Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking](https://doi.org/10.18653/v1/2022.emnlp-main.502) |  | 0 | Self-supervised pre-training has achieved remarkable success in extensive natural language processing tasks. Masked language modeling (MLM) has been widely used for pre-training effective bidirectional representations but comes at a substantial training cost. In this paper, we propose a novel... | Mingyu Lee, JunHyung Park, Junho Kim, KangMin Kim, SangKeun Lee |  |
| 1158 |  |  [Subword Evenness (SuE) as a Predictor of Cross-lingual Transfer to Low-resource Languages](https://doi.org/10.18653/v1/2022.emnlp-main.503) |  | 0 | Pre-trained multilingual models, such as mBERT, XLM-R and mT5, are used to improve the performance on various tasks in low-resource languages via cross-lingual transfer. In this framework, English is usually seen as the most natural choice for a transfer language (for fine-tuning or continued... | Olga Pelloni, Anastassia Shaitarova, Tanja Samardzic |  |
| 1159 |  |  [A Unified Neural Network Model for Readability Assessment with Feature Projection and Length-Balanced Loss](https://doi.org/10.18653/v1/2022.emnlp-main.504) |  | 0 | Readability assessment is a basic research task in the field of education. Traditional methods mainly employ machine learning classifiers with hundreds of linguistic features. Although the deep learning model has become the prominent approach for almost all NLP tasks, it is less explored for... | Wenbiao Li, Ziyang Wang, Yunfang Wu |  |
| 1160 |  |  [Speaker Overlap-aware Neural Diarization for Multi-party Meeting Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.505) |  | 0 | Recently, hybrid systems of clustering and neural diarization models have been successfully applied in multi-party meeting analysis. However, current models always treat overlapped speaker diarization as a multi-label classification problem, where speaker dependency and overlaps are not well... | Zhihao Du, Shiliang Zhang, Siqi Zheng, ZhiJie Yan |  |
| 1161 |  |  [GREENER: Graph Neural Networks for News Media Profiling](https://doi.org/10.18653/v1/2022.emnlp-main.506) |  | 0 | We study the problem of profiling news media on the Web with respect to their factuality of reporting and bias. This is an important but under-studied problem related to disinformation and “fake news” detection, but it addresses the issue at a coarser granularity compared to looking at an... | Panayot Panayotov, Utsav Shukla, Husrev Taha Sencar, Mohamed Nabeel, Preslav Nakov |  |
| 1162 |  |  [Graph Hawkes Transformer for Extrapolated Reasoning on Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.507) |  | 0 | Temporal Knowledge Graph (TKG) reasoning has attracted increasing attention due to its enormous potential value, and the critical issue is how to model the complex temporal structure information effectively. Recent studies use the method of encoding graph snapshots into hidden vector space and then... | Haohai Sun, Shangyi Geng, Jialun Zhong, Han Hu, Kun He |  |
| 1163 |  |  [UniRPG: Unified Discrete Reasoning over Table and Text as Program Generation](https://doi.org/10.18653/v1/2022.emnlp-main.508) |  | 0 | Question answering requiring discrete reasoning, e.g., arithmetic computing, comparison, and counting, over knowledge is a challenging task.In this paper, we propose UniRPG, a semantic-parsing-based approach advanced in interpretability and scalability, to perform Unified discrete Reasoning over... | Yongwei Zhou, Junwei Bao, Chaoqun Duan, Youzheng Wu, Xiaodong He, Tiejun Zhao |  |
| 1164 |  |  [Don't Prompt, Search! Mining-based Zero-Shot Learning with Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.509) |  | 0 | Masked language models like BERT can perform text classification in a zero-shot fashion by reformulating downstream tasks as text infilling. However, this approach is highly sensitive to the template used to prompt the model, yet practitioners are blind when designing them in strict zero-shot... | Mozes van de Kar, Mengzhou Xia, Danqi Chen, Mikel Artetxe |  |
| 1165 |  |  [SEMGraph: Incorporating Sentiment Knowledge and Eye Movement into Graph Model for Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.510) |  | 0 | This paper investigates the sentiment analysis task from a novel perspective by incorporating sentiment knowledge and eye movement into a graph architecture, aiming to draw the eye movement-based sentiment relationships for learning the sentiment expression of the context. To be specific, we first... | Bingbing Wang, Bin Liang, Jiachen Du, Min Yang, Ruifeng Xu |  |
| 1166 |  |  [Cross-lingual neural fuzzy matching for exploiting target-language monolingual corpora in computer-aided translation](https://doi.org/10.18653/v1/2022.emnlp-main.511) |  | 0 | Computer-aided translation (CAT) tools based on translation memories (MT) play a prominent role in the translation workflow of professional translators. However, the reduced availability of in-domain TMs, as compared to in-domain monolingual corpora, limits its adoption for a number of translation... | Miquel EsplàGomis, Víctor M. SánchezCartagena, Juan Antonio PérezOrtiz, Felipe SánchezMartínez |  |
| 1167 |  |  [Multi-Label Intent Detection via Contrastive Task Specialization of Sentence Encoders](https://doi.org/10.18653/v1/2022.emnlp-main.512) |  | 0 | Deploying task-oriented dialog ToD systems for new domains and tasks requires natural language understanding models that are 1) resource-efficient and work under low-data regimes; 2) adaptable, efficient, and quick-to-train; 3) expressive and can handle complex ToD scenarios with multiple user... | Ivan Vulic, Iñigo Casanueva, Georgios Spithourakis, Avishek Mondal, TsungHsien Wen, Pawel Budzianowski |  |
| 1168 |  |  [Discovering Language-neutral Sub-networks in Multilingual Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.513) |  | 0 | Multilingual pre-trained language models transfer remarkably well on cross-lingual downstream tasks. However, the extent to which they learn language-neutral representations (i.e., shared representations that encode similar phenomena across languages), and the effect of such representations on... | Negar Foroutan, Mohammadreza Banaei, Rémi Lebret, Antoine Bosselut, Karl Aberer |  |
| 1169 |  |  [Parameter-Efficient Tuning Makes a Good Classification Head](https://doi.org/10.18653/v1/2022.emnlp-main.514) |  | 0 | In recent years, pretrained models revolutionized the paradigm of natural language understanding (NLU), where we append a randomly initialized classification head after the pretrained backbone, e.g. BERT, and finetune the whole model. As the pretrained backbone makes a major contribution to the... | Zhuoyi Yang, Ming Ding, Yanhui Guo, Qingsong Lv, Jie Tang |  |
| 1170 |  |  [STGN: an Implicit Regularization Method for Learning with Noisy Labels in Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-main.515) |  | 0 | Noisy labels are ubiquitous in natural language processing (NLP) tasks. Existing work, namely learning with noisy labels in NLP, is often limited to dedicated tasks or specific training procedures, making it hard to be widely used. To address this issue, SGD noise has been explored to provide a... | Tingting Wu, Xiao Ding, Minji Tang, Hao Zhang, Bing Qin, Ting Liu |  |
| 1171 |  |  [Cross-Modal Similarity-Based Curriculum Learning for Image Captioning](https://doi.org/10.18653/v1/2022.emnlp-main.516) |  | 0 | Image captioning models require the high-level generalization ability to describe the contents of various images in words. Most existing approaches treat the image–caption pairs equally in their training without considering the differences in their learning difficulties. Several image captioning... | Hongkuan Zhang, Saku Sugawara, Akiko Aizawa, Lei Zhou, Ryohei Sasano, Koichi Takeda |  |
| 1172 |  |  [Debiasing Masks: A New Framework for Shortcut Mitigation in NLU](https://doi.org/10.18653/v1/2022.emnlp-main.517) |  | 0 | Debiasing language models from unwanted behaviors in Natural Language Understanding (NLU) tasks is a topic with rapidly increasing interest in the NLP community. Spurious statistical correlations in the data allow models to perform shortcuts and avoid uncovering more advanced and desirable... | Johannes Mario Meissner, Saku Sugawara, Akiko Aizawa |  |
| 1173 |  |  [Extending Phrase Grounding with Pronouns in Visual Dialogues](https://doi.org/10.18653/v1/2022.emnlp-main.518) |  | 0 | Conventional phrase grounding aims to localize noun phrases mentioned in a given caption to their corresponding image regions, which has achieved great success recently. Apparently, sole noun phrase grounding is not enough for cross-modal visual language understanding. Here we extend the task by... | Panzhong Lu, Xin Zhang, Meishan Zhang, Min Zhang |  |
| 1174 |  |  [EUR-Lex-Sum: A Multi- and Cross-lingual Dataset for Long-form Summarization in the Legal Domain](https://doi.org/10.18653/v1/2022.emnlp-main.519) |  | 0 | Existing summarization datasets come with two main drawbacks: (1) They tend to focus on overly exposed domains, such as news articles or wiki-like texts, and (2) are primarily monolingual, with few multilingual datasets.In this work, we propose a novel dataset, called EUR-Lex-Sum, based on manually... | Dennis Aumiller, Ashish Chouhan, Michael Gertz |  |
| 1175 |  |  [Differentiable Data Augmentation for Contrastive Sentence Representation Learning](https://doi.org/10.18653/v1/2022.emnlp-main.520) |  | 0 | Fine-tuning a pre-trained language model via the contrastive learning framework with a large amount of unlabeled sentences or labeled sentence pairs is a common way to obtain high-quality sentence representations. Although the contrastive learning framework has shown its superiority on sentence... | Tianduo Wang, Wei Lu |  |
| 1176 |  |  [Text Style Transferring via Adversarial Masking and Styled Filling](https://doi.org/10.18653/v1/2022.emnlp-main.521) |  | 0 | Text style transfer is an important task in natural language processing with broad applications. Existing models following the masking and filling scheme suffer two challenges: the word masking procedure may mistakenly remove unexpected words and the selected words in the word filling procedure may... | Jiarui Wang, Richong Zhang, Junfan Chen, Jaein Kim, Yongyi Mao |  |
| 1177 |  |  [Character-level White-Box Adversarial Attacks against Transformers via Attachable Subwords Substitution](https://doi.org/10.18653/v1/2022.emnlp-main.522) |  | 0 | We propose the first character-level white-box adversarial attack method against transformer models. The intuition of our method comes from the observation that words are split into subtokens before being fed into the transformer models and the substitution between two close subtokens has a similar... | Aiwei Liu, Honghai Yu, Xuming Hu, Shu'ang Li, Li Lin, Fukun Ma, Yawen Yang, Lijie Wen |  |
| 1178 |  |  [Query-based Instance Discrimination Network for Relational Triple Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.523) |  | 0 | Joint entity and relation extraction has been a core task in the field of information extraction. Recent approaches usually consider the extraction of relational triples from a stereoscopic perspective, either learning a relation-specific tagger or separate classifiers for each relation type.... | Zeqi Tan, Yongliang Shen, Xuming Hu, Wenqi Zhang, Xiaoxia Cheng, Weiming Lu, Yueting Zhuang |  |
| 1179 |  |  [Learning Inter-Entity-Interaction for Few-Shot Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.emnlp-main.524) |  | 0 | Few-shot knowledge graph completion (FKGC) aims to infer unknown fact triples of a relation using its few-shot reference entity pairs. Recent FKGC studies focus on learning semantic representations of entity pairs by separately encoding the neighborhoods of head and tail entities. Such practice,... | Yuling Li, Kui Yu, Xiaoling Huang, Yuhong Zhang |  |
| 1180 |  |  [Empowering the Fact-checkers! Automatic Identification of Claim Spans on Twitter](https://doi.org/10.18653/v1/2022.emnlp-main.525) |  | 0 | The widespread diffusion of medical and political claims in the wake of COVID-19 has led to a voluminous rise in misinformation and fake news. The current vogue is to employ manual fact-checkers to efficiently classify and verify such data to combat this avalanche of claim-ridden misinformation.... | Megha Sundriyal, Atharva Kulkarni, Vaibhav Pulastya, Md. Shad Akhtar, Tanmoy Chakraborty |  |
| 1181 |  |  [ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.526) |  | 0 | We present ClidSum, a benchmark dataset towards building cross-lingual summarization systems on dialogue documents. It consists of 67k+ dialogue documents and 112k+ annotated summaries in different target languages. Based on the proposed ClidSum, we introduce two benchmark settings for supervised... | Jiaan Wang, Fandong Meng, Ziyao Lu, Duo Zheng, Zhixu Li, Jianfeng Qu, Jie Zhou |  |
| 1182 |  |  [Spectral Probing](https://doi.org/10.18653/v1/2022.emnlp-main.527) |  | 0 | Linguistic information is encoded at varying timescales (subwords, phrases, etc.) and communicative levels, such as syntax and semantics. Contextualized embeddings have analogously been found to capture these phenomena at distinctive layers and frequencies. Leveraging these findings, we develop a... | Max MüllerEberstein, Rob van der Goot, Barbara Plank |  |
| 1183 |  |  [QASem Parsing: Text-to-text Modeling of QA-based Semantics](https://doi.org/10.18653/v1/2022.emnlp-main.528) |  | 0 | Various works suggest the appeals of incorporating explicit semantic representations when addressing challenging realistic NLP scenarios. Common approaches offer either comprehensive linguistically-based formalisms, like AMR, or alternatively Open-IE, which provides a shallow and partial... | Ayal Klein, Eran Hirsch, Ron Eliav, Valentina Pyatkin, Avi Caciularu, Ido Dagan |  |
| 1184 |  |  [Keyphrase Generation via Soft and Hard Semantic Corrections](https://doi.org/10.18653/v1/2022.emnlp-main.529) |  | 0 | Keyphrase generation aims to generate a set of condensed phrases given a source document. Although maximum likelihood estimation (MLE) based keyphrase generation methods have shown impressive performance, they suffer from the bias on the source-prediction sequence pair and the bias on the... | Guangzhen Zhao, Guoshun Yin, Peng Yang, Yu Yao |  |
| 1185 |  |  [Modal-specific Pseudo Query Generation for Video Corpus Moment Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.530) |  | 0 | Video corpus moment retrieval (VCMR) is the task to retrieve the most relevant video moment from a large video corpus using a natural language query.For narrative videos, e.g., drama or movies, the holistic understanding of temporal dynamics and multimodal reasoning are crucial.Previous works have... | Minjoon Jung, Seongho Choi, Joochan Kim, JinHwa Kim, ByoungTak Zhang |  |
| 1186 |  |  [DuQM: A Chinese Dataset of Linguistically Perturbed Natural Questions for Evaluating the Robustness of Question Matching Models](https://doi.org/10.18653/v1/2022.emnlp-main.531) |  | 0 | In this paper, we focus on the robustness evaluation of Chinese Question Matching (QM) models. Most of the previous work on analyzing robustness issues focus on just one or a few types of artificial adversarial examples. Instead, we argue that a comprehensive evaluation should be conducted on... | Hongyu Zhu, Yan Chen, Jing Yan, Jing Liu, Yu Hong, Ying Chen, Hua Wu, Haifeng Wang |  |
| 1187 |  |  [DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages](https://doi.org/10.18653/v1/2022.emnlp-main.532) |  | 0 | We introduce DivEMT, the first publicly available post-editing study of Neural Machine Translation (NMT) over a typologically diverse set of target languages. Using a strictly controlled setup, 18 professional translators were instructed to translate or post-edit the same set of English documents... | Gabriele Sarti, Arianna Bisazza, Ana Guerberof Arenas, Antonio Toral |  |
| 1188 |  |  [Bridging Fairness and Environmental Sustainability in Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-main.533) |  | 0 | Fairness and environmental impact are important research directions for the sustainable development of artificial intelligence. However, while each topic is an active research area in natural language processing (NLP), there is a surprising lack of research on the interplay between the two fields.... | Marius Hessenthaler, Emma Strubell, Dirk Hovy, Anne Lauscher |  |
| 1189 |  |  [UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.534) |  | 0 | Multimodal sentiment analysis (MSA) and emotion recognition in conversation (ERC) are key research topics for computers to understand human behaviors. From a psychological perspective, emotions are the expression of affect or feelings during a short period, while sentiments are formed and held for... | Guimin Hu, TingEn Lin, Yi Zhao, Guangming Lu, Yuchuan Wu, Yongbin Li |  |
| 1190 |  |  [Is the Brain Mechanism for Hierarchical Structure Building Universal Across Languages? An fMRI Study of Chinese and English](https://doi.org/10.18653/v1/2022.emnlp-main.535) |  | 0 | Evidence from psycholinguistic studies suggests that the human brain builds a hierarchical syntactic structure during language comprehension. However, it is still unknown whether the neural basis of such structures is universal across languages. In this paper, we first analyze the differences in... | Xiaohan Zhang, Shaonan Wang, Nan Lin, Chengqing Zong |  |
| 1191 |  |  [HashFormers: Towards Vocabulary-independent Pre-trained Transformers](https://doi.org/10.18653/v1/2022.emnlp-main.536) |  | 0 | Transformer-based pre-trained language models are vocabulary-dependent, mapping by default each token to its corresponding embedding. This one-to-one mapping results into embedding matrices that occupy a lot of memory (i.e. millions of parameters) and grow linearly with the size of the vocabulary.... | Huiyin Xue, Nikolaos Aletras |  |
| 1192 |  |  [MatchPrompt: Prompt-based Open Relation Extraction with Semantic Consistency Guided Clustering](https://doi.org/10.18653/v1/2022.emnlp-main.537) |  | 0 | Relation clustering is a general approach for open relation extraction (OpenRE). Current methods have two major problems. One is that their good performance relies on large amounts of labeled and pre-defined relational instances for pre-training, which are costly to acquire in reality. The other is... | Jiaxin Wang, Lingling Zhang, Jun Liu, Xi Liang, Yujie Zhong, Yaqiang Wu |  |
| 1193 |  |  [Improving Aspect Sentiment Quad Prediction via Template-Order Data Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.538) |  | 0 | Recently, aspect sentiment quad prediction (ASQP) has become a popular task in the field of aspect-level sentiment analysis. Previous work utilizes a predefined template to paraphrase the original sentence into a structure target sequence, which can be easily decoded as quadruplets of the form... | Mengting Hu, Yike Wu, Hang Gao, Yinhao Bai, Shiwan Zhao |  |
| 1194 |  |  [SocioProbe: What, When, and Where Language Models Learn about Sociodemographics](https://doi.org/10.18653/v1/2022.emnlp-main.539) |  | 0 | Pre-trained language models (PLMs) have outperformed other NLP models on a wide range of tasks. Opting for a more thorough understanding of their capabilities and inner workings, researchers have established the extend to which they capture lower-level knowledge like grammaticality, and mid-level... | Anne Lauscher, Federico Bianchi, Samuel R. Bowman, Dirk Hovy |  |
| 1195 |  |  [When does Parameter-Efficient Transfer Learning Work for Machine Translation?](https://doi.org/10.18653/v1/2022.emnlp-main.540) |  | 0 | Parameter-efficient fine-tuning methods (PEFTs) offer the promise of adapting large pre-trained models while only tuning a small number of parameters. They have been shown to be competitive with full model fine-tuning for many downstream tasks. However, prior work indicates that PEFTs may not work... | Ahmet Üstün, Asa Cooper Stickland |  |
| 1196 |  |  [Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer](https://doi.org/10.18653/v1/2022.emnlp-main.541) |  | 0 | Massively multilingual models are promising for transfer learning across tasks and languages. However, existing methods are unable to fully leverage training data when it is available in different task-language combinations. To exploit such heterogeneous supervision, we propose Hyper-X, a single... | Ahmet Üstün, Arianna Bisazza, Gosse Bouma, Gertjan van Noord, Sebastian Ruder |  |
| 1197 |  |  [Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems](https://doi.org/10.18653/v1/2022.emnlp-main.542) |  | 0 | Numerical Question Answering is the task of answering questions that require numerical capabilities. Previous works introduce general adversarial attacks to Numerical Question Answering, while not systematically exploring numerical capabilities specific to the topic. In this paper, we propose to... | Jialiang Xu, Mengyu Zhou, Xinyi He, Shi Han, Dongmei Zhang |  |
| 1198 |  |  [Enhancing Joint Multiple Intent Detection and Slot Filling with Global Intent-Slot Co-occurrence](https://doi.org/10.18653/v1/2022.emnlp-main.543) |  | 0 | Multi-intent detection and slot filling joint model attracts more and more attention since it can handle multi-intent utterances, which is closer to complex real-world scenarios. Most existing joint models rely entirely on the training procedure to obtain the implicit correlation between intents... | Mengxiao Song, Bowen Yu, Quangang Li, Yubin Wang, Tingwen Liu, Hongbo Xu |  |
| 1199 |  |  [Towards Pragmatic Production Strategies for Natural Language Generation Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.544) |  | 0 | This position paper proposes a conceptual framework for the design of Natural Language Generation (NLG) systems that follow efficient and effective production strategies in order to achieve complex communicative goals. In this general framework, efficiency is characterised as the parsimonious... | Mario Giulianelli |  |
| 1200 |  |  [LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.545) |  | 0 | Recent large-scale video-language pre-trained models have shown appealing performance on various downstream tasks. However, the pre-training process is computationally expensive due to the requirement of millions of video-text pairs and the redundant data structure of each video. To mitigate these... | Dongsheng Chen, Chaofan Tao, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu |  |
| 1201 |  |  [Communication breakdown: On the low mutual intelligibility between human and neural captioning](https://doi.org/10.18653/v1/2022.emnlp-main.546) |  | 0 | We compare the 0-shot performance of a neural caption-based image retriever when given as input either human-produced captions or captions generated by a neural captioner. We conduct this comparison on the recently introduced ImageCoDe data-set (Krojer et al. 2022), which contains hard distractors... | Roberto Dessì, Eleonora Gualdoni, Francesca Franzon, Gemma Boleda, Marco Baroni |  |
| 1202 |  |  [Normalizing Mutual Information for Robust Adaptive Training for Translation](https://doi.org/10.18653/v1/2022.emnlp-main.547) |  | 0 | Despite the success of neural machine translation models, tensions between fluency of optimizing target language modeling and source-faithfulness remain as challenges. Previously, Conditional Bilingual Mutual Information (CBMI), a scoring metric for the importance of target sentences and tokens,... | Youngwon Lee, Changmin Lee, Hojin Lee, Seungwon Hwang |  |
| 1203 |  |  [Bilingual Synchronization: Restoring Translational Relationships with Editing Operations](https://doi.org/10.18653/v1/2022.emnlp-main.548) |  | 0 | Machine Translation (MT) is usually viewed as a one-shot process that generates the target language equivalent of some source text from scratch. We consider here a more general setting which assumes an initial target sequence, that must be transformed into a valid translation of the source, thereby... | Jitao Xu, Josep Maria Crego, François Yvon |  |
| 1204 |  |  [Human-Machine Collaboration Approaches to Build a Dialogue Dataset for Hate Speech Countering](https://doi.org/10.18653/v1/2022.emnlp-main.549) |  | 0 | Fighting online hate speech is a challenge that is usually addressed using Natural Language Processing via automatic detection and removal of hate content. Besides this approach, counter narratives have emerged as an effective tool employed by NGOs to respond to online hate on social media... | Helena Bonaldi, Sara Dellantonio, Serra Sinem Tekiroglu, Marco Guerini |  |
| 1205 |  |  [JANUS: Joint Autoregressive and Non-autoregressive Training with Auxiliary Loss for Sequence Generation](https://doi.org/10.18653/v1/2022.emnlp-main.550) |  | 0 | Transformer-based autoregressive and non-autoregressive models have played an essential role in sequence generation tasks. The autoregressive model can obtain excellent performance, while the non-autoregressive model brings fast decoding speed for inference. In this paper, we propose JANUS, a Joint... | Xiaobo Liang, Lijun Wu, Juntao Li, Min Zhang |  |
| 1206 |  |  [Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.551) |  | 0 | Most Outside-Knowledge Visual Question Answering (OK-VQA) systems employ a two-stage framework that first retrieves external knowledge given the visual question and then predicts the answer based on the retrieved content. However, the retrieved knowledge is often inadequate. Retrievals are... | Jialin Wu, Raymond J. Mooney |  |
| 1207 |  |  [Cross-Linguistic Syntactic Difference in Multilingual BERT: How Good is It and How Does It Affect Transfer?](https://doi.org/10.18653/v1/2022.emnlp-main.552) |  | 0 | Multilingual BERT (mBERT) has demonstrated considerable cross-lingual syntactic ability, whereby it enables effective zero-shot cross-lingual transfer of syntactic knowledge. The transfer is more successful between some languages, but it is not well understood what leads to this variation and... | Ningyu Xu, Tao Gui, Ruotian Ma, Qi Zhang, Jingting Ye, Menghan Zhang, Xuanjing Huang |  |
| 1208 |  |  ["It's Not Just Hate": A Multi-Dimensional Perspective on Detecting Harmful Speech Online](https://doi.org/10.18653/v1/2022.emnlp-main.553) |  | 0 | Well-annotated data is a prerequisite for good Natural Language Processing models. Too often, though, annotation decisions are governed by optimizing time or annotator agreement. We make a case for nuanced efforts in an interdisciplinary setting for annotating offensive online speech. Detecting... | Federico Bianchi, Stefanie Anja Hills, Patrícia G. C. Rossini, Dirk Hovy, Rebekah Tromble, Nava Tintarev |  |
| 1209 |  |  [Long Text Generation with Topic-aware Discrete Latent Variable Model](https://doi.org/10.18653/v1/2022.emnlp-main.554) |  | 0 | Generating coherent long texts is an important yet challenging task, particularly forthe open-ended generation. Prior work based on discrete latent codes focuses on the modeling of discourse relation, resulting in discrete codes only learning shallow semantics (Ji and Huang, 2021). A natural text... | Erguang Yang, Mingtong Liu, Deyi Xiong, Yujie Zhang, Yufeng Chen, Jinan Xu |  |
| 1210 |  |  [TIARA: Multi-grained Retrieval for Robust Question Answering over Large Knowledge Base](https://doi.org/10.18653/v1/2022.emnlp-main.555) |  | 0 | Pre-trained language models (PLMs) have shown their effectiveness in multiple scenarios. However, KBQA remains challenging, especially regarding coverage and generalization settings. This is due to two main factors: i) understanding the semantics of both questions and relevant knowledge from the... | Yiheng Shu, Zhiwei Yu, Yuhan Li, Börje F. Karlsson, Tingting Ma, Yuzhong Qu, ChinYew Lin |  |
| 1211 |  |  [Structure-Unified M-Tree Coding Solver for Math Word Problem](https://doi.org/10.18653/v1/2022.emnlp-main.556) |  | 0 | As one of the challenging NLP tasks, designing math word problem (MWP) solvers has attracted increasing research attention for the past few years. In previous work, models designed by taking into account the properties of the binary tree structure of mathematical expressions at the output side have... | Bin Wang, Jiangzhou Ju, Yang Fan, Xinyu Dai, Shujian Huang, Jiajun Chen |  |
| 1212 |  |  [FormLM: Recommending Creation Ideas for Online Forms by Modelling Semantic and Structural Information](https://doi.org/10.18653/v1/2022.emnlp-main.557) |  | 0 | Online forms are widely used to collect data from human and have a multi-billion market. Many software products provide online services for creating semi-structured forms where questions and descriptions are organized by predefined structures. However, the design and creation process of forms is... | Yijia Shao, Mengyu Zhou, Yifan Zhong, Tao Wu, Hongwei Han, Shi Han, Gideon Huang, Dongmei Zhang |  |
| 1213 |  |  [Generate, Discriminate and Contrast: A Semi-Supervised Sentence Representation Learning Framework](https://doi.org/10.18653/v1/2022.emnlp-main.558) |  | 0 | Most sentence embedding techniques heavily rely on expensive human-annotated sentence pairs as the supervised signals. Despite the use of large-scale unlabeled data, the performance of unsupervised methods typically lags far behind that of the supervised counterparts in most downstream tasks. In... | Yiming Chen, Yan Zhang, Bin Wang, Zuozhu Liu, Haizhou Li |  |
| 1214 |  |  [GPS: Genetic Prompt Search for Efficient Few-Shot Learning](https://doi.org/10.18653/v1/2022.emnlp-main.559) |  | 0 | Prompt-based techniques have demostrated great potential for improving the few-shot generalization of pretrained language models. However, their performance heavily relies on the manual design of prompts and thus requiring a lot of human efforts. In this paper, we introduce Genetic Prompt Search... | Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, Zhilin Yang |  |
| 1215 |  |  [Multitask Instruction-based Prompting for Fallacy Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.560) |  | 0 | Fallacies are used as seemingly valid arguments to support a position and persuade the audience about its validity. Recognizing fallacies is an intrinsically difficult task both for humans and machines. Moreover, a big challenge for computational models lies in the fact that fallacies are... | Tariq Alhindi, Tuhin Chakrabarty, Elena Musi, Smaranda Muresan |  |
| 1216 |  |  [Rethinking Multi-Modal Alignment in Multi-Choice VideoQA from Feature and Sample Perspectives](https://doi.org/10.18653/v1/2022.emnlp-main.561) |  | 0 | Reasoning about causal and temporal event relations in videos is a new destination of Video Question Answering (VideoQA). The major stumbling block to achieve this purpose is the semantic gap between language and video since they are at different levels of abstraction. Existing efforts mainly focus... | Shaoning Xiao, Long Chen, Kaifeng Gao, Zhao Wang, Yi Yang, Zhimeng Zhang, Jun Xiao |  |
| 1217 |  |  [Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach](https://doi.org/10.18653/v1/2022.emnlp-main.562) |  | 0 | Although remarkable progress on the neural table-to-text methods has been made, the generalization issues hinder the applicability of these models due to the limited source tables. Large-scale pretrained language models sound like a promising solution to tackle such issues. However, how to... | Miao Chen, Xinjiang Lu, Tong Xu, Yanyan Li, Jingbo Zhou, Dejing Dou, Hui Xiong |  |
| 1218 |  |  [Hierarchical Phrase-Based Sequence-to-Sequence Learning](https://doi.org/10.18653/v1/2022.emnlp-main.563) |  | 0 | This paper describes a neural transducer that maintains the flexibility of standard sequence-to-sequence (seq2seq) models while incorporating hierarchical phrases as a source of inductive bias during training and as explicit constraints during inference. Our approach trains two models: a... | Bailin Wang, Ivan Titov, Jacob Andreas, Yoon Kim |  |
| 1219 |  |  [Natural Language Deduction with Incomplete Information](https://doi.org/10.18653/v1/2022.emnlp-main.564) |  | 0 | A growing body of work studies how to answer a question or verify a claim by generating a natural language “proof:” a chain of deductive inferences yielding the answer based on a set of premises. However, these methods can only make sound deductions when they follow from evidence that is given. We... | Zayne Sprague, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett |  |
| 1220 |  |  [Character-centric Story Visualization via Visual Planning and Token Alignment](https://doi.org/10.18653/v1/2022.emnlp-main.565) |  | 0 | Story visualization advances the traditional text-to-image generation by enabling multiple image generation based on a complete story. This task requires machines to 1) understand long text inputs, and 2) produce a globally consistent image sequence that illustrates the contents of the story. A key... | Hong Chen, Rujun Han, TeLin Wu, Hideki Nakayama, Nanyun Peng |  |
| 1221 |  |  [ASQA: Factoid Questions Meet Long-Form Answers](https://doi.org/10.18653/v1/2022.emnlp-main.566) |  | 0 | Recent progress on open domain factoid question answering (QA) does not easily transfer to the task of long-form QA, where the goal is to answer questions that require in-depth explanations. The hurdles include a lack of high-quality data and the absence of a well-defined notion of an answer’s... | Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, MingWei Chang |  |
| 1222 |  |  [Algorithms for Acyclic Weighted Finite-State Automata with Failure Arcs](https://doi.org/10.18653/v1/2022.emnlp-main.567) |  | 0 | Weighted finite-state automata (WSFAs) arecommonly used in NLP. Failure transitions area useful extension for compactly representingbackoffs or interpolation in n-gram modelsand CRFs, which are special cases of WFSAs.Unfortunately, applying standard algorithmsfor computing the pathsum requires... | Anej Svete, Benjamin Dayan, Ryan Cotterell, Tim Vieira, Jason Eisner |  |
| 1223 |  |  [Towards Better Document-level Relation Extraction via Iterative Inference](https://doi.org/10.18653/v1/2022.emnlp-main.568) |  | 0 | Document-level relation extraction (RE) aims to extract the relations between entities from the input document that usually containing many difficultly-predicted entity pairs whose relations can only be predicted through relational inference. Existing methods usually directly predict the relations... | Liang Zhang, Jinsong Su, Yidong Chen, Zhongjian Miao, Zijun Min, Qingguo Hu, Xiaodong Shi |  |
| 1224 |  |  [Efficient Adversarial Training with Robust Early-Bird Tickets](https://doi.org/10.18653/v1/2022.emnlp-main.569) |  | 0 | Adversarial training is one of the most powerful methods to improve the robustness of pre-trained language models (PLMs). However, this approach is typically more expensive than traditional fine-tuning because of the necessity to generate adversarial examples via gradient descent. Delving into the... | Zhiheng Xi, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 1225 |  |  [Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks](https://doi.org/10.18653/v1/2022.emnlp-main.570) |  | 0 | The wide adoption and application of Masked language models (MLMs) on sensitive data (from legal to medical) necessitates a thorough quantitative investigation into their privacy vulnerabilities. Prior attempts at measuring leakage of MLMs via membership inference attacks have been inconclusive,... | Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor BergKirkpatrick, Reza Shokri |  |
| 1226 |  |  [SMaLL-100: Introducing Shallow Multilingual Machine Translation Model for Low-Resource Languages](https://doi.org/10.18653/v1/2022.emnlp-main.571) |  | 0 | In recent years, multilingual machine translation models have achieved promising performance on low-resource language pairs by sharing information between similar languages, thus enabling zero-shot translation. To overcome the “curse of multilinguality”, these models often opt for scaling up the... | Alireza Mohammadshahi, Vassilina Nikoulina, Alexandre Berard, Caroline Brun, James Henderson, Laurent Besacier |  |
| 1227 |  |  [TextFusion: Privacy-Preserving Pre-trained Model Inference via Token Fusion](https://doi.org/10.18653/v1/2022.emnlp-main.572) |  | 0 | Recently, more and more pre-trained language models are released as a cloud service. It allows users who lack computing resources to perform inference with a powerful model by uploading data to the cloud. The plain text may contain private information, as the result, users prefer to do partial... | Xin Zhou, Jinzhu Lu, Tao Gui, Ruotian Ma, Zichu Fei, Yuran Wang, Yong Ding, Yibo Cheung, Qi Zhang, Xuanjing Huang |  |
| 1228 |  |  [Learning to Explain Selectively: A Case Study on Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.573) |  | 0 | Explanations promise to bridge the gap between humans and AI, yet it remains difficult to achieve consistent improvement in AI-augmented human decision making. The usefulness of AI explanations depends on many factors, and always showing the same type of explanation in all cases is suboptimal—so is... | Shi Feng, Jordan L. BoydGraber |  |
| 1229 |  |  [ConsistTL: Modeling Consistency in Transfer Learning for Low-Resource Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.574) |  | 0 | Transfer learning is a simple and powerful method that can be used to boost model performance of low-resource neural machine translation (NMT). Existing transfer learning methods for NMT are static, which simply transfer knowledge from a parent model to a child model once via parameter... | Zhaocong Li, Xuebo Liu, Derek F. Wong, Lidia S. Chao, Min Zhang |  |
| 1230 |  |  [Better Hit the Nail on the Head than Beat around the Bush: Removing Protected Attributes with a Single Projection](https://doi.org/10.18653/v1/2022.emnlp-main.575) |  | 0 | Bias elimination and recent probing studies attempt to remove specific information from embedding spaces. Here it is important to remove as much of the target information as possible, while preserving any other information present. INLP is a popular recent method which removes specific information... | Pantea Haghighatkhah, Antske Fokkens, Pia Sommerauer, Bettina Speckmann, Kevin Verbeek |  |
| 1231 |  |  [IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.576) |  | 0 | We introduce a new open information extraction (OIE) benchmark for pre-trained language models (LM). Recent studies have demonstrated that pre-trained LMs, such as BERT and GPT, may store linguistic and relational knowledge. In particular, LMs are able to answer “fill-in-the-blank” questions when... | Chenguang Wang, Xiao Liu, Dawn Song |  |
| 1232 |  |  [ConNER: Consistency Training for Cross-lingual Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.577) |  | 0 | Cross-lingual named entity recognition (NER) suffers from data scarcity in the target languages, especially under zero-shot settings. Existing translate-train or knowledge distillation methods attempt to bridge the language gap, but often introduce a high level of noise. To solve this problem,... | Ran Zhou, Xin Li, Lidong Bing, Erik Cambria, Luo Si, Chunyan Miao |  |
| 1233 |  |  [A Sequential Flow Control Framework for Multi-hop Knowledge Base Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.578) |  | 0 | One of the key challenges of knowledge base question answering (KBQA) is the multi-hop reasoning. Since in different hops, one attends to different parts of question, it is important to dynamically represent the question semantics for each hop. Existing methods, however, (i) infer the dynamic... | Minghui Xie, Chuzhan Hao, Peng Zhang |  |
| 1234 |  |  [ACENet: Attention Guided Commonsense Reasoning on Hybrid Knowledge Graph](https://doi.org/10.18653/v1/2022.emnlp-main.579) |  | 0 | Augmenting pre-trained language models (PLMs) with knowledge graphs (KGs) has demonstrated superior performance on commonsense reasoning. Given a commonsense based QA context (question and multiple choices), existing approaches usually estimate the plausibility of candidate choices separately based... | Chuzhan Hao, Minghui Xie, Peng Zhang |  |
| 1235 |  |  [Revisiting DocRED - Addressing the False Negative Problem in Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.580) |  | 0 | The DocRED dataset is one of the most popular and widely used benchmarks for document-level relation extraction (RE). It adopts a recommend-revise annotation scheme so as to have a large-scale annotated dataset. However, we find that the annotation of DocRED is incomplete, i.e., false negative... | Qingyu Tan, Lu Xu, Lidong Bing, Hwee Tou Ng, Sharifah Mahani Aljunied |  |
| 1236 |  |  [Towards Summary Candidates Fusion](https://doi.org/10.18653/v1/2022.emnlp-main.581) |  | 0 | Sequence-to-sequence deep neural models fine-tuned for abstractive summarization can achieve great performance on datasets with enough human annotations. Yet, it has been shown that they have not reached their full potential, with a wide gap between the top beam search output and the oracle beam.... | Mathieu Ravaut, Shafiq R. Joty, Nancy F. Chen |  |
| 1237 |  |  [Multimodal Robustness for Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.582) |  | 0 | In this paper, we look at the case of a Generic text-to-text NMT model that has to deal with data coming from various modalities, like speech, images, or noisy text extracted from the web. We propose a two-step method, based on composable adapters, to deal with this problem of Multimodal... | Yuting Zhao, Ioan Calapodescu |  |
| 1238 |  |  [TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction](https://doi.org/10.18653/v1/2022.emnlp-main.583) |  | 0 | Knowledge graph embedding methods are important for the knowledge graph completion (or link prediction) task.One state-of-the-art method, PairRE, leverages two separate vectors to model complex relations (i.e., 1-to-N, N-to-1, and N-to-N) in knowledge graphs. However, such a method strictly... | Yizhi Li, Wei Fan, Chao Liu, Chenghua Lin, Jiang Qian |  |
| 1239 |  |  [IRRGN: An Implicit Relational Reasoning Graph Network for Multi-turn Response Selection](https://doi.org/10.18653/v1/2022.emnlp-main.584) |  | 0 | The task of response selection in multi-turn dialogue is to find the best option from all candidates. In order to improve the reasoning ability of the model, previous studies pay more attention to using explicit algorithms to model the dependencies between utterances, which are deterministic,... | Jingcheng Deng, Hengwei Dai, Xuewei Guo, Yuanchen Ju, Wei Peng |  |
| 1240 |  |  [Predicting Prerequisite Relations for Unseen Concepts](https://doi.org/10.18653/v1/2022.emnlp-main.585) |  | 0 | Concept prerequisite learning (CPL) plays a key role in developing technologies that assist people to learn a new complex topic or concept. Previous work commonly assumes that all concepts are given at training time and solely focuses on predicting the unseen prerequisite relationships between... | Yaxin Zhu, Hamed Zamani |  |
| 1241 |  |  [Contrastive Learning with Expectation-Maximization for Weakly Supervised Phrase Grounding](https://doi.org/10.18653/v1/2022.emnlp-main.586) |  | 0 | Weakly supervised phrase grounding aims to learn an alignment between phrases in a caption and objects in a corresponding image using only caption-image annotations, i.e., without phrase-object annotations. Previous methods typically use a caption-image contrastive loss to indirectly supervise the... | Keqin Chen, Richong Zhang, Samuel Mensah, Yongyi Mao |  |
| 1242 |  |  [Beyond prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations](https://doi.org/10.18653/v1/2022.emnlp-main.587) |  | 0 | Recent work has demonstrated that pre-trained language models (PLMs) are zero-shot learners. However, most existing zero-shot methods involve heavy human engineering or complicated self-training pipelines, hindering their application to new situations. In this work, we show that zero-shot text... | Yu Fei, Zhao Meng, Ping Nie, Roger Wattenhofer, Mrinmaya Sachan |  |
| 1243 |  |  [Generalizing over Long Tail Concepts for Medical Term Normalization](https://doi.org/10.18653/v1/2022.emnlp-main.588) |  | 0 | Medical term normalization consists in mapping a piece of text to a large number of output classes.Given the small size of the annotated datasets and the extremely long tail distribution of the concepts, it is of utmost importance to develop models that are capable to generalize to scarce or unseen... | Beatrice Portelli, Simone Scaboro, Enrico Santus, Hooman Sedghamiz, Emmanuele Chersoni, Giuseppe Serra |  |
| 1244 |  |  [Unsupervised Opinion Summarisation in the Wasserstein Space](https://doi.org/10.18653/v1/2022.emnlp-main.589) |  | 0 | Opinion summarisation synthesises opinions expressed in a group of documents discussingthe same topic to produce a single summary. Recent work has looked at opinion summarisation of clusters of social media posts. Such posts are noisy and have unpredictable structure, posing additional challenges... | Jiayu Song, Iman Munire Bilal, Adam Tsakalidis, Rob Procter, Maria Liakata |  |
| 1245 |  |  [Bloom Library: Multimodal Datasets in 300+ Languages for a Variety of Downstream Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.590) |  | 0 | We present Bloom Library, a linguistically diverse set of multimodal and multilingual datasets for language modeling, image captioning, visual storytelling, and speech synthesis/recognition. These datasets represent either the most, or among the most, multilingual datasets for each of the included... | Colin Leong, Joshua Nemecek, Jacob Mansdorfer, Anna Filighera, Abraham Owodunni, Daniel Whitenack |  |
| 1246 |  |  [Disentangling Uncertainty in Machine Translation Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.591) |  | 0 | Trainable evaluation metrics for machine translation (MT) exhibit strong correlation with human judgements, but they are often hard to interpret and might produce unreliable scores under noisy or out-of-domain data. Recent work has attempted to mitigate this with simple uncertainty quantification... | Chrysoula Zerva, Taisiya Glushkova, Ricardo Rei, André F. T. Martins |  |
| 1247 |  |  [Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing](https://doi.org/10.18653/v1/2022.emnlp-main.592) |  | 0 | Entity typing aims at predicting one or more words that describe the type(s) of a specific mention in a sentence. Due to shortcuts from surface patterns to annotated entity labels and biased training, existing entity typing models are subject to the problem of spurious correlations. To... | Nan Xu, Fei Wang, Bangzheng Li, Mingtao Dong, Muhao Chen |  |
| 1248 |  |  [EDIN: An End-to-end Benchmark and Pipeline for Unknown Entity Discovery and Indexing](https://doi.org/10.18653/v1/2022.emnlp-main.593) |  | 0 | Existing work on Entity Linking mostly assumes that the reference knowledge base is complete, and therefore all mentions can be linked. In practice this is hardly ever the case, as knowledge bases are incomplete and because novel concepts arise constantly. We introduce the temporally segmented... | Nora Kassner, Fabio Petroni, Mikhail Plekhanov, Sebastian Riedel, Nicola Cancedda |  |
| 1249 |  |  [POQue: Asking Participant-specific Outcome Questions for a Deeper Understanding of Complex Events](https://doi.org/10.18653/v1/2022.emnlp-main.594) |  | 0 | Knowledge about outcomes is critical for complex event understanding but is hard to acquire.We show that by pre-identifying a participant in a complex event, crowdworkers are ableto (1) infer the collective impact of salient events that make up the situation, (2) annotate the volitional engagement... | Sai Vallurupalli, Sayontan Ghosh, Katrin Erk, Niranjan Balasubramanian, Francis Ferraro |  |
| 1250 |  |  [Measuring the Mixing of Contextual Information in the Transformer](https://doi.org/10.18653/v1/2022.emnlp-main.595) |  | 0 | The Transformer architecture aggregates input information through the self-attention mechanism, but there is no clear understanding of how this information is mixed across the entire model. Additionally, recent works have demonstrated that attention weights alone are not enough to describe the flow... | Javier Ferrando, Gerard I. Gállego, Marta R. Costajussà |  |
| 1251 |  |  [Dealing with Abbreviations in the Slovenian Biographical Lexicon](https://doi.org/10.18653/v1/2022.emnlp-main.596) |  | 0 | Abbreviations present a significant challenge for NLP systems because they cause tokenization and out-of-vocabulary errors. They can also make the text less readable, especially in reference printed books, where they are extensively used. Abbreviations are especially problematic in low-resource... | Angel Daza, Antske Fokkens, Tomaz Erjavec |  |
| 1252 |  |  [AfriCLIRMatrix: Enabling Cross-Lingual Information Retrieval for African Languages](https://doi.org/10.18653/v1/2022.emnlp-main.597) |  | 0 | Language diversity in NLP is critical in enabling the development of tools for a wide range of users.However, there are limited resources for building such tools for many languages, particularly those spoken in Africa.For search, most existing datasets feature few or no African languages, directly... | Odunayo Ogundepo, Xinyu Zhang, Shuo Sun, Kevin Duh, Jimmy Lin |  |
| 1253 |  |  [CONDAQA: A Contrastive Reading Comprehension Dataset for Reasoning about Negation](https://doi.org/10.18653/v1/2022.emnlp-main.598) |  | 0 | The full power of human language-based communication cannot be realized without negation. All human languages have some form of negation. Despite this, negation remains a challenging phenomenon for current natural language understanding systems. To facilitate the future development of models that... | Abhilasha Ravichander, Matt Gardner, Ana Marasovic |  |
| 1254 |  |  [Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer](https://doi.org/10.18653/v1/2022.emnlp-main.599) |  | 0 | In Neural Machine Translation (NMT), each token prediction is conditioned on the source sentence and the target prefix (what has been previously translated at a decoding step). However, previous work on interpretability in NMT has mainly focused solely on source sentence tokens’ attributions.... | Javier Ferrando, Gerard I. Gállego, Belen Alastruey, Carlos Escolano, Marta R. Costajussà |  |
| 1255 |  |  [ArtELingo: A Million Emotion Annotations of WikiArt with Emphasis on Diversity over Language and Culture](https://doi.org/10.18653/v1/2022.emnlp-main.600) |  | 0 | This paper introduces ArtELingo, a new benchmark and dataset, designed to encourage work on diversity across languages and cultures. Following ArtEmis, a collection of 80k artworks from WikiArt with 0.45M emotion labels and English-only captions, ArtELingo adds another 0.79M annotations in Arabic... | Youssef Mohamed, Mohamed Abdelfattah, Shyma Alhuwaider, Feifan Li, Xiangliang Zhang, Kenneth Church, Mohamed Elhoseiny |  |
| 1256 |  |  [Decoding a Neural Retriever's Latent Space for Query Suggestion](https://doi.org/10.18653/v1/2022.emnlp-main.601) |  | 0 | Neural retrieval models have superseded classic bag-of-words methods such as BM25 as the retrieval framework of choice. However, neural systems lack the interpretability of bag-of-words models; it is not trivial to connect a query change to a change in the latent space that ultimately determines... | Leonard Adolphs, Michelle Chen Huebscher, Christian Buck, Sertan Girgin, Olivier Bachem, Massimiliano Ciaramita, Thomas Hofmann |  |
| 1257 |  |  [T-STAR: Truthful Style Transfer using AMR Graph as Intermediate Representation](https://doi.org/10.18653/v1/2022.emnlp-main.602) |  | 0 | Unavailability of parallel corpora for training text style transfer (TST) models is a very challenging yet common scenario. Also, TST models implicitly need to preserve the content while transforming a source sentence into the target style. To tackle these problems, an intermediate representation... | Anubhav Jangra, Preksha Nema, Aravindan Raghuveer |  |
| 1258 |  |  [PromptBERT: Improving BERT Sentence Embeddings with Prompts](https://doi.org/10.18653/v1/2022.emnlp-main.603) |  | 0 | We propose PromptBERT, a novel contrastive learning method for learning better sentence representation. We firstly analysis the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers. Then we propose... | Ting Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang, Deqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen Huang, Denvy Deng, Qi Zhang |  |
| 1259 |  |  [Extending Logic Explained Networks to Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.604) |  | 0 | Recently, Logic Explained Networks (LENs) have been proposed as explainable-by-design neural models providing logic explanations for their predictions.However, these models have only been applied to vision and tabular data, and they mostly favour the generation of global explanations, while local... | Rishabh Jain, Gabriele Ciravegna, Pietro Barbiero, Francesco Giannini, Davide Buffelli, Pietro Liò |  |
| 1260 |  |  [Uni-Parser: Unified Semantic Parser for Question Answering on Knowledge Base and Database](https://doi.org/10.18653/v1/2022.emnlp-main.605) |  | 0 | Parsing natural language questions into executable logical forms is a useful and interpretable way to perform question answering on structured data such as knowledge bases (KB) or databases (DB). However, existing approaches on semantic parsing cannot adapt to both modalities, as they suffer from... | Ye Liu, Semih Yavuz, Rui Meng, Dragomir Radev, Caiming Xiong, Yingbo Zhou |  |
| 1261 |  |  [RAPO: An Adaptive Ranking Paradigm for Bilingual Lexicon Induction](https://doi.org/10.18653/v1/2022.emnlp-main.606) |  | 0 | Bilingual lexicon induction induces the word translations by aligning independently trained word embeddings in two languages. Existing approaches generally focus on minimizing the distances between words in the aligned pairs, while suffering from low discriminative capability to distinguish the... | Zhoujin Tian, Chaozhuo Li, Shuo Ren, Zhiqiang Zuo, Zengxuan Wen, Xinyue Hu, Xiao Han, Haizhen Huang, Denvy Deng, Qi Zhang, Xing Xie |  |
| 1262 |  |  [On Parsing as Tagging](https://doi.org/10.18653/v1/2022.emnlp-main.607) |  | 0 | There are many proposals to reduce constituency parsing to tagging. To figure out what these approaches have in common, we offer a unifying pipeline, which consists of three steps: linearization, learning, and decoding. We prove that classic shift–reduce parsing can be reduced to tetratagging—the... | Afra Amini, Ryan Cotterell |  |
| 1263 |  |  [Distilled Dual-Encoder Model for Vision-Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.608) |  | 0 | On vision-language understanding (VLU) tasks, fusion-encoder vision-language models achieve superior results but sacrifice efficiency because of the simultaneous encoding of images and text. On the contrary, the dual encoder model that separately encodes images and text has the advantage in... | Zekun Wang, Wenhui Wang, Haichao Zhu, Ming Liu, Bing Qin, Furu Wei |  |
| 1264 |  |  [Argument Mining for Review Helpfulness Prediction](https://doi.org/10.18653/v1/2022.emnlp-main.609) |  | 0 | The importance of reliably determining the helpfulness of product reviews is rising as both helpful and unhelpful reviews continue to accumulate on e-commerce websites. And argumentational features—such as the structure of arguments and the types of underlying elementary units—have shown to be... | Zaiqian Chen, Daniel Verdi do Amarante, Jenna Donaldson, Yohan Jo, Joonsuk Park |  |
| 1265 |  |  [Hierarchical Multi-Label Classification of Scientific Documents](https://doi.org/10.18653/v1/2022.emnlp-main.610) |  | 0 | Automatic topic classification has been studied extensively to assist managing and indexing scientific documents in a digital collection. With the large number of topics being available in recent years, it has become necessary to arrange them in a hierarchy. Therefore, the automatic classification... | Mobashir Sadat, Cornelia Caragea |  |
| 1266 |  |  [Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.611) |  | 0 | Knowledge underpins reasoning. Recent research demonstrates that when relevant knowledge is provided as additional context to commonsense question answering (QA), it can substantially enhance the performance even on top of state-of-the-art. The fundamental challenge is where and how to find such... | Jiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, Yejin Choi |  |
| 1267 |  |  [A Major Obstacle for NLP Research: Let's Talk about Time Allocation!](https://doi.org/10.18653/v1/2022.emnlp-main.612) |  | 0 | The field of natural language processing (NLP) has grown over the last few years: conferences have become larger, we have published an incredible amount of papers, and state-of-the-art research has been implemented in a large variety of customer-facing products. However, this paper argues that we... | Katharina Kann, Shiran Dudy, Arya D. McCarthy |  |
| 1268 |  |  [Towards Inter-character Relationship-driven Story Generation](https://doi.org/10.18653/v1/2022.emnlp-main.613) |  | 0 | In this paper, we introduce the task of modeling interpersonal relationships for story generation. For addressing this task, we propose Relationships as Latent Variables for Story Generation, (ReLiSt). ReLiSt generates stories sentence by sentence and has two major components - a relationship... | Anvesh Rao Vijjini, Faeze Brahman, Snigdha Chaturvedi |  |
| 1269 |  |  [Incorporating Relevance Feedback for Information-Seeking Retrieval using Few-Shot Document Re-Ranking](https://doi.org/10.18653/v1/2022.emnlp-main.614) |  | 0 | Pairing a lexical retriever with a neural re-ranking model has set state-of-the-art performance on large-scale information retrieval datasets. This pipeline covers scenarios like question answering or navigational queries, however, for information-seeking scenarios, users often provide information... | Tim Baumgärtner, Leonardo F. R. Ribeiro, Nils Reimers, Iryna Gurevych |  |
| 1270 |  |  [ReasTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples](https://doi.org/10.18653/v1/2022.emnlp-main.615) |  | 0 | Reasoning over tabular data requires both table structure understanding and a broad set of table reasoning skills. Current models with table-specific architectures and pre-training methods perform well on understanding table structures, but they still struggle with tasks that require various table... | Yilun Zhao, Linyong Nan, Zhenting Qi, Rui Zhang, Dragomir Radev |  |
| 1271 |  |  [Few-shot Learning with Multilingual Generative Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.616) |  | 0 | Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train... | Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T. Diab, Veselin Stoyanov, Xian Li |  |
| 1272 |  |  [Are representations built from the ground up? An empirical examination of local composition in language models](https://doi.org/10.18653/v1/2022.emnlp-main.617) |  | 0 | Compositionality, the phenomenon where the meaning of a phrase can be derived from its constituent parts, is a hallmark of human language. At the same time, many phrases are non-compositional, carrying a meaning beyond that of each part in isolation. Representing both of these types of phrases is... | Emmy Liu, Graham Neubig |  |
| 1273 |  |  [Detecting Label Errors by Using Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.618) |  | 0 | We show that large pre-trained language models are inherently highly capable of identifying label errors in natural language datasets: simply examining out-of-sample data points in descending order of fine-tuned task loss significantly outperforms more complex error-detection mechanisms proposed in... | Derek Chong, Jenny Hong, Christopher D. Manning |  |
| 1274 |  |  [Intriguing Properties of Compression on Multilingual Models](https://doi.org/10.18653/v1/2022.emnlp-main.619) |  | 0 | Multilingual models are often particularly dependent on scaling to generalize to a growing number of languages. Compression techniques are widely relied upon to reconcile the growth in model size with real world resource constraints, but compression can have a disparate effect on model performance... | Kelechi Ogueji, Orevaoghene Ahia, Gbemileke Onilude, Sebastian Gehrmann, Sara Hooker, Julia Kreutzer |  |
| 1275 |  |  [Sequence Models for Document Structure Identification in an Undeciphered Script](https://doi.org/10.18653/v1/2022.emnlp-main.620) |  | 0 | This work describes the first thorough analysis of “header” signs in proto-Elamite, an undeciphered script from 3100-2900 BCE. Headers are a category of signs which have been provisionally identified through painstaking manual analysis of this script by domain experts. We use unsupervised neural... | Logan Born, M. Willis Monroe, Kathryn Kelley, Anoop Sarkar |  |
| 1276 |  |  [English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.621) |  | 0 | Universal cross-lingual sentence embeddings map semantically similar cross-lingual sentences into a shared embedding space. Aligning cross-lingual sentence embeddings usually requires supervised cross-lingual parallel sentences. In this work, we propose mSimCSE, which extends SimCSE to multilingual... | YauShian Wang, Ashley Wu, Graham Neubig |  |
| 1277 |  |  [Active Example Selection for In-Context Learning](https://doi.org/10.18653/v1/2022.emnlp-main.622) |  | 0 | With a handful of demonstration examples, large-scale language models demonstrate strong capability to perform various tasks by in-context learning from these examples, without any fine-tuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples,... | Yiming Zhang, Shi Feng, Chenhao Tan |  |
| 1278 |  |  [Improving Factual Consistency in Summarization with Compression-Based Post-Editing](https://doi.org/10.18653/v1/2022.emnlp-main.623) |  | 0 | State-of-the-art summarization models still struggle to be factually consistent with the input text. A model-agnostic way to address this problem is post-editing the generated summaries. However, existing approaches typically fail to remove entity errors if a suitable input entity replacement is... | Alexander R. Fabbri, Prafulla Kumar Choubey, Jesse Vig, ChienSheng Wu, Caiming Xiong |  |
| 1279 |  |  [Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.624) |  | 0 | Despite their strong performance on many tasks, pre-trained language models have been shown to struggle on out-of-distribution compositional generalization. Meanwhile, recent work has shown considerable improvements on many NLP tasks from model scaling. Can scaling up model size also improve... | Linlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi, Jonathan Herzig, Emily Pitler, Fei Sha, Kristina Toutanova |  |
| 1280 |  |  ["I'm sorry to hear that": Finding New Biases in Language Models with a Holistic Descriptor Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.625) |  | 0 | As language models grow in popularity, it becomes increasingly important to clearly measure all possible markers of demographic identity in order to avoid perpetuating existing societal harms. Many datasets for measuring bias currently exist, but they are restricted in their coverage of demographic... | Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, Adina Williams |  |
| 1281 |  |  [Understanding ME? Multimodal Evaluation for Fine-grained Visual Commonsense](https://doi.org/10.18653/v1/2022.emnlp-main.626) |  | 0 | Visual commonsense understanding requires Vision Language (VL) models to not only understand image and text but also cross-reference in-between to fully integrate and achieve comprehension of the visual scene described. Recently, various approaches have been developed and have achieved high... | Zhecan Wang, Haoxuan You, Yicheng He, Wenhao Li, KaiWei Chang, ShihFu Chang |  |
| 1282 |  |  [Semantic Novelty Detection and Characterization in Factual Text Involving Named Entities](https://doi.org/10.18653/v1/2022.emnlp-main.627) |  | 0 | Much of the existing work on text novelty detection has been studied at the topic level, i.e., identifying whether the topic of a document or a sentence is novel or not. Little work has been done at the fine-grained semantic level (or contextual level). For example, given that we know Elon Musk is... | Nianzu Ma, Sahisnu Mazumder, Alexander Politowicz, Bing Liu, Eric Robertson, Scott Grigsby |  |
| 1283 |  |  [CN-AutoMIC: Distilling Chinese Commonsense Knowledge from Pretrained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.628) |  | 0 | Commonsense knowledge graphs (CKGs) are increasingly applied in various natural language processing tasks. However, most existing CKGs are limited to English, which hinders related research in non-English languages. Meanwhile, directly generating commonsense knowledge from pretrained language... | Chenhao Wang, Jiachun Li, Yubo Chen, Kang Liu, Jun Zhao |  |
| 1284 |  |  [Calibrating Student Models for Emotion-related Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.629) |  | 0 | Knowledge Distillation (KD) is an effective method to transfer knowledge from one network (a.k.a. teacher) to another (a.k.a. student). In this paper, we study KD on the emotion-related tasks from a new perspective: calibration. We further explore the impact of the mixup data augmentation technique... | Mahshid Hosseini, Cornelia Caragea |  |
| 1285 |  |  [Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation](https://doi.org/10.18653/v1/2022.emnlp-main.630) |  | 0 | In this paper, we explore the challenging problem of performing a generative task in a target language when labeled data is only available in English, using summarization as a case study. We assume a strict setting with no access to parallel data or machine translation and find that common transfer... | Tu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mohit Iyyer, Noah Constant |  |
| 1286 |  |  [Improving Large-scale Paraphrase Acquisition and Generation](https://doi.org/10.18653/v1/2022.emnlp-main.631) |  | 0 | This paper addresses the quality issues in existing Twitter-based paraphrase datasets, and discusses the necessity of using two separate definitions of paraphrase for identification and generation tasks. We present a new Multi-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total... | Yao Dou, Chao Jiang, Wei Xu |  |
| 1287 |  |  [Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal](https://doi.org/10.18653/v1/2022.emnlp-main.632) |  | 0 | Transformer-based large language models are trained to make predictions about the next word by aggregating representations of previous tokens through their self-attention mechanism. In the field of cognitive modeling, such attention patterns have recently been interpreted as embodying the process... | ByungDoh Oh, William Schuler |  |
| 1288 |  |  [A Survey of Computational Framing Analysis Approaches](https://doi.org/10.18653/v1/2022.emnlp-main.633) |  | 0 | Framing analysis is predominantly qualitative and quantitative, examining a small dataset with manual coding. Easy access to digital data in the last two decades prompts scholars in both computation and social sciences to utilize various computational methods to explore frames in large-scale... | Mohammad Ali, Naeemul Hassan |  |
| 1289 |  |  [Learning Cross-Task Dependencies for Joint Extraction of Entities, Events, Event Arguments, and Relations](https://doi.org/10.18653/v1/2022.emnlp-main.634) |  | 0 | Extracting entities, events, event arguments, and relations (i.e., task instances) from text represents four main challenging tasks in information extraction (IE), which have been solved jointly (JointIE) to boost the overall performance for IE. As such, previous work often leverages two types of... | Minh Van Nguyen, Bonan Min, Franck Dernoncourt, Thien Huu Nguyen |  |
| 1290 |  |  [Don't Copy the Teacher: Data and Model Challenges in Embodied Dialogue](https://doi.org/10.18653/v1/2022.emnlp-main.635) |  | 0 | Embodied dialogue instruction following requires an agent to complete a complex sequence of tasks from a natural language exchange. The recent introduction of benchmarks raises the question of how best to train and evaluate models for this multi-turn, multi-agent, long-horizon task. This paper... | So Yeon Min, Hao Zhu, Ruslan Salakhutdinov, Yonatan Bisk |  |
| 1291 |  |  [ALFRED-L: Investigating the Role of Language for Action Learning in Interactive Visual Environments](https://doi.org/10.18653/v1/2022.emnlp-main.636) |  | 0 | Embodied Vision and Language Task Completion requires an embodied agent to interpret natural language instructions and egocentric visual observations to navigate through and interact with environments. In this work, we examine ALFRED, a challenging benchmark for embodied task completion, with the... | Arjun R. Akula, Spandana Gella, Aishwarya Padmakumar, Mahdi Namazifar, Mohit Bansal, Jesse Thomason, Dilek HakkaniTur |  |
| 1292 |  |  [Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence](https://doi.org/10.18653/v1/2022.emnlp-main.637) |  | 0 | AI researchers have posited Dungeons and Dragons (D&D) as a challenge problem to test systems on various language-related capabilities. In this paper, we frame D&D specifically as a dialogue system challenge, where the tasks are to both generate the next conversational turn in the game and predict... | Chris CallisonBurch, Gaurav Singh Tomar, Lara J. Martin, Daphne Ippolito, Suma Bailis, David Reitter |  |
| 1293 |  |  [Unsupervised Entity Linking with Guided Summarization and Multiple-Choice Selection](https://doi.org/10.18653/v1/2022.emnlp-main.638) |  | 0 | Entity linking, the task of linking potentially ambiguous mentions in texts to corresponding knowledge-base entities, is an important component for language understanding. We address two challenge in entity linking: how to leverage wider contexts surrounding a mention, and how to deal with limited... | Young Min Cho, Li Zhang, Chris CallisonBurch |  |
| 1294 |  |  [Weakly-Supervised Temporal Article Grounding](https://doi.org/10.18653/v1/2022.emnlp-main.639) |  | 0 | Given a long untrimmed video and natural language queries, video grounding (VG) aims to temporally localize the semantically-aligned video segments. Almost all existing VG work holds two simple but unrealistic assumptions: 1) All query sentences can be grounded in the corresponding video. 2) All... | Long Chen, Yulei Niu, Brian Chen, Xudong Lin, Guangxing Han, Christopher Thomas, Hammad A. Ayyubi, Heng Ji, ShihFu Chang |  |
| 1295 |  |  [Exploring Dual Encoder Architectures for Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.640) |  | 0 | Dual encoders have been used for question-answering (QA) and information retrieval (IR) tasks with good results. There are two major types of dual encoders, Siamese Dual Encoders (SDE), with parameters shared across two encoders, and Asymmetric Dual Encoder (ADE), with two distinctly parameterized... | Zhe Dong, Jianmo Ni, Dan Bikel, Enrique Alfonseca, Yuan Wang, Chen Qu, Imed Zitouni |  |
| 1296 |  |  [arXivEdits: Understanding the Human Revision Process in Scientific Writing](https://doi.org/10.18653/v1/2022.emnlp-main.641) |  | 0 | Scientific publications are the primary means to communicate research discoveries, where the writing quality is of crucial importance. However, prior work studying the human editing process in this domain mainly focused on the abstract or introduction sections, resulting in an incomplete picture.... | Chao Jiang, Wei Xu, Samuel Stevens |  |
| 1297 |  |  [Why Do You Feel This Way? Summarizing Triggers of Emotions in Social Media Posts](https://doi.org/10.18653/v1/2022.emnlp-main.642) |  | 0 | Crises such as the COVID-19 pandemic continuously threaten our world and emotionally affect billions of people worldwide in distinct ways. Understanding the triggers leading to people’s emotions is of crucial importance. Social media posts can be a good source of such analysis, yet these texts tend... | Hongli Zhan, Tiberiu Sosea, Cornelia Caragea, Junyi Jessy Li |  |
| 1298 |  |  [Analogical Math Word Problems Solving with Enhanced Problem-Solution Association](https://doi.org/10.18653/v1/2022.emnlp-main.643) |  | 0 | Math word problem (MWP) solving is an important task in question answering which requires human-like reasoning ability. Analogical reasoning has long been used in mathematical education, as it enables students to apply common relational structures of mathematical situations to solve new problems.... | Zhenwen Liang, Jipeng Zhang, Xiangliang Zhang |  |
| 1299 |  |  [Towards Teachable Reasoning Systems: Using a Dynamic Memory of User Feedback for Continual System Improvement](https://doi.org/10.18653/v1/2022.emnlp-main.644) |  | 0 | Our goal is a teachable reasoning system for question-answering (QA), where a user can interact with faithful answer explanations, and correct its errors so that the system improves over time. Our approach is to augment a QA model with a dynamic memory of user feedback, containing user-supplied... | Bhavana Dalvi Mishra, Oyvind Tafjord, Peter Clark |  |
| 1300 |  |  [Knowledge Transfer from Answer Ranking to Answer Generation](https://doi.org/10.18653/v1/2022.emnlp-main.645) |  | 0 | Recent studies show that Question Answering (QA) based on Answer Sentence Selection (AS2) can be improved by generating an improved answer from the top-k ranked answer sentences (termed GenQA). This allows for synthesizing the information from multiple candidates into a concise, natural-sounding... | Matteo Gabburo, Rik KoncelKedziorski, Siddhant Garg, Luca Soldaini, Alessandro Moschitti |  |
| 1301 |  |  [Perturbation Augmentation for Fairer NLP](https://doi.org/10.18653/v1/2022.emnlp-main.646) |  | 0 | Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets. In this work, we ask whether training on demographically perturbed data leads to fairer language models. We collect a large dataset of human annotated text perturbations and... | Rebecca Qian, Candace Ross, Jude Fernandes, Eric Michael Smith, Douwe Kiela, Adina Williams |  |
| 1302 |  |  [Automatic Document Selection for Efficient Encoder Pretraining](https://doi.org/10.18653/v1/2022.emnlp-main.647) |  | 0 | Building pretrained language models is considered expensive and data-intensive, but must we increase dataset size to achieve better performance? We propose an alternative to larger training sets by automatically identifying smaller yet domain-representative subsets. We extend Cynical Data... | Yukun Feng, Patrick Xia, Benjamin Van Durme, João Sedoc |  |
| 1303 |  |  [The Aligned Multimodal Movie Treebank: An audio, video, dependency-parse treebank](https://doi.org/10.18653/v1/2022.emnlp-main.648) |  | 0 | Treebanks have traditionally included only text and were derived from written sources such as newspapers or the web. We introduce the Aligned Multimodal Movie Treebank (AMMT), an English language treebank derived from dialog in Hollywood movies which includes transcriptions of the audio-visual... | Adam Uri Yaari, Jan DeWitt, Henry Hu, Bennett Stankovits, Sue Felshin, Yevgeni Berzak, Helena Aparicio, Boris Katz, Ignacio Cases, Andrei Barbu |  |
| 1304 |  |  [DEMETR: Diagnosing Evaluation Metrics for Translation](https://doi.org/10.18653/v1/2022.emnlp-main.649) |  | 0 | While machine translation evaluation metrics based on string overlap (e.g., BLEU) have their limitations, their computations are transparent: the BLEU score assigned to a particular candidate translation can be traced back to the presence or absence of certain words. The operations of newer learned... | Marzena Karpinska, Nishant Raj, Katherine Thai, Yixiao Song, Ankita Gupta, Mohit Iyyer |  |
| 1305 |  |  [Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.650) |  | 0 | Answering open-domain questions requires world knowledge about in-context entities. As pre-trained Language Models (LMs) lack the power to store all required knowledge, external knowledge sources, such as knowledge graphs, are often used to augment LMs. In this work, we propose knOwledge REasOning... | Ziniu Hu, Yichong Xu, Wenhao Yu, Shuohang Wang, Ziyi Yang, Chenguang Zhu, KaiWei Chang, Yizhou Sun |  |
| 1306 |  |  [Debiasing Pretrained Text Encoders by Paying Attention to Paying Attention](https://doi.org/10.18653/v1/2022.emnlp-main.651) |  | 0 | Natural Language Processing (NLP) models are found to exhibit discriminatory stereotypes across many social constructs, e.g. gender and race. In comparison to the progress made in reducing bias from static word embeddings, fairness in sentence-level text encoders received little consideration... | Yacine Gaci, Boualem Benatallah, Fabio Casati, Khalid Benabdeslem |  |
| 1307 |  |  [MEE: A Novel Multilingual Event Extraction Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.652) |  | 0 | Event Extraction (EE) is one of the fundamental tasks in Information Extraction (IE) that aims to recognize event mentions and their arguments (i.e., participants) from text. Due to its importance, extensive methods and resources have been developed for Event Extraction. However, one limitation of... | Amir Pouran Ben Veyseh, Javid Ebrahimi, Franck Dernoncourt, Thien Huu Nguyen |  |
| 1308 |  |  [RobustLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners](https://doi.org/10.18653/v1/2022.emnlp-main.653) |  | 0 | Transformers have been shown to be able to perform deductive reasoning on inputs containing rules and statements written in the English natural language. However, it is unclear if these models indeed follow rigorous logical reasoning to arrive at the prediction or rely on spurious correlation... | Soumya Sanyal, Zeyi Liao, Xiang Ren |  |
| 1309 |  |  [Evaluating and Improving Factuality in Multimodal Abstractive Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.654) |  | 0 | Current metrics for evaluating factuality for abstractive document summarization have achieved high correlations with human judgment, but they do not account for the vision modality and thus are not adequate for vision-and-language summarization. We propose CLIPBERTSCORE, a simple weighted... | David Wan, Mohit Bansal |  |
| 1310 |  |  [Referee: Reference-Free Sentence Summarization with Sharper Controllability through Symbolic Knowledge Distillation](https://doi.org/10.18653/v1/2022.emnlp-main.655) |  | 0 | We present Referee, a novel framework for sentence summarization that can be trained reference-free (i.e., requiring no gold summaries for supervision), while allowing direct control for compression ratio. Our work is the first to demonstrate that reference-free, controlled sentence summarization... | Melanie Sclar, Peter West, Sachin Kumar, Yulia Tsvetkov, Yejin Choi |  |
| 1311 |  |  [Algorithms for Weighted Pushdown Automata](https://doi.org/10.18653/v1/2022.emnlp-main.656) |  | 0 | Weighted pushdown automata (WPDAs) are at the core of many natural language processing tasks, like syntax-based statistical machine translation and transition-based dependency parsing. As most existing dynamic programming algorithms are designed for context-free grammars (CFGs), algorithms for PDAs... | Alexandra Butoi, Brian DuSell, Tim Vieira, Ryan Cotterell, David Chiang |  |
| 1312 |  |  [MABEL: Attenuating Gender Bias using Textual Entailment Data](https://doi.org/10.18653/v1/2022.emnlp-main.657) |  | 0 | Pre-trained language models encode undesirable social biases, which are further exacerbated in downstream use. To this end, we propose MABEL (a Method for Attenuating Gender Bias using Entailment Labels), an intermediate pre-training approach for mitigating gender bias in contextualized... | Jacqueline He, Mengzhou Xia, Christiane Fellbaum, Danqi Chen |  |
| 1313 |  |  [Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs](https://doi.org/10.18653/v1/2022.emnlp-main.658) |  | 0 | Can we teach models designed for language understanding tasks to track and improve their beliefs through intermediate points in text? Besides making their inner workings more transparent, this would also help make models more reliable and consistent. To this end, we propose a representation... | Kyle Richardson, Ronen Tamari, Oren Sultan, Dafna Shahaf, Reut Tsarfaty, Ashish Sabharwal |  |
| 1314 |  |  [Late Fusion with Triplet Margin Objective for Multimodal Ideology Prediction and Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.659) |  | 0 | Prior work on ideology prediction has largely focused on single modalities, i.e., text or images. In this work, we introduce the task of multimodal ideology prediction, where a model predicts binary or five-point scale ideological leanings, given a text-image pair with political content. We first... | Changyuan Qiu, Winston Wu, Xinliang Frederick Zhang, Lu Wang |  |
| 1315 |  |  [Leveraging QA Datasets to Improve Generative Data Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.660) |  | 0 | The ability of generative language models (GLMs) to generate text has improved considerably in the last few years, enabling their use for generative data augmentation. In this work, we propose CONDA, an approach to further improve GLM’s ability to generate synthetic data by reformulating data... | Dheeraj Mekala, Tu Vu, Timo Schick, Jingbo Shang |  |
| 1316 |  |  [Meta-Learning Fast Weight Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.661) |  | 0 | Dynamic evaluation of language models (LMs) adapts model parameters at test time using gradient information from previous tokens and substantially improves LM performance. However, it requires over 3x more compute than standard inference. We present Fast Weight Layers (FWLs), a neural component... | Kevin Clark, Kelvin Guu, MingWei Chang, Panupong Pasupat, Geoffrey E. Hinton, Mohammad Norouzi |  |
| 1317 |  |  [CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations](https://doi.org/10.18653/v1/2022.emnlp-main.662) |  | 0 | Well-designed diagnostic tasks have played a key role in studying the failure of neural nets (NNs) to generalize systematically. Famous examples include SCAN and Compositional Table Lookup (CTL). Here we introduce CTL++, a new diagnostic dataset based on compositions of unary symbolic functions.... | Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber |  |
| 1318 |  |  [Learning with Rejection for Abstractive Text Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.663) |  | 0 | State-of-the-art abstractive summarization systems frequently hallucinate content that is not supported by the source document, mainly due to noise in the training dataset.Existing methods opt to drop the noisy samples or tokens from the training set entirely, reducing the effective training set... | Meng Cao, Yue Dong, Jingyi He, Jackie Chi Kit Cheung |  |
| 1319 |  |  [Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation](https://doi.org/10.18653/v1/2022.emnlp-main.664) |  | 0 | Overconfidence has been shown to impair generalization and calibration of a neural network. Previous studies remedy this issue by adding a regularization term to a loss function, preventing a model from making a peaked distribution. Label smoothing smoothes target labels with a pre-defined prior... | Dongkyu Lee, Ka Chun Cheung, Nevin L. Zhang |  |
| 1320 |  |  [Hard Gate Knowledge Distillation - Leverage Calibration for Robust and Reliable Language Model](https://doi.org/10.18653/v1/2022.emnlp-main.665) |  | 0 | In knowledge distillation, a student model is trained with supervisions from both knowledge from a teacher and observations drawn from a training data distribution. Knowledge of a teacher is considered a subject that holds inter-class relations which send a meaningful supervision to a student;... | Dongkyu Lee, Zhiliang Tian, Yingxiu Zhao, Ka Chun Cheung, Nevin Lianwen Zhang |  |
| 1321 |  |  [Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens](https://doi.org/10.18653/v1/2022.emnlp-main.666) |  | 0 | The term ‘spurious correlations’ has been used in NLP to informally denote any undesirable feature-label correlations. However, a correlation can be undesirable because (i) the feature is irrelevant to the label (e.g. punctuation in a review), or (ii) the feature’s effect on the label depends on... | Nitish Joshi, Xiang Pan, He He |  |
| 1322 |  |  [Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling](https://doi.org/10.18653/v1/2022.emnlp-main.667) |  | 0 | Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content. Recent works focus on correcting factual errors in generated summaries via post-editing. Such correction models are trained using adversarial non-factual summaries constructed... | Vidhisha Balachandran, Hannaneh Hajishirzi, William W. Cohen, Yulia Tsvetkov |  |
| 1323 |  |  [Coordinated Topic Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.668) |  | 0 | We propose a new problem called coordinated topic modeling that imitates human behavior while describing a text corpus. It considers a set of well-defined topics like the axes of a semantic space with a reference representation. It then uses the axes to model a corpus for easily understandable... | Pritom Saha Akash, Jie Huang, Kevin ChenChuan Chang |  |
| 1324 |  |  [Large Dual Encoders Are Generalizable Retrievers](https://doi.org/10.18653/v1/2022.emnlp-main.669) |  | 0 | It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dot-product between a query vector and a passage vector, is too limited... | Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao, Yi Luan, Keith B. Hall, MingWei Chang, Yinfei Yang |  |
| 1325 |  |  [CRIPP-VQA: Counterfactual Reasoning about Implicit Physical Properties via Video Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.670) |  | 0 | Videos often capture objects, their visible properties, their motion, and the interactions between different objects. Objects also have physical properties such as mass, which the imaging pipeline is unable to directly capture. However, these properties can be estimated by utilizing cues from... | Maitreya Patel, Tejas Gokhale, Chitta Baral, Yezhou Yang |  |
| 1326 |  |  [Entity-centered Cross-document Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.671) |  | 0 | Relation Extraction (RE) is a fundamental task of information extraction, which has attracted a large amount of research attention. Previous studies focus on extracting the relations within a sentence or document, while currently researchers begin to explore cross-document RE. However, current... | Fengqi Wang, Fei Li, Hao Fei, Jingye Li, Shengqiong Wu, Fangfang Su, Wenxuan Shi, Donghong Ji, Bo Cai |  |
| 1327 |  |  [Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature](https://doi.org/10.18653/v1/2022.emnlp-main.672) |  | 0 | Literary translation is a culturally significant task, but it is bottlenecked by the small number of qualified literary translators relative to the many untranslated works published around the world. Machine translation (MT) holds potential to complement the work of human translators by improving... | Katherine Thai, Marzena Karpinska, Kalpesh Krishna, Bill Ray, Moira Inghilleri, John Wieting, Mohit Iyyer |  |
| 1328 |  |  [Label-aware Multi-level Contrastive Learning for Cross-lingual Spoken Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.673) |  | 0 | Despite the great success of spoken language understanding (SLU) in high-resource languages, it remains challenging in low-resource languages mainly due to the lack of labeled training data. The recent multilingual code-switching approach achieves better alignments of model representations across... | Shining Liang, Linjun Shou, Jian Pei, Ming Gong, Wanli Zuo, Xianglin Zuo, Daxin Jiang |  |
| 1329 |  |  [Polyglot Prompt: Multilingual Multitask Prompt Training](https://doi.org/10.18653/v1/2022.emnlp-main.674) |  | 0 | This paper aims for a potential architectural improvement for multilingual learning and asks: Can different tasks from different languages be modeled in a monolithic framework, i.e. without any task/language-specific module? The benefit of achieving this could open new doors for future multilingual... | Jinlan Fu, SeeKiong Ng, Pengfei Liu |  |
| 1330 |  |  [VisToT: Vision-Augmented Table-to-Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.675) |  | 0 | Table-to-text generation has been widely studied in the Natural Language Processing community in the recent years. We give a new perspective to this problem by incorporating signals from both tables as well as associated images to generate relevant text. While tables contain a structured list of... | Prajwal Gatti, Anand Mishra, Manish Gupta, Mithun Das Gupta |  |
| 1331 |  |  [Generative Entity-to-Entity Stance Detection with Knowledge Graph Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.676) |  | 0 | Stance detection is typically framed as predicting the sentiment in a given text towards a target entity. However, this setup overlooks the importance of the source entity, i.e., who is expressing the opinion. In this paper, we emphasize the imperative need for studying interactions among entities... | Xinliang Frederick Zhang, Nick Beauchamp, Lu Wang |  |
| 1332 |  |  [Symptom Identification for Interpretable Detection of Multiple Mental Disorders on Social Media](https://doi.org/10.18653/v1/2022.emnlp-main.677) |  | 0 | Mental disease detection (MDD) from social media has suffered from poor generalizability and interpretability, due to lack of symptom modeling. This paper introduces PsySym, the first annotated symptom identification corpus of multiple psychiatric disorders, to facilitate further research progress.... | Zhiling Zhang, Siyuan Chen, Mengyue Wu, Kenny Q. Zhu |  |
| 1333 |  |  [Improving Iterative Text Revision by Learning Where to Edit from Other Revision Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.678) |  | 0 | Iterative text revision improves text quality by fixing grammatical errors, rephrasing for better readability or contextual appropriateness, or reorganizing sentence structures throughout a document.Most recent research has focused on understanding and classifying different types of edits in the... | Zae Myung Kim, Wanyu Du, Vipul Raheja, Dhruv Kumar, Dongyeop Kang |  |
| 1334 |  |  [CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning](https://doi.org/10.18653/v1/2022.emnlp-main.679) |  | 0 | Compared to standard retrieval tasks, passage retrieval for conversational question answering (CQA) poses new challenges in understanding the current user question, as each question needs to be interpreted within the dialogue context. Moreover, it can be expensive to re-train well-established... | Zeqiu Wu, Yi Luan, Hannah Rashkin, David Reitter, Hannaneh Hajishirzi, Mari Ostendorf, Gaurav Singh Tomar |  |
| 1335 |  |  [Specializing Multi-domain NMT via Penalizing Low Mutual Information](https://doi.org/10.18653/v1/2022.emnlp-main.680) |  | 0 | Multi-domain Neural Machine Translation (NMT) trains a single model with multiple domains. It is appealing because of its efficacy in handling multiple domains within one model. An ideal multi-domain NMT learns distinctive domain characteristics simultaneously, however, grasping the domain... | Jiyoung Lee, Hantae Kim, Hyunchang Cho, Edward Choi, Cheonbok Park |  |
| 1336 |  |  [A Simple Contrastive Learning Framework for Interactive Argument Pair Identification via Argument-Context Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.681) |  | 0 | Interactive argument pair identification is an emerging research task for argument mining, aiming to identify whether two arguments are interactively related. It is pointed out that the context of the argument is essential to improve identification performance. However, current context-based... | Lida Shi, Fausto Giunchiglia, Rui Song, Daqian Shi, Tongtong Liu, Xiaolei Diao, Hao Xu |  |
| 1337 |  |  [Sentence-level Media Bias Analysis Informed by Discourse Structures](https://doi.org/10.18653/v1/2022.emnlp-main.682) |  | 0 | As polarization continues to rise among both the public and the news media, increasing attention has been devoted to detecting media bias. Most recent work in the NLP community, however, identify bias at the level of individual articles. However, each article itself comprises multiple sentences,... | Yuanyuan Lei, Ruihong Huang, Lu Wang, Nick Beauchamp |  |
| 1338 |  |  [Towards Efficient Dialogue Pre-training with Transferable and Interpretable Latent Structure](https://doi.org/10.18653/v1/2022.emnlp-main.683) |  | 0 | With the availability of massive general-domain dialogue data, pre-trained dialogue generation appears to be super appealing to transfer knowledge from the general domain to downstream applications. In most existing work, such transferable ability is mainly obtained by fitting a large model with... | Xueliang Zhao, Lemao Liu, Tingchen Fu, Shuming Shi, Dongyan Zhao, Rui Yan |  |
| 1339 |  |  [An Empirical Revisiting of Linguistic Knowledge Fusion in Language Understanding Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.684) |  | 0 | Though linguistic knowledge emerges during large-scale language model pretraining, recent work attempt to explicitly incorporate human-defined linguistic priors into task-specific fine-tuning. Infusing language models with syntactic or semantic knowledge from parsers has shown improvements on many... | Changlong Yu, Tianyi Xiao, Lingpeng Kong, Yangqiu Song, Wilfred Ng |  |
| 1340 |  |  [Unsupervised Non-transferable Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.685) |  | 0 | Training a good deep learning model requires substantial data and computing resources, which makes the resulting neural model a valuable intellectual property. To prevent the neural network from being undesirably exploited, non-transferable learning has been proposed to reduce the model... | Guangtao Zeng, Wei Lu |  |
| 1341 |  |  [Adaptive Contrastive Learning on Multimodal Transformer for Review Helpfulness Prediction](https://doi.org/10.18653/v1/2022.emnlp-main.686) |  | 0 | Modern Review Helpfulness Prediction systems are dependent upon multiple modalities, typically texts and images. Unfortunately, those contemporary approaches pay scarce attention to polish representations of cross-modal relations and tend to suffer from inferior optimization. This might cause harm... | Thong Nguyen, Xiaobao Wu, Anh Tuan Luu, Zhen Hai, Lidong Bing |  |
| 1342 |  |  [Adaptive Token-level Cross-lingual Feature Mixing for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.687) |  | 0 | Multilingual neural machine translation aims to translate multiple language pairs in a single model and has shown great success thanks to the knowledge transfer across languages with the shared parameters. Despite promising, this share-all paradigm suffers from insufficient ability to capture... | Junpeng Liu, Kaiyu Huang, Jiuyi Li, Huan Liu, Jinsong Su, Degen Huang |  |
| 1343 |  |  [A Dataset for Hyper-Relational Extraction and a Cube-Filling Approach](https://doi.org/10.18653/v1/2022.emnlp-main.688) |  | 0 | Relation extraction has the potential for large-scale knowledge graph construction, but current methods do not consider the qualifier attributes for each relation triplet, such as time, quantity or location. The qualifiers form hyper-relational facts which better capture the rich and complex... | Yew Ken Chia, Lidong Bing, Sharifah Mahani Aljunied, Luo Si, Soujanya Poria |  |
| 1344 |  |  [Low-resource Neural Machine Translation with Cross-modal Alignment](https://doi.org/10.18653/v1/2022.emnlp-main.689) |  | 0 | How to achieve neural machine translation with limited parallel data? Existing techniques often rely on large-scale monolingual corpus, which is impractical for some low-resource languages. In this paper, we turn to connect several low-resource languages to a particular high-resource one by... | Zhe Yang, Qingkai Fang, Yang Feng |  |
| 1345 |  |  [Prompt-based Distribution Alignment for Domain Generalization in Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.690) |  | 0 | Prompt-based learning (a.k.a. prompting) achieves high performance by bridging the gap between the objectives of language modeling and downstream tasks. Domain generalization ability can be improved by prompting since classification across different domains can be unified into the prediction of the... | Chen Jia, Yue Zhang |  |
| 1346 |  |  [Two is Better than Many? Binary Classification as an Effective Approach to Multi-Choice Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.691) |  | 0 | We propose a simple refactoring of multi-choice question answering (MCQA) tasks as a series of binary classifications. The MCQA task is generally performed by scoring each (question, answer) pair normalized over all the pairs, and then selecting the answer from the pair that yield the highest... | Deepanway Ghosal, Navonil Majumder, Rada Mihalcea, Soujanya Poria |  |
| 1347 |  |  [HEGEL: Hypergraph Transformer for Long Document Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.692) |  | 0 | Extractive summarization for long documents is challenging due to the extended structured input context. The long-distance sentence dependency hinders cross-sentence relations modeling, the critical step of extractive summarization. This paper proposes HEGEL, a hypergraph neural network for long... | Haopeng Zhang, Xiao Liu, Jiawei Zhang |  |
| 1348 |  |  [Adapting a Language Model While Preserving its General Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.693) |  | 0 | Domain-adaptive pre-training (or DA-training for short), also known as post-training, aimsto train a pre-trained general-purpose language model (LM) using an unlabeled corpus of aparticular domain to adapt the LM so that end-tasks in the domain can give improved performances. However, existing... | Zixuan Ke, Yijia Shao, Haowei Lin, Hu Xu, Lei Shu, Bing Liu |  |
| 1349 |  |  [Human Guided Exploitation of Interpretable Attention Patterns in Summarization and Topic Segmentation](https://doi.org/10.18653/v1/2022.emnlp-main.694) |  | 0 | The multi-head self-attention mechanism of the transformer model has been thoroughly investigated recently. In one vein of study, researchers are interested in understanding why and how transformers work. In another vein, researchers propose new attention augmentation methods to make transformers... | Raymond Li, Wen Xiao, Linzi Xing, Lanjun Wang, Gabriel Murray, Giuseppe Carenini |  |
| 1350 |  |  [Continual Training of Language Models for Few-Shot Learning](https://doi.org/10.18653/v1/2022.emnlp-main.695) |  | 0 | Recent work on applying large language models (LMs) achieves impressive performance in many NLP applications. Adapting or posttraining an LM using an unlabeled domain corpus can produce even better performance for end-tasks in the domain. This paper proposes the problem of continually extending an... | Zixuan Ke, Haowei Lin, Yijia Shao, Hu Xu, Lei Shu, Bing Liu |  |
| 1351 |  |  [Dictionary-Assisted Supervised Contrastive Learning](https://doi.org/10.18653/v1/2022.emnlp-main.696) |  | 0 | Text analysis in the social sciences often involves using specialized dictionaries to reason with abstract concepts, such as perceptions about the economy or abuse on social media. These dictionaries allow researchers to impart domain knowledge and note subtle usages of words relating to a... | Patrick Y. Wu, Richard Bonneau, Joshua A. Tucker, Jonathan Nagler |  |
| 1352 |  |  [Fine-Tuning Pre-trained Transformers into Decaying Fast Weights](https://doi.org/10.18653/v1/2022.emnlp-main.697) |  | 0 | Autoregressive Transformers are strong language models but incur O(T) complexity during per-token generation due to the self-attention mechanism. Recent work proposes kernel-based methods to approximate causal self-attention by replacing it with recurrent formulations with various update rules and... | Huanru Henry Mao |  |
| 1353 |  |  [PRO-CS : An Instance-Based Prompt Composition Technique for Code-Switched Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.698) |  | 0 | Code-switched (CS) data is ubiquitous in today’s globalized world, but the dearth of annotated datasets in code-switching poses a significant challenge for learning diverse tasks across different language pairs. Parameter-efficient prompt-tuning approaches conditioned on frozen language models have... | Srijan Bansal, Suraj Tripathi, Sumit Agarwal, Teruko Mitamura, Eric Nyberg |  |
| 1354 |  |  [SentBS: Sentence-level Beam Search for Controllable Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.699) |  | 0 | A wide range of control perspectives have been explored in controllable text generation. Structure-controlled summarization is recently proposed as a useful and interesting research direction. However, current structure-controlling methods have limited effectiveness in enforcing the desired... | Chenhui Shen, Liying Cheng, Lidong Bing, Yang You, Luo Si |  |
| 1355 |  |  [A Fine-grained Chinese Software Privacy Policy Dataset for Sequence Labeling and Regulation Compliant Identification](https://doi.org/10.18653/v1/2022.emnlp-main.700) |  | 0 | Privacy protection raises great attention on both legal levels and user awareness. To protect user privacy, countries enact laws and regulations requiring software privacy policies to regulate their behavior. However, privacy policies are written in professional languages with many legal terms and... | Kaifa Zhao, Le Yu, Shiyao Zhou, Jing Li, Xiapu Luo, Aemon Yat Fei Chiu, Yutong Liu |  |
| 1356 |  |  [Saving Dense Retriever from Shortcut Dependency in Conversational Search](https://doi.org/10.18653/v1/2022.emnlp-main.701) |  | 0 | Conversational search (CS) needs a holistic understanding of conversational inputs to retrieve relevant passages. In this paper, we demonstrate the existence of a retrieval shortcut in CS, which causes models to retrieve passages solely relying on partial history while disregarding the latest... | Sungdong Kim, Gangwoo Kim |  |
| 1357 |  |  [Graph-Induced Transformers for Efficient Multi-Hop Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.702) |  | 0 | A graph is a suitable data structure to represent the structural information of text. Recently, multi-hop question answering (MHQA) tasks, which require inter-paragraph/sentence linkages, have come to exploit such properties of a graph. Previous approaches to MHQA relied on leveraging the graph... | Giwon Hong, Jeonghwan Kim, Junmo Kang, SungHyon Myaeng |  |
| 1358 |  |  [DiscoSense: Commonsense Reasoning with Discourse Connectives](https://doi.org/10.18653/v1/2022.emnlp-main.703) |  | 0 | We present DiscoSense, a benchmark for commonsense reasoning via understanding a wide variety of discourse connectives. We generate compelling distractors in DiscoSense using Conditional Adversarial Filtering, an extension of Adversarial Filtering that employs conditional generation. We show that... | Prajjwal Bhargava, Vincent Ng |  |
| 1359 |  |  [Boosting Document-Level Relation Extraction by Mining and Injecting Logical Rules](https://doi.org/10.18653/v1/2022.emnlp-main.704) |  | 0 | Document-level relation extraction (DocRE) aims at extracting relations of all entity pairs in a document. A key challenge to DocRE lies in the complex interdependency between the relations of entity pairs. Unlike most prior efforts focusing on implicitly powerful representations, the recently... | Shengda Fan, Shasha Mo, Jianwei Niu |  |
| 1360 |  |  [MOCHA: A Multi-Task Training Approach for Coherent Text Generation from Cognitive Perspective](https://doi.org/10.18653/v1/2022.emnlp-main.705) |  | 0 | Teaching neural models to generate narrative coherent texts is a critical problem. Recent pre-trained language models have achieved promising results, but there is still a gap between human written texts and machine-generated outputs. In this work, we propose a novel multi-task training strategy... | Zhe Hu, Hou Pong Chan, Lifu Huang |  |
| 1361 |  |  [Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation](https://doi.org/10.18653/v1/2022.emnlp-main.706) |  | 0 | In this paper, we propose a variational autoencoder with disentanglement priors, VAE-Dprior, for task-specific natural language generation with none or a handful of task-specific labeled examples. In order to tackle compositional generalization across tasks, our model performs disentangled... | Zhuang Li, Lizhen Qu, Qiongkai Xu, Tongtong Wu, Tianyang Zhan, Gholamreza Haffari |  |
| 1362 |  |  [CISLR: Corpus for Indian Sign Language Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.707) |  | 0 | Indian Sign Language, though used by a diverse community, still lacks well-annotated resources for developing systems that would enable sign language processing. In recent years researchers have actively worked for sign languages like American Sign Languages, however, Indian Sign language is still... | Abhinav Joshi, Ashwani Bhat, Pradeep S, Priya Gole, Shashwat Gupta, Shreyansh Agarwal, Ashutosh Modi |  |
| 1363 |  |  [Mask the Correct Tokens: An Embarrassingly Simple Approach for Error Correction](https://doi.org/10.18653/v1/2022.emnlp-main.708) |  | 0 | Text error correction aims to correct the errors in text sequences such as those typed by humans or generated by speech recognition models.Previous error correction methods usually take the source (incorrect) sentence as encoder input and generate the target (correct) sentence through the decoder.... | Kai Shen, Yichong Leng, Xu Tan, Siliang Tang, Yuan Zhang, Wenjie Liu, Edward Lin |  |
| 1364 |  |  [AMAL: Meta Knowledge-Driven Few-Shot Adapter Learning](https://doi.org/10.18653/v1/2022.emnlp-main.709) |  | 0 | NLP has advanced greatly together with the proliferation of Transformer-based pre-trained language models. To adapt to a downstream task, the pre-trained language models need to be fine-tuned with a sufficient supply of annotated examples. In recent years, Adapter-based fine-tuning methods have... | S. K. Hong, Tae Young Jang |  |
| 1365 |  |  [Discourse Context Predictability Effects in Hindi Word Order](https://doi.org/10.18653/v1/2022.emnlp-main.710) |  | 0 | We test the hypothesis that discourse predictability influences Hindi syntactic choice. While prior work has shown that a number of factors (e.g., information status, dependency length, and syntactic surprisal) influence Hindi word order preferences, the role of discourse predictability is... | Sidharth Ranjan, Marten van Schijndel, Sumeet Agarwal, Rajakrishnan Rajkumar |  |
| 1366 |  |  ["Covid vaccine is against Covid but Oxford vaccine is made at Oxford!" Semantic Interpretation of Proper Noun Compounds](https://doi.org/10.18653/v1/2022.emnlp-main.711) |  | 0 | Proper noun compounds, e.g., “Covid vaccine”, convey information in a succinct manner (a “Covid vaccine” is a “vaccine that immunizes against the Covid disease”). These are commonly used in short-form domains, such as news headlines, but are largely ignored in information-seeking applications. To... | Keshav Kolluru, Gabriel Stanovsky, Mausam |  |
| 1367 |  |  [Context Limitations Make Neural Language Models More Human-Like](https://doi.org/10.18653/v1/2022.emnlp-main.712) |  | 0 | Language models (LMs) have been used in cognitive modeling as well as engineering studies—they compute information-theoretic complexity metrics that simulate humans’ cognitive load during reading.This study highlights a limitation of modern neural LMs as the model of choice for this purpose: there... | Tatsuki Kuribayashi, Yohei Oseki, Ana Brassard, Kentaro Inui |  |
| 1368 |  |  [A Generative Model for End-to-End Argument Mining with Reconstructed Positional Encoding and Constrained Pointer Mechanism](https://doi.org/10.18653/v1/2022.emnlp-main.713) |  | 0 | Argument mining (AM) is a challenging task as it requires recognizing the complex argumentation structures involving multiple subtasks.To handle all subtasks of AM in an end-to-end fashion, previous works generally transform AM into a dependency parsing task.However, such methods largely require... | Jianzhu Bao, Yuhang He, Yang Sun, Bin Liang, Jiachen Du, Bing Qin, Min Yang, Ruifeng Xu |  |
| 1369 |  |  [Reflect, Not Reflex: Inference-Based Common Ground Improves Dialogue Response Quality](https://doi.org/10.18653/v1/2022.emnlp-main.714) |  | 0 | Human communication relies on common ground (CG), the mutual knowledge and beliefs shared by participants, to produce coherent and interesting conversations. In this paper, we demonstrate that current response generation (RG) models produce generic and dull responses in dialogues because they act... | Pei Zhou, Hyundong Cho, Pegah Jandaghi, DongHo Lee, Bill Yuchen Lin, Jay Pujara, Xiang Ren |  |
| 1370 |  |  [FlowEval: A Consensus-Based Dialogue Evaluation Framework Using Segment Act Flows](https://doi.org/10.18653/v1/2022.emnlp-main.715) |  | 0 | Despite recent progress in open-domain dialogue evaluation, how to develop automatic metrics remains an open problem. We explore the potential of dialogue evaluation featuring dialog act information, which was hardly explicitly modeled in previous methods. However, defined at the utterance level in... | Jianqiao Zhao, Yanyang Li, Wanyu Du, Yangfeng Ji, Dong Yu, Michael R. Lyu, Liwei Wang |  |
| 1371 |  |  [FaD-VLP: Fashion Vision-and-Language Pre-training towards Unified Retrieval and Captioning](https://doi.org/10.18653/v1/2022.emnlp-main.716) |  | 0 | Multimodal tasks in the fashion domain have significant potential for e-commerce, but involve challenging vision-and-language learning problems—e.g., retrieving a fashion item given a reference image plus text feedback from a user. Prior works on multimodal fashion tasks have either been limited by... | Suvir Mirchandani, Licheng Yu, Mengjiao Wang, Animesh Sinha, Wenwen Jiang, Tao Xiang, Ning Zhang |  |
| 1372 |  |  [MM-Align: Learning Optimal Transport-based Alignment Dynamics for Fast and Accurate Inference on Missing Modality Sequences](https://doi.org/10.18653/v1/2022.emnlp-main.717) |  | 0 | Existing multimodal tasks mostly target at the complete input modality setting, i.e., each modality is either complete or completely missing in both training and test sets. However, the randomly missing situations have still been underexplored. In this paper, we present a novel approach named... | Wei Han, Hui Chen, MinYen Kan, Soujanya Poria |  |
| 1373 |  |  [Evaluating the Knowledge Dependency of Questions](https://doi.org/10.18653/v1/2022.emnlp-main.718) |  | 0 | The automatic generation of Multiple Choice Questions (MCQ) has the potential to reduce the time educators spend on student assessment significantly. However, existing evaluation metrics for MCQ generation, such as BLEU, ROUGE, and METEOR, focus on the n-gram based similarity of the generated MCQ... | Hyeongdon Moon, Yoonseok Yang, Hangyeol Yu, Seunghyun Lee, Myeongho Jeong, Juneyoung Park, Jamin Shin, Minsam Kim, Seungtaek Choi |  |
| 1374 |  |  [MoSE: Modality Split and Ensemble for Multimodal Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.emnlp-main.719) |  | 0 | Multimodal knowledge graph completion (MKGC) aims to predict missing entities in MKGs. Previous works usually share relation representation across modalities. This results in mutual interference between modalities during training, since for a pair of entities, the relation from one modality... | Yu Zhao, Xiangrui Cai, Yike Wu, Haiwei Zhang, Ying Zhang, Guoqing Zhao, Ning Jiang |  |
| 1375 |  |  [Entropy-Based Vocabulary Substitution for Incremental Learning in Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.720) |  | 0 | In a practical real-world scenario, the longstanding goal is that a universal multilingual translation model can be incrementally updated when new language pairs arrive. Specifically, the initial vocabulary only covers some of the words in new languages, which hurts the translation quality for... | Kaiyu Huang, Peng Li, Jin Ma, Yang Liu |  |
| 1376 |  |  [Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation](https://doi.org/10.18653/v1/2022.emnlp-main.721) |  | 0 | Recent advances in large-scale pre-training provide large models with the potential to learn knowledge from the raw text. It is thus natural to ask whether it is possible to leverage these large models as knowledge bases for downstream tasks. In this work, we answer the aforementioned question in... | Yanyang Li, Jianqiao Zhao, Michael R. Lyu, Liwei Wang |  |
| 1377 |  |  [An Unsupervised, Geometric and Syntax-aware Quantification of Polysemy](https://doi.org/10.18653/v1/2022.emnlp-main.722) |  | 0 | Polysemy is the phenomenon where a single word form possesses two or more related senses. It is an extremely ubiquitous part of natural language and analyzing it has sparked rich discussions in the linguistics, psychology and philosophy communities alike. With scarce attention paid to polysemy in... | Anmol Goel, Charu Sharma, Ponnurangam Kumaraguru |  |
| 1378 |  |  [Reorder and then Parse, Fast and Accurate Discontinuous Constituency Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.723) |  | 0 | Discontinuous constituency parsing is still kept developing for its efficiency and accuracy are far behind its continuous counterparts. Motivated by the observation that a discontinuous constituent tree can be simply transformed into a pseudo-continuous one by artificially reordering words in the... | Kailai Sun, Zuchao Li, Hai Zhao |  |
| 1379 |  |  [Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature](https://doi.org/10.18653/v1/2022.emnlp-main.724) |  | 0 | Lay summarisation aims to jointly summarise and simplify a given text, thus making its content more comprehensible to non-experts.Automatic approaches for lay summarisation can provide significant value in broadening access to scientific literature, enabling a greater degree of both... | Tomas Goldsack, Zhihao Zhang, Chenghua Lin, Carolina Scarton |  |
| 1380 |  |  [Looking at the Overlooked: An Analysis on the Word-Overlap Bias in Natural Language Inference](https://doi.org/10.18653/v1/2022.emnlp-main.725) |  | 0 | It has been shown that NLI models are usually biased with respect to the word-overlap between the premise and the hypothesis, as they take this feature as a primary cue for predicting the entailment label. In this paper, we focus on an overlooked aspect of the overlap bias in the NLI models: the... | Sara Rajaee, Yadollah Yaghoobzadeh, Mohammad Taher Pilehvar |  |
| 1381 |  |  [An Empirical Study on the Transferability of Transformer Modules in Parameter-efficient Fine-tuning](https://doi.org/10.18653/v1/2022.emnlp-main.726) |  | 0 | Parameter-efficient fine-tuning has garnered lots of attention in recent studies.On this subject, we investigate the capability of different transformer modules in transferring knowledge from a pre-trained model to a downstream task. Our empirical results suggest that every transformer module is a... | Mohammad AkbarTajari, Sara Rajaee, Mohammad Taher Pilehvar |  |
| 1382 |  |  [CODER: An efficient framework for improving retrieval through COntextual Document Embedding Reranking](https://doi.org/10.18653/v1/2022.emnlp-main.727) |  | 0 | Contrastive learning has been the dominant approach to training dense retrieval models. In this work, we investigate the impact of ranking context - an often overlooked aspect of learning dense retrieval models. In particular, we examine the effect of its constituent parts: jointly scoring a large... | George Zerveas, Navid Rekabsaz, Daniel Cohen, Carsten Eickhoff |  |
| 1383 |  |  [AdapterShare: Task Correlation Modeling with Adapter Differentiation](https://doi.org/10.18653/v1/2022.emnlp-main.728) |  | 0 | Thanks to the development of pre-trained language models, multitask learning (MTL) methods achieve a great success in natural language understanding area.However, current MTL methods pay more attention to task selection or model design to fuse as much knowledge as possible, while intrinsic task... | Zhi Chen, Bei Chen, Lu Chen, Kai Yu, JianGuang Lou |  |
| 1384 |  |  [Rethinking Task-Specific Knowledge Distillation: Contextualized Corpus as Better Textbook](https://doi.org/10.18653/v1/2022.emnlp-main.729) |  | 0 | Knowledge distillation has been proven effective when customizing small language models for specific tasks. Here, a corpus as ‘textbook’ plays an indispensable role, only through which the teacher can teach the student. Prevailing methods adopt a two-stage distillation paradigm: general... | Chang Liu, Chongyang Tao, Jianxin Liang, Tao Shen, Jiazhan Feng, Quzhe Huang, Dongyan Zhao |  |
| 1385 |  |  [Recovering Gold from Black Sand: Multilingual Dense Passage Retrieval with Hard and False Negative Samples](https://doi.org/10.18653/v1/2022.emnlp-main.730) |  | 0 | Negative samples have not been efficiently explored in multilingual dense passage retrieval. In this paper, we propose a novel multilingual dense passage retrieval framework, mHFN, to recover and utilize hard and false negative samples. mHFN consists of three key components: 1) a multilingual hard... | Tianhao Shen, Mingtong Liu, Ming Zhou, Deyi Xiong |  |
| 1386 |  |  [The "Problem" of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.731) |  | 0 | Human variation in labeling is often considered noise. Annotation projects for machine learning (ML) aim at minimizing human label variation, with the assumption to maximize data quality and in turn optimize and maximize machine learning metrics. However, thisconventional practice assumes that... | Barbara Plank |  |
| 1387 |  |  [Quality Scoring of Source Words in Neural Translation Models](https://doi.org/10.18653/v1/2022.emnlp-main.732) |  | 0 | Word-level quality scores on input source sentences can provide useful feedback to an end-user when translating into an unfamiliar target language. Recent approaches either require training special word-scoring models based on synthetic data or require repeated invocation of the translation model.... | Priyesh Jain, Sunita Sarawagi, Tushar Tomar |  |
| 1388 |  |  [Pneg: Prompt-based Negative Response Generation for Dialogue Response Selection Task](https://doi.org/10.18653/v1/2022.emnlp-main.733) |  | 0 | In retrieval-based dialogue systems, a response selection model acts as a ranker to select the most appropriate response among several candidates. However, such selection models tend to rely on context-response content similarity, which makes models vulnerable to adversarial responses that are... | Nyoungwoo Lee, ChaeHun Park, HoJin Choi, Jaegul Choo |  |
| 1389 |  |  [Facilitating Contrastive Learning of Discourse Relational Senses by Exploiting the Hierarchy of Sense Relations](https://doi.org/10.18653/v1/2022.emnlp-main.734) |  | 0 | Implicit discourse relation recognition is a challenging task that involves identifying the sense or senses that hold between two adjacent spans of text, in the absense of an explicit connective between them. In both PDTB-2 (prasad et al., 2008) and PDTB-3 (Webber et al., 2019), discourse... | Wanqiu Long, Bonnie Webber |  |
| 1390 |  |  [Simplified Graph Learning for Inductive Short Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.735) |  | 0 | Short text classification (STC) is hard as short texts lack context information and labeled data is not enough. Graph neural networks obtain the state-of-the-art on STC since they can merge various auxiliary information via the message passing framework. However, existing works conduct transductive... | Kaixin Zheng, Yaqing Wang, Quanming Yao, Dejing Dou |  |
| 1391 |  |  [Don't Stop Fine-Tuning: On Training Regimes for Few-Shot Cross-Lingual Transfer with Multilingual Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.736) |  | 0 | A large body of recent work highlights the fallacies of zero-shot cross-lingual transfer (ZS-XLT) with large multilingual language models. Namely, their performance varies substantially for different target languages and is the weakest where needed the most: for low-resource languages distant to... | Fabian David Schmidt, Ivan Vulic, Goran Glavas |  |
| 1392 |  |  [Towards Compositional Generalization in Code Search](https://doi.org/10.18653/v1/2022.emnlp-main.737) |  | 0 | We study compositional generalization, which aims to generalize on unseen combinations of seen structural elements, for code search. Unlike existing approaches of partially pursuing this goal, we study how to extract structural elements, which we name a template that directly targets compositional... | Hojae Han, Seungwon Hwang, Shuai Lu, Nan Duan, Seungtaek Choi |  |
| 1393 |  |  [Towards relation extraction from speech](https://doi.org/10.18653/v1/2022.emnlp-main.738) |  | 0 | Relation extraction typically aims to extract semantic relationships between entities from the unstructured text.One of the most essential data sources for relation extraction is the spoken language, such as interviews and dialogues.However, the error propagation introduced in automatic speech... | Tongtong Wu, Guitao Wang, Jinming Zhao, Zhaoran Liu, Guilin Qi, YuanFang Li, Gholamreza Haffari |  |
| 1394 |  |  [Structural Constraints and Natural Language Inference for End-to-End Flowchart Grounded Dialog Response Generation](https://doi.org/10.18653/v1/2022.emnlp-main.739) |  | 0 | Flowchart grounded dialog systems converse with users by following a given flowchart and a corpus of FAQs. The existing state-of-the-art approach (Raghu et al, 2021) for learning such a dialog system, named FLONET, has two main limitations. (1) It uses a Retrieval Augmented Generation (RAG)... | Dinesh Raghu, Suraj Joshi, Sachindra Joshi, Mausam |  |
| 1395 |  |  [SLICER: Sliced Fine-Tuning for Low-Resource Cross-Lingual Transfer for Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.740) |  | 0 | Large multilingual language models generally demonstrate impressive results in zero-shot cross-lingual transfer, yet often fail to successfully transfer to low-resource languages, even for token-level prediction tasks like named entity recognition (NER). In this work, we introduce a simple yet... | Fabian David Schmidt, Ivan Vulic, Goran Glavas |  |
| 1396 |  |  [EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation](https://doi.org/10.18653/v1/2022.emnlp-main.741) |  | 0 | We introduce EdgeFormer – a parameter-efficient Transformer for on-device seq2seq generation under the strict computation and memory constraints. Compared with the previous parameter-efficient Transformers, EdgeFormer applies two novel principles for cost-effective parameterization, allowing it to... | Tao Ge, SiQing Chen, Furu Wei |  |
| 1397 |  |  [End-to-End Unsupervised Vision-and-Language Pre-training with Referring Expression Matching](https://doi.org/10.18653/v1/2022.emnlp-main.742) |  | 0 | Recently there has been an emerging interest in unsupervised vision-and-language pre-training (VLP) that learns multimodal representations without parallel image-caption data. These pioneering works significantly reduce the cost of VLP on data collection and achieve promising results compared to... | Chi Chen, Peng Li, Maosong Sun, Yang Liu |  |
| 1398 |  |  [Faithful Knowledge Graph Explanations in Commonsense Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.743) |  | 0 | Knowledge graphs are commonly used as sources of information in commonsense question answering, and can also be used to express explanations for the model’s answer choice. A common way of incorporating facts from the graph is to encode them separately from the question, and then combine the two... | Guy Aglionby, Simone Teufel |  |
| 1399 |  |  [KOLD: Korean Offensive Language Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.744) |  | 0 | Recent directions for offensive language detection are hierarchical modeling, identifying the type and the target of offensive language, and interpretability with offensive span annotation and prediction. These improvements are focused on English and do not transfer well to other languages because... | Younghoon Jeong, Juhyun Oh, Jongwon Lee, Jaimeen Ahn, Jihyung Moon, Sungjoon Park, Alice Oh |  |
| 1400 |  |  [Evade the Trap of Mediocrity: Promoting Diversity and Novelty in Text Generation via Concentrating Attention](https://doi.org/10.18653/v1/2022.emnlp-main.745) |  | 0 | Recently, powerful Transformer architectures have proven superior in generating high-quality sentences. Nevertheless, these models tend to produce dull high-frequency phrases, severely hurting the diversity and novelty of generated text. In this work, we dig into the intrinsic mechanism of this... | Wenhao Li, Xiaoyuan Yi, Jinyi Hu, Maosong Sun, Xing Xie |  |
| 1401 |  |  [The better your Syntax, the better your Semantics? Probing Pretrained Language Models for the English Comparative Correlative](https://doi.org/10.18653/v1/2022.emnlp-main.746) |  | 0 | Construction Grammar (CxG) is a paradigm from cognitive linguistics emphasising the connection between syntax and semantics. Rather than rules that operate on lexical items, it posits constructions as the central building blocks of language, i.e., linguistic units of different granularity that... | Leonie Weissweiler, Valentin Hofmann, Abdullatif Köksal, Hinrich Schütze |  |
| 1402 |  |  [ProofInfer: Generating Proof via Iterative Hierarchical Inference](https://doi.org/10.18653/v1/2022.emnlp-main.747) |  | 0 | Proof generation focuses on deductive reasoning: given a hypothesis and a set of theories, including some supporting facts and logical rules expressed in natural language, the model generates a proof tree indicating how to deduce the hypothesis from given theories.Current models with... | Zichu Fei, Qi Zhang, Xin Zhou, Tao Gui, Xuanjing Huang |  |
| 1403 |  |  [ECTSum: A New Benchmark Dataset For Bullet Point Summarization of Long Earnings Call Transcripts](https://doi.org/10.18653/v1/2022.emnlp-main.748) |  | 0 | Despite tremendous progress in automatic summarization, state-of-the-art methods are predominantly trained to excel in summarizing short newswire articles, or documents with strong layout biases such as scientific articles or government reports. Efficient techniques to summarize financial... | Rajdeep Mukherjee, Abhinav Bohra, Akash Banerjee, Soumya Sharma, Manjunath Hegde, Afreen Shaikh, Shivani Shrivastava, Koustuv Dasgupta, Niloy Ganguly, Saptarshi Ghosh, Pawan Goyal |  |
| 1404 |  |  [Cross-domain Generalization for AMR Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.749) |  | 0 | Abstract Meaning Representation (AMR) parsing aims to predict an AMR graph from textual input. Recently, there has been notable growth in AMR parsing performance. However, most existing work focuses on improving the performance in the specific domain, ignoring the potential domain dependence of AMR... | Xuefeng Bai, Sen Yang, Leyang Cui, Linfeng Song, Yue Zhang |  |
| 1405 |  |  [CiteSum: Citation Text-guided Scientific Extreme Summarization and Domain Adaptation with Limited Supervision](https://doi.org/10.18653/v1/2022.emnlp-main.750) |  | 0 | Scientific extreme summarization (TLDR) aims to form ultra-short summaries of scientific papers. Previous efforts on curating scientific TLDR datasets failed to scale up due to the heavy human annotation and domain expertise required. In this paper, we propose a simple yet effective approach to... | Yuning Mao, Ming Zhong, Jiawei Han |  |
| 1406 |  |  [FETA: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue](https://doi.org/10.18653/v1/2022.emnlp-main.751) |  | 0 | Task transfer, transferring knowledge contained in related tasks, holds the promise of reducing the quantity of labeled data required to fine-tune language models. Dialogue understanding encompasses many diverse tasks, yet task transfer has not been thoroughly studied in conversational AI. This... | Alon Albalak, YiLin Tuan, Pegah Jandaghi, Connor Pryor, Luke Yoffe, Deepak Ramachandran, Lise Getoor, Jay Pujara, William Yang Wang |  |
| 1407 |  |  [Do Children Texts Hold The Key To Commonsense Knowledge?](https://doi.org/10.18653/v1/2022.emnlp-main.752) |  | 0 | Compiling comprehensive repositories of commonsense knowledge is a long-standing problem in AI. Many concerns revolve around the issue of reporting bias, i.e., that frequency in text sources is not a good proxy for relevance or truth. This paper explores whether children’s texts hold the key to... | Julien Romero, Simon Razniewski |  |
| 1408 |  |  [On the Limitations of Reference-Free Evaluations of Generated Text](https://doi.org/10.18653/v1/2022.emnlp-main.753) |  | 0 | There is significant interest in developing evaluation metrics which accurately estimate the quality of generated text without the aid of a human-written reference text, which can be time consuming and expensive to collect or entirely unavailable in online applications. However, in this work, we... | Daniel Deutsch, Rotem Dror, Dan Roth |  |
| 1409 |  |  [Sampling-Based Approximations to Minimum Bayes Risk Decoding for Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.754) |  | 0 | In NMT we search for the mode of the model distribution to form predictions. The mode and other high-probability translations found by beam search have been shown to often be inadequate in a number of ways. This prevents improving translation quality through better search, as these idiosyncratic... | Bryan Eikema, Wilker Aziz |  |
| 1410 |  |  [IndicXNLI: Evaluating Multilingual Inference for Indian Languages](https://doi.org/10.18653/v1/2022.emnlp-main.755) |  | 0 | While Indic NLP has made rapid advances recently in terms of the availability of corpora and pre-trained models, benchmark datasets on standard NLU tasks are limited. To this end, we introduce INDICXNLI, an NLI dataset for 11 Indic languages. It has been created by high-quality machine translation... | Divyanshu Aggarwal, Vivek Gupta, Anoop Kunchukuttan |  |
| 1411 |  |  [Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems](https://doi.org/10.18653/v1/2022.emnlp-main.756) |  | 0 | Do all instances need inference through the big models for a correct prediction? Perhaps not; some instances are easy and can be answered correctly by even small capacity models. This provides opportunities for improving the computational efficiency of systems. In this work, we present an... | Neeraj Varshney, Chitta Baral |  |
| 1412 |  |  [Semantic Simplification for Sentiment Classification](https://doi.org/10.18653/v1/2022.emnlp-main.757) |  | 0 | Recent work on document-level sentiment classification has shown that the sentiment in the original text is often hard to capture, since the sentiment is usually either expressed implicitly or shifted due to the occurrences of negation and rhetorical words. To this end, we enhance the original text... | Xiaotong Jiang, Zhongqing Wang, Guodong Zhou |  |
| 1413 |  |  [XPrompt: Exploring the Extreme of Prompt Tuning](https://doi.org/10.18653/v1/2022.emnlp-main.758) |  | 0 | Prompt tuning learns soft prompts to condition the frozen Pre-trained Language Models (PLMs) for performing downstream tasks in a parameter-efficient manner. While prompt tuning has gradually reached the performance level of fine-tuning as the model scale increases, there is still a large... | Fang Ma, Chen Zhang, Lei Ren, Jingang Wang, Qifan Wang, Wei Wu, Xiaojun Quan, Dawei Song |  |
| 1414 |  |  [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://doi.org/10.18653/v1/2022.emnlp-main.759) |  | 0 | Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the... | Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer |  |
| 1415 |  |  [The Curious Case of Control](https://doi.org/10.18653/v1/2022.emnlp-main.760) |  | 0 | Children acquiring English make systematic errors on subject control sentences even after they have reached near-adult competence (Chomsky, 1969), possibly due to heuristics based on semantic roles (Maratsos, 1974).Given the advanced fluency of large generative language models, we ask whether model... | Elias StengelEskin, Benjamin Van Durme |  |
| 1416 |  |  [SHARE: a System for Hierarchical Assistive Recipe Editing](https://doi.org/10.18653/v1/2022.emnlp-main.761) |  | 0 | The large population of home cooks with dietary restrictions is under-served by existing cooking resources and recipe generation models. To help them, we propose the task of controllable recipe editing: adapt a base recipe to satisfy a user-specified dietary constraint. This task is challenging,... | Shuyang Li, Yufei Li, Jianmo Ni, Julian J. McAuley |  |
| 1417 |  |  [IM⌃2: an Interpretable and Multi-category Integrated Metric Framework for Automatic Dialogue Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.762) |  | 0 | Evaluation metrics shine the light on the best models and thus strongly influence the research directions, such as the recently developed dialogue metrics USR, FED, and GRADE. However, most current metrics evaluate the dialogue data as isolated and static because they only focus on a single quality... | Zhihua Jiang, Guanghui Ye, Dongning Rao, Di Wang, Xin Miao |  |
| 1418 |  |  [PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models](https://doi.org/10.18653/v1/2022.emnlp-main.763) |  | 0 | Vision-language pre-training (VLP) has shown impressive performance on a wide range of cross-modal tasks, where VLP models without reliance on object detectors are becoming the mainstream due to their superior computation efficiency and competitive performance. However, the removal of object... | Yuan Yao, Qianyu Chen, Ao Zhang, Wei Ji, Zhiyuan Liu, TatSeng Chua, Maosong Sun |  |
| 1419 |  |  [Pre-training Language Models with Deterministic Factual Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.764) |  | 0 | Previous works show that Pre-trained Language Models (PLMs) can capture factual knowledge. However, some analyses reveal that PLMs fail to perform it robustly, e.g., being sensitive to the changes of prompts when extracting factual knowledge. To mitigate this issue, we propose to let PLMs learn the... | Shaobo Li, Xiaoguang Li, Lifeng Shang, Chengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang, Qun Liu |  |
| 1420 |  |  [Finding Skill Neurons in Pre-trained Transformer-based Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.765) |  | 0 | Transformer-based pre-trained language models have demonstrated superior performance on various natural language processing tasks. However, it remains unclear how the skills required to handle these tasks distribute among model parameters. In this paper, we find that after prompt tuning for... | Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, Juanzi Li |  |
| 1421 |  |  [Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue](https://doi.org/10.18653/v1/2022.emnlp-main.766) |  | 0 | Lifelong learning (LL) is vital for advanced task-oriented dialogue (ToD) systems. To address the catastrophic forgetting issue of LL, generative replay methods are widely employed to consolidate past knowledge with generated pseudo samples. However, most existing generative replay methods use only... | Yingxiu Zhao, Yinhe Zheng, Zhiliang Tian, Chang Gao, Jian Sun, Nevin L. Zhang |  |
| 1422 |  |  [PreQuEL: Quality Estimation of Machine Translation Outputs in Advance](https://doi.org/10.18653/v1/2022.emnlp-main.767) |  | 0 | We present the task of PreQuEL, Pre-(Quality-Estimation) Learning. A PreQuEL system predicts how well a given sentence will be translated, without recourse to the actual translation, thus eschewing unnecessary resource allocation when translation quality is bound to be low. PreQuEL can be defined... | Shachar DonYehiya, Leshem Choshen, Omri Abend |  |
| 1423 |  |  [Can Transformers Reason in Fragments of Natural Language?](https://doi.org/10.18653/v1/2022.emnlp-main.768) |  | 0 | State-of-the-art deep-learning-based approaches to Natural Language Processing (NLP) are credited with various capabilities that involve reasoning with natural language texts. %However, reasoning in this setting is often ill-defined and shallow. In this paper we carry out a large-scale empirical... | Viktor Schlegel, Kamen V. Pavlov, Ian PrattHartmann |  |
| 1424 |  |  [Textless Speech Emotion Conversion using Discrete & Decomposed Representations](https://doi.org/10.18653/v1/2022.emnlp-main.769) |  | 0 | Speech emotion conversion is the task of modifying the perceived emotion of a speech utterance while preserving the lexical content and speaker identity. In this study, we cast the problem of emotion conversion as a spoken language translation task. We use a decomposition of the speech signal into... | Felix Kreuk, Adam Polyak, Jade Copet, Eugene Kharitonov, Tu Anh Nguyen, Morgane Rivière, WeiNing Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, Yossi Adi |  |
| 1425 |  |  [Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks](https://doi.org/10.18653/v1/2022.emnlp-main.770) |  | 0 | Backdoor attacks are a kind of emergent security threat in deep learning. After being injected with a backdoor, a deep neural model will behave normally on standard inputs but give adversary-specified predictions once the input contains specific backdoor triggers. In this paper, we find two simple... | Yangyi Chen, Fanchao Qi, Hongcheng Gao, Zhiyuan Liu, Maosong Sun |  |
| 1426 |  |  [Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP](https://doi.org/10.18653/v1/2022.emnlp-main.771) |  | 0 | Textual adversarial samples play important roles in multiple subfields of NLP research, including security, evaluation, explainability, and data augmentation. However, most work mixes all these roles, obscuring the problem definitions and research goals of the security role that aims to reveal the... | Yangyi Chen, Hongcheng Gao, Ganqu Cui, Fanchao Qi, Longtao Huang, Zhiyuan Liu, Maosong Sun |  |
| 1427 |  |  [Retrieval Augmented Visual Question Answering with Outside Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.772) |  | 0 | Outside-Knowledge Visual Question Answering (OK-VQA) is a challenging VQA task that requires retrieval of external knowledge to answer questions about images. Recent OK-VQA systems use Dense Passage Retrieval (DPR) to retrieve documents from external knowledge bases, such as Wikipedia, but with DPR... | Weizhe Lin, Bill Byrne |  |
| 1428 |  |  [Instance Regularization for Discriminative Language Model Pre-training](https://doi.org/10.18653/v1/2022.emnlp-main.773) |  | 0 | Discriminative pre-trained language models (PrLMs) can be generalized as denoising auto-encoders that work with two procedures, ennoising and denoising. First, an ennoising process corrupts texts with arbitrary noising functions to construct training instances. Then, a denoising language model is... | Zhuosheng Zhang, Hai Zhao, Ming Zhou |  |
| 1429 |  |  [GuoFeng: A Benchmark for Zero Pronoun Recovery and Translation](https://doi.org/10.18653/v1/2022.emnlp-main.774) |  | 0 | The phenomenon of zero pronoun (ZP) has attracted increasing interest in the machine translation (MT) community due to its importance and difficulty. However, previous studies generally evaluate the quality of translating ZPs with BLEU scores on MT testsets, which is not expressive or sensitive... | Mingzhou Xu, Longyue Wang, Derek F. Wong, Hongye Liu, Linfeng Song, Lidia S. Chao, Shuming Shi, Zhaopeng Tu |  |
| 1430 |  |  [ScienceWorld: Is your Agent Smarter than a 5th Grader?](https://doi.org/10.18653/v1/2022.emnlp-main.775) |  | 0 | We present ScienceWorld, a benchmark to test agents’ scientific reasoning abilities in a new interactive text environment at the level of a standard elementary school science curriculum. Despite the transformer-based progress seen in question-answering and scientific text processing, we find that... | Ruoyao Wang, Peter A. Jansen, MarcAlexandre Côté, Prithviraj Ammanabrolu |  |
| 1431 |  |  [Improving Embeddings Representations for Comparing Higher Education Curricula: A Use Case in Computing](https://doi.org/10.18653/v1/2022.emnlp-main.776) |  | 0 | We propose an approach for comparing curricula of study programs in higher education. Pre-trained word embeddings are fine-tuned in a study program classification task, where each curriculum is represented by the names and content of its courses. By combining metric learning with a novel... | Jeffri MurrugarraLlerena, Fernando AlvaManchego, Nils MurrugarraLlerena |  |
| 1432 |  |  [Mitigating Spurious Correlation in Natural Language Understanding with Counterfactual Inference](https://doi.org/10.18653/v1/2022.emnlp-main.777) |  | 0 | Despite their promising results on standard benchmarks, NLU models are still prone to make predictions based on shortcuts caused by unintended bias in the dataset. For example, an NLI model may use lexical overlap as a shortcut to make entailment predictions due to repetitive data generation... | Can Udomcharoenchaikit, Wuttikorn Ponwitayarat, Patomporn Payoungkhamdee, Kanruethai Masuk, Weerayut Buaphet, Ekapol Chuangsuwanich, Sarana Nutanong |  |
| 1433 |  |  [End-to-End Neural Discourse Deixis Resolution in Dialogue](https://doi.org/10.18653/v1/2022.emnlp-main.778) |  | 0 | We adapt Lee et al.’s (2018) span-based entity coreference model to the task of end-to-end discourse deixis resolution in dialogue, specifically by proposing extensions to their model that exploit task-specific characteristics. The resulting model, dd-utt, achieves state-of-the-art results on the... | Shengjie Li, Vincent Ng |  |
| 1434 |  |  [Balancing out Bias: Achieving Fairness Through Balanced Training](https://doi.org/10.18653/v1/2022.emnlp-main.779) |  | 0 | Group bias in natural language processing tasks manifests as disparities in system error rates across texts authorized by different demographic groups, typically disadvantaging minority groups. Dataset balancing has been shown to be effective at mitigating bias, however existing approaches do not... | Xudong Han, Timothy Baldwin, Trevor Cohn |  |
| 1435 |  |  [Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models](https://doi.org/10.18653/v1/2022.emnlp-main.780) |  | 0 | Pre-trained masked language models successfully perform few-shot learning by formulating downstream tasks as text infilling. How- ever, as a strong alternative in full-shot settings, discriminative pre-trained models like ELECTRA do not fit into the paradigm. In this work, we adapt prompt-based... | Mengzhou Xia, Mikel Artetxe, Jingfei Du, Danqi Chen, Veselin Stoyanov |  |
| 1436 |  |  [Identifying Physical Object Use in Sentences](https://doi.org/10.18653/v1/2022.emnlp-main.781) |  | 0 | Commonsense knowledge about the typicalfunctions of physical objects allows people tomake inferences during sentence understanding.For example, we infer that “Sam enjoyedthe book” means that Sam enjoyed reading thebook, even though the action is implicit. Priorresearch has focused on learning the... | Tianyu Jiang, Ellen Riloff |  |
| 1437 |  |  [CDialog: A Multi-turn Covid-19 Conversation Dataset for Entity-Aware Dialog Generation](https://doi.org/10.18653/v1/2022.emnlp-main.782) |  | 0 |  | Deeksha Varshney, Aizan Zafar, Niranshu Kumar Behra, Asif Ekbal |  |
| 1438 |  |  [Robustifying Sentiment Classification by Maximally Exploiting Few Counterfactuals](https://doi.org/10.18653/v1/2022.emnlp-main.783) |  | 0 | For text classification tasks, finetuned language models perform remarkably well. Yet, they tend to rely on spurious patterns in training data, thus limiting their performance on out-of-distribution (OOD) test data. Among recent models aiming to avoid this spurious pattern problem, adding extra... | Maarten De Raedt, Fréderic Godin, Chris Develder, Thomas Demeester |  |
| 1439 |  |  [Data-Efficient Playlist Captioning With Musical and Linguistic Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.784) |  | 0 | Music streaming services feature billions of playlists created by users, professional editors or algorithms. In this content overload scenario, it is crucial to characterise playlists, so that music can be effectively organised and accessed. Playlist titles and descriptions are proposed in natural... | Giovanni Gabbolini, Romain Hennequin, Elena V. Epure |  |
| 1440 |  |  [Improved grammatical error correction by ranking elementary edits](https://doi.org/10.18653/v1/2022.emnlp-main.785) |  | 0 | We offer a two-stage reranking method for grammatical error correction: the first model serves as edit generator, while the second classifies the proposed edits as correct or false. We show how to use both encoder-decoder and sequence labeling models for the first step of our pipeline. We achieve... | Alexey Sorokin |  |
| 1441 |  |  [Improving Tokenisation by Alternative Treatment of Spaces](https://doi.org/10.18653/v1/2022.emnlp-main.786) |  | 0 |  | Edward GowSmith, Harish Tayyar Madabushi, Carolina Scarton, Aline Villavicencio |  |
| 1442 |  |  [GENIE: Toward Reproducible and Standardized Human Evaluation for Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.787) |  | 0 | While often assumed a gold standard, effective human evaluation of text generation remains an important, open area for research.We revisit this problem with a focus on producing consistent evaluations that are reproducible—over time and across different populations. We study this goal in different... | Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A. Smith, Daniel S. Weld |  |
| 1443 |  |  [Attentional Probe: Estimating a Module's Functional Potential](https://doi.org/10.18653/v1/2022.emnlp-main.788) |  | 0 | In this paper, we seek to measure how much information a component in a neural network could extract from the representations fed into it. Our work stands in contrast to prior probing work, most of which investigates how much information a model's representations contain. This shift in perspective... | Tiago Pimentel, Josef Valvoda, Niklas Stoehr, Ryan Cotterell |  |
| 1444 |  |  [When More Data Hurts: A Troubling Quirk in Developing Broad-Coverage Natural Language Understanding Systems](https://doi.org/10.18653/v1/2022.emnlp-main.789) |  | 0 | In natural language understanding (NLU) production systems, users’ evolving needs necessitate the addition of new features over time, indexed by new symbols added to the meaning representation space. This requires additional training data and results in ever-growing datasets. We present the first... | Elias StengelEskin, Emmanouil Antonios Platanios, Adam Pauls, Sam Thomson, Hao Fang, Benjamin Van Durme, Jason Eisner, Yu Su |  |
| 1445 |  |  [Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt](https://doi.org/10.18653/v1/2022.emnlp-main.790) |  | 0 | Prompt-based tuning has been proven effective for pretrained language models (PLMs). While most of the existing work focuses on the monolingual prompts, we study the multilingual prompts for multilingual PLMs, especially in the zero-shot cross-lingual setting. To alleviate the effort of designing... | Lianzhe Huang, Shuming Ma, Dongdong Zhang, Furu Wei, Houfeng Wang |  |
| 1446 |  |  [Three Real-World Datasets and Neural Computational Models for Classification Tasks in Patent Landscaping](https://doi.org/10.18653/v1/2022.emnlp-main.791) |  | 0 | Patent Landscaping, one of the central tasks of intellectual property management, includes selecting and grouping patents according to user-defined technical or application-oriented criteria. While recent transformer-based models have been shown to be effective for classifying patents into... | Subhash Chandra Pujari, Jannik Strötgen, Mark Giereth, Michael Gertz, Annemarie Friedrich |  |
| 1447 |  |  [Topic Modeling With Topological Data Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.792) |  | 0 | Recent unsupervised topic modelling ap-proaches that use clustering techniques onword, token or document embeddings can ex-tract coherent topics. A common limitationof such approaches is that they reveal noth-ing about inter-topic relationships which areessential in many real-world application... | Ciarán Byrne, Danijela Horak, Karo Moilanen, Amandla Mabona |  |
| 1448 |  |  [Predicting Fine-Tuning Performance with Probing](https://doi.org/10.18653/v1/2022.emnlp-main.793) |  | 0 | Large NLP models have recently shown impressive performance in language understanding tasks, typically evaluated by their fine-tuned performance. Alternatively, probing has received increasing attention as being a lightweight method for interpreting the intrinsic mechanisms of large NLP models. In... | Zining Zhu, Soroosh Shahtalebi, Frank Rudzicz |  |
| 1449 |  |  [Diverse Parallel Data Synthesis for Cross-Database Adaptation of Text-to-SQL Parsers](https://doi.org/10.18653/v1/2022.emnlp-main.794) |  | 0 | Text-to-SQL parsers typically struggle with databases unseen during the train time. Adapting Text-to-SQL parsers to new database schemas is a challenging problem owing to a vast diversity of schemas and zero availability of natural language queries in new schemas. We present ReFill, a framework for... | Abhijeet Awasthi, Ashutosh Sathe, Sunita Sarawagi |  |
| 1450 |  |  [Agent-Specific Deontic Modality Detection in Legal Language](https://doi.org/10.18653/v1/2022.emnlp-main.795) |  | 0 | Legal documents are typically long and written in legalese, which makes it particularly difficult for laypeople to understand their rights and duties. While natural language understanding technologies can be valuable in supporting such understanding in the legal domain, the limited availability of... | Abhilasha Sancheti, Aparna Garimella, Balaji Vasan Srinivasan, Rachel Rudinger |  |
| 1451 |  |  [COLD: A Benchmark for Chinese Offensive Language Detection](https://doi.org/10.18653/v1/2022.emnlp-main.796) |  | 0 | Offensive language detection is increasingly crucial for maintaining a civilized social media platform and deploying pre-trained language models. However, this task in Chinese is still under exploration due to the scarcity of reliable datasets. To this end, we propose a benchmark –COLD for Chinese... | Jiawen Deng, Jingyan Zhou, Hao Sun, Chujie Zheng, Fei Mi, Helen Meng, Minlie Huang |  |
| 1452 |  |  [Fixing Model Bugs with Natural Language Patches](https://doi.org/10.18653/v1/2022.emnlp-main.797) |  | 0 | Current approaches for fixing systematic problems in NLP models (e.g., regex patches, finetuning on more data) are either brittle, or labor-intensive and liable to shortcuts. In contrast, humans often provide corrections to each other through natural language. Taking inspiration from this, we... | Shikhar Murty, Christopher D. Manning, Scott M. Lundberg, Marco Túlio Ribeiro |  |
| 1453 |  |  [WeDef: Weakly Supervised Backdoor Defense for Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.798) |  | 0 | Existing backdoor defense methods are only effective for limited trigger types. To defend different trigger types at once, we start from the class-irrelevant nature of the poisoning process and propose a novel weakly supervised backdoor defense framework WeDef. Recent advances in weak supervision... | Lesheng Jin, Zihan Wang, Jingbo Shang |  |
| 1454 |  |  [Interventional Training for Out-Of-Distribution Natural Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.799) |  | 0 | Out-of-distribution (OOD) settings are used to measure a model’s performance when the distribution of the test data is different from that of the training data. NLU models are known to suffer in OOD. We study this issue from the perspective of causality, which sees confounding bias as the reason... | Sicheng Yu, Jing Jiang, Hao Zhang, Yulei Niu, Qianru Sun, Lidong Bing |  |
| 1455 |  |  [Pseudo-Relevance for Enhancing Document Representation](https://doi.org/10.18653/v1/2022.emnlp-main.800) |  | 0 | This paper studies how to enhance the document representation for the bi-encoder approach in dense document retrieval. The bi-encoder, separately encoding a query and a document as a single vector, is favored for high efficiency in large-scale information retrieval, compared to more effective but... | Jihyuk Kim, Seungwon Hwang, Seoho Song, Hyeseon Ko, YoungIn Song |  |
| 1456 |  |  [ZeroGen: Efficient Zero-shot Learning via Dataset Generation](https://doi.org/10.18653/v1/2022.emnlp-main.801) |  | 0 | There is a growing interest in dataset generation recently due to the superior generative capacity of large pre-trained language models (PLMs). In this paper, we study a flexible and efficient zero-short learning method, ZeroGen.Given a zero-shot task, we first generate a dataset from scratch using... | Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, Lingpeng Kong |  |
| 1457 |  |  [Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.802) |  | 0 | Learning scientific document representations can be substantially improved through contrastive learning objectives, where the challenge lies in creating positive and negative training samples that encode the desired similarity semantics. Prior work relies on discrete citation relations to generate... | Malte Ostendorff, Nils Rethmeier, Isabelle Augenstein, Bela Gipp, Georg Rehm |  |
| 1458 |  |  [SPE: Symmetrical Prompt Enhancement for Fact Probing](https://doi.org/10.18653/v1/2022.emnlp-main.803) |  | 0 | Pretrained language models (PLMs) have been shown to accumulate factual knowledge during pretraining (Petroni et al. 2019). Recent works probe PLMs for the extent of this knowledge through prompts either in discrete or continuous forms. However, these methods do not consider symmetry of the task:... | Yiyuan Li, Tong Che, Yezhen Wang, Zhengbao Jiang, Caiming Xiong, Snigdha Chaturvedi |  |
| 1459 |  |  [Efficient Large Scale Language Modeling with Mixtures of Experts](https://doi.org/10.18653/v1/2022.emnlp-main.804) |  | 0 | Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language... | Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giridharan Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeffrey Wang, Luke Zettlemoyer, Mona T. Diab, Zornitsa Kozareva, Veselin Stoyanov |  |
| 1460 |  |  [MedJEx: A Medical Jargon Extraction Model with Wiki's Hyperlink Span and Contextualized Masked Language Model Score](https://doi.org/10.18653/v1/2022.emnlp-main.805) |  | 0 | This paper proposes a new natural language processing (NLP) application for identifying medical jargon terms potentially difficult for patients to comprehend from electronic health record (EHR) notes. We first present a novel and publicly available dataset with expert-annotated medical jargon terms... | Sunjae Kwon, Zonghai Yao, Harmon S. Jordan, David A. Levy, Brian Corner, Hong Yu |  |
| 1461 |  |  [Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections](https://doi.org/10.18653/v1/2022.emnlp-main.806) |  | 0 | While there has been substantial progress in text comprehension through simple factoid question answering, more holistic comprehension of a discourse still presents a major challenge (Dunietz et al., 2020). Someone critically reflecting on a text as they read it will pose curiosity-driven, often... | WeiJen Ko, Cutter Dalton, Mark Simmons, Eliza Fisher, Greg Durrett, Junyi Jessy Li |  |
| 1462 |  |  [Learning to Generate Overlap Summaries through Noisy Synthetic Data](https://doi.org/10.18653/v1/2022.emnlp-main.807) |  | 0 | Semantic Overlap Summarization (SOS) is a novel and relatively under-explored seq-to-seq task which entails summarizing common information from multiple alternate narratives. One of the major challenges for solving this task is the lack of existing datasets for supervised training. To address this... | Naman Bansal, Mousumi Akter, Shubhra Kanti Karmaker Santu |  |
| 1463 |  |  [Mutual Exclusivity Training and Primitive Augmentation to Induce Compositionality](https://doi.org/10.18653/v1/2022.emnlp-main.808) |  | 0 | Recent datasets expose the lack of the systematic generalization ability in standard sequence-to-sequence models. In this work, we analyze this behavior of seq2seq models and identify two contributing factors: a lack of mutual exclusivity bias (one target sequence can only be mapped to one source... | Yichen Jiang, Xiang Zhou, Mohit Bansal |  |
| 1464 |  |  [Directions for NLP Practices Applied to Online Hate Speech Detection](https://doi.org/10.18653/v1/2022.emnlp-main.809) |  | 0 | Addressing hate speech in online spaces has been conceptualized as a classification task that uses Natural Language Processing (NLP) techniques. Through this conceptualization, the hate speech detection task has relied on common conventions and practices from NLP. For instance, inter-annotator... | Paula Fortuna, Mónica Domínguez, Leo Wanner, Zeerak Talat |  |
| 1465 |  |  [Pre-training Transformer Models with Sentence-Level Objectives for Answer Sentence Selection](https://doi.org/10.18653/v1/2022.emnlp-main.810) |  | 0 | An important task for designing QA systems is answer sentence selection (AS2): selecting the sentence containing (or constituting) the answer to a question from a set of retrieved relevant documents. In this paper, we propose three novel sentence-level transformer pre-training objectives that... | Luca Di Liello, Siddhant Garg, Luca Soldaini, Alessandro Moschitti |  |
| 1466 |  |  [OpenCQA: Open-ended Question Answering with Charts](https://doi.org/10.18653/v1/2022.emnlp-main.811) |  | 0 | Charts are very popular to analyze data and convey important insights. People often analyze visualizations to answer open-ended questions that require explanatory answers. Answering such questions are often difficult and time-consuming as it requires a lot of cognitive and perceptual efforts. To... | Shankar Kantharaj, Xuan Long Do, Rixie Tiffany Ko Leong, Jia Qing Tan, Enamul Hoque, Shafiq R. Joty |  |
| 1467 |  |  [A Systematic Investigation of Commonsense Knowledge in Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.812) |  | 0 | Language models (LMs) trained on large amounts of data have shown impressive performance on many NLP tasks under the zero-shot and few-shot setup. Here we aim to better understand the extent to which such models learn commonsense knowledge — a critical component of many NLP applications. We conduct... | Xiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoffmann, Cyprien de Masson d'Autume, Phil Blunsom, Aida Nematzadeh |  |
| 1468 |  |  [Transforming Sequence Tagging Into A Seq2Seq Task](https://doi.org/10.18653/v1/2022.emnlp-main.813) |  | 0 | Pretrained, large, generative language models (LMs) have had great success in a wide range of sequence tagging and structured prediction tasks. Casting a sequence tagging task as a Seq2Seq one requires deciding the formats of the input and output sequences. However, we lack a principled... | Karthik Raman, Iftekhar Naim, Jiecao Chen, Kazuma Hashimoto, Kiran Yalasangi, Krishna Srinivasan |  |
| 1469 |  |  [CycleKQR: Unsupervised Bidirectional Keyword-Question Rewriting](https://doi.org/10.18653/v1/2022.emnlp-main.814) |  | 0 | Users expect their queries to be answered by search systems, regardless of the query’s surface form, which include keyword queries and natural questions. Natural Language Understanding (NLU) components of Search and QA systems may fail to correctly interpret semantically equivalent inputs if this... | Andrea Iovine, Anjie Fang, Besnik Fetahu, Jie Zhao, Oleg Rokhlenko, Shervin Malmasi |  |
| 1470 |  |  [Model Criticism for Long-Form Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.815) |  | 0 | Language models have demonstrated the ability to generate highly fluent text; however, it remains unclear whether their output retains coherent high-level structure (e.g., story progression). Here, we propose to apply a statistical tool, model criticism in latent space, to evaluate the high-level... | Yuntian Deng, Volodymyr Kuleshov, Alexander M. Rush |  |
| 1471 |  |  [Improving Faithfulness by Augmenting Negative Summaries from Fake Documents](https://doi.org/10.18653/v1/2022.emnlp-main.816) |  | 0 | Current abstractive summarization systems tend to hallucinate content that is unfaithful to the source document, posing a risk of misinformation. To mitigate hallucination, we must teach the model to distinguish hallucinated summaries from faithful ones. However, the commonly used maximum... | Tianshu Wang, Faisal Ladhak, Esin Durmus, He He |  |
| 1472 |  |  [Joint Completion and Alignment of Multilingual Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.817) |  | 0 | Knowledge Graph Completion (KGC) predicts missing facts in an incomplete Knowledge Graph (KG). Multilingual KGs associate entities and relations with surface forms written in different languages. An entity or relation may be associated with distinct IDs in different KGs, necessitating entity... | Soumen Chakrabarti, Harkanwar Singh, Shubham Lohiya, Prachi Jain, Mausam |  |
| 1473 |  |  [Offer a Different Perspective: Modeling the Belief Alignment of Arguments in Multi-party Debates](https://doi.org/10.18653/v1/2022.emnlp-main.818) |  | 0 | In contexts where debate and deliberation are the norm, the participants are regularly presented with new information that conflicts with their original beliefs. When required to update their beliefs (belief alignment), they may choose arguments that align with their worldview (confirmation bias).... | Suzanna Sia, Kokil Jaidka, Hansin Ahuja, Niyati Chhaya, Kevin Duh |  |
| 1474 |  |  [A Federated Approach to Predicting Emojis in Hindi Tweets](https://doi.org/10.18653/v1/2022.emnlp-main.819) |  | 0 | The use of emojis affords a visual modality to, often private, textual communication.The task of predicting emojis however provides a challenge for machine learning as emoji use tends to cluster into the frequently used and the rarely used emojis.Much of the machine learning research on emoji use... | Deep Gandhi, Jash Mehta, Nirali Parekh, Karan Waghela, Lynette D'Mello, Zeerak Talat |  |
| 1475 |  |  [Injecting Domain Knowledge in Language Models for Task-oriented Dialogue Systems](https://doi.org/10.18653/v1/2022.emnlp-main.820) |  | 0 | Pre-trained language models (PLM) have advanced the state-of-the-art across NLP applications, but lack domain-specific knowledge that does not naturally occur in pre-training data. Previous studies augmented PLMs with symbolic knowledge for different downstream NLP tasks. However, knowledge bases... | Denis Emelin, Daniele Bonadiman, Sawsan Alqahtani, Yi Zhang, Saab Mansour |  |
| 1476 |  |  [TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack](https://doi.org/10.18653/v1/2022.emnlp-main.821) |  | 0 | We present Twin Answer Sentences Attack (TASA), an adversarial attack method for question answering (QA) models that produces fluent and grammatical adversarial contexts while maintaining gold answers. Despite phenomenal progress on general adversarial attacks, few works have investigated the... | Yu Cao, Dianqi Li, Meng Fang, Tianyi Zhou, Jun Gao, Yibing Zhan, Dacheng Tao |  |
| 1477 |  |  [Improving Low-Resource Languages in Pre-Trained Multilingual Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.822) |  | 0 | Pre-trained multilingual language models are the foundation of many NLP approaches, including cross-lingual transfer solutions. However, languages with small available monolingual corpora are often not well-supported by these models leading to poor performance. We propose an unsupervised approach... | Viktor Hangya, Hossain Shaikh Saadi, Alexander Fraser |  |
| 1478 |  |  [SCROLLS: Standardized CompaRison Over Long Language Sequences](https://doi.org/10.18653/v1/2022.emnlp-main.823) |  | 0 | NLP benchmarks have largely focused on short texts, such as sentences and paragraphs, even though long texts comprise a considerable amount of natural language in the wild. We introduce SCROLLS, a suite of tasks that require reasoning over long texts. We examine existing long-text datasets, and... | Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, Omer Levy |  |
| 1479 |  |  [PAR: Political Actor Representation Learning with Social Context and Expert Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.824) |  | 0 | Modeling the ideological perspectives of political actors is an essential task in computational political science with applications in many downstream tasks. Existing approaches are generally limited to textual data and voting records, while they neglect the rich social context and valuable expert... | Shangbin Feng, Zhaoxuan Tan, Zilong Chen, Ningnan Wang, Peisheng Yu, Qinghua Zheng, Xiaojun Chang, Minnan Luo |  |
| 1480 |  |  [JDDC 2.1: A Multimodal Chinese Dialogue Dataset with Joint Tasks of Query Rewriting, Response Generation, Discourse Parsing, and Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.825) |  | 0 | The popularity of multimodal dialogue has stimulated the need for a new generation of dialogue agents with multimodal interactivity.When users communicate with customer service, they may express their requirements by means of text, images, or even videos. Visual information usually acts as... | Nan Zhao, Haoran Li, Youzheng Wu, Xiaodong He |  |
| 1481 |  |  [PCL: Peer-Contrastive Learning with Diverse Augmentations for Unsupervised Sentence Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.826) |  | 0 | Learning sentence embeddings in an unsupervised manner is fundamental in natural language processing. Recent common practice is to couple pre-trained language models with unsupervised contrastive learning, whose success relies on augmenting a sentence with a semantically-close positive instance to... | Qiyu Wu, Chongyang Tao, Tao Shen, Can Xu, Xiubo Geng, Daxin Jiang |  |
| 1482 |  |  [Digging Errors in NMT: Evaluating and Understanding Model Errors from Partial Hypothesis Space](https://doi.org/10.18653/v1/2022.emnlp-main.827) |  | 0 | Solid evaluation of neural machine translation (NMT) is key to its understanding and improvement. Current evaluation of an NMT system is usually built upon a heuristic decoding algorithm (e.g., beam search) and an evaluation metric assessing similarity between the translation and golden reference.... | Jianhao Yan, Chenming Wu, Fandong Meng, Jie Zhou |  |
| 1483 |  |  [DialogConv: A Lightweight Fully Convolutional Network for Multi-view Response Selection](https://doi.org/10.18653/v1/2022.emnlp-main.828) |  | 0 | Current end-to-end retrieval-based dialogue systems are mainly based on Recurrent Neural Networks or Transformers with attention mechanisms. Although promising results have been achieved, these models often suffer from slow inference or huge number of parameters. In this paper, we propose a novel... | Yongkang Liu, Shi Feng, Wei Gao, Daling Wang, Yifei Zhang |  |
