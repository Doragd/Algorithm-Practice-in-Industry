# EMNLP2022

## 会议论文列表

本会议共有 1483 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022](https://aclanthology.org/volumes/2022.findings-emnlp/) |  | 0 |  | Yoav Goldberg, Yue Zhang, Zornitsa Kozareva |  |
| 2 |  |  [LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.1) |  | 0 | Recently, deep learning models have made great progress in MWP solving on answer accuracy. However, they are uninterpretable since they mainly rely on shallow heuristics to achieve high performance without understanding and reasoning the grounded math logic. To address this issue and make a step... | Jiaqi Chen, Jinghui Qin, Liang Lin, Xiaodan Liang, Zhicheng Yang |  |
| 3 |  |  [Commonsense Knowledge Salience Evaluation with a Benchmark Dataset in E-commerce](https://doi.org/10.18653/v1/2022.findings-emnlp.2) |  | 0 | In e-commerce, the salience of commonsense knowledge (CSK) is beneficial for widespread applications such as product search and recommendation. For example, when users search for “running” in e-commerce, they would like to find products highly related to running, such as “running shoes” rather than... | Chengming Wang, Huajun Chen, Hui Chen, Ningyu Zhang, Qiang Chen, Xiaoyu Wang, Yincen Qu, Zelin Dai |  |
| 4 |  |  [Automatic Rule Induction for Efficient Semi-Supervised Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.3) |  | 0 | Semi-supervised learning has shown promise in allowing NLP models to generalize from small amounts of labeled data. Meanwhile, pretrained transformer models act as black-box correlation engines that are difficult to explain and sometimes behave unreliably. In this paper, we propose tackling both of... | Chenguang Zhu, Michael Zeng, Reid Pryzant, Yichong Xu, Ziyi Yang |  |
| 5 |  |  [Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion](https://doi.org/10.18653/v1/2022.findings-emnlp.4) |  | 0 | Transformer-based pre-trained models like BERT have achieved great progress on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also shown general benefits in multiple NLP tasks. However, how to efficiently integrate dependency prior structure into pre-trained models to better... | Di Liang, Jian Song, Minlong Peng, Rumei Li, Sirui Wang, Wei Wu, Yongxin Yu, Yuntao Li |  |
| 6 |  |  [Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT](https://doi.org/10.18653/v1/2022.findings-emnlp.5) |  | 0 | We combine the capacity of sparsely gated Mixture-of-Experts (MoE) with the speed and stability of linear, mixing transformations to design the Sparse Mixer encoder model. Sparse Mixer slightly outperforms BERT on GLUE and SuperGLUE, but more importantly trains 65% faster and runs inference 61%... | James LeeThorp, Joshua Ainslie |  |
| 7 |  |  [KE-GCL: Knowledge Enhanced Graph Contrastive Learning for Commonsense Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.6) |  | 0 | Commonsense question answering (CQA) aims to choose the correct answers for commonsense questions. Most existing works focus on extracting and reasoning over external knowledge graphs (KG). However, the noise in KG prevents these models from learning effective representations. In this paper, we... | Lihui Zhang, Ruifan Li |  |
| 8 |  |  [Acceptability Judgements via Examining the Topology of Attention Maps](https://doi.org/10.18653/v1/2022.findings-emnlp.7) |  | 0 | The role of the attention mechanism in encoding linguistic knowledge has received special interest in NLP. However, the ability of the attention heads to judge the grammatical acceptability of a sentence has been underexplored. This paper approaches the paradigm of acceptability judgments with... | Daniil Cherniavskii, Dmitri Piontkovski, Eduard Tulchinskii, Ekaterina Artemova, Evgeny Burnaev, Irina Piontkovskaya, Irina Proskurina, Laida Kushnareva, Serguei Barannikov, Vladislav Mikhailov |  |
| 9 |  |  [Clip-Tuning: Towards Derivative-free Prompt Learning with a Mixture of Rewards](https://doi.org/10.18653/v1/2022.findings-emnlp.8) |  | 0 | Derivative-free prompt learning has emerged as a lightweight alternative to prompt tuning, which only requires model inference to optimize the prompts. However, existing work did not take full advantage of the over-parameterized characteristics of large pre-trained language models (PLMs). In this... | Haifeng Wang, Hao Tian, Hua Wu, Shuohuan Wang, Yekun Chai, Yu Sun |  |
| 10 |  |  [Soft-Labeled Contrastive Pre-Training for Function-Level Code Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.9) |  | 0 | Code contrastive pre-training has recently achieved significant progress on code-related tasks. In this paper, we present SCodeR, a Soft-labeled contrastive pre-training framework with two positive sample construction methods to learn functional-level Code Representation. Considering the relevance... | Daxin Jiang, Daya Guo, Nan Duan, Weizhu Chen, Xiaonan Li, Xipeng Qiu, Yelong Shen, Yeyun Gong, Yun Lin |  |
| 11 |  |  [Conditioned Masked Language and Image Modeling for Image-Text Dense Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.10) |  | 0 | Image-text retrieval is a fundamental cross-modal task that takes image/text as a query to retrieve relevant data of another type. The large-scale two-stream pre-trained models like CLIP have achieved tremendous success in this area. They embed the images and texts into instance representations... | GongZheng Li, Jing Ma, Rongsheng Zhang, Yadong Xi, Zeng Zhao, Ziyang Luo |  |
| 12 |  |  [Does Simultaneous Speech Translation need Simultaneous Models?](https://doi.org/10.18653/v1/2022.findings-emnlp.11) |  | 0 | In simultaneous speech translation (SimulST), finding the best trade-off between high output quality and low latency is a challenging task. To meet the latency constraints posed by different application scenarios, multiple dedicated SimulST models are usually trained and maintained, generating high... | Marco Gaido, Marco Turchi, Matteo Negri, Sara Papi |  |
| 13 |  |  [Utilizing Language-Image Pretraining for Efficient and Robust Bilingual Word Alignment](https://doi.org/10.18653/v1/2022.findings-emnlp.12) |  | 0 | Word translation without parallel corpora has become feasible, rivaling the performance of supervised methods. Recent findings have shown the improvement in accuracy and robustness of unsupervised word translation (UWT) by utilizing visual observations, which are universal representations across... | Dimitris S. Papailiopoulos, Junjie Hu, Jyyong Sohn, Kangwook Lee, Shashank Rajput, Timothy Ossowski, Tuan Dinh, Yifei Ming |  |
| 14 |  |  [Grape: Knowledge Graph Enhanced Passage Reader for Open-domain Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.13) |  | 0 | A common thread of open-domain question answering (QA) models employs a retriever-reader pipeline that first retrieves a handful of relevant passages from Wikipedia and then peruses the passages to produce an answer. However, even state-of-the-art readers fail to capture the complex relationships... | Chuxu Zhang, Mingxuan Ju, Tong Zhao, Wenhao Yu, Yanfang Ye |  |
| 15 |  |  [NarraSum: A Large-Scale Dataset for Abstractive Narrative Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.14) |  | 0 | Narrative summarization aims to produce a distilled version of a narrative to describe its most salient events and characters. Writing a summary for a narrative is challenging as it requires an understanding of event causality and character behaviors. To encourage research in this direction, we... | Chao Zhao, Dian Yu, Faeze Brahman, Kaiqiang Song, Snigdha Chaturvedi, Wenlin Yao |  |
| 16 |  |  [NMTScore: A Multilingual Analysis of Translation-based Text Similarity Measures](https://doi.org/10.18653/v1/2022.findings-emnlp.15) |  | 0 | Being able to rank the similarity of short text segments is an interesting bonus feature of neural machine translation. Translation-based similarity measures include direct and pivot translation probability, as well as translation cross-likelihood, which has not been studied so far. We analyze... | Jannis Vamvas, Rico Sennrich |  |
| 17 |  |  [Language Models Understand Us, Poorly](https://doi.org/10.18653/v1/2022.findings-emnlp.16) |  | 0 | Some claim language models understand us. Others won’t hear it. To clarify, I investigate three views of human language understanding: as-mapping, as-reliability and as-representation. I argue that while behavioral reliability is necessary for understanding, internal representations are sufficient;... | Jared Moore |  |
| 18 |  |  [Dialogue Meaning Representation for Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.17) |  | 0 | Dialogue meaning representation formulates natural language utterance semantics in their conversational context in an explicit and machine-readable form. Previous work typically follows the intent-slot framework, which is easy for annotation yet limited in scalability for complex linguistic... | Hang Yan, Junqi Dai, Qipeng Guo, Xiangkun Hu, Xipeng Qiu, Yi Zhang, Zheng Zhang |  |
| 19 |  |  [Learning from the Dictionary: Heterogeneous Knowledge Guided Fine-tuning for Chinese Spell Checking](https://doi.org/10.18653/v1/2022.findings-emnlp.18) |  | 0 | Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling errors. Recent researches start from the pretrained knowledge of language models and take multimodal information into CSC models to improve the performance. However, they overlook the rich knowledge in the dictionary, the... | Chao Li, Haitao Zheng, Qingyu Zhou, Ruiyang Liu, Shirong Ma, Shulin Huang, Yangning Li, Yinghui Li, Yunbo Cao, Zhongli Li |  |
| 20 |  |  [Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?](https://doi.org/10.18653/v1/2022.findings-emnlp.19) |  | 0 | Despite their recent popularity and well-known advantages, dense retrievers still lag behind sparse methods such as BM25 in their ability to reliably match salient phrases and rare entities in the query and to generalize to out-of-domain data. It has been argued that this is an inherent limitation... | Anchit Gupta, Barlas Oguz, Kushal Lakhotia, Patrick S. H. Lewis, Sonal Gupta, Stan Peshterliev, Wentau Yih, Xilun Chen, Yashar Mehdad |  |
| 21 |  |  [SMARTAVE: Structured Multimodal Transformer for Product Attribute Value Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.20) |  | 0 | Automatic product attribute value extraction refers to the task of identifying values of an attribute from the product information. Product attributes are essential in improving online shopping experience for customers. Most existing methods focus on extracting attribute values from product title... | Bo Dai, Hao Ma, Jingang Wang, Jitin Krishnan, Li Yang, Madian Khabsa, Qifan Wang, Sinong Wang, Zenglin Xu |  |
| 22 |  |  [When Language Model Meets Private Library](https://doi.org/10.18653/v1/2022.findings-emnlp.21) |  | 0 | With the rapid development of pre-training techniques, a number of language models have been pre-trained on large-scale code corpora and perform well in code generation. In this paper, we investigate how to equip pre-trained language models with the ability of code generation for private libraries.... | Bei Chen, Bei Guan, Daoguang Zan, JianGuang Lou, Yongji Wang, Zeqi Lin |  |
| 23 |  |  [Cross-Domain Sentiment Classification using Semantic Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.22) |  | 0 | Previous studies on cross-domain sentiment classification depend on the pivot features or utilize the target data for representation learning, which ignore the semantic relevance between different domains. To this end, we exploit Abstract Meaning Representation (AMR) to help with cross-domain... | Guodong Zhou, Shichen Li, Xiaotong Jiang, Zhongqing Wang |  |
| 24 |  |  [Yes-Yes-Yes: Proactive Data Collection for ACL Rolling Review and Beyond](https://doi.org/10.18653/v1/2022.findings-emnlp.23) |  | 0 | The shift towards publicly available text sources has enabled language processing at unprecedented scale, yet leaves under-serviced the domains where public and openly licensed data is scarce. Proactively collecting text data for research is a viable strategy to address this scarcity, but lacks... | Ilia Kuznetsov, Iryna Gurevych, Nils Dycke |  |
| 25 |  |  [AssistSR: Task-oriented Video Segment Retrieval for Personal AI Assistant](https://doi.org/10.18653/v1/2022.findings-emnlp.24) |  | 0 | It is still a pipe dream that personal AI assistants on the phone and AR glasses can assist our daily life in addressing our questions like “how to adjust the date for this watch?” and “how to set its heating duration? (while pointing at an oven)”. The queries used in conventional tasks (i.e. Video... | Difei Gao, Dongxing Mao, Lingmin Ran, Mike Zheng Shou, Weixian Lei, Yuxuan Wang, Zihan Liang |  |
| 26 |  |  [Dim-Krum: Backdoor-Resistant Federated Learning for NLP with Dimension-wise Krum-Based Aggregation](https://doi.org/10.18653/v1/2022.findings-emnlp.25) |  | 0 | Despite the potential of federated learning, it is known to be vulnerable to backdoor attacks. Many robust federated aggregation methods are proposed to reduce the potential backdoor risk. However, they are mainly validated in the CV field. In this paper, we find that NLP backdoors are hard to... | Qi Su, Xu Sun, Zhiyuan Zhang |  |
| 27 |  |  [Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.26) |  | 0 | Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks. In Natural Language Processing (NLP), DNNs are often backdoored during the fine-tuning process of a large-scale Pre-trained Language Model (PLM) with poisoned samples. Although the clean weights of PLMs are readily... | Chenguang Wang, Lingjuan Lyu, Xingjun Ma, Xu Sun, Zhiyuan Zhang |  |
| 28 |  |  [Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion](https://doi.org/10.18653/v1/2022.findings-emnlp.27) |  | 0 | Language models (LMs) have recently been shown to generate more factual responses by employing modularity (Zhou et al., 2022) in combination with retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et al. (2021) to include internet search as a module. Our SeeKeR (Search... | Arthur Szlam, Jason Weston, Kurt Shuster, Leonard Adolphs, Mojtaba Komeili, Stephen Roller |  |
| 29 |  |  [Stretching Sentence-pair NLI Models to Reason over Long Documents and Clusters](https://doi.org/10.18653/v1/2022.findings-emnlp.28) |  | 0 | Natural Language Inference (NLI) has been extensively studied by the NLP community as a framework for estimating the semantic relation between sentence pairs. While early work identified certain biases in NLI models, recent advancements in modeling and datasets demonstrated promising performance.In... | Alex Fabrikant, Donald Metzler, Senaka Buthpitiya, Sihao Chen, Tal Schuster |  |
| 30 |  |  [Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study](https://doi.org/10.18653/v1/2022.findings-emnlp.29) |  | 0 | This paper presents an empirical study to build relation extraction systems in low-resource settings. Based upon recent pre-trained language models, we comprehensively investigate three schemes to evaluate the performance in low-resource settings: (i) different types of prompt-based methods with... | Huajun Chen, Ningyu Zhang, Xi Chen, Xiang Chen, Xin Xie, Xin Xu |  |
| 31 |  |  [CLLE: A Benchmark for Continual Language Learning Evaluation in Multilingual Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.30) |  | 0 | Continual Language Learning (CLL) in multilingual translation is inevitable when new languages are required to be translated. Due to the lack of unified and generalized benchmarks, the evaluation of existing methods is greatly influenced by experimental design which usually has a big gap from the... | Bin Liang, Han Zhang, Hui Wang, Jinsong Su, Ruifeng Xu, Sheng Zhang, Yang Xiang, Zhongjian Miao |  |
| 32 |  |  [Lexicon-Enhanced Self-Supervised Training for Multilingual Dense Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.31) |  | 0 | Recent multilingual pre-trained models have shown better performance in various multilingual tasks. However, these models perform poorly on multilingual retrieval tasks due to lacking multilingual training data. In this paper, we propose to mine and generate self-supervised training data based on a... | Daxin Jiang, Houxing Ren, Jian Pei, Linjun Shou, Ming Gong, Ning Wu |  |
| 33 |  |  [Improve Interpretability of Neural Networks via Sparse Contrastive Coding](https://doi.org/10.18653/v1/2022.findings-emnlp.32) |  | 0 | Although explainable artificial intelligence (XAI) has achieved remarkable developments in recent years, there are few efforts have been devoted to the following problems, namely, i) how to develop an explainable method that could explain the black-box in a model-agnostic way? and ii) how to... | Jia Liu, Junhong Liu, Liang Jiang, Xi Peng, Yijie Lin, Zujie Wen |  |
| 34 |  |  [LEMON: Language-Based Environment Manipulation via Execution-Guided Pre-training](https://doi.org/10.18653/v1/2022.findings-emnlp.33) |  | 0 | Language-based environment manipulation requires agents to manipulate the environment following natural language instructions, which is challenging due to the huge space of the environments.To address this challenge, various approaches have been proposed in recent work. Although these approaches... | Bei Chen, JianGuang Lou, Qi Shi, Qian Liu, Ting Liu, Yu Zhang |  |
| 35 |  |  [CROP: Zero-shot Cross-lingual Named Entity Recognition with Multilingual Labeled Sequence Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.34) |  | 0 | Named entity recognition (NER) suffers from the scarcity of annotated training data, especially for low-resource languages without labeled data. Cross-lingual NER has been proposed to alleviate this issue by transferring knowledge from high-resource languages to low-resource languages via aligned... | Dongdong Zhang, Furu Wei, Hongcheng Guo, Jian Yang, Li Dong, Shaohan Huang, Shuming Ma, Yuwei Yin, Zhoujun Li |  |
| 36 |  |  [Handling and Presenting Harmful Text in NLP Research](https://doi.org/10.18653/v1/2022.findings-emnlp.35) |  | 0 | Text data can pose a risk of harm. However, the risks are not fully understood, and how to handle, present, and discuss harmful text in a safe way remains an unresolved issue in the NLP community. We provide an analytical framework categorising harms on three axes: (1) the harm type (e.g.,... | Abeba Birhane, Bertie Vidgen, Hannah Kirk, Leon Derczynski |  |
| 37 |  |  [Multimodal Contrastive Learning via Uni-Modal Coding and Cross-Modal Prediction for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2022.findings-emnlp.36) |  | 0 | Multimodal representation learning is a challenging task in which previous work mostly focus on either uni-modality pre-training or cross-modality fusion. In fact, we regard modeling multimodal representation as building a skyscraper, where laying stable foundation and designing the main structure... | Haifeng Hu, Ronghao Lin |  |
| 38 |  |  [Towards Unified Prompt Tuning for Few-shot Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.37) |  | 0 | Prompt-based fine-tuning has boosted the performance of Pre-trained Language Models (PLMs) on few-shot text classification by employing task-specific prompts. Yet, PLMs are unfamiliar with prompt-style expressions during pre-training, which limits the few-shot learning performance on downstream... | Chengyu Wang, Chuanqi Tan, Fei Yang, Fuli Luo, Jianing Wang, Ming Gao, Minghui Qiu, Qiuhui Shi, Songfang Huang |  |
| 39 |  |  [Can language models learn from explanations in context?](https://doi.org/10.18653/v1/2022.findings-emnlp.38) |  | 0 | Language Models (LMs) can perform new tasks by adapting to a few in-context examples. For humans, explanations that connect examples to task principles can improve learning. We therefore investigate whether explanations of few-shot examples can help LMs. We annotate questions from 40 challenging... | Andrew K. Lampinen, Antonia Creswell, Felix Hill, Ishita Dasgupta, James L. McClelland, Jane Wang, Kory W. Mathewson, Michael Henry Tessler, Stephanie C. Y. Chan |  |
| 40 |  |  [GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.39) |  | 0 | Recently, retrieval models based on dense representations are dominant in passage retrieval tasks, due to their outstanding ability in terms of capturing semantics of input text compared to the traditional sparse vector space models. A common practice of dense retrieval models is to exploit a... | Dongyan Zhao, Jiahao Liu, Jiduan Liu, Jingang Wang, Rui Yan, Wei Wu, Yang Yang |  |
| 41 |  |  [Linguistic Rules-Based Corpus Generation for Native Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2022.findings-emnlp.40) |  | 0 | Chinese Grammatical Error Correction (CGEC) is both a challenging NLP task and a common application in human daily life. Recently, many data-driven approaches are proposed for the development of CGEC research. However, there are two major limitations in the CGEC field: First, the lack of... | Ding Zhang, Haitao Zheng, Qingyu Zhou, Rongyi Sun, Ruiyang Liu, Shirong Ma, Shulin Huang, Yangning Li, Ying Shen, Yinghui Li, Yunbo Cao, Zhongli Li |  |
| 42 |  |  [Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding](https://doi.org/10.18653/v1/2022.findings-emnlp.41) |  | 0 | Temporal sentence grounding (TSG) aims to identify the temporal boundary of a specific segment from an untrimmed video by a sentence query. All existing works first utilize a sparse sampling strategy to extract a fixed number of video frames and then interact them with query for reasoning.However,... | Daizong Liu, Jiahao Zhu, Lichao Sun, Pan Zhou, Song Yang, Wenzheng Xu, Xing Di, Yao Wan, Yu Cheng, Zeyu Xiong, Zichuan Xu |  |
| 43 |  |  [System 1 + System 2 = Better World: Neural-Symbolic Chain of Logic Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.42) |  | 0 | Logical reasoning is a challenge for many current NLP neural network models since it requires more than the ability of learning informative representations from data. Inspired by the Dual Process Theory in cognitive science — which proposes that human cognition process involves two stages: an... | Wenyue Hua, Yongfeng Zhang |  |
| 44 |  |  [Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation](https://doi.org/10.18653/v1/2022.findings-emnlp.43) |  | 0 | Federated learning (FL) can be essential in knowledge representation, reasoning, and data mining applications over multi-source knowledge graphs (KGs). A recent study FedE first proposes an FL framework that shares entity embeddings of KGs across all clients. However, entity embedding sharing from... | Carl Yang, Hongyi Wang, Kai Zhang, Lichao Sun, Lifu Huang, Xun Chen, Yu Wang |  |
| 45 |  |  [TextHacker: Learning based Hybrid Local Search Algorithm for Text Hard-label Adversarial Attack](https://doi.org/10.18653/v1/2022.findings-emnlp.44) |  | 0 | Existing textual adversarial attacks usually utilize the gradient or prediction confidence to generate adversarial examples, making it hard to be deployed in real-world applications. To this end, we consider a rarely investigated but more rigorous setting, namely hard-label attack, in which the... | Kun He, Wanxiang Che, Xiaosen Wang, Zhen Yu |  |
| 46 |  |  [Visualizing the Obvious: A Concreteness-based Ensemble Model for Noun Property Prediction](https://doi.org/10.18653/v1/2022.findings-emnlp.45) |  | 0 | Neural language models encode rich knowledge about entities and their relationships which can be extracted from their representations using probing. Common properties of nouns (e.g., red strawberries, small ant) are, however, more challenging to extract compared to other types of knowledge because... | Artemis Panagopoulou, Chris CallisonBurch, Marianna Apidianaki, Mark Yatskar, Yue Yang |  |
| 47 |  |  [It's Better to Teach Fishing than Giving a Fish: An Auto-Augmented Structure-aware Generative Model for Metaphor Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.46) |  | 0 | Metaphor Detection aims to identify the metaphorical meaning of words in the sentence. Most existing work is discriminant models, which use the contextual semantic information extracted by transformers for classifications directly. Due to insufficient training data and corresponding paraphrases,... | Huawen Feng, Qianli Ma |  |
| 48 |  |  [Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks](https://doi.org/10.18653/v1/2022.findings-emnlp.47) |  | 0 | Natural language processing (NLP) models are known to be vulnerable to backdoor attacks, which poses a newly arisen threat to NLP models. Prior online backdoor defense methods for NLP models only focus on the anomalies at either the input or output level, still suffering from fragility to adaptive... | Sishuo Chen, Wenkai Yang, Xiaohan Bi, Xu Sun, Zhiyuan Zhang |  |
| 49 |  |  [Diving Deep into Modes of Fact Hallucinations in Dialogue Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.48) |  | 0 | Knowledge Graph(KG) grounded conversations often use large pre-trained models and usually suffer from fact hallucination. Frequently entities with no references in knowledge sources and conversation history are introduced into responses, thus hindering the flow of the conversation—existing work... | Rohini K. Srihari, Sougata Saha, Souvik Das |  |
| 50 |  |  [Representation Learning for Resource-Constrained Keyphrase Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.49) |  | 0 | State-of-the-art keyphrase generation methods generally depend on large annotated datasets, limiting their performance in domains with limited annotated data. To overcome this challenge, we design a data-oriented approach that first identifies salient information using retrieval-based corpus-level... | Di Wu, KaiWei Chang, Sunipa Dev, Wasi Uddin Ahmad |  |
| 51 |  |  [Systematicity in GPT-3's Interpretation of Novel English Noun Compounds](https://doi.org/10.18653/v1/2022.findings-emnlp.50) |  | 0 | Levin et al. (2019) show experimentally that the interpretations of novel English noun compounds (e.g., stew skillet), while not fully compositional, are highly predictable based on whether the modifier and head refer to artifacts or natural kinds. Is the large language model GPT-3 governed by the... | Christopher Potts, Riley Carlson, Siyan Li |  |
| 52 |  |  [CARE: Causality Reasoning for Empathetic Responses by Conditional Graph Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.51) |  | 0 | Recent approaches to empathetic response generation incorporate emotion causalities to enhance comprehension of both the user’s feelings and experiences. However, these approaches suffer from two critical issues. First, they only consider causalities between the user’s emotion and the user’s... | Jiashuo Wang, Wenjie Li, Yi Cheng |  |
| 53 |  |  [TransAdv: A Translation-based Adversarial Learning Framework for Zero-Resource Cross-Lingual Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.52) |  | 0 | Zero-Resource Cross-Lingual Named Entity Recognition aims at training an NER model of the target language using only labeled source language data and unlabeled target language data. Existing methods are mainly divided into three categories: model transfer based, data transfer based and knowledge... | Gongshen Liu, Huijia Zhu, Jintao Du, Yichun Zhao |  |
| 54 |  |  [BARLE: Background-Aware Representation Learning for Background Shift Out-of-Distribution Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.53) |  | 0 | Machine learning models often suffer from a performance drop when they are applied to out-of-distribution (OOD) samples, i.e., those drawn far away from the training data distribution. Existing OOD detection work mostly focuses on identifying semantic-shift OOD samples, e.g., instances from unseen... | Ahmed Abbasi, Hanyu Duan, Kar Yan Tam, Yi Yang |  |
| 55 |  |  [What Language Model to Train if You Have One Million GPU Hours?](https://doi.org/10.18653/v1/2022.findings-emnlp.54) |  | 0 | The crystallization of modeling methods around the Transformer architecture has been a boon for practitioners. Simple, well-motivated architectural variations can transfer across tasks and scale, increasing the impact of modeling research. However, with the emergence of state-of-the-art 100B+... | Colin Raffel, Daniel Hesslow, Hady Elsahar, Iz Beltagy, Jaesung Tae, Jason Phang, Julien Launay, Lintang Sutawika, Lucile Saulnier, M. Saiful Bari, Niklas Muennighoff, Ofir Press, Sheng Shen, Stas Bekman, Stella Biderman, Teven Le Scao, Thomas Wang, Victor Sanh, Zheng Xin Yong |  |
| 56 |  |  [Enhancing Out-of-Distribution Detection in Natural Language Understanding via Implicit Layer Ensemble](https://doi.org/10.18653/v1/2022.findings-emnlp.55) |  | 0 | Out-of-distribution (OOD) detection aims to discern outliers from the intended data distribution, which is crucial to maintaining high reliability and a good user experience.Most recent studies in OOD detection utilize the information from a single representation that resides in the penultimate... | Choonghyun Park, Hyunsoo Cho, Jaewook Kang, Kang Min Yoo, Sanggoo Lee, Taeuk Kim |  |
| 57 |  |  [Contrastive Demonstration Tuning for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.56) |  | 0 | Pretrained language models can be effectively stimulated by textual prompts or demonstrations, especially in low-data scenarios. Recent works have focused on automatically searching discrete or continuous prompts or optimized verbalizers, yet studies for the demonstration are still limited.... | Chuanqi Tan, Huajun Chen, Ningyu Zhang, Siyuan Cheng, Xiaozhuan Liang, Zhenru Zhang |  |
| 58 |  |  [Detect-Localize-Repair: A Unified Framework for Learning to Debug with CodeT5](https://doi.org/10.18653/v1/2022.findings-emnlp.57) |  | 0 | Automated software debugging is a crucial task for improving the productivity of software developers. Many neural-based techniques have been proven effective for debugging-related tasks such as bug localization and program repair (or bug fixing). However, these techniques often focus only on either... | Nghi Bui, Steven C. H. Hoi, Yue Wang |  |
| 59 |  |  [Influence Functions for Sequence Tagging Models](https://doi.org/10.18653/v1/2022.findings-emnlp.58) |  | 0 | Many standard tasks in NLP (e.g., Named Entity Recognition, Part-of-Speech tagging, and Semantic Role Labeling) are naturally framed as sequence tagging problems. However, there has been comparatively little work on interpretability methods for sequence tagging models. In this paper, we extend... | Ani Nenkova, Byron C. Wallace, Sarthak Jain, Varun Manjunatha |  |
| 60 |  |  [Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.59) |  | 0 | Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with... | Matt Gardner, Robert L. Logan IV, Sameer Singh, Yasaman Razeghi |  |
| 61 |  |  [Syntactic and Semantic Uniformity for Semantic Parsing and Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.60) |  | 0 | This paper proposes a data representation framework for semantic parsing and task-oriented dialogue systems, aiming to achieve a uniform representation for syntactically and semantically diverse machine-readable formats.Current NLP systems heavily rely on adapting pre-trained language models to... | Bowen Chen, Yusuke Miyao |  |
| 62 |  |  [Knowledge-Rich Self-Supervision for Biomedical Entity Linking](https://doi.org/10.18653/v1/2022.findings-emnlp.61) |  | 0 | Entity linking faces significant challenges such as prolific variations and prevalent ambiguities, especially in high-value domains with myriad entities. Standard classification approaches suffer from the annotation bottleneck and cannot effectively handle unseen entities. Zero-shot entity linking... | Cliff Wong, Hao Cheng, Hoifung Poon, Jianfeng Gao, Jinfeng Xiao, Sheng Zhang, Shikhar Vashishth, Tristan Naumann, Xiaodong Liu |  |
| 63 |  |  [ARTIST: A Transformer-based Chinese Text-to-Image Synthesizer Digesting Linguistic and World Knowledge](https://doi.org/10.18653/v1/2022.findings-emnlp.62) |  | 0 | Text-to-Image Synthesis (TIS) is a popular task to convert natural language texts into realistic images. Recently, transformer-based TIS models (such as DALL-E) have been proposed using the encoder-decoder architectures. Yet, these billion-scale TIS models are difficult to tune and deploy in... | Chengyu Wang, Jun Huang, Lei Li, Ming Gao, Minghui Qiu, Tingting Liu, Xiangru Zhu, Yanghua Xiao |  |
| 64 |  |  [From Spelling to Grammar: A New Framework for Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2022.findings-emnlp.63) |  | 0 | Chinese Grammatical Error Correction (CGEC) aims to generate a correct sentence from an erroneous sequence, where different kinds of errors are mixed. This paper divides the CGEC task into two steps, namely spelling error correction and grammatical error correction. We firstly propose a novel... | Xiuyu Wu, Yunfang Wu |  |
| 65 |  |  [Language Models Are Poor Learners of Directional Inference](https://doi.org/10.18653/v1/2022.findings-emnlp.64) |  | 0 | We examine LMs’ competence of directional predicate entailments by supervised fine-tuning with prompts. Our analysis shows that contrary to their apparent success on standard NLI, LMs show limited ability to learn such directional inference; moreover, existing datasets fail to test directionality,... | Mark Steedman, Mohammad Javad Hosseini, Sabine Weber, Tianyi Li |  |
| 66 |  |  [Wish I Can Feel What You Feel: A Neural Approach for Empathetic Response Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.65) |  | 0 | Expressing empathy is important in everyday conversations, and exploring how empathy arises is crucial in automatic response generation. Most previous approaches consider only a single factor that affects empathy. However, in practice, empathy generation and expression is a very complex and dynamic... | Chunfeng Liang, Yangbin Chen |  |
| 67 |  |  [Measuring and Improving Semantic Diversity of Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.66) |  | 0 | Response diversity has become an important criterion for evaluating the quality of open-domain dialogue generation models. However, current evaluation metrics for response diversity often fail to capture the semantic diversity of generated responses, as they mainly consider lexical aspects of the... | Beomsu Kim, Buru Chang, Seungju Han |  |
| 68 |  |  [Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training](https://doi.org/10.18653/v1/2022.findings-emnlp.67) |  | 0 | Visual question answering (VQA) is a hallmark of vision and language reasoningand a challenging task under the zero-shot setting.We propose Plug-and-Play VQA (PNP-VQA),a modular framework for zero-shot VQA.In contrast to most existing works, which require substantial adaptation of pretrained... | Anthony Meng Huat Tiong, Boyang Li, Junnan Li, Silvio Savarese, Steven C. H. Hoi |  |
| 69 |  |  [TSGP: Two-Stage Generative Prompting for Unsupervised Commonsense Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.68) |  | 0 | Without training on labeled task data, unsupervised commonsense question answering seems challenging since it requires commonsense knowledge beyond the context of questions. Previous methods typically retrieved from traditional knowledge bases or used pre-trained language models (PrLMs) to generate... | Le Qi, Qi Shi, Yu Zhang, Yueqing Sun |  |
| 70 |  |  [Subword-Delimited Downsampling for Better Character-Level Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.69) |  | 0 | Subword-level models have been the dominant paradigm in NLP. However, character-level models have the benefit of seeing each character individually, providing the model with more detailed information that ultimately could lead to better models. Recent works have shown character-level models to be... | Antonio Toral, Gertjan van Noord, Lukas Edman |  |
| 71 |  |  [Autoregressive Structured Prediction with Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.70) |  | 0 | Recent years have seen a paradigm shift in NLP towards using pretrained language models (PLM) for a wide range of tasks. However, there are many difficult design decisions to represent structures (e.g. tagged text, coreference chains) in a way such that they can be captured by PLMs. Prior work on... | Mrinmaya Sachan, Nicholas Monath, Ryan Cotterell, Tianyu Liu, Yuchen Eleanor Jiang |  |
| 72 |  |  [XDoc: Unified Pre-training for Cross-Format Document Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.71) |  | 0 | The surge of pre-training has witnessed the rapid development of document understanding recently. Pre-training and fine-tuning framework has been effectively used to tackle texts in various formats, including plain texts, document texts, and web texts. Despite achieving promising performance,... | Cha Zhang, Furu Wei, Jingye Chen, Lei Cui, Tengchao Lv |  |
| 73 |  |  [A Few More Examples May Be Worth Billions of Parameters](https://doi.org/10.18653/v1/2022.findings-emnlp.72) |  | 0 | We investigate the dynamics of increasing the number of model parameters versus the number of labeled examples across a wide variety of tasks. Our exploration reveals that while scaling parameters consistently yields performance improvements, the contribution of additional examples highly depends... | Omer Levy, Patrick Lewis, Sebastian Riedel, Yuval Kirstain |  |
| 74 |  |  [MCP: Self-supervised Pre-training for Personalized Chatbots with Multi-level Contrastive Sampling](https://doi.org/10.18653/v1/2022.findings-emnlp.73) |  | 0 | Personalized chatbots focus on endowing the chatbots with a consistent personality to behave like real users and further act as personal assistants. Previous studies have explored generating implicit user profiles from the user’s dialogue history for building personalized chatbots. However, these... | Yutao Zhu, Zhaoheng Huang, Zhengyi Ma, Zhicheng Dou |  |
| 75 |  |  [ExpertPLM: Pre-training Expert Representation for Expert Finding](https://doi.org/10.18653/v1/2022.findings-emnlp.74) |  | 0 | Expert Finding is an important task in Community Question Answering (CQA) platforms, which could help route questions to potential users to answer. The key is to learn representations of experts based on their historical answered questions accurately. In this paper, inspired by the strong text... | Hongtao Liu, Qiyao Peng |  |
| 76 |  |  [You Truly Understand What I Need : Intellectual and Friendly Dialog Agents grounding Persona and Knowledge](https://doi.org/10.18653/v1/2022.findings-emnlp.75) |  | 0 | To build a conversational agent that interacts fluently with humans, previous studies blend knowledge or personal profile into the pre-trained language model. However, the model that considers knowledge and persona at the same time is still limited, leading to hallucination and a passive way of... | DongHoon Shin, Dongyub Lee, Heuiseok Lim, Hyesung Ji, Jinsung Kim, Jungwoo Lim, Myunghoon Kang, Seung Won Jeong, Seungryong Kim, Yoonna Jang, Yuna Hur |  |
| 77 |  |  [Faithful to the Document or to the World? Mitigating Hallucinations via Entity-Linked Knowledge in Abstractive Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.76) |  | 0 | Existing abstractive summarization systems are hampered by content hallucinations in which models generate text that is not directly inferable from the source alone. Annotations from prior work have shown that some of these hallucinations, while being ‘unfaithful’ to the source, are nonetheless... | John Wieting, Pat Verga, Yue Dong |  |
| 78 |  |  [RL with KL penalties is better viewed as Bayesian inference](https://doi.org/10.18653/v1/2022.findings-emnlp.77) |  | 0 | Reinforcement learning (RL) is frequently employed in fine-tuning large language models (LMs), such as GPT-3, to penalize them for undesirable features of generated sequences, such as offensiveness, social bias, harmfulness or falsehood. The RL formulation involves treating the LM as a policy and... | Christopher L. Buckley, Ethan Perez, Tomasz Korbak |  |
| 79 |  |  [Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.78) |  | 0 | With the recent success of dense retrieval methods based on bi-encoders, studies have applied this approach to various interesting downstream retrieval tasks with good efficiency and in-domain effectiveness.Recently, we have also seen the presence of dense retrieval models in Math Information... | JhengHong Yang, Jimmy Lin, Wei Zhong, Yuqing Xie |  |
| 80 |  |  [Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem](https://doi.org/10.18653/v1/2022.findings-emnlp.79) |  | 0 | Math word problem solver requires both precise relation reasoning about quantities in the text and reliable generation for the diverse equation. Current sequence-to-tree or relation extraction methods regard this only from a fixed view, struggling to simultaneously handle complex semantics and... | Qingpeng Nong, Weiming Lu, Wenqi Zhang, Xiaoxia Cheng, Yanna Ma, Yongliang Shen, Zeqi Tan |  |
| 81 |  |  [Few-shot initializing of Active Learner via Meta-Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.80) |  | 0 | Despite the important evolutions in few-shot and zero-shot learning techniques, domain specific applications still require expert knowledge and significant effort in annotating and labeling a large volume of unstructured textual data. To mitigate this problem, active learning, and meta-learning... | George Tsatsaronis, Vikrant Yadav, Zi Long Zhu, Zubair Afzal |  |
| 82 |  |  [Bootstrapping meaning through listening: Unsupervised learning of spoken sentence embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.81) |  | 0 | Inducing semantic representations directly from speech signals is a highly challenging task but has many useful applications in speech mining and spoken language understanding. This study tackles the unsupervised learning of semantic representations for spoken utterances. Through converting speech... | ChiaWen Lo, Cong Zhang, Jian Zhu, Yadong Liu, Zuoyu Tian |  |
| 83 |  |  [Progressive Sentiment Analysis for Code-Switched Text Data](https://doi.org/10.18653/v1/2022.findings-emnlp.82) |  | 0 | Multilingual transformer language models have recently attracted much attention from researchers and are used in cross-lingual transfer learning for many NLP tasks such as text classification and named entity recognition.However, similar methods for transfer learning from monolingual text to... | Dheeraj Mekala, Jingbo Shang, Sudhanshu Ranjan |  |
| 84 |  |  [Knowledge Stimulated Contrastive Prompting for Low-Resource Stance Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.83) |  | 0 | Stance Detection Task (SDT) aims at identifying the stance of the sentence towards a specific target and is usually modeled as a classification problem. Backgound knowledge is often necessary for stance detection with respect to a specific target, especially when there is no target explicitly... | Fei Xu, Kai Zheng, Qingfeng Sun, Yaming Yang |  |
| 85 |  |  [WSpeller: Robust Word Segmentation for Enhancing Chinese Spelling Check](https://doi.org/10.18653/v1/2022.findings-emnlp.84) |  | 0 | Chinese spelling check (CSC) detects and corrects spelling errors in Chinese texts. Previous approaches have combined character-level phonetic and graphic information, ignoring the importance of segment-level information. According to our pilot study, spelling errors are always associated with... | Fangfang Li, Junwen Duan, Minlie Huang, Xingliang Mao, Youran Shan |  |
| 86 |  |  [Extracting Trigger-sharing Events via an Event Matrix](https://doi.org/10.18653/v1/2022.findings-emnlp.85) |  | 0 | A growing interest emerges in event extraction which aims to extract multiple events with triggers and arguments. Previous methods mitigate the problem of multiple events extraction by predicting the arguments conditioned on the event trigger and event type, assuming that these arguments belong to... | Jun Xu, Mengshu Sun, Taifeng Wang, Wei Chu, Weidi Xu |  |
| 87 |  |  [TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.86) |  | 0 | Knowledge graph embedding (KGE) aims to learn continuous vector representations of relations and entities in knowledge graph (KG). Recently, transition-based KGE methods have become popular and achieved promising performance. However, scoring patterns like TransE are not suitable for complex... | Dongliang Xu, Qing Yang, Xuanyu Zhang |  |
| 88 |  |  [Sequential Topic Selection Model with Latent Variable for Topic-Grounded Dialogue](https://doi.org/10.18653/v1/2022.findings-emnlp.87) |  | 0 | Recently, topic-grounded dialogue system has attracted significant attention due to its effectiveness in predicting the next topic to yield better responses via the historical context and given topic sequence. However, almost all existing topic prediction solutions focus on only the current... | Wei Wei, XianLing Mao, Xiaofei Wen |  |
| 89 |  |  [Robust Task-Oriented Dialogue Generation with Contrastive Pre-training and Adversarial Filtering](https://doi.org/10.18653/v1/2022.findings-emnlp.88) |  | 0 | Data artifacts incentivize machine learning models to learn non-transferable generalizations by taking advantage of shortcuts in the data, andthere is growing evidence that data artifacts play a role for the strong results that deep learning models achieve in recent natural language processing... | Jey Han Lau, Sarah M. Erfani, Shiquan Yang, Xinting Huang |  |
| 90 |  |  [STAR: SQL Guided Pre-Training for Context-dependent Text-to-SQL Parsing](https://doi.org/10.18653/v1/2022.findings-emnlp.89) |  | 0 | In this paper, we propose a novel SQL guided pre-training framework STAR for context-dependent text-to-SQL parsing, which leverages contextual information to enrich natural language (NL) utterance and table schema representations for text-to-SQL conversations. Concretely, we propose two novel... | Binhua Li, Binyuan Hui, Bowen Li, Fei Huang, Luo Si, Min Yang, Weijie Li, Xiangyu Li, Yongbin Li, Zefeng Cai, Zheng Cao |  |
| 91 |  |  [Is MultiWOZ a Solved Task? An Interactive TOD Evaluation Framework with User Simulator](https://doi.org/10.18653/v1/2022.findings-emnlp.90) |  | 0 | Task-Oriented Dialogue (TOD) systems are drawing more and more attention in recent studies.Current methods focus on constructing pre-trained models or fine-tuning strategies while the evaluation of TOD is limited by a policy mismatch problem.That is, during evaluation, the user utterances are from... | Feng Gao, Guofeng Quan, Linyang Li, Qinyuan Cheng, Xiaofeng Mou, Xipeng Qiu |  |
| 92 |  |  [Translating Hanja Historical Documents to Contemporary Korean and English](https://doi.org/10.18653/v1/2022.findings-emnlp.91) |  | 0 | The Annals of Joseon Dynasty (AJD) contain the daily records of the Kings of Joseon, the 500-year kingdom preceding the modern nation of Korea.The Annals were originally written in an archaic Korean writing system, ‘Hanja’, and were translated into Korean from 1968 to 1993.The resulting translation... | Alice Oh, Haneul Yoo, Jiho Jin, JinYeong Bak, Juhee Son, Kyunghyun Cho |  |
| 93 |  |  [Exploring Compositional Image Retrieval with Hybrid Compositional Learning and Heuristic Negative Mining](https://doi.org/10.18653/v1/2022.findings-emnlp.92) |  | 0 | Compositional image retrieval (CIR) is a challenging retrieval task, where the query is composed of a reference image and a modification text, and the target is another image reflecting the modification to the reference image. Due to the great success of the pre-trained vision-and-language model... | Chao Wang, Ehsan Nezhadarya, Shengdong Zhang, Tanmana Sadhu |  |
| 94 |  |  [Outlier Dimensions that Disrupt Transformers are Driven by Frequency](https://doi.org/10.18653/v1/2022.findings-emnlp.93) |  | 0 | While Transformer-based language models are generally very robust to pruning, there is the recently discovered outlier phenomenon: disabling only 48 out of 110M parameters in BERT-base drops its performance by nearly 30% on MNLI. We replicate the original evidence for the outlier phenomenon and we... | Aleksandr Drozd, Anna Rogers, Felice Dell'Orletta, Giovanni Puccetti |  |
| 95 |  |  [MiST: a Large-Scale Annotated Resource and Neural Models for Functions of Modal Verbs in English Scientific Text](https://doi.org/10.18653/v1/2022.findings-emnlp.94) |  | 0 | Modal verbs (e.g., can, should or must) occur highly frequently in scientific articles. Decoding their function is not straightforward: they are often used for hedging, but they may also denote abilities and restrictions. Understanding their meaning is important for accurate information extraction... | Annemarie Friedrich, Nicole Macher, Sophie Henning, Stefan Grünewald |  |
| 96 |  |  [Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts](https://doi.org/10.18653/v1/2022.findings-emnlp.95) |  | 0 | Prompt tuning is a parameter-efficient tuning (PETuning) method for utilizing pre-trained models (PTMs) that simply prepends a soft prompt to the input and only optimizes the prompt to adapt PTMs to downstream tasks. Although it is parameter- and deployment-efficient, its performance still lags... | Tianxiang Sun, Xiangyang Liu, Xipeng Qiu, Xuanjing Huang |  |
| 97 |  |  [MICO: A Multi-alternative Contrastive Learning Framework for Commonsense Knowledge Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.96) |  | 0 | Commonsense reasoning tasks such as commonsense knowledge graph completion and commonsense question answering require powerful representation learning. In this paper, we propose to learn commonsense knowledge representation by MICO, a Multi-alternative contrastIve learning framework on COmmonsense... | Hongming Zhang, Tianqing Fang, Tong Zhang, Yangqiu Song, Ying Su, Zihao Wang |  |
| 98 |  |  [Leveraging Only the Category Name for Aspect Detection through Prompt-based Constrained Clustering](https://doi.org/10.18653/v1/2022.findings-emnlp.97) |  | 0 | Aspect category detection (ACD) aims to automatically identify user-concerned aspects from online reviews, which is of great value for evaluating the fine-grained performance of a product. The most recent solutions tackle this problem via weakly supervised methods, achieving remarkable improvement... | Lujia Pan, Pengyun Wang, Yadao Wang, Yasheng Wang, Yazheng Li, Yong Dai, Zenglin Xu |  |
| 99 |  |  [Controllable Factuality in Document-Grounded Dialog Systems Using a Noisy Channel Model](https://doi.org/10.18653/v1/2022.findings-emnlp.98) |  | 0 | In this work, we present a model for document-grounded response generation in dialog that is decomposed into two components according to Bayes’ theorem.One component is a traditional ungrounded response generation model and the other component models the reconstruction of the grounding document... | Christian Dugast, David Thulke, Hermann Ney, Nico Daheim |  |
| 100 |  |  [Transformer Language Models without Positional Encodings Still Learn Positional Information](https://doi.org/10.18653/v1/2022.findings-emnlp.99) |  | 0 | Causal transformer language models (LMs), such as GPT-3, typically require some form of positional encoding, such as positional embeddings. However, we show that LMs without any explicit positional encoding are still competitive with standard models and that this phenomenon is robust across... | Adi Haviv, Ofir Press, Omer Levy, Ori Ram, Peter Izsak |  |
| 101 |  |  [Beyond Model Interpretability: On the Faithfulness and Adversarial Robustness of Contrastive Textual Explanations](https://doi.org/10.18653/v1/2022.findings-emnlp.100) |  | 0 | Contrastive explanation methods go beyond transparency and address the contrastive aspect of explanations. Such explanations are emerging as an attractive option to provide actionable change to scenarios adversely impacted by classifiers’ decisions. However, their extension to textual data is... | Julia El Zini, Mariette Awad |  |
| 102 |  |  [How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers](https://doi.org/10.18653/v1/2022.findings-emnlp.101) |  | 0 | The attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language... | Daniel Rotem, Hao Peng, Ivan Montero, Jungo Kasai, Michael Hassid, Noah A. Smith, Roy Schwartz |  |
| 103 |  |  [What Has Been Enhanced in my Knowledge-Enhanced Language Model?](https://doi.org/10.18653/v1/2022.findings-emnlp.102) |  | 0 | A number of knowledge integration (KI) methods have recently been proposed to incorporate external knowledge into pretrained language models (LMs). Even though knowledge-enhanced LMs (KELMs) outperform base LMs on knowledge-intensive tasks, the inner-workings of these KI methods are not... | Guoji Fu, Mrinmaya Sachan, Yifan Hou |  |
| 104 |  |  [Towards Generalized Open Information Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.103) |  | 0 | Open Information Extraction (OpenIE) facilitates the open-domain discovery of textual facts. However, the prevailing solutions evaluate OpenIE models on in-domain test sets aside from the training corpus, which certainly violates the initial task principle of domain-independence. In this paper, we... |  |  |
| 105 |  |  [BioLORD: Learning Ontological Representations from Definitions for Biomedical Concepts and their Textual Descriptions](https://doi.org/10.18653/v1/2022.findings-emnlp.104) |  | 0 | This work introduces BioLORD, a new pre-training strategy for producing meaningful representations for clinical sentences and biomedical concepts. State-of-the-art methodologies operate by maximizing the similarity in representation of names referring to the same concept, and preventing collapse... | François Remy, Kris Demuynck, Thomas Demeester |  |
| 106 |  |  [Improving the Extraction of Supertags for Constituency Parsing with Linear Context-Free Rewriting Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.105) |  | 0 | In parsing phrase structures, supertagging achieves a symbiosis between the interpretability of formal grammars and the accuracy and speed of more recent neural models.The approach was only recently transferred to parsing discontinuous constituency structures with linear context-free rewriting... | Thomas Ruprecht |  |
| 107 |  |  [Mask More and Mask Later: Efficient Pre-training of Masked Language Models by Disentangling the [MASK] Token](https://doi.org/10.18653/v1/2022.findings-emnlp.106) |  | 0 | The pre-training of masked language models (MLMs) consumes massive computation to achieve good results on downstream NLP tasks, resulting in a large carbon footprint. In the vanilla MLM, the virtual tokens, [MASK]s, act as placeholders and gather the contextualized information from unmasked tokens... | Baohao Liao, Christof Monz, David Thulke, Hermann Ney, Sanjika Hewavitharana |  |
| 108 |  |  [SMSMix: Sense-Maintained Sentence Mixup for Word Sense Disambiguation](https://doi.org/10.18653/v1/2022.findings-emnlp.107) |  | 0 | Word Sense Disambiguation (WSD) is an NLP task aimed at determining the correct sense of a word in a sentence from discrete sense choices. Although current systems have attained unprecedented performances for such tasks, the nonuniform distribution of word senses during training generally results... | Chang Dong Yoo, Eunseop Yoon, Hee Suk Yoon, John B. Harvill, Mark HasegawaJohnson, Sunjae Yoon |  |
| 109 |  |  [On the Effectiveness of Automated Metrics for Text Generation Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.108) |  | 0 | A major challenge in the field of Text Generation is evaluation, because we lack a sound theory that can be leveraged to extract guidelines for evaluation campaigns. In this work, we propose a first step towards such a theory that incorporates different sources of uncertainty, such as imperfect... | Don Tuggener, Jan Deriu, Mark Cieliebak, Pius von Däniken |  |
| 110 |  |  [Residual Learning of Neural Text Generation with n-gram Language Model](https://doi.org/10.18653/v1/2022.findings-emnlp.109) |  | 0 | N-gram language models (LM) has been largely superseded by neural LMs as the latter exhibits better performance. However, we find that n-gram models can achieve satisfactory performance on a large proportion of testing cases, indicating they have already captured abundant knowledge of the language... | Deng Cai, Huayang Li, Jin Xu, Taro Watanabe |  |
| 111 |  |  [DiffG-RL: Leveraging Difference between Environment State and Common Sense](https://doi.org/10.18653/v1/2022.findings-emnlp.110) |  | 0 | Taking into account background knowledge as the context has always been an important part of solving tasks that involve natural language. One representative example of such tasks is text-based games, where players need to make decisions based on both description text previously shown in the game,... | Daiki Kimura, Michiaki Tatsubori, Tsunehiko Tanaka |  |
| 112 |  |  [Unsupervised Syntactically Controlled Paraphrase Generation with Abstract Meaning Representations](https://doi.org/10.18653/v1/2022.findings-emnlp.111) |  | 0 | Syntactically controlled paraphrase generation has become an emerging research direction in recent years. Most existing approaches require annotated paraphrase pairs for training and are thus costly to extend to new domains. Unsupervised approaches, on the other hand, do not need paraphrase pairs... | Anoop Kumar, Aram Galstyan, KaiWei Chang, KuanHao Huang, Sriram Venkatapathy, Varun Iyer |  |
| 113 |  |  [Can AMR Assist Legal and Logical Reasoning?](https://doi.org/10.18653/v1/2022.findings-emnlp.112) |  | 0 | Abstract Meaning Representation (AMR) has been shown to be useful for many downstream tasks. In this work, we explore the use of AMR for legal and logical reasoning. Specifically, we investigate if AMR can help capture logical relationships on multiple choice question answering (MCQA) tasks. We... | Daniel Hershcovich, Hugo López, Nikolaus Schrack, Ruixiang Cui |  |
| 114 |  |  [Data Selection Curriculum for Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.113) |  | 0 | Neural Machine Translation (NMT) models are typically trained on heterogeneous data that are concatenated and randomly shuffled. However, not all of the training data are equally useful to the model. Curriculum training aims to present the data to the NMT models in a meaningful order. In this work,... | James Cross, Philipp Koehn, Shafiq R. Joty, Shruti Bhosale, Tasnim Mohiuddin, Vishrav Chaudhary |  |
| 115 |  |  [Text Editing as Imitation Game](https://doi.org/10.18653/v1/2022.findings-emnlp.114) |  | 0 | Text editing, such as grammatical error correction, arises naturally from imperfect textual data. Recent works frame text editing as a multi-round sequence tagging task, where operations – such as insertion and substitution – are represented as a sequence of tags. While achieving good results, this... | Bin Tang, Bo Yuan, Jie Fu, Longtao Huang, Ning Shi, Yewen Pu, Zhouhan Lin |  |
| 116 |  |  [Seeded Hierarchical Clustering for Expert-Crafted Taxonomies](https://doi.org/10.18653/v1/2022.findings-emnlp.115) |  | 0 | Practitioners from many disciplines (e.g., political science) use expert-crafted taxonomies to make sense of large, unlabeled corpora. In this work, we study Seeded Hierarchical Clustering (SHC): the task of automatically fitting unlabeled data to such taxonomies using a small set of labeled... | Amith Ananthram, Anish Saha, Emily Allaway, Heng Ji, Kathleen R. McKeown |  |
| 117 |  |  [Knowledge Graph Generation From Text](https://doi.org/10.18653/v1/2022.findings-emnlp.116) |  | 0 | In this work we propose a novel end-to-end multi-stage Knowledge Graph (KG) generation system from textual inputs, separating the overall process into two stages. The graph nodes are generated first using pretrained language model, followed by a simple edge construction head, enabling efficient KG... | Igor Melnyk, Payel Das, Pierre L. Dognin |  |
| 118 |  |  [DialogueGAT: A Graph Attention Network for Financial Risk Prediction by Modeling the Dialogues in Earnings Conference Calls](https://doi.org/10.18653/v1/2022.findings-emnlp.117) |  | 0 | Financial risk prediction is an essential task for risk management in capital markets. While traditional prediction models are built based on the hard information of numerical data, recent studies have shown that the soft information of verbal cues in earnings conference calls is significant for... | Yang Bao, Yunxin Sang |  |
| 119 |  |  [Investigating Ensemble Methods for Model Robustness Improvement of Text Classifiers](https://doi.org/10.18653/v1/2022.findings-emnlp.118) |  | 0 | Large pre-trained language models have shown remarkable performance over the past few years. These models, however, sometimes learn superficial features from the dataset and cannot generalize to the distributions that are dissimilar to the training scenario. There have been several approaches... | Jieyu Zhao, Jilin Chen, KaiWei Chang, Xuezhi Wang, Yao Qin |  |
| 120 |  |  [Adaptive Ranking-based Sample Selection for Weakly Supervised Class-imbalanced Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.119) |  | 0 | To obtain a large amount of training labels inexpensively, researchers have recently adopted the weak supervision (WS) paradigm, which leverages labeling rules to synthesize training labels rather than using individual annotations to achieve competitive results for natural language processing (NLP)... | Jieyu Zhang, Linxin Song, Masayuki Goto, Tianxiang Yang |  |
| 121 |  |  [ComFact: A Benchmark for Linking Contextual Commonsense Knowledge](https://doi.org/10.18653/v1/2022.findings-emnlp.120) |  | 0 | Understanding rich narratives, such as dialogues and stories, often requires natural language processing systems to access relevant knowledge from commonsense knowledge graphs. However, these systems typically retrieve facts from KGs using simple heuristics that disregard the complex challenges of... | Antoine Bosselut, Hiromi Wakaki, Jena D. Hwang, Saya Kanno, Silin Gao, Yuki Mitsufuji |  |
| 122 |  |  [Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.121) |  | 0 | How to usefully encode compositional task structure has long been a core challenge in AI. Recent work in chain of thought prompting has shown that for very large neural language models (LMs), explicitly demonstrating the inferential steps involved in a target task may improve performance over... | David Demeter, Doug Downey, Larry Birnbaum, Victor S. Bursztyn |  |
| 123 |  |  [Topic Taxonomy Expansion via Hierarchy-Aware Topic Phrase Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.122) |  | 0 | Topic taxonomies display hierarchical topic structures of a text corpus and provide topical knowledge to enhance various NLP applications. To dynamically incorporate new topic information, several recent studies have tried to expand (or complete) a topic taxonomy by inserting emerging topics... | Dongha Lee, Hwanjo Yu, Jiaming Shen, Jiawei Han, Seonghyeon Lee, Susik Yoon |  |
| 124 |  |  [Language as a fingerprint: Self-supervised learning of user encodings using transformers](https://doi.org/10.18653/v1/2022.findings-emnlp.123) |  | 0 | The way we talk carries information about who we are. Demographics, personality, clinical conditions, political preferences influence what we speak about and how, suggesting that many individual attributes could be inferred from adequate encodings of linguistic behavior. Conversely, conditioning... | Roberta Rocca, Tal Yarkoni |  |
| 125 |  |  [Hyperdecoders: Instance-specific decoders for multi-task NLP](https://doi.org/10.18653/v1/2022.findings-emnlp.124) |  | 0 | We investigate input-conditioned hypernetworks for multi-tasking in NLP, generating parameter-efficient adaptations for a decoder using a hypernetwork conditioned on the output of an encoder. This approach produces a unique decoder adaptation for every input instance, allowing the network a larger... | Hamish Ivison, Matthew E. Peters |  |
| 126 |  |  [Evaluating the Faithfulness of Importance Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining](https://doi.org/10.18653/v1/2022.findings-emnlp.125) |  | 0 | To explain NLP models a popular approach is to use importance measures, such as attention, which inform input tokens are important for making a prediction. However, an open question is how well these explanations accurately reflect a model’s logic, a property called faithfulness. To answer this... | Andreas Madsen, Nicholas Meade, Siva Reddy, Vaibhav Adlakha |  |
| 127 |  |  [Towards Explaining Subjective Ground of Individuals on Social Media](https://doi.org/10.18653/v1/2022.findings-emnlp.126) |  | 0 | Large-scale language models have been reducing the gap between machines and humans in understanding the real world, yet understanding an individual’s theory of mind and behavior from text is far from being resolved. This research proposes a neural model—Subjective Ground Attention—that learns... | Dan Goldwasser, Younghun Lee |  |
| 128 |  |  [Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot ICD Coding](https://doi.org/10.18653/v1/2022.findings-emnlp.127) |  | 0 | Automatic International Classification of Diseases (ICD) coding aims to assign multiple ICD codes to a medical note with average length of 3,000+ tokens. This task is challenging due to a high-dimensional space of multi-label assignment (tens of thousands of ICD codes) and the long-tail challenge:... | Avijit Mitra, Bhanu Pratap Singh Rawat, Hong Yu, Shufan Wang, Zhichao Yang |  |
| 129 |  |  [Do Language Models Understand Measurements?](https://doi.org/10.18653/v1/2022.findings-emnlp.128) |  | 0 | Recent success of pre-trained language models (PLMs) has stimulated interest in their ability to understand and work with numbers. Yet, the numerical reasoning over measurements has not been formally studied despite their importance. In this study, we show that PLMs lack the capability required for... | Edward Choi, Seungwoo Ryu, Sungjin Park |  |
| 130 |  |  [Reconciliation of Pre-trained Models and Prototypical Neural Networks in Few-shot Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.129) |  | 0 | Incorporating large-scale pre-trained models with the prototypical neural networks is a de-facto paradigm in few-shot named entity recognition. Existing methods, unfortunately, are not aware of the fact that embeddings from pre-trained models contain a prominently large amount of information... | Jiancheng Lv, Jie Fu, Wenqiang Lei, Youcheng Huang |  |
| 131 |  |  [HCL-TAT: A Hybrid Contrastive Learning Method for Few-shot Event Detection with Task-Adaptive Threshold](https://doi.org/10.18653/v1/2022.findings-emnlp.130) |  | 0 | Event detection has been suffering from constantly emerging event types with lack of sufficient data. Existing works formulate the new problem as few-shot event detection (FSED), and employ two-stage or unified models based on meta-learning to address the problem. However, these methods fall far... | Dangyang Chen, Rui Fang, Ruihan Zhang, Wei Wei, XianLing Mao |  |
| 132 |  |  [Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots](https://doi.org/10.18653/v1/2022.findings-emnlp.131) |  | 0 | This paper introduces Doc2Bot, a novel dataset for building machines that help users seek information via conversations. This is of particular interest for companies and organizations that own a large number of manuals or instruction books. Despite its potential, the nature of our task poses... | CamTu Nguyen, Fei Huang, Haiyang Yu, Haomin Fu, Jian Sun, Luo Si, Yeqin Zhang, Yongbin Li |  |
| 133 |  |  [DualNER: A Dual-Teaching framework for Zero-shot Cross-lingual Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.132) |  | 0 | We present DualNER, a simple and effective framework to make full use of both annotated source language corpus and unlabeled target language text for zero-shot cross-lingual named entity recognition (NER). In particular, we combine two complementary learning paradigms of NER, i.e., sequence... | Binghuai Lin, Jiali Zeng, Xu Wang, Yongjing Yin, Yufan Jiang, Yunbo Cao |  |
| 134 |  |  [Knowledge-augmented Self-training of A Question Rewriter for Conversational Knowledge Base Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.133) |  | 0 | The recent rise of conversational applications such as online customer service systems and intelligent personal assistants has promoted the development of conversational knowledge base question answering (ConvKBQA). Different from the traditional single-turn KBQA, ConvKBQA usually explores... | Cuiping Li, Hong Chen, Jing Zhang, Juanzi Li, Shulin Cao, Xin Lv, Xirui Ke, Yiqi Xu |  |
| 135 |  |  [Extractive Summarization of Legal Decisions using Multi-task Learning and Maximal Marginal Relevance](https://doi.org/10.18653/v1/2022.findings-emnlp.134) |  | 0 | Summarizing legal decisions requires the expertise of law practitioners, which is both time- and cost-intensive. This paper presents techniques for extractive summarization of legal decisions in a low-resource setting using limited expert annotated data. We test a set of models that locate relevant... | Abhishek Agarwal, Matthias Grabmair, Shanshan Xu |  |
| 136 |  |  [MovieUN: A Dataset for Movie Understanding and Narrating](https://doi.org/10.18653/v1/2022.findings-emnlp.135) |  | 0 | Automatic movie narration generation and narration grounding are very important to provide a true movie experience for the blind and visually impaired. To tell the movie story well, it is necessary to mention plot-related details (such as character names) and keep the narrations in a plot coherent.... | Anwen Hu, Qi Zhang, Qin Jin, Zihao Yue, Ziheng Wang |  |
| 137 |  |  [ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.136) |  | 0 | Data-to-text generation is challenging due to the great variety of the input data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse predicates). Recent end-to-end neural methods thus require substantial training examples to learn to disambiguate and describe the data. Yet,... | Eric P. Xing, Jiannan Xiang, Yucheng Zhou, Zhengzhong Liu, Zhiting Hu |  |
| 138 |  |  [FCGEC: Fine-Grained Corpus for Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2022.findings-emnlp.137) |  | 0 | Grammatical Error Correction (GEC) has been broadly applied in automatic correction and proofreading system recently. However, it is still immature in Chinese GEC due to limited high-quality data from native speakers in terms of category and scale. In this paper, we present FCGEC, a fine-grained... | Jianwang Wu, Jiawei Peng, Jiayu Fu, Lvxiaowei Xu, Ming Cai |  |
| 139 |  |  [Audience-Centric Natural Language Generation via Style Infusion](https://doi.org/10.18653/v1/2022.findings-emnlp.138) |  | 0 | Adopting contextually appropriate, audience-tailored linguistic styles is critical to the success of user-centric language generation systems (e.g., chatbots, computer-aided writing, dialog systems). While existing approaches demonstrate text style transfer (TST) with large volumes of parallel or... | Adit Krishnan, Aravind Sankar, Ewa Maslowska, Hari Sundaram, Samraj Moorjani |  |
| 140 |  |  [DocFin: Multimodal Financial Prediction and Bias Mitigation using Semi-structured Documents](https://doi.org/10.18653/v1/2022.findings-emnlp.139) |  | 0 | Financial prediction is complex due to the stochastic nature of the stock market. Semi-structured financial documents present comprehensive financial data in tabular formats, such as earnings, profit-loss statements, and balance sheets, and can often contain rich technical analysis along with a... | Dinesh Manocha, Franck Dernoncourt, Jochen L. Leidner, Mihir Goyal, Puneet Mathur, Ramit Sawhney, Ritik Mathur |  |
| 141 |  |  [Not Just Plain Text! Fuel Document-Level Relation Extraction with Explicit Syntax Refinement and Subsentence Modeling](https://doi.org/10.18653/v1/2022.findings-emnlp.140) |  | 0 | Document-level relation extraction (DocRE) aims to identify semantic labels among entities within a single document. One major challenge of DocRE is to dig decisive details regarding a specific entity pair from long text. However, in many cases, only a fraction of text carries required information,... | Jianyong Wang, Xiuxing Li, Zhenyu Li, Zhichao Duan, Zhuo Wang |  |
| 142 |  |  [Self-supervised Rewiring of Pre-trained Speech Encoders: Towards Faster Fine-tuning with Less Labels in Speech Processing](https://doi.org/10.18653/v1/2022.findings-emnlp.141) |  | 0 | Pre-trained speech Transformers have facilitated great success across various speech processing tasks. However, fine-tuning these encoders for downstream tasks require sufficiently large training data to converge or to achieve state-of-the-art. In text domain this has been partly attributed to... | Ehsan Shareghi, Gholamreza Haffari, Hao Yang, Jinming Zhao |  |
| 143 |  |  [RedApt: An Adaptor for wav2vec 2 EncodingFaster and Smaller Speech Translation without Quality Compromise](https://doi.org/10.18653/v1/2022.findings-emnlp.142) |  | 0 | Pre-trained speech Transformers in speech translation (ST) have facilitated state-of-the-art (SotA) results; yet, using such encoders is computationally expensive. To improve this, we present a novel Reducer Adaptor block, RedApt, that could be seamlessly integrated within any Transformer-based... | Ehsan Shareghi, Gholamreza Haffari, Hao Yang, Jinming Zhao |  |
| 144 |  |  [How sensitive are translation systems to extra contexts? Mitigating gender bias in Neural Machine Translation models through relevant contexts](https://doi.org/10.18653/v1/2022.findings-emnlp.143) |  | 0 | Neural Machine Translation systems built on top of Transformer-based architectures are routinely improving the state-of-the-art in translation quality according to word-overlap metrics. However, a growing number of studies also highlight the inherent gender bias that these models incorporate during... | Koustuv Sinha, Manan Dey, Shanya Sharma |  |
| 145 |  |  [P\textM²\textF²N: Patient Multi-view Multi-modal Feature Fusion Networks for Clinical Outcome Prediction](https://doi.org/10.18653/v1/2022.findings-emnlp.144) |  | 0 | Clinical outcome prediction is critical to the condition prediction of patients and management of hospital capacities. There are two kinds of medical data, including time series signals recorded by various devices and clinical notes in electronic health records (EHR), which are used for two common... | Baohang Zhou, Guoqing Zhao, Kehui Song, Ning Jiang, Xiaojie Yuan, Xuhui Sui, Ying Zhang |  |
| 146 |  |  [Long Text and Multi-Table Summarization: Dataset and Method](https://doi.org/10.18653/v1/2022.findings-emnlp.145) |  | 0 | Automatic document summarization aims to produce a concise summary covering the input document’s salient information. Within a report document, the salient information can be scattered in the textual and non-textual content. However, existing document summarization datasets and methods usually... | Jiannong Cao, Ruosong Yang, Shuaiqi Liu, Zhiyuan Wen |  |
| 147 |  |  [MatRank: Text Re-ranking by Latent Preference Matrix](https://doi.org/10.18653/v1/2022.findings-emnlp.146) |  | 0 | Text ranking plays a key role in providing content that best answers user queries. It is usually divided into two sub-tasks to perform efficient information retrieval given a query: text retrieval and text re-ranking. Recent research on pretrained language models (PLM) has demonstrated efficiency... | Chenglin Li, Di Niu, Jinwen Luo, Jiuding Yang, Weidong Guo, Yu Xu |  |
| 148 |  |  [Can Language Models Serve as Temporal Knowledge Bases?](https://doi.org/10.18653/v1/2022.findings-emnlp.147) |  | 0 | Recent progress regarding the use of language models (LMs) as knowledge bases (KBs) has shown that language models can act as structured knowledge bases for storing relational facts. However, most existing works only considered the LM-as-KB paradigm in a static setting, which ignores the analysis... | Feng Zhao, Guandong Xu, Hai Jin, Ruilin Zhao, Sixiao Zhang |  |
| 149 |  |  [Are Large Pre-Trained Language Models Leaking Your Personal Information?](https://doi.org/10.18653/v1/2022.findings-emnlp.148) |  | 0 | Are Large Pre-Trained Language Models Leaking Your Personal Information? In this paper, we analyze whether Pre-Trained Language Models (PLMs) are prone to leaking personal information. Specifically, we query PLMs for email addresses with contexts of the email address or prompts containing the... | Hanyin Shao, Jie Huang, Kevin ChenChuan Chang |  |
| 150 |  |  [Self-Distillation with Meta Learning for Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.findings-emnlp.149) |  | 0 | In this paper, we propose a self-distillation framework with meta learning (MetaSD) for knowledge graph completion with dynamic pruning, which aims to learn compressed graph embeddings and tackle the long-tail samples. Specifically, we first propose a dynamic pruning technique to obtain a small... | Chengming Li, Junhao Liu, Min Yang, Yunshui Li |  |
| 151 |  |  [CQR-SQL: Conversational Question Reformulation Enhanced Context-Dependent Text-to-SQL Parsers](https://doi.org/10.18653/v1/2022.findings-emnlp.150) |  | 0 | Context-dependent text-to-SQL is the task of translating multi-turn questions into database-related SQL queries. Existing methods typically focus on making full use of history context or previously predicted SQL for currently SQL parsing, while neglecting to explicitly comprehend the schema and... | Dongling Xiao, Linzheng Chai, QianWen Zhang, Yunbo Cao, Zhao Yan, Zhoujun Li |  |
| 152 |  |  [Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document](https://doi.org/10.18653/v1/2022.findings-emnlp.151) |  | 0 | Given the recent proliferation of false claims online, there has been a lot of manual fact-checking effort. As this is very time-consuming, human fact-checkers can benefit from tools that can support them and make them more efficient. Here, we focus on building a system that could provide such... | Aisha Mohamed, Firoj Alam, Giovanni Da San Martino, Nikola Georgiev, Preslav Nakov, Shaden Shaar |  |
| 153 |  |  [No Word Embedding Model Is Perfect: Evaluating the Representation Accuracy for Social Bias in the Media](https://doi.org/10.18653/v1/2022.findings-emnlp.152) |  | 0 | News articles both shape and reflect public opinion across the political spectrum. Analyzing them for social bias can thus provide valuable insights, such as prevailing stereotypes in society and the media, which are often adopted by NLP models trained on respective data. Recent work has relied on... | Henning Wachsmuth, Maximilian Keiff, Maximilian Spliethöver |  |
| 154 |  |  [Scientific and Creative Analogies in Pretrained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.153) |  | 0 | This paper examines the encoding of analogy in large-scale pretrained language models, such as BERT and GPT-2. Existing analogy datasets typically focus on a limited set of analogical relations, with a high similarity of the two domains between which the analogy holds. As a more realistic setup, we... | Ekaterina Shutova, Helen Yannakoudakis, Pushkar Mishra, Tamara Czinczoll |  |
| 155 |  |  [Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages](https://doi.org/10.18653/v1/2022.findings-emnlp.154) |  | 0 | Scaling multilingual representation learning beyond the hundred most frequent languages is challenging, in particular to cover the long tail of low-resource languages. We move away from the popular one-for-all multilingual models and focus on training multiple language (family) specific... | Holger Schwenk, Kevin Heffernan, Onur Çelebi |  |
| 156 |  |  [Towards Generalizable and Robust Text-to-SQL Parsing](https://doi.org/10.18653/v1/2022.findings-emnlp.155) |  | 0 | Text-to-SQL parsing tackles the problem of mapping natural language questions to executable SQL queries. In practice, text-to-SQL parsers often encounter various challenging scenarios, requiring them to be generalizable and robust. While most existing work addresses a particular generalization or... | Binhua Li, Bowen Li, Chang Gao, Fei Huang, Luo Si, Wai Lam, Wenxuan Zhang, Yongbin Li |  |
| 157 |  |  [EdiT5: Semi-Autoregressive Text Editing with T5 Warm-Start](https://doi.org/10.18653/v1/2022.findings-emnlp.156) |  | 0 | We present EdiT5 - a novel semi-autoregressive text-editing approach designed to combine the strengths of non-autoregressive text-editing and autoregressive decoding. EdiT5 is faster at inference times than conventional sequence-to-sequence (seq2seq) models, while being capable of modeling flexible... | Aliaksei Severyn, Eric Malmi, Jakub Adámek, Jonathan Mallinson |  |
| 158 |  |  [A Critical Reflection and Forward Perspective on Empathy and Natural Language Processing](https://doi.org/10.18653/v1/2022.findings-emnlp.157) |  | 0 | We review the state of research on empathy in natural language processing and identify the following issues: (1) empathy definitions are absent or abstract, which (2) leads to low construct validity and reproducibility. Moreover, (3) emotional empathy is overemphasized, skewing our focus to a... | Allison Lahnala, Charles Welch, David Jurgens, Lucie Flek |  |
| 159 |  |  [A Neural-Symbolic Approach to Natural Language Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.158) |  | 0 | Deep neural networks, empowered by pre-trained language models, have achieved remarkable results in natural language understanding (NLU) tasks. However, their performances can drastically deteriorate when logical reasoning is needed. This is because NLU in principle depends on not only analogical... | Hang Li, Yuan Lin, Zhixuan Liu, Zihao Wang |  |
| 160 |  |  [Social-aware Sparse Attention Network for Session-based Social Recommendation](https://doi.org/10.18653/v1/2022.findings-emnlp.159) |  | 0 | Session-based Social Recommendation (SSR) aims to use users’ social networks and historical sessions to provide more personalized recommendations for the current session.Unfortunately, existing SSR methods have two limitations.First, they do not screen users’ useless social relationships and noisy... | Chen Tang, Haitao Zheng, Kai Ouyang, Wang Chen, Xianghong Xu |  |
| 161 |  |  [SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters](https://doi.org/10.18653/v1/2022.findings-emnlp.160) |  | 0 | Adapter Tuning, which freezes the pretrained language models (PLMs) and only fine-tunes a few extra modules, becomes an appealing efficient alternative to the full model fine-tuning. Although computationally efficient, the recent Adapters often increase parameters (e.g. bottleneck dimension) for... | Dacheng Tao, Daize Dong, Jeremy Zhang, Liang Ding, Shwai He |  |
| 162 |  |  [Measurement Extraction with Natural Language Processing: A Review](https://doi.org/10.18653/v1/2022.findings-emnlp.161) |  | 0 | Quantitative data is important in many domains. Information extraction methods draw structured data from documents. However, the extraction of quantities and their contexts has received little attention in the history of information extraction. In this review, an overview of prior work on... | Detlef Stolten, Jan Göpfert, Jann M. Weinand, Leander Kotzur, Patrick Kuckertz |  |
| 163 |  |  [Summarizing Procedural Text: Data and Approach](https://doi.org/10.18653/v1/2022.findings-emnlp.162) |  | 0 | Procedural text is a widely used genre that contains many steps of instructions of how to cook a dish or how to conduct a chemical experiment and analyze the procedural text has become a popular task in the NLP field. Since the procedural text can be very long and contains many details, summarizing... | Dongyan Zhao, Haotong Zhang, Rui Yan, Shen Gao, Xiuying Chen |  |
| 164 |  |  [Snapshot-Guided Domain Adaptation for ELECTRA](https://doi.org/10.18653/v1/2022.findings-emnlp.163) |  | 0 | Discriminative pre-trained language models, such as ELECTRA, have achieved promising performances in a variety of general tasks. However, these generic pre-trained models struggle to capture domain-specific knowledge of domain-related tasks. In this work, we propose a novel domain-adaptation method... | Daixuan Cheng, Denvy Deng, Furu Wei, Hao Sun, Jianfeng Liu, Qi Zhang, Shaohan Huang, Yuefeng Zhan |  |
| 165 |  |  [Exploiting Labeled and Unlabeled Data via Transformer Fine-tuning for Peer-Review Score Prediction](https://doi.org/10.18653/v1/2022.findings-emnlp.164) |  | 0 | Automatic Peer-review Aspect Score Prediction (PASP) of academic papers can be a helpful assistant tool for both reviewers and authors. Most existing works on PASP utilize supervised learning techniques. However, the limited number of peer-review data deteriorates the performance of PASP. This... | Fumiyo Fukumoto, Jiyi Li, Panitan Muangkammuen, Yoshimi Suzuki |  |
| 166 |  |  [HARALD: Augmenting Hate Speech Data Sets with Real Data](https://doi.org/10.18653/v1/2022.findings-emnlp.165) |  | 0 | The successful completion of the hate speech detection task hinges upon the availability of rich and variable labeled data, which is hard to obtain. In this work, we present a new approach for data augmentation that uses as input real unlabelled data, which is carefully selected from online... | Dan Vilenchik, Ilan Tal |  |
| 167 |  |  [Wait-info Policy: Balancing Source and Target at Information Level for Simultaneous Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.166) |  | 0 | Simultaneous machine translation (SiMT) outputs the translation while receiving the source inputs, and hence needs to balance the received source information and translated target information to make a reasonable decision between waiting for inputs or outputting translation. Previous methods always... | Shaolei Zhang, Shoutao Guo, Yang Feng |  |
| 168 |  |  [Turning Fixed to Adaptive: Integrating Post-Evaluation into Simultaneous Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.167) |  | 0 | Simultaneous machine translation (SiMT) starts its translation before reading the whole source sentence and employs either fixed or adaptive policy to generate the target sentence. Compared to the fixed policy, the adaptive policy achieves better latency-quality tradeoffs by adopting a flexible... | Shaolei Zhang, Shoutao Guo, Yang Feng |  |
| 169 |  |  [Alleviating Sparsity of Open Knowledge Graphs with Ternary Contrastive Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.168) |  | 0 | Sparsity of formal knowledge and roughness of non-ontological construction make sparsity problem particularly prominent in Open Knowledge Graphs (OpenKGs). Due to sparse links, learning effective representation for few-shot entities becomes difficult. We hypothesize that by introducing negative... | Daling Wang, Qian Li, Shafiq R. Joty, Shi Feng, Yifei Zhang |  |
| 170 |  |  [Using Developer Discussions to Guide Fixing Bugs in Software](https://doi.org/10.18653/v1/2022.findings-emnlp.169) |  | 0 | Automatically fixing software bugs is a challenging task. While recent work showed that natural language context is useful in guiding bug-fixing models, the approach required prompting developers to provide this context, which was simulated through commit messages written after the bug-fixing code... | Junyi Jessy Li, Milos Gligoric, Raymond J. Mooney, Sheena Panthaplackel |  |
| 171 |  |  [AutoCAD: Automatically Generate Counterfactuals for Mitigating Shortcut Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.170) |  | 0 | Recent studies have shown the impressive efficacy of counterfactually augmented data (CAD) for reducing NLU models’ reliance on spurious features and improving their generalizability. However, current methods still heavily rely on human efforts or task-specific designs to generate counterfactuals,... | Jiaxin Wen, Jie Zhou, Jinchao Zhang, Minlie Huang, Yeshuang Zhu |  |
| 172 |  |  [A Multi-Modal Knowledge Graph for Classical Chinese Poetry](https://doi.org/10.18653/v1/2022.findings-emnlp.171) |  | 0 | Classical Chinese poetry has a long history and is a precious cultural heritage of humankind. Displaying the classical Chinese poetry in a visual way, helps to cross cultural barriers in different countries, making it enjoyable for all the people. In this paper, we construct a multi-modal knowledge... | Bin Wu, JiRong Wen, Ruihua Song, Ting Bai, Yuqing Li, Yuxin Zhang |  |
| 173 |  |  [Assessing Non-autoregressive Alignment in Neural Machine Translation via Word Reordering](https://doi.org/10.18653/v1/2022.findings-emnlp.172) |  | 0 | Recent work on non-autoregressive neural machine translation (NAT) that leverages alignment information to explicitly reduce the modality of target distribution has reported comparable performance with counterparts that tackle multi-modality problem by implicitly modeling dependencies.... | ChunHin Tse, Ester Leung, William K. Cheung |  |
| 174 |  |  [Syntax-guided Localized Self-attention by Constituency Syntactic Distance](https://doi.org/10.18653/v1/2022.findings-emnlp.173) |  | 0 | Recent works have revealed that Transformers are implicitly learning the syntactic information in its lower layers from data, albeit is highly dependent on the quality and scale of the training data. However, learning syntactic information from data is not necessary if we can leverage an external... | Bingyu Zhu, Bo Yuan, Haotian Xue, Jushi Kai, Longtao Huang, Shengyuan Hou, Xinbing Wang, Zhouhan Lin |  |
| 175 |  |  [CodeExp: Explanatory Code Document Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.174) |  | 0 | Developing models that can automatically generate detailed code explanation can greatly benefit software maintenance and programming education. However, existing code-to-text generation models often produce only high-level summaries of code that do not capture implementation-level choices essential... | Bo Wang, Chenglong Wang, Haotian Cui, Jeevana Priya Inala, Jianfeng Gao, Junjie Huang, Nan Duan, Todd Mytkowicz |  |
| 176 |  |  [PAUQ: Text-to-SQL in Russian](https://doi.org/10.18653/v1/2022.findings-emnlp.175) |  | 0 | Semantic parsing is an important task that allows to democratize human-computer interaction. One of the most popular text-to-SQL datasets with complex and diverse natural language (NL) questions and SQL queries is Spider. We construct and complement a Spider dataset for Russian, thus creating the... | Daria Bakshandaeva, Ekaterina Dmitrieva, Elena Tutubalina, Oleg Somov, Vera Davydova |  |
| 177 |  |  [Event-Centric Question Answering via Contrastive Learning and Invertible Event Transformation](https://doi.org/10.18653/v1/2022.findings-emnlp.176) |  | 0 | Human reading comprehension often requires reasoning of event semantic relations in narratives, represented by Event-centric Question-Answering (QA). To address event-centric QA, we propose a novel QA model with contrastive learning and invertible event transformation, call TranCLR. Our proposed... | Gabriele Pergola, Junru Lu, Lin Gui, Xingwei Tan, Yulan He |  |
| 178 |  |  [Label-Driven Denoising Framework for Multi-Label Few-Shot Aspect Category Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.177) |  | 0 | Multi-Label Few-Shot Aspect Category Detection (FS-ACD) is a new sub-task of aspect-based sentiment analysis, which aims to detect aspect categories accurately with limited training instances. Recently, dominant works use the prototypical network to accomplish this task, and employ the attention... | Fei Zhao, Xinyu Dai, Yuchen Shen, Zhen Wu |  |
| 179 |  |  [Visual Named Entity Linking: A New Dataset and A Baseline](https://doi.org/10.18653/v1/2022.findings-emnlp.178) |  | 0 | Visual Entity Linking (VEL) is a task to link regions of images with their corresponding entities in Knowledge Bases (KBs), which is beneficial for many computer vision tasks such as image retrieval, image caption, and visual question answering. While existing tasks in VEL either rely on textual... | Jiafeng Guo, Ruqing Zhang, Wen Sun, Xueqi Cheng, Yixing Fan |  |
| 180 |  |  [MAGMA - Multimodal Augmentation of Generative Models through Adapter-based Finetuning](https://doi.org/10.18653/v1/2022.findings-emnlp.179) |  | 0 | Large-scale pretraining is fast becoming the norm in Vision-Language (VL) modeling. However, prevailing VL approaches are limited by the requirement for labeled data and the use of complex multi-step pretraining objectives. We present MAGMA - a simple method for augmenting generative language... | Anette Frank, Constantin Eichenberg, Letitia Parcalabescu, Samuel Weinbach, Sidney Black |  |
| 181 |  |  [Towards Tracing Knowledge in Language Models Back to the Training Data](https://doi.org/10.18653/v1/2022.findings-emnlp.180) |  | 0 | Language models (LMs) have been shown to memorize a great deal of factual knowledge contained in their training data. But when an LM generates an assertion, it is often difficult to determine where it learned this information and whether it is true. In this paper, we propose the problem of fact... | Binbin Xiong, Ekin Akyürek, Frederick Liu, Ian Tenney, Jacob Andreas, Kelvin Guu, Tolga Bolukbasi |  |
| 182 |  |  [ReaRev: Adaptive Reasoning for Question Answering over Knowledge Graphs](https://doi.org/10.18653/v1/2022.findings-emnlp.181) |  | 0 | Knowledge Graph Question Answering (KGQA) involves retrieving entities as answers from a Knowledge Graph (KG) using natural language queries. The challenge is to learn to reason over question-relevant KG facts that traverse KG entities and lead to the question answers. To facilitate reasoning, the... | Costas Mavromatis, George Karypis |  |
| 183 |  |  [Understanding Social Media Cross-Modality Discourse in Linguistic Space](https://doi.org/10.18653/v1/2022.findings-emnlp.182) |  | 0 | The multimedia communications with texts and images are popular on social media. However, limited studies concern how images are structured with texts to form coherent meanings in human cognition. To fill in the gap, we present a novel concept of cross-modality discourse, reflecting how human... | Chunpu Xu, Hanzhuo Tan, Jing Li, Piji Li |  |
| 184 |  |  [TAPE: Assessing Few-shot Russian Language Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.183) |  | 0 | Recent advances in zero-shot and few-shot learning have shown promise for a scope of research and practical purposes. However, this fast-growing area lacks standardized evaluation suites for non-English languages, hindering progress outside the Anglo-centric paradigm. To address this line of... | Albina Akhmetgareeva, Alena Fenogenova, Alena Spiridonova, Anastasiia Bashmakova, Denis Shevelev, Ekaterina Artemova, Ekaterina Taktasheva, Maria Tikhonova, Nadezhda Katricheva, Oleg Zinkevich, Svetlana Iordanskaia, Tatiana Shavrina, Valentina Kurenshchikova, Vladislav Mikhailov |  |
| 185 |  |  [A Hierarchical N-Gram Framework for Zero-Shot Link Prediction](https://doi.org/10.18653/v1/2022.findings-emnlp.184) |  | 0 | Knowledge graphs typically contain a large number of entities but often cover only a fraction of all relations between them (i.e., incompleteness). Zero-shot link prediction (ZSLP) is a popular way to tackle the problem by automatically identifying unobserved relations between entities. Most recent... | Junfan Chen, Mingchen Li, Nikolaos Aletras, Samuel Mensah, Xiulong Yang, Yang Ye |  |
| 186 |  |  [Quadapter: Adapter for GPT-2 Quantization](https://doi.org/10.18653/v1/2022.findings-emnlp.185) |  | 0 | Transformer language models such as GPT-2 are difficult to quantize because of outliers in the activations leading to a large quantization error. To adapt to the error, one must use quantization-aware training, which entails a fine-tuning process based on the dataset and the training pipeline... | Jaeseong You, Markus Nagel, Minseop Park, Simyung Chang |  |
| 187 |  |  [BanglaRQA: A Benchmark Dataset for Under-resourced Bangla Language Reading Comprehension-based Question Answering with Diverse Question-Answer Types](https://doi.org/10.18653/v1/2022.findings-emnlp.186) |  | 0 | High-resource languages, such as English, have access to a plethora of datasets with various question-answer types resembling real-world reading comprehension. However, there is a severe lack of diverse and comprehensive question-answering datasets in under-resourced languages like Bangla. The ones... | Abu Raihan Mostofa Kamal, Adham Arik Rahman, Md Azam Hossain, Md Mezbaur Rahman, Md. Sajid Altaf, Mehrab Mustafy Rahman, Mohammed Saidul Islam, Syed Mohammed Sartaj Ekram |  |
| 188 |  |  [Chaining Simultaneous Thoughts for Numerical Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.187) |  | 0 | Given that rich information is hidden behind ubiquitous numbers in text, numerical reasoning over text should be an essential skill of AI systems. To derive precise equations to solve numerical reasoning problems, previous work focused on modeling the structures of equations, and has proposed... | Fei Huang, Minlie Huang, Zhihong Shao |  |
| 189 |  |  [Inferring Implicit Relations in Complex Questions with Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.188) |  | 0 | A prominent challenge for modern language understanding systems is the ability to answer implicit reasoning questions, where the required reasoning steps for answering the question are not mentioned in the text explicitly. In this work, we investigate why current models struggle with implicit... | Jonathan Berant, Mor Geva, Uri Katz |  |
| 190 |  |  [Eliciting and Understanding Cross-task Skills with Task-level Mixture-of-Experts](https://doi.org/10.18653/v1/2022.findings-emnlp.189) |  | 0 | Recent works suggest that transformer models are capable of multi-tasking on diverse NLP tasks and adapt to new tasks efficiently. However, the potential of these multi-task models may be limited as they use the same set of parameters for all tasks. In contrast, humans tackle tasks in a more... | Juan Zha, Qinyuan Ye, Xiang Ren |  |
| 191 |  |  [On the Curious Case of l2 norm of Sense Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.190) |  | 0 | We show that the l2 norm of a static sense embedding encodes information related to the frequency of that sense in the training corpus used to learn the sense embeddings. This finding can be seen as an extension of a previously known relationship for word embeddings to sense embeddings. Our... | Danushka Bollegala, Yi Zhou |  |
| 192 |  |  [Partially-Random Initialization: A Smoking Gun for Binarization Hypothesis of BERT](https://doi.org/10.18653/v1/2022.findings-emnlp.191) |  | 0 | In the past few years, pre-trained BERT has become one of the most popular deep-learning language models due to their remarkable performance in natural language processing (NLP) tasks. However, the superior performance of BERT comes at the cost of high computational and memory complexity, hindering... | Arash Ardakani |  |
| 193 |  |  [Prompt Consistency for Zero-Shot Task Generalization](https://doi.org/10.18653/v1/2022.findings-emnlp.192) |  | 0 | One of the most impressive results of recent NLP history is the ability of pre-trained language models to solve new tasks in a zero-shot setting. To achieve this, NLP tasks are framed as natural language prompts, generating a response indicating the predicted output. Nonetheless, the performance in... | Chunting Zhou, Graham Neubig, Junxian He, Taylor BergKirkpatrick, Xuezhe Ma |  |
| 194 |  |  [In-Context Learning for Few-Shot Dialogue State Tracking](https://doi.org/10.18653/v1/2022.findings-emnlp.193) |  | 0 | Collecting and annotating task-oriented dialogues is time-consuming and costly. Thus, zero and few shot learning for dialogue tasks presents an exciting opportunity. In this work, we propose an in-context (IC) learning framework for zero-shot and few-shot learning dialogue state tracking (DST),... | ChiaHsuan Lee, Mari Ostendorf, Noah A. Smith, Tao Yu, Tianbao Xie, Yushi Hu |  |
| 195 |  |  [On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization](https://doi.org/10.18653/v1/2022.findings-emnlp.194) |  | 0 | Combining the visual modality with pretrained language models has been surprisingly effective for simple descriptive tasks such as image captioning. More general text generation however remains elusive. We take a step back and ask: How do these models work for more complex generative tasks, i.e.... | Akshita Bhagia, Alan W. Black, Ana Marasovic, Florian Metze, Shruti Palaskar, Yonatan Bisk |  |
| 196 |  |  [The challenges of temporal alignment on Twitter during crises](https://doi.org/10.18653/v1/2022.findings-emnlp.195) |  | 0 | Language use changes over time, and this impacts the effectiveness of NLP systems. This phenomenon is even more prevalent in social media data during crisis events where meaning and frequency of word usage may change over the course of days. Contextual language models fail to adapt temporally,... | Aniket Pramanick, Iryna Gurevych, Kevin Stowe, Tilman Beck |  |
| 197 |  |  [Experimental Standards for Deep Learning in Natural Language Processing Research](https://doi.org/10.18653/v1/2022.findings-emnlp.196) |  | 0 | The field of Deep Learning (DL) has undergone explosive growth during the last decade, with a substantial impact on Natural Language Processing (NLP) as well. Yet, compared to more established disciplines, a lack of common experimental standards remains an open challenge to the field at large.... | Barbara Plank, Christian Hardmeier, Daniel Varab, Dennis Ulmer, Elisa Bassignana, Max MüllerEberstein, Mike Zhang, Rob van der Goot |  |
| 198 |  |  [Few-Shot Anaphora Resolution in Scientific Protocols via Mixtures of In-Context Experts](https://doi.org/10.18653/v1/2022.findings-emnlp.197) |  | 0 | Anaphora resolution is an important task for information extraction across a range of languages, text genres, and domains, motivating the need for methods that do not require large annotated datasets. In-context learning has emerged as a promising approach, yet there are a number of challenges in... | Alan Ritter, Fan Bai, Nghia T. Le |  |
| 199 |  |  [Exploring Predictive Uncertainty and Calibration in NLP: A Study on the Impact of Method & Data Scarcity](https://doi.org/10.18653/v1/2022.findings-emnlp.198) |  | 0 | We investigate the problem of determining the predictive confidence (or, conversely, uncertainty) of a neural classifier through the lens of low-resource languages. By training models on sub-sampled datasets in three different languages, we assess the quality of estimates from a wide array of... | Christian Hardmeier, Dennis Ulmer, Jes Frellsen |  |
| 200 |  |  [Conditional Supervised Contrastive Learning for Fair Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.199) |  | 0 | Contrastive representation learning has gained much attention due to its superior performance in learning representations from both image and sequential data. However, the learned representations could potentially lead to performance disparities in downstream tasks, such as increased silencing of... | Han Zhao, Jianfeng Chi, KaiWei Chang, William Shand, Yaodong Yu, Yuan Tian |  |
| 201 |  |  [SpaBERT: A Pretrained Language Model from Geographic Data for Geo-Entity Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.200) |  | 0 | Named geographic entities (geo-entities for short) are the building blocks of many geographic datasets. Characterizing geo-entities is integral to various application domains, such as geo-intelligence and map comprehension, while a key challenge is to capture the spatial-varying context of an... | Jina Kim, Muhao Chen, YaoYi Chiang, Zekun Li |  |
| 202 |  |  [Self-training with Two-phase Self-augmentation for Few-shot Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.201) |  | 0 | In task-oriented dialogue systems, response generation from meaning representations (MRs) often suffers from limited training examples, due to the high cost of annotating MR-to-Text pairs. Previous works on self-training leverage fine-tuned conversational models to automatically generate... | Hanjie Chen, Wanyu Du, Yangfeng Ji |  |
| 203 |  |  [Is NLP Ready for Standardization?](https://doi.org/10.18653/v1/2022.findings-emnlp.202) |  | 0 | While standardization is a well-established activity in other scientific fields such as telecommunications, networks or multimedia, in the field of AI and more specifically NLP it is still at its dawn. In this paper, we explore how various aspects of NLP (evaluation, data, tasks...) lack standards... | Lauriane Aufrant |  |
| 204 |  |  [Probing for Incremental Parse States in Autoregressive Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.203) |  | 0 | Next-word predictions from autoregressive neural language models show remarkable sensitivity to syntax. This work evaluates the extent to which this behavior arises as a result of a learned ability to maintain implicit representations of incremental syntactic structures. We extend work in syntactic... | Roger Levy, Tiwalayo Eisape, Vineet Gangireddy, Yoon Kim |  |
| 205 |  |  [Re-Examining Calibration: The Case of Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.204) |  | 0 | For users to trust model predictions, they need to understand model outputs, particularly their confidence — calibration aims to adjust (calibrate) models’ confidence to match expected accuracy. We argue that the traditional calibration evaluation does not promote effective calibrations: for... | Chen Zhao, Chenglei Si, Jordan L. BoydGraber, Sewon Min |  |
| 206 |  |  [Accelerating Learned Sparse Indexes Via Term Impact Decomposition](https://doi.org/10.18653/v1/2022.findings-emnlp.205) |  | 0 | Novel inverted index-based learned sparse ranking models provide more effective, but less efficient, retrieval performance compared to traditional ranking models like BM25. In this paper, we introduce a technique we call postings clipping to improve the query efficiency of learned representations.... | Alistair Moffat, Antonio Mallia, Joel Mackenzie, Matthias Petri |  |
| 207 |  |  [Do Text-to-Text Multi-Task Learners Suffer from Task Conflict?](https://doi.org/10.18653/v1/2022.findings-emnlp.206) |  | 0 | Traditional multi-task learning architectures learn a single model across multiple tasks through a shared encoder followed by task-specific decoders. Learning these models often requires specialized training algorithms that address task-conflict in the shared parameter updates, which otherwise can... | David Mueller, Mark Dredze, Nicholas Andrews |  |
| 208 |  |  [MANTa: Efficient Gradient-Based Tokenization for End-to-End Robust Language Modeling](https://doi.org/10.18653/v1/2022.findings-emnlp.207) |  | 0 | Static subword tokenization algorithms have been an essential component of recent works on language modeling. However, their static nature results in important flaws that degrade the models’ downstream performance and robustness. In this work, we propose MANTa, a Module for Adaptive Neural... | Benoît Sagot, Nathan Godey, Roman Castagné, Éric de la Clergerie |  |
| 209 |  |  [Towards Intelligent Clinically-Informed Language Analyses of People with Bipolar Disorder and Schizophrenia](https://doi.org/10.18653/v1/2022.findings-emnlp.208) |  | 0 | NLP offers a myriad of opportunities to support mental health research. However, prior work has almost exclusively focused on social media data, for which diagnoses are difficult or impossible to validate. We present a first-of-its-kind dataset of manually transcribed interactions with people... | Amy E. Pinkham, Ankit Aich, Avery Quynh, Colin A. Depp, Natalie Parde, Philip D. Harvey, Varsha D. Badal |  |
| 210 |  |  [Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes](https://doi.org/10.18653/v1/2022.findings-emnlp.209) |  | 0 | Multi-hop Question Answering (QA) is a challenging task since it requires an accurate aggregation of information from multiple context paragraphs and a thorough understanding of the underlying reasoning chains. Recent work in multi-hop QA has shown that performance can be boosted by first... | Kaige Xie, Mark O. Riedl, Sarah Wiegreffe |  |
| 211 |  |  [CheckHARD: Checking Hard Labels for Adversarial Text Detection, Prediction Correction, and Perturbed Word Suggestion](https://doi.org/10.18653/v1/2022.findings-emnlp.210) |  | 0 | An adversarial attack generates harmful text that fools a target model. More dangerously, this text is unrecognizable by humans. Existing work detects adversarial text and corrects a target’s prediction by identifying perturbed words and changing them into their synonyms, but many benign words are... | HoangQuoc NguyenSon, Huy Quang Ung, Kazuhide Fukushima, Seira Hidano, Shinsaku Kiyomoto |  |
| 212 |  |  [Mitigating Covertly Unsafe Text within Natural Language Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.211) |  | 0 | An increasingly prevalent problem for intelligent technologies is text safety, as uncontrolled systems may generate recommendations to their users that lead to injury or life-threatening consequences. However, the degree of explicitness of a generated statement that can cause physical harm varies.... | Alex Mei, Anisha Kabir, Bruce Bimber, Desmond Patton, Emily Allaway, John Judge, Kathleen R. McKeown, Melanie Subbiah, Sharon Levy, William Yang Wang |  |
| 213 |  |  ["I Know Who You Are": Character-Based Features for Conversational Humor Recognition in Chinese](https://doi.org/10.18653/v1/2022.findings-emnlp.212) |  | 0 | Humor plays an important role in our daily life, as it is an essential and fascinating element in the communication between persons. Therefore, how to recognize punchlines from the dialogue, i.e. conversational humor recognition, has attracted much interest of computational linguistics communities.... | Binyang Li, Fangchun Yang, Jiangjiang Zhao, KamFai Wong, Wenbo Shang, Zezhong Wang |  |
| 214 |  |  [DebiasGAN: Eliminating Position Bias in News Recommendation with Adversarial Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.213) |  | 0 | Click behaviors are widely used for learning news recommendation models, but they are heavily affected by the biases brought by the news display positions. It is important to remove position biases to train unbiased recommendation model and capture unbiased user interest. In this paper, we propose... | Chuhan Wu, Fangzhao Wu, Xiangnan He, Yongfeng Huang |  |
| 215 |  |  [Generating Multiple-Length Summaries via Reinforcement Learning for Unsupervised Sentence Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.214) |  | 0 | Sentence summarization shortens given texts while maintaining core contents of the texts. Unsupervised approaches have been studied to summarize texts without ground-truth summaries. However, recent unsupervised models are extractive, which remove words from texts and thus they are less flexible... | Chanyoung Park, Dongmin Hyun, Hwanjo Yu, Xing Xie, Xiting Wang |  |
| 216 |  |  [Multilingual Sentence Transformer as A Multilingual Word Aligner](https://doi.org/10.18653/v1/2022.findings-emnlp.215) |  | 0 | Multilingual pretrained language models (mPLMs) have shown their effectiveness in multilingual word alignment induction. However, these methods usually start from mBERT or XLM-R. In this paper, we investigate whether multilingual sentence Transformer LaBSE is a strong multilingual word aligner.... | Guanhua Chen, Hanqing Wang, Weikang Wang, Yue Han, Yun Chen |  |
| 217 |  |  [CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.216) |  | 0 | Counterfactual data augmentation (CDA) – i.e., adding minimally perturbed inputs during training – helps reduce model reliance on spurious correlations and improves generalization to out-of-distribution (OOD) data. Prior work on generating counterfactuals only considered restricted classes of... | Bhargavi Paranjape, Hannaneh Hajishirzi, Luke Zettlemoyer, Tanay Dixit |  |
| 218 |  |  [Conversation Disentanglement with Bi-Level Contrastive Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.217) |  | 0 | Conversation disentanglement aims to group utterances into detached sessions, which is a fundamental task in processing multi-party conversations. Existing methods have two main drawbacks. First, they overemphasize pairwise utterance relations but pay inadequate attention to the... | Chengyu Huang, Hao Fei, Lizi Liao, Zheng Zhang |  |
| 219 |  |  [You can't pick your neighbors, or can you? When and How to Rely on Retrieval in the kNN-LM](https://doi.org/10.18653/v1/2022.findings-emnlp.218) |  | 0 | Retrieval-enhanced language models (LMs), which condition their predictions on text retrieved from large external datastores, have recently shown significant perplexity improvements compared to standard LMs. One such approach, the kNN-LM, interpolates any existing LM’s predictions with the output... | Andrew Drozdov, Andrew McCallum, Hamed Zamani, Mohit Iyyer, Razieh Rahimi, Shufan Wang |  |
| 220 |  |  [StuBot: Learning by Teaching a Conversational Agent Through Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.findings-emnlp.219) |  | 0 | This paper proposes StuBot, a text-based conversational agent that provides adaptive feedback for learning by teaching. StuBot first asks the users to teach the learning content by summarizing and explaining it in their own words. After the users inputted the explanation text for teaching, StuBot... | Hana Lee, Nayoung Jin |  |
| 221 |  |  [Improved Universal Sentence Embeddings with Prompt-based Contrastive Learning and Energy-based Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.220) |  | 0 | Contrastive learning has been demonstrated to be effective in enhancing pre-trained language models (PLMs) to derive superior universal sentence embeddings. However, existing contrastive methods still have two limitations. Firstly, previous works may acquire poor performance under domain shift... | Linhan Zhang, Wei Wang, Yuxin Jiang |  |
| 222 |  |  [RaP: Redundancy-aware Video-language Pre-training for Text-Video Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.221) |  | 0 | Video language pre-training methods have mainly adopted sparse sampling techniques to alleviate the temporal redundancy of videos. Though effective, sparse sampling still suffers inter-modal redundancy: visual redundancy and textual redundancy. Compared with highly generalized text, sparsely... | Chaochen Gao, Jizhong Han, Songlin Hu, Xing Wu, Zhongyuan Wang, Zijia Lin |  |
| 223 |  |  [FCGCL: Fine- and Coarse-Granularity Contrastive Learning for Speech Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.222) |  | 0 | It is notoriously difficult to implement end-to-end speech translation (E2E-ST) model because of the task complexity and data scarcity. Existing techniques often attempt to carry out implicit knowledge transfer from machine translation (MT) to ST model by imposing various constraints. However, in... | Dan Qu, Hao Zhang, Nianwen Si, Tong Niu, Xukui Yang, Yaqi Chen, Zhen Li |  |
| 224 |  |  [InfoCSE: Information-aggregated Contrastive Learning of Sentence Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.223) |  | 0 | Contrastive learning has been extensively studied in sentence embedding learning, which assumes that the embeddings of different views of the same sentence are closer. The constraint brought by this assumption is weak, and a good sentence representation should also be able to reconstruct the... | Chaochen Gao, Jizhong Han, Songlin Hu, Xing Wu, Zhongyuan Wang, Zijia Lin |  |
| 225 |  |  [Benchmarking Language Models for Code Syntax Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.224) |  | 0 | Pre-trained language models have demonstrated impressive performance in both natural language processing and program understanding, which represent the input as a token sequence without explicitly modeling its structure. Some prior works show that pre-trained language models can capture the... | Chenguang Wang, Da Shen, Dawn Song, Koushik Sen, Xinyun Chen |  |
| 226 |  |  [Learning When and What to Quote: A Quotation Recommender System with Mutual Promotion of Recommendation and Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.225) |  | 0 | This work extends the current quotation recommendation task to a more realistic quotation recommender system that learns to predict when to quote and what to quote jointly. The system consists of three modules (tasks), a prediction module to predict whether to quote given conversation contexts, a... | KamFai Wong, Lingzhi Wang, Xingshan Zeng |  |
| 227 |  |  [Think Beyond Words: Exploring Context-Relevant Visual Commonsense for Diverse Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.226) |  | 0 | Commonsense knowledge has been widely considered for building intelligent open-domain dialogue agents, aiming to generate meaningful and diverse responses. Previous works in this field usually lack the ability to effectively obtain and utilize auxiliary commonsense from the external visual world.... | Beichen Zhang, Liang Li, Qingming Huang, Yiting Liu |  |
| 228 |  |  [Gender Bias in Meta-Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.227) |  | 0 | Different methods have been proposed to develop meta-embeddings from a given set of source embeddings. However, the source embeddings can contain unfair gender-related biases, and how these influence the meta-embeddings has not been studied yet.We study the gender bias in meta-embeddings created... | Danushka Bollegala, Masahiro Kaneko, Naoaki Okazaki |  |
| 229 |  |  [Third-Party Aligner for Neural Word Alignments](https://doi.org/10.18653/v1/2022.findings-emnlp.228) |  | 0 | Word alignment is to find translationally equivalent words between source and target sentences. Previous work has demonstrated that self-training can achieve competitive word alignment results. In this paper, we propose to use word alignments generated by a third-party word aligner to supervise the... | Chuanqi Dong, Jinpeng Zhang, Min Zhang, Xiangyu Duan, Yuqi Zhang |  |
| 230 |  |  [QaDialMoE: Question-answering Dialogue based Fact Verification with Mixture of Experts](https://doi.org/10.18653/v1/2022.findings-emnlp.229) |  | 0 | Fact verification is an essential tool to mitigate the spread of false information online, which has gained a widespread attention recently. However, a fact verification in the question-answering dialogue is still underexplored. In this paper, we propose a neural network based approach called... | Chaoyang Yan, Chuang Zhang, Lei Zhang, Longzheng Wang, Peng Zhang, Xiaoyu Lu |  |
| 231 |  |  [Multimodal Knowledge Learning for Named Entity Disambiguation](https://doi.org/10.18653/v1/2022.findings-emnlp.230) |  | 0 | With the popularity of online social media, massive-scale multimodal information has brought new challenges to traditional Named Entity Disambiguation (NED) tasks. Recently, Multimodal Named Entity Disambiguation (MNED) has been proposed to link ambiguous mentions with the textual and visual... | Dongjie Zhang, Longtao Huang |  |
| 232 |  |  [Generative Prompt Tuning for Relation Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.231) |  | 0 | Using prompts to explore the knowledge contained within pre-trained language models for downstream tasks has now become an active topic. Current prompt tuning methods mostly convert the downstream tasks to masked language modeling problems by adding cloze-style phrases and mapping all labels to... | Bo Cheng, Jiale Han, Shengkun Ma, Shuai Zhao, Wei Lu |  |
| 233 |  |  [Formulating Few-shot Fine-tuning Towards Language Model Pre-training: A Pilot Study on Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.232) |  | 0 | Fine-tuning pre-trained language models is a common practice in building NLP models for various tasks, including the case with less supervision. We argue that under the few-shot setting, formulating fine-tuning closer to the pre-training objective shall be able to unleash more benefits from the... | Jingbo Shang, Kewen Zhao, Zihan Wang, Zilong Wang |  |
| 234 |  |  [Masked Language Models Know Which are Popular: A Simple Ranking Strategy for Commonsense Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.233) |  | 0 | We propose a simple ranking strategy to solve a generative commonsense question answering (QA) problem. Compared with multiple-choice QA, it is challenging because the answers to a question are not unique and they are supposed to be popular and diverse. Our strategy exploits the dataset itself and... | Bing Qin, Chuang Fan, Ruifeng Xu, Wanguo Jiang, Xuan Luo, Yice Zhang |  |
| 235 |  |  [DialogUSR: Complex Dialogue Utterance Splitting and Reformulation for Multiple Intent Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.234) |  | 0 | While interacting with chatbots, users may elicit multiple intents in a single dialogue utterance. Instead of training a dedicated multi-intent detection model, we propose DialogUSR, a dialogue utterance splitting and reformulation task that first splits multi-intent user query into several... | Binghuai Lin, Haoran Meng, He Feng, Tianyu Liu, Xuemin Zhao, Yunbo Cao, Zheng Xin, Zhifang Sui, Zizhen Wang |  |
| 236 |  |  [Low-resource Interactive Active Labeling for Fine-tuning Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.235) |  | 0 | Recently, active learning (AL) methods have been used to effectively fine-tune pre-trained language models for various NLP tasks such as sentiment analysis and document classification. However, given the task of fine-tuning language models, understanding the impact of different aspects on AL... | Dan Zhang, Estevam Hruschka, Hannah Kim, Sajjadur Rahman, Seiji Maekawa |  |
| 237 |  |  [Getting the Most out of Simile Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.236) |  | 0 | Simile recognition involves two subtasks: simile sentence classification that discriminates whether a sentence contains simile, and simile component extraction that locates the corresponding objects (i.e., tenors and vehicles).Recent work ignores features other than surface strings and suffers from... | Chulun Zhou, Hualin Zeng, Jinsong Su, Linfeng Song, Xiaoyue Wang, Xin Liu |  |
| 238 |  |  [A Unified Framework for Pun Generation with Humor Principles](https://doi.org/10.18653/v1/2022.findings-emnlp.237) |  | 0 | We propose a unified framework to generate both homophonic and homographic puns to resolve the split-up in existing works. Specifically, we incorporate three linguistic attributes of puns to the language models: ambiguity, distinctiveness, and surprise. Our framework consists of three parts: 1) a... | Divyanshu Sheth, Nanyun Peng, Yufei Tian |  |
| 239 |  |  [Improving English-Arabic Transliteration with Phonemic Memories](https://doi.org/10.18653/v1/2022.findings-emnlp.238) |  | 0 | Transliteration is an important task in natural language processing (NLP) which aims to convert a name in the source language to the target language without changing its pronunciation. Particularly, transliteration from English to Arabic is highly needed in many applications, especially in... | Lianxi Wang, Renze Lou, Shengyi Jiang, Xiangyu Pang, Yan Song, Yuanhe Tian |  |
| 240 |  |  [Mix-and-Match: Scalable Dialog Response Retrieval using Gaussian Mixture Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.239) |  | 0 | Embedding-based approaches for dialog response retrieval embed the context-response pairs as points in the embedding space. These approaches are scalable, but fail to account for the complex, many-to-many relationships that exist between context-response pairs. On the other end of the spectrum,... | Danish Contractor, Gaurav Pandey, Sachindra Joshi |  |
| 241 |  |  [AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.240) |  | 0 | There are growing interests in adapting large-scale language models using parameter-efficient fine-tuning methods. However, accelerating the model itself and achieving better inference efficiency through model compression has not been thoroughly explored yet.Model compression could provide the... | Baeseong Park, Byeongwook Kim, Dongsoo Lee, Jeonghoon Kim, Jeongin Bae, JinHwa Kim, JungWoo Ha, Kang Min Yoo, Nako Sung, Se Jung Kwon |  |
| 242 |  |  [Learning Invariant Representation Improves Robustness for MRC Models](https://doi.org/10.18653/v1/2022.findings-emnlp.241) |  | 0 | The prosperity of Pretrained Language Models(PLM) has greatly promoted the development of Machine Reading Comprehension (MRC). However, these models are vulnerable and not robust to adversarial examples. In this paper, we propose Stable and Contrastive Question Answering (SCQA) to improve... | Haoran Meng, Houfeng Wang, Liang Wen, Tianyu Liu, Yu Hai |  |
| 243 |  |  [ER-Test: Evaluating Explanation Regularization Methods for Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.242) |  | 0 | By explaining how humans would solve a given task, human rationales can provide strong learning signal for neural language models (NLMs). Explanation regularization (ER) aims to improve NLM generalization by pushing the NLM’s machine rationales (Which input tokens did the NLM focus on?) to align... | Aaron Chan, Brihi Joshi, Hamed Firooz, Maziar Sanjabi, Shaoliang Nie, Xiang Ren, Ziyi Liu |  |
| 244 |  |  [Learning Cooperative Interactions for Multi-Overlap Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.243) |  | 0 | Aspect sentiment triplet extraction (ASTE) is an essential task, which aims to extract triplets(aspect, opinion, sentiment). However, overlapped triplets, especially multi-overlap triplets,make ASTE a challenge. Most existing methods suffer from multi-overlap triplets becausethey focus on the... | Shiman Zhao, Tengjiao Wang, Wei Chen |  |
| 245 |  |  [Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Parameter-Efficient Tuning](https://doi.org/10.18653/v1/2022.findings-emnlp.244) |  | 0 | Delta tuning (DET, also known as parameter-efficient tuning) is deemed as the new paradigm for using pre-trained language models (PLMs). Up to now, various DETs with distinct design elements have been proposed, achieving performance on par with fine-tuning. However, the mechanisms behind the above... | Jie Zhou, Jing Yi, Maosong Sun, Ning Ding, Weize Chen, Xu Han, Yankai Lin, Yujia Qin, Zhiyuan Liu |  |
| 246 |  |  [Explainable Slot Type Attentions to Improve Joint Intent Detection and Slot Filling](https://doi.org/10.18653/v1/2022.findings-emnlp.245) |  | 0 | Joint intent detection and slot filling is a key research topic in natural language understanding (NLU). Existing joint intent and slot filling systems analyze and compute features collectively for all slot types, and importantly, have no way to explain the slot filling model decisions. In this... | Akhila Yerukola, Hongxia Jin, Kalpa Gunaratna, Vijay Srinivasan |  |
| 247 |  |  [PseudoReasoner: Leveraging Pseudo Labels for Commonsense Knowledge Base Population](https://doi.org/10.18653/v1/2022.findings-emnlp.246) |  | 0 | Commonsense Knowledge Base (CSKB) Population aims at reasoning over unseen entities and assertions on CSKBs, and is an important yet hard commonsense reasoning task. One challenge is that it requires out-of-domain generalization ability as the source CSKB for training is of a relatively smaller... | Ginny Y. Wong, Hongming Zhang, Quyet V. Do, Simon See, Tianqing Fang, Yangqiu Song |  |
| 248 |  |  [History-Aware Hierarchical Transformer for Multi-session Open-domain Dialogue System](https://doi.org/10.18653/v1/2022.findings-emnlp.247) |  | 0 | With the evolution of pre-trained language models, current open-domain dialogue systems have achieved great progress in conducting one-session conversations. In contrast, Multi-Session Conversation (MSC), which consists of multiple sessions over a long term with the same user, is... | Boyang Li, Chunyan Miao, Lizhen Cui, Pengwei Wang, Tong Zhang, Yong Liu, Yuan You, Zhiwei Zeng |  |
| 249 |  |  [Guiding Abstractive Dialogue Summarization with Content Planning](https://doi.org/10.18653/v1/2022.findings-emnlp.248) |  | 0 | Abstractive dialogue summarization has recently been receiving more attention. We propose a coarse-to-fine model for generating abstractive dialogue summaries, and introduce a fact-aware reinforcement learning (RL) objective that improves the fact consistency between the dialogue and the generated... | Xiaojun Wan, Ye Wang, Zhiping Cai |  |
| 250 |  |  [Truncation Sampling as Language Model Desmoothing](https://doi.org/10.18653/v1/2022.findings-emnlp.249) |  | 0 | Long samples of text from neural language models can be of poor quality. Truncation sampling algorithms–like top-p or top-k—address this by setting some words’ probabilities to zero at each step. This work investigates why these methods are important, and how to improve them. We propose thinking of... | Christopher D. Manning, John Hewitt, Percy Liang |  |
| 251 |  |  [Knowledge-grounded Dialog State Tracking](https://doi.org/10.18653/v1/2022.findings-emnlp.250) |  | 0 | Knowledge (including structured knowledge such as schema and ontology and unstructured knowledge such as web corpus) is a critical part of dialog understanding, especially for unseen tasks and domains. Traditionally, such domain-specific knowledge is encoded implicitly into model parameters for the... | Dian Yu, Hagen Soltau, Izhak Shafran, Laurent El Shafey, Mingqiu Wang, Yuan Cao |  |
| 252 |  |  [Context-aware Information-theoretic Causal De-biasing for Interactive Sequence Labeling](https://doi.org/10.18653/v1/2022.findings-emnlp.251) |  | 0 | Supervised training of existing deep learning models for sequence labeling relies on large scale labeled datasets. Such datasets are generally created with crowd-source labeling. However, crowd-source labeling for tasks of sequence labeling can be expensive and time-consuming. Further, crowd-source... | Ani Nenkova, Handong Zhao, Junda Wu, Ricardo Henao, Rui Wang, Ruiyi Zhang, Shuai Li, Tong Yu |  |
| 253 |  |  [Simple but Challenging: Natural Language Inference Models Fail on Simple Sentences](https://doi.org/10.18653/v1/2022.findings-emnlp.252) |  | 0 | Natural language inference (NLI) is a task to infer the relationship between a premise and a hypothesis (e.g., entailment, neutral, or contradiction), and transformer-based models perform well on current NLI datasets such as MNLI and SNLI. Nevertheless, given the linguistic complexity of the... | Cheng Luo, Jiajie Zou, Jieyu Lin, Ming Xiang, Nai Ding, Wei Liu |  |
| 254 |  |  [DORE: Document Ordered Relation Extraction based on Generative Framework](https://doi.org/10.18653/v1/2022.findings-emnlp.253) |  | 0 | In recent years, there is a surge of generation-based information extraction work, which allows a more direct use of pre-trained language models and efficiently captures output dependencies. However, previous generative methods using lexical representation do not naturally fit document-level... | Hang Yan, Qipeng Guo, Xipeng Qiu, Yuqing Yang, Zheng Zhang |  |
| 255 |  |  [Explicit Role Interaction Network for Event Argument Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.254) |  | 0 | Event argument extraction is a challenging subtask of event extraction, aiming to identify and assign roles to arguments under a certain event. Existing methods extract arguments of each role independently, ignoring the relationship between different roles. Such an approach hinders the model from... | Chunming Hu, Kai Sun, Nan Ding, Richong Zhang, Samuel Mensah |  |
| 256 |  |  [Few-Shot Out-of-Domain Transfer Learning of Natural Language Explanations in a Label-Abundant Setup](https://doi.org/10.18653/v1/2022.findings-emnlp.255) |  | 0 | Training a model to provide natural language explanations (NLEs) for its predictions usually requires the acquisition of task-specific NLEs, which is time- and resource-consuming. A potential solution is the few-shot out-of-domain transfer of NLEs from a parent task with many NLEs to a child... | OanaMaria Camburu, Thomas Lukasiewicz, Vid Kocijan, Yordan Yordanov |  |
| 257 |  |  [RoChBert: Towards Robust BERT Fine-tuning for Chinese](https://doi.org/10.18653/v1/2022.findings-emnlp.256) |  | 0 | Despite of the superb performance on a wide range of tasks, pre-trained language models (e.g., BERT) have been proved vulnerable to adversarial texts. In this paper, we present RoChBERT, a framework to build more Robust BERT-based models by utilizing a more comprehensive adversarial graph to fuse... | Bo Yuan, Chao Zhang, Donghong Sun, Hui Xue, Jinfeng Li, Ning Shi, Rong Zhang, Xiangyu Liu, Zihan Zhang |  |
| 258 |  |  [Lexical Entailment with Hierarchy Representations by Deep Metric Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.257) |  | 0 | In this paper, we introduce a novel method for lexical entailment tasks, which detects a hyponym-hypernym relation among words. Existing lexical entailment studies are lacking in generalization performance, as they cannot be applied to words that are not included in the training dataset. Moreover,... | Aori Shimizu, Ichiro Sakata, Kimitaka Asatani, Masaru Isonuma, Naomi Sato, Shoya Ishizuka |  |
| 259 |  |  [Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation](https://doi.org/10.18653/v1/2022.findings-emnlp.258) |  | 0 | Prompt tuning, or the conditioning of a frozen pretrained language model (PLM) with soft prompts learned from data, has demonstrated impressive performance on a wide range of NLP tasks. However, prompt tuning requires a large training dataset to be effective and is outperformed by finetuning the... | Boyang Li, Han Yu, Xu Guo |  |
| 260 |  |  [McPhraSy: Multi-Context Phrase Similarity and Clustering](https://doi.org/10.18653/v1/2022.findings-emnlp.259) |  | 0 | Phrase similarity is a key component of many NLP applications. Current phrase similarity methods focus on embedding the phrase itself and use the phrase context only during training of the pretrained model. To better leverage the information in the context, we propose McPhraSy (Multi-context Phrase... | Amir David Nissan Cohen, Hila Gonen, Ori Shapira, Ran Levy, Yoav Goldberg |  |
| 261 |  |  [CANarEx: Contextually Aware Narrative Extraction for Semantically Rich Text-as-data Applications](https://doi.org/10.18653/v1/2022.findings-emnlp.260) |  | 0 | Narrative modelling is an area of active research, motivated by the acknowledgement of narratives as drivers of societal decision making. These research efforts conceptualize narratives as connected entity chains, and modeling typically focuses on the identification of entities and their... | Lachlan O'Neill, Nandini Anantharama, Simon D. Angus |  |
| 262 |  |  [Narrate Dialogues for Better Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.261) |  | 0 | Dialogue summarization models aim to generate a concise and accurate summary for multi-party dialogue. The complexity of dialogue, including coreference, dialogue acts, and inter-speaker interactions bring unique challenges to dialogue summarization. Most recent neural models achieve state-of-art... | Chenguang Zhu, Michael Zeng, Ruochen Xu |  |
| 263 |  |  [Towards Identifying Social Bias in Dialog Systems: Framework, Dataset, and Benchmark](https://doi.org/10.18653/v1/2022.findings-emnlp.262) |  | 0 | Among all the safety concerns that hinder the deployment of open-domain dialog systems (e.g., offensive languages, biases, and toxic behaviors), social bias presents an insidious challenge. Addressing this challenge requires rigorous analyses and normative reasoning. In this paper, we focus our... | Fei Mi, Helen Meng, Jiawen Deng, Jingyan Zhou, Minlie Huang, Qun Liu, Xin Jiang, Yasheng Wang, Yitong Li |  |
| 264 |  |  [CrossRE: A Cross-Domain Dataset for Relation Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.263) |  | 0 | Relation Extraction (RE) has attracted increasing attention, but current RE evaluation is limited to in-domain evaluation setups. Little is known on how well a RE system fares in challenging, but realistic out-of-distribution evaluation setups. To address this gap, we propose CrossRE, a new,... | Barbara Plank, Elisa Bassignana |  |
| 265 |  |  [Probing Structural Knowledge from Pre-trained Language Model for Argumentation Relation Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.264) |  | 0 | Extracting fine-grained structural information between argumentation component (AC) pairs is essential for argumentation relation classification (ARC). However, most previous studies attempt to model the relationship between AC pairs using AC level similarity or semantically relevant features. They... | Bin Liang, Jianzhu Bao, Min Yang, Ruifeng Xu, Yang Sun |  |
| 266 |  |  [LogicNMR: Probing the Non-monotonic Reasoning Ability of Pre-trained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.265) |  | 0 | The logical reasoning capabilities of pre-trained language models have recently received much attention. As one of the vital reasoning paradigms, non-monotonic reasoning refers to the fact that conclusions may be invalidated with new information. Existing work has constructed a non-monotonic... | Yeliang Xiu, Yongmei Liu, Zhanhao Xiao |  |
| 267 |  |  [Cheater's Bowl: Human vs. Computer Search Strategies for Open-Domain QA](https://doi.org/10.18653/v1/2022.findings-emnlp.266) |  | 0 | For humans and computers, the first step in answering an open-domain question is retrieving a set of relevant documents from a large corpus. However, the strategies that computers use fundamentally differ from those of humans. To better understand these differences, we design a gamified interface... | Andrew Mao, Jordan L. BoydGraber, Wanrong He |  |
| 268 |  |  [FRSUM: Towards Faithful Abstractive Summarization via Enhancing Factual Robustness](https://doi.org/10.18653/v1/2022.findings-emnlp.267) |  | 0 | Despite being able to generate fluent and grammatical text, current Seq2Seq summarization models still suffering from the unfaithful generation problem.In this paper, we study the faithfulness of existing systems from a new perspective of factual robustness which is the ability to correctly... | Hua Wu, Jiachen Liu, Sujian Li, Wei Li, Wenhao Wu, Xinyan Xiao, Ziqiang Cao |  |
| 269 |  |  [PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.268) |  | 0 | Formal verse poetry imposes strict constraints on the meter and rhyme scheme of poems. Most prior work on generating this type of poetry uses existing poems for supervision, which are difficult to obtain for most languages and poetic forms. In this work, we propose an unsupervised approach to... | Aitor Ormazabal, Aitor Soroa, Eneko Agirre, Manex Agirrezabal, Mikel Artetxe |  |
| 270 |  |  [ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback](https://doi.org/10.18653/v1/2022.findings-emnlp.269) |  | 0 | Recently, dataset-generation-based zero-shot learning has shown promising results by training a task-specific model with a dataset synthesized from large pre-trained language models (PLMs). The final task-specific model often achieves compatible or even better performance than PLMs under the... | Jiacheng Ye, Jiahui Gao, Jiangtao Feng, Lingpeng Kong, Tao Yu, Zhiyong Wu |  |
| 271 |  |  [Constructing Highly Inductive Contexts for Dialogue Safety through Controllable Reverse Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.270) |  | 0 | Large pretrained language models can easily produce toxic or biased content, which is prohibitive for practical use. In order to detect such toxic generations, existing methods rely on templates, real-world data extraction, crowdsourcing workers or automatic generation to construct adversarial... | Fei Mi, Hao Sun, Jiale Cheng, Jiawen Deng, Lifeng Shang, Minlie Huang, Yasheng Wang, Zhexin Zhang |  |
| 272 |  |  [Language Prior Is Not the Only Shortcut: A Benchmark for Shortcut Learning in VQA](https://doi.org/10.18653/v1/2022.findings-emnlp.271) |  | 0 | Visual Question Answering (VQA) models are prone to learn the shortcut solution formed by dataset biases rather than the intended solution. To evaluate the VQA models’ reasoning ability beyond shortcut learning, the VQA-CP v2 dataset introduces a distribution shift between the training and test set... | Fandong Meng, Jie Zhou, Mingyu Zheng, Peng Fu, Qingyi Si, Weiping Wang, Yanan Cao, Yuanxin Liu, Zheng Lin |  |
| 273 |  |  [Bridging the Training-Inference Gap for Dense Phrase Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.272) |  | 0 | Building dense retrievers requires a series of standard procedures, including training and validating neural models and creating indexes for efficient search. However, these procedures are often misaligned in that training objectives do not exactly reflect the retrieval scenario at inference time.... | Barlas Oguz, Gyuwan Kim, Jinhyuk Lee, Wenhan Xiong, William Yang Wang, Yashar Mehdad, Yizhe Zhang |  |
| 274 |  |  [Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources](https://doi.org/10.18653/v1/2022.findings-emnlp.273) |  | 0 | While the NLP community is generally aware of resource disparities among languages, we lack research that quantifies the extent and types of such disparity. Prior surveys estimating the availability of resources based on the number of datasets can be misleading as dataset quality varies: many... | Akari Asai, Eunsol Choi, Junjie Hu, Trina Chatterjee, Xinyan Yu |  |
| 275 |  |  [ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.274) |  | 0 | Recent years have witnessed the rise and success of pre-training techniques in visually-rich document understanding. However, most existing methods lack the systematic mining and utilization of layout-centered knowledge, leading to sub-optimal performances. In this paper, we propose ERNIE-Layout, a... | Bin Luo, Haifeng Wang, Hao Tian, Hua Wu, Qiming Peng, Shikun Feng, Weichong Yin, Wenjin Wang, Yin Zhang, Yinxu Pan, Yongfeng Chen, Yu Sun, Yuhui Cao, Zhengjie Huang, Zhenyu Zhang |  |
| 276 |  |  [Do Charge Prediction Models Learn Legal Theory?](https://doi.org/10.18653/v1/2022.findings-emnlp.275) |  | 0 | The charge prediction task aims to predict the charge for a case given its fact description. Recent models have already achieved impressive accuracy in this task, however, little is understood about the mechanisms they use to perform the judgment.For practical applications, a charge prediction... | Cong Jiang, Dongyan Zhao, Quzhe Huang, Yansong Feng, Zhenwei An |  |
| 277 |  |  [Keep Me Updated! Memory Management in Long-term Conversations](https://doi.org/10.18653/v1/2022.findings-emnlp.276) |  | 0 | Remembering important information from the past and continuing to talk about it in the present are crucial in long-term conversations. However, previous literature does not deal with cases where the memorized information is outdated, which may cause confusion in later conversations. To address this... | DongHyun Kwak, Hyeri Kim, Min Young Lee, Nako Sung, SangWoo Lee, Sanghwan Bae, Soyoung Kang, Sungdong Kim, WooMyoung Park, Yuin Jeong |  |
| 278 |  |  [A Unified Dialogue User Simulator for Few-shot Data Augmentation](https://doi.org/10.18653/v1/2022.findings-emnlp.277) |  | 0 | Pre-trained language models have shown superior performance in task-oriented dialogues. However, existing datasets are on limited scales, which cannot support large-scale pre-training. Fortunately, various data augmentation methods have been developed to augment large-scale task-oriented dialogue... | Dazhen Wan, Lizi Liao, Minlie Huang, Qi Zhu, Zheng Zhang |  |
| 279 |  |  [An Error-Guided Correction Model for Chinese Spelling Error Correction](https://doi.org/10.18653/v1/2022.findings-emnlp.278) |  | 0 | Although existing neural network approaches have achieved great progress on Chinese spelling correction, there is still room to improve. The model is required to avoid over-correction and to distinguish a correct token from its phonological and visual similar ones. In this paper, we propose an... | Rui Sun, Xiuyu Wu, Yunfang Wu |  |
| 280 |  |  [Describing Sets of Images with Textual-PCA](https://doi.org/10.18653/v1/2022.findings-emnlp.279) |  | 0 | We seek to semantically describe a set of images, capturing both the attributes of single images and the variations within the set. Our procedure is analogous to Principle Component Analysis, in which the role of projection vectors is replaced with generated phrases. First, a centroid phrase that... | Idan Schwartz, Lior Wolf, Oded Hupert |  |
| 281 |  |  [Learning to Model Editing Processes](https://doi.org/10.18653/v1/2022.findings-emnlp.280) |  | 0 | Most existing sequence generation models produce outputs in one pass, usually left-to-right. However, this is in contrast with a more natural approach that humans use in generating content; iterative refinement and editing. Recent work has introduced edit-based models for various tasks (such as... | Graham Neubig, Machel Reid |  |
| 282 |  |  [PALT: Parameter-Lite Transfer of Language Models for Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.findings-emnlp.281) |  | 0 | This paper presents a parameter-lite transfer learning approach of pretrained language models (LM) for knowledge graph (KG) completion. Instead of finetuning, which modifies all LM parameters, we only tune a few new parameters while keeping the original LM parameters fixed. We establish this via... | Chenguang Wang, Dawn Song, Heng Ji, Jianhao Shen, Jiawei Han, Koushik Sen, Ming Zhang, Ye Yuan |  |
| 283 |  |  [Prompt-based Connective Prediction Method for Fine-grained Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.282) |  | 0 | Due to the absence of connectives, implicit discourse relation recognition (IDRR) is still a challenging and crucial task in discourse analysis. Most of the current work adopted multitask learning to aid IDRR through explicit discourse relation recognition (EDRR) or utilized dependencies between... | Hao Zhou, Man Lan, Meirong Ma, Yuanbin Wu, Yuefeng Chen |  |
| 284 |  |  [On Utilizing Constituent Language Resources to Improve Downstream Tasks in Hinglish](https://doi.org/10.18653/v1/2022.findings-emnlp.283) |  | 0 | Performance of downstream NLP tasks on code-switched Hindi-English (aka ) continues to remain a significant challenge. Intuitively, Hindi and English corpora should aid improve task performance on Hinglish. We show that meta-learning framework can effectively utilize the the labelled resources of... | Rudra Murthy, Tejas I. Dhamecha, Vishwajeet Kumar |  |
| 285 |  |  [SYGMA: A System for Generalizable and Modular Question Answering Over Knowledge Bases](https://doi.org/10.18653/v1/2022.findings-emnlp.284) |  | 0 | Knowledge Base Question Answering (KBQA) involving complex reasoning is emerging as an important research direction. However, most KBQA systems struggle with generalizability, particularly on two dimensions: (a) across multiple knowledge bases, where existing KBQA approaches are typically tuned to... | Achille Fokoue, Alexander Gray, Cezar Pendus, Dinesh Garg, Dinesh Khandelwal, Francois P. S. Luus, G. P. Shrivatsa Bhargav, Guilherme Lima, Hima Karanam, Ibrahim Abdelaziz, L. Venkata Subramaniam, Maria Chang, Nandana Mihindukulasooriya, Pavan Kapanipathi, Rosario UcedaSosa, Ryan Riegel, Sairam Gurajada, Salim Roukos, Santosh K. Srivastava, Saswati Dana, Shajith Ikbal, Srinivas Ravishankar, Sumit Neelam, Udit Sharma, YoungSuk Lee |  |
| 286 |  |  [Instance-Guided Prompt Learning for Few-Shot Text Matching](https://doi.org/10.18653/v1/2022.findings-emnlp.285) |  | 0 | Few-shot text matching is a more practical technique in natural language processing (NLP) to determine whether two texts are semantically identical. They primarily design patterns to reformulate text matching into a pre-trained task with uniform prompts across all instances. But they fail to take... | Dongliang Xu, Jia Du, Kai Wang, Lei Li, Qing Yang, Siyi Wang, Xuanyu Zhang, Yanquan Zhou |  |
| 287 |  |  [M3: Multi-level dataset for Multi-document summarisation of Medical studies](https://doi.org/10.18653/v1/2022.findings-emnlp.286) |  | 0 | We present M3 (Multi-level dataset for Multi-document summarisation of Medical studies), a benchmark dataset for evaluating the quality of summarisation systems in the biomedical domain. The dataset contains sets of multiple input documents and target summaries of three levels of complexity:... | Antonio JimenoYepes, Jey Han Lau, Karin Verspoor, Timothy Baldwin, Yulia Otmakhova |  |
| 288 |  |  [Adapters for Enhanced Modeling of Multilingual Knowledge and Text](https://doi.org/10.18653/v1/2022.findings-emnlp.287) |  | 0 | Large language models appear to learn facts from the large text corpora they are trained on. Such facts are encoded implicitly within their many parameters, making it difficult to verify or manipulate what knowledge has been learned. Language models have recently been extended to multilingual... | Carl Allen, Meizhen Liu, Mrinmaya Sachan, Wenxiang Jiao, Yifan Hou, Zhaopeng Tu |  |
| 289 |  |  [SepLL: Separating Latent Class Labels from Weak Supervision Noise](https://doi.org/10.18653/v1/2022.findings-emnlp.288) |  | 0 | In the weakly supervised learning paradigm, labeling functions automatically assign heuristic, often noisy, labels to data samples. In this work, we provide a method for learning from weak labels by separating two types of complementary information associated with the labeling functions:... | Andreas Stephan, Benjamin Roth, Vasiliki Kougia |  |
| 290 |  |  [Probing Relational Knowledge in Language Models via Word Analogies](https://doi.org/10.18653/v1/2022.findings-emnlp.289) |  | 0 | Understanding relational knowledge plays an integral part in natural language comprehension. When it comes to pre-trained language models (PLM), prior work has been focusing on probing relational knowledge this by filling the blanks in pre-defined prompts such as “The capital of France is —".... | José CamachoCollados, Kiamehr Rezaee |  |
| 291 |  |  [Semi-Supervised Lifelong Language Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.290) |  | 0 | Lifelong learning aims to accumulate knowledge and alleviate catastrophic forgetting when learning tasks sequentially. However, existing lifelong language learning methods only focus on the supervised learning setting. Unlabeled data, which can be easily accessed in real-world scenarios, are... | Bowen Yu, Dongkyu Lee, Jian Sun, Nevin L. Zhang, Yingxiu Zhao, Yinhe Zheng, Yongbin Li, Zhiliang Tian |  |
| 292 |  |  [Parameter-free Automatically Prompting: A Latent Pseudo Label Mapping Model for Prompt-based Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.291) |  | 0 | Prompt-based learning has achieved excellent performance in few-shot learning by mapping the outputs of the pre-trained language model to the labels with the help of a label mapping component. Existing manual label mapping (MLM) methods achieve good results but heavily rely on expensive human... | Jaein Kim, Jirui Qi, Junfan Chen, Richong Zhang, Yongyi Mao |  |
| 293 |  |  [Exploring Logographic Image for Chinese Aspect-based Sentiment Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.292) |  | 0 | In logographic languages like Chinese, word meanings are constructed using specific character formations, which can help to disambiguate word senses and are beneficial for sentiment classification. However, such knowledge is rarely explored in previous sentiment analysis methods. In this paper, we... | Renjie Feng, Xiabing Zhou, Xiaotong Jiang, Zhongqing Wang |  |
| 294 |  |  [On the Role of Bidirectionality in Language Model Pre-Training](https://doi.org/10.18653/v1/2022.findings-emnlp.293) |  | 0 | Prior work on language model pre-training has explored different architectures and learning objectives, but differences in data, hyperparameters and evaluation make a principled comparison difficult. In this work, we focus on bidirectionality as a key factor that differentiates existing approaches,... | Jingfei Du, Luke Zettlemoyer, Mikel Artetxe, Naman Goyal, Veselin Stoyanov |  |
| 295 |  |  [You Are What You Talk About: Inducing Evaluative Topics for Personality Analysis](https://doi.org/10.18653/v1/2022.findings-emnlp.294) |  | 0 | Expressing attitude or stance toward entities and concepts is an integral part of human behavior and personality. Recently, evaluative language data has become more accessible with social media’s rapid growth, enabling large-scale opinion analysis. However, surprisingly little research examines the... | Iva Vukojevic, Jan Snajder, Josip Jukic |  |
| 296 |  |  [CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure](https://doi.org/10.18653/v1/2022.findings-emnlp.295) |  | 0 | Code pre-trained models (CodePTMs) have recently demonstrated significant success in code intelligence. To interpret these models, some probing methods have been applied. However, these methods fail to consider the inherent characteristics of codes. In this paper, to address the problem, we propose... | Ming Gao, Nuo Chen, Qiushi Sun, Renyu Zhu, Xiang Li, Xuesong Lu |  |
| 297 |  |  [Learning to Revise References for Faithful Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.296) |  | 0 | In real-world scenarios with naturally occurring datasets, reference summaries are noisy and may contain information that cannot be inferred from the source text. On large news corpora, removing low quality samples has been shown to reduce model hallucinations. Yet, for smaller, and/or noisier... | Christopher Winestock, Griffin Adams, HanChin Shing, Kathleen R. McKeown, Noémie Elhadad, Qing Sun |  |
| 298 |  |  [Towards Intention Understanding in Suicidal Risk Assessment with Natural Language Processing](https://doi.org/10.18653/v1/2022.findings-emnlp.297) |  | 0 | Recent applications of natural language processing techniques to suicidal ideation detection and risk assessment frame the detection or assessment task as a text classification problem. Recent advances have developed many models, especially deep learning models, to boost predictive... | Shaoxiong Ji |  |
| 299 |  |  [On the Impact of Temporal Concept Drift on Model Explanations](https://doi.org/10.18653/v1/2022.findings-emnlp.298) |  | 0 | Explanation faithfulness of model predictions in natural language processing is typically evaluated on held-out data from the same temporal distribution as the training data (i.e. synchronous settings). While model performance often deteriorates due to temporal variation (i.e. temporal concept... | George Chrysostomou, Kalina Bontcheva, Nikolaos Aletras, Zhixue Zhao |  |
| 300 |  |  [Text-Only Training for Image Captioning using Noise-Injected CLIP](https://doi.org/10.18653/v1/2022.findings-emnlp.299) |  | 0 | We consider the task of image-captioning using only the CLIP model and additional text data at training time and no additional captioned images. Our approach relies on the fact that CLIP is trained to make visual and textual embeddings similar. Therefore, we only need to learn how to translate CLIP... | Amir Globerson, David Nukrai, Ron Mokady |  |
| 301 |  |  [Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.300) |  | 0 | Fine-tuning large pretrained language models on a limited training corpus usually suffers from poor generalization. Prior works show that the recently-proposed sharpness-aware minimization (SAM) optimization method can improve the model generalization. However, SAM adds a perturbation to each model... | Bo Du, Dacheng Tao, Juhua Liu, Li Shen, Liang Ding, Peng Mi, Qihuang Zhong |  |
| 302 |  |  [TINA: Textual Inference with Negation Augmentation](https://doi.org/10.18653/v1/2022.findings-emnlp.301) |  | 0 | Transformer-based language models achieve state-of-the-art results on several natural language processing tasks. One of these is textual entailment, i.e., the task of determining whether a premise logically entails a hypothesis. However, the models perform poorly on this task when the examples... | Chadi Helwe, Chloé Clavel, Fabian M. Suchanek, Simon Coumes |  |
| 303 |  |  [Improving Bilingual Lexicon Induction with Cross-Encoder Reranking](https://doi.org/10.18653/v1/2022.findings-emnlp.302) |  | 0 | Bilingual lexicon induction (BLI) with limited bilingual supervision is a crucial yet challenging task in multilingual NLP. Current state-of-the-art BLI methods rely on the induction of cross-lingual word embeddings (CLWEs) to capture cross-lingual word similarities; such CLWEs are obtained... | Anna Korhonen, Fangyu Liu, Ivan Vulic, Yaoyiran Li |  |
| 304 |  |  [Mixed-modality Representation Learning and Pre-training for Joint Table-and-Text Retrieval in OpenQA](https://doi.org/10.18653/v1/2022.findings-emnlp.303) |  | 0 | Retrieving evidences from tabular and textual resources is essential for open-domain question answering (OpenQA), which provides more comprehensive information. However, training an effective dense table-text retriever is difficult due to the challenges of table-text discrepancy and data sparsity... | Daxin Jiang, Junjie Huang, Ming Gong, Nan Duan, Qian Liu, Wanjun Zhong |  |
| 305 |  |  [The Effects of Corpus Choice and Morphosyntax on Multilingual Space Induction](https://doi.org/10.18653/v1/2022.findings-emnlp.304) |  | 0 | In an effort to study the inductive biases of language models, numerous studies have attempted to use linguistically motivated tasks as a proxy of sorts, wherein performance on these tasks would imply an inductive bias towards a specific linguistic phenomenon. In this study, we attempt to analyse... | Joakim Nivre, Vinit Ravishankar |  |
| 306 |  |  [Modeling Complex Dialogue Mappings via Sentence Semantic Segmentation Guided Conditional Variational Auto-Encoder](https://doi.org/10.18653/v1/2022.findings-emnlp.305) |  | 0 | Complex dialogue mappings (CDM), including one-to-many and many-to-one mappings, tend to make dialogue models generate incoherent or dull responses, and modeling these mappings remains a huge challenge for neural dialogue systems. To alleviate these problems, methods like introducing external... | Bin Sun, Fei Mi, Kan Li, Shaoxiong Feng, Weichao Wang, Yitong Li, Yiwei Li |  |
| 307 |  |  [Graph Embeddings for Argumentation Quality Assessment](https://doi.org/10.18653/v1/2022.findings-emnlp.306) |  | 0 | Argumentation is used by people both internally, by evaluating arguments and counterarguments to make sense of a situation and take a decision, and externally, e.g., in a debate, by exchanging arguments to reach an agreement or to promote an individual position. In this context, the assessment of... | Elena Cabrio, Santiago Marro, Serena Villata |  |
| 308 |  |  [SMiLE: Schema-augmented Multi-level Contrastive Learning for Knowledge Graph Link Prediction](https://doi.org/10.18653/v1/2022.findings-emnlp.307) |  | 0 | Link prediction is the task of inferring missing links between entities in knowledge graphs. Embedding-based methods have shown effectiveness in addressing this problem by modeling relational patterns in triples. However, the link prediction task often requires contextual information in entity... | Ben Liu, Hua Wang, Miao Peng, Min Peng, Qianqian Xie, Wenjie Xu |  |
| 309 |  |  [Multilingual Multimodal Learning with Machine Translated Text](https://doi.org/10.18653/v1/2022.findings-emnlp.308) |  | 0 | Most vision-and-language pretraining research focuses on English tasks. However, the creation of multilingual multimodal evaluation datasets (e.g. Multi30K, xGQA, XVNLI, and MaRVL) poses a new challenge in finding high-quality training data that is both multilingual and multimodal. In this paper,... | Chen Qiu, Dan Oneata, Desmond Elliott, Emanuele Bugliarello, Stella Frank |  |
| 310 |  |  [Learning From the Source Document: Unsupervised Abstractive Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.309) |  | 0 | Most of the state-of-the-art methods for abstractive text summarization are under supervised learning settings, while heavily relying on high-quality and large-scale parallel corpora. In this paper, we remove the need for reference summaries and present an unsupervised learning method SCR... | Congbo Ma, Haojie Zhuang, Jian Yang, Quan Z. Sheng, Wei Emma Zhang, Yutong Qu |  |
| 311 |  |  [How to Do Things without Words: Modeling Semantic Drift of Emoji](https://doi.org/10.18653/v1/2022.findings-emnlp.310) |  | 0 | Emoji have become a significant part of our informal textual communication. Previous work, addressing the societal and linguistic functions of emoji, overlooked the relation between the semantics and the visual variations of the symbols. In this paper we model and analyze the semantic drift of... | Eyal Arviv, Oren Tsur |  |
| 312 |  |  [Mind Your Bias: A Critical Review of Bias Detection Methods for Contextual Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.311) |  | 0 | The awareness and mitigation of biases are of fundamental importance for the fair and transparent use of contextual language models, yet they crucially depend on the accurate detection of biases as a precursor. Consequently, numerous bias detection methods have been proposed, which vary in their... | Andreas Spitz, Silke Husse |  |
| 313 |  |  [ZeroPrompt: Scaling Prompt-Based Pretraining to 1, 000 Tasks Improves Zero-Shot Generalization](https://doi.org/10.18653/v1/2022.findings-emnlp.312) |  | 0 | We propose a multitask pretraining approach ZeroPrompt for zero-shot generalization, focusing on task scaling and zero-shot prompting.While previous models are trained on only a few dozen tasks, we scale to 1,000 tasks for the first time using real-world data. This leads to a crucial discovery that... | Haiyu Li, Hanwei Xu, Nan Shao, Yanggang Wang, Yujun Chen, Yulun Du, Zhilin Yang |  |
| 314 |  |  [Semantic Role Labeling Meets Definition Modeling: Using Natural Language to Describe Predicate-Argument Structures](https://doi.org/10.18653/v1/2022.findings-emnlp.313) |  | 0 | One of the common traits of past and present approaches for Semantic Role Labeling (SRL) is that they rely upon discrete labels drawn from a predefined linguistic inventory to classify predicate senses and their arguments.However, we argue this need not be the case. In this paper, we present an... | Alessandro Scirè, Edoardo Barba, Roberto Navigli, Simone Conia |  |
| 315 |  |  [Is anisotropy really the cause of BERT embeddings not being semantic?](https://doi.org/10.18653/v1/2022.findings-emnlp.314) |  | 0 | In this paper we conduct a set of experiments aimed to improve our understanding of the lack of semantic isometry in BERT, i.e. the lack of correspondence between the embedding and meaning spaces of its contextualized word representations. Our empirical results show that, contrary to popular... | Alejandro Fuster Baggetto, Víctor Fresno |  |
| 316 |  |  [m⌃4 Adapter: Multilingual Multi-Domain Adaptation for Machine Translation with a Meta-Adapter](https://doi.org/10.18653/v1/2022.findings-emnlp.315) |  | 0 | Multilingual neural machine translation models (MNMT) yield state-of-the-art performance when evaluated on data from a domain and language pair seen at training time. However, when a MNMT model is used to translate under domain shift or to a new language pair, performance drops dramatically. We... | Alexander Fraser, Alexandra Chronopoulou, Wen Lai |  |
| 317 |  |  [Textual Enhanced Contrastive Learning for Solving Math Word Problems](https://doi.org/10.18653/v1/2022.findings-emnlp.316) |  | 0 | Solving math word problems is the task that analyses the relation of quantities e and requires an accurate understanding of contextual natural language information. Recent studies show that current models rely on shallow heuristics to predict solutions and could be easily misled by small textual... | Fei Cheng, Qianying Liu, Sadao Kurohashi, Yibin Shen, Zhuoyuan Mao |  |
| 318 |  |  [What Do Compressed Multilingual Machine Translation Models Forget?](https://doi.org/10.18653/v1/2022.findings-emnlp.317) |  | 0 | Recently, very large pre-trained models achieve state-of-the-art results in various natural language processing (NLP) tasks, but their size makes it more challenging to apply them in resource-constrained environments. Compression techniques allow to drastically reduce the size of the models and... | Alexandre Berard, Alireza Mohammadshahi, Caroline Brun, James Henderson, Laurent Besacier, Vassilina Nikoulina |  |
| 319 |  |  [Controllable Dialogue Simulation with In-context Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.318) |  | 0 | Building dialogue systems requires a large corpus of annotated dialogues. Such datasets are usually created via crowdsourcing, which is expensive and time-consuming. In this paper, we propose Dialogic, a novel dialogue simulation method based on large language model in-context learning to automate... | Hong Wang, Jing Qian, Shiyang Li, Wenhu Chen, Xifeng Yan, Zekun Li |  |
| 320 |  |  [Improving the Factual Correctness of Radiology Report Generation with Semantic Rewards](https://doi.org/10.18653/v1/2022.findings-emnlp.319) |  | 0 | Neural image-to-text radiology report generation systems offer the potential to improve radiology reporting by reducing the repetitive process of report drafting and identifying possible medical errors. These systems have achieved promising performance as measured by widely used NLG metrics such as... | Christian Bluethgen, Curtis P. Langlotz, Emily Bao Tsai, JeanBenoit Delbrouck, Omar Almusa, Pierre J. Chambon |  |
| 321 |  |  [Recursive Neural Networks with Bottlenecks Diagnose (Non-)Compositionality](https://doi.org/10.18653/v1/2022.findings-emnlp.320) |  | 0 | A recent line of work in NLP focuses on the (dis)ability of models to generalise compositionally for artificial languages.However, when considering natural language tasks, the data involved is not strictly, or locally, compositional.Quantifying the compositionality of data is a challenging task,... | Ivan Titov, Verna Dankers |  |
| 322 |  |  [HumSet: Dataset of Multilingual Information Extraction and Classification for Humanitarian Crises Response](https://doi.org/10.18653/v1/2022.findings-emnlp.321) |  | 0 | Timely and effective response to humanitarian crises requires quick and accurate analysis of large amounts of text data – a process that can highly benefit from expert-assisted NLP systems trained on validated and annotated data in the humanitarian response domain. To enable creation of such NLP... | Benjamin Minixhofer, Ewan Oglethorpe, Navid Rekabsaz, Nicolò Tamagnone, Ranjan Shrestha, Selim Fekih, Ximena Contla |  |
| 323 |  |  [Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.322) |  | 0 | Non-autoregressive models achieve significant decoding speedup in neural machine translation but lack the ability to capture sequential dependency. Directed Acyclic Transformer (DA-Transformer) was recently proposed to model sequential dependency with a directed acyclic graph. Consequently, it has... | Chenze Shao, Yang Feng, Zhengrui Ma |  |
| 324 |  |  [Lexical Generalization Improves with Larger Models and Longer Training](https://doi.org/10.18653/v1/2022.findings-emnlp.323) |  | 0 | While fine-tuned language models perform well on many language tasks, they were also shown to rely on superficial surface features such as lexical overlap. Excessive utilization of such heuristics can lead to failure on challenging inputs. We analyze the use of lexical overlap heuristics in natural... | Elron Bandel, Yanai Elazar, Yoav Goldberg |  |
| 325 |  |  [Realistic Data Augmentation Framework for Enhancing Tabular Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.324) |  | 0 | Existing approaches to constructing training data for Natural Language Inference (NLI) tasks, such as for semi-structured table reasoning, are either via crowdsourcing or fully automatic methods. However, the former is expensive and time consuming and thus limits scale, and the latter often... | Dibyakanti Kumar, Shuo Zhang, Soumya Sharma, Vivek Gupta |  |
| 326 |  |  [Inducing Generalizable and Interpretable Lexica](https://doi.org/10.18653/v1/2022.findings-emnlp.325) |  | 0 | Lexica – words and associated scores – are widely used as simple, interpretable, generalizable language features to predict sentiment, emotions, mental health, and personality. They also provide insight into the psychological features behind those moods and traits. Such lexica, historically created... | João Sedoc, Lyle H. Ungar, Roshan Santhosh, Tejas Srivastava, Yilin Geng, Zetian Wu |  |
| 327 |  |  [The Curious Case of Absolute Position Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.326) |  | 0 | Transformer language models encode the notion of word order using positional information. Most commonly, this positional information is represented by absolute position embeddings (APEs), that are learned from the pretraining data. However, in natural language, it is not absolute position that... | Adina Williams, Amirhossein Kazemnejad, Dieuwke Hupkes, Joelle Pineau, Koustuv Sinha, Siva Reddy |  |
| 328 |  |  [Goal-oriented Vision-and-Dialog Navigation via Reinforcement Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.327) |  | 0 | Vision-and-dialog navigation is a recent benchmark for evaluating the AI capabilities of perception, interaction, and decision making. While existing methods developed for this benchmark have demonstrated great successes, they mostly rely on large datasets, where data collection can be a challenge,... | David DeFazio, Keting Lu, Shiqi Zhang, Yan Cao |  |
| 329 |  |  [Leveraging Data Recasting to Enhance Tabular Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.328) |  | 0 | Creating challenging tabular inference data is essential for learning complex reasoning. Prior work has mostly relied on two data generation strategies. The first is human annotation, which yields linguistically diverse data but is difficult to scale. The second category for creation is synthetic... | Aashna Jena, Julian Martin Eisenschlos, Manish Shrivastava, Vivek Gupta |  |
| 330 |  |  [Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again](https://doi.org/10.18653/v1/2022.findings-emnlp.329) |  | 0 | Large pre-trained language models (PLMs) such as GPT-3 have shown strong in-context learning capabilities, which are highly appealing for domains such as biomedicine that feature high and diverse demands of language technologies but also high data annotation costs. In this paper, we present the... | Bernal Jimenez Gutierrez, Clayton Washington, Huan Sun, Lang Li, Nikolas McNeal, You Chen, Yu Su |  |
| 331 |  |  [Attention weights accurately predict language representations in the brain](https://doi.org/10.18653/v1/2022.findings-emnlp.330) |  | 0 | In Transformer-based language models (LMs) the attention mechanism converts token embeddings into contextual embeddings that incorporate information from neighboring words. The resulting contextual hidden state embeddings have enabled highly accurate models of brain responses, suggesting that the... | Catherine Chen, Fatma Deniz, Mathis Lamarre |  |
| 332 |  |  [Improving HowNet-Based Chinese Word Sense Disambiguation with Translations](https://doi.org/10.18653/v1/2022.findings-emnlp.331) |  | 0 | Word sense disambiguation (WSD) is the task of identifying the intended sense of a word in context. While prior work on unsupervised WSD has leveraged lexical knowledge bases, such as WordNet and BabelNet, these resources have proven to be less effective for Chinese. Instead, the most widely used... | Bradley Hauer, Grzegorz Kondrak, Xiang Zhang |  |
| 333 |  |  [Mask-then-Fill: A Flexible and Effective Data Augmentation Framework for Event Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.332) |  | 0 | We present Mask-then-Fill, a flexible and effective data augmentation framework for event extraction. Our approach allows for more flexible manipulation of text and thus can generate more diverse data while keeping the original event structure unchanged as much as possible. Specifically, it first... | Changlong Yu, Huan Zhao, Jun Gao, Ruifeng Xu, Wei Wang |  |
| 334 |  |  [MOBA-E2C: Generating MOBA Game Commentaries via Capturing Highlight Events from the Meta-Data](https://doi.org/10.18653/v1/2022.findings-emnlp.333) |  | 0 | MOBA (Multiplayer Online Battle Arena) games such as Dota2 are currently one of the most popular e-sports gaming genres. Following professional commentaries is a great way to understand and enjoy a MOBA game. However, massive game competitions lack commentaries because of the shortage of... | Dawei Zhang, Sixing Wu, Xiangqun Chen, Yao Guo |  |
| 335 |  |  [Enhancing Automatic Readability Assessment with Pre-training and Soft Labels for Ordinal Regression](https://doi.org/10.18653/v1/2022.findings-emnlp.334) |  | 0 | The readability assessment task aims to assign a difficulty grade to a text. While neural models have recently demonstrated impressive performance, most do not exploit the ordinal nature of the difficulty grades, and make little effort for model initialization to facilitate fine-tuning. We address... | DingXuan Zhou, Jinshan Zeng, John Lee, Xianglong Yu, Yudong Xie |  |
| 336 |  |  [Opening up Minds with Argumentative Dialogues](https://doi.org/10.18653/v1/2022.findings-emnlp.335) |  | 0 | Recent research on argumentative dialogues has focused on persuading people to take some action, changing their stance on the topic of discussion, or winning debates. In this work, we focus on argumentative dialogues that aim to open up (rather than change) people’s minds to help them become more... | Andreas Vlachos, Charlotte O. Brand, Jacopo Amidei, Paul Piwek, Svetlana Stoyanchev, Tom Stafford, Youmna Farag |  |
| 337 |  |  [You Are My Type! Type Embeddings for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.336) |  | 0 | One reason for the positive impact of Pre-trained Language Models (PLMs) in NLP tasks is their ability to encode semantic types, such as ‘European City’ or ‘Woman’. While previous work has analyzed such information in the context of interpretability, it is not clear how to use types to steer the... | Mohammed Saeed, Paolo Papotti |  |
| 338 |  |  [Generating Textual Adversaries with Minimal Perturbation](https://doi.org/10.18653/v1/2022.findings-emnlp.337) |  | 0 | Many word-level adversarial attack approaches for textual data have been proposed in recent studies. However, due to the massive search space consisting of combinations of candidate words, the existing approaches face the problem of preserving the semantics of texts when crafting adversarial... | Depeng Xu, Lu Zhang, Shuhan Yuan, Xingyi Zhao |  |
| 339 |  |  [SensePOLAR: Word sense aware interpretability for pre-trained contextual word embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.338) |  | 0 | Adding interpretability to word embeddings represents an area of active research in textrepresentation. Recent work has explored the potential of embedding words via so-called polardimensions (e.g. good vs. bad, correct vs. wrong). Examples of such recent approachesinclude SemAxis, POLAR,... | Jan Engler, Markus Strohmaier, Marlene Lutz, Sandipan Sikdar |  |
| 340 |  |  [Contextualizing Language Models for Norms Diverging from Social Majority](https://doi.org/10.18653/v1/2022.findings-emnlp.339) |  | 0 | To comprehensibly contextualize decisions, artificial systems in social situations need a high degree of awareness of the rules of conduct of human behavior. Especially transformer-based language models have recently been shown to exhibit some such awareness. But what if norms in some social... | Hermann Kroll, Niklas Kiehne, WolfTilo Balke |  |
| 341 |  |  [Empathetic Dialogue Generation via Sensitive Emotion Recognition and Sensible Knowledge Selection](https://doi.org/10.18653/v1/2022.findings-emnlp.340) |  | 0 | Empathy, which is widely used in psychological counseling, is a key trait of everyday human conversations. Equipped with commonsense knowledge, current approaches to empathetic response generation focus on capturing implicit emotion within dialogue context, where the emotions are treated as a... | Chenxu Yang, Fandong Meng, Jiangnan Li, Jie Zhou, Lanrui Wang, Weiping Wang, Zheng Lin |  |
| 342 |  |  [Joint Multilingual Knowledge Graph Completion and Alignment](https://doi.org/10.18653/v1/2022.findings-emnlp.341) |  | 0 | Knowledge graph (KG) alignment and completion are usually treated as two independent tasks. While recent work has leveraged entity and relation alignments from multiple KGs, such as alignments between multilingual KGs with common entities and relations, a deeper understanding of the ways in which... | Dat Quoc Nguyen, Mathias Niepert, Quoc Viet Hung Nguyen, Tam Thanh Nguyen, Trung Thanh Huynh, Vinh Tong |  |
| 343 |  |  [A Framework for Automatic Generation of Spoken Question-Answering Data](https://doi.org/10.18653/v1/2022.findings-emnlp.342) |  | 0 | This paper describes a framework to automatically generate a spoken question answering (QA) dataset. The framework consists of a question generation (QG) module to generate questions automatically from given text documents, a text-to-speech (TTS) module to convert the text documents into spoken... | Arzucan Özgür, Ebru Arisoy, Merve Ünlü Menevse, Yusufcan Manav |  |
| 344 |  |  [Readability Controllable Biomedical Document Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.343) |  | 0 | Different from general documents, it is recognised that the ease with which people can understand a biomedical text is eminently varied, owing to the highly technical nature of biomedical documents and the variance of readers’ domain knowledge. However, existing biomedical document summarization... | Qianqian Xie, Sophia Ananiadou, Zheheng Luo |  |
| 345 |  |  [Beyond Additive Fusion: Learning Non-Additive Multimodal Interactions](https://doi.org/10.18653/v1/2022.findings-emnlp.344) |  | 0 | Multimodal fusion addresses the problem of analyzing spoken words in the multimodal context, including visual expressions and prosodic cues. Even when multimodal models lead to performance improvements, it is often unclear whether bimodal and trimodal interactions are learned or whether modalities... | Jeffrey F. Cohn, Lisa Sheeber, LouisPhilippe Morency, Nicholas B. Allen, Torsten Wörtwein |  |
| 346 |  |  [Generalization Differences between End-to-End and Neuro-Symbolic Vision-Language Reasoning Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.345) |  | 0 | For vision-and-language reasoning tasks, both fully connectionist, end-to-end methods and hybrid, neuro-symbolic methods have achieved high in-distribution performance. In which out-of-distribution settings does each paradigm excel? We investigate this question on both single-image and multi-image... | Jesse Thomason, Robin Jia, Wang Zhu |  |
| 347 |  |  [Learning to Model Multimodal Semantic Alignment for Story Visualization](https://doi.org/10.18653/v1/2022.findings-emnlp.346) |  | 0 | Story visualization aims to generate a sequence of images to narrate each sentence in a multi-sentence story, where the images should be realistic and keep global consistency across dynamic scenes and characters. Current works face the problem of semantic misalignment because of their fixed... | Bowen Li, Thomas Lukasiewicz |  |
| 348 |  |  [SciFact-Open: Towards open-domain scientific claim verification](https://doi.org/10.18653/v1/2022.findings-emnlp.347) |  | 0 | While research on scientific claim verification has led to the development of powerful systems that appear to approach human performance, these approaches have yet to be tested in a realistic setting against large corpora of scientific literature. Moving to this open-domain evaluation setting,... | Arman Cohan, Bailey Kuehl, David Wadden, Hannaneh Hajishirzi, Iz Beltagy, Kyle Lo, Lucy Lu Wang |  |
| 349 |  |  [COMET-QE and Active Learning for Low-Resource Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.348) |  | 0 | Active learning aims to deliver maximum benefit when resources are scarce. We use COMET-QE, a reference-free evaluation metric, to select sentences for low-resource neural machine translation. Using Swahili, Kinyarwanda and Spanish for our experiments, we show that COMET-QE significantly... | Bruce A. Bassett, Everlyn Chimoto |  |
| 350 |  |  [MedicalSum: A Guided Clinical Abstractive Summarization Model for Generating Medical Reports from Patient-Doctor Conversations](https://doi.org/10.18653/v1/2022.findings-emnlp.349) |  | 0 | We introduce MedicalSum, a transformer-based sequence-to-sequence architecture for summarizing medical conversations by integrating medical domain knowledge from the Unified Medical Language System (UMLS). The novel knowledge augmentation is performed in three ways: (i) introducing a guidance... | Gagandeep Singh, George Michalopoulos, Kyle Williams, Thomas Lin |  |
| 351 |  |  [Leveraging Training Dynamics and Self-Training for Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.350) |  | 0 | The effectiveness of pre-trained language models in downstream tasks is highly dependent on the amount of labeled data available for training. Semi-supervised learning (SSL) is a promising technique that has seen wide attention recently due to its effectiveness in improving deep learning models... | Cornelia Caragea, Tiberiu Sosea |  |
| 352 |  |  [Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference](https://doi.org/10.18653/v1/2022.findings-emnlp.351) |  | 0 | Natural Language Inference (NLI) or Recognizing Textual Entailment (RTE) aims at predicting the relation between a pair of sentences (premise and hypothesis) as entailment, contradiction or semantic independence. Although deep learning models have shown promising performance for NLI in recent... | Cornelia Caragea, Mobashir Sadat |  |
| 353 |  |  [Unsupervised Text Deidentification](https://doi.org/10.18653/v1/2022.findings-emnlp.352) |  | 0 | Deidentification seeks to anonymize textual data prior to distribution. Automatic deidentification primarily uses supervised named entity recognition from human-labeled data points. We propose an unsupervised deidentification method that masks words that leak personally-identifying information. The... | Alexander M. Rush, John X. Morris, Justin T. Chiu, Ramin Zabih |  |
| 354 |  |  [Federated Continual Learning for Text Classification via Selective Inter-client Transfer](https://doi.org/10.18653/v1/2022.findings-emnlp.353) |  | 0 | In this work, we combine the two paradigms: Federated Learning (FL) and Continual Learning (CL) for text classification task in cloud-edge continuum. The objective of Federated Continual Learning (FCL) is to improve deep learning models over life time at each client by (relevant and efficient)... | Hinrich Schütze, Matthias Schubert, Pankaj Gupta, Pranav Rai, Yatin Chaudhary |  |
| 355 |  |  [DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents](https://doi.org/10.18653/v1/2022.findings-emnlp.354) |  | 0 | In the real world, autonomous driving agents navigate in highly dynamic environments full of unexpected situations where pre-trained models are unreliable. In these situations, what is immediately available to vehicles is often only human operators. Empowering autonomous driving agents with the... | Benjamin VanDerPloeg, CristianPaul Bara, EuiIn Kim, Felix Gervits, Joyce Chai, Matthew Marge, Yidong Huang, Ziqiao Ma |  |
| 356 |  |  [He Said, She Said: Style Transfer for Shifting the Perspective of Dialogues](https://doi.org/10.18653/v1/2022.findings-emnlp.355) |  | 0 | In this work, we define a new style transfer task: perspective shift, which reframes a dialouge from informal first person to a formal third person rephrasing of the text. This task requires challenging coreference resolution, emotion attribution, and interpretation of informal text. We explore... | Amanda Bertsch, Graham Neubig, Matthew R. Gormley |  |
| 357 |  |  [Dynamic Augmentation Data Selection for Few-shot Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.356) |  | 0 | Data augmentation has been a popular method for fine-tuning pre-trained language models to increase model robustness and performance. With augmentation data coming from modifying gold train data (in-sample augmentation) or being harvested from general domain unlabeled data (out-of-sample... | Guangliang Liu, Jiayu Zhou, Lifeng Jin, Owen Yuan |  |
| 358 |  |  [KPDROP: Improving Absent Keyphrase Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.357) |  | 0 | Keyphrase generation is the task of generating phrases (keyphrases) that summarize the main topics of a given document. Keyphrases can be either present or absent from the given document. While the extraction of present keyphrases has received much attention in the past, only recently a stronger... | Cornelia Caragea, Jishnu Ray Chowdhury, Seoyeon Park, Tuhin Kundu |  |
| 359 |  |  [Natural Language Deduction through Search over Statement Compositions](https://doi.org/10.18653/v1/2022.findings-emnlp.358) |  | 0 | In settings from fact-checking to question answering, we frequently want to know whether a collection of evidence (premises) entails a hypothesis. Existing methods primarily focus on the end-to-end discriminative version of this task, but less work has treated the generative version in which a... | Greg Durrett, Kaj Bostrom, Swarat Chaudhuri, Zayne Sprague |  |
| 360 |  |  [EnDex: Evaluation of Dialogue Engagingness at Scale](https://doi.org/10.18653/v1/2022.findings-emnlp.359) |  | 0 | We propose EnDex, the first human-reaction based model to evaluate dialogue engagingness. EnDex is trained on 80k Reddit-based Engagement Dataset (RED) curated using a novel distant-supervision framework. Engagingness is a key measure that captures high-level quality of AI dialogue systems and... | Fabrice HarelCanada, Guangxuan Xu, Nanyun Peng, Nischal Reddy Chandra, Ruibo Liu |  |
| 361 |  |  [LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.360) |  | 0 | Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance... | Chengyu Dong, Dheeraj Mekala, Jingbo Shang |  |
| 362 |  |  [Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models](https://doi.org/10.18653/v1/2022.findings-emnlp.361) |  | 0 | Model compression by way of parameter pruning, quantization, or distillation has recently gained popularity as an approach for reducing the computational requirements of modern deep neural network models for NLP. Inspired by prior works suggesting a connection between simpler, more generalizable... | Clara Na, Emma Strubell, Sanket Vaibhav Mehta |  |
| 363 |  |  [Structural Contrastive Representation Learning for Zero-shot Multi-label Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.362) |  | 0 | Zero-shot multi-label text classification (ZMTC) is a fundamental task in natural language processing with applications in the cold start problem of recommendation systems. Ideally, one would learn an expressive representation of both input text and label features so that ZMTC is transformed into a... | Anshumali Shrivastava, Tharun Medini, Tianyi Zhang, Zhaozhuo Xu |  |
| 364 |  |  [Improving Generalization of Pre-trained Language Models via Stochastic Weight Averaging](https://doi.org/10.18653/v1/2022.findings-emnlp.363) |  | 0 | Knowledge Distillation (KD) is a commonly used technique for improving the generalization of compact Pre-trained Language Models (PLMs) on downstream tasks. However, such methods impose the additional burden of training a separate teacher model for every new dataset.Alternatively, one may directly... | Ahmad Rashid, Ali Ghodsi, Ivan Kobyzev, Mehdi Rezagholizadeh, Peng Lu, Philippe Langlais |  |
| 365 |  |  [Learn What Is Possible, Then Choose What Is Best: Disentangling One-To-Many Relations in Language Through Text-based Games](https://doi.org/10.18653/v1/2022.findings-emnlp.364) |  | 0 | Language models pre-trained on large self-supervised corpora, followed by task-specific fine-tuning has become the dominant paradigm in NLP. These pre-training datasets often have a one-to-many structure—e.g. in dialogue there are many valid responses for a given context. However, only some of... | Benjamin Towle, Ke Zhou |  |
| 366 |  |  [Structurally Diverse Sampling for Sample-Efficient Training and Comprehensive Evaluation](https://doi.org/10.18653/v1/2022.findings-emnlp.365) |  | 0 | A growing body of research has demonstrated the inability of NLP models to generalize compositionally and has tried to alleviate it through specialized architectures, training schemes, and data augmentation, among other approaches. In this work, we study a different approach: training on instances... | Matt Gardner, Sameer Singh, Shivanshu Gupta |  |
| 367 |  |  [Unsupervised Multi-Granularity Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.366) |  | 0 | Text summarization is a user-preference based task, i.e., for one document, users often have different priorities for the summary. As a key aspect of customization in summarization, granularity is used to measure the semantic coverage between the summary and source document. However, developing... | Chenguang Zhu, Jiawei Han, Michael Zeng, Ming Zhong, Suyu Ge, Xingxing Zhang, Yang Liu, Yichong Xu, Yizhu Jiao, Yuning Mao |  |
| 368 |  |  [HeLo: Learning-Free Lookahead Decoding for Conversation Infilling](https://doi.org/10.18653/v1/2022.findings-emnlp.367) |  | 0 | We propose Heuristic Guided Lookahead Decoding (HeLo), a novel decoding strategy for conversation infilling. Conversation infilling aims to generate a seamless bridge of utterances connecting a given pair of source and target utterances. HeLo does not require fine-tuning or extra models – only the... | Ivan Lee, Taylor BergKirkpatrick |  |
| 369 |  |  [Invernet: An Inversion Attack Framework to Infer Fine-Tuning Datasets through Word Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.368) |  | 0 | Word embedding aims to learn the dense representation of words and has become a regular input preparation in many NLP tasks. Due to the data and computation intensive nature of learning embeddings from scratch, a more affordable way is to borrow the pretrained embedding available in public and... | Bo Luo, Ishrak Hayet, Zijun Yao |  |
| 370 |  |  [LawngNLI: A Long-Premise Benchmark for In-Domain Generalization from Short to Long Contexts and for Implication-Based Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.369) |  | 0 | Natural language inference has trended toward studying contexts beyond the sentence level. An important application area is law: past cases often do not foretell how they apply to new situations and implications must be inferred. This paper introduces LawngNLI, constructed from U.S. legal opinions... | Dan Roth, William Bruno |  |
| 371 |  |  [Distillation-Resistant Watermarking for Model Protection in NLP](https://doi.org/10.18653/v1/2022.findings-emnlp.370) |  | 0 | How can we protect the intellectual property of trained NLP models? Modern NLP models are prone to stealing by querying and distilling from their publicly exposed APIs. However, existing protection methods such as watermarking only work for images but are not applicable to text. We propose... | Lei Li, Xuandong Zhao, YuXiang Wang |  |
| 372 |  |  [NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation](https://doi.org/10.18653/v1/2022.findings-emnlp.371) |  | 0 | While counterfactual data augmentation offers a promising step towards robust generalization in natural language processing, producing a set of counterfactuals that offer valuable inductive bias for models remains a challenge. Most existing approaches for producing counterfactuals, manual or... | Gadi Singer, Phillip Howard, Swabha Swayamdipta, Vasudev Lal, Yejin Choi |  |
| 373 |  |  [Don't Just Clean It, Proxy Clean It: Mitigating Bias by Proxy in Pre-Trained Models](https://doi.org/10.18653/v1/2022.findings-emnlp.372) |  | 0 | Transformer-based pre-trained models are known to encode societal biases not only in their contextual representations, but also in downstream predictions when fine-tuned on task-specific data.We present D-Bias, an approach that selectively eliminates stereotypical associations (e.g, co-occurrence... | Ari Kobren, Michael L. Wick, Qinlan Shen, Swetasudha Panda |  |
| 374 |  |  [The Undesirable Dependence on Frequency of Gender Bias Metrics Based on Word Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.373) |  | 0 | Numerous works use word embedding-based metrics to quantify societal biases and stereotypes in texts. Recent studies have found that word embeddings can capture semantic similarity but may be affected by word frequency. In this work we study the effect of frequency when measuring female vs. male... | Diego Fernández Slezak, Edgar Altszyler, Francisco Valentini, Germán Rosati |  |
| 375 |  |  [BioNLI: Generating a Biomedical NLI Dataset Using Lexico-semantic Constraints for Adversarial Examples](https://doi.org/10.18653/v1/2022.findings-emnlp.374) |  | 0 | Natural language inference (NLI) is critical in many domains requiring complex decision-making, such as the biomedical domain. We introduce a novel semi-supervised procedure that bootstraps biomedical NLI datasets from positive entailment examples present in abstracts of biomedical publications. We... | Mihai Surdeanu, Mohaddeseh Bastan, Niranjan Balasubramanian |  |
| 376 |  |  [Self-supervised Cross-modal Pretraining for Speech Emotion Recognition and Sentiment Analysis](https://doi.org/10.18653/v1/2022.findings-emnlp.375) |  | 0 | Multimodal speech emotion recognition (SER) and sentiment analysis (SA) are important techniques for human-computer interaction. Most existing multimodal approaches utilize either shallow cross-modal fusion of pretrained features, or deep cross-modal fusion with raw features. Recently, attempts... | IekHeng Chu, Jing Xiao, Mei Han, Peng Chang, Xinlu Yu, Ziyi Chen |  |
| 377 |  |  [Multimodal Conversation Modelling for Topic Derailment Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.376) |  | 0 | Conversations on social media tend to go off-topic and turn into different and sometimes toxic exchanges. Previous work focuses on analysing textual dialogues that have derailed into toxic content, but the range of derailment types is much broader, including spam or bot content, tangential... | Lucia Specia, Marek Rei, Zhenhao Li |  |
| 378 |  |  [Active Learning for Abstractive Text Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.377) |  | 0 | Construction of human-curated annotated datasets for abstractive text summarization (ATS) is very time-consuming and expensive because creating each instance requires a human annotator to read a long document and compose a shorter summary that would preserve the key information relayed by the... | Akim Tsvigun, Alexander Panchenko, Artem Shelmanov, Artemy Belousov, Danila Sedashov, Eldar Damirov, Ivan Lazichny, Ivan Lysenko, Leonid Sanochkin, Maxim Panov, Mikhail Burtsev, Vladimir Karlov |  |
| 379 |  |  [Finding Memo: Extractive Memorization in Constrained Sequence Generation Tasks](https://doi.org/10.18653/v1/2022.findings-emnlp.378) |  | 0 | Memorization presents a challenge for several constrained Natural Language Generation (NLG) tasks such as Neural Machine Translation (NMT), wherein the proclivity of neural models to memorize noisy and atypical samples reacts adversely with the noisy (web crawled) datasets. However, previous... | Arul Menezes, Vikas Raunak |  |
| 380 |  |  [SALTED: A Framework for SAlient Long-tail Translation Error Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.379) |  | 0 | Traditional machine translation (MT) metrics provide an average measure of translation quality that is insensitive to the long tail of behavioral problems. Examples include translation of numbers, physical units, dropped content and hallucinations. These errors, which occur rarely and unpredictably... | Arul Menezes, Matt Post, Vikas Raunak |  |
| 381 |  |  [Discord Questions: A Computational Approach To Diversity Analysis in News Coverage](https://doi.org/10.18653/v1/2022.findings-emnlp.380) |  | 0 | There are many potential benefits to news readers accessing diverse sources. Modern news aggregators do the hard work of organizing the news, offering readers a plethora of source options, but choosing which source to read remains challenging.We propose a new framework to assist readers in... | Caiming Xiong, ChienSheng Wu, Lidiya Murakhovs'ka, Philippe Laban, Xiang 'Anthony' Chen |  |
| 382 |  |  [FocusQA: Open-Domain Question Answering with a Context in Focus](https://doi.org/10.18653/v1/2022.findings-emnlp.381) |  | 0 | We introduce question answering with a cotext in focus, a task that simulates a free interaction with a QA system. The user reads on a screen some information about a topic, and they can follow-up with questions that can be either related or not to the topic; and the answer can be found in the... | Adrià de Gispert, Alessandro Moschitti, Bill Byrne, Gianni Barlacchi, Ivano Lauriola, Marco Del Tredici, Thuy Vu, Xiaoyu Shen |  |
| 383 |  |  [Challenges and Opportunities in Information Manipulation Detection: An Examination of Wartime Russian Media](https://doi.org/10.18653/v1/2022.findings-emnlp.382) |  | 0 | NLP research on public opinion manipulation campaigns has primarily focused on detecting overt strategies such as fake news and disinformation. However, information manipulation in the ongoing Russia-Ukraine war exemplifies how governments and media also employ more nuanced strategies. We release a... | Anjalie Field, Chan Young Park, Julia Mendelsohn, Yulia Tsvetkov |  |
| 384 |  |  [Disentangling Task Relations for Few-shot Text Classification via Self-Supervised Hierarchical Task Clustering](https://doi.org/10.18653/v1/2022.findings-emnlp.383) |  | 0 | Few-Shot Text Classification (FSTC) imitates humans to learn a new text classifier efficiently with only few examples, by leveraging prior knowledge from historical tasks. However, most prior works assume that all the tasks are sampled from a single data source, which cannot adapt to real-world... | Juan Zha, Ying Wei, Yu Zhang, Zheng Li |  |
| 385 |  |  [XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing](https://doi.org/10.18653/v1/2022.findings-emnlp.384) |  | 0 | In-context learning using large language models has recently shown surprising results for semantic parsing tasks such as Text-to-SQL translation.Prompting GPT-3 or Codex using several examples of question-SQL pairs can produce excellent results, comparable to state-of-the-art finetuning-based... | He Bai, Jimmy Lin, Peng Shi, Rui Zhang |  |
| 386 |  |  [Continuation KD: Improved Knowledge Distillation through the Lens of Continuation Optimization](https://doi.org/10.18653/v1/2022.findings-emnlp.385) |  | 0 | Knowledge Distillation (KD) has been extensively used for natural language understanding (NLU) tasks to improve a small model’s (a student) generalization by transferring the knowledge from a larger model (a teacher). Although KD methods achieve state-of-the-art performance in numerous settings,... | Ali Ghodsi, Aref Jafari, Ivan Kobyzev, Mehdi Rezagholizadeh, Pascal Poupart |  |
| 387 |  |  [Detecting Dementia from Long Neuropsychological Interviews](https://doi.org/10.18653/v1/2022.findings-emnlp.386) |  | 0 | Neuropsychological exams are commonly used to diagnose various kinds of cognitive impairment. They typically involve a trained examiner who conducts a series of cognitive tests with a subject. In recent years, there has been growing interest in developing machine learning methods to extract speech... | James R. Glass, Nauman Dawalatabad, Rhoda Au, Sameer Khurana, Yuan Gong |  |
| 388 |  |  [Sarcasm Detection is Way Too Easy! An Empirical Comparison of Human and Machine Sarcasm Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.387) |  | 0 | Recently, author-annotated sarcasm datasets, which focus on intended, rather than perceived sarcasm, have been introduced. Although datasets collected using first-party annotation have important benefits, there is no comparison of human and machine performance on these new datasets. In this paper,... | Ibrahim Abu Farha, Silviu Oprea, Steven R. Wilson, Walid Magdy |  |
| 389 |  |  [Cross-lingual Text-to-SQL Semantic Parsing with Representation Mixup](https://doi.org/10.18653/v1/2022.findings-emnlp.388) |  | 0 | We focus on the cross-lingual Text-to-SQL semantic parsing task,where the parsers are expected to generate SQL for non-English utterances based on English database schemas.Intuitively, English translation as side information is an effective way to bridge the language gap,but noise introduced by the... | Dong Yu, Haitao Mi, He Bai, Jimmy Lin, Lifeng Jin, Linfeng Song, Peng Shi |  |
| 390 |  |  [JamPatoisNLI: A Jamaican Patois Natural Language Inference Dataset](https://doi.org/10.18653/v1/2022.findings-emnlp.389) |  | 0 | JamPatoisNLI provides the first dataset for natural language inference in a creole language, Jamaican Patois.Many of the most-spoken low-resource languages are creoles. These languages commonly have a lexicon derived from a major world language and a distinctive grammar reflecting the languages of... | Christopher D. Manning, John Hewitt, RuthAnn Armstrong |  |
| 391 |  |  [Are Neural Topic Models Broken?](https://doi.org/10.18653/v1/2022.findings-emnlp.390) |  | 0 | Recently, the relationship between automated and human evaluation of topic models has been called into question. Method developers have staked the efficacy of new topic model variants on automated measures, and their failure to approximate human preferences places these models on uncertain ground.... | Alexander Miserlis Hoyle, Philip Resnik, Pranav Goel, Rupak Sarkar |  |
| 392 |  |  [Know Thy Strengths: Comprehensive Dialogue State Tracking Diagnostics](https://doi.org/10.18653/v1/2022.findings-emnlp.391) |  | 0 | Recent works that revealed the vulnerability of dialogue state tracking (DST) models to distributional shifts have made holistic comparisons on robustness and qualitative analyses increasingly important for understanding their relative performance. We present our findings from standardized and... | Ahmad Beirami, Asli Celikyilmaz, Chinnadhurai Sankar, Christopher Lin, Hyundong Cho, Jonathan May, Kaushik Ram Sadagopan, Shahin Shayandeh |  |
| 393 |  |  [Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge](https://doi.org/10.18653/v1/2022.findings-emnlp.392) |  | 0 | We propose a novel open-domain question answering (ODQA) framework for answering single/multi-hop questions across heterogeneous knowledge sources.The key novelty of our method is the introduction of the intermediary modules into the current retriever-reader pipeline.Unlike previous methods that... | Eric Nyberg, Hao Cheng, Jianfeng Gao, Kaixin Ma, Xiaodong Liu |  |
| 394 |  |  [Detecting Languages Unintelligible to Multilingual Models through Local Structure Probes](https://doi.org/10.18653/v1/2022.findings-emnlp.393) |  | 0 | Providing better language tools for low-resource and endangered languages is imperative for equitable growth.Recent progress with massively multilingual pretrained models has proven surprisingly effective at performing zero-shot transfer to a wide variety of languages.However, this transfer is not... | Amal Zouaq, Louis Clouâtre, Prasanna Parthasarathi, Sarath Chandar |  |
| 395 |  |  [Cards Against AI: Predicting Humor in a Fill-in-the-blank Party Game](https://doi.org/10.18653/v1/2022.findings-emnlp.394) |  | 0 | Humor is an inherently social phenomenon, with humorous utterances shaped by what is socially and culturally accepted. Understanding humor is an important NLP challenge, with many applications to human-computer interactions. In this work we explore humor in the context of Cards Against Humanity – a... | Dafna Shahaf, Dan Ofer |  |
| 396 |  |  [Open-Vocabulary Argument Role Prediction For Event Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.395) |  | 0 | The argument role in event extraction refers to the relation between an event and an argument participating in it. Despite the great progress in event extraction, existing studies still depend on roles pre-defined by domain experts. These studies expose obvious weakness when extending to emerging... | Heng Ji, Jiawei Han, Ming Zhong, Sha Li, Yiqing Xie, Yizhu Jiao |  |
| 397 |  |  [Token-level Sequence Labeling for Spoken Language Understanding using Compositional End-to-End Models](https://doi.org/10.18653/v1/2022.findings-emnlp.396) |  | 0 | End-to-end spoken language understanding (SLU) systems are gaining popularity over cascaded approaches due to their simplicity and ability to avoid error propagation. However, these systems model sequence labeling as a sequence prediction task causing a divergence from its well-established... | Alan W. Black, Brian Yan, Florian Metze, Shinji Watanabe, Siddhant Arora, Siddharth Dalmia |  |
| 398 |  |  [Baked-in State Probing](https://doi.org/10.18653/v1/2022.findings-emnlp.397) |  | 0 | Neural language models have been analyzed for their linguistic and extra-linguistic knowledge via probing. Of particular interest has been the following question: how much can a language model trained only on form learn about meaning? Recent work has demonstrated via probing classifiers that in the... | Karen Livescu, Kevin Gimpel, Sam Wiseman, Shubham Toshniwal |  |
| 399 |  |  [ClinicalT5: A Generative Language Model for Clinical Text](https://doi.org/10.18653/v1/2022.findings-emnlp.398) |  | 0 | In the past few years, large pre-trained language models (PLMs) have been widely adopted in different areas and have made fundamental improvements over a variety of downstream tasks in natural language processing (NLP). Meanwhile, domain-specific variants of PLMs are being proposed to address the... | Dejing Dou, Qiuhao Lu, Thien Huu Nguyen |  |
| 400 |  |  [Find Someone Who: Visual Commonsense Understanding in Human-Centric Grounding](https://doi.org/10.18653/v1/2022.findings-emnlp.399) |  | 0 | From a visual scene containing multiple people, human is able to distinguish each individual given the context descriptions about what happened before, their mental/physical states or intentions, etc. Above ability heavily relies on human-centric commonsense knowledge and reasoning. For example, if... | Haoxuan You, KaiWei Chang, Rui Sun, ShihFu Chang, Zhecan Wang |  |
| 401 |  |  [CrisisLTLSum: A Benchmark for Local Crisis Event Timeline Extraction and Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.400) |  | 0 | Social media has increasingly played a key role in emergency response: first responders can use public posts to better react to ongoing crisis events and deploy the necessary resources where they are most needed. Timeline extraction and abstractive summarization are critical technical tasks to... | Alejandro Jaimes, Bashar Alhafni, Hossein Rajaby Faghihi, Joel R. Tetreault, Ke Zhang, Shihao Ran |  |
| 402 |  |  [Prompt-Tuning Can Be Much Better Than Fine-Tuning on Cross-lingual Understanding With Multilingual Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.401) |  | 0 | Pre-trained multilingual language models show significant performance gains for zero-shot cross-lingual model transfer on a wide range of natural language understanding (NLU) tasks. Previously, for zero-shot cross-lingual evaluation, pre-trained models are only fine-tuned on English data and tested... | Caiming Xiong, Lifu Tu, Yingbo Zhou |  |
| 403 |  |  [BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model](https://doi.org/10.18653/v1/2022.findings-emnlp.402) |  | 0 | This paper presents BERT-CTC, a novel formulation of end-to-end speech recognition that adapts BERT for connectionist temporal classification (CTC). Our formulation relaxes the conditional independence assumptions used in conventional CTC and incorporates linguistic knowledge through the explicit... | Brian Yan, Shinji Watanabe, Siddhant Arora, Tetsuji Ogawa, Tetsunori Kobayashi, Yosuke Higuchi |  |
| 404 |  |  [EtriCA: Event-Triggered Context-Aware Story Generation Augmented by Cross Attention](https://doi.org/10.18653/v1/2022.findings-emnlp.403) |  | 0 | One of the key challenges of automatic story generation is how to generate a long narrative that can maintain fluency, relevance, and coherence. Despite recent progress, current story generation systems still face the challenge of how to effectively capture contextual and event features, which has... | Chen Tang, Chenghua Lin, Frank Guerin, Henglin Huang, Zhihao Zhang |  |
| 405 |  |  [LADIS: Language Disentanglement for 3D Shape Editing](https://doi.org/10.18653/v1/2022.findings-emnlp.404) |  | 0 | Natural language interaction is a promising direction for democratizing 3D shape design. However, existing methods for text-driven 3D shape editing face challenges in producing decoupled, local edits to 3D shapes. We address this problem by learning disentangled latent representations that ground... | Ian Huang, Leonidas J. Guibas, Minhyuk Sung, Panos Achlioptas, Sergey Tulyakov, Tianyi Zhang |  |
| 406 |  |  [Effective Pretraining Objectives for Transformer-based Autoencoders](https://doi.org/10.18653/v1/2022.findings-emnlp.405) |  | 0 | In this paper, we study trade-offs between efficiency, cost and accuracy when pre-training Transformer encoders with different pre-training objectives. For this purpose, we analyze features of common objectives and combine them to create new effective pre-training approaches. Specifically, we... | Alessandro Moschitti, Luca Di Liello, Matteo Gabburo |  |
| 407 |  |  [Language Model Detoxification in Dialogue with Contextualized Stance Control](https://doi.org/10.18653/v1/2022.findings-emnlp.406) |  | 0 | To reduce the toxic degeneration in a pretrained Language Model (LM), previous work on Language Model detoxification has focused on reducing the toxicity of the generation itself (self-toxicity) without consideration of the context. As a result, a type of implicit offensive language where the... | Jing Qian, Xifeng Yan |  |
| 408 |  |  [Multilingual SubEvent Relation Extraction: A Novel Dataset and Structure Induction Method](https://doi.org/10.18653/v1/2022.findings-emnlp.407) |  | 0 | Subevent Relation Extraction (SRE) is a task in Information Extraction that aims to recognize spatial and temporal containment relations between event mentions in text. Recent methods have utilized pre-trained language models to represent input texts for SRE. However, a key issue in existing SRE... | Franck Dernoncourt, Hieu Man, Linh Ngo Van, Thien Huu Nguyen, Viet Dac Lai |  |
| 409 |  |  [A Two-Stage Approach towards Generalization in Knowledge Base Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.408) |  | 0 | Most existing approaches for Knowledge Base Question Answering (KBQA) focus on a specific underlying knowledge base either because of inherent assumptions in the approach, or because evaluating it on a different knowledge base requires non-trivial changes. However, many popular knowledge bases... | Achille Fokoue, Dung Thai, Gaetano Rossiello, Ibrahim Abdelaziz, Nandana Mihindukulasooriya, Pavan Kapanipathi, Srinivas Ravishankar, Tahira Naseem |  |
| 410 |  |  [Few-Shot (Dis)Agreement Identification in Online Discussions with Regularized and Augmented Meta-Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.409) |  | 0 | Online discussions are abundant with opinions towards a common topic, and identifying (dis)agreement between a pair of comments enables many opinion mining applications. Realizing the increasing needs to analyze opinions for emergent new topics that however tend to lack annotations, we present the... | Ruihong Huang, Yuanyuan Lei |  |
| 411 |  |  [Data Cartography for Low-Resource Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.410) |  | 0 | While collecting or generating more parallel data is necessary to improve machine translation (MT) in low-resource settings, we lack an understanding of how the limited amounts of existing data are actually used to help guide the collection of further resources. In this paper, we apply data... | Aquia Richburg, Marine Carpuat |  |
| 412 |  |  [Augmenting Multi-Turn Text-to-SQL Datasets with Self-Play](https://doi.org/10.18653/v1/2022.findings-emnlp.411) |  | 0 | The task of context-dependent text-to-SQL aims to convert multi-turn user utterances to formal SQL queries. This is a challenging task due to both the scarcity of training data from which to learn complex contextual dependencies and to generalize to unseen databases. In this paper we explore... | Linfeng Song, Phil Blunsom, Qi Liu, Tao Yu, Zihuiwen Ye |  |
| 413 |  |  [Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.412) |  | 0 | We explore the idea of compressing the prompts used to condition language models, and show that compressed prompts can retain a substantive amount of information about the original prompt. For severely compressed prompts, while fine-grained information is lost, abstract information and general... | David Wingate, Mohammad Shoeybi, Taylor Sorensen |  |
| 414 |  |  [NaturalAdversaries: Can Naturalistic Adversaries Be as Effective as Artificial Adversaries?](https://doi.org/10.18653/v1/2022.findings-emnlp.413) |  | 0 | While a substantial body of prior work has explored adversarial example generation for natural language understanding tasks, these examples are often unrealistic and diverge from the real-world data distributions. In this work, we introduce a two-stage adversarial example generation framework... | Hamid Palangi, Saadia Gabriel, Yejin Choi |  |
| 415 |  |  [Multi-Path Transformer is Better: A Case Study on Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.414) |  | 0 | For years the model performance in machine learning obeyed a power-law relationship with the model size. For the consideration of parameter efficiency, recent studies focus on increasing model depth rather than width to achieve better performance. In this paper, we study how model width affects the... | Anxiang Ma, Jingbo Zhu, Shuhan Zhou, Tong Xiao, Yanyang Li, Ye Lin |  |
| 416 |  |  [Unsupervised Learning of Hierarchical Conversation Structure](https://doi.org/10.18653/v1/2022.findings-emnlp.415) |  | 0 | Human conversations can evolve in many different ways, creating challenges for automatic understanding and summarization. Goal-oriented conversations often have meaningful sub-dialogue structure, but it can be highly domain-dependent. This work introduces an unsupervised approach to learning... | BoRu Lu, Hao Cheng, Mari Ostendorf, Noah A. Smith, Yushi Hu |  |
| 417 |  |  [Task Compass: Scaling Multi-task Pre-training with Task Prefix](https://doi.org/10.18653/v1/2022.findings-emnlp.416) |  | 0 | Leveraging task-aware annotated data as supervised signals to assist with self-supervised learning on large-scale unlabeled data has become a new trend in pre-training language models. Existing studies show that multi-task learning with large-scale supervised tasks suffers from negative effects... | Chenguang Zhu, Hai Zhao, Michael Zeng, Shuohang Wang, Wenhao Yu, Yang Liu, Yichong Xu, Yuwei Fang, Zhuosheng Zhang |  |
| 418 |  |  [Sharpness-Aware Minimization with Dynamic Reweighting](https://doi.org/10.18653/v1/2022.findings-emnlp.417) |  | 0 | Deep neural networks are often overparameterized and may not easily achieve model generalization. Adversarial training has shown effectiveness in improving generalization by regularizing the change of loss on top of adversarially chosen perturbations. The recently proposed sharpness-aware... | Fangyu Liu, Huan Zhang, Muhao Chen, Wenxuan Zhou |  |
| 419 |  |  [Predicting Long-Term Citations from Short-Term Linguistic Influence](https://doi.org/10.18653/v1/2022.findings-emnlp.418) |  | 0 | A standard measure of the influence of a research paper is the number of times it is cited. However, papers may be cited for many reasons, and citation count is not informative about the extent to which a paper affected the content of subsequent publications. We therefore propose a novel method to... | David Bamman, Jacob Eisenstein, Sandeep Soni |  |
| 420 |  |  [Joint Audio/Text Training for Transformer Rescorer of Streaming Speech Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.419) |  | 0 | Recently, there has been an increasing interest in two-pass streaming end-to-end speech recognition (ASR) that incorporates a 2nd-pass rescoring model on top of the conventional 1st-pass streaming ASR model to improve recognition accuracy while keeping latency low. One of the latest 2nd-pass... | Duc Le, Jiedan Zhu, Ke Li, Lucas Kabela, Ozlem Kalinli, Ron Huang, Suyoun Kim |  |
| 421 |  |  [TyDiP: A Dataset for Politeness Classification in Nine Typologically Diverse Languages](https://doi.org/10.18653/v1/2022.findings-emnlp.420) |  | 0 | We study politeness phenomena in nine typologically diverse languages. Politeness is an important facet of communication and is sometimes argued to be cultural-specific, yet existing computational linguistic study is limited to English. We create TyDiP, a dataset containing three-way politeness... | Anirudh Srinivasan, Eunsol Choi |  |
| 422 |  |  [Probing Cross-modal Semantics Alignment Capability from the Textual Perspective](https://doi.org/10.18653/v1/2022.findings-emnlp.421) |  | 0 | In recent years, vision and language pre-training (VLP) models have advanced the state-of-the-art results in a variety of cross-modal downstream tasks. Aligning cross-modal semantics is claimed to be one of the essential capabilities of VLP models. However, it still remains unclear about the inner... | Jiajun Chen, Jianbing Zhang, Mianzhi Pan, Shi Zong, Shujian Huang, Xinyu Dai, Zheng Ma |  |
| 423 |  |  [Hidden State Variability of Pretrained Language Models Can Guide Computation Reduction for Transfer Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.422) |  | 0 | While transferring a pretrained language model, common approaches conventionally attach their task-specific classifiers to the top layer and adapt all the pretrained layers. We investigate whether one could make a task-specific selection on which subset of the layers to adapt and where to place the... | Ankita Pasad, Hongyuan Mei, Jiahao Qiu, Li Du, Qing Qu, Shuo Xie |  |
| 424 |  |  [Language Models as Agent Models](https://doi.org/10.18653/v1/2022.findings-emnlp.423) |  | 0 | Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in the outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them—a fact... | Jacob Andreas |  |
| 425 |  |  [Combinatory Grammar Tells Underlying Relevance among Entities](https://doi.org/10.18653/v1/2022.findings-emnlp.424) |  | 0 | Relation extraction (RE) is an important task in natural language processing which aims to annotate the relation between two given entities, which requires a deep understanding of the running text. To import model performance, existing approaches leverage syntactic information to facilitate the... | Yan Song, Yuanhe Tian |  |
| 426 |  |  [Leveraging Open Data and Task Augmentation to Automated Behavioral Coding of Psychotherapy Conversations in Low-Resource Scenarios](https://doi.org/10.18653/v1/2022.findings-emnlp.425) |  | 0 | In psychotherapy interactions, the quality of a session is assessed by codifying the communicative behaviors of participants during the conversation through manual observation and annotation. Developing computational approaches for automated behavioral coding can reduce the burden on human coders... | David C. Atkins, Nikolaos Flemotomos, Shrikanth Narayanan, Zac E. Imel, Zhuohao Chen |  |
| 427 |  |  [Learning to Detect Noisy Labels Using Model-Based Features](https://doi.org/10.18653/v1/2022.findings-emnlp.426) |  | 0 | Label noise is ubiquitous in various machine learning scenarios such as self-labeling with model predictions and erroneous data annotation. Many existing approaches are based on heuristics such as sample losses, which might not be flexible enough to achieve optimal solutions. Meta learning based... | Guidong Zheng, Junjie Wen, Peiqi Liu, Xianxin Chen, Yujun Chen, Zhihao Wang, Zhilin Yang, Zongyu Lin |  |
| 428 |  |  [Keyphrase Generation Beyond the Boundaries of Title and Abstract](https://doi.org/10.18653/v1/2022.findings-emnlp.427) |  | 0 | Keyphrase generation aims at generating important phrases (keyphrases) that best describe a given document. In scholarly domains, current approaches have largely used only the title and abstract of the articles to generate keyphrases. In this paper, we comprehensively explore whether the... | Cornelia Caragea, Jishnu Ray Chowdhury, Krishna Garg |  |
| 429 |  |  [Composition, Attention, or Both?](https://doi.org/10.18653/v1/2022.findings-emnlp.428) |  | 0 | In this paper, we propose a novel architecture called Composition Attention Grammars (CAGs) that recursively compose subtrees into a single vector representation with a composition function, and selectively attend to previous structural information with a self-attention mechanism. We investigate... | Ryo Yoshida, Yohei Oseki |  |
| 430 |  |  [CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model](https://doi.org/10.18653/v1/2022.findings-emnlp.429) |  | 0 | Manually designing cloze test consumes enormous time and efforts. The major challenge lies in wrong option (distractor) selection. Having carefully-design distractors improves the effectiveness of learner ability assessment. As a result, the idea of automatically generating cloze distractor is... | ShangHsuan Chiang, SsuCheng Wang, YaoChung Fan |  |
| 431 |  |  [G3: Geolocation via Guidebook Grounding](https://doi.org/10.18653/v1/2022.findings-emnlp.430) |  | 0 | We demonstrate how language can improve geolocation: the task of predicting the location where an image was taken. Here we study explicit knowledge from human-written guidebooks that describe the salient and class-discriminative visual features humans use for geolocation. We propose the task of... | Anna Rohrbach, Daniel Fried, Giscard Biamby, Grace Luo, Trevor Darrell |  |
| 432 |  |  [Controlling Bias Exposure for Fair Interpretable Predictions](https://doi.org/10.18653/v1/2022.findings-emnlp.431) |  | 0 | Recent work on reducing bias in NLP models usually focuses on protecting or isolating information related to a sensitive attribute (like gender or race). However, when sensitive information is semantically entangled with the task information of the input, e.g., gender information is predictive for... | Bodhisattwa Prasad Majumder, Julian J. McAuley, Yu Wang, Zexue He |  |
| 433 |  |  [Investigating the Benefits of Free-Form Rationales](https://doi.org/10.18653/v1/2022.findings-emnlp.432) |  | 0 | Free-form rationales aim to aid model interpretability by supplying the background knowledge that can help understand model decisions. Crowdsourced rationales are provided for commonsense QA instances in popular datasets such as CoS-E and ECQA, but their utility remains under-investigated. We... | Jiao Sun, Jonathan May, Swabha Swayamdipta, Xuezhe Ma |  |
| 434 |  |  [Data-Efficient Concept Extraction from Pre-trained Language Models for Commonsense Explanation Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.433) |  | 0 | Predicting the key explanation concept is essential for generating commonsense explanations. This paper introduces a method to predict the concept from pre-trained language models for commonsense explanation generation. Our experiment found that adopting a language model as the concept extractor... | Yanbo Fang, Yongfeng Zhang |  |
| 435 |  |  [Unsupervised Domain Adaptation for Joint Information Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.434) |  | 0 | Joint Information Extraction (JIE) aims to jointly solve multiple tasks in the Information Extraction pipeline (e.g., entity mention, event trigger, relation, and event argument extraction). Due to their ability to leverage task dependencies and avoid error propagation, JIE models have presented... | Bonan Min, Nghia Trung Ngo, Thien Huu Nguyen |  |
| 436 |  |  [Foiling Training-Time Attacks on Neural Machine Translation Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.435) |  | 0 | Neural machine translation (NMT) systems are vulnerable to backdoor attacks, whereby an attacker injects poisoned samples into training such that a trained model produces malicious translations. Nevertheless, there is little research on defending against such backdoor attacks in NMT. In this paper,... | Benjamin I. P. Rubinstein, Jun Wang, Trevor Cohn, Xuanli He |  |
| 437 |  |  [Learning Action-Effect Dynamics for Hypothetical Vision-Language Reasoning Task](https://doi.org/10.18653/v1/2022.findings-emnlp.436) |  | 0 | ‘Actions’ play a vital role in how humans interact with the world. Thus, autonomous agents that would assist us in everyday tasks also require the capability to perform ‘Reasoning about Actions & Change’ (RAC). This has been an important research direction in Artificial Intelligence (AI) in... | Chitta Baral, Pratyay Banerjee, Shailaja Keyur Sampat, Yezhou Yang |  |
| 438 |  |  [Named Entity and Relation Extraction with Multi-Modal Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.437) |  | 0 | Multi-modal named entity recognition (NER) and relation extraction (RE) aim to leverage relevant image information to improve the performance of NER and RE. Most existing efforts largely focused on directly extracting potentially useful information from images (such as pixel-level features,... | Jiong Cai, Kewei Tu, Pengjun Xie, Wei Lu, Xinyu Wang, Yong Jiang |  |
| 439 |  |  [Calibrating Factual Knowledge in Pretrained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.438) |  | 0 | Previous literature has proved that Pretrained Language Models (PLMs) can store factual knowledge. However, we find that facts stored in the PLMs are not always correct. It motivates us to explore a fundamental question: How do we calibrate factual knowledge in PLMs without re-training from... | Damai Dai, Jingjing Xu, Lei Li, Qingxiu Dong, Yifan Song, Zhifang Sui |  |
| 440 |  |  [MCPG: A Flexible Multi-Level Controllable Framework for Unsupervised Paraphrase Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.439) |  | 0 | We present MCPG: a simple and effectiveapproach for controllable unsupervised paraphrase generation, which is also flexible toadapt to specific domains without extra training. MCPG is controllable in different levels: local lexicons, global semantics, and universal styles. The unsupervised paradigm... | Haiyun Jiang, Lemao Liu, Rui Wang, Ruifeng Xu, Shuming Shi, Yi Chen |  |
| 441 |  |  [WordTies: Measuring Word Associations in Language Models via Constrained Sampling](https://doi.org/10.18653/v1/2022.findings-emnlp.440) |  | 0 | Word associations are widely used in psychology to provide insights on how humans perceive and understand concepts. Comparing word associations in language models (LMs) to those generated by human subjects can serve as a proxy to uncover embedded lexical and commonsense knowledge in language... | Denilson Barbosa, Peiran Yao, Tobias Renwick |  |
| 442 |  |  [Exploring The Landscape of Distributional Robustness for Question Answering Models](https://doi.org/10.18653/v1/2022.findings-emnlp.441) |  | 0 | We conduct a large empirical evaluation to investigate the landscape of distributional robustness in question answering. Our investigation spans over 350 models and 16 question answering datasets, including a diverse set of architectures, model sizes, and adaptation methods (e.g., fine-tuning,... | Anas Awadalla, Gabriel Ilharco, Hannaneh Hajishirzi, Ian Magnusson, Ludwig Schmidt, Mitchell Wortsman, Sewon Min |  |
| 443 |  |  [Collaborative Reasoning on Multi-Modal Semantic Graphs for Video-Grounded Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.442) |  | 0 | We study video-grounded dialogue generation, where a response is generated based on the dialogue context and the associated video. The primary challenges of this task lie in (1) the difficulty of integrating video data into pre-trained language models (PLMs) which presents obstacles to exploiting... | Chenshuo Wang, Chongyang Tao, Dongyan Zhao, Xueliang Zhao, Yuxuan Wang |  |
| 444 |  |  [Partitioned Gradient Matching-based Data Subset Selection for Compute-Efficient Robust ASR Training](https://doi.org/10.18653/v1/2022.findings-emnlp.443) |  | 0 | Training state-of-the-art ASR systems such as RNN-T often has a high associated financial and environmental cost. Training with a subset of training data could mitigate this problem if the subset selected could achieve on-par performance with training with the entire dataset. Although there are... | Ashish R. Mittal, Durga Sivasubramanian, Ganesh Ramakrishnan, Preethi Jyothi, Rishabh K. Iyer |  |
| 445 |  |  [Adaptive Graph Convolutional Network for Knowledge Graph Entity Alignment](https://doi.org/10.18653/v1/2022.findings-emnlp.444) |  | 0 | Entity alignment (EA) aims to identify equivalent entities from different Knowledge Graphs (KGs), which is a fundamental task for integrating KGs. Throughout its development, Graph Convolutional Network (GCN) has become one of the mainstream methods for EA. These GCN-based methods learn the... | Meng Ma, Ping Wang, Renbo Zhu, Xukun Luo |  |
| 446 |  |  [Towards Robust NLG Bias Evaluation with Syntactically-diverse Prompts](https://doi.org/10.18653/v1/2022.findings-emnlp.445) |  | 0 | We present a robust methodology for evaluating biases in natural language generation(NLG) systems. Previous works use fixed hand-crafted prefix templates with mentions of various demographic groups to prompt models to generate continuations for bias analysis. These fixed prefix templates could... | Arshiya Aggarwal, Jiao Sun, Nanyun Peng |  |
| 447 |  |  [PcMSP: A Dataset for Scientific Action Graphs Extraction from Polycrystalline Materials Synthesis Procedure Text](https://doi.org/10.18653/v1/2022.findings-emnlp.446) |  | 0 | Scientific action graphs extraction from materials synthesis procedures is important for reproducible research, machine automation, and material prediction. But the lack of annotated data has hindered progress in this field. We demonstrate an effort to annotate Polycrystalline Materials Synthesis... | Julia Zuo, Linda R. Petzold, Stephen D. Wilson, Xianjun Yang, Xinlu Zhang, Ya Zhuo |  |
| 448 |  |  [Validity Assessment of Legal Will Statements as Natural Language Inference](https://doi.org/10.18653/v1/2022.findings-emnlp.447) |  | 0 | This work introduces a natural language inference (NLI) dataset that focuses on the validity of statements in legal wills. This dataset is unique because: (a) each entailment decision requires three inputs: the statement from the will, the law, and the conditions that hold at the time of the... | Alice Saebom Kwak, Clayton T. Morrison, Derek E. Bambauer, Jacob O. Israelsen, Mihai Surdeanu |  |
| 449 |  |  [AdaPrompt: Adaptive Model Training for Prompt-based NLP](https://doi.org/10.18653/v1/2022.findings-emnlp.448) |  | 0 | Prompt-based learning, with its capability to tackle zero-shot and few-shot NLP tasks, has gained much attention in the community.The main idea is to bridge the gap between NLP downstream tasks and language modeling (LM), by mapping these tasks into natural language prompts, which are then filled... | Chenguang Zhu, Li Dong, Michael Zeng, Shuohang Wang, Yang Liu, Yue Zhang, Yulong Chen |  |
| 450 |  |  [Code Generation From Flowcharts with Texts: A Benchmark Dataset and An Approach](https://doi.org/10.18653/v1/2022.findings-emnlp.449) |  | 0 | Currently, researchers focus on generating codes from the requirement documents. However, current approaches still perform poorly on some requirements needing complex problem-solving skills. In reality, to tackle such complex requirements, instead of directly translating requirement documents into... | Deyu Zhou, Lin Li, Xiaoyu Hu, Xu Zhang, Yanzheng Xiang, Zejie Liu |  |
| 451 |  |  [Focus! Relevant and Sufficient Context Selection for News Image Captioning](https://doi.org/10.18653/v1/2022.findings-emnlp.450) |  | 0 | News Image Captioning requires describing an image by leveraging additional context derived from a news article. Previous works only coarsely leverage the article to extract the necessary context, which makes it challenging for models to identify relevant events and named entities. In our paper, we... | Anna Rohrbach, Grace Luo, Mingyang Zhou, Zhou Yu |  |
| 452 |  |  [Generative Aspect-Based Sentiment Analysis with Contrastive Learning and Expressive Structure](https://doi.org/10.18653/v1/2022.findings-emnlp.451) |  | 0 | Generative models have demonstrated impressive results on Aspect-based Sentiment Analysis (ABSA) tasks, particularly for the emerging task of extracting Aspect-Category-Opinion-Sentiment (ACOS) quadruples. However, these models struggle with implicit sentiment expressions, which are commonly... | Joseph Peper, Lu Wang |  |
| 453 |  |  [Semantic Dependency Parsing with Edge GNNs](https://doi.org/10.18653/v1/2022.findings-emnlp.452) |  | 0 | Second-order neural parsers have obtained high accuracy in semantic dependency parsing. Inspired by the factor graph representation of second-order parsing, we propose edge graph neural networks (E-GNNs). In an E-GNN, each node corresponds to a dependency edge, and the neighbors are defined in... | Kewei Tu, Songlin Yang |  |
| 454 |  |  [Explore Unsupervised Structures in Pretrained Models for Relation Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.453) |  | 0 | Syntactic trees have been widely applied in relation extraction (RE). However, since parsing qualities are not stable on different text domains and a pre-defined grammar may not well fit the target relation schema, the introduction of syntactic structures sometimes fails to improve RE performances... | Tao Ji, Xi Yang, Yuanbin Wu |  |
| 455 |  |  [Identifying Human Strategies for Generating Word-Level Adversarial Examples](https://doi.org/10.18653/v1/2022.findings-emnlp.454) |  | 0 | Adversarial examples in NLP are receiving increasing research attention. One line of investigation is the generation of word-level adversarial examples against fine-tuned Transformer models that preserve naturalness and grammaticality. Previous work found that human- and machine-generated... | Bennett Kleinberg, Lewis D. Griffin, Maximilian Mozes |  |
| 456 |  |  [Refinement Matters: Textual Description Needs to be Refined for Zero-shot Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.455) |  | 0 | Zero-Shot Learning (ZSL) has shown great promise at the intersection of vision and language, and generative methods for ZSL are predominant owing to their efficiency. Moreover, textual description or attribute plays a critical role in transferring knowledge from the seen to unseen classes in ZSL.... | Chandan Gautam, Savitha Ramasamy, Sethupathy Parameswaran, Suresh Sundaram, Vinay Verma |  |
| 457 |  |  [SAT: Improving Semi-Supervised Text Classification with Simple Instance-Adaptive Self-Training](https://doi.org/10.18653/v1/2022.findings-emnlp.456) |  | 0 | Self-training methods have been explored in recent years and have exhibited great performance in improving semi-supervised learning. This work presents a simple instance-adaptive self-training method (SAT) for semi-supervised text classification. SAT first generates two augmented views for each... | Hui Chen, Soujanya Poria, Wei Han |  |
| 458 |  |  [Answer Quality Aware Aggregation for Extractive QA Crowdsourcing](https://doi.org/10.18653/v1/2022.findings-emnlp.457) |  | 0 | Quality control is essential for creating extractive question answering (EQA) datasets via crowdsourcing. Aggregation across answers, i.e. word spans within passages annotated, by different crowd workers is one major focus for ensuring its quality. However, crowd workers cannot reach a consensus on... | Avishek Anand, Claudia Hauff, Jie Yang, Peide Zhu, Zhen Wang |  |
| 459 |  |  [Search to Pass Messages for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.findings-emnlp.458) |  | 0 | Completing missing facts is a fundamental task for temporal knowledge graphs (TKGs).Recently, graph neural network (GNN) based methods, which can simultaneously explore topological and temporal information, have become the state-of-the-art (SOTA) to complete TKGs. However, these studies are based... | Haotong Du, Quanming Yao, Xuelong Li, Zhen Wang |  |
| 460 |  |  [Code Vulnerability Detection via Nearest Neighbor Mechanism](https://doi.org/10.18653/v1/2022.findings-emnlp.459) |  | 0 | Code vulnerability detection is a fundamental and challenging task in the software security field. Existing research works aim to learn semantic information from the source code by utilizing NLP technologies. However, in vulnerability detection tasks, some vulnerable samples are very similar to... | Gang Zhao, Qianjin Du, Xiaohui Kuang |  |
| 461 |  |  [Robust Question Answering against Distribution Shifts with Test-Time Adaption: An Empirical Study](https://doi.org/10.18653/v1/2022.findings-emnlp.460) |  | 0 | A deployed question answering (QA) model can easily fail when the test data has a distribution shift compared to the training data. Robustness tuning (RT) methods have been widely studied to enhance model robustness against distribution shifts before model deployment. However, can we improve a... | Hai Ye, Hwee Tou Ng, Juntao Li, Yuyang Ding |  |
| 462 |  |  [ParaMac: A General Unsupervised Paraphrase Generation Framework Leveraging Semantic Constraints and Diversifying Mechanisms](https://doi.org/10.18653/v1/2022.findings-emnlp.461) |  | 0 | Paraphrase generation reflects the ability to understand the meaning from the language surface form and rephrase it to other expressions. Recent paraphrase generation works have paid attention to unsupervised approaches based on Pre-trained Language Models (PLMs) to avoid heavy reliance on parallel... | Ji Qi, Jiaxin Shi, Jinxin Liu, Juanzi Li, Lei Hou, Qi Tian |  |
| 463 |  |  [Semi-supervised New Slot Discovery with Incremental Clustering](https://doi.org/10.18653/v1/2022.findings-emnlp.462) |  | 0 | Discovering new slots is critical to the success of dialogue systems. Most existing methods rely on automatic slot induction in unsupervised fashion or perform domain adaptation across zero or few-shot scenarios. They have difficulties in providing high-quality supervised signals to learn... | Lizi Liao, TatSeng Chua, Xueming Qian, Yuxia Wu |  |
| 464 |  |  [Con-NAT: Contrastive Non-autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.463) |  | 0 | Inspired by the success of contrastive learning in natural language processing, we incorporate contrastive learning into the conditional masked language model which is extensively used in non-autoregressive neural machine translation (NAT). Accordingly, we propose a Contrastive Non-autoregressive... | Hao Cheng, Zhihua Zhang |  |
| 465 |  |  [Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection](https://doi.org/10.18653/v1/2022.findings-emnlp.464) |  | 0 | Knowledge distillation addresses the problem of transferring knowledge from a teacher model to a student model.In this process, we typically have multiple types of knowledge extracted from the teacher model.The problem is to make full use of them to train the student model.Our preliminary study... | Chenglong Wang, Jingbo Zhu, Tong Xiao, Yi Lu, Yimin Hu, Yongyu Mu |  |
| 466 |  |  [Syntactically Robust Training on Partially-Observed Data for Open Information Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.465) |  | 0 | Open Information Extraction models have shown promising results with sufficient supervision. However, these models face a fundamental challenge that the syntactic distribution of training data is partially observable in comparison to the real world. In this paper, we propose a syntactically robust... | Bin Xu, Ji Qi, Juanzi Li, Lei Hou, Yuxiang Chen |  |
| 467 |  |  [A Benchmark and Dataset for Post-OCR text correction in Sanskrit](https://doi.org/10.18653/v1/2022.findings-emnlp.466) |  | 0 | Sanskrit is a classical language with about 30 million extant manuscripts fit for digitisation, available in written, printed or scanned-image forms. However, it is still considered to be a low-resource language when it comes to available digital resources. In this work, we release a post-OCR text... | Amrith Krishna, Ayush Maheshwari, Ganesh Ramakrishnan, Nikhil Singh |  |
| 468 |  |  [Knowledge-Enhanced Self-Supervised Prototypical Network for Few-Shot Event Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.467) |  | 0 | Prototypical network based joint methods have attracted much attention in few-shot event detection, which carry out event detection in a unified sequence tagging framework. However, these methods suffer from the inaccurate prototype representation problem, due to two main reasons: the number of... | Jiafeng Guo, Kailin Zhao, Long Bai, Xiaolong Jin, Xueqi Cheng |  |
| 469 |  |  [VarMAE: Pre-training of Variational Masked Autoencoder for Domain-adaptive Language Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.468) |  | 0 | Pre-trained language models have been widely applied to standard benchmarks. Due to the flexibility of natural language, the available resources in a certain domain can be restricted to support obtaining precise representation. To address this issue, we propose a novel Transformer-based language... | Dou Hu, Lianxin Jiang, Mengyuan Zhou, Xiaofeng Shi, Xiaolong Hou, Xiyang Du, Yang Mo |  |
| 470 |  |  [Exploring Methods for Building Dialects-Mandarin Code-Mixing Corpora: A Case Study in Taiwanese Hokkien](https://doi.org/10.18653/v1/2022.findings-emnlp.469) |  | 0 | In natural language processing (NLP), code-mixing (CM) is a challenging task, especially when the mixed languages include dialects. In Southeast Asian countries such as Singapore, Indonesia, and Malaysia, Hokkien-Mandarin is the most widespread code-mixed language pair among Chinese immigrants, and... | BoHan Lu, ChaoYi Lu, Richard TzongHan Tsai, SinEn Lu |  |
| 471 |  |  [Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in Transformer-Based Variational AutoEncoder for Diverse Text Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.470) |  | 0 | Variational Auto-Encoder (VAE) has been widely adopted in text generation. Among many variants, recurrent VAE learns token-wise latent variables with each conditioned on the preceding ones, which captures sequential variability better in the era of RNN. However, it is unclear how to incorporate... | Jinyi Hu, Maosong Sun, Wenhao Li, Xiaoyuan Yi, Xing Xie |  |
| 472 |  |  [Tweet Based Reach Aware Temporal Attention Network for NFT Valuation](https://doi.org/10.18653/v1/2022.findings-emnlp.471) |  | 0 | Non-Fungible Tokens (NFTs) are a relatively unexplored class of assets. Designing strategies to forecast NFT trends is an intricate task due to its extremely volatile nature. The market is largely driven by public sentiment and “hype”, which in turn has a high correlation with conversations taking... | Atula Tejaswi Neerkaje, Dipanwita Guhathakurta, Megh Thakkar, Ramit Sawhney, Ritesh Soun, Sudheer Chava, Vasu Sharma |  |
| 473 |  |  [Entity Embedding Completion for Wide-Coverage Entity Disambiguation](https://doi.org/10.18653/v1/2022.findings-emnlp.472) |  | 0 | Entity disambiguation (ED) is typically solved by learning to classify a given mention into one of the entities in the model’s entity vocabulary by referring to their embeddings. However, this approach cannot address mentions of entities that are not covered by the entity vocabulary. Aiming to... | Daisuke Oba, Ikuya Yamada, Masashi Toyoda, Naoki Yoshinaga |  |
| 474 |  |  [Entity-level Interaction via Heterogeneous Graph for Multimodal Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.473) |  | 0 | Multimodal Named Entity Recognition (MNER) faces two specific challenges: 1) How to capture useful entity-related visual information. 2) How to alleviate the interference of visual noise. Previous works have gained progress by improving interacting mechanisms or seeking for better visual features.... | Gang Zhao, Guanting Dong, Haolong Yan, Si Li, Weiran Xu, Yidong Shi |  |
| 475 |  |  [Status Biases in Deliberation Online: Evidence from a Randomized Experiment on ChangeMyView](https://doi.org/10.18653/v1/2022.findings-emnlp.474) |  | 0 | Status is widely used to incentivize user engagement online. However, visible status indicators could inadvertently bias online deliberation to favor high-status users. In this work, we design and deploy a randomized experiment on the ChangeMyView platform to quantify status biases in deliberation... | Alan Montgomery, Emaad Manzoor, Yohan Jo |  |
| 476 |  |  [Empathetic and Emotionally Positive Conversation Systems with an Emotion-specific Query-Response Memory](https://doi.org/10.18653/v1/2022.findings-emnlp.475) |  | 0 | Emotional conversation systems generate responses for the input queries considering the speaker’s emotions in a conversation. Existing emotional conversation systems output emotional responses according to either a given emotion or the user’s emotion reflected in the input queries. Following a... | Chi Zhang, Dongkyu Lee, Dongsheng Li, Nevin L. Zhang, Yingxiu Zhao, Yinliang Wang, Yiping Song, Zhiliang Tian |  |
| 477 |  |  [Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision](https://doi.org/10.18653/v1/2022.findings-emnlp.476) |  | 0 | Clinical trials are essential for drug development but are extremely expensive and time-consuming to conduct. It is beneficial to study similar historical trials when designing a clinical trial. However, lengthy trial documents and lack of labeled data make trial similarity search difficult. We... | Jimeng Sun, Zifeng Wang |  |
| 478 |  |  [From Mimicking to Integrating: Knowledge Integration for Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.477) |  | 0 | Investigating better ways to reuse the released pre-trained language models (PLMs) can significantly reduce the computational cost and the potential environmental side-effects. This paper explores a novel PLM reuse paradigm, Knowledge Integration (KI). Without human annotations available, KI aims... | Guangxiang Zhao, Jie Zhou, Lei Li, Peng Li, Xu Sun, Xuancheng Ren, Yankai Lin |  |
| 479 |  |  [Model and Data Transfer for Cross-Lingual Sequence Labelling in Zero-Resource Settings](https://doi.org/10.18653/v1/2022.findings-emnlp.478) |  | 0 | Zero-resource cross-lingual transfer approaches aim to apply supervised modelsfrom a source language to unlabelled target languages. In this paper we performan in-depth study of the two main techniques employed so far for cross-lingualzero-resource sequence labelling, based either on data or model... | German Rigau, Iker GarcíaFerrero, Rodrigo Agerri |  |
| 480 |  |  [Early Guessing for Dialect Identification](https://doi.org/10.18653/v1/2022.findings-emnlp.479) |  | 0 | This paper deals with the problem of incre-mental dialect identification. Our goal is toreliably determine the dialect before the fullutterance is given as input. The major partof the previous research on dialect identification has been model-centric, focusing on performance. We address a new... | Fabio Rinaldi, Ljiljana Dolamic, Tanja Samardzic, Vani Kanjirangat |  |
| 481 |  |  [R-AT: Regularized Adversarial Training for Natural Language Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.480) |  | 0 | Currently, adversarial training has become a popular and powerful regularization method in the natural language domain. In this paper, we Regularized Adversarial Training (R-AT) via dropout, which forces the output probability distributions of different sub-models generated by dropout to be... | HungYu Kao, Jiawen Li, Shiwen Ni |  |
| 482 |  |  [Multi-View Active Learning for Short Text Classification in User-Generated Data](https://doi.org/10.18653/v1/2022.findings-emnlp.481) |  | 0 | Mining user-generated data often suffers from the lack of enough labeled data, short document lengths, and the informal user language. In this paper, we propose a novel active learning model to overcome these obstacles in the tasks tailored for query phrases–e.g., detecting positive reports of... | Li Xiong, Negin Karisani, Payam Karisani |  |
| 483 |  |  [Forging Multiple Training Objectives for Pre-trained Language Models via Meta-Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.482) |  | 0 | Multiple pre-training objectives fill the vacancy of the understanding capability of single-objective language modeling, which serves the ultimate purpose of pre-trained language models (PrLMs), generalizing well on a mass of scenarios. However, learning multiple training objectives in a single... | Boli Chen, Fei Huang, Hai Zhao, Hongqiu Wu, Min Zhang, Pengjun Xie, Ruixue Ding |  |
| 484 |  |  [ConGen: Unsupervised Control and Generalization Distillation For Sentence Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.483) |  | 0 | Sentence representations are essential in many NLP tasks operating at the sentence level.Recently, research attention has shifted towards learning how to represent sentences without any annotations, i.e., unsupervised representation learning. Despite the benefit of training without supervised data,... | Ekapol Chuangsuwanich, Lalita Lowphansirikul, Peerat Limkonchotiwat, Sarana Nutanong, Wuttikorn Ponwitayarat |  |
| 485 |  |  [Large-Scale Differentially Private BERT](https://doi.org/10.18653/v1/2022.findings-emnlp.484) |  | 0 | In this work, we study the large-scale pretraining of BERT-Large (Devlin et al., 2019) with differentially private SGD (DP-SGD). We show that combined with a careful implementation, scaling up the batch size to millions (i.e., mega-batches) improves the utility of the DP-SGD step for BERT; we also... | Badih Ghazi, Pasin Manurangsi, Ravi Kumar, Rohan Anil, Vineet Gupta |  |
| 486 |  |  [Improving Zero-Shot Multilingual Translation with Universal Representations and Cross-Mapping](https://doi.org/10.18653/v1/2022.findings-emnlp.485) |  | 0 | The many-to-many multilingual neural machine translation can translate between language pairs unseen during training, i.e., zero-shot translation. Improving zero-shot translation requires the model to learn universal representations and cross-mapping relationships to transfer the knowledge learned... | Shuhao Gu, Yang Feng |  |
| 487 |  |  [Controllable Fake Document Infilling for Cyber Deception](https://doi.org/10.18653/v1/2022.findings-emnlp.486) |  | 0 | Recent works in cyber deception study how to deter malicious intrusion by generating multiple fake versions of a critical document to impose costs on adversaries who need to identify the correct information. However, existing approaches are context-agnostic, resulting in sub-optimal and unvaried... | Erick Skorupa Parolin, Kevin W. Hamlen, Latifur Khan, Yibo Hu, Yu Lin |  |
| 488 |  |  [Weakly Supervised Headline Dependency Parsing](https://doi.org/10.18653/v1/2022.findings-emnlp.487) |  | 0 | English news headlines form a register with unique syntactic properties that have been documented in linguistics literature since the 1930s. However, headlines have received surprisingly little attention from the NLP syntactic parsing community. We aim to bridge this gap by providing the first news... | Adrian Benton, Igor Malioutov, Ozan Irsoy, Tianze Shi |  |
| 489 |  |  [BOOKSUM: A Collection of Datasets for Long-form Narrative Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.488) |  | 0 | The majority of existing text summarization datasets include short-form source documents that lack long-range causal and temporal dependencies, and often contain strong layout and stylistic biases. While relevant, such datasets will offer limited challenges for future text summarization systems. We... | Caiming Xiong, Divyansh Agarwal, Dragomir Radev, Nazneen Rajani, Wojciech Kryscinski |  |
| 490 |  |  [Not All Errors are Equal: Learning Text Generation Metrics using Stratified Error Synthesis](https://doi.org/10.18653/v1/2022.findings-emnlp.489) |  | 0 | Is it possible to build a general and automatic natural language generation (NLG) evaluation metric? Existing learned metrics either perform unsatisfactorily or are restricted to tasks where large human rating data is already available. We introduce SESCORE, a model-based metric that is highly... | Lei Li, Michael Saxon, Wenda Xu, William Yang Wang, YiLin Tuan, Yujie Lu |  |
| 491 |  |  [Summarization as Indirect Supervision for Relation Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.490) |  | 0 | Relation extraction (RE) models have been challenged by their reliance on training data with expensive annotations. Considering that summarization tasks aim at acquiring concise expressions of synoptical information from the longer context, these tasks naturally align with the objective of RE,... | IHung Hsu, Keming Lu, Mingyu Derek Ma, Muhao Chen, Wenxuan Zhou |  |
| 492 |  |  [DIGAT: Modeling News Recommendation with Dual-Graph Interaction](https://doi.org/10.18653/v1/2022.findings-emnlp.491) |  | 0 | News recommendation (NR) is essential for online news services. Existing NR methods typically adopt a news-user representation learning framework, facing two potential limitations. First, in news encoder, single candidate news encoding suffers from an insufficient semantic information problem.... | Hongru Wang, Jian Li, KamFai Wong, Xingshan Zeng, Zhiming Mao |  |
| 493 |  |  [SMASH: Improving SMAll Language Models' Few-SHot Ability with Prompt-Based Distillation](https://doi.org/10.18653/v1/2022.findings-emnlp.492) |  | 0 | Large-scale language models coupled with prompts have shown remarkable performance on few-shot learning. However, through systematic experiments, we find that the few-shot performance of small language models is poor, and using prompts on them brings fewer improvements than on larger ones. In this... | Chang Liu, Dongyan Zhao, Kai Chen, Xi Wang, Yueqian Wang |  |
| 494 |  |  [Consecutive Question Generation via Dynamic Multitask Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.493) |  | 0 | In this paper, we propose the task of consecutive question generation (CQG), which generates a set of logically related question-answer pairs to understand a whole passage, with a comprehensive consideration of the aspects including accuracy, coverage, and informativeness.To achieve this, we first... | Sujian Li, Xing Shi, Yunji Li |  |
| 495 |  |  [Subword Segmental Language Modelling for Nguni Languages](https://doi.org/10.18653/v1/2022.findings-emnlp.494) |  | 0 | Subwords have become the standard units of text in NLP, enabling efficient open-vocabulary models. With algorithms like byte-pair encoding (BPE), subword segmentation is viewed as a preprocessing step applied to the corpus before training. This can lead to sub-optimal segmentations for low-resource... | Francois Meyer, Jan Buys |  |
| 496 |  |  [Towards Robust Visual Question Answering: Making the Most of Biased Samples via Contrastive Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.495) |  | 0 | Models for Visual Question Answering (VQA) often rely on the spurious correlations, i.e., the language priors, that appear in the biased samples of training set, which make them brittle against the out-of-distribution (OOD) test data. Recent methods have achieved promising progress in overcoming... | Fandong Meng, Jie Zhou, Peng Fu, Qingyi Si, Weiping Wang, Yanan Cao, Yuanxin Liu, Zheng Lin |  |
| 497 |  |  [P3LM: Probabilistically Permuted Prophet Language Modeling for Generative Pre-Training](https://doi.org/10.18653/v1/2022.findings-emnlp.496) |  | 0 | Conventional autoregressive left-to-right (L2R) sequence generation faces two issues during decoding: limited to unidirectional target sequence modeling, and constrained on strong local dependencies.To address the aforementioned problem, we propose P3LM, a probabilistically permuted prophet... | Jing Zhao, Junwei Bao, Xiaodong He, Yeyun Gong, Yifan Wang, Ying Jiangyong, Youzheng Wu |  |
| 498 |  |  [Holistic Sentence Embeddings for Better Out-of-Distribution Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.497) |  | 0 | Detecting out-of-distribution (OOD) instances is significant for the safe deployment of NLP models. Among recent textual OOD detection works based on pretrained language models (PLMs), distance-based methods have shown superior performance. However, they estimate sample distance scores in the... | Rundong Gao, Sishuo Chen, Xiaohan Bi, Xu Sun |  |
| 499 |  |  [MuGER2: Multi-Granularity Evidence Retrieval and Reasoning for Hybrid Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.498) |  | 0 | Hybrid question answering (HQA) aims to answer questions over heterogeneous data, including tables and passages linked to table cells. The heterogeneous data can provide different granularity evidence to HQA models, e.t., column, row, cell, and link. Conventional HQA models usually retrieve coarse-... | Chaoqun Duan, Junwei Bao, Tiejun Zhao, Xiaodong He, Yingyao Wang, Youzheng Wu |  |
| 500 |  |  [EntityCS: Improving Zero-Shot Cross-lingual Transfer with Entity-Centric Code Switching](https://doi.org/10.18653/v1/2022.findings-emnlp.499) |  | 0 | Accurate alignment between languages is fundamental for improving cross-lingual pre-trained language models (XLMs). Motivated by the natural phenomenon of code-switching (CS) in multilingual speakers, CS has been used as an effective data augmentation method that offers language alignment at word-... | Chenxi Whitehouse, Fenia Christopoulou, Ignacio Iacobacci |  |
| 501 |  |  [MBTI Personality Prediction for Fictional Characters Using Movie Scripts](https://doi.org/10.18653/v1/2022.findings-emnlp.500) |  | 0 | An NLP model that understands stories should be able to understand the characters in them. To support the development of neural models for this purpose, we construct a benchmark, Story2Personality. The task is to predict a movie character’s MBTI or Big 5 personality types based on the narratives of... | Dakuo Wang, Jeffrey M. Stanton, Jing Li, Mo Yu, Xiangyang Mou, Yisi Sang |  |
| 502 |  |  [A Simple and Strong Baseline for End-to-End Neural RST-style Discourse Parsing](https://doi.org/10.18653/v1/2022.findings-emnlp.501) |  | 0 | To promote and further develop RST-style discourse parsing models, we need a strong baseline that can be regarded as a reference for reporting reliable experimental results. This paper explores a strong baseline by integrating existing simple parsing strategies, top-down and bottom-up, with various... | Hidetaka Kamigaito, Manabu Okumura, Masaaki Nagata, Naoki Kobayashi, Tsutomu Hirao |  |
| 503 |  |  [Probing for Constituency Structure in Neural Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.502) |  | 0 | In this paper, we investigate to which extent contextual neural language models (LMs) implicitly learn syntactic structure. More concretely, we focus on constituent structure as represented in the Penn Treebank (PTB). Using standard probing techniques based on diagnostic classifiers, we assess the... | David Arps, Hassan Sajjad, Laura Kallmeyer, Younes Samih |  |
| 504 |  |  [Table-To-Text generation and pre-training with TabT5](https://doi.org/10.18653/v1/2022.findings-emnlp.503) |  | 0 | Encoder-only transformer models have been successfully applied to different table understanding tasks, as in TAPAS. A major limitation of these architectures is that they are constrained to classification-like tasks such as cell selection or entailment detection. We present TabT5, an... | Ewa Andrejczuk, Francesco Piccinno, Julian Martin Eisenschlos, Syrine Krichene, Yasemin Altun |  |
| 505 |  |  [A POMDP Dialogue Policy with 3-way Grounding and Adaptive Sensing for Learning through Communication](https://doi.org/10.18653/v1/2022.findings-emnlp.504) |  | 0 | Agents to assist with rescue, surgery, and similar activities could collaborate better with humans if they could learn new strategic behaviors through communication. We introduce a novel POMDP dialogue policy for learning from people. The policy has 3-way grounding of language in the shared... | Alan R. Wagner, Maryam Zare, Rebecca J. Passonneau |  |
| 506 |  |  [PaCo: Preconditions Attributed to Commonsense Knowledge](https://doi.org/10.18653/v1/2022.findings-emnlp.505) |  | 0 | Humans can seamlessly reason with circumstantial preconditions of commonsense knowledge. We understand that a glass is used for drinking water, unless the glass is broken or the water is toxic. Despite state-of-the-art (SOTA) language models’ (LMs) impressive performance on inferring commonsense... | Ehsan Qasemi, Filip Ilievski, Muhao Chen, Pedro A. Szekely |  |
| 507 |  |  [Improving Few-Shot Domain Transfer for Named Entity Disambiguation with Pattern Exploitation](https://doi.org/10.18653/v1/2022.findings-emnlp.506) |  | 0 | Named entity disambiguation (NED) is a critical subtask of entity linking, which seeks to connect knowledge base entities with textual mentions of those entities. Naturally, the performance of a model depends on the domain it was trained on; thus, reducing the amount of data required to train... | Kfir Bar, Philip Blair |  |
| 508 |  |  [Capturing Topic Framing via Masked Language Modeling](https://doi.org/10.18653/v1/2022.findings-emnlp.507) |  | 0 | Differential framing of issues can lead to divergent world views on important issues. This is especially true in domains where the information presented can reach a large audience, such as traditional and social media. Scalable and reliable measurement of such differential framing is an important... | Soroush Vosoughi, Weicheng Ma, Xiaobo Guo |  |
| 509 |  |  [WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation](https://doi.org/10.18653/v1/2022.findings-emnlp.508) |  | 0 | A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel approach for dataset creation based on worker and AI collaboration, which brings together the... | Alisa Liu, Noah A. Smith, Swabha Swayamdipta, Yejin Choi |  |
| 510 |  |  [Sequentially Controlled Text Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.509) |  | 0 | While GPT-2 generates sentences that are remarkably human-like, longer documents can ramble and do not follow human-like writing structure. We study the problem of imposing structure on long-range text. We propose a novel controlled text generation task, sequentially controlled text generation, and... | Alexander Spangher, Nanyun Peng, Xinyu Hua, Yao Ming |  |
| 511 |  |  [Revisiting the Roles of "Text" in Text Games](https://doi.org/10.18653/v1/2022.findings-emnlp.510) |  | 0 | Text games present opportunities for natural language understanding (NLU) methods to tackle reinforcement learning (RL) challenges. However, recent work has questioned the necessity of NLU by showing random text hashes could perform decently. In this paper, we pursue a fine-grained investigation... | Chuang Gan, Josh Tenenbaum, Mo Yu, Shunyu Yao, Yi Gu |  |
| 512 |  |  [FPT: Improving Prompt Tuning Efficiency via Progressive Training](https://doi.org/10.18653/v1/2022.findings-emnlp.511) |  | 0 | Recently, prompt tuning (PT) has gained increasing attention as a parameter-efficient way of tuning pre-trained language models (PLMs). Despite extensively reducing the number of tunable parameters and achieving satisfying performance, PT is training-inefficient due to its slow convergence. To... | Huadong Wang, Maosong Sun, Qun Liu, Yichun Yin, Yufei Huang, Yujia Qin, Zhiyuan Liu |  |
| 513 |  |  [Prompt-learning for Fine-grained Entity Typing](https://doi.org/10.18653/v1/2022.findings-emnlp.512) |  | 0 | As an effective approach to adapting pre-trained language models (PLMs) for specific tasks, prompt-learning has recently attracted much attention from researchers. By using cloze-style language prompts to stimulate the versatile knowledge of PLMs, prompt-learning can achieve promising results on a... | Guangwei Xu, Haitao Zheng, HongGee Kim, Juanzi Li, Ning Ding, Pengjun Xie, Xiaobin Wang, Xu Han, Yulin Chen, Zhiyuan Liu |  |
| 514 |  |  [TransLIST: A Transformer-Based Linguistically Informed Sanskrit Tokenizer](https://doi.org/10.18653/v1/2022.findings-emnlp.513) |  | 0 | Sanskrit Word Segmentation (SWS) is essential in making digitized texts available and in deploying downstream tasks. It is, however, non-trivial because of the sandhi phenomenon that modifies the characters at the word boundaries, and needs special treatment. Existing lexicon driven approaches for... | Jivnesh Sandhan, Laxmidhar Behera, Narein Rao, Pawan Goyal, Rathin Singha, Suvendu Samanta |  |
| 515 |  |  [Fair NLP Models with Differentially Private Text Encoders](https://doi.org/10.18653/v1/2022.findings-emnlp.514) |  | 0 | Encoded text representations often capture sensitive attributes about individuals (e.g., race or gender), which raise privacy concerns and can make downstream models unfair to certain groups. In this work, we propose FEDERATE, an approach that combines ideas from differential privacy and... | Aurélien Bellet, Gaurav Maheshwari, Mikaela Keller, Pascal Denis |  |
| 516 |  |  [Modeling Context With Linear Attention for Scalable Document-Level Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.515) |  | 0 | Document-level machine translation leverages inter-sentence dependencies to produce more coherent and consistent translations. However, these models, predominantly based on transformers, are difficult to scale to long documents as their attention layers have quadratic complexity in the sequence... | Hao Peng, Nikolaos Pappas, Noah A. Smith, Zhaofeng Wu |  |
| 517 |  |  [What do Large Language Models Learn beyond Language?](https://doi.org/10.18653/v1/2022.findings-emnlp.516) |  | 0 | Large language models (LMs) have rapidly become a mainstay in Natural Language Processing. These models are known to acquire rich linguistic knowledge from training on large amounts of text. In this paper, we investigate if pre-training on text also confers these models with helpful ‘inductive... | Avinash Madasu, Shashank Srivastava |  |
| 518 |  |  [CONSISTENT: Open-Ended Question Generation From News Articles](https://doi.org/10.18653/v1/2022.findings-emnlp.517) |  | 0 | Recent work on question generation has largely focused on factoid questions such as who, what,where, when about basic facts. Generating open-ended why, how, what, etc. questions thatrequire long-form answers have proven more difficult. To facilitate the generation of openended questions, we propose... | Justin Lewis, Smaranda Muresan, Tuhin Chakrabarty |  |
| 519 |  |  [Efficient (Soft) Q-Learning for Text Generation with Limited Good Data](https://doi.org/10.18653/v1/2022.findings-emnlp.518) |  | 0 | Maximum likelihood estimation (MLE) is the predominant algorithm for training text generation models. This paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models.... | Bowen Tan, Eric P. Xing, Han Guo, Zhengzhong Liu, Zhiting Hu |  |
| 520 |  |  [Lexi: Self-Supervised Learning of the UI Language](https://doi.org/10.18653/v1/2022.findings-emnlp.519) |  | 0 | Humans can learn to operate the user interface (UI) of an application by reading an instruction manual or how-to guide. Along with text, these resources include visual content such as UI screenshots and images of application icons referenced in the text. We explore how to leverage this data to... | Chitta Baral, Kushal Arora, Oriana Riva, Pratyay Banerjee, Shweti Mahajan |  |
| 521 |  |  [Inferring the Reader: Guiding Automated Story Generation with Commonsense Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.520) |  | 0 | Transformer-based language model approaches to automated story generation currently provide state-of-the-art results. However, they still suffer from plot incoherence when generatingnarratives over time, and critically lack basiccommonsense reasoning. Furthermore, existing methods generally focus... | Mark O. Riedl, Sarah Wiegreffe, Siyan Li, Xiangyu Peng |  |
| 522 |  |  [How to Stop an Avalanche? JoDeM: Joint Decision Making through Compare and Contrast for Dialog State Tracking](https://doi.org/10.18653/v1/2022.findings-emnlp.521) |  | 0 | Dialog state tracking (DST) is a core component in task-oriented dialog systems. Existing state-of-the-art DST model incorporates insight and intuition from the human experience into design of supplementary labels, which greatly assisted the training process of turn-by-turn DST model. Though the... | Haoming Wang, Wang Xin |  |
| 523 |  |  [Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding](https://doi.org/10.18653/v1/2022.findings-emnlp.522) |  | 0 | Contrastive learning has become a new paradigm for unsupervised sentence embeddings.Previous studies focus on instance-wise contrastive learning, attempting to construct positive pairs with textual data augmentation. In this paper, we propose a novel Contrastive learning method with Prompt-derived... | Jiali Zeng, Shuangzhi Wu, Yongjing Yin, Yufan Jiang, Yunbo Cao |  |
| 524 |  |  [Weight Perturbation as Defense against Adversarial Word Substitutions](https://doi.org/10.18653/v1/2022.findings-emnlp.523) |  | 0 | The existence and pervasiveness of textual adversarial examples have raised serious concerns to security-critical applications. Many methods have been developed to defend against adversarial attacks for neural natural language processing (NLP) models.Adversarial training is one of the most... | ChoJui Hsieh, Jianhan Xu, Jiping Zhang, KaiWei Chang, Linyang Li, Xiaoqing Zheng, Xuanjing Huang |  |
| 525 |  |  [CORT: A New Baseline for Comparative Opinion Classification by Dual Prompts](https://doi.org/10.18653/v1/2022.findings-emnlp.524) |  | 0 | Comparative opinion is a common linguistic phenomenon. The opinion is expressed by comparing multiple targets on a shared aspect, e.g., “camera A is better than camera B in picture quality”. Among the various subtasks in opinion mining, comparative opinion classification is relatively less studied.... | Aixin Sun, Hengran Zhang, Xuying Meng, Yequan Wang |  |
| 526 |  |  [APEACH: Attacking Pejorative Expressions with Analysis on Crowd-Generated Hate Speech Evaluation Datasets](https://doi.org/10.18653/v1/2022.findings-emnlp.525) |  | 0 | In hate speech detection, developing training and evaluation datasets across various domains is the critical issue. Whereas, major approaches crawl social media texts and hire crowd-workers to annotate the data. Following this convention often restricts the scope of pejorative expressions to a... | Kichang Yang, WonIk Cho, Wonjun Jang |  |
| 527 |  |  [Guiding Neural Story Generation with Reader Models](https://doi.org/10.18653/v1/2022.findings-emnlp.526) |  | 0 | Automated storytelling has long captured the attention of researchers for the ubiquity of narratives in everyday life. However, it is challenging to maintain coherence and stay on-topictoward a specific ending when generating narratives with neural language models. In this paper, we introduce Story... | Amal Alabdulkarim, Harshith Kayam, Kaige Xie, Mark O. Riedl, Samihan Dani, Xiangyu Peng |  |
| 528 |  |  [Reason first, then respond: Modular Generation for Knowledge-infused Dialogue](https://doi.org/10.18653/v1/2022.findings-emnlp.527) |  | 0 | Large language models can produce fluent dialogue but often hallucinate factual inaccuracies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simultaneously. In this work, we... | Arthur Szlam, Jack Urbanek, Jason Weston, Kurt Shuster, Leonard Adolphs |  |
| 529 |  |  [Adapting Multilingual Models for Code-Mixed Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.528) |  | 0 | The scarcity of gold standard code-mixed to pure language parallel data makes it difficult to train translation models reliably.Prior work has addressed the paucity of parallel data with data augmentation techniques.Such methods rely heavily on external resources making systems difficult to train... | Abhirut Gupta, Aditya Vavre, Sunita Sarawagi |  |
| 530 |  |  [LPC: A Logits and Parameter Calibration Framework for Continual Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.529) |  | 0 | When we execute the typical fine-tuning paradigm on continuously sequential tasks, the model will suffer from the catastrophic forgetting problem (i.e., the model tends to adjust old parameters according to the new knowledge, which leads to the loss of previously acquired concepts). People proposed... | Bhavani Thuraisingham, Dingcheng Li, Latifur Khan, Xiaodi Li, Zhuoyi Wang |  |
| 531 |  |  [SlovakBERT: Slovak Masked Language Model](https://doi.org/10.18653/v1/2022.findings-emnlp.530) |  | 0 | We introduce a new Slovak masked language model called SlovakBERT. This is to our best knowledge the first paper discussing Slovak transformers-based language models. We evaluate our model on several NLP tasks and achieve state-of-the-art results. This evaluation is likewise the first attempt to... | Filip Uhlárik, Marián Simko, Martin Konopka, Martin Tamajka, Matús Pikuliak, Michal Trnka, Miroslav Blsták, Pavol Balázik, Stefan Grivalsky, Viktor Bachratý |  |
| 532 |  |  [Efficient Zero-shot Event Extraction with Context-Definition Alignment](https://doi.org/10.18653/v1/2022.findings-emnlp.531) |  | 0 | Event extraction (EE) is the task of identifying interested event mentions from text.Conventional efforts mainly focus on the supervised setting. However, these supervised models cannot generalize to event types out of the pre-defined ontology. To fill this gap, many efforts have been devoted to... | Dong Yu, Hongming Zhang, Wenlin Yao |  |
| 533 |  |  [Logical Fallacy Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.532) |  | 0 | Reasoning is central to human intelligence. However, fallacious arguments are common, and some exacerbate problems such as spreading misinformation about climate change. In this paper, we propose the task of logical fallacy detection, and provide a new dataset (Logic) of logical fallacies generally... | Abhinav Lalwani, Bernhard Schölkopf, Mrinmaya Sachan, Rada Mihalcea, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, Zhiheng Lyu, Zhijing Jin |  |
| 534 |  |  [Topic-Aware Response Generation in Task-Oriented Dialogue with Unstructured Knowledge Access](https://doi.org/10.18653/v1/2022.findings-emnlp.533) |  | 0 | To alleviate the problem of structured databases’ limited coverage, recent task-oriented dialogue systems incorporate external unstructured knowledge to guide the generation of system responses. However, these usually use word or sentence level similarities to detect the relevant knowledge context,... | Gerasimos Lampouras, Ignacio Iacobacci, Yue Feng |  |
| 535 |  |  [Revisiting Transformer-based Models for Long Document Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.534) |  | 0 | The recent literature in text classification is biased towards short text sequences (e.g., sentences or paragraphs). In real-world applications, multi-page multi-paragraph documents are common and they cannot be efficiently encoded by vanilla Transformer-based models. We compare different... | Desmond Elliott, Ilias Chalkidis, Sune Darkner, Xiang Dai |  |
| 536 |  |  [Time-aware Prompting for Text Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.535) |  | 0 | In this paper, we study the effects of incorporating timestamps, such as document creation dates, into generation systems. Two types of time-aware prompts are investigated: (1) textual prompts that encode document timestamps in natural language sentences; and (2) linear prompts that convert... | Lu Wang, Shuyang Cao |  |
| 537 |  |  [Improving Scheduled Sampling with Elastic Weight Consolidation for Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.536) |  | 0 | Despite strong performance in many sequence-to-sequence tasks, autoregressive models trained with maximum likelihood estimation suffer from exposure bias, i.e. the discrepancy between the ground-truth prefixes used during training and the model-generated prefixes used at inference time. Scheduled... | Andreas Vlachos, Michalis Korakakis |  |
| 538 |  |  [Ensemble Transformer for Efficient and Accurate Ranking Tasks: an Application to Question Answering Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.537) |  | 0 | Large transformer models can highly improve Answer Sentence Selection (AS2) tasks, but their high computational costs prevent their use in many real-world applications. In this paper, we explore the following research question: How can we make the AS2 models more accurate without significantly... | Alessandro Moschitti, Eric Lind, Luca Soldaini, Yoshitomo Matsubara |  |
| 539 |  |  [Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis](https://doi.org/10.18653/v1/2022.findings-emnlp.538) |  | 0 | Pre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration... | LouisPhilippe Morency, Paul Pu Liang, Ruslan Salakhutdinov, Umang Bhatt, Willie Neiswanger, Yuxin Xiao |  |
| 540 |  |  [How to Represent Context Better? An Empirical Study on Context Modeling for Multi-turn Response Selection](https://doi.org/10.18653/v1/2022.findings-emnlp.539) |  | 0 | Building retrieval-based dialogue models that can predict appropriate responses based on the understanding of multi-turn context messages is a challenging problem. Early models usually concatenate all utterances or independently encode each dialogue turn, which may lead to an inadequate... | Chang Liu, Chongyang Tao, Dongyan Zhao, Jiazhan Feng, Rui Yan |  |
| 541 |  |  [CHIA: CHoosing Instances to Annotate for Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.540) |  | 0 | Neural machine translation (MT) systems have been shown to perform poorly on low-resource language pairs, for which large-scale parallel data is unavailable. Making the data annotation process faster and cheaper is therefore important to ensure equitable access to MT systems. To make optimal use of... | Ananya Ganesh, Katharina Kann, Rajat Bhatnagar |  |
| 542 |  |  [Guiding Neural Machine Translation with Semantic Kernels](https://doi.org/10.18653/v1/2022.findings-emnlp.541) |  | 0 | Machine Translation task has made great progress with the help of auto-regressive decoding paradigm and Transformer architecture. In this paradigm, though the encoder can obtain global source representations, the decoder can only use translation history to determine the current word. Previous... | Luxi Xing, Ping Guo, Xiangpeng Wei, Yubing Ren, Yue Hu, Yunpeng Li, Yuqiang Xie |  |
| 543 |  |  [HiSMatch: Historical Structure Matching based Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.542) |  | 0 | A Temporal Knowledge Graph (TKG) is a sequence of KGs with respective timestamps, which adopts quadruples in the form of (subject, relation, object, timestamp) to describe dynamic facts. TKG reasoning has facilitated many real-world applications via answering such queries as (query entity, query... | Jiafeng Guo, Long Bai, Saiping Guan, Wei Li, Weihua Peng, Xiaolong Jin, Xueqi Cheng, Yajuan Lyu, Zhongni Hou, Zixuan Li |  |
| 544 |  |  [Dependency Parsing via Sequence Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.543) |  | 0 | Dependency parsing aims to extract syntactic dependency structure or semantic dependency structure for sentences.Existing methods for dependency parsing include transition-based method, graph-based method and sequence-to-sequence method.These methods obtain excellent performance and we notice them... | Binghao Tang, Boda Lin, Jiaxin Shi, Juanzi Li, Lei Hou, Shulin Cao, Si Li, Yong Luo, Zijun Yao |  |
| 545 |  |  [Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments](https://doi.org/10.18653/v1/2022.findings-emnlp.544) |  | 0 | Neural scaling laws define a predictable relationship between a model’s parameter count and its performance after training in the form of a power law. However, most research to date has not explicitly investigated whether scaling laws can be used to accelerate model development. In this work, we... | Jonathan Berant, Maor Ivgi, Yair Carmon |  |
| 546 |  |  [Analyzing the Limits of Self-Supervision in Handling Bias in Language](https://doi.org/10.18653/v1/2022.findings-emnlp.545) |  | 0 | Prompting inputs with natural language task descriptions has emerged as a popular mechanism to elicit reasonably accurate outputs from large-scale generative language models with little to no in-context supervision. This also helps gain insight into how well language models capture the semantics of... | Dilek HakkaniTur, Karthik Gopalakrishnan, Lisa Bauer, Mohit Bansal, Spandana Gella, Yang Liu |  |
| 547 |  |  [Multiple Instance Learning for Offensive Language Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.546) |  | 0 | Automatic offensive language detection has become a crucial issue in recent years. Existing researches on this topic are usually based on a large amount of data annotated at sentence level to train a robust model. However, sentence-level annotations are expensive in practice as the scenario... | Dehan Kong, Dinghui Mao, Hui Xue, Jiexi Liu, Longtao Huang |  |
| 548 |  |  [Grounded Keys-to-Text Generation: Towards Factual Open-Ended Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.547) |  | 0 | Large pre-trained language models have recently enabled open-ended generation frameworks (e.g., prompt-to-text NLG) to tackle a variety of tasks going beyond the traditional data-to-text generation. While this framework is more general, it is under-specified and often leads to a lack of... | Baolin Peng, Bill Dolan, Faeze Brahman, Jianfeng Gao, Michel Galley, Snigdha Chaturvedi, Sudha Rao |  |
| 549 |  |  [CogKTR: A Knowledge-Enhanced Text Representation Toolkit for Natural Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-demos.1) |  | 0 | As the first step of modern natural language processing, text representation encodes discrete texts as continuous embeddings. Pre-trained language models (PLMs) have demonstrated strong ability in text representation and significantly promoted the development of natural language understanding... | Hongbang Yuan, Jun Zhao, Kang Liu, Pengfei Cao, Tianyi Men, Yubo Chen, Yuyang Zhou, Zhipeng Xue, Zhuoran Jin |  |
| 550 |  |  [LM-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models](https://doi.org/10.18653/v1/2022.emnlp-demos.2) |  | 0 | The opaque nature and unexplained behavior of transformer-based language models (LMs) have spurred a wide interest in interpreting their predictions. However, current interpretation methods mostly focus on probing models from outside, executing behavioral tests, and analyzing salience input... | Avi Caciularu, Bar Tamir, Guy Dar, Micah Shlain, Mor Geva, Paul Roit, Shoval Sadde, Yoav Goldberg |  |
| 551 |  |  [EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-demos.3) |  | 0 | Pre-Trained Models (PTMs) have reshaped the development of Natural Language Processing (NLP) and achieved significant improvement in various benchmarks. Yet, it is not easy for industrial practitioners to obtain high-performing PTM-based models without a large amount of labeled training data and... | Chengyu Wang, Jianing Wang, Jun Huang, Lei Li, Ming Wang, Minghui Qiu, Taolin Zhang, Tingting Liu, Wei Lin |  |
| 552 |  |  [An Explainable Toolbox for Evaluating Pre-trained Vision-Language Models](https://doi.org/10.18653/v1/2022.emnlp-demos.4) |  | 0 | We introduce VL-CheckList, a toolbox for evaluating Vision-Language Pretraining (VLP) models, including the preliminary datasets that deepen the image-texting ability of a VLP model. Most existing VLP works evaluated their systems by comparing the fine-tuned downstream task performance. However,... | Haozhan Shen, Jianwei Yin, Kyusong Lee, Mingwei Zhu, Tiancheng Zhao, Tianqi Zhang, Xiaopeng Lu |  |
| 553 |  |  [TweetNLP: Cutting-Edge Natural Language Processing for Social Media](https://doi.org/10.18653/v1/2022.emnlp-demos.5) |  | 0 | In this paper we present TweetNLP, an integrated platform for Natural Language Processing (NLP) in social media. TweetNLP supports a diverse set of NLP tasks, including generic focus areas such as sentiment analysis and named entity recognition, as well as social media-specific tasks such as emoji... | Asahi Ushio, Daniel Loureiro, Dimosthenis Antypas, Eugenio Martínez Cámara, Fangyu Liu, Joanne Boisson, José CamachoCollados, Kiamehr Rezaee, Luis Espinosa Anke, Talayeh Riahi |  |
| 554 |  |  [JoeyS2T: Minimalistic Speech-to-Text Modeling with JoeyNMT](https://doi.org/10.18653/v1/2022.emnlp-demos.6) |  | 0 | JoeyS2T is a JoeyNMT extension for speech-to-text tasks such as automatic speech recognition and end-to-end speech translation. It inherits the core philosophy of JoeyNMT, a minimalist NMT toolkit built on PyTorch, seeking simplicity and accessibility. JoeyS2T’s workflow is self-contained, starting... | Julia Kreutzer, Mayumi Ohta, Stefan Riezler |  |
| 555 |  |  [FairLib: A Unified Framework for Assessing and Improving Fairness](https://doi.org/10.18653/v1/2022.emnlp-demos.7) |  | 0 | This paper presents FairLib, an open-source python library for assessing and improving model fairness. It provides a systematic framework for quickly accessing benchmark datasets, reproducing existing debiasing baseline models, developing new methods, evaluating models with different metrics, and... | Aili Shen, Lea Frermann, Timothy Baldwin, Trevor Cohn, Xudong Han, Yitong Li |  |
| 556 |  |  [ELEVANT: A Fully Automatic Fine-Grained Entity Linking Evaluation and Analysis Tool](https://doi.org/10.18653/v1/2022.emnlp-demos.8) |  | 0 | We present Elevant, a tool for the fully automatic fine-grained evaluation of a set of entity linkers on a set of benchmarks. Elevant provides an automatic breakdown of the performance by various error categories and by entity type. Elevant also provides a rich and compact, yet very intuitive and... | Hannah Bast, Matthias Hertel, Natalie Prange |  |
| 557 |  |  [A Pipeline for Generating, Annotating and Employing Synthetic Data for Real World Question Answering](https://doi.org/10.18653/v1/2022.emnlp-demos.9) |  | 0 | Question Answering (QA) is a growing area of research, often used to facilitate the extraction of information from within documents. State-of-the-art QA models are usually pre-trained on domain-general corpora like Wikipedia and thus tend to struggle on out-of-domain documents without fine-tuning.... | James Ravenscroft, Maria Liakata, Matthew Maufe, Rob Procter |  |
| 558 |  |  [DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population](https://doi.org/10.18653/v1/2022.emnlp-demos.10) |  | 0 | We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in the knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation... | Haiyang Yu, Hongbin Ye, Lei Li, Liankuan Tao, Ningyu Zhang, Shuofei Qiao, Xiang Chen, Xin Xie, Xin Xu, Zhoubo Li |  |
| 559 |  |  [AnEMIC: A Framework for Benchmarking ICD Coding Models](https://doi.org/10.18653/v1/2022.emnlp-demos.11) |  | 0 | Diagnostic coding, or ICD coding, is the task of assigning diagnosis codes defined by the ICD (International Classification of Diseases) standard to patient visits based on clinical notes. The current process of manual ICD coding is time-consuming and often error-prone, which suggests the need for... | Abheesht Sharma, Jeremy C. Weiss, Juyong Kim, Pradeep Ravikumar, Suhas Shanbhogue |  |
| 560 |  |  [SPEAR : Semi-supervised Data Programming in Python](https://doi.org/10.18653/v1/2022.emnlp-demos.12) |  | 0 | We present SPEAR, an open-source python library for data programming with semi supervision. The package implements several recent data programming approaches including facility to programmatically label and build training data. SPEAR facilitates weak supervision in the form of heuristics (or rules)... | Ayush Maheshwari, Ganesh Ramakrishnan, Guttu Sai Abhishek, Harshad Ingole, Parth Laturia, Rishabh K. Iyer, Vineeth Dorna |  |
| 561 |  |  [Evaluate & Evaluation on the Hub: Better Best Practices for Data and Model Measurements](https://doi.org/10.18653/v1/2022.emnlp-demos.13) |  | 0 | Evaluation is a key part of machine learning (ML), yet there is a lack of support and tooling to enable its informed and systematic practice. We introduce Evaluate and Evaluation on the Hub—a set of tools to facilitate the evaluation of models and datasets in ML. Evaluate is a library to support... | Abhishek Thakur, Aleksandra Piktus, Felix Marty, Helen Ngo, Leandro von Werra, Lewis Tunstall, Nazneen Rajani, Sasha Luccioni, Tristan Thrush, Victor Mustar |  |
| 562 |  |  [KeywordScape: Visual Document Exploration using Contextualized Keyword Embeddings](https://doi.org/10.18653/v1/2022.emnlp-demos.14) |  | 0 | Although contextualized word embeddings have led to great improvements in automatic language understanding, their potential for practical applications in document exploration and visualization has been little explored. Common visualization techniques used for, e.g., model analysis usually provide... | Henrik Voigt, Kai Lawonn, Monique Meuschke, Sina Zarrieß |  |
| 563 |  |  [MedConQA: Medical Conversational Question Answering System based on Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-demos.15) |  | 0 | The medical conversational system can relieve doctors’ burden and improve healthcare efficiency, especially during the COVID-19 pandemic. However, the existing medical dialogue systems have the problems of weak scalability, insufficient knowledge, and poor controllability. Thus, we propose a... | Bin Li, Bin Sun, Fei Xia, Jun Zhao, Kang Liu, Shizhu He, Shutao Li, Yixuan Weng |  |
| 564 |  |  [Label Sleuth: From Unlabeled Text to a Classifier in a Few Hours](https://doi.org/10.18653/v1/2022.emnlp-demos.16) |  | 0 | Label Sleuth is an open source platform for building text classifiers which does not require coding skills nor machine learning knowledge.- Project website: [https://www.label-sleuth.org/](https://www.label-sleuth.org/)- Link to screencast video:... | Alon Halfon, Ariel Gera, Dakuo Wang, Dina Epelboim, Eyal Shnarch, Leshem Choshen, Marina Danilevsky, Martín Santillán Cooper, Yannis Katsis, Zheng Zhang |  |
| 565 |  |  [AGReE: A system for generating Automated Grammar Reading Exercises](https://doi.org/10.18653/v1/2022.emnlp-demos.17) |  | 0 | We describe the AGReE system, which takes user-submitted passages as input and automatically generates grammar practice exercises that can be completed while reading. Multiple-choice practice items are generated for a variety of different grammar constructs: punctuation, articles, conjunctions,... | Debanjan Ghosh, Mengxuan Zhao, Sophia Chan, Swapna Somasundaran |  |
| 566 |  |  [BotSIM: An End-to-End Bot Simulation Framework for Commercial Task-Oriented Dialog Systems](https://doi.org/10.18653/v1/2022.emnlp-demos.18) |  | 0 | We present BotSIM, a data-efficient end-to-end Bot SIMulation framework for commercial task-oriented dialog (TOD) systems. BotSIM consists of three major components: 1) a Generator that can infer semantic-level dialog acts and entities from bot definitions and generate user queries via model-based... | Gang Wu, Guangsen Wang, Jimmy Au, Samson Tan, Shafiq R. Joty, Steven C. H. Hoi |  |
| 567 |  |  [DeepGen: Diverse Search Ad Generation and Real-Time Customization](https://doi.org/10.18653/v1/2022.emnlp-demos.19) |  | 0 | Demo: https://youtu.be/WQLL93TPB-cAbstract:We present DeepGen, a system deployed at web scale for automatically creating sponsored search advertisements (ads) for BingAds customers. We leverage state-of-the-art natural language generation (NLG) models to generate fluent ads from advertiser’s web... | Bingyu Chi, Jie Cao, Junyi Chai, Konstantin Golobokov, Mandy Gu, Victor Ye Dong, Yi Liu, Yulan Yan |  |
| 568 |  |  [ACCoRD: A Multi-Document Approach to Generating Diverse Descriptions of Scientific Concepts](https://doi.org/10.18653/v1/2022.emnlp-demos.20) |  | 0 | Systems that automatically define unfamiliar terms hold the promise of improving the accessibility of scientific texts, especially for readers who may lack prerequisite background knowledge. However, current systems assume a single “best” description per concept, which fails to account for the many... | Bailey Kuehl, Chandra Bhagavatula, Daniel King, Daniel S. Weld, Doug Downey, Jonathan Borchardt, Kyle Lo, Sonia K. Murthy, Sophie Johnson, Tom Hope |  |
| 569 |  |  [Automatic Comment Generation for Chinese Student Narrative Essays](https://doi.org/10.18653/v1/2022.emnlp-demos.21) |  | 0 | Automatic essay evaluation can help reduce teachers’ workload and enable students to refine their works rapidly. Previous studies focus mainly on giving discrete scores for either the holistic quality orseveral distinct traits. However, real-world teachers usually provide detailed comments in... | Guowei Xu, Jian Guan, Minlie Huang, Yixiang Tian, Zhexin Zhang |  |
| 570 |  |  [MIC: A Multi-task Interactive Curation Tool](https://doi.org/10.18653/v1/2022.emnlp-demos.22) |  | 0 | This paper introduces MIC, a Multi-task Interactive Curation tool, a human-machine collaborative curation tool for multiple NLP tasks. The tool aims to borrow recent advances in literature to solve pain-points in real NLP tasks. Firstly, it supports multiple projects with multiple users which... | Jerrod Parker, Mingfeng Yang, Shi Yu, Stephen Brock |  |
| 571 |  |  [SUMMARY WORKBENCH: Unifying Application and Evaluation of Text Summarization Models](https://doi.org/10.18653/v1/2022.emnlp-demos.23) |  | 0 | This paper presents Summary Workbench, a new tool for developing and evaluating text summarization models. New models and evaluation measures can be easily integrated as Docker-based plugins, allowing to examine the quality of their summaries against any input and to evaluate them using various... | Dominik Schwabe, Martin Potthast, Shahbaz Syed |  |
| 572 |  |  [Arabic Word-level Readability Visualization for Assisted Text Simplification](https://doi.org/10.18653/v1/2022.emnlp-demos.24) |  | 0 | This demo paper presents a Google Docs add-on for automatic Arabic word-level readability visualization. The add-on includes a lemmatization component that is connected to a five-level readability lexicon and Arabic WordNet-based substitution suggestions. The add-on can be used for assessing the... | Bashar Alhafni, Hind Saddiki, Muhamed AlKhalil, Nizar Habash, Reem Hazim |  |
| 573 |  |  [LogiTorch: A PyTorch-based library for logical reasoning on natural language](https://doi.org/10.18653/v1/2022.emnlp-demos.25) |  | 0 | Logical reasoning on natural language is one of the most challenging tasks for deep learning models. There has been an increasing interest in developing new benchmarks to evaluate the reasoning capabilities of language models such as BERT. In parallel, new models based on transformers have emerged... | Chadi Helwe, Chloé Clavel, Fabian M. Suchanek |  |
| 574 |  |  [stopes - Modular Machine Translation Pipelines](https://doi.org/10.18653/v1/2022.emnlp-demos.26) |  | 0 | Neural machine translation, as other natural language deep learning applications, is hungry for data. As research evolves, the data pipelines supporting that research evolve too, oftentimes re-implementing the same core components. Despite the potential of modular codebases, researchers have but... | Alexandre Mourachko, Ammar Kamran, Angela Fan, Anna Y. Sun, Guillaume Wenzek, Holger Schwenk, Kevin Heffernan, Onur Çelebi, Pierre Andrews, Yingzhe Guo |  |
| 575 |  |  [GEMv2: Multilingual NLG Benchmarking in a Single Line of Code](https://doi.org/10.18653/v1/2022.emnlp-demos.27) |  | 0 | Evaluations in machine learning rarely use the latest metrics, datasets, or human evaluation in favor of remaining compatible with prior work. The compatibility, often facilitated through leaderboards, thus leads to outdated but standardized evaluation practices. We pose that the standardization is... | Abhik Bhattacharjee, Abinaya Mahendiran, Alex Wang, Alexandros Papangelis, Aman Madaan, Angelina McMillanMajor, Anna Shvets, Ashish Upadhyay, Bernd Bohnet, Sebastian Gehrmann |  |
| 576 |  |  [KGI: An Integrated Framework for Knowledge Intensive Language Tasks](https://doi.org/10.18653/v1/2022.emnlp-demos.28) |  | 0 | In this paper, we present a system to showcase the capabilities of the latest state-of-the-art retrieval augmented generation models trained on knowledge-intensive language tasks, such as slot filling, open domain question answering, dialogue, and fact-checking. Moreover, given a user query, we... | Alfio Gliozzo, Gaetano Rossiello, Md. Faisal Mahbub Chowdhury, Michael R. Glass, Nandana Mihindukulasooriya |  |
| 577 |  |  [Twitter-Demographer: A Flow-based Tool to Enrich Twitter Data](https://doi.org/10.18653/v1/2022.emnlp-demos.29) |  | 0 | Twitter data have become essential to Natural Language Processing (NLP) and social science research, driving various scientific discoveries in recent years. However, the textual data alone are often not enough to conduct studies: especially, social scientists need more variables to perform their... | Dirk Hovy, Federico Bianchi, Vincenzo Cutrona |  |
| 578 |  |  [Azimuth: Systematic Error Analysis for Text Classification](https://doi.org/10.18653/v1/2022.emnlp-demos.30) |  | 0 | We present Azimuth, an open-source and easy-to-use tool to perform error analysis for text classification. Compared to other stages of the ML development cycle, such as model training and hyper-parameter tuning, the process and tooling for the error analysis stage are less mature. However, this... | Chris Tyler, Di Le, Frederic BranchaudCharron, Gabrielle Gauthier Melançon, Joseph Marinier, Karine Grande, Lindsay Brin, Orlando Marquez Ayala |  |
| 579 |  |  [SynKB: Semantic Search for Synthetic Procedures](https://doi.org/10.18653/v1/2022.emnlp-demos.31) |  | 0 | In this paper we present SynKB, an open-source, automatically extracted knowledge base of chemical synthesis protocols. Similar to proprietary chemistry databases such as Reaxsys, SynKB allows chemists to retrieve structured knowledge about synthetic procedures. By taking advantage of recent... | Alan Ritter, Dayne Freitag, Fan Bai, John Niekrasz, Peter B. Madrid |  |
| 580 |  |  [Camelira: An Arabic Multi-Dialect Morphological Disambiguator](https://doi.org/10.18653/v1/2022.emnlp-demos.32) |  | 0 | We present Camelira, a web-based Arabic multi-dialect morphological disambiguation tool that covers four major variants of Arabic: Modern Standard Arabic, Egyptian, Gulf, and Levantine.Camelira offers a user-friendly web interface that allows researchers and language learners to explore various... | Go Inoue, Nizar Habash, Ossama Obeid |  |
| 581 |  |  [POTATO: The Portable Text Annotation Tool](https://doi.org/10.18653/v1/2022.emnlp-demos.33) |  | 0 | We present POTATO, the Portable text annotation tool, a free, fully open-sourced annotation system that 1) supports labeling many types of text and multimodal data; 2) offers easy-to-configure features to maximize the productivity of both deployers and annotators (convenient templates for common... | Aparna Ananthasubramaniam, Apostolos Dedeloudis, David Jurgens, Jackson Sargent, Jiaxin Pei, Naitian Zhou, Xingyao Wang |  |
| 582 |  |  [KGxBoard: Explainable and Interactive Leaderboard for Evaluation of Knowledge Graph Completion Models](https://doi.org/10.18653/v1/2022.emnlp-demos.34) |  | 0 | Knowledge Graphs (KGs) store information in the form of (head, predicate, tail)-triples. To augment KGs with new knowledge, researchers proposed models for KG Completion (KGC) tasks such as link prediction; i.e., answering (h; p; ?) or (?; p; t) queries. Such models are usually evaluated with... | Carolin Lawrence, Christopher Malon, Daniel Ruffinelli, Graham Neubig, Haris Widjaja, Kiril Gashteovski, Pengfei Liu, Wiem Ben Rim |  |
| 583 |  |  [FALTE: A Toolkit for Fine-grained Annotation for Long Text Evaluation](https://doi.org/10.18653/v1/2022.emnlp-demos.35) |  | 0 | A growing swath of NLP research is tackling problems related to generating long text, including tasks such as open-ended story generation, summarization, dialogue, and more. However, we currently lack appropriate tools to evaluate these long outputs of generation models: classic automatic metrics... | Greg Durrett, Junyi Jessy Li, Tanya Goyal |  |
| 584 |  |  [SEAL: Interactive Tool for Systematic Error Analysis and Labeling](https://doi.org/10.18653/v1/2022.emnlp-demos.36) |  | 0 | With the advent of Transformers, large language models (LLMs) have saturated well-known NLP benchmarks and leaderboards with high aggregate performance. However, many times these models systematically fail on tail data or rare groups not obvious in aggregate evaluation. Identifying such problematic... | James Zou, Lingjiao Chen, Margaret Mitchell, Nazneen Rajani, Weixin Liang |  |
| 585 |  |  [Hands-On Interactive Neuro-Symbolic NLP with DRaiL](https://doi.org/10.18653/v1/2022.emnlp-demos.37) |  | 0 | We recently introduced DRaiL, a declarative neural-symbolic modeling framework designed to support a wide variety of NLP scenarios. In this paper, we enhance DRaiL with an easy to use Python interface, equipped with methods to define, modify and augment DRaiL models interactively, as well as with... | Dan Goldwasser, Maria Leonor Pacheco, Shamik Roy |  |
| 586 |  |  [Paraphrastic Representations at Scale](https://doi.org/10.18653/v1/2022.emnlp-demos.38) |  | 0 | We present a system that allows users to train their own state-of-the-art paraphrastic sentence representations in a variety of languages. We release trained models for English, Arabic, German, Spanish, French, Russian, Turkish, and Chinese. We train these models on large amounts of data, achieving... | Graham Neubig, John Wieting, Kevin Gimpel, Taylor BergKirkpatrick |  |
| 587 |  |  [Snoopy: An Online Interface for Exploring the Effect of Pretraining Term Frequencies on Few-Shot LM Performance](https://doi.org/10.18653/v1/2022.emnlp-demos.39) |  | 0 | Current evaluation schemes for large language models often fail to consider the impact of the overlap between pretraining corpus and test data on model performance statistics. Snoopy is an online interface that allows researchers to study this impact in few-shot learning settings. Our demo provides... | Matt Gardner, Raja Sekhar Reddy Mekala, Robert L. Logan IV, Sameer Singh, Yasaman Razeghi |  |
| 588 |  |  [BMCook: A Task-agnostic Compression Toolkit for Big Models](https://doi.org/10.18653/v1/2022.emnlp-demos.40) |  | 0 | Recently, pre-trained language models (PLMs) have achieved great success on various NLP tasks and have shown a trend of exponential growth in model size. To alleviate the unaffordable computational costs brought by the size growth, model compression has been widely explored. Existing efforts have... | Baitao Gong, Guoyang Zeng, Maosong Sun, Weilin Zhao, Xu Han, Yanxu Chen, Yingfa Chen, Zhengyan Zhang, Zhiyuan Liu |  |
| 589 |  |  [ALToolbox: A Set of Tools for Active Learning Annotation of Natural Language Texts](https://doi.org/10.18653/v1/2022.emnlp-demos.41) |  | 0 | We present ALToolbox – an open-source framework for active learning (AL) annotation in natural language processing. Currently, the framework supports text classification, sequence tagging, and seq2seq tasks. Besides state-of-the-art query strategies, ALToolbox provides a set of tools that help to... | Akim Tsvigun, Aleksandr Rubashevskii, Artem Vazhentsev, Daniil Larionov, Danil Kireev, Gleb Kuzmin, Ivan Lazichny, Leonid Sanochkin, Nikita Khromov, Olga Shahmatova |  |
| 590 |  |  [TextBox 2.0: A Text Generation Library with Pre-trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-demos.42) |  | 0 | To facilitate research on text generation, this paper presents a comprehensive and unified library, TextBox 2.0, focusing on the use of pre-trained language models (PLMs). To be comprehensive, our library covers 13 common text generation tasks and their corresponding 83 datasets and further... | JiRong Wen, JianYun Nie, Junyi Li, Tianyi Tang, Wayne Xin Zhao, Wenxun Dai, Yiwen Hu, Zhipeng Chen, Zhuohao Yu |  |
| 591 |  |  [Unsupervised Term Extraction for Highly Technical Domains](https://doi.org/10.18653/v1/2022.emnlp-industry.1) |  | 0 | Term extraction is an information extraction task at the root of knowledge discovery platforms. Developing term extractors that are able to generalize across very diverse and potentially highly technical domains is challenging, as annotations for domains requiring in-depth expertise are scarce and... | Diego Antognini, Francesco Fusco, Peter W. J. Staar |  |
| 592 |  |  [DynaMaR: Dynamic Prompt with Mask Token Representation](https://doi.org/10.18653/v1/2022.emnlp-industry.2) |  | 0 | Recent research has shown that large language models pretrained using unsupervised approaches can achieve significant performance improvement on many downstream tasks. Typically when adapting these language models to downstream tasks, like a classification or regression task, we employ a... | Belinda Zeng, Iman Keivanloo, Priyanka Nigam, Sunny Rajagopalan, Trishul Chilimbi, Weiyi Lu, Xiaodi Sun, Yi Xu |  |
| 593 |  |  [A Hybrid Approach to Cross-lingual Product Review Summarization](https://doi.org/10.18653/v1/2022.emnlp-industry.3) |  | 0 | We present a hybrid approach for product review summarization which consists of: (i) an unsupervised extractive step to extract the most important sentences out of all the reviews, and (ii) a supervised abstractive step to summarize the extracted sentences into a coherent short summary. This... | Ke Tran, Saleh Soltan, Victor Soto, Wael Hamza |  |
| 594 |  |  [Augmenting Operations Research with Auto-Formulation of Optimization Models From Problem Descriptions](https://doi.org/10.18653/v1/2022.emnlp-industry.4) |  | 0 | We describe an augmented intelligence system for simplifying and enhancing the modeling experience for operations research. Using this system, the user receives a suggested formulation of an optimization problem based on its description. To facilitate this process, we build an intuitive user... | Amin BanitalebiDehkordi, Haley Li, Rindra Ramamonjison, Shiqi He, Timothy T. L. Yu, Vishnu Rengan, Yong Zhang, Zirui Zhou |  |
| 595 |  |  [Knowledge Distillation based Contextual Relevance Matching for E-commerce Product Search](https://doi.org/10.18653/v1/2022.emnlp-industry.5) |  | 0 | Online relevance matching is an essential task of e-commerce product search to boost the utility of search engines and ensure a smooth user experience. Previous work adopts either classical relevance matching models or Transformer-style models to address it. However, they ignore the inherent... | Chaokun Wang, Hao Feng, Lingfei Wu, Liqun Yang, Ziyang Liu |  |
| 596 |  |  [Accelerating the Discovery of Semantic Associations from Medical Literature: Mining Relations Between Diseases and Symptoms](https://doi.org/10.18653/v1/2022.emnlp-industry.6) |  | 0 | Medical literature is a vast and constantly expanding source of information about diseases, their diagnoses and treatments. One of the ways to extract insights from this type of data is through mining association rules between such entities. However, existing solutions do not take into account the... | Alberto Purpura, Francesca Bonin, Joao H. BettencourtSilva |  |
| 597 |  |  [PENTATRON: PErsonalized coNText-Aware Transformer for Retrieval-based cOnversational uNderstanding](https://doi.org/10.18653/v1/2022.emnlp-industry.7) |  | 0 | Conversational understanding is an integral part of modern intelligent devices. In a large fraction of the global traffic from customers using smart digital assistants, frictions in dialogues may be attributed to incorrect understanding of the entities in a customer’s query due to factors including... | Ankit Ankit, Chenlei Guo, Jie Hao, Sungjin Lee, UmaNaresh Niranjan, Xing Fan, Ziyan Jiang |  |
| 598 |  |  [Machine translation impact in E-commerce multilingual search](https://doi.org/10.18653/v1/2022.emnlp-industry.8) |  | 0 | Previous work suggests that performance of cross-lingual information retrieval correlates highly with the quality of Machine Translation. However, there may be a threshold beyond which improving query translation quality yields little or no benefit to further improve the retrieval performance. This... | Amita Misra, Bryan Zhang |  |
| 599 |  |  [Ask-and-Verify: Span Candidate Generation and Verification for Attribute Value Extraction](https://doi.org/10.18653/v1/2022.emnlp-industry.9) |  | 0 | The product attribute value extraction (AVE) task aims to capture key factual information from product profiles, and is useful for several downstream applications in e-Commerce platforms. Previous contributions usually formulate this task using sequence labeling or reading comprehension... | Christan Grant, Nasser Zalmout, Tim Weninger, Xian Li, Yan Liang, Yifan Ding |  |
| 600 |  |  [Consultation Checklists: Standardising the Human Evaluation of Medical Note Generation](https://doi.org/10.18653/v1/2022.emnlp-industry.10) |  | 0 | Evaluating automatically generated text is generally hard due to the inherently subjective nature of many aspects of the output quality. This difficulty is compounded in automatic consultation note generation by differing opinions between medical experts both about which patient statements should... | Aleksandar Savkov, Alex PapadopoulosKorfiatis, Anya Belz, Ehud Reiter, Francesco Moramarco, Mark Perera |  |
| 601 |  |  [Towards Need-Based Spoken Language Understanding Model Updates: What Have We Learned?](https://doi.org/10.18653/v1/2022.emnlp-industry.11) |  | 0 | In productionized machine learning systems, online model performance is known to deteriorate over time when there is a distributional drift between offline training and online application data. As a remedy, models are typically retrained at fixed time intervals, implying high computational and... | Daniil Sorokin, Judith Gaspers, Patrick Lehnen, Quynh Do |  |
| 602 |  |  [Knowledge Distillation Transfer Sets and their Impact on Downstream NLU Tasks](https://doi.org/10.18653/v1/2022.emnlp-industry.12) |  | 0 | Teacher-student knowledge distillation is a popular technique for compressing today’s prevailing large language models into manageable sizes that fit low-latency downstream applications. Both the teacher and the choice of transfer set used for distillation are crucial ingredients in creating a high... | Charith Peris, Gokmen Oz, Lizhen Tan, Pan Wei, Thomas Gueudré, Turan Gojayev |  |
| 603 |  |  [Exploiting In-Domain Bilingual Corpora for Zero-Shot Transfer Learning in NLU of Intra-Sentential Code-Switching Chatbot Interactions](https://doi.org/10.18653/v1/2022.emnlp-industry.13) |  | 0 | Code-switching (CS) is a very common phenomenon in regions with various co-existing languages. Since CS is such a frequent habit in informal communications, both spoken and written, it also arises naturally in Human-Machine Interactions. Therefore, in order for natural language understanding (NLU)... | Arantza del Pozo, Ariane Méndez, Jacobo LopezFernandez, Laura GarcíaSardiña, Maia Aguirre, Manex Serras |  |
| 604 |  |  [Calibrating Imbalanced Classifiers with Focal Loss: An Empirical Study](https://doi.org/10.18653/v1/2022.emnlp-industry.14) |  | 0 | Imbalanced data distribution is a practical and common challenge in building production-level machine learning (ML) models in industry, where data usually exhibits long-tail distributions. For instance, in virtual AI Assistants, such as Google Assistant, Amazon Alexa and Apple Siri, the “play... | Cheng Wang, György Szarvas, Jorge Balazs, Lahari Poddar, Patrick Ernst, Pavel Danchenko |  |
| 605 |  |  [Unsupervised training data re-weighting for natural language understanding with local distribution approximation](https://doi.org/10.18653/v1/2022.emnlp-industry.15) |  | 0 | One of the major challenges of training Natural Language Understanding (NLU) production models lies in the discrepancy between the distributions of the offline training data and of the online live data, due to, e.g., biased sampling scheme, cyclic seasonality shifts, annotated training data coming... | Bei Chen, DieuThu Le, Jose Garrido Ramas, Kay Rottmann, Manoj Kumar |  |
| 606 |  |  [Cross-Encoder Data Annotation for Bi-Encoder Based Product Matching](https://doi.org/10.18653/v1/2022.emnlp-industry.16) |  | 0 | Matching a seller listed item to an appropriate product is an important step for an e-commerce platform. With the recent advancement in deep learning, there are different encoder based approaches being proposed as solution. When textual data for two products are available, cross-encoder approaches... | Justin Chiu, Keiji Shinzato |  |
| 607 |  |  [Deploying a Retrieval based Response Model for Task Oriented Dialogues](https://doi.org/10.18653/v1/2022.emnlp-industry.17) |  | 0 | Task-oriented dialogue systems in industry settings need to have high conversational capability, be easily adaptable to changing situations and conform to business constraints. This paper describes a 3-step procedure to develop a conversational model that satisfies these criteria and can... | Cheng Wang, György Szarvas, Jorge Balazs, Lahari Poddar, Patrick Ernst, Pavel Danchenko |  |
| 608 |  |  [Tackling Temporal Questions in Natural Language Interface to Databases](https://doi.org/10.18653/v1/2022.emnlp-industry.18) |  | 0 | Temporal aspect is one of the most challenging areas in Natural Language Interface to Databases (NLIDB). This paper addresses and examines how temporal questions being studied and supported by the research community at both levels: popular annotated dataset (e.g. Spider) and recent advanced models.... | Irene Manotas, Ngoc Phuoc An Vo, Octavian Popescu, Vadim Sheinin |  |
| 609 |  |  [Multi-Tenant Optimization For Few-Shot Task-Oriented FAQ Retrieval](https://doi.org/10.18653/v1/2022.emnlp-industry.19) |  | 0 | Business-specific Frequently Asked Questions (FAQ) retrieval in task-oriented dialog systems poses unique challenges vis à vis community based FAQs. Each FAQ question represents an intent which is usually an umbrella term for many related user queries. We evaluate performance for such Business FAQs... | Asha Vishwanathan, Chandra Shekhar Kandpal, Gautham Vadakkekara Suresh, Rajeev Unnikrishnan Warrier |  |
| 610 |  |  [Iterative Stratified Testing and Measurement for Automated Model Updates](https://doi.org/10.18653/v1/2022.emnlp-industry.20) |  | 0 | Automating updates to machine learning systems is an important but understudied challenge in AutoML. The high model variance of many cutting-edge deep learning architectures means that retraining a model provides no guarantee of accurate inference on all sample types. To address this concern, we... | Elizabeth Dekeyser, Fengtao Wu, Kanna Shimizu, Lisa A. Haverty, Nicholas Comment, Rajat Kumar, Shermin Pei, Shruti Rai |  |
| 611 |  |  [SLATE: A Sequence Labeling Approach for Task Extraction from Free-form Inked Content](https://doi.org/10.18653/v1/2022.emnlp-industry.21) |  | 0 | We present SLATE, a sequence labeling approach for extracting tasks from free-form content such as digitally handwritten (or “inked”) notes on a virtual whiteboard. Our approach allows us to create a single, low-latency model to simultaneously perform sentence segmentation and classification of... | Apurva Gandhi, Biyi Fang, Ehi Nosakhare, Gilbert Antonius, Irene Shaffer, Jenna Hong, Ryan Serrao, Sheng Yi, Soundararajan Srinivasan, Tra My Nguyen |  |
| 612 |  |  [Gaining Insights into Unrecognized User Utterances in Task-Oriented Dialog Systems](https://doi.org/10.18653/v1/2022.emnlp-industry.22) |  | 0 | The rapidly growing market demand for automatic dialogue agents capable of goal-oriented behavior has caused many tech-industry leaders to invest considerable efforts into task-oriented dialog systems. The success of these systems is highly dependent on the accuracy of their intent identification –... | Ateret AnabyTavor, David Boaz, Ella Rabinovich, Gaurav Pandey, Matan Vetzler, Vineet Kumar |  |
| 613 |  |  [CoCoID: Learning Contrastive Representations and Compact Clusters for Semi-Supervised Intent Discovery](https://doi.org/10.18653/v1/2022.emnlp-industry.23) |  | 0 | Intent discovery is to mine new intents from user utterances, which are not present in the set of manually predefined intents. Previous approaches to intent discovery usually automatically cluster novel intents with prior knowledge from intent-labeled data in a semi-supervised way. In this paper,... | Deyi Xiong, Qian Cao, Qinlong Wang, Xia Peng |  |
| 614 |  |  [Tractable & Coherent Multi-Document Summarization: Discrete Optimization of Multiple Neural Modeling Streams via Integer Linear Programming](https://doi.org/10.18653/v1/2022.emnlp-industry.24) |  | 0 | One key challenge in multi-document summarization is the generated summary is often less coherent compared to single document summarization due to the larger heterogeneity of the input source content. In this work, we propose a generic framework to jointly consider coherence and informativeness in... | Litton J. Kurisinkel, Nancy Chen |  |
| 615 |  |  [Grafting Pre-trained Models for Multimodal Headline Generation](https://doi.org/10.18653/v1/2022.emnlp-industry.25) |  | 0 | Multimodal headline utilizes both video frames and transcripts to generate the natural language title of the videos. Due to a lack of large-scale, manually annotated data, the task of annotating grounded headlines for video is labor intensive and impractical. Previous researches on pre-trained... | Bo Ren, Chen Wu, Di Yin, Haoyuan Peng, Lingfeng Qiao, Ye Liu |  |
| 616 |  |  [Semi-supervised Adversarial Text Generation based on Seq2Seq models](https://doi.org/10.18653/v1/2022.emnlp-industry.26) |  | 0 | To improve deep learning models’ robustness, adversarial training has been frequently used in computer vision with satisfying results. However, adversarial perturbation on text have turned out to be more challenging due to the discrete nature of text. The generated adversarial text might not sound... | Chris Church, DieuThu Le, Hieu Le, Kay Rottmann, Melanie Bradford, Peter Chin, Verena Weber |  |
| 617 |  |  [Is it out yet? Automatic Future Product Releases Extraction from Web Data](https://doi.org/10.18653/v1/2022.emnlp-industry.27) |  | 0 | Identifying the release of new products and their predicted demand in advance is highly valuable for E-Commerce marketplaces and retailers. The information of an upcoming product release is used for inventory management, marketing campaigns and pre-order suggestions. Often, the announcement of an... | Gilad Fuchs, Ido BenShaul, Matan Mandelbrod |  |
| 618 |  |  [Automatic Scene-based Topic Channel Construction System for E-Commerce](https://doi.org/10.18653/v1/2022.emnlp-industry.28) |  | 0 | Scene marketing that well demonstrates user interests within a certain scenario has proved effective for offline shopping. To conduct scene marketing for e-commerce platforms, this work presents a novel product form, scene-based topic channel which typically consists of a list of diverse products... | Bo Long, Lingfei Wu, Mian Ma, Peng Lin, Yanyan Zou, Zhuoye Ding |  |
| 619 |  |  [SpeechNet: Weakly Supervised, End-to-End Speech Recognition at Industrial Scale](https://doi.org/10.18653/v1/2022.emnlp-industry.29) |  | 0 | End-to-end automatic speech recognition systems represent the state of the art, but they rely on thousands of hours of manually annotated speech for training, as well as heavyweight computation for inference. Of course, this impedes commercialization since most companies lack vast human and... | Akshat Pandey, Ferhan Ture, G. Craig Murray, Gefei Yang, Jimmy Lin, Karun Kumar, Madhuri Emmadi, Raphael Tang, Vladislav Belyaev, Yajie Mao |  |
| 620 |  |  [Controlled Language Generation for Language Learning Items](https://doi.org/10.18653/v1/2022.emnlp-industry.30) |  | 0 | This work aims to employ natural language generation (NLG) to rapidly generate items for English language learning applications: this requires both language models capable of generating fluent, high-quality English, and to control the output of the generation to match the requirements of the... | Debanjan Ghosh, Kevin Stowe, Mengxuan Zhao |  |
| 621 |  |  [Improving Text-to-SQL Semantic Parsing with Fine-grained Query Understanding](https://doi.org/10.18653/v1/2022.emnlp-industry.31) |  | 0 | Most recent research on Text-to-SQL semantic parsing relies on either parser itself or simple heuristic based approach to understand natural language query (NLQ). When synthesizing a SQL query, there is no explicit semantic information of NLQ available to the parser which leads to undesirable... | Alexander Hanbo Li, Bing Xiang, Jiarong Jiang, Jun Wang, Patrick Ng, Ramesh Nallapati, Sudipta Sengupta, Zhiguo Wang |  |
| 622 |  |  [Unsupervised Dense Retrieval for Scientific Articles](https://doi.org/10.18653/v1/2022.emnlp-industry.32) |  | 0 | In this work, we build a dense retrieval based semantic search engine on scientific articles from Elsevier. The major challenge is that there is no labeled data for training and testing. We apply a state-of-the-art unsupervised dense retrieval model called Generative Pseudo Labeling that generates... | Dan Li, George Tsatsaronis, Vikrant Yadav, Zubair Afzal |  |
| 623 |  |  [Learning Geolocations for Cold-Start and Hard-to-Resolve Addresses via Deep Metric Learning](https://doi.org/10.18653/v1/2022.emnlp-industry.33) |  | 0 | With evergrowing digital adoption in the society and increasing demand for businesses to deliver to customers doorstep, the last mile hop of transportation planning poses unique challenges in emerging geographies with unstructured addresses. One of the crucial inputs to facilitate effective... | Govind, Saurabh Sohoney |  |
| 624 |  |  [Meta-learning Pathologies from Radiology Reports using Variance Aware Prototypical Networks](https://doi.org/10.18653/v1/2022.emnlp-industry.34) |  | 0 | Large pretrained Transformer-based language models like BERT and GPT have changed the landscape of Natural Language Processing (NLP). However, fine tuning such models still requires a large number of training examples for each target task, thus annotating multiple datasets and training these models... | Anasuya Das, Arijit Sehanobish, Benjamin Odry, Kawshik Kannan, Nabila Abraham |  |
| 625 |  |  [Named Entity Recognition in Industrial Tables using Tabular Language Models](https://doi.org/10.18653/v1/2022.emnlp-industry.35) |  | 0 | Specialized transformer-based models for encoding tabular data have gained interest in academia. Although tabular data is omnipresent in industry, applications of table transformers are still missing. In this paper, we study how these models can be applied to an industrial Named Entity Recognition... | Aneta Koleva, Mark Buckley, Martin Ringsquandl, Rakebul Hasan, Volker Tresp |  |
| 626 |  |  [Reinforced Question Rewriting for Conversational Question Answering](https://doi.org/10.18653/v1/2022.emnlp-industry.36) |  | 0 | Conversational Question Answering (CQA) aims to answer questions contained within dialogues, which are not easily interpretable without context. Developing a model to rewrite conversational questions into self-contained ones is an emerging solution in industry settings as it allows using existing... | Anjie Fang, Besnik Fetahu, Jie Zhao, Oleg Rokhlenko, Shervin Malmasi, Zhiyu Chen |  |
| 627 |  |  [Improving Large-Scale Conversational Assistants using Model Interpretation based Training Sample Selection](https://doi.org/10.18653/v1/2022.emnlp-industry.37) |  | 0 | This paper presents an approach to identify samples from live traffic where the customer implicitly communicated satisfaction with Alexa’s responses, by leveraging interpretations of model behavior. Such customer signals are noisy and adding a large number of samples from live traffic to training... | Anil Ramakrishna, Kiana Hajebi, Manoj Kumar, Morteza Ziyadi, Pradeep Natarajan, Rahul Gupta, Sriram Venkatapathy, Stefan Schroedl |  |
| 628 |  |  [Improving Precancerous Case Characterization via Transformer-based Ensemble Learning](https://doi.org/10.18653/v1/2022.emnlp-industry.38) |  | 0 | The application of natural language processing (NLP) to cancer pathology reports has been focused on detecting cancer cases, largely ignoring precancerous cases. Improving the characterization of precancerous adenomas assists in developing diagnostic tests for early cancer detection and prevention,... | Ellen Loo, Jiajie Xiao, Jimmy Lin, Mahan Matin, Ofer Shapira, Richard Bourgon, Thomas Vetterli, Yizhen Zhong |  |
| 629 |  |  [Developing Prefix-Tuning Models for Hierarchical Text Classification](https://doi.org/10.18653/v1/2022.emnlp-industry.39) |  | 0 | Hierarchical text classification (HTC) is a key problem and task in many industrial applications, which aims to predict labels organized in a hierarchy for given input text. For example, HTC can group the descriptions of online products into a taxonomy or organizing customer reviews into a... | Hou Wei Chou, Lei Chen, Xiaodan Zhu |  |
| 630 |  |  [PAIGE: Personalized Adaptive Interactions Graph Encoder for Query Rewriting in Dialogue Systems](https://doi.org/10.18653/v1/2022.emnlp-industry.40) |  | 0 | Unexpected responses or repeated clarification questions from conversational agents detract from the users’ experience with technology meant to streamline their daily tasks. To reduce these frictions, Query Rewriting (QR) techniques replace transcripts of faulty queries with alternatives that lead... | Chenlei Guo, Daniel Bis, Jie Hao, Saurabh Gupta, Xing Fan |  |
| 631 |  |  [Fast Vocabulary Transfer for Language Model Compression](https://doi.org/10.18653/v1/2022.emnlp-industry.41) |  | 0 | Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer... | Andrea Zugarini, Leonardo Rigutini, Leonidas Gee, Paolo Torroni |  |
| 632 |  |  [Multimodal Context Carryover](https://doi.org/10.18653/v1/2022.emnlp-industry.42) |  | 0 | Multi-modality support has become an integral part of creating a seamless user experience with modern voice assistants with smart displays. Users refer to images, video thumbnails, or the accompanying text descriptions on the screen through voice communication with AI powered devices. This raises... | Chengwei Su, Emre Barut, Fan Yang, Kechen Qin, Nalin Gupta, Prashan Wanigasekara, Spurthi Sandiri, Stephen Rawls, Xinyue Liu, Zeynab Raeesy |  |
| 633 |  |  [Distilling Multilingual Transformers into CNNs for Scalable Intent Classification](https://doi.org/10.18653/v1/2022.emnlp-industry.43) |  | 0 | We describe an application of Knowledge Distillation used to distill and deploy multilingual Transformer models for voice assistants, enabling text classification for customers globally. Transformers have set new state-of-the-art results for tasks like intent classification, and multilingual models... | Akash Veeragouni, Besnik Fetahu, Oleg Rokhlenko, Shervin Malmasi |  |
| 634 |  |  [Bringing the State-of-the-Art to Customers: A Neural Agent Assistant Framework for Customer Service Support](https://doi.org/10.18653/v1/2022.emnlp-industry.44) |  | 0 | Building Agent Assistants that can help improve customer service support requires inputs from industry users and their customers, as well as knowledge about state-of-the-art Natural Language Processing (NLP) technology. We combine expertise from academia and industry to bridge the gap and build... | Alif Munim, Elaine Lau, Faiza Khan Khattak, Jingcheng Niu, Karthik Raja K. Bhaskar, Sean Robertson, Shirley Wang, Stephen Obadinma, Tania Sidhorn, Winnie Au |  |
| 635 |  |  [Zero-Shot Dynamic Quantization for Transformer Inference](https://doi.org/10.18653/v1/2022.emnlp-industry.45) |  | 0 | We introduce a novel run-time method for significantly reducing the accuracy loss associated with quantizing BERT-like models to 8-bit integers. Existing methods for quantizing models either modify the training procedure, or they require an additional calibration step to adjust parameters that also... | Avi Sil, Jerry Quinn, Yousef ElKurdi |  |
| 636 |  |  [Fact Checking Machine Generated Text with Dependency Trees](https://doi.org/10.18653/v1/2022.emnlp-industry.46) |  | 0 | Factual and logical errors made by Natural Language Generation (NLG) systems limit their applicability in many settings. We study this problem in a conversational search and recommendation setting, and observe that we can often make two simplifying assumptions in this domain: (i) there exists a... | Alex Estes, Marcus D. Collins, Matt Cecil, Nikhita Vedula, Oleg Rokhlenko |  |
| 637 |  |  [Prototype-Representations for Training Data Filtering in Weakly-Supervised Information Extraction](https://doi.org/10.18653/v1/2022.emnlp-industry.47) |  | 0 | The availability of high quality training data is still a bottleneck for the practical utilization of information extraction models, despite the breakthroughs in zero and few-shot learning techniques. This is further exacerbated for industry applications, where new tasks, domains, and specific use... | Nasser Zalmout, Xian Li |  |
| 638 |  |  [CGF: Constrained Generation Framework for Query Rewriting in Conversational AI](https://doi.org/10.18653/v1/2022.emnlp-industry.48) |  | 0 | In conversational AI agents, Query Rewriting (QR) plays a crucial role in reducing user frictions and satisfying their daily demands. User frictions are caused by various reasons, such as errors in the conversational AI system, users’ accent or their abridged language. In this work, we present a... | Chenlei Guo, Gökhan Tür, Jie Hao, Pradeep Natarajan, Rakesh Chada, Saleh Soltan, Saurabh Gupta, Xing Fan, Yang Liu |  |
| 639 |  |  [Entity-level Sentiment Analysis in Contact Center Telephone Conversations](https://doi.org/10.18653/v1/2022.emnlp-industry.49) |  | 0 | Entity-level sentiment analysis predicts the sentiment about entities mentioned in a given text. It is very useful in a business context to understand user emotions towards certain entities, such as products or companies. In this paper, we demonstrate how we developed an entity-level sentiment... | Cheng Chen, Md. Tahmid Rahman Laskar, Pooja Hiranandani, Shashi Bhushan TN, Shayna Gardiner, XueYong Fu |  |
| 640 |  |  [QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation](https://doi.org/10.18653/v1/2022.emnlp-industry.50) |  | 0 | Large Language Models (LLMs) have shown impressive results on a variety of text understanding tasks. Search queries though pose a unique challenge, given their short-length and lack of nuance or context. Complicated feature engineering efforts do not always lead to downstream improvements as their... | Anupam Samanta, Karthik Raman, Krishna Srinivasan, Lingrui Liao, Luca Bertelli, Michael Bendersky |  |
| 641 |  |  [Distinguish Sense from Nonsense: Out-of-Scope Detection for Virtual Assistants](https://doi.org/10.18653/v1/2022.emnlp-industry.51) |  | 0 | Out of Scope (OOS) detection in Conversational AI solutions enables a chatbot to handle a conversation gracefully when it is unable to make sense of the end-user query. Accurately tagging a query as out-of-domain is particularly hard in scenarios when the chatbot is not equipped to handle a topic... | Cheng Qian, Gengyu Wang, Haode Qi, Ladislav Kunc, Saloni Potdar |  |
| 642 |  |  [PLATO-Ad: A Unified Advertisement Text Generation Framework with Multi-Task Prompt Learning](https://doi.org/10.18653/v1/2022.emnlp-industry.52) |  | 0 | Online advertisement text generation aims at generating attractive and persuasive text ads to appeal to users clicking ads or purchasing products. While pretraining-based models have achieved remarkable success in generating high-quality text ads, some challenges still remain, such as ad generation... | Chao Zhang, Haifeng Wang, Hua Wu, Shuanglong Li, Wenquan Wu, Xinchao Xu, Yi Yang, Zeyang Lei, ZhengYu Niu |  |
| 643 |  |  [Dense Feature Memory Augmented Transformers for COVID-19 Vaccination Search Classification](https://doi.org/10.18653/v1/2022.emnlp-industry.53) |  | 0 | With the devastating outbreak of COVID-19, vaccines are one of the crucial lines of defense against mass infection in this global pandemic. Given the protection they provide, vaccines are becoming mandatory in certain social and professional settings. This paper presents a classification model for... | Chaitanya Kamath, Donald Metzler, Evgeniy Gabrilovich, Jai Gupta, Mimi Sun, Shailesh Bavadekar, Vinh Tran, Yi Tay |  |
| 644 |  |  [Full-Stack Information Extraction System for Cybersecurity Intelligence](https://doi.org/10.18653/v1/2022.emnlp-industry.54) |  | 0 | Due to rapidly growing cyber-attacks and security vulnerabilities, many reports on cyber-threat intelligence (CTI) are being published daily. While these reports can help security analysts to understand on-going cyber threats,the overwhelming amount of information makes it difficult to digest the... | Taesung Lee, Youngja Park |  |
| 645 |  |  [Deploying Unified BERT Moderation Model for E-Commerce Reviews](https://doi.org/10.18653/v1/2022.emnlp-industry.55) |  | 0 | Moderation of user-generated e-commerce content has become crucial due to the large and diverse user base on the platforms. Product reviews and ratings have become an integral part of the shopping experience to build trust among users. Due to the high volume of reviews generated on a vast catalog... | Nikesh Garera, Ravindra Nayak |  |
| 646 |  |  [SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval](https://doi.org/10.18653/v1/2022.emnlp-industry.56) |  | 0 | Sampling proper negatives from a large document pool is vital to effectively train a dense retrieval model. However, existing negative sampling strategies suffer from the uninformative or false negative problem. In this work, we empirically show that according to the measured relevance scores, the... | Anlei Dong, JiRong Wen, Jingwen Lu, Kun Zhou, Nan Duan, Rangan Majumder, Wayne Xin Zhao, Xiao Liu, Yelong Shen, Yeyun Gong |  |
| 647 |  |  [Revisiting and Advancing Chinese Natural Language Understanding with Accelerated Heterogeneous Knowledge Pre-training](https://doi.org/10.18653/v1/2022.emnlp-industry.57) |  | 0 | Recently, knowledge-enhanced pre-trained language models (KEPLMs) improve context-aware representations via learning from structured relations in knowledge bases, and/or linguistic knowledge from syntactic or dependency analysis. Unlike English, there is a lack of high-performing open-source... | Ang Wang, Chengyu Wang, Jianing Wang, Jun Huang, Junwei Dong, Taolin Zhang, Xiaofeng He, Yinghui Liu, Yong Li |  |
| 648 |  |  [A Stacking-based Efficient Method for Toxic Language Detection on Live Streaming Chat](https://doi.org/10.18653/v1/2022.emnlp-industry.58) |  | 0 | In a live streaming chat on a video streaming service, it is crucial to filter out toxic comments with online processing to prevent users from reading comments in real-time. However, recent toxic language detection methods rely on deep learning methods, which can not be scalable considering... | Koji Murakami, Yuki Nakayama, Yuto Oikawa |  |
| 649 |  |  [End-to-End Speech to Intent Prediction to improve E-commerce Customer Support Voicebot in Hindi and English](https://doi.org/10.18653/v1/2022.emnlp-industry.59) |  | 0 | Automation of on-call customer support relies heavily on accurate and efficient speech-to-intent (S2I) systems. Building such systems using multi-component pipelines can pose various challenges because they require large annotated datasets, have higher latency, and have complex deployment. These... | Abhinav Goyal, Anupam Singh, Nikesh Garera |  |
| 650 |  |  [PILE: Pairwise Iterative Logits Ensemble for Multi-Teacher Labeled Distillation](https://doi.org/10.18653/v1/2022.emnlp-industry.60) |  | 0 | Pre-trained language models have become a crucial part of ranking systems and achieved very impressive effects recently. To maintain high performance while keeping efficient computations, knowledge distillation is widely used. In this paper, we focus on two key questions in knowledge distillation... | Daiting Shi, Dawei Yin, Dehong Ma, Jun Fan, Lianshang Cai, Linhao Zhang, Simiu Gu, Yi Wu, Zhicong Cheng |  |
| 651 |  |  [A Comprehensive Evaluation of Biomedical Entity-centric Search](https://doi.org/10.18653/v1/2022.emnlp-industry.61) |  | 0 | Biomedical information retrieval has often been studied as a task of detecting whether a system correctly detects entity spans and links these entities to concepts from a given terminology. Most academic research has focused on evaluation of named entity recognition (NER) and entity linking (EL)... | Anastasia Shneyderman, Elena Tutubalina, Vladimir Muravlev, Zulfat Miftahutdinov |  |
| 652 |  |  [Domain Adaptation of Machine Translation with Crowdworkers](https://doi.org/10.18653/v1/2022.emnlp-industry.62) |  | 0 | Although a machine translation model trained with a large in-domain parallel corpus achieves remarkable results, it still works poorly when no in-domain data are available. This situation restricts the applicability of machine translation when the target domain’s data are limited. However, there is... | Jun Suzuki, Makoto Morishita, Masaaki Nagata |  |
| 653 |  |  [Biomedical NER for the Enterprise with Distillated BERN2 and the Kazu Framework](https://doi.org/10.18653/v1/2022.emnlp-industry.63) |  | 0 | In order to assist the drug discovery/development process, pharmaceutical companies often apply biomedical NER and linking techniques over internal and public corpora. Decades of study of the field of BioNLP has produced a plethora of algorithms, systems and datasets. However, our experience has... | Elliot Ford, Jaewoo Kang, Richard Jackson, Vladimir Poroshin, Wonjin Yoon |  |
| 654 |  |  [Large-scale Machine Translation for Indian Languages in E-commerce under Low Resource Constraints](https://doi.org/10.18653/v1/2022.emnlp-industry.64) |  | 0 | The democratization of e-commerce platforms has moved an increasingly diversified Indian user base to shop online. We have deployed reliable and precise large-scale Machine Translation systems for several Indian regional languages in this work. Building such systems is a challenge because of the... | Amey Patil, Nikesh Garera |  |
| 655 |  |  [Topic Modeling by Clustering Language Model Embeddings: Human Validation on an Industry Dataset](https://doi.org/10.18653/v1/2022.emnlp-industry.65) |  | 0 | Topic models are powerful tools to get an overview of large collections of text data, a situation that is prevalent in industry applications. A rising trend within topic modeling is to directly cluster dimension-reduced embeddings created with pretrained language models. It is difficult to evaluate... | Anton Eklund, Mona Forsman |  |
| 656 |  |  [Generative Knowledge Graph Construction: A Review](https://doi.org/10.18653/v1/2022.emnlp-main.1) |  | 0 | Generative Knowledge Graph Construction (KGC) refers to those methods that leverage the sequence-to-sequence framework for building knowledge graphs, which is flexible and can be adapted to widespread tasks. In this study, we summarize the recent compelling progress in generative knowledge graph... | Hongbin Ye, Huajun Chen, Hui Chen, Ningyu Zhang |  |
| 657 |  |  [CDConv: A Benchmark for Contradiction Detection in Chinese Conversations](https://doi.org/10.18653/v1/2022.emnlp-main.2) |  | 0 | Dialogue contradiction is a critical issue in open-domain dialogue systems. The contextualization nature of conversations makes dialogue contradiction detection rather challenging. In this work, we propose a benchmark for Contradiction Detection in Chinese Conversations, namely CDConv. It contains... | Chujie Zheng, Hua Wu, Jinfeng Zhou, Libiao Peng, Minlie Huang, Wenquan Wu, Yinhe Zheng, Zhen Guo, ZhengYu Niu |  |
| 658 |  |  [Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space](https://doi.org/10.18653/v1/2022.emnlp-main.3) |  | 0 | Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the... | Avi Caciularu, Kevin Ro Wang, Mor Geva, Yoav Goldberg |  |
| 659 |  |  [Learning to Generate Question by Asking Question: A Primal-Dual Approach with Uncommon Word Generation](https://doi.org/10.18653/v1/2022.emnlp-main.4) |  | 0 | Automatic question generation (AQG) is the task of generating a question from a given passage and an answer. Most existing AQG methods aim at encoding the passage and the answer to generate the question. However, limited work has focused on modeling the correlation between the target answer and the... | Dongfang Liu, Fuli Feng, Hao Ma, Li Yang, Qifan Wang, Sinong Wang, Xiaojun Quan, Zenglin Xu |  |
| 660 |  |  [Graph-based Model Generation for Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.5) |  | 0 | Few-shot relation extraction (FSRE) has been a challenging problem since it only has a handful of training instances. Existing models follow a ‘one-for-all’ scheme where one general large model performs all individual N-way-K-shot tasks in FSRE, which prevents the model from achieving the optimal... | Tieyun Qian, Wanli Li |  |
| 661 |  |  [Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling](https://doi.org/10.18653/v1/2022.emnlp-main.6) |  | 0 | Recent advances in federated learning have demonstrated its promising capability to learn on decentralized datasets. However, a considerable amount of work has raised concerns due to the potential risks of adversaries participating in the framework to poison the global model for an adversarial... | KiYoon Yoo, Nojun Kwak |  |
| 662 |  |  [Generating Natural Language Proofs with Verifier-Guided Search](https://doi.org/10.18653/v1/2022.emnlp-main.7) |  | 0 | Reasoning over natural language is a challenging problem in NLP. In this work, we focus on proof generation: Given a hypothesis and a set of supporting facts, the model generates a proof tree indicating how to derive the hypothesis from supporting facts. Compared to generating the entire proof in... | Danqi Chen, Jia Deng, Kaiyu Yang |  |
| 663 |  |  [Toward Unifying Text Segmentation and Long Document Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.8) |  | 0 | Text segmentation is important for signaling a document’s structure. Without segmenting a long document into topically coherent sections, it is difficult for readers to comprehend the text, let alone find important information. The problem is only exacerbated by a lack of segmentation in... | Dong Yu, Fei Liu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang |  |
| 664 |  |  [The Geometry of Multilingual Language Model Representations](https://doi.org/10.18653/v1/2022.emnlp-main.9) |  | 0 | We assess how multilingual language models maintain a shared multilingual representation space while still encoding language-sensitive information in each language. Using XLM-R as a case study, we show that languages occupy similar linear subspaces after mean-centering, evaluated based on causal... | Benjamin K. Bergen, Tyler A. Chang, Zhuowen Tu |  |
| 665 |  |  [Improving Complex Knowledge Base Question Answering via Question-to-Action and Question-to-Question Alignment](https://doi.org/10.18653/v1/2022.emnlp-main.10) |  | 0 | Complex knowledge base question answering can be achieved by converting questions into sequences of predefined actions. However, there is a significant semantic and structural gap between natural language and action sequences, which makes this conversion difficult. In this paper, we introduce an... | Weiming Lu, Xiaoxia Cheng, Yechun Tang |  |
| 666 |  |  [PAIR: Prompt-Aware margIn Ranking for Counselor Reflection Scoring in Motivational Interviewing](https://doi.org/10.18653/v1/2022.emnlp-main.11) |  | 0 | Counselor reflection is a core verbal skill used by mental health counselors to express understanding and affirmation of the client’s experience and concerns. In this paper, we propose a system for the analysis of counselor reflections. Specifically, our system takes as input one dialog turn... | Do June Min, Kenneth Resnicow, Rada Mihalcea, Verónica PérezRosas |  |
| 667 |  |  [Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.12) |  | 0 | Recent graph-based models for joint multiple intent detection and slot filling have obtained promising results through modeling the guidance from the prediction of intents to the decoding of slot filling.However, existing methods (1) only model the unidirectional guidance from intent to slot; (2)... | Bowen Xing, Ivor W. Tsang |  |
| 668 |  |  [The Importance of Being Parameters: An Intra-Distillation Method for Serious Gains](https://doi.org/10.18653/v1/2022.emnlp-main.13) |  | 0 | Recent model pruning methods have demonstrated the ability to remove redundant parameters without sacrificing model performance. Common methods remove redundant parameters according to the parameter sensitivity, a gradient-based measure reflecting the contribution of the parameters. In this paper,... | Haoran Xu, Kenton Murray, Philipp Koehn |  |
| 669 |  |  [Interpreting Language Models with Contrastive Explanations](https://doi.org/10.18653/v1/2022.emnlp-main.14) |  | 0 | Model interpretability methods are often used to explain NLP model decisions on tasks such as text classification, where the output space is relatively small. However, when applied to language generation, where the output space often consists of tens of thousands of tokens, these methods are unable... | Graham Neubig, Kayo Yin |  |
| 670 |  |  [RankGen: Improving Text Generation with Large Ranking Models](https://doi.org/10.18653/v1/2022.emnlp-main.15) |  | 0 | Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues we present RankGen, a 1.2B parameter... | John Wieting, Kalpesh Krishna, Mohit Iyyer, Yapei Chang |  |
| 671 |  |  [Learning a Grammar Inducer from Massive Uncurated Instructional Videos](https://doi.org/10.18653/v1/2022.emnlp-main.16) |  | 0 | Video-aided grammar induction aims to leverage video information for finding more accurate syntactic grammars for accompanying text. While previous work focuses on building systems for inducing grammars on text that are well-aligned with video content, we investigate the scenario, in which text and... | Dong Yu, Haitao Mi, Jiebo Luo, Kun Xu, Lifeng Jin, Linfeng Song, Songyang Zhang |  |
| 672 |  |  [Normalized Contrastive Learning for Text-Video Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.17) |  | 0 | Cross-modal contrastive learning has led the recent advances in multimodal retrieval with its simplicity and effectiveness. In this work, however, we reveal that cross-modal contrastive learning suffers from incorrect normalization of the sum retrieval probabilities of each text or video instance.... | Bo Xiong, Florian Metze, Gourab Kundu, Kirmani Ahmed, Mahmoud Azab, Seungwhan Moon, Yookoon Park |  |
| 673 |  |  [Estimating Soft Labels for Out-of-Domain Intent Detection](https://doi.org/10.18653/v1/2022.emnlp-main.18) |  | 0 | Out-of-Domain (OOD) intent detection is important for practical dialog systems. To alleviate the issue of lacking OOD training samples, some works propose synthesizing pseudo OOD samples and directly assigning one-hot OOD labels to these pseudo samples. However, these one-hot labels introduce... | Fei Huang, Hao Lang, Jian Sun, Luo Si, Yinhe Zheng, Yongbin Li |  |
| 674 |  |  [Multi-VQG: Generating Engaging Questions for Multiple Images](https://doi.org/10.18653/v1/2022.emnlp-main.19) |  | 0 | Generating engaging content has drawn much recent attention in the NLP community. Asking questions is a natural way to respond to photos and promote awareness. However, most answers to questions in traditional question-answering (QA) datasets are factoids, which reduce individuals’ willingness to... | LunWei Ku, MinHsuan Yeh, TingHao (Kenneth) Huang, Vincent Chen |  |
| 675 |  |  [Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.20) |  | 0 | The predictions of question answering (QA) systems are typically evaluated against manually annotated finite sets of one or more answers. This leads to a coverage limitation that results in underestimating the true performance of systems, and is typically addressed by extending over exact match... | Benjamin Börschinger, Christian Buck, Jannis Bulian, Tal Schuster, Wojciech Gajewski |  |
| 676 |  |  [Non-Parametric Domain Adaptation for End-to-End Speech Translation](https://doi.org/10.18653/v1/2022.emnlp-main.21) |  | 0 | The end-to-end speech translation (E2E-ST) has received increasing attention due to the potential of its less error propagation, lower latency and fewer parameters. However, the effectiveness of neural-based approaches to this task is severely limited by the available training corpus, especially... | Boxing Chen, Enhong Chen, Jun Xie, Tong Xu, Weizhi Wang, Yichao Du, Zhirui Zhang |  |
| 677 |  |  [Prompting for Multimodal Hateful Meme Classification](https://doi.org/10.18653/v1/2022.emnlp-main.22) |  | 0 | Hateful meme classification is a challenging multimodal task that requires complex reasoning and contextual background knowledge. Ideally, we could leverage an explicit external knowledge base to supplement contextual and cultural information in hateful memes. However, there is no known explicit... | Jing Jiang, Roy KaWei Lee, Rui Cao, WenHaw Chong |  |
| 678 |  |  [Certified Error Control of Candidate Set Pruning for Two-Stage Relevance Ranking](https://doi.org/10.18653/v1/2022.emnlp-main.23) |  | 0 | In information retrieval (IR), candidate set pruning has been commonly used to speed up two-stage relevance ranking. However, such an approach lacks accurate error control and often trades accuracy against computational efficiency in an empirical fashion, missing theoretical guarantees. In this... | Hongyang Zhang, Ji Xin, Jimmy Lin, Minghan Li, Xinyu Zhang |  |
| 679 |  |  [Linearizing Transformer with Key-Value Memory](https://doi.org/10.18653/v1/2022.emnlp-main.24) |  | 0 | Efficient transformer variants with linear time complexity have been developed to mitigate the quadratic computational overhead of the vanilla transformer. Among them are low-rank projection methods such as Linformer and kernel-based Transformers. Despite their unique merits, they usually suffer... | Deng Cai, Yizhe Zhang |  |
| 680 |  |  [Robustness of Fusion-based Multimodal Classifiers to Cross-Modal Content Dilutions](https://doi.org/10.18653/v1/2022.emnlp-main.25) |  | 0 | As multimodal learning finds applications in a wide variety of high-stakes societal tasks, investigating their robustness becomes important. Existing work has focused on understanding the robustness of vision-and-language models to imperceptible variations on benchmark tasks. In this work, we... | Gaurav Verma, Ryan A. Rossi, Srijan Kumar, Vishwa Vinay |  |
| 681 |  |  [Translation between Molecules and Natural Language](https://doi.org/10.18653/v1/2022.emnlp-main.26) |  | 0 | We present MolT5 - a self-supervised learning framework for pretraining models on a vast amount of unlabeled natural language text and molecule strings. MolT5 allows for new, useful, and challenging analogs of traditional vision-language tasks, such as molecule captioning and text-based de novo... | Carl Edwards, Garrett Honke, Heng Ji, Kevin Ros, Kyunghyun Cho, Tuan Manh Lai |  |
| 682 |  |  [What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment](https://doi.org/10.18653/v1/2022.emnlp-main.27) |  | 0 | The instruction learning paradigm—where a model learns to perform new tasks from task descriptions alone—has become popular in research on general-purpose models. The capabilities of large transformer models as instruction learners, however, remain poorly understood. We use a controlled synthetic... | Ashish Sabharwal, Kyle Richardson, Matthew Finlayson, Peter Clark |  |
| 683 |  |  [Sentence-Incremental Neural Coreference Resolution](https://doi.org/10.18653/v1/2022.emnlp-main.28) |  | 0 | We propose a sentence-incremental neural coreference resolution system which incrementally builds clusters after marking mention boundaries in a shift-reduce method. The system is aimed at bridging two recent approaches at coreference resolution: (1) state-of-the-art non-incremental models that... | Mark Steedman, Matt Grenander, Shay B. Cohen |  |
| 684 |  |  [SNaC: Coherence Error Detection for Narrative Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.29) |  | 0 | Progress in summarizing long texts is inhibited by the lack of appropriate evaluation frameworks. A long summary that appropriately covers the facets of that text must also present a coherent narrative, but current automatic and human evaluation methods fail to identify gaps in coherence. In this... | Greg Durrett, Junyi Jessy Li, Tanya Goyal |  |
| 685 |  |  [HydraSum: Disentangling Style Features in Text Summarization with Multi-Decoder Models](https://doi.org/10.18653/v1/2022.emnlp-main.30) |  | 0 | Summarization systems make numerous “decisions” about summary properties during inference, e.g. degree of copying, specificity and length of outputs, etc. However, these are implicitly encoded within model parameters and specific styles cannot be enforced. To address this, we introduce HydraSum, a... | Nazneen Rajani, Tanya Goyal, Wenhao Liu, Wojciech Kryscinski |  |
| 686 |  |  [A Good Neighbor, A Found Treasure: Mining Treasured Neighbors for Knowledge Graph Entity Typing](https://doi.org/10.18653/v1/2022.emnlp-main.31) |  | 0 | The task of knowledge graph entity typing (KGET) aims to infer the missing types for entities in knowledge graphs. Some pioneering work has proved that neighbor information is very important for the task. However, existing methods only leverage the one-hop neighbor information of the central... | Jun Zhao, Kang Liu, Pengfei Cao, Yubo Chen, Zhuoran Jin |  |
| 687 |  |  [Guiding Neural Entity Alignment with Compatibility](https://doi.org/10.18653/v1/2022.emnlp-main.32) |  | 0 | Entity Alignment (EA) aims to find equivalent entities between two Knowledge Graphs (KGs). While numerous neural EA models have been devised, they are mainly learned using labelled data only. In this work, we argue that different entities within one KG should have compatible counterparts in the... | Bing Liu, Genghong Zhao, Guido Zuccon, Harrisen Scells, Wen Hua, Xia Zhang |  |
| 688 |  |  [InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning](https://doi.org/10.18653/v1/2022.emnlp-main.33) |  | 0 | Instruction tuning is an emergent paradigm in NLP wherein natural language instructions are leveraged with language models to induce zero-shot performance on unseen tasks. Dialogue is an especially interesting area in which to explore instruction tuning because dialogue systems perform multiple... | Cathy Jiao, Jeffrey P. Bigham, Maxine Eskénazi, Prakhar Gupta, Shikib Mehri, YiTing Yeh |  |
| 689 |  |  [Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling](https://doi.org/10.18653/v1/2022.emnlp-main.34) |  | 0 | Boundary information is critical for various Chinese language processing tasks, such as word segmentation, part-of-speech tagging, and named entity recognition. Previous studies usually resorted to the use of a high-quality external lexicon, where lexicon items can offer explicit boundary... | Dingkun Long, Meishan Zhang, Min Zhang, Peijie Jiang, Pengjun Xie, Yanzhao Zhang |  |
| 690 |  |  [RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder](https://doi.org/10.18653/v1/2022.emnlp-main.35) |  | 0 | Despite pre-training’s progress in many important NLP tasks, it remains to explore effective pre-training strategies for dense retrieval. In this paper, we propose RetroMAE, a new retrieval oriented pre-training paradigm based on Masked Auto-Encoder (MAE). RetroMAE is highlighted by three critical... | Shitao Xiao, Yingxia Shao, Zhao Cao, Zheng Liu |  |
| 691 |  |  [Aligning Recommendation and Conversation via Dual Imitation](https://doi.org/10.18653/v1/2022.emnlp-main.36) |  | 0 | Human conversations of recommendation naturally involve the shift of interests which can align the recommendation actions and conversation process to make accurate recommendations with rich explanations. However, existing conversational recommendation systems (CRS) ignore the advantage of user... | Bo Wang, Dongming Zhao, Jinfeng Zhou, Kun Huang, Minlie Huang, Ruifang He, Yuexian Hou |  |
| 692 |  |  [QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance](https://doi.org/10.18653/v1/2022.emnlp-main.37) |  | 0 | Existing metrics for assessing question generation not only require costly human reference but also fail to take into account the input context of generation, rendering the lack of deep understanding of the relevance between the generated questions and input contexts. As a result, they may wrongly... | Bang Liu, Lingfei Wu, Siliang Tang, Xiaoqiang Wang |  |
| 693 |  |  [Abstract Visual Reasoning with Tangram Shapes](https://doi.org/10.18653/v1/2022.emnlp-main.38) |  | 0 | We introduce KiloGram, a resource for studying abstract visual reasoning in humans and machines. Drawing on the history of tangram puzzles as stimuli in cognitive science, we build a richly annotated dataset that, with >1k distinct stimuli, is orders of magnitude larger and more diverse than prior... | Alane Suhr, Anya Ji, Noah Rush, Noriyuki Kojima, Robert D. Hawkins, Wai Keen Vong, Yoav Artzi |  |
| 694 |  |  [UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.39) |  | 0 | Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different... | Ansong Ni, Bailin Wang, Caiming Xiong, Chen Henry Wu, Chengzu Li, ChienSheng Wu, Connor Boyle, Dragomir Radev, Lingpeng Kong, Luke Zettlemoyer, Michihiro Yasunaga, Ming Zhong, Noah A. Smith, Peng Shi, Pengcheng Yin, Rui Zhang, Ruiqi Zhong, Sida I. Wang, Tao Yu, Tianbao Xie, Torsten Scholak, Victor Zhong, Ziyu Yao |  |
| 695 |  |  [Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models](https://doi.org/10.18653/v1/2022.emnlp-main.40) |  | 0 | Traditional (fickle) adversarial examples involve finding a small perturbation that does not change an input’s true label but confuses the classifier into outputting a different prediction. Conversely, obstinate adversarial examples occur when an adversary finds a small perturbation that preserves... | David E. Evans, Hannah Chen, Yangfeng Ji |  |
| 696 |  |  [When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks](https://doi.org/10.18653/v1/2022.emnlp-main.41) |  | 0 | Humans can reason compositionally whilst grounding language utterances to the real world. Recent benchmarks like ReaSCAN (Wu et al., 2021) use navigation tasks grounded in a grid world to assess whether neural models exhibit similar capabilities. In this work, we present a simple transformer-based... | Ankur Sikarwar, Arkil Patel, Navin Goyal |  |
| 697 |  |  [Generative Language Models for Paragraph-Level Question Generation](https://doi.org/10.18653/v1/2022.emnlp-main.42) |  | 0 | Powerful generative models have led to recent progress in question generation (QG). However, it is difficult to measure advances in QG research since there are no standardized resources that allow a uniform comparison among approaches. In this paper, we introduce QG-Bench, a multilingual and... | Asahi Ushio, Fernando AlvaManchego, José CamachoCollados |  |
| 698 |  |  [A Unified Encoder-Decoder Framework with Entity Memory](https://doi.org/10.18653/v1/2022.emnlp-main.43) |  | 0 | Entities, as important carriers of real-world knowledge, play a key role in many NLP tasks.We focus on incorporating entity knowledge into an encoder-decoder framework for informative text generation. Existing approaches tried to index, retrieve, and read external documents as evidence, but they... | Chenguang Zhu, Meng Jiang, Wenhao Yu, Zhihan Zhang |  |
| 699 |  |  [Segmenting Numerical Substitution Ciphers](https://doi.org/10.18653/v1/2022.emnlp-main.44) |  | 0 | Deciphering historical substitution ciphers is a challenging problem. Example problems that have been previously studied include detecting cipher type, detecting plaintext language, and acquiring the substitution key for segmented ciphers. However, attacking unsegmented ciphers is still a... | Jonathan May, Nada Aldarrab |  |
| 700 |  |  [Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.45) |  | 0 | Research in massively multilingual image captioning has been severely hampered by a lack of high-quality evaluation datasets. In this paper we present the Crossmodal-3600 dataset (XM3600 in short), a geographically diverse set of 3600 images annotated with human-generated reference captions in 36... | Ashish V. Thapliyal, Jordi PontTuset, Radu Soricut, Xi Chen |  |
| 701 |  |  [ReSel: N-ary Relation Extraction from Scientific Text and Tables by Learning to Retrieve and Select](https://doi.org/10.18653/v1/2022.emnlp-main.46) |  | 0 | We study the problem of extracting N-ary relation tuples from scientific articles. This task is challenging because the target knowledge tuples can reside in multiple parts and modalities of the document. Our proposed method ReSel decomposes this task into a two-stage procedure that first retrieves... | Chao Zhang, Junyang Zhang, Le Song, Xiang Chen, Yinghao Li, Yingjun Mou, Yuchen Zhuang, Yue Yu |  |
| 702 |  |  [GammaE: Gamma Embeddings for Logical Queries on Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.47) |  | 0 | Embedding knowledge graphs (KGs) for multi-hop logical reasoning is a challenging problem due to massive and complicated structures in many KGs. Recently, many promising works projected entities and queries into a geometric space to efficiently find answers. However, it remains challenging to model... | Dong Yang, Haonan Lu, Peijun Qing, Xiaodong Lin, Yang Li |  |
| 703 |  |  [Reasoning Like Program Executors](https://doi.org/10.18653/v1/2022.emnlp-main.48) |  | 0 | Reasoning over natural language is a long-standing goal for the research community. However, studies have shown that existing language models are inadequate in reasoning. To address the issue, we present POET, a novel reasoning pre-training paradigm. Through pre-training language models with... | Bei Chen, JianGuang Lou, Morteza Ziyadi, Qian Liu, Qiang Fu, Weizhu Chen, Xinyu Pi, Yan Gao, Zeqi Lin |  |
| 704 |  |  [SEM-F1: an Automatic Way for Semantic Evaluation of Multi-Narrative Overlap Summaries at Scale](https://doi.org/10.18653/v1/2022.emnlp-main.49) |  | 0 | Recent work has introduced an important yet relatively under-explored NLP task called Semantic Overlap Summarization (SOS) that entails generating a summary from multiple alternative narratives which conveys the common information provided by those narratives. Previous work also published a... | Mousumi Akter, Naman Bansal, Shubhra Kanti Karmaker Santu |  |
| 705 |  |  [Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning](https://doi.org/10.18653/v1/2022.emnlp-main.50) |  | 0 | Prefix-tuning, or more generally continuous prompt tuning, has become an essential paradigm of parameter-efficient transfer learning. Using a large pre-trained language model (PLM), prefix-tuning can obtain strong performance by training only a small portion of parameters. In this paper, we propose... | Devamanyu Hazarika, Di Jin, Dilek HakkaniTur, Mahdi Namazifar, Yang Liu, Yifan Chen |  |
| 706 |  |  [DocInfer: Document-level Natural Language Inference using Optimal Evidence Selection](https://doi.org/10.18653/v1/2022.emnlp-main.51) |  | 0 | We present DocInfer - a novel, end-to-end Document-level Natural Language Inference model that builds a hierarchical document graph enriched through inter-sentence relations (topical, entity-based, concept-based), performs paragraph pruning using the novel SubGraph Pooling layer, followed by... | Dinesh Manocha, Gautam Kunapuli, Maneesh Singh, Manish Shrivastava, Puneet Mathur, Riyaz A. Bhat |  |
| 707 |  |  [LightEA: A Scalable, Robust, and Interpretable Entity Alignment Framework via Three-view Label Propagation](https://doi.org/10.18653/v1/2022.emnlp-main.52) |  | 0 | Entity Alignment (EA) aims to find equivalent entity pairs between KGs, which is the core step to bridging and integrating multi-source KGs. In this paper, we argue that existing complex EA methods inevitably inherit the inborn defects from their neural network lineage: poor interpretability and... | Man Lan, Wenting Wang, Xin Mao, Yuanbin Wu |  |
| 708 |  |  [Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.53) |  | 0 | Commonsense generation aims to generate a realistic sentence describing a daily scene under the given concepts, which is very challenging, since it requires models to have relational reasoning and compositional generalization capabilities. Previous work focuses on retrieving prototype sentences for... | ALong Jin, Bartuer Zhou, Biao Cheng, Hang Zhang, Jian Jiao, Nan Duan, SiuMing Yiu, Weizhen Qi, Xingwei He, Yeyun Gong |  |
| 709 |  |  [Efficient Document Retrieval by End-to-End Refining and Quantizing BERT Embedding with Contrastive Product Quantization](https://doi.org/10.18653/v1/2022.emnlp-main.54) |  | 0 | Efficient document retrieval heavily relies on the technique of semantic hashing, which learns a binary code for every document and employs Hamming distance to evaluate document distances. However, existing semantic hashing methods are mostly established on outdated TFIDF features, which obviously... | Jianxing Yu, Qinliang Su, Shijing Si, Zexuan Qiu |  |
| 710 |  |  [Curriculum Knowledge Distillation for Emoji-supervised Cross-lingual Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.55) |  | 0 | Existing sentiment analysis models have achieved great advances with the help of sufficient sentiment annotations. Unfortunately, many languages do not have sufficient sentiment corpus. To this end, recent studies have proposed cross-lingual sentiment analysis to transfer sentiment analysis models... | Fengmao Lv, Guowu Yang, Jianyang Zhang, Mingyang Wan, Tao Liang |  |
| 711 |  |  [Correctable-DST: Mitigating Historical Context Mismatch between Training and Inference for Improved Dialogue State Tracking](https://doi.org/10.18653/v1/2022.emnlp-main.56) |  | 0 | Recently proposed dialogue state tracking (DST) approaches predict the dialogue state of a target turn sequentially based on the previous dialogue state. During the training time, the ground-truth previous dialogue state is utilized as the historical context. However, only the previously predicted... | Bo Zou, Hao Huang, Haoxiang Su, Hongyan Xie, Jianghua Lin, Kun Deng, Shuangyong Song, Xiaodong He, Zhihui Zhang |  |
| 712 |  |  [DropMix: A Textual Data Augmentation Combining Dropout with Mixup](https://doi.org/10.18653/v1/2022.emnlp-main.57) |  | 0 | Overfitting is a notorious problem when there is insufficient data to train deep neural networks in machine learning tasks. Data augmentation regularization methods such as Dropout, Mixup, and their enhanced variants are effective and prevalent, and achieve promising performance to overcome... | Fanshuang Kong, Richong Zhang, Samuel Mensah, Xiaohui Guo, Yongyi Mao |  |
| 713 |  |  [Cross-document Event Coreference Search: Task, Dataset and Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.58) |  | 0 | The task of Cross-document Coreference Resolution has been traditionally formulated as requiring to identify all coreference links across a given set of documents. We propose an appealing, and often more applicable, complementary set up for the task – Cross-document Coreference Search, focusing in... | Alon Eirew, Avi Caciularu, Ido Dagan |  |
| 714 |  |  [VIRT: Improving Representation-based Text Matching via Virtual Interaction](https://doi.org/10.18653/v1/2022.emnlp-main.59) |  | 0 | Text matching is a fundamental research problem in natural language understanding. Interaction-based approaches treat the text pair as a single sequence and encode it through cross encoders, while representation-based models encode the text pair independently with siamese or dual encoders.... | Dan Li, Enhong Chen, Hongyin Tang, Jiahao Liu, Jingang Wang, Qifan Wang, Tong Xu, Wei Wu, Yang Yang |  |
| 715 |  |  [MAVEN-ERE: A Unified Large-scale Dataset for Event Coreference, Temporal, Causal, and Subevent Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.60) |  | 0 | The diverse relationships among real-world events, including coreference, temporal, causal, and subevent relations, are fundamental to understanding natural languages. However, two drawbacks of existing datasets limit event relation extraction (ERE) tasks: (1) Small scale. Due to the annotation... | Hao Peng, Jie Zhou, Juanzi Li, Lei Hou, Ning Ding, Peng Li, Xiaozhi Wang, Xu Han, Yankai Lin, Yulin Chen, Zhiyuan Liu, Zimu Wang |  |
| 716 |  |  [Entity Extraction in Low Resource Domains with Selective Pre-training of Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.61) |  | 0 | Transformer-based language models trained on large natural language corpora have been very useful in downstream entity extraction tasks. However, they often result in poor performances when applied to domains that are different from those they are pretrained on. Continued pretraining using... | Anandhavelu Natarajan, Aniruddha Mahapatra, Aparna Garimella, Sharmila Reddy Nangi |  |
| 717 |  |  [How Large Language Models are Transforming Machine-Paraphrase Plagiarism](https://doi.org/10.18653/v1/2022.emnlp-main.62) |  | 0 | The recent success of large language models for text generation poses a severe threat to academic integrity, as plagiarists can generate realistic paraphrases indistinguishable from original work.However, the role of large autoregressive models in generating machine-paraphrased plagiarism and their... | Bela Gipp, Frederic Kirstein, Jan Philip Wahle, Terry Ruas |  |
| 718 |  |  [M2D2: A Massively Multi-Domain Language Modeling Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.63) |  | 0 | We present M2D2, a fine-grained, massively multi-domain corpus for studying domain adaptation in language models (LMs). M2D2 consists of 8.5B tokens and spans 145 domains extracted from Wikipedia and Semantic Scholar. Using ontologies derived from Wikipedia and ArXiv categories, we organize the... | Luke Zettlemoyer, Machel Reid, Suchin Gururangan, Victor Zhong |  |
| 719 |  |  ["Will You Find These Shortcuts?" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.64) |  | 0 | Feature attribution a.k.a. input salience methods which assign an importance score to a feature are abundant but may produce surprisingly different results for the same model on the same input. While differences are expected if disparate definitions of importance are assumed, most methods claim to... | Anders Sandholm, Jasmijn Bastings, Katja Filippova, Polina Zablotskaia, Sebastian Ebert |  |
| 720 |  |  [Information-Transport-based Policy for Simultaneous Translation](https://doi.org/10.18653/v1/2022.emnlp-main.65) |  | 0 | Simultaneous translation (ST) outputs translation while receiving the source inputs, and hence requires a policy to determine whether to translate a target token or wait for the next source token. The major challenge of ST is that each target token can only be translated based on the current... | Shaolei Zhang, Yang Feng |  |
| 721 |  |  [Learning to Adapt to Low-Resource Paraphrase Generation](https://doi.org/10.18653/v1/2022.emnlp-main.66) |  | 0 | Paraphrase generation is a longstanding NLP task and achieves great success with the aid of large corpora. However, transferring a paraphrasing model to another domain encounters the problem of domain shifting especially when the data is sparse. At the same time, widely using large pre-trained... | Jianfeng Li, Rizhao Fan, Shaojun Wang, Yanmeng Wang, Ye Wang, Zhigen Li |  |
| 722 |  |  [A Distributional Lens for Multi-Aspect Controllable Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.67) |  | 0 | Multi-aspect controllable text generation is a more challenging and practical task than single-aspect control. Existing methods achieve complex multi-aspect control by fusing multiple controllers learned from single-aspect, but suffer from attribute degeneration caused by the mutual interference of... | Bing Qin, Heng Gong, Lingyuan Zhang, Sicheng Ma, Xiaocheng Feng, Yuxuan Gu |  |
| 723 |  |  [ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.68) |  | 0 | We study the text generation task under the approach of pre-trained language models (PLMs). Typically, an auto-regressive (AR) method is adopted for generating texts in a token-by-token manner. Despite many advantages of AR generation, it usually suffers from inefficient inference. Therefore,... | JiRong Wen, JianYun Nie, Junyi Li, Tianyi Tang, Wayne Xin Zhao |  |
| 724 |  |  [Multilingual Relation Classification via Efficient and Effective Prompting](https://doi.org/10.18653/v1/2022.emnlp-main.69) |  | 0 | Prompting pre-trained language models has achieved impressive performance on various NLP tasks, especially in low data regimes. Despite the success of prompting in monolingual settings, applying prompt-based methods in multilingual scenarios has been limited to a narrow set of tasks, due to the... | David Harbecke, Leonhard Hennig, Yuxuan Chen |  |
| 725 |  |  [Topic-Regularized Authorship Representation Learning](https://doi.org/10.18653/v1/2022.emnlp-main.70) |  | 0 | Authorship attribution is a task that aims to identify the author of a given piece of writing. We aim to develop a generalized solution that can handle a large number of texts from authors and topics unavailable in training data. Previous studies have proposed strategies to address only either... | Can Udomcharoenchaikit, Jitkapat Sawatphol, Nonthakit Chaiwong, Sarana Nutanong |  |
| 726 |  |  [Fine-grained Contrastive Learning for Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.71) |  | 0 | Recent relation extraction (RE) works have shown encouraging improvements by conducting contrastive learning on silver labels generated by distant supervision before fine-tuning on gold labels. Existing methods typically assume all these silver labels are accurate and treat them equally; however,... | Jiacheng Li, Jingbo Shang, William Hogan |  |
| 727 |  |  [Curriculum Prompt Learning with Self-Training for Abstractive Dialogue Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.72) |  | 0 | Succinctly summarizing dialogue is a task of growing interest, but inherent challenges, such as insufficient training data and low information density impede our ability to train abstractive models. In this work, we propose a novel curriculum-based prompt learning method with self-training to... | Changqun Li, Gerard de Melo, Liang He, Linlin Wang, Xin Lin |  |
| 728 |  |  [Zero-Shot Text Classification with Self-Training](https://doi.org/10.18653/v1/2022.emnlp-main.73) |  | 0 | Recent advances in large pretrained language models have increased attention to zero-shot text classification. In particular, models finetuned on natural language inference datasets have been widely adopted as zero-shot classifiers due to their promising results and off-the-shelf availability.... | Alon Halfon, Ariel Gera, Eyal Shnarch, Liat EinDor, Noam Slonim, Yotam Perlitz |  |
| 729 |  |  [Deconfounding Legal Judgment Prediction for European Court of Human Rights Cases Towards Better Alignment with Experts](https://doi.org/10.18653/v1/2022.emnlp-main.74) |  | 0 | This work demonstrates that Legal Judgement Prediction systems without expert-informed adjustments can be vulnerable to shallow, distracting surface signals that arise from corpus construction, case distribution, and confounding factors. To mitigate this, we use domain expertise to strategically... | Matthias Grabmair, Oana Ichim, Shanshan Xu, Tokala Yaswanth Sri Sai Santosh |  |
| 730 |  |  [SQuALITY: Building a Long-Document Summarization Dataset the Hard Way](https://doi.org/10.18653/v1/2022.emnlp-main.75) |  | 0 | Summarization datasets are often assembled either by scraping naturally occurring public-domain summaries—which are nearly always in difficult-to-work-with technical domains—or by using approximate heuristics to extract them from everyday text—which frequently yields unfaithful summaries. In this... | Alex Wang, Angelica Chen, Jason Phang, Richard Yuanzhe Pang, Samuel R. Bowman |  |
| 731 |  |  [MetaASSIST: Robust Dialogue State Tracking with Meta Learning](https://doi.org/10.18653/v1/2022.emnlp-main.76) |  | 0 | Existing dialogue datasets contain lots of noise in their state annotations. Such noise can hurt model training and ultimately lead to poor generalization performance. A general framework named ASSIST has recently been proposed to train robust dialogue state tracking (DST) models. It introduces an... | Emine Yilmaz, Fanghua Ye, Jie Huang, Samuel Stern, Shenghui Li, Xi Wang |  |
| 732 |  |  [Multilingual Machine Translation with Hyper-Adapters](https://doi.org/10.18653/v1/2022.emnlp-main.77) |  | 0 | Multilingual machine translation suffers from negative interference across languages. A common solution is to relax parameter sharing with language-specific modules like adapters. However, adapters of related languages are unable to transfer information, and their total number of parameters becomes... | Christos Baziotis, James Cross, Mikel Artetxe, Shruti Bhosale |  |
| 733 |  |  [Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination](https://doi.org/10.18653/v1/2022.emnlp-main.78) |  | 0 | Large-scale pretrained language models have made significant advances in solving downstream language understanding tasks. However, they generally suffer from reporting bias, the phenomenon describing the lack of explicit commonsense knowledge in written text, e.g., ”an orange is orange”. To... | Dong Yu, Hongming Zhang, Jianshu Chen, Wenlin Yao, Xiaoyang Wang, Yue Yang |  |
| 734 |  |  [Using Commonsense Knowledge to Answer Why-Questions](https://doi.org/10.18653/v1/2022.emnlp-main.79) |  | 0 | Answering questions in narratives about why events happened often requires commonsense knowledge external to the text. What aspects of this knowledge are available in large language models? What aspects can be made accessible via external commonsense resources? We study these questions in the... | Horace Liu, Nathanael Chambers, Niket Tandon, Niranjan Balasubramanian, Raymond J. Mooney, Tanvi Aggarwal, Yash Kumar Lal |  |
| 735 |  |  [Affective Idiosyncratic Responses to Music](https://doi.org/10.18653/v1/2022.emnlp-main.80) |  | 0 | Affective responses to music are highly personal. Despite consensus that idiosyncratic factors play a key role in regulating how listeners emotionally respond to music, precisely measuring the marginal effects of these variables has proved challenging. To address this gap, we develop computational... | Evan Li, Oliver Li, Sky CHWang, Smaranda Muresan, Zhou Yu |  |
| 736 |  |  [Successive Prompting for Decomposing Complex Questions](https://doi.org/10.18653/v1/2022.emnlp-main.81) |  | 0 | Answering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output... | Dheeru Dua, Matt Gardner, Sameer Singh, Shivanshu Gupta |  |
| 737 |  |  [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://doi.org/10.18653/v1/2022.emnlp-main.82) |  | 0 | Pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which... | Chandra Bhagavatula, Faeze Brahman, Jaehun Jung, Lianhui Qin, Ronan Le Bras, Sean Welleck, Yejin Choi |  |
| 738 |  |  [DANLI: Deliberative Agent for Following Natural Language Instructions](https://doi.org/10.18653/v1/2022.emnlp-main.83) |  | 0 | Recent years have seen an increasing amount of work on embodied AI agents that can perform tasks by following human language instructions. However, most of these agents are reactive, meaning that they simply learn and imitate behaviors encountered in the training data. These reactive agents are... | Jianing Yang, Jiayi Pan, Joyce Chai, Keunwoo Peter Yu, Nikhil Devraj, Shane Storks, Yichi Zhang, Yuwei Bao, Ziqiao Ma |  |
| 739 |  |  [Tracing Semantic Variation in Slang](https://doi.org/10.18653/v1/2022.emnlp-main.84) |  | 0 | The meaning of a slang term can vary in different communities. However, slang semantic variation is not well understood and under-explored in the natural language processing of slang. One existing view argues that slang semantic variation is driven by culture-dependent communicative needs. An... | Yang Xu, Zhewei Sun |  |
| 740 |  |  [Fine-grained Category Discovery under Coarse-grained supervision with Hierarchical Weighted Self-contrastive Learning](https://doi.org/10.18653/v1/2022.emnlp-main.85) |  | 0 | Novel category discovery aims at adapting models trained on known categories to novel categories. Previous works only focus on the scenario where known and novel categories are of the same granularity.In this paper, we investigate a new practical scenario called Fine-grained Category Discovery... | Feng Tian, Ping Chen, Qianying Wang, Qinghua Zheng, Siliang Tang, Wenbin An |  |
| 741 |  |  [PLM-based World Models for Text-based Games](https://doi.org/10.18653/v1/2022.emnlp-main.86) |  | 0 | World models have improved the ability of reinforcement learning agents to operate in a sample efficient manner, by being trained to predict plausible changes in the underlying environment. As the core tasks of world models are future prediction and commonsense understanding, our claim is that... | Dohyeon Lee, Minsoo Kim, Seungwon Hwang, YeonJoon Jung |  |
| 742 |  |  [Prompt-Based Meta-Learning For Few-shot Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.87) |  | 0 | Few-shot Text Classification predicts the semantic label of a given text with a handful of supporting instances. Current meta-learning methods have achieved satisfying results in various few-shot situations. Still, they often require a large amount of data to construct many few-shot tasks for... | Haibo Huang, Haoxing Zhang, Lei Yu, Xiaofeng Zhang |  |
| 743 |  |  [How well can Text-to-Image Generative Models understand Ethical Natural Language Interventions?](https://doi.org/10.18653/v1/2022.emnlp-main.88) |  | 0 | Text-to-image generative models have achieved unprecedented success in generating high-quality images based on natural language descriptions. However, it is shown that these models tend to favor specific social groups when prompted with neutral text descriptions (e.g., ‘a photo of a lawyer’).... | Da Yin, Hritik Bansal, KaiWei Chang, Masoud Monajatipoor |  |
| 744 |  |  [Geographic Citation Gaps in NLP Research](https://doi.org/10.18653/v1/2022.emnlp-main.89) |  | 0 | In a fair world, people have equitable opportunities to education, to conduct scientific research, to publish, and to get credit for their work, regardless of where they live. However, it is common knowledge among researchers that a vast number of papers accepted at top NLP venues come from a... | Diyi Yang, Janvijay Singh, Mukund Rungta, Saif M. Mohammad |  |
| 745 |  |  [Language Models of Code are Few-Shot Commonsense Learners](https://doi.org/10.18653/v1/2022.emnlp-main.90) |  | 0 | We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph.To employ large language models (LMs) for this task, existing approaches ‘serialize’ the output graph as a flat list of nodes and... | Aman Madaan, Graham Neubig, Shuyan Zhou, Uri Alon, Yiming Yang |  |
| 746 |  |  [Numerical Optimizations for Weighted Low-rank Estimation on Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.91) |  | 0 | Singular value decomposition (SVD) is one of the most popular compression methods that approximate a target matrix with smaller matrices. However, standard SVD treats the parameters within the matrix with equal importance, which is a simple but unrealistic assumption. The parameters of a trained... | Felicity Wang, Hongxia Jin, Qian Lou, Ting Hua, YenChang Hsu, Yilin Shen |  |
| 747 |  |  [Generative Multi-hop Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.92) |  | 0 | A common practice for text retrieval is to use an encoder to map the documents and the query to a common vector space and perform a nearest neighbor search (NNS); multi-hop retrieval also often adopts the same paradigm, usually with a modification of iteratively reformulating the query vector so... | Hanseok Oh, Hyunji Lee, Minjoon Seo, Sohee Yang |  |
| 748 |  |  [Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.93) |  | 0 | Image-to-text tasks such as open-ended image captioning and controllable image description have received extensive attention for decades. Here we advance this line of work further, presenting Visual Spatial Description (VSD), a new perspective for image-to-text toward spatial semantics. Given an... | Jianguo Wei, Meishan Zhang, Min Zhang, Yu Zhao, Yueheng Sun, Zhichao Lin |  |
| 749 |  |  [M3: A Multi-View Fusion and Multi-Decoding Network for Multi-Document Reading Comprehension](https://doi.org/10.18653/v1/2022.emnlp-main.94) |  | 0 | Multi-document reading comprehension task requires collecting evidences from different documents for answering questions. Previous research works either use the extractive modeling method to naively integrate the scores from different documents on the encoder side or use the generative modeling... | Houfeng Wang, Liang Wen, Xiaolin Wang, Yingwei Luo |  |
| 750 |  |  [COCO-DR: Combating the Distribution Shift in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning](https://doi.org/10.18653/v1/2022.emnlp-main.95) |  | 0 | We present a new zero-shot dense retrieval (ZeroDR) method, COCO-DR, to improve the generalization ability of dense retrieval by combating the distribution shifts between source training tasks and target scenarios. To mitigate the impact of document differences, COCO-DR continues pretraining the... | Arnold Overwijk, Chao Zhang, Chenyan Xiong, Si Sun, Yue Yu |  |
| 751 |  |  [Language Model Pre-Training with Sparse Latent Typing](https://doi.org/10.18653/v1/2022.emnlp-main.96) |  | 0 | Modern large-scale Pre-trained Language Models (PLMs) have achieved tremendous success on a wide range of downstream tasks. However, most of the LM pre-training objectives only focus on text reconstruction, but have not sought to learn latent-level interpretable representations of sentences. In... | ChengXiang Zhai, Clare R. Voss, Han Wang, Heng Ji, Liliang Ren, Zixuan Zhang |  |
| 752 |  |  [On the Transformation of Latent Space in Fine-Tuned NLP Models](https://doi.org/10.18653/v1/2022.emnlp-main.97) |  | 0 | We study the evolution of latent space in fine-tuned NLP models. Different from the commonly used probing-framework, we opt for an unsupervised method to analyze representations. More specifically, we discover latent concepts in the representational space using hierarchical clustering. We then use... | Fahim Dalvi, Firoj Alam, Hassan Sajjad, Nadir Durrani |  |
| 753 |  |  [Watch the Neighbors: A Unified K-Nearest Neighbor Contrastive Learning Framework for OOD Intent Discovery](https://doi.org/10.18653/v1/2022.emnlp-main.98) |  | 0 | Discovering out-of-domain (OOD) intent is important for developing new skills in task-oriented dialogue systems. The key challenges lie in how to transfer prior in-domain (IND) knowledge to OOD clustering, as well as jointly learn OOD representations and cluster assignments. Previous methods suffer... | Jingang Wang, Keqing He, Pei Wang, Wei Wu, Weiran Xu, Yanan Wu, Yutao Mou |  |
| 754 |  |  [Extracted BERT Model Leaks More Information than You Think!](https://doi.org/10.18653/v1/2022.emnlp-main.99) |  | 0 | The collection and availability of big data, combined with advances in pre-trained models (e.g. BERT), have revolutionized the predictive performance of natural language processing tasks. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned... | Chen Chen, Lingjuan Lyu, Qiongkai Xu, Xuanli He |  |
| 755 |  |  [Do Vision-and-Language Transformers Learn Grounded Predicate-Noun Dependencies?](https://doi.org/10.18653/v1/2022.emnlp-main.100) |  | 0 | Recent advances in vision-and-language modeling have seen the development of Transformer architectures that achieve remarkable performance on multimodal reasoning tasks.Yet, the exact capabilities of these black-box models are still poorly understood. While much of previous work has focused on... | Abdellah Fourtassi, Benoît Favre, Emmanuelle Salin, Mitja Nikolaus, Stéphane Ayache |  |
| 756 |  |  [A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference](https://doi.org/10.18653/v1/2022.emnlp-main.101) |  | 0 | Most evaluations of attribution methods focus on the English language. In this work, we present a multilingual approach for evaluating attribution methods for the Natural Language Inference (NLI) task in terms of faithfulness and plausibility.First, we introduce a novel cross-lingual strategy to... | Kerem Zaman, Yonatan Belinkov |  |
| 757 |  |  [Graph-Based Multilingual Label Propagation for Low-Resource Part-of-Speech Tagging](https://doi.org/10.18653/v1/2022.emnlp-main.102) |  | 0 | Part-of-Speech (POS) tagging is an important component of the NLP pipeline, but many low-resource languages lack labeled data for training. An established method for training a POS tagger in such a scenario is to create a labeled training set by transferring from high-resource languages. In this... | Ayyoob Imani, François Yvon, Hinrich Schütze, Masoud Jalili Sabet, Silvia Severini |  |
| 758 |  |  [SubeventWriter: Iterative Sub-event Sequence Generation with Coherence Controller](https://doi.org/10.18653/v1/2022.emnlp-main.103) |  | 0 | In this paper, we propose a new task of sub-event generation for an unseen process to evaluate the understanding of the coherence of sub-event actions and objects. To solve the problem, we design SubeventWriter, a sub-event sequence generation framework with a coherence controller. Given an unseen... | Ginny Y. Wong, Hongming Zhang, Simon See, Tianqing Fang, Yangqiu Song, Zhaowei Wang |  |
| 759 |  |  [Infinite SCAN: An Infinite Model of Diachronic Semantic Change](https://doi.org/10.18653/v1/2022.emnlp-main.104) |  | 0 | In this study, we propose a Bayesian model that can jointly estimate the number of senses of words and their changes through time.The model combines a dynamic topic model on Gaussian Markov random fields with a logistic stick-breaking process that realizes Dirichlet process. In the experiments, we... | Daichi Mochihashi, Hiroya Takamura, Mamoru Komachi, Seiichi Inoue, Toshinobu Ogiso |  |
| 760 |  |  [Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization](https://doi.org/10.18653/v1/2022.emnlp-main.105) |  | 0 | Training language models to learn from human instructions for zero-shot cross-task generalization has attracted much attention in NLP communities. Recently, instruction tuning (IT), which fine-tunes a pre-trained language model on a massive collection of tasks described via human-craft... | Minlie Huang, Pei Ke, Xiaoyan Zhu, Yuxian Gu |  |
| 761 |  |  [Counterfactual Data Augmentation via Perspective Transition for Open-Domain Dialogues](https://doi.org/10.18653/v1/2022.emnlp-main.106) |  | 0 | The construction of open-domain dialogue systems requires high-quality dialogue datasets. The dialogue data admits a wide variety of responses for a given dialogue history, especially responses with different semantics. However, collecting high-quality such a dataset in most scenarios is... | Jiao Ou, Jie Zhou, Jinchao Zhang, Yang Feng |  |
| 762 |  |  [SQUIRE: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.107) |  | 0 | Multi-hop knowledge graph (KG) reasoning has been widely studied in recent years to provide interpretable predictions on missing links with evidential paths. Most previous works use reinforcement learning (RL) based methods that learn to navigate the path towards the target entity. However, these... | Feiyu Xiong, Juanzi Li, Lei Hou, Xin Lv, Yincen Qu, Yushi Bai, Zelin Dai |  |
| 763 |  |  [SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training](https://doi.org/10.18653/v1/2022.emnlp-main.108) |  | 0 | The rapid development of single-modal pre-training has prompted researchers to pay more attention to cross-modal pre-training methods. In this paper, we propose a unified-modal speech-unit-text pre-training model, SpeechUT, to connect the representations of a speech encoder and a text decoder with... | Furu Wei, Jinyu Li, Junyi Ao, Lirong Dai, Long Zhou, Shujie Liu, Ziqiang Zhang |  |
| 764 |  |  [Learning Label Modular Prompts for Text Classification in the Wild](https://doi.org/10.18653/v1/2022.emnlp-main.109) |  | 0 | Machine learning models usually assume i.i.d data during training and testing, but data and tasks in real world often change over time. To emulate the transient nature of real world, we propose a challenging but practical task: text classification in-the-wild, which introduces different... | Amrita Saha, Hailin Chen, Shafiq R. Joty, Steven C. H. Hoi |  |
| 765 |  |  [Unbiased and Efficient Sampling of Dependency Trees](https://doi.org/10.18653/v1/2022.emnlp-main.110) |  | 0 | Most computational models of dependency syntax consist of distributions over spanning trees. However, the majority of dependency treebanks require that every valid dependency tree has a single edge coming out of the ROOT node, a constraint that is not part of the definition of spanning trees. For... | Milos Stanojevic |  |
| 766 |  |  [Continual Learning of Neural Machine Translation within Low Forgetting Risk Regions](https://doi.org/10.18653/v1/2022.emnlp-main.111) |  | 0 | This paper considers continual learning of large-scale pretrained neural machine translation model without accessing the previous training data or introducing model separation. We argue that the widely used regularization-based methods, which perform multi-objective learning with an auxiliary loss,... | Bojie Hu, Shuhao Gu, Yang Feng |  |
| 767 |  |  [COST-EFF: Collaborative Optimization of Spatial and Temporal Efficiency with Slenderized Multi-exit Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.112) |  | 0 | Transformer-based pre-trained language models (PLMs) mostly suffer from excessive overhead despite their advanced capacity. For resource-constrained devices, there is an urgent need for a spatially and temporally efficient model which retains the major capacity of PLMs. However, existing statically... | Bowen Shen, Lei Wang, Weiping Wang, Yuanxin Liu, Zheng Lin, Zhengxiao Liu |  |
| 768 |  |  [Rescue Implicit and Long-tail Cases: Nearest Neighbor Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.113) |  | 0 | Relation extraction (RE) has achieved remarkable progress with the help of pre-trained language models. However, existing RE models are usually incapable of handling two situations: implicit expressions and long-tail relation types, caused by language complexity and data sparsity. In this paper, we... | Fei Cheng, Jiwei Li, Qianying Liu, Sadao Kurohashi, Zhen Wan, Zhuoyuan Mao |  |
| 769 |  |  [StoryER: Automatic Story Evaluation via Ranking, Rating and Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.114) |  | 0 | Existing automatic story evaluation methods place a premium on story lexical level coherence, deviating from human preference.We go beyond this limitation by considering a novel Story Evaluation method that mimics human preference when judging a story, namely StoryER, which consists of three... | Duc Minh Vo, Hideki Nakayama, Hiroya Takamura, Hong Chen, Yusuke Miyao |  |
| 770 |  |  [Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference](https://doi.org/10.18653/v1/2022.emnlp-main.115) |  | 0 | While large pre-trained language models are powerful, their predictions often lack logical consistency across test inputs. For example, a state-of-the-art Macaw question-answering (QA) model answers <i>Yes</i> to <i>Is a sparrow a bird?</i> and <i>Does a bird have feet?</i> but answers <i>No</i> to... | Ananth Agarwal, Chelsea Finn, Christopher D. Manning, Eric Mitchell, Joseph J. Noh, Patrick Liu, Siyan Li, William S. Armstrong |  |
| 771 |  |  [Robustness of Demonstration-based Learning Under Limited Data Scenario](https://doi.org/10.18653/v1/2022.emnlp-main.116) |  | 0 | Demonstration-based learning has shown great potential in stimulating pretrained language models’ ability under limited data scenario. Simply augmenting the input with some demonstrations can significantly improve performance on few-shot NER. However, why such demonstrations are beneficial for the... | Diyi Yang, Hongxin Zhang, Ruiyi Zhang, Yanzhe Zhang |  |
| 772 |  |  [Modeling Information Change in Science Communication with Semantically Matched Paraphrases](https://doi.org/10.18653/v1/2022.emnlp-main.117) |  | 0 | Whether the media faithfully communicate scientific information has long been a core issue to the science community. Automatically identifying paraphrased scientific findings could enable large-scale tracking and analysis of information changes in the science communication process, but this... | David Jurgens, Dustin Wright, Isabelle Augenstein, Jiaxin Pei |  |
| 773 |  |  [Word Order Matters When You Increase Masking](https://doi.org/10.18653/v1/2022.emnlp-main.118) |  | 0 | Word order, an essential property of natural languages, is injected in Transformer-based neural language models using position encoding. However, recent experiments have shown that explicit position encoding is not always useful, since some models without such feature managed to achieve... | Alessandro Lenci, Karim Lasri, Thierry Poibeau |  |
| 774 |  |  [An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.119) |  | 0 | Large language models are shown to present privacy risks through memorization of training data, andseveral recent works have studied such risks for the pre-training phase. Little attention, however, has been given to the fine-tuning phase and it is not well understood how different fine-tuning... | Archit Uniyal, David E. Evans, Fatemehsadat Mireshghallah, Taylor BergKirkpatrick, Tianhao Wang |  |
| 775 |  |  [Style Transfer as Data Augmentation: A Case Study on Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.120) |  | 0 | In this work, we take the named entity recognition task in the English language as a case study and explore style transfer as a data augmentation method to increase the size and diversity of training data in low-resource scenarios. We propose a new method to effectively transform the text from a... | Leonardo Neves, Shuguang Chen, Thamar Solorio |  |
| 776 |  |  [Linguistic Corpus Annotation for Automatic Text Simplification Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.121) |  | 0 | Evaluating automatic text simplification (ATS) systems is a difficult task that is either performed by automatic metrics or user-based evaluations. However, from a linguistic point-of-view, it is not always clear on what bases these evaluations operate. In this paper, we propose annotations of the... | Adeline Müller, Adrien Bibal, David Alfter, Magali Norré, Patrick Watrin, Rodrigo Wilkens, Rémi Cardon, Thomas François |  |
| 777 |  |  [Semantic Framework based Query Generation for Temporal Question Answering over Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.122) |  | 0 | Answering factual questions with temporal intent over knowledge graphs (temporal KGQA) attracts rising attention in recent years.In the generation of temporal queries, existing KGQA methods ignore the fact that some intrinsic connections between events can make them temporally related, which may... | Hao Chen, Huayu Li, Wentao Ding, Yuzhong Qu |  |
| 778 |  |  [There Is No Standard Answer: Knowledge-Grounded Dialogue Generation with Adversarial Activated Multi-Reference Learning](https://doi.org/10.18653/v1/2022.emnlp-main.123) |  | 0 | Knowledge-grounded dialogue (KGC) shows excellent potential to deliver an engaging and informative response. However, existing approaches emphasize selecting one golden knowledge given a particular dialogue context, overlooking the one-to-many phenomenon in dialogue. As a result, existing paradigm... | Chongyang Tao, Rui Yan, Tingchen Fu, Xueliang Zhao |  |
| 779 |  |  [Stop Measuring Calibration When Humans Disagree](https://doi.org/10.18653/v1/2022.emnlp-main.124) |  | 0 | Calibration is a popular framework to evaluate whether a classifier knows when it does not know - i.e., its predictive probabilities are a good indication of how likely a prediction is to be correct. Correctness is commonly estimated against the human majority class. Recently, calibration to human... | Barbara Plank, Joris Baan, Raquel Fernández, Wilker Aziz |  |
| 780 |  |  [Improving compositional generalization for multi-step quantitative reasoning in question answering](https://doi.org/10.18653/v1/2022.emnlp-main.125) |  | 0 | Quantitative reasoning is an important aspect of question answering, especially when numeric and verbal cues interact to indicate sophisticated, multi-step programs. In this paper, we demonstrate how modeling the compositional nature of quantitative text can enhance the performance and robustness... | Armineh Nourbakhsh, Carolyn P. Rosé, Cathy Jiao, Sameena Shah |  |
| 781 |  |  [A Comprehensive Comparison of Neural Networks as Cognitive Models of Inflection](https://doi.org/10.18653/v1/2022.emnlp-main.126) |  | 0 | Neural networks have long been at the center of a debate around the cognitive mechanism by which humans process inflectional morphology. This debate has gravitated into NLP by way of the question: Are neural networks a feasible account for human behavior in morphological inflection?We address that... | Adam Wiemerslage, Katharina Kann, Shiran Dudy |  |
| 782 |  |  [Can Visual Context Improve Automatic Speech Recognition for an Embodied Agent?](https://doi.org/10.18653/v1/2022.emnlp-main.127) |  | 0 | The usage of automatic speech recognition (ASR) systems are becoming omnipresent ranging from personal assistant to chatbots, home, and industrial automation systems, etc. Modern robots are also equipped with ASR capabilities for interacting with humans as speech is the most natural interaction... | Chayan Sarkar, Pradip Pramanick |  |
| 783 |  |  [AfroLID: A Neural Language Identification Tool for African Languages](https://doi.org/10.18653/v1/2022.emnlp-main.128) |  | 0 | Language identification (LID) is a crucial precursor for NLP, especially for mining web data. Problematically, most of the world’s 7000+ languages today are not covered by LID technologies. We address this pressing issue for Africa by introducing AfroLID, a neural LID toolkit for 517 African... | AbdelRahim A. Elmadany, Alcides Alcoba Inciarte, Ife Adebara, Muhammad AbdulMageed |  |
| 784 |  |  [EvEntS ReaLM: Event Reasoning of Entity States via Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.129) |  | 0 | This paper investigates models of event implications. Specifically, how well models predict entity state-changes, by targeting their understanding of physical attributes. Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our... | Artidoro Pagnoni, Eduard H. Hovy, Evangelia Spiliopoulou, Yonatan Bisk |  |
| 785 |  |  [Large language models are few-shot clinical information extractors](https://doi.org/10.18653/v1/2022.emnlp-main.130) |  | 0 | A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such... | David A. Sontag, Hunter Lang, Monica Agrawal, Stefan Hegselmann, Yoon Kim |  |
| 786 |  |  [Towards a Unified Multi-Dimensional Evaluator for Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.131) |  | 0 | Multi-dimensional evaluation is the dominant paradigm for human evaluation in Natural Language Generation (NLG), i.e., evaluating the generated text from multiple explainable dimensions, such as coherence and fluency. However, automatic evaluation in NLG is still dominated by similarity-based... | Chenguang Zhu, Da Yin, Heng Ji, Jiawei Han, Ming Zhong, Pengfei Liu, Yang Liu, Yizhu Jiao, Yuning Mao |  |
| 787 |  |  [GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.132) |  | 0 | Recent work has shown that Pre-trained Language Models (PLMs) store the relational knowledge learned from data and utilize it for performing downstream tasks. However, commonsense knowledge across different regions may vary. For instance, the color of bridal dress is white in American weddings... | Da Yin, Hritik Bansal, KaiWei Chang, Liunian Harold Li, Masoud Monajatipoor |  |
| 788 |  |  [The (Undesired) Attenuation of Human Biases by Multilinguality](https://doi.org/10.18653/v1/2022.emnlp-main.133) |  | 0 | Some human preferences are universal. The odor of vanilla is perceived as pleasant all around the world. We expect neural models trained on human texts to exhibit these kind of preferences, i.e. biases, but we show that this is not always the case. We explore 16 static and contextual embedding... | Alberto BarrónCedeño, Cristina EspañaBonet |  |
| 789 |  |  [Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.134) |  | 0 | Our goal is a question-answering (QA) system that can show how its answers are implied by its own internal beliefs via a systematic chain of reasoning. Such a capability would allow better understanding of why a model produced the answer it did. Our approach is to recursively combine a trained... | Bhavana Dalvi Mishra, Oyvind Tafjord, Peter Clark |  |
| 790 |  |  [Near-Negative Distinction: Giving a Second Life to Human Evaluation Datasets](https://doi.org/10.18653/v1/2022.emnlp-main.135) |  | 0 | Precisely assessing the progress in natural language generation (NLG) tasks is challenging, and human evaluation to establish a preference in a model’s output over another is often necessary.However, human evaluation is usually costly, difficult to reproduce, and non-reusable.In this paper, we... | Caiming Xiong, ChienSheng Wu, Philippe Laban, Wenhao Liu |  |
| 791 |  |  [ToKen: Task Decomposition and Knowledge Infusion for Few-Shot Hate Speech Detection](https://doi.org/10.18653/v1/2022.emnlp-main.136) |  | 0 | Hate speech detection is complex; it relies on commonsense reasoning, knowledge of stereotypes, and an understanding of social nuance that differs from one culture to the next. It is also difficult to collect a large-scale hate speech annotated dataset. In this work, we frame this problem as a... | Asli Celikyilmaz, Badr AlKhamissi, Faisal Ladhak, Lambert Mathias, Mona T. Diab, Pascale Fung, Srini Iyer, Veselin Stoyanov, Xian Li, Zornitsa Kozareva |  |
| 792 |  |  [Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations](https://doi.org/10.18653/v1/2022.emnlp-main.137) |  | 0 | Recent work on explainable NLP has shown that few-shot prompting can enable large pre-trained language models (LLMs) to generate grammatical and factual natural language explanations for data labels. In this work, we study the connection between explainability and sample hardness by investigating... | Mohit Bansal, Nazneen Rajani, Peter Hase, Swarnadeep Saha |  |
| 793 |  |  [Stanceosaurus: Classifying Stance Towards Multicultural Misinformation](https://doi.org/10.18653/v1/2022.emnlp-main.138) |  | 0 | We present Stanceosaurus, a new corpus of 28,033 tweets in English, Hindi and Arabic annotated with stance towards 250 misinformation claims. As far as we are aware, it is the largest corpus annotated with stance towards misinformation claims. The claims in Stanceosaurus originate from 15... | Alan Ritter, Ashutosh Baheti, Jonathan Zheng, Tarek Naous, Wei Xu |  |
| 794 |  |  [Gendered Mental Health Stigma in Masked Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.139) |  | 0 | Mental health stigma prevents many individuals from receiving the appropriate care, and social psychology studies have shown that mental health tends to be overlooked in men. In this work, we investigate gendered mental health stigma in masked language models. In doing so, we operationalize mental... | Anjalie Field, Ashish Sharma, Inna W. Lin, Katharina Reinecke, Lucille Njoo, Tim Althoff, Yulia Tsvetkov |  |
| 795 |  |  [Efficient Nearest Neighbor Search for Cross-Encoder Models using Matrix Factorization](https://doi.org/10.18653/v1/2022.emnlp-main.140) |  | 0 | Efficient k-nearest neighbor search is a fundamental task, foundational for many problems in NLP. When the similarity is measured by dot-product between dual-encoder vectors or L2-distance, there already exist many scalable and efficient search methods. But not so when similarity is measured by... | Andrew McCallum, Manzil Zaheer, Nicholas Monath, Nishant Yadav, Rico Angell |  |
| 796 |  |  [Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.141) |  | 0 | We propose a method for arbitrary textual style transfer (TST)—the task of transforming a text into any given style—utilizing general-purpose pre-trained language models. Our method, Prompt-and-Rerank, is based on a mathematical formulation of the TST task, decomposing it into three constituent... | Dan Jurafsky, Luke MelasKyriazi, Mirac Suzgun |  |
| 797 |  |  [Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts](https://doi.org/10.18653/v1/2022.emnlp-main.142) |  | 0 | Explicit decomposition modeling, which involves breaking down complex tasks into more straightforward and often more interpretable sub-tasks, has long been a central theme in developing robust and interpretable NLU systems. However, despite the many datasets and resources built as part of this... | Ben Zhou, Dan Roth, Kyle Richardson, Xiaodong Yu |  |
| 798 |  |  [Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality](https://doi.org/10.18653/v1/2022.emnlp-main.143) |  | 0 | Recent visuolinguistic pre-trained models show promising progress on various end tasks such as image retrieval and video captioning. Yet, they fail miserably on the recently proposed Winoground dataset, which challenges models to match paired images and English captions, with items constructed to... | Anuj Diwan, David Harwath, Eunsol Choi, Kyle Mahowald, Layne Berry |  |
| 799 |  |  [Gradient-based Constrained Sampling from Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.144) |  | 0 | Large pretrained language models are successful at generating fluent text but are notoriously hard to controllably sample from. In this work, we study constrained sampling from such language models, i.e., generating text that satisfies user-defined constraints, while maintaining fluency and model’s... | Biswajit Paria, Sachin Kumar, Yulia Tsvetkov |  |
| 800 |  |  [TaCube: Pre-computing Data Cubes for Answering Numerical-Reasoning Questions over Tabular Data](https://doi.org/10.18653/v1/2022.emnlp-main.145) |  | 0 | Existing auto-regressive pre-trained language models (PLMs) like T5 and BART, have been well applied to table question answering by UNIFIEDSKG and TAPEX, respectively, and demonstrated state-of-the-art results on multiple benchmarks. However, auto-regressive PLMs are challenged by recent emerging... | Dongmei Zhang, Fan Cheng, Fan Zhou, Haoyu Dong, Mengkang Hu, Shi Han, Zhoujun Cheng |  |
| 801 |  |  [Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence](https://doi.org/10.18653/v1/2022.emnlp-main.146) |  | 0 | Question answering models can use rich knowledge sources — up to one hundred retrieved passages and parametric knowledge in the large-scale language model (LM). Prior work assumes information in such knowledge sources is consistent with each other, paying little attention to how models blend... | Eunsol Choi, HungTing Chen, Michael J. Q. Zhang |  |
| 802 |  |  [QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation](https://doi.org/10.18653/v1/2022.emnlp-main.147) |  | 0 | Question answering (QA) has recently shown impressive results for answering questions from customized domains. Yet, a common challenge is to adapt QA models to an unseen target domain. In this paper, we propose a novel self-supervised framework called QADA for QA domain adaptation. QADA introduces... | Bernhard Kratzwald, Dong Wang, Huimin Zeng, Stefan Feuerriegel, Zhenrui Yue |  |
| 803 |  |  [When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain](https://doi.org/10.18653/v1/2022.emnlp-main.148) |  | 0 | Pre-trained language models have shown impressive performance on a variety of tasks and domains. Previous research on financial language models usually employs a generic training scheme to train standard model architectures, without completely leveraging the richness of the financial data. We... | Agam Shah, Charese Smiley, Dheeraj Eidnani, Diyi Yang, Jiaao Chen, Kunal Chawla, Natraj Raman, Raj Sanjay Shah, Sudheer Chava, Wendi Du |  |
| 804 |  |  [Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer](https://doi.org/10.18653/v1/2022.emnlp-main.149) |  | 0 | Systems for knowledge-intensive tasks such as open-domain question answering (QA) usually consist of two stages: efficient retrieval of relevant documents from a large corpus and detailed reading of the selected documents. This is usually done through two separate models, a retriever that encodes... | Graham Neubig, Haibo Ding, Jamie Callan, Jun Araki, Luyu Gao, Zhengbao Jiang, Zhiruo Wang |  |
| 805 |  |  [Reproducibility in Computational Linguistics: Is Source Code Enough?](https://doi.org/10.18653/v1/2022.emnlp-main.150) |  | 0 | The availability of source code has been put forward as one of the most critical factors for improving the reproducibility of scientific research. This work studies trends in source code availability at major computational linguistics conferences, namely, ACL, EMNLP, LREC, NAACL, and COLING. We... | Luís Pina, Mohammad Arvan, Natalie Parde |  |
| 806 |  |  [Generating Information-Seeking Conversations from Unlabeled Documents](https://doi.org/10.18653/v1/2022.emnlp-main.151) |  | 0 | Synthesizing datasets for conversational question answering (CQA) from unlabeled documents remains challenging due to its interactive nature.Moreover, while modeling information needs is an essential key, only few studies have discussed it.In this paper, we introduce a novel framework,... | Gangwoo Kim, Jaewoo Kang, Kang Min Yoo, Sungdong Kim |  |
| 807 |  |  [Distill The Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.152) |  | 0 | Past works on multimodal machine translation (MMT) elevate bilingual setup by incorporating additional aligned vision information.However, an image-must requirement of the multimodal dataset largely hinders MMT’s development — namely that it demands an aligned form of [image, source text, target... | Jake Zhao, Ru Peng, Yawen Zeng |  |
| 808 |  |  [A Multifaceted Framework to Evaluate Evasion, Content Preservation, and Misattribution in Authorship Obfuscation Techniques](https://doi.org/10.18653/v1/2022.emnlp-main.153) |  | 0 | Authorship obfuscation techniques have commonly been evaluated based on their ability to hide the author’s identity (evasion) while preserving the content of the original text. However, to avoid overstating the systems’ effectiveness, evasion detection must be evaluated using competitive... | Benjamin C. M. Fung, Jackie Chi Kit Cheung, Malik H. Altakrori, Thomas Scialom |  |
| 809 |  |  [SafeText: A Benchmark for Exploring Physical Safety in Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.154) |  | 0 | Understanding what constitutes safe text is an important issue in natural language processing and can often prevent the deployment of models deemed harmful and unsafe. One such type of safety that has been scarcely studied is commonsense physical safety, i.e. text that is not explicitly violent and... | Desmond Patton, Emily Allaway, Kathleen R. McKeown, Lydia B. Chilton, Melanie Subbiah, Sharon Levy, William Yang Wang |  |
| 810 |  |  [Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations](https://doi.org/10.18653/v1/2022.emnlp-main.155) |  | 0 | Despite recent explosion of interests in in-context learning, the underlying mechanism and the precise impact of the quality of demonstrations remain elusive.Intuitively, ground-truth labels should have as much impact in in-context learning (ICL) as supervised learning, but recent work reported... | Hwiyeol Jo, Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Kang Min Yoo, SangWoo Lee, Sanggoo Lee, Taeuk Kim |  |
| 811 |  |  [D4: a Chinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat](https://doi.org/10.18653/v1/2022.emnlp-main.156) |  | 0 | In a depression-diagnosis-directed clinical session, doctors initiate a conversation with ample emotional support that guides the patients to expose their symptoms based on clinical diagnosis criteria. Such a dialogue system is distinguished from existing single-purpose human-machine dialog... | Binwei Yao, Chao Shi, Kai Yu, Likai Zou, Lingfeng Dai, Lu Chen, Mengyue Wu, Zhen Wang |  |
| 812 |  |  [Exploiting domain-slot related keywords description for Few-Shot Cross-Domain Dialogue State Tracking](https://doi.org/10.18653/v1/2022.emnlp-main.157) |  | 0 | Collecting dialogue data with domain-slot-value labels for dialogue state tracking (DST) could be a costly process. In this paper, we propose a novel framework based on domain-slot related description to tackle the challenge of few-shot cross-domain DST. Specifically, we design an extraction module... | Chen Zeng, Daichi Guo, Guanting Dong, Liwen Wang, Mingyang Sun, QiXiang Gao, Weiran Xu, Yutao Mou |  |
| 813 |  |  [CoCoa: An Encoder-Decoder Model for Controllable Code-switched Generation](https://doi.org/10.18653/v1/2022.emnlp-main.158) |  | 0 | Code-switching has seen growing interest in recent years as an important multilingual NLP phenomenon. Generating code-switched text for data augmentation has been sufficiently well-explored. However, there is no prior work on generating code-switched text with fine-grained control on the degree of... | Aravindan Raghuveer, Preethi Jyothi, Ritika, Shreya Pathak, Sneha Mondal |  |
| 814 |  |  [Towards Climate Awareness in NLP Research](https://doi.org/10.18653/v1/2022.emnlp-main.159) |  | 0 | The climate impact of AI, and NLP research in particular, has become a serious issue given the enormous amount of energy that is increasingly being used for training and running computational models. Consequently, increasing focus is placed on efficient NLP. However, this important initiative lacks... | Daniel Hershcovich, Julia Anna Bingler, Markus Leippold, Mathias Kraus, Nicolas Webersinke |  |
| 815 |  |  [Navigating Connected Memories with a Task-oriented Dialog System](https://doi.org/10.18653/v1/2022.emnlp-main.160) |  | 0 | Recent years have seen an increasing trend in the volume of personal media captured by users, thanks to the advent of smartphones and smart glasses, resulting in large media collections. Despite conversation being an intuitive human-computer interface, current efforts focus mostly on single-shot... | Alborz Geramifard, Babak Damavandi, Satwik Kottur, Seungwhan Moon |  |
| 816 |  |  [Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.161) |  | 0 | Pre-trained language models (LMs), such as BERT (Devlin et al., 2018) and its variants, have led to significant improvements on various NLP tasks in past years. However, a theoretical framework for studying their relationships is still missing. In this paper, we fill this gap by investigating the... | Hao Zhang |  |
| 817 |  |  [SynGEC: Syntax-Enhanced Grammatical Error Correction with a Tailored GEC-Oriented Parser](https://doi.org/10.18653/v1/2022.emnlp-main.162) |  | 0 | This work proposes a syntax-enhanced grammatical error correction (GEC) approach named SynGEC that effectively incorporates dependency syntactic information into the encoder part of GEC models. The key challenge for this idea is that off-the-shelf parsers are unreliable when processing... | Bo Zhang, Chen Li, Min Zhang, Yue Zhang, Zhenghua Li, Zuyi Bao |  |
| 818 |  |  [Varifocal Question Generation for Fact-checking](https://doi.org/10.18653/v1/2022.emnlp-main.163) |  | 0 | Fact-checking requires retrieving evidence related to a claim under investigation. The task can be formulated as question generation based on a claim, followed by question answering.However, recent question generation approaches assume that the answer is known and typically contained in a passage... | Andreas Vlachos, Nedjma Ousidhoum, Zhangdie Yuan |  |
| 819 |  |  [Bilingual Lexicon Induction for Low-Resource Languages using Graph Matching via Optimal Transport](https://doi.org/10.18653/v1/2022.emnlp-main.164) |  | 0 | Bilingual lexicons form a critical component of various natural language processing applications, including unsupervised and semisupervised machine translation and crosslingual information retrieval. In this work, we improve bilingual lexicon induction performance across 40 language pairs with a... | Ali SaadEldin, Carey E. Priebe, Kelly Marchisio, Kevin Duh, Philipp Koehn |  |
| 820 |  |  [Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection](https://doi.org/10.18653/v1/2022.emnlp-main.165) |  | 0 | Language models increasingly rely on massive web crawls for diverse text data. However, these sources are rife with undesirable content. As such, resources like Wikipedia, books, and news often serve as anchors for automatically selecting web text most suitable for language modeling, a process... | Dallas Card, Emily K. Gade, Leroy Z. Wang, Luke Zettlemoyer, Noah A. Smith, Sarah K. Dreier, Suchin Gururangan, Zeyu Wang |  |
| 821 |  |  [ConReader: Exploring Implicit Relations in Contracts for Contract Clause Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.166) |  | 0 | We study automatic Contract Clause Extraction (CCE) by modeling implicit relations in legal contracts. Existing CCE methods mostly treat contracts as plain text, creating a substantial barrier to understanding contracts of high complexity. In this work, we first comprehensively analyze the... | TatSeng Chua, Wai Lam, Weiwen Xu, Wenlong Zhao, Wenqiang Lei, Yang Deng |  |
| 822 |  |  [Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU](https://doi.org/10.18653/v1/2022.emnlp-main.167) |  | 0 | Curriculum Learning (CL) is a technique of training models via ranking examples in a typically increasing difficulty trend with the aim of accelerating convergence and improving generalisability. Current approaches for Natural Language Understanding (NLU) tasks use CL to improve in-distribution... | Fenia Christopoulou, Gerasimos Lampouras, Ignacio Iacobacci |  |
| 823 |  |  [Revisiting Parameter-Efficient Tuning: Are We Really There Yet?](https://doi.org/10.18653/v1/2022.emnlp-main.168) |  | 0 | Parameter-Efficient Tuning (PETuning) methods have been deemed by many as the new paradigm for using pretrained language models (PLMs). By tuning just a fraction amount of parameters comparing to full model finetuning, PETuning methods claim to have achieved performance on par with or even better... | Fangyu Liu, Guanzheng Chen, Shangsong Liang, Zaiqiao Meng |  |
| 824 |  |  [Transfer Learning from Semantic Role Labeling to Event Argument Extraction with Template-based Slot Querying](https://doi.org/10.18653/v1/2022.emnlp-main.169) |  | 0 | In this work, we investigate transfer learning from semantic role labeling (SRL) to event argument extraction (EAE), considering their similar argument structures. We view the extraction task as a role querying problem, unifying various methods into a single framework. There are key discrepancies... | Eduard H. Hovy, Emma Strubell, Zhisong Zhang |  |
| 825 |  |  [Calibrating Zero-shot Cross-lingual (Un-)structured Predictions](https://doi.org/10.18653/v1/2022.emnlp-main.170) |  | 0 | We investigate model calibration in the setting of zero-shot cross-lingual transfer with large-scale pre-trained language models. The level of model calibration is an important metric for evaluating the trustworthiness of predictive models. There exists an essential need for model calibration when... | Anqi Liu, Benjamin Van Durme, Zhengping Jiang |  |
| 826 |  |  [PRINCE: Prefix-Masked Decoding for Knowledge Enhanced Sequence-to-Sequence Pre-Training](https://doi.org/10.18653/v1/2022.emnlp-main.171) |  | 0 | Pre-trained Language Models (PLMs) have shown effectiveness in various Natural Language Processing (NLP) tasks. Denoising autoencoder is one of the most successful pre-training frameworks, learning to recompose the original text given a noise-corrupted one. The existing studies mainly focus on... | Haoran Li, Peng Yuan, Song Xu, Xiaodong He, Youzheng Wu |  |
| 827 |  |  [How Far are We from Robust Long Abstractive Summarization?](https://doi.org/10.18653/v1/2022.emnlp-main.172) |  | 0 | Abstractive summarization has made tremendous progress in recent years. In this work, we perform fine-grained human annotations to evaluate long document abstractive summarization systems (i.e., models and metrics) with the aim of implementing them to generate reliable summaries. For long document... | He Zhang, Huan Yee Koh, Jiaxin Ju, Ming Liu, Shirui Pan |  |
| 828 |  |  [Measuring Context-Word Biases in Lexical Semantic Datasets](https://doi.org/10.18653/v1/2022.emnlp-main.173) |  | 0 | State-of-the-art pretrained contextualized models (PCM) eg. BERT use tasks such as WiC and WSD to evaluate their word-in-context representations. This inherently assumes that performance in these tasks reflect how well a model represents the coupled word and context semantics. We question this... | Anna Korhonen, Diana McCarthy, Qianchu Liu |  |
| 829 |  |  [Iteratively Prompt Pre-trained Language Models for Chain of Thought](https://doi.org/10.18653/v1/2022.emnlp-main.174) |  | 0 | While Pre-trained Language Models (PLMs) internalize a great amount of world knowledge, they have been shown incapable of recalling these knowledge to solve tasks requiring complex & multi-step reasoning. Similar to how humans develop a “chain of thought” for these tasks, how can we equip PLMs with... | Boshi Wang, Huan Sun, Xiang Deng |  |
| 830 |  |  [Unobserved Local Structures Make Compositional Generalization Hard](https://doi.org/10.18653/v1/2022.emnlp-main.175) |  | 0 | While recent work has shown that sequence-to-sequence models struggle to generalize to new compositions (termed compositional generalization), little is known on what makes compositional generalization hard on a particular test instance. In this work, we investigate the factors that make... | Ben Bogin, Jonathan Berant, Shivanshu Gupta |  |
| 831 |  |  [Mitigating Data Sparsity for Short Text Topic Modeling by Topic-Semantic Contrastive Learning](https://doi.org/10.18653/v1/2022.emnlp-main.176) |  | 0 | To overcome the data sparsity issue in short text topic modeling, existing methods commonly rely on data augmentation or the data characteristic of short texts to introduce more word co-occurrence information. However, most of them do not make full use of the augmented data or the data... | Anh Tuan Luu, Xiaobao Wu, Xinshuai Dong |  |
| 832 |  |  [Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.177) |  | 0 | Multi-turn dialogue modeling as a challenging branch of natural language understanding (NLU), aims to build representations for machines to understand human dialogues, which provides a solid foundation for multiple downstream tasks. Recent studies of dialogue modeling commonly employ pre-trained... | Hai Zhao, Yiyang Li, Zhuosheng Zhang |  |
| 833 |  |  [Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates](https://doi.org/10.18653/v1/2022.emnlp-main.178) |  | 0 | Calibration strengthens the trustworthiness of black-box models by producing better accurate confidence estimates on given examples. However, little is known about if model explanations can help confidence calibration. Intuitively, humans look at important features attributions and decide whether... | Baotian Hu, Dongfang Li, Qingcai Chen |  |
| 834 |  |  [Non-Autoregressive Neural Machine Translation: A Call for Clarity](https://doi.org/10.18653/v1/2022.emnlp-main.179) |  | 0 | Non-autoregressive approaches aim to improve the inference speed of translation models by only requiring a single forward pass to generate the output sequence instead of iteratively producing each predicted token. Consequently, their translation quality still tends to be inferior to their... | Jonas Lööf, Robin M. Schmidt, Stephan Peitz, Telmo Pires |  |
| 835 |  |  [RED-ACE: Robust Error Detection for ASR using Confidence Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.180) |  | 0 | ASR Error Detection (AED) models aim to post-process the output of Automatic Speech Recognition (ASR) systems, in order to detect transcription errors. Modern approaches usually use text-based input, comprised solely of the ASR transcription hypothesis, disregarding additional signals from the ASR... | Dina Zverinski, Genady Beryozkin, Jonathan Mallinson, Zorik Gekhman |  |
| 836 |  |  [Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation](https://doi.org/10.18653/v1/2022.emnlp-main.181) |  | 0 | Chart-based models have shown great potential in unsupervised grammar induction, running recursively and hierarchically, but requiring O(n³) time-complexity. The Recursive Transformer based on Differentiable Trees (R2D2) makes it possible to scale to large language model pretraining even with a... | Gerard de Melo, Haitao Mi, Liang Li, Xiang Hu |  |
| 837 |  |  [A Localized Geometric Method to Match Knowledge in Low-dimensional Hyperbolic Space](https://doi.org/10.18653/v1/2022.emnlp-main.182) |  | 0 | Matching equivalent entities across Knowledge graphs is a pivotal step for knowledge fusion. Previous approaches usually study the problem in Euclidean space. However, recent works have shown that hyperbolic space has a higher capacity than Euclidean space and hyperbolic embedding can represent the... | Bo Hui, Tian Xia, WeiShinn Ku |  |
| 838 |  |  [Memory-assisted prompt editing to improve GPT-3 after deployment](https://doi.org/10.18653/v1/2022.emnlp-main.183) |  | 0 | Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret “What word is similar to good?” to mean a homophone, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with... | Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang |  |
| 839 |  |  [LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.184) |  | 0 | Multimodal Machine Translation (MMT) focuses on enhancing text-only translation with visual features, which has attracted considerable attention from both natural language processing and computer vision communities. Recent advances still struggle to train a separate model for each language pair,... | Dongdong Zhang, Haoyang Huang, Hongcheng Guo, Jiaheng Liu, Jian Yang, Zheng Cui, Zhoujun Li |  |
| 840 |  |  [PromptEHR: Conditional Electronic Healthcare Records Generation with Prompt Learning](https://doi.org/10.18653/v1/2022.emnlp-main.185) |  | 0 | Accessing longitudinal multimodal Electronic Healthcare Records (EHRs) is challenging due to privacy concerns, which hinders the use of ML for healthcare applications. Synthetic EHRs generation bypasses the need to share sensitive real patient records. However, existing methods generate... | Jimeng Sun, Zifeng Wang |  |
| 841 |  |  [ROSE: Robust Selective Fine-tuning for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.186) |  | 0 | Even though the large-scale language models have achieved excellent performances, they suffer from various adversarial attacks.A large body of defense methods has been proposed. However, they are still limited due to redundant attack search spaces and the inability to defend against various types... | Hao Zhou, Jie Zhou, Lan Jiang, Peng Li, Rui Jiang, Yankai Lin |  |
| 842 |  |  [CodeRetriever: A Large Scale Contrastive Pre-Training Method for Code Search](https://doi.org/10.18653/v1/2022.emnlp-main.187) |  | 0 | In this paper, we propose the CodeRetriever model, which learns the function-level code semantic representations through large-scale code-text contrastive pre-training. We adopt two contrastive learning schemes in CodeRetriever: unimodal contrastive learning and bimodal contrastive learning. For... | Bolun Yao, Daxin Jiang, Hang Zhang, Nan Duan, Weizhen Qi, Weizhu Chen, Xiaonan Li, Xipeng Qiu, Yelong Shen, Yeyun Gong |  |
| 843 |  |  [Open-Topic False Information Detection on Social Networks with Contrastive Adversarial Learning](https://doi.org/10.18653/v1/2022.emnlp-main.188) |  | 0 | Current works about false information detection based on conversation graphs on social networks focus primarily on two research streams from the standpoint of topic distribution: in-topic and cross-topic techniques, which assume that the data topic distribution is identical or cross, respectively.... | Chunming Hu, Guanghui Ma, Hong Zhang, Ling Ge |  |
| 844 |  |  [Mitigating Inconsistencies in Multimodal Sentiment Analysis under Uncertain Missing Modalities](https://doi.org/10.18653/v1/2022.emnlp-main.189) |  | 0 | For the missing modality problem in Multimodal Sentiment Analysis (MSA), the inconsistency phenomenon occurs when the sentiment changes due to the absence of a modality. The absent modality that determines the overall semantic can be considered as a key missing modality. However, previous works all... | Jiandian Zeng, Jiantao Zhou, Tianyi Liu |  |
| 845 |  |  [ConvTrans: Transforming Web Search Sessions for Conversational Dense Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.190) |  | 0 | Conversational search provides users with a natural and convenient new search experience. Recently, conversational dense retrieval has shown to be a promising technique for realizing conversational search. However, as conversational search systems have not been widely deployed, it is hard to get... | Fengran Mo, Hongjin Qian, Kelong Mao, Xiaohua Cheng, Zhao Cao, Zhicheng Dou |  |
| 846 |  |  [MUSIED: A Benchmark for Event Detection from Multi-Source Heterogeneous Informal Texts](https://doi.org/10.18653/v1/2022.emnlp-main.191) |  | 0 | Event detection (ED) identifies and classifies event triggers from unstructured texts, serving as a fundamental task for information extraction. Despite the remarkable progress achieved in the past several years, most research efforts focus on detecting events from formal texts (e.g., news... | Fan Yang, Guanglu Wan, Jianwei Lv, Shuaipeng Liu, Wei Ye, Xiangyu Xi |  |
| 847 |  |  [Reproducibility Issues for BERT-based Evaluation Metrics](https://doi.org/10.18653/v1/2022.emnlp-main.192) |  | 0 | Reproducibility is of utmost concern in machine learning and natural language processing (NLP). In the field of natural language generation (especially machine translation), the seminal paper of Post (2018) has pointed out problems of reproducibility of the dominant metric, BLEU, at the time of... | Jonas Belouadi, Steffen Eger, Yanran Chen |  |
| 848 |  |  [Improving Multi-task Stance Detection with Multi-task Interaction Network](https://doi.org/10.18653/v1/2022.emnlp-main.193) |  | 0 | Stance detection aims to identify people’s standpoints expressed in the text towards a target, which can provide powerful information for various downstream tasks.Recent studies have proposed multi-task learning models that introduce sentiment information to boost stance detection.However, they... | Binxing Fang, Heyan Chai, Jinhao Cui, Qing Liao, Siyu Tang, Ye Ding |  |
| 849 |  |  [Neural-based Mixture Probabilistic Query Embedding for Answering FOL queries on Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.194) |  | 0 | Query embedding (QE)—which aims to embed entities and first-order logical (FOL) queries in a vector space, has shown great power in answering FOL queries on knowledge graphs (KGs). Existing QE methods divide a complex query into a sequence of mini-queries according to its computation graph and... | Aodi Li, Houqiang Li, Liansheng Zhuang, Shafei Wang, Xiao Long |  |
| 850 |  |  [Improving Multi-turn Emotional Support Dialogue Generation with Lookahead Strategy Planning](https://doi.org/10.18653/v1/2022.emnlp-main.195) |  | 0 | Providing Emotional Support (ES) to soothe people in emotional distress is an essential capability in social interactions. Most existing researches on building ES conversation systems only considered single-turn interactions with users, which was over-simplified. In comparison, multi-turn ES... | Bang Liu, Jiashuo Wang, Ruihui Zhao, Wenge Liu, Wenjie Li, Xiaodan Liang, Yefeng Zheng, Yi Cheng |  |
| 851 |  |  [Conformal Predictor for Improving Zero-Shot Text Classification Efficiency](https://doi.org/10.18653/v1/2022.emnlp-main.196) |  | 0 | Pre-trained language models (PLMs) have been shown effective for zero-shot (0shot) text classification. 0shot models based on natural language inference (NLI) and next sentence prediction (NSP) employ cross-encoder architecture and infer by making a forward pass through the model for each... | ChienSheng Wu, Nazneen Rajani, Prafulla Kumar Choubey, Wenhao Liu, Yu Bai |  |
| 852 |  |  [Effective and Efficient Query-aware Snippet Extraction for Web Search](https://doi.org/10.18653/v1/2022.emnlp-main.197) |  | 0 | Query-aware webpage snippet extraction is widely used in search engines to help users better understand the content of the returned webpages before clicking. The extracted snippet is expected to summarize the webpage in the context of the input query. Existing snippet extraction methods mainly rely... | Binxing Jiao, Chuhan Wu, Fangzhao Wu, Guangzhong Sun, Jingwei Yi, Xiaolong Huang, Xing Xie |  |
| 853 |  |  [You Only Need One Model for Open-domain Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.198) |  | 0 | Recent approaches to Open-domain Question Answering refer to an external knowledge base using a retriever model, optionally rerank passages with a separate reranker model and generate an answer using another reader model. Despite performing related tasks, the models have separate parameters and are... | Akhil Kedia, Ashwin Paranjape, Christopher D. Manning, Haejun Lee, Jongwon Lee, KyoungGu Woo |  |
| 854 |  |  [Generative Entity Typing with Curriculum Learning](https://doi.org/10.18653/v1/2022.emnlp-main.199) |  | 0 | Entity typing aims to assign types to the entity mentions in given texts. The traditional classification-based entity typing paradigm has two unignorable drawbacks: 1) it fails to assign an entity to the types beyond the predefined type set, and 2) it can hardly handle few-shot and zero-shot... | Deqing Yang, Jiaqing Liang, Jingyue Huang, Jinxi Liu, Siyu Yuan, Yanghua Xiao, Zhixu Li |  |
| 855 |  |  [SetGNER: General Named Entity Recognition as Entity Set Generation](https://doi.org/10.18653/v1/2022.emnlp-main.200) |  | 0 | Recently, joint recognition of flat, nested and discontinuous entities has received increasing attention. Motivated by the observation that the target output of NER is essentially a set of sequences, we propose a novel entity set generation framework for general NER scenes in this paper. Different... | Buzhou Tang, Yuxin He |  |
| 856 |  |  [Opinion Summarization by Weak-Supervision from Mix-structured Data](https://doi.org/10.18653/v1/2022.emnlp-main.201) |  | 0 | Opinion summarization of multiple reviews suffers from the lack of reference summaries for training.Most previous approaches construct multiple reviews and their summary based on textual similarities between reviews,resulting in information mismatch between the review input and the summary. In this... | Kenny Q. Zhu, Qi Jia, Yizhu Liu |  |
| 857 |  |  [Multi-level Distillation of Semantic Knowledge for Pre-training Multilingual Language Model](https://doi.org/10.18653/v1/2022.emnlp-main.202) |  | 0 | Pre-trained multilingual language models play an important role in cross-lingual natural language understanding tasks. However, existing methods did not focus on learning the semantic structure of representation, and thus could not optimize their performance. In this paper, we propose Multi-level... | Dan Zhang, Fei Ding, Feng Luo, Hongxin Hu, Long Cheng, Mingqi Li |  |
| 858 |  |  [Empowering Dual-Encoder with Query Generator for Cross-Lingual Dense Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.203) |  | 0 | In monolingual dense retrieval, lots of works focus on how to distill knowledge from cross-encoder re-ranker to dual-encoder retriever and these methods achieve better performance due to the effectiveness of cross-encoder re-ranker. However, we find that the performance of the cross-encoder... | Daxin Jiang, Houxing Ren, Linjun Shou, Ming Gong, Ning Wu |  |
| 859 |  |  [R2F: A General Retrieval, Reading and Fusion Framework for Document-level Natural Language Inference](https://doi.org/10.18653/v1/2022.emnlp-main.204) |  | 0 | Document-level natural language inference (DOCNLI) is a new challenging task in natural language processing, aiming at judging the entailment relationship between a pair of hypothesis and premise documents. Current datasets and baselines largely follow sentence-level settings, but fail to address... | Hao Wang, Jing Shao, Kun Wang, Yangguang Li, Yixin Cao, Zhen Huang |  |
| 860 |  |  [Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-main.205) |  | 0 | There is a growing body of work in recent years to develop pre-trained language models (PLMs) for the Arabic language. This work addresses two major problems in existing Arabic PLMs that limit the progress of the Arabic NLU and NLG fields. First, existing Arabic PLMs are not well-explored and their... | Abbas Ghaddar, Ahmad Rashid, Baoxing Huai, Chao Xing, Khalil Bibi, Mehdi Rezagholizadeh, Philippe Langlais, Qun Liu, Sunyam Bagga, Xin Jiang, Xinyu Duan, Yasheng Wang, Yimeng Wu, Zhefeng Wang |  |
| 861 |  |  [KECP: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.206) |  | 0 | Extractive Question Answering (EQA) is one of the most essential tasks in Machine Reading Comprehension (MRC), which can be solved by fine-tuning the span selecting heads of Pre-trained Language Models (PLMs). However, most existing approaches for MRC may perform poorly in the few-shot learning... | Chengyu Wang, Hongbin Wang, Jianing Wang, Jun Huang, Ming Gao, Minghui Qiu, Qiuhui Shi |  |
| 862 |  |  [Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.207) |  | 0 | Knowledge-enhanced Pre-trained Language Model (PLM) has recently received significant attention, which aims to incorporate factual knowledge into PLMs. However, most existing methods modify the internal structures of fixed types of PLMs by stacking complicated modules, and introduce redundant and... | Hongbin Wang, Jianing Wang, Ming Gao, Minghui Qiu, Qiuhui Shi, Wenkang Huang, Xiang Li |  |
| 863 |  |  [On the Evaluation Metrics for Paraphrase Generation](https://doi.org/10.18653/v1/2022.emnlp-main.208) |  | 0 | In this paper we revisit automatic metrics for paraphrase evaluation and obtain two findings that disobey conventional wisdom: (1) Reference-free metrics achieve better performance than their reference-based counterparts. (2) Most commonly used metrics do not align well with human... | Haiyun Jiang, Lemao Liu, Lingfeng Shen, Shuming Shi |  |
| 864 |  |  [Curriculum Learning Meets Weakly Supervised Multimodal Correlation Learning](https://doi.org/10.18653/v1/2022.emnlp-main.209) |  | 0 | In the field of multimodal sentiment analysis (MSA), a few studies have leveraged the inherent modality correlation information stored in samples for self-supervised learning. However, they feed the training pairs in a random order without consideration of difficulty. Without human annotation, the... | Haifeng Hu, Sijie Mai, Ya Sun |  |
| 865 |  |  [Rethinking Positional Encoding in Tree Transformer for Code Representation](https://doi.org/10.18653/v1/2022.emnlp-main.210) |  | 0 | Transformers are now widely used in code representation, and several recent works further develop tree Transformers to capture the syntactic structure in source code. Specifically, novel tree positional encodings have been proposed to incorporate inductive bias into Transformer.In this work, we... | Ge Li, Han Peng, Yunfei Zhao, Zhi Jin |  |
| 866 |  |  [RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL](https://doi.org/10.18653/v1/2022.emnlp-main.211) |  | 0 | Relational structures such as schema linking and schema encoding have been validated as a key component to qualitatively translating natural language into SQL queries. However, introducing these structural relations comes with prices: they often result in a specialized model structure, which... | Chenghu Zhou, Jiexing Qi, Jingyao Tang, Quanshi Zhang, Xiangpeng Wan, Xinbing Wang, Yu Cheng, Zhouhan Lin, Ziwei He |  |
| 867 |  |  [COM-MRC: A COntext-Masked Machine Reading Comprehension Framework for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.212) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) aims to extract sentiment triplets from sentences, which was recently formalized as an effective machine reading comprehension (MRC) based framework. However, when facing multiple aspect terms, the MRC-based methods could fail due to the interference from... | Fangxiang Feng, Hao Chen, Ruifan Li, Xiaojie Wang, Zepeng Zhai |  |
| 868 |  |  [CEM: Machine-Human Chatting Handoff via Causal-Enhance Module](https://doi.org/10.18653/v1/2022.emnlp-main.213) |  | 0 | Aiming to ensure chatbot quality by predicting chatbot failure and enabling human-agent collaboration, Machine-Human Chatting Handoff (MHCH) has attracted lots of attention from both industry and academia in recent years. However, most existing methods mainly focus on the dialogue context or assist... | Daifeng Li, Jinghui Qin, ShanShan Zhong, Zhongzhan Huang |  |
| 869 |  |  [Nearest Neighbor Zero-Shot Inference](https://doi.org/10.18653/v1/2022.emnlp-main.214) |  | 0 | Retrieval-augmented language models (LMs) use non-parametric memory to substantially outperform their non-retrieval counterparts on perplexity-based evaluations, but it is an open question whether they achieve similar gains in few- and zero-shot end-task accuracy. We extensively study one such... | Julian Michael, Luke Zettlemoyer, Suchin Gururangan, Weijia Shi |  |
| 870 |  |  [Robots-Dont-Cry: Understanding Falsely Anthropomorphic Utterances in Dialog Systems](https://doi.org/10.18653/v1/2022.emnlp-main.215) |  | 0 | Dialog systems are often designed or trained to output human-like responses. However, some responses may be impossible for a machine to truthfully say (e.g. “that movie made me cry”). Highly anthropomorphic responses might make users uncomfortable or implicitly deceive them into thinking they are... | David Gros, Yu Li, Zhou Yu |  |
| 871 |  |  [A Joint Learning Framework for Restaurant Survival Prediction and Explanation](https://doi.org/10.18653/v1/2022.emnlp-main.216) |  | 0 | The bloom of the Internet and the recent breakthroughs in deep learning techniques open a new door to AI for E-commence, with a trend of evolving from using a few financial factors such as liquidity and profitability to using more advanced AI techniques to process complex and multi-modal data. In... | Hao Liao, MingYang Zhou, Peng Jia, Rui Mao, Xiaojie Zhang, Xin Li, Xing Xie |  |
| 872 |  |  [Making Pretrained Language Models Good Long-tailed Learners](https://doi.org/10.18653/v1/2022.emnlp-main.217) |  | 0 | Prompt-tuning has shown appealing performance in few-shot classification by virtue of its capability in effectively exploiting pre-trained knowledge. This motivates us to check the hypothesis that prompt-tuning is also a promising choice for long-tailed classification, since the tail classes are... | Chen Zhang, Dawei Song, Jingang Wang, Lei Ren, Wei Wu |  |
| 873 |  |  [UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression](https://doi.org/10.18653/v1/2022.emnlp-main.218) |  | 0 | Geometry problem solving is a well-recognized testbed for evaluating the high-level multi-modal reasoning capability of deep models. In most existing works, two main geometry problems: calculation and proving, are usually treated as two specific tasks, hindering a deep model to unify its reasoning... | Chongyu Chen, Jiaqi Chen, Jinghui Qin, Liang Lin, Pan Lu, Tong Li, Xiaodan Liang |  |
| 874 |  |  [Face-Sensitive Image-to-Emotional-Text Cross-modal Translation for Multimodal Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.219) |  | 0 | Aspect-level multimodal sentiment analysis, which aims to identify the sentiment of the target aspect from multimodal data, recently has attracted extensive attention in the community of multimedia and natural language processing. Despite the recent success in textual aspect-based sentiment... | Bing Qin, Hao Yang, Yanyan Zhao |  |
| 875 |  |  [FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.220) |  | 0 | Recent model-based reference-free metrics for open-domain dialogue evaluation exhibit promising correlations with human judgment. However, they either perform turn-level evaluation or look at a single dialogue quality dimension. One would expect a good evaluation metric to assess multiple quality... | Chen Zhang, Haizhou Li, Luis Fernando D'Haro, Qiquan Zhang, Thomas Friedrichs |  |
| 876 |  |  [Sentence Representation Learning with Generative Objective rather than Contrastive Objective](https://doi.org/10.18653/v1/2022.emnlp-main.221) |  | 0 | Though offering amazing contextualized token-level representations, current pre-trained language models take less attention on accurately acquiring sentence-level representation during their self-supervised pre-training. However, contrastive objectives which dominate the current sentence... | Bohong Wu, Hai Zhao |  |
| 877 |  |  [RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning](https://doi.org/10.18653/v1/2022.emnlp-main.222) |  | 0 | Prompting has shown impressive success in enabling large pre-trained language models (LMs) to perform diverse NLP tasks, especially with only few downstream data. Automatically finding the optimal prompt for each task, however, is challenging. Most existing work resorts to tuning \*soft\* prompts... | ChengPing Hsieh, Eric P. Xing, Han Guo, Jianyu Wang, Meng Song, Mingkai Deng, Tianmin Shu, Yihan Wang, Zhiting Hu |  |
| 878 |  |  [DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.223) |  | 0 | Prompt learning with immensely large Casual Language Models (CLMs) has been shown promising for attribute-controllable text generation (CTG). However, vanilla prompt tuning tends to imitate training corpus characteristics beyond the control attributes, resulting in a poor generalization ability.... | Dawei Song, Hanqing Zhang |  |
| 879 |  |  [CPL: Counterfactual Prompt Learning for Vision and Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.224) |  | 0 | Prompt tuning is a new few-shot transfer learning technique that only tunes the learnable prompt for pre-trained vision and language models such as CLIP. However, existing prompt tuning methods tend to learn spurious or entangled representations, which leads to poor generalization to unseen... | Arjun R. Akula, Diji Yang, Pradyumna Narayana, Sugato Basu, TsuJui Fu, Varun Jampani, Weixi Feng, William Yang Wang, Xin Wang, Xuehai He |  |
| 880 |  |  [Red Teaming Language Models with Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.225) |  | 0 | Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of... | Amelia Glaese, Ethan Perez, Geoffrey Irving, H. Francis Song, John Aslanides, Nat McAleese, Roman Ring, Saffron Huang, Trevor Cai |  |
| 881 |  |  [CapOnImage: Context-driven Dense-Captioning on Image](https://doi.org/10.18653/v1/2022.emnlp-main.226) |  | 0 | Existing image captioning systems are dedicated to generating narrative captions for images, which are spatially detached from theimage in presentation. However, texts can also be used as decorations on the image to highlight the key points and increase theattractiveness of images. In this work, we... | Peng Wang, Tiezheng Ge, Xinglin Hou, Yiqi Gao, Yuanmeng Zhang, Yuning Jiang |  |
| 882 |  |  [SpanProto: A Two-stage Span-based Prototypical Network for Few-shot Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.227) |  | 0 | Few-shot Named Entity Recognition (NER) aims to identify named entities with very little annotated data. Previous methods solve this problem based on token-wise classification, which ignores the information of entity boundaries, and inevitably the performance is affected by the massive non-entity... | Chengyu Wang, Chuanqi Tan, Jianing Wang, Jun Huang, Ming Gao, Minghui Qiu, Songfang Huang |  |
| 883 |  |  [Discovering Differences in the Representation of People using Contextualized Semantic Axes](https://doi.org/10.18653/v1/2022.emnlp-main.228) |  | 0 | A common paradigm for identifying semantic differences across social and temporal contexts is the use of static word embeddings and their distances. In particular, past work has compared embeddings against “semantic axes” that represent two opposing concepts. We extend this paradigm to BERT... | David Bamman, Divya Tadimeti, Li Lucy |  |
| 884 |  |  [Generating Literal and Implied Subquestions to Fact-check Complex Claims](https://doi.org/10.18653/v1/2022.emnlp-main.229) |  | 0 | Verifying political claims is a challenging task, as politicians can use various tactics to subtly misrepresent the facts for their agenda. Existing automatic fact-checking systems fall short here, and their predictions like “half-true” are not very useful in isolation, since it is unclear which... | Aniruddh Sriram, Eunsol Choi, Greg Durrett, Jifan Chen |  |
| 885 |  |  [Machine Translation Robustness to Natural Asemantic Variation](https://doi.org/10.18653/v1/2022.emnlp-main.230) |  | 0 | Current Machine Translation (MT) models still struggle with more challenging input, such as noisy data and tail-end words and phrases. Several works have addressed this robustness issue by identifying specific categories of noise and variation then tuning models to perform better on them. An... | Jacob Bremerman, Jonathan May, Xiang Ren |  |
| 886 |  |  [Natural Language to Code Translation with Execution](https://doi.org/10.18653/v1/2022.emnlp-main.231) |  | 0 | Generative models of code, pretrained on large corpora of programs, have shown great success in translating natural language to code (Chen et al., 2021; Austin et al., 2021; Li et al., 2022, inter alia). While these models do not explicitly incorporate program semantics (i.e., execution results)... | Daniel Fried, Freda Shi, Luke Zettlemoyer, Marjan Ghazvininejad, Sida I. Wang |  |
| 887 |  |  [Life is a Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes](https://doi.org/10.18653/v1/2022.emnlp-main.232) |  | 0 | Analogy-making gives rise to reasoning, abstraction, flexible categorization and counterfactual inference – abilities lacking in even the best AI systems today. Much research has suggested that analogies are key to non-brittle systems that can adapt to new domains. Despite their importance,... | Dafna Shahaf, Oren Sultan |  |
| 888 |  |  [Language Contamination Helps Explains the Cross-lingual Capabilities of English Pretrained Models](https://doi.org/10.18653/v1/2022.emnlp-main.233) |  | 0 | English pretrained language models, which make up the backbone of many modern NLP systems, require huge amounts of unlabeled training data. These models are generally presented as being trained only on English text but have been found to transfer surprisingly well to other languages. We investigate... | Luke Zettlemoyer, Terra Blevins |  |
| 889 |  |  [Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.234) |  | 0 | The emergent cross-lingual transfer seen in multilingual pretrained models has sparked significant interest in studying their behavior. However, because these analyses have focused on fully trained multilingual models, little is known about the dynamics of the multilingual pretraining process. We... | Hila Gonen, Luke Zettlemoyer, Terra Blevins |  |
| 890 |  |  [Neural Machine Translation with Contrastive Translation Memories](https://doi.org/10.18653/v1/2022.emnlp-main.235) |  | 0 | Retrieval-augmented Neural Machine Translation models have been successful in many translation scenarios. Different from previous works that make use of mutually similar but redundant translation memories (TMs), we propose a new retrieval-augmented NMT to model contrastively retrieved translation... | Dongyan Zhao, Lemao Liu, Rui Yan, Shen Gao, Xin Cheng |  |
| 891 |  |  [Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.236) |  | 0 | Continual Learning for Named Entity Recognition (CL-NER) aims to learn a growing number of entity types over time from a stream of data. However, simply learning Other-Class in the same way as new entity types amplifies the catastrophic forgetting and leads to a substantial performance drop. The... | Haibin Chen, Junhao Zheng, Qianli Ma, Zhanxian Liang |  |
| 892 |  |  [Exploring the Secrets Behind the Learning Difficulty of Meaning Representations for Semantic Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.237) |  | 0 | Previous research has shown that the design of Meaning Representation (MR) greatly influences the final model performance of a neural semantic parser. Therefore, designing a good MR is a long-term goal for semantic parsing. However, it is still an art as there is no quantitative indicator that can... | JianGuang Lou, Jiaqi Guo, Qian Liu, Tao Xie, Zhenwen Li |  |
| 893 |  |  [That's the Wrong Lung! Evaluating and Improving the Interpretability of Unsupervised Multimodal Encoders for Medical Data](https://doi.org/10.18653/v1/2022.emnlp-main.238) |  | 0 | Pretraining multimodal models on Electronic Health Records (EHRs) provides a means of learning representations that can transfer to downstream tasks with minimal supervision. Recent multimodal models induce soft local alignments between image regions and sentences. This is of particular interest in... | Byron C. Wallace, Denis Jered McInerney, Geoffrey S. Young, JanWillem van de Meent |  |
| 894 |  |  [Unsupervised Tokenization Learning](https://doi.org/10.18653/v1/2022.emnlp-main.239) |  | 0 | In the presented study, we discover that the so-called “transition freedom” metric appears superior for unsupervised tokenization purposes in comparison to statistical metrics such as mutual information and conditional probability, providing F-measure scores in range from 0.71 to 1.0 across... | Anton Kolonin, Vignav Ramesh |  |
| 895 |  |  [A Template-based Method for Constrained Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.240) |  | 0 | Machine translation systems are expected to cope with various types of constraints in many practical scenarios. While neural machine translation (NMT) has achieved strong performance in unconstrained cases, it is non-trivial to impose pre-specified constraints into the translation process of NMT... | Maosong Sun, Peng Li, Shuo Wang, Yang Liu, Zhaopeng Tu, Zhixing Tan |  |
| 896 |  |  [PATS: Sensitivity-aware Noisy Learning for Pretrained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.241) |  | 0 | A wide range of NLP tasks benefit from the fine-tuning of pretrained language models (PLMs). However, a number of redundant parameters which contribute less to the downstream task are observed in a directly fine-tuned model. We consider the gap between pretraining and downstream tasks hinders the... | Hongzhi Zhang, Sirui Wang, Wei Wu, Yupeng Zhang, Zhoujun Li |  |
| 897 |  |  [Towards Reinterpreting Neural Topic Models via Composite Activations](https://doi.org/10.18653/v1/2022.emnlp-main.242) |  | 0 | Most Neural Topic Models (NTM) use a variational auto-encoder framework producing K topics limited to the size of the encoder’s output. These topics are interpreted through the selection of the top activated words via the weights or reconstructed vector of the decoder that are directly connected to... | Hady W. Lauw, Jia Peng Lim |  |
| 898 |  |  [Few-shot Query-Focused Summarization with Prefix-Merging](https://doi.org/10.18653/v1/2022.emnlp-main.243) |  | 0 | Query-focused summarization has been considered as an important extension for text summarization. It aims to generate a concise highlight for a given query. Different from text summarization, query-focused summarization has long been plagued by the problem of lacking high-quality large-scale... | Ruifeng Yuan, Wenjie Li, Zili Wang, Ziqiang Cao |  |
| 899 |  |  [Cross-Align: Modeling Deep Cross-lingual Interactions for Word Alignment](https://doi.org/10.18653/v1/2022.emnlp-main.244) |  | 0 | Word alignment which aims to extract lexicon translation equivalents between source and target sentences, serves as a fundamental tool for natural language processing. Recent studies in this area have yielded substantial improvements by generating alignments from contextualized embeddings of the... | Fandong Meng, Jie Zhou, Jinan Xu, Siyu Lai, Yufeng Chen, Zhen Yang |  |
| 900 |  |  [BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.245) |  | 0 | Automatic evaluation metrics are crucial to the development of generative systems. In recent years, pre-trained language model (PLM) based metrics, such as BERTScore, have been commonly adopted in various generation tasks. However, it has been demonstrated that PLMs encode a range of stereotypical... | Junliang He, Tianxiang Sun, Xipeng Qiu, Xuanjing Huang |  |
| 901 |  |  [HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.246) |  | 0 | Hierarchical text classification (HTC) is a challenging subtask of multi-label classification due to its complex label hierarchy.Recently, the pretrained language models (PLM)have been widely adopted in HTC through a fine-tuning paradigm. However, in this paradigm, there exists a huge gap between... | Binghuai Lin, Houfeng Wang, Peiyi Wang, Tianyu Liu, Yunbo Cao, Zhifang Sui, Zihan Wang |  |
| 902 |  |  [Not to Overfit or Underfit the Source Domains? An Empirical Study of Domain Generalization in Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.247) |  | 0 | Machine learning models are prone to overfitting their training (source) domains, which is commonly believed to be the reason why they falter in novel target domains. Here we examine the contrasting view that multi-source domain generalization (DG) is first and foremost a problem of mitigating... | Avi Sil, Md. Arafat Sultan, Radu Florian |  |
| 903 |  |  [Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs](https://doi.org/10.18653/v1/2022.emnlp-main.248) |  | 0 | Social intelligence and Theory of Mind (TOM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allows humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social... | Daniel Fried, Maarten Sap, Ronan Le Bras, Yejin Choi |  |
| 904 |  |  [Improving Passage Retrieval with Zero-Shot Question Generation](https://doi.org/10.18653/v1/2022.emnlp-main.249) |  | 0 | We propose a simple and effective re-ranking method for improving passage retrieval in open question answering. The re-ranker re-scores retrieved passages with a zero-shot question generation model, which uses a pre-trained language model to compute the probability of the input question conditioned... | Armen Aghajanyan, Devendra Singh Sachan, Joelle Pineau, Luke Zettlemoyer, Mandar Joshi, Mike Lewis, Wentau Yih |  |
| 905 |  |  [Summarizing Community-based Question-Answer Pairs](https://doi.org/10.18653/v1/2022.emnlp-main.250) |  | 0 | Community-based Question Answering (CQA), which allows users to acquire their desired information, has increasingly become an essential component of online services in various domains such as E-commerce, travel, and dining. However, an overwhelming number of CQA pairs makes it difficult for users... | TingYao Hsu, Xiaolan Wang, Yoshi Suhara |  |
| 906 |  |  [Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models](https://doi.org/10.18653/v1/2022.emnlp-main.251) |  | 0 | Current Natural Language Inference (NLI) models achieve impressive results, sometimes outperforming humans when evaluating on in-distribution test sets. However, as these models are known to learn from annotation artefacts and dataset biases, it is unclear to what extent the models are learning the... | Haim Dubossarsky, Joe Stacey, Marek Rei, Pasquale Minervini |  |
| 907 |  |  [How to disagree well: Investigating the dispute tactics used on Wikipedia](https://doi.org/10.18653/v1/2022.emnlp-main.252) |  | 0 | Disagreements are frequently studied from the perspective of either detecting toxicity or analysing argument structure. We propose a framework of dispute tactics which unifies these two perspectives, as well as other dialogue acts which play a role in resolving disputes, such as asking questions... | Andreas Vlachos, Christine de Kock |  |
| 908 |  |  [Chapter Ordering in Novels](https://doi.org/10.18653/v1/2022.emnlp-main.253) |  | 0 | Understanding narrative flow and text coherence in long-form documents (novels) remains an open problem in NLP.To gain insight, we explore the task of chapter ordering, reconstructing the original order of chapters in novel given a random permutation of the text. This can be seen as extending the... | Allen Kim, Steven Skiena |  |
| 909 |  |  [Open-ended Knowledge Tracing for Computer Science Education](https://doi.org/10.18653/v1/2022.emnlp-main.254) |  | 0 | In educational applications, knowledge tracing refers to the problem of estimating students’ time-varying concept/skill mastery level from their past responses to questions and predicting their future performance.One key limitation of most existing knowledge tracing methods is that they treat... | Andrew S. Lan, Naiming Liu, Richard G. Baraniuk, Zichao Wang |  |
| 910 |  |  [Logical Neural Networks for Knowledge Base Completion with Embeddings & Rules](https://doi.org/10.18653/v1/2022.emnlp-main.255) |  | 0 | Knowledge base completion (KBC) has benefitted greatly by learning explainable rules in an human-interpretable dialect such as first-order logic. Rule-based KBC has so far, mainly focussed on learning one of two types of rules: conjunction-of-disjunctions and disjunction-of-conjunctions. We... | Alexander G. Gray, Breno W. S. R. de Carvalho, Ibrahim Abdelaziz, Pavan Kapanipathi, Prithviraj Sen, Salim Roukos |  |
| 911 |  |  [MedCLIP: Contrastive Learning from Unpaired Medical Images and Text](https://doi.org/10.18653/v1/2022.emnlp-main.256) |  | 0 | Existing vision-text contrastive learning like CLIP aims to match the paired image and caption embeddings while pushing others apart, which improves representation transferability and supports zero-shot prediction. However, medical image-text datasets are orders of magnitude below the general... | Dinesh Agarwal, Jimeng Sun, Zhenbang Wu, Zifeng Wang |  |
| 912 |  |  [GA-SAM: Gradient-Strength based Adaptive Sharpness-Aware Minimization for Improved Generalization](https://doi.org/10.18653/v1/2022.emnlp-main.257) |  | 0 | Recently, Sharpness-Aware Minimization (SAM) algorithm has shown state-of-the-art generalization abilities in vision tasks. It demonstrates that flat minima tend to imply better generalization abilities. However, it has some difficulty implying SAM to some natural language tasks, especially to... | Qi Su, Ruixuan Luo, Xu Sun, Zhiyuan Zhang |  |
| 913 |  |  [Sparse Teachers Can Be Dense with Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.258) |  | 0 | Recent advances in distilling pretrained language models have discovered that, besides the expressiveness of knowledge, the student-friendliness should be taken into consideration to realize a truly knowledgeable teacher. Based on a pilot study, we find that over-parameterized teachers can produce... | Chen Zhang, Dawei Song, Yi Yang |  |
| 914 |  |  [BBTv2: Towards a Gradient-Free Future with Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.259) |  | 0 | Most downstream adaptation methods tune all or part of the parameters of pre-trained models (PTMs) through gradient descent, where the tuning cost increases linearly with the growth of the model size.By contrast, gradient-free methods only require the forward computation of the PTM to tune the... | Hong Qian, Tianxiang Sun, Xipeng Qiu, Xuanjing Huang, Yunhua Zhou, Zhengfu He |  |
| 915 |  |  [Passage-Mask: A Learnable Regularization Strategy for Retriever-Reader Models](https://doi.org/10.18653/v1/2022.emnlp-main.260) |  | 0 | Retriever-reader models achieve competitive performance across many different NLP tasks such as open question answering and dialogue conversations. In this work, we notice these models easily overfit the top-rank retrieval passages and standard training fails to reason over the entire retrieval... | Chengyue Gong, Shujian Zhang, Xingchao Liu |  |
| 916 |  |  [Mixed-effects transformers for hierarchical adaptation](https://doi.org/10.18653/v1/2022.emnlp-main.261) |  | 0 | Language differs dramatically from context to context. To some degree, large language models like GPT-3 account for such variation by conditioning on strings of initial input text, or prompts. However, prompting can be ineffective when contexts are sparse, out-of-sample, or extra-textual. In this... | Julia White, Noah D. Goodman, Robert X. D. Hawkins |  |
| 917 |  |  [On Measuring the Intrinsic Few-Shot Hardness of Datasets](https://doi.org/10.18653/v1/2022.emnlp-main.262) |  | 0 | While advances in pre-training have led to dramatic improvements in few-shot learning of NLP tasks, there is limited understanding of what drives successful few-shot adaptation in datasets. In particular, given a new dataset and a pre-trained model, what properties of the dataset make it few-shot... | Christopher D. Manning, Shikhar Murty, Xinran Zhao |  |
| 918 |  |  [Group is better than individual: Exploiting Label Topologies and Label Relations for Joint Multiple Intent Detection and Slot Filling](https://doi.org/10.18653/v1/2022.emnlp-main.263) |  | 0 | Recent joint multiple intent detection and slot filling models employ label embeddings to achieve the semantics-label interactions.However, they treat all labels and label embeddings as uncorrelated individuals, ignoring the dependencies among them. Besides, they conduct the decoding for the two... | Bowen Xing, Ivor W. Tsang |  |
| 919 |  |  [An Empirical Study on Finding Spans](https://doi.org/10.18653/v1/2022.emnlp-main.264) |  | 0 | We present an empirical study on methods for span finding, the selection of consecutive tokens in text for some downstream tasks. We focus on approaches that can be employed in training end-to-end information extraction systems, and find there is no definitive solution without considering task... | Benjamin Van Durme, Boyuan Zheng, Tongfei Chen, Weiwei Gu, Yunmo Chen |  |
| 920 |  |  [MGDoc: Pre-training with Multi-granular Hierarchy for Document Image Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.265) |  | 0 | Document images are a ubiquitous source of data where the text is organized in a complex hierarchical structure ranging from fine granularity (e.g., words), medium granularity (e.g., regions such as paragraphs or figures), to coarse granularity (e.g., the whole page). The spatial hierarchical... | Ani Nenkova, Chris Tensmeyer, Jingbo Shang, Jiuxiang Gu, Nikolaos Barmpalios, Tong Sun, Vlad I. Morariu, Zilong Wang |  |
| 921 |  |  [Understanding Jargon: Combining Extraction and Generation for Definition Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.266) |  | 0 | Can machines know what twin prime is? From the composition of this phrase, machines may guess twin prime is a certain kind of prime, but it is still difficult to deduce exactly what twin stands for without additional knowledge. Here, twin prime is a jargon - a specialized term used by experts in a... | Hanyin Shao, Jie Huang, Jinjun Xiong, Kevin ChenChuan Chang, WenMei Hwu |  |
| 922 |  |  [ProsocialDialog: A Prosocial Backbone for Conversational Agents](https://doi.org/10.18653/v1/2022.emnlp-main.267) |  | 0 | Most existing dialogue systems fail to respond properly to potentially unsafe user utterances by either ignoring or passively agreeing with them. To address this issue, we introduce ProsocialDialog, the first large-scale multi-turn dialogue dataset to teach conversational agents to respond to... | Daniel Khashabi, Gunhee Kim, Hyunwoo Kim, Liwei Jiang, Maarten Sap, Ximing Lu, Yejin Choi, Youngjae Yu |  |
| 923 |  |  [Exploiting Global and Local Hierarchies for Hierarchical Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.268) |  | 0 | Hierarchical text classification aims to leverage label hierarchy in multi-label text classification. Existing methods encode label hierarchy in a global view, where label hierarchy is treated as the static hierarchical structure containing all labels. Since global hierarchy is static and... | Deqing Wang, Fuzhen Zhuang, Leilei Sun, Qinghong Yang, Ting Jiang, Zhongzhi Chen |  |
| 924 |  |  [Semantic-aware Contrastive Learning for More Accurate Semantic Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.269) |  | 0 | Since the meaning representations are detailed and accurate annotations which express fine-grained sequence-level semtantics, it is usually hard to train discriminative semantic parsers via Maximum Likelihood Estimation (MLE) in an autoregressive fashion. In this paper, we propose a semantic-aware... | Bo Chen, Chunlei Xin, Le Sun, Shan Wu, Xianpei Han |  |
| 925 |  |  [Scientific Paper Extractive Summarization Enhanced by Citation Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.270) |  | 0 | In a citation graph, adjacent paper nodes share related scientific terms and topics. The graph thus conveys unique structure information of document-level relatedness that can be utilized in the paper summarization task, for exploring beyond the intra-document information.In this work, we focus on... | Mingzhe Li, Rui Yan, Shen Gao, Xiangliang Zhang, Xin Gao, Xiuying Chen |  |
| 926 |  |  [Hardness-guided domain adaptation to recognise biomedical named entities under low-resource scenarios](https://doi.org/10.18653/v1/2022.emnlp-main.271) |  | 0 | Domain adaptation is an effective solution to data scarcity in low-resource scenarios. However, when applied to token-level tasks such as bioNER, domain adaptation methods often suffer from the challenging linguistic characteristics that clinical narratives possess, which leads to unsatsifactory... | Changyou Chen, Lan Du, Ngoc Dang Nguyen, Richard Beare, Wray L. Buntine |  |
| 927 |  |  [Syntactic Multi-view Learning for Open Information Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.272) |  | 0 | Open Information Extraction (OpenIE) aims to extract relational tuples from open-domain sentences. Traditional rule-based or statistical models were developed based on syntactic structure of sentence, identified by syntactic parsers. However, previous neural OpenIE models under-explored the useful... | Aixin Sun, JungJae Kim, Kuicai Dong, Xiaoli Li |  |
| 928 |  |  [TRIPS: Efficient Vision-and-Language Pre-training with Text-Relevant Image Patch Selection](https://doi.org/10.18653/v1/2022.emnlp-main.273) |  | 0 | Vision Transformers (ViTs) have been widely used in large-scale Vision and Language Pre-training (VLP) models. Though previous VLP works have proved the effectiveness of ViTs, they still suffer from computational efficiency brought by the long visual sequence. To tackle this problem, in this paper,... | Bin Bi, Chaoya Jiang, Chenliang Li, Haiyang Xu, Ming Yan, Shikun Zhang, Songfang Huang, Wei Ye |  |
| 929 |  |  [CGoDial: A Large-Scale Benchmark for Chinese Goal-oriented Dialog Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.274) |  | 0 | Practical dialog systems need to deal with various knowledge sources, noisy user expressions, and the shortage of annotated data. To better solve the above problems, we propose CGoDial, a new challenging and comprehensive Chinese benchmark for multi-domain Goal-oriented Dialog evaluation. It... | Bowen Li, Jian Sun, Wanwei He, Yinpei Dai, Yongbin Li, Yuchuan Wu, Zheng Cao, Zhongqi An |  |
| 930 |  |  [Kernel-Whitening: Overcome Dataset Bias with Isotropic Sentence Embedding](https://doi.org/10.18653/v1/2022.emnlp-main.275) |  | 0 | Dataset bias has attracted increasing attention recently for its detrimental effect on the generalization ability of fine-tuned models. The current mainstream solution is designing an additional shallow model to pre-identify biased instances. However, such two-stage methods scale up the... | Qi Zhang, Shihan Dou, Songyang Gao, Xuanjing Huang |  |
| 931 |  |  [A Unified Positive-Unlabeled Learning Framework for Document-Level Relation Extraction with Different Levels of Labeling](https://doi.org/10.18653/v1/2022.emnlp-main.276) |  | 0 | Document-level relation extraction (RE) aims to identify relations between entities across multiple sentences. Most previous methods focused on document-level RE under full supervision. However, in real-world scenario, it is expensive and difficult to completely label all relations in a document... | Tao Zhang, Wenxin Hu, Xinxin Liu, Ye Wang |  |
| 932 |  |  [Automatic Generation of Socratic Subquestions for Teaching Math Word Problems](https://doi.org/10.18653/v1/2022.emnlp-main.277) |  | 0 | Socratic questioning is an educational method that allows students to discover answers to complex problems by asking them a series of thoughtful questions. Generation of didactically sound questions is challenging, requiring understanding of the reasoning process involved in the problem. We... | Jakub Macina, Kumar Shridhar, Manu Kapur, Mennatallah ElAssady, Mrinmaya Sachan, Tanmay Sinha |  |
| 933 |  |  [Mixture of Attention Heads: Selecting Attention Heads Per Token](https://doi.org/10.18653/v1/2022.emnlp-main.278) |  | 0 | Mixture-of-Experts (MoE) networks have been proposed as an efficient way to scale up model capacity and implement conditional computing. However, the study of MoE components mostly focused on the feedforward layer in Transformer architecture. This paper proposes the Mixture of Attention Heads... | Jie Zhou, Wenge Rong, Xiaofeng Zhang, Yikang Shen, Zeyu Huang, Zhang Xiong |  |
| 934 |  |  [The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.279) |  | 0 | In this paper, we consider the problem of sparsifying BERT models, which are a key building block for natural language processing, in order to reduce their storage and computational cost. We introduce the Optimal BERT Surgeon (oBERT), an efficient and accurate pruning method based on approximate... | Benjamin Fineran, Dan Alistarh, Daniel Campos, Eldar Kurtic, Elias Frantar, Mark Kurtz, Michael Goin, Tuan Nguyen |  |
| 935 |  |  [Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue](https://doi.org/10.18653/v1/2022.emnlp-main.280) |  | 0 | Video-grounded Dialogue (VGD) aims to decode an answer sentence to a question regarding a given video and dialogue context. Despite the recent success of multi-modal reasoning to generate answer sentences, existing dialogue systems still suffer from a text hallucination problem, which denotes... | Chang Dong Yoo, Eunseop Yoon, Hee Suk Yoon, Junyeong Kim, Sunjae Yoon |  |
| 936 |  |  [DSM: Question Generation over Knowledge Base via Modeling Diverse Subgraphs with Meta-learner](https://doi.org/10.18653/v1/2022.emnlp-main.281) |  | 0 | Existing methods on knowledge base question generation (KBQG) learn a one-size-fits-all model by training together all subgraphs without distinguishing the diverse semantics of subgraphs. In this work, we show that making use of the past experience on semantically similar subgraphs can reduce the... | Cuiping Li, Hong Chen, Jing Zhang, Qianyi Zhang, Shasha Guo, Yanling Wang |  |
| 937 |  |  [RelU-Net: Syntax-aware Graph U-Net for Relational Triple Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.282) |  | 0 | Relational triple extraction is a critical task for natural language processing. Existing methods mainly focused on capturing semantic information, but suffered from ignoring the syntactic structure of the sentence, which is proved in the relation classification task to contain rich relational... | Yongfeng Huang, Yubo Chen, Yunqi Zhang |  |
| 938 |  |  [Evidence \textgreater Intuition: Transferability Estimation for Encoder Selection](https://doi.org/10.18653/v1/2022.emnlp-main.283) |  | 0 | With the increase in availability of large pre-trained language models (LMs) in Natural Language Processing (NLP), it becomes critical to assess their fit for a specific target task a priori—as fine-tuning the entire space of available LMs is computationally prohibitive and unsustainable. However,... | Barbara Plank, Elisa Bassignana, Max MüllerEberstein, Mike Zhang |  |
| 939 |  |  [Chunk-based Nearest Neighbor Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.284) |  | 0 | Semi-parametric models, which augment generation with retrieval, have led to impressive results in language modeling and machine translation, due to their ability to retrieve fine-grained information from a datastore of examples. One of the most prominent approaches, kNN-MT, exhibits strong domain... | André F. T. Martins, Pedro Henrique Martins, Zita Marinho |  |
| 940 |  |  [FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.285) |  | 0 | Generative models have recently started to outperform extractive models in Open Domain Question Answering, largely by leveraging their decoder to attend over multiple encoded passages and combining their information. However, generative models tend to be larger than extractive models due to the... | Akhil Kedia, Haejun Lee, Mohd Abbas Zaidi |  |
| 941 |  |  [Inductive Relation Prediction with Logical Reasoning Using Contrastive Representations](https://doi.org/10.18653/v1/2022.emnlp-main.286) |  | 0 | Relation prediction in knowledge graphs (KGs) aims at predicting missing relations in incomplete triples, whereas the dominant embedding paradigm has a restriction on handling unseen entities during testing. In the real-world scenario, the inductive setting is more common because entities in the... | Jun Liu, Lingling Zhang, Qianying Wang, Qika Lin, Tianzhe Zhao, Xin Hu, Yudai Pan |  |
| 942 |  |  [Improving Chinese Spelling Check by Character Pronunciation Prediction: The Effects of Adaptivity and Granularity](https://doi.org/10.18653/v1/2022.emnlp-main.287) |  | 0 | Chinese spelling check (CSC) is a fundamental NLP task that detects and corrects spelling errors in Chinese texts. As most of these spelling errors are caused by phonetic similarity, effectively modeling the pronunciation of Chinese characters is a key factor for CSC. In this paper, we consider... | Jiahao Li, Junbo Guo, Quan Wang, Yanyan Yang, Yongdong Zhang, Zhendong Mao |  |
| 943 |  |  [MT-GenEval: A Counterfactual and Contextual Dataset for Evaluating Gender Accuracy in Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.288) |  | 0 | As generic machine translation (MT) quality has improved, the need for targeted benchmarks that explore fine-grained aspects of quality has increased. In particular, gender accuracy in translation can have implications in terms of output fluency, translation accuracy, and ethics. In this paper, we... | Anna Currey, Benjamin Hsu, Georgiana Dinu, Maria Nadejde, Mia Mayer, Raghavendra Reddy Pappagari, Stanislas Lauly, Xing Niu |  |
| 944 |  |  [A Span-level Bidirectional Network for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.289) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) is a new fine-grained sentiment analysis task that aims to extract triplets of aspect terms, sentiments, and opinion terms from review sentences. Recently, span-level models achieve gratifying results on ASTE task by taking advantage of the predictions of... | Keming Chen, Xian Sun, Yuqi Chen, Zequn Zhang |  |
| 945 |  |  [On the Calibration of Massively Multilingual Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.290) |  | 0 | Massively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how... | Kabir Ahuja, Monojit Choudhury, Sandipan Dandapat, Sunayana Sitaram |  |
| 946 |  |  [Momentum Contrastive Pre-training for Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.291) |  | 0 | Existing pre-training methods for extractive Question Answering (QA) generate cloze-like queries different from natural questions in syntax structure, which could overfit pre-trained models to simple keyword matching. In order to address this problem, we propose a novel Momentum Contrastive... | Irwin King, Minda Hu, Muzhi Li, Yasheng Wang |  |
| 947 |  |  [A Second Wave of UD Hebrew Treebanking and Cross-Domain Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.292) |  | 0 | Foundational Hebrew NLP tasks such as segmentation, tagging and parsing, have relied to date on various versions of the Hebrew Treebank (HTB, Sima’an et al. 2001). However, the data in HTB, a single-source newswire corpus, is now over 30 years old, and does not cover many aspects of contemporary... | Amir Zeldes, Nick Howell, Noam Ordan, Yifat Ben Moshe |  |
| 948 |  |  [Finding Dataset Shortcuts with Grammar Induction](https://doi.org/10.18653/v1/2022.emnlp-main.293) |  | 0 | Many NLP datasets have been found to contain shortcuts: simple decision rules that achieve surprisingly high accuracy. However, it is difficult to discover shortcuts automatically. Prior work on automatic shortcut detection has focused on enumerating features like unigrams or bigrams, which can... | Alexander Wettig, Dan Friedman, Danqi Chen |  |
| 949 |  |  [Retrieval Augmentation for Commonsense Reasoning: A Unified Approach](https://doi.org/10.18653/v1/2022.emnlp-main.294) |  | 0 | A common thread of retrieval-augmented methods in the existing literature focuses on retrieving encyclopedic knowledge, such as Wikipedia, which facilitates well-defined entity and relation spaces that can be modeled. However, applying such methods to commonsense reasoning tasks faces two unique... | Chenguang Zhu, Meng Jiang, Shuohang Wang, Wenhao Yu, Yuwei Fang, Zhihan Zhang, Zhuosheng Zhang |  |
| 950 |  |  [Open World Classification with Adaptive Negative Samples](https://doi.org/10.18653/v1/2022.emnlp-main.295) |  | 0 | Open world classification is a task in natural language processing with key practical relevance and impact.Since the open or unknown category data only manifests in the inference phase, finding a model with a suitable decision boundary accommodating for the identification of known classes and... | Guoyin Wang, Jiwei Li, Ke Bai, Lawrence Carin, Puyang Xu, Ricardo Henao, Sunghyun Park, Sungjin Lee |  |
| 951 |  |  [Re3: Generating Longer Stories With Recursive Reprompting and Revision](https://doi.org/10.18653/v1/2022.emnlp-main.296) |  | 0 | We consider the problem of automatically generating longer stories of over two thousand words. Compared to prior work on shorter stories, long-range plot coherence and relevance are more central challenges here. We propose the Recursive Reprompting and Revision framework (Re3) to address these... | Dan Klein, Kevin Yang, Nanyun Peng, Yuandong Tian |  |
| 952 |  |  [Does Joint Training Really Help Cascaded Speech Translation?](https://doi.org/10.18653/v1/2022.emnlp-main.297) |  | 0 | Currently, in speech translation, the straightforward approach - cascading a recognition system with a translation system - delivers state-of-the-art results.However, fundamental challenges such as error propagation from the automatic speech recognition system still remain.To mitigate these... | Christian Herold, David Thulke, Hermann Ney, Viet Anh Khoa Tran, Yingbo Gao |  |
| 953 |  |  [MasakhaNER 2.0: Africa-centric Transfer Learning for Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.298) |  | 0 | African languages are spoken by over a billion people, but they are under-represented in NLP research and development. Multiple challenges exist, including the limited availability of annotated training and evaluation datasets as well as the lack of understanding of which settings, languages, and... | Allahsera Auguste Tapo, Amelia V. Taylor, Andiswa Bukula, Aremu Anuoluwapo, Blessing K. Sibanda, Bonaventure F. P. Dossou, Catherine Gitau, Cheikh M. Bamba Dione, Chester PalenMichel, Chiamaka Chukwuneke, Chris Chinenye Emezue, Constantine Lignos, David Ifeoluwa Adelani, Derguene Mbaye, Dietrich Klakow, Edwin MunkohBuabeng, Elvis Mboning, Fatoumata Ouoba Kabore, Gilles Hacheme, Godson Kalipe, Graham Neubig, Happy Buzaaba, Idris Abdulmumin, Ignatius Ezeani, Jesujoba O. Alabi, Jonathan Mukiibi, Joyce NakatumbaNabende, Michael Beukman, Mofetoluwa Adeyemi, Neo L. Mokono, Odunayo Ogundepo, Oreen Yousuf, Orevaoghene Ahia, Perez Ogayo, Peter Nabende, Rooweither Mabuya, Sebastian Ruder, Shamsuddeen Hassan Muhammad, Shruti Rijhwani, Tajuddeen Gwadabe, Tatiana Moteu Ngoli, Tebogo Macucwa, Tosin P. Adewumi, Victoire Memdjokam Koagne, Vukosi Marivate |  |
| 954 |  |  [Ethics consideration sections in natural language processing papers](https://doi.org/10.18653/v1/2022.emnlp-main.299) |  | 0 | In this paper, we present the results of a manual classification of all ethical consideration sections for ACL 2021. We also compare how many papers had an ethics consideration section per track and per world region in ACL 2021. We classified papers according to the ethical issues covered (research... | Luciana Benotti, Patrick Blackburn |  |
| 955 |  |  [Continued Pretraining for Better Zero- and Few-Shot Promptability](https://doi.org/10.18653/v1/2022.emnlp-main.300) |  | 0 | Recently introduced language model prompting methods can achieve high accuracy in zero- and few-shot settings while requiring few to no learned task-specific parameters. Nevertheless, these methods still often trail behind full model finetuning. In this work, we investigate if a dedicated continued... | Akshita Bhagia, Dirk Groeneveld, Iz Beltagy, Pete Walsh, Robert L. Logan IV, Sameer Singh, Zhaofeng Wu |  |
| 956 |  |  [Less is More: Summary of Long Instructions is Better for Program Synthesis](https://doi.org/10.18653/v1/2022.emnlp-main.301) |  | 0 | Despite the success of large pre-trained language models (LMs) such as Codex, they show below-par performance on the larger and more complicated programming related questions. We show that LMs benefit from the summarized version of complicated questions. Our findings show that superfluous... | Chitta Baral, Kirby Kuznia, Mihir Parmar, Swaroop Mishra |  |
| 957 |  |  [Is a Question Decomposition Unit All We Need?](https://doi.org/10.18653/v1/2022.emnlp-main.302) |  | 0 | Large Language Models (LMs) have achieved state-of-the-art performance on many Natural Language Processing (NLP) benchmarks. With the growing number of new benchmarks, we build bigger and more complex LMs. However, building new LMs may not be an ideal option owing to the cost, time and... | Chitta Baral, Mihir Parmar, Pruthvi Patel, Swaroop Mishra |  |
| 958 |  |  [Discourse-Aware Soft Prompting for Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.303) |  | 0 | Current efficient fine-tuning methods(e.g., adapters, prefix-tuning, etc.) have optimized conditional text generation via training a small set of extra parameters of the neural language model, while freezing the rest for efficiency. While showing strong performance on some generation tasks, they... | Asli Celikyilmaz, Marjan Ghazvininejad, Vera Gor, Vladimir Karpukhin |  |
| 959 |  |  [ExPUNations: Augmenting Puns with Keywords and Explanations](https://doi.org/10.18653/v1/2022.emnlp-main.304) |  | 0 | The tasks of humor understanding and generation are challenging and subjective even for humans, requiring commonsense and real-world knowledge to master. Puns, in particular, add the challenge of fusing that knowledge with the ability to interpret lexical-semantic ambiguity. In this paper, we... | Alessandra Cervone, Anjali NarayanChen, Jiao Sun, Jing Huang, Nanyun Peng, Shereen Oraby, Tagyoung Chung, Yang Liu |  |
| 960 |  |  [SLING: Sino Linguistic Evaluation of Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.305) |  | 0 | To understand what kinds of linguistic knowledge are encoded by pretrained Chinese language models (LMs), we introduce the benchmark of Sino LINGuistics (SLING), which consists of 38K minimal sentence pairs in Mandarin Chinese grouped into 9 high-level linguistic phenomena. Each pair demonstrates... | Kalpesh Krishna, Mohit Iyyer, Rajesh Bhatt, Yixiao Song |  |
| 961 |  |  [Context-Situated Pun Generation](https://doi.org/10.18653/v1/2022.emnlp-main.306) |  | 0 | Previous work on pun generation commonly begins with a given pun word (a pair of homophones for heterographic pun generation and a polyseme for homographic pun generation) and seeks to generate an appropriate pun. While this may enable efficient pun generation, we believe that a pun is most... | Anjali NarayanChen, Jiao Sun, Jing Huang, Nanyun Peng, Shereen Oraby, Shuyang Gao, Tagyoung Chung, Yang Liu |  |
| 962 |  |  [Retrieval-Augmented Generative Question Answering for Event Argument Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.307) |  | 0 | Event argument extraction has long been studied as a sequential prediction problem with extractive-based methods, tackling each argument in isolation. Although recent work proposes generation-based methods to capture cross-argument dependency, they require generating and post-processing a... | Heng Ji, Xinya Du |  |
| 963 |  |  [Concadia: Towards Image-Based Text Generation with a Purpose](https://doi.org/10.18653/v1/2022.emnlp-main.308) |  | 0 | Current deep learning models often achieve excellent results on benchmark image-to-text datasets but fail to generate texts that are useful in practice. We argue that to close this gap, it is vital to distinguish descriptions from captions based on their distinct communicative roles. Descriptions... | Christopher Potts, Elisa Kreiss, Fei Fang, Noah D. Goodman |  |
| 964 |  |  [Context Matters for Image Descriptions for Accessibility: Challenges for Referenceless Evaluation Metrics](https://doi.org/10.18653/v1/2022.emnlp-main.309) |  | 0 | Few images on the Web receive alt-text descriptions that would make them accessible to blind and low vision (BLV) users. Image-based NLG systems have progressed to the point where they can begin to address this persistent societal problem, but these systems will not be fully successful unless we... | Christopher Potts, Cynthia L. Bennett, Elisa Kreiss, Eric Zelikman, Meredith Ringel Morris, Shayan Hooshmand |  |
| 965 |  |  [MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure](https://doi.org/10.18653/v1/2022.emnlp-main.310) |  | 0 | In this paper, we propose a comprehensive benchmark to investigate models’ logical reasoning capabilities in complex real-life scenarios. Current explanation datasets often employ synthetic data with simple reasoning structures. Therefore, it cannot express more complex reasoning processes, such as... | Changshui Zhang, Dong Yu, Hongming Zhang, Ruixin Hong, Xiaodan Liang, Yinya Huang |  |
| 966 |  |  [Explicit Query Rewriting for Conversational Dense Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.311) |  | 0 | In a conversational search scenario, a query might be context-dependent because some words are referred to previous expressions or omitted. Previous works tackle the issue by either reformulating the query into a self-contained query (query rewriting) or learning a contextualized query embedding... | Hongjin Qian, Zhicheng Dou |  |
| 967 |  |  [Efficient Nearest Neighbor Emotion Classification with BERT-whitening](https://doi.org/10.18653/v1/2022.emnlp-main.312) |  | 0 | Retrieval-based methods have been proven effective in many NLP tasks. Previous methods use representations from the pre-trained model for similarity search directly. However, the sentence representations from the pre-trained model like BERT perform poorly in retrieving semantically similar... | Lin Shang, Wenbiao Yin |  |
| 968 |  |  [FastClass: A Time-Efficient Approach to Weakly-Supervised Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.313) |  | 0 | Weakly-supervised text classification aims to train a classifier using only class descriptions and unlabeled data. Recent research shows that keyword-driven methods can achieve state-of-the-art performance on various tasks. However, these methods not only rely on carefully-crafted class... | Tingyu Xia, Yi Chang, Yuan Tian, Yue Wang |  |
| 969 |  |  [Neural-Symbolic Inference for Robust Autoregressive Graph Parsing via Compositional Uncertainty Quantification](https://doi.org/10.18653/v1/2022.emnlp-main.314) |  | 0 | Pre-trained seq2seq models excel at graph semantic parsing with rich annotated data, but generalize worse to out-of-distribution (OOD) and long-tail examples. In comparison, symbolic parsers under-perform on population-level metrics, but exhibit unique strength in OOD and tail generalization. In... | Jeremiah Z. Liu, Jingbo Shang, Zi Lin |  |
| 970 |  |  [A Speaker-Aware Co-Attention Framework for Medical Dialogue Information Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.315) |  | 0 | With the development of medical digitization, the extraction and structuring of Electronic Medical Records (EMRs) have become challenging but fundamental tasks. How to accurately and automatically extract structured information from medical dialogues is especially difficult because the information... | Chao Lu, Haifeng Huang, Jiayu Xu, Jingbo Zhou, Junwei Liu, Lei Wang, Xia Zhang, Yehui Yang, Yuan Xia, Zhenhui Shi |  |
| 971 |  |  [Towards Interactivity and Interpretability: A Rationale-based Legal Judgment Prediction Framework](https://doi.org/10.18653/v1/2022.emnlp-main.316) |  | 0 | Legal judgment prediction (LJP) is a fundamental task in legal AI, which aims to assist the judge to hear the case and determine the judgment. The legal judgment usually consists of the law article, charge, and term of penalty. In the real trial scenario, the judge usually makes the decision... | Changlong Sun, Fei Wu, Jun Feng, Kun Kuang, Weiming Lu, Yating Zhang, Yifei Liu, Yiquan Wu |  |
| 972 |  |  [RelCLIP: Adapting Language-Image Pretraining for Visual Relationship Detection via Relational Contrastive Learning](https://doi.org/10.18653/v1/2022.emnlp-main.317) |  | 0 | Conventional visual relationship detection models only use the numeric ids of relation labels for training, but ignore the semantic correlation between the labels, which leads to severe training biases and harms the generalization ability of representations. In this paper, we introduce compact... | Bingqian Lin, Feng Zhao, Jianzhuang Liu, Xiaodan Liang, Yi Zhu, Zhaoqing Zhu |  |
| 973 |  |  [Candidate Soups: Fusing Candidate Results Improves Translation Quality for Non-Autoregressive Translation](https://doi.org/10.18653/v1/2022.emnlp-main.318) |  | 0 | Non-autoregressive translation (NAT) model achieves a much faster inference speed than the autoregressive translation (AT) model because it can simultaneously predict all tokens during inference. However, its translation quality suffers from degradation compared to AT. And existing NAT methods only... | Huanran Zheng, Pengfei Wang, Wei Zhu, Xiaoling Wang |  |
| 974 |  |  [Evaluating Parameter Efficient Learning for Generation](https://doi.org/10.18653/v1/2022.emnlp-main.319) |  | 0 | Parameter efficient learning methods (PERMs)have recently gained significant attention asthey provide an efficient way for pre-trainedlanguage models (PLMs) to adapt to a downstream task. However, these conclusions aremostly drawn from in-domain evaluations overthe full training set. In this paper,... | Bryan Catanzaro, Mohammad Shoeybi, Mostofa Patwary, Nayeon Lee, Peng Xu, Ryan Prenger, Shrimai Prabhumoye, Virginia Adams, Wei Ping |  |
| 975 |  |  [McQueen: a Benchmark for Multimodal Conversational Query Rewrite](https://doi.org/10.18653/v1/2022.emnlp-main.320) |  | 0 | The task of query rewrite aims to convert an in-context query to its fully-specified version where ellipsis and coreference are completed and referred-back according to the history context. Although much progress has been made, less efforts have been paid to real scenario conversations that involve... | Chen Shi, Feijun Jiang, Liyi Chen, Runze Wang, Wai Lam, Yifei Yuan, Yuan You |  |
| 976 |  |  [Self-supervised Graph Masking Pre-training for Graph-to-Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.321) |  | 0 | Large-scale pre-trained language models (PLMs) have advanced Graph-to-Text (G2T) generation by processing the linearised version of a graph. However, the linearisation is known to ignore the structural information. Additionally, PLMs are typically pre-trained on free text which introduces domain... | Ehsan Shareghi, Jiuzhou Han |  |
| 977 |  |  [Improving Stability of Fine-Tuning Pretrained Language Models via Component-Wise Gradient Norm Clipping](https://doi.org/10.18653/v1/2022.emnlp-main.322) |  | 0 | Fine-tuning over large pretrained language models (PLMs) has established many state-of-the-art results. Despite its superior performance, such fine-tuning can be unstable, resulting in significant variance in performance and potential risks for practical applications. Previous works have attributed... | Chenghao Yang, Xuezhe Ma |  |
| 978 |  |  [Differentially Private Language Models for Secure Data Sharing](https://doi.org/10.18653/v1/2022.emnlp-main.323) |  | 0 | To protect the privacy of individuals whose data is being shared, it is of high importance to develop methods allowing researchers and companies to release textual data while providing formal privacy guarantees to its originators. In the field of NLP, substantial efforts have been directed at... | Benjamin Weggenmann, Bernhard Schölkopf, Justus Mattern, Mrinmaya Sachan, Zhijing Jin |  |
| 979 |  |  [Conditional set generation using Seq2seq models](https://doi.org/10.18653/v1/2022.emnlp-main.324) |  | 0 | Conditional set generation learns a mapping from an input sequence of tokens to a set. Several NLP tasks, such as entity typing and dialogue emotion tagging, are instances of set generation. Seq2Seq models are a popular choice to model set generation but they treat a set as a sequence and do not... | Aman Madaan, Antoine Bosselut, Dheeraj Rajagopal, Niket Tandon, Yiming Yang |  |
| 980 |  |  [Analyzing and Evaluating Faithfulness in Dialogue Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.325) |  | 0 | Dialogue summarization is abstractive in nature, making it suffer from factual errors. The factual correctness of summaries has the highest priority before practical applications. Many efforts have been made to improve faithfulness in text summarization. However, there is a lack of systematic study... | Bin Wang, Chen Zhang, Haizhou Li, Yan Zhang, Yiming Chen |  |
| 981 |  |  [Twist Decoding: Diverse Generators Guide Each Other](https://doi.org/10.18653/v1/2022.emnlp-main.326) |  | 0 | Many language generation models are now available for a wide range of generation tasks, including machine translation and summarization. Combining such diverse models may lead to further progress, but ensembling generation models is challenging during inference: conventional ensembling methods... | Dragomir Radev, Hao Peng, Jungo Kasai, Keisuke Sakaguchi, Noah A. Smith, Ronan Le Bras, Ximing Lu, Yejin Choi |  |
| 982 |  |  [Exploring Representation-level Augmentation for Code Search](https://doi.org/10.18653/v1/2022.emnlp-main.327) |  | 0 | Code search, which aims at retrieving the most relevant code fragment for a given natural language query, is a common activity in software development practice. Recently, contrastive learning is widely used in code search research, where many data augmentation approaches for source code (e.g.,... | Chunyan Miao, Cyril Leung, Haochen Li, Hongyu Zhang, Yanlin Wang, Yanxian Huang, Yuan Huang |  |
| 983 |  |  [Learning Semantic Textual Similarity via Topic-informed Discrete Latent Variables](https://doi.org/10.18653/v1/2022.emnlp-main.328) |  | 0 | Recently, discrete latent variable models have received a surge of interest in both Natural Language Processing (NLP) and Computer Vision (CV), attributed to their comparable performance to the continuous counterparts in representation learning, while being more interpretable in their predictions.... | Erxin Yu, Lan Du, Yi Chang, Yuan Jin, Zhepei Wei |  |
| 984 |  |  [STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension](https://doi.org/10.18653/v1/2022.emnlp-main.329) |  | 0 | Abstractive dialogue summarization has long been viewed as an important standalone task in natural language processing, but no previous work has explored the possibility of whether abstractive dialogue summarization can also be used as a means to boost an NLP system’s performance on other important... | Arjun Nair, Asli Celikyilmaz, Borui Wang, Chengcheng Feng, Dragomir Radev, Haoran Li, Jai Desai, Madelyn Mao, Yashar Mehdad |  |
| 985 |  |  [Competency-Aware Neural Machine Translation: Can Machine Translation Know its Own Translation Quality?](https://doi.org/10.18653/v1/2022.emnlp-main.330) |  | 0 | Neural machine translation (NMT) is often criticized for failures that happenwithout awareness. The lack of competency awareness makes NMT untrustworthy. This is in sharp contrast to human translators who give feedback or conduct further investigations whenever they are in doubt about predictions.... | Baosong Yang, Dayiheng Liu, Haoran Wei, Jun Xie, Kai Fan, Luo Si, Pei Zhang |  |
| 986 |  |  [PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training](https://doi.org/10.18653/v1/2022.emnlp-main.331) |  | 0 | Fact verification has attracted a lot of attention recently, e.g., in journalism, marketing, and policymaking, as misinformation and dis- information can sway one’s opinion and affect one’s actions. While fact-checking is a hard task in general, in many cases, false statements can be easily... | Ju Fan, Nan Tang, Preslav Nakov, Xiaoman Zhao, Xiaoyong Du, Zihui Gu |  |
| 987 |  |  [Sentiment-Aware Word and Sentence Level Pre-training for Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.332) |  | 0 | Most existing pre-trained language representation models (PLMs) are sub-optimal in sentiment analysis tasks, as they capture the sentiment information from word-level while under-considering sentence-level information. In this paper, we propose SentiWSP, a novel Sentiment-aware pre-trained language... | Chen Lin, Hang Zhang, Haonan Li, Jian Guo, Jinsong Su, Nan Duan, Shuai Fan, Yeyun Gong, Zhenghao Lin |  |
| 988 |  |  [Towards Multi-Modal Sarcasm Detection via Hierarchical Congruity Modeling with Knowledge Enhancement](https://doi.org/10.18653/v1/2022.emnlp-main.333) |  | 0 | Sarcasm is a linguistic phenomenon indicating a discrepancy between literal meanings and implied intentions. Due to its sophisticated nature, it is usually difficult to be detected from the text itself. As a result, multi-modal sarcasm detection has received more and more attention in both academia... | Haoliang Li, Hui Liu, Wenya Wang |  |
| 989 |  |  [Efficiently Tuned Parameters Are Task Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.334) |  | 0 | Intermediate-task transfer can benefit a wide range of NLP tasks with properly selected source datasets. However, it is computationally infeasible to experiment with all intermediate transfer combinations, making choosing a useful source task a challenging problem. In this paper, we anticipate that... | Canwen Xu, Julian J. McAuley, Wangchunshu Zhou |  |
| 990 |  |  [COPEN: Probing Conceptual Knowledge in Pre-trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.335) |  | 0 | Conceptual knowledge is fundamental to human cognition and knowledge bases. However, existing knowledge probing works only focus on evaluating factual knowledge of pre-trained language models (PLMs) and ignore conceptual knowledge. Since conceptual knowledge often appears as implicit commonsense... | Hailong Jin, Hao Peng, Juanzi Li, Lei Hou, Qun Liu, Shengding Hu, Xiaozhi Wang, Zhiyuan Liu |  |
| 991 |  |  [Capturing Global Structural Information in Long Document Question Answering with Compressive Graph Selector Network](https://doi.org/10.18653/v1/2022.emnlp-main.336) |  | 0 | Long document question answering is a challenging task due to its demands for complex reasoning over long text. Previous works usually take long documents as non-structured flat texts or only consider the local structure in long documents. However, these methods usually ignore the global structure... | Heyan Huang, Wei Wei, XianLing Mao, Yuxiang Nie |  |
| 992 |  |  [Structural generalization is hard for sequence-to-sequence models](https://doi.org/10.18653/v1/2022.emnlp-main.337) |  | 0 | Sequence-to-sequence (seq2seq) models have been successful across many NLP tasks,including ones that require predicting linguistic structure. However, recent work on compositional generalization has shown that seq2seq models achieve very low accuracy in generalizing to linguistic structures that... | Alexander Koller, Yuekun Yao |  |
| 993 |  |  [Contrastive Learning enhanced Author-Style Headline Generation](https://doi.org/10.18653/v1/2022.emnlp-main.338) |  | 0 | Headline generation is a task of generating an appropriate headline for a given article, which can be further used for machine-aided writing or enhancing the click-through ratio. Current works only use the article itself in the generation, but have not taken the writing style of headlines into... | Hui Liu, Weidong Guo, Xiangyang Li, Yige Chen |  |
| 994 |  |  [Multi-Granularity Optimization for Non-Autoregressive Translation](https://doi.org/10.18653/v1/2022.emnlp-main.339) |  | 0 | Despite low latency, non-autoregressive machine translation (NAT) suffers severe performance deterioration due to the naive independence assumption. This assumption is further strengthened by cross-entropy loss, which encourages a strict match between the hypothesis and the reference token by... | Leyang Cui, Yafu Li, Yongjing Yin, Yue Zhang |  |
| 995 |  |  [Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.340) |  | 0 | How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types,... | Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Pegah Alipoormolabashi, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Swaroop Mishra, Tanay Dixit, Xudong Shen, Yeganeh Kordi, Yizhong Wang |  |
| 996 |  |  [MetaFill: Text Infilling for Meta-Path Generation on Heterogeneous Information Networks](https://doi.org/10.18653/v1/2022.emnlp-main.341) |  | 0 | Heterogeneous information network (HIN) is essential to study complicated networks containing multiple edge types and node types. Meta-path, a sequence of node types and edge types, is the core technique to embed HINs. Since manually curating meta-paths is time-consuming, there is a pressing need... | Hanwen Xu, Junwei Yang, Kefei Duan, Ming Zhang, Sheng Wang, Zequn Liu |  |
| 997 |  |  [DRLK: Dynamic Hierarchical Reasoning with Language Model and Knowledge Graph for Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.342) |  | 0 | In recent years, Graph Neural Network (GNN) approaches with enhanced knowledge graphs (KG) perform well in question answering (QA) tasks. One critical challenge is how to effectively utilize interactions between the QA context and KG. However, existing work only adopts the identical QA context... | Miao Zhang, Ming Dong, Rufeng Dai, Tingting He |  |
| 998 |  |  [AEG: Argumentative Essay Generation via A Dual-Decoder Model with Content Planning](https://doi.org/10.18653/v1/2022.emnlp-main.343) |  | 0 | Argument generation is an important but challenging task in computational argumentation.Existing studies have mainly focused on generating individual short arguments, while research on generating long and coherent argumentative essays is still under-explored.In this paper, we propose a new task,... | Fei Mi, Jianzhu Bao, Ruifeng Xu, Yasheng Wang, Yitong Li |  |
| 999 |  |  [BotsTalk: Machine-sourced Framework for Automatic Curation of Large-scale Multi-skill Dialogue Datasets](https://doi.org/10.18653/v1/2022.emnlp-main.344) |  | 0 | To build open-domain chatbots that are able to use diverse communicative skills, we propose a novel framework BotsTalk, where multiple agents grounded to the specific target skills participate in a conversation to automatically annotate multi-skill dialogues. We further present Blended Skill... | Chaehyeong Kim, Jinyoung Yeo, Minju Kim, Seungwon Hwang, Yongho Song |  |
| 1000 |  |  [Wider & Closer: Mixture of Short-channel Distillers for Zero-shot Cross-lingual Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.345) |  | 0 | Zero-shot cross-lingual named entity recognition (NER) aims at transferring knowledge from annotated and rich-resource data in source languages to unlabeled and lean-resource data in target languages. Existing mainstream methods based on the teacher-student distillation framework ignore the rich... | Beiduo Chen, Cong Liu, JiaChen Gu, JunYu Ma, Quan Liu, Wu Guo, Zhenhua Ling, Zhigang Chen |  |
| 1001 |  |  [An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.346) |  | 0 | Access to external knowledge is essential for many natural language processing tasks, such as question answering and dialogue. Existing methods often rely on a parametric model that stores knowledge in its parameters, or use a retrieval-augmented model that has access to an external knowledge... | Baotian Hu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel, Yu Zhao, Yuxiang Wu |  |
| 1002 |  |  [Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation](https://doi.org/10.18653/v1/2022.emnlp-main.347) |  | 0 | Capturing emotions within a conversation plays an essential role in modern dialogue systems. However, the weak correlation between emotions and semantics brings many challenges to emotion recognition in conversation (ERC). Even semantically similar utterances, the emotion may vary drastically... | Hui Xue, Longtao Huang, Songlin Hu, Xiaohui Song |  |
| 1003 |  |  [RuCoLA: Russian Corpus of Linguistic Acceptability](https://doi.org/10.18653/v1/2022.emnlp-main.348) |  | 0 | Linguistic acceptability (LA) attracts the attention of the research community due to its many uses, such as testing the grammatical knowledge of language models and filtering implausible texts with acceptability classifiers.However, the application scope of LA in languages other than English is... | Alena Pestova, Ekaterina Artemova, Ivan Smurov, Max Ryabinin, Tatiana Shamardina, Vladislav Mikhailov |  |
| 1004 |  |  [Complex Hyperbolic Knowledge Graph Embeddings with Fast Fourier Transform](https://doi.org/10.18653/v1/2022.emnlp-main.349) |  | 0 | The choice of geometric space for knowledge graph (KG) embeddings can have significant effects on the performance of KG completion tasks. The hyperbolic geometry has been shown to capture the hierarchical patterns due to its tree-like metrics, which addressed the limitations of the Euclidean... | Ginny Y. Wong, Huiru Xiao, Simon See, Xin Liu, Yangqiu Song |  |
| 1005 |  |  [Towards Knowledge-Intensive Text-to-SQL Semantic Parsing with Formulaic Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.350) |  | 0 | In this paper, we study the problem of knowledge-intensive text-to-SQL, in which domain knowledge is necessary to parse expert questions into SQL queries over domain-specific tables. We formalize this scenario by building a new benchmark KnowSQL consisting of domain-specific questions covering... | Dechen Zhan, Dingzirui Wang, JianGuang Lou, Longxu Dou, MinYen Kan, Mingyang Pan, Wanxiang Che, Xuqi Liu, Yan Gao |  |
| 1006 |  |  [Should We Ban English NLP for a Year?](https://doi.org/10.18653/v1/2022.emnlp-main.351) |  | 0 | Around two thirds of NLP research at top venues is devoted exclusively to developing technology for speakers of English, most speech data comes from young urban speakers, and most texts used to train language models come from male writers. These biases feed into consumer technologies to widen... | Anders Søgaard |  |
| 1007 |  |  [LittleBird: Efficient Faster & Longer Transformer for Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.352) |  | 0 | BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a limitation dealing with long inputs due to its attention mechanism. Longformer, ETC and BigBird addressed this issue and effectively solved the quadratic dependency problem.However we find that these models are not... | Kijong Han, Minchul Lee, Myeong Cheol Shin |  |
| 1008 |  |  [WeTS: A Benchmark for Translation Suggestion](https://doi.org/10.18653/v1/2022.emnlp-main.353) |  | 0 | Translation suggestion (TS), which provides alternatives for specific words or phrases given the entire documents generated by machine translation (MT), has been proven to play a significant role in post-editing (PE). There are two main pitfalls for existing researches in this line. First, most... | Ernan Li, Fandong Meng, Jie Zhou, Yingxue Zhang, Zhen Yang |  |
| 1009 |  |  [Discrete Cross-Modal Alignment Enables Zero-Shot Speech Translation](https://doi.org/10.18653/v1/2022.emnlp-main.354) |  | 0 | End-to-end Speech Translation (ST) aims at translating the source language speech into target language text without generating the intermediate transcriptions. However, the training of end-to-end methods relies on parallel ST data, which are difficult and expensive to obtain. Fortunately, the... | Boxing Chen, Chen Wang, Chengqing Zong, Jiajun Zhang, Wei Luo, Yuchen Liu, Zhongqiang Huang |  |
| 1010 |  |  [Abstractive Summarization Guided by Latent Hierarchical Document Structure](https://doi.org/10.18653/v1/2022.emnlp-main.355) |  | 0 | Sequential abstractive neural summarizers often do not use the underlying structure in the input article or dependencies between the input sentences. This structure is essential to integrate and consolidate information from different parts of the text. To address this shortcoming, we propose a... | Shay B. Cohen, Yifu Qiu |  |
| 1011 |  |  [Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.356) |  | 0 | Multi-hop Question Answering is an agent task for testing the reasoning ability. With the development of pre-trained models, the implicit reasoning ability has been surprisingly improved and can even surpass human performance. However, the nature of the black box hinders the construction of... | Hong Liu, Jianguo Mao, Qiaoqiao She, Wenbin Jiang, Xiangdong Wang, Yajuan Lyu, Yu Xia |  |
| 1012 |  |  [DuReader-Retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine](https://doi.org/10.18653/v1/2022.emnlp-main.357) |  | 0 | In this paper, we present DuReader-retrieval, a large-scale Chinese dataset for passage retrieval. DuReader-retrieval contains more than 90K queries and over 8M unique passages from a commercial search engine. To alleviate the shortcomings of other datasets and ensure the quality of our benchmark,... | Haifeng Wang, Hongyu Li, Hua Wu, Jing Liu, Qiaoqiao She, Yifu Qiu, Ying Chen, Yingqi Qu |  |
| 1013 |  |  [Pair-Based Joint Encoding with Relational Graph Convolutional Networks for Emotion-Cause Pair Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.358) |  | 0 | Emotion-cause pair extraction (ECPE) aims to extract emotion clauses and corresponding cause clauses, which have recently received growing attention. Previous methods sequentially encode features with a specified order. They first encode the emotion and cause features for clause extraction and then... | Junlong Liu, Qianli Ma, Xichen Shang |  |
| 1014 |  |  [Affective Knowledge Enhanced Multiple-Graph Fusion Networks for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.359) |  | 0 | Aspect-based sentiment analysis aims to identify sentiment polarity of social media users toward different aspects. Most recent methods adopt the aspect-centric latent tree to connect aspects and their corresponding opinion words, thinking that would facilitate establishing the relationship between... | Binxing Fang, Cuiyun Gao, Heyan Chai, Qing Liao, Siyu Tang, Ye Ding, Ziyi Yao |  |
| 1015 |  |  [IndicNLG Benchmark: Multilingual Datasets for Diverse NLG Tasks in Indic Languages](https://doi.org/10.18653/v1/2022.emnlp-main.360) |  | 0 | Natural Language Generation (NLG) for non-English languages is hampered by the scarcity of datasets in these languages. We present the IndicNLG Benchmark, a collection of datasets for benchmarking NLG for 11 Indic languages. We focus on five diverse tasks, namely, biography generation using... | Aman Kumar, Amogh Mishra, Anoop Kunchukuttan, Himani Shrotriya, Mitesh M. Khapra, Prachi Sahu, Pratyush Kumar, Raj Dabre, Ratish Puduppully |  |
| 1016 |  |  [Improving Machine Translation with Phrase Pair Injection and Corpus Filtering](https://doi.org/10.18653/v1/2022.emnlp-main.361) |  | 0 | In this paper, we show that the combination of Phrase Pair Injection and Corpus Filtering boosts the performance of Neural Machine Translation (NMT) systems. We extract parallel phrases and sentences from the pseudo-parallel corpus and augment it with the parallel corpus to train the NMT models.... | Akshay Batheja, Pushpak Bhattacharyya |  |
| 1017 |  |  [An Anchor-based Relative Position Embedding Method for Cross-Modal Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.362) |  | 0 | Position Embedding (PE) is essential for transformer to capture the sequence ordering of input tokens. Despite its general effectiveness verified in Natural Language Processing (NLP) and Computer Vision (CV), its application in cross-modal tasks remains unexplored and suffers from two challenges:... | Chengzhong Xu, Fengzong Lian, Xingwu Sun, Ya Wang, Zhanhui Kang |  |
| 1018 |  |  [Norm-based Noisy Corpora Filtering and Refurbishing in Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.363) |  | 0 | Recent advances in neural machine translation depend on massive parallel corpora, which are collected from any open source without much guarantee of quality. It stresses the need for noisy corpora filtering, but existing methods are insufficient to solve this issue. They spend much time ensembling... | Jiajun Zhang, Yu Lu |  |
| 1019 |  |  [TeleMelody: Lyric-to-Melody Generation with a Template-Based Two-Stage Method](https://doi.org/10.18653/v1/2022.emnlp-main.364) |  | 0 | Lyric-to-melody generation is an important task in automatic songwriting. Previous lyric-to-melody generation systems usually adopt end-to-end models that directly generate melodies from lyrics, which suffer from several issues: 1) lack of paired lyric-melody training data; 2) lack of control on... | Chen Zhang, Kejun Zhang, Peiling Lu, Rui Wang, Songruoyao Wu, Tao Qin, TieYan Liu, XiangYang Li, Xu Tan, Zeqian Ju |  |
| 1020 |  |  [SEEN: Structured Event Enhancement Network for Explainable Need Detection of Information Recall Assistance](https://doi.org/10.18653/v1/2022.emnlp-main.365) |  | 0 | When recalling life experiences, people often forget or confuse life events, which necessitates information recall services. Previous work on information recall focuses on providing such assistance reactively, i.e., by retrieving the life event of a given query. Proactively detecting the need for... | AnZi Yen, HenHsen Huang, HsinHsi Chen, YouEn Lin |  |
| 1021 |  |  [Rethinking Style Transformer with Energy-based Interpretation: Adversarial Unsupervised Style Transfer using a Pretrained Model](https://doi.org/10.18653/v1/2022.emnlp-main.366) |  | 0 | Style control, content preservation, and fluency determine the quality of text style transfer models. To train on a nonparallel corpus, several existing approaches aim to deceive the style discriminator with an adversarial loss. However, adversarial training significantly degrades fluency compared... | ChaeHun Park, Dohee Kim, Edward Choi, Hojun Cho, Hyungjong Noh, Jaegul Choo, JeongIn Hwang, Minseok Choi, Seungwoo Ryu |  |
| 1022 |  |  [Towards Robust k-Nearest-Neighbor Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.367) |  | 0 | k-Nearest-Neighbor Machine Translation (kNN-MT) becomes an important research direction of NMT in recent years. Its main idea is to retrieve useful key-value pairs from an additional datastore to modify translations without updating the NMT model. However, the underlying retrieved noisy pairs will... | Chulun Zhou, Degen Huang, Fandong Meng, Hui Jiang, Jie Zhou, Jinsong Su, Ziyao Lu |  |
| 1023 |  |  [Tiny-NewsRec: Effective and Efficient PLM-based News Recommendation](https://doi.org/10.18653/v1/2022.emnlp-main.368) |  | 0 | News recommendation is a widely adopted technique to provide personalized news feeds for the user. Recently, pre-trained language models (PLMs) have demonstrated the great capability of natural language understanding and benefited news recommendation via improving news modeling. However, most... | Chuhan Wu, Fangzhao Wu, Jingwei Yi, Qi Liu, Yang Yu |  |
| 1024 |  |  [TABS: Efficient Textual Adversarial Attack for Pre-trained NL Code Model Using Semantic Beam Search](https://doi.org/10.18653/v1/2022.emnlp-main.369) |  | 0 | As pre-trained models have shown successful performance in program language processing as well as natural language processing, adversarial attacks on these models also attract attention.However, previous works on black-box adversarial attacks generated adversarial examples in a very inefficient way... | Hyojun Kim, JeeHyong Lee, YunSeok Choi |  |
| 1025 |  |  [Investigating the Robustness of Natural Language Generation from Logical Forms via Counterfactual Samples](https://doi.org/10.18653/v1/2022.emnlp-main.370) |  | 0 | The aim of Logic2Text is to generate controllable and faithful texts conditioned on tables and logical forms, which not only requires a deep understanding of the tables and logical forms, but also warrants symbolic reasoning over the tables according to the logical forms. State-of-the-art methods... | Chengyuan Liu, Fei Wu, Kun Kuang, Leilei Gan |  |
| 1026 |  |  [Helping the Weak Makes You Strong: Simple Multi-Task Learning Improves Non-Autoregressive Translators](https://doi.org/10.18653/v1/2022.emnlp-main.371) |  | 0 | Recently, non-autoregressive (NAR) neural machine translation models have received increasing attention due to their efficient parallel decoding.However, the probabilistic framework of NAR models necessitates conditional independence assumption on target sequences, falling short of characterizing... | Shujian Huang, Xinyou Wang, Zaixiang Zheng |  |
| 1027 |  |  [RACE: Retrieval-augmented Commit Message Generation](https://doi.org/10.18653/v1/2022.emnlp-main.372) |  | 0 | Commit messages are important for software development and maintenance. Many neural network-based approaches have been proposed and shown promising results on automatic commit message generation. However, the generated commit messages could be repetitive or redundant. In this paper, we propose... | Dongmei Zhang, Ensheng Shi, Hongbin Sun, Hongyu Zhang, Lun Du, Shi Han, Wei Tao, Yanlin Wang |  |
| 1028 |  |  [PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.373) |  | 0 | Logical table-to-text generation is a task that involves generating logically faithful sentences from tables, which requires models to derive logical-level facts from table records via logical inference. It raises a new challenge on the logical-level content planning of table-to-text models.... | Ao Liu, Dongmei Zhang, Haoyu Dong, Naoaki Okazaki, Shi Han |  |
| 1029 |  |  [GHAN: Graph-Based Hierarchical Aggregation Network for Text-Video Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.374) |  | 0 | Text-video retrieval focuses on two aspects: cross-modality interaction and video-language encoding. Currently, the mainstream approach is to train a joint embedding space for multimodal interactions. However, there are structural and semantic differences between text and video, making this... | Bojie Hu, Yahan Yu, Yu Li |  |
| 1030 |  |  [MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text](https://doi.org/10.18653/v1/2022.emnlp-main.375) |  | 0 | While language Models store a massive amount of world knowledge implicitly in their parameters, even very large models often fail to encode information about rare entities and events, while incurring huge computational costs. Recently, retrieval-augmented models, such as REALM, RAG, and RETRO, have... | Hexiang Hu, Pat Verga, Wenhu Chen, William W. Cohen, Xi Chen |  |
| 1031 |  |  [PHEE: A Dataset for Pharmacovigilance Event Extraction from Text](https://doi.org/10.18653/v1/2022.emnlp-main.376) |  | 0 | The primary goal of drug safety researchers and regulators is to promptly identify adverse drug reactions. Doing so may in turn prevent or reduce the harm to patients and ultimately improve public health. Evaluating and monitoring drug safety (i.e., pharmacovigilance) involves analyzing an ever... | Bino John, Byron C. Wallace, Gabriele Pergola, Jiazheng Li, Joseph Kim, Nigel Greene, Yulan He, Zhaoyue Sun |  |
| 1032 |  |  [OTSeq2Set: An Optimal Transport Enhanced Sequence-to-Set Model for Extreme Multi-label Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.377) |  | 0 | Extreme multi-label text classification (XMTC) is the task of finding the most relevant subset labels from an extremely large-scale label collection. Recently, some deep learning models have achieved state-of-the-art results in XMTC tasks. These models commonly predict scores for all labels by a... | Jie Cao, Yin Zhang |  |
| 1033 |  |  [SimQA: Detecting Simultaneous MT Errors through Word-by-Word Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.378) |  | 0 | Detractors of neural machine translation admit that while its translations are fluent, it sometimes gets key facts wrong. This is particularly important in simultaneous interpretation where translations have to be provided as fast as possible: before a sentence is complete. Yet, evaluations of... | HyoJung Han, Jordan L. BoydGraber, Marine Carpuat |  |
| 1034 |  |  [Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations](https://doi.org/10.18653/v1/2022.emnlp-main.379) |  | 0 | Large pretrained multilingual language models (ML-LMs) have shown remarkable capabilities of zero-shot cross-lingual transfer, without direct cross-lingual supervision. While these results are promising, follow-up works found that, within the multilingual embedding spaces, there exists strong... | Handong Zhao, Shuai Li, Tong Yu, Zhihui Xie |  |
| 1035 |  |  [Rethinking the Authorship Verification Experimental Setups](https://doi.org/10.18653/v1/2022.emnlp-main.380) |  | 0 | One of the main drivers of the recent advances in authorship verification is the PAN large-scale authorship dataset. Despite generating significant progress in the field, inconsistent performance differences between the closed and open test sets have been reported. To this end, we improve the... | Andrei Manolache, Antonio Barbalau, Elena Burceanu, Florin Brad, Marius Popescu, Radu Tudor Ionescu |  |
| 1036 |  |  [Borrowing Human Senses: Comment-Aware Self-Training for Social Media Multimodal Classification](https://doi.org/10.18653/v1/2022.emnlp-main.381) |  | 0 | Social media is daily creating massive multimedia content with paired image and text, presenting the pressing need to automate the vision and language understanding for various multimodal classification tasks. Compared to the commonly researched visual-lingual data, social media posts tend to... | Chunpu Xu, Jing Li |  |
| 1037 |  |  [Training Language Models with Memory Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.382) |  | 0 | Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language... | Danqi Chen, Tao Lei, Zexuan Zhong |  |
| 1038 |  |  [Data-Efficient Strategies for Expanding Hate Speech Detection into Under-Resourced Languages](https://doi.org/10.18653/v1/2022.emnlp-main.383) |  | 0 | Hate speech is a global phenomenon, but most hate speech datasets so far focus on English-language content. This hinders the development of more effective hate speech detection models in hundreds of languages spoken by billions across the world. More data is needed, but annotating hateful content... | Debora Nozza, Dirk Hovy, Federico Bianchi, Paul Röttger |  |
| 1039 |  |  [Dimension Reduction for Efficient Dense Retrieval via Conditional Autoencoder](https://doi.org/10.18653/v1/2022.emnlp-main.384) |  | 0 | Dense retrievers encode queries and documents and map them in an embedding space using pre-trained language models. These embeddings need to be high-dimensional to fit training signals and guarantee the retrieval effectiveness of dense retrievers. However, these high-dimensional embeddings lead to... | Chenyan Xiong, Han Zhang, Xiaohua Li, Yu Gu, Zhenghao Liu, Zhiyuan Liu |  |
| 1040 |  |  [Controlled Text Reduction](https://doi.org/10.18653/v1/2022.emnlp-main.385) |  | 0 | Producing a reduced version of a source text, as in generic or focused summarization, inherently involves two distinct subtasks: deciding on targeted content and generating a coherent text conveying it. While some popular approaches address summarization as a single end-to-end task, prominent works... | Aviv Slobodkin, Eran Hirsch, Ido Dagan, Ori Ernst, Paul Roit |  |
| 1041 |  |  [Questioning the Validity of Summarization Datasets and Improving Their Factual Consistency](https://doi.org/10.18653/v1/2022.emnlp-main.386) |  | 0 | The topic of summarization evaluation has recently attracted a surge of attention due to the rapid development of abstractive summarization systems. However, the formulation of the task is rather ambiguous, neither the linguistic nor the natural language processing communities have succeeded in... | Chloé Clavel, Michalis Vazirgiannis, Moussa Kamal Eddine, Yanzhu Guo |  |
| 1042 |  |  [Invariant Language Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.387) |  | 0 | Modern pretrained language models are critical components of NLP pipelines. Yet, they suffer from spurious correlations, poor out-of-domain generalization, and biases.Inspired by recent progress in causal machine learning, in particular the invariant risk minimization (IRM) paradigm, we propose... | Barun Patra, Dean Carignan, Emre Kiciman, Martin Josifoski, Maxime Peyrard, Robert West, Sarvjeet Singh Ghotra, Saurabh Tiwary, Vidhan Agarwal |  |
| 1043 |  |  [AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning](https://doi.org/10.18653/v1/2022.emnlp-main.388) |  | 0 | Standard fine-tuning of large pre-trained language models (PLMs) for downstream tasks requires updating hundreds of millions to billions of parameters, and storing a large copy of the PLM weights for every task resulting in increased cost for storing, sharing and serving the models. To address... | Ahmed Hassan Awadallah, Jianfeng Gao, Jing Gao, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Yaqing Wang |  |
| 1044 |  |  [How "Multi" is Multi-Document Summarization?](https://doi.org/10.18653/v1/2022.emnlp-main.389) |  | 0 | The task of multi-document summarization (MDS) aims at models that, given multiple documents as input, are able to generate a summary that combines disperse information, originally spread __across__ these documents. Accordingly, it is expected that both reference summaries in MDS datasets, as well... | Arie Cattan, Ido Dagan, Ori Ernst, Ruben Wolhandler |  |
| 1045 |  |  [BioReader: a Retrieval-Enhanced Text-to-Text Transformer for Biomedical Literature](https://doi.org/10.18653/v1/2022.emnlp-main.390) |  | 0 | The latest batch of research has equipped language models with the ability to attend over relevant and factual information from non-parametric external sources, drawing a complementary path to architectural scaling. Besides mastering language, exploiting and contextualizing the latent world... | Giacomo Frisoni, Gianluca Moro, Lorenzo Valgimigli, Miki Mizutani |  |
| 1046 |  |  [T-Modules: Translation Modules for Zero-Shot Cross-Modal Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.391) |  | 0 | We present a new approach to perform zero-shot cross-modal transfer between speech and text for translation tasks. Multilingual speech and text are encoded in a joint fixed-size representation space. Then, we compare different approaches to decode these multimodal and multilingual fixed-size... | Benoît Sagot, Holger Schwenk, Hongyu Gong, PaulAmbroise Duquenne |  |
| 1047 |  |  [LILA: A Unified Benchmark for Mathematical Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.392) |  | 0 | Mathematical reasoning skills are essential for general-purpose intelligentsystems to perform tasks from grocery shopping to climate modeling.Towards evaluating and improving AI systems in this domain, we proposeLILA, a unified mathematical reasoning benchmark consisting of 23 diversetasks along... | Ashish Sabharwal, Ashwin Kalyan, Chitta Baral, Leonard Tang, Matthew Finlayson, Oyvind Tafjord, Pan Lu, Peter Clark, Sean Welleck, Swaroop Mishra, Tanmay Rajpurohit |  |
| 1048 |  |  [Leveraging Affirmative Interpretations from Negation Improves Natural Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.393) |  | 0 | Negation poses a challenge in many natural language understanding tasks. Inspired by the fact that understanding a negated statement often requires humans to infer affirmative interpretations, in this paper we show that doing so benefits models for three natural language understanding tasks. We... | Eduardo Blanco, Md Mosharaf Hossain |  |
| 1049 |  |  [GraphQ IR: Unifying the Semantic Parsing of Graph Query Languages with One Intermediate Representation](https://doi.org/10.18653/v1/2022.emnlp-main.394) |  | 0 | Subject to the huge semantic gap between natural and formal languages, neural semantic parsing is typically bottlenecked by its complexity of dealing with both input semantics and output syntax. Recent works have proposed several forms of supplementary supervision but none is generalized across... | Jiaxin Shi, Jidong Zhai, Jiuding Sun, Juanzi Li, Lei Hou, Lunyiu Nie, Qi Tian, Shulin Cao |  |
| 1050 |  |  [InforMask: Unsupervised Informative Masking for Language Model Pretraining](https://doi.org/10.18653/v1/2022.emnlp-main.395) |  | 0 | Masked language modeling is widely used for pretraining large language models for natural language understanding (NLU). However, random masking is suboptimal, allocating an equal masking rate for all tokens. In this paper, we propose InforMask, a new unsupervised masking strategy for training... | Canwen Xu, Julian J. McAuley, Nafis Sadeq |  |
| 1051 |  |  [CTRLsum: Towards Generic Controllable Text Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.396) |  | 0 | Current summarization systems yield generic summaries that are disconnected from users’ preferences and expectations. To address this limitation, we present CTRLsum, a generic framework to control generated summaries through a set of keywords. During training keywords are extracted automatically... | Bryan McCann, Caiming Xiong, Junxian He, Nazneen Rajani, Wojciech Kryscinski |  |
| 1052 |  |  [Missing Counter-Evidence Renders NLP Fact-Checking Unrealistic for Misinformation](https://doi.org/10.18653/v1/2022.emnlp-main.397) |  | 0 | Misinformation emerges in times of uncertainty when credible information is limited. This is challenging for NLP-based fact-checking as it relies on counter-evidence, which may not yet be available. Despite increasing interest in automatic fact-checking, it is still unclear if automated approaches... | Iryna Gurevych, Max Glockner, Yufang Hou |  |
| 1053 |  |  [A Framework for Adapting Pre-Trained Language Models to Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.emnlp-main.398) |  | 0 | Recent work has demonstrated that entity representations can be extracted from pre-trained language models to develop knowledge graph completion models that are more robust to the naturally occurring sparsity found in knowledge graphs. In this work, we conduct a comprehensive exploration of how to... | Carolyn P. Rosé, Justin Lovelace |  |
| 1054 |  |  [Mutual Information Alleviates Hallucinations in Abstractive Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.399) |  | 0 | Despite significant progress in the quality of language generated from abstractive summarization models, these models still exhibit the tendency to hallucinate, i.e., output content not supported by the source document. A number of works have tried to fix—or at least uncover the source of—the... | Clara Meister, Liam van der Poel, Ryan Cotterell |  |
| 1055 |  |  [Toward the Limitation of Code-Switching in Cross-Lingual Transfer](https://doi.org/10.18653/v1/2022.emnlp-main.400) |  | 0 | Multilingual pretrained models have shown strong cross-lingual transfer ability. Some works used code-switching sentences, which consist of tokens from multiple languages, to enhance the cross-lingual representation further, and have shown success in many zero-shot cross-lingual tasks. However,... | Feng Li, Philipp Koehn, Yukun Feng |  |
| 1056 |  |  [Syntactically Rich Discriminative Training: An Effective Method for Open Information Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.401) |  | 0 | Open information extraction (OIE) is the task of extracting facts "(Subject, Relation, Object)” from natural language text. We propose several new methods for training neural OIE models in this paper. First, we propose a novel method for computing syntactically rich text embeddings using the... | Frank Mtumbuka, Thomas Lukasiewicz |  |
| 1057 |  |  [Transformer-based Entity Typing in Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.402) |  | 0 | We investigate the knowledge graph entity typing task which aims at inferring plausible entity types. In this paper, we propose a novel Transformer-based Entity Typing (TET) approach, effectively encoding the content of neighbours of an entity by means of a transformer mechanism. More precisely,... | Jeff Z. Pan, Ru Li, Víctor GutiérrezBasulto, Zhiliang Xiang, Zhiwei Hu |  |
| 1058 |  |  [NewsClaims: A New Benchmark for Claim Detection from News with Attribute Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.403) |  | 0 | Claim detection and verification are crucial for news understanding and have emerged as promising technologies for mitigating misinformation and disinformation in the news. However, most existing work has focused on claim sentence analysis while overlooking additional crucial attributes (e.g., the... | Ahmed Elsayed, Eduard H. Hovy, Heng Ji, Kathryn Conger, Kevin Small, Martha Palmer, Preslav Nakov, Revanth Gangi Reddy, Sai Chetan Chinthakindi, Yi R. Fung, Zhenhailong Wang |  |
| 1059 |  |  [IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces](https://doi.org/10.18653/v1/2022.emnlp-main.404) |  | 0 | The ability to extract high-quality translation dictionaries from monolingual word embedding spaces depends critically on the geometric similarity of the spaces—their degree of “isomorphism.” We address the root-cause of faulty cross-lingual mapping: that word embedding training resulted in the... | Kelly Marchisio, Kevin Duh, Neha Verma, Philipp Koehn |  |
| 1060 |  |  [Adversarial Concept Erasure in Kernel Space](https://doi.org/10.18653/v1/2022.emnlp-main.405) |  | 0 | The representation space of neural models for textual data emerges in an unsupervised manner during training. Understanding how human-interpretable concepts, such as gender, are encoded in these representations would improve the ability of users to control the content of these representations and... | Francisco Vargas, Ryan Cotterell, Shauli Ravfogel, Yoav Goldberg |  |
| 1061 |  |  [The Authenticity Gap in Human Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.406) |  | 0 | Human ratings are the gold standard in NLG evaluation. The standard protocol is to collect ratings of generated text, average across annotators, and rank NLG systems by their average scores. However, little consideration has been given as to whether this approach faithfully captures human... | Dan Jurafsky, Kawin Ethayarajh |  |
| 1062 |  |  [BERT in Plutarch's Shadows](https://doi.org/10.18653/v1/2022.emnlp-main.407) |  | 0 | The extensive surviving corpus of the ancient scholar Plutarch of Chaeronea (ca. 45-120 CE) also contains several texts which, according to current scholarly opinion, did not originate with him and are therefore attributed to an anonymous author Pseudo-Plutarch. These include, in particular, the... | Alexey Tikhonov, Charlotte Schubert, Ivan P. Yamshchikov, Jürgen Jost, Yorgos Pantis |  |
| 1063 |  |  [Leveraging Locality in Abstractive Text Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.408) |  | 0 | Neural attention models have achieved significant improvements on many natural language processing tasks. However, the quadratic memory complexity of the self-attention module with respect to the input length hinders their applications in long text summarization. Instead of designing more efficient... | Ahmed Hassan Awadallah, Ansong Ni, Budhaditya Deb, Chenguang Zhu, Dragomir Radev, Linyong Nan, Yixin Liu |  |
| 1064 |  |  [Salience Allocation as Guidance for Abstractive Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.409) |  | 0 | Abstractive summarization models typically learn to capture the salient information from scratch implicitly.Recent literature adds extractive summaries as guidance for abstractive summarization models to provide hints of salient content and achieves better performance.However, extractive summaries... | Dong Yu, Fei Wang, Hongming Zhang, Kaiqiang Song, Lifeng Jin, Muhao Chen, Sangwoo Cho, Wenlin Yao, Xiaoyang Wang |  |
| 1065 |  |  [Fine-tuned Language Models are Continual Learners](https://doi.org/10.18653/v1/2022.emnlp-main.410) |  | 0 | Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions and that models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models even though... | Smaranda Muresan, Thomas Scialom, Tuhin Chakrabarty |  |
| 1066 |  |  [Natural Logic-guided Autoregressive Multi-hop Document Retrieval for Fact Verification](https://doi.org/10.18653/v1/2022.emnlp-main.411) |  | 0 | A key component of fact verification is the evidence retrieval, often from multiple documents. Recent approaches use dense representations and condition the retrieval of each document on the previously retrieved ones. The latter step is performed over all the documents in the collection, requiring... | Andreas Vlachos, Rami Aly |  |
| 1067 |  |  [AX-MABSA: A Framework for Extremely Weakly Supervised Multi-label Aspect Based Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.412) |  | 0 | Aspect Based Sentiment Analysis is a dominant research area with potential applications in social media analytics, business, finance, and health. Prior works in this area are primarily based on supervised methods, with a few techniques using weak supervision limited to predicting a single aspect... | MingXue Wang, Sabyasachi Kamila, Sourav Dutta, Walid Magdy |  |
| 1068 |  |  [Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.413) |  | 0 | Recent research shows synthetic data as a source of supervision helps pretrained language models (PLM) transfer learning to new target tasks/domains. However, this idea is less explored for spatial language. We provide two new data resources on multiple spatial language processing tasks. The first... | Parisa Kordjamshidi, Roshanak Mirzaee |  |
| 1069 |  |  [A Survey of Active Learning for Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-main.414) |  | 0 | In this work, we provide a literature review of active learning (AL) for its applications in natural language processing (NLP). In addition to a fine-grained categorization of query strategies, we also investigate several other important aspects of applying AL to NLP problems. These include AL for... | Eduard H. Hovy, Emma Strubell, Zhisong Zhang |  |
| 1070 |  |  [Bernice: A Multilingual Pre-trained Encoder for Twitter](https://doi.org/10.18653/v1/2022.emnlp-main.415) |  | 0 | The language of Twitter differs significantly from that of other domains commonly included in large language model training. While tweets are typically multilingual and contain informal language, including emoji and hashtags, most pre-trained language models for Twitter are either monolingual,... | Aaron Mueller, Alexandra DeLucia, Carlos Alejandro Aguirre, Mark Dredze, Philip Resnik, Shijie Wu |  |
| 1071 |  |  [CEFR-Based Sentence Difficulty Annotation and Assessment](https://doi.org/10.18653/v1/2022.emnlp-main.416) |  | 0 | Controllable text simplification is a crucial assistive technique for language learning and teaching. One of the primary factors hindering its advancement is the lack of a corpus annotated with sentence difficulty levels based on language ability descriptions. To address this problem, we created... | Satoru Uchida, Tomoyuki Kajiwara, Yuki Arase |  |
| 1072 |  |  [Simple Questions Generate Named Entity Recognition Datasets](https://doi.org/10.18653/v1/2022.emnlp-main.417) |  | 0 | Recent named entity recognition (NER) models often rely on human-annotated datasets requiring the vast engagement of professional knowledge on the target domain and entities. This work introduces an ask-to-generate approach, which automatically generates NER datasets by asking simple natural... | Hyunjae Kim, Jaehyo Yoo, Jaewoo Kang, Jinhyuk Lee, Seunghyun Yoon |  |
| 1073 |  |  [TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.418) |  | 0 | Language Models (LMs) become outdated as the world changes; they often fail to perform tasks requiring recent factual information which was absent or different during training, a phenomenon called temporal misalignment. This is especially a challenging problem because the research community still... | Changho Lee, Gyeonghun Kim, Janghoon Han, Joel Jang, Joongbo Shin, Minjoon Seo, Seonghyeon Ye, Sohee Yang |  |
| 1074 |  |  [Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.419) |  | 0 | Recently, prompt-tuning has attracted growing interests in event argument extraction (EAE). However, the existing prompt-tuning methods have not achieved satisfactory performance due to the lack of consideration of entity information. In this paper, we propose a bi-directional iterative... | Bang Wang, Lu Dai, Wei Xiang, Yijun Mo |  |
| 1075 |  |  [Learning Robust Representations for Continual Relation Extraction via Adversarial Class Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.420) |  | 0 | Continual relation extraction (CRE) aims to continually learn new relations from a class-incremental data stream. CRE model usually suffers from catastrophic forgetting problem, i.e., the performance of old relations seriously degrades when the model learns new relations. Most previous work... | Binghuai Lin, Peiyi Wang, Sujian Li, Tianyu Liu, Yifan Song, Yunbo Cao, Zhifang Sui |  |
| 1076 |  |  [ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.421) |  | 0 | With the recent advance in large pre-trained language models, researchers have achieved record performances in NLP tasks that mostly focus on language pattern matching. The community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning... | Charese Smiley, Sameena Shah, Shiyang Li, William Yang Wang, Zhiqiang Ma, Zhiyu Chen |  |
| 1077 |  |  [A Span-based Multimodal Variational Autoencoder for Semi-supervised Multimodal Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.422) |  | 0 | Multimodal named entity recognition (MNER) on social media is a challenging task which aims to extract named entities in free text and incorporate images to classify them into user-defined types. However, the annotation for named entities on social media demands a mount of human efforts. The... | Baohang Zhou, Guoqing Zhao, Hongbin Wang, Kehui Song, Wenya Guo, Xiaojie Yuan, Ying Zhang |  |
| 1078 |  |  [R-TeaFor: Regularized Teacher-Forcing for Abstractive Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.423) |  | 0 | Teacher-forcing is widely used in training sequence generation models to improve sampling efficiency and to stabilize training. However, teacher-forcing is vulnerable to the exposure bias problem. Previous works have attempted to address exposure bias by modifying the training data to simulate... | GuanYu Lin, PuJen Cheng |  |
| 1079 |  |  [Modeling Consistency Preference via Lexical Chains for Document-level Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.424) |  | 0 | In this paper we aim to relieve the issue of lexical translation inconsistency for document-level neural machine translation (NMT) by modeling consistency preference for lexical chains, which consist of repeated words in a source-side document and provide a representation of the lexical consistency... | Hao Yang, Junhui Li, Min Zhang, Shimin Tao, Xinglin Lyu, Ying Qin |  |
| 1080 |  |  [Just Fine-tune Twice: Selective Differential Privacy for Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.425) |  | 0 | Protecting large language models from privacy leakage is becoming increasingly crucial with their wide adoption in real-world products. Yet applying \*differential privacy\* (DP), a canonical notion with provable privacy guarantees for machine learning models, to those models remains challenging... | Chiyuan Zhang, Ruoxi Jia, Ryan Shea, Si Chen, Weiyan Shi, Zhou Yu |  |
| 1081 |  |  [Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents](https://doi.org/10.18653/v1/2022.emnlp-main.426) |  | 0 | We argue that disentangling content selection from the budget used to cover salient content improves the performance and applicability of abstractive summarizers. Our method, FactorSum, does this disentanglement by factorizing summarization into two steps through an energy function: (1) generation... | Marcio Fonseca, Shay B. Cohen, Yftah Ziser |  |
| 1082 |  |  [Open-Domain Sign Language Translation Learned from Online Video](https://doi.org/10.18653/v1/2022.emnlp-main.427) |  | 0 | Existing work on sign language translation – that is, translation from sign language videos into sentences in a written language – has focused mainly on (1) data collected in a controlled environment or (2) data in a specific domain, which limits the applicability to real-world settings. In this... | Bowen Shi, Diane Brentari, Gregory Shakhnarovich, Karen Livescu |  |
| 1083 |  |  [Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change](https://doi.org/10.18653/v1/2022.emnlp-main.428) |  | 0 | Recent research has revealed that neural language models at scale suffer from poor temporal generalization capability, i.e., language model pre-trained on static data from past years performs worse over time on emerging data. Existing methods mainly perform continual training to mitigate such a... | Juntao Li, Lijun Wu, Min Zhang, Xinyan Guan, Zecheng Tang, Zhaochen Su |  |
| 1084 |  |  [ULN: Towards Underspecified Vision-and-Language Navigation](https://doi.org/10.18653/v1/2022.emnlp-main.429) |  | 0 | Vision-and-Language Navigation (VLN) is a task to guide an embodied agent moving to a target position using language instructions. Despite the significant performance improvement, the wide use of fine-grained instructions fails to characterize more practical linguistic variations in reality. To... | TsuJui Fu, Weixi Feng, William Yang Wang, Yujie Lu |  |
| 1085 |  |  [Federated Model Decomposition with Private Vocabulary for Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.430) |  | 0 | With the necessity of privacy protection, it becomes increasingly vital to train deep neural models in a federated learning manner for natural language processing (NLP) tasks. However, recent studies show eavesdroppers (i.e., dishonest servers) can still reconstruct the private input in federated... | Lizhen Qu, Qifan Wang, Xiangjing Hu, Zenglin Xu, Zhuo Zhang |  |
| 1086 |  |  [ReCo: Reliable Causal Chain Reasoning via Structural Causal Recurrent Neural Networks](https://doi.org/10.18653/v1/2022.emnlp-main.431) |  | 0 | Causal chain reasoning (CCR) is an essential ability for many decision-making AI systems, which requires the model to build reliable causal chains by connecting causal pairs. However, CCR suffers from two main transitive problems: threshold effect and scene drift. In other words, the causal pairs... | Baoxing Huai, Bing Qin, Kai Xiong, Li Du, Ting Liu, Xiao Ding, Yi Zheng, Zhongyang Li |  |
| 1087 |  |  [Video Question Answering: Datasets, Algorithms and Challenges](https://doi.org/10.18653/v1/2022.emnlp-main.432) |  | 0 | This survey aims to sort out the recent advances in video question answering (VideoQA) and point towards future directions. We firstly categorize the datasets into 1) normal VideoQA, multi-modal VideoQA and knowledge-based VideoQA, according to the modalities invoked in the question-answer pairs,... | Junbin Xiao, TatSeng Chua, Wei Ji, Weihong Deng, Yaoyao Zhong, Yicong Li |  |
| 1088 |  |  [Retrofitting Multilingual Sentence Embeddings with Abstract Meaning Representation](https://doi.org/10.18653/v1/2022.emnlp-main.433) |  | 0 | We introduce a new method to improve existing multilingual sentence embeddings with Abstract Meaning Representation (AMR). Compared with the original textual input, AMR is a structured semantic representation that presents the core concepts and relations in a sentence explicitly and unambiguously.... | Deng Cai, Jackie ChunSing Ho, Lidong Bing, Wai Lam, Xin Li |  |
| 1089 |  |  [Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.434) |  | 0 | Existing research generally treats Chinese character as a minimum unit for representation. However, such Chinese character representation will suffer two bottlenecks: 1) Learning bottleneck, the learning cannot benefit from its rich internal features (e.g., radicals and strokes); and 2) Parameter... | Min Zhang, Xuebo Liu, Zhijun Wang |  |
| 1090 |  |  [Boundary-Driven Table-Filling for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.435) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) aims to extract the aspect terms along with the corresponding opinion terms and the expressed sentiments in the review, which is an important task in sentiment analysis. Previous research efforts generally address the ASTE task in an end-to-end fashion... | Bin Liang, Min Yang, Ruifeng Xu, Shiwei Chen, Yice Zhang, Yifan Yang, Yihui Li, Yixue Dang |  |
| 1091 |  |  [Attention and Edge-Label Guided Graph Convolutional Networks for Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.436) |  | 0 | It has been shown that named entity recognition (NER) could benefit from incorporating the long-distance structured information captured by dependency trees. However, dependency trees built by tools usually have a certain percentage of errors. Under such circumstances, how to better use relevant... | Jian Wan, Jilin Zhang, Qiang Liu, Renjie Zhou, Yong Liao, Zhongyi Xie |  |
| 1092 |  |  [Title2Event: Benchmarking Open Event Extraction with a Large-scale Chinese Title Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.437) |  | 0 | Event extraction (EE) is crucial to downstream tasks such as new aggregation and event knowledge graph construction. Most existing EE datasets manually define fixed event types and design specific schema for each of them, failing to cover diverse events emerging from the online text. Moreover, news... | Changlong Yu, Haolin Deng, Jin Ma, Jun Gao, Nan Yang, Tianhua Zhou, Wangyang Ying, Wei Wang, Xiang Chen, Xiaoling Bai, Yanan Zhang, Yangfan Zhang |  |
| 1093 |  |  [Cascading Biases: Investigating the Effect of Heuristic Annotation Strategies on Data and Models](https://doi.org/10.18653/v1/2022.emnlp-main.438) |  | 0 | Cognitive psychologists have documented that humans use cognitive heuristics, or mental shortcuts, to make quick decisions while expending less effort. While performing annotation work on crowdsourcing platforms, we hypothesize that such heuristic use among annotators cascades on to data quality... | Chaitanya Malaviya, Mark Yatskar, Sudeep Bhatia |  |
| 1094 |  |  [Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts](https://doi.org/10.18653/v1/2022.emnlp-main.439) |  | 0 | Question-answering datasets require a broad set of reasoning skills. We show how to use question decompositions to teach language models these broad reasoning skills in a robust fashion. Specifically, we use widely available QDMR representations to programmatically create hard-to-cheat synthetic... | Ashish Sabharwal, Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot |  |
| 1095 |  |  [ADDMU: Detection of Far-Boundary Adversarial Examples with Data and Model Uncertainty Estimation](https://doi.org/10.18653/v1/2022.emnlp-main.440) |  | 0 | Adversarial Examples Detection (AED) is a crucial defense technique against adversarial attacks and has drawn increasing attention from the Natural Language Processing (NLP) community. Despite the surge of new AED methods, our studies show that existing methods heavily rely on a shortcut to achieve... | ChoJui Hsieh, Fan Yin, KaiWei Chang, Yao Li |  |
| 1096 |  |  [G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.441) |  | 0 | General pre-trained language models (PLMs), such as BERT, have achieved remarkable performance on various NLP tasks. Recently, domain-specific PLMs have been proposed to boost the task performance of specific domains (e.g., biomedical and computer science) by continuing to pre-train general PLMs... | Guangyong Chen, Jiaxin Shi, Lifeng Shang, Qun Liu, Wei Zhang, Xin Jiang, Yichun Yin, Zhongwei Wan |  |
| 1097 |  |  [Towards Unifying Reference Expression Generation and Comprehension](https://doi.org/10.18653/v1/2022.emnlp-main.442) |  | 0 | Reference Expression Generation (REG) and Comprehension (REC) are two highly correlated tasks. Modeling REG and REC simultaneously for utilizing the relation between them is a promising way to improve both. However, the problem of distinct inputs, as well as building connections between them in a... | Duo Zheng, Jiaan Wang, Tao Kong, Xiaojie Wang, Ya Jing |  |
| 1098 |  |  [Textual Manifold-based Defense Against Natural Language Adversarial Examples](https://doi.org/10.18653/v1/2022.emnlp-main.443) |  | 0 | Despite the recent success of large pretrained language models in NLP, they are susceptible to adversarial examples. Concurrently, several studies on adversarial images have observed an intriguing property: the adversarial images tend to leave the low-dimensional natural data manifold. In this... | Anh Tuan Luu, Dang Nguyen Minh |  |
| 1099 |  |  [Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters](https://doi.org/10.18653/v1/2022.emnlp-main.444) |  | 0 | Adapter-tuning is a paradigm that transfers a pretrained language model to downstream tasks by adding and tuning a small number of new parameters. Previously proposed adapter architectures are all feed-forward neural networks. In this paper, we investigate the effectiveness of using... | Hao Tan, Hongyu Zhao, Hongyuan Mei |  |
| 1100 |  |  [Reduce Catastrophic Forgetting of Dense Retrieval Training with Teleportation Negatives](https://doi.org/10.18653/v1/2022.emnlp-main.445) |  | 0 | In this paper, we investigate the instability in the standard dense retrieval training, which iterates between model training and hard negative selection using the being-trained model. We show the catastrophic forgetting phenomena behind the training instability, where models learn and forget... | Arnold Overwijk, Chenyan Xiong, Jie Bao, Si Sun, Yue Yu, Zhiyuan Liu |  |
| 1101 |  |  [ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts](https://doi.org/10.18653/v1/2022.emnlp-main.446) |  | 0 | This work introduces a new multi-task, parameter-efficient language model (LM) tuning method that learns to transfer knowledge across different tasks via a mixture of soft prompts—small prefix embedding vectors pre-trained for different tasks. Our method, called ATTEMPT (ATTEntional Mixtures of... | Akari Asai, Hannaneh Hajishirzi, Matthew E. Peters, Mohammadreza Salehi |  |
| 1102 |  |  [Exploration of the Usage of Color Terms by Color-blind Participants in Online Discussion Platforms](https://doi.org/10.18653/v1/2022.emnlp-main.447) |  | 0 | Prominent questions about the role of sensory vs. linguistic input in the way we acquire and use language have been extensively studied in the psycholinguistic literature. However, the relative effect of various factors in a person’s overall experience on their linguistic system remains unclear. We... | Boaz Carmeli, Ella Rabinovich |  |
| 1103 |  |  [DEER: Descriptive Knowledge Graph for Explaining Entity Relationships](https://doi.org/10.18653/v1/2022.emnlp-main.448) |  | 0 | We propose DEER (Descriptive Knowledge Graph for Explaining Entity Relationships) - an open and informative form of modeling entity relationships. In DEER, relationships between entities are represented by free-text relation descriptions. For instance, the relationship between entities of machine... | Jie Huang, Jinjun Xiong, Kerui Zhu, Kevin ChenChuan Chang, WenMei Hwu |  |
| 1104 |  |  [META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI](https://doi.org/10.18653/v1/2022.emnlp-main.449) |  | 0 | Task-oriented dialogue (TOD) systems have been widely used by mobile phone intelligent assistants to accomplish tasks such as calendar scheduling or hotel reservation. Current TOD systems usually focus on multi-turn text/speech interaction, then they would call back-end APIs designed for TODs to... | Kai Yu, Liangtai Sun, Lu Chen, Tianle Dai, Xingyu Chen, Zichen Zhu |  |
| 1105 |  |  [Understanding and Improving Knowledge Distillation for Quantization Aware Training of Large Transformer Encoders](https://doi.org/10.18653/v1/2022.emnlp-main.450) |  | 0 | Knowledge distillation (KD) has been a ubiquitous method for model compression to strengthen the capability of a lightweight model with the transferred knowledge from the teacher. In particular, KD has been employed in quantization-aware training (QAT) of Transformer encoders like BERT to improve... | DuSeong Chang, Jungwook Choi, Minsoo Kim, Sihwa Lee, Sukjin Hong |  |
| 1106 |  |  [Exploring Mode Connectivity for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.451) |  | 0 | Recent years have witnessed the prevalent application of pre-trained language models (PLMs) in NLP. From the perspective of parameter space, PLMs provide generic initialization, starting from which high-performance minima could be found. Although plenty of works have studied how to effectively and... | Cheng Qian, Jie Zhou, Jing Yi, Maosong Sun, Weize Chen, Xu Han, Yankai Lin, Yujia Qin, Zhiyuan Liu |  |
| 1107 |  |  [Synergy with Translation Artifacts for Training and Inference in Multilingual Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.452) |  | 0 | Translation has played a crucial role in improving the performance on multilingual tasks: (1) to generate the target language data from the source language data for training and (2) to generate the source language data from the target language data for inference. However, prior works have not... | Jaehoon Oh, Jongwoo Ko, SeYoung Yun |  |
| 1108 |  |  [Increasing Visual Awareness in Multimodal Neural Machine Translation from an Information Theoretic Perspective](https://doi.org/10.18653/v1/2022.emnlp-main.453) |  | 0 | Multimodal machine translation (MMT) aims to improve translation quality by equipping the source sentence with its corresponding image. Despite the promising performance, MMT models still suffer the problem of input degradation: models focus more on textual information while visual information is... | Baijun Ji, Bojie Hu, Si Shen, Tong Zhang, Yicheng Zou |  |
| 1109 |  |  [Improving Event Coreference Resolution Using Document-level and Topic-level Information](https://doi.org/10.18653/v1/2022.emnlp-main.454) |  | 0 | Event coreference resolution (ECR) aims to cluster event mentions that refer to the same real-world events. Deep learning methods have achieved SOTA results on the ECR task. However, due to the encoding length limitation, previous methods either adopt classical pairwise models based on... | Peifeng Li, Qiaoming Zhu, Sheng Xu |  |
| 1110 |  |  [Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.455) |  | 0 | Prompt Tuning has been largely successful as a parameter-efficient method of conditioning large-scale pre-trained language models to perform downstream tasks. Thus far, soft prompt tuning learns a fixed set of task-specific continuous vectors, i.e., soft tokens that remain static across the task... | Amrita Saha, Rishabh Bhardwaj, Soujanya Poria, Steven C. H. Hoi |  |
| 1111 |  |  [Boosting Natural Language Generation from Instructions with Meta-Learning](https://doi.org/10.18653/v1/2022.emnlp-main.456) |  | 0 | Recent work has shown that language models (LMs) trained with multi-task instructional learning (MTIL) can solve diverse NLP tasks in zero- and few-shot settings with improved performance compared to prompt tuning. MTIL illustrates that LMs can extract and use information about the task from... | Ahmed Hassan Awadallah, Budhaditya Deb, Guoqing Zheng |  |
| 1112 |  |  [Topical Segmentation of Spoken Narratives: A Test Case on Holocaust Survivor Testimonies](https://doi.org/10.18653/v1/2022.emnlp-main.457) |  | 0 | The task of topical segmentation is well studied, but previous work has mostly addressed it in the context of structured, well-defined segments, such as segmentation into paragraphs, chapters, or segmenting text that originated from multiple sources. We tackle the task of segmenting running... | Amit Pinchevski, Eitan Wagner, Omri Abend, Renana Keydar |  |
| 1113 |  |  [Unifying the Convergences in Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.458) |  | 0 | Although all-in-one-model multilingual neural machine translation (MNMT) has achieved remarkable progress, the convergence inconsistency in the joint training is ignored, i.e., different language pairs reaching convergence in different epochs. This leads to the trained MNMT model over-fitting... | Bing Qin, Xiaocheng Feng, Xinwei Geng, YiChong Huang |  |
| 1114 |  |  [Modeling Label Correlations for Ultra-Fine Entity Typing with Neural Pairwise Conditional Random Field](https://doi.org/10.18653/v1/2022.emnlp-main.459) |  | 0 | Ultra-fine entity typing (UFET) aims to predict a wide range of type phrases that correctly describe the categories of a given entity mention in a sentence. Most recent works infer each entity type independently, ignoring the correlations between types, e.g., when an entity is inferred as a... | Chengyue Jiang, Kewei Tu, Pengjun Xie, Weiqi Wu, Yong Jiang |  |
| 1115 |  |  [Help me write a Poem - Instruction Tuning as a Vehicle for Collaborative Poetry Writing](https://doi.org/10.18653/v1/2022.emnlp-main.460) |  | 0 | Recent work in training large language models (LLMs) to follow natural language instructions has opened up exciting opportunities for natural language interface design. Building on the prior success of large language models in the realm of computer assisted creativity, in this work, we present... | He He, Tuhin Chakrabarty, Vishakh Padmakumar |  |
| 1116 |  |  [Open Relation and Event Type Discovery with Type Abstraction](https://doi.org/10.18653/v1/2022.emnlp-main.461) |  | 0 | Conventional “closed-world” information extraction (IE) approaches rely on human ontologies to define the scope for extraction. As a result, such approaches fall short when applied to new domains. This calls for systems that can automatically infer new types from given corpora, a task which we... | Heng Ji, Jiawei Han, Sha Li |  |
| 1117 |  |  [Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples](https://doi.org/10.18653/v1/2022.emnlp-main.462) |  | 0 | Knowledge-enhanced language representation learning has shown promising results across various knowledge-intensive NLP tasks. However, prior methods are limited in efficient utilization of multilingual knowledge graph (KG) data for language model (LM) pretraining. They often train LMs with KGs in... | Lidong Bing, Linlin Liu, Luo Si, Ruidan He, Shafiq R. Joty, Xin Li |  |
| 1118 |  |  [Revisiting Grammatical Error Correction Evaluation and Beyond](https://doi.org/10.18653/v1/2022.emnlp-main.463) |  | 0 | Pretraining-based (PT-based) automatic evaluation metrics (e.g., BERTScore and BARTScore) have been widely used in several sentence generation tasks (e.g., machine translation and text summarization) due to their better correlation with human judgments over traditional overlap-based methods.... | Heyan Huang, Min Zhang, Peiyuan Gong, Xuebo Liu |  |
| 1119 |  |  [R2D2: Robust Data-to-Text with Replacement Detection](https://doi.org/10.18653/v1/2022.emnlp-main.464) |  | 0 | Unfaithful text generation is a common problem for text generation systems. In the case of Data-to-Text (D2T) systems, the factuality of the generated text is particularly crucial for any real-world applications. We introduce R2D2, a training framework that addresses unfaithful Data-to-Text... | Dragomir Radev, Linyong Nan, Lorenzo Jaime Yu Flores, Luke Benson, Weijin Zou, Yilun Zhao, Yixin Liu |  |
| 1120 |  |  [IDK-MRC: Unanswerable Questions for Indonesian Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.emnlp-main.465) |  | 0 | Machine Reading Comprehension (MRC) has become one of the essential tasks in Natural Language Understanding (NLU) as it is often included in several NLU benchmarks (Liang et al., 2020; Wilie et al., 2020). However, most MRC datasets only have answerable question type, overlooking the importance of... | Alice Oh, Rifki Afina Putri |  |
| 1121 |  |  [XLM-D: Decorate Cross-lingual Pre-training Model as Non-Autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.466) |  | 0 | Pre-training language models have achieved thriving success in numerous natural language understanding and autoregressive generation tasks, but non-autoregressive generation in applications such as machine translation has not sufficiently benefited from the pre-training paradigm. In this work, we... | Daxin Jiang, Guanhua Chen, Shilin He, Yong Wang, Yun Chen |  |
| 1122 |  |  [Cross-stitching Text and Knowledge Graph Encoders for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.467) |  | 0 | Bi-encoder architectures for distantly-supervised relation extraction are designed to make use of the complementary information found in text and knowledge graphs (KG).However, current architectures suffer from two drawbacks. They either do not allow any sharing between the text encoder and the KG... | Benjamin Heinzerling, Kentaro Inui, Qin Dai |  |
| 1123 |  |  [Assist Non-native Viewers: Multimodal Cross-Lingual Summarization for How2 Videos](https://doi.org/10.18653/v1/2022.emnlp-main.468) |  | 0 | Multimodal summarization for videos aims to generate summaries from multi-source information (videos, audio transcripts), which has achieved promising progress. However, existing works are restricted to monolingual video scenarios, ignoring the demands of non-native video viewers to understand the... | Fanglong Yao, Guangluan Xu, Hongfeng Yu, Kaiwen Wei, Li Jin, Nayu Liu, Xian Sun, Zhi Guo |  |
| 1124 |  |  [PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance](https://doi.org/10.18653/v1/2022.emnlp-main.469) |  | 0 | To facilitate conversational question answering (CQA) over hybrid contexts in finance, we present a new dataset, named PACIFIC. Compared with existing CQA datasets, PACIFIC exhibits three key features: (i) proactivity, (ii) numerical reasoning, and (iii) hybrid context of tables and text. A new... | TatSeng Chua, Wai Lam, Wenqiang Lei, Wenxuan Zhang, Yang Deng |  |
| 1125 |  |  [Generative Data Augmentation with Contrastive Learning for Zero-Shot Stance Detection](https://doi.org/10.18653/v1/2022.emnlp-main.470) |  | 0 | Stance detection aims to identify whether the author of an opinionated text is in favor of, against, or neutral towards a given target. Remarkable success has been achieved when sufficient labeled training data is available. However, it is labor-intensive to annotate sufficient data and train the... | Jiawei Yuan, Yang Li |  |
| 1126 |  |  [Better Few-Shot Relation Extraction with Label Prompt Dropout](https://doi.org/10.18653/v1/2022.emnlp-main.471) |  | 0 | Few-shot relation extraction aims to learn to identify the relation between two entities based on very limited training examples. Recent efforts found that textual labels (i.e., relation names and relation descriptions) could be extremely useful for learning class representations, which will... | Peiyuan Zhang, Wei Lu |  |
| 1127 |  |  [Break it Down into BTS: Basic, Tiniest Subword Units for Korean](https://doi.org/10.18653/v1/2022.emnlp-main.472) |  | 0 | We introduce Basic, Tiniest Subword (BTS) units for the Korean language, which are inspired by the invention principle of Hangeul, the Korean writing system. Instead of relying on 51 Korean consonant and vowel letters, we form the letters from BTS units by adding strokes or combining them. To... | Eojin Jeon, JoonYoung Choi, JunHyung Park, Nayeon Kim, SangKeun Lee, Youjin Kang |  |
| 1128 |  |  [The Devil in Linear Transformer](https://doi.org/10.18653/v1/2022.emnlp-main.473) |  | 0 | Linear transformers aim to reduce the quadratic space-time complexity of vanilla transformers. However, they usually suffer from degraded performances on various tasks and corpus. In this paper, we examine existing kernel-based linear transformers and identify two key issues that lead to such... | Dongxu Li, Lingpeng Kong, Nick Barnes, Weixuan Sun, Xiaodong Han, Yiran Zhong, Zhen Qin |  |
| 1129 |  |  [Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective](https://doi.org/10.18653/v1/2022.emnlp-main.474) |  | 0 | We propose a new paradigm for zero-shot learners that is format agnostic, i.e., it is compatible with any format and applicable to a list of language tasks, such as text classification, commonsense reasoning, coreference resolution, and sentiment analysis. Zero-shot learning aims to train a model... | Jiaxing Zhang, Junjie Wang, Lin Zhang, Ping Yang, Ruyi Gan, Tetsuya Sakai, Xinyu Gao, Xinyu Zhu, Ziwei Wu |  |
| 1130 |  |  [Hypoformer: Hybrid Decomposition Transformer for Edge-friendly Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.475) |  | 0 | Transformer has been demonstrated effective in Neural Machine Translation (NMT). However, it is memory-consuming and time-consuming in edge devices, resulting in some difficulties for real-time feedback. To compress and accelerate Transformer, we propose a Hybrid Tensor-Train (HTT) decomposition,... | Benyou Wang, Guobing Gan, Peng Zhang, Sunzhu Li, Victor Junqiu Wei, Xin Jiang, Xiuqing Lv |  |
| 1131 |  |  [FigMemes: A Dataset for Figurative Language Identification in Politically-Opinionated Memes](https://doi.org/10.18653/v1/2022.emnlp-main.476) |  | 0 | Real-world politically-opinionated memes often rely on figurative language to cloak propaganda and radical ideas to help them spread. It is not only a scientific challenge to develop machine learning models to recognize them in memes, but also sociologically beneficial to understand hidden meanings... | Chen Liu, Gregor Geigle, Iryna Gurevych, Robin Krebs |  |
| 1132 |  |  [UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.477) |  | 0 | Relational triple extraction is challenging for its difficulty in capturing rich correlations between entities and relations. Existing works suffer from 1) heterogeneous representations of entities and relations, and 2) heterogeneous modeling of entity-entity interactions and entity-relation... | Benfeng Xu, Haiyong Xie, Wei Tang, Yifeng Liu, Yong Liao, Yuyue Zhao, Zhendong Mao |  |
| 1133 |  |  [X-FACTOR: A Cross-metric Evaluation of Factual Correctness in Abstractive Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.478) |  | 0 | Abstractive summarization models often produce factually inconsistent summaries that are not supported by the original article. Recently, a number of fact-consistent evaluation techniques have been proposed to address this issue; however, a detailed analysis of how these metrics agree with one... | Alexander Gray, Daiki Kimura, Keerthiram Murugesan, Maxwell Crouse, Pavan Kapanipathi, R. Chulaka Gunasekara, Ramón Fernandez Astudillo, Sarathkrishna Swaminathan, Srinivas Ravishankar, Subhajit Chaudhury, Tahira Naseem |  |
| 1134 |  |  [ParaTag: A Dataset of Paraphrase Tagging for Fine-Grained Labels, NLG Evaluation, and Data Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.479) |  | 0 | Paraphrase identification has been formulated as a binary classification task to decide whether two sentences hold a paraphrase relationship. Existing paraphrase datasets only annotate a binary label for each sentence pair. However, after a systematical analysis of existing paraphrase datasets, we... | Chenguang Zhu, Michael Zeng, Ruochen Xu, Shuohang Wang, Yang Liu |  |
| 1135 |  |  [Factual Accuracy is not Enough: Planning Consistent Description Order for Radiology Report Generation](https://doi.org/10.18653/v1/2022.emnlp-main.480) |  | 0 | Radiology report generation systems have the potential to reduce the workload of radiologists by automatically describing the findings in medical images.To broaden the application of the report generation system, the system should generate reports that are not only factually accurate but also... | Noriyuki Tomiyama, Shoji Kido, Tomoki Taniguchi, Tomoko Ohkuma, Toru Nishino, Yasuhide Miura, Yuki Suzuki |  |
| 1136 |  |  [FLUTE: Figurative Language Understanding through Textual Explanations](https://doi.org/10.18653/v1/2022.emnlp-main.481) |  | 0 | Figurative language understanding has been recently framed as a recognizing textual entailment (RTE) task (a.k.a. natural language inference (NLI)). However, similar to classical RTE/NLI datasets they suffer from spurious correlations and annotation artifacts. To tackle this problem, work on NLI... | Arkadiy Saakyan, Debanjan Ghosh, Smaranda Muresan, Tuhin Chakrabarty |  |
| 1137 |  |  [Precisely the Point: Adversarial Augmentations for Faithful and Informative Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.482) |  | 0 | Though model robustness has been extensively studied in language understanding, the robustness of Seq2Seq generation remains understudied.In this paper, we conduct the first quantitative analysis on the robustness of pre-trained Seq2Seq models. We find that even current SOTA pre-trained Seq2Seq... | Jiachen Liu, Sujian Li, Wei Li, Wenhao Wu, Xinyan Xiao, Yajuan Lyu |  |
| 1138 |  |  [RLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees](https://doi.org/10.18653/v1/2022.emnlp-main.483) |  | 0 | Interpreting the reasoning process from questions to answers poses a challenge in approaching explainable QA. A recently proposed structured reasoning format, entailment tree, manages to offer explicit logical deductions with entailment steps in a tree structure. To generate entailment trees, prior... | Qipeng Guo, Tengxiao Liu, Xiangkun Hu, Xipeng Qiu, Yue Zhang, Zheng Zhang |  |
| 1139 |  |  [Let the CAT out of the bag: Contrastive Attributed explanations for Text](https://doi.org/10.18653/v1/2022.emnlp-main.484) |  | 0 | Contrastive explanations for understanding the behavior of black box models has gained a lot of attention recently as they provide potential for recourse. In this paper, we propose a method Contrastive Attributed explanations for Text (CAT) which provides contrastive explanations for natural... | Amar Prakash Azad, Amit Dhurandhar, Ronny Luss, Saneem A. Chemmengath |  |
| 1140 |  |  [monoQA: Multi-Task Learning of Reranking and Answer Extraction for Open-Retrieval Conversational Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.485) |  | 0 | To address the Conversational Question Answering (ORConvQA) task, previous work has considered an effective three-stage architecture, consisting of a retriever, a reranker, and a reader to extract the answers. In order to effectively answer the users’ questions, a number of existing approaches have... | Craig Macdonald, Iadh Ounis, Sarawoot Kongyoung |  |
| 1141 |  |  [Composing Ci with Reinforced Non-autoregressive Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.486) |  | 0 | Composing Ci (also widely known as Song Ci), a special type of classical Chinese poetry, requires to follow particular format once their tune patterns are given. To automatically generate a well-formed Ci, text generation systems should strictly take into account pre-defined rigid formats (e.g.,... | Yan Song |  |
| 1142 |  |  [MetaTKG: Learning Evolutionary Meta-Knowledge for Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.487) |  | 0 | Reasoning over Temporal Knowledge Graphs (TKGs) aims to predict future facts based on given history. One of the key challenges for prediction is to learn the evolution of facts. Most existing works focus on exploring evolutionary information in history to obtain effective temporal embeddings for... | Mengqi Zhang, Qiang Liu, Shu Wu, Xiaoyu Zhang, Yuwei Xia |  |
| 1143 |  |  [mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections](https://doi.org/10.18653/v1/2022.emnlp-main.488) |  | 0 | Large-scale pre-trained foundation models have been an emerging paradigm for building artificial intelligence (AI) systems, which can be quickly adapted to a wide range of downstream tasks. This paper presents mPLUG, a new vision-language foundation model for both cross-modal understanding and... | Bin Bi, Chenliang Li, Fei Huang, Guohai Xu, Haiyang Xu, He Chen, Ji Zhang, Jiabo Ye, Jingren Zhou, Junfeng Tian, Luo Si, Ming Yan, Songfang Huang, Wei Wang, Zheng Cao |  |
| 1144 |  |  [Q-TOD: A Query-driven Task-oriented Dialogue System](https://doi.org/10.18653/v1/2022.emnlp-main.489) |  | 0 | Existing pipelined task-oriented dialogue systems usually have difficulties adapting to unseen domains, whereas end-to-end systems are plagued by large-scale knowledge bases in practice. In this paper, we introduce a novel query-driven task-oriented dialogue system, namely Q-TOD. The essential... | Fan Wang, Hua Wu, Huang He, Mengfei Song, Shuqi Sun, Siqi Bao, Xin Tian, Yingzhan Lin |  |
| 1145 |  |  [Dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.490) |  | 0 | In this paper, we introduce the task of learning unsupervised dialogue embeddings.Trivial approaches such as combining pre-trained word or sentence embeddings and encoding through pre-trained language models (PLMs) have been shown to be feasible for this task.However, these approaches typically... | Che Liu, Fei Huang, Junfeng Jiang, Rui Wang, Yongbin Li |  |
| 1146 |  |  [WR-One2Set: Towards Well-Calibrated Keyphrase Generation](https://doi.org/10.18653/v1/2022.emnlp-main.491) |  | 0 | Keyphrase generation aims to automatically generate short phrases summarizing an input document. The recently emerged ONE2SET paradigm (Ye et al., 2021) generates keyphrases as a set and has achieved competitive performance. Nevertheless, we observe serious calibration errors outputted by ONE2SET,... | Baosong Yang, Binbin Xie, Huan Lin, Jinsong Su, Jun Xie, Min Zhang, Xiangpeng Wei, Xiaoli Wang |  |
| 1147 |  |  [Eeny, meeny, miny, moe. How to choose data for morphological inflection](https://doi.org/10.18653/v1/2022.emnlp-main.492) |  | 0 | Data scarcity is a widespread problem for numerous natural language processing (NLP) tasks within low-resource languages. Within morphology, the labour-intensive task of tagging/glossing data is a serious bottleneck for both NLP and fieldwork. Active learning (AL) aims to reduce the cost of data... | Mans Hulden, Saliha Muradoglu |  |
| 1148 |  |  [An Adaptive Logical Rule Embedding Model for Inductive Reasoning over Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.493) |  | 0 | Temporal knowledge graphs (TKGs) extrapolation reasoning predicts future events based on historical information, which has great research significance and broad application value. Existing methods can be divided into embedding-based methods and logical rule-based methods. Embedding-based methods... | Libin Yang, Xiaoyan Cai, Xin Mei, Zuowei Jiang |  |
| 1149 |  |  [UniNL: Aligning Representation Learning with Scoring Function for OOD Detection via Unified Neighborhood Learning](https://doi.org/10.18653/v1/2022.emnlp-main.494) |  | 0 | Detecting out-of-domain (OOD) intents from user queries is essential for avoiding wrong operations in task-oriented dialogue systems. The key challenge is how to distinguish in-domain (IND) and OOD intents. Previous methods ignore the alignment between representation learning and scoring function,... | Jingang Wang, Keqing He, Pei Wang, Wei Wu, Weiran Xu, Yanan Wu, Yutao Mou |  |
| 1150 |  |  [Open-domain Video Commentary Generation](https://doi.org/10.18653/v1/2022.emnlp-main.495) |  | 0 | Live commentary plays an important role in sports broadcasts and video games, making spectators more excited and immersed. In this context, though approaches for automatically generating such commentary have been proposed in the past, they have been generally concerned with specific fields, where... | Edison MarreseTaylor, Goran Topic, Hiroya Takamura, Ichiro Kobayashi, Tatsuya Ishigaki, Yumi Hamazono, Yusuke Miyao |  |
| 1151 |  |  [One size does not fit all: Investigating strategies for differentially-private learning across NLP tasks](https://doi.org/10.18653/v1/2022.emnlp-main.496) |  | 0 | Preserving privacy in contemporary NLP models allows us to work with sensitive data, but unfortunately comes at a price. We know that stricter privacy guarantees in differentially-private stochastic gradient descent (DP-SGD) generally degrade model performance. However, previous research on the... | Ivan Habernal, Manuel Senge, Timour Igamberdiev |  |
| 1152 |  |  [Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario](https://doi.org/10.18653/v1/2022.emnlp-main.497) |  | 0 | People can acquire knowledge in an unsupervised manner by reading, and compose the knowledge to make novel combinations. In this paper, we investigate whether pretrained language models can perform compositional generalization in a realistic setting: recipe generation. We design the counterfactual... | Chengang Hu, Dongyan Zhao, Jizhi Tang, Xiao Liu, Yansong Feng |  |
| 1153 |  |  [Tutoring Helps Students Learn Better: Improving Knowledge Distillation for BERT with Tutor Network](https://doi.org/10.18653/v1/2022.emnlp-main.498) |  | 0 | Pre-trained language models have achieved remarkable successes in natural language processing tasks, coming at the cost of increasing model size. To address this issue, knowledge distillation (KD) has been widely applied to compress language models. However, typical KD approaches for language... | JoonYoung Choi, JunHyung Park, Junho Kim, Mingyu Lee, SangKeun Lee, WingLam Mok |  |
| 1154 |  |  [Does Corpus Quality Really Matter for Low-Resource Languages?](https://doi.org/10.18653/v1/2022.emnlp-main.499) |  | 0 | The vast majority of non-English corpora are derived from automatically filtered versions of CommonCrawl. While prior work has identified major issues on the quality of these datasets (Kreutzer et al., 2021), it is not clear how this impacts downstream performance. Taking representation learning in... | Aitor Soroa, Itziar Aldabe, Mikel Artetxe, Olatz PerezdeViñaspre, Rodrigo Agerri |  |
| 1155 |  |  [Unifying Data Perspectivism and Personalization: An Application to Social Norms](https://doi.org/10.18653/v1/2022.emnlp-main.500) |  | 0 | Instead of using a single ground truth for language processing tasks, several recent studies have examined how to represent and predict the labels of the set of annotators. However, often little or no information about annotators is known, or the set of annotators is small. In this work, we examine... | Béla Neuendorf, Charles Welch, Joan Plepi, Lucie Flek |  |
| 1156 |  |  [Does Self-Rationalization Improve Robustness to Spurious Correlations?](https://doi.org/10.18653/v1/2022.emnlp-main.501) |  | 0 | Rationalization is fundamental to human reasoning and learning. NLP models trained to produce rationales along with predictions, called self-rationalization models, have been investigated for their interpretability and utility to end-users. However, the extent to which training with human-written... | Alexis Ross, Ana Marasovic, Matthew E. Peters |  |
| 1157 |  |  [Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking](https://doi.org/10.18653/v1/2022.emnlp-main.502) |  | 0 | Self-supervised pre-training has achieved remarkable success in extensive natural language processing tasks. Masked language modeling (MLM) has been widely used for pre-training effective bidirectional representations but comes at a substantial training cost. In this paper, we propose a novel... | JunHyung Park, Junho Kim, KangMin Kim, Mingyu Lee, SangKeun Lee |  |
| 1158 |  |  [Subword Evenness (SuE) as a Predictor of Cross-lingual Transfer to Low-resource Languages](https://doi.org/10.18653/v1/2022.emnlp-main.503) |  | 0 | Pre-trained multilingual models, such as mBERT, XLM-R and mT5, are used to improve the performance on various tasks in low-resource languages via cross-lingual transfer. In this framework, English is usually seen as the most natural choice for a transfer language (for fine-tuning or continued... | Anastassia Shaitarova, Olga Pelloni, Tanja Samardzic |  |
| 1159 |  |  [A Unified Neural Network Model for Readability Assessment with Feature Projection and Length-Balanced Loss](https://doi.org/10.18653/v1/2022.emnlp-main.504) |  | 0 | Readability assessment is a basic research task in the field of education. Traditional methods mainly employ machine learning classifiers with hundreds of linguistic features. Although the deep learning model has become the prominent approach for almost all NLP tasks, it is less explored for... | Wenbiao Li, Yunfang Wu, Ziyang Wang |  |
| 1160 |  |  [Speaker Overlap-aware Neural Diarization for Multi-party Meeting Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.505) |  | 0 | Recently, hybrid systems of clustering and neural diarization models have been successfully applied in multi-party meeting analysis. However, current models always treat overlapped speaker diarization as a multi-label classification problem, where speaker dependency and overlaps are not well... | Shiliang Zhang, Siqi Zheng, ZhiJie Yan, Zhihao Du |  |
| 1161 |  |  [GREENER: Graph Neural Networks for News Media Profiling](https://doi.org/10.18653/v1/2022.emnlp-main.506) |  | 0 | We study the problem of profiling news media on the Web with respect to their factuality of reporting and bias. This is an important but under-studied problem related to disinformation and “fake news” detection, but it addresses the issue at a coarser granularity compared to looking at an... | Husrev Taha Sencar, Mohamed Nabeel, Panayot Panayotov, Preslav Nakov, Utsav Shukla |  |
| 1162 |  |  [Graph Hawkes Transformer for Extrapolated Reasoning on Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.507) |  | 0 | Temporal Knowledge Graph (TKG) reasoning has attracted increasing attention due to its enormous potential value, and the critical issue is how to model the complex temporal structure information effectively. Recent studies use the method of encoding graph snapshots into hidden vector space and then... | Han Hu, Haohai Sun, Jialun Zhong, Kun He, Shangyi Geng |  |
| 1163 |  |  [UniRPG: Unified Discrete Reasoning over Table and Text as Program Generation](https://doi.org/10.18653/v1/2022.emnlp-main.508) |  | 0 | Question answering requiring discrete reasoning, e.g., arithmetic computing, comparison, and counting, over knowledge is a challenging task.In this paper, we propose UniRPG, a semantic-parsing-based approach advanced in interpretability and scalability, to perform Unified discrete Reasoning over... | Chaoqun Duan, Junwei Bao, Tiejun Zhao, Xiaodong He, Yongwei Zhou, Youzheng Wu |  |
| 1164 |  |  [Don't Prompt, Search! Mining-based Zero-Shot Learning with Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.509) |  | 0 | Masked language models like BERT can perform text classification in a zero-shot fashion by reformulating downstream tasks as text infilling. However, this approach is highly sensitive to the template used to prompt the model, yet practitioners are blind when designing them in strict zero-shot... | Danqi Chen, Mengzhou Xia, Mikel Artetxe, Mozes van de Kar |  |
| 1165 |  |  [SEMGraph: Incorporating Sentiment Knowledge and Eye Movement into Graph Model for Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.510) |  | 0 | This paper investigates the sentiment analysis task from a novel perspective by incorporating sentiment knowledge and eye movement into a graph architecture, aiming to draw the eye movement-based sentiment relationships for learning the sentiment expression of the context. To be specific, we first... | Bin Liang, Bingbing Wang, Jiachen Du, Min Yang, Ruifeng Xu |  |
| 1166 |  |  [Cross-lingual neural fuzzy matching for exploiting target-language monolingual corpora in computer-aided translation](https://doi.org/10.18653/v1/2022.emnlp-main.511) |  | 0 | Computer-aided translation (CAT) tools based on translation memories (MT) play a prominent role in the translation workflow of professional translators. However, the reduced availability of in-domain TMs, as compared to in-domain monolingual corpora, limits its adoption for a number of translation... | Felipe SánchezMartínez, Juan Antonio PérezOrtiz, Miquel EsplàGomis, Víctor M. SánchezCartagena |  |
| 1167 |  |  [Multi-Label Intent Detection via Contrastive Task Specialization of Sentence Encoders](https://doi.org/10.18653/v1/2022.emnlp-main.512) |  | 0 | Deploying task-oriented dialog ToD systems for new domains and tasks requires natural language understanding models that are 1) resource-efficient and work under low-data regimes; 2) adaptable, efficient, and quick-to-train; 3) expressive and can handle complex ToD scenarios with multiple user... | Avishek Mondal, Georgios Spithourakis, Ivan Vulic, Iñigo Casanueva, Pawel Budzianowski, TsungHsien Wen |  |
| 1168 |  |  [Discovering Language-neutral Sub-networks in Multilingual Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.513) |  | 0 | Multilingual pre-trained language models transfer remarkably well on cross-lingual downstream tasks. However, the extent to which they learn language-neutral representations (i.e., shared representations that encode similar phenomena across languages), and the effect of such representations on... | Antoine Bosselut, Karl Aberer, Mohammadreza Banaei, Negar Foroutan, Rémi Lebret |  |
| 1169 |  |  [Parameter-Efficient Tuning Makes a Good Classification Head](https://doi.org/10.18653/v1/2022.emnlp-main.514) |  | 0 | In recent years, pretrained models revolutionized the paradigm of natural language understanding (NLU), where we append a randomly initialized classification head after the pretrained backbone, e.g. BERT, and finetune the whole model. As the pretrained backbone makes a major contribution to the... | Jie Tang, Ming Ding, Qingsong Lv, Yanhui Guo, Zhuoyi Yang |  |
| 1170 |  |  [STGN: an Implicit Regularization Method for Learning with Noisy Labels in Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-main.515) |  | 0 | Noisy labels are ubiquitous in natural language processing (NLP) tasks. Existing work, namely learning with noisy labels in NLP, is often limited to dedicated tasks or specific training procedures, making it hard to be widely used. To address this issue, SGD noise has been explored to provide a... | Bing Qin, Hao Zhang, Minji Tang, Ting Liu, Tingting Wu, Xiao Ding |  |
| 1171 |  |  [Cross-Modal Similarity-Based Curriculum Learning for Image Captioning](https://doi.org/10.18653/v1/2022.emnlp-main.516) |  | 0 | Image captioning models require the high-level generalization ability to describe the contents of various images in words. Most existing approaches treat the image–caption pairs equally in their training without considering the differences in their learning difficulties. Several image captioning... | Akiko Aizawa, Hongkuan Zhang, Koichi Takeda, Lei Zhou, Ryohei Sasano, Saku Sugawara |  |
| 1172 |  |  [Debiasing Masks: A New Framework for Shortcut Mitigation in NLU](https://doi.org/10.18653/v1/2022.emnlp-main.517) |  | 0 | Debiasing language models from unwanted behaviors in Natural Language Understanding (NLU) tasks is a topic with rapidly increasing interest in the NLP community. Spurious statistical correlations in the data allow models to perform shortcuts and avoid uncovering more advanced and desirable... | Akiko Aizawa, Johannes Mario Meissner, Saku Sugawara |  |
| 1173 |  |  [Extending Phrase Grounding with Pronouns in Visual Dialogues](https://doi.org/10.18653/v1/2022.emnlp-main.518) |  | 0 | Conventional phrase grounding aims to localize noun phrases mentioned in a given caption to their corresponding image regions, which has achieved great success recently. Apparently, sole noun phrase grounding is not enough for cross-modal visual language understanding. Here we extend the task by... | Meishan Zhang, Min Zhang, Panzhong Lu, Xin Zhang |  |
| 1174 |  |  [EUR-Lex-Sum: A Multi- and Cross-lingual Dataset for Long-form Summarization in the Legal Domain](https://doi.org/10.18653/v1/2022.emnlp-main.519) |  | 0 | Existing summarization datasets come with two main drawbacks: (1) They tend to focus on overly exposed domains, such as news articles or wiki-like texts, and (2) are primarily monolingual, with few multilingual datasets.In this work, we propose a novel dataset, called EUR-Lex-Sum, based on manually... | Ashish Chouhan, Dennis Aumiller, Michael Gertz |  |
| 1175 |  |  [Differentiable Data Augmentation for Contrastive Sentence Representation Learning](https://doi.org/10.18653/v1/2022.emnlp-main.520) |  | 0 | Fine-tuning a pre-trained language model via the contrastive learning framework with a large amount of unlabeled sentences or labeled sentence pairs is a common way to obtain high-quality sentence representations. Although the contrastive learning framework has shown its superiority on sentence... | Tianduo Wang, Wei Lu |  |
| 1176 |  |  [Text Style Transferring via Adversarial Masking and Styled Filling](https://doi.org/10.18653/v1/2022.emnlp-main.521) |  | 0 | Text style transfer is an important task in natural language processing with broad applications. Existing models following the masking and filling scheme suffer two challenges: the word masking procedure may mistakenly remove unexpected words and the selected words in the word filling procedure may... | Jaein Kim, Jiarui Wang, Junfan Chen, Richong Zhang, Yongyi Mao |  |
| 1177 |  |  [Character-level White-Box Adversarial Attacks against Transformers via Attachable Subwords Substitution](https://doi.org/10.18653/v1/2022.emnlp-main.522) |  | 0 | We propose the first character-level white-box adversarial attack method against transformer models. The intuition of our method comes from the observation that words are split into subtokens before being fed into the transformer models and the substitution between two close subtokens has a similar... | Aiwei Liu, Fukun Ma, Honghai Yu, Li Lin, Lijie Wen, Shu'ang Li, Xuming Hu, Yawen Yang |  |
| 1178 |  |  [Query-based Instance Discrimination Network for Relational Triple Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.523) |  | 0 | Joint entity and relation extraction has been a core task in the field of information extraction. Recent approaches usually consider the extraction of relational triples from a stereoscopic perspective, either learning a relation-specific tagger or separate classifiers for each relation type.... | Weiming Lu, Wenqi Zhang, Xiaoxia Cheng, Xuming Hu, Yongliang Shen, Yueting Zhuang, Zeqi Tan |  |
| 1179 |  |  [Learning Inter-Entity-Interaction for Few-Shot Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.emnlp-main.524) |  | 0 | Few-shot knowledge graph completion (FKGC) aims to infer unknown fact triples of a relation using its few-shot reference entity pairs. Recent FKGC studies focus on learning semantic representations of entity pairs by separately encoding the neighborhoods of head and tail entities. Such practice,... | Kui Yu, Xiaoling Huang, Yuhong Zhang, Yuling Li |  |
| 1180 |  |  [Empowering the Fact-checkers! Automatic Identification of Claim Spans on Twitter](https://doi.org/10.18653/v1/2022.emnlp-main.525) |  | 0 | The widespread diffusion of medical and political claims in the wake of COVID-19 has led to a voluminous rise in misinformation and fake news. The current vogue is to employ manual fact-checkers to efficiently classify and verify such data to combat this avalanche of claim-ridden misinformation.... | Atharva Kulkarni, Md. Shad Akhtar, Megha Sundriyal, Tanmoy Chakraborty, Vaibhav Pulastya |  |
| 1181 |  |  [ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.526) |  | 0 | We present ClidSum, a benchmark dataset towards building cross-lingual summarization systems on dialogue documents. It consists of 67k+ dialogue documents and 112k+ annotated summaries in different target languages. Based on the proposed ClidSum, we introduce two benchmark settings for supervised... | Duo Zheng, Fandong Meng, Jiaan Wang, Jianfeng Qu, Jie Zhou, Zhixu Li, Ziyao Lu |  |
| 1182 |  |  [Spectral Probing](https://doi.org/10.18653/v1/2022.emnlp-main.527) |  | 0 | Linguistic information is encoded at varying timescales (subwords, phrases, etc.) and communicative levels, such as syntax and semantics. Contextualized embeddings have analogously been found to capture these phenomena at distinctive layers and frequencies. Leveraging these findings, we develop a... | Barbara Plank, Max MüllerEberstein, Rob van der Goot |  |
| 1183 |  |  [QASem Parsing: Text-to-text Modeling of QA-based Semantics](https://doi.org/10.18653/v1/2022.emnlp-main.528) |  | 0 | Various works suggest the appeals of incorporating explicit semantic representations when addressing challenging realistic NLP scenarios. Common approaches offer either comprehensive linguistically-based formalisms, like AMR, or alternatively Open-IE, which provides a shallow and partial... | Avi Caciularu, Ayal Klein, Eran Hirsch, Ido Dagan, Ron Eliav, Valentina Pyatkin |  |
| 1184 |  |  [Keyphrase Generation via Soft and Hard Semantic Corrections](https://doi.org/10.18653/v1/2022.emnlp-main.529) |  | 0 | Keyphrase generation aims to generate a set of condensed phrases given a source document. Although maximum likelihood estimation (MLE) based keyphrase generation methods have shown impressive performance, they suffer from the bias on the source-prediction sequence pair and the bias on the... | Guangzhen Zhao, Guoshun Yin, Peng Yang, Yu Yao |  |
| 1185 |  |  [Modal-specific Pseudo Query Generation for Video Corpus Moment Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.530) |  | 0 | Video corpus moment retrieval (VCMR) is the task to retrieve the most relevant video moment from a large video corpus using a natural language query.For narrative videos, e.g., drama or movies, the holistic understanding of temporal dynamics and multimodal reasoning are crucial.Previous works have... | ByoungTak Zhang, JinHwa Kim, Joochan Kim, Minjoon Jung, Seongho Choi |  |
| 1186 |  |  [DuQM: A Chinese Dataset of Linguistically Perturbed Natural Questions for Evaluating the Robustness of Question Matching Models](https://doi.org/10.18653/v1/2022.emnlp-main.531) |  | 0 | In this paper, we focus on the robustness evaluation of Chinese Question Matching (QM) models. Most of the previous work on analyzing robustness issues focus on just one or a few types of artificial adversarial examples. Instead, we argue that a comprehensive evaluation should be conducted on... | Haifeng Wang, Hongyu Zhu, Hua Wu, Jing Liu, Jing Yan, Yan Chen, Ying Chen, Yu Hong |  |
| 1187 |  |  [DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages](https://doi.org/10.18653/v1/2022.emnlp-main.532) |  | 0 | We introduce DivEMT, the first publicly available post-editing study of Neural Machine Translation (NMT) over a typologically diverse set of target languages. Using a strictly controlled setup, 18 professional translators were instructed to translate or post-edit the same set of English documents... | Ana Guerberof Arenas, Antonio Toral, Arianna Bisazza, Gabriele Sarti |  |
| 1188 |  |  [Bridging Fairness and Environmental Sustainability in Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-main.533) |  | 0 | Fairness and environmental impact are important research directions for the sustainable development of artificial intelligence. However, while each topic is an active research area in natural language processing (NLP), there is a surprising lack of research on the interplay between the two fields.... | Anne Lauscher, Dirk Hovy, Emma Strubell, Marius Hessenthaler |  |
| 1189 |  |  [UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.534) |  | 0 | Multimodal sentiment analysis (MSA) and emotion recognition in conversation (ERC) are key research topics for computers to understand human behaviors. From a psychological perspective, emotions are the expression of affect or feelings during a short period, while sentiments are formed and held for... | Guangming Lu, Guimin Hu, TingEn Lin, Yi Zhao, Yongbin Li, Yuchuan Wu |  |
| 1190 |  |  [Is the Brain Mechanism for Hierarchical Structure Building Universal Across Languages? An fMRI Study of Chinese and English](https://doi.org/10.18653/v1/2022.emnlp-main.535) |  | 0 | Evidence from psycholinguistic studies suggests that the human brain builds a hierarchical syntactic structure during language comprehension. However, it is still unknown whether the neural basis of such structures is universal across languages. In this paper, we first analyze the differences in... | Chengqing Zong, Nan Lin, Shaonan Wang, Xiaohan Zhang |  |
| 1191 |  |  [HashFormers: Towards Vocabulary-independent Pre-trained Transformers](https://doi.org/10.18653/v1/2022.emnlp-main.536) |  | 0 | Transformer-based pre-trained language models are vocabulary-dependent, mapping by default each token to its corresponding embedding. This one-to-one mapping results into embedding matrices that occupy a lot of memory (i.e. millions of parameters) and grow linearly with the size of the vocabulary.... | Huiyin Xue, Nikolaos Aletras |  |
| 1192 |  |  [MatchPrompt: Prompt-based Open Relation Extraction with Semantic Consistency Guided Clustering](https://doi.org/10.18653/v1/2022.emnlp-main.537) |  | 0 | Relation clustering is a general approach for open relation extraction (OpenRE). Current methods have two major problems. One is that their good performance relies on large amounts of labeled and pre-defined relational instances for pre-training, which are costly to acquire in reality. The other is... | Jiaxin Wang, Jun Liu, Lingling Zhang, Xi Liang, Yaqiang Wu, Yujie Zhong |  |
| 1193 |  |  [Improving Aspect Sentiment Quad Prediction via Template-Order Data Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.538) |  | 0 | Recently, aspect sentiment quad prediction (ASQP) has become a popular task in the field of aspect-level sentiment analysis. Previous work utilizes a predefined template to paraphrase the original sentence into a structure target sequence, which can be easily decoded as quadruplets of the form... | Hang Gao, Mengting Hu, Shiwan Zhao, Yike Wu, Yinhao Bai |  |
| 1194 |  |  [SocioProbe: What, When, and Where Language Models Learn about Sociodemographics](https://doi.org/10.18653/v1/2022.emnlp-main.539) |  | 0 | Pre-trained language models (PLMs) have outperformed other NLP models on a wide range of tasks. Opting for a more thorough understanding of their capabilities and inner workings, researchers have established the extend to which they capture lower-level knowledge like grammaticality, and mid-level... | Anne Lauscher, Dirk Hovy, Federico Bianchi, Samuel R. Bowman |  |
| 1195 |  |  [When does Parameter-Efficient Transfer Learning Work for Machine Translation?](https://doi.org/10.18653/v1/2022.emnlp-main.540) |  | 0 | Parameter-efficient fine-tuning methods (PEFTs) offer the promise of adapting large pre-trained models while only tuning a small number of parameters. They have been shown to be competitive with full model fine-tuning for many downstream tasks. However, prior work indicates that PEFTs may not work... | Ahmet Üstün, Asa Cooper Stickland |  |
| 1196 |  |  [Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer](https://doi.org/10.18653/v1/2022.emnlp-main.541) |  | 0 | Massively multilingual models are promising for transfer learning across tasks and languages. However, existing methods are unable to fully leverage training data when it is available in different task-language combinations. To exploit such heterogeneous supervision, we propose Hyper-X, a single... | Ahmet Üstün, Arianna Bisazza, Gertjan van Noord, Gosse Bouma, Sebastian Ruder |  |
| 1197 |  |  [Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems](https://doi.org/10.18653/v1/2022.emnlp-main.542) |  | 0 | Numerical Question Answering is the task of answering questions that require numerical capabilities. Previous works introduce general adversarial attacks to Numerical Question Answering, while not systematically exploring numerical capabilities specific to the topic. In this paper, we propose to... | Dongmei Zhang, Jialiang Xu, Mengyu Zhou, Shi Han, Xinyi He |  |
| 1198 |  |  [Enhancing Joint Multiple Intent Detection and Slot Filling with Global Intent-Slot Co-occurrence](https://doi.org/10.18653/v1/2022.emnlp-main.543) |  | 0 | Multi-intent detection and slot filling joint model attracts more and more attention since it can handle multi-intent utterances, which is closer to complex real-world scenarios. Most existing joint models rely entirely on the training procedure to obtain the implicit correlation between intents... | Bowen Yu, Hongbo Xu, Mengxiao Song, Quangang Li, Tingwen Liu, Yubin Wang |  |
| 1199 |  |  [Towards Pragmatic Production Strategies for Natural Language Generation Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.544) |  | 0 | This position paper proposes a conceptual framework for the design of Natural Language Generation (NLG) systems that follow efficient and effective production strategies in order to achieve complex communicative goals. In this general framework, efficiency is characterised as the parsimonious... | Mario Giulianelli |  |
| 1200 |  |  [LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.545) |  | 0 | Recent large-scale video-language pre-trained models have shown appealing performance on various downstream tasks. However, the pre-training process is computationally expensive due to the requirement of millions of video-text pairs and the redundant data structure of each video. To mitigate these... | Chaofan Tao, Dongsheng Chen, Lifeng Shang, Lu Hou, Qun Liu, Xin Jiang |  |
| 1201 |  |  [Communication breakdown: On the low mutual intelligibility between human and neural captioning](https://doi.org/10.18653/v1/2022.emnlp-main.546) |  | 0 | We compare the 0-shot performance of a neural caption-based image retriever when given as input either human-produced captions or captions generated by a neural captioner. We conduct this comparison on the recently introduced ImageCoDe data-set (Krojer et al. 2022), which contains hard distractors... | Eleonora Gualdoni, Francesca Franzon, Gemma Boleda, Marco Baroni, Roberto Dessì |  |
| 1202 |  |  [Normalizing Mutual Information for Robust Adaptive Training for Translation](https://doi.org/10.18653/v1/2022.emnlp-main.547) |  | 0 | Despite the success of neural machine translation models, tensions between fluency of optimizing target language modeling and source-faithfulness remain as challenges. Previously, Conditional Bilingual Mutual Information (CBMI), a scoring metric for the importance of target sentences and tokens,... | Changmin Lee, Hojin Lee, Seungwon Hwang, Youngwon Lee |  |
| 1203 |  |  [Bilingual Synchronization: Restoring Translational Relationships with Editing Operations](https://doi.org/10.18653/v1/2022.emnlp-main.548) |  | 0 | Machine Translation (MT) is usually viewed as a one-shot process that generates the target language equivalent of some source text from scratch. We consider here a more general setting which assumes an initial target sequence, that must be transformed into a valid translation of the source, thereby... | François Yvon, Jitao Xu, Josep Maria Crego |  |
| 1204 |  |  [Human-Machine Collaboration Approaches to Build a Dialogue Dataset for Hate Speech Countering](https://doi.org/10.18653/v1/2022.emnlp-main.549) |  | 0 | Fighting online hate speech is a challenge that is usually addressed using Natural Language Processing via automatic detection and removal of hate content. Besides this approach, counter narratives have emerged as an effective tool employed by NGOs to respond to online hate on social media... | Helena Bonaldi, Marco Guerini, Sara Dellantonio, Serra Sinem Tekiroglu |  |
| 1205 |  |  [JANUS: Joint Autoregressive and Non-autoregressive Training with Auxiliary Loss for Sequence Generation](https://doi.org/10.18653/v1/2022.emnlp-main.550) |  | 0 | Transformer-based autoregressive and non-autoregressive models have played an essential role in sequence generation tasks. The autoregressive model can obtain excellent performance, while the non-autoregressive model brings fast decoding speed for inference. In this paper, we propose JANUS, a Joint... | Juntao Li, Lijun Wu, Min Zhang, Xiaobo Liang |  |
| 1206 |  |  [Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.551) |  | 0 | Most Outside-Knowledge Visual Question Answering (OK-VQA) systems employ a two-stage framework that first retrieves external knowledge given the visual question and then predicts the answer based on the retrieved content. However, the retrieved knowledge is often inadequate. Retrievals are... | Jialin Wu, Raymond J. Mooney |  |
| 1207 |  |  [Cross-Linguistic Syntactic Difference in Multilingual BERT: How Good is It and How Does It Affect Transfer?](https://doi.org/10.18653/v1/2022.emnlp-main.552) |  | 0 | Multilingual BERT (mBERT) has demonstrated considerable cross-lingual syntactic ability, whereby it enables effective zero-shot cross-lingual transfer of syntactic knowledge. The transfer is more successful between some languages, but it is not well understood what leads to this variation and... | Jingting Ye, Menghan Zhang, Ningyu Xu, Qi Zhang, Ruotian Ma, Tao Gui, Xuanjing Huang |  |
| 1208 |  |  ["It's Not Just Hate": A Multi-Dimensional Perspective on Detecting Harmful Speech Online](https://doi.org/10.18653/v1/2022.emnlp-main.553) |  | 0 | Well-annotated data is a prerequisite for good Natural Language Processing models. Too often, though, annotation decisions are governed by optimizing time or annotator agreement. We make a case for nuanced efforts in an interdisciplinary setting for annotating offensive online speech. Detecting... | Dirk Hovy, Federico Bianchi, Nava Tintarev, Patrícia G. C. Rossini, Rebekah Tromble, Stefanie Anja Hills |  |
| 1209 |  |  [Long Text Generation with Topic-aware Discrete Latent Variable Model](https://doi.org/10.18653/v1/2022.emnlp-main.554) |  | 0 | Generating coherent long texts is an important yet challenging task, particularly forthe open-ended generation. Prior work based on discrete latent codes focuses on the modeling of discourse relation, resulting in discrete codes only learning shallow semantics (Ji and Huang, 2021). A natural text... | Deyi Xiong, Erguang Yang, Jinan Xu, Mingtong Liu, Yufeng Chen, Yujie Zhang |  |
| 1210 |  |  [TIARA: Multi-grained Retrieval for Robust Question Answering over Large Knowledge Base](https://doi.org/10.18653/v1/2022.emnlp-main.555) |  | 0 | Pre-trained language models (PLMs) have shown their effectiveness in multiple scenarios. However, KBQA remains challenging, especially regarding coverage and generalization settings. This is due to two main factors: i) understanding the semantics of both questions and relevant knowledge from the... | Börje F. Karlsson, ChinYew Lin, Tingting Ma, Yiheng Shu, Yuhan Li, Yuzhong Qu, Zhiwei Yu |  |
| 1211 |  |  [Structure-Unified M-Tree Coding Solver for Math Word Problem](https://doi.org/10.18653/v1/2022.emnlp-main.556) |  | 0 | As one of the challenging NLP tasks, designing math word problem (MWP) solvers has attracted increasing research attention for the past few years. In previous work, models designed by taking into account the properties of the binary tree structure of mathematical expressions at the output side have... | Bin Wang, Jiajun Chen, Jiangzhou Ju, Shujian Huang, Xinyu Dai, Yang Fan |  |
| 1212 |  |  [FormLM: Recommending Creation Ideas for Online Forms by Modelling Semantic and Structural Information](https://doi.org/10.18653/v1/2022.emnlp-main.557) |  | 0 | Online forms are widely used to collect data from human and have a multi-billion market. Many software products provide online services for creating semi-structured forms where questions and descriptions are organized by predefined structures. However, the design and creation process of forms is... | Dongmei Zhang, Gideon Huang, Hongwei Han, Mengyu Zhou, Shi Han, Tao Wu, Yifan Zhong, Yijia Shao |  |
| 1213 |  |  [Generate, Discriminate and Contrast: A Semi-Supervised Sentence Representation Learning Framework](https://doi.org/10.18653/v1/2022.emnlp-main.558) |  | 0 | Most sentence embedding techniques heavily rely on expensive human-annotated sentence pairs as the supervised signals. Despite the use of large-scale unlabeled data, the performance of unsupervised methods typically lags far behind that of the supervised counterparts in most downstream tasks. In... | Bin Wang, Haizhou Li, Yan Zhang, Yiming Chen, Zuozhu Liu |  |
| 1214 |  |  [GPS: Genetic Prompt Search for Efficient Few-Shot Learning](https://doi.org/10.18653/v1/2022.emnlp-main.559) |  | 0 | Prompt-based techniques have demostrated great potential for improving the few-shot generalization of pretrained language models. However, their performance heavily relies on the manual design of prompts and thus requiring a lot of human efforts. In this paper, we introduce Genetic Prompt Search... | Haiyu Li, Hanwei Xu, Nan Shao, Yanggang Wang, Yujun Chen, Yulun Du, Zhilin Yang |  |
| 1215 |  |  [Multitask Instruction-based Prompting for Fallacy Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.560) |  | 0 | Fallacies are used as seemingly valid arguments to support a position and persuade the audience about its validity. Recognizing fallacies is an intrinsically difficult task both for humans and machines. Moreover, a big challenge for computational models lies in the fact that fallacies are... | Elena Musi, Smaranda Muresan, Tariq Alhindi, Tuhin Chakrabarty |  |
| 1216 |  |  [Rethinking Multi-Modal Alignment in Multi-Choice VideoQA from Feature and Sample Perspectives](https://doi.org/10.18653/v1/2022.emnlp-main.561) |  | 0 | Reasoning about causal and temporal event relations in videos is a new destination of Video Question Answering (VideoQA). The major stumbling block to achieve this purpose is the semantic gap between language and video since they are at different levels of abstraction. Existing efforts mainly focus... | Jun Xiao, Kaifeng Gao, Long Chen, Shaoning Xiao, Yi Yang, Zhao Wang, Zhimeng Zhang |  |
| 1217 |  |  [Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach](https://doi.org/10.18653/v1/2022.emnlp-main.562) |  | 0 | Although remarkable progress on the neural table-to-text methods has been made, the generalization issues hinder the applicability of these models due to the limited source tables. Large-scale pretrained language models sound like a promising solution to tackle such issues. However, how to... | Dejing Dou, Hui Xiong, Jingbo Zhou, Miao Chen, Tong Xu, Xinjiang Lu, Yanyan Li |  |
| 1218 |  |  [Hierarchical Phrase-Based Sequence-to-Sequence Learning](https://doi.org/10.18653/v1/2022.emnlp-main.563) |  | 0 | This paper describes a neural transducer that maintains the flexibility of standard sequence-to-sequence (seq2seq) models while incorporating hierarchical phrases as a source of inductive bias during training and as explicit constraints during inference. Our approach trains two models: a... | Bailin Wang, Ivan Titov, Jacob Andreas, Yoon Kim |  |
| 1219 |  |  [Natural Language Deduction with Incomplete Information](https://doi.org/10.18653/v1/2022.emnlp-main.564) |  | 0 | A growing body of work studies how to answer a question or verify a claim by generating a natural language “proof:” a chain of deductive inferences yielding the answer based on a set of premises. However, these methods can only make sound deductions when they follow from evidence that is given. We... | Greg Durrett, Kaj Bostrom, Swarat Chaudhuri, Zayne Sprague |  |
| 1220 |  |  [Character-centric Story Visualization via Visual Planning and Token Alignment](https://doi.org/10.18653/v1/2022.emnlp-main.565) |  | 0 | Story visualization advances the traditional text-to-image generation by enabling multiple image generation based on a complete story. This task requires machines to 1) understand long text inputs, and 2) produce a globally consistent image sequence that illustrates the contents of the story. A key... | Hideki Nakayama, Hong Chen, Nanyun Peng, Rujun Han, TeLin Wu |  |
| 1221 |  |  [ASQA: Factoid Questions Meet Long-Form Answers](https://doi.org/10.18653/v1/2022.emnlp-main.566) |  | 0 | Recent progress on open domain factoid question answering (QA) does not easily transfer to the task of long-form QA, where the goal is to answer questions that require in-depth explanations. The hurdles include a lack of high-quality data and the absence of a well-defined notion of an answer’s... | Bhuwan Dhingra, Ivan Stelmakh, MingWei Chang, Yi Luan |  |
| 1222 |  |  [Algorithms for Acyclic Weighted Finite-State Automata with Failure Arcs](https://doi.org/10.18653/v1/2022.emnlp-main.567) |  | 0 | Weighted finite-state automata (WSFAs) arecommonly used in NLP. Failure transitions area useful extension for compactly representingbackoffs or interpolation in n-gram modelsand CRFs, which are special cases of WFSAs.Unfortunately, applying standard algorithmsfor computing the pathsum requires... | Anej Svete, Benjamin Dayan, Jason Eisner, Ryan Cotterell, Tim Vieira |  |
| 1223 |  |  [Towards Better Document-level Relation Extraction via Iterative Inference](https://doi.org/10.18653/v1/2022.emnlp-main.568) |  | 0 | Document-level relation extraction (RE) aims to extract the relations between entities from the input document that usually containing many difficultly-predicted entity pairs whose relations can only be predicted through relational inference. Existing methods usually directly predict the relations... | Jinsong Su, Liang Zhang, Qingguo Hu, Xiaodong Shi, Yidong Chen, Zhongjian Miao, Zijun Min |  |
| 1224 |  |  [Efficient Adversarial Training with Robust Early-Bird Tickets](https://doi.org/10.18653/v1/2022.emnlp-main.569) |  | 0 | Adversarial training is one of the most powerful methods to improve the robustness of pre-trained language models (PLMs). However, this approach is typically more expensive than traditional fine-tuning because of the necessity to generate adversarial examples via gradient descent. Delving into the... | Qi Zhang, Rui Zheng, Tao Gui, Xuanjing Huang, Zhiheng Xi |  |
| 1225 |  |  [Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks](https://doi.org/10.18653/v1/2022.emnlp-main.570) |  | 0 | The wide adoption and application of Masked language models (MLMs) on sensitive data (from legal to medical) necessitates a thorough quantitative investigation into their privacy vulnerabilities. Prior attempts at measuring leakage of MLMs via membership inference attacks have been inconclusive,... | Archit Uniyal, Fatemehsadat Mireshghallah, Kartik Goyal, Reza Shokri, Taylor BergKirkpatrick |  |
| 1226 |  |  [SMaLL-100: Introducing Shallow Multilingual Machine Translation Model for Low-Resource Languages](https://doi.org/10.18653/v1/2022.emnlp-main.571) |  | 0 | In recent years, multilingual machine translation models have achieved promising performance on low-resource language pairs by sharing information between similar languages, thus enabling zero-shot translation. To overcome the “curse of multilinguality”, these models often opt for scaling up the... | Alexandre Berard, Alireza Mohammadshahi, Caroline Brun, James Henderson, Laurent Besacier, Vassilina Nikoulina |  |
| 1227 |  |  [TextFusion: Privacy-Preserving Pre-trained Model Inference via Token Fusion](https://doi.org/10.18653/v1/2022.emnlp-main.572) |  | 0 | Recently, more and more pre-trained language models are released as a cloud service. It allows users who lack computing resources to perform inference with a powerful model by uploading data to the cloud. The plain text may contain private information, as the result, users prefer to do partial... | Jinzhu Lu, Qi Zhang, Ruotian Ma, Tao Gui, Xin Zhou, Xuanjing Huang, Yibo Cheung, Yong Ding, Yuran Wang, Zichu Fei |  |
| 1228 |  |  [Learning to Explain Selectively: A Case Study on Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.573) |  | 0 | Explanations promise to bridge the gap between humans and AI, yet it remains difficult to achieve consistent improvement in AI-augmented human decision making. The usefulness of AI explanations depends on many factors, and always showing the same type of explanation in all cases is suboptimal—so is... | Jordan L. BoydGraber, Shi Feng |  |
| 1229 |  |  [ConsistTL: Modeling Consistency in Transfer Learning for Low-Resource Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.574) |  | 0 | Transfer learning is a simple and powerful method that can be used to boost model performance of low-resource neural machine translation (NMT). Existing transfer learning methods for NMT are static, which simply transfer knowledge from a parent model to a child model once via parameter... | Derek F. Wong, Lidia S. Chao, Min Zhang, Xuebo Liu, Zhaocong Li |  |
| 1230 |  |  [Better Hit the Nail on the Head than Beat around the Bush: Removing Protected Attributes with a Single Projection](https://doi.org/10.18653/v1/2022.emnlp-main.575) |  | 0 | Bias elimination and recent probing studies attempt to remove specific information from embedding spaces. Here it is important to remove as much of the target information as possible, while preserving any other information present. INLP is a popular recent method which removes specific information... | Antske Fokkens, Bettina Speckmann, Kevin Verbeek, Pantea Haghighatkhah, Pia Sommerauer |  |
| 1231 |  |  [IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.576) |  | 0 | We introduce a new open information extraction (OIE) benchmark for pre-trained language models (LM). Recent studies have demonstrated that pre-trained LMs, such as BERT and GPT, may store linguistic and relational knowledge. In particular, LMs are able to answer “fill-in-the-blank” questions when... | Chenguang Wang, Dawn Song, Xiao Liu |  |
| 1232 |  |  [ConNER: Consistency Training for Cross-lingual Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.577) |  | 0 | Cross-lingual named entity recognition (NER) suffers from data scarcity in the target languages, especially under zero-shot settings. Existing translate-train or knowledge distillation methods attempt to bridge the language gap, but often introduce a high level of noise. To solve this problem,... | Chunyan Miao, Erik Cambria, Lidong Bing, Luo Si, Ran Zhou, Xin Li |  |
| 1233 |  |  [A Sequential Flow Control Framework for Multi-hop Knowledge Base Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.578) |  | 0 | One of the key challenges of knowledge base question answering (KBQA) is the multi-hop reasoning. Since in different hops, one attends to different parts of question, it is important to dynamically represent the question semantics for each hop. Existing methods, however, (i) infer the dynamic... | Chuzhan Hao, Minghui Xie, Peng Zhang |  |
| 1234 |  |  [ACENet: Attention Guided Commonsense Reasoning on Hybrid Knowledge Graph](https://doi.org/10.18653/v1/2022.emnlp-main.579) |  | 0 | Augmenting pre-trained language models (PLMs) with knowledge graphs (KGs) has demonstrated superior performance on commonsense reasoning. Given a commonsense based QA context (question and multiple choices), existing approaches usually estimate the plausibility of candidate choices separately based... | Chuzhan Hao, Minghui Xie, Peng Zhang |  |
| 1235 |  |  [Revisiting DocRED - Addressing the False Negative Problem in Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.580) |  | 0 | The DocRED dataset is one of the most popular and widely used benchmarks for document-level relation extraction (RE). It adopts a recommend-revise annotation scheme so as to have a large-scale annotated dataset. However, we find that the annotation of DocRED is incomplete, i.e., false negative... | Hwee Tou Ng, Lidong Bing, Lu Xu, Qingyu Tan, Sharifah Mahani Aljunied |  |
| 1236 |  |  [Towards Summary Candidates Fusion](https://doi.org/10.18653/v1/2022.emnlp-main.581) |  | 0 | Sequence-to-sequence deep neural models fine-tuned for abstractive summarization can achieve great performance on datasets with enough human annotations. Yet, it has been shown that they have not reached their full potential, with a wide gap between the top beam search output and the oracle beam.... | Mathieu Ravaut, Nancy F. Chen, Shafiq R. Joty |  |
| 1237 |  |  [Multimodal Robustness for Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.582) |  | 0 | In this paper, we look at the case of a Generic text-to-text NMT model that has to deal with data coming from various modalities, like speech, images, or noisy text extracted from the web. We propose a two-step method, based on composable adapters, to deal with this problem of Multimodal... | Ioan Calapodescu, Yuting Zhao |  |
| 1238 |  |  [TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction](https://doi.org/10.18653/v1/2022.emnlp-main.583) |  | 0 | Knowledge graph embedding methods are important for the knowledge graph completion (or link prediction) task.One state-of-the-art method, PairRE, leverages two separate vectors to model complex relations (i.e., 1-to-N, N-to-1, and N-to-N) in knowledge graphs. However, such a method strictly... | Chao Liu, Chenghua Lin, Jiang Qian, Wei Fan, Yizhi Li |  |
| 1239 |  |  [IRRGN: An Implicit Relational Reasoning Graph Network for Multi-turn Response Selection](https://doi.org/10.18653/v1/2022.emnlp-main.584) |  | 0 | The task of response selection in multi-turn dialogue is to find the best option from all candidates. In order to improve the reasoning ability of the model, previous studies pay more attention to using explicit algorithms to model the dependencies between utterances, which are deterministic,... | Hengwei Dai, Jingcheng Deng, Wei Peng, Xuewei Guo, Yuanchen Ju |  |
| 1240 |  |  [Predicting Prerequisite Relations for Unseen Concepts](https://doi.org/10.18653/v1/2022.emnlp-main.585) |  | 0 | Concept prerequisite learning (CPL) plays a key role in developing technologies that assist people to learn a new complex topic or concept. Previous work commonly assumes that all concepts are given at training time and solely focuses on predicting the unseen prerequisite relationships between... | Hamed Zamani, Yaxin Zhu |  |
| 1241 |  |  [Contrastive Learning with Expectation-Maximization for Weakly Supervised Phrase Grounding](https://doi.org/10.18653/v1/2022.emnlp-main.586) |  | 0 | Weakly supervised phrase grounding aims to learn an alignment between phrases in a caption and objects in a corresponding image using only caption-image annotations, i.e., without phrase-object annotations. Previous methods typically use a caption-image contrastive loss to indirectly supervise the... | Keqin Chen, Richong Zhang, Samuel Mensah, Yongyi Mao |  |
| 1242 |  |  [Beyond prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations](https://doi.org/10.18653/v1/2022.emnlp-main.587) |  | 0 | Recent work has demonstrated that pre-trained language models (PLMs) are zero-shot learners. However, most existing zero-shot methods involve heavy human engineering or complicated self-training pipelines, hindering their application to new situations. In this work, we show that zero-shot text... | Mrinmaya Sachan, Ping Nie, Roger Wattenhofer, Yu Fei, Zhao Meng |  |
| 1243 |  |  [Generalizing over Long Tail Concepts for Medical Term Normalization](https://doi.org/10.18653/v1/2022.emnlp-main.588) |  | 0 | Medical term normalization consists in mapping a piece of text to a large number of output classes.Given the small size of the annotated datasets and the extremely long tail distribution of the concepts, it is of utmost importance to develop models that are capable to generalize to scarce or unseen... | Beatrice Portelli, Emmanuele Chersoni, Enrico Santus, Giuseppe Serra, Hooman Sedghamiz, Simone Scaboro |  |
| 1244 |  |  [Unsupervised Opinion Summarisation in the Wasserstein Space](https://doi.org/10.18653/v1/2022.emnlp-main.589) |  | 0 | Opinion summarisation synthesises opinions expressed in a group of documents discussingthe same topic to produce a single summary. Recent work has looked at opinion summarisation of clusters of social media posts. Such posts are noisy and have unpredictable structure, posing additional challenges... | Adam Tsakalidis, Iman Munire Bilal, Jiayu Song, Maria Liakata, Rob Procter |  |
| 1245 |  |  [Bloom Library: Multimodal Datasets in 300+ Languages for a Variety of Downstream Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.590) |  | 0 | We present Bloom Library, a linguistically diverse set of multimodal and multilingual datasets for language modeling, image captioning, visual storytelling, and speech synthesis/recognition. These datasets represent either the most, or among the most, multilingual datasets for each of the included... | Abraham Owodunni, Anna Filighera, Colin Leong, Daniel Whitenack, Jacob Mansdorfer, Joshua Nemecek |  |
| 1246 |  |  [Disentangling Uncertainty in Machine Translation Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.591) |  | 0 | Trainable evaluation metrics for machine translation (MT) exhibit strong correlation with human judgements, but they are often hard to interpret and might produce unreliable scores under noisy or out-of-domain data. Recent work has attempted to mitigate this with simple uncertainty quantification... | André F. T. Martins, Chrysoula Zerva, Ricardo Rei, Taisiya Glushkova |  |
| 1247 |  |  [Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing](https://doi.org/10.18653/v1/2022.emnlp-main.592) |  | 0 | Entity typing aims at predicting one or more words that describe the type(s) of a specific mention in a sentence. Due to shortcuts from surface patterns to annotated entity labels and biased training, existing entity typing models are subject to the problem of spurious correlations. To... | Bangzheng Li, Fei Wang, Mingtao Dong, Muhao Chen, Nan Xu |  |
| 1248 |  |  [EDIN: An End-to-end Benchmark and Pipeline for Unknown Entity Discovery and Indexing](https://doi.org/10.18653/v1/2022.emnlp-main.593) |  | 0 | Existing work on Entity Linking mostly assumes that the reference knowledge base is complete, and therefore all mentions can be linked. In practice this is hardly ever the case, as knowledge bases are incomplete and because novel concepts arise constantly. We introduce the temporally segmented... | Fabio Petroni, Mikhail Plekhanov, Nicola Cancedda, Nora Kassner, Sebastian Riedel |  |
| 1249 |  |  [POQue: Asking Participant-specific Outcome Questions for a Deeper Understanding of Complex Events](https://doi.org/10.18653/v1/2022.emnlp-main.594) |  | 0 | Knowledge about outcomes is critical for complex event understanding but is hard to acquire.We show that by pre-identifying a participant in a complex event, crowdworkers are ableto (1) infer the collective impact of salient events that make up the situation, (2) annotate the volitional engagement... | Francis Ferraro, Katrin Erk, Niranjan Balasubramanian, Sai Vallurupalli, Sayontan Ghosh |  |
| 1250 |  |  [Measuring the Mixing of Contextual Information in the Transformer](https://doi.org/10.18653/v1/2022.emnlp-main.595) |  | 0 | The Transformer architecture aggregates input information through the self-attention mechanism, but there is no clear understanding of how this information is mixed across the entire model. Additionally, recent works have demonstrated that attention weights alone are not enough to describe the flow... | Gerard I. Gállego, Javier Ferrando, Marta R. Costajussà |  |
| 1251 |  |  [Dealing with Abbreviations in the Slovenian Biographical Lexicon](https://doi.org/10.18653/v1/2022.emnlp-main.596) |  | 0 | Abbreviations present a significant challenge for NLP systems because they cause tokenization and out-of-vocabulary errors. They can also make the text less readable, especially in reference printed books, where they are extensively used. Abbreviations are especially problematic in low-resource... | Angel Daza, Antske Fokkens, Tomaz Erjavec |  |
| 1252 |  |  [AfriCLIRMatrix: Enabling Cross-Lingual Information Retrieval for African Languages](https://doi.org/10.18653/v1/2022.emnlp-main.597) |  | 0 | Language diversity in NLP is critical in enabling the development of tools for a wide range of users.However, there are limited resources for building such tools for many languages, particularly those spoken in Africa.For search, most existing datasets feature few or no African languages, directly... | Jimmy Lin, Kevin Duh, Odunayo Ogundepo, Shuo Sun, Xinyu Zhang |  |
| 1253 |  |  [CONDAQA: A Contrastive Reading Comprehension Dataset for Reasoning about Negation](https://doi.org/10.18653/v1/2022.emnlp-main.598) |  | 0 | The full power of human language-based communication cannot be realized without negation. All human languages have some form of negation. Despite this, negation remains a challenging phenomenon for current natural language understanding systems. To facilitate the future development of models that... | Abhilasha Ravichander, Ana Marasovic, Matt Gardner |  |
| 1254 |  |  [Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer](https://doi.org/10.18653/v1/2022.emnlp-main.599) |  | 0 | In Neural Machine Translation (NMT), each token prediction is conditioned on the source sentence and the target prefix (what has been previously translated at a decoding step). However, previous work on interpretability in NMT has mainly focused solely on source sentence tokens’ attributions.... | Belen Alastruey, Carlos Escolano, Gerard I. Gállego, Javier Ferrando, Marta R. Costajussà |  |
| 1255 |  |  [ArtELingo: A Million Emotion Annotations of WikiArt with Emphasis on Diversity over Language and Culture](https://doi.org/10.18653/v1/2022.emnlp-main.600) |  | 0 | This paper introduces ArtELingo, a new benchmark and dataset, designed to encourage work on diversity across languages and cultures. Following ArtEmis, a collection of 80k artworks from WikiArt with 0.45M emotion labels and English-only captions, ArtELingo adds another 0.79M annotations in Arabic... | Feifan Li, Kenneth Church, Mohamed Abdelfattah, Mohamed Elhoseiny, Shyma Alhuwaider, Xiangliang Zhang, Youssef Mohamed |  |
| 1256 |  |  [Decoding a Neural Retriever's Latent Space for Query Suggestion](https://doi.org/10.18653/v1/2022.emnlp-main.601) |  | 0 | Neural retrieval models have superseded classic bag-of-words methods such as BM25 as the retrieval framework of choice. However, neural systems lack the interpretability of bag-of-words models; it is not trivial to connect a query change to a change in the latent space that ultimately determines... | Christian Buck, Leonard Adolphs, Massimiliano Ciaramita, Michelle Chen Huebscher, Olivier Bachem, Sertan Girgin, Thomas Hofmann |  |
| 1257 |  |  [T-STAR: Truthful Style Transfer using AMR Graph as Intermediate Representation](https://doi.org/10.18653/v1/2022.emnlp-main.602) |  | 0 | Unavailability of parallel corpora for training text style transfer (TST) models is a very challenging yet common scenario. Also, TST models implicitly need to preserve the content while transforming a source sentence into the target style. To tackle these problems, an intermediate representation... | Anubhav Jangra, Aravindan Raghuveer, Preksha Nema |  |
| 1258 |  |  [PromptBERT: Improving BERT Sentence Embeddings with Prompts](https://doi.org/10.18653/v1/2022.emnlp-main.603) |  | 0 | We propose PromptBERT, a novel contrastive learning method for learning better sentence representation. We firstly analysis the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers. Then we propose... | Denvy Deng, Deqing Wang, Furu Wei, Fuzhen Zhuang, Haizhen Huang, Jian Jiao, Qi Zhang, Shaohan Huang, Ting Jiang, Zihan Zhang |  |
| 1259 |  |  [Extending Logic Explained Networks to Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.604) |  | 0 | Recently, Logic Explained Networks (LENs) have been proposed as explainable-by-design neural models providing logic explanations for their predictions.However, these models have only been applied to vision and tabular data, and they mostly favour the generation of global explanations, while local... | Davide Buffelli, Francesco Giannini, Gabriele Ciravegna, Pietro Barbiero, Pietro Liò, Rishabh Jain |  |
| 1260 |  |  [Uni-Parser: Unified Semantic Parser for Question Answering on Knowledge Base and Database](https://doi.org/10.18653/v1/2022.emnlp-main.605) |  | 0 | Parsing natural language questions into executable logical forms is a useful and interpretable way to perform question answering on structured data such as knowledge bases (KB) or databases (DB). However, existing approaches on semantic parsing cannot adapt to both modalities, as they suffer from... | Caiming Xiong, Dragomir Radev, Rui Meng, Semih Yavuz, Ye Liu, Yingbo Zhou |  |
| 1261 |  |  [RAPO: An Adaptive Ranking Paradigm for Bilingual Lexicon Induction](https://doi.org/10.18653/v1/2022.emnlp-main.606) |  | 0 | Bilingual lexicon induction induces the word translations by aligning independently trained word embeddings in two languages. Existing approaches generally focus on minimizing the distances between words in the aligned pairs, while suffering from low discriminative capability to distinguish the... | Chaozhuo Li, Denvy Deng, Haizhen Huang, Qi Zhang, Shuo Ren, Xiao Han, Xing Xie, Xinyue Hu, Zengxuan Wen, Zhiqiang Zuo, Zhoujin Tian |  |
| 1262 |  |  [On Parsing as Tagging](https://doi.org/10.18653/v1/2022.emnlp-main.607) |  | 0 | There are many proposals to reduce constituency parsing to tagging. To figure out what these approaches have in common, we offer a unifying pipeline, which consists of three steps: linearization, learning, and decoding. We prove that classic shift–reduce parsing can be reduced to tetratagging—the... | Afra Amini, Ryan Cotterell |  |
| 1263 |  |  [Distilled Dual-Encoder Model for Vision-Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.608) |  | 0 | On vision-language understanding (VLU) tasks, fusion-encoder vision-language models achieve superior results but sacrifice efficiency because of the simultaneous encoding of images and text. On the contrary, the dual encoder model that separately encodes images and text has the advantage in... | Bing Qin, Furu Wei, Haichao Zhu, Ming Liu, Wenhui Wang, Zekun Wang |  |
| 1264 |  |  [Argument Mining for Review Helpfulness Prediction](https://doi.org/10.18653/v1/2022.emnlp-main.609) |  | 0 | The importance of reliably determining the helpfulness of product reviews is rising as both helpful and unhelpful reviews continue to accumulate on e-commerce websites. And argumentational features—such as the structure of arguments and the types of underlying elementary units—have shown to be... | Daniel Verdi do Amarante, Jenna Donaldson, Joonsuk Park, Yohan Jo, Zaiqian Chen |  |
| 1265 |  |  [Hierarchical Multi-Label Classification of Scientific Documents](https://doi.org/10.18653/v1/2022.emnlp-main.610) |  | 0 | Automatic topic classification has been studied extensively to assist managing and indexing scientific documents in a digital collection. With the large number of topics being available in recent years, it has become necessary to arrange them in a hierarchy. Therefore, the automatic classification... | Cornelia Caragea, Mobashir Sadat |  |
| 1266 |  |  [Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.611) |  | 0 | Knowledge underpins reasoning. Recent research demonstrates that when relevant knowledge is provided as additional context to commonsense question answering (QA), it can substantially enhance the performance even on top of state-of-the-art. The fundamental challenge is where and how to find such... | Hannaneh Hajishirzi, Jiacheng Liu, Pengfei He, Sean Welleck, Skyler Hallinan, Ximing Lu, Yejin Choi |  |
| 1267 |  |  [A Major Obstacle for NLP Research: Let's Talk about Time Allocation!](https://doi.org/10.18653/v1/2022.emnlp-main.612) |  | 0 | The field of natural language processing (NLP) has grown over the last few years: conferences have become larger, we have published an incredible amount of papers, and state-of-the-art research has been implemented in a large variety of customer-facing products. However, this paper argues that we... | Arya D. McCarthy, Katharina Kann, Shiran Dudy |  |
| 1268 |  |  [Towards Inter-character Relationship-driven Story Generation](https://doi.org/10.18653/v1/2022.emnlp-main.613) |  | 0 | In this paper, we introduce the task of modeling interpersonal relationships for story generation. For addressing this task, we propose Relationships as Latent Variables for Story Generation, (ReLiSt). ReLiSt generates stories sentence by sentence and has two major components - a relationship... | Anvesh Rao Vijjini, Faeze Brahman, Snigdha Chaturvedi |  |
| 1269 |  |  [Incorporating Relevance Feedback for Information-Seeking Retrieval using Few-Shot Document Re-Ranking](https://doi.org/10.18653/v1/2022.emnlp-main.614) |  | 0 | Pairing a lexical retriever with a neural re-ranking model has set state-of-the-art performance on large-scale information retrieval datasets. This pipeline covers scenarios like question answering or navigational queries, however, for information-seeking scenarios, users often provide information... | Iryna Gurevych, Leonardo F. R. Ribeiro, Nils Reimers, Tim Baumgärtner |  |
| 1270 |  |  [ReasTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples](https://doi.org/10.18653/v1/2022.emnlp-main.615) |  | 0 | Reasoning over tabular data requires both table structure understanding and a broad set of table reasoning skills. Current models with table-specific architectures and pre-training methods perform well on understanding table structures, but they still struggle with tasks that require various table... | Dragomir Radev, Linyong Nan, Rui Zhang, Yilun Zhao, Zhenting Qi |  |
| 1271 |  |  [Few-shot Learning with Multilingual Generative Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.616) |  | 0 | Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train... | Brian O'Horo, Daniel Simig, Jeff Wang, Jingfei Du, Luke Zettlemoyer, Mikel Artetxe, Mona T. Diab, Myle Ott, Naman Goyal, Punit Singh Koura, Ramakanth Pasunuru, Sam Shleifer, Shruti Bhosale, Shuohui Chen, Tianlu Wang, Todor Mihaylov, Veselin Stoyanov, Vishrav Chaudhary, Xi Victoria Lin, Xian Li, Zornitsa Kozareva |  |
| 1272 |  |  [Are representations built from the ground up? An empirical examination of local composition in language models](https://doi.org/10.18653/v1/2022.emnlp-main.617) |  | 0 | Compositionality, the phenomenon where the meaning of a phrase can be derived from its constituent parts, is a hallmark of human language. At the same time, many phrases are non-compositional, carrying a meaning beyond that of each part in isolation. Representing both of these types of phrases is... | Emmy Liu, Graham Neubig |  |
| 1273 |  |  [Detecting Label Errors by Using Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.618) |  | 0 | We show that large pre-trained language models are inherently highly capable of identifying label errors in natural language datasets: simply examining out-of-sample data points in descending order of fine-tuned task loss significantly outperforms more complex error-detection mechanisms proposed in... | Christopher D. Manning, Derek Chong, Jenny Hong |  |
| 1274 |  |  [Intriguing Properties of Compression on Multilingual Models](https://doi.org/10.18653/v1/2022.emnlp-main.619) |  | 0 | Multilingual models are often particularly dependent on scaling to generalize to a growing number of languages. Compression techniques are widely relied upon to reconcile the growth in model size with real world resource constraints, but compression can have a disparate effect on model performance... | Gbemileke Onilude, Julia Kreutzer, Kelechi Ogueji, Orevaoghene Ahia, Sara Hooker, Sebastian Gehrmann |  |
| 1275 |  |  [Sequence Models for Document Structure Identification in an Undeciphered Script](https://doi.org/10.18653/v1/2022.emnlp-main.620) |  | 0 | This work describes the first thorough analysis of “header” signs in proto-Elamite, an undeciphered script from 3100-2900 BCE. Headers are a category of signs which have been provisionally identified through painstaking manual analysis of this script by domain experts. We use unsupervised neural... | Anoop Sarkar, Kathryn Kelley, Logan Born, M. Willis Monroe |  |
| 1276 |  |  [English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.621) |  | 0 | Universal cross-lingual sentence embeddings map semantically similar cross-lingual sentences into a shared embedding space. Aligning cross-lingual sentence embeddings usually requires supervised cross-lingual parallel sentences. In this work, we propose mSimCSE, which extends SimCSE to multilingual... | Ashley Wu, Graham Neubig, YauShian Wang |  |
| 1277 |  |  [Active Example Selection for In-Context Learning](https://doi.org/10.18653/v1/2022.emnlp-main.622) |  | 0 | With a handful of demonstration examples, large-scale language models demonstrate strong capability to perform various tasks by in-context learning from these examples, without any fine-tuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples,... | Chenhao Tan, Shi Feng, Yiming Zhang |  |
| 1278 |  |  [Improving Factual Consistency in Summarization with Compression-Based Post-Editing](https://doi.org/10.18653/v1/2022.emnlp-main.623) |  | 0 | State-of-the-art summarization models still struggle to be factually consistent with the input text. A model-agnostic way to address this problem is post-editing the generated summaries. However, existing approaches typically fail to remove entity errors if a suitable input entity replacement is... | Alexander R. Fabbri, Caiming Xiong, ChienSheng Wu, Jesse Vig, Prafulla Kumar Choubey |  |
| 1279 |  |  [Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.624) |  | 0 | Despite their strong performance on many tasks, pre-trained language models have been shown to struggle on out-of-distribution compositional generalization. Meanwhile, recent work has shown considerable improvements on many NLP tasks from model scaling. Can scaling up model size also improve... | Emily Pitler, Fei Sha, Jonathan Herzig, Kristina Toutanova, Linlu Qiu, Panupong Pasupat, Peter Shaw, Tianze Shi |  |
| 1280 |  |  ["I'm sorry to hear that": Finding New Biases in Language Models with a Holistic Descriptor Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.625) |  | 0 | As language models grow in popularity, it becomes increasingly important to clearly measure all possible markers of demographic identity in order to avoid perpetuating existing societal harms. Many datasets for measuring bias currently exist, but they are restricted in their coverage of demographic... | Adina Williams, Eleonora Presani, Eric Michael Smith, Melanie Kambadur, Melissa Hall |  |
| 1281 |  |  [Understanding ME? Multimodal Evaluation for Fine-grained Visual Commonsense](https://doi.org/10.18653/v1/2022.emnlp-main.626) |  | 0 | Visual commonsense understanding requires Vision Language (VL) models to not only understand image and text but also cross-reference in-between to fully integrate and achieve comprehension of the visual scene described. Recently, various approaches have been developed and have achieved high... | Haoxuan You, KaiWei Chang, ShihFu Chang, Wenhao Li, Yicheng He, Zhecan Wang |  |
| 1282 |  |  [Semantic Novelty Detection and Characterization in Factual Text Involving Named Entities](https://doi.org/10.18653/v1/2022.emnlp-main.627) |  | 0 | Much of the existing work on text novelty detection has been studied at the topic level, i.e., identifying whether the topic of a document or a sentence is novel or not. Little work has been done at the fine-grained semantic level (or contextual level). For example, given that we know Elon Musk is... | Alexander Politowicz, Bing Liu, Eric Robertson, Nianzu Ma, Sahisnu Mazumder, Scott Grigsby |  |
| 1283 |  |  [CN-AutoMIC: Distilling Chinese Commonsense Knowledge from Pretrained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.628) |  | 0 | Commonsense knowledge graphs (CKGs) are increasingly applied in various natural language processing tasks. However, most existing CKGs are limited to English, which hinders related research in non-English languages. Meanwhile, directly generating commonsense knowledge from pretrained language... | Chenhao Wang, Jiachun Li, Jun Zhao, Kang Liu, Yubo Chen |  |
| 1284 |  |  [Calibrating Student Models for Emotion-related Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.629) |  | 0 | Knowledge Distillation (KD) is an effective method to transfer knowledge from one network (a.k.a. teacher) to another (a.k.a. student). In this paper, we study KD on the emotion-related tasks from a new perspective: calibration. We further explore the impact of the mixup data augmentation technique... | Cornelia Caragea, Mahshid Hosseini |  |
| 1285 |  |  [Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation](https://doi.org/10.18653/v1/2022.emnlp-main.630) |  | 0 | In this paper, we explore the challenging problem of performing a generative task in a target language when labeled data is only available in English, using summarization as a case study. We assume a strict setting with no access to parallel data or machine translation and find that common transfer... | Aditya Barua, Brian Lester, Daniel Cer, Mohit Iyyer, Noah Constant, Tu Vu |  |
| 1286 |  |  [Improving Large-scale Paraphrase Acquisition and Generation](https://doi.org/10.18653/v1/2022.emnlp-main.631) |  | 0 | This paper addresses the quality issues in existing Twitter-based paraphrase datasets, and discusses the necessity of using two separate definitions of paraphrase for identification and generation tasks. We present a new Multi-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total... | Chao Jiang, Wei Xu, Yao Dou |  |
| 1287 |  |  [Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal](https://doi.org/10.18653/v1/2022.emnlp-main.632) |  | 0 | Transformer-based large language models are trained to make predictions about the next word by aggregating representations of previous tokens through their self-attention mechanism. In the field of cognitive modeling, such attention patterns have recently been interpreted as embodying the process... | ByungDoh Oh, William Schuler |  |
| 1288 |  |  [A Survey of Computational Framing Analysis Approaches](https://doi.org/10.18653/v1/2022.emnlp-main.633) |  | 0 | Framing analysis is predominantly qualitative and quantitative, examining a small dataset with manual coding. Easy access to digital data in the last two decades prompts scholars in both computation and social sciences to utilize various computational methods to explore frames in large-scale... | Mohammad Ali, Naeemul Hassan |  |
| 1289 |  |  [Learning Cross-Task Dependencies for Joint Extraction of Entities, Events, Event Arguments, and Relations](https://doi.org/10.18653/v1/2022.emnlp-main.634) |  | 0 | Extracting entities, events, event arguments, and relations (i.e., task instances) from text represents four main challenging tasks in information extraction (IE), which have been solved jointly (JointIE) to boost the overall performance for IE. As such, previous work often leverages two types of... | Bonan Min, Franck Dernoncourt, Minh Van Nguyen, Thien Huu Nguyen |  |
| 1290 |  |  [Don't Copy the Teacher: Data and Model Challenges in Embodied Dialogue](https://doi.org/10.18653/v1/2022.emnlp-main.635) |  | 0 | Embodied dialogue instruction following requires an agent to complete a complex sequence of tasks from a natural language exchange. The recent introduction of benchmarks raises the question of how best to train and evaluate models for this multi-turn, multi-agent, long-horizon task. This paper... | Hao Zhu, Ruslan Salakhutdinov, So Yeon Min, Yonatan Bisk |  |
| 1291 |  |  [ALFRED-L: Investigating the Role of Language for Action Learning in Interactive Visual Environments](https://doi.org/10.18653/v1/2022.emnlp-main.636) |  | 0 | Embodied Vision and Language Task Completion requires an embodied agent to interpret natural language instructions and egocentric visual observations to navigate through and interact with environments. In this work, we examine ALFRED, a challenging benchmark for embodied task completion, with the... | Aishwarya Padmakumar, Arjun R. Akula, Dilek HakkaniTur, Jesse Thomason, Mahdi Namazifar, Mohit Bansal, Spandana Gella |  |
| 1292 |  |  [Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence](https://doi.org/10.18653/v1/2022.emnlp-main.637) |  | 0 | AI researchers have posited Dungeons and Dragons (D&D) as a challenge problem to test systems on various language-related capabilities. In this paper, we frame D&D specifically as a dialogue system challenge, where the tasks are to both generate the next conversational turn in the game and predict... | Chris CallisonBurch, Daphne Ippolito, David Reitter, Gaurav Singh Tomar, Lara J. Martin, Suma Bailis |  |
| 1293 |  |  [Unsupervised Entity Linking with Guided Summarization and Multiple-Choice Selection](https://doi.org/10.18653/v1/2022.emnlp-main.638) |  | 0 | Entity linking, the task of linking potentially ambiguous mentions in texts to corresponding knowledge-base entities, is an important component for language understanding. We address two challenge in entity linking: how to leverage wider contexts surrounding a mention, and how to deal with limited... | Chris CallisonBurch, Li Zhang, Young Min Cho |  |
| 1294 |  |  [Weakly-Supervised Temporal Article Grounding](https://doi.org/10.18653/v1/2022.emnlp-main.639) |  | 0 | Given a long untrimmed video and natural language queries, video grounding (VG) aims to temporally localize the semantically-aligned video segments. Almost all existing VG work holds two simple but unrealistic assumptions: 1) All query sentences can be grounded in the corresponding video. 2) All... | Brian Chen, Christopher Thomas, Guangxing Han, Hammad A. Ayyubi, Heng Ji, Long Chen, ShihFu Chang, Xudong Lin, Yulei Niu |  |
| 1295 |  |  [Exploring Dual Encoder Architectures for Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.640) |  | 0 | Dual encoders have been used for question-answering (QA) and information retrieval (IR) tasks with good results. There are two major types of dual encoders, Siamese Dual Encoders (SDE), with parameters shared across two encoders, and Asymmetric Dual Encoder (ADE), with two distinctly parameterized... | Chen Qu, Dan Bikel, Enrique Alfonseca, Imed Zitouni, Jianmo Ni, Yuan Wang, Zhe Dong |  |
| 1296 |  |  [arXivEdits: Understanding the Human Revision Process in Scientific Writing](https://doi.org/10.18653/v1/2022.emnlp-main.641) |  | 0 | Scientific publications are the primary means to communicate research discoveries, where the writing quality is of crucial importance. However, prior work studying the human editing process in this domain mainly focused on the abstract or introduction sections, resulting in an incomplete picture.... | Chao Jiang, Samuel Stevens, Wei Xu |  |
| 1297 |  |  [Why Do You Feel This Way? Summarizing Triggers of Emotions in Social Media Posts](https://doi.org/10.18653/v1/2022.emnlp-main.642) |  | 0 | Crises such as the COVID-19 pandemic continuously threaten our world and emotionally affect billions of people worldwide in distinct ways. Understanding the triggers leading to people’s emotions is of crucial importance. Social media posts can be a good source of such analysis, yet these texts tend... | Cornelia Caragea, Hongli Zhan, Junyi Jessy Li, Tiberiu Sosea |  |
| 1298 |  |  [Analogical Math Word Problems Solving with Enhanced Problem-Solution Association](https://doi.org/10.18653/v1/2022.emnlp-main.643) |  | 0 | Math word problem (MWP) solving is an important task in question answering which requires human-like reasoning ability. Analogical reasoning has long been used in mathematical education, as it enables students to apply common relational structures of mathematical situations to solve new problems.... | Jipeng Zhang, Xiangliang Zhang, Zhenwen Liang |  |
| 1299 |  |  [Towards Teachable Reasoning Systems: Using a Dynamic Memory of User Feedback for Continual System Improvement](https://doi.org/10.18653/v1/2022.emnlp-main.644) |  | 0 | Our goal is a teachable reasoning system for question-answering (QA), where a user can interact with faithful answer explanations, and correct its errors so that the system improves over time. Our approach is to augment a QA model with a dynamic memory of user feedback, containing user-supplied... | Bhavana Dalvi Mishra, Oyvind Tafjord, Peter Clark |  |
| 1300 |  |  [Knowledge Transfer from Answer Ranking to Answer Generation](https://doi.org/10.18653/v1/2022.emnlp-main.645) |  | 0 | Recent studies show that Question Answering (QA) based on Answer Sentence Selection (AS2) can be improved by generating an improved answer from the top-k ranked answer sentences (termed GenQA). This allows for synthesizing the information from multiple candidates into a concise, natural-sounding... | Alessandro Moschitti, Luca Soldaini, Matteo Gabburo, Rik KoncelKedziorski, Siddhant Garg |  |
| 1301 |  |  [Perturbation Augmentation for Fairer NLP](https://doi.org/10.18653/v1/2022.emnlp-main.646) |  | 0 | Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets. In this work, we ask whether training on demographically perturbed data leads to fairer language models. We collect a large dataset of human annotated text perturbations and... | Adina Williams, Candace Ross, Douwe Kiela, Eric Michael Smith, Jude Fernandes, Rebecca Qian |  |
| 1302 |  |  [Automatic Document Selection for Efficient Encoder Pretraining](https://doi.org/10.18653/v1/2022.emnlp-main.647) |  | 0 | Building pretrained language models is considered expensive and data-intensive, but must we increase dataset size to achieve better performance? We propose an alternative to larger training sets by automatically identifying smaller yet domain-representative subsets. We extend Cynical Data... | Benjamin Van Durme, João Sedoc, Patrick Xia, Yukun Feng |  |
| 1303 |  |  [The Aligned Multimodal Movie Treebank: An audio, video, dependency-parse treebank](https://doi.org/10.18653/v1/2022.emnlp-main.648) |  | 0 | Treebanks have traditionally included only text and were derived from written sources such as newspapers or the web. We introduce the Aligned Multimodal Movie Treebank (AMMT), an English language treebank derived from dialog in Hollywood movies which includes transcriptions of the audio-visual... | Adam Uri Yaari, Andrei Barbu, Bennett Stankovits, Boris Katz, Helena Aparicio, Henry Hu, Ignacio Cases, Jan DeWitt, Sue Felshin, Yevgeni Berzak |  |
| 1304 |  |  [DEMETR: Diagnosing Evaluation Metrics for Translation](https://doi.org/10.18653/v1/2022.emnlp-main.649) |  | 0 | While machine translation evaluation metrics based on string overlap (e.g., BLEU) have their limitations, their computations are transparent: the BLEU score assigned to a particular candidate translation can be traced back to the presence or absence of certain words. The operations of newer learned... | Ankita Gupta, Katherine Thai, Marzena Karpinska, Mohit Iyyer, Nishant Raj, Yixiao Song |  |
| 1305 |  |  [Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.650) |  | 0 | Answering open-domain questions requires world knowledge about in-context entities. As pre-trained Language Models (LMs) lack the power to store all required knowledge, external knowledge sources, such as knowledge graphs, are often used to augment LMs. In this work, we propose knOwledge REasOning... | Chenguang Zhu, KaiWei Chang, Shuohang Wang, Wenhao Yu, Yichong Xu, Yizhou Sun, Ziniu Hu, Ziyi Yang |  |
| 1306 |  |  [Debiasing Pretrained Text Encoders by Paying Attention to Paying Attention](https://doi.org/10.18653/v1/2022.emnlp-main.651) |  | 0 | Natural Language Processing (NLP) models are found to exhibit discriminatory stereotypes across many social constructs, e.g. gender and race. In comparison to the progress made in reducing bias from static word embeddings, fairness in sentence-level text encoders received little consideration... | Boualem Benatallah, Fabio Casati, Khalid Benabdeslem, Yacine Gaci |  |
| 1307 |  |  [MEE: A Novel Multilingual Event Extraction Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.652) |  | 0 | Event Extraction (EE) is one of the fundamental tasks in Information Extraction (IE) that aims to recognize event mentions and their arguments (i.e., participants) from text. Due to its importance, extensive methods and resources have been developed for Event Extraction. However, one limitation of... | Amir Pouran Ben Veyseh, Franck Dernoncourt, Javid Ebrahimi, Thien Huu Nguyen |  |
| 1308 |  |  [RobustLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners](https://doi.org/10.18653/v1/2022.emnlp-main.653) |  | 0 | Transformers have been shown to be able to perform deductive reasoning on inputs containing rules and statements written in the English natural language. However, it is unclear if these models indeed follow rigorous logical reasoning to arrive at the prediction or rely on spurious correlation... | Soumya Sanyal, Xiang Ren, Zeyi Liao |  |
| 1309 |  |  [Evaluating and Improving Factuality in Multimodal Abstractive Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.654) |  | 0 | Current metrics for evaluating factuality for abstractive document summarization have achieved high correlations with human judgment, but they do not account for the vision modality and thus are not adequate for vision-and-language summarization. We propose CLIPBERTSCORE, a simple weighted... | David Wan, Mohit Bansal |  |
| 1310 |  |  [Referee: Reference-Free Sentence Summarization with Sharper Controllability through Symbolic Knowledge Distillation](https://doi.org/10.18653/v1/2022.emnlp-main.655) |  | 0 | We present Referee, a novel framework for sentence summarization that can be trained reference-free (i.e., requiring no gold summaries for supervision), while allowing direct control for compression ratio. Our work is the first to demonstrate that reference-free, controlled sentence summarization... | Melanie Sclar, Peter West, Sachin Kumar, Yejin Choi, Yulia Tsvetkov |  |
| 1311 |  |  [Algorithms for Weighted Pushdown Automata](https://doi.org/10.18653/v1/2022.emnlp-main.656) |  | 0 | Weighted pushdown automata (WPDAs) are at the core of many natural language processing tasks, like syntax-based statistical machine translation and transition-based dependency parsing. As most existing dynamic programming algorithms are designed for context-free grammars (CFGs), algorithms for PDAs... | Alexandra Butoi, Brian DuSell, David Chiang, Ryan Cotterell, Tim Vieira |  |
| 1312 |  |  [MABEL: Attenuating Gender Bias using Textual Entailment Data](https://doi.org/10.18653/v1/2022.emnlp-main.657) |  | 0 | Pre-trained language models encode undesirable social biases, which are further exacerbated in downstream use. To this end, we propose MABEL (a Method for Attenuating Gender Bias using Entailment Labels), an intermediate pre-training approach for mitigating gender bias in contextualized... | Christiane Fellbaum, Danqi Chen, Jacqueline He, Mengzhou Xia |  |
| 1313 |  |  [Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs](https://doi.org/10.18653/v1/2022.emnlp-main.658) |  | 0 | Can we teach models designed for language understanding tasks to track and improve their beliefs through intermediate points in text? Besides making their inner workings more transparent, this would also help make models more reliable and consistent. To this end, we propose a representation... | Ashish Sabharwal, Dafna Shahaf, Kyle Richardson, Oren Sultan, Reut Tsarfaty, Ronen Tamari |  |
| 1314 |  |  [Late Fusion with Triplet Margin Objective for Multimodal Ideology Prediction and Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.659) |  | 0 | Prior work on ideology prediction has largely focused on single modalities, i.e., text or images. In this work, we introduce the task of multimodal ideology prediction, where a model predicts binary or five-point scale ideological leanings, given a text-image pair with political content. We first... | Changyuan Qiu, Lu Wang, Winston Wu, Xinliang Frederick Zhang |  |
| 1315 |  |  [Leveraging QA Datasets to Improve Generative Data Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.660) |  | 0 | The ability of generative language models (GLMs) to generate text has improved considerably in the last few years, enabling their use for generative data augmentation. In this work, we propose CONDA, an approach to further improve GLM’s ability to generate synthetic data by reformulating data... | Dheeraj Mekala, Jingbo Shang, Timo Schick, Tu Vu |  |
| 1316 |  |  [Meta-Learning Fast Weight Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.661) |  | 0 | Dynamic evaluation of language models (LMs) adapts model parameters at test time using gradient information from previous tokens and substantially improves LM performance. However, it requires over 3x more compute than standard inference. We present Fast Weight Layers (FWLs), a neural component... | Geoffrey E. Hinton, Kelvin Guu, Kevin Clark, MingWei Chang, Mohammad Norouzi, Panupong Pasupat |  |
| 1317 |  |  [CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations](https://doi.org/10.18653/v1/2022.emnlp-main.662) |  | 0 | Well-designed diagnostic tasks have played a key role in studying the failure of neural nets (NNs) to generalize systematically. Famous examples include SCAN and Compositional Table Lookup (CTL). Here we introduce CTL++, a new diagnostic dataset based on compositions of unary symbolic functions.... | Jürgen Schmidhuber, Kazuki Irie, Róbert Csordás |  |
| 1318 |  |  [Learning with Rejection for Abstractive Text Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.663) |  | 0 | State-of-the-art abstractive summarization systems frequently hallucinate content that is not supported by the source document, mainly due to noise in the training dataset.Existing methods opt to drop the noisy samples or tokens from the training set entirely, reducing the effective training set... | Jackie Chi Kit Cheung, Jingyi He, Meng Cao, Yue Dong |  |
| 1319 |  |  [Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation](https://doi.org/10.18653/v1/2022.emnlp-main.664) |  | 0 | Overconfidence has been shown to impair generalization and calibration of a neural network. Previous studies remedy this issue by adding a regularization term to a loss function, preventing a model from making a peaked distribution. Label smoothing smoothes target labels with a pre-defined prior... | Dongkyu Lee, Ka Chun Cheung, Nevin L. Zhang |  |
| 1320 |  |  [Hard Gate Knowledge Distillation - Leverage Calibration for Robust and Reliable Language Model](https://doi.org/10.18653/v1/2022.emnlp-main.665) |  | 0 | In knowledge distillation, a student model is trained with supervisions from both knowledge from a teacher and observations drawn from a training data distribution. Knowledge of a teacher is considered a subject that holds inter-class relations which send a meaningful supervision to a student;... | Dongkyu Lee, Ka Chun Cheung, Nevin Lianwen Zhang, Yingxiu Zhao, Zhiliang Tian |  |
| 1321 |  |  [Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens](https://doi.org/10.18653/v1/2022.emnlp-main.666) |  | 0 | The term ‘spurious correlations’ has been used in NLP to informally denote any undesirable feature-label correlations. However, a correlation can be undesirable because (i) the feature is irrelevant to the label (e.g. punctuation in a review), or (ii) the feature’s effect on the label depends on... | He He, Nitish Joshi, Xiang Pan |  |
| 1322 |  |  [Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling](https://doi.org/10.18653/v1/2022.emnlp-main.667) |  | 0 | Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content. Recent works focus on correcting factual errors in generated summaries via post-editing. Such correction models are trained using adversarial non-factual summaries constructed... | Hannaneh Hajishirzi, Vidhisha Balachandran, William W. Cohen, Yulia Tsvetkov |  |
| 1323 |  |  [Coordinated Topic Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.668) |  | 0 | We propose a new problem called coordinated topic modeling that imitates human behavior while describing a text corpus. It considers a set of well-defined topics like the axes of a semantic space with a reference representation. It then uses the axes to model a corpus for easily understandable... | Jie Huang, Kevin ChenChuan Chang, Pritom Saha Akash |  |
| 1324 |  |  [Large Dual Encoders Are Generalizable Retrievers](https://doi.org/10.18653/v1/2022.emnlp-main.669) |  | 0 | It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dot-product between a query vector and a passage vector, is too limited... | Chen Qu, Gustavo Hernández Ábrego, Ji Ma, Jianmo Ni, Jing Lu, Keith B. Hall, MingWei Chang, Vincent Y. Zhao, Yi Luan, Yinfei Yang, Zhuyun Dai |  |
| 1325 |  |  [CRIPP-VQA: Counterfactual Reasoning about Implicit Physical Properties via Video Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.670) |  | 0 | Videos often capture objects, their visible properties, their motion, and the interactions between different objects. Objects also have physical properties such as mass, which the imaging pipeline is unable to directly capture. However, these properties can be estimated by utilizing cues from... | Chitta Baral, Maitreya Patel, Tejas Gokhale, Yezhou Yang |  |
| 1326 |  |  [Entity-centered Cross-document Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.671) |  | 0 | Relation Extraction (RE) is a fundamental task of information extraction, which has attracted a large amount of research attention. Previous studies focus on extracting the relations within a sentence or document, while currently researchers begin to explore cross-document RE. However, current... | Bo Cai, Donghong Ji, Fangfang Su, Fei Li, Fengqi Wang, Hao Fei, Jingye Li, Shengqiong Wu, Wenxuan Shi |  |
| 1327 |  |  [Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature](https://doi.org/10.18653/v1/2022.emnlp-main.672) |  | 0 | Literary translation is a culturally significant task, but it is bottlenecked by the small number of qualified literary translators relative to the many untranslated works published around the world. Machine translation (MT) holds potential to complement the work of human translators by improving... | Bill Ray, John Wieting, Kalpesh Krishna, Katherine Thai, Marzena Karpinska, Mohit Iyyer, Moira Inghilleri |  |
| 1328 |  |  [Label-aware Multi-level Contrastive Learning for Cross-lingual Spoken Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.673) |  | 0 | Despite the great success of spoken language understanding (SLU) in high-resource languages, it remains challenging in low-resource languages mainly due to the lack of labeled training data. The recent multilingual code-switching approach achieves better alignments of model representations across... | Daxin Jiang, Jian Pei, Linjun Shou, Ming Gong, Shining Liang, Wanli Zuo, Xianglin Zuo |  |
| 1329 |  |  [Polyglot Prompt: Multilingual Multitask Prompt Training](https://doi.org/10.18653/v1/2022.emnlp-main.674) |  | 0 | This paper aims for a potential architectural improvement for multilingual learning and asks: Can different tasks from different languages be modeled in a monolithic framework, i.e. without any task/language-specific module? The benefit of achieving this could open new doors for future multilingual... | Jinlan Fu, Pengfei Liu, SeeKiong Ng |  |
| 1330 |  |  [VisToT: Vision-Augmented Table-to-Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.675) |  | 0 | Table-to-text generation has been widely studied in the Natural Language Processing community in the recent years. We give a new perspective to this problem by incorporating signals from both tables as well as associated images to generate relevant text. While tables contain a structured list of... | Anand Mishra, Manish Gupta, Mithun Das Gupta, Prajwal Gatti |  |
| 1331 |  |  [Generative Entity-to-Entity Stance Detection with Knowledge Graph Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.676) |  | 0 | Stance detection is typically framed as predicting the sentiment in a given text towards a target entity. However, this setup overlooks the importance of the source entity, i.e., who is expressing the opinion. In this paper, we emphasize the imperative need for studying interactions among entities... | Lu Wang, Nick Beauchamp, Xinliang Frederick Zhang |  |
| 1332 |  |  [Symptom Identification for Interpretable Detection of Multiple Mental Disorders on Social Media](https://doi.org/10.18653/v1/2022.emnlp-main.677) |  | 0 | Mental disease detection (MDD) from social media has suffered from poor generalizability and interpretability, due to lack of symptom modeling. This paper introduces PsySym, the first annotated symptom identification corpus of multiple psychiatric disorders, to facilitate further research progress.... | Kenny Q. Zhu, Mengyue Wu, Siyuan Chen, Zhiling Zhang |  |
| 1333 |  |  [Improving Iterative Text Revision by Learning Where to Edit from Other Revision Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.678) |  | 0 | Iterative text revision improves text quality by fixing grammatical errors, rephrasing for better readability or contextual appropriateness, or reorganizing sentence structures throughout a document.Most recent research has focused on understanding and classifying different types of edits in the... | Dhruv Kumar, Dongyeop Kang, Vipul Raheja, Wanyu Du, Zae Myung Kim |  |
| 1334 |  |  [CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning](https://doi.org/10.18653/v1/2022.emnlp-main.679) |  | 0 | Compared to standard retrieval tasks, passage retrieval for conversational question answering (CQA) poses new challenges in understanding the current user question, as each question needs to be interpreted within the dialogue context. Moreover, it can be expensive to re-train well-established... | David Reitter, Gaurav Singh Tomar, Hannah Rashkin, Hannaneh Hajishirzi, Mari Ostendorf, Yi Luan, Zeqiu Wu |  |
| 1335 |  |  [Specializing Multi-domain NMT via Penalizing Low Mutual Information](https://doi.org/10.18653/v1/2022.emnlp-main.680) |  | 0 | Multi-domain Neural Machine Translation (NMT) trains a single model with multiple domains. It is appealing because of its efficacy in handling multiple domains within one model. An ideal multi-domain NMT learns distinctive domain characteristics simultaneously, however, grasping the domain... | Cheonbok Park, Edward Choi, Hantae Kim, Hyunchang Cho, Jiyoung Lee |  |
| 1336 |  |  [A Simple Contrastive Learning Framework for Interactive Argument Pair Identification via Argument-Context Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.681) |  | 0 | Interactive argument pair identification is an emerging research task for argument mining, aiming to identify whether two arguments are interactively related. It is pointed out that the context of the argument is essential to improve identification performance. However, current context-based... | Daqian Shi, Fausto Giunchiglia, Hao Xu, Lida Shi, Rui Song, Tongtong Liu, Xiaolei Diao |  |
| 1337 |  |  [Sentence-level Media Bias Analysis Informed by Discourse Structures](https://doi.org/10.18653/v1/2022.emnlp-main.682) |  | 0 | As polarization continues to rise among both the public and the news media, increasing attention has been devoted to detecting media bias. Most recent work in the NLP community, however, identify bias at the level of individual articles. However, each article itself comprises multiple sentences,... | Lu Wang, Nick Beauchamp, Ruihong Huang, Yuanyuan Lei |  |
| 1338 |  |  [Towards Efficient Dialogue Pre-training with Transferable and Interpretable Latent Structure](https://doi.org/10.18653/v1/2022.emnlp-main.683) |  | 0 | With the availability of massive general-domain dialogue data, pre-trained dialogue generation appears to be super appealing to transfer knowledge from the general domain to downstream applications. In most existing work, such transferable ability is mainly obtained by fitting a large model with... | Dongyan Zhao, Lemao Liu, Rui Yan, Shuming Shi, Tingchen Fu, Xueliang Zhao |  |
| 1339 |  |  [An Empirical Revisiting of Linguistic Knowledge Fusion in Language Understanding Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.684) |  | 0 | Though linguistic knowledge emerges during large-scale language model pretraining, recent work attempt to explicitly incorporate human-defined linguistic priors into task-specific fine-tuning. Infusing language models with syntactic or semantic knowledge from parsers has shown improvements on many... | Changlong Yu, Lingpeng Kong, Tianyi Xiao, Wilfred Ng, Yangqiu Song |  |
| 1340 |  |  [Unsupervised Non-transferable Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.685) |  | 0 | Training a good deep learning model requires substantial data and computing resources, which makes the resulting neural model a valuable intellectual property. To prevent the neural network from being undesirably exploited, non-transferable learning has been proposed to reduce the model... | Guangtao Zeng, Wei Lu |  |
| 1341 |  |  [Adaptive Contrastive Learning on Multimodal Transformer for Review Helpfulness Prediction](https://doi.org/10.18653/v1/2022.emnlp-main.686) |  | 0 | Modern Review Helpfulness Prediction systems are dependent upon multiple modalities, typically texts and images. Unfortunately, those contemporary approaches pay scarce attention to polish representations of cross-modal relations and tend to suffer from inferior optimization. This might cause harm... | Anh Tuan Luu, Lidong Bing, Thong Nguyen, Xiaobao Wu, Zhen Hai |  |
| 1342 |  |  [Adaptive Token-level Cross-lingual Feature Mixing for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.687) |  | 0 | Multilingual neural machine translation aims to translate multiple language pairs in a single model and has shown great success thanks to the knowledge transfer across languages with the shared parameters. Despite promising, this share-all paradigm suffers from insufficient ability to capture... | Degen Huang, Huan Liu, Jinsong Su, Jiuyi Li, Junpeng Liu, Kaiyu Huang |  |
| 1343 |  |  [A Dataset for Hyper-Relational Extraction and a Cube-Filling Approach](https://doi.org/10.18653/v1/2022.emnlp-main.688) |  | 0 | Relation extraction has the potential for large-scale knowledge graph construction, but current methods do not consider the qualifier attributes for each relation triplet, such as time, quantity or location. The qualifiers form hyper-relational facts which better capture the rich and complex... | Lidong Bing, Luo Si, Sharifah Mahani Aljunied, Soujanya Poria, Yew Ken Chia |  |
| 1344 |  |  [Low-resource Neural Machine Translation with Cross-modal Alignment](https://doi.org/10.18653/v1/2022.emnlp-main.689) |  | 0 | How to achieve neural machine translation with limited parallel data? Existing techniques often rely on large-scale monolingual corpus, which is impractical for some low-resource languages. In this paper, we turn to connect several low-resource languages to a particular high-resource one by... | Qingkai Fang, Yang Feng, Zhe Yang |  |
| 1345 |  |  [Prompt-based Distribution Alignment for Domain Generalization in Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.690) |  | 0 | Prompt-based learning (a.k.a. prompting) achieves high performance by bridging the gap between the objectives of language modeling and downstream tasks. Domain generalization ability can be improved by prompting since classification across different domains can be unified into the prediction of the... | Chen Jia, Yue Zhang |  |
| 1346 |  |  [Two is Better than Many? Binary Classification as an Effective Approach to Multi-Choice Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.691) |  | 0 | We propose a simple refactoring of multi-choice question answering (MCQA) tasks as a series of binary classifications. The MCQA task is generally performed by scoring each (question, answer) pair normalized over all the pairs, and then selecting the answer from the pair that yield the highest... | Deepanway Ghosal, Navonil Majumder, Rada Mihalcea, Soujanya Poria |  |
| 1347 |  |  [HEGEL: Hypergraph Transformer for Long Document Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.692) |  | 0 | Extractive summarization for long documents is challenging due to the extended structured input context. The long-distance sentence dependency hinders cross-sentence relations modeling, the critical step of extractive summarization. This paper proposes HEGEL, a hypergraph neural network for long... | Haopeng Zhang, Jiawei Zhang, Xiao Liu |  |
| 1348 |  |  [Adapting a Language Model While Preserving its General Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.693) |  | 0 | Domain-adaptive pre-training (or DA-training for short), also known as post-training, aimsto train a pre-trained general-purpose language model (LM) using an unlabeled corpus of aparticular domain to adapt the LM so that end-tasks in the domain can give improved performances. However, existing... | Bing Liu, Haowei Lin, Hu Xu, Lei Shu, Yijia Shao, Zixuan Ke |  |
| 1349 |  |  [Human Guided Exploitation of Interpretable Attention Patterns in Summarization and Topic Segmentation](https://doi.org/10.18653/v1/2022.emnlp-main.694) |  | 0 | The multi-head self-attention mechanism of the transformer model has been thoroughly investigated recently. In one vein of study, researchers are interested in understanding why and how transformers work. In another vein, researchers propose new attention augmentation methods to make transformers... | Gabriel Murray, Giuseppe Carenini, Lanjun Wang, Linzi Xing, Raymond Li, Wen Xiao |  |
| 1350 |  |  [Continual Training of Language Models for Few-Shot Learning](https://doi.org/10.18653/v1/2022.emnlp-main.695) |  | 0 | Recent work on applying large language models (LMs) achieves impressive performance in many NLP applications. Adapting or posttraining an LM using an unlabeled domain corpus can produce even better performance for end-tasks in the domain. This paper proposes the problem of continually extending an... | Bing Liu, Haowei Lin, Hu Xu, Lei Shu, Yijia Shao, Zixuan Ke |  |
| 1351 |  |  [Dictionary-Assisted Supervised Contrastive Learning](https://doi.org/10.18653/v1/2022.emnlp-main.696) |  | 0 | Text analysis in the social sciences often involves using specialized dictionaries to reason with abstract concepts, such as perceptions about the economy or abuse on social media. These dictionaries allow researchers to impart domain knowledge and note subtle usages of words relating to a... | Jonathan Nagler, Joshua A. Tucker, Patrick Y. Wu, Richard Bonneau |  |
| 1352 |  |  [Fine-Tuning Pre-trained Transformers into Decaying Fast Weights](https://doi.org/10.18653/v1/2022.emnlp-main.697) |  | 0 | Autoregressive Transformers are strong language models but incur O(T) complexity during per-token generation due to the self-attention mechanism. Recent work proposes kernel-based methods to approximate causal self-attention by replacing it with recurrent formulations with various update rules and... | Huanru Henry Mao |  |
| 1353 |  |  [PRO-CS : An Instance-Based Prompt Composition Technique for Code-Switched Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.698) |  | 0 | Code-switched (CS) data is ubiquitous in today’s globalized world, but the dearth of annotated datasets in code-switching poses a significant challenge for learning diverse tasks across different language pairs. Parameter-efficient prompt-tuning approaches conditioned on frozen language models have... | Eric Nyberg, Srijan Bansal, Sumit Agarwal, Suraj Tripathi, Teruko Mitamura |  |
| 1354 |  |  [SentBS: Sentence-level Beam Search for Controllable Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.699) |  | 0 | A wide range of control perspectives have been explored in controllable text generation. Structure-controlled summarization is recently proposed as a useful and interesting research direction. However, current structure-controlling methods have limited effectiveness in enforcing the desired... | Chenhui Shen, Lidong Bing, Liying Cheng, Luo Si, Yang You |  |
| 1355 |  |  [A Fine-grained Chinese Software Privacy Policy Dataset for Sequence Labeling and Regulation Compliant Identification](https://doi.org/10.18653/v1/2022.emnlp-main.700) |  | 0 | Privacy protection raises great attention on both legal levels and user awareness. To protect user privacy, countries enact laws and regulations requiring software privacy policies to regulate their behavior. However, privacy policies are written in professional languages with many legal terms and... | Aemon Yat Fei Chiu, Jing Li, Kaifa Zhao, Le Yu, Shiyao Zhou, Xiapu Luo, Yutong Liu |  |
| 1356 |  |  [Saving Dense Retriever from Shortcut Dependency in Conversational Search](https://doi.org/10.18653/v1/2022.emnlp-main.701) |  | 0 | Conversational search (CS) needs a holistic understanding of conversational inputs to retrieve relevant passages. In this paper, we demonstrate the existence of a retrieval shortcut in CS, which causes models to retrieve passages solely relying on partial history while disregarding the latest... | Gangwoo Kim, Sungdong Kim |  |
| 1357 |  |  [Graph-Induced Transformers for Efficient Multi-Hop Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.702) |  | 0 | A graph is a suitable data structure to represent the structural information of text. Recently, multi-hop question answering (MHQA) tasks, which require inter-paragraph/sentence linkages, have come to exploit such properties of a graph. Previous approaches to MHQA relied on leveraging the graph... | Giwon Hong, Jeonghwan Kim, Junmo Kang, SungHyon Myaeng |  |
| 1358 |  |  [DiscoSense: Commonsense Reasoning with Discourse Connectives](https://doi.org/10.18653/v1/2022.emnlp-main.703) |  | 0 | We present DiscoSense, a benchmark for commonsense reasoning via understanding a wide variety of discourse connectives. We generate compelling distractors in DiscoSense using Conditional Adversarial Filtering, an extension of Adversarial Filtering that employs conditional generation. We show that... | Prajjwal Bhargava, Vincent Ng |  |
| 1359 |  |  [Boosting Document-Level Relation Extraction by Mining and Injecting Logical Rules](https://doi.org/10.18653/v1/2022.emnlp-main.704) |  | 0 | Document-level relation extraction (DocRE) aims at extracting relations of all entity pairs in a document. A key challenge to DocRE lies in the complex interdependency between the relations of entity pairs. Unlike most prior efforts focusing on implicitly powerful representations, the recently... | Jianwei Niu, Shasha Mo, Shengda Fan |  |
| 1360 |  |  [MOCHA: A Multi-Task Training Approach for Coherent Text Generation from Cognitive Perspective](https://doi.org/10.18653/v1/2022.emnlp-main.705) |  | 0 | Teaching neural models to generate narrative coherent texts is a critical problem. Recent pre-trained language models have achieved promising results, but there is still a gap between human written texts and machine-generated outputs. In this work, we propose a novel multi-task training strategy... | Hou Pong Chan, Lifu Huang, Zhe Hu |  |
| 1361 |  |  [Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation](https://doi.org/10.18653/v1/2022.emnlp-main.706) |  | 0 | In this paper, we propose a variational autoencoder with disentanglement priors, VAE-Dprior, for task-specific natural language generation with none or a handful of task-specific labeled examples. In order to tackle compositional generalization across tasks, our model performs disentangled... | Gholamreza Haffari, Lizhen Qu, Qiongkai Xu, Tianyang Zhan, Tongtong Wu, Zhuang Li |  |
| 1362 |  |  [CISLR: Corpus for Indian Sign Language Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.707) |  | 0 | Indian Sign Language, though used by a diverse community, still lacks well-annotated resources for developing systems that would enable sign language processing. In recent years researchers have actively worked for sign languages like American Sign Languages, however, Indian Sign language is still... | Abhinav Joshi, Ashutosh Modi, Ashwani Bhat, Pradeep S, Priya Gole, Shashwat Gupta, Shreyansh Agarwal |  |
| 1363 |  |  [Mask the Correct Tokens: An Embarrassingly Simple Approach for Error Correction](https://doi.org/10.18653/v1/2022.emnlp-main.708) |  | 0 | Text error correction aims to correct the errors in text sequences such as those typed by humans or generated by speech recognition models.Previous error correction methods usually take the source (incorrect) sentence as encoder input and generate the target (correct) sentence through the decoder.... | Edward Lin, Kai Shen, Siliang Tang, Wenjie Liu, Xu Tan, Yichong Leng, Yuan Zhang |  |
| 1364 |  |  [AMAL: Meta Knowledge-Driven Few-Shot Adapter Learning](https://doi.org/10.18653/v1/2022.emnlp-main.709) |  | 0 | NLP has advanced greatly together with the proliferation of Transformer-based pre-trained language models. To adapt to a downstream task, the pre-trained language models need to be fine-tuned with a sufficient supply of annotated examples. In recent years, Adapter-based fine-tuning methods have... | S. K. Hong, Tae Young Jang |  |
| 1365 |  |  [Discourse Context Predictability Effects in Hindi Word Order](https://doi.org/10.18653/v1/2022.emnlp-main.710) |  | 0 | We test the hypothesis that discourse predictability influences Hindi syntactic choice. While prior work has shown that a number of factors (e.g., information status, dependency length, and syntactic surprisal) influence Hindi word order preferences, the role of discourse predictability is... | Marten van Schijndel, Rajakrishnan Rajkumar, Sidharth Ranjan, Sumeet Agarwal |  |
| 1366 |  |  ["Covid vaccine is against Covid but Oxford vaccine is made at Oxford!" Semantic Interpretation of Proper Noun Compounds](https://doi.org/10.18653/v1/2022.emnlp-main.711) |  | 0 | Proper noun compounds, e.g., “Covid vaccine”, convey information in a succinct manner (a “Covid vaccine” is a “vaccine that immunizes against the Covid disease”). These are commonly used in short-form domains, such as news headlines, but are largely ignored in information-seeking applications. To... | Gabriel Stanovsky, Keshav Kolluru, Mausam |  |
| 1367 |  |  [Context Limitations Make Neural Language Models More Human-Like](https://doi.org/10.18653/v1/2022.emnlp-main.712) |  | 0 | Language models (LMs) have been used in cognitive modeling as well as engineering studies—they compute information-theoretic complexity metrics that simulate humans’ cognitive load during reading.This study highlights a limitation of modern neural LMs as the model of choice for this purpose: there... | Ana Brassard, Kentaro Inui, Tatsuki Kuribayashi, Yohei Oseki |  |
| 1368 |  |  [A Generative Model for End-to-End Argument Mining with Reconstructed Positional Encoding and Constrained Pointer Mechanism](https://doi.org/10.18653/v1/2022.emnlp-main.713) |  | 0 | Argument mining (AM) is a challenging task as it requires recognizing the complex argumentation structures involving multiple subtasks.To handle all subtasks of AM in an end-to-end fashion, previous works generally transform AM into a dependency parsing task.However, such methods largely require... | Bin Liang, Bing Qin, Jiachen Du, Jianzhu Bao, Min Yang, Ruifeng Xu, Yang Sun, Yuhang He |  |
| 1369 |  |  [Reflect, Not Reflex: Inference-Based Common Ground Improves Dialogue Response Quality](https://doi.org/10.18653/v1/2022.emnlp-main.714) |  | 0 | Human communication relies on common ground (CG), the mutual knowledge and beliefs shared by participants, to produce coherent and interesting conversations. In this paper, we demonstrate that current response generation (RG) models produce generic and dull responses in dialogues because they act... | Bill Yuchen Lin, DongHo Lee, Hyundong Cho, Jay Pujara, Pegah Jandaghi, Pei Zhou, Xiang Ren |  |
| 1370 |  |  [FlowEval: A Consensus-Based Dialogue Evaluation Framework Using Segment Act Flows](https://doi.org/10.18653/v1/2022.emnlp-main.715) |  | 0 | Despite recent progress in open-domain dialogue evaluation, how to develop automatic metrics remains an open problem. We explore the potential of dialogue evaluation featuring dialog act information, which was hardly explicitly modeled in previous methods. However, defined at the utterance level in... | Dong Yu, Jianqiao Zhao, Liwei Wang, Michael R. Lyu, Wanyu Du, Yangfeng Ji, Yanyang Li |  |
| 1371 |  |  [FaD-VLP: Fashion Vision-and-Language Pre-training towards Unified Retrieval and Captioning](https://doi.org/10.18653/v1/2022.emnlp-main.716) |  | 0 | Multimodal tasks in the fashion domain have significant potential for e-commerce, but involve challenging vision-and-language learning problems—e.g., retrieving a fashion item given a reference image plus text feedback from a user. Prior works on multimodal fashion tasks have either been limited by... | Animesh Sinha, Licheng Yu, Mengjiao Wang, Ning Zhang, Suvir Mirchandani, Tao Xiang, Wenwen Jiang |  |
| 1372 |  |  [MM-Align: Learning Optimal Transport-based Alignment Dynamics for Fast and Accurate Inference on Missing Modality Sequences](https://doi.org/10.18653/v1/2022.emnlp-main.717) |  | 0 | Existing multimodal tasks mostly target at the complete input modality setting, i.e., each modality is either complete or completely missing in both training and test sets. However, the randomly missing situations have still been underexplored. In this paper, we present a novel approach named... | Hui Chen, MinYen Kan, Soujanya Poria, Wei Han |  |
| 1373 |  |  [Evaluating the Knowledge Dependency of Questions](https://doi.org/10.18653/v1/2022.emnlp-main.718) |  | 0 | The automatic generation of Multiple Choice Questions (MCQ) has the potential to reduce the time educators spend on student assessment significantly. However, existing evaluation metrics for MCQ generation, such as BLEU, ROUGE, and METEOR, focus on the n-gram based similarity of the generated MCQ... | Hangyeol Yu, Hyeongdon Moon, Jamin Shin, Juneyoung Park, Minsam Kim, Myeongho Jeong, Seunghyun Lee, Seungtaek Choi, Yoonseok Yang |  |
| 1374 |  |  [MoSE: Modality Split and Ensemble for Multimodal Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.emnlp-main.719) |  | 0 | Multimodal knowledge graph completion (MKGC) aims to predict missing entities in MKGs. Previous works usually share relation representation across modalities. This results in mutual interference between modalities during training, since for a pair of entities, the relation from one modality... | Guoqing Zhao, Haiwei Zhang, Ning Jiang, Xiangrui Cai, Yike Wu, Ying Zhang, Yu Zhao |  |
| 1375 |  |  [Entropy-Based Vocabulary Substitution for Incremental Learning in Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.720) |  | 0 | In a practical real-world scenario, the longstanding goal is that a universal multilingual translation model can be incrementally updated when new language pairs arrive. Specifically, the initial vocabulary only covers some of the words in new languages, which hurts the translation quality for... | Jin Ma, Kaiyu Huang, Peng Li, Yang Liu |  |
| 1376 |  |  [Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation](https://doi.org/10.18653/v1/2022.emnlp-main.721) |  | 0 | Recent advances in large-scale pre-training provide large models with the potential to learn knowledge from the raw text. It is thus natural to ask whether it is possible to leverage these large models as knowledge bases for downstream tasks. In this work, we answer the aforementioned question in... | Jianqiao Zhao, Liwei Wang, Michael R. Lyu, Yanyang Li |  |
| 1377 |  |  [An Unsupervised, Geometric and Syntax-aware Quantification of Polysemy](https://doi.org/10.18653/v1/2022.emnlp-main.722) |  | 0 | Polysemy is the phenomenon where a single word form possesses two or more related senses. It is an extremely ubiquitous part of natural language and analyzing it has sparked rich discussions in the linguistics, psychology and philosophy communities alike. With scarce attention paid to polysemy in... | Anmol Goel, Charu Sharma, Ponnurangam Kumaraguru |  |
| 1378 |  |  [Reorder and then Parse, Fast and Accurate Discontinuous Constituency Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.723) |  | 0 | Discontinuous constituency parsing is still kept developing for its efficiency and accuracy are far behind its continuous counterparts. Motivated by the observation that a discontinuous constituent tree can be simply transformed into a pseudo-continuous one by artificially reordering words in the... | Hai Zhao, Kailai Sun, Zuchao Li |  |
| 1379 |  |  [Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature](https://doi.org/10.18653/v1/2022.emnlp-main.724) |  | 0 | Lay summarisation aims to jointly summarise and simplify a given text, thus making its content more comprehensible to non-experts.Automatic approaches for lay summarisation can provide significant value in broadening access to scientific literature, enabling a greater degree of both... | Carolina Scarton, Chenghua Lin, Tomas Goldsack, Zhihao Zhang |  |
| 1380 |  |  [Looking at the Overlooked: An Analysis on the Word-Overlap Bias in Natural Language Inference](https://doi.org/10.18653/v1/2022.emnlp-main.725) |  | 0 | It has been shown that NLI models are usually biased with respect to the word-overlap between the premise and the hypothesis, as they take this feature as a primary cue for predicting the entailment label. In this paper, we focus on an overlooked aspect of the overlap bias in the NLI models: the... | Mohammad Taher Pilehvar, Sara Rajaee, Yadollah Yaghoobzadeh |  |
| 1381 |  |  [An Empirical Study on the Transferability of Transformer Modules in Parameter-efficient Fine-tuning](https://doi.org/10.18653/v1/2022.emnlp-main.726) |  | 0 | Parameter-efficient fine-tuning has garnered lots of attention in recent studies.On this subject, we investigate the capability of different transformer modules in transferring knowledge from a pre-trained model to a downstream task. Our empirical results suggest that every transformer module is a... | Mohammad AkbarTajari, Mohammad Taher Pilehvar, Sara Rajaee |  |
| 1382 |  |  [CODER: An efficient framework for improving retrieval through COntextual Document Embedding Reranking](https://doi.org/10.18653/v1/2022.emnlp-main.727) |  | 0 | Contrastive learning has been the dominant approach to training dense retrieval models. In this work, we investigate the impact of ranking context - an often overlooked aspect of learning dense retrieval models. In particular, we examine the effect of its constituent parts: jointly scoring a large... | Carsten Eickhoff, Daniel Cohen, George Zerveas, Navid Rekabsaz |  |
| 1383 |  |  [AdapterShare: Task Correlation Modeling with Adapter Differentiation](https://doi.org/10.18653/v1/2022.emnlp-main.728) |  | 0 | Thanks to the development of pre-trained language models, multitask learning (MTL) methods achieve a great success in natural language understanding area.However, current MTL methods pay more attention to task selection or model design to fuse as much knowledge as possible, while intrinsic task... | Bei Chen, JianGuang Lou, Kai Yu, Lu Chen, Zhi Chen |  |
| 1384 |  |  [Rethinking Task-Specific Knowledge Distillation: Contextualized Corpus as Better Textbook](https://doi.org/10.18653/v1/2022.emnlp-main.729) |  | 0 | Knowledge distillation has been proven effective when customizing small language models for specific tasks. Here, a corpus as ‘textbook’ plays an indispensable role, only through which the teacher can teach the student. Prevailing methods adopt a two-stage distillation paradigm: general... | Chang Liu, Chongyang Tao, Dongyan Zhao, Jianxin Liang, Jiazhan Feng, Quzhe Huang, Tao Shen |  |
| 1385 |  |  [Recovering Gold from Black Sand: Multilingual Dense Passage Retrieval with Hard and False Negative Samples](https://doi.org/10.18653/v1/2022.emnlp-main.730) |  | 0 | Negative samples have not been efficiently explored in multilingual dense passage retrieval. In this paper, we propose a novel multilingual dense passage retrieval framework, mHFN, to recover and utilize hard and false negative samples. mHFN consists of three key components: 1) a multilingual hard... | Deyi Xiong, Ming Zhou, Mingtong Liu, Tianhao Shen |  |
| 1386 |  |  [The "Problem" of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.731) |  | 0 | Human variation in labeling is often considered noise. Annotation projects for machine learning (ML) aim at minimizing human label variation, with the assumption to maximize data quality and in turn optimize and maximize machine learning metrics. However, thisconventional practice assumes that... | Barbara Plank |  |
| 1387 |  |  [Quality Scoring of Source Words in Neural Translation Models](https://doi.org/10.18653/v1/2022.emnlp-main.732) |  | 0 | Word-level quality scores on input source sentences can provide useful feedback to an end-user when translating into an unfamiliar target language. Recent approaches either require training special word-scoring models based on synthetic data or require repeated invocation of the translation model.... | Priyesh Jain, Sunita Sarawagi, Tushar Tomar |  |
| 1388 |  |  [Pneg: Prompt-based Negative Response Generation for Dialogue Response Selection Task](https://doi.org/10.18653/v1/2022.emnlp-main.733) |  | 0 | In retrieval-based dialogue systems, a response selection model acts as a ranker to select the most appropriate response among several candidates. However, such selection models tend to rely on context-response content similarity, which makes models vulnerable to adversarial responses that are... | ChaeHun Park, HoJin Choi, Jaegul Choo, Nyoungwoo Lee |  |
| 1389 |  |  [Facilitating Contrastive Learning of Discourse Relational Senses by Exploiting the Hierarchy of Sense Relations](https://doi.org/10.18653/v1/2022.emnlp-main.734) |  | 0 | Implicit discourse relation recognition is a challenging task that involves identifying the sense or senses that hold between two adjacent spans of text, in the absense of an explicit connective between them. In both PDTB-2 (prasad et al., 2008) and PDTB-3 (Webber et al., 2019), discourse... | Bonnie Webber, Wanqiu Long |  |
| 1390 |  |  [Simplified Graph Learning for Inductive Short Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.735) |  | 0 | Short text classification (STC) is hard as short texts lack context information and labeled data is not enough. Graph neural networks obtain the state-of-the-art on STC since they can merge various auxiliary information via the message passing framework. However, existing works conduct transductive... | Dejing Dou, Kaixin Zheng, Quanming Yao, Yaqing Wang |  |
| 1391 |  |  [Don't Stop Fine-Tuning: On Training Regimes for Few-Shot Cross-Lingual Transfer with Multilingual Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.736) |  | 0 | A large body of recent work highlights the fallacies of zero-shot cross-lingual transfer (ZS-XLT) with large multilingual language models. Namely, their performance varies substantially for different target languages and is the weakest where needed the most: for low-resource languages distant to... | Fabian David Schmidt, Goran Glavas, Ivan Vulic |  |
| 1392 |  |  [Towards Compositional Generalization in Code Search](https://doi.org/10.18653/v1/2022.emnlp-main.737) |  | 0 | We study compositional generalization, which aims to generalize on unseen combinations of seen structural elements, for code search. Unlike existing approaches of partially pursuing this goal, we study how to extract structural elements, which we name a template that directly targets compositional... | Hojae Han, Nan Duan, Seungtaek Choi, Seungwon Hwang, Shuai Lu |  |
| 1393 |  |  [Towards relation extraction from speech](https://doi.org/10.18653/v1/2022.emnlp-main.738) |  | 0 | Relation extraction typically aims to extract semantic relationships between entities from the unstructured text.One of the most essential data sources for relation extraction is the spoken language, such as interviews and dialogues.However, the error propagation introduced in automatic speech... | Gholamreza Haffari, Guilin Qi, Guitao Wang, Jinming Zhao, Tongtong Wu, YuanFang Li, Zhaoran Liu |  |
| 1394 |  |  [Structural Constraints and Natural Language Inference for End-to-End Flowchart Grounded Dialog Response Generation](https://doi.org/10.18653/v1/2022.emnlp-main.739) |  | 0 | Flowchart grounded dialog systems converse with users by following a given flowchart and a corpus of FAQs. The existing state-of-the-art approach (Raghu et al, 2021) for learning such a dialog system, named FLONET, has two main limitations. (1) It uses a Retrieval Augmented Generation (RAG)... | Dinesh Raghu, Mausam, Sachindra Joshi, Suraj Joshi |  |
| 1395 |  |  [SLICER: Sliced Fine-Tuning for Low-Resource Cross-Lingual Transfer for Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.740) |  | 0 | Large multilingual language models generally demonstrate impressive results in zero-shot cross-lingual transfer, yet often fail to successfully transfer to low-resource languages, even for token-level prediction tasks like named entity recognition (NER). In this work, we introduce a simple yet... | Fabian David Schmidt, Goran Glavas, Ivan Vulic |  |
| 1396 |  |  [EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation](https://doi.org/10.18653/v1/2022.emnlp-main.741) |  | 0 | We introduce EdgeFormer – a parameter-efficient Transformer for on-device seq2seq generation under the strict computation and memory constraints. Compared with the previous parameter-efficient Transformers, EdgeFormer applies two novel principles for cost-effective parameterization, allowing it to... | Furu Wei, SiQing Chen, Tao Ge |  |
| 1397 |  |  [End-to-End Unsupervised Vision-and-Language Pre-training with Referring Expression Matching](https://doi.org/10.18653/v1/2022.emnlp-main.742) |  | 0 | Recently there has been an emerging interest in unsupervised vision-and-language pre-training (VLP) that learns multimodal representations without parallel image-caption data. These pioneering works significantly reduce the cost of VLP on data collection and achieve promising results compared to... | Chi Chen, Maosong Sun, Peng Li, Yang Liu |  |
| 1398 |  |  [Faithful Knowledge Graph Explanations in Commonsense Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.743) |  | 0 | Knowledge graphs are commonly used as sources of information in commonsense question answering, and can also be used to express explanations for the model’s answer choice. A common way of incorporating facts from the graph is to encode them separately from the question, and then combine the two... | Guy Aglionby, Simone Teufel |  |
| 1399 |  |  [KOLD: Korean Offensive Language Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.744) |  | 0 | Recent directions for offensive language detection are hierarchical modeling, identifying the type and the target of offensive language, and interpretability with offensive span annotation and prediction. These improvements are focused on English and do not transfer well to other languages because... | Alice Oh, Jaimeen Ahn, Jihyung Moon, Jongwon Lee, Juhyun Oh, Sungjoon Park, Younghoon Jeong |  |
| 1400 |  |  [Evade the Trap of Mediocrity: Promoting Diversity and Novelty in Text Generation via Concentrating Attention](https://doi.org/10.18653/v1/2022.emnlp-main.745) |  | 0 | Recently, powerful Transformer architectures have proven superior in generating high-quality sentences. Nevertheless, these models tend to produce dull high-frequency phrases, severely hurting the diversity and novelty of generated text. In this work, we dig into the intrinsic mechanism of this... | Jinyi Hu, Maosong Sun, Wenhao Li, Xiaoyuan Yi, Xing Xie |  |
| 1401 |  |  [The better your Syntax, the better your Semantics? Probing Pretrained Language Models for the English Comparative Correlative](https://doi.org/10.18653/v1/2022.emnlp-main.746) |  | 0 | Construction Grammar (CxG) is a paradigm from cognitive linguistics emphasising the connection between syntax and semantics. Rather than rules that operate on lexical items, it posits constructions as the central building blocks of language, i.e., linguistic units of different granularity that... | Abdullatif Köksal, Hinrich Schütze, Leonie Weissweiler, Valentin Hofmann |  |
| 1402 |  |  [ProofInfer: Generating Proof via Iterative Hierarchical Inference](https://doi.org/10.18653/v1/2022.emnlp-main.747) |  | 0 | Proof generation focuses on deductive reasoning: given a hypothesis and a set of theories, including some supporting facts and logical rules expressed in natural language, the model generates a proof tree indicating how to deduce the hypothesis from given theories.Current models with... | Qi Zhang, Tao Gui, Xin Zhou, Xuanjing Huang, Zichu Fei |  |
| 1403 |  |  [ECTSum: A New Benchmark Dataset For Bullet Point Summarization of Long Earnings Call Transcripts](https://doi.org/10.18653/v1/2022.emnlp-main.748) |  | 0 | Despite tremendous progress in automatic summarization, state-of-the-art methods are predominantly trained to excel in summarizing short newswire articles, or documents with strong layout biases such as scientific articles or government reports. Efficient techniques to summarize financial... | Abhinav Bohra, Afreen Shaikh, Akash Banerjee, Koustuv Dasgupta, Manjunath Hegde, Niloy Ganguly, Pawan Goyal, Rajdeep Mukherjee, Saptarshi Ghosh, Shivani Shrivastava, Soumya Sharma |  |
| 1404 |  |  [Cross-domain Generalization for AMR Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.749) |  | 0 | Abstract Meaning Representation (AMR) parsing aims to predict an AMR graph from textual input. Recently, there has been notable growth in AMR parsing performance. However, most existing work focuses on improving the performance in the specific domain, ignoring the potential domain dependence of AMR... | Leyang Cui, Linfeng Song, Sen Yang, Xuefeng Bai, Yue Zhang |  |
| 1405 |  |  [CiteSum: Citation Text-guided Scientific Extreme Summarization and Domain Adaptation with Limited Supervision](https://doi.org/10.18653/v1/2022.emnlp-main.750) |  | 0 | Scientific extreme summarization (TLDR) aims to form ultra-short summaries of scientific papers. Previous efforts on curating scientific TLDR datasets failed to scale up due to the heavy human annotation and domain expertise required. In this paper, we propose a simple yet effective approach to... | Jiawei Han, Ming Zhong, Yuning Mao |  |
| 1406 |  |  [FETA: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue](https://doi.org/10.18653/v1/2022.emnlp-main.751) |  | 0 | Task transfer, transferring knowledge contained in related tasks, holds the promise of reducing the quantity of labeled data required to fine-tune language models. Dialogue understanding encompasses many diverse tasks, yet task transfer has not been thoroughly studied in conversational AI. This... | Alon Albalak, Connor Pryor, Deepak Ramachandran, Jay Pujara, Lise Getoor, Luke Yoffe, Pegah Jandaghi, William Yang Wang, YiLin Tuan |  |
| 1407 |  |  [Do Children Texts Hold The Key To Commonsense Knowledge?](https://doi.org/10.18653/v1/2022.emnlp-main.752) |  | 0 | Compiling comprehensive repositories of commonsense knowledge is a long-standing problem in AI. Many concerns revolve around the issue of reporting bias, i.e., that frequency in text sources is not a good proxy for relevance or truth. This paper explores whether children’s texts hold the key to... | Julien Romero, Simon Razniewski |  |
| 1408 |  |  [On the Limitations of Reference-Free Evaluations of Generated Text](https://doi.org/10.18653/v1/2022.emnlp-main.753) |  | 0 | There is significant interest in developing evaluation metrics which accurately estimate the quality of generated text without the aid of a human-written reference text, which can be time consuming and expensive to collect or entirely unavailable in online applications. However, in this work, we... | Dan Roth, Daniel Deutsch, Rotem Dror |  |
| 1409 |  |  [Sampling-Based Approximations to Minimum Bayes Risk Decoding for Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.754) |  | 0 | In NMT we search for the mode of the model distribution to form predictions. The mode and other high-probability translations found by beam search have been shown to often be inadequate in a number of ways. This prevents improving translation quality through better search, as these idiosyncratic... | Bryan Eikema, Wilker Aziz |  |
| 1410 |  |  [IndicXNLI: Evaluating Multilingual Inference for Indian Languages](https://doi.org/10.18653/v1/2022.emnlp-main.755) |  | 0 | While Indic NLP has made rapid advances recently in terms of the availability of corpora and pre-trained models, benchmark datasets on standard NLU tasks are limited. To this end, we introduce INDICXNLI, an NLI dataset for 11 Indic languages. It has been created by high-quality machine translation... | Anoop Kunchukuttan, Divyanshu Aggarwal, Vivek Gupta |  |
| 1411 |  |  [Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems](https://doi.org/10.18653/v1/2022.emnlp-main.756) |  | 0 | Do all instances need inference through the big models for a correct prediction? Perhaps not; some instances are easy and can be answered correctly by even small capacity models. This provides opportunities for improving the computational efficiency of systems. In this work, we present an... | Chitta Baral, Neeraj Varshney |  |
| 1412 |  |  [Semantic Simplification for Sentiment Classification](https://doi.org/10.18653/v1/2022.emnlp-main.757) |  | 0 | Recent work on document-level sentiment classification has shown that the sentiment in the original text is often hard to capture, since the sentiment is usually either expressed implicitly or shifted due to the occurrences of negation and rhetorical words. To this end, we enhance the original text... | Guodong Zhou, Xiaotong Jiang, Zhongqing Wang |  |
| 1413 |  |  [XPrompt: Exploring the Extreme of Prompt Tuning](https://doi.org/10.18653/v1/2022.emnlp-main.758) |  | 0 | Prompt tuning learns soft prompts to condition the frozen Pre-trained Language Models (PLMs) for performing downstream tasks in a parameter-efficient manner. While prompt tuning has gradually reached the performance level of fine-tuning as the model scale increases, there is still a large... | Chen Zhang, Dawei Song, Fang Ma, Jingang Wang, Lei Ren, Qifan Wang, Wei Wu, Xiaojun Quan |  |
| 1414 |  |  [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://doi.org/10.18653/v1/2022.emnlp-main.759) |  | 0 | Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the... | Ari Holtzman, Hannaneh Hajishirzi, Luke Zettlemoyer, Mike Lewis, Mikel Artetxe, Sewon Min, Xinxi Lyu |  |
| 1415 |  |  [The Curious Case of Control](https://doi.org/10.18653/v1/2022.emnlp-main.760) |  | 0 | Children acquiring English make systematic errors on subject control sentences even after they have reached near-adult competence (Chomsky, 1969), possibly due to heuristics based on semantic roles (Maratsos, 1974).Given the advanced fluency of large generative language models, we ask whether model... | Benjamin Van Durme, Elias StengelEskin |  |
| 1416 |  |  [SHARE: a System for Hierarchical Assistive Recipe Editing](https://doi.org/10.18653/v1/2022.emnlp-main.761) |  | 0 | The large population of home cooks with dietary restrictions is under-served by existing cooking resources and recipe generation models. To help them, we propose the task of controllable recipe editing: adapt a base recipe to satisfy a user-specified dietary constraint. This task is challenging,... | Jianmo Ni, Julian J. McAuley, Shuyang Li, Yufei Li |  |
| 1417 |  |  [IM⌃2: an Interpretable and Multi-category Integrated Metric Framework for Automatic Dialogue Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.762) |  | 0 | Evaluation metrics shine the light on the best models and thus strongly influence the research directions, such as the recently developed dialogue metrics USR, FED, and GRADE. However, most current metrics evaluate the dialogue data as isolated and static because they only focus on a single quality... | Di Wang, Dongning Rao, Guanghui Ye, Xin Miao, Zhihua Jiang |  |
| 1418 |  |  [PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models](https://doi.org/10.18653/v1/2022.emnlp-main.763) |  | 0 | Vision-language pre-training (VLP) has shown impressive performance on a wide range of cross-modal tasks, where VLP models without reliance on object detectors are becoming the mainstream due to their superior computation efficiency and competitive performance. However, the removal of object... | Ao Zhang, Maosong Sun, Qianyu Chen, TatSeng Chua, Wei Ji, Yuan Yao, Zhiyuan Liu |  |
| 1419 |  |  [Pre-training Language Models with Deterministic Factual Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.764) |  | 0 | Previous works show that Pre-trained Language Models (PLMs) can capture factual knowledge. However, some analyses reveal that PLMs fail to perform it robustly, e.g., being sensitive to the changes of prompts when extracting factual knowledge. To mitigate this issue, we propose to let PLMs learn the... | Bingquan Liu, Chengjie Sun, Lifeng Shang, Qun Liu, Shaobo Li, Xiaoguang Li, Xin Jiang, Zhenzhou Ji |  |
| 1420 |  |  [Finding Skill Neurons in Pre-trained Transformer-based Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.765) |  | 0 | Transformer-based pre-trained language models have demonstrated superior performance on various natural language processing tasks. However, it remains unclear how the skills required to handle these tasks distribute among model parameters. In this paper, we find that after prompt tuning for... | Juanzi Li, Kaiyue Wen, Lei Hou, Xiaozhi Wang, Zhengyan Zhang, Zhiyuan Liu |  |
| 1421 |  |  [Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue](https://doi.org/10.18653/v1/2022.emnlp-main.766) |  | 0 | Lifelong learning (LL) is vital for advanced task-oriented dialogue (ToD) systems. To address the catastrophic forgetting issue of LL, generative replay methods are widely employed to consolidate past knowledge with generated pseudo samples. However, most existing generative replay methods use only... | Chang Gao, Jian Sun, Nevin L. Zhang, Yingxiu Zhao, Yinhe Zheng, Zhiliang Tian |  |
| 1422 |  |  [PreQuEL: Quality Estimation of Machine Translation Outputs in Advance](https://doi.org/10.18653/v1/2022.emnlp-main.767) |  | 0 | We present the task of PreQuEL, Pre-(Quality-Estimation) Learning. A PreQuEL system predicts how well a given sentence will be translated, without recourse to the actual translation, thus eschewing unnecessary resource allocation when translation quality is bound to be low. PreQuEL can be defined... | Leshem Choshen, Omri Abend, Shachar DonYehiya |  |
| 1423 |  |  [Can Transformers Reason in Fragments of Natural Language?](https://doi.org/10.18653/v1/2022.emnlp-main.768) |  | 0 | State-of-the-art deep-learning-based approaches to Natural Language Processing (NLP) are credited with various capabilities that involve reasoning with natural language texts. %However, reasoning in this setting is often ill-defined and shallow. In this paper we carry out a large-scale empirical... | Ian PrattHartmann, Kamen V. Pavlov, Viktor Schlegel |  |
| 1424 |  |  [Textless Speech Emotion Conversion using Discrete & Decomposed Representations](https://doi.org/10.18653/v1/2022.emnlp-main.769) |  | 0 | Speech emotion conversion is the task of modifying the perceived emotion of a speech utterance while preserving the lexical content and speaker identity. In this study, we cast the problem of emotion conversion as a spoken language translation task. We use a decomposition of the speech signal into... | Abdelrahman Mohamed, Adam Polyak, Emmanuel Dupoux, Eugene Kharitonov, Felix Kreuk, Jade Copet, Morgane Rivière, Tu Anh Nguyen, WeiNing Hsu, Yossi Adi |  |
| 1425 |  |  [Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks](https://doi.org/10.18653/v1/2022.emnlp-main.770) |  | 0 | Backdoor attacks are a kind of emergent security threat in deep learning. After being injected with a backdoor, a deep neural model will behave normally on standard inputs but give adversary-specified predictions once the input contains specific backdoor triggers. In this paper, we find two simple... | Fanchao Qi, Hongcheng Gao, Maosong Sun, Yangyi Chen, Zhiyuan Liu |  |
| 1426 |  |  [Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP](https://doi.org/10.18653/v1/2022.emnlp-main.771) |  | 0 | Textual adversarial samples play important roles in multiple subfields of NLP research, including security, evaluation, explainability, and data augmentation. However, most work mixes all these roles, obscuring the problem definitions and research goals of the security role that aims to reveal the... | Fanchao Qi, Ganqu Cui, Hongcheng Gao, Longtao Huang, Maosong Sun, Yangyi Chen, Zhiyuan Liu |  |
| 1427 |  |  [Retrieval Augmented Visual Question Answering with Outside Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.772) |  | 0 | Outside-Knowledge Visual Question Answering (OK-VQA) is a challenging VQA task that requires retrieval of external knowledge to answer questions about images. Recent OK-VQA systems use Dense Passage Retrieval (DPR) to retrieve documents from external knowledge bases, such as Wikipedia, but with DPR... | Bill Byrne, Weizhe Lin |  |
| 1428 |  |  [Instance Regularization for Discriminative Language Model Pre-training](https://doi.org/10.18653/v1/2022.emnlp-main.773) |  | 0 | Discriminative pre-trained language models (PrLMs) can be generalized as denoising auto-encoders that work with two procedures, ennoising and denoising. First, an ennoising process corrupts texts with arbitrary noising functions to construct training instances. Then, a denoising language model is... | Hai Zhao, Ming Zhou, Zhuosheng Zhang |  |
| 1429 |  |  [GuoFeng: A Benchmark for Zero Pronoun Recovery and Translation](https://doi.org/10.18653/v1/2022.emnlp-main.774) |  | 0 | The phenomenon of zero pronoun (ZP) has attracted increasing interest in the machine translation (MT) community due to its importance and difficulty. However, previous studies generally evaluate the quality of translating ZPs with BLEU scores on MT testsets, which is not expressive or sensitive... | Derek F. Wong, Hongye Liu, Lidia S. Chao, Linfeng Song, Longyue Wang, Mingzhou Xu, Shuming Shi, Zhaopeng Tu |  |
| 1430 |  |  [ScienceWorld: Is your Agent Smarter than a 5th Grader?](https://doi.org/10.18653/v1/2022.emnlp-main.775) |  | 0 | We present ScienceWorld, a benchmark to test agents’ scientific reasoning abilities in a new interactive text environment at the level of a standard elementary school science curriculum. Despite the transformer-based progress seen in question-answering and scientific text processing, we find that... | MarcAlexandre Côté, Peter A. Jansen, Prithviraj Ammanabrolu, Ruoyao Wang |  |
| 1431 |  |  [Improving Embeddings Representations for Comparing Higher Education Curricula: A Use Case in Computing](https://doi.org/10.18653/v1/2022.emnlp-main.776) |  | 0 | We propose an approach for comparing curricula of study programs in higher education. Pre-trained word embeddings are fine-tuned in a study program classification task, where each curriculum is represented by the names and content of its courses. By combining metric learning with a novel... | Fernando AlvaManchego, Jeffri MurrugarraLlerena, Nils MurrugarraLlerena |  |
| 1432 |  |  [Mitigating Spurious Correlation in Natural Language Understanding with Counterfactual Inference](https://doi.org/10.18653/v1/2022.emnlp-main.777) |  | 0 | Despite their promising results on standard benchmarks, NLU models are still prone to make predictions based on shortcuts caused by unintended bias in the dataset. For example, an NLI model may use lexical overlap as a shortcut to make entailment predictions due to repetitive data generation... | Can Udomcharoenchaikit, Ekapol Chuangsuwanich, Kanruethai Masuk, Patomporn Payoungkhamdee, Sarana Nutanong, Weerayut Buaphet, Wuttikorn Ponwitayarat |  |
| 1433 |  |  [End-to-End Neural Discourse Deixis Resolution in Dialogue](https://doi.org/10.18653/v1/2022.emnlp-main.778) |  | 0 | We adapt Lee et al.’s (2018) span-based entity coreference model to the task of end-to-end discourse deixis resolution in dialogue, specifically by proposing extensions to their model that exploit task-specific characteristics. The resulting model, dd-utt, achieves state-of-the-art results on the... | Shengjie Li, Vincent Ng |  |
| 1434 |  |  [Balancing out Bias: Achieving Fairness Through Balanced Training](https://doi.org/10.18653/v1/2022.emnlp-main.779) |  | 0 | Group bias in natural language processing tasks manifests as disparities in system error rates across texts authorized by different demographic groups, typically disadvantaging minority groups. Dataset balancing has been shown to be effective at mitigating bias, however existing approaches do not... | Timothy Baldwin, Trevor Cohn, Xudong Han |  |
| 1435 |  |  [Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models](https://doi.org/10.18653/v1/2022.emnlp-main.780) |  | 0 | Pre-trained masked language models successfully perform few-shot learning by formulating downstream tasks as text infilling. How- ever, as a strong alternative in full-shot settings, discriminative pre-trained models like ELECTRA do not fit into the paradigm. In this work, we adapt prompt-based... | Danqi Chen, Jingfei Du, Mengzhou Xia, Mikel Artetxe, Veselin Stoyanov |  |
| 1436 |  |  [Identifying Physical Object Use in Sentences](https://doi.org/10.18653/v1/2022.emnlp-main.781) |  | 0 | Commonsense knowledge about the typicalfunctions of physical objects allows people tomake inferences during sentence understanding.For example, we infer that “Sam enjoyedthe book” means that Sam enjoyed reading thebook, even though the action is implicit. Priorresearch has focused on learning the... | Ellen Riloff, Tianyu Jiang |  |
| 1437 |  |  [CDialog: A Multi-turn Covid-19 Conversation Dataset for Entity-Aware Dialog Generation](https://doi.org/10.18653/v1/2022.emnlp-main.782) |  | 0 |  | Aizan Zafar, Asif Ekbal, Deeksha Varshney, Niranshu Kumar Behra |  |
| 1438 |  |  [Robustifying Sentiment Classification by Maximally Exploiting Few Counterfactuals](https://doi.org/10.18653/v1/2022.emnlp-main.783) |  | 0 | For text classification tasks, finetuned language models perform remarkably well. Yet, they tend to rely on spurious patterns in training data, thus limiting their performance on out-of-distribution (OOD) test data. Among recent models aiming to avoid this spurious pattern problem, adding extra... | Chris Develder, Fréderic Godin, Maarten De Raedt, Thomas Demeester |  |
| 1439 |  |  [Data-Efficient Playlist Captioning With Musical and Linguistic Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.784) |  | 0 | Music streaming services feature billions of playlists created by users, professional editors or algorithms. In this content overload scenario, it is crucial to characterise playlists, so that music can be effectively organised and accessed. Playlist titles and descriptions are proposed in natural... | Elena V. Epure, Giovanni Gabbolini, Romain Hennequin |  |
| 1440 |  |  [Improved grammatical error correction by ranking elementary edits](https://doi.org/10.18653/v1/2022.emnlp-main.785) |  | 0 | We offer a two-stage reranking method for grammatical error correction: the first model serves as edit generator, while the second classifies the proposed edits as correct or false. We show how to use both encoder-decoder and sequence labeling models for the first step of our pipeline. We achieve... | Alexey Sorokin |  |
| 1441 |  |  [Improving Tokenisation by Alternative Treatment of Spaces](https://doi.org/10.18653/v1/2022.emnlp-main.786) |  | 0 |  | Aline Villavicencio, Carolina Scarton, Edward GowSmith, Harish Tayyar Madabushi |  |
| 1442 |  |  [GENIE: Toward Reproducible and Standardized Human Evaluation for Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.787) |  | 0 | While often assumed a gold standard, effective human evaluation of text generation remains an important, open area for research.We revisit this problem with a focus on producing consistent evaluations that are reproducible—over time and across different populations. We study this goal in different... | Daniel Khashabi, Daniel S. Weld, Gabriel Stanovsky, Jonathan Bragg, Jungo Kasai, Nicholas Lourie, Noah A. Smith, Yejin Choi |  |
| 1443 |  |  [Attentional Probe: Estimating a Module's Functional Potential](https://doi.org/10.18653/v1/2022.emnlp-main.788) |  | 0 | In this paper, we seek to measure how much information a component in a neural network could extract from the representations fed into it. Our work stands in contrast to prior probing work, most of which investigates how much information a model's representations contain. This shift in perspective... | Josef Valvoda, Niklas Stoehr, Ryan Cotterell, Tiago Pimentel |  |
| 1444 |  |  [When More Data Hurts: A Troubling Quirk in Developing Broad-Coverage Natural Language Understanding Systems](https://doi.org/10.18653/v1/2022.emnlp-main.789) |  | 0 | In natural language understanding (NLU) production systems, users’ evolving needs necessitate the addition of new features over time, indexed by new symbols added to the meaning representation space. This requires additional training data and results in ever-growing datasets. We present the first... | Adam Pauls, Benjamin Van Durme, Elias StengelEskin, Emmanouil Antonios Platanios, Hao Fang, Jason Eisner, Sam Thomson, Yu Su |  |
| 1445 |  |  [Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt](https://doi.org/10.18653/v1/2022.emnlp-main.790) |  | 0 | Prompt-based tuning has been proven effective for pretrained language models (PLMs). While most of the existing work focuses on the monolingual prompts, we study the multilingual prompts for multilingual PLMs, especially in the zero-shot cross-lingual setting. To alleviate the effort of designing... | Dongdong Zhang, Furu Wei, Houfeng Wang, Lianzhe Huang, Shuming Ma |  |
| 1446 |  |  [Three Real-World Datasets and Neural Computational Models for Classification Tasks in Patent Landscaping](https://doi.org/10.18653/v1/2022.emnlp-main.791) |  | 0 | Patent Landscaping, one of the central tasks of intellectual property management, includes selecting and grouping patents according to user-defined technical or application-oriented criteria. While recent transformer-based models have been shown to be effective for classifying patents into... | Annemarie Friedrich, Jannik Strötgen, Mark Giereth, Michael Gertz, Subhash Chandra Pujari |  |
| 1447 |  |  [Topic Modeling With Topological Data Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.792) |  | 0 | Recent unsupervised topic modelling ap-proaches that use clustering techniques onword, token or document embeddings can ex-tract coherent topics. A common limitationof such approaches is that they reveal noth-ing about inter-topic relationships which areessential in many real-world application... | Amandla Mabona, Ciarán Byrne, Danijela Horak, Karo Moilanen |  |
| 1448 |  |  [Predicting Fine-Tuning Performance with Probing](https://doi.org/10.18653/v1/2022.emnlp-main.793) |  | 0 | Large NLP models have recently shown impressive performance in language understanding tasks, typically evaluated by their fine-tuned performance. Alternatively, probing has received increasing attention as being a lightweight method for interpreting the intrinsic mechanisms of large NLP models. In... | Frank Rudzicz, Soroosh Shahtalebi, Zining Zhu |  |
| 1449 |  |  [Diverse Parallel Data Synthesis for Cross-Database Adaptation of Text-to-SQL Parsers](https://doi.org/10.18653/v1/2022.emnlp-main.794) |  | 0 | Text-to-SQL parsers typically struggle with databases unseen during the train time. Adapting Text-to-SQL parsers to new database schemas is a challenging problem owing to a vast diversity of schemas and zero availability of natural language queries in new schemas. We present ReFill, a framework for... | Abhijeet Awasthi, Ashutosh Sathe, Sunita Sarawagi |  |
| 1450 |  |  [Agent-Specific Deontic Modality Detection in Legal Language](https://doi.org/10.18653/v1/2022.emnlp-main.795) |  | 0 | Legal documents are typically long and written in legalese, which makes it particularly difficult for laypeople to understand their rights and duties. While natural language understanding technologies can be valuable in supporting such understanding in the legal domain, the limited availability of... | Abhilasha Sancheti, Aparna Garimella, Balaji Vasan Srinivasan, Rachel Rudinger |  |
| 1451 |  |  [COLD: A Benchmark for Chinese Offensive Language Detection](https://doi.org/10.18653/v1/2022.emnlp-main.796) |  | 0 | Offensive language detection is increasingly crucial for maintaining a civilized social media platform and deploying pre-trained language models. However, this task in Chinese is still under exploration due to the scarcity of reliable datasets. To this end, we propose a benchmark –COLD for Chinese... | Chujie Zheng, Fei Mi, Hao Sun, Helen Meng, Jiawen Deng, Jingyan Zhou, Minlie Huang |  |
| 1452 |  |  [Fixing Model Bugs with Natural Language Patches](https://doi.org/10.18653/v1/2022.emnlp-main.797) |  | 0 | Current approaches for fixing systematic problems in NLP models (e.g., regex patches, finetuning on more data) are either brittle, or labor-intensive and liable to shortcuts. In contrast, humans often provide corrections to each other through natural language. Taking inspiration from this, we... | Christopher D. Manning, Marco Túlio Ribeiro, Scott M. Lundberg, Shikhar Murty |  |
| 1453 |  |  [WeDef: Weakly Supervised Backdoor Defense for Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.798) |  | 0 | Existing backdoor defense methods are only effective for limited trigger types. To defend different trigger types at once, we start from the class-irrelevant nature of the poisoning process and propose a novel weakly supervised backdoor defense framework WeDef. Recent advances in weak supervision... | Jingbo Shang, Lesheng Jin, Zihan Wang |  |
| 1454 |  |  [Interventional Training for Out-Of-Distribution Natural Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.799) |  | 0 | Out-of-distribution (OOD) settings are used to measure a model’s performance when the distribution of the test data is different from that of the training data. NLU models are known to suffer in OOD. We study this issue from the perspective of causality, which sees confounding bias as the reason... | Hao Zhang, Jing Jiang, Lidong Bing, Qianru Sun, Sicheng Yu, Yulei Niu |  |
| 1455 |  |  [Pseudo-Relevance for Enhancing Document Representation](https://doi.org/10.18653/v1/2022.emnlp-main.800) |  | 0 | This paper studies how to enhance the document representation for the bi-encoder approach in dense document retrieval. The bi-encoder, separately encoding a query and a document as a single vector, is favored for high efficiency in large-scale information retrieval, compared to more effective but... | Hyeseon Ko, Jihyuk Kim, Seoho Song, Seungwon Hwang, YoungIn Song |  |
| 1456 |  |  [ZeroGen: Efficient Zero-shot Learning via Dataset Generation](https://doi.org/10.18653/v1/2022.emnlp-main.801) |  | 0 | There is a growing interest in dataset generation recently due to the superior generative capacity of large pre-trained language models (PLMs). In this paper, we study a flexible and efficient zero-short learning method, ZeroGen.Given a zero-shot task, we first generate a dataset from scratch using... | Hang Xu, Jiacheng Ye, Jiahui Gao, Jiangtao Feng, Lingpeng Kong, Qintong Li, Tao Yu, Zhiyong Wu |  |
| 1457 |  |  [Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.802) |  | 0 | Learning scientific document representations can be substantially improved through contrastive learning objectives, where the challenge lies in creating positive and negative training samples that encode the desired similarity semantics. Prior work relies on discrete citation relations to generate... | Bela Gipp, Georg Rehm, Isabelle Augenstein, Malte Ostendorff, Nils Rethmeier |  |
| 1458 |  |  [SPE: Symmetrical Prompt Enhancement for Fact Probing](https://doi.org/10.18653/v1/2022.emnlp-main.803) |  | 0 | Pretrained language models (PLMs) have been shown to accumulate factual knowledge during pretraining (Petroni et al. 2019). Recent works probe PLMs for the extent of this knowledge through prompts either in discrete or continuous forms. However, these methods do not consider symmetry of the task:... | Caiming Xiong, Snigdha Chaturvedi, Tong Che, Yezhen Wang, Yiyuan Li, Zhengbao Jiang |  |
| 1459 |  |  [Efficient Large Scale Language Modeling with Mixtures of Experts](https://doi.org/10.18653/v1/2022.emnlp-main.804) |  | 0 | Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language... | Brian O'Horo, Giridharan Anantharaman, Halil Akin, Jeffrey Wang, Jingfei Du, Louis Martin, Luke Zettlemoyer, Mandeep Baines, Mikel Artetxe, Mona T. Diab, Myle Ott, Naman Goyal, Punit Singh Koura, Ramakanth Pasunuru, Sam Shleifer, Shruti Bhosale, Shuohui Chen, Srinivasan Iyer, Todor Mihaylov, Veselin Stoyanov, Xi Victoria Lin, Xian Li, Xing Zhou, Zornitsa Kozareva |  |
| 1460 |  |  [MedJEx: A Medical Jargon Extraction Model with Wiki's Hyperlink Span and Contextualized Masked Language Model Score](https://doi.org/10.18653/v1/2022.emnlp-main.805) |  | 0 | This paper proposes a new natural language processing (NLP) application for identifying medical jargon terms potentially difficult for patients to comprehend from electronic health record (EHR) notes. We first present a novel and publicly available dataset with expert-annotated medical jargon terms... | Brian Corner, David A. Levy, Harmon S. Jordan, Hong Yu, Sunjae Kwon, Zonghai Yao |  |
| 1461 |  |  [Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections](https://doi.org/10.18653/v1/2022.emnlp-main.806) |  | 0 | While there has been substantial progress in text comprehension through simple factoid question answering, more holistic comprehension of a discourse still presents a major challenge (Dunietz et al., 2020). Someone critically reflecting on a text as they read it will pose curiosity-driven, often... | Cutter Dalton, Eliza Fisher, Greg Durrett, Junyi Jessy Li, Mark Simmons, WeiJen Ko |  |
| 1462 |  |  [Learning to Generate Overlap Summaries through Noisy Synthetic Data](https://doi.org/10.18653/v1/2022.emnlp-main.807) |  | 0 | Semantic Overlap Summarization (SOS) is a novel and relatively under-explored seq-to-seq task which entails summarizing common information from multiple alternate narratives. One of the major challenges for solving this task is the lack of existing datasets for supervised training. To address this... | Mousumi Akter, Naman Bansal, Shubhra Kanti Karmaker Santu |  |
| 1463 |  |  [Mutual Exclusivity Training and Primitive Augmentation to Induce Compositionality](https://doi.org/10.18653/v1/2022.emnlp-main.808) |  | 0 | Recent datasets expose the lack of the systematic generalization ability in standard sequence-to-sequence models. In this work, we analyze this behavior of seq2seq models and identify two contributing factors: a lack of mutual exclusivity bias (one target sequence can only be mapped to one source... | Mohit Bansal, Xiang Zhou, Yichen Jiang |  |
| 1464 |  |  [Directions for NLP Practices Applied to Online Hate Speech Detection](https://doi.org/10.18653/v1/2022.emnlp-main.809) |  | 0 | Addressing hate speech in online spaces has been conceptualized as a classification task that uses Natural Language Processing (NLP) techniques. Through this conceptualization, the hate speech detection task has relied on common conventions and practices from NLP. For instance, inter-annotator... | Leo Wanner, Mónica Domínguez, Paula Fortuna, Zeerak Talat |  |
| 1465 |  |  [Pre-training Transformer Models with Sentence-Level Objectives for Answer Sentence Selection](https://doi.org/10.18653/v1/2022.emnlp-main.810) |  | 0 | An important task for designing QA systems is answer sentence selection (AS2): selecting the sentence containing (or constituting) the answer to a question from a set of retrieved relevant documents. In this paper, we propose three novel sentence-level transformer pre-training objectives that... | Alessandro Moschitti, Luca Di Liello, Luca Soldaini, Siddhant Garg |  |
| 1466 |  |  [OpenCQA: Open-ended Question Answering with Charts](https://doi.org/10.18653/v1/2022.emnlp-main.811) |  | 0 | Charts are very popular to analyze data and convey important insights. People often analyze visualizations to answer open-ended questions that require explanatory answers. Answering such questions are often difficult and time-consuming as it requires a lot of cognitive and perceptual efforts. To... | Enamul Hoque, Jia Qing Tan, Rixie Tiffany Ko Leong, Shafiq R. Joty, Shankar Kantharaj, Xuan Long Do |  |
| 1467 |  |  [A Systematic Investigation of Commonsense Knowledge in Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.812) |  | 0 | Language models (LMs) trained on large amounts of data have shown impressive performance on many NLP tasks under the zero-shot and few-shot setup. Here we aim to better understand the extent to which such models learn commonsense knowledge — a critical component of many NLP applications. We conduct... | Adhiguna Kuncoro, Aida Nematzadeh, Cyprien de Masson d'Autume, Jordan Hoffmann, Phil Blunsom, Xiang Lorraine Li |  |
| 1468 |  |  [Transforming Sequence Tagging Into A Seq2Seq Task](https://doi.org/10.18653/v1/2022.emnlp-main.813) |  | 0 | Pretrained, large, generative language models (LMs) have had great success in a wide range of sequence tagging and structured prediction tasks. Casting a sequence tagging task as a Seq2Seq one requires deciding the formats of the input and output sequences. However, we lack a principled... | Iftekhar Naim, Jiecao Chen, Karthik Raman, Kazuma Hashimoto, Kiran Yalasangi, Krishna Srinivasan |  |
| 1469 |  |  [CycleKQR: Unsupervised Bidirectional Keyword-Question Rewriting](https://doi.org/10.18653/v1/2022.emnlp-main.814) |  | 0 | Users expect their queries to be answered by search systems, regardless of the query’s surface form, which include keyword queries and natural questions. Natural Language Understanding (NLU) components of Search and QA systems may fail to correctly interpret semantically equivalent inputs if this... | Andrea Iovine, Anjie Fang, Besnik Fetahu, Jie Zhao, Oleg Rokhlenko, Shervin Malmasi |  |
| 1470 |  |  [Model Criticism for Long-Form Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.815) |  | 0 | Language models have demonstrated the ability to generate highly fluent text; however, it remains unclear whether their output retains coherent high-level structure (e.g., story progression). Here, we propose to apply a statistical tool, model criticism in latent space, to evaluate the high-level... | Alexander M. Rush, Volodymyr Kuleshov, Yuntian Deng |  |
| 1471 |  |  [Improving Faithfulness by Augmenting Negative Summaries from Fake Documents](https://doi.org/10.18653/v1/2022.emnlp-main.816) |  | 0 | Current abstractive summarization systems tend to hallucinate content that is unfaithful to the source document, posing a risk of misinformation. To mitigate hallucination, we must teach the model to distinguish hallucinated summaries from faithful ones. However, the commonly used maximum... | Esin Durmus, Faisal Ladhak, He He, Tianshu Wang |  |
| 1472 |  |  [Joint Completion and Alignment of Multilingual Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.817) |  | 0 | Knowledge Graph Completion (KGC) predicts missing facts in an incomplete Knowledge Graph (KG). Multilingual KGs associate entities and relations with surface forms written in different languages. An entity or relation may be associated with distinct IDs in different KGs, necessitating entity... | Harkanwar Singh, Mausam, Prachi Jain, Shubham Lohiya, Soumen Chakrabarti |  |
| 1473 |  |  [Offer a Different Perspective: Modeling the Belief Alignment of Arguments in Multi-party Debates](https://doi.org/10.18653/v1/2022.emnlp-main.818) |  | 0 | In contexts where debate and deliberation are the norm, the participants are regularly presented with new information that conflicts with their original beliefs. When required to update their beliefs (belief alignment), they may choose arguments that align with their worldview (confirmation bias).... | Hansin Ahuja, Kevin Duh, Kokil Jaidka, Niyati Chhaya, Suzanna Sia |  |
| 1474 |  |  [A Federated Approach to Predicting Emojis in Hindi Tweets](https://doi.org/10.18653/v1/2022.emnlp-main.819) |  | 0 | The use of emojis affords a visual modality to, often private, textual communication.The task of predicting emojis however provides a challenge for machine learning as emoji use tends to cluster into the frequently used and the rarely used emojis.Much of the machine learning research on emoji use... | Deep Gandhi, Jash Mehta, Karan Waghela, Lynette D'Mello, Nirali Parekh, Zeerak Talat |  |
| 1475 |  |  [Injecting Domain Knowledge in Language Models for Task-oriented Dialogue Systems](https://doi.org/10.18653/v1/2022.emnlp-main.820) |  | 0 | Pre-trained language models (PLM) have advanced the state-of-the-art across NLP applications, but lack domain-specific knowledge that does not naturally occur in pre-training data. Previous studies augmented PLMs with symbolic knowledge for different downstream NLP tasks. However, knowledge bases... | Daniele Bonadiman, Denis Emelin, Saab Mansour, Sawsan Alqahtani, Yi Zhang |  |
| 1476 |  |  [TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack](https://doi.org/10.18653/v1/2022.emnlp-main.821) |  | 0 | We present Twin Answer Sentences Attack (TASA), an adversarial attack method for question answering (QA) models that produces fluent and grammatical adversarial contexts while maintaining gold answers. Despite phenomenal progress on general adversarial attacks, few works have investigated the... | Dacheng Tao, Dianqi Li, Jun Gao, Meng Fang, Tianyi Zhou, Yibing Zhan, Yu Cao |  |
| 1477 |  |  [Improving Low-Resource Languages in Pre-Trained Multilingual Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.822) |  | 0 | Pre-trained multilingual language models are the foundation of many NLP approaches, including cross-lingual transfer solutions. However, languages with small available monolingual corpora are often not well-supported by these models leading to poor performance. We propose an unsupervised approach... | Alexander Fraser, Hossain Shaikh Saadi, Viktor Hangya |  |
| 1478 |  |  [SCROLLS: Standardized CompaRison Over Long Language Sequences](https://doi.org/10.18653/v1/2022.emnlp-main.823) |  | 0 | NLP benchmarks have largely focused on short texts, such as sentences and paragraphs, even though long texts comprise a considerable amount of natural language in the wild. We introduce SCROLLS, a suite of tasks that require reasoning over long texts. We examine existing long-text datasets, and... | Adi Haviv, Ankit Gupta, Avia Efrat, Elad Segal, Jonathan Berant, Maor Ivgi, Mor Geva, Omer Levy, Ori Yoran, Uri Shaham, Wenhan Xiong |  |
| 1479 |  |  [PAR: Political Actor Representation Learning with Social Context and Expert Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.824) |  | 0 | Modeling the ideological perspectives of political actors is an essential task in computational political science with applications in many downstream tasks. Existing approaches are generally limited to textual data and voting records, while they neglect the rich social context and valuable expert... | Minnan Luo, Ningnan Wang, Peisheng Yu, Qinghua Zheng, Shangbin Feng, Xiaojun Chang, Zhaoxuan Tan, Zilong Chen |  |
| 1480 |  |  [JDDC 2.1: A Multimodal Chinese Dialogue Dataset with Joint Tasks of Query Rewriting, Response Generation, Discourse Parsing, and Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.825) |  | 0 | The popularity of multimodal dialogue has stimulated the need for a new generation of dialogue agents with multimodal interactivity.When users communicate with customer service, they may express their requirements by means of text, images, or even videos. Visual information usually acts as... | Haoran Li, Nan Zhao, Xiaodong He, Youzheng Wu |  |
| 1481 |  |  [PCL: Peer-Contrastive Learning with Diverse Augmentations for Unsupervised Sentence Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.826) |  | 0 | Learning sentence embeddings in an unsupervised manner is fundamental in natural language processing. Recent common practice is to couple pre-trained language models with unsupervised contrastive learning, whose success relies on augmenting a sentence with a semantically-close positive instance to... | Can Xu, Chongyang Tao, Daxin Jiang, Qiyu Wu, Tao Shen, Xiubo Geng |  |
| 1482 |  |  [Digging Errors in NMT: Evaluating and Understanding Model Errors from Partial Hypothesis Space](https://doi.org/10.18653/v1/2022.emnlp-main.827) |  | 0 | Solid evaluation of neural machine translation (NMT) is key to its understanding and improvement. Current evaluation of an NMT system is usually built upon a heuristic decoding algorithm (e.g., beam search) and an evaluation metric assessing similarity between the translation and golden reference.... | Chenming Wu, Fandong Meng, Jianhao Yan, Jie Zhou |  |
| 1483 |  |  [DialogConv: A Lightweight Fully Convolutional Network for Multi-view Response Selection](https://doi.org/10.18653/v1/2022.emnlp-main.828) |  | 0 | Current end-to-end retrieval-based dialogue systems are mainly based on Recurrent Neural Networks or Transformers with attention mechanisms. Although promising results have been achieved, these models often suffer from slow inference or huge number of parameters. In this paper, we propose a novel... | Daling Wang, Shi Feng, Wei Gao, Yifei Zhang, Yongkang Liu |  |
