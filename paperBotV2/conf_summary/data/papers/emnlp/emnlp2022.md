# EMNLP2022

## 会议论文列表

本会议共有 1483 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022](https://aclanthology.org/volumes/2022.findings-emnlp/) |  | 0 |  | Yoav Goldberg, Zornitsa Kozareva, Yue Zhang |  |
| 2 |  |  [LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.1) |  | 0 | Recently, deep learning models have made great progress in MWP solving on answer accuracy. However, they are uninterpretable since they mainly rely on shallow heuristics to achieve high performance without understanding and reasoning the grounded math logic. To address this issue and make a step towards interpretable MWP solving, we first construct a high-quality MWP dataset named InterMWP which consists of 11,495 MWPs and annotates interpretable logical formulas based on algebraic knowledge as the grounded linguistic logic of each solution equation. Different from existing MWP datasets, our InterMWP benchmark asks for a solver to not only output the solution expressions but also predict the corresponding logical formulas. We further propose a novel approach with logical prompt and interpretation generation, called LogicSolver. For each MWP, our LogicSolver first retrieves some highly-correlated algebraic knowledge and then passes them to the backbone model as prompts to improve the semantic representations of MWPs. With these improved semantic representations, our LogicSolver generates corresponding solution expressions and interpretable knowledge formulas in accord with the generated solution expressions, simultaneously. Experimental results show that our LogicSolver has stronger logical formula-based interpretability than baselines while achieving higher answer accuracy with the help of logical prompts, simultaneously. The source code and dataset will be available at https://github.com/yangzhch6/InterMWP. | Zhicheng Yang, Jinghui Qin, Jiaqi Chen, Liang Lin, Xiaodan Liang |  |
| 3 |  |  [Commonsense Knowledge Salience Evaluation with a Benchmark Dataset in E-commerce](https://doi.org/10.18653/v1/2022.findings-emnlp.2) |  | 0 | In e-commerce, the salience of commonsense knowledge (CSK) is beneficial for widespread applications such as product search and recommendation. For example, when users search for “running” in e-commerce, they would like to find products highly related to running, such as “running shoes” rather than “shoes”. Nevertheless, many existing CSK collections rank statements solely by confidence scores, and there is no information about which ones are salient from a human perspective. In this work, we define the task of supervised salience evaluation, where given a CSK triple, the model is required to learn whether the triple is salient or not. In addition to formulating the new task, we also release a new Benchmark dataset of Salience Evaluation in E-commerce (BSEE) and hope to promote related research on commonsense knowledge salience evaluation. We conduct experiments in the dataset with several representative baseline models. The experimental results show that salience evaluation is a hard task where models perform poorly on our evaluation set. We further propose a simple but effective approach, PMI-tuning, which shows promise for solving this novel problem. Code is available in https://github.com/OpenBGBenchmark/OpenBG-CSK. | Yincen Qu, Ningyu Zhang, Hui Chen, Zelin Dai, Chengming Wang, Xiaoyu Wang, Qiang Chen, Huajun Chen |  |
| 4 |  |  [Automatic Rule Induction for Efficient Semi-Supervised Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.3) |  | 0 | Semi-supervised learning has shown promise in allowing NLP models to generalize from small amounts of labeled data. Meanwhile, pretrained transformer models act as black-box correlation engines that are difficult to explain and sometimes behave unreliably. In this paper, we propose tackling both of these challenges via Automatic Rule Induction (ARI), a simple and general-purpose framework for the automatic discovery and integration of symbolic rules into pretrained transformer models. First, we extract weak symbolic rules from low-capacity machine learning models trained on small amounts of labeled data. Next, we use an attention mechanism to integrate these rules into high-capacity pretrained transformer models. Last, the rule-augmented system becomes part of a self-training framework to boost supervision signal on unlabeled data. These steps can be layered beneath a variety of existing weak supervision and semi-supervised NLP algorithms in order to improve performance and interpretability. Experiments across nine sequence classification and relation extraction tasks suggest that ARI can improve state-of-the-art methods with no manual effort and minimal computational overhead. | Reid Pryzant, Ziyi Yang, Yichong Xu, Chenguang Zhu, Michael Zeng |  |
| 5 |  |  [Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion](https://doi.org/10.18653/v1/2022.findings-emnlp.4) |  | 0 | Transformer-based pre-trained models like BERT have achieved great progress on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also shown general benefits in multiple NLP tasks. However, how to efficiently integrate dependency prior structure into pre-trained models to better model complex semantic matching relations is still unsettled. In this paper, we propose the Dependency-Enhanced Adaptive Fusion Attention (DAFA), which explicitly introduces dependency structure into pre-trained models and adaptively fuses it with semantic information. Specifically, (i) DAFA first proposes a structure-sensitive paradigm to construct a dependency matrix for calibrating attention weights. (ii) It adopts an adaptive fusion module to integrate the obtained dependency information and the original semantic signals. Moreover, DAFA reconstructs the attention calculation flow and provides better interpretability. By applying it on BERT, our method achieves state-of-the-art or competitive performance on 10 public datasets, demonstrating the benefits of adaptively fusing dependency structure in semantic matching task. | Jian Song, Di Liang, Rumei Li, Yuntao Li, Sirui Wang, Minlong Peng, Wei Wu, Yongxin Yu |  |
| 6 |  |  [Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT](https://doi.org/10.18653/v1/2022.findings-emnlp.5) |  | 0 | We combine the capacity of sparsely gated Mixture-of-Experts (MoE) with the speed and stability of linear, mixing transformations to design the Sparse Mixer encoder model. Sparse Mixer slightly outperforms BERT on GLUE and SuperGLUE, but more importantly trains 65% faster and runs inference 61% faster. We also present a faster variant, prosaically named Fast Sparse Mixer, that marginally underperforms BERT on SuperGLUE, but trains and runs nearly twice as fast. We justify the design of these two models by carefully ablating through various mixing mechanisms, MoE configurations, and hyperparameters. Sparse Mixer overcomes many of the latency and stability concerns of MoE models and offers the prospect of serving sparse student models, without resorting to distilling them to dense variants. | James LeeThorp, Joshua Ainslie |  |
| 7 |  |  [KE-GCL: Knowledge Enhanced Graph Contrastive Learning for Commonsense Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.6) |  | 0 | Commonsense question answering (CQA) aims to choose the correct answers for commonsense questions. Most existing works focus on extracting and reasoning over external knowledge graphs (KG). However, the noise in KG prevents these models from learning effective representations. In this paper, we propose a Knowledge Enhanced Graph Contrastive Learning model (KE-GCL) by incorporating the contextual descriptions of entities and adopting a graph contrastive learning scheme. Specifically, for QA pairs we represent the knowledge from KG and contextual descriptions. Then, the representations of contextual descriptions as context nodes are inserted into KG, forming the knowledge-enhanced graphs.Moreover, we design a contrastive learning method on graphs. For knowledge-enhanced graphs, we build their augmented views with an adaptive sampling strategy. After that, we reason over graphs to update their representations by scattering edges and aggregating nodes. To further improve GCL, hard graph negatives are chosen based on incorrect answers. Extensive experiments on two benchmark datasets demonstrate the effectiveness of our proposed KE-GCL, which outperforms previous methods consistently. | Lihui Zhang, Ruifan Li |  |
| 8 |  |  [Acceptability Judgements via Examining the Topology of Attention Maps](https://doi.org/10.18653/v1/2022.findings-emnlp.7) |  | 0 | The role of the attention mechanism in encoding linguistic knowledge has received special interest in NLP. However, the ability of the attention heads to judge the grammatical acceptability of a sentence has been underexplored. This paper approaches the paradigm of acceptability judgments with topological data analysis (TDA), showing that the geometric properties of the attention graph can be efficiently exploited for two standard practices in linguistics: binary judgments and linguistic minimal pairs. Topological features enhance the BERT-based acceptability classifier scores by 8%-24% on CoLA in three languages (English, Italian, and Swedish). By revealing the topological discrepancy between attention maps of minimal pairs, we achieve the human-level performance on the BLiMP benchmark, outperforming nine statistical and Transformer LM baselines. At the same time, TDA provides the foundation for analyzing the linguistic functions of attention heads and interpreting the correspondence between the graph features and grammatical phenomena. We publicly release the code and other materials used in the experiments. | Daniil Cherniavskii, Eduard Tulchinskii, Vladislav Mikhailov, Irina Proskurina, Laida Kushnareva, Ekaterina Artemova, Serguei Barannikov, Irina Piontkovskaya, Dmitri Piontkovski, Evgeny Burnaev |  |
| 9 |  |  [Clip-Tuning: Towards Derivative-free Prompt Learning with a Mixture of Rewards](https://doi.org/10.18653/v1/2022.findings-emnlp.8) |  | 0 | Derivative-free prompt learning has emerged as a lightweight alternative to prompt tuning, which only requires model inference to optimize the prompts. However, existing work did not take full advantage of the over-parameterized characteristics of large pre-trained language models (PLMs). In this paper, we propose Clip-Tuning, a simple yet effective method that adopts diverse frozen “thinned” networks of PLMs to obtain \*a mixture of rewards\* and thus advance the derivative-free prompt learning. The thinned networks consist of all the hidden units that survive a stationary dropout strategy, whose inference predictions reflect an ensemble of partial views over prompted training samples. Our method outperforms previous gradient-free prompt learning methods and achieves parity with gradient-based counterparts on seven language understanding benchmarks under few-shot settings. | Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang |  |
| 10 |  |  [Soft-Labeled Contrastive Pre-Training for Function-Level Code Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.9) |  | 0 | Code contrastive pre-training has recently achieved significant progress on code-related tasks. In this paper, we present SCodeR, a Soft-labeled contrastive pre-training framework with two positive sample construction methods to learn functional-level Code Representation. Considering the relevance between codes in a large-scale code corpus, the soft-labeled contrastive pre-training can obtain fine-grained soft-labels through an iterative adversarial manner and use them to learn better code representation. The positive sample construction is another key for contrastive pre-training. Previous works use transformation-based methods like variable renaming to generate semantically equal positive codes. However, they usually result in the generated code with a highly similar surface form, and thus mislead the model to focus on superficial code structure instead of code semantics. To encourage SCodeR to capture semantic information from the code, we utilize code comments and abstract syntax sub-trees of the code to build positive samples. We conduct experiments on four code-related tasks over seven datasets. Extensive experimental results show that SCodeR achieves new state-of-the-art performance on all of them, which illustrates the effectiveness of the proposed pre-training method. | Xiaonan Li, Daya Guo, Yeyun Gong, Yun Lin, Yelong Shen, Xipeng Qiu, Daxin Jiang, Weizhu Chen, Nan Duan |  |
| 11 |  |  [Conditioned Masked Language and Image Modeling for Image-Text Dense Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.10) |  | 0 | Image-text retrieval is a fundamental cross-modal task that takes image/text as a query to retrieve relevant data of another type. The large-scale two-stream pre-trained models like CLIP have achieved tremendous success in this area. They embed the images and texts into instance representations with two separate encoders, aligning them on the instance-level with contrastive learning. Beyond this, the following works adopt the fine-grained token-level interaction (Masked Language and Image Modeling) to boost performance further. However, the vanilla token-level objectives are not designed to aggregate the image-text alignment information into the instance representations, but the token representations, causing a gap between pre-training and application. To address this issue, we carefully design two novel conditioned token-level pre-training objectives, Conditioned Masked Language and Image Modeling (ConMLM and ConMIM), forcing models to aggregate the token-level alignment information into the instance representations. Combing with the instance-level contrastive learning, we propose our cross-modal dense retrieval framework, Conditioned Language-Image Pre-training (ConLIP). Experimental results on two popular cross-modal retrieval benchmarks (MSCOCO and Flickr30k) reveal the effectiveness of our methods. | Ziyang Luo, Yadong Xi, Rongsheng Zhang, GongZheng Li, Zeng Zhao, Jing Ma |  |
| 12 |  |  [Does Simultaneous Speech Translation need Simultaneous Models?](https://doi.org/10.18653/v1/2022.findings-emnlp.11) |  | 0 | In simultaneous speech translation (SimulST), finding the best trade-off between high output quality and low latency is a challenging task. To meet the latency constraints posed by different application scenarios, multiple dedicated SimulST models are usually trained and maintained, generating high computational costs. In this paper, also motivated by the increased sensitivity towards sustainable AI, we investigate whether a single model trained offline can serve both offline and simultaneous applications under different latency regimes without additional training or adaptation. Experiments on en->de, es show that, aside from facilitating the adoption of well-established offline architectures and training strategies without affecting latency, offline training achieves similar or better quality compared to the standard SimulST training protocol, also being competitive with the state-of-the-art system. | Sara Papi, Marco Gaido, Matteo Negri, Marco Turchi |  |
| 13 |  |  [Utilizing Language-Image Pretraining for Efficient and Robust Bilingual Word Alignment](https://doi.org/10.18653/v1/2022.findings-emnlp.12) |  | 0 | Word translation without parallel corpora has become feasible, rivaling the performance of supervised methods. Recent findings have shown the improvement in accuracy and robustness of unsupervised word translation (UWT) by utilizing visual observations, which are universal representations across languages.Our work investigates the potential of using not only visual observations but also pretrained language-image models for enabling a more efficient and robust UWT. We develop a novel UWT method dubbed Word Alignment using Language-Image Pretraining (WALIP), leveraging visual observations via the shared image-text embedding space of CLIPs (Radford et al., 2021). WALIP has a two-step procedure. First, we retrieve word pairs with high confidences of similarity, computed using our proposed image-based fingerprints, which define the initial pivot for the alignment.Second, we apply our robust Procrustes algorithm to estimate the linear mapping between two embedding spaces, which iteratively corrects and refines the estimated alignment.Our extensive experiments show that WALIP improves upon the state-of-the-art performance of bilingual word alignment for a few language pairs across different word embeddings and displays great robustness to the dissimilarity of language pairs or training corpora for two word embeddings. | Tuan Dinh, Jyyong Sohn, Shashank Rajput, Timothy Ossowski, Yifei Ming, Junjie Hu, Dimitris S. Papailiopoulos, Kangwook Lee |  |
| 14 |  |  [Grape: Knowledge Graph Enhanced Passage Reader for Open-domain Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.13) |  | 0 | A common thread of open-domain question answering (QA) models employs a retriever-reader pipeline that first retrieves a handful of relevant passages from Wikipedia and then peruses the passages to produce an answer. However, even state-of-the-art readers fail to capture the complex relationships between entities appearing in questions and retrieved passages, leading to answers that contradict the facts. In light of this, we propose a novel knowledge graph enhanced passage reader, namely Grape, to improve the reader performance for open-domain QA. Specifically, for each pair of question and retrieved passage, we first construct a localized bipartite graph, attributed to entity embeddings extracted from the intermediate layer of the reader model. Then, a graph neural network learns relational knowledge while fusing graph and contextual representations into the hidden states of the reader model. Experiments on three open-domain QA benchmarks show Grape can improve the state-of-the-art performance by up to 2.2 exact match score with a negligible overhead increase, with the same retriever and retrieved passages. Our code is publicly available at https://github.com/jumxglhf/GRAPE. | Mingxuan Ju, Wenhao Yu, Tong Zhao, Chuxu Zhang, Yanfang Ye |  |
| 15 |  |  [NarraSum: A Large-Scale Dataset for Abstractive Narrative Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.14) |  | 0 | Narrative summarization aims to produce a distilled version of a narrative to describe its most salient events and characters. Writing a summary for a narrative is challenging as it requires an understanding of event causality and character behaviors. To encourage research in this direction, we propose NarraSum, a large-scale narrative summarization dataset. It contains 122K narratives, which are collected from the synopses of movies and TV episodes with diverse genres, and their corresponding abstractive summaries. Experiments show that there is a large performance gap between humans and the state-of-the-art summarization models on NarraSum. We hope that this dataset will promote future research in summarization, as well as broader studies of natural language understanding and generation. The dataset is available at https://github.com/zhaochaocs/narrasum. | Chao Zhao, Faeze Brahman, Kaiqiang Song, Wenlin Yao, Dian Yu, Snigdha Chaturvedi |  |
| 16 |  |  [NMTScore: A Multilingual Analysis of Translation-based Text Similarity Measures](https://doi.org/10.18653/v1/2022.findings-emnlp.15) |  | 0 | Being able to rank the similarity of short text segments is an interesting bonus feature of neural machine translation. Translation-based similarity measures include direct and pivot translation probability, as well as translation cross-likelihood, which has not been studied so far. We analyze these measures in the common framework of multilingual NMT, releasing the NMTScore library. Compared to baselines such as sentence embeddings, translation-based measures prove competitive in paraphrase identification and are more robust against adversarial or multilingual input, especially if proper normalization is applied. When used for reference-based evaluation of data-to-text generation in 2 tasks and 17 languages, translation-based measures show a relatively high correlation to human judgments. | Jannis Vamvas, Rico Sennrich |  |
| 17 |  |  [Language Models Understand Us, Poorly](https://doi.org/10.18653/v1/2022.findings-emnlp.16) |  | 0 | Some claim language models understand us. Others won’t hear it. To clarify, I investigate three views of human language understanding: as-mapping, as-reliability and as-representation. I argue that while behavioral reliability is necessary for understanding, internal representations are sufficient; they climb the right hill. I review state-of-the-art language and multi-modal models: they are pragmatically challenged by under-specification of form. I question the Scaling Paradigm: limits on resources may prohibit scaled-up models from approaching understanding. Last, I describe how as-representation advances a science of understanding. We need work which probes model internals, adds more of human language, and measures what models can learn. | Jared Moore |  |
| 18 |  |  [Dialogue Meaning Representation for Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.17) |  | 0 | Dialogue meaning representation formulates natural language utterance semantics in their conversational context in an explicit and machine-readable form. Previous work typically follows the intent-slot framework, which is easy for annotation yet limited in scalability for complex linguistic expressions. A line of works alleviates the representation issue by introducing hierarchical structures but challenging to express complex compositional semantics, such as negation and coreference. We propose Dialogue Meaning Representation (DMR), a pliable and easily extendable representation for task-oriented dialogue. Our representation contains a set of nodes and edges to represent rich compositional semantics. Moreover, we propose an inheritance hierarchy mechanism focusing on domain extensibility. Additionally, we annotated DMR-FastFood, a multi-turn dialogue dataset with more than 70k utterances, with DMR. We propose two evaluation tasks to evaluate different dialogue models and a novel coreference resolution model GNNCoref for the graph-based coreference resolution task. Experiments show that DMR can be parsed well with pre-trained Seq2Seq models, and GNNCoref outperforms the baseline models by a large margin.The dataset and code are available at https://github.com/amazon-research/dialogue-meaning-representation | Xiangkun Hu, Junqi Dai, Hang Yan, Yi Zhang, Qipeng Guo, Xipeng Qiu, Zheng Zhang |  |
| 19 |  |  [Learning from the Dictionary: Heterogeneous Knowledge Guided Fine-tuning for Chinese Spell Checking](https://doi.org/10.18653/v1/2022.findings-emnlp.18) |  | 0 | Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling errors. Recent researches start from the pretrained knowledge of language models and take multimodal information into CSC models to improve the performance. However, they overlook the rich knowledge in the dictionary, the reference book where one can learn how one character should be pronounced, written, and used. In this paper, we propose the LEAD framework, which renders the CSC model to learn heterogeneous knowledge from the dictionary in terms of phonetics, vision, and meaning. LEAD first constructs positive and negative samples according to the knowledge of character phonetics, glyphs, and definitions in the dictionary. Then a unified contrastive learning-based training scheme is employed to refine the representations of the CSC models. Extensive experiments and detailed analyses on the SIGHAN benchmark datasets demonstrate the effectiveness of our proposed methods. | Yinghui Li, Shirong Ma, Qingyu Zhou, Zhongli Li, Yangning Li, Shulin Huang, Ruiyang Liu, Chao Li, Yunbo Cao, Haitao Zheng |  |
| 20 |  |  [Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?](https://doi.org/10.18653/v1/2022.findings-emnlp.19) |  | 0 | Despite their recent popularity and well-known advantages, dense retrievers still lag behind sparse methods such as BM25 in their ability to reliably match salient phrases and rare entities in the query and to generalize to out-of-domain data. It has been argued that this is an inherent limitation of dense models. We rebut this claim by introducing the Salient Phrase Aware Retriever (SPAR), a dense retriever with the lexical matching capacity of a sparse model. We show that a dense Lexical Model Λ can be trained to imitate a sparse one, and SPAR is built by augmenting a standard dense retriever with Λ. Empirically, SPAR shows superior performance on a range of tasks including five question answering datasets, MS MARCO passage retrieval, as well as the EntityQuestions and BEIR benchmarks for out-of-domain evaluation, exceeding the performance of state-of-the-art dense and sparse retrievers. The code and models of SPAR are available at: https://github.com/facebookresearch/dpr-scale/tree/main/spar | Xilun Chen, Kushal Lakhotia, Barlas Oguz, Anchit Gupta, Patrick S. H. Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, Wentau Yih |  |
| 21 |  |  [SMARTAVE: Structured Multimodal Transformer for Product Attribute Value Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.20) |  | 0 | Automatic product attribute value extraction refers to the task of identifying values of an attribute from the product information. Product attributes are essential in improving online shopping experience for customers. Most existing methods focus on extracting attribute values from product title and description.However, in many real-world applications, a product is usually represented by multiple modalities beyond title and description, such as product specifications, text and visual information from the product image, etc. In this paper, we propose SMARTAVE, a Structure Mltimodal trAnsformeR for producT Attribute Value Extraction, which jointly encodes the structured product information from multiple modalities. Specifically, in SMARTAVE encoder, we introduce hyper-tokens to represent the modality-level information, and local-tokens to represent the original text and visual inputs. Structured attention patterns are designed among the hyper-tokens and local-tokens for learning effective product representation. The attribute values are then extracted based on the learned embeddings. We conduct extensive experiments on two multimodal product datasets. Experimental results demonstrate the superior performance of the proposed approach over several state-of-the-art methods. Ablation studies validate the effectiveness of the structured attentions in modeling the multimodal product information. | Qifan Wang, Li Yang, Jingang Wang, Jitin Krishnan, Bo Dai, Sinong Wang, Zenglin Xu, Madian Khabsa, Hao Ma |  |
| 22 |  |  [When Language Model Meets Private Library](https://doi.org/10.18653/v1/2022.findings-emnlp.21) |  | 0 | With the rapid development of pre-training techniques, a number of language models have been pre-trained on large-scale code corpora and perform well in code generation. In this paper, we investigate how to equip pre-trained language models with the ability of code generation for private libraries. In practice, it is common for programmers to write code using private libraries. However, this is a challenge for language models since they have never seen private APIs during training. Motivated by the fact that private libraries usually come with elaborate API documentation, we propose a novel framework with two modules: the APIRetriever finds useful APIs, and then the APICoder generates code using these APIs. For APIRetriever, we present a dense retrieval system and also design a friendly interaction to involve uses. For APICoder, we can directly use off-the-shelf language models, or continually pre-train the base model on a code corpus containing API information. Both modules are trained with data from public libraries and can be generalized to private ones. Furthermore, we craft three benchmarks for private libraries, named TorchDataEval, MonkeyEval, and BeatNumEval. Experimental results demonstrate the impressive performance of our framework. | Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, JianGuang Lou |  |
| 23 |  |  [Cross-Domain Sentiment Classification using Semantic Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.22) |  | 0 | Previous studies on cross-domain sentiment classification depend on the pivot features or utilize the target data for representation learning, which ignore the semantic relevance between different domains. To this end, we exploit Abstract Meaning Representation (AMR) to help with cross-domain sentiment classification. Compared with the textual input, AMR reduces data sparsity and explicitly provides core semantic knowledge and correlations between different domains. In particular, we develop an algorithm to construct a sentiment-driven semantic graph from sentence-level AMRs. We further design two strategies to linearize the semantic graph and propose a text-graph interaction model to fuse the text and semantic graph representations for cross-domain sentiment classification. Empirical studies show the effectiveness of our proposed model over several strong baselines. The results also indicate the importance of the proposed sentiment-driven semantic graph for cross-domain sentiment classification. | Shichen Li, Zhongqing Wang, Xiaotong Jiang, Guodong Zhou |  |
| 24 |  |  [Yes-Yes-Yes: Proactive Data Collection for ACL Rolling Review and Beyond](https://doi.org/10.18653/v1/2022.findings-emnlp.23) |  | 0 | The shift towards publicly available text sources has enabled language processing at unprecedented scale, yet leaves under-serviced the domains where public and openly licensed data is scarce. Proactively collecting text data for research is a viable strategy to address this scarcity, but lacks systematic methodology taking into account the many ethical, legal and confidentiality-related aspects of data collection. Our work presents a case study on proactive data collection in peer review – a challenging and under-resourced NLP domain. We outline ethical and legal desiderata for proactive data collection and introduce “Yes-Yes-Yes”, the first donation-based peer reviewing data collection workflow that meets these requirements. We report on the implementation of Yes-Yes-Yes at ACL Rolling Review and empirically study the implications of proactive data collection for the dataset size and the biases induced by the donation behavior on the peer reviewing platform. | Nils Dycke, Ilia Kuznetsov, Iryna Gurevych |  |
| 25 |  |  [AssistSR: Task-oriented Video Segment Retrieval for Personal AI Assistant](https://doi.org/10.18653/v1/2022.findings-emnlp.24) |  | 0 | It is still a pipe dream that personal AI assistants on the phone and AR glasses can assist our daily life in addressing our questions like “how to adjust the date for this watch?” and “how to set its heating duration? (while pointing at an oven)”. The queries used in conventional tasks (i.e. Video Question Answering, Video Retrieval, Moment Localization) are often factoid and based on pure text. In contrast, we present a new task called Task-oriented Question-driven Video Segment Retrieval (TQVSR). Each of our questions is an image-box-text query that focuses on affordance of items in our daily life and expects relevant answer segments to be retrieved from a corpus of instructional video-transcript segments. To support the study of this TQVSR task, we construct a new dataset called AssistSR. We design novel guidelines to create high-quality samples. This dataset contains 3.2k multimodal questions on 1.6k video segments from instructional videos on diverse daily-used items. To address TQVSR, we develop a simple yet effective model called Dual Multimodal Encoders (DME) that significantly outperforms several baseline methods while still having large room for improvement in the future. Moreover, we present detailed ablation analyses. Code and data are available at https://github.com/StanLei52/TQVSR. | Weixian Lei, Difei Gao, Yuxuan Wang, Dongxing Mao, Zihan Liang, Lingmin Ran, Mike Zheng Shou |  |
| 26 |  |  [Dim-Krum: Backdoor-Resistant Federated Learning for NLP with Dimension-wise Krum-Based Aggregation](https://doi.org/10.18653/v1/2022.findings-emnlp.25) |  | 0 | Despite the potential of federated learning, it is known to be vulnerable to backdoor attacks. Many robust federated aggregation methods are proposed to reduce the potential backdoor risk. However, they are mainly validated in the CV field. In this paper, we find that NLP backdoors are hard to defend against than CV, and we provide a theoretical analysis that the malicious update detection error probabilities are determined by the relative backdoor strengths. NLP attacks tend to have small relative backdoor strengths, which may result in the failure of robust federated aggregation methods for NLP attacks. Inspired by the theoretical results, we can choose some dimensions with higher backdoor strengths to settle this issue. We propose a novel federated aggregation algorithm, Dim-Krum, for NLP tasks, and experimental results validate its effectiveness. | Zhiyuan Zhang, Qi Su, Xu Sun |  |
| 27 |  |  [Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.26) |  | 0 | Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks. In Natural Language Processing (NLP), DNNs are often backdoored during the fine-tuning process of a large-scale Pre-trained Language Model (PLM) with poisoned samples. Although the clean weights of PLMs are readily available, existing methods have ignored this information in defending NLP models against backdoor attacks. In this work, we take the first step to exploit the pre-trained (unfine-tuned) weights to mitigate backdoors in fine-tuned language models. Specifically, we leverage the clean pre-trained weights via two complementary techniques: (1) a two-step Fine-mixing technique, which first mixes the backdoored weights (fine-tuned on poisoned data) with the pre-trained weights, then fine-tunes the mixed weights on a small subset of clean data; (2) an Embedding Purification (E-PUR) technique, which mitigates potential backdoors existing in the word embeddings. We compare Fine-mixing with typical backdoor mitigation methods on three single-sentence sentiment classification tasks and two sentence-pair classification tasks and show that it outperforms the baselines by a considerable margin in all scenarios. We also show that our E-PUR method can benefit existing mitigation methods. Our work establishes a simple but strong baseline defense for secure fine-tuned NLP models against backdoor attacks. | Zhiyuan Zhang, Lingjuan Lyu, Xingjun Ma, Chenguang Wang, Xu Sun |  |
| 28 |  |  [Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion](https://doi.org/10.18653/v1/2022.findings-emnlp.27) |  | 0 | Language models (LMs) have recently been shown to generate more factual responses by employing modularity (Zhou et al., 2022) in combination with retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et al. (2021) to include internet search as a module. Our SeeKeR (Search engine->Knowledge->Response) method thus applies a single LM to three modular tasks in succession: search, generating knowledge, and generating a final response. We show that, when using SeeKeR as a dialogue model, it outperforms the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain knowledge-grounded conversations for the same number of parameters, in terms of consistency, knowledge and per-turn engagingness. SeeKeR applied to topical prompt completions as a standard language model outperforms GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality, despite GPT3 being a vastly larger model. Our code and models are made publicly available. | Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, Jason Weston |  |
| 29 |  |  [Stretching Sentence-pair NLI Models to Reason over Long Documents and Clusters](https://doi.org/10.18653/v1/2022.findings-emnlp.28) |  | 0 | Natural Language Inference (NLI) has been extensively studied by the NLP community as a framework for estimating the semantic relation between sentence pairs. While early work identified certain biases in NLI models, recent advancements in modeling and datasets demonstrated promising performance.In this work, we further explore the direct zero-shot applicability of NLI models to real applications, beyond the sentence-pair setting they were trained on. First, we analyze the robustness of these models to longer and out-of-domain inputs. Then, we develop new aggregation methods to allow operating over full documents, reaching state-of-the-art performance on the ContractNLI dataset. Interestingly, we find NLI scores to provide strong retrieval signals, leading to more relevant evidence extractions compared to common similarity-based methods. Finally, we go further and investigate whole document clusters to identify both discrepancies and consensus among sources. In a test case, we find real inconsistencies between Wikipedia pages in different languages about the same topic. | Tal Schuster, Sihao Chen, Senaka Buthpitiya, Alex Fabrikant, Donald Metzler |  |
| 30 |  |  [Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study](https://doi.org/10.18653/v1/2022.findings-emnlp.29) |  | 0 | This paper presents an empirical study to build relation extraction systems in low-resource settings. Based upon recent pre-trained language models, we comprehensively investigate three schemes to evaluate the performance in low-resource settings: (i) different types of prompt-based methods with few-shot labeled data; (ii) diverse balancing methods to address the long-tailed distribution issue; (iii) data augmentation technologies and self-training to generate more labeled in-domain data. We create a benchmark with 8 relation extraction (RE) datasets covering different languages, domains and contexts and perform extensive comparisons over the proposed schemes with combinations. Our experiments illustrate: (i) Though prompt-based tuning is beneficial in low-resource RE, there is still much potential for improvement, especially in extracting relations from cross-sentence contexts with multiple relational triples; (ii) Balancing methods are not always helpful for RE with long-tailed distribution; (iii) Data augmentation complements existing baselines and can bring much performance gain, while self-training may not consistently achieve advancement to low-resource RE. Code and datasets are in https://github.com/zjunlp/LREBench. | Xin Xu, Xiang Chen, Ningyu Zhang, Xin Xie, Xi Chen, Huajun Chen |  |
| 31 |  |  [CLLE: A Benchmark for Continual Language Learning Evaluation in Multilingual Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.30) |  | 0 | Continual Language Learning (CLL) in multilingual translation is inevitable when new languages are required to be translated. Due to the lack of unified and generalized benchmarks, the evaluation of existing methods is greatly influenced by experimental design which usually has a big gap from the industrial demands. In this work, we propose the first Continual Language Learning Evaluation benchmark CLLE in multilingual translation. CLLE consists of a Chinese-centric corpus — CN-25 and two CLL tasks — the close-distance language continual learning task and the language family continual learning task designed for real and disparate demands. Different from existing translation benchmarks, CLLE considers several restrictions for CLL, including domain distribution alignment, content overlap, language diversity, and the balance of corpus. Furthermore, we propose a novel framework COMETA based on Constrained Optimization and META-learning to alleviate catastrophic forgetting and dependency on history training data by using a meta-model to retain the important parameters for old languages. Our experiments prove that CLLE is a challenging CLL benchmark and that our proposed method is effective when compared with other strong baselines. Due to the construction of the corpus, the task designing and the evaluation method are independent of the centric language, we also construct and release the English-centric corpus EN-25 to facilitate academic research. | Han Zhang, Sheng Zhang, Yang Xiang, Bin Liang, Jinsong Su, Zhongjian Miao, Hui Wang, Ruifeng Xu |  |
| 32 |  |  [Lexicon-Enhanced Self-Supervised Training for Multilingual Dense Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.31) |  | 0 | Recent multilingual pre-trained models have shown better performance in various multilingual tasks. However, these models perform poorly on multilingual retrieval tasks due to lacking multilingual training data. In this paper, we propose to mine and generate self-supervised training data based on a large-scale unlabeled corpus. We carefully design a mining method which combines the sparse and dense models to mine the relevance of unlabeled queries and passages. And we introduce a query generator to generate more queries in target languages for unlabeled passages. Through extensive experiments on Mr. TYDI dataset and an industrial dataset from a commercial search engine, we demonstrate that our method performs better than baselines based on various pre-trained multilingual models. Our method even achieves on-par performance with the supervised method on the latter dataset. | Houxing Ren, Linjun Shou, Jian Pei, Ning Wu, Ming Gong, Daxin Jiang |  |
| 33 |  |  [Improve Interpretability of Neural Networks via Sparse Contrastive Coding](https://doi.org/10.18653/v1/2022.findings-emnlp.32) |  | 0 | Although explainable artificial intelligence (XAI) has achieved remarkable developments in recent years, there are few efforts have been devoted to the following problems, namely, i) how to develop an explainable method that could explain the black-box in a model-agnostic way? and ii) how to improve the performance and interpretability of the black-box using such explanations instead of pre-collected important attributions? To explore the potential solution, we propose a model-agnostic explanation method termed as Sparse Contrastive Coding (SCC) and verify its effectiveness in text classification and natural language inference. In brief, SCC explains the feature attributions which characterize the importance of words based on the hidden states of each layer of the model. With such word-level explainability, SCC adaptively divides the input sentences into foregrounds and backgrounds in terms of task relevance. Through maximizing the similarity between the foregrounds and input sentences while minimizing the similarity between the backgrounds and input sentences, SSC employs a supervised contrastive learning loss to boost the interpretability and performance of the model. Extensive experiments show the superiority of our method over five state-of-the-art methods in terms of interpretability and classification measurements. The code is available at https://pengxi.me. | Junhong Liu, Yijie Lin, Liang Jiang, Jia Liu, Zujie Wen, Xi Peng |  |
| 34 |  |  [LEMON: Language-Based Environment Manipulation via Execution-Guided Pre-training](https://doi.org/10.18653/v1/2022.findings-emnlp.33) |  | 0 | Language-based environment manipulation requires agents to manipulate the environment following natural language instructions, which is challenging due to the huge space of the environments.To address this challenge, various approaches have been proposed in recent work. Although these approaches work well for their intended environments, they are difficult to generalize across environments. In this work, we propose LEMON, a general framework for language-based environment manipulation tasks. Specifically, we first specify a general approach for language-based environment manipulation tasks, which can deal with various environments using the same generative language model. Then we propose an execution-guided pre-training strategy to inject prior knowledge of environments to the language model with a pure synthetic pre-training corpus. Experimental results on tasks including Alchemy, Scene, Tangrams, ProPara and Recipes demonstrate the effectiveness of LEMON: it achieves new state-of-the-art results on four of the tasks, and the execution-guided pre-training strategy brings remarkable improvements on all experimental tasks. | Qi Shi, Qian Liu, Bei Chen, Yu Zhang, Ting Liu, JianGuang Lou |  |
| 35 |  |  [CROP: Zero-shot Cross-lingual Named Entity Recognition with Multilingual Labeled Sequence Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.34) |  | 0 | Named entity recognition (NER) suffers from the scarcity of annotated training data, especially for low-resource languages without labeled data. Cross-lingual NER has been proposed to alleviate this issue by transferring knowledge from high-resource languages to low-resource languages via aligned cross-lingual representations or machine translation results. However, the performance of cross-lingual NER methods is severely affected by the unsatisfactory quality of translation or label projection. To address these problems, we propose a Cross-lingual Entity Projection framework (CROP) to enable zero-shot cross-lingual NER with the help of a multilingual labeled sequence translation model. Specifically, the target sequence is first translated into the source language and then tagged by a source NER model. We further adopt a labeled sequence translation model to project the tagged sequence back to the target language and label the target raw sentence. Ultimately, the whole pipeline is integrated into an end-to-end model by the way of self-training. Experimental results on two benchmarks demonstrate that our method substantially outperforms the previous strong baseline by a large margin of +3 7 F1 scores and achieves state-of-the-art performance. | Jian Yang, Shaohan Huang, Shuming Ma, Yuwei Yin, Li Dong, Dongdong Zhang, Hongcheng Guo, Zhoujun Li, Furu Wei |  |
| 36 |  |  [Handling and Presenting Harmful Text in NLP Research](https://doi.org/10.18653/v1/2022.findings-emnlp.35) |  | 0 | Text data can pose a risk of harm. However, the risks are not fully understood, and how to handle, present, and discuss harmful text in a safe way remains an unresolved issue in the NLP community. We provide an analytical framework categorising harms on three axes: (1) the harm type (e.g., misinformation, hate speech or racial stereotypes); (2) whether a harm is sought as a feature of the research design if explicitly studying harmful content (e.g., training a hate speech classifier), versus unsought if harmful content is encountered when working on unrelated problems (e.g., language generation or part-of-speech tagging); and (3) who it affects, from people (mis)represented in the data to those handling the data and those publishing on the data. We provide advice for practitioners, with concrete steps for mitigating harm in research and in publication. To assist implementation we introduce HarmCheck – a documentation standard for handling and presenting harmful text in research. | Hannah Kirk, Abeba Birhane, Bertie Vidgen, Leon Derczynski |  |
| 37 |  |  [Multimodal Contrastive Learning via Uni-Modal Coding and Cross-Modal Prediction for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2022.findings-emnlp.36) |  | 0 | Multimodal representation learning is a challenging task in which previous work mostly focus on either uni-modality pre-training or cross-modality fusion. In fact, we regard modeling multimodal representation as building a skyscraper, where laying stable foundation and designing the main structure are equally essential. The former is like encoding robust uni-modal representation while the later is like integrating interactive information among different modalities, both of which are critical to learning an effective multimodal representation. Recently, contrastive learning has been successfully applied in representation learning, which can be utilized as the pillar of the skyscraper and benefit the model to extract the most important features contained in the multimodal data. In this paper, we propose a novel framework named MultiModal Contrastive Learning (MMCL) for multimodal representation to capture intra- and inter-modality dynamics simultaneously. Specifically, we devise uni-modal contrastive coding with an efficient uni-modal feature augmentation strategy to filter inherent noise contained in acoustic and visual modality and acquire more robust uni-modality representations. Besides, a pseudo siamese network is presented to predict representation across different modalities, which successfully captures cross-modal dynamics. Moreover, we design two contrastive learning tasks, instance- and sentiment-based contrastive learning, to promote the process of prediction and learn more interactive information related to sentiment. Extensive experiments conducted on two public datasets demonstrate that our method surpasses the state-of-the-art methods. | Ronghao Lin, Haifeng Hu |  |
| 38 |  |  [Towards Unified Prompt Tuning for Few-shot Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.37) |  | 0 | Prompt-based fine-tuning has boosted the performance of Pre-trained Language Models (PLMs) on few-shot text classification by employing task-specific prompts. Yet, PLMs are unfamiliar with prompt-style expressions during pre-training, which limits the few-shot learning performance on downstream tasks.It would be desirable if the models can acquire some prompting knowledge before adapting to specific NLP tasks. We present the Unified Prompt Tuning (UPT) framework, leading to better few-shot text classification for BERT-style models by explicitly capturing prompting semantics from non-target NLP datasets. In UPT, a novel paradigm Prompt-Options-Verbalizer is proposed for joint prompt learning across different NLP tasks, forcing PLMs to capture task-invariant prompting knowledge. We further design a self-supervised task named Knowledge-enhanced Selective Masked Language Modeling to improve the PLM’s generalization abilities for accurate adaptation to previously unseen tasks. After multi-task learning across multiple tasks, the PLM can be better prompt-tuned towards any dissimilar target tasks in low-resourced settings. Experiments over a variety of NLP tasks show that UPT consistently outperforms state-of-the-arts for prompt-based fine-tuning. | Jianing Wang, Chengyu Wang, Fuli Luo, Chuanqi Tan, Minghui Qiu, Fei Yang, Qiuhui Shi, Songfang Huang, Ming Gao |  |
| 39 |  |  [Can language models learn from explanations in context?](https://doi.org/10.18653/v1/2022.findings-emnlp.38) |  | 0 | Language Models (LMs) can perform new tasks by adapting to a few in-context examples. For humans, explanations that connect examples to task principles can improve learning. We therefore investigate whether explanations of few-shot examples can help LMs. We annotate questions from 40 challenging tasks with answer explanations, and various matched control explanations. We evaluate how different types of explanations, instructions, and controls affect zero- and few-shot performance. We analyze these results using statistical multilevel modeling techniques that account for the nested dependencies among conditions, tasks, prompts, and models. We find that explanations can improve performance—even without tuning. Furthermore, explanations hand-tuned for performance on a small validation set offer substantially larger benefits, and building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone. Finally, even untuned explanations outperform carefully matched controls, suggesting that the benefits are due to the link between an example and its explanation, rather than lower-level features. However, only large models benefit. In summary, explanations can support the in-context learning of large LMs on challenging tasks. | Andrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y. Chan, Kory W. Mathewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane Wang, Felix Hill |  |
| 40 |  |  [GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.39) |  | 0 | Recently, retrieval models based on dense representations are dominant in passage retrieval tasks, due to their outstanding ability in terms of capturing semantics of input text compared to the traditional sparse vector space models. A common practice of dense retrieval models is to exploit a dual-encoder architecture to represent a query and a passage independently. Though efficient, such a structure loses interaction between the query-passage pair, resulting in inferior accuracy. To enhance the performance of dense retrieval models without loss of efficiency, we propose a GNN-encoder model in which query (passage) information is fused into passage (query) representations via graph neural networks that are constructed by queries and their top retrieved passages. By this means, we maintain a dual-encoder structure, and retain some interaction information between query-passage pairs in their representations, which enables us to achieve both efficiency and efficacy in passage retrieval. Evaluation results indicate that our method significantly outperforms the existing models on MSMARCO, Natural Questions and TriviaQA datasets, and achieves the new state-of-the-art on these datasets. | Jiduan Liu, Jiahao Liu, Yang Yang, Jingang Wang, Wei Wu, Dongyan Zhao, Rui Yan |  |
| 41 |  |  [Linguistic Rules-Based Corpus Generation for Native Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2022.findings-emnlp.40) |  | 0 | Chinese Grammatical Error Correction (CGEC) is both a challenging NLP task and a common application in human daily life. Recently, many data-driven approaches are proposed for the development of CGEC research. However, there are two major limitations in the CGEC field: First, the lack of high-quality annotated training corpora prevents the performance of existing CGEC models from being significantly improved. Second, the grammatical errors in widely used test sets are not made by native Chinese speakers, resulting in a significant gap between the CGEC models and the real application. In this paper, we propose a linguistic rules-based approach to construct large-scale CGEC training corpora with automatically generated grammatical errors. Additionally, we present a challenging CGEC benchmark derived entirely from errors made by native Chinese speakers in real-world scenarios. Extensive experiments and detailed analyses not only demonstrate that the training data constructed by our method effectively improves the performance of CGEC models, but also reflect that our benchmark is an excellent resource for further development of the CGEC field. | Shirong Ma, Yinghui Li, Rongyi Sun, Qingyu Zhou, Shulin Huang, Ding Zhang, Yangning Li, Ruiyang Liu, Zhongli Li, Yunbo Cao, Haitao Zheng, Ying Shen |  |
| 42 |  |  [Rethinking the Video Sampling and Reasoning Strategies for Temporal Sentence Grounding](https://doi.org/10.18653/v1/2022.findings-emnlp.41) |  | 0 | Temporal sentence grounding (TSG) aims to identify the temporal boundary of a specific segment from an untrimmed video by a sentence query. All existing works first utilize a sparse sampling strategy to extract a fixed number of video frames and then interact them with query for reasoning.However, we argue that these methods have overlooked two indispensable issues:1) Boundary-bias: The annotated target segment generally refers to two specific frames as corresponding start and end timestamps. The video downsampling process may lose these two frames and take the adjacent irrelevant frames as new boundaries.2) Reasoning-bias: Such incorrect new boundary frames also lead to the reasoning bias during frame-query interaction, reducing the generalization ability of model.To alleviate above limitations, in this paper, we propose a novel Siamese Sampling and Reasoning Network (SSRN) for TSG, which introduces a siamese sampling mechanism to generate additional contextual frames to enrich and refine the new boundaries. Specifically, a reasoning strategy is developed to learn the inter-relationship among these frames and generate soft labels on boundaries for more accurate frame-query reasoning. Such mechanism is also able to supplement the absent consecutive visual semantics to the sampled sparse frames for fine-grained activity understanding.Extensive experiments demonstrate the effectiveness of SSRN on three challenging datasets. | Jiahao Zhu, Daizong Liu, Pan Zhou, Xing Di, Yu Cheng, Song Yang, Wenzheng Xu, Zichuan Xu, Yao Wan, Lichao Sun, Zeyu Xiong |  |
| 43 |  |  [System 1 + System 2 = Better World: Neural-Symbolic Chain of Logic Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.42) |  | 0 | Logical reasoning is a challenge for many current NLP neural network models since it requires more than the ability of learning informative representations from data. Inspired by the Dual Process Theory in cognitive science — which proposes that human cognition process involves two stages: an intuitive, unconscious and fast process relying on perception calledSystem 1, and a logical, conscious and slow process performing complex reasoning called System 2 — we leverage neural logic reasoning (System 2) on top of the representation learning models (System 1), which conducts explicit neural-based differentiable logical reasoning on top of the representations learned by the base neural models. Based on experiments on the commonsense knowledge graph completion task, we show that the two-system architecture always improves from its System 1 model alone. Experiments also show that both the rule-driven logical regularizer and the data-driven value regularizer are important and the performance improvement is marginal without the two regularizers, which indicates that learning from both logical prior and training data is important for reasoning tasks. | Wenyue Hua, Yongfeng Zhang |  |
| 44 |  |  [Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation](https://doi.org/10.18653/v1/2022.findings-emnlp.43) |  | 0 | Federated learning (FL) can be essential in knowledge representation, reasoning, and data mining applications over multi-source knowledge graphs (KGs). A recent study FedE first proposes an FL framework that shares entity embeddings of KGs across all clients. However, entity embedding sharing from FedE would incur a severe privacy leakage. Specifically, the known entity embedding can be used to infer whether a specific relation between two entities exists in a private client. In this paper, we introduce a novel attack method that aims to recover the original data based on the embedding information, which is further used to evaluate the vulnerabilities of FedE. Furthermore, we propose a Federated learning paradigm with privacy-preserving Relation embedding aggregation (FedR) to tackle the privacy issue in FedE. Besides, relation embedding sharing can significantly reduce the communication cost due to its smaller size of queries. We conduct extensive experiments to evaluate FedR with five different KG embedding models and three datasets. Compared to FedE, FedR achieves similar utility and significant improvements regarding privacy-preserving effect and communication efficiency on the link prediction task. | Kai Zhang, Yu Wang, Hongyi Wang, Lifu Huang, Carl Yang, Xun Chen, Lichao Sun |  |
| 45 |  |  [TextHacker: Learning based Hybrid Local Search Algorithm for Text Hard-label Adversarial Attack](https://doi.org/10.18653/v1/2022.findings-emnlp.44) |  | 0 | Existing textual adversarial attacks usually utilize the gradient or prediction confidence to generate adversarial examples, making it hard to be deployed in real-world applications. To this end, we consider a rarely investigated but more rigorous setting, namely hard-label attack, in which the attacker can only access the prediction label. In particular, we find we can learn the importance of different words via the change on prediction label caused by word substitutions on the adversarial examples. Based on this observation, we propose a novel adversarial attack, termed Text Hard-label attacker (TextHacker). TextHacker randomly perturbs lots of words to craft an adversarial example. Then, TextHacker adopts a hybrid local search algorithm with the estimation of word importance from the attack history to minimize the adversarial perturbation. Extensive evaluations for text classification and textual entailment show that TextHacker significantly outperforms existing hard-label attacks regarding the attack performance as well as adversary quality. | Zhen Yu, Xiaosen Wang, Wanxiang Che, Kun He |  |
| 46 |  |  [Visualizing the Obvious: A Concreteness-based Ensemble Model for Noun Property Prediction](https://doi.org/10.18653/v1/2022.findings-emnlp.45) |  | 0 | Neural language models encode rich knowledge about entities and their relationships which can be extracted from their representations using probing. Common properties of nouns (e.g., red strawberries, small ant) are, however, more challenging to extract compared to other types of knowledge because they are rarely explicitly stated in texts.We hypothesize this to mainly be the case for perceptual properties which are obvious to the participants in the communication. We propose to extract these properties from images and use them in an ensemble model, in order to complement the information that is extracted from language models. We consider perceptual properties to be more concrete than abstract properties (e.g., interesting, flawless). We propose to use the adjectives’ concreteness score as a lever to calibrate the contribution of each source (text vs. images). We evaluate our ensemble model in a ranking task where the actual properties of a noun need to be ranked higher than other non-relevant properties. Our results show that the proposed combination of text and images greatly improves noun property prediction compared to powerful text-based language models. | Yue Yang, Artemis Panagopoulou, Marianna Apidianaki, Mark Yatskar, Chris CallisonBurch |  |
| 47 |  |  [It's Better to Teach Fishing than Giving a Fish: An Auto-Augmented Structure-aware Generative Model for Metaphor Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.46) |  | 0 | Metaphor Detection aims to identify the metaphorical meaning of words in the sentence. Most existing work is discriminant models, which use the contextual semantic information extracted by transformers for classifications directly. Due to insufficient training data and corresponding paraphrases, recent methods focus on how to get external resources and utilize them to introduce more knowledge. Currently, contextual modeling and external data are two key issues in the field. In this paper, we propose \*\*A\*\*n \*\*A\*\*uto-\*\*A\*\*ugmented \*\*S\*\*tructure-aware generative model (\*\*AAAS\*\*) for metaphor detection, which transforms the classification task into a keywords-extraction task. Specifically, we propose the task of structure information extraction to allow the model to use the ‘structural language’ to describe the whole sentence. Furthermore, without any other external resources, we design a simple but effective auto-augmented method to expand the limited datasets. Experimental results show that \*\*AAAS\*\* obtains competitive results compared with state-of-the-art methods. | Huawen Feng, Qianli Ma |  |
| 48 |  |  [Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks](https://doi.org/10.18653/v1/2022.findings-emnlp.47) |  | 0 | Natural language processing (NLP) models are known to be vulnerable to backdoor attacks, which poses a newly arisen threat to NLP models. Prior online backdoor defense methods for NLP models only focus on the anomalies at either the input or output level, still suffering from fragility to adaptive attacks and high computational cost. In this work, we take the first step to investigate the unconcealment of textual poisoned samples at the intermediate-feature level and propose a feature-based efficient online defense method. Through extensive experiments on existing attacking methods, we find that the poisoned samples are far away from clean samples in the intermediate feature space of a poisoned NLP model. Motivated by this observation, we devise a distance-based anomaly score (DAN) to distinguish poisoned samples from clean samples at the feature level. Experiments on sentiment analysis and offense detection tasks demonstrate the superiority of DAN, as it substantially surpasses existing online defense methods in terms of defending performance and enjoys lower inference costs. Moreover, we show that DAN is also resistant to adaptive attacks based on feature-level regularization. Our code is available at https://github.com/lancopku/DAN. | Sishuo Chen, Wenkai Yang, Zhiyuan Zhang, Xiaohan Bi, Xu Sun |  |
| 49 |  |  [Diving Deep into Modes of Fact Hallucinations in Dialogue Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.48) |  | 0 | Knowledge Graph(KG) grounded conversations often use large pre-trained models and usually suffer from fact hallucination. Frequently entities with no references in knowledge sources and conversation history are introduced into responses, thus hindering the flow of the conversation—existing work attempt to overcome this issue by tweaking the training procedure or using a multi-step refining method. However, minimal effort is put into constructing an entity-level hallucination detection system, which would provide fine-grained signals that control fallacious content while generating responses. As a first step to address this issue, we dive deep to identify various modes of hallucination in KG-grounded chatbots through human feedback analysis. Secondly, we propose a series of perturbation strategies to create a synthetic dataset named FADE (FActual Dialogue Hallucination DEtection Dataset). Finally, we conduct comprehensive data analyses and create multiple baseline models for hallucination detection to compare against human-verified data and already established benchmarks. | Souvik Das, Sougata Saha, Rohini K. Srihari |  |
| 50 |  |  [Representation Learning for Resource-Constrained Keyphrase Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.49) |  | 0 | State-of-the-art keyphrase generation methods generally depend on large annotated datasets, limiting their performance in domains with limited annotated data. To overcome this challenge, we design a data-oriented approach that first identifies salient information using retrieval-based corpus-level statistics, and then learns a task-specific intermediate representation based on a pre-trained language model using large-scale unlabeled documents. We introduce salient span recovery and salient span prediction as denoising training objectives that condense the intra-article and inter-article knowledge essential for keyphrase generation. Through experiments on multiple keyphrase generation benchmarks, we show the effectiveness of the proposed approach for facilitating low-resource keyphrase generation and zero-shot domain adaptation. Our method especially benefits the generation of absent keyphrases, approaching the performance of models trained with large training sets. | Di Wu, Wasi Uddin Ahmad, Sunipa Dev, KaiWei Chang |  |
| 51 |  |  [Systematicity in GPT-3's Interpretation of Novel English Noun Compounds](https://doi.org/10.18653/v1/2022.findings-emnlp.50) |  | 0 | Levin et al. (2019) show experimentally that the interpretations of novel English noun compounds (e.g., stew skillet), while not fully compositional, are highly predictable based on whether the modifier and head refer to artifacts or natural kinds. Is the large language model GPT-3 governed by the same interpretive principles? To address this question, we first compare Levin et al.’s experimental data with GPT-3 generations, finding a high degree of similarity. However, this evidence is consistent with GPT-3 reasoning only about specific lexical items rather than the more abstract conceptual categories of Levin et al.’s theory. To probe more deeply, we construct prompts that require the relevant kind of conceptual reasoning. Here, we fail to find convincing evidence that GPT-3 is reasoning about more than just individual lexical items. These results highlight the importance of controlling for low-level distributional regularities when assessing whether a large language model latently encodes a deeper theory. | Siyan Li, Riley Carlson, Christopher Potts |  |
| 52 |  |  [CARE: Causality Reasoning for Empathetic Responses by Conditional Graph Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.51) |  | 0 | Recent approaches to empathetic response generation incorporate emotion causalities to enhance comprehension of both the user’s feelings and experiences. However, these approaches suffer from two critical issues. First, they only consider causalities between the user’s emotion and the user’s experiences, and ignore those between the user’s experiences. Second, they neglect interdependence among causalities and reason them independently. To solve the above problems, we expect to reason all plausible causalities interdependently and simultaneously, given the user’s emotion, dialogue history, and future dialogue content. Then, we infuse these causalities into response generation for empathetic responses. Specifically, we design a new model, i.e., the Conditional Variational Graph Auto-Encoder (CVGAE), for the causality reasoning, and adopt a multi-source attention mechanism in the decoder for the causality infusion. We name the whole framework as CARE, abbreviated for CAusality Reasoning for Empathetic conversation. Experimental results indicate that our method achieves state-of-the-art performance. | Jiashuo Wang, Yi Cheng, Wenjie Li |  |
| 53 |  |  [TransAdv: A Translation-based Adversarial Learning Framework for Zero-Resource Cross-Lingual Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.52) |  | 0 | Zero-Resource Cross-Lingual Named Entity Recognition aims at training an NER model of the target language using only labeled source language data and unlabeled target language data. Existing methods are mainly divided into three categories: model transfer based, data transfer based and knowledge transfer based. Each method has its own disadvantages, and combining more than one of them often leads to better performance. However, the performance of data transfer based methods is often limited by inevitable noise in the translation process. To handle the problem, we propose a framework named TransAdv to mitigate lexical and syntactic errors of word-by-word translated data, better utilizing the data by multi-level adversarial learning and multi-model knowledge distillation. Extensive experiments are conducted over 6 target languages with English as the source language, and the results show that TransAdv achieves competitive performance to the state-of-the-art models. | Yichun Zhao, Jintao Du, Gongshen Liu, Huijia Zhu |  |
| 54 |  |  [BARLE: Background-Aware Representation Learning for Background Shift Out-of-Distribution Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.53) |  | 0 | Machine learning models often suffer from a performance drop when they are applied to out-of-distribution (OOD) samples, i.e., those drawn far away from the training data distribution. Existing OOD detection work mostly focuses on identifying semantic-shift OOD samples, e.g., instances from unseen new classes. However, background-shift OOD detection, which identifies samples with domain or style-change, represents a more practical yet challenging task. In this paper, we propose Background-Aware Representation Learning (BARLE) for background-shift OOD detection in NLP. Specifically, we generate semantics-preserving background-shifted pseudo OOD samples from pretrained masked language models. We then contrast the in-distribution (ID) samples with their pseudo OOD counterparts. Unlike prior semantic-shift OOD detection work that often leverages an external text corpus, BARLE only uses ID data, which is more flexible and cost-efficient. In experiments across several text classification tasks, we demonstrate that BARLE is capable of improving background-shift OOD detection performance while maintaining ID classification accuracy. We further investigate the properties of the generated pseudo OOD samples, uncovering the working mechanism of BARLE. | Hanyu Duan, Yi Yang, Ahmed Abbasi, Kar Yan Tam |  |
| 55 |  |  [What Language Model to Train if You Have One Million GPU Hours?](https://doi.org/10.18653/v1/2022.findings-emnlp.54) |  | 0 | The crystallization of modeling methods around the Transformer architecture has been a boon for practitioners. Simple, well-motivated architectural variations can transfer across tasks and scale, increasing the impact of modeling research. However, with the emergence of state-of-the-art 100B+ parameters models, large language models are increasingly expensive to accurately design and train. Notably, it can be difficult to evaluate how modeling decisions may impact emergent capabilities, given that these capabilities arise mainly from sheer scale alone.In the process of building BLOOM–the Big Science Large Open-science Open-access Multilingual language model–our goal is to identify an architecture and training setup that makes the best use of our 1,000,000 A100-GPU-hours budget.Specifically, we perform an ablation study at the billion-parameter scale comparing different modeling practices and their impact on zero-shot generalization.In addition, we study the impact of various popular pre-training corpora on zero-shot generalization. We also study the performance of a multilingual model and how it compares to the English-only one. Finally, we consider the scaling behaviour of Transformers to choose the target model size, shape, and training setup. All our models and code are open-sourced at https://huggingface.co/bigscience. | Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M. Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, Iz Beltagy |  |
| 56 |  |  [Enhancing Out-of-Distribution Detection in Natural Language Understanding via Implicit Layer Ensemble](https://doi.org/10.18653/v1/2022.findings-emnlp.55) |  | 0 | Out-of-distribution (OOD) detection aims to discern outliers from the intended data distribution, which is crucial to maintaining high reliability and a good user experience.Most recent studies in OOD detection utilize the information from a single representation that resides in the penultimate layer to determine whether the input is anomalous or not.Although such a method is straightforward, the potential of diverse information in the intermediate layers is overlooked.In this paper, we propose a novel framework based on contrastive learning that encourages intermediate features to learn layer-specialized representations and assembles them implicitly into a single representation to absorb rich information in the pre-trained language model. Extensive experiments in various intent classification and OOD datasets demonstrate that our approach is significantly more effective than other works. | Hyunsoo Cho, Choonghyun Park, Jaewook Kang, Kang Min Yoo, Taeuk Kim, Sanggoo Lee |  |
| 57 |  |  [Contrastive Demonstration Tuning for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.56) |  | 0 | Pretrained language models can be effectively stimulated by textual prompts or demonstrations, especially in low-data scenarios. Recent works have focused on automatically searching discrete or continuous prompts or optimized verbalizers, yet studies for the demonstration are still limited. Concretely, the demonstration examples are crucial for an excellent final performance of prompt-tuning. In this paper, we propose a novel pluggable, extensible, and efficient approach named contrastive demonstration tuning, which is free of demonstration sampling. Furthermore, the proposed approach can be: (i) Plugged into any previous prompt-tuning approaches; (ii) Extended to widespread classification tasks with a large number of categories. Experimental results on 16 datasets illustrate that our method integrated with previous approaches LM-BFF and P-tuning can yield better performance. Code is available in https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning. | Xiaozhuan Liang, Ningyu Zhang, Siyuan Cheng, Zhenru Zhang, Chuanqi Tan, Huajun Chen |  |
| 58 |  |  [Detect-Localize-Repair: A Unified Framework for Learning to Debug with CodeT5](https://doi.org/10.18653/v1/2022.findings-emnlp.57) |  | 0 | Automated software debugging is a crucial task for improving the productivity of software developers. Many neural-based techniques have been proven effective for debugging-related tasks such as bug localization and program repair (or bug fixing). However, these techniques often focus only on either one of them or approach them in a stage-wise manner, ignoring the mutual benefits between them. In this work, we propose a novel unified Detect-Localize-Repair framework based on a pretrained programming language model CodeT5 to seamlessly address these tasks, named CodeT5-DLR. Specifically, we propose three objectives to adapt the generic CodeT5 for debugging: a bug detection objective to determine whether a given code snippet is buggy or not, a bug localization objective to identify the buggy lines, and a program repair objective to translate the buggy code to its fixed version. We evaluate it on each of these tasks and their combined setting on two newly collected line-level debugging datasets in Java and Python. Extensive results show that our model significantly outperforms existing baselines from both NLP and software engineering domains. | Nghi Bui, Yue Wang, Steven C. H. Hoi |  |
| 59 |  |  [Influence Functions for Sequence Tagging Models](https://doi.org/10.18653/v1/2022.findings-emnlp.58) |  | 0 | Many standard tasks in NLP (e.g., Named Entity Recognition, Part-of-Speech tagging, and Semantic Role Labeling) are naturally framed as sequence tagging problems. However, there has been comparatively little work on interpretability methods for sequence tagging models. In this paper, we extend influence functions — which aim to trace predictions back to the training points that informed them — to sequence tagging tasks. We define the influence of a training instance segment as the effect that perturbing the labels within this segment has on a test segment level prediction. We provide an efficient approximation to compute this, and show that it tracks with the “true” segment influence (measured empirically). We show the practical utility of segment influence by using the method to identify noisy annotations in NER corpora. | Sarthak Jain, Varun Manjunatha, Byron C. Wallace, Ani Nenkova |  |
| 60 |  |  [Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.59) |  | 0 | Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above 70% (absolute) more accurate on the top 10% frequent terms in comparison to the bottom 10%. Overall, although LMs appear successful at few-shot numerical reasoning, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results. | Yasaman Razeghi, Robert L. Logan IV, Matt Gardner, Sameer Singh |  |
| 61 |  |  [Syntactic and Semantic Uniformity for Semantic Parsing and Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.60) |  | 0 | This paper proposes a data representation framework for semantic parsing and task-oriented dialogue systems, aiming to achieve a uniform representation for syntactically and semantically diverse machine-readable formats.Current NLP systems heavily rely on adapting pre-trained language models to specific tasks, and this approach has been proven effective for modeling natural language texts.However, little attention has been paid to the representation of machine-readable formats, such as database queries and dialogue states.We present a method for converting original machine-readable formats of semantic parsing and task-oriented dialogue datasets into a syntactically and semantically uniform representation.We define a meta grammar for syntactically uniform representations and translate semantically equivalent functions into a uniform vocabulary.Empirical experiments on 13 datasets show that accuracy consistently improves over original formats, revealing the advantage of the proposed representation.Additionally, we show that the proposed representation allows for transfer learning across datasets. | Bowen Chen, Yusuke Miyao |  |
| 62 |  |  [Knowledge-Rich Self-Supervision for Biomedical Entity Linking](https://doi.org/10.18653/v1/2022.findings-emnlp.61) |  | 0 | Entity linking faces significant challenges such as prolific variations and prevalent ambiguities, especially in high-value domains with myriad entities. Standard classification approaches suffer from the annotation bottleneck and cannot effectively handle unseen entities. Zero-shot entity linking has emerged as a promising direction for generalizing to new entities, but it still requires example gold entity mentions during training and canonical descriptions for all entities, both of which are rarely available outside of Wikipedia. In this paper, we explore Knowledge-RIch Self-Supervision (KRISS) for biomedical entity linking, by leveraging readily available domain knowledge. In training, it generates self-supervised mention examples on unlabeled text using a domain ontology and trains a contextual encoder using contrastive learning. For inference, it samples self-supervised mentions as prototypes for each entity and conducts linking by mapping the test mention to the most similar prototype. Our approach can easily incorporate entity descriptions and gold mention labels if available. We conducted extensive experiments on seven standard datasets spanning biomedical literature and clinical notes. Without using any labeled information, our method produces KRISSBERT, a universal entity linker for four million UMLS entities that attains new state of the art, outperforming prior self-supervised methods by as much as 20 absolute points in accuracy. We released KRISSBERT at https://aka.ms/krissbert. | Sheng Zhang, Hao Cheng, Shikhar Vashishth, Cliff Wong, Jinfeng Xiao, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon |  |
| 63 |  |  [ARTIST: A Transformer-based Chinese Text-to-Image Synthesizer Digesting Linguistic and World Knowledge](https://doi.org/10.18653/v1/2022.findings-emnlp.62) |  | 0 | Text-to-Image Synthesis (TIS) is a popular task to convert natural language texts into realistic images. Recently, transformer-based TIS models (such as DALL-E) have been proposed using the encoder-decoder architectures. Yet, these billion-scale TIS models are difficult to tune and deploy in resource-constrained environments. In addition, there is a lack of language-specific TIS benchmarks for Chinese, together with high-performing models with moderate sizes. In this work, we present ARTIST, A tRansformer-based Chinese Text-to-Image SynThesizer for high-resolution image generation. In ARTIST, the rich linguistic and relational knowledge facts are injected into the model to ensure better model performance without the usage of ultra-large models. We further establish a large-scale Chinese TIS benchmark with the re-production results of state-of-the-art transformer-based TIS models.Results show ARTIST outperforms previous approaches. | Tingting Liu, Chengyu Wang, Xiangru Zhu, Lei Li, Minghui Qiu, Jun Huang, Ming Gao, Yanghua Xiao |  |
| 64 |  |  [From Spelling to Grammar: A New Framework for Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2022.findings-emnlp.63) |  | 0 | Chinese Grammatical Error Correction (CGEC) aims to generate a correct sentence from an erroneous sequence, where different kinds of errors are mixed. This paper divides the CGEC task into two steps, namely spelling error correction and grammatical error correction. We firstly propose a novel zero-shot approach for spelling error correction, which is simple but effective, obtaining a high precision to avoid error accumulation of the pipeline structure. To handle grammatical error correction, we design part-of-speech (POS) features and semantic class features to enhance the neural network model, and propose an auxiliary task to predict the POS sequence of the target sentence. Our proposed framework achieves a 42.11 F-0.5 score on CGEC dataset without using any synthetic data or data augmentation methods, which outperforms the previous state-of-the-art by a wide margin of 1.30 points. Moreover, our model produces meaningful POS representations that capture different POS words and convey reasonable POS transition rules. | Xiuyu Wu, Yunfang Wu |  |
| 65 |  |  [Language Models Are Poor Learners of Directional Inference](https://doi.org/10.18653/v1/2022.findings-emnlp.64) |  | 0 | We examine LMs’ competence of directional predicate entailments by supervised fine-tuning with prompts. Our analysis shows that contrary to their apparent success on standard NLI, LMs show limited ability to learn such directional inference; moreover, existing datasets fail to test directionality, and/or are infested by artefacts that can be learnt as proxy for entailments, yielding over-optimistic results. In response, we present BoOQA (Boolean Open QA), a robust multi-lingual evaluation benchmark for directional predicate entailments, extrinsic to existing training sets. On BoOQA, we establish baselines and show evidence of existing LM-prompting models being incompetent directional entailment learners, in contrast to entailment graphs, however limited by sparsity. | Tianyi Li, Mohammad Javad Hosseini, Sabine Weber, Mark Steedman |  |
| 66 |  |  [Wish I Can Feel What You Feel: A Neural Approach for Empathetic Response Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.65) |  | 0 | Expressing empathy is important in everyday conversations, and exploring how empathy arises is crucial in automatic response generation. Most previous approaches consider only a single factor that affects empathy. However, in practice, empathy generation and expression is a very complex and dynamic psychological process. A listener needs to find out events which cause a speaker’s emotions (emotion cause extraction), project the events into some experience (knowledge extension), and express empathy in the most appropriate way (communication mechanism).To this end, we propose a novel approach, which integrates the three components - emotion cause, knowledge graph, and communication mechanism for empathetic response generation.Experimental results on the benchmark dataset demonstrate the effectiveness of our method and show that incorporating the key components generates more informative and empathetic responses. | Yangbin Chen, Chunfeng Liang |  |
| 67 |  |  [Measuring and Improving Semantic Diversity of Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.66) |  | 0 | Response diversity has become an important criterion for evaluating the quality of open-domain dialogue generation models. However, current evaluation metrics for response diversity often fail to capture the semantic diversity of generated responses, as they mainly consider lexical aspects of the generated responses. In this paper, we introduce a new automatic evaluation metric to measure the semantic diversity of generated responses. Through human evaluation, we demonstrate that our proposed metric captures human judgments on response diversity better than existing lexical-level diversity metrics. Furthermore, motivated by analyzing an existing dialogue dataset, we propose a simple yet effective learning method that improves the semantic diversity of generated responses. Our learning method weights training samples based on the semantic distribution of the training set.We show that our learning method improves response diversity and coherency better than other baseline methods through automatic and human evaluation. | Seungju Han, Beomsu Kim, Buru Chang |  |
| 68 |  |  [Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training](https://doi.org/10.18653/v1/2022.findings-emnlp.67) |  | 0 | Visual question answering (VQA) is a hallmark of vision and language reasoningand a challenging task under the zero-shot setting.We propose Plug-and-Play VQA (PNP-VQA),a modular framework for zero-shot VQA.In contrast to most existing works, which require substantial adaptation of pretrained language models (PLMs) for the vision modality,PNP-VQA requires no additional training of the PLMs.Instead, we propose to use natural language and network interpretation as an intermediate representation that glues pretrained models together. We first generate question-guided informative image captions,and pass the captions to a PLM as context for question answering.Surpassing end-to-end trained baselines, PNP-VQA achieves state-of-the-art results on zero-shot VQAv2 and GQA. With 11B parameters, it outperforms the 80B-parameter Flamingo model by 8.5% on VQAv2. With 738M PLM parameters, PNP-VQA achieves an improvement of 9.1% on GQA over FewVLM with 740M PLM parameters. | Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, Steven C. H. Hoi |  |
| 69 |  |  [TSGP: Two-Stage Generative Prompting for Unsupervised Commonsense Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.68) |  | 0 | Without training on labeled task data, unsupervised commonsense question answering seems challenging since it requires commonsense knowledge beyond the context of questions. Previous methods typically retrieved from traditional knowledge bases or used pre-trained language models (PrLMs) to generate fixed types of knowledge, which have poor generalization ability.In this paper, we aim to address the above limitation by leveraging the implicit knowledge stored in PrLMs and propose a two-stage prompt-based unsupervised commonsense question answering framework (TSGP). We first use knowledge generation prompts to generate the knowledge required for questions with unlimited types and possible candidate answers independent of specified choices. Then, we further utilize answer generation prompts to generate possible candidate answers independent of specified choices. Experimental results and analysis on three different commonsense reasoning tasks, CommonsenseQA, OpenBookQA, and SocialIQA, demonstrate that TSGP significantly improves the reasoning ability of language models in unsupervised settings. | Yueqing Sun, Yu Zhang, Le Qi, Qi Shi |  |
| 70 |  |  [Subword-Delimited Downsampling for Better Character-Level Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.69) |  | 0 | Subword-level models have been the dominant paradigm in NLP. However, character-level models have the benefit of seeing each character individually, providing the model with more detailed information that ultimately could lead to better models. Recent works have shown character-level models to be competitive with subword models, but costly in terms of time and computation. Character-level models with a downsampling component alleviate this, but at the cost of quality, particularly for machine translation. This work analyzes the problems of previous downsampling methods and introduces a novel downsampling method which is informed by subwords.This new downsampling method not only outperforms existing downsampling methods, showing that downsampling characters can be done without sacrificing quality, but also leads to promising performance compared to subword models for translation. | Lukas Edman, Antonio Toral, Gertjan van Noord |  |
| 71 |  |  [Autoregressive Structured Prediction with Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.70) |  | 0 | Recent years have seen a paradigm shift in NLP towards using pretrained language models (PLM) for a wide range of tasks. However, there are many difficult design decisions to represent structures (e.g. tagged text, coreference chains) in a way such that they can be captured by PLMs. Prior work on structured prediction with PLMs typically flattens the structured output into a sequence, which limits the quality of structural information being learned and leads to inferior performance compared to classic discriminative models. In this work, we describe an approach to model structures as sequences of actions in an autoregressive manner with PLMs, allowing in-structure dependencies to be learned without any loss. Our approach achieves the new state-of-the-art on all the structured prediction tasks we looked at, namely, named entity recognition, end-to-end relation extraction, and coreference resolution. | Tianyu Liu, Yuchen Eleanor Jiang, Nicholas Monath, Ryan Cotterell, Mrinmaya Sachan |  |
| 72 |  |  [XDoc: Unified Pre-training for Cross-Format Document Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.71) |  | 0 | The surge of pre-training has witnessed the rapid development of document understanding recently. Pre-training and fine-tuning framework has been effectively used to tackle texts in various formats, including plain texts, document texts, and web texts. Despite achieving promising performance, existing pre-trained models usually target one specific document format at one time, making it difficult to combine knowledge from multiple document formats. To address this, we propose XDoc, a unified pre-trained model which deals with different document formats in a single model. For parameter efficiency, we share backbone parameters for different formats such as the word embedding layer and the Transformer layers. Meanwhile, we introduce adaptive layers with lightweight parameters to enhance the distinction across different formats. Experimental results have demonstrated that with only 36.7% parameters, XDoc achieves comparable or even better performance on a variety of downstream tasks compared with the individual pre-trained models, which is cost effective for real-world deployment. The code and pre-trained models are publicly available at https://aka.ms/xdoc. | Jingye Chen, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei |  |
| 73 |  |  [A Few More Examples May Be Worth Billions of Parameters](https://doi.org/10.18653/v1/2022.findings-emnlp.72) |  | 0 | We investigate the dynamics of increasing the number of model parameters versus the number of labeled examples across a wide variety of tasks. Our exploration reveals that while scaling parameters consistently yields performance improvements, the contribution of additional examples highly depends on the task’s format. Specifically, in open question answering tasks, enlarging the training set does not improve performance. In contrast, classification, extractive question answering, and multiple choice tasks benefit so much from additional examples that collecting a few hundred examples is often “worth” billions of parameters. We hypothesize that unlike open question answering, which involves recalling specific information, solving strategies for tasks with a more restricted output space transfer across examples, and can therefore be learned with small amounts of labeled data. | Yuval Kirstain, Patrick Lewis, Sebastian Riedel, Omer Levy |  |
| 74 |  |  [MCP: Self-supervised Pre-training for Personalized Chatbots with Multi-level Contrastive Sampling](https://doi.org/10.18653/v1/2022.findings-emnlp.73) |  | 0 | Personalized chatbots focus on endowing the chatbots with a consistent personality to behave like real users and further act as personal assistants. Previous studies have explored generating implicit user profiles from the user’s dialogue history for building personalized chatbots. However, these studies only use the response generation loss to train the entire model, thus it is prone to suffer from the problem of data sparsity. Besides, they overemphasize the final generated response’s quality while ignoring the correlations and fusions between the user’s dialogue history, leading to rough data representations and performance degradation. To tackle these problems, we propose a self-supervised learning framework MCP for capturing better representations from users’ dialogue history for personalized chatbots. Specifically, we apply contrastive sampling methods to leverage the supervised signals hidden in user dialog history, and generate the pre-training samples for enhancing the model. We design three pre-training tasks based on three types of contrastive pairs from user dialogue history, namely response pairs, sequence augmentation pairs, and user pairs. We pre-train the utterance encoder and the history encoder towards the contrastive objectives and use these pre-trained encoders for generating user profiles while personalized response generation. Experimental results on two real-world datasets show a significant improvement in our proposed model MCP compared with the existing methods. | Zhaoheng Huang, Zhicheng Dou, Yutao Zhu, Zhengyi Ma |  |
| 75 |  |  [ExpertPLM: Pre-training Expert Representation for Expert Finding](https://doi.org/10.18653/v1/2022.findings-emnlp.74) |  | 0 | Expert Finding is an important task in Community Question Answering (CQA) platforms, which could help route questions to potential users to answer. The key is to learn representations of experts based on their historical answered questions accurately. In this paper, inspired by the strong text understanding ability of Pretrained Language modelings (PLMs), we propose a pre-training and fine-tuning expert finding framework. The core is that we design an expert-level pre-training paradigm, that effectively integrates expert interest and expertise simultaneously. Specifically different from the typical corpus-level pre-training, we treat each expert as the basic pre-training unit including all the historical answered question titles of the expert, which could fully indicate the expert interests for questions. Besides, we integrate the vote score information along with each answer of the expert into the pre-training phrase to model the expert ability explicitly. Finally, we propose a novel reputation-augmented Masked Language Model (MLM) pre-training strategy to capture the expert reputation information. In this way, our method could learn expert representation comprehensively, which then will be adopted and fine-tuned in the down-streaming expert-finding task. Extensive experimental results on six real-world CQA datasets demonstrate the effectiveness of our method. | Qiyao Peng, Hongtao Liu |  |
| 76 |  |  [You Truly Understand What I Need : Intellectual and Friendly Dialog Agents grounding Persona and Knowledge](https://doi.org/10.18653/v1/2022.findings-emnlp.75) |  | 0 | To build a conversational agent that interacts fluently with humans, previous studies blend knowledge or personal profile into the pre-trained language model. However, the model that considers knowledge and persona at the same time is still limited, leading to hallucination and a passive way of using personas. We propose an effective dialogue agent that grounds external knowledge and persona simultaneously. The agent selects the proper knowledge and persona to use for generating the answers with our candidate scoring implemented with a poly-encoder. Then, our model generates the utterance with lesser hallucination and more engagingness utilizing retrieval augmented generation with knowledge-persona enhanced query. We conduct experiments on the persona-knowledge chat and achieve state-of-the-art performance in grounding and generation tasks on the automatic metrics. Moreover, we validate the answers from the models regarding hallucination and engagingness through human evaluation and qualitative results. We show our retriever’s effectiveness in extracting relevant documents compared to the other previous retrievers, along with the comparison of multiple candidate scoring methods. Code is available at https://github.com/dlawjddn803/INFO | Jungwoo Lim, Myunghoon Kang, Yuna Hur, Seung Won Jeong, Jinsung Kim, Yoonna Jang, Dongyub Lee, Hyesung Ji, DongHoon Shin, Seungryong Kim, Heuiseok Lim |  |
| 77 |  |  [Faithful to the Document or to the World? Mitigating Hallucinations via Entity-Linked Knowledge in Abstractive Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.76) |  | 0 | Existing abstractive summarization systems are hampered by content hallucinations in which models generate text that is not directly inferable from the source alone. Annotations from prior work have shown that some of these hallucinations, while being ‘unfaithful’ to the source, are nonetheless factual. Our analysis in this paper suggests that these factual hallucinations occur as a result of the prevalence of factual yet unfaithful entities in summarization datasets. We find that these entities are not aberrations, but instead examples of additional world knowledge being readily used to latently connect entities and concepts – in this case connecting entities in the source document to those in the target summary. In our analysis and experiments, we demonstrate that connecting entities to an external knowledge base can lend provenance to many of these unfaithful yet factual entities, and further, this knowledge can be used to improve the factuality of summaries without simply making them more extractive. | Yue Dong, John Wieting, Pat Verga |  |
| 78 |  |  [RL with KL penalties is better viewed as Bayesian inference](https://doi.org/10.18653/v1/2022.findings-emnlp.77) |  | 0 | Reinforcement learning (RL) is frequently employed in fine-tuning large language models (LMs), such as GPT-3, to penalize them for undesirable features of generated sequences, such as offensiveness, social bias, harmfulness or falsehood. The RL formulation involves treating the LM as a policy and updating it to maximise the expected value of a reward function which captures human preferences, such as non-offensiveness. In this paper, we analyze challenges associated with treating a language model as an RL policy and show how avoiding those challenges requires moving beyond the RL paradigm. We start by observing that the standard RL approach is flawed as an objective for fine-tuning LMs because it leads to distribution collapse: turning the LM into a degenerate distribution. Then, we analyze KL-regularised RL, a widely used recipe for fine-tuning LMs, which additionally constrains the fine-tuned LM to stay close to its original distribution in terms of Kullback-Leibler (KL) divergence. We show that KL-regularised RL is equivalent to variational inference: approximating a Bayesian posterior which specifies how to update a prior LM to conform with evidence provided by the reward function. We argue that this Bayesian inference view of KL-regularised RL is more insightful than the typically employed RL perspective. The Bayesian inference view explains how KL-regularised RL avoids the distribution collapse problem and offers a first-principles derivation for its objective. While this objective happens to be equivalent to RL (with a particular choice of parametric reward), there exist other objectives for fine-tuning LMs which are no longer equivalent to RL. That observation leads to a more general point: RL is not an adequate formal framework for problems such as fine-tuning language models. These problems are best viewed as Bayesian inference: approximating a pre-defined target distribution. | Tomasz Korbak, Ethan Perez, Christopher L. Buckley |  |
| 79 |  |  [Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.78) |  | 0 | With the recent success of dense retrieval methods based on bi-encoders, studies have applied this approach to various interesting downstream retrieval tasks with good efficiency and in-domain effectiveness.Recently, we have also seen the presence of dense retrieval models in Math Information Retrieval (MIR) tasks,but the most effective systems remain classic retrieval methods that consider hand-crafted structure features.In this work, we try to combine the best of both worlds: a well-defined structure search method for effective formula search and efficient bi-encoder dense retrieval models to capture contextual similarities.Specifically, we have evaluated two representative bi-encoder models for token-level and passage-level dense retrieval on recent MIR tasks.Our results show that bi-encoder models are highly complementary to existing structure search methods, and we are able to advance the state-of-the-art on MIR datasets. | Wei Zhong, JhengHong Yang, Yuqing Xie, Jimmy Lin |  |
| 80 |  |  [Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem](https://doi.org/10.18653/v1/2022.findings-emnlp.79) |  | 0 | Math word problem solver requires both precise relation reasoning about quantities in the text and reliable generation for the diverse equation. Current sequence-to-tree or relation extraction methods regard this only from a fixed view, struggling to simultaneously handle complex semantics and diverse equations. However, human solving naturally involves two consistent reasoning views: top-down and bottom-up, just as math equations also can be expressed in multiple equivalent forms: pre-order and post-order. We propose a multi-view consistent contrastive learning for a more complete semantics-to-equation mapping. The entire process is decoupled into two independent but consistent views: top-down decomposition and bottom-up construction, and the two reasoning views are aligned in multi-granularity for consistency, enhancing global generation and precise reasoning. Experiments on multiple datasets across two languages show our approach significantly outperforms the existing baselines, especially on complex problems. We also show after consistent alignment, multi-view can absorb the merits of both views and generate more diverse results consistent with the mathematical laws. | Wenqi Zhang, Yongliang Shen, Yanna Ma, Xiaoxia Cheng, Zeqi Tan, Qingpeng Nong, Weiming Lu |  |
| 81 |  |  [Few-shot initializing of Active Learner via Meta-Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.80) |  | 0 | Despite the important evolutions in few-shot and zero-shot learning techniques, domain specific applications still require expert knowledge and significant effort in annotating and labeling a large volume of unstructured textual data. To mitigate this problem, active learning, and meta-learning attempt to reach a high performance with the least amount of labeled data. In this paper, we introduce a novel approach to combine both lines of work by initializing an active learner with meta-learned parameters obtained through meta-training on tasks similar to the target task during active learning. In this approach we use the pre-trained BERT as our text-encoder and meta-learn its parameters with LEOPARD, which extends the model-agnostic meta-learning method by generating task dependent softmax weights to enable learning across tasks with different number of classes. We demonstrate the effectiveness of our method by performing active learning on five natural language understanding tasks and six datasets with five different acquisition functions. We train two different meta-initializations, and we use the pre-trained BERT base initialization as baseline. We observe that our approach performs better than the baseline at low budget, especially when closely related tasks were present during meta-learning. Moreover, our results show that better performance in the initial phase, i.e., with fewer labeled samples, leads to better performance when larger acquisition batches are used. We also perform an ablation study of the proposed method, showing that active learning with only the meta-learned weights is beneficial and adding the meta-learned learning rates and generating the softmax have negative consequences for the performance. | Zi Long Zhu, Vikrant Yadav, Zubair Afzal, George Tsatsaronis |  |
| 82 |  |  [Bootstrapping meaning through listening: Unsupervised learning of spoken sentence embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.81) |  | 0 | Inducing semantic representations directly from speech signals is a highly challenging task but has many useful applications in speech mining and spoken language understanding. This study tackles the unsupervised learning of semantic representations for spoken utterances. Through converting speech signals into hidden units generated from acoustic unit discovery, we propose WavEmbed, a multimodal sequential autoencoder that predicts hidden units from a dense representation of speech. Secondly, we also propose S-HuBERT to induce meaning through knowledge distillation, in which a sentence embedding model is first trained on hidden units and passes its knowledge to a speech encoder through contrastive learning. The best performing model achieves a moderate correlation (0.5 0.6) with human judgments, without relying on any labels or transcriptions. Furthermore, these models can also be easily extended to leverage textual transcriptions of speech to learn much better speech embeddings that are strongly correlated with human annotations. Our proposed methods are applicable to the development of purely data-driven systems for speech mining, indexing and search. | Jian Zhu, Zuoyu Tian, Yadong Liu, Cong Zhang, ChiaWen Lo |  |
| 83 |  |  [Progressive Sentiment Analysis for Code-Switched Text Data](https://doi.org/10.18653/v1/2022.findings-emnlp.82) |  | 0 | Multilingual transformer language models have recently attracted much attention from researchers and are used in cross-lingual transfer learning for many NLP tasks such as text classification and named entity recognition.However, similar methods for transfer learning from monolingual text to code-switched text have not been extensively explored mainly due to the following challenges:(1) Code-switched corpus, unlike monolingual corpus, consists of more than one language and existing methods can’t be applied efficiently,(2) Code-switched corpus is usually made of resource-rich and low-resource languages and upon using multilingual pre-trained language models, the final model might bias towards resource-rich language. In this paper, we focus on code-switched sentiment analysis where we have a labelled resource-rich language dataset and unlabelled code-switched data. We propose a framework that takes the distinction between resource-rich and low-resource language into account.Instead of training on the entire code-switched corpus at once, we create buckets based on the fraction of words in the resource-rich language and progressively train from resource-rich language dominated samples to low-resource language dominated samples. Extensive experiments across multiple language pairs demonstrate that progressive training helps low-resource language dominated samples. | Sudhanshu Ranjan, Dheeraj Mekala, Jingbo Shang |  |
| 84 |  |  [Knowledge Stimulated Contrastive Prompting for Low-Resource Stance Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.83) |  | 0 | Stance Detection Task (SDT) aims at identifying the stance of the sentence towards a specific target and is usually modeled as a classification problem. Backgound knowledge is often necessary for stance detection with respect to a specific target, especially when there is no target explicitly mentioned in text. This paper focuses on the knowledge stimulation for low-resource stance detection tasks. We firstly explore to formalize stance detection as a prompt based contrastive learning task. At the same time, to make prompt learning suit to stance detection, we design a template mechanism to incorporate corresponding target into instance representation. Furthermore, we propose a masked language prompt joint contrastive learning approach to stimulate the knowledge inherit from the pre-trained model. The experimental results on three benchmarks show that knowledge stimulation is effective in stance detection accompanied with our proposed mechanism. | Kai Zheng, Qingfeng Sun, Yaming Yang, Fei Xu |  |
| 85 |  |  [WSpeller: Robust Word Segmentation for Enhancing Chinese Spelling Check](https://doi.org/10.18653/v1/2022.findings-emnlp.84) |  | 0 | Chinese spelling check (CSC) detects and corrects spelling errors in Chinese texts. Previous approaches have combined character-level phonetic and graphic information, ignoring the importance of segment-level information. According to our pilot study, spelling errors are always associated with incorrect word segmentation. When appropriate word boundaries are provided, CSC performance is greatly enhanced. Based on these findings, we present WSpeller, a CSC model that takes into account word segmentation. A fundamental component of WSpeller is a W-MLM, which is trained by predicting visually and phonetically similar words. Through modification of the embedding layer’s input, word segmentation information can be incorporated. Additionally, a robust module is trained to assist the W-MLM-based correction module by predicting the correct word segmentations from sentences containing spelling errors. We evaluate WSpeller on the widely used benchmark datasets SIGHAN13, SIGHAN14, and SIGHAN15. Our model is superior to state-of-the-art baselines on SIGHAN13 and SIGHAN15 and maintains equal performance on SIGHAN14. | Fangfang Li, Youran Shan, Junwen Duan, Xingliang Mao, Minlie Huang |  |
| 86 |  |  [Extracting Trigger-sharing Events via an Event Matrix](https://doi.org/10.18653/v1/2022.findings-emnlp.85) |  | 0 | A growing interest emerges in event extraction which aims to extract multiple events with triggers and arguments. Previous methods mitigate the problem of multiple events extraction by predicting the arguments conditioned on the event trigger and event type, assuming that these arguments belong to a single event. However, the assumption is invalid in general as there may be multiple events. Therefore, we present a unified framework called MatEE for trigger-sharing events extraction. It resolves the kernel bottleneck by effectively modeling the relations between arguments by an event matrix, where trigger-sharing events are represented by multiple cliques. We verify the proposed method on 3 widely-used benchmark datasets of event extraction. The experimental results show that it beats all the advanced competitors, significantly improving the state-of-the-art performances in event extraction. | Jun Xu, Weidi Xu, Mengshu Sun, Taifeng Wang, Wei Chu |  |
| 87 |  |  [TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.86) |  | 0 | Knowledge graph embedding (KGE) aims to learn continuous vector representations of relations and entities in knowledge graph (KG). Recently, transition-based KGE methods have become popular and achieved promising performance. However, scoring patterns like TransE are not suitable for complex scenarios where the same entity pair has different relations. Although some models attempt to employ entity-relation interaction or projection to improve entity representation for one-to-many/many-to-one/many-to-many complex relations, they still continue the traditional scoring pattern, where only a single relation vector in the relation part is used to translate the head entity to the tail entity or their variants. And recent research shows that entity representation only needs to consider entities and their interactions to achieve better performance. Thus, in this paper, we propose a novel transition-based method, TranS, for KGE. The single relation vector of the relation part in the traditional scoring pattern is replaced by the synthetic relation representation with entity-relation interactions to solve these issues. And the entity part still retains its independence through entity-entity interactions. Experiments on a large KG dataset, ogbl-wikikg2, show that our model achieves state-of-the-art results. | Xuanyu Zhang, Qing Yang, Dongliang Xu |  |
| 88 |  |  [Sequential Topic Selection Model with Latent Variable for Topic-Grounded Dialogue](https://doi.org/10.18653/v1/2022.findings-emnlp.87) |  | 0 | Recently, topic-grounded dialogue system has attracted significant attention due to its effectiveness in predicting the next topic to yield better responses via the historical context and given topic sequence. However, almost all existing topic prediction solutions focus on only the current conversation and corresponding topic sequence to predict the next conversation topic, without exploiting other topic-guided conversations which may contain relevant topic-transitions to current conversation. To address the problem, in this paper we propose a novel approach, named Sequential Global Topic Attention (SGTA) to exploit topic transition over all conversations in a subtle way for better modeling post-to-response topic-transition and guiding the response generation to the current conversation. Specifically, we introduce a latent space modeled as a Multivariate Skew-Normal distribution with hybrid kernel functions to flexibly integrate the global-level information with sequence-level information, and predict the topic based on the distribution sampling results. We also leverage a topic-aware prior-posterior approach for secondary selection of predicted topics, which is utilized to optimize the response generation task. Extensive experiments demonstrate that our model outperforms competitive baselines on prediction and generation tasks. | Xiaofei Wen, Wei Wei, XianLing Mao |  |
| 89 |  |  [Robust Task-Oriented Dialogue Generation with Contrastive Pre-training and Adversarial Filtering](https://doi.org/10.18653/v1/2022.findings-emnlp.88) |  | 0 | Data artifacts incentivize machine learning models to learn non-transferable generalizations by taking advantage of shortcuts in the data, andthere is growing evidence that data artifacts play a role for the strong results that deep learning models achieve in recent natural language processing benchmarks.In this paper, we focus on task-oriented dialogue and investigate whether popular datasets such as MultiWOZ contain such data artifacts.We found that by only keeping frequent phrases in the trainingexamples, state-of-the-art models perform similarly compared to the variant trained with full data, suggesting they exploit these spurious correlationsto solve the task. Motivated by this, we propose a contrastive learning based framework to encourage the model to ignore these cues and focus on learning generalisable patterns. We also experiment with adversarial filtering to remove easy training instances so that the model would focus on learning from the harder instances. We conduct a number of generalization experiments — e.g., cross-domain/dataset and adversarial tests — to assess the robustness of our approach and found that it works exceptionally well. | Shiquan Yang, Xinting Huang, Jey Han Lau, Sarah M. Erfani |  |
| 90 |  |  [STAR: SQL Guided Pre-Training for Context-dependent Text-to-SQL Parsing](https://doi.org/10.18653/v1/2022.findings-emnlp.89) |  | 0 | In this paper, we propose a novel SQL guided pre-training framework STAR for context-dependent text-to-SQL parsing, which leverages contextual information to enrich natural language (NL) utterance and table schema representations for text-to-SQL conversations. Concretely, we propose two novel pre-training objectives which respectively explore the context-dependent interactions of NL utterances and SQL queries within each text-to-SQL conversation: (i) schema state tracking (SST) objective that tracks and explores the schema states of context-dependent SQL queries in the form of schema-states by predicting and updating the value of each schema slot during interaction; (ii) utterance dependency tracking (UDT) objective that employs weighted contrastive learning to pull together two semantically similar NL utterances and push away the representations of semantically dissimilar NL utterances within each conversation. In addition, we construct a high-quality large-scale context-dependent text-to-SQL conversation corpus to pre-train STAR. Extensive experiments show that STAR achieves new state-of-the-art performance on two downstream benchmarks (SParC and CoSQL), significantly outperforming previous pre-training methods and ranking first on the leaderboard. We believe the release of the constructed corpus, codebase and pre-trained STAR checkpoints would push forward the research in this area. | Zefeng Cai, Xiangyu Li, Binyuan Hui, Min Yang, Bowen Li, Binhua Li, Zheng Cao, Weijie Li, Fei Huang, Luo Si, Yongbin Li |  |
| 91 |  |  [Is MultiWOZ a Solved Task? An Interactive TOD Evaluation Framework with User Simulator](https://doi.org/10.18653/v1/2022.findings-emnlp.90) |  | 0 | Task-Oriented Dialogue (TOD) systems are drawing more and more attention in recent studies.Current methods focus on constructing pre-trained models or fine-tuning strategies while the evaluation of TOD is limited by a policy mismatch problem.That is, during evaluation, the user utterances are from the annotated dataset while these utterances should interact with previous responses which can have many alternatives besides annotated texts.Therefore, in this work, we propose an interactive evaluation framework for TOD. We first build a goal-oriented user simulator based on pre-trained models and then use the user simulator to interact with the dialogue system to generate dialogues.Besides, we introduce a sentence-level and a session-level score to measure the sentence fluency and session coherence in the interactive evaluation. Experimental results show that RL-based TOD systems trained by our proposed user simulator can achieve nearly 98% inform and success rates in the interactive evaluation of MultiWOZ dataset and the proposed scores measure the response quality besides the inform and success rates.We are hoping that our work will encourage simulator-based interactive evaluations in the TOD task. | Qinyuan Cheng, Linyang Li, Guofeng Quan, Feng Gao, Xiaofeng Mou, Xipeng Qiu |  |
| 92 |  |  [Translating Hanja Historical Documents to Contemporary Korean and English](https://doi.org/10.18653/v1/2022.findings-emnlp.91) |  | 0 | The Annals of Joseon Dynasty (AJD) contain the daily records of the Kings of Joseon, the 500-year kingdom preceding the modern nation of Korea.The Annals were originally written in an archaic Korean writing system, ‘Hanja’, and were translated into Korean from 1968 to 1993.The resulting translation was however too literal and contained many archaic Korean words; thus, a new expert translation effort began in 2012. Since then, the records of only one king have been completed in a decade.In parallel, expert translators are working on English translation, also at a slow pace and produced only one king’s records in English so far.Thus, we propose H2KE, a neural machine translation model, that translates historical documents in Hanja to more easily understandable Korean and to English.Built on top of multilingual neural machine translation, H2KE learns to translate a historical document written in Hanja, from both a full dataset of outdated Korean translation and a small dataset of more recently translated contemporary Korean and English.We compare our method against two baselines:a recent model that simultaneously learns to restore and translate Hanja historical documentand a Transformer based model trained only on newly translated corpora.The experiments reveal that our method significantly outperforms the baselines in terms of BLEU scores for both contemporary Korean and English translations.We further conduct extensive human evaluation which shows that our translation is preferred over the original expert translations by both experts and non-expert Korean speakers. | Juhee Son, Jiho Jin, Haneul Yoo, JinYeong Bak, Kyunghyun Cho, Alice Oh |  |
| 93 |  |  [Exploring Compositional Image Retrieval with Hybrid Compositional Learning and Heuristic Negative Mining](https://doi.org/10.18653/v1/2022.findings-emnlp.92) |  | 0 | Compositional image retrieval (CIR) is a challenging retrieval task, where the query is composed of a reference image and a modification text, and the target is another image reflecting the modification to the reference image. Due to the great success of the pre-trained vision-and-language model CLIP and its favorable applicability to large-scale retrieval tasks, we propose a CIR model HyCoLe-HNM with CLIP as the backbone. In HyCoLe-HNM, we follow the contrastive pre-training method of CLIP to perform cross-modal representation learning. On this basis, we propose a hybrid compositional learning mechanism, which includes both image compositional learning and text compositional learning. In hybrid compositional learning, we borrow a gated fusion mechanism from a question answering model to perform compositional fusion, and propose a heuristic negative mining method to filter negative samples. Privileged information in the form of image-related texts is utilized in cross-modal representation learning and hybrid compositional learning. Experimental results show that HyCoLe-HNM achieves state-of-the-art performance on three CIR datasets, namely FashionIQ, Fashion200K, and MIT-States. | Chao Wang, Ehsan Nezhadarya, Tanmana Sadhu, Shengdong Zhang |  |
| 94 |  |  [Outlier Dimensions that Disrupt Transformers are Driven by Frequency](https://doi.org/10.18653/v1/2022.findings-emnlp.93) |  | 0 | While Transformer-based language models are generally very robust to pruning, there is the recently discovered outlier phenomenon: disabling only 48 out of 110M parameters in BERT-base drops its performance by nearly 30% on MNLI. We replicate the original evidence for the outlier phenomenon and we link it to the geometry of the embedding space. We find that in both BERT and RoBERTa the magnitude of hidden state coefficients corresponding to outlier dimensions correlate with the frequencies of encoded tokens in pre-training data, and they also contribute to the “vertical” self-attention pattern enabling the model to focus on the special tokens. This explains the drop in performance from disabling the outliers, and it suggests that to decrease anisotopicity in future models we need pre-training schemas that would better take into account the skewed token distributions. | Giovanni Puccetti, Anna Rogers, Aleksandr Drozd, Felice Dell'Orletta |  |
| 95 |  |  [MiST: a Large-Scale Annotated Resource and Neural Models for Functions of Modal Verbs in English Scientific Text](https://doi.org/10.18653/v1/2022.findings-emnlp.94) |  | 0 | Modal verbs (e.g., can, should or must) occur highly frequently in scientific articles. Decoding their function is not straightforward: they are often used for hedging, but they may also denote abilities and restrictions. Understanding their meaning is important for accurate information extraction from scientific text.To foster research on the usage of modals in this genre, we introduce the MIST (Modals In Scientific Text) dataset, which contains 3737 modal instances in five scientific domains annotated for their semantic, pragmatic, or rhetorical function. We systematically evaluate a set of competitive neural architectures on MIST. Transfer experiments reveal that leveraging non-scientific data is of limited benefit for modeling the distinctions in MIST. Our corpus analysis provides evidence that scientific communities differ in their usage of modal verbs, yet, classifiers trained on scientific data generalize to some extent to unseen scientific domains. | Sophie Henning, Nicole Macher, Stefan Grünewald, Annemarie Friedrich |  |
| 96 |  |  [Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts](https://doi.org/10.18653/v1/2022.findings-emnlp.95) |  | 0 | Prompt tuning is a parameter-efficient tuning (PETuning) method for utilizing pre-trained models (PTMs) that simply prepends a soft prompt to the input and only optimizes the prompt to adapt PTMs to downstream tasks. Although it is parameter- and deployment-efficient, its performance still lags behind other state-of-the-art PETuning methods. Besides, the training cost of prompt tuning is not significantly reduced due to the back-propagation through the entire model. Through empirical analyses, we shed some light on the lagging performance of prompt tuning and recognize a trade-off between the propagation distance from label signals to the inserted prompt and the influence of the prompt on model outputs. Further, we present Late Prompt Tuning (LPT) that inserts a late prompt into an intermediate layer of the PTM instead of the input layer or all layers. The late prompt is obtained by a neural prompt generator conditioned on the hidden states before the prompt insertion layer and therefore is instance-dependent. Through extensive experimental results across various tasks and PTMs, we show that LPT can achieve competitive performance to full model tuning and other PETuning methods under both full-data and few-shot scenarios while possessing faster training speed and lower memory cost. | Xiangyang Liu, Tianxiang Sun, Xuanjing Huang, Xipeng Qiu |  |
| 97 |  |  [MICO: A Multi-alternative Contrastive Learning Framework for Commonsense Knowledge Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.96) |  | 0 | Commonsense reasoning tasks such as commonsense knowledge graph completion and commonsense question answering require powerful representation learning. In this paper, we propose to learn commonsense knowledge representation by MICO, a Multi-alternative contrastIve learning framework on COmmonsense knowledge graphs (MICO). MICO generates the commonsense knowledge representation by contextual interaction between entity nodes and relations with multi-alternative contrastive learning. In MICO, the head and tail entities in an (h,r,t) knowledge triple are converted to two relation-aware sequence pairs (a premise and an alternative) in the form of natural language. Semantic representations generated by MICO can benefit the following two tasks by simply comparing the similarity score between the representations: 1) zero-shot commonsense question answering tasks; 2) inductive commonsense knowledge graph completion tasks. Extensive experiments show the effectiveness of our method. | Ying Su, Zihao Wang, Tianqing Fang, Hongming Zhang, Yangqiu Song, Tong Zhang |  |
| 98 |  |  [Leveraging Only the Category Name for Aspect Detection through Prompt-based Constrained Clustering](https://doi.org/10.18653/v1/2022.findings-emnlp.97) |  | 0 | Aspect category detection (ACD) aims to automatically identify user-concerned aspects from online reviews, which is of great value for evaluating the fine-grained performance of a product. The most recent solutions tackle this problem via weakly supervised methods, achieving remarkable improvement over unsupervised methods. However, a closer look at these methods reveals that the required human efforts are nontrivial and can sometimes be hard to obtain. In this study, we explore the possibility of minimizing human guidance while improving detection performance, with a deep clustering method that relies merely on the category name of each aspect and a pretrained language model (LM). The LM, combined with prompt techniques, is employed as a knowledge base to automatically generate constraints for clustering, as well as to provide a representation space to perform the clustering. Our method (1) extracts extensive keywords to expand our understanding of each aspect, (2) automatically generates instance-level and concept-level constraints for clustering, and (3) trains the clustering model with the above constraints. We demonstrate the capability of the proposed framework through extensive experiments on nine benchmark datasets. Our model not only performs noticeably better than existing unsupervised approaches but also considerably surpasses weakly supervised methods that require more human efforts. | Yazheng Li, Pengyun Wang, Yasheng Wang, Yong Dai, Yadao Wang, Lujia Pan, Zenglin Xu |  |
| 99 |  |  [Controllable Factuality in Document-Grounded Dialog Systems Using a Noisy Channel Model](https://doi.org/10.18653/v1/2022.findings-emnlp.98) |  | 0 | In this work, we present a model for document-grounded response generation in dialog that is decomposed into two components according to Bayes’ theorem.One component is a traditional ungrounded response generation model and the other component models the reconstruction of the grounding document based on the dialog context and generated response.We propose different approximate decoding schemes and evaluate our approach on multiple open-domain and task-oriented document-grounded dialog datasets.Our experiments show that the model is more factual in terms of automatic factuality metrics than the baseline model.Furthermore, we outline how introducing scaling factors between the components allows for controlling the tradeoff between factuality and fluency in the model output.Finally, we compare our approach to a recently proposed method to control factuality in grounded dialog, CTRL (Rashkin et al., 2021), and show that both approaches can be combined to achieve additional improvements. | Nico Daheim, David Thulke, Christian Dugast, Hermann Ney |  |
| 100 |  |  [Transformer Language Models without Positional Encodings Still Learn Positional Information](https://doi.org/10.18653/v1/2022.findings-emnlp.99) |  | 0 | Causal transformer language models (LMs), such as GPT-3, typically require some form of positional encoding, such as positional embeddings. However, we show that LMs without any explicit positional encoding are still competitive with standard models and that this phenomenon is robust across different datasets, model sizes, and sequence lengths.Probing experiments reveal that such models acquire an implicit notion of absolute positions throughout the network, effectively compensating for the missing information.We conjecture that causal attention enables the model to infer the number of predecessors that each token can attend to, thereby approximating its absolute position.Our findings indicate that causal LMs might derive positional awareness not only from the explicit positioning mechanism but also from the effects of the causal mask. | Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, Omer Levy |  |
| 101 |  |  [Beyond Model Interpretability: On the Faithfulness and Adversarial Robustness of Contrastive Textual Explanations](https://doi.org/10.18653/v1/2022.findings-emnlp.100) |  | 0 | Contrastive explanation methods go beyond transparency and address the contrastive aspect of explanations. Such explanations are emerging as an attractive option to provide actionable change to scenarios adversely impacted by classifiers’ decisions. However, their extension to textual data is under-explored and there is little investigation on their vulnerabilities and limitations. This work motivates textual counterfactuals by highlighting the social limitations of non-contrastive explainability. We also lay the ground for a novel evaluation scheme inspired by the faithfulness of explanations. Accordingly, we extend the computation of three metrics, proximity, connectedness and stability, to textual data and we benchmark two successful contrastive methods, POLYJUICE and MiCE, on our suggested metrics. Experiments on sentiment analysis data show that the connectedness of counterfactuals to their original counterparts is not obvious in both models. More interestingly, the generated contrastive texts are more attainable with POLYJUICE which highlights the significance of latent representations in counterfactual search. Finally, we perform the first semantic adversarial attack on textual recourse methods. The results demonstrate the robustness of POLYJUICE and the role that latent input representations play in robustness and reliability. | Julia El Zini, Mariette Awad |  |
| 102 |  |  [How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers](https://doi.org/10.18653/v1/2022.findings-emnlp.101) |  | 0 | The attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language models. We introduce PAPA, a new probing method that replaces the input-dependent attention matrices with constant ones—the average attention weights over multiple inputs. We use PAPA to analyze several established pretrained Transformers on six downstream tasks. We find that without any input-dependent attention, all models achieve competitive performance—an average relative drop of only 8% from the probing baseline. Further, little or no performance drop is observed when replacing half of the input-dependent attention matrices with constant (input-independent) ones. Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success. Our results motivate research on simpler alternatives to input-dependent attention, as well as on methods for better utilization of this mechanism in the Transformer architecture. | Michael Hassid, Hao Peng, Daniel Rotem, Jungo Kasai, Ivan Montero, Noah A. Smith, Roy Schwartz |  |
| 103 |  |  [What Has Been Enhanced in my Knowledge-Enhanced Language Model?](https://doi.org/10.18653/v1/2022.findings-emnlp.102) |  | 0 | A number of knowledge integration (KI) methods have recently been proposed to incorporate external knowledge into pretrained language models (LMs). Even though knowledge-enhanced LMs (KELMs) outperform base LMs on knowledge-intensive tasks, the inner-workings of these KI methods are not well-understood. For instance, it is unclear which knowledge is effectively integrated into KELMs and which is not; and if such integration led to catastrophic forgetting of already learned knowledge. We show that existing model interpretation methods such as linear probes and prompts have some key limitations in answering these questions. Then, we revisit KI from an information-theoretic view and propose a new theoretically sound probe model called Graph Convolution Simulator (GCS) for KI interpretation. GCS is eventually quite simple – it uses graph attention on the corresponding knowledge graph for interpretation.We conduct various experiments to verify that GCS provides reasonable interpretation results for two well-known KELMs: ERNIE and K-Adapter. Our experiments reveal that only little knowledge is successfully integrated in these models, and simply increasing the size of the KI corpus may not lead to better KELMs. | Yifan Hou, Guoji Fu, Mrinmaya Sachan |  |
| 104 |  |  [Towards Generalized Open Information Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.103) |  | 0 | Open Information Extraction (OpenIE) facilitates the open-domain discovery of textual facts. However, the prevailing solutions evaluate OpenIE models on in-domain test sets aside from the training corpus, which certainly violates the initial task principle of domain-independence. In this paper, we propose to advance OpenIE towards a more realistic scenario: generalizing over unseen target domains with different data distributions from the source training domains, termed Generalized OpenIE. For this purpose, we first introduce GLOBE, a large-scale human-annotated multi-domain OpenIE benchmark, to examine the robustness of recent OpenIE models to domain shifts, and the relative performance degradation of up to 70% implies the challenges of generalized OpenIE. Then, we propose DragonIE, which explores a minimalist expression of textual fact: directed acyclic graph, to improve the OpenIE generalization ability. Extensive experiments demonstrate that DragonIE beats the previous methods in both in-domain and out-of-domain settings by as much as 6.0% in F1 score absolutely, but there is still ample room for improvement. |  |  |
| 105 |  |  [BioLORD: Learning Ontological Representations from Definitions for Biomedical Concepts and their Textual Descriptions](https://doi.org/10.18653/v1/2022.findings-emnlp.104) |  | 0 | This work introduces BioLORD, a new pre-training strategy for producing meaningful representations for clinical sentences and biomedical concepts. State-of-the-art methodologies operate by maximizing the similarity in representation of names referring to the same concept, and preventing collapse through contrastive learning. However, because biomedical names are not always self-explanatory, it sometimes results in non-semantic representations. BioLORD overcomes this issue by grounding its concept representations using definitions, as well as short descriptions derived from a multi-relational knowledge graph consisting of biomedical ontologies. Thanks to this grounding, our model produces more semantic concept representations that match more closely the hierarchical structure of ontologies. BioLORD establishes a new state of the art for text similarity on both clinical sentences (MedSTS) and biomedical concepts (MayoSRS). | François Remy, Kris Demuynck, Thomas Demeester |  |
| 106 |  |  [Improving the Extraction of Supertags for Constituency Parsing with Linear Context-Free Rewriting Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.105) |  | 0 | In parsing phrase structures, supertagging achieves a symbiosis between the interpretability of formal grammars and the accuracy and speed of more recent neural models.The approach was only recently transferred to parsing discontinuous constituency structures with linear context-free rewriting systems (LCFRS).We reformulate and parameterize the previously fixed extraction process for LCFRS supertags with the aim to improve the overall parsing quality.These parameters are set in the context of several steps in the extraction process and are used to control the granularity of extracted grammar rules as well as the association of lexical symbols with each supertag.We evaluate the influence of the parameters on the sets of extracted supertags and the parsing quality using three treebanks in the English and German language, and we compare the best-performing configurations to recent state-of-the-art parsers in the area.Our results show that some of our configurations and the slightly modified parsing process improve the quality and speed of parsing with our supertags over the previous approach.Moreover, we achieve parsing scores that either surpass or are among the state-of-the-art in discontinuous constituent parsing. | Thomas Ruprecht |  |
| 107 |  |  [Mask More and Mask Later: Efficient Pre-training of Masked Language Models by Disentangling the [MASK] Token](https://doi.org/10.18653/v1/2022.findings-emnlp.106) |  | 0 | The pre-training of masked language models (MLMs) consumes massive computation to achieve good results on downstream NLP tasks, resulting in a large carbon footprint. In the vanilla MLM, the virtual tokens, [MASK]s, act as placeholders and gather the contextualized information from unmasked tokens to restore the corrupted information. It raises the question of whether we can append [MASK]s at a later layer, to reduce the sequence length for earlier layers and make the pre-training more efficient. We show: (1) [MASK]s can indeed be appended at a later layer, being disentangled from the word embedding; (2) The gathering of contextualized information from unmasked tokens can be conducted with a few layers. By further increasing the masking rate from 15% to 50%, we can pre-train RoBERTa-base and RoBERTa-large from scratch with only 78% and 68% of the original computational budget without any degradation on the GLUE benchmark. When pre-training with the original budget, our method outperforms RoBERTa for 6 out of 8 GLUE tasks, on average by 0.4%. | Baohao Liao, David Thulke, Sanjika Hewavitharana, Hermann Ney, Christof Monz |  |
| 108 |  |  [SMSMix: Sense-Maintained Sentence Mixup for Word Sense Disambiguation](https://doi.org/10.18653/v1/2022.findings-emnlp.107) |  | 0 | Word Sense Disambiguation (WSD) is an NLP task aimed at determining the correct sense of a word in a sentence from discrete sense choices. Although current systems have attained unprecedented performances for such tasks, the nonuniform distribution of word senses during training generally results in systems performing poorly on rare senses. To this end, we consider data augmentation to increase the frequency of these least frequent senses (LFS) to reduce the distributional bias of senses during training. We propose Sense-Maintained Sentence Mixup (SMSMix), a novel word-level mixup method that maintains the sense of a target word. SMSMix smoothly blends two sentences using mask prediction while preserving the relevant span determined by saliency scores to maintain a specific word’s sense. To the best of our knowledge, this is the first attempt to apply mixup in NLP while preserving the meaning of a specific word. With extensive experiments, we validate that our augmentation method can effectively give more information about rare senses during training with maintained target sense label. | Hee Suk Yoon, Eunseop Yoon, John B. Harvill, Sunjae Yoon, Mark HasegawaJohnson, Chang Dong Yoo |  |
| 109 |  |  [On the Effectiveness of Automated Metrics for Text Generation Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.108) |  | 0 | A major challenge in the field of Text Generation is evaluation, because we lack a sound theory that can be leveraged to extract guidelines for evaluation campaigns. In this work, we propose a first step towards such a theory that incorporates different sources of uncertainty, such as imperfect automated metrics and insufficiently sized test sets. The theory has practical applications, such as determining the number of samples needed to reliably distinguish the performance of a set of Text Generation systems in a given setting. We showcase the application of the theory on the WMT 21 and Spot-The-Bot evaluation data and outline how it can be leveraged to improve the evaluation protocol regarding the reliability, robustness, and significance of the evaluation outcome. | Pius von Däniken, Jan Deriu, Don Tuggener, Mark Cieliebak |  |
| 110 |  |  [Residual Learning of Neural Text Generation with n-gram Language Model](https://doi.org/10.18653/v1/2022.findings-emnlp.109) |  | 0 | N-gram language models (LM) has been largely superseded by neural LMs as the latter exhibits better performance. However, we find that n-gram models can achieve satisfactory performance on a large proportion of testing cases, indicating they have already captured abundant knowledge of the language with relatively low computational cost. With this observation, we propose to learn a neural LM that fits the residual between an n-gram LM and the real-data distribution. The combination of n-gram LMs and neural LMs not only allows the neural part to focus on deeper understanding of the language, but also provides a flexible way to customize a LM by switching the underlying n-gram model without changing the neural model. Experimental results on three typical language tasks (i.e., language modeling, machine translation, and summarization) demonstrate that our approach attains additional performance gains over popular standalone neural models consistently. We also show that our approach allows for effective domain adaptation by simply switching to a domain-specific n-gram model, without any extra training. | Huayang Li, Deng Cai, Jin Xu, Taro Watanabe |  |
| 111 |  |  [DiffG-RL: Leveraging Difference between Environment State and Common Sense](https://doi.org/10.18653/v1/2022.findings-emnlp.110) |  | 0 | Taking into account background knowledge as the context has always been an important part of solving tasks that involve natural language. One representative example of such tasks is text-based games, where players need to make decisions based on both description text previously shown in the game, and their own background knowledge about the language and common sense. In this work, we investigate not simply giving common sense, as can be seen in prior research, but also its effective usage. We assume that a part of the environment states different from common sense should constitute one of the grounds for action selection. We propose a novel agent, DiffG-RL, which constructs a Difference Graph that organizes the environment states and common sense by means of interactive objects with a dedicated graph encoder. DiffG-RL also contains a framework for extracting the appropriate amount and representation of common sense from the source to support the construction of the graph. We validate DiffG-RL in experiments with text-based games that require common sense and show that it outperforms baselines by 17% of scores. We will make our code publicly available. | Tsunehiko Tanaka, Daiki Kimura, Michiaki Tatsubori |  |
| 112 |  |  [Unsupervised Syntactically Controlled Paraphrase Generation with Abstract Meaning Representations](https://doi.org/10.18653/v1/2022.findings-emnlp.111) |  | 0 | Syntactically controlled paraphrase generation has become an emerging research direction in recent years. Most existing approaches require annotated paraphrase pairs for training and are thus costly to extend to new domains. Unsupervised approaches, on the other hand, do not need paraphrase pairs but suffer from relatively poor performance in terms of syntactic control and quality of generated paraphrases. In this paper, we demonstrate that leveraging Abstract Meaning Representations (AMR) can greatly improve the performance of unsupervised syntactically controlled paraphrase generation.Our proposed model, AMR-enhanced Paraphrase Generator (AMRPG), separately encodes the AMR graph and the constituency parse of the input sentence into two disentangled semantic and syntactic embeddings. A decoder is then learned to reconstruct the input sentence from the semantic and syntactic embeddings. Our experiments show that AMRPG generates more accurate syntactically controlled paraphrases, both quantitatively and qualitatively, compared to the existing unsupervised approaches. We also demonstrate that the paraphrases generated by AMRPG can be used for data augmentation to improve the robustness of NLP models. | KuanHao Huang, Varun Iyer, Anoop Kumar, Sriram Venkatapathy, KaiWei Chang, Aram Galstyan |  |
| 113 |  |  [Can AMR Assist Legal and Logical Reasoning?](https://doi.org/10.18653/v1/2022.findings-emnlp.112) |  | 0 | Abstract Meaning Representation (AMR) has been shown to be useful for many downstream tasks. In this work, we explore the use of AMR for legal and logical reasoning. Specifically, we investigate if AMR can help capture logical relationships on multiple choice question answering (MCQA) tasks. We propose neural architectures that utilize linearised AMR graphs in combination with pre-trained language models. While these models are not able to outperform text-only baselines, they correctly solve different instances than the text models, suggesting complementary abilities. Error analysis further reveals that AMR parsing quality is the most prominent challenge, especially regarding inputs with multiple sentences. We conduct a theoretical analysis of how logical relations are represented in AMR and conclude it might be helpful in some logical statements but not for others. | Nikolaus Schrack, Ruixiang Cui, Hugo López, Daniel Hershcovich |  |
| 114 |  |  [Data Selection Curriculum for Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.113) |  | 0 | Neural Machine Translation (NMT) models are typically trained on heterogeneous data that are concatenated and randomly shuffled. However, not all of the training data are equally useful to the model. Curriculum training aims to present the data to the NMT models in a meaningful order. In this work, we introduce a two-stage training framework for NMT where we fine-tune a base NMT model on subsets of data, selected by both deterministic scoring using pre-trained methods and online scoring that considers prediction scores of the emerging NMT model. Through comprehensive experiments on six language pairs comprising low- and high-resource languages from WMT’21, we have shown that our curriculum strategies consistently demonstrate better quality (up to +2.2 BLEU improvement) and faster convergence (approximately 50% fewer updates). | Tasnim Mohiuddin, Philipp Koehn, Vishrav Chaudhary, James Cross, Shruti Bhosale, Shafiq R. Joty |  |
| 115 |  |  [Text Editing as Imitation Game](https://doi.org/10.18653/v1/2022.findings-emnlp.114) |  | 0 | Text editing, such as grammatical error correction, arises naturally from imperfect textual data. Recent works frame text editing as a multi-round sequence tagging task, where operations – such as insertion and substitution – are represented as a sequence of tags. While achieving good results, this encoding is limited in flexibility as all actions are bound to token-level tags. In this work, we reformulate text editing as an imitation game using behavioral cloning. Specifically, we convert conventional sequence-to-sequence data into state-to-action demonstrations, where the action space can be as flexible as needed. Instead of generating the actions one at a time, we introduce a dual decoders structure to parallel the decoding while retaining the dependencies between action tokens, coupled with trajectory augmentation to alleviate the distribution shift that imitation learning often suffers. In experiments on a suite of Arithmetic Equation benchmarks, our model consistently outperforms the autoregressive baselines in terms of performance, efficiency, and robustness. We hope our findings will shed light on future studies in reinforcement learning applying sequence-level action generation to natural language processing. | Ning Shi, Bin Tang, Bo Yuan, Longtao Huang, Yewen Pu, Jie Fu, Zhouhan Lin |  |
| 116 |  |  [Seeded Hierarchical Clustering for Expert-Crafted Taxonomies](https://doi.org/10.18653/v1/2022.findings-emnlp.115) |  | 0 | Practitioners from many disciplines (e.g., political science) use expert-crafted taxonomies to make sense of large, unlabeled corpora. In this work, we study Seeded Hierarchical Clustering (SHC): the task of automatically fitting unlabeled data to such taxonomies using a small set of labeled examples. We propose HierSeed, a novel weakly supervised algorithm for this task that uses only a small set of labeled seed examples in a computation and data efficient manner. HierSeed assigns documents to topics by weighing document density against topic hierarchical structure. It outperforms unsupervised and supervised baselines for the SHC task on three real-world datasets. | Anish Saha, Amith Ananthram, Emily Allaway, Heng Ji, Kathleen R. McKeown |  |
| 117 |  |  [Knowledge Graph Generation From Text](https://doi.org/10.18653/v1/2022.findings-emnlp.116) |  | 0 | In this work we propose a novel end-to-end multi-stage Knowledge Graph (KG) generation system from textual inputs, separating the overall process into two stages. The graph nodes are generated first using pretrained language model, followed by a simple edge construction head, enabling efficient KG extraction from the text. For each stage we consider several architectural choices that can be used depending on the available training resources. We evaluated the model on a recent WebNLG 2020 Challenge dataset, matching the state-of-the-art performance on text-to-RDF generation task, as well as on New York Times (NYT) and a large-scale TekGen datasets, showing strong overall performance, outperforming the existing baselines. We believe that the proposed system can serve as a viable KG construction alternative to the existing linearization or sampling-based graph generation approaches. | Igor Melnyk, Pierre L. Dognin, Payel Das |  |
| 118 |  |  [DialogueGAT: A Graph Attention Network for Financial Risk Prediction by Modeling the Dialogues in Earnings Conference Calls](https://doi.org/10.18653/v1/2022.findings-emnlp.117) |  | 0 | Financial risk prediction is an essential task for risk management in capital markets. While traditional prediction models are built based on the hard information of numerical data, recent studies have shown that the soft information of verbal cues in earnings conference calls is significant for predicting market risk due to its less constrained fashion and direct interaction between managers and analysts. However, most existing models mainly focus on extracting useful semantic information from the textual conference call transcripts but ignore their subtle yet important information of dialogue structures. To bridge this gap, we develop a graph attention network called DialogueGAT for financial risk prediction by simultaneously modeling the speakers and their utterances in dialogues in conference calls. Different from previous studies, we propose a new method for constructing the graph of speakers and utterances in a dialogue, and design contextual attention at both speaker and utterance levels for disentangling their effects on the downstream prediction task. For model evaluation, we extend an existing dataset of conference call transcripts by adding the dialogue structure and speaker information. Empirical results on our dataset of S&P1500 companies demonstrate the superiority of our proposed model over competitive baselines from the extant literature. | Yunxin Sang, Yang Bao |  |
| 119 |  |  [Investigating Ensemble Methods for Model Robustness Improvement of Text Classifiers](https://doi.org/10.18653/v1/2022.findings-emnlp.118) |  | 0 | Large pre-trained language models have shown remarkable performance over the past few years. These models, however, sometimes learn superficial features from the dataset and cannot generalize to the distributions that are dissimilar to the training scenario. There have been several approaches proposed to reduce model’s reliance on these bias features which can improve model robustness in the out-of-distribution setting. However, existing methods usually use a fixed low-capacity model to deal with various bias features, which ignore the learnability of those features. In this paper, we analyze a set of existing bias features and demonstrate there is no single model that works best for all the cases. We further show that by choosing an appropriate bias model, we can obtain a better robustness result than baselines with a more sophisticated model design. | Jieyu Zhao, Xuezhi Wang, Yao Qin, Jilin Chen, KaiWei Chang |  |
| 120 |  |  [Adaptive Ranking-based Sample Selection for Weakly Supervised Class-imbalanced Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.119) |  | 0 | To obtain a large amount of training labels inexpensively, researchers have recently adopted the weak supervision (WS) paradigm, which leverages labeling rules to synthesize training labels rather than using individual annotations to achieve competitive results for natural language processing (NLP) tasks. However, data imbalance is often overlooked in applying the WS paradigm, despite being a common issue in a variety of NLP tasks. To address this challenge, we propose Adaptive Ranking-based Sample Selection (ARS2), a model-agnostic framework to alleviate the data imbalance issue in the WS paradigm. Specifically, it calculates a probabilistic margin score based on the output of the current model to measure and rank the cleanliness of each data point. Then, the ranked data are sampled based on both class-wise and rule-aware ranking. In particular, the two sample strategies corresponds to our motivations: (1) to train the model with balanced data batches to reduce the data imbalance issue and (2) to exploit the expertise of each labeling rule for collecting clean samples. Experiments on four text classification datasets with four different imbalance ratios show that ARS2 outperformed the state-of-the-art imbalanced learning and WS methods, leading to a 2%-57.8% improvement on their F1-score. | Linxin Song, Jieyu Zhang, Tianxiang Yang, Masayuki Goto |  |
| 121 |  |  [ComFact: A Benchmark for Linking Contextual Commonsense Knowledge](https://doi.org/10.18653/v1/2022.findings-emnlp.120) |  | 0 | Understanding rich narratives, such as dialogues and stories, often requires natural language processing systems to access relevant knowledge from commonsense knowledge graphs. However, these systems typically retrieve facts from KGs using simple heuristics that disregard the complex challenges of identifying situationally-relevant commonsense knowledge (e.g., contextualization, implicitness, ambiguity).In this work, we propose the new task of commonsense fact linking, where models are given contexts and trained to identify situationally-relevant commonsense knowledge from KGs. Our novel benchmark, ComFact, contains ~293k in-context relevance annotations for commonsense triplets across four stylistically diverse dialogue and storytelling datasets. Experimental results confirm that heuristic fact linking approaches are imprecise knowledge extractors. Learned fact linking models demonstrate across-the-board performance improvements (~34.6% F1) over these heuristics. Furthermore, improved knowledge retrieval yielded average downstream improvements of 9.8% for a dialogue response generation task. However, fact linking models still significantly underperform humans, suggesting our benchmark is a promising testbed for research in commonsense augmentation of NLP systems. | Silin Gao, Jena D. Hwang, Saya Kanno, Hiromi Wakaki, Yuki Mitsufuji, Antoine Bosselut |  |
| 122 |  |  [Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.121) |  | 0 | How to usefully encode compositional task structure has long been a core challenge in AI. Recent work in chain of thought prompting has shown that for very large neural language models (LMs), explicitly demonstrating the inferential steps involved in a target task may improve performance over end-to-end learning that focuses on the target task alone. However, chain of thought prompting has significant limitations due to its dependency on huge pretrained LMs. In this work, we present compositional fine-tuning (CFT): an approach based on explicitly decomposing a target task into component tasks, and then fine-tuning smaller LMs on a curriculum of such component tasks. We apply CFT to recommendation tasks in two domains, world travel and local dining, as well as a previously studied inferential task (sports understanding). We show that CFT outperforms end-to-end learning even with equal amounts of data, and gets consistently better as more component tasks are modeled via fine-tuning. Compared with chain of thought prompting, CFT performs at least as well using LMs only 7.4% of the size, and is moreover applicable to task domains for which data are not available during pretraining. | Victor S. Bursztyn, David Demeter, Doug Downey, Larry Birnbaum |  |
| 123 |  |  [Topic Taxonomy Expansion via Hierarchy-Aware Topic Phrase Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.122) |  | 0 | Topic taxonomies display hierarchical topic structures of a text corpus and provide topical knowledge to enhance various NLP applications. To dynamically incorporate new topic information, several recent studies have tried to expand (or complete) a topic taxonomy by inserting emerging topics identified in a set of new documents. However, existing methods focus only on frequent terms in documents and the local topic-subtopic relations in a taxonomy, which leads to limited topic term coverage and fails to model the global taxonomy structure. In this work, we propose a novel framework for topic taxonomy expansion, named TopicExpan, which directly generates topic-related terms belonging to new topics. Specifically, TopicExpan leverages the hierarchical relation structure surrounding a new topic and the textual content of an input document for topic term generation. This approach encourages newly-inserted topics to further cover important but less frequent terms as well as to keep their relation consistency within the taxonomy. Experimental results on two real-world text corpora show that TopicExpan significantly outperforms other baseline methods in terms of the quality of output taxonomies. | Dongha Lee, Jiaming Shen, Seonghyeon Lee, Susik Yoon, Hwanjo Yu, Jiawei Han |  |
| 124 |  |  [Language as a fingerprint: Self-supervised learning of user encodings using transformers](https://doi.org/10.18653/v1/2022.findings-emnlp.123) |  | 0 | The way we talk carries information about who we are. Demographics, personality, clinical conditions, political preferences influence what we speak about and how, suggesting that many individual attributes could be inferred from adequate encodings of linguistic behavior. Conversely, conditioning text representations on author attributes has been shown to improve model performance in many NLP tasks. Previous research on individual differences and language representations has mainly focused on predicting selected attributes from text, or on conditioning text representations on such attributes for author-based contextualization. Here, we present a self-supervised approach to learning language-based user encodings using transformers. Using a large corpus of Reddit submissions, we fine-tune DistilBERT on user-based triplet loss. We show that fine-tuned models can pick up on complex linguistic signatures of users, and that they are able to infer rich information about them. Through a series of intrinsic analyses and probing tasks, we provide evidence that fine-tuning enhances models’ ability to abstract generalizable user information, which yields performance advantages for user-based downstream tasks. We discuss applications in language-based assessment and contextualized and personalized NLP. | Roberta Rocca, Tal Yarkoni |  |
| 125 |  |  [Hyperdecoders: Instance-specific decoders for multi-task NLP](https://doi.org/10.18653/v1/2022.findings-emnlp.124) |  | 0 | We investigate input-conditioned hypernetworks for multi-tasking in NLP, generating parameter-efficient adaptations for a decoder using a hypernetwork conditioned on the output of an encoder. This approach produces a unique decoder adaptation for every input instance, allowing the network a larger degree of flexibility than prior work that only produces one decoder adaptation per task. We apply our method to sequence classification tasks, extractive QA, and summarisation and find that it surpasses previous parameter efficient fine-tuning methods and often outperforms fully finetuning the underlying model. An analysis of the embeddings used by our hypernetwork shows that they are sensitive to output label and type, suggesting that our approach better maps from encoder representations to output labels. Our code is publicly available at https://github.com/allenai/hyperdecoders. | Hamish Ivison, Matthew E. Peters |  |
| 126 |  |  [Evaluating the Faithfulness of Importance Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining](https://doi.org/10.18653/v1/2022.findings-emnlp.125) |  | 0 | To explain NLP models a popular approach is to use importance measures, such as attention, which inform input tokens are important for making a prediction. However, an open question is how well these explanations accurately reflect a model’s logic, a property called faithfulness. To answer this question, we propose Recursive ROAR, a new faithfulness metric. This works by recursively masking allegedly important tokens and then retraining the model. The principle is that this should result in worse model performance compared to masking random tokens. The result is a performance curve given a masking-ratio. Furthermore, we propose a summarizing metric using area-between-curves (ABC), which allows for easy comparison across papers, models, and tasks. We evaluate 4 different importance measures on 8 different datasets, using both LSTM-attention models and RoBERTa models. We find that the faithfulness of importance measures is both model-dependent and task-dependent. This conclusion contradicts previous evaluations in both computer vision and faithfulness of attention literature. | Andreas Madsen, Nicholas Meade, Vaibhav Adlakha, Siva Reddy |  |
| 127 |  |  [Towards Explaining Subjective Ground of Individuals on Social Media](https://doi.org/10.18653/v1/2022.findings-emnlp.126) |  | 0 | Large-scale language models have been reducing the gap between machines and humans in understanding the real world, yet understanding an individual’s theory of mind and behavior from text is far from being resolved. This research proposes a neural model—Subjective Ground Attention—that learns subjective grounds of individuals and accounts for their judgments on situations of others posted on social media. Using simple attention modules as well as taking one’s previous activities into consideration, we empirically show that our model provides human-readable explanations of an individual’s subjective preference in judging social situations. We further qualitatively evaluate the explanations generated by the model and claim that our model learns an individual’s subjective orientation towards abstract moral concepts. | Younghun Lee, Dan Goldwasser |  |
| 128 |  |  [Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot ICD Coding](https://doi.org/10.18653/v1/2022.findings-emnlp.127) |  | 0 | Automatic International Classification of Diseases (ICD) coding aims to assign multiple ICD codes to a medical note with average length of 3,000+ tokens. This task is challenging due to a high-dimensional space of multi-label assignment (tens of thousands of ICD codes) and the long-tail challenge: only a few codes (common diseases) are frequently assigned while most codes (rare diseases) are infrequently assigned. This study addresses the long-tail challenge by adapting a prompt-based fine-tuning technique with label semantics, which has been shown to be effective under few-shot setting. To further enhance the performance in medical domain, we propose a knowledge-enhanced longformer by injecting three domain-specific knowledge: hierarchy, synonym, and abbreviation with additional pretraining using contrastive learning. Experiments on MIMIC-III-full, a benchmark dataset of code assignment, show that our proposed method outperforms previous state-of-the-art method in 14.5% in marco F1 (from 10.3 to 11.8, P<0.001). To further test our model on few-shot setting, we created a new rare diseases coding dataset, MIMIC-III-rare50, on which our model improves marco F1 from 17.1 to 30.4 and micro F1 from 17.2 to 32.6 compared to previous method. | Zhichao Yang, Shufan Wang, Bhanu Pratap Singh Rawat, Avijit Mitra, Hong Yu |  |
| 129 |  |  [Do Language Models Understand Measurements?](https://doi.org/10.18653/v1/2022.findings-emnlp.128) |  | 0 | Recent success of pre-trained language models (PLMs) has stimulated interest in their ability to understand and work with numbers. Yet, the numerical reasoning over measurements has not been formally studied despite their importance. In this study, we show that PLMs lack the capability required for reasoning over measurements. Furthermore, we find that a language model trained on a measurement-rich corpus shows better performance on understanding measurements. We propose a simple embedding strategy to better distinguish between numbers and units, which leads to a significant improvement in the probing tasks. | Sungjin Park, Seungwoo Ryu, Edward Choi |  |
| 130 |  |  [Reconciliation of Pre-trained Models and Prototypical Neural Networks in Few-shot Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.129) |  | 0 | Incorporating large-scale pre-trained models with the prototypical neural networks is a de-facto paradigm in few-shot named entity recognition. Existing methods, unfortunately, are not aware of the fact that embeddings from pre-trained models contain a prominently large amount of information regarding word frequencies, biasing prototypical neural networks against learning word entities. This discrepancy constrains the two models’ synergy. Thus, we propose a one-line-code normalization method to reconcile such a mismatch with empirical and theoretical grounds. Our experiments based on nine benchmark datasets show the superiority of our method over the counterpart models and are comparable to the state-of-the-art methods. In addition to the model enhancement, our work also provides an analytical viewpoint for addressing the general problems in few-shot name entity recognition or other tasks that rely on pre-trained models or prototypical neural networks. | Youcheng Huang, Wenqiang Lei, Jie Fu, Jiancheng Lv |  |
| 131 |  |  [HCL-TAT: A Hybrid Contrastive Learning Method for Few-shot Event Detection with Task-Adaptive Threshold](https://doi.org/10.18653/v1/2022.findings-emnlp.130) |  | 0 | Event detection has been suffering from constantly emerging event types with lack of sufficient data. Existing works formulate the new problem as few-shot event detection (FSED), and employ two-stage or unified models based on meta-learning to address the problem. However, these methods fall far short of expectations due to: (i) insufficient learning of discriminative representations in low-resource scenarios, and (ii) representation overlap between triggers and non-triggers. To resolve the above issues, in this paper, we propose a novel Hybrid Contrastive Learning method with a Task-Adaptive Threshold (abbreviated as HCL-TAT), which enables discriminative representation learning with a two-view contrastive loss (support-support and prototype-query), and devises an easily-adapted threshold to alleviate misidentification of triggers. Extensive experiments on the benchmark dataset FewEvent demonstrate the superiority of our method to achieve better results compared to the state-of-the-arts. All the data and codes will be available to facilitate future research. | Ruihan Zhang, Wei Wei, XianLing Mao, Rui Fang, Dangyang Chen |  |
| 132 |  |  [Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots](https://doi.org/10.18653/v1/2022.findings-emnlp.131) |  | 0 | This paper introduces Doc2Bot, a novel dataset for building machines that help users seek information via conversations. This is of particular interest for companies and organizations that own a large number of manuals or instruction books. Despite its potential, the nature of our task poses several challenges: (1) documents contain various structures that hinder the ability of machines to comprehend, and (2) user information needs are often underspecified. Compared to prior datasets that either focus on a single structural type or overlook the role of questioning to uncover user needs, the Doc2Bot dataset is developed to target such challenges systematically. Our dataset contains over 100,000 turns based on Chinese documents from five domains, larger than any prior document-grounded dialog dataset for information seeking. We propose three tasks in Doc2Bot: (1) dialog state tracking to track user intentions, (2) dialog policy learning to plan system actions and contents, and (3) response generation which generates responses based on the outputs of the dialog policy. Baseline methods based on the latest deep learning models are presented, indicating that our proposed tasks are challenging and worthy of further research. | Haomin Fu, Yeqin Zhang, Haiyang Yu, Jian Sun, Fei Huang, Luo Si, Yongbin Li, CamTu Nguyen |  |
| 133 |  |  [DualNER: A Dual-Teaching framework for Zero-shot Cross-lingual Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.132) |  | 0 | We present DualNER, a simple and effective framework to make full use of both annotated source language corpus and unlabeled target language text for zero-shot cross-lingual named entity recognition (NER). In particular, we combine two complementary learning paradigms of NER, i.e., sequence labeling and span prediction, into a unified multi-task framework. After obtaining a sufficient NER model trained on the source data, we further train it on the target data in a dual-teaching manner, in which the pseudo-labels for one task are constructed from the prediction of the other task. Moreover, based on the span prediction, an entity-aware regularization is proposed to enhance the intrinsic cross-lingual alignment between the same entities in different languages. Experiments and analysis demonstrate the effectiveness of our DualNER. | Jiali Zeng, Yufan Jiang, Yongjing Yin, Xu Wang, Binghuai Lin, Yunbo Cao |  |
| 134 |  |  [Knowledge-augmented Self-training of A Question Rewriter for Conversational Knowledge Base Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.133) |  | 0 | The recent rise of conversational applications such as online customer service systems and intelligent personal assistants has promoted the development of conversational knowledge base question answering (ConvKBQA). Different from the traditional single-turn KBQA, ConvKBQA usually explores multi-turn questions around a topic, where ellipsis and coreference pose great challenges to the single-turn KBQA systems which require self-contained questions. In this paper, we propose a rewrite-and-reason framework to first produce a full-fledged rewritten question based on the conversation history and then reason the answer by existing single-turn KBQA models. To overcome the absence of the rewritten supervision signals, we introduce a knowledge-augmented self-training mechanism to transfer the question rewriter from another dataset to adapt to the current knowledge base. Our question rewriter is decoupled from the subsequent QA process, which makes it easy to be united with either retrieval-based or semantic parsing-based KBQA models. Experiment results demonstrate the effectiveness of our method and a new state-of-the-art result is achieved. The code and dataset are available online now. | Xirui Ke, Jing Zhang, Xin Lv, Yiqi Xu, Shulin Cao, Cuiping Li, Hong Chen, Juanzi Li |  |
| 135 |  |  [Extractive Summarization of Legal Decisions using Multi-task Learning and Maximal Marginal Relevance](https://doi.org/10.18653/v1/2022.findings-emnlp.134) |  | 0 | Summarizing legal decisions requires the expertise of law practitioners, which is both time- and cost-intensive. This paper presents techniques for extractive summarization of legal decisions in a low-resource setting using limited expert annotated data. We test a set of models that locate relevant content using a sequential model and tackle redundancy by leveraging maximal marginal relevance to compose summaries. We also demonstrate an implicit approach to help train our proposed models generate more informative summaries. Our multi-task learning model variant leverages rhetorical role identification as an auxiliary task to further improve the summarizer. We perform extensive experiments on datasets containing legal decisions from the US Board of Veterans’ Appeals and conduct quantitative and expert-ranked evaluations of our models. Our results show that the proposed approaches can achieve ROUGE scores vis-à-vis expert extracted summaries that match those achieved by inter-annotator comparison. | Abhishek Agarwal, Shanshan Xu, Matthias Grabmair |  |
| 136 |  |  [MovieUN: A Dataset for Movie Understanding and Narrating](https://doi.org/10.18653/v1/2022.findings-emnlp.135) |  | 0 | Automatic movie narration generation and narration grounding are very important to provide a true movie experience for the blind and visually impaired. To tell the movie story well, it is necessary to mention plot-related details (such as character names) and keep the narrations in a plot coherent. Taking these two points into consideration, we construct a Chinese large-scale video benchmark from 101 movies for Movie Understanding and Narrating (MovieUN) to support the Movie Clip Narrating (MCN) task and Temporal Narration Grounding (TNG) task. We split movies in MovieUN into movie clips according to plots, and pair them with corresponding narrations provided by the movie narrators. Ultimately, the TNG task involves 3,253 long video clips totaling 179 hours. The MCN task contains 33,060 video clips totaling 105 hours. We benchmark state-of-the-art video captioning models and temporal grounding models in MCN and TNG tasks, respectively. Furthermore, to accurately comprehend plots of different characters, we propose methods to incorporate portraits of actors as external knowledge in both tasks. The experiment results demonstrate the effectiveness of our proposed methods. The dataset and codes are released at https://github.com/yuezih/MovieUN. | Qi Zhang, Zihao Yue, Anwen Hu, Ziheng Wang, Qin Jin |  |
| 137 |  |  [ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.136) |  | 0 | Data-to-text generation is challenging due to the great variety of the input data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse predicates). Recent end-to-end neural methods thus require substantial training examples to learn to disambiguate and describe the data. Yet, real-world data-to-text problems often suffer from various data-scarce issues: one may have access to only a handful of or no training examples, and/or have to rely on examples in a different domain or schema. To fill this gap, we propose Any-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse settings by making efficient use of any given (or no) examples. ASDOT consists of two steps, data disambiguation and sentence fusion, both of which are amenable to be solved with off-the-shelf pretrained language models (LMs) with optional finetuning. In the data disambiguation stage, we employ the prompted GPT-3 model to understand possibly ambiguous triples from the input data and convert each into a short sentence with reduced ambiguity. The sentence fusion stage then uses an LM like T5 to fuse all the resulting sentences into a coherent paragraph as the final description. We evaluate extensively on various datasets in different scenarios, including the zero-/few-/full-shot settings, and generalization to unseen predicates and out-of-domain data. Experimental results show that ASDOT consistently achieves significant improvement over baselines, e.g., a 30.81 BLEU gain on the DART dataset under the zero-shot setting. | Jiannan Xiang, Zhengzhong Liu, Yucheng Zhou, Eric P. Xing, Zhiting Hu |  |
| 138 |  |  [FCGEC: Fine-Grained Corpus for Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2022.findings-emnlp.137) |  | 0 | Grammatical Error Correction (GEC) has been broadly applied in automatic correction and proofreading system recently. However, it is still immature in Chinese GEC due to limited high-quality data from native speakers in terms of category and scale. In this paper, we present FCGEC, a fine-grained corpus to detect, identify and correct the grammatical errors. FCGEC is a human-annotated corpus with multiple references, consisting of 41,340 sentences collected mainly from multi-choice questions in public school Chinese examinations. Furthermore, we propose a Switch-Tagger-Generator (STG) baseline model to correct the grammatical errors in low-resource settings. Compared to other GEC benchmark models, experimental results illustrate that STG outperforms them on our FCGEC. However, there exists a significant gap between benchmark models and humans that encourages future models to bridge it. | Lvxiaowei Xu, Jianwang Wu, Jiawei Peng, Jiayu Fu, Ming Cai |  |
| 139 |  |  [Audience-Centric Natural Language Generation via Style Infusion](https://doi.org/10.18653/v1/2022.findings-emnlp.138) |  | 0 | Adopting contextually appropriate, audience-tailored linguistic styles is critical to the success of user-centric language generation systems (e.g., chatbots, computer-aided writing, dialog systems). While existing approaches demonstrate text style transfer (TST) with large volumes of parallel or non-parallel data, we argue that grounding style on audience-independent external factors is innately limiting for two reasons. First, it is difficult to collect large volumes of audience-specific stylistic data. Second, some stylistic objectives (e.g., persuasiveness, memorability, empathy) are hard to define without audience feedback. In this paper, we propose the novel task of style infusion - infusing the stylistic preferences of audiences in pretrained language generation models. Since humans are better at pairwise comparisons than direct scoring - i.e., is Sample-A more persuasive/polite/empathic than Sample-B - we leverage limited pairwise human judgments to bootstrap a style analysis model and augment our seed set of judgments. We then infuse the learned textual style in a GPT-2 based text generator while balancing fluency and style adoption. With quantitative and qualitative assessments, we show that our infusion approach can generate compelling stylized examples with generic text prompts. We make the anonymized code and data accessible. | Samraj Moorjani, Adit Krishnan, Hari Sundaram, Ewa Maslowska, Aravind Sankar |  |
| 140 |  |  [DocFin: Multimodal Financial Prediction and Bias Mitigation using Semi-structured Documents](https://doi.org/10.18653/v1/2022.findings-emnlp.139) |  | 0 | Financial prediction is complex due to the stochastic nature of the stock market. Semi-structured financial documents present comprehensive financial data in tabular formats, such as earnings, profit-loss statements, and balance sheets, and can often contain rich technical analysis along with a textual discussion of corporate history, and management analysis, compliance, and risks. Existing research focuses on the textual and audio modalities of financial disclosures from company conference calls to forecast stock volatility and price movement, but ignores the rich tabular data available in financial reports. Moreover, the economic realm is still plagued with a severe under-representation of various communities spanning diverse demographics, gender, and native speakers. In this work, we show that combining tabular data from financial semi-structured documents with text transcripts and audio recordings not only improves stock volatility and price movement prediction by 5-12% but also reduces gender bias caused due to audio-based neural networks by over 30%. | Puneet Mathur, Mihir Goyal, Ramit Sawhney, Ritik Mathur, Jochen L. Leidner, Franck Dernoncourt, Dinesh Manocha |  |
| 141 |  |  [Not Just Plain Text! Fuel Document-Level Relation Extraction with Explicit Syntax Refinement and Subsentence Modeling](https://doi.org/10.18653/v1/2022.findings-emnlp.140) |  | 0 | Document-level relation extraction (DocRE) aims to identify semantic labels among entities within a single document. One major challenge of DocRE is to dig decisive details regarding a specific entity pair from long text. However, in many cases, only a fraction of text carries required information, even in the manually labeled supporting evidence. To better capture and exploit instructive information, we propose a novel expLicit syntAx Refinement and Subsentence mOdeliNg based framework (LARSON). By introducing extra syntactic information, LARSON can model subsentences of arbitrary granularity and efficiently screen instructive ones. Moreover, we incorporate refined syntax into text representations which further improves the performance of LARSON. Experimental results on three benchmark datasets (DocRED, CDR, and GDA) demonstrate that LARSON significantly outperforms existing methods. | Zhichao Duan, Xiuxing Li, Zhenyu Li, Zhuo Wang, Jianyong Wang |  |
| 142 |  |  [Self-supervised Rewiring of Pre-trained Speech Encoders: Towards Faster Fine-tuning with Less Labels in Speech Processing](https://doi.org/10.18653/v1/2022.findings-emnlp.141) |  | 0 | Pre-trained speech Transformers have facilitated great success across various speech processing tasks. However, fine-tuning these encoders for downstream tasks require sufficiently large training data to converge or to achieve state-of-the-art. In text domain this has been partly attributed to sub-optimality of the representation space in pre-trained Transformers. In this work, we take a sober look into pre-trained speech encoders and rewire their representation space without requiring any task-specific labels. Our method utilises neutrally synthesised version of audio inputs along with frame masking to construct positive pairs for contrastive self-supervised learning. When used for augmenting the wav2vec 2 encoder, we observe consistent improvement of isotropy in the representation space. Our experiments on 6 speech processing tasks, exhibit a significant convergence speedup during task fine-tuning as well as consistent task improvement, specially in low-resource settings. | Hao Yang, Jinming Zhao, Gholamreza Haffari, Ehsan Shareghi |  |
| 143 |  |  [RedApt: An Adaptor for wav2vec 2 EncodingFaster and Smaller Speech Translation without Quality Compromise](https://doi.org/10.18653/v1/2022.findings-emnlp.142) |  | 0 | Pre-trained speech Transformers in speech translation (ST) have facilitated state-of-the-art (SotA) results; yet, using such encoders is computationally expensive. To improve this, we present a novel Reducer Adaptor block, RedApt, that could be seamlessly integrated within any Transformer-based speech encoding architecture. Integrating the pretrained wav2vec 2 speech encoder with RedAptbrings 41% speedup, 33% memory reduction with 24% fewer FLOPs at inference. To our positive surprise, our ST model with RedApt outperforms the SotA architecture by an average of 0.68 BLEU score on 8 language pairs from Must-C. | Jinming Zhao, Hao Yang, Gholamreza Haffari, Ehsan Shareghi |  |
| 144 |  |  [How sensitive are translation systems to extra contexts? Mitigating gender bias in Neural Machine Translation models through relevant contexts](https://doi.org/10.18653/v1/2022.findings-emnlp.143) |  | 0 | Neural Machine Translation systems built on top of Transformer-based architectures are routinely improving the state-of-the-art in translation quality according to word-overlap metrics. However, a growing number of studies also highlight the inherent gender bias that these models incorporate during training, which reflects poorly in their translations. In this work, we investigate whether these models can be instructed to fix their bias during inference using targeted, guided instructions as contexts. By translating relevant contextual sentences during inference along with the input, we observe large improvements in reducing the gender bias in translations, across three popular test suites (WinoMT, BUG, SimpleGen). We further propose a novel metric to assess several large pre-trained models (OPUS-MT, M2M-100) on their sensitivity towards using contexts during translation to correct their biases. Our approach requires no fine-tuning, and thus can be used easily in production systems to de-bias translations from stereotypical gender-occupation bias. We hope our method, along with our metric, can be used to build better, bias-free translation systems. | Shanya Sharma, Manan Dey, Koustuv Sinha |  |
| 145 |  |  [P\textM²\textF²N: Patient Multi-view Multi-modal Feature Fusion Networks for Clinical Outcome Prediction](https://doi.org/10.18653/v1/2022.findings-emnlp.144) |  | 0 | Clinical outcome prediction is critical to the condition prediction of patients and management of hospital capacities. There are two kinds of medical data, including time series signals recorded by various devices and clinical notes in electronic health records (EHR), which are used for two common prediction targets: mortality and length of stay. Traditional methods focused on utilizing time series data but ignored clinical notes. With the development of deep learning, natural language processing (NLP) and multi-modal learning methods are exploited to jointly model the time series and clinical notes with different modals. However, the existing methods failed to fuse the multi-modal features of patients from different views. Therefore, we propose the patient multi-view multi-modal feature fusion networks for clinical outcome prediction. Firstly, from patient inner view, we propose to utilize the co-attention module to enhance the fine-grained feature interaction between time series and clinical notes from each patient. Secondly, the patient outer view is the correlation between patients, which can be reflected by the structural knowledge in clinical notes. We exploit the structural information extracted from clinical notes to construct the patient correlation graph, and fuse patients’ multi-modal features by graph neural networks (GNN). The experimental results on MIMIC-III benchmark demonstrate the superiority of our method. | Ying Zhang, Baohang Zhou, Kehui Song, Xuhui Sui, Guoqing Zhao, Ning Jiang, Xiaojie Yuan |  |
| 146 |  |  [Long Text and Multi-Table Summarization: Dataset and Method](https://doi.org/10.18653/v1/2022.findings-emnlp.145) |  | 0 | Automatic document summarization aims to produce a concise summary covering the input document’s salient information. Within a report document, the salient information can be scattered in the textual and non-textual content. However, existing document summarization datasets and methods usually focus on the text and filter out the non-textual content. Missing tabular data can limit produced summaries’ informativeness, especially when summaries require covering quantitative descriptions of critical metrics in tables. Existing datasets and methods cannot meet the requirements of summarizing long text and multiple tables in each report. To deal with the scarcity of available data, we propose FINDSum, the first large-scale dataset for long text and multi-table summarization. Built on 21,125 annual reports from 3,794 companies, it has two subsets for summarizing each company’s results of operations and liquidity. To summarize the long text and dozens of tables in each report, we present three types of summarization methods. Besides, we propose a set of evaluation metrics to assess the usage of numerical information in produced summaries. Dataset analyses and experimental results indicate the importance of jointly considering input textual and tabular data when summarizing report documents. | Shuaiqi Liu, Jiannong Cao, Ruosong Yang, Zhiyuan Wen |  |
| 147 |  |  [MatRank: Text Re-ranking by Latent Preference Matrix](https://doi.org/10.18653/v1/2022.findings-emnlp.146) |  | 0 | Text ranking plays a key role in providing content that best answers user queries. It is usually divided into two sub-tasks to perform efficient information retrieval given a query: text retrieval and text re-ranking. Recent research on pretrained language models (PLM) has demonstrated efficiency and gain on both sub-tasks. However, while existing methods have benefited from pre-trained language models and achieved high recall rates on passage retrieval, the ranking performance still demands further improvement. In this paper, we propose MatRank, which learns to re-rank the text retrieved for a given query by learning to predict the most relevant passage based on a latent preference matrix. Specifically, MatRank uses a PLM to generate an asymmetric latent matrix of relative preference scores between all pairs of retrieved passages. Then, the latent matrix is aggregated row-wise and column-wise to obtain global preferences and predictions of the most relevant passage in two of these directions, respectively. We conduct extensive experiments on MS MACRO, WikiAQ, and SemEval datasets. Experimental results show that MatRank has achieved new state-of-the-art results on these datasets, outperforming all prior methods on ranking performance metrics. | Jinwen Luo, Jiuding Yang, Weidong Guo, Chenglin Li, Di Niu, Yu Xu |  |
| 148 |  |  [Can Language Models Serve as Temporal Knowledge Bases?](https://doi.org/10.18653/v1/2022.findings-emnlp.147) |  | 0 | Recent progress regarding the use of language models (LMs) as knowledge bases (KBs) has shown that language models can act as structured knowledge bases for storing relational facts. However, most existing works only considered the LM-as-KB paradigm in a static setting, which ignores the analysis of temporal dynamics of world knowledge. Furthermore, a basic function of KBs, i.e., the ability to store conflicting information (i.e., 1-N, N-1, and N-M relations), is underexplored. In this paper, we formulate two practical requirements for treating LMs as temporal KBs: (i) The capacity to store temporally-scoped knowledge that contains conflicting information and (ii) the ability to use stored knowledge for temporally-scoped knowledge queries. We introduce a new dataset called LAMA-TK which is aimed at probing temporally-scoped knowledge, and investigate the two above requirements to explore the LM-as-KB paradigm in the temporal domain. On the one hand, experiments show that LMs can memorize millions of temporally-scoped facts with relatively high accuracy and transfer stored knowledge to temporal knowledge queries, thereby expanding the LM-as-KB paradigm to the temporal domain. On the other hand, we show that memorizing conflicting information, which has been neglected by previous works, is still challenging for LMs and hinders the memorization of other unrelated one-to-one relationships. | Ruilin Zhao, Feng Zhao, Guandong Xu, Sixiao Zhang, Hai Jin |  |
| 149 |  |  [Are Large Pre-Trained Language Models Leaking Your Personal Information?](https://doi.org/10.18653/v1/2022.findings-emnlp.148) |  | 0 | Are Large Pre-Trained Language Models Leaking Your Personal Information? In this paper, we analyze whether Pre-Trained Language Models (PLMs) are prone to leaking personal information. Specifically, we query PLMs for email addresses with contexts of the email address or prompts containing the owner’s name. We find that PLMs do leak personal information due to memorization. However, since the models are weak at association, the risk of specific personal information being extracted by attackers is low. We hope this work could help the community to better understand the privacy risk of PLMs and bring new insights to make PLMs safe. | Jie Huang, Hanyin Shao, Kevin ChenChuan Chang |  |
| 150 |  |  [Self-Distillation with Meta Learning for Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.findings-emnlp.149) |  | 0 | In this paper, we propose a self-distillation framework with meta learning (MetaSD) for knowledge graph completion with dynamic pruning, which aims to learn compressed graph embeddings and tackle the long-tail samples. Specifically, we first propose a dynamic pruning technique to obtain a small pruned model from a large source model, where the pruning mask of the pruned model could be updated adaptively per epoch after the model weights are updated. The pruned model is supposed to be more sensitive to difficult-to-memorize samples (e.g., long-tail samples) than the source model. Then, we propose a one-step meta self-distillation method for distilling comprehensive knowledge from the source model to the pruned model, where the two models co-evolve in a dynamic manner during training. In particular, we exploit the performance of the pruned model, which is trained alongside the source model in one iteration, to improve the source model’s knowledge transfer ability for the next iteration via meta learning. Extensive experiments show that MetaSD achieves competitive performance compared to strong baselines, while being 10x smaller than baselines. | Yunshui Li, Junhao Liu, Min Yang, Chengming Li |  |
| 151 |  |  [CQR-SQL: Conversational Question Reformulation Enhanced Context-Dependent Text-to-SQL Parsers](https://doi.org/10.18653/v1/2022.findings-emnlp.150) |  | 0 | Context-dependent text-to-SQL is the task of translating multi-turn questions into database-related SQL queries. Existing methods typically focus on making full use of history context or previously predicted SQL for currently SQL parsing, while neglecting to explicitly comprehend the schema and conversational dependency, such as co-reference, ellipsis and user focus change. In this paper, we propose CQR-SQL, which uses auxiliary Conversational Question Reformulation (CQR) learning to explicitly exploit schema and decouple contextual dependency for multi-turn SQL parsing. Specifically, we first present a schema enhanced recursive CQR method to produce domain-relevant self-contained questions. Secondly, we train CQR-SQL models to map the semantics of multi-turn questions and auxiliary self-contained questions into the same latent space through schema grounding consistency task and tree-structured SQL parsing consistency task, which enhances the abilities of SQL parsing by adequately contextual understanding. At the time of writing, our CQR-SQL achieves new state-of-the-art results on two context-dependent text-to-SQL benchmarks SParC and CoSQL. | Dongling Xiao, Linzheng Chai, QianWen Zhang, Zhao Yan, Zhoujun Li, Yunbo Cao |  |
| 152 |  |  [Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document](https://doi.org/10.18653/v1/2022.findings-emnlp.151) |  | 0 | Given the recent proliferation of false claims online, there has been a lot of manual fact-checking effort. As this is very time-consuming, human fact-checkers can benefit from tools that can support them and make them more efficient. Here, we focus on building a system that could provide such support. Given an input document, it aims to detect all sentences that contain a claim that can be verified by some previously fact-checked claims (from a given database). The output is a re-ranked list of the document sentences, so that those that can be verified are ranked as high as possible, together with corresponding evidence. Unlike previous work, which has looked into claim retrieval, here we take a document-level perspective. We create a new manually annotated dataset for the task, and we propose suitable evaluation measures. We further experiment with a learning-to-rank approach, achieving sizable performance gains over several strong baselines. Our analysis demonstrates the importance of modeling text similarity and stance, while also taking into account the veracity of the retrieved previously fact-checked claims. We believe that this research would be of interest to fact-checkers, journalists, media, and regulatory authorities. | Shaden Shaar, Nikola Georgiev, Firoj Alam, Giovanni Da San Martino, Aisha Mohamed, Preslav Nakov |  |
| 153 |  |  [No Word Embedding Model Is Perfect: Evaluating the Representation Accuracy for Social Bias in the Media](https://doi.org/10.18653/v1/2022.findings-emnlp.152) |  | 0 | News articles both shape and reflect public opinion across the political spectrum. Analyzing them for social bias can thus provide valuable insights, such as prevailing stereotypes in society and the media, which are often adopted by NLP models trained on respective data. Recent work has relied on word embedding bias measures, such as WEAT. However, several representation issues of embeddings can harm the measures’ accuracy, including low-resource settings and token frequency differences. In this work, we study what kind of embedding algorithm serves best to accurately measure types of social bias known to exist in US online news articles. To cover the whole spectrum of political bias in the US, we collect 500k articles and review psychology literature with respect to expected social bias. We then quantify social bias using WEAT along with embedding algorithms that account for the aforementioned issues. We compare how models trained with the algorithms on news articles represent the expected social bias. Our results suggest that the standard way to quantify bias does not align well with knowledge from psychology. While the proposed algorithms reduce the gap, they still do not fully match the literature. | Maximilian Spliethöver, Maximilian Keiff, Henning Wachsmuth |  |
| 154 |  |  [Scientific and Creative Analogies in Pretrained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.153) |  | 0 | This paper examines the encoding of analogy in large-scale pretrained language models, such as BERT and GPT-2. Existing analogy datasets typically focus on a limited set of analogical relations, with a high similarity of the two domains between which the analogy holds. As a more realistic setup, we introduce the Scientific and Creative Analogy dataset (SCAN), a novel analogy dataset containing systematic mappings of multiple attributes and relational structures across dissimilar domains. Using this dataset, we test the analogical reasoning capabilities of several widely-used pretrained language models (LMs). We find that state-of-the-art LMs achieve low performance on these complex analogy tasks, highlighting the challenges still posed by analogy understanding. | Tamara Czinczoll, Helen Yannakoudakis, Pushkar Mishra, Ekaterina Shutova |  |
| 155 |  |  [Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages](https://doi.org/10.18653/v1/2022.findings-emnlp.154) |  | 0 | Scaling multilingual representation learning beyond the hundred most frequent languages is challenging, in particular to cover the long tail of low-resource languages. We move away from the popular one-for-all multilingual models and focus on training multiple language (family) specific representations, but most prominently enable all languages to still be encoded in the same representational space. We focus on teacher-student training, allowing all encoders to be mutually compatible for bitext mining, and enabling fast learning of new languages. We also combine supervised and self-supervised training, allowing encoders to take advantage of monolingual training data.Our approach significantly outperforms the original LASER encoder. We study very low-resource languages and handle 44 African languages, many of which are not covered by any other model. For these languages, we train sentence encoders and mine bitexts. Adding these mined bitexts yielded an improvement of 3.8 BLEU for NMT into English. | Kevin Heffernan, Onur Çelebi, Holger Schwenk |  |
| 156 |  |  [Towards Generalizable and Robust Text-to-SQL Parsing](https://doi.org/10.18653/v1/2022.findings-emnlp.155) |  | 0 | Text-to-SQL parsing tackles the problem of mapping natural language questions to executable SQL queries. In practice, text-to-SQL parsers often encounter various challenging scenarios, requiring them to be generalizable and robust. While most existing work addresses a particular generalization or robustness challenge, we aim to study it in a more comprehensive manner. In specific, we believe that text-to-SQL parsers should be (1) generalizable at three levels of generalization, namely i.i.d., zero-shot, and compositional, and (2) robust against input perturbations. To enhance these capabilities of the parser, we propose a novel TKK framework consisting of Task decomposition, Knowledge acquisition, and Knowledge composition to learn text-to-SQL parsing in stages. By dividing the learning process into multiple stages, our framework improves the parser’s ability to acquire general SQL knowledge instead of capturing spurious patterns, making it more generalizable and robust. Experimental results under various generalization and robustness settings show that our framework is effective in all scenarios and achieves state-of-the-art performance on the Spider, SParC, and CoSQL datasets. | Chang Gao, Bowen Li, Wenxuan Zhang, Wai Lam, Binhua Li, Fei Huang, Luo Si, Yongbin Li |  |
| 157 |  |  [EdiT5: Semi-Autoregressive Text Editing with T5 Warm-Start](https://doi.org/10.18653/v1/2022.findings-emnlp.156) |  | 0 | We present EdiT5 - a novel semi-autoregressive text-editing approach designed to combine the strengths of non-autoregressive text-editing and autoregressive decoding. EdiT5 is faster at inference times than conventional sequence-to-sequence (seq2seq) models, while being capable of modeling flexible input-output transformations.This is achieved by decomposing the generation process into three sub-tasks: (1) tagging to decide on the subset of input tokens to be preserved in the output, (2) re-ordering to define their order in the output text, and (3) insertion to infill the missing tokens that are not present in the input. The tagging and re-ordering steps, which are responsible for generating the largest portion of the output, are non-autoregressive, while the insertion uses an autoregressive decoder.Depending on the task, EdiT5 requires significantly fewer autoregressive steps demonstrating speedups of up to 25x when compared to classic seq2seq models. Quality-wise, EdiT5 is initialized with a pre-trained T5 checkpoint yielding comparable performance to T5 in high-resource settings and clearly outperforms it on low-resource settings when evaluated on three NLG tasks: Sentence Fusion, Grammatical Error Correction, and Decontextualization. | Jonathan Mallinson, Jakub Adámek, Eric Malmi, Aliaksei Severyn |  |
| 158 |  |  [A Critical Reflection and Forward Perspective on Empathy and Natural Language Processing](https://doi.org/10.18653/v1/2022.findings-emnlp.157) |  | 0 | We review the state of research on empathy in natural language processing and identify the following issues: (1) empathy definitions are absent or abstract, which (2) leads to low construct validity and reproducibility. Moreover, (3) emotional empathy is overemphasized, skewing our focus to a narrow subset of simplified tasks. We believe these issues hinder research progress and argue that current directions will benefit from a clear conceptualization that includes operationalizing cognitive empathy components. Our main objectives are to provide insight and guidance on empathy conceptualization for NLP research objectives and to encourage researchers to pursue the overlooked opportunities in this area, highly relevant, e.g., for clinical and educational sectors. | Allison Lahnala, Charles Welch, David Jurgens, Lucie Flek |  |
| 159 |  |  [A Neural-Symbolic Approach to Natural Language Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.158) |  | 0 | Deep neural networks, empowered by pre-trained language models, have achieved remarkable results in natural language understanding (NLU) tasks. However, their performances can drastically deteriorate when logical reasoning is needed. This is because NLU in principle depends on not only analogical reasoning, which deep neural networks are good at, but also logical reasoning. According to the dual-process theory, analogical reasoning and logical reasoning are respectively carried out by System 1 and System 2 in the human brain. Inspired by the theory, we present a novel framework for NLU called Neural-Symbolic Processor (NSP), which performs analogical reasoning based on neural processing and logical reasoning based on both neural and symbolic processing. As a case study, we conduct experiments on two NLU tasks, question answering (QA) and natural language inference (NLI), when numerical reasoning (a type of logical reasoning) is necessary. The experimental results show that our method significantly outperforms state-of-the-art methods in both tasks. | Zhixuan Liu, Zihao Wang, Yuan Lin, Hang Li |  |
| 160 |  |  [Social-aware Sparse Attention Network for Session-based Social Recommendation](https://doi.org/10.18653/v1/2022.findings-emnlp.159) |  | 0 | Session-based Social Recommendation (SSR) aims to use users’ social networks and historical sessions to provide more personalized recommendations for the current session.Unfortunately, existing SSR methods have two limitations.First, they do not screen users’ useless social relationships and noisy irrelevant interactions.However, user preferences are mainly affected by several close friends and key interactions.Second, when modeling the current session, they do not take full advantage of user preference information.To tackle these issues, we propose a novel Social-aware Sparse Attention Network for SSR, abbreviated as SSAN.It mainly consists of the Heterogeneous Graph Embedding (HGE) module and the Social-aware Encoder-decoder Network (SEN) module.In the HGE module, we adopt a modified heterogeneous graph neural network, which focuses more on close friends and key historical interactions, to enhance user/item representations. In the SEN module, we use the user representation as a bridge between the Encoder and Decoder to incorporate user preferences when modeling the current session.Extensive experiments on two benchmark datasets demonstrate the superiority of SSAN over the state-of-the-art models. | Kai Ouyang, Xianghong Xu, Chen Tang, Wang Chen, Haitao Zheng |  |
| 161 |  |  [SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters](https://doi.org/10.18653/v1/2022.findings-emnlp.160) |  | 0 | Adapter Tuning, which freezes the pretrained language models (PLMs) and only fine-tunes a few extra modules, becomes an appealing efficient alternative to the full model fine-tuning. Although computationally efficient, the recent Adapters often increase parameters (e.g. bottleneck dimension) for matching the performance of full model fine-tuning, which we argue goes against their original intention. In this work, we re-examine the parameter-efficiency of Adapter through the lens of network pruning (we name such plug-in concept as SparseAdapter) and find that SparseAdapter can achieve comparable or better performance than standard Adapters when the sparse ratio reaches up to 80%. Based on our findings, we introduce an easy but effective setting “Large-Sparse” to improve the model capacity of Adapters under the same parameter budget. Experiments on five competitive Adapters upon three advanced PLMs show that with proper sparse method (e.g. SNIP) and ratio (e.g. 40%) SparseAdapter can consistently outperform their corresponding counterpart. Encouragingly, with the Large-Sparse setting, we can obtain further appealing gains, even outperforming the full fine-tuning by a large margin. | Shwai He, Liang Ding, Daize Dong, Jeremy Zhang, Dacheng Tao |  |
| 162 |  |  [Measurement Extraction with Natural Language Processing: A Review](https://doi.org/10.18653/v1/2022.findings-emnlp.161) |  | 0 | Quantitative data is important in many domains. Information extraction methods draw structured data from documents. However, the extraction of quantities and their contexts has received little attention in the history of information extraction. In this review, an overview of prior work on measurement extraction is presented. We describe different approaches to measurement extraction and outline the challenges posed by this task. The review concludes with an outline of potential future research. Research strains in measurement extraction tend to be isolated and lack a common terminology. Improvements in numerical reasoning, more extensive datasets, and the consideration of wider contexts may lead to significant improvements in measurement extraction. | Jan Göpfert, Patrick Kuckertz, Jann M. Weinand, Leander Kotzur, Detlef Stolten |  |
| 163 |  |  [Summarizing Procedural Text: Data and Approach](https://doi.org/10.18653/v1/2022.findings-emnlp.162) |  | 0 | Procedural text is a widely used genre that contains many steps of instructions of how to cook a dish or how to conduct a chemical experiment and analyze the procedural text has become a popular task in the NLP field. Since the procedural text can be very long and contains many details, summarizing the whole procedural text or giving an overview for each complicated procedure step can save time for readers and help them to capture the core information in the text. In this paper, we propose the procedural text summarization task with two summarization granularity: step-view and global-view, which summarizes each step in the procedural text separately or gives an overall summary for all steps respectively. To tackle this task, we propose an Entity-State Graph-based Summarizer (ESGS) which is based on state-of-the-art entity state tracking methods and constructs a heterogeneous graph to aggregate contextual information for each procedure. In order to help the summarization model focus on the salient entities, we propose to use the contextualized procedure graph representation to predict the salient entities. Experiments conducted on two datasets verify the effectiveness of our proposed model. Our code and datasets will be released on https://github.com/gsh199449/procedural-summ. | Shen Gao, Haotong Zhang, Xiuying Chen, Rui Yan, Dongyan Zhao |  |
| 164 |  |  [Snapshot-Guided Domain Adaptation for ELECTRA](https://doi.org/10.18653/v1/2022.findings-emnlp.163) |  | 0 | Discriminative pre-trained language models, such as ELECTRA, have achieved promising performances in a variety of general tasks. However, these generic pre-trained models struggle to capture domain-specific knowledge of domain-related tasks. In this work, we propose a novel domain-adaptation method for ELECTRA, which can dynamically select domain-specific tokens and guide the discriminator to emphasize them, without introducing new training parameters. We show that by re-weighting the losses of domain-specific tokens, ELECTRA can be effectively adapted to different domains. The experimental results in both computer science and biomedical domains show that the proposed method can achieve state-of-the-art results on the domain-related tasks. | Daixuan Cheng, Shaohan Huang, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Furu Wei, Denvy Deng, Qi Zhang |  |
| 165 |  |  [Exploiting Labeled and Unlabeled Data via Transformer Fine-tuning for Peer-Review Score Prediction](https://doi.org/10.18653/v1/2022.findings-emnlp.164) |  | 0 | Automatic Peer-review Aspect Score Prediction (PASP) of academic papers can be a helpful assistant tool for both reviewers and authors. Most existing works on PASP utilize supervised learning techniques. However, the limited number of peer-review data deteriorates the performance of PASP. This paper presents a novel semi-supervised learning (SSL) method that incorporates the Transformer fine-tuning into the Γ-model, a variant of the Ladder network, to leverage contextual features from unlabeled data. Backpropagation simultaneously minimizes the sum of supervised and unsupervised cost functions, avoiding the need for layer-wise pre-training. The experimental results show that our model outperforms the supervised and naive semi-supervised learning baselines. Our source codes are available online. | Panitan Muangkammuen, Fumiyo Fukumoto, Jiyi Li, Yoshimi Suzuki |  |
| 166 |  |  [HARALD: Augmenting Hate Speech Data Sets with Real Data](https://doi.org/10.18653/v1/2022.findings-emnlp.165) |  | 0 | The successful completion of the hate speech detection task hinges upon the availability of rich and variable labeled data, which is hard to obtain. In this work, we present a new approach for data augmentation that uses as input real unlabelled data, which is carefully selected from online platforms where invited hate speech is abundant. We show that by harvesting and processing this data (in an automatic manner), one can augment existing manually-labeled datasets to improve the classification performance of hate speech classification models. We observed an improvement in F1-score ranging from 2.7% and up to 9.5%, depending on the task (in- or cross-domain) and the model used. | Ilan Tal, Dan Vilenchik |  |
| 167 |  |  [Wait-info Policy: Balancing Source and Target at Information Level for Simultaneous Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.166) |  | 0 | Simultaneous machine translation (SiMT) outputs the translation while receiving the source inputs, and hence needs to balance the received source information and translated target information to make a reasonable decision between waiting for inputs or outputting translation. Previous methods always balance source and target information at the token level, either directly waiting for a fixed number of tokens or adjusting the waiting based on the current token. In this paper, we propose a Wait-info Policy to balance source and target at the information level. We first quantify the amount of information contained in each token, named info. Then during simultaneous translation, the decision of waiting or outputting is made based on the comparison results between the total info of previous target outputs and received source inputs. Experiments show that our method outperforms strong baselines under and achieves better balance via the proposed info. | Shaolei Zhang, Shoutao Guo, Yang Feng |  |
| 168 |  |  [Turning Fixed to Adaptive: Integrating Post-Evaluation into Simultaneous Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.167) |  | 0 | Simultaneous machine translation (SiMT) starts its translation before reading the whole source sentence and employs either fixed or adaptive policy to generate the target sentence. Compared to the fixed policy, the adaptive policy achieves better latency-quality tradeoffs by adopting a flexible translation policy. If the policy can evaluate rationality before taking action, the probability of incorrect actions will also decrease. However, previous methods lack evaluation of actions before taking them. In this paper, we propose a method of performing the adaptive policy via integrating post-evaluation into the fixed policy. Specifically, whenever a candidate token is generated, our model will evaluate the rationality of the next action by measuring the change in the source content. Our model will then take different actions based on the evaluation results. Experiments on three translation tasks show that our method can exceed strong baselines under all latency. | Shoutao Guo, Shaolei Zhang, Yang Feng |  |
| 169 |  |  [Alleviating Sparsity of Open Knowledge Graphs with Ternary Contrastive Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.168) |  | 0 | Sparsity of formal knowledge and roughness of non-ontological construction make sparsity problem particularly prominent in Open Knowledge Graphs (OpenKGs). Due to sparse links, learning effective representation for few-shot entities becomes difficult. We hypothesize that by introducing negative samples, a contrastive learning (CL) formulation could be beneficial in such scenarios. However, existing CL methods model KG triplets as binary objects of entities ignoring the relation-guided ternary propagation patterns and they are too generic, i.e., they ignore zero-shot, few-shot and synonymity problems that appear in OpenKGs. To address this, we propose TernaryCL, a CL framework based on ternary propagation patterns among head, relation and tail. TernaryCL designs Contrastive Entity and Contrastive Relation to mine ternary discriminative features with both negative entities and relations, introduces Contrastive Self to help zero- and few-shot entities learn discriminative features, Contrastive Synonym to model synonymous entities, and Contrastive Fusion to aggregate graph features from multiple paths. Extensive experiments on benchmarks demonstrate the superiority of TernaryCL over state-of-the-art models. | Qian Li, Shafiq R. Joty, Daling Wang, Shi Feng, Yifei Zhang |  |
| 170 |  |  [Using Developer Discussions to Guide Fixing Bugs in Software](https://doi.org/10.18653/v1/2022.findings-emnlp.169) |  | 0 | Automatically fixing software bugs is a challenging task. While recent work showed that natural language context is useful in guiding bug-fixing models, the approach required prompting developers to provide this context, which was simulated through commit messages written after the bug-fixing code changes were made. We instead propose using bug report discussions, which are available before the task is performed and are also naturally occurring, avoiding the need for any additional information from developers. For this, we augment standard bug-fixing datasets with bug report discussions. Using these newly compiled datasets, we demonstrate that various forms of natural language context derived from such discussions can aid bug-fixing, even leading to improved performance over using commit messages corresponding to the oracle bug-fixing commits. | Sheena Panthaplackel, Milos Gligoric, Junyi Jessy Li, Raymond J. Mooney |  |
| 171 |  |  [AutoCAD: Automatically Generate Counterfactuals for Mitigating Shortcut Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.170) |  | 0 | Recent studies have shown the impressive efficacy of counterfactually augmented data (CAD) for reducing NLU models’ reliance on spurious features and improving their generalizability. However, current methods still heavily rely on human efforts or task-specific designs to generate counterfactuals, thereby impeding CAD’s applicability to a broad range of NLU tasks. In this paper, we present AutoCAD, a fully automatic and task-agnostic CAD generation framework. AutoCAD first leverages a classifier to unsupervisedly identify rationales as spans to be intervened, which disentangles spurious and causal features. Then, AutoCAD performs controllable generation enhanced by unlikelihood training to produce diverse counterfactuals. Extensive evaluations on multiple out-of-domain and challenge benchmarks demonstrate that AutoCAD consistently and significantly boosts the out-of-distribution performance of powerful pre-trained models across different NLU tasks, which is comparable or even better than previous state-of-the-art human-in-the-loop or task-specific CAD methods. | Jiaxin Wen, Yeshuang Zhu, Jinchao Zhang, Jie Zhou, Minlie Huang |  |
| 172 |  |  [A Multi-Modal Knowledge Graph for Classical Chinese Poetry](https://doi.org/10.18653/v1/2022.findings-emnlp.171) |  | 0 | Classical Chinese poetry has a long history and is a precious cultural heritage of humankind. Displaying the classical Chinese poetry in a visual way, helps to cross cultural barriers in different countries, making it enjoyable for all the people. In this paper, we construct a multi-modal knowledge graph for classical Chinese poetry (PKG), in which the visual information of words in the poetry are incorporated. Then a multi-modal pre-training language model, PKG-Bert, is proposed to obtain the poetry representation with visual information, which bridges the semantic gap between different modalities. PKG-Bert achieves the state-of-the-art performance on the poetry-image retrieval task, showing the effectiveness of incorporating the multi-modal knowledge. The large-scale multi-modal knowledge graph of classical Chinese poetry will be released to promote the researches in classical Chinese culture area. | Yuqing Li, Yuxin Zhang, Bin Wu, JiRong Wen, Ruihua Song, Ting Bai |  |
| 173 |  |  [Assessing Non-autoregressive Alignment in Neural Machine Translation via Word Reordering](https://doi.org/10.18653/v1/2022.findings-emnlp.172) |  | 0 | Recent work on non-autoregressive neural machine translation (NAT) that leverages alignment information to explicitly reduce the modality of target distribution has reported comparable performance with counterparts that tackle multi-modality problem by implicitly modeling dependencies. Effectiveness in handling alignment is vital for models that follow this approach, where a token reordering mechanism is typically involved and plays a vital role. We review the reordering capability of the respective mechanisms in recent NAT models, and our experimental results show that their performance is sub-optimal. We propose to learn a non-autoregressive language model (NALM) based on transformer which can be combined with Viterbi decoding to achieve better reordering performance. We evaluate the proposed NALM using the PTB dataset where sentences with words permuted in different ways are expected to have their ordering recovered. Our empirical results show that the proposed method can outperform the state-of-the-art reordering mechanisms under different word permutation settings, with a 2-27 BLEU improvement, suggesting high potential for word alignment in NAT. | ChunHin Tse, Ester Leung, William K. Cheung |  |
| 174 |  |  [Syntax-guided Localized Self-attention by Constituency Syntactic Distance](https://doi.org/10.18653/v1/2022.findings-emnlp.173) |  | 0 | Recent works have revealed that Transformers are implicitly learning the syntactic information in its lower layers from data, albeit is highly dependent on the quality and scale of the training data. However, learning syntactic information from data is not necessary if we can leverage an external syntactic parser, which provides better parsing quality with well-defined syntactic structures. This could potentially improve Transformer’s performance and sample efficiency. In this work, we propose a syntax-guided localized self-attention for Transformer that allows directly incorporating grammar structures from an external constituency parser. It prohibits the attention mechanism to overweight the grammatically distant tokens over close ones. Experimental results show that our model could consistently improve translation performance on a variety of machine translation datasets, ranging from small to large dataset sizes, and with different source languages. | Shengyuan Hou, Jushi Kai, Haotian Xue, Bingyu Zhu, Bo Yuan, Longtao Huang, Xinbing Wang, Zhouhan Lin |  |
| 175 |  |  [CodeExp: Explanatory Code Document Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.174) |  | 0 | Developing models that can automatically generate detailed code explanation can greatly benefit software maintenance and programming education. However, existing code-to-text generation models often produce only high-level summaries of code that do not capture implementation-level choices essential for these scenarios. To fill in this gap, we propose the code explanation generation task. We first conducted a human study to identify the criteria for high-quality explanatory docstring for code. Based on that, we collected and refined a large-scale code docstring corpus and formulated automatic evaluation metrics that best match human assessments. Finally, we present a multi-stage fine-tuning strategy and baseline models for the task. Our experiments show that (1) our refined training dataset lets models achieve better performance in the explanation generation tasks compared to larger-scale unrefined data (15x larger), and (2) fine-tuned models can generate well-structured long docstrings comparable to human-written ones. We envision our training dataset, human-evaluation protocol, recommended metrics, and fine-tuning strategy can boost future code explanation research. The code and annotated data are available at https://github.com/subercui/CodeExp. | Haotian Cui, Chenglong Wang, Junjie Huang, Jeevana Priya Inala, Todd Mytkowicz, Bo Wang, Jianfeng Gao, Nan Duan |  |
| 176 |  |  [PAUQ: Text-to-SQL in Russian](https://doi.org/10.18653/v1/2022.findings-emnlp.175) |  | 0 | Semantic parsing is an important task that allows to democratize human-computer interaction. One of the most popular text-to-SQL datasets with complex and diverse natural language (NL) questions and SQL queries is Spider. We construct and complement a Spider dataset for Russian, thus creating the first publicly available text-to-SQL dataset for this language. While examining its components - NL questions, SQL queries and databases content - we identify limitations of the existing database structure, fill out missing values for tables and add new requests for underrepresented categories. We select thirty functional test sets with different features that can be used for the evaluation of neural models’ abilities. To conduct the experiments, we adapt baseline architectures RAT-SQL and BRIDGE and provide in-depth query component analysis. On the target language, both models demonstrate strong results with monolingual training and improved accuracy in multilingual scenario. In this paper, we also study trade-offs between machine-translated and manually-created NL queries. At present, Russian text-to-SQL is lacking in datasets as well as trained models, and we view this work as an important step towards filling this gap. | Daria Bakshandaeva, Oleg Somov, Ekaterina Dmitrieva, Vera Davydova, Elena Tutubalina |  |
| 177 |  |  [Event-Centric Question Answering via Contrastive Learning and Invertible Event Transformation](https://doi.org/10.18653/v1/2022.findings-emnlp.176) |  | 0 | Human reading comprehension often requires reasoning of event semantic relations in narratives, represented by Event-centric Question-Answering (QA). To address event-centric QA, we propose a novel QA model with contrastive learning and invertible event transformation, call TranCLR. Our proposed model utilizes an invertible transformation matrix to project semantic vectors of events into a common event embedding space, trained with contrastive learning, and thus naturally inject event semantic knowledge into mainstream QA pipelines. The transformation matrix is fine-tuned with the annotated event relation types between events that occurred in questions and those in answers, using event-aware question vectors. Experimental results on the Event Semantic Relation Reasoning (ESTER) dataset show significant improvements in both generative and extractive settings compared to the existing strong baselines, achieving over 8.4% gain in the token-level F1 score and 3.0% gain in Exact Match (EM) score under the multi-answer setting. Qualitative analysis reveals the high quality of the generated answers by TranCLR, demonstrating the feasibility of injecting event knowledge into QA model learning. Our code and models can be found at https://github.com/LuJunru/TranCLR. | Junru Lu, Xingwei Tan, Gabriele Pergola, Lin Gui, Yulan He |  |
| 178 |  |  [Label-Driven Denoising Framework for Multi-Label Few-Shot Aspect Category Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.177) |  | 0 | Multi-Label Few-Shot Aspect Category Detection (FS-ACD) is a new sub-task of aspect-based sentiment analysis, which aims to detect aspect categories accurately with limited training instances. Recently, dominant works use the prototypical network to accomplish this task, and employ the attention mechanism to extract keywords of aspect category from the sentences to produce the prototype for each aspect. However, they still suffer from serious noise problems: (1) due to lack of sufficient supervised data, the previous methods easily catch noisy words irrelevant to the current aspect category, which largely affects the quality of the generated prototype; (2) the semantically-close aspect categories usually generate similar prototypes, which are mutually noisy and confuse the classifier seriously. In this paper, we resort to the label information of each aspect to tackle the above problems, along with proposing a novel Label-Driven Denoising Framework (LDF). Extensive experimental results show that our framework achieves better performance than other state-of-the-art methods. | Fei Zhao, Yuchen Shen, Zhen Wu, Xinyu Dai |  |
| 179 |  |  [Visual Named Entity Linking: A New Dataset and A Baseline](https://doi.org/10.18653/v1/2022.findings-emnlp.178) |  | 0 | Visual Entity Linking (VEL) is a task to link regions of images with their corresponding entities in Knowledge Bases (KBs), which is beneficial for many computer vision tasks such as image retrieval, image caption, and visual question answering. While existing tasks in VEL either rely on textual data to complement a multi-modal linking or only link objects with general entities, which fails to perform named entity linking on large amounts of image data. In this paper, we consider a purely Visual-based Named Entity Linking (VNEL) task, where the input only consists of an image. The task is to identify objects of interest (i.e., visual entity mentions) in images and link them to corresponding named entities in KBs. Since each entity often contains rich visual and textual information in KBs, we thus propose three different sub-tasks, i.e., visual to visual entity linking (V2VEL), visual to textual entity linking (V2TEL), and visual to visual-textual entity linking (V2VTEL). In addition, we present a high-quality human-annotated visual person linking dataset, named WIKIPerson. Based on WIKIPerson, we establish a series of baseline algorithms for the solution of each sub-task, and conduct experiments to verify the quality of the proposed datasets and the effectiveness of baseline methods. We envision this work to be helpful for soliciting more works regarding VNEL in the future. The codes and datasets are publicly available at https: //github.com/ict-bigdatalab/VNEL. | Wen Sun, Yixing Fan, Jiafeng Guo, Ruqing Zhang, Xueqi Cheng |  |
| 180 |  |  [MAGMA - Multimodal Augmentation of Generative Models through Adapter-based Finetuning](https://doi.org/10.18653/v1/2022.findings-emnlp.179) |  | 0 | Large-scale pretraining is fast becoming the norm in Vision-Language (VL) modeling. However, prevailing VL approaches are limited by the requirement for labeled data and the use of complex multi-step pretraining objectives. We present MAGMA - a simple method for augmenting generative language models with additional modalities using adapter-based finetuning. Building on Frozen, we train a series of VL models that autoregressively generate text from arbitrary combinations of visual and textual input. The pretraining is entirely end-to-end using a single language modeling objective, simplifying optimization compared to previous approaches. Importantly, the language model weights remain unchanged during training, allowing for transfer of encyclopedic knowledge and in-context learning abilities from language pretraining. MAGMA outperforms Frozen on open-ended generative tasks, achieving state of the art results on the OKVQA benchmark and competitive results on a range of other popular VL benchmarks, while pretraining on 0.2 % of the number of samples used to train SimVLM. | Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, Anette Frank |  |
| 181 |  |  [Towards Tracing Knowledge in Language Models Back to the Training Data](https://doi.org/10.18653/v1/2022.findings-emnlp.180) |  | 0 | Language models (LMs) have been shown to memorize a great deal of factual knowledge contained in their training data. But when an LM generates an assertion, it is often difficult to determine where it learned this information and whether it is true. In this paper, we propose the problem of fact tracing: identifying which training examples taught an LM to generate a particular factual assertion. Prior work on training data attribution (TDA) may offer effective tools for identifying such examples, known as “proponents”. We present the first quantitative benchmark to evaluate this. We compare two popular families of TDA methods — gradient-based and embedding-based — and find that much headroom remains. For example, both methods have lower proponent-retrieval precision than an information retrieval baseline (BM25) that does not have access to the LM at all. We identify key challenges that may be necessary for further improvement such as overcoming the problem of gradient saturation, and also show how several nuanced implementation details of existing neural TDA methods can significantly improve overall fact tracing performance. | Ekin Akyürek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, Kelvin Guu |  |
| 182 |  |  [ReaRev: Adaptive Reasoning for Question Answering over Knowledge Graphs](https://doi.org/10.18653/v1/2022.findings-emnlp.181) |  | 0 | Knowledge Graph Question Answering (KGQA) involves retrieving entities as answers from a Knowledge Graph (KG) using natural language queries. The challenge is to learn to reason over question-relevant KG facts that traverse KG entities and lead to the question answers. To facilitate reasoning, the question is decoded into instructions, which are dense question representations used to guide the KG traversals. However, if the derived instructions do not exactly match the underlying KG information, they may lead to reasoning under irrelevant context.Our method, termed ReaRev, introduces a new way to KGQA reasoning with respectto both instruction decoding and execution. To improve instruction decoding, we perform reasoning in an adaptive manner, where KG-aware information is used to iteratively update the initial instructions. To improve instruction execution, we emulate breadth-first search (BFS) with graph neural networks (GNNs). The BFS strategy treats the instructions as a set and allows our method to decide on their execution order on the fly. Experimental results on three KGQA benchmarks demonstrate the ReaRev’s effectiveness compared with previous state-of-the-art, especially when the KG is incomplete or when we tackle complex questions. Our code is publicly available at https://github.com/cmavro/ReaRev_KGQA. | Costas Mavromatis, George Karypis |  |
| 183 |  |  [Understanding Social Media Cross-Modality Discourse in Linguistic Space](https://doi.org/10.18653/v1/2022.findings-emnlp.182) |  | 0 | The multimedia communications with texts and images are popular on social media. However, limited studies concern how images are structured with texts to form coherent meanings in human cognition. To fill in the gap, we present a novel concept of cross-modality discourse, reflecting how human readers couple image and text understandings. Text descriptions are first derived from images (named as subtitles) in the multimedia contexts. Five labels – entity-level insertion, projection and concretization and scene-level restatement and extension — are further employed to shape the structure of subtitles and texts and present their joint meanings. As a pilot study, we also build the very first dataset containing over 16K multimedia tweets with manually annotated discourse labels. The experimental results show that trendy multimedia encoders based on multi-head attention (with captions) are unable to well understand cross-modality discourse and additionally modeling texts at the output layer helps yield the-state-of-the-art results. | Chunpu Xu, Hanzhuo Tan, Jing Li, Piji Li |  |
| 184 |  |  [TAPE: Assessing Few-shot Russian Language Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.183) |  | 0 | Recent advances in zero-shot and few-shot learning have shown promise for a scope of research and practical purposes. However, this fast-growing area lacks standardized evaluation suites for non-English languages, hindering progress outside the Anglo-centric paradigm. To address this line of research, we propose TAPE (Text Attack and Perturbation Evaluation), a novel benchmark that includes six more complex NLU tasks for Russian, covering multi-hop reasoning, ethical concepts, logic and commonsense knowledge. The TAPE’s design focuses on systematic zero-shot and few-shot NLU evaluation: (i) linguistic-oriented adversarial attacks and perturbations for analyzing robustness, and (ii) subpopulations for nuanced interpretation. The detailed analysis of testing the autoregressive baselines indicates that simple spelling-based perturbations affect the performance the most, while paraphrasing the input has a more negligible effect. At the same time, the results demonstrate a significant gap between the neural and human baselines for most tasks. We publicly release TAPE (https://tape-benchmark.com) to foster research on robust LMs that can generalize to new tasks when little to no supervision is available. | Ekaterina Taktasheva, Alena Fenogenova, Denis Shevelev, Nadezhda Katricheva, Maria Tikhonova, Albina Akhmetgareeva, Oleg Zinkevich, Anastasiia Bashmakova, Svetlana Iordanskaia, Valentina Kurenshchikova, Alena Spiridonova, Ekaterina Artemova, Tatiana Shavrina, Vladislav Mikhailov |  |
| 185 |  |  [A Hierarchical N-Gram Framework for Zero-Shot Link Prediction](https://doi.org/10.18653/v1/2022.findings-emnlp.184) |  | 0 | Knowledge graphs typically contain a large number of entities but often cover only a fraction of all relations between them (i.e., incompleteness). Zero-shot link prediction (ZSLP) is a popular way to tackle the problem by automatically identifying unobserved relations between entities. Most recent approaches use textual features of relations (e.g., surface name or textual descriptions) as auxiliary information to improve the encoded representation. These methods lack robustness as they are bound to support only tokens from a fixed vocabulary and unable to model out-of-vocabulary (OOV) words. Subword units such as character n-grams have the capability of generating more expressive representations for OOV words. Hence, in this paper, we propose a Hierarchical N-gram framework for Zero-Shot Link Prediction (HNZSLP) that leverages character n-gram information for ZSLP. Our approach works by first constructing a hierarchical n-gram graph from the surface name of relations. Subsequently, a new Transformer-based network models the hierarchical n-gram graph to learn a relation embedding for ZSLP. Experimental results show that our proposed HNZSLP method achieves state-of-the-art performance on two standard ZSLP datasets. | Mingchen Li, Junfan Chen, Samuel Mensah, Nikolaos Aletras, Xiulong Yang, Yang Ye |  |
| 186 |  |  [Quadapter: Adapter for GPT-2 Quantization](https://doi.org/10.18653/v1/2022.findings-emnlp.185) |  | 0 | Transformer language models such as GPT-2 are difficult to quantize because of outliers in the activations leading to a large quantization error. To adapt to the error, one must use quantization-aware training, which entails a fine-tuning process based on the dataset and the training pipeline identical to those for the original model. Pretrained language models, however, often do not grant access to their datasets and training pipelines, forcing us to rely on arbitrary ones for fine-tuning. In that case, it is observed that quantization-aware training overfits the model to the fine-tuning data. To this end introduced is a quantization adapter (Quadapter), a small set of parameters that are learned to make activations quantization-friendly by scaling them channel-wise.For quantization without overfitting, we introduce a quantization adapter (Quadapter), a small set of parameters that are learned to make activations quantization-friendly by scaling them channel-wise. It keeps the model parameters unchanged. By applying our method to the challenging task of quantizing GPT-2, we demonstrate that it effectively prevents the overfitting and improves the quantization performance. | Minseop Park, Jaeseong You, Markus Nagel, Simyung Chang |  |
| 187 |  |  [BanglaRQA: A Benchmark Dataset for Under-resourced Bangla Language Reading Comprehension-based Question Answering with Diverse Question-Answer Types](https://doi.org/10.18653/v1/2022.findings-emnlp.186) |  | 0 | High-resource languages, such as English, have access to a plethora of datasets with various question-answer types resembling real-world reading comprehension. However, there is a severe lack of diverse and comprehensive question-answering datasets in under-resourced languages like Bangla. The ones available are either translated versions of English datasets with a niche answer format or created by human annotations focusing on a specific domain, question type, or answer type. To address these limitations, this paper introduces BanglaRQA, a reading comprehension-based Bangla question-answering dataset with various question-answer types. BanglaRQA consists of 3,000 context passages and 14,889 question-answer pairs created from those passages. The dataset comprises answerable and unanswerable questions covering four unique categories of questions and three types of answers. In addition, this paper also implemented four different Transformer models for question-answering on the proposed dataset. The best-performing model achieved an overall 62.42% EM and 78.11% F1 score. However, detailed analyses showed that the performance varies across question-answer types, leaving room for substantial improvement of the model performance. Furthermore, we demonstrated the effectiveness of BanglaRQA as a training resource by showing strong results on the bn_squad dataset. Therefore, BanglaRQA has the potential to contribute to the advancement of future research by enhancing the capability of language models. The dataset and codes are available at https://github.com/sartajekram419/BanglaRQA | Syed Mohammed Sartaj Ekram, Adham Arik Rahman, Md. Sajid Altaf, Mohammed Saidul Islam, Mehrab Mustafy Rahman, Md Mezbaur Rahman, Md Azam Hossain, Abu Raihan Mostofa Kamal |  |
| 188 |  |  [Chaining Simultaneous Thoughts for Numerical Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.187) |  | 0 | Given that rich information is hidden behind ubiquitous numbers in text, numerical reasoning over text should be an essential skill of AI systems. To derive precise equations to solve numerical reasoning problems, previous work focused on modeling the structures of equations, and has proposed various structured decoders. Though structure modeling proves to be effective, these structured decoders construct a single equation in a pre-defined autoregressive order, potentially placing an unnecessary restriction on how a model should grasp the reasoning process. Intuitively, humans may have numerous pieces of thoughts popping up in no pre-defined order; thoughts are not limited to the problem at hand, and can even be concerned with other related problems. By comparing diverse thoughts and chaining relevant pieces, humans are less prone to errors. In this paper, we take this inspiration and propose CANTOR, a numerical reasoner that models reasoning steps using a directed acyclic graph where we produce diverse reasoning steps simultaneously without pre-defined decoding dependencies, and compare and chain relevant ones to reach a solution. Extensive experiments demonstrated the effectiveness of CANTOR under both fully-supervised and weakly-supervised settings. | Zhihong Shao, Fei Huang, Minlie Huang |  |
| 189 |  |  [Inferring Implicit Relations in Complex Questions with Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.188) |  | 0 | A prominent challenge for modern language understanding systems is the ability to answer implicit reasoning questions, where the required reasoning steps for answering the question are not mentioned in the text explicitly. In this work, we investigate why current models struggle with implicit reasoning question answering (QA) tasks, by decoupling inference of reasoning steps from their execution.We define a new task of implicit relation inference and construct a benchmark, IMPLICITRELATIONS, where given a question, a model should output a list of concept-relation pairs, where the relations describe the implicit reasoning steps required for answering the question.Using IMPLICITRELATIONS, we evaluate models from the GPT-3 family and find that, while these models struggle on the implicit reasoning QA task, they often succeed at inferring implicit relations.This suggests that the challenge in implicit reasoning questions does not stem from the need to plan a reasoning strategy alone, but to do it while also retrieving and reasoning over relevant information. | Uri Katz, Mor Geva, Jonathan Berant |  |
| 190 |  |  [Eliciting and Understanding Cross-task Skills with Task-level Mixture-of-Experts](https://doi.org/10.18653/v1/2022.findings-emnlp.189) |  | 0 | Recent works suggest that transformer models are capable of multi-tasking on diverse NLP tasks and adapt to new tasks efficiently. However, the potential of these multi-task models may be limited as they use the same set of parameters for all tasks. In contrast, humans tackle tasks in a more flexible way, by making proper presumptions on what skills and knowledge are relevant and executing only the necessary computations. Inspired by this, we propose to use task-level mixture-of-expert models, which has a collection of transformer layers (i.e., experts) and a router component to choose among these experts dynamically and flexibly. We find that these models help improve the average performance gain (ARG) metric by 2.6% when adapting to unseen tasks in few-shot settings, and by 5.6% in zero-shot generalization settings. Further, we show that the learned routing decisions and experts partly rediscover human categorization of NLP tasks – certain experts are strongly associated with extractive tasks, some with classification tasks, and some with tasks requiring world knowledge. | Qinyuan Ye, Juan Zha, Xiang Ren |  |
| 191 |  |  [On the Curious Case of l2 norm of Sense Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.190) |  | 0 | We show that the l2 norm of a static sense embedding encodes information related to the frequency of that sense in the training corpus used to learn the sense embeddings. This finding can be seen as an extension of a previously known relationship for word embeddings to sense embeddings. Our experimental results show that in spite of its simplicity, the l2 norm of sense embeddings is a surprisingly effective feature for several word sense related tasks such as (a) most frequent sense prediction, (b) word-in-context (WiC), and (c) word sense disambiguation (WSD). In particular, by simply including the l2 norm of a sense embedding as a feature in a classifier, we show that we can improve WiC and WSD methods that use static sense embeddings. | Yi Zhou, Danushka Bollegala |  |
| 192 |  |  [Partially-Random Initialization: A Smoking Gun for Binarization Hypothesis of BERT](https://doi.org/10.18653/v1/2022.findings-emnlp.191) |  | 0 | In the past few years, pre-trained BERT has become one of the most popular deep-learning language models due to their remarkable performance in natural language processing (NLP) tasks. However, the superior performance of BERT comes at the cost of high computational and memory complexity, hindering its envisioned widespread deployment in edge devices with limited computing resources. Binarization can alleviate these limitations by reducing storage requirements and improving computing performance. However, obtaining a comparable accuracy performance for binary BERT w.r.t. its full-precision counterpart is still a difficult task. We observe that direct binarization of pre-trained BERT provides a poor initialization during the fine-tuning phase, making the model incapable of achieving a decent accuracy on downstream tasks. Based on this observation, we put forward the following hypothesis: partially randomly-initialized BERT with binary weights and activations can reach to a decent accuracy performance by distilling knowledge from the its full-precision counterpart. We show that BERT with pre-trained embedding layer and randomly-initialized encoder is a smoking gun for this hypothesis. We identify the smoking gun through a series of experiments and show that it yields a new set of state-of-the-art results on the GLUE and SQuAD benchmarks. | Arash Ardakani |  |
| 193 |  |  [Prompt Consistency for Zero-Shot Task Generalization](https://doi.org/10.18653/v1/2022.findings-emnlp.192) |  | 0 | One of the most impressive results of recent NLP history is the ability of pre-trained language models to solve new tasks in a zero-shot setting. To achieve this, NLP tasks are framed as natural language prompts, generating a response indicating the predicted output. Nonetheless, the performance in such settings often lags far behind its supervised counterpart, suggesting a large space for potential improvement. In this paper, we explore methods to utilize unlabeled data to improve zero-shot performance. Specifically, we take advantage of the fact that multiple prompts can be used to specify a single task, and propose to regularize prompt consistency, encouraging consistent predictions over this diverse set of prompts. Our method makes it possible to fine-tune the model either with extra unlabeled training data, or directly on test input at inference time in an unsupervised manner. In experiments, our approach outperforms the state-of-the-art zero-shot learner, T0, on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points in terms of accuracy. The gains are often attained with a small number of unlabeled examples. | Chunting Zhou, Junxian He, Xuezhe Ma, Taylor BergKirkpatrick, Graham Neubig |  |
| 194 |  |  [In-Context Learning for Few-Shot Dialogue State Tracking](https://doi.org/10.18653/v1/2022.findings-emnlp.193) |  | 0 | Collecting and annotating task-oriented dialogues is time-consuming and costly. Thus, zero and few shot learning for dialogue tasks presents an exciting opportunity. In this work, we propose an in-context (IC) learning framework for zero-shot and few-shot learning dialogue state tracking (DST), where a large pretrained language model (LM) takes a test instance and a few exemplars as input, and directly decodes the dialogue state without any parameter updates. This approach is more flexible and scalable than prior DST work when adapting to new domains and scenarios. To better leverage a tabular domain description in the LM prompt, we reformulate DST into a text-to-SQL problem. We also propose a novel approach to retrieve annotated dialogues as exemplars. Empirical results on MultiWOZ show that our method IC-DST substantially outperforms previous fine-tuned state-of-the-art models in few-shot settings. In addition, we test IC-DST in zero-shot settings, in which the model only takes a fixed task instruction as input, finding that it outperforms previous zero-shot methods by a large margin. | Yushi Hu, ChiaHsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, Mari Ostendorf |  |
| 195 |  |  [On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization](https://doi.org/10.18653/v1/2022.findings-emnlp.194) |  | 0 | Combining the visual modality with pretrained language models has been surprisingly effective for simple descriptive tasks such as image captioning. More general text generation however remains elusive. We take a step back and ask: How do these models work for more complex generative tasks, i.e. conditioning on both text and images? Are multimodal models simply visually adapted language models, or do they combine they reason jointly over modalities?We investigate these questions in the context of self-rationalization (jointly generating task labels/answers and free-text explanations) of three tasks: (i) visual question answering in VQA-X, (ii) visual commonsense reasoning in VCR, and (iii) visual-textual entailment in E-SNLI-VE. We show that recent unimodal advances, CLIP image representations and scaling of language models, do not consistently improveself-rationalization in multimodal tasks. We find that no single model type works universally best across tasks, datasets, and finetuning data sizes. Our findings motivate the need for novel general backbones that move text generation from images and text beyond image captioning. | Shruti Palaskar, Akshita Bhagia, Yonatan Bisk, Florian Metze, Alan W. Black, Ana Marasovic |  |
| 196 |  |  [The challenges of temporal alignment on Twitter during crises](https://doi.org/10.18653/v1/2022.findings-emnlp.195) |  | 0 | Language use changes over time, and this impacts the effectiveness of NLP systems. This phenomenon is even more prevalent in social media data during crisis events where meaning and frequency of word usage may change over the course of days. Contextual language models fail to adapt temporally, emphasizing the need for temporal adaptation in models which need to be deployed over an extended period of time. While existing approaches consider data spanning large periods of time (from years to decades), shorter time spans are critical for crisis data. We quantify temporal degradation for this scenario and propose methods to cope with performance loss by leveraging techniques from domain adaptation. To the best of our knowledge, this is the first effort to explore effects of rapid language change driven by adversarial adaptations, particularly during natural and human-induced disasters. Through extensive experimentation on diverse crisis datasets, we analyze under what conditions our approaches outperform strong baselines while highlighting the current limitations of temporal adaptation methods in scenarios where access to unlabeled data is scarce. | Aniket Pramanick, Tilman Beck, Kevin Stowe, Iryna Gurevych |  |
| 197 |  |  [Experimental Standards for Deep Learning in Natural Language Processing Research](https://doi.org/10.18653/v1/2022.findings-emnlp.196) |  | 0 | The field of Deep Learning (DL) has undergone explosive growth during the last decade, with a substantial impact on Natural Language Processing (NLP) as well. Yet, compared to more established disciplines, a lack of common experimental standards remains an open challenge to the field at large. Starting from fundamental scientific principles, we distill ongoing discussions on experimental standards in NLP into a single, widely-applicable methodology. Following these best practices is crucial to strengthen experimental evidence, improve reproducibility and enable scientific progress. These standards are further collected in a public repository to help them transparently adapt to future needs. | Dennis Ulmer, Elisa Bassignana, Max MüllerEberstein, Daniel Varab, Mike Zhang, Rob van der Goot, Christian Hardmeier, Barbara Plank |  |
| 198 |  |  [Few-Shot Anaphora Resolution in Scientific Protocols via Mixtures of In-Context Experts](https://doi.org/10.18653/v1/2022.findings-emnlp.197) |  | 0 | Anaphora resolution is an important task for information extraction across a range of languages, text genres, and domains, motivating the need for methods that do not require large annotated datasets. In-context learning has emerged as a promising approach, yet there are a number of challenges in applying in-context learning to resolve anaphora. For example, encoding a single in-context demonstration that consists of: an anaphor, a paragraph-length context, and a list of corresponding antecedents, requires conditioning a language model on a long sequence of tokens, limiting the number of demonstrations per prompt.In this paper, we present Mice (Mixtures of In-Context Experts), which we demonstrate is effective for few-shot anaphora resolution in scientific protocols. Given only a handful of training examples, Mice combines the predictions of hundreds of in-context experts, yielding a 30% increase in F1 score over a competitive prompt retrieval baseline. Furthermore, we show Mice can be used to train compact student models without sacrificing performance. As far as we are aware, this is the first work to present experimental results demonstrating the effectiveness of in-context learning on the task of few-shot anaphora resolution in scientific protocols. | Nghia T. Le, Fan Bai, Alan Ritter |  |
| 199 |  |  [Exploring Predictive Uncertainty and Calibration in NLP: A Study on the Impact of Method & Data Scarcity](https://doi.org/10.18653/v1/2022.findings-emnlp.198) |  | 0 | We investigate the problem of determining the predictive confidence (or, conversely, uncertainty) of a neural classifier through the lens of low-resource languages. By training models on sub-sampled datasets in three different languages, we assess the quality of estimates from a wide array of approaches and their dependence on the amount of available data. We find that while approaches based on pre-trained models and ensembles achieve the best results overall, the quality of uncertainty estimates can surprisingly suffer with more data. We also perform a qualitative analysis of uncertainties on sequences, discovering that a model’s total uncertainty seems to be influenced to a large degree by its data uncertainty, not model uncertainty. All model implementations are open-sourced in a software package. | Dennis Ulmer, Jes Frellsen, Christian Hardmeier |  |
| 200 |  |  [Conditional Supervised Contrastive Learning for Fair Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.199) |  | 0 | Contrastive representation learning has gained much attention due to its superior performance in learning representations from both image and sequential data. However, the learned representations could potentially lead to performance disparities in downstream tasks, such as increased silencing of underrepresented groups in toxicity comment classification. In light of this challenge, in this work, we study learning fair representations that satisfy a notion of fairness known as equalized odds for text classification via contrastive learning. Specifically, we first theoretically analyze the connections between learning representations with a fairness constraint and conditional supervised contrastive objectives, and then propose to use conditional supervised contrastive objectives to learn fair representations for text classification. We conduct experiments on two text datasets to demonstrate the effectiveness of our approaches in balancing the trade-offs between task performance and bias mitigation among existing baselines for text classification. Furthermore, we also show that the proposed methods are stable in different hyperparameter settings. | Jianfeng Chi, William Shand, Yaodong Yu, KaiWei Chang, Han Zhao, Yuan Tian |  |
| 201 |  |  [SpaBERT: A Pretrained Language Model from Geographic Data for Geo-Entity Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.200) |  | 0 | Named geographic entities (geo-entities for short) are the building blocks of many geographic datasets. Characterizing geo-entities is integral to various application domains, such as geo-intelligence and map comprehension, while a key challenge is to capture the spatial-varying context of an entity. We hypothesize that we shall know the characteristics of a geo-entity by its surrounding entities, similar to knowing word meanings by their linguistic context. Accordingly, we propose a novel spatial language model, SpaBERT, which provides a general-purpose geo-entity representation based on neighboring entities in geospatial data. SpaBERT extends BERT to capture linearized spatial context, while incorporating a spatial coordinate embedding mechanism to preserve spatial relations of entities in the 2-dimensional space. SpaBERT is pretrained with masked language modeling and masked entity prediction tasks to learn spatial dependencies. We apply SpaBERT to two downstream tasks: geo-entity typing and geo-entity linking. Compared with the existing language models that do not use spatial context, SpaBERT shows significant performance improvement on both tasks. We also analyze the entity representation from SpaBERT in various settings and the effect of spatial coordinate embedding. | Zekun Li, Jina Kim, YaoYi Chiang, Muhao Chen |  |
| 202 |  |  [Self-training with Two-phase Self-augmentation for Few-shot Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.201) |  | 0 | In task-oriented dialogue systems, response generation from meaning representations (MRs) often suffers from limited training examples, due to the high cost of annotating MR-to-Text pairs. Previous works on self-training leverage fine-tuned conversational models to automatically generate pseudo-labeled MR-to-Text pairs for further fine-tuning. However, some self-augmented data may be noisy or uninformative for the model to learn from. In this work, we propose a two-phase self-augmentation procedure to generate high-quality pseudo-labeled MR-to-Text pairs: the first phase selects the most informative MRs based on model’s prediction uncertainty; with the selected MRs, the second phase generates accurate responses by aggregating multiple perturbed latent representations from each MR. Empirical experiments on two benchmark datasets, FewShotWOZ and FewShotSGD, show that our method generally outperforms existing self-training methods on both automatic and human evaluations. | Wanyu Du, Hanjie Chen, Yangfeng Ji |  |
| 203 |  |  [Is NLP Ready for Standardization?](https://doi.org/10.18653/v1/2022.findings-emnlp.202) |  | 0 | While standardization is a well-established activity in other scientific fields such as telecommunications, networks or multimedia, in the field of AI and more specifically NLP it is still at its dawn. In this paper, we explore how various aspects of NLP (evaluation, data, tasks...) lack standards and how that can impact science, but also the society, the industry, and regulations. We argue that the numerous initiatives to rationalize the field and establish good practices are only the first step, and developing formal standards remains needed to bring further clarity to NLP research and industry, at a time where this community faces various crises regarding ethics or reproducibility. We thus encourage NLP researchers to contribute to existing and upcoming standardization projects, so that they can express their needs and concerns, while sharing their expertise. | Lauriane Aufrant |  |
| 204 |  |  [Probing for Incremental Parse States in Autoregressive Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.203) |  | 0 | Next-word predictions from autoregressive neural language models show remarkable sensitivity to syntax. This work evaluates the extent to which this behavior arises as a result of a learned ability to maintain implicit representations of incremental syntactic structures. We extend work in syntactic probing to the incremental setting and present several probes for extracting incomplete syntactic structure (operationalized through parse states from a stack-based parser) from autoregressive language models. We find that our probes can be used to predict model preferences on ambiguous sentence prefixes and causally intervene on model representations and steer model behavior. This suggests implicit incremental syntactic inferences underlie next-word predictions in autoregressive neural language models. | Tiwalayo Eisape, Vineet Gangireddy, Roger Levy, Yoon Kim |  |
| 205 |  |  [Re-Examining Calibration: The Case of Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.204) |  | 0 | For users to trust model predictions, they need to understand model outputs, particularly their confidence — calibration aims to adjust (calibrate) models’ confidence to match expected accuracy. We argue that the traditional calibration evaluation does not promote effective calibrations: for example, it can encourage always assigning a mediocre confidence score to all predictions, which does not help users distinguish correct predictions from wrong ones. Building on those observations, we propose a new calibration metric, MacroCE, that better captures whether the model assigns low confidence to wrong predictions and high confidence to correct predictions. Focusing on the practical application of open-domain question answering, we examine conventional calibration methods applied on the widely-used retriever-reader pipeline, all of which do not bring significant gains under our new MacroCE metric. Toward better calibration, we propose a new calibration method (ConsCal) that uses not just final model predictions but whether multiple model checkpoints make consistent predictions. Altogether, we provide an alternative view of calibration along with a new metric, re-evaluation of existing calibration methods on our metric, and proposal of a more effective calibration method. | Chenglei Si, Chen Zhao, Sewon Min, Jordan L. BoydGraber |  |
| 206 |  |  [Accelerating Learned Sparse Indexes Via Term Impact Decomposition](https://doi.org/10.18653/v1/2022.findings-emnlp.205) |  | 0 | Novel inverted index-based learned sparse ranking models provide more effective, but less efficient, retrieval performance compared to traditional ranking models like BM25. In this paper, we introduce a technique we call postings clipping to improve the query efficiency of learned representations. Our technique amplifies the benefit of dynamic pruning query processing techniques by accounting for changes in term importance distributions of learned ranking models. The new clipping mechanism accelerates top-k retrieval by up to 9.6X without any loss in effectiveness. | Joel Mackenzie, Antonio Mallia, Alistair Moffat, Matthias Petri |  |
| 207 |  |  [Do Text-to-Text Multi-Task Learners Suffer from Task Conflict?](https://doi.org/10.18653/v1/2022.findings-emnlp.206) |  | 0 | Traditional multi-task learning architectures learn a single model across multiple tasks through a shared encoder followed by task-specific decoders. Learning these models often requires specialized training algorithms that address task-conflict in the shared parameter updates, which otherwise can lead to negative transfer. A new type of multi-task learning within NLP homogenizes multi-task architectures as a shared encoder and language model decoder, which does surprisingly well across a range of diverse tasks. Does this new architecture suffer from task-conflicts that require specialized training algorithms? We study how certain factors in the shift towards text-to-text models affects multi-task conflict and negative transfer, finding that both directional conflict and transfer are surprisingly constant across architectures. | David Mueller, Nicholas Andrews, Mark Dredze |  |
| 208 |  |  [MANTa: Efficient Gradient-Based Tokenization for End-to-End Robust Language Modeling](https://doi.org/10.18653/v1/2022.findings-emnlp.207) |  | 0 | Static subword tokenization algorithms have been an essential component of recent works on language modeling. However, their static nature results in important flaws that degrade the models’ downstream performance and robustness. In this work, we propose MANTa, a Module for Adaptive Neural TokenizAtion. MANTa is a differentiable tokenizer trained end-to-end with the language model. The resulting system offers a trade-off between the expressiveness of byte-level models and the speed of models trained using subword tokenization. In addition, our tokenizer is highly explainable since it produces an explicit segmentation of sequences into blocks. We evaluate our pre-trained model on several English datasets from different domains as well as on synthetic noise. We find that MANTa improves robustness to character perturbations and out-of-domain data. We then show that MANTa performs comparably to other models on the general-domain GLUE benchmark. Finally, we show that it is considerably faster than strictly byte-level models. | Nathan Godey, Roman Castagné, Éric de la Clergerie, Benoît Sagot |  |
| 209 |  |  [Towards Intelligent Clinically-Informed Language Analyses of People with Bipolar Disorder and Schizophrenia](https://doi.org/10.18653/v1/2022.findings-emnlp.208) |  | 0 | NLP offers a myriad of opportunities to support mental health research. However, prior work has almost exclusively focused on social media data, for which diagnoses are difficult or impossible to validate. We present a first-of-its-kind dataset of manually transcribed interactions with people clinically diagnosed with bipolar disorder and schizophrenia, as well as healthy controls. Data was collected through validated clinical tasks and paired with diagnostic measures. We extract 100+ temporal, sentiment, psycholinguistic, emotion, and lexical features from the data and establish classification validity using a variety of models to study language differences between diagnostic groups. Our models achieve strong classification performance (maximum F1=0.93-0.96), and lead to the discovery of interesting associations between linguistic features and diagnostic class. It is our hope that this dataset will offer high value to clinical and NLP researchers, with potential for widespread broader impacts. | Ankit Aich, Avery Quynh, Varsha D. Badal, Amy E. Pinkham, Philip D. Harvey, Colin A. Depp, Natalie Parde |  |
| 210 |  |  [Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes](https://doi.org/10.18653/v1/2022.findings-emnlp.209) |  | 0 | Multi-hop Question Answering (QA) is a challenging task since it requires an accurate aggregation of information from multiple context paragraphs and a thorough understanding of the underlying reasoning chains. Recent work in multi-hop QA has shown that performance can be boosted by first decomposing the questions into simpler, single-hop questions. In this paper, we explore one additional utility of the multi-hop decomposition from the perspective of explainable NLP: to create explanation by probing a neural QA model with them. We hypothesize that in doing so, users will be better able to predict when the underlying QA system will give the correct answer. Through human participant studies, we verify that exposing the decomposition probes and answers to the probes to users can increase their ability to predict system performance on a question instance basis. We show that decomposition is an effective form of probing QA systems as well as a promising approach to explanation generation. In-depth analyses show the need for improvements in decomposition systems. | Kaige Xie, Sarah Wiegreffe, Mark O. Riedl |  |
| 211 |  |  [CheckHARD: Checking Hard Labels for Adversarial Text Detection, Prediction Correction, and Perturbed Word Suggestion](https://doi.org/10.18653/v1/2022.findings-emnlp.210) |  | 0 | An adversarial attack generates harmful text that fools a target model. More dangerously, this text is unrecognizable by humans. Existing work detects adversarial text and corrects a target’s prediction by identifying perturbed words and changing them into their synonyms, but many benign words are also changed. In this paper, we directly detect adversarial text, correct the prediction, and suggest perturbed words by checking the change in the hard labels from the target’s predictions after replacing a word with its transformation using a model that we call CheckHARD. The experiments demonstrate that CheckHARD outperforms existing work on various attacks, models, and datasets. | HoangQuoc NguyenSon, Huy Quang Ung, Seira Hidano, Kazuhide Fukushima, Shinsaku Kiyomoto |  |
| 212 |  |  [Mitigating Covertly Unsafe Text within Natural Language Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.211) |  | 0 | An increasingly prevalent problem for intelligent technologies is text safety, as uncontrolled systems may generate recommendations to their users that lead to injury or life-threatening consequences. However, the degree of explicitness of a generated statement that can cause physical harm varies. In this paper, we distinguish types of text that can lead to physical harm and establish one particularly underexplored category: covertly unsafe text. Then, we further break down this category with respect to the system’s information and discuss solutions to mitigate the generation of text in each of these subcategories. Ultimately, our work defines the problem of covertly unsafe language that causes physical harm and argues that this subtle yet dangerous issue needs to be prioritized by stakeholders and regulators. We highlight mitigation strategies to inspire future researchers to tackle this challenging problem and help improve safety within smart systems. | Alex Mei, Anisha Kabir, Sharon Levy, Melanie Subbiah, Emily Allaway, John Judge, Desmond Patton, Bruce Bimber, Kathleen R. McKeown, William Yang Wang |  |
| 213 |  |  ["I Know Who You Are": Character-Based Features for Conversational Humor Recognition in Chinese](https://doi.org/10.18653/v1/2022.findings-emnlp.212) |  | 0 | Humor plays an important role in our daily life, as it is an essential and fascinating element in the communication between persons. Therefore, how to recognize punchlines from the dialogue, i.e. conversational humor recognition, has attracted much interest of computational linguistics communities. However, most existing work attempted to understand the conversational humor by analyzing the contextual information of the dialogue, but neglected the character of the interlocutor, such as age, gender, occupation, and so on. For instance, the same utterance could bring out humorous from a serious person, but may be a plain expression from a naive person. To this end, this paper proposes a Character Fusion Conversational Humor Recognition model (CFCHR) to explore character information to recognize conversational humor. CFCHR utilizes a multi-task learning framework that unifies two highly pertinent tasks, i.e., character extraction and punchline identification. Based on deep neural networks, we trained both tasks jointly by sharing weight to extract the common and task-invariant features while each task could still learn its task-specific features. Experiments were conducted on Chinese sitcoms corpus, which consisted of 12,677 utterances from 22 characters. The experimental results demonstrated that CFCHR could achieve 33.08% improvements in terms of F1-score over some strong baselines, and proved the effectiveness of the character information to identify the punchlines. | Wenbo Shang, Jiangjiang Zhao, Zezhong Wang, Binyang Li, Fangchun Yang, KamFai Wong |  |
| 214 |  |  [DebiasGAN: Eliminating Position Bias in News Recommendation with Adversarial Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.213) |  | 0 | Click behaviors are widely used for learning news recommendation models, but they are heavily affected by the biases brought by the news display positions. It is important to remove position biases to train unbiased recommendation model and capture unbiased user interest. In this paper, we propose a news recommendation method named DebiasGAN that can effectively alleviate position biases via adversarial learning. The core idea is modeling the personalized effect of position bias on click behaviors in a candidate-aware way, and learning debiased candidate-aware user embeddings from which the position information cannot be discriminated. More specifically, we use a bias-aware click model to capture the effect of position bias on click behaviors, and use a bias-invariant click model with random candidate positions to estimate the ideally unbiased click scores. We apply adversarial learning to the embeddings learned by the two models to help the bias-invariant click model capture debiased user interest. Experimental results on two real-world datasets show that DebiasGAN effectively improves news recommendation by eliminating position biases. | Chuhan Wu, Fangzhao Wu, Xiangnan He, Yongfeng Huang |  |
| 215 |  |  [Generating Multiple-Length Summaries via Reinforcement Learning for Unsupervised Sentence Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.214) |  | 0 | Sentence summarization shortens given texts while maintaining core contents of the texts. Unsupervised approaches have been studied to summarize texts without ground-truth summaries. However, recent unsupervised models are extractive, which remove words from texts and thus they are less flexible than abstractive summarization. In this work, we devise an abstractive model based on reinforcement learning without ground-truth summaries. We formulate the unsupervised summarization based on the Markov decision process with rewards representing the summary quality. To further enhance the summary quality, we develop a multi-summary learning mechanism that generates multiple summaries with varying lengths for a given text, while making the summaries mutually enhance each other. Experimental results show that the proposed model substantially outperforms both abstractive and extractive models, yet frequently generating new words not contained in input texts. | Dongmin Hyun, Xiting Wang, Chanyoung Park, Xing Xie, Hwanjo Yu |  |
| 216 |  |  [Multilingual Sentence Transformer as A Multilingual Word Aligner](https://doi.org/10.18653/v1/2022.findings-emnlp.215) |  | 0 | Multilingual pretrained language models (mPLMs) have shown their effectiveness in multilingual word alignment induction. However, these methods usually start from mBERT or XLM-R. In this paper, we investigate whether multilingual sentence Transformer LaBSE is a strong multilingual word aligner. This idea is non-trivial as LaBSE is trained to learn language-agnostic sentence-level embeddings, while the alignment extraction task requires the more fine-grained word-level embeddings to be language-agnostic. We demonstrate that the vanilla LaBSE outperforms other mPLMs currently used in the alignment task, and then propose to finetune LaBSE on parallel corpus for further improvement. Experiment results on seven language pairs show that our best aligner outperforms previous state-of-the-art models of all varieties. In addition, our aligner supports different language pairs in a single model, and even achieves new state-of-the-art on zero-shot language pairs that does not appear in the finetuning process. | Weikang Wang, Guanhua Chen, Hanqing Wang, Yue Han, Yun Chen |  |
| 217 |  |  [CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.216) |  | 0 | Counterfactual data augmentation (CDA) – i.e., adding minimally perturbed inputs during training – helps reduce model reliance on spurious correlations and improves generalization to out-of-distribution (OOD) data. Prior work on generating counterfactuals only considered restricted classes of perturbations, limiting their effectiveness. We present Counterfactual Generation via Retrieval and Editing (CORE), a retrieval-augmented generation framework for creating diverse counterfactual perturbations for CDA. For each training example, CORE first performs a dense retrieval over a task-related unlabeled text corpus using a learned bi-encoder and extracts relevant counterfactual excerpts. CORE then incorporates these into prompts to a large language model with few-shot learning capabilities, for counterfactual editing. Conditioning language model edits on naturally occurring data results in more diverse perturbations. Experiments on natural language inference and sentiment analysis benchmarks show that CORE counterfactuals are more effective at improving generalization to OOD data compared to other DA approaches. We also show that the CORE retrieval framework can be used to encourage diversity in manually authored perturbations. | Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, Luke Zettlemoyer |  |
| 218 |  |  [Conversation Disentanglement with Bi-Level Contrastive Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.217) |  | 0 | Conversation disentanglement aims to group utterances into detached sessions, which is a fundamental task in processing multi-party conversations. Existing methods have two main drawbacks. First, they overemphasize pairwise utterance relations but pay inadequate attention to the utterance-to-context relation modeling. Second, huge amount of human annotated data is required for training, which is expensive to obtain in practice. To address these issues, we propose a general disentangle model based on bi-level contrastive learning. It brings closer utterances in the same session while encourages each utterance to be near its clustered session prototypes in the representation space. Unlike existing approaches, our disentangle model works in both supervised setting with labeled data and unsupervised setting when no such data is available. The proposed method achieves new state-of-the-art performance on both settings across several public datasets. | Chengyu Huang, Zheng Zhang, Hao Fei, Lizi Liao |  |
| 219 |  |  [You can't pick your neighbors, or can you? When and How to Rely on Retrieval in the kNN-LM](https://doi.org/10.18653/v1/2022.findings-emnlp.218) |  | 0 | Retrieval-enhanced language models (LMs), which condition their predictions on text retrieved from large external datastores, have recently shown significant perplexity improvements compared to standard LMs. One such approach, the kNN-LM, interpolates any existing LM’s predictions with the output of a k-nearest neighbors model and requires no additional training. In this paper, we explore the importance of lexical and semantic matching in the context of items retrieved by kNN-LM. We find two trends: (1) the presence of large overlapping n-grams between the datastore and evaluation set plays an important factor in strong performance, even when the datastore is derived from the training data; and (2) the kNN-LM is most beneficial when retrieved items have high semantic similarity with the query. Based on our analysis, we define a new formulation of the kNN-LM that uses retrieval quality to assign the interpolation coefficient. We empirically measure the effectiveness of our approach on two English language modeling datasets, Wikitext-103 and PG-19. Our re-formulation of the kNN-LM is beneficial in both cases, and leads to nearly 4% improvement in perplexity on the Wikitext-103 test set. | Andrew Drozdov, Shufan Wang, Razieh Rahimi, Andrew McCallum, Hamed Zamani, Mohit Iyyer |  |
| 220 |  |  [StuBot: Learning by Teaching a Conversational Agent Through Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.findings-emnlp.219) |  | 0 | This paper proposes StuBot, a text-based conversational agent that provides adaptive feedback for learning by teaching. StuBot first asks the users to teach the learning content by summarizing and explaining it in their own words. After the users inputted the explanation text for teaching, StuBot uses a machine reading comprehension (MRC) engine to provide adaptive feedback with further questions about the insufficient parts of the explanation text. We conducted a within-subject study to evaluate the effectiveness of adaptive feedback by StuBot. Both the quantitative and qualitative results showed that learning by teaching with adaptive feedback can improve learning performance, immersion, and overall experience. | Nayoung Jin, Hana Lee |  |
| 221 |  |  [Improved Universal Sentence Embeddings with Prompt-based Contrastive Learning and Energy-based Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.220) |  | 0 | Contrastive learning has been demonstrated to be effective in enhancing pre-trained language models (PLMs) to derive superior universal sentence embeddings. However, existing contrastive methods still have two limitations. Firstly, previous works may acquire poor performance under domain shift settings, thus hindering the application of sentence representations in practice. We attribute this low performance to the over-parameterization of PLMs with millions of parameters. To alleviate it, we propose PromCSE (Prompt-based Contrastive Learning for Sentence Embeddings), which only trains small-scale Soft Prompt (i.e., a set of trainable vectors) while keeping PLMs fixed. Secondly, the commonly used NT-Xent loss function of contrastive learning does not fully exploit hard negatives in supervised learning settings. To this end, we propose to integrate an Energy-based Hinge loss to enhance the pairwise discriminative power, inspired by the connection between the NT-Xent loss and the Energy-based Learning paradigm. Empirical results on seven standard semantic textual similarity (STS) tasks and a domain-shifted STS task both show the effectiveness of our method compared with the current state-of-the-art sentence embedding models. | Yuxin Jiang, Linhan Zhang, Wei Wang |  |
| 222 |  |  [RaP: Redundancy-aware Video-language Pre-training for Text-Video Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.221) |  | 0 | Video language pre-training methods have mainly adopted sparse sampling techniques to alleviate the temporal redundancy of videos. Though effective, sparse sampling still suffers inter-modal redundancy: visual redundancy and textual redundancy. Compared with highly generalized text, sparsely sampled frames usually contain text-independent portions, called visual redundancy. Sparse sampling is also likely to miss important frames corresponding to some text portions, resulting in textual redundancy. Inter-modal redundancy leads to a mismatch of video and text information, hindering the model from better learning the shared semantics across modalities. To alleviate it, we propose Redundancy-aware Video-language Pre-training. We design a redundancy measurement of video patches and text tokens by calculating the cross-modal minimum dis-similarity. Then, we penalize the high-redundant video patches and text tokens through a proposed redundancy-aware contrastive learning. We evaluate our method on four benchmark datasets, MSRVTT, MSVD, DiDeMo, and LSMDC, achieving a significant improvement over the previous state-of-the-art results. | Xing Wu, Chaochen Gao, Zijia Lin, Zhongyuan Wang, Jizhong Han, Songlin Hu |  |
| 223 |  |  [FCGCL: Fine- and Coarse-Granularity Contrastive Learning for Speech Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.222) |  | 0 | It is notoriously difficult to implement end-to-end speech translation (E2E-ST) model because of the task complexity and data scarcity. Existing techniques often attempt to carry out implicit knowledge transfer from machine translation (MT) to ST model by imposing various constraints. However, in this transfer scenario, a significant problem is that the performance of the MT will drop significantly and the final transfer effect is also restricted. In this article, we recommend Fine and Coarse Granularity Contrastive Learning (FCGCL), which conduct explicit knowledge transfer from MT to ST model. Specially, we ensure through multi granularity contrastive learning that inputs with similar semantic between different modalities are encoded closely in the shared semantic space while inputs with different semantics are kept apart. Experiments on the MuST-C datasets on all 8 languages and further analysis show that our method can effectively improve the E2E-ST performance and achieves an average BLEU of 29.0. | Hao Zhang, Nianwen Si, Yaqi Chen, Zhen Li, Tong Niu, Xukui Yang, Dan Qu |  |
| 224 |  |  [InfoCSE: Information-aggregated Contrastive Learning of Sentence Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.223) |  | 0 | Contrastive learning has been extensively studied in sentence embedding learning, which assumes that the embeddings of different views of the same sentence are closer. The constraint brought by this assumption is weak, and a good sentence representation should also be able to reconstruct the original sentence fragments. Therefore, this paper proposes an information-aggregated contrastive learning framework for learning unsupervised sentence embeddings, termed InfoCSE.InfoCSE forces the representation of [CLS] positions to aggregate denser sentence information by introducing an additional Masked language model task and a well-designed network. We evaluate the proposed InfoCSE on several benchmark datasets w.r.t the semantic text similarity (STS) task. Experimental results show that InfoCSE outperforms SimCSE by an average Spearman correlation of 2.60% on BERT-base, and 1.77% on BERT-large, achieving state-of-the-art results among unsupervised sentence representation learning methods. | Xing Wu, Chaochen Gao, Zijia Lin, Jizhong Han, Zhongyuan Wang, Songlin Hu |  |
| 225 |  |  [Benchmarking Language Models for Code Syntax Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.224) |  | 0 | Pre-trained language models have demonstrated impressive performance in both natural language processing and program understanding, which represent the input as a token sequence without explicitly modeling its structure. Some prior works show that pre-trained language models can capture the syntactic rules of natural languages without finetuning on syntax understanding tasks. However, there is limited understanding of how well pre-trained models understand the code structure so far. In this work, we perform the first thorough benchmarking of the state-of-the-art pre-trained models for identifying the syntactic structures of programs. Specifically, we introduce CodeSyntax, a large-scale dataset of programs annotated with the syntactic relationships in their corresponding abstract syntax trees. Our key observation is that pre-training on massive code data does not result in decent code syntax understanding. In fact, these pre-trained programming language models fail to match the performance of naive baselines based on positional offsets and keywords. We also present a natural language benchmark to highlight the differences between natural languages and programming languages in terms of understanding corresponding syntactic structures. Our findings point out key limitations of existing pre-training methods and suggest the importance of modeling syntactic structures for the programming language. | Da Shen, Xinyun Chen, Chenguang Wang, Koushik Sen, Dawn Song |  |
| 226 |  |  [Learning When and What to Quote: A Quotation Recommender System with Mutual Promotion of Recommendation and Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.225) |  | 0 | This work extends the current quotation recommendation task to a more realistic quotation recommender system that learns to predict when to quote and what to quote jointly. The system consists of three modules (tasks), a prediction module to predict whether to quote given conversation contexts, a recommendation module to recommend suitable quotations and a generation module generating quotations or sentences in ordinary language to continue the conversation. We benchmark several competitive models for the two newly introduced tasks (i.e., when-to-quote and what-to-continue). For quotation recommendation, compared with previous work that is either generation-based or ranking-based recommendation, we propose a novel framework with mutual promotion of generation module and ranking-based recommendation module. Experiments show that our framework achieves significantly better performance than baselines on two datasets. Further experiments and analyses validate the effectiveness of the proposed mechanisms and get a better understanding of the quotation recommendation task. | Lingzhi Wang, Xingshan Zeng, KamFai Wong |  |
| 227 |  |  [Think Beyond Words: Exploring Context-Relevant Visual Commonsense for Diverse Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.226) |  | 0 | Commonsense knowledge has been widely considered for building intelligent open-domain dialogue agents, aiming to generate meaningful and diverse responses. Previous works in this field usually lack the ability to effectively obtain and utilize auxiliary commonsense from the external visual world. In this paper, we argue that exploiting logical information in images related to context can be effective to enrich and steer the generation process. In view of this, we propose VICTOR, a context-relevant VIsual Commonsense enhanced dialogue generaTOR for generating coherent and informative responses. To obtain the associated visual commonsense, we devise a novel approach that expands topic words on the knowledge graph and maps them into daily scenarios. During the generation, the model adopts multimodal fusion mechanism to integrate visual and textual information, and adaptively combine their decoding distributions for better response generation. The experimental results on two public datasets show that our proposed method outperforms the latest competitive methods in terms of coherence and diversity. | Yiting Liu, Liang Li, Beichen Zhang, Qingming Huang |  |
| 228 |  |  [Gender Bias in Meta-Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.227) |  | 0 | Different methods have been proposed to develop meta-embeddings from a given set of source embeddings. However, the source embeddings can contain unfair gender-related biases, and how these influence the meta-embeddings has not been studied yet.We study the gender bias in meta-embeddings created under three different settings:(1) meta-embedding multiple sources without performing any debiasing (Multi-Source No-Debiasing),(2) meta-embedding multiple sources debiased by a single method (Multi-Source Single-Debiasing), and(3) meta-embedding a single source debiased by different methods (Single-Source Multi-Debiasing).Our experimental results show that meta-embedding amplifies the gender biases compared to input source embeddings.We find that debiasing not only the sources but also their meta-embedding is needed to mitigate those biases.Moreover, we propose a novel debiasing method based on meta-embedding learning where we use multiple debiasing methods on a single source embedding and then create a single unbiased meta-embedding. | Masahiro Kaneko, Danushka Bollegala, Naoaki Okazaki |  |
| 229 |  |  [Third-Party Aligner for Neural Word Alignments](https://doi.org/10.18653/v1/2022.findings-emnlp.228) |  | 0 | Word alignment is to find translationally equivalent words between source and target sentences. Previous work has demonstrated that self-training can achieve competitive word alignment results. In this paper, we propose to use word alignments generated by a third-party word aligner to supervise the neural word alignment training. Specifically, source word and target word of each word pair aligned by the third-party aligner are trained to be close neighbors to each other in the contextualized embedding space when fine-tuning a pre-trained cross-lingual language model. Experiments on the benchmarks of various language pairs show that our approach can surprisingly do self-correction over the third-party supervision by finding more accurate word alignments and deleting wrong word alignments, leading to better performance than various third-party word aligners, including the currently best one. When we integrate all supervisions from various third-party aligners, we achieve state-of-the-art word alignment performances, with averagely more than two points lower alignment error rates than the best third-party aligner.We released our code at https://github.com/sdongchuanqi/Third-Party-Supervised-Aligner. | Jinpeng Zhang, Chuanqi Dong, Xiangyu Duan, Yuqi Zhang, Min Zhang |  |
| 230 |  |  [QaDialMoE: Question-answering Dialogue based Fact Verification with Mixture of Experts](https://doi.org/10.18653/v1/2022.findings-emnlp.229) |  | 0 | Fact verification is an essential tool to mitigate the spread of false information online, which has gained a widespread attention recently. However, a fact verification in the question-answering dialogue is still underexplored. In this paper, we propose a neural network based approach called question-answering dialogue based fact verification with mixture of experts (QaDialMoE). It exploits questions and evidence effectively in the verification process and can significantly improve the performance of fact verification. Specifically, we exploit the mixture of experts to focus on various interactions among responses, questions and evidence. A manager with an attention guidance module is implemented to guide the training of experts and assign a reasonable attention score to each expert. A prompt module is developed to generate synthetic questions that make our approach more generalizable. Finally, we evaluate the QaDialMoE and conduct a comparative study on three benchmark datasets. The experimental results demonstrate that our QaDialMoE outperforms previous approaches by a large margin and achieves new state-of-the-art results on all benchmarks. This includes the accuracy improvements on the HEALTHVER as 84.26%, the FAVIQ A dev set as 78.7%, the FAVIQ R dev set as 86.1%, test set as 86.0%, and the COLLOQUIAL as 89.5%. To our best knowledge, this is the first work to investigate a question-answering dialogue based fact verification, and achieves new state-of-the-art results on various benchmark datasets. | Longzheng Wang, Peng Zhang, Xiaoyu Lu, Lei Zhang, Chaoyang Yan, Chuang Zhang |  |
| 231 |  |  [Multimodal Knowledge Learning for Named Entity Disambiguation](https://doi.org/10.18653/v1/2022.findings-emnlp.230) |  | 0 | With the popularity of online social media, massive-scale multimodal information has brought new challenges to traditional Named Entity Disambiguation (NED) tasks. Recently, Multimodal Named Entity Disambiguation (MNED) has been proposed to link ambiguous mentions with the textual and visual contexts to a predefined knowledge graph. Existing attempts usually perform MNED by annotating multimodal mentions and adding multimodal features to traditional NED models. However, these studies may suffer from 1) failing to model multimodal information at the knowledge level, and 2) lacking multimodal annotation data against the large-scale unlabeled corpus. In this paper, we explore a pioneer study on leveraging multimodal knowledge learning to address the MNED task. Specifically, we first harvest multimodal knowledge in the Meta-Learning way, which is much easier than collecting ambiguous mention corpus. Then we design a knowledge-guided transfer learning strategy to extract unified representation from different modalities. Finally, we propose an Interactive Multimodal Learning Network (IMN) to fully utilize the multimodal information on both the mention and knowledge sides. Extensive experiments conducted on two public MNED datasets demonstrate that the proposed method achieves improvements over the state-of-the-art multimodal methods. | Dongjie Zhang, Longtao Huang |  |
| 232 |  |  [Generative Prompt Tuning for Relation Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.231) |  | 0 | Using prompts to explore the knowledge contained within pre-trained language models for downstream tasks has now become an active topic. Current prompt tuning methods mostly convert the downstream tasks to masked language modeling problems by adding cloze-style phrases and mapping all labels to verbalizations with fixed length, which has proven effective for tasks with simple label spaces. However, when applied to relation classification exhibiting complex label spaces, vanilla prompt tuning methods may struggle with label verbalizations with arbitrary lengths due to rigid prompt restrictions. Inspired by the text infilling task for pre-training generative models that can flexibly predict missing spans, we propose a novel generative prompt tuning method to reformulate relation classification as an infilling problem, which frees our approach from limitations of current prompt based approaches and thus fully exploits rich semantics of entity and relation types. In addition, we design entity-guided decoding and discriminative relation scoring to generate and align relations effectively and efficiently during inference. Extensive experiments under fully supervised settings and low-resource settings demonstrate the effectiveness of our approach. | Jiale Han, Shuai Zhao, Bo Cheng, Shengkun Ma, Wei Lu |  |
| 233 |  |  [Formulating Few-shot Fine-tuning Towards Language Model Pre-training: A Pilot Study on Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.232) |  | 0 | Fine-tuning pre-trained language models is a common practice in building NLP models for various tasks, including the case with less supervision. We argue that under the few-shot setting, formulating fine-tuning closer to the pre-training objective shall be able to unleash more benefits from the pre-trained language models. In this work, we take few-shot named entity recognition (NER) for a pilot study, where existing fine-tuning strategies are much different from pre-training. We propose a novel few-shot fine-tuning framework for NER, FFF-NER. Specifically, we introduce three new types of tokens, “is-entity”, “which-type” and “bracket”, so we can formulate the NER fine-tuning as (masked) token prediction or generation, depending on the choice of the pre-training objective. In our experiments, we apply to fine-tune both BERT and BART for few-shot NER on several benchmark datasets and observe significant improvements over existing fine-tuning strategies, including sequence labeling, prototype meta-learning, and prompt-based approaches. We further perform a series of ablation studies, showing few-shot NER performance is strongly correlated with the similarity between fine-tuning and pre-training. | Zihan Wang, Kewen Zhao, Zilong Wang, Jingbo Shang |  |
| 234 |  |  [Masked Language Models Know Which are Popular: A Simple Ranking Strategy for Commonsense Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.233) |  | 0 | We propose a simple ranking strategy to solve a generative commonsense question answering (QA) problem. Compared with multiple-choice QA, it is challenging because the answers to a question are not unique and they are supposed to be popular and diverse. Our strategy exploits the dataset itself and negative samples that we collect from WordNet to train a ranker that picks out the most popular answers for commonsense questions. The effectiveness of our strategy is verified on different pre-trained masked language models (MLMs) in a pipeline framework, where an MLM reranks the generated answers. Further, we explore an end-to-end framework where MLMs are utilized to guide the generation of generative language models (GLMs). Taking advantage of reinforcement learning, we apply policy gradient to train a GLM with the rewards fed back by an MLM. Empirical results on ProtoQA dataset demonstrate that MLMs can acquire the ability to distinguish the popular answers and improve the typical answer generation of GLMs as well. | Xuan Luo, Chuang Fan, Yice Zhang, Wanguo Jiang, Bing Qin, Ruifeng Xu |  |
| 235 |  |  [DialogUSR: Complex Dialogue Utterance Splitting and Reformulation for Multiple Intent Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.234) |  | 0 | While interacting with chatbots, users may elicit multiple intents in a single dialogue utterance. Instead of training a dedicated multi-intent detection model, we propose DialogUSR, a dialogue utterance splitting and reformulation task that first splits multi-intent user query into several single-intent sub-queries and then recovers all the coreferred and omitted information in the sub-queries. DialogUSR can serve as a plug-in and domain-agnostic module that empowers the multi-intent detection for the deployed chatbots with minimal efforts. We collect a high-quality naturally occurring dataset that covers 23 domains with a multi-step crowd-souring procedure. To benchmark the proposed dataset, we propose multiple action-based generative models that involve end-to-end and two-stage training, and conduct in-depth analyses on the pros and cons of the proposed baselines. | Haoran Meng, Zheng Xin, Tianyu Liu, Zizhen Wang, He Feng, Binghuai Lin, Xuemin Zhao, Yunbo Cao, Zhifang Sui |  |
| 236 |  |  [Low-resource Interactive Active Labeling for Fine-tuning Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.235) |  | 0 | Recently, active learning (AL) methods have been used to effectively fine-tune pre-trained language models for various NLP tasks such as sentiment analysis and document classification. However, given the task of fine-tuning language models, understanding the impact of different aspects on AL methods such as labeling cost, sample acquisition latency, and the diversity of the datasets necessitates a deeper investigation. This paper examines the performance of existing AL methods within a low-resource, interactive labeling setting. We observe that existing methods often underperform in such a setting while exhibiting higher latency and a lack of generalizability. To overcome these challenges, we propose a novel active learning method TYROUGE that employs a hybrid sampling strategy to minimize labeling cost and acquisition latency while providing a framework for adapting to dataset diversity via user guidance. Through our experiments, we observe that compared to SOTA methods, TYROUGE reduces the labeling cost by up to 43% and the acquisition latency by as much as 11X, while achieving comparable accuracy. Finally, we discuss the strengths and weaknesses of TYROUGE by exploring the impact of dataset characteristics. | Seiji Maekawa, Dan Zhang, Hannah Kim, Sajjadur Rahman, Estevam Hruschka |  |
| 237 |  |  [Getting the Most out of Simile Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.236) |  | 0 | Simile recognition involves two subtasks: simile sentence classification that discriminates whether a sentence contains simile, and simile component extraction that locates the corresponding objects (i.e., tenors and vehicles).Recent work ignores features other than surface strings and suffers from the data hunger issue.We explore expressive features for this task to help achieve more effective data utilization.In particular, we study two types of features: 1) input-side features that include POS tags, dependency trees and word definitions, and 2) decoding features that capture the interdependence among various decoding decisions.We further construct a model named HGSR, which merges the input-side features as a heterogeneous graph and leverages decoding features via distillation.Experiments show that HGSR significantly outperforms the current state-of-the-art systems and carefully designed baselines, verifying the effectiveness of introduced features. We will release our code upon paper acceptance. | Xiaoyue Wang, Linfeng Song, Xin Liu, Chulun Zhou, Hualin Zeng, Jinsong Su |  |
| 238 |  |  [A Unified Framework for Pun Generation with Humor Principles](https://doi.org/10.18653/v1/2022.findings-emnlp.237) |  | 0 | We propose a unified framework to generate both homophonic and homographic puns to resolve the split-up in existing works. Specifically, we incorporate three linguistic attributes of puns to the language models: ambiguity, distinctiveness, and surprise. Our framework consists of three parts: 1) a context words/phrases selector to promote the aforementioned attributes, 2) a generation model trained on non-pun sentences to incorporate the context words/phrases into the generation output, and 3) a label predictor that learns the structure of puns which is used to steer the generation model at inference time. Evaluation results on both pun types demonstrate the efficacy of our model over strong baselines. | Yufei Tian, Divyanshu Sheth, Nanyun Peng |  |
| 239 |  |  [Improving English-Arabic Transliteration with Phonemic Memories](https://doi.org/10.18653/v1/2022.findings-emnlp.238) |  | 0 | Transliteration is an important task in natural language processing (NLP) which aims to convert a name in the source language to the target language without changing its pronunciation. Particularly, transliteration from English to Arabic is highly needed in many applications, especially in countries (e.g., United Arab Emirates (UAE)) whose most citizens are foreigners but the official language is Arabic. In such a task-oriented scenario, namely transliterating the English names to the corresponding Arabic ones, the performance of the transliteration model is highly important. However, most existing neural approaches mainly apply a universal transliteration model with advanced encoders and decoders to the task, where limited attention is paid to leveraging the phonemic association between English and Arabic to further improve model performance. In this paper, we focus on transliteration of people’s names from English to Arabic for the general public. In doing so, we collect a corpus named EANames by extracting high quality name pairs from online resources which better represent the names in the general public than linked Wikipedia entries that are always names of famous people). We propose a model for English-Arabic transliteration, where a memory module modeling the phonemic association between English and Arabic is used to guide the transliteration process. We run experiments on the collected data and the results demonstrate the effectiveness of our approach for English-Arabic transliteration. | Yuanhe Tian, Renze Lou, Xiangyu Pang, Lianxi Wang, Shengyi Jiang, Yan Song |  |
| 240 |  |  [Mix-and-Match: Scalable Dialog Response Retrieval using Gaussian Mixture Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.239) |  | 0 | Embedding-based approaches for dialog response retrieval embed the context-response pairs as points in the embedding space. These approaches are scalable, but fail to account for the complex, many-to-many relationships that exist between context-response pairs. On the other end of the spectrum, there are approaches that feed the context-response pairs jointly through multiple layers of neural networks. These approaches can model the complex relationships between context-response pairs, but fail to scale when the set of responses is moderately large (>1000). In this paper, we propose a scalable model that can learn complex relationships between context-response pairs. Specifically, the model maps the contexts as well as responses to probability distributions over the embedding space. We train the models by optimizing the Kullback-Leibler divergence between the distributions induced by context-response pairs in the training data. We show that the resultant model achieves better performance as compared to other embedding-based approaches on publicly available conversation data. | Gaurav Pandey, Danish Contractor, Sachindra Joshi |  |
| 241 |  |  [AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.240) |  | 0 | There are growing interests in adapting large-scale language models using parameter-efficient fine-tuning methods. However, accelerating the model itself and achieving better inference efficiency through model compression has not been thoroughly explored yet.Model compression could provide the benefits of reducing memory footprints, enabling low-precision computations, and ultimately achieving cost-effective inference.To combine parameter-efficient adaptation and model compression, we propose AlphaTuning consisting of post-training quantization of the pre-trained language model and fine-tuning only some parts of quantized parameters for a target task.Specifically, AlphaTuning works by employing binary-coding quantization, which factorizes the full-precision parameters into binary parameters and a separate set of scaling factors.During the adaptation phase, the binary values are frozen for all tasks, while the scaling factors are fine-tuned for the downstream task.We demonstrate that AlphaTuning, when applied to GPT-2 and OPT, performs competitively with full fine-tuning on a variety of downstream tasks while achieving >10x compression ratio under 4-bit quantization and >1,000x reduction in the number of trainable parameters. | Se Jung Kwon, Jeonghoon Kim, Jeongin Bae, Kang Min Yoo, JinHwa Kim, Baeseong Park, Byeongwook Kim, JungWoo Ha, Nako Sung, Dongsoo Lee |  |
| 242 |  |  [Learning Invariant Representation Improves Robustness for MRC Models](https://doi.org/10.18653/v1/2022.findings-emnlp.241) |  | 0 | The prosperity of Pretrained Language Models(PLM) has greatly promoted the development of Machine Reading Comprehension (MRC). However, these models are vulnerable and not robust to adversarial examples. In this paper, we propose Stable and Contrastive Question Answering (SCQA) to improve invariance of representation to alleviate these robustness issues. Specifically, we first construct positive example pairs which have same answer through data augmentation. Then SCQA learns enhanced representations with better alignment between positive pairs by introducing stability and contrastive loss. Experimental results show that our approach can boost the robustness of QA models cross different MRC tasks and attack sets significantly and consistently. | Yu Hai, Liang Wen, Haoran Meng, Tianyu Liu, Houfeng Wang |  |
| 243 |  |  [ER-Test: Evaluating Explanation Regularization Methods for Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.242) |  | 0 | By explaining how humans would solve a given task, human rationales can provide strong learning signal for neural language models (NLMs). Explanation regularization (ER) aims to improve NLM generalization by pushing the NLM’s machine rationales (Which input tokens did the NLM focus on?) to align with human rationales (Which input tokens would humans focus on). Though prior works primarily study ER via in-distribution (ID) evaluation, out-of-distribution (OOD) generalization is often more critical in real-world scenarios, yet ER’s effect on OOD generalization has been underexplored.In this paper, we introduce ER-Test, a framework for evaluating ER models’ OOD generalization along three dimensions: unseen datasets, contrast set tests, and functional tests. Using ER-Test, we comprehensively analyze how ER models’ OOD generalization varies with the rationale alignment criterion (loss function), human rationale type (instance-level v/s task-level), number and choice of rationale-annotated instances, and time budget for rationale annotation. Across two tasks and six datasets, we show that ER has little impact on ID performance but yields large OOD performance gains, with the best ER criterion being task-dependent. Also, ER can improve OOD performance even with task-level or few human rationales. Finally, we find that rationale annotation is more time-efficient than label annotation for improving OOD performance. Our results with ER-Test help demonstrate ER’s utility and establish best practices for using ER effectively. | Brihi Joshi, Aaron Chan, Ziyi Liu, Shaoliang Nie, Maziar Sanjabi, Hamed Firooz, Xiang Ren |  |
| 244 |  |  [Learning Cooperative Interactions for Multi-Overlap Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.243) |  | 0 | Aspect sentiment triplet extraction (ASTE) is an essential task, which aims to extract triplets(aspect, opinion, sentiment). However, overlapped triplets, especially multi-overlap triplets,make ASTE a challenge. Most existing methods suffer from multi-overlap triplets becausethey focus on the single interactions between an aspect and an opinion. To solve the aboveissues, we propose a novel multi-overlap triplet extraction method, which decodes the complexrelations between multiple aspects and opinions by learning their cooperative interactions. Overall, the method is based on an encoder-decoder architecture. During decoding, we design ajoint decoding mechanism, which employs a multi-channel strategy to generate aspects andopinions through the cooperative interactions between them jointly. Furthermore, we constructa correlation-enhanced network to reinforce the interactions between related aspectsand opinions for sentiment prediction. Besides, a relation-wise calibration scheme is adoptedto further improve performance. Experiments show that our method outperforms baselines,especially multi-overlap triplets. | Shiman Zhao, Wei Chen, Tengjiao Wang |  |
| 245 |  |  [Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Parameter-Efficient Tuning](https://doi.org/10.18653/v1/2022.findings-emnlp.244) |  | 0 | Delta tuning (DET, also known as parameter-efficient tuning) is deemed as the new paradigm for using pre-trained language models (PLMs). Up to now, various DETs with distinct design elements have been proposed, achieving performance on par with fine-tuning. However, the mechanisms behind the above success are still under-explored, especially the connections among various DETs. To fathom the mystery, we hypothesize that the adaptations of different DETs could all be reparameterized as low-dimensional optimizations in a unified optimization subspace, which could be found by jointly decomposing independent solutions of different DETs. Then we explore the connections among different DETs by conducting optimization within the subspace. In experiments, we find that, for a certain DET, conducting optimization simply in the subspace could achieve comparable performance to its original space, and the found solution in the subspace could be transferred to another DET and achieve non-trivial performance. We also visualize the performance landscape of the subspace, and find that, there exists a substantial region where different DETs all perform well. Finally, we extend our analysis and show the strong connections between fine-tuning and DETs. The codes are publicly available at https://github.com/thunlp/Unified-DeltaTuning. | Jing Yi, Weize Chen, Yujia Qin, Yankai Lin, Ning Ding, Xu Han, Zhiyuan Liu, Maosong Sun, Jie Zhou |  |
| 246 |  |  [Explainable Slot Type Attentions to Improve Joint Intent Detection and Slot Filling](https://doi.org/10.18653/v1/2022.findings-emnlp.245) |  | 0 | Joint intent detection and slot filling is a key research topic in natural language understanding (NLU). Existing joint intent and slot filling systems analyze and compute features collectively for all slot types, and importantly, have no way to explain the slot filling model decisions. In this work, we propose a novel approach that: (i) learns to generate additional slot type specific features in order to improve accuracy and (ii) provides explanations for slot filling decisions for the first time in a joint NLU model. We perform an additional constrained supervision using a set of binary classifiers for the slot type specific feature learning, thus ensuring appropriate attention weights are learned in the process to explain slot filling decisions for utterances. Our model is inherently explainable and does not need any post-hoc processing. We evaluate our approach on two widely used datasets and show accuracy improvements. Moreover, a detailed analysis is also provided for the exclusive slot explainability. | Kalpa Gunaratna, Vijay Srinivasan, Akhila Yerukola, Hongxia Jin |  |
| 247 |  |  [PseudoReasoner: Leveraging Pseudo Labels for Commonsense Knowledge Base Population](https://doi.org/10.18653/v1/2022.findings-emnlp.246) |  | 0 | Commonsense Knowledge Base (CSKB) Population aims at reasoning over unseen entities and assertions on CSKBs, and is an important yet hard commonsense reasoning task. One challenge is that it requires out-of-domain generalization ability as the source CSKB for training is of a relatively smaller scale (1M) while the whole candidate space for population is way larger (200M). We propose PseudoReasoner, a semi-supervised learning framework for CSKB population that uses a teacher model pre-trained on CSKBs to provide pseudo labels on the unlabeled candidate dataset for a student model to learn from. The teacher can be a generative model rather than restricted to discriminative models as previous works.In addition, we design a new filtering procedure for pseudo labels based on influence function and the student model’s prediction to further improve the performance. The framework can improve the backbone model KG-BERT (RoBERTa-large) by 3.3 points on the overall performance and especially, 5.3 points on the out-of-domain performance, and achieves the state-of-the-art. The codes will be made public on acceptance. Codes and data are available at https://github.com/HKUST-KnowComp/PseudoReasoner. | Tianqing Fang, Quyet V. Do, Hongming Zhang, Yangqiu Song, Ginny Y. Wong, Simon See |  |
| 248 |  |  [History-Aware Hierarchical Transformer for Multi-session Open-domain Dialogue System](https://doi.org/10.18653/v1/2022.findings-emnlp.247) |  | 0 | With the evolution of pre-trained language models, current open-domain dialogue systems have achieved great progress in conducting one-session conversations. In contrast, Multi-Session Conversation (MSC), which consists of multiple sessions over a long term with the same user, is under-investigated. In this paper, we propose History-Aware Hierarchical Transformer (HAHT) for multi-session open-domain dialogue. HAHT maintains a long-term memory of history conversations and utilizes history information to understand current conversation context and generate well-informed and context-relevant responses. Specifically, HAHT first encodes history conversation sessions hierarchically into a history memory. Then, HAHT leverages historical information to facilitate the understanding of the current conversation context by encoding the history memory together with the current context with attention-based mechanisms. Finally, to explicitly utilize historical information, HAHT uses a history-aware response generator that switches between a generic vocabulary and a history-aware vocabulary. Experimental results on a large-scale MSC dataset suggest that the proposed HAHT model consistently outperforms baseline models. Human evaluation results support that HAHT generates more human-like, context-relevant, and history-relevant responses than baseline models. | Tong Zhang, Yong Liu, Boyang Li, Zhiwei Zeng, Pengwei Wang, Yuan You, Chunyan Miao, Lizhen Cui |  |
| 249 |  |  [Guiding Abstractive Dialogue Summarization with Content Planning](https://doi.org/10.18653/v1/2022.findings-emnlp.248) |  | 0 | Abstractive dialogue summarization has recently been receiving more attention. We propose a coarse-to-fine model for generating abstractive dialogue summaries, and introduce a fact-aware reinforcement learning (RL) objective that improves the fact consistency between the dialogue and the generated summary. Initially, the model generates the predicate-argument spans of the dialogue, and then generates the final summary through a fact-aware RL objective. Extensive experiments and analysis on two benchmark datasets demonstrate that our proposed method effectively improves the quality of the generated summary, especially in coherence and consistency. | Ye Wang, Xiaojun Wan, Zhiping Cai |  |
| 250 |  |  [Truncation Sampling as Language Model Desmoothing](https://doi.org/10.18653/v1/2022.findings-emnlp.249) |  | 0 | Long samples of text from neural language models can be of poor quality. Truncation sampling algorithms–like top-p or top-k—address this by setting some words’ probabilities to zero at each step. This work investigates why these methods are important, and how to improve them. We propose thinking of a neural language model as a mixture of a true distribution and a smoothing distribution that avoids infinite perplexity. In this light, truncation algorithms aim to perform desmoothing, estimating a subset of the support of the true distribution. Finding a good subset is crucial: we show that top-p unnecessarily truncates high-probability words, for example causing it to truncate all words but Trump for a document that starts with Donald. We introduce eta-sampling, which truncates words below an entropy-dependent probability threshold. Compared to previous algorithms, our eta-sampling generates more plausible long documents according to humans, is better at breaking out of repetition, and behaves more reasonably on a battery of test distributions. | John Hewitt, Christopher D. Manning, Percy Liang |  |
| 251 |  |  [Knowledge-grounded Dialog State Tracking](https://doi.org/10.18653/v1/2022.findings-emnlp.250) |  | 0 | Knowledge (including structured knowledge such as schema and ontology and unstructured knowledge such as web corpus) is a critical part of dialog understanding, especially for unseen tasks and domains. Traditionally, such domain-specific knowledge is encoded implicitly into model parameters for the execution of downstream tasks, which makes training inefficient. In addition , such models are not easily transferable to new tasks with different schemas. In this work, we propose to perform dialog state tracking grounded on knowledge encoded externally. We query relevant knowledge of various forms based on the dialog context where such information can grounds the prediction of dialog states. We demonstrate superior performance of our proposed method over strong baselines, especially in the few-shot learning setting. | Dian Yu, Mingqiu Wang, Yuan Cao, Laurent El Shafey, Izhak Shafran, Hagen Soltau |  |
| 252 |  |  [Context-aware Information-theoretic Causal De-biasing for Interactive Sequence Labeling](https://doi.org/10.18653/v1/2022.findings-emnlp.251) |  | 0 | Supervised training of existing deep learning models for sequence labeling relies on large scale labeled datasets. Such datasets are generally created with crowd-source labeling. However, crowd-source labeling for tasks of sequence labeling can be expensive and time-consuming. Further, crowd-source labeling by external annotators may not be appropriate for data that contains user private information. Considering the above limitations of crowd-source labeling, we study interactive sequence labeling that allows training directly with the user feedback, which alleviates the annotation cost and maintains the user privacy. We identify two bias, namely, context bias and feedback bias, by formulating interactive sequence labeling via a Structural Causal Model (SCM). To alleviate the context and feedback bias based on the SCM, we identify the frequent context tokens as confounders in the backdoor adjustment and further propose an entropy-based modulation that is inspired by information theory. entities more sample-efficiently. With extensive experiments, we validate that our approach can effectively alleviate the biases and our models can be efficiently learnt with the user feedback. | Junda Wu, Rui Wang, Tong Yu, Ruiyi Zhang, Handong Zhao, Shuai Li, Ricardo Henao, Ani Nenkova |  |
| 253 |  |  [Simple but Challenging: Natural Language Inference Models Fail on Simple Sentences](https://doi.org/10.18653/v1/2022.findings-emnlp.252) |  | 0 | Natural language inference (NLI) is a task to infer the relationship between a premise and a hypothesis (e.g., entailment, neutral, or contradiction), and transformer-based models perform well on current NLI datasets such as MNLI and SNLI. Nevertheless, given the linguistic complexity of the large-scale datasets, it remains controversial whether these models can truly infer the relationship between sentences or they simply guess the answer via shallow heuristics. Here, we introduce a controlled evaluation set called Simple Pair to test the basic sentence inference ability of NLI models using sentences with syntactically simple structures. Three popular transformer-based models, i.e., BERT, RoBERTa, and DeBERTa, are employed. We find that these models fine-tuned on MNLI or SNLI perform very poorly on Simple Pair (< 35.4% accuracy). Further analyses reveal event coreference and compositional binding problems in these models. To improve the model performance, we augment the training set, i.e., MNLI or SNLI, with a few examples constructed based on Simple Pair ( 1% of the size of the original SNLI/MNLI training sets). Models fine-tuned on the augmented training set maintain high performance on MNLI/SNLI and perform very well on Simple Pair (~100% accuracy). Furthermore, the positive performance of the augmented training models can transfer to more complex examples constructed based on sentences from MNLI and SNLI. Taken together, the current work shows that (1) models achieving high accuracy on mainstream large-scale datasets still lack the capacity to draw accurate inferences on simple sentences, and (2) augmenting mainstream datasets with a small number of target simple sentences can effectively improve model performance. | Cheng Luo, Wei Liu, Jieyu Lin, Jiajie Zou, Ming Xiang, Nai Ding |  |
| 254 |  |  [DORE: Document Ordered Relation Extraction based on Generative Framework](https://doi.org/10.18653/v1/2022.findings-emnlp.253) |  | 0 | In recent years, there is a surge of generation-based information extraction work, which allows a more direct use of pre-trained language models and efficiently captures output dependencies. However, previous generative methods using lexical representation do not naturally fit document-level relation extraction (DocRE) where there are multiple entities and relational facts. In this paper, we investigate the root cause of the underwhelming performance of the existing generative DocRE models and discover that the culprit is the inadequacy of the training paradigm, instead of the capacities of the models. We propose to generate a symbolic and ordered sequence from the relation matrix which is deterministic and easier for model to learn. Moreover, we design a parallel row generation method to process overlong target sequences. Besides, we introduce several negative sampling strategies to improve the performance with balanced signals. Experimental results on four datasets show that our proposed method can improve the performance of the generative DocRE models. | Qipeng Guo, Yuqing Yang, Hang Yan, Xipeng Qiu, Zheng Zhang |  |
| 255 |  |  [Explicit Role Interaction Network for Event Argument Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.254) |  | 0 | Event argument extraction is a challenging subtask of event extraction, aiming to identify and assign roles to arguments under a certain event. Existing methods extract arguments of each role independently, ignoring the relationship between different roles. Such an approach hinders the model from learning explicit interactions between different roles to improve the performance of individual argument extraction. As a solution, we design a neural model that we refer to as the Explicit Role Interaction Network (ERIN) which allows for dynamically capturing the correlations between different argument roles within an event. Extensive experiments on the benchmark dataset ACE2005 demonstrate the superiority of our proposed model to existing approaches. | Nan Ding, Chunming Hu, Kai Sun, Samuel Mensah, Richong Zhang |  |
| 256 |  |  [Few-Shot Out-of-Domain Transfer Learning of Natural Language Explanations in a Label-Abundant Setup](https://doi.org/10.18653/v1/2022.findings-emnlp.255) |  | 0 | Training a model to provide natural language explanations (NLEs) for its predictions usually requires the acquisition of task-specific NLEs, which is time- and resource-consuming. A potential solution is the few-shot out-of-domain transfer of NLEs from a parent task with many NLEs to a child task.In this work, we examine the setup in which the child task has few NLEs but abundant labels. We establish four few-shot transfer learning methods that cover the possible fine-tuning combinations of the labels and NLEs for the parent and child tasks. We transfer explainability from a large natural language inference dataset (e-SNLI) separately to two child tasks: (1) hard cases of pronoun resolution, where we introduce the small-e-WinoGrande dataset of NLEs on top of the WinoGrande dataset, and (2) commonsense validation (ComVE). Our results demonstrate that the parent task helps with NLE generation and we establish the best methods for this setup. | Yordan Yordanov, Vid Kocijan, Thomas Lukasiewicz, OanaMaria Camburu |  |
| 257 |  |  [RoChBert: Towards Robust BERT Fine-tuning for Chinese](https://doi.org/10.18653/v1/2022.findings-emnlp.256) |  | 0 | Despite of the superb performance on a wide range of tasks, pre-trained language models (e.g., BERT) have been proved vulnerable to adversarial texts. In this paper, we present RoChBERT, a framework to build more Robust BERT-based models by utilizing a more comprehensive adversarial graph to fuse Chinese phonetic and glyph features into pre-trained representations during fine-tuning. Inspired by curriculum learning, we further propose to augment the training dataset with adversarial texts in combination with intermediate samples. Extensive experiments demonstrate that RoChBERT outperforms previous methods in significant ways: (i) robust – RoChBERT greatly improves the model robustness without sacrificing accuracy on benign texts. Specifically, the defense lowers the success rates of unlimited and limited attacks by 59.43% and 39.33% respectively, while remaining accuracy of 93.30%; (ii) flexible – RoChBERT can easily extend to various language models to solve different downstream tasks with excellent performance; and (iii) efficient – RoChBERT can be directly applied to the fine-tuning stage without pre-training language model from scratch, and the proposed data augmentation method is also low-cost. | Zihan Zhang, Jinfeng Li, Ning Shi, Bo Yuan, Xiangyu Liu, Rong Zhang, Hui Xue, Donghong Sun, Chao Zhang |  |
| 258 |  |  [Lexical Entailment with Hierarchy Representations by Deep Metric Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.257) |  | 0 | In this paper, we introduce a novel method for lexical entailment tasks, which detects a hyponym-hypernym relation among words. Existing lexical entailment studies are lacking in generalization performance, as they cannot be applied to words that are not included in the training dataset. Moreover, existing work evaluates the performance by using the dataset that contains words used for training. This study proposes a method that learns a mapping from word embeddings to the hierarchical embeddings in order to predict the hypernymy relations of any input words. To validate the generalization performance, we conduct experiments using a train dataset that does not overlap with the evaluation dataset. As a result, our method achieved state-of-the-art performance and showed robustness for unknown words. | Naomi Sato, Masaru Isonuma, Kimitaka Asatani, Shoya Ishizuka, Aori Shimizu, Ichiro Sakata |  |
| 259 |  |  [Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation](https://doi.org/10.18653/v1/2022.findings-emnlp.258) |  | 0 | Prompt tuning, or the conditioning of a frozen pretrained language model (PLM) with soft prompts learned from data, has demonstrated impressive performance on a wide range of NLP tasks. However, prompt tuning requires a large training dataset to be effective and is outperformed by finetuning the entire PLM in data-scarce regimes. Previous work (Gu et al., 2022, Vu et al., 2022) proposed to transfer soft prompts pretrained on the source domain to the target domain. In this paper, we explore domain adaptation for prompt tuning, a problem setting where unlabeled data from the target domain are available during pretraining. We propose bOosting Prompt TunIng with doMain Adaptation (OPTIMA), which regularizes the decision boundary to be smooth around regions where source and target data distributions are similar. Extensive experiments demonstrate that OPTIMA significantly enhances the transferability and sample-efficiency of prompt tuning compared to strong baselines. Moreover, in few-shot settings, OPTIMA exceeds full-model tuning by a large margin. | Xu Guo, Boyang Li, Han Yu |  |
| 260 |  |  [McPhraSy: Multi-Context Phrase Similarity and Clustering](https://doi.org/10.18653/v1/2022.findings-emnlp.259) |  | 0 | Phrase similarity is a key component of many NLP applications. Current phrase similarity methods focus on embedding the phrase itself and use the phrase context only during training of the pretrained model. To better leverage the information in the context, we propose McPhraSy (Multi-context Phrase Similarity), a novel algorithm for estimating the similarity of phrases based on multiple contexts. At inference time, McPhraSy represents each phrase by considering multiple contexts in which it appears and computes the similarity of two phrases by aggregating the pairwise similarities between the contexts of the phrases. Incorporating context during inference enables McPhraSy to outperform current state-of-the-art models on two phrase similarity datasets by up to 13.3%. Finally, we also present a new downstream task that relies on phrase similarity – keyphrase clustering – and create a new benchmark for it in the product reviews domain. We show that McPhraSy surpasses all other baselines for this task. | Amir David Nissan Cohen, Hila Gonen, Ori Shapira, Ran Levy, Yoav Goldberg |  |
| 261 |  |  [CANarEx: Contextually Aware Narrative Extraction for Semantically Rich Text-as-data Applications](https://doi.org/10.18653/v1/2022.findings-emnlp.260) |  | 0 | Narrative modelling is an area of active research, motivated by the acknowledgement of narratives as drivers of societal decision making. These research efforts conceptualize narratives as connected entity chains, and modeling typically focuses on the identification of entities and their connections within a text. An emerging approach to narrative modelling is the use of semantic role labeling (SRL) to extract Entity-Verb-Entity (E-V-Es) tuples from a text, followed by dimensionality reduction to reduce the space of entities and connections separately. This process penalises the semantic richness of narratives and discards much contextual information along the way. Here, we propose an alternate narrative extraction approach - CANarEx, incorporating a pipeline of common contextual constructs through co-reference resolution, micro-narrative generation and clustering of these narratives through sentence embeddings. We evaluate our approach through testing the recovery of “narrative time-series clusters”, mimicking a desirable text-as-data task. The evaluation framework leverages synthetic data generated using a GPT-3 model. The GPT-3 model is trained to generate similar sentences using a large dataset of news articles. The synthetic data maps to three topics in the news dataset. We then generate narrative time-series document cluster representations by mapping the synthetic data to three distinct signals synthetically injected into the testing corpus. Evaluation results demonstrate the superior ability of CANarEx to recover narrative time-series through reduced MSE and improved precision/recall relative to existing methods. The validity is further reinforced through ablation studies and qualitative analysis. | Nandini Anantharama, Simon D. Angus, Lachlan O'Neill |  |
| 262 |  |  [Narrate Dialogues for Better Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.261) |  | 0 | Dialogue summarization models aim to generate a concise and accurate summary for multi-party dialogue. The complexity of dialogue, including coreference, dialogue acts, and inter-speaker interactions bring unique challenges to dialogue summarization. Most recent neural models achieve state-of-art performance following the pretrain-then-finetune recipe, where the large-scale language model (LLM) is pretrained on large-scale single-speaker written text, but later finetuned on multi-speaker dialogue text. To mitigate the gap between pretraining and finetuning, we propose several approaches to convert the dialogue into a third-person narrative style and show that the narration serves as a valuable annotation for LLMs. Empirical results on three benchmark datasets show our simple approach achieves higher scores on the ROUGE and a factual correctness metric. | Ruochen Xu, Chenguang Zhu, Michael Zeng |  |
| 263 |  |  [Towards Identifying Social Bias in Dialog Systems: Framework, Dataset, and Benchmark](https://doi.org/10.18653/v1/2022.findings-emnlp.262) |  | 0 | Among all the safety concerns that hinder the deployment of open-domain dialog systems (e.g., offensive languages, biases, and toxic behaviors), social bias presents an insidious challenge. Addressing this challenge requires rigorous analyses and normative reasoning. In this paper, we focus our investigation on social bias measurement to facilitate the development of unbiased dialog systems. We first propose a novel Dial-Bias Framework for analyzing the social bias in conversations using a holistic method beyond bias lexicons or dichotomous annotations. Leveraging the proposed framework, we further introduce the CDial-Bias Dataset which is, to the best of our knowledge, the first annotated Chinese social bias dialog dataset. We also establish a fine-grained dialog bias measurement benchmark and conduct in-depth ablation studies to shed light on the utility of the detailed annotations in the proposed dataset. Finally, we evaluate representative Chinese generative models with our classifiers to unveil the presence of social bias in these systems. | Jingyan Zhou, Jiawen Deng, Fei Mi, Yitong Li, Yasheng Wang, Minlie Huang, Xin Jiang, Qun Liu, Helen Meng |  |
| 264 |  |  [CrossRE: A Cross-Domain Dataset for Relation Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.263) |  | 0 | Relation Extraction (RE) has attracted increasing attention, but current RE evaluation is limited to in-domain evaluation setups. Little is known on how well a RE system fares in challenging, but realistic out-of-distribution evaluation setups. To address this gap, we propose CrossRE, a new, freely-available cross-domain benchmark for RE, which comprises six distinct text domains and includes multi-label annotations. An additional innovation is that we release meta-data collected during annotation, to include explanations and flags of difficult instances. We provide an empirical evaluation with a state-of-the-art model for relation classification. As the meta-data enables us to shed new light on the state-of-the-art model, we provide a comprehensive analysis on the impact of difficult cases and find correlations between model and human annotations. Overall, our empirical investigation highlights the difficulty of cross-domain RE. We release our dataset, to spur more research in this direction. | Elisa Bassignana, Barbara Plank |  |
| 265 |  |  [Probing Structural Knowledge from Pre-trained Language Model for Argumentation Relation Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.264) |  | 0 | Extracting fine-grained structural information between argumentation component (AC) pairs is essential for argumentation relation classification (ARC). However, most previous studies attempt to model the relationship between AC pairs using AC level similarity or semantically relevant features. They ignore the complex interaction between AC pairs and cannot effectively reason the argumentation relation deeply.Therefore, in this paper, we propose a novel dual prior graph neural network (DPGNN) to jointly explore the probing knowledge derived from pre-trained language models (PLMs) and the syntactical information for comprehensively modeling the relationship between AC pairs. Specifically, we construct a probing graph by using probing knowledge derived from PLMs to recognize and align the relational information within and across the argumentation components. In addition, we propose a mutual dependency graph for the AC pair to reason the fine-grained syntactic structural information, in which the syntactical correlation between words is set by the dependency information within AC and mutual attention mechanism across ACs. The knowledge learned from the probing graph and the dependency graph are combined to comprehensively capture the aligned relationships of AC pairs for improving the results of ARC. Experimental results on three public datasets show that DPGNN outperforms the state-of-the-art baselines by a noticeable margin. | Yang Sun, Bin Liang, Jianzhu Bao, Min Yang, Ruifeng Xu |  |
| 266 |  |  [LogicNMR: Probing the Non-monotonic Reasoning Ability of Pre-trained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.265) |  | 0 | The logical reasoning capabilities of pre-trained language models have recently received much attention. As one of the vital reasoning paradigms, non-monotonic reasoning refers to the fact that conclusions may be invalidated with new information. Existing work has constructed a non-monotonic inference dataset 𝛿-NLI and explored the performance of language models on it. However, the 𝛿-NLI dataset is entangled with commonsense reasoning. In this paper, we explore the pure non-monotonic reasoning ability of pre-trained language models. We build a non-monotonic reasoning benchmark, named LogicNMR, with explicit default rules and iterative updates. In the experimental part, the performance of popular language models on LogicNMR is explored from the perspectives of accuracy, generalization, proof-based traceability and robustness. The experimental results show that even though the fine-tuned language models achieve an accuracy of more than 94.4% on LogicNMR, they perform unsatisfactorily, with a significant drop, in generalization and proof-based traceability. | Yeliang Xiu, Zhanhao Xiao, Yongmei Liu |  |
| 267 |  |  [Cheater's Bowl: Human vs. Computer Search Strategies for Open-Domain QA](https://doi.org/10.18653/v1/2022.findings-emnlp.266) |  | 0 | For humans and computers, the first step in answering an open-domain question is retrieving a set of relevant documents from a large corpus. However, the strategies that computers use fundamentally differ from those of humans. To better understand these differences, we design a gamified interface for data collection—Cheater’s Bowl—where a human answers complex questions with access to both traditional and modern search tools. We collect a dataset of human search sessions, analyze human search strategies, and compare them to state-of-the-art multi-hop QA models. Humans query logically, apply dynamic search chains, and use world knowledge to boost searching. We demonstrate how human queries can improve the accuracy of existing systems and propose improving the future design of QA models. | Wanrong He, Andrew Mao, Jordan L. BoydGraber |  |
| 268 |  |  [FRSUM: Towards Faithful Abstractive Summarization via Enhancing Factual Robustness](https://doi.org/10.18653/v1/2022.findings-emnlp.267) |  | 0 | Despite being able to generate fluent and grammatical text, current Seq2Seq summarization models still suffering from the unfaithful generation problem.In this paper, we study the faithfulness of existing systems from a new perspective of factual robustness which is the ability to correctly generate factual information over adversarial unfaithful information.We first measure a model’sfactual robustness by its success rate to defend against adversarial attacks when generating factual information.The factual robustness analysis on a wide range of current systems shows its good consistency with human judgments on faithfulness.Inspired by these findings, we propose to improve the faithfulness of a model by enhancing its factual robustness.Specifically, we propose a novel training strategy, namely FRSUM, which teaches the model to defend against both explicit adversarial samples and implicit factual adversarial perturbations.Extensive automatic and human evaluation results show that FRSUM consistently improves the faithfulness of various Seq2Seq models, such as T5, BART. | Wenhao Wu, Wei Li, Jiachen Liu, Xinyan Xiao, Ziqiang Cao, Sujian Li, Hua Wu |  |
| 269 |  |  [PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.268) |  | 0 | Formal verse poetry imposes strict constraints on the meter and rhyme scheme of poems. Most prior work on generating this type of poetry uses existing poems for supervision, which are difficult to obtain for most languages and poetic forms. In this work, we propose an unsupervised approach to generate poems that follow any given meter and rhyme scheme, without requiring any poetic text for training. Our method works by splitting a regular, non-poetic corpus into phrases, prepending control codes that describe the length and end rhyme of each phrase, and training a transformer language model in the augmented corpus. The transformer learns to link the structure descriptor with the control codes to the number of lines, their length and their end rhyme. During inference, we build control codes for the desired meter and rhyme scheme, and condition our language model on them to generate formal verse poetry. Experiments in Spanish and Basque show that our approach is able to generate valid poems, which are often comparable in quality to those written by humans. | Aitor Ormazabal, Mikel Artetxe, Manex Agirrezabal, Aitor Soroa, Eneko Agirre |  |
| 270 |  |  [ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback](https://doi.org/10.18653/v1/2022.findings-emnlp.269) |  | 0 | Recently, dataset-generation-based zero-shot learning has shown promising results by training a task-specific model with a dataset synthesized from large pre-trained language models (PLMs). The final task-specific model often achieves compatible or even better performance than PLMs under the zero-shot setting, with orders of magnitude fewer parameters.However, synthetic datasets have their drawbacks. They have long being suffering from the low-quality issue (e.g., low informativeness, redundancy). This explains why the massive synthetic data does not lead to better performance – a scenario we would expect in the human-labeled data. To improve the quality in dataset synthesis, we propose a progressive zero-shot dataset generation framework, ProGen, which leverages the feedback from the task-specific model to guide the generation of new training data via in-context examples.Extensive experiments on five text classification datasets demonstrate the effectiveness of the proposed approach. We also show ProGen achieves on-par or superior performance with only 1% synthetic dataset size, when comparing to baseline methods without in-context feedback. | Jiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong |  |
| 271 |  |  [Constructing Highly Inductive Contexts for Dialogue Safety through Controllable Reverse Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.270) |  | 0 | Large pretrained language models can easily produce toxic or biased content, which is prohibitive for practical use. In order to detect such toxic generations, existing methods rely on templates, real-world data extraction, crowdsourcing workers or automatic generation to construct adversarial contexts that are likely to induce toxic generations. However, what type of context is more likely to induce unsafe responses is still under-explored. In this paper, we identify that context toxicity and context category (e.g., profanity, insult, drugs, etc.) are two important factors to cause safety issues in response generation. Hence, we propose a method called reverse generation to construct adversarial contexts conditioned on a given response, with the flexibility to control category, toxicity level and inductivity of the generated contexts. Via reverse generation, we augment the existing BAD dataset and construct a new dataset BAD+ which contains more than 120K diverse and highly inductive contexts in 12 categories. We test three popular pretrained dialogue models (Blender, DialoGPT and Plato2) and find that BAD+ can largely expose their safety problems. Furthermore, we show that BAD+ can greatly enhance the safety of generation, and we reveal the key factors of safety improvement. Our code and dataset is available at https://github.com/thu-coai/Reverse_Generation. | Zhexin Zhang, Jiale Cheng, Hao Sun, Jiawen Deng, Fei Mi, Yasheng Wang, Lifeng Shang, Minlie Huang |  |
| 272 |  |  [Language Prior Is Not the Only Shortcut: A Benchmark for Shortcut Learning in VQA](https://doi.org/10.18653/v1/2022.findings-emnlp.271) |  | 0 | Visual Question Answering (VQA) models are prone to learn the shortcut solution formed by dataset biases rather than the intended solution. To evaluate the VQA models’ reasoning ability beyond shortcut learning, the VQA-CP v2 dataset introduces a distribution shift between the training and test set given a question type. In this way, the model cannot use the training set shortcut (from question type to answer) to perform well on the test set. However, VQA-CP v2 only considers one type of shortcut and thus still cannot guarantee that the model relies on the intended solution rather than a solution specific to this shortcut. To overcome this limitation, we propose a new dataset that considers varying types of shortcuts by constructing different distribution shifts in multiple OOD test sets. In addition, we overcome the three troubling practices in the use of VQA-CP v2, e.g., selecting models using OOD test sets, and further standardize OOD evaluation procedure. Our benchmark provides a more rigorous and comprehensive testbed for shortcut learning in VQA. We benchmark recent methods and find that methods specifically designed for particular shortcuts fail to simultaneously generalize to our varying OOD test sets. We also systematically study the varying shortcuts and provide several valuable findings, which may promote the exploration of shortcut learning in VQA. | Qingyi Si, Fandong Meng, Mingyu Zheng, Zheng Lin, Yuanxin Liu, Peng Fu, Yanan Cao, Weiping Wang, Jie Zhou |  |
| 273 |  |  [Bridging the Training-Inference Gap for Dense Phrase Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.272) |  | 0 | Building dense retrievers requires a series of standard procedures, including training and validating neural models and creating indexes for efficient search. However, these procedures are often misaligned in that training objectives do not exactly reflect the retrieval scenario at inference time. In this paper, we explore how the gap between training and inference in dense retrieval can be reduced, focusing on dense phrase retrieval (Lee et al., 2021) where billions of representations are indexed at inference. Since validating every dense retriever with a large-scale index is practically infeasible, we propose an efficient way of validating dense retrievers using a small subset of the entire corpus. This allows us to validate various training strategies including unifying contrastive loss terms and using hard negatives for phrase retrieval, which largely reduces the training-inference discrepancy. As a result, we improve top-1 phrase retrieval accuracy by 2 3 points and top-20 passage retrieval accuracy by 2 4 points for open-domain question answering. Our work urges modeling dense retrievers with careful consideration of training and inference via efficient validation while advancing phrase retrieval as a general solution for dense retrieval. | Gyuwan Kim, Jinhyuk Lee, Barlas Oguz, Wenhan Xiong, Yizhe Zhang, Yashar Mehdad, William Yang Wang |  |
| 274 |  |  [Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources](https://doi.org/10.18653/v1/2022.findings-emnlp.273) |  | 0 | While the NLP community is generally aware of resource disparities among languages, we lack research that quantifies the extent and types of such disparity. Prior surveys estimating the availability of resources based on the number of datasets can be misleading as dataset quality varies: many datasets are automatically induced or translated from English data. To provide a more comprehensive picture of language resources, we examine the characteristics of 156 publicly available NLP datasets. We manually annotate how they are created, including input text and label sources and tools used to build them, and what they study, tasks they address and motivations for their creation. After quantifying the qualitative NLP resource gap across languages, we discuss how to improve data collection in low-resource languages. We survey language-proficient NLP researchers and crowd workers per language, finding that their estimated availability correlates with dataset availability. Through crowdsourcing experiments, we identify strategies for collecting high-quality multilingual data on the Mechanical Turk platform. We conclude by making macro and micro-level suggestions to the NLP community and individual researchers for future multilingual data development. | Xinyan Yu, Trina Chatterjee, Akari Asai, Junjie Hu, Eunsol Choi |  |
| 275 |  |  [ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.274) |  | 0 | Recent years have witnessed the rise and success of pre-training techniques in visually-rich document understanding. However, most existing methods lack the systematic mining and utilization of layout-centered knowledge, leading to sub-optimal performances. In this paper, we propose ERNIE-Layout, a novel document pre-training solution with layout knowledge enhancement in the whole workflow, to learn better representations that combine the features from text, layout, and image. Specifically, we first rearrange input sequences in the serialization stage, and then present a correlative pre-training task, reading order prediction, to learn the proper reading order of documents. To improve the layout awareness of the model, we integrate a spatial-aware disentangled attention into the multi-modal transformer and a replaced regions prediction task into the pre-training phase. Experimental results show that ERNIE-Layout achieves superior performance on various downstream tasks, setting new state-of-the-art on key information extraction, document image classification, and document question answering datasets. The code and models are publicly available at PaddleNLP. | Qiming Peng, Yinxu Pan, Wenjin Wang, Bin Luo, Zhenyu Zhang, Zhengjie Huang, Yuhui Cao, Weichong Yin, Yongfeng Chen, Yin Zhang, Shikun Feng, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang |  |
| 276 |  |  [Do Charge Prediction Models Learn Legal Theory?](https://doi.org/10.18653/v1/2022.findings-emnlp.275) |  | 0 | The charge prediction task aims to predict the charge for a case given its fact description. Recent models have already achieved impressive accuracy in this task, however, little is understood about the mechanisms they use to perform the judgment.For practical applications, a charge prediction model should conform to the certain legal theory in civil law countries, as under the framework of civil law, all cases are judged according to certain local legal theories. In China, for example, nearly all criminal judges make decisions based on the Four Elements Theory (FET).In this paper, we argue that trustworthy charge prediction models should take legal theories into consideration, and standing on prior studies in model interpretation, we propose three principles for trustworthy models should follow in this task, which are sensitive, selective, and presumption of innocence.We further design a new framework to evaluate whether existing charge prediction models learn legal theories. Our findings indicate that, while existing charge prediction models meet the selective principle on a benchmark dataset, most of them are still not sensitive enough and do not satisfy the presumption of innocence. Our code and dataset are released at https://github.com/ZhenweiAn/EXP_LJP. | Zhenwei An, Quzhe Huang, Cong Jiang, Yansong Feng, Dongyan Zhao |  |
| 277 |  |  [Keep Me Updated! Memory Management in Long-term Conversations](https://doi.org/10.18653/v1/2022.findings-emnlp.276) |  | 0 | Remembering important information from the past and continuing to talk about it in the present are crucial in long-term conversations. However, previous literature does not deal with cases where the memorized information is outdated, which may cause confusion in later conversations. To address this issue, we present a novel task and a corresponding dataset of memory management in long-term conversations, in which bots keep track of and bring up the latest information about users while conversing through multiple sessions. In order to support more precise and interpretable memory, we represent memory as unstructured text descriptions of key information and propose a new mechanism of memory management that selectively eliminates invalidated or redundant information. Experimental results show that our approach outperforms the baselines that leave the stored memory unchanged in terms of engagingness and humanness, with larger performance gap especially in the later sessions. | Sanghwan Bae, DongHyun Kwak, Soyoung Kang, Min Young Lee, Sungdong Kim, Yuin Jeong, Hyeri Kim, SangWoo Lee, WooMyoung Park, Nako Sung |  |
| 278 |  |  [A Unified Dialogue User Simulator for Few-shot Data Augmentation](https://doi.org/10.18653/v1/2022.findings-emnlp.277) |  | 0 | Pre-trained language models have shown superior performance in task-oriented dialogues. However, existing datasets are on limited scales, which cannot support large-scale pre-training. Fortunately, various data augmentation methods have been developed to augment large-scale task-oriented dialogue corpora. However, they heavily rely on annotated data in the target domain, which require a tremendous amount of data collection and human labeling work. In this paper, we build a unified dialogue user simulation model by pre-training on several publicly available datasets. The model can then be tuned on a target domain with few-shot data. The experiments on a target dataset across multiple domains show that our proposed model brings remarkable performance increases through data augmentation. | Dazhen Wan, Zheng Zhang, Qi Zhu, Lizi Liao, Minlie Huang |  |
| 279 |  |  [An Error-Guided Correction Model for Chinese Spelling Error Correction](https://doi.org/10.18653/v1/2022.findings-emnlp.278) |  | 0 | Although existing neural network approaches have achieved great progress on Chinese spelling correction, there is still room to improve. The model is required to avoid over-correction and to distinguish a correct token from its phonological and visual similar ones. In this paper, we propose an error-guided correction model to address these issues. By borrowing the powerful ability of the pre-trained BERT model, we propose a novel zero-shot error detection method to do a preliminary detection, which guides our model to attend more on the probably wrong tokens in encoding and to avoid modifying the correct tokens in generating. Furthermore, we introduce a new loss function to integrate the error confusion set, which enables our model to distinguish similar tokens. Moreover, our model supports highly parallel decoding to meet real applications. Experiments are conducted on widely used benchmarks. Our model achieves superior performance against state-of-the-art approaches by a remarkable margin, on both the quality and computation speed. | Rui Sun, Xiuyu Wu, Yunfang Wu |  |
| 280 |  |  [Describing Sets of Images with Textual-PCA](https://doi.org/10.18653/v1/2022.findings-emnlp.279) |  | 0 | We seek to semantically describe a set of images, capturing both the attributes of single images and the variations within the set. Our procedure is analogous to Principle Component Analysis, in which the role of projection vectors is replaced with generated phrases. First, a centroid phrase that has the largest average semantic similarity to the images in the set is generated, where both the computation of the similarity and the generation are based on pretrained vision-language models. Then, the phrase that generates the highest variation among the similarity scores is generated, using the same models. The next phrase maximizes the variance subject to being orthogonal, in the latent space, to the highest-variance phrase, and the process continues. Our experiments show that our method is able to convincingly capture the essence of image sets and describe the individual elements in a semantically meaningful way within the context of the entire set. Our code is available at: https://github.com/OdedH/textual-pca. | Oded Hupert, Idan Schwartz, Lior Wolf |  |
| 281 |  |  [Learning to Model Editing Processes](https://doi.org/10.18653/v1/2022.findings-emnlp.280) |  | 0 | Most existing sequence generation models produce outputs in one pass, usually left-to-right. However, this is in contrast with a more natural approach that humans use in generating content; iterative refinement and editing. Recent work has introduced edit-based models for various tasks (such as neural machine translation and text style transfer), but these generally model a single edit step. In this work, we propose modeling editing processes, modeling the whole process of iteratively generating sequences. We form a conceptual framework to describe the likelihood of multi-step edits, and describe neural models that can learn a generative model of sequences based on these multistep edits. We introduce baseline results and metrics on this task, finding that modeling editing processes improves performance on a variety of axes on both our proposed task and related downstream tasks compared to previous single-step models of edits. | Machel Reid, Graham Neubig |  |
| 282 |  |  [PALT: Parameter-Lite Transfer of Language Models for Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.findings-emnlp.281) |  | 0 | This paper presents a parameter-lite transfer learning approach of pretrained language models (LM) for knowledge graph (KG) completion. Instead of finetuning, which modifies all LM parameters, we only tune a few new parameters while keeping the original LM parameters fixed. We establish this via reformulating KG completion as a “fill-in-the-blank” task, and introducing a parameter-lite encoder on top of the original LMs. We show that, by tuning far fewer parameters than finetuning, LMs transfer non-trivially to most tasks and reach competitiveness with prior state-of-the-art approaches. For instance, we outperform the fully finetuning approaches on a KG completion benchmark by tuning only 1% of the parameters. | Jianhao Shen, Chenguang Wang, Ye Yuan, Jiawei Han, Heng Ji, Koushik Sen, Ming Zhang, Dawn Song |  |
| 283 |  |  [Prompt-based Connective Prediction Method for Fine-grained Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.282) |  | 0 | Due to the absence of connectives, implicit discourse relation recognition (IDRR) is still a challenging and crucial task in discourse analysis. Most of the current work adopted multitask learning to aid IDRR through explicit discourse relation recognition (EDRR) or utilized dependencies between discourse relation labels to constrain model predictions. But these methods still performed poorly on fine-grained IDRR and even utterly misidentified on most of the few-shot discourse relation classes. To address these problems, we propose a novel Prompt-based Connective Prediction (PCP) method for IDRR. Our method instructs large-scale pre-trained models to use knowledge relevant to discourse relation and utilizes the strong correlation between connectives and discourse relation to help the model recognize implicit discourse relations. Experimental results show that our method surpasses the current state-of-the-art model and achieves significant improvements on those fine-grained few-shot discourse relation. Moreover, our approach is able to be transferred to EDRR and obtain acceptable results. Our code is released in https://github.com/zh-i9/PCP-for-IDRR. | Hao Zhou, Man Lan, Yuanbin Wu, Yuefeng Chen, Meirong Ma |  |
| 284 |  |  [On Utilizing Constituent Language Resources to Improve Downstream Tasks in Hinglish](https://doi.org/10.18653/v1/2022.findings-emnlp.283) |  | 0 | Performance of downstream NLP tasks on code-switched Hindi-English (aka ) continues to remain a significant challenge. Intuitively, Hindi and English corpora should aid improve task performance on Hinglish. We show that meta-learning framework can effectively utilize the the labelled resources of the downstream tasks in the constituent languages. The proposed approach improves the performance on downstream tasks on code-switched language. We experiment with code-switching benchmark GLUECoS and report significant improvements. | Vishwajeet Kumar, Rudra Murthy, Tejas I. Dhamecha |  |
| 285 |  |  [SYGMA: A System for Generalizable and Modular Question Answering Over Knowledge Bases](https://doi.org/10.18653/v1/2022.findings-emnlp.284) |  | 0 | Knowledge Base Question Answering (KBQA) involving complex reasoning is emerging as an important research direction. However, most KBQA systems struggle with generalizability, particularly on two dimensions: (a) across multiple knowledge bases, where existing KBQA approaches are typically tuned to a single knowledge base, and (b) across multiple reasoning types, where majority of datasets and systems have primarily focused on multi-hop reasoning. In this paper, we present SYGMA, a modular KBQA approach developed with goal of generalization across multiple knowledge bases and multiple reasoning types. To facilitate this, SYGMA is designed as two high level modules: 1) KB-agnostic question understanding module that remain common across KBs, and generates logic representation of the question with high level reasoning constructs that are extensible, and 2) KB-specific question mapping and answering module to address the KB-specific aspects of the answer extraction. We evaluated SYGMA on multiple datasets belonging to distinct knowledge bases (DBpedia and Wikidata) and distinct reasoning types (multi-hop and temporal). State-of-the-art or competitive performances achieved on those datasets demonstrate its generalization capability. | Sumit Neelam, Udit Sharma, Hima Karanam, Shajith Ikbal, Pavan Kapanipathi, Ibrahim Abdelaziz, Nandana Mihindukulasooriya, YoungSuk Lee, Santosh K. Srivastava, Cezar Pendus, Saswati Dana, Dinesh Garg, Achille Fokoue, G. P. Shrivatsa Bhargav, Dinesh Khandelwal, Srinivas Ravishankar, Sairam Gurajada, Maria Chang, Rosario UcedaSosa, Salim Roukos, Alexander Gray, Guilherme Lima, Ryan Riegel, Francois P. S. Luus, L. Venkata Subramaniam |  |
| 286 |  |  [Instance-Guided Prompt Learning for Few-Shot Text Matching](https://doi.org/10.18653/v1/2022.findings-emnlp.285) |  | 0 | Few-shot text matching is a more practical technique in natural language processing (NLP) to determine whether two texts are semantically identical. They primarily design patterns to reformulate text matching into a pre-trained task with uniform prompts across all instances. But they fail to take into account the connection between prompts and instances. This paper argues that dynamically strengthening the correlation between particular instances and the prompts is necessary because fixed prompts cannot adequately fit all diverse instances in inference. We suggest IGATE: Instance-Guided prompt leArning for few-shoT tExt matching, a novel pluggable prompt learning method. The gate mechanism used by IGATE, which is between the embedding and the PLM encoders, makes use of the semantics of instances to regulate the effects of the gate on the prompt tokens. The experimental findings show that IGATE achieves SOTA performance on MRPC and QQP, outperforming strong baselines. GitHub will host the release of codes. | Jia Du, Xuanyu Zhang, Siyi Wang, Kai Wang, Yanquan Zhou, Lei Li, Qing Yang, Dongliang Xu |  |
| 287 |  |  [M3: Multi-level dataset for Multi-document summarisation of Medical studies](https://doi.org/10.18653/v1/2022.findings-emnlp.286) |  | 0 | We present M3 (Multi-level dataset for Multi-document summarisation of Medical studies), a benchmark dataset for evaluating the quality of summarisation systems in the biomedical domain. The dataset contains sets of multiple input documents and target summaries of three levels of complexity: documents, sentences, and propositions. The dataset also includes several levels of annotation, including biomedical entities, direction, and strength of relations between them, and the discourse relationships between the input documents (“contradiction” or “agreement”). We showcase usage scenarios of the dataset by testing 10 generic and domain-specific summarisation models in a zero-shot setting, and introduce a probing task based on counterfactuals to test if models are aware of the direction and strength of the conclusions generated from input studies. | Yulia Otmakhova, Karin Verspoor, Timothy Baldwin, Antonio JimenoYepes, Jey Han Lau |  |
| 288 |  |  [Adapters for Enhanced Modeling of Multilingual Knowledge and Text](https://doi.org/10.18653/v1/2022.findings-emnlp.287) |  | 0 | Large language models appear to learn facts from the large text corpora they are trained on. Such facts are encoded implicitly within their many parameters, making it difficult to verify or manipulate what knowledge has been learned. Language models have recently been extended to multilingual language models (MLLMs), enabling knowledge to be learned across hundreds of languages. Meanwhile, knowledge graphs contain facts in an explicit triple format, which require careful and costly curation and are only available in a few high-resource languages, restricting their research and application. To address these issues, we propose to enhance MLLMs with knowledge from multilingual knowledge graphs (MLKGs) so as to tackle language and knowledge graph tasks across many languages, including low-resource ones. Specifically, we introducea lightweight adapter set to enhance MLLMs with cross-lingual entity alignment and facts from MLKGs for many languages. Experiments on common benchmarks show that such enhancement benefits both MLLMs and MLKGs, achieving: (1) comparable or improved performance for knowledge graph completion and entity alignment relative to baselines, especially for low-resource languages (for which knowledge graphs are unavailable); and (2) improved MLLM performance on language understanding tasks that require multilingual factual knowledge; all while maintaining performance on other general language tasks. | Yifan Hou, Wenxiang Jiao, Meizhen Liu, Carl Allen, Zhaopeng Tu, Mrinmaya Sachan |  |
| 289 |  |  [SepLL: Separating Latent Class Labels from Weak Supervision Noise](https://doi.org/10.18653/v1/2022.findings-emnlp.288) |  | 0 | In the weakly supervised learning paradigm, labeling functions automatically assign heuristic, often noisy, labels to data samples. In this work, we provide a method for learning from weak labels by separating two types of complementary information associated with the labeling functions: information related to the target label and information specific to one labeling function only. Both types of information are reflected to different degrees by all labeled instances. In contrast to previous works that aimed at correcting or removing wrongly labeled instances, we learn a branched deep model that uses all data as-is, but splits the labeling function information in the latent space. Specifically, we propose the end-to-end model SepLL which extends a transformer classifier by introducing a latent space for labeling function specific and task-specific information. The learning signal is only given by the labeling functions matches, no pre-processing or label model is required for our method. Notably, the task prediction is made from the latent layer without any direct task signal. Experiments on Wrench text classification tasks show that our model is competitive with the state-of-the-art, and yields a new best average performance. | Andreas Stephan, Vasiliki Kougia, Benjamin Roth |  |
| 290 |  |  [Probing Relational Knowledge in Language Models via Word Analogies](https://doi.org/10.18653/v1/2022.findings-emnlp.289) |  | 0 | Understanding relational knowledge plays an integral part in natural language comprehension. When it comes to pre-trained language models (PLM), prior work has been focusing on probing relational knowledge this by filling the blanks in pre-defined prompts such as “The capital of France is —". However, these probes may be affected by the co-occurrence of target relation words and entities (e.g. “capital”, “France” and “Paris”) in the pre-training corpus. In this work, we extend these probing methodologies leveraging analogical proportions as a proxy to probe relational knowledge in transformer-based PLMs without directly presenting the desired relation. In particular, we analysed the ability of PLMs to understand (1) the directionality of a given relation (e.g. Paris-France is not the same as France-Paris); (2) the ability to distinguish types on a given relation (both France and Japan are countries); and (3) the relation itself (Paris is the capital of France, but not Rome). Our results show how PLMs are extremely accurate at (1) and (2), but have clear room for improvement for (3). To better understand the reasons behind this behaviour and mistakes made by PLMs, we provide an extended quantitative analysis based on relevant factors such as frequency. | Kiamehr Rezaee, José CamachoCollados |  |
| 291 |  |  [Semi-Supervised Lifelong Language Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.290) |  | 0 | Lifelong learning aims to accumulate knowledge and alleviate catastrophic forgetting when learning tasks sequentially. However, existing lifelong language learning methods only focus on the supervised learning setting. Unlabeled data, which can be easily accessed in real-world scenarios, are underexplored. In this paper, we explore a novel setting, semi-supervised lifelong language learning (SSLL), where a model learns sequentially arriving language tasks with both labeled and unlabeled data. We propose an unlabeled data enhanced lifelong learner to explore SSLL. Specially, we dedicate task-specific modules to alleviate catastrophic forgetting and design two modules to exploit unlabeled data: (1) a virtual supervision enhanced task solver is constructed on a teacher-student framework to mine the underlying knowledge from unlabeled data; and (2) a backward augmented learner is built to encourage knowledge transfer from newly arrived unlabeled data to previous tasks. Experimental results on various language tasks demonstrate our model’s effectiveness and superiority over competitive baselines under the new setting SSLL. | Yingxiu Zhao, Yinhe Zheng, Bowen Yu, Zhiliang Tian, Dongkyu Lee, Jian Sun, Yongbin Li, Nevin L. Zhang |  |
| 292 |  |  [Parameter-free Automatically Prompting: A Latent Pseudo Label Mapping Model for Prompt-based Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.291) |  | 0 | Prompt-based learning has achieved excellent performance in few-shot learning by mapping the outputs of the pre-trained language model to the labels with the help of a label mapping component. Existing manual label mapping (MLM) methods achieve good results but heavily rely on expensive human knowledge. Automatic label mapping (ALM) methods that learn the mapping functions with extra parameters have shown their potentiality. However, no effective ALM model comparable to MLM methods is developed yet due to the limited data. In this paper, we propose a Latent Pseudo Label Mapping (LPLM) method that optimizes the label mapping without human knowledge and extra parameters. LPLM is built upon a probabilistic latent model and is iteratively self-improved with the EM-style algorithm. The empirical results demonstrate that our LPLM method is superior to the mainstream ALM methods and significantly outperforms the SOTA method in few-shot classification tasks. Moreover, LPLM also shows impressively better performance than the vanilla MLM method which requires extra task-specific prior knowledge. | Jirui Qi, Richong Zhang, Junfan Chen, Jaein Kim, Yongyi Mao |  |
| 293 |  |  [Exploring Logographic Image for Chinese Aspect-based Sentiment Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.292) |  | 0 | In logographic languages like Chinese, word meanings are constructed using specific character formations, which can help to disambiguate word senses and are beneficial for sentiment classification. However, such knowledge is rarely explored in previous sentiment analysis methods. In this paper, we focus on exploring the logographic information for aspect-based sentiment classification in Chinese text. Specifically, we employ a logographic image to capture an internal morphological structure from the character sequence. The logographic image is also used to learn the external relations among context and aspect words. Furthermore, we propose a multimodal language model to explicitly incorporate a logographic image with review text for aspect-based sentiment classification in Chinese. Experimental results show that our method brings substantial performance improvement over strong baselines. The results also indicate that the logographic image is very important for exploring the internal structure and external relations from the character sequence. | Xiabing Zhou, Renjie Feng, Xiaotong Jiang, Zhongqing Wang |  |
| 294 |  |  [On the Role of Bidirectionality in Language Model Pre-Training](https://doi.org/10.18653/v1/2022.findings-emnlp.293) |  | 0 | Prior work on language model pre-training has explored different architectures and learning objectives, but differences in data, hyperparameters and evaluation make a principled comparison difficult. In this work, we focus on bidirectionality as a key factor that differentiates existing approaches, and present a comprehensive study of its role in next token prediction, text infilling, zero-shot priming and fine-tuning. We propose a new framework that generalizes prior approaches, including fully unidirectional models like GPT, fully bidirectional models like BERT, and hybrid models like CM3 and prefix LM. Our framework distinguishes between two notions of bidirectionality (bidirectional context and bidirectional attention) and allows us to control each of them separately. We find that the optimal configuration is largely application-dependent (e.g., bidirectional attention is beneficial for fine-tuning and infilling, but harmful for next token prediction and zero-shot priming). We train models with up to 6.7B parameters, and find differences to remain consistent at scale. While prior work on scaling has focused on left-to-right autoregressive models, our results suggest that this approach comes with some trade-offs, and it might be worthwhile to develop very large bidirectional models. | Mikel Artetxe, Jingfei Du, Naman Goyal, Luke Zettlemoyer, Veselin Stoyanov |  |
| 295 |  |  [You Are What You Talk About: Inducing Evaluative Topics for Personality Analysis](https://doi.org/10.18653/v1/2022.findings-emnlp.294) |  | 0 | Expressing attitude or stance toward entities and concepts is an integral part of human behavior and personality. Recently, evaluative language data has become more accessible with social media’s rapid growth, enabling large-scale opinion analysis. However, surprisingly little research examines the relationship between personality and evaluative language. To bridge this gap, we introduce the notion of evaluative topics, obtained by applying topic models to pre-filtered evaluative text from social media. We then link evaluative topics to individual text authors to build their evaluative profiles. We apply evaluative profiling to Reddit comments labeled with personality scores and conduct an exploratory study on the relationship between evaluative topics and Big Five personality facets, aiming for a more interpretable, facet-level analysis. Finally, we validate our approach by observing correlations consistent with prior research in personality psychology. | Josip Jukic, Iva Vukojevic, Jan Snajder |  |
| 296 |  |  [CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure](https://doi.org/10.18653/v1/2022.findings-emnlp.295) |  | 0 | Code pre-trained models (CodePTMs) have recently demonstrated significant success in code intelligence. To interpret these models, some probing methods have been applied. However, these methods fail to consider the inherent characteristics of codes. In this paper, to address the problem, we propose a novel probing method CAT-probing to quantitatively interpret how CodePTMs attend code structure. We first denoise the input code sequences based on the token types pre-defined by the compilers to filter those tokens whose attention scores are too small. After that, we define a new metric CAT-score to measure the commonality between the token-level attention scores generated in CodePTMs and the pair-wise distances between corresponding AST nodes. The higher the CAT-score, the stronger the ability of CodePTMs to capture code structure. We conduct extensive experiments to integrate CAT-probing with representative CodePTMs for different programming languages. Experimental results show the effectiveness of CAT-probing in CodePTM interpretation. Our codes and data are publicly available at https://github.com/nchen909/CodeAttention. | Nuo Chen, Qiushi Sun, Renyu Zhu, Xiang Li, Xuesong Lu, Ming Gao |  |
| 297 |  |  [Learning to Revise References for Faithful Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.296) |  | 0 | In real-world scenarios with naturally occurring datasets, reference summaries are noisy and may contain information that cannot be inferred from the source text. On large news corpora, removing low quality samples has been shown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora, filtering is detrimental to performance. To improve reference quality while retaining all data, we propose a new approach: to selectively re-write unsupported reference sentences to better reflect source data. We automatically generate a synthetic dataset of positive and negative revisions by corrupting supported sentences and learn to revise reference sentences with contrastive learning. The intensity of revisions is treated as a controllable attribute so that, at inference, diverse candidates can be over-generated-then-rescored to balance faithfulness and abstraction. To test our methods, we extract noisy references from publicly available MIMIC-III discharge summaries for the task of hospital-course summarization, and vary the data on which models are trained. According to metrics and human evaluation, models trained on revised clinical references are much more faithful, informative, and fluent than models trained on original or filtered data. | Griffin Adams, HanChin Shing, Qing Sun, Christopher Winestock, Kathleen R. McKeown, Noémie Elhadad |  |
| 298 |  |  [Towards Intention Understanding in Suicidal Risk Assessment with Natural Language Processing](https://doi.org/10.18653/v1/2022.findings-emnlp.297) |  | 0 | Recent applications of natural language processing techniques to suicidal ideation detection and risk assessment frame the detection or assessment task as a text classification problem. Recent advances have developed many models, especially deep learning models, to boost predictive performance.Though the performance (in terms of aggregated evaluation scores) is improving, this position paper urges that better intention understanding is required for reliable suicidal risk assessment with computational methods. This paper reflects the state of natural language processing applied to suicide-associated text classification tasks, differentiates suicidal risk assessment and intention understanding, and points out potential limitations of sentiment features and pretrained language models in suicidal intention understanding.Besides, it urges the necessity for sequential intention understanding and risk assessment, discusses some critical issues in evaluation such as uncertainty, and studies the lack of benchmarks. | Shaoxiong Ji |  |
| 299 |  |  [On the Impact of Temporal Concept Drift on Model Explanations](https://doi.org/10.18653/v1/2022.findings-emnlp.298) |  | 0 | Explanation faithfulness of model predictions in natural language processing is typically evaluated on held-out data from the same temporal distribution as the training data (i.e. synchronous settings). While model performance often deteriorates due to temporal variation (i.e. temporal concept drift), it is currently unknown how explanation faithfulness is impacted when the time span of the target data is different from the data used to train the model (i.e. asynchronous settings). For this purpose, we examine the impact of temporal variation on model explanations extracted by eight feature attribution methods and three select-then-predict models across six text classification tasks. Our experiments show that (i) faithfulness is not consistent under temporal variations across feature attribution methods (e.g. it decreases or increases depending on the method), with an attention-based method demonstrating the most robust faithfulness scores across datasets; and (ii) select-then-predict models are mostly robust in asynchronous settings with only small degradation in predictive performance. Finally, feature attribution methods show conflicting behavior when used in FRESH (i.e. a select-and-predict model) and for measuring sufficiency/comprehensiveness (i.e. as post-hoc methods), suggesting that we need more robust metrics to evaluate post-hoc explanation faithfulness. Code will be made publicly available. | Zhixue Zhao, George Chrysostomou, Kalina Bontcheva, Nikolaos Aletras |  |
| 300 |  |  [Text-Only Training for Image Captioning using Noise-Injected CLIP](https://doi.org/10.18653/v1/2022.findings-emnlp.299) |  | 0 | We consider the task of image-captioning using only the CLIP model and additional text data at training time and no additional captioned images. Our approach relies on the fact that CLIP is trained to make visual and textual embeddings similar. Therefore, we only need to learn how to translate CLIP textual embeddings back into text, and we can learn how to do this by learning a decoder for the frozen CLIP text encoder using only text. We argue that this intuition is “almost correct” because of a gap between the embedding spaces, and propose to rectify this via noise injection during training. We demonstrate the effectiveness of our approach by showing SOTA zero-shot image captioning across four benchmarks, including style transfer. Code, data, and models are available at https://github.com/DavidHuji/CapDec. | David Nukrai, Ron Mokady, Amir Globerson |  |
| 301 |  |  [Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.300) |  | 0 | Fine-tuning large pretrained language models on a limited training corpus usually suffers from poor generalization. Prior works show that the recently-proposed sharpness-aware minimization (SAM) optimization method can improve the model generalization. However, SAM adds a perturbation to each model parameter equally (but not all parameters contribute equally to the optimization of training), which we argue is sub-optimal and will lead to excessive computation. In this paper, we propose a novel optimization procedure, namely FSAM, which introduces a Fisher mask to improve the efficiency and performance of SAM. In short, instead of adding perturbation to all parameters, FSAM uses the Fisher information to identity the important parameters and formulates a Fisher mask to obtain the sparse perturbation, i.e., making the optimizer focus on these important parameters. Experiments on various tasks in GLUE and SuperGLUE benchmarks show that FSAM consistently outperforms the vanilla SAM by 0.67 1.98 average score among four different pretrained models. We also empirically show that FSAM works well in other complex scenarios, e.g., fine-tuning on generation tasks or limited training data. Encouragingly, when training data is limited, FSAM improves the SAM by a large margin, i.e., up to 15.1. | Qihuang Zhong, Liang Ding, Li Shen, Peng Mi, Juhua Liu, Bo Du, Dacheng Tao |  |
| 302 |  |  [TINA: Textual Inference with Negation Augmentation](https://doi.org/10.18653/v1/2022.findings-emnlp.301) |  | 0 | Transformer-based language models achieve state-of-the-art results on several natural language processing tasks. One of these is textual entailment, i.e., the task of determining whether a premise logically entails a hypothesis. However, the models perform poorly on this task when the examples contain negations. In this paper, we propose a new definition of textual entailment that captures also negation. This allows us to develop TINA (Textual Inference with Negation Augmentation), a principled technique for negated data augmentation that can be combined with the unlikelihood loss function.Our experiments with different transformer-based models show that our method can significantly improve the performance of the models on textual entailment datasets with negation – without sacrificing performance on datasets without negation. | Chadi Helwe, Simon Coumes, Chloé Clavel, Fabian M. Suchanek |  |
| 303 |  |  [Improving Bilingual Lexicon Induction with Cross-Encoder Reranking](https://doi.org/10.18653/v1/2022.findings-emnlp.302) |  | 0 | Bilingual lexicon induction (BLI) with limited bilingual supervision is a crucial yet challenging task in multilingual NLP. Current state-of-the-art BLI methods rely on the induction of cross-lingual word embeddings (CLWEs) to capture cross-lingual word similarities; such CLWEs are obtained <b>1)</b> via traditional static models (e.g., VecMap), or <b>2)</b> by extracting type-level CLWEs from multilingual pretrained language models (mPLMs), or <b>3)</b> through combining the former two options. In this work, we propose a novel semi-supervised <i>post-hoc</i> reranking method termed <b>BLICEr</b> (<b>BLI</b> with <b>C</b>ross-<b>E</b>ncoder <b>R</b>eranking), applicable to any precalculated CLWE space, which improves their BLI capability. The key idea is to ‘extract’ cross-lingual lexical knowledge from mPLMs, and then combine it with the original CLWEs. This crucial step is done via <b>1)</b> creating a word similarity dataset, comprising positive word pairs (i.e., true translations) and hard negative pairs induced from the original CLWE space, and then <b>2)</b> fine-tuning an mPLM (e.g., mBERT or XLM-R) in a cross-encoder manner to predict the similarity scores. At inference, we <b>3)</b> combine the similarity score from the original CLWE space with the score from the BLI-tuned cross-encoder. BLICEr establishes new state-of-the-art results on two standard BLI benchmarks spanning a wide spectrum of diverse languages: it substantially outperforms a series of strong baselines across the board. We also validate the robustness of BLICEr with different CLWEs. | Yaoyiran Li, Fangyu Liu, Ivan Vulic, Anna Korhonen |  |
| 304 |  |  [Mixed-modality Representation Learning and Pre-training for Joint Table-and-Text Retrieval in OpenQA](https://doi.org/10.18653/v1/2022.findings-emnlp.303) |  | 0 | Retrieving evidences from tabular and textual resources is essential for open-domain question answering (OpenQA), which provides more comprehensive information. However, training an effective dense table-text retriever is difficult due to the challenges of table-text discrepancy and data sparsity problem. To address the above challenges, we introduce an optimized OpenQA Table-Text Retriever (OTTeR) to jointly retrieve tabular and textual evidences. Firstly, we propose to enhance mixed-modality representation learning via two mechanisms: modality-enhanced representation and mixed-modality negative sampling strategy. Secondly, to alleviate data sparsity problem and enhance the general retrieval ability, we conduct retrieval-centric mixed-modality synthetic pre-training. Experimental results demonstrate that OTTeR substantially improves the performance of table-and-text retrieval on the OTT-QA dataset. Comprehensive analyses examine the effectiveness of all the proposed mechanisms. Besides, equipped with OTTeR, our OpenQA system achieves the state-of-the-art result on the downstream QA task, with 10.1% absolute improvement in terms of the exact match over the previous best system. | Junjie Huang, Wanjun Zhong, Qian Liu, Ming Gong, Daxin Jiang, Nan Duan |  |
| 305 |  |  [The Effects of Corpus Choice and Morphosyntax on Multilingual Space Induction](https://doi.org/10.18653/v1/2022.findings-emnlp.304) |  | 0 | In an effort to study the inductive biases of language models, numerous studies have attempted to use linguistically motivated tasks as a proxy of sorts, wherein performance on these tasks would imply an inductive bias towards a specific linguistic phenomenon. In this study, we attempt to analyse the inductive biases of language models with respect to natural language phenomena, in the context of building multilingual embedding spaces.We sample corpora from 2 sources in 15 languages and train language models on pseudo-bilingual variants of each corpus, created by duplicating each corpus and shifting token indices for half the resulting corpus. We evaluate the cross-lingual capabilities of these LMs, and show that while correlations with language families tend to be weak, other corpus-level characteristics, such as type-token ratio, tend to be more strongly correlated. Finally, we show that multilingual spaces can be built, albeit less effectively, even when additional destructive perturbations are applied to the training corpora, implying that (effectively) bag-of-words models also have an inductive bias that is sufficient for inducing multilingual spaces. | Vinit Ravishankar, Joakim Nivre |  |
| 306 |  |  [Modeling Complex Dialogue Mappings via Sentence Semantic Segmentation Guided Conditional Variational Auto-Encoder](https://doi.org/10.18653/v1/2022.findings-emnlp.305) |  | 0 | Complex dialogue mappings (CDM), including one-to-many and many-to-one mappings, tend to make dialogue models generate incoherent or dull responses, and modeling these mappings remains a huge challenge for neural dialogue systems. To alleviate these problems, methods like introducing external information, reconstructing the optimization function, and manipulating data samples are proposed, while they primarily focus on avoiding training with CDM, inevitably weakening the model’s ability of understanding CDM in human conversations and limiting further improvements in model performance. This paper proposes a Sentence Semantic Segmentation guided Conditional Variational Auto-Encoder (SegCVAE) method which can model and take advantages of the CDM data. Specifically, to tackle the incoherent problem caused by one-to-many, SegCVAE uses response-related prominent semantics to constrained the latent variable. To mitigate the non-diverse problem brought by many-to-one, SegCVAE segments multiple prominent semantics to enrich the latent variables. Three novel components, Internal Separation, External Guidance, and Semantic Norms, are proposed to achieve SegCVAE. On dialogue generation tasks, both the automatic and human evaluation results show that SegCVAE achieves new state-of-the-art performance. | Bin Sun, Shaoxiong Feng, Yiwei Li, Weichao Wang, Fei Mi, Yitong Li, Kan Li |  |
| 307 |  |  [Graph Embeddings for Argumentation Quality Assessment](https://doi.org/10.18653/v1/2022.findings-emnlp.306) |  | 0 | Argumentation is used by people both internally, by evaluating arguments and counterarguments to make sense of a situation and take a decision, and externally, e.g., in a debate, by exchanging arguments to reach an agreement or to promote an individual position. In this context, the assessment of the quality of the arguments is of extreme importance, as it strongly influences the evaluation of the overall argumentation, impacting on the decision making process. The automatic assessment of the quality of natural language arguments is recently attracting interest in the Argument Mining field. However, the issue of automatically assessing the quality of an argumentation largely remains a challenging unsolved task. Our contribution is twofold: first, we present a novel resource of 402 student persuasive essays, where three main quality dimensions (i.e., cogency, rhetoric, and reasonableness) have been annotated, leading to 1908 arguments tagged with quality facets; second, we address this novel task of argumentation quality assessment proposing a novel neural architecture based on graph embeddings, that combines both the textual features of the natural language arguments and the overall argument graph, i.e., considering also the support and attack relations holding among the arguments. Results on the persuasive essays dataset outperform state-of-the-art and standard baselines’ performance. | Santiago Marro, Elena Cabrio, Serena Villata |  |
| 308 |  |  [SMiLE: Schema-augmented Multi-level Contrastive Learning for Knowledge Graph Link Prediction](https://doi.org/10.18653/v1/2022.findings-emnlp.307) |  | 0 | Link prediction is the task of inferring missing links between entities in knowledge graphs. Embedding-based methods have shown effectiveness in addressing this problem by modeling relational patterns in triples. However, the link prediction task often requires contextual information in entity neighborhoods, while most existing embedding-based methods fail to capture it. Additionally, little attention is paid to the diversity of entity representations in different contexts, which often leads to false prediction results. In this situation, we consider that the schema of knowledge graph contains the specific contextual information, and it is beneficial for preserving the consistency of entities across contexts. In this paper, we propose a novel Schema-augmented Multi-level contrastive LEarning framework (SMiLE) to conduct knowledge graph link prediction. Specifically, we first exploit network schema as the prior constraint to sample negatives and pre-train our model by employing a multi-level contrastive learning method to yield both prior schema and contextual information. Then we fine-tune our model under the supervision of individual triples to learn subtler representations for link prediction. Extensive experimental results on four knowledge graph datasets with thorough analysis of each component demonstrate the effectiveness of our proposed framework against state-of-the-art baselines. The implementation of SMiLE is available at https://github.com/GKNL/SMiLE. | Miao Peng, Ben Liu, Qianqian Xie, Wenjie Xu, Hua Wang, Min Peng |  |
| 309 |  |  [Multilingual Multimodal Learning with Machine Translated Text](https://doi.org/10.18653/v1/2022.findings-emnlp.308) |  | 0 | Most vision-and-language pretraining research focuses on English tasks. However, the creation of multilingual multimodal evaluation datasets (e.g. Multi30K, xGQA, XVNLI, and MaRVL) poses a new challenge in finding high-quality training data that is both multilingual and multimodal. In this paper, we investigate whether machine translating English multimodal data can be an effective proxy for the lack of readily available multilingual data. We call this framework TD-MML: Translated Data for Multilingual Multimodal Learning, and it can be applied to any multimodal dataset and model. We apply it to both pretraining and fine-tuning data with a state-of-the-art model. In order to prevent models from learning from low-quality translated text, we propose two metrics for automatically removing such translations from the resulting datasets. In experiments on five tasks across 20 languages in the IGLUE benchmark, we show that translated data can provide a useful signal for multilingual multimodal learning, both at pretraining and fine-tuning. | Chen Qiu, Dan Oneata, Emanuele Bugliarello, Stella Frank, Desmond Elliott |  |
| 310 |  |  [Learning From the Source Document: Unsupervised Abstractive Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.309) |  | 0 | Most of the state-of-the-art methods for abstractive text summarization are under supervised learning settings, while heavily relying on high-quality and large-scale parallel corpora. In this paper, we remove the need for reference summaries and present an unsupervised learning method SCR (Summarize, Contrast and Review) for abstractive summarization, which leverages contrastive learning and is the first work to apply contrastive learning for unsupervised abstractive summarization. Particularly, we use the true source documents as positive source document examples, and strategically generated fake source documents as negative source document examples to train the model to generate good summaries. Furthermore, we consider and improve the writing quality of the generated summaries by guiding them to be similar to human-written texts. The promising results on extensive experiments show that SCR outperforms other unsupervised abstractive summarization baselines, which demonstrates its effectiveness. | Haojie Zhuang, Wei Emma Zhang, Jian Yang, Congbo Ma, Yutong Qu, Quan Z. Sheng |  |
| 311 |  |  [How to Do Things without Words: Modeling Semantic Drift of Emoji](https://doi.org/10.18653/v1/2022.findings-emnlp.310) |  | 0 | Emoji have become a significant part of our informal textual communication. Previous work, addressing the societal and linguistic functions of emoji, overlooked the relation between the semantics and the visual variations of the symbols. In this paper we model and analyze the semantic drift of emoji and discuss the features that may be contributing to the drift, some are unique to emoji and some are more general. Specifically, we explore the relations between graphical changes and semantic changes. | Eyal Arviv, Oren Tsur |  |
| 312 |  |  [Mind Your Bias: A Critical Review of Bias Detection Methods for Contextual Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.311) |  | 0 | The awareness and mitigation of biases are of fundamental importance for the fair and transparent use of contextual language models, yet they crucially depend on the accurate detection of biases as a precursor. Consequently, numerous bias detection methods have been proposed, which vary in their approach, the considered type of bias, and the data used for evaluation. However, while most detection methods are derived from the word embedding association test for static word embeddings, the reported results are heterogeneous, inconsistent, and ultimately inconclusive. To address this issue, we conduct a rigorous analysis and comparison of bias detection methods for contextual language models. Our results show that minor design and implementation decisions (or errors) have a substantial and often significant impact on the derived bias scores. Overall, we find the state of the field to be both worse than previously acknowledged due to systematic and propagated errors in implementations, yet better than anticipated since divergent results in the literature homogenize after accounting for implementation errors. Based on our findings, we conclude with a discussion of paths towards more robust and consistent bias detection methods. | Silke Husse, Andreas Spitz |  |
| 313 |  |  [ZeroPrompt: Scaling Prompt-Based Pretraining to 1, 000 Tasks Improves Zero-Shot Generalization](https://doi.org/10.18653/v1/2022.findings-emnlp.312) |  | 0 | We propose a multitask pretraining approach ZeroPrompt for zero-shot generalization, focusing on task scaling and zero-shot prompting.While previous models are trained on only a few dozen tasks, we scale to 1,000 tasks for the first time using real-world data. This leads to a crucial discovery that task scaling can be an efficient alternative to model scaling; i.e., the model size has less impact on performance with an extremely large number of tasks. Our results show that task scaling can improve training efficiency by 30 times in FLOPs.Empirically, ZeroPrompt substantially improves both the efficiency and the performance of zero-shot learning across a variety of academic and production datasets. | Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, Zhilin Yang |  |
| 314 |  |  [Semantic Role Labeling Meets Definition Modeling: Using Natural Language to Describe Predicate-Argument Structures](https://doi.org/10.18653/v1/2022.findings-emnlp.313) |  | 0 | One of the common traits of past and present approaches for Semantic Role Labeling (SRL) is that they rely upon discrete labels drawn from a predefined linguistic inventory to classify predicate senses and their arguments.However, we argue this need not be the case. In this paper, we present an approach that leverages Definition Modeling to introduce a generalized formulation of SRL as the task of describing predicate-argument structures using natural language definitions instead of discrete labels. Our novel formulation takes a first step towards placing interpretability and flexibility foremost, and yet our experiments and analyses on PropBank-style and FrameNet-style, dependency-based and span-based SRL also demonstrate that a flexible model with an interpretable output does not necessarily come at the expense of performance. We release our software for research purposes at https://github.com/SapienzaNLP/dsrl. | Simone Conia, Edoardo Barba, Alessandro Scirè, Roberto Navigli |  |
| 315 |  |  [Is anisotropy really the cause of BERT embeddings not being semantic?](https://doi.org/10.18653/v1/2022.findings-emnlp.314) |  | 0 | In this paper we conduct a set of experiments aimed to improve our understanding of the lack of semantic isometry in BERT, i.e. the lack of correspondence between the embedding and meaning spaces of its contextualized word representations. Our empirical results show that, contrary to popular belief, the anisotropy is not the root cause of the poor performance of these contextual models’ embeddings in semantic tasks. What does affect both the anisotropy and semantic isometry is a set of known biases: frequency, subword, punctuation, and case. For each one of them, we measure its magnitude and the effect of its removal, showing that these biases contribute but do not completely explain the phenomenon of anisotropy and lack of semantic isometry of these contextual language models. | Alejandro Fuster Baggetto, Víctor Fresno |  |
| 316 |  |  [m⌃4 Adapter: Multilingual Multi-Domain Adaptation for Machine Translation with a Meta-Adapter](https://doi.org/10.18653/v1/2022.findings-emnlp.315) |  | 0 | Multilingual neural machine translation models (MNMT) yield state-of-the-art performance when evaluated on data from a domain and language pair seen at training time. However, when a MNMT model is used to translate under domain shift or to a new language pair, performance drops dramatically. We consider a very challenging scenario: adapting the MNMT model both to a new domain and to a new language pair at the same time. In this paper, we propose m^4Adapter (Multilingual Multi-Domain Adaptation for Machine Translation with a Meta-Adapter), which combines domain and language knowledge using meta-learning with adapters. We present results showing that our approach is a parameter-efficient solution which effectively adapts a model to both a new language pair and a new domain, while outperforming other adapter methods. An ablation study also shows that our approach more effectively transfers domain knowledge across different languages and language information across different domains. | Wen Lai, Alexandra Chronopoulou, Alexander Fraser |  |
| 317 |  |  [Textual Enhanced Contrastive Learning for Solving Math Word Problems](https://doi.org/10.18653/v1/2022.findings-emnlp.316) |  | 0 | Solving math word problems is the task that analyses the relation of quantities e and requires an accurate understanding of contextual natural language information. Recent studies show that current models rely on shallow heuristics to predict solutions and could be easily misled by small textual perturbations. To address this problem, we propose a Textual Enhanced Contrastive Learning framework, which enforces the models to distinguish semantically similar examples while holding different mathematical logic. We adopt a self-supervised manner strategy to enrich examples with subtle textual variance by textual reordering or problem re-construction. We then retrieve the hardest to differentiate samples from both equation and textual perspectives and guide the model to learn their representations. Experimental results show that our method achieves state-of-the-art on both widely used benchmark datasets and also exquisitely designed challenge datasets in English and Chinese. | Yibin Shen, Qianying Liu, Zhuoyuan Mao, Fei Cheng, Sadao Kurohashi |  |
| 318 |  |  [What Do Compressed Multilingual Machine Translation Models Forget?](https://doi.org/10.18653/v1/2022.findings-emnlp.317) |  | 0 | Recently, very large pre-trained models achieve state-of-the-art results in various natural language processing (NLP) tasks, but their size makes it more challenging to apply them in resource-constrained environments. Compression techniques allow to drastically reduce the size of the models and therefore their inference time with negligible impact on top-tier metrics. However, the general performance averaged across multiple tasks and/or languages may hide a drastic performance drop on under-represented features, which could result in the amplification of biases encoded by the models. In this work, we assess the impact of compression methods on Multilingual Neural Machine Translation models (MNMT) for various language groups, gender, and semantic biases by extensive analysis of compressed models on different machine translation benchmarks, i.e. FLORES-101, MT-Gender, and DiBiMT. We show that the performance of under-represented languages drops significantly, while the average BLEU metric only slightly decreases. Interestingly, the removal of noisy memorization with compression leads to a significant improvement for some medium-resource languages. Finally, we demonstrate that compression amplifies intrinsic gender and semantic biases, even in high-resource languages. | Alireza Mohammadshahi, Vassilina Nikoulina, Alexandre Berard, Caroline Brun, James Henderson, Laurent Besacier |  |
| 319 |  |  [Controllable Dialogue Simulation with In-context Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.318) |  | 0 | Building dialogue systems requires a large corpus of annotated dialogues. Such datasets are usually created via crowdsourcing, which is expensive and time-consuming. In this paper, we propose Dialogic, a novel dialogue simulation method based on large language model in-context learning to automate dataset creation. Seeded with a few annotated dialogues, Dialogic automatically selects in-context examples for demonstration and prompts GPT-3 to generate new dialogues and annotations in a controllable way. Our method can rapidly expand a small set of dialogue data with minimum or zero human involvement and parameter update and is thus much more cost-efficient and time-saving than crowdsourcing. Experimental results on the MultiWOZ dataset demonstrate that training a model on the simulated dialogues leads to even better performance than using the same amount of human-generated dialogues under the challenging low-resource settings, with as few as 85 dialogues as a seed. When the full training set is given, our method can still serve as an effective data augmentation method to further improve performance. Human evaluation results also show that our simulated dialogues have near-human fluency and annotation accuracy. The code and data are available at https://github.com/Leezekun/dialogic. | Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, Xifeng Yan |  |
| 320 |  |  [Improving the Factual Correctness of Radiology Report Generation with Semantic Rewards](https://doi.org/10.18653/v1/2022.findings-emnlp.319) |  | 0 | Neural image-to-text radiology report generation systems offer the potential to improve radiology reporting by reducing the repetitive process of report drafting and identifying possible medical errors. These systems have achieved promising performance as measured by widely used NLG metrics such as BLEU and CIDEr. However, the current systems face important limitations. First, they present an increased complexity in architecture that offers only marginal improvements on NLG metrics. Secondly, these systems that achieve high performance on these metrics are not always factually complete or consistent due to both inadequate training and evaluation. Recent studies have shown the systems can be substantially improved by using new methods encouraging 1) the generation of domain entities consistent with the reference and 2) describing these entities in inferentially consistent ways. So far, these methods rely on weakly-supervised approaches (rule-based) and named entity recognition systems that are not specific to the chest X-ray domain. To overcome this limitation, we propose a new method, the RadGraph reward, to further improve the factual completeness and correctness of generated radiology reports. More precisely, we leverage the RadGraph dataset containing annotated chest X-ray reports with entities and relations between entities. On two open radiology report datasets, our system substantially improves the scores up to 14.2% and 25.3% on metrics evaluating the factual correctness and completeness of reports. | JeanBenoit Delbrouck, Pierre J. Chambon, Christian Bluethgen, Emily Bao Tsai, Omar Almusa, Curtis P. Langlotz |  |
| 321 |  |  [Recursive Neural Networks with Bottlenecks Diagnose (Non-)Compositionality](https://doi.org/10.18653/v1/2022.findings-emnlp.320) |  | 0 | A recent line of work in NLP focuses on the (dis)ability of models to generalise compositionally for artificial languages.However, when considering natural language tasks, the data involved is not strictly, or locally, compositional.Quantifying the compositionality of data is a challenging task, which has been investigated primarily for short utterances.We use recursive neural models (Tree-LSTMs) with bottlenecks that limit the transfer of information between nodes.We illustrate that comparing data’s representations in models with and without the bottleneck can be used to produce a compositionality metric.The procedure is applied to the evaluation of arithmetic expressions using synthetic data, and sentiment classification using natural language data.We demonstrate that compression through a bottleneck impacts non-compositional examples disproportionatelyand then use the bottleneck compositionality metric (BCM) to distinguish compositional from non-compositional samples, yielding a compositionality ranking over a dataset. | Verna Dankers, Ivan Titov |  |
| 322 |  |  [HumSet: Dataset of Multilingual Information Extraction and Classification for Humanitarian Crises Response](https://doi.org/10.18653/v1/2022.findings-emnlp.321) |  | 0 | Timely and effective response to humanitarian crises requires quick and accurate analysis of large amounts of text data – a process that can highly benefit from expert-assisted NLP systems trained on validated and annotated data in the humanitarian response domain. To enable creation of such NLP systems, we introduce and release HumSet, a novel and rich multilingual dataset of humanitarian response documents annotated by experts in the humanitarian response community. The dataset provides documents in three languages (English, French, Spanish) and covers a variety of humanitarian crises from 2018 to 2021 across the globe. For each document, HUMSET provides selected snippets (entries) as well as assigned classes to each entry annotated using common humanitarian information analysis frameworks. HUMSET also provides novel and challenging entry extraction and multi-label entry classification tasks. In this paper, we take a first step towards approaching these tasks and conduct a set of experiments on Pre-trained Language Models (PLM) to establish strong baselines for future research in this domain. The dataset is available at https://blog.thedeep.io/humset/. | Selim Fekih, Nicolò Tamagnone, Benjamin Minixhofer, Ranjan Shrestha, Ximena Contla, Ewan Oglethorpe, Navid Rekabsaz |  |
| 323 |  |  [Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.322) |  | 0 | Non-autoregressive models achieve significant decoding speedup in neural machine translation but lack the ability to capture sequential dependency. Directed Acyclic Transformer (DA-Transformer) was recently proposed to model sequential dependency with a directed acyclic graph. Consequently, it has to apply a sequential decision process at inference time, which harms the global translation accuracy. In this paper, we present a Viterbi decoding framework for DA-Transformer, which guarantees to find the joint optimal solution for the translation and decoding path under any length constraint. Experimental results demonstrate that our approach consistently improves the performance of DA-Transformer while maintaining a similar decoding speedup. | Chenze Shao, Zhengrui Ma, Yang Feng |  |
| 324 |  |  [Lexical Generalization Improves with Larger Models and Longer Training](https://doi.org/10.18653/v1/2022.findings-emnlp.323) |  | 0 | While fine-tuned language models perform well on many language tasks, they were also shown to rely on superficial surface features such as lexical overlap. Excessive utilization of such heuristics can lead to failure on challenging inputs. We analyze the use of lexical overlap heuristics in natural language inference, paraphrase detection, and reading comprehension (using a novel contrastive dataset),and find that larger models are much less susceptible to adopting lexical overlap heuristics. We also find that longer training leads models to abandon lexical overlap heuristics. Finally, We provide evidence that the disparity between models size has its source in the pre-trained model. | Elron Bandel, Yoav Goldberg, Yanai Elazar |  |
| 325 |  |  [Realistic Data Augmentation Framework for Enhancing Tabular Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.324) |  | 0 | Existing approaches to constructing training data for Natural Language Inference (NLI) tasks, such as for semi-structured table reasoning, are either via crowdsourcing or fully automatic methods. However, the former is expensive and time consuming and thus limits scale, and the latter often produces naive examples that may lack complex reasoning. This paper develops a realistic semi-automated framework for data augmentation for tabular inference. Instead of manually generating a hypothesis for each table, our methodology generates hypothesis templates transferable to similar tables. In addition, our framework entails the creation of rational counterfactual tables based on human written logical constraints and premise paraphrasing. For our case study, we use the INFOTABS (Gupta et al., 2020), which is an entity centric tabular inference dataset. We observed that our framework could generate human-like tabular inference examples, which could benefit training data augmentation, especially in the scenario with limited supervision. | Dibyakanti Kumar, Vivek Gupta, Soumya Sharma, Shuo Zhang |  |
| 326 |  |  [Inducing Generalizable and Interpretable Lexica](https://doi.org/10.18653/v1/2022.findings-emnlp.325) |  | 0 | Lexica – words and associated scores – are widely used as simple, interpretable, generalizable language features to predict sentiment, emotions, mental health, and personality. They also provide insight into the psychological features behind those moods and traits. Such lexica, historically created by human experts, are valuable to linguists, psychologists, and social scientists, but they take years of refinement and have limited coverage. In this paper, we investigate how the lexica that provide psycholinguistic insights could be computationally induced and how they should be assessed. We identify generalizability and interpretability as two essential properties of such lexica. We induce lexica using both context-oblivious and context-aware approaches, compare their predictive performance both within the training corpus and across various corpora, and evaluate their quality using crowd-worker assessment. We find that lexica induced from context-oblivious models are more generalizable and interpretable than those from more accurate context-aware transformer models. In addition, lexicon scores can identify explanatory words more reliably than a high performing transformer with feature-importance measures like SHAP. | Yilin Geng, Zetian Wu, Roshan Santhosh, Tejas Srivastava, Lyle H. Ungar, João Sedoc |  |
| 327 |  |  [The Curious Case of Absolute Position Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.326) |  | 0 | Transformer language models encode the notion of word order using positional information. Most commonly, this positional information is represented by absolute position embeddings (APEs), that are learned from the pretraining data. However, in natural language, it is not absolute position that matters, but relative position, and the extent to which APEs can capture this type of information has not been studied. In this work, we observe that models trained with APE over-rely on positional information to the point that they break-down when subjected to sentences with shifted position information. Specifically, when models are subjected to sentences starting from a non-zero position (excluding the effect of priming), they exhibit noticeably degraded performance on zero- to full-shot tasks, across a range of model families and model sizes. Our findings raise questions about the efficacy of APEs to model the relativity of position information, and invite further introspection on the sentence and word order processing strategies employed by these models. | Koustuv Sinha, Amirhossein Kazemnejad, Siva Reddy, Joelle Pineau, Dieuwke Hupkes, Adina Williams |  |
| 328 |  |  [Goal-oriented Vision-and-Dialog Navigation via Reinforcement Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.327) |  | 0 | Vision-and-dialog navigation is a recent benchmark for evaluating the AI capabilities of perception, interaction, and decision making. While existing methods developed for this benchmark have demonstrated great successes, they mostly rely on large datasets, where data collection can be a challenge, and the learned policies are not adaptive to domain changes. In this paper, we focus on a new problem, referred to as goal-oriented vision-and-dialog navigation (GVDN), where an agent uses reinforcement learning techniques to compute dialog-navigation policies from trial and error. A robot conducts visual navigation to locate target objects, and can talk to a remote human operator as needed. Our remote human is able to provide guidance on navigation only if the robot correctly conveys its location through dialog. Experiments have been conducted using photo-realistic simulation environments. Results suggest that, our agent outperforms competitive baselines in success rate. | Yan Cao, Keting Lu, David DeFazio, Shiqi Zhang |  |
| 329 |  |  [Leveraging Data Recasting to Enhance Tabular Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.328) |  | 0 | Creating challenging tabular inference data is essential for learning complex reasoning. Prior work has mostly relied on two data generation strategies. The first is human annotation, which yields linguistically diverse data but is difficult to scale. The second category for creation is synthetic generation, which is scalable and cost effective but lacks inventiveness. In this research, we present a framework for semi-automatically recasting existing tabular data to make use of the benefits of both approaches. We utilize our framework to build tabular NLI instances from five datasets that were initially intended for tasks like table2text creation, tabular Q/A, and semantic parsing. We demonstrate that recasted data could be used as evaluation benchmarks as well as augmentation data to enhance performance on tabular NLI tasks. Furthermore, we investigate the effectiveness of models trained on recasted data in the zero-shot scenario, and analyse trends in performance across different recasted datasets types. | Aashna Jena, Vivek Gupta, Manish Shrivastava, Julian Martin Eisenschlos |  |
| 330 |  |  [Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again](https://doi.org/10.18653/v1/2022.findings-emnlp.329) |  | 0 | Large pre-trained language models (PLMs) such as GPT-3 have shown strong in-context learning capabilities, which are highly appealing for domains such as biomedicine that feature high and diverse demands of language technologies but also high data annotation costs. In this paper, we present the first systematic and comprehensive study to compare the few-shot performance of GPT-3 in-context learning with fine-tuning smaller (i.e., BERT-sized) PLMs on two representative biomedical information extraction (IE) tasks: named entity recognition and relation extraction. We follow the true few-shot setting to avoid overestimating models’ few-shot performance by model selection over a large validation set. We also optimize GPT-3’s performance with known techniques such as contextual calibration and dynamic in-context example retrieval. However, our results show that GPT-3 still significantly underperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3 in-context learning also yields smaller gains in accuracy when more training data becomes available. More in-depth analyses further reveal issues of in-context learning that may be detrimental to IE tasks in general. Given the high cost of experimenting with GPT-3, we hope our study provides helpful guidance for biomedical researchers and practitioners towards more practical solutions such as fine-tuning small PLMs before better in-context learning is available for biomedical IE. | Bernal Jimenez Gutierrez, Nikolas McNeal, Clayton Washington, You Chen, Lang Li, Huan Sun, Yu Su |  |
| 331 |  |  [Attention weights accurately predict language representations in the brain](https://doi.org/10.18653/v1/2022.findings-emnlp.330) |  | 0 | In Transformer-based language models (LMs) the attention mechanism converts token embeddings into contextual embeddings that incorporate information from neighboring words. The resulting contextual hidden state embeddings have enabled highly accurate models of brain responses, suggesting that the attention mechanism constructs contextual embeddings that carry information reflected in language-related brain representations. However, it is unclear whether the attention weights that are used to integrate information across words are themselves related to language representations in the brain. To address this question we analyzed functional magnetic resonance imaging (fMRI) recordings of participants reading English language narratives. We provided the narrative text as input to two LMs (BERT and GPT-2) and extracted their corresponding attention weights. We then used encoding models to determine how well attention weights can predict recorded brain responses. We find that attention weights accurately predict brain responses in much of the frontal and temporal cortices. Our results suggest that the attention mechanism itself carries information that is reflected in brain representations. Moreover, these results indicate cortical areas in which context integration may occur. | Mathis Lamarre, Catherine Chen, Fatma Deniz |  |
| 332 |  |  [Improving HowNet-Based Chinese Word Sense Disambiguation with Translations](https://doi.org/10.18653/v1/2022.findings-emnlp.331) |  | 0 | Word sense disambiguation (WSD) is the task of identifying the intended sense of a word in context. While prior work on unsupervised WSD has leveraged lexical knowledge bases, such as WordNet and BabelNet, these resources have proven to be less effective for Chinese. Instead, the most widely used lexical knowledge base for Chinese is HowNet. Previous HowNet-based WSD methods have not exploited contextual translation information. In this paper, we present the first HowNet-based WSD system which combines monolingual contextual information from a pretrained neural language model with bilingual information obtained via machine translation and sense translation information from HowNet. The results of our evaluation experiment on a test set from prior work demonstrate that our new method achieves a new state of the art for unsupervised Chinese WSD. | Xiang Zhang, Bradley Hauer, Grzegorz Kondrak |  |
| 333 |  |  [Mask-then-Fill: A Flexible and Effective Data Augmentation Framework for Event Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.332) |  | 0 | We present Mask-then-Fill, a flexible and effective data augmentation framework for event extraction. Our approach allows for more flexible manipulation of text and thus can generate more diverse data while keeping the original event structure unchanged as much as possible. Specifically, it first randomly masks out an adjunct sentence fragment and then infills a variable-length text span with a fine-tuned infilling model. The main advantage lies in that it can replace a fragment of arbitrary length in the text with another fragment of variable length, compared to the existing methods which can only replace a single word or a fixed-length fragment. On trigger and argument extraction tasks, the proposed framework is more effective than baseline methods and it demonstrates particularly strong results in the low-resource setting. Our further analysis shows that it achieves a good balance between diversity and distributional similarity. | Jun Gao, Changlong Yu, Wei Wang, Huan Zhao, Ruifeng Xu |  |
| 334 |  |  [MOBA-E2C: Generating MOBA Game Commentaries via Capturing Highlight Events from the Meta-Data](https://doi.org/10.18653/v1/2022.findings-emnlp.333) |  | 0 | MOBA (Multiplayer Online Battle Arena) games such as Dota2 are currently one of the most popular e-sports gaming genres. Following professional commentaries is a great way to understand and enjoy a MOBA game. However, massive game competitions lack commentaries because of the shortage of professional human commentators. As an alternative, employing machine commentators that can work at any time and place is a feasible solution. Considering the challenges in modeling MOBA games, we propose a data-driven MOBA commentary generation framework, MOBA-E2C, allowing a model to generate commentaries based on the game meta-data. Subsequently, to alleviate the burden of collecting supervised data, we propose a MOBA-FuseGPT generator to generate MOBA game commentaries by fusing the power of a rule-based generator and a generative GPT generator. Finally, in the experiments, we take a popular MOBA game Dota2 as our case and construct a Chinese Dota2 commentary generation dataset Dota2-Commentary. Experimental results demonstrate the superior performance of our approach. To the best of our knowledge, this work is the first Dota2 machine commentator and Dota2-Commentary is the first dataset. | Dawei Zhang, Sixing Wu, Yao Guo, Xiangqun Chen |  |
| 335 |  |  [Enhancing Automatic Readability Assessment with Pre-training and Soft Labels for Ordinal Regression](https://doi.org/10.18653/v1/2022.findings-emnlp.334) |  | 0 | The readability assessment task aims to assign a difficulty grade to a text. While neural models have recently demonstrated impressive performance, most do not exploit the ordinal nature of the difficulty grades, and make little effort for model initialization to facilitate fine-tuning. We address these limitations with soft labels for ordinal regression, and with model pre-training through prediction of pairwise relative text difficulty. We incorporate these two components into a model based on hierarchical attention networks, and evaluate its performance on both English and Chinese datasets. Experimental results show that our proposed model outperforms competitive neural models and statistical classifiers on most datasets. | Jinshan Zeng, Yudong Xie, Xianglong Yu, John Lee, DingXuan Zhou |  |
| 336 |  |  [Opening up Minds with Argumentative Dialogues](https://doi.org/10.18653/v1/2022.findings-emnlp.335) |  | 0 | Recent research on argumentative dialogues has focused on persuading people to take some action, changing their stance on the topic of discussion, or winning debates. In this work, we focus on argumentative dialogues that aim to open up (rather than change) people’s minds to help them become more understanding to views that are unfamiliar or in opposition to their own convictions. To this end, we present a dataset of 183 argumentative dialogues about 3 controversial topics: veganism, Brexit and COVID-19 vaccination. The dialogues were collected using the Wizard of Oz approach, where wizards leverage a knowledge-base of arguments to converse with participants. Open-mindedness is measured before and after engaging in the dialogue using a questionnaire from the psychology literature, and success of the dialogue is measured as the change in the participant’s stance towards those who hold opinions different to theirs. We evaluate two dialogue models: a Wikipedia-based and an argument-based model. We show that while both models perform closely in terms of opening up minds, the argument-based model is significantly better on other dialogue properties such as engagement and clarity. | Youmna Farag, Charlotte O. Brand, Jacopo Amidei, Paul Piwek, Tom Stafford, Svetlana Stoyanchev, Andreas Vlachos |  |
| 337 |  |  [You Are My Type! Type Embeddings for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.336) |  | 0 | One reason for the positive impact of Pre-trained Language Models (PLMs) in NLP tasks is their ability to encode semantic types, such as ‘European City’ or ‘Woman’. While previous work has analyzed such information in the context of interpretability, it is not clear how to use types to steer the PLM output. For example, in a cloze statement, it is desirable to steer the model to generate a token that satisfies a user-specified type, e.g., predict a date rather than a location. In this work, we introduce Type Embeddings (TEs), an input embedding that promotes desired types in a PLM. Our proposal is to define a type by a small set of word examples. We empirically study the ability of TEs both in representing types and in steering masking predictions without changes to the prompt text in BERT. Finally, using the LAMA datasets, we show how TEs highly improve the precision in extracting facts from PLMs. | Mohammed Saeed, Paolo Papotti |  |
| 338 |  |  [Generating Textual Adversaries with Minimal Perturbation](https://doi.org/10.18653/v1/2022.findings-emnlp.337) |  | 0 | Many word-level adversarial attack approaches for textual data have been proposed in recent studies. However, due to the massive search space consisting of combinations of candidate words, the existing approaches face the problem of preserving the semantics of texts when crafting adversarial counterparts. In this paper, we develop a novel attack strategy to find adversarial texts with high similarity to the original texts while introducing minimal perturbation. The rationale is that we expect the adversarial texts with small perturbation can better preserve the semantic meaning of original texts. Experiments show that, compared with state-of-the-art attack approaches, our approach achieves higher success rates and lower perturbation rates in four benchmark datasets. | Xingyi Zhao, Lu Zhang, Depeng Xu, Shuhan Yuan |  |
| 339 |  |  [SensePOLAR: Word sense aware interpretability for pre-trained contextual word embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.338) |  | 0 | Adding interpretability to word embeddings represents an area of active research in textrepresentation. Recent work has explored the potential of embedding words via so-called polardimensions (e.g. good vs. bad, correct vs. wrong). Examples of such recent approachesinclude SemAxis, POLAR, FrameAxis, and BiImp. Although these approaches provide interpretabledimensions for words, they have not been designed to deal with polysemy, i.e. they can not easily distinguish between different senses of words. To address this limitation, we present SensePOLAR, an extension of the original POLAR framework that enables wordsense aware interpretability for pre-trained contextual word embeddings. The resulting interpretable word embeddings achieve a level ofperformance that is comparable to original contextual word embeddings across a variety ofnatural language processing tasks including the GLUE and SQuAD benchmarks. Our workremoves a fundamental limitation of existing approaches by offering users sense aware interpretationsfor contextual word embeddings. | Jan Engler, Sandipan Sikdar, Marlene Lutz, Markus Strohmaier |  |
| 340 |  |  [Contextualizing Language Models for Norms Diverging from Social Majority](https://doi.org/10.18653/v1/2022.findings-emnlp.339) |  | 0 | To comprehensibly contextualize decisions, artificial systems in social situations need a high degree of awareness of the rules of conduct of human behavior. Especially transformer-based language models have recently been shown to exhibit some such awareness. But what if norms in some social setting do not adhere to or even blatantly deviate from the mainstream? In this paper, we introduce a novel mechanism based on deontic logic to allow for a flexible adaptation of individual norms by de-biasing training data sets and a task-reduction to textual entailment. Building on the popular ‘Moral Stories’ dataset we on the one hand highlight the intrinsic bias of current language models, on the other hand characterize the adaptability of pre-trained models to deviating norms in fine-tuning settings. | Niklas Kiehne, Hermann Kroll, WolfTilo Balke |  |
| 341 |  |  [Empathetic Dialogue Generation via Sensitive Emotion Recognition and Sensible Knowledge Selection](https://doi.org/10.18653/v1/2022.findings-emnlp.340) |  | 0 | Empathy, which is widely used in psychological counseling, is a key trait of everyday human conversations. Equipped with commonsense knowledge, current approaches to empathetic response generation focus on capturing implicit emotion within dialogue context, where the emotions are treated as a static variable throughout the conversations. However, emotions change dynamically between utterances, which makes previous works difficult to perceive the emotion flow and predict the correct emotion of the target response, leading to inappropriate response. Furthermore, simply importing commonsense knowledge without harmonization may trigger the conflicts between knowledge and emotion, which confuse the model to choose the correct information to guide the generation process. To address the above problems, we propose a Serial Encoding and Emotion-Knowledge interaction (SEEK) method for empathetic dialogue generation. We use a fine-grained encoding strategy which is more sensitive to the emotion dynamics (emotion flow) in the conversations to predict the emotion-intent characteristic of response. Besides, we design a novel framework to model the interaction between knowledge and emotion to solve the conflicts generate more sensible response. Extensive experiments on the utterance-level annotated EMPATHETICDIALOGUES demonstrate that SEEK outperforms the strong baseline in both automatic and manual evaluations. | Lanrui Wang, Jiangnan Li, Zheng Lin, Fandong Meng, Chenxu Yang, Weiping Wang, Jie Zhou |  |
| 342 |  |  [Joint Multilingual Knowledge Graph Completion and Alignment](https://doi.org/10.18653/v1/2022.findings-emnlp.341) |  | 0 | Knowledge graph (KG) alignment and completion are usually treated as two independent tasks. While recent work has leveraged entity and relation alignments from multiple KGs, such as alignments between multilingual KGs with common entities and relations, a deeper understanding of the ways in which multilingual KG completion (MKGC) can aid the creation of multilingual KG alignments (MKGA) is still limited. Motivated by the observation that structural inconsistencies – the main challenge for MKGA models – can be mitigated through KG completion methods, we propose a novel model for jointly completing and aligning knowledge graphs. The proposed model combines two components that jointly accomplish KG completion and alignment. These two components employ relation-aware graph neural networks that we propose to encode multi-hop neighborhood structures into entity and relation representations. Moreover, we also propose (i) a structural inconsistency reduction mechanism to incorporate information from the completion into the alignment component, and (ii) an alignment seed enlargement and triple transferring mechanism to enlarge alignment seeds and transfer triples during KGs alignment. Extensive experiments on a public multilingual benchmark show that our proposed model outperforms existing competitive baselines, obtaining new state-of-the-art results on both MKGC and MKGA tasks. | Vinh Tong, Dat Quoc Nguyen, Trung Thanh Huynh, Tam Thanh Nguyen, Quoc Viet Hung Nguyen, Mathias Niepert |  |
| 343 |  |  [A Framework for Automatic Generation of Spoken Question-Answering Data](https://doi.org/10.18653/v1/2022.findings-emnlp.342) |  | 0 | This paper describes a framework to automatically generate a spoken question answering (QA) dataset. The framework consists of a question generation (QG) module to generate questions automatically from given text documents, a text-to-speech (TTS) module to convert the text documents into spoken form and an automatic speech recognition (ASR) module to transcribe the spoken content. The final dataset contains question-answer pairs for both the reference text and ASR transcriptions as well as the audio files corresponding to each reference text. For QG and ASR systems we used pre-trained multilingual encoder-decoder transformer models and fine-tuned these models using a limited amount of manually generated QA data and TTS-based speech data, respectively. As a proof of concept, we investigated the proposed framework for Turkish and generated the Turkish Question Answering (TurQuAse) dataset using Wikipedia articles. Manual evaluation of the automatically generated question- answer pairs and QA performance evaluation with state of-the-art models on TurQuAse show that the proposed framework is efficient for automatically generating spoken QA datasets. To the best of our knowledge, TurQuAse is the first publicly available spoken question answering dataset for Turkish. The proposed framework can be easily extended to other languages where a limited amount of QA data is available. | Merve Ünlü Menevse, Yusufcan Manav, Ebru Arisoy, Arzucan Özgür |  |
| 344 |  |  [Readability Controllable Biomedical Document Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.343) |  | 0 | Different from general documents, it is recognised that the ease with which people can understand a biomedical text is eminently varied, owing to the highly technical nature of biomedical documents and the variance of readers’ domain knowledge. However, existing biomedical document summarization systems have paid little attention to readability control, leaving users with summaries that are incompatible with their levels of expertise.In recognition of this urgent demand, we introduce a new task of readability controllable summarization for biomedical documents, which aims to recognise users’ readability demands and generate summaries that better suit their needs: technical summaries for experts and plain language summaries (PLS) for laymen.To establish this task, we construct a corpus consisting of biomedical papers with technical summaries and PLSs written by the authors, and benchmark multiple advanced controllable abstractive and extractive summarization models based on pre-trained language models (PLMs) with prevalent controlling and generation techniques.Moreover, we propose a novel masked language model (MLM) based metric and its variant to effectively evaluate the readability discrepancy between lay and technical summaries.Experimental results from automated and human evaluations show that though current control techniques allow for a certain degree of readability adjustment during generation, the performance of existing controllable summarization methods is far from desirable in this task. | Zheheng Luo, Qianqian Xie, Sophia Ananiadou |  |
| 345 |  |  [Beyond Additive Fusion: Learning Non-Additive Multimodal Interactions](https://doi.org/10.18653/v1/2022.findings-emnlp.344) |  | 0 | Multimodal fusion addresses the problem of analyzing spoken words in the multimodal context, including visual expressions and prosodic cues. Even when multimodal models lead to performance improvements, it is often unclear whether bimodal and trimodal interactions are learned or whether modalities are processed independently of each other. We propose Multimodal Residual Optimization (MRO) to separate unimodal, bimodal, and trimodal interactions in a multimodal model. This improves interpretability as the multimodal interaction can be quantified. Inspired by Occam’s razor, the main intuition of MRO is that (simpler) unimodal contributions should be learned before learning (more complex) bimodal and trimodal interactions. For example, bimodal predictions should learn to correct the mistakes (residuals) of unimodal predictions, thereby letting the bimodal predictions focus on the remaining bimodal interactions. Empirically, we observe that MRO successfully separates unimodal, bimodal, and trimodal interactions while not degrading predictive performance. We complement our empirical results with a human perception study and observe that MRO learns multimodal interactions that align with human judgments. | Torsten Wörtwein, Lisa Sheeber, Nicholas B. Allen, Jeffrey F. Cohn, LouisPhilippe Morency |  |
| 346 |  |  [Generalization Differences between End-to-End and Neuro-Symbolic Vision-Language Reasoning Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.345) |  | 0 | For vision-and-language reasoning tasks, both fully connectionist, end-to-end methods and hybrid, neuro-symbolic methods have achieved high in-distribution performance. In which out-of-distribution settings does each paradigm excel? We investigate this question on both single-image and multi-image visual question-answering through four types of generalization tests: a novel segment-combine test for multi-image queries, contrast set, compositional generalization, and cross-benchmark transfer.Vision-and-language end-to-end trained systems exhibit sizeable performance drops across all these tests. Neuro-symbolic methods suffer even more on cross-benchmark transfer from GQA to VQA, but they show smaller accuracy drops on the other generalization tests and their performance quickly improves by few-shot training. Overall, our results demonstrate the complementary benefits of these two paradigms, and emphasize the importance of using a diverse suite of generalization tests to fully characterize model robustness to distribution shift. | Wang Zhu, Jesse Thomason, Robin Jia |  |
| 347 |  |  [Learning to Model Multimodal Semantic Alignment for Story Visualization](https://doi.org/10.18653/v1/2022.findings-emnlp.346) |  | 0 | Story visualization aims to generate a sequence of images to narrate each sentence in a multi-sentence story, where the images should be realistic and keep global consistency across dynamic scenes and characters. Current works face the problem of semantic misalignment because of their fixed architecture and diversity of input modalities. To address this problem, we explore the semantic alignment between text and image representations by learning to match their semantic levels in the GAN-based generative model. More specifically, we introduce dynamic interactions according to learning to dynamically explore various semantic depths and fuse the different-modal information at a matched semantic level, which thus relieves the text-image semantic misalignment problem. Extensive experiments on different datasets demonstrate the improvements of our approach, neither using segmentation masks nor auxiliary captioning networks, on image quality and story consistency, compared with state-of-the-art methods. | Bowen Li, Thomas Lukasiewicz |  |
| 348 |  |  [SciFact-Open: Towards open-domain scientific claim verification](https://doi.org/10.18653/v1/2022.findings-emnlp.347) |  | 0 | While research on scientific claim verification has led to the development of powerful systems that appear to approach human performance, these approaches have yet to be tested in a realistic setting against large corpora of scientific literature. Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. Drawing upon pooling techniques from information retrieval, we collect evidence for scientific claims by pooling and annotating the top predictions of four state-of-the-art scientific claim verification models. We find that systems developed on smaller corpora struggle to generalize to SciFact-Open, exhibiting performance drops of at least 15 F1. In addition, analysis of the evidence in SciFact-Open reveals interesting phenomena likely to appear when claim verification systems are deployed in practice, e.g., cases where the evidence supports only a special case of the claim. Our dataset is available at https://github.com/dwadden/scifact-open. | David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, Hannaneh Hajishirzi |  |
| 349 |  |  [COMET-QE and Active Learning for Low-Resource Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.348) |  | 0 | Active learning aims to deliver maximum benefit when resources are scarce. We use COMET-QE, a reference-free evaluation metric, to select sentences for low-resource neural machine translation. Using Swahili, Kinyarwanda and Spanish for our experiments, we show that COMET-QE significantly outperforms two variants of Round Trip Translation Likelihood (RTTL) and random sentence selection by up to 5 BLEU points for 20k sentences selected by Active Learning on a 30k baseline. This suggests that COMET-QE is a powerful tool for sentence selection in the very low-resource limit. | Everlyn Chimoto, Bruce A. Bassett |  |
| 350 |  |  [MedicalSum: A Guided Clinical Abstractive Summarization Model for Generating Medical Reports from Patient-Doctor Conversations](https://doi.org/10.18653/v1/2022.findings-emnlp.349) |  | 0 | We introduce MedicalSum, a transformer-based sequence-to-sequence architecture for summarizing medical conversations by integrating medical domain knowledge from the Unified Medical Language System (UMLS). The novel knowledge augmentation is performed in three ways: (i) introducing a guidance signal that consists of the medical words in the input sequence, (ii) leveraging semantic type knowledge in UMLS to create clinically meaningful input embeddings, and (iii) making use of a novel weighted loss function that provides a stronger incentive for the model to correctly predict words with a medical meaning. By applying these three strategies, MedicalSum takes clinical knowledge into consideration during the summarization process and achieves state-of-the-art ROUGE score improvements of 0.8-2.1 points (including 6.2% ROUGE-1 error reduction in the PE section) when producing medical summaries of patient-doctor conversations. | George Michalopoulos, Kyle Williams, Gagandeep Singh, Thomas Lin |  |
| 351 |  |  [Leveraging Training Dynamics and Self-Training for Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.350) |  | 0 | The effectiveness of pre-trained language models in downstream tasks is highly dependent on the amount of labeled data available for training. Semi-supervised learning (SSL) is a promising technique that has seen wide attention recently due to its effectiveness in improving deep learning models when training data is scarce. Common approaches employ a teacher-student self-training framework, where a teacher network generates pseudo-labels for unlabeled data, which are then used to iteratively train a student network. In this paper, we propose a new self-training approach for text classification that leverages training dynamics of unlabeled data. We evaluate our approach on a wide range of text classification tasks, including emotion detection, sentiment analysis, question classification and gramaticality, which span a variety of domains, e.g, Reddit, Twitter, and online forums. Notably, our method is successful on all benchmarks, obtaining an average increase in F1 score of 3.5% over strong baselines in low resource settings. | Tiberiu Sosea, Cornelia Caragea |  |
| 352 |  |  [Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference](https://doi.org/10.18653/v1/2022.findings-emnlp.351) |  | 0 | Natural Language Inference (NLI) or Recognizing Textual Entailment (RTE) aims at predicting the relation between a pair of sentences (premise and hypothesis) as entailment, contradiction or semantic independence. Although deep learning models have shown promising performance for NLI in recent years, they rely on large scale expensive human-annotated datasets. Semi-supervised learning (SSL) is a popular technique for reducing the reliance on human annotation by leveraging unlabeled data for training. However, despite its substantial success on single sentence classification tasks where the challenge in making use of unlabeled data is to assign “good enough” pseudo-labels, for NLI tasks, the nature of unlabeled data is more complex: one of the sentences in the pair (usually the hypothesis) along with the class label are missing from the data and require human annotations, which makes SSL for NLI more challenging. In this paper, we propose a novel way to incorporate unlabeled data in SSL for NLI where we use a conditional language model, BART to generate the hypotheses for the unlabeled sentences (used as premises). Our experiments show that our SSL framework successfully exploits unlabeled data and substantially improves the performance of four NLI datasets in low-resource settings. We release our code here: https://github.com/msadat3/SSL_for_NLI | Mobashir Sadat, Cornelia Caragea |  |
| 353 |  |  [Unsupervised Text Deidentification](https://doi.org/10.18653/v1/2022.findings-emnlp.352) |  | 0 | Deidentification seeks to anonymize textual data prior to distribution. Automatic deidentification primarily uses supervised named entity recognition from human-labeled data points. We propose an unsupervised deidentification method that masks words that leak personally-identifying information. The approach utilizes a specially trained reidentification model to identify individuals from redacted personal documents. Motivated by K-anonymity based privacy, we generate redactions that ensure a minimum reidentification rank for the correct profile of the document. To evaluate this approach, we consider the task of deidentifying Wikipedia Biographies, and evaluate using an adversarial reidentification metric. Compared to a set of unsupervised baselines, our approach deidentifies documents more completely while removing fewer words. Qualitatively, we see that the approach eliminates many identifying aspects that would fall outside of the common named entity based approach. | John X. Morris, Justin T. Chiu, Ramin Zabih, Alexander M. Rush |  |
| 354 |  |  [Federated Continual Learning for Text Classification via Selective Inter-client Transfer](https://doi.org/10.18653/v1/2022.findings-emnlp.353) |  | 0 | In this work, we combine the two paradigms: Federated Learning (FL) and Continual Learning (CL) for text classification task in cloud-edge continuum. The objective of Federated Continual Learning (FCL) is to improve deep learning models over life time at each client by (relevant and efficient) knowledge transfer without sharing data. Here, we address challenges in minimizing inter-client interference while knowledge sharing due to heterogeneous tasks across clients in FCL setup. In doing so, we propose a novel framework, Federated Selective Inter-client Transfer (FedSeIT) which selectively combines model parameters of foreign clients. To further maximize knowledge transfer, we assess domain overlap and select informative tasks from the sequence of historical tasks at each foreign client while preserving privacy. Evaluating against the baselines, we show improved performance, a gain of (average) 12.4% in text classification over a sequence of tasks using five datasets from diverse domains. To the best of our knowledge, this is the first work that applies FCL to NLP. | Yatin Chaudhary, Pranav Rai, Matthias Schubert, Hinrich Schütze, Pankaj Gupta |  |
| 355 |  |  [DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents](https://doi.org/10.18653/v1/2022.findings-emnlp.354) |  | 0 | In the real world, autonomous driving agents navigate in highly dynamic environments full of unexpected situations where pre-trained models are unreliable. In these situations, what is immediately available to vehicles is often only human operators. Empowering autonomous driving agents with the ability to navigate in a continuous and dynamic environment and to communicate with humans through sensorimotor-grounded dialogue becomes critical. To this end, we introduce Dialogue On the ROad To Handle Irregular Events (DOROTHIE), a novel interactive simulation platform that enables the creation of unexpected situations on the fly to support empirical studies on situated communication with autonomous driving agents. Based on this platform, we created the Situated Dialogue Navigation (SDN), a navigation benchmark of 183 trials with a total of 8415 utterances, around 18.7 hours of control streams, and 2.9 hours of trimmed audio. SDN is developed to evaluate the agent’s ability to predict dialogue moves from humans as well as generate its own dialogue moves and physical navigation actions. We further developed a transformer-based baseline model for these SDN tasks. Our empirical results indicate that language guided-navigation in a highly dynamic environment is an extremely difficult task for end-to-end models. These results will provide insight towards future work on robust autonomous driving agents | Ziqiao Ma, Benjamin VanDerPloeg, CristianPaul Bara, Yidong Huang, EuiIn Kim, Felix Gervits, Matthew Marge, Joyce Chai |  |
| 356 |  |  [He Said, She Said: Style Transfer for Shifting the Perspective of Dialogues](https://doi.org/10.18653/v1/2022.findings-emnlp.355) |  | 0 | In this work, we define a new style transfer task: perspective shift, which reframes a dialouge from informal first person to a formal third person rephrasing of the text. This task requires challenging coreference resolution, emotion attribution, and interpretation of informal text. We explore several baseline approaches and discuss further directions on this task when applied to short dialogues. As a sample application, we demonstrate that applying perspective shifting to a dialogue summarization dataset (SAMSum) substantially improves the zero-shot performance of extractive news summarization models on this data. Additionally, supervised extractive models perform better when trained on perspective shifted data than on the original dialogues. We release our code publicly. | Amanda Bertsch, Graham Neubig, Matthew R. Gormley |  |
| 357 |  |  [Dynamic Augmentation Data Selection for Few-shot Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.356) |  | 0 | Data augmentation has been a popular method for fine-tuning pre-trained language models to increase model robustness and performance. With augmentation data coming from modifying gold train data (in-sample augmentation) or being harvested from general domain unlabeled data (out-of-sample augmentation), the quality of such data is the key to successful fine-tuning. In this paper, we propose a dynamic data selection method to select effective augmentation data from different augmentation sources according to the model’s learning stage, by identifying a set of augmentation samples that optimally facilitates the learning process of the most current model. The method firstly filters out augmentation samples with noisy pseudo labels through a curriculum learning strategy, then estimates the effectiveness of reserved augmentation data by its influence scores on the current model at every update, allowing the data selection process tightly tailored to model parameters. And the two-stage augmentation strategy considers in-sample augmentation and out-of-sample augmentation in different learning stages. Experiments with both kinds of augmentation data on a variety of sentence classification tasks show that our method outperforms strong baselines, proving the effectiveness of our method. Analysis confirms the dynamic nature of the data effectiveness and the importance of model learning stages in utilization of augmentation data. | Guangliang Liu, Lifeng Jin, Owen Yuan, Jiayu Zhou |  |
| 358 |  |  [KPDROP: Improving Absent Keyphrase Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.357) |  | 0 | Keyphrase generation is the task of generating phrases (keyphrases) that summarize the main topics of a given document. Keyphrases can be either present or absent from the given document. While the extraction of present keyphrases has received much attention in the past, only recently a stronger focus has been placed on the generation of absent keyphrases. However, generating absent keyphrases is challenging; even the best methods show only a modest degree of success. In this paper, we propose a model-agnostic approach called keyphrase dropout (or KPDrop) to improve absent keyphrase generation. In this approach, we randomly drop present keyphrases from the document and turn them into artificial absent keyphrases during training. We test our approach extensively and show that it consistently improves the absent performance of strong baselines in both supervised and resource-constrained semi-supervised settings. | Jishnu Ray Chowdhury, Seoyeon Park, Tuhin Kundu, Cornelia Caragea |  |
| 359 |  |  [Natural Language Deduction through Search over Statement Compositions](https://doi.org/10.18653/v1/2022.findings-emnlp.358) |  | 0 | In settings from fact-checking to question answering, we frequently want to know whether a collection of evidence (premises) entails a hypothesis. Existing methods primarily focus on the end-to-end discriminative version of this task, but less work has treated the generative version in which a model searches over the space of statements entailed by the premises to constructively derive the hypothesis. We propose a system for doing this kind of deductive reasoning in natural language by decomposing the task into separate steps coordinated by a search procedure, producing a tree of intermediate conclusions that faithfully reflects the system’s reasoning process. Our experiments on the EntailmentBank dataset (Dalvi et al., 2021) demonstrate that the proposed system can successfully prove true statements while rejecting false ones. Moreover, it produces natural language explanations with a 17% absolute higher step validity than those produced by an end-to-end T5 model. | Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, Greg Durrett |  |
| 360 |  |  [EnDex: Evaluation of Dialogue Engagingness at Scale](https://doi.org/10.18653/v1/2022.findings-emnlp.359) |  | 0 | We propose EnDex, the first human-reaction based model to evaluate dialogue engagingness. EnDex is trained on 80k Reddit-based Engagement Dataset (RED) curated using a novel distant-supervision framework. Engagingness is a key measure that captures high-level quality of AI dialogue systems and closely reflects actual user experience. However, data shortage, plus the abstract and extensive definition of engagingness makes it challenging to develop an automatic metric. Our work departs from mainstream approaches that use synthetic negative examples to train binary classifiers, and instead, proposes a solution using distant-supervision from human-reaction feedback. To support the soundness of our EnDex metric, we offer a theoretical foundation for engagement, an extensive ablation study, and empirical evidence of high correlation on five engagingness related datasets. We will release code, off-the-shelf EnDex model, and a large-scale dataset upon paper publication to facilitate future research. | Guangxuan Xu, Ruibo Liu, Fabrice HarelCanada, Nischal Reddy Chandra, Nanyun Peng |  |
| 361 |  |  [LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.360) |  | 0 | Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets. | Dheeraj Mekala, Chengyu Dong, Jingbo Shang |  |
| 362 |  |  [Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models](https://doi.org/10.18653/v1/2022.findings-emnlp.361) |  | 0 | Model compression by way of parameter pruning, quantization, or distillation has recently gained popularity as an approach for reducing the computational requirements of modern deep neural network models for NLP. Inspired by prior works suggesting a connection between simpler, more generalizable models and those that lie within wider loss basins, we hypothesize that optimizing for flat minima should lead to simpler parameterizations and thus more compressible models. We propose to combine sharpness-aware minimization (SAM) with various task-specific model compression methods, including iterative magnitude pruning (IMP), structured pruning with a distillation objective, and post-training dynamic quantization. Empirically, we show that optimizing for flatter minima consistently leads to greater compressibility of parameters compared to vanilla Adam when fine-tuning BERT models, with little to no loss in accuracy on the GLUE text classification and SQuAD question answering benchmarks. Moreover, SAM finds superior winning tickets during IMP that 1) are amenable to vanilla Adam optimization, and 2) transfer more effectively across tasks. | Clara Na, Sanket Vaibhav Mehta, Emma Strubell |  |
| 363 |  |  [Structural Contrastive Representation Learning for Zero-shot Multi-label Text Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.362) |  | 0 | Zero-shot multi-label text classification (ZMTC) is a fundamental task in natural language processing with applications in the cold start problem of recommendation systems. Ideally, one would learn an expressive representation of both input text and label features so that ZMTC is transformed into a nearest neighbor search problem. However, the existing representation learning approaches for ZMTC struggle with accuracy as well as poor training efficiency. Firstly, the input text is structural, consisting of both short title sentences and long content paragraphs. It is challenging to model the correlation between short label descriptions and long structural input documents. Secondly, the enormous label space in ZMTC forces the existing approaches to perform multi-stage learning with label engineering. As a result, the training overhead is significant. In this paper, we address both problems by introducing an end-to-end structural contrastive representation learning approach. We propose a randomized text segmentation (RTS) technique to generate high-quality contrastive pairs. This RTS technique allows us to model title-content correlation. Additionally, we simplify the multi-stage ZMTC learning strategy by avoiding label engineering. Extensive experiments demonstrate that our approach leads to up to 2.33% improvement in precision@1 and 5.94x speedup in training time on publicly available datasets. Our code is available publicly. | Tianyi Zhang, Zhaozhuo Xu, Tharun Medini, Anshumali Shrivastava |  |
| 364 |  |  [Improving Generalization of Pre-trained Language Models via Stochastic Weight Averaging](https://doi.org/10.18653/v1/2022.findings-emnlp.363) |  | 0 | Knowledge Distillation (KD) is a commonly used technique for improving the generalization of compact Pre-trained Language Models (PLMs) on downstream tasks. However, such methods impose the additional burden of training a separate teacher model for every new dataset.Alternatively, one may directly work on the improvement of the optimization procedure of the compact model towards better generalization. Recent works observe that the flatness of the local minimum correlates well with better generalization.In this work, we adapt Stochastic Weight Averaging (SWA), a method encouraging convergence to a flatter minimum, to fine-tuning PLMs. We conduct extensive experiments on various NLP tasks (text classification, question answering, and generation) and different model architectures and demonstrate that our adaptation improves the generalization without extra computation cost. Moreover, we observe that this simple optimization technique is able to outperform the state-of-the-art KD methods for compact models. | Peng Lu, Ivan Kobyzev, Mehdi Rezagholizadeh, Ahmad Rashid, Ali Ghodsi, Philippe Langlais |  |
| 365 |  |  [Learn What Is Possible, Then Choose What Is Best: Disentangling One-To-Many Relations in Language Through Text-based Games](https://doi.org/10.18653/v1/2022.findings-emnlp.364) |  | 0 | Language models pre-trained on large self-supervised corpora, followed by task-specific fine-tuning has become the dominant paradigm in NLP. These pre-training datasets often have a one-to-many structure—e.g. in dialogue there are many valid responses for a given context. However, only some of these responses will be desirable in our downstream task. This raises the question of how we should train the model such that it can emulate the desirable behaviours, but not the undesirable ones. Current approaches train in a one-to-one setup—only a single target response is given for a single dialogue context—leading to models only learning to predict the average response, while ignoring the full range of possible responses. Using text-based games as a testbed, our approach, PASA, uses discrete latent variables to capture the range of different behaviours represented in our larger pre-training dataset. We then use knowledge distillation to distil the posterior probability distribution into a student model. This probability distribution is far richer than learning from only the hard targets of the dataset, and thus allows the student model to benefit from the richer range of actions the teacher model has learned. Results show up to 49% empirical improvement over the previous state-of-the-art model on the Jericho Walkthroughs dataset. | Benjamin Towle, Ke Zhou |  |
| 366 |  |  [Structurally Diverse Sampling for Sample-Efficient Training and Comprehensive Evaluation](https://doi.org/10.18653/v1/2022.findings-emnlp.365) |  | 0 | A growing body of research has demonstrated the inability of NLP models to generalize compositionally and has tried to alleviate it through specialized architectures, training schemes, and data augmentation, among other approaches. In this work, we study a different approach: training on instances with diverse structures. We propose a model-agnostic algorithm for subsampling such sets of instances from a labeled instance pool with structured outputs. Evaluating on both compositional template splits and traditional IID splits of 5 semantic parsing datasets of varying complexity, we show that structurally diverse training using our algorithm leads to comparable or better generalization than prior algorithms in 9 out of 10 dataset-split type pairs. In general, we find structural diversity to consistently improve sample efficiency compared to random train sets. Moreover, we show that structurally diverse sampling yields comprehensive test sets that are a lot more challenging than IID test sets. Finally, we provide two explanations for improved generalization from diverse train sets: 1) improved coverage of output substructures, and 2) a reduction in spurious correlations between these substructures. | Shivanshu Gupta, Sameer Singh, Matt Gardner |  |
| 367 |  |  [Unsupervised Multi-Granularity Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.366) |  | 0 | Text summarization is a user-preference based task, i.e., for one document, users often have different priorities for the summary. As a key aspect of customization in summarization, granularity is used to measure the semantic coverage between the summary and source document. However, developing systems that can generate summaries with customizable semantic coverage is still an under-explored topic. In this paper, we propose the first unsupervised multi-granularity summarization framework, GranuSum. We take events as the basic semantic units of the source documents and propose to rank these events by their salience. We also develop a model to summarize input documents with given events as anchors and hints. By inputting different numbers of events, GranuSum is capable of producing multi-granular summaries in an unsupervised manner. Meanwhile, we annotate a new benchmark GranuDUC that contains multiple summaries at different granularities for each document cluster. Experimental results confirm the substantial superiority of GranuSum on multi-granularity summarization over strong baselines. Furthermore, by exploiting the event information, GranuSum also exhibits state-of-the-art performance under the conventional unsupervised abstractive setting. | Ming Zhong, Yang Liu, Suyu Ge, Yuning Mao, Yizhu Jiao, Xingxing Zhang, Yichong Xu, Chenguang Zhu, Michael Zeng, Jiawei Han |  |
| 368 |  |  [HeLo: Learning-Free Lookahead Decoding for Conversation Infilling](https://doi.org/10.18653/v1/2022.findings-emnlp.367) |  | 0 | We propose Heuristic Guided Lookahead Decoding (HeLo), a novel decoding strategy for conversation infilling. Conversation infilling aims to generate a seamless bridge of utterances connecting a given pair of source and target utterances. HeLo does not require fine-tuning or extra models – only the generating model itself. Instead, HeLo leverages a greedy lookahead phase before committing to any token. The HeLo framework is simple and can augment conventional decoding strategies paired with any autoregressive language model. Smooth transitions between utterances are encouraged with an annealing schedule. Our experiments show HeLo outperforms several baselines when evaluated with both automatic and human evaluation metrics, which, we argue, are appropriate for the task. | Ivan Lee, Taylor BergKirkpatrick |  |
| 369 |  |  [Invernet: An Inversion Attack Framework to Infer Fine-Tuning Datasets through Word Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.368) |  | 0 | Word embedding aims to learn the dense representation of words and has become a regular input preparation in many NLP tasks. Due to the data and computation intensive nature of learning embeddings from scratch, a more affordable way is to borrow the pretrained embedding available in public and fine-tune the embedding through a domain specific downstream dataset. A privacy concern can arise if a malicious owner of the pretrained embedding gets access to the fine-tuned embedding and tries to infer the critical information from the downstream datasets. In this study, we propose a novel embedding inversion framework called Invernet that materializes the privacy concern by inferring the context distribution in the downstream dataset, which can lead to key information breach. With extensive experimental studies on two real-world news datasets: Antonio Gulli’s News and New York Times, we validate the feasibility of proposed privacy attack and demonstrate the effectiveness of Invernet on inferring downstream datasets based on multiple word embedding methods. | Ishrak Hayet, Zijun Yao, Bo Luo |  |
| 370 |  |  [LawngNLI: A Long-Premise Benchmark for In-Domain Generalization from Short to Long Contexts and for Implication-Based Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.369) |  | 0 | Natural language inference has trended toward studying contexts beyond the sentence level. An important application area is law: past cases often do not foretell how they apply to new situations and implications must be inferred. This paper introduces LawngNLI, constructed from U.S. legal opinions with automatic labels with high human-validated accuracy. Premises are long and multigranular. Experiments show two use cases. First, LawngNLI can benchmark for in-domain generalization from short to long contexts. It has remained unclear if large-scale long-premise NLI datasets actually need to be constructed: near-top performance on long premises could be achievable by fine-tuning using short premises. Without multigranularity, benchmarks cannot distinguish lack of fine-tuning on long premises versus domain shift between short and long datasets. In contrast, our long and short premises share the same examples and domain. Models fine-tuned using several past NLI datasets and/or our short premises fall short of top performance on our long premises. So for at least certain domains (such as ours), large-scale long-premise datasets are needed. Second, LawngNLI can benchmark for implication-based retrieval. Queries are entailed or contradicted by target documents, allowing users to move between arguments and evidence. Leading retrieval models perform reasonably zero shot on a LawngNLI-derived retrieval task. We compare different systems for re-ranking, including lexical overlap and cross-encoders fine-tuned using a modified LawngNLI or past NLI datasets. LawngNLI can train and test systems for implication-based case retrieval and argumentation. | William Bruno, Dan Roth |  |
| 371 |  |  [Distillation-Resistant Watermarking for Model Protection in NLP](https://doi.org/10.18653/v1/2022.findings-emnlp.370) |  | 0 | How can we protect the intellectual property of trained NLP models? Modern NLP models are prone to stealing by querying and distilling from their publicly exposed APIs. However, existing protection methods such as watermarking only work for images but are not applicable to text. We propose Distillation-Resistant Watermarking (DRW), a novel technique to protect NLP models from being stolen via distillation. DRW protects a model by injecting watermarks into the victim’s prediction probability corresponding to a secret key and is able to detect such a key by probing a suspect model. We prove that a protected model still retains the original accuracy within a certain bound. We evaluate DRW on a diverse set of NLP tasks including text classification, part-of-speech tagging, and named entity recognition. Experiments show that DRW protects the original model and detects stealing suspects at 100% mean average precision for all four tasks while the prior method fails on two. | Xuandong Zhao, Lei Li, YuXiang Wang |  |
| 372 |  |  [NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation](https://doi.org/10.18653/v1/2022.findings-emnlp.371) |  | 0 | While counterfactual data augmentation offers a promising step towards robust generalization in natural language processing, producing a set of counterfactuals that offer valuable inductive bias for models remains a challenge. Most existing approaches for producing counterfactuals, manual or automated, rely on small perturbations via minimal edits, resulting in simplistic changes. We introduce NeuroCounterfactuals, designed as loose counterfactuals, allowing for larger edits which result in naturalistic generations containing linguistic diversity, while still bearing similarity to the original document. Our novel generative approach bridges the benefits of constrained decoding, with those of language model adaptation for sentiment steering. Training data augmentation with our generations results in both in-domain and out-of-domain improvements for sentiment classification, outperforming even manually curated counterfactuals, under select settings. We further present detailed analyses to show the advantages of NeuroCounterfactuals over approaches involving simple, minimal edits. | Phillip Howard, Gadi Singer, Vasudev Lal, Yejin Choi, Swabha Swayamdipta |  |
| 373 |  |  [Don't Just Clean It, Proxy Clean It: Mitigating Bias by Proxy in Pre-Trained Models](https://doi.org/10.18653/v1/2022.findings-emnlp.372) |  | 0 | Transformer-based pre-trained models are known to encode societal biases not only in their contextual representations, but also in downstream predictions when fine-tuned on task-specific data.We present D-Bias, an approach that selectively eliminates stereotypical associations (e.g, co-occurrence statistics) at fine-tuning, such that the model doesn’t learn to excessively rely on those signals.D-Bias attenuates biases from both identity words and frequently co-occurring proxies, which we select using pointwise mutual information.We apply D-Bias to a) occupation classification, and b) toxicity classification and find that our approach substantially reduces downstream biases (e.g. by > 60% in toxicity classification, for identities that are most frequently flagged as toxic on online platforms).In addition, we show that D-Bias dramatically improves upon scrubbing, i.e., removing only the identity words in question.We also demonstrate that D-Bias easily extends to multiple identities, and achieves competitive performance with two recently proposed debiasing approaches: R-LACE and INLP. | Swetasudha Panda, Ari Kobren, Michael L. Wick, Qinlan Shen |  |
| 374 |  |  [The Undesirable Dependence on Frequency of Gender Bias Metrics Based on Word Embeddings](https://doi.org/10.18653/v1/2022.findings-emnlp.373) |  | 0 | Numerous works use word embedding-based metrics to quantify societal biases and stereotypes in texts. Recent studies have found that word embeddings can capture semantic similarity but may be affected by word frequency. In this work we study the effect of frequency when measuring female vs. male gender bias with word embedding-based bias quantification methods. We find that Skip-gram with negative sampling and GloVe tend to detect male bias in high frequency words, while GloVe tends to return female bias in low frequency words. We show these behaviors still exist when words are randomly shuffled. This proves that the frequency-based effect observed in unshuffled corpora stems from properties of the metric rather than from word associations. The effect is spurious and problematic since bias metrics should depend exclusively on word co-occurrences and not individual word frequencies. Finally, we compare these results with the ones obtained with an alternative metric based on Pointwise Mutual Information. We find that this metric does not show a clear dependence on frequency, even though it is slightly skewed towards male bias across all frequencies. | Francisco Valentini, Germán Rosati, Diego Fernández Slezak, Edgar Altszyler |  |
| 375 |  |  [BioNLI: Generating a Biomedical NLI Dataset Using Lexico-semantic Constraints for Adversarial Examples](https://doi.org/10.18653/v1/2022.findings-emnlp.374) |  | 0 | Natural language inference (NLI) is critical in many domains requiring complex decision-making, such as the biomedical domain. We introduce a novel semi-supervised procedure that bootstraps biomedical NLI datasets from positive entailment examples present in abstracts of biomedical publications. We focus on challenging texts where the hypothesis includes mechanistic information such as biochemical interactions between two entities. A key contribution of this work is automating the creation of negative examples that are informative without being simplistic. We generate a range of negative examples using nine strategies that manipulate the structure of the underlying mechanisms both with rules, e.g., flip the roles of the entities in the interaction, and, more importantly, by imposing the perturbed conditions as logical constraints in a neuro-logical decoding system (CITATION).We use this procedure to create a novel dataset for NLI in the biomedical domain, called . The accuracy of neural classifiers on this dataset is in the mid 70s F1, which indicates that this NLI task remains to be solved. Critically, we observe that the performance on the different classes of negative examples varies widely, from 97% F1 on the simple negative examples that change the role of the entities in the hypothesis, to barely better than chance on the negative examples generated using neuro-logic decoding. | Mohaddeseh Bastan, Mihai Surdeanu, Niranjan Balasubramanian |  |
| 376 |  |  [Self-supervised Cross-modal Pretraining for Speech Emotion Recognition and Sentiment Analysis](https://doi.org/10.18653/v1/2022.findings-emnlp.375) |  | 0 | Multimodal speech emotion recognition (SER) and sentiment analysis (SA) are important techniques for human-computer interaction. Most existing multimodal approaches utilize either shallow cross-modal fusion of pretrained features, or deep cross-modal fusion with raw features. Recently, attempts have been made to fuse pretrained feature representations in a deep fusion manner during fine-tuning stage. However those approaches have not led to improved results, partially due to their relatively simple fusion mechanisms and lack of proper cross-modal pretraining. In this work, leveraging single-modal pretrained models (RoBERTa and HuBERT), we propose a novel deeply-fused audio-text bi-modal transformer with carefully designed cross-modal fusion mechanism and a stage-wise cross-modal pretraining scheme to fully facilitate the cross-modal learning. Our experiment results show that the proposed method achieves state-of-the-art results on the public IEMOCAP emotion and CMU-MOSEI sentiment datasets, exceeding the previous benchmarks by a large margin. | IekHeng Chu, Ziyi Chen, Xinlu Yu, Mei Han, Jing Xiao, Peng Chang |  |
| 377 |  |  [Multimodal Conversation Modelling for Topic Derailment Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.376) |  | 0 | Conversations on social media tend to go off-topic and turn into different and sometimes toxic exchanges. Previous work focuses on analysing textual dialogues that have derailed into toxic content, but the range of derailment types is much broader, including spam or bot content, tangential comments, etc. In addition, existing work disregards conversations that involve visual information (i.e. images or videos), which are prevalent on most platforms. In this paper, we take a broader view of conversation derailment and propose a new challenge: detecting derailment based on the “change of conversation topic”, where the topic is defined by an initial post containing both a text and an image. For that, we (i) create the first Multimodal Conversation Derailment (MCD) dataset, and (ii) introduce a new multimodal conversational architecture (MMConv) that utilises visual and conversational contexts to classify comments for derailment. Experiments show that MMConv substantially outperforms previous text-based approaches to detect conversation derailment, as well as general multimodal classifiers. MMConv is also more robust to textual noise, since it relies on richer contextual information. | Zhenhao Li, Marek Rei, Lucia Specia |  |
| 378 |  |  [Active Learning for Abstractive Text Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.377) |  | 0 | Construction of human-curated annotated datasets for abstractive text summarization (ATS) is very time-consuming and expensive because creating each instance requires a human annotator to read a long document and compose a shorter summary that would preserve the key information relayed by the original document. Active Learning (AL) is a technique developed to reduce the amount of annotation required to achieve a certain level of machine learning model performance. In information extraction and text classification, AL can reduce the amount of labor up to multiple times. Despite its potential for aiding expensive annotation, as far as we know, there were no effective AL query strategies for ATS. This stems from the fact that many AL strategies rely on uncertainty estimation, while as we show in our work, uncertain instances are usually noisy, and selecting them can degrade the model performance compared to passive annotation. We address this problem by proposing the first effective query strategy for AL in ATS based on diversity principles. We show that given a certain annotation budget, using our strategy in AL annotation helps to improve the model performance in terms of ROUGE and consistency scores. Additionally, we analyze the effect of self-learning and show that it can additionally increase the performance of the model. | Akim Tsvigun, Ivan Lysenko, Danila Sedashov, Ivan Lazichny, Eldar Damirov, Vladimir Karlov, Artemy Belousov, Leonid Sanochkin, Maxim Panov, Alexander Panchenko, Mikhail Burtsev, Artem Shelmanov |  |
| 379 |  |  [Finding Memo: Extractive Memorization in Constrained Sequence Generation Tasks](https://doi.org/10.18653/v1/2022.findings-emnlp.378) |  | 0 | Memorization presents a challenge for several constrained Natural Language Generation (NLG) tasks such as Neural Machine Translation (NMT), wherein the proclivity of neural models to memorize noisy and atypical samples reacts adversely with the noisy (web crawled) datasets. However, previous studies of memorization in constrained NLG tasks have only focused on counterfactual memorization, linking it to the problem of hallucinations. In this work, we propose a new, inexpensive algorithm for extractive memorization (exact training data generation under insufficient context) in constrained sequence generation tasks and use it to study extractive memorization and its effects in NMT. We demonstrate that extractive memorization poses a serious threat to NMT reliability by qualitatively and quantitatively characterizing the memorized samples as well as the model behavior in their vicinity. Based on empirical observations, we develop a simple algorithm which elicits non-memorized translations of memorized samples from the same model, for a large fraction of such samples. Finally, we show that the proposed algorithm could also be leveraged to mitigate memorization in the model through finetuning. We have released the code to reproduce our results at https://github.com/vyraun/Finding-Memo. | Vikas Raunak, Arul Menezes |  |
| 380 |  |  [SALTED: A Framework for SAlient Long-tail Translation Error Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.379) |  | 0 | Traditional machine translation (MT) metrics provide an average measure of translation quality that is insensitive to the long tail of behavioral problems. Examples include translation of numbers, physical units, dropped content and hallucinations. These errors, which occur rarely and unpredictably in Neural Machine Translation (NMT), greatly undermine the reliability of state-of-the-art MT systems. Consequently, it is important to have visibility into these problems during model development.Towards this end, we introduce SALTED, a specifications-based framework for behavioral testing of NMT models. At the core of our approach is the use of high-precision detectors that flag errors (or alternatively, verify output correctness) between a source sentence and a system output. These detectors provide fine-grained measurements of long-tail errors, providing a trustworthy view of problems that were previously invisible. We demonstrate that such detectors could be used not just to identify salient long-tail errors in MT systems, but also for higher-recall filtering of the training data, fixing targeted errors with model fine-tuning in NMT and generating novel data for metamorphic testing to elicit further bugs in models. | Vikas Raunak, Matt Post, Arul Menezes |  |
| 381 |  |  [Discord Questions: A Computational Approach To Diversity Analysis in News Coverage](https://doi.org/10.18653/v1/2022.findings-emnlp.380) |  | 0 | There are many potential benefits to news readers accessing diverse sources. Modern news aggregators do the hard work of organizing the news, offering readers a plethora of source options, but choosing which source to read remains challenging.We propose a new framework to assist readers in identifying source differences and gaining an understanding of news coverage diversity.The framework is based on the generation of Discord Questions: questions with a diverse answer pool, explicitly illustrating source differences.To assemble a prototype of the framework, we focus on two components: (1) discord question generation, the task of generating questions answered differently by sources, for which we propose an automatic scoring method, and create a model that improves performance from current question generation (QG) methods by 5%, (2) answer consolidation, the task of grouping answers to a question that are semantically similar, for which we collect data and repurpose a method that achieves 81% balanced accuracy on our realistic test set.We illustrate the framework’s feasibility through a prototype interface. Even though model performance at discord QG still lags human performance by more than 15%, generated questions are judged to be more interesting than factoid questions and can reveal differences in the level of detail, sentiment, and reasoning of sources in news coverage. Code is available at https://github.com/Salesforce/discord_questions. | Philippe Laban, ChienSheng Wu, Lidiya Murakhovs'ka, Xiang 'Anthony' Chen, Caiming Xiong |  |
| 382 |  |  [FocusQA: Open-Domain Question Answering with a Context in Focus](https://doi.org/10.18653/v1/2022.findings-emnlp.381) |  | 0 | We introduce question answering with a cotext in focus, a task that simulates a free interaction with a QA system. The user reads on a screen some information about a topic, and they can follow-up with questions that can be either related or not to the topic; and the answer can be found in the document containing the screen content or from other pages. We call such information context. To study the task, we construct FocusQA, a dataset for answer sentence selection (AS2) with 12,165011unique question/context pairs, and a total of 109,940 answers. To build the dataset, we developed a novel methodology that takes existing questions and pairs them with relevant contexts. To show the benefits of this approach, we present a comparative analysis with a set of questions written by humans after reading the context, showing that our approach greatly helps in eliciting more realistic question/context pairs. Finally, we show that the task poses several challenges for incorporating contextual information. In this respect, we introduce strong baselines for answer sentence selection that outperform the precision of state-of-the-art models for AS2 up to 21.3% absolute points. | Gianni Barlacchi, Ivano Lauriola, Alessandro Moschitti, Marco Del Tredici, Xiaoyu Shen, Thuy Vu, Bill Byrne, Adrià de Gispert |  |
| 383 |  |  [Challenges and Opportunities in Information Manipulation Detection: An Examination of Wartime Russian Media](https://doi.org/10.18653/v1/2022.findings-emnlp.382) |  | 0 | NLP research on public opinion manipulation campaigns has primarily focused on detecting overt strategies such as fake news and disinformation. However, information manipulation in the ongoing Russia-Ukraine war exemplifies how governments and media also employ more nuanced strategies. We release a new dataset, VoynaSlov, containing 38M+ posts from Russian media outlets on Twitter and VKontakte, as well as public activity and responses, immediately preceding and during the 2022 Russia-Ukraine war. We apply standard and recently-developed NLP models on VoynaSlov to examine agenda setting, framing, and priming, several strategies underlying information manipulation, and reveal variation across media outlet control, social media platform, and time. Our examination of these media effects and extensive discussion of current approaches’ limitations encourage further development of NLP models for understanding information manipulation in emerging crises, as well as other real-world and interdisciplinary tasks. | Chan Young Park, Julia Mendelsohn, Anjalie Field, Yulia Tsvetkov |  |
| 384 |  |  [Disentangling Task Relations for Few-shot Text Classification via Self-Supervised Hierarchical Task Clustering](https://doi.org/10.18653/v1/2022.findings-emnlp.383) |  | 0 | Few-Shot Text Classification (FSTC) imitates humans to learn a new text classifier efficiently with only few examples, by leveraging prior knowledge from historical tasks. However, most prior works assume that all the tasks are sampled from a single data source, which cannot adapt to real-world scenarios where tasks are heterogeneous and lie in different distributions. As such, existing methods may suffer from their globally knowledge-shared mechanisms to handle the task heterogeneity. On the other hand, inherent task relationships are not explicitly captured, making task knowledge unorganized and hard to transfer to new tasks. Thus, we explore a new FSTC setting where tasks can come from a diverse range of data sources. To address the task heterogeneity, we propose a self-supervised hierarchical task clustering (SS-HTC) method. SS-HTC not only customizes the cluster-specific knowledge by dynamically organizing heterogeneous tasks into different clusters in hierarchical levels but also disentangles the underlying relations between tasks to improve the interpretability. Empirically, extensive experiments on five public FSTC benchmark datasets demonstrate the effectiveness of SS-HTC. | Juan Zha, Zheng Li, Ying Wei, Yu Zhang |  |
| 385 |  |  [XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing](https://doi.org/10.18653/v1/2022.findings-emnlp.384) |  | 0 | In-context learning using large language models has recently shown surprising results for semantic parsing tasks such as Text-to-SQL translation.Prompting GPT-3 or Codex using several examples of question-SQL pairs can produce excellent results, comparable to state-of-the-art finetuning-based models.However, existing work primarily focuses on English datasets, and it is unknown whether large language models can serve as competitive semantic parsers for other languages.To bridge this gap, our work focuses on cross-lingual Text-to-SQL semantic parsing for translating non-English utterances into SQL queries based on an English schema.We consider a zero-shot transfer learning setting with the assumption that we do not have any labeled examples in the target language (but have annotated examples in English).This work introduces the XRICL framework, which learns to retrieve relevant English exemplars for a given query to construct prompts.We also include global translation exemplars for a target language to facilitate the translation process for large language models.To systematically evaluate our model, we construct two new benchmark datasets, XSpider and XKaggle-dbqa, which include questions in Chinese, Vietnamese, Farsi, and Hindi.Our experiments show that XRICL effectively leverages large pre-trained language models to outperform existing baselines.Data and code are publicly available at https://github.com/Impavidity/XRICL. | Peng Shi, Rui Zhang, He Bai, Jimmy Lin |  |
| 386 |  |  [Continuation KD: Improved Knowledge Distillation through the Lens of Continuation Optimization](https://doi.org/10.18653/v1/2022.findings-emnlp.385) |  | 0 | Knowledge Distillation (KD) has been extensively used for natural language understanding (NLU) tasks to improve a small model’s (a student) generalization by transferring the knowledge from a larger model (a teacher). Although KD methods achieve state-of-the-art performance in numerous settings, they suffer from several problems limiting their performance. It is shown in the literature that the capacity gap between the teacher and the student networks can make KD ineffective. Additionally, existing KD techniques do not mitigate the noise in the teacher’s output: modeling the noisy behaviour of the teacher can distract the student from learning more useful features. We propose a new KD method that addresses these problems and facilitates the training compared to previous techniques. Inspired by continuation optimization, we design a training procedure that optimizes the highly non-convex KD objective by starting with the smoothed version of this objective and making it more complex as the training proceeds. Our method (Continuation-KD) achieves state-of-the-art performance across various compact architectures on NLU (GLUE benchmark) and computer vision tasks (CIFAR-10 and CIFAR-100). | Aref Jafari, Ivan Kobyzev, Mehdi Rezagholizadeh, Pascal Poupart, Ali Ghodsi |  |
| 387 |  |  [Detecting Dementia from Long Neuropsychological Interviews](https://doi.org/10.18653/v1/2022.findings-emnlp.386) |  | 0 | Neuropsychological exams are commonly used to diagnose various kinds of cognitive impairment. They typically involve a trained examiner who conducts a series of cognitive tests with a subject. In recent years, there has been growing interest in developing machine learning methods to extract speech and language biomarkers from exam recordings to provide automated input for cognitive assessment. Inspired by recent findings suggesting that the examiner’s language can influence cognitive impairment classifications, in this paper, we study the influence of the examiner on automatic dementia identification decisions in real-world neuropsychological exams. To mitigate the influence of the examiner, we propose a systematic three-stage pipeline for detecting dementia from exam recordings. In the first stage, we perform audio-based speaker diarization (i.e., estimating who spoke when?) by incorporating speaker discriminative features. In the second stage, we employ text-based language models to identify the role of the speaker (i.e., examiner or subject). Finally, in the third stage, we employ text- and audio-based models to detect cognitive impairment from hypothesized subject segments. Our studies suggest that incorporating audio-based diarization followed by text-based role identification helps mitigate the influences from the examiner’s segments. Further, we found that the text and audio modalities complement each other, and the performance improves when we use both modalities. We also perform several carefully designed experimental studies to assess the performance of each stage. | Nauman Dawalatabad, Yuan Gong, Sameer Khurana, Rhoda Au, James R. Glass |  |
| 388 |  |  [Sarcasm Detection is Way Too Easy! An Empirical Comparison of Human and Machine Sarcasm Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.387) |  | 0 | Recently, author-annotated sarcasm datasets, which focus on intended, rather than perceived sarcasm, have been introduced. Although datasets collected using first-party annotation have important benefits, there is no comparison of human and machine performance on these new datasets. In this paper, we collect new annotations to provide human-level benchmarks for these first-party annotated sarcasm tasks in both English and Arabic, and compare the performance of human annotators to that of state-of-the-art sarcasm detection systems. Our analysis confirms that sarcasm detection is extremely challenging, with individual humans performing close to or slightly worse than the best trained models. With majority voting, however, humans are able to achieve the best results on all tasks. We also perform error analysis, finding that some of the most challenging examples are those that require additional context. We also highlight common features and patterns used to express sarcasm in English and Arabic such as idioms and proverbs. We suggest that to better capture sarcasm, future sarcasm detection datasets and models should focus on representing conversational and cultural context while leveraging world knowledge and common sense. | Ibrahim Abu Farha, Steven R. Wilson, Silviu Oprea, Walid Magdy |  |
| 389 |  |  [Cross-lingual Text-to-SQL Semantic Parsing with Representation Mixup](https://doi.org/10.18653/v1/2022.findings-emnlp.388) |  | 0 | We focus on the cross-lingual Text-to-SQL semantic parsing task,where the parsers are expected to generate SQL for non-English utterances based on English database schemas.Intuitively, English translation as side information is an effective way to bridge the language gap,but noise introduced by the translation system may affect parser effectiveness.In this work, we propose a Representation Mixup Framework (Rex) for effectively exploiting translations in the cross-lingual Text-to-SQL task.Particularly, it uses a general encoding layer, a transition layer, and a target-centric layer to properly guide the information flow of the English translation.Experimental results on CSpider and VSpider show that our framework can benefit from cross-lingual training and improve the effectiveness of semantic parsers, achieving state-of-the-art performance. | Peng Shi, Linfeng Song, Lifeng Jin, Haitao Mi, He Bai, Jimmy Lin, Dong Yu |  |
| 390 |  |  [JamPatoisNLI: A Jamaican Patois Natural Language Inference Dataset](https://doi.org/10.18653/v1/2022.findings-emnlp.389) |  | 0 | JamPatoisNLI provides the first dataset for natural language inference in a creole language, Jamaican Patois.Many of the most-spoken low-resource languages are creoles. These languages commonly have a lexicon derived from a major world language and a distinctive grammar reflecting the languages of the original speakers and the process of language birth by creolization. This gives them a distinctive place in exploring the effectiveness of transfer from large monolingual or multilingual pretrained models. While our work, along with previous work, shows that transfer from these models to low-resource languages that are unrelated to languages in their training set is not very effective, we would expect stronger results from transfer to creoles. Indeed, our experiments show considerably better results from few-shot learning of JamPatoisNLI than for such unrelated languages, and help us begin to understand how the unique relationship between creoles and their high-resource base languages affect cross-lingual transfer. JamPatoisNLI, which consists of naturally-occurring premises and expert-written hypotheses, is a step towards steering research into a traditionally underserved language and a useful benchmark for understanding cross-lingual NLP. | RuthAnn Armstrong, John Hewitt, Christopher D. Manning |  |
| 391 |  |  [Are Neural Topic Models Broken?](https://doi.org/10.18653/v1/2022.findings-emnlp.390) |  | 0 | Recently, the relationship between automated and human evaluation of topic models has been called into question. Method developers have staked the efficacy of new topic model variants on automated measures, and their failure to approximate human preferences places these models on uncertain ground. Moreover, existing evaluation paradigms are often divorced from real-world use.Motivated by content analysis as a dominant real-world use case for topic modeling, we analyze two related aspects of topic models that affect their effectiveness and trustworthiness in practice for that purpose: the stability of their estimates and the extent to which the model’s discovered categories align with human-determined categories in the data. We find that neural topic models fare worse in both respects compared to an established classical method. We take a step toward addressing both issues in tandem by demonstrating that a straightforward ensembling method can reliably outperform the members of the ensemble. | Alexander Miserlis Hoyle, Rupak Sarkar, Pranav Goel, Philip Resnik |  |
| 392 |  |  [Know Thy Strengths: Comprehensive Dialogue State Tracking Diagnostics](https://doi.org/10.18653/v1/2022.findings-emnlp.391) |  | 0 | Recent works that revealed the vulnerability of dialogue state tracking (DST) models to distributional shifts have made holistic comparisons on robustness and qualitative analyses increasingly important for understanding their relative performance. We present our findings from standardized and comprehensive DST diagnoses, which have previously been sparse and uncoordinated, using our toolkit, CheckDST, a collection of robustness tests and failure mode analytics. We discover that different classes of DST models have clear strengths and weaknesses, where generation models are more promising for handling language variety while span-based classification models are more robust to unseen entities. Prompted by this discovery, we also compare checkpoints from the same model and find that the standard practice of selecting checkpoints using validation loss/accuracy is prone to overfitting and each model class has distinct patterns of failure. Lastly, we demonstrate how our diagnoses motivate a pre-finetuning procedure with non-dialogue data that offers comprehensive improvements to generation models by alleviating the impact of distributional shifts through transfer learning. | Hyundong Cho, Chinnadhurai Sankar, Christopher Lin, Kaushik Ram Sadagopan, Shahin Shayandeh, Asli Celikyilmaz, Jonathan May, Ahmad Beirami |  |
| 393 |  |  [Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge](https://doi.org/10.18653/v1/2022.findings-emnlp.392) |  | 0 | We propose a novel open-domain question answering (ODQA) framework for answering single/multi-hop questions across heterogeneous knowledge sources.The key novelty of our method is the introduction of the intermediary modules into the current retriever-reader pipeline.Unlike previous methods that solely rely on the retriever for gathering all evidence in isolation,our intermediary performs a chain of reasoning over the retrieved set.Specifically, our method links the retrieved evidence with its related global context into graphs and organizes them into a candidate list of evidence chains.Built upon pretrained language models, our system achieves competitive performance on two ODQA datasets, OTT-QA and NQ, against tables and passages from Wikipedia.In particular, our model substantially outperforms the previous state-of-the-art on OTT-QA with an exact match score of 47.3 (45% relative gain). | Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg, Jianfeng Gao |  |
| 394 |  |  [Detecting Languages Unintelligible to Multilingual Models through Local Structure Probes](https://doi.org/10.18653/v1/2022.findings-emnlp.393) |  | 0 | Providing better language tools for low-resource and endangered languages is imperative for equitable growth.Recent progress with massively multilingual pretrained models has proven surprisingly effective at performing zero-shot transfer to a wide variety of languages.However, this transfer is not universal, with many languages not currently understood by multilingual approaches.It is estimated that only 72 languages possess a “small set of labeled datasets” on which we could test a model’s performance, the vast majority of languages not having the resources available to simply evaluate performances on.In this work, we attempt to clarify which languages do and do not currently benefit from such transfer.To that end, we develop a general approach that requires only unlabelled text to detect which languages are not well understood by a cross-lingual model.Our approach is derived from the hypothesis that if a model’s understanding is insensitive to perturbations to text in a language, it is likely to have a limited understanding of that language.We construct a cross-lingual sentence similarity task to evaluate our approach empirically on 350, primarily low-resource, languages. | Louis Clouâtre, Prasanna Parthasarathi, Amal Zouaq, Sarath Chandar |  |
| 395 |  |  [Cards Against AI: Predicting Humor in a Fill-in-the-blank Party Game](https://doi.org/10.18653/v1/2022.findings-emnlp.394) |  | 0 | Humor is an inherently social phenomenon, with humorous utterances shaped by what is socially and culturally accepted. Understanding humor is an important NLP challenge, with many applications to human-computer interactions. In this work we explore humor in the context of Cards Against Humanity – a party game where players complete fill-in-the-blank statements using cards that can be offensive or politically incorrect.We introduce a novel dataset of 300,000 online games of Cards Against Humanity, including 785K unique jokes, analyze it and provide insights. We trained machine learning models to predict the winning joke per game, achieving performance twice as good (20%) as random, even without any user information.On the more difficult task of judging novel cards, we see the models’ ability to generalize is moderate. Interestingly, we find that our models are primarily focused on punchline card, with the context having little impact.Analyzing feature importance, we observe that short, crude, juvenile punchlines tend to win. | Dan Ofer, Dafna Shahaf |  |
| 396 |  |  [Open-Vocabulary Argument Role Prediction For Event Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.395) |  | 0 | The argument role in event extraction refers to the relation between an event and an argument participating in it. Despite the great progress in event extraction, existing studies still depend on roles pre-defined by domain experts. These studies expose obvious weakness when extending to emerging event types or new domains without available roles. Therefore, more attention and effort needs to be devoted to automatically customizing argument roles. In this paper, we define this essential but under-explored task: open-vocabulary argument role prediction. The goal of this task is to infer a set of argument roles for a given event type. We propose a novel unsupervised framework, RolePred for this task. Specifically, we formulate the role prediction problem as an in-filling task and construct prompts for a pre-trained language model to generate candidate roles. By extracting and analyzing the candidate arguments, the event-specific roles are further merged and selected. To standardize the research of this task, we collect a new human-annotated event extraction dataset including 143 customized argument roles with rich semantics. On this dataset, RolePred outperforms the existing methods by a large margin. | Yizhu Jiao, Sha Li, Yiqing Xie, Ming Zhong, Heng Ji, Jiawei Han |  |
| 397 |  |  [Token-level Sequence Labeling for Spoken Language Understanding using Compositional End-to-End Models](https://doi.org/10.18653/v1/2022.findings-emnlp.396) |  | 0 | End-to-end spoken language understanding (SLU) systems are gaining popularity over cascaded approaches due to their simplicity and ability to avoid error propagation. However, these systems model sequence labeling as a sequence prediction task causing a divergence from its well-established token-level tagging formulation. We build compositional end-to-end SLU systems that explicitly separate the added complexity of recognizing spoken mentions in SLU from the NLU task of sequence labeling. By relying on intermediate decoders trained for ASR, our end-to-end systems transform the input modality from speech to token-level representations that can be used in the traditional sequence labeling framework. This composition of ASR and NLU formulations in our end-to-end SLU system offers direct compatibility with pre-trained ASR and NLU systems, allows performance monitoring of individual components and enables the use of globally normalized losses like CRF, making them attractive in practical scenarios. Our models outperform both cascaded and direct end-to-end models on a labeling task of named entity recognition across SLU benchmarks. | Siddhant Arora, Siddharth Dalmia, Brian Yan, Florian Metze, Alan W. Black, Shinji Watanabe |  |
| 398 |  |  [Baked-in State Probing](https://doi.org/10.18653/v1/2022.findings-emnlp.397) |  | 0 | Neural language models have been analyzed for their linguistic and extra-linguistic knowledge via probing. Of particular interest has been the following question: how much can a language model trained only on form learn about meaning? Recent work has demonstrated via probing classifiers that in the setting of simple procedural text, where by “meaning” we mean the underlying world state, language models have a non-trivial performance on world state tracking. However, our proposed evaluation based on model predictions shows differing results, suggesting that these models are either not capturing the world state or not using it. How do these results change if the model has access to the world state? We explore this alternate setting with access to the underlying world state only during training and investigate ways of “baking in” the state knowledge along with the primary task of language modeling. Our proposed approaches allow for state probing during inference simply via text prompts, avoiding any probing classifier machinery. In terms of performance, we show that baking in the state knowledge during training leads to significant improvements in state tracking performance and text generation quality, | Shubham Toshniwal, Sam Wiseman, Karen Livescu, Kevin Gimpel |  |
| 399 |  |  [ClinicalT5: A Generative Language Model for Clinical Text](https://doi.org/10.18653/v1/2022.findings-emnlp.398) |  | 0 | In the past few years, large pre-trained language models (PLMs) have been widely adopted in different areas and have made fundamental improvements over a variety of downstream tasks in natural language processing (NLP). Meanwhile, domain-specific variants of PLMs are being proposed to address the needs of domains that demonstrate a specific pattern of writing and vocabulary, e.g., BioBERT for the biomedical domain and ClinicalBERT for the clinical domain. Recently, generative language models like BART and T5 are gaining popularity with their competitive performance on text generation as well as on tasks cast as generative problems. However, in the clinical domain, such domain-specific generative variants are still underexplored. To address this need, our work introduces a T5-based text-to-text transformer model pre-trained on clinical text, i.e., ClinicalT5. We evaluate the proposed model both intrinsically and extrinsically over a diverse set of tasks across multiple datasets, and show that ClinicalT5 dramatically outperforms T5 in the domain-specific tasks and compares favorably with its close baselines. | Qiuhao Lu, Dejing Dou, Thien Huu Nguyen |  |
| 400 |  |  [Find Someone Who: Visual Commonsense Understanding in Human-Centric Grounding](https://doi.org/10.18653/v1/2022.findings-emnlp.399) |  | 0 | From a visual scene containing multiple people, human is able to distinguish each individual given the context descriptions about what happened before, their mental/physical states or intentions, etc. Above ability heavily relies on human-centric commonsense knowledge and reasoning. For example, if asked to identify the “person who needs healing” in an image, we need to first know that they usually have injuries or suffering expressions, then find the corresponding visual clues before finally grounding the person. We present a new commonsense task, Human-centric Commonsense Grounding, that tests the models’ ability to ground individuals given the context descriptions about what happened before, and their mental/physical states or intentions. We further create a benchmark, HumanCog, a dataset with 130k grounded commonsensical descriptions annotated on 67k images, covering diverse types of commonsense and visual scenes. We set up a context-object-aware method as a strong baseline that outperforms previous pre-trained and non-pretrained models. Further analysis demonstrates that rich visual commonsense and powerful integration of multi-modal commonsense are essential, which sheds light on future works. Data and code will be available at https://github.com/Hxyou/HumanCog. | Haoxuan You, Rui Sun, Zhecan Wang, KaiWei Chang, ShihFu Chang |  |
| 401 |  |  [CrisisLTLSum: A Benchmark for Local Crisis Event Timeline Extraction and Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.400) |  | 0 | Social media has increasingly played a key role in emergency response: first responders can use public posts to better react to ongoing crisis events and deploy the necessary resources where they are most needed. Timeline extraction and abstractive summarization are critical technical tasks to leverage large numbers of social media posts about events. Unfortunately, there are few datasets for benchmarking technical approaches for those tasks. This paper presents , the largest dataset of local crisis event timelines available to date. contains 1,000 crisis event timelines across four domains: wildfires, local fires, traffic, and storms. We built using a semi-automated cluster-then-refine approach to collect data from the public Twitter stream. Our initial experiments indicate a significant gap between the performance of strong baselines compared to the human performance on both tasks.Our dataset, code, and models are publicly available (https://github.com/CrisisLTLSum/CrisisTimelines). | Hossein Rajaby Faghihi, Bashar Alhafni, Ke Zhang, Shihao Ran, Joel R. Tetreault, Alejandro Jaimes |  |
| 402 |  |  [Prompt-Tuning Can Be Much Better Than Fine-Tuning on Cross-lingual Understanding With Multilingual Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.401) |  | 0 | Pre-trained multilingual language models show significant performance gains for zero-shot cross-lingual model transfer on a wide range of natural language understanding (NLU) tasks. Previously, for zero-shot cross-lingual evaluation, pre-trained models are only fine-tuned on English data and tested on a variety of target languages. In this paper, we do cross-lingualevaluation on various NLU tasks (sentence classification, sequence labeling, question answering) using prompt-tuning and compare it with fine-tuning. The results show that prompt tuning achieves much better cross-lingual transfer than fine-tuning across datasets, with only 0.1% to 0.3% tuned parameters. Additionally, we demonstrate through the analysis that prompt tuning can have better cross-lingual transfer-ability of representations on downstream tasks with better aligned decision boundaries. | Lifu Tu, Caiming Xiong, Yingbo Zhou |  |
| 403 |  |  [BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model](https://doi.org/10.18653/v1/2022.findings-emnlp.402) |  | 0 | This paper presents BERT-CTC, a novel formulation of end-to-end speech recognition that adapts BERT for connectionist temporal classification (CTC). Our formulation relaxes the conditional independence assumptions used in conventional CTC and incorporates linguistic knowledge through the explicit output dependency obtained by BERT contextual embedding. BERT-CTC attends to the full contexts of the input and hypothesized output sequences via the self-attention mechanism. This mechanism encourages a model to learn inner/inter-dependencies between the audio and token representations while maintaining CTC’s training efficiency. During inference, BERT-CTC combines a mask-predict algorithm with CTC decoding, which iteratively refines an output sequence. The experimental results reveal that BERT-CTC improves over conventional approaches across variations in speaking styles and languages. Finally, we show that the semantic representations in BERT-CTC are beneficial towards downstream spoken language understanding tasks. | Yosuke Higuchi, Brian Yan, Siddhant Arora, Tetsuji Ogawa, Tetsunori Kobayashi, Shinji Watanabe |  |
| 404 |  |  [EtriCA: Event-Triggered Context-Aware Story Generation Augmented by Cross Attention](https://doi.org/10.18653/v1/2022.findings-emnlp.403) |  | 0 | One of the key challenges of automatic story generation is how to generate a long narrative that can maintain fluency, relevance, and coherence. Despite recent progress, current story generation systems still face the challenge of how to effectively capture contextual and event features, which has a profound impact on a model’s generation performance. To address these challenges, we present EtriCA, a novel neural generation model, which improves the relevance and coherence of the generated stories through residually mapping context features to event sequences with a cross-attention mechanism. Such a feature capturing mechanism allows our model to better exploit the logical relatedness between events when generating stories. Extensive experiments based on both automatic and human evaluations show that our model significantly outperforms state-of-the-art baselines, demonstrating the effectiveness of our model in leveraging context and event features. | Chen Tang, Chenghua Lin, Henglin Huang, Frank Guerin, Zhihao Zhang |  |
| 405 |  |  [LADIS: Language Disentanglement for 3D Shape Editing](https://doi.org/10.18653/v1/2022.findings-emnlp.404) |  | 0 | Natural language interaction is a promising direction for democratizing 3D shape design. However, existing methods for text-driven 3D shape editing face challenges in producing decoupled, local edits to 3D shapes. We address this problem by learning disentangled latent representations that ground language in 3D geometry. To this end, we propose a complementary tool set including a novel network architecture, a disentanglement loss, and a new editing procedure. Additionally, to measure edit locality, we define a new metric that we call part-wise edit precision. We show that our method outperforms existing SOTA methods by 20% in terms of edit locality, and up to 6.6% in terms of language reference resolution accuracy. Human evaluations additionally show that compared to the existing SOTA, our method produces shape edits that are more local, more semantically accurate, and more visually obvious. Our work suggests that by solely disentangling language representations, downstream 3D shape editing can become more local to relevant parts, even if the model was never given explicit part-based supervision. | Ian Huang, Panos Achlioptas, Tianyi Zhang, Sergey Tulyakov, Minhyuk Sung, Leonidas J. Guibas |  |
| 406 |  |  [Effective Pretraining Objectives for Transformer-based Autoencoders](https://doi.org/10.18653/v1/2022.findings-emnlp.405) |  | 0 | In this paper, we study trade-offs between efficiency, cost and accuracy when pre-training Transformer encoders with different pre-training objectives. For this purpose, we analyze features of common objectives and combine them to create new effective pre-training approaches. Specifically, we designed light token generators based on a straightforward statistical approach, which can replace ELECTRA computationally heavy generators, thus highly reducing cost. Our experiments also show that (i) there are more efficient alternatives to BERT’s MLM, and (ii) it is possible to efficiently pre-train Transformer-based models using lighter generators without a significant drop in performance. | Luca Di Liello, Matteo Gabburo, Alessandro Moschitti |  |
| 407 |  |  [Language Model Detoxification in Dialogue with Contextualized Stance Control](https://doi.org/10.18653/v1/2022.findings-emnlp.406) |  | 0 | To reduce the toxic degeneration in a pretrained Language Model (LM), previous work on Language Model detoxification has focused on reducing the toxicity of the generation itself (self-toxicity) without consideration of the context. As a result, a type of implicit offensive language where the generations support the offensive language in the context is ignored. Different from the LM controlling tasks in previous work, where the desired attributes are fixed for generation, the desired stance of the generation depends on the offensiveness of the context. Therefore, we propose a novel control method to do context-dependent detoxification with the stance taken into consideration. We introduce meta prefixes to learn the contextualized stance control strategy and to generate the stance control prefix according to the input context. The generated stance prefix is then combined with the toxicity control prefix to guide the response generation. Experimental results show that our proposed method can effectively learn the context-dependent stance control strategies while keeping a low self-toxicity of the underlying LM. | Jing Qian, Xifeng Yan |  |
| 408 |  |  [Multilingual SubEvent Relation Extraction: A Novel Dataset and Structure Induction Method](https://doi.org/10.18653/v1/2022.findings-emnlp.407) |  | 0 | Subevent Relation Extraction (SRE) is a task in Information Extraction that aims to recognize spatial and temporal containment relations between event mentions in text. Recent methods have utilized pre-trained language models to represent input texts for SRE. However, a key issue in existing SRE methods is the employment of sequential order of words in texts to feed into representation learning methods, thus unable to explicitly focus on important context words and their interactions to enhance representations. In this work, we introduce a new method for SRE that learns to induce effective graph structures for input texts to boost representation learning. Our method features a word alignment framework with dependency paths and optimal transport to identify important context words to form effective graph structures for SRE. In addition, to enable SRE research on non-English languages, we present a new multilingual SRE dataset for five typologically different languages. Extensive experiments reveal the state-of-the-art performance for our method on different datasets and languages. | Viet Dac Lai, Hieu Man, Linh Ngo Van, Franck Dernoncourt, Thien Huu Nguyen |  |
| 409 |  |  [A Two-Stage Approach towards Generalization in Knowledge Base Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.408) |  | 0 | Most existing approaches for Knowledge Base Question Answering (KBQA) focus on a specific underlying knowledge base either because of inherent assumptions in the approach, or because evaluating it on a different knowledge base requires non-trivial changes. However, many popular knowledge bases share similarities in their underlying schemas that can be leveraged to facilitate generalization across knowledge bases. To achieve this generalization, we introduce a KBQA framework based on a 2-stage architecture that explicitly separates semantic parsing from the knowledge base interaction, facilitating transfer learning across datasets and knowledge graphs. We show that pretraining on datasets with a different underlying knowledge base can nevertheless provide significant performance gains and reduce sample complexity. Our approach achieves comparable or state-of-the-art performance for LC-QuAD (DBpedia), WebQSP (Freebase), SimpleQuestions (Wikidata) and MetaQA (Wikimovies-KG). | Srinivas Ravishankar, Dung Thai, Ibrahim Abdelaziz, Nandana Mihindukulasooriya, Tahira Naseem, Pavan Kapanipathi, Gaetano Rossiello, Achille Fokoue |  |
| 410 |  |  [Few-Shot (Dis)Agreement Identification in Online Discussions with Regularized and Augmented Meta-Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.409) |  | 0 | Online discussions are abundant with opinions towards a common topic, and identifying (dis)agreement between a pair of comments enables many opinion mining applications. Realizing the increasing needs to analyze opinions for emergent new topics that however tend to lack annotations, we present the first meta-learning approach for few-shot (dis)agreement identification that can be quickly applied to analyze opinions for new topics with few labeled instances. Furthermore, we enhance the meta-learner’s domain generalization ability from two perspectives. The first is domain-invariant regularization, where we design a lexicon-based regularization loss to enable the meta-learner to learn domain-invariant cues. The second is domain-aware augmentation, where we propose domain-aware task augmentation for meta-training to learn domain-specific expressions. In addition to using an existing dataset, we also evaluate our approach on two very recent new topics, mask mandate and COVID vaccine, using our newly annotated datasets containing 1.5k and 1.4k SubReddits comment pairs respectively. Extensive experiments on three domains/topics demonstrate the effectiveness of our meta-learning approach. | Yuanyuan Lei, Ruihong Huang |  |
| 411 |  |  [Data Cartography for Low-Resource Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.410) |  | 0 | While collecting or generating more parallel data is necessary to improve machine translation (MT) in low-resource settings, we lack an understanding of how the limited amounts of existing data are actually used to help guide the collection of further resources. In this paper, we apply data cartography techniques (Swayamdipta et al., 2020) to characterize the contribution of training samples in two low-resource MT tasks (Swahili-English and Turkish-English) throughout the training of standard neural MT models. Our empirical study shows that, unlike in prior work for classification tasks, most samples contribute to model training in low-resource MT, albeit not uniformly throughout the training process. Furthermore, uni-dimensional characterizations of samples – e.g., based on dual cross-entropy or word frequency – do not suffice to characterize to what degree they are hard or easy to learn. Taken together, our results suggest that data augmentation strategies for low-resource MT would benefit from model-in-the-loop strategies to maximize improvements. | Aquia Richburg, Marine Carpuat |  |
| 412 |  |  [Augmenting Multi-Turn Text-to-SQL Datasets with Self-Play](https://doi.org/10.18653/v1/2022.findings-emnlp.411) |  | 0 | The task of context-dependent text-to-SQL aims to convert multi-turn user utterances to formal SQL queries. This is a challenging task due to both the scarcity of training data from which to learn complex contextual dependencies and to generalize to unseen databases. In this paper we explore augmenting the training datasets using self-play, which leverages contextual information to synthesize new interactions to adapt the model to new databases. We first design a SQL-to-text model conditioned on a sampled goal query, which represents a user’s intent, that then converses with a text-to-SQL semantic parser to generate new interactions. We then filter the synthesized interactions and retrain the models with the augmented data. We find that self-play improves the accuracy of a strong baseline on SParC and CoSQL, two widely used cross-domain text-to-SQL datasets. Our analysis shows that self-play simulates various conversational thematic relations, enhances cross-domain generalization and improves beam-search. | Qi Liu, Zihuiwen Ye, Tao Yu, Linfeng Song, Phil Blunsom |  |
| 413 |  |  [Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.412) |  | 0 | We explore the idea of compressing the prompts used to condition language models, and show that compressed prompts can retain a substantive amount of information about the original prompt. For severely compressed prompts, while fine-grained information is lost, abstract information and general sentiments can be retained with surprisingly few parameters, which can be useful in the context of decode-time algorithms for controllability and toxicity reduction. We find that some complex prompts can be effectively compressed into a single token to guide generation. We also show that compressed prompts are largely compositional, and can be constructed such that they can be used to control independent aspects of generated text. | David Wingate, Mohammad Shoeybi, Taylor Sorensen |  |
| 414 |  |  [NaturalAdversaries: Can Naturalistic Adversaries Be as Effective as Artificial Adversaries?](https://doi.org/10.18653/v1/2022.findings-emnlp.413) |  | 0 | While a substantial body of prior work has explored adversarial example generation for natural language understanding tasks, these examples are often unrealistic and diverge from the real-world data distributions. In this work, we introduce a two-stage adversarial example generation framework (NaturalAdversaries), for designing adversaries that are effective at fooling a given classifier and demonstrate natural-looking failure cases that could plausibly occur during in-the-wild deployment of the models. At the first stage a token attribution method is used to summarize a given classifier’s behavior as a function of the key tokens in the input. In the second stage a generative model is conditioned on the key tokens from the first stage. NaturalAdversaries is adaptable to both black-box and white-box adversarial attacks based on the level of access to the model parameters. Our results indicate these adversaries generalize across domains, and offer insights for future research on improving robustness of neural text classification models. | Saadia Gabriel, Hamid Palangi, Yejin Choi |  |
| 415 |  |  [Multi-Path Transformer is Better: A Case Study on Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.414) |  | 0 | For years the model performance in machine learning obeyed a power-law relationship with the model size. For the consideration of parameter efficiency, recent studies focus on increasing model depth rather than width to achieve better performance. In this paper, we study how model width affects the Transformer model through a parameter-efficient multi-path structure. To better fuse features extracted from different paths, we add three additional operations to each sublayer: a normalization at the end of each path, a cheap operation to produce more features, and a learnable weighted mechanism to fuse all features flexibly. Extensive experiments on 12 WMT machine translation tasks show that, with the same number of parameters, the shallower multi-path model can achieve similar or even better performance than the deeper model. It reveals that we should pay more attention to the multi-path structure, and there should be a balance between the model depth and width to train a better large-scale Transformer. | Ye Lin, Shuhan Zhou, Yanyang Li, Anxiang Ma, Tong Xiao, Jingbo Zhu |  |
| 416 |  |  [Unsupervised Learning of Hierarchical Conversation Structure](https://doi.org/10.18653/v1/2022.findings-emnlp.415) |  | 0 | Human conversations can evolve in many different ways, creating challenges for automatic understanding and summarization. Goal-oriented conversations often have meaningful sub-dialogue structure, but it can be highly domain-dependent. This work introduces an unsupervised approach to learning hierarchical conversation structure, including turn and sub-dialogue segment labels, corresponding roughly to dialogue acts and sub-tasks, respectively. The decoded structure is shown to be useful in enhancing neural models of language for three conversation-level understanding tasks. Further, the learned finite-state sub-dialogue network is made interpretable through automatic summarization. | BoRu Lu, Yushi Hu, Hao Cheng, Noah A. Smith, Mari Ostendorf |  |
| 417 |  |  [Task Compass: Scaling Multi-task Pre-training with Task Prefix](https://doi.org/10.18653/v1/2022.findings-emnlp.416) |  | 0 | Leveraging task-aware annotated data as supervised signals to assist with self-supervised learning on large-scale unlabeled data has become a new trend in pre-training language models. Existing studies show that multi-task learning with large-scale supervised tasks suffers from negative effects across tasks. To tackle the challenge, we propose a task prefix guided multi-task pre-training framework to explore the relationships among tasks. We conduct extensive experiments on 40 datasets, which show that our model can not only serve as the strong foundation backbone for a wide range of tasks but also be feasible as a probing tool for analyzing task relationships. The task relationships reflected by the prefixes align transfer learning performance between tasks. They also suggest directions for data augmentation with complementary tasks, which help our model achieve human-parity results on commonsense reasoning leaderboards. Code is available at https://github.com/cooelf/CompassMTL. | Zhuosheng Zhang, Shuohang Wang, Yichong Xu, Yuwei Fang, Wenhao Yu, Yang Liu, Hai Zhao, Chenguang Zhu, Michael Zeng |  |
| 418 |  |  [Sharpness-Aware Minimization with Dynamic Reweighting](https://doi.org/10.18653/v1/2022.findings-emnlp.417) |  | 0 | Deep neural networks are often overparameterized and may not easily achieve model generalization. Adversarial training has shown effectiveness in improving generalization by regularizing the change of loss on top of adversarially chosen perturbations. The recently proposed sharpness-aware minimization (SAM) algorithm conducts adversarial weight perturbation, encouraging the model to converge to a flat minima. SAM finds a common adversarial weight perturbation per-batch. Although per-instance adversarial weight perturbations are stronger adversaries and can potentially lead to better generalization performance, their computational cost is very high and thus it is impossible to use per-instance perturbations efficiently in SAM. In this paper, we tackle this efficiency bottleneck and propose sharpness-aware minimization with dynamic reweighting (delta-SAM). Our theoretical analysis motivates that it is possible to approach the stronger, per-instance adversarial weight perturbations using reweighted per-batch weight perturbations. delta-SAM dynamically reweights perturbation within each batch according to the theoretically principled weighting factors, serving as a good approximation to per-instance perturbation. Experiments on various natural language understanding tasks demonstrate the effectiveness of delta-SAM. | Wenxuan Zhou, Fangyu Liu, Huan Zhang, Muhao Chen |  |
| 419 |  |  [Predicting Long-Term Citations from Short-Term Linguistic Influence](https://doi.org/10.18653/v1/2022.findings-emnlp.418) |  | 0 | A standard measure of the influence of a research paper is the number of times it is cited. However, papers may be cited for many reasons, and citation count is not informative about the extent to which a paper affected the content of subsequent publications. We therefore propose a novel method to quantify linguistic influence in timestamped document collections. There are two main steps: first, identify lexical and semantic changes using contextual embeddings and word frequencies; second, aggregate information about these changes into per-document influence parameters by estimating a high-dimensional Hawkes process with a low-rank parameter matrix. The resulting measures of linguistic influence are predictive of future citations. Specifically, the estimate of linguistic influence from the two years after a paper’s publication is correlated with and predictive of its citation count in the following three years. This is demonstrated using an online evaluation with incremental temporal training/test splits, in comparison with a strong baseline that includes predictors for initial citation counts, topics, and lexical features. | Sandeep Soni, David Bamman, Jacob Eisenstein |  |
| 420 |  |  [Joint Audio/Text Training for Transformer Rescorer of Streaming Speech Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.419) |  | 0 | Recently, there has been an increasing interest in two-pass streaming end-to-end speech recognition (ASR) that incorporates a 2nd-pass rescoring model on top of the conventional 1st-pass streaming ASR model to improve recognition accuracy while keeping latency low. One of the latest 2nd-pass rescoring model, Transformer Rescorer, takes the n-best initial outputs and audio embeddings from the 1st-pass model, and then choose the best output by re-scoring the n-best initial outputs. However, training this Transformer Rescorer requires expensive paired audio-text training data because the model uses audio embeddings as input. In this work, we present our Joint Audio/Text training method for Transformer Rescorer, to leverage unpaired text-only data which is relatively cheaper than paired audio-text data. We evaluate Transformer Rescorer with our Joint Audio/Text training on Librispeech dataset as well as our large-scale in-house dataset and show that our training method can improve word error rate (WER) significantly compared to standard Transformer Rescorer without requiring any extra model parameters or latency. | Suyoun Kim, Ke Li, Lucas Kabela, Ron Huang, Jiedan Zhu, Ozlem Kalinli, Duc Le |  |
| 421 |  |  [TyDiP: A Dataset for Politeness Classification in Nine Typologically Diverse Languages](https://doi.org/10.18653/v1/2022.findings-emnlp.420) |  | 0 | We study politeness phenomena in nine typologically diverse languages. Politeness is an important facet of communication and is sometimes argued to be cultural-specific, yet existing computational linguistic study is limited to English. We create TyDiP, a dataset containing three-way politeness annotations for 500 examples in each language, totaling 4.5K examples. We evaluate how well multilingual models can identify politeness levels – they show a fairly robust zero-shot transfer ability, yet fall short of estimated human accuracy significantly. We further study mapping the English politeness strategy lexicon into nine languages via automatic translation and lexicon induction, analyzing whether each strategy’s impact stays consistent across languages. Lastly, we empirically study the complicated relationship between formality and politeness through transfer experiments. We hope our dataset will support various research questions and applications, from evaluating multilingual models to constructing polite multilingual agents. | Anirudh Srinivasan, Eunsol Choi |  |
| 422 |  |  [Probing Cross-modal Semantics Alignment Capability from the Textual Perspective](https://doi.org/10.18653/v1/2022.findings-emnlp.421) |  | 0 | In recent years, vision and language pre-training (VLP) models have advanced the state-of-the-art results in a variety of cross-modal downstream tasks. Aligning cross-modal semantics is claimed to be one of the essential capabilities of VLP models. However, it still remains unclear about the inner working mechanism of alignment in VLP models. In this paper, we propose a new probing method that is based on image captioning to first empirically study the cross-modal semantics alignment of VLP models. Our probing method is built upon the fact that given an image-caption pair, the VLP models will give a score, indicating how well two modalities are aligned; maximizing such scores will generate sentences that VLP models believe are of good alignment. Analyzing these sentences thus will reveal in what way different modalities are aligned and how well these alignments are in VLP models. We apply our probing method to five popular VLP models, including UNITER, ROSITA, ViLBERT, CLIP, and LXMERT, and provide a comprehensive analysis of the generated captions guided by these models. Our results show that VLP models (1) focus more on just aligning objects with visual words, while neglecting global semantics; (2) prefer fixed sentence patterns, thus ignoring more important textual information including fluency and grammar; and (3) deem the captions with more visual words are better aligned with images. These findings indicate that VLP models still have weaknesses in cross-modal semantics alignment and we hope this work will draw researchers’ attention to such problems when designing a new VLP model. | Zheng Ma, Shi Zong, Mianzhi Pan, Jianbing Zhang, Shujian Huang, Xinyu Dai, Jiajun Chen |  |
| 423 |  |  [Hidden State Variability of Pretrained Language Models Can Guide Computation Reduction for Transfer Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.422) |  | 0 | While transferring a pretrained language model, common approaches conventionally attach their task-specific classifiers to the top layer and adapt all the pretrained layers. We investigate whether one could make a task-specific selection on which subset of the layers to adapt and where to place the classifier. The goal is to reduce the computation cost of transfer learning methods (e.g. fine-tuning or adapter-tuning) without sacrificing its performance.We propose to select layers based on the variability of their hidden states given a task-specific corpus. We say a layer is already “well-specialized” in a task if the within-class variability of its hidden states is low relative to the between-class variability. Our variability metric is cheap to compute and doesn’t need any training or hyperparameter tuning. It is robust to data imbalance and data scarcity. Extensive experiments on the GLUE benchmark demonstrate that selecting layers based on our metric can yield significantly stronger performance than using the same number of top layers and often match the performance of fine-tuning or adapter-tuning the entire language model. | Shuo Xie, Jiahao Qiu, Ankita Pasad, Li Du, Qing Qu, Hongyuan Mei |  |
| 424 |  |  [Language Models as Agent Models](https://doi.org/10.18653/v1/2022.findings-emnlp.423) |  | 0 | Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in the outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them—a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of communicative intentions in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents’ communicative intentions influence their language. I survey findings from the recent literature showing that—even in today’s non-robust and error-prone models—LMs infer and use representations of fine-grained communicative intentions and high-level beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally. | Jacob Andreas |  |
| 425 |  |  [Combinatory Grammar Tells Underlying Relevance among Entities](https://doi.org/10.18653/v1/2022.findings-emnlp.424) |  | 0 | Relation extraction (RE) is an important task in natural language processing which aims to annotate the relation between two given entities, which requires a deep understanding of the running text. To import model performance, existing approaches leverage syntactic information to facilitate the relation extraction process, where they mainly focus on dependencies among words while paying limited attention to other types of syntactic structure. Considering that combinatory categorial grammar (CCG) is a lexicalized grammatical formalism that carries the syntactic and semantic knowledge for text understanding, we propose an alternative solution for RE that takes advantage of CCG to detect the relation between entities. In doing so, we perform a multi-task learning process to learn from RE and auto-annotated CCG supertags, where an attention mechanism is performed over all input words to distinguish the important ones for RE with the attention weights guided by the supertag decoding process. We evaluate our model on two widely used English benchmark datasets (i.e., ACE2005EN and SemEval 2010 Task 8 datasets) for RE, where the effectiveness of our approach is demonstrated by the experimental results with our approach achieving state-of-the-art performance on both datasets. | Yuanhe Tian, Yan Song |  |
| 426 |  |  [Leveraging Open Data and Task Augmentation to Automated Behavioral Coding of Psychotherapy Conversations in Low-Resource Scenarios](https://doi.org/10.18653/v1/2022.findings-emnlp.425) |  | 0 | In psychotherapy interactions, the quality of a session is assessed by codifying the communicative behaviors of participants during the conversation through manual observation and annotation. Developing computational approaches for automated behavioral coding can reduce the burden on human coders and facilitate the objective evaluation of the intervention. In the real world, however, implementing such algorithms is associated with data sparsity challenges since privacy concerns lead to limited available in-domain data. In this paper, we leverage a publicly available conversation-based dataset and transfer knowledge to the low-resource behavioral coding task by performing an intermediate language model training via meta-learning. We introduce a task augmentation method to produce a large number of “analogy tasks” — tasks similar to the target one — and demonstrate that the proposed framework predicts target behaviors more accurately than all the other baseline models. | Zhuohao Chen, Nikolaos Flemotomos, Zac E. Imel, David C. Atkins, Shrikanth Narayanan |  |
| 427 |  |  [Learning to Detect Noisy Labels Using Model-Based Features](https://doi.org/10.18653/v1/2022.findings-emnlp.426) |  | 0 | Label noise is ubiquitous in various machine learning scenarios such as self-labeling with model predictions and erroneous data annotation. Many existing approaches are based on heuristics such as sample losses, which might not be flexible enough to achieve optimal solutions. Meta learning based methods address this issue by learning a data selection function, but can be hard to optimize. In light of these pros and cons, we propose SENT (Selection-Enhanced Noisy label Training) that does not rely on meta learning while having the flexibility of being data-driven. SENT transfers the noise distribution to a clean set and trains a model to distinguish noisy labels from clean ones using model-based features. Empirically, on a wide range of tasks including text classification and speech recognition, SENT improves performance over strong baselines under the settings of self-training and label corruption. | Zhihao Wang, Zongyu Lin, Junjie Wen, Xianxin Chen, Peiqi Liu, Guidong Zheng, Yujun Chen, Zhilin Yang |  |
| 428 |  |  [Keyphrase Generation Beyond the Boundaries of Title and Abstract](https://doi.org/10.18653/v1/2022.findings-emnlp.427) |  | 0 | Keyphrase generation aims at generating important phrases (keyphrases) that best describe a given document. In scholarly domains, current approaches have largely used only the title and abstract of the articles to generate keyphrases. In this paper, we comprehensively explore whether the integration of additional information from the full text of a given article or from semantically similar articles can be helpful for a neural keyphrase generation model or not. We discover that adding sentences from the full text, particularly in the form of the extractive summary of the article can significantly improve the generation of both types of keyphrases that are either present or absent from the text. Experimental results with three widely used models for keyphrase generation along with one of the latest transformer models suitable for longer documents, Longformer Encoder-Decoder (LED) validate the observation. We also present a new large-scale scholarly dataset FullTextKP for keyphrase generation. Unlike prior large-scale datasets, FullTextKP includes the full text of the articles along with the title and abstract. We release the source code at https://github.com/kgarg8/FullTextKP. | Krishna Garg, Jishnu Ray Chowdhury, Cornelia Caragea |  |
| 429 |  |  [Composition, Attention, or Both?](https://doi.org/10.18653/v1/2022.findings-emnlp.428) |  | 0 | In this paper, we propose a novel architecture called Composition Attention Grammars (CAGs) that recursively compose subtrees into a single vector representation with a composition function, and selectively attend to previous structural information with a self-attention mechanism. We investigate whether these components—the composition function and the self-attention mechanism—can both induce human-like syntactic generalization. Specifically, we train language models (LMs) with and without these two components with the model sizes carefully controlled, and evaluate their syntactic generalization performance against six test circuits on the SyntaxGym benchmark. The results demonstrated that the composition function and the self-attention mechanism both play an important role to make LMs more human-like, and closer inspection of linguistic phenomenon implied that the composition function allowed syntactic features, but not semantic features, to percolate into subtree representations. | Ryo Yoshida, Yohei Oseki |  |
| 430 |  |  [CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model](https://doi.org/10.18653/v1/2022.findings-emnlp.429) |  | 0 | Manually designing cloze test consumes enormous time and efforts. The major challenge lies in wrong option (distractor) selection. Having carefully-design distractors improves the effectiveness of learner ability assessment. As a result, the idea of automatically generating cloze distractor is motivated. In this paper, we investigate cloze distractor generation by exploring the employment of pre-trained language models (PLMs) as an alternative for candidate distractor generation. Experiments show that the PLM-enhanced model brings a substantial performance improvement. Our best performing model advances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our code and dataset is available at https://github.com/AndyChiangSH/CDGP. | ShangHsuan Chiang, SsuCheng Wang, YaoChung Fan |  |
| 431 |  |  [G3: Geolocation via Guidebook Grounding](https://doi.org/10.18653/v1/2022.findings-emnlp.430) |  | 0 | We demonstrate how language can improve geolocation: the task of predicting the location where an image was taken. Here we study explicit knowledge from human-written guidebooks that describe the salient and class-discriminative visual features humans use for geolocation. We propose the task of Geolocation via Guidebook Grounding that uses a dataset of StreetView images from a diverse set of locations and an associated textual guidebook for GeoGuessr, a popular interactive geolocation game. Our approach predicts a country for each image by attending over the clues automatically extracted from the guidebook. Supervising attention with country-level pseudo labels achieves the best performance. Our approach substantially outperforms a state-of-the-art image-only geolocation method, with an improvement of over 5% in Top-1 accuracy. Our dataset and code can be found at https://github.com/g-luo/geolocation_via_guidebook_grounding. | Grace Luo, Giscard Biamby, Trevor Darrell, Daniel Fried, Anna Rohrbach |  |
| 432 |  |  [Controlling Bias Exposure for Fair Interpretable Predictions](https://doi.org/10.18653/v1/2022.findings-emnlp.431) |  | 0 | Recent work on reducing bias in NLP models usually focuses on protecting or isolating information related to a sensitive attribute (like gender or race). However, when sensitive information is semantically entangled with the task information of the input, e.g., gender information is predictive for a profession, a fair trade-off between task performance and bias mitigation is difficult to achieve. Existing approaches perform this trade-off by eliminating bias information from the latent space, lacking control over how much bias is necessarily required to be removed. We argue that a favorable debiasing method should use sensitive information ‘fairly’, rather than blindly eliminating it (Caliskan et al., 2017; Sun et al., 2019; Bogen et al., 2020). In this work, we provide a novel debiasing algorithm by adjustingthe predictive model’s belief to (1) ignore the sensitive information if it is not useful for the task; (2) use sensitive information minimally as necessary for the prediction (while also incurring a penalty). Experimental results on two text classification tasks (influenced by gender) and an open-ended generation task (influenced by race) indicate that our model achieves a desirable trade-off between debiasing and task performance along with producing debiased rationales as evidence. | Zexue He, Yu Wang, Julian J. McAuley, Bodhisattwa Prasad Majumder |  |
| 433 |  |  [Investigating the Benefits of Free-Form Rationales](https://doi.org/10.18653/v1/2022.findings-emnlp.432) |  | 0 | Free-form rationales aim to aid model interpretability by supplying the background knowledge that can help understand model decisions. Crowdsourced rationales are provided for commonsense QA instances in popular datasets such as CoS-E and ECQA, but their utility remains under-investigated. We present human studies which show that ECQA rationales indeed provide additional background information to understand a decision, while over 88% of CoS-E rationales do not. Inspired by this finding, we ask: can the additional context provided by free-form rationales benefit models, similar to human users? We investigate the utility of rationales as an additional source of supervision, by varying the quantity and quality of rationales during training. After controlling for instances where rationales leak the correct answer while not providing additional background knowledge, we find that incorporating only 5% of rationales during training can boost model performance by 47.22% for CoS-E and 57.14% for ECQA during inference. Moreover, we also show that rationale quality matters: compared to crowdsourced rationales, T5-generated rationales provide not only weaker supervision to models, but are also not helpful for humans in aiding model interpretability. | Jiao Sun, Swabha Swayamdipta, Jonathan May, Xuezhe Ma |  |
| 434 |  |  [Data-Efficient Concept Extraction from Pre-trained Language Models for Commonsense Explanation Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.433) |  | 0 | Predicting the key explanation concept is essential for generating commonsense explanations. This paper introduces a method to predict the concept from pre-trained language models for commonsense explanation generation. Our experiment found that adopting a language model as the concept extractor and fine-tuning it with 20% training data can improve the quality and accuracy of the generated explanations over multiple evaluation metrics. Compared with conventional methods that search concepts over knowledge graphs, our method does not require the preparation and training models to search through knowledge graphs. To better understand the results from pre-trained language models, we also designed a metric to evaluate the retrieved concepts. Through analysis and experiments, we show the correlation between this metric and the performance of the generators, and we also show the importance of attaching concepts for generating high-quality sentences. | Yanbo Fang, Yongfeng Zhang |  |
| 435 |  |  [Unsupervised Domain Adaptation for Joint Information Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.434) |  | 0 | Joint Information Extraction (JIE) aims to jointly solve multiple tasks in the Information Extraction pipeline (e.g., entity mention, event trigger, relation, and event argument extraction). Due to their ability to leverage task dependencies and avoid error propagation, JIE models have presented state-of-the-art performance for different IE tasks. However, an issue with current JIE methods is that they only focus on standard supervised learning setting where training and test data comes from the same domain. Cross-domain/domain adaptation learning with training and test data in different domains have not been explored for JIE, thus hindering the application of this technology to different domains in practice. To address this issue, our work introduces the first study to evaluate performance of JIE models in unsupervised domain adaptation setting. In addition, we present a novel method to induce domain-invariant representations for the tasks in JIE, called Domain Adaptation for Joint Information Extraction (DA4JIE). In DA4JIE, we propose an Instance-relational Domain Adaptation mechanism that seeks to align representations of task instances in JIE across domains through a generalized version of domain-adversarial learning approach. We further devise a Context-invariant Structure Learning technique to filter domain-specialized contextual information from induced representations to boost performance of JIE models in new domains. Extensive experiments and analyses demonstrate that DA4JIE can significantly improve out-of-domain performance for current state-of-the-art JIE systems for all IE tasks. | Nghia Trung Ngo, Bonan Min, Thien Huu Nguyen |  |
| 436 |  |  [Foiling Training-Time Attacks on Neural Machine Translation Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.435) |  | 0 | Neural machine translation (NMT) systems are vulnerable to backdoor attacks, whereby an attacker injects poisoned samples into training such that a trained model produces malicious translations. Nevertheless, there is little research on defending against such backdoor attacks in NMT. In this paper, we first show that backdoor attacks that have been successful in text classification are also effective against machine translation tasks. We then present a novel defence method that exploits a key property of most backdoor attacks: namely the asymmetry between the source and target language sentences, which is used to facilitate malicious text insertions, substitutions and suchlike. Our technique uses word alignment coupled with language model scoring to detect outlier tokens, and thus can find and filter out training instances which may contain backdoors. Experimental results demonstrate that our technique can significantly reduce the success of various attacks by up to 89.0%, while not affecting predictive accuracy. | Jun Wang, Xuanli He, Benjamin I. P. Rubinstein, Trevor Cohn |  |
| 437 |  |  [Learning Action-Effect Dynamics for Hypothetical Vision-Language Reasoning Task](https://doi.org/10.18653/v1/2022.findings-emnlp.436) |  | 0 | ‘Actions’ play a vital role in how humans interact with the world. Thus, autonomous agents that would assist us in everyday tasks also require the capability to perform ‘Reasoning about Actions & Change’ (RAC). This has been an important research direction in Artificial Intelligence (AI) in general, but the study of RAC with visual and linguistic inputs is relatively recent. The CLEVR_HYP (Sampat et. al., 2021) is one such testbed for hypothetical vision-language reasoning with actions as the key focus. In this work, we propose a novel learning strategy that can improve reasoning about the effects of actions. We implement an encoder-decoder architecture to learn the representation of actions as vectors. We combine the aforementioned encoder-decoder architecture with existing modality parsers and a scene graph question answering model to evaluate our proposed system on the CLEVR_HYP dataset. We conduct thorough experiments to demonstrate the effectiveness of our proposed approach and discuss its advantages over previous baselines in terms of performance, data efficiency, and generalization capability. | Shailaja Keyur Sampat, Pratyay Banerjee, Yezhou Yang, Chitta Baral |  |
| 438 |  |  [Named Entity and Relation Extraction with Multi-Modal Retrieval](https://doi.org/10.18653/v1/2022.findings-emnlp.437) |  | 0 | Multi-modal named entity recognition (NER) and relation extraction (RE) aim to leverage relevant image information to improve the performance of NER and RE. Most existing efforts largely focused on directly extracting potentially useful information from images (such as pixel-level features, identified objects, and associated captions).However, such extraction processes may not be knowledge aware, resulting in information that may not be highly relevant.In this paper, we propose a novel Multi-modal Retrieval based framework (MoRe).MoRe contains a text retrieval module and an image-based retrieval module, which retrieve related knowledge of the input text and image in the knowledge corpus respectively.Next, the retrieval results are sent to the textual and visual models respectively for predictions.Finally, a Mixture of Experts (MoE) module combines the predictions from the two models to make the final decision.Our experiments show that both our textual model and visual model can achieve state-of-the-art performance on four multi-modal NER datasets and one multi-modal RE dataset.With MoE, the model performance can be further improved and our analysis demonstrates the benefits of integrating both textual and visual cues for such tasks. | Xinyu Wang, Jiong Cai, Yong Jiang, Pengjun Xie, Kewei Tu, Wei Lu |  |
| 439 |  |  [Calibrating Factual Knowledge in Pretrained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.438) |  | 0 | Previous literature has proved that Pretrained Language Models (PLMs) can store factual knowledge. However, we find that facts stored in the PLMs are not always correct. It motivates us to explore a fundamental question: How do we calibrate factual knowledge in PLMs without re-training from scratch? In this work, we propose a simple and lightweight method CaliNet to achieve this goal. To be specific, we first detect whether PLMs can learn the right facts via a contrastive score between right and fake facts. If not, we then use a lightweight method to add and adapt new parameters to specific factual texts. Experiments on the knowledge probing task show the calibration effectiveness and efficiency. In addition, through closed-book question answering, we find that the calibrated PLM possesses knowledge generalization ability after finetuning.Beyond the calibration performance, we further investigate and visualize the knowledge calibration mechanism. | Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui, Lei Li |  |
| 440 |  |  [MCPG: A Flexible Multi-Level Controllable Framework for Unsupervised Paraphrase Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.439) |  | 0 | We present MCPG: a simple and effectiveapproach for controllable unsupervised paraphrase generation, which is also flexible toadapt to specific domains without extra training. MCPG is controllable in different levels: local lexicons, global semantics, and universal styles. The unsupervised paradigm ofMCPG combines factual keywords and diversified semantic embeddings as local lexical andglobal semantic constraints. The semantic embeddings are diversified by standard dropout,which is exploited for the first time to increaseinference diversity by us. Moreover, MCPGis qualified with good domain adaptability byadding a transfer vector as a universal style constraint, which is refined from the exemplars retrieved from the corpus of the target domain in atraining-free way. Extensive experiments showthat MCPG outperforms state-of-the-art unsupervised baselines by a margin. Meanwhile,our domain-adapted MCPG also achieves competitive performance with strong supervisedbaselines even without training. | Yi Chen, Haiyun Jiang, Lemao Liu, Rui Wang, Shuming Shi, Ruifeng Xu |  |
| 441 |  |  [WordTies: Measuring Word Associations in Language Models via Constrained Sampling](https://doi.org/10.18653/v1/2022.findings-emnlp.440) |  | 0 | Word associations are widely used in psychology to provide insights on how humans perceive and understand concepts. Comparing word associations in language models (LMs) to those generated by human subjects can serve as a proxy to uncover embedded lexical and commonsense knowledge in language models. While much helpful work has been done applying direct metrics, such as cosine similarity, to help understand latent spaces, these metrics are symmetric, while human word associativity is asymmetric. We propose WordTies, an algorithm based on constrained sampling from LMs, which allows an asymmetric measurement of associated words, given a cue word as the input. Comparing to existing methods, word associations found by this method share more overlap with associations provided by humans, and observe the asymmetric property of human associations. To examine possible reasons behind associations, we analyze the knowledge and reasoning behind the word pairings as they are linked to lexical and commonsense knowledge graphs.When the knowledge about the nature of the word pairings is combined with a probability that the LM has learned that information, we have a new way to examine what information is captured in LMs. | Peiran Yao, Tobias Renwick, Denilson Barbosa |  |
| 442 |  |  [Exploring The Landscape of Distributional Robustness for Question Answering Models](https://doi.org/10.18653/v1/2022.findings-emnlp.441) |  | 0 | We conduct a large empirical evaluation to investigate the landscape of distributional robustness in question answering. Our investigation spans over 350 models and 16 question answering datasets, including a diverse set of architectures, model sizes, and adaptation methods (e.g., fine-tuning, adapter tuning, in-context learning, etc.). We find that, in many cases, model variations do not affect robustness and in-distribution performance alone determines out-of-distribution performance.Moreover, our findings indicate thati) zero-shot and in-context learning methods are more robust to distribution shifts than fully fine-tuned models;ii) few-shot prompt fine-tuned models exhibit better robustness than few-shot fine-tuned span prediction models;iii) parameter-efficient and robustness enhancing training methods provide no significant robustness improvements.In addition, we publicly release all evaluations to encourage researchers to further analyze robustness trends for question answering models. | Anas Awadalla, Mitchell Wortsman, Gabriel Ilharco, Sewon Min, Ian Magnusson, Hannaneh Hajishirzi, Ludwig Schmidt |  |
| 443 |  |  [Collaborative Reasoning on Multi-Modal Semantic Graphs for Video-Grounded Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.442) |  | 0 | We study video-grounded dialogue generation, where a response is generated based on the dialogue context and the associated video. The primary challenges of this task lie in (1) the difficulty of integrating video data into pre-trained language models (PLMs) which presents obstacles to exploiting the power of large-scale pre-training; and (2) the necessity of taking into account the complementarity of various modalities throughout the reasoning process. Although having made remarkable progress in video-grounded dialogue generation, existing methods still fall short when it comes to integrating with PLMs in a way that allows information from different modalities to complement each other. To alleviate these issues, we first propose extracting pertinent information from videos and turning it into reasoning paths that are acceptable to PLMs. Additionally, we propose a multi-agent reinforcement learning method to collaboratively perform reasoning on different modalities (i.e., video and dialogue context). Empirical experiment results on two public datasets indicate that the proposed model can significantly outperform state-of-the-art models by large margins on both automatic and human evaluations. | Xueliang Zhao, Yuxuan Wang, Chongyang Tao, Chenshuo Wang, Dongyan Zhao |  |
| 444 |  |  [Partitioned Gradient Matching-based Data Subset Selection for Compute-Efficient Robust ASR Training](https://doi.org/10.18653/v1/2022.findings-emnlp.443) |  | 0 | Training state-of-the-art ASR systems such as RNN-T often has a high associated financial and environmental cost. Training with a subset of training data could mitigate this problem if the subset selected could achieve on-par performance with training with the entire dataset. Although there are many data subset selection(DSS) algorithms, direct application to the RNN-T is difficult, especially the DSS algorithms that are adaptive and use learning dynamics such as gradients, as RNN-T tend to have gradients with a significantly larger memory footprint. In this paper, we propose Partitioned Gradient Matching (PGM) a novel distributable DSS algorithm, suitable for massive datasets like those used to train RNN-T. Through extensive experiments on Librispeech 100H and Librispeech 960H, we show that PGM achieves between 3x to 6x speedup with only a very small accuracy degradation (under 1% absolute WER difference). In addition, we demonstrate similar results for PGM even in settings where the training data is corrupted with noise. | Ashish R. Mittal, Durga Sivasubramanian, Rishabh K. Iyer, Preethi Jyothi, Ganesh Ramakrishnan |  |
| 445 |  |  [Adaptive Graph Convolutional Network for Knowledge Graph Entity Alignment](https://doi.org/10.18653/v1/2022.findings-emnlp.444) |  | 0 | Entity alignment (EA) aims to identify equivalent entities from different Knowledge Graphs (KGs), which is a fundamental task for integrating KGs. Throughout its development, Graph Convolutional Network (GCN) has become one of the mainstream methods for EA. These GCN-based methods learn the representations of entities from two KGs by message passing mechanism and then make alignments via measuring the similarity between entity embeddings. The key idea that GCN works in EA is that entities with similar neighbor structures are highly likely to be aligned. However, the noisy neighbors of entities transfer invalid information, drown out equivalent information, lead to inaccurate entity embeddings, and finally reduce the performance of EA. Based on the Sinkhorn algorithm, we design a reliability measure for potential equivalent entities and propose Adaptive Graph Convolutional Network to deal with neighbor noises in GCN. During the training, the network dynamically updates the adaptive weights of relation triples to weaken the propagation of noises. While calculating entity similarity, it comprehensively considers the self-similarity and neighborhood similarity of the entity pair to alleviate the influence of noises. Furthermore, we design a straightforward but efficient strategy to construct pseudo alignments for unsupervised EA. Extensive experiments on benchmark datasets demonstrate that our framework outperforms the state-of-the-art methods in both supervised and unsupervised settings. | Renbo Zhu, Xukun Luo, Meng Ma, Ping Wang |  |
| 446 |  |  [Towards Robust NLG Bias Evaluation with Syntactically-diverse Prompts](https://doi.org/10.18653/v1/2022.findings-emnlp.445) |  | 0 | We present a robust methodology for evaluating biases in natural language generation(NLG) systems. Previous works use fixed hand-crafted prefix templates with mentions of various demographic groups to prompt models to generate continuations for bias analysis. These fixed prefix templates could themselves be specific in terms of styles or linguistic structures, which may lead to unreliable fairness conclusions that are not representative of the general trends from tone varying prompts. To study this problem, we paraphrase the prompts with different syntactic structures and use these to evaluate demographic bias in NLG systems. Our results suggest similar overall bias trends but some syntactic structures lead to contradictory conclusions compared to past works. We show that our methodology is more robust and that some syntactic structures prompt more toxic content while others could prompt less biased generation. This suggests the importance of not relying on a fixed syntactic structure and using tone-invariant prompts. Introducing syntactically-diverse prompts can achieve more robust NLG (bias) evaluation. | Arshiya Aggarwal, Jiao Sun, Nanyun Peng |  |
| 447 |  |  [PcMSP: A Dataset for Scientific Action Graphs Extraction from Polycrystalline Materials Synthesis Procedure Text](https://doi.org/10.18653/v1/2022.findings-emnlp.446) |  | 0 | Scientific action graphs extraction from materials synthesis procedures is important for reproducible research, machine automation, and material prediction. But the lack of annotated data has hindered progress in this field. We demonstrate an effort to annotate Polycrystalline Materials Synthesis Procedures PcMSP from 305 open access scientific articles for the construction of synthesis action graphs. This is a new dataset for material science information extraction that simultaneously contains the synthesis sentences extracted from the experimental paragraphs, as well as the entity mentions and intra-sentence relations. A two-step human annotation and inter-annotator agreement study guarantee the high quality of the PcMSP corpus. We introduce four natural language processing tasks: sentence classification, named entity recognition, relation classification, and joint extraction of entities and relations. Comprehensive experiments validate the effectiveness of several state-of-the-art models for these challenges while leaving large space for improvement. We also perform the error analysis and point out some unique challenges that require further investigation. We will release our annotation scheme, the corpus, and codes to the research community to alleviate the scarcity of labeled data in this domain. | Xianjun Yang, Ya Zhuo, Julia Zuo, Xinlu Zhang, Stephen D. Wilson, Linda R. Petzold |  |
| 448 |  |  [Validity Assessment of Legal Will Statements as Natural Language Inference](https://doi.org/10.18653/v1/2022.findings-emnlp.447) |  | 0 | This work introduces a natural language inference (NLI) dataset that focuses on the validity of statements in legal wills. This dataset is unique because: (a) each entailment decision requires three inputs: the statement from the will, the law, and the conditions that hold at the time of the testator’s death; and (b) the included texts are longer than the ones in current NLI datasets. We trained eight neural NLI models in this dataset. All the models achieve more than 80% macro F1 and accuracy, which indicates that neural approaches can handle this task reasonably well. However, group accuracy, a stricter evaluation measure that is calculated with a group of positive and negative examples generated from the same statement as a unit, is in mid 80s at best, which suggests that the models’ understanding of the task remains superficial. Further ablative analyses and explanation experiments indicate that all three text segments are used for prediction, but some decisions rely on semantically irrelevant tokens. This indicates that overfitting on these longer texts likely happens, and that additional research is required for this task to be solved. | Alice Saebom Kwak, Jacob O. Israelsen, Clayton T. Morrison, Derek E. Bambauer, Mihai Surdeanu |  |
| 449 |  |  [AdaPrompt: Adaptive Model Training for Prompt-based NLP](https://doi.org/10.18653/v1/2022.findings-emnlp.448) |  | 0 | Prompt-based learning, with its capability to tackle zero-shot and few-shot NLP tasks, has gained much attention in the community.The main idea is to bridge the gap between NLP downstream tasks and language modeling (LM), by mapping these tasks into natural language prompts, which are then filled by pre-trained language models (PLMs).However, for prompt learning, there are still two salient gaps between NLP tasks and pretraining.First, prompt information is not necessarily sufficiently present during LM pre-training. Second, task-specific data are not necessarily well represented during pre-training. We address these two issues by proposing AdaPrompt, adaptively retrieving external data for continual pretraining of PLMs by making use of both task and prompt characteristics. In addition, we make use of knowledge in Natural Language Inference models for deriving adaptive verbalizers.Experimental results on five NLP benchmarks show that AdaPrompt can improve over standard PLMs in few-shot settings. In addition, in zero-shot settings, our method outperforms standard prompt-based methods by up to 26.35% relative error reduction. | Yulong Chen, Yang Liu, Li Dong, Shuohang Wang, Chenguang Zhu, Michael Zeng, Yue Zhang |  |
| 450 |  |  [Code Generation From Flowcharts with Texts: A Benchmark Dataset and An Approach](https://doi.org/10.18653/v1/2022.findings-emnlp.449) |  | 0 | Currently, researchers focus on generating codes from the requirement documents. However, current approaches still perform poorly on some requirements needing complex problem-solving skills. In reality, to tackle such complex requirements, instead of directly translating requirement documents into codes, software engineers write codes via unified modeling language diagrams, such as flowcharts, an intermediate tool to analyze and visualize the system. Therefore, we propose a new source code generation task, that is, to generate source code from flowcharts with texts. We manually construct a benchmark dataset containing 320 flowcharts with their corresponding source codes. Obviously, it is not straightforward to employ the current approaches for the new source code generation task since (1) the flowchart is a graph that contains various structures, including loop, selection, and others which is different from texts; (2) the connections between nodes in the flowchart are abundant and diverse which need to be carefully handled. To solve the above problems, we propose a two-stage code generation model. In the first stage, a structure recognition algorithm is employed to transform the flowchart into pseudo-code containing the structural conventions of a typical programming language such as while, if. In the second stage, a code generation model is employed to convert the pseudo-code into code. Experimental results show that the proposed approach can achieve some improvement over the baselines. | Zejie Liu, Xiaoyu Hu, Deyu Zhou, Lin Li, Xu Zhang, Yanzheng Xiang |  |
| 451 |  |  [Focus! Relevant and Sufficient Context Selection for News Image Captioning](https://doi.org/10.18653/v1/2022.findings-emnlp.450) |  | 0 | News Image Captioning requires describing an image by leveraging additional context derived from a news article. Previous works only coarsely leverage the article to extract the necessary context, which makes it challenging for models to identify relevant events and named entities. In our paper, we first demonstrate that by combining more fine-grained context that captures the key named entities (obtained via an oracle) and the global context that summarizes the news, we can dramatically improve the model’s ability to generate accurate news captions. This begs the question, how to automatically extract such key entities from an image? We propose to use pre-trained vision and language retrieval model CLIP to localize the visually grounded entities in the news article, and then capture the non-visual entities via a open relation extraction model. Our experiments demonstrate that by simply selecting better context from the article, we can significantly improve the performance of existing models and achieve the new state-of-the-art performance on multiple benchmarks. | Mingyang Zhou, Grace Luo, Anna Rohrbach, Zhou Yu |  |
| 452 |  |  [Generative Aspect-Based Sentiment Analysis with Contrastive Learning and Expressive Structure](https://doi.org/10.18653/v1/2022.findings-emnlp.451) |  | 0 | Generative models have demonstrated impressive results on Aspect-based Sentiment Analysis (ABSA) tasks, particularly for the emerging task of extracting Aspect-Category-Opinion-Sentiment (ACOS) quadruples. However, these models struggle with implicit sentiment expressions, which are commonly observed in opinionated content such as online reviews. In this work, we introduce GEN-SCL-NAT, which consists of two techniques for improved structured generation for ACOS quadruple extraction. First, we propose GEN-SCL, a supervised contrastive learning objective that aids quadruple prediction by encouraging the model to produce input representations that are discriminable across key input attributes, such as sentiment polarity and the existence of implicit opinions and aspects. Second, we introduce GEN-NAT, a new structured generation format that better adapts pre-trained autoregressive encoder-decoder models to extract quadruples in a generative fashion. Experimental results show that GEN-SCL-NAT achieves top performance across three ACOS datasets, averaging 1.48% F1 improvement, with a maximum 1.73% increase on the LAPTOP-L1 dataset. Additionally, we see significant gains on implicit aspect and opinion splits that have been shown as challenging for existing ACOS approaches. | Joseph Peper, Lu Wang |  |
| 453 |  |  [Semantic Dependency Parsing with Edge GNNs](https://doi.org/10.18653/v1/2022.findings-emnlp.452) |  | 0 | Second-order neural parsers have obtained high accuracy in semantic dependency parsing. Inspired by the factor graph representation of second-order parsing, we propose edge graph neural networks (E-GNNs). In an E-GNN, each node corresponds to a dependency edge, and the neighbors are defined in terms of sibling, co-parent, and grandparent relationships. We conduct experiments on SemEval 2015 Task 18 English datasets, showing the superior performance of E-GNNs. | Songlin Yang, Kewei Tu |  |
| 454 |  |  [Explore Unsupervised Structures in Pretrained Models for Relation Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.453) |  | 0 | Syntactic trees have been widely applied in relation extraction (RE). However, since parsing qualities are not stable on different text domains and a pre-defined grammar may not well fit the target relation schema, the introduction of syntactic structures sometimes fails to improve RE performances consistently. In this work, we study RE models with various unsupervised structures mined from pre-trained language models (e.g., BERT). We show that, similar to syntactic trees, unsupervised structures are quite informative for RE task: they are able to obtain competitive (even the best) performance scores on benchmark RE datasets (ACE05, WebNLG, SciERC). We also conduct detailed analyses on their abilities of adapting new RE domains and influence of noise links in those structures. The results suggest that unsupervised structures are reasonable alternatives of commonly used syntactic structures in relation extraction models. | Xi Yang, Tao Ji, Yuanbin Wu |  |
| 455 |  |  [Identifying Human Strategies for Generating Word-Level Adversarial Examples](https://doi.org/10.18653/v1/2022.findings-emnlp.454) |  | 0 | Adversarial examples in NLP are receiving increasing research attention. One line of investigation is the generation of word-level adversarial examples against fine-tuned Transformer models that preserve naturalness and grammaticality. Previous work found that human- and machine-generated adversarial examples are comparable in their naturalness and grammatical correctness. Most notably, humans were able to generate adversarial examples much more effortlessly than automated attacks. In this paper, we provide a detailed analysis of exactly how humans create these adversarial examples. By exploring the behavioural patterns of human workers during the generation process, we identify statistically significant tendencies based on which words humans prefer to select for adversarial replacement (e.g., word frequencies, word saliencies, sentiment) as well as where and when words are replaced in an input sequence. With our findings, we seek to inspire efforts that harness human strategies for more robust NLP models. | Maximilian Mozes, Bennett Kleinberg, Lewis D. Griffin |  |
| 456 |  |  [Refinement Matters: Textual Description Needs to be Refined for Zero-shot Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.455) |  | 0 | Zero-Shot Learning (ZSL) has shown great promise at the intersection of vision and language, and generative methods for ZSL are predominant owing to their efficiency. Moreover, textual description or attribute plays a critical role in transferring knowledge from the seen to unseen classes in ZSL. Such generative approaches for ZSL are very costly to train and require the class description of the unseen classes during training. In this work, we propose a non-generative gating-based attribute refinement network for ZSL, which achieves similar accuracies to generative methods of ZSL, at a much lower computational cost. The refined attributes are mapped into the visual domain through an attribute embedder, and the whole network is guided by the circle loss and the well-known softmax cross-entropy loss to obtain a robust class embedding. We refer to our approach as Circle loss guided gating-based Attribute-Refinement Network (CARNet). We perform extensive experiments on the five benchmark datasets over the various challenging scenarios viz., Generalized ZSL (GZSL), Continual GZSL (CGZSL), and conventional ZSL. We observe that the CARNet significantly outperforms recent non-generative ZSL methods and most generative ZSL methods in all three settings by a significant margin. Our extensive ablation study disentangles the performance of various components and justifies their importance. The source code is available at https://github.com/Sethup123/CARNet. | Chandan Gautam, Sethupathy Parameswaran, Vinay Verma, Suresh Sundaram, Savitha Ramasamy |  |
| 457 |  |  [SAT: Improving Semi-Supervised Text Classification with Simple Instance-Adaptive Self-Training](https://doi.org/10.18653/v1/2022.findings-emnlp.456) |  | 0 | Self-training methods have been explored in recent years and have exhibited great performance in improving semi-supervised learning. This work presents a simple instance-adaptive self-training method (SAT) for semi-supervised text classification. SAT first generates two augmented views for each unlabeled data, and then trains a meta learner to automatically identify the relative strength of augmentations based on the similarity between the original view and the augmented views. The weakly-augmented view is fed to the model to produce a pseudo-label and the strongly-augmented view is used to train the model to predict the same pseudo-label. We conducted extensive experiments and analyses on three text classification datasets and found that with varying sizes of labeled training data, SAT consistently shows competitive performance compared to existing semi-supervised learning methods. | Hui Chen, Wei Han, Soujanya Poria |  |
| 458 |  |  [Answer Quality Aware Aggregation for Extractive QA Crowdsourcing](https://doi.org/10.18653/v1/2022.findings-emnlp.457) |  | 0 | Quality control is essential for creating extractive question answering (EQA) datasets via crowdsourcing. Aggregation across answers, i.e. word spans within passages annotated, by different crowd workers is one major focus for ensuring its quality. However, crowd workers cannot reach a consensus on a considerable portion of questions. We introduce a simple yet effective answer aggregation method that takes into account the relations among the answer, question, and context passage. We evaluate answer quality from both the view of question answering model to determine how confident the QA model is about each answer and the view of the answer verification model to determine whether the answer is correct. Then we compute aggregation scores with each answer’s quality and its contextual embedding produced by pre-trained language models. The experiments on a large real crowdsourced EQA dataset show that our framework outperforms baselines by around 16% on precision and effectively conduct answer aggregation for extractive QA task. | Peide Zhu, Zhen Wang, Claudia Hauff, Jie Yang, Avishek Anand |  |
| 459 |  |  [Search to Pass Messages for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.findings-emnlp.458) |  | 0 | Completing missing facts is a fundamental task for temporal knowledge graphs (TKGs).Recently, graph neural network (GNN) based methods, which can simultaneously explore topological and temporal information, have become the state-of-the-art (SOTA) to complete TKGs. However, these studies are based on hand-designed architectures and fail to explore the diverse topological and temporal properties of TKG.To address this issue, we propose to use neural architecture search (NAS) to design data-specific message passing architecture for TKG completion.In particular, we develop a generalized framework to explore topological and temporal information in TKGs.Based on this framework, we design an expressive search space to fully capture various properties of different TKGs. Meanwhile, we adopt a search algorithm, which trains a supernet structure by sampling single path for efficient search with less cost.We further conduct extensive experiments on three benchmark datasets. The results show that the searched architectures by our method achieve the SOTA performances.Besides, the searched models can also implicitly reveal diverse properties in different TKGs.Our code is released in https://github.com/striderdu/SPA. | Zhen Wang, Haotong Du, Quanming Yao, Xuelong Li |  |
| 460 |  |  [Code Vulnerability Detection via Nearest Neighbor Mechanism](https://doi.org/10.18653/v1/2022.findings-emnlp.459) |  | 0 | Code vulnerability detection is a fundamental and challenging task in the software security field. Existing research works aim to learn semantic information from the source code by utilizing NLP technologies. However, in vulnerability detection tasks, some vulnerable samples are very similar to non-vulnerable samples, which are difficult to identify. To address this issue and improve detection performance, we introduce the k-nearest neighbor mechanism which retrieves multiple neighbor samples and utilizes label information of retrieved neighbor samples to provide help for model predictions. Besides, we use supervised contrastive learning to make the model learn the discriminative representation and ensure that label information of retrieved neighbor samples is as consistent as possible with the label information of testing samples. Extensive experiments show that our method can achieve obvious performance improvements compared to baseline models. | Qianjin Du, Xiaohui Kuang, Gang Zhao |  |
| 461 |  |  [Robust Question Answering against Distribution Shifts with Test-Time Adaption: An Empirical Study](https://doi.org/10.18653/v1/2022.findings-emnlp.460) |  | 0 | A deployed question answering (QA) model can easily fail when the test data has a distribution shift compared to the training data. Robustness tuning (RT) methods have been widely studied to enhance model robustness against distribution shifts before model deployment. However, can we improve a model after deployment? To answer this question, we evaluate test-time adaptation (TTA) to improve a model after deployment. We first introduce ColdQA, a unified evaluation benchmark for robust QA against text corruption and changes in language and domain. We then evaluate previous TTA methods on ColdQA and compare them to RT methods. We also propose a novel TTA method called online imitation learning (OIL). Through extensive experiments, we find that TTA is comparable to RT methods, and applying TTA after RT can significantly boost the performance on ColdQA. Our proposed OIL improves TTA to be more robust to variation in hyper-parameters and test distributions over time. | Hai Ye, Yuyang Ding, Juntao Li, Hwee Tou Ng |  |
| 462 |  |  [ParaMac: A General Unsupervised Paraphrase Generation Framework Leveraging Semantic Constraints and Diversifying Mechanisms](https://doi.org/10.18653/v1/2022.findings-emnlp.461) |  | 0 | Paraphrase generation reflects the ability to understand the meaning from the language surface form and rephrase it to other expressions. Recent paraphrase generation works have paid attention to unsupervised approaches based on Pre-trained Language Models (PLMs) to avoid heavy reliance on parallel data by utilizing PLMs’ generation ability. However, the generated pairs of existing unsupervised methods are usually weak either in semantic equivalence or expression diversity. In this paper, we present a novel unsupervised paraphrase generation framework called Paraphrase Machine. By employing multi-aspect equivalence constraints and multi-granularity diversifying mechanisms, Paraphrase Machine is able to achieve good semantic equivalence and expressive diversity, producing a high-quality unsupervised paraphrase dataset. Based on this dataset, we train a general paraphrase model, which can be directly applied to rewrite the input sentence of various domains without any fine-tuning, and achieves substantial gains of 9.1% and 3.3% absolutely in BLEU score over previous SOTA on Quora and MSCOCO. By further fine-tuning our model with domain-specific training sets, the improvement can be increased to even 18.0% and 4.6%. Most importantly, by applying it to language understanding and generation tasks under the low-resource setting, we demonstrate that our model can serve as a universal data augmentor to boost the few-shot performance (e.g., average 2.0% gain on GLUE). | Jinxin Liu, Jiaxin Shi, Ji Qi, Lei Hou, Juanzi Li, Qi Tian |  |
| 463 |  |  [Semi-supervised New Slot Discovery with Incremental Clustering](https://doi.org/10.18653/v1/2022.findings-emnlp.462) |  | 0 | Discovering new slots is critical to the success of dialogue systems. Most existing methods rely on automatic slot induction in unsupervised fashion or perform domain adaptation across zero or few-shot scenarios. They have difficulties in providing high-quality supervised signals to learn clustering-friendly features, and are limited in effectively transferring the prior knowledge from known slots to new slots. In this work, we propose a Semi-supervised Incremental Clustering method (SIC), to discover new slots with the aid of existing linguistic annotation models and limited known slot data. Specifically, we harvest slot value candidates with NLP model cues and innovatively formulate the slot discovery task under an incremental clustering framework. The model gradually calibrate slot representations under the supervision of generated pseudo-labels, and automatically learns to terminate when no more salient slot remains. Our thorough evaluation on five public datasets demonstrates that it significantly outperforms state-of-the-art models. | Yuxia Wu, Lizi Liao, Xueming Qian, TatSeng Chua |  |
| 464 |  |  [Con-NAT: Contrastive Non-autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.463) |  | 0 | Inspired by the success of contrastive learning in natural language processing, we incorporate contrastive learning into the conditional masked language model which is extensively used in non-autoregressive neural machine translation (NAT). Accordingly, we propose a Contrastive Non-autoregressive Neural Machine Translation (Con-NAT) model. Con-NAT optimizes the similarity of several different representations of the same token in the same sentence. We propose two methods to obtain various representations: Contrastive Common Mask and Contrastive Dropout. Positive pairs are various different representations of the same token, while negative pairs are representations of different tokens. In the feature space, the model with contrastive loss pulls positive pairs together and pushes negative pairs away. We conduct extensive experiments on six translation directions with different data sizes. The results demonstrate that Con-NAT showed a consistent and significant improvement in fully and iterative NAT. Con-NAT is state-of-the-art on WMT’16 Ro-En (34.18 BLEU). | Hao Cheng, Zhihua Zhang |  |
| 465 |  |  [Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection](https://doi.org/10.18653/v1/2022.findings-emnlp.464) |  | 0 | Knowledge distillation addresses the problem of transferring knowledge from a teacher model to a student model.In this process, we typically have multiple types of knowledge extracted from the teacher model.The problem is to make full use of them to train the student model.Our preliminary study shows that: (1) not all of the knowledge is necessary for learning a good student model, and (2) knowledge distillation can benefit from certain knowledge at different training steps.In response to these, we propose an actor-critic approach to selecting appropriate knowledge to transfer during the process of knowledge distillation.In addition, we offer a refinement of the training algorithm to ease the computational burden.Experimental results on the GLUE datasets show that our method outperforms several strong knowledge distillation baselines significantly. | Chenglong Wang, Yi Lu, Yongyu Mu, Yimin Hu, Tong Xiao, Jingbo Zhu |  |
| 466 |  |  [Syntactically Robust Training on Partially-Observed Data for Open Information Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.465) |  | 0 | Open Information Extraction models have shown promising results with sufficient supervision. However, these models face a fundamental challenge that the syntactic distribution of training data is partially observable in comparison to the real world. In this paper, we propose a syntactically robust training framework that enables models to be trained on a syntactic-abundant distribution based on diverse paraphrase generation. To tackle the intrinsic problem of knowledge deformation of paraphrasing, two algorithms based on semantic similarity matching and syntactic tree walking are used to restore the expressionally transformed knowledge. The training framework can be generally applied to other syntactic partial observable domains. Based on the proposed framework, we build a new evaluation set called CaRB-AutoPara, a syntactically diverse dataset consistent with the real-world setting for validating the robustness of the models. Experiments including a thorough analysis show that the performance of the model degrades with the increase of the difference in syntactic distribution, while our framework gives a robust boundary. | Ji Qi, Yuxiang Chen, Lei Hou, Juanzi Li, Bin Xu |  |
| 467 |  |  [A Benchmark and Dataset for Post-OCR text correction in Sanskrit](https://doi.org/10.18653/v1/2022.findings-emnlp.466) |  | 0 | Sanskrit is a classical language with about 30 million extant manuscripts fit for digitisation, available in written, printed or scanned-image forms. However, it is still considered to be a low-resource language when it comes to available digital resources. In this work, we release a post-OCR text correction dataset containing around 218,000 sentences, with 1.5 million words, from 30 different books. Texts in Sanskrit are known to be diverse in terms of their linguistic and stylistic usage since Sanskrit was the ‘lingua francua’ for discourse in the Indian subcontinent for about 3 millennia. Keeping this in mind, we release a multi-domain dataset, from areas as diverse as astronomy, medicine and mathematics, with some of them as old as 18 centuries. Further, we release multiple strong baselines as benchmarks for the task, based on pre-trained Seq2Seq language models. We find that our best-performing model, consisting of byte level tokenization in conjunction with phonetic encoding (Byt5+SLP1), yields a 23% point increase over the OCR output in terms of word and character error rates. Moreover, we perform extensive experiments in evaluating these models on their performance and analyse common causes of mispredictions both at the graphemic and lexical levels. Our code and dataset is publicly available at https://github.com/ayushbits/pe-ocr-sanskrit. | Ayush Maheshwari, Nikhil Singh, Amrith Krishna, Ganesh Ramakrishnan |  |
| 468 |  |  [Knowledge-Enhanced Self-Supervised Prototypical Network for Few-Shot Event Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.467) |  | 0 | Prototypical network based joint methods have attracted much attention in few-shot event detection, which carry out event detection in a unified sequence tagging framework. However, these methods suffer from the inaccurate prototype representation problem, due to two main reasons: the number of instances for calculating prototypes is limited; And, they do not well capture the relationships among event prototypes. To deal with this problem, we propose a Knowledge-Enhanced self-supervised Prototypical Network, called KE-PN, for few-shot event detection. KE-PN adopts hybrid rules, which can automatically align event types to an external knowledge base, i.e., FrameNet, to obtain more instances.It proposes a self-supervised learning method to filter out noisy data from enhanced instances. KE-PN is further equipped with an auxiliary event type relationship classification module, which injects the relationship information into representations of event prototypes. Extensive experiments on three benchmark datasets, i.e., FewEvent, MAVEN, and ACE2005 demonstrate the state-of-the-art performance of KE-PN. | Kailin Zhao, Xiaolong Jin, Long Bai, Jiafeng Guo, Xueqi Cheng |  |
| 469 |  |  [VarMAE: Pre-training of Variational Masked Autoencoder for Domain-adaptive Language Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.468) |  | 0 | Pre-trained language models have been widely applied to standard benchmarks. Due to the flexibility of natural language, the available resources in a certain domain can be restricted to support obtaining precise representation. To address this issue, we propose a novel Transformer-based language model named VarMAE for domain-adaptive language understanding. Under the masked autoencoding objective, we design a context uncertainty learning module to encode the token’s context into a smooth latent distribution. The module can produce diverse and well-formed contextual representations. Experiments on science- and finance-domain NLU tasks demonstrate that VarMAE can be efficiently adapted to new domains with limited resources. | Dou Hu, Xiaolong Hou, Xiyang Du, Mengyuan Zhou, Lianxin Jiang, Yang Mo, Xiaofeng Shi |  |
| 470 |  |  [Exploring Methods for Building Dialects-Mandarin Code-Mixing Corpora: A Case Study in Taiwanese Hokkien](https://doi.org/10.18653/v1/2022.findings-emnlp.469) |  | 0 | In natural language processing (NLP), code-mixing (CM) is a challenging task, especially when the mixed languages include dialects. In Southeast Asian countries such as Singapore, Indonesia, and Malaysia, Hokkien-Mandarin is the most widespread code-mixed language pair among Chinese immigrants, and it is also common in Taiwan. However, dialects such as Hokkien often have a scarcity of resources and the lack of an official writing system, limiting the development of dialect CM research. In this paper, we propose a method to construct a Hokkien-Mandarin CM dataset to mitigate the limitation, overcome the morphological issue under the Sino-Tibetan language family, and offer an efficient Hokkien word segmentation method through a linguistics-based toolkit. Furthermore, we use our proposed dataset and employ transfer learning to train the XLM (cross-lingual language model) for translation tasks. To fit the code-mixing scenario, we adapt XLM slightly. We found that by using linguistic knowledge, rules, and language tags, the model produces good results on CM data translation while maintaining monolingual translation quality. | SinEn Lu, BoHan Lu, ChaoYi Lu, Richard TzongHan Tsai |  |
| 471 |  |  [Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in Transformer-Based Variational AutoEncoder for Diverse Text Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.470) |  | 0 | Variational Auto-Encoder (VAE) has been widely adopted in text generation. Among many variants, recurrent VAE learns token-wise latent variables with each conditioned on the preceding ones, which captures sequential variability better in the era of RNN. However, it is unclear how to incorporate such recurrent dynamics into the recently dominant Transformer due to its parallelism. In this work, we propose TRACE, a Transformer-based recurrent VAE structure. TRACE imposes recurrence on segment-wise latent variables with arbitrarily separated text segments and constructs the posterior distribution with residual parameterization. Besides, we design an acceleration method by approximating idempotent matrices, which allows parallelism while maintaining the conditional dependence of latent variables. We demonstrate that TRACE could deduce a non-zero lower bound of the KL term and enhance the entanglement of each segment and preceding latent variables, providing a theoretical guarantee of generation diversity. Experiments on two unconditional and one conditional generation task show that TRACE achieves significantly improved diversity while maintaining satisfactory generation quality. | Jinyi Hu, Xiaoyuan Yi, Wenhao Li, Maosong Sun, Xing Xie |  |
| 472 |  |  [Tweet Based Reach Aware Temporal Attention Network for NFT Valuation](https://doi.org/10.18653/v1/2022.findings-emnlp.471) |  | 0 | Non-Fungible Tokens (NFTs) are a relatively unexplored class of assets. Designing strategies to forecast NFT trends is an intricate task due to its extremely volatile nature. The market is largely driven by public sentiment and “hype”, which in turn has a high correlation with conversations taking place on social media platforms like Twitter. Prior work done for modelling stock market data does not take into account the extent of impact certain highly influential tweets and their authors can have on the market. Building on these limitations and the nature of the NFT market, we propose a novel reach-aware temporal learning approach to make predictions for forecasting future trends in the NFT market. We perform experiments on a new dataset consisting of over 1.3 million tweets and 180 thousand NFT transactions spanning over 15 NFT collections curated by us. Our model (TA-NFT) outperforms other state-of-the-art methods by an average of 36%. Through extensive quantitative and ablative analysis, we demonstrate the ability of our approach as a practical method for predicting NFT trends. | Ramit Sawhney, Megh Thakkar, Ritesh Soun, Atula Tejaswi Neerkaje, Vasu Sharma, Dipanwita Guhathakurta, Sudheer Chava |  |
| 473 |  |  [Entity Embedding Completion for Wide-Coverage Entity Disambiguation](https://doi.org/10.18653/v1/2022.findings-emnlp.472) |  | 0 | Entity disambiguation (ED) is typically solved by learning to classify a given mention into one of the entities in the model’s entity vocabulary by referring to their embeddings. However, this approach cannot address mentions of entities that are not covered by the entity vocabulary. Aiming to enhance the applicability of ED models, we propose a method of extending a state-of-the-art ED model by dynamically computing embeddings of out-of-vocabulary entities. Specifically, our method computes embeddings from entity descriptions and mention contexts. Experiments with standard benchmark datasets show that the extended model performs comparable to or better than existing models whose entity embeddings are trained for all candidate entities as well as embedding-free models. We release our source code and model checkpoints at https://github.com/studio-ousia/steel. | Daisuke Oba, Ikuya Yamada, Naoki Yoshinaga, Masashi Toyoda |  |
| 474 |  |  [Entity-level Interaction via Heterogeneous Graph for Multimodal Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-emnlp.473) |  | 0 | Multimodal Named Entity Recognition (MNER) faces two specific challenges: 1) How to capture useful entity-related visual information. 2) How to alleviate the interference of visual noise. Previous works have gained progress by improving interacting mechanisms or seeking for better visual features. However, existing methods neglect the integrity of entity semantics and conduct cross-modal interaction at token-level, which cuts apart the semantics of entities and makes non-entity tokens easily interfered with by irrelevant visual noise. Thus in this paper, we propose an end-to-end heterogeneous Graph-based Entity-level Interacting model (GEI) for MNER. GEI first utilizes a span detection subtask to obtain entity representations, which serve as the bridge between two modalities. Then, the heterogeneous graph interacting network interacts entity with object nodes to capture entity-related visual information, and fuses it into only entity-associated tokens to rid non-entity tokens of the visual noise. Experiments on two widely used datasets demonstrate the effectiveness of our method. Our code will be available at https://github.com/GangZhao98/GEI. | Gang Zhao, Guanting Dong, Yidong Shi, Haolong Yan, Weiran Xu, Si Li |  |
| 475 |  |  [Status Biases in Deliberation Online: Evidence from a Randomized Experiment on ChangeMyView](https://doi.org/10.18653/v1/2022.findings-emnlp.474) |  | 0 | Status is widely used to incentivize user engagement online. However, visible status indicators could inadvertently bias online deliberation to favor high-status users. In this work, we design and deploy a randomized experiment on the ChangeMyView platform to quantify status biases in deliberation online. We find strong evidence of status bias: hiding status on ChangeMyView increases the persuasion rate of moderate-status users by 84% and decreases the persuasion rate of high-status users by 41% relative to the control group. We also find that the persuasive power of status is moderated by verbosity, suggesting that status is used as an information-processing heuristic under cognitive load. Finally, we find that a user’s status influences the argumentation behavior of other users they interact with in a manner that disadvantages low and moderate-status users. | Emaad Manzoor, Yohan Jo, Alan Montgomery |  |
| 476 |  |  [Empathetic and Emotionally Positive Conversation Systems with an Emotion-specific Query-Response Memory](https://doi.org/10.18653/v1/2022.findings-emnlp.475) |  | 0 | Emotional conversation systems generate responses for the input queries considering the speaker’s emotions in a conversation. Existing emotional conversation systems output emotional responses according to either a given emotion or the user’s emotion reflected in the input queries. Following a given emotion may lead to an emotional drift between the given emotion and the conversation state, and following only the user’s emotion may aggravate the user’s negative feelings if users suffer from a negative mood. In this paper, we propose to generate empathetic responses catering to the user’s emotions while leading the conversation to be emotionally positive. Particularly, by abstracting the conversation corpus, we extract and store the different responding strategies for different users’ emotions and conversational topics into a memory. We encourage positive emotions in conversation via a sentiment evaluator. We model the memory outputs with a Gaussian mixture distribution and sample a final responding strategy from the distribution. The strategy acts as a condition to a transformer model to generate responses. The experiments verify our model surpasses the baseline methods in appropriateness, diversity, and generating emotionally positive responses. | Zhiliang Tian, Yinliang Wang, Yiping Song, Chi Zhang, Dongkyu Lee, Yingxiu Zhao, Dongsheng Li, Nevin L. Zhang |  |
| 477 |  |  [Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision](https://doi.org/10.18653/v1/2022.findings-emnlp.476) |  | 0 | Clinical trials are essential for drug development but are extremely expensive and time-consuming to conduct. It is beneficial to study similar historical trials when designing a clinical trial. However, lengthy trial documents and lack of labeled data make trial similarity search difficult. We propose a zero-shotclinical trial retrieval method, called Trial2Vec, which learns through self-supervision without the need for annotating similar clinical trials. Specifically, the meta-structure of trial documents (e.g., title, eligibility criteria, target disease) along with clinical knowledge (e.g., UMLS knowledge base) are leveraged to automatically generate contrastive samples. Besides, encodes trial documents considering meta-structure thus producing compact embeddings aggregating multi-aspect information from the whole document. We show that our method yields medically interpretable embeddings by visualization and it gets 15% average improvement over the best baselines on precision/recall for trial retrieval, which is evaluated on our labeled 1600 trial pairs. In addition, we prove the pretrained embeddings benefit the downstream trial outcome prediction task over 240k trials. Software is available at https://github.com/RyanWangZf/Trial2Vec. | Zifeng Wang, Jimeng Sun |  |
| 478 |  |  [From Mimicking to Integrating: Knowledge Integration for Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.477) |  | 0 | Investigating better ways to reuse the released pre-trained language models (PLMs) can significantly reduce the computational cost and the potential environmental side-effects. This paper explores a novel PLM reuse paradigm, Knowledge Integration (KI). Without human annotations available, KI aims to merge the knowledge from different teacher-PLMs, each of which specializes in a different classification problem, into a versatile student model. To achieve this, we first derive the correlation between virtual golden supervision and teacher predictions. We then design a Model Uncertainty–aware Knowledge Integration (MUKI) framework to recover the golden supervision for the student. Specifically, MUKI adopts Monte-Carlo Dropout to estimate model uncertainty for the supervision integration. An instance-wise re-weighting mechanism based on the margin of uncertainty scores is further incorporated, to deal with the potential conflicting supervision from teachers.Experimental results demonstrate that MUKI achieves substantial improvements over baselines on benchmark datasets. Further analysis shows that MUKI can generalize well for merging teacher models with heterogeneous architectures, and even teachers major in cross-lingual datasets. | Lei Li, Yankai Lin, Xuancheng Ren, Guangxiang Zhao, Peng Li, Jie Zhou, Xu Sun |  |
| 479 |  |  [Model and Data Transfer for Cross-Lingual Sequence Labelling in Zero-Resource Settings](https://doi.org/10.18653/v1/2022.findings-emnlp.478) |  | 0 | Zero-resource cross-lingual transfer approaches aim to apply supervised modelsfrom a source language to unlabelled target languages. In this paper we performan in-depth study of the two main techniques employed so far for cross-lingualzero-resource sequence labelling, based either on data or model transfer.Although previous research has proposed translation and annotation projection(data-based cross-lingual transfer) as an effective technique for cross-lingualsequence labelling, in this paper we experimentally demonstrate that highcapacity multilingual language models applied in a zero-shot (model-basedcross-lingual transfer) setting consistently outperform data-basedcross-lingual transfer approaches. A detailed analysis of our results suggeststhat this might be due to important differences in language use. Morespecifically, machine translation often generates a textual signal which isdifferent to what the models are exposed to when using gold standard data,which affects both the fine-tuning and evaluation processes. Our results alsoindicate that data-based cross-lingual transfer approaches remain a competitiveoption when high-capacity multilingual language models are not available. | Iker GarcíaFerrero, Rodrigo Agerri, German Rigau |  |
| 480 |  |  [Early Guessing for Dialect Identification](https://doi.org/10.18653/v1/2022.findings-emnlp.479) |  | 0 | This paper deals with the problem of incre-mental dialect identification. Our goal is toreliably determine the dialect before the fullutterance is given as input. The major partof the previous research on dialect identification has been model-centric, focusing on performance. We address a new question: How much input is needed to identify a dialect? Ourapproach is a data-centric analysis that resultsin general criteria for finding the shortest inputneeded to make a plausible guess. Workingwith three sets of language dialects (Swiss German, Indo-Aryan and Arabic languages), weshow that it is possible to generalize across dialects and datasets with two input shorteningcriteria: model confidence and minimal inputlength (adjusted for the input type). The sourcecode for experimental analysis can be found atGithub. | Vani Kanjirangat, Tanja Samardzic, Fabio Rinaldi, Ljiljana Dolamic |  |
| 481 |  |  [R-AT: Regularized Adversarial Training for Natural Language Understanding](https://doi.org/10.18653/v1/2022.findings-emnlp.480) |  | 0 | Currently, adversarial training has become a popular and powerful regularization method in the natural language domain. In this paper, we Regularized Adversarial Training (R-AT) via dropout, which forces the output probability distributions of different sub-models generated by dropout to be consistent under the same adversarial samples. Specifically, we generate adversarial samples by perturbing the word embeddings. For each adversarial sample fed to the model, R-AT minimizes both the adversarial risk and the bidirectional KL-divergence between the adversarial output distributions of two sub-models sampled by dropout. Through extensive experiments on 13 public natural language understanding datasets, we found that R-AT has improvements for many models (e.g., rnn-based, cnn-based, and transformer-based models). For the GLUE benchmark, when R-AT is only applied to the fine-tuning stage, it is able to improve the overall test score of the BERT-base model from 78.3 to 79.6 and the RoBERTa-large model from 88.1 to 88.6. Theoretical analysis reveals that R-AT has potential gradient regularization during the training process. Furthermore, R-AT can reduce the inconsistency between training and testing of models with dropout. | Shiwen Ni, Jiawen Li, HungYu Kao |  |
| 482 |  |  [Multi-View Active Learning for Short Text Classification in User-Generated Data](https://doi.org/10.18653/v1/2022.findings-emnlp.481) |  | 0 | Mining user-generated data often suffers from the lack of enough labeled data, short document lengths, and the informal user language. In this paper, we propose a novel active learning model to overcome these obstacles in the tasks tailored for query phrases–e.g., detecting positive reports of natural disasters. Our model has three novelties: 1) It is the first approach to employ multi-view active learning in this domain. 2) It uses the Parzen-Rosenblatt window method to integrate the representativeness measure into multi-view active learning. 3) It employs a query-by-committee strategy, based on the agreement between predictors, to address the usually noisy language of the documents in this domain. We evaluate our model in four publicly available Twitter datasets with distinctly different applications. We also compare our model with a wide range of baselines including those with multiple classifiers. The experiments testify that our model is highly consistent and outperforms existing models. | Payam Karisani, Negin Karisani, Li Xiong |  |
| 483 |  |  [Forging Multiple Training Objectives for Pre-trained Language Models via Meta-Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.482) |  | 0 | Multiple pre-training objectives fill the vacancy of the understanding capability of single-objective language modeling, which serves the ultimate purpose of pre-trained language models (PrLMs), generalizing well on a mass of scenarios. However, learning multiple training objectives in a single model is challenging due to the unknown relative significance as well as the potential contrariety between them. Empirical studies have shown that the current objective sampling in an ad-hoc manual setting makes the learned language representation barely converge to the desired optimum. Thus, we propose MOMETAS, a novel adaptive sampler based on meta-learning, which learns the latent sampling pattern on arbitrary pre-training objectives. Such a design is lightweight with negligible additional training overhead. To validate our approach, we adopt five objectives and conduct continual pre-training with BERT-base and BERT-large models, where MOMETAS demonstrates universal performance gain over other rule-based sampling strategies on 14 natural language processing tasks. | Hongqiu Wu, Ruixue Ding, Hai Zhao, Boli Chen, Pengjun Xie, Fei Huang, Min Zhang |  |
| 484 |  |  [ConGen: Unsupervised Control and Generalization Distillation For Sentence Representation](https://doi.org/10.18653/v1/2022.findings-emnlp.483) |  | 0 | Sentence representations are essential in many NLP tasks operating at the sentence level.Recently, research attention has shifted towards learning how to represent sentences without any annotations, i.e., unsupervised representation learning. Despite the benefit of training without supervised data, there is still a performance penalty compared to supervised methods.Furthermore, the supervised-unsupervised performance gap widens as we reduce the model size. In this paper, we propose an unsupervised sentence representation method to reduce the supervised-unsupervised performance gap, especially for smaller models. Utilizing the concept for knowledge distillation, we derive a distillation framework comprising two training objectives, control and generalize, called ConGen. Experiments on semantic textual similarity (STS), text classification (transfer), and natural language inference (NLI) tasks show that ConGen is on par with supervised training even on smaller models.Furthermore, our method consistently outperformed competitors on multilingual STS.The code and models are available at https://github.com/KornWtp/ConGen. | Peerat Limkonchotiwat, Wuttikorn Ponwitayarat, Lalita Lowphansirikul, Ekapol Chuangsuwanich, Sarana Nutanong |  |
| 485 |  |  [Large-Scale Differentially Private BERT](https://doi.org/10.18653/v1/2022.findings-emnlp.484) |  | 0 | In this work, we study the large-scale pretraining of BERT-Large (Devlin et al., 2019) with differentially private SGD (DP-SGD). We show that combined with a careful implementation, scaling up the batch size to millions (i.e., mega-batches) improves the utility of the DP-SGD step for BERT; we also enhance the training efficiency by using an increasing batch size schedule. Our implementation builds on the recent work of Subramani et al (2020), who demonstrated that the overhead of a DP-SGD step is minimized with effective use of JAX (Bradbury et al., 2018; Frostig et al., 2018) primitives in conjunction with the XLA compiler (XLA team and collaborators, 2017). Our implementation achieves a masked language model accuracy of 60.5% at a batch size of 2M, for epsilon=5, which is a reasonable privacy setting. To put this number in perspective, non-private BERT models achieve an accuracy of ∼70%. | Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, Pasin Manurangsi |  |
| 486 |  |  [Improving Zero-Shot Multilingual Translation with Universal Representations and Cross-Mapping](https://doi.org/10.18653/v1/2022.findings-emnlp.485) |  | 0 | The many-to-many multilingual neural machine translation can translate between language pairs unseen during training, i.e., zero-shot translation. Improving zero-shot translation requires the model to learn universal representations and cross-mapping relationships to transfer the knowledge learned on the supervised directions to the zero-shot directions. In this work, we propose the state mover’s distance based on the optimal theory to model the difference of the representations output by the encoder. Then, we bridge the gap between the semantic-equivalent representations of different languages at the token level by minimizing the proposed distance to learn universal representations. Besides, we propose an agreement-based training scheme, which can help the model make consistent predictions based on the semantic-equivalent sentences to learn universal cross-mapping relationships for all translation directions. The experimental results on diverse multilingual datasets show that our method can improve consistently compared with the baseline system and other contrast methods. The analysis proves that our method can better align the semantic space and improve the prediction consistency. | Shuhao Gu, Yang Feng |  |
| 487 |  |  [Controllable Fake Document Infilling for Cyber Deception](https://doi.org/10.18653/v1/2022.findings-emnlp.486) |  | 0 | Recent works in cyber deception study how to deter malicious intrusion by generating multiple fake versions of a critical document to impose costs on adversaries who need to identify the correct information. However, existing approaches are context-agnostic, resulting in sub-optimal and unvaried outputs. We propose a novel context-aware model, Fake Document Infilling (FDI), by converting the problem to a controllable mask-then-infill procedure. FDI masks important concepts of varied lengths in the document, then infills a realistic but fake alternative considering both the previous and future contexts. We conduct comprehensive evaluations on technical documents and news stories. Results show that FDI outperforms the baselines in generating highly believable fakes with moderate modification to protect critical information and deceive adversaries. | Yibo Hu, Yu Lin, Erick Skorupa Parolin, Latifur Khan, Kevin W. Hamlen |  |
| 488 |  |  [Weakly Supervised Headline Dependency Parsing](https://doi.org/10.18653/v1/2022.findings-emnlp.487) |  | 0 | English news headlines form a register with unique syntactic properties that have been documented in linguistics literature since the 1930s. However, headlines have received surprisingly little attention from the NLP syntactic parsing community. We aim to bridge this gap by providing the first news headline corpus of Universal Dependencies annotated syntactic dependency trees, which enables us to evaluate existing state-of-the-art dependency parsers on news headlines. To improve English news headline parsing accuracies, we develop a projection method to bootstrap silver training data from unlabeled news headline-article lead sentence pairs. Models trained on silver headline parses demonstrate significant improvements in performance over models trained solely on gold-annotated long-form texts. Ultimately, we find that, although projected silver training data improves parser performance across different news outlets, the improvement is moderated by constructions idiosyncratic to outlet. | Adrian Benton, Tianze Shi, Ozan Irsoy, Igor Malioutov |  |
| 489 |  |  [BOOKSUM: A Collection of Datasets for Long-form Narrative Summarization](https://doi.org/10.18653/v1/2022.findings-emnlp.488) |  | 0 | The majority of existing text summarization datasets include short-form source documents that lack long-range causal and temporal dependencies, and often contain strong layout and stylistic biases. While relevant, such datasets will offer limited challenges for future text summarization systems. We address these issues by introducing BOOKSUM, a collection of datasets for long-form narrative summarization. Our dataset covers documents from the literature domain, such as novels, plays and stories, and includes highly abstractive, human written summaries on three levels of granularity of increasing difficulty: paragraph-, chapter-, and book-level. The domain and structure of our dataset poses a unique set of challenges for summarization systems, which include: processing very long documents, non-trivial causal and temporal dependencies, and rich discourse structures. To facilitate future work, we trained and evaluated multiple extractive and abstractive summarization models as baselines for our dataset. | Wojciech Kryscinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, Dragomir Radev |  |
| 490 |  |  [Not All Errors are Equal: Learning Text Generation Metrics using Stratified Error Synthesis](https://doi.org/10.18653/v1/2022.findings-emnlp.489) |  | 0 | Is it possible to build a general and automatic natural language generation (NLG) evaluation metric? Existing learned metrics either perform unsatisfactorily or are restricted to tasks where large human rating data is already available. We introduce SESCORE, a model-based metric that is highly correlated with human judgements without requiring human annotation, by utilizing a novel, iterative error synthesis and severity scoring pipeline. This pipeline applies a series of plausible errors to raw text and assigns severity labels by simulating human judgements with entailment. We evaluate SESCORE against existing metrics by comparing how their scores correlate with human ratings. SESCORE outperforms all prior unsupervised metrics on multiple diverse NLG tasks including machine translation, image captioning, and WebNLG text generation. For WMT 20/21En-De and Zh-En, SESCORE improve the average Kendall correlation with human judgement from 0.154 to 0.195. SESCORE even achieves comparable performance to the best supervised metric COMET, despite receiving no human annotated training data. | Wenda Xu, YiLin Tuan, Yujie Lu, Michael Saxon, Lei Li, William Yang Wang |  |
| 491 |  |  [Summarization as Indirect Supervision for Relation Extraction](https://doi.org/10.18653/v1/2022.findings-emnlp.490) |  | 0 | Relation extraction (RE) models have been challenged by their reliance on training data with expensive annotations. Considering that summarization tasks aim at acquiring concise expressions of synoptical information from the longer context, these tasks naturally align with the objective of RE, i.e., extracting a kind of synoptical information that describes the relation of entity mentions. We present SuRE, which converts RE into a summarization formulation. SuRE leads to more precise and resource-efficient RE based on indirect supervision from summarization tasks. To achieve this goal, we develop sentence and relation conversion techniques that essentially bridge the formulation of summarization and RE tasks. We also incorporate constraint decoding techniques with Trie scoring to further enhance summarization-based RE with robust inference. Experiments on three RE datasets demonstrate the effectiveness of SuRE in both full-dataset and low-resource settings, showing that summarization is a promising source of indirect supervision signals to improve RE models. | Keming Lu, IHung Hsu, Wenxuan Zhou, Mingyu Derek Ma, Muhao Chen |  |
| 492 |  |  [DIGAT: Modeling News Recommendation with Dual-Graph Interaction](https://doi.org/10.18653/v1/2022.findings-emnlp.491) |  | 0 | News recommendation (NR) is essential for online news services. Existing NR methods typically adopt a news-user representation learning framework, facing two potential limitations. First, in news encoder, single candidate news encoding suffers from an insufficient semantic information problem. Second, existing graph-based NR methods are promising but lack effective news-user feature interaction, rendering the graph-based recommendation suboptimal. To overcome these limitations, we propose dual-interactive graph attention networks (DIGAT) consisting of news- and user-graph channels. In the news-graph channel, we enrich the semantics of single candidate news by incorporating the semantically relevant news information with a semantic-augmented graph (SAG). In the user-graph channel, multi-level user interests are represented with a news-topic graph. Most notably, we design a dual-graph interaction process to perform effective feature interaction between the news and user graphs, which facilitates accurate news-user representation matching. Experiment results on the benchmark dataset MIND show that DIGAT outperforms existing news recommendation methods. Further ablation studies and analyses validate the effectiveness of (1) semantic-augmented news graph modeling and (2) dual-graph interaction. | Zhiming Mao, Jian Li, Hongru Wang, Xingshan Zeng, KamFai Wong |  |
| 493 |  |  [SMASH: Improving SMAll Language Models' Few-SHot Ability with Prompt-Based Distillation](https://doi.org/10.18653/v1/2022.findings-emnlp.492) |  | 0 | Large-scale language models coupled with prompts have shown remarkable performance on few-shot learning. However, through systematic experiments, we find that the few-shot performance of small language models is poor, and using prompts on them brings fewer improvements than on larger ones. In this paper, we propose SMASH, an approach to improve SMAll language models’ few-SHot ability by training on intermediate tasks before prompt-based fine-tuning on downstream tasks. We design intermediate tasks for sentence-pair tasks and sentiment classification tasks by creating training examples with prompt templates similar to downstream tasks using sentences sampled from a large-scale unsupervised corpus, and apply knowledge distillation to distill from outputs of larger pre-trained models as the training objective. We conduct extensive experiments and show that SMASH can make a 6-layer DistilRoBRETa-base achieve comparable performance on few-shot datasets with a 12-layer RoBERTa-base at a low cost. | Yueqian Wang, Chang Liu, Kai Chen, Xi Wang, Dongyan Zhao |  |
| 494 |  |  [Consecutive Question Generation via Dynamic Multitask Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.493) |  | 0 | In this paper, we propose the task of consecutive question generation (CQG), which generates a set of logically related question-answer pairs to understand a whole passage, with a comprehensive consideration of the aspects including accuracy, coverage, and informativeness.To achieve this, we first examine the four key elements of CQG, i.e., question, answer, rationale, and context history, and propose a novel dynamic multitask framework with one main task generating a question-answer pair, and four auxiliary tasks generating other elements. It directly helps the model generate good questions through both joint training and self-reranking. At the same time, to fully explore the worth-asking information in a given passage, we make use of the reranking losses to sample the rationales and search for the best question series globally.Finally, we measure our strategy by QA data augmentation and manual evaluation, as well as a novel application of generated question-answer pairs on DocNLI. We prove that our strategy can improve question generation significantly and benefit multiple related NLP tasks. | Yunji Li, Sujian Li, Xing Shi |  |
| 495 |  |  [Subword Segmental Language Modelling for Nguni Languages](https://doi.org/10.18653/v1/2022.findings-emnlp.494) |  | 0 | Subwords have become the standard units of text in NLP, enabling efficient open-vocabulary models. With algorithms like byte-pair encoding (BPE), subword segmentation is viewed as a preprocessing step applied to the corpus before training. This can lead to sub-optimal segmentations for low-resource languages with complex morphologies. We propose a subword segmental language model (SSLM) that learns how to segment words while being trained for autoregressive language modelling. By unifying subword segmentation and language modelling, our model learns subwords that optimise LM performance. We train our model on the 4 Nguni languages of South Africa. These are low-resource agglutinative languages, so subword information is critical. As an LM, SSLM outperforms existing approaches such as BPE-based models on average across the 4 languages. Furthermore, it outperforms standard subword segmenters on unsupervised morphological segmentation. We also train our model as a word-level sequence model, resulting in an unsupervised morphological segmenter that outperforms existing methods by a large margin for all 4 languages. Our results show that learning subword segmentation is an effective alternative to existing subword segmenters, enabling the model to discover morpheme-like subwords that improve its LM capabilities. | Francois Meyer, Jan Buys |  |
| 496 |  |  [Towards Robust Visual Question Answering: Making the Most of Biased Samples via Contrastive Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.495) |  | 0 | Models for Visual Question Answering (VQA) often rely on the spurious correlations, i.e., the language priors, that appear in the biased samples of training set, which make them brittle against the out-of-distribution (OOD) test data. Recent methods have achieved promising progress in overcoming this problem by reducing the impact of biased samples on model training. However, these models reveal a trade-off that the improvements on OOD data severely sacrifice the performance on the in-distribution (ID) data (which is dominated by the biased samples). Therefore, we propose a novel contrastive learning approach, MMBS, for building robust VQA models by Making the Most of Biased Samples. Specifically, we construct positive samples for contrastive learning by eliminating the information related to spurious correlation from the original training samples and explore several strategies to use the constructed positive samples for training. Instead of undermining the importance of biased samples in model training, our approach precisely exploits the biased samples for unbiased information that contributes to reasoning. The proposed method is compatible with various VQA backbones. We validate our contributions by achieving competitive performance on the OOD dataset VQA-CP v2 while preserving robust performance on the ID dataset VQA v2. | Qingyi Si, Yuanxin Liu, Fandong Meng, Zheng Lin, Peng Fu, Yanan Cao, Weiping Wang, Jie Zhou |  |
| 497 |  |  [P3LM: Probabilistically Permuted Prophet Language Modeling for Generative Pre-Training](https://doi.org/10.18653/v1/2022.findings-emnlp.496) |  | 0 | Conventional autoregressive left-to-right (L2R) sequence generation faces two issues during decoding: limited to unidirectional target sequence modeling, and constrained on strong local dependencies.To address the aforementioned problem, we propose P3LM, a probabilistically permuted prophet language model, which strengthens the modeling of bidirectional information and long token dependencies for sequence generation.Specifically, P3LM learns to generate tokens in permuted order upon an order-aware transformer decoder, as well as to generate the corresponding future N tokens with a multi-stream attention mechanism.Extensive experiments are conducted on the GLGE benchmark, which includes four datasets for summarization, two for question generation, one for conversational question answering, and one for dialog response generation, where P3LM achieves state-of-the-art results compared with strong publicly available generative pre-training methods. | Junwei Bao, Yifan Wang, Ying Jiangyong, Yeyun Gong, Jing Zhao, Youzheng Wu, Xiaodong He |  |
| 498 |  |  [Holistic Sentence Embeddings for Better Out-of-Distribution Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.497) |  | 0 | Detecting out-of-distribution (OOD) instances is significant for the safe deployment of NLP models. Among recent textual OOD detection works based on pretrained language models (PLMs), distance-based methods have shown superior performance. However, they estimate sample distance scores in the last-layer CLS embedding space and thus do not make full use of linguistic information underlying in PLMs. To address the issue, we propose to boost OOD detection by deriving more holistic sentence embeddings. On the basis of the observations that token averaging and layer combination contribute to improving OOD detection, we propose a simple embedding approach named Avg-Avg, which averages all token representations from each intermediate layer as the sentence embedding and significantly surpasses the state-of-the-art on a comprehensive suite of benchmarks by a 9.33% FAR95 margin. Furthermore, our analysis demonstrates that it indeed helps preserve general linguistic knowledge in fine-tuned PLMs and substantially benefits detecting background shifts. The simple yet effective embedding method can be applied to fine-tuned PLMs with negligible extra costs, providing a free gain in OOD detection. Our code is available at https://github.com/lancopku/Avg-Avg. | Sishuo Chen, Xiaohan Bi, Rundong Gao, Xu Sun |  |
| 499 |  |  [MuGER2: Multi-Granularity Evidence Retrieval and Reasoning for Hybrid Question Answering](https://doi.org/10.18653/v1/2022.findings-emnlp.498) |  | 0 | Hybrid question answering (HQA) aims to answer questions over heterogeneous data, including tables and passages linked to table cells. The heterogeneous data can provide different granularity evidence to HQA models, e.t., column, row, cell, and link. Conventional HQA models usually retrieve coarse- or fine-grained evidence to reason the answer. Through comparison, we find that coarse-grained evidence is easier to retrieve but contributes less to the reasoner, while fine-grained evidence is the opposite. To preserve the advantage and eliminate the disadvantage of different granularity evidence, we propose MuGER2, a Multi-Granularity Evidence Retrieval and Reasoning approach. In evidence retrieval, a unified retriever is designed to learn the multi-granularity evidence from the heterogeneous data. In answer reasoning, an evidence selector is proposed to navigate the fine-grained evidence for the answer reader based on the learned multi-granularity evidence. Experiment results on the HybridQA dataset show that MuGER2 significantly boosts the HQA performance. Further ablation analysis verifies the effectiveness of both the retrieval and reasoning designs. | Yingyao Wang, Junwei Bao, Chaoqun Duan, Youzheng Wu, Xiaodong He, Tiejun Zhao |  |
| 500 |  |  [EntityCS: Improving Zero-Shot Cross-lingual Transfer with Entity-Centric Code Switching](https://doi.org/10.18653/v1/2022.findings-emnlp.499) |  | 0 | Accurate alignment between languages is fundamental for improving cross-lingual pre-trained language models (XLMs). Motivated by the natural phenomenon of code-switching (CS) in multilingual speakers, CS has been used as an effective data augmentation method that offers language alignment at word- or phrase-level, in contrast to sentence-level via parallel instances. Existing approaches either use dictionaries or parallel sentences with word-alignment to generate CS data by randomly switching words in a sentence. However, such methods can be suboptimal as dictionaries disregard semantics, and syntax might become invalid after random word switching. In this work, we propose EntityCS, a method that focuses on Entity-level Code-Switching to capture fine-grained cross-lingual semantics without corrupting syntax. We use Wikidata and the English Wikipedia to construct an entity-centric CS corpus by switching entities to their counterparts in other languages. We further propose entity-oriented masking strategies during intermediate model training on the EntityCS corpus for improving entity prediction. Evaluation of the trained models on four entity-centric downstream tasks shows consistent improvements over the baseline with a notable increase of 10% in Fact Retrieval. We release the corpus and models to assist research on code-switching and enriching XLMs with external knowledge. | Chenxi Whitehouse, Fenia Christopoulou, Ignacio Iacobacci |  |
| 501 |  |  [MBTI Personality Prediction for Fictional Characters Using Movie Scripts](https://doi.org/10.18653/v1/2022.findings-emnlp.500) |  | 0 | An NLP model that understands stories should be able to understand the characters in them. To support the development of neural models for this purpose, we construct a benchmark, Story2Personality. The task is to predict a movie character’s MBTI or Big 5 personality types based on the narratives of the character. Experiments show that our task is challenging for the existing text classification models, as none is able to largely outperform random guesses. We further proposed a multi-view model for personality prediction using both verbal and non-verbal descriptions, which gives improvement compared to using only verbal descriptions. The uniqueness and challenges in our dataset call for the development of narrative comprehension techniques from the perspective of understanding characters. | Yisi Sang, Xiangyang Mou, Mo Yu, Dakuo Wang, Jing Li, Jeffrey M. Stanton |  |
| 502 |  |  [A Simple and Strong Baseline for End-to-End Neural RST-style Discourse Parsing](https://doi.org/10.18653/v1/2022.findings-emnlp.501) |  | 0 | To promote and further develop RST-style discourse parsing models, we need a strong baseline that can be regarded as a reference for reporting reliable experimental results. This paper explores a strong baseline by integrating existing simple parsing strategies, top-down and bottom-up, with various transformer-based pre-trained language models.The experimental results obtained from two benchmark datasets demonstrate that the parsing performance strongly relies on the pre-trained language models rather than the parsing strategies.In particular, the bottom-up parser achieves large performance gains compared to the current best parser when employing DeBERTa.We further reveal that language models with a span-masking scheme especially boost the parsing performance through our analysis within intra- and multi-sentential parsing, and nuclearity prediction. | Naoki Kobayashi, Tsutomu Hirao, Hidetaka Kamigaito, Manabu Okumura, Masaaki Nagata |  |
| 503 |  |  [Probing for Constituency Structure in Neural Language Models](https://doi.org/10.18653/v1/2022.findings-emnlp.502) |  | 0 | In this paper, we investigate to which extent contextual neural language models (LMs) implicitly learn syntactic structure. More concretely, we focus on constituent structure as represented in the Penn Treebank (PTB). Using standard probing techniques based on diagnostic classifiers, we assess the accuracy of representing constituents of different categories within the neuron activations of a LM such as RoBERTa. In order to make sure that our probe focuses on syntactic knowledge and not on implicit semantic generalizations, we also experiment on a PTB version that is obtained by randomly replacing constituents with each other while keeping syntactic structure, i.e., a semantically ill-formed but syntactically well-formed version of the PTB. We find that 4 pretrained transfomer LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM. Moreover, we show that a complete constituency tree can be linearly separated from LM representations. | David Arps, Younes Samih, Laura Kallmeyer, Hassan Sajjad |  |
| 504 |  |  [Table-To-Text generation and pre-training with TabT5](https://doi.org/10.18653/v1/2022.findings-emnlp.503) |  | 0 | Encoder-only transformer models have been successfully applied to different table understanding tasks, as in TAPAS. A major limitation of these architectures is that they are constrained to classification-like tasks such as cell selection or entailment detection. We present TabT5, an encoder-decoder model that generates natural language text based on tables and textual inputs. TabT5 overcomes the encoder-only limitation by incorporating a decoder component and leverages the input structure with table specific embeddings and pre-training. TabT5 achieves new state-of-the-art results on several domains, including spreadsheet formula prediction with a 15% increase in sequence accuracy, QA with a 2.5% increase in sequence accuracy and data-to-text generation with a 2.5% increase in BLEU. | Ewa Andrejczuk, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Yasemin Altun |  |
| 505 |  |  [A POMDP Dialogue Policy with 3-way Grounding and Adaptive Sensing for Learning through Communication](https://doi.org/10.18653/v1/2022.findings-emnlp.504) |  | 0 | Agents to assist with rescue, surgery, and similar activities could collaborate better with humans if they could learn new strategic behaviors through communication. We introduce a novel POMDP dialogue policy for learning from people. The policy has 3-way grounding of language in the shared physical context, the dialogue context, and persistent knowledge. It can learn distinct but related games, and can continue learning across dialogues for complex games. A novel sensing component supports adaptation to information-sharing differences across people. The single policy performs better than oracle policies customized to specific games and information behavior. | Maryam Zare, Alan R. Wagner, Rebecca J. Passonneau |  |
| 506 |  |  [PaCo: Preconditions Attributed to Commonsense Knowledge](https://doi.org/10.18653/v1/2022.findings-emnlp.505) |  | 0 | Humans can seamlessly reason with circumstantial preconditions of commonsense knowledge. We understand that a glass is used for drinking water, unless the glass is broken or the water is toxic. Despite state-of-the-art (SOTA) language models’ (LMs) impressive performance on inferring commonsense knowledge, it is unclear whether they understand the circumstantial preconditions. To address this gap, we propose a novel challenge of reasoning with circumstantial preconditions. We collect a dataset, called PaCo, consisting of 12.4 thousand preconditions of commonsense statements expressed in natural language. Based on this dataset, we create three canonical evaluation tasks and use them to examine the capability of existing LMs to understand situational preconditions. Our results reveal a 10-30% gap between machine and human performance on our tasks, which shows that reasoning with preconditions is an open challenge. | Ehsan Qasemi, Filip Ilievski, Muhao Chen, Pedro A. Szekely |  |
| 507 |  |  [Improving Few-Shot Domain Transfer for Named Entity Disambiguation with Pattern Exploitation](https://doi.org/10.18653/v1/2022.findings-emnlp.506) |  | 0 | Named entity disambiguation (NED) is a critical subtask of entity linking, which seeks to connect knowledge base entities with textual mentions of those entities. Naturally, the performance of a model depends on the domain it was trained on; thus, reducing the amount of data required to train models is advantageous. In this work, we leverage recent research on pattern exploitation for NED and explore whether it can reduce the amount of data required for domain adaptation by reformulating the disambiguation task as a masked language modeling problem. Using ADAPET (Tam et al., 2021), which implements a new approach for few-shot learning using fine-tuned transformer-based language models, we produce an NED model which yields, without any sacrifice of in-domain accuracy, a 7% improvement in zero-shot cross-domain performance as evaluated on NEDMed, a new NED dataset of mental health news which we release with this work. | Philip Blair, Kfir Bar |  |
| 508 |  |  [Capturing Topic Framing via Masked Language Modeling](https://doi.org/10.18653/v1/2022.findings-emnlp.507) |  | 0 | Differential framing of issues can lead to divergent world views on important issues. This is especially true in domains where the information presented can reach a large audience, such as traditional and social media. Scalable and reliable measurement of such differential framing is an important first step in addressing them. In this work, based on the intuition that framing affects the tone and word choices in written language, we propose a framework for modeling the differential framing of issues through masked token prediction via large-scale fine-tuned language models (LMs). Specifically, we explore three key factors for our framework: 1) prompt generation methods for the masked token prediction; 2) methods for normalizing the output of fine-tuned LMs; 3) robustness to the choice of pre-trained LMs used for fine-tuning. Through experiments on a dataset of articles from traditional media outlets covering five diverse and politically polarized topics, we show that our framework can capture differential framing of these topics with high reliability. | Xiaobo Guo, Weicheng Ma, Soroush Vosoughi |  |
| 509 |  |  [WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation](https://doi.org/10.18653/v1/2022.findings-emnlp.508) |  | 0 | A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel approach for dataset creation based on worker and AI collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers. The resulting dataset, WANLI, consists of 107,885 NLI examples and presents unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI improves performance on eight out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI, compared to training on the 4x larger MultiNLI. Moreover, it continues to be more effective than MultiNLI augmented with other NLI datasets. Our results demonstrate the promise of leveraging natural language generation techniques and re-imagining the role of humans in the dataset creation process. | Alisa Liu, Swabha Swayamdipta, Noah A. Smith, Yejin Choi |  |
| 510 |  |  [Sequentially Controlled Text Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.509) |  | 0 | While GPT-2 generates sentences that are remarkably human-like, longer documents can ramble and do not follow human-like writing structure. We study the problem of imposing structure on long-range text. We propose a novel controlled text generation task, sequentially controlled text generation, and identify a dataset, NewsDiscourse as a starting point for this task. We develop a sequential controlled text generation pipeline with generation and editing. We test different degrees of structural awareness and show that, in general, more structural awareness results in higher control- accuracy, grammaticality, coherency and topicality, approaching human-level writing performance. | Alexander Spangher, Yao Ming, Xinyu Hua, Nanyun Peng |  |
| 511 |  |  [Revisiting the Roles of "Text" in Text Games](https://doi.org/10.18653/v1/2022.findings-emnlp.510) |  | 0 | Text games present opportunities for natural language understanding (NLU) methods to tackle reinforcement learning (RL) challenges. However, recent work has questioned the necessity of NLU by showing random text hashes could perform decently. In this paper, we pursue a fine-grained investigation into the roles of text in the face of different RL challenges, and reconcile that semantic and non-semantic language representations could be complementary rather than contrasting. Concretely, we propose a simple scheme to extract relevant contextual information into an approximate state hash as extra input for an RNN-based text agent. Such a lightweight plug-in achieves competitive performance with state-of-the-art text agents using advanced NLU techniques such as knowledge graph and passage retrieval, suggesting non-NLU methods might suffice to tackle the challenge of partial observability. However, if we remove RNN encoders and use approximate or even ground-truth state hash alone, the model performs miserably, which confirms the importance of semantic function approximation to tackle the challenge of combinatorially large observation and action spaces. Our findings and analysis provide new insights for designing better text game task setups and agents. | Yi Gu, Shunyu Yao, Chuang Gan, Josh Tenenbaum, Mo Yu |  |
| 512 |  |  [FPT: Improving Prompt Tuning Efficiency via Progressive Training](https://doi.org/10.18653/v1/2022.findings-emnlp.511) |  | 0 | Recently, prompt tuning (PT) has gained increasing attention as a parameter-efficient way of tuning pre-trained language models (PLMs). Despite extensively reducing the number of tunable parameters and achieving satisfying performance, PT is training-inefficient due to its slow convergence. To improve PT’s training efficiency, we first make some novel observations about the prompt transferability of “partial PLMs”, which are defined by compressing a PLM in depth or width. We observe that the soft prompts learned by different partial PLMs of various sizes are similar in the parameter space, implying that these soft prompts could potentially be transferred among partial PLMs. Inspired by these observations, we propose Fast Prompt Tuning (FPT), which starts by conducting PT using a small-scale partial PLM, and then progressively expands its depth and width until the full-model size. After each expansion, we recycle the previously learned soft prompts as initialization for the enlarged partial PLM and then proceed PT. We demonstrate the feasibility of FPT on 5 tasks and show that FPT could save over 30% training computations while achieving comparable performance. The codes are publicly available at https://github.com/thunlp/FastPromptTuning. | Yufei Huang, Yujia Qin, Huadong Wang, Yichun Yin, Maosong Sun, Zhiyuan Liu, Qun Liu |  |
| 513 |  |  [Prompt-learning for Fine-grained Entity Typing](https://doi.org/10.18653/v1/2022.findings-emnlp.512) |  | 0 | As an effective approach to adapting pre-trained language models (PLMs) for specific tasks, prompt-learning has recently attracted much attention from researchers. By using cloze-style language prompts to stimulate the versatile knowledge of PLMs, prompt-learning can achieve promising results on a series of NLP tasks, such as natural language inference, sentiment classification, and knowledge probing. In this work, we investigate the application of prompt-learning on fine-grained entity typing in fully supervised, few-shot, and zero-shot scenarios. We first develop a simple and effective prompt-learning pipeline by constructing entity-oriented verbalizers and templates and conducting masked language modeling. Further, to tackle the zero-shot regime, we propose a self-supervised strategy that carries out distribution-level optimization in prompt-learning to automatically summarize the information of entity types. Extensive experiments on four fine-grained entity typing benchmarks under fully supervised, few-shot, and zero-shot settings show the effectiveness of the prompt-learning paradigm and further make a powerful alternative to vanilla fine-tuning. | Ning Ding, Yulin Chen, Xu Han, Guangwei Xu, Xiaobin Wang, Pengjun Xie, Haitao Zheng, Zhiyuan Liu, Juanzi Li, HongGee Kim |  |
| 514 |  |  [TransLIST: A Transformer-Based Linguistically Informed Sanskrit Tokenizer](https://doi.org/10.18653/v1/2022.findings-emnlp.513) |  | 0 | Sanskrit Word Segmentation (SWS) is essential in making digitized texts available and in deploying downstream tasks. It is, however, non-trivial because of the sandhi phenomenon that modifies the characters at the word boundaries, and needs special treatment. Existing lexicon driven approaches for SWS make use of Sanskrit Heritage Reader, a lexicon-driven shallow parser, to generate the complete candidate solution space, over which various methods are applied to produce the most valid solution. However, these approaches fail while encountering out-of-vocabulary tokens. On the other hand, purely engineering methods for SWS have made use of recent advances in deep learning, but cannot make use of the latent word information on availability. To mitigate the shortcomings of both families of approaches, we propose Transformer based Linguistically Informed Sanskrit Tokenizer (TransLIST) consisting of (1) a module that encodes the character input along with latent-word information, which takes into account the sandhi phenomenon specific to SWS and is apt to work with partial or no candidate solutions, (2) a novel soft-masked attention to prioritize potential candidate words and (3) a novel path ranking algorithm to rectify the corrupted predictions. Experiments on the benchmark datasets for SWS show that TransLIST outperforms the current state-of-the-art system by an average 7.2 points absolute gain in terms of perfect match (PM) metric. | Jivnesh Sandhan, Rathin Singha, Narein Rao, Suvendu Samanta, Laxmidhar Behera, Pawan Goyal |  |
| 515 |  |  [Fair NLP Models with Differentially Private Text Encoders](https://doi.org/10.18653/v1/2022.findings-emnlp.514) |  | 0 | Encoded text representations often capture sensitive attributes about individuals (e.g., race or gender), which raise privacy concerns and can make downstream models unfair to certain groups. In this work, we propose FEDERATE, an approach that combines ideas from differential privacy and adversarial training to learn private text representations which also induces fairer models. We empirically evaluate the trade-off between the privacy of the representations and the fairness and accuracy of the downstream model on four NLP datasets. Our results show that FEDERATE consistently improves upon previous methods, and thus suggest that privacy and fairness can positively reinforce each other. | Gaurav Maheshwari, Pascal Denis, Mikaela Keller, Aurélien Bellet |  |
| 516 |  |  [Modeling Context With Linear Attention for Scalable Document-Level Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.515) |  | 0 | Document-level machine translation leverages inter-sentence dependencies to produce more coherent and consistent translations. However, these models, predominantly based on transformers, are difficult to scale to long documents as their attention layers have quadratic complexity in the sequence length. Recent efforts on efficient attention improve scalability, but their effect on document translation remains unexplored. In this work, we investigate the efficacy of a recent linear attention model by Peng et al. (2021) on document translation and augment it with a sentential gate to promote a recency inductive bias. We evaluate the model on IWSLT 2015 and OpenSubtitles 2018 against the transformer, demonstrating substantially increased decoding speed on long sequences with similar or better BLEU scores. We show that sentential gating further improves translation quality on IWSLT. | Zhaofeng Wu, Hao Peng, Nikolaos Pappas, Noah A. Smith |  |
| 517 |  |  [What do Large Language Models Learn beyond Language?](https://doi.org/10.18653/v1/2022.findings-emnlp.516) |  | 0 | Large language models (LMs) have rapidly become a mainstay in Natural Language Processing. These models are known to acquire rich linguistic knowledge from training on large amounts of text. In this paper, we investigate if pre-training on text also confers these models with helpful ‘inductive biases’ for non-linguistic reasoning. On a set of 19 diverse non-linguistic tasks involving quantitative computations, recognizing regular expressions and reasoning over strings. We find that pretrained models significantly outperform comparable non-pretrained neural models. This remains true also in experiments with training non-pretrained models with fewer parameters to account for model regularization effects. We further explore the effect of text domain on LMs by pretraining models from text from different domains and provenances. Our experiments surprisingly reveal that the positive effects of pre-training persist even when pretraining on multi-lingual text or computer code, and even for text generated from synthetic languages. Our findings suggest a hithertho unexplored deep connection between pre-training and inductive learning abilities of language models | Avinash Madasu, Shashank Srivastava |  |
| 518 |  |  [CONSISTENT: Open-Ended Question Generation From News Articles](https://doi.org/10.18653/v1/2022.findings-emnlp.517) |  | 0 | Recent work on question generation has largely focused on factoid questions such as who, what,where, when about basic facts. Generating open-ended why, how, what, etc. questions thatrequire long-form answers have proven more difficult. To facilitate the generation of openended questions, we propose CONSISTENT, a new end-to-end system for generating openended questions that are answerable from and faithful to the input text. Using news articles asa trustworthy foundation for experimentation, we demonstrate our model’s strength over several baselines using both automatic and human based evaluations. We contribute an evaluationdataset of expert-generated open-ended questions. We discuss potential downstream applications for news media organizations. | Tuhin Chakrabarty, Justin Lewis, Smaranda Muresan |  |
| 519 |  |  [Efficient (Soft) Q-Learning for Text Generation with Limited Good Data](https://doi.org/10.18653/v1/2022.findings-emnlp.518) |  | 0 | Maximum likelihood estimation (MLE) is the predominant algorithm for training text generation models. This paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models. Reinforcement learning (RL) on the other hand offers a more flexible solution by allowing users to plug in arbitrary task metrics as reward. Yet previous RL algorithms for text generation, such as policy gradient (on-policy RL) and Q-learning (off-policy RL), are often notoriously inefficient or unstable to train due to the large sequence space and the sparse reward received only at the end of sequences. In this paper, we introduce a new RL formulation for text generation from the soft Q-learning (SQL) perspective. It enables us to draw from the latest RL advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward. We apply the approach to a wide range of novel text generation tasks, including learning from noisy/negative examples, adversarial attacks, and prompt generation. Experiments show our approach consistently outperforms both task-specialized algorithms and the previous RL methods. | Han Guo, Bowen Tan, Zhengzhong Liu, Eric P. Xing, Zhiting Hu |  |
| 520 |  |  [Lexi: Self-Supervised Learning of the UI Language](https://doi.org/10.18653/v1/2022.findings-emnlp.519) |  | 0 | Humans can learn to operate the user interface (UI) of an application by reading an instruction manual or how-to guide. Along with text, these resources include visual content such as UI screenshots and images of application icons referenced in the text. We explore how to leverage this data to learn generic visio-linguistic representations of UI screens and their components. These representations are useful in many real applications, such as accessibility, voice navigation, and task automation. Prior UI representation models rely on UI metadata (UI trees and accessibility labels), which is often missing, incompletely defined, or not accessible. We avoid such a dependency, and propose Lexi, a pre-trained vision and language model designed to handle the unique features of UI screens, including their text richness and context sensitivity. To train Lexi we curate the UICaption dataset consisting of 114k UI images paired with descriptions of their functionality. We evaluate Lexi on four tasks: UI action entailment, instruction-based UI image retrieval, grounding referring expressions, and UI entity recognition. | Pratyay Banerjee, Shweti Mahajan, Kushal Arora, Chitta Baral, Oriana Riva |  |
| 521 |  |  [Inferring the Reader: Guiding Automated Story Generation with Commonsense Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.520) |  | 0 | Transformer-based language model approaches to automated story generation currently provide state-of-the-art results. However, they still suffer from plot incoherence when generatingnarratives over time, and critically lack basiccommonsense reasoning. Furthermore, existing methods generally focus only on single-character stories, or fail to track charactersat all. To improve the coherence of generated narratives and to expand the scope ofcharacter-centric narrative generation, we introduce Commonsense-inference Augmentedneural StoryTelling (CAST), a framework forintroducing commonsense reasoning into thegeneration process with the option to model theinteraction between multiple characters. Wefind that our CAST method produces significantly more coherent, on-topic, enjoyable andfluent stories than existing models in both thesingle-character and two-character settings inthree storytelling domains. | Xiangyu Peng, Siyan Li, Sarah Wiegreffe, Mark O. Riedl |  |
| 522 |  |  [How to Stop an Avalanche? JoDeM: Joint Decision Making through Compare and Contrast for Dialog State Tracking](https://doi.org/10.18653/v1/2022.findings-emnlp.521) |  | 0 | Dialog state tracking (DST) is a core component in task-oriented dialog systems. Existing state-of-the-art DST model incorporates insight and intuition from the human experience into design of supplementary labels, which greatly assisted the training process of turn-by-turn DST model. Though the turn-by-turn scheme and supplementary labels enabled satisfactory performance on the task, most of the DST models of this fashion label or process the raw dialogue data on the premise that the last turn dialogue state is always correct, which is usually not the case. In this paper, we address the negative impact resulted from the premise above as the avalanche phenomenon. After that, we propose JoDeM, a state-of-the-art DST model which can tackle the Avalanche phenomenon with two mechanisms. First mechanism is a jointly decision making method to extract key information from the dialogue. Second mechanism is a compare and contrast dialogue update technique to prevent error accumulation. Example study and graph analysis are presented to support our claim about the harmfulness of avalanche phenomenon. We also conduct quantitative and qualitative experiments on the high quality MultiWOZ2.3 corpus dataset to demonstrate that the proposed model not only outperforms the existing state-of-the-art methods, but also proves the validity of solving avalanche degradation problem. | Haoming Wang, Wang Xin |  |
| 523 |  |  [Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding](https://doi.org/10.18653/v1/2022.findings-emnlp.522) |  | 0 | Contrastive learning has become a new paradigm for unsupervised sentence embeddings.Previous studies focus on instance-wise contrastive learning, attempting to construct positive pairs with textual data augmentation. In this paper, we propose a novel Contrastive learning method with Prompt-derived Virtual semantic Prototypes (ConPVP). Specifically, with the help of prompts, we construct virtual semantic prototypes to each instance, and derive negative prototypes by using the negative form of the prompts.Using a prototypical contrastive loss, we enforce the anchor sentence embedding to be close to its corresponding semantic prototypes, and far apart from the negative prototypes as well as the prototypes of other sentences.Extensive experimental results on semantic textual similarity, transfer, and clustering tasks demonstrate the effectiveness of our proposed model compared to strong baselines.Code is available at https://github.com/lemon0830/promptCSE. | Jiali Zeng, Yongjing Yin, Yufan Jiang, Shuangzhi Wu, Yunbo Cao |  |
| 524 |  |  [Weight Perturbation as Defense against Adversarial Word Substitutions](https://doi.org/10.18653/v1/2022.findings-emnlp.523) |  | 0 | The existence and pervasiveness of textual adversarial examples have raised serious concerns to security-critical applications. Many methods have been developed to defend against adversarial attacks for neural natural language processing (NLP) models.Adversarial training is one of the most successful defense methods by adding some random or intentional perturbations to the original input texts and making the models robust to the perturbed examples.In this study, we explore the feasibility of improving the adversarial robustness of NLP models by performing perturbations in the parameter space rather than the input feature space.The weight perturbation helps to find a better solution (i.e., the values of weights) that minimizes the adversarial loss among other feasible solutions.We found that the weight perturbation can significantly improve the robustness of NLP models when it is combined with the perturbation in the input embedding space, yielding the highest accuracy on both clean and adversarial examples across different datasets. | Jianhan Xu, Linyang Li, Jiping Zhang, Xiaoqing Zheng, KaiWei Chang, ChoJui Hsieh, Xuanjing Huang |  |
| 525 |  |  [CORT: A New Baseline for Comparative Opinion Classification by Dual Prompts](https://doi.org/10.18653/v1/2022.findings-emnlp.524) |  | 0 | Comparative opinion is a common linguistic phenomenon. The opinion is expressed by comparing multiple targets on a shared aspect, e.g., “camera A is better than camera B in picture quality”. Among the various subtasks in opinion mining, comparative opinion classification is relatively less studied. Current solutions use rules or classifiers to identify opinions, i.e., better, worse, or same, through feature engineering. Because the features are directly derived from the input sentence, these solutions are sensitive to the order of the targets mentioned in the sentence. For example, “camera A is better than camera B” means the same as “camera B is worse than camera A”; but the features of these two sentences are completely different. In this paper, we approach comparative opinion classification through prompt learning, taking the advantage of embedded knowledge in pre-trained language model. We design a twin framework with dual prompts, named CORT. This extremely simple model delivers state-of-the-art and robust performance on all benchmark datasets for comparative opinion classification. We believe CORT well serves as a new baseline for comparative opinion classification. | Yequan Wang, Hengran Zhang, Aixin Sun, Xuying Meng |  |
| 526 |  |  [APEACH: Attacking Pejorative Expressions with Analysis on Crowd-Generated Hate Speech Evaluation Datasets](https://doi.org/10.18653/v1/2022.findings-emnlp.525) |  | 0 | In hate speech detection, developing training and evaluation datasets across various domains is the critical issue. Whereas, major approaches crawl social media texts and hire crowd-workers to annotate the data. Following this convention often restricts the scope of pejorative expressions to a single domain lacking generalization. Sometimes domain overlap between training corpus and evaluation set overestimate the prediction performance when pretraining language models on low-data language. To alleviate these problems in Korean, we propose APEACH that asks unspecified users to generate hate speech examples followed by minimal post-labeling. We find that APEACH can collect useful datasets that are less sensitive to the lexical overlaps between the pretraining corpus and the evaluation set, thereby properly measuring the model performance. | Kichang Yang, Wonjun Jang, WonIk Cho |  |
| 527 |  |  [Guiding Neural Story Generation with Reader Models](https://doi.org/10.18653/v1/2022.findings-emnlp.526) |  | 0 | Automated storytelling has long captured the attention of researchers for the ubiquity of narratives in everyday life. However, it is challenging to maintain coherence and stay on-topictoward a specific ending when generating narratives with neural language models. In this paper, we introduce Story generation with ReaderModels (StoRM), a framework in which areader model is used to reason about the storyshould progress. A reader model infers whata human reader believes about the concepts,entities, and relations about the fictional storyworld. We show how an explicit reader modelrepresented as a knowledge graph affords the storycoherence and provides controllability in theform of achieving a given story world stategoal. Experiments show that our model produces significantly more coherent and on-topicstories, outperforming baselines in dimensionsincluding plot plausibility and staying on topic | Xiangyu Peng, Kaige Xie, Amal Alabdulkarim, Harshith Kayam, Samihan Dani, Mark O. Riedl |  |
| 528 |  |  [Reason first, then respond: Modular Generation for Knowledge-infused Dialogue](https://doi.org/10.18653/v1/2022.findings-emnlp.527) |  | 0 | Large language models can produce fluent dialogue but often hallucinate factual inaccuracies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simultaneously. In this work, we propose a modular model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps. K2R first generates a knowledge sequence, given a dialogue context, as an intermediate step. After this “reasoning step”, the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In detailed experiments, we find that such a model hallucinates less in knowledge-grounded dialogue tasks, and has advantages in terms of interpretability and modularity. In particular, it can be used to fuse QA and dialogue systems together to enable dialogue agents to give knowledgeable answers, or QA models to give conversational responses in a zero-shot setting. | Leonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, Jason Weston |  |
| 529 |  |  [Adapting Multilingual Models for Code-Mixed Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.528) |  | 0 | The scarcity of gold standard code-mixed to pure language parallel data makes it difficult to train translation models reliably.Prior work has addressed the paucity of parallel data with data augmentation techniques.Such methods rely heavily on external resources making systems difficult to train and scale effectively for multiple languages.We present a simple yet highly effective two-stage back-translation based training scheme for adapting multilingual models to the task of code-mixed translation which eliminates dependence on external resources.We show a substantial improvement in translation quality (measured through BLEU), beating existing prior work by up to +3.8 BLEU on code-mixed Hi→En, Mr→En, and Bn→En tasks. On the LinCE Machine Translation leader board, we achieve the highest score for code-mixed Es→En, beating existing best baseline by +6.5 BLEU, and our own stronger baseline by +1.1 BLEU. | Aditya Vavre, Abhirut Gupta, Sunita Sarawagi |  |
| 530 |  |  [LPC: A Logits and Parameter Calibration Framework for Continual Learning](https://doi.org/10.18653/v1/2022.findings-emnlp.529) |  | 0 | When we execute the typical fine-tuning paradigm on continuously sequential tasks, the model will suffer from the catastrophic forgetting problem (i.e., the model tends to adjust old parameters according to the new knowledge, which leads to the loss of previously acquired concepts). People proposed replay-based methods by accessing old data from extra storage and maintaining the parameters of old concepts, which actually raise the privacy issue and larger memory requirements. In this work, we aim to achieve the sequential/continual learning of knowledge without accessing the old data. The core idea is to calibrate the parameters and logits (output) so that preserving old parameters and generalized learning on new concepts can be solved simultaneously. Our proposed framework includes two major components, Logits Calibration (LC) and Parameter Calibration (PC). The LC focuses on calibrating the learning of novel models with old models, and PC aims to preserve the parameters of old models. These two operations can maintain the old knowledge while learning new tasks without storing previous data. We conduct experiments on various scenarios of the GLUE (the General Language Understanding Evaluation) benchmark. The experimental results show that our model achieves state-of-the-art performance in all scenarios. | Xiaodi Li, Zhuoyi Wang, Dingcheng Li, Latifur Khan, Bhavani Thuraisingham |  |
| 531 |  |  [SlovakBERT: Slovak Masked Language Model](https://doi.org/10.18653/v1/2022.findings-emnlp.530) |  | 0 | We introduce a new Slovak masked language model called SlovakBERT. This is to our best knowledge the first paper discussing Slovak transformers-based language models. We evaluate our model on several NLP tasks and achieve state-of-the-art results. This evaluation is likewise the first attempt to establish a benchmark for Slovak language models. We publish the masked language model, as well as the fine-tuned models for part-of-speech tagging, sentiment analysis and semantic textual similarity. | Matús Pikuliak, Stefan Grivalsky, Martin Konopka, Miroslav Blsták, Martin Tamajka, Viktor Bachratý, Marián Simko, Pavol Balázik, Michal Trnka, Filip Uhlárik |  |
| 532 |  |  [Efficient Zero-shot Event Extraction with Context-Definition Alignment](https://doi.org/10.18653/v1/2022.findings-emnlp.531) |  | 0 | Event extraction (EE) is the task of identifying interested event mentions from text.Conventional efforts mainly focus on the supervised setting. However, these supervised models cannot generalize to event types out of the pre-defined ontology. To fill this gap, many efforts have been devoted to the zero-shot EE problem. This paper follows the trend of modeling event-type semantics but moves one step further. We argue that using the static embedding of the event type name might not be enough because a single word could be ambiguous, and we need a sentence to define the type semantics accurately. To model the definition semantics, we use two separate transformer models to project the contextualized event mentions and corresponding definitions into the same embedding space and then minimize their embedding distance via contrastive learning. On top of that, we also propose a warming phase to help the model learn the minor difference between similar definitions. We name our approach Zero-shot Event extraction with Definition (ZED). Experiments on the MAVEN dataset show that our model significantly outperforms all previous zero-shot EE methods with fast inference speed due to the disjoint design. Further experiments also show that can be easily applied to the few-shot setting when the annotation is available and consistently outperforms baseline supervised methods. | Hongming Zhang, Wenlin Yao, Dong Yu |  |
| 533 |  |  [Logical Fallacy Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.532) |  | 0 | Reasoning is central to human intelligence. However, fallacious arguments are common, and some exacerbate problems such as spreading misinformation about climate change. In this paper, we propose the task of logical fallacy detection, and provide a new dataset (Logic) of logical fallacies generally found in text, together with an additional challenge set for detecting logical fallacies in climate change claims (LogicClimate). Detecting logical fallacies is a hard problem as the model must understand the underlying logical structure of the argument. We find that existing pretrained large language models perform poorly on this task. In contrast, we show that a simple structure-aware classifier outperforms the best language model by 5.46% F1 scores on Logic and 4.51% on LogicClimate. We encourage future work to explore this task since (a) it can serve as a new reasoning challenge for language models, and (b) it can have potential applications in tackling the spread of misinformation. Our dataset and code are available at https://github.com/causalNLP/logical-fallacy | Zhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, Zhiheng Lyu, Mrinmaya Sachan, Rada Mihalcea, Bernhard Schölkopf |  |
| 534 |  |  [Topic-Aware Response Generation in Task-Oriented Dialogue with Unstructured Knowledge Access](https://doi.org/10.18653/v1/2022.findings-emnlp.533) |  | 0 | To alleviate the problem of structured databases’ limited coverage, recent task-oriented dialogue systems incorporate external unstructured knowledge to guide the generation of system responses. However, these usually use word or sentence level similarities to detect the relevant knowledge context, which only partially capture the topical level relevance. In this paper, we examine how to better integrate topical information in knowledge grounded task-oriented dialogue and propose “Topic-Aware Response Generation” (TARG), an end-to-end response generation model. TARG incorporates multiple topic-aware attention mechanisms to derive the importance weighting scheme over dialogue utterances and external knowledge sources towards a better understanding of the dialogue history. Experimental results indicate that TARG achieves state-of-the-art performance in knowledge selection and response generation, outperforming previous state-of-the-art by 3.2, 3.6, and 4.2 points in EM, F1 and BLEU-4 respectively on Doc2Dial, and performing comparably with previous work on DSTC9; both being knowledge-grounded task-oriented dialogue datasets. | Yue Feng, Gerasimos Lampouras, Ignacio Iacobacci |  |
| 535 |  |  [Revisiting Transformer-based Models for Long Document Classification](https://doi.org/10.18653/v1/2022.findings-emnlp.534) |  | 0 | The recent literature in text classification is biased towards short text sequences (e.g., sentences or paragraphs). In real-world applications, multi-page multi-paragraph documents are common and they cannot be efficiently encoded by vanilla Transformer-based models. We compare different Transformer-based Long Document Classification (TrLDC) approaches that aim to mitigate the computational overhead of vanilla transformers to encode much longer text, namely sparse attention and hierarchical encoding methods.We examine several aspects of sparse attention (e.g., size of local attention window, use of global attention) and hierarchical (e.g., document splitting strategy) transformers on four document classification datasets covering different domains. We observe a clear benefit from being able to process longer text, and, based on our results, we derive practical advice of applying Transformer-based models on long document classification tasks. | Xiang Dai, Ilias Chalkidis, Sune Darkner, Desmond Elliott |  |
| 536 |  |  [Time-aware Prompting for Text Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.535) |  | 0 | In this paper, we study the effects of incorporating timestamps, such as document creation dates, into generation systems. Two types of time-aware prompts are investigated: (1) textual prompts that encode document timestamps in natural language sentences; and (2) linear prompts that convert timestamps into continuous vectors. To explore extrapolation to future data points, we further introduce a new data-to-text generation dataset, TempWikiBio, containing more than 4 millions of chronologically ordered revisions of biographical articles from English Wikipedia, each paired with structured personal profiles.Through data-to-text generation on TempWikiBio, text-to-text generation on the content transfer dataset, and summarization on XSum,we show that linear prompts on encoder and textual prompts improve the generation quality on all datasets.Despite having less performance drop when testing on data drawn from a later time, linear prompts focus more on non-temporal information and are less sensitive to the given timestamps, according to human evaluations and sensitivity analyses.Meanwhile, textual prompts establish the association between the given timestamps and the output dates, yielding more factual temporal information in the output. | Shuyang Cao, Lu Wang |  |
| 537 |  |  [Improving Scheduled Sampling with Elastic Weight Consolidation for Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.536) |  | 0 | Despite strong performance in many sequence-to-sequence tasks, autoregressive models trained with maximum likelihood estimation suffer from exposure bias, i.e. the discrepancy between the ground-truth prefixes used during training and the model-generated prefixes used at inference time. Scheduled sampling is a simple and empirically successful approach which addresses this issue by incorporating model-generated prefixes into training. However, it has been argued that it is an inconsistent training objective leading to models ignoring the prefixes altogether. In this paper, we conduct systematic experiments and find that scheduled sampling, while it ameliorates exposure bias by increasing model reliance on the input sequence, worsens performance when the prefix at inference time is correct, a form of catastrophic forgetting. We propose to use Elastic Weight Consolidation to better balance mitigating exposure bias with retaining performance. Experiments on four IWSLT’14 and WMT’14 translation datasets demonstrate that our approach alleviates catastrophic forgetting and significantly outperforms maximum likelihood estimation and scheduled sampling baselines. | Michalis Korakakis, Andreas Vlachos |  |
| 538 |  |  [Ensemble Transformer for Efficient and Accurate Ranking Tasks: an Application to Question Answering Systems](https://doi.org/10.18653/v1/2022.findings-emnlp.537) |  | 0 | Large transformer models can highly improve Answer Sentence Selection (AS2) tasks, but their high computational costs prevent their use in many real-world applications. In this paper, we explore the following research question: How can we make the AS2 models more accurate without significantly increasing their model complexity? To address the question, we propose a Multiple Heads Student architecture (named CERBERUS), an efficient neural network designed to distill an ensemble of large transformers into a single smaller model. CERBERUS consists of two components: a stack of transformer layers that is used to encode inputs, and a set of ranking heads; unlike traditional distillation technique, each of them is trained by distilling a different large transformer architecture in a way that preserves the diversity of the ensemble members. The resulting model captures the knowledge of heterogeneous transformer models by using just a few extra parameters. We show the effectiveness of CERBERUS on three English datasets for AS2; our proposed approach outperforms all single-model distillations we consider, rivaling the state-of-the-art large AS2 models that have 2.7× more parameters and run 2.5× slower. Code for our model is available at https://github.com/amazon-research/wqa-cerberus. | Yoshitomo Matsubara, Luca Soldaini, Eric Lind, Alessandro Moschitti |  |
| 539 |  |  [Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis](https://doi.org/10.18653/v1/2022.findings-emnlp.538) |  | 0 | Pre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration error, especially in safety-critical applications. That is, the pipeline should reliably indicate when we can trust its predictions. In particular, there are various considerations behind the pipeline: (1) the choice and (2) the size of PLM, (3) the choice of uncertainty quantifier, (4) the choice of fine-tuning loss, and many more. Although prior work has looked into some of these considerations, they usually draw conclusions based on a limited scope of empirical studies. There still lacks a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline. To fill this void, we compare a wide range of popular options for each consideration based on three prevalent NLP classification tasks and the setting of domain shift. In response, we recommend the following: (1) use ELECTRA for PLM encoding, (2) use larger PLMs if possible, (3) use Temp Scaling as the uncertainty quantifier, and (4) use Focal Loss for fine-tuning. | Yuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, LouisPhilippe Morency |  |
| 540 |  |  [How to Represent Context Better? An Empirical Study on Context Modeling for Multi-turn Response Selection](https://doi.org/10.18653/v1/2022.findings-emnlp.539) |  | 0 | Building retrieval-based dialogue models that can predict appropriate responses based on the understanding of multi-turn context messages is a challenging problem. Early models usually concatenate all utterances or independently encode each dialogue turn, which may lead to an inadequate understanding of dialogue status. Although a few researchers have noticed the importance of context modeling in multi-turn response prediction, there is no systematic comparison to analyze how to model context effectively and no framework to unify those methods. In this paper, instead of configuring new architectures, we investigate how to improve existing models with a better context modeling method. Specifically, we heuristically summarize three categories of turn-aware context modeling strategies which model the context messages from the perspective of sequential relationship, local relationship, and query-aware manner respectively. A Turn-Aware Context Modeling (TACM) layer is explored to flexibly adapt and unify these context modeling strategies to several advanced response selection models. Evaluation results on three public data sets indicate that employing each individual context modeling strategy or multiple strategies can consistently improve the performance of existing models. | Jiazhan Feng, Chongyang Tao, Chang Liu, Rui Yan, Dongyan Zhao |  |
| 541 |  |  [CHIA: CHoosing Instances to Annotate for Machine Translation](https://doi.org/10.18653/v1/2022.findings-emnlp.540) |  | 0 | Neural machine translation (MT) systems have been shown to perform poorly on low-resource language pairs, for which large-scale parallel data is unavailable. Making the data annotation process faster and cheaper is therefore important to ensure equitable access to MT systems. To make optimal use of a limited annotation budget, we present CHIA (choosing instances to annotate), a method for selecting instances to annotate for machine translation. Using an existing multi-way parallel dataset of high-resource languages, we first identify instances, based on model training dynamics, that are most informative for training MT models for high-resource languages. We find that there are cross-lingual commonalities in instances that are useful for MT model training, which we use to identify instances that will be useful to train models on a new target language. Evaluating on 20 languages from two corpora, we show that training on instances selected using our method provides an average performance improvement of 1.59 BLEU over training on randomly selected instances of the same size. | Rajat Bhatnagar, Ananya Ganesh, Katharina Kann |  |
| 542 |  |  [Guiding Neural Machine Translation with Semantic Kernels](https://doi.org/10.18653/v1/2022.findings-emnlp.541) |  | 0 | Machine Translation task has made great progress with the help of auto-regressive decoding paradigm and Transformer architecture. In this paradigm, though the encoder can obtain global source representations, the decoder can only use translation history to determine the current word. Previous promising works attempted to address this issue by applying a draft or a fixed-length semantic embedding as target-side global information. However, these methods either degrade model efficiency or show limitations in expressing semantics. Motivated by Functional Equivalence Theory, we extract several semantic kernels from a source sentence, each of which can express one semantic segment of the original sentence. Together, these semantic kernels can capture global semantic information, and we project them into target embedding space to guide target sentence generation. We further force our model to use semantic kernels at each decoding step through an adaptive mask algorithm. Empirical studies on various machine translation benchmarks show that our approach gains approximately an improvement of 1 BLEU score on most benchmarks over the Transformer baseline and about 1.7 times faster than previous works on average at inference time. | Ping Guo, Yue Hu, Xiangpeng Wei, Yubing Ren, Yunpeng Li, Luxi Xing, Yuqiang Xie |  |
| 543 |  |  [HiSMatch: Historical Structure Matching based Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.542) |  | 0 | A Temporal Knowledge Graph (TKG) is a sequence of KGs with respective timestamps, which adopts quadruples in the form of (subject, relation, object, timestamp) to describe dynamic facts. TKG reasoning has facilitated many real-world applications via answering such queries as (query entity, query relation, ?, future timestamp) about future. This is actually a matching task between a query and candidate entities based on their historical structures, which reflect behavioral trends of the entities at different timestamps. In addition, recent KGs provide background knowledge of all the entities, which is also helpful for the matching. Thus, in this paper, we propose the Historical Structure Matching (HiSMatch) model. It applies two structure encoders to capture the semantic information contained in the historical structures of the query and candidate entities. Besides, it adopts another encoder to integrate the background knowledge into the model. TKG reasoning experiments on six benchmark datasets demonstrate the significant improvement of the proposed HiSMatch model, with up to 5.6% performance improvement in MRR, compared to the state-of-the-art baselines. | Zixuan Li, Zhongni Hou, Saiping Guan, Xiaolong Jin, Weihua Peng, Long Bai, Yajuan Lyu, Wei Li, Jiafeng Guo, Xueqi Cheng |  |
| 544 |  |  [Dependency Parsing via Sequence Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.543) |  | 0 | Dependency parsing aims to extract syntactic dependency structure or semantic dependency structure for sentences.Existing methods for dependency parsing include transition-based method, graph-based method and sequence-to-sequence method.These methods obtain excellent performance and we notice them belong to labeling method.Therefore, it may be very valuable and interesting to explore the possibility of using generative method to implement dependency parsing.In this paper, we propose to achieve Dependency Parsing (DP) via Sequence Generation (SG) by utilizing only the pre-trained language model without any auxiliary structures.We first explore different serialization designing strategies for converting parsing structures into sequences.Then we design dependency units and concatenate these units into the sequence for DPSG.We verify the DPSG is capable of parsing on widely used DP benchmarks, i.e., PTB, UD2.2, SDP15 and SemEval16.In addition, we also investigate the astonishing low-resource applicability of DPSG, which includes unsupervised cross-domain conducted on CODT and few-shot cross-task conducted on SDP15.Our research demonstrates that sequence generation is one of the effective methods to achieve dependency parsing.Our codes are available now. | Boda Lin, Zijun Yao, Jiaxin Shi, Shulin Cao, Binghao Tang, Si Li, Yong Luo, Juanzi Li, Lei Hou |  |
| 545 |  |  [Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments](https://doi.org/10.18653/v1/2022.findings-emnlp.544) |  | 0 | Neural scaling laws define a predictable relationship between a model’s parameter count and its performance after training in the form of a power law. However, most research to date has not explicitly investigated whether scaling laws can be used to accelerate model development. In this work, we perform such an empirical investigation across a wide range of language understanding tasks, starting from models with as few as 10K parameters, and evaluate downstream performance across 9 language understanding tasks.We find that scaling laws emerge at finetuning time in some NLP tasks, and that they can also be exploited for debugging convergence when training large models. Moreover, for tasks where scaling laws exist, they can be used to predict the performance of larger models, which enables effective model selection. However, revealing scaling lawsrequires careful hyperparameter tuning and multiple runs for the purpose of uncertainty estimation, which incurs additional overhead, partially offsetting the computational benefits. | Maor Ivgi, Yair Carmon, Jonathan Berant |  |
| 546 |  |  [Analyzing the Limits of Self-Supervision in Handling Bias in Language](https://doi.org/10.18653/v1/2022.findings-emnlp.545) |  | 0 | Prompting inputs with natural language task descriptions has emerged as a popular mechanism to elicit reasonably accurate outputs from large-scale generative language models with little to no in-context supervision. This also helps gain insight into how well language models capture the semantics of a wide range of downstream tasks purely from self-supervised pre-training on massive corpora of unlabeled text. Such models have naturally also been exposed to a lot of undesirable content like racist and sexist language and there is only some work on awareness of models along these dimensions. In this paper, we define and comprehensively evaluate how well such language models capture the semantics of four tasks for bias: diagnosis, identification, extraction and rephrasing. We define three broad classes of task descriptions for these tasks: statement, question, and completion, with numerous lexical variants within each class. We study the efficacy of prompting for each task using these classes and the null task description across several decoding methods and few-shot examples. Our analyses indicate that language models are capable of performing these tasks to widely varying degrees across different bias dimensions, such as gender and political affiliation. We believe our work is an important step towards unbiased language models by quantifying the limits of current self-supervision objectives at accomplishing such sociologically challenging tasks. | Lisa Bauer, Karthik Gopalakrishnan, Spandana Gella, Yang Liu, Mohit Bansal, Dilek HakkaniTur |  |
| 547 |  |  [Multiple Instance Learning for Offensive Language Detection](https://doi.org/10.18653/v1/2022.findings-emnlp.546) |  | 0 | Automatic offensive language detection has become a crucial issue in recent years. Existing researches on this topic are usually based on a large amount of data annotated at sentence level to train a robust model. However, sentence-level annotations are expensive in practice as the scenario expands, while there exist a large amount of natural labels from historical information on online platforms such as reports and punishments. Notably, these natural labels are usually in bag-level corresponding to the whole documents (articles, user profiles, conversations, etc.). Therefore, we target at proposing an approach capable of utilizing the bag-level labeled data for offensive language detection in this study. For this purpose, we formalize this task into a multiple instance learning (MIL) problem. We break down the design of existing MIL methods and propose a hybrid fusion MIL model with mutual-attention mechanism. In order to verify the validity of the proposed method, we present two new bag-level labeled datasets for offensive language detection: OLID-bags and MINOR. Experimental results based on the proposed datasets demonstrate the effectiveness of the mutual-attention method at both sentence level and bag level. | Jiexi Liu, Dehan Kong, Longtao Huang, Dinghui Mao, Hui Xue |  |
| 548 |  |  [Grounded Keys-to-Text Generation: Towards Factual Open-Ended Generation](https://doi.org/10.18653/v1/2022.findings-emnlp.547) |  | 0 | Large pre-trained language models have recently enabled open-ended generation frameworks (e.g., prompt-to-text NLG) to tackle a variety of tasks going beyond the traditional data-to-text generation. While this framework is more general, it is under-specified and often leads to a lack of controllability restricting their real-world usage. We propose a new grounded keys-to-text generation task: the task is to generate a factual description about an entity given a set of guiding keys, and grounding passages. To address this task, we introduce a new dataset, called EntDeGen. Inspired by recent QA-based evaluation measures, we propose an automatic metric, MAFE, for factual correctness of generated descriptions. Our EntDescriptor model is equipped with strong rankers to fetch helpful passages and generate entity descriptions. Experimental result shows a good correlation (60.14) between our proposed metric and human judgments of factuality. Our rankers significantly improved the factual correctness of generated descriptions (15.95% and 34.51% relative gains in recall and precision). Finally, our ablation study highlights the benefit of combining keys and groundings. | Faeze Brahman, Baolin Peng, Michel Galley, Sudha Rao, Bill Dolan, Snigdha Chaturvedi, Jianfeng Gao |  |
| 549 |  |  [CogKTR: A Knowledge-Enhanced Text Representation Toolkit for Natural Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-demos.1) |  | 0 | As the first step of modern natural language processing, text representation encodes discrete texts as continuous embeddings. Pre-trained language models (PLMs) have demonstrated strong ability in text representation and significantly promoted the development of natural language understanding (NLU). However, existing PLMs represent a text solely by its context, which is not enough to support knowledge-intensive NLU tasks. Knowledge is power, and fusing external knowledge explicitly into PLMs can provide knowledgeable text representations. Since previous knowledge-enhanced methods differ in many aspects, making it difficult for us to reproduce previous methods, implement new methods, and transfer between different methods. It is highly desirable to have a unified paradigm to encompass all kinds of methods in one framework. In this paper, we propose CogKTR, a knowledge-enhanced text representation toolkit for natural language understanding. According to our proposed Unified Knowledge-Enhanced Paradigm (UniKEP), CogKTR consists of four key stages, including knowledge acquisition, knowledge representation, knowledge injection, and knowledge application. CogKTR currently supports easy-to-use knowledge acquisition interfaces, multi-source knowledge embeddings, diverse knowledge-enhanced models, and various knowledge-intensive NLU tasks. Our unified, knowledgeable and modular toolkit is publicly available at GitHub, with an online system and a short instruction video. | Zhuoran Jin, Tianyi Men, Hongbang Yuan, Yuyang Zhou, Pengfei Cao, Yubo Chen, Zhipeng Xue, Kang Liu, Jun Zhao |  |
| 550 |  |  [LM-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models](https://doi.org/10.18653/v1/2022.emnlp-demos.2) |  | 0 | The opaque nature and unexplained behavior of transformer-based language models (LMs) have spurred a wide interest in interpreting their predictions. However, current interpretation methods mostly focus on probing models from outside, executing behavioral tests, and analyzing salience input features, while the internal prediction construction process is largely not understood. In this work, we introduce LM-Debugger, an interactive debugger tool for transformer-based LMs, which provides a fine-grained interpretation of the model’s internal prediction process, as well as a powerful framework for intervening in LM behavior. For its backbone, LM-Debugger relies on a recent method that interprets the inner token representations and their updates by the feed-forward layers in the vocabulary space. We demonstrate the utility of LM-Debugger for single-prediction debugging, by inspecting the internal disambiguation process done by GPT2. Moreover, we show how easily LM-Debugger allows to shift model behavior in a direction of the user’s choice, by identifying a few vectors in the network and inducing effective interventions to the prediction process. We release LM-Debugger as an open-source tool and a demo over GPT2 models. | Mor Geva, Avi Caciularu, Guy Dar, Paul Roit, Shoval Sadde, Micah Shlain, Bar Tamir, Yoav Goldberg |  |
| 551 |  |  [EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-demos.3) |  | 0 | Pre-Trained Models (PTMs) have reshaped the development of Natural Language Processing (NLP) and achieved significant improvement in various benchmarks. Yet, it is not easy for industrial practitioners to obtain high-performing PTM-based models without a large amount of labeled training data and deploy them online with fast inference speed. To bridge this gap, EasyNLP is designed to make it easy to build NLP applications, which supports a comprehensive suite of NLP algorithms. It further features knowledge-enhanced pre-training, knowledge distillation and few-shot learning functionalities, and provides a unified framework of model training, inference and deployment for real-world applications. EasyNLP has powered over ten business units within Alibaba Group and is seamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud. The source code of EasyNLP is released at GitHub (https://github.com/alibaba/EasyNLP). | Chengyu Wang, Minghui Qiu, Taolin Zhang, Tingting Liu, Lei Li, Jianing Wang, Ming Wang, Jun Huang, Wei Lin |  |
| 552 |  |  [An Explainable Toolbox for Evaluating Pre-trained Vision-Language Models](https://doi.org/10.18653/v1/2022.emnlp-demos.4) |  | 0 | We introduce VL-CheckList, a toolbox for evaluating Vision-Language Pretraining (VLP) models, including the preliminary datasets that deepen the image-texting ability of a VLP model. Most existing VLP works evaluated their systems by comparing the fine-tuned downstream task performance. However, only average downstream task accuracy provides little information about the pros and cons of each VLP method. In this paper, we demonstrate how minor input changes in language and vision will affect the prediction outputs. Then, we describe the detailed user guidelines to utilize and contribute to the community. We show new findings on one of the representative VLP models to provide an example analysis. The data/code is available at https://github.com/om-ai-lab/VL-CheckList | Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, Jianwei Yin |  |
| 553 |  |  [TweetNLP: Cutting-Edge Natural Language Processing for Social Media](https://doi.org/10.18653/v1/2022.emnlp-demos.5) |  | 0 | In this paper we present TweetNLP, an integrated platform for Natural Language Processing (NLP) in social media. TweetNLP supports a diverse set of NLP tasks, including generic focus areas such as sentiment analysis and named entity recognition, as well as social media-specific tasks such as emoji prediction and offensive language identification. Task-specific systems are powered by reasonably-sized Transformer-based language models specialized on social media text (in particular, Twitter) which can be run without the need for dedicated hardware or cloud services. The main contributions of TweetNLP are: (1) an integrated Python library for a modern toolkit supporting social media analysis using our various task-specific models adapted to the social domain; (2) an interactive online demo for codeless experimentation using our models; and (3) a tutorial covering a wide variety of typical social media applications. | José CamachoCollados, Kiamehr Rezaee, Talayeh Riahi, Asahi Ushio, Daniel Loureiro, Dimosthenis Antypas, Joanne Boisson, Luis Espinosa Anke, Fangyu Liu, Eugenio Martínez Cámara |  |
| 554 |  |  [JoeyS2T: Minimalistic Speech-to-Text Modeling with JoeyNMT](https://doi.org/10.18653/v1/2022.emnlp-demos.6) |  | 0 | JoeyS2T is a JoeyNMT extension for speech-to-text tasks such as automatic speech recognition and end-to-end speech translation. It inherits the core philosophy of JoeyNMT, a minimalist NMT toolkit built on PyTorch, seeking simplicity and accessibility. JoeyS2T’s workflow is self-contained, starting from data pre-processing, over model training and prediction to evaluation, and is seamlessly integrated into JoeyNMT’s compact and simple code base. On top of JoeyNMT’s state-of-the-art Transformer-based Encoder-Decoder architecture, JoeyS2T provides speech-oriented components such as convolutional layers, SpecAugment, CTC-loss, and WER evaluation. Despite its simplicity compared to prior implementations, JoeyS2T performs competitively on English speech recognition and English-to-German speech translation benchmarks. The implementation is accompanied by a walk-through tutorial and available on https://github.com/may-/joeys2t. | Mayumi Ohta, Julia Kreutzer, Stefan Riezler |  |
| 555 |  |  [FairLib: A Unified Framework for Assessing and Improving Fairness](https://doi.org/10.18653/v1/2022.emnlp-demos.7) |  | 0 | This paper presents FairLib, an open-source python library for assessing and improving model fairness. It provides a systematic framework for quickly accessing benchmark datasets, reproducing existing debiasing baseline models, developing new methods, evaluating models with different metrics, and visualizing their results. Its modularity and extensibility enable the framework to be used for diverse types of inputs, including natural language, images, and audio. We implement 14 debiasing methods, including pre-processing,at-training-time, and post-processing approaches. The built-in metrics cover the most commonly acknowledged fairness criteria and can be further generalized and customized for fairness evaluation. | Xudong Han, Aili Shen, Yitong Li, Lea Frermann, Timothy Baldwin, Trevor Cohn |  |
| 556 |  |  [ELEVANT: A Fully Automatic Fine-Grained Entity Linking Evaluation and Analysis Tool](https://doi.org/10.18653/v1/2022.emnlp-demos.8) |  | 0 | We present Elevant, a tool for the fully automatic fine-grained evaluation of a set of entity linkers on a set of benchmarks. Elevant provides an automatic breakdown of the performance by various error categories and by entity type. Elevant also provides a rich and compact, yet very intuitive and self-explanatory visualization of the results of a linker on a benchmark in comparison to the ground truth. A live demo, the link to the complete code base on GitHub and a link to a demo video are provided under https://elevant.cs.uni-freiburg.de . | Hannah Bast, Matthias Hertel, Natalie Prange |  |
| 557 |  |  [A Pipeline for Generating, Annotating and Employing Synthetic Data for Real World Question Answering](https://doi.org/10.18653/v1/2022.emnlp-demos.9) |  | 0 | Question Answering (QA) is a growing area of research, often used to facilitate the extraction of information from within documents. State-of-the-art QA models are usually pre-trained on domain-general corpora like Wikipedia and thus tend to struggle on out-of-domain documents without fine-tuning. We demonstrate that synthetic domain-specific datasets can be generated easily using domain-general models, while still providing significant improvements to QA performance. We present two new tools for this task: A flexible pipeline for validating the synthetic QA data and training down stream models on it, and an online interface to facilitate human annotation of this generated data. Using this interface, crowdworkers labelled 1117 synthetic QA pairs, which we then used to fine-tune downstream models and improve domain-specific QA performance by 8.75 F1. | Matthew Maufe, James Ravenscroft, Rob Procter, Maria Liakata |  |
| 558 |  |  [DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population](https://doi.org/10.18653/v1/2022.emnlp-demos.10) |  | 0 | We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in the knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured data according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. We release the source code at GitHub in https://github.com/zjunlp/DeepKE with Google Colab tutorials and comprehensive documents for beginners. Besides, we present an online system in http://deepke.openkg.cn/EN/re_doc_show.html for real-time extraction of various tasks, and a demo video. | Ningyu Zhang, Xin Xu, Liankuan Tao, Haiyang Yu, Hongbin Ye, Shuofei Qiao, Xin Xie, Xiang Chen, Zhoubo Li, Lei Li |  |
| 559 |  |  [AnEMIC: A Framework for Benchmarking ICD Coding Models](https://doi.org/10.18653/v1/2022.emnlp-demos.11) |  | 0 | Diagnostic coding, or ICD coding, is the task of assigning diagnosis codes defined by the ICD (International Classification of Diseases) standard to patient visits based on clinical notes. The current process of manual ICD coding is time-consuming and often error-prone, which suggests the need for automatic ICD coding. However, despite the long history of automatic ICD coding, there have been no standardized frameworks for benchmarking ICD coding models. We open-source an easy-to-use tool named AnEMIC, which provides a streamlined pipeline for preprocessing, training, and evaluating for automatic ICD coding. We correct errors in preprocessing by existing works, and provide key models and weights trained on the correctly preprocessed datasets. We also provide an interactive demo performing real-time inference from custom inputs, and visualizations drawn from explainable AI to analyze the models. We hope the framework helps move the research of ICD coding forward and helps professionals explore the potential of ICD coding. The framework and the associated code are available here. | Juyong Kim, Abheesht Sharma, Suhas Shanbhogue, Jeremy C. Weiss, Pradeep Ravikumar |  |
| 560 |  |  [SPEAR : Semi-supervised Data Programming in Python](https://doi.org/10.18653/v1/2022.emnlp-demos.12) |  | 0 | We present SPEAR, an open-source python library for data programming with semi supervision. The package implements several recent data programming approaches including facility to programmatically label and build training data. SPEAR facilitates weak supervision in the form of heuristics (or rules) and association of noisy labels to the training dataset. These noisy labels are aggregated to assign labels to the unlabeled data for downstream tasks. We have implemented several label aggregation approaches that aggregate the noisy labels and then train using the noisily labeled set in a cascaded manner. Our implementation also includes other approaches that jointly aggregate and train the model for text classification tasks. Thus, in our python package, we integrate several cascade and joint data-programming approaches while also providing the facility of data programming by letting the user define labeling functions or rules. The code and tutorial notebooks are available at https://github.com/decile-team/spear. Further, extensive documentation can be found at https://spear-decile.readthedocs.io/. Video tutorials demonstrating the usage of our package are available https://youtube.com/playlist?list=PLW8agt_HvkVnOJoJAqBpaerFb-z-ZlqlP. We also present some real-world use cases of SPEAR. | Guttu Sai Abhishek, Harshad Ingole, Parth Laturia, Vineeth Dorna, Ayush Maheshwari, Ganesh Ramakrishnan, Rishabh K. Iyer |  |
| 561 |  |  [Evaluate & Evaluation on the Hub: Better Best Practices for Data and Model Measurements](https://doi.org/10.18653/v1/2022.emnlp-demos.13) |  | 0 | Evaluation is a key part of machine learning (ML), yet there is a lack of support and tooling to enable its informed and systematic practice. We introduce Evaluate and Evaluation on the Hub—a set of tools to facilitate the evaluation of models and datasets in ML. Evaluate is a library to support best practices for measurements, metrics, and comparisons of data and models. Its goal is to support reproducibility of evaluation, centralize and document the evaluation process, and broaden evaluation to cover more facets of model performance. It includes over 50 efficient canonical implementations for a variety of domains and scenarios, interactive documentation, and the ability to easily share implementations and outcomes. The library is available at https://github.com/huggingface/evaluate. In addition, we introduce Evaluation on the Hub, a platform that enables the large-scale evaluation of over 75,000 models and 11,000 datasets on the Hugging Face Hub, for free, at the click of a button. Evaluation on the Hub is available at https://huggingface.co/autoevaluate. | Leandro von Werra, Lewis Tunstall, Abhishek Thakur, Sasha Luccioni, Tristan Thrush, Aleksandra Piktus, Felix Marty, Nazneen Rajani, Victor Mustar, Helen Ngo |  |
| 562 |  |  [KeywordScape: Visual Document Exploration using Contextualized Keyword Embeddings](https://doi.org/10.18653/v1/2022.emnlp-demos.14) |  | 0 | Although contextualized word embeddings have led to great improvements in automatic language understanding, their potential for practical applications in document exploration and visualization has been little explored. Common visualization techniques used for, e.g., model analysis usually provide simple scatter plots of token-level embeddings that do not provide insight into their contextual use. In this work, we propose KeywordScape, a visual exploration tool that allows to overview, summarize, and explore the semantic content of documents based on their keywords. While existing keyword-based exploration tools assume that keywords have static meanings, our tool represents keywords in terms of their contextualized embeddings. Our application visualizes these embeddings in a semantic landscape that represents keywords as islands on a spherical map. This keeps keywords with similar context close to each other, allowing for a more precise search and comparison of documents. | Henrik Voigt, Monique Meuschke, Sina Zarrieß, Kai Lawonn |  |
| 563 |  |  [MedConQA: Medical Conversational Question Answering System based on Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-demos.15) |  | 0 | The medical conversational system can relieve doctors’ burden and improve healthcare efficiency, especially during the COVID-19 pandemic. However, the existing medical dialogue systems have the problems of weak scalability, insufficient knowledge, and poor controllability. Thus, we propose a medical conversational question-answering (CQA) system based on the knowledge graph, namely MedConQA, which is designed as a pipeline framework to maintain high flexibility. Our system utilizes automated medical procedures, including medical triage, consultation, image-text drug recommendation, and record. Each module has been open-sourced as a tool, which can be used alone or in combination, with robust scalability. Besides, to conduct knowledge-grounded dialogues with users, we first construct a Chinese Medical Knowledge Graph (CMKG) and collect a large-scale Chinese Medical CQA (CMCQA) dataset, and we design a series of methods for reasoning more intellectually. Finally, we use several state-of-the-art (SOTA) techniques to keep the final generated response more controllable, which is further assured by hospital and professional evaluations. We have open-sourced related code, datasets, web pages, and tools, hoping to advance future research. | Fei Xia, Bin Li, Yixuan Weng, Shizhu He, Kang Liu, Bin Sun, Shutao Li, Jun Zhao |  |
| 564 |  |  [Label Sleuth: From Unlabeled Text to a Classifier in a Few Hours](https://doi.org/10.18653/v1/2022.emnlp-demos.16) |  | 0 | Label Sleuth is an open source platform for building text classifiers which does not require coding skills nor machine learning knowledge.- Project website: [https://www.label-sleuth.org/](https://www.label-sleuth.org/)- Link to screencast video: [https://vimeo.com/735675461](https://vimeo.com/735675461)### AbstractText classification can be useful in many real-world scenarios, saving a lot of time for end users. However, building a classifier generally requires coding skills and ML knowledge, which poses a significant barrier for many potential users. To lift this barrier we introduce \*Label Sleuth\*, a free open source system for labeling and creating text classifiers. This system is unique for: - being a no-code system, making NLP accessible for non-experts. - guiding its users throughout the entire labeling process until they obtain their desired classifier, making the process efficient - from cold start to a classifier in a few hours. - being open for configuration and extension by developers. By open sourcing Label Sleuth we hope to build a community of users and developers that will widen the utilization of NLP models. | Eyal Shnarch, Alon Halfon, Ariel Gera, Marina Danilevsky, Yannis Katsis, Leshem Choshen, Martín Santillán Cooper, Dina Epelboim, Zheng Zhang, Dakuo Wang |  |
| 565 |  |  [AGReE: A system for generating Automated Grammar Reading Exercises](https://doi.org/10.18653/v1/2022.emnlp-demos.17) |  | 0 | We describe the AGReE system, which takes user-submitted passages as input and automatically generates grammar practice exercises that can be completed while reading. Multiple-choice practice items are generated for a variety of different grammar constructs: punctuation, articles, conjunctions, pronouns, prepositions, verbs, and nouns. We also conducted a large-scale human evaluation with around 4,500 multiple-choice practice items. We notice for 95% of items, a majority of raters out of five were able to identify the correct answer, for 85% of cases, raters agree that there is only one correct answer among the choices. Finally, the error analysis shows that raters made the most mistakes for punctuation and conjunctions. | Sophia Chan, Swapna Somasundaran, Debanjan Ghosh, Mengxuan Zhao |  |
| 566 |  |  [BotSIM: An End-to-End Bot Simulation Framework for Commercial Task-Oriented Dialog Systems](https://doi.org/10.18653/v1/2022.emnlp-demos.18) |  | 0 | We present BotSIM, a data-efficient end-to-end Bot SIMulation framework for commercial task-oriented dialog (TOD) systems. BotSIM consists of three major components: 1) a Generator that can infer semantic-level dialog acts and entities from bot definitions and generate user queries via model-based paraphrasing; 2) an agenda-based dialog user Simulator (ABUS) to simulate conversations with the dialog agents; 3) a Remediator to analyze the simulated conversations, visualize the bot health reports and provide actionable remediation suggestions for bot troubleshooting and improvement. We demonstrate BotSIM’s effectiveness in end-to-end evaluation, remediation and multi-intent dialog generation via case studies on two commercial bot platforms. BotSIM’s “generation-simulation-remediation” paradigm accelerates the end-to-end bot evaluation and iteration process by: 1) reducing manual test cases creation efforts; 2) enabling a holistic gauge of the bot in terms of NLU and end-to-end performance via extensive dialog simulation; 3) improving the bot troubleshooting process with actionable suggestions. A demo of our system can be found at https://tinyurl.com/mryu74cd and a demo video at https://youtu.be/qLPJm6_UOKY. | Guangsen Wang, Samson Tan, Shafiq R. Joty, Gang Wu, Jimmy Au, Steven C. H. Hoi |  |
| 567 |  |  [DeepGen: Diverse Search Ad Generation and Real-Time Customization](https://doi.org/10.18653/v1/2022.emnlp-demos.19) |  | 0 | Demo: https://youtu.be/WQLL93TPB-cAbstract:We present DeepGen, a system deployed at web scale for automatically creating sponsored search advertisements (ads) for BingAds customers. We leverage state-of-the-art natural language generation (NLG) models to generate fluent ads from advertiser’s web pages in an abstractive fashion and solve practical issues such as factuality and inference speed. In addition, our system creates a customized ad in real-time in response to the user’s search query, therefore highlighting different aspects of the same product based on what the user is looking for. To achieve this, our system generates a diverse choice of smaller pieces of the ad ahead of time and, at query time, selects the most relevant ones to be stitched into a complete ad. We improve generation diversity by training a controllable NLG model to generate multiple ads for the same web page highlighting different selling points. Our system design further improves diversity horizontally by first running an ensemble of generation models trained with different objectives and then using a diversity sampling algorithm to pick a diverse subset of generation results for online selection. Experimental results show the effectiveness of our proposed system design. Our system is currently deployed in production, serving ~4% of global ads served in Bing. | Konstantin Golobokov, Junyi Chai, Victor Ye Dong, Mandy Gu, Bingyu Chi, Jie Cao, Yulan Yan, Yi Liu |  |
| 568 |  |  [ACCoRD: A Multi-Document Approach to Generating Diverse Descriptions of Scientific Concepts](https://doi.org/10.18653/v1/2022.emnlp-demos.20) |  | 0 | Systems that automatically define unfamiliar terms hold the promise of improving the accessibility of scientific texts, especially for readers who may lack prerequisite background knowledge. However, current systems assume a single “best” description per concept, which fails to account for the many ways a concept can be described. We present ACCoRD, an end-to-end system tackling the novel task of generating sets of descriptions of scientific concepts. Our system takes advantage of the myriad ways a concept is mentioned across the scientific literature to produce distinct, diverse descriptions oftarget concepts in terms of different reference concepts. In a user study, we find that users prefer (1) descriptions produced by our end-to-end system, and (2) multiple descriptions to a single “best” description. We release the ACCoRD corpus which includes 1,275 labeled contexts and 1,787 expert-authored concept descriptions to support research on our task. | Sonia K. Murthy, Kyle Lo, Daniel King, Chandra Bhagavatula, Bailey Kuehl, Sophie Johnson, Jonathan Borchardt, Daniel S. Weld, Tom Hope, Doug Downey |  |
| 569 |  |  [Automatic Comment Generation for Chinese Student Narrative Essays](https://doi.org/10.18653/v1/2022.emnlp-demos.21) |  | 0 | Automatic essay evaluation can help reduce teachers’ workload and enable students to refine their works rapidly. Previous studies focus mainly on giving discrete scores for either the holistic quality orseveral distinct traits. However, real-world teachers usually provide detailed comments in natural language, which are more informative than single scores. In this paper, we present the comment generation task, which aims to generate commentsfor specified segments from given student narrative essays. To tackle this task, we propose a planning-based generation model, which first plans a sequence of keywords, and then expands these keywords into a complete comment. To improve the correctness and informativeness of generated comments, we adopt two following techniques: (1) training an error correction module to filter out incorrect keywords, and (2) recognizing fine-grained structured features from source essays to enrich the keywords. To support the evaluation of the task, we collect a human-written Chinese dataset, which contains 22,399 essay-comment pairs. Extensive experiments show that our model outperforms strong baselines significantly. Moreover, we exert explicit control on our model to generate comments to describe the strengths or weaknesses of inputs with a 91% success rate. We deploy the model at http://coai.cs.tsinghua.edu.cn/static/essayComment/. A demo video is available at https://youtu.be/IuFVk8dUxbI. Our code and data are available at https://github.com/thu-coai/EssayCommentGen. | Zhexin Zhang, Jian Guan, Guowei Xu, Yixiang Tian, Minlie Huang |  |
| 570 |  |  [MIC: A Multi-task Interactive Curation Tool](https://doi.org/10.18653/v1/2022.emnlp-demos.22) |  | 0 | This paper introduces MIC, a Multi-task Interactive Curation tool, a human-machine collaborative curation tool for multiple NLP tasks. The tool aims to borrow recent advances in literature to solve pain-points in real NLP tasks. Firstly, it supports multiple projects with multiple users which enables collaborative annotations. Secondly, MIC allows easy integration of pre-trained models, rules, and dictionaries to auto label the text and speed up the labeling process. Thirdly, MIC supports annotation at different scales (span of characters and words, tokens and lines, or document) and different types (free text, sentence labels, entity labels, and relationship triplets) with easy GUI operations. | Shi Yu, Mingfeng Yang, Jerrod Parker, Stephen Brock |  |
| 571 |  |  [SUMMARY WORKBENCH: Unifying Application and Evaluation of Text Summarization Models](https://doi.org/10.18653/v1/2022.emnlp-demos.23) |  | 0 | This paper presents Summary Workbench, a new tool for developing and evaluating text summarization models. New models and evaluation measures can be easily integrated as Docker-based plugins, allowing to examine the quality of their summaries against any input and to evaluate them using various evaluation measures. Visual analyses combining multiple measures provide insights into the models’ strengths and weaknesses. The tool is hosted at https://tldr.demo.webis.de and also supports local deployment for private resources. | Shahbaz Syed, Dominik Schwabe, Martin Potthast |  |
| 572 |  |  [Arabic Word-level Readability Visualization for Assisted Text Simplification](https://doi.org/10.18653/v1/2022.emnlp-demos.24) |  | 0 | This demo paper presents a Google Docs add-on for automatic Arabic word-level readability visualization. The add-on includes a lemmatization component that is connected to a five-level readability lexicon and Arabic WordNet-based substitution suggestions. The add-on can be used for assessing the reading difficulty of a text and identifying difficult words as part of the task of manual text simplification. We make our add-on and its code publicly available. | Reem Hazim, Hind Saddiki, Bashar Alhafni, Muhamed AlKhalil, Nizar Habash |  |
| 573 |  |  [LogiTorch: A PyTorch-based library for logical reasoning on natural language](https://doi.org/10.18653/v1/2022.emnlp-demos.25) |  | 0 | Logical reasoning on natural language is one of the most challenging tasks for deep learning models. There has been an increasing interest in developing new benchmarks to evaluate the reasoning capabilities of language models such as BERT. In parallel, new models based on transformers have emerged to achieve ever better performance on these datasets. However, there is currently no library for logical reasoning that includes such benchmarks and models. This paper introduces LogiTorch, a PyTorch-based library that includes different logical reasoning benchmarks, different models, as well as utility functions such as co-reference resolution. This makes it easy to directly use the preprocessed datasets, to run the models, or to finetune them with different hyperparameters. LogiTorch is open source and can be found on GitHub. | Chadi Helwe, Chloé Clavel, Fabian M. Suchanek |  |
| 574 |  |  [stopes - Modular Machine Translation Pipelines](https://doi.org/10.18653/v1/2022.emnlp-demos.26) |  | 0 |  | Pierre Andrews, Guillaume Wenzek, Kevin Heffernan, Onur Çelebi, Anna Y. Sun, Ammar Kamran, Yingzhe Guo, Alexandre Mourachko, Holger Schwenk, Angela Fan |  |
| 575 |  |  [GEMv2: Multilingual NLG Benchmarking in a Single Line of Code](https://doi.org/10.18653/v1/2022.emnlp-demos.27) |  | 0 | Evaluations in machine learning rarely use the latest metrics, datasets, or human evaluation in favor of remaining compatible with prior work. The compatibility, often facilitated through leaderboards, thus leads to outdated but standardized evaluation practices. We pose that the standardization is taking place in the wrong spot. Evaluation infrastructure should enable researchers to use the latest methods and what should be standardized instead is how to incorporate these new evaluation advances. We introduce GEMv2, the new version of the Generation, Evaluation, and Metrics Benchmark which uses a modular infrastructure for dataset, model, and metric developers to benefit from each other’s work. GEMv2 supports 40 documented datasets in 51 languages, ongoing online evaluation for all datasets, and our interactive tools make it easier to add new datasets to the living benchmark. | Sebastian Gehrmann, Abhik Bhattacharjee, Abinaya Mahendiran, Alex Wang, Alexandros Papangelis, Aman Madaan, Angelina McMillanMajor, Anna Shvets, Ashish Upadhyay, Bernd Bohnet |  |
| 576 |  |  [KGI: An Integrated Framework for Knowledge Intensive Language Tasks](https://doi.org/10.18653/v1/2022.emnlp-demos.28) |  | 0 | In this paper, we present a system to showcase the capabilities of the latest state-of-the-art retrieval augmented generation models trained on knowledge-intensive language tasks, such as slot filling, open domain question answering, dialogue, and fact-checking. Moreover, given a user query, we show how the output from these different models can be combined to cross-examine the outputs of each other. Particularly, we show how accuracy in dialogue can be improved using the question answering model. We are also releasing all models used in the demo as a contribution of this paper. A short video demonstrating the system is available at https://ibm.box.com/v/emnlp2022-demos. | Md. Faisal Mahbub Chowdhury, Michael R. Glass, Gaetano Rossiello, Alfio Gliozzo, Nandana Mihindukulasooriya |  |
| 577 |  |  [Twitter-Demographer: A Flow-based Tool to Enrich Twitter Data](https://doi.org/10.18653/v1/2022.emnlp-demos.29) |  | 0 | Twitter data have become essential to Natural Language Processing (NLP) and social science research, driving various scientific discoveries in recent years. However, the textual data alone are often not enough to conduct studies: especially, social scientists need more variables to perform their analysis and control for various factors. How we augment this information, such as users’ location, age, or tweet sentiment, has ramifications for anonymity and reproducibility, and requires dedicated effort. This paper describes Twitter-Demographer, a simple, flow-based tool to enrich Twitter data with additional information about tweets and users. \tool is aimed at NLP practitioners, psycho-linguists, and (computational) social scientists who want to enrich their datasets with aggregated information, facilitating reproducibility, and providing algorithmic privacy-by-design measures for pseudo-anonymity. We discuss our design choices, inspired by the flow-based programming paradigm, to use black-box components that can easily be chained together and extended. We also analyze the ethical issues related to the use of this tool, and the built-in measures to facilitate pseudo-anonymity. | Federico Bianchi, Vincenzo Cutrona, Dirk Hovy |  |
| 578 |  |  [Azimuth: Systematic Error Analysis for Text Classification](https://doi.org/10.18653/v1/2022.emnlp-demos.30) |  | 0 | We present Azimuth, an open-source and easy-to-use tool to perform error analysis for text classification. Compared to other stages of the ML development cycle, such as model training and hyper-parameter tuning, the process and tooling for the error analysis stage are less mature. However, this stage is critical for the development of reliable and trustworthy AI systems. To make error analysis more systematic, we propose an approach comprising dataset analysis and model quality assessment, which Azimuth facilitates. We aim to help AI practitioners discover and address areas where the model does not generalize by leveraging and integrating a range of ML techniques, such as saliency maps, similarity, uncertainty, and behavioral analyses, all in one tool. Our code and documentation are available at github.com/servicenow/azimuth. | Gabrielle Gauthier Melançon, Orlando Marquez Ayala, Lindsay Brin, Chris Tyler, Frederic BranchaudCharron, Joseph Marinier, Karine Grande, Di Le |  |
| 579 |  |  [SynKB: Semantic Search for Synthetic Procedures](https://doi.org/10.18653/v1/2022.emnlp-demos.31) |  | 0 | In this paper we present SynKB, an open-source, automatically extracted knowledge base of chemical synthesis protocols. Similar to proprietary chemistry databases such as Reaxsys, SynKB allows chemists to retrieve structured knowledge about synthetic procedures. By taking advantage of recent advances in natural language processing for procedural texts, SynKB supports more flexible queries about reaction conditions, and thus has the potential to help chemists search the literature for conditions used in relevant reactions as they design new synthetic routes. Using customized Transformer models to automatically extract information from 6 million synthesis procedures described in U.S. and EU patents, we show that for many queries, SynKB has higher recall than Reaxsys, while maintaining high precision. We plan to make SynKB available as an open-source tool; in contrast, proprietary chemistry databases require costly subscriptions. | Fan Bai, Alan Ritter, Peter B. Madrid, Dayne Freitag, John Niekrasz |  |
| 580 |  |  [Camelira: An Arabic Multi-Dialect Morphological Disambiguator](https://doi.org/10.18653/v1/2022.emnlp-demos.32) |  | 0 | We present Camelira, a web-based Arabic multi-dialect morphological disambiguation tool that covers four major variants of Arabic: Modern Standard Arabic, Egyptian, Gulf, and Levantine.Camelira offers a user-friendly web interface that allows researchers and language learners to explore various linguistic information, such as part-of-speech, morphological features, and lemmas. Our system also provides an option to automatically choose an appropriate dialect-specific disambiguator based on the prediction of a dialect identification component. Camelira is publicly accessible at http://camelira.camel-lab.com. | Ossama Obeid, Go Inoue, Nizar Habash |  |
| 581 |  |  [POTATO: The Portable Text Annotation Tool](https://doi.org/10.18653/v1/2022.emnlp-demos.33) |  | 0 | We present POTATO, the Portable text annotation tool, a free, fully open-sourced annotation system that 1) supports labeling many types of text and multimodal data; 2) offers easy-to-configure features to maximize the productivity of both deployers and annotators (convenient templates for common ML/NLP tasks, active learning, keypress shortcuts, keyword highlights, tooltips); and 3) supports a high degree of customization (editable UI, inserting pre-screening questions, attention and qualification tests). Experiments over two annotation tasks suggest that POTATO improves labeling speed through its specially-designed productivity features, especially for long documents and complex tasks. POTATO is available at https://github.com/davidjurgens/potato and will continue to be updated. | Jiaxin Pei, Aparna Ananthasubramaniam, Xingyao Wang, Naitian Zhou, Apostolos Dedeloudis, Jackson Sargent, David Jurgens |  |
| 582 |  |  [KGxBoard: Explainable and Interactive Leaderboard for Evaluation of Knowledge Graph Completion Models](https://doi.org/10.18653/v1/2022.emnlp-demos.34) |  | 0 | Knowledge Graphs (KGs) store information in the form of (head, predicate, tail)-triples. To augment KGs with new knowledge, researchers proposed models for KG Completion (KGC) tasks such as link prediction; i.e., answering (h; p; ?) or (?; p; t) queries. Such models are usually evaluated with averaged metrics on a held-out test set. While useful for tracking progress, averaged single-score metrics cannotreveal what exactly a model has learned — or failed to learn. To address this issue, we propose KGxBoard: an interactive framework for performing fine-grained evaluation on meaningful subsets of the data, each of which tests individual and interpretable capabilities of a KGC model. In our experiments, we highlight the findings that we discovered with the use of KGxBoard, which would have been impossible to detect with standard averaged single-score metrics. | Haris Widjaja, Kiril Gashteovski, Wiem Ben Rim, Pengfei Liu, Christopher Malon, Daniel Ruffinelli, Carolin Lawrence, Graham Neubig |  |
| 583 |  |  [FALTE: A Toolkit for Fine-grained Annotation for Long Text Evaluation](https://doi.org/10.18653/v1/2022.emnlp-demos.35) |  | 0 | A growing swath of NLP research is tackling problems related to generating long text, including tasks such as open-ended story generation, summarization, dialogue, and more. However, we currently lack appropriate tools to evaluate these long outputs of generation models: classic automatic metrics such as ROUGE have been shown to perform poorly, and newer learned metrics do not necessarily work wellfor all tasks and domains of text. Human rating and error analysis remains a crucial component for any evaluation of long text generation. In this paper, we introduce FALTE, a web-based annotation toolkit designed to address this shortcoming. Our tool allows researchers to collect fine-grained judgments of text quality from crowdworkers using an error taxonomy specific to the downstream task. Using the taskinterface, annotators can select and assign error labels to text span selections in an incremental paragraph-level annotation workflow. The latter functionality is designed to simplify the document-level task into smaller units and reduce cognitive load on the annotators. Our tool has previously been used to run a large-scale annotation study that evaluates the coherence of long generated summaries, demonstrating its utility. | Tanya Goyal, Junyi Jessy Li, Greg Durrett |  |
| 584 |  |  [SEAL: Interactive Tool for Systematic Error Analysis and Labeling](https://doi.org/10.18653/v1/2022.emnlp-demos.36) |  | 0 | With the advent of Transformers, large language models (LLMs) have saturated well-known NLP benchmarks and leaderboards with high aggregate performance. However, many times these models systematically fail on tail data or rare groups not obvious in aggregate evaluation. Identifying such problematic data groups is even more challenging when there are no explicit labels (e.g., ethnicity, gender, etc.) and further compounded for NLP datasets due to the lack of visual features to characterize failure modes (e.g., Asian males, animals indoors, waterbirds on land etc.). This paper introduces an interactive Systematic Error Analysis and Labeling (SEAL) tool that uses a two-step approach to first identify high-error slices of data and then, in the second step, introduce methods to give human-understandable semantics to those underperforming slices. We explore a variety of methods for coming up with coherent semantics for the error groups using language models for semantic labeling and a text-to-image model for generating visual features.SEAL is available at https://huggingface.co/spaces/nazneen/seal. | Nazneen Rajani, Weixin Liang, Lingjiao Chen, Margaret Mitchell, James Zou |  |
| 585 |  |  [Hands-On Interactive Neuro-Symbolic NLP with DRaiL](https://doi.org/10.18653/v1/2022.emnlp-demos.37) |  | 0 | We recently introduced DRaiL, a declarative neural-symbolic modeling framework designed to support a wide variety of NLP scenarios. In this paper, we enhance DRaiL with an easy to use Python interface, equipped with methods to define, modify and augment DRaiL models interactively, as well as with methods to debug and visualize the predictions made. We demonstrate this interface with a challenging NLP task: predicting sentence and entity level moral sentiment in political tweets. | Maria Leonor Pacheco, Shamik Roy, Dan Goldwasser |  |
| 586 |  |  [Paraphrastic Representations at Scale](https://doi.org/10.18653/v1/2022.emnlp-demos.38) |  | 0 |  | John Wieting, Kevin Gimpel, Graham Neubig, Taylor BergKirkpatrick |  |
| 587 |  |  [Snoopy: An Online Interface for Exploring the Effect of Pretraining Term Frequencies on Few-Shot LM Performance](https://doi.org/10.18653/v1/2022.emnlp-demos.39) |  | 0 | Current evaluation schemes for large language models often fail to consider the impact of the overlap between pretraining corpus and test data on model performance statistics. Snoopy is an online interface that allows researchers to study this impact in few-shot learning settings. Our demo provides term frequency statistics for the Pile, which is an 800 GB corpus, accompanied by the precomputed performance of EleutherAI/GPT models on more than 20 NLP benchmarks, including numerical, commonsense reasoning, natural language understanding, and question-answering tasks. Snoopy allows a user to interactively align specific terms in test instances with their frequency in the Pile, enabling exploratory analysis of how term frequency is related to the accuracy of the models, which are hard to discover through automated means. A user can look at correlations over various model sizes and numbers of in-context examples and visualize the result across multiple (potentially aggregated) datasets. Using Snoopy, we show that a researcher can quickly replicate prior analyses for numerical tasks, while simultaneously allowing for much more expansive exploration that was previously challenging. Snoopy is available at https://nlp.ics.uci.edu/snoopy. | Yasaman Razeghi, Raja Sekhar Reddy Mekala, Robert L. Logan IV, Matt Gardner, Sameer Singh |  |
| 588 |  |  [BMCook: A Task-agnostic Compression Toolkit for Big Models](https://doi.org/10.18653/v1/2022.emnlp-demos.40) |  | 0 | Recently, pre-trained language models (PLMs) have achieved great success on various NLP tasks and have shown a trend of exponential growth in model size. To alleviate the unaffordable computational costs brought by the size growth, model compression has been widely explored. Existing efforts have achieved promising results in compressing medium-sized models for specific tasks, while task-agnostic compression for big models with over billions of parameters is rarely studied. Task-agnostic compression can provide an efficient and versatile big model for both prompting and delta tuning, leading to a more general impact than task-specific compression. Hence, we introduce a task-agnostic compression toolkit BMCook for big models. In BMCook, we implement four representative compression methods, including quantization, pruning, distillation, and MoEfication. Developers can easily combine these methods towards better efficiency. To evaluate BMCook, we apply it to compress T5-3B (a PLM with 3 billion parameters). We achieve nearly 12x efficiency improvement while maintaining over 97% of the original T5-3B performance on three typical NLP benchmarks. Moreover, the final compressed model also significantly outperforms T5-base (a PLM with 220 million parameters), which has a similar computational cost. BMCook is publicly available at https://github.com/OpenBMB/BMCook. | Zhengyan Zhang, Baitao Gong, Yingfa Chen, Xu Han, Guoyang Zeng, Weilin Zhao, Yanxu Chen, Zhiyuan Liu, Maosong Sun |  |
| 589 |  |  [ALToolbox: A Set of Tools for Active Learning Annotation of Natural Language Texts](https://doi.org/10.18653/v1/2022.emnlp-demos.41) |  | 0 |  | Akim Tsvigun, Leonid Sanochkin, Daniil Larionov, Gleb Kuzmin, Artem Vazhentsev, Ivan Lazichny, Nikita Khromov, Danil Kireev, Aleksandr Rubashevskii, Olga Shahmatova |  |
| 590 |  |  [TextBox 2.0: A Text Generation Library with Pre-trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-demos.42) |  | 0 | To facilitate research on text generation, this paper presents a comprehensive and unified library, TextBox 2.0, focusing on the use of pre-trained language models (PLMs). To be comprehensive, our library covers 13 common text generation tasks and their corresponding 83 datasets and further incorporates 45 PLMs covering general, translation, Chinese, dialogue, controllable, distilled, prompting, and lightweight PLMs. We also implement 4 efficient training strategies and provide 4 generation objectives for pre-training new PLMs from scratch. To be unified, we design the interfaces to support the entire research pipeline (from data loading to training and evaluation), ensuring that each step can be fulfilled in a unified way. Despite the rich functionality, it is easy to use our library, either through the friendly Python API or command line. To validate the effectiveness of our library, we conduct extensive experiments and exemplify four types of research scenarios. The project is released at the link: https://github.com/RUCAIBox/TextBox#2.0. | Tianyi Tang, Junyi Li, Zhipeng Chen, Yiwen Hu, Zhuohao Yu, Wenxun Dai, Wayne Xin Zhao, JianYun Nie, JiRong Wen |  |
| 591 |  |  [Unsupervised Term Extraction for Highly Technical Domains](https://doi.org/10.18653/v1/2022.emnlp-industry.1) |  | 0 | Term extraction is an information extraction task at the root of knowledge discovery platforms. Developing term extractors that are able to generalize across very diverse and potentially highly technical domains is challenging, as annotations for domains requiring in-depth expertise are scarce and expensive to obtain. In this paper, we describe the term extraction subsystem of a commercial knowledge discovery platform that targets highly technical fields such as pharma, medical, and material science. To be able to generalize across domains, we introduce a fully unsupervised annotator (UA). It extracts terms by combining novel morphological signals from sub-word tokenization with term-to-topic and intra-term similarity metrics, computed using general-domain pre-trained sentence-encoders. The annotator is used to implement a weakly-supervised setup, where transformer-models are fine-tuned (or pre-trained) over the training data generated by running the UA over large unlabeled corpora. Our experiments demonstrate that our setup can improve the predictive performance while decreasing the inference latency on both CPUs and GPUs. Our annotators provide a very competitive baseline for all the cases where annotations are not available. | Francesco Fusco, Peter W. J. Staar, Diego Antognini |  |
| 592 |  |  [DynaMaR: Dynamic Prompt with Mask Token Representation](https://doi.org/10.18653/v1/2022.emnlp-industry.2) |  | 0 | Recent research has shown that large language models pretrained using unsupervised approaches can achieve significant performance improvement on many downstream tasks. Typically when adapting these language models to downstream tasks, like a classification or regression task, we employ a fine-tuning paradigm in which the sentence representation from the language model is input to a task-specific head; the model is then fine-tuned end-to-end. However, with the emergence of models like GPT-3, prompt-based fine-tuning has been proven to be a successful approach for few-shot tasks. Inspired by this work, we study discrete prompt technologies in practice. There are two issues that arise with the standard prompt approach. First, it can overfit on the prompt template. Second, it requires manual effort to formulate the downstream task as a language model problem. In this paper, we propose an improvement to prompt-based fine-tuning that addresses these two issues. We refer to our approach as DynaMaR – Dynamic Prompt with Mask Token Representation. Results show that DynaMaR can achieve an average improvement of 10% in few-shot settings and improvement of 3.7% in data-rich settings over the standard fine-tuning approach on four e-commerce applications. | Xiaodi Sun, Sunny Rajagopalan, Priyanka Nigam, Weiyi Lu, Yi Xu, Iman Keivanloo, Belinda Zeng, Trishul Chilimbi |  |
| 593 |  |  [A Hybrid Approach to Cross-lingual Product Review Summarization](https://doi.org/10.18653/v1/2022.emnlp-industry.3) |  | 0 | We present a hybrid approach for product review summarization which consists of: (i) an unsupervised extractive step to extract the most important sentences out of all the reviews, and (ii) a supervised abstractive step to summarize the extracted sentences into a coherent short summary. This approach allows us to develop an efficient cross-lingual abstractive summarizer that can generate summaries in any language, given the extracted sentences out of thousands of reviews in a source language. In order to train and test the abstractive model, we create the Cross-lingual Amazon Reviews Summarization (CARS) dataset which provides English summaries for training, and English, French, Italian, Arabic, and Hindi summaries for testing based on selected English reviews. We show that the summaries generated by our model are as good as human written summaries in coherence, informativeness, non-redundancy, and fluency. | Saleh Soltan, Victor Soto, Ke Tran, Wael Hamza |  |
| 594 |  |  [Augmenting Operations Research with Auto-Formulation of Optimization Models From Problem Descriptions](https://doi.org/10.18653/v1/2022.emnlp-industry.4) |  | 0 | We describe an augmented intelligence system for simplifying and enhancing the modeling experience for operations research. Using this system, the user receives a suggested formulation of an optimization problem based on its description. To facilitate this process, we build an intuitive user interface system that enables the users to validate and edit the suggestions. We investigate controlled generation techniques to obtain an automatic suggestion of formulation. Then, we evaluate their effectiveness with a newly created dataset of linear programming problems drawn from various application domains. | Rindra Ramamonjison, Haley Li, Timothy T. L. Yu, Shiqi He, Vishnu Rengan, Amin BanitalebiDehkordi, Zirui Zhou, Yong Zhang |  |
| 595 |  |  [Knowledge Distillation based Contextual Relevance Matching for E-commerce Product Search](https://doi.org/10.18653/v1/2022.emnlp-industry.5) |  | 0 | Online relevance matching is an essential task of e-commerce product search to boost the utility of search engines and ensure a smooth user experience. Previous work adopts either classical relevance matching models or Transformer-style models to address it. However, they ignore the inherent bipartite graph structures that are ubiquitous in e-commerce product search logs and are too inefficient to deploy online. In this paper, we design an efficient knowledge distillation framework for e-commerce relevance matching to integrate the respective advantages of Transformer-style models and classical relevance matching models. Especially for the core student model of the framework, we propose a novel method using k-order relevance modeling. The experimental results on large-scale real-world data (the size is 6 174 million) show that the proposed method significantly improves the prediction accuracy in terms of human relevance judgment. We deploy our method to JD.com online search platform. The A/B testing results show that our method significantly improves most business metrics under price sort mode and default sort mode. | Ziyang Liu, Chaokun Wang, Hao Feng, Lingfei Wu, Liqun Yang |  |
| 596 |  |  [Accelerating the Discovery of Semantic Associations from Medical Literature: Mining Relations Between Diseases and Symptoms](https://doi.org/10.18653/v1/2022.emnlp-industry.6) |  | 0 | Medical literature is a vast and constantly expanding source of information about diseases, their diagnoses and treatments. One of the ways to extract insights from this type of data is through mining association rules between such entities. However, existing solutions do not take into account the semantics of sentences from which entity co-occurrences are extracted. We propose a scalable solution for the automated discovery of semantic associations between different entities such as diseases and their symptoms. Our approach employs the UMLS semantic network and a binary relation classification model trained with distant supervision to validate and help ranking the most likely entity associations pairs extracted with frequency-based association rule mining algorithms. We evaluate the proposed system on the task of extracting disease-symptom associations from a collection of over 14M PubMed abstracts and validate our results against a publicly available known list of disease-symptom pairs. | Alberto Purpura, Francesca Bonin, Joao H. BettencourtSilva |  |
| 597 |  |  [PENTATRON: PErsonalized coNText-Aware Transformer for Retrieval-based cOnversational uNderstanding](https://doi.org/10.18653/v1/2022.emnlp-industry.7) |  | 0 | Conversational understanding is an integral part of modern intelligent devices. In a large fraction of the global traffic from customers using smart digital assistants, frictions in dialogues may be attributed to incorrect understanding of the entities in a customer’s query due to factors including ambiguous mentions, mispronunciation, background noise and faulty on-device signal processing. Such errors are compounded by two common deficiencies from intelligent devices namely, (1) the device not being tailored to individual customers, and (2) the device responses being unaware of the context in the conversation session. Viewing this problem via the lens of retrieval-based search engines, we build and evaluate a scalable entity correction system, PENTATRON. The system leverages a parametric transformer-based language model to learn patterns from in-session customer-device interactions coupled with a non-parametric personalized entity index to compute the correct query, which aids downstream components in reasoning about the best response. In addition to establishing baselines and demonstrating the value of personalized and context-aware systems, we use multitasking to learn the domain of the correct entity. We also investigate the utility of language model prompts. Through extensive experiments, we show a significant upward movement of the key metric (Exact Match) by up to 500.97% (relative to the baseline). | UmaNaresh Niranjan, Ziyan Jiang, Ankit Ankit, Sungjin Lee, Jie Hao, Xing Fan, Chenlei Guo |  |
| 598 |  |  [Machine translation impact in E-commerce multilingual search](https://doi.org/10.18653/v1/2022.emnlp-industry.8) |  | 0 | Previous work suggests that performance of cross-lingual information retrieval correlates highly with the quality of Machine Translation. However, there may be a threshold beyond which improving query translation quality yields little or no benefit to further improve the retrieval performance. This threshold may depend upon multiple factors including the source and target languages, the existing MT system quality and the search pipeline. In order to identify the benefit of improving an MT system for a given search pipeline, we investigate the sensitivity of retrieval quality to the presence of different levels of MT quality using experimental datasets collected from actual traffic. We systematically improve the performance of our MT systems quality on language pairs as measured by MT evaluation metrics including Bleu and Chrf to determine their impact on search precision metrics and extract signals that help to guide the improvement strategies. Using this information we develop techniques to compare query translations for multiple language pairs and identify the most promising language pairs to invest and improve. | Bryan Zhang, Amita Misra |  |
| 599 |  |  [Ask-and-Verify: Span Candidate Generation and Verification for Attribute Value Extraction](https://doi.org/10.18653/v1/2022.emnlp-industry.9) |  | 0 | The product attribute value extraction (AVE) task aims to capture key factual information from product profiles, and is useful for several downstream applications in e-Commerce platforms. Previous contributions usually formulate this task using sequence labeling or reading comprehension architectures. However, sequence labeling models tend to be conservative in their predictions resulting in a high false negative rate. Existing reading comprehension formulations, on the other hand, can over-generate attribute values which hinders precision. In the present work we address these limitations with a new end-to-end pipeline framework called Ask-and-Verify. Given a product and an attribute query, the Ask step detects the top-K span candidates (i.e. possible attribute values) from the product profiles, then the Verify step filters out false positive candidates. We evaluate Ask-and-Verify model on Amazon’s product pages and AliExpress public dataset, and present a comparative analysis as well as a detailed ablation study. Despite its simplicity, we show that Ask-and-Verify outperforms recent state-of-the-art models by up to 3.1% F1 absolute improvement points, while also scaling to thousands of attributes. | Yifan Ding, Yan Liang, Nasser Zalmout, Xian Li, Christan Grant, Tim Weninger |  |
| 600 |  |  [Consultation Checklists: Standardising the Human Evaluation of Medical Note Generation](https://doi.org/10.18653/v1/2022.emnlp-industry.10) |  | 0 | Evaluating automatically generated text is generally hard due to the inherently subjective nature of many aspects of the output quality. This difficulty is compounded in automatic consultation note generation by differing opinions between medical experts both about which patient statements should be included in generated notes and about their respective importance in arriving at a diagnosis. Previous real-world evaluations of note-generation systems saw substantial disagreement between expert evaluators. In this paper we propose a protocol that aims to increase objectivity by grounding evaluations in Consultation Checklists, which are created in a preliminary step and then used as a common point of reference during quality assessment. We observed good levels of inter-annotator agreement in a first evaluation study using the protocol; further, using Consultation Checklists produced in the study as reference for automatic metrics such as ROUGE or BERTScore improves their correlation with human judgements compared to using the original human note. | Aleksandar Savkov, Francesco Moramarco, Alex PapadopoulosKorfiatis, Mark Perera, Anya Belz, Ehud Reiter |  |
| 601 |  |  [Towards Need-Based Spoken Language Understanding Model Updates: What Have We Learned?](https://doi.org/10.18653/v1/2022.emnlp-industry.11) |  | 0 | In productionized machine learning systems, online model performance is known to deteriorate over time when there is a distributional drift between offline training and online application data. As a remedy, models are typically retrained at fixed time intervals, implying high computational and manual costs. This work aims at decreasing such costs in productionized, large-scale Spoken Language Understanding systems. In particular, we develop a need-based re-training strategy guided by an efficient drift detector and discuss the arising challenges including system complexity, overlapping model releases, observation limitation and the absence of annotated resources at runtime. We present empirical results on historical data and confirm the utility of our design decisions via an online A/B experiment. | Quynh Do, Judith Gaspers, Daniil Sorokin, Patrick Lehnen |  |
| 602 |  |  [Knowledge Distillation Transfer Sets and their Impact on Downstream NLU Tasks](https://doi.org/10.18653/v1/2022.emnlp-industry.12) |  | 0 | Teacher-student knowledge distillation is a popular technique for compressing today’s prevailing large language models into manageable sizes that fit low-latency downstream applications. Both the teacher and the choice of transfer set used for distillation are crucial ingredients in creating a high quality student. Yet, the generic corpora used to pretrain the teacher and the corpora associated with the downstream target domain are often significantly different, which raises a natural question: should the student be distilled over the generic corpora, so as to learn from high-quality teacher predictions, or over the downstream task corpora to align with finetuning? Our study investigates this trade-off using Domain Classification (DC) and Intent Classification/Named Entity Recognition (ICNER) as downstream tasks. We distill several multilingual students from a larger multilingual LM with varying proportions of generic and task-specific datasets, and report their performance after finetuning on DC and ICNER. We observe significant improvements across tasks and test sets when only task-specific corpora is used. We also report on how the impact of adding task-specific data to the transfer set correlates with the similarity between generic and task-specific data. Our results clearly indicate that, while distillation from a generic LM benefits downstream tasks, students learn better using target domain data even if it comes at the price of noisier teacher predictions. In other words, target domain data still trumps teacher knowledge. | Charith Peris, Lizhen Tan, Thomas Gueudré, Turan Gojayev, Pan Wei, Gokmen Oz |  |
| 603 |  |  [Exploiting In-Domain Bilingual Corpora for Zero-Shot Transfer Learning in NLU of Intra-Sentential Code-Switching Chatbot Interactions](https://doi.org/10.18653/v1/2022.emnlp-industry.13) |  | 0 |  | Maia Aguirre, Manex Serras, Laura GarcíaSardiña, Jacobo LopezFernandez, Ariane Méndez, Arantza del Pozo |  |
| 604 |  |  [Calibrating Imbalanced Classifiers with Focal Loss: An Empirical Study](https://doi.org/10.18653/v1/2022.emnlp-industry.14) |  | 0 | Imbalanced data distribution is a practical and common challenge in building production-level machine learning (ML) models in industry, where data usually exhibits long-tail distributions. For instance, in virtual AI Assistants, such as Google Assistant, Amazon Alexa and Apple Siri, the “play music” or “set timer” utterance is exposed to an order of magnitude more traffic than other skills. This can easily cause trained models to overfit to the majority classes, categories or intents, lead to model miscalibration. The uncalibrated models output unreliable (mostly overconfident) predictions, which are at high risk of affecting downstream decision-making systems. In this work, we study the calibration of production models in the industry use-case of predicting product return reason codes in customer service conversations of an online retail store; The returns reasons also exhibit class imbalance. To alleviate the resulting miscalibration in the production ML model, we streamline the model development and deployment using focal loss (CITATION).We empirically show the effectiveness of model training with focal loss in learning better calibrated models, as compared to standard cross-entropy loss. Better calibration, in turn, enables better control of the precision-recall trade-off for the models deployed in production. | Cheng Wang, Jorge Balazs, György Szarvas, Patrick Ernst, Lahari Poddar, Pavel Danchenko |  |
| 605 |  |  [Unsupervised training data re-weighting for natural language understanding with local distribution approximation](https://doi.org/10.18653/v1/2022.emnlp-industry.15) |  | 0 | One of the major challenges of training Natural Language Understanding (NLU) production models lies in the discrepancy between the distributions of the offline training data and of the online live data, due to, e.g., biased sampling scheme, cyclic seasonality shifts, annotated training data coming from a variety of different sources, and a changing pool of users. Consequently, the model trained by the offline data is biased. We often observe this problem especially in task-oriented conversational systems, where topics of interest and the characteristics of users using the system change over time. In this paper we propose an unsupervised approach to mitigate the offline training data sampling bias in multiple NLU tasks. We show that a local distribution approximation in the pre-trained embedding space enables the estimation of importance weights for training samples guiding re-sampling for an effective bias mitigation. We illustrate our novel approach using multiple NLU datasets and show improvements obtained without additional annotation, making this a general approach for mitigating effects of sampling bias. | Jose Garrido Ramas, DieuThu Le, Bei Chen, Manoj Kumar, Kay Rottmann |  |
| 606 |  |  [Cross-Encoder Data Annotation for Bi-Encoder Based Product Matching](https://doi.org/10.18653/v1/2022.emnlp-industry.16) |  | 0 | Matching a seller listed item to an appropriate product is an important step for an e-commerce platform. With the recent advancement in deep learning, there are different encoder based approaches being proposed as solution. When textual data for two products are available, cross-encoder approaches encode them jointly while bi-encoder approaches encode them separately. Since cross-encoders are computationally heavy, approaches based on bi-encoders are a common practice for this challenge. In this paper, we propose cross-encoder data annotation; a technique to annotate or refine human annotated training data for bi-encoder models using a cross-encoder model. This technique enables us to build a robust model without annotation on newly collected training data or further improve model performance on annotated training data. We evaluate the cross-encoder data annotation on the product matching task using a real-world e-commerce dataset containing 104 million products. Experimental results show that the cross-encoder data annotation improves 4% absolute accuracy when no annotation for training data is available, and 2% absolute accuracy when annotation for training data is available. | Justin Chiu, Keiji Shinzato |  |
| 607 |  |  [Deploying a Retrieval based Response Model for Task Oriented Dialogues](https://doi.org/10.18653/v1/2022.emnlp-industry.17) |  | 0 |  | Lahari Poddar, György Szarvas, Cheng Wang, Jorge Balazs, Pavel Danchenko, Patrick Ernst |  |
| 608 |  |  [Tackling Temporal Questions in Natural Language Interface to Databases](https://doi.org/10.18653/v1/2022.emnlp-industry.18) |  | 0 | Temporal aspect is one of the most challenging areas in Natural Language Interface to Databases (NLIDB). This paper addresses and examines how temporal questions being studied and supported by the research community at both levels: popular annotated dataset (e.g. Spider) and recent advanced models. We present a new dataset with accompanied databases supporting temporal questions in NLIDB. We experiment with two SOTA models (Picard and ValueNet) to investigate how our new dataset helps these models learn and improve performance in temporal aspect. | Ngoc Phuoc An Vo, Octavian Popescu, Irene Manotas, Vadim Sheinin |  |
| 609 |  |  [Multi-Tenant Optimization For Few-Shot Task-Oriented FAQ Retrieval](https://doi.org/10.18653/v1/2022.emnlp-industry.19) |  | 0 | Business-specific Frequently Asked Questions (FAQ) retrieval in task-oriented dialog systems poses unique challenges vis à vis community based FAQs. Each FAQ question represents an intent which is usually an umbrella term for many related user queries. We evaluate performance for such Business FAQs both with standard FAQ retrieval techniques using query-Question (q-Q) similarity and few-shot intent detection techniques. Implementing a real-world solution for FAQ retrieval in order to support multiple tenants (FAQ sets) entails optimizing speed, accuracy and cost. We propose a novel approach to scale multi-tenant FAQ applications in real-world context by contrastive fine-tuning of the last layer in sentence Bi-Encoders along with tenant-specific weight switching. | Asha Vishwanathan, Rajeev Unnikrishnan Warrier, Gautham Vadakkekara Suresh, Chandra Shekhar Kandpal |  |
| 610 |  |  [Iterative Stratified Testing and Measurement for Automated Model Updates](https://doi.org/10.18653/v1/2022.emnlp-industry.20) |  | 0 | Automating updates to machine learning systems is an important but understudied challenge in AutoML. The high model variance of many cutting-edge deep learning architectures means that retraining a model provides no guarantee of accurate inference on all sample types. To address this concern, we present Automated Data-Shape Stratified Model Updates (ADSMU), a novel framework that relies on iterative model building coupled with data-shape stratified model testing and improvement. Using ADSMU, we observed a 26% (relative) improvement in accuracy for new model use cases on a large-scale NLU system, compared to a naive (manually) retrained baseline and current cutting-edge methods. | Elizabeth Dekeyser, Nicholas Comment, Shermin Pei, Rajat Kumar, Shruti Rai, Fengtao Wu, Lisa A. Haverty, Kanna Shimizu |  |
| 611 |  |  [SLATE: A Sequence Labeling Approach for Task Extraction from Free-form Inked Content](https://doi.org/10.18653/v1/2022.emnlp-industry.21) |  | 0 | We present SLATE, a sequence labeling approach for extracting tasks from free-form content such as digitally handwritten (or “inked”) notes on a virtual whiteboard. Our approach allows us to create a single, low-latency model to simultaneously perform sentence segmentation and classification of these sentences into task/non-task sentences. SLATE greatly outperforms a baseline two-model (sentence segmentation followed by classification model) approach, achieving a task F1 score of 84.4%, a sentence segmentation (boundary similarity) score of 88.4% and three times lower latency compared to the baseline. Furthermore, we provide insights into tackling challenges of performing NLP on the inking domain. We release both our code and dataset for this novel task. | Apurva Gandhi, Ryan Serrao, Biyi Fang, Gilbert Antonius, Jenna Hong, Tra My Nguyen, Sheng Yi, Ehi Nosakhare, Irene Shaffer, Soundararajan Srinivasan |  |
| 612 |  |  [Gaining Insights into Unrecognized User Utterances in Task-Oriented Dialog Systems](https://doi.org/10.18653/v1/2022.emnlp-industry.22) |  | 0 | The rapidly growing market demand for automatic dialogue agents capable of goal-oriented behavior has caused many tech-industry leaders to invest considerable efforts into task-oriented dialog systems. The success of these systems is highly dependent on the accuracy of their intent identification – the process of deducing the goal or meaning of the user’s request and mapping it to one of the known intents for further processing. Gaining insights into unrecognized utterances – user requests the systems fails to attribute to a known intent – is therefore a key process in continuous improvement of goal-oriented dialog systems. We present an end-to-end pipeline for processing unrecognized user utterances, deployed in a real-world, commercial task-oriented dialog system, including a specifically-tailored clustering algorithm, a novel approach to cluster representative extraction, and cluster naming. We evaluated the proposed components, demonstrating their benefits in the analysis of unrecognized user requests. | Ella Rabinovich, Matan Vetzler, David Boaz, Vineet Kumar, Gaurav Pandey, Ateret AnabyTavor |  |
| 613 |  |  [CoCoID: Learning Contrastive Representations and Compact Clusters for Semi-Supervised Intent Discovery](https://doi.org/10.18653/v1/2022.emnlp-industry.23) |  | 0 | Intent discovery is to mine new intents from user utterances, which are not present in the set of manually predefined intents. Previous approaches to intent discovery usually automatically cluster novel intents with prior knowledge from intent-labeled data in a semi-supervised way. In this paper, we focus on the discriminative user utterance representation learning and the compactness of the learned intent clusters. We propose a novel semi-supervised intent discovery framework CoCoID with two essential components: contrastive user utterance representation learning and intra-cluster knowledge distillation. The former attempts to detect similar and dissimilar intents from a minibatch-wise perspective. The latter regularizes the predictive distribution of the model over samples in a cluster-wise way. We conduct experiments on both real-life challenging datasets (i.e., CLINC and BANKING) that are curated to emulate the true environment of commercial/production systems and traditional datasets (i.e., StackOverflow and DBPedia) to evaluate the proposed CoCoID. Experiment results demonstrate that our model substantially outperforms state-of-the-art intent discovery models (12 baselines) by over 1.4 ACC and ARI points and 1.1 NMI points across the four datasets. Further analyses suggest that CoCoID is able to learn contrastive representations and compact clusters for intent discovery. | Qian Cao, Deyi Xiong, Qinlong Wang, Xia Peng |  |
| 614 |  |  [Tractable & Coherent Multi-Document Summarization: Discrete Optimization of Multiple Neural Modeling Streams via Integer Linear Programming](https://doi.org/10.18653/v1/2022.emnlp-industry.24) |  | 0 | One key challenge in multi-document summarization is the generated summary is often less coherent compared to single document summarization due to the larger heterogeneity of the input source content. In this work, we propose a generic framework to jointly consider coherence and informativeness in multi-document summarization and offers provisions to replace individual components based on the domain of source text. In particular, the framework characterizes coherence through verb transitions and entity mentions and takes advantage of syntactic parse trees and neural modeling for intra-sentential noise pruning. The framework cast the entire problem as an integer linear programming optimization problem with neural and non-neural models as linear components. We evaluate our method in the news and legal domains. The proposed approach consistently performs better than competitive baselines for both objective metrics and human evaluation. | Litton J. Kurisinkel, Nancy Chen |  |
| 615 |  |  [Grafting Pre-trained Models for Multimodal Headline Generation](https://doi.org/10.18653/v1/2022.emnlp-industry.25) |  | 0 | Multimodal headline utilizes both video frames and transcripts to generate the natural language title of the videos. Due to a lack of large-scale, manually annotated data, the task of annotating grounded headlines for video is labor intensive and impractical. Previous researches on pre-trained language models and video-language models have achieved significant progress in related downstream tasks. However, none of them can be directly applied to multimodal headline architecture where we need both multimodal encoder and sentence decoder. A major challenge in simply gluing language model and video-language model is the modality balance, which is aimed at combining visual-language complementary abilities. In this paper, we propose a novel approach to graft the video encoder from the pre-trained video-language model on the generative pre-trained language model. We also present a consensus fusion mechanism for the integration of different components, via inter/intra modality relation. Empirically, experiments show that the grafted model achieves strong results on a brand-new dataset collected from real-world applications. | Lingfeng Qiao, Chen Wu, Ye Liu, Haoyuan Peng, Di Yin, Bo Ren |  |
| 616 |  |  [Semi-supervised Adversarial Text Generation based on Seq2Seq models](https://doi.org/10.18653/v1/2022.emnlp-industry.26) |  | 0 | To improve deep learning models’ robustness, adversarial training has been frequently used in computer vision with satisfying results. However, adversarial perturbation on text have turned out to be more challenging due to the discrete nature of text. The generated adversarial text might not sound natural or does not preserve semantics, which is the key for real world applications where text classification is based on semantic meaning. In this paper, we describe a new way for generating adversarial samples by using pseudo-labeled in-domain text data to train a seq2seq model for adversarial generation and combine it with paraphrase detection. We showcase the benefit of our approach for a real-world Natural Language Understanding (NLU) task, which maps a user’s request to an intent. Furthermore, we experiment with gradient-based training for the NLU task and try using token importance scores to guide the adversarial text generation. We show that our approach can generate realistic and relevant adversarial samples compared to other state-of-the-art adversarial training methods. Applying adversarial training using these generated samples helps the NLU model to recover up to 70% of these types of errors and makes the model more robust, especially in the tail distribution in a large scale real world application. | Hieu Le, DieuThu Le, Verena Weber, Chris Church, Kay Rottmann, Melanie Bradford, Peter Chin |  |
| 617 |  |  [Is it out yet? Automatic Future Product Releases Extraction from Web Data](https://doi.org/10.18653/v1/2022.emnlp-industry.27) |  | 0 | Identifying the release of new products and their predicted demand in advance is highly valuable for E-Commerce marketplaces and retailers. The information of an upcoming product release is used for inventory management, marketing campaigns and pre-order suggestions. Often, the announcement of an upcoming product release is widely available in multiple web pages such as blogs, chats or news articles. However, to the best of our knowledge, an automatic system to extract future product releases from web data has not been presented. In this work we describe an ML-powered multi-stage pipeline to automatically identify future product releases and rank their predicted demand from unstructured pages across the whole web. Our pipeline includes a novel Longformer-based model which uses a global attention mechanism guided by pre-calculated Named Entity Recognition predictions related to product releases. The model training data is based on a new corpus of 30K web pages manually annotated to identify future product releases. We made the dataset openly available at https://doi.org/10.5281/zenodo.6894770. | Gilad Fuchs, Ido BenShaul, Matan Mandelbrod |  |
| 618 |  |  [Automatic Scene-based Topic Channel Construction System for E-Commerce](https://doi.org/10.18653/v1/2022.emnlp-industry.28) |  | 0 | Scene marketing that well demonstrates user interests within a certain scenario has proved effective for offline shopping. To conduct scene marketing for e-commerce platforms, this work presents a novel product form, scene-based topic channel which typically consists of a list of diverse products belonging to the same usage scenario and a topic title that describes the scenario with marketing words. As manual construction of channels is time-consuming due to billions of products as well as dynamic and diverse customers’ interests, it is necessary to leverage AI techniques to automatically construct channels for certain usage scenarios and even discover novel topics. To be specific, we first frame the channel construction task as a two-step problem, i.e., scene-based topic generation and product clustering, and propose an E-commerce Scene-based Topic Channel construction system (i.e., ESTC) to achieve automated production, consisting of scene-based topic generation model for the e-commerce domain, product clustering on the basis of topic similarity, as well as quality control based on automatic model filtering and human screening. Extensive offline experiments and online A/B test validates the effectiveness of such a novel product form as well as the proposed system. In addition, we also introduce the experience of deploying the proposed system on a real-world e-commerce recommendation platform. | Peng Lin, Yanyan Zou, Lingfei Wu, Mian Ma, Zhuoye Ding, Bo Long |  |
| 619 |  |  [SpeechNet: Weakly Supervised, End-to-End Speech Recognition at Industrial Scale](https://doi.org/10.18653/v1/2022.emnlp-industry.29) |  | 0 | End-to-end automatic speech recognition systems represent the state of the art, but they rely on thousands of hours of manually annotated speech for training, as well as heavyweight computation for inference. Of course, this impedes commercialization since most companies lack vast human and computational resources. In this paper, we explore training and deploying an ASR system in the label-scarce, compute-limited setting. To reduce human labor, we use a third-party ASR system as a weak supervision source, supplemented with labeling functions derived from implicit user feedback. To accelerate inference, we propose to route production-time queries across a pool of CUDA graphs of varying input lengths, the distribution of which best matches the traffic’s. Compared to our third-party ASR, we achieve a relative improvement in word-error rate of 8% and a speedup of 600%. Our system, called SpeechNet, currently serves 12 million queries per day on our voice-enabled smart television. To our knowledge, this is the first time a large-scale, Wav2vec-based deployment has been described in the academic literature. | Raphael Tang, Karun Kumar, Gefei Yang, Akshat Pandey, Yajie Mao, Vladislav Belyaev, Madhuri Emmadi, G. Craig Murray, Ferhan Ture, Jimmy Lin |  |
| 620 |  |  [Controlled Language Generation for Language Learning Items](https://doi.org/10.18653/v1/2022.emnlp-industry.30) |  | 0 | This work aims to employ natural language generation (NLG) to rapidly generate items for English language learning applications: this requires both language models capable of generating fluent, high-quality English, and to control the output of the generation to match the requirements of the relevant items. We experiment with deep pretrained models for this task, developing novel methods for controlling items for factors relevant in language learning: diverse sentences for different proficiency levels and argument structure to test grammar. Human evaluation demonstrates high grammatically scores for all models (3.4 and above out of 4), and higher length (24%) and complexity (9%) over the baseline for the advanced proficiency model. Our results show that we can achieve strong performance while adding additional control to ensure diverse, tailored content for individual users. | Kevin Stowe, Debanjan Ghosh, Mengxuan Zhao |  |
| 621 |  |  [Improving Text-to-SQL Semantic Parsing with Fine-grained Query Understanding](https://doi.org/10.18653/v1/2022.emnlp-industry.31) |  | 0 | Most recent research on Text-to-SQL semantic parsing relies on either parser itself or simple heuristic based approach to understand natural language query (NLQ). When synthesizing a SQL query, there is no explicit semantic information of NLQ available to the parser which leads to undesirable generalization performance. In addition, without lexical-level fine-grained query understanding, linking between query and database can only rely on fuzzy string match which leads to suboptimal performance in real applications. In view of this, in this paper we present a general-purpose, modular neural semantic parsing framework that is based on token-level fine-grained query understanding. Our framework consists of three modules: named entity recognizer (NER), neural entity linker (NEL) and neural semantic parser (NSP). By jointly modeling query and database, NER model analyzes user intents and identifies entities in the query. NEL model links typed entities to schema and cell values in database. Parser model leverages available semantic information and linking results and synthesizes tree-structured SQL queries based on dynamically generated grammar. Experiments on SQUALL, a newly released semantic parsing dataset, show that we can achieve 56.8% execution accuracy on WikiTableQuestions (WTQ) test set, which outperforms the state-of-the-art model by 2.7%. | Jun Wang, Patrick Ng, Alexander Hanbo Li, Jiarong Jiang, Zhiguo Wang, Bing Xiang, Ramesh Nallapati, Sudipta Sengupta |  |
| 622 |  |  [Unsupervised Dense Retrieval for Scientific Articles](https://doi.org/10.18653/v1/2022.emnlp-industry.32) |  | 0 | In this work, we build a dense retrieval based semantic search engine on scientific articles from Elsevier. The major challenge is that there is no labeled data for training and testing. We apply a state-of-the-art unsupervised dense retrieval model called Generative Pseudo Labeling that generates high-quality pseudo training labels. Furthermore, since the articles are unbalanced across different domains, we select passages from multiple domains to form balanced training data. For the evaluation, we create two test sets: one manually annotated and one automatically created from the meta-information of our data. We compare the semantic search engine with the currently deployed lexical search engine on the two test sets. The results of the experiment show that the semantic search engine trained with pseudo training labels can significantly improve search performance. | Dan Li, Vikrant Yadav, Zubair Afzal, George Tsatsaronis |  |
| 623 |  |  [Learning Geolocations for Cold-Start and Hard-to-Resolve Addresses via Deep Metric Learning](https://doi.org/10.18653/v1/2022.emnlp-industry.33) |  | 0 | With evergrowing digital adoption in the society and increasing demand for businesses to deliver to customers doorstep, the last mile hop of transportation planning poses unique challenges in emerging geographies with unstructured addresses. One of the crucial inputs to facilitate effective planning is the task of geolocating customer addresses. Existing systems operate by aggregating historical delivery locations or by resolving/matching addresses to known buildings and campuses to vend a high-precision geolocation. However, by design they fail to cater to a significant fraction of addresses which are new in the system and have inaccurate or missing building level information. We propose a framework to resolve these addresses (referred to as hard-to-resolve henceforth) to a shallower granularity termed as neighbourhood. Specifically, we propose a weakly supervised deep metric learning model to encode the geospatial semantics in address embeddings. We present empirical evaluation on India (IN) and the United Arab Emirates (UAE) hard-to-resolve addresses to show significant improvements in learning geolocations i.e., 22% (IN) & 55% (UAE) reduction in delivery defects (where learnt geocode is Y meters away from actual location), and 43% (IN) & 90% (UAE) reduction in 50th percentile (p50) distance between learnt and actual delivery locations over the existing production system. | Govind, Saurabh Sohoney |  |
| 624 |  |  [Meta-learning Pathologies from Radiology Reports using Variance Aware Prototypical Networks](https://doi.org/10.18653/v1/2022.emnlp-industry.34) |  | 0 | Large pretrained Transformer-based language models like BERT and GPT have changed the landscape of Natural Language Processing (NLP). However, fine tuning such models still requires a large number of training examples for each target task, thus annotating multiple datasets and training these models on various downstream tasks becomes time consuming and expensive. In this work, we propose a simple extension of the Prototypical Networks for few-shot text classification. Our main idea is to replace the class prototypes by Gaussians and introduce a regularization term that encourages the examples to be clustered near the appropriate class centroids. Experimental results show that our method outperforms various strong baselines on 13 public and 4 internal datasets. Furthermore, we use the class distributions as a tool for detecting potential out-of-distribution (OOD) data points during deployment. | Arijit Sehanobish, Kawshik Kannan, Nabila Abraham, Anasuya Das, Benjamin Odry |  |
| 625 |  |  [Named Entity Recognition in Industrial Tables using Tabular Language Models](https://doi.org/10.18653/v1/2022.emnlp-industry.35) |  | 0 | Specialized transformer-based models for encoding tabular data have gained interest in academia. Although tabular data is omnipresent in industry, applications of table transformers are still missing. In this paper, we study how these models can be applied to an industrial Named Entity Recognition (NER) problem where the entities are mentioned in tabular-structured spreadsheets. The highly technical nature of spreadsheets as well as the lack of labeled data present major challenges for fine-tuning transformer-based models. Therefore, we develop a dedicated table data augmentation strategy based on available domain-specific knowledge graphs. We show that this boosts performance in our low-resource scenario considerably. Further, we investigate the benefits of tabular structure as inductive bias compared to tables as linearized sequences. Our experiments confirm that a table transformer outperforms other baselines and that its tabular inductive bias is vital for convergence of transformer-based models. | Aneta Koleva, Martin Ringsquandl, Mark Buckley, Rakebul Hasan, Volker Tresp |  |
| 626 |  |  [Reinforced Question Rewriting for Conversational Question Answering](https://doi.org/10.18653/v1/2022.emnlp-industry.36) |  | 0 | Conversational Question Answering (CQA) aims to answer questions contained within dialogues, which are not easily interpretable without context. Developing a model to rewrite conversational questions into self-contained ones is an emerging solution in industry settings as it allows using existing single-turn QA systems to avoid training a CQA model from scratch. Previous work trains rewriting models using human rewrites as supervision. However, such objectives are disconnected with QA models and therefore more human-like rewrites do not guarantee better QA performance. In this paper we propose using QA feedback to supervise the rewriting model with reinforcement learning. Experiments show that our approach can effectively improve QA performance over baselines for both extractive and retrieval QA. Furthermore, human evaluation shows that our method can generate more accurate and detailed rewrites when compared to human annotations. | Zhiyu Chen, Jie Zhao, Anjie Fang, Besnik Fetahu, Oleg Rokhlenko, Shervin Malmasi |  |
| 627 |  |  [Improving Large-Scale Conversational Assistants using Model Interpretation based Training Sample Selection](https://doi.org/10.18653/v1/2022.emnlp-industry.37) |  | 0 | This paper presents an approach to identify samples from live traffic where the customer implicitly communicated satisfaction with Alexa’s responses, by leveraging interpretations of model behavior. Such customer signals are noisy and adding a large number of samples from live traffic to training set makes re-training infeasible. Our work addresses these challenges by identifying a small number of samples that grow training set by ~0.05% while producing statistically significant improvements in both offline and online tests. | Stefan Schroedl, Manoj Kumar, Kiana Hajebi, Morteza Ziyadi, Sriram Venkatapathy, Anil Ramakrishna, Rahul Gupta, Pradeep Natarajan |  |
| 628 |  |  [Improving Precancerous Case Characterization via Transformer-based Ensemble Learning](https://doi.org/10.18653/v1/2022.emnlp-industry.38) |  | 0 | The application of natural language processing (NLP) to cancer pathology reports has been focused on detecting cancer cases, largely ignoring precancerous cases. Improving the characterization of precancerous adenomas assists in developing diagnostic tests for early cancer detection and prevention, especially for colorectal cancer (CRC). Here we developed transformer-based deep neural network NLP models to perform the CRC phenotyping, with the goal of extracting precancerous lesion attributes and distinguishing cancer and precancerous cases. We achieved 0.914 macro-F1 scores for classifying patients into negative, non-advanced adenoma, advanced adenoma and CRC. We further improved the performance to 0.923 using an ensemble of classifiers for cancer status classification and lesion size named-entity recognition (NER). Our results demonstrated the potential of using NLP to leverage real-world health record data to facilitate the development of diagnostic tests for early cancer prevention. | Yizhen Zhong, Jiajie Xiao, Thomas Vetterli, Mahan Matin, Ellen Loo, Jimmy Lin, Richard Bourgon, Ofer Shapira |  |
| 629 |  |  [Developing Prefix-Tuning Models for Hierarchical Text Classification](https://doi.org/10.18653/v1/2022.emnlp-industry.39) |  | 0 | Hierarchical text classification (HTC) is a key problem and task in many industrial applications, which aims to predict labels organized in a hierarchy for given input text. For example, HTC can group the descriptions of online products into a taxonomy or organizing customer reviews into a hierarchy of categories. In real-life applications, while Pre-trained Language Models (PLMs) have dominated many NLP tasks, they face significant challenges too—the conventional fine-tuning process needs to modify and save models with a huge number of parameters. This is becoming more critical for HTC in both global and local modelling—the latter needs to learn multiple classifiers at different levels/nodes in a hierarchy. The concern will be even more serious since PLM sizes are continuing to increase in order to attain more competitive performances. Most recently, prefix tuning has become a very attractive technology by only tuning and saving a tiny set of parameters. Exploring prefix turning for HTC is hence highly desirable and has timely impact. In this paper, we investigate prefix tuning on HTC in two typical setups: local and global HTC. Our experiment shows that the prefix-tuning model only needs less than 1% of parameters and can achieve performance comparable to regular full fine-tuning. We demonstrate that using contrastive learning in learning prefix vectors can further improve HTC performance. | Lei Chen, Hou Wei Chou, Xiaodan Zhu |  |
| 630 |  |  [PAIGE: Personalized Adaptive Interactions Graph Encoder for Query Rewriting in Dialogue Systems](https://doi.org/10.18653/v1/2022.emnlp-industry.40) |  | 0 | Unexpected responses or repeated clarification questions from conversational agents detract from the users’ experience with technology meant to streamline their daily tasks. To reduce these frictions, Query Rewriting (QR) techniques replace transcripts of faulty queries with alternatives that lead to responses thatsatisfy the users’ needs. Despite their successes, existing QR approaches are limited in their ability to fix queries that require considering users’ personal preferences. We improve QR by proposing Personalized Adaptive Interactions Graph Encoder (PAIGE).PAIGE is the first QR architecture that jointly models user’s affinities and query semantics end-to-end. The core idea is to represent previous user-agent interactions and world knowledge in a structured form — a heterogeneous graph — and apply message passing to propagate latent representations of users’ affinities to refine utterance embeddings.Using these embeddings, PAIGE can potentially provide different rewrites given the same query for users with different preferences. Our model, trained without any human-annotated data, improves the rewrite retrieval precision of state-of-the-art baselines by 12.5–17.5% while having nearly ten times fewer parameters. | Daniel Bis, Saurabh Gupta, Jie Hao, Xing Fan, Chenlei Guo |  |
| 631 |  |  [Fast Vocabulary Transfer for Language Model Compression](https://doi.org/10.18653/v1/2022.emnlp-industry.41) |  | 0 | Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance. | Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, Paolo Torroni |  |
| 632 |  |  [Multimodal Context Carryover](https://doi.org/10.18653/v1/2022.emnlp-industry.42) |  | 0 | Multi-modality support has become an integral part of creating a seamless user experience with modern voice assistants with smart displays. Users refer to images, video thumbnails, or the accompanying text descriptions on the screen through voice communication with AI powered devices. This raises the need to either augment existing commercial voice only dialogue systems with state-of-the-art multimodal components, or to introduce entirely new architectures; where the latter can lead to costly system revamps. To support the emerging visual navigation and visual product selection use cases, we propose to augment commercially deployed voice-only dialogue systems with additional multi-modal components. In this work, we present a novel yet pragmatic approach to expand an existing dialogue-based context carryover system (Chen et al., 2019a) in a voice assistant with state-of-the-art multimodal components to facilitate quick delivery of visual modality support with minimum changes. We demonstrate a 35% accuracy improvement over the existing system on an in-house multi-modal visual navigation data set. | Prashan Wanigasekara, Nalin Gupta, Fan Yang, Emre Barut, Zeynab Raeesy, Kechen Qin, Stephen Rawls, Xinyue Liu, Chengwei Su, Spurthi Sandiri |  |
| 633 |  |  [Distilling Multilingual Transformers into CNNs for Scalable Intent Classification](https://doi.org/10.18653/v1/2022.emnlp-industry.43) |  | 0 | We describe an application of Knowledge Distillation used to distill and deploy multilingual Transformer models for voice assistants, enabling text classification for customers globally. Transformers have set new state-of-the-art results for tasks like intent classification, and multilingual models exploit cross-lingual transfer to allow serving requests across 100+ languages. However, their prohibitive inference time makes them impractical to deploy in real-world scenarios with low latency requirements, such as is the case of voice assistants. We address the problem of cross-architecture distillation of multilingual Transformers to simpler models, while maintaining multilinguality without performance degradation. Training multilingual student models has received little attention, and is our main focus. We show that a teacher-student framework, where the teacher’s unscaled activations (logits) on unlabelled data are used to supervise student model training, enables distillation of Transformers into efficient multilingual CNN models. Our student model achieves equivalent performance as the teacher, and outperforms a similar model trained on the labelled data used to train the teacher model. This approach has enabled us to accurately serve global customer requests at speed (18x improvement), scale, and low cost. | Besnik Fetahu, Akash Veeragouni, Oleg Rokhlenko, Shervin Malmasi |  |
| 634 |  |  [Bringing the State-of-the-Art to Customers: A Neural Agent Assistant Framework for Customer Service Support](https://doi.org/10.18653/v1/2022.emnlp-industry.44) |  | 0 | Building Agent Assistants that can help improve customer service support requires inputs from industry users and their customers, as well as knowledge about state-of-the-art Natural Language Processing (NLP) technology. We combine expertise from academia and industry to bridge the gap and build task/domain-specific Neural Agent Assistants (NAA) with three high-level components for: (1) Intent Identification, (2) Context Retrieval, and (3) Response Generation. In this paper, we outline the pipeline of the NAA’s core system and also present three case studies in which three industry partners successfully adapt the framework to find solutions to their unique challenges. Our findings suggest that a collaborative process is instrumental in spurring the development of emerging NLP models for Conversational AI tasks in industry. The full reference implementation code and results are available at https://github.com/VectorInstitute/NAA. | Stephen Obadinma, Faiza Khan Khattak, Shirley Wang, Tania Sidhorn, Elaine Lau, Sean Robertson, Jingcheng Niu, Winnie Au, Alif Munim, Karthik Raja K. Bhaskar |  |
| 635 |  |  [Zero-Shot Dynamic Quantization for Transformer Inference](https://doi.org/10.18653/v1/2022.emnlp-industry.45) |  | 0 | We introduce a novel run-time method for significantly reducing the accuracy loss associated with quantizing BERT-like models to 8-bit integers. Existing methods for quantizing models either modify the training procedure, or they require an additional calibration step to adjust parameters that also requires a selected held-out dataset. Our method permits taking advantage of quantization without the need for these adjustments. We present results on several NLP tasks demonstrating the usefulness of this technique. | Yousef ElKurdi, Jerry Quinn, Avi Sil |  |
| 636 |  |  [Fact Checking Machine Generated Text with Dependency Trees](https://doi.org/10.18653/v1/2022.emnlp-industry.46) |  | 0 | Factual and logical errors made by Natural Language Generation (NLG) systems limit their applicability in many settings. We study this problem in a conversational search and recommendation setting, and observe that we can often make two simplifying assumptions in this domain: (i) there exists a body of structured knowledge we can use for verifying factuality of generated text; and (ii) the text to be factually assessed typically has a well-defined structure and style. Grounded in these assumptions, we propose a fast, unsupervised and explainable technique, DepChecker, that assesses factuality of input text based on rules derived from structured knowledge patterns and dependency relations with respect to the input text. We show that DepChecker outperforms state-of-the-art, general purpose fact-checking techniques in this special, but important case. | Alex Estes, Nikhita Vedula, Marcus D. Collins, Matt Cecil, Oleg Rokhlenko |  |
| 637 |  |  [Prototype-Representations for Training Data Filtering in Weakly-Supervised Information Extraction](https://doi.org/10.18653/v1/2022.emnlp-industry.47) |  | 0 | The availability of high quality training data is still a bottleneck for the practical utilization of information extraction models, despite the breakthroughs in zero and few-shot learning techniques. This is further exacerbated for industry applications, where new tasks, domains, and specific use cases keep arising, which makes it impractical to depend on manually annotated data. Therefore, weak and distant supervision emerged as popular approaches to bootstrap training, utilizing labeling functions to guide the annotation process. Weakly-supervised annotation of training data is fast and efficient, however, it results in many irrelevant and out-of-context matches. This is a challenging problem that can degrade the performance in downstream models, or require a manual data cleaning step that can incur significant overhead. In this paper we present a prototype-based filtering approach, that can be utilized to denoise weakly supervised training data. The system is very simple, unsupervised, scalable, and requires little manual intervention, yet results in significant precision gains. We apply the technique in the task of attribute value extraction in e-commerce websites, and achieve up to 9% gain in precision for the downstream models, with a minimal drop in recall. | Nasser Zalmout, Xian Li |  |
| 638 |  |  [CGF: Constrained Generation Framework for Query Rewriting in Conversational AI](https://doi.org/10.18653/v1/2022.emnlp-industry.48) |  | 0 | In conversational AI agents, Query Rewriting (QR) plays a crucial role in reducing user frictions and satisfying their daily demands. User frictions are caused by various reasons, such as errors in the conversational AI system, users’ accent or their abridged language. In this work, we present a novel Constrained Generation Framework (CGF) for query rewriting at both global and personalized levels. It is based on the encoder-decoder framework, where the encoder takes the query and its previous dialogue turns as the input to form a context-enhanced representation, and the decoder uses constrained decoding to generate the rewrites based on the pre-defined global or personalized constrained decoding space. Extensive offline and online A/B experiments show that the proposed CGF significantly boosts the query rewriting performance. | Jie Hao, Yang Liu, Xing Fan, Saurabh Gupta, Saleh Soltan, Rakesh Chada, Pradeep Natarajan, Chenlei Guo, Gökhan Tür |  |
| 639 |  |  [Entity-level Sentiment Analysis in Contact Center Telephone Conversations](https://doi.org/10.18653/v1/2022.emnlp-industry.49) |  | 0 | Entity-level sentiment analysis predicts the sentiment about entities mentioned in a given text. It is very useful in a business context to understand user emotions towards certain entities, such as products or companies. In this paper, we demonstrate how we developed an entity-level sentiment analysis system that analyzes English telephone conversation transcripts in contact centers to provide business insight. We present two approaches, one entirely based on the transformer-based DistilBERT model, and another that uses a neural network supplemented with some heuristic rules. | XueYong Fu, Cheng Chen, Md. Tahmid Rahman Laskar, Shayna Gardiner, Pooja Hiranandani, Shashi Bhushan TN |  |
| 640 |  |  [QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation](https://doi.org/10.18653/v1/2022.emnlp-industry.50) |  | 0 | Large Language Models (LLMs) have shown impressive results on a variety of text understanding tasks. Search queries though pose a unique challenge, given their short-length and lack of nuance or context. Complicated feature engineering efforts do not always lead to downstream improvements as their performance benefits may be offset by increased complexity of knowledge distillation. Thus, in this paper we make the following contributions: (1) We demonstrate that Retrieval Augmentation of queries provides LLMs with valuable additional context enabling improved understanding. While Retrieval Augmentation typically increases latency of LMs (thus hurting distillation efficacy), (2) we provide a practical and effective way of distilling Retrieval Augmentation LLMs. Specifically, we use a novel two-stage distillation approach that allows us to carry over the gains of retrieval augmentation, without suffering the increased compute typically associated with it. (3) We demonstrate the benefits of the proposed approach (QUILL) on a billion-scale, real-world query understanding system resulting in huge gains. Via extensive experiments, including on public benchmarks, we believe this work offers a recipe for practical use of retrieval-augmented query understanding. | Krishna Srinivasan, Karthik Raman, Anupam Samanta, Lingrui Liao, Luca Bertelli, Michael Bendersky |  |
| 641 |  |  [Distinguish Sense from Nonsense: Out-of-Scope Detection for Virtual Assistants](https://doi.org/10.18653/v1/2022.emnlp-industry.51) |  | 0 | Out of Scope (OOS) detection in Conversational AI solutions enables a chatbot to handle a conversation gracefully when it is unable to make sense of the end-user query. Accurately tagging a query as out-of-domain is particularly hard in scenarios when the chatbot is not equipped to handle a topic which has semantic overlap with an existing topic it is trained on. We propose a simple yet effective OOS detection method that outperforms standard OOS detection methods in a real-world deployment of virtual assistants. We discuss the various design and deployment considerations for a cloud platform solution to train virtual assistants and deploy them at scale. Additionally, we propose a collection of datasets that replicates real-world scenarios and show comprehensive results in various settings using both offline and online evaluation metrics. | Cheng Qian, Haode Qi, Gengyu Wang, Ladislav Kunc, Saloni Potdar |  |
| 642 |  |  [PLATO-Ad: A Unified Advertisement Text Generation Framework with Multi-Task Prompt Learning](https://doi.org/10.18653/v1/2022.emnlp-industry.52) |  | 0 | Online advertisement text generation aims at generating attractive and persuasive text ads to appeal to users clicking ads or purchasing products. While pretraining-based models have achieved remarkable success in generating high-quality text ads, some challenges still remain, such as ad generation in low-resource scenarios and training efficiency for multiple ad tasks. In this paper, we propose a novel unified text ad generation framework with multi-task prompt learning, called PLATO-Ad, totackle these problems. Specifically, we design a three-phase transfer learning mechanism to tackle the low-resource ad generation problem. Furthermore, we present a novel multi-task prompt learning mechanism to efficiently utilize a single lightweight model to solve multiple ad generation tasks without loss of performance compared to training a separate model for each task. Finally, we conduct offline and online evaluations and experiment results show that PLATO-Ad significantly outperforms the state-of-the-art on both offline and online metrics. PLATO-Ad has been deployed in a leading advertising platform with 3.5% CTR improvement on search ad descriptions and 10.4% CTR improvement on feed ad titles. | Zeyang Lei, Chao Zhang, Xinchao Xu, Wenquan Wu, ZhengYu Niu, Hua Wu, Haifeng Wang, Yi Yang, Shuanglong Li |  |
| 643 |  |  [Dense Feature Memory Augmented Transformers for COVID-19 Vaccination Search Classification](https://doi.org/10.18653/v1/2022.emnlp-industry.53) |  | 0 | With the devastating outbreak of COVID-19, vaccines are one of the crucial lines of defense against mass infection in this global pandemic. Given the protection they provide, vaccines are becoming mandatory in certain social and professional settings. This paper presents a classification model for detecting COVID-19 vaccination related search queries, a machine learning model that is used to generate search insights for COVID-19 vaccinations. The proposed method combines and leverages advancements from modern state-of-the-art (SOTA) natural language understanding (NLU) techniques such as pretrained Transformers with traditional dense features. We propose a novel approach of considering dense features as memory tokens that the model can attend to. We show that this new modeling approach enables a significant improvement to the Vaccine Search Insights (VSI) task, improving a strong well-established gradient-boosting baseline by relative +15% improvement in F1 score and +14% in precision. | Jai Gupta, Yi Tay, Chaitanya Kamath, Vinh Tran, Donald Metzler, Shailesh Bavadekar, Mimi Sun, Evgeniy Gabrilovich |  |
| 644 |  |  [Full-Stack Information Extraction System for Cybersecurity Intelligence](https://doi.org/10.18653/v1/2022.emnlp-industry.54) |  | 0 | Due to rapidly growing cyber-attacks and security vulnerabilities, many reports on cyber-threat intelligence (CTI) are being published daily. While these reports can help security analysts to understand on-going cyber threats,the overwhelming amount of information makes it difficult to digest the information in a timely manner. This paper presents, SecIE, an industrial-strength full-stack information extraction (IE) system for the security domain. SecIE can extract a large number of security entities, relations and the temporal information of the relations, which is critical for cyberthreat investigations. Our evaluation with 133 labeled threat reports containing 108,021 tokens shows thatSecIE achieves over 92% F1-score for entity extraction and about 70% F1-score for relation extraction. We also showcase how SecIE can be used for downstream security applications. | Youngja Park, Taesung Lee |  |
| 645 |  |  [Deploying Unified BERT Moderation Model for E-Commerce Reviews](https://doi.org/10.18653/v1/2022.emnlp-industry.55) |  | 0 | Moderation of user-generated e-commerce content has become crucial due to the large and diverse user base on the platforms. Product reviews and ratings have become an integral part of the shopping experience to build trust among users. Due to the high volume of reviews generated on a vast catalog of products, manual moderation is infeasible, making machine moderation a necessity. In this work, we described our deployed system and models for automated moderation of user-generated content. At the heart of our approach, we outline several rejection reasons for review & rating moderation and explore a unified BERT model to moderate them. We convey the importance of product vertical embeddings for the relevancy of the review for a given product and highlight the advantages of pre-training the BERT models with monolingual data to cope with the domain gap in the absence of huge labelled datasets. We observe a 4.78% F1 increase with less labelled data and a 2.57% increase in F1 score on the review data compared to the publicly available BERT-based models. Our best model In-House-BERT-vertical sends only 5.89% of total reviews to manual moderation and has been deployed in production serving live traffic for millions of users. | Ravindra Nayak, Nikesh Garera |  |
| 646 |  |  [SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval](https://doi.org/10.18653/v1/2022.emnlp-industry.56) |  | 0 | Sampling proper negatives from a large document pool is vital to effectively train a dense retrieval model. However, existing negative sampling strategies suffer from the uninformative or false negative problem. In this work, we empirically show that according to the measured relevance scores, the negatives ranked around the positives are generally more informative and less likely to be false negatives. Intuitively, these negatives are not too hard (may be false negatives) or too easy (uninformative). They are the ambiguous negatives and need more attention during training.Thus, we propose a simple ambiguous negatives sampling method, SimANS, which incorporates a new sampling probability distribution to sample more ambiguous negatives.Extensive experiments on four public and one industry datasets show the effectiveness of our approach.We made the code and models publicly available in https://github.com/microsoft/SimXNS. | Kun Zhou, Yeyun Gong, Xiao Liu, Wayne Xin Zhao, Yelong Shen, Anlei Dong, Jingwen Lu, Rangan Majumder, JiRong Wen, Nan Duan |  |
| 647 |  |  [Revisiting and Advancing Chinese Natural Language Understanding with Accelerated Heterogeneous Knowledge Pre-training](https://doi.org/10.18653/v1/2022.emnlp-industry.57) |  | 0 | Recently, knowledge-enhanced pre-trained language models (KEPLMs) improve context-aware representations via learning from structured relations in knowledge bases, and/or linguistic knowledge from syntactic or dependency analysis. Unlike English, there is a lack of high-performing open-source Chinese KEPLMs in the natural language processing (NLP) community to support various language understanding applications. In this paper, we revisit and advance the development of Chinese natural language understanding with a series of novel Chinese KEPLMs released in various parameter sizes, namely CKBERT (Chinese knowledge-enhanced BERT). Specifically, both relational and linguistic knowledge is effectively injected into CKBERT based on two novel pre-training tasks, i.e., linguistic-aware masked language modeling and contrastive multi-hop relation modeling. Based on the above two pre-training paradigms and our in-house implemented TorchAccelerator, we have pre-trained base (110M), large (345M) and huge (1.3B) versions of CKBERT efficiently on GPU clusters. Experiments demonstrate that CKBERT consistently outperforms strong baselines for Chinese over various benchmark NLP tasks and in terms of different model sizes. | Taolin Zhang, Junwei Dong, Jianing Wang, Chengyu Wang, Ang Wang, Yinghui Liu, Jun Huang, Yong Li, Xiaofeng He |  |
| 648 |  |  [A Stacking-based Efficient Method for Toxic Language Detection on Live Streaming Chat](https://doi.org/10.18653/v1/2022.emnlp-industry.58) |  | 0 | In a live streaming chat on a video streaming service, it is crucial to filter out toxic comments with online processing to prevent users from reading comments in real-time. However, recent toxic language detection methods rely on deep learning methods, which can not be scalable considering inference speed. Also, these methods do not consider constraints of computational resources expected depending on a deployed system (e.g., no GPU resource).This paper presents an efficient method for toxic language detection that is aware of real-world scenarios. Our proposed architecture is based on partial stacking that feeds initial results with low confidence to meta-classifier. Experimental results show that our method achieves a much faster inference speed than BERT-based models with comparable performance. | Yuto Oikawa, Yuki Nakayama, Koji Murakami |  |
| 649 |  |  [End-to-End Speech to Intent Prediction to improve E-commerce Customer Support Voicebot in Hindi and English](https://doi.org/10.18653/v1/2022.emnlp-industry.59) |  | 0 | Automation of on-call customer support relies heavily on accurate and efficient speech-to-intent (S2I) systems. Building such systems using multi-component pipelines can pose various challenges because they require large annotated datasets, have higher latency, and have complex deployment. These pipelines are also prone to compounding errors. To overcome these challenges, we discuss an end-to-end (E2E) S2I model for customer support voicebot task in a bilingual setting. We show how we can solve E2E intent classification by leveraging a pre-trained automatic speech recognition (ASR) model with slight modification and fine-tuning on small annotated datasets. Experimental results show that our best E2E model outperforms a conventional pipeline by a relative ~27% on the F1 score. | Abhinav Goyal, Anupam Singh, Nikesh Garera |  |
| 650 |  |  [PILE: Pairwise Iterative Logits Ensemble for Multi-Teacher Labeled Distillation](https://doi.org/10.18653/v1/2022.emnlp-industry.60) |  | 0 | Pre-trained language models have become a crucial part of ranking systems and achieved very impressive effects recently. To maintain high performance while keeping efficient computations, knowledge distillation is widely used. In this paper, we focus on two key questions in knowledge distillation for ranking models: 1) how to ensemble knowledge from multi-teacher; 2) how to utilize the label information of data in the distillation process. We propose a unified algorithm called Pairwise Iterative Logits Ensemble (PILE) to tackle these two questions simultaneously. PILE ensembles multi-teacher logits supervised by label information in an iterative way and achieved competitive performance in both offline and online experiments. The proposed method has been deployed in a real-world commercial search system. | Lianshang Cai, Linhao Zhang, Dehong Ma, Jun Fan, Daiting Shi, Yi Wu, Zhicong Cheng, Simiu Gu, Dawei Yin |  |
| 651 |  |  [A Comprehensive Evaluation of Biomedical Entity-centric Search](https://doi.org/10.18653/v1/2022.emnlp-industry.61) |  | 0 | Biomedical information retrieval has often been studied as a task of detecting whether a system correctly detects entity spans and links these entities to concepts from a given terminology. Most academic research has focused on evaluation of named entity recognition (NER) and entity linking (EL) models which are key components to recognizing diseases and genes in PubMed abstracts. In this work, we perform a fine-grained evaluation intended to understand the efficiency of state-of-the-art BERT-based information extraction (IE) architecture as a biomedical search engine. We present a novel manually annotated dataset of abstracts for disease and gene search. The dataset contains 23K query-abstract pairs, where 152 queries are selected from logs of our target discovery platform and PubMed abstracts annotated with relevance judgments. Specifically, the query list also includes a subset of concepts with at least one ambiguous concept name. As a baseline, we use off-she-shelf Elasticsearch with BM25. Our experiments on NER, EL, and retrieval in a zero-shot setup show the neural IE architecture shows superior performance for both disease and gene concept queries. | Elena Tutubalina, Zulfat Miftahutdinov, Vladimir Muravlev, Anastasia Shneyderman |  |
| 652 |  |  [Domain Adaptation of Machine Translation with Crowdworkers](https://doi.org/10.18653/v1/2022.emnlp-industry.62) |  | 0 | Although a machine translation model trained with a large in-domain parallel corpus achieves remarkable results, it still works poorly when no in-domain data are available. This situation restricts the applicability of machine translation when the target domain’s data are limited. However, there is great demand for high-quality domain-specific machine translation models for many domains. We propose a framework that efficiently and effectively collects parallel sentences in a target domain from the web with the help of crowdworkers.With the collected parallel data, we can quickly adapt a machine translation model to the target domain. Our experiments show that the proposed method can collect target-domain parallel data over a few days at a reasonable cost. We tested it with five domains, and the domain-adapted model improved the BLEU scores to +19.7 by an average of +7.8 points compared to a general-purpose translation model. | Makoto Morishita, Jun Suzuki, Masaaki Nagata |  |
| 653 |  |  [Biomedical NER for the Enterprise with Distillated BERN2 and the Kazu Framework](https://doi.org/10.18653/v1/2022.emnlp-industry.63) |  | 0 | In order to assist the drug discovery/development process, pharmaceutical companies often apply biomedical NER and linking techniques over internal and public corpora. Decades of study of the field of BioNLP has produced a plethora of algorithms, systems and datasets. However, our experience has been that no single open source system meets all the requirements of a modern pharmaceutical company. In this work, we describe these requirements according to our experience of the industry, and present Kazu, a highly extensible, scalable open source framework designed to support BioNLP for the pharmaceutical sector. Kazu is a built around a computationally efficient version of the BERN2 NER model (TinyBERN2), and subsequently wraps several other BioNLP technologies into one coherent system. | Wonjin Yoon, Richard Jackson, Elliot Ford, Vladimir Poroshin, Jaewoo Kang |  |
| 654 |  |  [Large-scale Machine Translation for Indian Languages in E-commerce under Low Resource Constraints](https://doi.org/10.18653/v1/2022.emnlp-industry.64) |  | 0 | The democratization of e-commerce platforms has moved an increasingly diversified Indian user base to shop online. We have deployed reliable and precise large-scale Machine Translation systems for several Indian regional languages in this work. Building such systems is a challenge because of the low-resource nature of the Indian languages. We develop a structured model development pipeline as a closed feedback loop with external manual feedback through an Active Learning component. We show strong synthetic parallel data generation capability and consistent improvements to the model over iterations. Starting with 1.2M parallel pairs for English-Hindi we have compiled a corpus with 400M+ synthetic high quality parallel pairs across different domains. Further, we need colloquial translations to preserve the intent and friendliness of English content in regional languages, and make it easier to understand for our users. We perform robust and effective domain adaptation steps to achieve colloquial such translations. Over iterations, we show 9.02 BLEU points improvement for English to Hindi translation model. Along with Hindi, we show that the overall approach and best practices extends well to other Indian languages, resulting in deployment of our models across 7 Indian Languages. | Amey Patil, Nikesh Garera |  |
| 655 |  |  [Topic Modeling by Clustering Language Model Embeddings: Human Validation on an Industry Dataset](https://doi.org/10.18653/v1/2022.emnlp-industry.65) |  | 0 | Topic models are powerful tools to get an overview of large collections of text data, a situation that is prevalent in industry applications. A rising trend within topic modeling is to directly cluster dimension-reduced embeddings created with pretrained language models. It is difficult to evaluate these models because there is no ground truth and automatic measurements may not mimic human judgment. To address this problem, we created a tool called STELLAR for interactive topic browsing which we used for human evaluation of topics created from a real-world dataset used in industry. Embeddings created with BERT were used together with UMAP and HDBSCAN to model the topics. The human evaluation found that our topic model creates coherent topics. The following discussion revolves around the requirements of industry and what research is needed for production-ready systems. | Anton Eklund, Mona Forsman |  |
| 656 |  |  [Generative Knowledge Graph Construction: A Review](https://doi.org/10.18653/v1/2022.emnlp-main.1) |  | 0 | Generative Knowledge Graph Construction (KGC) refers to those methods that leverage the sequence-to-sequence framework for building knowledge graphs, which is flexible and can be adapted to widespread tasks. In this study, we summarize the recent compelling progress in generative knowledge graph construction. We present the advantages and weaknesses of each paradigm in terms of different generation targets and provide theoretical insight and empirical analysis. Based on the review, we suggest promising research directions for the future. Our contributions are threefold: (1) We present a detailed, complete taxonomy for the generative KGC methods; (2) We provide a theoretical and empirical analysis of the generative KGC methods; (3) We propose several research directions that can be developed in the future. | Hongbin Ye, Ningyu Zhang, Hui Chen, Huajun Chen |  |
| 657 |  |  [CDConv: A Benchmark for Contradiction Detection in Chinese Conversations](https://doi.org/10.18653/v1/2022.emnlp-main.2) |  | 0 | Dialogue contradiction is a critical issue in open-domain dialogue systems. The contextualization nature of conversations makes dialogue contradiction detection rather challenging. In this work, we propose a benchmark for Contradiction Detection in Chinese Conversations, namely CDConv. It contains 12K multi-turn conversations annotated with three typical contradiction categories: Intra-sentence Contradiction, Role Confusion, and History Contradiction. To efficiently construct the CDConv conversations, we devise a series of methods for automatic conversation generation, which simulate common user behaviors that trigger chatbots to make contradictions. We conduct careful manual quality screening of the constructed conversations and show that state-of-the-art Chinese chatbots can be easily goaded into making contradictions. Experiments on CDConv show that properly modeling contextual information is critical for dialogue contradiction detection, but there are still unresolved challenges that require future research. | Chujie Zheng, Jinfeng Zhou, Yinhe Zheng, Libiao Peng, Zhen Guo, Wenquan Wu, ZhengYu Niu, Hua Wu, Minlie Huang |  |
| 658 |  |  [Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space](https://doi.org/10.18653/v1/2022.emnlp-main.3) |  | 0 | Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50%, and for improving computation efficiency with a simple early exit rule, saving 20% of computation on average. | Mor Geva, Avi Caciularu, Kevin Ro Wang, Yoav Goldberg |  |
| 659 |  |  [Learning to Generate Question by Asking Question: A Primal-Dual Approach with Uncommon Word Generation](https://doi.org/10.18653/v1/2022.emnlp-main.4) |  | 0 | Automatic question generation (AQG) is the task of generating a question from a given passage and an answer. Most existing AQG methods aim at encoding the passage and the answer to generate the question. However, limited work has focused on modeling the correlation between the target answer and the generated question. Moreover, unseen or rare word generation has not been studied in previous works. In this paper, we propose a novel approach which incorporates question generation with its dual problem, question answering, into a unified primal-dual framework. Specifically, the question generation component consists of an encoder that jointly encodes the answer with the passage, and a decoder that produces the question. The question answering component then re-asks the generated question on the passage to ensure that the target answer is obtained. We further introduce a knowledge distillation module to improve the model generalization ability. We conduct an extensive set of experiments on SQuAD and HotpotQA benchmarks. Experimental results demonstrate the superior performance of the proposed approach over several state-of-the-art methods. | Qifan Wang, Li Yang, Xiaojun Quan, Fuli Feng, Dongfang Liu, Zenglin Xu, Sinong Wang, Hao Ma |  |
| 660 |  |  [Graph-based Model Generation for Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.5) |  | 0 | Few-shot relation extraction (FSRE) has been a challenging problem since it only has a handful of training instances. Existing models follow a ‘one-for-all’ scheme where one general large model performs all individual N-way-K-shot tasks in FSRE, which prevents the model from achieving the optimal point on each task. In view of this, we propose a model generation framework that consists of one general model for all tasks and many tiny task-specific models for each individual task. The general model generates and passes the universal knowledge to the tiny models which will be further fine-tuned when performing specific tasks. In this way, we decouple the complexity of the entire task space from that of all individual tasks while absorbing the universal knowledge.Extensive experimental results on two public datasets demonstrate that our framework reaches a new state-of-the-art performance for FRSE tasks. Our code is available at: https://github.com/NLPWM-WHU/GM_GEN. | Wanli Li, Tieyun Qian |  |
| 661 |  |  [Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling](https://doi.org/10.18653/v1/2022.emnlp-main.6) |  | 0 | Recent advances in federated learning have demonstrated its promising capability to learn on decentralized datasets. However, a considerable amount of work has raised concerns due to the potential risks of adversaries participating in the framework to poison the global model for an adversarial purpose. This paper investigates the feasibility of model poisoning for backdoor attacks through rare word embeddings of NLP models. In text classification, less than 1% of adversary clients suffices to manipulate the model output without any drop in the performance of clean sentences. For a less complex dataset, a mere 0.1% of adversary clients is enough to poison the global model effectively. We also propose a technique specialized in the federated learning scheme called gradient ensemble, which enhances the backdoor performance in all experimental settings. | KiYoon Yoo, Nojun Kwak |  |
| 662 |  |  [Generating Natural Language Proofs with Verifier-Guided Search](https://doi.org/10.18653/v1/2022.emnlp-main.7) |  | 0 | Reasoning over natural language is a challenging problem in NLP. In this work, we focus on proof generation: Given a hypothesis and a set of supporting facts, the model generates a proof tree indicating how to derive the hypothesis from supporting facts. Compared to generating the entire proof in one shot, stepwise generation can better exploit the compositionality and generalize to longer proofs but has achieved limited success on real-world data. Existing stepwise methods struggle to generate proof steps that are both logically valid and relevant to the hypothesis. Instead, they tend to hallucinate invalid steps given the hypothesis. In this paper, we present a novel stepwise method, NLProofS (Natural Language Proof Search), which learns to generate relevant steps conditioning on the hypothesis. At the core of our approach, we train an independent verifier to check the validity of the proof steps to prevent hallucination. Instead of generating steps greedily, we search for proofs maximizing a global proof score judged by the verifier. NLProofS achieves state-of-the-art performance on EntailmentBank and RuleTaker. Specifically, it improves the correctness of predicted proofs from 27.7% to 33.3% in the distractor setting of EntailmentBank, demonstrating the effectiveness of NLProofS in generating challenging human-authored proofs. | Kaiyu Yang, Jia Deng, Danqi Chen |  |
| 663 |  |  [Toward Unifying Text Segmentation and Long Document Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.8) |  | 0 | Text segmentation is important for signaling a document’s structure. Without segmenting a long document into topically coherent sections, it is difficult for readers to comprehend the text, let alone find important information. The problem is only exacerbated by a lack of segmentation in transcripts of audio/video recordings. In this paper, we explore the role that section segmentation plays in extractive summarization of written and spoken documents. Our approach learns robust sentence representations by performing summarization and segmentation simultaneously, which is further enhanced by an optimization-based regularizer to promote selection of diverse summary sentences. We conduct experiments on multiple datasets ranging from scientific articles to spoken transcripts to evaluate the model’s performance. Our findings suggest that the model can not only achieve state-of-the-art performance on publicly available benchmarks, but demonstrate better cross-genre transferability when equipped with text segmentation. We perform a series of analyses to quantify the impact of section segmentation on summarizing written and spoken documents of substantial length and complexity. | Sangwoo Cho, Kaiqiang Song, Xiaoyang Wang, Fei Liu, Dong Yu |  |
| 664 |  |  [The Geometry of Multilingual Language Model Representations](https://doi.org/10.18653/v1/2022.emnlp-main.9) |  | 0 | We assess how multilingual language models maintain a shared multilingual representation space while still encoding language-sensitive information in each language. Using XLM-R as a case study, we show that languages occupy similar linear subspaces after mean-centering, evaluated based on causal effects on language modeling performance and direct comparisons between subspaces for 88 languages. The subspace means differ along language-sensitive axes that are relatively stable throughout middle layers, and these axes encode information such as token vocabularies. Shifting representations by language means is sufficient to induce token predictions in different languages. However, we also identify stable language-neutral axes that encode information such as token positions and part-of-speech. We visualize representations projected onto language-sensitive and language-neutral axes, identifying language family and part-of-speech clusters, along with spirals, toruses, and curves representing token position information. These results demonstrate that multilingual language models encode information along orthogonal language-sensitive and language-neutral axes, allowing the models to extract a variety of features for downstream tasks and cross-lingual transfer learning. | Tyler A. Chang, Zhuowen Tu, Benjamin K. Bergen |  |
| 665 |  |  [Improving Complex Knowledge Base Question Answering via Question-to-Action and Question-to-Question Alignment](https://doi.org/10.18653/v1/2022.emnlp-main.10) |  | 0 | Complex knowledge base question answering can be achieved by converting questions into sequences of predefined actions. However, there is a significant semantic and structural gap between natural language and action sequences, which makes this conversion difficult. In this paper, we introduce an alignment-enhanced complex question answering framework, called ALCQA, which mitigates this gap through question-to-action alignment and question-to-question alignment. We train a question rewriting model to align the question and each action, and utilize a pretrained language model to implicitly align the question and KG artifacts. Moreover, considering that similar questions correspond to similar action sequences, we retrieve top-k similar question-answer pairs at the inference stage through question-to-question alignment and propose a novel reward-guided action sequence selection strategy to select from candidate action sequences. We conduct experiments on CQA and WQSP datasets, and the results show that our approach outperforms state-of-the-art methods and obtains a 9.88% improvements in the F1 metric on CQA dataset. Our source code is available at https://github.com/TTTTTTTTy/ALCQA. | Yechun Tang, Xiaoxia Cheng, Weiming Lu |  |
| 666 |  |  [PAIR: Prompt-Aware margIn Ranking for Counselor Reflection Scoring in Motivational Interviewing](https://doi.org/10.18653/v1/2022.emnlp-main.11) |  | 0 | Counselor reflection is a core verbal skill used by mental health counselors to express understanding and affirmation of the client’s experience and concerns. In this paper, we propose a system for the analysis of counselor reflections. Specifically, our system takes as input one dialog turn containing a client prompt and a counselor response, and outputs a score indicating the level of reflection in the counselor response. We compile a dataset consisting of different levels of reflective listening skills, and propose the Prompt-Aware margIn Ranking (PAIR) framework that contrasts positive and negative prompt and response pairs using specially designed multi-gap and prompt-aware margin ranking losses. Through empirical evaluations and deployment of our system in a real-life educational environment, we show that our analysis model outperforms several baselines on different metrics, and can be used to provide useful feedback to counseling trainees. | Do June Min, Verónica PérezRosas, Kenneth Resnicow, Rada Mihalcea |  |
| 667 |  |  [Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.12) |  | 0 | Recent graph-based models for joint multiple intent detection and slot filling have obtained promising results through modeling the guidance from the prediction of intents to the decoding of slot filling.However, existing methods (1) only model the unidirectional guidance from intent to slot; (2) adopt homogeneous graphs to model the interactions between the slot semantics nodes and intent label nodes, which limit the performance.In this paper, we propose a novel model termed Co-guiding Net, which implements a two-stage framework achieving the mutual guidances between the two tasks.In the first stage, the initial estimated labels of both tasks are produced, and then they are leveraged in the second stage to model the mutual guidances.Specifically, we propose two heterogeneous graph attention networks working on the proposed two heterogeneous semantics-label graphs, which effectively represent the relations among the semantics nodes and label nodes.Experiment results show that our model outperforms existing models by a large margin, obtaining a relative improvement of 19.3% over the previous best model on MixATIS dataset in overall accuracy. | Bowen Xing, Ivor W. Tsang |  |
| 668 |  |  [The Importance of Being Parameters: An Intra-Distillation Method for Serious Gains](https://doi.org/10.18653/v1/2022.emnlp-main.13) |  | 0 |  | Haoran Xu, Philipp Koehn, Kenton Murray |  |
| 669 |  |  [Interpreting Language Models with Contrastive Explanations](https://doi.org/10.18653/v1/2022.emnlp-main.14) |  | 0 | Model interpretability methods are often used to explain NLP model decisions on tasks such as text classification, where the output space is relatively small. However, when applied to language generation, where the output space often consists of tens of thousands of tokens, these methods are unable to provide informative explanations. Language models must consider various features to predict a token, such as its part of speech, number, tense, or semantics.Existing explanation methods conflate evidence for all these features into a single explanation, which is less interpretable for human understanding.To disentangle the different decisions in language modeling, we focus on explaining language models contrastively: we look for salient input tokens that explain why the model predicted one token instead of another. We demonstrate that contrastive explanations are quantifiably better than non-contrastive explanations in verifying major grammatical phenomena, and that they significantly improve contrastive model simulatability for human observers. We also identify groups of contrastive decisions where the model uses similar evidence, and we are able to characterize what input tokens models use during various language generation decisions. | Kayo Yin, Graham Neubig |  |
| 670 |  |  [RankGen: Improving Text Generation with Large Ranking Models](https://doi.org/10.18653/v1/2022.emnlp-main.15) |  | 0 | Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues we present RankGen, a 1.2B parameter encoder model for English that scores model generations given a prefix. RankGen can be flexibly incorporated as a scoring function in beam search and used to decode from any pretrained language model. We train RankGen using large-scale contrastive learning to map a prefix close to the ground-truth sequence that follows it and far away from two types of negatives: (1) random sequences from the same document as the prefix, and (2) sequences generated from a large language model conditioned on the prefix. Experiments across four different language models (345M-11B parameters) and two domains show that RankGen significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling on both automatic metrics (85.0 vs 77.3 MAUVE) as well as human evaluations with English writers (74.5% human preference over nucleus sampling). Analysis reveals that RankGen outputs are more relevant to the prefix and improve continuity and coherence compared to baselines. We release our model checkpoints, code, and human preference data with explanations to facilitate future research. | Kalpesh Krishna, Yapei Chang, John Wieting, Mohit Iyyer |  |
| 671 |  |  [Learning a Grammar Inducer from Massive Uncurated Instructional Videos](https://doi.org/10.18653/v1/2022.emnlp-main.16) |  | 0 |  | Songyang Zhang, Linfeng Song, Lifeng Jin, Haitao Mi, Kun Xu, Dong Yu, Jiebo Luo |  |
| 672 |  |  [Normalized Contrastive Learning for Text-Video Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.17) |  | 0 |  | Yookoon Park, Mahmoud Azab, Seungwhan Moon, Bo Xiong, Florian Metze, Gourab Kundu, Kirmani Ahmed |  |
| 673 |  |  [Estimating Soft Labels for Out-of-Domain Intent Detection](https://doi.org/10.18653/v1/2022.emnlp-main.18) |  | 0 |  | Hao Lang, Yinhe Zheng, Jian Sun, Fei Huang, Luo Si, Yongbin Li |  |
| 674 |  |  [Multi-VQG: Generating Engaging Questions for Multiple Images](https://doi.org/10.18653/v1/2022.emnlp-main.19) |  | 0 |  | MinHsuan Yeh, Vincent Chen, TingHao (Kenneth) Huang, LunWei Ku |  |
| 675 |  |  [Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.20) |  | 0 |  | Jannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Börschinger, Tal Schuster |  |
| 676 |  |  [Non-Parametric Domain Adaptation for End-to-End Speech Translation](https://doi.org/10.18653/v1/2022.emnlp-main.21) |  | 0 |  | Yichao Du, Weizhi Wang, Zhirui Zhang, Boxing Chen, Tong Xu, Jun Xie, Enhong Chen |  |
| 677 |  |  [Prompting for Multimodal Hateful Meme Classification](https://doi.org/10.18653/v1/2022.emnlp-main.22) |  | 0 |  | Rui Cao, Roy KaWei Lee, WenHaw Chong, Jing Jiang |  |
| 678 |  |  [Certified Error Control of Candidate Set Pruning for Two-Stage Relevance Ranking](https://doi.org/10.18653/v1/2022.emnlp-main.23) |  | 0 |  | Minghan Li, Xinyu Zhang, Ji Xin, Hongyang Zhang, Jimmy Lin |  |
| 679 |  |  [Linearizing Transformer with Key-Value Memory](https://doi.org/10.18653/v1/2022.emnlp-main.24) |  | 0 |  | Yizhe Zhang, Deng Cai |  |
| 680 |  |  [Robustness of Fusion-based Multimodal Classifiers to Cross-Modal Content Dilutions](https://doi.org/10.18653/v1/2022.emnlp-main.25) |  | 0 |  | Gaurav Verma, Vishwa Vinay, Ryan A. Rossi, Srijan Kumar |  |
| 681 |  |  [Translation between Molecules and Natural Language](https://doi.org/10.18653/v1/2022.emnlp-main.26) |  | 0 |  | Carl Edwards, Tuan Manh Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, Heng Ji |  |
| 682 |  |  [What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment](https://doi.org/10.18653/v1/2022.emnlp-main.27) |  | 0 |  | Matthew Finlayson, Kyle Richardson, Ashish Sabharwal, Peter Clark |  |
| 683 |  |  [Sentence-Incremental Neural Coreference Resolution](https://doi.org/10.18653/v1/2022.emnlp-main.28) |  | 0 |  | Matt Grenander, Shay B. Cohen, Mark Steedman |  |
| 684 |  |  [SNaC: Coherence Error Detection for Narrative Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.29) |  | 0 |  | Tanya Goyal, Junyi Jessy Li, Greg Durrett |  |
| 685 |  |  [HydraSum: Disentangling Style Features in Text Summarization with Multi-Decoder Models](https://doi.org/10.18653/v1/2022.emnlp-main.30) |  | 0 |  | Tanya Goyal, Nazneen Rajani, Wenhao Liu, Wojciech Kryscinski |  |
| 686 |  |  [A Good Neighbor, A Found Treasure: Mining Treasured Neighbors for Knowledge Graph Entity Typing](https://doi.org/10.18653/v1/2022.emnlp-main.31) |  | 0 |  | Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao |  |
| 687 |  |  [Guiding Neural Entity Alignment with Compatibility](https://doi.org/10.18653/v1/2022.emnlp-main.32) |  | 0 |  | Bing Liu, Harrisen Scells, Wen Hua, Guido Zuccon, Genghong Zhao, Xia Zhang |  |
| 688 |  |  [InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning](https://doi.org/10.18653/v1/2022.emnlp-main.33) |  | 0 |  | Prakhar Gupta, Cathy Jiao, YiTing Yeh, Shikib Mehri, Maxine Eskénazi, Jeffrey P. Bigham |  |
| 689 |  |  [Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling](https://doi.org/10.18653/v1/2022.emnlp-main.34) |  | 0 |  | Peijie Jiang, Dingkun Long, Yanzhao Zhang, Pengjun Xie, Meishan Zhang, Min Zhang |  |
| 690 |  |  [RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder](https://doi.org/10.18653/v1/2022.emnlp-main.35) |  | 0 |  | Shitao Xiao, Zheng Liu, Yingxia Shao, Zhao Cao |  |
| 691 |  |  [Aligning Recommendation and Conversation via Dual Imitation](https://doi.org/10.18653/v1/2022.emnlp-main.36) |  | 0 |  | Jinfeng Zhou, Bo Wang, Minlie Huang, Dongming Zhao, Kun Huang, Ruifang He, Yuexian Hou |  |
| 692 |  |  [QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance](https://doi.org/10.18653/v1/2022.emnlp-main.37) |  | 0 |  | Xiaoqiang Wang, Bang Liu, Siliang Tang, Lingfei Wu |  |
| 693 |  |  [Abstract Visual Reasoning with Tangram Shapes](https://doi.org/10.18653/v1/2022.emnlp-main.38) |  | 0 |  | Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert D. Hawkins, Yoav Artzi |  |
| 694 |  |  [UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.39) |  | 0 |  | Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, ChienSheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, Tao Yu |  |
| 695 |  |  [Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models](https://doi.org/10.18653/v1/2022.emnlp-main.40) |  | 0 |  | Hannah Chen, Yangfeng Ji, David E. Evans |  |
| 696 |  |  [When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks](https://doi.org/10.18653/v1/2022.emnlp-main.41) |  | 0 |  | Ankur Sikarwar, Arkil Patel, Navin Goyal |  |
| 697 |  |  [Generative Language Models for Paragraph-Level Question Generation](https://doi.org/10.18653/v1/2022.emnlp-main.42) |  | 0 |  | Asahi Ushio, Fernando AlvaManchego, José CamachoCollados |  |
| 698 |  |  [A Unified Encoder-Decoder Framework with Entity Memory](https://doi.org/10.18653/v1/2022.emnlp-main.43) |  | 0 |  | Zhihan Zhang, Wenhao Yu, Chenguang Zhu, Meng Jiang |  |
| 699 |  |  [Segmenting Numerical Substitution Ciphers](https://doi.org/10.18653/v1/2022.emnlp-main.44) |  | 0 |  | Nada Aldarrab, Jonathan May |  |
| 700 |  |  [Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.45) |  | 0 |  | Ashish V. Thapliyal, Jordi PontTuset, Xi Chen, Radu Soricut |  |
| 701 |  |  [ReSel: N-ary Relation Extraction from Scientific Text and Tables by Learning to Retrieve and Select](https://doi.org/10.18653/v1/2022.emnlp-main.46) |  | 0 |  | Yuchen Zhuang, Yinghao Li, Junyang Zhang, Yue Yu, Yingjun Mou, Xiang Chen, Le Song, Chao Zhang |  |
| 702 |  |  [GammaE: Gamma Embeddings for Logical Queries on Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.47) |  | 0 |  | Dong Yang, Peijun Qing, Yang Li, Haonan Lu, Xiaodong Lin |  |
| 703 |  |  [Reasoning Like Program Executors](https://doi.org/10.18653/v1/2022.emnlp-main.48) |  | 0 |  | Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Qiang Fu, Yan Gao, JianGuang Lou, Weizhu Chen |  |
| 704 |  |  [SEM-F1: an Automatic Way for Semantic Evaluation of Multi-Narrative Overlap Summaries at Scale](https://doi.org/10.18653/v1/2022.emnlp-main.49) |  | 0 |  | Naman Bansal, Mousumi Akter, Shubhra Kanti Karmaker Santu |  |
| 705 |  |  [Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning](https://doi.org/10.18653/v1/2022.emnlp-main.50) |  | 0 |  | Yifan Chen, Devamanyu Hazarika, Mahdi Namazifar, Yang Liu, Di Jin, Dilek HakkaniTur |  |
| 706 |  |  [DocInfer: Document-level Natural Language Inference using Optimal Evidence Selection](https://doi.org/10.18653/v1/2022.emnlp-main.51) |  | 0 |  | Puneet Mathur, Gautam Kunapuli, Riyaz A. Bhat, Manish Shrivastava, Dinesh Manocha, Maneesh Singh |  |
| 707 |  |  [LightEA: A Scalable, Robust, and Interpretable Entity Alignment Framework via Three-view Label Propagation](https://doi.org/10.18653/v1/2022.emnlp-main.52) |  | 0 |  | Xin Mao, Wenting Wang, Yuanbin Wu, Man Lan |  |
| 708 |  |  [Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.53) |  | 0 |  | Xingwei He, Yeyun Gong, ALong Jin, Weizhen Qi, Hang Zhang, Jian Jiao, Bartuer Zhou, Biao Cheng, SiuMing Yiu, Nan Duan |  |
| 709 |  |  [Efficient Document Retrieval by End-to-End Refining and Quantizing BERT Embedding with Contrastive Product Quantization](https://doi.org/10.18653/v1/2022.emnlp-main.54) |  | 0 |  | Zexuan Qiu, Qinliang Su, Jianxing Yu, Shijing Si |  |
| 710 |  |  [Curriculum Knowledge Distillation for Emoji-supervised Cross-lingual Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.55) |  | 0 |  | Jianyang Zhang, Tao Liang, Mingyang Wan, Guowu Yang, Fengmao Lv |  |
| 711 |  |  [Correctable-DST: Mitigating Historical Context Mismatch between Training and Inference for Improved Dialogue State Tracking](https://doi.org/10.18653/v1/2022.emnlp-main.56) |  | 0 |  | Hongyan Xie, Haoxiang Su, Shuangyong Song, Hao Huang, Bo Zou, Kun Deng, Jianghua Lin, Zhihui Zhang, Xiaodong He |  |
| 712 |  |  [DropMix: A Textual Data Augmentation Combining Dropout with Mixup](https://doi.org/10.18653/v1/2022.emnlp-main.57) |  | 0 |  | Fanshuang Kong, Richong Zhang, Xiaohui Guo, Samuel Mensah, Yongyi Mao |  |
| 713 |  |  [Cross-document Event Coreference Search: Task, Dataset and Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.58) |  | 0 |  | Alon Eirew, Avi Caciularu, Ido Dagan |  |
| 714 |  |  [VIRT: Improving Representation-based Text Matching via Virtual Interaction](https://doi.org/10.18653/v1/2022.emnlp-main.59) |  | 0 |  | Dan Li, Yang Yang, Hongyin Tang, Jiahao Liu, Qifan Wang, Jingang Wang, Tong Xu, Wei Wu, Enhong Chen |  |
| 715 |  |  [MAVEN-ERE: A Unified Large-scale Dataset for Event Coreference, Temporal, Causal, and Subevent Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.60) |  | 0 |  | Xiaozhi Wang, Yulin Chen, Ning Ding, Hao Peng, Zimu Wang, Yankai Lin, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu, Peng Li, Jie Zhou |  |
| 716 |  |  [Entity Extraction in Low Resource Domains with Selective Pre-training of Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.61) |  | 0 |  | Aniruddha Mahapatra, Sharmila Reddy Nangi, Aparna Garimella, Anandhavelu Natarajan |  |
| 717 |  |  [How Large Language Models are Transforming Machine-Paraphrase Plagiarism](https://doi.org/10.18653/v1/2022.emnlp-main.62) |  | 0 |  | Jan Philip Wahle, Terry Ruas, Frederic Kirstein, Bela Gipp |  |
| 718 |  |  [M2D2: A Massively Multi-Domain Language Modeling Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.63) |  | 0 |  | Machel Reid, Victor Zhong, Suchin Gururangan, Luke Zettlemoyer |  |
| 719 |  |  ["Will You Find These Shortcuts?" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.64) |  | 0 |  | Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, Katja Filippova |  |
| 720 |  |  [Information-Transport-based Policy for Simultaneous Translation](https://doi.org/10.18653/v1/2022.emnlp-main.65) |  | 0 |  | Shaolei Zhang, Yang Feng |  |
| 721 |  |  [Learning to Adapt to Low-Resource Paraphrase Generation](https://doi.org/10.18653/v1/2022.emnlp-main.66) |  | 0 |  | Zhigen Li, Yanmeng Wang, Rizhao Fan, Ye Wang, Jianfeng Li, Shaojun Wang |  |
| 722 |  |  [A Distributional Lens for Multi-Aspect Controllable Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.67) |  | 0 |  | Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Lingyuan Zhang, Heng Gong, Bing Qin |  |
| 723 |  |  [ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.68) |  | 0 |  | Junyi Li, Tianyi Tang, Wayne Xin Zhao, JianYun Nie, JiRong Wen |  |
| 724 |  |  [Multilingual Relation Classification via Efficient and Effective Prompting](https://doi.org/10.18653/v1/2022.emnlp-main.69) |  | 0 |  | Yuxuan Chen, David Harbecke, Leonhard Hennig |  |
| 725 |  |  [Topic-Regularized Authorship Representation Learning](https://doi.org/10.18653/v1/2022.emnlp-main.70) |  | 0 |  | Jitkapat Sawatphol, Nonthakit Chaiwong, Can Udomcharoenchaikit, Sarana Nutanong |  |
| 726 |  |  [Fine-grained Contrastive Learning for Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.71) |  | 0 |  | William Hogan, Jiacheng Li, Jingbo Shang |  |
| 727 |  |  [Curriculum Prompt Learning with Self-Training for Abstractive Dialogue Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.72) |  | 0 |  | Changqun Li, Linlin Wang, Xin Lin, Gerard de Melo, Liang He |  |
| 728 |  |  [Zero-Shot Text Classification with Self-Training](https://doi.org/10.18653/v1/2022.emnlp-main.73) |  | 0 |  | Ariel Gera, Alon Halfon, Eyal Shnarch, Yotam Perlitz, Liat EinDor, Noam Slonim |  |
| 729 |  |  [Deconfounding Legal Judgment Prediction for European Court of Human Rights Cases Towards Better Alignment with Experts](https://doi.org/10.18653/v1/2022.emnlp-main.74) |  | 0 |  | Tokala Yaswanth Sri Sai Santosh, Shanshan Xu, Oana Ichim, Matthias Grabmair |  |
| 730 |  |  [SQuALITY: Building a Long-Document Summarization Dataset the Hard Way](https://doi.org/10.18653/v1/2022.emnlp-main.75) |  | 0 |  | Alex Wang, Richard Yuanzhe Pang, Angelica Chen, Jason Phang, Samuel R. Bowman |  |
| 731 |  |  [MetaASSIST: Robust Dialogue State Tracking with Meta Learning](https://doi.org/10.18653/v1/2022.emnlp-main.76) |  | 0 |  | Fanghua Ye, Xi Wang, Jie Huang, Shenghui Li, Samuel Stern, Emine Yilmaz |  |
| 732 |  |  [Multilingual Machine Translation with Hyper-Adapters](https://doi.org/10.18653/v1/2022.emnlp-main.77) |  | 0 |  | Christos Baziotis, Mikel Artetxe, James Cross, Shruti Bhosale |  |
| 733 |  |  [Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination](https://doi.org/10.18653/v1/2022.emnlp-main.78) |  | 0 |  | Yue Yang, Wenlin Yao, Hongming Zhang, Xiaoyang Wang, Dong Yu, Jianshu Chen |  |
| 734 |  |  [Using Commonsense Knowledge to Answer Why-Questions](https://doi.org/10.18653/v1/2022.emnlp-main.79) |  | 0 |  | Yash Kumar Lal, Niket Tandon, Tanvi Aggarwal, Horace Liu, Nathanael Chambers, Raymond J. Mooney, Niranjan Balasubramanian |  |
| 735 |  |  [Affective Idiosyncratic Responses to Music](https://doi.org/10.18653/v1/2022.emnlp-main.80) |  | 0 |  | Sky CHWang, Evan Li, Oliver Li, Smaranda Muresan, Zhou Yu |  |
| 736 |  |  [Successive Prompting for Decomposing Complex Questions](https://doi.org/10.18653/v1/2022.emnlp-main.81) |  | 0 |  | Dheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner |  |
| 737 |  |  [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://doi.org/10.18653/v1/2022.emnlp-main.82) |  | 0 |  | Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi |  |
| 738 |  |  [DANLI: Deliberative Agent for Following Natural Language Instructions](https://doi.org/10.18653/v1/2022.emnlp-main.83) |  | 0 |  | Yichi Zhang, Jianing Yang, Jiayi Pan, Shane Storks, Nikhil Devraj, Ziqiao Ma, Keunwoo Peter Yu, Yuwei Bao, Joyce Chai |  |
| 739 |  |  [Tracing Semantic Variation in Slang](https://doi.org/10.18653/v1/2022.emnlp-main.84) |  | 0 |  | Zhewei Sun, Yang Xu |  |
| 740 |  |  [Fine-grained Category Discovery under Coarse-grained supervision with Hierarchical Weighted Self-contrastive Learning](https://doi.org/10.18653/v1/2022.emnlp-main.85) |  | 0 |  | Wenbin An, Feng Tian, Ping Chen, Siliang Tang, Qinghua Zheng, Qianying Wang |  |
| 741 |  |  [PLM-based World Models for Text-based Games](https://doi.org/10.18653/v1/2022.emnlp-main.86) |  | 0 |  | Minsoo Kim, YeonJoon Jung, Dohyeon Lee, Seungwon Hwang |  |
| 742 |  |  [Prompt-Based Meta-Learning For Few-shot Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.87) |  | 0 |  | Haoxing Zhang, Xiaofeng Zhang, Haibo Huang, Lei Yu |  |
| 743 |  |  [How well can Text-to-Image Generative Models understand Ethical Natural Language Interventions?](https://doi.org/10.18653/v1/2022.emnlp-main.88) |  | 0 |  | Hritik Bansal, Da Yin, Masoud Monajatipoor, KaiWei Chang |  |
| 744 |  |  [Geographic Citation Gaps in NLP Research](https://doi.org/10.18653/v1/2022.emnlp-main.89) |  | 0 |  | Mukund Rungta, Janvijay Singh, Saif M. Mohammad, Diyi Yang |  |
| 745 |  |  [Language Models of Code are Few-Shot Commonsense Learners](https://doi.org/10.18653/v1/2022.emnlp-main.90) |  | 0 |  | Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig |  |
| 746 |  |  [Numerical Optimizations for Weighted Low-rank Estimation on Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.91) |  | 0 |  | Ting Hua, YenChang Hsu, Felicity Wang, Qian Lou, Yilin Shen, Hongxia Jin |  |
| 747 |  |  [Generative Multi-hop Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.92) |  | 0 |  | Hyunji Lee, Sohee Yang, Hanseok Oh, Minjoon Seo |  |
| 748 |  |  [Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.93) |  | 0 |  | Yu Zhao, Jianguo Wei, Zhichao Lin, Yueheng Sun, Meishan Zhang, Min Zhang |  |
| 749 |  |  [M3: A Multi-View Fusion and Multi-Decoding Network for Multi-Document Reading Comprehension](https://doi.org/10.18653/v1/2022.emnlp-main.94) |  | 0 |  | Liang Wen, Houfeng Wang, Yingwei Luo, Xiaolin Wang |  |
| 750 |  |  [COCO-DR: Combating the Distribution Shift in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning](https://doi.org/10.18653/v1/2022.emnlp-main.95) |  | 0 |  | Yue Yu, Chenyan Xiong, Si Sun, Chao Zhang, Arnold Overwijk |  |
| 751 |  |  [Language Model Pre-Training with Sparse Latent Typing](https://doi.org/10.18653/v1/2022.emnlp-main.96) |  | 0 |  | Liliang Ren, Zixuan Zhang, Han Wang, Clare R. Voss, ChengXiang Zhai, Heng Ji |  |
| 752 |  |  [On the Transformation of Latent Space in Fine-Tuned NLP Models](https://doi.org/10.18653/v1/2022.emnlp-main.97) |  | 0 |  | Nadir Durrani, Hassan Sajjad, Fahim Dalvi, Firoj Alam |  |
| 753 |  |  [Watch the Neighbors: A Unified K-Nearest Neighbor Contrastive Learning Framework for OOD Intent Discovery](https://doi.org/10.18653/v1/2022.emnlp-main.98) |  | 0 |  | Yutao Mou, Keqing He, Pei Wang, Yanan Wu, Jingang Wang, Wei Wu, Weiran Xu |  |
| 754 |  |  [Extracted BERT Model Leaks More Information than You Think!](https://doi.org/10.18653/v1/2022.emnlp-main.99) |  | 0 |  | Xuanli He, Lingjuan Lyu, Chen Chen, Qiongkai Xu |  |
| 755 |  |  [Do Vision-and-Language Transformers Learn Grounded Predicate-Noun Dependencies?](https://doi.org/10.18653/v1/2022.emnlp-main.100) |  | 0 |  | Mitja Nikolaus, Emmanuelle Salin, Stéphane Ayache, Abdellah Fourtassi, Benoît Favre |  |
| 756 |  |  [A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference](https://doi.org/10.18653/v1/2022.emnlp-main.101) |  | 0 |  | Kerem Zaman, Yonatan Belinkov |  |
| 757 |  |  [Graph-Based Multilingual Label Propagation for Low-Resource Part-of-Speech Tagging](https://doi.org/10.18653/v1/2022.emnlp-main.102) |  | 0 |  | Ayyoob Imani, Silvia Severini, Masoud Jalili Sabet, François Yvon, Hinrich Schütze |  |
| 758 |  |  [SubeventWriter: Iterative Sub-event Sequence Generation with Coherence Controller](https://doi.org/10.18653/v1/2022.emnlp-main.103) |  | 0 |  | Zhaowei Wang, Hongming Zhang, Tianqing Fang, Yangqiu Song, Ginny Y. Wong, Simon See |  |
| 759 |  |  [Infinite SCAN: An Infinite Model of Diachronic Semantic Change](https://doi.org/10.18653/v1/2022.emnlp-main.104) |  | 0 |  | Seiichi Inoue, Mamoru Komachi, Toshinobu Ogiso, Hiroya Takamura, Daichi Mochihashi |  |
| 760 |  |  [Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization](https://doi.org/10.18653/v1/2022.emnlp-main.105) |  | 0 |  | Yuxian Gu, Pei Ke, Xiaoyan Zhu, Minlie Huang |  |
| 761 |  |  [Counterfactual Data Augmentation via Perspective Transition for Open-Domain Dialogues](https://doi.org/10.18653/v1/2022.emnlp-main.106) |  | 0 |  | Jiao Ou, Jinchao Zhang, Yang Feng, Jie Zhou |  |
| 762 |  |  [SQUIRE: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.107) |  | 0 |  | Yushi Bai, Xin Lv, Juanzi Li, Lei Hou, Yincen Qu, Zelin Dai, Feiyu Xiong |  |
| 763 |  |  [SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training](https://doi.org/10.18653/v1/2022.emnlp-main.108) |  | 0 |  | Ziqiang Zhang, Long Zhou, Junyi Ao, Shujie Liu, Lirong Dai, Jinyu Li, Furu Wei |  |
| 764 |  |  [Learning Label Modular Prompts for Text Classification in the Wild](https://doi.org/10.18653/v1/2022.emnlp-main.109) |  | 0 |  | Hailin Chen, Amrita Saha, Shafiq R. Joty, Steven C. H. Hoi |  |
| 765 |  |  [Unbiased and Efficient Sampling of Dependency Trees](https://doi.org/10.18653/v1/2022.emnlp-main.110) |  | 0 |  | Milos Stanojevic |  |
| 766 |  |  [Continual Learning of Neural Machine Translation within Low Forgetting Risk Regions](https://doi.org/10.18653/v1/2022.emnlp-main.111) |  | 0 |  | Shuhao Gu, Bojie Hu, Yang Feng |  |
| 767 |  |  [COST-EFF: Collaborative Optimization of Spatial and Temporal Efficiency with Slenderized Multi-exit Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.112) |  | 0 |  | Bowen Shen, Zheng Lin, Yuanxin Liu, Zhengxiao Liu, Lei Wang, Weiping Wang |  |
| 768 |  |  [Rescue Implicit and Long-tail Cases: Nearest Neighbor Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.113) |  | 0 |  | Zhen Wan, Qianying Liu, Zhuoyuan Mao, Fei Cheng, Sadao Kurohashi, Jiwei Li |  |
| 769 |  |  [StoryER: Automatic Story Evaluation via Ranking, Rating and Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.114) |  | 0 |  | Hong Chen, Duc Minh Vo, Hiroya Takamura, Yusuke Miyao, Hideki Nakayama |  |
| 770 |  |  [Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference](https://doi.org/10.18653/v1/2022.emnlp-main.115) |  | 0 |  | Eric Mitchell, Joseph J. Noh, Siyan Li, William S. Armstrong, Ananth Agarwal, Patrick Liu, Chelsea Finn, Christopher D. Manning |  |
| 771 |  |  [Robustness of Demonstration-based Learning Under Limited Data Scenario](https://doi.org/10.18653/v1/2022.emnlp-main.116) |  | 0 |  | Hongxin Zhang, Yanzhe Zhang, Ruiyi Zhang, Diyi Yang |  |
| 772 |  |  [Modeling Information Change in Science Communication with Semantically Matched Paraphrases](https://doi.org/10.18653/v1/2022.emnlp-main.117) |  | 0 |  | Dustin Wright, Jiaxin Pei, David Jurgens, Isabelle Augenstein |  |
| 773 |  |  [Word Order Matters When You Increase Masking](https://doi.org/10.18653/v1/2022.emnlp-main.118) |  | 0 |  | Karim Lasri, Alessandro Lenci, Thierry Poibeau |  |
| 774 |  |  [An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.119) |  | 0 |  | Fatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David E. Evans, Taylor BergKirkpatrick |  |
| 775 |  |  [Style Transfer as Data Augmentation: A Case Study on Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.120) |  | 0 |  | Shuguang Chen, Leonardo Neves, Thamar Solorio |  |
| 776 |  |  [Linguistic Corpus Annotation for Automatic Text Simplification Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.121) |  | 0 |  | Rémi Cardon, Adrien Bibal, Rodrigo Wilkens, David Alfter, Magali Norré, Adeline Müller, Patrick Watrin, Thomas François |  |
| 777 |  |  [Semantic Framework based Query Generation for Temporal Question Answering over Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.122) |  | 0 |  | Wentao Ding, Hao Chen, Huayu Li, Yuzhong Qu |  |
| 778 |  |  [There Is No Standard Answer: Knowledge-Grounded Dialogue Generation with Adversarial Activated Multi-Reference Learning](https://doi.org/10.18653/v1/2022.emnlp-main.123) |  | 0 |  | Xueliang Zhao, Tingchen Fu, Chongyang Tao, Rui Yan |  |
| 779 |  |  [Stop Measuring Calibration When Humans Disagree](https://doi.org/10.18653/v1/2022.emnlp-main.124) |  | 0 |  | Joris Baan, Wilker Aziz, Barbara Plank, Raquel Fernández |  |
| 780 |  |  [Improving compositional generalization for multi-step quantitative reasoning in question answering](https://doi.org/10.18653/v1/2022.emnlp-main.125) |  | 0 |  | Armineh Nourbakhsh, Cathy Jiao, Sameena Shah, Carolyn P. Rosé |  |
| 781 |  |  [A Comprehensive Comparison of Neural Networks as Cognitive Models of Inflection](https://doi.org/10.18653/v1/2022.emnlp-main.126) |  | 0 |  | Adam Wiemerslage, Shiran Dudy, Katharina Kann |  |
| 782 |  |  [Can Visual Context Improve Automatic Speech Recognition for an Embodied Agent?](https://doi.org/10.18653/v1/2022.emnlp-main.127) |  | 0 |  | Pradip Pramanick, Chayan Sarkar |  |
| 783 |  |  [AfroLID: A Neural Language Identification Tool for African Languages](https://doi.org/10.18653/v1/2022.emnlp-main.128) |  | 0 |  | Ife Adebara, AbdelRahim A. Elmadany, Muhammad AbdulMageed, Alcides Alcoba Inciarte |  |
| 784 |  |  [EvEntS ReaLM: Event Reasoning of Entity States via Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.129) |  | 0 |  | Evangelia Spiliopoulou, Artidoro Pagnoni, Yonatan Bisk, Eduard H. Hovy |  |
| 785 |  |  [Large language models are few-shot clinical information extractors](https://doi.org/10.18653/v1/2022.emnlp-main.130) |  | 0 |  | Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, David A. Sontag |  |
| 786 |  |  [Towards a Unified Multi-Dimensional Evaluator for Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.131) |  | 0 |  | Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, Jiawei Han |  |
| 787 |  |  [GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.132) |  | 0 |  | Da Yin, Hritik Bansal, Masoud Monajatipoor, Liunian Harold Li, KaiWei Chang |  |
| 788 |  |  [The (Undesired) Attenuation of Human Biases by Multilinguality](https://doi.org/10.18653/v1/2022.emnlp-main.133) |  | 0 |  | Cristina EspañaBonet, Alberto BarrónCedeño |  |
| 789 |  |  [Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.134) |  | 0 |  | Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark |  |
| 790 |  |  [Near-Negative Distinction: Giving a Second Life to Human Evaluation Datasets](https://doi.org/10.18653/v1/2022.emnlp-main.135) |  | 0 |  | Philippe Laban, ChienSheng Wu, Wenhao Liu, Caiming Xiong |  |
| 791 |  |  [ToKen: Task Decomposition and Knowledge Infusion for Few-Shot Hate Speech Detection](https://doi.org/10.18653/v1/2022.emnlp-main.136) |  | 0 |  | Badr AlKhamissi, Faisal Ladhak, Srini Iyer, Veselin Stoyanov, Zornitsa Kozareva, Xian Li, Pascale Fung, Lambert Mathias, Asli Celikyilmaz, Mona T. Diab |  |
| 792 |  |  [Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations](https://doi.org/10.18653/v1/2022.emnlp-main.137) |  | 0 |  | Swarnadeep Saha, Peter Hase, Nazneen Rajani, Mohit Bansal |  |
| 793 |  |  [Stanceosaurus: Classifying Stance Towards Multicultural Misinformation](https://doi.org/10.18653/v1/2022.emnlp-main.138) |  | 0 |  | Jonathan Zheng, Ashutosh Baheti, Tarek Naous, Wei Xu, Alan Ritter |  |
| 794 |  |  [Gendered Mental Health Stigma in Masked Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.139) |  | 0 |  | Inna W. Lin, Lucille Njoo, Anjalie Field, Ashish Sharma, Katharina Reinecke, Tim Althoff, Yulia Tsvetkov |  |
| 795 |  |  [Efficient Nearest Neighbor Search for Cross-Encoder Models using Matrix Factorization](https://doi.org/10.18653/v1/2022.emnlp-main.140) |  | 0 |  | Nishant Yadav, Nicholas Monath, Rico Angell, Manzil Zaheer, Andrew McCallum |  |
| 796 |  |  [Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.141) |  | 0 |  | Mirac Suzgun, Luke MelasKyriazi, Dan Jurafsky |  |
| 797 |  |  [Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts](https://doi.org/10.18653/v1/2022.emnlp-main.142) |  | 0 |  | Ben Zhou, Kyle Richardson, Xiaodong Yu, Dan Roth |  |
| 798 |  |  [Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality](https://doi.org/10.18653/v1/2022.emnlp-main.143) |  | 0 |  | Anuj Diwan, Layne Berry, Eunsol Choi, David Harwath, Kyle Mahowald |  |
| 799 |  |  [Gradient-based Constrained Sampling from Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.144) |  | 0 |  | Sachin Kumar, Biswajit Paria, Yulia Tsvetkov |  |
| 800 |  |  [TaCube: Pre-computing Data Cubes for Answering Numerical-Reasoning Questions over Tabular Data](https://doi.org/10.18653/v1/2022.emnlp-main.145) |  | 0 |  | Fan Zhou, Mengkang Hu, Haoyu Dong, Zhoujun Cheng, Fan Cheng, Shi Han, Dongmei Zhang |  |
| 801 |  |  [Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence](https://doi.org/10.18653/v1/2022.emnlp-main.146) |  | 0 |  | HungTing Chen, Michael J. Q. Zhang, Eunsol Choi |  |
| 802 |  |  [QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation](https://doi.org/10.18653/v1/2022.emnlp-main.147) |  | 0 |  | Zhenrui Yue, Huimin Zeng, Bernhard Kratzwald, Stefan Feuerriegel, Dong Wang |  |
| 803 |  |  [When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain](https://doi.org/10.18653/v1/2022.emnlp-main.148) |  | 0 |  | Raj Sanjay Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Natraj Raman, Charese Smiley, Jiaao Chen, Diyi Yang |  |
| 804 |  |  [Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer](https://doi.org/10.18653/v1/2022.emnlp-main.149) |  | 0 |  | Zhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun Araki, Haibo Ding, Jamie Callan, Graham Neubig |  |
| 805 |  |  [Reproducibility in Computational Linguistics: Is Source Code Enough?](https://doi.org/10.18653/v1/2022.emnlp-main.150) |  | 0 |  | Mohammad Arvan, Luís Pina, Natalie Parde |  |
| 806 |  |  [Generating Information-Seeking Conversations from Unlabeled Documents](https://doi.org/10.18653/v1/2022.emnlp-main.151) |  | 0 |  | Gangwoo Kim, Sungdong Kim, Kang Min Yoo, Jaewoo Kang |  |
| 807 |  |  [Distill The Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.152) |  | 0 |  | Ru Peng, Yawen Zeng, Jake Zhao |  |
| 808 |  |  [A Multifaceted Framework to Evaluate Evasion, Content Preservation, and Misattribution in Authorship Obfuscation Techniques](https://doi.org/10.18653/v1/2022.emnlp-main.153) |  | 0 |  | Malik H. Altakrori, Thomas Scialom, Benjamin C. M. Fung, Jackie Chi Kit Cheung |  |
| 809 |  |  [SafeText: A Benchmark for Exploring Physical Safety in Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.154) |  | 0 |  | Sharon Levy, Emily Allaway, Melanie Subbiah, Lydia B. Chilton, Desmond Patton, Kathleen R. McKeown, William Yang Wang |  |
| 810 |  |  [Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations](https://doi.org/10.18653/v1/2022.emnlp-main.155) |  | 0 |  | Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, SangWoo Lee, Sanggoo Lee, Taeuk Kim |  |
| 811 |  |  [D4: a Chinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat](https://doi.org/10.18653/v1/2022.emnlp-main.156) |  | 0 |  | Binwei Yao, Chao Shi, Likai Zou, Lingfeng Dai, Mengyue Wu, Lu Chen, Zhen Wang, Kai Yu |  |
| 812 |  |  [Exploiting domain-slot related keywords description for Few-Shot Cross-Domain Dialogue State Tracking](https://doi.org/10.18653/v1/2022.emnlp-main.157) |  | 0 |  | QiXiang Gao, Guanting Dong, Yutao Mou, Liwen Wang, Chen Zeng, Daichi Guo, Mingyang Sun, Weiran Xu |  |
| 813 |  |  [CoCoa: An Encoder-Decoder Model for Controllable Code-switched Generation](https://doi.org/10.18653/v1/2022.emnlp-main.158) |  | 0 |  | Sneha Mondal, Ritika, Shreya Pathak, Preethi Jyothi, Aravindan Raghuveer |  |
| 814 |  |  [Towards Climate Awareness in NLP Research](https://doi.org/10.18653/v1/2022.emnlp-main.159) |  | 0 |  | Daniel Hershcovich, Nicolas Webersinke, Mathias Kraus, Julia Anna Bingler, Markus Leippold |  |
| 815 |  |  [Navigating Connected Memories with a Task-oriented Dialog System](https://doi.org/10.18653/v1/2022.emnlp-main.160) |  | 0 |  | Satwik Kottur, Seungwhan Moon, Alborz Geramifard, Babak Damavandi |  |
| 816 |  |  [Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.161) |  | 0 |  | Hao Zhang |  |
| 817 |  |  [SynGEC: Syntax-Enhanced Grammatical Error Correction with a Tailored GEC-Oriented Parser](https://doi.org/10.18653/v1/2022.emnlp-main.162) |  | 0 |  | Yue Zhang, Bo Zhang, Zhenghua Li, Zuyi Bao, Chen Li, Min Zhang |  |
| 818 |  |  [Varifocal Question Generation for Fact-checking](https://doi.org/10.18653/v1/2022.emnlp-main.163) |  | 0 |  | Nedjma Ousidhoum, Zhangdie Yuan, Andreas Vlachos |  |
| 819 |  |  [Bilingual Lexicon Induction for Low-Resource Languages using Graph Matching via Optimal Transport](https://doi.org/10.18653/v1/2022.emnlp-main.164) |  | 0 |  | Kelly Marchisio, Ali SaadEldin, Kevin Duh, Carey E. Priebe, Philipp Koehn |  |
| 820 |  |  [Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection](https://doi.org/10.18653/v1/2022.emnlp-main.165) |  | 0 |  | Suchin Gururangan, Dallas Card, Sarah K. Dreier, Emily K. Gade, Leroy Z. Wang, Zeyu Wang, Luke Zettlemoyer, Noah A. Smith |  |
| 821 |  |  [ConReader: Exploring Implicit Relations in Contracts for Contract Clause Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.166) |  | 0 |  | Weiwen Xu, Yang Deng, Wenqiang Lei, Wenlong Zhao, TatSeng Chua, Wai Lam |  |
| 822 |  |  [Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU](https://doi.org/10.18653/v1/2022.emnlp-main.167) |  | 0 |  | Fenia Christopoulou, Gerasimos Lampouras, Ignacio Iacobacci |  |
| 823 |  |  [Revisiting Parameter-Efficient Tuning: Are We Really There Yet?](https://doi.org/10.18653/v1/2022.emnlp-main.168) |  | 0 |  | Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, Shangsong Liang |  |
| 824 |  |  [Transfer Learning from Semantic Role Labeling to Event Argument Extraction with Template-based Slot Querying](https://doi.org/10.18653/v1/2022.emnlp-main.169) |  | 0 |  | Zhisong Zhang, Emma Strubell, Eduard H. Hovy |  |
| 825 |  |  [Calibrating Zero-shot Cross-lingual (Un-)structured Predictions](https://doi.org/10.18653/v1/2022.emnlp-main.170) |  | 0 |  | Zhengping Jiang, Anqi Liu, Benjamin Van Durme |  |
| 826 |  |  [PRINCE: Prefix-Masked Decoding for Knowledge Enhanced Sequence-to-Sequence Pre-Training](https://doi.org/10.18653/v1/2022.emnlp-main.171) |  | 0 |  | Song Xu, Haoran Li, Peng Yuan, Youzheng Wu, Xiaodong He |  |
| 827 |  |  [How Far are We from Robust Long Abstractive Summarization?](https://doi.org/10.18653/v1/2022.emnlp-main.172) |  | 0 |  | Huan Yee Koh, Jiaxin Ju, He Zhang, Ming Liu, Shirui Pan |  |
| 828 |  |  [Measuring Context-Word Biases in Lexical Semantic Datasets](https://doi.org/10.18653/v1/2022.emnlp-main.173) |  | 0 |  | Qianchu Liu, Diana McCarthy, Anna Korhonen |  |
| 829 |  |  [Iteratively Prompt Pre-trained Language Models for Chain of Thought](https://doi.org/10.18653/v1/2022.emnlp-main.174) |  | 0 |  | Boshi Wang, Xiang Deng, Huan Sun |  |
| 830 |  |  [Unobserved Local Structures Make Compositional Generalization Hard](https://doi.org/10.18653/v1/2022.emnlp-main.175) |  | 0 |  | Ben Bogin, Shivanshu Gupta, Jonathan Berant |  |
| 831 |  |  [Mitigating Data Sparsity for Short Text Topic Modeling by Topic-Semantic Contrastive Learning](https://doi.org/10.18653/v1/2022.emnlp-main.176) |  | 0 |  | Xiaobao Wu, Anh Tuan Luu, Xinshuai Dong |  |
| 832 |  |  [Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.177) |  | 0 |  | Yiyang Li, Hai Zhao, Zhuosheng Zhang |  |
| 833 |  |  [Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates](https://doi.org/10.18653/v1/2022.emnlp-main.178) |  | 0 |  | Dongfang Li, Baotian Hu, Qingcai Chen |  |
| 834 |  |  [Non-Autoregressive Neural Machine Translation: A Call for Clarity](https://doi.org/10.18653/v1/2022.emnlp-main.179) |  | 0 |  | Robin M. Schmidt, Telmo Pires, Stephan Peitz, Jonas Lööf |  |
| 835 |  |  [RED-ACE: Robust Error Detection for ASR using Confidence Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.180) |  | 0 |  | Zorik Gekhman, Dina Zverinski, Jonathan Mallinson, Genady Beryozkin |  |
| 836 |  |  [Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation](https://doi.org/10.18653/v1/2022.emnlp-main.181) |  | 0 |  | Xiang Hu, Haitao Mi, Liang Li, Gerard de Melo |  |
| 837 |  |  [A Localized Geometric Method to Match Knowledge in Low-dimensional Hyperbolic Space](https://doi.org/10.18653/v1/2022.emnlp-main.182) |  | 0 |  | Bo Hui, Tian Xia, WeiShinn Ku |  |
| 838 |  |  [Memory-assisted prompt editing to improve GPT-3 after deployment](https://doi.org/10.18653/v1/2022.emnlp-main.183) |  | 0 |  | Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang |  |
| 839 |  |  [LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.184) |  | 0 |  | Hongcheng Guo, Jiaheng Liu, Haoyang Huang, Jian Yang, Zhoujun Li, Dongdong Zhang, Zheng Cui |  |
| 840 |  |  [PromptEHR: Conditional Electronic Healthcare Records Generation with Prompt Learning](https://doi.org/10.18653/v1/2022.emnlp-main.185) |  | 0 |  | Zifeng Wang, Jimeng Sun |  |
| 841 |  |  [ROSE: Robust Selective Fine-tuning for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.186) |  | 0 |  | Lan Jiang, Hao Zhou, Yankai Lin, Peng Li, Jie Zhou, Rui Jiang |  |
| 842 |  |  [CodeRetriever: A Large Scale Contrastive Pre-Training Method for Code Search](https://doi.org/10.18653/v1/2022.emnlp-main.187) |  | 0 |  | Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang, Bolun Yao, Weizhen Qi, Daxin Jiang, Weizhu Chen, Nan Duan |  |
| 843 |  |  [Open-Topic False Information Detection on Social Networks with Contrastive Adversarial Learning](https://doi.org/10.18653/v1/2022.emnlp-main.188) |  | 0 |  | Guanghui Ma, Chunming Hu, Ling Ge, Hong Zhang |  |
| 844 |  |  [Mitigating Inconsistencies in Multimodal Sentiment Analysis under Uncertain Missing Modalities](https://doi.org/10.18653/v1/2022.emnlp-main.189) |  | 0 |  | Jiandian Zeng, Jiantao Zhou, Tianyi Liu |  |
| 845 |  |  [ConvTrans: Transforming Web Search Sessions for Conversational Dense Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.190) |  | 0 |  | Kelong Mao, Zhicheng Dou, Hongjin Qian, Fengran Mo, Xiaohua Cheng, Zhao Cao |  |
| 846 |  |  [MUSIED: A Benchmark for Event Detection from Multi-Source Heterogeneous Informal Texts](https://doi.org/10.18653/v1/2022.emnlp-main.191) |  | 0 |  | Xiangyu Xi, Jianwei Lv, Shuaipeng Liu, Wei Ye, Fan Yang, Guanglu Wan |  |
| 847 |  |  [Reproducibility Issues for BERT-based Evaluation Metrics](https://doi.org/10.18653/v1/2022.emnlp-main.192) |  | 0 |  | Yanran Chen, Jonas Belouadi, Steffen Eger |  |
| 848 |  |  [Improving Multi-task Stance Detection with Multi-task Interaction Network](https://doi.org/10.18653/v1/2022.emnlp-main.193) |  | 0 |  | Heyan Chai, Siyu Tang, Jinhao Cui, Ye Ding, Binxing Fang, Qing Liao |  |
| 849 |  |  [Neural-based Mixture Probabilistic Query Embedding for Answering FOL queries on Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.194) |  | 0 |  | Xiao Long, Liansheng Zhuang, Aodi Li, Shafei Wang, Houqiang Li |  |
| 850 |  |  [Improving Multi-turn Emotional Support Dialogue Generation with Lookahead Strategy Planning](https://doi.org/10.18653/v1/2022.emnlp-main.195) |  | 0 |  | Yi Cheng, Wenge Liu, Wenjie Li, Jiashuo Wang, Ruihui Zhao, Bang Liu, Xiaodan Liang, Yefeng Zheng |  |
| 851 |  |  [Conformal Predictor for Improving Zero-Shot Text Classification Efficiency](https://doi.org/10.18653/v1/2022.emnlp-main.196) |  | 0 |  | Prafulla Kumar Choubey, Yu Bai, ChienSheng Wu, Wenhao Liu, Nazneen Rajani |  |
| 852 |  |  [Effective and Efficient Query-aware Snippet Extraction for Web Search](https://doi.org/10.18653/v1/2022.emnlp-main.197) |  | 0 |  | Jingwei Yi, Fangzhao Wu, Chuhan Wu, Xiaolong Huang, Binxing Jiao, Guangzhong Sun, Xing Xie |  |
| 853 |  |  [You Only Need One Model for Open-domain Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.198) |  | 0 |  | Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paranjape, Christopher D. Manning, KyoungGu Woo |  |
| 854 |  |  [Generative Entity Typing with Curriculum Learning](https://doi.org/10.18653/v1/2022.emnlp-main.199) |  | 0 |  | Siyu Yuan, Deqing Yang, Jiaqing Liang, Zhixu Li, Jinxi Liu, Jingyue Huang, Yanghua Xiao |  |
| 855 |  |  [SetGNER: General Named Entity Recognition as Entity Set Generation](https://doi.org/10.18653/v1/2022.emnlp-main.200) |  | 0 |  | Yuxin He, Buzhou Tang |  |
| 856 |  |  [Opinion Summarization by Weak-Supervision from Mix-structured Data](https://doi.org/10.18653/v1/2022.emnlp-main.201) |  | 0 |  | Yizhu Liu, Qi Jia, Kenny Q. Zhu |  |
| 857 |  |  [Multi-level Distillation of Semantic Knowledge for Pre-training Multilingual Language Model](https://doi.org/10.18653/v1/2022.emnlp-main.202) |  | 0 |  | Mingqi Li, Fei Ding, Dan Zhang, Long Cheng, Hongxin Hu, Feng Luo |  |
| 858 |  |  [Empowering Dual-Encoder with Query Generator for Cross-Lingual Dense Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.203) |  | 0 |  | Houxing Ren, Linjun Shou, Ning Wu, Ming Gong, Daxin Jiang |  |
| 859 |  |  [R2F: A General Retrieval, Reading and Fusion Framework for Document-level Natural Language Inference](https://doi.org/10.18653/v1/2022.emnlp-main.204) |  | 0 |  | Hao Wang, Yixin Cao, Yangguang Li, Zhen Huang, Kun Wang, Jing Shao |  |
| 860 |  |  [Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-main.205) |  | 0 |  | Abbas Ghaddar, Yimeng Wu, Sunyam Bagga, Ahmad Rashid, Khalil Bibi, Mehdi Rezagholizadeh, Chao Xing, Yasheng Wang, Xinyu Duan, Zhefeng Wang, Baoxing Huai, Xin Jiang, Qun Liu, Philippe Langlais |  |
| 861 |  |  [KECP: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.206) |  | 0 |  | Jianing Wang, Chengyu Wang, Minghui Qiu, Qiuhui Shi, Hongbin Wang, Jun Huang, Ming Gao |  |
| 862 |  |  [Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.207) |  | 0 |  | Jianing Wang, Wenkang Huang, Minghui Qiu, Qiuhui Shi, Hongbin Wang, Xiang Li, Ming Gao |  |
| 863 |  |  [On the Evaluation Metrics for Paraphrase Generation](https://doi.org/10.18653/v1/2022.emnlp-main.208) |  | 0 |  | Lingfeng Shen, Lemao Liu, Haiyun Jiang, Shuming Shi |  |
| 864 |  |  [Curriculum Learning Meets Weakly Supervised Multimodal Correlation Learning](https://doi.org/10.18653/v1/2022.emnlp-main.209) |  | 0 |  | Sijie Mai, Ya Sun, Haifeng Hu |  |
| 865 |  |  [Rethinking Positional Encoding in Tree Transformer for Code Representation](https://doi.org/10.18653/v1/2022.emnlp-main.210) |  | 0 |  | Han Peng, Ge Li, Yunfei Zhao, Zhi Jin |  |
| 866 |  |  [RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL](https://doi.org/10.18653/v1/2022.emnlp-main.211) |  | 0 |  | Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, Zhouhan Lin |  |
| 867 |  |  [COM-MRC: A COntext-Masked Machine Reading Comprehension Framework for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.212) |  | 0 |  | Zepeng Zhai, Hao Chen, Fangxiang Feng, Ruifan Li, Xiaojie Wang |  |
| 868 |  |  [CEM: Machine-Human Chatting Handoff via Causal-Enhance Module](https://doi.org/10.18653/v1/2022.emnlp-main.213) |  | 0 |  | ShanShan Zhong, Jinghui Qin, Zhongzhan Huang, Daifeng Li |  |
| 869 |  |  [Nearest Neighbor Zero-Shot Inference](https://doi.org/10.18653/v1/2022.emnlp-main.214) |  | 0 |  | Weijia Shi, Julian Michael, Suchin Gururangan, Luke Zettlemoyer |  |
| 870 |  |  [Robots-Dont-Cry: Understanding Falsely Anthropomorphic Utterances in Dialog Systems](https://doi.org/10.18653/v1/2022.emnlp-main.215) |  | 0 |  | David Gros, Yu Li, Zhou Yu |  |
| 871 |  |  [A Joint Learning Framework for Restaurant Survival Prediction and Explanation](https://doi.org/10.18653/v1/2022.emnlp-main.216) |  | 0 |  | Xin Li, Xiaojie Zhang, Peng Jia, Rui Mao, MingYang Zhou, Xing Xie, Hao Liao |  |
| 872 |  |  [Making Pretrained Language Models Good Long-tailed Learners](https://doi.org/10.18653/v1/2022.emnlp-main.217) |  | 0 |  | Chen Zhang, Lei Ren, Jingang Wang, Wei Wu, Dawei Song |  |
| 873 |  |  [UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression](https://doi.org/10.18653/v1/2022.emnlp-main.218) |  | 0 |  | Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, Xiaodan Liang |  |
| 874 |  |  [Face-Sensitive Image-to-Emotional-Text Cross-modal Translation for Multimodal Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.219) |  | 0 |  | Hao Yang, Yanyan Zhao, Bing Qin |  |
| 875 |  |  [FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.220) |  | 0 |  | Chen Zhang, Luis Fernando D'Haro, Qiquan Zhang, Thomas Friedrichs, Haizhou Li |  |
| 876 |  |  [Sentence Representation Learning with Generative Objective rather than Contrastive Objective](https://doi.org/10.18653/v1/2022.emnlp-main.221) |  | 0 |  | Bohong Wu, Hai Zhao |  |
| 877 |  |  [RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning](https://doi.org/10.18653/v1/2022.emnlp-main.222) |  | 0 |  | Mingkai Deng, Jianyu Wang, ChengPing Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, Zhiting Hu |  |
| 878 |  |  [DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.223) |  | 0 |  | Hanqing Zhang, Dawei Song |  |
| 879 |  |  [CPL: Counterfactual Prompt Learning for Vision and Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.224) |  | 0 |  | Xuehai He, Diji Yang, Weixi Feng, TsuJui Fu, Arjun R. Akula, Varun Jampani, Pradyumna Narayana, Sugato Basu, William Yang Wang, Xin Wang |  |
| 880 |  |  [Red Teaming Language Models with Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.225) |  | 0 |  | Ethan Perez, Saffron Huang, H. Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, Geoffrey Irving |  |
| 881 |  |  [CapOnImage: Context-driven Dense-Captioning on Image](https://doi.org/10.18653/v1/2022.emnlp-main.226) |  | 0 |  | Yiqi Gao, Xinglin Hou, Yuanmeng Zhang, Tiezheng Ge, Yuning Jiang, Peng Wang |  |
| 882 |  |  [SpanProto: A Two-stage Span-based Prototypical Network for Few-shot Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.227) |  | 0 |  | Jianing Wang, Chengyu Wang, Chuanqi Tan, Minghui Qiu, Songfang Huang, Jun Huang, Ming Gao |  |
| 883 |  |  [Discovering Differences in the Representation of People using Contextualized Semantic Axes](https://doi.org/10.18653/v1/2022.emnlp-main.228) |  | 0 |  | Li Lucy, Divya Tadimeti, David Bamman |  |
| 884 |  |  [Generating Literal and Implied Subquestions to Fact-check Complex Claims](https://doi.org/10.18653/v1/2022.emnlp-main.229) |  | 0 |  | Jifan Chen, Aniruddh Sriram, Eunsol Choi, Greg Durrett |  |
| 885 |  |  [Machine Translation Robustness to Natural Asemantic Variation](https://doi.org/10.18653/v1/2022.emnlp-main.230) |  | 0 |  | Jacob Bremerman, Xiang Ren, Jonathan May |  |
| 886 |  |  [Natural Language to Code Translation with Execution](https://doi.org/10.18653/v1/2022.emnlp-main.231) |  | 0 |  | Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, Sida I. Wang |  |
| 887 |  |  [Life is a Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes](https://doi.org/10.18653/v1/2022.emnlp-main.232) |  | 0 |  | Oren Sultan, Dafna Shahaf |  |
| 888 |  |  [Language Contamination Helps Explains the Cross-lingual Capabilities of English Pretrained Models](https://doi.org/10.18653/v1/2022.emnlp-main.233) |  | 0 |  | Terra Blevins, Luke Zettlemoyer |  |
| 889 |  |  [Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.234) |  | 0 |  | Terra Blevins, Hila Gonen, Luke Zettlemoyer |  |
| 890 |  |  [Neural Machine Translation with Contrastive Translation Memories](https://doi.org/10.18653/v1/2022.emnlp-main.235) |  | 0 |  | Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, Rui Yan |  |
| 891 |  |  [Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.236) |  | 0 |  | Junhao Zheng, Zhanxian Liang, Haibin Chen, Qianli Ma |  |
| 892 |  |  [Exploring the Secrets Behind the Learning Difficulty of Meaning Representations for Semantic Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.237) |  | 0 |  | Zhenwen Li, Jiaqi Guo, Qian Liu, JianGuang Lou, Tao Xie |  |
| 893 |  |  [That's the Wrong Lung! Evaluating and Improving the Interpretability of Unsupervised Multimodal Encoders for Medical Data](https://doi.org/10.18653/v1/2022.emnlp-main.238) |  | 0 |  | Denis Jered McInerney, Geoffrey S. Young, JanWillem van de Meent, Byron C. Wallace |  |
| 894 |  |  [Unsupervised Tokenization Learning](https://doi.org/10.18653/v1/2022.emnlp-main.239) |  | 0 |  | Anton Kolonin, Vignav Ramesh |  |
| 895 |  |  [A Template-based Method for Constrained Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.240) |  | 0 |  | Shuo Wang, Peng Li, Zhixing Tan, Zhaopeng Tu, Maosong Sun, Yang Liu |  |
| 896 |  |  [PATS: Sensitivity-aware Noisy Learning for Pretrained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.241) |  | 0 |  | Yupeng Zhang, Hongzhi Zhang, Sirui Wang, Wei Wu, Zhoujun Li |  |
| 897 |  |  [Towards Reinterpreting Neural Topic Models via Composite Activations](https://doi.org/10.18653/v1/2022.emnlp-main.242) |  | 0 |  | Jia Peng Lim, Hady W. Lauw |  |
| 898 |  |  [Few-shot Query-Focused Summarization with Prefix-Merging](https://doi.org/10.18653/v1/2022.emnlp-main.243) |  | 0 |  | Ruifeng Yuan, Zili Wang, Ziqiang Cao, Wenjie Li |  |
| 899 |  |  [Cross-Align: Modeling Deep Cross-lingual Interactions for Word Alignment](https://doi.org/10.18653/v1/2022.emnlp-main.244) |  | 0 |  | Siyu Lai, Zhen Yang, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou |  |
| 900 |  |  [BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.245) |  | 0 |  | Tianxiang Sun, Junliang He, Xipeng Qiu, Xuanjing Huang |  |
| 901 |  |  [HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.246) |  | 0 |  | Zihan Wang, Peiyi Wang, Tianyu Liu, Binghuai Lin, Yunbo Cao, Zhifang Sui, Houfeng Wang |  |
| 902 |  |  [Not to Overfit or Underfit the Source Domains? An Empirical Study of Domain Generalization in Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.247) |  | 0 |  | Md. Arafat Sultan, Avi Sil, Radu Florian |  |
| 903 |  |  [Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs](https://doi.org/10.18653/v1/2022.emnlp-main.248) |  | 0 |  | Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Choi |  |
| 904 |  |  [Improving Passage Retrieval with Zero-Shot Question Generation](https://doi.org/10.18653/v1/2022.emnlp-main.249) |  | 0 |  | Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wentau Yih, Joelle Pineau, Luke Zettlemoyer |  |
| 905 |  |  [Summarizing Community-based Question-Answer Pairs](https://doi.org/10.18653/v1/2022.emnlp-main.250) |  | 0 |  | TingYao Hsu, Yoshi Suhara, Xiaolan Wang |  |
| 906 |  |  [Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models](https://doi.org/10.18653/v1/2022.emnlp-main.251) |  | 0 |  | Joe Stacey, Pasquale Minervini, Haim Dubossarsky, Marek Rei |  |
| 907 |  |  [How to disagree well: Investigating the dispute tactics used on Wikipedia](https://doi.org/10.18653/v1/2022.emnlp-main.252) |  | 0 |  | Christine de Kock, Andreas Vlachos |  |
| 908 |  |  [Chapter Ordering in Novels](https://doi.org/10.18653/v1/2022.emnlp-main.253) |  | 0 |  | Allen Kim, Steven Skiena |  |
| 909 |  |  [Open-ended Knowledge Tracing for Computer Science Education](https://doi.org/10.18653/v1/2022.emnlp-main.254) |  | 0 |  | Naiming Liu, Zichao Wang, Richard G. Baraniuk, Andrew S. Lan |  |
| 910 |  |  [Logical Neural Networks for Knowledge Base Completion with Embeddings & Rules](https://doi.org/10.18653/v1/2022.emnlp-main.255) |  | 0 |  | Prithviraj Sen, Breno W. S. R. de Carvalho, Ibrahim Abdelaziz, Pavan Kapanipathi, Salim Roukos, Alexander G. Gray |  |
| 911 |  |  [MedCLIP: Contrastive Learning from Unpaired Medical Images and Text](https://doi.org/10.18653/v1/2022.emnlp-main.256) |  | 0 |  | Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, Jimeng Sun |  |
| 912 |  |  [GA-SAM: Gradient-Strength based Adaptive Sharpness-Aware Minimization for Improved Generalization](https://doi.org/10.18653/v1/2022.emnlp-main.257) |  | 0 |  | Zhiyuan Zhang, Ruixuan Luo, Qi Su, Xu Sun |  |
| 913 |  |  [Sparse Teachers Can Be Dense with Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.258) |  | 0 |  | Yi Yang, Chen Zhang, Dawei Song |  |
| 914 |  |  [BBTv2: Towards a Gradient-Free Future with Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.259) |  | 0 |  | Tianxiang Sun, Zhengfu He, Hong Qian, Yunhua Zhou, Xuanjing Huang, Xipeng Qiu |  |
| 915 |  |  [Passage-Mask: A Learnable Regularization Strategy for Retriever-Reader Models](https://doi.org/10.18653/v1/2022.emnlp-main.260) |  | 0 |  | Shujian Zhang, Chengyue Gong, Xingchao Liu |  |
| 916 |  |  [Mixed-effects transformers for hierarchical adaptation](https://doi.org/10.18653/v1/2022.emnlp-main.261) |  | 0 |  | Julia White, Noah D. Goodman, Robert X. D. Hawkins |  |
| 917 |  |  [On Measuring the Intrinsic Few-Shot Hardness of Datasets](https://doi.org/10.18653/v1/2022.emnlp-main.262) |  | 0 |  | Xinran Zhao, Shikhar Murty, Christopher D. Manning |  |
| 918 |  |  [Group is better than individual: Exploiting Label Topologies and Label Relations for Joint Multiple Intent Detection and Slot Filling](https://doi.org/10.18653/v1/2022.emnlp-main.263) |  | 0 |  | Bowen Xing, Ivor W. Tsang |  |
| 919 |  |  [An Empirical Study on Finding Spans](https://doi.org/10.18653/v1/2022.emnlp-main.264) |  | 0 |  | Weiwei Gu, Boyuan Zheng, Yunmo Chen, Tongfei Chen, Benjamin Van Durme |  |
| 920 |  |  [MGDoc: Pre-training with Multi-granular Hierarchy for Document Image Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.265) |  | 0 |  | Zilong Wang, Jiuxiang Gu, Chris Tensmeyer, Nikolaos Barmpalios, Ani Nenkova, Tong Sun, Jingbo Shang, Vlad I. Morariu |  |
| 921 |  |  [Understanding Jargon: Combining Extraction and Generation for Definition Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.266) |  | 0 |  | Jie Huang, Hanyin Shao, Kevin ChenChuan Chang, Jinjun Xiong, WenMei Hwu |  |
| 922 |  |  [ProsocialDialog: A Prosocial Backbone for Conversational Agents](https://doi.org/10.18653/v1/2022.emnlp-main.267) |  | 0 |  | Hyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing Lu, Daniel Khashabi, Gunhee Kim, Yejin Choi, Maarten Sap |  |
| 923 |  |  [Exploiting Global and Local Hierarchies for Hierarchical Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.268) |  | 0 |  | Ting Jiang, Deqing Wang, Leilei Sun, Zhongzhi Chen, Fuzhen Zhuang, Qinghong Yang |  |
| 924 |  |  [Semantic-aware Contrastive Learning for More Accurate Semantic Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.269) |  | 0 |  | Shan Wu, Chunlei Xin, Bo Chen, Xianpei Han, Le Sun |  |
| 925 |  |  [Scientific Paper Extractive Summarization Enhanced by Citation Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.270) |  | 0 |  | Xiuying Chen, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, Xiangliang Zhang |  |
| 926 |  |  [Hardness-guided domain adaptation to recognise biomedical named entities under low-resource scenarios](https://doi.org/10.18653/v1/2022.emnlp-main.271) |  | 0 |  | Ngoc Dang Nguyen, Lan Du, Wray L. Buntine, Changyou Chen, Richard Beare |  |
| 927 |  |  [Syntactic Multi-view Learning for Open Information Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.272) |  | 0 |  | Kuicai Dong, Aixin Sun, JungJae Kim, Xiaoli Li |  |
| 928 |  |  [TRIPS: Efficient Vision-and-Language Pre-training with Text-Relevant Image Patch Selection](https://doi.org/10.18653/v1/2022.emnlp-main.273) |  | 0 |  | Chaoya Jiang, Haiyang Xu, Chenliang Li, Ming Yan, Wei Ye, Shikun Zhang, Bin Bi, Songfang Huang |  |
| 929 |  |  [CGoDial: A Large-Scale Benchmark for Chinese Goal-oriented Dialog Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.274) |  | 0 |  | Yinpei Dai, Wanwei He, Bowen Li, Yuchuan Wu, Zheng Cao, Zhongqi An, Jian Sun, Yongbin Li |  |
| 930 |  |  [Kernel-Whitening: Overcome Dataset Bias with Isotropic Sentence Embedding](https://doi.org/10.18653/v1/2022.emnlp-main.275) |  | 0 |  | Songyang Gao, Shihan Dou, Qi Zhang, Xuanjing Huang |  |
| 931 |  |  [A Unified Positive-Unlabeled Learning Framework for Document-Level Relation Extraction with Different Levels of Labeling](https://doi.org/10.18653/v1/2022.emnlp-main.276) |  | 0 |  | Ye Wang, Xinxin Liu, Wenxin Hu, Tao Zhang |  |
| 932 |  |  [Automatic Generation of Socratic Subquestions for Teaching Math Word Problems](https://doi.org/10.18653/v1/2022.emnlp-main.277) |  | 0 |  | Kumar Shridhar, Jakub Macina, Mennatallah ElAssady, Tanmay Sinha, Manu Kapur, Mrinmaya Sachan |  |
| 933 |  |  [Mixture of Attention Heads: Selecting Attention Heads Per Token](https://doi.org/10.18653/v1/2022.emnlp-main.278) |  | 0 |  | Xiaofeng Zhang, Yikang Shen, Zeyu Huang, Jie Zhou, Wenge Rong, Zhang Xiong |  |
| 934 |  |  [The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.279) |  | 0 |  | Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, Dan Alistarh |  |
| 935 |  |  [Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue](https://doi.org/10.18653/v1/2022.emnlp-main.280) |  | 0 |  | Sunjae Yoon, Eunseop Yoon, Hee Suk Yoon, Junyeong Kim, Chang Dong Yoo |  |
| 936 |  |  [DSM: Question Generation over Knowledge Base via Modeling Diverse Subgraphs with Meta-learner](https://doi.org/10.18653/v1/2022.emnlp-main.281) |  | 0 |  | Shasha Guo, Jing Zhang, Yanling Wang, Qianyi Zhang, Cuiping Li, Hong Chen |  |
| 937 |  |  [RelU-Net: Syntax-aware Graph U-Net for Relational Triple Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.282) |  | 0 |  | Yunqi Zhang, Yubo Chen, Yongfeng Huang |  |
| 938 |  |  [Evidence \textgreater Intuition: Transferability Estimation for Encoder Selection](https://doi.org/10.18653/v1/2022.emnlp-main.283) |  | 0 |  | Elisa Bassignana, Max MüllerEberstein, Mike Zhang, Barbara Plank |  |
| 939 |  |  [Chunk-based Nearest Neighbor Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.284) |  | 0 |  | Pedro Henrique Martins, Zita Marinho, André F. T. Martins |  |
| 940 |  |  [FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.285) |  | 0 |  | Akhil Kedia, Mohd Abbas Zaidi, Haejun Lee |  |
| 941 |  |  [Inductive Relation Prediction with Logical Reasoning Using Contrastive Representations](https://doi.org/10.18653/v1/2022.emnlp-main.286) |  | 0 |  | Yudai Pan, Jun Liu, Lingling Zhang, Tianzhe Zhao, Qika Lin, Xin Hu, Qianying Wang |  |
| 942 |  |  [Improving Chinese Spelling Check by Character Pronunciation Prediction: The Effects of Adaptivity and Granularity](https://doi.org/10.18653/v1/2022.emnlp-main.287) |  | 0 |  | Jiahao Li, Quan Wang, Zhendong Mao, Junbo Guo, Yanyan Yang, Yongdong Zhang |  |
| 943 |  |  [MT-GenEval: A Counterfactual and Contextual Dataset for Evaluating Gender Accuracy in Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.288) |  | 0 |  | Anna Currey, Maria Nadejde, Raghavendra Reddy Pappagari, Mia Mayer, Stanislas Lauly, Xing Niu, Benjamin Hsu, Georgiana Dinu |  |
| 944 |  |  [A Span-level Bidirectional Network for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.289) |  | 0 |  | Yuqi Chen, Keming Chen, Xian Sun, Zequn Zhang |  |
| 945 |  |  [On the Calibration of Massively Multilingual Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.290) |  | 0 |  | Kabir Ahuja, Sunayana Sitaram, Sandipan Dandapat, Monojit Choudhury |  |
| 946 |  |  [Momentum Contrastive Pre-training for Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.291) |  | 0 |  | Minda Hu, Muzhi Li, Yasheng Wang, Irwin King |  |
| 947 |  |  [A Second Wave of UD Hebrew Treebanking and Cross-Domain Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.292) |  | 0 |  | Amir Zeldes, Nick Howell, Noam Ordan, Yifat Ben Moshe |  |
| 948 |  |  [Finding Dataset Shortcuts with Grammar Induction](https://doi.org/10.18653/v1/2022.emnlp-main.293) |  | 0 |  | Dan Friedman, Alexander Wettig, Danqi Chen |  |
| 949 |  |  [Retrieval Augmentation for Commonsense Reasoning: A Unified Approach](https://doi.org/10.18653/v1/2022.emnlp-main.294) |  | 0 |  | Wenhao Yu, Chenguang Zhu, Zhihan Zhang, Shuohang Wang, Zhuosheng Zhang, Yuwei Fang, Meng Jiang |  |
| 950 |  |  [Open World Classification with Adaptive Negative Samples](https://doi.org/10.18653/v1/2022.emnlp-main.295) |  | 0 |  | Ke Bai, Guoyin Wang, Jiwei Li, Sunghyun Park, Sungjin Lee, Puyang Xu, Ricardo Henao, Lawrence Carin |  |
| 951 |  |  [Re3: Generating Longer Stories With Recursive Reprompting and Revision](https://doi.org/10.18653/v1/2022.emnlp-main.296) |  | 0 |  | Kevin Yang, Yuandong Tian, Nanyun Peng, Dan Klein |  |
| 952 |  |  [Does Joint Training Really Help Cascaded Speech Translation?](https://doi.org/10.18653/v1/2022.emnlp-main.297) |  | 0 |  | Viet Anh Khoa Tran, David Thulke, Yingbo Gao, Christian Herold, Hermann Ney |  |
| 953 |  |  [MasakhaNER 2.0: Africa-centric Transfer Learning for Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.298) |  | 0 |  | David Ifeoluwa Adelani, Graham Neubig, Sebastian Ruder, Shruti Rijhwani, Michael Beukman, Chester PalenMichel, Constantine Lignos, Jesujoba O. Alabi, Shamsuddeen Hassan Muhammad, Peter Nabende, Cheikh M. Bamba Dione, Andiswa Bukula, Rooweither Mabuya, Bonaventure F. P. Dossou, Blessing K. Sibanda, Happy Buzaaba, Jonathan Mukiibi, Godson Kalipe, Derguene Mbaye, Amelia V. Taylor, Fatoumata Ouoba Kabore, Chris Chinenye Emezue, Aremu Anuoluwapo, Perez Ogayo, Catherine Gitau, Edwin MunkohBuabeng, Victoire Memdjokam Koagne, Allahsera Auguste Tapo, Tebogo Macucwa, Vukosi Marivate, Elvis Mboning, Tajuddeen Gwadabe, Tosin P. Adewumi, Orevaoghene Ahia, Joyce NakatumbaNabende, Neo L. Mokono, Ignatius Ezeani, Chiamaka Chukwuneke, Mofetoluwa Adeyemi, Gilles Hacheme, Idris Abdulmumin, Odunayo Ogundepo, Oreen Yousuf, Tatiana Moteu Ngoli, Dietrich Klakow |  |
| 954 |  |  [Ethics consideration sections in natural language processing papers](https://doi.org/10.18653/v1/2022.emnlp-main.299) |  | 0 |  | Luciana Benotti, Patrick Blackburn |  |
| 955 |  |  [Continued Pretraining for Better Zero- and Few-Shot Promptability](https://doi.org/10.18653/v1/2022.emnlp-main.300) |  | 0 |  | Zhaofeng Wu, Robert L. Logan IV, Pete Walsh, Akshita Bhagia, Dirk Groeneveld, Sameer Singh, Iz Beltagy |  |
| 956 |  |  [Less is More: Summary of Long Instructions is Better for Program Synthesis](https://doi.org/10.18653/v1/2022.emnlp-main.301) |  | 0 |  | Kirby Kuznia, Swaroop Mishra, Mihir Parmar, Chitta Baral |  |
| 957 |  |  [Is a Question Decomposition Unit All We Need?](https://doi.org/10.18653/v1/2022.emnlp-main.302) |  | 0 |  | Pruthvi Patel, Swaroop Mishra, Mihir Parmar, Chitta Baral |  |
| 958 |  |  [Discourse-Aware Soft Prompting for Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.303) |  | 0 |  | Marjan Ghazvininejad, Vladimir Karpukhin, Vera Gor, Asli Celikyilmaz |  |
| 959 |  |  [ExPUNations: Augmenting Puns with Keywords and Explanations](https://doi.org/10.18653/v1/2022.emnlp-main.304) |  | 0 |  | Jiao Sun, Anjali NarayanChen, Shereen Oraby, Alessandra Cervone, Tagyoung Chung, Jing Huang, Yang Liu, Nanyun Peng |  |
| 960 |  |  [SLING: Sino Linguistic Evaluation of Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.305) |  | 0 |  | Yixiao Song, Kalpesh Krishna, Rajesh Bhatt, Mohit Iyyer |  |
| 961 |  |  [Context-Situated Pun Generation](https://doi.org/10.18653/v1/2022.emnlp-main.306) |  | 0 |  | Jiao Sun, Anjali NarayanChen, Shereen Oraby, Shuyang Gao, Tagyoung Chung, Jing Huang, Yang Liu, Nanyun Peng |  |
| 962 |  |  [Retrieval-Augmented Generative Question Answering for Event Argument Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.307) |  | 0 |  | Xinya Du, Heng Ji |  |
| 963 |  |  [Concadia: Towards Image-Based Text Generation with a Purpose](https://doi.org/10.18653/v1/2022.emnlp-main.308) |  | 0 |  | Elisa Kreiss, Fei Fang, Noah D. Goodman, Christopher Potts |  |
| 964 |  |  [Context Matters for Image Descriptions for Accessibility: Challenges for Referenceless Evaluation Metrics](https://doi.org/10.18653/v1/2022.emnlp-main.309) |  | 0 |  | Elisa Kreiss, Cynthia L. Bennett, Shayan Hooshmand, Eric Zelikman, Meredith Ringel Morris, Christopher Potts |  |
| 965 |  |  [MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure](https://doi.org/10.18653/v1/2022.emnlp-main.310) |  | 0 |  | Yinya Huang, Hongming Zhang, Ruixin Hong, Xiaodan Liang, Changshui Zhang, Dong Yu |  |
| 966 |  |  [Explicit Query Rewriting for Conversational Dense Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.311) |  | 0 |  | Hongjin Qian, Zhicheng Dou |  |
| 967 |  |  [Efficient Nearest Neighbor Emotion Classification with BERT-whitening](https://doi.org/10.18653/v1/2022.emnlp-main.312) |  | 0 |  | Wenbiao Yin, Lin Shang |  |
| 968 |  |  [FastClass: A Time-Efficient Approach to Weakly-Supervised Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.313) |  | 0 |  | Tingyu Xia, Yue Wang, Yuan Tian, Yi Chang |  |
| 969 |  |  [Neural-Symbolic Inference for Robust Autoregressive Graph Parsing via Compositional Uncertainty Quantification](https://doi.org/10.18653/v1/2022.emnlp-main.314) |  | 0 |  | Zi Lin, Jeremiah Z. Liu, Jingbo Shang |  |
| 970 |  |  [A Speaker-Aware Co-Attention Framework for Medical Dialogue Information Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.315) |  | 0 |  | Yuan Xia, Zhenhui Shi, Jingbo Zhou, Jiayu Xu, Chao Lu, Yehui Yang, Lei Wang, Haifeng Huang, Xia Zhang, Junwei Liu |  |
| 971 |  |  [Towards Interactivity and Interpretability: A Rationale-based Legal Judgment Prediction Framework](https://doi.org/10.18653/v1/2022.emnlp-main.316) |  | 0 |  | Yiquan Wu, Yifei Liu, Weiming Lu, Yating Zhang, Jun Feng, Changlong Sun, Fei Wu, Kun Kuang |  |
| 972 |  |  [RelCLIP: Adapting Language-Image Pretraining for Visual Relationship Detection via Relational Contrastive Learning](https://doi.org/10.18653/v1/2022.emnlp-main.317) |  | 0 |  | Yi Zhu, Zhaoqing Zhu, Bingqian Lin, Xiaodan Liang, Feng Zhao, Jianzhuang Liu |  |
| 973 |  |  [Candidate Soups: Fusing Candidate Results Improves Translation Quality for Non-Autoregressive Translation](https://doi.org/10.18653/v1/2022.emnlp-main.318) |  | 0 |  | Huanran Zheng, Wei Zhu, Pengfei Wang, Xiaoling Wang |  |
| 974 |  |  [Evaluating Parameter Efficient Learning for Generation](https://doi.org/10.18653/v1/2022.emnlp-main.319) |  | 0 |  | Peng Xu, Mostofa Patwary, Shrimai Prabhumoye, Virginia Adams, Ryan Prenger, Wei Ping, Nayeon Lee, Mohammad Shoeybi, Bryan Catanzaro |  |
| 975 |  |  [McQueen: a Benchmark for Multimodal Conversational Query Rewrite](https://doi.org/10.18653/v1/2022.emnlp-main.320) |  | 0 |  | Yifei Yuan, Chen Shi, Runze Wang, Liyi Chen, Feijun Jiang, Yuan You, Wai Lam |  |
| 976 |  |  [Self-supervised Graph Masking Pre-training for Graph-to-Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.321) |  | 0 |  | Jiuzhou Han, Ehsan Shareghi |  |
| 977 |  |  [Improving Stability of Fine-Tuning Pretrained Language Models via Component-Wise Gradient Norm Clipping](https://doi.org/10.18653/v1/2022.emnlp-main.322) |  | 0 |  | Chenghao Yang, Xuezhe Ma |  |
| 978 |  |  [Differentially Private Language Models for Secure Data Sharing](https://doi.org/10.18653/v1/2022.emnlp-main.323) |  | 0 |  | Justus Mattern, Zhijing Jin, Benjamin Weggenmann, Bernhard Schölkopf, Mrinmaya Sachan |  |
| 979 |  |  [Conditional set generation using Seq2seq models](https://doi.org/10.18653/v1/2022.emnlp-main.324) |  | 0 |  | Aman Madaan, Dheeraj Rajagopal, Niket Tandon, Yiming Yang, Antoine Bosselut |  |
| 980 |  |  [Analyzing and Evaluating Faithfulness in Dialogue Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.325) |  | 0 |  | Bin Wang, Chen Zhang, Yan Zhang, Yiming Chen, Haizhou Li |  |
| 981 |  |  [Twist Decoding: Diverse Generators Guide Each Other](https://doi.org/10.18653/v1/2022.emnlp-main.326) |  | 0 |  | Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Hao Peng, Ximing Lu, Dragomir Radev, Yejin Choi, Noah A. Smith |  |
| 982 |  |  [Exploring Representation-level Augmentation for Code Search](https://doi.org/10.18653/v1/2022.emnlp-main.327) |  | 0 |  | Haochen Li, Chunyan Miao, Cyril Leung, Yanxian Huang, Yuan Huang, Hongyu Zhang, Yanlin Wang |  |
| 983 |  |  [Learning Semantic Textual Similarity via Topic-informed Discrete Latent Variables](https://doi.org/10.18653/v1/2022.emnlp-main.328) |  | 0 |  | Erxin Yu, Lan Du, Yuan Jin, Zhepei Wei, Yi Chang |  |
| 984 |  |  [STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension](https://doi.org/10.18653/v1/2022.emnlp-main.329) |  | 0 |  | Borui Wang, Chengcheng Feng, Arjun Nair, Madelyn Mao, Jai Desai, Asli Celikyilmaz, Haoran Li, Yashar Mehdad, Dragomir Radev |  |
| 985 |  |  [Competency-Aware Neural Machine Translation: Can Machine Translation Know its Own Translation Quality?](https://doi.org/10.18653/v1/2022.emnlp-main.330) |  | 0 |  | Pei Zhang, Baosong Yang, Haoran Wei, Dayiheng Liu, Kai Fan, Luo Si, Jun Xie |  |
| 986 |  |  [PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training](https://doi.org/10.18653/v1/2022.emnlp-main.331) |  | 0 |  | Zihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiaoman Zhao, Xiaoyong Du |  |
| 987 |  |  [Sentiment-Aware Word and Sentence Level Pre-training for Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.332) |  | 0 |  | Shuai Fan, Chen Lin, Haonan Li, Zhenghao Lin, Jinsong Su, Hang Zhang, Yeyun Gong, Jian Guo, Nan Duan |  |
| 988 |  |  [Towards Multi-Modal Sarcasm Detection via Hierarchical Congruity Modeling with Knowledge Enhancement](https://doi.org/10.18653/v1/2022.emnlp-main.333) |  | 0 |  | Hui Liu, Wenya Wang, Haoliang Li |  |
| 989 |  |  [Efficiently Tuned Parameters Are Task Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.334) |  | 0 |  | Wangchunshu Zhou, Canwen Xu, Julian J. McAuley |  |
| 990 |  |  [COPEN: Probing Conceptual Knowledge in Pre-trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.335) |  | 0 |  | Hao Peng, Xiaozhi Wang, Shengding Hu, Hailong Jin, Lei Hou, Juanzi Li, Zhiyuan Liu, Qun Liu |  |
| 991 |  |  [Capturing Global Structural Information in Long Document Question Answering with Compressive Graph Selector Network](https://doi.org/10.18653/v1/2022.emnlp-main.336) |  | 0 |  | Yuxiang Nie, Heyan Huang, Wei Wei, XianLing Mao |  |
| 992 |  |  [Structural generalization is hard for sequence-to-sequence models](https://doi.org/10.18653/v1/2022.emnlp-main.337) |  | 0 |  | Yuekun Yao, Alexander Koller |  |
| 993 |  |  [Contrastive Learning enhanced Author-Style Headline Generation](https://doi.org/10.18653/v1/2022.emnlp-main.338) |  | 0 |  | Hui Liu, Weidong Guo, Yige Chen, Xiangyang Li |  |
| 994 |  |  [Multi-Granularity Optimization for Non-Autoregressive Translation](https://doi.org/10.18653/v1/2022.emnlp-main.339) |  | 0 |  | Yafu Li, Leyang Cui, Yongjing Yin, Yue Zhang |  |
| 995 |  |  [Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.340) |  | 0 |  | Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, Xudong Shen |  |
| 996 |  |  [MetaFill: Text Infilling for Meta-Path Generation on Heterogeneous Information Networks](https://doi.org/10.18653/v1/2022.emnlp-main.341) |  | 0 |  | Zequn Liu, Kefei Duan, Junwei Yang, Hanwen Xu, Ming Zhang, Sheng Wang |  |
| 997 |  |  [DRLK: Dynamic Hierarchical Reasoning with Language Model and Knowledge Graph for Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.342) |  | 0 |  | Miao Zhang, Rufeng Dai, Ming Dong, Tingting He |  |
| 998 |  |  [AEG: Argumentative Essay Generation via A Dual-Decoder Model with Content Planning](https://doi.org/10.18653/v1/2022.emnlp-main.343) |  | 0 |  | Jianzhu Bao, Yasheng Wang, Yitong Li, Fei Mi, Ruifeng Xu |  |
| 999 |  |  [BotsTalk: Machine-sourced Framework for Automatic Curation of Large-scale Multi-skill Dialogue Datasets](https://doi.org/10.18653/v1/2022.emnlp-main.344) |  | 0 |  | Minju Kim, Chaehyeong Kim, Yongho Song, Seungwon Hwang, Jinyoung Yeo |  |
| 1000 |  |  [Wider & Closer: Mixture of Short-channel Distillers for Zero-shot Cross-lingual Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.345) |  | 0 |  | JunYu Ma, Beiduo Chen, JiaChen Gu, Zhenhua Ling, Wu Guo, Quan Liu, Zhigang Chen, Cong Liu |  |
| 1001 |  |  [An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.346) |  | 0 |  | Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel |  |
| 1002 |  |  [Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation](https://doi.org/10.18653/v1/2022.emnlp-main.347) |  | 0 |  | Xiaohui Song, Longtao Huang, Hui Xue, Songlin Hu |  |
| 1003 |  |  [RuCoLA: Russian Corpus of Linguistic Acceptability](https://doi.org/10.18653/v1/2022.emnlp-main.348) |  | 0 |  | Vladislav Mikhailov, Tatiana Shamardina, Max Ryabinin, Alena Pestova, Ivan Smurov, Ekaterina Artemova |  |
| 1004 |  |  [Complex Hyperbolic Knowledge Graph Embeddings with Fast Fourier Transform](https://doi.org/10.18653/v1/2022.emnlp-main.349) |  | 0 |  | Huiru Xiao, Xin Liu, Yangqiu Song, Ginny Y. Wong, Simon See |  |
| 1005 |  |  [Towards Knowledge-Intensive Text-to-SQL Semantic Parsing with Formulaic Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.350) |  | 0 |  | Longxu Dou, Yan Gao, Xuqi Liu, Mingyang Pan, Dingzirui Wang, Wanxiang Che, Dechen Zhan, MinYen Kan, JianGuang Lou |  |
| 1006 |  |  [Should We Ban English NLP for a Year?](https://doi.org/10.18653/v1/2022.emnlp-main.351) |  | 0 |  | Anders Søgaard |  |
| 1007 |  |  [LittleBird: Efficient Faster & Longer Transformer for Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.352) |  | 0 |  | Minchul Lee, Kijong Han, Myeong Cheol Shin |  |
| 1008 |  |  [WeTS: A Benchmark for Translation Suggestion](https://doi.org/10.18653/v1/2022.emnlp-main.353) |  | 0 |  | Zhen Yang, Fandong Meng, Yingxue Zhang, Ernan Li, Jie Zhou |  |
| 1009 |  |  [Discrete Cross-Modal Alignment Enables Zero-Shot Speech Translation](https://doi.org/10.18653/v1/2022.emnlp-main.354) |  | 0 |  | Chen Wang, Yuchen Liu, Boxing Chen, Jiajun Zhang, Wei Luo, Zhongqiang Huang, Chengqing Zong |  |
| 1010 |  |  [Abstractive Summarization Guided by Latent Hierarchical Document Structure](https://doi.org/10.18653/v1/2022.emnlp-main.355) |  | 0 |  | Yifu Qiu, Shay B. Cohen |  |
| 1011 |  |  [Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.356) |  | 0 |  | Jianguo Mao, Wenbin Jiang, Xiangdong Wang, Hong Liu, Yu Xia, Yajuan Lyu, Qiaoqiao She |  |
| 1012 |  |  [DuReader-Retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine](https://doi.org/10.18653/v1/2022.emnlp-main.357) |  | 0 |  | Yifu Qiu, Hongyu Li, Yingqi Qu, Ying Chen, Qiaoqiao She, Jing Liu, Hua Wu, Haifeng Wang |  |
| 1013 |  |  [Pair-Based Joint Encoding with Relational Graph Convolutional Networks for Emotion-Cause Pair Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.358) |  | 0 |  | Junlong Liu, Xichen Shang, Qianli Ma |  |
| 1014 |  |  [Affective Knowledge Enhanced Multiple-Graph Fusion Networks for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.359) |  | 0 |  | Siyu Tang, Heyan Chai, Ziyi Yao, Ye Ding, Cuiyun Gao, Binxing Fang, Qing Liao |  |
| 1015 |  |  [IndicNLG Benchmark: Multilingual Datasets for Diverse NLG Tasks in Indic Languages](https://doi.org/10.18653/v1/2022.emnlp-main.360) |  | 0 |  | Aman Kumar, Himani Shrotriya, Prachi Sahu, Amogh Mishra, Raj Dabre, Ratish Puduppully, Anoop Kunchukuttan, Mitesh M. Khapra, Pratyush Kumar |  |
| 1016 |  |  [Improving Machine Translation with Phrase Pair Injection and Corpus Filtering](https://doi.org/10.18653/v1/2022.emnlp-main.361) |  | 0 |  | Akshay Batheja, Pushpak Bhattacharyya |  |
| 1017 |  |  [An Anchor-based Relative Position Embedding Method for Cross-Modal Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.362) |  | 0 |  | Ya Wang, Xingwu Sun, Fengzong Lian, Zhanhui Kang, Chengzhong Xu |  |
| 1018 |  |  [Norm-based Noisy Corpora Filtering and Refurbishing in Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.363) |  | 0 |  | Yu Lu, Jiajun Zhang |  |
| 1019 |  |  [TeleMelody: Lyric-to-Melody Generation with a Template-Based Two-Stage Method](https://doi.org/10.18653/v1/2022.emnlp-main.364) |  | 0 |  | Zeqian Ju, Peiling Lu, Xu Tan, Rui Wang, Chen Zhang, Songruoyao Wu, Kejun Zhang, XiangYang Li, Tao Qin, TieYan Liu |  |
| 1020 |  |  [SEEN: Structured Event Enhancement Network for Explainable Need Detection of Information Recall Assistance](https://doi.org/10.18653/v1/2022.emnlp-main.365) |  | 0 |  | YouEn Lin, AnZi Yen, HenHsen Huang, HsinHsi Chen |  |
| 1021 |  |  [Rethinking Style Transformer with Energy-based Interpretation: Adversarial Unsupervised Style Transfer using a Pretrained Model](https://doi.org/10.18653/v1/2022.emnlp-main.366) |  | 0 |  | Hojun Cho, Dohee Kim, Seungwoo Ryu, ChaeHun Park, Hyungjong Noh, JeongIn Hwang, Minseok Choi, Edward Choi, Jaegul Choo |  |
| 1022 |  |  [Towards Robust k-Nearest-Neighbor Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.367) |  | 0 |  | Hui Jiang, Ziyao Lu, Fandong Meng, Chulun Zhou, Jie Zhou, Degen Huang, Jinsong Su |  |
| 1023 |  |  [Tiny-NewsRec: Effective and Efficient PLM-based News Recommendation](https://doi.org/10.18653/v1/2022.emnlp-main.368) |  | 0 |  | Yang Yu, Fangzhao Wu, Chuhan Wu, Jingwei Yi, Qi Liu |  |
| 1024 |  |  [TABS: Efficient Textual Adversarial Attack for Pre-trained NL Code Model Using Semantic Beam Search](https://doi.org/10.18653/v1/2022.emnlp-main.369) |  | 0 |  | YunSeok Choi, Hyojun Kim, JeeHyong Lee |  |
| 1025 |  |  [Investigating the Robustness of Natural Language Generation from Logical Forms via Counterfactual Samples](https://doi.org/10.18653/v1/2022.emnlp-main.370) |  | 0 |  | Chengyuan Liu, Leilei Gan, Kun Kuang, Fei Wu |  |
| 1026 |  |  [Helping the Weak Makes You Strong: Simple Multi-Task Learning Improves Non-Autoregressive Translators](https://doi.org/10.18653/v1/2022.emnlp-main.371) |  | 0 |  | Xinyou Wang, Zaixiang Zheng, Shujian Huang |  |
| 1027 |  |  [RACE: Retrieval-augmented Commit Message Generation](https://doi.org/10.18653/v1/2022.emnlp-main.372) |  | 0 |  | Ensheng Shi, Yanlin Wang, Wei Tao, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Hongbin Sun |  |
| 1028 |  |  [PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.373) |  | 0 |  | Ao Liu, Haoyu Dong, Naoaki Okazaki, Shi Han, Dongmei Zhang |  |
| 1029 |  |  [GHAN: Graph-Based Hierarchical Aggregation Network for Text-Video Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.374) |  | 0 |  | Yahan Yu, Bojie Hu, Yu Li |  |
| 1030 |  |  [MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text](https://doi.org/10.18653/v1/2022.emnlp-main.375) |  | 0 |  | Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen |  |
| 1031 |  |  [PHEE: A Dataset for Pharmacovigilance Event Extraction from Text](https://doi.org/10.18653/v1/2022.emnlp-main.376) |  | 0 |  | Zhaoyue Sun, Jiazheng Li, Gabriele Pergola, Byron C. Wallace, Bino John, Nigel Greene, Joseph Kim, Yulan He |  |
| 1032 |  |  [OTSeq2Set: An Optimal Transport Enhanced Sequence-to-Set Model for Extreme Multi-label Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.377) |  | 0 |  | Jie Cao, Yin Zhang |  |
| 1033 |  |  [SimQA: Detecting Simultaneous MT Errors through Word-by-Word Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.378) |  | 0 |  | HyoJung Han, Marine Carpuat, Jordan L. BoydGraber |  |
| 1034 |  |  [Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations](https://doi.org/10.18653/v1/2022.emnlp-main.379) |  | 0 |  | Zhihui Xie, Handong Zhao, Tong Yu, Shuai Li |  |
| 1035 |  |  [Rethinking the Authorship Verification Experimental Setups](https://doi.org/10.18653/v1/2022.emnlp-main.380) |  | 0 |  | Florin Brad, Andrei Manolache, Elena Burceanu, Antonio Barbalau, Radu Tudor Ionescu, Marius Popescu |  |
| 1036 |  |  [Borrowing Human Senses: Comment-Aware Self-Training for Social Media Multimodal Classification](https://doi.org/10.18653/v1/2022.emnlp-main.381) |  | 0 |  | Chunpu Xu, Jing Li |  |
| 1037 |  |  [Training Language Models with Memory Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.382) |  | 0 |  | Zexuan Zhong, Tao Lei, Danqi Chen |  |
| 1038 |  |  [Data-Efficient Strategies for Expanding Hate Speech Detection into Under-Resourced Languages](https://doi.org/10.18653/v1/2022.emnlp-main.383) |  | 0 |  | Paul Röttger, Debora Nozza, Federico Bianchi, Dirk Hovy |  |
| 1039 |  |  [Dimension Reduction for Efficient Dense Retrieval via Conditional Autoencoder](https://doi.org/10.18653/v1/2022.emnlp-main.384) |  | 0 |  | Zhenghao Liu, Han Zhang, Chenyan Xiong, Zhiyuan Liu, Yu Gu, Xiaohua Li |  |
| 1040 |  |  [Controlled Text Reduction](https://doi.org/10.18653/v1/2022.emnlp-main.385) |  | 0 |  | Aviv Slobodkin, Paul Roit, Eran Hirsch, Ori Ernst, Ido Dagan |  |
| 1041 |  |  [Questioning the Validity of Summarization Datasets and Improving Their Factual Consistency](https://doi.org/10.18653/v1/2022.emnlp-main.386) |  | 0 |  | Yanzhu Guo, Chloé Clavel, Moussa Kamal Eddine, Michalis Vazirgiannis |  |
| 1042 |  |  [Invariant Language Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.387) |  | 0 |  | Maxime Peyrard, Sarvjeet Singh Ghotra, Martin Josifoski, Vidhan Agarwal, Barun Patra, Dean Carignan, Emre Kiciman, Saurabh Tiwary, Robert West |  |
| 1043 |  |  [AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning](https://doi.org/10.18653/v1/2022.emnlp-main.388) |  | 0 |  | Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, Jianfeng Gao |  |
| 1044 |  |  [How "Multi" is Multi-Document Summarization?](https://doi.org/10.18653/v1/2022.emnlp-main.389) |  | 0 |  | Ruben Wolhandler, Arie Cattan, Ori Ernst, Ido Dagan |  |
| 1045 |  |  [BioReader: a Retrieval-Enhanced Text-to-Text Transformer for Biomedical Literature](https://doi.org/10.18653/v1/2022.emnlp-main.390) |  | 0 |  | Giacomo Frisoni, Miki Mizutani, Gianluca Moro, Lorenzo Valgimigli |  |
| 1046 |  |  [T-Modules: Translation Modules for Zero-Shot Cross-Modal Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.391) |  | 0 |  | PaulAmbroise Duquenne, Hongyu Gong, Benoît Sagot, Holger Schwenk |  |
| 1047 |  |  [LILA: A Unified Benchmark for Mathematical Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.392) |  | 0 |  | Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin Kalyan |  |
| 1048 |  |  [Leveraging Affirmative Interpretations from Negation Improves Natural Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.393) |  | 0 |  | Md Mosharaf Hossain, Eduardo Blanco |  |
| 1049 |  |  [GraphQ IR: Unifying the Semantic Parsing of Graph Query Languages with One Intermediate Representation](https://doi.org/10.18653/v1/2022.emnlp-main.394) |  | 0 |  | Lunyiu Nie, Shulin Cao, Jiaxin Shi, Jiuding Sun, Qi Tian, Lei Hou, Juanzi Li, Jidong Zhai |  |
| 1050 |  |  [InforMask: Unsupervised Informative Masking for Language Model Pretraining](https://doi.org/10.18653/v1/2022.emnlp-main.395) |  | 0 |  | Nafis Sadeq, Canwen Xu, Julian J. McAuley |  |
| 1051 |  |  [CTRLsum: Towards Generic Controllable Text Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.396) |  | 0 |  | Junxian He, Wojciech Kryscinski, Bryan McCann, Nazneen Rajani, Caiming Xiong |  |
| 1052 |  |  [Missing Counter-Evidence Renders NLP Fact-Checking Unrealistic for Misinformation](https://doi.org/10.18653/v1/2022.emnlp-main.397) |  | 0 |  | Max Glockner, Yufang Hou, Iryna Gurevych |  |
| 1053 |  |  [A Framework for Adapting Pre-Trained Language Models to Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.emnlp-main.398) |  | 0 |  | Justin Lovelace, Carolyn P. Rosé |  |
| 1054 |  |  [Mutual Information Alleviates Hallucinations in Abstractive Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.399) |  | 0 |  | Liam van der Poel, Ryan Cotterell, Clara Meister |  |
| 1055 |  |  [Toward the Limitation of Code-Switching in Cross-Lingual Transfer](https://doi.org/10.18653/v1/2022.emnlp-main.400) |  | 0 |  | Yukun Feng, Feng Li, Philipp Koehn |  |
| 1056 |  |  [Syntactically Rich Discriminative Training: An Effective Method for Open Information Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.401) |  | 0 |  | Frank Mtumbuka, Thomas Lukasiewicz |  |
| 1057 |  |  [Transformer-based Entity Typing in Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.402) |  | 0 |  | Zhiwei Hu, Víctor GutiérrezBasulto, Zhiliang Xiang, Ru Li, Jeff Z. Pan |  |
| 1058 |  |  [NewsClaims: A New Benchmark for Claim Detection from News with Attribute Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.403) |  | 0 |  | Revanth Gangi Reddy, Sai Chetan Chinthakindi, Zhenhailong Wang, Yi R. Fung, Kathryn Conger, Ahmed Elsayed, Martha Palmer, Preslav Nakov, Eduard H. Hovy, Kevin Small, Heng Ji |  |
| 1059 |  |  [IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces](https://doi.org/10.18653/v1/2022.emnlp-main.404) |  | 0 |  | Kelly Marchisio, Neha Verma, Kevin Duh, Philipp Koehn |  |
| 1060 |  |  [Adversarial Concept Erasure in Kernel Space](https://doi.org/10.18653/v1/2022.emnlp-main.405) |  | 0 |  | Shauli Ravfogel, Francisco Vargas, Yoav Goldberg, Ryan Cotterell |  |
| 1061 |  |  [The Authenticity Gap in Human Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.406) |  | 0 |  | Kawin Ethayarajh, Dan Jurafsky |  |
| 1062 |  |  [BERT in Plutarch's Shadows](https://doi.org/10.18653/v1/2022.emnlp-main.407) |  | 0 |  | Ivan P. Yamshchikov, Alexey Tikhonov, Yorgos Pantis, Charlotte Schubert, Jürgen Jost |  |
| 1063 |  |  [Leveraging Locality in Abstractive Text Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.408) |  | 0 |  | Yixin Liu, Ansong Ni, Linyong Nan, Budhaditya Deb, Chenguang Zhu, Ahmed Hassan Awadallah, Dragomir Radev |  |
| 1064 |  |  [Salience Allocation as Guidance for Abstractive Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.409) |  | 0 |  | Fei Wang, Kaiqiang Song, Hongming Zhang, Lifeng Jin, Sangwoo Cho, Wenlin Yao, Xiaoyang Wang, Muhao Chen, Dong Yu |  |
| 1065 |  |  [Fine-tuned Language Models are Continual Learners](https://doi.org/10.18653/v1/2022.emnlp-main.410) |  | 0 |  | Thomas Scialom, Tuhin Chakrabarty, Smaranda Muresan |  |
| 1066 |  |  [Natural Logic-guided Autoregressive Multi-hop Document Retrieval for Fact Verification](https://doi.org/10.18653/v1/2022.emnlp-main.411) |  | 0 |  | Rami Aly, Andreas Vlachos |  |
| 1067 |  |  [AX-MABSA: A Framework for Extremely Weakly Supervised Multi-label Aspect Based Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.412) |  | 0 |  | Sabyasachi Kamila, Walid Magdy, Sourav Dutta, MingXue Wang |  |
| 1068 |  |  [Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.413) |  | 0 |  | Roshanak Mirzaee, Parisa Kordjamshidi |  |
| 1069 |  |  [A Survey of Active Learning for Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-main.414) |  | 0 |  | Zhisong Zhang, Emma Strubell, Eduard H. Hovy |  |
| 1070 |  |  [Bernice: A Multilingual Pre-trained Encoder for Twitter](https://doi.org/10.18653/v1/2022.emnlp-main.415) |  | 0 |  | Alexandra DeLucia, Shijie Wu, Aaron Mueller, Carlos Alejandro Aguirre, Philip Resnik, Mark Dredze |  |
| 1071 |  |  [CEFR-Based Sentence Difficulty Annotation and Assessment](https://doi.org/10.18653/v1/2022.emnlp-main.416) |  | 0 |  | Yuki Arase, Satoru Uchida, Tomoyuki Kajiwara |  |
| 1072 |  |  [Simple Questions Generate Named Entity Recognition Datasets](https://doi.org/10.18653/v1/2022.emnlp-main.417) |  | 0 |  | Hyunjae Kim, Jaehyo Yoo, Seunghyun Yoon, Jinhyuk Lee, Jaewoo Kang |  |
| 1073 |  |  [TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.418) |  | 0 |  | Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Minjoon Seo |  |
| 1074 |  |  [Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.419) |  | 0 |  | Lu Dai, Bang Wang, Wei Xiang, Yijun Mo |  |
| 1075 |  |  [Learning Robust Representations for Continual Relation Extraction via Adversarial Class Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.420) |  | 0 |  | Peiyi Wang, Yifan Song, Tianyu Liu, Binghuai Lin, Yunbo Cao, Sujian Li, Zhifang Sui |  |
| 1076 |  |  [ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.421) |  | 0 |  | Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, William Yang Wang |  |
| 1077 |  |  [A Span-based Multimodal Variational Autoencoder for Semi-supervised Multimodal Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.422) |  | 0 |  | Baohang Zhou, Ying Zhang, Kehui Song, Wenya Guo, Guoqing Zhao, Hongbin Wang, Xiaojie Yuan |  |
| 1078 |  |  [R-TeaFor: Regularized Teacher-Forcing for Abstractive Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.423) |  | 0 |  | GuanYu Lin, PuJen Cheng |  |
| 1079 |  |  [Modeling Consistency Preference via Lexical Chains for Document-level Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.424) |  | 0 |  | Xinglin Lyu, Junhui Li, Shimin Tao, Hao Yang, Ying Qin, Min Zhang |  |
| 1080 |  |  [Just Fine-tune Twice: Selective Differential Privacy for Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.425) |  | 0 |  | Weiyan Shi, Ryan Shea, Si Chen, Chiyuan Zhang, Ruoxi Jia, Zhou Yu |  |
| 1081 |  |  [Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents](https://doi.org/10.18653/v1/2022.emnlp-main.426) |  | 0 |  | Marcio Fonseca, Yftah Ziser, Shay B. Cohen |  |
| 1082 |  |  [Open-Domain Sign Language Translation Learned from Online Video](https://doi.org/10.18653/v1/2022.emnlp-main.427) |  | 0 |  | Bowen Shi, Diane Brentari, Gregory Shakhnarovich, Karen Livescu |  |
| 1083 |  |  [Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change](https://doi.org/10.18653/v1/2022.emnlp-main.428) |  | 0 |  | Zhaochen Su, Zecheng Tang, Xinyan Guan, Lijun Wu, Min Zhang, Juntao Li |  |
| 1084 |  |  [ULN: Towards Underspecified Vision-and-Language Navigation](https://doi.org/10.18653/v1/2022.emnlp-main.429) |  | 0 |  | Weixi Feng, TsuJui Fu, Yujie Lu, William Yang Wang |  |
| 1085 |  |  [Federated Model Decomposition with Private Vocabulary for Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.430) |  | 0 |  | Zhuo Zhang, Xiangjing Hu, Lizhen Qu, Qifan Wang, Zenglin Xu |  |
| 1086 |  |  [ReCo: Reliable Causal Chain Reasoning via Structural Causal Recurrent Neural Networks](https://doi.org/10.18653/v1/2022.emnlp-main.431) |  | 0 |  | Kai Xiong, Xiao Ding, Zhongyang Li, Li Du, Ting Liu, Bing Qin, Yi Zheng, Baoxing Huai |  |
| 1087 |  |  [Video Question Answering: Datasets, Algorithms and Challenges](https://doi.org/10.18653/v1/2022.emnlp-main.432) |  | 0 |  | Yaoyao Zhong, Wei Ji, Junbin Xiao, Yicong Li, Weihong Deng, TatSeng Chua |  |
| 1088 |  |  [Retrofitting Multilingual Sentence Embeddings with Abstract Meaning Representation](https://doi.org/10.18653/v1/2022.emnlp-main.433) |  | 0 |  | Deng Cai, Xin Li, Jackie ChunSing Ho, Lidong Bing, Wai Lam |  |
| 1089 |  |  [Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.434) |  | 0 |  | Zhijun Wang, Xuebo Liu, Min Zhang |  |
| 1090 |  |  [Boundary-Driven Table-Filling for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.435) |  | 0 |  | Yice Zhang, Yifan Yang, Yihui Li, Bin Liang, Shiwei Chen, Yixue Dang, Min Yang, Ruifeng Xu |  |
| 1091 |  |  [Attention and Edge-Label Guided Graph Convolutional Networks for Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.436) |  | 0 |  | Renjie Zhou, Zhongyi Xie, Jian Wan, Jilin Zhang, Yong Liao, Qiang Liu |  |
| 1092 |  |  [Title2Event: Benchmarking Open Event Extraction with a Large-scale Chinese Title Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.437) |  | 0 |  | Haolin Deng, Yanan Zhang, Yangfan Zhang, Wangyang Ying, Changlong Yu, Jun Gao, Wei Wang, Xiaoling Bai, Nan Yang, Jin Ma, Xiang Chen, Tianhua Zhou |  |
| 1093 |  |  [Cascading Biases: Investigating the Effect of Heuristic Annotation Strategies on Data and Models](https://doi.org/10.18653/v1/2022.emnlp-main.438) |  | 0 |  | Chaitanya Malaviya, Sudeep Bhatia, Mark Yatskar |  |
| 1094 |  |  [Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts](https://doi.org/10.18653/v1/2022.emnlp-main.439) |  | 0 |  | Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal |  |
| 1095 |  |  [ADDMU: Detection of Far-Boundary Adversarial Examples with Data and Model Uncertainty Estimation](https://doi.org/10.18653/v1/2022.emnlp-main.440) |  | 0 |  | Fan Yin, Yao Li, ChoJui Hsieh, KaiWei Chang |  |
| 1096 |  |  [G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.441) |  | 0 |  | Zhongwei Wan, Yichun Yin, Wei Zhang, Jiaxin Shi, Lifeng Shang, Guangyong Chen, Xin Jiang, Qun Liu |  |
| 1097 |  |  [Towards Unifying Reference Expression Generation and Comprehension](https://doi.org/10.18653/v1/2022.emnlp-main.442) |  | 0 |  | Duo Zheng, Tao Kong, Ya Jing, Jiaan Wang, Xiaojie Wang |  |
| 1098 |  |  [Textual Manifold-based Defense Against Natural Language Adversarial Examples](https://doi.org/10.18653/v1/2022.emnlp-main.443) |  | 0 |  | Dang Nguyen Minh, Anh Tuan Luu |  |
| 1099 |  |  [Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters](https://doi.org/10.18653/v1/2022.emnlp-main.444) |  | 0 |  | Hongyu Zhao, Hao Tan, Hongyuan Mei |  |
| 1100 |  |  [Reduce Catastrophic Forgetting of Dense Retrieval Training with Teleportation Negatives](https://doi.org/10.18653/v1/2022.emnlp-main.445) |  | 0 |  | Si Sun, Chenyan Xiong, Yue Yu, Arnold Overwijk, Zhiyuan Liu, Jie Bao |  |
| 1101 |  |  [ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts](https://doi.org/10.18653/v1/2022.emnlp-main.446) |  | 0 |  | Akari Asai, Mohammadreza Salehi, Matthew E. Peters, Hannaneh Hajishirzi |  |
| 1102 |  |  [Exploration of the Usage of Color Terms by Color-blind Participants in Online Discussion Platforms](https://doi.org/10.18653/v1/2022.emnlp-main.447) |  | 0 |  | Ella Rabinovich, Boaz Carmeli |  |
| 1103 |  |  [DEER: Descriptive Knowledge Graph for Explaining Entity Relationships](https://doi.org/10.18653/v1/2022.emnlp-main.448) |  | 0 |  | Jie Huang, Kerui Zhu, Kevin ChenChuan Chang, Jinjun Xiong, WenMei Hwu |  |
| 1104 |  |  [META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI](https://doi.org/10.18653/v1/2022.emnlp-main.449) |  | 0 |  | Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, Kai Yu |  |
| 1105 |  |  [Understanding and Improving Knowledge Distillation for Quantization Aware Training of Large Transformer Encoders](https://doi.org/10.18653/v1/2022.emnlp-main.450) |  | 0 |  | Minsoo Kim, Sihwa Lee, Sukjin Hong, DuSeong Chang, Jungwook Choi |  |
| 1106 |  |  [Exploring Mode Connectivity for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.451) |  | 0 |  | Yujia Qin, Cheng Qian, Jing Yi, Weize Chen, Yankai Lin, Xu Han, Zhiyuan Liu, Maosong Sun, Jie Zhou |  |
| 1107 |  |  [Synergy with Translation Artifacts for Training and Inference in Multilingual Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.452) |  | 0 |  | Jaehoon Oh, Jongwoo Ko, SeYoung Yun |  |
| 1108 |  |  [Increasing Visual Awareness in Multimodal Neural Machine Translation from an Information Theoretic Perspective](https://doi.org/10.18653/v1/2022.emnlp-main.453) |  | 0 |  | Baijun Ji, Tong Zhang, Yicheng Zou, Bojie Hu, Si Shen |  |
| 1109 |  |  [Improving Event Coreference Resolution Using Document-level and Topic-level Information](https://doi.org/10.18653/v1/2022.emnlp-main.454) |  | 0 |  | Sheng Xu, Peifeng Li, Qiaoming Zhu |  |
| 1110 |  |  [Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.455) |  | 0 |  | Rishabh Bhardwaj, Amrita Saha, Steven C. H. Hoi, Soujanya Poria |  |
| 1111 |  |  [Boosting Natural Language Generation from Instructions with Meta-Learning](https://doi.org/10.18653/v1/2022.emnlp-main.456) |  | 0 |  | Budhaditya Deb, Ahmed Hassan Awadallah, Guoqing Zheng |  |
| 1112 |  |  [Topical Segmentation of Spoken Narratives: A Test Case on Holocaust Survivor Testimonies](https://doi.org/10.18653/v1/2022.emnlp-main.457) |  | 0 |  | Eitan Wagner, Renana Keydar, Amit Pinchevski, Omri Abend |  |
| 1113 |  |  [Unifying the Convergences in Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.458) |  | 0 |  | YiChong Huang, Xiaocheng Feng, Xinwei Geng, Bing Qin |  |
| 1114 |  |  [Modeling Label Correlations for Ultra-Fine Entity Typing with Neural Pairwise Conditional Random Field](https://doi.org/10.18653/v1/2022.emnlp-main.459) |  | 0 |  | Chengyue Jiang, Yong Jiang, Weiqi Wu, Pengjun Xie, Kewei Tu |  |
| 1115 |  |  [Help me write a Poem - Instruction Tuning as a Vehicle for Collaborative Poetry Writing](https://doi.org/10.18653/v1/2022.emnlp-main.460) |  | 0 |  | Tuhin Chakrabarty, Vishakh Padmakumar, He He |  |
| 1116 |  |  [Open Relation and Event Type Discovery with Type Abstraction](https://doi.org/10.18653/v1/2022.emnlp-main.461) |  | 0 |  | Sha Li, Heng Ji, Jiawei Han |  |
| 1117 |  |  [Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples](https://doi.org/10.18653/v1/2022.emnlp-main.462) |  | 0 |  | Linlin Liu, Xin Li, Ruidan He, Lidong Bing, Shafiq R. Joty, Luo Si |  |
| 1118 |  |  [Revisiting Grammatical Error Correction Evaluation and Beyond](https://doi.org/10.18653/v1/2022.emnlp-main.463) |  | 0 |  | Peiyuan Gong, Xuebo Liu, Heyan Huang, Min Zhang |  |
| 1119 |  |  [R2D2: Robust Data-to-Text with Replacement Detection](https://doi.org/10.18653/v1/2022.emnlp-main.464) |  | 0 |  | Linyong Nan, Lorenzo Jaime Yu Flores, Yilun Zhao, Yixin Liu, Luke Benson, Weijin Zou, Dragomir Radev |  |
| 1120 |  |  [IDK-MRC: Unanswerable Questions for Indonesian Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.emnlp-main.465) |  | 0 |  | Rifki Afina Putri, Alice Oh |  |
| 1121 |  |  [XLM-D: Decorate Cross-lingual Pre-training Model as Non-Autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.466) |  | 0 |  | Yong Wang, Shilin He, Guanhua Chen, Yun Chen, Daxin Jiang |  |
| 1122 |  |  [Cross-stitching Text and Knowledge Graph Encoders for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.467) |  | 0 |  | Qin Dai, Benjamin Heinzerling, Kentaro Inui |  |
| 1123 |  |  [Assist Non-native Viewers: Multimodal Cross-Lingual Summarization for How2 Videos](https://doi.org/10.18653/v1/2022.emnlp-main.468) |  | 0 |  | Nayu Liu, Kaiwen Wei, Xian Sun, Hongfeng Yu, Fanglong Yao, Li Jin, Zhi Guo, Guangluan Xu |  |
| 1124 |  |  [PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance](https://doi.org/10.18653/v1/2022.emnlp-main.469) |  | 0 |  | Yang Deng, Wenqiang Lei, Wenxuan Zhang, Wai Lam, TatSeng Chua |  |
| 1125 |  |  [Generative Data Augmentation with Contrastive Learning for Zero-Shot Stance Detection](https://doi.org/10.18653/v1/2022.emnlp-main.470) |  | 0 |  | Yang Li, Jiawei Yuan |  |
| 1126 |  |  [Better Few-Shot Relation Extraction with Label Prompt Dropout](https://doi.org/10.18653/v1/2022.emnlp-main.471) |  | 0 |  | Peiyuan Zhang, Wei Lu |  |
| 1127 |  |  [Break it Down into BTS: Basic, Tiniest Subword Units for Korean](https://doi.org/10.18653/v1/2022.emnlp-main.472) |  | 0 |  | Nayeon Kim, JunHyung Park, JoonYoung Choi, Eojin Jeon, Youjin Kang, SangKeun Lee |  |
| 1128 |  |  [The Devil in Linear Transformer](https://doi.org/10.18653/v1/2022.emnlp-main.473) |  | 0 |  | Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, Yiran Zhong |  |
| 1129 |  |  [Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective](https://doi.org/10.18653/v1/2022.emnlp-main.474) |  | 0 |  | Ping Yang, Junjie Wang, Ruyi Gan, Xinyu Zhu, Lin Zhang, Ziwei Wu, Xinyu Gao, Jiaxing Zhang, Tetsuya Sakai |  |
| 1130 |  |  [Hypoformer: Hybrid Decomposition Transformer for Edge-friendly Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.475) |  | 0 |  | Sunzhu Li, Peng Zhang, Guobing Gan, Xiuqing Lv, Benyou Wang, Victor Junqiu Wei, Xin Jiang |  |
| 1131 |  |  [FigMemes: A Dataset for Figurative Language Identification in Politically-Opinionated Memes](https://doi.org/10.18653/v1/2022.emnlp-main.476) |  | 0 |  | Chen Liu, Gregor Geigle, Robin Krebs, Iryna Gurevych |  |
| 1132 |  |  [UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.477) |  | 0 |  | Wei Tang, Benfeng Xu, Yuyue Zhao, Zhendong Mao, Yifeng Liu, Yong Liao, Haiyong Xie |  |
| 1133 |  |  [X-FACTOR: A Cross-metric Evaluation of Factual Correctness in Abstractive Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.478) |  | 0 |  | Subhajit Chaudhury, Sarathkrishna Swaminathan, R. Chulaka Gunasekara, Maxwell Crouse, Srinivas Ravishankar, Daiki Kimura, Keerthiram Murugesan, Ramón Fernandez Astudillo, Tahira Naseem, Pavan Kapanipathi, Alexander Gray |  |
| 1134 |  |  [ParaTag: A Dataset of Paraphrase Tagging for Fine-Grained Labels, NLG Evaluation, and Data Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.479) |  | 0 |  | Shuohang Wang, Ruochen Xu, Yang Liu, Chenguang Zhu, Michael Zeng |  |
| 1135 |  |  [Factual Accuracy is not Enough: Planning Consistent Description Order for Radiology Report Generation](https://doi.org/10.18653/v1/2022.emnlp-main.480) |  | 0 |  | Toru Nishino, Yasuhide Miura, Tomoki Taniguchi, Tomoko Ohkuma, Yuki Suzuki, Shoji Kido, Noriyuki Tomiyama |  |
| 1136 |  |  [FLUTE: Figurative Language Understanding through Textual Explanations](https://doi.org/10.18653/v1/2022.emnlp-main.481) |  | 0 |  | Tuhin Chakrabarty, Arkadiy Saakyan, Debanjan Ghosh, Smaranda Muresan |  |
| 1137 |  |  [Precisely the Point: Adversarial Augmentations for Faithful and Informative Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.482) |  | 0 |  | Wenhao Wu, Wei Li, Jiachen Liu, Xinyan Xiao, Sujian Li, Yajuan Lyu |  |
| 1138 |  |  [RLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees](https://doi.org/10.18653/v1/2022.emnlp-main.483) |  | 0 |  | Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang |  |
| 1139 |  |  [Let the CAT out of the bag: Contrastive Attributed explanations for Text](https://doi.org/10.18653/v1/2022.emnlp-main.484) |  | 0 |  | Saneem A. Chemmengath, Amar Prakash Azad, Ronny Luss, Amit Dhurandhar |  |
| 1140 |  |  [monoQA: Multi-Task Learning of Reranking and Answer Extraction for Open-Retrieval Conversational Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.485) |  | 0 |  | Sarawoot Kongyoung, Craig Macdonald, Iadh Ounis |  |
| 1141 |  |  [Composing Ci with Reinforced Non-autoregressive Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.486) |  | 0 |  | Yan Song |  |
| 1142 |  |  [MetaTKG: Learning Evolutionary Meta-Knowledge for Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2022.emnlp-main.487) |  | 0 |  | Yuwei Xia, Mengqi Zhang, Qiang Liu, Shu Wu, Xiaoyu Zhang |  |
| 1143 |  |  [mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections](https://doi.org/10.18653/v1/2022.emnlp-main.488) |  | 0 |  | Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, He Chen, Guohai Xu, Zheng Cao, Ji Zhang, Songfang Huang, Fei Huang, Jingren Zhou, Luo Si |  |
| 1144 |  |  [Q-TOD: A Query-driven Task-oriented Dialogue System](https://doi.org/10.18653/v1/2022.emnlp-main.489) |  | 0 |  | Xin Tian, Yingzhan Lin, Mengfei Song, Siqi Bao, Fan Wang, Huang He, Shuqi Sun, Hua Wu |  |
| 1145 |  |  [Dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.490) |  | 0 |  | Che Liu, Rui Wang, Junfeng Jiang, Yongbin Li, Fei Huang |  |
| 1146 |  |  [WR-One2Set: Towards Well-Calibrated Keyphrase Generation](https://doi.org/10.18653/v1/2022.emnlp-main.491) |  | 0 |  | Binbin Xie, Xiangpeng Wei, Baosong Yang, Huan Lin, Jun Xie, Xiaoli Wang, Min Zhang, Jinsong Su |  |
| 1147 |  |  [Eeny, meeny, miny, moe. How to choose data for morphological inflection](https://doi.org/10.18653/v1/2022.emnlp-main.492) |  | 0 |  | Saliha Muradoglu, Mans Hulden |  |
| 1148 |  |  [An Adaptive Logical Rule Embedding Model for Inductive Reasoning over Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.493) |  | 0 |  | Xin Mei, Libin Yang, Xiaoyan Cai, Zuowei Jiang |  |
| 1149 |  |  [UniNL: Aligning Representation Learning with Scoring Function for OOD Detection via Unified Neighborhood Learning](https://doi.org/10.18653/v1/2022.emnlp-main.494) |  | 0 |  | Yutao Mou, Pei Wang, Keqing He, Yanan Wu, Jingang Wang, Wei Wu, Weiran Xu |  |
| 1150 |  |  [Open-domain Video Commentary Generation](https://doi.org/10.18653/v1/2022.emnlp-main.495) |  | 0 |  | Edison MarreseTaylor, Yumi Hamazono, Tatsuya Ishigaki, Goran Topic, Yusuke Miyao, Ichiro Kobayashi, Hiroya Takamura |  |
| 1151 |  |  [One size does not fit all: Investigating strategies for differentially-private learning across NLP tasks](https://doi.org/10.18653/v1/2022.emnlp-main.496) |  | 0 |  | Manuel Senge, Timour Igamberdiev, Ivan Habernal |  |
| 1152 |  |  [Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario](https://doi.org/10.18653/v1/2022.emnlp-main.497) |  | 0 |  | Xiao Liu, Yansong Feng, Jizhi Tang, Chengang Hu, Dongyan Zhao |  |
| 1153 |  |  [Tutoring Helps Students Learn Better: Improving Knowledge Distillation for BERT with Tutor Network](https://doi.org/10.18653/v1/2022.emnlp-main.498) |  | 0 |  | Junho Kim, JunHyung Park, Mingyu Lee, WingLam Mok, JoonYoung Choi, SangKeun Lee |  |
| 1154 |  |  [Does Corpus Quality Really Matter for Low-Resource Languages?](https://doi.org/10.18653/v1/2022.emnlp-main.499) |  | 0 |  | Mikel Artetxe, Itziar Aldabe, Rodrigo Agerri, Olatz PerezdeViñaspre, Aitor Soroa |  |
| 1155 |  |  [Unifying Data Perspectivism and Personalization: An Application to Social Norms](https://doi.org/10.18653/v1/2022.emnlp-main.500) |  | 0 |  | Joan Plepi, Béla Neuendorf, Lucie Flek, Charles Welch |  |
| 1156 |  |  [Does Self-Rationalization Improve Robustness to Spurious Correlations?](https://doi.org/10.18653/v1/2022.emnlp-main.501) |  | 0 |  | Alexis Ross, Matthew E. Peters, Ana Marasovic |  |
| 1157 |  |  [Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking](https://doi.org/10.18653/v1/2022.emnlp-main.502) |  | 0 |  | Mingyu Lee, JunHyung Park, Junho Kim, KangMin Kim, SangKeun Lee |  |
| 1158 |  |  [Subword Evenness (SuE) as a Predictor of Cross-lingual Transfer to Low-resource Languages](https://doi.org/10.18653/v1/2022.emnlp-main.503) |  | 0 |  | Olga Pelloni, Anastassia Shaitarova, Tanja Samardzic |  |
| 1159 |  |  [A Unified Neural Network Model for Readability Assessment with Feature Projection and Length-Balanced Loss](https://doi.org/10.18653/v1/2022.emnlp-main.504) |  | 0 |  | Wenbiao Li, Ziyang Wang, Yunfang Wu |  |
| 1160 |  |  [Speaker Overlap-aware Neural Diarization for Multi-party Meeting Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.505) |  | 0 |  | Zhihao Du, Shiliang Zhang, Siqi Zheng, ZhiJie Yan |  |
| 1161 |  |  [GREENER: Graph Neural Networks for News Media Profiling](https://doi.org/10.18653/v1/2022.emnlp-main.506) |  | 0 |  | Panayot Panayotov, Utsav Shukla, Husrev Taha Sencar, Mohamed Nabeel, Preslav Nakov |  |
| 1162 |  |  [Graph Hawkes Transformer for Extrapolated Reasoning on Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.507) |  | 0 |  | Haohai Sun, Shangyi Geng, Jialun Zhong, Han Hu, Kun He |  |
| 1163 |  |  [UniRPG: Unified Discrete Reasoning over Table and Text as Program Generation](https://doi.org/10.18653/v1/2022.emnlp-main.508) |  | 0 |  | Yongwei Zhou, Junwei Bao, Chaoqun Duan, Youzheng Wu, Xiaodong He, Tiejun Zhao |  |
| 1164 |  |  [Don't Prompt, Search! Mining-based Zero-Shot Learning with Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.509) |  | 0 |  | Mozes van de Kar, Mengzhou Xia, Danqi Chen, Mikel Artetxe |  |
| 1165 |  |  [SEMGraph: Incorporating Sentiment Knowledge and Eye Movement into Graph Model for Sentiment Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.510) |  | 0 |  | Bingbing Wang, Bin Liang, Jiachen Du, Min Yang, Ruifeng Xu |  |
| 1166 |  |  [Cross-lingual neural fuzzy matching for exploiting target-language monolingual corpora in computer-aided translation](https://doi.org/10.18653/v1/2022.emnlp-main.511) |  | 0 |  | Miquel EsplàGomis, Víctor M. SánchezCartagena, Juan Antonio PérezOrtiz, Felipe SánchezMartínez |  |
| 1167 |  |  [Multi-Label Intent Detection via Contrastive Task Specialization of Sentence Encoders](https://doi.org/10.18653/v1/2022.emnlp-main.512) |  | 0 |  | Ivan Vulic, Iñigo Casanueva, Georgios Spithourakis, Avishek Mondal, TsungHsien Wen, Pawel Budzianowski |  |
| 1168 |  |  [Discovering Language-neutral Sub-networks in Multilingual Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.513) |  | 0 |  | Negar Foroutan, Mohammadreza Banaei, Rémi Lebret, Antoine Bosselut, Karl Aberer |  |
| 1169 |  |  [Parameter-Efficient Tuning Makes a Good Classification Head](https://doi.org/10.18653/v1/2022.emnlp-main.514) |  | 0 |  | Zhuoyi Yang, Ming Ding, Yanhui Guo, Qingsong Lv, Jie Tang |  |
| 1170 |  |  [STGN: an Implicit Regularization Method for Learning with Noisy Labels in Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-main.515) |  | 0 |  | Tingting Wu, Xiao Ding, Minji Tang, Hao Zhang, Bing Qin, Ting Liu |  |
| 1171 |  |  [Cross-Modal Similarity-Based Curriculum Learning for Image Captioning](https://doi.org/10.18653/v1/2022.emnlp-main.516) |  | 0 |  | Hongkuan Zhang, Saku Sugawara, Akiko Aizawa, Lei Zhou, Ryohei Sasano, Koichi Takeda |  |
| 1172 |  |  [Debiasing Masks: A New Framework for Shortcut Mitigation in NLU](https://doi.org/10.18653/v1/2022.emnlp-main.517) |  | 0 |  | Johannes Mario Meissner, Saku Sugawara, Akiko Aizawa |  |
| 1173 |  |  [Extending Phrase Grounding with Pronouns in Visual Dialogues](https://doi.org/10.18653/v1/2022.emnlp-main.518) |  | 0 |  | Panzhong Lu, Xin Zhang, Meishan Zhang, Min Zhang |  |
| 1174 |  |  [EUR-Lex-Sum: A Multi- and Cross-lingual Dataset for Long-form Summarization in the Legal Domain](https://doi.org/10.18653/v1/2022.emnlp-main.519) |  | 0 |  | Dennis Aumiller, Ashish Chouhan, Michael Gertz |  |
| 1175 |  |  [Differentiable Data Augmentation for Contrastive Sentence Representation Learning](https://doi.org/10.18653/v1/2022.emnlp-main.520) |  | 0 |  | Tianduo Wang, Wei Lu |  |
| 1176 |  |  [Text Style Transferring via Adversarial Masking and Styled Filling](https://doi.org/10.18653/v1/2022.emnlp-main.521) |  | 0 |  | Jiarui Wang, Richong Zhang, Junfan Chen, Jaein Kim, Yongyi Mao |  |
| 1177 |  |  [Character-level White-Box Adversarial Attacks against Transformers via Attachable Subwords Substitution](https://doi.org/10.18653/v1/2022.emnlp-main.522) |  | 0 |  | Aiwei Liu, Honghai Yu, Xuming Hu, Shu'ang Li, Li Lin, Fukun Ma, Yawen Yang, Lijie Wen |  |
| 1178 |  |  [Query-based Instance Discrimination Network for Relational Triple Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.523) |  | 0 |  | Zeqi Tan, Yongliang Shen, Xuming Hu, Wenqi Zhang, Xiaoxia Cheng, Weiming Lu, Yueting Zhuang |  |
| 1179 |  |  [Learning Inter-Entity-Interaction for Few-Shot Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.emnlp-main.524) |  | 0 |  | Yuling Li, Kui Yu, Xiaoling Huang, Yuhong Zhang |  |
| 1180 |  |  [Empowering the Fact-checkers! Automatic Identification of Claim Spans on Twitter](https://doi.org/10.18653/v1/2022.emnlp-main.525) |  | 0 |  | Megha Sundriyal, Atharva Kulkarni, Vaibhav Pulastya, Md. Shad Akhtar, Tanmoy Chakraborty |  |
| 1181 |  |  [ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.526) |  | 0 |  | Jiaan Wang, Fandong Meng, Ziyao Lu, Duo Zheng, Zhixu Li, Jianfeng Qu, Jie Zhou |  |
| 1182 |  |  [Spectral Probing](https://doi.org/10.18653/v1/2022.emnlp-main.527) |  | 0 |  | Max MüllerEberstein, Rob van der Goot, Barbara Plank |  |
| 1183 |  |  [QASem Parsing: Text-to-text Modeling of QA-based Semantics](https://doi.org/10.18653/v1/2022.emnlp-main.528) |  | 0 |  | Ayal Klein, Eran Hirsch, Ron Eliav, Valentina Pyatkin, Avi Caciularu, Ido Dagan |  |
| 1184 |  |  [Keyphrase Generation via Soft and Hard Semantic Corrections](https://doi.org/10.18653/v1/2022.emnlp-main.529) |  | 0 |  | Guangzhen Zhao, Guoshun Yin, Peng Yang, Yu Yao |  |
| 1185 |  |  [Modal-specific Pseudo Query Generation for Video Corpus Moment Retrieval](https://doi.org/10.18653/v1/2022.emnlp-main.530) |  | 0 |  | Minjoon Jung, Seongho Choi, Joochan Kim, JinHwa Kim, ByoungTak Zhang |  |
| 1186 |  |  [DuQM: A Chinese Dataset of Linguistically Perturbed Natural Questions for Evaluating the Robustness of Question Matching Models](https://doi.org/10.18653/v1/2022.emnlp-main.531) |  | 0 |  | Hongyu Zhu, Yan Chen, Jing Yan, Jing Liu, Yu Hong, Ying Chen, Hua Wu, Haifeng Wang |  |
| 1187 |  |  [DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages](https://doi.org/10.18653/v1/2022.emnlp-main.532) |  | 0 |  | Gabriele Sarti, Arianna Bisazza, Ana Guerberof Arenas, Antonio Toral |  |
| 1188 |  |  [Bridging Fairness and Environmental Sustainability in Natural Language Processing](https://doi.org/10.18653/v1/2022.emnlp-main.533) |  | 0 |  | Marius Hessenthaler, Emma Strubell, Dirk Hovy, Anne Lauscher |  |
| 1189 |  |  [UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.534) |  | 0 |  | Guimin Hu, TingEn Lin, Yi Zhao, Guangming Lu, Yuchuan Wu, Yongbin Li |  |
| 1190 |  |  [Is the Brain Mechanism for Hierarchical Structure Building Universal Across Languages? An fMRI Study of Chinese and English](https://doi.org/10.18653/v1/2022.emnlp-main.535) |  | 0 |  | Xiaohan Zhang, Shaonan Wang, Nan Lin, Chengqing Zong |  |
| 1191 |  |  [HashFormers: Towards Vocabulary-independent Pre-trained Transformers](https://doi.org/10.18653/v1/2022.emnlp-main.536) |  | 0 |  | Huiyin Xue, Nikolaos Aletras |  |
| 1192 |  |  [MatchPrompt: Prompt-based Open Relation Extraction with Semantic Consistency Guided Clustering](https://doi.org/10.18653/v1/2022.emnlp-main.537) |  | 0 |  | Jiaxin Wang, Lingling Zhang, Jun Liu, Xi Liang, Yujie Zhong, Yaqiang Wu |  |
| 1193 |  |  [Improving Aspect Sentiment Quad Prediction via Template-Order Data Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.538) |  | 0 |  | Mengting Hu, Yike Wu, Hang Gao, Yinhao Bai, Shiwan Zhao |  |
| 1194 |  |  [SocioProbe: What, When, and Where Language Models Learn about Sociodemographics](https://doi.org/10.18653/v1/2022.emnlp-main.539) |  | 0 |  | Anne Lauscher, Federico Bianchi, Samuel R. Bowman, Dirk Hovy |  |
| 1195 |  |  [When does Parameter-Efficient Transfer Learning Work for Machine Translation?](https://doi.org/10.18653/v1/2022.emnlp-main.540) |  | 0 |  | Ahmet Üstün, Asa Cooper Stickland |  |
| 1196 |  |  [Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer](https://doi.org/10.18653/v1/2022.emnlp-main.541) |  | 0 |  | Ahmet Üstün, Arianna Bisazza, Gosse Bouma, Gertjan van Noord, Sebastian Ruder |  |
| 1197 |  |  [Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems](https://doi.org/10.18653/v1/2022.emnlp-main.542) |  | 0 |  | Jialiang Xu, Mengyu Zhou, Xinyi He, Shi Han, Dongmei Zhang |  |
| 1198 |  |  [Enhancing Joint Multiple Intent Detection and Slot Filling with Global Intent-Slot Co-occurrence](https://doi.org/10.18653/v1/2022.emnlp-main.543) |  | 0 |  | Mengxiao Song, Bowen Yu, Quangang Li, Yubin Wang, Tingwen Liu, Hongbo Xu |  |
| 1199 |  |  [Towards Pragmatic Production Strategies for Natural Language Generation Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.544) |  | 0 |  | Mario Giulianelli |  |
| 1200 |  |  [LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.545) |  | 0 |  | Dongsheng Chen, Chaofan Tao, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu |  |
| 1201 |  |  [Communication breakdown: On the low mutual intelligibility between human and neural captioning](https://doi.org/10.18653/v1/2022.emnlp-main.546) |  | 0 |  | Roberto Dessì, Eleonora Gualdoni, Francesca Franzon, Gemma Boleda, Marco Baroni |  |
| 1202 |  |  [Normalizing Mutual Information for Robust Adaptive Training for Translation](https://doi.org/10.18653/v1/2022.emnlp-main.547) |  | 0 |  | Youngwon Lee, Changmin Lee, Hojin Lee, Seungwon Hwang |  |
| 1203 |  |  [Bilingual Synchronization: Restoring Translational Relationships with Editing Operations](https://doi.org/10.18653/v1/2022.emnlp-main.548) |  | 0 |  | Jitao Xu, Josep Maria Crego, François Yvon |  |
| 1204 |  |  [Human-Machine Collaboration Approaches to Build a Dialogue Dataset for Hate Speech Countering](https://doi.org/10.18653/v1/2022.emnlp-main.549) |  | 0 |  | Helena Bonaldi, Sara Dellantonio, Serra Sinem Tekiroglu, Marco Guerini |  |
| 1205 |  |  [JANUS: Joint Autoregressive and Non-autoregressive Training with Auxiliary Loss for Sequence Generation](https://doi.org/10.18653/v1/2022.emnlp-main.550) |  | 0 |  | Xiaobo Liang, Lijun Wu, Juntao Li, Min Zhang |  |
| 1206 |  |  [Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.551) |  | 0 |  | Jialin Wu, Raymond J. Mooney |  |
| 1207 |  |  [Cross-Linguistic Syntactic Difference in Multilingual BERT: How Good is It and How Does It Affect Transfer?](https://doi.org/10.18653/v1/2022.emnlp-main.552) |  | 0 |  | Ningyu Xu, Tao Gui, Ruotian Ma, Qi Zhang, Jingting Ye, Menghan Zhang, Xuanjing Huang |  |
| 1208 |  |  ["It's Not Just Hate": A Multi-Dimensional Perspective on Detecting Harmful Speech Online](https://doi.org/10.18653/v1/2022.emnlp-main.553) |  | 0 |  | Federico Bianchi, Stefanie Anja Hills, Patrícia G. C. Rossini, Dirk Hovy, Rebekah Tromble, Nava Tintarev |  |
| 1209 |  |  [Long Text Generation with Topic-aware Discrete Latent Variable Model](https://doi.org/10.18653/v1/2022.emnlp-main.554) |  | 0 |  | Erguang Yang, Mingtong Liu, Deyi Xiong, Yujie Zhang, Yufeng Chen, Jinan Xu |  |
| 1210 |  |  [TIARA: Multi-grained Retrieval for Robust Question Answering over Large Knowledge Base](https://doi.org/10.18653/v1/2022.emnlp-main.555) |  | 0 |  | Yiheng Shu, Zhiwei Yu, Yuhan Li, Börje F. Karlsson, Tingting Ma, Yuzhong Qu, ChinYew Lin |  |
| 1211 |  |  [Structure-Unified M-Tree Coding Solver for Math Word Problem](https://doi.org/10.18653/v1/2022.emnlp-main.556) |  | 0 |  | Bin Wang, Jiangzhou Ju, Yang Fan, Xinyu Dai, Shujian Huang, Jiajun Chen |  |
| 1212 |  |  [FormLM: Recommending Creation Ideas for Online Forms by Modelling Semantic and Structural Information](https://doi.org/10.18653/v1/2022.emnlp-main.557) |  | 0 |  | Yijia Shao, Mengyu Zhou, Yifan Zhong, Tao Wu, Hongwei Han, Shi Han, Gideon Huang, Dongmei Zhang |  |
| 1213 |  |  [Generate, Discriminate and Contrast: A Semi-Supervised Sentence Representation Learning Framework](https://doi.org/10.18653/v1/2022.emnlp-main.558) |  | 0 |  | Yiming Chen, Yan Zhang, Bin Wang, Zuozhu Liu, Haizhou Li |  |
| 1214 |  |  [GPS: Genetic Prompt Search for Efficient Few-Shot Learning](https://doi.org/10.18653/v1/2022.emnlp-main.559) |  | 0 |  | Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, Zhilin Yang |  |
| 1215 |  |  [Multitask Instruction-based Prompting for Fallacy Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.560) |  | 0 |  | Tariq Alhindi, Tuhin Chakrabarty, Elena Musi, Smaranda Muresan |  |
| 1216 |  |  [Rethinking Multi-Modal Alignment in Multi-Choice VideoQA from Feature and Sample Perspectives](https://doi.org/10.18653/v1/2022.emnlp-main.561) |  | 0 |  | Shaoning Xiao, Long Chen, Kaifeng Gao, Zhao Wang, Yi Yang, Zhimeng Zhang, Jun Xiao |  |
| 1217 |  |  [Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach](https://doi.org/10.18653/v1/2022.emnlp-main.562) |  | 0 |  | Miao Chen, Xinjiang Lu, Tong Xu, Yanyan Li, Jingbo Zhou, Dejing Dou, Hui Xiong |  |
| 1218 |  |  [Hierarchical Phrase-Based Sequence-to-Sequence Learning](https://doi.org/10.18653/v1/2022.emnlp-main.563) |  | 0 |  | Bailin Wang, Ivan Titov, Jacob Andreas, Yoon Kim |  |
| 1219 |  |  [Natural Language Deduction with Incomplete Information](https://doi.org/10.18653/v1/2022.emnlp-main.564) |  | 0 |  | Zayne Sprague, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett |  |
| 1220 |  |  [Character-centric Story Visualization via Visual Planning and Token Alignment](https://doi.org/10.18653/v1/2022.emnlp-main.565) |  | 0 |  | Hong Chen, Rujun Han, TeLin Wu, Hideki Nakayama, Nanyun Peng |  |
| 1221 |  |  [ASQA: Factoid Questions Meet Long-Form Answers](https://doi.org/10.18653/v1/2022.emnlp-main.566) |  | 0 |  | Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, MingWei Chang |  |
| 1222 |  |  [Algorithms for Acyclic Weighted Finite-State Automata with Failure Arcs](https://doi.org/10.18653/v1/2022.emnlp-main.567) |  | 0 |  | Anej Svete, Benjamin Dayan, Ryan Cotterell, Tim Vieira, Jason Eisner |  |
| 1223 |  |  [Towards Better Document-level Relation Extraction via Iterative Inference](https://doi.org/10.18653/v1/2022.emnlp-main.568) |  | 0 |  | Liang Zhang, Jinsong Su, Yidong Chen, Zhongjian Miao, Zijun Min, Qingguo Hu, Xiaodong Shi |  |
| 1224 |  |  [Efficient Adversarial Training with Robust Early-Bird Tickets](https://doi.org/10.18653/v1/2022.emnlp-main.569) |  | 0 |  | Zhiheng Xi, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 1225 |  |  [Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks](https://doi.org/10.18653/v1/2022.emnlp-main.570) |  | 0 |  | Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor BergKirkpatrick, Reza Shokri |  |
| 1226 |  |  [SMaLL-100: Introducing Shallow Multilingual Machine Translation Model for Low-Resource Languages](https://doi.org/10.18653/v1/2022.emnlp-main.571) |  | 0 |  | Alireza Mohammadshahi, Vassilina Nikoulina, Alexandre Berard, Caroline Brun, James Henderson, Laurent Besacier |  |
| 1227 |  |  [TextFusion: Privacy-Preserving Pre-trained Model Inference via Token Fusion](https://doi.org/10.18653/v1/2022.emnlp-main.572) |  | 0 |  | Xin Zhou, Jinzhu Lu, Tao Gui, Ruotian Ma, Zichu Fei, Yuran Wang, Yong Ding, Yibo Cheung, Qi Zhang, Xuanjing Huang |  |
| 1228 |  |  [Learning to Explain Selectively: A Case Study on Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.573) |  | 0 |  | Shi Feng, Jordan L. BoydGraber |  |
| 1229 |  |  [ConsistTL: Modeling Consistency in Transfer Learning for Low-Resource Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.574) |  | 0 |  | Zhaocong Li, Xuebo Liu, Derek F. Wong, Lidia S. Chao, Min Zhang |  |
| 1230 |  |  [Better Hit the Nail on the Head than Beat around the Bush: Removing Protected Attributes with a Single Projection](https://doi.org/10.18653/v1/2022.emnlp-main.575) |  | 0 |  | Pantea Haghighatkhah, Antske Fokkens, Pia Sommerauer, Bettina Speckmann, Kevin Verbeek |  |
| 1231 |  |  [IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.576) |  | 0 |  | Chenguang Wang, Xiao Liu, Dawn Song |  |
| 1232 |  |  [ConNER: Consistency Training for Cross-lingual Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.577) |  | 0 |  | Ran Zhou, Xin Li, Lidong Bing, Erik Cambria, Luo Si, Chunyan Miao |  |
| 1233 |  |  [A Sequential Flow Control Framework for Multi-hop Knowledge Base Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.578) |  | 0 |  | Minghui Xie, Chuzhan Hao, Peng Zhang |  |
| 1234 |  |  [ACENet: Attention Guided Commonsense Reasoning on Hybrid Knowledge Graph](https://doi.org/10.18653/v1/2022.emnlp-main.579) |  | 0 |  | Chuzhan Hao, Minghui Xie, Peng Zhang |  |
| 1235 |  |  [Revisiting DocRED - Addressing the False Negative Problem in Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.580) |  | 0 |  | Qingyu Tan, Lu Xu, Lidong Bing, Hwee Tou Ng, Sharifah Mahani Aljunied |  |
| 1236 |  |  [Towards Summary Candidates Fusion](https://doi.org/10.18653/v1/2022.emnlp-main.581) |  | 0 |  | Mathieu Ravaut, Shafiq R. Joty, Nancy F. Chen |  |
| 1237 |  |  [Multimodal Robustness for Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.582) |  | 0 |  | Yuting Zhao, Ioan Calapodescu |  |
| 1238 |  |  [TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction](https://doi.org/10.18653/v1/2022.emnlp-main.583) |  | 0 |  | Yizhi Li, Wei Fan, Chao Liu, Chenghua Lin, Jiang Qian |  |
| 1239 |  |  [IRRGN: An Implicit Relational Reasoning Graph Network for Multi-turn Response Selection](https://doi.org/10.18653/v1/2022.emnlp-main.584) |  | 0 |  | Jingcheng Deng, Hengwei Dai, Xuewei Guo, Yuanchen Ju, Wei Peng |  |
| 1240 |  |  [Predicting Prerequisite Relations for Unseen Concepts](https://doi.org/10.18653/v1/2022.emnlp-main.585) |  | 0 |  | Yaxin Zhu, Hamed Zamani |  |
| 1241 |  |  [Contrastive Learning with Expectation-Maximization for Weakly Supervised Phrase Grounding](https://doi.org/10.18653/v1/2022.emnlp-main.586) |  | 0 |  | Keqin Chen, Richong Zhang, Samuel Mensah, Yongyi Mao |  |
| 1242 |  |  [Beyond prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations](https://doi.org/10.18653/v1/2022.emnlp-main.587) |  | 0 |  | Yu Fei, Zhao Meng, Ping Nie, Roger Wattenhofer, Mrinmaya Sachan |  |
| 1243 |  |  [Generalizing over Long Tail Concepts for Medical Term Normalization](https://doi.org/10.18653/v1/2022.emnlp-main.588) |  | 0 |  | Beatrice Portelli, Simone Scaboro, Enrico Santus, Hooman Sedghamiz, Emmanuele Chersoni, Giuseppe Serra |  |
| 1244 |  |  [Unsupervised Opinion Summarisation in the Wasserstein Space](https://doi.org/10.18653/v1/2022.emnlp-main.589) |  | 0 |  | Jiayu Song, Iman Munire Bilal, Adam Tsakalidis, Rob Procter, Maria Liakata |  |
| 1245 |  |  [Bloom Library: Multimodal Datasets in 300+ Languages for a Variety of Downstream Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.590) |  | 0 |  | Colin Leong, Joshua Nemecek, Jacob Mansdorfer, Anna Filighera, Abraham Owodunni, Daniel Whitenack |  |
| 1246 |  |  [Disentangling Uncertainty in Machine Translation Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.591) |  | 0 |  | Chrysoula Zerva, Taisiya Glushkova, Ricardo Rei, André F. T. Martins |  |
| 1247 |  |  [Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing](https://doi.org/10.18653/v1/2022.emnlp-main.592) |  | 0 |  | Nan Xu, Fei Wang, Bangzheng Li, Mingtao Dong, Muhao Chen |  |
| 1248 |  |  [EDIN: An End-to-end Benchmark and Pipeline for Unknown Entity Discovery and Indexing](https://doi.org/10.18653/v1/2022.emnlp-main.593) |  | 0 |  | Nora Kassner, Fabio Petroni, Mikhail Plekhanov, Sebastian Riedel, Nicola Cancedda |  |
| 1249 |  |  [POQue: Asking Participant-specific Outcome Questions for a Deeper Understanding of Complex Events](https://doi.org/10.18653/v1/2022.emnlp-main.594) |  | 0 |  | Sai Vallurupalli, Sayontan Ghosh, Katrin Erk, Niranjan Balasubramanian, Francis Ferraro |  |
| 1250 |  |  [Measuring the Mixing of Contextual Information in the Transformer](https://doi.org/10.18653/v1/2022.emnlp-main.595) |  | 0 |  | Javier Ferrando, Gerard I. Gállego, Marta R. Costajussà |  |
| 1251 |  |  [Dealing with Abbreviations in the Slovenian Biographical Lexicon](https://doi.org/10.18653/v1/2022.emnlp-main.596) |  | 0 |  | Angel Daza, Antske Fokkens, Tomaz Erjavec |  |
| 1252 |  |  [AfriCLIRMatrix: Enabling Cross-Lingual Information Retrieval for African Languages](https://doi.org/10.18653/v1/2022.emnlp-main.597) |  | 0 |  | Odunayo Ogundepo, Xinyu Zhang, Shuo Sun, Kevin Duh, Jimmy Lin |  |
| 1253 |  |  [CONDAQA: A Contrastive Reading Comprehension Dataset for Reasoning about Negation](https://doi.org/10.18653/v1/2022.emnlp-main.598) |  | 0 |  | Abhilasha Ravichander, Matt Gardner, Ana Marasovic |  |
| 1254 |  |  [Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer](https://doi.org/10.18653/v1/2022.emnlp-main.599) |  | 0 |  | Javier Ferrando, Gerard I. Gállego, Belen Alastruey, Carlos Escolano, Marta R. Costajussà |  |
| 1255 |  |  [ArtELingo: A Million Emotion Annotations of WikiArt with Emphasis on Diversity over Language and Culture](https://doi.org/10.18653/v1/2022.emnlp-main.600) |  | 0 |  | Youssef Mohamed, Mohamed Abdelfattah, Shyma Alhuwaider, Feifan Li, Xiangliang Zhang, Kenneth Church, Mohamed Elhoseiny |  |
| 1256 |  |  [Decoding a Neural Retriever's Latent Space for Query Suggestion](https://doi.org/10.18653/v1/2022.emnlp-main.601) |  | 0 |  | Leonard Adolphs, Michelle Chen Huebscher, Christian Buck, Sertan Girgin, Olivier Bachem, Massimiliano Ciaramita, Thomas Hofmann |  |
| 1257 |  |  [T-STAR: Truthful Style Transfer using AMR Graph as Intermediate Representation](https://doi.org/10.18653/v1/2022.emnlp-main.602) |  | 0 |  | Anubhav Jangra, Preksha Nema, Aravindan Raghuveer |  |
| 1258 |  |  [PromptBERT: Improving BERT Sentence Embeddings with Prompts](https://doi.org/10.18653/v1/2022.emnlp-main.603) |  | 0 |  | Ting Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang, Deqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen Huang, Denvy Deng, Qi Zhang |  |
| 1259 |  |  [Extending Logic Explained Networks to Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.604) |  | 0 |  | Rishabh Jain, Gabriele Ciravegna, Pietro Barbiero, Francesco Giannini, Davide Buffelli, Pietro Liò |  |
| 1260 |  |  [Uni-Parser: Unified Semantic Parser for Question Answering on Knowledge Base and Database](https://doi.org/10.18653/v1/2022.emnlp-main.605) |  | 0 |  | Ye Liu, Semih Yavuz, Rui Meng, Dragomir Radev, Caiming Xiong, Yingbo Zhou |  |
| 1261 |  |  [RAPO: An Adaptive Ranking Paradigm for Bilingual Lexicon Induction](https://doi.org/10.18653/v1/2022.emnlp-main.606) |  | 0 |  | Zhoujin Tian, Chaozhuo Li, Shuo Ren, Zhiqiang Zuo, Zengxuan Wen, Xinyue Hu, Xiao Han, Haizhen Huang, Denvy Deng, Qi Zhang, Xing Xie |  |
| 1262 |  |  [On Parsing as Tagging](https://doi.org/10.18653/v1/2022.emnlp-main.607) |  | 0 |  | Afra Amini, Ryan Cotterell |  |
| 1263 |  |  [Distilled Dual-Encoder Model for Vision-Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.608) |  | 0 |  | Zekun Wang, Wenhui Wang, Haichao Zhu, Ming Liu, Bing Qin, Furu Wei |  |
| 1264 |  |  [Argument Mining for Review Helpfulness Prediction](https://doi.org/10.18653/v1/2022.emnlp-main.609) |  | 0 |  | Zaiqian Chen, Daniel Verdi do Amarante, Jenna Donaldson, Yohan Jo, Joonsuk Park |  |
| 1265 |  |  [Hierarchical Multi-Label Classification of Scientific Documents](https://doi.org/10.18653/v1/2022.emnlp-main.610) |  | 0 |  | Mobashir Sadat, Cornelia Caragea |  |
| 1266 |  |  [Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.611) |  | 0 |  | Jiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, Yejin Choi |  |
| 1267 |  |  [A Major Obstacle for NLP Research: Let's Talk about Time Allocation!](https://doi.org/10.18653/v1/2022.emnlp-main.612) |  | 0 |  | Katharina Kann, Shiran Dudy, Arya D. McCarthy |  |
| 1268 |  |  [Towards Inter-character Relationship-driven Story Generation](https://doi.org/10.18653/v1/2022.emnlp-main.613) |  | 0 |  | Anvesh Rao Vijjini, Faeze Brahman, Snigdha Chaturvedi |  |
| 1269 |  |  [Incorporating Relevance Feedback for Information-Seeking Retrieval using Few-Shot Document Re-Ranking](https://doi.org/10.18653/v1/2022.emnlp-main.614) |  | 0 |  | Tim Baumgärtner, Leonardo F. R. Ribeiro, Nils Reimers, Iryna Gurevych |  |
| 1270 |  |  [ReasTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples](https://doi.org/10.18653/v1/2022.emnlp-main.615) |  | 0 |  | Yilun Zhao, Linyong Nan, Zhenting Qi, Rui Zhang, Dragomir Radev |  |
| 1271 |  |  [Few-shot Learning with Multilingual Generative Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.616) |  | 0 |  | Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T. Diab, Veselin Stoyanov, Xian Li |  |
| 1272 |  |  [Are representations built from the ground up? An empirical examination of local composition in language models](https://doi.org/10.18653/v1/2022.emnlp-main.617) |  | 0 |  | Emmy Liu, Graham Neubig |  |
| 1273 |  |  [Detecting Label Errors by Using Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.618) |  | 0 |  | Derek Chong, Jenny Hong, Christopher D. Manning |  |
| 1274 |  |  [Intriguing Properties of Compression on Multilingual Models](https://doi.org/10.18653/v1/2022.emnlp-main.619) |  | 0 |  | Kelechi Ogueji, Orevaoghene Ahia, Gbemileke Onilude, Sebastian Gehrmann, Sara Hooker, Julia Kreutzer |  |
| 1275 |  |  [Sequence Models for Document Structure Identification in an Undeciphered Script](https://doi.org/10.18653/v1/2022.emnlp-main.620) |  | 0 |  | Logan Born, M. Willis Monroe, Kathryn Kelley, Anoop Sarkar |  |
| 1276 |  |  [English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.621) |  | 0 |  | YauShian Wang, Ashley Wu, Graham Neubig |  |
| 1277 |  |  [Active Example Selection for In-Context Learning](https://doi.org/10.18653/v1/2022.emnlp-main.622) |  | 0 |  | Yiming Zhang, Shi Feng, Chenhao Tan |  |
| 1278 |  |  [Improving Factual Consistency in Summarization with Compression-Based Post-Editing](https://doi.org/10.18653/v1/2022.emnlp-main.623) |  | 0 |  | Alexander R. Fabbri, Prafulla Kumar Choubey, Jesse Vig, ChienSheng Wu, Caiming Xiong |  |
| 1279 |  |  [Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.624) |  | 0 |  | Linlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi, Jonathan Herzig, Emily Pitler, Fei Sha, Kristina Toutanova |  |
| 1280 |  |  ["I'm sorry to hear that": Finding New Biases in Language Models with a Holistic Descriptor Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.625) |  | 0 |  | Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, Adina Williams |  |
| 1281 |  |  [Understanding ME? Multimodal Evaluation for Fine-grained Visual Commonsense](https://doi.org/10.18653/v1/2022.emnlp-main.626) |  | 0 |  | Zhecan Wang, Haoxuan You, Yicheng He, Wenhao Li, KaiWei Chang, ShihFu Chang |  |
| 1282 |  |  [Semantic Novelty Detection and Characterization in Factual Text Involving Named Entities](https://doi.org/10.18653/v1/2022.emnlp-main.627) |  | 0 |  | Nianzu Ma, Sahisnu Mazumder, Alexander Politowicz, Bing Liu, Eric Robertson, Scott Grigsby |  |
| 1283 |  |  [CN-AutoMIC: Distilling Chinese Commonsense Knowledge from Pretrained Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.628) |  | 0 |  | Chenhao Wang, Jiachun Li, Yubo Chen, Kang Liu, Jun Zhao |  |
| 1284 |  |  [Calibrating Student Models for Emotion-related Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.629) |  | 0 |  | Mahshid Hosseini, Cornelia Caragea |  |
| 1285 |  |  [Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation](https://doi.org/10.18653/v1/2022.emnlp-main.630) |  | 0 |  | Tu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mohit Iyyer, Noah Constant |  |
| 1286 |  |  [Improving Large-scale Paraphrase Acquisition and Generation](https://doi.org/10.18653/v1/2022.emnlp-main.631) |  | 0 |  | Yao Dou, Chao Jiang, Wei Xu |  |
| 1287 |  |  [Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal](https://doi.org/10.18653/v1/2022.emnlp-main.632) |  | 0 |  | ByungDoh Oh, William Schuler |  |
| 1288 |  |  [A Survey of Computational Framing Analysis Approaches](https://doi.org/10.18653/v1/2022.emnlp-main.633) |  | 0 |  | Mohammad Ali, Naeemul Hassan |  |
| 1289 |  |  [Learning Cross-Task Dependencies for Joint Extraction of Entities, Events, Event Arguments, and Relations](https://doi.org/10.18653/v1/2022.emnlp-main.634) |  | 0 |  | Minh Van Nguyen, Bonan Min, Franck Dernoncourt, Thien Huu Nguyen |  |
| 1290 |  |  [Don't Copy the Teacher: Data and Model Challenges in Embodied Dialogue](https://doi.org/10.18653/v1/2022.emnlp-main.635) |  | 0 |  | So Yeon Min, Hao Zhu, Ruslan Salakhutdinov, Yonatan Bisk |  |
| 1291 |  |  [ALFRED-L: Investigating the Role of Language for Action Learning in Interactive Visual Environments](https://doi.org/10.18653/v1/2022.emnlp-main.636) |  | 0 |  | Arjun R. Akula, Spandana Gella, Aishwarya Padmakumar, Mahdi Namazifar, Mohit Bansal, Jesse Thomason, Dilek HakkaniTur |  |
| 1292 |  |  [Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence](https://doi.org/10.18653/v1/2022.emnlp-main.637) |  | 0 |  | Chris CallisonBurch, Gaurav Singh Tomar, Lara J. Martin, Daphne Ippolito, Suma Bailis, David Reitter |  |
| 1293 |  |  [Unsupervised Entity Linking with Guided Summarization and Multiple-Choice Selection](https://doi.org/10.18653/v1/2022.emnlp-main.638) |  | 0 |  | Young Min Cho, Li Zhang, Chris CallisonBurch |  |
| 1294 |  |  [Weakly-Supervised Temporal Article Grounding](https://doi.org/10.18653/v1/2022.emnlp-main.639) |  | 0 |  | Long Chen, Yulei Niu, Brian Chen, Xudong Lin, Guangxing Han, Christopher Thomas, Hammad A. Ayyubi, Heng Ji, ShihFu Chang |  |
| 1295 |  |  [Exploring Dual Encoder Architectures for Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.640) |  | 0 |  | Zhe Dong, Jianmo Ni, Dan Bikel, Enrique Alfonseca, Yuan Wang, Chen Qu, Imed Zitouni |  |
| 1296 |  |  [arXivEdits: Understanding the Human Revision Process in Scientific Writing](https://doi.org/10.18653/v1/2022.emnlp-main.641) |  | 0 |  | Chao Jiang, Wei Xu, Samuel Stevens |  |
| 1297 |  |  [Why Do You Feel This Way? Summarizing Triggers of Emotions in Social Media Posts](https://doi.org/10.18653/v1/2022.emnlp-main.642) |  | 0 |  | Hongli Zhan, Tiberiu Sosea, Cornelia Caragea, Junyi Jessy Li |  |
| 1298 |  |  [Analogical Math Word Problems Solving with Enhanced Problem-Solution Association](https://doi.org/10.18653/v1/2022.emnlp-main.643) |  | 0 |  | Zhenwen Liang, Jipeng Zhang, Xiangliang Zhang |  |
| 1299 |  |  [Towards Teachable Reasoning Systems: Using a Dynamic Memory of User Feedback for Continual System Improvement](https://doi.org/10.18653/v1/2022.emnlp-main.644) |  | 0 |  | Bhavana Dalvi Mishra, Oyvind Tafjord, Peter Clark |  |
| 1300 |  |  [Knowledge Transfer from Answer Ranking to Answer Generation](https://doi.org/10.18653/v1/2022.emnlp-main.645) |  | 0 |  | Matteo Gabburo, Rik KoncelKedziorski, Siddhant Garg, Luca Soldaini, Alessandro Moschitti |  |
| 1301 |  |  [Perturbation Augmentation for Fairer NLP](https://doi.org/10.18653/v1/2022.emnlp-main.646) |  | 0 |  | Rebecca Qian, Candace Ross, Jude Fernandes, Eric Michael Smith, Douwe Kiela, Adina Williams |  |
| 1302 |  |  [Automatic Document Selection for Efficient Encoder Pretraining](https://doi.org/10.18653/v1/2022.emnlp-main.647) |  | 0 |  | Yukun Feng, Patrick Xia, Benjamin Van Durme, João Sedoc |  |
| 1303 |  |  [The Aligned Multimodal Movie Treebank: An audio, video, dependency-parse treebank](https://doi.org/10.18653/v1/2022.emnlp-main.648) |  | 0 |  | Adam Uri Yaari, Jan DeWitt, Henry Hu, Bennett Stankovits, Sue Felshin, Yevgeni Berzak, Helena Aparicio, Boris Katz, Ignacio Cases, Andrei Barbu |  |
| 1304 |  |  [DEMETR: Diagnosing Evaluation Metrics for Translation](https://doi.org/10.18653/v1/2022.emnlp-main.649) |  | 0 |  | Marzena Karpinska, Nishant Raj, Katherine Thai, Yixiao Song, Ankita Gupta, Mohit Iyyer |  |
| 1305 |  |  [Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.650) |  | 0 |  | Ziniu Hu, Yichong Xu, Wenhao Yu, Shuohang Wang, Ziyi Yang, Chenguang Zhu, KaiWei Chang, Yizhou Sun |  |
| 1306 |  |  [Debiasing Pretrained Text Encoders by Paying Attention to Paying Attention](https://doi.org/10.18653/v1/2022.emnlp-main.651) |  | 0 |  | Yacine Gaci, Boualem Benatallah, Fabio Casati, Khalid Benabdeslem |  |
| 1307 |  |  [MEE: A Novel Multilingual Event Extraction Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.652) |  | 0 |  | Amir Pouran Ben Veyseh, Javid Ebrahimi, Franck Dernoncourt, Thien Huu Nguyen |  |
| 1308 |  |  [RobustLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners](https://doi.org/10.18653/v1/2022.emnlp-main.653) |  | 0 |  | Soumya Sanyal, Zeyi Liao, Xiang Ren |  |
| 1309 |  |  [Evaluating and Improving Factuality in Multimodal Abstractive Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.654) |  | 0 |  | David Wan, Mohit Bansal |  |
| 1310 |  |  [Referee: Reference-Free Sentence Summarization with Sharper Controllability through Symbolic Knowledge Distillation](https://doi.org/10.18653/v1/2022.emnlp-main.655) |  | 0 |  | Melanie Sclar, Peter West, Sachin Kumar, Yulia Tsvetkov, Yejin Choi |  |
| 1311 |  |  [Algorithms for Weighted Pushdown Automata](https://doi.org/10.18653/v1/2022.emnlp-main.656) |  | 0 |  | Alexandra Butoi, Brian DuSell, Tim Vieira, Ryan Cotterell, David Chiang |  |
| 1312 |  |  [MABEL: Attenuating Gender Bias using Textual Entailment Data](https://doi.org/10.18653/v1/2022.emnlp-main.657) |  | 0 |  | Jacqueline He, Mengzhou Xia, Christiane Fellbaum, Danqi Chen |  |
| 1313 |  |  [Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs](https://doi.org/10.18653/v1/2022.emnlp-main.658) |  | 0 |  | Kyle Richardson, Ronen Tamari, Oren Sultan, Dafna Shahaf, Reut Tsarfaty, Ashish Sabharwal |  |
| 1314 |  |  [Late Fusion with Triplet Margin Objective for Multimodal Ideology Prediction and Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.659) |  | 0 |  | Changyuan Qiu, Winston Wu, Xinliang Frederick Zhang, Lu Wang |  |
| 1315 |  |  [Leveraging QA Datasets to Improve Generative Data Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.660) |  | 0 |  | Dheeraj Mekala, Tu Vu, Timo Schick, Jingbo Shang |  |
| 1316 |  |  [Meta-Learning Fast Weight Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.661) |  | 0 |  | Kevin Clark, Kelvin Guu, MingWei Chang, Panupong Pasupat, Geoffrey E. Hinton, Mohammad Norouzi |  |
| 1317 |  |  [CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations](https://doi.org/10.18653/v1/2022.emnlp-main.662) |  | 0 |  | Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber |  |
| 1318 |  |  [Learning with Rejection for Abstractive Text Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.663) |  | 0 |  | Meng Cao, Yue Dong, Jingyi He, Jackie Chi Kit Cheung |  |
| 1319 |  |  [Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation](https://doi.org/10.18653/v1/2022.emnlp-main.664) |  | 0 |  | Dongkyu Lee, Ka Chun Cheung, Nevin L. Zhang |  |
| 1320 |  |  [Hard Gate Knowledge Distillation - Leverage Calibration for Robust and Reliable Language Model](https://doi.org/10.18653/v1/2022.emnlp-main.665) |  | 0 |  | Dongkyu Lee, Zhiliang Tian, Yingxiu Zhao, Ka Chun Cheung, Nevin Lianwen Zhang |  |
| 1321 |  |  [Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens](https://doi.org/10.18653/v1/2022.emnlp-main.666) |  | 0 |  | Nitish Joshi, Xiang Pan, He He |  |
| 1322 |  |  [Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling](https://doi.org/10.18653/v1/2022.emnlp-main.667) |  | 0 |  | Vidhisha Balachandran, Hannaneh Hajishirzi, William W. Cohen, Yulia Tsvetkov |  |
| 1323 |  |  [Coordinated Topic Modeling](https://doi.org/10.18653/v1/2022.emnlp-main.668) |  | 0 |  | Pritom Saha Akash, Jie Huang, Kevin ChenChuan Chang |  |
| 1324 |  |  [Large Dual Encoders Are Generalizable Retrievers](https://doi.org/10.18653/v1/2022.emnlp-main.669) |  | 0 |  | Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao, Yi Luan, Keith B. Hall, MingWei Chang, Yinfei Yang |  |
| 1325 |  |  [CRIPP-VQA: Counterfactual Reasoning about Implicit Physical Properties via Video Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.670) |  | 0 |  | Maitreya Patel, Tejas Gokhale, Chitta Baral, Yezhou Yang |  |
| 1326 |  |  [Entity-centered Cross-document Relation Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.671) |  | 0 |  | Fengqi Wang, Fei Li, Hao Fei, Jingye Li, Shengqiong Wu, Fangfang Su, Wenxuan Shi, Donghong Ji, Bo Cai |  |
| 1327 |  |  [Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature](https://doi.org/10.18653/v1/2022.emnlp-main.672) |  | 0 |  | Katherine Thai, Marzena Karpinska, Kalpesh Krishna, Bill Ray, Moira Inghilleri, John Wieting, Mohit Iyyer |  |
| 1328 |  |  [Label-aware Multi-level Contrastive Learning for Cross-lingual Spoken Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.673) |  | 0 |  | Shining Liang, Linjun Shou, Jian Pei, Ming Gong, Wanli Zuo, Xianglin Zuo, Daxin Jiang |  |
| 1329 |  |  [Polyglot Prompt: Multilingual Multitask Prompt Training](https://doi.org/10.18653/v1/2022.emnlp-main.674) |  | 0 |  | Jinlan Fu, SeeKiong Ng, Pengfei Liu |  |
| 1330 |  |  [VisToT: Vision-Augmented Table-to-Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.675) |  | 0 |  | Prajwal Gatti, Anand Mishra, Manish Gupta, Mithun Das Gupta |  |
| 1331 |  |  [Generative Entity-to-Entity Stance Detection with Knowledge Graph Augmentation](https://doi.org/10.18653/v1/2022.emnlp-main.676) |  | 0 |  | Xinliang Frederick Zhang, Nick Beauchamp, Lu Wang |  |
| 1332 |  |  [Symptom Identification for Interpretable Detection of Multiple Mental Disorders on Social Media](https://doi.org/10.18653/v1/2022.emnlp-main.677) |  | 0 |  | Zhiling Zhang, Siyuan Chen, Mengyue Wu, Kenny Q. Zhu |  |
| 1333 |  |  [Improving Iterative Text Revision by Learning Where to Edit from Other Revision Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.678) |  | 0 |  | Zae Myung Kim, Wanyu Du, Vipul Raheja, Dhruv Kumar, Dongyeop Kang |  |
| 1334 |  |  [CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning](https://doi.org/10.18653/v1/2022.emnlp-main.679) |  | 0 |  | Zeqiu Wu, Yi Luan, Hannah Rashkin, David Reitter, Hannaneh Hajishirzi, Mari Ostendorf, Gaurav Singh Tomar |  |
| 1335 |  |  [Specializing Multi-domain NMT via Penalizing Low Mutual Information](https://doi.org/10.18653/v1/2022.emnlp-main.680) |  | 0 |  | Jiyoung Lee, Hantae Kim, Hyunchang Cho, Edward Choi, Cheonbok Park |  |
| 1336 |  |  [A Simple Contrastive Learning Framework for Interactive Argument Pair Identification via Argument-Context Extraction](https://doi.org/10.18653/v1/2022.emnlp-main.681) |  | 0 |  | Lida Shi, Fausto Giunchiglia, Rui Song, Daqian Shi, Tongtong Liu, Xiaolei Diao, Hao Xu |  |
| 1337 |  |  [Sentence-level Media Bias Analysis Informed by Discourse Structures](https://doi.org/10.18653/v1/2022.emnlp-main.682) |  | 0 |  | Yuanyuan Lei, Ruihong Huang, Lu Wang, Nick Beauchamp |  |
| 1338 |  |  [Towards Efficient Dialogue Pre-training with Transferable and Interpretable Latent Structure](https://doi.org/10.18653/v1/2022.emnlp-main.683) |  | 0 |  | Xueliang Zhao, Lemao Liu, Tingchen Fu, Shuming Shi, Dongyan Zhao, Rui Yan |  |
| 1339 |  |  [An Empirical Revisiting of Linguistic Knowledge Fusion in Language Understanding Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.684) |  | 0 |  | Changlong Yu, Tianyi Xiao, Lingpeng Kong, Yangqiu Song, Wilfred Ng |  |
| 1340 |  |  [Unsupervised Non-transferable Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.685) |  | 0 |  | Guangtao Zeng, Wei Lu |  |
| 1341 |  |  [Adaptive Contrastive Learning on Multimodal Transformer for Review Helpfulness Prediction](https://doi.org/10.18653/v1/2022.emnlp-main.686) |  | 0 |  | Thong Nguyen, Xiaobao Wu, Anh Tuan Luu, Zhen Hai, Lidong Bing |  |
| 1342 |  |  [Adaptive Token-level Cross-lingual Feature Mixing for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.687) |  | 0 |  | Junpeng Liu, Kaiyu Huang, Jiuyi Li, Huan Liu, Jinsong Su, Degen Huang |  |
| 1343 |  |  [A Dataset for Hyper-Relational Extraction and a Cube-Filling Approach](https://doi.org/10.18653/v1/2022.emnlp-main.688) |  | 0 |  | Yew Ken Chia, Lidong Bing, Sharifah Mahani Aljunied, Luo Si, Soujanya Poria |  |
| 1344 |  |  [Low-resource Neural Machine Translation with Cross-modal Alignment](https://doi.org/10.18653/v1/2022.emnlp-main.689) |  | 0 |  | Zhe Yang, Qingkai Fang, Yang Feng |  |
| 1345 |  |  [Prompt-based Distribution Alignment for Domain Generalization in Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.690) |  | 0 |  | Chen Jia, Yue Zhang |  |
| 1346 |  |  [Two is Better than Many? Binary Classification as an Effective Approach to Multi-Choice Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.691) |  | 0 |  | Deepanway Ghosal, Navonil Majumder, Rada Mihalcea, Soujanya Poria |  |
| 1347 |  |  [HEGEL: Hypergraph Transformer for Long Document Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.692) |  | 0 |  | Haopeng Zhang, Xiao Liu, Jiawei Zhang |  |
| 1348 |  |  [Adapting a Language Model While Preserving its General Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.693) |  | 0 |  | Zixuan Ke, Yijia Shao, Haowei Lin, Hu Xu, Lei Shu, Bing Liu |  |
| 1349 |  |  [Human Guided Exploitation of Interpretable Attention Patterns in Summarization and Topic Segmentation](https://doi.org/10.18653/v1/2022.emnlp-main.694) |  | 0 |  | Raymond Li, Wen Xiao, Linzi Xing, Lanjun Wang, Gabriel Murray, Giuseppe Carenini |  |
| 1350 |  |  [Continual Training of Language Models for Few-Shot Learning](https://doi.org/10.18653/v1/2022.emnlp-main.695) |  | 0 |  | Zixuan Ke, Haowei Lin, Yijia Shao, Hu Xu, Lei Shu, Bing Liu |  |
| 1351 |  |  [Dictionary-Assisted Supervised Contrastive Learning](https://doi.org/10.18653/v1/2022.emnlp-main.696) |  | 0 |  | Patrick Y. Wu, Richard Bonneau, Joshua A. Tucker, Jonathan Nagler |  |
| 1352 |  |  [Fine-Tuning Pre-trained Transformers into Decaying Fast Weights](https://doi.org/10.18653/v1/2022.emnlp-main.697) |  | 0 |  | Huanru Henry Mao |  |
| 1353 |  |  [PRO-CS : An Instance-Based Prompt Composition Technique for Code-Switched Tasks](https://doi.org/10.18653/v1/2022.emnlp-main.698) |  | 0 |  | Srijan Bansal, Suraj Tripathi, Sumit Agarwal, Teruko Mitamura, Eric Nyberg |  |
| 1354 |  |  [SentBS: Sentence-level Beam Search for Controllable Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.699) |  | 0 |  | Chenhui Shen, Liying Cheng, Lidong Bing, Yang You, Luo Si |  |
| 1355 |  |  [A Fine-grained Chinese Software Privacy Policy Dataset for Sequence Labeling and Regulation Compliant Identification](https://doi.org/10.18653/v1/2022.emnlp-main.700) |  | 0 |  | Kaifa Zhao, Le Yu, Shiyao Zhou, Jing Li, Xiapu Luo, Aemon Yat Fei Chiu, Yutong Liu |  |
| 1356 |  |  [Saving Dense Retriever from Shortcut Dependency in Conversational Search](https://doi.org/10.18653/v1/2022.emnlp-main.701) |  | 0 |  | Sungdong Kim, Gangwoo Kim |  |
| 1357 |  |  [Graph-Induced Transformers for Efficient Multi-Hop Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.702) |  | 0 |  | Giwon Hong, Jeonghwan Kim, Junmo Kang, SungHyon Myaeng |  |
| 1358 |  |  [DiscoSense: Commonsense Reasoning with Discourse Connectives](https://doi.org/10.18653/v1/2022.emnlp-main.703) |  | 0 |  | Prajjwal Bhargava, Vincent Ng |  |
| 1359 |  |  [Boosting Document-Level Relation Extraction by Mining and Injecting Logical Rules](https://doi.org/10.18653/v1/2022.emnlp-main.704) |  | 0 |  | Shengda Fan, Shasha Mo, Jianwei Niu |  |
| 1360 |  |  [MOCHA: A Multi-Task Training Approach for Coherent Text Generation from Cognitive Perspective](https://doi.org/10.18653/v1/2022.emnlp-main.705) |  | 0 |  | Zhe Hu, Hou Pong Chan, Lifu Huang |  |
| 1361 |  |  [Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation](https://doi.org/10.18653/v1/2022.emnlp-main.706) |  | 0 |  | Zhuang Li, Lizhen Qu, Qiongkai Xu, Tongtong Wu, Tianyang Zhan, Gholamreza Haffari |  |
| 1362 |  |  [CISLR: Corpus for Indian Sign Language Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.707) |  | 0 |  | Abhinav Joshi, Ashwani Bhat, Pradeep S, Priya Gole, Shashwat Gupta, Shreyansh Agarwal, Ashutosh Modi |  |
| 1363 |  |  [Mask the Correct Tokens: An Embarrassingly Simple Approach for Error Correction](https://doi.org/10.18653/v1/2022.emnlp-main.708) |  | 0 |  | Kai Shen, Yichong Leng, Xu Tan, Siliang Tang, Yuan Zhang, Wenjie Liu, Edward Lin |  |
| 1364 |  |  [AMAL: Meta Knowledge-Driven Few-Shot Adapter Learning](https://doi.org/10.18653/v1/2022.emnlp-main.709) |  | 0 |  | S. K. Hong, Tae Young Jang |  |
| 1365 |  |  [Discourse Context Predictability Effects in Hindi Word Order](https://doi.org/10.18653/v1/2022.emnlp-main.710) |  | 0 |  | Sidharth Ranjan, Marten van Schijndel, Sumeet Agarwal, Rajakrishnan Rajkumar |  |
| 1366 |  |  ["Covid vaccine is against Covid but Oxford vaccine is made at Oxford!" Semantic Interpretation of Proper Noun Compounds](https://doi.org/10.18653/v1/2022.emnlp-main.711) |  | 0 |  | Keshav Kolluru, Gabriel Stanovsky, Mausam |  |
| 1367 |  |  [Context Limitations Make Neural Language Models More Human-Like](https://doi.org/10.18653/v1/2022.emnlp-main.712) |  | 0 |  | Tatsuki Kuribayashi, Yohei Oseki, Ana Brassard, Kentaro Inui |  |
| 1368 |  |  [A Generative Model for End-to-End Argument Mining with Reconstructed Positional Encoding and Constrained Pointer Mechanism](https://doi.org/10.18653/v1/2022.emnlp-main.713) |  | 0 |  | Jianzhu Bao, Yuhang He, Yang Sun, Bin Liang, Jiachen Du, Bing Qin, Min Yang, Ruifeng Xu |  |
| 1369 |  |  [Reflect, Not Reflex: Inference-Based Common Ground Improves Dialogue Response Quality](https://doi.org/10.18653/v1/2022.emnlp-main.714) |  | 0 |  | Pei Zhou, Hyundong Cho, Pegah Jandaghi, DongHo Lee, Bill Yuchen Lin, Jay Pujara, Xiang Ren |  |
| 1370 |  |  [FlowEval: A Consensus-Based Dialogue Evaluation Framework Using Segment Act Flows](https://doi.org/10.18653/v1/2022.emnlp-main.715) |  | 0 |  | Jianqiao Zhao, Yanyang Li, Wanyu Du, Yangfeng Ji, Dong Yu, Michael R. Lyu, Liwei Wang |  |
| 1371 |  |  [FaD-VLP: Fashion Vision-and-Language Pre-training towards Unified Retrieval and Captioning](https://doi.org/10.18653/v1/2022.emnlp-main.716) |  | 0 |  | Suvir Mirchandani, Licheng Yu, Mengjiao Wang, Animesh Sinha, Wenwen Jiang, Tao Xiang, Ning Zhang |  |
| 1372 |  |  [MM-Align: Learning Optimal Transport-based Alignment Dynamics for Fast and Accurate Inference on Missing Modality Sequences](https://doi.org/10.18653/v1/2022.emnlp-main.717) |  | 0 |  | Wei Han, Hui Chen, MinYen Kan, Soujanya Poria |  |
| 1373 |  |  [Evaluating the Knowledge Dependency of Questions](https://doi.org/10.18653/v1/2022.emnlp-main.718) |  | 0 |  | Hyeongdon Moon, Yoonseok Yang, Hangyeol Yu, Seunghyun Lee, Myeongho Jeong, Juneyoung Park, Jamin Shin, Minsam Kim, Seungtaek Choi |  |
| 1374 |  |  [MoSE: Modality Split and Ensemble for Multimodal Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.emnlp-main.719) |  | 0 |  | Yu Zhao, Xiangrui Cai, Yike Wu, Haiwei Zhang, Ying Zhang, Guoqing Zhao, Ning Jiang |  |
| 1375 |  |  [Entropy-Based Vocabulary Substitution for Incremental Learning in Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.720) |  | 0 |  | Kaiyu Huang, Peng Li, Jin Ma, Yang Liu |  |
| 1376 |  |  [Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation](https://doi.org/10.18653/v1/2022.emnlp-main.721) |  | 0 |  | Yanyang Li, Jianqiao Zhao, Michael R. Lyu, Liwei Wang |  |
| 1377 |  |  [An Unsupervised, Geometric and Syntax-aware Quantification of Polysemy](https://doi.org/10.18653/v1/2022.emnlp-main.722) |  | 0 |  | Anmol Goel, Charu Sharma, Ponnurangam Kumaraguru |  |
| 1378 |  |  [Reorder and then Parse, Fast and Accurate Discontinuous Constituency Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.723) |  | 0 |  | Kailai Sun, Zuchao Li, Hai Zhao |  |
| 1379 |  |  [Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature](https://doi.org/10.18653/v1/2022.emnlp-main.724) |  | 0 |  | Tomas Goldsack, Zhihao Zhang, Chenghua Lin, Carolina Scarton |  |
| 1380 |  |  [Looking at the Overlooked: An Analysis on the Word-Overlap Bias in Natural Language Inference](https://doi.org/10.18653/v1/2022.emnlp-main.725) |  | 0 |  | Sara Rajaee, Yadollah Yaghoobzadeh, Mohammad Taher Pilehvar |  |
| 1381 |  |  [An Empirical Study on the Transferability of Transformer Modules in Parameter-efficient Fine-tuning](https://doi.org/10.18653/v1/2022.emnlp-main.726) |  | 0 |  | Mohammad AkbarTajari, Sara Rajaee, Mohammad Taher Pilehvar |  |
| 1382 |  |  [CODER: An efficient framework for improving retrieval through COntextual Document Embedding Reranking](https://doi.org/10.18653/v1/2022.emnlp-main.727) |  | 0 |  | George Zerveas, Navid Rekabsaz, Daniel Cohen, Carsten Eickhoff |  |
| 1383 |  |  [AdapterShare: Task Correlation Modeling with Adapter Differentiation](https://doi.org/10.18653/v1/2022.emnlp-main.728) |  | 0 |  | Zhi Chen, Bei Chen, Lu Chen, Kai Yu, JianGuang Lou |  |
| 1384 |  |  [Rethinking Task-Specific Knowledge Distillation: Contextualized Corpus as Better Textbook](https://doi.org/10.18653/v1/2022.emnlp-main.729) |  | 0 |  | Chang Liu, Chongyang Tao, Jianxin Liang, Tao Shen, Jiazhan Feng, Quzhe Huang, Dongyan Zhao |  |
| 1385 |  |  [Recovering Gold from Black Sand: Multilingual Dense Passage Retrieval with Hard and False Negative Samples](https://doi.org/10.18653/v1/2022.emnlp-main.730) |  | 0 |  | Tianhao Shen, Mingtong Liu, Ming Zhou, Deyi Xiong |  |
| 1386 |  |  [The "Problem" of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.731) |  | 0 |  | Barbara Plank |  |
| 1387 |  |  [Quality Scoring of Source Words in Neural Translation Models](https://doi.org/10.18653/v1/2022.emnlp-main.732) |  | 0 |  | Priyesh Jain, Sunita Sarawagi, Tushar Tomar |  |
| 1388 |  |  [Pneg: Prompt-based Negative Response Generation for Dialogue Response Selection Task](https://doi.org/10.18653/v1/2022.emnlp-main.733) |  | 0 |  | Nyoungwoo Lee, ChaeHun Park, HoJin Choi, Jaegul Choo |  |
| 1389 |  |  [Facilitating Contrastive Learning of Discourse Relational Senses by Exploiting the Hierarchy of Sense Relations](https://doi.org/10.18653/v1/2022.emnlp-main.734) |  | 0 |  | Wanqiu Long, Bonnie Webber |  |
| 1390 |  |  [Simplified Graph Learning for Inductive Short Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.735) |  | 0 |  | Kaixin Zheng, Yaqing Wang, Quanming Yao, Dejing Dou |  |
| 1391 |  |  [Don't Stop Fine-Tuning: On Training Regimes for Few-Shot Cross-Lingual Transfer with Multilingual Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.736) |  | 0 |  | Fabian David Schmidt, Ivan Vulic, Goran Glavas |  |
| 1392 |  |  [Towards Compositional Generalization in Code Search](https://doi.org/10.18653/v1/2022.emnlp-main.737) |  | 0 |  | Hojae Han, Seungwon Hwang, Shuai Lu, Nan Duan, Seungtaek Choi |  |
| 1393 |  |  [Towards relation extraction from speech](https://doi.org/10.18653/v1/2022.emnlp-main.738) |  | 0 |  | Tongtong Wu, Guitao Wang, Jinming Zhao, Zhaoran Liu, Guilin Qi, YuanFang Li, Gholamreza Haffari |  |
| 1394 |  |  [Structural Constraints and Natural Language Inference for End-to-End Flowchart Grounded Dialog Response Generation](https://doi.org/10.18653/v1/2022.emnlp-main.739) |  | 0 |  | Dinesh Raghu, Suraj Joshi, Sachindra Joshi, Mausam |  |
| 1395 |  |  [SLICER: Sliced Fine-Tuning for Low-Resource Cross-Lingual Transfer for Named Entity Recognition](https://doi.org/10.18653/v1/2022.emnlp-main.740) |  | 0 |  | Fabian David Schmidt, Ivan Vulic, Goran Glavas |  |
| 1396 |  |  [EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation](https://doi.org/10.18653/v1/2022.emnlp-main.741) |  | 0 |  | Tao Ge, SiQing Chen, Furu Wei |  |
| 1397 |  |  [End-to-End Unsupervised Vision-and-Language Pre-training with Referring Expression Matching](https://doi.org/10.18653/v1/2022.emnlp-main.742) |  | 0 |  | Chi Chen, Peng Li, Maosong Sun, Yang Liu |  |
| 1398 |  |  [Faithful Knowledge Graph Explanations in Commonsense Question Answering](https://doi.org/10.18653/v1/2022.emnlp-main.743) |  | 0 |  | Guy Aglionby, Simone Teufel |  |
| 1399 |  |  [KOLD: Korean Offensive Language Dataset](https://doi.org/10.18653/v1/2022.emnlp-main.744) |  | 0 |  | Younghoon Jeong, Juhyun Oh, Jongwon Lee, Jaimeen Ahn, Jihyung Moon, Sungjoon Park, Alice Oh |  |
| 1400 |  |  [Evade the Trap of Mediocrity: Promoting Diversity and Novelty in Text Generation via Concentrating Attention](https://doi.org/10.18653/v1/2022.emnlp-main.745) |  | 0 |  | Wenhao Li, Xiaoyuan Yi, Jinyi Hu, Maosong Sun, Xing Xie |  |
| 1401 |  |  [The better your Syntax, the better your Semantics? Probing Pretrained Language Models for the English Comparative Correlative](https://doi.org/10.18653/v1/2022.emnlp-main.746) |  | 0 |  | Leonie Weissweiler, Valentin Hofmann, Abdullatif Köksal, Hinrich Schütze |  |
| 1402 |  |  [ProofInfer: Generating Proof via Iterative Hierarchical Inference](https://doi.org/10.18653/v1/2022.emnlp-main.747) |  | 0 |  | Zichu Fei, Qi Zhang, Xin Zhou, Tao Gui, Xuanjing Huang |  |
| 1403 |  |  [ECTSum: A New Benchmark Dataset For Bullet Point Summarization of Long Earnings Call Transcripts](https://doi.org/10.18653/v1/2022.emnlp-main.748) |  | 0 |  | Rajdeep Mukherjee, Abhinav Bohra, Akash Banerjee, Soumya Sharma, Manjunath Hegde, Afreen Shaikh, Shivani Shrivastava, Koustuv Dasgupta, Niloy Ganguly, Saptarshi Ghosh, Pawan Goyal |  |
| 1404 |  |  [Cross-domain Generalization for AMR Parsing](https://doi.org/10.18653/v1/2022.emnlp-main.749) |  | 0 |  | Xuefeng Bai, Sen Yang, Leyang Cui, Linfeng Song, Yue Zhang |  |
| 1405 |  |  [CiteSum: Citation Text-guided Scientific Extreme Summarization and Domain Adaptation with Limited Supervision](https://doi.org/10.18653/v1/2022.emnlp-main.750) |  | 0 |  | Yuning Mao, Ming Zhong, Jiawei Han |  |
| 1406 |  |  [FETA: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue](https://doi.org/10.18653/v1/2022.emnlp-main.751) |  | 0 |  | Alon Albalak, YiLin Tuan, Pegah Jandaghi, Connor Pryor, Luke Yoffe, Deepak Ramachandran, Lise Getoor, Jay Pujara, William Yang Wang |  |
| 1407 |  |  [Do Children Texts Hold The Key To Commonsense Knowledge?](https://doi.org/10.18653/v1/2022.emnlp-main.752) |  | 0 |  | Julien Romero, Simon Razniewski |  |
| 1408 |  |  [On the Limitations of Reference-Free Evaluations of Generated Text](https://doi.org/10.18653/v1/2022.emnlp-main.753) |  | 0 |  | Daniel Deutsch, Rotem Dror, Dan Roth |  |
| 1409 |  |  [Sampling-Based Approximations to Minimum Bayes Risk Decoding for Neural Machine Translation](https://doi.org/10.18653/v1/2022.emnlp-main.754) |  | 0 |  | Bryan Eikema, Wilker Aziz |  |
| 1410 |  |  [IndicXNLI: Evaluating Multilingual Inference for Indian Languages](https://doi.org/10.18653/v1/2022.emnlp-main.755) |  | 0 |  | Divyanshu Aggarwal, Vivek Gupta, Anoop Kunchukuttan |  |
| 1411 |  |  [Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems](https://doi.org/10.18653/v1/2022.emnlp-main.756) |  | 0 |  | Neeraj Varshney, Chitta Baral |  |
| 1412 |  |  [Semantic Simplification for Sentiment Classification](https://doi.org/10.18653/v1/2022.emnlp-main.757) |  | 0 |  | Xiaotong Jiang, Zhongqing Wang, Guodong Zhou |  |
| 1413 |  |  [XPrompt: Exploring the Extreme of Prompt Tuning](https://doi.org/10.18653/v1/2022.emnlp-main.758) |  | 0 |  | Fang Ma, Chen Zhang, Lei Ren, Jingang Wang, Qifan Wang, Wei Wu, Xiaojun Quan, Dawei Song |  |
| 1414 |  |  [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://doi.org/10.18653/v1/2022.emnlp-main.759) |  | 0 |  | Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer |  |
| 1415 |  |  [The Curious Case of Control](https://doi.org/10.18653/v1/2022.emnlp-main.760) |  | 0 |  | Elias StengelEskin, Benjamin Van Durme |  |
| 1416 |  |  [SHARE: a System for Hierarchical Assistive Recipe Editing](https://doi.org/10.18653/v1/2022.emnlp-main.761) |  | 0 |  | Shuyang Li, Yufei Li, Jianmo Ni, Julian J. McAuley |  |
| 1417 |  |  [IM⌃2: an Interpretable and Multi-category Integrated Metric Framework for Automatic Dialogue Evaluation](https://doi.org/10.18653/v1/2022.emnlp-main.762) |  | 0 |  | Zhihua Jiang, Guanghui Ye, Dongning Rao, Di Wang, Xin Miao |  |
| 1418 |  |  [PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models](https://doi.org/10.18653/v1/2022.emnlp-main.763) |  | 0 |  | Yuan Yao, Qianyu Chen, Ao Zhang, Wei Ji, Zhiyuan Liu, TatSeng Chua, Maosong Sun |  |
| 1419 |  |  [Pre-training Language Models with Deterministic Factual Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.764) |  | 0 |  | Shaobo Li, Xiaoguang Li, Lifeng Shang, Chengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang, Qun Liu |  |
| 1420 |  |  [Finding Skill Neurons in Pre-trained Transformer-based Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.765) |  | 0 |  | Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, Juanzi Li |  |
| 1421 |  |  [Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue](https://doi.org/10.18653/v1/2022.emnlp-main.766) |  | 0 |  | Yingxiu Zhao, Yinhe Zheng, Zhiliang Tian, Chang Gao, Jian Sun, Nevin L. Zhang |  |
| 1422 |  |  [PreQuEL: Quality Estimation of Machine Translation Outputs in Advance](https://doi.org/10.18653/v1/2022.emnlp-main.767) |  | 0 |  | Shachar DonYehiya, Leshem Choshen, Omri Abend |  |
| 1423 |  |  [Can Transformers Reason in Fragments of Natural Language?](https://doi.org/10.18653/v1/2022.emnlp-main.768) |  | 0 |  | Viktor Schlegel, Kamen V. Pavlov, Ian PrattHartmann |  |
| 1424 |  |  [Textless Speech Emotion Conversion using Discrete & Decomposed Representations](https://doi.org/10.18653/v1/2022.emnlp-main.769) |  | 0 |  | Felix Kreuk, Adam Polyak, Jade Copet, Eugene Kharitonov, Tu Anh Nguyen, Morgane Rivière, WeiNing Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, Yossi Adi |  |
| 1425 |  |  [Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks](https://doi.org/10.18653/v1/2022.emnlp-main.770) |  | 0 |  | Yangyi Chen, Fanchao Qi, Hongcheng Gao, Zhiyuan Liu, Maosong Sun |  |
| 1426 |  |  [Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP](https://doi.org/10.18653/v1/2022.emnlp-main.771) |  | 0 |  | Yangyi Chen, Hongcheng Gao, Ganqu Cui, Fanchao Qi, Longtao Huang, Zhiyuan Liu, Maosong Sun |  |
| 1427 |  |  [Retrieval Augmented Visual Question Answering with Outside Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.772) |  | 0 |  | Weizhe Lin, Bill Byrne |  |
| 1428 |  |  [Instance Regularization for Discriminative Language Model Pre-training](https://doi.org/10.18653/v1/2022.emnlp-main.773) |  | 0 |  | Zhuosheng Zhang, Hai Zhao, Ming Zhou |  |
| 1429 |  |  [GuoFeng: A Benchmark for Zero Pronoun Recovery and Translation](https://doi.org/10.18653/v1/2022.emnlp-main.774) |  | 0 |  | Mingzhou Xu, Longyue Wang, Derek F. Wong, Hongye Liu, Linfeng Song, Lidia S. Chao, Shuming Shi, Zhaopeng Tu |  |
| 1430 |  |  [ScienceWorld: Is your Agent Smarter than a 5th Grader?](https://doi.org/10.18653/v1/2022.emnlp-main.775) |  | 0 |  | Ruoyao Wang, Peter A. Jansen, MarcAlexandre Côté, Prithviraj Ammanabrolu |  |
| 1431 |  |  [Improving Embeddings Representations for Comparing Higher Education Curricula: A Use Case in Computing](https://doi.org/10.18653/v1/2022.emnlp-main.776) |  | 0 |  | Jeffri MurrugarraLlerena, Fernando AlvaManchego, Nils MurrugarraLlerena |  |
| 1432 |  |  [Mitigating Spurious Correlation in Natural Language Understanding with Counterfactual Inference](https://doi.org/10.18653/v1/2022.emnlp-main.777) |  | 0 |  | Can Udomcharoenchaikit, Wuttikorn Ponwitayarat, Patomporn Payoungkhamdee, Kanruethai Masuk, Weerayut Buaphet, Ekapol Chuangsuwanich, Sarana Nutanong |  |
| 1433 |  |  [End-to-End Neural Discourse Deixis Resolution in Dialogue](https://doi.org/10.18653/v1/2022.emnlp-main.778) |  | 0 |  | Shengjie Li, Vincent Ng |  |
| 1434 |  |  [Balancing out Bias: Achieving Fairness Through Balanced Training](https://doi.org/10.18653/v1/2022.emnlp-main.779) |  | 0 |  | Xudong Han, Timothy Baldwin, Trevor Cohn |  |
| 1435 |  |  [Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models](https://doi.org/10.18653/v1/2022.emnlp-main.780) |  | 0 |  | Mengzhou Xia, Mikel Artetxe, Jingfei Du, Danqi Chen, Veselin Stoyanov |  |
| 1436 |  |  [Identifying Physical Object Use in Sentences](https://doi.org/10.18653/v1/2022.emnlp-main.781) |  | 0 |  | Tianyu Jiang, Ellen Riloff |  |
| 1437 |  |  [CDialog: A Multi-turn Covid-19 Conversation Dataset for Entity-Aware Dialog Generation](https://doi.org/10.18653/v1/2022.emnlp-main.782) |  | 0 |  | Deeksha Varshney, Aizan Zafar, Niranshu Kumar Behra, Asif Ekbal |  |
| 1438 |  |  [Robustifying Sentiment Classification by Maximally Exploiting Few Counterfactuals](https://doi.org/10.18653/v1/2022.emnlp-main.783) |  | 0 |  | Maarten De Raedt, Fréderic Godin, Chris Develder, Thomas Demeester |  |
| 1439 |  |  [Data-Efficient Playlist Captioning With Musical and Linguistic Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.784) |  | 0 |  | Giovanni Gabbolini, Romain Hennequin, Elena V. Epure |  |
| 1440 |  |  [Improved grammatical error correction by ranking elementary edits](https://doi.org/10.18653/v1/2022.emnlp-main.785) |  | 0 |  | Alexey Sorokin |  |
| 1441 |  |  [Improving Tokenisation by Alternative Treatment of Spaces](https://doi.org/10.18653/v1/2022.emnlp-main.786) |  | 0 |  | Edward GowSmith, Harish Tayyar Madabushi, Carolina Scarton, Aline Villavicencio |  |
| 1442 |  |  [GENIE: Toward Reproducible and Standardized Human Evaluation for Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.787) |  | 0 |  | Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A. Smith, Daniel S. Weld |  |
| 1443 |  |  [Attentional Probe: Estimating a Module's Functional Potential](https://doi.org/10.18653/v1/2022.emnlp-main.788) |  | 0 |  | Tiago Pimentel, Josef Valvoda, Niklas Stoehr, Ryan Cotterell |  |
| 1444 |  |  [When More Data Hurts: A Troubling Quirk in Developing Broad-Coverage Natural Language Understanding Systems](https://doi.org/10.18653/v1/2022.emnlp-main.789) |  | 0 |  | Elias StengelEskin, Emmanouil Antonios Platanios, Adam Pauls, Sam Thomson, Hao Fang, Benjamin Van Durme, Jason Eisner, Yu Su |  |
| 1445 |  |  [Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt](https://doi.org/10.18653/v1/2022.emnlp-main.790) |  | 0 |  | Lianzhe Huang, Shuming Ma, Dongdong Zhang, Furu Wei, Houfeng Wang |  |
| 1446 |  |  [Three Real-World Datasets and Neural Computational Models for Classification Tasks in Patent Landscaping](https://doi.org/10.18653/v1/2022.emnlp-main.791) |  | 0 |  | Subhash Chandra Pujari, Jannik Strötgen, Mark Giereth, Michael Gertz, Annemarie Friedrich |  |
| 1447 |  |  [Topic Modeling With Topological Data Analysis](https://doi.org/10.18653/v1/2022.emnlp-main.792) |  | 0 |  | Ciarán Byrne, Danijela Horak, Karo Moilanen, Amandla Mabona |  |
| 1448 |  |  [Predicting Fine-Tuning Performance with Probing](https://doi.org/10.18653/v1/2022.emnlp-main.793) |  | 0 |  | Zining Zhu, Soroosh Shahtalebi, Frank Rudzicz |  |
| 1449 |  |  [Diverse Parallel Data Synthesis for Cross-Database Adaptation of Text-to-SQL Parsers](https://doi.org/10.18653/v1/2022.emnlp-main.794) |  | 0 |  | Abhijeet Awasthi, Ashutosh Sathe, Sunita Sarawagi |  |
| 1450 |  |  [Agent-Specific Deontic Modality Detection in Legal Language](https://doi.org/10.18653/v1/2022.emnlp-main.795) |  | 0 |  | Abhilasha Sancheti, Aparna Garimella, Balaji Vasan Srinivasan, Rachel Rudinger |  |
| 1451 |  |  [COLD: A Benchmark for Chinese Offensive Language Detection](https://doi.org/10.18653/v1/2022.emnlp-main.796) |  | 0 |  | Jiawen Deng, Jingyan Zhou, Hao Sun, Chujie Zheng, Fei Mi, Helen Meng, Minlie Huang |  |
| 1452 |  |  [Fixing Model Bugs with Natural Language Patches](https://doi.org/10.18653/v1/2022.emnlp-main.797) |  | 0 |  | Shikhar Murty, Christopher D. Manning, Scott M. Lundberg, Marco Túlio Ribeiro |  |
| 1453 |  |  [WeDef: Weakly Supervised Backdoor Defense for Text Classification](https://doi.org/10.18653/v1/2022.emnlp-main.798) |  | 0 |  | Lesheng Jin, Zihan Wang, Jingbo Shang |  |
| 1454 |  |  [Interventional Training for Out-Of-Distribution Natural Language Understanding](https://doi.org/10.18653/v1/2022.emnlp-main.799) |  | 0 |  | Sicheng Yu, Jing Jiang, Hao Zhang, Yulei Niu, Qianru Sun, Lidong Bing |  |
| 1455 |  |  [Pseudo-Relevance for Enhancing Document Representation](https://doi.org/10.18653/v1/2022.emnlp-main.800) |  | 0 |  | Jihyuk Kim, Seungwon Hwang, Seoho Song, Hyeseon Ko, YoungIn Song |  |
| 1456 |  |  [ZeroGen: Efficient Zero-shot Learning via Dataset Generation](https://doi.org/10.18653/v1/2022.emnlp-main.801) |  | 0 |  | Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, Lingpeng Kong |  |
| 1457 |  |  [Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.802) |  | 0 |  | Malte Ostendorff, Nils Rethmeier, Isabelle Augenstein, Bela Gipp, Georg Rehm |  |
| 1458 |  |  [SPE: Symmetrical Prompt Enhancement for Fact Probing](https://doi.org/10.18653/v1/2022.emnlp-main.803) |  | 0 |  | Yiyuan Li, Tong Che, Yezhen Wang, Zhengbao Jiang, Caiming Xiong, Snigdha Chaturvedi |  |
| 1459 |  |  [Efficient Large Scale Language Modeling with Mixtures of Experts](https://doi.org/10.18653/v1/2022.emnlp-main.804) |  | 0 |  | Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giridharan Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeffrey Wang, Luke Zettlemoyer, Mona T. Diab, Zornitsa Kozareva, Veselin Stoyanov |  |
| 1460 |  |  [MedJEx: A Medical Jargon Extraction Model with Wiki's Hyperlink Span and Contextualized Masked Language Model Score](https://doi.org/10.18653/v1/2022.emnlp-main.805) |  | 0 |  | Sunjae Kwon, Zonghai Yao, Harmon S. Jordan, David A. Levy, Brian Corner, Hong Yu |  |
| 1461 |  |  [Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections](https://doi.org/10.18653/v1/2022.emnlp-main.806) |  | 0 |  | WeiJen Ko, Cutter Dalton, Mark Simmons, Eliza Fisher, Greg Durrett, Junyi Jessy Li |  |
| 1462 |  |  [Learning to Generate Overlap Summaries through Noisy Synthetic Data](https://doi.org/10.18653/v1/2022.emnlp-main.807) |  | 0 |  | Naman Bansal, Mousumi Akter, Shubhra Kanti Karmaker Santu |  |
| 1463 |  |  [Mutual Exclusivity Training and Primitive Augmentation to Induce Compositionality](https://doi.org/10.18653/v1/2022.emnlp-main.808) |  | 0 |  | Yichen Jiang, Xiang Zhou, Mohit Bansal |  |
| 1464 |  |  [Directions for NLP Practices Applied to Online Hate Speech Detection](https://doi.org/10.18653/v1/2022.emnlp-main.809) |  | 0 |  | Paula Fortuna, Mónica Domínguez, Leo Wanner, Zeerak Talat |  |
| 1465 |  |  [Pre-training Transformer Models with Sentence-Level Objectives for Answer Sentence Selection](https://doi.org/10.18653/v1/2022.emnlp-main.810) |  | 0 |  | Luca Di Liello, Siddhant Garg, Luca Soldaini, Alessandro Moschitti |  |
| 1466 |  |  [OpenCQA: Open-ended Question Answering with Charts](https://doi.org/10.18653/v1/2022.emnlp-main.811) |  | 0 |  | Shankar Kantharaj, Xuan Long Do, Rixie Tiffany Ko Leong, Jia Qing Tan, Enamul Hoque, Shafiq R. Joty |  |
| 1467 |  |  [A Systematic Investigation of Commonsense Knowledge in Large Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.812) |  | 0 |  | Xiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoffmann, Cyprien de Masson d'Autume, Phil Blunsom, Aida Nematzadeh |  |
| 1468 |  |  [Transforming Sequence Tagging Into A Seq2Seq Task](https://doi.org/10.18653/v1/2022.emnlp-main.813) |  | 0 |  | Karthik Raman, Iftekhar Naim, Jiecao Chen, Kazuma Hashimoto, Kiran Yalasangi, Krishna Srinivasan |  |
| 1469 |  |  [CycleKQR: Unsupervised Bidirectional Keyword-Question Rewriting](https://doi.org/10.18653/v1/2022.emnlp-main.814) |  | 0 |  | Andrea Iovine, Anjie Fang, Besnik Fetahu, Jie Zhao, Oleg Rokhlenko, Shervin Malmasi |  |
| 1470 |  |  [Model Criticism for Long-Form Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.815) |  | 0 |  | Yuntian Deng, Volodymyr Kuleshov, Alexander M. Rush |  |
| 1471 |  |  [Improving Faithfulness by Augmenting Negative Summaries from Fake Documents](https://doi.org/10.18653/v1/2022.emnlp-main.816) |  | 0 |  | Tianshu Wang, Faisal Ladhak, Esin Durmus, He He |  |
| 1472 |  |  [Joint Completion and Alignment of Multilingual Knowledge Graphs](https://doi.org/10.18653/v1/2022.emnlp-main.817) |  | 0 |  | Soumen Chakrabarti, Harkanwar Singh, Shubham Lohiya, Prachi Jain, Mausam |  |
| 1473 |  |  [Offer a Different Perspective: Modeling the Belief Alignment of Arguments in Multi-party Debates](https://doi.org/10.18653/v1/2022.emnlp-main.818) |  | 0 |  | Suzanna Sia, Kokil Jaidka, Hansin Ahuja, Niyati Chhaya, Kevin Duh |  |
| 1474 |  |  [A Federated Approach to Predicting Emojis in Hindi Tweets](https://doi.org/10.18653/v1/2022.emnlp-main.819) |  | 0 |  | Deep Gandhi, Jash Mehta, Nirali Parekh, Karan Waghela, Lynette D'Mello, Zeerak Talat |  |
| 1475 |  |  [Injecting Domain Knowledge in Language Models for Task-oriented Dialogue Systems](https://doi.org/10.18653/v1/2022.emnlp-main.820) |  | 0 |  | Denis Emelin, Daniele Bonadiman, Sawsan Alqahtani, Yi Zhang, Saab Mansour |  |
| 1476 |  |  [TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack](https://doi.org/10.18653/v1/2022.emnlp-main.821) |  | 0 |  | Yu Cao, Dianqi Li, Meng Fang, Tianyi Zhou, Jun Gao, Yibing Zhan, Dacheng Tao |  |
| 1477 |  |  [Improving Low-Resource Languages in Pre-Trained Multilingual Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.822) |  | 0 |  | Viktor Hangya, Hossain Shaikh Saadi, Alexander Fraser |  |
| 1478 |  |  [SCROLLS: Standardized CompaRison Over Long Language Sequences](https://doi.org/10.18653/v1/2022.emnlp-main.823) |  | 0 |  | Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, Omer Levy |  |
| 1479 |  |  [PAR: Political Actor Representation Learning with Social Context and Expert Knowledge](https://doi.org/10.18653/v1/2022.emnlp-main.824) |  | 0 |  | Shangbin Feng, Zhaoxuan Tan, Zilong Chen, Ningnan Wang, Peisheng Yu, Qinghua Zheng, Xiaojun Chang, Minnan Luo |  |
| 1480 |  |  [JDDC 2.1: A Multimodal Chinese Dialogue Dataset with Joint Tasks of Query Rewriting, Response Generation, Discourse Parsing, and Summarization](https://doi.org/10.18653/v1/2022.emnlp-main.825) |  | 0 |  | Nan Zhao, Haoran Li, Youzheng Wu, Xiaodong He |  |
| 1481 |  |  [PCL: Peer-Contrastive Learning with Diverse Augmentations for Unsupervised Sentence Embeddings](https://doi.org/10.18653/v1/2022.emnlp-main.826) |  | 0 |  | Qiyu Wu, Chongyang Tao, Tao Shen, Can Xu, Xiubo Geng, Daxin Jiang |  |
| 1482 |  |  [Digging Errors in NMT: Evaluating and Understanding Model Errors from Partial Hypothesis Space](https://doi.org/10.18653/v1/2022.emnlp-main.827) |  | 0 |  | Jianhao Yan, Chenming Wu, Fandong Meng, Jie Zhou |  |
| 1483 |  |  [DialogConv: A Lightweight Fully Convolutional Network for Multi-view Response Selection](https://doi.org/10.18653/v1/2022.emnlp-main.828) |  | 0 |  | Yongkang Liu, Shi Feng, Wei Gao, Daling Wang, Yifei Zhang |  |
