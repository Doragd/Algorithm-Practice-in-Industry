# EMNLP2023

## 会议论文列表

本会议共有 2241 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Frontmatter](https://aclanthology.org/2023.emnlp-demo.0) |  | 0 |  |  |  |
| 2 |  |  [Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs](https://doi.org/10.18653/v1/2023.emnlp-demo.1) |  | 0 | Most NLP tasks are modeled as supervised learning and thus require labeled training data to train effective models. However, manually producing such data at sufficient quality and quantity is known to be costly and time-intensive. Current research addresses this bottleneck by exploring a novel paradigm called zero-shot learning via dataset generation. Here, a powerful LLM is prompted with a task description to generate labeled data that can be used to train a downstream NLP model. For instance,... | Jonas Golde, Patrick Haller, Felix Hamborg, Julian Risch, Alan Akbik |  |
| 3 |  |  [End-to-End Evaluation for Low-Latency Simultaneous Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-demo.2) |  | 0 | The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches. In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech... | Christian Huber, Tu Anh Dinh, Carlos Mullov, NgocQuan Pham, Thai Binh Nguyen, Fabian Retkowski, Stefan Constantin, Enes Yavuz Ugan, Danni Liu, Zhaolin Li, Sai Koneru, Jan Niehues, Alexander Waibel |  |
| 4 |  |  [CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools](https://doi.org/10.18653/v1/2023.emnlp-demo.3) |  | 0 | In the face of climate change, are companies really taking substantial steps toward more sustainable operations? A comprehensive answer lies in the dense, information-rich landscape of corporate sustainability reports. However, the sheer volume and complexity of these reports make human analysis very costly. Therefore, only a few entities worldwide have the resources to analyze these reports at scale, which leads to a lack of transparency in sustainability reporting. Empowering stakeholders... | Jingwei Ni, Julia Anna Bingler, Chiara Colesanti Senni, Mathias Kraus, Glen Gostlow, Tobias Schimanski, Dominik Stammbach, Saeid Ashraf Vaghefi, Qian Wang, Nicolas Webersinke, Tobias Wekhof, Tingyu Yu, Markus Leippold |  |
| 5 |  |  [RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.4) |  | 0 | Retrieval-augmented large language models (R-LLMs) combine pre-trained large language models (LLMs) with information retrieval systems to improve the accuracy of factual question-answering. However, current libraries for building R-LLMs provide high-level abstractions without sufficient transparency for evaluating and optimizing prompts within specific inference processes such as retrieval and generation. To address this gap, we present RaLLe, an open-source framework designed to facilitate the... | Yasuto Hoshi, Daisuke Miyashita, Youyang Ng, Kento Tatsuno, Yasuhiro Morioka, Osamu Torii, Jun Deguchi |  |
| 6 |  |  [VIST5: An Adaptive, Retrieval-Augmented Language Model for Visualization-oriented Dialog](https://doi.org/10.18653/v1/2023.emnlp-demo.5) |  | 0 | The advent of large language models has brought about new ways of interacting with data intuitively via natural language. In recent years, a variety of visualization systems have explored the use of natural language to create and modify visualizations through visualization-oriented dialog. However, the majority of these systems rely on tailored dialog agents to analyze domain-specific data and operate domain-specific visualization tools and libraries. This is a major challenge when trying to... | Henrik Voigt, Nuno Carvalhais, Monique Meuschke, Markus Reichstein, Sina Zarrieß, Kai Lawonn |  |
| 7 |  |  [H2O Open Ecosystem for State-of-the-art Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.6) |  | 0 | Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7 to 70 Billion parameters. We also introduce... | Arno Candel, Jon McKinney, Philipp Singer, Pascal Pfeiffer, Maximilian Jeblick, Chun Ming Lee, Marcos V. Conde |  |
| 8 |  |  [Koala: An Index for Quantifying Overlaps with Pre-training Corpora](https://doi.org/10.18653/v1/2023.emnlp-demo.7) |  | 0 | In very recent years more attention has been placed on probing the role of pre-training data in Large Language Models (LLMs) downstream behaviour. Despite the importance, there is no public tool that supports such analysis of pre-training corpora at large scale. To help research in this space, we launch Koala, a searchable index over large pre-training corpora using lossless compressed suffix arrays with highly efficient compression rate and search support. In its first release we index the... | ThuyTrang Vu, Xuanli He, Gholamreza Haffari, Ehsan Shareghi |  |
| 9 |  |  [Sudowoodo: A Chinese Lyric Imitation System with Source Lyrics](https://doi.org/10.18653/v1/2023.emnlp-demo.8) |  | 0 | Lyrics generation is a well-known application in natural language generation research, with several previous studies focusing on generating accurate lyrics using precise control such as keywords, rhymes, etc. However, lyrics imitation, which involves writing new lyrics by imitating the style and content of the source lyrics, remains a challenging task due to the lack of a parallel corpus. In this paper, we introduce Sudowoodo, a Chinese lyrics imitation system that can generate new lyrics based... | Yongzhu Chang, Rongsheng Zhang, Lin Jiang, Qihang Chen, Le Zhang, Jiashu Pu |  |
| 10 |  |  [ConvLab-3: A Flexible Dialogue System Toolkit Based on a Unified Data Format](https://doi.org/10.18653/v1/2023.emnlp-demo.9) |  | 0 | Task-oriented dialogue (TOD) systems function as digital assistants, guiding users through various tasks such as booking flights or finding restaurants. Existing toolkits for building TOD systems often fall short in delivering comprehensive arrays of data, model, and experimental environments with a user-friendly experience. We introduce ConvLab-3: a multifaceted dialogue system toolkit crafted to bridge this gap. Our unified data format simplifies the integration of diverse datasets and... | Qi Zhu, Christian Geishauser, HsienChin Lin, Carel van Niekerk, Baolin Peng, Zheng Zhang, Shutong Feng, Michael Heck, Nurul Lubis, Dazhen Wan, Xiaochen Zhu, Jianfeng Gao, Milica Gasic, Minlie Huang |  |
| 11 |  |  [FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge](https://doi.org/10.18653/v1/2023.emnlp-demo.10) |  | 0 | Detecting factual errors of textual information, whether generated by large language models (LLM) or curated by humans, is crucial for making informed decisions. LLMs’ inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses. Humans, too, are prone to factual errors in their writing. Since manual detection and correction of factual er- rors is labor-intensive, developing an automatic approach can greatly reduce... | Farima Fatahi Bayat, Kun Qian, Benjamin Han, Yisi Sang, Anton Belyi, Samira Khorshidi, Fei Wu, Ihab F. Ilyas, Yunyao Li |  |
| 12 |  |  [YATO: Yet Another deep learning based Text analysis Open toolkit](https://doi.org/10.18653/v1/2023.emnlp-demo.11) |  | 0 | We introduce YATO, an open-source, easy-to-use toolkit for text analysis with deep learning. Different from existing heavily engineered toolkits and platforms, YATO is lightweight and user-friendly for researchers from cross-disciplinary areas. Designed in a hierarchical structure, YATO supports free combinations of three types of widely used features including 1) traditional neural networks (CNN, RNN, etc.); 2) pre-trained language models (BERT, RoBERTa, ELECTRA, etc.); and 3) user-customized... | Zeqiang Wang, Yile Wang, Jiageng Wu, Zhiyang Teng, Jie Yang |  |
| 13 |  |  [Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face](https://doi.org/10.18653/v1/2023.emnlp-demo.12) |  | 0 | We present Spacerini, a tool that integrates the Pyserini toolkit for reproducible information retrieval research with Hugging Face to enable the seamless construction and deployment of interactive search engines. Spacerini makes state-of-the-art sparse and dense retrieval models more accessible to non-IR practitioners while minimizing deployment effort. This is useful for NLP researchers who want to better understand and validate their research by performing qualitative analyses of training... | Christopher Akiki, Odunayo Ogundepo, Aleksandra Piktus, Xinyu Zhang, Akintunde Oladipo, Jimmy Lin, Martin Potthast |  |
| 14 |  |  [Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning](https://doi.org/10.18653/v1/2023.emnlp-demo.13) |  | 0 | We introduce Adapters, an open-source library that unifies parameter-efficient and modular transfer learning in large language models. By integrating 10 diverse adapter methods into a unified interface, Adapters offers ease of use and flexible configuration. Our library allows researchers and practitioners to leverage adapter modularity through composition blocks, enabling the design of complex adapter setups. We demonstrate the library’s efficacy by evaluating its performance against full... | Clifton Poth, Hannah Sterz, Indraneil Paul, Sukannya Purkayastha, Leon Engländer, Timo Imhof, Ivan Vulic, Sebastian Ruder, Iryna Gurevych, Jonas Pfeiffer |  |
| 15 |  |  [INTELMO: Enhancing Models' Adoption of Interactive Interfaces](https://doi.org/10.18653/v1/2023.emnlp-demo.14) |  | 0 | This paper presents INTELMO, an easy-to-use library to help model developers adopt user-faced interactive interfaces and articles from real-time RSS sources for their language models. The library categorizes common NLP tasks and provides default style patterns, streamlining the process of creating interfaces with minimal code modifications while ensuring an intuitive user experience. Moreover, INTELMO employs a multi-granular hierarchical abstraction to provide developers with fine-grained and... | Chunxu Yang, ChienSheng Wu, Lidiya Murakhovs'ka, Philippe Laban, Xiang Chen |  |
| 16 |  |  [Humanoid Agents: Platform for Simulating Human-like Generative Agents](https://doi.org/10.18653/v1/2023.emnlp-demo.15) |  | 0 | Just as computational simulations of atoms, molecules and cells have shaped the way we study the sciences, true-to-life simulations of human-like agents can be valuable tools for studying human behavior. We propose Humanoid Agents, a system that guides Generative Agents to behave more like humans by introducing three elements of System 1 processing: Basic needs (e.g. hunger, health and energy), Emotion and Closeness in Relationships. Humanoid Agents are able to use these dynamic elements to... | Zhilin Wang, Yu Ying Chiu, Yu Cheung Chiu |  |
| 17 |  |  [TP-Detector: Detecting Turning Points in the Engineering Process of Large-scale Projects](https://doi.org/10.18653/v1/2023.emnlp-demo.16) |  | 0 | This paper introduces a novel task of detecting turning points in the engineering process of large-scale projects, wherein the turning points signify significant transitions occurring between phases. Given the complexities involving diverse critical events and limited comprehension in individual news reports, we approach the problem by treating the sequence of related news streams as a window with multiple instances. To capture the evolution of changes effectively, we adopt a deep Multiple... | Qi Wu, WenHan Chao, Xian Zhou, Zhunchen Luo |  |
| 18 |  |  [CLEVA: Chinese Language Models EVAluation Platform](https://doi.org/10.18653/v1/2023.emnlp-demo.17) |  | 0 | With the continuous emergence of Chinese Large Language Models (LLMs), how to evaluate a model’s capabilities has become an increasingly significant issue. The absence of a comprehensive Chinese benchmark that thoroughly assesses a model’s performance, the unstandardized and incomparable prompting procedure, and the prevalent risk of contamination pose major challenges in the current evaluation of Chinese LLMs. We present CLEVA, a user-friendly platform crafted to holistically evaluate Chinese... | Yanyang Li, Jianqiao Zhao, Duo Zheng, ZiYuan Hu, Zhi Chen, Xiaohui Su, Yongfeng Huang, Shijia Huang, Dahua Lin, Michael R. Lyu, Liwei Wang |  |
| 19 |  |  [DOPA METER - A Tool Suite for Metrical Document Profiling and Aggregation](https://doi.org/10.18653/v1/2023.emnlp-demo.18) |  | 0 | We present DOPA METER, a tool suite for the metrical investigation of written language, that provides diagnostic means for its division into discourse categories, such as registers, genres, and style. The quantitative basis of our system are 120 metrics covering a wide range of lexical, syntactic, and semantic features relevant for language profiling. The scores can be summarized, compared, and aggregated using visualization tools that can be tailored according to the users’ needs. We also... | Christina Lohr, Udo Hahn |  |
| 20 |  |  [Muted: Multilingual Targeted Offensive Speech Identification and Visualization](https://doi.org/10.18653/v1/2023.emnlp-demo.19) |  | 0 | Offensive language such as hate, abuse, and profanity (HAP) occurs in various content on the web. While previous work has mostly dealt with sentence level annotations, there have been a few recent attempts to identify offensive spans as well. We build upon this work and introduce MUTED, a system to identify multilingual HAP content by displaying offensive arguments and their targets using heat maps to indicate their intensity. MUTED can leverage any transformer-based HAP-classification model... | Christoph Tillmann, Aashka Trivedi, Sara Rosenthal, Santosh Borse, Rong Zhang, Avirup Sil, Bishwaranjan Bhattacharjee |  |
| 21 |  |  [Gentopia.AI: A Collaborative Platform for Tool-Augmented LLMs](https://doi.org/10.18653/v1/2023.emnlp-demo.20) |  | 0 | Augmented Language Models (ALMs) empower large language models with the ability to use tools, transforming them into intelligent agents for real-world interactions. However, most existing frameworks for ALMs, to varying degrees, are deficient in the following critical features: flexible customization, collaborative democratization, and holistic evaluation. This paper proposes Gentopia, a lightweight and extensible framework for ALMs. Gentopia allows the flexible customization of agents through... | Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, Dongkuan Xu |  |
| 22 |  |  [MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.21) |  | 0 | AI-empowered music processing is a diverse feld that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classifcation). For developers and amateurs, it is very diffcult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to... | Dingyao Yu, Kaitao Song, Peiling Lu, Tianyu He, Xu Tan, Wei Ye, Shikun Zhang, Jiang Bian |  |
| 23 |  |  [SentAlign: Accurate and Scalable Sentence Alignment](https://doi.org/10.18653/v1/2023.emnlp-demo.22) |  | 0 | We present SentAlign, an accurate sentence alignment tool designed to handle very large parallel document pairs. Given user-defined parameters, the alignment algorithm evaluates all possible alignment paths in fairly large documents of thousands of sentences and uses a divide-and-conquer approach to align documents containing tens of thousands of sentences. The scoring function is based on LaBSE bilingual sentence representations. SentAlign outperforms five other sentence alignment tools when... | Steinþór Steingrímsson, Hrafn Loftsson, Andy Way |  |
| 24 |  |  [QACheck: A Demonstration System for Question-Guided Multi-Hop Fact-Checking](https://doi.org/10.18653/v1/2023.emnlp-demo.23) |  | 0 | Fact-checking real-world claims often requires intricate, multi-step reasoning due to the absence of direct evidence to support or refute them. However, existing fact-checking systems often lack transparency in their decision-making, making it challenging for users to comprehend their reasoning process. To address this, we propose the Question-guided Multi-hop Fact-Checking (QACheck) system, which guides the model’s reasoning process by asking a series of questions critical for verifying a... | Liangming Pan, Xinyuan Lu, MinYen Kan, Preslav Nakov |  |
| 25 |  |  [RobustQA: A Framework for Adversarial Text Generation Analysis on Question Answering Systems](https://doi.org/10.18653/v1/2023.emnlp-demo.24) |  | 0 | Question answering (QA) systems have reached human-level accuracy; however, these systems are not robust enough and are vulnerable to adversarial examples. Recently, adversarial attacks have been widely investigated in text classification. However, there have been few research efforts on this topic in QA. In this article, we have modified the attack algorithms widely used in text classification to fit those algorithms for QA systems. We have evaluated the impact of various attack methods on QA... | Yasaman Boreshban, Seyed Morteza Mirbostani, Seyedeh Fatemeh Ahmadi, Gita Shojaee, Fatemeh Kamani, Gholamreza GhassemSani, Seyed Abolghasem Mirroshandel |  |
| 26 |  |  [Kandinsky: An Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion](https://doi.org/10.18653/v1/2023.emnlp-demo.25) |  | 0 | Text-to-image generation is a significant domain in modern computer vision and achieved substantial improvements through the evolution of generative architectures. Among these, diffusion-based models demonstrated essential quality enhancements. These models generally split into two categories: pixel-level and latent-level approaches. We present Kandinsky – a novel exploration of latent diffusion architecture, combining the principles of image prior models with latent diffusion techniques. The... | Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov, Angelina Kuts, Alexander Panchenko, Andrey Kuznetsov, Denis Dimitrov |  |
| 27 |  |  [NewsRecLib: A PyTorch-Lightning Library for Neural News Recommendation](https://doi.org/10.18653/v1/2023.emnlp-demo.26) |  | 0 | NewsRecLib is an open-source library based on Pytorch-Lightning and Hydra developed for training and evaluating neural news recommendation models. The foremost goals of NewsRecLib are to promote reproducible research and rigorous experimental evaluation by (i) providing a unified and highly configurable framework for exhaustive experimental studies and (ii) enabling a thorough analysis of the performance contribution of different model architecture components and training regimes. NewsRecLib is... | Andreea Iana, Goran Glavas, Heiko Paulheim |  |
| 28 |  |  [MiniChain: A Small Library for Coding with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.27) |  | 0 | Programming augmented by large language models (LLMs) opens up many new application areas, but also requires care. LLMs are accurate enough, on average, to replace core functionality, yet make basic mistakes that demonstrate a lack of robustness. An ecosystem of prompting tools, from intelligent agents to new programming languages, have emerged with different solutions for patching LLMs with other tools. In this work, we introduce MiniChain, an opinionated tool for LLM augmented programming,... | Alexander M. Rush |  |
| 29 |  |  [Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback](https://doi.org/10.18653/v1/2023.emnlp-demo.28) |  | 0 | A key technology for large language models (LLMs) involves instruction tuning that helps align the models’ responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are applied to produce the best commercial LLMs. To improve the accessibility of LLMs, various instruction-tuned open-source LLMs have also been introduced recently.... | Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A. Rossi, Thien Huu Nguyen |  |
| 30 |  |  [SAGEViz: SchemA GEneration and Visualization](https://doi.org/10.18653/v1/2023.emnlp-demo.29) |  | 0 | Schema induction involves creating a graph representation depicting how events unfold in a scenario. We present SAGEViz, an intuitive and modular tool that utilizes human-AI collaboration to create and update complex schema graphs efficiently, where multiple annotators (humans and models) can work simultaneously on a schema graph from any domain. The tool consists of two components: (1) a curation component powered by plug-and-play event language models to create and expand event sequences... | Sugam Devare, Mahnaz Koupaee, Gautham Gunapati, Sayontan Ghosh, Sai Vallurupalli, Yash Kumar Lal, Francis Ferraro, Nathanael Chambers, Greg Durrett, Raymond J. Mooney, Katrin Erk, Niranjan Balasubramanian |  |
| 31 |  |  [Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained Text Evaluation](https://doi.org/10.18653/v1/2023.emnlp-demo.30) |  | 0 | Fine-grained, span-level human evaluation has emerged as a reliable and robust method for evaluating text generation tasks such as summarization, simplification, machine translation and news generation, and the derived annotations have been useful for training automatic metrics and improving language models. However, existing annotation tools implemented for these evaluation frameworks lack the adaptability to be extended to different domains or languages, or modify annotation settings... | David Heineman, Yao Dou, Wei Xu |  |
| 32 |  |  [InsightPilot: An LLM-Empowered Automated Data Exploration System](https://doi.org/10.18653/v1/2023.emnlp-demo.31) |  | 0 | Exploring data is crucial in data analysis, as it helps users understand and interpret the data more effectively. However, performing effective data exploration requires in-depth knowledge of the dataset, the user intent and expertise in data analysis techniques. Not being familiar with either can create obstacles that make the process time-consuming and overwhelming. To address this issue, we introduce InsightPilot, an LLM (Large Language Model)-based, automated data exploration system... | Pingchuan Ma, Rui Ding, Shuai Wang, Shi Han, Dongmei Zhang |  |
| 33 |  |  [SynJax: Structured Probability Distributions for JAX](https://doi.org/10.18653/v1/2023.emnlp-demo.32) |  | 0 | The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees... | Milos Stanojevic, Laurent Sartran |  |
| 34 |  |  [RESIN-EDITOR: A Schema-guided Hierarchical Event Graph Visualizer and Editor](https://doi.org/10.18653/v1/2023.emnlp-demo.33) |  | 0 | In this paper, we present RESIN-EDITOR, an interactive event graph visualizer and editor designed for analyzing complex events. Our RESIN-EDITOR system allows users to render and freely edit hierarchical event graphs extracted from multimedia and multi-document news clusters with guidance from human-curated event schemas. RESIN-EDITOR’s unique features include hierarchical graph visualization, comprehensive source tracing, and interactive user editing, which significantly outperforms existing... | Khanh Duy Nguyen, Zixuan Zhang, Reece Suchocki, Sha Li, Martha Palmer, Susan Windisch Brown, Jiawei Han, Heng Ji |  |
| 35 |  |  [DRGCoder: Explainable Clinical Coding for the Early Prediction of Diagnostic-Related Groups](https://doi.org/10.18653/v1/2023.emnlp-demo.34) |  | 0 | Medical claim coding is the process of transforming medical records, usually presented as free texts written by clinicians, or discharge summaries, into structured codes in a classification system such as ICD-10 (International Classification of Diseases, Tenth Revision) or DRG (Diagnosis-Related Group) codes. This process is essential for medical billing and transitional care; however, manual coding is time-consuming, error-prone, and expensive. To solve these issues, we propose DRGCoder, an... | Daniel Hajialigol, Derek Kaknes, Tanner Barbour, Daphne Yao, Chris North, Jimeng Sun, David Liem, Xuan Wang |  |
| 36 |  |  [CAMRA: Copilot for AMR Annotation](https://doi.org/10.18653/v1/2023.emnlp-demo.35) |  | 0 | In this paper, we introduce CAMRA (Copilot for AMR Annotatations), a cutting-edge web-based tool designed for constructing Abstract Meaning Representation (AMR) from natural language text. CAMRA offers a novel approach to deep lexical semantics annotation such as AMR, treating AMR annotation akin to coding in programming languages. Leveraging the familiarity of programming paradigms, CAMRA encompasses all essential features of existing AMR editors, including example lookup, while going a step... | Jon Z. Cai, Shafiuddin Rehan Ahmed, Julia Bonn, Kristin WrightBettner, Martha Palmer, James H. Martin |  |
| 37 |  |  [Reaction Miner: An Integrated System for Chemical Reaction Extraction from Textual Data](https://doi.org/10.18653/v1/2023.emnlp-demo.36) |  | 0 | Chemical reactions, as a core entity in the realm of chemistry, hold crucial implications in diverse areas ranging from hands-on laboratory research to advanced computational drug design. Despite a burgeoning interest in employing NLP techniques to extract these reactions, aligning this task with the real-world requirements of chemistry practitioners remains an ongoing challenge. In this paper, we present Reaction Miner, a system specifically designed to interact with raw scientific literature,... | Ming Zhong, Siru Ouyang, Yizhu Jiao, Priyanka Kargupta, Leo Luo, Yanzhen Shen, Bobby Zhou, Xianrui Zhong, Xuan Liu, Hongxiang Li, Jinfeng Xiao, Minhao Jiang, Vivian Hu, Xuan Wang, Heng Ji, Martin D. Burke, Huimin Zhao, Jiawei Han |  |
| 38 |  |  [CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies](https://doi.org/10.18653/v1/2023.emnlp-demo.37) |  | 0 | Various NLP tasks require a complex hierarchical structure over nodes, where each node is a cluster of items. Examples include generating entailment graphs, hierarchical cross-document coreference resolution, annotating event and subevent relations, etc. To enable efficient annotation of such hierarchical structures, we release CHAMP, an open source tool allowing to incrementally construct both clusters and hierarchy simultaneously over any type of texts. This incremental approach significantly... | Arie Cattan, Tom Hope, Doug Downey, Roy BarHaim, Lilach Eden, Yoav Kantor, Ido Dagan |  |
| 39 |  |  [Prompt2Model: Generating Deployable Models from Natural Language Instructions](https://doi.org/10.18653/v1/2023.emnlp-demo.38) |  | 0 | Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description... | Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu, Graham Neubig |  |
| 40 |  |  [NewsSense: Reference-free Verification via Cross-document Comparison](https://doi.org/10.18653/v1/2023.emnlp-demo.39) |  | 0 | We present NewsSense, a novel sensemaking tool and reading interface designed to collect and integrate information from multiple news articles on a central topic. NewsSense provides “reference-free verification,” augmenting a central grounding article of the user’s choice by: (1) linking to related articles from different sources; and (2) providing inline highlights on how specific claims are either supported or contradicted by information from other articles. Using NewsSense, users can... | Jeremiah Milbauer, Ziqi Ding, Zhijin Wu, Tongshuang Wu |  |
| 41 |  |  [NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails](https://doi.org/10.18653/v1/2023.emnlp-demo.40) |  | 0 | NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using... | Traian Rebedea, Razvan Dinu, Makesh Narsimhan Sreedhar, Christopher Parisien, Jonathan Cohen |  |
| 42 |  |  [LM-Polygraph: Uncertainty Estimation for Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.41) |  | 0 | Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields. However, a significant challenge arises as these models often “hallucinate”, i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods are one path to safer, more responsible, and more effective use of LLMs. However, to date, research on UE methods for LLMs... | Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, Timothy Baldwin, Artem Shelmanov |  |
| 43 |  |  [Descriptive Knowledge Graph in Biomedical Domain](https://doi.org/10.18653/v1/2023.emnlp-demo.42) |  | 0 | We present a novel system that automatically extracts and generates informative and descriptive sentences from the biomedical corpus and facilitates the efficient search for relational knowledge. Unlike previous search engines or exploration systems that retrieve unconnected passages, our system organizes descriptive sentences as a relational graph, enabling researchers to explore closely related biomedical entities (e.g., diseases treated by a chemical) or indirectly connected entities (e.g.,... | Kerui Zhu, Jie Huang, Kevin ChenChuan Chang |  |
| 44 |  |  [Prompterator: Iterate Efficiently towards More Effective Prompts](https://doi.org/10.18653/v1/2023.emnlp-demo.43) |  | 0 | With the advent of Large Language Models (LLMs) the process known as prompting, which entices the LLM to solve an arbitrary language processing task without the need for finetuning, has risen to prominence. Finding well-performing prompts, however, is a non-trivial task which requires experimentation in order to arrive at a prompt that solves a specific task. When a given task does not readily reduce to one that can be easily measured with well established metrics, human evaluation of the... | Samuel Sucik, Daniel Skala, Andrej Svec, Peter Hraska, Marek Suppa |  |
| 45 |  |  [ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.44) |  | 0 | The unprecedented performance of LLMs requires comprehensive and accurate evaluation. We argue that for LLMs evaluation, benchmarks need to be comprehensive and systematic. To this end, we propose the Zhujiu benchmark, which has the following strengths: (1) Multi-dimensional ability coverage: We comprehensively evaluate LLMs across 7 ability dimensions covering 51 tasks. Especially, we also propose a new benchmark that focus on knowledge ability of LLMs. (2) Multi-faceted evaluation methods... | Baoli Zhang, Haining Xie, Pengfan Du, Junhao Chen, Pengfei Cao, Yubo Chen, Shengping Liu, Kang Liu, Jun Zhao |  |
| 46 |  |  [PaperMage: A Unified Toolkit for Processing, Representing, and Manipulating Visually-Rich Scientific Documents](https://doi.org/10.18653/v1/2023.emnlp-demo.45) |  | 0 | Despite growing interest in applying natural language processing (NLP) and computer vision (CV) models to the scholarly domain, scientific documents remain challenging to work with. They’re often in difficult-to-use PDF formats, and the ecosystem of models to process them is fragmented and incomplete. We introduce PaperMage, an open-source Python toolkit for analyzing and processing visually-rich, structured scientific documents. PaperMage offers clean and intuitive abstractions for seamlessly... | Kyle Lo, Zejiang Shen, Benjamin Newman, Joseph Chee Chang, Russell Authur, Erin Bransom, Stefan Candra, Yoganand Chandrasekhar, Regan Huff, Bailey Kuehl, Amanpreet Singh, Chris Wilhelm, Angele Zamarron, Marti A. Hearst, Daniel S. Weld, Doug Downey, Luca Soldaini |  |
| 47 |  |  [OmniEvent: A Comprehensive, Fair, and Easy-to-Use Toolkit for Event Understanding](https://doi.org/10.18653/v1/2023.emnlp-demo.46) |  | 0 | Event understanding aims at understanding the content and relationship of events within texts, which covers multiple complicated information extraction tasks: event detection, event argument extraction, and event relation extraction. To facilitate related research and application, we present an event understanding toolkit OmniEvent, which features three desiderata: (1) Comprehensive. OmniEvent supports mainstream modeling paradigms of all the event understanding tasks and the processing of 15... | Hao Peng, Xiaozhi Wang, Feng Yao, Zimu Wang, Chuzhao Zhu, Kaisheng Zeng, Lei Hou, Juanzi Li |  |
| 48 |  |  [CocoSciSum: A Scientific Summarization Toolkit with Compositional Controllability](https://doi.org/10.18653/v1/2023.emnlp-demo.47) |  | 0 | We present a novel toolkit for controlled summarization of scientific documents, designed for the specific needs of the scientific community. Our system generates summaries based on user preferences, adjusting key attributes specifically of length and keyword inclusion. A distinguishing feature is its ability to manage multiple attributes concurrently, demonstrating Compositional Controllability for Scientific Summarization (CocoSciSum). Benchmarked against the strong Flan-T5 baseline,... | Yixi Ding, Yanxia Qin, Qian Liu, MinYen Kan |  |
| 49 |  |  [CoLLiE: Collaborative Training of Large Language Models in an Efficient Way](https://doi.org/10.18653/v1/2023.emnlp-demo.48) |  | 0 | Large language models (LLMs) are increasingly pivotal in a wide range of natural language processing tasks. Access to pre-trained models, courtesy of the open-source community, has made it possible to adapt these models to specific applications for enhanced performance. However, the substantial resources required for training these models necessitate efficient solutions. This paper introduces CoLLiE, an efficient library that facilitates collaborative training of large language models using 3D... | Kai Lv, Shuo Zhang, Tianle Gu, Shuhao Xing, Jiawei Hong, Keyu Chen, Xiaoran Liu, Yuqing Yang, Honglin Guo, Tengxiao Liu, Yu Sun, Qipeng Guo, Hang Yan, Xipeng Qiu |  |
| 50 |  |  [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://doi.org/10.18653/v1/2023.emnlp-demo.49) |  | 0 | We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual... | Hang Zhang, Xin Li, Lidong Bing |  |
| 51 |  |  [SummHelper: Collaborative Human-Computer Summarization](https://doi.org/10.18653/v1/2023.emnlp-demo.50) |  | 0 | Current approaches for text summarization are predominantly automatic, with rather limited space for human intervention and control over the process. In this paper, we introduce SummHelper, and screencast demo at https://www.youtube.com/watch?v=nGcknJwGhxk a 2-phase summarization assistant designed to foster human-machine collaboration. The initial phase involves content selection, where the system recommends potential content, allowing users to accept, modify, or introduce additional... | Aviv Slobodkin, Niv Nachum, Shmuel Amar, Ori Shapira, Ido Dagan |  |
| 52 |  |  [ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.51) |  | 0 | Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent frameworks that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications,... | Chenliang Li, He Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, Hongzhu Shi, Ji Zhang, Fei Huang, Jingren Zhou |  |
| 53 |  |  [EfficientOCR: An Extensible, Open-Source Package for Efficiently Digitizing World Knowledge](https://doi.org/10.18653/v1/2023.emnlp-demo.52) |  | 0 | Billions of public domain documents remain trapped in hard copy or lack an accurate digitization. Modern natural language processing methods cannot be used to index, retrieve, and summarize their texts; conduct computational textual analyses; or extract information for statistical analyses, and these texts cannot be incorporated into language model training. Given the diversity and sheer quantity of public domain texts, liberating them at scale requires optical character recognition (OCR) that... | Tom Bryan, Jacob Carlson, Abhishek Arora, Melissa Dell |  |
| 54 |  |  [Frontmatter](https://aclanthology.org/2023.emnlp-industry.0) |  | 0 |  |  |  |
| 55 |  |  [BeautifulPrompt: Towards Automatic Prompt Engineering for Text-to-Image Synthesis](https://doi.org/10.18653/v1/2023.emnlp-industry.1) |  | 0 | Recently, diffusion-based deep generative models (e.g., Stable Diffusion) have shown impressive results in text-to-image synthesis. However, current text-to-image models often require multiple passes of prompt engineering by humans in order to produce satisfactory results for real-world applications. We propose BeautifulPrompt, a deep generative model to produce high-quality prompts from very simple raw descriptions, which enables diffusion-based models to generate more beautiful images. In our... | Tingfeng Cao, Chengyu Wang, Bingyan Liu, Ziheng Wu, Jinhui Zhu, Jun Huang |  |
| 56 |  |  [Enhancing Language Model with Unit Test Techniques for Efficient Regular Expression Generation](https://doi.org/10.18653/v1/2023.emnlp-industry.2) |  | 0 | Recent research has investigated the use of generative language models to produce regular expressions with semantic-based approaches. However, these approaches have shown shortcomings in practical applications, particularly in terms of functional correctness, which refers to the ability to reproduce the intended function inputs by the user. To address this issue, we present a novel method called Unit-Test Driven Reinforcement Learning (UTD-RL). Our approach differs from previous methods by... | Chenhui Mao, Xiexiong Lin, Xin Jin, Xin Zhang |  |
| 57 |  |  [A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models](https://doi.org/10.18653/v1/2023.emnlp-industry.3) |  | 0 | Large language models have become a vital component in modern NLP, achieving state of the art performance in a variety of tasks. However, they are often inefficient for real-world deployment due to their expensive inference costs. Knowledge distillation is a promising technique to improve their efficiency while retaining most of their effectiveness. In this paper, we reproduce, compare and analyze several representative methods for task-agnostic (general-purpose) distillation of Transformer... | Takuma Udagawa, Aashka Trivedi, Michele Merler, Bishwaranjan Bhattacharjee |  |
| 58 |  |  [Towards Effective Automatic Debt Collection with Persona Awareness](https://doi.org/10.18653/v1/2023.emnlp-industry.4) |  | 0 | Understanding debtor personas is crucial for collectors to empathize with debtors and develop more effective collection strategies. In this paper, we take the first step towards comprehensively investigating the significance of debtor personas and present a successful commercial practice on automatic debt collection agents. Specifically, we organize the debtor personas into a taxonomy and construct a persona-aware conversation dataset. Building upon it, we implement a simple yet effective... | Tong Zhang, Junhong Liu, Chen Huang, Jia Liu, Hongru Liang, Zujie Wen, Wenqiang Lei |  |
| 59 |  |  [Gatekeeper to save COGS and improve efficiency of Text Prediction](https://doi.org/10.18653/v1/2023.emnlp-industry.5) |  | 0 | The text prediction (TP) workflow calls a Large Language Model (LLM), almost, after every character to get subsequent sequence of characters, till user accepts a suggestion. The confidence score of the prediction is commonly used for filtering the results to ensure that only correct predictions are shown to user. As LLMs require massive amounts of computation and storage, such an approach incurs network and high execution cost. So, we propose a Model gatekeeper (GK) to stop the LLM calls that... | Nidhi Tiwari, Sneha Kola, Milos Milunovic, Siqing Chen, Marjan Slavkovski |  |
| 60 |  |  [Efficient Transformer Knowledge Distillation: A Performance Review](https://doi.org/10.18653/v1/2023.emnlp-industry.6) |  | 0 | As pretrained transformer language models continue to achieve state-of-the-art performance, the Natural Language Processing community has pushed for advances in model compression and efficient attention mechanisms to address high computational requirements and limited input sequence length. Despite these separate efforts, no investigation has been done into the intersection of these two fields. In this work, we provide an evaluation of model compression via knowledge distillation on efficient... | Nathan Brown, Ashton Williamson, Tahj Anderson, Logan Lawrence |  |
| 61 |  |  [CDD: A Large Scale Dataset for Legal Intelligence Research](https://doi.org/10.18653/v1/2023.emnlp-industry.7) |  | 0 | As an important application of Artificial Intelligence, legal intelligence has recently attracted the attention of many researchers. Previous works investigated diverse issues like predicting crimes, predicting outcomes of judicial debates, or extracting information/knowledge from various kinds of legal documents. Although many advances have been made, the research on supporting prediction of court judgments remains relatively scarce, while the lack of large-scale data resources limits the... | Changzhen Ji, Yating Zhang, Adam Jatowt, Haipang Wu |  |
| 62 |  |  [MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning](https://doi.org/10.18653/v1/2023.emnlp-industry.8) |  | 0 | In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source... | Noé Tits |  |
| 63 |  |  [Personalized Dense Retrieval on Global Index for Voice-enabled Conversational Systems](https://doi.org/10.18653/v1/2023.emnlp-industry.9) |  | 0 | Voice-controlled AI dialogue systems are susceptible to noise from phonetic variations and failure to resolve ambiguous entities. Typically, personalized entity resolution (ER) and/or query rewrites (QR) are deployed to recover from these error modes. Previous work in this field achieves personalization by constraining retrieval search space to personalized indices built from user’s historical interactions with the device. While constrained retrieval achieves high precision, predictions are... | Masha Belyi, Charlotte Dzialo, Chaitanya Dwivedi, Prajit Muppidi, Kanna Shimizu |  |
| 64 |  |  [Text2Topic: Multi-Label Text Classification System for Efficient Topic Detection in User Generated Content with Zero-Shot Capabilities](https://doi.org/10.18653/v1/2023.emnlp-industry.10) |  | 0 | Multi-label text classification is a critical task in the industry. It helps to extract structured information from large amount of textual data. We propose Text to Topic (Text2Topic), which achieves high multi-label classification performance by employing a Bi-Encoder Transformer architecture that utilizes concatenation, subtraction, and multiplication of embeddings on both text and topic. Text2Topic also supports zero-shot predictions, produces domain-specific text embeddings, and enables... | Fengjun Wang, Moran Beladev, Ofri Kleinfeld, Elina Frayerman, Tal Shachar, Eran Fainman, Karen Lastmann Assaraf, Sarai Mizrachi, Benjamin Wang |  |
| 65 |  |  [Deep Metric Learning to Hierarchically Rank - An Application in Product Retrieval](https://doi.org/10.18653/v1/2023.emnlp-industry.11) |  | 0 | Most e-commerce search engines use customer behavior signals to augment lexical matching and improve search relevance. Many e-commerce companies like Amazon, Alibaba, Ebay etc. operate in multiple countries with country specific stores. However, customer behavior data is sparse in newer stores. To compensate for sparsity of behavioral data in low traffic stores, search engines often use cross-listed products in some form. However, cross-listing across stores is not uniform and in many cases... | Kee Kiat Koo, Ashutosh Joshi, Nishaanth Reddy, Karim Bouyarmane, Ismail B. Tutar, Vaclav Petricek, Changhe Yuan |  |
| 66 |  |  [A Pretrained Language Model for Cyber Threat Intelligence](https://doi.org/10.18653/v1/2023.emnlp-industry.12) |  | 0 | We present a new BERT model for the cybersecurity domain, CTI-BERT, which can improve the accuracy of cyber threat intelligence (CTI) extraction, enabling organizations to better defend against potential cyber threats. We provide detailed information about the domain corpus collection, the training methodology and its effectiveness for a variety of NLP tasks for the cybersecurity domain. The experiments show that CTI-BERT significantly outperforms several general-domain and security-domain... | Youngja Park, Weiqiu You |  |
| 67 |  |  [SAMP: A Model Inference Toolkit of Post-Training Quantization for Text Processing via Self-Adaptive Mixed-Precision](https://doi.org/10.18653/v1/2023.emnlp-industry.13) |  | 0 | The latest industrial inference engines, such as FasterTransformer and TurboTransformers, have verified that half-precision floating point (FP16) and 8-bit integer (INT8) quantization can greatly improve model inference speed. However, the existing INT8 quantization methods are too complicated, and improper usage will lead to model performance damage greatly. In this paper, we develop a toolkit for users to easily quantize their models for inference, in which Self-Adaptive Mixed-Precision... | Rong Tian, Zijing Zhao, Weijie Liu, Haoyan Liu, Weiquan Mao, Zhe Zhao, Kan Zhou |  |
| 68 |  |  [KD-Boost: Boosting Real-Time Semantic Matching in E-commerce with Knowledge Distillation](https://doi.org/10.18653/v1/2023.emnlp-industry.14) |  | 0 | Real-time semantic matching is vital to web and product search. Transformer-based models have shown to be highly effective at encoding queries into an embedding space where semantically similar entities (queries or results) are in close proximity. However, the computational complexity of large transformer models limits their utilization for real-time matching. In this paper, we propose KD-Boost, a novel knowledge distillation algorithm designed for real-time semantic matching. KD-Boost trains... | Sanjay Agrawal, Vivek Sembium, Ankith M. S |  |
| 69 |  |  [Multi-teacher Distillation for Multilingual Spelling Correction](https://doi.org/10.18653/v1/2023.emnlp-industry.15) |  | 0 | Accurate spelling correction is a critical step in modern search interfaces, especially in an era of mobile devices and speech-to-text interfaces. For services that are deployed around the world, this poses a significant challenge for multilingual NLP: spelling errors need to be caught and corrected in all languages, and even in queries that use multiple languages. In this paper, we tackle this challenge using multi-teacher distillation. On our approach, a monolingual teacher model is trained... | Jingfen Zhang, Xuan Guo, Sravan Bodapati, Christopher Potts |  |
| 70 |  |  [Does Named Entity Recognition Truly Not Scale Up to Real-world Product Attribute Extraction?](https://doi.org/10.18653/v1/2023.emnlp-industry.16) |  | 0 | The key challenge in the attribute-value extraction (AVE) task from e-commerce sites is the scalability to diverse attributes for a large number of products in real-world e-commerce sites. To make AVE scalable to diverse attributes, recent researchers adopted a question-answering (QA)-based approach that additionally inputs the target attribute as a query to extract its values, and confirmed its advantage over a classical approach based on named-entity recognition (NER) on real-word e-commerce... | WeiTe Chen, Keiji Shinzato, Naoki Yoshinaga, Yandi Xia |  |
| 71 |  |  [Investigating Table-to-Text Generation Capabilities of Large Language Models in Real-World Information Seeking Scenarios](https://doi.org/10.18653/v1/2023.emnlp-industry.17) |  | 0 | Tabular data is prevalent across various industries, necessitating significant time and effort for users to understand and manipulate for their information-seeking purposes. The advancements in large language models (LLMs) have shown enormous potential to improve user efficiency. However, the adoption of LLMs in real-world applications for table information seeking remains underexplored. In this paper, we investigate the table-to-text capabilities of different LLMs using four datasets within... | Yilun Zhao, Haowei Zhang, Shengyun Si, Linyong Nan, Xiangru Tang, Arman Cohan |  |
| 72 |  |  [TMID: A Comprehensive Real-world Dataset for Trademark Infringement Detection in E-Commerce](https://doi.org/10.18653/v1/2023.emnlp-industry.18) |  | 0 | Annually, e-commerce platforms incur substantial financial losses due to trademark infringements, making it crucial to identify and mitigate potential legal risks tied to merchant information registered to the platforms. However, the absence of high-quality datasets hampers research in this area. To address this gap, our study introduces TMID, a novel dataset to detect trademark infringement in merchant registrations. This is a real-world dataset sourced directly from Alipay, one of the world’s... | Tongxin Hu, Zhuang Li, Xin Jin, Lizhen Qu, Xin Zhang |  |
| 73 |  |  [Joint Dialogue Topic Segmentation and Categorization: A Case Study on Clinical Spoken Conversations](https://doi.org/10.18653/v1/2023.emnlp-industry.19) |  | 0 | Utilizing natural language processing techniques in clinical conversations is effective to improve the efficiency of health management workflows for medical staff and patients. Dialogue segmentation and topic categorization are two fundamental steps for processing verbose spoken conversations and highlighting informative spans for downstream tasks. However, in practical use cases, due to the variety of segmentation granularity and topic definition, and the lack of diverse annotated corpora, no... | Zhengyuan Liu, Siti Umairah Md. Salleh, Hong Choon Oh, Pavitra Krishnaswamy, Nancy F. Chen |  |
| 74 |  |  [AdapterDistillation: Non-Destructive Task Composition with Knowledge Distillation](https://doi.org/10.18653/v1/2023.emnlp-industry.20) |  | 0 | Leveraging knowledge from multiple tasks through introducing a small number of task specific parameters into each transformer layer, also known as adapters, receives much attention recently. However, adding an extra fusion layer to implement knowledge composition not only increases the inference time but also is non-scalable for some applications. To avoid these issues, we propose a two-stage knowledge distillation algorithm called AdapterDistillation. In the first stage, we extract task... | Junjie Wang, Yicheng Chen, Wangshu Zhang, Sen Hu, Teng Xu, Jing Zheng |  |
| 75 |  |  [PROMINET: Prototype-based Multi-View Network for Interpretable Email Response Prediction](https://doi.org/10.18653/v1/2023.emnlp-industry.21) |  | 0 | Email is a widely used tool for business communication, and email marketing has emerged as a cost-effective strategy for enterprises. While previous studies have examined factors affecting email marketing performance, limited research has focused on understanding email response behavior by considering email content and metadata. This study proposes a Prototype-based Multi-view Network (PROMINET) that incorporates semantic and structural information from email data. By utilizing prototype... | Yuqing Wang, Prashanth Vijayaraghavan, Ehsan Degan |  |
| 76 |  |  [Retrieval-Enhanced Dual Encoder Training for Product Matching](https://doi.org/10.18653/v1/2023.emnlp-industry.22) |  | 0 | Product matching is the task of matching a seller-listed item to an appropriate product. It is a critical task for an e-commerce platform, and the approach needs to be efficient to run in a large-scale setting. A dual encoder approach has been a common practice for product matching recently, due to its high performance and computation efficiency. In this paper, we propose a two-stage training for the dual encoder model. Stage 1 trained a dual encoder to identify the more informative training... | Justin Chiu |  |
| 77 |  |  [WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-industry.23) |  | 0 | This paper introduces WordArt Designer, a user-driven framework for artistic typography synthesis, relying on the Large Language Model (LLM). The system incorporates four key modules: the LLM Engine, SemTypo, StyTypo, and TexTypo modules. 1) The LLM Engine, empowered by the LLM (e.g. GPT-3.5), interprets user inputs and generates actionable prompts for the other modules, thereby transforming abstract concepts into tangible designs. 2) The SemTypo module optimizes font designs using semantic... | JunYan He, ZhiQi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Xianhui Lin, Xiaoyang Kang, Zengke Jin, Yusen Hu, Bin Luo, Yifeng Geng, Xuansong Xie |  |
| 78 |  |  [Lattice Path Edit Distance: A Romanization-aware Edit Distance for Extracting Misspelling-Correction Pairs from Japanese Search Query Logs](https://doi.org/10.18653/v1/2023.emnlp-industry.24) |  | 0 | Edit distance has been successfully used to extract training data, i.e., misspelling-correction pairs, of spelling correction models from search query logs in languages including English. However, the success does not readily apply to Japanese, where misspellings are often dissimilar to correct spellings due to the romanization-based input methods. To address this problem, we introduce lattice path edit distance, which utilizes romanization lattices to efficiently consider all possible... | Nobuhiro Kaji |  |
| 79 |  |  [Learning Multilingual Sentence Representations with Cross-lingual Consistency Regularization](https://doi.org/10.18653/v1/2023.emnlp-industry.25) |  | 0 | Multilingual sentence representations are the foundation for similarity-based bitext mining, which is crucial for scaling multilingual neural machine translation (NMT) system to more languages. In this paper, we introduce MuSR: a one-for-all Multilingual Sentence Representation model that supports 223 languages. Leveraging billions of English-centric parallel corpora, we train a multilingual Transformer encoder, coupled with an auxiliary Transformer decoder, by adopting a multilingual NMT... | Pengzhi Gao, Liwen Zhang, Zhongjun He, Hua Wu, Haifeng Wang |  |
| 80 |  |  [Unveiling Identity Biases in Toxicity Detection : A Game-Focused Dataset and Reactivity Analysis Approach](https://doi.org/10.18653/v1/2023.emnlp-industry.26) |  | 0 | Identity biases arise commonly from annotated datasets, can be propagated in language models and can cause further harm to marginal groups. Existing bias benchmarking datasets are mainly focused on gender or racial biases and are made to pinpoint which class the model is biased towards. They also are not designed for the gaming industry, a concern for models built for toxicity detection in videogames’ chat. We propose a dataset and a method to highlight oversensitive terms using reactivity... | Josiane Van Dorpe, Zachary Yang, Nicolas GrenonGodbout, Grégoire Winterstein |  |
| 81 |  |  [ORANGE: Text-video Retrieval via Watch-time-aware Heterogeneous Graph Contrastive Learning](https://doi.org/10.18653/v1/2023.emnlp-industry.27) |  | 0 | With the explosive growth of short-video data on industrial video-sharing platforms such as TikTok and YouTube, text-video retrieval techniques have become increasingly important. Most existing works for text-video retrieval focus on designing informative representation learning methods and delicate matching mechanisms, which leverage the content information of queries and videos themselves (i.e., textual information of queries and multimodal information of videos). However, real-world... | Yucheng Lin, Tim Chang, Yaning Chang, Jianqiang Ma, Donghui Li, Ting Peng, Zang Li, Zhiyi Zhou, Feng Wang |  |
| 82 |  |  [Compute-Efficient Churn Reduction for Conversational Agents](https://doi.org/10.18653/v1/2023.emnlp-industry.28) |  | 0 | Model churn occurs when re-training a model yields different predictions despite using the same data and hyper-parameters. Churn reduction is crucial for industry conversational systems where users expect consistent results for the same queries. In this setting, compute resources are often limited due to latency requirements during serving and overall time constraints during re-training. To address this issue, we propose a compute-efficient method that mitigates churn without requiring extra... | Christopher Hidey, Sarthak Sarthak |  |
| 83 |  |  [Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering](https://doi.org/10.18653/v1/2023.emnlp-industry.29) |  | 0 | Large Language Model (LLM) has gained popularity and achieved remarkable results in open-domain tasks, but its performance in real industrial domain-specific scenarios is average due to its lack of specific domain knowledge. This issue has attracted widespread attention, but there are few relevant benchmarks available. In this paper, we provide a benchmark Question Answering (QA) dataset named MSQA, centered around Microsoft products and IT technical problems encountered by customers. This... | Fangkai Yang, Pu Zhao, Zezhong Wang, Lu Wang, Bo Qiao, Jue Zhang, Mohit Garg, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang |  |
| 84 |  |  [Enhancing Extreme Multi-Label Text Classification: Addressing Challenges in Model, Data, and Evaluation](https://doi.org/10.18653/v1/2023.emnlp-industry.30) |  | 0 | Extreme multi-label text classification is a prevalent task in industry, but it frequently encounters challenges in terms of machine learning perspectives, including model limitations, data scarcity, and time-consuming evaluation. This paper aims to mitigate these issues by introducing novel approaches. Firstly, we propose a label ranking model as an alternative to the conventional SciBERT-based classification model, enabling efficient handling of large-scale labels and accommodating new... | Dan Li, Zi Long Zhu, Janneke van de Loo, Agnes Masip Gomez, Vikrant Yadav, Georgios Tsatsaronis, Zubair Afzal |  |
| 85 |  |  [Query-aware Multi-modal based Ranking Relevance in Video Search](https://doi.org/10.18653/v1/2023.emnlp-industry.31) |  | 0 | Relevance ranking system plays a crucial role in video search on streaming platforms. Most relevance ranking methods focus on text modality, incapable of fully exploiting cross-modal cues present in video. Recent multi-modal models have demonstrated promise in various vision-language tasks but provide limited help for downstream query-video relevance tasks due to the discrepency between relevance ranking-agnostic pre-training objectives and the real video search scenarios that demand... | Chengcan Ye, Ting Peng, Tim Chang, Zhiyi Zhou, Feng Wang |  |
| 86 |  |  [Coordinated Replay Sample Selection for Continual Federated Learning](https://doi.org/10.18653/v1/2023.emnlp-industry.32) |  | 0 | Continual Federated Learning (CFL) combines Federated Learning (FL), the decentralized learning of a central model on a number of client devices that may not communicate their data, and Continual Learning (CL), the learning of a model from a continual stream of data without keeping the entire history. In CL, the main challenge is forgetting what was learned from past data. While replay-based algorithms that keep a small pool of past training data are effective to reduce forgetting, only simple... | Jack Good, Jimit Majmudar, Christophe Dupuy, Jixuan Wang, Charith Peris, Clement Chung, Richard S. Zemel, Rahul Gupta |  |
| 87 |  |  [Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective](https://doi.org/10.18653/v1/2023.emnlp-industry.33) |  | 0 | This paper studies how to effectively build meeting summarization systems for real-world usage using large language models (LLMs). For this purpose, we conduct an extensive evaluation and comparison of various closed-source and open-source LLMs, namely, GPT-4, GPT-3.5, PaLM-2, and LLaMA-2. Our findings reveal that most closed-source LLMs are generally better in terms of performance. However, much smaller open-source models like LLaMA-2 (7B and 13B) could still achieve performance comparable to... | Md. Tahmid Rahman Laskar, XueYong Fu, Cheng Chen, Shashi Bhushan TN |  |
| 88 |  |  [Creator Context for Tweet Recommendation](https://doi.org/10.18653/v1/2023.emnlp-industry.34) |  | 0 | When discussing a tweet, people usually not only refer to the content it delivers, but also to the person behind the tweet. In other words, grounding the interpretation of the tweet in the context of its creator plays an important role in deciphering the true intent and the importance of the tweet. In this paper, we attempt to answer the question of how creator context should be used to advance tweet understanding. Specifically, we investigate the usefulness of different types of creator... | Spurthi Amba Hombaiah, Tao Chen, Mingyang Zhang, Michael Bendersky, Marc Najork, Matt Colen, Sergey Levi, Vladimir Ofitserov, Tanvir Amin |  |
| 89 |  |  [AdaBERT-CTC: Leveraging BERT-CTC for Text-Only Domain Adaptation in ASR](https://doi.org/10.18653/v1/2023.emnlp-industry.35) |  | 0 | End-to-end (E2E) automatic speech recognition (ASR) models are becoming increasingly popular in commercial applications, such as virtual assistants, closed captioning, and dictation systems. The accuracy of the ASR is crucial to their success. However, E2E models still struggle to recognize out-of-domain words such as proper nouns and domain-specific terms. In this paper we introduce AdaBERT-CTC, a domain adaptation technique that relies solely on textual data. Our method allows for text-only... | Tyler Vuong, Karel Mundnich, Dhanush Bekal, Veera Raghavendra Elluru, Srikanth Ronanki, Sravan Bodapati |  |
| 90 |  |  [Conversing with databases: Practical Natural Language Querying](https://doi.org/10.18653/v1/2023.emnlp-industry.36) |  | 0 | In this work, we designed, developed and released in production DataQue – a hybrid NLQ (Natural Language Querying) system for conversational DB querying. We address multiple practical problems that are not accounted for in public Text-to-SQL solutions – numerous complex implied conditions in user questions, jargon and abbreviations, custom calculations, non-SQL operations, a need to inject all those into pipeline fast and to have guaranteed parsing results for demanding users, cold-start... | Denis Kochedykov, Fenglin Yin, Sreevidya Khatravath |  |
| 91 |  |  [AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications](https://doi.org/10.18653/v1/2023.emnlp-industry.37) |  | 0 | Adversarially testing large language models (LLMs) is crucial for their safe and responsible deployment in practice. We introduce an AI-assisted approach for automated generation of adversarial evaluation datasets to test the safety of LLM generations on new downstream applications. We call it AART AI-assisted Red-Teaming - an automated alternative to current manual red-teaming efforts. AART offers a data generation and augmentation pipeline of reusable and customizable recipes that reduce... | Bhaktipriya Radharapu, Kevin Robinson, Lora Aroyo, Preethi Lahoti |  |
| 92 |  |  [Speakerly: A Voice-based Writing Assistant for Text Composition](https://doi.org/10.18653/v1/2023.emnlp-industry.38) |  | 0 | We present Speakerly, a new real-time voice-based writing assistance system that helps users with text composition across various use cases such as emails, instant messages, and notes. The user can interact with the system through instructions or dictation, and the system generates a well-formatted and coherent document. We describe the system architecture and detail how we address the various challenges while building and deploying such a system at scale. More specifically, our system uses a... | Dhruv Kumar, Vipul Raheja, Alice KaiserSchatzlein, Robyn Perry, Apurva Joshi, Justin HuguesNuger, Samuel Lou, Navid Chowdhury |  |
| 93 |  |  [Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks](https://doi.org/10.18653/v1/2023.emnlp-industry.39) |  | 0 | The most recent large language models (LLMs) such as ChatGPT and GPT-4 have shown exceptional capabilities of generalist models, achieving state-of-the-art performance on a wide range of NLP tasks with little or no adaptation. How effective are such models in the finance domain? Understanding this basic question would have a significant impact on many downstream financial analytical tasks. In this paper, we conduct empirical studies and provide experimental evidences of their performance on a... | Xianzhi Li, Samuel Chan, Xiaodan Zhu, Yulong Pei, Zhiqiang Ma, Xiaomo Liu, Sameena Shah |  |
| 94 |  |  [CL-QR: Cross-Lingual Enhanced Query Reformulation for Multi-lingual Conversational AI Agents](https://doi.org/10.18653/v1/2023.emnlp-industry.40) |  | 0 | The growing popularity of conversational AI agents such as Alexa, Google Assistant, and Siri rely on accurate spoken language comprehension. The query reformulation (QR) method, which reformulates defective user queries, has been broadly adopted to mitigate the challenges posed by understanding user’s intent from imperfect spoken recognition result. However, due to the scarcity of non-English QR labels, providing high-quality QR for non-English users still remains a challenge. This work... | Zhongkai Sun, Zhengyang Zhao, Sixing Lu, Chengyuan Ma, Xiaohu Liu, Xing Fan, Wei Shen, Chenlei Guo |  |
| 95 |  |  [Improving Contextual Query Rewrite for Conversational AI Agents through User-preference Feedback Learning](https://doi.org/10.18653/v1/2023.emnlp-industry.41) |  | 0 | Contextual query rewriting (CQR) is a crucial component in Conversational AI agents, leveraging the contextual information from previous user-agent conversations to improve the comprehension of current user intent. However, traditional CQR methods often concentrate on supervised fine-tuning only, neglecting the opportunities to learn from user feedback to align with user preferences. Inspired by recent advances in learning from human feedback (LHF), this paper proposes a novel Preference... | Zhongkai Sun, Yingxue Zhou, Jie Hao, Xing Fan, Yanbin Lu, Chengyuan Ma, Wei Shen, Chenlei Guo |  |
| 96 |  |  [Scaling Neural ITN for Numbers and Temporal Expressions in Tamil: Findings for an Agglutinative Low-resource Language](https://doi.org/10.18653/v1/2023.emnlp-industry.42) |  | 0 | ITN involves rewriting the verbalised form of text from spoken transcripts to its corresponding written form. The task inherently expects challenges in identifying ITN entries due to spelling variations in words arising out of dialects, transcription errors etc. Additionally, in Tamil, word boundaries between adjacent words in a sentence often get obscured due to Punarchi, i.e. phonetic transformation of these boundaries. Being morphologically rich, the words in Tamil show a high degree of... | Bhavuk Singhal, Sindhuja Gopalan, Amrith Krishna, Malolan Chetlur |  |
| 97 |  |  [EELBERT: Tiny Models through Dynamic Embeddings](https://doi.org/10.18653/v1/2023.emnlp-industry.43) |  | 0 | We introduce EELBERT, an approach for compression of transformer-based models (e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is achieved by replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations. Since the input embedding layer occupies a large portion of the model size, especially for the smaller BERT variants, replacing this layer with an embedding computation function helps us reduce the model size significantly.... | Gabrielle Cohn, Rishika Agarwal, Deepanshu Gupta, Siddharth Patwardhan |  |
| 98 |  |  [Gold Standard Bangla OCR Dataset: An In-Depth Look at Data Preprocessing and Annotation Processes](https://doi.org/10.18653/v1/2023.emnlp-industry.44) |  | 0 | This research paper focuses on developing an improved Bangla Optical Character Recognition (OCR) system, addressing the challenges posed by the complexity of Bangla text structure, diverse handwriting styles, and the scarcity of comprehensive datasets. Leveraging recent advancements in Deep Learning and OCR techniques, we anticipate a significant enhancement in the performance of Bangla OCR by utilizing a large and diverse collection of labeled Bangla text image datasets. This study introduces... | Hasmot Ali, AKM Shahariar Azad Rabby, Md. Majedul Islam, A. k. m Mahamud, Nazmul Hasan, Fuad Rahman |  |
| 99 |  |  [PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching](https://doi.org/10.18653/v1/2023.emnlp-industry.45) |  | 0 | Instruction fine-tuning has conventionally been employed to adapt Large Language Models (LLMs) to a variety of diverse tasks. Nonetheless, this technique often necessitates substantial computational resources, making it impractical for deployment by individuals or small-scale entities. Recently, Low-Rank Adaptation (LoRA) has become a promising alternative, offering tuning capabilities with reduced resource overhead. However, attaining satisfactory performance through the fine-tuning of LoRA is... | Zhenting Qi, Xiaoyu Tan, Shaojie Shi, Chao Qu, Yinghui Xu, Yuan Qi |  |
| 100 |  |  [Welcome to the Real World: Efficient, Incremental and Scalable Key Point Analysis](https://doi.org/10.18653/v1/2023.emnlp-industry.46) |  | 0 | Key Point Analysis (KPA) is an emerging summarization framework, which extracts the main points from a collection of opinions, and quantifies their prevalence. It has been successfully applied to diverse types of data, including arguments, user reviews and survey responses. Despite the growing academic interest in KPA, little attention has been given to the practical challenges of implementing a KPA system in production. This work presents a deployed KPA system, which regularly serves multiple... | Lilach Eden, Yoav Kantor, Matan Orbach, Yoav Katz, Noam Slonim, Roy BarHaim |  |
| 101 |  |  [Automatic Linking of Judgements to UK Supreme Court Hearings](https://doi.org/10.18653/v1/2023.emnlp-industry.47) |  | 0 | One the most important archived legal material in the UK is the Supreme Court published judgements and video recordings of court sittings for the decided cases. The impact of Supreme Court published material extends far beyond the parties involved in any given case as it provides landmark rulings on arguable points of law of the greatest public and constitutional importance. However, the recordings of a case are usually very long which makes it both time and effort consuming for legal... | Hadeel Saadany, Constantin Orasan |  |
| 102 |  |  [Automatic Marketing Theme and Commodity Construction System for E-commerce](https://doi.org/10.18653/v1/2023.emnlp-industry.48) |  | 0 | When consumers’ shopping needs are concentrated, they are more interested in the collection of commodities under the specific marketing theme. Therefore, mining marketing themes and their commodities collections can help customers save shopping costs and improve user clicks and purchases for recommendation system. However, the current system invites experts to write marketing themes and select the relevant commodities, which suffer from difficulty in mass production, poor timeliness and low... | Zhiping Wang, Peng Lin, Hainan Zhang, Hongshen Chen, Tianhao Li, Zhuoye Ding, Sulong Xu, Jinghe Hu |  |
| 103 |  |  [Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures](https://doi.org/10.18653/v1/2023.emnlp-industry.49) |  | 0 | This paper introduces a new IncidentAI dataset for safety prevention. Different from prior corpora that usually contain a single task, our dataset comprises three tasks: named entity recognition, cause-effect extraction, and information retrieval. The dataset is annotated by domain experts who have at least six years of practical experience as high-pressure gas conservation managers. We validate the contribution of the dataset in the scenario of safety prevention. Preliminary results on the... | Shumpei Inoue, MinhTien Nguyen, Hiroki Mizokuchi, TuanAnh D. Nguyen, HuuHiep Nguyen, Dung Le |  |
| 104 |  |  [An Auxiliary Task Boosted Multi-task Learning Method for Service Account Retrieval with Limited Human Annotation](https://doi.org/10.18653/v1/2023.emnlp-industry.50) |  | 0 | Service accounts, including organizations’ official accounts and mini-programs, provide various convenient services for users, and have become crucial components of a number of applications. Therefore, retrieving service accounts quickly and accurately is vital. However, this task suffers from the problem of limited human annotation, i.e., manually assessing account functionality and assigning ratings based on user experience is both labor-intensive and time-consuming. To this end, this paper... | Yuanzhou Yao, Zhao Zhang, Kaijia Yang, Huasheng Liang, Qiang Yan, Yongjun Xu |  |
| 105 |  |  [VKIE: The Application of Key Information Extraction on Video Text](https://doi.org/10.18653/v1/2023.emnlp-industry.51) |  | 0 | Extracting structured information from videos is critical for numerous downstream applications in the industry. In this paper, we define a significant task of extracting hierarchical key information from visual texts on videos. To fulfill this task, we decouple it into four subtasks and introduce two implementation solutions called PipVKIE and UniVKIE. PipVKIE sequentially completes the four subtasks in continuous stages, while UniVKIE is improved by unifying all the subtasks into one backbone.... | Siyu An, Ye Liu, Haoyuan Peng, Di Yin |  |
| 106 |  |  [Investigating the Role and Impact of Disfluency on Summarization](https://doi.org/10.18653/v1/2023.emnlp-industry.52) |  | 0 | Contact centers handle both chat and voice calls for the same domain. As part of their workflow, it is a standard practice to summarize the conversations once they conclude. A significant distinction between chat and voice communication lies in the presence of disfluencies in voice calls, such as repetitions, restarts, and replacements. These disfluencies are generally considered noise for downstream natural language understanding (NLU) tasks. While a separate summarization model for voice... | Varun Nathan, Ayush Kumar, Jithendra Vepa |  |
| 107 |  |  [InsightNet : Structured Insight Mining from Customer Feedback](https://doi.org/10.18653/v1/2023.emnlp-industry.53) |  | 0 | We propose InsightNet, a novel approach for the automated extraction of structured insights from customer reviews. Our end-to-end machine learning framework is designed to overcome the limitations of current solutions, including the absence of structure for identified topics, non-standard aspect names, and lack of abundant training data. The proposed solution builds a semi-supervised multi-level taxonomy from raw reviews, a semantic similarity heuristic approach to generate labelled data and... | Sandeep Sricharan Mukku, Manan Soni, Chetan Aggarwal, Jitenkumar Rana, Promod Yenigalla, Rashmi Patange, Shyam Mohan |  |
| 108 |  |  [E2E Spoken Entity Extraction for Virtual Agents](https://doi.org/10.18653/v1/2023.emnlp-industry.54) |  | 0 | In human-computer conversations, extracting entities such as names, street addresses and email addresses from speech is a challenging task. In this paper, we study the impact of fine-tuning pre-trained speech encoders on extracting spoken entities in human-readable form directly from speech without the need for text transcription. We illustrate that such a direct approach optimizes the encoder to transcribe only the entity relevant portions of speech ignoring the superfluous portions such as... | Karan Singla, YeonJun Kim, Srinivas Bangalore |  |
| 109 |  |  [Generative Models for Product Attribute Extraction](https://doi.org/10.18653/v1/2023.emnlp-industry.55) |  | 0 | Product attribute extraction is an emerging field in information extraction and e-commerce, with applications including knowledge base construction, product recommendation, and enhancing customer experiences. In this work, we explore the use of generative models for product attribute extraction. We analyze their utility with hard and soft prompting methods, and demonstrate their ability to generate implicit attribute values, which state-of-the-art sequence tagging models are unable to extract.... | Ansel Blume, Nasser Zalmout, Heng Ji, Xian Li |  |
| 110 |  |  [CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering](https://doi.org/10.18653/v1/2023.emnlp-industry.56) |  | 0 | Large language models (LLMs) have demonstrated remarkable performance by following natural language instructions without fine-tuning them on domain-specific tasks and data. However, leveraging LLMs for domain-specific question answering suffers from severe limitations. The generated answer tends to hallucinate due to the training data collection time (when using off-the-shelf), complex user utterance and wrong retrieval (in retrieval-augmented generation). Furthermore, due to the lack of... | Md. Rashad Al Hasan Rony, Christian Suess, Sinchana Ramakanth Bhat, Viju Sudhi, Julia Schneider, Maximilian Vogel, Roman Teucher, Ken E. Friedl, Soumya R. Sahoo |  |
| 111 |  |  [BUSTER: a "BUSiness Transaction Entity Recognition" dataset](https://doi.org/10.18653/v1/2023.emnlp-industry.57) |  | 0 | Albeit Natural Language Processing has seen major breakthroughs in the last few years, transferring such advances into real-world business cases can be challenging. One of the reasons resides in the displacement between popular benchmarks and actual data. Lack of supervision, unbalanced classes, noisy data and long documents often affect real problems in vertical domains such as finance, law and health. To support industry-oriented research, we present BUSTER, a BUSiness Transaction Entity... | Andrea Zugarini, Andrew Zamai, Marco Ernandes, Leonardo Rigutini |  |
| 112 |  |  [Multi-word Tokenization for Sequence Compression](https://doi.org/10.18653/v1/2023.emnlp-industry.58) |  | 0 | Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this paper, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence... | Leonidas Gee, Leonardo Rigutini, Marco Ernandes, Andrea Zugarini |  |
| 113 |  |  [JarviX: A LLM No code Platform for Tabular Data Analysis and Optimization](https://doi.org/10.18653/v1/2023.emnlp-industry.59) |  | 0 | In this study, we introduce JarviX, a sophisticated data analytics framework. JarviX is designed to employ Large Language Models (LLMs) to facilitate an automated guide and execute high-precision data analyzes on tabular datasets. This framework emphasizes the significance of varying column types, capitalizing on state-of-the-art LLMs to generate concise data insight summaries, propose relevant analysis inquiries, visualize data effectively, and provide comprehensive explanations for results... | Shangching Liu, Shengkun Wang, Tsungyao Chang, Wenqi Lin, ChungWei Hsiung, YiChen Hsieh, YuPing Cheng, SianHong Luo, Jianwei Zhang |  |
| 114 |  |  [Retrieve and Copy: Scaling ASR Personalization to Large Catalogs](https://doi.org/10.18653/v1/2023.emnlp-industry.60) |  | 0 | Personalization of automatic speech recognition (ASR) models is a widely studied topic because of its many practical applications. Most recently, attention-based contextual biasing techniques are used to improve the recognition of rare words and/or domain specific entities. However, due to performance constraints, the biasing is often limited to a few thousand entities, restricting real-world usability. To address this, we first propose a “Retrieve and Copy” mechanism to improve latency while... | Sai Muralidhar Jayanthi, Devang Kulshreshtha, Saket Dingliwal, Srikanth Ronanki, Sravan Bodapati |  |
| 115 |  |  [STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants](https://doi.org/10.18653/v1/2023.emnlp-industry.61) |  | 0 | In the context of a voice assistant system, steering refers to the phenomenon in which a user issues a follow-up command attempting to direct or clarify a previous turn. We propose STEER, a steering detection model that predicts whether a follow-up turn is a user’s attempt to steer the previous command. Constructing a training dataset for steering use cases poses challenges due to the cold-start problem. To overcome this, we developed heuristic rules to sample opt-in usage data, approximating... | Leon Liyang Zhang, Jiarui Lu, Joel Ruben Antony Moniz, Aditya Kulkarni, Dhivya Piraviperumal, Tien Dung Tran, Nick Tzou, Hong Yu |  |
| 116 |  |  [Self-Criticism: Aligning Large Language Models with their Understanding of Helpfulness, Honesty, and Harmlessness](https://doi.org/10.18653/v1/2023.emnlp-industry.62) |  | 0 | Recently, there has been a notable surge in the significance of large language models (LLMs) that engage in conversational-style interactions, such as ChatGPT and Claude, as they contribute significantly to the progress of artificial general intelligence (AGI). Typically, these models undergo a two-phase fine-tuning process: instruction fine-tuning (IF) and reinforcement learning from human feedback (RLHF). These methods aim to align the LLMs to be helpful, honest, and harmless (HHH). However,... | Xiaoyu Tan, Shaojie Shi, Xihe Qiu, Chao Qu, Zhenting Qi, Yinghui Xu, Yuan Qi |  |
| 117 |  |  [InstructPTS: Instruction-Tuning LLMs for Product Title Summarization](https://doi.org/10.18653/v1/2023.emnlp-industry.63) |  | 0 | E-commerce product catalogs contain billions of items. Most products have lengthy titles, as sellers pack them with product attributes to improve retrieval, and highlight key product aspects. This results in a gap between such unnatural products titles, and how customers refer to them. It also limits how e-commerce stores can use these seller-provided titles for recommendation, QA, or review summarization. Inspired by recent work on instruction-tuned LLMs, we present InstructPTS, a controllable... | Besnik Fetahu, Zhiyu Chen, Oleg Rokhlenko, Shervin Malmasi |  |
| 118 |  |  [LLM4Vis: Explainable Visualization Recommendation using ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-industry.64) |  | 0 | Data visualization is a powerful tool for exploring and communicating insights in various domains. To automate visualization choice for datasets, a task known as visualization recommendation has been proposed. Various machine-learning-based approaches have been developed for this purpose, but they often require a large corpus of dataset-visualization pairs for training and lack natural explanations for their results. To address this research gap, we propose LLM4Vis, a novel ChatGPT-based... | Lei Wang, Songheng Zhang, Yun Wang, EePeng Lim, Yong Wang |  |
| 119 |  |  [DUBLIN: Visual Document Understanding By Language-Image Network](https://doi.org/10.18653/v1/2023.emnlp-industry.65) |  | 0 | In this paper, we present DUBLIN, a pixel-based model for visual document understanding that does not rely on OCR. DUBLIN can process both images and texts in documents just by the pixels and handle diverse document types and tasks. DUBLIN is pretrained on a large corpus of document images with novel tasks that enhance its visual and linguistic abilities. We evaluate DUBLIN on various benchmarks and show that it achieves state-of-the-art performance on extractive tasks such as DocVQA, InfoVQA,... | Kriti Aggarwal, Aditi Khandelwal, Kumar Tanmay, Owais Khan Mohammed, Qiang Liu, Monojit Choudhury, Hardik Hansrajbhai Chauhan, Subhojit Som, Vishrav Chaudhary, Saurabh Tiwary |  |
| 120 |  |  [DocumentNet: Bridging the Data Gap in Document Pre-training](https://doi.org/10.18653/v1/2023.emnlp-industry.66) |  | 0 | Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to... | Lijun Yu, Jin Miao, Xiaoyu Sun, Jiayi Chen, Alexander G. Hauptmann, Hanjun Dai, Wei Wei |  |
| 121 |  |  [Relevance-assisted Generation for Robust Zero-shot Retrieval](https://doi.org/10.18653/v1/2023.emnlp-industry.67) |  | 0 | Zero-shot retrieval tasks such as the BEIR benchmark reveal out-of-domain generalization as a key weakness of high-performance dense retrievers. As a solution, domain adaptation for dense retrievers has been actively studied. A notable approach is synthesizing domain-specific data, by generating pseudo queries (PQ), for fine-tuning with domain-specific relevance between PQ and documents. Our contribution is showing that key biases can cause sampled PQ to be irrelevant, negatively contributing... | Jihyuk Kim, Minsoo Kim, Joonsuk Park, Seungwon Hwang |  |
| 122 |  |  [Too much of product information : Don't worry, let's look for evidence!](https://doi.org/10.18653/v1/2023.emnlp-industry.68) |  | 0 | Product question answering (PQA) aims to provide an instant response to customer questions posted on shopping message boards, social media, brand websites and retail stores. In this paper, we propose a distantly supervised solution to answer customer questions by using product information. Auto-answering questions using product information poses two main challenges:(i) labelled data is not readily available (ii)lengthy product information requires attending to various parts of the text to... | Aryan Jain, Jitenkumar Rana, Chetan Aggarwal |  |
| 123 |  |  [Harnessing LLMs for Temporal Data - A Study on Explainable Financial Time Series Forecasting](https://doi.org/10.18653/v1/2023.emnlp-industry.69) |  | 0 | Applying machine learning to financial time series has been an active area of industrial research enabling innovation in market insights, risk management, strategic decision-making, and policy formation. This paper explores the novel use of Large Language Models (LLMs) for explainable financial time series forecasting, addressing challenges in cross-sequence reasoning, multi-modal data integration, and result interpretation that are inherent in traditional approaches. Focusing on NASDAQ-100... | Xinli Yu, Zheng Chen, Yanbin Lu |  |
| 124 |  |  [ViGPTQA - State-of-the-Art LLMs for Vietnamese Question Answering: System Overview, Core Models Training, and Evaluations](https://doi.org/10.18653/v1/2023.emnlp-industry.70) |  | 0 | Large language models (LLMs) and their applications in low-resource languages (such as in Vietnamese) are limited due to lack of training data and benchmarking datasets. This paper introduces a practical real-world implementation of a question answering system for Vietnamese, called ViGPTQA, leveraging the power of LLM. Since there is no effective LLM in Vietnamese to date, we also propose, evaluate, and open-source an instruction-tuned LLM for Vietnamese, named ViGPT. ViGPT demonstrates... | Minh Thuan Nguyen, KhanhTung Tran, NhuVan Nguyen, XuanSon Vu |  |
| 125 |  |  [An Integrated Search System for Korea Weather Data](https://doi.org/10.18653/v1/2023.emnlp-industry.71) |  | 0 | We introduce WeatherSearch, an integrated search system deployed at the Korea Meteorological Administration (KMA). WeatherSearch enables users to retrieve all the relevant data for weather forecasting from a massive weather database with simple natural language queries. We carefully design and conduct multiple expert surveys and interviews for template creation and apply data augmentation techniques including template filling to collect 4 million data points with minimal human labors. We then... | Jinkyung Jo, Dayeon Ki, Soyoung Yoon, Minjoon Seo |  |
| 126 |  |  [Adaptive Hyper-parameter Learning for Deep Semantic Retrieval](https://doi.org/10.18653/v1/2023.emnlp-industry.72) |  | 0 | Deep semantic retrieval has achieved remarkable success in online E-commerce applications. The majority of methods aim to distinguish positive items and negative items for each query by utilizing margin loss or softmax loss. Despite their decent performance, these methods are highly sensitive to hyper-parameters, i.e., margin and temperature 𝜏, which measure the similarity of negative pairs and affect the distribution of items in metric space. How to design and choose adaptively parameters for... | Mingming Li, Chunyuan Yuan, Huimu Wang, Peng Wang, Jingwei Zhuo, Binbin Wang, Lin Liu, Sulong Xu |  |
| 127 |  |  [On Sample-Efficient Code Generation](https://doi.org/10.18653/v1/2023.emnlp-industry.73) |  | 0 | Large language models often struggle to predict runtime behavior in code generation tasks, leading to a reliance on rejection sampling (best-of-n) to generate multiple code snippets then select the best. Our distinction is reducing sampling costs, without compromising generation quality. We introduce EFFICODE, a novel framework that prioritizes sampling on test problems that models can solve. We show how EFFICODE estimates solvability to optimize computational costs during multiple sampling.... | Hojae Han, Yu Jin Kim, Byoungjip Kim, Youngwon Lee, Kyungjae Lee, Kyungmin Lee, Moontae Lee, Kyunghoon Bae, Seungwon Hwang |  |
| 128 |  |  [Batch Prompting: Efficient Inference with Large Language Model APIs](https://doi.org/10.18653/v1/2023.emnlp-industry.74) |  | 0 | Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use. We propose batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs... | Zhoujun Cheng, Jungo Kasai, Tao Yu |  |
| 129 |  |  [Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding](https://doi.org/10.18653/v1/2023.emnlp-industry.75) |  | 0 | A Personalized Query Rewriting system strives to minimize defective queries to ensure robust conversational functionality by considering individual user behavior and preferences. It’s designed as a search-based system, maintaining a user index of past successful interactions with the conversational AI. However, this method faces challenges with unseen interactions, which refers to novel user interactions not covered by the user’s historical index. This paper introduces our Collaborative Query... | Zheng Chen, Ziyan Jiang, Fan Yang, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Aram Galstyan |  |
| 130 |  |  [DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues](https://doi.org/10.18653/v1/2023.emnlp-industry.76) |  | 0 | Controversy is a reflection of our zeitgeist, and an important aspect to any discourse. The rise of large language models (LLMs) as conversational systems has increased public reliance on these systems for answers to their various questions. Consequently, it is crucial to systematically examine how these models respond to questions that pertaining to ongoing debates. However, few such datasets exist in providing human-annotated labels reflecting the contemporary discussions. To foster research... | David Q. Sun, Artem Abzaliev, Hadas Kotek, Christopher Klein, Zidi Xiu, Jason D. Williams |  |
| 131 |  |  [Angel: Enterprise Search System for the Non-Profit Industry](https://doi.org/10.18653/v1/2023.emnlp-industry.77) |  | 0 | Non-profit industry need a system for accurately matching fund-seekers (e.g., AMERICAN NATIONAL RED CROSS) with fund-givers (e.g., BILL AND MELINDA GATES FOUNDATION) aligned in cause (e.g., cancer) and target beneficiary group (e.g., children). In this paper, we create an enterprise search system “ANGEL” for the non-profit industry that takes a fund-giver’s mission description as input and returns a ranked list of fund-seekers as output, and vice-versa. ANGEL employs ColBERT, a neural... | Saiful Haq, Ashutosh Sharma, Pushpak Bhattacharyya |  |
| 132 |  |  [Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023](https://aclanthology.org/volumes/2023.findings-emnlp/) |  | 0 |  | Houda Bouamor, Juan Pino, Kalika Bali |  |
| 133 |  |  [Frontmatter](https://aclanthology.org/2023.findings-emnlp.0) |  | 0 |  |  |  |
| 134 |  |  [Multi Document Summarization Evaluation in the Presence of Damaging Content](https://doi.org/10.18653/v1/2023.findings-emnlp.1) |  | 0 | In the Multi-document summarization (MDS) task, a summary is produced for a given set of documents. A recent line of research introduced the concept of damaging documents, denoting documents that should not be exposed to readers due to various reasons. In the presence of damaging documents, a summarizer is ideally expected to exclude damaging content in its output. Existing metrics evaluate a summary based on aspects such as relevance and consistency with the source documents. We propose to... | Avshalom Manevich, David Carmel, Nachshon Cohen, Elad Kravi, Ori Shapira |  |
| 135 |  |  [Guiding AMR Parsing with Reverse Graph Linearization](https://doi.org/10.18653/v1/2023.findings-emnlp.2) |  | 0 | Abstract Meaning Representation (AMR) parsing aims to extract an abstract semantic graph from a given sentence. The sequence-to-sequence approaches, which linearize the semantic graph into a sequence of nodes and edges and generate the linearized graph directly, have achieved good performance. However, we observed that these approaches suffer from structure loss accumulation during the decoding process, leading to a much lower F1-score for nodes and edges decoded later compared to those decoded... | Bofei Gao, Liang Chen, Peiyi Wang, Zhifang Sui, Baobao Chang |  |
| 136 |  |  [Translate the Beauty in Songs: Jointly Learning to Align Melody and Translate Lyrics](https://doi.org/10.18653/v1/2023.findings-emnlp.3) |  | 0 | Song translation requires both translation of lyrics and alignment of music notes so that the resulting verse can be sung to the accompanying melody, which is a challenging problem that has attracted some interests in different aspects of the translation process. In this paper, we propose Lyrics-Melody Translation with Adaptive Grouping (LTAG), a holistic solution to automatic song translation by jointly modeling lyric translation and lyrics-melody alignment. It is a novel encoder-decoder... | Chengxi Li, Kai Fan, Jiajun Bu, Boxing Chen, Zhongqiang Huang, Zhi Yu |  |
| 137 |  |  [Aksharantar: Open Indic-language Transliteration datasets and models for the Next Billion Users](https://doi.org/10.18653/v1/2023.findings-emnlp.4) |  | 0 | Transliteration is very important in the Indian language context due to the usage of multiple scripts and the widespread use of romanized inputs. However, few training and evaluation sets are publicly available. We introduce Aksharantar, the largest publicly available transliteration dataset for Indian languages created by mining from monolingual and parallel corpora, as well as collecting data from human annotators. The dataset contains 26 million transliteration pairs for 21 Indic languages... | Yash Madhani, Sushane Parthan, Priyanka Bedekar, Gokul NC, Ruchi Khapra, Anoop Kunchukuttan, Pratyush Kumar, Mitesh M. Khapra |  |
| 138 |  |  [Pretraining Without Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.5) |  | 0 | Transformers have been essential to pretraining success in NLP. While other architectures have been used, downstream accuracy is either significantly worse, or requires attention layers to match standard benchmarks such as GLUE. This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs). Our proposed model, Bidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative gating architecture that has been effective in... | Junxiong Wang, Jing Nathan Yan, Albert Gu, Alexander M. Rush |  |
| 139 |  |  [Time-Aware Representation Learning for Time-Sensitive Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.6) |  | 0 | Time is one of the crucial factors in real-world question answering (QA) problems. However, language models have difficulty understanding the relationships between time specifiers, such as ‘after’ and ‘before’, and numbers, since existing QA datasets do not include sufficient time expressions. To address this issue, we propose a Time-Context aware Question Answering (TCQA) framework. We suggest a Time-Context dependent Span Extraction (TCSE) task, and build a time-context dependent data... | Jungbin Son, Alice Oh |  |
| 140 |  |  [EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics](https://doi.org/10.18653/v1/2023.findings-emnlp.7) |  | 0 | Efficiency is a key property to foster inclusiveness and reduce environmental costs, especially in an era of LLMs. In this work, we provide a comprehensive evaluation of efficiency for MT evaluation metrics. Our approach involves replacing computation-intensive transformers with lighter alternatives and employing linear and quadratic approximations for alignment algorithms on top of LLM representations. We evaluate six (reference-free and reference-based) metrics across three MT datasets and... | Daniil Larionov, Jens Grünwald, Christoph Leiter, Steffen Eger |  |
| 141 |  |  [Unsupervised Opinion Summarization Using Approximate Geodesics](https://doi.org/10.18653/v1/2023.findings-emnlp.8) |  | 0 | Opinion summarization is the task of creating summaries capturing popular opinions from user reviews. In this paper, we introduce Geodesic Summarizer (GeoSumm), a novel system to perform unsupervised extractive opinion summarization. GeoSumm consists of an encoder-decoder based representation learning model that generates topical representations of texts. These representations capture the underlying semantics of the text as a distribution over learnable latent units. GeoSumm generates these... | Somnath Basu Roy Chowdhury, Nicholas Monath, Avinava Dubey, Amr Ahmed, Snigdha Chaturvedi |  |
| 142 |  |  [Investigating the Frequency Distortion of Word Embeddings and Its Impact on Bias Metrics](https://doi.org/10.18653/v1/2023.findings-emnlp.9) |  | 0 | Recent research has shown that static word embeddings can encode words’ frequencies. However, little has been studied about this behavior. In the present work, we study how frequency and semantic similarity relate to one another in static word embeddings, and we assess the impact of this relationship on embedding-based bias metrics. We find that Skip-gram, GloVe and FastText embeddings tend to produce higher similarity between high-frequency words than between other frequency combinations. We... | Francisco Valentini, Juan Sosa, Diego Fernández Slezak, Edgar Altszyler |  |
| 143 |  |  [Improving Classifier Robustness through Active Generative Counterfactual Data Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.10) |  | 0 | Counterfactual Data Augmentation (CDA) is a commonly used technique for improving robustness in natural language classifiers. However, one fundamental challenge is how to discover meaningful counterfactuals and efficiently label them, with minimal human labeling cost. Most existing methods either completely rely on human-annotated labels, an expensive process which limits the scale of counterfactual data, or implicitly assume label invariance, which may mislead the model with incorrect labels.... | Ananth Balashankar, Xuezhi Wang, Yao Qin, Ben Packer, Nithum Thain, Ed H. Chi, Jilin Chen, Alex Beutel |  |
| 144 |  |  [Data Augmentation Techniques for Machine Translation of Code-Switched Texts: A Comparative Study](https://doi.org/10.18653/v1/2023.findings-emnlp.11) |  | 0 | Code-switching (CSW) text generation has been receiving increasing attention as a solution to address data scarcity. In light of this growing interest, we need more comprehensive studies comparing different augmentation approaches. In this work, we compare three popular approaches: lexical replacements, linguistic theories, and back-translation (BT), in the context of Egyptian Arabic-English CSW. We assess the effectiveness of the approaches on machine translation and the quality of... | Injy Hamed, Nizar Habash, Thang Vu |  |
| 145 |  |  [On the Relation between Sensitivity and Accuracy in In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.12) |  | 0 | In-context learning (ICL) suffers from oversensitivity to the prompt, making it unreliable in real-world scenarios. We study the sensitivity of ICL with respect to multiple perturbation types. First, we find that label bias obscures the true sensitivity, and therefore prior work may have significantly underestimated ICL sensitivity. Second, we observe a strong negative correlation between ICL sensitivity and accuracy: predictions sensitive to perturbations are less likely to be correct.... | Yanda Chen, Chen Zhao, Zhou Yu, Kathleen R. McKeown, He He |  |
| 146 |  |  [Self-distilled Transitive Instance Weighting for Denoised Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.13) |  | 0 | The widespread existence of wrongly labeled instances is a challenge to distantly supervised relation extraction. Most of the previous works are trained in a bag-level setting to alleviate such noise. However, sentence-level training better utilizes the information than bag-level training, as long as combined with effective noise alleviation. In this work, we propose a novel Transitive Instance Weighting mechanism integrated with the self-distilled BERT backbone, utilizing information in the... | Xiangyu Lin, Weijia Jia, Zhiguo Gong |  |
| 147 |  |  [MWE as WSD: Solving Multiword Expression Identification with Word Sense Disambiguation](https://doi.org/10.18653/v1/2023.findings-emnlp.14) |  | 0 | Recent approaches to word sense disambiguation (WSD) utilize encodings of the sense gloss (definition), in addition to the input context, to improve performance. In this work we demonstrate that this approach can be adapted for use in multiword expression (MWE) identification by training models which use gloss and context information to filter MWE candidates produced by a rule-based extraction pipeline. Our approach substantially improves precision, outperforming the state-of-the-art in MWE... | Joshua Tanner, Jacob Hoffman |  |
| 148 |  |  [Dual Contrastive Learning Framework for Incremental Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.15) |  | 0 | Incremental learning plays a pivotal role in the context of online knowledge discovery, as it encourages large models (LM) to learn and refresh knowledge continuously. Many approaches have been proposed to simultaneously preserve knowledge from previous tasks while learning new concepts in online NLP applications. In this paper, we primarily focus on learning a more generalized embedding space that could be better transferred to various downstream sequence tasks. The key idea is to learn from... | Yigong Wang, Zhuoyi Wang, Yu Lin, Jinghui Guo, Sadaf Md. Halim, Latifur Khan |  |
| 149 |  |  [Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards](https://doi.org/10.18653/v1/2023.findings-emnlp.16) |  | 0 | Community Question-Answering (CQA) portals serve as a valuable tool for helping users within an organization. However, making them accessible to non-English-speaking users continues to be a challenge. Translating questions can broaden the community’s reach, benefiting individuals with similar inquiries in various languages. Translating questions using Neural Machine Translation (NMT) poses more challenges, especially in noisy environments, where the grammatical correctness of the questions is... | Baban Gain, Ramakrishna Appicharla, Soumya Chennabasavaraj, Nikesh Garera, Asif Ekbal, Muthusamy Chelliah |  |
| 150 |  |  [Filtered Semi-Markov CRF](https://doi.org/10.18653/v1/2023.findings-emnlp.17) |  | 0 | Semi-Markov CRF has been proposed as an alternative to the traditional Linear Chain CRF for text segmentation tasks such as Named Entity Recognition (NER). Unlike CRF, which treats text segmentation as token-level prediction, Semi-CRF considers segments as the basic unit, making it more expressive. However, Semi-CRF suffers from two major drawbacks: (1) quadratic complexity over sequence length, as it operates on every span of the input sequence, and (2) inferior performance compared to CRF for... | Urchade Zaratiana, Nadi Tomeh, Niama El Khbir, Pierre Holat, Thierry Charnois |  |
| 151 |  |  [Data Pruning for Efficient Model Pruning in Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.18) |  | 0 | Model pruning methods reduce memory requirements and inference time of large-scale pre-trained language models after deployment. However, the actual pruning procedure is computationally intensive, involving repeated training and pruning until the required sparsity is achieved. This paper combines data pruning with movement pruning for Neural Machine Translation (NMT) to enable efficient fine-pruning. We design a dataset pruning strategy by leveraging cross-entropy scores of individual training... | Abdul Hameed Azeemi, Ihsan Ayyub Qazi, Agha Ali Raza |  |
| 152 |  |  [Long-Form Speech Translation through Segmentation with Finite-State Decoding Constraints on Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.19) |  | 0 | One challenge in speech translation is that plenty of spoken content is long-form, but short units are necessary for obtaining high-quality translations. To address this mismatch, we adapt large language models (LLMs) to split long ASR transcripts into segments that can be independently translated so as to maximize the overall translation quality. We overcome the tendency of hallucination in LLMs by incorporating finite-state constraints during decoding; these eliminate invalid outputs without... | Arya McCarthy, Hao Zhang, Shankar Kumar, Felix Stahlberg, Ke Wu |  |
| 153 |  |  [Re-Temp: Relation-Aware Temporal Representation Learning for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.findings-emnlp.20) |  | 0 | Temporal Knowledge Graph Completion (TKGC) under the extrapolation setting aims to predict the missing entity from a fact in the future, posing a challenge that aligns more closely with real-world prediction problems. Existing research mostly encodes entities and relations using sequential graph neural networks applied to recent snapshots. However, these approaches tend to overlook the ability to skip irrelevant snapshots according to entity-related relations in the query and disregard the... | Kunze Wang, Soyeon Caren Han, Josiah Poon |  |
| 154 |  |  [RethinkingTMSC: An Empirical Study for Target-Oriented Multimodal Sentiment Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.21) |  | 0 | Recently, Target-oriented Multimodal Sentiment Classification (TMSC) has gained significant attention among scholars. However, current multimodal models have reached a performance bottleneck. To investigate the causes of this problem, we perform extensive empirical evaluation and in-depth analysis of the datasets to answer the following questions: \*\*Q1\*\*: Are the modalities equally important for TMSC? \*\*Q2\*\*: Which multimodal fusion modules are more effective? \*\*Q3\*\*: Do existing... | Junjie Ye, Jie Zhou, Junfeng Tian, Rui Wang, Qi Zhang, Tao Gui, Xuanjing Huang |  |
| 155 |  |  [Lexical Entrainment for Conversational Systems](https://doi.org/10.18653/v1/2023.findings-emnlp.22) |  | 0 | Conversational agents have become ubiquitous in assisting with daily tasks, and are expected to possess human-like features. One such feature is lexical entrainment (LE), a phenomenon in which speakers in human-human conversations tend to naturally and subconsciously align their lexical choices with those of their interlocutors, leading to more successful and engaging conversations. As an example, if a digital assistant replies “Your appointment for Jinling Noodle Pub is at 7 pm” to the... | Zhengxiang Shi, Procheta Sen, Aldo Lipani |  |
| 156 |  |  [AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies](https://doi.org/10.18653/v1/2023.findings-emnlp.23) |  | 0 | We show that dialogue models can detect errors in their own messages, by calculating the likelihood of replies that are indicative of poor messages. For example, if an agent believes its partner is likely to respond “I don’t understand” to a candidate message, that message may not make sense, so an alternative message should be chosen. We evaluate our approach on a dataset from the game Diplomacy, which contains long dialogues richly grounded in the game state, on which existing models make... | Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis |  |
| 157 |  |  [Follow-on Question Suggestion via Voice Hints for Voice Assistants](https://doi.org/10.18653/v1/2023.findings-emnlp.24) |  | 0 | The adoption of voice assistants like Alexa or Siri has grown rapidly, allowing users to instantly access information via voice search. Query suggestion is a standard feature of screen-based search experiences, allowing users to explore additional topics. However, this is not trivial to implement in voice-based settings. To enable this, we tackle the novel task of suggesting questions with compact and natural voice hints to allow users to ask follow-up questions. We define the task, ground it... | Besnik Fetahu, Pedro Faustini, Anjie Fang, Giuseppe Castellucci, Oleg Rokhlenko, Shervin Malmasi |  |
| 158 |  |  [Bidirectional Masked Self-attention and N-gram Span Attention for Constituency Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.25) |  | 0 | Attention mechanisms have become a crucial aspect of deep learning, particularly in natural language processing (NLP) tasks. However, in tasks such as constituency parsing, attention mechanisms can lack the directional information needed to form sentence spans. To address this issue, we propose a Bidirectional masked and N-gram span Attention (BNA) model, which is designed by modifying the attention mechanisms to capture the explicit dependencies between each word and enhance the representation... | Soohyeong Kim, Whanhee Cho, Minji Kim, Yong Choi |  |
| 159 |  |  [CR-COPEC: Causal Rationale of Corporate Performance Changes to learn from Financial Reports](https://doi.org/10.18653/v1/2023.findings-emnlp.26) |  | 0 | In this paper, we introduce CR-COPEC called Causal Rationale of Corporate Performance Changes from financial reports. This is a comprehensive large-scale domain-adaptation causal sentence dataset to detect financial performance changes of corporate. CR-COPEC contributes to two major achievements. First, it detects causal rationale from 10-K annual reports of the U.S. companies, which contain experts’ causal analysis following accounting standards in a formal manner. This dataset can be widely... | Ye Eun Chun, Sunjae Kwon, Kyunghwan Sohn, Nakwon Sung, Junyoup Lee, Byoung Seo, Kevin Compher, Seungwon Hwang, Jaesik Choi |  |
| 160 |  |  [Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT](https://doi.org/10.18653/v1/2023.findings-emnlp.27) |  | 0 | The goal of this paper is to explore how Transformer language models process semantic knowledge, especially regarding the plausibility of noun-verb relations. First, I demonstrate GPT2 exhibits a higher degree of similarity with humans in plausibility processing compared to other Transformer language models. Next, I delve into how knowledge of plausibility is contained within attention heads of GPT2 and how these heads causally contribute to GPT2’s plausibility processing ability. Through... | Soo Ryu |  |
| 161 |  |  [Automatic Unit Test Data Generation and Actor-Critic Reinforcement Learning for Code Synthesis](https://doi.org/10.18653/v1/2023.findings-emnlp.28) |  | 0 | The advent of large pre-trained language models in the domain of Code Synthesis has shown remarkable performance on various benchmarks, treating the problem of Code Generation in a fashion similar to Natural Language Generation, trained with a Language Modelling (LM) objective. In addition, the property of programming language code being precisely evaluable with respect to its semantics – through the use of Unit Tests to check its functional correctness – lends itself to using Reinforcement... | Philip John Gorinski, Matthieu Zimmer, Gerasimos Lampouras, DerrickGohXin Deik, Ignacio Iacobacci |  |
| 162 |  |  [Unlocking the Heterogeneous Landscape of Big Data NLP with DUUI](https://doi.org/10.18653/v1/2023.findings-emnlp.29) |  | 0 | Automatic analysis of large corpora is a complex task, especially in terms of time efficiency. This complexity is increased by the fact that flexible, extensible text analysis requires the continuous integration of ever new tools. Since there are no adequate frameworks for these purposes in the field of NLP, and especially in the context of UIMA, that are not outdated or unusable for security reasons, we present a new approach to address the latter task: Docker Unified UIMA Interface (DUUI), a... | Alexander Leonhardt, Giuseppe Abrami, Daniel Baumartz, Alexander Mehler |  |
| 163 |  |  [Towards Agile Text Classifiers for Everyone](https://doi.org/10.18653/v1/2023.findings-emnlp.30) |  | 0 | Text-based safety classifiers are widely used for content moderation and increasingly to tune generative language model behavior - a topic of growing concern for the safety of digital assistants and chatbots. However, different policies require different classifiers, and safety policies themselves improve from iteration and adaptation. This paper introduces and evaluates methods for agile text classification, whereby classifiers are trained using small, targeted datasets that can be quickly... | Maximilian Mozes, Jessica Hoffmann, Katrin Tomanek, Muhamed Kouate, Nithum Thain, Ann Yuan, Tolga Bolukbasi, Lucas Dixon |  |
| 164 |  |  [Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good](https://doi.org/10.18653/v1/2023.findings-emnlp.31) |  | 0 | With the recent advances in natural language processing (NLP), a vast number of applications have emerged across various use cases. Among the plethora of NLP applications, many academic researchers are motivated to do work that has a positive social impact, in line with the recent initiatives of NLP for Social Good (NLP4SG). However, it is not always obvious to researchers how their research efforts are tackling today’s big social problems. Thus, in this paper, we introduce NLP4SGPapers, a... | Fernando Gonzalez Adauto, Zhijing Jin, Bernhard Schölkopf, Tom Hope, Mrinmaya Sachan, Rada Mihalcea |  |
| 165 |  |  [PAXQA: Generating Cross-lingual Question Answering Examples at Training Scale](https://doi.org/10.18653/v1/2023.findings-emnlp.32) |  | 0 | Existing question answering (QA) systems owe much of their success to large, high-quality training data. Such annotation efforts are costly, and the difficulty compounds in the cross-lingual setting. Therefore, prior cross-lingual QA work has focused on releasing evaluation datasets, and then applying zero-shot methods as baselines. This work proposes a synthetic data generation method for cross-lingual QA which leverages indirect supervision from existing parallel corpora. Our method termed... | Bryan Li, Chris CallisonBurch |  |
| 166 |  |  [Sharing, Teaching and Aligning: Knowledgeable Transfer Learning for Cross-Lingual Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.33) |  | 0 | In cross-lingual language understanding, machine translation is often utilized to enhance the transferability of models across languages, either by translating the training data from the source language to the target, or from the target to the source to aid inference. However, in cross-lingual machine reading comprehension (MRC), it is difficult to perform a deep level of assistance to enhance cross-lingual transfer because of the variation of answer span positions in different languages. In... | Tingfeng Cao, Chengyu Wang, Chuanqi Tan, Jun Huang, Jinhui Zhu |  |
| 167 |  |  [BERT Goes Off-Topic: Investigating the Domain Transfer Challenge using Genre Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.34) |  | 0 | While performance of many text classification tasks has been recently improved due to Pretrained Language Models (PLMs), in this paper we show that they still suffer from a performance gap when the underlying distribution of topics changes. For example, a genre classifier trained on political topics often fails when tested on documents in the same genre, but about sport or medicine. In this work, we quantify this phenomenon empirically with a large corpus and a large set of topics. Thus, we... | Dmitri Roussinov, Serge Sharoff |  |
| 168 |  |  [Toward Stronger Textual Attack Detectors](https://doi.org/10.18653/v1/2023.findings-emnlp.35) |  | 0 | The landscape of available textual adversarial attacks keeps growing, posing severe threats and raising concerns regarding deep NLP systems integrity. However, the crucial problem of defending against malicious attacks has only drawn few attention in the NLP community. The latter is nonetheless instrumental to develop robust and trustworthy systems. This paper makes two important contributions in this line of search: (i) we introduce LAROUSSE, a new framework to detect textual adversarial... | Pierre Colombo, Marine Picot, Nathan Noiry, Guillaume Staerman, Pablo Piantanida |  |
| 169 |  |  [MEAL: Stable and Active Learning for Few-Shot Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.36) |  | 0 | Few-shot classification has made great strides due to foundation models that, through priming and prompting, are highly effective few-shot learners. However, this approach has high variance both across different sets of few shots (\*data selection\*) and across different finetuning runs (\*run variability\*). This is problematic not only because it impedes the fair comparison of different approaches, but especially because it makes few-shot learning too unreliable for many real-world... | Abdullatif Köksal, Timo Schick, Hinrich Schütze |  |
| 170 |  |  [Structure and Label Constrained Data Augmentation for Cross-domain Few-shot NER](https://doi.org/10.18653/v1/2023.findings-emnlp.37) |  | 0 | Cross-domain few-shot named entity recognition (NER) is a challenging task that aims to recognize entities in target domains with limited labeled data by leveraging relevant knowledge from source domains. However, domain gaps limit the effect of knowledge transfer and harm the performance of NER models. In this paper, we analyze those domain gaps from two new perspectives, i.e., entity annotations and entity structures and leverage word-to-tag and word-to-word relations to model them,... | Jingyi Zhang, Ying Zhang, Yufeng Chen, Jinan Xu |  |
| 171 |  |  [Weakly-supervised Deep Cognate Detection Framework for Low-Resourced Languages Using Morphological Knowledge of Closely-Related Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.38) |  | 0 | Exploiting cognates for transfer learning in under-resourced languages is an exciting opportunity for language understanding tasks, including unsupervised machine translation, named entity recognition and information retrieval. Previous approaches mainly focused on supervised cognate detection tasks based on orthographic, phonetic or state-of-the-art contextual language models, which under-perform for most under-resourced languages. This paper proposes a novel language-agnostic... | Koustava Goswami, Priya Rani, Theodorus Fransen, John P. McCrae |  |
| 172 |  |  [SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data](https://doi.org/10.18653/v1/2023.findings-emnlp.39) |  | 0 | Text-to-SQL aims to automate the process of generating SQL queries on a database from natural language text. In this work, we propose “SQLPrompt”, tailored to improve the few-shot prompting capabilities of Text-to-SQL for Large Language Models (LLMs). Our methods include innovative prompt design, execution-based consistency decoding strategy which selects the SQL with the most consistent execution outcome among other SQL proposals, and a method that aims to improve performance by diversifying... | Ruoxi Sun, Sercan Ö. Arik, Rajarishi Sinha, Hootan Nakhost, Hanjun Dai, Pengcheng Yin, Tomas Pfister |  |
| 173 |  |  [Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.40) |  | 0 | Foundation models or pre-trained models have substantially improved the performance of various language, vision, and vision-language understanding tasks. However, existing foundation models can only perform the best in one type of tasks, namely language, vision, or vision-language. It is still an open question whether it is possible to construct a general foundation model performing the best for all the understanding tasks. In this paper, we propose a new method for training the general... | Xinsong Zhang, Yan Zeng, Jipeng Zhang, Hang Li |  |
| 174 |  |  [Trigger Warnings: Bootstrapping a Violence Detector for Fan Fiction](https://doi.org/10.18653/v1/2023.findings-emnlp.41) |  | 0 | We present the first dataset and evaluation results on a newly defined task: assigning trigger warnings. We introduce a labeled corpus of narrative fiction from Archive of Our Own (AO3), a popular fan fiction site, and define a document-level classification task to determine whether or not to assign a trigger warning to an English story. We focus on the most commonly assigned trigger type “violence’ using the warning labels provided by AO3 authors as ground-truth labels. We trained SVM, BERT,... | Magdalena Wolska, Matti Wiegmann, Christopher Schröder, Ole Borchardt, Benno Stein, Martin Potthast |  |
| 175 |  |  [Pass-Tuning: Towards Structure-Aware Parameter-Efficient Tuning for Code Representation Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.42) |  | 0 | Code pre-trained models (CodePTMs) have recently become the de-facto paradigm for various tasks in the domain of code intelligence. To achieve excellent performance, the widely used strategy is to fine-tune all the parameters of CodePTMs. However, as the model size increases along with the number of downstream tasks, this strategy becomes excessively expensive. There are also some prior works that utilize Parameter-Efficient Learning (PEL) methods for model tuning in natural language processing... | Nuo Chen, Qiushi Sun, Jianing Wang, Xiang Li, Ming Gao |  |
| 176 |  |  [Counterfactual Augmentation for Multimodal Learning Under Presentation Bias](https://doi.org/10.18653/v1/2023.findings-emnlp.43) |  | 0 | In real-world machine learning systems, labels are often derived from user behaviors that the system wishes to encourage. Over time, new models must be trained as new training examples and features become available. However, feedback loops between users and models can bias future user behavior, inducing a \*presentation bias\* in the labels that compromises the ability to train new models. In this paper, we propose \*counterfactual augmentation\*, a novel causal method for correcting... | Victoria Lin, LouisPhilippe Morency, Dimitrios Dimitriadis, Srinagesh Sharma |  |
| 177 |  |  [A Table-to-Text Framework with Heterogeneous Multidominance Attention and Self-Evaluated Multi-Pass Deliberation](https://doi.org/10.18653/v1/2023.findings-emnlp.44) |  | 0 | Though big progress in table-to-text works, effectively leveraging table structure signals, e.g., hierarchical structure, remains challenging. Besides, deliberating generated descriptions proves to be effective for table-to-text. However, determining the appropriate outcome when encountering multi-pass candidates is another challenge. To this end, we propose a novel table-to-text approach on top of Self-evaluated multi-pass Generation and Heterogenous Multidominance Attention, namely SG-HMA.... | Xi Chen, Xinjiang Lu, Haoran Xin, Wenjun Peng, Haoyang Duan, Feihu Jiang, Jingbo Zhou, Hui Xiong |  |
| 178 |  |  [Crossing the Aisle: Unveiling Partisan and Counter-Partisan Events in News Reporting](https://doi.org/10.18653/v1/2023.findings-emnlp.45) |  | 0 | News media is expected to uphold unbiased reporting. Yet they may still affect public opinion by selectively including or omitting events that support or contradict their ideological positions. Prior work in NLP has only studied media bias via linguistic style and word usage. In this paper, we study to which degree media balances news reporting and affects consumers through event inclusion or omission. We first introduce the task of detecting both partisan and counter-partisan events: events... | Kaijian Zou, Xinliang Frederick Zhang, Winston Wu, Nicholas Beauchamp, Lu Wang |  |
| 179 |  |  [Video-Text Retrieval by Supervised Sparse Multi-Grained Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.46) |  | 0 | While recent progress in video-text retrieval has been advanced by the exploration of better representation learning, in this paper, we present a novel multi-grained sparse learning framework, S3MA, to learn an aligned sparse space shared between the video and the text for video-text retrieval. The shared sparse space is initialized with a finite number of sparse concepts, each of which refers to a number of words. With the text data at hand, we learn and update the shared sparse space in a... | Yimu Wang, Peng Shi |  |
| 180 |  |  [Zero-Shot-BERT-Adapters: a Zero-Shot Pipeline for Unknown Intent Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.47) |  | 0 | Intent discovery is a crucial task in natural language processing, and it is increasingly relevant for various of industrial applications. Identifying novel, unseen intents from user inputs remains one of the biggest challenges in this field. Herein, we propose Zero-Shot-BERT-Adapters, a two-stage method for multilingual intent discovery relying on a Transformer architecture, fine-tuned with Adapters. We train the model for Natural Language Inference (NLI) and later perform unknown intent... | Daniele Comi, Dimitrios Christofidellis, Pier Francesco Piazza, Matteo Manica |  |
| 181 |  |  [ReFSQL: A Retrieval-Augmentation Framework for Text-to-SQL Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.48) |  | 0 | Text-to-SQL is the task that aims at translating natural language questions into SQL queries. Existing methods directly align the natural language with SQL Language and train one encoder-decoder-based model to fit all questions. However, they underestimate the inherent structural characteristics of SQL, as well as the gap between specific structure knowledge and general knowledge. This leads to structure errors in the generated SQL. To address the above challenges, we propose a... | Kun Zhang, Xiexiong Lin, Yuanzhuo Wang, Xin Zhang, Fei Sun, Jianhe Cen, Hexiang Tan, Xuhui Jiang, Huawei Shen |  |
| 182 |  |  [Approximating Two-Layer Feedforward Networks for Efficient Transformers](https://doi.org/10.18653/v1/2023.findings-emnlp.49) |  | 0 | How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that \*unifies\* various methods to \*approximate two-layer NNs\* (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we... | Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber |  |
| 183 |  |  [Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.50) |  | 0 | Adapting a large language model for multiple-attribute text style transfer via fine-tuning can be challenging due to the substantial amount of computational resources and labeled data required for the specific downstream task. In this paper, we address this challenge by introducing Adapter-TST, a framework that freezes the pre-trained model’s original parameters and enables the development of a multiple-attribute text style transfer model. Using BART as the backbone model, Adapter-TST utilizes... | Zhiqiang Hu, Nancy F. Chen, Roy KaWei Lee |  |
| 184 |  |  [Solving the Right Problem is Key for Translational NLP: A Case Study in UMLS Vocabulary Insertion](https://doi.org/10.18653/v1/2023.findings-emnlp.51) |  | 0 | As the immense opportunities enabled by large language models become more apparent, NLP systems will be increasingly expected to excel in real-world settings. However, in many instances, powerful models alone will not yield translational NLP solutions, especially if the formulated problem is not well aligned with the real-world task. In this work, we study the case of UMLS vocabulary insertion, an important real-world task in which hundreds of thousands of new terms, referred to as atoms, are... | Bernal Jimenez Gutierrez, Yuqing Mao, Vinh Nguyen, Kin Wah Fung, Yu Su, Olivier Bodenreider |  |
| 185 |  |  [Improving Cross-lingual Transfer through Subtree-aware Word Reordering](https://doi.org/10.18653/v1/2023.findings-emnlp.52) |  | 0 | Despite the impressive growth of the abilities of multilingual language models, such as XLM-R and mT5, it has been shown that they still face difficulties when tackling typologically-distant languages, particularly in the low-resource setting. One obstacle for effective cross-lingual transfer is variability in word-order patterns. It can be potentially mitigated via source- or target-side word reordering, and numerous approaches to reordering have been proposed. However, they rely on... | Ofir Arviv, Dmitry Nikolaev, Taelin Karidi, Omri Abend |  |
| 186 |  |  [Novel Slot Detection With an Incremental Setting](https://doi.org/10.18653/v1/2023.findings-emnlp.53) |  | 0 | Current dialogue systems face diverse user requests and rapid change domains, making quickly adapt to scenarios with previous unseen slot types become a major challenge. Recently, researchers have introduced novel slot detection (NSD) to discover potential new types. However, dialogue system with NSD does not bring practical improvements due to the system still cannot handle novel slots in subsequent interactions. In this paper, we define incremental novel slot detection (INSD), which separates... | Chen Liang, Hongliang Li, Changhao Guan, Qingbin Liu, Jian Liu, Jinan Xu, Zhe Zhao |  |
| 187 |  |  [Self-supervised Post-processing Method to Enrich Pretrained Word Vectors](https://doi.org/10.18653/v1/2023.findings-emnlp.54) |  | 0 | Retrofitting techniques, which inject external resources into word representations, have compensated for the weakness of distributed representations in semantic and relational knowledge between words. However, the previous methods require additional external resources and strongly depend on the lexicon. To address the issues, we propose a simple extension of extrofitting, self-supervised extrofitting: extrofitting by its own word vector distribution. Our methods improve the vanilla embeddings... | Hwiyeol Jo |  |
| 188 |  |  [Automatic Model Selection with Large Language Models for Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.55) |  | 0 | Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two distinct reasoning methods, each with its own strengths. CoT employs natural language, offering flexibility and interpretability, while PAL utilizes programming language, yielding more structured and rigorous logic. We introduce a model selection method to combine the best of both worlds by employing a large language model (LLM) to dynamically select between them. Our theoretical analysis underscores the feasibility of... | James Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, Michael Qizhe Xie |  |
| 189 |  |  [ARKitSceneRefer: Text-based Localization of Small Objects in Diverse Real-World 3D Indoor Scenes](https://doi.org/10.18653/v1/2023.findings-emnlp.56) |  | 0 | 3D referring expression comprehension is a task to ground text representations onto objects in 3D scenes. It is a crucial task for indoor household robots or augmented reality devices to localize objects referred to in user instructions. However, existing indoor 3D referring expression comprehension datasets typically cover larger object classes that are easy to localize, such as chairs, tables, or doors, and often overlook small objects, such as cooking tools or office supplies. Based on the... | Shunya Kato, Shuhei Kurita, Chenhui Chu, Sadao Kurohashi |  |
| 190 |  |  [Improving Question Generation with Multi-level Content Planning](https://doi.org/10.18653/v1/2023.findings-emnlp.57) |  | 0 | This paper addresses the problem of generating questions from a given context and an answer, specifically focusing on questions that require multi-hop reasoning across an extended context. Previous studies have suggested that key phrase selection is essential for question generation (QG), yet it is still challenging to connect such disjointed phrases into meaningful questions, particularly for long context. To mitigate this issue, we propose MultiFactor, a novel QG framework based on... | Zehua Xia, Qi Gou, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li, CamTu Nguyen |  |
| 191 |  |  [Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing](https://doi.org/10.18653/v1/2023.findings-emnlp.58) |  | 0 | The emergence of Large Language Models (LLMs), such as ChatGPT, has revolutionized general natural language preprocessing (NLP) tasks. However, their expertise in the financial domain lacks a comprehensive evaluation. To assess the ability of LLMs to solve financial NLP tasks, we present FinLMEval, a framework for Financial Language Model Evaluation, comprising nine datasets designed to evaluate the performance of language models. This study compares the performance of fine-tuned auto-encoding... | Yue Guo, Zian Xu, Yi Yang |  |
| 192 |  |  [DelucionQA: Detecting Hallucinations in Domain-specific Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.59) |  | 0 | Hallucination is a well-known phenomenon in text generated by large language models (LLMs). The existence of hallucinatory responses is found in almost all application scenarios e.g., summarization, question-answering (QA) etc. For applications requiring high reliability (e.g., customer-facing assistants), the potential existence of hallucination in LLM-generated text is a critical problem. The amount of hallucination can be reduced by leveraging information retrieval to provide relevant... | Mobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun Araki, Arsalan Gundroo, Bingqing Wang, Rakesh R. Menon, Md. Rizwan Parvez, Zhe Feng |  |
| 193 |  |  [InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution](https://doi.org/10.18653/v1/2023.findings-emnlp.60) |  | 0 | Over recent decades, significant advancements in cross-modal retrieval is mainly driven by breakthroughs in visual and linguistic modeling. However, a recent study shows that multi-modal data representations tend to cluster within a limited convex cone (as representation degeneration problem), which hinders retrieval performance due to the inseparability of these representations. In our study, we first empirically validate the presence of the representation degeneration problem across multiple... | Xiangru Jian, Yimu Wang |  |
| 194 |  |  [Dissecting In-Context Learning of Translations in GPT-3](https://doi.org/10.18653/v1/2023.findings-emnlp.61) |  | 0 | Most of the recent work in leveraging Large Language Models (LLMs) such as GPT-3 for Machine Translation (MT) has focused on selecting the few-shot samples for prompting. In this work, we try to better understand the role of demonstration attributes for the in-context learning of translations through perturbations of high-quality, in-domain demonstrations. We find that asymmetric perturbation of the source-target mappings yield vastly different results. We show that the perturbation of the... | Vikas Raunak, Arul Menezes, Hany Hassan Awadalla |  |
| 195 |  |  [Social Commonsense-Guided Search Query Generation for Open-Domain Knowledge-Powered Conversations](https://doi.org/10.18653/v1/2023.findings-emnlp.62) |  | 0 | Open-domain dialog involves generating search queries that help obtain relevant knowledge for holding informative conversations. However, it can be challenging to determine what information to retrieve when the user is passive and does not express a clear need or request. To tackle this issue, we present a novel approach that focuses on generating internet search queries that are guided by social commonsense. Specifically, we leverage a commonsense dialog system to establish connections related... | Revanth Gangi Reddy, Hao Bai, Wentao Yao, Sharath Chandra Etagi Suresh, Heng Ji, ChengXiang Zhai |  |
| 196 |  |  [MixTEA: Semi-supervised Entity Alignment with Mixture Teaching](https://doi.org/10.18653/v1/2023.findings-emnlp.63) |  | 0 | Semi-supervised entity alignment (EA) is a practical and challenging task because of the lack of adequate labeled mappings as training data. Most works address this problem by generating pseudo mappings for unlabeled entities. However, they either suffer from the erroneous (noisy) pseudo mappings or largely ignore the uncertainty of pseudo mappings. In this paper, we propose a novel semi-supervised EA method, termed as MixTEA, which guides the model learning with an end-to-end mixture teaching... | Feng Xie, Xin Song, Xiang Zeng, Xuechen Zhao, Lei Tian, Bin Zhou, Yusong Tan |  |
| 197 |  |  [EZ-STANCE: A Large Dataset for Zero-Shot Stance Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.64) |  | 0 |  | Chenye Zhao, Cornelia Caragea |  |
| 198 |  |  [Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.65) |  | 0 | Neural ‘dense’ retrieval models are state of the art for many datasets, however these models often exhibit limited domain transfer ability. Existing approaches to adaptation are unwieldy, such as requiring explicit supervision, complex model architectures, or massive external models. We present ABEL, a simple but effective unsupervised method to enhance passage retrieval in zero-shot settings. Our technique follows a straightforward loop: a dense retriever learns from supervision signals... | Fan Jiang, Qiongkai Xu, Tom Drummond, Trevor Cohn |  |
| 199 |  |  [TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.66) |  | 0 | Large-scale video-language pre-training has made remarkable strides in advancing video-language understanding tasks. However, the heavy computational burden of video encoding remains a formidable efficiency bottleneck, particularly for long-form videos. These videos contain massive visual tokens due to their inherent 3D properties and spatiotemporal redundancy, making it challenging to capture complex temporal and spatial relationships. To tackle this issue, we propose an efficient method... | Shuhuai Ren, Sishuo Chen, Shicheng Li, Xu Sun, Lu Hou |  |
| 200 |  |  [Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.67) |  | 0 | Answering time-sensitive questions from long documents requires temporal reasoning over the times in questions and documents. An important open question is whether large language models can perform such reasoning solely using a provided text document, or whether they can benefit from additional temporal information extracted using other systems. We address this research question by applying existing temporal information extraction systems to construct temporal graphs of events, times, and... | Xin Su, Phillip Howard, Nagib Hakim, Steven Bethard |  |
| 201 |  |  [The Internal State of an LLM Knows When It's Lying](https://doi.org/10.18653/v1/2023.findings-emnlp.68) |  | 0 | While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM’s internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is... | Amos Azaria, Tom M. Mitchell |  |
| 202 |  |  [Factual Relation Discrimination for Factuality-oriented Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.69) |  | 0 | Most neural abstractive summarization models are capable of producing high-quality summaries. However, they still frequently contain factual errors. Existing factuality-oriented abstractive summarization models only consider the integration of factual information and ignore the causes of factual errors. To address this issue, we propose a factuality-oriented abstractive summarization model DASum, which is based on a new task factual relation discrimination that is able to identify the causes of... | Zhiguang Gao, Peifeng Li, Feng Jiang, Xiaomin Chu, Qiaoming Zhu |  |
| 203 |  |  [Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment](https://doi.org/10.18653/v1/2023.findings-emnlp.70) |  | 0 | Multi-Modal Entity Alignment (MMEA) is a critical task that aims to identify equivalent entity pairs across multi-modal knowledge graphs (MMKGs). However, this task faces challenges due to the presence of different types of information, including neighboring entities, multi-modal attributes, and entity types. Directly incorporating the above information (e.g., concatenation or attention) can lead to an unaligned information space. To address these challenges, we propose a novel MMEA... | Qian Li, Cheng Ji, Shu Guo, Zhaoji Liang, Lihong Wang, Jianxin Li |  |
| 204 |  |  [Is a Prestigious Job the same as a Prestigious Country? A Case Study on Multilingual Sentence Embeddings and European Countries](https://doi.org/10.18653/v1/2023.findings-emnlp.71) |  | 0 | We study how multilingual sentence representations capture European countries and occupations and how this differs across European languages. We prompt the models with templated sentences that we machine-translate into 12 European languages and analyze the most prominent dimensions in the embeddings. Our analysis reveals that the most prominent feature in the embedding is the political distinction between Eastern and Western Europe and the country’s economic strength in terms of GDP. When... | Jindrich Libovický |  |
| 205 |  |  [Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.72) |  | 0 | Large Language Models (LLMs) have generated considerable interest and debate regarding their potential emergence of Theory of Mind (ToM). Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones primarily focus on different aspects of ToM and are prone to shortcuts and data leakage. In this position paper, we seek to answer two road-blocking questions: (1) How can we taxonomize a holistic landscape of machine ToM?... | Ziqiao Ma, Jacob Sansom, Run Peng, Joyce Chai |  |
| 206 |  |  [Text Augmented Spatial Aware Zero-shot Referring Image Segmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.73) |  | 0 | In this paper, we study a challenging task of zero-shot referring image segmentation. This task aims to identify the instance mask that is most related to a referring expression without training on pixel-level annotations. Previous research takes advantage of pre-trained cross-modal models, e.g., CLIP, to align instance-level masks with referring expressions. Yet, CLIP only considers the global-level alignment of image-text pairs, neglecting fine-grained matching between the referring sentence... | Yucheng Suo, Linchao Zhu, Yi Yang |  |
| 207 |  |  [IRFL: Image Recognition of Figurative Language](https://doi.org/10.18653/v1/2023.findings-emnlp.74) |  | 0 | Figures of speech such as metaphors, similes, and idioms are integral parts of human communication. They are ubiquitous in many forms of discourse, allowing people to convey complex, abstract ideas and evoke emotion. As figurative forms are often conveyed through multiple modalities (e.g., both text and images), understanding multimodal figurative language is an important AI challenge, weaving together profound vision, language, commonsense and cultural knowledge. In this work, we develop the... | Ron Yosef, Yonatan Bitton, Dafna Shahaf |  |
| 208 |  |  [Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization](https://doi.org/10.18653/v1/2023.findings-emnlp.75) |  | 0 | Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily overfit to few-shot training samples, thereby undermining generalizability. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they fail to... | Kaihang Pan, Juncheng Li, Hongye Song, Jun Lin, Xiaozhong Liu, Siliang Tang |  |
| 209 |  |  [An Adaptive Prompt Generation Framework for Task-oriented Dialogue System](https://doi.org/10.18653/v1/2023.findings-emnlp.76) |  | 0 | The de facto way of utilizing black-box large language models (LLMs) to perform various downstream tasks is prompting. However, obtaining suitable prompts for specific tasks is still a challenging problem. While existing LLM-based methods demonstrate promising performance in task-oriented dialogue (TOD) task, they often require manual adjustment in prompt selection, or focus solely on dialogue understanding or generation. To address these issues, we propose an adaptive prompt generation... | Jun Gao, Liuyu Xiang, Huijia Wu, Han Zhao, Yiqi Tong, Zhaofeng He |  |
| 210 |  |  [Temporal Knowledge Graph Reasoning Based on N-tuple Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.77) |  | 0 | Reasoning over Temporal Knowledge Graphs (TKGs) that predicts temporal facts (e.g., events) in the future is crucial for many applications. The temporal facts in existing TKGs only contain their core entities (i.e., the entities playing core roles therein) and formulate them as quadruples, i.e., (subject entity, predicate, object entity, timestamp). This formulation oversimplifies temporal facts and inevitably causes information loss. Therefore, we propose to describe a temporal fact more... | Zhongni Hou, Xiaolong Jin, Zixuan Li, Long Bai, Saiping Guan, Yutao Zeng, Jiafeng Guo, Xueqi Cheng |  |
| 211 |  |  [Make Your Decision Convincing! A Unified Two-Stage Framework: Self-Attribution and Decision-Making](https://doi.org/10.18653/v1/2023.findings-emnlp.78) |  | 0 | Explaining black-box model behavior with natural language has achieved impressive results in various NLP tasks. Recent research has explored the utilization of subsequences from the input text as a rationale, providing users with evidence to support the model decision. Although existing frameworks excel in generating high-quality rationales while achieving high task performance, they neglect to account for the unreliable link between the generated rationale and model decision. In simpler terms,... | Yanrui Du, Sendong Zhao, Haochun Wang, Yuhan Chen, Rui Bai, Zewen Qiang, Muzhen Cai, Bing Qin |  |
| 212 |  |  [Adaptive Structure Induction for Aspect-based Sentiment Analysis with Spectral Perspective](https://doi.org/10.18653/v1/2023.findings-emnlp.79) |  | 0 | Recently, incorporating structure information (e.g. dependency syntactic tree) can enhance the performance of aspect-based sentiment analysis (ABSA). However, this structure information is obtained from off-the-shelf parsers, which is often sub-optimal and cumbersome. Thus, automatically learning adaptive structures is conducive to solving this problem. In this work, we concentrate on structure induction from pre-trained language models (PLMs) and throw the structure induction into a spectrum... | Hao Niu, Yun Xiong, Xiaosu Wang, Wenjing Yu, Yao Zhang, Zhonglei Guo |  |
| 213 |  |  [NovaCOMET: Open Commonsense Foundation Models with Symbolic Knowledge Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.80) |  | 0 | We present NovaCOMET, an open commonsense knowledge model, that combines the best aspects of knowledge and general task models. Compared to previous knowledge models, NovaCOMET allows open-format relations enabling direct application to reasoning tasks; compared to general task models like Flan-T5, it explicitly centers knowledge, enabling superior performance for commonsense reasoning. NovaCOMET leverages the knowledge of opaque proprietary models to create an open knowledge pipeline. First,... | Peter West, Ronan Le Bras, Taylor Sorensen, Bill Yuchen Lin, Liwei Jiang, Ximing Lu, Khyathi Raghavi Chandu, Jack Hessel, Ashutosh Baheti, Chandra Bhagavatula, Yejin Choi |  |
| 214 |  |  [In-Context Demonstration Selection with Cross Entropy Difference](https://doi.org/10.18653/v1/2023.findings-emnlp.81) |  | 0 | Large language models (LLMs) can use in-context demonstrations to improve performance on zero-shot tasks. However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples. We present a cross-entropy difference (CED) method for selecting in-context demonstrations. Our method is based on the observation that the effectiveness of in-context demonstrations negatively correlates with the perplexity of the test example by a... | Dan Iter, Reid Pryzant, Ruochen Xu, Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu |  |
| 215 |  |  [The Past, Present, and Future of Typological Databases in NLP](https://doi.org/10.18653/v1/2023.findings-emnlp.82) |  | 0 | Typological information has the potential to be beneficial in the development of NLP models, particularly for low-resource languages. Unfortunately, current large-scale typological databases, notably WALS and Grambank, are inconsistent both with each other and with other sources of typological information, such as linguistic grammars. Some of these inconsistencies stem from coding errors or linguistic variation, but many of the disagreements are due to the discrete categorical nature of these... | Emi Baylor, Esther Ploeger, Johannes Bjerva |  |
| 216 |  |  [SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations](https://doi.org/10.18653/v1/2023.findings-emnlp.83) |  | 0 | Large language models (LLMs) have been widely applied in various fields due to their excellent capability for memorizing knowledge and chain of thought (CoT). When these language models are applied in the field of psychological counseling, they often rush to provide universal advice. However, when users seek psychological support, they need to gain empathy, trust, understanding and comfort, rather than just reasonable advice. To this end, we constructed a multi-turn empathetic conversation... | Yirong Chen, Xiaofen Xing, Jingkai Lin, Huimin Zheng, Zhenyu Wang, Qi Liu, Xiangmin Xu |  |
| 217 |  |  [Can ChatGPT Assess Human Personalities? A General Evaluation Framework](https://doi.org/10.18653/v1/2023.findings-emnlp.84) |  | 0 | Large Language Models (LLMs) especially ChatGPT have produced impressive results in various areas, but their potential human-like psychology is still largely unexplored. Existing works study the virtual personalities of LLMs but rarely explore the possibility of analyzing human personalities via LLMs. This paper presents a generic evaluation framework for LLMs to assess human personalities based on Myers–Briggs Type Indicator (MBTI) tests. Specifically, we first devise unbiased prompts by... | Haocong Rao, Cyril Leung, Chunyan Miao |  |
| 218 |  |  [MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model](https://doi.org/10.18653/v1/2023.findings-emnlp.85) |  | 0 | Multi-modal open-domain question answering typically requires evidence retrieval from databases across diverse modalities, such as images, tables, passages, etc. Even Large Language Models (LLMs) like GPT-4 fall short in this task. To enable LLMs to tackle the task in a zero-shot manner, we introduce MoqaGPT, a straightforward and flexible framework. Using a divide-and-conquer strategy that bypasses intricate multi-modality ranking, our framework can accommodate new modalities and seamlessly... | Le Zhang, Yihong Wu, Fengran Mo, JianYun Nie, Aishwarya Agrawal |  |
| 219 |  |  [Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search](https://doi.org/10.18653/v1/2023.findings-emnlp.86) |  | 0 | Precisely understanding users’ contextual search intent has been an important challenge for conversational search. As conversational search sessions are much more diverse and long-tailed, existing methods trained on limited data still show unsatisfactory effectiveness and robustness to handle real conversational search scenarios. Recently, large language models (LLMs) have demonstrated amazing capabilities for text generation and conversation understanding. In this work, we present a simple yet... | Kelong Mao, Zhicheng Dou, Fengran Mo, Jiewen Hou, Haonan Chen, Hongjin Qian |  |
| 220 |  |  [DocAsRef: An Empirical Study on Repurposing Reference-based Summary Quality Metrics as Reference-free Metrics](https://doi.org/10.18653/v1/2023.findings-emnlp.87) |  | 0 | Automated summary quality assessment falls into two categories: reference-based and reference-free. Reference-based metrics, historically deemed more accurate due to the additional information provided by human-written references, are limited by their reliance on human input. In this paper, we hypothesize that the comparison methodologies used by some reference-based metrics to evaluate a system summary against its corresponding reference can be effectively adapted to assess it against its... | Forrest Sheng Bao, Ruixuan Tu, Ge Luo, Yinfei Yang, Hebi Li, Minghui Qiu, Youbiao He, Cen Chen |  |
| 221 |  |  [Toxicity in chatgpt: Analyzing persona-assigned language models](https://doi.org/10.18653/v1/2023.findings-emnlp.88) |  | 0 | Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Legislation has recognized its significance and recently drafted a “Blueprint For An AI Bill Of Rights” which... | Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan |  |
| 222 |  |  [Execution-Based Evaluation for Open-Domain Code Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.89) |  | 0 | To extend the scope of coding queries to more realistic settings, we propose ODEX, the first Open-Domain EXecution-based natural language (NL) to Python code generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse libraries, along with 1,707 human-written test cases for execution. Our NL-Code pairs are harvested from StackOverflow forums to encourage natural and practical coding queries. Moreover, ODEX supports four natural languages as intents, in English, Spanish, Japanese, and... | Zhiruo Wang, Shuyan Zhou, Daniel Fried, Graham Neubig |  |
| 223 |  |  [Syntax-Aware Retrieval Augmented Code Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.90) |  | 0 | Neural code generation models are nowadays widely adopted to generate code from natural language descriptions automatically. Recently, pre-trained neural models equipped with token-level retrieval capabilities have exhibited great potentials in neural machine translation. However, applying them directly to code generation experience challenges: the use of the retrieval-based mechanism inevitably introduces extraneous noise to the generation process, resulting in even syntactically incorrect... | Xiangyu Zhang, Yu Zhou, Guang Yang, Taolue Chen |  |
| 224 |  |  [Selecting Key Views for Zero-Shot Entity Linking](https://doi.org/10.18653/v1/2023.findings-emnlp.91) |  | 0 | Entity linking, which aligns mentions in the text to entities in knowledge bases, is essential for many natural language processing tasks. Considering the real-world scenarios, recent research hotspot of entity linking has focused on the zero-shot setting, where mentions need to link to unseen entities and only the description of each entity is provided. This task challenges the language understanding ability of models to capture the coherence evidence between the mention context and entity... | Xuhui Sui, Ying Zhang, Kehui Song, Baohang Zhou, Xiaojie Yuan, Wensheng Zhang |  |
| 225 |  |  [Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term](https://doi.org/10.18653/v1/2023.findings-emnlp.92) |  | 0 | With advancements in natural language processing (NLP) models, automatic explanation generation has been proposed to mitigate misinformation on social media platforms in addition to adding warning labels to identified fake news. While many researchers have focused on generating good explanations, how these explanations can really help humans combat fake news is under-explored. In this study, we compare the effectiveness of a warning label and the state-of- the-art counterfactual explanations... | YiLi Hsu, ShihChieh Dai, Aiping Xiong, LunWei Ku |  |
| 226 |  |  [Improving the Robustness of Summarization Models by Detecting and Removing Input Noise](https://doi.org/10.18653/v1/2023.findings-emnlp.93) |  | 0 | The evaluation of abstractive summarization models typically uses test data that is identically distributed as training data. In real-world practice, documents to be summarized may contain input noise caused by text extraction artifacts or data pipeline bugs. The robustness of model performance under distribution shift caused by such noise is relatively under studied. We present a large empirical study quantifying the sometimes severe loss in performance – up to 12 ROUGE-1 points – from... | Kundan Krishna, Yao Zhao, Jie Ren, Balaji Lakshminarayanan, Jiaming Luo, Mohammad Saleh, Peter J. Liu |  |
| 227 |  |  [How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts](https://doi.org/10.18653/v1/2023.findings-emnlp.94) |  | 0 | In recent years, there has been a rapid proliferation of AI-generated text, primarily driven by the release of powerful pre-trained language models (PLMs). To address the issue of misuse associated with AI-generated text, various high-performing detectors have been developed, including the OpenAI detector and the Stanford DetectGPT. In our study, we ask how reliable these detectors are. We answer the question by designing a novel approach that can prompt any PLM to generate text that evades... | Tharindu Kumarage, Paras Sheth, Raha Moraffah, Joshua Garland, Huan Liu |  |
| 228 |  |  [Knowledge is a Region in Weight Space for Fine-tuned Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.95) |  | 0 | Research on neural networks has focused on understanding a single model trained on a single dataset. However, relatively little is known about the relationships between different models, particularly those trained or tested on different datasets. We address this by studying how the weight space and the underlying loss landscape of different models are interconnected. Specifically, we demonstrate that finetuned models that were optimized for high performance, reside in well-defined regions in... | Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, Leshem Choshen |  |
| 229 |  |  [Unveiling the Multi-Annotation Process: Examining the Influence of Annotation Quantity and Instance Difficulty on Model Performance](https://doi.org/10.18653/v1/2023.findings-emnlp.96) |  | 0 | The NLP community has long advocated for the construction of multi-annotator datasets to better capture the nuances of language interpretation, subjectivity, and ambiguity. This paper conducts a retrospective study to show how performance scores can vary when a dataset expands from a single annotation per instance to multiple annotations. We propose a novel multi-annotator simulation process to generate datasets with varying annotation budgets. We show that similar datasets with the same... | Pritam Kadasi, Mayank Singh |  |
| 230 |  |  [On the Risk of Misinformation Pollution with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.97) |  | 0 | We investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation... | Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, MinYen Kan, William Yang Wang |  |
| 231 |  |  [Dolphin: A Challenging and Diverse Benchmark for Arabic NLG](https://doi.org/10.18653/v1/2023.findings-emnlp.98) |  | 0 | We present Dolphin, a novel benchmark that addresses the need for a natural language generation (NLG) evaluation framework dedicated to the wide collection of Arabic languages and varieties. The proposed benchmark encompasses a broad range of 13 different NLG tasks, including dialogue generation, question answering, machine translation, summarization, among others. Dolphin comprises a substantial corpus of 40 diverse and representative public datasets across 50 test splits, carefully curated to... | El Moatez Billah Nagoudi, AbdelRahim A. Elmadany, Ahmed Oumar ElShangiti, Muhammad AbdulMageed |  |
| 232 |  |  [Hierarchical Enhancement Framework for Aspect-based Argument Mining](https://doi.org/10.18653/v1/2023.findings-emnlp.99) |  | 0 | Aspect-Based Argument Mining (ABAM) is a critical task in computational argumentation. Existing methods have primarily treated ABAM as a nested named entity recognition problem, overlooking the need for tailored strategies to effectively address the specific challenges of ABAM tasks. To this end, we propose a layer-based Hierarchical Enhancement Framework (HEF) for ABAM, and introduce three novel components: the Semantic and Syntactic Fusion (SSF) component, the Batch-level Heterogeneous Graph... | Yujie Fu, Yang Li, Suge Wang, Xiaoli Li, Deyu Li, Jian Liao, Jianxing Zheng |  |
| 233 |  |  [MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.100) |  | 0 | Large language models (LLMs) have shown nearly saturated performance on many natural language processing (NLP) tasks. As a result, it is natural for people to believe that LLMs have also mastered abilities such as time understanding and reasoning. However, research on the temporal sensitivity of LLMs has been insufficiently emphasized. To fill this gap, this paper constructs Multiple Sensitive Factors Time QA (MenatQA), which encompasses three temporal factors (scope factor, order factor,... | Yifan Wei, Yisong Su, Huanhuan Ma, Xiaoyan Yu, Fangyu Lei, Yuanzhe Zhang, Jun Zhao, Kang Liu |  |
| 234 |  |  [What Makes Chain-of-Thought Prompting Effective? A Counterfactual Study](https://doi.org/10.18653/v1/2023.findings-emnlp.101) |  | 0 | The effectiveness of Chain-of-thought prompting (CoT) has been widely recognized, but the underlying mechanisms behind its success, the reason why it just works for a wide range of tasks, remains an open question. To investigate this, we employ a counterfactual prompting approach, systematically manipulating elements of examples used in a few-shot prompt, and testing the consequences on model behavior. This allows us to understand the relative contributions of prompt elements such as symbols... | Aman Madaan, Katherine Hermann, Amir Yazdanbakhsh |  |
| 235 |  |  [Perceptual Structure in the absence of grounding: the impact of abstractedness and subjectivity in color language for LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.102) |  | 0 | The need for grounding in language understanding is an active research topic. Previous work has suggested that color perception and color language appear as a suitable test bed to empirically study the problem, given its cognitive significance and showing that there is considerable alignment between a defined color space and the feature space defined by a language model. To further study this issue, we collect a large scale source of colors and their descriptions, containing almost a 1 million... | Pablo Loyola, Edison MarreseTaylor, Andrés Hoyos Idrobo |  |
| 236 |  |  [A Dataset for Investigating the Impact of Context for Offensive Language Detection in Tweets](https://doi.org/10.18653/v1/2023.findings-emnlp.103) |  | 0 | Offensive language detection is crucial in natural language processing (NLP). We investigated the importance of context for detecting such language in reply tweets on Twitter, where the use of offensive language is widespread. We collected a Turkish tweet dataset where the target group was unvaccinated people during the Covid period. Tweets in the dataset were enriched with contextual information by adding the original tweet to which a particular tweet was posted as a reply. The dataset, which... | Musa Ihtiyar, Ömer Özdemir, Mustafa Erengül, Arzucan Özgür |  |
| 237 |  |  [Remember what you did so you know what to do next](https://doi.org/10.18653/v1/2023.findings-emnlp.104) |  | 0 | We explore using the 6B parameter GPT-J language model to create a plan for a simulated robot to achieve 30 classes of goals in ScienceWorld, a text game simulator for elementary science experiments and for which previously published empirical work has shown large language models (LLM)s to be a poor fit (Wang et al., 2022). Using the Markov assumption, the LLM outperforms the state-of-the-art based on reinforcement learning by a factor of 1.4. When we fill the LLM’s input buffer with as many... | Manuel R. Ciosici, Alex Hedges, Yash Kankanampati, Justin Martin, Marjorie Freedman, Ralph M. Weischedel |  |
| 238 |  |  [An Empirical Study of Multimodal Model Merging](https://doi.org/10.18653/v1/2023.findings-emnlp.105) |  | 0 | Model merging (e.g., via interpolation or task arithmetic) fuses multiple models trained on different tasks to generate a multi-task solution. The technique has been proven successful in previous studies, where the models are trained on similar tasks and with the same initialization. In this paper, we expand on this concept to a multimodal setup by merging transformers trained on different modalities. Furthermore, we conduct our study for a novel goal where we can merge vision, language, and... | YiLin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, Lijuan Wang |  |
| 239 |  |  [Learning to Abstract with Nonparametric Variational Information Bottleneck](https://doi.org/10.18653/v1/2023.findings-emnlp.106) |  | 0 | Learned representations at the level of characters, sub-words, words, and sentences, have each contributed to advances in understanding different NLP tasks and linguistic phenomena. However, learning textual embeddings is costly as they are tokenization specific and require different models to be trained for each level of abstraction. We introduce a novel language representation model which can learn to compress to different levels of abstraction at different layers of the same model. We apply... | Melika Behjati, Fabio Fehr, James Henderson |  |
| 240 |  |  [Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document](https://doi.org/10.18653/v1/2023.findings-emnlp.107) |  | 0 | Visual Relation Extraction (VRE) is a powerful means of discovering relationships between entities within visually-rich documents. Existing methods often focus on manipulating entity features to find pairwise relations, yet neglect the more fundamental structural information that links disparate entity pairs together. The absence of global structure information may make the model struggle to learn long-range relations and easily predict conflicted results. To alleviate such limitations, we... | Xiangnan Chen, Qian Xiao, Juncheng Li, Duo Dong, Jun Lin, Xiaozhong Liu, Siliang Tang |  |
| 241 |  |  [Learning to Compose Representations of Different Encoder Layers towards Improving Compositional Generalization](https://doi.org/10.18653/v1/2023.findings-emnlp.108) |  | 0 | Recent studies have shown that sequence-to-sequence (seq2seq) models struggle with compositional generalization (CG), i.e., the ability to systematically generalize to unseen compositions of seen components. There is mounting evidence that one of the reasons hindering CG is the representation of the encoder uppermost layer is entangled, i.e., the syntactic and semantic representations of sequences are entangled. However, we consider that the previously identified representation entanglement... | Lei Lin, Shuangtao Li, Yafang Zheng, Biao Fu, Shan Liu, Yidong Chen, Xiaodong Shi |  |
| 242 |  |  [SelectNoise: Unsupervised Noise Injection to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.109) |  | 0 | In this work, we focus on the task of machine translation (MT) from extremely low-resource language (ELRLs) to English. The unavailability of parallel data, lack of representation from large multilingual pre-trained models, and limited monolingual data hinder the development of MT systems for ELRLs. However, many ELRLs often share lexical similarities with high-resource languages (HRLs) due to factors such as dialectical variations, geographical proximity, and language structure. We utilize... | Maharaj Brahma, Kaushal Maurya, Maunendra Sankar Desarkar |  |
| 243 |  |  [Breaking Boundaries in Retrieval Systems: Unsupervised Domain Adaptation with Denoise-Finetuning](https://doi.org/10.18653/v1/2023.findings-emnlp.110) |  | 0 | Dense retrieval models have exhibited remarkable effectiveness, but they rely on abundant labeled data and face challenges when applied to different domains. Previous domain adaptation methods have employed generative models to generate pseudo queries, creating pseudo datasets to enhance the performance of dense retrieval models. However, these approaches typically use unadapted rerank models, leading to potentially imprecise labels. In this paper, we demonstrate the significance of adapting... | Che Chen, Ching Yang, ChunYi Lin, HungYu Kao |  |
| 244 |  |  [Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach](https://doi.org/10.18653/v1/2023.findings-emnlp.111) |  | 0 | Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence. Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains. However, cognitive research on the overall knowledge structure of LLMs is still lacking. In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously... | Zheyuan Zhang, Jifan Yu, Juanzi Li, Lei Hou |  |
| 245 |  |  [Simpler neural networks prefer subregular languages](https://doi.org/10.18653/v1/2023.findings-emnlp.112) |  | 0 | We apply a continuous relaxation of L0 regularization (Louizos et al., 2017), which induces sparsity, to study the inductive biases of LSTMs. In particular, we are interested in the patterns of formal languages which are readily learned and expressed by LSTMs. Across a wide range of tests we find sparse LSTMs prefer subregular languages over regular languages and the strength of this preference increases as we increase the pressure for sparsity. Furthermore LSTMs which are trained on subregular... | Charles Torres, Richard Futrell |  |
| 246 |  |  [Simple Hardware-Efficient PCFGs with Independent Left and Right Productions](https://doi.org/10.18653/v1/2023.findings-emnlp.113) |  | 0 | Scaling dense PCFGs to thousands of nonterminals via low-rank parameterizations of the rule probability tensor has been shown to be beneficial for unsupervised parsing. However, PCFGs scaled this way still perform poorly as a language model, and even underperform similarly-sized HMMs. This work introduces SimplePCFG, a simple PCFG formalism with independent left and right productions. Despite imposing a stronger independence assumption than the low-rank approach, we find that this formalism... | Wei Liu, Songlin Yang, Yoon Kim, Kewei Tu |  |
| 247 |  |  [R³ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context](https://doi.org/10.18653/v1/2023.findings-emnlp.114) |  | 0 | With the help of Chain-of-Thought (CoT) prompting, Large Language Models (LLMs) have achieved remarkable performance on various reasoning tasks. However, most of them have been evaluated under noise-free context and the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated. Existing studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction. Inspired... | Qingyuan Tian, Hanlun Zhu, Lei Wang, Yang Li, Yunshi Lan |  |
| 248 |  |  [Quality Estimation-Assisted Automatic Post-Editing](https://doi.org/10.18653/v1/2023.findings-emnlp.115) |  | 0 | Automatic Post-Editing (APE) systems are prone to over-correction of the Machine Translation (MT) outputs. While Word-level Quality Estimation (QE) system can provide a way to curtail the over-correction, a significant performance gain has not been observed thus far by utilizing existing APE and QE combination strategies. In this paper, we propose joint training of a model on APE and QE tasks to improve the APE. Our proposed approach utilizes a multi-task learning (MTL) methodology, which shows... | Sourabh Dattatray Deoghare, Diptesh Kanojia, Frédéric Blain, Tharindu Ranasinghe, Pushpak Bhattacharyya |  |
| 249 |  |  [Adapter Pruning using Tropical Characterization](https://doi.org/10.18653/v1/2023.findings-emnlp.116) |  | 0 | Adapters are widely popular parameter-efficient transfer learning approaches in natural language processing that insert trainable modules in between layers of a pre-trained language model. Apart from several heuristics, however, there has been a lack of studies analyzing the optimal number of adapter parameters needed for downstream applications. Thus, we propose an adapter pruning approach by studying the tropical characteristics of trainable modules. We cast it as an optimization problem that... | Rishabh Bhardwaj, Tushar Vaidya, Soujanya Poria |  |
| 250 |  |  [Self-Supervised Rule Learning to Link Text Segments to Relational Elements of Structured Knowledge](https://doi.org/10.18653/v1/2023.findings-emnlp.117) |  | 0 | We present a neuro-symbolic approach to self-learn rules that serve as interpretable knowledge to perform relation linking in knowledge base question answering systems. These rules define natural language text predicates as a weighted mixture of knowledge base paths. The weights learned during training effectively serve the mapping needed to perform relation linking. We use popular masked training strategy to self-learn the rules. A key distinguishing aspect of our work is that the masked... | Shajith Ikbal, Udit Sharma, Hima Karanam, Sumit Neelam, Ronny Luss, Dheeraj Sreedhar, Pavan Kapanipathi, Naweed Khan, Kyle Erwin, Ndivhuwo Makondo, Ibrahim Abdelaziz, Achille Fokoue, Alexander Gray, Maxwell Crouse, Subhajit Chaudhury, Chitra Subramanian |  |
| 251 |  |  [TaTA: A Multilingual Table-to-Text Dataset for African Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.118) |  | 0 | Existing data-to-text generation datasets are mostly limited to English. To address this lack of data, we create Table-to-Text in African languages (TaTA), the first large multilingual table-to-text dataset with a focus on African languages. We created TaTA by transcribing figures and accompanying text in bilingual reports by the Demographic and Health Surveys Program, followed by professional translation to make the dataset fully parallel. TaTA includes 8,700 examples in nine languages... | Sebastian Gehrmann, Sebastian Ruder, Vitaly Nikolaev, Jan A. Botha, Michael Chavinda, Ankur P. Parikh, Clara Rivera |  |
| 252 |  |  [Explain-then-translate: an analysis on improving program translation with self-generated explanations](https://doi.org/10.18653/v1/2023.findings-emnlp.119) |  | 0 | This work explores the use of self-generated natural language explanations as an intermediate step for code-to-code translation with language models. Across three types of explanations and 19 programming languages constructed from the MultiPL-E dataset, we find the explanations to be particularly effective in the zero-shot case, improving performance by 12% on average. Improvements with natural language explanations are particularly pronounced on difficult programs. We release our dataset,... | Zilu Tang, Mayank Agarwal, Alexander Shypula, Bailin Wang, Derry Wijaya, Jie Chen, Yoon Kim |  |
| 253 |  |  [Can Brain Signals Reveal Inner Alignment with Human Languages?](https://doi.org/10.18653/v1/2023.findings-emnlp.120) |  | 0 | Brain Signals, such as Electroencephalography (EEG), and human languages have been widely explored independently for many downstream tasks, however, the connection between them has not been well explored. In this study, we explore the relationship and dependency between EEG and language. To study at the representation level, we introduced MTAM, a Multimodal Transformer Alignment Model, to observe coordinated representations between the two modalities. We used various relationship... | Jielin Qiu, William Han, Jiacheng Zhu, Mengdi Xu, Douglas Weber, Bo Li, Ding Zhao |  |
| 254 |  |  [DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource Event Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.121) |  | 0 | Most current Event Extraction (EE) methods focus on the high-resource scenario, which requires a large amount of annotated data and can hardly be applied to low-resource domains. To address EE more effectively with limited resources, we propose the Demonstration-enhanced Schema-guided Generation (DemoSG) model, which benefits low-resource EE from two aspects: Firstly, we propose the demonstration-based learning paradigm for EE to fully use the annotated data, which transforms them into... | Gang Zhao, Xiaocheng Gong, Xinjie Yang, Guanting Dong, Shudong Lu, Si Li |  |
| 255 |  |  [GLGR: Question-aware Global-to-Local Graph Reasoning for Multi-party Dialogue Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.122) |  | 0 | Graph reasoning contributes to the integration of discretely-distributed attentive information (clues) for Multi-party Dialogue Reading Comprehension (MDRC). This is attributed primarily to multi-hop reasoning over global conversational structures. However, existing approaches barely apply questions for anti-noise graph reasoning. More seriously, the local semantic structures in utterances are neglected, although they are beneficial for bridging across semantically-related clues. In this paper,... | Yanling Li, Bowei Zou, Yifan Fan, Xibo Li, Ai Ti Aw, Yu Hong |  |
| 256 |  |  [Towards Mitigating LLM Hallucination via Self Reflection](https://doi.org/10.18653/v1/2023.findings-emnlp.123) |  | 0 | Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of “hallucination”, where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of... | Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung |  |
| 257 |  |  [Making Body Movement in Sign Language Corpus Accessible for Linguists and Machines with Three-Dimensional Normalization of MediaPipe](https://doi.org/10.18653/v1/2023.findings-emnlp.124) |  | 0 | Linguists can access movement in the sign language video corpus through manual annotation or computational methods. The first relies on a predefinition of features, and the second requires technical knowledge. Methods like MediaPipe and OpenPose are now more often used in sign language processing. MediaPipe detects a two-dimensional (2D) body pose in a single image with a limited approximation of the depth coordinate. Such 2D projection of a three-dimensional (3D) body pose limits the potential... | Victor Skobov, Mayumi Bono |  |
| 258 |  |  [XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.125) |  | 0 | Data scarcity is a crucial issue for the development of highly multilingual NLP systems. Yet for many under-represented languages (ULs) — languages for which NLP research is particularly far behind in meeting user needs — it is feasible to annotate small amounts of data. Motivated by this, we propose XTREME-UP, a benchmark defined by: its focus on the scarce-data scenario rather than zero-shot; its focus on user-centric tasks — tasks with broad adoption by speakers of high-resource languages;... | Sebastian Ruder, Jonathan H. Clark, Alexander Gutkin, Mihir Kale, Min Ma, Massimo Nicosia, Shruti Rijhwani, Parker Riley, Jean Michel A. Sarr, Xinyi Wang, John Wieting, Nitish Gupta, Anna Katanova, Christo Kirov, Dana L. Dickinson, Brian Roark, Bidisha Samanta, Connie Tao, David Ifeoluwa Adelani, Vera Axelrod, Isaac Caswell, Colin Cherry, Dan Garrette, R. Reeve Ingle, Melvin Johnson, Dmitry Panteleev, Partha Talukdar |  |
| 259 |  |  [DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models](https://doi.org/10.18653/v1/2023.findings-emnlp.126) |  | 0 | Recent advances in image and video creation, especially AI-based image synthesis, have led to the production of numerous visual scenes that exhibit a high level of abstractness and diversity. Consequently, Visual Storytelling (VST), a task that involves generating meaningful and coherent narratives from a collection of images, has become even more challenging and is increasingly desired beyond real-world imagery. While existing VST techniques, which typically use autoregressive decoders, have... | Shengguang Wu, Mei Yuan, Qi Su |  |
| 260 |  |  [DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias](https://doi.org/10.18653/v1/2023.findings-emnlp.127) |  | 0 | Numerous debiasing techniques have been proposed to mitigate the gender bias that is prevalent in pretrained language models. These are often evaluated on datasets that check the extent to which the model is gender-neutral in its predictions. Importantly, this evaluation protocol overlooks the possible adverse impact of bias mitigation on useful gender knowledge. To fill this gap, we propose \*\*DiFair\*\*, a manually curated dataset based on masked language modeling objectives. \*\*DiFair\*\*... | Mahdi Zakizadeh, Kaveh Eskandari Miandoab, Mohammad Taher Pilehvar |  |
| 261 |  |  [Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens](https://doi.org/10.18653/v1/2023.findings-emnlp.128) |  | 0 | Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies. The current work aims to consolidate these findings by evaluating surprisal estimates from Transformer-based language model variants that vary systematically in the... | ByungDoh Oh, William Schuler |  |
| 262 |  |  [ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination](https://doi.org/10.18653/v1/2023.findings-emnlp.129) |  | 0 | In the field of Large Language Models (LLMs), researchers are increasingly exploring their effectiveness across a wide range of tasks. However, a critical area that requires further investigation is the interpretability of these models, particularly the ability to generate rational explanations for their decisions. Most existing explanation datasets are limited to the English language and the general domain, which leads to a scarcity of linguistic diversity and a lack of resources in... | Dongfang Li, Jindi Yu, Baotian Hu, Zhenran Xu, Min Zhang |  |
| 263 |  |  [CLASS: A Design Framework for Building Intelligent Tutoring Systems Based on Learning Science principles](https://doi.org/10.18653/v1/2023.findings-emnlp.130) |  | 0 | We present a design framework called Conversational Learning with Analytical Step-by-Step Strategies (CLASS) for building advanced Intelligent Tutoring Systems (ITS) powered by high-performance Large Language Models (LLMs). The CLASS framework empowers ITS with two key capabilities. First, through a carefully curated scaffolding dataset, CLASS equips ITS with essential problem-solving strategies, enabling it to provide tutor-like, step-by-step guidance to students. Second, by using a dynamic... | Shashank Sonkar, Naiming Liu, Debshila Basu Mallick, Richard G. Baraniuk |  |
| 264 |  |  [Normal-Abnormal Decoupling Memory for Medical Report Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.131) |  | 0 | The automatic generation of medical reports plays a crucial role in clinical automation. In contrast to natural images, radiological images exhibit a high degree of similarity, while medical data are prone to data bias and complex noise, posing challenges for existing methods in capturing nuanced visual information. To address these challenges, we introduce a novel normal-abnormal semantic decoupling network that utilizes abnormal pattern memory. Different from directly optimizing the network... | Guosheng Zhao, Yan Yan, Zijian Zhao |  |
| 265 |  |  [mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations](https://doi.org/10.18653/v1/2023.findings-emnlp.132) |  | 0 | Multilingual sequence-to-sequence models perform poorly with increased language coverage and fail to consistently generate text in the correct target language in few-shot settings. To address these challenges, we propose mmT5, a modular multilingual sequence-to-sequence model. mmT5 utilizes language-specific modules during pre-training, which disentangle language-specific information from language-agnostic information. We identify representation drift during fine-tuning as a key limitation of... | Jonas Pfeiffer, Francesco Piccinno, Massimo Nicosia, Xinyi Wang, Machel Reid, Sebastian Ruder |  |
| 266 |  |  [ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories](https://doi.org/10.18653/v1/2023.findings-emnlp.133) |  | 0 | Recently, Large Language Models (LLMs) have been serving as general-purpose interfaces, posing a significant demand for comprehensive visual knowledge. However, it remains unclear how well current LLMs and their visually augmented counterparts (VaLMs) can master visual commonsense knowledge. To investigate this, we propose ImageNetVC, a human-annotated dataset specifically designed for zero- and few-shot visual commonsense evaluation across 1,000 ImageNet categories. Utilizing ImageNetVC, we... | Heming Xia, Qingxiu Dong, Lei Li, Jingjing Xu, Tianyu Liu, Ziwei Qin, Zhifang Sui |  |
| 267 |  |  [MultiCoNER v2: a Large Multilingual dataset for Fine-grained and Noisy Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.134) |  | 0 | We present MULTICONER V2, a dataset for fine-grained Named Entity Recognition covering 33 entity classes across 12 languages, in both monolingual and multilingual settings. This dataset aims to tackle the following practical challenges in NER: (i) effective handling of fine-grained classes that include complex entities like movie titles, and (ii) performance degradation due to noise generated from typing mistakes or OCR errors. The dataset is compiled from open resources like Wikipedia and... | Besnik Fetahu, Zhiyu Chen, Sudipta Kar, Oleg Rokhlenko, Shervin Malmasi |  |
| 268 |  |  [A Query-Parallel Machine Reading Comprehension Framework for Low-resource NER](https://doi.org/10.18653/v1/2023.findings-emnlp.135) |  | 0 | Named entity recognition (NER) is a fundamental task in natural language processing. Recently, NER has been formulated as a machine reading comprehension (MRC) task, in which manually-crafted queries are used to extract entities of different types. However, current MRC-based NER techniques are limited to extracting a single type of entities at a time and are largely geared towards resource-rich settings. This renders them inefficient during the inference phase, while also leaving their... | Yuhao Zhang, Yongliang Wang |  |
| 269 |  |  [BiSPN: Generating Entity Set and Relation Set Coherently in One Pass](https://doi.org/10.18653/v1/2023.findings-emnlp.136) |  | 0 | By modeling the interaction among instances and avoiding error propagation, Set Prediction Networks (SPNs) achieve state-of-the-art performance on the tasks of named entity recognition and relation triple extraction respectively. However, how to jointly extract entities and relation triples via SPNs remains an unexplored problem, where the main challenge is the maintenance of coherence between the predicted entity/relation sets during one-pass generation. In this work, we present Bipartite Set... | Yuxin He, Buzhou Tang |  |
| 270 |  |  [MEEP: Is this Engaging? Prompting Large Language Models for Dialogue Evaluation in Multilingual Settings](https://doi.org/10.18653/v1/2023.findings-emnlp.137) |  | 0 | As dialogue systems become more popular, evaluation of their response quality gains importance. Engagingness highly correlates with overall quality and creates a sense of connection that gives human participants a more fulfilling experience. Although qualities like coherence and fluency are readily measured with well-worn automatic metrics, evaluating engagingness often relies on human assessment, which is a costly and time-consuming process. Existing automatic engagingness metrics evaluate the... | Amila Ferron, Amber Shore, Ekata Mitra, Ameeta Agrawal |  |
| 271 |  |  [Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.138) |  | 0 | Over the past few years, various domain-specific pretrained language models (PLMs) have been proposed and have outperformed general-domain PLMs in specialized areas such as biomedical, scientific, and clinical domains. In addition, financial PLMs have been studied because of the high economic impact of financial data analysis. However, we found that financial PLMs were not pretrained on sufficiently diverse financial data. This lack of diverse training data leads to a subpar generalization... | Jaeyoung Choe, Keonwoong Noh, Nayeon Kim, Seyun Ahn, Woohwan Jung |  |
| 272 |  |  [LLMDet: A Third Party Large Language Models Generated Text Detection Tool](https://doi.org/10.18653/v1/2023.findings-emnlp.139) |  | 0 | Generated texts from large language models (LLMs) are remarkably close to high-quality human-authored text, raising concerns about their potential misuse in spreading false information and academic misconduct. Consequently, there is an urgent need for a highly practical detection tool capable of accurately identifying the source of a given text. However, existing detection tools typically rely on access to LLMs and can only differentiate between machine-generated and human-authored text,... | Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng, TatSeng Chua |  |
| 273 |  |  [RECAP: Towards Precise Radiology Report Generation via Dynamic Disease Progression Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.140) |  | 0 | Automating radiology report generation can significantly alleviate radiologists’ workloads. Previous research has primarily focused on realizing highly concise observations while neglecting the precise attributes that determine the severity of diseases (e.g., small pleural effusion). Since incorrect attributes will lead to imprecise radiology reports, strengthening the generation process with precise attribute modeling becomes necessary. Additionally, the temporal information contained in the... | Wenjun Hou, Yi Cheng, Kaishuai Xu, Wenjie Li, Jiang Liu |  |
| 274 |  |  [Causal Intervention for Abstractive Related Work Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.141) |  | 0 | Abstractive related work generation has attracted increasing attention in generating coherent related work that helps readers grasp the current research. However, most existing models ignore the inherent causality during related work generation, leading to spurious correlations which downgrade the models’ generation quality and generalizability. In this study, we argue that causal intervention can address such limitations and improve the quality and coherence of generated related work. To this... | Jiachang Liu, Qi Zhang, Chongyang Shi, Usman Naseem, Shoujin Wang, Liang Hu, Ivor W. Tsang |  |
| 275 |  |  [G-SPEED: General SParse Efficient Editing MoDel](https://doi.org/10.18653/v1/2023.findings-emnlp.142) |  | 0 | Large Language Models (LLMs) have demonstrated incredible capabilities in understanding, generating, and manipulating languages. Through human-model interactions, LLMs can automatically understand human-issued instructions and output the expected contents, which can significantly increase working efficiency. In various types of real-world demands, editing-oriented tasks account for a considerable proportion, which involves an interactive process that entails the continuous refinement of... | Haoke Zhang, Yue Wang, Juntao Li, Xiabing Zhou, Min Zhang |  |
| 276 |  |  [Attack Prompt Generation for Red Teaming and Defending Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.143) |  | 0 | Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. Specifically, considering the impressive capabilities of newly emerged LLMs, we... | Boyi Deng, Wenjie Wang, Fuli Feng, Yang Deng, Qifan Wang, Xiangnan He |  |
| 277 |  |  [Smart "Chef": Verifying the Effect of Role-based Paraphrasing for Aspect Term Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.144) |  | 0 | We tackle Aspect Term Extraction (ATE), a task of automatically extracting aspect terms from sentences. The current Pretrained Language Model (PLM) based extractors have achieved significant improvements. They primarily benefit from context-aware encoding. However, a considerable number of sentences in ATE corpora contain uninformative or low-quality contexts. Such sentences frequently act as “troublemakers” during test. In this study, we explore the context-oriented quality improvement method.... | Jiaxiang Chen, Yu Hong, Qingting Xu, Jianmin Yao |  |
| 278 |  |  [Multi-Defendant Legal Judgment Prediction via Hierarchical Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.145) |  | 0 | Multiple defendants in a criminal fact description generally exhibit complex interactions, and cannot be well handled by existing Legal Judgment Prediction (LJP) methods which focus on predicting judgment results (e.g., law articles, charges, and terms of penalty) for single-defendant cases. To address this problem, we propose the task of multi-defendant LJP, which aims to automatically predict the judgment results for each defendant of multi-defendant cases. Two challenges arise with the task... | Yougang Lyu, Jitai Hao, Zihan Wang, Kai Zhao, Shen Gao, Pengjie Ren, Zhumin Chen, Fang Wang, Zhaochun Ren |  |
| 279 |  |  [Interpreting Indirect Answers to Yes-No Questions in Multiple Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.146) |  | 0 | Yes-no questions expect a yes or no for an answer, but people often skip polar keywords. Instead, they answer with long explanations that must be interpreted. In this paper, we focus on this challenging problem and release new benchmarks in eight languages. We present a distant supervision approach to collect training data, and demonstrate that direct answers (i.e., with polar keywords) are useful to train models to interpret indirect answers (i.e., without polar keywords). We show that... | Zijie Wang, Md Mosharaf Hossain, Shivam Mathur, Terry Cruz Melo, Kadir Bulut Özler, Keun Hee Park, Jacob Quintero, MohammadHossein Rezaei, Shreya Nupur Shakya, Md Nayem Uddin, Eduardo Blanco |  |
| 280 |  |  [Generalizing Few-Shot Named Entity Recognizers to Unseen Domains with Type-Related Features](https://doi.org/10.18653/v1/2023.findings-emnlp.147) |  | 0 | Few-shot named entity recognition (NER) has shown remarkable progress in identifying entities in low-resource domains. However, few-shot NER methods still struggle with out-of-domain (OOD) examples due to their reliance on manual labeling for the target domain. To address this limitation, recent studies enable generalization to an unseen target domain with only a few labeled examples using data augmentation techniques. Two important challenges remain: First, augmentation is limited to the... | Zihan Wang, Ziqi Zhao, Zhumin Chen, Pengjie Ren, Maarten de Rijke, Zhaochun Ren |  |
| 281 |  |  [Intervention-Based Alignment of Code Search with Execution Feedback](https://doi.org/10.18653/v1/2023.findings-emnlp.148) |  | 0 | One of the fundamental goals in code search is to retrieve a functionally correct code for a given natural language query. As annotating for correctness requires executing test cases (i.e. obtaining execution feedback), existing code search training datasets approximate text-code co-occurrences as positive execution feedback. However, this approximation may misalign models’ retrieval decisions from ground-truth correctness. To address such limitation, we propose Code Intervention-based... | Hojae Han, Minsoo Kim, Seungwon Hwang, Nan Duan, Shuai Lu |  |
| 282 |  |  [Enhancing Neural Machine Translation with Semantic Units](https://doi.org/10.18653/v1/2023.findings-emnlp.149) |  | 0 | Conventional neural machine translation (NMT) models typically use subwords and words as the basic units for model input and comprehension. However, complete words and phrases composed of several tokens are often the fundamental units for expressing semantics, referred to as semantic units. To address this issue, we propose a method Semantic Units for Machine Translation (SU4MT) which models the integral meanings of semantic units within a sentence, and then leverages them to provide a new... | Langlin Huang, Shuhao Gu, Zhuocheng Zhang, Yang Feng |  |
| 283 |  |  [DRAFT: Dense Retrieval Augmented Few-shot Topic classifier Framework](https://doi.org/10.18653/v1/2023.findings-emnlp.150) |  | 0 | With the growing volume of diverse information, the demand for classifying arbitrary topics has become increasingly critical. To address this challenge, we introduce DRAFT, a simple framework designed to train a classifier for few-shot topic classification. DRAFT uses a few examples of a specific topic as queries to construct Customized dataset with a dense retriever model. Multi-query retrieval (MQR) algorithm, which effectively handles multiple queries related to a specific topic, is applied... | Keonwoo Kim, Younggun Lee |  |
| 284 |  |  [A Framework for Exploring Player Perceptions of LLM-Generated Dialogue in Commercial Video Games](https://doi.org/10.18653/v1/2023.findings-emnlp.151) |  | 0 | The growing capabilities of large language models (LLMs) have inspired recent efforts to integrate LLM-generated dialogue into video games. However, evaluation remains a major challenge: how do we assess the player experience in a commercial game augmented with LLM-generated dialogue? To explore this question, we introduce a dynamic evaluation framework for the dialogue management systems that govern the task-oriented dialogue often found in roleplaying video games. We first extract dialogue... | Nader Akoury, Qian Yang, Mohit Iyyer |  |
| 285 |  |  [Generative Calibration for In-context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.152) |  | 0 | As one of the most exciting features of large language models (LLMs), in-context learning is a mixed blessing. While it allows users to fast-prototype a task solver with only a few training examples, the performance is generally sensitive to various configurations of the prompt such as the choice or order of the training examples. In this paper, we for the first time theoretically and empirically identify that such a paradox is mainly due to the label shift of the in-context model to the data... | Zhongtao Jiang, Yuanzhe Zhang, Cao Liu, Jun Zhao, Kang Liu |  |
| 286 |  |  [Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.153) |  | 0 | Few-shot relation extraction involves identifying the type of relationship between two specific entities within a text, using a limited number of annotated samples. A variety of solutions to this problem have emerged by applying meta-learning and neural graph techniques which typically necessitate a training process for adaptation. Recently, the strategy of in-context learning has been demonstrating notable results without the need of training. Few studies have already utilized in-context... | Xilai Ma, Jing Li, Min Zhang |  |
| 287 |  |  [AdaTranS: Adapting with Boundary-based Shrinking for End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.154) |  | 0 | To alleviate the data scarcity problem in End-to-end speech translation (ST), pre-training on data for speech recognition and machine translation is considered as an important technique. However, the modality gap between speech and text prevents the ST model from efficiently inheriting knowledge from the pre-trained models. In this work, we propose AdaTranS for end-to-end ST. It adapts the speech features with a new shrinking mechanism to mitigate the length mismatch between speech and text... | Xingshan Zeng, Liangyou Li, Qun Liu |  |
| 288 |  |  [No offence, Bert - I insult only humans! Multilingual sentence-level attack on toxicity detection networks](https://doi.org/10.18653/v1/2023.findings-emnlp.155) |  | 0 | We introduce a simple yet efficient sentence-level attack on black-box toxicity detector models. By adding several positive words or sentences to the end of a hateful message, we are able to change the prediction of a neural network and pass the toxicity detection system check. This approach is shown to be working on seven languages from three different language families. We also describe the defence mechanism against the aforementioned attack and discuss its limitations. | Sergey Berezin, Reza Farahbakhsh, Noël Crespi |  |
| 289 |  |  [Manipulating the Perceived Personality Traits of Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.156) |  | 0 | Psychology research has long explored aspects of human personality like extroversion, agreeableness and emotional stability, three of the personality traits that make up the ‘Big Five’. Categorizations like the ‘Big Five’ are commonly used to assess and diagnose personality types. In this work, we explore whether text generated from large language models exhibits consistency in it’s perceived ‘Big Five’ personality traits. For example, is a language model such as GPT2 likely to respond in a... | Graham Caron, Shashank Srivastava |  |
| 290 |  |  [WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia](https://doi.org/10.18653/v1/2023.findings-emnlp.157) |  | 0 | This paper presents the first few-shot LLM-based chatbot that almost never hallucinates and has high conversationality and low latency. WikiChat is grounded on the English Wikipedia, the largest curated free-text corpus. WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses. We distill WikiChat based on GPT-4 into a 7B-parameter LLaMA model with minimal loss of... | Sina J. Semnani, Violet Z. Yao, Heidi C. Zhang, Monica S. Lam |  |
| 291 |  |  [Automated Few-Shot Classification with Instruction-Finetuned Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.158) |  | 0 | A particularly successful class of approaches for few-shot learning combines language models with prompts - hand-crafted task descriptions that complement data samples. However, designing prompts by hand for each task commonly requires domain knowledge and substantial guesswork. We observe, in the context of classification tasks, that instruction finetuned language models are remarkably robust towards some dimensions of a prompt’s design. We subsequently propose a simple method to eliminate the... | Rami Aly, Xingjian Shi, Kaixiang Lin, Aston Zhang, Andrew Gordon Wilson |  |
| 292 |  |  [Meta-Learning of Prompt Generation for Lightweight Prompt Engineering on Language-Model-as-a-Service](https://doi.org/10.18653/v1/2023.findings-emnlp.159) |  | 0 | Recently, many companies have been providing the capabilities of large language models as services. These Language-Model-as-a-Service (LMaaS) offerings support a variety of user tasks through in-context learning from prompts, which include instructions and demonstrations of the task. However, for users, manually crafting prompts or running automatic prompt tuning methods themselves can be demanding. Despite these challenges, LMaaS providers do not offer automatic prompt engineering methods as... | Hyeonmin Ha, Jihye Lee, Wookje Han, ByungGon Chun |  |
| 293 |  |  [Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction](https://doi.org/10.18653/v1/2023.findings-emnlp.160) |  | 0 | The vital role of analogical reasoning in human cognition allows us to grasp novel concepts by linking them with familiar ones through shared relational structures. Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition. In response to this, our... | Siyu Yuan, Jiangjie Chen, Xuyang Ge, Yanghua Xiao, Deqing Yang |  |
| 294 |  |  [HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings](https://doi.org/10.18653/v1/2023.findings-emnlp.161) |  | 0 | In this paper, we propose a hierarchical contrastive learning framework, HiCL, which considers local segment-level and global sequence-level relationships to improve training efficiency and effectiveness. Traditional methods typically encode a sequence in its entirety for contrast with others, often neglecting local representation learning, leading to challenges in generalizing to shorter texts. Conversely, HiCL improves its effectiveness by dividing the sequence into several segments and... | Zhuofeng Wu, Chaowei Xiao, V. G. Vinod Vydiswaran |  |
| 295 |  |  [Density-Aware Prototypical Network for Few-Shot Relation Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.162) |  | 0 | In recent years, few-shot relation classification has evoked many research interests. Yet a more challenging problem, i.e. none-of-the-above (NOTA), is under-explored. Existing works mainly regard NOTA as an extra class and treat it the same as known relations. However, such a solution ignores the overall instance distribution, where NOTA instances are actually outliers and distributed unnaturally compared with known ones. In this paper, we propose a density-aware prototypical network (D-Proto)... | Jianfeng Wu, Mengting Hu, Yike Wu, Bingzhe Wu, Yalan Xie, Mingming Liu, Renhong Cheng |  |
| 296 |  |  [Improved Training of Deep Text Clustering](https://doi.org/10.18653/v1/2023.findings-emnlp.163) |  | 0 | The classical deep clustering optimization methods basically leverage information such as clustering centers, mutual information, and distance metrics to construct implicit generalized labels to establish information feedback (weak supervision) and thus optimize the deep model. However, the resulting generalized labels have different degrees of errors in the whole clustering process due to the limitation of clustering accuracy, which greatly interferes with the clustering process. To this end,... | Zonghao Yang, Wenpeng Hu, Yushan Tan, Zhunchen Luo |  |
| 297 |  |  [RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.164) |  | 0 | Retrieval-augmented language models show promise in addressing issues like outdated information and hallucinations in language models (LMs). However, current research faces two main problems: 1) determining what information to retrieve, and 2) effectively combining retrieved information during generation. We argue that valuable retrieved information should not only be related to the current source text but also consider the future target text, given the nature of LMs that model future tokens.... | Jingcheng Deng, Liang Pang, Huawei Shen, Xueqi Cheng |  |
| 298 |  |  [RefGPT: Dialogue Generation of GPT, by GPT, and for GPT](https://doi.org/10.18653/v1/2023.findings-emnlp.165) |  | 0 | Large Language Models (LLMs) have attained the impressive capability to resolve a wide range of NLP tasks by fine-tuning high-quality instruction data. However, collecting human-written data of high quality, especially multi-turn dialogues, is expensive and unattainable for most people. Though previous studies have used powerful LLMs to generate the dialogues automatically, they all suffer from generating untruthful dialogues because of the model hallucination. Therefore, we propose a method... | Dongjie Yang, Ruifeng Yuan, Yuantao Fan, Yifei Yang, Zili Wang, Shusen Wang, Hai Zhao |  |
| 299 |  |  [INA: An Integrative Approach for Enhancing Negotiation Strategies with Reward-Based Dialogue Agent](https://doi.org/10.18653/v1/2023.findings-emnlp.166) |  | 0 | In this paper, we propose a novel negotiation agent designed for the online marketplace. Our dialogue agent is integrative in nature i.e, it possesses the capability to negotiate on price as well as other factors, such as the addition or removal of items from a deal bundle, thereby offering a more flexible and comprehensive negotiation experience. To enable this functionality, we create a new dataset called Integrative Negotiation Dataset (IND). For this dataset creation, we introduce a new... | Zishan Ahmad, Suman Saurabh, Vaishakh Sreekanth Menon, Asif Ekbal, Roshni R. Ramnani, Anutosh Maitra |  |
| 300 |  |  [Large Language Models are Better Reasoners with Self-Verification](https://doi.org/10.18653/v1/2023.findings-emnlp.167) |  | 0 | Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact, after inferring... | Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao |  |
| 301 |  |  [Multi-Granularity Information Interaction Framework for Incomplete Utterance Rewriting](https://doi.org/10.18653/v1/2023.findings-emnlp.168) |  | 0 | Recent approaches in Incomplete Utterance Rewriting (IUR) fail to capture the source of important words, which is crucial to edit the incomplete utterance, and introduce words from irrelevant utterances. We propose a novel and effective multi-task information interaction framework including context selection, edit matrix construction, and relevance merging to capture the multi-granularity of semantic information. Benefiting from fetching the relevant utterance and figuring out the important... | Haowei Du, Dinghao Zhang, Chen Li, Yang Li, Dongyan Zhao |  |
| 302 |  |  [Accuracy is not enough: Evaluating Personalization in Summarizers](https://doi.org/10.18653/v1/2023.findings-emnlp.169) |  | 0 | Text summarization models are evaluated in terms of their accuracy and quality using various measures such as ROUGE, BLEU, METEOR, BERTScore, PYRAMID, readability, and several other recently proposed ones. The central objective of all accuracy measures is to evaluate the model’s ability to capture saliency accurately. Since saliency is subjective w.r.t the readers’ preferences, there cannot be a fit-all summary for a given document. This means that in many use-cases, summarization models need... | Rahul Vansh, Darsh Rank, Sourish Dasgupta, Tanmoy Chakraborty |  |
| 303 |  |  [For Generated Text, Is NLI-Neutral Text the Best Text?](https://doi.org/10.18653/v1/2023.findings-emnlp.170) |  | 0 | We explore incorporating natural language inference (NLI) into the text generative pipeline by using a pre-trained NLI model to assess whether a generated sentence entails, contradicts, or is neutral to the prompt and preceding text. First, we show that the NLI task is predictive of generation errors made by GPT-3. We use these results to develop an NLI-informed generation procedure for GPT-J. Then, we evaluate these generations by obtaining human annotations on error types and overall quality.... | Michail Mersinias, Kyle Mahowald |  |
| 304 |  |  [Combining Counting Processes and Classification Improves a Stopping Rule for Technology Assisted Review](https://doi.org/10.18653/v1/2023.findings-emnlp.171) |  | 0 | Technology Assisted Review (TAR) stopping rules aim to reduce the cost of manually assessing documents for relevance by minimising the number of documents that need to be examined to ensure a desired level of recall. This paper extends an effective stopping rule using information derived from a text classifier that can be trained without the need for any additional annotation. Experiments on multiple data sets (CLEF e-Health, TREC Total Recall, TREC Legal and RCV1) showed that the proposed... | Reem Bin Hezam, Mark Stevenson |  |
| 305 |  |  [Complexity-Guided Curriculum Learning for Text Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.172) |  | 0 | Curriculum learning provides a systematic approach to training. It refines training progressively, tailors training to task requirements, and improves generalization through exposure to diverse examples. We present a curriculum learning approach that builds on existing knowledge about text and graph complexity formalisms for training with text graph data. The core part of our approach is a novel data scheduler, which employs “spaced repetition” and complexity formalisms to guide the training... | Nidhi Vakil, Hadi Amiri |  |
| 306 |  |  [CoVariance-based Causal Debiasing for Entity and Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.173) |  | 0 | Joint entity and relation extraction tasks aim to recognize named entities and extract relations simultaneously. Suffering from a variety of data biases, such as data selection bias, and distribution bias (out of distribution, long-tail distribution), serious concerns can be witnessed to threaten the model’s transferability, robustness, and generalization. In this work, we address the above problems from a causality perspective. We propose a novel causal framework called c ̲ovariance and... | Lin Ren, Yongbin Liu, Yixin Cao, Chunping Ouyang |  |
| 307 |  |  [Multi-label and Multi-target Sampling of Machine Annotation for Computational Stance Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.174) |  | 0 | Data collection from manual labeling provides domain-specific and task-aligned supervision for data-driven approaches, and a critical mass of well-annotated resources is required to achieve reasonable performance in natural language processing tasks. However, manual annotations are often challenging to scale up in terms of time and budget, especially when domain knowledge, capturing subtle semantic features, and reasoning steps are needed. In this paper, we investigate the efficacy of... | Zhengyuan Liu, Hai Leong Chieu, Nancy F. Chen |  |
| 308 |  |  [In What Languages are Generative Language Models the Most Formal? Analyzing Formality Distribution across Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.175) |  | 0 | Multilingual generative language models (LMs) are increasingly fluent in a large variety of languages. Trained on the concatenation of corpora in multiple languages, they enable powerful transfer from high-resource languages to low-resource ones. However, it is still unknown what cultural biases are induced in the predictions of these models. In this work, we focus on one language property highly influenced by culture: formality. We analyze the formality distributions of XGLM and BLOOM’s... | Asim Ersoy, Gerson Vizcarra, Tasmiah Tahsin Mayeesha, Benjamin Muller |  |
| 309 |  |  [MaXM: Towards Multilingual Visual Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.176) |  | 0 | Visual Question Answering (VQA) has been primarily studied through the lens of the English language. Yet, tackling VQA in other languages in the same manner would require a considerable amount of resources. In this paper, we propose scalable solutions to multilingual visual question answering (mVQA), on both data and modeling fronts. We first propose a translation-based framework to mVQA data generation that requires much less human annotation efforts than the conventional approach of directly... | Soravit Changpinyo, Linting Xue, Michal Yarom, Ashish V. Thapliyal, Idan Szpektor, Julien Amelot, Xi Chen, Radu Soricut |  |
| 310 |  |  [Efficient Latent Variable Modeling for Knowledge-Grounded Dialogue Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.177) |  | 0 | Knowledge-grounded dialogue generation requires first retrieving appropriate external knowledge based on a conversational context and then generating a response grounded on the retrieved knowledge. In general, these two sequential modules, a knowledge retriever and a response generator, have been separately trained in a supervised manner. However, obtaining intermediate labels of the ground-truth knowledge is expensive, especially in open-domain conversations. Latent variable modeling avoids... | Gunsoo Han, Daejin Jo, Daniel Wontae Nam, Eunseop Yoon, Taehwan Kwon, Seungeun Rho, KyoungWoon On, Chang Dong Yoo, Sungwoong Kim |  |
| 311 |  |  [Ask To The Point: Open-Domain Entity-Centric Question Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.178) |  | 0 | We introduce a new task called \*entity-centric question generation\* (ECQG), motivated by real-world applications such as topic-specific learning, assisted reading, and fact-checking. The task aims to generate questions from an entity perspective. To solve ECQG, we propose a coherent PLM-based framework GenCONE with two novel modules: content focusing and question verification. The content focusing module first identifies a focus as “what to ask” to form draft questions, and the question... | Yuxiang Liu, Jie Huang, Kevin ChenChuan Chang |  |
| 312 |  |  [Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.179) |  | 0 | In open-domain question-answering (ODQA), most existing questions require single-hop reasoning on commonsense. To further extend this task, we officially introduce open-domain multi-hop reasoning (ODMR) by answering multi-hop questions with explicit reasoning steps in open-domain setting. Recently, large language models (LLMs) have found significant utility in facilitating ODQA without external corpus. Furthermore, chain-of-thought (CoT) prompting boosts the reasoning capability of LLMs to a... | Jinyuan Wang, Junlong Li, Hai Zhao |  |
| 313 |  |  [CASE: Commonsense-Augmented Score with an Expanded Answer Space](https://doi.org/10.18653/v1/2023.findings-emnlp.180) |  | 0 | LLMs have demonstrated impressive zero-shot performance on NLP tasks thanks to the knowledge they acquired in their training. In multiple-choice QA tasks, the LM probabilities are used as an imperfect measure of the plausibility of each answer choice. One of the major limitations of the basic score is that it treats all words as equally important. We propose CASE, a Commonsense-Augmented Score with an Expanded Answer Space. CASE addresses this limitation by assigning importance weights for... | Wenkai Chen, Sahithya Ravi, Vered Shwartz |  |
| 314 |  |  [GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.181) |  | 0 | Self-supervised representation learning on text-attributed graphs, which aims to create expressive and generalizable representations for various downstream tasks, has received increasing research attention lately. However, existing methods either struggle to capture the full extent of structural context information or rely on task-specific training labels, which largely hampers their effectiveness and generalizability in practice. To solve the problem of self-supervised representation learning... | Yichuan Li, Kaize Ding, Kyumin Lee |  |
| 315 |  |  [Sources of Hallucination by Large Language Models on Inference Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.182) |  | 0 | Large Language Models (LLMs) are claimed to be capable of Natural Language Inference (NLI), necessary for applied tasks like question answering and summarization. We present a series of behavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled experiments. We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs. First, memorization... | Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, Mark Steedman |  |
| 316 |  |  [Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer](https://doi.org/10.18653/v1/2023.findings-emnlp.183) |  | 0 | Pretrained transformer models have demonstrated remarkable performance across various natural language processing tasks. These models leverage the attention mechanism to capture long- and short-range dependencies in the sequence. However, the (full) attention mechanism incurs high computational cost – quadratic in the sequence length, which is not affordable in tasks with long sequences, e.g., inputs with 8k tokens. Although sparse attention can be used to improve computational efficiency, as... | Qingru Zhang, Dhananjay Ram, Cole Hawkins, Sheng Zha, Tuo Zhao |  |
| 317 |  |  [Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with Auxiliary Refined Knowledge](https://doi.org/10.18653/v1/2023.findings-emnlp.184) |  | 0 | Multimodal Named Entity Recognition (MNER) on social media aims to enhance textual entity prediction by incorporating image-based clues. Existing studies mainly focus on maximizing the utilization of pertinent image information or incorporating external knowledge from explicit knowledge bases. However, these methods either neglect the necessity of providing the model with external knowledge, or encounter issues of high redundancy in the retrieved knowledge. In this paper, we present PGIM — a... | Jinyuan Li, Han Li, Zhuo Pan, Di Sun, Jiahao Wang, Wenkun Zhang, Gang Pan |  |
| 318 |  |  [Understanding HTML with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.185) |  | 0 | Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding – i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval – have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii)... | Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin V. Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, Aleksandra Faust |  |
| 319 |  |  [The PEACE-Reviews dataset: Modeling Cognitive Appraisals in Emotion Text Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.186) |  | 0 | Cognitive appraisal plays a pivotal role in deciphering emotions. Recent studies have delved into its significance, yet the interplay between various forms of cognitive appraisal and specific emotions, such as joy and anger, remains an area of exploration in consumption contexts. Our research introduces the PEACE-Reviews dataset, a unique compilation of annotated autobiographical accounts where individuals detail their emotional and appraisal experiences during interactions with personally... | Gerard Yeo, Kokil Jaidka |  |
| 320 |  |  [UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model](https://doi.org/10.18653/v1/2023.findings-emnlp.187) |  | 0 | Text is ubiquitous in our visual world, conveying crucial information, such as in documents, websites, and everyday photographs. In this work, we propose UReader, a first exploration of universal OCR-free visually-situated language understanding based on the Multimodal Large Language Model (MLLM). By leveraging the shallow text recognition ability of the MLLM, we only finetuned 1.2% parameters and the training cost is much lower than previous work following domain-specific pretraining and... | Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Lin, Fei Huang |  |
| 321 |  |  [Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback](https://doi.org/10.18653/v1/2023.findings-emnlp.188) |  | 0 | Reinforcement learning from human feedback serves as a crucial bridge, aligning large language models with human and societal values. This alignment requires a vast corpus of human feedback to learn a reward model, which is subsequently used to finetune language models. However, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor... | Wei Shen, Rui Zheng, WenYu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 322 |  |  [Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions](https://doi.org/10.18653/v1/2023.findings-emnlp.189) |  | 0 | Large Language Models (LLMs) demonstrate impressive reasoning ability and the maintenance of world knowledge not only in natural language tasks, but also in some vision-language tasks such as open-domain knowledge-based visual question answering (OK-VQA). As images are invisible to LLMs, researchers convert images to text to engage LLMs into the visual question reasoning procedure. This leads to discrepancies between images and their textual representations presented to LLMs, which consequently... | Ziyue Wang, Chi Chen, Peng Li, Yang Liu |  |
| 323 |  |  [Take a Closer Look at Multilinguality! Improve Multilingual Pre-Training Using Monolingual Corpora Only](https://doi.org/10.18653/v1/2023.findings-emnlp.190) |  | 0 | Recent studies have revealed the remarkable cross-lingual capability of multilingual pre-trained language models (mPLMs), even when pre-trained without parallel corpora (mono-mPLMs). Intuitively, semantic alignments may be the reason behind such capability but remain under-explored. In this work, we investigate the alignment properties from the token perspective in mono-mPLMs and find that the alignments correspond to the geometric similarity of embedding space across different languages.... | Jinliang Lu, Yu Lu, Jiajun Zhang |  |
| 324 |  |  [LogiCoT: Logical Chain-of-Thought Instruction Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.191) |  | 0 | Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive chain-of-thought reasoning ability. Recent work on self-instruction tuning, such as Alpaca, has focused on enhancing the general proficiency of models. These instructions enable the model to achieve performance comparable to GPT-3.5 on general tasks like open-domain text generation and paraphrasing. However, they fall short of helping the model handle complex reasoning tasks. To bridge the gap, this paper presents LogiCoT, a... | Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, Yue Zhang |  |
| 325 |  |  [Hiding in Plain Sight: Tweets with Hate Speech Masked by Homoglyphs](https://doi.org/10.18653/v1/2023.findings-emnlp.192) |  | 0 | To avoid detection by current NLP monitoring applications, progenitors of hate speech often replace one or more letters in offensive words with homoglyphs, visually similar Unicode characters. Harvesting real-world hate speech containing homoglyphs is challenging due to the vast replacement possibilities. We developed a character substitution scraping method and assembled the Offensive Tweets with Homoglyphs (OTH) Dataset (N=90,788) with more than 1.5 million occurrences of 1,281 non-Latin... | Portia Cooper, Mihai Surdeanu, Eduardo Blanco |  |
| 326 |  |  [Reducing Spurious Correlations in Aspect-based Sentiment Analysis with Explanation from Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.193) |  | 0 | Recently, aspect-based sentiment analysis (ABSA) models have yielded promising results. However, they are susceptible to learning spurious correlations between certain words of the input text and output labels while modeling the sentiment feature of the aspect. This spurious correlation will potentially undermine the performance of ABSA models. One direct solution for this problem is to make the model see and learn an explanation of sentiment expression rather than certain words. Motivated by... | Qianlong Wang, Keyang Ding, Bin Liang, Min Yang, Ruifeng Xu |  |
| 327 |  |  [High-quality argumentative information in low resources approaches improve counter-narrative generation](https://doi.org/10.18653/v1/2023.findings-emnlp.194) |  | 0 | It has been shown that high quality fine-tuning boosts the performance of language models, even if the size of the fine-tuning is small. In this work we show how highly targeted fine-tuning improves the task of hate speech counter-narrative generation in user-generated text, even for very small sizes of training (1722 counter-narratives for English and 355 for Spanish). Providing a small subset of examples focusing on single argumentative strategies, together with the argumentative analysis... | Damián Ariel Furman, Pablo Torres, José A. Rodríguez, Diego Letzen, Maria Vanina Martinez, Laura Alonso Alemany |  |
| 328 |  |  [A Reference-free Segmentation Quality Index (SegReFree)](https://doi.org/10.18653/v1/2023.findings-emnlp.195) |  | 0 | Topic segmentation, in the context of natural language processing, is the process of finding boundaries in a sequence of sentences that separate groups of adjacent sentences at shifts in semantic meaning. Currently, assessing the quality of a segmentation is done by comparing segmentation boundaries selected by a human or algorithm to those selected by a known good reference. This means that it is not possible to quantify the quality of a segmentation without a human annotator, which can be... | Evan Lucas, Dylan Kangas, Timothy C. Havens |  |
| 329 |  |  [In-context Learning for Few-shot Multimodal Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.196) |  | 0 | Thanks in part to the availability of copious annotated resources for some entity categories, existing studies have achieved superior performance in multimodal named entity recognition (MNER). However, in the real-world scenario, it is infeasible to enumerate all entity categories in advance. Therefore, in this paper, we formulate a new few-shot multimodal named entity recognition (FewMNER) task, which aims to effectively locate and identify named entities for a text-image pair only using a... | Chenran Cai, Qianlong Wang, Bin Liang, Bing Qin, Min Yang, KamFai Wong, Ruifeng Xu |  |
| 330 |  |  [On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study](https://doi.org/10.18653/v1/2023.findings-emnlp.197) |  | 0 | Modern deep models for summarization attains impressive benchmark performance, but they are prone to generating miscalibrated predictive uncertainty. This means that they assign high confidence to low-quality predictions, leading to compromised reliability and trustworthiness in real-world applications. Probabilistic deep learning methods are common solutions to the miscalibration problem. However, their relative effectiveness in complex autoregressive summarization tasks are not... | Polina Zablotskaia, Du Phan, Joshua Maynez, Shashi Narayan, Jie Ren, Jeremiah Z. Liu |  |
| 331 |  |  [Handshape-Aware Sign Language Recognition: Extended Datasets and Exploration of Handshape-Inclusive Methods](https://doi.org/10.18653/v1/2023.findings-emnlp.198) |  | 0 | The majority of existing work on sign language recognition encodes signed videos without explicitly acknowledging the phonological attributes of signs. Given that handshape is a vital parameter in sign languages, we explore the potential of handshape-aware sign language recognition. We augment the PHOENIX14T dataset with gloss-level handshape labels, resulting in the new PHOENIX14T-HS dataset. Two unique methods are proposed for handshape-inclusive sign language recognition: a single-encoder... | Xuan Zhang, Kevin Duh |  |
| 332 |  |  [SimCKP: Simple Contrastive Learning of Keyphrase Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.199) |  | 0 | Keyphrase generation (KG) aims to generate a set of summarizing words or phrases given a source document, while keyphrase extraction (KE) aims to identify them from the text. Because the search space is much smaller in KE, it is often combined with KG to predict keyphrases that may or may not exist in the corresponding document. However, current unified approaches adopt sequence labeling and maximization-based generation that primarily operate at a token level, falling short in observing and... | Minseok Choi, Chaeheon Gwak, Seho Kim, Si Hyeong Kim, Jaegul Choo |  |
| 333 |  |  [LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain](https://doi.org/10.18653/v1/2023.findings-emnlp.200) |  | 0 | Lately, propelled by phenomenal advances around the transformer architecture, the legal NLP field has enjoyed spectacular growth. To measure progress, well-curated and challenging benchmarks are crucial. Previous efforts have produced numerous benchmarks for general NLP models, typically based on news or Wikipedia. However, these may not fit specific domains such as law, with its unique lexicons and intricate sentence structures. Even though there is a rising need to build NLP systems for... | Joel Niklaus, Veton Matoshi, Pooja Rani, Andrea Galassi, Matthias Stürmer, Ilias Chalkidis |  |
| 334 |  |  [Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.201) |  | 0 | Due to the remarkable language understanding and generation abilities of large language models (LLMs), their use in educational applications has been explored. However, little work has been done on investigating the pedagogical ability of LLMs in helping students to learn mathematics. In this position paper, we discuss the challenges associated with employing LLMs to enhance students’ mathematical problem-solving skills by providing adaptive feedback. Apart from generating the wrong reasoning... | AnZi Yen, WeiLing Hsu |  |
| 335 |  |  [Simultaneous Machine Translation with Tailored Reference](https://doi.org/10.18653/v1/2023.findings-emnlp.202) |  | 0 | Simultaneous machine translation (SiMT) generates translation while reading the whole source sentence. However, existing SiMT models are typically trained using the same reference disregarding the varying amounts of available source information at different latency. Training the model with ground-truth at low latency may introduce forced anticipations, whereas utilizing reference consistent with the source word order at high latency results in performance degradation. Consequently, it is... | Shoutao Guo, Shaolei Zhang, Yang Feng |  |
| 336 |  |  [Dynamic Voting for Efficient Reasoning in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.203) |  | 0 | Multi-path voting methods like Self-consistency have been used to mitigate reasoning errors in large language models caused by factual errors and illusion generation. However, these methods require excessive computing resources as they generate numerous reasoning paths for each problem. And our experiments show that on the arithmetic reasoning task, SVAMP, half of the problems fail to obtain noticeable accuracy gains when voting with more than three paths. In this paper, we propose a novel... | Mingfeng Xue, Dayiheng Liu, Wenqiang Lei, Xingzhang Ren, Baosong Yang, Jun Xie, Yidan Zhang, Dezhong Peng, Jiancheng Lv |  |
| 337 |  |  [On Surgical Fine-tuning for Language Encoders](https://doi.org/10.18653/v1/2023.findings-emnlp.204) |  | 0 | Fine-tuning all the layers of a pre-trained neural language encoder (either using all the parameters or using parameter-efficient methods) is often the de-facto way of adapting it to a new task. We show evidence that for different downstream language tasks, fine-tuning only a subset of layers is sufficient to obtain performance that is close to and often better than fine-tuning all the layers in the language encoder. We propose an efficient metric based on the diagonal of the Fisher information... | Abhilasha Lodha, Gayatri Belapurkar, Saloni Chalkapurkar, Yuanming Tao, Reshmi Ghosh, Samyadeep Basu, Dmitrii Petrov, Soundararajan Srinivasan |  |
| 338 |  |  [AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.205) |  | 0 | Recent large language models (LLMs) are promising for making decisions in grounded environments. However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment. Existing methods require either costly gradient computation or lengthy in-context demonstrations. In this paper, we propose AutoPlan, an approach to guide LLM-based agents to accomplish interactive decision-making tasks. AutoPlan... | Siqi Ouyang, Lei Li |  |
| 339 |  |  [Measuring Faithful and Plausible Visual Grounding in VQA](https://doi.org/10.18653/v1/2023.findings-emnlp.206) |  | 0 | Metrics for Visual Grounding (VG) in Visual Question Answering (VQA) systems primarily aim to measure a system’s reliance on relevant parts of the image when inferring an answer to the given question. Lack of VG has been a common problem among state-of-the-art VQA systems and can manifest in over-reliance on irrelevant image parts or a disregard for the visual modality entirely. Although inference capabilities of VQA models are often illustrated by a few qualitative illustrations, most systems... | Daniel Reich, Felix Putze, Tanja Schultz |  |
| 340 |  |  [Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.207) |  | 0 | Large language models (LLMs) enable zero-shot approaches in open-domain question answering (ODQA), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cost and the need for labeled data. We find that LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers. To tackle... | Sukmin Cho, Jeongyeon Seo, Soyeong Jeong, Jong C. Park |  |
| 341 |  |  [Can you Summarize my learnings? Towards Perspective-based Educational Dialogue Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.208) |  | 0 | The steady increase in the utilization of Virtual Tutors (VT) over recent years has allowed for a more efficient, personalized, and interactive AI-based learning experiences. A vital aspect in these educational chatbots is summarizing the conversations between the VT and the students, as it is critical in consolidating learning points and monitoring progress. However, the approach to summarization should be tailored according to the perspective. Summarization from the VTs perspective should... | Raghav Jain, Tulika Saha, Jhagrut Lalwani, Sriparna Saha |  |
| 342 |  |  [Adaptive Textual Label Noise Learning based on Pre-trained Models](https://doi.org/10.18653/v1/2023.findings-emnlp.209) |  | 0 | The label noise in real-world scenarios is unpredictable and can even be a mixture of different types of noise. To meet this challenge, we develop an adaptive textual label noise learning framework based on pre-trained models, which consists of an adaptive warm-up stage and a hybrid training stage. Specifically, an early stopping method, relying solely on the training set, is designed to dynamically terminate the warm-up process based on the model’s fit level to different noise scenarios. The... | Shaohuan Cheng, Wenyu Chen, Mingsheng Fu, Xuanting Xie, Hong Qu |  |
| 343 |  |  [Towards Informative Open-ended Text Generation with Dynamic Knowledge Triples](https://doi.org/10.18653/v1/2023.findings-emnlp.210) |  | 0 | Pretrained language models (PLMs), especially large language models (LLMs) demonstrate impressive capabilities in open-ended text generation. While our statistical results show that LLMs often suffer from over-concentrated information, where the generated texts overly focus on the given prompt and fail to provide sufficient background and detailed information as humans do. To address this issue, we propose a dynamic knowledge-guided informative open-ended text generation approach, that utilizes... | Zixuan Ren, Yang Zhao, Chengqing Zong |  |
| 344 |  |  [Novel Relation Detection: Discovering Unknown Relation Types via Multi-Strategy Self-Supervised Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.211) |  | 0 | Conventional approaches to relation extraction can only recognize predefined relation types. In the real world, new or out-of-scope relation types may keep challenging the deployed models. In this paper, we formalize such a challenging problem as Novel Relation Detection (NRD), which aims to discover potential new relation types based on training samples of known relations. To this end, we construct two NRD datasets and exhaustively investigate a variety of out-of-scope detection methods. We... | Qingbin Liu, Yin Kung, Yanchao Hao, Dianbo Sui, Siyuan Cheng, Xi Chen, Ningyu Zhang, Jiaoyan Chen |  |
| 345 |  |  [Ask Language Model to Clean Your Noisy Translation Data](https://doi.org/10.18653/v1/2023.findings-emnlp.212) |  | 0 | TTransformer models have demonstrated remarkable performance in neural machine translation (NMT). However, their vulnerability to noisy input poses a significant challenge in practical implementation, where generating clean output from noisy input is crucial. The MTNT dataset is widely used as a benchmark for evaluating the robustness of NMT models against noisy input. Nevertheless, its utility is limited due to the presence of noise in both the source and target sentences. To address this... | Quinten Bolding, Baohao Liao, Brandon James Denis, Jun Luo, Christof Monz |  |
| 346 |  |  [Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users](https://doi.org/10.18653/v1/2023.findings-emnlp.213) |  | 0 | While most task-oriented dialogues assume conversations between the agent and one user at a time, dialogue systems are increasingly expected to communicate with multiple users simultaneously who make decisions collaboratively. To facilitate development of such systems, we release the Multi-User MultiWOZ dataset: task-oriented dialogues among two users and one agent. To collect this dataset, each user utterance from MultiWOZ 2.2 was replaced with a small chat between two users that is... | Yohan Jo, Xinyan Zhao, Arijit Biswas, Nikoletta Basiou, Vincent Auvray, Nikolaos Malandrakis, Angeliki Metallinou, Alexandros Potamianos |  |
| 347 |  |  [Extractive Summarization via ChatGPT for Faithful Summary Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.214) |  | 0 | Extractive summarization is a crucial task in natural language processing that aims to condense long documents into shorter versions by directly extracting sentences. The recent introduction of large language models has attracted significant interest in the NLP community due to its remarkable performance on a wide range of downstream tasks. This paper first presents a thorough evaluation of ChatGPT’s performance on extractive summarization and compares it with traditional fine-tuning methods on... | Haopeng Zhang, Xiao Liu, Jiawei Zhang |  |
| 348 |  |  [MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization](https://doi.org/10.18653/v1/2023.findings-emnlp.215) |  | 0 | Prompt engineering, as an efficient and effective way to leverage Large Language Models (LLM), has drawn a lot of attention from the research community. The existing research primarily emphasizes the importance of adapting prompts to specific tasks, rather than specific LLMs. However, a good prompt is not solely defined by its wording, but also binds to the nature of the LLM in question. In this work, we first quantitatively demonstrate that different prompts should be adapted to different LLMs... | Yuyan Chen, Zhihao Wen, Ge Fan, Zhengyu Chen, Wei Wu, Dayiheng Liu, Zhixu Li, Bang Liu, Yanghua Xiao |  |
| 349 |  |  [PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.216) |  | 0 | Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual’s personality from their written texts, remains largely unexplored. Drawing inspiration from Psychological Questionnaires, which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items, we argue that... | Tao Yang, Tianyuan Shi, Fanqi Wan, Xiaojun Quan, Qifan Wang, Bingzhe Wu, Jiaxiang Wu |  |
| 350 |  |  [Harnessing the power of LLMs: Evaluating human-AI text co-creation through the lens of news headline generation](https://doi.org/10.18653/v1/2023.findings-emnlp.217) |  | 0 | To explore how humans can best leverage LLMs for writing and how interacting with these models affects feelings of ownership and trust in the writing process, we compared common human-AI interaction types (e.g., guiding system, selecting from system outputs, post-editing outputs) in the context of LLM-assisted news headline generation. While LLMs alone can generate satisfactory news headlines, on average, human control is needed to fix undesirable model outputs. Of the interaction methods,... | Zijian Ding, Alison SmithRenner, Wenjuan Zhang, Joel R. Tetreault, Alejandro Jaimes |  |
| 351 |  |  [NERetrieve: Dataset for Next Generation Named Entity Recognition and Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.218) |  | 0 | Recognizing entities in texts is a central need in many information-seeking scenarios, and indeed, Named Entity Recognition (NER) is arguably one of the most successful examples of a widely adopted NLP task and corresponding NLP technology. Recent advances in large language models (LLMs) appear to provide effective solutions (also) for NER tasks that were traditionally handled with dedicated models, often matching or surpassing the abilities of the dedicated models. Should NER be considered a... | Uri Katz, Matan Vetzler, Amir David Nissan Cohen, Yoav Goldberg |  |
| 352 |  |  [SWEET - Weakly Supervised Person Name Extraction for Fighting Human Trafficking](https://doi.org/10.18653/v1/2023.findings-emnlp.219) |  | 0 | In this work, we propose a weak supervision pipeline SWEET: Supervise Weakly for Entity Extraction to fight Trafficking for extracting person names from noisy escort advertisements. Our method combines the simplicity of rule-matching (through antirules, i.e., negated rules) and the generalizability of large language models fine-tuned on benchmark, domain-specific and synthetic datasets, treating them as weak labels. One of the major challenges in this domain is limited labeled data. SWEET... | Javin Liu, Hao Yu, Vidya Sujaya, Pratheeksha Nair, Kellin Pelrine, Reihaneh Rabbany |  |
| 353 |  |  [Watermarking LLMs with Weight Quantization](https://doi.org/10.18653/v1/2023.findings-emnlp.220) |  | 0 | Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains... | Linyang Li, Botian Jiang, Pengyu Wang, Ke Ren, Hang Yan, Xipeng Qiu |  |
| 354 |  |  [Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.221) |  | 0 | Spatial reasoning over text is challenging as the models not only need to extract the direct spatial information from the text but also reason over those and infer implicit spatial relations. Recent studies highlight the struggles even large language models encounter when it comes to performing spatial reasoning over text. In this paper, we explore the potential benefits of disentangling the processes of information extraction and reasoning in models to address this challenge. To explore this,... | Roshanak Mirzaee, Parisa Kordjamshidi |  |
| 355 |  |  [PsyAttention: Psychological Attention Model for Personality Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.222) |  | 0 | Work on personality detection has tended to incorporate psychological features from different personality models, such as BigFive and MBTI. There are more than 900 psychological features, each of which is helpful for personality detection. However, when used in combination, the application of different calculation standards among these features may result in interference between features calculated using distinct systems, thereby introducing noise and reducing performance. This paper adapts... | Baohua Zhang, Yongyi Huang, Wenyao Cui, Huaping Zhang, Jianyun Shang |  |
| 356 |  |  [RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training](https://doi.org/10.18653/v1/2023.findings-emnlp.223) |  | 0 | Fine-tuning pre-trained language models (LMs) has become the de facto standard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to robustness issues, such as adversarial robustness and model calibration. Several perspectives of robustness for LMs have been studied independently, but lacking a unified consideration in multiple perspectives. In this paper, we propose Robustifying LMs via Adversarial perturbation with Selective Training (RoAST), a simple yet effective fine-tuning... | Jaehyung Kim, Yuning Mao, Rui Hou, Hanchao Yu, Davis Liang, Pascale Fung, Qifan Wang, Fuli Feng, Lifu Huang, Madian Khabsa |  |
| 357 |  |  [The Law and NLP: Bridging Disciplinary Disconnects](https://doi.org/10.18653/v1/2023.findings-emnlp.224) |  | 0 | Legal practice is intrinsically rooted in the fabric of language, yet legal practitioners and scholars have been slow to adopt tools from natural language processing (NLP). At the same time, the legal system is experiencing an access to justice crisis, which could be partially alleviated with NLP. In this position paper, we argue that the slow uptake of NLP in legal practice is exacerbated by a disconnect between the needs of the legal community and the focus of NLP researchers. In a review of... | Robert Mahari, Dominik Stammbach, Elliott Ash, Alex Pentland |  |
| 358 |  |  [Symbolization, Prompt, and Classification: A Framework for Implicit Speaker Identification in Novels](https://doi.org/10.18653/v1/2023.findings-emnlp.225) |  | 0 | Speaker identification in novel dialogues can be widely applied to various downstream tasks, such as producing multi-speaker audiobooks and converting novels into scripts. However, existing state-of-the-art methods are limited to handling explicit narrative patterns like “Tom said, '...'", unable to thoroughly understand long-range contexts and to deal with complex cases. To this end, we propose a framework named SPC, which identifies implicit speakers in novels via symbolization, prompt, and... | Yue Chen, TianWei He, Hongbin Zhou, JiaChen Gu, Heng Lu, ZhenHua Ling |  |
| 359 |  |  [Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.226) |  | 0 | Pre-trained and frozen LLMs can effectively map simple scene re-arrangement instructions to programs over a robot’s visuomotor functions through appropriate few-shot example prompting. To parse open-domain natural language and adapt to a user’s idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short. In this paper, we introduce HELPER, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into... | Gabriel Sarch, Yue Wu, Michael J. Tarr, Katerina Fragkiadaki |  |
| 360 |  |  [ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought](https://doi.org/10.18653/v1/2023.findings-emnlp.227) |  | 0 | Recently Large Language Models (LLMs) have been proven to have strong abilities in various domains and tasks. We study the problem of prompt designing in the text-to-SQL task and attempt to improve the LLMs’ reasoning ability when generating SQL queries. Besides the trivial few-shot in-context learning setting, we design our chain-of-thought (CoT) prompt with a similar method to schema linking. We provide a method named ACT-SQL to automatically generate auto-CoT exemplars and thus the whole... | Hanchong Zhang, Ruisheng Cao, Lu Chen, Hongshen Xu, Kai Yu |  |
| 361 |  |  [Manifold-Preserving Transformers are Effective for Short-Long Range Encoding](https://doi.org/10.18653/v1/2023.findings-emnlp.228) |  | 0 | Multi-head self-attention-based Transformers have shown promise in different learning tasks. Albeit these models exhibit significant improvement in understanding short-term and long-term contexts from sequences, encoders of Transformers and their variants fail to preserve layer-wise contextual information. Transformers usually project tokens onto sparse manifolds and fail to preserve mathematical equivalence among the token representations. In this work, we propose TransJect, an encoder model... | Ayan Sengupta, Md. Shad Akhtar, Tanmoy Chakraborty |  |
| 362 |  |  [ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.229) |  | 0 | We present ASPIRO, an approach for structured data verbalisation into short template sentences in zero to few-shot settings. Unlike previous methods, our approach prompts Large Language Models (LLMs) to directly produce entity-agnostic templates, rather than relying on LLMs to faithfully copy the given example entities, or validating/crafting the templates manually. We incorporate LLM re-prompting, triggered by algorithmic parsing checks, as well as the PARENT metric induced consistency... | Martin Vejvar, Yasutaka Fujimoto |  |
| 363 |  |  [Detecting Syntactic Change with Pre-trained Transformer Models](https://doi.org/10.18653/v1/2023.findings-emnlp.230) |  | 0 | We investigate the ability of Transformer-based language models to find syntactic differences between the English of the early 1800s and that of the late 1900s. First, we show that a fine-tuned BERT model can distinguish between text from these two periods using syntactic information only; to show this, we employ a strategy to hide semantic information from the text. Second, we make further use of fine-tuned BERT models to identify specific instances of syntactic change and specific words for... | Liwen Hou, David Smith |  |
| 364 |  |  [A Word Sense Distribution-based approach for Semantic Change Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.231) |  | 0 | Semantic Change Detection of words is an important task for various NLP applications that must make time-sensitive predictions. Some words are used over time in novel ways to express new meanings, and these new meanings establish themselves as novel senses of existing words. On the other hand, Word Sense Disambiguation (WSD) methods associate ambiguous words with sense ids, depending on the context in which they occur. Given this relationship between WSD and SCD, we explore the possibility of... | Xiaohang Tang, Yi Zhou, Taichi Aida, Procheta Sen, Danushka Bollegala |  |
| 365 |  |  [Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.232) |  | 0 | Commonsense Knowledge Graphs (CSKGs) are crucial for commonsense reasoning, yet constructing them through human annotations can be costly. As a result, various automatic methods have been proposed to construct CSKG with larger semantic coverage. However, these unsupervised approaches introduce spurious noise that can lower the quality of the resulting CSKG, which cannot be tackled easily by existing denoising algorithms due to the unique characteristics of nodes and structures in CSKGs. To... | Zheye Deng, Weiqi Wang, Zhaowei Wang, Xin Liu, Yangqiu Song |  |
| 366 |  |  [Improving Conversational Recommendation Systems via Bias Analysis and Language-Model-Enhanced Data Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.233) |  | 0 | Conversational Recommendation System (CRS) is a rapidly growing research area that has gained significant attention alongside advancements in language modelling techniques. However, the current state of conversational recommendation faces numerous challenges due to its relative novelty and limited existing contributions. In this study, we delve into benchmark datasets for developing CRS models and address potential biases arising from the feedback loop inherent in multi-turn interactions,... | Xi Wang, Hossein A. Rahmani, Jiqun Liu, Emine Yilmaz |  |
| 367 |  |  [Exploring Graph Pre-training for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.234) |  | 0 | Existing studies tend to extract the sentiment elements in a generative manner in order to avoid complex modeling. Despite their effectiveness, they ignore importance of the relationships between sentiment elements that could be crucial, making the large pre-trained generative models sub-optimal for modeling sentiment knowledge. Therefore, we introduce two pre-training paradigms to improve the generation model by exploring graph pre-training that targeting to strengthen the model in capturing... | Xiaoyi Bao, Zhongqing Wang, Guodong Zhou |  |
| 368 |  |  [DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding](https://doi.org/10.18653/v1/2023.findings-emnlp.235) |  | 0 | Temporal Language Grounding seeks to localize video moments that semantically correspond to a natural language query. Recent advances employ the attention mechanism to learn the relations between video moments and the text query. However, naive attention might not be able to appropriately capture such relations, resulting in ineffective distributions where target video moments are difficult to separate from the remaining ones. To resolve the issue, we propose an energy-based model framework to... | Thong Nguyen, Xiaobao Wu, Xinshuai Dong, CongDuy Nguyen, SeeKiong Ng, Anh Tuan Luu |  |
| 369 |  |  [Test-time Augmentation for Factual Probing](https://doi.org/10.18653/v1/2023.findings-emnlp.236) |  | 0 | Factual probing is a method that uses prompts to test if a language model “knows” certain world knowledge facts. A problem in factual probing is that small changes to the prompt can lead to large changes in model output. Previous work aimed to alleviate this problem by optimizing prompts via text mining or fine-tuning. However, such approaches are relation-specific and do not generalize to unseen relation types. Here, we propose to use test-time augmentation (TTA) as a relation-agnostic method... | Go Kamoda, Benjamin Heinzerling, Keisuke Sakaguchi, Kentaro Inui |  |
| 370 |  |  [Methodological Insights in Detecting Subtle Semantic Shifts with Contextualized and Static Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.237) |  | 0 | In this paper, we investigate automatic detection of subtle semantic shifts between social communities of different political convictions in Dutch and English. We perform a methodological study comparing methods using static and contextualized language models. We investigate the impact of specializing contextualized models through fine-tuning on target corpora, word sense disambiguation and sentiment. We furthermore propose a new approach using masked token prediction, that relies on behavioral... | Sanne Hoeken, Özge Alaçam, Antske Fokkens, Pia Sommerauer |  |
| 371 |  |  [Disfluent Cues for Enhanced Speech Understanding in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.238) |  | 0 | In computational linguistics, the common practice is to “clean” disfluent content from spontaneous speech. However, we hypothesize that these disfluencies might serve as more than mere noise, potentially acting as informative cues. We use a range of pre-trained models for a reading comprehension task involving disfluent queries, specifically featuring different types of speech repairs. The findings indicate that certain disfluencies can indeed improve model performance, particularly those... | Morteza Rohanian, Farhad Nooralahzadeh, Omid Rohanian, David A. Clifton, Michael Krauthammer |  |
| 372 |  |  [Watermarking PLMs on Classification Tasks by Combining Contrastive Learning with Weight Perturbation](https://doi.org/10.18653/v1/2023.findings-emnlp.239) |  | 0 | Large pre-trained language models (PLMs) have achieved remarkable success, making them highly valuable intellectual property due to their expensive training costs. Consequently, model watermarking, a method developed to protect the intellectual property of neural models, has emerged as a crucial yet underexplored technique. The problem of watermarking PLMs has remained unsolved since the parameters of PLMs will be updated when fine-tuned on downstream datasets, and then embedded watermarks... | Chenxi Gu, Xiaoqing Zheng, Jianhan Xu, Muling Wu, Cenyuan Zhang, Chengsong Huang, Hua Cai, Xuanjing Huang |  |
| 373 |  |  [BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla Lemmatizer](https://doi.org/10.18653/v1/2023.findings-emnlp.240) |  | 0 | Lemmatization holds significance in both natural language processing (NLP) and linguistics, as it effectively decreases data density and aids in comprehending contextual meaning. However, due to the highly inflected nature and morphological richness, lemmatization in Bangla text poses a complex challenge. In this study, we propose linguistic rules for lemmatization and utilize a dictionary along with the rules to design a lemmatizer specifically for Bangla. Our system aims to lemmatize words... | Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, Md. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed |  |
| 374 |  |  [Exploring the Sensitivity of LLMs' Decision-Making Capabilities: Insights from Prompt Variations and Hyperparameters](https://doi.org/10.18653/v1/2023.findings-emnlp.241) |  | 0 | The advancement of Large Language Models (LLMs) has led to their widespread use across a broad spectrum of tasks, including decision-making. Prior studies have compared the decision-making abilities of LLMs with those of humans from a psychological perspective. However, these studies have not always properly accounted for the sensitivity of LLMs’ behavior to hyperparameters and variations in the prompt. In this study, we examine LLMs’ performance on the Horizon decision-making task studied by... | Manikanta Loya, Divya Sinha, Richard Futrell |  |
| 375 |  |  [Search Augmented Instruction Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.242) |  | 0 | Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from... | Hongyin Luo, Tianhua Zhang, YungSung Chuang, Yuan Gong, Yoon Kim, Xixin Wu, Helen Meng, James R. Glass |  |
| 376 |  |  ["Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters](https://doi.org/10.18653/v1/2023.findings-emnlp.243) |  | 0 | Large Language Models (LLMs) have recently emerged as an effective tool to assist individuals in writing various types of content, including professional documents such as recommendation letters. Though bringing convenience, this application also introduces unprecedented fairness concerns. Model-generated reference letters might be directly used by users in professional scenarios. If underlying biases exist in these model-constructed letters, using them without scrutinization could lead to... | Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, KaiWei Chang, Nanyun Peng |  |
| 377 |  |  [TextMixer: Mixing Multiple Inputs for Privacy-Preserving Inference](https://doi.org/10.18653/v1/2023.findings-emnlp.244) |  | 0 | Pre-trained language models (PLMs) are often deployed as cloud services, enabling users to upload textual data and perform inference remotely. However, users’ personal text often contains sensitive information, and sharing such data directly with the service providers can lead to serious privacy leakage. To address this problem, we introduce a novel privacy-preserving inference framework called MixPi , which prevents plaintext leakage during the inference phase. Inspired by k-anonymity, MixPi... | Xin Zhou, Yi Lu, Ruotian Ma, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 378 |  |  [FinePrompt: Unveiling the Role of Finetuned Inductive Bias on Compositional Reasoning in GPT-4](https://doi.org/10.18653/v1/2023.findings-emnlp.245) |  | 0 | Compositional reasoning across texts has been a long-standing challenge in natural language processing. With large language models like GPT-4 taking over the field, prompting techniques such as chain-of-thought (CoT) were proposed to unlock compositional, multi-step reasoning capabilities of LLMs. Despite their success, the prompts demand significant human effort to discover and validate them. Our work draws attention to the idea of transferring task-specific inductive biases from finetuned... | Jeonghwan Kim, Giwon Hong, SungHyon Myaeng, Joyce Jiyoung Whang |  |
| 379 |  |  [Teacher Perception of Automatically Extracted Grammar Concepts for L2 Language Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.246) |  | 0 | One of the challenges in language teaching is how best to organize rules regarding syntax, semantics, or phonology in a meaningful manner. This not only requires content creators to have pedagogical skills, but also have that language’s deep understanding. While comprehensive materials to develop such curricula are available in English and some broadly spoken languages, for many other languages, teachers need to manually create them in response to their students’ needs. This is challenging... | Aditi Chaudhary, Arun Sampath, Ashwin Sheshadri, Antonios Anastasopoulos, Graham Neubig |  |
| 380 |  |  [Allies: Prompting Large Language Model with Beam Search](https://doi.org/10.18653/v1/2023.findings-emnlp.247) |  | 0 | With the advance of large language models (LLMs), the research field of LLM applications becomes more and more popular and the idea of constructing pipelines to accomplish complex tasks by stacking LLM API calls come true. However, this kind of methods face two limitations: narrow information coverage and low fault tolerance. In this work, we propose a novel method called ALLIES. Given an input query, ALLIES leverages LLMs to iteratively generate new queries related to the original query,... | Hao Sun, Xiao Liu, Yeyun Gong, Yan Zhang, Daxin Jiang, Linjun Yang, Nan Duan |  |
| 381 |  |  [Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.248) |  | 0 | Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes... | Liangming Pan, Alon Albalak, Xinyi Wang, William Yang Wang |  |
| 382 |  |  [SiMFy: A Simple Yet Effective Approach for Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.249) |  | 0 | Temporal Knowledge Graph (TKG) reasoning, which focuses on leveraging temporal information to infer future facts in knowledge graphs, plays a vital role in knowledge graph completion. Typically, existing works for this task design graph neural networks and recurrent neural networks to respectively capture the structural and temporal information in KGs. Despite their effectiveness, in our practice, we find that they tend to suffer the issues of low training efficiency and insufficient... | Zhengtao Liu, Lei Tan, Mengfan Li, Yao Wan, Hai Jin, Xuanhua Shi |  |
| 383 |  |  [Understanding Translationese in Cross-Lingual Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.250) |  | 0 | Given a document in a source language, cross-lingual summarization (CLS) aims at generating a concise summary in a different target language. Unlike monolingual summarization (MS), naturally occurring source-language documents paired with target-language summaries are rare. To collect large-scale CLS data, existing datasets typically involve translation in their creation. However, the translated text is distinguished from the text originally written in that language, i.e., translationese. In... | Jiaan Wang, Fandong Meng, Yunlong Liang, Tingyi Zhang, Jiarong Xu, Zhixu Li, Jie Zhou |  |
| 384 |  |  [The Truth, The Whole Truth, and Nothing but the Truth: A New Benchmark Dataset for Hebrew Text Credibility Assessment](https://doi.org/10.18653/v1/2023.findings-emnlp.251) |  | 0 | In the age of information overload, it is more important than ever to discern fact from fiction. From the internet to traditional media, we are constantly confronted with a deluge of information, much of which comes from politicians and other public figures who wield significant influence. In this paper, we introduce HeTrue: a new, publicly available dataset for evaluating the credibility of statements made by Israeli public figures and politicians. This dataset consists of 1021 statements,... | Ben Hagag, Reut Tsarfaty |  |
| 385 |  |  [IndiSocialFT: Multilingual Word Representation for Indian languages in code-mixed environment](https://doi.org/10.18653/v1/2023.findings-emnlp.252) |  | 0 | The increasing number of Indian language users on the internet necessitates the development of Indian language technologies. In response to this demand, our paper presents a generalized representation vector for diverse text characteristics, including native scripts, transliterated text, multilingual, code-mixed, and social media-related attributes. We gather text from both social media and well-formed sources and utilize the FastText model to create the “IndiSocialFT” embedding. Through... | Saurabh Kumar, Sanasam Ranbir Singh, Sukumar Nandi |  |
| 386 |  |  [Adaptive Hinge Balance Loss for Document-Level Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.253) |  | 0 | Document-Level Relation Extraction aims at predicting relations between entities from multiple sentences. A common practice is to select multi-label classification thresholds to decide whether a relation exists between an entity pair. However, in the document-level task, most entity pairs do not express any relations, resulting in a highly imbalanced distribution between positive and negative classes. We argue that the imbalance problem affects threshold selection and may lead to incorrect... | Jize Wang, Xinyi Le, Xiaodi Peng, Cailian Chen |  |
| 387 |  |  [Answer-state Recurrent Relational Network (AsRRN) for Constructed Response Assessment and Feedback Grouping](https://doi.org/10.18653/v1/2023.findings-emnlp.254) |  | 0 | STEM educators must trade off the ease of assessing selected response (SR) questions, like multiple choice, with constructed response (CR) questions, where students articulate their own reasoning. Our work addresses a CR type new to NLP but common in college STEM, consisting of multiple questions per context. To relate the context, the questions, the reference responses, and students’ answers, we developed an Answer-state Recurrent Relational Network (AsRRN). In recurrent time-steps, relation... | Zhaohui Li, Susan Lloyd, Matthew Beckman, Rebecca J. Passonneau |  |
| 388 |  |  [Low-Resource Comparative Opinion Quintuple Extraction by Data Augmentation with Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.255) |  | 0 | Comparative Opinion Quintuple Extraction (COQE) aims to predict comparative opinion quintuples from comparative sentences. These quintuples include subject, object, shareable aspect, comparative opinion, and preference. The existing pipeline-based COQE method fails in error propagation. In addition, the complexity and insufficient amounts of annotated data hinder the performance of COQE models. In this paper, we introduce a novel approach called low-resource comparative opinion quintuple... | Qingting Xu, Yu Hong, Fubang Zhao, Kaisong Song, Yangyang Kang, Jiaxiang Chen, Guodong Zhou |  |
| 389 |  |  [A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.256) |  | 0 | Large Language Models (LLMs) have shown their ability to collaborate effectively with humans in real-world scenarios. However, LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks. In this paper, we propose a self-check approach based on reverse validation to detect factual errors automatically in a zero-resource fashion. To facilitate future studies and assess different... | Shiping Yang, Renliang Sun, Xiaojun Wan |  |
| 390 |  |  [Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.257) |  | 0 | We propose Speculative Decoding (SpecDec), for the first time ever, to formally study exploiting the idea of speculative execution to accelerate autoregressive (AR) decoding. Speculative Decoding has two innovations: Spec-Drafter – an independent model specially optimized for efficient and accurate drafting – and Spec-Verification – a reliable method for verifying the drafted tokens efficiently in the decoding paradigm. Experimental results on various seq2seq tasks including machine translation... | Heming Xia, Tao Ge, Peiyi Wang, SiQing Chen, Furu Wei, Zhifang Sui |  |
| 391 |  |  [APP: Adaptive Prototypical Pseudo-Labeling for Few-shot OOD Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.258) |  | 0 | Detecting out-of-domain (OOD) intents from user queries is essential for a task-oriented dialogue system. Previous OOD detection studies generally work on the assumption that plenty of labeled IND intents exist. In this paper, we focus on a more practical few-shot OOD setting where there are only a few labeled IND data and massive unlabeled mixed data that may belong to IND or OOD. The new scenario carries two key challenges: learning discriminative representations using limited IND data and... | Pei Wang, Keqing He, Yutao Mou, Xiaoshuai Song, Yanan Wu, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu |  |
| 392 |  |  [2INER: Instructive and In-Context Learning on Few-Shot Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.259) |  | 0 | Prompt-based learning has emerged as a powerful technique in natural language processing (NLP) due to its ability to leverage pre-training knowledge for downstream few-shot tasks. In this paper, we propose 2INER, a novel text-to-text framework for Few-Shot Named Entity Recognition (NER) tasks. Our approach employs instruction finetuning based on InstructionNER to enable the model to effectively comprehend and process task-specific instructions, including both main and auxiliary tasks. We also... | Jiasheng Zhang, Xikai Liu, Xinyi Lai, Yan Gao, Shusen Wang, Yao Hu, Yiqing Lin |  |
| 393 |  |  [Generative Emotion Cause Triplet Extraction in Conversations with Commonsense Knowledge](https://doi.org/10.18653/v1/2023.findings-emnlp.260) |  | 0 | Emotion Cause Triplet Extraction in Conversations (ECTEC) aims to simultaneously extract emotion utterances, emotion categories, and cause utterances from conversations. However, existing studies mainly decompose the ECTEC task into multiple subtasks and solve them in a pipeline manner. Moreover, since conversations tend to contain many informal and implicit expressions, it often requires external knowledge and reasoning-based inference to accurately identify emotional and causal clues... | Fanfan Wang, Jianfei Yu, Rui Xia |  |
| 394 |  |  [Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.261) |  | 0 | Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such as their focus on low-level features and lack of explainability at higher-level text units. In this work, we introduce proto-lm, a prototypical network-based white-box framework that allows LLMs to learn immediately... | Sean Xie, Soroush Vosoughi, Saeed Hassanpour |  |
| 395 |  |  [GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence](https://doi.org/10.18653/v1/2023.findings-emnlp.262) |  | 0 | Conditional story generation is significant in human-machine interaction, particularly in producing stories with complex plots. While Large language models (LLMs) perform well on multiple NLP tasks, including story generation, it is challenging to generate stories with both complex and creative plots. Existing methods often rely on detailed prompts to guide LLMs to meet target conditions, which inadvertently restrict the creative potential of the generated stories. We argue that leveraging... | Zhihua Wen, Zhiliang Tian, Wei Wu, Yuxin Yang, Yanqi Shi, Zhen Huang, Dongsheng Li |  |
| 396 |  |  [KAPALM: Knowledge grAPh enhAnced Language Models for Fake News Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.263) |  | 0 | Social media has not only facilitated news consumption, but also led to the wide spread of fake news. Because news articles in social media is usually condensed and full of knowledge entities, existing methods of fake news detection use external entity knowledge. However, majority of these methods focus on news entity information and ignore the structured knowledge among news entities. To address this issue, in this work, we propose a Knowledge grAPh enhAnced Language Model (KAPALM) which is a... | Jing Ma, Chen Chen, Chunyan Hou, Xiaojie Yuan |  |
| 397 |  |  [Comparing the Evaluation and Production of Loophole Behavior in Humans and Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.264) |  | 0 | In law, lore, and everyday life, loopholes are commonplace. When people exploit a loophole, they understand the intended meaning or goal of another person, but choose to go with a different interpretation. Past and current AI research has shown that artificial intelligence engages in what seems superficially like the exploitation of loopholes, but this is likely anthropomorphization. It remains unclear to what extent current models, especially Large Language Models (LLMs), capture the pragmatic... | Sonia K. Murthy, Kiera Parece, Sophie Bridgers, Peng Qian, Tomer D. Ullman |  |
| 398 |  |  [InstructExcel: A Benchmark for Natural Language Instruction in Excel](https://doi.org/10.18653/v1/2023.findings-emnlp.265) |  | 0 | With the evolution of Large Language Models (LLMs) we can solve increasingly more complex NLP tasks across various domains, including spreadsheets. This work investigates whether LLMs can generate code (Excel OfficeScripts, a TypeScript API for executing many tasks in Excel) that solves Excel specific tasks provided via natural language user instructions. To do so we introduce a new large-scale benchmark, InstructExcel, created by leveraging the ‘Automate’ feature in Excel to automatically... | Justin Payan, Swaroop Mishra, Mukul Singh, Carina Negreanu, Christian Pölitz, Chitta Baral, Subhro Roy, Rasika Chakravarthy, Benjamin Van Durme, Elnaz Nouri |  |
| 399 |  |  [Hallucination Detection for Grounded Instruction Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.266) |  | 0 | We investigate the problem of generating instructions to guide humans to navigate in simulated residential environments. A major issue with current models is hallucination: they generate references to actions or objects that are inconsistent with what a human follower would perform or encounter along the described path. We develop a model that detects these hallucinated references by adopting a model pre-trained on a large corpus of image-text pairs, and fine-tuning it with a contrastive loss... | Lingjun Zhao, Khanh Nguyen, Hal Daumé III |  |
| 400 |  |  [Definitions Matter: Guiding GPT for Multi-label Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.267) |  | 0 | Large language models have recently risen in popularity due to their ability to perform many natural language tasks without requiring any fine-tuning. In this work, we focus on two novel ideas: (1) generating definitions from examples and using them for zero-shot classification, and (2) investigating how an LLM makes use of the definitions. We thoroughly analyze the performance of GPT-3 model for fine-grained multi-label conspiracy theory classification of tweets using zero-shot labeling. In... | Youri Peskine, Damir Korencic, Ivan Grubisic, Paolo Papotti, Raphaël Troncy, Paolo Rosso |  |
| 401 |  |  [ECHo: A Visio-Linguistic Dataset for Event Causality Inference via Human-Centric Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.268) |  | 0 | We introduce ECHo (Event Causality Inference via Human-Centric Reasoning), a diagnostic dataset of event causality inference grounded in visio-linguistic social scenarios. ECHo employs real-world human-centric deductive information building on a television crime drama. ECHo requires the Theory-of-Mind (ToM) ability to understand and reason about social interactions based on multimodal information. Using ECHo, we propose a unified Chain-of-Thought (CoT) framework to assess the reasoning... | Yuxi Xie, Guanzhen Li, MinYen Kan |  |
| 402 |  |  [An Empirical Study of Instruction-tuning Large Language Models in Chinese](https://doi.org/10.18653/v1/2023.findings-emnlp.269) |  | 0 | The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the open-source community’s interest in instruction-tuning, which is deemed to accelerate ChatGPT’s replication process. However, research on instruction-tuning LLMs in Chinese, the world’s most spoken language, is still in its early stages. Therefore, this paper makes an in-depth empirical study of instruction-tuning LLMs in... | Qingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, Weiping Wang |  |
| 403 |  |  [Debiasing Multimodal Models via Causal Information Minimization](https://doi.org/10.18653/v1/2023.findings-emnlp.270) |  | 0 | Most existing debiasing methods for multimodal models, including causal intervention and inference methods, utilize approximate heuristics to represent the biases, such as shallow features from early stages of training or unimodal features for multimodal tasks like VQA, etc., which may not be accurate. In this paper, we study bias arising from confounders in a causal graph for multimodal data, and examine a novel approach that leverages causally-motivated information minimization to learn the... | Vaidehi Patil, Adyasha Maharana, Mohit Bansal |  |
| 404 |  |  [Evaluating Emotion Arcs Across Languages: Bridging the Global Divide in Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.271) |  | 0 | Emotion arcs capture how an individual (or a population) feels over time. They are widely used in industry and research; however, there is little work on evaluating the automatically generated arcs. This is because of the difficulty of establishing the true (gold) emotion arc. Our work, for the first time, systematically and quantitatively evaluates automatically generated emotion arcs. We also compare two common ways of generating emotion arcs: Machine-Learning (ML) models and Lexicon-Only... | Daniela Teodorescu, Saif M. Mohammad |  |
| 405 |  |  [Multi-step Jailbreaking Privacy Attacks on ChatGPT](https://doi.org/10.18653/v1/2023.findings-emnlp.272) |  | 0 | With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given appropriate prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is... | Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, Yangqiu Song |  |
| 406 |  |  [Chain-of-Thought Embeddings for Stance Detection on Social Media](https://doi.org/10.18653/v1/2023.findings-emnlp.273) |  | 0 | Stance detection on social media is challenging for Large Language Models (LLMs), as emerging slang and colloquial language in online conversations often contain deeply implicit stance labels. Chain-of-Thought (COT) prompting has recently been shown to improve performance on stance detection tasks — alleviating some of these issues. However, COT prompting still struggles with implicit stance identification. This challenge arises because many samples are initially challenging to comprehend... | Joseph Gatto, Omar Sharif, Sarah Preum |  |
| 407 |  |  [Using LLM for Improving Key Event Discovery: Temporal-Guided News Stream Clustering with Event Summaries](https://doi.org/10.18653/v1/2023.findings-emnlp.274) |  | 0 | Understanding and characterizing the discus- sions around key events in news streams is important for analyzing political discourse. In this work, we study the problem of identification of such key events and the news articles associated with those events from news streams. We propose a generic framework for news stream clustering that analyzes the temporal trend of news articles to automatically extract the underlying key news events that draw significant media attention. We characterize such... | Nishanth Sridhar Nakshatri, Siyi Liu, Sihao Chen, Dan Roth, Dan Goldwasser, Daniel Hopkins |  |
| 408 |  |  [Descriptive Prompt Paraphrasing for Target-Oriented Multimodal Sentiment Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.275) |  | 0 | Target-Oriented Multimodal Sentiment Classification (TMSC) aims to perform sentiment polarity on a target jointly considering its corresponding multiple modalities including text, image, and others. Current researches mainly work on either of two types of targets in a decentralized manner. One type is entity, such as a person name, a location name, etc. and the other is aspect, such as ‘food’, ‘service’, etc. We believe that this target type based division in task modelling is not necessary... | Dan Liu, Lin Li, Xiaohui Tao, Jian Cui, Qing Xie |  |
| 409 |  |  [Joint Semantic and Strategy Matching for Persuasive Dialogue](https://doi.org/10.18653/v1/2023.findings-emnlp.276) |  | 0 | Persuasive dialogue aims to persuade users to achieve some targets by conversations. While previous persuasion models have achieved notable successes, they mostly base themselves on utterance semantic matching, and an important aspect has been ignored, that is, the strategy of the conversations, for example, the agent can choose an emotional-appeal strategy to impress users. Compared with utterance semantics, conversation strategies are high-level concepts, which can be informative and provide... | Chuhao Jin, Yutao Zhu, Lingzhen Kong, Shijie Li, Xiao Zhang, Ruihua Song, Xu Chen, Huan Chen, Yuchong Sun, Yu Chen, Jun Xu |  |
| 410 |  |  [Non-Autoregressive Sentence Ordering](https://doi.org/10.18653/v1/2023.findings-emnlp.277) |  | 0 | Existing sentence ordering approaches generally employ encoder-decoder frameworks with the pointer net to recover the coherence by recurrently predicting each sentence step-by-step. Such an autoregressive manner only leverages unilateral dependencies during decoding and cannot fully explore the semantic dependency between sentences for ordering. To overcome these limitations, in this paper, we propose a novel Non-Autoregressive Ordering Network, dubbed NAON, which explores bilateral... | Yi Bin, Wenhao Shi, Bin Ji, Jipeng Zhang, Yujuan Ding, Yang Yang |  |
| 411 |  |  [Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.278) |  | 0 | With the recent undeniable advancement in reasoning abilities in large language models (LLMs) like ChatGPT and GPT-4, there is a growing trend for using LLMs on various tasks. One area where LLMs can be employed is as an alternative evaluation metric for complex generative tasks, which generally demands expensive human judges to complement the traditional automatic metrics for various evaluation dimensions such as fluency and consistency. In this work, we conduct extensive analysis to... | Chenhui Shen, Liying Cheng, XuanPhi Nguyen, Yang You, Lidong Bing |  |
| 412 |  |  [Women Wearing Lipstick: Measuring the Bias Between an Object and Its Related Gender](https://doi.org/10.18653/v1/2023.findings-emnlp.279) |  | 0 | In this paper, we investigate the impact of objects on gender bias in image captioning systems. Our results show that only gender-specific objects have a strong gender bias (e.g., women-lipstick). In addition, we propose a visual semantic-based gender score that measures the degree of bias and can be used as a plug-in for any image captioning system. Our experiments demonstrate the utility of the gender score, since we observe that our score can measure the bias relation between a caption and... | Ahmed Sabir, Lluís Padró |  |
| 413 |  |  [FREDSum: A Dialogue Summarization Corpus for French Political Debates](https://doi.org/10.18653/v1/2023.findings-emnlp.280) |  | 0 | Recent advances in deep learning, and especially the invention of encoder-decoder architectures, have significantly improved the performance of abstractive summarization systems. While the majority of research has focused on written documents, we have observed an increasing interest in the summarization of dialogues and multi-party conversations over the past few years. In this paper, we present a dataset of French political debates for the purpose of enhancing resources for multi-lingual... | Virgile Rennard, Guokan Shang, Damien Grari, Julie Hunter, Michalis Vazirgiannis |  |
| 414 |  |  [Towards Zero-shot Relation Extraction in Web Mining: A Multimodal Approach with Relative XML Path](https://doi.org/10.18653/v1/2023.findings-emnlp.281) |  | 0 | The rapid growth of web pages and the increasing complexity of their structure poses a challenge for web mining models. Web mining models are required to understand semi-structured web pages, particularly when little is known about the subject or template of a new page. Current methods migrate language models to web mining by embedding the XML source code into the transformer or encoding the rendered layout with graph neural networks. However, these approaches do not take into account the... | Zilong Wang, Jingbo Shang |  |
| 415 |  |  [Narrative Style and the Spread of Health Misinformation on Twitter](https://doi.org/10.18653/v1/2023.findings-emnlp.282) |  | 0 | Using a narrative style is an effective way to communicate health information both on and off social media. Given the amount of misinformation being spread online and its potential negative effects, it is crucial to investigate the interplay between narrative communication style and misinformative health content on user engagement on social media platforms. To explore this in the context of Twitter, we start with previously annotated health misinformation tweets (n ≈15,000) and annotate a... | Achyutarama R. Ganti, Eslam Ali Hassan Hussein, Steven R. Wilson, Zexin Ma, Xinyan Zhao |  |
| 416 |  |  [HadSkip: Homotopic and Adaptive Layer Skipping of Pre-trained Language Models for Efficient Inference](https://doi.org/10.18653/v1/2023.findings-emnlp.283) |  | 0 | Pre-trained language models (LMs) have brought remarkable performance on numerous NLP tasks. However, they require significant resources and entail high computational costs for inference, making them challenging to deploy in real-world and real-time systems. Existing early exiting methods aim to reduce computational complexity by selecting the layer at which to exit, but suffer from the limitation that they have to sequentially traverse through all layers prior to the selected exit layer, which... | Haoyu Wang, Yaqing Wang, Tianci Liu, Tuo Zhao, Jing Gao |  |
| 417 |  |  [Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.284) |  | 0 | Mental illness remains one of the most critical public health issues of our time, due to the severe scarcity and accessibility limit of professionals. Psychotherapy requires high-level expertise to conduct deep, complex reasoning and analysis on the cognition modeling of the patients. In the era of Large Language Models, we believe it is the right time to develop AI assistance for computational psychotherapy. We study the task of cognitive distortion detection and propose the Diagnosis of... | Zhiyu Chen, Yujie Lu, William Yang Wang |  |
| 418 |  |  [Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.285) |  | 0 | While pre-trained language models (PLMs) have shown evidence of acquiring vast amounts of knowledge, it remains unclear how much of this parametric knowledge is actually usable in performing downstream tasks. We propose a systematic framework to measure parametric knowledge utilization in PLMs. Our framework first extracts knowledge from a PLM’s parameters and subsequently constructs a downstream task around this extracted knowledge. Performance on this task thus depends exclusively on... | Amirhossein Kazemnejad, Mehdi Rezagholizadeh, Prasanna Parthasarathi, Sarath Chandar |  |
| 419 |  |  [Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.286) |  | 0 | Non-compositional expressions, by virtue of their non-compositionality, are a classic ‘pain in the neck’ for NLP systems. Different from the general language modeling and generation tasks that are primarily compositional, generating non-compositional expressions is more challenging for current neural models, including large pre-trained language models. The main reasons are 1) their non-compositionality, and 2) the limited data resources. Therefore, to make the best use of available data for... | Jianing Zhou, Ziheng Zeng, Hongyu Gong, Suma Bhat |  |
| 420 |  |  [Information Extraction from Legal Wills: How Well Does GPT-4 Do?](https://doi.org/10.18653/v1/2023.findings-emnlp.287) |  | 0 | This work presents a manually annotated dataset for Information Extraction (IE) from legal wills, and relevant in-context learning experiments on the dataset. The dataset consists of entities, binary relations between the entities (e.g., relations between testator and beneficiary), and n-ary events (e.g., bequest) extracted from 45 legal wills from two US states. This dataset can serve as a foundation for downstream tasks in the legal domain. Another use case of this dataset is evaluating the... | Alice Saebom Kwak, Cheonkam Jeong, Gaetano Forte, Derek E. Bambauer, Clayton T. Morrison, Mihai Surdeanu |  |
| 421 |  |  [Transparency at the Source: Evaluating and Interpreting Language Models With Access to the True Distribution](https://doi.org/10.18653/v1/2023.findings-emnlp.288) |  | 0 | We present a setup for training, evaluating and interpreting neural language models, that uses artificial, language-like data. The data is generated using a massive probabilistic grammar (based on state-split PCFGs), that is itself derived from a large natural language corpus, but also provides us complete control over the generative process. We describe and release both grammar and corpus, and test for the naturalness of our generated data. This approach allows us define closed-form... | Jaap Jumelet, Willem H. Zuidema |  |
| 422 |  |  [Continual Generalized Intent Discovery: Marching Towards Dynamic and Open-world Intent Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.289) |  | 0 | In a practical dialogue system, users may input out-of-domain (OOD) queries. The Generalized Intent Discovery (GID) task aims to discover OOD intents from OOD queries and extend them to the in-domain (IND) classifier. However, GID only considers one stage of OOD learning, and needs to utilize the data in all previous stages for joint training, which limits its wide application in reality. In this paper, we introduce a new task, Continual Generalized Intent Discovery (CGID), which aims to... | Xiaoshuai Song, Yutao Mou, Keqing He, Yueyan Qiu, Jinxu Zhao, Pei Wang, Weiran Xu |  |
| 423 |  |  [Frugal Prompting for Dialog Models](https://doi.org/10.18653/v1/2023.findings-emnlp.290) |  | 0 | The use of large language models (LLMs) in natural language processing (NLP) tasks is rapidly increasing, leading to changes in how researchers approach problems in the field. To fully utilize these models’ abilities, a better understanding of their behavior for different input protocols is required. With LLMs, users can directly interact with the models through a text-based interface to define and solve various tasks. Hence, understanding the conversational abilities of these LLMs, which may... | Bishal Santra, Sakya Basak, Abhinandan De, Manish Gupta, Pawan Goyal |  |
| 424 |  |  [The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.291) |  | 0 | End-to-end spoken language understanding (SLU) remains elusive even with current large pretrained language models on text and speech, especially in multilingual cases. Machine translation has been established as a powerful pretraining objective on text as it enables the model to capture high-level semantics of the input utterance and associations between different languages, which is desired for speech models that work on lower-level acoustic frames. Motivated particularly by the task of... | Mutian He, Philip N. Garner |  |
| 425 |  |  [MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space](https://doi.org/10.18653/v1/2023.findings-emnlp.292) |  | 0 | Multi-aspect controllable text generation aims to generate fluent sentences that possess multiple desired attributes simultaneously. Traditional methods either require expensive iteration / searching within the discrete text space during the decoding stage, or train separate controllers for each aspect, resulting in a degradation of text quality due to the discrepancy between different aspects. To address these limitations, we introduce a novel approach for Multi-aspect control, namely MacLaSa,... | Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, Xueqi Cheng, TatSeng Chua |  |
| 426 |  |  [HPE: Answering Complex Questions over Text by Hybrid Question Parsing and Execution](https://doi.org/10.18653/v1/2023.findings-emnlp.293) |  | 0 | The dominant paradigm of textual question answering systems is based on end-to-end neural networks, which excels at answering natural language questions but falls short on complex ones. This stands in contrast to the broad adaptation of semantic parsing approaches over structured data sources (e.g., relational database, knowledge graphs), that convert natural language questions to logical forms and execute them with query engines. Towards combining the strengths of neural and symbolic methods,... | Ye Liu, Semih Yavuz, Rui Meng, Dragomir Radev, Caiming Xiong, Shafiq Joty, Yingbo Zhou |  |
| 427 |  |  [Length-Adaptive Distillation: Customizing Small Language Model for Dynamic Token Pruning](https://doi.org/10.18653/v1/2023.findings-emnlp.294) |  | 0 | Pre-trained language models greatly improve the performance of various tasks but at a cost of high computation overhead. To facilitate practical applications, there are mainly two lines of research to accelerate model inference: model compression and dynamic computation (e.g., dynamic token pruning). Existing works either adopt these methods individually or simply apply dynamic computation approaches upon a compressed small language model. We argue that they are sub-optimal since the two... | Chang Liu, Chongyang Tao, Jianxin Liang, Jiazhan Feng, Tao Shen, Quzhe Huang, Dongyan Zhao |  |
| 428 |  |  [Toxicity, Morality, and Speech Act Guided Stance Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.295) |  | 0 | In this work, we focus on the task of determining the public attitude toward various social issues discussed on social media platforms. Platforms such as Twitter, however, are often used to spread misinformation, fake news through polarizing views. Existing literature suggests that higher levels of toxicity prevalent in Twitter conversations often spread negativity and delay addressing issues. Further, the embedded moral values and speech acts specifying the intention of the tweet correlate... | Apoorva Upadhyaya, Marco Fisichella, Wolfgang Nejdl |  |
| 429 |  |  [Reasoning about Ambiguous Definite Descriptions](https://doi.org/10.18653/v1/2023.findings-emnlp.296) |  | 0 | Natural language reasoning plays an increasingly important role in improving language models’ ability to solve complex language understanding tasks. An interesting use case for reasoning is the resolution of context-dependent ambiguity. But no resources exist to evaluate how well Large Language Models can use explicit reasoning to resolve ambiguity in language. We propose to use ambiguous definite descriptions for this purpose and create and publish the first benchmark dataset consisting of... | Stefan F. Schouten, Peter Bloem, Ilia Markov, Piek Vossen |  |
| 430 |  |  [A Framework for Bidirectional Decoding: Case Study in Morphological Inflection](https://doi.org/10.18653/v1/2023.findings-emnlp.297) |  | 0 | Transformer-based encoder-decoder models that generate outputs in a left-to-right fashion have become standard for sequence-to-sequence tasks. In this paper, we propose a framework for decoding that produces sequences from the “outside-in”: at each step, the model chooses to generate a token on the left, on the right, or join the left and right sequences. We argue that this is more principled than prior bidirectional decoders. Our proposal supports a variety of model architectures and includes... | Marc E. Canby, Julia Hockenmaier |  |
| 431 |  |  [Text-guided 3D Human Generation from 2D Collections](https://doi.org/10.18653/v1/2023.findings-emnlp.298) |  | 0 | 3D human modeling has been widely used for engaging interaction in gaming, film, and animation. The customization of these characters is crucial for creativity and scalability, which highlights the importance of controllability. In this work, we introduce Text-guided 3D Human Generation (T3H), where a model is to generate a 3D human, guided by the fashion description. There are two goals: 1) the 3D human should render articulately, and 2) its outfit is controlled by the given text. To address... | TsuJui Fu, Wenhan Xiong, Yixin Nie, Jingyu Liu, Barlas Oguz, William Wang |  |
| 432 |  |  [Statistically Profiling Biases in Natural Language Reasoning Datasets and Models](https://doi.org/10.18653/v1/2023.findings-emnlp.299) |  | 0 | Recent studies have shown that many natural language understanding and reasoning datasets contain statistical cues that can be exploited by NLP models, resulting in an overestimation of their capabilities. Existing methods, such as “hypothesis-only” tests and CheckList, are limited in identifying these cues and evaluating model weaknesses. We introduce ICQ (I-See-Cue), a lightweight, general statistical profiling framework that automatically identifies potential biases in multiple-choice NLU... | Shanshan Huang, Kenny Q. Zhu |  |
| 433 |  |  [Verb Conjugation in Transformers Is Determined by Linear Encodings of Subject Number](https://doi.org/10.18653/v1/2023.findings-emnlp.300) |  | 0 | Deep architectures such as Transformers are sometimes criticized for having uninterpretable “black-box” representations. We use causal intervention analysis to show that, in fact, some linguistic features are represented in a linear, interpretable format. Specifically, we show that BERT’s ability to conjugate verbs relies on a linear encoding of subject number that can be manipulated with predictable effects on conjugation accuracy. This encoding is found in the subject position at the first... | Sophie Hao, Tal Linzen |  |
| 434 |  |  [MUX-PLMs: Data Multiplexing for High-throughput Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.301) |  | 0 | The widespread adoption of large language models such as ChatGPT and Bard has led to unprecedented demand for these technologies. The burgeoning cost of inference for ever-increasing model sizes coupled with hardware shortages has limited affordable access and poses a pressing need for efficiency approaches geared towards high throughput and performance. Multi-input multi-output (MIMO) algorithms such as data multiplexing, offer a promising solution with a many-fold increase in throughput by... | Vishvak Murahari, Ameet Deshpande, Carlos E. Jimenez, Izhak Shafran, Mingqiu Wang, Yuan Cao, Karthik Narasimhan |  |
| 435 |  |  [That was the last straw, we need more: Are Translation Systems Sensitive to Disambiguating Context?](https://doi.org/10.18653/v1/2023.findings-emnlp.302) |  | 0 | The translation of ambiguous text presents a challenge for translation systems, as it requires using the surrounding context to disambiguate the intended meaning as much as possible. While prior work has studied ambiguities that result from different grammatical features of the source and target language, we study semantic ambiguities that exist in the source (English in this work) itself. In particular, we focus on idioms that are open to both literal and figurative interpretations (e.g.,... | Jaechan Lee, Alisa Liu, Orevaoghene Ahia, Hila Gonen, Noah A. Smith |  |
| 436 |  |  [MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic](https://doi.org/10.18653/v1/2023.findings-emnlp.303) |  | 0 | Theory of Mind (ToM) is a critical component of intelligence but its assessment remains the subject of heated debates. Prior research applied human ToM assessments to natural language processing models using either human-created standardized tests or rule-based templates. However, these methods primarily focus on simplistic reasoning and require further validation. Here, we leverage dynamic epistemic logic to isolate a particular component of ToM and to generate controlled problems. We also... | Damien Sileo, Antoine Lernould |  |
| 437 |  |  [LATENTLOGIC: Learning Logic Rules in Latent Space over Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.304) |  | 0 | Learning logic rules for knowledge graph reasoning is essential as such rules provide interpretable explanations for reasoning and can be generalized to different domains. However, existing methods often face challenges such as searching in a vast search space (e.g., enumeration of relational paths or multiplication of high-dimensional matrices) and inefficient optimization (e.g., techniques based on reinforcement learning or EM algorithm). To address these limitations, this paper proposes a... | Junnan Liu, Qianren Mao, Chenghua Lin, Yangqiu Song, Jianxin Li |  |
| 438 |  |  [RobustEmbed: Robust Sentence Embeddings Using Self-Supervised Contrastive Pre-Training](https://doi.org/10.18653/v1/2023.findings-emnlp.305) |  | 0 | Pre-trained language models (PLMs) have demonstrated their exceptional performance across a wide range of natural language processing tasks. The utilization of PLM-based sentence embeddings enables the generation of contextual representations that capture rich semantic information. However, despite their success with unseen samples, current PLM-based representations suffer from poor robustness in adversarial scenarios. In this paper, we propose RobustEmbed, a self-supervised sentence embedding... | Javad Rafiei Asl, Eduardo Blanco, Daniel Takabi |  |
| 439 |  |  [More than Votes? Voting and Language based Partisanship in the US Supreme Court](https://doi.org/10.18653/v1/2023.findings-emnlp.306) |  | 0 | Understanding the prevalence and dynamics of justice partisanship and ideology in the US Supreme Court is critical in studying jurisdiction. Most research quantifies partisanship based on voting behavior, and oral arguments in the courtroom — the last essential procedure before the final case outcome — have not been well studied for this purpose. To address this gap, we present a framework for analyzing the language of justices in the courtroom for partisan signals, and study how partisanship... | Biaoyan Fang, Trevor Cohn, Timothy Baldwin, Lea Frermann |  |
| 440 |  |  [Automatic Evaluation of Attribution by Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.307) |  | 0 | A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate automatic evaluation of attribution given by LLMs. We begin... | Xiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, Huan Sun |  |
| 441 |  |  [Modeling Highlighting of Metaphors in Multitask Contrastive Learning Paradigms](https://doi.org/10.18653/v1/2023.findings-emnlp.308) |  | 0 | Metaphorical language, such as “spending time together”, projects meaning from a source domain (here, money) to a target domain (time). Thereby, it highlights certain aspects of the target domain, such as the effort behind the time investment. Highlighting aspects with metaphors (while hiding others) bridges the two domains and is the core of metaphorical meaning construction. For metaphor interpretation, linguistic theories stress that identifying the highlighted aspects is important for a... | Meghdut Sengupta, Milad Alshomary, Ingrid Scharlau, Henning Wachsmuth |  |
| 442 |  |  [LDM²: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement](https://doi.org/10.18653/v1/2023.findings-emnlp.309) |  | 0 | With the rapid development of large language models (LLMs), it is highly demanded that LLMs can be adopted to make decisions to enable the artificial general intelligence. Most approaches leverage manually crafted examples to prompt the LLMs to imitate the decision process of human. However, designing optimal prompts is difficult and the patterned prompts can hardly be generalized to more complex environments. In this paper, we propose a novel model named Large Decision Model with Memory... | Xingjin Wang, Linjing Li, Daniel Dajun Zeng |  |
| 443 |  |  [ZARA: Improving Few-Shot Self-Rationalization for Small Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.310) |  | 0 | Language models (LMs) that jointly generate end-task answers as well as free-text rationales are known as self-rationalization models. Recent works demonstrate great performance gain for self-rationalization by few-shot prompting LMs with rationale-augmented exemplars. However, the ability to benefit from explanations only emerges with large-scale LMs, which have poor accessibility. In this work, we explore the less-studied setting of leveraging explanations for small LMs to improve few-shot... | WeiLin Chen, AnZi Yen, ChengKuang Wu, HenHsen Huang, HsinHsi Chen |  |
| 444 |  |  [ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation](https://doi.org/10.18653/v1/2023.findings-emnlp.311) |  | 0 | Despite remarkable advances that large language models have achieved in chatbots nowadays, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media contents, leaving the unique challenges inherent to real-world user-AI interactions insufficiently explored. In this work, we introduce ToxicChat, a novel benchmark constructed based on real user... | Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, Jingbo Shang |  |
| 445 |  |  [Mind the Gap: Automated Corpus Creation for Enthymeme Detection and Reconstruction in Learner Arguments](https://doi.org/10.18653/v1/2023.findings-emnlp.312) |  | 0 | Writing strong arguments can be challenging for learners. It requires to select and arrange multiple argumentative discourse units (ADUs) in a logical and coherent way as well as to decide which ADUs to leave implicit, so called enthymemes. However, when important ADUs are missing, readers might not be able to follow the reasoning or understand the argument’s main point. This paper introduces two new tasks for learner arguments: to identify gaps in arguments (enthymeme detection) and to fill... | Maja Stahl, Nick Düsterhus, MeiHua Chen, Henning Wachsmuth |  |
| 446 |  |  [Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.313) |  | 0 | Current variational dialog models have employed pre-trained language models (PLMs) to parameterize the likelihood and posterior distributions. However, the Gaussian assumption made on the prior distribution is incompatible with these distributions, thus restricting the diversity of generated responses. These models also suffer from posterior collapse, i.e., the decoder tends to ignore latent variables and directly access information captured in the encoder through the cross-attention mechanism.... | Tianyu Yang, Thy Thy Tran, Iryna Gurevych |  |
| 447 |  |  [Retrieving Multimodal Information for Augmented Generation: A Survey](https://doi.org/10.18653/v1/2023.findings-emnlp.314) |  | 0 | As Large Language Models (LLMs) become popular, there emerged an important trend of using multimodality to augment the LLMs’ generation ability, which enables LLMs to better interact with the world. However, there lacks a unified perception of at which stage and how to incorporate different modalities. In this survey, we review methods that assist and augment generative models by retrieving multimodal knowledge, whose formats range from images, codes, tables, graphs, to audio. Such methods... | Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Do Xuan Long, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, Shafiq Joty |  |
| 448 |  |  [Improving Contrastive Learning of Sentence Embeddings with Focal InfoNCE](https://doi.org/10.18653/v1/2023.findings-emnlp.315) |  | 0 | The recent success of SimCSE has greatly advanced state-of-the-art sentence representations. However, the original formulation of SimCSE does not fully exploit the potential of hard negative samples in contrastive learning. This study introduces an unsupervised contrastive learning framework that combines SimCSE with hard negative mining, aiming to enhance the quality of sentence embeddings. The proposed focal-InfoNCE function introduces self-paced modulation terms in the contrastive objective,... | Pengyue Hou, Xingyu Li |  |
| 449 |  |  [The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.316) |  | 0 | We present The Vault, an open-source dataset of high quality code-text pairs in multiple programming languages for training large language models to understand and generate code. We propose methods for thoroughly extracting samples that use both rules and deep learning to ensure that they contain high-quality pairs of code and text, resulting in a dataset of 43 million high-quality code-text pairs. We thoroughly evaluated this dataset and discovered that when used to train common code language... | Dung Nguyen Manh, Le Nam Hai, Anh T. V. Dau, Anh Minh Nguyen, Khanh Nghiem, Jin Guo, Nghi D. Q. Bui |  |
| 450 |  |  [SDOH-NLI: a Dataset for Inferring Social Determinants of Health from Clinical Notes](https://doi.org/10.18653/v1/2023.findings-emnlp.317) |  | 0 | Social and behavioral determinants of health (SDOH) play a significant role in shaping health outcomes, and extracting these determinants from clinical notes is a first step to help healthcare providers systematically identify opportunities to provide appropriate care and address disparities. Progress on using NLP methods for this task has been hindered by the lack of high-quality publicly available labeled data, largely due to the privacy and regulatory constraints on the use of real patients’... | Ádám D. Lelkes, Eric Loreaux, Tal Schuster, MingJun Chen, Alvin Rajkomar |  |
| 451 |  |  [On the Zero-Shot Generalization of Machine-Generated Text Detectors](https://doi.org/10.18653/v1/2023.findings-emnlp.318) |  | 0 | The rampant proliferation of large language models, fluent enough to generate text indistinguishable from human-written language, gives unprecedented importance to the detection of machine-generated text. This work is motivated by an important research question: How will the detectors of machine-generated text perform on outputs of a new generator, that the detectors were not trained on? We begin by collecting generation data from a wide range of LLMs, and train neural detectors on data from... | Xiao Pu, Jingyu Zhang, Xiaochuang Han, Yulia Tsvetkov, Tianxing He |  |
| 452 |  |  [Complex Event Schema Induction with Knowledge-Enriched Diffusion Model](https://doi.org/10.18653/v1/2023.findings-emnlp.319) |  | 0 | The concept of a complex event schema pertains to the graph structure that represents real-world knowledge of events and their multi-dimensional relationships. However, previous studies on event schema induction have been hindered by challenges such as error propagation and data quality issues. To tackle these challenges, we propose a knowledge-enriched discrete diffusion model. Specifically, we distill the abundant event scenario knowledge of Large Language Models (LLMs) through an... | Yupu Hao, Pengfei Cao, Yubo Chen, Kang Liu, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Jun Zhao |  |
| 453 |  |  [Exploiting Emotion-Semantic Correlations for Empathetic Response Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.320) |  | 0 | Empathetic response generation aims to generate empathetic responses by understanding the speaker’s emotional feelings from the language of dialogue. Recent methods capture emotional words in the language of communicators and construct them as static vectors to perceive nuanced emotions. However, linguistic research has shown that emotional words in language are dynamic and have correlations with other grammar semantic roles, i.e., words with semantic meanings, in grammar. Previous methods... | Zhou Yang, Zhaochun Ren, Yufeng Wang, Xiaofei Zhu, Zhihao Chen, Tiecheng Cai, Yunbing Wu, Yisong Su, Sibo Ju, Xiangwen Liao |  |
| 454 |  |  [Long-Range Language Modeling with Selective Cache](https://doi.org/10.18653/v1/2023.findings-emnlp.321) |  | 0 | The computational cost of transformer-based language models grows quadratically with the sequence length. In this paper, we introduce the selective cache, which stores the selected key-value pairs from the previous context. By selecting important key-value pairs the model makes better use of the cache so that in limited cache size, a longer context history can be stored. We design three kinds of selection methods. The first is based on human language processing. The key-value pairs are selected... | Xinting Huang, Nora Hollenstein |  |
| 455 |  |  [Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding](https://doi.org/10.18653/v1/2023.findings-emnlp.322) |  | 0 | Text simplification has emerged as an increasingly useful application of AI for bridging the communication gap in specialized fields such as medicine, where the lexicon is often dominated by technical jargon and complex constructs. Despite notable progress, methods in medical simplification sometimes result in the generated text having lower quality and diversity. In this work, we explore ways to further improve the readability of text simplification in the medical domain. We propose (1) a new... | Lorenzo Jaime Yu Flores, Heyuan Huang, Kejian Shi, Sophie Chheang, Arman Cohan |  |
| 456 |  |  [FaLA: Fast Linear Adaptation for Replacing Backbone Models on Edge Devices](https://doi.org/10.18653/v1/2023.findings-emnlp.323) |  | 0 | In this work, we study the language model backbone replacement problem for personalized downstream tasks in a non-stationary on-device scenario. In real world, company may periodically update the knowledge and architectures of backbones to keep the competitive in the market, meanwhile, to accommodate the users’ own preference, models are personalized to fit users’ own distribution locally. Traditional full model tuning or transfer learning for such replacements often incur considerable local... | Shuo Huang, Lizhen Qu, Xingliang Yuan, Chunyang Chen |  |
| 457 |  |  [Intuitive Multilingual Audio-Visual Speech Recognition with a Single-Trained Model](https://doi.org/10.18653/v1/2023.findings-emnlp.324) |  | 0 | We present a novel approach to multilingual audio-visual speech recognition tasks by introducing a single model on a multilingual dataset. Motivated by a human cognitive system where humans can intuitively distinguish different languages without any conscious effort or guidance, we propose a model that can capture which language is given as an input speech by distinguishing the inherent similarities and differences between languages. To do so, we design a prompt fine-tuning technique into the... | Joanna Hong, Se Jin Park, Yong Man Ro |  |
| 458 |  |  [Controllable Chest X-Ray Report Generation from Longitudinal Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.325) |  | 0 | Radiology reports are detailed text descriptions of the content of medical scans. Each report describes the presence/absence and location of relevant clinical findings, commonly including comparison with prior exams of the same patient to describe how they evolved. Radiology reporting is a time-consuming process, and scan results are often subject to delays. One strategy to speed up reporting is to integrate automated reporting systems, however clinical deployment requires high accuracy and... | Francesco Dalla Serra, Chaoyang Wang, Fani Deligianni, Jeff Dalton, Alison O'Neil |  |
| 459 |  |  [Is ChatGPT a Good Multi-Party Conversation Solver?](https://doi.org/10.18653/v1/2023.findings-emnlp.326) |  | 0 | Large Language Models (LLMs) have emerged as influential instruments within the realm of natural language processing; nevertheless, their capacity to handle multi-party conversations (MPCs) – a scenario marked by the presence of multiple interlocutors involved in intricate information exchanges – remains uncharted. In this paper, we delve into the potential of generative LLMs such as ChatGPT and GPT-4 within the context of MPCs. An empirical analysis is conducted to assess the zero-shot... | ChaoHong Tan, JiaChen Gu, ZhenHua Ling |  |
| 460 |  |  [Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis](https://doi.org/10.18653/v1/2023.findings-emnlp.327) |  | 0 | Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent... | Jianqiao Lu, Wenyong Huang, Nianzu Zheng, Xingshan Zeng, Yu Ting Yeung, Xiao Chen |  |
| 461 |  |  [Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders](https://doi.org/10.18653/v1/2023.findings-emnlp.328) |  | 0 | Pre-trained sentence representations are crucial for identifying significant sentences in unsupervised document extractive summarization. However, the traditional two-step paradigm of pre-training and sentence-ranking, creates a gap due to differing optimization objectives. To address this issue, we argue that utilizing pre-trained embeddings derived from a process specifically designed to optimize informative and distinctive sentence representations helps rank significant sentences. To do so,... | Qianren Mao, Shaobo Zhao, Jiarui Li, Xiaolei Gu, Shizhu He, Bo Li, Jianxin Li |  |
| 462 |  |  [Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.329) |  | 0 | Prompt tuning, in which prompts are optimized to adapt large-scale pre-trained language models to downstream tasks instead of fine-tuning the full model parameters, has been shown to be particularly effective when the prompts are trained in the multi-task transfer learning setting. These methods generally involve individually training prompts for each source task and then aggregating them to provide the initialization of the prompt for the target task. However, this approach critically ignores... | Haeju Lee, Minchan Jeong, SeYoung Yun, KeeEung Kim |  |
| 463 |  |  [CCIM: Cross-modal Cross-lingual Interactive Image Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.330) |  | 0 | Text image machine translation (TIMT) which translates source language text images into target language texts has attracted intensive attention in recent years. Although the end-to-end TIMT model directly generates target translation from encoded text image features with an efficient architecture, it lacks the recognized source language information resulting in a decrease in translation performance. In this paper, we propose a novel Cross-modal Cross-lingual Interactive Model (CCIM) to... | Cong Ma, Yaping Zhang, Mei Tu, Yang Zhao, Yu Zhou, Chengqing Zong |  |
| 464 |  |  [TRAMS: Training-free Memory Selection for Long-range Language Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.331) |  | 0 | The Transformer architecture is crucial for numerous AI models, but it still faces challenges in long-range language modeling. Though several specific transformer architectures have been designed to tackle issues of long-range dependencies, existing methods like Transformer-XL are plagued by a high percentage of ineffective memories. In this study, we present a plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation... | Haofei Yu, Cunxiang Wang, Yue Zhang, Wei Bi |  |
| 465 |  |  [A Critical Analysis of Document Out-of-Distribution Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.332) |  | 0 | Large-scale pre-training is widely used in recent document understanding tasks. During deployment, one may expect that models should trigger a conservative fallback policy when encountering out-of-distribution (OOD) samples, which highlights the importance of OOD detection. However, most existing OOD detection methods focus on single-modal inputs such as images or texts. While documents are multi-modal in nature, it is underexplored if and how multi-modal information in documents can be... | Jiuxiang Gu, Yifei Ming, Yi Zhou, Jason Kuen, Vlad I. Morariu, Handong Zhao, Ruiyi Zhang, Nikolaos Barmpalios, Anqi Liu, Yixuan Li, Tong Sun, Ani Nenkova |  |
| 466 |  |  [Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.333) |  | 0 | Improving neural machine translation (NMT) systems with prompting has achieved significant progress in recent years. In this work, we focus on how to integrate multi-knowledge, multiple types of knowledge, into NMT models to enhance the performance with prompting. We propose a unified framework, which can integrate effectively multiple types of knowledge including sentences, terminologies/phrases and translation templates into NMT models. We utilize multiple types of knowledge as prefix-prompts... | Ke Wang, Jun Xie, Yuqi Zhang, Yu Zhao |  |
| 467 |  |  [Active Learning Principles for In-Context Learning with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.334) |  | 0 | The remarkable advancements in large language models (LLMs) have significantly enhanced predictive performance in few-shot learning settings. By using only a small number of labeled examples, referred to as demonstrations, LLMs can effectively perform the task at hand through in-context learning. However, the process of selecting demonstrations for maximizing performance has received limited attention in prior work. This paper addresses the issue of identifying the most informative... | Katerina Margatina, Timo Schick, Nikolaos Aletras, Jane DwivediYu |  |
| 468 |  |  [InteMATs: Integrating Granularity-Specific Multilingual Adapters for Cross-Lingual Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.335) |  | 0 | Multilingual language models (MLLMs) have achieved remarkable success in various cross-lingual transfer tasks. However, they suffer poor performance in zero-shot low-resource languages, particularly when dealing with longer contexts. Existing research mainly relies on full-model fine-tuning on large parallel datasets to enhance the cross-lingual alignment of MLLMs, which is computationally expensive. In this paper, we propose InteMATs, a novel approach that integrates multilingual adapters... | Meizhen Liu, Xu Guo, Jiakai He, Jianye Chen, Fengyu Zhou, Siu Cheung Hui |  |
| 469 |  |  [PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.336) |  | 0 | The patient-centered medical dialogue systems strive to offer diagnostic interpretation services to users who are less knowledgeable about medical knowledge, through emphasizing the importance of providing responses specific to the patients. It is difficult for the large language models (LLMs) to guarantee the specificity of responses in spite of its promising performance even in some tasks in medical field. Inspired by in-context learning, we propose PlugMed, a Plug-and-Play Medical Dialogue... | Chengfeng Dou, Zhi Jin, Wenpin Jiao, Haiyan Zhao, Yongqiang Zhao, Zhengwei Tao |  |
| 470 |  |  [CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.337) |  | 0 | Recent code translation techniques exploit neural machine translation models to translate source code from one programming language to another to satisfy production compatibility or to improve efficiency of codebase maintenance. Most existing code translation datasets only focus on a single pair of popular programming languages. To advance research on code translation and meet diverse requirements of real-world applications, we construct \*\*CodeTransOcean\*\*, a large-scale comprehensive... | Weixiang Yan, Yuchen Tian, Yunzhe Li, Qian Chen, Wen Wang |  |
| 471 |  |  [impact of sample selection on in-context learning for entity extraction from scientific writing](https://doi.org/10.18653/v1/2023.findings-emnlp.338) |  | 0 | Prompt-based usage of Large Language Models (LLMs) is an increasingly popular way to tackle many well-known natural language problems. This trend is due, in part, to the appeal of the In-Context Learning (ICL) prompt set-up, in which a few selected training examples are provided along with the inference request. ICL, a type of few-shot learning, is especially attractive for natural language processing (NLP) tasks defined for specialised domains, such as entity extraction from scientific... | Necva Bölücü, Maciej Rybinski, Stephen Wan |  |
| 472 |  |  [Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models](https://doi.org/10.18653/v1/2023.findings-emnlp.339) |  | 0 | Considerable effort has been dedicated to mitigating toxicity, but existing methods often require drastic modifications to model parameters or the use of computationally intensive auxiliary models. Furthermore, previous approaches have often neglected the crucial factor of language’s evolving nature over time. In this work, we present a comprehensive perspective on toxicity mitigation that takes into account its changing nature. We introduce Goodtriever, a flexible methodology that matches the... | Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker |  |
| 473 |  |  [Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks](https://doi.org/10.18653/v1/2023.findings-emnlp.340) |  | 0 | We investigate MT evaluation metric performance on adversarially-synthesized texts, to shed light on metric robustness. We experiment with word- and character-level attacks on three popular machine translation metrics: BERTScore, BLEURT, and COMET. Our human experiments validate that automatic metrics tend to overpenalize adversarially-degraded translations. We also identify inconsistencies in BERTScore ratings, where it judges the original sentence and the adversarially-degraded one as... | Yichen Huang, Timothy Baldwin |  |
| 474 |  |  [Time-Considerable Dialogue Models via Reranking by Time Dependency](https://doi.org/10.18653/v1/2023.findings-emnlp.341) |  | 0 | In the last few years, generative dialogue models have shown excellent performance and have been used for various applications. As chatbots become more prevalent in our daily lives, more and more people expect them to behave more like humans, but existing dialogue models do not consider the time information that people are constantly aware of. In this paper, we aim to construct a time-considerable dialogue model that actively utilizes time information. First, we categorize responses by their... | Yuiko Tsunomori, Masakazu Ishihata, Hiroaki Sugiyama |  |
| 475 |  |  [Non-Compositionality in Sentiment: New Data and Analyses](https://doi.org/10.18653/v1/2023.findings-emnlp.342) |  | 0 | When natural language phrases are combined, their meaning is often more than the sum of their parts. In the context of NLP tasks such as sentiment analysis, where the meaning of a phrase is its sentiment, that still applies. Many NLP studies on sentiment analysis, however, focus on the fact that sentiment computations are largely compositional. We, instead, set out to obtain non-compositionality ratings for phrases with respect to their sentiment. Our contributions are as follows: a) a... | Verna Dankers, Christopher Lucas |  |
| 476 |  |  [MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.343) |  | 0 | The large language models have achieved superior performance on various natural language tasks. One major drawback of such approaches is they are resource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a resource-efficient solution to fine-tune the pre-trained language models (PLMs) while keeping their weight frozen. Existing soft prompt methods mainly focus on designing the input-independent prompts that steer the model to fit the domain of the new dataset. Those methods... | Guoxin Chen, Yiming Qian, Bowen Wang, Liangzhi Li |  |
| 477 |  |  [DocTrack: A Visually-Rich Document Dataset Really Aligned with Human Eye Movement for Machine Reading](https://doi.org/10.18653/v1/2023.findings-emnlp.344) |  | 0 | The use of visually-rich documents in various fields has created a demand for Document AI models that can read and comprehend documents like humans, which requires the overcoming of technical, linguistic, and cognitive barriers. Unfortunately, the lack of appropriate datasets has significantly hindered advancements in the field. To address this issue, we introduce DocTrack, a visually-rich document dataset really aligned with human eye-movement information using eye-tracking technology. This... | Hao Wang, Qingxuan Wang, Yue Li, Changqing Wang, Chenhui Chu, Rui Wang |  |
| 478 |  |  [Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.345) |  | 0 | Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. \*Selective prediction\* is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation... | Jiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan Ö. Arik, Tomas Pfister, Somesh Jha |  |
| 479 |  |  [Bi-Drop: Enhancing Fine-tuning Generalization via Synchronous sub-net Estimation and Optimization](https://doi.org/10.18653/v1/2023.findings-emnlp.346) |  | 0 | Pretrained language models have achieved remarkable success in natural language understanding. However, fine-tuning pretrained models on limited training data tends to overfit and thus diminish performance. This paper presents Bi-Drop, a fine-tuning strategy that selectively updates model parameters using gradients from various sub-nets dynamically generated by dropout. The sub-net estimation of Bi-Drop is performed in an in-batch manner, so it overcomes the problem of hysteresis in sub-net... | Shoujie Tong, Heming Xia, Damai Dai, Runxin Xu, Tianyu Liu, Binghuai Lin, Yunbo Cao, Zhifang Sui |  |
| 480 |  |  [ClozEx: A Task toward Generation of English Cloze Explanation](https://doi.org/10.18653/v1/2023.findings-emnlp.347) |  | 0 | Providing explanations for cloze questions in language assessment (LA) has been recognized as a valuable approach to enhancing the language proficiency of learners. However, there is a noticeable absence of dedicated tasks and datasets specifically designed for generating language learner explanations. In response to this gap, this paper introduces a novel task ClozEx of generating explanations for cloze questions in LA, with a particular focus on English as a Second Language (ESL) learners. To... | Zizheng Zhang, Masato Mita, Mamoru Komachi |  |
| 481 |  |  [Is Probing All You Need? Indicator Tasks as an Alternative to Probing Embedding Spaces](https://doi.org/10.18653/v1/2023.findings-emnlp.348) |  | 0 | The ability to identify and control different kinds of linguistic information encoded in vector representations of words has many use cases, especially for explainability and bias removal. This is usually done via a set of simple classification tasks, termed probes, to evaluate the information encoded in the embedding space. However, the involvement of a trainable classifier leads to entanglement between the probe’s results and the classifier’s nature. As a result, contemporary works on probing... | Tal Levy, Omer Goldman, Reut Tsarfaty |  |
| 482 |  |  [The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.349) |  | 0 | Compressing large language models (LLMs), often consisting of billions of parameters, provides faster inference, smaller memory footprints, and enables local deployment. The standard compression techniques are pruning and quantization, with the former eliminating redundant connections in model layers and the latter representing model parameters with as little as 4 bits. The key tradeoff is between the degree of compression and the impact on the quality of the compressed model. Existing research... | Satya Sai Srinath Namburi, Makesh Sreedhar, Srinath Srinivasan, Frederic Sala |  |
| 483 |  |  [CoEdIT: Text Editing by Task-Specific Instruction Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.350) |  | 0 | We introduce CoEdIT, a state-of-the-art text editing system for writing assistance. CoEdIT takes instructions from the user specifying the attributes of the desired text, such as “Make the sentence simpler” or “Write it in a more neutral style,” and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing... | Vipul Raheja, Dhruv Kumar, Ryan Koo, Dongyeop Kang |  |
| 484 |  |  [Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.351) |  | 0 | Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes. Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class. Indiscriminately using such knowledge causes catastrophic damage to... | Yi Dai, Hao Lang, Kaisheng Zeng, Fei Huang, Yongbin Li |  |
| 485 |  |  [Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information](https://doi.org/10.18653/v1/2023.findings-emnlp.352) |  | 0 | Real-world Knowledge Graphs (KGs) often suffer from incompleteness, which limits their potential performance. Knowledge Graph Completion (KGC) techniques aim to address this issue. However, traditional KGC methods are computationally intensive and impractical for large-scale KGs, necessitating the learning of dense node embeddings and computing pairwise distances. Generative transformer-based language models (e.g., T5 and recent KGT5) offer a promising solution as they can predict the tail... | Alla Chepurova, Aydar Bulatov, Yuri Kuratov, Mikhail Burtsev |  |
| 486 |  |  [DeltaScore: Fine-Grained Story Evaluation with Perturbations](https://doi.org/10.18653/v1/2023.findings-emnlp.353) |  | 0 | Numerous evaluation metrics have been developed for natural language generation tasks, but their effectiveness in evaluating stories is limited as they are not specifically tailored to assess intricate aspects of storytelling, such as fluency and interestingness. In this paper, we introduce DeltaScore, a novel methodology that uses perturbation techniques for the evaluation of nuanced story aspects. We posit that the extent to which a story excels in a specific aspect (e.g., fluency) correlates... | Zhuohan Xie, Miao Li, Trevor Cohn, Jey Han Lau |  |
| 487 |  |  [MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields](https://doi.org/10.18653/v1/2023.findings-emnlp.354) |  | 0 | Previous research has demonstrated the advantages of integrating data from multiple sources over traditional unimodal data, leading to the emergence of numerous novel multimodal applications. We propose a multimodal classification benchmark MuG with eight datasets that allows researchers to evaluate and improve their models. These datasets are collected from four various genres of games that cover tabular, textual, and visual modalities. We conduct multi-aspect data analysis to provide insights... | Jiaying Lu, Yongchen Qian, Shifan Zhao, Yuanzhe Xi, Carl Yang |  |
| 488 |  |  [Don't waste a single annotation: improving single-label classifiers through soft labels](https://doi.org/10.18653/v1/2023.findings-emnlp.355) |  | 0 | In this paper, we address the limitations of the common data annotation and training methods for objective single-label classification tasks. Typically, when annotating such tasks annotators are only asked to provide a single label for each sample and annotator disagreement is discarded when a final hard label is decided through majority voting. We challenge this traditional approach, acknowledging that determining the appropriate label can be difficult due to the ambiguity and lack of context... | Ben Wu, Yue Li, Yida Mu, Carolina Scarton, Kalina Bontcheva, Xingyi Song |  |
| 489 |  |  [Black-Box Tuning of Vision-Language Models with Effective Gradient Approximation](https://doi.org/10.18653/v1/2023.findings-emnlp.356) |  | 0 | Parameter-efficient fine-tuning (PEFT) methods have provided an effective way for adapting large vision-language models to specific tasks or scenarios. Typically, they learn a very small scale of parameters for pre-trained models in a white-box formulation, which assumes model architectures to be known and parameters to be accessible. However, large models are often not open-source due to considerations of preventing abuse or commercial factors, hence posing a barrier to the deployment of... | Zixian Guo, Yuxiang Wei, Ming Liu, Zhilong Ji, Jinfeng Bai, Yiwen Guo, Wangmeng Zuo |  |
| 490 |  |  [How to Determine the Most Powerful Pre-trained Language Model without Brute Force Fine-tuning? An Empirical Survey](https://doi.org/10.18653/v1/2023.findings-emnlp.357) |  | 0 | Transferability estimation has been attached to great attention in the computer vision fields. Researchers try to estimate with low computational cost the performance of a model when transferred from a source task to a given target task. Considering the effectiveness of such estimations, the communities of natural language processing also began to study similar problems for the selection of pre-trained language models. However, there is a lack of a comprehensive comparison between these... | Jun Bai, Xiaofeng Zhang, Chen Li, Hanhua Hong, Xi Xu, Chenghua Lin, Wenge Rong |  |
| 491 |  |  [Licon: A Diverse, Controllable and Challenging Linguistic Concept Learning Benchmark](https://doi.org/10.18653/v1/2023.findings-emnlp.358) |  | 0 | Concept Learning requires learning the definition of a general category from given training examples. Most of the existing methods focus on learning concepts from images. However, the visual information cannot present abstract concepts exactly, which struggles the introduction of novel concepts related to known concepts (e.g., ‘Plant’→‘Asteroids’). In this paper, inspired by the fact that humans learn most concepts through linguistic description, we introduce Linguistic Concept Learning... | Shenglong Yu, Ying Zhang, Wenya Guo, Zhengkun Zhang, Ru Zhou, Xiaojie Yuan |  |
| 492 |  |  [InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations](https://doi.org/10.18653/v1/2023.findings-emnlp.359) |  | 0 | While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TalkToModel (Slack et al., 2022)... | Nils Feldhus, Qianli Wang, Tatiana Anikina, Sahil Chopra, Cennet Oguz, Sebastian Möller |  |
| 493 |  |  [INVITE: a Testbed of Automatically Generated Invalid Questions to Evaluate Large Language Models for Hallucinations](https://doi.org/10.18653/v1/2023.findings-emnlp.360) |  | 0 | Recent advancements in Large language models (LLMs) have enabled them to hold free form conversations over multiple turns, but they exhibit a tendency to make unfounded and incorrect statements, commonly known as hallucinations. In particular, LLMs hallucinate frequently when given invalid questions, i.e. ones with incorrect assumptions. The most common approach to evaluate LLMs on hallucinations is to test them on Question Answering (QA) test sets such as TruthfulQA. However, LLMs are... | Anil Ramakrishna, Rahul Gupta, Jens Lehmann, Morteza Ziyadi |  |
| 494 |  |  [Multimodal Automated Fact-Checking: A Survey](https://doi.org/10.18653/v1/2023.findings-emnlp.361) |  | 0 | Misinformation is often conveyed in multiple modalities, e.g. a miscaptioned image. Multimodal misinformation is perceived as more credible by humans, and spreads faster than its text-only counterparts. While an increasing body of research investigates automated fact-checking (AFC), previous surveys mostly focus on text. In this survey, we conceptualise a framework for AFC including subtasks unique to multimodal misinformation. Furthermore, we discuss related terms used in different communities... | Mubashara Akhtar, Michael Sejr Schlichtkrull, Zhijiang Guo, Oana Cocarascu, Elena Simperl, Andreas Vlachos |  |
| 495 |  |  [PROTEGE: Prompt-based Diverse Question Generation from Web Articles](https://doi.org/10.18653/v1/2023.findings-emnlp.362) |  | 0 | Rich and diverse knowledge bases (KB) are foundational building blocks for online knowledge sharing communities such as StackOverflow and Quora, and applications such as conversational assistants (aka chatbots). A popular format for knowledge bases is question-answer pairs (or FAQs), where questions are designed to accurately match a multitude of queries. In this paper, we address the problem of automatic creation of such Q&A-based knowledge bases from domain-specific, long-form textual content... | Vinayak Puranik, Anirban Majumder, Vineet Chaoji |  |
| 496 |  |  [GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions](https://doi.org/10.18653/v1/2023.findings-emnlp.363) |  | 0 | There is growing interest in systems that generate captions for scientific figures. However, assessing these systems’ output poses a significant challenge. Human evaluation requires academic expertise and is costly, while automatic evaluation depends on often low-quality author-written captions. This paper investigates using large language models (LLMs) as a cost-effective, reference-free method for evaluating figure captions. We first constructed SCICAP-EVAL, a human evaluation dataset that... | TingYao Hsu, ChiehYang Huang, Ryan A. Rossi, Sungchul Kim, C. Lee Giles, TingHao Kenneth Huang |  |
| 497 |  |  [Mulan: A Multi-Level Alignment Model for Video Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.364) |  | 0 | Video Question Answering (VideoQA) aims to answer questions about the visual content of a video. Current methods mainly focus on improving joint representations of video and text. However, these methods pay little attention to the fine-grained semantic interaction between video and text. In this paper, we propose Mulan: a Multi-Level Alignment Model for Video Question Answering, which establishes alignment between visual and textual modalities at the object-level, frame-level, and video-level.... | Yu Fu, Cong Cao, Yuling Yang, Yuhai Lu, Fangfang Yuan, Dakui Wang, Yanbing Liu |  |
| 498 |  |  [HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.365) |  | 0 | With the proliferation of social media, accurate detection of hate speech has become critical to ensure safety online. To combat nuanced forms of hate speech, it is important to identify and thoroughly explain hate speech to help users understand its harmful effects. Recent benchmarks have attempted to tackle this issue by training generative models on free-text annotations of implications in hateful text. However, we find significant reasoning gaps in the existing annotations schemes, which... | Yongjin Yang, Joonkee Kim, Yujin Kim, Namgyu Ho, James Thorne, SeYoung Yun |  |
| 499 |  |  [ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.366) |  | 0 | Predicting chemical reactions, a fundamental challenge in chemistry, involves forecasting the resulting products from a given reaction process. Conventional techniques, notably those employing Graph Neural Networks (GNNs), are often limited by insufficient training data and their inability to utilize textual information, undermining their applicability in real-world applications. In this work, we propose \*\*ReLM\*\*, a novel framework that leverages the chemical knowledge encoded in language... | Yaorui Shi, An Zhang, Enzhi Zhang, Zhiyuan Liu, Xiang Wang |  |
| 500 |  |  [Decomposing Complex Queries for Tip-of-the-tongue Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.367) |  | 0 | When re-finding items, users who forget or are uncertain about identifying details often rely on creative strategies for expressing their information needs—complex queries that describe content elements (e.g., book characters or events), information beyond the document text (e.g., descriptions of book covers), or personal context (e.g., when they read a book). Standard retrieval models that rely on lexical or semantic overlap between query and document text are challenged in such retrieval... | Kevin Lin, Kyle Lo, Joseph Gonzalez, Dan Klein |  |
| 501 |  |  [Values, Ethics, Morals? On the Use of Moral Concepts in NLP Research](https://doi.org/10.18653/v1/2023.findings-emnlp.368) |  | 0 | With language technology increasingly affecting individuals’ lives, many recent works have investigated the ethical aspects of NLP. Among other topics, researchers focused on the notion of morality, investigating, for example, which moral judgements language models make. However, there has been little to no discussion of the terminology and the theories underpinning those efforts and their implications. This lack is highly problematic, as it hides the works’ underlying assumptions and hinders a... | Karina Vida, Judith Simon, Anne Lauscher |  |
| 502 |  |  [Self-Supervised Behavior Cloned Transformers are Path Crawlers for Text Games](https://doi.org/10.18653/v1/2023.findings-emnlp.369) |  | 0 | In this work, we introduce a self-supervised behavior cloning transformer for text games, which are challenging benchmarks for multi-step reasoning in virtual environments. Traditionally, Behavior Cloning Transformers excel in such tasks but rely on supervised training data. Our approach auto-generates training data by exploring trajectories (defined by common macro-action sequences) that lead to reward within the games, while determining the generality and utility of these trajectories by... | Ruoyao Wang, Peter A. Jansen |  |
| 503 |  |  [Adapting Pretrained Text-to-Text Models for Long Text Sequences](https://doi.org/10.18653/v1/2023.findings-emnlp.370) |  | 0 | We present an empirical study of adapting an existing pretrained text-to-text model for long-sequence inputs. Through a comprehensive study along three axes of the pretraining pipeline – model architecture, optimization objective, and pretraining corpus, we propose an effective recipe to build long-context models from existing short-context models. Specifically, we replace the full attention in transformers with pooling-augmented blockwise attention, and pretrain the model with a masked-span... | Wenhan Xiong, Anchit Gupta, Shubham Toshniwal, Yashar Mehdad, Scott Yih |  |
| 504 |  |  [xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark](https://doi.org/10.18653/v1/2023.findings-emnlp.371) |  | 0 | Recent advancements in reference-free learned metrics for open-domain dialogue evaluation have been driven by the progress in pre-trained language models and the availability of dialogue data with high-quality human annotations. However, current studies predominantly concentrate on English dialogues, and the generalization of these metrics to other languages has not been fully examined. This is largely due to the absence of a multilingual dialogue evaluation benchmark. To address the issue, we... | Chen Zhang, Luis F. D'Haro, Chengguang Tang, Ke Shi, Guohua Tang, Haizhou Li |  |
| 505 |  |  [MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems](https://doi.org/10.18653/v1/2023.findings-emnlp.372) |  | 0 | While automatic dialogue tutors hold great potential in making education personalized and more accessible, research on such systems has been hampered by a lack of sufficiently large and high-quality datasets. Collecting such datasets remains challenging, as recording tutoring sessions raises privacy concerns and crowdsourcing leads to insufficient data quality. To address this, we propose a framework to generate such dialogues by pairing human teachers with a Large Language Model (LLM) prompted... | Jakub Macina, Nico Daheim, Sankalan Pal Chowdhury, Tanmay Sinha, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan |  |
| 506 |  |  [Towards Making the Most of ChatGPT for Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.373) |  | 0 | ChatGPT shows remarkable capabilities for machine translation (MT). Several prior studies have shown that it achieves comparable results to commercial systems for high-resource languages, but lags behind in complex tasks, e.g, low-resource and distant-language-pairs translation. However, they usually adopt simple prompts which can not fully elicit the capability of ChatGPT. In this report, we aim to further mine ChatGPT’s translation ability by revisiting several aspects: temperature, task... | Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang, Dacheng Tao |  |
| 507 |  |  [Enhancing Reasoning Capabilities by Instruction Learning and Chain-of-Thoughts for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.374) |  | 0 | The aim of implicit discourse relation recognition is to comprehend the sense of connection between two arguments. In this work, we present a classification method that is solely based on generative models. Our proposed approach employs a combination of instruction templates and in-context learning to refine the generative model for effectively addressing the implicit discourse relation recognition task. Furthermore, we utilize Chain-of-Thoughts to partition the inference process into a... | Yuxiang Lu, Yu Hong, Zhipang Wang, Guodong Zhou |  |
| 508 |  |  [Large-Scale and Multi-Perspective Opinion Summarization with Diverse Review Subsets](https://doi.org/10.18653/v1/2023.findings-emnlp.375) |  | 0 | Opinion summarization is expected to digest larger review sets and provide summaries from different perspectives. However, most existing solutions are deficient in epitomizing extensive reviews and offering opinion summaries from various angles due to the lack of designs for information selection. To this end, we propose SubSumm, a supervised summarization framework for large-scale multi-perspective opinion summarization. SubSumm consists of a review sampling strategy set and a two-stage... | Han Jiang, Rui Wang, Zhihua Wei, Yu Li, Xinpeng Wang |  |
| 509 |  |  [Topic-Informed Dialogue Summarization using Topic Distribution and Prompt-based Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.376) |  | 0 | Dealing with multiple topics should be considered an important issue in dialogue summarization, because dialogues, unlike documents, are prone to topic drift. Thus, we propose a new dialogue summarization model that reflects dialogue topic distribution to consider all topics present in the dialogue. First, the distribution of dialogue topics is estimated by an effective topic discovery model. Then topic-informed prompt transfers estimated topic distribution information to the output of encoder... | Jaeah You, Youngjoong Ko |  |
| 510 |  |  [Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy](https://doi.org/10.18653/v1/2023.findings-emnlp.377) |  | 0 | We address an important gap in detecting political bias in news articles. Previous works that perform document classification can be influenced by the writing style of each news outlet, leading to overfitting and limited generalizability. Our approach overcomes this limitation by considering both the sentence-level semantics and the document-level rhetorical structure, resulting in a more robust and style-agnostic approach to detecting political bias in news articles. We introduce a novel... | Jiwoo Hong, Yejin Cho, Jiyoung Han, Jaemin Jung, James Thorne |  |
| 511 |  |  [Measuring and Narrowing the Compositionality Gap in Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.378) |  | 0 | We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3... | Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, Mike Lewis |  |
| 512 |  |  [Unsupervised Candidate Answer Extraction through Differentiable Masker-Reconstructor Model](https://doi.org/10.18653/v1/2023.findings-emnlp.379) |  | 0 | Question generation is a widely used data augmentation approach with extensive applications, and extracting qualified candidate answers from context passages is a critical step for most question generation systems. However, existing methods for candidate answer extraction are reliant on linguistic rules or annotated data that face the partial annotation issue and challenges in generalization. To overcome these limitations, we propose a novel unsupervised candidate answer extraction approach... | Zhuoer Wang, Yicheng Wang, Ziwei Zhu, James Caverlee |  |
| 513 |  |  [HoneyBee: Progressive Instruction Finetuning of Large Language Models for Materials Science](https://doi.org/10.18653/v1/2023.findings-emnlp.380) |  | 0 | We propose an instruction-based process for trustworthy data curation in materials science (MatSci-Instruct), which we then apply to finetune a LLaMa-based language model targeted for materials science (HoneyBee). MatSci-Instruct helps alleviate the scarcity of relevant, high-quality materials science textual data available in the open literature, and HoneyBee is the first billion-parameter language model specialized to materials science. In MatSci-Instruct we improve the trustworthiness of... | Yu Song, Santiago Miret, Huan Zhang, Bang Liu |  |
| 514 |  |  [Prompt-Based Editing for Text Style Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.381) |  | 0 | Prompting approaches have been recently explored in text style transfer, where a textual prompt is used to query a pretrained language model (PLM) to generate style-transferred texts word by word in an autoregressive manner. However, such a generation process is less controllable and early prediction errors may affect future word predictions. In this paper, we propose a prompt-based editing approach to text style transfer. Specifically, we prompt a PLM for style classification and use the... | Guoqing Luo, Yutong Han, Lili Mou, Mauajama Firdaus |  |
| 515 |  |  [Representativeness as a Forgotten Lesson for Multilingual and Code-switched Data Collection and Preparation](https://doi.org/10.18653/v1/2023.findings-emnlp.382) |  | 0 | Multilingualism is widespread around the world and code-switching (CSW) is a common practice among different language pairs/tuples across locations and regions. However, there is still not much progress in building successful CSW systems, despite the recent advances in Massive Multilingual Language Models (MMLMs). We investigate the reasons behind this setback through a critical study about the existing CSW data sets (68) across language pairs in terms of the collection and preparation (e.g.... | A. Seza Dogruöz, Sunayana Sitaram, Zheng Xin Yong |  |
| 516 |  |  [NERvous About My Health: Constructing a Bengali Medical Named Entity Recognition Dataset](https://doi.org/10.18653/v1/2023.findings-emnlp.383) |  | 0 | The ability to identify important entities in a text, known as Named Entity Recognition (NER), is useful in a large variety of downstream tasks in the biomedical domain. This is a considerably difficult task when working with Consumer Health Questions (CHQs), which consist of informal language used in day-to-day life by patients. These difficulties are amplified in the case of Bengali, which allows for a huge amount of flexibility in sentence structures and has significant variances in regional... | Alvi Khan, Fida Kamal, Nuzhat Nower, Tasnim Ahmed, Sabbir Ahmed, Tareque Chowdhury |  |
| 517 |  |  [Sparse Black-Box Multimodal Attack for Vision-Language Adversary Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.384) |  | 0 | Deep neural networks have been widely applied in real-world scenarios, such as product restrictions on e-commerce and hate speech monitoring on social media, to ensure secure governance of various platforms. However, illegal merchants often deceive the detection models by adding large-scale perturbations to prohibited products, so as to earn illegal profits. Current adversarial attacks using imperceptible perturbations encounter challenges in simulating such adversarial behavior and evaluating... | Zhen Yu, Zhou Qin, Zhenhua Chen, Meihui Lian, Haojun Fu, Weigao Wen, Hui Xue, Kun He |  |
| 518 |  |  [Towards a Unified Framework for Reference Retrieval and Related Work Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.385) |  | 0 | The task of related work generation aims to generate a comprehensive survey of related research topics automatically, saving time and effort for authors. Existing methods simplify this task by using human-annotated references in a large-scale scientific corpus as information sources, which is time- and cost-intensive. To this end, we propose a Unified Reference Retrieval and Related Work Generation Model (UR3WG), which combines reference retrieval and related work generation processes in a... | Zhengliang Shi, Shen Gao, Zhen Zhang, Xiuying Chen, Zhumin Chen, Pengjie Ren, Zhaochun Ren |  |
| 519 |  |  [Visual Storytelling with Question-Answer Plans](https://doi.org/10.18653/v1/2023.findings-emnlp.386) |  | 0 | Visual storytelling aims to generate compelling narratives from image sequences. Existing models often focus on enhancing the representation of the image sequence, e.g., with external knowledge sources or advanced graph structures. Despite recent progress, the stories are often repetitive, illogical, and lacking in detail. To mitigate these issues, we present a novel framework which integrates visual representations with pretrained language models and planning. Our model translates the image... | Danyang Liu, Mirella Lapata, Frank Keller |  |
| 520 |  |  [Investigating Online Community Engagement through Stancetaking](https://doi.org/10.18653/v1/2023.findings-emnlp.387) |  | 0 | Much work has explored lexical and semantic variation in online communities, and drawn connections to community identity and user engagement patterns. Communities also express identity through the sociolinguistic concept of stancetaking. Large-scale computational work on stancetaking has explored community similarities in their preferences for stance markers – words that serve to indicate aspects of a speaker’s stance – without considering the stance-relevant properties of the contexts in which... | Jai Aggarwal, Brian Diep, Julia Watson, Suzanne Stevenson |  |
| 521 |  |  [ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.388) |  | 0 | As large language models are integrated into society, robustness toward a suite of prompts is increasingly important to maintain reliability in a high-variance environment.Robustness evaluations must comprehensively encapsulate the various settings in which a user may invoke an intelligent system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming, consisting of three methods – semantically aligned augmentation, target bootstrapping, and adversarial knowledge injection. For... | Alex Mei, Sharon Levy, William Yang Wang |  |
| 522 |  |  [Learning to Correct Noisy Labels for Fine-Grained Entity Typing via Co-Prediction Prompt Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.389) |  | 0 | Fine-grained entity typing (FET) is an essential task in natural language processing that aims to assign semantic types to entities in text. However, FET poses a major challenge known as the noise labeling problem, whereby current methods rely on estimating noise distribution to identify noisy labels but are confused by diverse noise distribution deviation. To address this limitation, we introduce Co-Prediction Prompt Tuning for noise correction in FET, which leverages multiple prediction... | Minghao Tang, Yongquan He, Yongxiu Xu, Hongbo Xu, Wenyuan Zhang, Yang Lin |  |
| 523 |  |  [Co²PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.390) |  | 0 | Pre-trained Language Models are widely used in many important real-world applications. However, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications. To address this challenge, we propose Co2PT, an efficient and effective \*debias-while-prompt tuning\* method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks. Our experiments conducted on three extrinsic bias... | Xiangjue Dong, Ziwei Zhu, Zhuoer Wang, Maria Teleki, James Caverlee |  |
| 524 |  |  [A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.391) |  | 0 | Pre-trained language models (PLMs) have achieved outstanding achievements in abstractive single-document summarization (SDS). However, such benefits may not fully extend to multi-document summarization (MDS), where the handling of cross-document information is more complex. Previous works either design new MDS architectures or apply PLMs bluntly with concatenated source documents as a reformulated SDS task. While the former does not utilize previous pre-training efforts and may not generalize... | Chenhui Shen, Liying Cheng, XuanPhi Nguyen, Yang You, Lidong Bing |  |
| 525 |  |  [Universal Domain Adaptation for Robust Handling of Distributional Shifts in NLP](https://doi.org/10.18653/v1/2023.findings-emnlp.392) |  | 0 | When deploying machine learning systems to the wild, it is highly desirable for them to effectively leverage prior knowledge to the unfamiliar domain while also firing alarms to anomalous inputs. In order to address these requirements, Universal Domain Adaptation (UniDA) has emerged as a novel research area in computer vision, focusing on achieving both adaptation ability and robustness (i.e., the ability to detect out-of-distribution samples). While UniDA has led significant progress in... | Hyuhng Joon Kim, Hyunsoo Cho, SangWoo Lee, Junyeob Kim, Choonghyun Park, Sanggoo Lee, Kang Min Yoo, Taeuk Kim |  |
| 526 |  |  [Aligning Language Models to User Opinions](https://doi.org/10.18653/v1/2023.findings-emnlp.393) |  | 0 | An important aspect of developing LLMs that interact with humans is to align models’ behavior to their users. It is possible to prompt an LLM into behaving as a certain persona, especially a user group or ideological persona the model captured during its pertaining stage. But, how to best align an LLM with a specific user and not a demographic or ideological group remains an open question. Mining public opinion surveys (by PEW research), we find that the opinions of a user and their... | EunJeong Hwang, Bodhisattwa Prasad Majumder, Niket Tandon |  |
| 527 |  |  [CCSRD: Content-Centric Speech Representation Disentanglement Learning for End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.394) |  | 0 | Deep neural networks have demonstrated their capacity in extracting features from speech inputs. However, these features may include non-linguistic speech factors such as timbre and speaker identity, which are not directly related to translation. In this paper, we propose a content-centric speech representation disentanglement learning framework for speech translation, CCSRD, which decomposes speech representations into content representations and non-linguistic representations via... | Xiaohu Zhao, Haoran Sun, Yikun Lei, Shaolin Zhu, Deyi Xiong |  |
| 528 |  |  [Miracle: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control](https://doi.org/10.18653/v1/2023.findings-emnlp.395) |  | 0 | Personalized dialogue systems aim to endow the chatbot agent with more anthropomorphic traits for human-like interactions. Previous approaches have explored explicitly user profile modeling using text descriptions, implicit derivation of user embeddings, or utilizing handicraft prompts for ChatGPT-like models. However, textual personas are limited in describing multi-faceted attributes (e.g., language style, inner character nuances), implicit embedding suffers from personality sparsity, and... | Zhenyi Lu, Wei Wei, Xiaoye Qu, XianLing Mao, Dangyang Chen, Jixiong Chen |  |
| 529 |  |  [Towards Multilingual Interlinear Morphological Glossing](https://doi.org/10.18653/v1/2023.findings-emnlp.396) |  | 0 | Interlinear Morphological Glosses are annotations produced in the context of language documentation. Their goal is to identify morphs occurring in an L1 sentence and to explicit their function and meaning, with the further support of an associated translation in L2. We study here the task of automatic glossing, aiming to provide linguists with adequate tools to facilitate this process. Our formalisation of glossing uses a latent variable Conditional Random Field (CRF), which labels the L1... | Shu Okabe, François Yvon |  |
| 530 |  |  [Transformer Working Memory Enables Regular Language Reasoning And Natural Language Length Extrapolation](https://doi.org/10.18653/v1/2023.findings-emnlp.397) |  | 0 | Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of... | TaChung Chi, TingHan Fan, Alexander Rudnicky, Peter J. Ramadge |  |
| 531 |  |  [Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting](https://doi.org/10.18653/v1/2023.findings-emnlp.398) |  | 0 | Query rewriting plays a vital role in enhancing conversational search by transforming context-dependent user queries into standalone forms. Existing approaches primarily leverage human-rewritten queries as labels to train query rewriting models. However, human rewrites may lack sufficient information for optimal retrieval performance. To overcome this limitation, we propose utilizing large language models (LLMs) as query rewriters, enabling the generation of informative query rewrites through... | Fanghua Ye, Meng Fang, Shenghui Li, Emine Yilmaz |  |
| 532 |  |  [Distilling ChatGPT for Explainable Automated Student Answer Assessment](https://doi.org/10.18653/v1/2023.findings-emnlp.399) |  | 0 | Providing explainable and faithful feedback is crucial for automated student answer assessment. In this paper, we introduce a novel framework that explores using ChatGPT, a cutting-edge large language model, for the concurrent tasks of student answer scoring and rationale generation. We identify the appropriate instructions by prompting ChatGPT with different templates to collect the rationales, where inconsistent rationales are refined to align with marking standards. The refined ChatGPT... | Jiazheng Li, Lin Gui, Yuxiang Zhou, David West, Cesare Aloisi, Yulan He |  |
| 533 |  |  [Grammatical Error Correction via Mixed-Grained Weighted Training](https://doi.org/10.18653/v1/2023.findings-emnlp.400) |  | 0 | The task of Grammatical Error Correction (GEC) aims to automatically correct grammatical errors in natural texts. Almost all previous works treat annotated training data equally, but inherent discrepancies in data are neglected. In this paper, the inherent discrepancies are manifested in two aspects, namely, accuracy of data annotation and diversity of potential annotations. To this end, we propose MainGEC, which designs token-level and sentence-level training weights based on inherent... | Jiahao Li, Quan Wang, Chiwei Zhu, Zhendong Mao, Yongdong Zhang |  |
| 534 |  |  [A Unified Framework for Synaesthesia Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.401) |  | 0 | Synaesthesia refers to the description of perceptions in one sensory modality through concepts from other modalities. It involves not only a linguistic phenomenon, but also a cognitive phenomenon structuring human thought and action, which makes understanding it challenging. As a means of cognition, synaesthesia is rendered by more than sensory modalities, cue and stimulus can also play an important role in expressing and understanding it. In addition, understanding synaesthesia involves many... | Kun Sheng, Zhongqing Wang, Qingqing Zhao, Xiaotong Jiang, Guodong Zhou |  |
| 535 |  |  [Domain Private Transformers for Multi-Domain Dialog Systems](https://doi.org/10.18653/v1/2023.findings-emnlp.402) |  | 0 | Large, general purpose language models have demonstrated impressive performance across many different conversational domains. While multi-domain language models achieve low overall perplexity, their outputs are not guaranteed to stay within the domain of a given input prompt. This paper proposes domain privacy as a novel way to quantify how likely a conditional language model will leak across domains. We also develop policy functions based on token-level domain classification, and propose an... | Anmol Kabra, Ethan R. Elenberg |  |
| 536 |  |  [Visual Elements Mining as Prompts for Instruction Learning for Target-Oriented Multimodal Sentiment Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.403) |  | 0 | Target-oriented Multimodal Sentiment Classification (TMSC) aims to incorporate visual modality with text modality to identify the sentiment polarity towards a specific target within a sentence. To address this task, we propose a Visual Elements Mining as Prompts (VEMP) method, which describes the semantic information of visual elements with Text Symbols Embedded in the Image (TSEI), Target-aware Adjective-Noun Pairs (TANPs) and image scene caption, and then transform them into prompts for... | Bin Yang, Jinlong Li |  |
| 537 |  |  [NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.404) |  | 0 | Structured pruning methods have proven effective in reducing the model size and accelerating inference speed in various network architectures such as Transformers. Despite the versatility of encoder-decoder models in numerous NLP tasks, the structured pruning methods on such models are relatively less explored compared to encoder-only models. In this study, we investigate the behavior of the structured pruning of the encoder-decoder models in the decoupled pruning perspective of the encoder and... | Jongwoo Ko, Seungjoon Park, Yujin Kim, Sumyeong Ahn, DuSeong Chang, Euijai Ahn, SeYoung Yun |  |
| 538 |  |  [GBT: Generative Boosting Training Approach for Paraphrase Identification](https://doi.org/10.18653/v1/2023.findings-emnlp.405) |  | 0 | Paraphrase Identification (PI), a task of determining whether a pair of sentences express the same meaning, is widely applied in Information Retrieval and Question Answering. Data Augmentation (DA) is proven effective in tackling the PI task. However, the majority of DA methods still suffer from two limitations: inefficiency and poor quality. In this study, we propose the Generative Boosting Training (GBT) approach for PI. GBT designs a boosting learning method for a single model based on the... | Rui Peng, Zhiling Jin, Yu Hong |  |
| 539 |  |  [DeCrisisMB: Debiased Semi-Supervised Learning for Crisis Tweet Classification via Memory Bank](https://doi.org/10.18653/v1/2023.findings-emnlp.406) |  | 0 | During crisis events, people often use social media platforms such as Twitter to disseminate information about the situation, warnings, advice, and support. Emergency relief organizations leverage such information to acquire timely crisis circumstances and expedite rescue operations. While existing works utilize such information to build models for crisis event analysis, fully-supervised approaches require annotating vast amounts of data and are impractical due to limited response time. On the... | Henry Peng Zou, Yue Zhou, Weizhi Zhang, Cornelia Caragea |  |
| 540 |  |  [Probing LLMs for hate speech detection: strengths and vulnerabilities](https://doi.org/10.18653/v1/2023.findings-emnlp.407) |  | 0 | Recently efforts have been made by social media platforms as well as researchers to detect hateful or toxic language using large language models. However, none of these works aim to use explanation, additional context and victim community information in the detection process. We utilise different prompt variation, input information and evaluate large language models in zero shot setting (without adding any in-context examples). We select two large language models (GPT-3.5 and text-davinci) and... | Sarthak Roy, Ashish Harshavardhan, Animesh Mukherjee, Punyajoy Saha |  |
| 541 |  |  [From Simple to Complex: A Progressive Framework for Document-level Informative Argument Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.408) |  | 0 | Document-level Event Argument Extraction (EAE) requires the model to extract arguments of multiple events from a single document. Considering the underlying dependencies between these events, recent efforts leverage the idea of “memory”, where the results of already predicted events are cached and can be retrieved to help the prediction of upcoming events. These methods extract events according to their appearance order in the document, however, the event that appears in the first sentence does... | Quzhe Huang, Yanxi Zhang, Dongyan Zhao |  |
| 542 |  |  [MultiCMET: A Novel Chinese Benchmark for Understanding Multimodal Metaphor](https://doi.org/10.18653/v1/2023.findings-emnlp.409) |  | 0 | Metaphor is a pervasive aspect of human communication, and its presence in multimodal forms has become more prominent with the progress of mass media. However, there is limited research on multimodal metaphor resources beyond the English language. Furthermore, the existing work in natural language processing does not address the exploration of categorizing the source and target domains in metaphors. This omission is significant considering the extensive research conducted in the fields of... | Dongyu Zhang, Jingwei Yu, Senyuan Jin, Liang Yang, Hongfei Lin |  |
| 543 |  |  [GlotLID: Language Identification for Low-Resource Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.410) |  | 0 | Several recent papers have published good solutions for language identification (LID) for about 300 high-resource and medium-resource languages. However, there is no LID available that (i) covers a wide range of low-resource languages, (ii) is rigorously evaluated and reliable and (iii) efficient and easy to use. Here, we publish GlotLID-M, an LID model that satisfies the desiderata of wide coverage, reliability and efficiency. It identifies 1665 languages, a large increase in coverage compared... | Amir Hossein Kargaran, Ayyoob Imani, François Yvon, Hinrich Schütze |  |
| 544 |  |  [Finding Support Examples for In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.411) |  | 0 | In-context learning is a new learning paradigm where a language model observes a few examples and directly outputs the test input’s prediction. Previous works have shown that it is sensitive to the provided examples and randomly sampled examples probably cause inferior performance. In this paper, we propose finding “support examples” for in-context learning: Given a training dataset, it aims to select one permutation of a few examples, which can well characterize the task for in-context... | Xiaonan Li, Xipeng Qiu |  |
| 545 |  |  [Uncovering the Root of Hate Speech: A Dataset for Identifying Hate Instigating Speech](https://doi.org/10.18653/v1/2023.findings-emnlp.412) |  | 0 | While many prior studies have applied computational approaches, such as machine learning, to detect and moderate hate speech, only scant attention has been paid to the task of identifying the underlying cause of hate speech. In this study, we introduce the concept of hate instigating speech, which refers to a specific type of textual posts on online platforms that stimulate or provoke others to engage in hate speech. The identification of hate instigating speech carries substantial practical... | Hyoungjun Park, Ho Shim, Kyuhan Lee |  |
| 546 |  |  [Responsible AI Considerations in Text Summarization Research: A Review of Current Practices](https://doi.org/10.18653/v1/2023.findings-emnlp.413) |  | 0 | AI and NLP publication venues have increasingly encouraged researchers to reflect on possible ethical considerations, adverse impacts, and other responsible AI issues their work might engender. However, for specific NLP tasks our understanding of how prevalent such issues are, or when and why these issues are likely to arise, remains limited. Focusing on text summarization—a common NLP task largely overlooked by the responsible AI community—we examine research and reporting practices in the... | Yu Lu Liu, Meng Cao, Su Lin Blodgett, Jackie Chi Kit Cheung, Alexandra Olteanu, Adam Trischler |  |
| 547 |  |  [Improving Speech Translation by Fusing Speech and Text](https://doi.org/10.18653/v1/2023.findings-emnlp.414) |  | 0 | In speech translation, leveraging multimodal data to improve model performance and address limitations of individual modalities has shown significant effectiveness. In this paper, we harness the complementary strengths of speech and text to improve speech translation. However, speech and text are disparate modalities, we observe three aspects of modality gap that impede their integration in a speech translation model. To tackle these gaps, we propose \*\*Fuse\*\*-\*\*S\*\*peech-\*\*T\*\*ext... | Wenbiao Yin, Zhicheng Liu, Chengqi Zhao, Tao Wang, Jian Tong, Rong Ye |  |
| 548 |  |  [Narrative Order Aware Story Generation via Bidirectional Pretraining Model with Optimal Transport Reward](https://doi.org/10.18653/v1/2023.findings-emnlp.415) |  | 0 | To create a captivating story, a writer often plans a sequence of logically coherent events and ingeniously manipulates the narrative order to generate flashback in place. However, existing storytelling systems suffer from both insufficient understanding of event correlations and inadequate awareness of event temporal order (e.g., go to hospital <after> get ill), making it challenging to generate high-quality events that balance the logic and narrative order of story. In this paper, we propose... | Zhicong Lu, Li Jin, Guangluan Xu, Linmei Hu, Nayu Liu, Xiaoyu Li, Xian Sun, Zequn Zhang, Kaiwen Wei |  |
| 549 |  |  [Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.416) |  | 0 | Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. Additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents... | Haoran Wang, Kai Shu |  |
| 550 |  |  [Strong and Efficient Baselines for Open Domain Conversational Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.417) |  | 0 | Unlike the Open Domain Question Answering (ODQA) setting, the conversational (ODConvQA) domain has received limited attention when it comes to reevaluating baselines for both efficiency and effectiveness. In this paper, we study the State-of-the-Art (SotA) Dense Passage Retrieval (DPR) retriever and Fusion-in-Decoder (FiD) reader pipeline, and show that it significantly underperforms when applied to ODConvQA tasks due to various limitations. We then propose and evaluate strong yet simple and... | Andrei C. Coman, Gianni Barlacchi, Adrià de Gispert |  |
| 551 |  |  [Efficient Continue Training of Temporal Language Model with Structural Information](https://doi.org/10.18653/v1/2023.findings-emnlp.418) |  | 0 | Current language models are mainly trained on snap-shots of data gathered at a particular time, which decreases their capability to generalize over time and model language change. To model the time variable, existing works have explored temporal language models (e.g., TempoBERT) by directly incorporating the timestamp into the training process. While effective to some extent, these methods are limited by the superficial temporal information brought by timestamps, which fails to learn the... | Zhaochen Su, Juntao Li, Zikang Zhang, Zihan Zhou, Min Zhang |  |
| 552 |  |  [Retrieval-Augmented Parsing for Complex Graphs by Exploiting Structure and Uncertainty](https://doi.org/10.18653/v1/2023.findings-emnlp.419) |  | 0 | Retrieval augmentation enhances generative language models by retrieving informative exemplars relevant for output prediction. However, in realistic graph parsing problems where the output space is large and complex, classic retrieval methods based on input-sentence similarity can fail to identify the most informative exemplars that target graph elements the model is most struggling about, leading to suboptimal retrieval and compromised prediction under limited retrieval budget. In this work,... | Zi Lin, Quan Yuan, Panupong Pasupat, Jeremiah Z. Liu, Jingbo Shang |  |
| 553 |  |  [When it Rains, it Pours: Modeling Media Storms and the News Ecosystem](https://doi.org/10.18653/v1/2023.findings-emnlp.420) |  | 0 | Most events in the world receive at most brief coverage by the news media. Occasionally, however, an event will trigger a media storm, with voluminous and widespread coverage lasting for weeks instead of days. In this work, we develop and apply a pairwise article similarity model, allowing us to identify story clusters in corpora covering local and national online news, and thereby create a comprehensive corpus of media storms over a nearly two year period. Using this corpus, we investigate... | Benjamin Litterer, David Jurgens, Dallas Card |  |
| 554 |  |  [Intra-Event and Inter-Event Dependency-Aware Graph Network for Event Argument Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.421) |  | 0 | Event argument extraction is critical to various natural language processing tasks for providing structured information. Existing works usually extract the event arguments one by one, and mostly neglect to build dependency information among event argument roles, especially from the perspective of event structure. Such an approach hinders the model from learning the interactions between different roles. In this paper, we raise our research question: How to adequately model dependencies between... | Hao Li, Yanan Cao, Yubing Ren, Fang Fang, Lanxue Zhang, Yingjie Li, Shi Wang |  |
| 555 |  |  [From Relevance to Utility: Evidence Retrieval with Feedback for Fact Verification](https://doi.org/10.18653/v1/2023.findings-emnlp.422) |  | 0 | Retrieval-enhanced methods have become a primary approach in fact verification (FV); it requires reasoning over multiple retrieved pieces of evidence to verify the integrity of a claim. To retrieve evidence, existing work often employs off-the-shelf retrieval models whose design is based on the probability ranking principle. We argue that, rather than relevance, for FV we need to focus on the utility that a claim verifier derives from the retrieved evidence. We introduce the feedback-based... | Hengran Zhang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng |  |
| 556 |  |  [How to Train Your Dragon: Diverse Augmentation Towards Generalizable Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.423) |  | 0 | Various techniques have been developed in recent years to improve dense retrieval (DR), such as unsupervised contrastive learning and pseudo-query generation. Existing DRs, however, often suffer from effectiveness tradeoffs between supervised and zero-shot retrieval, which some argue was due to the limited model capacity. We contradict this hypothesis and show that a generalizable DR can be trained to achieve high accuracy in both supervised and zero-shot retrieval without increasing model... | ShengChieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wentau Yih, Xilun Chen |  |
| 557 |  |  [Discovering Highly Influential Shortcut Reasoning: An Automated Template-Free Approach](https://doi.org/10.18653/v1/2023.findings-emnlp.424) |  | 0 | Shortcut reasoning is an irrational process of inference, which degrades the robustness of an NLP model. While a number of previous work has tackled the identification of shortcut reasoning, there are still two major limitations: (i) a method for quantifying the severity of the discovered shortcut reasoning is not provided; (ii) certain types of shortcut reasoning may be missed. To address these issues, we propose a novel method for identifying shortcut reasoning. The proposed method quantifies... | Daichi Haraguchi, Kiyoaki Shirai, Naoya Inoue, Natthawut Kertkeidkachorn |  |
| 558 |  |  [Schema-adaptable Knowledge Graph Construction](https://doi.org/10.18653/v1/2023.findings-emnlp.425) |  | 0 | Conventional Knowledge Graph Construction (KGC) approaches typically follow the static information extraction paradigm with a closed set of pre-defined schema. As a result, such approaches fall short when applied to dynamic scenarios or domains, whereas a new type of knowledge emerges. This necessitates a system that can handle evolving schema automatically to extract information for KGC. To address this need, we propose a new task called schema-adaptable KGC, which aims to continually extract... | Hongbin Ye, Honghao Gui, Xin Xu, Xi Chen, Huajun Chen, Ningyu Zhang |  |
| 559 |  |  [Evaluating the Knowledge Base Completion Potential of GPT](https://doi.org/10.18653/v1/2023.findings-emnlp.426) |  | 0 | Structured knowledge bases (KBs) are an asset for search engines and other applications but are inevitably incomplete. Language models (LMs) have been proposed for unsupervised knowledge base completion (KBC), yet, their ability to do this at scale and with high accuracy remains an open question. Prior experimental studies mostly fall short because they only evaluate on popular subjects, or sample already existing facts from KBs. In this work, we perform a careful evaluation of GPT’s potential... | Blerta Veseli, Simon Razniewski, JanChristoph Kalo, Gerhard Weikum |  |
| 560 |  |  [Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset](https://doi.org/10.18653/v1/2023.findings-emnlp.427) |  | 0 | Mathematical understanding and reasoning are crucial tasks for assessing the capabilities of artificial intelligence (AI). However, existing benchmarks either require just a few steps of reasoning, or only contain a small amount of data in one specific topic, making it hard to analyse AI’s behaviour with reference to different problems within a specific topic in detail. In this work, we propose Conic10K, a challenging math problem dataset on conic sections in Chinese senior high school... | Haoyi Wu, Wenyang Hui, Yezeng Chen, Weiqi Wu, Kewei Tu, Yi Zhou |  |
| 561 |  |  [DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text](https://doi.org/10.18653/v1/2023.findings-emnlp.428) |  | 0 | Spatial reasoning in text plays a crucial role in various real-world applications. Existing approaches for spatial reasoning typically infer spatial relations from pure text, which overlook the gap between natural language and symbolic structures. Graph neural networks (GNNs) have showcased exceptional proficiency in inducing and aggregating symbolic structures. However, classical GNNs face challenges in handling multi-hop spatial reasoning due to the over-smoothing issue, i.e., the performance... | Shuaiyi Li, Yang Deng, Wai Lam |  |
| 562 |  |  [TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised Intent Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.429) |  | 0 | The ability to detect intent in dialogue systems has become increasingly important in modern technology. These systems often generate a large amount of unlabeled data, and manually labeling this data requires substantial human effort. Semi-supervised methods attempt to remedy this cost by using a model trained on a few labeled examples and then by assigning pseudo-labels to further a subset of unlabeled examples that has a model prediction confidence higher than a certain threshold. However,... | Nicholas Botzer, David Vasquez, Tim Weninger, Issam H. Laradji |  |
| 563 |  |  [Late Fusion of Transformers for Sentiment Analysis of Code-Switched Data](https://doi.org/10.18653/v1/2023.findings-emnlp.430) |  | 0 | Code-switching is a common phenomenon in multilingual communities and is often used on social media. However, sentiment analysis of code-switched data is a challenging yet less explored area of research. This paper aims to develop a sentiment analysis system for code-switched data. In this paper, we present a novel approach combining two transformers using logits of their output and feeding them to a neural network for classification. We show the efficacy of our approach using two benchmark... | Gagan Sharma, R. Chinmay, Raksha Sharma |  |
| 564 |  |  [Inductive Relation Inference of Knowledge Graph Enhanced by Ontology Information](https://doi.org/10.18653/v1/2023.findings-emnlp.431) |  | 0 | The inductive inference of the knowledge graph aims to complete the potential relations between the new unknown entities in the graph. Most existing methods are based on entity-independent features such as graph structure information and relationship information to inference. However, the neighborhood of these new entities is often too sparse to obtain enough information to build these features effectively. In this work, we propose a knowledge graph inductive inference method that fuses... | Wentao Zhou, Jun Zhao, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 565 |  |  [Dynamic Stance: Modeling Discussions by Labeling the Interactions](https://doi.org/10.18653/v1/2023.findings-emnlp.432) |  | 0 | Stance detection is an increasingly popular task that has been mainly modeled as a static task, by assigning the expressed attitude of a text toward a given topic. Such a framing presents limitations, with trained systems showing poor generalization capabilities and being strongly topic-dependent. In this work, we propose modeling stance as a dynamic task, by focusing on the interactions between a message and their replies. For this purpose, we present a new annotation scheme that enables the... | Blanca Calvo Figueras, Irene Baucells de la Peña, Tommaso Caselli |  |
| 566 |  |  [Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements](https://doi.org/10.18653/v1/2023.findings-emnlp.433) |  | 0 | Empathetic dialogue is an indispensable part of building harmonious social relationships and contributes to the development of a helpful AI. Previous approaches are mainly based on fine small-scale language models. With the advent of ChatGPT, the application effect of large language models (LLMs) in this field has attracted great attention. This work empirically investigates the performance of LLMs in generating empathetic responses and proposes three improvement methods of semantically similar... | Yushan Qian, Weinan Zhang, Ting Liu |  |
| 567 |  |  [GPT Deciphering Fedspeak: Quantifying Dissent Among Hawks and Doves](https://doi.org/10.18653/v1/2023.findings-emnlp.434) |  | 0 | Markets and policymakers around the world hang on the consequential monetary policy decisions made by the Federal Open Market Committee (FOMC). Publicly available textual documentation of their meetings provides insight into members’ attitudes about the economy. We use GPT-4 to quantify dissent among members on the topic of inflation. We find that transcripts and minutes reflect the diversity of member views about the macroeconomic outlook in a way that is lost or omitted from the public... | Denis Peskoff, Adam Visokay, Sander Schulhoff, Benjamin Wachspress, Alan Blinder, Brandon M. Stewart |  |
| 568 |  |  [DialogQAE: N-to-N Question Answer Pair Extraction from Customer Service Chatlog](https://doi.org/10.18653/v1/2023.findings-emnlp.435) |  | 0 | Harvesting question-answer (QA) pairs from customer service chatlog in the wild is an efficient way to enrich the knowledge base for customer service chatbots in the cold start or continuous integration scenarios. Prior work attempts to obtain 1-to-1 QA pairs from growing customer service chatlog, which fails to integrate the incomplete utterances from the dialog context for composite QA retrieval. In this paper, we propose N-to-N QA extraction task in which the derived questions and... | Xin Zheng, Tianyu Liu, Haoran Meng, Xu Wang, Yufan Jiang, MengLiang Rao, Binghuai Lin, Yunbo Cao, Zhifang Sui |  |
| 569 |  |  [Inverse Reinforcement Learning for Text Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.436) |  | 0 | We introduce inverse reinforcement learning (IRL) as an effective paradigm for training abstractive summarization models, imitating human summarization behaviors. Our IRL model estimates the reward function using a suite of important sub-rewards for summarization and concurrently optimizes the policy network. Experimental results across datasets in different domains (CNN/DailyMail and WikiHow) and various model sizes (BART-base and BART-large) demonstrate the superiority of our proposed IRL... | Yu Fu, Deyi Xiong, Yue Dong |  |
| 570 |  |  [MM-Reasoner: A Multi-Modal Knowledge-Aware Framework for Knowledge-Based Visual Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.437) |  | 0 | Thanks to the strong reasoning capabilities of Large Language Models (LLMs), recent approaches to knowledge-based visual question answering (KVQA) utilize LLMs with a global caption of an input image to answer a question. However, these approaches may miss key visual information that is not captured by the caption. Moreover, they cannot fully utilize the visual information required to answer the question. To address these issues, we introduce a new framework called Multi-Modal Knowledge-Aware... | Mahmoud Khademi, Ziyi Yang, Felipe Frujeri, Chenguang Zhu |  |
| 571 |  |  [Toward Joint Language Modeling for Speech Units and Text](https://doi.org/10.18653/v1/2023.findings-emnlp.438) |  | 0 | Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed... | JuChieh Chou, ChungMing Chien, WeiNing Hsu, Karen Livescu, Arun Babu, Alexis Conneau, Alexei Baevski, Michael Auli |  |
| 572 |  |  [From Chaos to Clarity: Claim Normalization to Empower Fact-Checking](https://doi.org/10.18653/v1/2023.findings-emnlp.439) |  | 0 | With the proliferation of social media platforms, users are exposed to vast information, including posts containing misleading claims. However, the pervasive noise inherent in these posts presents a challenge in identifying precise and prominent claims that require verification. Extracting the core assertions from such posts is arduous and time-consuming. We introduce a novel task, called Claim Normalization (aka ClaimNorm) that aims to decompose complex and noisy social media posts into more... | Megha Sundriyal, Tanmoy Chakraborty, Preslav Nakov |  |
| 573 |  |  [Mitigating Biases in Hate Speech Detection from A Causal Perspective](https://doi.org/10.18653/v1/2023.findings-emnlp.440) |  | 0 | Nowadays, many hate speech detectors are built to automatically detect hateful content. However, their training sets are sometimes skewed towards certain stereotypes (e.g., race or religion-related). As a result, the detectors are prone to depend on some shortcuts for predictions. Previous works mainly focus on token-level analysis and heavily rely on human experts’ annotations to identify spurious correlations, which is not only costly but also incapable of discovering higher-level artifacts.... | Zhehao Zhang, Jiaao Chen, Diyi Yang |  |
| 574 |  |  [Unmasking the Hidden Meaning: Bridging Implicit and Explicit Hate Speech Embedding Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.441) |  | 0 | Research on automatic hate speech (HS) detection has mainly focused on identifying explicit forms of hateful expressions on user-generated content. Recently, a few works have started to investigate methods to address more implicit and subtle abusive content. However, despite these efforts, automated systems still struggle to correctly recognize implicit and more veiled forms of HS. As these systems heavily rely on proper textual representations for classification, it is crucial to investigate... | Nicolás Benjamín Ocampo, Elena Cabrio, Serena Villata |  |
| 575 |  |  [PerturbScore: Connecting Discrete and Continuous Perturbations in NLP](https://doi.org/10.18653/v1/2023.findings-emnlp.442) |  | 0 | With the rapid development of neural network applications in NLP, model robustness problem is gaining more attention. Different from computer vision, the discrete nature of texts makes it more challenging to explore robustness in NLP. Therefore, in this paper, we aim to connect discrete perturbations with continuous perturbations, therefore we can use such connections as a bridge to help understand discrete perturbations in NLP models. Specifically, we first explore how to connect and measure... | Linyang Li, Ke Ren, Yunfan Shao, Pengyu Wang, Xipeng Qiu |  |
| 576 |  |  [InstructoR: Instructing Unsupervised Conversational Dense Retrieval with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.443) |  | 0 | Compared to traditional single-turn ad-hoc retrieval, conversational retrieval needs to handle the multi-turn conversation and understand the user’s real query intent. However, most existing methods simply fine-tune the pre-trained ad-hoc retriever on limited supervised data, making it challenging for the retriever to fully grasp the entirety of the conversation. In this paper, we find that large language models (LLMs) can accurately discover the user’s query intent from the complex... | Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao |  |
| 577 |  |  [The Iron(ic) Melting Pot: Reviewing Human Evaluation in Humour, Irony and Sarcasm Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.444) |  | 0 | Human evaluation in often considered to be the gold standard method of evaluating a Natural Language Generation system. However, whilst its importance is accepted by the community at large, the quality of its execution is often brought into question. In this position paper, we argue that the generation of more esoteric forms of language - humour, irony and sarcasm - constitutes a subdomain where the characteristics of selected evaluator panels are of utmost importance, and every effort should... | Tyler Loakman, Aaron Maladry, Chenghua Lin |  |
| 578 |  |  [INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.445) |  | 0 | A salient characteristic of pre-trained language models (PTLMs) is a remarkable improvement in their generalization capability and emergence of new capabilities with increasing model capacity and pre-training dataset size. Consequently, we are witnessing the development of enormous models pushing the state-of-the-art. It is, however, imperative to realize that this inevitably leads to prohibitively long training times, extortionate computing costs, and a detrimental environmental impact.... | H. S. V. N. S. Kowndinya Renduchintala, Krishnateja Killamsetty, Sumit Bhatia, Milan Aggarwal, Ganesh Ramakrishnan, Rishabh K. Iyer, Balaji Krishnamurthy |  |
| 579 |  |  [Towards General Error Diagnosis via Behavioral Testing in Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.446) |  | 0 | Behavioral testing offers a crucial means of diagnosing linguistic errors and assessing capabilities of NLP models. However, applying behavioral testing to machine translation (MT) systems is challenging as it generally requires human efforts to craft references for evaluating the translation quality of such systems on newly generated test cases. Existing works in behavioral testing of MT systems circumvent this by evaluating translation quality without references, but this restricts diagnosis... | Junjie Wu, Lemao Liu, DitYan Yeung |  |
| 580 |  |  [Retrieval-Augmented Few-shot Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.447) |  | 0 | Retrieval-augmented methods are successful in the standard scenario where the retrieval space is sufficient; whereas in the few-shot scenario with limited retrieval space, this paper shows it is non-trivial to put them into practice. First, it is impossible to retrieve semantically similar examples by using an off-the-shelf metric and it is crucial to learn a task-specific retrieval metric; Second, our preliminary experiments demonstrate that it is difficult to optimize a plausible metric by... | Guoxin Yu, Lemao Liu, Haiyun Jiang, Shuming Shi, Xiang Ao |  |
| 581 |  |  [Temporal Extrapolation and Knowledge Transfer for Lifelong Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.448) |  | 0 | Real-world Temporal Knowledge Graphs keep growing with time and new entities and facts emerge continually, necessitating a model that can extrapolate to future timestamps and transfer knowledge for new components. Therefore, our work first dives into this more realistic issue, lifelong TKG reasoning, where existing methods can only address part of the challenges. Specifically, we formulate lifelong TKG reasoning as a temporal-path-based reinforcement learning (RL) framework. Then, we add... | Zhongwu Chen, Chengjin Xu, Fenglong Su, Zhen Huang, Yong Dou |  |
| 582 |  |  [Comparing Prompt-Based and Standard Fine-Tuning for Urdu Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.449) |  | 0 | Recent advancements in natural language processing have demonstrated the efficacy of pre-trained language models for various downstream tasks through prompt-based fine-tuning. In contrast to standard fine-tuning, which relies solely on labeled examples, prompt-based fine-tuning combines a few labeled examples (few shot) with guidance through prompts tailored for the specific language and task. For low-resource languages, where labeled examples are limited, prompt-based fine-tuning appears to be... | Faizad Ullah, Ubaid Azam, Ali Faheem, Faisal Kamiran, Asim Karim |  |
| 583 |  |  [Explore the Way: Exploring Reasoning Path by Bridging Entities for Effective Cross-Document Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.450) |  | 0 | Cross-document relation extraction (CodRED) task aims to infer the relation between two entities mentioned in different documents within a reasoning path. Previous studies have concentrated on merely capturing implicit relations between the entities. However, humans usually utilize explicit information chains such as hyperlinks or additional searches to find the relations between two entities. Inspired by this, we propose Path wIth expLOraTion (PILOT) that provides the enhanced reasoning path... | Junyoung Son, Jinsung Kim, Jungwoo Lim, Yoonna Jang, Heuiseok Lim |  |
| 584 |  |  [The student becomes the master: Outperforming GPT3 on Scientific Factual Error Correction](https://doi.org/10.18653/v1/2023.findings-emnlp.451) |  | 0 | Due to the prohibitively high cost of creating error correction datasets, most Factual Claim Correction methods rely on a powerful verification model to guide the correction process. This leads to a significant drop in performance in domains like Scientific Claim Correction, where good verification models do not always exist. In this work we introduce SciFix, a claim correction system that does not require a verifier but is able to outperform existing methods by a considerable margin —... | Dhananjay Ashok, Atharva Kulkarni, Hai Pham, Barnabás Póczos |  |
| 585 |  |  [Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.452) |  | 0 | Neural models, including large language models (LLMs), achieve superior performance on multi-hop question-answering. To elicit reasoning capabilities from LLMs, recent works propose using the chain-of-thought (CoT) mechanism to generate both the reasoning chain and the answer, which enhances the model’s capabilities in conducting multi-hop reasoning. However, several challenges still remain: such as struggling with inaccurate reasoning, hallucinations, and lack of interpretability. On the other... | Ruosen Li, Xinya Du |  |
| 586 |  |  [Hierarchical Catalogue Generation for Literature Review: A Benchmark](https://doi.org/10.18653/v1/2023.findings-emnlp.453) |  | 0 | Scientific literature review generation aims to extract and organize important information from an abundant collection of reference papers and produces corresponding reviews while lacking a clear and logical hierarchy. We observe that a high-quality catalogue-guided generation process can effectively alleviate this problem. Therefore, we present an atomic and challenging task named Hierarchical Catalogue Generation for Literature Review as the first step for review generation, which aims to... | Kun Zhu, Xiaocheng Feng, Xiachong Feng, Yingsheng Wu, Bing Qin |  |
| 587 |  |  [MCC-KD: Multi-CoT Consistent Knowledge Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.454) |  | 0 | Large language models (LLMs) have showcased remarkable capabilities in complex reasoning through chain of thought (CoT) prompting. Recently, there has been a growing interest in transferring these reasoning abilities from LLMs to smaller models. However, achieving both the diversity and consistency in rationales presents a challenge. In this paper, we focus on enhancing these two aspects and propose Multi-CoT Consistent Knowledge Distillation (MCC-KD) to efficiently distill the reasoning... | Hongzhan Chen, Siyue Wu, Xiaojun Quan, Rui Wang, Ming Yan, Ji Zhang |  |
| 588 |  |  [An Empirical Study of Frame Selection for Text-to-Video Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.455) |  | 0 | Text-to-video retrieval (TVR) aims to find the most relevant video in a large video gallery given a query text. The intricate and abundant context of the video challenges the performance and efficiency of TVR. To handle the serialized video contexts, existing methods typically select a subset of frames within a video to represent the video content for TVR. How to select the most representative frames is a crucial issue, whereby the selected frames are required to not only retain the semantic... | Mengxia Wu, Min Cao, Yang Bai, Ziyin Zeng, Chen Chen, Liqiang Nie, Min Zhang |  |
| 589 |  |  [Conditional Natural Language Inference](https://doi.org/10.18653/v1/2023.findings-emnlp.456) |  | 0 | To properly explain sentence pairs that provide contradictory (different) information for different conditions, we introduce the task of conditional natural language inference (Cond-NLI) and focus on automatically extracting contradictory aspects and their conditions from a sentence pair. Cond-NLI can help to provide a full spectrum of information, such as when there are multiple answers to a question each addressing a specific condition, or reviews with different opinions for different... | Youngwoo Kim, Razieh Rahimi, James Allan |  |
| 590 |  |  [Contrastive Distant Supervision for Debiased and Denoised Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.457) |  | 0 | Distant Supervision (DS) is a promising learning approach for MRC by leveraging easily-obtained question-answer pairs. Unfortunately, the heuristically annotated dataset will inevitably lead to mislabeled instances, resulting in answer bias and context noise problems. To learn debiased and denoised MRC models, this paper proposes the Contrastive Distant Supervision algorithm – CDS, which can learn to distinguish confusing and noisy instances via confidence-aware contrastive learning.... | Ning Bian, Hongyu Lin, Xianpei Han, Ben He, Le Sun |  |
| 591 |  |  [KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness](https://doi.org/10.18653/v1/2023.findings-emnlp.458) |  | 0 | In recent years, Pre-trained Language Models (PLMs) have shown their superiority by pre-training on unstructured text corpus and then fine-tuning on downstream tasks. On entity-rich textual resources like Wikipedia, Knowledge-Enhanced PLMs (KEPLMs) incorporate the interactions between tokens and mentioned entities in pre-training, and are thus more effective on entity-centric tasks such as entity linking and relation classification. Although exploiting Wikipedia’s rich structures to some... | Yichuan Li, Jialong Han, Kyumin Lee, Chengyuan Ma, Benjamin Z. Yao, Xiaohu Liu |  |
| 592 |  |  [Revisiting Large Language Models as Zero-shot Relation Extractors](https://doi.org/10.18653/v1/2023.findings-emnlp.459) |  | 0 | Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we... | Guozheng Li, Peng Wang, Wenjun Ke |  |
| 593 |  |  [Multi-Stage Pre-training Enhanced by ChatGPT for Multi-Scenario Multi-Domain Dialogue Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.460) |  | 0 | Dialogue summarization involves a wide range of scenarios and domains. However, existing methods generally only apply to specific scenarios or domains. In this study, we propose a new pre-trained model specifically designed for multi-scenario multi-domain dialogue summarization. It adopts a multi-stage pre-training strategy to reduce the gap between the pre-training objective and fine-tuning objective. Specifically, we first conduct domain-aware pre-training using large-scale multi-scenario... | Weixiao Zhou, Gengyao Li, Xianfu Cheng, Xinnian Liang, Junnan Zhu, Feifei Zhai, Zhoujun Li |  |
| 594 |  |  [Towards large language model-based personal agents in the enterprise: Current trends and open problems](https://doi.org/10.18653/v1/2023.findings-emnlp.461) |  | 0 | There is an emerging trend to use large language models (LLMs) to reason about complex goals and orchestrate a set of pluggable tools or APIs to accomplish a goal. This functionality could, among other use cases, be used to build personal assistants for knowledge workers. While there are impressive demos of LLMs being used as autonomous agents or for tool composition, these solutions are not ready mission-critical enterprise settings. For example, they are brittle to input changes, and can... | Vinod Muthusamy, Yara Rizk, Kiran Kate, Praveen Venkateswaran, Vatche Isahagian, Ashu Gulati, Parijat Dube |  |
| 595 |  |  [CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.462) |  | 0 | Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved. To overcome these limitations, we propose CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization. CREATOR disentangles abstract tool creation and concrete decision execution, resulting in improved performance.... | Cheng Qian, Chi Han, Yi Ren Fung, Yujia Qin, Zhiyuan Liu, Heng Ji |  |
| 596 |  |  [Query-based Image Captioning from Multi-context 360cdegree Images](https://doi.org/10.18653/v1/2023.findings-emnlp.463) |  | 0 | A 360-degree image captures the entire scene without the limitations of a camera’s field of view, which makes it difficult to describe all the contexts in a single caption. We propose a novel task called Query-based Image Captioning (QuIC) for 360-degree images, where a query (words or short phrases) specifies the context to describe. This task is more challenging than the conventional image captioning task, which describes salient objects in images, as it requires fine-grained scene... | Koki Maeda, Shuhei Kurita, Taiki Miyanishi, Naoaki Okazaki |  |
| 597 |  |  [Auto Search Indexer for End-to-End Document Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.464) |  | 0 | Generative retrieval, which is a new advanced paradigm for document retrieval, has recently attracted research interests, since it encodes all documents into the model and directly generates the retrieved documents. However, its power is still underutilized since it heavily relies on the “preprocessed” document identifiers (docids), thus limiting its retrieval performance and ability to retrieve new documents. In this paper, we propose a novel fully end-to-end retrieval paradigm. It can not... | Tianchi Yang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang |  |
| 598 |  |  ['Person' == Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion](https://doi.org/10.18653/v1/2023.findings-emnlp.465) |  | 0 | We study stereotypes embedded within one of the most popular text-to-image generators: Stable Diffusion. We answer the question: what stereotypes of gender and nationality/continental identity does Stable Diffusion display in the absence of such information i.e. what gender and nationality/continental identity is assigned to ‘a person,’ or to ‘a person from Asia.’ Using CLIP-cosine similarity for zero-shot classification of images generated by CLIP-based Stable Diffusion v2.1 verified by manual... | Sourojit Ghosh, Aylin Caliskan |  |
| 599 |  |  [Task-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.466) |  | 0 | The size and the computational load of fine-tuning large-scale pre-trained neural network are becoming two major obstacles in adopting machine learning in many applications. Continual learning (CL) can serve as a remedy through enabling knowledge-transfer across sequentially arriving tasks which relaxes the need to fine-tune all network weights from scratch. However, existing CL algorithms primarily consider learning unimodal vision-only or language-only tasks. We develop a transformer-based CL... | Yuliang Cai, Jesse Thomason, Mohammad Rostami |  |
| 600 |  |  [Evaluating Verifiability in Generative Search Engines](https://doi.org/10.18653/v1/2023.findings-emnlp.467) |  | 0 | Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and accurately (high citation precision; every cite supports its associated statement). We conduct human evaluation to audit four popular generative search engines—Bing Chat, NeevaAI, perplexity.ai, and... | Nelson F. Liu, Tianyi Zhang, Percy Liang |  |
| 601 |  |  [Enhancing Abstractiveness of Summarization Models through Calibrated Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.468) |  | 0 | In this paper, we propose a novel approach named DisCal to enhance the level of abstractiveness (measured by n-gram overlap) without sacrificing the informativeness (measured by ROUGE) of generated summaries. DisCal exposes diverse pseudo summaries with two supervision to the student model. Firstly, the best pseudo summary is identified in terms of abstractiveness and informativeness and used for sequence-level distillation. Secondly, their ranks are used to ensure the student model to assign... | Hwanjun Song, Igor Shalyminov, Hang Su, Siffi Singh, Kaisheng Yao, Saab Mansour |  |
| 602 |  |  [Visually Grounded Continual Language Learning with Selective Specialization](https://doi.org/10.18653/v1/2023.findings-emnlp.469) |  | 0 | A desirable trait of an artificial agent acting in the visual world is to continually learn a sequence of language-informed tasks while striking a balance between sufficiently specializing in each task and building a generalized knowledge for transfer. Selective specialization, i.e., a careful selection of model components to specialize in each task, is a strategy to provide control over this trade-off. However, the design of selection strategies requires insights on the role of each model... | Kyra Ahrens, Lennart Bengtson, Jae Hee Lee, Stefan Wermter |  |
| 603 |  |  [RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.470) |  | 0 | We introduce RoMQA, the first benchmark for robust, multi-evidence, multi-answer question answering (QA). RoMQA contains clusters of questions that are derived from related constraints mined from the Wikidata knowledge graph. RoMQA evaluates robustness of QA models to varying constraints by measuring worst-case performance within each question cluster. Compared to prior QA datasets, RoMQA has more human-written questions that require reasoning over more evidence text and have, on average, many... | Victor Zhong, Weijia Shi, Wentau Yih, Luke Zettlemoyer |  |
| 604 |  |  [Leveraging Multiple Teachers for Test-Time Adaptation of Language-Guided Classifiers](https://doi.org/10.18653/v1/2023.findings-emnlp.471) |  | 0 | Recent approaches have explored language- guided classifiers capable of classifying examples from novel tasks when provided with task-specific natural language explanations, instructions or prompts (Sanh et al., 2022; R. Menon et al., 2022). While these classifiers can generalize in zero-shot settings, their task performance often varies substantially between different language explanations in unpredictable ways (Lu et al., 2022; Gonen et al., 2022). Also, current approaches fail to leverage... | Kangda Wei, Sayan Ghosh, Rakesh R. Menon, Shashank Srivastava |  |
| 605 |  |  [Summarizing Multiple Documents with Conversational Structure for Meta-Review Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.472) |  | 0 | We present PeerSum, a novel dataset for generating meta-reviews of scientific papers. The meta-reviews can be interpreted as abstractive summaries of reviews, multi-turn discussions and the paper abstract. These source documents have a rich inter-document relationship with an explicit hierarchical conversational structure, cross-references and (occasionally) conflicting information. To introduce the structural inductive bias into pre-trained language models, we introduce RAMMER... | Miao Li, Eduard H. Hovy, Jey Han Lau |  |
| 606 |  |  [VIPHY: Probing "Visible" Physical Commonsense Knowledge](https://doi.org/10.18653/v1/2023.findings-emnlp.473) |  | 0 | Vision-language models (VLMs) have shown remarkable performance on visual reasoning tasks (e.g. attributes, location). While such tasks measure the requisite knowledge to ground and reason over a given visual instance, they do not, however, measure the ability of VLMs to retain and generalize such knowledge. In this work, we evaluate VLMs’ ability to acquire “visible” physical knowledge – the information that is easily accessible from images of static scenes, particularly along the dimensions... | Shikhar Singh, Ehsan Qasemi, Muhao Chen |  |
| 607 |  |  [Two Directions for Clinical Data Generation with Large Language Models: Data-to-Label and Label-to-Data](https://doi.org/10.18653/v1/2023.findings-emnlp.474) |  | 0 | Large language models (LLMs) can generate natural language texts for various domains and tasks, but their potential for clinical text mining, a domain with scarce, sensitive, and imbalanced medical data, is under-explored. We investigate whether LLMs can augment clinical data for detecting Alzheimer’s Disease (AD)-related signs and symptoms from electronic health records (EHRs), a challenging task that requires high expertise. We create a novel pragmatic taxonomy for AD sign and symptom... | Rumeng Li, Xun Wang, Hong Yu |  |
| 608 |  |  [Stylized Dialogue Generation with Feature-Guided Knowledge Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.475) |  | 0 | Stylized dialogue generation systems aim to produce coherent and context-aware dialogues while effectively emulating the desired style. Generating stylized dialogue is valuable yet challenging due to the scarce parallel data. Existing methods often synthesize pseudo data through back translation, yet suffer from noisy and context-agnostic style signals caused by insufficient guidance on target style features. To address this, we propose the knowledge-augmented stylized dialogue generation... | Jinpeng Li, Zekai Zhang, Xiuying Chen, Dongyan Zhao, Rui Yan |  |
| 609 |  |  [Probing LLMs for Joint Encoding of Linguistic Categories](https://doi.org/10.18653/v1/2023.findings-emnlp.476) |  | 0 | Large Language Models (LLMs) exhibit impressive performance on a range of NLP tasks, due to the general-purpose linguistic knowledge acquired during pretraining. Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing. Yet, little is known about how encodings of different linguistic phenomena interact within the... | Giulio Starace, Konstantinos Papakostas, Rochelle Choenni, Apostolos Panagiotopoulos, Matteo Rosati, Alina Leidinger, Ekaterina Shutova |  |
| 610 |  |  [On Robustness of Finetuned Transformer-based NLP Models](https://doi.org/10.18653/v1/2023.findings-emnlp.477) |  | 0 | Transformer-based pretrained models like BERT, GPT-2 and T5 have been finetuned for a large number of natural language processing (NLP) tasks, and have been shown to be very effective. However, while finetuning, what changes across layers in these models with respect to pretrained checkpoints is under-studied. Further, how robust are these models to perturbations in input text? Does the robustness vary depending on the NLP task for which the models have been finetuned? While there exists some... | Pavan Kalyan Reddy Neerudu, Subba Reddy Oota, Mounika Marreddy, Venkateswara Rao Kagita, Manish Gupta |  |
| 611 |  |  [Measuring and Mitigating Constraint Violations of In-Context Learning for Utterance-to-API Semantic Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.478) |  | 0 | In executable task-oriented semantic parsing, the system aims to translate users’ utterances in natural language to machine-interpretable programs (API calls) that can be executed according to pre-defined API specifications. With the popularity of Large Language Models (LLMs), in-context learning offers a strong baseline for such scenarios, especially in data-limited regimes. However, LLMs are known to hallucinate and therefore pose a formidable challenge in constraining generated content.... | Shufan Wang, Sébastien Jean, Sailik Sengupta, James Gung, Nikolaos Pappas, Yi Zhang |  |
| 612 |  |  [Entity Disambiguation on a Tight Labeling Budget](https://doi.org/10.18653/v1/2023.findings-emnlp.479) |  | 0 | Many real-world NLP applications face the challenge of training an entity disambiguation model for a specific domain with a small labeling budget. In this setting there is often access to a large unlabeled pool of documents. It is then natural to ask the question: which samples should be selected for annotation? In this paper we propose a solution that combines feature diversity with low rank correction. Our sampling strategy is formulated in the context of bilinear tensor models. Our... | Audi Primadhanty, Ariadna Quattoni |  |
| 613 |  |  [Topic-DPR: Topic-based Prompts for Dense Passage Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.480) |  | 0 | Prompt-based learning’s efficacy across numerous natural language processing tasks has led to its integration into dense passage retrieval. Prior research has mainly focused on enhancing the semantic understanding of pre-trained language models by optimizing a single vector as a continuous prompt. This approach, however, leads to a semantic space collapse; identical semantic information seeps into all representations, causing their distributions to converge in a restricted region. This hinders... | Qingfa Xiao, Shuangyin Li, Lei Chen |  |
| 614 |  |  [Quantifying the Dialect Gap and its Correlates Across Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.481) |  | 0 | Historically, researchers and consumers have noticed a decrease in quality when applying NLP tools to minority variants of languages (i.e. Puerto Rican Spanish or Swiss German), but studies exploring this have been limited to a select few languages. Additionally, past studies have mainly been conducted in a monolingual context, so cross-linguistic trends have not been identified and tied to external factors. In this work, we conduct a comprehensive evaluation of the most influential,... | Anjali Kantharuban, Ivan Vulic, Anna Korhonen |  |
| 615 |  |  [RECAL: Sample-Relation Guided Confidence Calibration over Tabular Data](https://doi.org/10.18653/v1/2023.findings-emnlp.482) |  | 0 | Tabular-format data is widely adopted in various real-world applications. Various machine learning models have achieved remarkable success in both industrial applications and data-science competitions. Despite these successes, most current machine learning methods for tabular data lack accurate confidence estimation, which is needed by some high-risk sensitive applications such as credit modeling and financial fraud detection. In this paper, we study the confidence estimation of machine... | Haotian Wang, Zhen Zhang, Mengting Hu, Qichao Wang, Liang Chen, Yatao Bian, Bingzhe Wu |  |
| 616 |  |  [Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment](https://doi.org/10.18653/v1/2023.findings-emnlp.483) |  | 0 | Pre-trained vision and language models such as CLIP have witnessed remarkable success in connecting images and texts with a primary focus on English texts. Despite recent efforts to extend CLIP to support other languages, disparities in performance among different languages have been observed due to uneven resource availability. Additionally, current cross-lingual transfer methods of those pre-trained models would consume excessive resources for a large number of languages. Therefore, we... | Zhen Zhang, Jialu Wang, Xin Eric Wang |  |
| 617 |  |  [Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries](https://doi.org/10.18653/v1/2023.findings-emnlp.484) |  | 0 | Ideal summarization models should generalize to novel summary-worthy content without remembering reference training summaries by rote. However, a single average performance score on the entire test set is inadequate in determining such model competencies. We propose a fine-grained evaluation protocol by partitioning a test set based on the lexical similarity of reference test summaries with training summaries. We observe up to a 5x (1.2x) difference in ROUGE-2 (entity recall) scores between the... | Prafulla Kumar Choubey, Alexander R. Fabbri, Caiming Xiong, ChienSheng Wu |  |
| 618 |  |  [Pseudointelligence: A Unifying Lens on Language Model Evaluation](https://doi.org/10.18653/v1/2023.findings-emnlp.485) |  | 0 | With large language models surpassing human performance on an increasing number of benchmarks, we must take a principled approach for targeted evaluation of model capabilities. Inspired by pseudorandomness, we propose pseudointelligence, which captures the maxim that “(perceived) intelligence lies in the eye of the beholder.” That is, that claims of intelligence are meaningful only when their evaluator is taken into account. Concretely, we propose a complexity-theoretic framework of model... | Shikhar Murty, Orr Paradise, Pratyusha Sharma |  |
| 619 |  |  [GDA: Grammar-based Data Augmentation for Text Classification using Slot Information](https://doi.org/10.18653/v1/2023.findings-emnlp.486) |  | 0 | Recent studies propose various data augmentation approaches to resolve the low-resource problem in natural language processing tasks. Data augmentation is a successful solution to this problem and recent strategies give variation on sentence structures to boost performance. However, these approaches can potentially lead to semantic errors and produce semantically noisy data due to the unregulated variation of sentence structures. In an effort to combat these semantic errors, we leverage slot... | Joonghyuk Hahn, Hyunjoon Cheon, Elizabeth Orwig, SuHyeon Kim, SangKi Ko, YoSub Han |  |
| 620 |  |  [Implicit Sense-labeled Connective Recognition as Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.487) |  | 0 | Implicit Discourse Relation Recognition (IDRR) involves identifying the sense label of an implicit connective between adjacent text spans. This has traditionally been approached as a classification task. However, some downstream tasks require more than just a sense label as well as the specific connective used. This paper presents Implicit Sense-labeled Connective Recognition (ISCR), which identifies the implicit connectives and their sense labels between adjacent text spans. ISCR can be... | Yui Oka, Tsutomu Hirao |  |
| 621 |  |  [VISTA: Visual-Textual Knowledge Graph Representation Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.488) |  | 0 | Knowledge graphs represent human knowledge using triplets composed of entities and relations. While most existing knowledge graph embedding methods only consider the structure of a knowledge graph, a few recently proposed multimodal methods utilize images or text descriptions of entities in a knowledge graph. In this paper, we propose visual-textual knowledge graphs (VTKGs), where not only entities but also triplets can be explained using images, and both entities and relations can accompany... | Jaejun Lee, Chanyoung Chung, Hochang Lee, Sungho Jo, Joyce Jiyoung Whang |  |
| 622 |  |  [Dynamic Stashing Quantization for Efficient Transformer Training](https://doi.org/10.18653/v1/2023.findings-emnlp.489) |  | 0 | Large Language Models (LLMs) have demonstrated impressive performance on a range of Natural Language Processing (NLP) tasks. Unfortunately, the immense amount of computations and memory accesses required for LLM training makes them prohibitively expensive in terms of hardware cost, and thus challenging to deploy in use cases such as on-device learning. In this paper, motivated by the observation that LLM training is memory-bound, we propose a novel dynamic quantization strategy, termed Dynamic... | Guo Yang, Daniel Lo, Robert D. Mullins, Yiren Zhao |  |
| 623 |  |  [A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.490) |  | 0 | Large language models (LLMs) have demonstrated great potential for domain-specific applications, such as the law domain. However, recent disputes over GPT-4’s law evaluation raise questions concerning their performance in real-world legal tasks. To systematically investigate their competency in the law, we design practical baseline solutions based on LLMs and test on the task of legal judgment prediction. In our solutions, LLMs can work alone to answer open questions or coordinate with an... | Ruihao Shui, Yixin Cao, Xiang Wang, TatSeng Chua |  |
| 624 |  |  [A Lightweight Method to Generate Unanswerable Questions in English](https://doi.org/10.18653/v1/2023.findings-emnlp.491) |  | 0 | If a question cannot be answered with the available information, robust systems for question answering (QA) should know \*not\* to answer. One way to build QA models that do this is with additional training data comprised of unanswerable questions, created either by employing annotators or through automated methods for unanswerable question generation. To show that the model complexity of existing automated approaches is not justified, we examine a simpler data augmentation method for... | Vagrant Gautam, Miaoran Zhang, Dietrich Klakow |  |
| 625 |  |  [Automatic Evaluate Dialogue Appropriateness by Using Dialogue Act](https://doi.org/10.18653/v1/2023.findings-emnlp.492) |  | 0 | Evaluation of dialogue systems requires assessing various aspects, among which appropriateness holds significance as a core element of communicative language competence. However, current evaluations heavily rely on human judgments, which are time-consuming, labor-intensive, prone to biases, and lacking objectivity. In this paper, we introduce Dialogue Act Appropriateness (DAA), a novel method that utilizes the underlying patterns of dialogue act transitions to evaluate the appropriateness of... | Bao Chen, Yuanjie Wang, Zeming Liu, Yuhang Guo |  |
| 626 |  |  [TabPrompt: Graph-based Pre-training and Prompting for Few-shot Table Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.493) |  | 0 | Table Understanding (TU) is a crucial aspect of information extraction that enables machines to comprehend the semantics behind tabular data. However, existing methods of TU cannot deal with the scarcity of labeled tabular data. In addition, these methods primarily focus on the textual content within the table, disregarding the inherent topological information of the table. This can lead to a misunderstanding of the tabular semantics. In this paper, we propose TabPrompt, a new framework to... | Rihui Jin, Jianan Wang, Wei Tan, Yongrui Chen, Guilin Qi, Wang Hao |  |
| 627 |  |  [Towards Formality-Aware Neural Machine Translation by Leveraging Context Information](https://doi.org/10.18653/v1/2023.findings-emnlp.494) |  | 0 | Formality is one of the most important linguistic properties to determine the naturalness of translation. Although a target-side context contains formality-related tokens, the sparsity within the context makes it difficult for context-aware neural machine translation (NMT) models to properly discern them. In this paper, we introduce a novel training method to explicitly inform the NMT model by pinpointing key informative tokens using a formality classifier. Given a target context, the formality... | Dohee Kim, Yujin Baek, Soyoung Yang, Jaegul Choo |  |
| 628 |  |  [Improving Seq2Seq Grammatical Error Correction via Decoding Interventions](https://doi.org/10.18653/v1/2023.findings-emnlp.495) |  | 0 | The sequence-to-sequence (Seq2Seq) approach has recently been widely used in grammatical error correction (GEC) and shows promising performance. However, the Seq2Seq GEC approach still suffers from two issues. First, a Seq2Seq GEC model can only be trained on parallel data, which, in GEC task, is often noisy and limited in quantity. Second, the decoder of a Seq2Seq GEC model lacks an explicit awareness of the correctness of the token being generated. In this paper, we propose a unified decoding... | Houquan Zhou, Yumeng Liu, Zhenghua Li, Min Zhang, Bo Zhang, Chen Li, Ji Zhang, Fei Huang |  |
| 629 |  |  [Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses](https://doi.org/10.18653/v1/2023.findings-emnlp.496) |  | 0 | In this paper, we explore the application of large language models (LLMs) for generating code-tracing questions in introductory programming courses. We designed targeted prompts for GPT4, guiding it to generate code-tracing questions based on code snippets and descriptions. We established a set of human evaluation metrics to assess the quality of questions produced by the model compared to those created by human experts. Our analysis provides insights into the capabilities and potential of LLMs... | Aysa Xuemo Fan, Haoran Zhang, Luc Paquette, Rui Zhang |  |
| 630 |  |  [Learning Easily Updated General Purpose Text Representations with Adaptable Task-Specific Prefix](https://doi.org/10.18653/v1/2023.findings-emnlp.497) |  | 0 | Many real-world applications require making multiple predictions from the same text. Fine-tuning a large pre-trained language model for each downstream task causes computational burdens in the inference time due to several times of forward passes. To amortize the computational cost, freezing the language model and building lightweight models for downstream tasks based on fixed text representations are common solutions. Accordingly, how to learn fixed but general text representations that can... | KuanHao Huang, Liang Tan, Rui Hou, Sinong Wang, Amjad Almahairi, Ruty Rinott |  |
| 631 |  |  [Good Meta-tasks Make A Better Cross-lingual Meta-transfer Learning for Low-resource Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.498) |  | 0 | Model-agnostic meta-learning has garnered attention as a promising technique for enhancing few-shot cross-lingual transfer learning in low-resource scenarios. However, little attention was paid to the impact of data selection strategies on this cross-lingual meta-transfer method, particularly the sampling of cross-lingual meta-training data (i.e. meta-tasks) at the syntactic level to reduce language gaps. In this paper, we propose a Meta-Task Collector-based Cross-lingual Meta-Transfer... | Linjuan Wu, Zongyi Guo, Baoliang Cui, Haihong Tang, Weiming Lu |  |
| 632 |  |  [Reasoning Makes Good Annotators : An Automatic Task-specific Rules Distilling Framework for Low-resource Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.499) |  | 0 | Relation extraction is often challenged by insufficient labeled data. Previous methods exploit knowledge from unlabeled data by generating pseudo labels in a self-training pipeline, which suffers a gradual drift problem. Logic rules, a transferable and explainable form of expert knowledge, have achieved promising success by improving the model with weak labels. But manually writing comprehensive rules set is challenging and tedious. To alleviate the human labor of writing high-quality rules, in... | Yilin Lu, Juncheng Li, Xiaoqiang Wang, Haochen Shi, Tao Chen, Siliang Tang |  |
| 633 |  |  [Co-training and Co-distillation for Quality Improvement and Compression of Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.500) |  | 0 | Knowledge Distillation (KD) compresses computationally expensive pre-trained language models (PLMs) by transferring their knowledge to smaller models, allowing their use in resource-constrained or real-time settings. However, most smaller models fail to surpass the performance of the original larger model, resulting in sacrificing performance to improve inference speed. To address this issue, we propose Co-Training and Co-Distillation (CTCD), a novel framework that improves performance and... | Hayeon Lee, Rui Hou, Jongpil Kim, Davis Liang, Hongbo Zhang, Sung Ju Hwang, Alexander Min |  |
| 634 |  |  [ReadPrompt: A Readable Prompting Method for Reliable Knowledge Probing](https://doi.org/10.18653/v1/2023.findings-emnlp.501) |  | 0 | Knowledge probing is a task to assess the knowledge encoded within pre-trained language models (PLMs) by having the PLM complete prompts such as “Italy is located in __,”. The model’s prediction precision serves as a lower bound for the amount of knowledge it contains. Subsequent works explore training a series of vectors as prompts to guide PLMs towards more accurate predictions. However, these methods compromise the readability of the prompts. We cannot directly understand these prompts from... | Zezhong Wang, Luyao Ye, Hongru Wang, WaiChung Kwan, David Ho, KamFai Wong |  |
| 635 |  |  [Coherent Entity Disambiguation via Modeling Topic and Categorical Dependency](https://doi.org/10.18653/v1/2023.findings-emnlp.502) |  | 0 | Previous entity disambiguation (ED) methods adopt a discriminative paradigm, where prediction is made based on matching scores between mention context and candidate entities using length-limited encoders. However, these methods often struggle to capture explicit discourse-level dependencies, resulting in incoherent predictions at the abstract level (e.g. topic or category). We propose CoherentED, an ED system equipped with novel designs aimed at enhancing the coherence of entity predictions.... | Zilin Xiao, Linjun Shou, Xingyao Zhang, Jie Wu, Ming Gong, Daxin Jiang |  |
| 636 |  |  [How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench](https://doi.org/10.18653/v1/2023.findings-emnlp.503) |  | 0 | We investigate the predictability of large language model (LLM) capabilities: given records of past experiments using different model families, numbers of parameters, tasks, and numbers of in-context examples, can we accurately predict LLM performance on new experiment configurations? Answering this question has practical implications for LLM users (e.g., deciding which models to try), developers (e.g., prioritizing evaluation on representative tasks), and the research community (e.g.,... | Qinyuan Ye, Harvey Yiyun Fu, Xiang Ren, Robin Jia |  |
| 637 |  |  [POSQA: Probe the World Models of LLMs with Size Comparisons](https://doi.org/10.18653/v1/2023.findings-emnlp.504) |  | 0 | Embodied language comprehension emphasizes that language understanding is not solely a matter of mental processing in the brain but also involves interactions with the physical and social environment. With the explosive growth of Large Language Models (LLMs) and their already ubiquitous presence in our daily lives, it is becoming increasingly necessary to verify their real-world understanding. Inspired by cognitive theories, we propose POSQA: a Physical Object Size Question Answering dataset... | Chang Shu, Jiuzhou Han, Fangyu Liu, Ehsan Shareghi, Nigel Collier |  |
| 638 |  |  [Hierarchical Fusion for Online Multimodal Dialog Act Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.505) |  | 0 | We propose a framework for online multimodal dialog act (DA) classification based on raw audio and ASR-generated transcriptions of current and past utterances. Existing multimodal DA classification approaches are limited by ineffective audio modeling and late-stage fusion. We showcase significant improvements in multimodal DA classification by integrating modalities at a more granular level and incorporating recent advancements in large language and audio models for audio feature extraction. We... | Md Messal Monem Miah, Adarsh Pyarelal, Ruihong Huang |  |
| 639 |  |  [STEER: Unified Style Transfer with Expert Reinforcement](https://doi.org/10.18653/v1/2023.findings-emnlp.506) |  | 0 | While text style transfer has many applications across natural language processing, the core premise of transferring from a single source style is unrealistic in a real-world setting. In this work, we focus on arbitrary style transfer: rewriting a text from an arbitrary, unknown style to a target style. We propose STEER: Unified Style Transfer with Expert Reinforcement, a unified frame-work developed to overcome the challenge of limited parallel data for style transfer. STEER involves... | Skyler Hallinan, Faeze Brahman, Ximing Lu, Jaehun Jung, Sean Welleck, Yejin Choi |  |
| 640 |  |  [Enhancing Argument Structure Extraction with Efficient Leverage of Contextual Information](https://doi.org/10.18653/v1/2023.findings-emnlp.507) |  | 0 | Argument structure extraction (ASE) aims to identify the discourse structure of arguments within documents. Previous research has demonstrated that contextual information is crucial for developing an effective ASE model. However, we observe that merely concatenating sentences in a contextual window does not fully utilize contextual information and can sometimes lead to excessive attention on less informative sentences. To tackle this challenge, we propose an Efficient Context-aware ASE model... | Yun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Jie Zhou, Yue Zhang |  |
| 641 |  |  [Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate](https://doi.org/10.18653/v1/2023.findings-emnlp.508) |  | 0 | Large Language Models (LLMs) have shown impressive capabilities in various applications, but they still face various inconsistency issues. Existing works primarily focus on the inconsistency issues within a single LLM, while we complementarily explore the inter-consistency among multiple LLMs for collaboration. To examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal, we focus on commonsense reasoning, and introduce a formal debate framework (FORD) to conduct... | Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, Bing Qin |  |
| 642 |  |  [Culturally Aware Natural Language Inference](https://doi.org/10.18653/v1/2023.findings-emnlp.509) |  | 0 | Humans produce and consume language in a particular cultural context, which includes knowledge about specific norms and practices. A listener’s awareness of the cultural context is critical for interpreting the speaker’s meaning. A simple expression like \*I didn’t leave a tip\* implies a strong sense of dissatisfaction when tipping is assumed to be the norm. As NLP systems reach users from different cultures, achieving culturally aware language understanding becomes increasingly important.... | Jing Huang, Diyi Yang |  |
| 643 |  |  [End-to-End Autoregressive Retrieval via Bootstrapping for Smart Reply Systems](https://doi.org/10.18653/v1/2023.findings-emnlp.510) |  | 0 | Reply suggestion systems represent a staple component of many instant messaging and email systems. However, the requirement to produce sets of replies, rather than individual replies, makes the task poorly suited for out-of-the-box retrieval architectures, which only consider individual message-reply similarity. As a result, these system often rely on additional post-processing modules to diversify the outputs. However, these approaches are ultimately bottlenecked by the performance of the... | Benjamin Towle, Ke Zhou |  |
| 644 |  |  [Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness](https://doi.org/10.18653/v1/2023.findings-emnlp.511) |  | 0 | The potential of using a large language model (LLM) as a knowledge base (KB) has sparked significant interest. To maintain the knowledge acquired by LLMs, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge. Existing work on editing LLMs has partially addressed the issue of dependency, when the editing of a fact should apply to its lexical variations without disrupting irrelevant ones. However, they neglect the... | Zichao Li, Ines Arous, Siva Reddy, Jackie Chi Kit Cheung |  |
| 645 |  |  [Effects of Human Adversarial and Affable Samples on BERT Generalizability](https://doi.org/10.18653/v1/2023.findings-emnlp.512) |  | 0 | BERT-based models have had strong performance on leaderboards, yet have been demonstrably worse in real-world settings requiring generalization. Limited quantities of training data is considered a key impediment to achieving generalizability in machine learning. In this paper, we examine the impact of training data quality, not quantity, on a model’s generalizability. We consider two characteristics of training data: the portion of human-adversarial (h-adversarial), i.e. sample pairs with... | Aparna Elangovan, Estrid He, Yuan Li, Karin Verspoor |  |
| 646 |  |  [Logic Unveils Truth, While Disguise Obscures It: Transition Logic Augmented Response Selection for Multi-Turn Dialogue](https://doi.org/10.18653/v1/2023.findings-emnlp.513) |  | 0 | Multi-turn response selection aims to retrieve a response for a dialogue context from a candidate pool and negative sampling is the key to its retrieval performance. However, previous methods of negative samples tend to yield false negatives due to the one-to-many property in open-domain dialogue, which is detrimental to the optimization process. To deal with the problem, we propose a sequential variational ladder auto-encoder to capture the diverse one-to-many transition pattern of multiple... | Tingchen Fu, Xueliang Zhao, Lemao Liu, Rui Yan |  |
| 647 |  |  [Are Language Models Worse than Humans at Following Prompts? It's Complicated](https://doi.org/10.18653/v1/2023.findings-emnlp.514) |  | 0 | Prompts have been the center of progress in advancing language models’ zero-shot and few-shot performance. However, recent work finds that models can perform surprisingly well when given intentionally irrelevant or misleading prompts. Such results may be interpreted as evidence that model behavior is not “human like’. In this study, we challenge a central assumption in such work: that humans would perform badly when given pathological instructions. We find that humans are able to reliably... | Albert Webson, Alyssa Marie Loo, Qinan Yu, Ellie Pavlick |  |
| 648 |  |  [A Sequence-to-Structure Approach to Document-level Targeted Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.515) |  | 0 | Most previous studies on aspect-based sentiment analysis (ABSA) were carried out at the sentence level, while the research of document-level ABSA has not received enough attention. In this work, we focus on the document-level targeted sentiment analysis task, which aims to extract the opinion targets consisting of multi-level entities from a review document and predict their sentiments. We propose a Sequence-to-Structure (Seq2Struct) approach to address the task, which is able to explicitly... | Nan Song, Hongjie Cai, Rui Xia, Jianfei Yu, Zhen Wu, Xinyu Dai |  |
| 649 |  |  [Generating Extractive Answers: Gated Recurrent Memory Reader for Conversational Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.516) |  | 0 | Conversational question answering (CQA) is a more complicated task than traditional single-turn machine reading comprehension (MRC). Different from large language models (LLMs) like ChatGPT, the models of CQA need to extract answers from given contents to answer follow-up questions according to conversation history. In this paper, we propose a novel architecture, i.e., Gated Recurrent Memory Reader (GRMR), which integrates traditional extractive MRC models into a generalized... | Xuanyu Zhang, Qing Yang |  |
| 650 |  |  [Text2Tree: Aligning Text Representation to the Label Tree Hierarchy for Imbalanced Medical Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.517) |  | 0 | Deep learning approaches exhibit promising performances on various text tasks. However, they are still struggling on medical text classification since samples are often extremely imbalanced and scarce. Different from existing mainstream approaches that focus on supplementary semantics with external medical information, this paper aims to rethink the data challenges in medical texts and present a novel framework-agnostic algorithm called Text2Tree that only utilizes internal label hierarchy in... | Jiahuan Yan, Haojun Gao, Kai Zhang, Weize Liu, Danny Chen, Jian Wu, Jintai Chen |  |
| 651 |  |  [Impact of Co-occurrence on Factual Knowledge of Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.518) |  | 0 | Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and... | Cheongwoong Kang, Jaesik Choi |  |
| 652 |  |  [CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.519) |  | 0 | Large language models have demonstrated the capability to perform on machine translation when the input is prompted with a few examples (in-context learning). Translation quality depends on various features of the selected examples, such as their quality and relevance, but previous work has predominantly focused on individual features in isolation. In this paper, we propose a general framework for combining different features influencing example selection. We learn a regression model, CTQ... | Aswanth M., Ratish Puduppully, Raj Dabre, Anoop Kunchukuttan |  |
| 653 |  |  [Swap and Predict - Predicting the Semantic Changes in Words across Corpora by Context Swapping](https://doi.org/10.18653/v1/2023.findings-emnlp.520) |  | 0 | Meanings of words change over time and across domains. Detecting the semantic changes of words is an important task for various NLP applications that must make time-sensitive predictions. We consider the problem of predicting whether a given target word, w, changes its meaning between two different text corpora, 𝒞1 and 𝒞2. For this purpose, we propose Swapping-based Semantic Change Detection (SSCD), an unsupervised method that randomly swaps contexts between 𝒞1 and 𝒞2 where w occurs. We then... | Taichi Aida, Danushka Bollegala |  |
| 654 |  |  [Beyond Layout Embedding: Layout Attention with Gaussian Biases for Structured Document Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.521) |  | 0 | Effectively encoding layout information is a central problem in structured document understanding. Most existing methods rely heavily on millions of trainable parameters to learn the layout features of each word from Cartesian coordinates. However, two unresolved questions remain: (1) Is the Cartesian coordinate system the optimal choice for layout modeling? (2) Are massive learnable parameters truly necessary for layout representation? In this paper, we address these questions by proposing... | Xi Zhu, Xue Han, Shuyuan Peng, Shuo Lei, Chao Deng, Junlan Feng |  |
| 655 |  |  [ESPVR: Entity Spans Position Visual Regions for Multimodal Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.522) |  | 0 | Multimodal Named Entity Recognition (MNER) uses visual information to improve the performance of text-only Named Entity Recognition (NER). However, existing methods for acquiring local visual information suffer from certain limitations: (1) using an attention-based method to extract visual regions related to the text from visual regions obtained through convolutional architectures (e.g., ResNet), attention is distracted by the entire image, rather than being fully focused on the visual regions... | Xiujiao Li, Guanglu Sun, Xinyu Liu |  |
| 656 |  |  [Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency](https://doi.org/10.18653/v1/2023.findings-emnlp.523) |  | 0 | With growing capabilities of large language models, prompting them has become the dominant way to access them. This has motivated the development of strategies for automatically selecting effective language prompts. In this paper, we introduce \*\*pFlat\*\* (prompt flatness), a new metric to quantify the expected utility of a language prompt. This metric is inspired by \*flatness\* regularization in statistical learning that quantifies the robustness of the model towards its parameter... | Lingfeng Shen, Weiting Tan, Boyuan Zheng, Daniel Khashabi |  |
| 657 |  |  [Detecting Erroneously Recognized Handwritten Byzantine Text](https://doi.org/10.18653/v1/2023.findings-emnlp.524) |  | 0 | Handwritten text recognition (HTR) yields textual output that comprises errors, which are considerably more compared to that of recognised printed (OCRed) text. Post-correcting methods can eliminate such errors but may also introduce errors. In this study, we investigate the issues arising from this reality in Byzantine Greek. We investigate the properties of the texts that lead post-correction systems to this adversarial behaviour and we experiment with text classification systems that learn... | John Pavlopoulos, Vasiliki Kougia, Paraskevi Platanou, Holger Essler |  |
| 658 |  |  [Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment](https://doi.org/10.18653/v1/2023.findings-emnlp.525) |  | 0 | Pretrained language models (PLMs) based knowledge-grounded dialogue systems are prone to generate responses that are factually inconsistent with the provided knowledge source. In such inconsistent responses, the dialogue models fail to accurately express the external factual knowledge they rely upon. Inspired by previous work which identified that feedforward networks (FFNs) within Transformers are responsible for factual knowledge expressions, we investigate two methods to efficiently improve... | Boyang Xue, Weichao Wang, Hongru Wang, Fei Mi, Rui Wang, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, KamFai Wong |  |
| 659 |  |  [TRIP: Accelerating Document-level Multilingual Pre-training via Triangular Document-level Pre-training on Parallel Data Triplets](https://doi.org/10.18653/v1/2023.findings-emnlp.526) |  | 0 | Despite the success of multilingual sequence-to-sequence pre-training, most existing approaches rely on document-level monolingual corpora in many different languages, sentence-level bilingual corpora, and sometimes synthetic document-level bilingual corpora. This hampers the performance with cross-lingual document-level tasks such as document-level translation. Hence, we propose to mine and leverage document-level trilingual parallel corpora to improve sequence-to-sequence multilingual... | Hongyuan Lu, Haoyang Huang, Shuming Ma, Dongdong Zhang, Wai Lam, Zhaochuan Gao, Anthony Aue, Arul Menezes, Furu Wei |  |
| 660 |  |  [Frequency Balanced Datasets Lead to Better Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.527) |  | 0 | This paper reports on the experiments aimed to improve our understanding of the role of the amount of data required for training attention-based transformer language models. Specifically, we investigate the impact of reducing the immense amounts of required pre-training data through sampling strategies that identify and reduce high-frequency tokens as different studies have indicated that the existence of very high-frequency tokens in pre-training data might bias learning, causing undesired... | Rodolfo Zevallos, Mireia Farrús, Núria Bel |  |
| 661 |  |  [Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.528) |  | 0 | The recent success of large pre-trained language models (PLMs) heavily hinges on massive labeled data, which typically produces inferior performance in low-resource scenarios. To remedy this dilemma, we study self-training as one of the predominant semi-supervised learning (SSL) approaches, which utilizes large-scale unlabeled data to generate synthetic examples. However, too many noisy labels will hurt the model performance, and the self-training procedure requires multiple training iterations... | Jianing Wang, Qiushi Sun, Nuo Chen, Chengyu Wang, Jun Huang, Ming Gao, Xiang Li |  |
| 662 |  |  [TR-Rules: Rule-based Model for Link Forecasting on Temporal Knowledge Graph Considering Temporal Redundancy](https://doi.org/10.18653/v1/2023.findings-emnlp.529) |  | 0 | Temporal knowledge graph (TKG) has been proved to be an effective way for modeling dynamic facts in real world. Many efforts have been devoted into predicting future events i.e. extrapolation, on TKGs. Recently, rule-based knowledge graph completion methods which are considered to be more interpretable than embedding-based methods, have been transferred to temporal knowledge graph extrapolation. However, rule-based models suffer from temporal redundancy when leveraged under dynamic settings,... | Ningyuan Li, Haihong E, Shi Li, Mingzhi Sun, Tianyu Yao, Meina Song, Yong Wang, Haoran Luo |  |
| 663 |  |  [On the Transferability of Visually Grounded PCFGs](https://doi.org/10.18653/v1/2023.findings-emnlp.530) |  | 0 | There has been a significant surge of interest in visually grounded grammar induction in recent times. While a variety of models have been developed for the task and have demonstrated impressive performance, they have not been evaluated on text domains that are different from the training domain, so it is unclear if the improvements brought by visual groundings are transferable. Our study aims to fill this gap and assess the degree of transferability. We start by extending VC-PCFG (short for... | Yanpeng Zhao, Ivan Titov |  |
| 664 |  |  [Analysis of Style-Shifting on Social Media: Using Neural Language Model Conditioned by Social Meanings](https://doi.org/10.18653/v1/2023.findings-emnlp.531) |  | 0 | In this paper, we propose a novel framework for evaluating style-shifting in social media conversations. Our proposed framework captures changes in an individual’s conversational style based on surprisals predicted by a personalized neural language model for individuals. Our personalized language model integrates not only the linguistic contents of conversations but also non-linguistic factors, such as social meanings, including group membership, personal attributes, and individual beliefs. We... | Seiya Kawano, Shota Kanezaki, Angel Fernando Garcia Contreras, Akishige Yuguchi, Marie Katsurai, Koichiro Yoshino |  |
| 665 |  |  [Linguistic Compression in Single-Sentence Human-Written Summaries](https://doi.org/10.18653/v1/2023.findings-emnlp.532) |  | 0 | Summarizing texts involves significant cognitive efforts to compress information. While advances in automatic summarization systems have drawn attention from the NLP and linguistics communities to this topic, there is a lack of computational studies of linguistic patterns in human-written summaries. This work presents a large-scale corpus study of human-written single-sentence summaries. We analyzed the linguistic compression patterns from source documents to summaries at different... | Fangcong Yin, Marten van Schijndel |  |
| 666 |  |  [MCLF: A Multi-grained Contrastive Learning Framework for ASR-robust Spoken Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.533) |  | 0 | Enhancing the robustness towards Automatic Speech Recognition (ASR) errors is of great importance for Spoken Language Understanding (SLU). Trending ASR-robust SLU systems have witnessed impressive improvements through global contrastive learning. However, although most ASR errors occur only at local positions of utterances, they can easily lead to severe semantic changes, and utterance-level classification or comparison is difficult to distinguish such differences. To address the problem, we... | Zhiqi Huang, Dongsheng Chen, Zhihong Zhu, Xuxin Cheng |  |
| 667 |  |  [Beyond Candidates : Adaptive Dialogue Agent Utilizing Persona and Knowledge](https://doi.org/10.18653/v1/2023.findings-emnlp.534) |  | 0 | To build ultimate dialogue agents, previous studies suggest models that ground both persona and knowledge. However, applying the dialogue system directly to the usual conversation is still limited because the system requires a complete sentence-formed persona and knowledge candidate sets from the given dataset. In contrast to the dialogue setting in the dataset, humans utilize semantic concepts in their minds rather than a set of pre-defined candidate sentences. Following this manner of human... | Jungwoo Lim, Myunghoon Kang, Jinsung Kim, Jeongwook Kim, Yuna Hur, Heuiseok Lim |  |
| 668 |  |  [SmartSpanNER: Making SpanNER Robust in Low Resource Scenarios](https://doi.org/10.18653/v1/2023.findings-emnlp.535) |  | 0 | Named Entity Recognition (NER) is one of the most fundamental tasks in natural language processing. Span-level prediction (SpanNER) is more naturally suitable for nested NER than sequence labeling (SeqLab). However, according to our experiments, the SpanNER method is more sensitive to the amount of training data, i.e., the F1 score of SpanNER drops much more than that of SeqLab when the amount of training data drops. In order to improve the robustness of SpanNER in low resource scenarios, we... | Min Zhang, Xiaosong Qiao, Yanqing Zhao, Shimin Tao, Hao Yang |  |
| 669 |  |  [ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.536) |  | 0 | We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test and small validation sets, without training data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms... | Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, Omer Levy |  |
| 670 |  |  [Data Selection Curriculum for Abstractive Text Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.537) |  | 0 | Abstractive Text Summarization (ATS) models are commonly trained using large-scale data that is randomly shuffled. However, the impact of data selection and data ordering on ATS models remains a relatively unexplored research area, where a significant challenge lies in accurately assessing the learning difficulty of each training instance. This study introduces a Data Selection Curriculum (DSC) scoring system that incorporates both the difficulty of improving ATS model via an instance and the... | Shichao Sun, Ruifeng Yuan, Jianfei He, Ziqiang Cao, Wenjie Li, Xiaohua Jia |  |
| 671 |  |  [Romanization-based Large-scale Adaptation of Multilingual Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.538) |  | 0 | Large multilingual pretrained language models (mPLMs) have become the de facto state of the art for cross-lingual transfer in NLP. However, their large-scale deployment to many languages, besides pretraining data scarcity, is also hindered by the increase in vocabulary size and limitations in their parameter budget. In order to boost the capacity of mPLMs to deal with low-resource and unseen languages, we explore the potential of leveraging transliteration on a massive scale. In particular, we... | Sukannya Purkayastha, Sebastian Ruder, Jonas Pfeiffer, Iryna Gurevych, Ivan Vulic |  |
| 672 |  |  [Measuring bias in Instruction-Following models with P-AT](https://doi.org/10.18653/v1/2023.findings-emnlp.539) |  | 0 | Instruction-Following Language Models (IFLMs) are promising and versatile tools for solving many downstream, information-seeking tasks. Given their success, there is an urgent need to have a shared resource to determine whether existing and new IFLMs are prone to produce biased language interactions. In this paper, we propose Prompt Association Test (P-AT): a new resource for testing the presence of social biases in IFLMs. P-AT stems from WEAT (Caliskan et al., 2017) and generalizes the notion... | Dario Onorati, Elena Sofia Ruzzetti, Davide Venditti, Leonardo Ranaldi, Fabio Massimo Zanzotto |  |
| 673 |  |  [Open-ended Commonsense Reasoning with Unrestricted Answer Candidates](https://doi.org/10.18653/v1/2023.findings-emnlp.540) |  | 0 | Open-ended Commonsense Reasoning is defined as solving a commonsense question without providing 1) a short list of answer candidates and 2) a pre-defined answer scope. Conventional ways of formulating the commonsense question into a question-answering form or utilizing external knowledge to learn retrieval-based methods are less applicable in the open-ended setting due to an inherent challenge. Without pre-defining an answer scope or a few candidates, open-ended commonsense reasoning entails... | Chen Ling, Xuchao Zhang, Xujiang Zhao, Yanchi Liu, Wei Cheng, Mika Oishi, Takao Osaki, Katsushi Matsuda, Haifeng Chen, Liang Zhao |  |
| 674 |  |  [Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units](https://doi.org/10.18653/v1/2023.findings-emnlp.541) |  | 0 | We introduce DISSC, a novel, lightweight method that converts the rhythm, pitch contour and timbre of a recording to a target speaker in a textless manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on timbre, and ignore people’s unique speaking style (prosody). The proposed approach uses a pretrained, self-supervised model for encoding speech to discrete units, which makes it simple, effective, and fast to train. All conversion modules are only trained on reconstruction... | Gallil Maimon, Yossi Adi |  |
| 675 |  |  [Knowledge-Selective Pretraining for Attribute Value Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.542) |  | 0 | Attribute Value Extraction (AVE) aims to retrieve the values of attributes from the product profiles. The state-of-the-art methods tackle the AVE task through a question-answering (QA) paradigm, where the value is predicted from the context (i.e. product profile) given a query (i.e. attributes). Despite of the substantial advancements that have been made, the performance of existing methods on rare attributes is still far from satisfaction, and they cannot be easily extended to unseen... | Hui Liu, Qingyu Yin, Zhengyang Wang, Chenwei Zhang, Haoming Jiang, Yifan Gao, Zheng Li, Xian Li, Chao Zhang, Bing Yin, William Wang, Xiaodan Zhu |  |
| 676 |  |  [New Datasets and Controllable Iterative Data Augmentation Method for Code-switching ASR Error Correction](https://doi.org/10.18653/v1/2023.findings-emnlp.543) |  | 0 | With the wide use of automatic speech recognition(ASR) systems, researchers pay more attention to the ASR error correction task to improve the quality of recognition results. In particular, ASR in bilingual or multilingual settings, namely code-switching ASR, has greater challenges and research value. In this paper, we first present code-switching ASR correction datasets obtained from solid ASR systems and automatic annotators. The datasets contain Chinese-English code-switching dialogues of... | Zhaohong Wan, Xiaojun Wan, Wei Peng, Rongjun Li |  |
| 677 |  |  [Efficient k-NN Search with Cross-Encoders using Adaptive Multi-Round CUR Decomposition](https://doi.org/10.18653/v1/2023.findings-emnlp.544) |  | 0 | Cross-encoder models, which jointly encode and score a query-item pair, are prohibitively expensive for direct k-nearest neighbor (k-NN) search. Consequently, k-NN search typically employs a fast approximate retrieval (e.g. using BM25 or dual-encoder vectors), followed by reranking with a cross-encoder; however, the retrieval approximation often has detrimental recall regret. This problem is tackled by ANNCUR (Yadav et al., 2022), a recent work that employs a cross-encoder only, making search... | Nishant Yadav, Nicholas Monath, Manzil Zaheer, Andrew McCallum |  |
| 678 |  |  [Isotropic Representation Can Improve Zero-Shot Cross-Lingual Transfer on Multilingual Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.545) |  | 0 | With the development of multilingual pre-trained language models (mPLMs), zero-shot cross-lingual transfer shows great potential. To further improve the performance of cross-lingual transfer, many studies have explored representation misalignment caused by morphological differences but neglected the misalignment caused by the anisotropic distribution of contextual representations. In this work, we propose enhanced isotropy and constrained code-switching for zero-shot cross-lingual transfer to... | Yixin Ji, Jikai Wang, Juntao Li, Hai Ye, Min Zhang |  |
| 679 |  |  [Blackbird language matrices (BLM), a new task for rule-like generalization in neural networks: Can Large Language Models pass the test?](https://doi.org/10.18653/v1/2023.findings-emnlp.546) |  | 0 | How do we evaluate Large Language Models (LLMs) and determine the aspects and limits of their intelligent behaviour? It is currently conjectured that shortcomings of LLMs in multi-linguality and reasoning are due to a lack of ability to generalize. It has been argued that, instead, humans are better at generalization because they have a tendency at extracting rules from complex data. We propose a method to evaluate LLMs ability to rule-based generalization. When exposed to tests of analytic... | Paola Merlo |  |
| 680 |  |  [DistillCSE: Distilled Contrastive Learning for Sentence Embeddings](https://doi.org/10.18653/v1/2023.findings-emnlp.547) |  | 0 | This paper proposes the DistillCSE framework, which performs contrastive learning under the self-training paradigm with knowledge distillation. The potential advantage of DistillCSE is its self-enhancing feature: using a base model to provide additional supervision signals, a stronger model may be learned through knowledge distillation. However, the vanilla DistillCSE through the standard implementation of knowledge distillation only achieves marginal improvements. The quantitative analyses... | Jiahao Xu, Wei Shao, Lihui Chen, Lemao Liu |  |
| 681 |  |  [GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets](https://doi.org/10.18653/v1/2023.findings-emnlp.548) |  | 0 | Named Entity Recognition (NER) models play a crucial role in various NLP tasks, including information extraction (IE) and text understanding. In academic writing, references to machine learning models and datasets are fundamental components of various computer science publications and necessitate accurate models for identification. Despite the advancements in NER, existing ground truth datasets do not treat fine-grained types like ML model and model architecture as separate entity types, and... | Wolfgang Otto, Matthäus Zloch, Lu Gan, Saurav Karmakar, Stefan Dietze |  |
| 682 |  |  [Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.549) |  | 0 | Multi-document summarization (MDS) assumes a set of topic-related documents are provided as input. In practice, this document set is not always available; it would need to be retrieved given an information need, i.e. a question or topic statement, a setting we dub “open-domain’ MDS. We study this more challenging setting by formalizing the task and bootstrapping it using existing datasets, retrievers and summarizers. Via extensive automatic and human evaluation, we determine: (1)... | John M. Giorgi, Luca Soldaini, Bo Wang, Gary D. Bader, Kyle Lo, Lucy Lu Wang, Arman Cohan |  |
| 683 |  |  [Few-shot Unified Question Answering: Tuning Models or Prompts?](https://doi.org/10.18653/v1/2023.findings-emnlp.550) |  | 0 | Question-answering (QA) tasks often investigate specific question types, knowledge domains, or reasoning skills, leading to specialized models catering to specific categories of QA tasks. While recent research has explored the idea of unified QA models, such models are usually explored for high-resource scenarios and require re-training to extend their capabilities. To overcome these drawbacks, the paper explores the potential of two paradigms of tuning, model, and prompts, for unified QA under... | Srijan Bansal, Semih Yavuz, Bo Pang, Meghana Bhat, Yingbo Zhou |  |
| 684 |  |  [Finding Common Ground: Annotating and Predicting Common Ground in Spoken Conversations](https://doi.org/10.18653/v1/2023.findings-emnlp.551) |  | 0 | When we communicate with other humans, we do not simply generate a sequence of words. Rather, we use our cognitive state (beliefs, desires, intentions) and our model of the audience’s cognitive state to create utterances that affect the audience’s cognitive state in the intended manner. An important part of cognitive state is the common ground, which is the content the speaker believes, and the speaker believes the audience believes, and so on. While much attention has been paid to common... | Magdalena Markowska, Mohammad Taghizadeh, Adil Soubki, Seyed Abolghasem Mirroshandel, Owen Rambow |  |
| 685 |  |  [Getting MoRE out of Mixture of Language Model Reasoning Experts](https://doi.org/10.18653/v1/2023.findings-emnlp.552) |  | 0 | While recent large language models (LLMs) improve on various question answering (QA) datasets, it remains difficult for a single model to generalize across question types that require distinct reasoning abilities. We provide empirical evidence that state-of-the-art LLMs suffer from poor generalizability on reasoning types beyond those seen in the prompt. To remedy this, we propose a Mixture-of-Reasoning-Experts (MORE) framework that ensembles diverse specialized language models. We specialize... | Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, Jordan L. BoydGraber |  |
| 686 |  |  ["You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract Meaning Representation](https://doi.org/10.18653/v1/2023.findings-emnlp.553) |  | 0 | Large language models (LLMs) demonstrate an amazing proficiency and fluency in the use of language. Does that mean that they have also acquired insightful linguistic knowledge about the language, to an extent that they can serve as an “expert linguistic annotator’? In this paper, we examine the successes and limitations of the GPT-3, ChatGPT, and GPT-4 models, focusing on the Abstract Meaning Representation (AMR) parsing formalism (Banarescu et al., 2013), which provides rich graphical... | Allyson Ettinger, Jena D. Hwang, Valentina Pyatkin, Chandra Bhagavatula, Yejin Choi |  |
| 687 |  |  [Zero-Shot Data Maps. Efficient Dataset Cartography Without Model Training](https://doi.org/10.18653/v1/2023.findings-emnlp.554) |  | 0 | Data Maps (Swayamdipta, et al. 2020) have emerged as a powerful tool for diagnosing large annotated datasets. Given a model fitted on a dataset, these maps show each data instance from the dataset in a 2-dimensional space defined by a) the model’s confidence in the true class and b) the variability of this confidence. In previous work, confidence and variability are usually computed using training dynamics, which requires the fitting of a strong model to the dataset. In this paper, we introduce... | Angelo Basile, Marc FrancoSalvador, Paolo Rosso |  |
| 688 |  |  [Isotropy-Enhanced Conditional Masked Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.555) |  | 0 | Non-autoregressive models have been widely used for various text generation tasks to accelerate the inference process but at the cost of generation quality to some extent. To achieve a good balance between inference speedup and generation quality, iterative NAR models like CMLM and Disco are proposed. Researchers have made much follow-up progress based on them, and some recent iterative models can achieve very promising performance while maintaining significant speedup. In this paper, we give... | Pei Guo, Yisheng Xiao, Juntao Li, Yixin Ji, Min Zhang |  |
| 689 |  |  [Scaling Law for Document Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.556) |  | 0 | The scaling laws of language models have played a significant role in advancing large language models. In order to promote the development of document translation, we systematically examine the scaling laws in this field. In this paper, we carry out an in-depth analysis of the influence of three factors on translation quality: model scale, data scale, and sequence length. Our findings reveal that increasing sequence length effectively enhances model performance when model size is limited.... | Zhuocheng Zhang, Shuhao Gu, Min Zhang, Yang Feng |  |
| 690 |  |  [Automatic Pronunciation Assessment - A Review](https://doi.org/10.18653/v1/2023.findings-emnlp.557) |  | 0 | Pronunciation assessment and its application in computer-aided pronunciation training (CAPT) have seen impressive progress in recent years. With the rapid growth in language processing and deep learning over the past few years, there is a need for an updated review. In this paper, we review methods employed in pronunciation assessment for both phonemic and prosodic. We categorize the main challenges observed in prominent research trends, and highlight existing limitations, and available... | Yassine El Kheir, Ahmed Ali, Shammur Absar Chowdhury |  |
| 691 |  |  [Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model](https://doi.org/10.18653/v1/2023.findings-emnlp.558) |  | 0 | Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments and apply attention to the individual segments. We propose a segmented recurrent transformer (SRformer) that combines segmented (local) attention with recurrent attention.... | Yinghan Long, Sayeed Shafayet Chowdhury, Kaushik Roy |  |
| 692 |  |  [PUNR: Pre-training with User Behavior Modeling for News Recommendation](https://doi.org/10.18653/v1/2023.findings-emnlp.559) |  | 0 | News recommendation aims to predict click behaviors based on user behaviors. How to effectively model the user representations is the key to recommending preferred news. Existing works are mostly focused on improvements in the supervised fine-tuning stage. However, there is still a lack of PLM-based unsupervised pre-training methods optimized for user representations. In this work, we propose an unsupervised pre-training paradigm with two tasks, i.e. user behavior masking and user behavior... | Guangyuan Ma, Hongtao Liu, Xing Wu, Wanhui Qian, Zhepeng Lv, Qing Yang, Songlin Hu |  |
| 693 |  |  [Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design](https://doi.org/10.18653/v1/2023.findings-emnlp.560) |  | 0 | Discovering novel catalysts requires complex reasoning involving multiple chemical properties and resultant trade-offs, leading to a combinatorial growth in the search space. While large language models (LLM) have demonstrated novel capabilities for chemistry through complex instruction following capabilities and high quality reasoning, a goal-driven combinatorial search using LLMs has not been explored in detail. In this work, we present a Monte Carlo Tree Search-based approach that improves... | Henry Sprueill, Carl Edwards, Mariefel V. Olarte, Udishnu Sanyal, Heng Ji, Sutanay Choudhury |  |
| 694 |  |  [Measure Children's Mindreading Ability with Machine Reading](https://doi.org/10.18653/v1/2023.findings-emnlp.561) |  | 0 | Recently, much research in psychology has benefited from the advances in machine learning techniques. Some recent studies showed that it is possible to build automated scoring models for children’s mindreading. These models were trained on a set of manually-labeled question-response pairs, which were collected by asking children to answer one or two questions after a short story is told or a video clip is played. However, existing models did not take the features of the stories and video clips... | Yuliang Yan, Xiaohua Wang, Xiang Zhou, Xiaoqing Zheng, Xuanjing Huang |  |
| 695 |  |  [Crosslingual Transfer Learning for Low-Resource Languages Based on Multilingual Colexification Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.562) |  | 0 | In comparative linguistics, colexification refers to the phenomenon of a lexical form conveying two or more distinct meanings. Existing work on colexification patterns relies on annotated word lists, limiting scalability and usefulness in NLP. In contrast, we identify colexification patterns of more than 2,000 concepts across 1,335 languages directly from an unannotated parallel corpus. We then propose simple and effective methods to build multilingual graphs from the colexification patterns:... | Yihong Liu, Haotian Ye, Leonie Weissweiler, Renhao Pei, Hinrich Schütze |  |
| 696 |  |  [Injecting structural hints: Using language models to study inductive biases in language learning](https://doi.org/10.18653/v1/2023.findings-emnlp.563) |  | 0 | Both humans and transformer language models are able to learn language without explicit structural supervision. What cognitive inductive biases make this learning possible? Here, we examine the effect of different inductive learning biases by actively controlling the inductive biases of artificial learners: we structurally bias models by pretraining on synthetic formally-structured data, and evaluate these structural biases by fine-tuning on three typologically-distant human languages: English,... | Isabel Papadimitriou, Dan Jurafsky |  |
| 697 |  |  [Machine Reading Comprehension using Case-based Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.564) |  | 0 | We present an accurate and interpretable method for answer extraction in machine reading comprehension that is reminiscent of case-based reasoning (CBR) from classical AI. Our method (CBR-MRC) builds upon the hypothesis that contextualized answers to similar questions share semantic similarities with each other. Given a test question, CBR-MRC first retrieves a set of similar cases from a nonparametric memory and then predicts an answer by selecting the span in the test context that is most... | Dung Thai, Dhruv Agarwal, Mudit Chaudhary, Wenlong Zhao, Rajarshi Das, JayYoon Lee, Hannaneh Hajishirzi, Manzil Zaheer, Andrew McCallum |  |
| 698 |  |  [Unleashing the Power of Language Models in Text-Attributed Graph](https://doi.org/10.18653/v1/2023.findings-emnlp.565) |  | 0 | Representation learning on graph has been demonstrated to be a powerful tool for solving real-world problems. Text-attributed graph carries both semantic and structural information among different types of graphs. Existing works have paved the way for knowledge extraction of this type of data by leveraging language models or graph neural networks or combination of them. However, these works suffer from issues like underutilization of relationships between nodes or words or unaffordable memory... | Haoyu Kuang, Jiarong Xu, Haozhe Zhang, Zuyu Zhao, Qi Zhang, Xuanjing Huang, Zhongyu Wei |  |
| 699 |  |  [Locally Differentially Private Document Generation Using Zero Shot Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.566) |  | 0 | Numerous studies have highlighted the privacy risks associated with large language models. Our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When... | Saiteja Utpala, Sara Hooker, PinYu Chen |  |
| 700 |  |  [Contrastive Deterministic Autoencoders For Language Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.567) |  | 0 | Variational autoencoders (VAEs) are a popular family of generative models with wide applicability. Training VAEs, especially for text, often runs into the issue of posterior collapse, resulting in loss of representation quality. Deterministic autoencoders avoid this issue, and have been explored particularly well for images. It is however unclear how to best modify a deterministic model designed for images into a successful one for text. We show that with suitable adaptations, we can... | Amur Ghose, Pascal Poupart |  |
| 701 |  |  [CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.568) |  | 0 | We propose CHiLL (Crafting High-Level Latents), an approach for natural-language specification of features for linear models. CHiLL prompts LLMs with expert-crafted queries to generate interpretable features from health records. The resulting noisy labels are then used to train a simple linear classifier. Generating features based on queries to an LLM can empower physicians to use their domain expertise to craft features that are clinically meaningful for a downstream task of interest, without... | Denis Jered McInerney, Geoffrey S. Young, JanWillem van de Meent, Byron C. Wallace |  |
| 702 |  |  [Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers](https://doi.org/10.18653/v1/2023.findings-emnlp.569) |  | 0 | Recent applications of LLMs in Machine Reading Comprehension (MRC) systems have shown impressive results, but the use of shortcuts, mechanisms triggered by features spuriously correlated to the true label, has emerged as a potential threat to their reliability. We analyze the problem from two angles: LLMs as editors, guided to edit text to mislead LLMs; and LLMs as readers, who answer questions based on the edited text. We introduce a framework that guides an editor to add potential... | Mosh Levy, Shauli Ravfogel, Yoav Goldberg |  |
| 703 |  |  [Large Language Models Meet Harry Potter: A Dataset for Aligning Dialogue Agents with Characters](https://doi.org/10.18653/v1/2023.findings-emnlp.570) |  | 0 | In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT and GPT4 have demonstrated immense potential in constructing open-domain dialogue agents. However, aligning these agents with specific characters or individuals remains a considerable challenge due to the complexities of character representation and the lack of comprehensive annotations. In this paper, we introduce the Harry Potter Dialogue (HPD) dataset, designed to advance the study of dialogue agents and character... | Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan Li, Ziyang Chen, Longyue Wang, Jia Li |  |
| 704 |  |  [Quick Back-Translation for Unsupervised Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.571) |  | 0 | The field of unsupervised machine translation has seen significant advancement from the marriage of the Transformer and the back-translation algorithm. The Transformer is a powerful generative model, and back-translation leverages Transformer’s high-quality translations for iterative self-improvement. However, the Transformer is encumbered by the run-time of autoregressive inference during back-translation, and back-translation is limited by a lack of synthetic data efficiency. We propose a... | Benjamin Brimacombe, Jiawei Zhou |  |
| 705 |  |  [SIR-ABSC: Incorporating Syntax into RoBERTa-based Sentiment Analysis Models with a Special Aggregator Token](https://doi.org/10.18653/v1/2023.findings-emnlp.572) |  | 0 | We present a simple, but effective method to incorporate syntactic dependency information directly into transformer-based language models (e.g. RoBERTa) for tasks such as Aspect-Based Sentiment Classification (ABSC), where the desired output depends on specific input tokens. In contrast to prior approaches to ABSC that capture syntax by combining language models with graph neural networks over dependency trees, our model, Syntax-Integrated RoBERTa for ABSC (SIR-ABSC) incorporates syntax... | Ikhyun Cho, Yoonhwa Jung, Julia Hockenmaier |  |
| 706 |  |  [Citance-Contextualized Summarization of Scientific Papers](https://doi.org/10.18653/v1/2023.findings-emnlp.573) |  | 0 | Current approaches to automatic summarization of scientific papers generate informative summaries in the form of abstracts. However, abstracts are not intended to show the relationship between a paper and the references cited in it. We propose a new contextualized summarization approach that can generate an informative summary conditioned on a given sentence containing the citation of a reference (a so-called “citance”). This summary outlines content of the cited paper relevant to the citation... | Shahbaz Syed, Ahmad Dawar Hakimi, Khalid Al Khatib, Martin Potthast |  |
| 707 |  |  [SegAugment: Maximizing the Utility of Speech Translation Data with Segmentation-based Augmentations](https://doi.org/10.18653/v1/2023.findings-emnlp.574) |  | 0 | End-to-end Speech Translation is hindered by a lack of available data resources. While most of them are based on documents, a sentence-level version is available, which is however single and static, potentially impeding the usefulness of the data. We propose a new data augmentation strategy, SegAugment, to address this issue by generating multiple alternative sentence-level versions of a dataset. Our method utilizes an Audio Segmentation system, which re-segments the speech of each document... | Ioannis Tsiamas, José A. R. Fonollosa, Marta R. Costajussà |  |
| 708 |  |  [Intersectional Stereotypes in Large Language Models: Dataset and Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.575) |  | 0 | Despite many stereotypes targeting intersectional demographic groups, prior studies on stereotypes within Large Language Models (LLMs) primarily focus on broader, individual categories. This research bridges this gap by introducing a novel dataset of intersectional stereotypes, curated with the assistance of the ChatGPT model and manually validated. Moreover, this paper offers a comprehensive analysis of intersectional stereotype propagation in three contemporary LLMs by leveraging this... | Weicheng Ma, Brian Chiang, Tong Wu, Lili Wang, Soroush Vosoughi |  |
| 709 |  |  [Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond](https://doi.org/10.18653/v1/2023.findings-emnlp.576) |  | 0 | Vision-language (VL) understanding tasks evaluate models’ comprehension of complex visual scenes through multiple-choice questions. However, we have identified two dataset biases that models can exploit as shortcuts to resolve various VL tasks correctly without proper understanding. The first type of dataset bias is Unbalanced Matching bias, where the correct answer overlaps the question and image more than the incorrect answers. The second type of dataset bias is Distractor Similarity bias,... | Zhecan Wang, Long Chen, Haoxuan You, Keyang Xu, Yicheng He, Wenhao Li, Noel Codella, KaiWei Chang, ShihFu Chang |  |
| 710 |  |  [The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who](https://doi.org/10.18653/v1/2023.findings-emnlp.577) |  | 0 | Automated fact-checking is often presented as an epistemic tool that fact-checkers, social media consumers, and other stakeholders can use to fight misinformation. Nevertheless, few papers thoroughly discuss how. We document this by analysing 100 highly-cited papers, and annotating epistemic elements related to intended use, i.e., means, ends, and stakeholders. We find that narratives leaving out some of these aspects are common, that many papers propose inconsistent means and ends, and that... | Michael Sejr Schlichtkrull, Nedjma Ousidhoum, Andreas Vlachos |  |
| 711 |  |  [Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression](https://doi.org/10.18653/v1/2023.findings-emnlp.578) |  | 0 | Large-scale pre-trained language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, the massive size of these models poses huge challenges for their deployment in real-world applications. While numerous model compression techniques have been proposed, most of them are not well-suited for achieving extreme model compression when there is a significant gap in model scale. In this paper, we introduce a novel compression paradigm... | Jiduan Liu, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Dongyan Zhao, Ran Wang, Rui Yan |  |
| 712 |  |  [COUNT: COntrastive UNlikelihood Text Style Transfer for Text Detoxification](https://doi.org/10.18653/v1/2023.findings-emnlp.579) |  | 0 | Offensive and toxic text on social media platforms can lead to polarization and divisiveness within online communities and hinders constructive dialogue. Text detoxification is a crucial task in natural language processing to ensure the generation of non-toxic and safe text. Text detoxification is a special case of the Text Style Transfer (TST) problem, where an input text is rephrased to an output text that preserves its content while modifying the style (in this case to a more neutral,... | Mohammad Mahdi Abdollah Pour, Parsa Farinneya, Manasa Bharadwaj, Nikhil Verma, Ali Pesaranghader, Scott Sanner |  |
| 713 |  |  [KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.findings-emnlp.580) |  | 0 | Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC and they can be categorized into two main classes, including triple-based and test-based approaches. Triple-based methods struggle with long-tail entities due to limited structural information and imbalanced distributions of entities. Text-based methods alleviate this issue but require costly training for language models and... | Yanbin Wei, Qiushi Huang, Yu Zhang, James T. Kwok |  |
| 714 |  |  [Show, Write, and Retrieve: Entity-aware Article Generation and Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.581) |  | 0 | Article comprehension is an important challenge in natural language processing with many applications such as article generation or image-to-article retrieval. Prior work typically encodes all tokens in articles uniformly using pretrained language models. However, in many applications, such as understanding news stories, these articles are based on real-world events and may reference many named entities that are difficult to accurately recognize and predict by language models. To address this... | Zhongping Zhang, Yiwen Gu, Bryan A. Plummer |  |
| 715 |  |  [A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing](https://doi.org/10.18653/v1/2023.findings-emnlp.582) |  | 0 | Two of the central factors believed to underpin human sentence processing difficulty are expectations and retrieval from working memory. A recent attempt to create a unified cognitive model integrating these two factors have relied on the parallels between the self-attention mechanism of transformer language models and cue-based retrieval theories of working memory in human sentence processing (Ryu and Lewis 2021). While the authors show that attention patterns in specialized attention heads of... | William Timkey, Tal Linzen |  |
| 716 |  |  [Annotations Are Not All You Need: A Cross-modal Knowledge Transfer Network for Unsupervised Temporal Sentence Grounding](https://doi.org/10.18653/v1/2023.findings-emnlp.583) |  | 0 | This paper addresses the task of temporal sentence grounding (TSG). Although many respectable works have made decent achievements in this important topic, they severely rely on massive expensive video-query paired annotations, which require a tremendous amount of human effort to collect in real-world applications. To this end, in this paper, we target a more practical but challenging TSG setting: unsupervised temporal sentence grounding, where both paired video-query and segment boundary... | Xiang Fang, Daizong Liu, Wanlong Fang, Pan Zhou, Yu Cheng, Keke Tang, Kai Zou |  |
| 717 |  |  [Parameter Efficient Multi-task Fine-tuning by Learning to Transfer Token-wise Prompts](https://doi.org/10.18653/v1/2023.findings-emnlp.584) |  | 0 | Prompt tuning has been proven to be successful on various tasks by incorporating a small number of trainable parameters while freezing large pre-trained language models (PLMs). However, it is still unsettled how to generate more proper prompts for any individual examples and how to extend prompt tuning to multi-task learning scenarios by leveraging cross-task features. To address these challenges, we propose a token-wise prompt tuning (TPT), in which a bank of finer-grained soft prompt tokens... | Muling Wu, Wenhao Liu, Jianhan Xu, Changze Lv, Zixuan Ling, Tianlong Li, Longtao Huang, Xiaoqing Zheng, Xuanjing Huang |  |
| 718 |  |  [A Rewriting Approach for Gender Inclusivity in Portuguese](https://doi.org/10.18653/v1/2023.findings-emnlp.585) |  | 0 | In recent years, there has been a notable rise in research interest regarding the integration of gender-inclusive and gender-neutral language in natural language processing models. A specific area of focus that has gained practical and academic significant interest is gender-neutral rewriting, which involves converting binary-gendered text to its gender-neutral counterpart. However, current approaches to gender-neutral rewriting for gendered languages tend to rely on large datasets, which may... | Leonor Veloso, Luísa Coheur, Rui Ribeiro |  |
| 719 |  |  [EARA: Improving Biomedical Semantic Textual Similarity with Entity-Aligned Attention and Retrieval Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.586) |  | 0 | Measuring Semantic Textual Similarity (STS) is a fundamental task in biomedical text processing, which aims at quantifying the similarity between two input biomedical sentences. Unfortunately, the STS datasets in the biomedical domain are relatively smaller but more complex in semantics than common domain, often leading to overfitting issues and insufficient text representation even based on Pre-trained Language Models (PLMs) due to too many biomedical entities. In this paper, we propose EARA,... | Ying Xiong, Xin Yang, Linjing Liu, KaChun Wong, Qingcai Chen, Yang Xiang, Buzhou Tang |  |
| 720 |  |  [Neuro-Symbolic Sentiment Analysis with Dynamic Word Sense Disambiguation](https://doi.org/10.18653/v1/2023.findings-emnlp.587) |  | 0 | Sentiment analysis is a task that highly depends on the understanding of word senses. Traditional neural network models are black boxes that represent word senses as vectors that are uninterpretable for humans. On the other hand, the application of Word Sense Disambiguation (WSD) systems in downstream tasks poses challenges regarding i) which words need to be disambiguated, and ii) how to model explicit word senses into easily understandable terms for a downstream model. This work proposes a... | Xulang Zhang, Rui Mao, Kai He, Erik Cambria |  |
| 721 |  |  [Role of Context in Unsupervised Sentence Representation Learning: the Case of Dialog Act Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.588) |  | 0 | Unsupervised learning of word representations involves capturing the contextual information surrounding word occurrences, which can be grounded in the observation that word form is largely disconnected from word meaning. While there are fewer reasons to believe that the same holds for sentences, learning through context has been carried over to learning representations of word sequences. However, this work pays minimal to no attention to the role of context in inferring sentence... | Rastislav Hronsky, Emmanuel Keuleers |  |
| 722 |  |  [CLMSM: A Multi-Task Learning Framework for Pre-training on Procedural Text](https://doi.org/10.18653/v1/2023.findings-emnlp.589) |  | 0 | In this paper, we propose \*\*\*CLMSM\*\*\*, a domain-specific, continual pre-training framework, that learns from a large set of procedural recipes. \*\*\*CLMSM\*\*\* uses a Multi-Task Learning Framework to optimize two objectives - a) Contrastive Learning using hard triplets to learn fine-grained differences across entities in the procedures, and b) a novel Mask-Step Modelling objective to learn step-wise context of a procedure. We test the performance of \*\*\*CLMSM\*\*\* on the downstream... | Abhilash Nandy, Manav Nitin Kapadnis, Pawan Goyal, Niloy Ganguly |  |
| 723 |  |  [Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking](https://doi.org/10.18653/v1/2023.findings-emnlp.590) |  | 0 | In the field of information retrieval, Query Likelihood Models (QLMs) rank documents based on the probability of generating the query given the content of a document. Recently, advanced large language models (LLMs) have emerged as effective QLMs, showcasing promising ranking capabilities. This paper focuses on investigating the genuine zero-shot ranking effectiveness of recent LLMs, which are solely pre-trained on unstructured text data without supervised instruction fine-tuning. Our findings... | Shengyao Zhuang, Bing Liu, Bevan Koopman, Guido Zuccon |  |
| 724 |  |  [On General Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.591) |  | 0 | Natural Language Processing prides itself to be an empirically-minded, if not outright empiricist field, and yet lately it seems to get itself into essentialist debates on issues of meaning and measurement (“Do Large Language Models Understand Language, And If So, How Much?”). This is not by accident: Here, as everywhere, the evidence underspecifies the understanding. As a remedy, this paper sketches the outlines of a model of understanding, which can ground questions of the adequacy of current... | David Schlangen |  |
| 725 |  |  [USB: A Unified Summarization Benchmark Across Tasks and Domains](https://doi.org/10.18653/v1/2023.findings-emnlp.592) |  | 0 | While the NLP community has produced numerous summarization benchmarks, none provide the rich annotations required to simultaneously address many important problems related to control and reliability. We introduce a Wikipedia-derived benchmark, complemented by a rich set of crowd-sourced annotations, that supports 8 interrelated tasks: (i) extractive summarization; (ii) abstractive summarization; (iii) topic-based summarization; (iv) compressing selected sentences into a one-line summary; (v)... | Kundan Krishna, Prakhar Gupta, Sanjana Ramprasad, Byron C. Wallace, Jeffrey P. Bigham, Zachary C. Lipton |  |
| 726 |  |  [tagE: Enabling an Embodied Agent to Understand Human Instructions](https://doi.org/10.18653/v1/2023.findings-emnlp.593) |  | 0 | Natural language serves as the primary mode of communication when an intelligent agent with a physical presence engages with human beings. While a plethora of research focuses on natural language understanding (NLU), encompassing endeavors such as sentiment analysis, intent prediction, question answering, and summarization, the scope of NLU directed at situations necessitating tangible actions by an embodied agent remains limited. The inherent ambiguity and incompleteness inherent in natural... | Chayan Sarkar, Avik Mitra, Pradip Pramanick, Tapas Nayak |  |
| 727 |  |  [Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.594) |  | 0 | Hierarchical multi-label text classification (HMTC) aims at utilizing a label hierarchy in multi-label classification. Recent approaches to HMTC deal with the problem of imposing an overconstrained premise on the output space by using contrastive learning on generated samples in a semi-supervised manner to bring text and label embeddings closer. However, the generation of samples tends to introduce noise as it ignores the correlation between similar samples in the same batch. One solution to... | Simon Chi Lok U, Jie He, Víctor GutiérrezBasulto, Jeff Z. Pan |  |
| 728 |  |  [Uncovering Limitations in Text-to-Image Generation: A Contrastive Approach with Structured Semantic Alignment](https://doi.org/10.18653/v1/2023.findings-emnlp.595) |  | 0 | Despite significant advancements in text-to-image generation models, they still face challenges when it comes to producing highly detailed or complex images based on textual descriptions. In order to explore these limitations, we propose a Structured Semantic Alignment (SSA) method for evaluating text-to-image generation models. SSA focuses on learning structured semantic embeddings across different modalities and aligning them in a joint space. The method employs the following steps to achieve... | Qianyu Feng, Yulei Sui, Hongyu Zhang |  |
| 729 |  |  [An Intent-based and Annotation-free Method for Duplicate Question Detection in CQA Forums](https://doi.org/10.18653/v1/2023.findings-emnlp.596) |  | 0 | With the advent of large language models (LLMs), Community Question Answering (CQA) forums offer well-curated questions and answers that can be utilized for instruction-tuning, effectively training LLMs to be aligned with human intents. However, the issue of duplicate questions arises as the volume of content within CQA continues to grow, posing a threat to content quality. Recent research highlights the benefits of detecting and eliminating duplicate content. It not only enhances the LLMs’... | Yubo Shu, Hansu Gu, Peng Zhang, Tun Lu, Ning Gu |  |
| 730 |  |  [Accelerating Multiple Intent Detection and Slot Filling via Targeted Knowledge Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.597) |  | 0 | Recent non-autoregressive Spoken Language Understanding (SLU) models have attracted increasing attention because of their encouraging inference speed. However, most of existing methods (1) suffer from the multi-modality problem since they have little prior knowledge about the reference during inference; (2) fail to achieve a satisfactory inference speed limited by their complex frameworks. To tackle these issues, in this paper, we propose a Targeted Knowledge Distillation Framework (TKDF) for... | Xuxin Cheng, Zhihong Zhu, Wanshi Xu, Yaowei Li, Hongxiang Li, Yuexian Zou |  |
| 731 |  |  [Type-Aware Decomposed Framework for Few-Shot Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.598) |  | 0 | Despite the recent success achieved by several two-stage prototypical networks in few-shot named entity recognition (NER) task, the over-detected false spans at span detection stage and the inaccurate and unstable prototypes at type classification stage remain to be challenging problems. In this paper, we propose a novel Type-Aware Decomposed framework, namely TadNER, to solve these problems. We first present a type-aware span filtering strategy to filter out false spans by removing those... | Yongqi Li, Yu Yu, Tieyun Qian |  |
| 732 |  |  [A Closer Look into Using Large Language Models for Automatic Evaluation](https://doi.org/10.18653/v1/2023.findings-emnlp.599) |  | 0 | Using large language models (LLMs) to evaluate text quality has recently gained popularity. Some existing prior works explore the idea of using LLMs for evaluation, while they differ in some details of the evaluation process. In this paper, we analyze \*LLM evaluation\* and \*G-Eval\*, and we discuss how those details in the evaluation process change how well the ratings given by LLMs correlate with human ratings. We find that the auto Chain-of-Thought (CoT) used in G-Eval does not always make... | David ChengHan Chiang, Hungyi Lee |  |
| 733 |  |  [Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification using Graph Neural Networks?](https://doi.org/10.18653/v1/2023.findings-emnlp.600) |  | 0 | Given the success of Graph Neural Networks (GNNs) for structure-aware machine learning, many studies have explored their use for text classification, but mostly in specific domains with limited data characteristics. Moreover, some strategies prior to GNNs relied on graph mining and classical machine learning, making it difficult to assess their effectiveness in modern settings. This work extensively investigates graph representation methods for text classification, identifying practical... | Margarita Bugueño, Gerard de Melo |  |
| 734 |  |  [Natural Language Annotations for Reasoning about Program Semantics](https://doi.org/10.18653/v1/2023.findings-emnlp.601) |  | 0 | By grounding natural language inference in code (and vice versa), researchers aim to create programming assistants that explain their work, are “coachable” and can surface any gaps in their reasoning. Can we deduce automatically interesting properties of programs from their syntax and common-sense annotations alone, without resorting to static analysis? How much of program logic and behaviour can be captured in natural language? To stimulate research in this direction and attempt to answer... | Marco Zocca |  |
| 735 |  |  [Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate to Speech Emotion Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.602) |  | 0 | Previous work has established that a person’s demographics and speech style affect how well speech processing models perform for them. But where does this bias come from? In this work, we present the Speech Embedding Association Test (SpEAT), a method for detecting bias in one type of model used for many speech tasks: pre-trained models. The SpEAT is inspired by word embedding association tests in natural language processing, which quantify intrinsic bias in a model’s representations of... | Isaac Slaughter, Craig Greenberg, Reva Schwartz, Aylin Caliskan |  |
| 736 |  |  [Text Classification via Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.603) |  | 0 | Despite the remarkable success of large-scale Language Models (LLMs) such as GPT-3, their performances still significantly underperform fine-tuned models in the task of text classification.This is due to (1) the lack of reasoning ability in addressing complex linguistic phenomena (e.g., intensification, contrast, irony etc); (2) limited number of tokens allowed in in-context learning. In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts a progressive reasoning strategy... | Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, Guoyin Wang |  |
| 737 |  |  [On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.604) |  | 0 | Visually-rich document entity retrieval (VDER), which extracts key information (e.g. date, address) from document images like invoices and receipts, has become an important topic in industrial NLP applications. The emergence of new document types at a constant pace, each with its unique entity types, presents a unique challenge: many documents contain unseen entity types that occur only a couple of times. Addressing this challenge requires models to have the ability of learning entities in a... | Jiayi Chen, Hanjun Dai, Bo Dai, Aidong Zhang, Wei Wei |  |
| 738 |  |  [Semi-Structured Object Sequence Encoders](https://doi.org/10.18653/v1/2023.findings-emnlp.605) |  | 0 | In this paper we explore the task of modeling semi-structured object sequences; in particular, we focus our attention on the problem of developing a structure-aware input representation for such sequences. Examples of such data include user activity on websites, machine logs, and many others. This type of data is often represented as a sequence of sets of key-value pairs over time and can present modeling challenges due to an ever-increasing sequence length. We propose a two-part approach,... | Rudra Murthy V, Riyaz A. Bhat, R. Chulaka Gunasekara, Siva Sankalp Patel, Hui Wan, Tejas I. Dhamecha, Danish Contractor, Marina Danilevsky |  |
| 739 |  |  [DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM](https://doi.org/10.18653/v1/2023.findings-emnlp.606) |  | 0 | In the burgeoning field of natural language processing, Neural Topic Models (NTMs) and Large Language Models (LLMs) have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic generation. Our study addresses this gap by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based... | Weijie Xu, Wenxiang Hu, Fanyou Wu, Srinivasan H. Sengamedu |  |
| 740 |  |  [Energy and Carbon Considerations of Fine-Tuning BERT](https://doi.org/10.18653/v1/2023.findings-emnlp.607) |  | 0 | Despite the popularity of the pre-train then fine-tune paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of... | Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni |  |
| 741 |  |  [Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models](https://doi.org/10.18653/v1/2023.findings-emnlp.608) |  | 0 | The dominance of proprietary LLMs has led to restricted access and raised information privacy concerns. The SoTA open-source alternatives are crucial for information-sensitive and high-volume applications but often lag behind in performance. To address this gap, we propose (1) A generalized variant of iterative self-critique and self-refinement devoid of external influence. (2) A novel ranking metric - Performance, Refinement, and Inference Cost Score (PeRFICS) - to find the optimal model for a... | Sumuk Shashidhar, Abhinav Chinta, Vaibhav Sahai, Zhenhailong Wang, Heng Ji |  |
| 742 |  |  [Chinese Metaphorical Relation Extraction: Dataset and Models](https://doi.org/10.18653/v1/2023.findings-emnlp.609) |  | 0 | Metaphor identification is usually formulated as a sequence labeling or a syntactically related word-pair classification problem. In this paper, we propose a novel formulation of metaphor identification as a relation extraction problem. We introduce metaphorical relations, which are links between two spans, a target span and a source-related span, which are realized in sentences. Based on spans, we can use more flexible and precise text units beyond single words for capturing the properties of... | Guihua Chen, Tiantian Wu, MiaoMiao Cheng, Xu Han, Jiefu Gong, Shijin Wang, Wei Song |  |
| 743 |  |  [Example-based Hypernetworks for Multi-source Adaptation to Unseen Domains](https://doi.org/10.18653/v1/2023.findings-emnlp.610) |  | 0 | As Natural Language Processing (NLP) algorithms continually achieve new milestones, out-of-distribution generalization remains a significant challenge. This paper addresses the issue of multi-source adaptation for unfamiliar domains: We leverage labeled data from multiple source domains to generalize to unknown target domains at training. Our innovative framework employs example-based Hypernetwork adaptation: a T5 encoder-decoder initially generates a unique signature from an input example,... | Tomer Volk, Eyal BenDavid, Ohad Amosy, Gal Chechik, Roi Reichart |  |
| 744 |  |  [Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.611) |  | 0 | The age of social media is rife with memes. Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image. However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image. In this paper, we attempt to detect harmful memes based on advanced reasoning over the... | Hongzhan Lin, Ziyang Luo, Jing Ma, Long Chen |  |
| 745 |  |  [Domain Adaptation for Conversational Query Production with the RAG Model Feedback](https://doi.org/10.18653/v1/2023.findings-emnlp.612) |  | 0 | Conversational query production is an emerging fundamental task for the dialogue system, where search queries are generated to explore the vast and continually updating knowledge from a search engine. To accelerate this line of research, previous studies have released several datasets with human-annotated search queries. However, the limited annotations still can not cover conversations of various domains. To solve this challenge, we propose a novel domain adaptation framework. It is inspired... | Ante Wang, Linfeng Song, Ge Xu, Jinsong Su |  |
| 746 |  |  [LEGO: A Multi-agent Collaborative Framework with Role-playing and Iterative Feedback for Causality Explanation Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.613) |  | 0 | Causality Explanation Generation refers to generate an explanation in natural language given an initial cause-effect pair. It demands rigorous explicit rationales to demonstrate the acquisition of implicit commonsense knowledge, which is unlikely to be easily memorized, making it challenging for large language models since they are often suffering from spurious causal associations when they encounter the content that does not exist in their memory. In this work, we introduce LEGO, a Multi-agent... | Zhitao He, Pengfei Cao, Yubo Chen, Kang Liu, Ruopeng Li, Mengshu Sun, Jun Zhao |  |
| 747 |  |  [Ranking LLM-Generated Loop Invariants for Program Verification](https://doi.org/10.18653/v1/2023.findings-emnlp.614) |  | 0 | Synthesizing inductive loop invariants is fundamental to automating program verification. In this work we observe that Large Language Models (such as gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of programs in a 0-shot setting, yet require several samples to generate the correct invariants. This can lead to a large number a calls to a program verifier to establish an invariant. To address this issue, we propose a re-ranking approach for the generated results of... | Saikat Chakraborty, Shuvendu K. Lahiri, Sarah Fakhoury, Akash Lal, Madanlal Musuvathi, Aseem Rastogi, Aditya Senthilnathan, Rahul Sharma, Nikhil Swamy |  |
| 748 |  |  [WordNet Is All You Need: A Surprisingly Effective Unsupervised Method for Graded Lexical Entailment](https://doi.org/10.18653/v1/2023.findings-emnlp.615) |  | 0 | We propose a simple unsupervised approach which exclusively relies on WordNet (Miller,1995) for predicting graded lexical entailment (GLE) in English. Inspired by the seminal work of Resnik (1995), our method models GLE as the sum of two information-theoretic scores: a symmetric semantic similarity score and an asymmetric specificity loss score, both exploiting the hierarchical synset structure of WordNet. Our approach also includes a simple disambiguation mechanism to handle polysemy in a... | Joseph Renner, Pascal Denis, Rémi Gilleron |  |
| 749 |  |  [Knowledge Corpus Error in Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.616) |  | 0 | Recent works in open-domain question answering (QA) have explored generating context passages from large language models (LLMs), replacing the traditional retrieval step in the QA pipeline. However, it is not well understood why generated passages can be more effective than retrieved ones. This study revisits the conventional formulation of QA and introduces the concept of knowledge corpus error. This error arises when the knowledge corpus used for retrieval is only a subset of the entire... | Yejoon Lee, Philhoon Oh, James Thorne |  |
| 750 |  |  [Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.617) |  | 0 | Recent advances in machine translation (MT) have shown that Minimum Bayes Risk (MBR) decoding can be a powerful alternative to beam search decoding, especially when combined with neural-based utility functions. However, the performance of MBR decoding depends heavily on how and how many candidates are sampled from the model. In this paper, we explore how different sampling approaches for generating candidate lists for MBR decoding affect performance. We evaluate popular sampling approaches,... | Markus Freitag, Behrooz Ghorbani, Patrick Fernandes |  |
| 751 |  |  [The language of prompting: What linguistic properties make a prompt successful?](https://doi.org/10.18653/v1/2023.findings-emnlp.618) |  | 0 | The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with the task performance. In this work, we investigate how LLMs of different sizes, pre-trained and... | Alina Leidinger, Robert van Rooij, Ekaterina Shutova |  |
| 752 |  |  [When and Why Does Bias Mitigation Work?](https://doi.org/10.18653/v1/2023.findings-emnlp.619) |  | 0 | Neural models have been shown to exploit shallow surface features to perform language understanding tasks, rather than learning the deeper language understanding and reasoning skills that practitioners desire. Previous work has developed debiasing techniques to pressure models away from spurious features or artifacts in datasets, with the goal of having models instead learn useful, task-relevant representations. However, what do models actually learn as a result of such debiasing procedures? In... | Abhilasha Ravichander, Joe Stacey, Marek Rei |  |
| 753 |  |  [Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy](https://doi.org/10.18653/v1/2023.findings-emnlp.620) |  | 0 | Retrieval-augmented generation has raise extensive attention as it is promising to address the limitations of large language models including outdated knowledge and hallucinations. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to guide retrieval with generation. In this paper, we show that strong performance can... | Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen |  |
| 754 |  |  [Dynamic Low-rank Estimation for Transformer-based Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.621) |  | 0 | Matrix decomposition methods, such as Singular Value Decomposition (SVD) and its importance-weighted variants, have been widely used for compressing Transformer-based language models. While importance-weighted decomposition methods alleviate the strong assumption of equal importance for each parameter in SVD, they still rely on two fundamental assumptions: 1) unchanged importance distribution during further fine-tuning, 2) equal importance across weight matrices in different layers.... | Ting Hua, Xiao Li, Shangqian Gao, YenChang Hsu, Yilin Shen, Hongxia Jin |  |
| 755 |  |  [Non-parallel Accent Transfer based on Fine-grained Controllable Accent Modelling](https://doi.org/10.18653/v1/2023.findings-emnlp.622) |  | 0 | Existing accent transfer works rely on parallel data or speech recognition models. This paper focuses on the practical application of accent transfer and aims to implement accent transfer using non-parallel datasets. The study has encountered the challenge of speech representation disentanglement and modeling accents. In our accent modeling transfer framework, we manage to solve these problems by two proposed methods. First, we learn the suprasegmental information associated with tone to finely... | Linqin Wang, Zhengtao Yu, Yuanzhang Yang, Shengxiang Gao, Cunli Mao, Yuxin Huang |  |
| 756 |  |  [Compositional Generalization for Data-to-Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.623) |  | 0 | Data-to-text generation involves transforming structured data, often represented as predicate-argument tuples, into coherent textual descriptions. Despite recent advances, systems still struggle when confronted with unseen combinations of predicates, producing unfaithful descriptions (e.g.,hallucinations or omissions). We refer to this issue as compositional generalisation, and it encouraged us to create a benchmark for assessing the performance of different approaches on this specific problem.... | Xinnuo Xu, Ivan Titov, Mirella Lapata |  |
| 757 |  |  [In-Context Learning Creates Task Vectors](https://doi.org/10.18653/v1/2023.findings-emnlp.624) |  | 0 | In-context learning (ICL) in Large Language Models (LLMs) has emerged as a powerful new learning paradigm. However, its underlying mechanism is still not well understood. In particular, it is challenging to map it to the “standard’ machine learning framework, where one uses a training set S to find a best-fitting function f(x) in some hypothesis class. Here we make progress on this problem by showing that the functions learned by ICL often have a very simple structure: they correspond to the... | Roee Hendel, Mor Geva, Amir Globerson |  |
| 758 |  |  [TalkUp: Paving the Way for Understanding Empowering Language](https://doi.org/10.18653/v1/2023.findings-emnlp.625) |  | 0 | Empowering language is important in many real-world contexts, from education to workplace dynamics to healthcare. Though language technologies are growing more prevalent in these contexts, empowerment has seldom been studied in NLP, and moreover, it is inherently challenging to operationalize because of its implicit nature. This work builds from linguistic and social psychology literature to explore what characterizes empowering language. We then crowdsource a novel dataset of Reddit posts... | Lucille Njoo, Chan Young Park, Octavia Stappart, Marvin Thielk, Yi Chu, Yulia Tsvetkov |  |
| 759 |  |  [Unifying Text, Tables, and Images for Multimodal Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.626) |  | 0 | Multimodal question answering (MMQA), which aims to derive the answer from multiple knowledge modalities (e.g., text, tables, and images), has received increasing attention due to its board applications. Current approaches to MMQA often rely on single-modal or bi-modal QA models, which limits their ability to effectively integrate information across all modalities and leverage the power of pre-trained language models. To address these limitations, we propose a novel framework called UniMMQA,... | Haohao Luo, Ying Shen, Yang Deng |  |
| 760 |  |  [Unsupervised Lexical Simplification with Context Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.627) |  | 0 | We propose a new unsupervised lexical simplification method that uses only monolingual data and pre-trained language models. Given a target word and its context, our method generates substitutes based on the target context and also additional contexts sampled from monolingual data. We conduct experiments in English, Portuguese, and Spanish on the TSAR-2022 shared task, and show that our model substantially outperforms other unsupervised systems across all languages. We also establish a new... | Takashi Wada, Timothy Baldwin, Jey Han Lau |  |
| 761 |  |  [mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences](https://doi.org/10.18653/v1/2023.findings-emnlp.628) |  | 0 | We present our work on developing a multilingual, efficient text-to-text transformer that is suitable for handling long inputs. This model, called mLongT5, builds upon the architecture of LongT5, while leveraging the multilingual datasets used for pretraining mT5 and the pretraining tasks of UL2. We evaluate this model on a variety of multilingual summarization and question-answering tasks, and the results show stronger performance for mLongT5 when compared to existing multilingual models such... | David C. Uthus, Santiago Ontañón, Joshua Ainslie, Mandy Guo |  |
| 762 |  |  [Multilingual Lottery Tickets to Pretrain Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.629) |  | 0 | The curse of multilinguality in training multilingual pretrained language models (mPLMs) refers to the negative interference between languages, especially when the capacity is limited. While increasing the capacity may appear intuitive for overcoming this curse, it negatively affects both training and inference costs. Our distinction is pursuing the competing goals of reducing negative interference, while keeping capacity per each language more or less the same. Specifically, we first scale the... | Jaeseong Lee, Seungwon Hwang |  |
| 763 |  |  [Target-Aware Spatio-Temporal Reasoning via Answering Questions in Dynamic Audio-Visual Scenarios](https://doi.org/10.18653/v1/2023.findings-emnlp.630) |  | 0 | Audio-visual question answering (AVQA) is a challenging task that requires multistep spatio-temporal reasoning over multimodal contexts. Recent works rely on elaborate target-agnostic parsing of audio-visual scenes for spatial grounding while mistreating audio and video as separate entities for temporal grounding. This paper proposes a new target-aware joint spatio-temporal grounding network for AVQA. It consists of two key components: the target-aware spatial grounding module (TSG) and the... | Yuanyuan Jiang, Jianqin Yin |  |
| 764 |  |  [KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.631) |  | 0 | While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at... | Jiho Kim, Yeonsu Kwon, Yohan Jo, Edward Choi |  |
| 765 |  |  [Breaking the Language Barrier: Improving Cross-Lingual Reasoning with Structured Self-Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.632) |  | 0 | In this work, we study whether multilingual language models (MultiLMs) can transfer logical reasoning abilities to other languages when they are fine-tuned for reasoning in a different language. We evaluate the cross-lingual reasoning abilities of MultiLMs in two schemes: (1) where the language of the context and the question remain the same in the new languages that are tested (i.e., the reasoning is still monolingual, but the model must transfer the learned reasoning ability across... | Negar Foroutan, Mohammadreza Banaei, Karl Aberer, Antoine Bosselut |  |
| 766 |  |  [CITB: A Benchmark for Continual Instruction Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.633) |  | 0 | Continual learning (CL) is a paradigm that aims to replicate the human ability to learn and accumulate knowledge continually without forgetting previous knowledge and transferring it to new tasks. Recent instruction tuning (IT) involves fine-tuning models to make them more adaptable to solving NLP tasks in general. However, it is still uncertain how instruction tuning works in the context of CL tasks. This challenging yet practical problem is formulated as Continual Instruction Tuning (CIT). In... | Zihan Zhang, Meng Fang, Ling Chen, MohammadReza NamaziRad |  |
| 767 |  |  [Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.634) |  | 0 | In this work, we propose a method that combines two popular research areas by injecting linguistic structures into pre-trained language models in the parameter-efficient fine-tuning (PEFT) setting. In our approach, parallel adapter modules encoding different linguistic structures are combined using a novel Mixture-of-Linguistic-Experts architecture, where Gumbel-Softmax gates are used to determine the importance of these modules at each layer of the model. To reduce the number of parameters, we... | Raymond Li, Gabriel Murray, Giuseppe Carenini |  |
| 768 |  |  [Towards Better Representations for Multi-Label Text Classification with Multi-granularity Information](https://doi.org/10.18653/v1/2023.findings-emnlp.635) |  | 0 | Multi-label text classification (MLTC) aims to assign multiple labels to a given text. Previous works have focused on text representation learning and label correlations modeling using pre-trained language models (PLMs). However, studies have shown that PLMs generate word frequency-oriented text representations, causing texts with different labels to be closely distributed in a narrow region, which is difficult to classify. To address this, we present a novel framework CL( ̲Contrastive... | Fangfang Li, Puzhen Su, Junwen Duan, Weidong Xiao |  |
| 769 |  |  [PCMID: Multi-Intent Detection through Supervised Prototypical Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.636) |  | 0 | Intent detection is a major task in Natural Language Understanding (NLU) and is the component of dialogue systems for interpreting users’ intentions based on their utterances. Many works have explored detecting intents by assuming that each utterance represents only a single intent. Such systems have achieved very good results; however, intent detection is a far more challenging task in typical real-world scenarios, where each user utterance can be highly complex and express multiple intents.... | Yurun Song, Junchen Zhao, Spencer Koehler, Amir Abdullah, Ian G. Harris |  |
| 770 |  |  [Is GPT-4 a Good Data Analyst?](https://doi.org/10.18653/v1/2023.findings-emnlp.637) |  | 0 | As large language models (LLMs) have demonstrated their powerful capabilities in plenty of domains and tasks, including context understanding, code generation, language generation, data storytelling, etc., many data analysts may raise concerns if their jobs will be replaced by artificial intelligence (AI). This controversial topic has drawn great attention in public. However, we are still at a stage of divergent opinions without any definitive conclusion. Motivated by this, we raise the... | Liying Cheng, Xingxuan Li, Lidong Bing |  |
| 771 |  |  [DiffusionRet: Diffusion-Enhanced Generative Retriever using Constrained Decoding](https://doi.org/10.18653/v1/2023.findings-emnlp.638) |  | 0 | Generative retrieval, which maps from a query to its relevant document identifiers (docids), has recently emerged as a new information retrieval (IR) paradigm, however, having suffered from 1) the lack of the intermediate reasoning step, caused by the manner of merely using a query to perform the hierarchical classification, and 2) the pretrain-finetune discrepancy, which comes from the use of the artificial symbols of docids. To address these limitations, we propose the novel approach of using... | Shanbao Qiao, Xuebing Liu, SeungHoon Na |  |
| 772 |  |  [Estimating Large Language Model Capabilities without Labeled Test Data](https://doi.org/10.18653/v1/2023.findings-emnlp.639) |  | 0 | Large Language Models (LLMs) have exhibited an impressive ability to perform in-context learning (ICL) from only a few examples, but the success of ICL varies widely from task to task. Thus, it is important to quickly determine whether ICL is applicable to a new task, but directly evaluating ICL accuracy can be expensive in situations where test data is expensive to annotate—the exact situations where ICL is most appealing. In this paper, we propose the task of ICL accuracy estimation, in which... | Harvey Yiyun Fu, Qinyuan Ye, Albert Xu, Xiang Ren, Robin Jia |  |
| 773 |  |  [A Novel Contrastive Learning Method for Clickbait Detection on RoCliCo: A Romanian Clickbait Corpus of News Articles](https://doi.org/10.18653/v1/2023.findings-emnlp.640) |  | 0 | To increase revenue, news websites often resort to using deceptive news titles, luring users into clicking on the title and reading the full news. Clickbait detection is the task that aims to automatically detect this form of false advertisement and avoid wasting the precious time of online users. Despite the importance of the task, to the best of our knowledge, there is no publicly available clickbait corpus for the Romanian language. To this end, we introduce a novel Romanian Clickbait Corpus... | DariaMihaela Broscoteanu, Radu Tudor Ionescu |  |
| 774 |  |  [Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogues](https://doi.org/10.18653/v1/2023.findings-emnlp.641) |  | 0 | Open-domain dialogue system usually requires different sources of knowledge to generate more informative and evidential responses. However, existing knowledge-grounded dialogue systems either focus on a single knowledge source or overlook the dependency between multiple sources of knowledge, which may result in generating inconsistent or even paradoxical responses. To incorporate multiple knowledge sources and dependencies between them, we propose SAFARI, a novel framework that leverages the... | Hongru Wang, Minda Hu, Yang Deng, Rui Wang, Fei Mi, Weichao Wang, Yasheng Wang, WaiChung Kwan, Irwin King, KamFai Wong |  |
| 775 |  |  [Toxicity in Multilingual Machine Translation at Scale](https://doi.org/10.18653/v1/2023.findings-emnlp.642) |  | 0 | Machine Translation systems can produce different types of errors, some of which are characterized as critical or catastrophic due to the specific negative impact that they can have on users. In this paper we focus on one type of critical error: added toxicity. We evaluate and analyze added toxicity when translating a large evaluation dataset (HOLISTICBIAS, over 472k sentences, covering 13 demographic axes) from English into 164 languages. An automatic toxicity evaluation shows that added... | Marta R. Costajussà, Eric Michael Smith, Christophe Ropers, Daniel Licht, Jean Maillard, Javier Ferrando, Carlos Escolano |  |
| 776 |  |  [Conversational Recommender System and Large Language Model Are Made for Each Other in E-commerce Pre-sales Dialogue](https://doi.org/10.18653/v1/2023.findings-emnlp.643) |  | 0 | E-commerce pre-sales dialogue aims to understand and elicit user needs and preferences for the items they are seeking so as to provide appropriate recommendations. Conversational recommender systems (CRSs) learn user representation and provide accurate recommendations based on dialogue context, but rely on external knowledge. Large language models (LLMs) generate responses that mimic pre-sales dialogues after fine-tuning, but lack domain-specific knowledge for accurate recommendations.... | Yuanxing Liu, Weinan Zhang, Yifan Chen, Yuchi Zhang, Haopeng Bai, Fan Feng, Hengbin Cui, Yongbin Li, Wanxiang Che |  |
| 777 |  |  [VIP5: Towards Multimodal Foundation Models for Recommendation](https://doi.org/10.18653/v1/2023.findings-emnlp.644) |  | 0 | Computer Vision (CV), Natural Language Processing (NLP), and Recommender Systems (RecSys) are three prominent AI applications that have traditionally developed independently, resulting in disparate modeling and engineering methodologies. This has impeded the ability for these fields to directly benefit from each other’s advancements. With the recent development of foundation models, large language models have emerged as a potential general-purpose interface for unifying different modalities and... | Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, Yongfeng Zhang |  |
| 778 |  |  [A Spectral Viewpoint on Continual Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.645) |  | 0 | Continual Relation Extraction (CRE) aims to continuously train a model to learn new relations while preserving its ability on previously learned relations. Similar to other continual learning problems, in CRE, models experience representation shift, where learned deep space changes in the continual learning process, which leads to the downgrade in the performance of the old tasks. In this work, we will provide an insight into this phenomenon under the spectral viewpoint. Our key argument is... | Huy Nguyen, Chien Nguyen, Linh Ngo Van, Anh Tuan Luu, Thien Huu Nguyen |  |
| 779 |  |  [Learning to Follow Object-Centric Image Editing Instructions Faithfully](https://doi.org/10.18653/v1/2023.findings-emnlp.646) |  | 0 | Natural language instructions are a powerful interface for editing the outputs of text-to-image diffusion models. However, several challenges need to be addressed: 1) underspecification (the need to model the implicit meaning of instructions) 2) grounding (the need to localize where the edit has to be performed), 3) faithfulness (the need to preserve the elements of the image not affected by the edit instruction). Current approaches focusing on image editing with natural language instructions... | Tuhin Chakrabarty, Kanishk Singh, Arkadiy Saakyan, Smaranda Muresan |  |
| 780 |  |  [Zero-shot Topical Text Classification with LLMs - an Experimental Study](https://doi.org/10.18653/v1/2023.findings-emnlp.647) |  | 0 | Topical Text Classification (TTC) is an ancient, yet timely research area in natural language processing, with many practical applications. The recent dramatic advancements in large LMs raise the question of how well these models can perform in this task in a zero-shot scenario. Here, we share a first comprehensive study, comparing the zero-shot performance of a variety of LMs over TTC23, a large benchmark collection of 23 publicly available TTC datasets, covering a wide range of domains and... | Shai Gretz, Alon Halfon, Ilya Shnayderman, Orith ToledoRonen, Artem Spector, Lena Dankin, Yannis Katsis, Ofir Arviv, Yoav Katz, Noam Slonim, Liat EinDor |  |
| 781 |  |  [Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems](https://doi.org/10.18653/v1/2023.findings-emnlp.648) |  | 0 | Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. We define generic personas to represent demographic groups, such as “an Asian person”, whereas specific personas may take the form of specific popular Asian names like “Yumi”. While the adoption of personas enriches user experiences by making dialogue systems more engaging and approachable, it also casts a shadow of potential... | Yixin Wan, Jieyu Zhao, Aman Chadha, Nanyun Peng, KaiWei Chang |  |
| 782 |  |  [A Black-Box Attack on Code Models via Representation Nearest Neighbor Search](https://doi.org/10.18653/v1/2023.findings-emnlp.649) |  | 0 | Existing methods for generating adversarial code examples face several challenges: limted availability of substitute variables, high verification costs for these substitutes, and the creation of adversarial samples with noticeable perturbations. To address these concerns, our proposed approach, RNNS, uses a search seed based on historical attacks to find potential adversarial substitutes. Rather than directly using the discrete substitutes, they are mapped to a continuous vector space using a... | Jie Zhang, Wei Ma, Qiang Hu, Shangqing Liu, Xiaofei Xie, Yves Le Traon, Yang Liu |  |
| 783 |  |  [How Well Do Text Embedding Models Understand Syntax?](https://doi.org/10.18653/v1/2023.findings-emnlp.650) |  | 0 | Text embedding models have significantly contributed to advancements in natural language processing by adeptly capturing semantic properties of textual data. However, the ability of these models to generalize across a wide range of syntactic contexts remains under-explored. In this paper, we first develop an evaluation set, named SR, to scrutinize the capability for syntax understanding of text embedding models from two crucial syntactic aspects: Structural heuristics, and Relational... | Yan Zhang, Zhaopeng Feng, Zhiyang Teng, Zuozhu Liu, Haizhou Li |  |
| 784 |  |  [CASSI: Contextual and Semantic Structure-based Interpolation Augmentation for Low-Resource NER](https://doi.org/10.18653/v1/2023.findings-emnlp.651) |  | 0 | While text augmentation methods have been successful in improving performance in the low-resource setting, they suffer from annotation corruption for a token-level task like NER. Moreover, existing methods cannot reliably add context diversity to the dataset, which has been shown to be crucial for low-resource NER. In this work, we propose Contextual and Semantic Structure-based Interpolation (CASSI), a novel augmentation scheme that generates high-quality contextually diverse augmentations... | Tanmay Surana, ThiNga Ho, Kyaw Zin Tun, Eng Siong Chng |  |
| 785 |  |  [NEWTON: Are Large Language Models Capable of Physical Reasoning?](https://doi.org/10.18653/v1/2023.findings-emnlp.652) |  | 0 | Large Language Models (LLMs), through their contextualized representations, have been empirically proven to encapsulate syntactic, semantic, word sense, and common-sense knowledge. However, there has been limited exploration of their physical reasoning abilities, specifically concerning the crucial attributes for comprehending everyday objects. To address this gap, we introduce NEWTON, a repository and benchmark for evaluating the physics reasoning skills of LLMs. Further, to enable... | Yi Ru Wang, Jiafei Duan, Dieter Fox, Siddhartha S. Srinivasa |  |
| 786 |  |  [Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language](https://doi.org/10.18653/v1/2023.findings-emnlp.653) |  | 0 | Counterspeech, i.e., responses to counteract potential harms of hateful speech, has become an increasingly popular solution to address online hate speech without censorship. However, properly countering hateful language requires countering and dispelling the underlying inaccurate stereotypes implied by such language. In this work, we draw from psychology and philosophy literature to craft six psychologically inspired strategies to challenge the underlying stereotypical implications of hateful... | Jimin Mun, Emily Allaway, Akhila Yerukola, Laura Vianna, SarahJane Leslie, Maarten Sap |  |
| 787 |  |  [On the Calibration of Large Language Models and Alignment](https://doi.org/10.18653/v1/2023.findings-emnlp.654) |  | 0 | As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the... | Chiwei Zhu, Benfeng Xu, Quan Wang, Yongdong Zhang, Zhendong Mao |  |
| 788 |  |  [TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction](https://doi.org/10.18653/v1/2023.findings-emnlp.655) |  | 0 | Since ChatGPT released its API for public use, the number of applications built on top of commercial large language models (LLMs) increase exponentially. One popular usage of such models is leveraging its in-context learning ability and generating responses given user queries leveraging knowledge obtained by retrieval augmentation. One problem of deploying commercial retrieval-augmented LLMs is the cost due to the additionally retrieved context that largely increases the input token size of the... | Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, Yiming Qian |  |
| 789 |  |  [Identifying Conspiracy Theories News based on Event Relation Graph](https://doi.org/10.18653/v1/2023.findings-emnlp.656) |  | 0 | Conspiracy theories, as a type of misinformation, are narratives that explains an event or situation in an irrational or malicious manner. While most previous work examined conspiracy theory in social media short texts, limited attention was put on such misinformation in long news documents. In this paper, we aim to identify whether a news article contains conspiracy theories. We observe that a conspiracy story can be made up by mixing uncorrelated events together, or by presenting an unusual... | Yuanyuan Lei, Ruihong Huang |  |
| 790 |  |  [Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems](https://doi.org/10.18653/v1/2023.findings-emnlp.657) |  | 0 | Making big purchases requires consumers to research or consult a salesperson to gain domain expertise. However, existing conversational recommender systems (CRS) often overlook users’ lack of background knowledge, focusing solely on gathering preferences. In this work, we define a new problem space for conversational agents that aim to provide both product recommendations and educational value through mixed-type mixed-initiative dialog. We introduce SalesOps, a framework that facilitates the... | Lidiya Murakhovs'ka, Philippe Laban, Tian Xie, Caiming Xiong, ChienSheng Wu |  |
| 791 |  |  [Dynamic Open-book Prompt for Conversational Recommender System](https://doi.org/10.18653/v1/2023.findings-emnlp.658) |  | 0 | Conversational Recommender System (CRS) aims to deliver personalized recommendations through interactive dialogues. Recent advances in prompt learning have shed light on this task. However, the performance of existing methods is confined by the limited context within ongoing conversations. Moreover, these methods utilize training samples only for prompt parameter training. The constructed prompt lacks the ability to refer to the training data during inference, which exacerbates the problem of... | Xuan Ma, Tieyun Qian, Ke Sun |  |
| 792 |  |  [Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.659) |  | 0 | Large language models (LLMs) can perform a wide range of tasks by following natural language instructions, without the necessity of task-specific fine-tuning. Unfortunately, the performance of LLMs is greatly influenced by the quality of these instructions, and manually writing effective instructions for each task is a laborious and subjective process. In this paper, we introduce Auto-Instruct, a novel method to automatically improve the quality of instructions provided to LLMs. Our method... | Zhihan Zhang, Shuohang Wang, Wenhao Yu, Yichong Xu, Dan Iter, Qingkai Zeng, Yang Liu, Chenguang Zhu, Meng Jiang |  |
| 793 |  |  [DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models](https://doi.org/10.18653/v1/2023.findings-emnlp.660) |  | 0 | Diffusion models have gained prominence in generating high-quality sequences of text. Nevertheless, current approaches predominantly represent discrete text within a continuous diffusion space, which incurs substantial computational overhead during training and results in slower sampling speeds. In this paper, we introduce a soft absorbing state that facilitates the diffusion model in learning to reconstruct discrete mutations based on the underlying Gaussian space, thereby enhancing its... | Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, Lingpeng Kong |  |
| 794 |  |  [M2C: Towards Automatic Multimodal Manga Complement](https://doi.org/10.18653/v1/2023.findings-emnlp.661) |  | 0 | Multimodal manga analysis focuses on enhancing manga understanding with visual and textual features, which has attracted considerable attention from both natural language processing and computer vision communities. Currently, most comics are hand-drawn and prone to problems such as missing pages, text contamination, and text aging, resulting in missing comic text content and seriously hindering human comprehension. In other words, the Multimodal Manga Complement (M2C) task has not been... | Hongcheng Guo, Boyang Wang, Jiaqi Bai, Jiaheng Liu, Jian Yang, Zhoujun Li |  |
| 795 |  |  [Learn Your Tokens: Word-Pooled Tokenization for Language Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.662) |  | 0 | Language models typically tokenize text into subwords, using a deterministic, hand-engineered heuristic of combining characters into longer surface-level strings such as ‘ing’ or whole words. Recent literature has repeatedly shown the limitations of such a tokenization strategy, particularly for documents not written in English and for representing numbers. On the other extreme, byte/character-level language models are much less restricted but suffer from increased sequence description lengths... | Avijit Thawani, Saurabh Ghanekar, Xiaoyuan Zhu, Jay Pujara |  |
| 796 |  |  [Towards Detecting Contextual Real-Time Toxicity for In-Game Chat](https://doi.org/10.18653/v1/2023.findings-emnlp.663) |  | 0 | Real-time toxicity detection in online environments poses a significant challenge, due to the increasing prevalence of social media and gaming platforms. We introduce ToxBuster, a simple and scalable model that reliably detects toxic content in real-time for a line of chat by including chat history and metadata. ToxBuster consistently outperforms conventional toxicity models across popular multiplayer games, including Rainbow Six Siege, For Honor, and DOTA 2. We conduct an ablation study to... | Zachary Yang, Nicolas GrenonGodbout, Reihaneh Rabbany |  |
| 797 |  |  [JWSign: A Highly Multilingual Corpus of Bible Translations for more Diversity in Sign Language Processing](https://doi.org/10.18653/v1/2023.findings-emnlp.664) |  | 0 | Advancements in sign language processing have been hindered by a lack of sufficient data, impeding progress in recognition, translation, and production tasks. The absence of comprehensive sign language datasets across the world’s sign languages has widened the gap in this field, resulting in a few sign languages being studied more than others, making this research area extremely skewed mostly towards sign languages from high-income countries. In this work we introduce a new large and highly... | Shester Gueuwou, Sophie Siake, Colin Leong, Mathias Müller |  |
| 798 |  |  [Do Stochastic Parrots have Feelings Too? Improving Neural Detection of Synthetic Text via Emotion Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.665) |  | 0 | Recent developments in generative AI have shone a spotlight on high-performance synthetic text generation technologies. The now wide availability and ease of use of such models highlights the urgent need to provide equally powerful technologies capable of identifying synthetic text. With this in mind, we draw inspiration from psychological studies which suggest that people can be driven by emotion and encode emotion in the text they compose. We hypothesize that pretrained language models (PLMs)... | Alan Cowap, Yvette Graham, Jennifer Foster |  |
| 799 |  |  [Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules](https://doi.org/10.18653/v1/2023.findings-emnlp.666) |  | 0 | Large language models (LLMs) have achieved remarkable results on NLP tasks but at the expense of huge parameter sizes and the consequent computational costs. In this paper, we propose Variator, a parameter-efficient acceleration method that enhances computational efficiency through plug-and-play compression plugins. Compression plugins are designed to reduce the sequence length via compressing multiple hidden vectors into one and trained with original LLMs frozen. Different from traditional... | Chaojun Xiao, Yuqi Luo, Wenbin Zhang, Pengle Zhang, Xu Han, Yankai Lin, Zhengyan Zhang, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou |  |
| 800 |  |  [PivotFEC: Enhancing Few-shot Factual Error Correction with a Pivot Task Approach using Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.667) |  | 0 | Factual Error Correction (FEC) aims to rectify false claims by making minimal revisions to align them more accurately with supporting evidence. However, the lack of datasets containing false claims and their corresponding corrections has impeded progress in this field. Existing distantly supervised models typically employ the mask-then-correct paradigm, where a masker identifies problematic spans in false claims, followed by a corrector to predict the masked portions. Unfortunately, accurately... | Xingwei He, ALong Jin, Jun Ma, Yuan Yuan, Siu Ming Yiu |  |
| 801 |  |  [Semantic Similarity Covariance Matrix Shrinkage](https://doi.org/10.18653/v1/2023.findings-emnlp.668) |  | 0 | An accurate estimation of the covariance matrix is a critical component of many applications in finance, including portfolio optimization. The sample covariance suffers from the curse of dimensionality when the number of observations is in the same order or lower than the number of variables. This tends to be the case in portfolio optimization, where a portfolio manager can choose between thousands of stocks using historical daily returns to guide their investment decisions. To address this... | Guillaume Becquin, Saher Esmeir |  |
| 802 |  |  [LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.669) |  | 0 | Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen their data interpretation and coding over multiple iterations, making TA labor-intensive and time-consuming. Recently the emerging field of large language models (LLMs) research has shown that LLMs have... | ShihChieh Dai, Aiping Xiong, LunWei Ku |  |
| 803 |  |  [LLM aided semi-supervision for efficient Extractive Dialog Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.670) |  | 0 | Generating high-quality summaries for chat dialogs often requires large labeled datasets. We propose a method to efficiently use unlabeled data for extractive summarization of customer-agent dialogs. In our method, we frame summarization as a question-answering problem and use state-of-the-art large language models (LLMs) to generate pseudo-labels for a dialog. We then use these pseudo-labels to fine-tune a chat summarization model, effectively transferring knowledge from the large LLM into a... | Nishant Mishra, Gaurav Sahu, Iacer Calixto, Ameen AbuHanna, Issam H. Laradji |  |
| 804 |  |  [Investigating Multilingual Coreference Resolution by Universal Annotations](https://doi.org/10.18653/v1/2023.findings-emnlp.671) |  | 0 | Multilingual coreference resolution (MCR) has been a long-standing and challenging task. With the newly proposed multilingual coreference dataset, CorefUD (Nedoluzhko et al., 2022), we conduct an investigation into the task by using its harmonized universal morphosyntactic and coreference annotations. First, we study coreference by examining the ground truth data at different linguistic levels, namely mention, entity and document levels, and across different genres, to gain insights into the... | Haixia Chai, Michael Strube |  |
| 805 |  |  [FactSpotter: Evaluating the Factual Faithfulness of Graph-to-Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.672) |  | 0 | Graph-to-text (G2T) generation takes a graph as input and aims to generate a fluent and faith- ful textual representation of the information in the graph. The task has many applications, such as dialogue generation and question an- swering. In this work, we investigate to what extent the G2T generation problem is solved for previously studied datasets, and how pro- posed metrics perform when comparing generated texts. To help address their limitations, we propose a new metric that correctly... | Kun Zhang, Oana Balalau, Ioana Manolescu |  |
| 806 |  |  [LayoutDIT: Layout-Aware End-to-End Document Image Translation with Multi-Step Conductive Decoder](https://doi.org/10.18653/v1/2023.findings-emnlp.673) |  | 0 | Document image translation (DIT) aims to translate text embedded in images from one language to another. It is a challenging task that needs to understand visual layout with text semantics simultaneously. However, existing methods struggle to capture the crucial visual layout in real-world complex document images. In this work, we make the first attempt to incorporate layout knowledge into DIT in an end-to-end way. Specifically, we propose a novel Layout-aware end-to-end Document Image... | Zhiyang Zhang, Yaping Zhang, Yupu Liang, Lu Xiang, Yang Zhao, Yu Zhou, Chengqing Zong |  |
| 807 |  |  [Balaur: Language Model Pretraining with Lexical Semantic Relations](https://doi.org/10.18653/v1/2023.findings-emnlp.674) |  | 0 | Lexical semantic relations (LSRs) characterize meaning relationships between words and play an important role in systematic generalization on lexical inference tasks. Notably, several tasks that require knowledge of hypernymy still pose a challenge for pretrained language models (LMs) such as BERT, underscoring the need to better align their linguistic behavior with our knowledge of LSRs. In this paper, we propose Balaur, a model that addresses this challenge by modeling LSRs directly in the... | Andrei Mircea, Jackie C. K. Cheung |  |
| 808 |  |  [Exploring In-Context Learning for Knowledge Grounded Dialog Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.675) |  | 0 | Large neural-based dialog generation models have been applied in many real-life scenarios, yet they are prone to hallucination and tend to produce factually inaccurate outputs which raise great concerns. To alleviate this problem, we propose a plug-and-play retrieval-based framework IKA, which leverages in-context learning and retrieval techniques to enhance LLMs on knowledge grounded dialog generation. We design thorough experiments on a large-scale knowledge graph with 1M+ facts to... | Qinyu Chen, Wenhao Wu, Sujian Li |  |
| 809 |  |  [Towards Enhancing Relational Rules for Knowledge Graph Link Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.676) |  | 0 | Graph neural networks (GNNs) have shown promising performance for knowledge graph reasoning. A recent variant of GNN called progressive relational graph neural network (PRGNN), utilizes relational rules to infer missing knowledge in relational digraphs and achieves notable results. However, during reasoning with PRGNN, two important properties are often overlooked: (1) the sequentiality of relation composition, where the order of combining different relations affects the semantics of the... | Shuhan Wu, Huaiyu Wan, Wei Chen, Yuting Wu, Junfeng Shen, Youfang Lin |  |
| 810 |  |  [Are NLP Models Good at Tracing Thoughts: An Overview of Narrative Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.677) |  | 0 | Narrative understanding involves capturing the author’s cognitive processes, providing insights into their knowledge, intentions, beliefs, and desires. Although large language models (LLMs) excel in generating grammatically coherent text, their ability to comprehend the author’s thoughts remains uncertain. This limitation hinders the practical applications of narrative understanding. In this paper, we conduct a comprehensive survey of narrative understanding tasks, thoroughly examining their... | Lixing Zhu, Runcong Zhao, Lin Gui, Yulan He |  |
| 811 |  |  [Who is Speaking? Speaker-Aware Multiparty Dialogue Act Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.678) |  | 0 | Utterances do not occur in isolation in dialogues; it is essential to have the information of who the speaker of an utterance is to be able to recover the speaker’s intention with respect to the surrounding context. Beyond simply capturing speaker switches, identifying how speakers interact with each other in a dialogue is crucial to understanding conversational flow. This becomes increasingly important and simultaneously difficult to model when more than two interlocutors take part in a... | Ayesha Qamar, Adarsh Pyarelal, Ruihong Huang |  |
| 812 |  |  [Demystifying Prompts in Language Models via Perplexity Estimation](https://doi.org/10.18653/v1/2023.findings-emnlp.679) |  | 0 | Language models can be prompted to perform a wide variety of tasks with zero- and few-shot in-context learning. However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens. In this paper, we analyze the factors that contribute to this variance and establish a new empirical hypothesis: the performance of a prompt is predicted by the extent to which the model is familiar with the language it contains. Over a wide range of tasks, we show that... | Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith, Luke Zettlemoyer |  |
| 813 |  |  [C2D2 Dataset: A Resource for the Cognitive Distortion Analysis and Its Impact on Mental Health](https://doi.org/10.18653/v1/2023.findings-emnlp.680) |  | 0 | Cognitive distortions refer to patterns of irrational thinking that can lead to distorted perceptions of reality and mental health problems in individuals. Despite previous attempts to detect cognitive distortion through language, progress has been slow due to the lack of appropriate data. In this paper, we present the C2D2 dataset, the first expert-supervised Chinese Cognitive Distortion Dataset, which contains 7,500 cognitive distortion thoughts in everyday life scenes. Additionally, we... | Bichen Wang, Pengfei Deng, Yanyan Zhao, Bing Qin |  |
| 814 |  |  [MixEdit: Revisiting Data Augmentation and Beyond for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.findings-emnlp.681) |  | 0 | Data Augmentation through generating pseudo data has been proven effective in mitigating the challenge of data scarcity in the field of Grammatical Error Correction (GEC). Various augmentation strategies have been widely explored, most of which are motivated by two heuristics, i.e., increasing the distribution similarity and diversity of pseudo data. However, the underlying mechanism responsible for the effectiveness of these strategies remains poorly understood. In this paper, we aim to... | Jingheng Ye, Yinghui Li, Yangning Li, HaiTao Zheng |  |
| 815 |  |  [CCEval: A Representative Evaluation Benchmark for the Chinese-centric Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.682) |  | 0 | The Chinese-centric Multilingual Machine Translation (MMT) has gained more importance recently due to increasing demands from international business development and cross-cultural exchanges. However, an important factor that limits the progress of this area is the lack of highly representative and high-quality evaluation benchmarks. To fill this gap, we propose CCEval, an impartial and representative Chinese-centric MMT evaluation dataset. This benchmark dataset consists of 2500 Chinese... | Lianzhang Lou, Xi Yin, Yutao Xie, Yang Xiang |  |
| 816 |  |  [ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual Common Sense](https://doi.org/10.18653/v1/2023.findings-emnlp.683) |  | 0 | Humans possess a strong capability for reasoning beyond common sense. For example, given an unconventional image of a goldfish laying on the table next to an empty fishbowl, a human would effortlessly determine that the fish is not inside the fishbowl. The case, however, may be different for a vision-language model, whose reasoning could gravitate towards the common scenario that the fish is inside the bowl, despite the visual input. In this paper, we introduce a novel probing dataset named... | Kankan Zhou, Eason Lai, Wei Bin Au Yeong, Kyriakos Mouratidis, Jing Jiang |  |
| 817 |  |  [Automatic Analysis of Substantiation in Scientific Peer Reviews](https://doi.org/10.18653/v1/2023.findings-emnlp.684) |  | 0 | With the increasing amount of problematic peer reviews in top AI conferences, the community is urgently in need of automatic quality control measures. In this paper, we restrict our attention to substantiation — one popular quality aspect indicating whether the claims in a review are sufficiently supported by evidence — and provide a solution automatizing this evaluation process. To achieve this goal, we first formulate the problem as claim-evidence pair extraction in scientific peer reviews,... | Yanzhu Guo, Guokan Shang, Virgile Rennard, Michalis Vazirgiannis, Chloé Clavel |  |
| 818 |  |  [Hierarchical Prompting Assists Large Language Model on Web Navigation](https://doi.org/10.18653/v1/2023.findings-emnlp.685) |  | 0 | Large language models (LLMs) struggle on processing complicated observations in interactive decision making. To alleviate this issue, we propose a simple hierarchical prompting approach. Diverging from previous prompting approaches that always put the full observation (a web page) to the prompt, we propose to first construct an action-aware observation which is more condensed and relevant with a dedicated Summarizer prompt. The Actor prompt then predicts the next action based on the summarized... | Robert Lo, Abishek Sridhar, Frank F. Xu, Hao Zhu, Shuyan Zhou |  |
| 819 |  |  [Can Large Language Models Fix Data Annotation Errors? An Empirical Study Using Debatepedia for Query-Focused Text Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.686) |  | 0 | Debatepedia is a publicly available dataset consisting of arguments and counter-arguments on controversial topics that has been widely used for the single-document query-focused abstractive summarization task in recent years. However, it has been recently found that this dataset is limited by noise and even most queries in this dataset do not have any relevance to the respective document. In this paper, we study whether large language models (LLMs) can be utilized to clean the Debatepedia... | Md. Tahmid Rahman Laskar, Mizanur Rahman, Israt Jahan, Enamul Hoque, Jimmy Xiangji Huang |  |
| 820 |  |  [TSTR: Target Similarity Tuning Meets the Real World](https://doi.org/10.18653/v1/2023.findings-emnlp.687) |  | 0 | Target similarity tuning (TST) is a method of selecting relevant examples in natural language (NL) to code generation through large language models (LLMs) to improve performance. Its goal is to adapt a sentence embedding model to have the similarity between two NL inputs match the similarity between their associated code outputs. In this paper, we propose different methods to apply and improve TST in the real world. First, we replace the sentence transformer with embeddings from a larger model,... | Anirudh Khatry, Sumit Gulwani, Priyanshu Gupta, Vu Le, Mukul Singh, Ananya Singha, Gust Verbruggen |  |
| 821 |  |  [RealBehavior: A Framework for Faithfully Characterizing Foundation Models' Human-like Behavior Mechanisms](https://doi.org/10.18653/v1/2023.findings-emnlp.688) |  | 0 | Reports of human-like behaviors in foundation models are growing, with psychological theories providing enduring tools to investigate these behaviors. However, current research tends to directly apply these human-oriented tools without verifying the faithfulness of their outcomes. In this paper, we introduce a framework, RealBehavior, which is designed to characterize the humanoid behaviors of models faithfully. Beyond simply measuring behaviors, our framework assesses the faithfulness of... | Enyu Zhou, Rui Zheng, Zhiheng Xi, Songyang Gao, Xiaoran Fan, Zichu Fei, Jingting Ye, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 822 |  |  [Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance](https://doi.org/10.18653/v1/2023.findings-emnlp.689) |  | 0 | Large Language Models (LLMs) are increasingly utilized in educational tasks such as providing writing suggestions to students. Despite their potential, LLMs are known to harbor inherent biases which may negatively impact learners. Previous studies have investigated bias in models and data representations separately, neglecting the potential impact of LLM bias on human writing. In this paper, we investigate how bias transfers through an AI writing support pipeline. We conduct a large-scale user... | Thiemo Wambsganss, Xiaotian Su, Vinitra Swamy, Seyed Parsa Neshaei, Roman Rietsche, Tanja Käser |  |
| 823 |  |  [VERVE: Template-based ReflectiVE Rewriting for MotiVational IntErviewing](https://doi.org/10.18653/v1/2023.findings-emnlp.690) |  | 0 | Reflective listening is a fundamental skill that counselors must acquire to achieve proficiency in motivational interviewing (MI). It involves responding in a manner that acknowledges and explores the meaning of what the client has expressed in the conversation. In this work, we introduce the task of counseling response rewriting, which transforms non-reflective statements into reflective responses. We introduce VERVE, a template-based rewriting system with paraphrase-augmented training and... | Do June Min, Verónica PérezRosas, Ken Resnicow, Rada Mihalcea |  |
| 824 |  |  [Self-Knowledge Guided Retrieval Augmentation for Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.691) |  | 0 | Large language models (LLMs) have shown superior performance without task-specific fine-tuning. Despite the success, the knowledge stored in the parameters of LLMs could still be incomplete and difficult to update due to the computational costs. As complementary, retrieval-based methods can offer non-parametric world knowledge and improve the performance on tasks such as question answering. However, we find that the retrieved knowledge does not always help and even has a negative impact on... | Yile Wang, Peng Li, Maosong Sun, Yang Liu |  |
| 825 |  |  [Pretraining Language Models with Text-Attributed Heterogeneous Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.692) |  | 0 | In many real-world scenarios (e.g., academic networks, social platforms), different types of entities are not only associated with texts but also connected by various relationships, which can be abstracted as Text-Attributed Heterogeneous Graphs (TAHGs). Current pretraining tasks for Language Models (LMs) primarily focus on separately learning the textual information of each entity and overlook the crucial aspect of capturing topological connections among entities in TAHGs. In this paper, we... | Tao Zou, Le Yu, Yifei Huang, Leilei Sun, Bowen Du |  |
| 826 |  |  [CReTIHC: Designing Causal Reasoning Tasks about Temporal Interventions and Hallucinated Confoundings](https://doi.org/10.18653/v1/2023.findings-emnlp.693) |  | 0 | Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their ability to establish causal relationships, particularly in the context of temporal interventions and language hallucinations, remains challenging. This paper presents CReTIHC, a novel dataset designed to test and enhance the causal reasoning abilities of LLMs. The dataset is constructed using a unique approach that incorporates elements of verbal hallucinations and temporal... | Changwoo Chun, Songeun Lee, Jaehyung Seo, Heuiseok Lim |  |
| 827 |  |  [On the Dimensionality of Sentence Embeddings](https://doi.org/10.18653/v1/2023.findings-emnlp.694) |  | 0 | Learning sentence embeddings is a fundamental problem in natural language processing. While existing research primarily focuses on enhancing the quality of sentence embeddings, the exploration of sentence embedding dimensions is limited. Here we present a comprehensive and empirical analysis of the dimensionality of sentence embeddings. First, we demonstrate that the optimal dimension of sentence embeddings is usually smaller than the default value. Subsequently, to compress the dimension of... | Hongwei Wang, Hongming Zhang, Dong Yu |  |
| 828 |  |  [Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.695) |  | 0 | Scaling pre-trained language models has resulted in large performance gains in various natural language processing tasks but comes with a large cost in memory requirements. Inspired by the position embeddings in transformers, we aim to simplify and reduce the memory footprint of the multi-head attention (MHA) mechanism. We propose an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head. We empirically demonstrate that our MHE... | Huiyin Xue, Nikolaos Aletras |  |
| 829 |  |  [Entity-Based Evaluation of Political Bias in Automatic Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.696) |  | 0 | Growing literature has shown that NLP systems may encode social biases; however, the \*political\* bias of summarization models remains relatively unknown. In this work, we use an entity replacement method to investigate the portrayal of politicians in automatically generated summaries of news articles. We develop an entity-based computational framework to assess the sensitivities of several extractive and abstractive summarizers to the politicians Donald Trump and Joe Biden. We find consistent... | Karen Zhou, Chenhao Tan |  |
| 830 |  |  [StyleBART: Decorate Pretrained Model with Style Adapters for Unsupervised Stylistic Headline Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.697) |  | 0 | Stylistic headline generation is the task to generate a headline that not only summarizes the content of an article, but also reflects a desired style that attracts users. As style-specific article-headline pairs are scarce, previous researches focus on unsupervised approaches with a standard headline generation dataset and mono-style corpora. In this work, we follow this line and propose StyleBART, an unsupervised approach for stylistic headline generation. Our method decorates the pretrained... | Hanqing Wang, Yajing Luo, Boya Xiong, Guanhua Chen, Yun Chen |  |
| 831 |  |  [RSVP: Customer Intent Detection via Agent Response Contrastive and Generative Pre-Training](https://doi.org/10.18653/v1/2023.findings-emnlp.698) |  | 0 | The dialogue systems in customer services have been developed with neural models to provide users with precise answers and round-the-clock support in task-oriented conversations by detecting customer intents based on their utterances. Existing intent detection approaches have highly relied on adaptively pre-training language models with large-scale datasets, yet the predominant cost of data collection may hinder their superiority. In addition, they neglect the information within the... | YuChien Tang, WeiYao Wang, AnZi Yen, WenChih Peng |  |
| 832 |  |  [Improving Low-resource Question Answering by Augmenting Question Information](https://doi.org/10.18653/v1/2023.findings-emnlp.699) |  | 0 | In the era of large models, low-resource question-answering tasks lag, emphasizing the importance of data augmentation - a key research avenue in natural language processing. The main challenges include leveraging the large model’s internal knowledge for data augmentation, determining which QA data component - the question, passage, or answer - benefits most from augmentation, and retaining consistency in the augmented content without inducing excessive noise. To tackle these, we introduce PQQ,... | Andong Chen, Yuan Sun, Xiaobing Zhao, Rosella P. Galindo Esparza, Kehai Chen, Yang Xiang, Tiejun Zhao, Min Zhang |  |
| 833 |  |  [InstructSafety: A Unified Framework for Building Multidimensional and Explainable Safety Detector through Instruction Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.700) |  | 0 | Safety detection has been an increasingly important topic in recent years and it has become even more necessary to develop reliable safety detection systems with the rapid development of large language models. However, currently available safety detection systems have limitations in terms of their versatility and interpretability. In this paper, we first introduce InstructSafety, a safety detection framework that unifies 7 common sub-tasks for safety detection. These tasks are unified into a... | Zhexin Zhang, Jiale Cheng, Hao Sun, Jiawen Deng, Minlie Huang |  |
| 834 |  |  ["A Tale of Two Movements': Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.701) |  | 0 | Social media has become a major driver of social change, by facilitating the formation of online social movements. Automatically understanding the perspectives driving the movement and the voices opposing it, is a challenging task as annotated data is difficult to obtain. We propose a weakly supervised graph-based approach that explicitly models perspectives in #BackLivesMatter-related tweets. Our proposed approach utilizes a social-linguistic representation of the data. We convert the text to... | Shamik Roy, Dan Goldwasser |  |
| 835 |  |  [ClusterPrompt: Cluster Semantic Enhanced Prompt Learning for New Intent Discovery](https://doi.org/10.18653/v1/2023.findings-emnlp.702) |  | 0 | The discovery of new intent categories from user utterances is a crucial task in expanding agent skills. The key lies in how to efficiently solicit semantic evidence from utterances and properly transfer knowledge from existing intents to new intents. However, previous methods laid too much emphasis on relations among utterances or clusters for transfer learning, while paying less attention to the usage of semantics. As a result, these methods suffer from in-domain over-fitting and often... | Jinggui Liang, Lizi Liao |  |
| 836 |  |  [Investigating the Effect of Pre-finetuning BERT Models on NLI Involving Presuppositions](https://doi.org/10.18653/v1/2023.findings-emnlp.703) |  | 0 | We explore the connection between presupposition, discourse and sarcasm and propose to leverage that connection in a transfer learning scenario with the goal of improving the performance of NLI models on cases involving presupposition. We exploit advances in training transformer-based models that show that pre-finetuning—–i.e., finetuning the model on an additional task or dataset before the actual finetuning phase—–can help these models, in some cases, achieve a higher performance on a given... | Jad Kabbara, Jackie Chi Kit Cheung |  |
| 837 |  |  [MRRL: Modifying the Reference via Reinforcement Learning for Non-Autoregressive Joint Multiple Intent Detection and Slot Filling](https://doi.org/10.18653/v1/2023.findings-emnlp.704) |  | 0 | With the rise of non-autoregressive approach, some non-autoregressive models for joint multiple intent detection and slot filling have obtained the promising inference speed. However, most existing SLU models (1) suffer from the multi-modality problem that leads to reference intents and slots may not be suitable for training; (2) lack of alignment between the correct predictions of the two tasks, which extremely limits the overall accuracy. Therefore, in this paper, we propose Modifying the... | Xuxin Cheng, Zhihong Zhu, Bowen Cao, Qichen Ye, Yuexian Zou |  |
| 838 |  |  [DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy Slot Filling Task](https://doi.org/10.18653/v1/2023.findings-emnlp.705) |  | 0 | Recently, prompt-based generative frameworks have shown impressive capabilities in sequence labeling tasks. However, in practical dialogue scenarios, relying solely on simplistic templates and traditional corpora presents a challenge for these methods in generalizing to unknown input perturbations. To address this gap, we propose a multi-task demonstration-based generative framework for noisy slot filling, named DemoNSF. Specifically, we introduce three noisy auxiliary tasks, namely noisy... | Guanting Dong, Tingfeng Hui, Zhuoma Gongque, Jinxu Zhao, Daichi Guo, Gang Zhao, Keqing He, Weiran Xu |  |
| 839 |  |  [SHARCS: Efficient Transformers Through Routing with Dynamic Width Sub-networks](https://doi.org/10.18653/v1/2023.findings-emnlp.706) |  | 0 | We introduce SHARCS for adaptive inference that takes into account the hardness of input samples. SHARCS can train a router on any transformer network, enabling the model to direct different samples to sub-networks with varying widths. Our experiments demonstrate that: (1) SHARCS outperforms or complements existing per-sample adaptive inference methods across various classification tasks in terms of accuracy vs. FLOPs; (2) SHARCS generalizes across different architectures and can be even... | Mohammadreza Salehi, Sachin Mehta, Aditya Kusupati, Ali Farhadi, Hannaneh Hajishirzi |  |
| 840 |  |  [Always the Best Fit: Adaptive Domain Gap Filling from Causal Perspective for Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.707) |  | 0 | Cross-domain Relation Extraction aims to transfer knowledge from a source domain to a different target domain to address low-resource challenges. However, the semantic gap caused by data bias between domains is a major challenge, especially in few-shot scenarios. Previous work has mainly focused on transferring knowledge between domains through shared feature representations without analyzing the impact of each factor that may produce data bias based on the characteristics of each domain. This... | Ge Bai, Chenji Lu, Jiaxiang Geng, Shilong Li, Yidong Shi, Xiyan Liu, Ying Liu, Zhang Zhang, Ruifang Liu |  |
| 841 |  |  [MEGClass: Extremely Weakly Supervised Text Classification via Mutually-Enhancing Text Granularities](https://doi.org/10.18653/v1/2023.findings-emnlp.708) |  | 0 | Text classification is essential for organizing unstructured text. Traditional methods rely on human annotations or, more recently, a set of class seed words for supervision, which can be costly, particularly for specialized or emerging domains. To address this, using class surface names alone as extremely weak supervision has been proposed. However, existing approaches treat different levels of text granularity (documents, sentences, or words) independently, disregarding inter-granularity... | Priyanka Kargupta, Tanay Komarlu, Susik Yoon, Xuan Wang, Jiawei Han |  |
| 842 |  |  [Causal Inference from Text: Unveiling Interactions between Variables](https://doi.org/10.18653/v1/2023.findings-emnlp.709) |  | 0 | Adjusting for latent covariates is crucial for estimating causal effects from observational textual data. Most existing methods only account for confounding covariates that affect both treatment and outcome, potentially leading to biased causal effects. This bias arises from insufficient consideration of non-confounding covariates, which are relevant only to either the treatment or the outcome. In this work, we aim to mitigate the bias by unveiling interactions between different variables to... | Yuxiang Zhou, Yulan He |  |
| 843 |  |  [Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!](https://doi.org/10.18653/v1/2023.findings-emnlp.710) |  | 0 | Large Language Models (LLMs) have made remarkable strides in various tasks. Whether LLMs are competitive few-shot solvers for information extraction (IE) tasks, however, remains an open problem. In this work, we aim to provide a thorough answer to this question. Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs... | Yubo Ma, Yixin Cao, Yong Hong, Aixin Sun |  |
| 844 |  |  [Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration](https://doi.org/10.18653/v1/2023.findings-emnlp.711) |  | 0 | Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, they still possess limitations, such as failing to ask clarifying questions to ambiguous queries or refuse users’ unreasonable requests, both of which are considered as key aspects of a conversational agent’s proactivity. This raises the question of whether LLM-based conversational systems are equipped to handle proactive dialogue... | Yang Deng, Lizi Liao, Liang Chen, Hongru Wang, Wenqiang Lei, TatSeng Chua |  |
| 845 |  |  [Ecologically Valid Explanations for Label Variation in NLI](https://doi.org/10.18653/v1/2023.findings-emnlp.712) |  | 0 | Human label variation, or annotation disagreement, exists in many natural language processing (NLP) tasks, including natural language inference (NLI). To gain direct evidence of how NLI label variation arises, we build LiveNLI, an English dataset of 1,415 ecologically valid explanations (annotators explain the NLI labels they chose) for 122 MNLI items (at least 10 explanations per item). The LiveNLI explanations confirm that people can systematically vary on their interpretation and highlight... | NanJiang Jiang, Chenhao Tan, MarieCatherine de Marneffe |  |
| 846 |  |  [A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.713) |  | 0 | Semi-inductive link prediction (LP) in knowledge graphs (KG) is the task of predicting facts for new, previously unseen entities based on context information. Although new entities can be integrated by retraining the model from scratch in principle, such an approach is infeasible for large-scale KGs, where retraining is expensive and new entities may arise frequently. In this paper, we propose and describe a large-scale benchmark to evaluate semi-inductive LP models. The benchmark is based on... | Adrian Kochsiek, Rainer Gemulla |  |
| 847 |  |  [SummIt: Iterative Text Summarization via ChatGPT](https://doi.org/10.18653/v1/2023.findings-emnlp.714) |  | 0 | Existing text summarization systems have made significant progress in recent years, but typically generate summaries in a single step. The one-shot summarization setting is sometimes inadequate, however, as the generated summary may contain hallucinations or overlook important details related to the reader’s interests. In this paper, we address this limitation by proposing SummIt, an iterative text summarization framework based on large language models like ChatGPT. Our framework enables the... | Haopeng Zhang, Xiao Liu, Jiawei Zhang |  |
| 848 |  |  [Orthogonal Subspace Learning for Language Model Continual Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.715) |  | 0 | Benefiting from massive corpora and advanced hardware, large language models (LLMs) exhibit remarkable capabilities in language understanding and generation. However, their performance degrades in scenarios where multiple tasks are encountered sequentially, also known as catastrophic forgetting. In this paper, we propose orthogonal low-rank adaptation (O-LoRA), a simple and efficient approach for continual learning in language models, effectively mitigating catastrophic forgetting while... | Xiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong Bao, Rui Zheng, Qi Zhang, Tao Gui, Xuanjing Huang |  |
| 849 |  |  [Attention-Enhancing Backdoor Attacks Against BERT-based Models](https://doi.org/10.18653/v1/2023.findings-emnlp.716) |  | 0 | Recent studies have revealed that Backdoor Attacks can threaten the safety of natural language processing (NLP) models. Investigating the strategies of backdoor attacks will help to understand the model’s vulnerability. Most existing textual backdoor attacks focus on generating stealthy triggers or modifying model weights. In this paper, we directly target the interior structure of neural networks and the backdoor mechanism. We propose a novel Trojan Attention Loss (TAL), which enhances the... | Weimin Lyu, Songzhu Zheng, Lu Pang, Haibin Ling, Chao Chen |  |
| 850 |  |  [Hi-ToM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.717) |  | 0 | Theory of Mind (ToM) is the ability to reason about one’s own and others’ mental states. ToM plays a critical role in the development of intelligence, language understanding, and cognitive processes. While previous work has primarily focused on first and second-order ToM, we explore higher-order ToM, which involves recursive reasoning on others’ beliefs. %We also incorporate a new deception mechanism in ToM reasoning. We introduce Hi-ToM, a Higher Order Theory of Mind benchmark. Our... | Yufan Wu, Yinghui He, Yilin Jia, Rada Mihalcea, Yulong Chen, Naihao Deng |  |
| 851 |  |  [Image and Text: Fighting the same Battle? Super Resolution Learning for Imbalanced Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.718) |  | 0 | In this paper, we propose SRL4NLP, a new approach for data augmentation by drawing an analogy between image and text processing: Super-resolution learning. This method is based on using high-resolution images to overcome the problem of low resolution images. While this technique is a common usage in image processing when images have a low resolution or are too noisy, it has never been used in NLP. We therefore propose the first adaptation of this method for text classification and evaluate its... | Romain Meunier, Farah Benamara, Véronique Moriceau, Patricia Stolf |  |
| 852 |  |  [SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank](https://doi.org/10.18653/v1/2023.findings-emnlp.719) |  | 0 | Deep neural classifiers trained with cross-entropy loss (CE loss) often suffer from poor calibration, necessitating the task of out-of-distribution (OOD) detection. Traditional supervised OOD detection methods require expensive manual annotation of in-distribution and OOD samples. To address the annotation bottleneck, we introduce SELFOOD, a self-supervised OOD detection method that requires only in-distribution samples as supervision. We cast OOD detection as an inter-document intra-label... | Dheeraj Mekala, Adithya Samavedhi, Chengyu Dong, Jingbo Shang |  |
| 853 |  |  [Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.720) |  | 0 | Knowing how to end and resume conversations over time is a natural part of communication, allowing for discussions to span weeks, months, or years. The duration of gaps between conversations dictates which topics are relevant and which questions to ask, and dialogue systems which do not explicitly model time may generate responses that are unnatural. In this work we explore the idea of making dialogue models aware of time, and present GapChat, a multi-session dialogue dataset in which the time... | Qiang Zhang, Jason Naradowsky, Yusuke Miyao |  |
| 854 |  |  [A Structure-Aware Generative Adversarial Network for Bilingual Lexicon Induction](https://doi.org/10.18653/v1/2023.findings-emnlp.721) |  | 0 | Bilingual lexicon induction (BLI) is the task of inducing word translations with a learned mapping function that aligns monolingual word embedding spaces in two different languages. However, most previous methods treat word embeddings as isolated entities and fail to jointly consider both the intra-space and inter-space topological relations between words. This limitation makes it challenging to align words from embedding spaces with distinct topological structures, especially when the... | Bocheng Han, Qian Tao, Lusi Li, Zhihao Xiong |  |
| 855 |  |  [NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark](https://doi.org/10.18653/v1/2023.findings-emnlp.722) |  | 0 | In this position paper we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and... | Oscar Sainz, Jon Ander Campos, Iker GarcíaFerrero, Julen Etxaniz, Oier Lopez de Lacalle, Eneko Agirre |  |
| 856 |  |  [Improving Pacing in Long-Form Story Planning](https://doi.org/10.18653/v1/2023.findings-emnlp.723) |  | 0 | Existing LLM-based systems for writing long-form stories or story outlines frequently suffer from unnatural pacing, whether glossing over important events or over-elaborating on insignificant details, resulting in a jarring experience for the reader. We propose a \*\*CONC\*\*rete \*\*O\*\*utline \*\*C\*\*on\*\*T\*\*rol (CONCOCT) system to improve pacing when automatically generating story outlines. We first train a \*concreteness evaluator\* to judge which of two events is more concrete... | Yichen Wang, Kevin Yang, Xiaoming Liu, Dan Klein |  |
| 857 |  |  [Argument mining as a multi-hop generative machine reading comprehension task](https://doi.org/10.18653/v1/2023.findings-emnlp.724) |  | 0 | Argument mining (AM) is a natural language processing task that aims to generate an argumentative graph given an unstructured argumentative text. An argumentative graph that consists of argumentative components and argumentative relations contains completed information of an argument and exhibits the logic of an argument. As the argument structure of an argumentative text can be regarded as an answer to a “why” question, the whole argument structure is therefore similar to the “chain of... | Boyang Liu, Viktor Schlegel, Riza BatistaNavarro, Sophia Ananiadou |  |
| 858 |  |  [HuatuoGPT, Towards Taming Language Model to Be a Doctor](https://doi.org/10.18653/v1/2023.findings-emnlp.725) |  | 0 | In this paper, we present HuatuoGPT, a Large Language Model (LLM) for medical consultation. The core recipe of HuatuoGPT is to leverage both distilled data from \*\*ChatGPT\*\* and real-world data from \*\*doctors\*\* in the supervised fine-tuning stage. This is not only because purely using \*\*ChatGPT\*\*-distilled data might cause ‘model collapse’, but also because real-world data from \*\*doctors\*\* would be complementary to \*\*ChatGPT\*\*-distilled data. The responses from ChatGPT are... | Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Guiming Chen, Jianquan Li, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, Haizhou Li |  |
| 859 |  |  [Debias NLU Datasets via Training-free Perturbations](https://doi.org/10.18653/v1/2023.findings-emnlp.726) |  | 0 | Several recent studies have shown that advanced models for natural language understanding (NLU) are prone to capture biased features that are independent of the task but spuriously correlated to labels. Such models often perform well on in-distribution (ID) datasets but fail to generalize to out-of-distribution (OOD) datasets. Existing solutions can be separated into two orthogonal approaches: model-centric methods and data-centric methods. Model-centric methods improve OOD performance at the... | Qi Guo, Yuanhang Tang, Yawen Ouyang, Zhen Wu, Xinyu Dai |  |
| 860 |  |  [Aspect-to-Scope Oriented Multi-view Contrastive Learning for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.727) |  | 0 | Aspect-based sentiment analysis (ABSA) aims to align aspects and corresponding sentiment expressions, so as to identify the sentiment polarities of specific aspects. Most existing ABSA methods focus on mining syntactic or semantic information, which still suffers from noisy interference introduced by the attention mechanism and dependency tree when multiple aspects exist in a sentence. To address these issues, in this paper, we revisit ABSA from a novel perspective by proposing a novel... | Heyan Chai, Ziyi Yao, Siyu Tang, Ye Wang, Liqiang Nie, Binxing Fang, Qing Liao |  |
| 861 |  |  [Robustness of Named-Entity Replacements for In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.728) |  | 0 | A key feature of modern large language models (LLMs) is their ability to perform in-context learning, a prompting technique where query- answer demonstrations are shown before the final query. This allows for generalization to novel distributions at inference time where the LLM can learn new rules without parameter updates. However, the choice of demonstrations and their relationship to a particular query can have a profound impact on model accuracy, raising concerns about the true in-context... | Saeed Goodarzi, Nikhil Kagita, Dennis Minn, Shufan Wang, Roberto Dessì, Shubham Toshniwal, Adina Williams, Jack Lanchantin, Koustuv Sinha |  |
| 862 |  |  [Contrastive Learning-based Sentence Encoders Implicitly Weight Informative Words](https://doi.org/10.18653/v1/2023.findings-emnlp.729) |  | 0 | The performance of sentence encoders can be significantly improved through the simple practice of fine-tuning using contrastive loss. A natural question arises: what characteristics do models acquire during contrastive learning? This paper theoretically and experimentally shows that contrastive-based sentence encoders implicitly weight words based on information-theoretic quantities; that is, more informative words receive greater weight, while others receive less. The theory states that, in... | Hiroto Kurita, Goro Kobayashi, Sho Yokoi, Kentaro Inui |  |
| 863 |  |  [Legally Enforceable Hate Speech Detection for Public Forums](https://doi.org/10.18653/v1/2023.findings-emnlp.730) |  | 0 | Hate speech causes widespread and deep-seated societal issues. Proper enforcement of hate speech laws is key for protecting groups of people against harmful and discriminatory language. However, determining what constitutes hate speech is a complex task that is highly open to subjective interpretations. Existing works do not align their systems with enforceable definitions of hate speech, which can make their outputs inconsistent with the goals of regulators. This research introduces a new... | Chu Fei Luo, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu |  |
| 864 |  |  [ConPrompt: Pre-training a Language Model with Machine-Generated Data for Implicit Hate Speech Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.731) |  | 0 | Implicit hate speech detection is a challenging task in text classification since no explicit cues (e.g., swear words) exist in the text. While some pre-trained language models have been developed for hate speech detection, they are not specialized in implicit hate speech. Recently, an implicit hate speech dataset with a massive number of samples has been proposed by controlling machine generation. We propose a pre-training approach, ConPrompt, to fully leverage such machine-generated data.... | Youngwook Kim, Shinwoo Park, Youngsoo Namgoong, YoSub Han |  |
| 865 |  |  [Incorporating Syntactic Knowledge into Pre-trained Language Model using Optimization for Overcoming Catastrophic Forgetting](https://doi.org/10.18653/v1/2023.findings-emnlp.732) |  | 0 | Syntactic knowledge is invaluable information for many tasks which handle complex or long sentences, but typical pre-trained language models do not contain sufficient syntactic knowledge. Thus it results in failures in downstream tasks that require syntactic knowledge. In this paper, we explore additional training to incorporate syntactic knowledge to a language model. We designed four pre-training tasks that learn different syntactic perspectives. For adding new syntactic knowledge and keeping... | Ran Iwamoto, Issei Yoshida, Hiroshi Kanayama, Takuya Ohko, Masayasu Muraoka |  |
| 866 |  |  [Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?](https://doi.org/10.18653/v1/2023.findings-emnlp.733) |  | 0 | Large language models can perform downstream tasks in a zero-shot fashion, given natural language prompts that specify the desired behavior. Such prompts are typically hand engineered, but can also be learned with gradient-based methods from labeled data. However, it is underexplored what factors make the prompts effective, especially when the prompts are in natural language. In this paper, we investigate common attributes shared by effective prompts in classification problems. We first propose... | Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, Luke Zettlemoyer |  |
| 867 |  |  [Chain-of-Thought Reasoning in Tabular Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.734) |  | 0 | Tabular mathematical reasoning task requires models to perform multi-step operations including information look-up and numerical calculation, based on heterogeneous data from tables and questions. Existing solutions tend to extend chain-of-thought (CoT) reasoning into powerful large language models (LLMs) to promote multi-hop mathematical reasoning. However, such LLM-based approaches are not a viable solution in the scenario of privatization deployment or limited resources. To address this... | Mingyu Zheng, Hao Yang, Wenbin Jiang, Zheng Lin, Yajuan Lyu, Qiaoqiao She, Weiping Wang |  |
| 868 |  |  [Diffusion Language Model with Query-Document Relevance for Query-Focused Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.735) |  | 0 | Query-Focused Summarization (QFS) aims to generate summaries from source documents that can answer specific queries. Although the QFS task has gained increasing attention recently, its development is constrained by the fact that mainstream QFS models are BART variants, which are autoregressive and suffer from long-term dependencies and exposure bias. To address these problems, we adopt a diffusion language model that performs well in non-autoregressive scenarios to effectively resolve issues... | Shaoyao Huang, Luozheng Qin, Ziqiang Cao |  |
| 869 |  |  [Grounded and well-rounded: a methodological approach to the study of cross-modal and cross-lingual grounding](https://doi.org/10.18653/v1/2023.findings-emnlp.736) |  | 0 | Grounding has been argued to be a crucial component towards the development of more complete and truly semantically competent artificial intelligence systems. Literature has divided into two camps: While some argue that grounding allows for qualitatively different generalizations, others believe it can be compensated by mono-modal data quantity. Limited empirical evidence has emerged for or against either position, which we argue is due to the methodological challenges that come with studying... | Timothee Mickus, Elaine Zosa, Denis Paperno |  |
| 870 |  |  [EMO-KNOW: A Large Scale Dataset on Emotion-Cause](https://doi.org/10.18653/v1/2023.findings-emnlp.737) |  | 0 | Emotion-Cause analysis has attracted the attention of researchers in recent years. However, most existing datasets are limited in size and number of emotion categories. They often focus on extracting parts of the document that contain the emotion cause and fail to provide more abstractive, generalizable root cause. To bridge this gap, we introduce a large-scale dataset of emotion causes, derived from 9.8 million cleaned tweets over 15 years. We describe our curation process, which includes a... | Mia Huong Nguyen, Yasith Samaradivakara, Prasanth Sasikumar, Chitralekha Gupta, Suranga Nanayakkara |  |
| 871 |  |  [Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.738) |  | 0 | Parameter-shared pre-trained language models (PLMs) have emerged as a successful approach in resource-constrained environments, enabling substantial reductions in model storage and memory costs without significant performance compromise. However, it is important to note that parameter sharing does not alleviate computational burdens associated with inference, thus impeding its practicality in situations characterized by limited stringent latency requirements or computational resources. Building... | Weize Chen, Xiaoyue Xu, Xu Han, Yankai Lin, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou |  |
| 872 |  |  [Natural Response Generation for Chinese Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.739) |  | 0 | Machine reading comprehension (MRC) is an important area of conversation agents and draws a lot of attention. However, there is a notable limitation to current MRC benchmarks: The labeled answers are mostly either spans extracted from the target corpus or the choices of the given candidates, ignoring the natural aspect of high-quality responses. As a result, MRC models trained on these datasets can not generate human-like responses in real QA scenarios. To this end, we construct a new dataset... | Nuo Chen, Hongguang Li, Yinan Bao, Baoyuan Wang, Jia Li |  |
| 873 |  |  [Treepiece: Faster Semantic Parsing via Tree Tokenization](https://doi.org/10.18653/v1/2023.findings-emnlp.740) |  | 0 | Autoregressive (AR) encoder-decoder neural networks have proved successful in many NLP problems, including Semantic Parsing – a task that translates natural language to machine-readable parse trees. However, the sequential prediction process of AR models can be slow. To accelerate AR for semantic parsing, we introduce a new technique called TreePiece that tokenizes a parse tree into subtrees and generates one subtree per decoding step. On TOPv2 benchmark, TreePiece shows 4.6 times faster... | Sid Wang, Akshat Shrivastava, Aleksandr Livshits |  |
| 874 |  |  [Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking](https://doi.org/10.18653/v1/2023.findings-emnlp.741) |  | 0 | Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time-consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by... | Yuxiang Wu, Guanting Dong, Weiran Xu |  |
| 875 |  |  [Mitigating Framing Bias with Polarity Minimization Loss](https://doi.org/10.18653/v1/2023.findings-emnlp.742) |  | 0 | Framing bias plays a significant role in exacerbating political polarization by distorting the perception of actual events. Media outlets with divergent political stances often use polarized language in their reporting of the same event. We propose a new loss function that encourages the model to minimize the polarity difference between the polarized input articles to reduce framing bias. Specifically, our loss is designed to jointly optimize the model to map polarity ends bidirectionally. Our... | Yejin Bang, Nayeon Lee, Pascale Fung |  |
| 876 |  |  [Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation](https://doi.org/10.18653/v1/2023.findings-emnlp.743) |  | 0 | Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT’s causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting... | Jinglong Gao, Xiao Ding, Bing Qin, Ting Liu |  |
| 877 |  |  [Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.744) |  | 0 | Large language models (LLMs) are a promising avenue for machine translation (MT). However, current LLM-based MT systems are brittle: their effectiveness highly depends on the choice of few-shot examples and they often require extra post-processing due to overgeneration. Alternatives such as finetuning on translation instructions are computationally expensive and may weaken in-context learning capabilities, due to overspecialization. In this paper, we provide a closer look at this problem. We... | Duarte M. Alves, Nuno Miguel Guerreiro, João Alves, José Pombal, Ricardo Rei, José Guilherme Camargo de Souza, Pierre Colombo, André F. T. Martins |  |
| 878 |  |  [How Many Demonstrations Do You Need for In-context Learning?](https://doi.org/10.18653/v1/2023.findings-emnlp.745) |  | 0 | Large language models (LLMs) are capable to perform complex reasoning by in-context learning (ICL) when provided with a few input-output demonstrations (demos) and more powerful when intermediate reasoning steps (chain of thoughts (CoT)) of the demos are given. Is it necessary to use multi-demo in ICL? In this paper, we study ICL using fewer demos for each test query on the tasks in (Wei et al., 2022). Surprisingly, we do not observe significant degradation when using only one randomly chosen... | Jiuhai Chen, Lichang Chen, Chen Zhu, Tianyi Zhou |  |
| 879 |  |  [Improving word mover's distance by leveraging self-attention matrix](https://doi.org/10.18653/v1/2023.findings-emnlp.746) |  | 0 | Measuring the semantic similarity between two sentences is still an important task. The word mover’s distance (WMD) computes the similarity via the optimal alignment between the sets of word embeddings. However, WMD does not utilize word order, making it challenging to distinguish sentences with significant overlaps of similar words, even if they are semantically very different. Here, we attempt to improve WMD by incorporating the sentence structure represented by BERT’s self-attention matrix... | Hiroaki Yamagiwa, Sho Yokoi, Hidetoshi Shimodaira |  |
| 880 |  |  [Improving Span Representation by Efficient Span-Level Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.747) |  | 0 | High-quality span representations are crucial to natural language processing tasks involving span prediction and classification. Most existing methods derive a span representation by aggregation of token representations within the span. In contrast, we aim to improve span representations by considering span-span interactions as well as more comprehensive span-token interactions. Specifically, we introduce layers of span-level attention on top of a normal token-level transformer encoder. Given... | Pengyu Ji, Songlin Yang, Kewei Tu |  |
| 881 |  |  [Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.748) |  | 0 | Deception and persuasion play a critical role in long-horizon dialogues between multiple parties, especially when the interests, goals, and motivations of the participants are not aligned. Such complex tasks pose challenges for current Large Language Models (LLM) as deception and persuasion can easily mislead them, especially in long-horizon multi-party dialogues. To this end, we explore the game of Avalon: The Resistance, a social deduction game in which players must determine each other’s... | Simon Stepputtis, Joseph Campbell, Yaqi Xie, Zhengyang Qi, Wenxin Sharon Zhang, Ruiyi Wang, Sanketh Rangreji, Charles Lewis, Katia P. Sycara |  |
| 882 |  |  [Improving Sequential Model Editing with Fact Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.749) |  | 0 | The task of sequential model editing is to fix erroneous knowledge in Pre-trained Language Models (PLMs) efficiently, precisely and continuously. Although existing methods can deal with a small number of modifications, these methods experience a performance decline or require additional annotated data, when the number of edits increases. In this paper, we propose a Retrieval Augmented Sequential Model Editing framework (RASE) that leverages factual information to enhance editing generalization... | Xiaoqi Han, Ru Li, Hongye Tan, Yuanlong Wang, Qinghua Chai, Jeff Z. Pan |  |
| 883 |  |  [Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT - A Text-to-SQL Parsing Comparison](https://doi.org/10.18653/v1/2023.findings-emnlp.750) |  | 0 | The success of ChatGPT has ignited an AI race, with researchers striving to develop new large language models (LLMs) that can match or surpass the language understanding and generation abilities of commercial ones. In recent times, a number of models have emerged, claiming performance near that of GPT-3.5 or GPT-4 through various instruction-tuning methods. As practitioners of Text-to-SQL parsing, we are grateful for their valuable contributions to open-source research. However, it is important... | Shuo Sun, Yuchen Zhang, Jiahuan Yan, Yuze Gao, Donovan Ong, Bin Chen, Jian Su |  |
| 884 |  |  [KBioXLM: A Knowledge-anchored Biomedical Multilingual Pretrained Language Model](https://doi.org/10.18653/v1/2023.findings-emnlp.751) |  | 0 | Most biomedical pretrained language models are monolingual and cannot handle the growing cross-lingual requirements. The scarcity of non-English domain corpora, not to mention parallel data, poses a significant hurdle in training multilingual biomedical models. Since knowledge forms the core of domain-specific corpora and can be translated into various languages accurately, we propose a model called KBioXLM, which transforms the multilingual pretrained model XLM-R into the biomedical domain... | Lei Geng, Xu Yan, Ziqiang Cao, Juntao Li, Wenjie Li, Sujian Li, Xinjie Zhou, Yang Yang, Jun Zhang |  |
| 885 |  |  [Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?](https://doi.org/10.18653/v1/2023.findings-emnlp.752) |  | 0 | An important assumption that comes with using LLMs on psycholinguistic data has gone unverified. LLM-based predictions are based on subword tokenization, not decomposition of words into morphemes. Does that matter? We carefully test this by comparing surprisal estimates using orthographic, morphological, and BPE tokenization against reading time data. Our results replicate previous findings and provide evidence that \*in the aggregate\*, predictions using BPE tokenization do not suffer relative... | Sathvik Nair, Philip Resnik |  |
| 886 |  |  [A Zero-Shot Language Agent for Computer Control with Structured Reflection](https://doi.org/10.18653/v1/2023.findings-emnlp.753) |  | 0 | Large language models (LLMs) have shown increasing capacity at planning and executing a high-level goal in a live computer environment (e.g. MiniWoB++). To perform a task, recent works often require a model to learn from trace examples of the task via either supervised learning or few/many-shot prompting. Without these trace examples, it remains a challenge how an agent can autonomously learn and improve its control on a computer, which limits the ability of an agent to perform a new task. We... | Tao Li, Gang Li, Zhiwei Deng, Bryan Wang, Yang Li |  |
| 887 |  |  [SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF](https://doi.org/10.18653/v1/2023.findings-emnlp.754) |  | 0 | Model alignment with human preferences is an essential step in making Large Language Models (LLMs) helpful and consistent with human values. It typically consists of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) stages. However, RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time. Moreover, reward models in RLHF stage commonly rely on... | Yi Dong, Zhilin Wang, Makesh Narsimhan Sreedhar, Xianchao Wu, Oleksii Kuchaiev |  |
| 888 |  |  [IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.755) |  | 0 | The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a divide-and-conquer pipeline. In this paper, we argue that previous efforts have several inherent shortcomings: 1) They rely on domain-specific sub-question decomposing models. 2) They force models to predict the final... | Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad A. Ayyubi, KaiWei Chang, ShihFu Chang |  |
| 889 |  |  [GRI: Graph-based Relative Isomorphism of Word Embedding Spaces](https://doi.org/10.18653/v1/2023.findings-emnlp.756) |  | 0 | Automated construction of bi-lingual dictionaries using monolingual embedding spaces is a core challenge in machine translation. The end performance of these dictionaries relies on the geometric similarity of individual spaces, i.e., their degree of isomorphism. Existing attempts aimed at controlling the relative isomorphism of different spaces fail to incorporate the impact of lexically different but semantically related words in the training objective. To address this, we propose GRI that... | Muhammad Asif Ali, Yan Hu, Jianbin Qin, Di Wang |  |
| 890 |  |  [PersonaLM: Language Model Personalization via Domain-distributed Span Aggregated K-Nearest N-gram Retrieval Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.757) |  | 0 | We introduce PersonaLM - Domain-distributed Span-Aggregated K-nearest N-gram retrieval augmentation to improve language modeling for Automatic Speech Recognition (ASR) personalization. PersonaLM leverages contextually similar n-gram word frequencies for recognizing rare word patterns associated with unseen domains. It aggregates the next-word probability distribution based on the relative importance of different domains to the input query. To achieve this, we propose a Span Aggregated... | Puneet Mathur, Zhe Liu, Ke Li, Yingyi Ma, Gil Keren, Zeeshan Ahmed, Dinesh Manocha, Xuedong Zhang |  |
| 891 |  |  [Scaling Vision-Language Models with Sparse Mixture of Experts](https://doi.org/10.18653/v1/2023.findings-emnlp.758) |  | 0 | The field of natural language processing (NLP) has made significant strides in recent years, particularly in the development of large-scale vision-language models (VLMs). These models aim to bridge the gap between text and visual information, enabling a more comprehensive understanding of multimedia data. However, as these models become larger and more complex, they also become more challenging to train and deploy. One approach to addressing this challenge is the use of sparsely-gated... | Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He |  |
| 892 |  |  [Aspect-Category Enhanced Learning with a Neural Coherence Model for Implicit Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.759) |  | 0 | Aspect-based sentiment analysis (ABSA) has been widely studied since the explosive growth of social networking services. However, the recognition of implicit sentiments that do not contain obvious opinion words remains less explored. In this paper, we propose aspect-category enhanced learning with a neural coherence model (ELCoM). It captures document-level coherence by using contrastive learning, and sentence-level by a hypergraph to mine opinions from explicit sentences to aid implicit... | Jin Cui, Fumiyo Fukumoto, Xinfeng Wang, Yoshimi Suzuki, Jiyi Li, Wanzeng Kong |  |
| 893 |  |  [End-to-end Adversarial Sample Generation for Data Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.760) |  | 0 | Adversarial samples pose a significant challenge to neural inference models. In this paper, we propose a novel enhancing approach A3 for the robustness of the neural NLP models, which combines the adversarial training and data augmentation. We propose an adversarial sample generator that consists of a conditioned paraphrasing model and a condition generator. The latter aims to generate conditions which guides the paraphrasing model to generate adversarial samples. A pretrained discriminator is... | Tianyuan Liu, Yuqing Sun |  |
| 894 |  |  [Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries over Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.761) |  | 0 | Complex Query Answering (CQA) is a challenge task of Knowledge Graph (KG). Due to the incompleteness of KGs, query embedding (QE) methods have been proposed to encode queries and entities into the same embedding space, and treat logical operators as neural set operators to obtain answers. However, these methods train KG embeddings and neural set operators concurrently on both simple (one-hop) and complex (multi-hop and logical) queries, which causes performance degradation on simple queries and... | Yao Xu, Shizhu He, Cunguang Wang, Li Cai, Kang Liu, Jun Zhao |  |
| 895 |  |  [Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement](https://doi.org/10.18653/v1/2023.findings-emnlp.762) |  | 0 | To enhance the multi-step reasoning capabilities of large language models, researchers have extensively explored prompting methods, notably the Chain-of-Thought (CoT) method which explicitly elicits human-like rationales. However, they have inadvertently overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems. In this work, we start from the problem side and propose Self-Polish (SP), a novel method that facilitates the model’s reasoning by... | Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Jia Liu, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 896 |  |  [Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection](https://doi.org/10.18653/v1/2023.findings-emnlp.763) |  | 0 | It is widely acknowledged that large and sparse models have higher accuracy than small and dense models under the same model size constraints. This motivates us to train a large model and then remove its redundant neurons or weights by pruning. Most existing works pruned the networks in a deterministic way, the performance of which solely depends on a single pruning criterion and thus lacks variety. Instead, in this paper, we propose a model pruning strategy that first generates several pruning... | Jianwei Li, Weizhi Gao, Qi Lei, Dongkuan Xu |  |
| 897 |  |  [Eyes Show the Way: Modelling Gaze Behaviour for Hallucination Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.764) |  | 0 | Detecting hallucinations in natural language processing (NLP) is a critical undertaking that demands a deep understanding of both the semantic and pragmatic aspects of languages. Cognitive approaches that leverage users’ behavioural signals, such as gaze, have demonstrated effectiveness in addressing NLP tasks with similar linguistic complexities. However, their potential in the context of hallucination detection remains largely unexplored. In this paper, we propose a novel cognitive approach... | Kishan Maharaj, Ashita Saxena, Raja Kumar, Abhijit Mishra, Pushpak Bhattacharyya |  |
| 898 |  |  [Noisy Pair Corrector for Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.765) |  | 0 | Most dense retrieval models contain an implicit assumption: the training query-document pairs are exactly matched. Since it is expensive to annotate the corpus manually, training pairs in real-world applications are usually collected automatically, which inevitably introduces mismatched-pair noise. In this paper, we explore an interesting and challenging problem in dense retrieval, how to train an effective model with mismatched-pair noise. To solve this problem, we propose a novel approach... | Hang Zhang, Yeyun Gong, Xingwei He, Dayiheng Liu, Daya Guo, Jiancheng Lv, Jian Guo |  |
| 899 |  |  [Enhancing Accessible Communication: from European Portuguese to Portuguese Sign Language](https://doi.org/10.18653/v1/2023.findings-emnlp.766) |  | 0 | Portuguese Sign Language (LGP) is the official language in deaf education in Portugal. Current approaches in developing a translation system between European Portuguese and LGP rely on hand-crafted rules. In this paper, we present a fully automatic corpora-driven rule-based machine translation system between European Portuguese and LGP glosses, and also two neural machine translation models. We also contribute with the LGP-5-Domain corpus, composed of five different text domains, built with the... | Catarina Sousa, Luísa Coheur, Mara Moita |  |
| 900 |  |  [Diversifying language models for lesser-studied languages and language-usage contexts: A case of second language Korean](https://doi.org/10.18653/v1/2023.findings-emnlp.767) |  | 0 | This study investigates the extent to which currently available morpheme parsers/taggers apply to lesser-studied languages and language-usage contexts, with a focus on second language (L2) Korean. We pursue this inquiry by (1) training a neural-network model (pre-trained on first language [L1] Korean data) on varying L2 datasets and (2) measuring its morpheme parsing/POS tagging performance on L2 test sets from both the same and different sources of the L2 train sets. Results show that the L2... | Hakyung Sung, GyuHo Shin |  |
| 901 |  |  [Improving generalization in large langue model by learning prefix subspaces](https://doi.org/10.18653/v1/2023.findings-emnlp.768) |  | 0 | This article focuses on large language models (LLMs) fine-tuning in the scarce data regime (also known as “few-shot learning setting”). We propose a method to increase the generalization capabilities of LLMs based on neural network subspaces. This optimization method, recently introduced in computer vision, aims to improve model generalization by identifying wider local optima through the joint optimization of an entire simplex of models in parameter space. Although this property would be... | Louis Falissard, Vincent Guigue, Laure Soulier |  |
| 902 |  |  [Domain Adaptation for Sentiment Analysis Using Robust Internal Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.769) |  | 0 | Sentiment analysis is a costly yet necessary task for enterprises to study the opinions of their customers to improve their products and to determine optimal marketing strategies. Due to the existence of a wide range of domains across different products and services, cross-domain sentiment analysis methods have received significant attention. These methods mitigate the domain gap between different applications by training cross-domain generalizable classifiers which relax the need for data... | Mohammad Rostami, Digbalay Bose, Shrikanth Narayanan, Aram Galstyan |  |
| 903 |  |  [KeFVP: Knowledge-enhanced Financial Volatility Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.770) |  | 0 | Financial volatility prediction is vital for indicating a company’s risk profile. Transcripts of companies’ earnings calls are important unstructured data sources to be utilized to access companies’ performance and risk profiles. However, current works ignore the role of financial metrics knowledge (such as EBIT, EPS, and ROI) in transcripts, which is crucial for understanding companies’ performance, and little consideration is given to integrating text and price information. In this work, we... | Hao Niu, Yun Xiong, Xiaosu Wang, Wenjing Yu, Yao Zhang, Weizu Yang |  |
| 904 |  |  [A Frustratingly Easy Plug-and-Play Detection-and-Reasoning Module for Chinese Spelling Check](https://doi.org/10.18653/v1/2023.findings-emnlp.771) |  | 0 | In recent years, Chinese Spelling Check (CSC) has been greatly improved by designing task-specific pre-training methods or introducing auxiliary tasks, which mostly solve this task in an end-to-end fashion. In this paper, we propose to decompose the CSC workflow into detection, reasoning, and searching subtasks so that the rich external knowledge about the Chinese language can be leveraged more directly and efficiently. Specifically, we design a plug-and-play detection-and-reasoning module that... | Haojing Huang, Jingheng Ye, Qingyu Zhou, Yinghui Li, Yangning Li, Feng Zhou, HaiTao Zheng |  |
| 905 |  |  [Asking Clarification Questions to Handle Ambiguity in Open-Domain QA](https://doi.org/10.18653/v1/2023.findings-emnlp.772) |  | 0 | Ambiguous questions persist in open-domain question answering, because formulating a precise question with a unique answer is often challenging. Previous works have tackled this issue by asking disambiguated questions for all possible interpretations of the ambiguous question. Instead, we propose to ask a clarification question, where the user’s response will help identify the interpretation that best aligns with the user’s intention. We first present CAmbigNQ, a dataset consisting of 5,653... | Dongryeol Lee, Segwang Kim, Minwoo Lee, Hwanhee Lee, Joonsuk Park, SangWoo Lee, Kyomin Jung |  |
| 906 |  |  [Addressing the Length Bias Challenge in Document-Level Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.773) |  | 0 | Document-level neural machine translation (DNMT) has shown promising results by incorporating context information through increased maximum lengths of source and target sentences. However, this approach also introduces a length bias problem, whereby DNMT suffers from significant translation quality degradation when decoding sentences that are much shorter or longer than the maximum sentence length during training, i.e., the length bias problem. To prevent the model from neglecting shorter... | Zhuocheng Zhang, Shuhao Gu, Min Zhang, Yang Feng |  |
| 907 |  |  [EconBERTa: Towards Robust Extraction of Named Entities in Economics](https://doi.org/10.18653/v1/2023.findings-emnlp.774) |  | 0 | Adapting general-purpose language models has proven to be effective in tackling downstream tasks within specific domains. In this paper, we address the task of extracting entities from the economics literature on impact evaluation. To this end, we release EconBERTa, a large language model pretrained on scientific publications in economics, and ECON-IE, a new expert-annotated dataset of economics abstracts for Named Entity Recognition (NER). We find that EconBERTa reaches state-of-the-art... | Karim Lasri, Pedro Vitor Quinta de Castro, Mona Schirmer, Luis Eduardo San Martin, Linxi Wang, Tomás Dulka, Haaya Naushan, John PouguéBiyong, Arianna Legovini, Samuel Fraiberger |  |
| 908 |  |  [Consonant is all you need: a compact representation of English text for efficient NLP](https://doi.org/10.18653/v1/2023.findings-emnlp.775) |  | 0 | In natural language processing (NLP), the representation of text plays a crucial role in various tasks such as language modeling, sentiment analysis, and machine translation. The standard approach is to represent text in the same way as we, as humans, read and write. In this paper, we propose a novel approach to represent text with only consonants which presents a compact representation of English text that offers improved efficiency without sacrificing performance. We exploit the fact that... | Maged Saeed AlShaibani, Irfan Ahmad |  |
| 909 |  |  [Detrimental Contexts in Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.776) |  | 0 | For knowledge intensive NLP tasks, it has been widely accepted that accessing more information is a contributing factor to improvements in the model’s end-to-end performance. However, counter-intuitively, too much context can have a negative impact on the model when evaluated on common question answering (QA) datasets. In this paper, we analyze how passages can have a detrimental effect on retrieve-then-read architectures used in question answering. Our empirical evidence indicates that the... | Philhoon Oh, James Thorne |  |
| 910 |  |  [PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for Languages in India](https://doi.org/10.18653/v1/2023.findings-emnlp.777) |  | 0 | This paper introduces PMIndiaSum, a multilingual and massively parallel summarization corpus focused on languages in India. Our corpus provides a training and testing ground for four language families, 14 languages, and the largest to date with 196 language pairs. We detail our construction workflow including data acquisition, processing, and quality assurance. Furthermore, we publish benchmarks for monolingual, cross-lingual, and multilingual summarization by fine-tuning, prompting, as well as... | Ashok Urlana, Pinzhen Chen, Zheng Zhao, Shay B. Cohen, Manish Shrivastava, Barry Haddow |  |
| 911 |  |  [Beyond Labels: Empowering Human Annotators with Natural Language Explanations through a Novel Active-Learning Architecture](https://doi.org/10.18653/v1/2023.findings-emnlp.778) |  | 0 | Real-world domain experts (e.g., doctors) rarely annotate only a decision label in their day-to-day workflow without providing explanations. Yet, existing low-resource learning techniques, such as Active Learning (AL), that aim to support human annotators mostly focus on the label while neglecting the natural language explanation of a data point. This work proposes a novel AL architecture to support experts’ real-world need for label and explanation annotations in low-resource scenarios. Our AL... | Bingsheng Yao, Ishan Jindal, Lucian Popa, Yannis Katsis, Sayan Ghosh, Lihong He, Yuxuan Lu, Shashank Srivastava, Yunyao Li, James A. Hendler, Dakuo Wang |  |
| 912 |  |  [Decoding Stumpers: Large Language Models vs. Human Problem-Solvers](https://doi.org/10.18653/v1/2023.findings-emnlp.779) |  | 0 | This paper investigates the problem-solving capabilities of Large Language Models (LLMs) by evaluating their performance on stumpers, unique single-step intuition problems that pose challenges for human solvers but are easily verifiable. We compare the performance of four state-of-the-art LLMs (Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) to human participants. Our findings reveal that the new-generation LLMs excel in solving stumpers and surpass human performance. However, humans exhibit... | Alon Goldstein, Miriam Havin, Roi Reichart, Ariel Goldstein |  |
| 913 |  |  [Efficient Cross-Task Prompt Tuning for Few-Shot Conversational Emotion Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.780) |  | 0 | Emotion Recognition in Conversation (ERC) has been widely studied due to its importance in developing emotion-aware empathetic machines. The rise of pre-trained language models (PLMs) has further pushed the limit of ERC performance. However, most recent works on ERC using PLMs are heavily data-driven, and requires fine-tuning the entire PLMs. To improve both sample and computational efficiency, we propose a derivative-free optimization method called Cross-Task Prompt Tuning (CTPT) for few-shot... | Yige Xu, Zhiwei Zeng, Zhiqi Shen |  |
| 914 |  |  [SYMPTOMIFY: Transforming Symptom Annotations with Language Model Knowledge Harvesting](https://doi.org/10.18653/v1/2023.findings-emnlp.781) |  | 0 | Given the high-stakes nature of healthcare decision-making, we aim to improve the efficiency of human annotators rather than replacing them with fully automated solutions. We introduce a new comprehensive resource, SYMPTOMIFY, a dataset of annotated vaccine adverse reaction reports detailing individual vaccine reactions. The dataset, consisting of over 800k reports, surpasses previous datasets in size. Notably, it features reasoning-based explanations alongside background knowledge obtained via... | Bosung Kim, Ndapa Nakashole |  |
| 915 |  |  [TokenDrop + BucketSampler: Towards Efficient Padding-free Fine-tuning of Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.782) |  | 0 | The great success of Language Models (LMs) for various Natural Language Processing (NLP) tasks is accompanied by computational challenges during both pre-training and fine-tuning. Pre-training has attracted significant attention due to its huge computational footprint. We focus on the fine-tuning of pre-trained LMs, which is expected to be performed much more frequently as the pre-trained models are adapted to downstream tasks. During fine-tuning, the presence of variable-length input sequences... | Amrit Nagarajan, Anand Raghunathan |  |
| 916 |  |  [Unified Representation for Non-compositional and Compositional Expressions](https://doi.org/10.18653/v1/2023.findings-emnlp.783) |  | 0 | Accurate processing of non-compositional language relies on generating good representations for such expressions. In this work, we study the representation of language non-compositionality by proposing a language model, PIER+, that builds on BART and can create semantically meaningful and contextually appropriate representations for English potentially idiomatic expressions (PIEs). PIEs are characterized by their non-compositionality and contextual ambiguity in their literal and idiomatic... | Ziheng Zeng, Suma Bhat |  |
| 917 |  |  [Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.784) |  | 0 | Retrieval-augmented generation models augment knowledge encoded in a language model by providing additional relevant external knowledge (context) during generation. Although it has been shown that the quantity and quality of context impact the performance of retrieval-augmented generation models during inference, limited research explores how these characteristics affect model training. This paper explores how context quantity and quality during model training affect the performance of... | Kosuke Akimoto, Kunihiro Takeoka, Masafumi Oyamada |  |
| 918 |  |  [Error Detection for Text-to-SQL Semantic Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.785) |  | 0 | Despite remarkable progress in text-to-SQL semantic parsing in recent years, the performance of existing parsers is still far from perfect. Specifically, modern text-to-SQL parsers based on deep learning are often over-confident, thus casting doubt on their trustworthiness when deployed for real use. In this paper, we propose a parser-independent error detection model for text-to-SQL semantic parsing. Using a language model of code as its bedrock, we enhance our error detection model with graph... | Shijie Chen, Ziru Chen, Huan Sun, Yu Su |  |
| 919 |  |  [Ultra-Fine Entity Typing with Prior Knowledge about Labels: A Simple Clustering Based Strategy](https://doi.org/10.18653/v1/2023.findings-emnlp.786) |  | 0 | Ultra-fine entity typing (UFET) is the task of inferring the semantic types from a large set of fine-grained candidates that apply to a given entity mention. This task is especially challenging because we only have a small number of training examples for many types, even with distant supervision strategies. State-of-the-art models, therefore, have to rely on prior knowledge about the type labels in some way. In this paper, we show that the performance of existing methods can be improved using a... | Na Li, Zied Bouraoui, Steven Schockaert |  |
| 920 |  |  [Multilingual Coarse Political Stance Classification of Media. The Editorial Line of a ChatGPT and Bard Newspaper](https://doi.org/10.18653/v1/2023.findings-emnlp.787) |  | 0 | Neutrality is difficult to achieve and, in politics, subjective. Traditional media typically adopt an editorial line that can be used by their potential readers as an indicator of the media bias. Several platforms currently rate news outlets according to their political bias. The editorial line and the ratings help readers in gathering a balanced view of news. But in the advent of instruction-following language models, tasks such as writing a newspaper article can be delegated to computers.... | Cristina EspañaBonet |  |
| 921 |  |  [Do "English" Named Entity Recognizers Work Well on Global Englishes?](https://doi.org/10.18653/v1/2023.findings-emnlp.788) |  | 0 | The vast majority of the popular English named entity recognition (NER) datasets contain American or British English data, despite the existence of many global varieties of English. As such, it is unclear whether they generalize for analyzing use of English globally. To test this, we build a newswire dataset, the Worldwide English NER Dataset, to analyze NER model performance on low-resource English variants from around the world. We test widely used NER toolkits and transformer models,... | Alexander Shan, John Bauer, Riley Carlson, Christopher D. Manning |  |
| 922 |  |  [Affective and Dynamic Beam Search for Story Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.789) |  | 0 | Storytelling’s captivating potential makes it a fascinating research area, with implications for entertainment, education, therapy, and cognitive studies. In this paper, we propose Affective Story Generator (AffGen) for generating interesting narratives. AffGen introduces ‘intriguing twists’ in narratives by employing two novel techniques—Dynamic Beam Sizing and Affective Reranking. Dynamic Beam Sizing encourages less predictable, more captivating word choices using a contextual multi-arm... | Tenghao Huang, Ehsan Qasemi, Bangzheng Li, He Wang, Faeze Brahman, Muhao Chen, Snigdha Chaturvedi |  |
| 923 |  |  [Multiview Clickbait Detection via Jointly Modeling Subjective and Objective Preference](https://doi.org/10.18653/v1/2023.findings-emnlp.790) |  | 0 | Clickbait posts tend to spread inaccurate or misleading information to manipulate people’s attention and emotions, which greatly harms the credibility of social media. Existing clickbait detection models rely on analyzing the objective semantics in posts or correlating posts with article content only. However, these models fail to identify and exploit the manipulation intention of clickbait from a user’s subjective perspective, leading to limited capability to explore comprehensive clues of... | Chongyang Shi, Yijun Yin, Qi Zhang, Liang Xiao, Usman Naseem, Shoujin Wang, Liang Hu |  |
| 924 |  |  [Let's Synthesize Step by Step: Iterative Dataset Synthesis with Large Language Models by Extrapolating Errors from Small Models](https://doi.org/10.18653/v1/2023.findings-emnlp.791) |  | 0 | \*Data Synthesis\* is a promising way to train a small model with very little labeled data. One approach for data synthesis is to leverage the rich knowledge from large language models to synthesize pseudo training examples for small models, making it possible to achieve both data and compute efficiency at the same time. However, a key challenge in data synthesis is that the synthesized dataset often suffers from a large distributional discrepancy from the \*real task\* data distribution. Thus,... | Ruida Wang, Wangchunshu Zhou, Mrinmaya Sachan |  |
| 925 |  |  [Identifying Early Maladaptive Schemas from Mental Health Question Texts](https://doi.org/10.18653/v1/2023.findings-emnlp.792) |  | 0 | In Psychotherapy, maladaptive schemas– negative perceptions that an individual has of the self, others, or the world that endure despite objective reality–often lead to resistance to treatments and relapse of mental health issues such as depression, anxiety, panic attacks etc. Identification of early maladaptive schemas (EMS) is thus a crucial step during Schema Therapy-based counseling sessions, where patients go through a detailed and lengthy EMS questionnaire. However, such an approach is... | Sujatha Das Gollapalli, Beng Heng Ang, SeeKiong Ng |  |
| 926 |  |  [Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning](https://doi.org/10.18653/v1/2023.findings-emnlp.793) |  | 0 | Augmenting pretrained language models (LMs) with a vision encoder (e.g., Flamingo) has obtained state-of-the-art results in image-to-text generation. However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual concepts and very rich text descriptions. Additionally, they are inefficient in incorporating new data, requiring a computational-expensive fine-tuning process. In this work, we introduce a... | Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, DeAn Huang, Linxi Fan, Zhiding Yu, Shiyi Lan, Bo Li, Mohammad Shoeybi, MingYu Liu, Yuke Zhu, Bryan Catanzaro, Chaowei Xiao, Anima Anandkumar |  |
| 927 |  |  [Syntax Matters: Towards Spoken Language Understanding via Syntax-Aware Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.794) |  | 0 | Spoken Language Understanding (SLU), a crucial component of task-oriented dialogue systems, has consistently garnered attention from both academic and industrial communities. Although incorporating syntactic information into models has the potential to enhance the comprehension of user utterances and yield impressive results, its application in SLU systems remains largely unexplored. In this paper, we propose a carefully designed model termed Syntax-aware attention (SAT) to enhance SLU, where... | Yifeng Xie, Zhihong Zhu, Xuxin Cheng, Zhiqi Huang, Dongsheng Chen |  |
| 928 |  |  [Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate](https://doi.org/10.18653/v1/2023.findings-emnlp.795) |  | 0 | Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive performance in complex reasoning tasks. However, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way. In this work, we explore testing LLMs’ reasoning by engaging with them in a debate-like conversation, where given a question, the LLM and the user need to discuss to make the correct decision... | Boshi Wang, Xiang Yue, Huan Sun |  |
| 929 |  |  [Using In-Context Learning to Improve Dialogue Safety](https://doi.org/10.18653/v1/2023.findings-emnlp.796) |  | 0 | While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, often perpetuating social biases or stereotypes. We investigate a retrieval-based approach for reducing bias and toxicity in responses from chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an... | Nicholas Meade, Spandana Gella, Devamanyu Hazarika, Prakhar Gupta, Di Jin, Siva Reddy, Yang Liu, Dilek HakkaniTur |  |
| 930 |  |  [HEAR: Hearing Enhanced Audio Response for Video-grounded Dialogue](https://doi.org/10.18653/v1/2023.findings-emnlp.797) |  | 0 | Video-grounded Dialogue (VGD) aims to answer questions regarding a given multi-modal input comprising video, audio, and dialogue history. Although there have been numerous efforts in developing VGD systems to improve the quality of their responses, existing systems are competent only to incorporate the information in the video and text and tend to struggle in extracting the necessary information from the audio when generating appropriate responses to the question. The VGD system seems to be... | Sunjae Yoon, Dahyun Kim, Eunseop Yoon, Hee Suk Yoon, Junyeong Kim, Chang Dong Yoo |  |
| 931 |  |  [Improving Consistency for Text Summarization with Energy Functions](https://doi.org/10.18653/v1/2023.findings-emnlp.798) |  | 0 | Current abstractive summarization models often generate inconsistent content, i.e. texts that are not directly inferable from the source document, are not consistent with respect to world knowledge, or are self-contradictory. These inconsistencies motivate a new consistency taxonomy that we define as faithfulness, factuality, and self-supportiveness. However, most recent work on reducing inconsistency in document summarization only focuses on faithfulness detection and correction while ignoring... | Qi Zeng, Qingyu Yin, Zheng Li, Yifan Gao, Sreyashi Nag, Zhengyang Wang, Bing Yin, Heng Ji, Chao Zhang |  |
| 932 |  |  [Defining a New NLP Playground](https://doi.org/10.18653/v1/2023.findings-emnlp.799) |  | 0 | The recent explosion of performance of large language models (LLMs) has changed the field of Natural Language Processing (NLP) more abruptly and seismically than any other shift in the field’s 80 year history. This has resulted in concerns that the field will become homogenized and resource-intensive. This new status quo has put many academic researchers, especially PhD students, at a disadvantage. This paper aims to define a new NLP playground by proposing 20+ PhD-dissertation-worthy research... | Sha Li, Chi Han, Pengfei Yu, Carl Edwards, Manling Li, Xingyao Wang, Yi Ren Fung, Charles Yu, Joel R. Tetreault, Eduard H. Hovy, Heng Ji |  |
| 933 |  |  [UPTON: Preventing Authorship Leakage from Public Text Release via Data Poisoning](https://doi.org/10.18653/v1/2023.findings-emnlp.800) |  | 0 | Consider a scenario where an author (e.g., activist, whistle-blower) with many public writings wishes to write “anonymously” when attackers may have already built an authorship attribution (AA) model based off of public writings including those of the author. To enable her wish, we ask a question “can one make the publicly released writings, T , unattributable so that AA models trained on T cannot attribute its authorship well?” Toward this question, we present a novel solution, UPTON, that... | Ziyao Wang, Thai Le, Dongwon Lee |  |
| 934 |  |  [IAEval: A Comprehensive Evaluation of Instance Attribution on Natural Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.801) |  | 0 | Instance attribution (IA) aims to identify the training instances leading to the prediction of a test example, helping researchers understand the dataset better and optimize data processing. While many IA methods have been proposed recently, how to evaluate them still remains open. Previous evaluations of IA only focus on one or two dimensions and are not comprehensive. In this work, we introduce IAEval for IA methods, a systematic and comprehensive evaluation scheme covering four significant... | Peijian Gu, Yaozong Shen, Lijie Wang, Quan Wang, Hua Wu, Zhendong Mao |  |
| 935 |  |  [Scene Graph Enhanced Pseudo-Labeling for Referring Expression Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.802) |  | 0 | Referring Expression Comprehension (ReC) is a task that involves localizing objects in images based on natural language expressions. Most ReC methods typically approach the task as a supervised learning problem. However, the need for costly annotations, such as clear image-text pairs or region-text pairs, hinders the scalability of existing approaches. In this work, we propose a novel scene graph-based framework that automatically generates high-quality pseudo region-query pairs. Our method... | Cantao Wu, Yi Cai, Liuwu Li, Jiexin Wang |  |
| 936 |  |  [Noisy Self-Training with Synthetic Queries for Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.803) |  | 0 | Although existing neural retrieval models reveal promising results when training data is abundant and the performance keeps improving as training data increases, collecting high-quality annotated data is prohibitively costly. To this end, we introduce a novel noisy self-training framework combined with synthetic queries, showing that neural retrievers can be improved in a self-evolution manner with no reliance on any external models. Experimental results show that our method improves... | Fan Jiang, Tom Drummond, Trevor Cohn |  |
| 937 |  |  [Leveraging GPT-4 for Automatic Translation Post-Editing](https://doi.org/10.18653/v1/2023.findings-emnlp.804) |  | 0 | While Neural Machine Translation (NMT) represents the leading approach to Machine Translation (MT), the outputs of NMT models still require translation post-editing to rectify errors and enhance quality under critical settings. In this work, we formalize the task of direct translation post-editing with Large Language Models (LLMs) and explore the use of GPT-4 to automatically post-edit NMT outputs across several language pairs. Our results demonstrate that GPT-4 is adept at translation... | Vikas Raunak, Amr Sharaf, Yiren Wang, Hany Hassan Awadalla, Arul Menezes |  |
| 938 |  |  [Uniform Complexity for Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.805) |  | 0 | Large language models (LLMs) have shown promising results in a wide array of generative NLP tasks, such as summarization and machine translation. In the context of narrative generation, however, existing models still do not capture factors that contribute to producing consistent text. For instance, it is logical that a piece of text or a story should be uniformly readable throughout and that this form of complexity should be controllable. As such, if the complexity of an input text prompt is... | Joseph Marvin Imperial, Harish Tayyar Madabushi |  |
| 939 |  |  [Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.806) |  | 0 | Large Language Models (LLMs), such as ChatGPT, greatly empower dialogue systems with strong language understanding and generation capabilities. However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status exhibited in the context. Such in-depth dialogue scenarios are challenging for existing LLMs to figure out the user’s hidden needs and respond satisfactorily through a... | Hongru Wang, Rui Wang, Fei Mi, Yang Deng, Zezhong Wang, Bin Liang, Ruifeng Xu, KamFai Wong |  |
| 940 |  |  [CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.807) |  | 0 | Existing works on Aspect Sentiment Triplet Extraction (ASTE) explicitly focus on developing more efficient fine-tuning techniques for the task. Instead, our motivation is to come up with a generic approach that can improve the downstream performances of multiple ABSA tasks simultaneously. Towards this, we present CONTRASTE, a novel pre-training strategy using CONTRastive learning to enhance the ASTE performance. While we primarily focus on ASTE, we also demonstrate the advantage of our proposed... | Rajdeep Mukherjee, Nithish Kannen, Saurabh Kumar Pandey, Pawan Goyal |  |
| 941 |  |  [Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompts](https://doi.org/10.18653/v1/2023.findings-emnlp.808) |  | 0 | Continual pre-training has been urgent for adapting a pre-trained model to a multitude of domains and tasks in the fast-evolving world. In practice, a continually pre-trained model is expected to demonstrate not only greater capacity when fine-tuned on pre-trained domains but also a non-decreasing performance on unseen ones. In this work, we first investigate such anytime fine-tuning effectiveness of existing continual pre-training approaches, concluding with unanimously decreased performance... | Gangwei Jiang, Caigao Jiang, Siqiao Xue, James Zhang, Jun Zhou, Defu Lian, Ying Wei |  |
| 942 |  |  [Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts](https://doi.org/10.18653/v1/2023.findings-emnlp.809) |  | 0 | Visual question answering (VQA) is the task of answering questions about an image. The task assumes an understanding of both the image and the question to provide a natural language answer. VQA has gained popularity in recent years due to its potential applications in a wide range of fields, including robotics, education, and healthcare. In this paper, we focus on knowledge-augmented VQA, where answering the question requires commonsense knowledge, world knowledge, and reasoning about ideas and... | Deepanway Ghosal, Navonil Majumder, Roy KaWei Lee, Rada Mihalcea, Soujanya Poria |  |
| 943 |  |  [XLS-R fine-tuning on noisy word boundaries for unsupervised speech segmentation into words](https://doi.org/10.18653/v1/2023.findings-emnlp.810) |  | 0 | Due to the absence of explicit word boundaries in the speech stream, the task of segmenting spoken sentences into word units without text supervision is particularly challenging. In this work, we leverage the most recent self-supervised speech models that have proved to quickly adapt to new tasks through fine-tuning, even in low resource conditions. Taking inspiration from semi-supervised learning, we fine-tune an XLS-R model to predict word boundaries themselves produced by top-tier speech... | Robin Algayres, Pablo DiegoSimon, Benoît Sagot, Emmanuel Dupoux |  |
| 944 |  |  [Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data](https://doi.org/10.18653/v1/2023.findings-emnlp.811) |  | 0 | Chain-of-thought (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in complex reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications where labeled data is available without rational chains. This paper proposes a new strategy, AutomateCoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human... | Kashun Shum, Shizhe Diao, Tong Zhang |  |
| 945 |  |  [What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations](https://doi.org/10.18653/v1/2023.findings-emnlp.812) |  | 0 | Moral or ethical judgments rely heavily on the specific contexts in which they occur. Understanding varying shades of defeasible contextualizations (i.e., additional information that strengthens or attenuates the moral acceptability of an action) is critical to accurately represent the subtlety and intricacy of grounded human moral judgment in real-life scenarios. We introduce defeasible moral reasoning: a task to provide grounded contexts that make an action more or less morally acceptable,... | Kavel Rao, Liwei Jiang, Valentina Pyatkin, Yuling Gu, Niket Tandon, Nouha Dziri, Faeze Brahman, Yejin Choi |  |
| 946 |  |  [An Empirical Study on Multiple Knowledge from ChatGPT for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2023.findings-emnlp.813) |  | 0 | Multiple knowledge (e.g., co-reference, topics, emotional causes, etc) has been demonstrated effective for emotion detection. However, exploring this knowledge in Emotion Recognition in Conversations (ERC) is currently a blank slate due to the lack of annotated data and the high cost involved in obtaining such knowledge. Fortunately, the emergence of Large Language Models (LLMs) holds promise in filling this void. Therefore, we propose a Multiple Knowledge Fusion Model (MKFM) to effectively... | Geng Tu, Bin Liang, Bing Qin, KamFai Wong, Ruifeng Xu |  |
| 947 |  |  [Exploiting Contrastive Learning and Numerical Evidence for Confusing Legal Judgment Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.814) |  | 0 | Given the fact description text of a legal case, legal judgment prediction (LJP) aims to predict the case’s charge, applicable law article, and term of penalty. A core problem of LJP is distinguishing confusing legal cases where only subtle text differences exist. Previous studies fail to distinguish different classification errors with a standard cross-entropy classification loss and ignore the numbers in the fact description for predicting the term of penalty. To tackle these issues, in this... | Leilei Gan, Baokui Li, Kun Kuang, Yating Zhang, Lei Wang, Anh Tuan Luu, Yi Yang, Fei Wu |  |
| 948 |  |  [One For All & All For One: Bypassing Hyperparameter Tuning with Model Averaging for Cross-Lingual Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.815) |  | 0 | Multilingual language models enable zero-shot cross-lingual transfer (ZS-XLT): fine-tuned on sizable source-language task data, they perform the task in target languages without labeled instances. The effectiveness of ZS-XLT hinges on the linguistic proximity between languages and the amount of pretraining data for a language. Because of this, model selection based on source-language validation is unreliable: it picks model snapshots with suboptimal target-language performance. As a remedy,... | Fabian David Schmidt, Ivan Vulic, Goran Glavas |  |
| 949 |  |  [Dimensions of Online Conflict: Towards Modeling Agonism](https://doi.org/10.18653/v1/2023.findings-emnlp.816) |  | 0 | Agonism plays a vital role in democratic dialogue by fostering diverse perspectives and robust discussions. Within the realm of online conflict there is another type: hateful antagonism, which undermines constructive dialogue. Detecting conflict online is central to platform moderation and monetization. It is also vital for democratic dialogue, but only when it takes the form of agonism. To model these two types of conflict, we collected Twitter conversations related to trending controversial... | Matt Canute, Mali Jin, hannah holtzclaw, Alberto Lusoli, Philippa R. Adams, Mugdha Pandya, Maite Taboada, Diana Maynard, Wendy Hui Kyong Chun |  |
| 950 |  |  [Learning under Label Proportions for Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.817) |  | 0 | We present one of the preliminary NLP works under the challenging setup of Learning from Label Proportions (LLP), where the data is provided in an aggregate form called bags and only the proportion of samples in each class as the ground truth. This setup is inline with the desired characteristics of training models under Privacy settings and Weakly supervision. By characterizing some irregularities of the most widely used baseline technique DLLP, we propose a novel formulation that is also... | Jatin Chauhan, Xiaoxuan Wang, Wei Wang |  |
| 951 |  |  [MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition](https://doi.org/10.18653/v1/2023.findings-emnlp.818) |  | 0 | Humans have the ability to learn novel compositional concepts by recalling primitive concepts acquired from past experience and generalizing these primitive concepts to novel compositions. Inspired by the above human’s compositional learning procedure, in this paper, we propose MetaReVision, a retrievalenhanced meta-learning model to solve the visually grounded compositional concept learning problem. The proposed MetaReVision consists of a retrieval module and a meta-learning module which are... | Guangyue Xu, Parisa Kordjamshidi, Joyce Chai |  |
| 952 |  |  [PR-MCS: Perturbation Robust Metric for MultiLingual Image Captioning](https://doi.org/10.18653/v1/2023.findings-emnlp.819) |  | 0 | Vulnerability to lexical perturbation is a critical weakness of automatic evaluation metrics for image captioning. This paper proposes Perturbation Robust Multi-Lingual CLIPScore(PR-MCS), which exhibits robustness to such perturbations, as a novel reference-free image captioning metric applicable to multiple languages. To achieve perturbation robustness, we fine-tune the text encoder of CLIP with our language-agnostic method to distinguish the perturbed text from the original text. To verify... | Yongil Kim, Yerin Hwang, Hyeongu Yun, Seunghyun Yoon, Trung Bui, Kyomin Jung |  |
| 953 |  |  [Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.820) |  | 0 | Scientific literature understanding tasks have gained significant attention due to their potential to accelerate scientific discovery. Pre-trained language models (LMs) have shown effectiveness in these tasks, especially when tuned via contrastive learning. However, jointly utilizing pre-training data across multiple heterogeneous tasks (e.g., extreme multi-label paper classification, citation prediction, and literature search) remains largely unexplored. To bridge this gap, we propose a... | Yu Zhang, Hao Cheng, Zhihong Shen, Xiaodong Liu, YeYi Wang, Jianfeng Gao |  |
| 954 |  |  [BLM-s/lE: A structured dataset of English spray-load verb alternations for testing generalization in LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.821) |  | 0 | Current NLP models appear to be achieving performance comparable to human capabilities on well-established benchmarks. New benchmarks are now necessary to test deeper layers of understanding of natural languages by these models. Blackbird’s Language Matrices are a recently developed framework that draws inspiration from tests of human analytic intelligence. The BLM task has revealed that successful performances in previously studied linguistic problems do not yet stem from a deep understanding... | Giuseppe Samo, Vivi Nastase, Chunyang Jiang, Paola Merlo |  |
| 955 |  |  [Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt](https://doi.org/10.18653/v1/2023.findings-emnlp.822) |  | 0 | Enhancing the zero-shot performance of instruction-following models requires heavy computation, either by scaling the total number of training datasets or the model size. In this work, we explore how retrieval of soft prompts obtained through prompt tuning can efficiently assist hard prompts in zero-shot task generalization. Specifically, we train soft prompt embeddings for each prompt through prompt tuning, store the samples of the training instances mapped with the prompt embeddings, and... | Seonghyeon Ye, Joel Jang, Doyoung Kim, Yongrae Jo, Minjoon Seo |  |
| 956 |  |  [Geographical Erasure in Language Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.823) |  | 0 | Large language models (LLMs) encode vast amounts of world knowledge. However, since these models are trained on large swaths of internet data, they are at risk of inordinately capturing information about dominant groups. This imbalance can propagate into generated language. In this work, we study and operationalise a form of geographical erasure wherein language models underpredict certain countries. We demonstrate consistent instances of erasure across a range of LLMs. We discover that erasure... | Pola Schwöbel, Jacek Golebiowski, Michele Donini, Cédric Archambeau, Danish Pruthi |  |
| 957 |  |  [Can Foundation Models Watch, Talk and Guide You Step by Step to Make a Cake?](https://doi.org/10.18653/v1/2023.findings-emnlp.824) |  | 0 | Despite tremendous advances in AI, it remains a significant challenge to develop interactive task guidance systems that can offer situated, personalized guidance and assist humans in various tasks. These systems need to have a sophisticated understanding of the user as well as the environment, and make timely accurate decisions on when and what to say. To address this issue, we created a new multimodal benchmark dataset, Watch, Talk and Guide (WTaG) based on natural interaction between a human... | Yuwei Bao, Keunwoo Peter Yu, Yichi Zhang, Shane Storks, Itamar BarYossef, Alexander De La Iglesia, Megan Su, XiaoLin Zheng, Joyce Chai |  |
| 958 |  |  [Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?](https://doi.org/10.18653/v1/2023.findings-emnlp.825) |  | 0 | There have been a lot of interest in the scaling properties of Transformer models. However, not much has been done on the front of investigating the effect of scaling properties of different inductive biases and model architectures. Do model architectures scale differently? If so, how does inductive bias affect scaling behaviour? How does this influence upstream (pretraining) and downstream (transfer)? This paper conducts a systematic study of scaling behaviour of ten diverse model... | Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q. Tran, Dani Yogatama, Donald Metzler |  |
| 959 |  |  [Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.826) |  | 0 | Large language models (LLMs) demonstrate impressive multilingual capability, but their performance varies substantially across different languages. In this work, we introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to systematically improve the multilingual capability of LLMs. Specifically, XLT is a generic template prompt that stimulates cross-lingual and logical reasoning skills to enhance task performance across languages. We conduct comprehensive... | Haoyang Huang, Tianyi Tang, Dongdong Zhang, Xin Zhao, Ting Song, Yan Xia, Furu Wei |  |
| 960 |  |  [DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text](https://doi.org/10.18653/v1/2023.findings-emnlp.827) |  | 0 | With the rapid progress of Large language models (LLMs) and the huge amount of text they generate, it becomes impractical to manually distinguish whether a text is machine-generated. The growing use of LLMs in social media and education, prompts us to develop methods to detect machine-generated text, preventing malicious use such as plagiarism, misinformation, and propaganda. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the Log-Rank... | Jinyan Su, Terry Yue Zhuo, Di Wang, Preslav Nakov |  |
| 961 |  |  [From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with Small Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.828) |  | 0 | Reasoning is a distinctive human capacity, enabling us to address complex problems by breaking them down into a series of manageable cognitive steps. Yet, complex logical reasoning is still cumbersome for language models. Based on the dual process theory in cognitive science, we are the first to unravel the cognitive reasoning abilities of language models. Our framework employs an iterative methodology to construct a Cognitive Tree (CogTree). The root node of this tree represents the initial... | Junbing Yan, Chengyu Wang, Taolin Zhang, Xiaofeng He, Jun Huang, Wei Zhang |  |
| 962 |  |  [Macedon: Minimizing Representation Coding Rate Reduction for Cross-Lingual Natural Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.829) |  | 0 | Cross-lingual natural language understanding(NLU) is one of the fundamental tasks of NLP. The goal is to learn a model which can generalize well on both high-resource and low-resource language data. Recent pre-trained multilingual language models, e.g., multilingual BERT, XLM, have shown impressive performance on cross-lingual NLU tasks. However, such promising results request the use of sufficient training data, which is a difficult condition to satisfy for low-resource language. When the data... | Haoyu Wang, Yaqing Wang, Huaxiu Yao, Jing Gao |  |
| 963 |  |  [Adversarial Robustness for Large Language NER models using Disentanglement and Word Attributions](https://doi.org/10.18653/v1/2023.findings-emnlp.830) |  | 0 | Large language models (LLM’s) have been widely used for several applications such as question answering, text classification and clustering. While the preliminary results across the aforementioned tasks looks promising, recent work has dived deep into LLM’s performing poorly for complex Named Entity Recognition (NER) tasks in comparison to fine-tuned pre-trained language models (PLM’s). To enhance wider adoption of LLM’s, our paper investigates the robustness of such LLM NER models and its... | Xiaomeng Jin, Bhanukiran Vinzamuri, Sriram Venkatapathy, Heng Ji, Pradeep Natarajan |  |
| 964 |  |  [LLMs - the Good, the Bad or the Indispensable?: A Use Case on Legal Statute Prediction and Legal Judgment Prediction on Indian Court Cases](https://doi.org/10.18653/v1/2023.findings-emnlp.831) |  | 0 | The Large Language Models (LLMs) have impacted many real-life tasks. To examine the efficacy of LLMs in a high-stake domain like law, we have applied state-of-the-art LLMs for two popular tasks: Statute Prediction and Judgment Prediction, on Indian Supreme Court cases. We see that while LLMs exhibit excellent predictive performance in Statute Prediction, their performance dips in Judgment Prediction when compared with many standard models. The explanations generated by LLMs (along with... | Shaurya Vats, Atharva Zope, Somsubhra De, Anurag Sharma, Upal Bhattacharya, Shubham Kumar Nigam, Shouvik Kumar Guha, Koustav Rudra, Kripabandhu Ghosh |  |
| 965 |  |  [You Are What You Annotate: Towards Better Models through Annotator Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.832) |  | 0 | Annotator disagreement is ubiquitous in natural language processing (NLP) tasks. There are multiple reasons for such disagreements, including the subjectivity of the task, difficult cases, unclear guidelines, and so on. Rather than simply aggregating labels to obtain data annotations, we instead try to directly model the diverse perspectives of the annotators, and explicitly account for annotators’ idiosyncrasies in the modeling process by creating representations for each annotator... | Naihao Deng, Xinliang Frederick Zhang, Siyang Liu, Winston Wu, Lu Wang, Rada Mihalcea |  |
| 966 |  |  [Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers](https://doi.org/10.18653/v1/2023.findings-emnlp.833) |  | 0 | Backdoor attacks manipulate model predictions by inserting innocuous triggers into training and test data. We focus on more realistic and more challenging clean-label attacks where the adversarial training examples are correctly labeled. Our attack, LLMBkd, leverages language models to automatically insert diverse style-based triggers into texts. We also propose a poison selection technique to improve the effectiveness of both LLMBkd as well as existing textual backdoor attacks. Lastly, we... | Wencong You, Zayd Hammoudeh, Daniel Lowd |  |
| 967 |  |  [Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance](https://doi.org/10.18653/v1/2023.findings-emnlp.834) |  | 0 | Adopting a two-stage paradigm of pretraining followed by fine-tuning, Pretrained Language Models (PLMs) have achieved substantial advancements in the field of natural language processing. However, in real-world scenarios, data labels are often noisy due to the complex annotation process, making it essential to develop strategies for fine-tuning PLMs with such noisy labels. To this end, we introduce an innovative approach for fine-tuning PLMs using noisy labels, which incorporates the guidance... | Song Wang, Zhen Tan, Ruocheng Guo, Jundong Li |  |
| 968 |  |  [Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions](https://doi.org/10.18653/v1/2023.findings-emnlp.835) |  | 0 | Large language models (LLMs) are capable of answering knowledge-intensive complex questions with chain-of-thought (CoT) reasoning. However, they tend to generate factually incorrect reasoning steps when the required knowledge is not available or up-to-date in models’ parameters. Recent works turn to retrieving external knowledge to augment CoT reasoning. Despite being promising, these chain-based methods suffer from: 1) Negative retrieval. Unnecessary or incorrect retrieval may mislead the... | Shulin Cao, Jiajie Zhang, Jiaxin Shi, Xin Lv, Zijun Yao, Qi Tian, Lei Hou, Juanzi Li |  |
| 969 |  |  [Ensemble-Instruct: Instruction Tuning Data Generation with a Heterogeneous Mixture of LMs](https://doi.org/10.18653/v1/2023.findings-emnlp.836) |  | 0 | Using in-context learning (ICL) for data generation, techniques such as Self-Instruct (Wang et al., 2023) or the follow-up Alpaca (Taori et al., 2023) can train strong conversational agents with only a small amount of human supervision. One limitation of these approaches is that they resort to very large language models (around 175B parameters) that are also proprietary and non-public. Here we explore the application of such techniques to language models that are much smaller (around 10B–40B... | YoungSuk Lee, Md. Arafat Sultan, Yousef ElKurdi, Tahira Naseem, Asim Munawar, Radu Florian, Salim Roukos, Ramón Fernandez Astudillo |  |
| 970 |  |  [The Less the Merrier? Investigating Language Representation in Multilingual Models](https://doi.org/10.18653/v1/2023.findings-emnlp.837) |  | 0 | Multilingual Language Models offer a way to incorporate multiple languages in one model and utilize cross-language transfer learning to improve performance for different Natural Language Processing (NLP) tasks. Despite progress in multilingual models, not all languages are supported as well, particularly in low-resource settings. In this work, we investigate the linguistic representation of different languages in multilingual models. We start by asking the question which languages are supported... | Hellina Nigatu, Atnafu Lambebo Tonja, Jugal Kalita |  |
| 971 |  |  [SuperTweetEval: A Challenging, Unified and Heterogeneous Benchmark for Social Media NLP Research](https://doi.org/10.18653/v1/2023.findings-emnlp.838) |  | 0 | Despite its relevance, the maturity of NLP for social media pales in comparison with general-purpose models, metrics and benchmarks. This fragmented landscape makes it hard for the community to know, for instance, given a task, which is the best performing model and how it compares with others. To alleviate this issue, we introduce a unified benchmark for NLP evaluation in social media, SuperTweetEval, which includes a heterogeneous set of tasks and datasets combined, adapted and constructed... | Dimosthenis Antypas, Asahi Ushio, Francesco Barbieri, Leonardo Neves, Kiamehr Rezaee, Luis Espinosa Anke, Jiaxin Pei, José CamachoCollados |  |
| 972 |  |  [Enabling Unsupervised Neural Machine Translation with Word-level Visual Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.839) |  | 0 | Unsupervised neural machine translation has recently made remarkable strides, achieving impressive results with the exclusive use of monolingual corpora. Nonetheless, these methods still exhibit fundamental flaws, such as confusing similar words. A straightforward remedy to rectify this drawback is to employ bilingual dictionaries, however, high-quality bilingual dictionaries can be costly to obtain. To overcome this limitation, we propose a method that incorporates images at the word level to... | Chengpeng Fu, Xiaocheng Feng, Yichong Huang, Wenshuai Huo, Hui Wang, Bing Qin, Ting Liu |  |
| 973 |  |  [Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches](https://doi.org/10.18653/v1/2023.findings-emnlp.840) |  | 0 | People rely heavily on context to enrich meaning beyond what is literally said, enabling concise but effective communication. To interact successfully and naturally with people, user-facing artificial intelligence systems will require similar skills in pragmatics: relying on various types of context — from shared linguistic goals and conventions, to the visual and embodied world — to use language effectively. We survey existing grounded settings and pragmatic modeling approaches and analyze how... | Daniel Fried, Nicholas Tomlin, Jennifer Hu, Roma Patel, Aida Nematzadeh |  |
| 974 |  |  [MISCA: A Joint Model for Multiple Intent Detection and Slot Filling with Intent-Slot Co-Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.841) |  | 0 | The research study of detecting multiple intents and filling slots is becoming more popular because of its relevance to complicated real-world situations. Recent advanced approaches, which are joint models based on graphs, might still face two potential issues: (i) the uncertainty introduced by constructing graphs based on preliminary intents and slots, which may transfer intent-slot correlation information to incorrect label node destinations, and (ii) direct incorporation of multiple intent... | Thinh Pham, Tran Chi, Dat Quoc Nguyen |  |
| 975 |  |  [Enhancing Emotion Recognition in Conversation via Multi-view Feature Alignment and Memorization](https://doi.org/10.18653/v1/2023.findings-emnlp.842) |  | 0 | Emotion recognition in conversation (ERC) has attracted increasing attention in natural language processing community. Previous work commonly first extract semantic-view features via fine-tuning PLMs, then models context-view features based on the obtained semantic-view features by various graph neural networks. However, it is difficult to fully model interaction between utterances simply through a graph neural network and the features at semantic-view and context-view are not well aligned.... | Guiyang Hou, Yongliang Shen, Wenqi Zhang, Wei Xue, Weiming Lu |  |
| 976 |  |  [Mandarin classifier systems optimize to accommodate communicative pressures](https://doi.org/10.18653/v1/2023.findings-emnlp.843) |  | 0 | Previous work on noun classification implies that gender systems are inherently optimized to accommodate communicative pressures on human language learning and processing (Dye. et al 2017, 2018). They state that languages make use of either grammatical (e.g., gender) or probabilistic (pre-nominal modifiers) to smoothe the entropy of nouns in context. We show that even languages that are considered genderless, like Mandarin Chinese, possess a noun classification device that plays the same... | Yamei Wang, Géraldine Walther |  |
| 977 |  |  [Probing Representations for Document-level Event Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.844) |  | 0 | The probing classifiers framework has been employed for interpreting deep neural network models for a variety of natural language processing (NLP) applications. Studies, however, have largely focused on sentencelevel NLP tasks. This work is the first to apply the probing paradigm to representations learned for document-level information extraction (IE). We designed eight embedding probes to analyze surface, semantic, and event-understanding capabilities relevant to document-level event... | Barry Wang, Xinya Du, Claire Cardie |  |
| 978 |  |  [Cultural Compass: Predicting Transfer Learning Success in Offensive Language Detection with Cultural Features](https://doi.org/10.18653/v1/2023.findings-emnlp.845) |  | 0 | The increasing ubiquity of language technology necessitates a shift towards considering cultural diversity in the machine learning realm, particularly for subjective tasks that rely heavily on cultural nuances, such as Offensive Language Detection (OLD). Current understanding underscores that these tasks are substantially influenced by cultural values, however, a notable gap exists in determining if cultural features can accurately predict the success of cross-cultural transfer learning for... | Li Zhou, Antonia Karamolegkou, Wenyu Chen, Daniel Hershcovich |  |
| 979 |  |  [Linguistically Motivated Sign Language Segmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.846) |  | 0 | Sign language segmentation is a crucial task in sign language processing systems. It enables downstream tasks such as sign recognition, transcription, and machine translation. In this work, we consider two kinds of segmentation: segmentation into individual signs and segmentation into phrases, larger units comprising several signs. We propose a novel approach to jointly model these two tasks. Our method is motivated by linguistic cues observed in sign language corpora. We replace the... | Amit Moryossef, Zifan Jiang, Mathias Müller, Sarah Ebling, Yoav Goldberg |  |
| 980 |  |  [Re-weighting Tokens: A Simple and Effective Active Learning Strategy for Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.847) |  | 0 | Active learning, a widely adopted technique for enhancing machine learning models in text and image classification tasks with limited annotation resources, has received relatively little attention in the domain of Named Entity Recognition (NER). The challenge of data imbalance in NER has hindered the effectiveness of active learning, as sequence labellers lack sufficient learning signals. To address these challenges, this paper presents a novel re-weighting-based active learning strategy that... | Haocheng Luo, Wei Tan, Ngoc Dang Nguyen, Lan Du |  |
| 981 |  |  [Language-Agnostic Bias Detection in Language Models with Bias Probing](https://doi.org/10.18653/v1/2023.findings-emnlp.848) |  | 0 | Pretrained language models (PLMs) are key components in NLP, but they contain strong social biases. Quantifying these biases is challenging because current methods focusing on fill-the-mask objectives are sensitive to slight changes in input. To address this, we propose a bias probing technique called LABDet, for evaluating social bias in PLMs with a robust and language-agnostic method. For nationality as a case study, we show that LABDet “surfaces” nationality bias by training a classifier on... | Abdullatif Köksal, Omer Faruk Yalcin, Ahmet Akbiyik, M. Tahir Kilavuz, Anna Korhonen, Hinrich Schütze |  |
| 982 |  |  [CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.849) |  | 0 | How much success in Knowledge Graph Completion (KGC) would translate into the performance enhancement in downstream tasks is an important question that has not been studied in depth. In this paper, we introduce a novel benchmark, namely CompleQA, to comprehensively assess the influence of representative KGC methods on Knowledge Graph Question Answering (KGQA), one of the most important downstream applications. This benchmark includes a knowledge graph with 3 million triplets across 5 distinct... | Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang |  |
| 983 |  |  [Improving Multi-Criteria Chinese Word Segmentation through Learning Sentence Representation](https://doi.org/10.18653/v1/2023.findings-emnlp.850) |  | 0 | Recent Chinese word segmentation (CWS) models have shown competitive performance with pre-trained language models’ knowledge. However, these models tend to learn the segmentation knowledge through in-vocabulary words rather than understanding the meaning of the entire context. To address this issue, we introduce a context-aware approach that incorporates unsupervised sentence representation learning over different dropout masks into the multi-criteria training framework. We demonstrate that our... | Chun Lin, YingJia Lin, ChiaJen Yeh, YiTing Li, Ching Yang, HungYu Kao |  |
| 984 |  |  [A Joint Matrix Factorization Analysis of Multilingual Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.851) |  | 0 | We present an analysis tool based on joint matrix factorization for comparing latent representations of multilingual and monolingual models. An alternative to probing, this tool allows us to analyze multiple sets of representations in a joint manner. Using this tool, we study to what extent and how morphosyntactic features are reflected in the representations learned by multilingual pre-trained models. We conduct a large-scale empirical study of over 33 languages and 17 morphosyntactic... | Zheng Zhao, Yftah Ziser, Bonnie Webber, Shay B. Cohen |  |
| 985 |  |  [Don't Add, don't Miss: Effective Content Preserving Generation from Pre-Selected Text Spans](https://doi.org/10.18653/v1/2023.findings-emnlp.852) |  | 0 | The recently introduced Controlled Text Reduction (CTR) task isolates the text generation step within typical summarization-style tasks. It does so by challenging models to generate coherent text conforming to pre-selected content within the input text (“highlights”). This framing enables increased modularity in summarization-like tasks, allowing to couple a single CTR model with various content-selection setups and modules. However, there are currently no reliable CTR models, while the... | Aviv Slobodkin, Avi Caciularu, Eran Hirsch, Ido Dagan |  |
| 986 |  |  [A Computational Interface to Translate Strategic Intent from Unstructured Language in a Low-Data Setting](https://doi.org/10.18653/v1/2023.findings-emnlp.853) |  | 0 | Many real-world tasks involve a mixed-initiative setup, wherein humans and AI systems collaboratively perform a task. While significant work has been conducted towards enabling humans to specify, through language, exactly how an agent should complete a task (i.e., low-level specification), prior work lacks on interpreting the high-level strategic intent of the human commanders. Parsing strategic intent from language will allow autonomous systems to independently operate according to the user’s... | Pradyumna Tambwekar, Lakshita Dodeja, Nathan Vaska, Wei Xu, Matthew C. Gombolay |  |
| 987 |  |  [HFMRE: Constructing Huffman Tree in Bags to Find Excellent Instances for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.854) |  | 0 | Since the introduction of distantly supervised relation extraction methods, numerous approaches have been developed, the most representative of which is multi-instance learning (MIL). To find reliable features that are most representative of multi-instance bags, aggregation strategies such as AVG (average), ONE (at least one), and ATT (sentence-level attention) are commonly used. These strategies tend to train third-party vectors to select sentence-level features, leaving it to the third party... | Min Li, Cong Shao, Gang Li, Mingle Zhou |  |
| 988 |  |  [DISCO: A Large Scale Human Annotated Corpus for Disfluency Correction in Indo-European Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.855) |  | 0 | Disfluency correction (DC) is the process of removing disfluent elements like fillers, repetitions and corrections from spoken utterances to create readable and interpretable text. DC is a vital post-processing step applied to Automatic Speech Recognition (ASR) outputs, before subsequent processing by downstream language understanding tasks. Existing DC research has primarily focused on English due to the unavailability of large-scale open-source datasets. Towards the goal of multilingual... | Vineet Bhat, Preethi Jyothi, Pushpak Bhattacharyya |  |
| 989 |  |  [Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity](https://doi.org/10.18653/v1/2023.findings-emnlp.856) |  | 0 | Mixture-of-experts (MoE) models that employ sparse activation have demonstrated effectiveness in significantly increasing the number of parameters while maintaining low computational requirements per token. However, recent studies have established that MoE models are inherently parameter-inefficient as the improvement in performance diminishes with an increasing number of experts. We hypothesize this parameter inefficiency is a result of all experts having equal capacity, which may not... | Haoran Xu, Maha Elbayad, Kenton Murray, Jean Maillard, Vedanuj Goswami |  |
| 990 |  |  [Misery Loves Complexity: Exploring Linguistic Complexity in the Context of Emotion Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.857) |  | 0 | Given the omnipresence of social media in our society, thoughts and opinions are being shared online in an unprecedented manner. This means that both positive and negative emotions can be equally and freely expressed. However, the negativity bias posits that human beings are inherently drawn to and more moved by negativity and, as a consequence, negative emotions get more traffic. Correspondingly, when writing about emotions this negativity bias could lead to expressions of negative emotions... | Pranaydeep Singh, Luna De Bruyne, Orphée De Clercq, Els Lefever |  |
| 991 |  |  [Probing the "Creativity" of Large Language Models: Can models produce divergent semantic association?](https://doi.org/10.18653/v1/2023.findings-emnlp.858) |  | 0 | Large language models possess remarkable capacity for processing language, but it remains unclear whether these models can further generate creative content. The present study aims to investigate the creative thinking of large language models through a cognitive perspective. We utilize the divergent association task (DAT), an objective measurement of creativity that asks models to generate unrelated words and calculates the semantic distance between them. We compare the results across different... | Honghua Chen, Nai Ding |  |
| 992 |  |  [Code-Switching with Word Senses for Pretraining in Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.859) |  | 0 | Lexical ambiguity is a significant and pervasive challenge in Neural Machine Translation (NMT), with many state-of-the-art (SOTA) NMT systems struggling to handle polysemous words (Campolungo et al., 2022). The same holds for the NMT pretraining paradigm of denoising synthetic “code-switched” text (Pan et al., 2021; Iyer et al., 2023), where word senses are ignored in the noising stage – leading to harmful sense biases in the pretraining data that are subsequently inherited by the resulting... | Vivek Iyer, Edoardo Barba, Alexandra Birch, Jeff Z. Pan, Roberto Navigli |  |
| 993 |  |  [DiffusionSL: Sequence Labeling via Tag Diffusion Process](https://doi.org/10.18653/v1/2023.findings-emnlp.860) |  | 0 | Sequence Labeling (SL) is long-standing in Natural Language Processing (NLP). Traditionally, discriminative models have been widely used to capture the conditional distribution of sequence tags, rather than generative models. In this paper, we present DiffusionSL, a framework that utilizes a conditional discrete diffusion model for generating discrete tag data, resulting in a Tag Diffusion Process. We treat the natural language sequence as the conditional signal and the sequence tags as the... | Ziyang Huang, Pengfei Cao, Jun Zhao, Kang Liu |  |
| 994 |  |  [COMET-M: Reasoning about Multiple Events in Complex Sentences](https://doi.org/10.18653/v1/2023.findings-emnlp.861) |  | 0 | Understanding the speaker’s intended meaning often involves drawing commonsense inferences to reason about what is not stated explicitly. In multi-event sentences, it requires understanding the relationships between events based on contextual knowledge. We propose COMET-M (Multi-Event), an event-centric commonsense model capable of generating commonsense inferences for a target event within a complex sentence. COMET-M builds upon COMET (Bosselut et al., 2019), which excels at generating... | Sahithya Ravi, Raymond T. Ng, Vered Shwartz |  |
| 995 |  |  [On Event Individuation for Document-Level Information Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.862) |  | 0 | As information extraction (IE) systems have grown more adept at processing whole documents, the classic task of \*template filling\* has seen renewed interest as a benchmark for document-level IE. In this position paper, we call into question the suitability of template filling for this purpose. We argue that the task demands definitive answers to thorny questions of \*event individuation\* — the problem of distinguishing distinct events — about which even human experts disagree. Through an... | William Gantt, Reno Kriz, Yunmo Chen, Siddharth Vashishtha, Aaron Steven White |  |
| 996 |  |  [AniEE: A Dataset of Animal Experimental Literature for Event Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.863) |  | 0 | Event extraction (EE), as a crucial information extraction (IE) task, aims to identify event triggers and their associated arguments from unstructured text, subsequently classifying them into pre-defined types and roles. In the biomedical domain, EE is widely used to extract complex structures representing biological events from literature. Due to the complicated semantics and specialized domain knowledge, it is challenging to construct biomedical event extraction datasets. Additionally, most... | Dohee Kim, Ra Yoo, Soyoung Yang, Hee Yang, Jaegul Choo |  |
| 997 |  |  [From Words to Wires: Generating Functioning Electronic Devices from Natural Language Descriptions](https://doi.org/10.18653/v1/2023.findings-emnlp.864) |  | 0 | In this work, we show that contemporary language models have a previously unknown skill – the capacity for electronic circuit design from high-level textual descriptions, akin to code generation. We introduce two benchmarks: PINS100, assessing model knowledge of electrical components, and MICRO25, evaluating a model’s capability to design common microcontroller circuits and code in the Arduino ecosystem that involve input, output, sensors, motors, protocols, and logic – with models such as... | Peter Jansen |  |
| 998 |  |  [Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training](https://doi.org/10.18653/v1/2023.findings-emnlp.865) |  | 0 | In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model’s automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with... | Zhisong Zhang, Emma Strubell, Eduard H. Hovy |  |
| 999 |  |  [Explicit Alignment and Many-to-many Entailment Based Reasoning for Conversational Machine Reading](https://doi.org/10.18653/v1/2023.findings-emnlp.866) |  | 0 | Conversational Machine Reading (CMR) requires answering a user’s initial question through multi-turn dialogue interactions based on a given document. Although there exist many effective methods, they largely neglected the alignment between the document and the user-provided information, which significantly affects the intermediate decision-making and subsequent follow-up question generation. To address this issue, we propose a pipeline framework that (1) aligns the aforementioned two sides in... | Yangyang Luo, Shiyu Tian, Caixia Yuan, Xiaojie Wang |  |
| 1000 |  |  [Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers](https://doi.org/10.18653/v1/2023.findings-emnlp.867) |  | 0 | Neural networks have revolutionized language modeling and excelled in various downstream tasks. However, the extent to which these models achieve compositional generalization comparable to human cognitive abilities remains a topic of debate. While existing approaches in the field have mainly focused on novel architectures and alternative learning paradigms, we introduce a pioneering method harnessing the power of dataset cartography (Swayamdipta et al., 2020). By strategically identifying a... | Osman Batur Ince, Tanin Zeraati, Semih Yagcioglu, Yadollah Yaghoobzadeh, Erkut Erdem, Aykut Erdem |  |
| 1001 |  |  [Roles of Scaling and Instruction Tuning in Language Perception: Model vs. Human Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.868) |  | 0 | Recent large language models (LLMs) have revealed strong abilities to understand natural language. Since most of them share the same basic structure, i.e. the transformer block, possible contributors to their success in the training process are scaling and instruction tuning. However, how these factors affect the models’ language perception is unclear. This work compares the self-attention of several existing LLMs (LLaMA, Alpaca and Vicuna) in different sizes (7B, 13B, 30B, 65B), together with... | Changjiang Gao, Shujian Huang, Jixing Li, Jiajun Chen |  |
| 1002 |  |  [Efficient Data Learning for Open Information Extraction with Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.869) |  | 0 | Open Information Extraction (OpenIE) is a fundamental yet challenging task in Natural Language Processing, which involves extracting all triples (subject, predicate, object) from a given sentence. While labelling-based methods have their merits, generation-based techniques offer unique advantages, such as the ability to generate tokens not present in the original sentence. However, these generation-based methods often require a significant amount of training data to learn the task form of... | Zhiyuan Fan, Shizhu He |  |
| 1003 |  |  [Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning](https://doi.org/10.18653/v1/2023.findings-emnlp.870) |  | 0 | Prompt-based learning has been an effective paradigm for large pretrained language models (LLM), enabling few-shot or even zero-shot learning. Black-box prompt search has received growing interest recently for its distinctive properties of gradient-free optimization, proven particularly useful and powerful for model-as-a-service usage. However, the discrete nature and the complexity of combinatorial optimization hinder the efficiency of modern black-box approaches. Despite extensive research on... | Han Zhou, Xingchen Wan, Ivan Vulic, Anna Korhonen |  |
| 1004 |  |  [Towards Zero-shot Learning for End-to-end Cross-modal Translation Models](https://doi.org/10.18653/v1/2023.findings-emnlp.871) |  | 0 | One of the main problems in speech translation is the mismatches between different modalities. The second problem, scarcity of parallel data covering multiple modalities, means that the end-to-end multi-modal models tend to perform worse than cascade models, although there are exceptions under favorable conditions. To address these problems, we propose an end-to-end zero-shot speech translation model, connecting two pre-trained uni-modality modules via word rotator’s distance. The model retains... | Jichen Yang, Kai Fan, Minpeng Liao, Boxing Chen, Zhongqiang Huang |  |
| 1005 |  |  [LLMaAA: Making Large Language Models as Active Annotators](https://doi.org/10.18653/v1/2023.findings-emnlp.872) |  | 0 | Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of... | Ruoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, Lei Zou |  |
| 1006 |  |  [NLMs: Augmenting Negation in Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.873) |  | 0 | Negation is the fundamental component in a natural language that reverses the semantic meaning of a sentence. It plays an extremely important role across a wide range of applications, yet they are underrepresented in pre-trained language models (LMs), resulting often in wrong inferences. In this work, we try to improve the underlying understanding of the negation in the pre-trained LMs. To augment negation understanding, we propose a language model objective with a weighted cross-entropy loss... | Rituraj Singh, Rahul Kumar, Vivek Sridhar |  |
| 1007 |  |  [Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated Neural Text Retrievers](https://doi.org/10.18653/v1/2023.findings-emnlp.874) |  | 0 | Prompt tuning attempts to update few task-specific parameters in pre-trained models. It has achieved comparable performance to fine-tuning of the full parameter set on both language understanding and generation tasks. In this work, we study the problem of prompt tuning for neural text retrievers. We introduce parameter-efficient prompt tuning for text retrieval across in-domain, cross-domain, and cross-topic settings. Through an extensive analysis, we show that the strategy can mitigate the two... | Weng Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Jiahua Liu, Tao Li, Yuxiao Dong, Jie Tang |  |
| 1008 |  |  [X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity](https://doi.org/10.18653/v1/2023.findings-emnlp.875) |  | 0 | Cross-lingual transfer (XLT) is an emergent ability of multilingual language models that preserves their performance on a task to a significant extent when evaluated in languages that were not included in the fine-tuning process. While English, due to its widespread usage, is typically regarded as the primary language for model adaption in various tasks, recent studies have revealed that the efficacy of XLT can be amplified by selecting the most appropriate source languages based on specific... | Taejun Yun, Jinhyeon Kim, Deokyeong Kang, Seong Hoon Lim, Jihoon Kim, Taeuk Kim |  |
| 1009 |  |  [Noise-Robust Semi-Supervised Learning for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.876) |  | 0 | Distantly supervised relation extraction (DSRE) aims to extract relational facts from texts but suffers from noisy instances. To mitigate the influence of noisy labels, current methods typically use the Multi-Instance-Learning framework to extract relations for each bag. However, these approaches are not capable of extracting relation labels for individual sentences. Several studies have focused on sentence-level DSRE to solve the above problem. These studies primarily aim to develop methods... | Xin Sun, Qiang Liu, Shu Wu, Zilei Wang, Liang Wang |  |
| 1010 |  |  [Towards Concept-Aware Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.877) |  | 0 | Concepts play a pivotal role in various human cognitive functions, including learning, reasoning and communication. However, there is very little work on endowing machines with the ability to form and reason with concepts. In particular, state-of-the-art large language models (LLMs) work at the level of tokens, not concepts. In this work, we analyze how well contemporary LLMs capture human concepts and their structure. We then discuss ways to develop concept-aware LLMs, taking place at... | Chen Shani, Jilles Vreeken, Dafna Shahaf |  |
| 1011 |  |  [ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.878) |  | 0 | Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for ChatGPT in English, the model can process and generate texts for multiple... | Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, Thien Huu Nguyen |  |
| 1012 |  |  [Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training](https://doi.org/10.18653/v1/2023.findings-emnlp.879) |  | 0 | Representational spaces learned via language modeling are fundamental to Natural Language Processing (NLP), however there has been limited understanding regarding how and when during training various types of linguistic information emerge and interact. Leveraging a novel information theoretic probing suite, which enables direct comparisons of not just task performance, but their representational subspaces, we analyze nine tasks covering syntax, semantics and reasoning, across 2M pre-training... | Max MüllerEberstein, Rob van der Goot, Barbara Plank, Ivan Titov |  |
| 1013 |  |  [Not All Demonstration Examples are Equally Beneficial: Reweighting Demonstration Examples for In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.880) |  | 0 | Large Language Models (LLMs) have recently gained the In-Context Learning (ICL) ability with the models scaling up, allowing them to quickly adapt to downstream tasks with only a few demonstration examples prepended in the input sequence. Nonetheless, the current practice of ICL treats all demonstration examples equally, which still warrants improvement, as the quality of examples is usually uneven. In this paper, we investigate how to determine approximately optimal weights for demonstration... | Zhe Yang, Damai Dai, Peiyi Wang, Zhifang Sui |  |
| 1014 |  |  [Difference-Masking: Choosing What to Mask in Continued Pretraining](https://doi.org/10.18653/v1/2023.findings-emnlp.881) |  | 0 | The self-supervised objective of masked prediction has led to promising performance gains on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition that deciding what to mask can substantially improve learning outcomes. We investigate this in continued pretraining setting in which pretrained models continue to pretrain on domain-specific data before performing some downstream task. We introduce Difference-Masking, a masking strategy that... | Alex Wilf, Syeda Nahida Akter, Leena Mathur, Paul Pu Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, LouisPhilippe Morency |  |
| 1015 |  |  [Learn From One Specialized Sub-Teacher: One-to-One Mapping for Feature-Based Knowledge Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.882) |  | 0 | Knowledge distillation is known as an effective technique for compressing over-parameterized language models. In this work, we propose to break down the global feature distillation task into N local sub-tasks. In this new framework, we consider each neuron in the last hidden layer of the teacher network as a specialized sub-teacher. We also consider each neuron in the last hidden layer of the student network as a focused sub-student. We make each focused sub-student learn from one corresponding... | Khouloud Saadi, Jelena Mitrovic, Michael Granitzer |  |
| 1016 |  |  [IMU2CLIP: Language-grounded Motion Sensor Translation with Multimodal Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.883) |  | 0 | We present IMU2CLIP, a novel pre-training approach to align Inertial Measurement Unit (IMU) motion sensor recordings with text and video, by projecting them into the joint representation space of Contrastive Language-Image Pre-training (CLIP). The proposed approach allows IMU2CLIP to translate human motions (as measured by IMU sensors) into their corresponding textual descriptions and videos – while preserving the transitivity across these modalities. We introduce several new IMU-based Wearable... | Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Aparajita Saraf, Amy Bearman, Babak Damavandi |  |
| 1017 |  |  [Conditioning on Dialog Acts improves Empathy Style Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.884) |  | 0 | We explore the role of dialog acts in style transfer, specifically empathy style transfer – rewriting a sentence to make it more empathetic without changing its meaning. Specifically, we use two novel few-shot prompting strategies: target prompting, which only uses examples of the target style (unlike traditional prompting with source/target pairs), and dialog-act-conditioned prompting, which first estimates the dialog act of the source sentence and then makes it more empathetic using few-shot... | Renyi Qu, Lyle H. Ungar, João Sedoc |  |
| 1018 |  |  [Systematic Assessment of Factual Knowledge in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.885) |  | 0 | Previous studies have relied on existing question-answering benchmarks to evaluate the knowledge stored in large language models (LLMs). However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data. This paper proposes a framework to systematically assess the factual knowledge of LLMs by leveraging knowledge graphs (KGs). Our framework automatically generates a set of questions and expected... | Linhao Luo, ThuyTrang Vu, Dinh Q. Phung, Gholamreza Haffari |  |
| 1019 |  |  [From Speculation Detection to Trustworthy Relational Tuples in Information Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.886) |  | 0 | Speculation detection is an important NLP task to identify text factuality. However, the extracted speculative information (e.g., speculative polarity, cue, and scope) lacks structure and poses challenges for direct utilization in downstream tasks. Open Information Extraction (OIE), on the other hand, extracts structured tuples as facts, without examining the certainty of these tuples. Bridging this gap between speculation detection and information extraction becomes imperative to generate... | Kuicai Dong, Aixin Sun, JungJae Kim, Xiaoli Li |  |
| 1020 |  |  [Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.887) |  | 0 | Generative models have been widely applied to solve extractive tasks, where parts of the input is extracted to form the desired output, and achieved significant success. For example, in extractive question answering (QA), generative models have constantly yielded state-of-the-art results. In this work, we study the issue of tokenization inconsistency that is commonly neglected in training these models. This issue damages the extractive nature of these tasks after the input and output are... | Kaiser Sun, Peng Qi, Yuhao Zhang, Lan Liu, William Yang Wang, Zhiheng Huang |  |
| 1021 |  |  [Dialogue Medical Information Extraction with Medical-Item Graph and Dialogue-Status Enriched Representation](https://doi.org/10.18653/v1/2023.findings-emnlp.888) |  | 0 | The multi-turn doctor-patient dialogue includes rich medical knowledge, like the symptoms of the patient, the diagnosis and medication suggested by the doctor. If mined and represented properly, such medical knowledge can benefit a large range of clinical applications, including diagnosis assistance and medication recommendation. To derive structured knowledge from free text dialogues, we target a critical task: the Dialogue Medical Information Extraction (DMIE). DMIE aims to detect pre-defined... | Lei Gao, Xinnan Zhang, Xian Wu, Shen Ge, Yefeng Zheng |  |
| 1022 |  |  [LogicAttack: Adversarial Attacks for Evaluating Logical Consistency of Natural Language Inference](https://doi.org/10.18653/v1/2023.findings-emnlp.889) |  | 0 | Recently Large Language Models (LLMs) such as GPT-3, ChatGPT, and FLAN have led to impressive progress in Natural Language Inference (NLI) tasks. However, these models may rely on simple heuristics or artifacts in the evaluation data to achieve their high performance, which suggests that they still suffer from logical inconsistency. To assess the logical consistency of these models, we propose a LogicAttack, a method to attack NLI models using diverse logical forms of premise and hypothesis,... | Mutsumi Nakamura, Santosh Mashetty, Mihir Parmar, Neeraj Varshney, Chitta Baral |  |
| 1023 |  |  [Decomposed Prompt Tuning via Low-Rank Reparameterization](https://doi.org/10.18653/v1/2023.findings-emnlp.890) |  | 0 | While prompt tuning approaches have achieved competitive performance with high efficiency, we observe that they invariably employ the same initialization process, wherein the soft prompt is either randomly initialized or derived from an existing embedding vocabulary. In contrast to these conventional methods, this study aims to investigate an alternative way to derive soft prompt. Our empirical studies show that the soft prompt typically exhibits a low “intrinsic rank” characteristic. With such... | Yao Xiao, Lu Xu, Jiaxi Li, Wei Lu, Xiaoli Li |  |
| 1024 |  |  [SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.891) |  | 0 | Building and maintaining end-to-end task bots using minimal human effort is a long-standing challenge in dialog research. In this work, we introduce SGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems effortlessly based on large language models (LLMs). Utilizing the predefined task schema, i.e., belief instruction and dialog policy, we instruct fixed LLMs to generate appropriate responses on novel tasks, without the need for training data. Specifically, SGP-TOD comprises... | Xiaoying Zhang, Baolin Peng, Kun Li, Jingyan Zhou, Helen Meng |  |
| 1025 |  |  [Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.892) |  | 0 | In this position paper, we argue that instead of morally aligning LLMs to specific set of ethical principles, we should infuse generic ethical reasoning capabilities into them so that they can handle value pluralism at a global scale. When provided with an ethical policy, an LLM should be capable of making decisions that are ethically consistent to the policy. We develop a framework that integrates moral dilemmas with moral principles pertaining to different foramlisms of normative ethics, and... | Abhinav Rao, Aditi Khandelwal, Kumar Tanmay, Utkarsh Agarwal, Monojit Choudhury |  |
| 1026 |  |  [Vector-Quantized Prompt Learning for Paraphrase Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.893) |  | 0 | Deep generative modeling of natural languages has achieved many successes, such as producing fluent sentences and translating from one language into another. However, the development of generative modeling techniques for paraphrase generation still lags behind largely due to the challenges in addressing the complex conflicts between expression diversity and semantic preservation. This paper proposes to generate diverse and high-quality paraphrases by exploiting the pre-trained models with... | Haotian Luo, Yixin Liu, Peidong Liu, Xianggen Liu |  |
| 1027 |  |  [Rethinking the Construction of Effective Metrics for Understanding the Mechanisms of Pretrained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.894) |  | 0 | Pretrained language models are expected to effectively map input text to a set of vectors while preserving the inherent relationships within the text. Consequently, designing a white-box model to compute metrics that reflect the presence of specific internal relations in these vectors has become a common approach for post-hoc interpretability analysis of pretrained language models. However, achieving interpretability in white-box models and ensuring the rigor of metric computation becomes... | You Li, Jinhui Yin, Yuming Lin |  |
| 1028 |  |  [PARROT: Zero-Shot Narrative Reading Comprehension via Parallel Reading](https://doi.org/10.18653/v1/2023.findings-emnlp.895) |  | 0 | Narrative comprehension is a challenging task that requires a deep understanding of the foundational elements of narratives. Acquiring this skill requires extensive annotated data. To mitigate the burden of data annotation, we present Parrot, a zero-shot approach for narrative reading comprehension through parallel reading, which involves two parallel narratives that tell the same story. By leveraging one narrative as a source of supervision signal to guide the understanding of the other,... | Chao Zhao, Anvesh Rao Vijjini, Snigdha Chaturvedi |  |
| 1029 |  |  [BioDEX: Large-Scale Biomedical Adverse Drug Event Extraction for Real-World Pharmacovigilance](https://doi.org/10.18653/v1/2023.findings-emnlp.896) |  | 0 | Timely and accurate extraction of Adverse Drug Events (ADE) from biomedical literature is paramount for public safety, but involves slow and costly manual labor. We set out to improve drug safety monitoring (pharmacovigilance, PV) through the use of Natural Language Processing (NLP). We introduce BioDEX, a large-scale resource for Biomedical adverse Drug Event eXtraction, rooted in the historical output of drug safety reporting in the U.S. BioDEX consists of 65k abstracts and 19k full-text... | Karel D'Oosterlinck, François Remy, Johannes Deleu, Thomas Demeester, Chris Develder, Klim Zaporojets, Aneiss Ghodsi, Simon Ellershaw, Jack Collins, Christopher Potts |  |
| 1030 |  |  [Coarse-to-Fine Dual Encoders are Better Frame Identification Learners](https://doi.org/10.18653/v1/2023.findings-emnlp.897) |  | 0 | Frame identification aims to find semantic frames associated with target words in a sentence. Recent researches measure the similarity or matching score between targets and candidate frames by modeling frame definitions. However, they either lack sufficient representation learning of the definitions or face challenges in efficiently selecting the most suitable frame from over 1000 candidate frames. Moreover, commonly used lexicon filtering (lf) to obtain candidate frames for the target may... | Kaikai An, Ce Zheng, Bofei Gao, Haozhe Zhao, Baobao Chang |  |
| 1031 |  |  [Sound of Story: Multi-modal Storytelling with Audio](https://doi.org/10.18653/v1/2023.findings-emnlp.898) |  | 0 | Storytelling is multi-modal in the real world. When one tells a story, one may use all of the visualizations and sounds along with the story itself. However, prior studies on storytelling datasets and tasks have paid little attention to sound even though sound also conveys meaningful semantics of the story. Therefore, we propose to extend story understanding and telling areas by establishing a new component called background sound which is story context-based audio without any linguistic... | Jaeyeon Bae, Seokhoon Jeong, Seokun Kang, Namgi Han, JaeYon Lee, Hyounghun Kim, Taehwan Kim |  |
| 1032 |  |  [Synthesize, if you do not have: Effective Synthetic Dataset Creation Strategies for Self-Supervised Opinion Summarization in E-commerce](https://doi.org/10.18653/v1/2023.findings-emnlp.899) |  | 0 | In e-commerce, opinion summarization is the process of condensing the opinions presented in product reviews. However, the absence of large amounts of supervised datasets presents challenges in generating both aspect-specific and general opinion summaries. Existing approaches have attempted to address these challenges through synthetic dataset creation (SDC). However, general opinion summarization models struggle to generate summaries faithful to the input reviews whereas aspect-specific opinion... | Tejpalsingh Siledar, Suman Banerjee, Amey Patil, Sudhanshu Singh, Muthusamy Chelliah, Nikesh Garera, Pushpak Bhattacharyya |  |
| 1033 |  |  [Leveraging Contrastive Learning and Knowledge Distillation for Incomplete Modality Rumor Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.900) |  | 0 | Rumors spread rapidly through online social microblogs at a relatively low cost, causing substantial economic losses and negative consequences in our daily lives. Existing rumor detection models often neglect the underlying semantic coherence between text and image components in multimodal posts, as well as the challenges posed by incomplete modalities in single modal posts, such as missing text or images. This paper presents CLKD-IMRD, a novel framework for Incomplete Modality Rumor Detection.... | Fan Xu, Pinyun Fu, Qi Huang, Bowei Zou, AiTi Aw, Mingwen Wang |  |
| 1034 |  |  [Beyond Testers' Biases: Guiding Model Testing with Knowledge Bases using LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.901) |  | 0 | Current model testing work has mostly focused on creating test cases. Identifying what to test is a step that is largely ignored and poorly supported. We propose Weaver, an interactive tool that supports requirements elicitation for guiding model testing. Weaver uses large language models to generate knowledge bases and recommends concepts from them interactively, allowing testers to elicit requirements for further testing. Weaver provides rich external knowledge to testers and encourages... | Chenyang Yang, Rishabh Rustogi, Rachel A. BrowerSinning, Grace A. Lewis, Christian Kästner, Tongshuang Wu |  |
| 1035 |  |  [CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.902) |  | 0 | The task of zero-shot commonsense question answering evaluates models on their capacity to reason about general scenarios beyond those presented in specific datasets. Existing approaches for tackling this task leverage external knowledge from CommonSense Knowledge Bases (CSKBs) by pre-training the model on synthetic QA pairs constructed from CSKBs. In these approaches, negative examples (distractors) are formulated by randomly sampling from CSKBs using fairly primitive keyword constraints.... | Weiqi Wang, Tianqing Fang, Wenxuan Ding, Baixuan Xu, Xin Liu, Yangqiu Song, Antoine Bosselut |  |
| 1036 |  |  [kNN-CM: A Non-parametric Inference-Phase Adaptation of Parametric Text Classifiers](https://doi.org/10.18653/v1/2023.findings-emnlp.903) |  | 0 | Semi-parametric models exhibit the properties of both parametric and non-parametric modeling and have been shown to be effective in the next-word prediction language modeling task. However, there is a lack of studies on the text-discriminating properties of such models. We propose an inference-phase approach—k-Nearest Neighbor Classification Model (kNN-CM)—that enhances the capacity of a pre-trained parametric text classifier by incorporating a simple neighborhood search through the... | Rishabh Bhardwaj, Yingting Li, Navonil Majumder, Bo Cheng, Soujanya Poria |  |
| 1037 |  |  [Cross-modality Data Augmentation for End-to-End Sign Language Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.904) |  | 0 | End-to-end sign language translation (SLT) aims to directly convert sign language videos into spoken language texts without intermediate representations. It has been challenging due to the data scarcity of labeled data and the modality gap between sign videos and texts. To tackle these challenges, we propose a novel Cross-modality Data Augmentation (XmDA) framework to transfer the powerful gloss-to-text translation capabilities to end-to-end sign language translation (i.e., video-to-text).... | Jinhui Ye, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Hui Xiong |  |
| 1038 |  |  [Consistency is Key: On Data-Efficient Modality Transfer in Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.905) |  | 0 | End-to-end approaches have shown promising results for speech translation (ST), but they suffer from its data scarcity compared to machine translation (MT). To address this, progressive training has become a common practice, of using external MT data during the fine-tuning phase. Despite of its prevalence and computational overhead, its validity is not extensively corroborated yet. This paper conducts an empirical investigation and finds that progressive training is ineffective. We identify... | Hojin Lee, Changmin Lee, Seungwon Hwang |  |
| 1039 |  |  [Relation-Aware Question Answering for Heterogeneous Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.906) |  | 0 | Multi-hop Knowledge Base Question Answering(KBQA) aims to find the answer entity in a knowledge graph (KG), which requires multiple steps of reasoning. Existing retrieval-based approaches solve this task by concentrating on the specific relation at different hops and predicting the intermediate entity within the reasoning path. However, these models fail to utilize information from head-tail entities and the semantic connection between relations to enhance the current relation representation,... | Haowei Du, Quzhe Huang, Chen Li, Chen Zhang, Yang Li, Dongyan Zhao |  |
| 1040 |  |  [InstOptima: Evolutionary Multi-objective Instruction Optimization via Large Language Model-based Instruction Operators](https://doi.org/10.18653/v1/2023.findings-emnlp.907) |  | 0 | Instruction-based language modeling has received significant attention in pretrained language models. However, the efficiency of instruction engineering remains low and hinders the development of instruction studies. Recent studies have focused on automating instruction generation, but they primarily aim to improve performance without considering other crucial objectives that impact instruction quality, such as instruction length and perplexity. Therefore, we propose a novel approach (i.e.,... | Heng Yang, Ke Li |  |
| 1041 |  |  [Less than One-shot: Named Entity Recognition via Extremely Weak Supervision](https://doi.org/10.18653/v1/2023.findings-emnlp.908) |  | 0 | We study the named entity recognition (NER) problem under the extremely weak supervision (XWS) setting, where only one example entity per type is given in a context-free way. While one can see that XWS is lighter than one-shot in terms of the amount of supervision, we propose a novel method X-NER that can outperform the state-of-the-art one-shot NER methods. We first mine entity spans that are similar to the example entities from an unlabelled training corpus. Instead of utilizing entity span... | Letian Peng, Zihan Wang, Jingbo Shang |  |
| 1042 |  |  [Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.909) |  | 0 | Transformer-based models have achieved dominant performance in numerous NLP tasks. Despite their remarkable successes, pre-trained transformers such as BERT suffer from a computationally expensive self-attention mechanism that interacts with all tokens, including the ones unfavorable to classification performance. To overcome these challenges, we propose integrating two strategies: token pruning and token combining. Token pruning eliminates less important tokens in the attention mechanism’s key... | Jungmin Yun, Mihyeon Kim, Youngbin Kim |  |
| 1043 |  |  [Semantic Decomposition of Question and SQL for Text-to-SQL Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.910) |  | 0 | Text-to-SQL semantic parsing faces challenges in generalizing to cross-domain and complex queries. Recent research has employed a question decomposition strategy to enhance the parsing of complex SQL queries.However, this strategy encounters two major obstacles: (1) existing datasets lack question decomposition; (2) due to the syntactic complexity of SQL, most complex queries cannot be disentangled into sub-queries that can be readily recomposed. To address these challenges, we propose a new... | Ben Eyal, Moran Mahabi, Ophir Haroche, Amir Bachar, Michael Elhadad |  |
| 1044 |  |  [Time-Aware Language Modeling for Historical Text Dating](https://doi.org/10.18653/v1/2023.findings-emnlp.911) |  | 0 | Automatic text dating(ATD) is a challenging task since explicit temporal mentions usually do not appear in texts. Existing state-of-the-art approaches learn word representations via language models, whereas most of them ignore diachronic change of words, which may affect the efforts of text modeling. Meanwhile, few of them consider text modeling for long diachronic documents. In this paper, we present a time-aware language model named TALM, to learn temporal word representations by transferring... | Han Ren, Hai Wang, Yajie Zhao, Yafeng Ren |  |
| 1045 |  |  [A Read-and-Select Framework for Zero-shot Entity Linking](https://doi.org/10.18653/v1/2023.findings-emnlp.912) |  | 0 | Zero-shot entity linking (EL) aims at aligning entity mentions to unseen entities to challenge the generalization ability. Previous methods largely focus on the candidate retrieval stage and ignore the essential candidate ranking stage, which disambiguates among entities and makes the final linking prediction. In this paper, we propose a read-and-select (ReS) framework by modeling the main components of entity disambiguation, i.e., mention-entity matching and cross-entity comparison. First, for... | Zhenran Xu, Yulin Chen, Baotian Hu, Min Zhang |  |
| 1046 |  |  [Multi-Task Learning of Query Generation and Classification for Generative Conversational Question Rewriting](https://doi.org/10.18653/v1/2023.findings-emnlp.913) |  | 0 | In conversational search settings, users ask questions and receive answers as part of a conversation. The ambiguity in the questions is a common challenge, which can be effectively addressed by leveraging contextual information from the conversation history. In this context, determining topic continuity and reformulating questions into well-defined queries are crucial tasks. Previous approaches have typically addressed these tasks either as a classification task in the case of topic continuity... | Sarawoot Kongyoung, Craig MacDonald, Iadh Ounis |  |
| 1047 |  |  [DepNeCTI: Dependency-based Nested Compound Type Identification for Sanskrit](https://doi.org/10.18653/v1/2023.findings-emnlp.914) |  | 0 | Multi-component compounding is a prevalent phenomenon in Sanskrit, and understanding the implicit structure of a compound’s components is crucial for deciphering its meaning. Earlier approaches in Sanskrit have focused on binary compounds and neglected the multi-component compound setting. This work introduces the novel task of nested compound type identification (NeCTI), which aims to identify nested spans of a multi-component compound and decode the implicit semantic relations between them.... | Jivnesh Sandhan, Yaswanth Narsupalli, Sreevatsa Muppirala, Sriram Krishnan, Pavankumar Satuluri, Amba P. Kulkarni, Pawan Goyal |  |
| 1048 |  |  [HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark](https://doi.org/10.18653/v1/2023.findings-emnlp.915) |  | 0 | Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly on morpho-syntactic tasks, neglecting the semantic dimension of language understanding. To bridge this gap, we set out to deliver a Hebrew Machine Reading Comprehension (MRC) dataset, where MRC is to be realized as extractive Question Answering. The morphologically-rich nature of Hebrew poses a challenge to this endeavor: the indeterminacy and non-transparency of span boundaries in morphologically complex forms lead to... | Amir David Nissan Cohen, Hilla Merhav, Yoav Goldberg, Reut Tsarfaty |  |
| 1049 |  |  [HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.916) |  | 0 | Authorship Analysis, also known as stylometry, has been an essential aspect of Natural Language Processing (NLP) for a long time. Likewise, the recent advancement of Large Language Models (LLMs) has made authorship analysis increasingly crucial for distinguishing between human-written and AI-generated texts. However, these authorship analysis tasks have primarily been focused on written texts, not considering spoken texts. Thus, we introduce the largest benchmark for spoken texts - HANSEN(... | Nafis Irtiza Tripto, Adaku Uchendu, Thai Le, Mattia Setzu, Fosca Giannotti, Dongwon Lee |  |
| 1050 |  |  [Data Augmentation for Code Translation with Comparable Corpora and Multiple References](https://doi.org/10.18653/v1/2023.findings-emnlp.917) |  | 0 | One major challenge of translating code between programming languages is that parallel training data is often limited. To overcome this challenge, we present two data augmentation techniques, one that builds comparable corpora (i.e., code pairs with similar functionality), and another that augments existing parallel data with multiple reference translations. Specifically, we build and analyze multiple types of comparable corpora, including programs generated from natural language documentation... | Yiqing Xie, Atharva Naik, Daniel Fried, Carolyn P. Rosé |  |
| 1051 |  |  [Multilingual Generation and Answering of Questions from Texts and Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.918) |  | 0 | The ability to bridge Question Generation (QG) and Question Answering (QA) across structured and unstructured modalities has the potential for aiding different NLP applications. One key application is in QA-based methods that have recently been shown to be useful for automatically evaluating Natural Language (NL) texts generated from Knowledge Graphs (KG). While methods have been proposed for QG-QA across these modalities, these efforts have been in English only; in this work, we bring... | Kelvin Han, Claire Gardent |  |
| 1052 |  |  [InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.919) |  | 0 | Diffusion models have garnered considerable interest in the field of text generation. Several studies have explored text diffusion models with different structures and applied them to various tasks, including named entity recognition and summarization. However, there exists a notable disparity between the “easy-first” text generation process of current diffusion models and the “keyword-first” natural text generation process of humans, which has received limited attention. To bridge this gap, we... | Renzhi Wang, Jing Li, Piji Li |  |
| 1053 |  |  [Enhancing Scalability of Pre-trained Language Models via Efficient Parameter Sharing](https://doi.org/10.18653/v1/2023.findings-emnlp.920) |  | 0 | In this paper, we propose a highly parameter-efficient approach to scaling pre-trained language models (PLMs) to a deeper model depth. Unlike prior work that shares all parameters or uses extra blocks, we design a more capable parameter-sharing architecture based on matrix product operator (MPO), an efficient tensor decomposition method to factorize the parameter matrix into a set of local tensors. Based on such a decomposition, we share the important local tensor across all layers for reducing... | Peiyu Liu, ZeFeng Gao, Yushuo Chen, Xin Zhao, JiRong Wen |  |
| 1054 |  |  [Boosting Prompt-Based Self-Training With Mapping-Free Automatic Verbalizer for Multi-Class Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.921) |  | 0 | Recently, prompt-based fine-tuning has garnered considerable interest as a core technique for few-shot text classification task. This approach reformulates the fine-tuning objective to align with the Masked Language Modeling (MLM) objective. Leveraging unlabeled data, prompt-based self-training has shown greater effectiveness in binary and three-class classification. However, prompt-based self-training for multi-class classification has not been adequately investigated, despite its significant... | Yookyung Kho, Jaehee Kim, Pilsung Kang |  |
| 1055 |  |  [On the Impact of Cross-Domain Data on German Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.922) |  | 0 | Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M... | Amin Dada, Aokun Chen, Cheng Peng, Kaleb E. Smith, Ahmad IdrissiYaghir, Constantin Seibold, Jianning Li, Lars Heiliger, Christoph M. Friedrich, Daniel Truhn, Jan Egger, Jiang Bian, Jens Kleesiek, Yonghui Wu |  |
| 1056 |  |  [Dialect-to-Standard Normalization: A Large-Scale Multilingual Evaluation](https://doi.org/10.18653/v1/2023.findings-emnlp.923) |  | 0 | Text normalization methods have been commonly applied to historical language or user-generated content, but less often to dialectal transcriptions. In this paper, we introduce dialect-to-standard normalization – i.e., mapping phonetic transcriptions from different dialects to the orthographic norm of the standard variety – as a distinct sentence-level character transduction task and provide a large-scale analysis of dialect-to-standard normalization methods. To this end, we compile a... | Olli Kuparinen, Aleksandra Miletic, Yves Scherrer |  |
| 1057 |  |  [Re-Examining Summarization Evaluation across Multiple Quality Criteria](https://doi.org/10.18653/v1/2023.findings-emnlp.924) |  | 0 | The common practice for assessing automatic evaluation metrics is to measure the correlation between their induced system rankings and those obtained by reliable human evaluation, where a higher correlation indicates a better metric. Yet, an intricate setting arises when an NLP task is evaluated by multiple Quality Criteria (QCs), like for text summarization where prominent criteria including relevance, consistency, fluency and coherence. In this paper, we challenge the soundness of this... | Ori Ernst, Ori Shapira, Ido Dagan, Ran Levy |  |
| 1058 |  |  [A Parallel Corpus for Vietnamese Central-Northern Dialect Text Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.925) |  | 0 | The Vietnamese language embodies dialectal variants closely attached to the nation’s three macro-regions: the Northern, Central and Southern regions. As the northern dialect forms the basis of the standard language, it’s considered the prestige dialect. While the northern dialect differs from the remaining two in certain aspects, it almost shares an identical lexicon with the southern dialect, making the textual attributes nearly interchangeable. In contrast, the central dialect possesses a... | Thang Le, Anh Tuan Luu |  |
| 1059 |  |  [A Comprehensive Evaluation of Tool-Assisted Generation Strategies](https://doi.org/10.18653/v1/2023.findings-emnlp.926) |  | 0 | A growing area of research investigates augmenting language models with tools (e.g., search engines, calculators) to overcome their shortcomings (e.g., missing or incorrect knowledge, incorrect logical inferences). Various few-shot tool-usage strategies have been proposed. However, there is no systematic and fair comparison across different strategies, or between these strategies and strong baselines that do not leverage tools. We conduct an extensive empirical analysis, finding that (1) across... | Alon Jacovi, Avi Caciularu, Jonathan Herzig, Roee Aharoni, Bernd Bohnet, Mor Geva |  |
| 1060 |  |  [InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT](https://doi.org/10.18653/v1/2023.findings-emnlp.927) |  | 0 | While large models such as GPT-3 demonstrate exceptional performance in zeroshot and fewshot summarization tasks, their extensive serving and fine-tuning costs hinder their utilization in various applications. Conversely, previous studies have found that although automatic metrics tend to favor smaller fine-tuned models, the quality of the summaries they generate is inferior to that of larger models like GPT-3 when assessed by human evaluators. To address this issue, we propose InheritSumm, a... | Yichong Xu, Ruochen Xu, Dan Iter, Yang Liu, Shuohang Wang, Chenguang Zhu, Michael Zeng |  |
| 1061 |  |  [Learning to love diligent trolls: Accounting for rater effects in the dialogue safety task](https://doi.org/10.18653/v1/2023.findings-emnlp.928) |  | 0 | Chatbots have the risk of generating offensive utterances, which must be avoided. Post-deployment, one way for a chatbot to continuously improve is to source utterance/label pairs from feedback by live users. However, among users are trolls, who provide training examples with incorrect labels. To de-troll training data, previous work removed training examples that have high user-aggregated cross-validation (CV) error. However, CV is expensive; and in a coordinated attack, CV may be overwhelmed... | Michael John Ilagan |  |
| 1062 |  |  [Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal Scenarios Like a Lawyer?](https://doi.org/10.18653/v1/2023.findings-emnlp.929) |  | 0 | Large Language Models (LLMs), such as ChatGPT, have drawn a lot of attentions recently in the legal domain due to its emergent ability to tackle a variety of legal tasks. However, it is still unknown if LLMs are able to analyze a legal case and perform reasoning in the same manner as lawyers. Therefore, we constructed a novel corpus consisting of scenarios pertain to Contract Acts Malaysia and Australian Social Act for Dependent Child. ChatGPT is applied to perform analysis on the corpus using... | Xiaoxi Kang, Lizhen Qu, LayKi Soon, Adnan Trakic, Terry Yue Zhuo, Patrick Charles Emerton, Genevieve Grant |  |
| 1063 |  |  [Coverage-based Example Selection for In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.930) |  | 0 | In-context learning (ICL), the ability of large language models to perform novel tasks by conditioning on a prompt with a few task examples, requires these examples to be informative about the test instance. The standard approach of independently ranking and selecting the most similar examples selects redundant examples while omitting important information. In this work, we show that BERTScore-Recall (BSR) selects better examples that demonstrate more of the salient aspects, e.g. reasoning... | Shivanshu Gupta, Matt Gardner, Sameer Singh |  |
| 1064 |  |  [Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization](https://doi.org/10.18653/v1/2023.findings-emnlp.931) |  | 0 | Large language models (LLMs) have exhibited considerable cross-lingual generalization abilities, whereby they implicitly transfer knowledge across languages. However, the transfer is not equally successful for all languages, especially for low-resource ones, which poses an ongoing challenge. It is unclear whether we have reached the limits of implicit cross-lingual generalization and if explicit knowledge transfer is viable. In this paper, we investigate the potential for explicitly aligning... | Ningyu Xu, Qi Zhang, Jingting Ye, Menghan Zhang, Xuanjing Huang |  |
| 1065 |  |  [Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing](https://doi.org/10.18653/v1/2023.findings-emnlp.932) |  | 0 | Dual use, the intentional, harmful reuse of technology and scientific artefacts, is an ill-defined problem within the context of Natural Language Processing (NLP). As large language models (LLMs) have advanced in their capabilities and become more accessible, the risk of their intentional misuse becomes more prevalent. To prevent such intentional malicious use, it is necessary for NLP researchers and practitioners to understand and mitigate the risks of their research. Hence, we present an... | LucieAimée Kaffee, Arnav Arora, Zeerak Talat, Isabelle Augenstein |  |
| 1066 |  |  [BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions](https://doi.org/10.18653/v1/2023.findings-emnlp.933) |  | 0 | Text classification is a well-studied and versatile building block for many NLP applications. Yet, existing approaches require either large annotated corpora to train a model with or, when using large language models as a base, require carefully crafting the prompt as well as using a long context that can fit many examples. As a result, it is not possible for end-users to build classifiers for themselves. To address this issue, we propose a novel approach to few-shot text classification using... | Arth Bohra, Govert Verkes, Artem Harutyunyan, Pascal Weinberger, Giovanni Campagna |  |
| 1067 |  |  [Approximating CKY with Transformers](https://doi.org/10.18653/v1/2023.findings-emnlp.934) |  | 0 | We investigate the ability of transformer models to approximate the CKY algorithm, using them to directly predict a sentence’s parse and thus avoid the CKY algorithm’s cubic dependence on sentence length. We find that on standard constituency parsing benchmarks this approach achieves competitive or better performance than comparable parsers that make use of CKY, while being faster. We also evaluate the viability of this approach for parsing under random PCFGs. Here we find that performance... | Ghazal Khalighinejad, Ollie Liu, Sam Wiseman |  |
| 1068 |  |  [DialGuide: Aligning Dialogue Model Behavior with Developer Guidelines](https://doi.org/10.18653/v1/2023.findings-emnlp.935) |  | 0 | Dialogue models are able to generate coherent and fluent responses, but they can still be challenging to control and may produce non-engaging, unsafe results. This unpredictability diminishes user trust and can hinder the use of the models in the real world. To address this, we introduce DialGuide, a novel framework for controlling dialogue model behavior using natural language rules, or guidelines. These guidelines provide information about the context they are applicable to and what should be... | Prakhar Gupta, Yang Liu, Di Jin, Behnam Hedayatnia, Spandana Gella, Sijia Liu, Patrick Lange, Julia Hirschberg, Dilek HakkaniTur |  |
| 1069 |  |  [RWKV: Reinventing RNNs for the Transformer Era](https://doi.org/10.18653/v1/2023.findings-emnlp.936) |  | 0 | Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines... | Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan S. Wind, Stanislaw Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, RuiJie Zhu |  |
| 1070 |  |  [Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification](https://doi.org/10.18653/v1/2023.findings-emnlp.937) |  | 0 | Authorship verification (AV) is a fundamental task in natural language processing (NLP) and computational linguistics, with applications in forensic analysis, plagiarism detection, and identification of deceptive content. Existing AV techniques, including traditional stylometric and deep learning approaches, face limitations in terms of data requirements and lack of explainability. To address these limitations, this paper proposes PromptAV, a novel technique that leverages Large-Language Models... | ChiaYu Hung, Zhiqiang Hu, Yujia Hu, Roy KaWei Lee |  |
| 1071 |  |  [Transitioning Representations between Languages for Cross-lingual Event Detection via Langevin Dynamics](https://doi.org/10.18653/v1/2023.findings-emnlp.938) |  | 0 | Cross-lingual transfer learning (CLTL) for event detection (ED) aims to develop models in high-resource source languages that can be directly applied to produce effective performance for lower-resource target languages. Previous research in this area has focused on representation matching methods to develop a language-universal representation space into which source- and target-language example representations can be mapped to achieve cross-lingual transfer. However, as this approach modifies... | Chien Nguyen, Huy Nguyen, Franck Dernoncourt, Thien Huu Nguyen |  |
| 1072 |  |  [VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers](https://doi.org/10.18653/v1/2023.findings-emnlp.939) |  | 0 | Recent advances in interpretability suggest we can project weights and hidden states of transformer-based language models (LMs) to their vocabulary, a transformation that makes them more human interpretable. In this paper, we investigate LM attention heads and memory values, the vectors the models dynamically create and recall while processing a given input. By analyzing the tokens they represent through this projection, we identify patterns in the information flow inside the attention... | Shahar Katz, Yonatan Belinkov |  |
| 1073 |  |  [Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?](https://doi.org/10.18653/v1/2023.findings-emnlp.940) |  | 0 | Robustness, the ability of models to maintain performance in the face of perturbations, is critical for developing reliable NLP systems. Recent studies have shown promising results in improving the robustness of models through adversarial training and data augmentation. However, in machine translation, most of these studies have focused on bilingual machine translation with a single translation direction. In this paper, we investigate the transferability of robustness across different languages... | Leiyu Pan, Supryadi, Deyi Xiong |  |
| 1074 |  |  [Arabic Mini-ClimateGPT : A Climate Change and Sustainability Tailored Arabic LLM](https://doi.org/10.18653/v1/2023.findings-emnlp.941) |  | 0 | Climate change is one of the most significant challenges we face together as a society. Creating awareness and educating policy makers the wide-ranging impact of climate change is an essential step towards a sustainable future. Recently, Large Language Models (LLMs) like ChatGPT and Bard have shown impressive conversational abilities and excel in a wide variety of NLP tasks. While these models are close-source, recently alternative open-source LLMs such as Stanford Alpaca and Vicuna have shown... | Sahal Shaji Mullappilly, Abdelrahman M. Shaker, Omkar Thawakar, Hisham Cholakkal, Rao Muhammad Anwer, Salman H. Khan, Fahad Shahbaz Khan |  |
| 1075 |  |  [Interpreting Answers to Yes-No Questions in User-Generated Content](https://doi.org/10.18653/v1/2023.findings-emnlp.942) |  | 0 | Interpreting answers to yes-no questions in social media is difficult. Yes and no keywords are uncommon, and the few answers that include them are rarely to be interpreted what the keywords suggest. In this paper, we present a new corpus of 4,442 yes-no question-answer pairs from Twitter. We discuss linguistic characteristics of answers whose interpretation is yes or no, as well as answers whose interpretation is unknown. We show that large language models are far from solving this problem,... | Shivam Mathur, Keun Hee Park, Dhivya Chinnappa, Saketh Kotamraju, Eduardo Blanco |  |
| 1076 |  |  [Task-Aware Self-Supervised Framework for Dialogue Discourse Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.943) |  | 0 | Dialogue discourse parsing is a fundamental natural language processing task. It can benefit a series of conversation-related downstream tasks including dialogue summarization and emotion recognition in conversations. However, existing parsing approaches are constrained by predefined relation types, which can impede the adaptability of the parser for downstream tasks. To this end, we propose to introduce a task-aware paradigm to improve the versatility of the parser in this paper. Moreover, to... | Wei Li, Luyao Zhu, Wei Shao, Zonglin Yang, Erik Cambria |  |
| 1077 |  |  [Selective Demonstrations for Cross-domain Text-to-SQL](https://doi.org/10.18653/v1/2023.findings-emnlp.944) |  | 0 | Large language models (LLMs) with in-context learning have demonstrated impressive generalization capabilities in the cross-domain text-to-SQL task, without the use of in-domain annotations. However, incorporating in-domain demonstration examples has been found to greatly enhance LLMs’ performance. In this paper, we delve into the key factors within in-domain examples that contribute to the improvement and explore whether we can harness these benefits without relying on in-domain annotations.... | Shuaichen Chang, Eric FoslerLussier |  |
| 1078 |  |  [DocSplit: Simple Contrastive Pretraining for Large Document Embeddings](https://doi.org/10.18653/v1/2023.findings-emnlp.945) |  | 0 | Existing model pretraining methods only consider local information. For example, in the popular token masking strategy, the words closer to the masked token are more important for prediction than words far away. This results in pretrained models that generate high-quality sentence embeddings, but low-quality embeddings for large documents. We propose a new pretraining method called DocSplit which forces models to consider the entire global context of a large document. Our method uses a... | Yujie Wang, Mike Izbicki |  |
| 1079 |  |  [TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.946) |  | 0 | While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied and yet to be benchmarked. However, conducting such benchmarking studies is challenging because of the large variations in LLMs’ performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, this paper proposes a general... | Shubhra Kanti Karmaker Santu, Dongji Feng |  |
| 1080 |  |  [IntenDD: A Unified Contrastive Learning Approach for Intent Detection and Discovery](https://doi.org/10.18653/v1/2023.findings-emnlp.947) |  | 0 | Identifying intents from dialogue utterances forms an integral component of task-oriented dialogue systems. Intent-related tasks are typically formulated either as a classification task, where the utterances are classified into predefined categories or as a clustering task when new and previously unknown intent categories need to be discovered from these utterances. Further, the intent classification may be modeled in a multiclass (MC) or multilabel (ML) setup. While typically these tasks are... | Bhavuk Singhal, Ashim Gupta, Shivasankaran V. P, Amrith Krishna |  |
| 1081 |  |  [INarIG: Iterative Non-autoregressive Instruct Generation Model For Word-Level Auto Completion](https://doi.org/10.18653/v1/2023.findings-emnlp.948) |  | 0 | Computer-aided translation (CAT) aims to enhance human translation efficiency and is still important in scenarios where machine translation cannot meet quality requirements. One fundamental task within this field is Word-Level Auto Completion (WLAC). WLAC predicts a target word given a source sentence, translation context, and a human typed character sequence. Previous works either employ word classification models to exploit contextual information from both sides of the target word or directly... | Hengchao Shang, Zongyao Li, Daimeng Wei, Jiaxin Guo, Minghan Wang, Xiaoyu Chen, Lizhi Lei, Hao Yang |  |
| 1082 |  |  [Is the Answer in the Text? Challenging ChatGPT with Evidence Retrieval from Instructive Text](https://doi.org/10.18653/v1/2023.findings-emnlp.949) |  | 0 | Generative language models have recently shown remarkable success in generating answers to questions in a given textual context. However, these answers may suffer from hallucination, wrongly cite evidence, and spread misleading information. In this work, we address this problem by employing ChatGPT, a state-of-the-art generative model, as a machine-reading system. We ask it to retrieve answers to lexically varied and open-ended questions from trustworthy instructive texts. We introduce WHERE... | Sophie Henning, Talita Anthonio, Wei Zhou, Heike Adel, Mohsen Mesgar, Annemarie Friedrich |  |
| 1083 |  |  [PaRaDe: Passage Ranking using Demonstrations with LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.950) |  | 0 | Recent studies show that large language models (LLMs) can be instructed to effectively perform zero-shot passage re-ranking, in which the results of a first stage retrieval method, such as BM25, are rated and reordered to improve relevance. In this work, we improve LLM-based re-ranking by algorithmically selecting few-shot demonstrations to include in the prompt. Our analysis investigates the conditions where demonstrations are most helpful, and shows that adding even one demonstration is... | Andrew Drozdov, Honglei Zhuang, Zhuyun Dai, Zhen Qin, Razieh Rahimi, Xuanhui Wang, Dana Alon, Mohit Iyyer, Andrew McCallum, Donald Metzler, Kai Hui |  |
| 1084 |  |  [Learning Dynamic Representations for Discourse Dependency Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.951) |  | 0 | Transition systems have been widely used for the discourse dependency parsing task. Existing works often characterize transition states by examining a certain number of elementary discourse units (EDUs), while neglecting the arcs obtained from the transition history. In this paper, we propose to employ GAT-based encoder to learn dynamic representations for sub-trees constructed in previous transition steps. By incorporating these representations, our model is able to retain accessibility to all... | Tianyi Liu, Yansong Feng, Dongyan Zhao |  |
| 1085 |  |  [K-HATERS: A Hate Speech Detection Corpus in Korean with Target-Specific Ratings](https://doi.org/10.18653/v1/2023.findings-emnlp.952) |  | 0 | Numerous datasets have been proposed to combat the spread of online hate. Despite these efforts, a majority of these resources are English-centric, primarily focusing on overt forms of hate. This research gap calls for developing high-quality corpora in diverse languages that also encapsulate more subtle hate expressions. This study introduces K-HATERS, a new corpus for hate speech detection in Korean, comprising approximately 192K news comments with target-specific offensiveness ratings. This... | Chaewon Park, Soohwan Kim, Kyubyong Park, Kunwoo Park |  |
| 1086 |  |  [Mitigating Data Imbalance and Representation Degeneration in Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.953) |  | 0 | Despite advances in multilingual neural machine translation (MNMT), we argue that there are still two major challenges in this area: data imbalance and representation degeneration. The data imbalance problem refers to the imbalance in the amount of parallel corpora for all language pairs, especially for long-tail languages (i.e., very low-resource languages). The representation degeneration problem refers to the problem of encoded tokens tending to appear only in a small subspace of the full... | Wen Lai, Alexandra Chronopoulou, Alexander Fraser |  |
| 1087 |  |  [BotPercent: Estimating Bot Populations in Twitter Communities](https://doi.org/10.18653/v1/2023.findings-emnlp.954) |  | 0 | Twitter bot detection is vital in combating misinformation and safeguarding the integrity of social media discourse. While malicious bots are becoming more and more sophisticated and personalized, standard bot detection approaches are still agnostic to social environments (henceforth, communities) the bots operate at. In this work, we introduce community-specific bot detection, estimating the percentage of bots given the context of a community. Our method—BotPercent—is an amalgamation of... | Zhaoxuan Tan, Shangbin Feng, Melanie Sclar, Herun Wan, Minnan Luo, Yejin Choi, Yulia Tsvetkov |  |
| 1088 |  |  [The Locality and Symmetry of Positional Encodings](https://doi.org/10.18653/v1/2023.findings-emnlp.955) |  | 0 | Positional Encodings (PEs) are used to inject word-order information into transformer-based language models. While they can significantly enhance the quality of sentence representations, their specific contribution to language models is not fully understood, especially given recent findings that various positional encodings are insensitive to word order. In this work, we conduct a systematic study of positional encodings in Bidirectional Masked Language Models (BERT-style) , which complements... | Lihu Chen, Gaël Varoquaux, Fabian M. Suchanek |  |
| 1089 |  |  [Towards a Deep Understanding of Multilingual End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.956) |  | 0 | In this paper, we employ Singular Value Canonical Correlation Analysis (SVCCA) to analyze representations learnt in a multilingual end-to-end speech translation model trained over 22 languages. SVCCA enables us to estimate representational similarity across languages and layers, enhancing our understanding of the functionality of multilingual speech translation and its potential connection to multilingual neural machine translation. The multilingual speech translation model is trained on the... | Haoran Sun, Xiaohu Zhao, Yikun Lei, Shaolin Zhu, Deyi Xiong |  |
| 1090 |  |  [An Empirical Investigation of Implicit and Explicit Knowledge-Enhanced Methods for Ad Hoc Dataset Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.957) |  | 0 | Ad hoc dataset retrieval has become an important way of finding data on the Web, where the underlying problem is how to measure the relevance of a dataset to a query. State-of-the-art solutions for this task are still lexical methods, which cannot capture semantic similarity. Semantics-aware knowledge-enhanced retrieval methods, which achieved promising results on other tasks, have yet to be systematically studied on this specialized task. To fill the gap, in this paper, we present an empirical... | Weiqing Luo, Qiaosheng Chen, Zhiyang Zhang, Zixian Huang, Gong Cheng |  |
| 1091 |  |  [A Multi-Modal Multilingual Benchmark for Document Image Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.958) |  | 0 | Document image classification is different from plain-text document classification and consists of classifying a document by understanding the content and structure of documents such as forms, emails, and other such documents. We show that the only existing dataset for this task (Lewis et al., 2006) has several limitations and we introduce two newly curated multilingual datasets WIKI-DOC and MULTIEURLEX-DOC that overcome these limitations. We further undertake a comprehensive study of popular... | Yoshinari Fujinuma, Siddharth Varia, Nishant Sankaran, Srikar Appalaraju, Bonan Min, Yogarshi Vyas |  |
| 1092 |  |  [Unnatural language processing: How do language models handle machine-generated prompts?](https://doi.org/10.18653/v1/2023.findings-emnlp.959) |  | 0 | Language model prompt optimization research has shown that semantically and grammatically well-formed manually crafted prompts are routinely outperformed by automatically generated token sequences with no apparent meaning or syntactic structure, including sequences of vectors from a model’s embedding space. We use machine-generated prompts to probe how models respond to input that is not composed of natural language expressions. We study the behavior of models of different sizes in multiple... | Corentin Kervadec, Francesca Franzon, Marco Baroni |  |
| 1093 |  |  [Investigating the Effectiveness of Multiple Expert Models Collaboration](https://doi.org/10.18653/v1/2023.findings-emnlp.960) |  | 0 | This paper aims to investigate the effectiveness of several machine translation (MT) models and aggregation methods in a multi-domain setting under fair conditions and explore a direction for tackling multi-domain MT. We mainly compare the performance of the single model approach by jointly training all domains and the multi-expert models approach with a particular aggregation strategy. We conduct experiments on multiple domain datasets and demonstrate that a combination of smaller domain... | Ikumi Ito, Takumi Ito, Jun Suzuki, Kentaro Inui |  |
| 1094 |  |  [Gradually Excavating External Knowledge for Implicit Complex Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.961) |  | 0 | Recently, large language models (LLMs) have gained much attention for the emergence of human-comparable capabilities and huge potential. However, for open-domain implicit question-answering problems, LLMs may not be the ultimate solution due to the reasons of: 1) uncovered or out-of-date domain knowledge, 2) one-shot generation and hence restricted comprehensiveness. To this end, this work proposes a gradual knowledge excavation framework for open-domain complex question answering, where LLMs... | Chang Liu, Xiaoguang Li, Lifeng Shang, Xin Jiang, Qun Liu, Edmund Y. Lam, Ngai Wong |  |
| 1095 |  |  [Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.962) |  | 0 | The emotions we experience involve complex processes; besides physiological aspects, research in psychology has studied cognitive appraisals where people assess their situations subjectively, according to their own values (Scherer, 2005). Thus, the same situation can often result in different emotional experiences. While the detection of emotion is a well-established task, there is very limited work so far on the automatic prediction of cognitive appraisals. This work fills the gap by... | Hongli Zhan, Desmond C. Ong, Junyi Jessy Li |  |
| 1096 |  |  [Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.963) |  | 0 | The impressive achievements of transformers force NLP researchers to delve into how these models represent the underlying structure of natural language. In this paper, we propose a novel standpoint to investigate the above issue: using typological similarities among languages to observe how their respective monolingual models encode structural information. We aim to layer-wise compare transformers for typologically similar languages to observe whether these similarities emerge for particular... | Elena Sofia Ruzzetti, Federico Ranaldi, Felicia Logozzo, Michele Mastromattei, Leonardo Ranaldi, Fabio Massimo Zanzotto |  |
| 1097 |  |  [Discourse Sense Flows: Modelling the Rhetorical Style of Documents across Various Domains](https://doi.org/10.18653/v1/2023.findings-emnlp.964) |  | 0 | Recent research on shallow discourse parsing has given renewed attention to the role of discourse relation signals, in particular explicit connectives and so-called alternative lexicalizations. In our work, we first develop new models for extracting signals and classifying their senses, both for explicit connectives and alternative lexicalizations, based on the Penn Discourse Treebank v3 corpus. Thereafter, we apply these models to various raw corpora, and we introduce ‘discourse sense flows’,... | René Knaebel, Manfred Stede |  |
| 1098 |  |  [HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework for Cross-Domain Zero-Shot Slot Filling](https://doi.org/10.18653/v1/2023.findings-emnlp.965) |  | 0 | In task-oriented dialogue scenarios, cross-domain zero-shot slot filling plays a vital role in leveraging source domain knowledge to learn a model with high generalization ability in unknown target domain where annotated data is unavailable. However, the existing state-of-the-art zero-shot slot filling methods have limited generalization ability in target domain, they only show effective knowledge transfer on seen slots and perform poorly on unseen slots. To alleviate this issue, we present a... | Junwen Zhang, Yin Zhang |  |
| 1099 |  |  [A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing](https://doi.org/10.18653/v1/2023.findings-emnlp.966) |  | 0 | We evaluate a range of recent LLMs on English creative writing, a challenging and complex task that requires imagination, coherence, and style. We use a difficult, open-ended scenario chosen to avoid training data reuse: an epic narration of a single combat between Ignatius J. Reilly, the protagonist of the Pulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl, a prehistoric flying reptile. We ask several LLMs and humans to write such a story and conduct a human... | Carlos GómezRodríguez, Paul Williams |  |
| 1100 |  |  [1-PAGER: One Pass Answer Generation and Evidence Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.967) |  | 0 | We present 1-Pager the first system that answers a question and retrieves evidence using a single Transformer-based model and decoding process. 1-Pager incrementally partitions the retrieval corpus using constrained decoding to select a document and answer string, and we show that this is competitive with comparable retrieve-and-read alternatives according to both retrieval and answer accuracy metrics. 1-Pager also outperforms the equivalent ‘closed-book’ question answering model, by grounding... | Palak Jain, Livio Soares, Tom Kwiatkowski |  |
| 1101 |  |  [Context-faithful Prompting for Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.968) |  | 0 | Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs’ contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that... | Wenxuan Zhou, Sheng Zhang, Hoifung Poon, Muhao Chen |  |
| 1102 |  |  [InfoCL: Alleviating Catastrophic Forgetting in Continual Text Classification from An Information Theoretic Perspective](https://doi.org/10.18653/v1/2023.findings-emnlp.969) |  | 0 | Continual learning (CL) aims to constantly learn new knowledge over time while avoiding catastrophic forgetting on old tasks. We focus on continual text classification under the class-incremental setting. Recent CL studies have identified the severe performance decrease on analogous classes as a key factor for catastrophic forgetting. In this paper, through an in-depth exploration of the representation learning process in CL, we discover that the compression effect of the information bottleneck... | Yifan Song, Peiyi Wang, Weimin Xiong, Dawei Zhu, Tianyu Liu, Zhifang Sui, Sujian Li |  |
| 1103 |  |  [Sparse Frame Grouping Network with Action Centered for Untrimmed Video Paragraph Captioning](https://doi.org/10.18653/v1/2023.findings-emnlp.970) |  | 0 | Generating paragraph captions for untrimmed videos without event annotations is challenging, especially when aiming to enhance precision and minimize repetition at the same time. To address this challenge, we propose a module called Sparse Frame Grouping (SFG). It dynamically groups event information with the help of action information for the entire video and excludes redundant frames within pre-defined clips. To enhance the performance, an Intra Contrastive Learning technique is designed to... | Guorui Yu, Yimin Hu, Yuejie Zhang, Rui Feng, Tao Zhang, Shang Gao |  |
| 1104 |  |  [Unsupervised Binary Code Translation with Application to Code Clone Detection and Vulnerability Discovery](https://doi.org/10.18653/v1/2023.findings-emnlp.971) |  | 0 | Binary code analysis has immense importance in the research domain of software security. Today, software is very often compiled for various Instruction Set Architectures (ISAs). As a result, cross-architecture binary code analysis has become an emerging problem. Recently, deep learning-based binary analysis has shown promising success. It is widely known that training a deep learning model requires a massive amount of data. However, for some low-resource ISAs, an adequate amount of data is hard... | Iftakhar Ahmad, Lannan Luo |  |
| 1105 |  |  [Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.972) |  | 0 | We address the task of evidence retrieval for long document question answering, which involves locating relevant paragraphs within a document to answer a question. We aim to assess the applicability of large language models (LLMs) in the task of zero-shot long document evidence retrieval, owing to their unprecedented performance across various NLP tasks. However, currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global... | Inderjeet Nair, Shwetha Somasundaram, Apoorv Saxena, Koustava Goswami |  |
| 1106 |  |  [Emergent Inabilities? Inverse Scaling Over the Course of Pretraining](https://doi.org/10.18653/v1/2023.findings-emnlp.973) |  | 0 | Does inverse scaling only occur as a function of model size, or can it also occur over the course of training? We carry out an exploratory study investigating whether the performance of language models on specific tasks can decrease (while general performance remains high) during training on the language modeling task. We find 8 tasks on which Pythia 12B (Biderman et al., 2023) shows decreased performance over the course of training. Five of these tasks (TruthfulQA-MC1, TruthfulQA-MC2,... | James A. Michaelov, Ben Bergen |  |
| 1107 |  |  [Alignment Precedes Fusion: Open-Vocabulary Named Entity Recognition as Context-Type Semantic Matching](https://doi.org/10.18653/v1/2023.findings-emnlp.974) |  | 0 | Despite the significant progress in developing named entity recognition models, scaling to novel-emerging types still remains challenging in real-world scenarios. Continual learning and zero-shot learning approaches have been explored to handle novel-emerging types with less human supervision, but they have not been as successfully adopted as supervised approaches. Meanwhile, humans possess a much larger vocabulary size than these approaches and have the ability to learn the alignment between... | Zhuoran Jin, Pengfei Cao, Zhitao He, Yubo Chen, Kang Liu, Jun Zhao |  |
| 1108 |  |  [Representation Projection Invariance Mitigates Representation Collapse](https://doi.org/10.18653/v1/2023.findings-emnlp.975) |  | 0 | Fine-tuning contextualized representations learned by pre-trained language models remains a prevalent practice in NLP. However, fine-tuning can lead to representation degradation (also known as representation collapse), which may result in instability, sub-optimal performance, and weak generalization. In this paper, we propose Representation Projection Invariance (REPINA), a novel regularization method to maintain the information content of representation and reduce representation collapse... | Anastasia Razdaibiedina, Ashish Khetan, Zohar Karnin, Daniel Khashabi, Vivek Madan |  |
| 1109 |  |  [Tunable Soft Prompts are Messengers in Federated Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.976) |  | 0 | Federated learning (FL) enables multiple participants to collaboratively train machine learning models using decentralized data sources, alleviating privacy concerns that arise from directly sharing local data. However, the lack of model privacy protection in FL becomes an unneglectable challenge, especially when people want to federally finetune models based on a proprietary large language model. In this study, we propose a novel FL training approach that accomplishes information exchange... | Chenhe Dong, Yuexiang Xie, Bolin Ding, Ying Shen, Yaliang Li |  |
| 1110 |  |  [Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.977) |  | 0 | Automatically generated reports from medical images promise to improve the workflow of radiologists. Existing methods consider an image-to-report modeling task by directly generating a fully-fledged report from an image. However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports. To address this, we propose a two-step approach for radiology report generation. First, we... | Benjamin Yan, Ruochen Liu, David E. Kuo, Subathra Adithan, Eduardo Pontes Reis, Stephen Kwak, Vasantha Kumar Venugopal, Chloe O'Connell, Agustina Saenz, Pranav Rajpurkar, Michael Moor |  |
| 1111 |  |  [Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs](https://doi.org/10.18653/v1/2023.findings-emnlp.978) |  | 0 | This paper presents an in-depth study of multimodal machine translation (MMT), examining the prevailing understanding that MMT systems exhibit decreased sensitivity to visual information when text inputs are complete. Instead, we attribute this phenomenon to insufficient cross-modal interaction, rather than image information redundancy. A novel approach is proposed to generate parallel Visual Question-Answering (VQA) style pairs from the source text, fostering more robust cross-modal... | Yuxin Zuo, Bei Li, Chuanhao Lv, Tong Zheng, Tong Xiao, JingBo Zhu |  |
| 1112 |  |  [GenKIE: Robust Generative Multimodal Document Key Information Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.979) |  | 0 | Key information extraction (KIE) from scanned documents has gained increasing attention because of its applications in various domains. Although promising results have been achieved by some recent KIE approaches, they are usually built based on discriminative models, which lack the ability to handle optical character recognition (OCR) errors and require laborious token-level labeling. In this paper, we propose a novel generative end-to-end model, named GenKIE, to address the KIE task. GenKIE is... | Panfeng Cao, Ye Wang, Qiang Zhang, Zaiqiao Meng |  |
| 1113 |  |  [Improving Multimodal Sentiment Analysis: Supervised Angular margin-based Contrastive Learning for Enhanced Fusion Representation](https://doi.org/10.18653/v1/2023.findings-emnlp.980) |  | 0 | The effectiveness of a model is heavily reliant on the quality of the fusion representation of multiple modalities in multimodal sentiment analysis. Moreover, each modality is extracted from raw input and integrated with the rest to construct a multimodal representation. Although previous methods have proposed multimodal representations and achieved promising results, most of them focus on forming positive and negative pairs, neglecting the variation in sentiment scores within the same class.... | CongDuy Nguyen, Thong Nguyen, Duc Anh Vu, Anh Tuan Luu |  |
| 1114 |  |  [Efficient Multilingual Language Model Compression through Vocabulary Trimming](https://doi.org/10.18653/v1/2023.findings-emnlp.981) |  | 0 | Multilingual language models (LMs) have become a powerful tool in NLP, especially for non-English languages. Nevertheless, model parameters of multilingual LMs remain large due to the larger embedding matrix of the vocabulary covering tokens in different languages. Instead, monolingual LMs can be trained in a target language with the language-specific vocabulary only. In this paper, we propose vocabulary-trimming (VT), a method to reduce a multilingual LM vocabulary to a target language by... | Asahi Ushio, Yi Zhou, José CamachoCollados |  |
| 1115 |  |  [ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.982) |  | 0 | Most multilingual vision-and-language (V&L) research aims to accomplish multilingual and multimodal capabilities within one model. However, the scarcity of multilingual captions for images has hindered the development. To overcome this obstacle, we propose ICU, Image Caption Understanding, which divides a V&L task into two stages: a V&L model performs image captioning in English, and a multilingual language model (mLM), in turn, takes the caption as the alt text and performs cross-lingual... | Guojun Wu |  |
| 1116 |  |  [GTA: Gated Toxicity Avoidance for LM Performance Preservation](https://doi.org/10.18653/v1/2023.findings-emnlp.983) |  | 0 | Caution: This paper includes offensive words that could potentially cause unpleasantness. The fast-paced evolution of generative language models such as GPT-4 has demonstrated outstanding results in various NLP generation tasks. However, due to the potential generation of offensive words related to race or gender, various Controllable Text Generation (CTG) methods have been proposed to mitigate the occurrence of harmful words. However, existing CTG methods not only reduce toxicity but also... | Heegyu Kim, Hyunsouk Cho |  |
| 1117 |  |  [LMGQS: A Large-scale Dataset for Query-focused Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.984) |  | 0 | Query-focused summarization (QFS) aims to extract or generate a summary of an input document that directly answers or is relevant to a given query. The lack of large-scale datasets in the form of documents, queries, and summaries has hindered model development in this area. In contrast, multiple large-scale high-quality datasets for generic summarization exist. We hypothesize that there is a hidden query for each summary sentence in a generic summarization annotation, and we utilize a... | Ruochen Xu, Song Wang, Yang Liu, Shuohang Wang, Yichong Xu, Dan Iter, Pengcheng He, Chenguang Zhu, Michael Zeng |  |
| 1118 |  |  [ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.985) |  | 0 | Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning. To improve the reasoning abilities, we propose ChatCoT, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs (e.g., ChatGPT). In ChatCoT, we model the chain-of-thought (CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through... | Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Xin Zhao, JiRong Wen |  |
| 1119 |  |  [Non-Autoregressive Document-Level Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.986) |  | 0 | Non-autoregressive translation (NAT) models achieve comparable performance and superior speed compared to auto-regressive translation (AT) models in the context of sentence-level machine translation (MT). However, their abilities are unexplored in document-level MT, hindering their usage in real scenarios. In this paper, we conduct a comprehensive examination of typical NAT models in the context of document-level MT and further propose a simple but effective design of sentence alignment between... | Guangsheng Bao, Zhiyang Teng, Hao Zhou, Jianhao Yan, Yue Zhang |  |
| 1120 |  |  [Exploring the Effectiveness of Multi-Lingual Commonsense Knowledge-Aware Open-Domain Dialogue Response Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.987) |  | 0 | Prior works have shown the promising results of commonsense knowledge-aware models in improving informativeness while reducing the hallucination issue. Nonetheless, prior works often can only use monolingual knowledge whose language is consistent with the dialogue context. Except for a few high-resource languages, such as English and Chinese, most languages suffer from insufficient knowledge issues, especially minority languages. To this end, this work proposes a new task, Multi-Lingual... | Sixing Wu, Jiong Yu, Tianshi Che, Yang Zhou, Wei Zhou |  |
| 1121 |  |  [Mixture of Soft Prompts for Controllable Data Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.988) |  | 0 | Large language models (LLMs) effectively generate fluent text when the target output follows natural language patterns. However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations. We flip the problem on... | Derek Chen, Celine Lee, Yunan Lu, Domenic Rosati, Zhou Yu |  |
| 1122 |  |  [A Boundary Offset Prediction Network for Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.989) |  | 0 | Named entity recognition (NER) is a fundamental task in natural language processing that aims to identify and classify named entities in text. However, span-based methods for NER typically assign entity types to text spans, resulting in an imbalanced sample space and neglecting the connections between non-entity and entity spans. To address these issues, we propose a novel approach for NER, named the Boundary Offset Prediction Network (BOPN), which predicts the boundary offsets between... | Minghao Tang, Yongquan He, Yongxiu Xu, Hongbo Xu, Wenyuan Zhang, Yang Lin |  |
| 1123 |  |  [Prefix-Tuning Based Unsupervised Text Style Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.990) |  | 0 | Unsupervised text style transfer aims at training a generative model that can alter the style of the input sentence while preserving its content without using any parallel data. In this paper, we employ powerful pre-trained large language models and present a new prefix-tuning-based method for unsupervised text style transfer. We construct three different kinds of prefixes, i.e., shared prefix, style prefix, and content prefix, to encode task-specific information, target style, and the content... | Huiyu Mai, Wenhao Jiang, ZhiHong Deng |  |
| 1124 |  |  [Evaluating and Enhancing the Robustness of Code Pre-trained Models through Structure-Aware Adversarial Samples Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.991) |  | 0 | Code pre-trained models (CodePTMs) have significantly advanced the field of neural code intelligence. Despite their capabilities, these models are susceptible to adversarial attacks that subtly modify the model inputs, resulting in incorrect outputs or predictions. Previous methods of robustness evaluation for CodePTMs primarily stem from a textual perspective, without explicitly taking into account the structure of the code. Furthermore, prior studies fail to encompass a broad enough spectrum... | Nuo Chen, Qiushi Sun, Jianing Wang, Ming Gao, Xiaoli Li, Xiang Li |  |
| 1125 |  |  [Annotation Sensitivity: Training Data Collection Methods Affect Model Performance](https://doi.org/10.18653/v1/2023.findings-emnlp.992) |  | 0 | When training data are collected from human annotators, the design of the annotation instrument, the instructions given to annotators, the characteristics of the annotators, and their interactions can impact training data. This study demonstrates that design choices made when creating an annotation instrument also impact the models trained on the resulting annotations. We introduce the term annotation sensitivity to refer to the impact of annotation data collection methods on the annotations... | Christoph Kern, Stephanie Eckman, Jacob Beck, Rob Chew, Bolei Ma, Frauke Kreuter |  |
| 1126 |  |  [Qualitative Code Suggestion: A Human-Centric Approach to Qualitative Coding](https://doi.org/10.18653/v1/2023.findings-emnlp.993) |  | 0 | Qualitative coding is a content analysis method in which researchers read through a text corpus and assign descriptive labels or qualitative codes to passages. It is an arduous and manual process which human-computer interaction (HCI) studies have shown could greatly benefit from NLP techniques to assist qualitative coders. Yet, previous attempts at leveraging language technologies have set up qualitative coding as a fully automatable classification problem. In this work, we take a more... | Cesare Spinoso Di Piano, Samira Abbasgholizadeh Rahimi, Jackie Chi Kit Cheung |  |
| 1127 |  |  [D²TV: Dual Knowledge Distillation and Target-oriented Vision Modeling for Many-to-Many Multimodal Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.994) |  | 0 | Many-to-many multimodal summarization (M3S) task aims to generate summaries in any language with document inputs in any language and the corresponding image sequence, which essentially comprises of multimodal monolingual summarization (MMS) and multimodal cross-lingual summarization (MXLS) tasks. Although much work has been devoted to either MMS or MXLS, little research pays attention to the M3S task. Besides, existing studies mainly focus on 1) utilizing MMS to enhance MXLS via knowledge... | Yunlong Liang, Fandong Meng, Jiaan Wang, Jinan Xu, Yufeng Chen, Jie Zhou |  |
| 1128 |  |  [Improving Input-label Mapping with Demonstration Replay for In-context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.995) |  | 0 | In-context learning (ICL) is an emerging capability of large autoregressive language models where a few input-label demonstrations are appended to the input to enhance the model’s understanding of downstream NLP tasks, without directly adjusting the model parameters. The effectiveness of ICL can be attributed to the strong language modeling capabilities of large language models (LLMs), which enable them to learn the mapping between input and labels based on in-context demonstrations. Despite... | Zhuocheng Gong, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui Yan |  |
| 1129 |  |  [Enhancing Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies](https://doi.org/10.18653/v1/2023.findings-emnlp.996) |  | 0 | In-context learning (ICL) has emerged as a new approach to various natural language processing tasks, utilizing large language models (LLMs) to make predictions based on context that has been supplemented with a few examples or task-specific instructions. In this paper, we aim to extend this method to question answering tasks that utilize structured knowledge sources, and improve Text-to-SQL systems by exploring various prompt design strategies for employing LLMs. We conduct a systematic... | Linyong Nan, Yilun Zhao, Weijin Zou, Narutatsu Ri, Jaesung Tae, Ellen Zhang, Arman Cohan, Dragomir Radev |  |
| 1130 |  |  [Cross-lingual Open-Retrieval Question Answering for African Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.997) |  | 0 | African languages have far less in-language content available digitally, making it challenging for question answering systems to satisfy the information needs of users. Cross-lingual open-retrieval question answering (XOR QA) systems – those that retrieve answer content from other languages while serving people in their native language—offer a means of filling this gap. To this end, we create Our Dataset, the first cross-lingual QA dataset with a focus on African languages. Our Dataset includes... | Odunayo Ogundepo, Tajuddeen Gwadabe, Clara Rivera, Jonathan H. Clark, Sebastian Ruder, David Ifeoluwa Adelani, Bonaventure Dossou, Abdou Aziz Diop, Claytone Sikasote, Gilles Hacheme, Happy Buzaaba, Ignatius Ezeani, Rooweither Mabuya, Salomey Osei, Chris Emezue, Albert Kahira, Shamsuddeen Hassan Muhammad, Akintunde Oladipo, Abraham Toluwase Owodunni, Atnafu Lambebo Tonja, Iyanuoluwa Shode, Akari Asai, Aremu Anuoluwapo, Ayodele Awokoya, Bernard Opoku, Chiamaka Chukwuneke, Christine Mwase, Clemencia Siro, Stephen Arthur, Tunde Ajayi, Verrah Otiende, Andre Niyongabo Rubungo, Boyd Sinkala, Daniel A. Ajisafe, Emeka Onwuegbuzia, Falalu Ibrahim Lawan, Ibrahim Said Ahmad, Jesujoba O. Alabi, Chinedu E. Mbonu, Mofetoluwa Adeyemi, Mofya Phiri, Orevaoghene Ahia, Ruqayya Nasir Iro, Sonia Adhiambo |  |
| 1131 |  |  [Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens](https://doi.org/10.18653/v1/2023.findings-emnlp.998) |  | 0 | We argue that translation quality alone is not a sufficient metric for measuring knowledge transfer in multilingual neural machine translation. To support this claim, we introduce Representational Transfer Potential (RTP), which measures representational similarities between languages. We show that RTP can measure both positive and negative transfer (interference), and find that RTP is strongly correlated with changes in translation quality, indicating that transfer does occur. Furthermore, we... | David Stap, Vlad Niculae, Christof Monz |  |
| 1132 |  |  [Aligning Predictive Uncertainty with Clarification Questions in Grounded Dialog](https://doi.org/10.18653/v1/2023.findings-emnlp.999) |  | 0 | Asking for clarification is fundamental to effective collaboration. An interactive artificial agent must know when to ask a human instructor for more information in order to ascertain their goals. Previous work bases the timing of questions on supervised models learned from interactions between humans. Instead of a supervised classification task, we wish to ground the need for questions in the acting agent’s predictive uncertainty. In this work, we investigate if ambiguous linguistic... | Kata Naszádi, Putra Manggala, Christof Monz |  |
| 1133 |  |  [Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.1000) |  | 0 | Prompting Large Language Models (LLMs) performs impressively in zero- and few-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot afford the cost of creating large task-specific training datasets, but also the cost of pretraining their own LLMs, are increasingly turning to third-party services that allow them to prompt LLMs. However, such services currently require a payment per call, which becomes a significant operating expense (OpEx). Furthermore, customer inputs are... | Ilias Stogiannidis, Stavros Vassos, Prodromos Malakasiotis, Ion Androutsopoulos |  |
| 1134 |  |  [ParroT: Translating during Chat using Large Language Models tuned with Human Translation and Feedback](https://doi.org/10.18653/v1/2023.findings-emnlp.1001) |  | 0 | Large language models (LLMs) like ChatGPT have exhibited remarkable abilities on a wide range of natural language processing (NLP) tasks, including various machine translation abilities accomplished during chat. However, these models are only accessible through restricted APIs, which creates barriers to new research and advancements in the field. Therefore, we propose ParroT, a framework to enhance and regulate the translation abilities during chat based on open-source LLMs (e.g., LLaMA),... | Wenxiang Jiao, Jentse Huang, Wenxuan Wang, Zhiwei He, Tian Liang, Xing Wang, Shuming Shi, Zhaopeng Tu |  |
| 1135 |  |  [Dense Retrieval as Indirect Supervision for Large-space Decision Making](https://doi.org/10.18653/v1/2023.findings-emnlp.1002) |  | 0 | Many discriminative natural language understanding (NLU) tasks have large label spaces. Learning such a process of large-space decision making is particularly challenging due to the lack of training instances per label and the difficulty of selection among many fine-grained labels. Inspired by dense retrieval methods for passage finding in open-domain QA, we propose a reformulation of large-space discriminative NLU tasks as a learning-to-retrieve task, leading to a novel solution named Dense... | Nan Xu, Fei Wang, Mingtao Dong, Muhao Chen |  |
| 1136 |  |  [One-Model-Connects-All: A Unified Graph Pre-Training Model for Online Community Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.1003) |  | 0 | Online community is composed of communities, users, and user-generated textual content, with rich information that can help us solve social problems. Previous research hasn’t fully utilized these three components and the relationship among them. What’s more, they can’t adapt to a wide range of downstream tasks. To solve these problems, we focus on a framework that simultaneously considers communities, users, and texts. And it can easily connect with a variety of downstream tasks related to... | Ruoxue Ma, Jiarong Xu, Xinnong Zhang, Haozhe Zhang, Zuyu Zhao, Qi Zhang, Xuanjing Huang, Zhongyu Wei |  |
| 1137 |  |  [In-Image Neural Machine Translation with Segmented Pixel Sequence-to-Sequence Model](https://doi.org/10.18653/v1/2023.findings-emnlp.1004) |  | 0 | In-Image Machine Translation (IIMT) aims to convert images containing texts from one language to another. Traditional approaches for this task are cascade methods, which utilize optical character recognition (OCR) followed by neural machine translation (NMT) and text rendering. However, the cascade methods suffer from compounding errors of OCR and NMT, leading to a decrease in translation quality. In this paper, we propose an end-to-end model instead of the OCR, NMT and text rendering pipeline.... | Yanzhi Tian, Xiang Li, Zeming Liu, Yuhang Guo, Bin Wang |  |
| 1138 |  |  [NarrativeXL: a Large-scale Dataset for Long-Term Memory Models](https://doi.org/10.18653/v1/2023.findings-emnlp.1005) |  | 0 | We propose a new large-scale (nearly a million questions) ultra-long-context (more than 50,000 words average document length) reading comprehension dataset. Using GPT 3.5, we summarized each scene in 1,500 hand-curated fiction books from Project Gutenberg, which resulted in approximately 150 scene-level summaries per book. After that, we created a number of reading comprehension questions based on these summaries, including three types of multiple-choice scene recognition questions, as well as... | Arsenii Moskvichev, KyVinh Mai |  |
| 1139 |  |  [Dialogue Act-Aided Backchannel Prediction Using Multi-Task Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.1006) |  | 0 | Produced in the form of small injections such as “Yeah!” or “Uh-Huh” by listeners in a conversation, supportive verbal feedback (i.e., backchanneling) is essential for natural dialogue. Highlighting its tight relation to speaker intent and utterance type, we propose a multi-task learning approach that learns textual representations for the task of backchannel prediction in tandem with dialogue act classification. We demonstrate the effectiveness of our approach by improving the prediction of... | Wencke Liermann, YoHan Park, YongSeok Choi, KongJoo Lee |  |
| 1140 |  |  [mReFinED: An Efficient End-to-End Multilingual Entity Linking System](https://doi.org/10.18653/v1/2023.findings-emnlp.1007) |  | 0 | End-to-end multilingual entity linking (MEL) is concerned with identifying multilingual entity mentions and their corresponding entity IDs in a knowledge base. Existing works assumed that entity mentions were given and skipped the entity mention detection step due to a lack of high-quality multilingual training corpora. To overcome this limitation, we propose mReFinED, the first end-to-end multilingual entity linking. Additionally, we propose a bootstrapping mention detection framework that... | Peerat Limkonchotiwat, Weiwei Cheng, Christos Christodoulopoulos, Amir Saffari, Jens Lehmann |  |
| 1141 |  |  [Sub-network Discovery and Soft-masking for Continual Learning of Mixed Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.1008) |  | 0 | Continual learning (CL) has two main objectives: preventing catastrophic forgetting (CF) and encouraging knowledge transfer (KT). The existing literature mainly focused on overcoming CF. Some work has also been done on KT when the tasks are similar. To our knowledge, only one method has been proposed to learn a sequence of mixed tasks. However, these techniques still suffer from CF and/or limited KT. This paper proposes a new CL method to achieve both. It overcomes CF by isolating the knowledge... | Zixuan Ke, Bing Liu, Wenhan Xiong, Asli Celikyilmaz, Haoran Li |  |
| 1142 |  |  [PIVOINE: Instruction Tuning for Open-world Entity Profiling](https://doi.org/10.18653/v1/2023.findings-emnlp.1009) |  | 0 | This work considers the problem of Open-world Entity Profiling, a sub-domain of Open-world Information Extraction (Open-world IE). Unlike the conventional closed-world IE, Open-world IE is considered a more general situation where entities and relations could be beyond a predefined ontology. We seek to develop a large language model (LLM) that can perform Open-world Entity Profiling with instruction tuning to extract desirable entity profiles characterized by (possibly fine-grained) natural... | Keming Lu, Xiaoman Pan, Kaiqiang Song, Hongming Zhang, Dong Yu, Jianshu Chen |  |
| 1143 |  |  [DiQAD: A Benchmark Dataset for Open-domain Dialogue Quality Assessment](https://doi.org/10.18653/v1/2023.findings-emnlp.1010) |  | 0 | Dialogue assessment plays a critical role in the development of open-domain dialogue systems. Existing work are uncapable of providing an end-to-end and human-epistemic assessment dataset, while they only provide sub-metrics like coherence or the dialogues are conversed between annotators far from real user settings. In this paper, we release a large-scale dialogue quality assessment dataset (DiQAD), for automatically assessing open-domain dialogue quality. Specifically, we (1) establish the... | Yukun Zhao, Lingyong Yan, Weiwei Sun, Chong Meng, Shuaiqiang Wang, Zhicong Cheng, Zhaochun Ren, Dawei Yin |  |
| 1144 |  |  [Tuna: Instruction Tuning using Feedback from Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.1011) |  | 0 | Instruction tuning of open-source large language models (LLMs) like LLaMA, using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4, has proven to be a cost-effective way to align model behaviors with human preferences. However, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses. In this paper, we propose finetuning an instruction-tuned LLM using our novel probabilistic ranking and contextual ranking... | Haoran Li, Yiran Liu, Xingxing Zhang, Wei Lu, Furu Wei |  |
| 1145 |  |  [Emptying the Ocean with a Spoon: Should We Edit Models?](https://doi.org/10.18653/v1/2023.findings-emnlp.1012) |  | 0 | We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations. We contrast model editing with three similar but distinct approaches that pursue better defined objectives: (1) retrieval-based architectures, which decouple factual memory from inference and linguistic capabilities embodied in LLMs; (2) concept erasure methods, which aim at preventing systemic bias in generated text; and (3) attribution methods, which aim at... | Yuval Pinter, Michael Elhadad |  |
| 1146 |  |  [A Causal View of Entity Bias in (Large) Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.1013) |  | 0 | Entity bias widely affects pretrained (large) language models, causing them to rely on (biased) parametric knowledge to make unfaithful predictions. Although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying causal models in practice. The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits. To address these problems, we propose a... | Fei Wang, Wenjie Mo, Yiwei Wang, Wenxuan Zhou, Muhao Chen |  |
| 1147 |  |  [T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics](https://doi.org/10.18653/v1/2023.findings-emnlp.1014) |  | 0 | Modern embedding-based metrics for evaluation of generated text generally fall into one of two paradigms: discriminative metrics that are trained to directly predict which outputs are of higher quality according to supervised human annotations, and generative metrics that are trained to evaluate text based on the probabilities of a generative model. Both have their advantages; discriminative metrics are able to directly optimize for the problem of distinguishing between good and bad outputs,... | Yiwei Qin, Weizhe Yuan, Graham Neubig, Pengfei Liu |  |
| 1148 |  |  [T-Projection: High Quality Annotation Projection for Sequence Labeling Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.1015) |  | 0 | In the absence of readily available labeled data for a given sequence labeling task and language, annotation projection has been proposed as one of the possible strategies to automatically generate annotated data. Annotation projection has often been formulated as the task of transporting, on parallel corpora, the labels pertaining to a given span in the source language into its corresponding span in the target language. In this paper we present T-Projection, a novel approach for annotation... | Iker GarcíaFerrero, Rodrigo Agerri, German Rigau |  |
| 1149 |  |  [MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document](https://doi.org/10.18653/v1/2023.findings-emnlp.1016) |  | 0 | The facts and time in the document are intricately intertwined, making temporal reasoning over documents challenging. Previous work models time implicitly, making it difficult to handle such complex relationships. To address this issue, we propose MTGER, a novel Multi-view Temporal Graph Enhanced Reasoning framework for temporal reasoning over time-involved documents. Concretely, MTGER explicitly models the temporal relationships among facts by multi-view temporal graphs. On the one hand, the... | Zheng Chu, Zekun Wang, Jiafeng Liang, Ming Liu, Bing Qin |  |
| 1150 |  |  [MSCFFN: A New FFN with Multi-Space Cross to Accelerate Transformer](https://doi.org/10.18653/v1/2023.findings-emnlp.1017) |  | 0 | Transformer models have achieved impressive success in various natural language processing tasks. But it is also limited used in some areas and the heavy computation complexity is one of the main limitations. Many model structures have been proposed to reduce the computation complexity and some are really effective. The previous research can be divided into two categories. One is to use more effective training and inference strategies and the other is focused on how to replace the standard... | Tang Dongge, Qing Yang |  |
| 1151 |  |  [Dialect Transfer for Swiss German Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.1018) |  | 0 | This paper investigates the challenges in building Swiss German speech translation systems, specifically focusing on the impact of dialect diversity and differences between Swiss German and Standard German. Swiss German is a spoken language with no formal writing system, it comprises many diverse dialects and is a low-resource language with only around 5 million speakers. The study is guided by two key research questions: how does the inclusion and exclusion of dialects during the training of... | Claudio Paonessa, Yanick Schraner, Jan Deriu, Manuela Hürlimann, Manfred Vogel, Mark Cieliebak |  |
| 1152 |  |  [Masked Path Modeling for Vision-and-Language Navigation](https://doi.org/10.18653/v1/2023.findings-emnlp.1019) |  | 0 | Vision-and-language navigation (VLN) agents are trained to navigate in real-world environments based on natural language instructions. A major challenge in VLN is the limited available training data, which hinders the models’ ability to generalize effectively. Previous approaches have attempted to alleviate this issue by using external tools to generate pseudo-labeled data or integrating web-scaled image-text pairs during training. However, these methods often rely on automatically-generated or... | ZiYi Dou, Feng Gao, Nanyun Peng |  |
| 1153 |  |  [Learning Interpretable Style Embeddings via Prompting LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.1020) |  | 0 | Style representation learning builds content-independent representations of author style in text. To date, no large dataset of texts with stylometric annotations on a wide range of style dimensions has been compiled, perhaps because the linguistic expertise to perform such annotation would be prohibitively expensive. Therefore, current style representation approaches make use of unsupervised neural methods to disentangle style from content to create style vectors. These approaches, however,... | Ajay Patel, Delip Rao, Ansh Kothary, Kathleen R. McKeown, Chris CallisonBurch |  |
| 1154 |  |  [Exploring Context-Aware Evaluation Metrics for Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.1021) |  | 0 | Previous studies on machine translation evaluation mostly focused on the quality of individual sentences, while overlooking the important role of contextual information. Although WMT Metrics Shared Tasks have introduced context content into the human annotations of translation evaluation since 2019, the relevant metrics and methods still did not take advantage of the corresponding context. In this paper, we propose a context-aware machine translation evaluation metric called Cont-COMET, built... | Xinyu Hu, Xunjian Yin, Xiaojun Wan |  |
| 1155 |  |  [GRACE: Discriminator-Guided Chain-of-Thought Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.1022) |  | 0 | In the context of multi-step reasoning, e.g., with chain-of-thought, language models (LMs) can easily assign a high likelihood to incorrect steps. As a result, decoding strategies that optimize for solution likelihood often yield incorrect solutions. To address this issue, we propose Guiding chain-of-thought ReAsoning with a CorrectnEss Discriminator (GRACE), a stepwise decoding approach that steers the decoding process towards producing correct reasoning steps. GRACE employs a discriminator... | Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Lu Wang |  |
| 1156 |  |  [QADYNAMICS: Training Dynamics-Driven Synthetic QA Diagnostic for Zero-Shot Commonsense Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.1023) |  | 0 | Zero-shot commonsense Question-Answering (QA) requires models to reason about general situations beyond specific benchmarks. State-of-the-art approaches fine-tune language models on QA pairs constructed from CommonSense Knowledge Bases (CSKBs) to equip the models with more commonsense knowledge in a QA context. However, current QA synthesis protocols may introduce noise from the CSKBs and generate ungrammatical questions and false negative options, which impede the model’s ability to... | Haochen Shi, Weiqi Wang, Tianqing Fang, Baixuan Xu, Wenxuan Ding, Xin Liu, Yangqiu Song |  |
| 1157 |  |  [RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.1024) |  | 0 | Universal Information Extraction (UIE) is an area of interest due to the challenges posed by varying targets, heterogeneous structures, and demand-specific schemas. Previous works have achieved success by unifying a few tasks, such as Named Entity Recognition (NER) and Relation Extraction (RE), while they fall short of being true UIE models particularly when extracting other general schemas such as quadruples and quintuples. Additionally, these models used an implicit structural schema... | Chengyuan Liu, Fubang Zhao, Yangyang Kang, Jingyuan Zhang, Xiang Zhou, Changlong Sun, Kun Kuang, Fei Wu |  |
| 1158 |  |  [PromptARA: Improving Deep Representation in Hybrid Automatic Readability Assessment with Prompt and Orthogonal Projection](https://doi.org/10.18653/v1/2023.findings-emnlp.1025) |  | 0 | Readability assessment aims to automatically classify texts based on readers’ reading levels. The hybrid automatic readability assessment (ARA) models using both deep and linguistic features have attracted rising attention in recent years due to their impressive performance. However, deep features are not fully explored due to the scarcity of training data, and the fusion of deep and linguistic features is not very effective in existing hybrid ARA models. In this paper, we propose a novel... | Jinshan Zeng, Xianglong Yu, Xianchao Tong, Wenyan Xiao |  |
| 1159 |  |  [Does Listener Gaze in Face-to-Face Interaction Follow the Entropy Rate Constancy Principle: An Empirical Study](https://doi.org/10.18653/v1/2023.findings-emnlp.1026) |  | 0 | It is generally assumed that language (written and spoken) follows the entropy rate constancy (ERC) principle, which states that the information density of a text is constant over time. Recently, this has also been found for nonverbal gestures used in monologue, but it is still unclear whether the ERC principle also applies to listeners’ nonverbal signals. We focus on listeners’ gaze behaviour extracted from video-recorded conversations and trained a transformer-based neural sequence model to... | Yu Wang, Hendrik Buschmeier |  |
| 1160 |  |  [Incorporating Object-Level Visual Context for Multimodal Fine-Grained Entity Typing](https://doi.org/10.18653/v1/2023.findings-emnlp.1027) |  | 0 | Fine-grained entity typing (FGET) aims to assign appropriate fine-grained types to entity mentions within their context, which is an important foundational task in natural language processing. Previous approaches for FGET only utilized textual context information. However, in the form of short text, the contextual semantic information is often insufficient for FGET. In many real-world scenarios, text is often accompanied by images, and the visual context is valuable for FGET. To this end, we... | Ying Zhang, Wenbo Fan, Kehui Song, Yu Zhao, Xuhui Sui, Xiaojie Yuan |  |
| 1161 |  |  [Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data](https://doi.org/10.18653/v1/2023.findings-emnlp.1028) |  | 0 | Numerical data plays a crucial role in various real-world domains like finance, economics, and science. Thus, understanding and reasoning with numbers are essential in these fields. Recent benchmarks have assessed the numerical reasoning abilities of language models, revealing their limitations in limited and specific numerical aspects. In this paper, we propose a complete hierarchical taxonomy for numerical reasoning skills, encompassing over ten reasoning types across four levels:... | Mubashara Akhtar, Abhilash Reddy Shankarampeta, Vivek Gupta, Arpit Patil, Oana Cocarascu, Elena Simperl |  |
| 1162 |  |  [Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.1029) |  | 0 | Large language models have revolutionized the field of NLP by achieving state-of-the-art performance on various tasks. However, there is a concern that these models may disclose information in the training data. In this study, we focus on the summarization task and investigate the membership inference (MI) attack: given a sample and black-box access to a model’s API, it is possible to determine if the sample was part of the training data. We exploit text similarity and the model’s resistance to... | Ruixiang Tang, Gord Lueck, Rodolfo Quispe, Huseyin A. Inan, Janardhan Kulkarni, Xia Hu |  |
| 1163 |  |  [BERT Has More to Offer: BERT Layers Combination Yields Better Sentence Embeddings](https://doi.org/10.18653/v1/2023.findings-emnlp.1030) |  | 0 | Obtaining sentence representations from BERT-based models as feature extractors is invaluable as it takes much less time to pre-compute a one-time representation of the data and then use it for the downstream tasks, rather than fine-tune the whole BERT. Most previous works acquire a sentence’s representation by passing it to BERT and averaging its last layer. In this paper, we propose that the combination of certain layers of a BERT-based model rested on the data set and model can achieve... | Seyyed MohammadSaleh Hosseini, Munawara Munia, Latifur Khan |  |
| 1164 |  |  [Extrapolating Multilingual Understanding Models as Multilingual Generators](https://doi.org/10.18653/v1/2023.findings-emnlp.1031) |  | 0 | Multilingual understanding models (or encoder-based), pre-trained via masked language modeling, have achieved promising results on many language understanding tasks (e.g., mBERT). However, these models are not capable of generating high-quality text compared with decoder-based causal language models. Can we transform a pre-trained language understanding model into an effective language generation model? We propose a Semantic-Guided Alignment-then-Denoising (SGA) approach to adapt a multilingual... | Bohong Wu, Fei Yuan, Hai Zhao, Lei Li, Jingjing Xu |  |
| 1165 |  |  [SAC³: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency](https://doi.org/10.18653/v1/2023.findings-emnlp.1032) |  | 0 | Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check... | Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A. Malin, Kumar Sricharan |  |
| 1166 |  |  [Test-Time Self-Adaptive Small Language Models for Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.1033) |  | 0 | Recent instruction-finetuned large language models (LMs) have achieved notable performances in various tasks, such as question-answering (QA). However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks. Moreover, further finetuning LMs with labeled datasets is often infeasible due to their absence, but it is also questionable if we can... | Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong Park |  |
| 1167 |  |  [ExpNote: Black-box Large Language Models are better Task Solvers with Experience Notebook](https://doi.org/10.18653/v1/2023.findings-emnlp.1034) |  | 0 | Black-box Large Language Models (LLMs) have shown great power in solving various tasks and are considered general problem solvers. However, LLMs still fail in many specific tasks although understand the task instruction. In this paper, we focus on the problem of boosting the ability of black-box LLMs to solve downstream tasks. We propose ExpNote, an automated framework to help LLMs better adapt to unfamiliar tasks through reflecting and noting experiences from training data and retrieving them... | Wangtao Sun, Xuanqing Yu, Shizhu He, Jun Zhao, Kang Liu |  |
| 1168 |  |  [Evaluating Parameter-Efficient Finetuning Approaches for Pre-trained Models on the Financial Domain](https://doi.org/10.18653/v1/2023.findings-emnlp.1035) |  | 0 | Large-scale language models with millions, billions, or trillions of trainable parameters are becoming increasingly popular. However, they risk becoming rapidly over-parameterized and the adaptation cost of fully fine-tuning them increases significantly. Storing them becomes progressively impractical as it requires keeping a separate copy of all the fine-tuned weights for each task. By freezing all pre-trained weights during fine-tuning, parameter-efficient tuning approaches have become an... | Isabella Olariu, Cedric Lothritz, Jacques Klein, Tegawendé F. Bissyandé, Siwen Guo, Shohreh Haddadan |  |
| 1169 |  |  [Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model](https://doi.org/10.18653/v1/2023.findings-emnlp.1036) |  | 0 | Augmenting pretrained language models with retrievers has shown promise in effectively solving common NLP problems, such as language modeling and question answering. In this paper, we evaluate the strengths and weaknesses of popular retriever-augmented language models, namely kNN-LM, REALM, DPR + FiD, Contriever + ATLAS, and Contriever + Flan-T5, in reasoning over retrieved statements across different tasks. Our findings indicate that the simple similarity metric employed by retrievers is... | Parishad BehnamGhader, Santiago Miret, Siva Reddy |  |
| 1170 |  |  [BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text](https://doi.org/10.18653/v1/2023.findings-emnlp.1037) |  | 0 | Real-world NLP applications often deal with nonstandard text (e.g., dialectal, informal, or misspelled text). However, language models like BERT deteriorate in the face of dialect variation or noise. How do we push BERT’s modeling capabilities to encompass nonstandard text? Fine-tuning helps, but it is designed for specializing a model to a task and does not seem to bring about the deeper, more pervasive changes needed to adapt a model to nonstandard language. In this paper, we introduce the... | Aarohi Srivastava, David Chiang |  |
| 1171 |  |  [Closed Boundary Learning for Classification Tasks with the Universum Class](https://doi.org/10.18653/v1/2023.findings-emnlp.1038) |  | 0 | The Universum class, often known as the \*other\* class or the\*miscellaneous\* class, is defined as a collection of samples that do not belong to any class of interest. It is a typical class that exists in many classification-based tasks in NLP, such as relation extraction, named entity recognition, sentiment analysis, etc. The Universum class exhibits very different properties, namely heterogeneity and lack of representativeness in training data; however, existing methods often treat the... | Hanzhang Zhou, Zijian Feng, Kezhi Mao |  |
| 1172 |  |  [Revisiting Entropy Rate Constancy in Text](https://doi.org/10.18653/v1/2023.findings-emnlp.1039) |  | 0 | The uniform information density (UID) hypothesis states that humans tend to distribute information roughly evenly across an utterance or discourse. Early evidence in support of the UID hypothesis came from Genzel and Charniak (2002), which proposed an entropy rate constancy principle based on the probability of English text under n-gram language models. We re-evaluate the claims of Genzel and Charniak (2002) with neural language models, failing to find clear evidence in support of entropy rate... | Vivek Verma, Nicholas Tomlin, Dan Klein |  |
| 1173 |  |  [Calibrated Seq2seq Models for Efficient and Generalizable Ultra-fine Entity Typing](https://doi.org/10.18653/v1/2023.findings-emnlp.1040) |  | 0 | Ultra-fine entity typing plays a crucial role in information extraction by predicting fine-grained semantic types for entity mentions in text. However, this task poses significant challenges due to the massive number of entity types in the output space. The current state-of-the-art approaches, based on standard multi-label classifiers or cross-encoder models, suffer from poor generalization performance or inefficient inference speed. In this paper, we present CASENT, a seq2seq model designed... | Yanlin Feng, Adithya Pratapa, David R. Mortensen |  |
| 1174 |  |  [Learning Semantic Role Labeling from Compatible Label Sequences](https://doi.org/10.18653/v1/2023.findings-emnlp.1041) |  | 0 | Semantic role labeling (SRL) has multiple disjoint label sets, e.g., VerbNet and PropBank. Creating these datasets is challenging, therefore a natural question is how to use each one to help the other. Prior work has shown that cross-task interaction helps, but only explored multitask learning so far. A common issue with multi-task setup is that argument sequences are still separately decoded, running the risk of generating structurally inconsistent label sequences (as per lexicons like... | Tao Li, Ghazaleh Kazeminejad, Susan Windisch Brown, Vivek Srikumar, Martha Palmer |  |
| 1175 |  |  [QUADRo: Dataset and Models for QUestion-Answer Database Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.1042) |  | 0 | An effective approach to design automated Question Answering (QA) systems is to efficiently retrieve answers from pre-computed databases containing question/answer pairs. One of the main challenges to this design is the lack of training/testing data. Existing resources are limited in size and topics and either do not consider answers (question-question similarity only) or their quality in the annotation process. To fill this gap, we introduce a novel open-domain annotated resource to train and... | Stefano Campese, Ivano Lauriola, Alessandro Moschitti |  |
| 1176 |  |  [Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.1043) |  | 0 | Pre-trained Language Models (PLMs) are trained on vast unlabeled data, rich in world knowledge. This fact has sparked the interest of the community in quantifying the amount of factual knowledge present in PLMs, as this explains their performance on downstream tasks, and potentially justifies their use as knowledge bases. In this work, we survey methods and datasets that are used to probe PLMs for factual knowledge. Our contributions are: (1) We propose a categorization scheme for factual... | Paul Youssef, Osman Alperen Koras, Meijie Li, Jörg Schlötterer, Christin Seifert |  |
| 1177 |  |  [Is ChatGPT the ultimate Data Augmentation Algorithm?](https://doi.org/10.18653/v1/2023.findings-emnlp.1044) |  | 0 | In the aftermath of GPT-3.5, commonly known as ChatGPT, research have attempted to assess its capacity for lowering annotation cost, either by doing zero-shot learning, generating new data, or replacing human annotators. Some studies have also investigated its use for data augmentation (DA), but only in limited contexts, which still leaves the question of how ChatGPT performs compared to state-of-the-art algorithms. In this paper, we use ChatGPT to create new data both with paraphrasing and... | Frédéric Piedboeuf, Philippe Langlais |  |
| 1178 |  |  [Enhanced Simultaneous Machine Translation with Word-level Policies](https://doi.org/10.18653/v1/2023.findings-emnlp.1045) |  | 0 | Recent years have seen remarkable advances in the field of Simultaneous Machine Translation (SiMT) due to the introduction of innovative policies that dictate whether to READ or WRITE at each step of the translation process. However, a common assumption in many existing studies is that operations are carried out at the subword level, even though the standard unit for input and output in most practical scenarios is typically at the word level. This paper demonstrates that policies devised and... | Kang Kim, Hankyu Cho |  |
| 1179 |  |  [Causal Intervention-based Few-Shot Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.1046) |  | 0 | Few-shot named entity recognition (NER) systems aim to recognize new classes of entities with limited labeled samples. However, these systems face a significant challenge of overfitting compared to tasks with abundant samples. This overfitting is mainly caused by the spurious correlation resulting from the bias in selecting a few samples. To address this issue, we propose a causal intervention-based few-shot NER method in this paper. Our method, based on the prototypical network, intervenes in... | Zhen Yang, Yongbin Liu, Chunping Ouyang |  |
| 1180 |  |  [TADI: Topic-aware Attention and Powerful Dual-encoder Interaction for Recall in News Recommendation](https://doi.org/10.18653/v1/2023.findings-emnlp.1047) |  | 0 | News recommendation is one of the widest commercialization in natural language processing research area, which aims to recommend news according to user interests. New recall plays an important role in news recommendation. It is to recall candidates from a very large news database. Recent researches of news recall mostly adopt dual-encoder architecture as it provides a much faster recall scheme, and they encode each word equally. However, these works remain two challenges: irrelevant word... | Junxiang Jiang |  |
| 1181 |  |  [Unveiling the Power of Argument Arrangement in Online Persuasive Discussions](https://doi.org/10.18653/v1/2023.findings-emnlp.1048) |  | 0 | Previous research on argumentation in online discussions has largely focused on examining individual comments and neglected the interactive nature of discussions. In line with previous work, we represent individual comments as sequences of semantic argumentative unit types. However, because it is intuitively necessary for dialogical argumentation to address the opposing viewpoints, we extend this model by clustering type sequences into different argument arrangement patterns and representing... | Nailia Mirzakhmedova, Johannes Kiesel, Khalid Al Khatib, Benno Stein |  |
| 1182 |  |  [FFAEval: Evaluating Dialogue System via Free-For-All Ranking](https://doi.org/10.18653/v1/2023.findings-emnlp.1049) |  | 0 | Evaluating open-domain dialogue systems is currently an open question. Automatic evaluation metrics have shown poor correlation with human assessment in dialogue generation tasks. Human evaluation, which involves annotators for multi-dimension scoring, is trustworthy but time-consuming. In this work, we propose FFAEval, a reliable and efficient human evaluation framework using Free-For-All ranking approach. By sharing the dialogue history, the framework enables annotators to converse with... | Zeyao Ma, Zijun Yao, Jing Zhang, Jifan Yu, Xiaohan Zhang, Juanzi Li, Jie Tang |  |
| 1183 |  |  [Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.1050) |  | 0 | The conversational machine reading comprehension (CMRC) task aims to answer questions in conversations, which has been a hot research topic in recent years because of its wide applications. However, existing CMRC benchmarks in which each conversation is assigned a static passage are inconsistent with real scenarios. Thus, model’s comprehension ability towards real scenarios are hard to evaluate reasonably. To this end, we propose the first Chinese CMRC benchmark Orca and further provide... | Nuo Chen, Hongguang Li, Junqing He, Yinan Bao, Xinshi Lin, Qi Yang, Jianfeng Liu, Ruyi Gan, Jiaxing Zhang, Baoyuan Wang, Jia Li |  |
| 1184 |  |  [VER: Unifying Verbalizing Entities and Relations](https://doi.org/10.18653/v1/2023.findings-emnlp.1051) |  | 0 | Entities and relationships between entities are vital in the real world. Essentially, we understand the world by understanding entities and relations. For instance, to understand a field, e.g., computer science, we need to understand the relevant concepts, e.g., machine learning, and the relationships between concepts, e.g., machine learning and artificial intelligence. To understand a person, we should first know who he/she is and how he/she is related to others. To understand entities and... | Jie Huang, Kevin Chang |  |
| 1185 |  |  [The Linearity of the Effect of Surprisal on Reading Times across Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.1052) |  | 0 | In psycholinguistics, surprisal theory posits that the amount of online processing effort expended by a human comprehender per word positively correlates with the surprisal of that word given its preceding context. In addition to this overall correlation, more importantly, the specific quantitative form taken by the processing effort as a function of surprisal offers insights into the underlying cognitive mechanisms of language processing. Focusing on English, previous studies have looked into... | Weijie Xu, Jason Chon, Tianran Liu, Richard Futrell |  |
| 1186 |  |  [Adversarial Text Generation by Search and Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.1053) |  | 0 | Recent research has shown that evaluating the robustness of natural language processing models using textual attack methods is significant. However, most existing text attack methods only use heuristic replacement strategies or language models to generate replacement words at the word level. The blind pursuit of high attack success rates makes it difficult to ensure the quality of the generated adversarial text. As a result, adversarial text is often difficult for humans to understand. In fact,... | Guoyi Li, Bingkang Shi, Zongzhen Liu, Dehan Kong, Yulei Wu, Xiaodan Zhang, Longtao Huang, Honglei Lyu |  |
| 1187 |  |  [Measuring Pointwise \mathcalV-Usable Information In-Context-ly](https://doi.org/10.18653/v1/2023.findings-emnlp.1054) |  | 0 | In-context learning (ICL) is a new learning paradigm that has gained popularity along with the development of large language models. In this work, we adapt a recently proposed hardness metric, pointwise 𝒱-usable information (PVI), to an in-context version (in-context PVI). Compared to the original PVI, in-context PVI is more efficient in that it requires only a few exemplars and does not require fine-tuning. We conducted a comprehensive empirical analysis to evaluate the reliability of... | Sheng Lu, Shan Chen, Yingya Li, Danielle S. Bitterman, Guergana Savova, Iryna Gurevych |  |
| 1188 |  |  [SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities](https://doi.org/10.18653/v1/2023.findings-emnlp.1055) |  | 0 | Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech... | Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, Xipeng Qiu |  |
| 1189 |  |  [Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance via Probability Calibration](https://doi.org/10.18653/v1/2023.findings-emnlp.1056) |  | 0 | Pretrained multilingual encoder models can directly perform zero-shot multilingual tasks or linguistic probing by reformulating the input examples into cloze-style prompts. This is accomplished by predicting the probabilities of the label words at the masked token position, without requiring any updates to the model parameters. However, the performance of this method is limited by the model’s bias toward predicting label words which frequently occurred during the pretraining. These words... | Ercong Nie, Helmut Schmid, Hinrich Schütze |  |
| 1190 |  |  [A Thorough Examination on Zero-shot Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.1057) |  | 0 | Recent years have witnessed the significant advance in dense retrieval (DR) based on powerful pre-trained language models (PLM). DR models have achieved excellent performance in several benchmark datasets, while they are shown to be not as competitive as traditional sparse retrieval models (e.g., BM25) in a zero-shot retrieval setting. However, in the related literature, there still lacks a detailed and comprehensive study on zero-shot retrieval. In this paper, we present the first thorough... | Ruiyang Ren, Yingqi Qu, Jing Liu, Xin Zhao, Qifei Wu, Yuchen Ding, Hua Wu, Haifeng Wang, JiRong Wen |  |
| 1191 |  |  [Contrastive Pre-training for Personalized Expert Finding](https://doi.org/10.18653/v1/2023.findings-emnlp.1058) |  | 0 | Expert finding could help route questions to potential suitable users to answer in Community Question Answering (CQA) platforms. Hence it is essential to learn accurate representations of experts and questions according to the question text articles. Recently the pre-training and fine-tuning paradigms are powerful for natural language understanding, which has the potential for better question modeling and expert finding. Inspired by this, we propose a CQA-domain Contrastive Pre-training... | Qiyao Peng, Hongtao Liu, Zhepeng Lv, Qing Yang, Wenjun Wang |  |
| 1192 |  |  [Mitigating Intrinsic Named Entity-Related Hallucinations of Abstractive Text Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.1059) |  | 0 | Abstractive text summarization (ATS) is both important and challenging. Recent studies have shown that ATS still faces various forms of hallucination. Our study also indicates that a significant portion of hallucinations is named entity-related. They might appear in different forms, such as mistaken entities and erroneous entity references. The underlying causes implicit in data are complex: data samples pose varying learning conditions. Despite recent research efforts dedicated to named... | Jianbin Shen, Junyu Xuan, Christy Jie Liang |  |
| 1193 |  |  [Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.1060) |  | 0 | Large Language models (LLMs) possess the capability to engage In-context Learning (ICL) by leveraging a few demonstrations pertaining to a new downstream task as conditions. However, this particular learning paradigm suffers from high instability stemming from substantial variances induced by factors such as the input distribution of selected examples, their ordering, and prompt formats. In this work, we demonstrate that even when all these factors are held constant, the random selection of... | Hongfu Liu, Ye Wang |  |
| 1194 |  |  [Frontmatter](https://aclanthology.org/2023.emnlp-main.0) |  | 0 |  |  |  |
| 1195 |  |  [IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions](https://doi.org/10.18653/v1/2023.emnlp-main.1) |  | 0 | Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for open-domain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions. In this paper, we propose an Induction-Augmented Generation (IAG) framework that utilizes inductive knowledge along with the... | Zhebin Zhang, Xinyu Zhang, Yuanhang Ren, Saijiang Shi, Meng Han, Yongkang Wu, Ruofei Lai, Zhao Cao |  |
| 1196 |  |  [Absolute Position Embedding Learns Sinusoid-like Waves for Attention Based on Relative Position](https://doi.org/10.18653/v1/2023.emnlp-main.2) |  | 0 | Attention weight is a clue to interpret how a Transformer-based model makes an inference. In some attention heads, the attention focuses on the neighbors of each token. This allows the output vector of each token to depend on the surrounding tokens and contributes to make the inference context-dependent. We analyze the mechanism behind the concentration of attention on nearby tokens. We show that the phenomenon emerges as follows: (1) learned position embedding has sinusoid-like components, (2)... | Yuji Yamamoto, Takuya Matsuzaki |  |
| 1197 |  |  [Chinese Lexical Substitution: Dataset and Method](https://doi.org/10.18653/v1/2023.emnlp-main.3) |  | 0 | Existing lexical substitution (LS) benchmarks were collected by asking human annotators to think of substitutes from memory, resulting in benchmarks with limited coverage and relatively small scales. To overcome this problem, we propose a novel annotation method to construct an LS dataset based on human and machine collaboration. Based on our annotation method, we construct the first Chinese LS dataset CHNLS which consists of 33,695 instances and 144,708 substitutes, covering three text genres... | Jipeng Qiang, Kang Liu, Ying Li, Yun Li, Yi Zhu, YunHao Yuan, Xiaocheng Hu, Xiaoye Ouyang |  |
| 1198 |  |  [Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting](https://doi.org/10.18653/v1/2023.emnlp-main.4) |  | 0 | Automatic response forecasting for news media plays a crucial role in enabling content producers to efficiently predict the impact of news releases and prevent unexpected negative outcomes such as social conflict and moral injury. To effectively forecast responses, it is essential to develop measures that leverage the social dynamics and contextual information surrounding individuals, especially in cases where explicit profiles or historical actions of the users are limited (referred to as... | Chenkai Sun, Jinning Li, Yi Ren Fung, Hou Pong Chan, Tarek F. Abdelzaher, ChengXiang Zhai, Heng Ji |  |
| 1199 |  |  [Fine-grained Conversational Decoding via Isotropic and Proximal Search](https://doi.org/10.18653/v1/2023.emnlp-main.5) |  | 0 | General-purpose text decoding approaches are usually adopted for dialogue response generation. Although the quality of the generated responses can be improved with dialogue-specific encoding methods, conversational decoding methods are still under-explored. Inspired by SimDRC that a good dialogue feature space should follow the rules of locality and isotropy, we present a fine-grained conversational decoding method, termed isotropic and proximal search (IPS). Our method is designed to generate... | Yuxuan Yao, Han Wu, Qiling Xu, Linqi Song |  |
| 1200 |  |  [Holistic Inter-Annotator Agreement and Corpus Coherence Estimation in a Large-scale Multilingual Annotation Campaign](https://doi.org/10.18653/v1/2023.emnlp-main.6) |  | 0 | In this paper we report on the complexity of persuasion technique annotation in the context of a large multilingual annotation campaign involving 6 languages and approximately 40 annotators. We highlight the techniques that appear to be difficult for humans to annotate and elaborate on our findings on the causes of this phenomenon. We introduce Holistic IAA, a new word embedding-based annotator agreement metric and we report on various experiments using this metric and its correlation with the... | Nicolas Stefanovitch, Jakub Piskorski |  |
| 1201 |  |  [PHD: Pixel-Based Language Modeling of Historical Documents](https://doi.org/10.18653/v1/2023.emnlp-main.7) |  | 0 | The digitisation of historical documents has provided historians with unprecedented research opportunities. Yet, the conventional approach to analysing historical documents involves converting them from images to text using OCR, a process that overlooks the potential benefits of treating them as images and introduces high levels of noise. To bridge this gap, we take advantage of recent advancements in pixel-based language models trained to reconstruct masked patches of pixels instead of... | Nadav Borenstein, Phillip Rust, Desmond Elliott, Isabelle Augenstein |  |
| 1202 |  |  [Primacy Effect of ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.8) |  | 0 | Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT... | Yiwei Wang, Yujun Cai, Muhao Chen, Yuxuan Liang, Bryan Hooi |  |
| 1203 |  |  [Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension](https://doi.org/10.18653/v1/2023.emnlp-main.9) |  | 0 | To precisely evaluate a language model’s capability for logical reading comprehension, we present a dataset for testing the understanding of the rationale behind critical reasoning. For questions taken from an existing multiple-choice logical reading comprehension dataset, we crowdsource rationale texts that explain why we should select or eliminate answer options, resulting in 3,003 multiple-choice subquestions that are associated with 943 main questions. Experiments on our dataset show that... | Akira Kawabata, Saku Sugawara |  |
| 1204 |  |  [Evaluating and Modeling Attribution for Cross-Lingual Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.10) |  | 0 | Trustworthy answer content is abundant in many high-resource languages and is instantly accessible through question answering systems — yet this content can be hard to access for those that do not speak these languages. The leap forward in cross-lingual modeling quality offered by generative language models offers much promise, yet their raw generations often fall short in factuality. To improve trustworthiness in these systems, a promising direction is to attribute the answer to a retrieved... | Benjamin Muller, John Wieting, Jonathan H. Clark, Tom Kwiatkowski, Sebastian Ruder, Livio Soares, Roee Aharoni, Jonathan Herzig, Xinyi Wang |  |
| 1205 |  |  [Better Quality Pre-training Data and T5 Models for African Languages](https://doi.org/10.18653/v1/2023.emnlp-main.11) |  | 0 | In this study, we highlight the importance of enhancing the quality of pretraining data in multilingual language models. Existing web crawls have demonstrated quality issues, particularly in the context of low-resource languages. Consequently, we introduce a new multilingual pretraining corpus for 16 African languages, designed by carefully auditing existing pretraining corpora to understand and rectify prevalent quality issues. To compile this dataset, we undertake a rigorous examination of... | Akintunde Oladipo, Mofetoluwa Adeyemi, Orevaoghene Ahia, Abraham Toluwase Owodunni, Odunayo Ogundepo, David Ifeoluwa Adelani, Jimmy Lin |  |
| 1206 |  |  [Sparse Universal Transformer](https://doi.org/10.18653/v1/2023.emnlp-main.12) |  | 0 | The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers and is Turing-complete under certain assumptions. Empirical evidence also shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, most state-of-the-art NLP systems use VTs as their backbone model instead of UTs. This is mainly... | Shawn Tan, Yikang Shen, Zhenfang Chen, Aaron C. Courville, Chuang Gan |  |
| 1207 |  |  [Theory of Mind for Multi-Agent Collaboration via Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.13) |  | 0 | While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind... | Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Charles Lewis, Katia P. Sycara |  |
| 1208 |  |  [Establishing Trustworthiness: Rethinking Tasks and Model Evaluation](https://doi.org/10.18653/v1/2023.emnlp-main.14) |  | 0 | Language understanding is a multi-faceted cognitive capability, which the Natural Language Processing (NLP) community has striven to model computationally for decades. Traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols. With the advent of large language models (LLMs) the community has witnessed a dramatic shift towards general purpose, task-agnostic approaches powered by generative... | Robert Litschko, Max MüllerEberstein, Rob van der Goot, Leon WeberGenzel, Barbara Plank |  |
| 1209 |  |  [Let's Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought](https://doi.org/10.18653/v1/2023.emnlp-main.15) |  | 0 | Despite exciting recent results showing vision-language systems’ capacity to reason about images using natural language, their capacity for video reasoning remains underexplored. We motivate framing video reasoning as the sequential understanding of a small number of keyframes, thereby leveraging the power and robustness of vision-language while alleviating the computational complexities of processing videos. To evaluate this novel application, we introduce VIP, an inference-time challenge... | Vaishnavi Himakunthala, Andy Ouyang, Daniel Rose, Ryan He, Alex Mei, Yujie Lu, Chinmay Sonar, Michael Saxon, William Yang Wang |  |
| 1210 |  |  [GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP](https://doi.org/10.18653/v1/2023.emnlp-main.16) |  | 0 | ChatGPT’s emergence heralds a transformative phase in NLP, particularly demonstrated through its excellent performance on many English benchmarks. However, the model’s efficacy across diverse linguistic contexts remains largely uncharted territory. This work aims to bridge this knowledge gap, with a primary focus on assessing ChatGPT’s capabilities on Arabic languages and dialectal varieties. Our comprehensive study conducts a large-scale automated and human evaluation of ChatGPT, encompassing... | Md Tawkat Islam Khondaker, Abdul Waheed, El Moatez Billah Nagoudi, Muhammad AbdulMageed |  |
| 1211 |  |  [Dual-Channel Span for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.17) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) is one of the compound tasks of fine-grained aspect-based sentiment analysis (ABSA), aiming at extracting the triplets of aspect terms, corresponding opinion terms and the associated sentiment orientation. Recent efforts in exploiting span-level semantic interaction shown superior performance on ASTE task. However, most of the existing span-based approaches suffer from enumerating all possible spans, since it can introduce too much noise in sentiment... | Pan Li, Ping Li, Kai Zhang |  |
| 1212 |  |  [Cultural Concept Adaptation on Multimodal Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.18) |  | 0 | Developing cultural adaptation methods is important, which can improve the model performance on the low-resource ones and provide more equitable opportunities for everyone to benefit from advanced technology. Past methods primarily focused on multilingual and multimodal capabilities, and the improvement of multicultural competence is still an unexplored problem. This is largely due to the difficulty of data scarcity and expensive annotation. In this paper, we navigate this uncharted territory... | Zhi Li, Yin Zhang |  |
| 1213 |  |  [Understanding Compositional Data Augmentation in Typologically Diverse Morphological Inflection](https://doi.org/10.18653/v1/2023.emnlp-main.19) |  | 0 | Data augmentation techniques are widely used in low-resource automatic morphological inflection to address the issue of data sparsity. However, the full implications of these techniques remain poorly understood. In this study, we aim to shed light on the theoretical aspects of the data augmentation strategy StemCorrupt, a method that generates synthetic examples by randomly substituting stem characters in existing gold standard training examples. Our analysis uncovers that StemCorrupt brings... | Farhan Samir, Miikka Silfverberg |  |
| 1214 |  |  [Evaluating Object Hallucination in Large Vision-Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.20) |  | 0 | Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently proposed by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that they suffer from object hallucinations, i.e., they tend to generate objects inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object... | Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, JiRong Wen |  |
| 1215 |  |  [Event Ontology Completion with Hierarchical Structure Evolution Networks](https://doi.org/10.18653/v1/2023.emnlp-main.21) |  | 0 | Traditional event detection methods require predefined event schemas. However, manually defining event schemas is expensive and the coverage of schemas is limited. To this end, some works study the event type induction (ETI) task, which discovers new event types via clustering. However, the setting of ETI suffers from two limitations: event types are not linked into the existing hierarchy and have no semantic names. In this paper, we propose a new research task named Event Ontology Completion... | Pengfei Cao, Yupu Hao, Yubo Chen, Kang Liu, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Jun Zhao |  |
| 1216 |  |  [Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients](https://doi.org/10.18653/v1/2023.emnlp-main.22) |  | 0 | Fine-tuning all parameters of large language models (LLMs) requires significant computational resources and is time-consuming. Recent parameter-efficient tuning methods such as Adapter tuning, Prefix tuning, and LoRA allow for updating a small subset of parameters in large language models. However, they can only save approximately 30% of the training memory requirements, due to the problem that gradient computation and backpropagation are still necessary for these methods. This paper proposes a... | Feihu Jin, Jiajun Zhang, Chengqing Zong |  |
| 1217 |  |  [Discourse Structures Guided Fine-grained Propaganda Identification](https://doi.org/10.18653/v1/2023.emnlp-main.23) |  | 0 | Propaganda is a form of deceptive narratives that instigate or mislead the public, usually with a political purpose. In this paper, we aim to identify propaganda in political news at two fine-grained levels: sentence-level and token-level. We observe that propaganda content is more likely to be embedded in sentences that attribute causality or assert contrast to nearby sentences, as well as seen in opinionated evaluation, speculation and discussions of future expectation. Hence, we propose to... | Yuanyuan Lei, Ruihong Huang |  |
| 1218 |  |  [CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.24) |  | 0 | While many languages possess processes of joining two or more words to create compound words, previous studies have been typically limited only to languages with excessively productive compound formation (e.g., German, Dutch) and there is no public dataset containing compound and non-compound words across a large number of languages. In this work, we systematically study decompounding, the task of splitting compound words into their constituents, at a wide scale. We first address the data gap... | Benjamin Minixhofer, Jonas Pfeiffer, Ivan Vulic |  |
| 1219 |  |  [Improving Image Captioning via Predicting Structured Concepts](https://doi.org/10.18653/v1/2023.emnlp-main.25) |  | 0 | Having the difficulty of solving the semantic gap between images and texts for the image captioning task, conventional studies in this area paid some attention to treating semantic concepts as a bridge between the two modalities and improved captioning performance accordingly. Although promising results on concept prediction were obtained, the aforementioned studies normally ignore the relationship among concepts, which relies on not only objects in the image, but also word dependencies in the... | Ting Wang, Weidong Chen, Yuanhe Tian, Yan Song, Zhendong Mao |  |
| 1220 |  |  [GATITOS: Using a New Multilingual Lexicon for Low-resource Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.26) |  | 0 | Modern machine translation models and language models are able to translate without having been trained on parallel data, greatly expanding the set of languages that they can serve. However, these models still struggle in a variety of predictable ways, a problem that cannot be overcome without at least some trusted bilingual data. This work expands on a cheap and abundant resource to combat this problem: bilingual lexica. We test the efficacy of bilingual lexica in a real-world set-up, on... | Alexander Jones, Isaac Caswell, Orhan Firat, Ishank Saxena |  |
| 1221 |  |  [Continually Improving Extractive QA via Human Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.27) |  | 0 | We study continually improving an extractive question answering (QA) system via human user feedback. We design and deploy an iterative approach, where information-seeking users ask questions, receive model-predicted answers, and provide feedback. We conduct experiments involving thousands of user interactions under diverse setups to broaden the understanding of learning from feedback over time. Our experiments show effective improvement from user feedback of extractive QA models over time... | Ge Gao, HungTing Chen, Yoav Artzi, Eunsol Choi |  |
| 1222 |  |  [Using Interpretation Methods for Model Enhancement](https://doi.org/10.18653/v1/2023.emnlp-main.28) |  | 0 | In the age of neural natural language processing, there are plenty of works trying to derive interpretations of neural models. Intuitively, when gold rationales exist during training, one can additionally train the model to match its interpretation with the rationales. However, this intuitive idea has not been fully explored. In this paper, we propose a framework of utilizing interpretation methods and gold rationales to enhance models. Our framework is very general in the sense that it can... | Zhuo Chen, Chengyue Jiang, Kewei Tu |  |
| 1223 |  |  [An Expression Tree Decoding Strategy for Mathematical Equation Generation](https://doi.org/10.18653/v1/2023.emnlp-main.29) |  | 0 | Generating mathematical equations from natural language requires an accurate understanding of the relations among math expressions. Existing approaches can be broadly categorized into token-level and expression-level generation. The former treats equations as a mathematical language, sequentially generating math tokens. Expression-level methods generate each expression one by one. However, each expression represents a solving step, and there naturally exist parallel or dependent relations... | Wenqi Zhang, Yongliang Shen, Qingpeng Nong, Zeqi Tan, Yanna Ma, Weiming Lu |  |
| 1224 |  |  [Bootstrapping Small & High Performance Language Models with Unmasking-Removal Training Policy](https://doi.org/10.18653/v1/2023.emnlp-main.30) |  | 0 | BabyBERTa, a language model trained on small-scale child-directed speech while none of the words are unmasked during training, has been shown to achieve a level of grammaticality comparable to that of RoBERTa-base, which is trained on 6,000 times more words and 15 times more parameters. Relying on this promising result, we explore in this paper the performance of BabyBERTa-based models in downstream tasks, focusing on Semantic Role Labeling (SRL) and two Extractive Question Answering tasks,... | Yahan Yang, Elior Sulem, Insup Lee, Dan Roth |  |
| 1225 |  |  [Diversity Enhanced Narrative Question Generation for Storybooks](https://doi.org/10.18653/v1/2023.emnlp-main.31) |  | 0 | Question generation (QG) from a given context can enhance comprehension, engagement, assessment, and overall efficacy in learning or conversational environments. Despite recent advancements in QG, the challenge of enhancing or measuring the diversity of generated questions often remains unaddressed. In this paper, we introduce a multi-question generation model (mQG), which is capable of generating multiple, diverse, and answerable questions by focusing on context and questions. To validate the... | Hokeun Yoon, JinYeong Bak |  |
| 1226 |  |  [Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification](https://doi.org/10.18653/v1/2023.emnlp-main.32) |  | 0 | Recent advances in weakly supervised text classification mostly focus on designing sophisticated methods to turn high-level human heuristics into quality pseudo-labels. In this paper, we revisit the seed matching-based method, which is arguably the simplest way to generate pseudo-labels, and show that its power was greatly underestimated. We show that the limited performance of seed matching is largely due to the label bias injected by the simple seed-match rule, which prevents the classifier... | Chengyu Dong, Zihan Wang, Jingbo Shang |  |
| 1227 |  |  [How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.33) |  | 0 | Our investigation into the Affective Reasoning in Conversation (ARC) task highlights the challenge of causal discrimination. Almost all existing models, including large language models (LLMs), excel at capturing semantic correlations within utterance embeddings but fall short in determining the specific causal relationships. To overcome this limitation, we propose the incorporation of i.i.d. noise terms into the conversation process, thereby constructing a structural causal model (SCM). It... | Hang Chen, Xinyu Yang, Jing Luo, Wenjing Zhu |  |
| 1228 |  |  [Compressing and Debiasing Vision-Language Pre-Trained Models for Visual Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.34) |  | 0 | Despite the excellent performance of vision-language pre-trained models (VLPs) on conventional VQA task, they still suffer from two problems: First, VLPs tend to rely on language biases in datasets and fail to generalize to out-of-distribution (OOD) data. Second, they are inefficient in terms of memory footprint and computation. Although promising progress has been made in both problems, most existing works tackle them independently. To facilitate the application of VLP to VQA tasks, it is... | Qingyi Si, Yuanxin Liu, Zheng Lin, Peng Fu, Yanan Cao, Weiping Wang |  |
| 1229 |  |  [Selectively Answering Ambiguous Questions](https://doi.org/10.18653/v1/2023.emnlp-main.35) |  | 0 | Trustworthy language models should abstain from answering questions when they do not know the answer. However, the answer to a question can be unknown for a variety of reasons. Prior research has focused on the case in which the question is clear and the answer is unambiguous but possibly unknown. However, the answer to a question can also be unclear due to uncertainty of the questioner’s intent or context. We investigate question answering from this perspective, focusing on answering a subset... | Jeremy R. Cole, Michael J. Q. Zhang, Daniel Gillick, Julian Eisenschlos, Bhuwan Dhingra, Jacob Eisenstein |  |
| 1230 |  |  [Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.36) |  | 0 | Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts. In this paper, we develop an approach to use in-context learning (ICL) with large language models (LLMs) for TKG forecasting. Our extensive evaluation compares diverse baselines, including both simple heuristics and state-of-the-art (SOTA) supervised models, against pre-trained LLMs across several popular benchmarks and experimental settings. We observe that naive LLMs... | DongHo Lee, Kian Ahrabian, Woojeong Jin, Fred Morstatter, Jay Pujara |  |
| 1231 |  |  [Knowledge Graph Compression Enhances Diverse Commonsense Generation](https://doi.org/10.18653/v1/2023.emnlp-main.37) |  | 0 | Generating commonsense explanations requires reasoning about commonsense knowledge beyond what is explicitly mentioned in the context. Existing models use commonsense knowledge graphs such as ConceptNet to extract a subgraph of relevant knowledge pertaining to concepts in the input. However, due to the large coverage and, consequently, vast scale of ConceptNet, the extracted subgraphs may contain loosely related, redundant and irrelevant information, which can introduce noise into the model. We... | Eunjeong Hwang, Veronika Thost, Vered Shwartz, Tengfei Ma |  |
| 1232 |  |  [Pragmatic Reasoning Unlocks Quantifier Semantics for Foundation Models](https://doi.org/10.18653/v1/2023.emnlp-main.38) |  | 0 | Generalized quantifiers (e.g., few, most) are used to indicate the proportions predicates satisfy (for example, some apples are red). One way to interpret quantifier semantics is to explicitly bind these satisfactions with percentage scopes (e.g., 30%-40% of apples are red). This approach can be helpful for tasks like logic formalization and surface-form quantitative reasoning (Gordon and Schubert, 2010; Roy et al., 2015). However, it remains unclear if recent foundation models (Bommasani et... | Yiyuan Li, Rakesh R. Menon, Sayan Ghosh, Shashank Srivastava |  |
| 1233 |  |  [LLM-FP4: 4-Bit Floating-Point Quantized Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.39) |  | 0 | We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point (FP) quantization is more flexible and can better handle long-tail or bell-shaped distributions, and it has emerged as a default choice in many hardware platforms. One... | ShihYang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, KwangTing Cheng |  |
| 1234 |  |  [Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers](https://doi.org/10.18653/v1/2023.emnlp-main.40) |  | 0 | Abstracts derived from biomedical literature possess distinct domain-specific characteristics, including specialised writing styles and biomedical terminologies, which necessitate a deep understanding of the related literature. As a result, existing language models struggle to generate technical summaries that are on par with those produced by biomedical experts, given the absence of domain-specific background knowledge. This paper aims to enhance the performance of language models in... | Chen Tang, Shun Wang, Tomas Goldsack, Chenghua Lin |  |
| 1235 |  |  [Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting](https://doi.org/10.18653/v1/2023.emnlp-main.41) |  | 0 | Recent work has shown how to prompt large language models with explanations to obtain strong performance on textual reasoning tasks, i.e., the chain-of-thought paradigm. However, subtly different explanations can yield widely varying downstream task accuracy. Explanations that have not been “tuned” for a task, such as off-the-shelf explanations written by non-experts, may lead to mediocre performance. This paper tackles the problem of how to optimize explanation-infused prompts in a blackbox... | Xi Ye, Greg Durrett |  |
| 1236 |  |  [HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.42) |  | 0 | Hallucinations in machine translation are translations that contain information completely unrelated to the input. Omissions are translations that do not include some of the input information. While both cases tend to be catastrophic errors undermining user trust, annotated data with these types of pathologies is extremely scarce and is limited to a few high-resource languages. In this work, we release an annotated dataset for the hallucination and omission phenomena covering 18 translation... | David Dale, Elena Voita, Janice Lam, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Loïc Barrault, Marta R. Costajussà |  |
| 1237 |  |  [Gradient-based Gradual Pruning for Language-Specific Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.43) |  | 0 | Multilingual neural machine translation (MNMT) offers the convenience of translating between multiple languages with a single model. However, MNMT often suffers from performance degradation in high-resource languages compared to bilingual counterparts. This degradation is commonly attributed to parameter interference, which occurs when parameters are fully shared across all language pairs. In this work, to tackle this issue we propose a gradient-based gradual pruning technique for MNMT. Our... | Dan He, MinhQuang Pham, ThanhLe Ha, Marco Turchi |  |
| 1238 |  |  [LLM-powered Data Augmentation for Enhanced Cross-lingual Performance](https://doi.org/10.18653/v1/2023.emnlp-main.44) |  | 0 | This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare... | Chenxi Whitehouse, Monojit Choudhury, Alham Fikri Aji |  |
| 1239 |  |  [Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.45) |  | 0 | Implicit Discourse Relation Recognition (IDRR), which infers discourse relations without the help of explicit connectives, is still a crucial and challenging task for discourse parsing. Recent works tend to exploit the hierarchical structure information from the annotated senses, which demonstrate enhanced discourse relation representations can be obtained by integrating sense hierarchy. Nevertheless, the performance and robustness for IDRR are significantly constrained by the availability of... | Chenxu Wang, Ping Jian, Mu Huang |  |
| 1240 |  |  [VLIS: Unimodal Language Models Guide Multimodal Language Generation](https://doi.org/10.18653/v1/2023.emnlp-main.46) |  | 0 | Multimodal language generation, which leverages the synergy of language and vision, is a rapidly expanding field. However, existing vision-language models face challenges in tasks that require complex linguistic understanding. To address this issue, we introduce Visual-Language models as Importance Sampling weights (VLIS), a novel framework that combines the visual conditioning capability of vision-language models with the language understanding of unimodal text-only language models without... | Jiwan Chung, Youngjae Yu |  |
| 1241 |  |  [Conceptual structure coheres in human cognition but not in large language models](https://doi.org/10.18653/v1/2023.emnlp-main.47) |  | 0 | Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior in various semantic tasks. In contemporary language models, however, it is possible to interrogate the latent structure of conceptual representations using methods nearly identical to those commonly... | Siddharth Suresh, Kushin Mukherjee, Xizheng Yu, WeiChun Huang, Lisa Padua, Timothy T. Rogers |  |
| 1242 |  |  [Towards LLM-driven Dialogue State Tracking](https://doi.org/10.18653/v1/2023.emnlp-main.48) |  | 0 | Dialogue State Tracking (DST) is of paramount importance in ensuring accurate tracking of user goals and system actions within task-oriented dialogue systems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT has sparked considerable interest in assessing their efficacy across diverse applications. In this study, we conduct an initial examination of ChatGPT’s capabilities in DST. Our evaluation uncovers the exceptional performance of ChatGPT in this task, offering valuable... | Yujie Feng, Zexin Lu, Bo Liu, Liming Zhan, XiaoMing Wu |  |
| 1243 |  |  [Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.49) |  | 0 | Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich information from multiple sources (\*e.g.,\* language, video, and audio), the potential sentiment-irrelevant and conflicting information across modalities may hinder the performance from being further improved. To alleviate this, we present Adaptive Language-guided Multimodal Transformer (ALMT), which incorporates an Adaptive Hyper-modality Learning (AHL) module to learn an irrelevance/conflict-suppressing... | Haoyu Zhang, Yu Wang, Guanghao Yin, Kejun Liu, Yuanyuan Liu, Tianshu Yu |  |
| 1244 |  |  [Multitask Multimodal Prompted Training for Interactive Embodied Task Completion](https://doi.org/10.18653/v1/2023.emnlp-main.50) |  | 0 | Interactive and embodied tasks pose at least two fundamental challenges to existing Vision & Language (VL) models, including 1) grounding language in trajectories of actions and observations, and 2) referential disambiguation. To tackle these challenges, we propose an Embodied MultiModal Agent (EMMA): a unified encoder-decoder model that reasons over images and trajectories, and casts action prediction as multimodal text generation. By unifying all tasks as text generation, EMMA learns a... | Georgios Pantazopoulos, Malvina Nikandrou, Amit Parekh, Bhathiya Hemanthage, Arash Eshghi, Ioannis Konstas, Verena Rieser, Oliver Lemon, Alessandro Suglia |  |
| 1245 |  |  [We're Afraid Language Models Aren't Modeling Ambiguity](https://doi.org/10.18653/v1/2023.emnlp-main.51) |  | 0 | Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We capture ambiguity in a sentence through its effect on entailment relations with another sentence, and collect AmbiEnt, a... | Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah A. Smith, Yejin Choi |  |
| 1246 |  |  [Linear-Time Modeling of Linguistic Structure: An Order-Theoretic Perspective](https://doi.org/10.18653/v1/2023.emnlp-main.52) |  | 0 | Tasks that model the relation between pairs of tokens in a string are a vital part of understanding natural language. Such tasks, in general, require exhaustive pair-wise comparisons of tokens, thus having a quadratic runtime complexity in the length of the string. We show that these exhaustive comparisons can be avoided, and, moreover, the complexity of such tasks can be reduced to linear by casting the relation between tokens as a partial order over the string. Our method predicts real... | Tianyu Liu, Afra Amini, Mrinmaya Sachan, Ryan Cotterell |  |
| 1247 |  |  [GEMINI: Controlling The Sentence-Level Summary Style in Abstractive Text Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.53) |  | 0 | Human experts write summaries using different techniques, including extracting a sentence from the document and rewriting it, or fusing various information from the document to abstract it. These techniques are flexible and thus difficult to be imitated by any single method. To address this issue, we propose an adaptive model, GEMINI, that integrates a rewriter and a generator to mimic the sentence rewriting and abstracting techniques, respectively. GEMINI adaptively chooses to rewrite a... | Guangsheng Bao, Zebin Ou, Yue Zhang |  |
| 1248 |  |  [Fidelity-Enriched Contrastive Search: Reconciling the Faithfulness-Diversity Trade-Off in Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.54) |  | 0 | In this paper, we address the hallucination problem commonly found in natural language generation tasks. Language models often generate fluent and convincing content but can lack consistency with the provided source, resulting in potential inaccuracies. We propose a new decoding method called Fidelity-Enriched Contrastive Search (FECS), which augments the contrastive search framework with context-aware regularization terms. FECS promotes tokens that are semantically similar to the provided... | WeiLin Chen, ChengKuang Wu, HsinHsi Chen, ChungChi Chen |  |
| 1249 |  |  [Analyzing Norm Violations in Live-Stream Chat](https://doi.org/10.18653/v1/2023.emnlp-main.55) |  | 0 | Toxic language, such as hate speech, can deter users from participating in online communities and enjoying popular platforms. Previous approaches to detecting toxic language and norm violations have been primarily concerned with conversations from online forums and social media, such as Reddit and Twitter. These approaches are less effective when applied to conversations on live-streaming platforms, such as Twitch and YouTube Live, as each comment is only visible for a limited time and lacks a... | Jihyung Moon, DongHo Lee, Hyundong Cho, Woojeong Jin, Chan Young Park, Minwoo Kim, Jonathan May, Jay Pujara, Sungjoon Park |  |
| 1250 |  |  [Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality](https://doi.org/10.18653/v1/2023.emnlp-main.56) |  | 0 | Contrastively trained vision-language models have achieved remarkable progress in vision and language representation learning. However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations. Scene graphs have emerged as an effective way to understand images compositionally. These are graph-structured semantic representations of images that contain objects, their attributes, and relations... | Harman Singh, Pengchuan Zhang, Qifan Wang, Mengjiao Wang, Wenhan Xiong, Jingfei Du, Yu Chen |  |
| 1251 |  |  [Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms](https://doi.org/10.18653/v1/2023.emnlp-main.57) |  | 0 | Commonsense norms are defeasible by context: reading books is usually great, but not when driving a car. While contexts can be explicitly described in language, in embodied scenarios, contexts are often provided visually. This type of visually grounded reasoning about defeasible commonsense norms is generally easy for humans, but (as we show) poses a challenge for machines, as it necessitates both visual understanding and reasoning about commonsense norms. We construct a new multimodal... | Seungju Han, Junhyeok Kim, Jack Hessel, Liwei Jiang, Jiwan Chung, Yejin Son, Yejin Choi, Youngjae Yu |  |
| 1252 |  |  [Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus](https://doi.org/10.18653/v1/2023.emnlp-main.58) |  | 0 | Large Language Models (LLMs) have gained significant popularity for their impressive performance across diverse fields. However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications. Existing works for detecting hallucinations in LLMs either rely on external knowledge for reference retrieval or require sampling multiple responses from the LLM for consistency verification, making these methods costly and inefficient.... | Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng, Yue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing Wang, Luoyi Fu |  |
| 1253 |  |  [FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.59) |  | 0 | Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB—a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models... | Shangbin Feng, Vidhisha Balachandran, Yuyang Bai, Yulia Tsvetkov |  |
| 1254 |  |  [Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation](https://doi.org/10.18653/v1/2023.emnlp-main.60) |  | 0 | Modern NLP models are often trained over large untrusted datasets, raising the potential for a malicious adversary to compromise model behaviour. For instance, backdoors can be implanted through crafting training instances with a specific textual trigger and a target label. This paper posits that backdoor poisoning attacks exhibit a spurious correlation between simple text features and classification labels, and accordingly, proposes methods for mitigating spurious correlation as means of... | Xuanli He, Qiongkai Xu, Jun Wang, Benjamin I. P. Rubinstein, Trevor Cohn |  |
| 1255 |  |  [Symbol tuning improves in-context learning in language models](https://doi.org/10.18653/v1/2023.emnlp-main.61) |  | 0 | We present symbol tuning - finetuning language models on in-context input-label pairs where natural language labels (e.g., “positive/negative sentiment”) are replaced with arbitrary symbols (e.g., “foo/bar”). Symbol tuning leverages the intuition that when a model cannot use instructions or natural language labels to figure out a task, it must instead do so by learning the input-label mappings. We experiment with symbol tuning across PaLM models up to 540B parameters and observe benefits across... | Jerry W. Wei, Le Hou, Andrew K. Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, Quoc V. Le |  |
| 1256 |  |  [The neural dynamics of word recognition and integration](https://doi.org/10.18653/v1/2023.emnlp-main.62) |  | 0 | Listeners recognize and integrate words in rapid and noisy everyday speech by combining expectations about upcoming content with incremental sensory evidence. We present a computational model of word recognition which formalizes this perceptual process in Bayesian decision theory. We fit this model to explain scalp EEG signals recorded as subjects passively listened to a fictional story, revealing both the dynamics of the online auditory word recognition process and the neural correlates of the... | Jon Gauthier, Roger Levy |  |
| 1257 |  |  [Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.63) |  | 0 | Questions in open-domain question answering are often ambiguous, allowing multiple interpretations. One approach to handling them is to identify all possible interpretations of the ambiguous question (AQ) and to generate a long-form answer addressing them all, as suggested by Stelmakh et al., (2022). While it provides a comprehensive response without bothering the user for clarification, considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge. To... | Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, Jaewoo Kang |  |
| 1258 |  |  [Incorporating Worker Perspectives into MTurk Annotation Practices for NLP](https://doi.org/10.18653/v1/2023.emnlp-main.64) |  | 0 | Current practices regarding data collection for natural language processing on Amazon Mechanical Turk (MTurk) often rely on a combination of studies on data quality and heuristics shared among NLP researchers. However, without considering the perspectives of MTurk workers, these approaches are susceptible to issues regarding workers’ rights and poor response quality. We conducted a critical literature review and a survey of MTurk workers aimed at addressing open questions regarding best... | Olivia Huang, Eve Fleisig, Dan Klein |  |
| 1259 |  |  [Predict the Future from the Past? On the Temporal Data Distribution Shift in Financial Sentiment Classifications](https://doi.org/10.18653/v1/2023.emnlp-main.65) |  | 0 | Temporal data distribution shift is prevalent in the financial text. How can a financial sentiment analysis system be trained in a volatile market environment that can accurately infer sentiment and be robust to temporal data distribution shifts? In this paper, we conduct an empirical study on the financial sentiment analysis system under temporal data distribution shifts using a real-world financial social media dataset that spans three years. We find that the fine-tuned models suffer from... | Yue Guo, Chenxi Hu, Yi Yang |  |
| 1260 |  |  [Look-back Decoding for Open-Ended Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.66) |  | 0 | Given a prefix (context), open-ended generation aims to decode texts that are coherent, which do not abruptly drift from previous topics, and informative, which do not suffer from undesired repetitions. In this paper, we propose Look-back, an improved decoding algorithm that leverages the Kullback–Leibler divergence to track the distribution distance between current and historical decoding steps. Thus Look-back can automatically predict potential repetitive phrase and topic drift, and remove... | Nan Xu, Chunting Zhou, Asli Celikyilmaz, Xuezhe Ma |  |
| 1261 |  |  [Large Language Models Can Self-Improve](https://doi.org/10.18653/v1/2023.emnlp-main.67) |  | 0 | Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate “high-confidence” rationale-augmented answers for unlabeled questions using Chain-of-Though (CoT) prompting... | Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han |  |
| 1262 |  |  [CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://doi.org/10.18653/v1/2023.emnlp-main.68) |  | 0 | Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks, lacking the flexibility to operate in the optimal architecture for a specific task. Secondly, they often employ a limited set of pretraining objectives which might not be... | Yue Wang, Hung Le, Akhilesh Gotmare, Nghi D. Q. Bui, Junnan Li, Steven C. H. Hoi |  |
| 1263 |  |  [Structural generalization in COGS: Supertagging is (almost) all you need](https://doi.org/10.18653/v1/2023.emnlp-main.69) |  | 0 | In many Natural Language Processing applications, neural networks have been found to fail to generalize on out-of-distribution examples. In particular, several recent semantic parsing datasets have put forward important limitations of neural networks in cases where compositional generalization is required. In this work, we extend a neural graph-based parsing framework in several ways to alleviate this issue, notably: (1) the introduction of a supertagging step with valency constraints,... | Alban Petit, Caio F. Corro, François Yvon |  |
| 1264 |  |  [BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations](https://doi.org/10.18653/v1/2023.emnlp-main.70) |  | 0 | Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose BioT5, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical... | Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, Rui Yan |  |
| 1265 |  |  [Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings](https://doi.org/10.18653/v1/2023.emnlp-main.71) |  | 0 | Cross-lingual transfer learning is an important property of multilingual large language models (LLMs). But how do LLMs represent relationships between languages? Every language model has an input layer that maps tokens to vectors. This ubiquitous layer of language models is often overlooked. We find that similarities between these input embeddings are highly interpretable and that the geometry of these embeddings differs between model families. In one case (XLM-RoBERTa), embeddings encode... | Andrea W. WenYi, David Mimno |  |
| 1266 |  |  [Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation](https://doi.org/10.18653/v1/2023.emnlp-main.72) |  | 0 | Target-oriented dialogue systems, designed to proactively steer conversations toward predefined targets or accomplish specific system-side goals, are an exciting area in conversational AI. In this work, by formulating a <dialogue act, topic> pair as the conversation target, we explore a novel problem of personalized target-oriented dialogue by considering personalization during the target accomplishment process. However, there remains an emergent need for high-quality datasets, and building one... | Jian Wang, Yi Cheng, Dongding Lin, Chak Tou Leong, Wenjie Li |  |
| 1267 |  |  [SeqXGPT: Sentence-Level AI-Generated Text Detection](https://doi.org/10.18653/v1/2023.emnlp-main.73) |  | 0 | Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs. Therefore, it is important to build strong AI-generated text (AIGT) detectors. Current works only consider document-level AIGT detection, therefore, in this paper, we first introduce a sentence-level detection challenge by synthesizing a dataset that contains documents that are polished with LLMs, that is, the documents contain sentences written by humans and sentences modified... | Pengyu Wang, Linyang Li, Ke Ren, Botian Jiang, Dong Zhang, Xipeng Qiu |  |
| 1268 |  |  [QTSumm: Query-Focused Summarization over Tabular Data](https://doi.org/10.18653/v1/2023.emnlp-main.74) |  | 0 | People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users’ information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named... | Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin Liu, Weijin Zou, Simeng Han, Ruizhe Chen, Xiangru Tang, Yumo Xu, Dragomir Radev, Arman Cohan |  |
| 1269 |  |  [From Wrong To Right: A Recursive Approach Towards Vision-Language Explanation](https://doi.org/10.18653/v1/2023.emnlp-main.75) |  | 0 | Addressing the challenge of adapting pre-trained vision-language models for generating insightful explanations for visual reasoning tasks with limited annotations, we present ReVisE: a Recursive Visual Explanation algorithm. Our method iteratively computes visual features (conditioned on the text input), an answer, and an explanation, to improve the explanation quality step by step until the answer converges. We find that this multi-step approach guides the model to correct its own answers and... | Jiaxin Ge, Sanjay Subramanian, Trevor Darrell, Boyi Li |  |
| 1270 |  |  ['Don't Get Too Technical with Me': A Discourse Structure-Based Framework for Automatic Science Journalism](https://doi.org/10.18653/v1/2023.emnlp-main.76) |  | 0 | Science journalism refers to the task of reporting technical findings of a scientific paper as a less technical news article to the general public audience. We aim to design an automated system to support this real-world task (i.e., automatic science journalism ) by 1) introducing a newly-constructed and real-world dataset (SciTechNews), with tuples of a publicly-available scientific paper, its corresponding news article, and an expert-written short summary snippet; 2) proposing a novel... | Ronald Cardenas, Bingsheng Yao, Dakuo Wang, Yufang Hou |  |
| 1271 |  |  [LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following](https://doi.org/10.18653/v1/2023.emnlp-main.77) |  | 0 | End-to-end Transformers have demonstrated an impressive success rate for Embodied Instruction Following when the environment has been seen in training. However, they tend to struggle when deployed in an unseen environment. This lack of generalizability is due to the agent’s insensitivity to subtle changes in natural language instructions. To mitigate this issue, we propose explicitly aligning the agent’s hidden states with the instructions via contrastive learning. Nevertheless, the semantic... | ChengFu Yang, YenChun Chen, Jianwei Yang, Xiyang Dai, Lu Yuan, YuChiang Frank Wang, KaiWei Chang |  |
| 1272 |  |  [Penalty Decoding: Well Suppress the Self-Reinforcement Effect in Open-Ended Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.78) |  | 0 | The decoding algorithm is critical for open-ended text generation, transforming latent representations into coherent and meaningful outputs. This paper investigates the self-reinforcement effect in text generation and the effectiveness of a repetition penalty to mitigate it. However, determining the optimal repetition penalty value is challenging. To tackle this, we propose a forgetting mechanism that disregards distant tokens, reducing the burden of penalty selection. In addition, we introduce... | Wenhong Zhu, Hongkun Hao, Rui Wang |  |
| 1273 |  |  [Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.79) |  | 0 | The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they... | Jianwei Li, Qi Lei, Wei Cheng, Dongkuan Xu |  |
| 1274 |  |  [Clinical Contradiction Detection](https://doi.org/10.18653/v1/2023.emnlp-main.80) |  | 0 | Detecting contradictions in text is essential in determining the validity of the literature and sources that we consume. Medical corpora are riddled with conflicting statements. This is due to the large throughput of new studies and the difficulty in replicating experiments, such as clinical trials. Detecting contradictions in this domain is hard since it requires clinical expertise. We present a distant supervision approach that leverages a medical ontology to build a seed of potential... | Dave Makhervaks, Plia Gillis, Kira Radinsky |  |
| 1275 |  |  [Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements](https://doi.org/10.18653/v1/2023.emnlp-main.81) |  | 0 | Today’s language models can be remarkably intelligent yet still produce text that contains trivial commonsense errors. Therefore, we seek a retrospective verification approach that can reflect on the commonsense plausibility of the machine text, and introduce Vera, a general-purpose model that learns to estimate the commonsense plausibility of declarative statements. To support diverse commonsense domains, Vera is trained on ~7M commonsense statements that are automatically converted from 19 QA... | Jiacheng Liu, Wenya Wang, Dianzhuo Wang, Noah A. Smith, Yejin Choi, Hannaneh Hajishirzi |  |
| 1276 |  |  [Text-Transport: Toward Learning Causal Effects of Natural Language](https://doi.org/10.18653/v1/2023.emnlp-main.82) |  | 0 | As language technologies gain prominence in real-world settings, it is important to understand \*how\* changes to language affect reader perceptions. This can be formalized as the \*causal effect\* of varying a linguistic attribute (e.g., sentiment) on a reader’s response to the text. In this paper, we introduce Text-Transport, a method for estimation of causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong... | Victoria Lin, LouisPhilippe Morency, Eli BenMichael |  |
| 1277 |  |  [How Does Generative Retrieval Scale to Millions of Passages?](https://doi.org/10.18653/v1/2023.emnlp-main.83) |  | 0 | The emerging paradigm of generative retrieval re-frames the classic information retrieval problem into a sequence-to-sequence modeling task, forgoing external indices and encoding an entire document corpus within a single Transformer. Although many different approaches have been proposed to improve the effectiveness of generative retrieval, they have only been evaluated on document corpora on the order of 100K in size. We conduct the first empirical study of generative retrieval techniques... | Ronak Pradeep, Kai Hui, Jai Gupta, Ádám D. Lelkes, Honglei Zhuang, Jimmy Lin, Donald Metzler, Vinh Q. Tran |  |
| 1278 |  |  [Unveiling the Implicit Toxicity in Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.84) |  | 0 | The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use. While recent studies primarily focus on probing toxic outputs that can be easily detected with existing toxicity classifiers, we show that LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting. Moreover, we propose a reinforcement learning (RL) based attacking... | Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, Minlie Huang |  |
| 1279 |  |  [Is ChatGPT a General-Purpose Natural Language Processing Task Solver?](https://doi.org/10.18653/v1/2023.emnlp-main.85) |  | 0 | Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot—i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet... | Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang |  |
| 1280 |  |  [Length is a Curse and a Blessing for Document-level Semantics](https://doi.org/10.18653/v1/2023.emnlp-main.86) |  | 0 | In recent years, contrastive learning (CL) has been extensively utilized to recover sentence and document-level encoding capability from pre-trained language models. In this work, we question the length generalizability of CL-based models, i.e., their vulnerability towards length-induced semantic shift. We verify not only that length vulnerability is a significant yet overlooked research gap, but we can devise unsupervised CL methods solely depending on the semantic signal provided by document... | Chenghao Xiao, Yizhi Li, G. Thomas Hudson, Chenghua Lin, Noura Al Moubayed |  |
| 1281 |  |  [ALCUNA: Large Language Models Meet New Knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.87) |  | 0 | With the rapid development of NLP, large-scale language models (LLMs) excel in various tasks across multiple domains now. However, existing benchmarks may not adequately measure these models’ capabilities, especially when faced with new knowledge. In this paper, we address the lack of benchmarks to evaluate LLMs’ ability to handle new knowledge, an important and challenging aspect in the rapidly evolving world. We propose an approach called KnowGen that generates new knowledge by altering... | Xunjian Yin, Baizhou Huang, Xiaojun Wan |  |
| 1282 |  |  [Location-Aware Visual Question Generation with Lightweight Models](https://doi.org/10.18653/v1/2023.emnlp-main.88) |  | 0 | This work introduces a novel task, location-aware visual question generation (LocaVQG), which aims to generate engaging questions from data relevant to a particular geographical location. Specifically, we represent such location-aware information with surrounding images and a GPS coordinate. To tackle this task, we present a dataset generation pipeline that leverages GPT-4 to produce diverse and sophisticated questions. Then, we aim to learn a lightweight model that can address the LocaVQG task... | Nicholas Collin Suwono, Justin ChihYao Chen, TunMin Hung, TingHao (Kenneth) Huang, IBin Liao, YungHui Li, LunWei Ku, ShaoHua Sun |  |
| 1283 |  |  [MemeCap: A Dataset for Captioning and Interpreting Memes](https://doi.org/10.18653/v1/2023.emnlp-main.89) |  | 0 | Memes are a widely popular tool for web users to express their thoughts using visual metaphors. Understanding memes requires recognizing and interpreting visual metaphors with respect to the text inside or around the meme, often while employing background knowledge and reasoning abilities. We present the task of meme captioning and release a new dataset, MemeCap. Our dataset contains 6.3K memes along with the title of the post containing the meme, the meme captions, the literal image caption,... | Eunjeong Hwang, Vered Shwartz |  |
| 1284 |  |  [Where to start? Analyzing the potential value of intermediate models](https://doi.org/10.18653/v1/2023.emnlp-main.90) |  | 0 | Previous studies observed that finetuned models may be better base models than the vanilla pretrained model. Such a model, finetuned on some source dataset, may provide a better starting point for a new finetuning process on a desired target dataset. Here, we perform a systematic analysis of this intertraining scheme, over a wide range of English classification tasks. Surprisingly, our analysis suggests that the potential intertraining gain can be analyzed independently for the target dataset... | Leshem Choshen, Elad Venezian, Shachar DonYehiya, Noam Slonim, Yoav Katz |  |
| 1285 |  |  [Transcending Scaling Laws with 0.1% Extra Compute](https://doi.org/10.18653/v1/2023.emnlp-main.91) |  | 0 | Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model on a few more steps with UL2’s mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to... | Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q. Tran, David R. So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, Denny Zhou, Donald Metzler, Slav Petrov, Neil Houlsby, Quoc V. Le, Mostafa Dehghani |  |
| 1286 |  |  [CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation](https://doi.org/10.18653/v1/2023.emnlp-main.92) |  | 0 | Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators,... | Minzhi Li, Taiwei Shi, Caleb Ziems, MinYen Kan, Nancy F. Chen, Zhengyuan Liu, Diyi Yang |  |
| 1287 |  |  [Optimizing Retrieval-augmented Reader Models via Token Elimination](https://doi.org/10.18653/v1/2023.emnlp-main.93) |  | 0 | Fusion-in-Decoder (FiD) is an effective retrieval-augmented language model applied across a variety of open-domain tasks, such as question answering, fact checking, etc. In FiD, supporting passages are first retrieved and then processed using a generative model (Reader), which can cause a significant bottleneck in decoding time, particularly with long outputs. In this work, we analyze the contribution and necessity of all the retrieved passages to the performance of reader models, and propose... | Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, Moshe Wasserblat |  |
| 1288 |  |  [WSDMS: Debunk Fake News via Weakly Supervised Detection of Misinforming Sentences with Contextualized Social Wisdom](https://doi.org/10.18653/v1/2023.emnlp-main.94) |  | 0 | Fake news debunking primarily focuses on determining the truthfulness of news articles, which oversimplifies the issue as fake news often combines elements of both truth and falsehood. Thus, it becomes crucial to identify specific instances of misinformation within the articles. In this research, we investigate a novel task in the field of fake news debunking, which involves detecting sentence-level misinformation. One of the major challenges in this task is the absence of a training dataset... | Ruichao Yang, Wei Gao, Jing Ma, Hongzhan Lin, Zhiwei Yang |  |
| 1289 |  |  [Robust Prompt Optimization for Large Language Models Against Distribution Shifts](https://doi.org/10.18653/v1/2023.emnlp-main.95) |  | 0 | Large Language Model (LLM) has demonstrated significant ability in various Natural Language Processing tasks. However, their effectiveness is highly dependent on the phrasing of the task prompt, leading to research on automatic prompt optimization using labeled task data. We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis. In this light, we... | Moxin Li, Wenjie Wang, Fuli Feng, Yixin Cao, Jizhi Zhang, TatSeng Chua |  |
| 1290 |  |  [Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.96) |  | 0 | Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex... | Martin Josifoski, Marija Sakota, Maxime Peyrard, Robert West |  |
| 1291 |  |  [Condensing Multilingual Knowledge with Lightweight Language-Specific Modules](https://doi.org/10.18653/v1/2023.emnlp-main.97) |  | 0 | Incorporating language-specific (LS) modules or Mixture-of-Experts (MoE) are proven methods to boost performance in multilingual model performance, but the scalability of these approaches to hundreds of languages or experts tends to be hard to manage. We present Language-specific Matrix Synthesis (LMS), a novel method that addresses the issue. LMS utilizes parameter-efficient and lightweight modules, reducing the number of parameters while outperforming existing methods, e.g., +1.73 BLEU over... | Haoran Xu, Weiting Tan, Shuyue Stella Li, Yunmo Chen, Benjamin Van Durme, Philipp Koehn, Kenton Murray |  |
| 1292 |  |  [The Framework Tax: Disparities Between Inference Efficiency in NLP Research and Deployment](https://doi.org/10.18653/v1/2023.emnlp-main.98) |  | 0 | Increased focus on the computational efficiency of systems in natural language processing has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning... | Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell |  |
| 1293 |  |  [Evaluating Cross-Domain Text-to-SQL Models and Benchmarks](https://doi.org/10.18653/v1/2023.emnlp-main.99) |  | 0 | Text-to-SQL benchmarks play a crucial role in evaluating the progress made in the field and the ranking of different models. However, accurately matching a model-generated SQL query to a reference SQL query in a benchmark fails for various reasons, such as underspecified natural language queries, inherent assumptions in both model-generated and reference queries, and the non-deterministic nature of SQL output under certain conditions. In this paper, we conduct an extensive study of several... | Mohammadreza Pourreza, Davood Rafiei |  |
| 1294 |  |  [Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.100) |  | 0 | Recent work in Natural Language Processing and Computer Vision has been using textual information – e.g., entity names and descriptions – available in knowledge graphs to ground neural models to high-quality structured data. However, when it comes to non-English languages, the quantity and quality of textual information are comparatively scarce. To address this issue, we introduce the novel task of automatic Knowledge Graph Completion (KGE) and perform a thorough investigation on bridging the... | Simone Conia, Min Li, Daniel Lee, Umar Farooq Minhas, Ihab F. Ilyas, Yunyao Li |  |
| 1295 |  |  [Memory-Based Invariance Learning for Out-of-Domain Text Classification](https://doi.org/10.18653/v1/2023.emnlp-main.101) |  | 0 | We investigate the task of out-of-domain (OOD) text classification with the aim of extending a classification model, trained on multiple source domains, to an unseen target domain. Recent studies have shown that learning invariant representations can enhance the performance of OOD generalization. However, the inherent disparity in data distribution across different domains poses challenges for achieving effective invariance learning. This study addresses this issue by employing memory... | Chen Jia, Yue Zhang |  |
| 1296 |  |  [Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling](https://doi.org/10.18653/v1/2023.emnlp-main.102) |  | 0 | Post-training quantization (PTQ) of transformer language models faces significant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are concentrated in specific channels and are asymmetric across channels. To address this issue, we propose the Outlier Suppression+ (OS+) framework, which contains the channel-wise shifting for asymmetry and channel-wise scaling for concentration. We show that these operations can be seamlessly migrated into... | Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong Liu |  |
| 1297 |  |  [Three Stream Based Multi-level Event Contrastive Learning for Text-Video Event Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.103) |  | 0 | Text-video based multimodal event extraction refers to identifying event information from the given text-video pairs. Existing methods predominantly utilize video appearance features (VAF) and text sequence features (TSF) as input information. Some of them employ contrastive learning to align VAF with the event types extracted from TSF. However, they disregard the motion representations in videos and the optimization of contrastive objective could be misguided by the background noise from RGB... | Jiaqi Li, Chuanyi Zhang, Miaozeng Du, Dehai Min, Yongrui Chen, Guilin Qi |  |
| 1298 |  |  [Diversify Question Generation with Retrieval-Augmented Style Transfer](https://doi.org/10.18653/v1/2023.emnlp-main.104) |  | 0 | Given a textual passage and an answer, humans are able to ask questions with various expressions, but this ability is still challenging for most question generation (QG) systems. Existing solutions mainly focus on the internal knowledge within the given passage or the semantic word space for diverse content planning. These methods, however, have not considered the potential of external knowledge for expression diversity. To bridge this gap, we propose RAST, a framework for Retrieval-Augmented... | Qi Gou, Zehua Xia, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li, CamTu Nguyen |  |
| 1299 |  |  [Fast and Accurate Factual Inconsistency Detection Over Long Documents](https://doi.org/10.18653/v1/2023.emnlp-main.105) |  | 0 | Generative AI models exhibit remarkable potential; however, hallucinations across various tasks present a significant challenge, particularly for longer inputs that current approaches struggle to address effectively. We introduce SCALE (Source Chunking Approach for Large-scale inconsistency Evaluation), a task-agnostic model for detecting factual inconsistencies using a novel chunking strategy. Specifically, SCALE is a Natural Language Inference (NLI) based model that uses large text chunks to... | Barrett Martin Lattimer, Patrick Chen, Xinyuan Zhang, Yi Yang |  |
| 1300 |  |  [Interpreting Embedding Spaces by Conceptualization](https://doi.org/10.18653/v1/2023.emnlp-main.106) |  | 0 | One of the main methods for computational interpretation of a text is mapping it into a vector in some embedding space. Such vectors can then be used for a variety of textual processing tasks. Recently, most embedding spaces are a product of training large language models (LLMs). One major drawback of this type of representation is their incomprehensibility to humans. Understanding the embedding space is crucial for several important needs, including the need to debug the embedding method and... | Adi Simhi, Shaul Markovitch |  |
| 1301 |  |  [Knowledge-Augmented Language Model Verification](https://doi.org/10.18653/v1/2023.emnlp-main.107) |  | 0 | Recent Language Models (LMs) have shown impressive capabilities in generating texts with the knowledge internalized in parameters. Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated. To address this problem, previous works propose to augment LMs with the knowledge retrieved from an external knowledge source. However, such approaches often show suboptimal text generation performance due to two reasons:... | Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C. Park, Sung Ju Hwang |  |
| 1302 |  |  [A Generation-based Deductive Method for Math Word Problems](https://doi.org/10.18653/v1/2023.emnlp-main.108) |  | 0 | Math word problems (MWP) involving advanced operators such as linear equation solver cannot be easily tackled by earlier MWP methods, because the existing generation methods suffer from repeated sub-expression generation and deductive methods are restricted to dealing with binary operations. This paper propose a new multivariate directed acyclic graph (mDAG) as an alternative to the generation methods’ binary expression tree or the deductive methods’ binary directed acyclic graph. Then to... | Yuxuan Hu, Jing Zhang, Haoyang Li, Cuiping Li, Hong Chen |  |
| 1303 |  |  [Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation](https://doi.org/10.18653/v1/2023.emnlp-main.109) |  | 0 | Large Language Models (LLMs) have showcased impressive performance. However, due to their inability to capture relationships among samples, these frozen LLMs inevitably keep repeating similar mistakes. In this work, we propose our Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving their performance by learning from previous mistakes. Considering data arrives sequentially, LLMs gradually accumulate rules from incorrect cases, forming a rule collection. These rules are... | Zeyuan Yang, Peng Li, Yang Liu |  |
| 1304 |  |  [Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning](https://doi.org/10.18653/v1/2023.emnlp-main.110) |  | 0 | Maintaining a consistent persona is a key quality for any open domain dialogue system. Current state-of-the-art systems do this by training agents with supervised learning or online reinforcement learning (RL). However, systems trained with supervised learning often lack consistency as they are never punished for uttering contradictions. Additional training with RL can alleviate some of these issues, however the training process is expensive. Instead, we propose an offline RL framework to... | Ryan Shea, Zhou Yu |  |
| 1305 |  |  [Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories](https://doi.org/10.18653/v1/2023.emnlp-main.111) |  | 0 | In this paper we improve the zero-shot generalization ability of language models via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora (external memories), with the option to “plug in” unseen memory at inference time. We develop a joint learning mechanism that trains the augmentation component with latent labels derived from the end retrieval task, paired with hard negatives from the memory mixture. We instantiate the... | Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett |  |
| 1306 |  |  [Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.112) |  | 0 | Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions. However, how to select new tasks to improve the performance and generalizability of IT models remains an open question. Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance. In this work, we propose active instruction... | PoNien Kung, Fan Yin, Di Wu, KaiWei Chang, Nanyun Peng |  |
| 1307 |  |  [Towards Example-Based NMT with Multi-Levenshtein Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.113) |  | 0 | Retrieval-Augmented Machine Translation (RAMT) is attracting growing attention. This is because RAMT not only improves translation metrics, but is also assumed to implement some form of domain adaptation. In this contribution, we study another salient trait of RAMT, its ability to make translation decisions more transparent by allowing users to go back to examples that contributed to these decisions. For this, we propose a novel architecture aiming to increase this transparency. This model... | Maxime Bouthors, Josep Maria Crego, François Yvon |  |
| 1308 |  |  [DUnE: Dataset for Unified Editing](https://doi.org/10.18653/v1/2023.emnlp-main.114) |  | 0 | Even the most advanced language models remain susceptible to errors necessitating to modify these models without initiating a comprehensive retraining process. Model editing refers to the modification of a model’s knowledge or representations in a manner that produces the desired outcomes. Prior research primarily centered around editing factual data e.g. “Messi plays for Inter Miami” confining the definition of an edit to a knowledge triplet i.e. (subject, object, relation). However, as the... | Afra Feyza Akyürek, Eric Pan, Garry Kuwanto, Derry Wijaya |  |
| 1309 |  |  ["Fifty Shades of Bias": Normative Ratings of Gender Bias in GPT Generated English Text](https://doi.org/10.18653/v1/2023.emnlp-main.115) |  | 0 | Language serves as a powerful tool for the manifestation of societal belief systems. In doing so, it also perpetuates the prevalent biases in our society. Gender bias is one of the most pervasive biases in our society and is seen in online and offline discourses. With LLMs increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative. Prior work often treats gender bias as a binary classification task. However,... | Rishav Hada, Agrima Seth, Harshita Diddee, Kalika Bali |  |
| 1310 |  |  [Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.116) |  | 0 | Inverted file structure is a common technique for accelerating dense retrieval. It clusters documents based on their embeddings; during searching, it probes nearby clusters w.r.t. an input query and only evaluates documents within them by subsequent codecs, thus avoiding the expensive cost from exhaustive traversal. However, the clustering is always lossy, which results in the miss of relevant documents in the probed clusters and hence degrades retrieval quality. In contrast, lexical matching,... | Peitian Zhang, Zheng Liu, Shitao Xiao, Zhicheng Dou, Jing Yao |  |
| 1311 |  |  [ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness](https://doi.org/10.18653/v1/2023.emnlp-main.117) |  | 0 | The emergence of generative large language models (LLMs) raises the question: what will be its impact on crowdsourcing? Traditionally, crowdsourcing has been used for acquiring solutions to a wide variety of human-intelligence tasks, including ones involving text generation, modification or evaluation. For some of these tasks, models like ChatGPT can potentially substitute human workers. In this study, we investigate whether this is the case for the task of paraphrase generation for intent... | Ján Cegin, Jakub Simko, Peter Brusilovsky |  |
| 1312 |  |  [Query-as-context Pre-training for Dense Passage Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.118) |  | 0 | Recently, methods have been developed to improve the performance of dense passage retrieval by using context-supervised pre-training. These methods simply consider two passages from the same document to be relevant, without taking into account the potential negative impacts of weakly correlated pairs. Thus, this paper proposes query-as-context pre-training, a simple yet effective pre-training technique to alleviate the issue. Query-as-context pre-training assumes that the query derived from a... | Xing Wu, Guangyuan Ma, Wanhui Qian, Zijia Lin, Songlin Hu |  |
| 1313 |  |  [A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.119) |  | 0 | Webpages have been a rich, scalable resource for vision-language and language only tasks. Yet only pieces of webpages are kept in existing datasets: image-caption pairs, long text articles, or raw HTML, never all in one place. Webpage tasks have resultingly received little attention and structured image-text data left underused. To study multimodal webpage understanding, we introduce the Wikipedia Webpage suite (WikiWeb2M) containing 2M pages with all of the associated image, text, and... | Andrea Burns, Krishna Srinivasan, Joshua Ainslie, Geoff Brown, Bryan A. Plummer, Kate Saenko, Jianmo Ni, Mandy Guo |  |
| 1314 |  |  [Democratizing Reasoning Ability: Tailored Learning from Large Language Model](https://doi.org/10.18653/v1/2023.emnlp-main.120) |  | 0 | Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature. Recent research on advancing open-source smaller LMs by distilling knowledge from black-box LLMs has obtained promising results in the instruction-following ability. However, the reasoning ability which is more challenging to foster, is relatively rarely explored. In this paper, we propose a... | Zhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang |  |
| 1315 |  |  [OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.121) |  | 0 | The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspect-based summarization setting targeted in this paper. Previous datasets and studies for this setting have predominantly concentrated on a limited set of pre-defined aspects, focused solely on single document inputs, or relied on... | Shmuel Amar, Liat Schiff, Ori Ernst, Asi Shefer, Ori Shapira, Ido Dagan |  |
| 1316 |  |  [PEFTDebias : Capturing debiasing information using PEFTs](https://doi.org/10.18653/v1/2023.emnlp-main.122) |  | 0 | The increasing use of foundation models highlights the urgent need to address and eliminate implicit biases present in them that arise during pretraining. In this paper, we introduce PEFTDebias, a novel approach that employs parameter-efficient fine-tuning (PEFT) to mitigate the biases within foundation models. PEFTDebias consists of two main phases: an upstream phase for acquiring debiasing parameters along a specific bias axis, and a downstream phase where these parameters are incorporated... | Sumit Agarwal, Aditya Srikanth Veerubhotla, Srijan Bansal |  |
| 1317 |  |  [Byte Pair Encoding for Symbolic Music](https://doi.org/10.18653/v1/2023.emnlp-main.123) |  | 0 | When used with deep learning, the symbolic music modality is often coupled with language model architectures. To do so, the music needs to be tokenized, i.e. converted into a sequence of discrete tokens. This can be achieved by different approaches, as music can be composed of simultaneous tracks, of simultaneous notes with several attributes. Until now, the proposed tokenizations rely on small vocabularies of tokens describing the note attributes and time events, resulting in fairly long token... | Nathan Fradet, Nicolas Gutowski, Fabien Chhel, JeanPierre Briot |  |
| 1318 |  |  [Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models](https://doi.org/10.18653/v1/2023.emnlp-main.124) |  | 0 | Recently, using large pre-trained Transformer models for transfer learning tasks has evolved to the point where they have become one of the flagship trends in the Natural Language Processing (NLP) community, giving rise to various outlooks such as prompt-based, adapters, or combinations with unsupervised approaches, among many others. In this work, we propose a 3-Phase technique to adjust a base model for a classification task. First, we adapt the model’s signal to the data distribution by... | Alejo LopezAvila, Víctor SuárezPaniagua |  |
| 1319 |  |  [Self-Influence Guided Data Reweighting for Language Model Pre-training](https://doi.org/10.18653/v1/2023.emnlp-main.125) |  | 0 | Language Models (LMs) pre-trained with selfsupervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice. While data reweighting has been explored in the context of... | Megh Thakkar, Tolga Bolukbasi, Sriram Ganapathy, Shikhar Vashishth, Sarath Chandar, Partha Talukdar |  |
| 1320 |  |  [ACTOR: Active Learning with Annotator-specific Classification Heads to Embrace Human Label Variation](https://doi.org/10.18653/v1/2023.emnlp-main.126) |  | 0 | Label aggregation such as majority voting is commonly used to resolve annotator disagreement in dataset creation. However, this may disregard minority values and opinions. Recent studies indicate that learning from individual annotations outperforms learning from aggregated labels, though they require a considerable amount of annotation. Active learning, as an annotation cost-saving strategy, has not been fully explored in the context of learning from disagreement. We show that in the active... | Xinpeng Wang, Barbara Plank |  |
| 1321 |  |  [TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.127) |  | 0 | Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently... | Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, Idan Szpektor |  |
| 1322 |  |  [VivesDebate-Speech: A Corpus of Spoken Argumentation to Leverage Audio Features for Argument Mining](https://doi.org/10.18653/v1/2023.emnlp-main.128) |  | 0 | In this paper, we describe VivesDebate-Speech, a corpus of spoken argumentation created to leverage audio features for argument mining tasks. The creation of this corpus represents an important contribution to the intersection of speech processing and argument mining communities, and one of the most complete publicly available resources in this topic. Moreover, we have performed a set of first-of-their-kind experiments which show an improvement when integrating audio features into the argument... | Ramon RuizDolz, Javier Sanchez |  |
| 1323 |  |  [Tagging-Assisted Generation Model with Encoder and Decoder Supervision for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.129) |  | 0 | ASTE (Aspect Sentiment Triplet Extraction) has gained increasing attention. Recent advancements in the ASTE task have been primarily driven by Natural Language Generation-based (NLG) approaches. However, most NLG methods overlook the supervision of the encoder-decoder hidden representations and fail to fully utilize the semantic information provided by the labels to enhance supervision. These limitations can hinder the extraction of implicit aspects and opinions. To address these challenges, we... | Xianlong Luo, Meng Yang, Yihao Wang |  |
| 1324 |  |  [Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.130) |  | 0 | Language model probing is often used to test specific capabilities of models. However, conclusions from such studies may be limited when the probing benchmarks are small and lack statistical power. In this work, we introduce new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500) inspired by psycholinguistic studies. We dramatically extend existing NEG-136 and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44 sentence pairs to 750 each. We also create... | Namrata Shivagunde, Vladislav Lialin, Anna Rumshisky |  |
| 1325 |  |  [Norm of Word Embedding Encodes Information Gain](https://doi.org/10.18653/v1/2023.emnlp-main.131) |  | 0 | Distributed representations of words encode lexical semantic information, but what type of information is encoded and how? Focusing on the skip-gram with negative-sampling method, we found that the squared norm of static word embedding encodes the information gain conveyed by the word; the information gain is defined by the Kullback-Leibler divergence of the co-occurrence distribution of the word to the unigram distribution. Our findings are explained by the theoretical framework of the... | Momose Oyama, Sho Yokoi, Hidetoshi Shimodaira |  |
| 1326 |  |  [CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data](https://doi.org/10.18653/v1/2023.emnlp-main.132) |  | 0 | Large language models (LLMs) show powerful reasoning abilities on various text-based tasks. However, their reasoning capability on structured data such as tables has not been systematically explored. In this work, we first establish a comprehensive taxonomy of reasoning and operation types for tabular data analysis. Then, we construct a complex reasoning QA dataset over tabular data, named CRT-QA dataset (Complex Reasoning QA over Tabular data), with the following unique features: (1) it is the... | Zhehao Zhang, Xitao Li, Yan Gao, JianGuang Lou |  |
| 1327 |  |  [Promoting Topic Coherence and Inter-Document Consorts in Multi-Document Summarization via Simplicial Complex and Sheaf Graph](https://doi.org/10.18653/v1/2023.emnlp-main.133) |  | 0 | Multi-document Summarization (MDS) characterizes compressing information from multiple source documents to its succinct summary. An ideal summary should encompass all topics and accurately model cross-document relations expounded upon in the source documents. However, existing systems either impose constraints on the length of tokens during the encoding or falter in capturing the intricate cross-document relationships. These limitations impel the systems to produce summaries that are... | Yash Kumar Atri, Arun Iyer, Tanmoy Chakraborty, Vikram Goyal |  |
| 1328 |  |  [MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations](https://doi.org/10.18653/v1/2023.emnlp-main.134) |  | 0 | Humans possess a remarkable ability to assign novel interpretations to linguistic expressions, enabling them to learn new words and understand community-specific connotations. However, Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly. Therefore, it is crucial for LLMs to learn novel interpretations in-context. In this paper, we systematically analyse the ability of LLMs to acquire novel interpretations using in-context learning. To facilitate our study,... | Arkil Patel, Satwik Bhattamishra, Siva Reddy, Dzmitry Bahdanau |  |
| 1329 |  |  [Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency](https://doi.org/10.18653/v1/2023.emnlp-main.135) |  | 0 | Developing an educational test can be expensive and time-consuming, as each item must be written by experts and then evaluated by collecting hundreds of student responses. Moreover, many tests require multiple distinct sets of questions administered throughout the school year to closely monitor students’ progress, known as parallel tests. In this study, we focus on tests of silent sentence reading efficiency, used to assess students’ reading ability over time. To generate high-quality parallel... | Eric Zelikman, Wanjing Anya Ma, Jasmine E. Tran, Diyi Yang, Jason D. Yeatman, Nick Haber |  |
| 1330 |  |  [Counter Turing Test (CT2): AI-Generated Text Detection is Not as Easy as You May Think - Introducing AI Detectability Index (ADI)](https://doi.org/10.18653/v1/2023.emnlp-main.136) |  | 0 | With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. This triggered a series of events, including an open letter, signed by thousands of researchers and tech leaders in March 2023, demanding a six-month moratorium on the training of AI systems more sophisticated than GPT-4. To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office released a statement stating that “if the content is... | Megha Chakraborty, S. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman, Shreya Gautam, Tanay Kumar, Krish Sharma, Niyar R. Barman, Chandan Gupta, Vinija Jain, Aman Chadha, Amit P. Sheth, Amitava Das |  |
| 1331 |  |  [Revisiting the Optimality of Word Lengths](https://doi.org/10.18653/v1/2023.emnlp-main.137) |  | 0 | Zipf (1935) posited that wordforms are optimized to minimize utterances’ communicative costs. Under the assumption that cost is given by an utterance’s length, he supported this claim by showing that words’ lengths are inversely correlated with their frequencies. Communicative cost, however, can be operationalized in different ways. Piantadosi et al. (2011) claim that cost should be measured as the distance between an utterance’s information rate and channel capacity, which we dub the channel... | Tiago Pimentel, Clara Meister, Ethan Wilcox, Kyle Mahowald, Ryan Cotterell |  |
| 1332 |  |  [Document-level Relationship Extraction by Bidirectional Constraints of Beta Rules](https://doi.org/10.18653/v1/2023.emnlp-main.138) |  | 0 | Document-level Relation Extraction (DocRE) aims to extract relations among entity pairs in documents. Some works introduce logic constraints into DocRE, addressing the issues of opacity and weak logic in original DocRE models. However, they only focus on forward logic constraints and the rules mined in these works often suffer from pseudo rules with high standard-confidence but low support. In this paper, we proposes Bidirectional Constraints of Beta Rules(BCBR), a novel logic constraint... | Yichun Liu, Zizhong Zhu, Xiaowang Zhang, Zhiyong Feng, Daoqi Chen, Yaxin Li |  |
| 1333 |  |  [Instructed Language Models with Retrievers Are Powerful Entity Linkers](https://doi.org/10.18653/v1/2023.emnlp-main.139) |  | 0 | Generative approaches powered by large language models (LLMs) have demonstrated emergent abilities in tasks that require complex reasoning abilities. Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base. We present Instructed Generative Entity Linker (INSGENEL), the first approach that enables casual language models to perform... | Zilin Xiao, Ming Gong, Jie Wu, Xingyao Zhang, Linjun Shou, Daxin Jiang |  |
| 1334 |  |  [Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text](https://doi.org/10.18653/v1/2023.emnlp-main.140) |  | 0 | Linguistic communication is prevalent in Human-Computer Interaction (HCI). Speech (spoken language) serves as a convenient yet potentially ambiguous form due to noise and accents, exposing a gap compared to text. In this study, we investigate the prominent HCI task, Referring Video Object Segmentation (R-VOS), which aims to segment and track objects using linguistic references. While text input is well-investigated, speech input is under-explored. Our objective is to bridge the gap between... | Xiang Li, Jinglu Wang, Xiaohao Xu, Muqiao Yang, Fan Yang, Yizhou Zhao, Rita Singh, Bhiksha Raj |  |
| 1335 |  |  [PROSE: A Pronoun Omission Solution for Chinese-English Spoken Language Translation](https://doi.org/10.18653/v1/2023.emnlp-main.141) |  | 0 | Neural Machine Translation (NMT) systems encounter a significant challenge when translating a pro-drop (‘pronoun-dropping’) language (e.g., Chinese) to a non-pro-drop one (e.g., English), since the pro-drop phenomenon demands NMT systems to recover omitted pronouns. This unique and crucial task, however, lacks sufficient datasets for benchmarking. To bridge this gap, we introduce PROSE, a new benchmark featured in diverse pro-drop instances for document-level Chinese-English spoken language... | Ke Wang, Xiutian Zhao, Yanghui Li, Wei Peng |  |
| 1336 |  |  [A Diachronic Analysis of Paradigm Shifts in NLP Research: When, How, and Why?](https://doi.org/10.18653/v1/2023.emnlp-main.142) |  | 0 | Understanding the fundamental concepts and trends in a scientific field is crucial for keeping abreast of its continuous advancement. In this study, we propose a systematic framework for analyzing the evolution of research topics in a scientific field using causal discovery and inference techniques. We define three variables to encompass diverse facets of the evolution of research topics within NLP and utilize a causal discovery algorithm to unveil the causal connections among these variables... | Aniket Pramanick, Yufang Hou, Saif M. Mohammad, Iryna Gurevych |  |
| 1337 |  |  [Does the Correctness of Factual Knowledge Matter for Factual Knowledge-Enhanced Pre-trained Language Models?](https://doi.org/10.18653/v1/2023.emnlp-main.143) |  | 0 | In recent years, the injection of factual knowledge has been observed to have a significant positive correlation to the downstream task performance of pre-trained language models. However, existing work neither demonstrates that pre-trained models successfully learn the injected factual knowledge nor proves that there is a causal relation between injected factual knowledge and downstream performance improvements. In this paper, we introduce a counterfactual-based analysis framework to explore... | Boxi Cao, Qiaoyu Tang, Hongyu Lin, Xianpei Han, Le Sun |  |
| 1338 |  |  [Syntactic Substitutability as Unsupervised Dependency Syntax](https://doi.org/10.18653/v1/2023.emnlp-main.144) |  | 0 | Syntax is a latent hierarchical structure which underpins the robust and compositional nature of human language. In this work, we explore the hypothesis that syntactic dependencies can be represented in language model attention distributions and propose a new method to induce these structures theory-agnostically. Instead of modeling syntactic relations as defined by annotation schemata, we model a more general property implicit in the definition of dependency relations, syntactic... | Jasper Jian, Siva Reddy |  |
| 1339 |  |  [MProto: Multi-Prototype Network with Denoised Optimal Transport for Distantly Supervised Named Entity Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.145) |  | 0 | Distantly supervised named entity recognition (DS-NER) aims to locate entity mentions and classify their types with only knowledge bases or gazetteers and unlabeled corpus. However, distant annotations are noisy and degrade the performance of NER models. In this paper, we propose a noise-robust prototype network named MProto for the DS-NER task. Different from previous prototype-based NER methods, MProto represents each entity type with multiple prototypes to characterize the intra-class... | Shuhui Wu, Yongliang Shen, Zeqi Tan, Wenqi Ren, Jietian Guo, Shiliang Pu, Weiming Lu |  |
| 1340 |  |  [The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions](https://doi.org/10.18653/v1/2023.emnlp-main.146) |  | 0 | Recent progress in Large Language Models (LLMs) has produced models that exhibit remarkable performance across a variety of NLP tasks. However, it remains unclear whether the existing focus of NLP research accurately captures the genuine requirements of human users. This paper provides a comprehensive analysis of the divergence between academic research in NLP and the needs of real-world NLP applications via a large-scale collection of user-GPT conversations. We analyze a large-scale collection... | Siru Ouyang, Shuohang Wang, Yang Liu, Ming Zhong, Yizhu Jiao, Dan Iter, Reid Pryzant, Chenguang Zhu, Heng Ji, Jiawei Han |  |
| 1341 |  |  [Learning the Visualness of Text Using Large Vision-Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.147) |  | 0 | Visual text evokes an image in a person’s mind, while non-visual text fails to do so. A method to automatically detect visualness in text will enable text-to-image retrieval and generation models to augment text with relevant images. This is particularly challenging with long-form text as text-to-image generation and retrieval models are often triggered for text that is designed to be explicitly visual in nature, whereas long-form text could contain many non-visual sentences. To this end, we... | Gaurav Verma, Ryan A. Rossi, Christopher Tensmeyer, Jiuxiang Gu, Ani Nenkova |  |
| 1342 |  |  [The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values](https://doi.org/10.18653/v1/2023.emnlp-main.148) |  | 0 | Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories. First, we summarise the past, pre-LLM trends for integrating human feedback into... | Hannah Kirk, Andrew M. Bean, Bertie Vidgen, Paul Röttger, Scott Hale |  |
| 1343 |  |  [TempTabQA: Temporal Question Answering for Semi-Structured Tables](https://doi.org/10.18653/v1/2023.emnlp-main.149) |  | 0 | Semi-structured data, such as Infobox tables, often include temporal information about entities, either implicitly or explicitly. Can current NLP systems reason about such information in semi-structured tables? To tackle this question, we introduce the task of temporal question answering on semi-structured tables. We present a dataset, TEMPTABQA, which comprises 11,454 question-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning more than 90 distinct domains. Using this dataset,... | Vivek Gupta, Pranshu Kandoi, Mahek Bhavesh Vora, Shuo Zhang, Yujie He, Ridho Reinanda, Vivek Srikumar |  |
| 1344 |  |  [Task-Level Thinking Steps Help Large Language Models for Challenging Classification Task](https://doi.org/10.18653/v1/2023.emnlp-main.150) |  | 0 | Large language models (LLMs) have shown incredible performance on many tasks such as dialogue generation, commonsense reasoning and question answering. In-context learning (ICL) is an important paradigm for adapting LLMs to the downstream tasks by prompting few demonstrations. However, the distribution of demonstrations can severely affect the performance, especially for challenging classification tasks. In this paper, we propose the concept of task-level thinking steps that can eliminate bias... | Chunhui Du, Jidong Tian, Haoran Liao, Jindou Chen, Hao He, Yaohui Jin |  |
| 1345 |  |  [RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation](https://doi.org/10.18653/v1/2023.emnlp-main.151) |  | 0 | The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an... | Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, JianGuang Lou, Weizhu Chen |  |
| 1346 |  |  [Influence Scores at Scale for Efficient Language Data Sampling](https://doi.org/10.18653/v1/2023.emnlp-main.152) |  | 0 | Modern ML systems ingest data aggregated from diverse sources, such as synthetic, human-annotated, and live customer traffic. Understanding which examples are important to the performance of a learning algorithm is crucial for efficient model training. Recently, a growing body of literature has given rise to various “influence scores,” which use training artifacts such as model confidence or checkpointed gradients to identify important subsets of data. However, these methods have primarily been... | Nikhil Anand, Joshua Tan, Maria Minakova |  |
| 1347 |  |  [G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment](https://doi.org/10.18653/v1/2023.emnlp-main.153) |  | 0 | The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references.... | Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu |  |
| 1348 |  |  [Learning Retrieval Augmentation for Personalized Dialogue Generation](https://doi.org/10.18653/v1/2023.emnlp-main.154) |  | 0 | Personalized dialogue generation, focusing on generating highly tailored responses by leveraging persona profiles and dialogue context, has gained significant attention in conversational AI applications. However, persona profiles, a prevalent setting in current personalized dialogue datasets, typically composed of merely four to five sentences, may not offer comprehensive descriptions of the persona about the agent, posing a challenge to generate truly personalized dialogues. To handle this... | Qiushi Huang, Shuai Fu, Xubo Liu, Wenwu Wang, Tom Ko, Yu Zhang, Lilian Tang |  |
| 1349 |  |  [The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations](https://doi.org/10.18653/v1/2023.emnlp-main.155) |  | 0 | The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a... | Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S. M. Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das |  |
| 1350 |  |  [NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders](https://doi.org/10.18653/v1/2023.emnlp-main.156) |  | 0 | Neural document rerankers are extremely effective in terms of accuracy. However, the best models require dedicated hardware for serving, which is costly and often not feasible. To avoid this servingtime requirement, we present a method of capturing up to 86% of the gains of a Transformer cross-attention model with a lexicalized scoring function that only requires 10-6% of the Transformer’s FLOPs per document and can be served using commodity CPUs. When combined with a BM25 retriever, this... | Livio Soares, Daniel Gillick, Jeremy R. Cole, Tom Kwiatkowski |  |
| 1351 |  |  [Analyzing Modular Approaches for Visual Question Decomposition](https://doi.org/10.18653/v1/2023.emnlp-main.157) |  | 0 | Modular neural networks without additional training have recently been shown to surpass end-to-end neural networks on challenging vision–language tasks. The latest such methods simultaneously introduce LLM-based code generation to build programs and a number of skill-specific, task-oriented modules to execute them. In this paper, we focus on ViperGPT and ask where its additional performance comes from and how much is due to the (state-of-art, end-to-end) BLIP-2 model it subsumes vs. additional... | Apoorv Khandelwal, Ellie Pavlick, Chen Sun |  |
| 1352 |  |  [Improving Summarization with Human Edits](https://doi.org/10.18653/v1/2023.emnlp-main.158) |  | 0 | Recent work has shown the promise of learning with human feedback paradigms to produce human-determined high-quality text. Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training. In this paper, we focus on a less explored form of human feedback – Human Edits. We propose Sequence Alignment (un)Likelihood Training (SALT), a novel technique to use both the... | Zonghai Yao, Benjamin J. Schloss, Sai P. Selvaraj |  |
| 1353 |  |  [Did You Mean...? Confidence-based Trade-offs in Semantic Parsing](https://doi.org/10.18653/v1/2023.emnlp-main.159) |  | 0 | We illustrate how a calibrated model can help balance common trade-offs in task-oriented parsing. In a simulated annotator-in-the-loop experiment, we show that well-calibrated confidence scores allow us to balance cost with annotator load, improving accuracy with a small number of interactions. We then examine how confidence scores can help optimize the trade-off between usability and safety. We show that confidence-based thresholding can substantially reduce the number of incorrect... | Elias StengelEskin, Benjamin Van Durme |  |
| 1354 |  |  [The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages](https://doi.org/10.18653/v1/2023.emnlp-main.160) |  | 0 | Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate remarkable performance in a wide range of tasks. Despite numerous recent studies that examine the performance of instruction-tuned LLMs on various NLP benchmarks, there remains a lack of comprehensive investigation into their ability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning embedded within social and interactive contexts. This deficiency arises partly from SM not being adequately... | Chiyu Zhang, Khai Duy Doan, Qisheng Liao, Muhammad AbdulMageed |  |
| 1355 |  |  [Understanding the Effect of Model Compression on Social Bias in Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.161) |  | 0 | Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model’s predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs.... | Gustavo Gonçalves, Emma Strubell |  |
| 1356 |  |  [BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology](https://doi.org/10.18653/v1/2023.emnlp-main.162) |  | 0 | The ability to automatically generate accurate protocols for scientific experiments would represent a major step towards the automation of science. Large Language Models (LLMs) have impressive capabilities on a wide range of tasks, such as question answering and the generation of coherent text and code. However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments. Moreover, evaluation of the accuracy of scientific protocols... | Odhran O'Donoghue, Aleksandar Shtedritski, John Ginger, Ralph Abboud, Ali Essa Ghareeb, Samuel G. Rodriques |  |
| 1357 |  |  [Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages](https://doi.org/10.18653/v1/2023.emnlp-main.163) |  | 0 | Chain-of-thought (CoT) is capable of eliciting models to explicitly generate reasoning paths, thus promoting reasoning accuracy and attracting increasing attention. Specifically, zero-shot CoT achieves remarkable improvements in a wide range of reasoning tasks by simply instructing the LLM with the prompt “Let’s think step by step!”. Despite the success of zero-shot CoT, the existing zero-shot prompting techniques remain limited to a single language, making it challenging to generalize to other... | Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, Wanxiang Che |  |
| 1358 |  |  [FinGPT: Large Generative Models for a Small Language](https://doi.org/10.18653/v1/2023.emnlp-main.164) |  | 0 | Large language models (LLMs) excel in many tasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the challenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world population. We compile an extensive dataset of Finnish combining web crawls, news, social media and eBooks. We pursue two approaches to pretrain... | Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, HannaMari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, Thomas Wang, Nouamane Tazi, Teven Le Scao, Thomas Wolf, Osma Suominen, Samuli Sairanen, Mikko Merioksa, Jyrki Heinonen, Aija Vahtola, Samuel Antao, Sampo Pyysalo |  |
| 1359 |  |  [Boosting Summarization with Normalizing Flows and Aggressive Training](https://doi.org/10.18653/v1/2023.emnlp-main.165) |  | 0 | This paper presents FlowSUM, a normalizing flows-based variational encoder-decoder framework for Transformer-based summarization. Our approach tackles two primary challenges in variational summarization: insufficient semantic information in latent representations and posterior collapse during training. To address these challenges, we employ normalizing flows to enable flexible latent posterior modeling, and we propose a controlled alternate aggressive training (CAAT) strategy with an improved... | Yu Yang, Xiaotong Shen |  |
| 1360 |  |  [Indicative Summarization of Long Discussions](https://doi.org/10.18653/v1/2023.emnlp-main.166) |  | 0 | Online forums encourage the exchange and discussion of different stances on many topics. Not only do they provide an opportunity to present one’s own arguments, but may also gather a broad cross-section of others’ arguments. However, the resulting long discussions are difficult to overview. This paper presents a novel unsupervised approach using large language models (LLMs) to generating indicative summaries for long discussions that basically serve as tables of contents. Our approach first... | Shahbaz Syed, Dominik Schwabe, Khalid Al Khatib, Martin Potthast |  |
| 1361 |  |  [A Framework for Vision-Language Warm-up Tasks in Multimodal Dialogue Models](https://doi.org/10.18653/v1/2023.emnlp-main.167) |  | 0 | Most research on multimodal open-domain dialogue agents has focused on pretraining and multi-task learning using additional rich datasets beyond a given target dataset. However, methods for exploiting these additional datasets can be quite limited in real-world settings, creating a need for more efficient methods for constructing agents based solely on the target dataset. To address these issues, we present a new learning strategy called vision-language warm-up tasks for multimodal dialogue... | Jaewook Lee, Seongsik Park, SeongHeum Park, Hongjin Kim, Harksoo Kim |  |
| 1362 |  |  [Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair Modeling](https://doi.org/10.18653/v1/2023.emnlp-main.168) |  | 0 | Transformer-based models have achieved great success on sentence pair modeling tasks, such as answer selection and natural language inference (NLI). These models generally perform cross-attention over input pairs, leading to prohibitive computational cost. Recent studies propose dual-encoder and late interaction architectures for faster computation. However, the balance between the expressive of cross-attention and computation speedup still needs better coordinated. To this end, this paper... | Yuanhang Yang, Shiyi Qi, Chuanyi Liu, Qifan Wang, Cuiyun Gao, Zenglin Xu |  |
| 1363 |  |  [Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts](https://doi.org/10.18653/v1/2023.emnlp-main.169) |  | 0 | As large language models (LLMs) have shown effectiveness with different prompting methods, such as Chain of Thought, Program of Thought, we find that these methods have formed a great complementarity to each other on math reasoning tasks. In this work, we propose XoT, an integrated problem solving framework by prompting LLMs with diverse reasoning thoughts. For each question, XoT always begins with selecting the most suitable method then executes each method iteratively. Within each iteration,... | Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang |  |
| 1364 |  |  [GLEN: General-Purpose Event Detection for Thousands of Types](https://doi.org/10.18653/v1/2023.emnlp-main.170) |  | 0 | The progress of event extraction research has been hindered by the absence of wide-coverage, large-scale datasets. To make event extraction systems more accessible, we build a general-purpose event detection dataset GLEN, which covers 205K event mentions with 3,465 different types, making it more than 20x larger in ontology than today’s largest event dataset. GLEN is created by utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and PropBank rolesets. This enables us to... | Sha Li, Qiusi Zhan, Kathryn Conger, Martha Palmer, Heng Ji, Jiawei Han |  |
| 1365 |  |  [Hierarchical Pretraining on Multimodal Electronic Health Records](https://doi.org/10.18653/v1/2023.emnlp-main.171) |  | 0 | Pretraining has proven to be a powerful technique in natural language processing (NLP), exhibiting remarkable success in various NLP downstream tasks. However, in the medical domain, existing pretrained models on electronic health records (EHR) fail to capture the hierarchical nature of EHR data, limiting their generalization capability across diverse downstream tasks using a single pretrained model. To tackle this challenge, this paper introduces a novel, general, and unified pretraining... | Xiaochen Wang, Junyu Luo, Jiaqi Wang, Ziyi Yin, Suhan Cui, Yuan Zhong, Yaqing Wang, Fenglong Ma |  |
| 1366 |  |  [Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.172) |  | 0 | Hallucination of text ungrounded in the input is a well-known problem in neural data-to-text generation. Many methods have been proposed to mitigate it, but they typically require altering model architecture or collecting additional data, and thus cannot be easily applied to an existing model. In this paper, we explore a new way to mitigate hallucinations by combining the probabilistic output of a generator language model (LM) with the output of a special “text critic” classifier, which guides... | Mateusz Lango, Ondrej Dusek |  |
| 1367 |  |  [Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.173) |  | 0 | Multimodal machine translation (MMT) simultaneously takes the source sentence and a relevant image as input for translation. Since there is no paired image available for the input sentence in most cases, recent studies suggest utilizing powerful text-to-image generation models to provide image inputs. Nevertheless, synthetic images generated by these models often follow different distributions compared to authentic images. Consequently, using authentic images for training and synthetic images... | Wenyu Guo, Qingkai Fang, Dong Yu, Yang Feng |  |
| 1368 |  |  [DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.174) |  | 0 | Pretrained language models have learned a vast amount of human knowledge from large-scale corpora, but their powerful memorization capability also brings the risk of data leakage. Some risks may only be discovered after the model training is completed, such as the model memorizing a specific phone number and frequently outputting it. In such cases, model developers need to eliminate specific data influences from the model to mitigate legal and ethical penalties. To effectively mitigate these... | Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, Deyi Xiong |  |
| 1369 |  |  [Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques](https://doi.org/10.18653/v1/2023.emnlp-main.175) |  | 0 | This paper investigates the transferability of debiasing techniques across different languages within multilingual models. We examine the applicability of these techniques in English, French, German, and Dutch. Using multilingual BERT (mBERT), we demonstrate that cross-lingual transfer of debiasing techniques is not only feasible but also yields promising results. Surprisingly, our findings reveal no performance disadvantages when applying these techniques to non-English languages. Using... | Manon Reusens, Philipp Borchert, Margot Mieskes, Jochen De Weerdt, Bart Baesens |  |
| 1370 |  |  [Can Language Models Laugh at YouTube Short-form Videos?](https://doi.org/10.18653/v1/2023.emnlp-main.176) |  | 0 | As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans. Unfortunately, previous video humor datasets target specific domains such as speeches or sitcoms, and mostly focus on verbal cues. We curate a user-generated dataset of 10K multimodal funny videos from YouTube, called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both verbal and visual elements contributing to... | Dayoon Ko, Sangho Lee, Gunhee Kim |  |
| 1371 |  |  [Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation](https://doi.org/10.18653/v1/2023.emnlp-main.177) |  | 0 | Representation Learning on Knowledge Graphs (KGs) is essential for downstream tasks. The dominant approach, KG Embedding (KGE), represents entities with independent vectors and faces the scalability challenge. Recent studies propose an alternative way for parameter efficiency, which represents entities by composing entity-corresponding codewords matched from predefined small-scale codebooks. We refer to the process of obtaining corresponding codewords of each entity as entity quantization, for... | Jiaang Li, Quan Wang, Yi Liu, Licheng Zhang, Zhendong Mao |  |
| 1372 |  |  [Exploring All-In-One Knowledge Distillation Framework for Neural Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.178) |  | 0 | Conventional knowledge distillation(KD) approaches are commonly employed to compress neural machine translation(NMT) models. However, they only obtain one lightweight student each time. Consequently, we have to conduct KD multiple times when different students are required at the same time, which could be resource-intensive. Additionally, these students are individually optimized, and thus lack interactions with each other, leading to their potential not being fully exerted. In this work, we... | Zhongjian Miao, Wen Zhang, Jinsong Su, Xiang Li, Jian Luan, Yidong Chen, Bin Wang, Min Zhang |  |
| 1373 |  |  [HistAlign: Improving Context Dependency in Language Generation by Aligning with History](https://doi.org/10.18653/v1/2023.emnlp-main.179) |  | 0 | Language models (LMs) can generate hallucinations and incoherent outputs, which highlights their weak context dependency. Cache-LMs, which augment LMs with a memory of recent history, can increase context dependency and have shown remarkable performance in diverse language generation tasks. However, we find that even with training, the performance gain stemming from the cache component of current cache-LMs is suboptimal due to the misalignment between the current hidden states and those stored... | David Wan, Shiyue Zhang, Mohit Bansal |  |
| 1374 |  |  [CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models](https://doi.org/10.18653/v1/2023.emnlp-main.180) |  | 0 | Methods for adapting language models (LMs) to new tasks and domains have traditionally assumed white-box access to the model, and work by modifying its parameters. However, this is incompatible with a recent trend in the field, where the highest quality models are only available as black-boxes through inference APIs. Even when the model weights are available, the computational cost of fine-tuning large LMs can be prohibitive for most practitioners. In this work, we present a lightweight method... | Aitor Ormazabal, Mikel Artetxe, Eneko Agirre |  |
| 1375 |  |  [Image Manipulation via Multi-Hop Instructions - A New Dataset and Weakly-Supervised Neuro-Symbolic Approach](https://doi.org/10.18653/v1/2023.emnlp-main.181) |  | 0 | We are interested in image manipulation via natural language text – a task that is useful for multiple AI applications but requires complex reasoning over multi-modal spaces. We extend recently proposed Neuro Symbolic Concept Learning (NSCL), which has been quite effective for the task of Visual Question Answering (VQA), for the task of image manipulation. Our system referred to as NeuroSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in... | Harman Singh, Poorva Garg, Mohit Gupta, Kevin Shah, Ashish Goswami, Satyam Modi, Arnab Kumar Mondal, Dinesh Khandelwal, Dinesh Garg, Parag Singla |  |
| 1376 |  |  [Generative Spoken Language Model based on continuous word-sized audio tokens](https://doi.org/10.18653/v1/2023.emnlp-main.182) |  | 0 | In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio tokens that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical... | Robin Algayres, Yossi Adi, Tu Anh Nguyen, Jade Copet, Gabriel Synnaeve, Benoît Sagot, Emmanuel Dupoux |  |
| 1377 |  |  [Enhancing Chat Language Models by Scaling High-quality Instructional Conversations](https://doi.org/10.18653/v1/2023.emnlp-main.183) |  | 0 | Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to push the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human... | Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, Bowen Zhou |  |
| 1378 |  |  [Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining](https://doi.org/10.18653/v1/2023.emnlp-main.184) |  | 0 | Recent work in vision-and-language pretraining has investigated supervised signals from object detection data to learn better, fine-grained multimodal representations. In this work, we take a step further and explore how we can tap into supervision from small-scale visual relation data. In particular, we propose two pretraining approaches to contextualise visual entities in a multimodal setup. With verbalised scene graphs, we transform visual relation triplets into structured captions, and... | Emanuele Bugliarello, Aida Nematzadeh, Lisa Anne Hendricks |  |
| 1379 |  |  [Unsupervised Grammatical Error Correction Rivaling Supervised Methods](https://doi.org/10.18653/v1/2023.emnlp-main.185) |  | 0 | State-of-the-art grammatical error correction (GEC) systems rely on parallel training data (ungrammatical sentences and their manually corrected counterparts), which are expensive to construct. In this paper, we employ the Break-It-Fix-It (BIFI) method to build an unsupervised GEC system. The BIFI framework generates parallel data from unlabeled text using a fixer to transform ungrammatical sentences into grammatical ones, and a critic to predict sentence grammaticality. We present an... | Hannan Cao, Liping Yuan, Yuchen Zhang, Hwee Tou Ng |  |
| 1380 |  |  [S2abEL: A Dataset for Entity Linking from Scientific Tables](https://doi.org/10.18653/v1/2023.emnlp-main.186) |  | 0 | Entity linking (EL) is the task of linking a textual mention to its corresponding entry in a knowledge base, and is critical for many knowledge-intensive NLP applications. When applied to tables in scientific papers, EL is a step toward large-scale scientific knowledge bases that could enable advanced scientific question answering and analytics. We present the first dataset for EL in scientific tables. EL for scientific tables is especially challenging because scientific knowledge bases can be... | Yuze Lou, Bailey Kuehl, Erin Bransom, Sergey Feldman, Aakanksha Naik, Doug Downey |  |
| 1381 |  |  [API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.187) |  | 0 | Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs’ ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we... | Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li |  |
| 1382 |  |  [Language and Mental Health: Measures of Emotion Dynamics from Text as Linguistic Biosocial Markers](https://doi.org/10.18653/v1/2023.emnlp-main.188) |  | 0 | Research in psychopathology has shown that, at an aggregate level, the patterns of emotional change over time—emotion dynamics—are indicators of one’s mental health. One’s patterns of emotion change have traditionally been determined through self-reports of emotions; however, there are known issues with accuracy, bias, and convenience. Recent approaches to determining emotion dynamics from one’s everyday utterances, addresses many of these concerns, but it is not yet known whether these... | Daniela Teodorescu, Tiffany Cheng, Alona Fyshe, Saif M. Mohammad |  |
| 1383 |  |  [Lion: Adversarial Distillation of Proprietary Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.189) |  | 0 | The practice of transferring knowledge from a sophisticated, proprietary large language model (LLM) to a compact, open-source LLM has garnered considerable attention. Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher model to a set of instructions. Nevertheless, they overlooked the possibility of incorporating any “feedback”–identifying challenging instructions where the student model’s performance... | Yuxin Jiang, Chunkit Chan, Mingyang Chen, Wei Wang |  |
| 1384 |  |  [Evaluating Large Language Models on Controlled Generation Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.190) |  | 0 | While recent studies have looked into the abilities of large language models in various benchmark tasks, including question generation, reading comprehension, multilingual and etc, there have been few studies looking into the controllability of large language models on generation tasks. We present an extensive analysis of various benchmarks including a sentence planning benchmark with different granularities. After comparing large language models against state-of-the-start finetuned smaller... | Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John Frederick Wieting, Nanyun Peng, Xuezhe Ma |  |
| 1385 |  |  [DeSIQ: Towards an Unbiased, Challenging Benchmark for Social Intelligence Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.191) |  | 0 | Social intelligence is essential for understanding and reasoning about human expressions, intents and interactions. One representative benchmark for its study is Social Intelligence Queries (Social-IQ), a dataset of multiple-choice questions on videos of complex social interactions. We define a comprehensive methodology to study the soundness of Social-IQ, as the soundness of such benchmark datasets is crucial to the investigation of the underlying research problem. We define a comprehensive... | Xiaoyu Guo, YuanFang Li, Gholamreza Haffari |  |
| 1386 |  |  [Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation](https://doi.org/10.18653/v1/2023.emnlp-main.192) |  | 0 | We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence: a condition that we call evidential closure. Information about the truth or falsity of sentences is not statistically identified in the standard neural language generation setup, and so cannot be conditioned on to generate new strings. We then show how to constrain LLMs to produce output that satisfies evidential closure. A multimodal LLM must learn about the external... | Adam Bouyamourn |  |
| 1387 |  |  [A Question Answering Framework for Decontextualizing User-facing Snippets from Scientific Documents](https://doi.org/10.18653/v1/2023.emnlp-main.193) |  | 0 | Many real-world applications (e.g., note taking, search) require extracting a sentence or paragraph from a document and showing that snippet to a human outside of the source document. Yet, users may find snippets difficult to understand as they lack context from the original document. In this work, we use language models to rewrite snippets from scientific documents to be read on their own. First, we define the requirements and challenges for this user-facing decontextualization task, such as... | Benjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, Kyle Lo |  |
| 1388 |  |  [SLOG: A Structural Generalization Benchmark for Semantic Parsing](https://doi.org/10.18653/v1/2023.emnlp-main.194) |  | 0 | The goal of compositional generalization benchmarks is to evaluate how well models generalize to new complex linguistic expressions. Existing benchmarks often focus on lexical generalization, the interpretation of novel lexical items in syntactic structures familiar from training; structural generalization tasks, where a model needs to interpret syntactic structures that are themselves unfamiliar from training, are often underrepresented, resulting in overly optimistic perceptions of how well... | Bingzhi Li, Lucia Donatelli, Alexander Koller, Tal Linzen, Yuekun Yao, Najoung Kim |  |
| 1389 |  |  [Pushdown Layers: Encoding Recursive Structure in Transformer Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.195) |  | 0 | Recursion is a prominent feature of human language, and fundamentally challenging for self-attention due to the lack of an explicit recursive-state tracking mechanism. Consequently, Transformer language models poorly capture long-tail recursive structure and exhibit sample-inefficient syntactic generalization. This work introduces Pushdown Layers, a new self-attention layer that models recursive state via a stack tape that tracks estimated depths of every token in an incremental parse of the... | Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher D. Manning |  |
| 1390 |  |  [Can LLMs Facilitate Interpretation of Pre-trained Language Models?](https://doi.org/10.18653/v1/2023.emnlp-main.196) |  | 0 | Work done to uncover the knowledge encoded within pre-trained language models rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation. We propose using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis of pre-trained language models. We discover latent concepts within pre-trained language models by applying agglomerative hierarchical clustering over... | Basel Mousi, Nadir Durrani, Fahim Dalvi |  |
| 1391 |  |  [Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets](https://doi.org/10.18653/v1/2023.emnlp-main.197) |  | 0 | Named Entity Recognition (NER) frequently suffers from the problem of insufficient labeled data, particularly in fine-grained NER scenarios. Although K-shot learning techniques can be applied, their performance tends to saturate when the number of annotations exceeds several tens of labels. To overcome this problem, we utilize existing coarse-grained datasets that offer a large number of annotations. A straightforward approach to address this problem is pre-finetuning, which employs... | Su Ah Lee, Seokjin Oh, Woohwan Jung |  |
| 1392 |  |  [Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies](https://doi.org/10.18653/v1/2023.emnlp-main.198) |  | 0 | When we transfer a pretrained language model to a new language, there are many axes of variation that change at once. To disentangle the impact of different factors like syntactic similarity and vocabulary similarity, we propose a set of controlled transfer studies: we systematically transform the language of the GLUE benchmark, altering one axis of crosslingual variation at a time, and then measure the resulting drops in a pretrained model’s downstream performance. We find that models can... | Zhengxuan Wu, Alex Tamkin, Isabel Papadimitriou |  |
| 1393 |  |  [Non-Autoregressive Math Word Problem Solver with Unified Tree Structure](https://doi.org/10.18653/v1/2023.emnlp-main.199) |  | 0 | Existing MWP solvers employ sequence or binary tree to present the solution expression and decode it from given problem description. However, such structures fail to handle the variants that can be derived via mathematical manipulation, e.g., (a1+a2)\*a3 and a1 \* a3+a2 \* a3 can both be possible valid solutions for a same problem but formulated as different expression sequences or trees. The multiple solution variants depicting different possible solving procedures for the same input problem... | Yi Bin, Mengqun Han, Wenhao Shi, Lei Wang, Yang Yang, SeeKiong Ng, Heng Tao Shen |  |
| 1394 |  |  [Improving Chinese Pop Song and Hokkien Gezi Opera Singing Voice Synthesis by Enhancing Local Modeling](https://doi.org/10.18653/v1/2023.emnlp-main.200) |  | 0 | Singing Voice Synthesis (SVS) strives to synthesize pleasing vocals based on music scores and lyrics. The current acoustic models based on Transformer usually process the entire sequence globally and use a simple L1 loss. However, this approach overlooks the significance of local modeling within the sequence and the local optimization of the hard-to-synthesize parts in the predicted mel-spectrogram. Consequently, the synthesized audio exhibits local incongruities (e.g., local pronunciation... | Peng Bai, Yue Zhou, Meizhen Zheng, Wujin Sun, Xiaodong Shi |  |
| 1395 |  |  [What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on QA Systems](https://doi.org/10.18653/v1/2023.emnlp-main.201) |  | 0 | NLP systems have shown impressive performance at answering questions by retrieving relevant context. However, with the increasingly large models, it is impossible and often undesirable to constrain models’ knowledge or reasoning to only the retrieved context. This leads to a mismatch between the information that the models access to derive the answer and the information that is available to the user to assess the model predicted answer. In this work, we study how users interact with QA systems... | Navita Goyal, Eleftheria Briakou, Amanda Liu, Connor Baumler, Claire Bonial, Jeffrey Micher, Clare R. Voss, Marine Carpuat, Hal Daumé III |  |
| 1396 |  |  [GROOViST: A Metric for Grounding Objects in Visual Storytelling](https://doi.org/10.18653/v1/2023.emnlp-main.202) |  | 0 | A proper evaluation of stories generated for a sequence of images—the task commonly referred to as visual storytelling—must consider multiple aspects, such as coherence, grammatical correctness, and visual grounding. In this work, we focus on evaluating the degree of grounding, that is, the extent to which a story is about the entities shown in the images. We analyze current metrics, both designed for this purpose and for general vision-text alignment. Given their observed shortcomings, we... | Aditya K. Surikuchi, Sandro Pezzelle, Raquel Fernández |  |
| 1397 |  |  [VIBE: Topic-Driven Temporal Adaptation for Twitter Classification](https://doi.org/10.18653/v1/2023.emnlp-main.203) |  | 0 | Language features are evolving in real-world social media, resulting in the deteriorating performance of text classification in dynamics. To address this challenge, we study temporal adaptation, where models trained on past data are tested in the future. Most prior work focused on continued pretraining or knowledge updating, which may compromise their performance on noisy social media data. To tackle this issue, we reflect feature change via modeling latent topic evolution and propose a novel... | Yuji Zhang, Jing Li, Wenjie Li |  |
| 1398 |  |  [TOD-Flow: Modeling the Structure of Task-Oriented Dialogues](https://doi.org/10.18653/v1/2023.emnlp-main.204) |  | 0 | Task-Oriented Dialogue (TOD) systems have become crucial components in interactive artificial intelligence applications. While recent advances have capitalized on pre-trained language models (PLMs), they exhibit limitations regarding transparency and controllability. To address these challenges, we propose a novel approach focusing on inferring the TOD-flow graph from dialogue data annotated with dialog acts, uncovering the underlying task structure in the form of a graph. The inferred TOD-flow... | Sungryull Sohn, Yiwei Lyu, Anthony Z. Liu, Lajanugen Logeswaran, DongKi Kim, Dongsub Shim, Honglak Lee |  |
| 1399 |  |  [TopWORDS-Poetry: Simultaneous Text Segmentation and Word Discovery for Classical Chinese Poetry via Bayesian Inference](https://doi.org/10.18653/v1/2023.emnlp-main.205) |  | 0 | As a precious cultural heritage of human beings, classical Chinese poetry has a very unique writing style and often contains special words that rarely appear in general Chinese texts, posting critical challenges for natural language processing. Little effort has been made in the literature for processing texts from classical Chinese poetry. This study fills in this gap with TopWORDS-Poetry, an unsupervised method that can achieve reliable text segmentation and word discovery for classical... | Changzai Pan, Feiyue Li, Ke Deng |  |
| 1400 |  |  [Knowledge Rumination for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.206) |  | 0 | Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fails to fully utilize them when applying to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed... | Yunzhi Yao, Peng Wang, Shengyu Mao, Chuanqi Tan, Fei Huang, Huajun Chen, Ningyu Zhang |  |
| 1401 |  |  [Struct-XLM: A Structure Discovery Multilingual Language Model for Enhancing Cross-lingual Transfer through Reinforcement Learning](https://doi.org/10.18653/v1/2023.emnlp-main.207) |  | 0 | Cross-lingual transfer learning heavily relies on well-aligned cross-lingual representations. The syntactic structure is recognized as beneficial for cross-lingual transfer, but limited researches utilize it for aligning representation in multilingual pre-trained language models (PLMs). Additionally, existing methods require syntactic labels that are difficult to obtain and of poor quality for low-resource languages. To address this gap, we propose Struct-XLM, a novel multilingual language... | Linjuan Wu, Weiming Lu |  |
| 1402 |  |  [AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification](https://doi.org/10.18653/v1/2023.emnlp-main.208) |  | 0 | Recent work has found that few-shot sentence classification based on pre-trained Sentence Encoders (SEs) is efficient, robust, and effective. In this work, we investigate strategies for domain-specialization in the context of few-shot sentence classification with SEs. We first establish that unsupervised Domain-Adaptive Pre-Training (DAPT) of a base Pre-trained Language Model (PLM) (i.e., not an SE) substantially improves the accuracy of few-shot sentence classification by up to 8.4 points.... | Yongxin Huang, Kexin Wang, Sourav Dutta, Raj Nath Patel, Goran Glavas, Iryna Gurevych |  |
| 1403 |  |  [Interview Evaluation: A Novel Approach for Automatic Evaluation of Conversational Question Answering Models](https://doi.org/10.18653/v1/2023.emnlp-main.209) |  | 0 | Conversational Question Answering (CQA) aims to provide natural language answers to users in information-seeking dialogues. Existing CQA benchmarks often evaluate models using pre-collected human-human conversations. However, replacing the model-predicted dialogue history with ground truth compromises the naturalness and sustainability of CQA evaluation. While previous studies proposed using predicted history and rewriting techniques to address unresolved coreferences and incoherencies, this... | Xibo Li, Bowei Zou, Yifan Fan, Yanling Li, Ai Ti Aw, Yu Hong |  |
| 1404 |  |  [TCFLE-8: a Corpus of Learner Written Productions for French as a Foreign Language and its Application to Automated Essay Scoring](https://doi.org/10.18653/v1/2023.emnlp-main.210) |  | 0 | Automated Essay Scoring (AES) aims to automatically assess the quality of essays. Automation enables large-scale assessment, improvements in consistency, reliability, and standardization. Those characteristics are of particular relevance in the context of language certification exams. However, a major bottleneck in the development of AES systems is the availability of corpora, which, unfortunately, are scarce, especially for languages other than English. In this paper, we aim to foster the... | Rodrigo Wilkens, Alice Pintard, David Alfter, Vincent Folny, Thomas François |  |
| 1405 |  |  [Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA](https://doi.org/10.18653/v1/2023.emnlp-main.211) |  | 0 | Large language models (e.g., GPT-4) are uniquely capable of producing highly rated text simplification, yet current human evaluation methods fail to provide a clear understanding of systems’ specific strengths and weaknesses. To address this limitation, we introduce SALSA, an edit-based human annotation framework that enables holistic and fine-grained text simplification evaluation. We develop twenty one linguistically grounded edit types, covering the full spectrum of success and failure... | David Heineman, Yao Dou, Mounica Maddela, Wei Xu |  |
| 1406 |  |  [Confidence-based Ensembling of Perspective-aware Models](https://doi.org/10.18653/v1/2023.emnlp-main.212) |  | 0 | Research in the field of NLP has recently focused on the variability that people show in selecting labels when performing an annotation task. Exploiting disagreements in annotations has been shown to offer advantages for accurate modelling and fair evaluation. In this paper, we propose a strongly perspectivist model for supervised classification of natural language utterances. Our approach combines the predictions of several perspective-aware models using key information of their individual... | Silvia Casola, Soda Marem Lo, Valerio Basile, Simona Frenda, Alessandra Teresa Cignarella, Viviana Patti, Cristina Bosco |  |
| 1407 |  |  [ToViLaG: Your Visual-Language Generative Model is Also An Evildoer](https://doi.org/10.18653/v1/2023.emnlp-main.213) |  | 0 | Recent large-scale Visual-Language Generative Models (VLGMs) have achieved unprecedented improvement in multimodal image/text generation. However, these models might also generate toxic content, e.g., offensive text and pornography images, raising significant ethical risks. Despite exhaustive studies on toxic degeneration of language models, this problem remains largely unexplored within the context of visual-language generation. This work delves into the propensity for toxicity generation and... | Xinpeng Wang, Xiaoyuan Yi, Han Jiang, Shanlin Zhou, Zhihua Wei, Xing Xie |  |
| 1408 |  |  [GPT-RE: In-context Learning for Relation Extraction using Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.214) |  | 0 | In spite of the potential for ground-breaking achievements offered by large language models (LLMs) (e.g., GPT-3) via in-context learning (ICL), they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of ICL for RE: (1) low relevance regarding entity and relation in existing sentence-level demonstration retrieval approaches for ICL; and (2) the lack of explaining input-label mappings of... | Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song, Jiwei Li, Sadao Kurohashi |  |
| 1409 |  |  [Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment](https://doi.org/10.18653/v1/2023.emnlp-main.215) |  | 0 | Designing systems that can reason across cultures requires that they are grounded in the norms of the contexts in which they operate. However, current research on developing computational models of social norms has primarily focused on American society. Here, we propose a novel approach to discover and compare descriptive social norms across Chinese and American cultures. We demonstrate our approach by leveraging discussions on a Chinese Q&A platform—Zhihu—and the existing SocialChemistry... | Sky CHWang, Arkadiy Saakyan, Oliver Li, Zhou Yu, Smaranda Muresan |  |
| 1410 |  |  [INFORM : Information eNtropy based multi-step reasoning FOR large language Models](https://doi.org/10.18653/v1/2023.emnlp-main.216) |  | 0 | Large language models (LLMs) have demonstrated exceptional performance in reasoning tasks with dedicated Chain-of-Thought (CoT) prompts. Further enhancing CoT prompts with exquisite exemplars can significantly improve reasoning performance.However, the effectiveness of CoT prompts may fluctuate dramatically with different choices of in-context examples. Additionally, manual construction of rationale steps can be time-consuming, presenting challenges for the widespread adoption of CoT prompting.... | Chuyue Zhou, Wangjie You, Juntao Li, Jing Ye, Kehai Chen, Min Zhang |  |
| 1411 |  |  [Adaptive Gating in Mixture-of-Experts based Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.217) |  | 0 | Large language models have demonstrated exceptional language understanding capabilities in many NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE models adopt a fixed gating network where each token is computed by the same number of experts. This contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and,... | Jiamin Li, Qiang Su, Yitao Yang, Yimin Jiang, Cong Wang, Hong Xu |  |
| 1412 |  |  [On the Automatic Generation and Simplification of Children's Stories](https://doi.org/10.18653/v1/2023.emnlp-main.218) |  | 0 | With recent advances in large language models (LLMs), the concept of automatically generating children’s educational materials has become increasingly realistic. Working toward the goal of age-appropriate simplicity in generated educational texts, we first examine the ability of several popular LLMs to generate stories with properly adjusted lexical and readability levels. We find that, in spite of the growing capabilities of LLMs, they do not yet possess the ability to limit their vocabulary... | Maria R. Valentini, Jennifer Weber, Jesus Salcido, Téa Wright, Eliana Colunga, Katharina von der Wense |  |
| 1413 |  |  [When Do Decompositions Help for Machine Reading?](https://doi.org/10.18653/v1/2023.emnlp-main.219) |  | 0 | Answering complex questions often requires multi-step reasoning in order to obtain the final answer. Most research into decompositions of complex questions involves open-domain systems, which have shown success in using these decompositions for improved retrieval. In the machine reading setting, however, work to understand when decompositions are helpful is understudied. We conduct experiments on decompositions in machine reading to unify recent work in this space, using a range of models and... | Kangda Wei, Dawn J. Lawrie, Benjamin Van Durme, Yunmo Chen, Orion Weller |  |
| 1414 |  |  [The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.220) |  | 0 | Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior due to overconfidence. In this paper, we explore the behavior of LLMs when presented with (un)answerable queries. We ask: do models represent the fact that the question is (un)answerable when... | Aviv Slobodkin, Omer Goldman, Avi Caciularu, Ido Dagan, Shauli Ravfogel |  |
| 1415 |  |  [Identifying Informational Sources in News Articles](https://doi.org/10.18653/v1/2023.emnlp-main.221) |  | 0 | News articles are driven by the informational sources journalists use in reporting. Modeling when, how and why sources get used together in stories can help us better understand the information we consume and even help journalists with the task of producing it. In this work, we take steps toward this goal by constructing the largest and widest-ranging annotated dataset, to date, of informational sources used in news writing. We first show that our dataset can be used to train high-performing... | Alexander Spangher, Nanyun Peng, Emilio Ferrara, Jonathan May |  |
| 1416 |  |  [Retrofitting Light-weight Language Models for Emotions using Supervised Contrastive Learning](https://doi.org/10.18653/v1/2023.emnlp-main.222) |  | 0 | We present a novel retrofitting method to induce emotion aspects into pre-trained language models (PLMs) such as BERT and RoBERTa. Our method updates pre-trained network weights using contrastive learning so that the text fragments exhibiting similar emotions are encoded nearby in the representation space, and the fragments with different emotion content are pushed apart. While doing so, it also ensures that the linguistic knowledge already present in PLMs is not inadvertently perturbed. The... | Sapan Shah, Sreedhar Reddy, Pushpak Bhattacharyya |  |
| 1417 |  |  [Longtriever: a Pre-trained Long Text Encoder for Dense Document Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.223) |  | 0 | Pre-trained language models (PLMs) have achieved the preeminent position in dense retrieval due to their powerful capacity in modeling intrinsic semantics. However, most existing PLM-based retrieval models encounter substantial computational costs and are infeasible for processing long documents. In this paper, a novel retrieval model Longtriever is proposed to embrace three core challenges of long document retrieval: substantial computational cost, incomprehensive document understanding, and... | Junhan Yang, Zheng Liu, Chaozhuo Li, Guangzhong Sun, Xing Xie |  |
| 1418 |  |  [Revisiting De-Identification of Electronic Medical Records: Evaluation of Within- and Cross-Hospital Generalization](https://doi.org/10.18653/v1/2023.emnlp-main.224) |  | 0 | The de-identification task aims to detect and remove the protected health information from electronic medical records (EMRs). Previous studies generally focus on the within-hospital setting and achieve great successes, while the cross-hospital setting has been overlooked. This study introduces a new de-identification dataset comprising EMRs from three hospitals in China, creating a benchmark for evaluating both within- and cross-hospital generalization. We find significant domain discrepancy... | Yiyang Liu, Jinpeng Li, Enwei Zhu |  |
| 1419 |  |  [Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.225) |  | 0 | Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and... | Gurusha Juneja, Subhabrata Dutta, Soumen Chakrabarti, Sunny Manchanda, Tanmoy Chakraborty |  |
| 1420 |  |  [Language Representation Projection: Can We Transfer Factual Knowledge across Languages in Multilingual Language Models?](https://doi.org/10.18653/v1/2023.emnlp-main.226) |  | 0 | Multilingual pretrained language models serve as repositories of multilingual factual knowledge. Nevertheless, a substantial performance gap of factual knowledge probing exists between high-resource languages and low-resource languages, suggesting limited implicit factual knowledge transfer across languages in multilingual pretrained language models. This paper investigates the feasibility of explicitly transferring relatively rich factual knowledge from English to non-English languages. To... | Shaoyang Xu, Junzhuo Li, Deyi Xiong |  |
| 1421 |  |  [Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.227) |  | 0 | Abstract grammatical knowledge—of parts of speech and grammatical patterns—is key to the capacity for linguistic generalization in humans. But how abstract is grammatical knowledge in large language models? In the human literature, compelling evidence for grammatical abstraction comes from structural priming. A sentence that shares the same grammatical structure as a preceding sentence is processed and produced more readily. Because confounds exist when using stimuli in a single language,... | James A. Michaelov, Catherine Arnett, Tyler A. Chang, Ben Bergen |  |
| 1422 |  |  [ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph](https://doi.org/10.18653/v1/2023.emnlp-main.228) |  | 0 | Question Answering over Knowledge Graph (KGQA) aims to seek answer entities for the natural language question from a large-scale Knowledge Graph (KG). To better perform reasoning on KG, recent work typically adopts a pre-trained language model (PLM) to model the question, and a graph neural network (GNN) based module to perform multi-hop reasoning on the KG. Despite the effectiveness, due to the divergence in model architecture, the PLM and GNN are not closely integrated, limiting the knowledge... | Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yaliang Li, JiRong Wen |  |
| 1423 |  |  [Deep Natural Language Feature Learning for Interpretable Prediction](https://doi.org/10.18653/v1/2023.emnlp-main.229) |  | 0 | We propose a general method to break down a main complex task into a set of intermediary easier sub-tasks, which are formulated in natural language as binary questions related to the final target task. Our method allows for representing each example by a vector consisting of the answers to these questions. We call this representation Natural Language Learned Features (NLLF). NLLF is generated by a small transformer language model (e.g., BERT) that has been trained in a Natural Language... | Felipe Urrutia, Cristian Buc Calderon, Valentin Barrière |  |
| 1424 |  |  [ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.230) |  | 0 | As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups. In this work, our focus is... | David Esiobu, Xiaoqing Ellen Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude Fernandes, Jane DwivediYu, Eleonora Presani, Adina Williams, Eric Michael Smith |  |
| 1425 |  |  [Enhancing Task-oriented Dialogue Systems with Generative Post-processing Networks](https://doi.org/10.18653/v1/2023.emnlp-main.231) |  | 0 | Recently, post-processing networks (PPNs), which modify the outputs of arbitrary modules including non-differentiable ones in task-oriented dialogue systems, have been proposed. PPNs have successfully improved the dialogue performance by post-processing natural language understanding (NLU), dialogue state tracking (DST), and dialogue policy (Policy) modules with a classification-based approach. However, they cannot be applied to natural language generation (NLG) modules because the... | Atsumoto Ohashi, Ryuichiro Higashinaka |  |
| 1426 |  |  [Adapting Language Models to Compress Contexts](https://doi.org/10.18653/v1/2023.emnlp-main.232) |  | 0 | Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models are capable of compressing long contexts into summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents... | Alexis Chevalier, Alexander Wettig, Anirudh Ajith, Danqi Chen |  |
| 1427 |  |  [Selective Labeling: How to Radically Lower Data-Labeling Costs for Document Extraction Models](https://doi.org/10.18653/v1/2023.emnlp-main.233) |  | 0 | Building automatic extraction models for visually rich documents like invoices, receipts, bills, tax forms, etc. has received significant attention lately. A key bottleneck in developing extraction models for new document types is the cost of acquiring the several thousand high-quality labeled documents that are needed to train a model with acceptable accuracy. In this paper, we propose selective labeling as a solution to this problem. The key insight is to simplify the labeling task to provide... | Yichao Zhou, James B. Wendt, Navneet Potti, Jing Xie, Sandeep Tata |  |
| 1428 |  |  [TRAVEL: Tag-Aware Conversational FAQ Retrieval via Reinforcement Learning](https://doi.org/10.18653/v1/2023.emnlp-main.234) |  | 0 | Efficiently retrieving FAQ questions that match users’ intent is essential for online customer service. Existing methods aim to fully utilize the dynamic conversation context to enhance the semantic association between the user query and FAQ questions. However, the conversation context contains noise, e.g., users may click questions they don’t like, leading to inaccurate semantics modeling. To tackle this, we introduce tags of FAQ questions, which can help us eliminate irrelevant information.... | Yue Chen, Dingnan Jin, Chen Huang, Jia Liu, Wenqiang Lei |  |
| 1429 |  |  [Continual Dialogue State Tracking via Example-Guided Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.235) |  | 0 | Dialogue systems are frequently updated to accommodate new services, but naively updating them by continually training with data for new services in diminishing performance on previously learnt services. Motivated by the insight that dialogue state tracking (DST), a crucial component of dialogue systems that estimates the user’s goal as a conversation proceeds, is a simple natural language understanding task, we propose reformulating it as a bundle of granular example-guided question answering... | Hyundong Cho, Andrea Madotto, Zhaojiang Lin, Khyathi Raghavi Chandu, Satwik Kottur, Jing Xu, Jonathan May, Chinnadhurai Sankar |  |
| 1430 |  |  [Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media](https://doi.org/10.18653/v1/2023.emnlp-main.236) |  | 0 | Claim span identification (CSI) is an important step in fact-checking pipelines, aiming to identify text segments that contain a check-worthy claim or assertion in a social media post. Despite its importance to journalists and human fact-checkers, it remains a severely understudied problem, and the scarce research on this topic so far has only focused on English. Here we aim to bridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K real-world claims collected from numerous... | Shubham Mittal, Megha Sundriyal, Preslav Nakov |  |
| 1431 |  |  [COVID-19 Vaccine Misinformation in Middle Income Countries](https://doi.org/10.18653/v1/2023.emnlp-main.237) |  | 0 | This paper introduces a multilingual dataset of COVID-19 vaccine misinformation, consisting of annotated tweets from three middle-income countries: Brazil, Indonesia, and Nigeria. The expertly curated dataset includes annotations for 5,952 tweets, assessing their relevance to COVID-19 vaccines, presence of misinformation, and the themes of the misinformation. To address challenges posed by domain specificity, the low-resource setting, and data imbalance, we adopt two approaches for developing... | Jongin Kim, Byeo Bak, Aditya Agrawal, Jiaxi Wu, Veronika J. Wirtz, Traci Hong, Derry Wijaya |  |
| 1432 |  |  [Contrastive Learning of Sentence Embeddings from Scratch](https://doi.org/10.18653/v1/2023.emnlp-main.238) |  | 0 | Contrastive learning has been the dominant approach to train state-of-the-art sentence embeddings. Previous studies have typically learned sentence embeddings either through the use of human-annotated natural language inference (NLI) data or via large-scale unlabeled sentences in an unsupervised manner. However, even in the case of unlabeled data, their acquisition presents challenges in certain domains due to various reasons. due to copyright restrictions, data distribution issues, and messy... | Junlei Zhang, Zhenzhong Lan, Junxian He |  |
| 1433 |  |  [A Rose by Any Other Name would not Smell as Sweet: Social Bias in Names Mistranslation](https://doi.org/10.18653/v1/2023.emnlp-main.239) |  | 0 | We ask the question: Are there widespread disparities in machine translations of names across race/ethnicity, and gender? We hypothesize that the translation quality of names and surrounding context will be lower for names associated with US racial and ethnic minorities due to these systems’ tendencies to standardize language to predominant language patterns. We develop a dataset of names that are strongly demographically aligned and propose a translation evaluation procedure based on... | Sandra Sandoval, Jieyu Zhao, Marine Carpuat, Hal Daumé III |  |
| 1434 |  |  [Investigating Efficiently Extending Transformers for Long Input Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.240) |  | 0 | While large pretrained Transformer models have proven highly capable at tackling natural language tasks, handling long sequence inputs still poses a significant challenge. One such task is long input summarization, where inputs are longer than the maximum input context of most models. Through an extensive set of experiments, we investigate what model architectural changes and pretraining paradigms most efficiently adapt a pretrained Transformer for long input summarization. We find that a... | Jason Phang, Yao Zhao, Peter J. Liu |  |
| 1435 |  |  [CS2W: A Chinese Spoken-to-Written Style Conversion Dataset with Multiple Conversion Types](https://doi.org/10.18653/v1/2023.emnlp-main.241) |  | 0 | Spoken texts (either manual or automatic transcriptions from automatic speech recognition (ASR)) often contain disfluencies and grammatical errors, which pose tremendous challenges to downstream tasks. Converting spoken into written language is hence desirable. Unfortunately, the availability of datasets for this is limited. To address this issue, we present CS2W, a Chinese Spoken-to-Written style conversion dataset comprising 7,237 spoken sentences extracted from transcribed conversational... | Zishan Guo, Linhao Yu, Minghui Xu, Renren Jin, Deyi Xiong |  |
| 1436 |  |  [Unifying Cross-Lingual Transfer across Scenarios of Resource Scarcity](https://doi.org/10.18653/v1/2023.emnlp-main.242) |  | 0 | The scarcity of data in many of the world’s languages necessitates the transfer of knowledge from other, resource-rich languages. However, the level of scarcity varies significantly across multiple dimensions, including: i) the amount of task-specific data available in the source and target languages; ii) the amount of monolingual and parallel data available for both languages; and iii) the extent to which they are supported by pretrained multilingual and translation models. Prior work has... | Alan Ansell, Marinela Parovic, Ivan Vulic, Anna Korhonen, Edoardo M. Ponti |  |
| 1437 |  |  [A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.243) |  | 0 | Recent instruction fine-tuned models can solve multiple NLP tasks when prompted to do so, with machine translation (MT) being a prominent use case. However, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. In MT, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. In this work, we address this gap by investigating whether and to what extent... | Giuseppe Attanasio, Flor Miriam Plaza del Arco, Debora Nozza, Anne Lauscher |  |
| 1438 |  |  [DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining](https://doi.org/10.18653/v1/2023.emnlp-main.244) |  | 0 | Many text mining models are constructed by fine-tuning a large deep pre-trained language model (PLM) in downstream tasks. However, a significant challenge that arises nowadays is how to maintain performance when we use a lightweight model with limited labeled samples. We present DisCo, a semi-supervised learning (SSL) framework for fine-tuning a cohort of small student models generated from a large PLM using knowledge distillation. Our key insight is to share complementary knowledge among... | Weifeng Jiang, Qianren Mao, Chenghua Lin, Jianxin Li, Ting Deng, Weiyi Yang, Zheng Wang |  |
| 1439 |  |  [Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation](https://doi.org/10.18653/v1/2023.emnlp-main.245) |  | 0 | Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) to comprehend instructions and generate appropriate responses. Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets. In this paper, we propose Dynosaur, a dynamic growth paradigm for the automatic curation of instruction-tuning data. Based on the metadata of... | Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han, KaiWei Chang |  |
| 1440 |  |  [Are All Steps Equally Important? Benchmarking Essentiality Detection in Event Processes](https://doi.org/10.18653/v1/2023.emnlp-main.246) |  | 0 | Natural language often describes events in different granularities, such that more coarse-grained (goal) events can often be decomposed into fine-grained sequences of (step) events. A critical but overlooked challenge in understanding an event process lies in the fact that the step events are not equally important to the central goal. In this paper, we seek to fill this gap by studying how well current models can understand the essentiality of different step events towards a goal event. As... | Haoyu Wang, Hongming Zhang, Yueguan Wang, Yuqian Deng, Muhao Chen, Dan Roth |  |
| 1441 |  |  [Language Model is Suitable for Correction of Handwritten Mathematical Expressions Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.247) |  | 0 | Handwritten mathematical expression recognition (HMER) is a multidisciplinary task that generates LaTeX sequences from images. Existing approaches, employing tree decoders within attention-based encoder-decoder architectures, aim to capture the hierarchical tree structure, but are limited by CFGs and pre-generated triplet data, hindering expandability and neglecting visual ambiguity challenges. This article investigates the distinctive language characteristics of LaTeX mathematical expressions,... | Zui Chen, Jiaqi Han, Chaofan Yang, Yi Zhou |  |
| 1442 |  |  [Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection](https://doi.org/10.18653/v1/2023.emnlp-main.248) |  | 0 | Cross-lingual transfer learning from high-resource to medium and low-resource languages has shown encouraging results. However, the scarcity of resources in target languages remains a challenge. In this work, we resort to data augmentation and continual pre-training for domain adaptation to improve cross-lingual abusive language detection. For data augmentation, we analyze two existing techniques based on vicinal risk minimization and propose MIXAG, a novel data augmentation method which... | Gretel Liz De la Peña Sarracén, Paolo Rosso, Robert Litschko, Goran Glavas, Simone Paolo Ponzetto |  |
| 1443 |  |  [SuperDialseg: A Large-scale Dataset for Supervised Dialogue Segmentation](https://doi.org/10.18653/v1/2023.emnlp-main.249) |  | 0 | Dialogue segmentation is a crucial task for dialogue systems allowing a better understanding of conversational texts. Despite recent progress in unsupervised dialogue segmentation methods, their performances are limited by the lack of explicit supervised signals for training. Furthermore, the precise definition of segmentation points in conversations still remains as a challenging problem, increasing the difficulty of collecting manual annotations. In this paper, we provide a feasible... | Junfeng Jiang, Chengzhang Dong, Sadao Kurohashi, Akiko Aizawa |  |
| 1444 |  |  [ATFormer: A Learned Performance Model with Transfer Learning Across Devices for Deep Learning Tensor Programs](https://doi.org/10.18653/v1/2023.emnlp-main.250) |  | 0 | The training and inference efficiency of ever-larger deep neural networks highly rely on the performance of tensor operators on specific hardware platforms. Therefore, a compilation-based optimization flow with automatic tensor generation and parameter tuning is necessary for efficient model deployment. While compilation-based methods with performance models can provide dynamic and suitable code optimization, they suffer from a large design space exploration with rough measurement accuracy and... | Yang Bai, Wenqian Zhao, Shuo Yin, Zixiao Wang, Bei Yu |  |
| 1445 |  |  [mRedditSum: A Multimodal Abstractive Summarization Dataset of Reddit Threads with Images](https://doi.org/10.18653/v1/2023.emnlp-main.251) |  | 0 | The growing number of multimodal online discussions necessitates automatic summarization to save time and reduce content overload. However, existing summarization datasets are not suitable for this purpose, as they either do not cover discussions, multiple modalities, or both. To this end, we present mRedditSum, the first multimodal discussion summarization dataset. It consists of 3,033 discussion threads where a post solicits advice regarding an issue described with an image and text, and... | Keighley Overbay, Jaewoo Ahn, Fatemeh Pesaran Zadeh, Joonsuk Park, Gunhee Kim |  |
| 1446 |  |  [Sparse Low-rank Adaptation of Pre-trained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.252) |  | 0 | Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. The popular method of low-rank adaptation (LoRA) offers a notable approach, hypothesizing that the adaptation process is intrinsically low-dimensional. Although LoRA has demonstrated commendable performance, it is implemented with a fixed and unalterable intrinsic rank that might not always be the ideal choice. Recognizing the need for more flexible adaptation,... | Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, Maosong Sun |  |
| 1447 |  |  [Human Learning by Model Feedback: The Dynamics of Iterative Prompting with Midjourney](https://doi.org/10.18653/v1/2023.emnlp-main.253) |  | 0 | Generating images with a Text-to-Image model often requires multiple trials, where human users iteratively update their prompt based on feedback, namely the output image. Taking inspiration from cognitive work on reference games and dialogue alignment, this paper analyzes the dynamics of the user prompts along such iterations. We compile a dataset of iterative interactions of human users with Midjourney. Our analysis then reveals that prompts predictably converge toward specific traits along... | Shachar DonYehiya, Leshem Choshen, Omri Abend |  |
| 1448 |  |  [ULF: Unsupervised Labeling Function Correction using Cross-Validation for Weak Supervision](https://doi.org/10.18653/v1/2023.emnlp-main.254) |  | 0 | A cost-effective alternative to manual data labeling is weak supervision (WS), where data samples are automatically annotated using a predefined set of labeling functions (LFs), rule-based mechanisms that generate artificial labels for the associated classes. In this work, we investigate noise reduction techniques for WS based on the principle of k-fold cross-validation. We introduce a new algorithm ULF for Unsupervised Labeling Function correction, which denoises WS data by leveraging models... | Anastasiia Sedova, Benjamin Roth |  |
| 1449 |  |  [The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.255) |  | 0 | Chain-of-Thought (CoT) prompting enables large language models to solve complex reasoning problems by generating intermediate steps. However, confined by its inherent single-pass and sequential generation process, CoT heavily relies on the initial decisions, causing errors in early steps to accumulate and impact the final answers. In contrast, humans adopt recursive thinking when tackling complex reasoning problems, i.e. iteratively breaking the original problem into approachable sub-problems... | Jingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu, Di Jin, Qifan Wang, Lifu Huang |  |
| 1450 |  |  [Ideology Takes Multiple Looks: A High-Quality Dataset for Multifaceted Ideology Detection](https://doi.org/10.18653/v1/2023.emnlp-main.256) |  | 0 | Ideology detection (ID) is important for gaining insights about peoples’ opinions and stances on our world and society, which can find many applications in politics, economics and social sciences. It is not uncommon that a piece of text can contain descriptions of various issues. It is also widely accepted that a person can take different ideological stances in different facets. However, existing datasets for the ID task only label a text as ideologically left- or right-leaning as a whole,... | Songtao Liu, Ziling Luo, Minghua Xu, Lixiao Wei, Ziyao Wei, Han Yu, Wei Xiang, Bang Wang |  |
| 1451 |  |  [Transductive Learning for Textual Few-Shot Classification in API-based Embedding Models](https://doi.org/10.18653/v1/2023.emnlp-main.257) |  | 0 | Proprietary and closed APIs are becoming increasingly common to process natural language, and are impacting the practical applications of natural language processing, including few-shot classification. Few-shot classification involves training a model to perform a new classification task with a handful of labeled data. This paper presents three contributions. First, we introduce a scenario where the embedding of a pre-trained model is served through a gated API with compute-cost and... | Pierre Colombo, Victor Pellegrain, Malik Boudiaf, Myriam Tami, Victor Storchan, Ismail Ben Ayed, Pablo Piantanida |  |
| 1452 |  |  [MEGA: Multilingual Evaluation of Generative AI](https://doi.org/10.18653/v1/2023.emnlp-main.258) |  | 0 | Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in... | Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Uttama Nambi, Tanuja Ganu, Sameer Segal, Mohamed Ahmed, Kalika Bali, Sunayana Sitaram |  |
| 1453 |  |  [Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation](https://doi.org/10.18653/v1/2023.emnlp-main.259) |  | 0 | Mis- and disinformation online have become a major societal problem as major sources of online harms of different kinds. One common form of mis- and disinformation is out-of-context (OOC) information, where different pieces of information are falsely associated, e.g., a real image combined with a false textual caption or a misleading textual description. Although some past studies have attempted to defend against OOC mis- and disinformation through external evidence, they tend to disregard the... | Xin Yuan, Jie Guo, Weidong Qiu, Zheng Huang, Shujun Li |  |
| 1454 |  |  [Video-Helpful Multimodal Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.260) |  | 0 | Existing multimodal machine translation (MMT) datasets consist of images and video captions or instructional video subtitles, which rarely contain linguistic ambiguity, making visual information ineffective in generating appropriate translations. Recent work has constructed an ambiguous subtitles dataset to alleviate this problem but is still limited to the problem that videos do not necessarily contribute to disambiguation. We introduce EVA (Extensive training set and Video-helpful evaluation... | Yihang Li, Shuichiro Shimizu, Chenhui Chu, Sadao Kurohashi, Wei Li |  |
| 1455 |  |  [Large Language Models are Temporal and Causal Reasoners for Video Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.261) |  | 0 | Large Language Models (LLMs) have shown remarkable performances on a wide range of natural language understanding and generation tasks. We observe that the LLMs provide effective priors in exploiting linguistic shortcuts for temporal and causal reasoning in Video Question Answering (VideoQA). However, such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, i.e., linguistic bias, while ignoring visual content. This is also known as ‘ungrounded... | Dohwan Ko, Ji Soo Lee, WooYoung Kang, Byungseok Roh, Hyunwoo Kim |  |
| 1456 |  |  [Uncertainty Guided Global Memory Improves Multi-Hop Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.262) |  | 0 | Transformers have become the gold standard for many natural language processing tasks and, in particular, for multi-hop question answering (MHQA). This task includes processing a long document and reasoning over the multiple parts of it. The landscape of MHQA approaches can be classified into two primary categories. The first group focuses on extracting supporting evidence, thereby constraining the QA model’s context to predicted facts. Conversely, the second group relies on the attention... | Alsu Sagirova, Mikhail Burtsev |  |
| 1457 |  |  [Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation](https://doi.org/10.18653/v1/2023.emnlp-main.263) |  | 0 | The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability... | Yuanyuan Liang, Jianing Wang, Hanlun Zhu, Lei Wang, Weining Qian, Yunshi Lan |  |
| 1458 |  |  [TrojanSQL: SQL Injection against Natural Language Interface to Database](https://doi.org/10.18653/v1/2023.emnlp-main.264) |  | 0 | The technology of text-to-SQL has significantly enhanced the efficiency of accessing and manipulating databases. However, limited research has been conducted to study its vulnerabilities emerging from malicious user interaction. By proposing TrojanSQL, a backdoor-based SQL injection framework for text-to-SQL systems, we show how state-of-the-art text-to-SQL parsers can be easily misled to produce harmful SQL statements that can invalidate user queries or compromise sensitive information about... | Jinchuan Zhang, Yan Zhou, Binyuan Hui, Yaxin Liu, Ziming Li, Songlin Hu |  |
| 1459 |  |  [Preserving Privacy Through Dememorization: An Unlearning Technique For Mitigating Memorization Risks In Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.265) |  | 0 | Large Language models (LLMs) are trained on vast amounts of data, including sensitive information that poses a risk to personal privacy if exposed. LLMs have shown the ability to memorize and reproduce portions of their training data when prompted by adversaries. Prior research has focused on addressing this memorization issue and preventing verbatim replication through techniques like knowledge unlearning and data pre-processing. However, these methods have limitations regarding the number of... | Aly M. Kassem, Omar Mahmoud, Sherif Saad |  |
| 1460 |  |  [MingOfficial: A Ming Official Career Dataset and a Historical Context-Aware Representation Learning Framework](https://doi.org/10.18653/v1/2023.emnlp-main.266) |  | 0 | In Chinese studies, understanding the nuanced traits of historical figures, often not explicitly evident in biographical data, has been a key interest. However, identifying these traits can be challenging due to the need for domain expertise, specialist knowledge, and context-specific insights, making the process time-consuming and difficult to scale. Our focus on studying officials from China’s Ming Dynasty is no exception. To tackle this challenge, we propose MingOfficial, a large-scale... | YouJun Chen, HsinYi Hsieh, Yu Lin, Yingtao Tian, Bert Chan, YuSin Liu, YiHsuan Lin, Richard TzongHan Tsai |  |
| 1461 |  |  [DPP-TTS: Diversifying prosodic features of speech via determinantal point processes](https://doi.org/10.18653/v1/2023.emnlp-main.267) |  | 0 | With the rapid advancement in deep generative models, recent neural Text-To-Speech(TTS) models have succeeded in synthesizing human-like speech. There have been some efforts to generate speech with various prosody beyond monotonous prosody patterns. However, previous works have several limitations. First, typical TTS models depend on the scaled sampling temperature for boosting the diversity of prosody. Speech samples generated at high sampling temperatures often lack perceptual prosodic... | Seongho Joo, Hyukhun Koh, Kyomin Jung |  |
| 1462 |  |  [Meta-Learning Online Adaptation of Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.268) |  | 0 | Large language models encode impressively broad world knowledge in their parameters. However, the knowledge in static language models falls out of date, limiting the model’s effective “shelf life.” While online fine-tuning can reduce this degradation, we find that naively fine-tuning on a stream of documents leads to a low level of information uptake. We hypothesize that online fine-tuning does not sufficiently attend to important information. That is, the gradient signal from important tokens... | Nathan Hu, Eric Mitchell, Christopher D. Manning, Chelsea Finn |  |
| 1463 |  |  [Self-Detoxifying Language Models via Toxification Reversal](https://doi.org/10.18653/v1/2023.emnlp-main.269) |  | 0 | Language model detoxification aims to minimize the risk of generating offensive or harmful content in pretrained language models (PLMs) for safer deployment. Existing methods can be roughly categorized as finetuning-based and decoding-based. However, the former is often resource-intensive, while the latter relies on additional components and potentially compromises the generation fluency. In this paper, we propose a more lightweight approach that enables the PLM itself to achieve... | Chak Tou Leong, Yi Cheng, Jiashuo Wang, Jian Wang, Wenjie Li |  |
| 1464 |  |  [Interactive Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.270) |  | 0 | Users interact with text, image, code, or other editors on a daily basis. However, machine learning models are rarely trained in the settings that reflect the interactivity between users and their editor. This is understandable as training AI models with real users is not only slow and costly, but what these models learn may be specific to user interface design choices. Unfortunately, this means most of the research on text, code, and image generation has focused on non-interactive settings,... | Felix Faltings, Michel Galley, Kianté Brantley, Baolin Peng, Weixin Cai, Yizhe Zhang, Jianfeng Gao, Bill Dolan |  |
| 1465 |  |  [Knowledge Distillation \approx Label Smoothing: Fact or Fallacy?](https://doi.org/10.18653/v1/2023.emnlp-main.271) |  | 0 | Originally proposed as a method for knowledge transfer from one model to another, some recent studies have suggested that knowledge distillation (KD) is in fact a form of regularization. Perhaps the strongest argument of all for this new perspective comes from its apparent similarities with label smoothing (LS). Here we re-examine this stated equivalence between the two methods by comparing the predictive confidences of the models they train. Experiments on four text classification tasks... | Md. Sultan |  |
| 1466 |  |  [Analyzing Cognitive Plausibility of Subword Tokenization](https://doi.org/10.18653/v1/2023.emnlp-main.272) |  | 0 | Subword tokenization has become the de-facto standard for tokenization although comparative evaluations of their quality across languages are scarce. Existing evaluation studies focus on the effect of a tokenization algorithm on the performance in downstream tasks, or on engineering criteria such as the compression rate. We present a new evaluation paradigm that focuses on the cognitive plausibility of subword tokenization. We analyze the correlation of the tokenizer output with the reading... | Lisa Beinborn, Yuval Pinter |  |
| 1467 |  |  [POE: Process of Elimination for Multiple Choice Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.273) |  | 0 | Language models (LMs) are capable of conducting in-context learning for multiple choice reasoning tasks, but the options in these tasks are treated equally. As humans often first eliminate wrong options before picking the final correct answer, we argue a similar two-step strategy can make LMs better at these tasks. To this end, we present the Process of Elimination (POE), a two-step scoring method. In the first step, POE scores each option, and eliminates seemingly wrong options. In the second... | Chenkai Ma, Xinya Du |  |
| 1468 |  |  [NeuSTIP: A Neuro-Symbolic Model for Link and Time Prediction in Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.274) |  | 0 | Neuro-symbolic (NS) models for knowledge graph completion (KGC) combine the benefits of symbolic models (interpretable inference) with those of distributed representations (parameter sharing, high accuracy). While several NS models exist for KGs with static facts, there is limited work on temporal KGC (TKGC) for KGs where a fact is associated with a time interval. In response, we propose a novel NS model for TKGC called NeuSTIP, which performs link prediction and time interval prediction in a... | Ishaan Singh, Navdeep Kaur, Garima Gaur, Mausam |  |
| 1469 |  |  [Standardizing Distress Analysis: Emotion-Driven Distress Identification and Cause Extraction (DICE) in Multimodal Online Posts](https://doi.org/10.18653/v1/2023.emnlp-main.275) |  | 0 | Due to its growing impact on public opinion, hate speech on social media has garnered increased attention. While automated methods for identifying hate speech have been presented in the past, they have mostly been limited to analyzing textual content. The interpretability of such models has received very little attention, despite the social and legal consequences of erroneous predictions. In this work, we present a novel problem of Distress Identification and Cause Extraction (DICE) from... | Gopendra Vikram Singh, Soumitra Ghosh, Atul Verma, Chetna Painkra, Asif Ekbal |  |
| 1470 |  |  [Out-of-Distribution Generalization in Natural Language Processing: Past, Present, and Future](https://doi.org/10.18653/v1/2023.emnlp-main.276) |  | 0 | Machine learning (ML) systems in natural language processing (NLP) face significant challenges in generalizing to out-of-distribution (OOD) data, where the test distribution differs from the training data distribution. This poses important questions about the robustness of NLP models and their high accuracy, which may be artificially inflated due to their underlying sensitivity to systematic biases. Despite these challenges, there is a lack of comprehensive surveys on the generalization... | Linyi Yang, Yaoxian Song, Xuan Ren, Chenyang Lyu, Yidong Wang, Jingming Zhuo, Lingqiao Liu, Jindong Wang, Jennifer Foster, Yue Zhang |  |
| 1471 |  |  [Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.277) |  | 0 | Recent advances in prompt engineering enable large language models (LLMs) to solve multi-hop logical reasoning problems with impressive accuracy. However, there is little existing work investigating the robustness of LLMs with few-shot prompting techniques. Therefore, we introduce a systematic approach to test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic perturbations. We include perturbations at multiple levels of abstractions (e.g. lexical perturbations such as... | Hongyi Zheng, Abulhair Saparov |  |
| 1472 |  |  [Can Large Language Models Capture Dissenting Human Voices?](https://doi.org/10.18653/v1/2023.emnlp-main.278) |  | 0 | Large language models (LLMs) have shown impressive achievements in solving a broad range of tasks. Augmented by instruction fine-tuning, LLMs have also been shown to generalize in zero-shot settings as well. However, whether LLMs closely align with the human disagreement distribution has not been well-studied, especially within the scope of natural language inference (NLI). In this paper, we evaluate the performance and alignment of LLM distribution with humans using two different techniques to... | Noah Lee, Na An, James Thorne |  |
| 1473 |  |  [DecoMT: Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.279) |  | 0 | This study investigates machine translation between related languages i.e., languages within the same family that share linguistic characteristics such as word order and lexical similarity. Machine translation through few-shot prompting leverages a small set of translation pair examples to generate translations for test sentences. This procedure requires the model to learn how to generate translations while simultaneously ensuring that token ordering is maintained to produce a fluent and... | Ratish Puduppully, Anoop Kunchukuttan, Raj Dabre, Ai Ti Aw, Nancy Chen |  |
| 1474 |  |  [Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.280) |  | 0 | Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pre-trained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes. To overcome this issue, we propose Prototype-based HyperAdapter (PHA), a novel framework built on the adapter-tuning and hypernetwork. It introduces an... | Hao Zhao, Jie Fu, Zhaofeng He |  |
| 1475 |  |  [Towards Building More Robust NER datasets: An Empirical Study on NER Dataset Bias from a Dataset Difficulty View](https://doi.org/10.18653/v1/2023.emnlp-main.281) |  | 0 | Recently, many studies have illustrated the robustness problem of Named Entity Recognition (NER) systems: the NER models often rely on superficial entity patterns for predictions, without considering evidence from the context. Consequently, even state-of-the-art NER models generalize poorly to out-of-domain scenarios when out-of-distribution (OOD) entity patterns are introduced. Previous research attributes the robustness problem to the existence of NER dataset bias, where simpler and regular... | Ruotian Ma, Xiaolei Wang, Xin Zhou, Qi Zhang, Xuanjing Huang |  |
| 1476 |  |  [GradSim: Gradient-Based Language Grouping for Effective Multilingual Training](https://doi.org/10.18653/v1/2023.emnlp-main.282) |  | 0 | Most languages of the world pose low-resource challenges to natural language processing models. With multilingual training, knowledge can be shared among languages. However, not all languages positively influence each other and it is an open research question how to select the most suitable set of languages for multilingual training and avoid negative interference among languages whose characteristics or data distributions are not compatible. In this paper, we propose GradSim, a language... | Mingyang Wang, Heike Adel, Lukas Lange, Jannik Strötgen, Hinrich Schütze |  |
| 1477 |  |  [Discovering Universal Geometry in Embeddings with ICA](https://doi.org/10.18653/v1/2023.emnlp-main.283) |  | 0 | This study utilizes Independent Component Analysis (ICA) to unveil a consistent semantic structure within embeddings of words or images. Our approach extracts independent semantic components from the embeddings of a pre-trained model by leveraging anisotropic information that remains after the whitening process in Principal Component Analysis (PCA). We demonstrate that each embedding can be expressed as a composition of a few intrinsic interpretable axes and that these semantic axes remain... | Hiroaki Yamagiwa, Momose Oyama, Hidetoshi Shimodaira |  |
| 1478 |  |  [Toward a Critical Toponymy Framework for Named Entity Recognition: A Case Study of Airbnb in New York City](https://doi.org/10.18653/v1/2023.emnlp-main.284) |  | 0 | Critical toponymy examines the dynamics of power, capital, and resistance through place names and the sites to which they refer. Studies here have traditionally focused on the semantic content of toponyms and the top-down institutional processes that produce them. However, they have generally ignored the ways in which toponyms are used by ordinary people in everyday discourse, as well as the other strategies of geospatial description that accompany and contextualize toponymic reference. Here,... | Mikael Brunila, Jack LaViolette, Sky CHWang, Priyanka Verma, Clara Féré, Grant McKenzie |  |
| 1479 |  |  [Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.285) |  | 0 | Accurate knowledge selection is critical in knowledge-grounded dialogue systems. Towards a closer look at it, we offer a novel perspective to organize existing literature, i.e., knowledge selection coupled with, after, and before generation. We focus on the third under-explored category of study, which can not only select knowledge accurately in advance, but has the advantage to reduce the learning, adjustment, and interpretation burden of subsequent response generation models, especially LLMs.... | Lang Qin, Yao Zhang, Hongru Liang, Jun Wang, Zhenglu Yang |  |
| 1480 |  |  [Merging Generated and Retrieved Knowledge for Open-Domain QA](https://doi.org/10.18653/v1/2023.emnlp-main.286) |  | 0 | Open-domain question answering (QA) systems are often built with retrieval modules. However, retrieving passages from a given source is known to suffer from insufficient knowledge coverage. Alternatively, prompting large language models (LLMs) to generate contextual passages based on their parametric knowledge has been shown to improve QA performance. Yet, LLMs tend to “hallucinate” content that conflicts with the retrieved knowledge. Based on the intuition that answers supported by both... | Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Lu Wang |  |
| 1481 |  |  [Best of Both Worlds: Towards Improving Temporal Knowledge Base Question Answering via Targeted Fact Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.287) |  | 0 | Temporal question answering (QA) is a special category of complex question answering task that requires reasoning over facts asserting time intervals of events. Previous works have predominately relied on Knowledge Base Question Answering (KBQA) for temporal QA. One of the major challenges faced by these systems is their inability to retrieve all relevant facts due to factors such as incomplete KB and entity/relation linking errors. A failure to fetch even a single fact will block KBQA from... | Nithish Kannen, Udit Sharma, Sumit Neelam, Dinesh Khandelwal, Shajith Ikbal, Hima Karanam, L. Venkata Subramaniam |  |
| 1482 |  |  [Text Fact Transfer](https://doi.org/10.18653/v1/2023.emnlp-main.288) |  | 0 | Text style transfer is a prominent task that aims to control the style of text without inherently changing its factual content. To cover more text modification applications, such as adapting past news for current events and repurposing educational materials, we propose the task of text fact transfer, which seeks to transfer the factual content of a source text between topics without modifying its style. We find that existing language models struggle with text fact transfer, due to their... | Nishant Balepur, Jie Huang, Kevin ChenChuan Chang |  |
| 1483 |  |  [A Cheaper and Better Diffusion Language Model with Soft-Masked Noise](https://doi.org/10.18653/v1/2023.emnlp-main.289) |  | 0 | Diffusion models that are based on iterative denoising have been recently proposed and leveraged in various generation tasks like image generation. Whereas, as a way inherently built for continuous data, existing diffusion models still have some limitations in modeling discrete data, e.g., languages. For example, the generally used Gaussian noise can not handle the discrete corruption well, and the objectives in continuous spaces fail to be stable for textual data in the diffusion process... | Jiaao Chen, Aston Zhang, Mu Li, Alex Smola, Diyi Yang |  |
| 1484 |  |  [Mirages. On Anthropomorphism in Dialogue Systems](https://doi.org/10.18653/v1/2023.emnlp-main.290) |  | 0 | Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism is inevitable, conscious and unconscious design choices can guide users to personify them to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to transparency and trust issues, and high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have... | Gavin Abercrombie, Amanda Cercas Curry, Tanvi Dinkar, Verena Rieser, Zeerak Talat |  |
| 1485 |  |  [Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?](https://doi.org/10.18653/v1/2023.emnlp-main.291) |  | 0 | Neural language models (LMs) can be used to evaluate the truth of factual statements in two ways: they can be either queried for statement probabilities, or probed for internal representations of truthfulness. Past work has found that these two procedures sometimes disagree, and that probes tend to be more accurate than LM outputs. This has led some researchers to conclude that LMs “lie’ or otherwise encode non-cooperative communicative intents. Is this an accurate description of today’s LMs,... | Kevin Liu, Stephen Casper, Dylan HadfieldMenell, Jacob Andreas |  |
| 1486 |  |  [KEBAP: Korean Error Explainable Benchmark Dataset for ASR and Post-processing](https://doi.org/10.18653/v1/2023.emnlp-main.292) |  | 0 | Automatic Speech Recognition (ASR) systems are instrumental across various applications, with their performance being critically tied to user satisfaction. Conventional evaluation metrics for ASR systems produce a singular aggregate score, which is insufficient for understanding specific system vulnerabilities. Therefore, we aim to address the limitations of the previous ASR evaluation methods by introducing the Korean Error Explainable Benchmark Dataset for ASR and Post-processing (KEBAP).... | Seonmin Koo, Chanjun Park, Jinsung Kim, Jaehyung Seo, Sugyeong Eo, Hyeonseok Moon, Heuiseok Lim |  |
| 1487 |  |  [Adaptive Policy with Wait-k Model for Simultaneous Translation](https://doi.org/10.18653/v1/2023.emnlp-main.293) |  | 0 | Simultaneous machine translation (SiMT) requires a robust read/write policy in conjunction with a high-quality translation model. Traditional methods rely on either a fixed wait-k policy coupled with a standalone wait-k translation model, or an adaptive policy jointly trained with the translation model. In this study, we propose a more flexible approach by decoupling the adaptive policy model from the translation model. Our motivation stems from the observation that a standalone multi-path... | Libo Zhao, Kai Fan, Wei Luo, Jing Wu, Shushu Wang, Ziqian Zeng, Zhongqiang Huang |  |
| 1488 |  |  [Cross-Document Event Coreference Resolution on Discourse Structure](https://doi.org/10.18653/v1/2023.emnlp-main.294) |  | 0 | Cross-document event coreference resolution (CD-ECR) is a task of clustering event mentions across multiple documents that refer to the same real-world events. Previous studies usually model the CD-ECR task as a pairwise similarity comparison problem by using different event mention features, and consider the highly similar event mention pairs in the same cluster as coreferent. In general, most of them only consider the local context of event mentions and ignore their implicit global... | Xinyu Chen, Sheng Xu, Peifeng Li, Qiaoming Zhu |  |
| 1489 |  |  [Post-hoc Utterance Refining Method by Entity Mining for Faithful Knowledge Grounded Conversations](https://doi.org/10.18653/v1/2023.emnlp-main.295) |  | 0 | Despite the striking advances in recent language generation performance, model-generated responses have suffered from the chronic problem of hallucinations that are either untrue or unfaithful to a given source. Especially in the task of knowledge grounded conversation, the models are required to generate informative responses, but hallucinated utterances lead to miscommunication. In particular, entity-level hallucination that causes critical misinformation and undesirable conversation is one... | Yoonna Jang, Suhyune Son, Jeongwoo Lee, Junyoung Son, Yuna Hur, Jungwoo Lim, Hyeonseok Moon, Kisu Yang, Heuiseok Lim |  |
| 1490 |  |  [Can We Edit Factual Knowledge by In-Context Learning?](https://doi.org/10.18653/v1/2023.emnlp-main.296) |  | 0 | Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or outdated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs.... | Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, Baobao Chang |  |
| 1491 |  |  [EDIS: Entity-Driven Image Search over Multimodal Web Content](https://doi.org/10.18653/v1/2023.emnlp-main.297) |  | 0 | Making image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce Entity-Driven Image Search (EDIS), a challenging dataset for cross-modal image search in the news domain. EDIS consists of 1 million web images from actual search engine results and curated datasets, with each image paired with a textual description. Unlike datasets that assume a small set... | Siqi Liu, Weixi Feng, TsuJui Fu, Wenhu Chen, William Wang |  |
| 1492 |  |  [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://doi.org/10.18653/v1/2023.emnlp-main.298) |  | 0 | Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses... | Joshua Ainslie, James LeeThorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, Sumit Sanghai |  |
| 1493 |  |  [Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.299) |  | 0 | Recent work has shown that language models (LMs) have strong multi-step (i.e., procedural) reasoning capabilities. However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism. In this paper, we try to answer this question by exploring a mechanistic interpretation of LMs for multi-step reasoning tasks. Concretely, we hypothesize that the LM implicitly embeds a reasoning tree resembling the correct... | Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, Mrinmaya Sachan |  |
| 1494 |  |  [BiasX: "Thinking Slow" in Toxic Content Moderation with Explanations of Implied Social Biases](https://doi.org/10.18653/v1/2023.emnlp-main.300) |  | 0 | Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements’ implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations... | Yiming Zhang, Sravani Nanduri, Liwei Jiang, Tongshuang Wu, Maarten Sap |  |
| 1495 |  |  [Text encoders bottleneck compositionality in contrastive vision-language models](https://doi.org/10.18653/v1/2023.emnlp-main.301) |  | 0 | Performant vision-language (VL) models like CLIP represent captions using a single vector. How much information about language is lost in this bottleneck? We first curate CompPrompts, a set of increasingly compositional image captions that VL models should be able to capture (e.g., single object, to object+property, to multiple interacting objects). Then, we train text-only recovery probes that aim to reconstruct captions from single-vector text representations produced by several VL models.... | Amita Kamath, Jack Hessel, KaiWei Chang |  |
| 1496 |  |  [Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition](https://doi.org/10.18653/v1/2023.emnlp-main.302) |  | 0 | Large Language Models (LLMs) are increasingly being deployed in interactive contexts that involve direct user engagement, such as chatbots and writing assistants. These deployments are increasingly plagued by prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and instead follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of a large-scale resource... | Sander Schulhoff, Jeremy Pinto, Anaum Khan, LouisFrançois Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, Christopher Carnahan, Jordan L. BoydGraber |  |
| 1497 |  |  [MMNMT: Modularizing Multilingual Neural Machine Translation with Flexibly Assembled MoE and Dense Blocks](https://doi.org/10.18653/v1/2023.emnlp-main.303) |  | 0 | Mixture-of-Experts (MoE) based sparse architectures can significantly increase model capacity with sublinear computational overhead, which are hence widely used in massively multilingual neural machine translation (MNMT). However, they are prone to overfitting on low-resource language translation. In this paper, we propose a modularized MNMT framework that is able to flexibly assemble dense and MoE-based sparse modules to achieve the best of both worlds. The training strategy of the modularized... | Shangjie Li, Xiangpeng Wei, Shaolin Zhu, Jun Xie, Baosong Yang, Deyi Xiong |  |
| 1498 |  |  [Localizing Active Objects from Egocentric Vision with Symbolic World Knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.304) |  | 0 | The ability to actively ground task instructions from an egocentric view is crucial for AI agents to accomplish tasks or assist humans virtually. One important step towards this goal is to localize and track key active objects that undergo major state change as a consequence of human actions/interactions to the environment without being told exactly what/where to ground (e.g., localizing and tracking the ‘sponge‘ in video from the instruction “Dip the sponge into the bucket.”). While existing... | TeLin Wu, Yu Zhou, Nanyun Peng |  |
| 1499 |  |  [Introducing Rhetorical Parallelism Detection: A New Task with Datasets, Metrics, and Baselines](https://doi.org/10.18653/v1/2023.emnlp-main.305) |  | 0 | Rhetoric, both spoken and written, involves not only content but also style. One common stylistic tool is parallelism: the juxtaposition of phrases which have the same sequence of linguistic (e.g., phonological, syntactic, semantic) features. Despite the ubiquity of parallelism, the field of natural language processing has seldom investigated it, missing a chance to better understand the nature of the structure, meaning, and intent that humans convey. To address this, we introduce the task of... | Stephen Bothwell, Justin DeBenedetto, Theresa Crnkovich, Hildegund Müller, David Chiang |  |
| 1500 |  |  [Prompting is not a substitute for probability measurements in large language models](https://doi.org/10.18653/v1/2023.emnlp-main.306) |  | 0 | Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models’ probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models’ linguistic... | Jennifer Hu, Roger Levy |  |
| 1501 |  |  [Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings](https://doi.org/10.18653/v1/2023.emnlp-main.307) |  | 0 | Pre-trained language models (PLMs) have ignited a surge in demand for effective fine-tuning techniques, particularly in low-resource domains and languages. Active learning (AL), a set of algorithms designed to decrease labeling costs by minimizing label complexity, has shown promise in confronting the labeling bottleneck. In parallel, adapter modules designed for parameter-efficient fine-tuning (PEFT) have demonstrated notable potential in low-resource settings. However, the interplay between... | Josip Jukic, Jan Snajder |  |
| 1502 |  |  [Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks](https://doi.org/10.18653/v1/2023.emnlp-main.308) |  | 0 | Data contamination has become prevalent and challenging with the rise of models pretrained on large automatically-crawled corpora. For closed models, the training data becomes a trade secret, and even for open models, it is not trivial to detect contamination. Strategies such as leaderboards with hidden answers, or using test data which is guaranteed to be unseen, are expensive and become fragile with time. Assuming that all relevant actors value clean test data and will cooperate to mitigate... | Alon Jacovi, Avi Caciularu, Omer Goldman, Yoav Goldberg |  |
| 1503 |  |  [CoLT5: Faster Long-Range Transformers with Conditional Computation](https://doi.org/10.18653/v1/2023.emnlp-main.309) |  | 0 | Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive – not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both... | Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontañón, Siddhartha Brahma, Yury Zemlyanskiy, David C. Uthus, Mandy Guo, James LeeThorp, Yi Tay, YunHsuan Sung, Sumit Sanghai |  |
| 1504 |  |  [DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.310) |  | 0 | Dialogue State Tracking (DST), a key component of task-oriented conversation systems, represents user intentions by determining the values of pre-defined slots in an ongoing dialogue. Existing approaches use hand-crafted templates and additional slot information to fine-tune and prompt large pre-trained language models and elicit slot values from the dialogue context. Significant manual effort and domain knowledge is required to design effective prompts, limiting the generalizability of these... | Praveen Venkateswaran, Evelyn Duesterwald, Vatche Isahagian |  |
| 1505 |  |  [Cross-Cultural Analysis of Human Values, Morals, and Biases in Folk Tales](https://doi.org/10.18653/v1/2023.emnlp-main.311) |  | 0 | Folk tales are strong cultural and social influences in children’s lives, and they are known to teach morals and values. However, existing studies on folk tales are largely limited to European tales. In our study, we compile a large corpus of over 1,900 tales originating from 27 diverse cultures across six continents. Using a range of lexicons and correlation analyses, we examine how human values, morals, and gender biases are expressed in folk tales across cultures. We discover differences... | Winston Wu, Lu Wang, Rada Mihalcea |  |
| 1506 |  |  [Non-Programmers Can Label Programs Indirectly via Active Examples: A Case Study with Text-to-SQL](https://doi.org/10.18653/v1/2023.emnlp-main.312) |  | 0 | Can non-programmers annotate natural language utterances with complex programs that represent their meaning? We introduce APEL, a framework in which non-programmers select among candidate programs generated by a seed semantic parser (e.g., Codex). Since they cannot understand the candidate programs, we ask them to select indirectly by examining the programs’ input-ouput examples. For each utterance, APEL actively searches for a simple input on which the candidate programs tend to produce... | Ruiqi Zhong, Charlie Snell, Dan Klein, Jason Eisner |  |
| 1507 |  |  [LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers](https://doi.org/10.18653/v1/2023.emnlp-main.313) |  | 0 | Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead... | Theo Olausson, Alex Gu, Benjamin Lipkin, Cedegao E. Zhang, Armando SolarLezama, Joshua B. Tenenbaum, Roger Levy |  |
| 1508 |  |  [Non-autoregressive Streaming Transformer for Simultaneous Translation](https://doi.org/10.18653/v1/2023.emnlp-main.314) |  | 0 | Simultaneous machine translation (SiMT) models are trained to strike a balance between latency and translation quality. However, training these models to achieve high quality while maintaining low latency often leads to a tendency for aggressive anticipation. We argue that such issue stems from the autoregressive architecture upon which most existing SiMT models are built. To address those issues, we propose non-autoregressive streaming Transformer (NAST) which comprises a unidirectional... | Zhengrui Ma, Shaolei Zhang, Shoutao Guo, Chenze Shao, Min Zhang, Yang Feng |  |
| 1509 |  |  [ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing](https://doi.org/10.18653/v1/2023.emnlp-main.315) |  | 0 | English and Chinese, known as resource-rich languages, have witnessed the strong development of transformer-based language models for natural language processing tasks. Although Vietnam has approximately 100M people speaking Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA, performed well on general Vietnamese NLP tasks, including POS tagging and named entity recognition. These pre-trained language models are still limited to Vietnamese social media tasks. In this... | Nam Nguyen, Thang Phan, DucVu Nguyen, Kiet Van Nguyen |  |
| 1510 |  |  [RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.316) |  | 0 | How to identify semantic relations among entities in a document when only a few labeled documents are available? Few-shot document-level relation extraction (FSDLRE) is crucial for addressing the pervasive data scarcity problem in real-world scenarios. Metric-based meta-learning is an effective framework widely adopted for FSDLRE, which constructs class prototypes for classification. However, existing works often struggle to obtain class prototypes with accurate relational semantics: 1) To... | Shiao Meng, Xuming Hu, Aiwei Liu, Shuang Li, Fukun Ma, Yawen Yang, Lijie Wen |  |
| 1511 |  |  [GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.317) |  | 0 | Humans subconsciously engage in geospatial reasoning when reading articles. We recognize place names and their spatial relations in text and mentally associate them with their physical locations on Earth. Although pretrained language models can mimic this cognitive process using linguistic context, they do not utilize valuable geospatial information in large, widely available geographical databases, e.g., OpenStreetMap. This paper introduces GeoLM, a geospatially grounded language model that... | Zekun Li, Wenxuan Zhou, YaoYi Chiang, Muhao Chen |  |
| 1512 |  |  [Cross-Modal Conceptualization in Bottleneck Models](https://doi.org/10.18653/v1/2023.emnlp-main.318) |  | 0 | Concept Bottleneck Models (CBMs) assume that training examples (e.g., x-ray images) are annotated with high-level concepts (e.g., types of abnormalities), and perform classification by first predicting the concepts, followed by predicting the label relying on these concepts. However, the primary challenge in employing CBMs lies in the requirement of defining concepts predictive of the label and annotating training examples with these concepts. In our approach, we adopt a more moderate... | Danis Alukaev, Semen Kiselev, Ilya Pershin, Bulat Ibragimov, Vladimir Ivanov, Alexey Kornaev, Ivan Titov |  |
| 1513 |  |  [LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.319) |  | 0 | The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire... | Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, EePeng Lim, Lidong Bing, Xing Xu, Soujanya Poria, Roy KaWei Lee |  |
| 1514 |  |  [DREAM: Deployment of Recombination and Ensembles in Argument Mining](https://doi.org/10.18653/v1/2023.emnlp-main.320) |  | 0 | Current approaches to Argument Mining (AM) tend to take a holistic or black-box view of the overall pipeline. This paper, in contrast, aims to provide a solution to achieve increased performance based on current components instead of independent all-new solutions. To that end, it presents the Deployment of Recombination and Ensemble methods for Argument Miners (DREAM) framework that allows for the (automated) combination of AM components. Using ensemble methods, DREAM combines sets of AM... | Florian Ruosch, Cristina Sarasua, Abraham Bernstein |  |
| 1515 |  |  [MILDSum: A Novel Benchmark Dataset for Multilingual Summarization of Indian Legal Case Judgments](https://doi.org/10.18653/v1/2023.emnlp-main.321) |  | 0 | Automatic summarization of legal case judgments is a practically important problem that has attracted substantial research efforts in many countries. In the context of the Indian judiciary, there is an additional complexity – Indian legal case judgments are mostly written in complex English, but a significant portion of India’s population lacks command of the English language. Hence, it is crucial to summarize the legal documents in Indian languages to ensure equitable access to justice. While... | Debtanu Datta, Shubham Soni, Rajdeep Mukherjee, Saptarshi Ghosh |  |
| 1516 |  |  [Query Rewriting in Retrieval-Augmented Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.322) |  | 0 | Large Language Models (LLMs) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there... | Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan |  |
| 1517 |  |  [PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation](https://doi.org/10.18653/v1/2023.emnlp-main.323) |  | 0 | Data augmentation is a widely used technique to address the problem of text classification when there is a limited amount of training data. Recent work often tackles this problem using large language models (LLMs) like GPT3 that can generate new examples given already available ones. In this work, we propose a method to generate more helpful augmented data by utilizing the LLM’s abilities to follow instructions and perform few-shot classifications. Our specific PromptMix method consists of two... | Gaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, Issam H. Laradji |  |
| 1518 |  |  [COHESENTIA: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts](https://doi.org/10.18653/v1/2023.emnlp-main.324) |  | 0 | Coherence is a linguistic term that refers to the relations between small textual units (sentences, propositions), which make the text logically consistent and meaningful to the reader. With the advances of generative foundational models in NLP, there is a pressing need to automatically assess the human-perceived coherence of automatically generated texts. Up until now, little work has been done on explicitly assessing the coherence of generated texts and analyzing the factors contributing to... | Aviya Maimon, Reut Tsarfaty |  |
| 1519 |  |  [QUDeval: The Evaluation of Questions Under Discussion Discourse Parsing](https://doi.org/10.18653/v1/2023.emnlp-main.325) |  | 0 | Questions Under Discussion (QUD) is a versatile linguistic framework in which discourse progresses as continuously asking questions and answering them. Automatic parsing of a discourse to produce a QUD structure thus entails a complex question generation task: given a document and an answer sentence, generate a question that satisfies linguistic constraints of QUD and can be grounded in an anchor sentence in prior context. These questions are known to be curiosity-driven and open-ended. This... | Yating Wu, Ritika Mangla, Greg Durrett, Junyi Jessy Li |  |
| 1520 |  |  [PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter](https://doi.org/10.18653/v1/2023.emnlp-main.326) |  | 0 | The Retrieval Question Answering (ReQA) task employs the retrieval-augmented framework, composed of a retriever and generator. The generators formulate the answer based on the documents retrieved by the retriever. Incorporating Large Language Models (LLMs) as generators is beneficial due to their advanced QA capabilities, but they are typically too large to be fine-tuned with budget constraints while some of them are only accessible via APIs. To tackle this issue and further improve ReQA... | Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, Jing Xiao |  |
| 1521 |  |  [Exploring Chain of Thought Style Prompting for Text-to-SQL](https://doi.org/10.18653/v1/2023.emnlp-main.327) |  | 0 | In-context learning with large language models (LLMs) has recently caught increasing attention due to its superior few-shot performance on various tasks. However, its performance on text-to-SQL parsing still has much room for improvement. In this paper, we hypothesize that a crucial aspect of LLMs to improve for text-to-SQL parsing is their multi-step reasoning ability. Thus, we systematically study how to enhance LLMs’ reasoning ability through chain of thought (CoT) style prompting, including... | ChangYu Tai, Ziru Chen, Tianshu Zhang, Xiang Deng, Huan Sun |  |
| 1522 |  |  [Efficient Algorithms for Recognizing Weighted Tree-Adjoining Languages](https://doi.org/10.18653/v1/2023.emnlp-main.328) |  | 0 | The class of tree-adjoining languages can be characterized by various two-level formalisms, consisting of a context-free grammar (CFG) or pushdown automaton (PDA) controlling another CFG or PDA. These four formalisms are equivalent to tree-adjoining grammars (TAG), linear indexed grammars (LIG), pushdown-adjoining automata (PAA), and embedded pushdown automata (EPDA). We define semiring-weighted versions of the above two-level formalisms, and we design new algorithms for computing their... | Alexandra Butoi, Tim Vieira, Ryan Cotterell, David Chiang |  |
| 1523 |  |  [Harnessing Black-Box Control to Boost Commonsense in LM's Generation](https://doi.org/10.18653/v1/2023.emnlp-main.329) |  | 0 | Large language models (LLMs) such as GPT-3 have demonstrated a strong capability to generate coherent and contextually relevant text. However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times. Moreover, fine-tuning the entire LLM towards more commonsensical outputs is computationally expensive if not infeasible. In this paper, we present a computation-efficient framework that steers a frozen Pre-Trained Language Model (PTLM) towards more... | Yufei Tian, Felix Zhang, Nanyun Peng |  |
| 1524 |  |  [Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.330) |  | 0 | A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement... | Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher D. Manning |  |
| 1525 |  |  [Representative Demonstration Selection for In-Context Learning with Two-Stage Determinantal Point Process](https://doi.org/10.18653/v1/2023.emnlp-main.331) |  | 0 | Although In-Context Learning has proven effective across a broad array of tasks, its efficiency is noticeably influenced by the selection of demonstrations. Existing methods tend to select different demonstrations for each test instance, which is time-consuming and poses limitations in practical scenarios. Therefore, this study aims to address the challenge of selecting a representative subset of in-context demonstrations that can effectively prompt different test instances in a specific task.... | Zhao Yang, Yuanzhe Zhang, Dianbo Sui, Cao Liu, Jun Zhao, Kang Liu |  |
| 1526 |  |  [The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.332) |  | 0 | Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might supply the answer “Edinburgh” to “Anne Redpath passed away in X.” and “London” to “Anne Redpath’s life ended in X.” In this work, we identify potential causes of inconsistency and evaluate the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a... | Lovisa Hagström, Denitsa Saynova, Tobias Norlund, Moa Johansson, Richard Johansson |  |
| 1527 |  |  [ViPE: Visualise Pretty-much Everything](https://doi.org/10.18653/v1/2023.emnlp-main.333) |  | 0 | Figurative and non-literal expressions are profoundly integrated in human communication. Visualising such expressions allow us to convey our creative thoughts, and evoke nuanced emotions. Recent text-to-image models like Stable Diffusion, on the other hand, struggle to depict non-literal expressions. Recent works primarily deal with this issue by compiling humanly annotated datasets on a small scale, which not only demands specialized expertise but also proves highly inefficient. To address... | Hassan Shahmohammadi, Adhiraj Ghosh, Hendrik P. A. Lensch |  |
| 1528 |  |  [Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.334) |  | 0 | Document-level Relation Extraction (DocRE), which aims to extract relations from a long context, is a critical challenge in achieving fine-grained structural comprehension and generating interpretable document representations. Inspired by recent advances in in-context learning capabilities emergent from large language models (LLMs), such as ChatGPT, we aim to design an automated annotation method for DocRE with minimum human effort. Unfortunately, vanilla in-context learning is infeasible for... | Junpeng Li, Zixia Jia, Zilong Zheng |  |
| 1529 |  |  [Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.335) |  | 0 | The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs. Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like “I’m sure it’s”, “I think it’s”, or “Wikipedia says it’s” affect models, and whether they contribute to model failures. We... | Kaitlyn Zhou, Dan Jurafsky, Tatsunori Hashimoto |  |
| 1530 |  |  [Elaborative Simplification as Implicit Questions Under Discussion](https://doi.org/10.18653/v1/2023.emnlp-main.336) |  | 0 | Automated text simplification, a technique useful for making text more accessible to people such as children and emergent bilinguals, is often thought of as a monolingual translation task from complex sentences to simplified sentences using encoder-decoder models. This view fails to account for elaborative simplification, where new information is added into the simplified text. This paper proposes to view elaborative simplification through the lens of the Question Under Discussion (QUD)... | Yating Wu, William Sheffield, Kyle Mahowald, Junyi Jessy Li |  |
| 1531 |  |  [EntSUMv2: Dataset, Models and Evaluation for More Abstractive Entity-Centric Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.337) |  | 0 | Entity-centric summarization is a form of controllable summarization that aims to generate a summary for a specific entity given a document. Concise summaries are valuable in various real-life applications, as they enable users to quickly grasp the main points of the document focusing on an entity of interest. This paper presents ENTSUMV2, a more abstractive version of the original entity-centric ENTSUM summarization dataset. In ENTSUMV2 the annotated summaries are intentionally made shorter to... | Dhruv Mehra, Lingjue Xie, Ella HofmannCoyle, Mayank Kulkarni, Daniel PreotiucPietro |  |
| 1532 |  |  [SciRepEval: A Multi-Format Benchmark for Scientific Document Representations](https://doi.org/10.18653/v1/2023.emnlp-main.338) |  | 0 | Learned representations of scientific documents can serve as valuable input features for downstream tasks without further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 24 challenging and realistic tasks, 8 of which are new, across four formats: classification,... | Amanpreet Singh, Mike D'Arcy, Arman Cohan, Doug Downey, Sergey Feldman |  |
| 1533 |  |  [A Diachronic Perspective on User Trust in AI under Uncertainty](https://doi.org/10.18653/v1/2023.emnlp-main.339) |  | 0 | In human-AI collaboration, users typically form a mental model of the AI system, which captures the user’s beliefs about when the system performs well and when it does not. The construction of this mental model is guided by both the system’s veracity as well as the system output presented to the user e.g., the system’s confidence and an explanation for the prediction. However, modern NLP systems are seldom calibrated and are often confidently incorrect about their predictions, which violates... | Shehzaad Dhuliawala, Vilém Zouhar, Mennatallah ElAssady, Mrinmaya Sachan |  |
| 1534 |  |  [CT-GAT: Cross-Task Generative Adversarial Attack based on Transferability](https://doi.org/10.18653/v1/2023.emnlp-main.340) |  | 0 | Neural network models are vulnerable to adversarial examples, and adversarial transferability further increases the risk of adversarial attacks. Current methods based on transferability often rely on substitute models, which can be impractical and costly in real-world scenarios due to the unavailability of training data and the victim model’s structural details. In this paper, we propose a novel approach that directly constructs adversarial examples by extracting transferable features across... | Minxuan Lv, Chengwei Dai, Kun Li, Wei Zhou, Songlin Hu |  |
| 1535 |  |  [Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling](https://doi.org/10.18653/v1/2023.emnlp-main.341) |  | 0 | Topic segmentation is critical for obtaining structured documents and improving down- stream tasks such as information retrieval. Due to its ability of automatically exploring clues of topic shift from abundant labeled data, recent supervised neural models have greatly promoted the development of long document topic segmentation, but leaving the deeper relationship between coherence and topic segmentation underexplored. Therefore, this paper enhances the ability of supervised models to capture... | Hai Yu, Chong Deng, Qinglin Zhang, Jiaqing Liu, Qian Chen, Wen Wang |  |
| 1536 |  |  [Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents](https://doi.org/10.18653/v1/2023.emnlp-main.342) |  | 0 | Human-like chatbots necessitate the use of commonsense reasoning in order to effectively comprehend and respond to implicit information present within conversations. Achieving such coherence and informativeness in responses, however, is a non-trivial task. Even for large language models (LLMs), the task of identifying and aggregating key evidence within a single hop presents a substantial challenge. This complexity arises because such evidence is scattered across multiple turns in a... | Hyungjoo Chae, Yongho Song, Kai Tzuiunn Ong, Taeyoon Kwon, Minjin Kim, Youngjae Yu, Dongha Lee, Dongyeop Kang, Jinyoung Yeo |  |
| 1537 |  |  [Information Value: Measuring Utterance Predictability as Distance from Plausible Alternatives](https://doi.org/10.18653/v1/2023.emnlp-main.343) |  | 0 | We present information value, a measure which quantifies the predictability of an utterance relative to a set of plausible alternatives. We introduce a method to obtain interpretable estimates of information value using neural text generators, and exploit their psychometric predictive power to investigate the dimensions of predictability that drive human comprehension behaviour. Information value is a stronger predictor of utterance acceptability in written and spoken dialogue than aggregates... | Mario Giulianelli, Sarenne Wallbridge, Raquel Fernández |  |
| 1538 |  |  [Generating Commonsense Counterfactuals for Stable Relation Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.344) |  | 0 | Recent studies on counterfactual augmented data have achieved great success in the coarse-grained natural language processing tasks. However, existing methods encounter two major problems when dealing with the fine-grained relation extraction tasks. One is that they struggle to accurately identify causal terms under the invariant entity constraint. The other is that they ignore the commonsense constraint. To solve these problems, we propose a novel framework to generate commonsense... | Xin Miao, Yongqi Li, Tieyun Qian |  |
| 1539 |  |  [C-STS: Conditional Semantic Textual Similarity](https://doi.org/10.18653/v1/2023.emnlp-main.345) |  | 0 | Semantic textual similarity (STS) has been a cornerstone task in NLP that measures the degree of similarity between a pair of sentences, with applications in information retrieval, question answering, and embedding methods. However, it is an inherently ambiguous task, with the sentence similarity depending on the specific aspect of interest. We resolve this ambiguity by proposing a novel task called conditional STS (C-STS) which measures similarity conditioned on an aspect elucidated in natural... | Ameet Deshpande, Carlos E. Jimenez, Howard Chen, Vishvak Murahari, Victoria Graf, Tanmay Rajpurohit, Ashwin Kalyan, Danqi Chen, Karthik Narasimhan |  |
| 1540 |  |  [Cross-lingual Transfer Can Worsen Bias in Sentiment Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.346) |  | 0 | Sentiment analysis (SA) systems are widely deployed in many of the world’s languages, and there is well-documented evidence of demographic bias in these systems. In languages beyond English, scarcer training data is often supplemented with transfer learning using pre-trained models, including multilingual models trained on other languages. In some cases, even supervision data comes from other languages. Does cross-lingual transfer also import new biases? To answer this question, we use... | Seraphina GoldfarbTarrant, Björn Ross, Adam Lopez |  |
| 1541 |  |  [Rumor Detection on Social Media with Crowd Intelligence and ChatGPT-Assisted Networks](https://doi.org/10.18653/v1/2023.emnlp-main.347) |  | 0 | In the era of widespread dissemination through social media, the task of rumor detection plays a pivotal role in establishing a trustworthy and reliable information environment. Nonetheless, existing research on rumor detection confronts several challenges: the limited expressive power of text encoding sequences, difficulties in domain knowledge coverage and effective information extraction with knowledge graph-based methods, and insufficient mining of semantic structural information. To... | Chang Yang, Peng Zhang, Wenbo Qiao, Hui Gao, Jiaming Zhao |  |
| 1542 |  |  [Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?](https://doi.org/10.18653/v1/2023.emnlp-main.348) |  | 0 | Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human’s perception of reality isn’t always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine... | Yichi Zhang, Jiayi Pan, Yuchen Zhou, Rui Pan, Joyce Chai |  |
| 1543 |  |  [Analysing State-Backed Propaganda Websites: a New Dataset and Linguistic Study](https://doi.org/10.18653/v1/2023.emnlp-main.349) |  | 0 | This paper analyses two hitherto unstudied sites sharing state-backed disinformation, Reliable Recent News (rrn.world) and WarOnFakes (waronfakes.com), which publish content in Arabic, Chinese, English, French, German, and Spanish. We describe our content acquisition methodology and perform cross-site unsupervised topic clustering on the resulting multilingual dataset. We also perform linguistic and temporal analysis of the web page translations and topics over time, and investigate articles... | Freddy Heppell, Kalina Bontcheva, Carolina Scarton |  |
| 1544 |  |  [Controllable Contrastive Generation for Multilingual Biomedical Entity Linking](https://doi.org/10.18653/v1/2023.emnlp-main.350) |  | 0 | Multilingual biomedical entity linking (MBEL) aims to map language-specific mentions in the biomedical text to standardized concepts in a multilingual knowledge base (KB) such as Unified Medical Language System (UMLS). In this paper, we propose Con2GEN, a prompt-based controllable contrastive generation framework for MBEL, which summarizes multidimensional information of the UMLS concept mentioned in biomedical text into a natural sentence following a predefined template. Instead of tackling... | Tiantian Zhu, Yang Qin, Qingcai Chen, Xin Mu, Changlong Yu, Yang Xiang |  |
| 1545 |  |  [HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts](https://doi.org/10.18653/v1/2023.emnlp-main.351) |  | 0 | By routing input tokens to only a few split experts, Sparse Mixture-of-Experts has enabled efficient training of large language models. Recent findings suggest that fixing the routers can achieve competitive performance by alleviating the collapsing problem, where all experts eventually learn similar representations. However, this strategy has two key limitations: (i) the policy derived from random routers might be sub-optimal, and (ii) it requires extensive resources during training and... | Truong Do, Le Khiem, Quang Pham, TrungTin Nguyen, ThanhNam Doan, Binh Nguyen, Chenghao Liu, Savitha Ramasamy, Xiaoli Li, Steven C. H. Hoi |  |
| 1546 |  |  [MediaHG: Rethinking Eye-catchy Features in Social Media Headline Generation](https://doi.org/10.18653/v1/2023.emnlp-main.352) |  | 0 | An attractive blog headline on social media platforms can immediately grab readers and trigger more clicks. However, a good headline shall not only contract the main content but also be eye-catchy with domain platform features, which are decided by the website’s users and objectives. With effective headlines, bloggers can obtain more site traffic and profits, while readers can have easier access to topics of interest. In this paper, we propose a disentanglement-based headline generation model:... | Boning Zhang, Yang Yang |  |
| 1547 |  |  [Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata](https://doi.org/10.18653/v1/2023.emnlp-main.353) |  | 0 | While large language models (LLMs) can answer many questions correctly, they can also hallucinate and give wrong answers. Wikidata, with its over 12 billion facts, can be used to ground LLMs to improve their factuality. This paper presents WikiWebQuestions, a high-quality question answering benchmark for Wikidata. Ported over from WebQuestions for Freebase, it consists of real-world data with SPARQL annotation. This paper presents a few-shot sequence-to-sequence semantic parser for Wikidata. We... | Silei Xu, Shicheng Liu, Theo Culhane, Elizaveta Pertseva, MengHsi Wu, Sina J. Semnani, Monica S. Lam |  |
| 1548 |  |  [ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.354) |  | 0 | We explore the use of large language models (LLMs) for zero-shot semantic parsing. Semantic parsing involves mapping natural language utterances to task-specific meaning representations. LLMs are generally trained on publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting. In this work, we propose ZEROTOP, a zero-shot task-oriented parsing method that decomposes semantic parsing problem into a set of abstractive and... | Dheeraj Mekala, Jason Andrew Wolfe, Subhro Roy |  |
| 1549 |  |  [Efficient Grammatical Error Correction Via Multi-Task Training and Optimized Training Schedule](https://doi.org/10.18653/v1/2023.emnlp-main.355) |  | 0 | Progress in neural grammatical error correction (GEC) is hindered by the lack of annotated training data. Sufficient amounts of high-quality manually annotated data are not available, so recent research has relied on generating synthetic data, pretraining on it, and then fine-tuning on real datasets; performance gains have been achieved either by ensembling or by using huge pretrained models such as XXL-T5 as the backbone. In this work, we explore an orthogonal direction: how to use available... | Andrey Bout, Alexander Podolskiy, Sergey I. Nikolenko, Irina Piontkovskaya |  |
| 1550 |  |  [The BLA Benchmark: Investigating Basic Language Abilities of Pre-Trained Multimodal Models](https://doi.org/10.18653/v1/2023.emnlp-main.356) |  | 0 | Despite the impressive performance achieved by pre-trained language-and-vision models in downstream tasks, it remains an open question whether this reflects a proper understanding of image-text interaction. In this work, we explore to what extent they handle basic linguistic constructions—active-passive voice, coordination, and relative clauses—that even preschool children can typically master. We present BLA, a novel, automatically constructed benchmark to evaluate multimodal models on these... | Xinyi Chen, Raquel Fernández, Sandro Pezzelle |  |
| 1551 |  |  [RainProof: An Umbrella to Shield Text Generator from Out-Of-Distribution Data](https://doi.org/10.18653/v1/2023.emnlp-main.357) |  | 0 | Implementing effective control mechanisms to ensure the proper functioning and security of deployed NLP models, from translation to chatbots, is essential. A key ingredient to ensure safe system behaviour is Out-Of-Distribution (OOD) detection, which aims to detect whether an input sample is statistically far from the training distribution. Although OOD detection is a widely covered topic in classification tasks, most methods rely on hidden features output by the encoder. In this work, we focus... | Maxime Darrin, Pablo Piantanida, Pierre Colombo |  |
| 1552 |  |  [KEPL: Knowledge Enhanced Prompt Learning for Chinese Hypernym-Hyponym Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.358) |  | 0 | Modeling hypernym-hyponym (“is-a”) relations is very important for many natural language processing (NLP) tasks, such as classification, natural language inference and relation extraction. Existing work on is-a relation extraction is mostly in the English language environment. Due to the flexibility of language expression and the lack of high-quality Chinese annotation datasets, it is still a challenge to accurately identify such relations from Chinese unstructured texts. To tackle this... | Ningchen Ma, Dong Wang, Hongyun Bao, Lei He, Suncong Zheng |  |
| 1553 |  |  [Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings](https://doi.org/10.18653/v1/2023.emnlp-main.359) |  | 0 | Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks. To address this bias, we propose a simple and efficient unsupervised approach, Diagonal Attention Pooling (Ditto), which weights words with model-based importance estimations and... | Qian Chen, Wen Wang, Qinglin Zhang, Siqi Zheng, Chong Deng, Hai Yu, Jiaqing Liu, Yukun Ma, Chong Zhang |  |
| 1554 |  |  [Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.360) |  | 0 | The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial validation of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under... | Ji Qi, Chuchun Zhang, Xiaozhi Wang, Kaisheng Zeng, Jifan Yu, Jinxin Liu, Lei Hou, Juanzi Li, Xu Bin |  |
| 1555 |  |  [Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions](https://doi.org/10.18653/v1/2023.emnlp-main.361) |  | 0 | The moderation of content on online platforms is usually non-transparent. On Wikipedia, however, this discussion is carried out publicly and editors are encouraged to use the content moderation policies as explanations for making moderation decisions. Currently, only a few comments explicitly mention those policies – 20% of the English ones, but as few as 2% of the German and Turkish comments. To aid in this process of understanding how content is moderated, we construct a novel multilingual... | LucieAimée Kaffee, Arnav Arora, Isabelle Augenstein |  |
| 1556 |  |  [Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding](https://doi.org/10.18653/v1/2023.emnlp-main.362) |  | 0 | To tackle the high inference latency exhibited by autoregressive language models, previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust Early-Exiting... | Sangmin Bae, Jongwoo Ko, Hwanjun Song, SeYoung Yun |  |
| 1557 |  |  [End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions](https://doi.org/10.18653/v1/2023.emnlp-main.363) |  | 0 | End-to-end task-oriented dialogue (EToD) can directly generate responses in an end-to-end fashion without modular training, which attracts escalating popularity. The advancement of deep neural networks, especially the successful use of large pre-trained models, has further led to significant progress in EToD research in recent years. In this paper, we present a thorough review and provide a unified perspective to summarize existing approaches as well as recent trends to advance the development... | Libo Qin, Wenbo Pan, Qiguang Chen, Lizi Liao, Zhou Yu, Yue Zhang, Wanxiang Che, Min Li |  |
| 1558 |  |  [Answering Questions by Meta-Reasoning over Multiple Chains of Thought](https://doi.org/10.18653/v1/2023.emnlp-main.364) |  | 0 | Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for... | Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, Jonathan Berant |  |
| 1559 |  |  [INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.365) |  | 0 | Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics do not provide explicit explanation of their verdict, nor associate the scores with defects in the generated text. To address this limitation, we present INSTRUCTSCORE, a fine-grained explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a... | Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, Lei Li |  |
| 1560 |  |  [Multi-level Contrastive Learning for Script-based Character Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.366) |  | 0 | In this work, we tackle the scenario of understanding characters in scripts, which aims to learn the characters’ personalities and identities from their utterances. We begin by analyzing several challenges in this scenario, and then propose a multi-level contrastive learning framework to capture characters’ global information in a fine-grained manner. To validate the proposed framework, we conduct extensive experiments on three character understanding sub-tasks by comparing with strong... | Dawei Li, Hengyuan Zhang, Yanran Li, Shiping Yang |  |
| 1561 |  |  [CHEF in the Language Kitchen: A Generative Data Augmentation Leveraging Korean Morpheme Ingredients](https://doi.org/10.18653/v1/2023.emnlp-main.367) |  | 0 | Korean morphological variations present unique opportunities and challenges in natural language processing (NLP), necessitating an advanced understanding of morpheme-based sentence construction. The complexity of morphological variations allows for diverse sentence forms based on the syntactic-semantic integration of functional morphemes (i.e., affixes) to lexical morphemes (i.e., roots). With this in mind, we propose a method - CHEF, replicating the morphological transformations inherent in... | Jaehyung Seo, Hyeonseok Moon, Jaewook Lee, Sugyeong Eo, Chanjun Park, Heuiseok Lim |  |
| 1562 |  |  [Automatic Debate Evaluation with Argumentation Semantics and Natural Language Argument Graph Networks](https://doi.org/10.18653/v1/2023.emnlp-main.368) |  | 0 | The lack of annotated data on professional argumentation and complete argumentative debates has led to the oversimplification and the inability of approaching more complex natural language processing tasks. Such is the case of the automatic evaluation of complete professional argumentative debates. In this paper, we propose an original hybrid method to automatically predict the winning stance in this kind of debates. For that purpose, we combine concepts from argumentation theory such as... | Ramon RuizDolz, Stella Heras, Ana GarcíaFornes |  |
| 1563 |  |  [Transfer-Free Data-Efficient Multilingual Slot Labeling](https://doi.org/10.18653/v1/2023.emnlp-main.369) |  | 0 | Slot labeling (SL) is a core component of task-oriented dialogue (TOD) systems, where slots and corresponding values are usually language-, task- and domain-specific. Therefore, extending the system to any new language-domain-task configuration requires (re)running an expensive and resource-intensive data annotation process. To mitigate the inherent data scarcity issue, current research on multilingual ToD assumes that sufficient English-language annotated data are always available for... | Evgeniia Razumovskaia, Ivan Vulic, Anna Korhonen |  |
| 1564 |  |  [Towards Interpretable Mental Health Analysis with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.370) |  | 0 | The latest large language models (LLMs) such as ChatGPT, exhibit strong capabilities in automated mental health analysis. However, existing relevant studies bear several limitations, including inadequate evaluations, lack of prompting strategies, and ignorance of exploring LLMs for explainability. To bridge these gaps, we comprehensively evaluate the mental health analysis and emotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore the effects of different prompting... | Kailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, Ziyan Kuang, Sophia Ananiadou |  |
| 1565 |  |  [Learning to Rank Generation with Pairwise Partial Rewards](https://doi.org/10.18653/v1/2023.emnlp-main.371) |  | 0 | This paper studies the use of reinforcement learning for conditional text generation, which overcomes the limitation of the prevalent supervised maximum likelihood estimation approach. However, it still suffers from challenges including the large action space and the delayed reward, as the reward can be computed only after an entire sequence is generated. To address these challenges, we propose a method that provides partial rewards for intermediate actions taken on partial sequences. This... | Youngwon Lee, Jinu Lee, Seungwon Hwang |  |
| 1566 |  |  [GreedyCAS: Unsupervised Scientific Abstract Segmentation with Normalized Mutual Information](https://doi.org/10.18653/v1/2023.emnlp-main.372) |  | 0 | The abstracts of scientific papers typically contain both premises (e.g., background and observations) and conclusions. Although conclusion sentences are highlighted in structured abstracts, in non-structured abstracts the concluding information is not explicitly marked, which makes the automatic segmentation of conclusions from scientific abstracts a challenging task. In this work, we explore Normalized Mutual Information (NMI) as a means for abstract segmentation. We consider each abstract as... | Yingqiang Gao, Jessica Lam, Nianlong Gu, Richard H. R. Hahnloser |  |
| 1567 |  |  [Spoiler Detection as Semantic Text Matching](https://doi.org/10.18653/v1/2023.emnlp-main.373) |  | 0 | Engaging with discussion of TV shows online often requires individuals to refrain from consuming show-related content for extended periods to avoid spoilers. While existing research on spoiler detection shows promising results in safeguarding viewers from general spoilers, it fails to address the issue of users abstaining from show-related content during their watch. This is primarily because the definition of a spoiler varies depending on the viewer’s progress in the show, and conventional... | Ryan Tran, Canwen Xu, Julian J. McAuley |  |
| 1568 |  |  [Multimodal Embodied Plan Prediction Augmented with Synthetic Embodied Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.374) |  | 0 | Embodied task completion is a challenge where an agent in a simulated environment must predict environment actions to complete tasks based on natural language instructions and ego-centric visual observations. We propose a variant of this problem where the agent predicts actions at a higher level of abstraction called a plan, which helps make agent actions more interpretable and can be obtained from the appropriate prompting of large language models. We show that multimodal transformer models... | Aishwarya Padmakumar, Mert Inan, Spandana Gella, Patrick Lange, Dilek HakkaniTur |  |
| 1569 |  |  [GEM: Gestalt Enhanced Markup Language Model for Web Understanding via Render Tree](https://doi.org/10.18653/v1/2023.emnlp-main.375) |  | 0 | Inexhaustible web content carries abundant perceptible information beyond text. Unfortunately, most prior efforts in pre-trained Language Models (LMs) ignore such cyber-richness, while few of them only employ plain HTMLs, and crucial information in the rendered web, such as visual, layout, and style, are excluded. Intuitively, those perceptible web information can provide essential intelligence to facilitate content understanding tasks. This study presents an innovative Gestalt Enhanced Markup... | Zirui Shao, Feiyu Gao, Zhongda Qi, Hangdi Xing, Jiajun Bu, Zhi Yu, Qi Zheng, Xiaozhong Liu |  |
| 1570 |  |  [Abstractive Open Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.376) |  | 0 | Open Information Extraction (OpenIE) is a traditional NLP task that extracts structured information from unstructured text to be used for other downstream applications. Traditionally, OpenIE focuses on extracting the surface forms of relations as they appear in the raw text, which we term extractive OpenIE. One of the main drawbacks of this approach is that implicit semantic relations (inferred relations) can not be extracted, compromising the performance of downstream applications. In this... | Kevin Pei, Ishan Jindal, Kevin ChenChuan Chang |  |
| 1571 |  |  [CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network](https://doi.org/10.18653/v1/2023.emnlp-main.377) |  | 0 | The tremendous growth of social media users interacting in online conversations has led to significant growth in hate speech affecting people from various demographics. Most of the prior works focus on detecting explicit hate speech, which is overt and leverages hateful phrases, with very little work focusing on detecting hate speech that is implicit or denotes hatred through indirect or coded language. In this paper, we present CoSyn, a context synergized neural network that explicitly... | Sreyan Ghosh, Manan Suri, Purva Chiniya, Utkarsh Tyagi, Sonal Kumar, Dinesh Manocha |  |
| 1572 |  |  [CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.emnlp-main.378) |  | 0 | Evaluating the performance of Grammatical Error Correction (GEC) systems is a challenging task due to its subjectivity. Designing an evaluation metric that is as objective as possible is crucial to the development of GEC task. However, mainstream evaluation metrics, i.e., reference-based metrics, introduce bias into the multi-reference evaluation by extracting edits without considering the presence of multiple references. To overcome this issue, we propose Chunk-LE Multi-reference Evaluation... | Jingheng Ye, Yinghui Li, Qingyu Zhou, Yangning Li, Shirong Ma, HaiTao Zheng, Ying Shen |  |
| 1573 |  |  [Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods](https://doi.org/10.18653/v1/2023.emnlp-main.379) |  | 0 | Feature attribution scores are used for explaining the prediction of a text classifier to users by highlighting a k number of tokens. In this work, we propose a way to determine the number of optimal k tokens that should be displayed from sequential properties of the attribution scores. Our approach is dynamic across sentences, method-agnostic, and deals with sentence length bias. We compare agreement between multiple methods and humans on an NLI task, using fixed k and dynamic k. We find that... | Jonathan Kamp, Lisa Beinborn, Antske Fokkens |  |
| 1574 |  |  [SentiStream: A Co-Training Framework for Adaptive Online Sentiment Analysis in Evolving Data Streams](https://doi.org/10.18653/v1/2023.emnlp-main.380) |  | 0 | Online sentiment analysis has emerged as a crucial component in numerous data-driven applications, including social media monitoring, customer feedback analysis, and online reputation management. Despite their importance, current methodologies falter in effectively managing the continuously evolving nature of data streams, largely due to their reliance on substantial, pre-existing labelled datasets. This paper presents sentistream, a novel co-training framework specifically designed for... | Yuhao Wu, Karthick Sharma, Chun Seah, Shuhao Zhang |  |
| 1575 |  |  [HyperNetwork-based Decoupling to Improve Model Generalization for Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.381) |  | 0 | Few-shot relation extraction (FSRE) aims to train a model that can deal with new relations using only a few labeled examples. Most existing studies employ Prototypical Networks for FSRE, which usually overfits the relation classes in the training set and cannot generalize well to unseen relations. By investigating the class separation of an FSRE model, we find that model upper layers are prone to learn relation-specific knowledge. Therefore, in this paper, we propose a HyperNetwork-based... | Liang Zhang, Chulun Zhou, Fandong Meng, Jinsong Su, Yidong Chen, Jie Zhou |  |
| 1576 |  |  [Solving Hard Analogy Questions with Relation Embedding Chains](https://doi.org/10.18653/v1/2023.emnlp-main.382) |  | 0 | Modelling how concepts are related is a central topic in Lexical Semantics. A common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to model the relation between two concepts as a set of paths. However, KGs are limited to a fixed set of relation types, and they are incomplete and often noisy. Another strategy is to distill relation embeddings from a fine-tuned language model. However, this is less suitable for words that are only indirectly related and it does not readily... | Nitesh Kumar, Steven Schockaert |  |
| 1577 |  |  [Modeling Empathic Similarity in Personal Narratives](https://doi.org/10.18653/v1/2023.emnlp-main.383) |  | 0 | The most meaningful connections between people are often fostered through expression of shared vulnerability and emotional experiences in personal narratives. We introduce a new task of identifying similarity in personal stories based on empathic resonance, i.e., the extent to which two people empathize with each others’ experiences, as opposed to raw semantic or lexical similarity, as has predominantly been studied in NLP. Using insights from social psychology, we craft a framework that... | Jocelyn Shen, Maarten Sap, Pedro ColonHernandez, Hae Park, Cynthia Breazeal |  |
| 1578 |  |  [Tree Prompting: Efficient Task Adaptation without Fine-Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.384) |  | 0 | Prompting language models (LMs) is the main interface for applying them to new tasks. However, for smaller LMs, prompting provides low accuracy compared to gradient-based fine-tuning. Tree Prompting is an approach to prompting which builds a decision tree of prompts, linking multiple prompt-LM calls together to solve a task. At inference time, each call to the LM is determined by efficiently routing the outcome of the previous call using the tree. Experiments on classification datasets show... | Chandan Singh, John X. Morris, Alexander M. Rush, Jianfeng Gao, Yuntian Deng |  |
| 1579 |  |  [Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data](https://doi.org/10.18653/v1/2023.emnlp-main.385) |  | 0 | Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large... | Canwen Xu, Daya Guo, Nan Duan, Julian J. McAuley |  |
| 1580 |  |  [Empathy Intent Drives Empathy Detection](https://doi.org/10.18653/v1/2023.emnlp-main.386) |  | 0 | Empathy plays an important role in the human dialogue. Detecting the empathetic direction expressed by the user is necessary for empathetic dialogue systems because it is highly relevant to understanding the user’s needs. Several studies have shown that empathy intent information improves the ability to response capacity of empathetic dialogue. However, the interaction between empathy detection and empathy intent recognition has not been explored. To this end, we invite 3 experts to manually... | Liting Jiang, Di Wu, Bohui Mao, Yanbing Li, Wushour Slamu |  |
| 1581 |  |  [Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling](https://doi.org/10.18653/v1/2023.emnlp-main.387) |  | 0 | Recently slot filling has witnessed great development thanks to deep learning and the availability of large-scale annotated data. However, it poses a critical challenge to handle a novel domain whose samples are never seen during training. The recognition performance might be greatly degraded due to severe domain shifts. Most prior works deal with this problem in a two-pass pipeline manner based on metric learning. In practice, these dominant pipeline models may be limited in computational... | Yuanjun Shi, Linzhi Wu, Minglai Shao |  |
| 1582 |  |  [BasahaCorpus: An Expanded Linguistic Resource for Readability Assessment in Central Philippine Languages](https://doi.org/10.18653/v1/2023.emnlp-main.388) |  | 0 | Current research on automatic readability assessment (ARA) has focused on improving the performance of models in high-resource languages such as English. In this work, we introduce and release BasahaCorpus as part of an initiative aimed at expanding available corpora and baseline models for readability assessment in lower resource languages in the Philippines. We compiled a corpus of short fictional narratives written in Hiligaynon, Minasbate, Karay-a, and Rinconada—languages belonging to the... | Joseph Marvin Imperial, Ekaterina Kochmar |  |
| 1583 |  |  [ReTAG: Reasoning Aware Table to Analytic Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.389) |  | 0 | The task of table summarization involves generating text that both succinctly and accurately represents the table or a specific set of highlighted cells within a table. While significant progress has been made in table to text generation techniques, models still mostly generate descriptive summaries, which reiterates the information contained within the table in sentences. Through analysis of popular table to text benchmarks (ToTTo (Parikh et al., 2020 and InfoTabs (Gupta et al., 2020) we... | Deepanway Ghosal, Preksha Nema, Aravindan Raghuveer |  |
| 1584 |  |  [Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators](https://doi.org/10.18653/v1/2023.emnlp-main.390) |  | 0 | Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives – Factuality,... | Liang Chen, Yang Deng, Yatao Bian, Zeyu Qin, Bingzhe Wu, TatSeng Chua, KamFai Wong |  |
| 1585 |  |  [Compressing Context to Enhance Inference Efficiency of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.391) |  | 0 | Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM’s fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the... | Yucheng Li, Bo Dong, Frank Guerin, Chenghua Lin |  |
| 1586 |  |  [MoT: Memory-of-Thought Enables ChatGPT to Self-Improve](https://doi.org/10.18653/v1/2023.emnlp-main.392) |  | 0 | Large Language Models (LLMs) have shown impressive abilities on various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, humans can easily improve themselves by self-thinking and memory, without external resources. In this paper, we propose a framework, \*\*MoT\*\*, to let the LLM self-improve through \*\*M\*\*emory \*\*o\*\*f \*\*T\*\*houghts, without annotated datasets and parameter updates. Specifically,... | Xiaonan Li, Xipeng Qiu |  |
| 1587 |  |  [4 and 7-bit Labeling for Projective and Non-Projective Dependency Trees](https://doi.org/10.18653/v1/2023.emnlp-main.393) |  | 0 | We introduce an encoding for parsing as sequence labeling that can represent any projective dependency tree as a sequence of 4-bit labels, one per word. The bits in each word’s label represent (1) whether it is a right or left dependent, (2) whether it is the outermost (left/right) dependent of its parent, (3) whether it has any left children and (4) whether it has any right children. We show that this provides an injective mapping from trees to labels that can be encoded and decoded in linear... | Carlos GómezRodríguez, Diego Roca, David Vilares |  |
| 1588 |  |  [Can You Follow Me? Testing Situational Understanding for ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.394) |  | 0 | Understanding sentence meanings and updating information states appropriately across time—what we call “situational understanding” (SU)—is a critical ability for human-like AI agents. SU is essential in particular for chat models, such as ChatGPT, to enable consistent, coherent, and effective dialogue between humans and AI. Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs), but the extent and causes of these limitations are not well understood,... | Chenghao Yang, Allyson Ettinger |  |
| 1589 |  |  [Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4](https://doi.org/10.18653/v1/2023.emnlp-main.395) |  | 0 | Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4... | Kellin Pelrine, Anne Imouza, Camille Thibault, Meilina Reksoprodjo, Caleb Gupta, Joel Christoph, JeanFrançois Godbout, Reihaneh Rabbany |  |
| 1590 |  |  [Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation](https://doi.org/10.18653/v1/2023.emnlp-main.396) |  | 0 | Grammatical error correction (GEC) is a well-explored problem in English with many existing models and datasets. However, research on GEC in morphologically rich languages has been limited due to challenges such as data scarcity and language complexity. In this paper, we present the first results on Arabic GEC using two newly developed Transformer-based pretrained sequence-to-sequence models. We also define the task of multi-class Arabic grammatical error detection (GED) and present the first... | Bashar Alhafni, Go Inoue, Christian Khairallah, Nizar Habash |  |
| 1591 |  |  [HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.397) |  | 0 | Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HaluEval) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To... | Junyi Li, Xiaoxue Cheng, Xin Zhao, JianYun Nie, JiRong Wen |  |
| 1592 |  |  [Enabling Large Language Models to Generate Text with Citations](https://doi.org/10.18653/v1/2023.emnlp-main.398) |  | 0 | Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs’ Citation... | Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen |  |
| 1593 |  |  [Revisiting Machine Translation for Cross-lingual Classification](https://doi.org/10.18653/v1/2023.emnlp-main.399) |  | 0 | Machine Translation (MT) has been widely used for cross-lingual classification, either by translating the test set into English and running inference with a monolingual model (translate-test), or translating the training set into the target languages and finetuning a multilingual model (translate-train). However, most research in the area focuses on the multilingual models rather than the MT component. We show that, by using a stronger MT system and mitigating the mismatch between training on... | Mikel Artetxe, Vedanuj Goswami, Shruti Bhosale, Angela Fan, Luke Zettlemoyer |  |
| 1594 |  |  [Pre-Trained Language Models Augmented with Synthetic Scanpaths for Natural Language Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.400) |  | 0 | Human gaze data offer cognitive information that reflects natural language comprehension. Indeed, augmenting language models with human scanpaths has proven beneficial for a range of NLP tasks, including language understanding. However, the applicability of this approach is hampered because the abundance of text corpora is contrasted by a scarcity of gaze data. Although models for the generation of human-like scanpaths during reading have been developed, the potential of synthetic gaze data... | Shuwen Deng, Paul Prasse, David R. Reich, Tobias Scheffer, Lena A. Jäger |  |
| 1595 |  |  [Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model](https://doi.org/10.18653/v1/2023.emnlp-main.401) |  | 0 | Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human... | Leonie Weissweiler, Valentin Hofmann, Anjali Kantharuban, Anna Cai, Ritam Dutt, Amey Hengle, Anubha Kabra, Atharva Kulkarni, Abhishek Vijayakumar, Haofei Yu, Hinrich Schütze, Kemal Oflazer, David R. Mortensen |  |
| 1596 |  |  [Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.402) |  | 0 | Large language models (LLMs) have showcased their capability with few-shot inference known as in-context learning. However, in-domain demonstrations are not always readily available in real scenarios, leading to cross-domain in-context learning. Besides, LLMs are still facing challenges in long-tail knowledge in unseen and unfamiliar domains. The above limitations demonstrate the necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study the UDA problem under an in-context... | Quanyu Long, Wenya Wang, Sinno Jialin Pan |  |
| 1597 |  |  [Understanding the Inner-workings of Language Models Through Representation Dissimilarity](https://doi.org/10.18653/v1/2023.emnlp-main.403) |  | 0 | As language models are applied to an increasing number of real-world applications, understanding their inner workings has become an important issue in model trust, interpretability, and transparency. In this work we show that representation dissimilarity measures, which are functions that measure the extent to which two model’s internal representations differ, can be a valuable tool for gaining insight into the mechanics of language models. Among our insights are: (i) an apparent asymmetry in... | Davis Brown, Charles Godfrey, Nicholas Konz, Jonathan H. Tu, Henry Kvinge |  |
| 1598 |  |  [Efficient Classification of Long Documents via State-Space Models](https://doi.org/10.18653/v1/2023.emnlp-main.404) |  | 0 | Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tackling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space... | Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, Ivan Kobyzev |  |
| 1599 |  |  [Dual-Feedback Knowledge Retrieval for Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2023.emnlp-main.405) |  | 0 | Efficient knowledge retrieval plays a pivotal role in ensuring the success of end-to-end task-oriented dialogue systems by facilitating the selection of relevant information necessary to fulfill user requests. However, current approaches generally integrate knowledge retrieval and response generation, which poses scalability challenges when dealing with extensive knowledge bases. Taking inspiration from open-domain question answering, we propose a retriever-generator architecture that harnesses... | Tianyuan Shi, Liangzhi Li, Zijian Lin, Tao Yang, Xiaojun Quan, Qifan Wang |  |
| 1600 |  |  [Construction Artifacts in Metaphor Identification Datasets](https://doi.org/10.18653/v1/2023.emnlp-main.406) |  | 0 | Metaphor identification aims at understanding whether a given expression is used figuratively in context. However, in this paper we show how existing metaphor identification datasets can be gamed by fully ignoring the potential metaphorical expression or the context in which it occurs. We test this hypothesis in a variety of datasets and settings, and show that metaphor identification systems based on language models without complete information can be competitive with those using the full... | Joanne Boisson, Luis Espinosa Anke, José CamachoCollados |  |
| 1601 |  |  [MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.407) |  | 0 | Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through \*self-improvement\* using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in... | Deepak Nathani, David Wang, Liangming Pan, William Yang Wang |  |
| 1602 |  |  [Granularity Matters: Pathological Graph-driven Cross-modal Alignment for Brain CT Report Generation](https://doi.org/10.18653/v1/2023.emnlp-main.408) |  | 0 | The automatic Brain CT reports generation can improve the efficiency and accuracy of diagnosing cranial diseases. However, current methods are limited by 1) coarse-grained supervision: the training data in image-text format lacks detailed supervision for recognizing subtle abnormalities, and 2) coupled cross-modal alignment: visual-textual alignment may be inevitably coupled in a coarse-grained manner, resulting in tangled feature representation for report generation. In this paper, we propose... | Yanzhao Shi, Junzhong Ji, Xiaodan Zhang, Liangqiong Qu, Ying Liu |  |
| 1603 |  |  [Enhancing Structured Evidence Extraction for Fact Verification](https://doi.org/10.18653/v1/2023.emnlp-main.409) |  | 0 | Open-domain fact verification is the task of verifying claims in natural language texts against extracted evidence. FEVEROUS is a benchmark that requires extracting and integrating both unstructured and structured evidence to verify a given claim. Previous models suffer from low recall of structured evidence extraction, i.e., table extraction and cell selection. In this paper, we propose a simple but effective method to enhance the extraction of structured evidence by leveraging the row and... | Zirui Wu, Nan Hu, Yansong Feng |  |
| 1604 |  |  [Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models](https://doi.org/10.18653/v1/2023.emnlp-main.410) |  | 0 | Keyphrase Generation (KPG) is a longstanding task in NLP with widespread applications. The advent of sequence-to-sequence (seq2seq) pre-trained language models (PLMs) has ushered in a transformative era for KPG, yielding promising performance improvements. However, many design decisions remain unexplored and are often made arbitrarily. This paper undertakes a systematic analysis of the influence of model selection and decoding strategies on PLM-based KPG. We begin by elucidating why seq2seq... | Di Wu, Wasi Uddin Ahmad, KaiWei Chang |  |
| 1605 |  |  [A Fair and In-Depth Evaluation of Existing End-to-End Entity Linking Systems](https://doi.org/10.18653/v1/2023.emnlp-main.411) |  | 0 | Existing evaluations of entity linking systems often say little about how the system is going to perform for a particular application. There are two fundamental reasons for this. One is that many evaluations only use aggregate measures (like precision, recall, and F1 score), without a detailed error analysis or a closer look at the results. The other is that all of the widely used benchmarks have strong biases and artifacts, in particular: a strong focus on named entities, an unclear or missing... | Hannah Bast, Matthias Hertel, Natalie Prange |  |
| 1606 |  |  [A Multi-Task Dataset for Assessing Discourse Coherence in Chinese Essays: Structure, Theme, and Logic Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.412) |  | 0 | This paper introduces the Chinese Essay Discourse Coherence Corpus (CEDCC), a multi-task dataset for assessing discourse coherence. Existing research tends to focus on isolated dimensions of discourse coherence, a gap which the CEDCC addresses by integrating coherence grading, topical continuity, and discourse relations. This approach, alongside detailed annotations, captures the subtleties of real-world texts and stimulates progress in Chinese discourse coherence analysis. Our contributions... | Hongyi Wu, Xinshu Shen, Man Lan, Shaoguang Mao, Xiaopeng Bai, Yuanbin Wu |  |
| 1607 |  |  [SKD-NER: Continual Named Entity Recognition via Span-based Knowledge Distillation with Reinforcement Learning](https://doi.org/10.18653/v1/2023.emnlp-main.413) |  | 0 | Continual learning for named entity recognition (CL-NER) aims to enable models to continuously learn new entity types while retaining the ability to recognize previously learned ones. However, the current strategies fall short of effectively addressing the catastrophic forgetting of previously learned entity types. To tackle this issue, we propose the SKD-NER model, an efficient continual learning NER model based on the span-based approach, which innovatively incorporates reinforcement learning... | Yi Chen, Liang He |  |
| 1608 |  |  [Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation](https://doi.org/10.18653/v1/2023.emnlp-main.414) |  | 0 | Lifelong sequence generation (LSG), a problem in continual learning, aims to continually train a model on a sequence of generation tasks to learn constantly emerging new generation patterns while avoiding the forgetting of previous knowledge. Existing LSG methods mainly focus on maintaining old knowledge while paying little attention to knowledge transfer across tasks. In contrast, humans can better learn new tasks by leveraging previously acquired knowledge from similar tasks. Inspired by the... | Chengwei Qin, Chen Chen, Shafiq Joty |  |
| 1609 |  |  [When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.415) |  | 0 | Though majority vote among annotators is typically used for ground truth labels in machine learning, annotator disagreement in tasks such as hate speech detection may reflect systematic differences in opinion across groups, not noise. Thus, a crucial problem in hate speech detection is determining if a statement is offensive to the demographic group that it targets, when that group may be a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on... | Eve Fleisig, Rediet Abebe, Dan Klein |  |
| 1610 |  |  [Lazy-k Decoding: Constrained Decoding for Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.416) |  | 0 | We explore the possibility of improving probabilistic models in structured prediction. Specifically, we combine the models with constrained decoding approaches in the context of token classification for information extraction. The decoding methods search for constraint-satisfying label-assignments while maximizing the total probability. To do this, we evaluate several existing approaches, as well as propose a novel decoding method called Lazy-k. Our findings demonstrate that constrained... | Arthur Hemmer, Mickaël Coustaty, Nicola Bartolo, Jérôme Brachat, JeanMarc Ogier |  |
| 1611 |  |  [Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation](https://doi.org/10.18653/v1/2023.emnlp-main.417) |  | 0 | With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn. However, such standard distillation approach neglects the merits and conditions of the student model. Inspired by modern teaching principles, we design a personalised distillation process, in... | Hailin Chen, Amrita Saha, Steven ChuHong Hoi, Shafiq Joty |  |
| 1612 |  |  [Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.418) |  | 0 | Temporal reasoning represents a vital component of human communication and understanding, yet remains an underexplored area within the context of Large Language Models (LLMs). Despite LLMs demonstrating significant proficiency in a range of tasks, a comprehensive, large-scale analysis of their temporal reasoning capabilities is missing. Our paper addresses this gap, presenting the first extensive benchmarking of LLMs on temporal reasoning tasks. We critically evaluate 8 different LLMs across 6... | Raghav Jain, Daivik Sojitra, Arkadeep Acharya, Sriparna Saha, Adam Jatowt, Sandipan Dandapat |  |
| 1613 |  |  [Comparing Styles across Languages](https://doi.org/10.18653/v1/2023.emnlp-main.419) |  | 0 | Understanding how styles differ across languages is advantageous for training both humans and computers to generate culturally appropriate text. We introduce an explanation framework to extract stylistic differences from multilingual LMs and compare styles across languages. Our framework (1) generates comprehensive style lexica in any language and (2) consolidates feature importances from LMs into comparable lexical categories. We apply this framework to compare politeness, creating the first... | Shreya Havaldar, Matthew Pressimone, Eric Wong, Lyle H. Ungar |  |
| 1614 |  |  [Event Causality Extraction via Implicit Cause-Effect Interactions](https://doi.org/10.18653/v1/2023.emnlp-main.420) |  | 0 | Event Causality Extraction (ECE) aims to extract the cause-effect event pairs from the given text, which requires the model to possess a strong reasoning ability to capture event causalities. However, existing works have not adequately exploited the interactions between the cause and effect event that could provide crucial clues for causality reasoning. To this end, we propose an Implicit Cause-Effect interaction (ICE) framework, which formulates ECE as a template-based conditional generation... | Jintao Liu, Zequn Zhang, Kaiwen Wei, Zhi Guo, Xian Sun, Li Jin, Xiaoyu Li |  |
| 1615 |  |  [Evaluation of African American Language Bias in Natural Language Generation](https://doi.org/10.18653/v1/2023.emnlp-main.421) |  | 0 | While biases disadvantaging African American Language (AAL) have been uncovered in models for tasks such as speech recognition and toxicity detection, there has been little investigation of these biases for language generation models like ChatGPT. We evaluate how well LLMs understand AAL in comparison to White Mainstream English (WME), the encouraged “standard” form of English taught in American classrooms. We measure large language model performance on two tasks: a counterpart generation task,... | Nicholas Deas, Jessica Grieser, Shana Kleiner, Desmond Patton, Elsbeth Turcan, Kathleen R. McKeown |  |
| 1616 |  |  [A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2023.emnlp-main.422) |  | 0 | Achieving robust language technologies that can perform well across the world’s many languages is a central goal of multilingual NLP. In this work, we take stock of and empirically analyse task performance disparities that exist between multilingual task-oriented dialogue (ToD) systems. We first define new quantitative measures of absolute and relative equivalence in system performance, capturing disparities across languages and within individual languages. Through a series of controlled... | Songbo Hu, Han Zhou, Moy Yuan, Milan Gritta, Guchun Zhang, Ignacio Iacobacci, Anna Korhonen, Ivan Vulic |  |
| 1617 |  |  [Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction](https://doi.org/10.18653/v1/2023.emnlp-main.423) |  | 0 | Phonological reconstruction is one of the central problems in historical linguistics where a proto-word of an ancestral language is determined from the observed cognate words of daughter languages. Computational approaches to historical linguistics attempt to automate the task by learning models on available linguistic data. Several ideas and techniques drawn from computational biology have been successfully applied in this area of computational historical linguistics. Following these lines, we... | V. S. D. S. Mahesh Akavarapu, Arnab Bhattacharya |  |
| 1618 |  |  [Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning](https://doi.org/10.18653/v1/2023.emnlp-main.424) |  | 0 | While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g., GPT-4). We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as... | Ximing Lu, Faeze Brahman, Peter West, Jaehun Jung, Khyathi Raghavi Chandu, Abhilasha Ravichander, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, Nouha Dziri, Jillian Fisher, Bill Y. Lin, Skyler Hallinan, Lianhui Qin, Xiang Ren, Sean Welleck, Yejin Choi |  |
| 1619 |  |  [Weakly Supervised Semantic Parsing with Execution-based Spurious Program Filtering](https://doi.org/10.18653/v1/2023.emnlp-main.425) |  | 0 | The problem of spurious programs is a longstanding challenge when training a semantic parser from weak supervision. To eliminate such programs that have wrong semantics but correct denotation, existing methods focus on exploiting similarities between examples based on domain-specific knowledge. In this paper, we propose a domain-agnostic filtering mechanism based on program execution results. Specifically, for each program obtained through the search process, we first construct a representation... | Kangil Lee, Segwang Kim, Kyomin Jung |  |
| 1620 |  |  [Taxonomy Expansion for Named Entity Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.426) |  | 0 | Training a Named Entity Recognition (NER) model often involves fixing a taxonomy of entity types. However, requirements evolve and we might need the NER model to recognize additional entity types. A simple approach is to re-annotate entire dataset with both existing and additional entity types and then train the model on the re-annotated dataset. However, this is an extremely laborious task. To remedy this, we propose a novel approach called Partial Label Model (PLM) that uses only partially... | Karthikeyan K, Yogarshi Vyas, Jie Ma, Giovanni Paolini, Neha Anna John, Shuai Wang, Yassine Benajiba, Vittorio Castelli, Dan Roth, Miguel Ballesteros |  |
| 1621 |  |  [Rather a Nurse than a Physician - Contrastive Explanations under Investigation](https://doi.org/10.18653/v1/2023.emnlp-main.427) |  | 0 | Contrastive explanations, where one decision is explained \*in contrast to another\*, are supposed to be closer to how humans explain a decision than non-contrastive explanations, where the decision is not necessarily referenced to an alternative. This claim has never been empirically validated. We analyze four English text-classification datasets (SST2, DynaSent, BIOS and DBpedia-Animals). We fine-tune and extract explanations from three different models (RoBERTa, GTP-2, and T5), each in three... | Oliver Eberle, Ilias Chalkidis, Laura Cabello, Stephanie Brandl |  |
| 1622 |  |  [EtiCor: Corpus for Analyzing LLMs for Etiquettes](https://doi.org/10.18653/v1/2023.emnlp-main.428) |  | 0 | Etiquettes are an essential ingredient of day-to-day interactions among people. Moreover, etiquettes are region-specific, and etiquettes in one region might contradict those in other regions. In this paper, we propose EtiCor, an Etiquettes Corpus, having texts about social norms from five different regions across the globe. The corpus provides a test bed for evaluating LLMs for knowledge and understanding of region-specific etiquettes. Additionally, we propose the task of Etiquette Sensitivity.... | Ashutosh Dwivedi, Pradhyumna Lavania, Ashutosh Modi |  |
| 1623 |  |  [An Investigation of LLMs' Inefficacy in Understanding Converse Relations](https://doi.org/10.18653/v1/2023.emnlp-main.429) |  | 0 | Large Language Models (LLMs) have achieved remarkable success in many formal language oriented tasks, such as structural data-to-text and semantic parsing. However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages. In this paper, we investigate this problem on a special case, converse binary relation. We introduce a new benchmark ConvRe focusing... | Chengwen Qi, Bowen Li, Binyuan Hui, Bailin Wang, Jinyang Li, Jinwang Wu, Yuanjun Laili |  |
| 1624 |  |  [Towards Low-Resource Automatic Program Repair with Meta-Learning and Pretrained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.430) |  | 0 | Automatic program repair (APR) has gained increasing attention as an essential technique in software development to reduce manual debugging efforts and boost developers’ productivity. Recent advances in deep learning (DL) based models have demonstrated promising results by learning from large-scale bug-fix examples in a data-driven manner. However, in practical scenarios, software bugs have an imbalanced distribution, and the fixing knowledge learned by APR models often only capture the... | Weishi Wang, Yue Wang, Steven C. H. Hoi, Shafiq Joty |  |
| 1625 |  |  [ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters](https://doi.org/10.18653/v1/2023.emnlp-main.431) |  | 0 | We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via the use of language adapters (LAs). Most of the earlier works have explored training with adapter of a single source (often English), and testing either using the target LA or LA of another related language. Training target LA requires unlabeled data, which may not be readily available for low resource \*unseen\* languages: those that are neither seen by the underlying multilingual language model (e.g., mBERT), nor do we... | Vipul Rathore, Rajdeep Dhingra, Parag Singla, Mausam |  |
| 1626 |  |  [Log-FGAER: Logic-Guided Fine-Grained Address Entity Recognition from Multi-Turn Spoken Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.432) |  | 0 | Fine-grained address entity recognition (FGAER) from multi-turn spoken dialogues is particularly challenging. The major reason lies in that a full address is often formed through a conversation process. Different parts of an address are distributed through multiple turns of a dialogue with spoken noises. It is nontrivial to extract by turn and combine them. This challenge has not been well emphasized by main-stream entity extraction algorithms. To address this issue, we propose in this paper a... | Xue Han, Yitong Wang, Qian Hu, Pengwei Hu, Chao Deng, Junlan Feng |  |
| 1627 |  |  [Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning](https://doi.org/10.18653/v1/2023.emnlp-main.433) |  | 0 | Unified Sequence Labeling that articulates different sequence labeling problems such as Named Entity Recognition, Relation Extraction, Semantic Role Labeling, etc. in a generalized sequence-to-sequence format opens up the opportunity to make the maximum utilization of large language model knowledge toward structured prediction. Unfortunately, this requires formatting them into specialized augmented format unknown to the base pretrained language model (PLMs) necessitating finetuning to the... | Sarkar Snigdha Sarathi Das, Haoran Zhang, Peng Shi, Wenpeng Yin, Rui Zhang |  |
| 1628 |  |  [On the Representational Capacity of Recurrent Neural Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.434) |  | 0 | This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the... | Franz Nowak, Anej Svete, Li Du, Ryan Cotterell |  |
| 1629 |  |  [A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.435) |  | 0 | Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific... | Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan |  |
| 1630 |  |  [Benchmarking and Improving Text-to-SQL Generation under Ambiguity](https://doi.org/10.18653/v1/2023.emnlp-main.436) |  | 0 | Research in Text-to-SQL conversion has been largely benchmarked against datasets where each text query corresponds to one correct SQL. However, natural language queries over real-life databases frequently involve significant ambiguity about the intended SQL due to overlapping schema names and multiple confusing relationship paths. To bridge this gap, we develop a novel benchmark called AmbiQT with over 3000 examples where each text is interpretable as two plausible SQLs due to lexical and/or... | Adithya Bhaskar, Tushar Tomar, Ashutosh Sathe, Sunita Sarawagi |  |
| 1631 |  |  [Non-autoregressive Text Editing with Copy-aware Latent Alignments](https://doi.org/10.18653/v1/2023.emnlp-main.437) |  | 0 | Recent work has witnessed a paradigm shift from Seq2Seq to Seq2Edit in the field of text editing, with the aim of addressing the slow autoregressive inference problem posed by the former. Despite promising results, Seq2Edit approaches still face several challenges such as inflexibility in generation and difficulty in generalizing to other languages. In this work, we propose a novel non-autoregressive text editing method to circumvent the above issues, by modeling the edit process with latent... | Yu Zhang, Yue Zhang, Leyang Cui, Guohong Fu |  |
| 1632 |  |  [Translating away Translationese without Parallel Data](https://doi.org/10.18653/v1/2023.emnlp-main.438) |  | 0 | Translated texts exhibit systematic linguistic differences compared to original texts in the same language, and these differences are referred to as translationese. Translationese has effects on various cross-lingual natural language processing tasks, potentially leading to biased results. In this paper, we explore a novel approach to reduce translationese in translated texts: translation-based style transfer. As there are no parallel human-translated and original data in the same language, we... | Rricha Jalota, Koel Dutta Chowdhury, Cristina EspañaBonet, Josef van Genabith |  |
| 1633 |  |  [Prompt-Based Monte-Carlo Tree Search for Goal-oriented Dialogue Policy Planning](https://doi.org/10.18653/v1/2023.emnlp-main.439) |  | 0 | Planning for goal-oriented dialogue often requires simulating future dialogue interactions and estimating task progress. Many approaches thus consider training neural networks to perform look-ahead search algorithms such as A\* search and Monte Carlo Tree Search (MCTS). However, this training often require abundant annotated data, which creates challenges when faced with noisy annotations or low-resource settings. We introduce GDP-Zero, an approach using Open-Loop MCTS to perform goal-oriented... | Xiao Yu, Maximillian Chen, Zhou Yu |  |
| 1634 |  |  [UniMath: A Foundational and Multimodal Mathematical Reasoner](https://doi.org/10.18653/v1/2023.emnlp-main.440) |  | 0 | While significant progress has been made in natural language processing (NLP), existing methods exhibit limitations in effectively interpreting and processing diverse mathematical modalities. Therefore, we introduce UniMath, a versatile and unified system designed for multimodal mathematical reasoning tasks. Tackling complex problem-solving in arithmetic, geometry, and table-based math, UniMath utilizes a fine-tuned T5 model augmented with a variational autoencoder (VAE)-based image tokenizer.... | Zhenwen Liang, Tianyu Yang, Jipeng Zhang, Xiangliang Zhang |  |
| 1635 |  |  [CaseEncoder: A Knowledge-enhanced Pre-trained Model for Legal Case Encoding](https://doi.org/10.18653/v1/2023.emnlp-main.441) |  | 0 | Legal case retrieval is a critical process for modern legal information systems. While recent studies have utilized pre-trained language models (PLMs) based on the general domain self-supervised pre-training paradigm to build models for legal case retrieval, there are limitations in using general domain PLMs as backbones. Specifically, these models may not fully capture the underlying legal features in legal case documents. To address this issue, we propose CaseEncoder, a legal document encoder... | Yixiao Ma, Yueyue Wu, Weihang Su, Qingyao Ai, Yiqun Liu |  |
| 1636 |  |  [HiddenTables and PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies](https://doi.org/10.18653/v1/2023.emnlp-main.442) |  | 0 | A myriad of different Large Language Models (LLMs) face a common challenge in contextually analyzing table question-answering tasks. These challenges are engendered from (1) finite context windows for large tables, (2) multi-faceted discrepancies amongst tokenization patterns against cell boundaries, and (3) various limitations stemming from data confidentiality in the process of using external models such as gpt-35-turbo. We propose a cooperative game dubbed “HiddenTables” as a potential... | William Watson, Nicole Cho, Tucker Balch, Manuela Veloso |  |
| 1637 |  |  [Causal Document-Grounded Dialogue Pre-training](https://doi.org/10.18653/v1/2023.emnlp-main.443) |  | 0 | The goal of document-grounded dialogue (DocGD) is to generate a response by anchoring the evidence in a supporting document in accordance with the dialogue context. This entails four causally interconnected variables. While task-specific pre-training has significantly enhanced performances on numerous downstream tasks, existing DocGD methods still rely on general pre-trained language models without a specifically tailored pre-training approach that explicitly captures the causal relationships.... | Yingxiu Zhao, Bowen Yu, Bowen Li, Haiyang Yu, Jinyang Li, Chao Wang, Fei Huang, Yongbin Li, Nevin L. Zhang |  |
| 1638 |  |  [Accented Speech Recognition With Accent-specific Codebooks](https://doi.org/10.18653/v1/2023.emnlp-main.444) |  | 0 | Speech accents pose a significant challenge to state-of-the-art automatic speech recognition (ASR) systems. Degradation in performance across underrepresented accents is a severe deterrent to the inclusive adoption of ASR. In this work, we propose a novel accent adaptation approach for end-to-end ASR systems using cross-attention with a trainable set of codebooks. These learnable codebooks capture accent-specific information and are integrated within the ASR encoder layers. The model is trained... | Darshan Prabhu, Preethi Jyothi, Sriram Ganapathy, Vinit Unni |  |
| 1639 |  |  [Linking Surface Facts to Large-Scale Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.445) |  | 0 | Open Information Extraction (OIE) methods extract facts from natural language text in the form of (“subject”; “relation”; “object”) triples. These facts are, however, merely surface forms, the ambiguity of which impedes their downstream usage; e.g., the surface phrase “Michael Jordan” may refer to either the former basketball player or the university professor. Knowledge Graphs (KGs), on the other hand, contain facts in a canonical (i.e., unambiguous) form, but their coverage is limited by a... | Gorjan Radevski, Kiril Gashteovski, ChiaChien Hung, Carolin Lawrence, Goran Glavas |  |
| 1640 |  |  [Sentiment Analysis on Streaming User Reviews via Dual-Channel Dynamic Graph Neural Network](https://doi.org/10.18653/v1/2023.emnlp-main.446) |  | 0 | Sentiment analysis on user reviews has achieved great success thanks to the rapid growth of deep learning techniques. The large number of online streaming reviews also provides the opportunity to model temporal dynamics for users and products on the timeline. However, existing methods model users and products in the real world based on a static assumption and neglect their time-varying characteristics. In this paper, we present DC-DGNN, a dual-channel framework based on a dynamic graph neural... | Xin Zhang, Linhai Zhang, Deyu Zhou |  |
| 1641 |  |  [DUMB: A Dutch Model Benchmark](https://doi.org/10.18653/v1/2023.emnlp-main.447) |  | 0 | We introduce the Dutch Model Benchmark: DUMB. The benchmark includes a diverse set of datasets for low-, medium- and high-resource tasks. The total set of nine tasks includes four tasks that were previously not available in Dutch. Instead of relying on a mean score across tasks, we propose Relative Error Reduction (RER), which compares the DUMB performance of language models to a strong baseline which can be referred to in the future even when assessing different sets of language models.... | Wietse de Vries, Martijn Wieling, Malvina Nissim |  |
| 1642 |  |  [OssCSE: Overcoming Surface Structure Bias in Contrastive Learning for Unsupervised Sentence Embedding](https://doi.org/10.18653/v1/2023.emnlp-main.448) |  | 0 | Contrastive learning has been demonstrated effective in unsupervised sentence representation learning. Given one sentence, positive pairs are obtained by passing the sentence to the encoder twice using the different dropout masks, and negative pairs are obtained by taking another sentence in the same mini-batch. However, the method suffers from the surface structure bias, i.e., sentences with similar surface structures will be regarded as close in semantics while sentences with dissimilar... | Zhan Shi, Guoyin Wang, Ke Bai, Jiwei Li, Xiang Li, Qingjun Cui, Belinda Zeng, Trishul Chilimbi, Xiaodan Zhu |  |
| 1643 |  |  [End-to-End Single-Channel Speaker-Turn Aware Conversational Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-main.449) |  | 0 | Conventional speech-to-text translation (ST) systems are trained on single-speaker utterances, and they may not generalize to real-life scenarios where the audio contains conversations by multiple speakers. In this paper, we tackle single-channel multi-speaker conversational ST with an end-to-end and multi-task training model, named Speaker-Turn Aware Conversational Speech Translation, that combines automatic speech recognition, speech translation and speaker turn detection using special tokens... | Juan Pablo ZuluagaGomez, Zhaocheng Huang, Xing Niu, Rohit Paturi, Sundararajan Srinivasan, Prashant Mathur, Brian Thompson, Marcello Federico |  |
| 1644 |  |  [A Fine-Grained Taxonomy of Replies to Hate Speech](https://doi.org/10.18653/v1/2023.emnlp-main.450) |  | 0 | Countering rather than censoring hate speech has emerged as a promising strategy to address hatred. There are many types of counterspeech in user-generated content: addressing the hateful content or its author, generic requests, well-reasoned counter arguments, insults, etc. The effectiveness of counterspeech, which we define as subsequent incivility, depends on these types. In this paper, we present a theoretically grounded taxonomy of replies to hate speech and a new corpus. We work with... | Xinchen Yu, Ashley Zhao, Eduardo Blanco, Lingzi Hong |  |
| 1645 |  |  [JointMatch: A Unified Approach for Diverse and Collaborative Pseudo-Labeling to Semi-Supervised Text Classification](https://doi.org/10.18653/v1/2023.emnlp-main.451) |  | 0 | Semi-supervised text classification (SSTC) has gained increasing attention due to its ability to leverage unlabeled data. However, existing approaches based on pseudo-labeling suffer from the issues of pseudo-label bias and error accumulation. In this paper, we propose JointMatch, a holistic approach for SSTC that addresses these challenges by unifying ideas from recent semi-supervised learning and the task of learning with noise. JointMatch adaptively adjusts classwise thresholds based on the... | Henry Peng Zou, Cornelia Caragea |  |
| 1646 |  |  [Simple Temporal Adaptation to Changing Label Sets: Hashtag Prediction via Dense KNN](https://doi.org/10.18653/v1/2023.emnlp-main.452) |  | 0 | User-generated social media data is constantly changing as new trends influence online discussion and personal information is deleted due to privacy concerns. However, traditional NLP models rely on fixed training datasets, which means they are unable to adapt to temporal change—both test distribution shift and deleted training data—without frequent, costly re-training. In this paper, we study temporal adaptation through the task of longitudinal hashtag prediction and propose a non-parametric... | Niloofar Mireshghallah, Nikolai Vogler, Junxian He, Omar Florez, Ahmed ElKishky, Taylor BergKirkpatrick |  |
| 1647 |  |  [Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4](https://doi.org/10.18653/v1/2023.emnlp-main.453) |  | 0 | In this work, we carry out a data archaeology to infer books that are known to ChatGPT and GPT-4 using a name cloze membership inference query. We find that OpenAI models have memorized a wide collection of copyrighted materials, and that the degree of memorization is tied to the frequency with which passages of those books appear on the web. The ability of these models to memorize an unknown set of books complicates assessments of measurement validity for cultural analytics by contaminating... | Kent K. Chang, Mackenzie Cramer, Sandeep Soni, David Bamman |  |
| 1648 |  |  [A Study on Accessing Linguistic Information in Pre-Trained Language Models by Using Prompts](https://doi.org/10.18653/v1/2023.emnlp-main.454) |  | 0 | We study whether linguistic information in pre-trained multilingual language models can be accessed by human language: So far, there is no easy method to directly obtain linguistic information and gain insights into the linguistic principles encoded in such models. We use the technique of prompting and formulate linguistic tasks to test the LM’s access to explicit grammatical principles and study how effective this method is at providing access to linguistic features. Our experiments on German,... | Marion Di Marco, Katharina Hämmerl, Alexander Fraser |  |
| 1649 |  |  [CiteBench: A Benchmark for Scientific Citation Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.455) |  | 0 | Science progresses by building upon the prior body of knowledge documented in scientific publications. The acceleration of research makes it hard to stay up-to-date with the recent developments and to summarize the ever-growing body of prior work. To address this, the task of citation text generation aims to produce accurate textual summaries given a set of papers-to-cite and the citing paper context. Due to otherwise rare explicit anchoring of cited documents in the citing paper, citation text... | Martin Funkquist, Ilia Kuznetsov, Yufang Hou, Iryna Gurevych |  |
| 1650 |  |  [From Heuristic to Analytic: Cognitively Motivated Strategies for Coherent Physical Commonsense Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.456) |  | 0 | Pre-trained language models (PLMs) have shown impressive performance in various language tasks. However, they are prone to spurious correlations, and often generate illusory information. In real-world applications, PLMs should justify decisions with formalized, coherent reasoning chains, but this challenge remains under-explored. Cognitive psychology theorizes that humans are capable of utilizing fast and intuitive \*heuristic\* thinking to make decisions based on past experience, then... | Zheyuan Zhang, Shane Storks, Fengyuan Hu, Sungryull Sohn, Moontae Lee, Honglak Lee, Joyce Chai |  |
| 1651 |  |  [A Challenging Multimodal Video Summary: Simultaneously Extracting and Generating Keyframe-Caption Pairs from Video](https://doi.org/10.18653/v1/2023.emnlp-main.457) |  | 0 | This paper proposes a practical multimodal video summarization task setting and a dataset to train and evaluate the task. The target task involves summarizing a given video into a predefined number of keyframe-caption pairs and displaying them in a listable format to grasp the video content quickly. This task aims to extract crucial scenes from the video in the form of images (keyframes) and generate corresponding captions explaining each keyframe’s situation. This task is useful as a practical... | Keito Kudo, Haruki Nagasawa, Jun Suzuki, Nobuyuki Shimizu |  |
| 1652 |  |  [Copyright Violations and Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.458) |  | 0 | Language models may memorize more than just facts, including entire chunks of texts seen during training. Fair use exemptions to copyright laws typically allow for limited use of copyrighted material without permission from the copyright holder, but typically for extraction of information from copyrighted materials, rather than verbatim reproduction. This work explores the issue of copyright violations and large language models through the lens of verbatim memorization, focusing on possible... | Antonia Karamolegkou, Jiaang Li, Li Zhou, Anders Søgaard |  |
| 1653 |  |  [Effects of sub-word segmentation on performance of transformer language models](https://doi.org/10.18653/v1/2023.emnlp-main.459) |  | 0 | Language modeling is a fundamental task in natural language processing, which has been thoroughly explored with various architectures and hyperparameters. However, few studies focus on the effect of sub-word segmentation on the performance of language models (LMs). In this paper, we compare GPT and BERT models trained with the statistical segmentation algorithm BPE vs. two unsupervised algorithms for morphological segmentation — Morfessor and StateMorph. We train the models for several... | Jue Hou, Anisia Katinskaia, AnhDuc Vu, Roman Yangarber |  |
| 1654 |  |  [Symbolic Planning and Code Generation for Grounded Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.460) |  | 0 | Large language models (LLMs) excel at processing and generating text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system, consists of a reader and planner: the reader leverages an LLM to... | Justin T. Chiu, Wenting Zhao, Derek Chen, Saujas Vaduguru, Alexander M. Rush, Daniel Fried |  |
| 1655 |  |  [Universal Self-Adaptive Prompting](https://doi.org/10.18653/v1/2023.emnlp-main.461) |  | 0 | A hallmark of modern large language models (LLMs) is their impressive general zero-shot and few-shot abilities, often elicited through in-context learning (ICL) via prompting. However, while highly coveted and being the most general, zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable. In this study, we address this by presenting... | Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Hanjun Dai, Julian Eisenschlos, Sercan Ö. Arik, Tomas Pfister |  |
| 1656 |  |  [Somali Information Retrieval Corpus: Bridging the Gap between Query Translation and Dedicated Language Resources](https://doi.org/10.18653/v1/2023.emnlp-main.462) |  | 0 | Despite the growing use of the Somali language in various online domains, research on Somali language information retrieval remains limited and primarily relies on query translation due to the lack of a dedicated corpus. To address this problem, we collaborated with language experts and natural language processing (NLP) researchers to create an annotated corpus for Somali information retrieval. This corpus comprises 2335 documents collected from various well-known online sites, such as hiiraan... | Abdisalam Badel, Ting Zhong, Wenxin Tai, Fan Zhou |  |
| 1657 |  |  [Beat LLMs at Their Own Game: Zero-Shot LLM-Generated Text Detection via Querying ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.463) |  | 0 | Large language models (LLMs), e.g., ChatGPT, have revolutionized the domain of natural language processing because of their excellent performance on various tasks. Despite their great potential, LLMs also incur serious concerns as they are likely to be misused. There are already reported cases of academic cheating by using LLMs. Thus, it is a pressing problem to identify LLM-generated texts. In this work, we design a zero-shot black-box method for detecting LLM-generated texts. The key idea is... | Biru Zhu, Lifan Yuan, Ganqu Cui, Yangyi Chen, Chong Fu, Bingxiang He, Yangdong Deng, Zhiyuan Liu, Maosong Sun, Ming Gu |  |
| 1658 |  |  [Faithful Model Evaluation for Model-Based Metrics](https://doi.org/10.18653/v1/2023.emnlp-main.464) |  | 0 | Statistical significance testing is used in natural language processing (NLP) to determine whether the results of a study or experiment are likely to be due to chance or if they reflect a genuine relationship. A key step in significance testing is the estimation of confidence interval which is a function of sample variance. Sample variance calculation is straightforward when evaluating against ground truth. However, in many cases, a metric model is often used for evaluation. For example, to... | Qian Hu, Palash Goyal, Rahul Gupta |  |
| 1659 |  |  [Content- and Topology-Aware Representation Learning for Scientific Multi-Literature](https://doi.org/10.18653/v1/2023.emnlp-main.465) |  | 0 | Representation learning forms an essential building block in the development of natural language processing architectures. To date, mainstream approaches focus on learning textual information at the sentence- or document-level, unfortunately, overlooking the inter-document connections. This omission decreases the potency of downstream applications, particularly in multi-document settings. To address this issue, embeddings equipped with latent semantic and rich relatedness information are... | Kai Zhang, Kaisong Song, Yangyang Kang, Xiaozhong Liu |  |
| 1660 |  |  [Language Model Quality Correlates with Psychometric Predictive Power in Multiple Languages](https://doi.org/10.18653/v1/2023.emnlp-main.466) |  | 0 | Surprisal theory (Hale, 2001; Levy, 2008) posits that a word’s reading time is proportional to its surprisal (i.e., to its negative log probability given the proceeding context). Since we are unable to access a word’s ground-truth probability, surprisal theory has been empirically tested using surprisal estimates from language models (LMs). Under the premise that surprisal theory holds, we would expect that higher quality language models provide more powerful predictors of human reading... | Ethan Wilcox, Clara Meister, Ryan Cotterell, Tiago Pimentel |  |
| 1661 |  |  [Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks](https://doi.org/10.18653/v1/2023.emnlp-main.467) |  | 0 | Entity and Relation Extraction (ERE) is an important task in information extraction. Recent marker-based pipeline models achieve state-of-the-art performance, but still suffer from the error propagation issue. Also, most of current ERE models do not take into account higher-order interactions between multiple entities and relations, while higher-order modeling could be beneficial.In this work, we propose HyperGraph neural network for ERE (HGERE), which is built upon the PL-marker (a... | Zhaohui Yan, Songlin Yang, Wei Liu, Kewei Tu |  |
| 1662 |  |  [Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.468) |  | 0 | The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in... | Daman Arora, Himanshu Gaurav Singh, Mausam |  |
| 1663 |  |  [StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure](https://doi.org/10.18653/v1/2023.emnlp-main.469) |  | 0 | This work presents StrAE: a Structured Autoencoder framework that through strict adherence to explicit structure, and use of a novel contrastive objective over tree-structured representations, enables effective learning of multi-level representations. Through comparison over different forms of structure, we verify that our results are directly attributable to the informativeness of the structure provided as input, and show that this is not the case for existing tree models. We then further... | Mattia Opper, Victor Prokhorov, Siddharth Narayanaswamy |  |
| 1664 |  |  [WiCE: Real-World Entailment for Claims in Wikipedia](https://doi.org/10.18653/v1/2023.emnlp-main.470) |  | 0 | Textual entailment models are increasingly applied in settings like fact-checking, presupposition verification in question answering, or summary evaluation. However, these represent a significant domain shift from existing entailment datasets, and models underperform as a result. We propose WiCE, a new fine-grained textual entailment dataset built on natural claim and evidence pairs extracted from Wikipedia. In addition to standard claim-level entailment, WiCE provides entailment judgments over... | Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, Greg Durrett |  |
| 1665 |  |  [Natural Disaster Tweets Classification Using Multimodal Data](https://doi.org/10.18653/v1/2023.emnlp-main.471) |  | 0 | Social media platforms are extensively used for expressing opinions or conveying information. The information available on such platforms can be used for various humanitarian and disaster-related tasks as distributing messages in different formats through social media is quick and easy. Often this useful information during disaster events goes to waste as efficient systems don’t exist which can turn these unstructured data into meaningful format which can ultimately assist aid agencies. In... | Mohammad Basit, Bashir Alam, Zubaida Fatima, Salman Shaikh |  |
| 1666 |  |  [On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research](https://doi.org/10.18653/v1/2023.emnlp-main.472) |  | 0 | Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of findings that compare the relative merits of models and methods that aim to curb toxicity. Our findings suggest that research that... | Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker |  |
| 1667 |  |  [RoBoCoP: A Comprehensive ROmance BOrrowing COgnate Package and Benchmark for Multilingual Cognate Identification](https://doi.org/10.18653/v1/2023.emnlp-main.473) |  | 0 | The identification of cognates is a fundamental process in historical linguistics, on which any further research is based. Even though there are several cognate databases for Romance languages, they are rather scattered, incomplete, noisy, contain unreliable information, or have uncertain availability. In this paper we introduce a comprehensive database of Romance cognates and borrowings based on the etymological information provided by the dictionaries. We extract pairs of cognates between any... | Liviu P. Dinu, Ana Sabina Uban, Alina Maria Cristea, Anca Dinu, IoanBogdan Iordache, Simona Georgescu, Laurentiu Zoicas |  |
| 1668 |  |  [Instructive Dialogue Summarization with Query Aggregations](https://doi.org/10.18653/v1/2023.emnlp-main.474) |  | 0 | Conventional dialogue summarization methods directly generate summaries and do not consider user’s specific interests. This poses challenges in cases where the users are more focused on particular topics or aspects. With the advancement of instruction-finetuned language models, we introduce instruction-tuning to dialogues to expand the capability set of dialogue summarization models. To overcome the scarcity of instructive dialogue summarization data, we propose a three-step approach to... | Bin Wang, Zhengyuan Liu, Nancy F. Chen |  |
| 1669 |  |  [Semantic matching for text classification with complex class descriptions](https://doi.org/10.18653/v1/2023.emnlp-main.475) |  | 0 | Text classifiers are an indispensable tool for machine learning practitioners, but adapting them to new classes is expensive. To reduce the cost of new classes, previous work exploits class descriptions and/or labels from existing classes. However, these approaches leave a gap in the model development cycle as they support either zero- or few-shot learning, but not both. Existing classifiers either do not work on zero-shot problems, or fail to improve much with few-shot labels. Further, prior... | Brian de Silva, KuanWen Huang, Gwang Lee, Karen Hovsepian, Yan Xu, Mingwei Shen |  |
| 1670 |  |  [MADNet: Maximizing Addressee Deduction Expectation for Multi-Party Conversation Generation](https://doi.org/10.18653/v1/2023.emnlp-main.476) |  | 0 | Modeling multi-party conversations (MPCs) with graph neural networks has been proven effective at capturing complicated and graphical information flows. However, existing methods rely heavily on the necessary addressee labels and can only be applied to an ideal setting where each utterance must be tagged with an “@” or other equivalent addressee label. To study the scarcity of addressee labels which is a common issue in MPCs, we propose MADNet that maximizes addressee deduction expectation in... | JiaChen Gu, ChaoHong Tan, Caiyuan Chu, ZhenHua Ling, Chongyang Tao, Quan Liu, Cong Liu |  |
| 1671 |  |  [GLEN: Generative Retrieval via Lexical Index Learning](https://doi.org/10.18653/v1/2023.emnlp-main.477) |  | 0 | Generative retrieval shed light on a new paradigm of document retrieval, aiming to directly generate the identifier of a relevant document for a query. While it takes advantage of bypassing the construction of auxiliary index structures, existing studies face two significant challenges: (i) the discrepancy between the knowledge of pre-trained language models and identifiers and (ii) the gap between training and inference that poses difficulty in learning to rank. To overcome these challenges,... | Sunkyung Lee, Minjin Choi, Jongwuk Lee |  |
| 1672 |  |  [Turn-Level Active Learning for Dialogue State Tracking](https://doi.org/10.18653/v1/2023.emnlp-main.478) |  | 0 | Dialogue state tracking (DST) plays an important role in task-oriented dialogue systems. However, collecting a large amount of turn-by-turn annotated dialogue data is costly and inefficient. In this paper, we propose a novel turn-level active learning framework for DST to actively select turns in dialogues to annotate. Given the limited labelling budget, experimental results demonstrate the effectiveness of selective annotation of dialogue turns. Additionally, our approach can effectively... | Zihan Zhang, Meng Fang, Fanghua Ye, Ling Chen, MohammadReza NamaziRad |  |
| 1673 |  |  [ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.479) |  | 0 | Incorporating visual knowledge into text-only dialogue systems has become a potential direction to imitate the way humans think, imagine, and communicate. However, existing multimodal dialogue systems are either confined by the scale and quality of available datasets or the coarse concept of visual knowledge. To address these issues, we provide a new paradigm of constructing multimodal dialogues as well as two datasets extended from text-only dialogues under such paradigm (ReSee-WoW, ReSee-DD).... | Haoqin Tu, Yitong Li, Fei Mi, Zhongliang Yang |  |
| 1674 |  |  [Modeling Conceptual Attribute Likeness and Domain Inconsistency for Metaphor Detection](https://doi.org/10.18653/v1/2023.emnlp-main.480) |  | 0 | Metaphor detection is an important and challenging task in natural language processing, which aims to distinguish between metaphorical and literal expressions in text. Previous studies mainly leverage the incongruity of source and target domains and contextual clues for detection, neglecting similar attributes shared between source and target concepts in metaphorical expressions. Based on conceptual metaphor theory, these similar attributes are essential to infer implicit meanings conveyed by... | Yuan Tian, Nan Xu, Wenji Mao, Daniel Zeng |  |
| 1675 |  |  [Referring Image Segmentation via Joint Mask Contextual Embedding Learning and Progressive Alignment Network](https://doi.org/10.18653/v1/2023.emnlp-main.481) |  | 0 | Referring image segmentation is a task that aims to predict pixel-wise masks corresponding to objects in an image described by natural language expressions. Previous methods for referring image segmentation employ a cascade framework to break down complex problems into multiple stages. However, its defects also obvious: existing methods within the cascade framework may encounter challenges in both maintaining a strong focus on the most relevant information during specific stages of the... | Ziling Huang, Shin'ichi Satoh |  |
| 1676 |  |  [Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study](https://doi.org/10.18653/v1/2023.emnlp-main.482) |  | 0 | Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To answer it, we perform a comprehensive study on a scalable pre-trained retrieval-augmented LM (i.e., RETRO) compared with standard GPT and retrieval-augmented GPT incorporated at fine-tuning or inference... | Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, Anima Anandkumar, Bryan Catanzaro |  |
| 1677 |  |  [SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables](https://doi.org/10.18653/v1/2023.emnlp-main.483) |  | 0 | Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive... | Xinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov, MinYen Kan |  |
| 1678 |  |  [Training Simultaneous Speech Translation with Robust and Random Wait-k-Tokens Strategy](https://doi.org/10.18653/v1/2023.emnlp-main.484) |  | 0 | Simultaneous Speech Translation (SimulST) is a task focused on ensuring high-quality translation of speech in low-latency situations. Despite this, the modality gap (e.g., unknown word boundaries) between audio and text presents a challenge. This gap hinders the effective application of policies from simultaneous text translation (SimulMT) and compromises the performance of offline speech translation. To address this issue, we first leverage the Montreal Forced Aligner (MFA) and utilize audio... | Linlin Zhang, Kai Fan, Jiajun Bu, Zhongqiang Huang |  |
| 1679 |  |  [SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples](https://doi.org/10.18653/v1/2023.emnlp-main.485) |  | 0 | Detecting negatives (such as non-entailment relationships, unanswerable questions, and false claims) is an important and challenging aspect of many natural language understanding tasks. Though manually collecting challenging negative examples can help models detect them, it is both costly and domain-specific. In this work, we propose Self-labeled Counterfactuals for Extrapolating to Negative Examples (SCENE), an automatic method for synthesizing training data that greatly improves models’... | Deqing Fu, Ameya Godbole, Robin Jia |  |
| 1680 |  |  [Enhancing Code-Switching for Cross-lingual SLU: A Unified View of Semantic and Grammatical Coherence](https://doi.org/10.18653/v1/2023.emnlp-main.486) |  | 0 | Despite the success of spoken language understanding (SLU) in high-resource languages, achieving similar performance in low-resource settings, such as zero-shot scenarios, remains challenging due to limited labeled training data. To improve zero-shot cross-lingual SLU, recent studies have explored code-switched sentences containing tokens from multiple languages. However, vanilla code-switched sentences often lack semantic and grammatical coherence. We ascribe this lack to two issues: (1)... | Zhihong Zhu, Xuxin Cheng, Zhiqi Huang, Dongsheng Chen, Yuexian Zou |  |
| 1681 |  |  [Task-Agnostic Low-Rank Adapters for Unseen English Dialects](https://doi.org/10.18653/v1/2023.emnlp-main.487) |  | 0 | Large Language Models (LLMs) are trained on corpora disproportionally weighted in favor of Standard American English. As a result, speakers of other dialects experience significantly more failures when interacting with these technologies. In practice, these speakers often accommodate their speech to be better understood. Our work shares the belief that language technologies should be designed to accommodate the diversity in English dialects and not the other way around. However, prior work on... | Zedian Xiao, William Held, Yanchen Liu, Diyi Yang |  |
| 1682 |  |  [Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization](https://doi.org/10.18653/v1/2023.emnlp-main.488) |  | 0 | Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of... | Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor S. Sheng, Huaiyu Dai, Dejing Dou |  |
| 1683 |  |  [TheoremQA: A Theorem-driven Question Answering Dataset](https://doi.org/10.18653/v1/2023.emnlp-main.489) |  | 0 | The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models’ capabilities to apply theorems to solve challenging science problems.... | Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, Tony Xia |  |
| 1684 |  |  [Scalable-DSC: A Structural Template Prompt Approach to Scalable Dialogue State Correction](https://doi.org/10.18653/v1/2023.emnlp-main.490) |  | 0 | Dialogue state error correction has recently been proposed to correct wrong slot values in predicted dialogue states, thereby mitigating the error propagation problem for dialogue state tracking (DST). These approaches, though effective, are heavily intertwined with specific DST models, limiting their applicability to other DST models. To solve this problem, we propose Scalable Dialogue State Correction (Scalable-DSC), which can correct wrong slot values in the dialogue state predicted by any... | Haoxiang Su, Hongyan Xie, Hao Huang, Shuangyong Song, Ruiyu Fang, Xiaomeng Huang, Sijie Feng |  |
| 1685 |  |  [Don't Trust ChatGPT when your Question is not in English: A Study of Multilingual Abilities and Types of LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.491) |  | 0 | Large language models (LLMs) have demonstrated exceptional natural language understanding abilities, and have excelled in a variety of natural language processing (NLP) tasks. Despite the fact that most LLMs are trained predominantly on English, multiple studies have demonstrated their capabilities in a variety of languages. However, fundamental questions persist regarding how LLMs acquire their multilingual abilities and how performance varies across different languages. These inquiries are... | Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, Grzegorz Kondrak |  |
| 1686 |  |  [M³Seg: A Maximum-Minimum Mutual Information Paradigm for Unsupervised Topic Segmentation in ASR Transcripts](https://doi.org/10.18653/v1/2023.emnlp-main.492) |  | 0 | Topic segmentation aims to detect topic boundaries and split automatic speech recognition transcriptions (e.g., meeting transcripts) into segments that are bounded by thematic meanings. In this work, we propose M3Seg, a novel Maximum-Minimum Mutual information paradigm for linear topic segmentation without using any parallel data. Specifically, by employing sentence representations provided by pre-trained language models, M3Seg first learns a region-based segment encoder based on the... | Ke Wang, Xiutian Zhao, Yanghui Li, Wei Peng |  |
| 1687 |  |  [Empirical Study of Zero-Shot NER with ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.493) |  | 0 | Large language models (LLMs) exhibited powerful capability in various natural language processing tasks. This work focuses on exploring LLM performance on zero-shot information extraction, with a focus on the ChatGPT and named entity recognition (NER) task. Inspired by the remarkable reasoning capability of LLM on symbolic and arithmetic reasoning, we adapt the prevalent reasoning methods to NER and propose reasoning strategies tailored for NER. First, we explore a decomposed question-answering... | Tingyu Xie, Qi Li, Jian Zhang, Yan Zhang, Zuozhu Liu, Hongwei Wang |  |
| 1688 |  |  [Automatic Prompt Optimization with "Gradient Descent" and Beam Search](https://doi.org/10.18653/v1/2023.emnlp-main.494) |  | 0 | Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Prompt Optimization with Textual Gradients (ProTeGi), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to... | Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng |  |
| 1689 |  |  [Active Retrieval Augmented Generation](https://doi.org/10.18653/v1/2023.emnlp-main.495) |  | 0 | Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts,... | Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane DwivediYu, Yiming Yang, Jamie Callan, Graham Neubig |  |
| 1690 |  |  [GD-COMET: A Geo-Diverse Commonsense Inference Model](https://doi.org/10.18653/v1/2023.emnlp-main.496) |  | 0 | With the increasing integration of AI into everyday life, it’s becoming crucial to design AI systems to serve users from diverse backgrounds by making them culturally aware. In this paper, we present GD-COMET, a geo-diverse version of the COMET commonsense inference model. GD-COMET goes beyond Western commonsense knowledge and is capable of generating inferences pertaining to a broad range of cultures. We demonstrate the effectiveness of GD-COMET through a comprehensive human evaluation across... | Mehar Bhatia, Vered Shwartz |  |
| 1691 |  |  [Multi-level Adaptive Contrastive Learning for Knowledge Internalization in Dialogue Generation](https://doi.org/10.18653/v1/2023.emnlp-main.497) |  | 0 | Knowledge-grounded dialogue generation aims to mitigate the issue of text degeneration by incorporating external knowledge to supplement the context. However, the model often fails to internalize this information into responses in a human-like manner. Instead, it simply inserts segments of the provided knowledge into generic responses. As a result, the generated responses tend to be tedious, incoherent, and in lack of interactivity which means the degeneration problem is still unsolved. In this... | Chenxu Yang, Zheng Lin, Lanrui Wang, Chong Tian, Liang Pang, Jiangnan Li, Qirong Ho, Yanan Cao, Weiping Wang |  |
| 1692 |  |  [Enhancing Biomedical Lay Summarisation with External Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.498) |  | 0 | Previous approaches for automatic lay summarisation are exclusively reliant on the source article that, given it is written for a technical audience (e.g., researchers), is unlikely to explicitly define all technical concepts or state all of the background information that is relevant for a lay audience. We address this issue by augmenting eLife, an existing biomedical lay summarisation dataset, with article-specific knowledge graphs, each containing detailed information on relevant biomedical... | Tomas Goldsack, Zhihao Zhang, Chen Tang, Carolina Scarton, Chenghua Lin |  |
| 1693 |  |  [A Diffusion Weighted Graph Framework for New Intent Discovery](https://doi.org/10.18653/v1/2023.emnlp-main.499) |  | 0 | New Intent Discovery (NID) aims to recognize both new and known intents from unlabeled data with the aid of limited labeled data containing only known intents. Without considering structure relationships between samples, previous methods generate noisy supervisory signals which cannot strike a balance between quantity and quality, hindering the formation of new intent clusters and effective transfer of the pre-training knowledge. To mitigate this limitation, we propose a novel Diffusion... | Wenkai Shi, Wenbin An, Feng Tian, Qinghua Zheng, Qianying Wang, Ping Chen |  |
| 1694 |  |  [A Self-enhancement Multitask Framework for Unsupervised Aspect Category Detection](https://doi.org/10.18653/v1/2023.emnlp-main.500) |  | 0 | Our work addresses the problem of unsupervised Aspect Category Detection using a small set of seed words. Recent works have focused on learning embedding spaces for seed words and sentences to establish similarities between sentences and aspects. However, aspect representations are limited by the quality of initial seed words, and model performances are compromised by noise. To mitigate this limitation, we propose a simple framework that automatically enhances the quality of initial seed words... | ThiNhung Nguyen, Hoang Ngo, KiemHieu Nguyen, TuanDung Cao |  |
| 1695 |  |  [DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.501) |  | 0 | Chain-of-Thought (CoT) prompting has successfully enhanced the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective, or even detrimental, to the performance on reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters. In this paper, we propose Dialogue-guided Chain-of-Thought (DialCoT) to improve the reasoning capabilities of SLMs, with the aim of generating intermediate reasoning steps in a... | Chengcheng Han, Xiaowei Du, Che Zhang, Yixin Lian, Xiang Li, Ming Gao, Baoyuan Wang |  |
| 1696 |  |  [Recurrent Neural Language Models as Probabilistic Finite-state Automata](https://doi.org/10.18653/v1/2023.emnlp-main.502) |  | 0 | Studying language models (LMs) in terms of well-understood formalisms allows us to precisely characterize their abilities and limitations. Previous work has investigated the expressive power of recurrent neural network (RNN) LMs in terms of their capacity to recognize unweighted formal languages. However, LMs do not describe unweighted formal languages—rather, they define probability distributions over strings. In this work, we study what classes of such probability distributions RNN LMs can... | Anej Svete, Ryan Cotterell |  |
| 1697 |  |  [Revisiting Source Context in Nearest Neighbor Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.503) |  | 0 | Nearest neighbor machine translation (kNN-MT), which interpolates target token probabilities with estimates derived from additional examples, has achieved significant improvements and attracted extensive interest in recent years. However, existing research does not explicitly consider the source context when retrieving similar examples, potentially leading to suboptimal performance. To address this, we comprehensively revisit the role of source context and propose a simple and effective method... | Xuanhong Li, Peng Li, Po Hu |  |
| 1698 |  |  [Find-2-Find: Multitask Learning for Anaphora Resolution and Object Localization](https://doi.org/10.18653/v1/2023.emnlp-main.504) |  | 0 | In multimodal understanding tasks, visual and linguistic ambiguities can arise. Visual ambiguity can occur when visual objects require a model to ground a referring expression in a video without strong supervision, while linguistic ambiguity can occur from changes in entities in action flows. As an example from the cooking domain, “oil” mixed with “salt” and “pepper” could later be referred to as a “mixture”. Without a clear visual-linguistic alignment, we cannot know which among several... | Cennet Oguz, Pascal Denis, Emmanuel Vincent, Simon Ostermann, Josef van Genabith |  |
| 1699 |  |  [Background Summarization of Event Timelines](https://doi.org/10.18653/v1/2023.emnlp-main.505) |  | 0 | Generating concise summaries of news events is a challenging natural language processing task. While journalists often curate timelines to highlight key sub-events, newcomers to a news event face challenges in catching up on its historical context. In this paper, we address this need by introducing the task of background news summarization, which complements each timeline update with a background summary of relevant preceding events. We construct a dataset by merging existing timeline datasets... | Adithya Pratapa, Kevin Small, Markus Dreyer |  |
| 1700 |  |  [Superlim: A Swedish Language Understanding Evaluation Benchmark](https://doi.org/10.18653/v1/2023.emnlp-main.506) |  | 0 | We present Superlim, a multi-task NLP benchmark and analysis platform for evaluating Swedish language models, a counterpart to the English-language (Super)GLUE suite. We describe the dataset, the tasks, the leaderboard and report the baseline results yielded by a reference implementation. The tested models do not approach ceiling performance on any of the tasks, which suggests that Superlim is truly difficult, a desirable quality for a benchmark. We address methodological challenges, such as... | Aleksandrs Berdicevskis, Gerlof Bouma, Robin Kurtz, Felix Morger, Joey Öhman, Yvonne Adesam, Lars Borin, Dana Dannélls, Markus Forsberg, Tim Isbister, Anna Lindahl, Martin Malmsten, Faton Rekathati, Magnus Sahlgren, Elena Volodina, Love Börjeson, Simon Hengchen, Nina Tahmasebi |  |
| 1701 |  |  [Reasoning with Language Model is Planning with World Model](https://doi.org/10.18653/v1/2023.emnlp-main.507) |  | 0 | Large language models (LLMs) have shown remarkable reasoning capabilities, particularly with Chain-of-Thought-style prompts. However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning. This is due to LLMs’ absence of an internal world model for predicting world states (e.g., environment status, variable values) and simulating long-term action outcomes of actions. This prevents LLMs... | Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu |  |
| 1702 |  |  [LLM-enhanced Self-training for Cross-domain Constituency Parsing](https://doi.org/10.18653/v1/2023.emnlp-main.508) |  | 0 | Self-training has proven to be an effective approach for cross-domain tasks, and in this study, we explore its application to cross-domain constituency parsing. Traditional self-training methods rely on limited and potentially low-quality raw corpora. To overcome this limitation, we propose enhancing self-training with the large language model (LLM) to generate domain-specific raw corpora iteratively. For the constituency parsing, we introduce grammar rules that guide the LLM in generating raw... | Jianling Li, Meishan Zhang, Peiming Guo, Min Zhang, Yue Zhang |  |
| 1703 |  |  [Continual Named Entity Recognition without Catastrophic Forgetting](https://doi.org/10.18653/v1/2023.emnlp-main.509) |  | 0 | Continual Named Entity Recognition (CNER) is a burgeoning area, which involves updating an existing model by incorporating new entity types sequentially. Nevertheless, continual learning approaches are often severely afflicted by catastrophic forgetting. This issue is intensified in CNER due to the consolidation of old entity types from previous steps into the non-entity type at each step, leading to what is known as the semantic shift problem of the non-entity type. In this paper, we introduce... | Duzhen Zhang, Wei Cong, Jiahua Dong, Yahan Yu, Xiuyi Chen, Yonggang Zhang, Zhen Fang |  |
| 1704 |  |  [DSI++: Updating Transformer Memory with New Documents](https://doi.org/10.18653/v1/2023.emnlp-main.510) |  | 0 | Differentiable Search Indices (DSIs) encode a corpus of documents in the parameters of a model and use the same model to map queries directly to relevant document identifiers. Despite the solid performance of DSI models, successfully deploying them in scenarios where document corpora change with time is an open problem. In this work, we introduce DSI++, a continual learning challenge for DSI with the goal of continuously indexing new documents while being able to answer queries related to both... | Sanket Vaibhav Mehta, Jai Gupta, Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Jinfeng Rao, Marc Najork, Emma Strubell, Donald Metzler |  |
| 1705 |  |  [Editing Common Sense in Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.511) |  | 0 | Editing model parameters directly in Transformers makes updating open-source transformer-based models possible without re-training. However, these editing methods have only been evaluated on statements about encyclopedic knowledge with a single correct answer. Commonsense knowledge with multiple correct answers, e.g., an apple can be green or red but not transparent, has not been studied but is as essential for enhancing transformers’ reliability and usefulness. In this paper, we investigate... | Anshita Gupta, Debanjan Mondal, Akshay Krishna Sheshadri, Wenlong Zhao, Xiang Li, Sarah Wiegreffe, Niket Tandon |  |
| 1706 |  |  [Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.512) |  | 0 | Controllable text generation (CTG) aims to generate text with desired attributes, and decoding-time-based methods have shown promising performance on this task. However, in this paper, we identify the phenomenon of Attribute Collapse for the first time. It causes the fluency of generated text to rapidly decrease when the control strength exceeds a critical value, rendering the text completely unusable. This limitation hinders the effectiveness of decoding methods in achieving high levels of... | Tianqi Zhong, Quan Wang, Jingxuan Han, Yongdong Zhang, Zhendong Mao |  |
| 1707 |  |  [Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.513) |  | 0 | Transformers have become a key architecture in speech processing, but our understanding of how they build up representations of acoustic and linguistic structure is limited. In this study, we address this gap by investigating how measures of ‘context-mixing’ developed for text models can be adapted and applied to models of spoken language. We identify a linguistic phenomenon that is ideal for such a case study: homophony in French (e.g. livre vs livres), where a speech recognition model has to... | Hosein Mohebbi, Grzegorz Chrupala, Willem H. Zuidema, Afra Alishahi |  |
| 1708 |  |  [Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System](https://doi.org/10.18653/v1/2023.emnlp-main.514) |  | 0 | Developing an efficient retriever to retrieve knowledge from a large-scale knowledge base (KB) is critical for task-oriented dialogue systems to effectively handle localized and specialized tasks. However, widely used generative models such as T5 and ChatGPT often struggle to differentiate subtle differences among the retrieved KB records when generating responses, resulting in suboptimal quality of generated responses. In this paper, we propose the application of maximal marginal likelihood to... | Weizhou Shen, Yingqi Gao, Canbin Huang, Fanqi Wan, Xiaojun Quan, Wei Bi |  |
| 1709 |  |  [IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions](https://doi.org/10.18653/v1/2023.emnlp-main.515) |  | 0 | Although counterfactual reasoning is a fundamental aspect of intelligence, the lack of large-scale counterfactual open-domain question-answering (QA) benchmarks makes it difficult to evaluate and improve models on this ability. To address this void, we introduce the first such dataset, named IfQA, where each question is based on a counterfactual presupposition via an “if” clause. Such questions require models to go beyond retrieving direct factual knowledge from the Web: they must identify the... | Wenhao Yu, Meng Jiang, Peter Clark, Ashish Sabharwal |  |
| 1710 |  |  [How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances](https://doi.org/10.18653/v1/2023.emnlp-main.516) |  | 0 | Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era. This paper provides a comprehensive review of recent advances in aligning deployed LLMs with the ever-changing world knowledge. We categorize research works systemically and provide in-depth comparisons and discussions. We also discuss existing challenges and highlight future directions to... | Zihan Zhang, Meng Fang, Ling Chen, MohammadReza NamaziRad, Jun Wang |  |
| 1711 |  |  [PreWoMe: Exploiting Presuppositions as Working Memory for Long Form Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.517) |  | 0 | Information-seeking questions in long-form question answering (LFQA) often prove misleading due to ambiguity or false presupposition in the question. While many existing approaches handle misleading questions, they are tailored to limited questions, which are insufficient in a real-world setting with unpredictable input characteristics. In this work, we propose PreWoMe, a unified approach capable of handling any type of information-seeking question. The key idea of PreWoMe involves extracting... | Wookje Han, Jinsol Park, Kyungjae Lee |  |
| 1712 |  |  [Memorisation Cartography: Mapping out the Memorisation-Generalisation Continuum in Neural Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.518) |  | 0 | When training a neural network, it will quickly memorise some source-target mappings from your dataset but never learn some others. Yet, memorisation is not easily expressed as a binary feature that is good or bad: individual datapoints lie on a memorisation-generalisation continuum. What determines a datapoint’s position on that spectrum, and how does that spectrum influence neural models’ performance? We address these two questions for neural machine translation (NMT) models. We use the... | Verna Dankers, Ivan Titov, Dieuwke Hupkes |  |
| 1713 |  |  [DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4](https://doi.org/10.18653/v1/2023.emnlp-main.519) |  | 0 | Human preference judgments are pivotal in guiding large language models (LLMs) to produce outputs that align with human values. Human evaluations are also used in summarization tasks to compare outputs from various systems, complementing existing automatic metrics. Despite their significance, however, there has been limited research probing these pairwise or k-wise comparisons. The collective impact and relative importance of factors such as output length, informativeness, fluency, and factual... | Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Hassan Foroosh, Fei Liu |  |
| 1714 |  |  [Gender Biases in Automatic Evaluation Metrics for Image Captioning](https://doi.org/10.18653/v1/2023.emnlp-main.520) |  | 0 | Model-based evaluation metrics (e.g., CLIPScore and GPTScore) have demonstrated decent correlations with human judgments in various language generation tasks. However, their impact on fairness remains largely unexplored. It is widely recognized that pretrained models can inadvertently encode societal biases, thus employing these models for evaluation purposes may inadvertently perpetuate and amplify biases. For example, an evaluation metric may favor the caption “a woman is calculating an... | Haoyi Qiu, ZiYi Dou, Tianlu Wang, Asli Celikyilmaz, Nanyun Peng |  |
| 1715 |  |  [QA-NatVer: Question Answering for Natural Logic-based Fact Verification](https://doi.org/10.18653/v1/2023.emnlp-main.521) |  | 0 | Fact verification systems assess a claim’s veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are... | Rami Aly, Marek Strong, Andreas Vlachos |  |
| 1716 |  |  [Increasing Probability Mass on Answer Choices Does Not Always Improve Accuracy](https://doi.org/10.18653/v1/2023.emnlp-main.522) |  | 0 | When pretrained language models (LMs) are applied to discriminative tasks such as multiple-choice questions, they place probability mass on vocabulary tokens that aren’t among the given answer choices. Spreading probability mass across multiple surface forms with identical meaning (such as “bath” and “bathtub”) is thought to cause an underestimation of a model’s true performance, referred to as the “surface form competition” (SFC) hypothesis. This has motivated the introduction of various... | Sarah Wiegreffe, Matthew Finlayson, Oyvind Tafjord, Peter Clark, Ashish Sabharwal |  |
| 1717 |  |  [Generating Data for Symbolic Language with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.523) |  | 0 | While large language models (LLMs) bring not only performance but also complexity, recent work has started to turn LLMs into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily been applied to natural language tasks, and has not yet been explored for symbolic language tasks with complex structured outputs (e.g., semantic parsing and code generation). In this paper, we propose... | Jiacheng Ye, Chengzu Li, Lingpeng Kong, Tao Yu |  |
| 1718 |  |  [IDTraffickers: An Authorship Attribution Dataset to link and connect Potential Human-Trafficking Operations on Text Escort Advertisements](https://doi.org/10.18653/v1/2023.emnlp-main.524) |  | 0 | Human trafficking (HT) is a pervasive global issue affecting vulnerable individuals, violating their fundamental human rights. Investigations reveal that a significant number of HT cases are associated with online advertisements (ads), particularly in escort markets. Consequently, identifying and connecting HT vendors has become increasingly challenging for Law Enforcement Agencies (LEAs). To address this issue, we introduce IDTraffickers, an extensive dataset consisting of 87,595 text ads and... | Vageesh Saxena, Benjamin Bashpole, Gijs van Dijck, Gerasimos Spanakis |  |
| 1719 |  |  [Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.525) |  | 0 | Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. Therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. In this work, we define gender bias as our case study. We quantify bias amplification in pretraining and after... | Laura Cabello, Emanuele Bugliarello, Stephanie Brandl, Desmond Elliott |  |
| 1720 |  |  [Improving Dialogue Discourse Parsing via Reply-to Structures of Addressee Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.526) |  | 0 | Dialogue discourse parsing aims to reflect the relation-based structure of dialogue by establishing discourse links according to discourse relations. To alleviate data sparsity, previous studies have adopted multitasking approaches to jointly learn dialogue discourse parsing with related tasks (e.g., reading comprehension) that require additional human annotation, thus limiting their generality. In this paper, we propose a multitasking framework that integrates dialogue discourse parsing with... | Yaxin Fan, Feng Jiang, Peifeng Li, Fang Kong, Qiaoming Zhu |  |
| 1721 |  |  [Improving Language Models' Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary](https://doi.org/10.18653/v1/2023.emnlp-main.527) |  | 0 | The non-humanlike behaviour of contemporary pre-trained language models (PLMs) is a leading cause undermining their trustworthiness. A striking phenomenon of such faulty behaviours is the generation of inconsistent predictions, which produces logically contradictory results, such as generating different predictions for texts delivering the same meaning or violating logical properties. Previous studies exploited data augmentation or implemented specialised loss functions to alleviate the issue.... | Myeongjun Jang, Thomas Lukasiewicz |  |
| 1722 |  |  [DALE: Generative Data Augmentation for Low-Resource Legal NLP](https://doi.org/10.18653/v1/2023.emnlp-main.528) |  | 0 | We present DALE, a novel and effective generative Data Augmentation framework for low-resource LEgal NLP. DALE addresses the challenges existing frameworks pose in generating effective data augmentations of legal documents - legal language, with its specialized vocabulary and complex semantics, morphology, and syntax, does not benefit from data augmentations that merely rephrase the source sentence. To address this, DALE, built on an Encoder-Decoder Language Model, is pre-trained on a novel... | Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Ramaneswaran S., Sakshi Singh, Utkarsh Tyagi, Dinesh Manocha |  |
| 1723 |  |  [FedID: Federated Interactive Distillation for Large-Scale Pretraining Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.529) |  | 0 | The growing concerns and regulations surrounding the protection of user data privacy have necessitated decentralized training paradigms. To this end, federated learning (FL) is widely studied in user-related natural language processing (NLP). However, it suffers from several critical limitations including extensive communication overhead, inability to handle heterogeneity, and vulnerability to white-box inference attacks. Federated distillation (FD) is proposed to alleviate these limitations,... | Xinge Ma, Jiangming Liu, Jin Wang, Xuejie Zhang |  |
| 1724 |  |  [trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.530) |  | 0 | Reinforcement learning from human feedback (RLHF) utilizes human feedback to better align large language models with human preferences via online optimization against a learned reward model. Current RLHF paradigms rely on Proximal Policy Optimization (PPO), which quickly becomes a challenge to implement and scale up to large architectures. To address this difficulty we present the AutoRLHF library as a feature complete open-source framework for RLHF fine-tuning of models up to and exceeding 70... | Alexander Havrilla, Maksym Zhuravinskyi, Duy Phung, Aman Tiwari, Jonathan Tow, Stella Biderman, Quentin Anthony, Louis Castricato |  |
| 1725 |  |  [This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.531) |  | 0 | Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing. We try to clarify the reasons for the sub-optimal performance of LLMs understanding negation. We introduce a large semi-automatically generated dataset of circa 400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present... | Iker GarcíaFerrero, Begoña Altuna, Javier Álvez, Itziar GonzalezDios, German Rigau |  |
| 1726 |  |  [MT2: Towards a Multi-Task Machine Translation Model with Translation-Specific In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.532) |  | 0 | Sentence-level translation, document-level translation, translation memory, and terminology constrained translation play an important role in machine translation. Most of the previous work uses separate models or methods to solve these tasks, which is not conducive to knowledge transfer of different tasks and increases the complexity of system construction. In this work, we explore the potential of pre-trained language model in machine translation tasks and propose a Multi-Task Machine... | Chunyou Li, Mingtong Liu, Hongxiao Zhang, Yufeng Chen, Jinan Xu, Ming Zhou |  |
| 1727 |  |  [CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset](https://doi.org/10.18653/v1/2023.emnlp-main.533) |  | 0 | The CoNLL-03 corpus is arguably the most well-known and utilized benchmark dataset for named entity recognition (NER). However, prior works found significant numbers of annotation errors, incompleteness, and inconsistencies in the data. This poses challenges to objectively comparing NER approaches and analyzing their errors, as current state-of-the-art models achieve F1-scores that are comparable to or even exceed the estimated noise level in CoNLL-03. To address this issue, we present a... | Susanna Rücker, Alan Akbik |  |
| 1728 |  |  [Disentangling Transformer Language Models as Superposed Topic Models](https://doi.org/10.18653/v1/2023.emnlp-main.534) |  | 0 | Topic Modelling is an established research area where the quality of a given topic is measured using coherence metrics. Often, we infer topics from Neural Topic Models (NTM) by interpreting their decoder weights, consisting of top-activated words projected from individual neurons. Transformer-based Language Models (TLM) similarly consist of decoder weights. However, due to its hypothesised superposition properties, the final logits originating from the residual path are considered... | Jia Peng Lim, Hady W. Lauw |  |
| 1729 |  |  [Conversational Semantic Parsing using Dynamic Context Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.535) |  | 0 | In this paper we consider the task of conversational semantic parsing over general purpose knowledge graphs (KGs) with millions of entities, and thousands of relation-types. We focus on models which are capable of interactively mapping user utterances into executable logical forms (e.g., Sparql) in the context of the conversational history. Our key idea is to represent information about an utterance and its context via a subgraph which is created dynamically, i.e., the number of nodes varies... | Parag Jain, Mirella Lapata |  |
| 1730 |  |  [Not all quantifiers are equal: Probing Transformer-based language models' understanding of generalised quantifiers](https://doi.org/10.18653/v1/2023.emnlp-main.536) |  | 0 | How do different generalised quantifiers affect the behaviour of transformer-based language models (TLMs)? The recent popularity of TLMs and the central role generalised quantifiers have traditionally played in linguistics and logic bring this question into particular focus. The current research investigating this subject has not utilised a task defined purely in a logical sense, and thus, has not captured the underlying logical significance of generalised quantifiers. Consequently, they have... | Tharindu Madusanka, Iqra Zahid, Hao Li, Ian PrattHartmann, Riza BatistaNavarro |  |
| 1731 |  |  [Structure-aware Knowledge Graph-to-text Generation with Planning Selection and Similarity Distinction](https://doi.org/10.18653/v1/2023.emnlp-main.537) |  | 0 | The knowledge graph-to-text (KG-to-text) generation task aims to synthesize coherent and engaging sentences that accurately convey the complex information derived from an input knowledge graph. One of the primary challenges in this task is bridging the gap between the diverse structures of the KG and the target text, while preserving the details of the input KG. To address this, we propose a novel approach that efficiently integrates graph structure-aware modules with pre-trained language... | Feng Zhao, Hongzhi Zou, Cheng Yan |  |
| 1732 |  |  [SOUL: Towards Sentiment and Opinion Understanding of Language](https://doi.org/10.18653/v1/2023.emnlp-main.538) |  | 0 | Sentiment analysis is a well-established natural language processing task, with sentiment polarity classification being one of its most popular and representative tasks. However, despite the success of pre-trained language models in this area, they often fall short of capturing the broader complexities of sentiment analysis. To address this issue, we propose a new task called Sentiment and Opinion Understanding of Language (SOUL). SOUL aims to evaluate sentiment understanding through two... | Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing |  |
| 1733 |  |  [Regulation and NLP (RegNLP): Taming Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.539) |  | 0 | The scientific innovation in Natural Language Processing (NLP) and more broadly in artificial intelligence (AI) is at its fastest pace to date. As large language models (LLMs) unleash a new era of automation, important debates emerge regarding the benefits and risks of their development, deployment and use. Currently, these debates have been dominated by often polarized narratives mainly led by the AI Safety and AI Ethics movements. This polarization, often amplified by social media, is swaying... | Catalina Goanta, Nikolaos Aletras, Ilias Chalkidis, Sofia Ranchordás, Gerasimos Spanakis |  |
| 1734 |  |  [MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation](https://doi.org/10.18653/v1/2023.emnlp-main.540) |  | 0 | Curated datasets for healthcare are often limited due to the need of human annotations from experts. In this paper, we present MedEval, a multi-level, multi-task, and multi-domain medical benchmark to facilitate the development of language models for healthcare. MedEval is comprehensive and consists of data from several healthcare systems and spans 35 human body regions from 8 examination modalities. With 22,779 collected sentences and 21,228 reports, we provide expert annotations at multiple... | Zexue He, Yu Wang, An Yan, Yao Liu, Eric Y. Chang, Amilcare Gentili, Julian J. McAuley, ChunNan Hsu |  |
| 1735 |  |  [Seeing through the mess: evolutionary dynamics of lexical polysemy](https://doi.org/10.18653/v1/2023.emnlp-main.541) |  | 0 | Evidently, words can have multiple senses. For example, the word mess refers to a place to have food or to a confusing situation. How exactly multiple senses emerge is less clear. In this work, we propose and analyze a mathematical model of the evolution of lexical meaning to investigate mechanisms leading to polysemy. This model features factors that have been discussed to impact the semantic processing and transmission of words: word frequency, non-conformism, and semantic discriminability.... | Andreas Baumann, Andreas Stephan, Benjamin Roth |  |
| 1736 |  |  [Are Embedded Potatoes Still Vegetables? On the Limitations of WordNet Embeddings for Lexical Semantics](https://doi.org/10.18653/v1/2023.emnlp-main.542) |  | 0 | Knowledge Base Embedding (KBE) models have been widely used to encode structured information from knowledge bases, including WordNet. However, the existing literature has predominantly focused on link prediction as the evaluation task, often neglecting exploration of the models’ semantic capabilities. In this paper, we investigate the potential disconnect between the performance of KBE models of WordNet on link prediction and their ability to encode semantic information, highlighting the... | Xuyou Cheng, Michael Sejr Schlichtkrull, Guy Emerson |  |
| 1737 |  |  [Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.543) |  | 0 | Large Language Models (LLMs) evaluation is a patchy and inconsistent landscape, and it is becoming clear that the quality of automatic evaluation metrics is not keeping up with the pace of development of generative models. We aim to improve the understanding of current models’ performance by providing a preliminary and hybrid evaluation on a range of open and closed-source generative LLMs on three NLP benchmarks: text summarisation, text simplification and grammatical error correction (GEC),... | Andrea Sottana, Bin Liang, Kai Zou, Zheng Yuan |  |
| 1738 |  |  [Event-Location Tracking in Narratives: A Case Study on Holocaust Testimonies](https://doi.org/10.18653/v1/2023.emnlp-main.544) |  | 0 | This work focuses on the spatial dimension of narrative understanding and presents the task of event-location tracking in narrative texts. The task intends to extract the sequence of locations where the narrative is set through its progression. We present several architectures for the task that seeks to model the global structure of the sequence, with varying levels of context awareness. We compare these methods to several baselines, including the use of strong methods applied over narrow... | Eitan Wagner, Renana Keydar, Omri Abend |  |
| 1739 |  |  [Dialogizer: Context-aware Conversational-QA Dataset Generation from Textual Sources](https://doi.org/10.18653/v1/2023.emnlp-main.545) |  | 0 | To address the data scarcity issue in Conversational question answering (ConvQA), a dialog inpainting method, which utilizes documents to generate ConvQA datasets, has been proposed. However, the original dialog inpainting model is trained solely on the dialog reconstruction task, resulting in the generation of questions with low contextual relevance due to insufficient learning of question-answer alignment. To overcome this limitation, we propose a novel framework called Dialogizer, which has... | Yerin Hwang, Yongil Kim, Hyunkyung Bae, Hwanhee Lee, Jeesoo Bang, Kyomin Jung |  |
| 1740 |  |  [Learning to Predict Task Transferability via Soft Prompt](https://doi.org/10.18653/v1/2023.emnlp-main.546) |  | 0 | Fine-tuning pretrained language models on helpful intermediate tasks often greatly improves the performance of target tasks. However, how to efficiently find the source tasks that can successfully transfer still remains under-explored. In this work, we propose to learn an affinity scoring function to predict transferability between tasks. Specifically, we conduct prompt tuning and regard soft prompts as task embeddings that summarize task-specific information. Then we randomly sample task pairs... | Lingyun Feng |  |
| 1741 |  |  [Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.547) |  | 0 | We propose Chain-of-Questions, a framework that trains a model to robustly answer multistep questions by generating and answering sub-questions. We obtain supervision for sub-questions from human-annotated question decomposition meaning representation (QDMR), but QDMR does not include annotated answers to sub-questions. To overcome this technical challenge, we treat sub-answers as latent variables and infer them with a novel dynamic mixture of Hard-EM and MAPO. Chain-of-Questions is effective... | Wang Zhu, Jesse Thomason, Robin Jia |  |
| 1742 |  |  [Mirror: A Universal Framework for Various Information Extraction Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.548) |  | 0 | Sharing knowledge between information extraction tasks has always been a challenge due to the diverse data formats and task variations. Meanwhile, this divergence leads to information waste and increases difficulties in building complex applications in real scenarios. Recent studies often formulate IE tasks as a triplet extraction problem. However, such a paradigm does not support multi-span and n-ary extraction, leading to weak versatility. To this end, we reorganize IE problems into unified... | Tong Zhu, Junfei Ren, Zijian Yu, Mengsong Wu, Guoliang Zhang, Xiaoye Qu, Wenliang Chen, Zhefeng Wang, Baoxing Huai, Min Zhang |  |
| 1743 |  |  ["Mistakes Help Us Grow": Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms](https://doi.org/10.18653/v1/2023.emnlp-main.549) |  | 0 | Teachers’ growth mindset supportive language (GMSL)—rhetoric emphasizing that one’s skills can be improved over time—has been shown to significantly reduce disparities in academic achievement and enhance students’ learning outcomes. Although teachers espouse growth mindset principles, most find it difficult to adopt GMSL in their practice due the lack of effective coaching in this area. We explore whether large language models (LLMs) can provide automated, personalized coaching to support... | Kunal Handa, Margaret Clapper, Jessica Boyle, Rose E. Wang, Diyi Yang, David S. Yeager, Dorottya Demszky |  |
| 1744 |  |  [Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text](https://doi.org/10.18653/v1/2023.emnlp-main.550) |  | 0 | While Large Language Models (LLMs) have achieved remarkable performance in many tasks, much about their inner workings remains unclear. In this study, we present novel experimental insights into the resilience of LLMs, particularly GPT-4, when subjected to extensive character-level permutations. To investigate this, we first propose the Scrambled Bench, a suite designed to measure the capacity of LLMs to handle scrambled input, in terms of both recovering scrambled sentences and answering... | Qi Cao, Takeshi Kojima, Yutaka Matsuo, Yusuke Iwasawa |  |
| 1745 |  |  [Detecting and Mitigating Hallucinations in Multilingual Summarisation](https://doi.org/10.18653/v1/2023.emnlp-main.551) |  | 0 | Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource languages, where summarisation requires cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we... | Yifu Qiu, Yftah Ziser, Anna Korhonen, Edoardo Maria Ponti, Shay B. Cohen |  |
| 1746 |  |  [Exploring Linguistic Probes for Morphological Inflection](https://doi.org/10.18653/v1/2023.emnlp-main.552) |  | 0 | Modern work on the cross-linguistic computational modeling of morphological inflection has typically employed language-independent data splitting algorithms. In this paper, we supplement that approach with language-specific probes designed to test aspects of morphological generalization. Testing these probes on three morphologically distinct languages, English, Spanish, and Swahili, we find evidence that three leading morphological inflection systems employ distinct generalization strategies... | Jordan Kodner, Salam Khalifa, Sarah Ruth Brogden Payne |  |
| 1747 |  |  [AMR Parsing with Causal Hierarchical Attention and Pointers](https://doi.org/10.18653/v1/2023.emnlp-main.553) |  | 0 | Translation-based AMR parsers have recently gained popularity due to their simplicity and effectiveness. They predict linearized graphs as free texts, avoiding explicit structure modeling. However, this simplicity neglects structural locality in AMR graphs and introduces unnecessary tokens to represent coreferences. In this paper, we introduce new target forms of AMR parsing and a novel model, CHAP, which is equipped with causal hierarchical attention and the pointer mechanism, enabling the... | Chao Lou, Kewei Tu |  |
| 1748 |  |  [FLatS: Principled Out-of-Distribution Detection with Feature-Based Likelihood Ratio Score](https://doi.org/10.18653/v1/2023.emnlp-main.554) |  | 0 | Detecting out-of-distribution (OOD) instances is crucial for NLP models in practical applications. Although numerous OOD detection methods exist, most of them are empirical. Backed by theoretical analysis, this paper advocates for the measurement of the “OOD-ness” of a test case x through the likelihood ratio between out-distribution Pout and in-distribution Pin. We argue that the state-of-the-art (SOTA) feature-based OOD detection methods, such as Maha and KNN, are suboptimal since they only... | Haowei Lin, Yuntian Gu |  |
| 1749 |  |  [Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.555) |  | 0 | Text classification tasks often encounter few-shot scenarios with limited labeled data, and addressing data scarcity is crucial. Data augmentation with mixup merges sample pairs to generate new pseudos, which can relieve the data deficiency issue in text classification. However, the quality of pseudo-samples generated by mixup exhibits significant variations. Most of the mixup methods fail to consider the varying degree of learning difficulty in different stages of training. And mixup generates... | Haoqi Zheng, Qihuang Zhong, Liang Ding, Zhiliang Tian, Xin Niu, Changjian Wang, Dongsheng Li, Dacheng Tao |  |
| 1750 |  |  [IC3: Image Captioning by Committee Consensus](https://doi.org/10.18653/v1/2023.emnlp-main.556) |  | 0 | If you ask a human to describe an image, they might do so in a thousand different ways. Traditionally, image captioning models are trained to generate a single “best’ (most like a reference) image caption. Unfortunately, doing so encourages captions that are “informationally impoverished,’ and focus on only a subset of the possible details, while ignoring other potentially useful information in the scene. In this work, we introduce a simple, yet novel, method: “Image Captioning by Committee... | David Chan, Austin Myers, Sudheendra Vijayanarasimhan, David A. Ross, John F. Canny |  |
| 1751 |  |  [SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.557) |  | 0 | Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this... | Potsawee Manakul, Adian Liusie, Mark J. F. Gales |  |
| 1752 |  |  [Fair Without Leveling Down: A New Intersectional Fairness Definition](https://doi.org/10.18653/v1/2023.emnlp-main.558) |  | 0 | In this work, we consider the problem of intersectional group fairness in the classification setting, where the objective is to learn discrimination-free models in the presence of several intersecting sensitive groups. First, we illustrate various shortcomings of existing fairness measures commonly used to capture intersectional fairness. Then, we propose a new definition called the 𝛼-Intersectional Fairness, which combines the absolute and the relative performance across sensitive groups and... | Gaurav Maheshwari, Aurélien Bellet, Pascal Denis, Mikaela Keller |  |
| 1753 |  |  [Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications](https://doi.org/10.18653/v1/2023.emnlp-main.559) |  | 0 | Instruction Fine-Tuning (IFT) is a powerful paradigm that strengthens the zero-shot capabilities of Large Language Models (LLMs), but in doing so induces new evaluation metric requirements. We show LLM-based metrics to be well adapted to these requirements, and leverage them to conduct an investigation of task-specialization strategies, quantifying the trade-offs that emerge in practical industrial settings. Our findings offer practitioners actionable insights for real-world IFT model... | Manuel Faysse, Gautier Viaud, Céline Hudelot, Pierre Colombo |  |
| 1754 |  |  [CLAD-ST: Contrastive Learning with Adversarial Data for Robust Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-main.560) |  | 0 | The cascaded approach continues to be the most popular choice for speech translation (ST). This approach consists of an automatic speech recognition (ASR) model and a machine translation (MT) model that are used in a pipeline to translate speech in one language to text in another language. MT models are often trained on the well-formed text and therefore lack robustness while translating noisy ASR outputs in the cascaded approach, degrading the overall translation quality significantly. We... | Sathish Indurthi, Shamil Chollampatt, Ravi Agrawal, Marco Turchi |  |
| 1755 |  |  [M2DF: Multi-grained Multi-curriculum Denoising Framework for Multimodal Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.561) |  | 0 | Multimodal Aspect-based Sentiment Analysis (MABSA) is a fine-grained Sentiment Analysis task, which has attracted growing research interests recently. Existing work mainly utilizes image information to improve the performance of MABSA task. However, most of the studies overestimate the importance of images since there are many noise images unrelated to the text in the dataset, which will have a negative impact on model learning. Although some work attempts to filter low-quality noise images by... | Fei Zhao, Chunhui Li, Zhen Wu, Yawen Ouyang, Jianbing Zhang, Xinyu Dai |  |
| 1756 |  |  [Detection of Multiple Mental Disorders from Social Media with Two-Stream Psychiatric Experts](https://doi.org/10.18653/v1/2023.emnlp-main.562) |  | 0 | Existing Mental Disease Detection (MDD) research largely studies the detection of a single disorder, overlooking the fact that mental diseases might occur in tandem. Many approaches are not backed by domain knowledge (e.g., psychiatric symptoms) and thus fail to produce interpretable results. To tackle these issues, we propose an MDD framework that is capable of learning the shared clues of all diseases, while also capturing the specificity of each single disease. The two-stream architecture... | Siyuan Chen, Zhiling Zhang, Mengyue Wu, Kenny Q. Zhu |  |
| 1757 |  |  [Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?](https://doi.org/10.18653/v1/2023.emnlp-main.563) |  | 0 | Understanding how and what pre-trained language models (PLMs) learn about language is an open challenge in natural language processing. Previous work has focused on identifying whether they capture semantic and syntactic information, and how the data or the pre-training objective affects their performance. However, to the best of our knowledge, no previous work has specifically examined how information loss in input token characters affects the performance of PLMs. In this study, we address... | Ahmed Alajrami, Katerina Margatina, Nikolaos Aletras |  |
| 1758 |  |  [Improved Unsupervised Chinese Word Segmentation Using Pre-trained Knowledge and Pseudo-labeling Transfer](https://doi.org/10.18653/v1/2023.emnlp-main.564) |  | 0 | Unsupervised Chinese word segmentation (UCWS) has made progress by incorporating linguistic knowledge from pre-trained language models using parameter-free probing techniques. However, such approaches suffer from increased training time due to the need for multiple inferences using a pre-trained language model to perform word segmentation. This work introduces a novel way to enhance UCWS performance while maintaining training efficiency. Our proposed method integrates the segmentation signal... | HsiuWen Li, YingJia Lin, YiTing Li, Chun Lin, HungYu Kao |  |
| 1759 |  |  [EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.565) |  | 0 | Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this... | Hanlin Tang, Yifu Sun, Decheng Wu, Kai Liu, Jianchen Zhu, Zhanhui Kang |  |
| 1760 |  |  [Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings](https://doi.org/10.18653/v1/2023.emnlp-main.566) |  | 0 | Entity linking methods based on dense retrieval are widely adopted in large-scale applications for their efficiency, but they can fall short of generative models, as they are sensitive to the structure of the embedding space. To address this issue, this paper introduces DUCK, an approach to infusing structural information in the space of entity representations, using prior knowledge of entity types. Inspired by duck typing in programming languages, we define the type of an entity based on its... | Mattia Atzeni, Mikhail Plekhanov, Frédéric A. Dreyer, Nora Kassner, Simone Merello, Louis Martin, Nicola Cancedda |  |
| 1761 |  |  [APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.567) |  | 0 | With the continuous growth of large language models, the process of fine-tuning these models for new tasks has become increasingly parameter-intensive. Prompt tuning, a method that involves tuning a small set of soft prompts, has emerged as an effective and efficient approach for adapting large pre-trained language models. However, most existing prompt tuning approaches only introduce prompts at the input layer, limiting their performance and leaving large rooms for improvement. In this work,... | Qifan Wang, Yuning Mao, Jingang Wang, Hanchao Yu, Shaoliang Nie, Sinong Wang, Fuli Feng, Lifu Huang, Xiaojun Quan, Zenglin Xu, Dongfang Liu |  |
| 1762 |  |  [What's "up" with vision-language models? Investigating their struggle with spatial reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.568) |  | 0 | Recent vision-language (VL) models are powerful, but can they reliably distinguish “right” from “left”? We curate three new corpora to quantify model comprehension of such basic spatial relations. These tests isolate spatial reasoning more precisely than existing datasets like VQAv2, e.g., our What’sUp benchmark contains sets of photographs varying only the spatial relations of objects, keeping their identity fixed (see Figure 1: models must comprehend not only the usual case of a dog under a... | Amita Kamath, Jack Hessel, KaiWei Chang |  |
| 1763 |  |  [IBADR: an Iterative Bias-Aware Dataset Refinement Framework for Debiasing NLU models](https://doi.org/10.18653/v1/2023.emnlp-main.569) |  | 0 | As commonly-used methods for debiasing natural language understanding (NLU) models, dataset refinement approaches heavily rely on manual data analysis, and thus maybe unable to cover all the potential biased features. In this paper, we propose IBADR, an Iterative Bias-Aware Dataset Refinement framework, which debiases NLU models without predefining biased features. We maintain an iteratively expanded sample pool. Specifically, at each iteration, we first train a shallow model to quantify the... | Xiaoyue Wang, Xin Liu, Lijie Wang, Yaoxiang Wang, Jinsong Su, Hua Wu |  |
| 1764 |  |  [Learning Preference Model for LLMs via Automatic Preference Data Generation](https://doi.org/10.18653/v1/2023.emnlp-main.570) |  | 0 | Despite the advanced capacities of the state-of-the-art large language models (LLMs), they suffer from issues of hallucination, stereotype, etc. Preference models play an important role in LLM alignment, yet training preference models predominantly rely on human-annotated data. This reliance limits their versatility and scalability. In this paper, we propose learning the preference model for LLMs via automatic preference data generation (AutoPM). Our approach involves both In-Breadth Data... | Shijia Huang, Jianqiao Zhao, Yanyang Li, Liwei Wang |  |
| 1765 |  |  [Multilingual k-Nearest-Neighbor Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.571) |  | 0 | k-nearest-neighbor machine translation has demonstrated remarkable improvements in machine translation quality by creating a datastore of cached examples. However, these improvements have been limited to high-resource language pairs, with large datastores, and remain a challenge for low-resource languages. In this paper, we address this issue by combining representations from multiple languages into a single datastore. Our results consistently demonstrate substantial improvements not only in... | David Stap, Christof Monz |  |
| 1766 |  |  [Understanding Computational Models of Semantic Change: New Insights from the Speech Community](https://doi.org/10.18653/v1/2023.emnlp-main.572) |  | 0 | We investigate the descriptive relevance of widely used semantic change models in linguistic descriptions of present-day speech communities. We focus on the sociolinguistic issue of contact-induced semantic shifts in Quebec English, and analyze 40 target words using type-level and token-level word embeddings, empirical linguistic properties, and – crucially – acceptability ratings and qualitative remarks by 15 speakers from Montreal. Our results confirm the overall relevance of the... | Filip Miletic, Anne PrzewoznyDesriaux, Ludovic Tanguy |  |
| 1767 |  |  [Causal Reasoning through Two Cognition Layers for Improving Generalization in Visual Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.573) |  | 0 | Generalization in Visual Question Answering (VQA) requires models to answer questions about images with contexts beyond the training distribution. Existing attempts primarily refine unimodal aspects, overlooking enhancements in multimodal aspects. Besides, diverse interpretations of the input lead to various modes of answer generation, highlighting the role of causal reasoning between interpreting and answering steps in VQA. Through this lens, we propose Cognitive pathways VQA (CopVQA)... | Trang Nguyen, Naoaki Okazaki |  |
| 1768 |  |  [StructGPT: A General Framework for Large Language Model to Reason over Structured Data](https://doi.org/10.18653/v1/2023.emnlp-main.574) |  | 0 | In this paper, we aim to improve the reasoning ability of large language models (LLMs) over structured data in a unified way. Inspired by the studies on tool augmentation for LLMs, we develop an Iterative Reading-then-Reasoning (IRR) framework to solve question answering tasks based on structured data, called StructGPT. In this framework, we construct the specialized interfaces to collect relevant evidence from structured data (i.e., reading), and let LLMs concentrate on the reasoning task... | Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, JiRong Wen |  |
| 1769 |  |  [Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement](https://doi.org/10.18653/v1/2023.emnlp-main.575) |  | 0 | Generative language models (LMs) are increasingly used for document class-prediction tasks and promise enormous improvements in cost and efficiency. Existing research often examines simple classification tasks, but the capability of LMs to classify on complex or specialized tasks is less well understood. We consider a highly complex task that is challenging even for humans: the classification of legal reasoning according to jurisprudential philosophy. Using a novel dataset of historical United... | Rosamond Elizabeth Thalken, Edward H. Stiglitz, David Mimno, Matthew Wilkens |  |
| 1770 |  |  [Model-tuning Via Prompts Makes NLP Models Adversarially Robust](https://doi.org/10.18653/v1/2023.emnlp-main.576) |  | 0 | In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token’s hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP-FT). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations. In this work, we demonstrate... | Mrigank Raman, Pratyush Maini, J. Zico Kolter, Zachary C. Lipton, Danish Pruthi |  |
| 1771 |  |  [Learning Co-Speech Gesture for Multimodal Aphasia Type Detection](https://doi.org/10.18653/v1/2023.emnlp-main.577) |  | 0 | Aphasia, a language disorder resulting from brain damage, requires accurate identification of specific aphasia types, such as Broca’s and Wernicke’s aphasia, for effective treatment. However, little attention has been paid to developing methods to detect different types of aphasia. Recognizing the importance of analyzing co-speech gestures for distinguish aphasia types, we propose a multimodal graph neural network for aphasia type detection using speech and corresponding gesture patterns. By... | Daeun Lee, Sejung Son, Hyolim Jeon, Seungbae Kim, Jinyoung Han |  |
| 1772 |  |  [STINMatch: Semi-Supervised Semantic-Topological Iteration Network for Financial Risk Detection via News Label Diffusion](https://doi.org/10.18653/v1/2023.emnlp-main.578) |  | 0 | Commercial news provide rich semantics and timely information for automated financial risk detection. However, unaffordable large-scale annotation as well as training data sparseness barrier the full exploitation of commercial news in risk detection. To address this problem, we propose a semi-supervised Semantic-Topological Iteration Network, STINMatch, along with a news-enterprise knowledge graph (NEKG) to endorse the risk detection enhancement. The proposed model incorporates a label... | Xurui Li, Yue Qin, Rui Zhu, Tianqianjin Lin, Yongming Fan, Yangyang Kang, Kaisong Song, Fubang Zhao, Changlong Sun, Haixu Tang, Xiaozhong Liu |  |
| 1773 |  |  [Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection](https://doi.org/10.18653/v1/2023.emnlp-main.579) |  | 0 | The impact of AI models on marginalized communities has traditionally been measured by identifying performance differences between specified demographic subgroups. Though this approach aims to center vulnerable groups, it risks obscuring patterns of harm faced by intersectional subgroups or shared across multiple groups. To address this, we draw on theories of marginalization from disability studies and related disciplines, which state that people farther from the norm face greater adversity,... | Vyoma Raman, Eve Fleisig, Dan Klein |  |
| 1774 |  |  [Describe Me an Auklet: Generating Grounded Perceptual Category Descriptions](https://doi.org/10.18653/v1/2023.emnlp-main.580) |  | 0 | Human speakers can generate descriptions of perceptual concepts, abstracted from the instance-level. Moreover, such descriptions can be used by other speakers to learn provisional representations of those concepts. Learning and using abstract perceptual concepts is under-investigated in the language-and-vision field. The problem is also highly relevant to the field of representation learning in multi-modal NLP. In this paper, we introduce a framework for testing category-level perceptual... | Bill Noble, Nikolai Ilinykh |  |
| 1775 |  |  [Revisiting Automated Topic Model Evaluation with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.581) |  | 0 | Topic models help us make sense of large text collections. Automatically evaluating their output and determining the optimal number of topics are both longstanding challenges, with no effective automated solutions to date. This paper proposes using large language models (LLMs) for these tasks. We find that LLMs appropriately assess the resulting topics, correlating more strongly with human judgments than existing automated metrics. However, the setup of the evaluation task is crucial — LLMs... | Dominik Stammbach, Vilém Zouhar, Alexander Miserlis Hoyle, Mrinmaya Sachan, Elliott Ash |  |
| 1776 |  |  [ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection and Argumentative Dialogue Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.582) |  | 0 | Dialogue agents have been receiving increasing attention for years, and this trend has been further boosted by the recent progress of large language models (LLMs). Stance detection and dialogue summarization are two core tasks of dialogue agents in application scenarios that involve argumentative dialogues. However, research on these tasks is limited by the insufficiency of public datasets, especially for non-English languages. To address this language resource gap in Chinese, we present ORCHID... | Xiutian Zhao, Ke Wang, Wei Peng |  |
| 1777 |  |  [On the Benefits of Learning to Route in Mixture-of-Experts Models](https://doi.org/10.18653/v1/2023.emnlp-main.583) |  | 0 | Mixture-of-Expert (MoE) Transformer models, such as the Switch Transformer, allow us to successfully scale up model sizes while keeping the amount of compute time fixed. Prior work has established the computational efficiency benefits of using these models. A core component of these models is a router that routes input tokens to different experts in a layer. We show theoretical and empirical evidence that the router’s ability to route tokens intelligently confers a significant advantage to MoE... | Nishanth Dikkala, Nikhil Ghosh, Raghu Meka, Rina Panigrahy, Nikhil Vyas, Xin Wang |  |
| 1778 |  |  [SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation](https://doi.org/10.18653/v1/2023.emnlp-main.584) |  | 0 | Reliable automatic evaluation of summarization systems is challenging due to the multifaceted and subjective nature of the task. This is especially the case for languages other than English, where human evaluations are scarce. In this work, we introduce SEAHORSE, a dataset for multilingual, multifaceted summarization evaluation. SEAHORSE consists of 96K summaries with human ratings along 6 dimensions of text quality: comprehensibility, repetition, grammar, attribution, main ideas, and... | Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua Maynez, Roee Aharoni, Vitaly Nikolaev, Thibault Sellam, Aditya Siddhant, Dipanjan Das, Ankur P. Parikh |  |
| 1779 |  |  [Query2doc: Query Expansion with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.585) |  | 0 | This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query... | Liang Wang, Nan Yang, Furu Wei |  |
| 1780 |  |  [We Need to Talk About Reproducibility in NLP Model Comparison](https://doi.org/10.18653/v1/2023.emnlp-main.586) |  | 0 | NLPers frequently face reproducibility crisis in a comparison of various models of a real-world NLP task. Many studies have empirically showed that the standard splits tend to produce low reproducible and unreliable conclusions, and they attempted to improve the splits by using more random repetitions. However, the improvement on the reproducibility in a comparison of NLP models is limited attributed to a lack of investigation on the relationship between the reproducibility and the estimator... | Yan Xue, Xuefei Cao, Xingli Yang, Yu Wang, Ruibo Wang, Jihong Li |  |
| 1781 |  |  [Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration](https://doi.org/10.18653/v1/2023.emnlp-main.587) |  | 0 | Instruction-tuning can be substantially optimized through enhanced diversity, resulting in models capable of handling a broader spectrum of tasks. However, existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension and interactions within these areas. To address this deficiency, we propose Explore-Instruct, a novel approach to enhance the data coverage to be used in domain-specific instruction-tuning through... | Fanqi Wan, Xinting Huang, Tao Yang, Xiaojun Quan, Wei Bi, Shuming Shi |  |
| 1782 |  |  [Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions](https://doi.org/10.18653/v1/2023.emnlp-main.588) |  | 0 | Recent studies of the computational power of recurrent neural networks (RNNs) reveal a hierarchy of RNN architectures, given real-time and finite-precision assumptions. Here we study auto-regressive Transformers with linearised attention, a.k.a. linear Transformers (LTs) or Fast Weight Programmers (FWPs). LTs are special in the sense that they are equivalent to RNN-like sequence processors with a fixed-size state, while they can also be expressed as the now-popular self-attention networks. We... | Kazuki Irie, Róbert Csordás, Jürgen Schmidhuber |  |
| 1783 |  |  [InterFair: Debiasing with Natural Language Feedback for Fair Interpretable Predictions](https://doi.org/10.18653/v1/2023.emnlp-main.589) |  | 0 | Debiasing methods in NLP models traditionally focus on isolating information related to a sensitive attribute (e.g., gender or race). We instead argue that a favorable debiasing method should use sensitive information ‘fairly,’ with explanations, rather than blindly eliminating it. This fair balance is often subjective and can be challenging to achieve algorithmically. We explore two interactive setups with a frozen predictive model and show that users able to provide feedback can achieve a... | Bodhisattwa Prasad Majumder, Zexue He, Julian J. McAuley |  |
| 1784 |  |  [Just Adjust One Prompt: Enhancing In-Context Dialogue Scoring via Constructing the Optimal Subgraph of Demonstrations and Prompts](https://doi.org/10.18653/v1/2023.emnlp-main.590) |  | 0 | The use of modern Large Language Models (LLMs) as chatbots still has some problems such as hallucinations and lack of empathy. Identifying these issues can help improve chatbot performance. The community has been continually iterating on reference-free dialogue evaluation methods based on large language models (LLMs) that can be readily applied. However, many of these LLM-based metrics require selecting specific datasets and developing specialized training tasks for different evaluation... | Jiashu Pu, Ling Cheng, Lu Fan, Tangjie Lv, Rongsheng Zhang |  |
| 1785 |  |  [Multilingual estimation of political-party positioning: From label aggregation to long-input Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.591) |  | 0 | Scaling analysis is a technique in computational political science that assigns a political actor (e.g. politician or party) a score on a predefined scale based on a (typically long) body of text (e.g. a parliamentary speech or an election manifesto). For example, political scientists have often used the left–right scale to systematically analyse political landscapes of different countries. NLP methods for automatic scaling analysis can find broad application provided they (i) are able to deal... | Dmitry Nikolaev, Tanise Ceron, Sebastian Padó |  |
| 1786 |  |  [ART: rule bAsed futuRe-inference deducTion](https://doi.org/10.18653/v1/2023.emnlp-main.592) |  | 0 | Deductive reasoning is a crucial cognitive ability of humanity, allowing us to derive valid conclusions from premises and observations. However, existing works mainly focus on language-based premises and generally neglect deductive reasoning from visual observations. In this work, we introduce rule bAsed futuRe-inference deducTion (ART), which aims at deducing the correct future event based on the visual phenomenon (a video) and the rule-based premises, along with an explanation of the... | Mengze Li, Tianqi Zhao, Jionghao Bai, Baoyi He, Jiaxu Miao, Wei Ji, Zheqi Lv, Zhou Zhao, Shengyu Zhang, Wenqiao Zhang, Fei Wu |  |
| 1787 |  |  [EpiK-Eval: Evaluation for Language Models as Epistemic Models](https://doi.org/10.18653/v1/2023.emnlp-main.593) |  | 0 | In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents—a crucial ability in numerous applications—remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to... | Gabriele Prato, Jerry Huang, Prasanna Parthasarathi, Shagun Sodhani, Sarath Chandar |  |
| 1788 |  |  [From Dissonance to Insights: Dissecting Disagreements in Rationale Construction for Case Outcome Classification](https://doi.org/10.18653/v1/2023.emnlp-main.594) |  | 0 | In legal NLP, Case Outcome Classification (COC) must not only be accurate but also trustworthy and explainable. Existing work in explainable COC has been limited to annotations by a single expert. However, it is well-known that lawyers may disagree in their assessment of case facts. We hence collect a novel dataset RaVE: Rationale Variation in ECHR, which is obtained from two experts in the domain of international human rights law, for whom we observe weak agreement. We study their... | Shanshan Xu, T. Y. S. S. Santosh, Oana Ichim, Isabella Risini, Barbara Plank, Matthias Grabmair |  |
| 1789 |  |  [On Bilingual Lexicon Induction with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.595) |  | 0 | Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against... | Yaoyiran Li, Anna Korhonen, Ivan Vulic |  |
| 1790 |  |  [Statistical Depth for Ranking and Characterizing Transformer-Based Text Embeddings](https://doi.org/10.18653/v1/2023.emnlp-main.596) |  | 0 | The popularity of transformer-based text embeddings calls for better statistical tools for measuring distributions of such embeddings. One such tool would be a method for ranking texts within a corpus by centrality, i.e. assigning each text a number signifying how representative that text is of the corpus as a whole. However, an intrinsic center-outward ordering of high-dimensional text representations is not trivial. A statistical depth is a function for ranking k-dimensional objects by... | Parker Seegmiller, Sarah Preum |  |
| 1791 |  |  [CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model](https://doi.org/10.18653/v1/2023.emnlp-main.597) |  | 0 | Instruction tuning has recently been recognized as an effective way of aligning Large Language Models (LLMs) to enhance their generalization ability across various tasks. However, when tuning publicly accessible, centralized LLMs with private instruction data, privacy concerns are inevitable. While direct transfer of parameterized modules between models is a plausible approach to address this, its implications and effectiveness need further exploration. This paper focuses on Offsite-Tuning... | Kaiyan Zhang, Ning Ding, Biqing Qi, Xuekai Zhu, Xinwei Long, Bowen Zhou |  |
| 1792 |  |  [From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues](https://doi.org/10.18653/v1/2023.emnlp-main.598) |  | 0 | Understanding emotions during conversation is a fundamental aspect of human communication, driving NLP research for Emotion Recognition in Conversation (ERC). While considerable research has focused on discerning emotions of individual speakers in monolingual dialogues, understanding the emotional dynamics in code-mixed conversations has received relatively less attention. This motivates our undertaking of ERC for code-mixed conversations in this study. Recognizing that emotional intelligence... | Shivani Kumar, Ramaneswaran S., Md. Shad Akhtar, Tanmoy Chakraborty |  |
| 1793 |  |  [Large Language Models are biased to overestimate profoundness](https://doi.org/10.18653/v1/2023.emnlp-main.599) |  | 0 | Recent advancements in natural language processing by large language models (LLMs), such as GPT-4, have been suggested to approach Artificial General Intelligence. And yet, it is still under dispute whether LLMs possess similar reasoning abilities to humans. This study evaluates GPT-4 and various other LLMs in judging the profoundness of mundane, motivational, and pseudo-profound statements. We found a significant statement-to-statement correlation between the LLMs and humans, irrespective of... | Eugenio HerreraBerg, Tomás Vergara Browne, Pablo LeónVillagrá, MarcLluís Vives, Cristian Buc Calderon |  |
| 1794 |  |  [SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.600) |  | 0 | With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals issues with... | Philippe Laban, Wojciech Kryscinski, Divyansh Agarwal, Alexander R. Fabbri, Caiming Xiong, Shafiq Joty, ChienSheng Wu |  |
| 1795 |  |  [DIVE: Towards Descriptive and Diverse Visual Commonsense Generation](https://doi.org/10.18653/v1/2023.emnlp-main.601) |  | 0 | Towards human-level visual understanding, visual commonsense generation has been introduced to generate commonsense inferences beyond images. However, current research on visual commonsense generation has overlooked an important human cognitive ability: generating descriptive and diverse inferences. In this work, we propose a novel visual commonsense generation framework, called DIVE, which aims to improve the descriptiveness and diversity of generated inferences. DIVE involves two methods,... | JunHyung Park, Hyuntae Park, Youjin Kang, Eojin Jeon, SangKeun Lee |  |
| 1796 |  |  [Towards Conceptualization of "Fair Explanation": Disparate Impacts of anti-Asian Hate Speech Explanations on Content Moderators](https://doi.org/10.18653/v1/2023.emnlp-main.602) |  | 0 | Recent research at the intersection of AI explainability and fairness has focused on how explanations can improve human-plus-AI task performance as assessed by fairness measures. We propose to characterize what constitutes an explanation that is itself “fair” – an explanation that does not adversely impact specific populations. We formulate a novel evaluation method of “fair explanations” using not just accuracy and label time, but also psychological impact of explanations on different user... | Tin Nguyen, Jiannan Xu, Aayushi Roy, Hal Daumé III, Marine Carpuat |  |
| 1797 |  |  [Bridging Background Knowledge Gaps in Translation with Automatic Explicitation](https://doi.org/10.18653/v1/2023.emnlp-main.603) |  | 0 | Translations help people understand content written in another language. However, even correct literal translations do not fulfill that goal when people lack the necessary background to understand them. Professional translators incorporate explicitations to explain the missing context by considering cultural differences between source and target audiences. Despite its potential to help users, NLP research on explicitation is limited because of the dearth of adequate evaluation methods. This... | HyoJung Han, Jordan L. BoydGraber, Marine Carpuat |  |
| 1798 |  |  [A Quality-based Syntactic Template Retriever for Syntactically-Controlled Paraphrase Generation](https://doi.org/10.18653/v1/2023.emnlp-main.604) |  | 0 | Existing syntactically-controlled paraphrase generation (SPG) models perform promisingly with human-annotated or well-chosen syntactic templates. However, the difficulty of obtaining such templates actually hinders the practical application of SPG models. For one thing, the prohibitive cost makes it unfeasible to manually design decent templates for every source sentence. For another, the templates automatically retrieved by current heuristic methods are usually unreliable for SPG models to... | Xue Zhang, Songming Zhang, Yunlong Liang, Yufeng Chen, Jian Liu, Wenjuan Han, Jinan Xu |  |
| 1799 |  |  [Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.605) |  | 0 | Using a shared vocabulary is common practice in Multilingual Neural Machine Translation (MNMT). In addition to its simple design, shared tokens play an important role in positive knowledge transfer, which manifests naturally when the shared tokens refer to similar meanings across languages. However, when words overlap is small, e.g., using different writing systems, transfer is inhibited. In this paper, we propose a re-parameterized method for building embeddings to alleviate this problem. More... | Di Wu, Christof Monz |  |
| 1800 |  |  [Quantifying the redundancy between prosody and text](https://doi.org/10.18653/v1/2023.emnlp-main.606) |  | 0 | Prosody—the suprasegmental component of speech, including pitch, loudness, and tempo—carries critical aspects of meaning. However, the relationship between the information conveyed by prosody vs. by the words themselves remains poorly understood. We use large language models (LLMs) to estimate how much information is redundant between prosody and the words themselves. Using a large spoken corpus of English audiobooks, we extract prosodic features aligned to individual words and test how well... | Lukas Wolf, Tiago Pimentel, Evelina Fedorenko, Ryan Cotterell, Alex Warstadt, Ethan Wilcox, Tamar Regev |  |
| 1801 |  |  [CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.607) |  | 0 | Recent efforts in natural language processing (NLP) commonsense reasoning research have yielded a considerable number of new datasets and benchmarks. However, most of these datasets formulate commonsense reasoning challenges in artificial scenarios that are not reflective of the tasks which real-world NLP systems are designed to solve. In this work, we present CRoW, a manually-curated, multi-task benchmark that evaluates the ability of models to apply commonsense reasoning in the context of six... | Mete Ismayilzada, Debjit Paul, Syrielle Montariol, Mor Geva, Antoine Bosselut |  |
| 1802 |  |  [A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot](https://doi.org/10.18653/v1/2023.emnlp-main.608) |  | 0 | Multimedia content, such as advertisements and story videos, exhibit a rich blend of creativity and multiple modalities. They incorporate elements like text, visuals, audio, and storytelling techniques, employing devices like emotions, symbolism, and slogans to convey meaning. There is a dearth of large annotated training datasets in the multimedia domain hindering the development of supervised learning models with satisfactory performance for real-world applications. On the other hand, the... | Aanisha Bhattacharyya, Yaman Singla, Balaji Krishnamurthy, Rajiv Ratn Shah, Changyou Chen |  |
| 1803 |  |  [Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.609) |  | 0 | In-context learning (ICL) emerges as a promising capability of large language models (LLMs) by providing them with demonstration examples to perform diverse tasks. However, the underlying mechanism of how LLMs learn from the provided context remains under-explored. In this paper, we investigate the working mechanism of ICL through an information flow lens. Our findings reveal that label words in the demonstration examples function as anchors: (1) semantic information aggregates into label word... | Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun |  |
| 1804 |  |  [Prompting Scientific Names for Zero-Shot Species Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.610) |  | 0 | Trained on web-scale image-text pairs, Vision-Language Models (VLMs) such as CLIP can recognize images of common objects in a zero-shot fashion. However, it is underexplored how to use CLIP for zero-shot recognition of highly specialized concepts, e.g., species of birds, plants, and animals, for which their scientific names are written in Latin or Greek. Indeed, CLIP performs poorly for zero-shot species recognition with prompts that use scientific names, e.g., “a photo of Lepus Timidus” (which... | Shubham Parashar, Zhiqiu Lin, Yanan Li, Shu Kong |  |
| 1805 |  |  [Active Learning for Natural Language Generation](https://doi.org/10.18653/v1/2023.emnlp-main.611) |  | 0 | The field of Natural Language Generation (NLG) suffers from a severe shortage of labeled data due to the extremely expensive and time-consuming process involved in manual annotation. A natural approach for coping with this problem is active learning (AL), a well-known machine learning technique for improving annotation efficiency by selectively choosing the most informative examples to label. However, while AL has been well-researched in the context of text classification, its application to... | Yotam Perlitz, Ariel Gera, Michal ShmueliScheuer, Dafna Sheinwald, Noam Slonim, Liat EinDor |  |
| 1806 |  |  [Re³Dial: Retrieve, Reorganize and Rescale Conversations for Long-Turn Open-Domain Dialogue Pre-training](https://doi.org/10.18653/v1/2023.emnlp-main.612) |  | 0 | Pre-training on large-scale open-domain dialogue data can substantially improve the performance of dialogue models. However, the pre-trained dialogue model’s ability to utilize long-range context is limited due to the scarcity of long-turn dialogue sessions. Most dialogues in existing pre-training corpora contain fewer than three turns of dialogue. To alleviate this issue, we propose the Retrieve, Reorganize and Rescale framework (Re3Dial), which can automatically construct billion-scale... | Jiaxin Wen, Hao Zhou, Jian Guan, Jie Zhou, Minlie Huang |  |
| 1807 |  |  [MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup](https://doi.org/10.18653/v1/2023.emnlp-main.613) |  | 0 | Current disfluency detection models focus on individual utterances each from a single speaker. However, numerous discontinuity phenomena in spoken conversational transcripts occur across multiple turns, which can not be identified by disfluency detection models. This study addresses these phenomena by proposing an innovative Multi-Turn Cleanup task for spoken conversational transcripts and collecting a new dataset, MultiTurnCleanup. We design a data labeling schema to collect the high-quality... | Hua Shen, Vicky Zayats, Johann C. Rocholl, Daniel D. Walker, Dirk Padfield |  |
| 1808 |  |  [Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.614) |  | 0 | Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of “tokens” processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same... | Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, Yulia Tsvetkov |  |
| 1809 |  |  [Characterizing Mechanisms for Factual Recall in Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.615) |  | 0 | Language Models (LMs) often must integrate facts they memorized in pretraining with new information that appears in a given context. These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict. On a dataset that queries for knowledge of world capitals, we investigate both distributional and mechanistic determinants of LM behavior in such situations. Specifically, we measure the proportion of the time an LM will use a counterfactual... | Qinan Yu, Jack Merullo, Ellie Pavlick |  |
| 1810 |  |  [MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark](https://doi.org/10.18653/v1/2023.emnlp-main.616) |  | 0 | There is a lack of research into capabilities of recent LLMs to generate convincing text in languages other than English and into performance of detectors of machine-generated text in multilingual settings. This is also reflected in the available benchmarks which lack authentic texts in languages other than English and predominantly cover older generators. To fill this gap, we introduce MULTITuDE, a novel benchmarking dataset for multilingual machine-generated text detection comprising of... | Dominik Macko, Róbert Móro, Adaku Uchendu, Jason Samuel Lucas, Michiharu Yamashita, Matús Pikuliak, Ivan Srba, Thai Le, Dongwon Lee, Jakub Simko, Mária Bieliková |  |
| 1811 |  |  [Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?](https://doi.org/10.18653/v1/2023.emnlp-main.617) |  | 0 | The inference of Large language models (LLMs) requires immense computation and memory resources. To curtail these costs, quantisation has emerged as a promising solution, but existing LLM quantisation mainly focuses on 8-bit. In this work, we explore the statistical and learning properties of the LLM layer and attribute the bottleneck of LLM quantisation to numerical scaling offsets. To address this, we adapt block quantisations for LLMs, a family of methods that share scaling factors across... | Cheng Zhang, Jianyi Cheng, Ilia Shumailov, George A. Constantinides, Yiren Zhao |  |
| 1812 |  |  [Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.618) |  | 0 | We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR). Our methodology leverages both acoustic information and external linguistic representations to generate accurate speech transcription contexts. This marks a step towards a fresh paradigm in generative error correction within the realm of n-best hypotheses. Unlike the existing ranking-based rescoring methods, our approach adeptly uses distinct initialization techniques... | Srijith Radhakrishnan, ChaoHan Huck Yang, Sumeer Ahmad Khan, Rohit Kumar, Narsis A. Kiani, David GomezCabrero, Jesper Tegnér |  |
| 1813 |  |  [Reducing Sequence Length by Predicting Edit Spans with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.619) |  | 0 | Large Language Models (LLMs) have demonstrated remarkable performance in various tasks and gained significant attention. LLMs are also used for local sequence transduction tasks, including grammatical error correction (GEC) and formality style transfer, where most tokens in a source text are kept unchanged. However, the models that generate all target tokens in such tasks have a tendency to simply copy the input text as is, without making needed changes, because the difference between input and... | Masahiro Kaneko, Naoaki Okazaki |  |
| 1814 |  |  [Instruct and Extract: Instruction Tuning for On-Demand Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.620) |  | 0 | Large language models with instruction-following capabilities open the door to a wider group of users. However, when it comes to information extraction – a classic task in natural language processing – most task-specific systems cannot align well with long-tail ad hoc extraction use cases for non-expert users. To address this, we propose a novel paradigm, termed On-Demand Information Extraction, to fulfill the personalized demands of real-world users. Our task aims to follow the instructions to... | Yizhu Jiao, Ming Zhong, Sha Li, Ruining Zhao, Siru Ouyang, Heng Ji, Jiawei Han |  |
| 1815 |  |  [Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.621) |  | 0 | The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs. In this paper, we embark on an investigation into the utilization of ChatGPT for CRSs, revealing the inadequacy of the existing evaluation protocol. It might overemphasize the matching with ground-truth items annotated by humans while neglecting the interactive nature of CRSs. To... | Xiaolei Wang, Xinyu Tang, Xin Zhao, Jingyuan Wang, JiRong Wen |  |
| 1816 |  |  [ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness](https://doi.org/10.18653/v1/2023.emnlp-main.622) |  | 0 | Multi-step reasoning ability is fundamental to many natural language tasks, yet it is unclear what constitutes a good reasoning chain and how to evaluate them. Most existing methods focus solely on whether the reasoning chain leads to the correct conclusion, but this answer-oriented view may confound reasoning quality with other spurious shortcuts to predict the answer. To bridge this gap, we evaluate reasoning chains by viewing them as informal proofs that derive the final answer.... | Archiki Prasad, Swarnadeep Saha, Xiang Zhou, Mohit Bansal |  |
| 1817 |  |  [Expand, Highlight, Generate: RL-driven Document Generation for Passage Reranking](https://doi.org/10.18653/v1/2023.emnlp-main.623) |  | 0 | Generating synthetic training data based on large language models (LLMs) for ranking models has gained attention recently. Prior studies use LLMs to build pseudo query-document pairs by generating synthetic queries from documents in a corpus. In this paper, we propose a new perspective of data augmentation: generating synthetic documents from queries. To achieve this, we propose DocGen, that consists of a three-step pipeline that utilizes the few-shot capabilities of LLMs. DocGen pipeline... | Arian Askari, Mohammad Aliannejadi, Chuan Meng, Evangelos Kanoulas, Suzan Verberne |  |
| 1818 |  |  [Transformer-based Live Update Generation for Soccer Matches from Microblog Posts](https://doi.org/10.18653/v1/2023.emnlp-main.624) |  | 0 | It has been known to be difficult to generate adequate sports updates from a sequence of vast amounts of diverse live tweets, although the live sports viewing experience with tweets is gaining the popularity. In this paper, we focus on soccer matches and work on building a system to generate live updates for soccer matches from tweets so that users can instantly grasp a match’s progress and enjoy the excitement of the match from raw tweets. Our proposed system is based on a large pre-trained... | Masashi Oshika, Kosuke Yamada, Ryohei Sasano, Koichi Takeda |  |
| 1819 |  |  [Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets](https://doi.org/10.18653/v1/2023.emnlp-main.625) |  | 0 | Increasingly larger datasets have become a standard ingredient to advancing the state-of-the-art in NLP. However, data quality might have already become the bottleneck to unlock further gains. Given the diversity and the sizes of modern datasets, standard data filtering is not straight-forward to apply, because of the multifacetedness of the harmful data and elusiveness of filtering rules that would generalize across multiple tasks. We study the fitness of task-agnostic self-influence scores of... | Irina Bejan, Artem Sokolov, Katja Filippova |  |
| 1820 |  |  [Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews](https://doi.org/10.18653/v1/2023.emnlp-main.626) |  | 0 | Medical systematic reviews play a vital role in healthcare decision making and policy. However, their production is time-consuming, limiting the availability of high-quality and up-to-date evidence summaries. Recent advancements in LLMs offer the potential to automatically generate literature reviews on demand, addressing this issue. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and... | Hye Sun Yun, Iain James Marshall, Thomas A. Trikalinos, Byron C. Wallace |  |
| 1821 |  |  [PromptST: Abstract Prompt Learning for End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-main.627) |  | 0 | An end-to-end speech-to-text (S2T) translation model is usually initialized from a pre-trained speech recognition encoder and a pre-trained text-to-text (T2T) translation decoder. Although this straightforward setting has been shown empirically successful, there do not exist clear answers to the research questions: 1) how are speech and text modalities fused in S2T model and 2) how to better fuse the two modalities? In this paper, we take the first step toward understanding the fusion of speech... | Tengfei Yu, Liang Ding, Xuebo Liu, Kehai Chen, Meishan Zhang, Dacheng Tao, Min Zhang |  |
| 1822 |  |  [Text Rendering Strategies for Pixel Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.628) |  | 0 | Pixel-based language models process text rendered as images, which allows them to handle any script, making them a promising approach to open vocabulary language modelling. However, recent approaches use text renderers that produce a large set of almost-equivalent input patches, which may prove sub-optimal for downstream tasks, due to redundancy in the input representations. In this paper, we investigate four approaches to rendering text in the PIXEL model (Rust et al., 2023), and find that... | Jonas F. Lotz, Elizabeth Salesky, Phillip Rust, Desmond Elliott |  |
| 1823 |  |  [APoLLo : Unified Adapter and Prompt Learning for Vision Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.629) |  | 0 | The choice of input text prompt plays a critical role in the performance of Vision-Language Pretrained (VLP) models such as CLIP. We present APoLLo, a unified multi-modal approach that combines Adapter and Prompt learning for Vision-Language models. Our method is designed to substantially improve the generalization capabilities of VLP models when they are fine-tuned in a few-shot setting. We introduce trainable cross-attention-based adapter layers in conjunction with vision and language... | Sanjoy Chowdhury, Sayan Nag, Dinesh Manocha |  |
| 1824 |  |  [SAMRank: Unsupervised Keyphrase Extraction using Self-Attention Map in BERT and GPT-2](https://doi.org/10.18653/v1/2023.emnlp-main.630) |  | 0 | We propose a novel unsupervised keyphrase extraction approach, called SAMRank, which uses only a self-attention map in a pre-trained language model (PLM) to determine the importance of phrases. Most recent approaches for unsupervised keyphrase extraction mainly utilize contextualized embeddings to capture semantic relevance between words, sentences, and documents. However, due to the anisotropic nature of contextual embeddings, these approaches may not be optimal for semantic similarity... | Byungha Kang, Youhyun Shin |  |
| 1825 |  |  [Contrastive Learning for Inference in Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.631) |  | 0 | Inference, especially those derived from inductive processes, is a crucial component in our conversation to complement the information implicitly or explicitly conveyed by a speaker. While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning. In this paper, we analyze the behavior of the models based on the task difficulty defined by the semantic... | Etsuko Ishii, Yan Xu, Bryan Wilie, Ziwei Ji, Holy Lovenia, Willy Chung, Pascale Fung |  |
| 1826 |  |  [Editing Large Language Models: Problems, Methods, and Opportunities](https://doi.org/10.18653/v1/2023.emnlp-main.632) |  | 0 | Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to alter the behavior of LLMs efficiently within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In... | Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang |  |
| 1827 |  |  [MarkQA: A large scale KBQA dataset with numerical reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.633) |  | 0 | While question answering over knowledge bases (KBQA) has shown progress in addressing factoid questions, KBQA with numerical reasoning remains relatively unexplored. In this paper, we focus on the complex numerical reasoning in KBQA, and propose a new task, NR-KBQA, which necessitates the ability to perform both multi-hop reasoning and numerical reasoning. We also design a logic form in Python format called PyQL to represent the reasoning process of numerical reasoning questions. To facilitate... | Xiang Huang, Sitao Cheng, Yuheng Bao, Shanshan Huang, Yuzhong Qu |  |
| 1828 |  |  [Comparing Biases and the Impact of Multilingual Training across Multiple Languages](https://doi.org/10.18653/v1/2023.emnlp-main.634) |  | 0 | Studies in bias and fairness in natural language processing have primarily examined social biases within a single language and/or across few attributes (e.g. gender, race). However, biases can manifest differently across various languages for individual attributes. As a result, it is critical to examine biases within each language and attribute. Of equal importance is to study how these biases compare across languages and how the biases are affected when training a model on multilingual data... | Sharon Levy, Neha Anna John, Ling Liu, Yogarshi Vyas, Jie Ma, Yoshinari Fujinuma, Miguel Ballesteros, Vittorio Castelli, Dan Roth |  |
| 1829 |  |  [HutCRS: Hierarchical User-Interest Tracking for Conversational Recommender System](https://doi.org/10.18653/v1/2023.emnlp-main.635) |  | 0 | Conversational Recommender System (CRS) aims to explicitly acquire user preferences towards items and attributes through natural language conversations. However, existing CRS methods ask users to provide explicit answers (yes/no) for each attribute they require, regardless of users’ knowledge or interest, which may significantly reduce the user experience and semantic consistency. Furthermore, these methods assume that users like all attributes of the target item and dislike those unrelated to... | Mingjie Qian, Yongsen Zheng, Jinghui Qin, Liang Lin |  |
| 1830 |  |  [Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.636) |  | 0 | The tasks of out-of-domain (OOD) intent discovery and generalized intent discovery (GID) aim to extend a closed intent classifier to open-world intent sets, which is crucial to task-oriented dialogue (TOD) systems. Previous methods address them by fine-tuning discriminative models. Recently, although some studies has been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, it is still unclear for the ability of ChatGPT to discover and... | Xiaoshuai Song, Keqing He, Pei Wang, Guanting Dong, Yutao Mou, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu |  |
| 1831 |  |  [The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining](https://doi.org/10.18653/v1/2023.emnlp-main.637) |  | 0 | We analyze the masked language modeling pretraining objective function from the perspective of the Distributional Hypothesis. We investigate whether the better sample efficiency and the better generalization capability of models pretrained with masked language modeling can be attributed to the semantic similarity encoded in the pretraining data’s distributional property. Via a synthetic dataset, our analysis suggests that distributional property indeed leads to the better sample efficiency of... | TingRui Chiang, Dani Yogatama |  |
| 1832 |  |  [Simple and Effective Input Reformulations for Translation](https://doi.org/10.18653/v1/2023.emnlp-main.638) |  | 0 | Foundation language models learn from their finetuning input context in different ways. In this paper, we reformulate inputs during finetuning for challenging translation tasks, leveraging model strengths from pretraining in novel ways to improve downstream performance. These reformulations are simple data level modifications, require no additional collection of training data or modification of data at inference time. They can be applied either on single language pair translation tasks or... | Brian Yu, Hansen Lillemark, Kurt Keutzer |  |
| 1833 |  |  [Pointwise Mutual Information Based Metric and Decoding Strategy for Faithful Generation in Document Grounded Dialogs](https://doi.org/10.18653/v1/2023.emnlp-main.639) |  | 0 | A major concern in using deep learning based generative models for document-grounded dialogs is the potential generation of responses that are not faithful to the underlying document. Existing automated metrics used for evaluating the faithfulness of response with respect to the grounding document measure the degree of similarity between the generated response and the document’s content. However, these automated metrics are far from being well aligned with human judgments. Therefore, to improve... | Yatin Nandwani, Vineet Kumar, Dinesh Raghu, Sachindra Joshi, Luis A. Lastras |  |
| 1834 |  |  [The ACL OCL Corpus: Advancing Open Science in Computational Linguistics](https://doi.org/10.18653/v1/2023.emnlp-main.640) |  | 0 | We present ACL OCL, a scholarly corpus derived from the ACL Anthology to assist Open scientific research in the Computational Linguistics domain. Integrating and enhancing the previous versions of the ACL Anthology, the ACL OCL contributes metadata, PDF files, citation graphs and additional structured full texts with sections, figures, and links to a large knowledge resource (Semantic Scholar). The ACL OCL spans seven decades, containing 73K papers, alongside 210K figures. We spotlight how ACL... | Shaurya Rohatgi, Yanxia Qin, Benjamin Aw, Niranjana Unnithan, MinYen Kan |  |
| 1835 |  |  [Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.641) |  | 0 | Numerous studies have demonstrated the ability of neural language models to learn various linguistic properties without direct supervision. This work takes an initial step towards exploring the less researched topic of how neural models discover linguistic properties of words, such as gender, as well as the rules governing their usage. We propose to use an artificial corpus generated by a PCFG based on French to precisely control the gender distribution in the training data and determine under... | Lina Conti, Guillaume Wisniewski |  |
| 1836 |  |  [Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset](https://doi.org/10.18653/v1/2023.emnlp-main.642) |  | 0 | While recent pre-trained transformer-based models can perform named entity recognition (NER) with great accuracy, their limited range remains an issue when applied to long documents such as whole novels. To alleviate this issue, a solution is to retrieve relevant context at the document level. Unfortunately, the lack of supervision for such a task means one has to settle for unsupervised approaches. Instead, we propose to generate a synthetic context retrieval training dataset using Alpaca, an... | Arthur Amalvy, Vincent Labatut, Richard Dufour |  |
| 1837 |  |  [Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting](https://doi.org/10.18653/v1/2023.emnlp-main.643) |  | 0 | A crucial challenge for generative large language models (LLMs) is diversity: when a user’s prompt is under-specified, models may follow implicit assumptions while generating a response, which may result in homogenization of the responses, as well as certain demographic groups being under-represented or even erased from the generated responses. In this paper, we formalize the problem diversity of representation in LLM generations. We present evaluation datasets and propose metrics to measure... | Preethi Lahoti, Nicholas Blumm, Xiao Ma, Raghavendra Kotikalapudi, Sahitya Potluri, Qijun Tan, Hansa Srinivasan, Ben Packer, Ahmad Beirami, Alex Beutel, Jilin Chen |  |
| 1838 |  |  [Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection](https://doi.org/10.18653/v1/2023.emnlp-main.644) |  | 0 | Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises. Although several detectors have been proposed to address these concerns, their effectiveness against adversarial perturbations, specifically in the context of student essay writing, remains largely unexplored. This paper aims to... | Xinlin Peng, Ying Zhou, Ben He, Le Sun, Yingfei Sun |  |
| 1839 |  |  [Contextual Interaction for Argument Post Quality Assessment](https://doi.org/10.18653/v1/2023.emnlp-main.645) |  | 0 | Recently, there has been an increased emphasis on assessing the quality of natural language arguments. Existing approaches primarily focus on evaluating the quality of individual argument posts. However, they often fall short when it comes to effectively distinguishing arguments that possess a narrow quality margin. To address this limitation, this paper delves into two alternative methods for modeling the relative quality of different arguments. These approaches include: 1) Supervised... | Yiran Wang, Xuanang Chen, Ben He, Le Sun |  |
| 1840 |  |  [Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent Classification](https://doi.org/10.18653/v1/2023.emnlp-main.646) |  | 0 | Intent classification (IC) plays an important role in task-oriented dialogue systems. However, IC models often generalize poorly when training without sufficient annotated examples for each user intent. We propose a novel pre-training method for text encoders that uses contrastive learning with intent psuedo-labels to produce embeddings that are well-suited for IC tasks, reducing the need for manual annotations. By applying this pre-training strategy, we also introduce Pre-trained Intent-aware... | Mujeen Sung, James Gung, Elman Mansimov, Nikolaos Pappas, Raphael Shu, Salvatore Romeo, Yi Zhang, Vittorio Castelli |  |
| 1841 |  |  [Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations](https://doi.org/10.18653/v1/2023.emnlp-main.647) |  | 0 | The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better... | Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, Ming Yin |  |
| 1842 |  |  [GazeVQA: A Video Question Answering Dataset for Multiview Eye-Gaze Task-Oriented Collaborations](https://doi.org/10.18653/v1/2023.emnlp-main.648) |  | 0 | The usage of exocentric and egocentric videos in Video Question Answering (VQA) is a new endeavor in human-robot interaction and collaboration studies. Particularly for egocentric videos, one may leverage eye-gaze information to understand human intentions during the task. In this paper, we build a novel task-oriented VQA dataset, called GazeVQA, for collaborative tasks where gaze information is captured during the task process. GazeVQA is designed with a novel QA format that covers thirteen... | Muhammet Furkan Ilaslan, Chenan Song, Joya Chen, Difei Gao, Weixian Lei, Qianli Xu, Joo Lim, Mike Zheng Shou |  |
| 1843 |  |  [People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection](https://doi.org/10.18653/v1/2023.emnlp-main.649) |  | 0 | NLP models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. Therefore, it is imperative that these models are robust to spurious features. Past work has attempted to tackle such spurious features using training data augmentation, including Counterfactually Augmented Data (CADs). CADs introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious... | Indira Sen, Dennis Assenmacher, Mattia Samory, Isabelle Augenstein, Wil M. P. van der Aalst, Claudia Wagner |  |
| 1844 |  |  [Unraveling Feature Extraction Mechanisms in Neural Networks](https://doi.org/10.18653/v1/2023.emnlp-main.650) |  | 0 | The underlying mechanism of neural networks in capturing precise knowledge has been the subject of consistent research efforts. In this work, we propose a theoretical approach based on Neural Tangent Kernels (NTKs) to investigate such mechanisms. Specifically, considering the infinite network width, we hypothesize the learning dynamics of target models may intuitively unravel the features they acquire from training data, deepening our insights into their internal mechanisms. We apply our... | Xiaobing Sun, Jiaxi Li, Wei Lu |  |
| 1845 |  |  [CAPSTONE: Curriculum Sampling for Dense Retrieval with Document Expansion](https://doi.org/10.18653/v1/2023.emnlp-main.651) |  | 0 | The dual-encoder has become the de facto architecture for dense retrieval. Typically, it computes the latent representations of the query and document independently, thus failing to fully capture the interactions between the query and document. To alleviate this, recent research has focused on obtaining query-informed document representations. During training, it expands the document with a real query, but during inference, it replaces the real query with a generated one. This inconsistency... | Xingwei He, Yeyun Gong, ALong Jin, Hang Zhang, Anlei Dong, Jian Jiao, SiuMing Yiu, Nan Duan |  |
| 1846 |  |  [Balance Act: Mitigating Hubness in Cross-Modal Retrieval with Query and Gallery Banks](https://doi.org/10.18653/v1/2023.emnlp-main.652) |  | 0 | In this work, we present a post-processing solution to address the hubness problem in cross-modal retrieval, a phenomenon where a small number of gallery data points are frequently retrieved, resulting in a decline in retrieval performance. We first theoretically demonstrate the necessity of incorporating both the gallery and query data for addressing hubness as hubs always exhibit high similarity with gallery and query data. Second, building on our theoretical results, we propose a novel... | Yimu Wang, Xiangru Jian, Bo Xue |  |
| 1847 |  |  [E-CORE: Emotion Correlation Enhanced Empathetic Dialogue Generation](https://doi.org/10.18653/v1/2023.emnlp-main.653) |  | 0 | Achieving empathy is a crucial step toward humanized dialogue systems. Current approaches for empathetic dialogue generation mainly perceive an emotional label to generate an empathetic response conditioned on it, which simply treat emotions independently, but ignore the intrinsic emotion correlation in dialogues, resulting in inaccurate emotion perception and unsuitable response generation. In this paper, we propose a novel emotion correlation enhanced empathetic dialogue generation framework,... | Fengyi Fu, Lei Zhang, Quan Wang, Zhendong Mao |  |
| 1848 |  |  [What do Deck Chairs and Sun Hats Have in Common? Uncovering Shared Properties in Large Concept Vocabularies](https://doi.org/10.18653/v1/2023.emnlp-main.654) |  | 0 | Concepts play a central role in many applications. This includes settings where concepts have to be modelled in the absence of sentence context. Previous work has therefore focused on distilling decontextualised concept embeddings from language models. But concepts can be modelled from different perspectives, whereas concept embeddings typically mostly capture taxonomic structure. To address this issue, we propose a strategy for identifying what different concepts, from a potentially large... | Amit Gajbhiye, Zied Bouraoui, Na Li, Usashi Chatterjee, Luis Espinosa Anke, Steven Schockaert |  |
| 1849 |  |  [ALDi: Quantifying the Arabic Level of Dialectness of Text](https://doi.org/10.18653/v1/2023.emnlp-main.655) |  | 0 | Transcribed speech and user-generated text in Arabic typically contain a mixture of Modern Standard Arabic (MSA), the standardized language taught in schools, and Dialectal Arabic (DA), used in daily communications. To handle this variation, previous work in Arabic NLP has focused on Dialect Identification (DI) on the sentence or the token level. However, DI treats the task as binary, whereas we argue that Arabic speakers perceive a spectrum of dialectness, which we operationalize at the... | Amr Keleg, Sharon Goldwater, Walid Magdy |  |
| 1850 |  |  [3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding](https://doi.org/10.18653/v1/2023.emnlp-main.656) |  | 0 | 3D visual grounding aims to localize the target object in a 3D point cloud by a free-form language description. Typically, the sentences describing the target object tend to provide information about its relative relation between other objects and its position within the whole scene. In this work, we propose a relation-aware one-stage framework, named 3D Relative Position-aware Network (3DRP-Net), which can effectively capture the relative spatial relationships between objects and enhance... | Zehan Wang, Haifeng Huang, Yang Zhao, Linjun Li, Xize Cheng, Yichen Zhu, Aoxiong Yin, Zhou Zhao |  |
| 1851 |  |  [Goal-Driven Explainable Clustering via Language Descriptions](https://doi.org/10.18653/v1/2023.emnlp-main.657) |  | 0 | Unsupervised clustering is widely used to explore large corpora, but existing formulations neither consider the users’ goals nor explain clusters’ meanings. We propose a new task formulation, “Goal-Driven Clustering with Explanations” (GoalEx), which represents both the goal and the explanations as free-form language descriptions. For example, to categorize the errors made by a summarization system, the input to GoalEx is a corpus of annotator-written comments for system-generated summaries and... | Zihan Wang, Jingbo Shang, Ruiqi Zhong |  |
| 1852 |  |  [Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.658) |  | 0 | Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge... | Jirui Qi, Raquel Fernández, Arianna Bisazza |  |
| 1853 |  |  [Learning from Mistakes via Cooperative Study Assistant for Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.659) |  | 0 | Large language models (LLMs) have demonstrated their potential to refine their generation based on their own feedback. However, the feedback from LLM itself is often inaccurate, thereby limiting its benefits. In this paper, we propose Study Assistant for Large LAnguage Model (SALAM), a novel framework with an auxiliary agent to assist the main LLM in learning from mistakes through interactive cooperation. In the gathering phase, the student assistant agent probes the main LLM, analyzes its... | Danqing Wang, Lei Li |  |
| 1854 |  |  [Bridging the Digital Divide: Performance Variation across Socio-Economic Factors in Vision-Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.660) |  | 0 | Despite the impressive performance of current AI models reported across various tasks, performance reports often do not include evaluations of how these models perform on the specific groups that will be impacted by these technologies. Among the minority groups under-represented in AI, data from low-income households are often overlooked in data collection and model evaluation. We evaluate the performance of a state-of-the-art vision-language model (CLIP) on a geo-diverse dataset containing... | Joan Nwatu, Oana Ignat, Rada Mihalcea |  |
| 1855 |  |  [Conceptor-Aided Debiasing of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.661) |  | 0 | Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use \*conceptors\*–a soft projection method–to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing by the conceptor NOT operation; and (2) a new architecture,... | Yifei Li, Lyle H. Ungar, João Sedoc |  |
| 1856 |  |  [AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing Evaluation Suite](https://doi.org/10.18653/v1/2023.emnlp-main.662) |  | 0 | We present the Granular AMR Parsing Evaluation Suite (GrAPES), a challenge set for Abstract Meaning Representation (AMR) parsing with accompanying evaluation metrics. AMR parsers now obtain high scores on the standard AMR evaluation metric Smatch, close to or even above reported inter-annotator agreement. But that does not mean that AMR parsing is solved; in fact, human evaluation in previous work indicates that current parsers still quite frequently make errors on node labels or graph... | Jonas Groschwitz, Shay B. Cohen, Lucia Donatelli, Meaghan Fowlie |  |
| 1857 |  |  [Rethinking and Improving Multi-task Learning for End-to-end Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-main.663) |  | 0 | Significant improvements in end-to-end speech translation (ST) have been achieved through the application of multi-task learning. However, the extent to which auxiliary tasks are highly consistent with the ST task, and how much this approach truly helps, have not been thoroughly studied. In this paper, we investigate the consistency between different tasks, considering different times and modules. We find that the textual encoder primarily facilitates cross-modal conversion, but the presence of... | Yuhao Zhang, Chen Xu, Bei Li, Hao Chen, Tong Xiao, Chunliang Zhang, Jingbo Zhu |  |
| 1858 |  |  [AD-NLP: A Benchmark for Anomaly Detection in Natural Language Processing](https://doi.org/10.18653/v1/2023.emnlp-main.664) |  | 0 | Deep learning models have reignited the interest in Anomaly Detection research in recent years. Methods for Anomaly Detection in text have shown strong empirical results on ad-hoc anomaly setups that are usually made by downsampling some classes of a labeled dataset. This can lead to reproducibility issues and models that are biased toward detecting particular anomalies while failing to recognize them in more sophisticated scenarios. In the present work, we provide a unified benchmark for... | Matei Bejan, Andrei Manolache, Marius Popescu |  |
| 1859 |  |  [Enhancing the Ranking Context of Dense Retrieval through Reciprocal Nearest Neighbors](https://doi.org/10.18653/v1/2023.emnlp-main.665) |  | 0 | Sparse annotation poses persistent challenges to training dense retrieval models; for example, it distorts the training signal when unlabeled relevant documents are used spuriously as negatives in contrastive learning. To alleviate this problem, we introduce evidence-based label smoothing, a novel, computationally efficient method that prevents penalizing the model for assigning high relevance to false negatives. To compute the target relevance distribution over candidate documents within the... | George Zerveas, Navid Rekabsaz, Carsten Eickhoff |  |
| 1860 |  |  [Cross-Lingual Cross-Target Stance Detection with Dual Knowledge Distillation Framework](https://doi.org/10.18653/v1/2023.emnlp-main.666) |  | 0 | Stance detection aims to identify the user’s attitude toward specific targets from text, which is an important research area in text mining and benefits a variety of application domains. Existing studies on stance detection were conducted mainly in English. Due to the low-resource problem in most non-English languages, cross-lingual stance detection was proposed to transfer knowledge from high-resource (source) language to low-resource (target) language. However, previous research has ignored... | Ruike Zhang, Hanxuan Yang, Wenji Mao |  |
| 1861 |  |  [PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs](https://doi.org/10.18653/v1/2023.emnlp-main.667) |  | 0 | Research interest in task-oriented dialogs has increased as systems such as Google Assistant, Alexa and Siri have become ubiquitous in everyday life. However, the impact of academic research in this area has been limited by the lack of datasets that realistically capture the wide array of user pain points. To enable research on some of the more challenging aspects of parsing realistic conversations, we introduce PRESTO, a public dataset of over 550K contextual multilingual conversations between... | Rahul Goel, Waleed Ammar, Aditya Gupta, Siddharth Vashishtha, Motoki Sano, Faiz Surani, Max Chang, HyunJeong Choe, David Greene, Chuan He, Rattima Nitisaroj, Anna Trukhina, Shachi Paul, Pararth Shah, Rushin Shah, Zhou Yu |  |
| 1862 |  |  [An Iteratively Parallel Generation Method with the Pre-Filling Strategy for Document-level Event Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.668) |  | 0 | In document-level event extraction (DEE) tasks, a document typically contains many event records with multiple event roles. Therefore, accurately extracting all event records is a big challenge since the number of event records is not given. Previous works present the entity-based directed acyclic graph (EDAG) generation methods to autoregressively generate event roles, which requires a given generation order. Meanwhile, parallel methods are proposed to generate all event roles simultaneously,... | Guanhua Huang, Runxin Xu, Ying Zeng, Jiaze Chen, Zhouwang Yang, Weinan E |  |
| 1863 |  |  [CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations](https://doi.org/10.18653/v1/2023.emnlp-main.669) |  | 0 | Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and... | Myra Cheng, Tiziano Piccardi, Diyi Yang |  |
| 1864 |  |  [Reduce Human Labor On Evaluating Conversational Information Retrieval System: A Human-Machine Collaboration Approach](https://doi.org/10.18653/v1/2023.emnlp-main.670) |  | 0 | Evaluating conversational information retrieval (CIR) systems is a challenging task that requires a significant amount of human labor for annotation. It is imperative to invest significant effort into researching more labor-effective methods for evaluating CIR systems. To touch upon this challenge, we take the first step to involve active testing in CIR evaluation and propose a novel method, called HomCoE. It strategically selects a few data for human annotation, then calibrates the evaluation... | Chen Huang, Peixin Qin, Wenqiang Lei, Jiancheng Lv |  |
| 1865 |  |  [BERTie Bott's Every Flavor Labels: A Tasty Introduction to Semantic Role Labeling for Galician](https://doi.org/10.18653/v1/2023.emnlp-main.671) |  | 0 | In this paper, we leverage existing corpora, WordNet, and dependency parsing to build the first Galician dataset for training semantic role labeling systems in an effort to expand available NLP resources. Additionally, we introduce verb indexing, a new pre-processing method, which helps increase the performance when semantically parsing highly-complex sentences. We use transfer-learning to test both the resource and the verb indexing method. Our results show that the effects of verb indexing... | Micaella Bruton, Meriem Beloucif |  |
| 1866 |  |  [Program Translation via Code Distillation](https://doi.org/10.18653/v1/2023.emnlp-main.672) |  | 0 | Software version migration and program translation are an important and costly part of the lifecycle of large codebases. Traditional machine translation relies on parallel corpora for supervised translation, which is not feasible for program translation due to a dearth of aligned data. Recent unsupervised neural machine translation techniques have overcome data limitations by included techniques such as back translation and low level compiler intermediate representations (IR). These methods... | Yufan Huang, Mengnan Qi, Yongqiang Yao, Maoquan Wang, Bin Gu, Colin B. Clement, Neel Sundaresan |  |
| 1867 |  |  [FaMeSumm: Investigating and Improving Faithfulness of Medical Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.673) |  | 0 | Summaries of medical text shall be faithful by being consistent and factual with source inputs, which is an important but understudied topic for safety and efficiency in healthcare. In this paper, we investigate and improve faithfulness in summarization on a broad range of medical summarization tasks. Our investigation reveals that current summarization models often produce unfaithful outputs for medical input text. We then introduce FaMeSumm, a framework to improve faithfulness by fine-tuning... | Nan Zhang, Yusen Zhang, Wu Guo, Prasenjit Mitra, Rui Zhang |  |
| 1868 |  |  [Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning](https://doi.org/10.18653/v1/2023.emnlp-main.674) |  | 0 | Despite their impressive performance, large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly. To address this issue, grammar-constrained decoding (GCD) can be used to control the generation of LMs, guaranteeing that the output follows a given structure. Most existing GCD methods are, however, limited to specific tasks, such as parsing or code generation. In this work, we demonstrate that... | Saibo Geng, Martin Josifoski, Maxime Peyrard, Robert West |  |
| 1869 |  |  [Systematic word meta-sense extension](https://doi.org/10.18653/v1/2023.emnlp-main.675) |  | 0 | The meaning of polysemous words often varies in a highly productive yet predictable way. Generalizing the regularity between conventional senses to derive novel word meaning is crucial for automated processing of non-literal language uses such as figurative expressions. We introduce a novel task called systematic word meta-sense extension (SWORME) to test and improve language models’ ability to extend word meaning to denote new semantic domains (also called meta-senses) that bear regular... | Lei Yu |  |
| 1870 |  |  [Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory](https://doi.org/10.18653/v1/2023.emnlp-main.676) |  | 0 | We address a fundamental challenge in Natural Language Generation (NLG) model evaluation—the design and evaluation of evaluation metrics. Recognizing the limitations of existing automatic metrics and noises from how current human evaluation was conducted, we propose MetricEval, a framework informed by measurement theory, the foundation of educational test design, for conceptualizing and evaluating the reliability and validity of NLG evaluation metrics. The framework formalizes the source of... | Ziang Xiao, Susu Zhang, Vivian Lai, Q. Vera Liao |  |
| 1871 |  |  [Revisiting the Knowledge Injection Frameworks](https://doi.org/10.18653/v1/2023.emnlp-main.677) |  | 0 | In recent years, large language models (LLMs), such as GPTs, have attained great impact worldwide. However, how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved. Indeed, there have emerged a few works on this line where most of them rely on an alignment heuristic that is built to inject the corresponding knowledge tuple into the associated text sample. However, despite the promise, we identify a pivotal problem... | Peng Fu, Yiming Zhang, Haobo Wang, Weikang Qiu, Junbo Zhao |  |
| 1872 |  |  [We Are What We Repeatedly Do: Inducing and Deploying Habitual Schemas in Persona-Based Responses](https://doi.org/10.18653/v1/2023.emnlp-main.678) |  | 0 | Many practical applications of dialogue technology require the generation of responses according to a particular developer-specified persona. While a variety of personas can be elicited from recent large language models, the opaqueness and unpredictability of these models make it desirable to be able to specify personas in an explicit form. In previous work, personas have typically been represented as sets of one-off pieces of self-knowledge that are retrieved by the dialogue system for use in... | Benjamin Kane, Lenhart K. Schubert |  |
| 1873 |  |  [Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model](https://doi.org/10.18653/v1/2023.emnlp-main.679) |  | 0 | Despite tremendous improvements in natural language generation, summarization models still suffer from the unfaithfulness issue. Previous work evaluates faithfulness either using models trained on the other tasks or in-domain synthetic data, or prompting a large model such as ChatGPT. This paper proposes to do zero-shot faithfulness evaluation simply with a moderately-sized foundation language model. We introduce a new metric FFLM, which is a combination of probability changes based on the... | Qi Jia, Siyu Ren, Yizhu Liu, Kenny Q. Zhu |  |
| 1874 |  |  [TaskWeb: Selecting Better Source Tasks for Multi-task NLP](https://doi.org/10.18653/v1/2023.emnlp-main.680) |  | 0 | Recent work in NLP has shown promising results in training models on large amounts of tasks to achieve better generalization. However, it is not well-understood how tasks are related, and how helpful training tasks can be chosen for a new task. In this work, we investigate whether knowing task relationships via pairwise task transfer improves choosing one or more source tasks that help to learn a new target task. We provide TaskWeb, a large-scale benchmark of pairwise task transfers for 22 NLP... | Joongwon Kim, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi |  |
| 1875 |  |  [Improving Bias Mitigation through Bias Experts in Natural Language Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.681) |  | 0 | Biases in the dataset often enable the model to achieve high performance on in-distribution data, while poorly performing on out-of-distribution data. To mitigate the detrimental effect of the bias on the networks, previous works have proposed debiasing methods that down-weight the biased examples identified by an auxiliary model, which is trained with explicit bias labels. However, finding a type of bias in datasets is a costly process. Therefore, recent studies have attempted to make the... | Eojin Jeon, Mingyu Lee, Juhyeong Park, Yeachan Kim, WingLam Mok, SangKeun Lee |  |
| 1876 |  |  [Semi-supervised multimodal coreference resolution in image narrations](https://doi.org/10.18653/v1/2023.emnlp-main.682) |  | 0 | In this paper, we study multimodal coreference resolution, specifically where a longer descriptive text, i.e., a narration is paired with an image. This poses significant challenges due to fine-grained image-text alignment, inherent ambiguity present in narrative language, and unavailability of large annotated training sets. To tackle these challenges, we present a data efficient semi-supervised approach that utilizes image-narration pairs to resolve coreferences and narrative grounding in a... | Arushi Goel, Basura Fernando, Frank Keller, Hakan Bilen |  |
| 1877 |  |  [A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.683) |  | 0 | Various types of social biases have been reported with pretrained Masked Language Models (MLMs) in prior work. However, multiple underlying factors are associated with an MLM such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled, tokenization, and languages present in the pretrained corpora, to name a few. It remains unclear as to which of those factors influence social biases that are learned by MLMs. To study the relationship... | Yi Zhou, José CamachoCollados, Danushka Bollegala |  |
| 1878 |  |  [Argument-based Detection and Classification of Fallacies in Political Debates](https://doi.org/10.18653/v1/2023.emnlp-main.684) |  | 0 | Fallacies are arguments that employ faulty reasoning. Given their persuasive and seemingly valid nature, fallacious arguments are often used in political debates. Employing these misleading arguments in politics can have detrimental consequences for society, since they can lead to inaccurate conclusions and invalid inferences from the public opinion and the policymakers. Automatically detecting and classifying fallacious arguments represents therefore a crucial challenge to limit the spread of... | Pierpaolo Goffredo, Mariana Espinoza, Serena Villata, Elena Cabrio |  |
| 1879 |  |  [Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation](https://doi.org/10.18653/v1/2023.emnlp-main.685) |  | 0 | The field of text-to-image (T2I) generation has garnered significant attention both within the research community and among everyday users. Despite the advancements of T2I models, a common issue encountered by users is the need for repetitive editing of input prompts in order to receive a satisfactory image, which is time-consuming and labor-intensive. Given the demonstrated text generation power of large-scale language models, such as GPT-k, we investigate the potential of utilizing such... | Wanrong Zhu, Xinyi Wang, Yujie Lu, TsuJui Fu, Xin Wang, Miguel P. Eckstein, William Wang |  |
| 1880 |  |  [SpEL: Structured Prediction for Entity Linking](https://doi.org/10.18653/v1/2023.emnlp-main.686) |  | 0 | Entity linking is a prominent thread of research focused on structured data creation by linking spans of text to an ontology or knowledge source. We revisit the use of structured prediction for entity linking which classifies each individual input token as an entity, and aggregates the token predictions. Our system, called SpEL (Structured prediction for Entity Linking) is a state-of-the-art entity linking system that uses some new ideas to apply structured prediction to the task of entity... | Hassan Shavarani, Anoop Sarkar |  |
| 1881 |  |  [Architectural Sweet Spots for Modeling Human Label Variation by the Example of Argument Quality: It's Best to Relate Perspectives!](https://doi.org/10.18653/v1/2023.emnlp-main.687) |  | 0 | Many annotation tasks in natural language processing are highly subjective in that there can be different valid and justified perspectives on what is a proper label for a given example. This also applies to the judgment of argument quality, where the assignment of a single ground truth is often questionable. At the same time, there are generally accepted concepts behind argumentation that form a common ground. To best represent the interplay of individual and shared perspectives, we consider a... | Philipp Heinisch, Matthias Orlikowski, Julia Romberg, Philipp Cimiano |  |
| 1882 |  |  [Explicit Planning Helps Language Models in Logical Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.688) |  | 0 | Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose LEAP, a novel system that uses language models to perform multi-step logical reasoning and incorporates explicit planning into the inference procedure. Explicit planning enables the system to make more informed reasoning decisions at each step by looking ahead into their future effects. Moreover, we propose a training strategy that safeguards the planning... | Hongyu Zhao, Kangrui Wang, Mo Yu, Hongyuan Mei |  |
| 1883 |  |  [clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents](https://doi.org/10.18653/v1/2023.emnlp-main.689) |  | 0 | Recent work has proposed a methodology for the systematic evaluation of “Situated Language Understanding Agents” — agents that operate in rich linguistic and non-linguistic contexts — through testing them in carefully constructed interactive settings. Other recent work has argued that Large Language Models (LLMs), if suitably set up, can be understood as (simulators of) such agents. A connection suggests itself, which this paper explores: Can LLMs be evaluated meaningfully by exposing them to... | Kranti Chalamalasetti, Jana Götze, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, David Schlangen |  |
| 1884 |  |  [Explaining with Contrastive Phrasal Highlighting: A Case Study in Assisting Humans to Detect Translation Differences](https://doi.org/10.18653/v1/2023.emnlp-main.690) |  | 0 | Explainable NLP techniques primarily explain by answering “Which tokens in the input are responsible for this prediction?”. We argue that for NLP models that make predictions by comparing two input texts, it is more useful to explain by answering “What differences between the two inputs explain this prediction?”. We introduce a technique to generate contrastive phrasal highlights that explain the predictions of a semantic divergence model via phrase alignment guided erasure. We show that the... | Eleftheria Briakou, Navita Goyal, Marine Carpuat |  |
| 1885 |  |  [Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in Foundation Models](https://doi.org/10.18653/v1/2023.emnlp-main.691) |  | 0 | In this work, we assess the ability of foundation models to recall encyclopedic knowledge across a wide range of linguistic contexts. To support this, we: 1) produce a 20-language dataset that contains 303k factual associations paired with counterfactuals, 2) evaluate 5 models in a multilingual test, and 3) benchmark a diverse set of 24 models in an English-only test. Meta’s LLaMA achieves the highest scores in both multilingual and English-only evaluations. Yet, an analysis of LLaMA’s errors... | Tim Schott, Daniel Furman, Shreshta Bhat |  |
| 1886 |  |  [Anchoring Fine-tuning of Sentence Transformer with Semantic Label Information for Efficient Truly Few-shot Classification](https://doi.org/10.18653/v1/2023.emnlp-main.692) |  | 0 | Few-shot classification is a powerful technique, but training requires substantial computing power and data. We propose an efficient method with small model sizes and less training data with only 2-8 training instances per class. Our proposed method, AncSetFit, targets low data scenarios by anchoring the task and label information through sentence embeddings in fine-tuning a Sentence Transformer model. It uses contrastive learning and a triplet loss to enforce training instances of a class to... | Amalie Brogaard Pauli, Leon Derczynski, Ira Assent |  |
| 1887 |  |  [UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers](https://doi.org/10.18653/v1/2023.emnlp-main.693) |  | 0 | Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive... | Jon SaadFalcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md. Arafat Sultan, Christopher Potts |  |
| 1888 |  |  [TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings](https://doi.org/10.18653/v1/2023.emnlp-main.694) |  | 0 | Stance detection is important for understanding different attitudes and beliefs on the Internet. However, given that a passage’s stance toward a given topic is often highly dependent on that topic, building a stance detection model that generalizes to unseen topics is difficult. In this work, we propose using contrastive learning as well as an unlabeled dataset of news articles that cover a variety of different topics to train topic-agnostic/TAG and topic-aware/TAW embeddings for use in... | Hans W. A. Hanley, Zakir Durumeric |  |
| 1889 |  |  [Data Similarity is Not Enough to Explain Language Model Performance](https://doi.org/10.18653/v1/2023.emnlp-main.695) |  | 0 | Large language models achieve high performance on many but not all downstream tasks. The interaction between pretraining data and task data is commonly assumed to determine this variance: a task with data that is more similar to a model’s pretraining data is assumed to be easier for that model. We test whether distributional and example-specific similarity measures (embedding-, token- and model-based) correlate with language model performance through a large-scale comparison of the Pile and C4... | Gregory Yauney, Emily Reif, David Mimno |  |
| 1890 |  |  [Zero-shot Sharpness-Aware Quantization for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.696) |  | 0 | Quantization is a promising approach for reducing memory overhead and accelerating inference, especially in large pre-trained language model (PLM) scenarios. While having no access to original training data due to security and privacy concerns has emerged the demand for zero-shot quantization. Most of the cutting-edge zero-shot quantization methods primarily 1) apply to computer vision tasks, and 2) neglect of overfitting problem in the generative adversarial learning process, leading to... | Miaoxi Zhu, Qihuang Zhong, Li Shen, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao |  |
| 1891 |  |  [Deciphering Stereotypes in Pre-Trained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.697) |  | 0 | Warning: This paper contains content that is stereotypical and may be upsetting. This paper addresses the issue of demographic stereotypes present in Transformer-based pre-trained language models (PLMs) and aims to deepen our understanding of how these biases are encoded in these models. To accomplish this, we introduce an easy-to-use framework for examining the stereotype-encoding behavior of PLMs through a combination of model probing and textual analyses. Our findings reveal that a small... | Weicheng Ma, Henry Scheible, Brian Wang, Goutham Veeramachaneni, Pratim Chowdhary, Alan Sun, Andrew Koulogeorge, Lili Wang, Diyi Yang, Soroush Vosoughi |  |
| 1892 |  |  [An "Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives"](https://doi.org/10.18653/v1/2023.emnlp-main.698) |  | 0 | Mental health conversational agents (a.k.a. chatbots) are widely studied for their potential to offer accessible support to those experiencing mental health challenges. Previous surveys on the topic primarily consider papers published in either computer science or medicine, leading to a divide in understanding and hindering the sharing of beneficial knowledge between both domains. To bridge this gap, we conduct a comprehensive literature review using the PRISMA framework, reviewing 534 papers... | Young Min Cho, Sunny Rai, Lyle H. Ungar, João Sedoc, Sharath Chandra Guntuku |  |
| 1893 |  |  [Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark](https://doi.org/10.18653/v1/2023.emnlp-main.699) |  | 0 | Large language models (LLMs) have been shown to perform well at a variety of syntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well LLMs understand social language. Here, we introduce a new theory-driven benchmark, SocKET, that contains 58 NLP tasks testing social knowledge which we group into five categories: humor & sarcasm, offensiveness,... | Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, David Jurgens |  |
| 1894 |  |  [Interventional Rationalization](https://doi.org/10.18653/v1/2023.emnlp-main.700) |  | 0 | Selective rationalizations improve the explainability of neural networks by selecting a subsequence of the input (i.e., rationales) to explain the prediction results. Although existing methods have achieved promising results, they still suffer from adopting the spurious correlations in data (aka., shortcuts) to compose rationales and make predictions. Inspired by the causal theory, in this paper, we develop an interventional rationalization (Inter-RAT) to discover the causal rationales.... | Linan Yue, Qi Liu, Li Wang, Yanqing An, Yichao Du, Zhenya Huang |  |
| 1895 |  |  [Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting](https://doi.org/10.18653/v1/2023.emnlp-main.701) |  | 0 | Most existing stylistic text rewriting methods and evaluation metrics operate on a sentence level, but ignoring the broader context of the text can lead to preferring generic, ambiguous, and incoherent rewrites. In this paper, we investigate integrating the preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting, and introduce a new composite contextual evaluation metric CtxSimFit that combines similarity to the original sentence with contextual... | Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap |  |
| 1896 |  |  [Axiomatic Preference Modeling for Longform Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.702) |  | 0 | The remarkable abilities of large language models (LLMs) like ChatGPT and GPT-4 partially stem from the post-training processes involving human preferences encoded within a reward model as part of a Reinforcement Learning from Human Feedback (RLHF) regimen. These reward models (RMs) often lack direct knowledge of why, or under what principles, the preferences annotations were made. In this study, we identify principles that guide RMs to better align with human preferences, and then develop an... | Corby Rosset, Guoqing Zheng, Victor Dibia, Ahmed Awadallah, Paul N. Bennett |  |
| 1897 |  |  [Countering Misinformation via Emotional Response Generation](https://doi.org/10.18653/v1/2023.emnlp-main.703) |  | 0 | The proliferation of misinformation on social media platforms (SMPs) poses a significant danger to public health, social cohesion and ultimately democracy. Previous research has shown how social correction can be an effective way to curb misinformation, by engaging directly in a constructive dialogue with users who spread – often in good faith – misleading messages. Although professional fact-checkers are crucial to debunking viral claims, they usually do not engage in conversations on social... | Daniel Russo, Shane P. KaszefskiYaschuk, Jacopo Staiano, Marco Guerini |  |
| 1898 |  |  [Seq2seq is All You Need for Coreference Resolution](https://doi.org/10.18653/v1/2023.emnlp-main.704) |  | 0 | Existing works on coreference resolution suggest that task-specific models are necessary to achieve state-of-the-art performance. In this work, we present compelling evidence that such models are not necessary. We finetune a pretrained seq2seq transformer to map an input document to a tagged sequence encoding the coreference annotation. Despite the extreme simplicity, our model outperforms or closely matches the best coreference systems in the literature on an array of datasets. We consider an... | Wenzheng Zhang, Sam Wiseman, Karl Stratos |  |
| 1899 |  |  [Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection](https://doi.org/10.18653/v1/2023.emnlp-main.705) |  | 0 | When translating words referring to the speaker, speech translation (ST) systems should not resort to default masculine generics nor rely on potentially misleading vocal traits. Rather, they should assign gender according to the speakers’ preference. The existing solutions to do so, though effective, are hardly feasible in practice as they involve dedicated model re-training on gender-labeled ST data. To overcome these limitations, we propose the first inference-time solution to control... | Dennis Fucci, Marco Gaido, Sara Papi, Mauro Cettolo, Matteo Negri, Luisa Bentivogli |  |
| 1900 |  |  [StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.706) |  | 0 | Analogy-making between narratives is crucial for human reasoning. In this paper, we evaluate the ability to identify and generate analogies by constructing a first-of-its-kind large-scale story-level analogy corpus, StoryAnalogy, which contains 24K story pairs from diverse domains with human annotations on two similarities from the extended Structure-Mapping Theory. We design a set of tests on StoryAnalogy, presenting the first evaluation of story-level analogy identification and generation.... | Cheng Jiayang, Lin Qiu, Tsz Ho Chan, Tianqing Fang, Weiqi Wang, Chunkit Chan, Dongyu Ru, Qipeng Guo, Hongming Zhang, Yangqiu Song, Yue Zhang, Zheng Zhang |  |
| 1901 |  |  [Beyond Detection: A Defend-and-Summarize Strategy for Robust and Interpretable Rumor Analysis on Social Media](https://doi.org/10.18653/v1/2023.emnlp-main.707) |  | 0 | As the impact of social media gradually escalates, people are more likely to be exposed to indistinguishable fake news. Therefore, numerous studies have attempted to detect rumors on social media by analyzing the textual content and propagation paths. However, fewer works on rumor detection tasks consider the malicious attacks commonly observed at response level. Moreover, existing detection models have poor interpretability. To address these issues, we propose a novel framework named... | YiTing Chang, YunZhu Song, YiSyuan Chen, HongHan Shuai |  |
| 1902 |  |  [Crystal: Introspective Reasoners Reinforced with Self-Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.708) |  | 0 | Extensive work has shown that the performance and interpretability of commonsense reasoning can be improved via knowledge-augmented reasoning methods, where the knowledge that underpins the reasoning process is explicitly verbalized and utilized. However, existing implementations, including “chain-of-thought” and its variants, fall short in capturing the \*introspective\* nature of knowledge required in commonsense reasoning, and in accounting for the mutual adaptation between the generation... | Jiacheng Liu, Ramakanth Pasunuru, Hannaneh Hajishirzi, Yejin Choi, Asli Celikyilmaz |  |
| 1903 |  |  [DiffS2UT: A Semantic Preserving Diffusion Model for Textless Direct Speech-to-Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-main.709) |  | 0 | While Diffusion Generative Models have achieved great success on image generation tasks, how to efficiently and effectively incorporate them into speech generation especially translation tasks remains a non-trivial problem. Specifically, due to the low information density of speech data, the transformed discrete speech unit sequence is much longer than the corresponding text transcription, posing significant challenges to existing auto-regressive models. Furthermore, it is not optimal to... | Yongxin Zhu, Zhujin Gao, Xinyuan Zhou, Zhongyi Ye, Linli Xu |  |
| 1904 |  |  [BioFEG: Generate Latent Features for Biomedical Entity Linking](https://doi.org/10.18653/v1/2023.emnlp-main.710) |  | 0 | Biomedical entity linking is an essential task in biomedical text processing, which aims to map entity mentions in biomedical text, such as clinical notes, to standard terms in a given knowledge base. However, this task is challenging due to the rarity of many biomedical entities in real-world scenarios, which often leads to a lack of annotated data for them. Limited by understanding these unseen entities, traditional biomedical entity linking models suffer from multiple types of linking... | Xuhui Sui, Ying Zhang, Xiangrui Cai, Kehui Song, Baohang Zhou, Xiaojie Yuan, Wensheng Zhang |  |
| 1905 |  |  [TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.711) |  | 0 | Automated theorem proving (ATP) has become an appealing domain for exploring the reasoning ability of the recent successful generative language models. However, current ATP benchmarks are mainly focus on symbolic inference, but rarely involve the understanding of complex number combination reasoning. In this work, we propose TRIGO, an ATP benchmark that not only requires a model to reduce a trigonometric expression with step-by-step proof but also evaluates a generative LM’s reasoning ability... | Jing Xiong, Jianhao Shen, Ye Yuan, Haiming Wang, Yichun Yin, Zhengying Liu, Lin Li, Zhijiang Guo, Qingxing Cao, Yinya Huang, Chuanyang Zheng, Xiaodan Liang, Ming Zhang, Qun Liu |  |
| 1906 |  |  [Physician Detection of Clinical Harm in Machine Translation: Quality Estimation Aids in Reliance and Backtranslation Identifies Critical Errors](https://doi.org/10.18653/v1/2023.emnlp-main.712) |  | 0 | A major challenge in the practical use of Machine Translation (MT) is that users lack information on translation quality to make informed decisions about how to rely on outputs. Progress in quality estimation research provides techniques to automatically assess MT quality, but these techniques have primarily been evaluated in vitro by comparison against human judgments outside of a specific context of use. This paper evaluates quality estimation feedback in vivo with a human study in realistic... | Nikita Mehandru, Sweta Agrawal, Yimin Xiao, Ge Gao, Elaine C. Khoong, Marine Carpuat, Niloufar Salehi |  |
| 1907 |  |  [Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive](https://doi.org/10.18653/v1/2023.emnlp-main.713) |  | 0 | Offensive speech detection is a key component of content moderation. However, what is offensive can be highly subjective. This paper investigates how machine and human moderators disagree on what is offensive when it comes to real-world social web political discourse. We show that (1) there is extensive disagreement among the moderators (humans and machines); and (2) human and large-language-model classifiers are unable to predict how other human raters will respond, based on their political... | Tharindu Cyril Weerasooriya, Sujan Dutta, Tharindu Ranasinghe, Marcos Zampieri, Christopher Homan, Ashiqur R. KhudaBukhsh |  |
| 1908 |  |  [Generating Summaries with Controllable Readability Levels](https://doi.org/10.18653/v1/2023.emnlp-main.714) |  | 0 | Readability refers to how easily a reader can understand a written text. Several factors affect the readability level, such as the complexity of the text, its subject matter, and the reader’s background knowledge. Generating summaries based on different readability levels is critical for enabling knowledge consumption by diverse audiences. However, current text generation approaches lack refined control, resulting in texts that are not customized to readers’ proficiency levels. In this work, we... | Leonardo F. R. Ribeiro, Mohit Bansal, Markus Dreyer |  |
| 1909 |  |  [mAggretriever: A Simple yet Effective Approach to Zero-Shot Multilingual Dense Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.715) |  | 0 | Multilingual information retrieval (MLIR) is a crucial yet challenging task due to the need for human annotations in multiple languages, making training data creation labor-intensive. In this paper, we introduce mAggretriever, which effectively leverages semantic and lexical features from pre-trained multilingual transformers (e.g., mBERT and XLM-R) for dense retrieval. To enhance training and inference efficiency, we employ approximate masked-language modeling prediction for computing lexical... | ShengChieh Lin, Amin Ahmad, Jimmy Lin |  |
| 1910 |  |  [CodeFusion: A Pre-trained Diffusion Model for Code Generation](https://doi.org/10.18653/v1/2023.emnlp-main.716) |  | 0 | Imagine a developer who can only change their last line of code—how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural... | Mukul Singh, José Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Gust Verbruggen |  |
| 1911 |  |  [CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs](https://doi.org/10.18653/v1/2023.emnlp-main.717) |  | 0 | Instruction-based multitasking has played a critical role in the success of large language models (LLMs) in multi-turn dialog applications. While publicly available LLMs have shown promising performance, when exposed to complex instructions with multiple constraints, they lag against state-of-the-art models like ChatGPT. In this work, we hypothesize that the availability of large-scale complex demonstrations is crucial in bridging this gap. Focusing on dialog applications, we propose a novel... | Taha Aksu, Devamanyu Hazarika, Shikib Mehri, Seokhwan Kim, Dilek HakkaniTur, Yang Liu, Mahdi Namazifar |  |
| 1912 |  |  [VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights](https://doi.org/10.18653/v1/2023.emnlp-main.718) |  | 0 | Recognizing vulnerability is crucial for understanding and implementing targeted support to empower individuals in need. This is especially important at the European Court of Human Rights (ECtHR), where the court adapts Convention standards to meet actual individual needs and thus to ensure effective human rights protection. However, the concept of vulnerability remains elusive at the ECtHR and no prior NLP research has dealt with it. To enable future research in this area, we present VECHR, a... | Shanshan Xu, Leon Staufer, T. Y. S. S. Santosh, Oana Ichim, Corina Heri, Matthias Grabmair |  |
| 1913 |  |  [ACQUIRED: A Dataset for Answering Counterfactual Questions In Real-Life Videos](https://doi.org/10.18653/v1/2023.emnlp-main.719) |  | 0 | Multimodal counterfactual reasoning is a vital yet challenging ability for AI systems. It involves predicting the outcomes of hypothetical circumstances based on vision and language inputs, which enables AI models to learn from failures and explore hypothetical scenarios. Despite its importance, there are only a few datasets targeting the counterfactual reasoning abilities of multimodal models. Among them, they only cover reasoning over synthetic environments or specific types of events (e.g.... | TeLin Wu, ZiYi Dou, Qingyuan Hu, Yu Hou, Nischal Reddy Chandra, Marjorie Freedman, Ralph M. Weischedel, Nanyun Peng |  |
| 1914 |  |  [From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base](https://doi.org/10.18653/v1/2023.emnlp-main.720) |  | 0 | Parsing questions into executable logical forms has showed impressive results for knowledge-base question answering (KBQA). However, complex KBQA is a more challenging task that requires to perform complex multi-step reasoning. Recently, a new semantic parser called KoPL has been proposed to explicitly model the reasoning processes, which achieved the state-of-the-art on complex KBQA. In this paper, we further explore how to unlock the reasoning ability of semantic parsers by a simple proposed... | Wangzhen Guo, Linyin Luo, Hanjiang Lai, Jian Yin |  |
| 1915 |  |  [Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model](https://doi.org/10.18653/v1/2023.emnlp-main.721) |  | 0 | While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities... | Haikang Deng, Colin Raffel |  |
| 1916 |  |  [CORE: A Few-Shot Company Relation Classification Dataset for Robust Domain Adaptation](https://doi.org/10.18653/v1/2023.emnlp-main.722) |  | 0 | We introduce CORE, a dataset for few-shot relation classification (RC) focused on company relations and business entities. CORE includes 4,708 instances of 12 relation types with corresponding textual evidence extracted from company Wikipedia pages. Company names and business entities pose a challenge for few-shot RC models due to the rich and diverse information associated with them. For example, a company name may represent the legal entity, products, people, or business divisions depending... | Philipp Borchert, Jochen De Weerdt, Kristof Coussement, Arno De Caigny, MarieFrancine Moens |  |
| 1917 |  |  [Models See Hallucinations: Evaluating the Factuality in Video Captioning](https://doi.org/10.18653/v1/2023.emnlp-main.723) |  | 0 | Video captioning aims to describe events in a video with natural language. In recent years, many works have focused on improving captioning models’ performance. However, like other text generation tasks, it risks introducing factual errors not supported by the input video. Factual errors can seriously affect the quality of the generated text, sometimes making it completely unusable. Although factual consistency has received much research attention in text-to-text tasks (e.g., summarization), it... | Hui Liu, Xiaojun Wan |  |
| 1918 |  |  [Back Transcription as a Method for Evaluating Robustness of Natural Language Understanding Models to Speech Recognition Errors](https://doi.org/10.18653/v1/2023.emnlp-main.724) |  | 0 | In a spoken dialogue system, an NLU model is preceded by a speech recognition system that can deteriorate the performance of natural language understanding. This paper proposes a method for investigating the impact of speech recognition errors on the performance of natural language understanding models. The proposed method combines the back transcription procedure with a fine-grained technique for categorizing the errors that affect the performance of NLU models. The method relies on the usage... | Marek Kubis, Pawel Skórzewski, Marcin Sowanski, Tomasz Zietkiewicz |  |
| 1919 |  |  [Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models for Learning Conceptual Spaces](https://doi.org/10.18653/v1/2023.emnlp-main.725) |  | 0 | The theory of Conceptual Spaces is an influential cognitive-linguistic framework for representing the meaning of concepts. Conceptual spaces are constructed from a set of quality dimensions, which essentially correspond to primitive perceptual features (e.g. hue or size). These quality dimensions are usually learned from human judgements, which means that applications of conceptual spaces tend to be limited to narrow domains (e.g. modelling colour or taste). Encouraged by recent findings about... | Usashi Chatterjee, Amit Gajbhiye, Steven Schockaert |  |
| 1920 |  |  [Can Language Models Understand Physical Concepts?](https://doi.org/10.18653/v1/2023.emnlp-main.726) |  | 0 | Language models (LMs) gradually become general-purpose interfaces in the interactive and embodied world, where the understanding of physical concepts is an essential prerequisite. However, it is unclear whether LMs can understand physical concepts in the human world. To investigate this, we design a benchmark VEC that covers the tasks of (i) Visual concepts, such as the shape and material of objects, and (ii) Embodied Concepts, learned from the interaction with the world such as the temperature... | Lei Li, Jingjing Xu, Qingxiu Dong, Ce Zheng, Xu Sun, Lingpeng Kong, Qi Liu |  |
| 1921 |  |  [SPT: Learning to Selectively Insert Prompts for Better Prompt Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.727) |  | 0 | Prompt tuning prepends a soft prompt to the input embeddings or hidden states and only optimizes the prompt to adapt pretrained models (PTMs) to downstream tasks. The previous work manually selects prompt layers which are far from optimal and failed to exploit the potential of prompt tuning. In this work, we propose a novel framework, Selective Prompt Tuning (SPT), that learns to select the proper prompt layers by inserting a prompt controlled by a learnable probabilistic gate at each... | Wei Zhu, Ming Tan |  |
| 1922 |  |  [Once Upon a Time in Graph: Relative-Time Pretraining for Complex Temporal Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.728) |  | 0 | Our physical world is constantly evolving over time, rendering challenges for pre-trained language models to understand and reason over the temporal contexts of texts. Existing work focuses on strengthening the direct association between a piece of text and its time-stamp. However, the knowledge-time association is usually insufficient for the downstream tasks that require reasoning over temporal dependencies between knowledge. In this work, we make use of the underlying nature of time, all... | Sen Yang, Xin Li, Lidong Bing, Wai Lam |  |
| 1923 |  |  [Expository Text Generation: Imitate, Retrieve, Paraphrase](https://doi.org/10.18653/v1/2023.emnlp-main.729) |  | 0 | Expository documents are vital resources for conveying complex information to readers. Despite their usefulness, writing expository text by hand is a challenging process that requires careful content planning, obtaining facts from multiple sources, and the ability to clearly synthesize these facts. To ease these burdens, we propose the task of expository text generation, which seeks to automatically generate an accurate and stylistically consistent expository text for a topic by intelligently... | Nishant Balepur, Jie Huang, Kevin ChenChuan Chang |  |
| 1924 |  |  [Large-scale similarity search with Optimal Transport](https://doi.org/10.18653/v1/2023.emnlp-main.730) |  | 0 | Wasserstein distance is a powerful tool for comparing probability distributions and is widely used for document classification and retrieval tasks in NLP. In particular, it is known as the word mover’s distance (WMD) in the NLP community. WMD exhibits excellent performance for various NLP tasks; however, one of its limitations is its computational cost and thus is not useful for large-scale distribution comparisons. In this study, we propose a simple and effective nearest neighbor search based... | Cléa Laouar, Yuki Takezawa, Makoto Yamada |  |
| 1925 |  |  [Enhancing Textbooks with Visuals from the Web for Improved Learning](https://doi.org/10.18653/v1/2023.emnlp-main.731) |  | 0 | Textbooks are one of the main mediums for delivering high-quality education to students. In particular, explanatory and illustrative visuals play a key role in retention, comprehension and general transfer of knowledge. However, many textbooks lack these interesting visuals to support student learning. In this paper, we investigate the effectiveness of vision-language models to automatically enhance textbooks with images from the web. We collect a dataset of e-textbooks in the math, science,... | Janvijay Singh, Vilém Zouhar, Mrinmaya Sachan |  |
| 1926 |  |  [Continual Event Extraction with Semantic Confusion Rectification](https://doi.org/10.18653/v1/2023.emnlp-main.732) |  | 0 | We study continual event extraction, which aims to extract incessantly emerging event information while avoiding forgetting. We observe that the semantic confusion on event types stems from the annotations of the same text being updated over time. The imbalance between event types even aggravates this issue. This paper proposes a novel continual event extraction model with semantic confusion rectification. We mark pseudo labels for each sentence to alleviate semantic confusion. We transfer... | Zitao Wang, Xinyi Wang, Wei Hu |  |
| 1927 |  |  [An Empirical Study of Translation Hypothesis Ensembling with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.733) |  | 0 | Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output. In this paper, we investigate how hypothesis ensembling can improve the quality of the generated text for the specific problem of LLM-based machine translation. We experiment with several techniques for ensembling hypotheses produced by LLMs such as ChatGPT, LLaMA, and Alpaca. We provide a comprehensive study along multiple dimensions, including the method to generate... | António Farinhas, José Guilherme Camargo de Souza, André F. T. Martins |  |
| 1928 |  |  [FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning](https://doi.org/10.18653/v1/2023.emnlp-main.734) |  | 0 | Psychiatrists diagnose mental disorders via the linguistic use of patients. Still, due to data privacy, existing passive mental health monitoring systems use alternative features such as activity, app usage, and location via mobile devices. We propose FedTherapist, a mobile mental health monitoring system that utilizes continuous speech and keyboard input in a privacy-preserving way via federated learning. We explore multiple model designs by comparing their performance and overhead for... | Jaemin Shin, Hyungjun Yoon, Seungjoo Lee, Sungjoon Park, Yunxin Liu, Jinho D. Choi, SungJu Lee |  |
| 1929 |  |  [Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.735) |  | 0 | Recent advances in Large Language Models (LLMs) have stimulated a surge of research aimed at extending their applications to the visual domain. While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement. In this paper, we introduce Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing... | Geewook Kim, Hodong Lee, Daehee Kim, Haeji Jung, Sanghee Park, Yoonsik Kim, Sangdoo Yun, Taeho Kil, Bado Lee, Seunghyun Park |  |
| 1930 |  |  [Continual Learning for Multilingual Neural Machine Translation via Dual Importance-based Model Division](https://doi.org/10.18653/v1/2023.emnlp-main.736) |  | 0 | A persistent goal of multilingual neural machine translation (MNMT) is to continually adapt the model to support new language pairs or improve some current language pairs without accessing the previous training data. To achieve this, the existing methods primarily focus on preventing catastrophic forgetting by making compromises between the original and new language pairs, leading to sub-optimal performance on both translation tasks. To mitigate this problem, we propose a dual importance-based... | Junpeng Liu, Kaiyu Huang, Hao Yu, Jiuyi Li, Jinsong Su, Degen Huang |  |
| 1931 |  |  [SimCSE++: Improving Contrastive Learning for Sentence Embeddings from Two Perspectives](https://doi.org/10.18653/v1/2023.emnlp-main.737) |  | 0 | This paper improves contrastive learning for sentence embeddings from two perspectives: handling dropout noise and addressing feature corruption. Specifically, for the first perspective, we identify that the dropout noise from negative pairs affects the model’s performance. Therefore, we propose a simple yet effective method to deal with such type of noise. Secondly, we pinpoint the rank bottleneck of current solutions to feature corruption and propose a dimension-wise contrastive learning... | Jiahao Xu, Wei Shao, Lihui Chen, Lemao Liu |  |
| 1932 |  |  [Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.738) |  | 0 | Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning... | Jiaao Chen, Diyi Yang |  |
| 1933 |  |  [Simplicity Level Estimate (SLE): A Learned Reference-Less Metric for Sentence Simplification](https://doi.org/10.18653/v1/2023.emnlp-main.739) |  | 0 | Automatic evaluation for sentence simplification remains a challenging problem. Most popular evaluation metrics require multiple high-quality references – something not readily available for simplification – which makes it difficult to test performance on unseen domains. Furthermore, most existing metrics conflate simplicity with correlated attributes such as fluency or meaning preservation. We propose a new learned evaluation metric — SLE — which focuses on simplicity, outperforming almost all... | Liam Cripwell, Joël Legrand, Claire Gardent |  |
| 1934 |  |  [Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration](https://doi.org/10.18653/v1/2023.emnlp-main.740) |  | 0 | Legal Judgment Prediction (LJP) has become an increasingly crucial task in Legal AI, i.e., predicting the judgment of the case in terms of case fact description. Precedents are the previous legal cases with similar facts, which are the basis for the judgment of the subsequent case in national legal systems. Thus, it is worthwhile to explore the utilization of precedents in the LJP. Recent advances in deep learning have enabled a variety of techniques to be used to solve the LJP task. These can... | Yiquan Wu, Siying Zhou, Yifei Liu, Weiming Lu, Xiaozhong Liu, Yating Zhang, Changlong Sun, Fei Wu, Kun Kuang |  |
| 1935 |  |  [FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.741) |  | 0 | Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source.... | Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wentau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi |  |
| 1936 |  |  [Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems](https://doi.org/10.18653/v1/2023.emnlp-main.742) |  | 0 | Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation. We address this deficiency by creating Calc-X, a collection of datasets that demonstrates the appropriate use of a calculator in reasoning chains. Calc-X is suitable for teaching language models to offload computations to a symbolic system. We survey and unify several existing chain-of-thought datasets into a proposed format, resulting in a... | Marek Kadlcík, Michal Stefánik, Ondrej Sotolár, Vlastimil Martinek |  |
| 1937 |  |  [CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.743) |  | 0 | While Chain-of-Thought prompting is popular in reasoning tasks, its application to Large Language Models (LLMs) in Natural Language Understanding (NLU) is under-explored. Motivated by multi-step reasoning of LLMs, we propose Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks into multiple reasoning steps where LLMs can learn to acquire and leverage essential concepts to solve tasks from different granularities. Moreover, we propose leveraging semantic-based Abstract... | Hoang Nguyen, Ye Liu, Chenwei Zhang, Tao Zhang, Philip S. Yu |  |
| 1938 |  |  [When Language Models Fall in Love: Animacy Processing in Transformer Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.744) |  | 0 | Animacy—whether an entity is alive and sentient—is fundamental to cognitive processing, impacting areas such as memory, vision, and language. However, animacy is not always expressed directly in language: in English it often manifests indirectly, in the form of selectional constraints on verbs and adjectives. This poses a potential issue for transformer language models (LMs): they often train only on text, and thus lack access to extralinguistic information from which humans learn about... | Michael Hanna, Yonatan Belinkov, Sandro Pezzelle |  |
| 1939 |  |  [Improving Unsupervised Relation Extraction by Augmenting Diverse Sentence Pairs](https://doi.org/10.18653/v1/2023.emnlp-main.745) |  | 0 | Unsupervised relation extraction (URE) aims to extract relations between named entities from raw text without requiring manual annotations or pre-existing knowledge bases. In recent studies of URE, researchers put a notable emphasis on contrastive learning strategies for acquiring relation representations. However, these studies often overlook two important aspects: the inclusion of diverse positive pairs for contrastive learning and the exploration of appropriate loss functions. In this paper,... | Qing Wang, Kang Zhou, Qiao Qiao, Yuepei Li, Qi Li |  |
| 1940 |  |  [Paraphrase Types for Generation and Detection](https://doi.org/10.18653/v1/2023.emnlp-main.746) |  | 0 | Current approaches in paraphrase generation and detection heavily rely on a single general similarity score, ignoring the intricate linguistic properties of language. This paper introduces two new tasks to address this shortcoming by considering paraphrase types - specific linguistic perturbations at particular text positions. We name these tasks Paraphrase Type Generation and Paraphrase Type Detection. Our results suggest that while current techniques perform well in a binary classification... | Jan Philip Wahle, Bela Gipp, Terry Ruas |  |
| 1941 |  |  [Target-to-Source Augmentation for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.747) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) is an important task in sentiment analysis, aiming to extract aspect-level opinions and sentiments from user-generated reviews. The fine-grained nature of ASTE incurs a high annotation cost, while the scarcity of annotated data limits the performance of existing methods. This paper exploits data augmentation to address this issue. Traditional augmentation methods typically modify the input sentences of existing samples via heuristic rules or language... | Yice Zhang, Yifan Yang, Meng Li, Bin Liang, Shiwei Chen, Ruifeng Xu |  |
| 1942 |  |  [PAC-tuning: Fine-tuning Pre-trained Language Models with PAC-driven Perturbed Gradient Descent](https://doi.org/10.18653/v1/2023.emnlp-main.748) |  | 0 | Fine-tuning pretrained language models (PLMs) for downstream tasks is a large-scale optimization problem, in which the choice of the training algorithm critically determines how well the trained model can generalize to unseen test data, especially in the context of few-shot learning. To achieve good generalization performance and avoid overfitting, techniques such as data augmentation and pruning are often applied. However, adding these regularizations necessitates heavy tuning of the... | Guangliang Liu, Zhiyu Xue, Xitong Zhang, Kristen Marie Johnson, Rongrong Wang |  |
| 1943 |  |  [Emergence of Abstract State Representations in Embodied Sequence Modeling](https://doi.org/10.18653/v1/2023.emnlp-main.749) |  | 0 | Decision making via sequence modeling aims to mimic the success of language models, where actions taken by an embodied agent are modeled as tokens to predict. Despite their promising performance, it remains unclear if embodied sequence modeling leads to the emergence of internal representations that represent the environmental state information. A model that lacks abstract state representations would be liable to make decisions based on surface statistics which fail to generalize. We take the... | Tian Yun, Zilai Zeng, Kunal Handa, Ashish V. Thapliyal, Bo Pang, Ellie Pavlick, Chen Sun |  |
| 1944 |  |  [Accelerating Toeplitz Neural Network with Constant-time Inference Complexity](https://doi.org/10.18653/v1/2023.emnlp-main.750) |  | 0 | Toeplitz Neural Networks (TNNs) have exhibited outstanding performance in various sequence modeling tasks. They outperform commonly used Transformer-based models while benefiting from log-linear space-time complexities. On the other hand, State Space Models (SSMs) achieve lower performance than TNNs in language modeling but offer the advantage of constant inference complexity. In this paper, we aim to combine the strengths of TNNs and SSMs by converting TNNs to SSMs during inference, thereby... | Zhen Qin, Yiran Zhong |  |
| 1945 |  |  [Dissecting Recall of Factual Associations in Auto-Regressive Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.751) |  | 0 | Transformer-based language models (LMs) are known to capture factual knowledge in their parameters. While previous work looked into where factual associations are stored, only little is known about how they are retrieved internally during inference. We investigate this question through the lens of information flow. Given a subject-relation query, we study how the model aggregates information about the subject and relation to predict the correct attribute. With interventions on attention edges,... | Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson |  |
| 1946 |  |  [StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.752) |  | 0 | Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data. We propose a theoretically grounded framework called StereoMap to gain insights into their perceptions of how demographic groups have been viewed by society. The framework is grounded in the Stereotype Content Model (SCM); a well-established theory from psychology. According to SCM, stereotypes are not all alike. Instead, the dimensions of Warmth and Competence serve as... | Sullam Jeoung, Yubin Ge, Jana Diesner |  |
| 1947 |  |  [Select, Prompt, Filter: Distilling Large Language Models for Summarizing Conversations](https://doi.org/10.18653/v1/2023.emnlp-main.753) |  | 0 | Large language models (LLMs) like ChatGPT can be expensive to train, deploy, and use for specific natural language generation tasks such as text summarization and for certain domains. A promising alternative is to fine-tune relatively smaller language models (LMs) on a particular task using high-quality, in-domain datasets. However, it can be prohibitively expensive to get such high-quality training data. This issue has been mitigated by generating weakly supervised data via knowledge... | MinhQuang Pham, Sathish Indurthi, Shamil Chollampatt, Marco Turchi |  |
| 1948 |  |  [Human Raters Cannot Distinguish English Translations from Original English Texts](https://doi.org/10.18653/v1/2023.emnlp-main.754) |  | 0 | The term translationese describes the set of linguistic features unique to translated texts, which appear regardless of translation quality. Though automatic classifiers designed to distinguish translated texts achieve high accuracy and prior work has identified common hallmarks of translationese, human accuracy of identifying translated text is understudied. In this work, we perform a human evaluation of English original/translated texts in order to explore raters’ ability to classify texts as... | Shira Wein |  |
| 1949 |  |  [Impressions: Visual Semiotics and Aesthetic Impact Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.755) |  | 0 | Is aesthetic impact different from beauty? Is visual salience a reflection of its capacity for effective communication? We present Impressions, a novel dataset through which to investigate the semiotics of images, and how specific visual features and design choices can elicit specific emotions, thoughts and beliefs. We posit that the impactfulness of an image extends beyond formal definitions of aesthetics, to its success as a communicative act, where style contributes as much to meaning... | Julia Kruk, Caleb Ziems, Diyi Yang |  |
| 1950 |  |  [DNA: Denoised Neighborhood Aggregation for Fine-grained Category Discovery](https://doi.org/10.18653/v1/2023.emnlp-main.756) |  | 0 | Discovering fine-grained categories from coarsely labeled data is a practical and challenging task, which can bridge the gap between the demand for fine-grained analysis and the high annotation cost. Previous works mainly focus on instance-level discrimination to learn low-level features, but ignore semantic similarities between data, which may prevent these models learning compact cluster representations. In this paper, we propose Denoised Neighborhood Aggregation (DNA), a self-supervised... | Wenbin An, Feng Tian, Wenkai Shi, Yan Chen, Qinghua Zheng, Qianying Wang, Ping Chen |  |
| 1951 |  |  [Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.757) |  | 0 | The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as... | Shuai Zhao, Jinming Wen, Anh Tuan Luu, Junbo Zhao, Jie Fu |  |
| 1952 |  |  [UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation](https://doi.org/10.18653/v1/2023.emnlp-main.758) |  | 0 | Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on... | Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Weiwei Deng, Qi Zhang |  |
| 1953 |  |  [KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning](https://doi.org/10.18653/v1/2023.emnlp-main.759) |  | 0 | In task-oriented dialogs (TOD), reinforcement learning (RL) algorithms train a model to directly optimize response for task-related metrics. However, RL often needs to perform exploration, which can be time-consuming due to the slow auto-regressive sequence generation process. We investigate an approach to create a more efficient RL-based algorithm to improve TOD performance in an offline setting. First, we use a faster generation procedure that samples from independent next-word distributions... | Xiao Yu, Qingyang Wu, Kun Qian, Zhou Yu |  |
| 1954 |  |  [Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU](https://doi.org/10.18653/v1/2023.emnlp-main.760) |  | 0 | Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university... | Fajri Koto, Nurul Aisyah, Haonan Li, Timothy Baldwin |  |
| 1955 |  |  [Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.761) |  | 0 | A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient,... | Pranjal Aggarwal, Aman Madaan, Yiming Yang, Mausam |  |
| 1956 |  |  [Bridging Information-Theoretic and Geometric Compression in Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.762) |  | 0 | For a language model (LM) to faithfully model human language, it must compress vast, potentially infinite information into relatively few dimensions. We propose analyzing compression in (pre-trained) LMs from two points of view: geometric and information-theoretic. We demonstrate that the two views are highly correlated, such that the intrinsic geometric dimension of linguistic data predicts their coding length under the LM. We then show that, in turn, high compression of a linguistic dataset... | Emily Cheng, Corentin Kervadec, Marco Baroni |  |
| 1957 |  |  [Pre-training Language Models for Comparative Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.763) |  | 0 | Comparative reasoning is a process of comparing objects, concepts, or entities to draw conclusions, which constitutes a fundamental cognitive ability. In this paper, we propose a novel framework to pre-train language models for enhancing their abilities of comparative reasoning over texts. While there have been approaches for NLP tasks that require comparative reasoning, they suffer from costly manual data labeling and limited generalizability to different tasks. Our approach introduces a novel... | Mengxia Yu, Zhihan Zhang, Wenhao Yu, Meng Jiang |  |
| 1958 |  |  [Improved Pseudo Data for Machine Translation Quality Estimation with Constrained Beam Search](https://doi.org/10.18653/v1/2023.emnlp-main.764) |  | 0 | Machine translation (MT) quality estimation (QE) is a crucial task to estimate the quality of MT outputs when reference translations are unavailable. Many studies focus on generating pseudo data using large parallel corpus and achieve remarkable success in the supervised setting. However, pseudo data solutions are less satisfying in unsupervised scenarios because the pseudo labels are inaccurate or the pseudo translations differ from the real ones. To address these problems, we propose to... | Xiang Geng, Yu Zhang, Zhejian Lai, Shuaijie She, Wei Zou, Shimin Tao, Hao Yang, Jiajun Chen, Shujian Huang |  |
| 1959 |  |  [Text Embeddings Reveal (Almost) As Much As Text](https://doi.org/10.18653/v1/2023.emnlp-main.765) |  | 0 | How much private information do text embeddings reveal about the original text? We investigate the problem of embedding inversion, reconstructing the full text represented in dense text embeddings. We frame the problem as controlled generation: generating text that, when reembedded, is close to a fixed point in latent space. We find that although a naive model conditioned on the embedding performs poorly, a multi-step method that iteratively corrects and re-embeds text is able to recover 92% of... | John X. Morris, Volodymyr Kuleshov, Vitaly Shmatikov, Alexander M. Rush |  |
| 1960 |  |  [AutoTrial: Prompting Language Models for Clinical Trial Design](https://doi.org/10.18653/v1/2023.emnlp-main.766) |  | 0 | Clinical trials are critical for drug development. Constructing the appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for patient recruitment) is essential for the trial’s success. Proper design of clinical trial protocols should consider similar precedent trials and their eligibility criteria to ensure sufficient patient coverage. In this paper, we present a method named AutoTrial to aid the design of clinical eligibility criteria using language models. It allows (1)... | Zifeng Wang, Cao Xiao, Jimeng Sun |  |
| 1961 |  |  [Faster Minimum Bayes Risk Decoding with Confidence-based Pruning](https://doi.org/10.18653/v1/2023.emnlp-main.767) |  | 0 | Minimum Bayes risk (MBR) decoding outputs the hypothesis with the highest expected utility over the model distribution for some utility function. It has been shown to improve accuracy over beam search in conditional language generation problems and especially neural machine translation, in both human and automatic evaluations. However, the standard sampling-based algorithm for MBR is substantially more computationally expensive than beam search, requiring a large number of samples as well as a... | Julius Cheng, Andreas Vlachos |  |
| 1962 |  |  [Enhancing Generative Retrieval with Reinforcement Learning from Relevance Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.768) |  | 0 | The recent advent of end-to-end generative retrieval marks a significant shift in document retrieval methods, leveraging differentiable search indexes to directly produce relevant document identifiers (docids) in response to a specific query. Nevertheless, this approach faces two fundamental challenges: (i) a discrepancy between the token-level probabilistic optimization and the broader document-level relevance estimation; (ii) an overemphasis on top-1 results at the expense of overall ranking... | Yujia Zhou, Zhicheng Dou, JiRong Wen |  |
| 1963 |  |  [Multi-Source Probing for Open-Domain Conversational Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.769) |  | 0 | Dialogue comprehension and generation are vital to the success of open-domain dialogue systems. Although pre-trained generative conversation models have made significant progress in generating fluent responses, people have difficulty judging whether they understand and efficiently model the contextual information of the conversation. In this study, we propose a Multi-Source Probing (MSP) method to probe the dialogue comprehension abilities of open-domain dialogue models. MSP aggregates features... | Yuanxi Li, Hao Zhou, Jie Zhou, Minlie Huang |  |
| 1964 |  |  [Hallucination Mitigation in Natural Language Generation from Large-Scale Open-Domain Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.770) |  | 0 | In generating natural language descriptions for knowledge graph triples, prior works used either small-scale, human-annotated datasets or datasets with limited variety of graph shapes, e.g., those having mostly star graphs. Graph-to-text models trained and evaluated on such datasets are largely not assessed for more realistic large-scale, open-domain settings. We introduce a new dataset, GraphNarrative, to fill this gap. Fine-tuning transformer-based pre-trained language models has achieved... | Xiao Shi, Zhengyuan Zhu, Zeyu Zhang, Chengkai Li |  |
| 1965 |  |  [Multi-Source Multi-Type Knowledge Exploration and Exploitation for Dialogue Generation](https://doi.org/10.18653/v1/2023.emnlp-main.771) |  | 0 | Open-domain multi-turn dialogue generation encounters the significant challenge of lacking various types of knowledge from diverse sources. Existing models typically focus on identifying specific types of dialogue knowledge and utilize corresponding datasets for training. However, this approach often leads to limited generalization capabilities and increased computational resource requirements. Recently, large language models (LLMs) have shown impressive performance on natural language... | Xuanfan Ni, Hongliang Dai, Zhaochun Ren, Piji Li |  |
| 1966 |  |  [Focus Your Attention (with Adaptive IIR Filters)](https://doi.org/10.18653/v1/2023.emnlp-main.772) |  | 0 | We present a new layer in which dynamic (i.e., input-dependent) Infinite Impulse Response (IIR) filters of order two are used to process the input sequence prior to applying conventional attention. The input is split into chunks, and the coefficients of these filters are determined based on previous chunks to maintain causality. Despite their relatively low order, the causal adaptive filters are shown to focus attention on the relevant sequence elements. The new layer is grounded in control... | Shahar Lutati, Itamar Zimerman, Lior Wolf |  |
| 1967 |  |  [Identifying Statements Crucial for Awareness of Interpretive Nonsense to Prevent Communication Breakdowns](https://doi.org/10.18653/v1/2023.emnlp-main.773) |  | 0 | During remote conversations, communication breakdowns often occur when a listener misses certain statements. Our objective is to prevent such breakdowns by identifying Statements Crucial for Awareness of Interpretive Nonsense (SCAINs). If a listener misses a SCAIN, s/he may interpret subsequent statements differently from the speaker’s intended meaning. To identify SCAINs, we adopt a unique approach where we create a dialogue by omitting two consecutive statements from the original dialogue and... | Tomoyuki Maekawa, Michita Imai |  |
| 1968 |  |  [Multilingual Large Language Models Are Not (Yet) Code-Switchers](https://doi.org/10.18653/v1/2023.emnlp-main.774) |  | 0 | Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical... | Ruochen Zhang, Samuel Cahyawijaya, Jan Christian Blaise Cruz, Genta Indra Winata, Alham Fikri Aji |  |
| 1969 |  |  [Reinforced Target-driven Conversational Promotion](https://doi.org/10.18653/v1/2023.emnlp-main.775) |  | 0 | The ability to proactively engage with users towards pitching products is highly desired for conversational assistants. However, existing conversational recommendation methods overemphasize on acquiring user preferences while ignore the strategic planning for nudging users towards accepting a designated item. Hence, these methods fail to promote specified items with engaging responses. In this work, we propose a Reinforced Target-driven Conversational Promotion (RTCP) framework for... | Huy Dao, Lizi Liao, Dung D. Le, Yuxiang Nie |  |
| 1970 |  |  [Identification of Multimodal Stance Towards Frames of Communication](https://doi.org/10.18653/v1/2023.emnlp-main.776) |  | 0 | Frames of communication are often evoked in multimedia documents. When an author decides to add an image to a text, one or both of the modalities may evoke a communication frame. Moreover, when evoking the frame, the author also conveys her/his stance towards the frame. Until now, determining if the author is in favor of, against or has no stance towards the frame was performed automatically only when processing texts. This is due to the absence of stance annotations on multimedia documents. In... | Maxwell A. Weinzierl, Sanda M. Harabagiu |  |
| 1971 |  |  [Unsupervised Sounding Pixel Learning](https://doi.org/10.18653/v1/2023.emnlp-main.777) |  | 0 | Sounding source localization is a challenging cross-modal task due to the difficulty of cross-modal alignment. Although supervised cross-modal methods achieve encouraging performance, heavy manual annotations are expensive and inefficient. Thus it is valuable and meaningful to develop unsupervised solutions. In this paper, we propose an \*\*U\*\*nsupervised \*\*S\*\*ounding \*\*P\*\*ixel \*\*L\*\*earning (USPL) approach which enables a pixel-level sounding source localization in unsupervised... | Yining Zhang, Yanli Ji, Yang Yang |  |
| 1972 |  |  [LM vs LM: Detecting Factual Errors via Cross Examination](https://doi.org/10.18653/v1/2023.emnlp-main.778) |  | 0 | A prominent weakness of modern language models (LMs) is their tendency to generate factually incorrect text, which hinders their usability. A natural question is whether such factual errors can be detected automatically. Inspired by truth-seeking mechanisms in law, we propose a factuality evaluation framework for LMs that is based on cross-examination. Our key idea is that an incorrect claim is likely to result in inconsistency with other claims that the model generates. To discover such... | Roi Cohen, May Hamri, Mor Geva, Amir Globerson |  |
| 1973 |  |  [Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.779) |  | 0 | Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the debate and critically assess three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language... | Bram van Dijk, Tom Kouwenhoven, Marco Spruit, Max Johannes van Duijn |  |
| 1974 |  |  [PIEClass: Weakly-Supervised Text Classification with Prompting and Noise-Robust Iterative Ensemble Training](https://doi.org/10.18653/v1/2023.emnlp-main.780) |  | 0 | Weakly-supervised text classification trains a classifier using the label name of each target class as the only supervision, which largely reduces human annotation efforts. Most existing methods first use the label names as static keyword-based features to generate pseudo labels, which are then used for final classifier training. While reasonable, such a commonly adopted framework suffers from two limitations: (1) keywords can have different meanings in different contexts and some text may not... | Yunyi Zhang, Minhao Jiang, Yu Meng, Yu Zhang, Jiawei Han |  |
| 1975 |  |  [MeaeQ: Mount Model Extraction Attacks with Efficient Queries](https://doi.org/10.18653/v1/2023.emnlp-main.781) |  | 0 | We study model extraction attacks in natural language processing (NLP) where attackers aim to steal victim models by repeatedly querying the open Application Programming Interfaces (APIs). Recent works focus on limited-query budget settings and adopt random sampling or active learning-based sampling strategies on publicly available, unannotated data sources. However, these methods often result in selected queries that lack task relevance and data diversity, leading to limited success in... | Chengwei Dai, Minxuan Lv, Kun Li, Wei Zhou |  |
| 1976 |  |  [The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.782) |  | 0 | Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional... | Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, Minjoon Seo |  |
| 1977 |  |  [Explaining Interactions Between Text Spans](https://doi.org/10.18653/v1/2023.emnlp-main.783) |  | 0 | Reasoning over spans of tokens from different parts of the input is essential for natural language understanding (NLU) tasks such as fact-checking (FC), machine reading comprehension (MRC) or natural language inference (NLI). However, existing highlight-based explanations primarily focus on identifying individual important features or interactions only between adjacent tokens or tuples of tokens. Most notably, there is a lack of annotations capturing the human decision-making process with... | Sagnik Ray Choudhury, Pepa Atanasova, Isabelle Augenstein |  |
| 1978 |  |  [Predictive Chemistry Augmented with Text Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.784) |  | 0 | This paper focuses on using natural language descriptions to enhance predictive models in the chemistry field. Conventionally, chemoinformatics models are trained with extensive structured data manually extracted from the literature. In this paper, we introduce TextReact, a novel method that directly augments predictive chemistry with texts retrieved from the literature. TextReact retrieves text descriptions relevant for a given chemical reaction, and then aligns them with the molecular... | Yujie Qian, Zhening Li, Zhengkai Tu, Connor W. Coley, Regina Barzilay |  |
| 1979 |  |  [System Combination via Quality Estimation for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.emnlp-main.785) |  | 0 | Quality estimation models have been developed to assess the corrections made by grammatical error correction (GEC) models when the reference or gold-standard corrections are not available. An ideal quality estimator can be utilized to combine the outputs of multiple GEC systems by choosing the best subset of edits from the union of all edits proposed by the GEC base systems. However, we found that existing GEC quality estimation models are not good enough in differentiating good corrections... | Muhammad Reza Qorib, Hwee Tou Ng |  |
| 1980 |  |  [Rethinking Negative Pairs in Code Search](https://doi.org/10.18653/v1/2023.emnlp-main.786) |  | 0 | Recently, contrastive learning has become a key component in fine-tuning code search models for software development efficiency and effectiveness. It pulls together positive code snippets while pushing negative samples away given search queries. Among contrastive learning, InfoNCE is the most widely used loss function due to its better performance. However, the following problems in negative samples of InfoNCE may deteriorate its representation learning: 1) The existence of false negative... | Haochen Li, Xin Zhou, Anh Tuan Luu, Chunyan Miao |  |
| 1981 |  |  [Question Answering as Programming for Solving Time-Sensitive Questions](https://doi.org/10.18653/v1/2023.emnlp-main.787) |  | 0 | Question answering plays a pivotal role in human daily life because it involves our acquisition of knowledge about the world. However, due to the dynamic and ever-changing nature of real-world facts, the answer can be completely different when the time constraint in the question changes. Recently, Large Language Models (LLMs) have shown remarkable intelligence in question answering, while our experiments reveal that the aforementioned problems still pose a significant challenge to existing... | Xinyu Zhu, Cheng Yang, Bei Chen, Siheng Li, JianGuang Lou, Yujiu Yang |  |
| 1982 |  |  [Joint Geometrical and Statistical Domain Adaptation for Cross-domain Code Vulnerability Detection](https://doi.org/10.18653/v1/2023.emnlp-main.788) |  | 0 | In code vulnerability detection tasks, a detector trained on a label-rich source domain fails to provide accurate prediction on new or unseen target domains due to the lack of labeled training data on target domains. Previous studies mainly utilize domain adaptation to perform cross-domain vulnerability detection. But they ignore the negative effect of private semantic characteristics of the target domain for domain alignment, which easily causes the problem of negative transfer. In addition,... | Qianjin Du, Shiji Zhou, Xiaohui Kuang, Gang Zhao, Jidong Zhai |  |
| 1983 |  |  [Revisiting Sparse Retrieval for Few-shot Entity Linking](https://doi.org/10.18653/v1/2023.emnlp-main.789) |  | 0 | Entity linking aims to link ambiguous mentions to their corresponding entities in a knowledge base. One of the key challenges comes from insufficient labeled data for specific domains. Although dense retrievers have achieved excellent performance on several benchmarks, their performance decreases significantly when only a limited amount of in-domain labeled data is available. In such few-shot setting, we revisit the sparse retrieval method, and propose an ELECTRA-based keyword extractor to... | Yulin Chen, Zhenran Xu, Baotian Hu, Min Zhang |  |
| 1984 |  |  [Controlling Pre-trained Language Models for Grade-Specific Text Simplification](https://doi.org/10.18653/v1/2023.emnlp-main.790) |  | 0 | Text simplification systems rewrite text to make it more readable while preserving its content. However, what makes a text easy to read depends on the intended readers. Recent work has shown that pre-trained language models can simplify text using a wealth of techniques to control output simplicity, ranging from specifying only the desired reading grade level, to directly specifying low-level edit operations. Yet it remains unclear how to set these control parameters in practice. Existing... | Sweta Agrawal, Marine Carpuat |  |
| 1985 |  |  [CLEVR-Implicit: A Diagnostic Dataset for Implicit Reasoning in Referring Expression Comprehension](https://doi.org/10.18653/v1/2023.emnlp-main.791) |  | 0 | Recently, pre-trained vision-language (VL) models have achieved remarkable success in various cross-modal tasks, including referring expression comprehension (REC). These models are pre-trained on the large-scale image-text pairs to learn the alignment between words in textual descriptions and objects in the corresponding images and then fine-tuned on downstream tasks. However, the performance of VL models is hindered when dealing with implicit text, which describes objects through comparisons... | Jingwei Zhang, Xin Wu, Yi Cai |  |
| 1986 |  |  ["Are Your Explanations Reliable?" Investigating the Stability of LIME in Explaining Text Classifiers by Marrying XAI and Adversarial Attack](https://doi.org/10.18653/v1/2023.emnlp-main.792) |  | 0 | LIME has emerged as one of the most commonly referenced tools in explainable AI (XAI) frameworks that is integrated into critical machine learning applications (e.g., healthcare and finance). However, its stability remains little explored, especially in the context of text data, due to the unique text-space constraints. To address these challenges, in this paper, we first evaluate the inherent instability of LIME on text data to establish a baseline, and then propose a novel algorithm XAIFooler... | Christopher Burger, Lingwei Chen, Thai Le |  |
| 1987 |  |  [CQE: A Comprehensive Quantity Extractor](https://doi.org/10.18653/v1/2023.emnlp-main.793) |  | 0 | Quantities are essential in documents to describe factual information. They are ubiquitous in application domains such as finance, business, medicine, and science in general. Compared to other information extraction approaches, interestingly only a few works exist that describe methods for a proper extraction and representation of quantities in text. In this paper, we present such a comprehensive quantity extraction framework from text data. It efficiently detects combinations of values and... | Satya Almasian, Vivian Kazakova, Philip Göldner, Michael Gertz |  |
| 1988 |  |  [Context Compression for Auto-regressive Transformers with Sentinel Tokens](https://doi.org/10.18653/v1/2023.emnlp-main.794) |  | 0 | The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this work, we propose a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and... | Siyu Ren, Qi Jia, Kenny Q. Zhu |  |
| 1989 |  |  [A Unified View of Evaluation Metrics for Structured Prediction](https://doi.org/10.18653/v1/2023.emnlp-main.795) |  | 0 | We present a conceptual framework that unifies a variety of evaluation metrics for different structured prediction tasks (e.g. event and relation extraction, syntactic and semantic parsing). Our framework requires representing the outputs of these tasks as objects of certain data types, and derives metrics through matching of common substructures, possibly followed by normalization. We demonstrate how commonly used metrics for a number of tasks can be succinctly expressed by this framework, and... | Yunmo Chen, William Gantt, Tongfei Chen, Aaron Steven White, Benjamin Van Durme |  |
| 1990 |  |  [A Deeper (Autoregressive) Approach to Non-Convergent Discourse Parsing](https://doi.org/10.18653/v1/2023.emnlp-main.796) |  | 0 | Online social platforms provide a bustling arena for information-sharing and for multi-party discussions. Various frameworks for dialogic discourse parsing were developed and used for the processing of discussions and for predicting the productivity of a dialogue. However, most of these frameworks are not suitable for the analysis of contentious discussions that are commonplace in many online platforms. A novel multi-label scheme for contentious dialog parsing was recently introduced by... | Oren Tsur, Yoav Tulpan |  |
| 1991 |  |  [We are Who We Cite: Bridges of Influence Between Natural Language Processing and Other Academic Fields](https://doi.org/10.18653/v1/2023.emnlp-main.797) |  | 0 | Natural Language Processing (NLP) is poised to substantially influence the world. However, significant progress comes hand-in-hand with substantial risks. Addressing them requires broad engagement with various fields of study. Yet, little empirical work examines the state of such engagement (past or current). In this paper, we quantify the degree of influence between 23 fields of study and NLP (on each other). We analyzed ~77k NLP papers, ~3.1m citations from NLP papers to other papers, and... | Jan Philip Wahle, Terry Ruas, Mohamed Abdalla, Bela Gipp, Saif M. Mohammad |  |
| 1992 |  |  [Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration](https://doi.org/10.18653/v1/2023.emnlp-main.798) |  | 0 | Kendall’s tau is frequently used to meta-evaluate how well machine translation (MT) evaluation metrics score individual translations. Its focus on pairwise score comparisons is intuitive but raises the question of how ties should be handled, a gray area that has motivated different variants in the literature. We demonstrate that, in settings like modern MT meta-evaluation, existing variants have weaknesses arising from their handling of ties, and in some situations can even be gamed. We propose... | Daniel Deutsch, George F. Foster, Markus Freitag |  |
| 1993 |  |  [SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization](https://doi.org/10.18653/v1/2023.emnlp-main.799) |  | 0 | Data scarcity has been a long standing issue in the field of open-domain social dialogue. To quench this thirst, we present SODA: the first publicly available, million-scale high-quality social dialogue dataset. By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from a large language model. Human evaluation shows that conversations in SODA are more consistent, specific, and (surprisingly) natural... | Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, Yejin Choi |  |
| 1994 |  |  [Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.800) |  | 0 | Knowledge graph entity typing (KGET) aims at inferring plausible types of entities in knowledge graphs. Existing approaches to KGET focus on how to better encode the knowledge provided by the neighbors and types of an entity into its representation. However, they ignore the semantic knowledge provided by the way in which types can be clustered together. In this paper, we propose a novel method called Multi-view Contrastive Learning for knowledge graph Entity Typing MCLET, which effectively... | Zhiwei Hu, Víctor GutiérrezBasulto, Zhiliang Xiang, Ru Li, Jeff Z. Pan |  |
| 1995 |  |  [MailEx: Email Event and Argument Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.801) |  | 0 | In this work, we present the first dataset, MailEx, for performing event extraction from conversational email threads. To this end, we first proposed a new taxonomy covering 10 event types and 76 arguments in the email domain. Our final dataset includes 1.5K email threads and ~4K emails, which are annotated with a total of ~8K event instances. To understand the task challenges, we conducted a series of experiments comparing three types of approaches, i.e., fine-tuned sequence labeling,... | Saurabh Srivastava, Gaurav Singh, Shou Matsumoto, Ali K. Raz, Paulo C. G. Costa, Joshua Poore, Ziyu Yao |  |
| 1996 |  |  [Optimized Tokenization for Transcribed Error Correction](https://doi.org/10.18653/v1/2023.emnlp-main.802) |  | 0 | The challenges facing speech recognition systems, such as variations in pronunciations, adverse audio conditions, and the scarcity of labeled data, emphasize the necessity for a post-processing step that corrects recurring errors. Previous research has shown the advantages of employing dedicated error correction models, yet training such models requires large amounts of labeled data which is not easily obtained. To overcome this limitation, synthetic transcribed-like data is often utilized,... | Tomer Wullach, Shlomo E. Chazan |  |
| 1997 |  |  [Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.803) |  | 0 | Although pre-trained language models (PLM) have achieved great success in question answering (QA), their robustness is still insufficient to support their practical applications, especially in the face of distribution shifts. Recently, test-time adaptation (TTA) has shown great potential for solving this problem, which adapts the model to fit the test samples at test time. However, TTA sometimes causes model collapse, making almost all the model outputs incorrect, which has raised concerns... | Yi Su, Yixin Ji, Juntao Li, Hai Ye, Min Zhang |  |
| 1998 |  |  [Generative Adversarial Training with Perturbed Token Detection for Model Robustness](https://doi.org/10.18653/v1/2023.emnlp-main.804) |  | 0 | Adversarial training is the dominant strategy towards model robustness. Current adversarial training methods typically apply perturbations to embedding representations, whereas actual text-based attacks introduce perturbations as discrete tokens. Thus there exists a gap between the continuous embedding representations and discrete text tokens that hampers the effectiveness of adversarial training. Moreover, the continuous representations of perturbations cannot be further utilized, resulting in... | Jiahao Zhao, Wenji Mao |  |
| 1999 |  |  [Multi-Task Knowledge Distillation with Embedding Constraints for Scholarly Keyphrase Boundary Classification](https://doi.org/10.18653/v1/2023.emnlp-main.805) |  | 0 | The task of scholarly keyphrase boundary classification aims at identifying keyphrases from scientific papers and classifying them with their types from a set of predefined classes (e.g., task, process, or material). Despite the importance of keyphrases and their types in many downstream applications including indexing, searching, and question answering over scientific documents, scholarly keyphrase boundary classification is still an under-explored task. In this work, we propose a novel... | Seo Park, Cornelia Caragea |  |
| 2000 |  |  [Set Learning for Generative Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.806) |  | 0 | Recent efforts have endeavored to employ the sequence-to-sequence (Seq2Seq) model in Information Extraction (IE) due to its potential to tackle multiple IE tasks in a unified manner. Under this formalization, multiple structured objects are concatenated as the target sequence in a predefined order. However, structured objects, by their nature, constitute an unordered set. Consequently, this formalization introduces a potential order bias, which can impair model learning. Targeting this issue,... | Jiangnan Li, Yice Zhang, Bin Liang, KamFai Wong, Ruifeng Xu |  |
| 2001 |  |  [Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation](https://doi.org/10.18653/v1/2023.emnlp-main.807) |  | 0 | Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the goal of retrieving an image among a set of candidates, which better represents the meaning of an ambiguous word within a given context. In this paper, we make a substantial step towards unveiling this interesting task by applying a varying set of approaches. Since VWSD is primarily a text-image retrieval task, we explore the latest transformer-based methods for multimodal retrieval. Additionally, we utilize Large... | Anastasia Kritharoula, Maria Lymperaiou, Giorgos Stamou |  |
| 2002 |  |  [Be Selfish, But Wisely: Investigating the Impact of Agent Personality in Mixed-Motive Human-Agent Interactions](https://doi.org/10.18653/v1/2023.emnlp-main.808) |  | 0 | A natural way to design a negotiation dialogue system is via self-play RL: train an agent that learns to maximize its performance by interacting with a simulated user that has been designed to imitate human-human dialogue data. Although this procedure has been adopted in prior work, we find that it results in a fundamentally flawed system that fails to learn the value of compromise in a negotiation, which can often lead to no agreements (i.e., the partner walking away without a deal),... | Kushal Chawla, Ian Wu, Yu Rong, Gale M. Lucas, Jonathan Gratch |  |
| 2003 |  |  [Doolittle: Benchmarks and Corpora for Academic Writing Formalization](https://doi.org/10.18653/v1/2023.emnlp-main.809) |  | 0 | Improving the quality of academic writing is a meaningful but challenging task. Conventional methods of language refinement focus on narrow, specific linguistic features within isolated sentences, such as grammatical errors and improper word use. We propose a more general task, Academic Writing Formalization (AWF), to improve the overall quality of formal academic writing at the paragraph level. We formulate this language refinement task as a formal text style transfer task which transfers... | Shizhe Diao, Yongyu Lei, Liangming Pan, Tianqing Fang, Wangchunshu Zhou, Sedrick Scott Keh, MinYen Kan, Tong Zhang |  |
| 2004 |  |  [Token Prediction as Implicit Classification to Identify LLM-Generated Text](https://doi.org/10.18653/v1/2023.emnlp-main.810) |  | 0 | This paper introduces a novel approach for identifying the possible large language models (LLMs) involved in text generation. Instead of adding an additional classification layer to a base LM, we reframe the classification task as a next-token prediction task and directly fine-tune the base LM to perform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the backbone for our experiments. We compared our approach to the more direct approach of utilizing hidden states for... | Yutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, Bhiksha Raj |  |
| 2005 |  |  [On Evaluation of Bangla Word Analogies](https://doi.org/10.18653/v1/2023.emnlp-main.811) |  | 0 | This paper presents a benchmark dataset of Bangla word analogies for evaluating the quality of existing Bangla word embeddings. Despite being the 7th largest spoken language in the world, Bangla is still a low-resource language and popular NLP models often struggle to perform well on Bangla data sets. Therefore, developing a robust evaluation set is crucial for benchmarking and guiding future research on improving Bangla word embeddings, which is currently missing. To address this issue, we... | Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu |  |
| 2006 |  |  [Reconstruct Before Summarize: An Efficient Two-Step Framework for Condensing and Summarizing Meeting Transcripts](https://doi.org/10.18653/v1/2023.emnlp-main.812) |  | 0 | Meetings typically involve multiple participants and lengthy conversations, resulting in redundant and trivial content. To overcome these challenges, we propose a two-step framework, Reconstruct before Summarize (RbS), for effective and efficient meeting summarization. RbS first leverages a self-supervised paradigm to annotate essential contents by reconstructing the meeting transcripts. Secondly, we propose a relative positional bucketing (RPB) algorithm to equip (conventional) summarization... | Haochen Tan, Han Wu, Wei Shao, Xinyun Zhang, Mingjie Zhan, Zhaohui Hou, Ding Liang, Linqi Song |  |
| 2007 |  |  [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.813) |  | 0 | Large multilingual language models typically rely on a single vocabulary shared across 100+ languages. As these models have increased in parameter count and depth, vocabulary size has remained largely unchanged. This vocabulary bottleneck limits the representational capabilities of multilingual models like XLM-R. In this paper, we introduce a new approach for scaling to very large multilingual vocabularies by de-emphasizing token sharing between languages with little lexical overlap and... | Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa |  |
| 2008 |  |  [Character-LLM: A Trainable Agent for Role-Playing](https://doi.org/10.18653/v1/2023.emnlp-main.814) |  | 0 | Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we... | Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu |  |
| 2009 |  |  [Natural Language Decompositions of Implicit Content Enable Better Text Representations](https://doi.org/10.18653/v1/2023.emnlp-main.815) |  | 0 | When people interpret text, they rely on inferences that go beyond the observed language itself. Inspired by this observation, we introduce a method for the analysis of text that takes implicitly communicated content explicitly into account. We use a large language model to produce sets of propositions that are inferentially related to the text that has been observed, then validate the plausibility of the generated content via human judgments. Incorporating these explicit representations of... | Alexander Miserlis Hoyle, Rupak Sarkar, Pranav Goel, Philip Resnik |  |
| 2010 |  |  [A Scalable Framework for Table of Contents Extraction from Complex ESG Annual Reports](https://doi.org/10.18653/v1/2023.emnlp-main.816) |  | 0 | Table of contents (ToC) extraction centres on structuring documents in a hierarchical manner. In this paper, we propose a new dataset, ESGDoc, comprising 1,093 ESG annual reports from 563 companies spanning from 2001 to 2022. These reports pose significant challenges due to their diverse structures and extensive length. To address these challenges, we propose a new framework for Toc extraction, consisting of three steps: (1) Constructing an initial tree of text blocks based on reading order and... | Xinyu Wang, Lin Gui, Yulan He |  |
| 2011 |  |  [Semantic Space Grounded Weighted Decoding for Multi-Attribute Controllable Dialogue Generation](https://doi.org/10.18653/v1/2023.emnlp-main.817) |  | 0 | Controlling chatbot utterance generation with multiple attributes such as personalities, emotions and dialogue acts is a practically useful but under-studied problem. We propose a novel framework called DASC that possesses strong controllability with a weighted decoding paradigm, while improving generation quality with the grounding in an attribute semantics space. Generation with multiple attributes is then intuitively implemented with an interpolation of multiple attribute embeddings, which... | Zhiling Zhang, Mengyue Wu, Kenny Q. Zhu |  |
| 2012 |  |  [How do languages influence each other? Studying cross-lingual data sharing during LM fine-tuning](https://doi.org/10.18653/v1/2023.emnlp-main.818) |  | 0 | Multilingual language models (MLMs) are jointly trained on data from many different languages such that representation of individual languages can benefit from other languages’ data. Impressive performance in zero-shot cross-lingual transfer shows that these models are able to exploit this property. Yet, it remains unclear to what extent, and under which conditions, languages rely on each other’s data. To answer this question, we use TracIn (Pruthi et al., 2020), a training data attribution... | Rochelle Choenni, Dan Garrette, Ekaterina Shutova |  |
| 2013 |  |  [COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation](https://doi.org/10.18653/v1/2023.emnlp-main.819) |  | 0 | As language models become increasingly integrated into our digital lives, Personalized Text Generation (PTG) has emerged as a pivotal component with a wide range of applications. However, the bias inherent in user written text, often used for PTG model training, can inadvertently associate different levels of linguistic quality with users’ protected attributes. The model can inherit the bias and perpetuate inequality in generating text w.r.t. users’ protected attributes, leading to unfair... | Nan Wang, Qifan Wang, YiChia Wang, Maziar Sanjabi, Jingzhou Liu, Hamed Firooz, Hongning Wang, Shaoliang Nie |  |
| 2014 |  |  [NameGuess: Column Name Expansion for Tabular Data](https://doi.org/10.18653/v1/2023.emnlp-main.820) |  | 0 | Recent advances in large language models have revolutionized many sectors, including the database industry. One common challenge when dealing with large volumes of tabular data is the pervasive use of abbreviated column names, which can negatively impact performance on various data search, access, and understanding tasks. To address this issue, we introduce a new task, called NameGuess, to expand column names (used in database schema) as a natural language generation problem. We create a... | Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Shen Wang, Huzefa Rangwala, George Karypis |  |
| 2015 |  |  [BLESS: Benchmarking Large Language Models on Sentence Simplification](https://doi.org/10.18653/v1/2023.emnlp-main.821) |  | 0 | We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art Large Language Models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture, pre-training methods, and accessibility, on three test sets from different domains (Wikipedia, news, and medical) under a few-shot setting. Our analysis considers a suite of automatic metrics, as well... | Tannon Kew, Alison Chi, Laura VásquezRodríguez, Sweta Agrawal, Dennis Aumiller, Fernando AlvaManchego, Matthew Shardlow |  |
| 2016 |  |  [To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing](https://doi.org/10.18653/v1/2023.emnlp-main.822) |  | 0 | NLP is in a period of disruptive change that is impacting our methodologies, funding sources, and public perception. In this work, we seek to understand how to shape our future by better understanding our past. We study factors that shape NLP as a field, including culture, incentives, and infrastructure by conducting long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity. Our interviewees identify cyclical patterns in the field, as... | Sireesh Gururaja, Amanda Bertsch, Clara Na, David Gray Widder, Emma Strubell |  |
| 2017 |  |  [PALS: Personalized Active Learning for Subjective Tasks in NLP](https://doi.org/10.18653/v1/2023.emnlp-main.823) |  | 0 | For subjective NLP problems, such as classification of hate speech, aggression, or emotions, personalized solutions can be exploited. Then, the learned models infer about the perception of the content independently for each reader. To acquire training data, texts are commonly randomly assigned to users for annotation, which is expensive and highly inefficient. Therefore, for the first time, we suggest applying an active learning paradigm in a personalized context to better learn individual... | Kamil Kanclerz, Konrad Karanowski, Julita Bielaniewicz, Marcin Gruza, Piotr Milkowski, Jan Kocon, Przemyslaw Kazienko |  |
| 2018 |  |  [ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation](https://doi.org/10.18653/v1/2023.emnlp-main.824) |  | 0 | State-of-the-art vision-language models (VLMs) still have limited performance in structural knowledge extraction, such as relations between objects. In this work, we present ViStruct, a training framework to learn VLMs for effective visual structural knowledge extraction. Two novel designs are incorporated. First, we propose to leverage the inherent structure of programming language to depict visual structural information. This approach enables explicit and consistent representation of visual... | Yangyi Chen, Xingyao Wang, Manling Li, Derek Hoiem, Heng Ji |  |
| 2019 |  |  [LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.825) |  | 0 | Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic... | Huiqiang Jiang, Qianhui Wu, ChinYew Lin, Yuqing Yang, Lili Qiu |  |
| 2020 |  |  [EXPLAIN, EDIT, GENERATE: Rationale-Sensitive Counterfactual Data Augmentation for Multi-hop Fact Verification](https://doi.org/10.18653/v1/2023.emnlp-main.826) |  | 0 | Automatic multi-hop fact verification task has gained significant attention in recent years. Despite impressive results, these well-designed models perform poorly on out-of-domain data. One possible solution is to augment the training data with counterfactuals, which are generated by minimally altering the causal features of the original data. However, current counterfactual data augmentation techniques fail to handle multi-hop fact verification due to their incapability to preserve the complex... | Yingjie Zhu, Jiasheng Si, Yibo Zhao, Haiyang Zhu, Deyu Zhou, Yulan He |  |
| 2021 |  |  [An Exploration of Left-Corner Transformations](https://doi.org/10.18653/v1/2023.emnlp-main.827) |  | 0 | The left-corner transformation (Rosenkrantz and Lewis, 1970) is used to remove left recursion from context-free grammars, which is an important step towards making the grammar parsable top-down with simple techniques. This paper generalizes prior left-corner transformations to support semiring-weighted production rules and to provide finer-grained control over which left corners may be moved. Our generalized left-corner transformation (GLCT) arose from unifying the left-corner transformation... | Andreas Opedal, Eleftheria Tsipidi, Tiago Pimentel, Ryan Cotterell, Tim Vieira |  |
| 2022 |  |  [Characterizing and Verifying Scientific Claims: Qualitative Causal Structure is All You Need](https://doi.org/10.18653/v1/2023.emnlp-main.828) |  | 0 | A scientific claim typically begins with the formulation of a research question or hypothesis, which is a tentative statement or proposition about a phenomenon or relationship between variables. Within the realm of scientific claim verification, considerable research efforts have been dedicated to attention architectures and leveraging the text comprehension capabilities of Pre-trained Language Models (PLMs), yielding promising performances. However, these models overlook the causal structure... | Jinxuan Wu, Wenhan Chao, Xian Zhou, Zhunchen Luo |  |
| 2023 |  |  [FOCUS: Effective Embedding Initialization for Monolingual Specialization of Multilingual Models](https://doi.org/10.18653/v1/2023.emnlp-main.829) |  | 0 | Using model weights pretrained on a high-resource language as a warm start can reduce the need for data and compute to obtain high-quality language models for other, especially low-resource, languages. However, if we want to use a new tokenizer specialized for the target language, we cannot transfer the source model’s embedding matrix. In this paper, we propose FOCUS - \*\*F\*\*ast \*\*O\*\*verlapping Token \*\*C\*\*ombinations \*\*U\*\*sing \*\*S\*\*parsemax, a novel embedding initialization... | Konstantin Dobler, Gerard de Melo |  |
| 2024 |  |  [ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games](https://doi.org/10.18653/v1/2023.emnlp-main.830) |  | 0 | In this work we investigate the capacity of language models to generate explicit, interpretable, and interactive world models of scientific and common-sense reasoning tasks. We operationalize this as a task of generating text games, expressed as hundreds of lines of Python code. To facilitate this task, we introduce ByteSized32, a corpus of 32 reasoning-focused text games totalling 20k lines of Python code. We empirically demonstrate that GPT-4 can use these games as templates for single-shot... | Ruoyao Wang, Graham Todd, Xingdi Yuan, Ziang Xiao, MarcAlexandre Côté, Peter A. Jansen |  |
| 2025 |  |  [Skill-Based Few-Shot Selection for In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.831) |  | 0 | \*In-context learning\* is the paradigm that adapts large language models to downstream tasks by providing a few examples. \*Few-shot selection\*—selecting appropriate examples for each test instance separately—is important for in-context learning. In this paper, we propose \*\*Skill-KNN\*\*, a skill-based few-shot selection method for in-context learning. The key advantages of Skill-KNN include: (1) it addresses the problem that existing methods based on pre-trained embeddings can be easily... | Shengnan An, Bo Zhou, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Weizhu Chen, JianGuang Lou |  |
| 2026 |  |  [MaNtLE: Model-agnostic Natural Language Explainer](https://doi.org/10.18653/v1/2023.emnlp-main.832) |  | 0 | Understanding the internal reasoning behind the predictions of machine learning systems is increasingly vital, given their rising adoption and acceptance. While previous approaches, such as LIME generate algorithmic explanations by attributing importance to input features for individual examples, recent research indicates that practitioners prefer examining language explanations that explain sub-groups of examples (Lakkaraju et al., 2022). In this paper, we introduce MaNtLE, a model-agnostic... | Rakesh R. Menon, Kerem Zaman, Shashank Srivastava |  |
| 2027 |  |  [PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer](https://doi.org/10.18653/v1/2023.emnlp-main.833) |  | 0 | Recent studies show that prompt tuning can better leverage the power of large language models than fine-tuning on downstream natural language understanding tasks. However, the existing prompt tuning methods have training instability issues, as the variance of scores under different random seeds is quite large. To address this critical problem, we first investigate and find that the loss landscape of vanilla prompt tuning is precipitous when it is visualized, where a slight change of input data... | Lichang Chen, Jiuhai Chen, Heng Huang, Minhao Cheng |  |
| 2028 |  |  [Ling-CL: Understanding NLP Models through Linguistic Curricula](https://doi.org/10.18653/v1/2023.emnlp-main.834) |  | 0 | We employ a characterization of linguistic complexity from psycholinguistic and language acquisition research to develop data-driven curricula to understand the underlying linguistic knowledge that models learn to address NLP tasks. The novelty of our approach is in the development of linguistic curricula derived from data, existing knowledge about linguistic complexity, and model behavior during training. Through the evaluation of several benchmark NLP datasets, our curriculum learning... | Mohamed Elgaar, Hadi Amiri |  |
| 2029 |  |  [Towards Unsupervised Recognition of Token-level Semantic Differences in Related Documents](https://doi.org/10.18653/v1/2023.emnlp-main.835) |  | 0 | Automatically highlighting words that cause semantic differences between two documents could be useful for a wide range of applications. We formulate recognizing semantic differences (RSD) as a token-level regression task and study three unsupervised approaches that rely on a masked language model. To assess the approaches, we begin with basic English sentences and gradually move to more complex, cross-lingual document pairs. Our results show that an approach based on word alignment and... | Jannis Vamvas, Rico Sennrich |  |
| 2030 |  |  [Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance](https://doi.org/10.18653/v1/2023.emnlp-main.836) |  | 0 | Multilingual Neural Machine Translation (MNMT) facilitates knowledge sharing but often suffers from poor zero-shot (ZS) translation qualities. While prior work has explored the causes of overall low zero-shot translation qualities, our work introduces a fresh perspective: the presence of significant variations in zero-shot performance. This suggests that MNMT does not uniformly exhibit poor zero-shot capability; instead, certain translation directions yield reasonable results. Through... | Shaomu Tan, Christof Monz |  |
| 2031 |  |  [SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA](https://doi.org/10.18653/v1/2023.emnlp-main.837) |  | 0 | Question answering over hybrid contexts is a complex task, which requires the combination of information extracted from unstructured texts and structured tables in various ways. Recently, In-Context Learning demonstrated significant performance advances for reasoning tasks. In this paradigm, a large language model performs predictions based on a small set of supporting exemplars. The performance of In-Context Learning depends heavily on the selection procedure of the supporting exemplars,... | Jonathan Tonglet, Manon Reusens, Philipp Borchert, Bart Baesens |  |
| 2032 |  |  [Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations](https://doi.org/10.18653/v1/2023.emnlp-main.838) |  | 0 | In the field of natural language processing, open-domain chatbots have emerged as an important research topic. However, a major limitation of existing open-domain chatbot research is its singular focus on short single-session dialogue, neglecting the potential need for understanding contextual information in multiple consecutive sessions that precede an ongoing dialogue. Among the elements that compose the context in multi-session conversation settings, the time intervals between sessions and... | Jihyoung Jang, Minseong Boo, Hyounghun Kim |  |
| 2033 |  |  [DueT: Image-Text Contrastive Transfer Learning with Dual-adapter Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.839) |  | 0 | This paper presents DueT, a novel transfer learning method for vision and language models built by contrastive learning. In DueT, adapters are inserted into the image and text encoders, which have been initialized using models pre-trained on uni-modal corpora and then frozen. By training only these adapters, DueT enables efficient learning with a reduced number of trainable parameters. Moreover, unlike traditional adapters, those in DueT are equipped with a gating mechanism, enabling effective... | Taku Hasegawa, Kyosuke Nishida, Koki Maeda, Kuniko Saito |  |
| 2034 |  |  [Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation](https://doi.org/10.18653/v1/2023.emnlp-main.840) |  | 0 | In Conversational Recommendation System (CRS), an agent is asked to recommend a set of items to users within natural language conversations. To address the need for both conversational capability and personalized recommendations, prior works have utilized separate recommendation and dialogue modules. However, such approach inevitably results in a discrepancy between recommendation results and generated responses. To bridge the gap, we propose a multi-task learning for a unified CRS, where a... | Yeongseo Jung, Eunseo Jung, Lei Chen |  |
| 2035 |  |  [CLAIR: Evaluating Image Captions with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.841) |  | 0 | The evaluation of machine-generated image captions poses an interesting yet persistent challenge. Effective evaluation measures must consider numerous dimensions of similarity, including semantic relevance, visual structure, object interactions, caption diversity, and specificity. Existing highly-engineered measures attempt to capture specific aspects, but fall short in providing a holistic score that aligns closely with human judgments. Here, we propose CLAIR, a novel method that leverages the... | David M. Chan, Suzanne Petryk, Joseph Gonzalez, Trevor Darrell, John F. Canny |  |
| 2036 |  |  [MoPe: Model Perturbation based Privacy Attacks on Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.842) |  | 0 | Recent work has shown that Large Language Models (LLMs) can unintentionally leak sensitive information present in their training data. In this paper, we present Model Perturbations (MoPe), a new method to identify with high confidence if a given text is in the training data of a pre-trained language model, given white-box access to the models parameters. MoPe adds noise to the model in parameter space and measures the drop in log-likelihood at a given point x, a statistic we show approximates... | Marvin Li, Jason Wang, Jeffrey G. Wang, Seth Neel |  |
| 2037 |  |  [q2d: Turning Questions into Dialogs to Teach Models How to Search](https://doi.org/10.18653/v1/2023.emnlp-main.843) |  | 0 | One of the exciting capabilities of recent language models for dialog is their ability to independently search for relevant information to ground a given dialog response. However, obtaining training data to teach models how to issue search queries is time and resource consuming. In this work, we propose q2d: an automatic data generation pipeline that generates information-seeking dialogs from questions. We prompt a large language model (PaLM) to create conversational versions of question... | Yonatan Bitton, Shlomi CohenGanor, Ido Hakimi, Yoad Lewenberg, Roee Aharoni, Enav Weinreb |  |
| 2038 |  |  [Aligning Large Language Models through Synthetic Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.844) |  | 0 | Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT. In this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses... | Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, Minjoon Seo |  |
| 2039 |  |  [You Told Me That Joke Twice: A Systematic Investigation of Transferability and Robustness of Humor Detection Models](https://doi.org/10.18653/v1/2023.emnlp-main.845) |  | 0 | In this study, we focus on automatic humor detection, a highly relevant task for conversational AI. To date, there are several English datasets for this task, but little research on how models trained on them generalize and behave in the wild. To fill this gap, we carefully analyze existing datasets, train RoBERTa-based and Naïve Bayes classifiers on each of them, and test on the rest. Training and testing on the same dataset yields good results, but the transferability of the models varies... | Alexander Baranov, Vladimir Kniazhevsky, Pavel Braslavski |  |
| 2040 |  |  [Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction](https://doi.org/10.18653/v1/2023.emnlp-main.846) |  | 0 | Recent advances in multimodal pre-trained models have significantly improved information extraction from visually-rich documents (VrDs), in which named entity recognition (NER) is treated as a sequence-labeling task of predicting the BIO entity tags for tokens, following the typical setting of NLP. However, BIO-tagging scheme relies on the correct order of model inputs, which is not guaranteed in real-world NER on scanned VrDs where text are recognized and arranged by OCR systems. Such reading... | Chong Zhang, Ya Guo, Yi Tu, Huan Chen, Jinyang Tang, Huijia Zhu, Qi Zhang, Tao Gui |  |
| 2041 |  |  [Empower Nested Boolean Logic via Self-Supervised Curriculum Learning](https://doi.org/10.18653/v1/2023.emnlp-main.847) |  | 0 | Beyond the great cognitive powers showcased by language models, it is crucial to scrutinize whether their reasoning capabilities stem from strong generalization or merely exposure to relevant data. As opposed to constructing increasingly complex logic, this paper probes into the boolean logic, the root capability of a logical reasoner. We find that any pre-trained language models even including large language models only behave like a random selector in the face of multi-nested boolean logic, a... | Hongqiu Wu, Linfeng Liu, Hai Zhao, Min Zhang |  |
| 2042 |  |  [The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.848) |  | 0 | We conduct an inquiry into the sociotechnical aspects of sentiment analysis (SA) by critically examining 189 peer-reviewed papers on their applications, models, and datasets. Our investigation stems from the recognition that SA has become an integral component of diverse sociotechnical systems, exerting influence on both social and technical users. By delving into sociological and technological literature on sentiment, we unveil distinct conceptualizations of this term in domains such as... | Pranav Venkit, Mukund Srinath, Sanjana Gautam, Saranya Venkatraman, Vipul Gupta, Rebecca J. Passonneau, Shomir Wilson |  |
| 2043 |  |  [Poisoning Retrieval Corpora by Injecting Adversarial Passages](https://doi.org/10.18653/v1/2023.emnlp-main.849) |  | 0 | Dense retrievers have achieved state-of-the-art performance in various information retrieval tasks, but to what extent can they be safely deployed in real-world applications? In this work, we propose a novel attack for dense retrieval systems in which a malicious user generates a small number of adversarial passages by perturbing discrete tokens to maximize similarity with a provided set of training queries. When these adversarial passages are inserted into a large retrieval corpus, we show... | Zexuan Zhong, Ziqing Huang, Alexander Wettig, Danqi Chen |  |
| 2044 |  |  [DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules](https://doi.org/10.18653/v1/2023.emnlp-main.850) |  | 0 | Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects. While existing mitigations tackle discrepancies for individual target dialects, they assume access to high-accuracy dialect identification systems. The boundaries between dialects are inherently flexible, making it difficult to categorize language into discrete predefined categories. In this paper, we propose DADA... | Yanchen Liu, William Held, Diyi Yang |  |
| 2045 |  |  [Clustering Pseudo Language Family in Multilingual Translation Models with Fisher Information Matrix](https://doi.org/10.18653/v1/2023.emnlp-main.851) |  | 0 | In multilingual translation research, the comprehension and utilization of language families are of paramount importance. Nevertheless, clustering languages based solely on their ancestral families can yield suboptimal results due to variations in the datasets employed during the model’s training phase. To mitigate this challenge, we introduce an innovative method that leverages the fisher information matrix (FIM) to cluster language families, anchored on the multilingual translation model’s... | Xinyu Ma, Xuebo Liu, Min Zhang |  |
| 2046 |  |  [Unifying Discrete and Continuous Representations for Unsupervised Paraphrase Generation](https://doi.org/10.18653/v1/2023.emnlp-main.852) |  | 0 | Unsupervised paraphrase generation is a challenging task that benefits a variety of downstream NLP applications. Current unsupervised methods for paraphrase generation typically employ round-trip translation or denoising, which require translation corpus and result in paraphrases overly similar to the original sentences in surface structure. Most of these methods lack explicit control over the similarity between the original and generated sentences, and the entities are also less correctly... | Mingfeng Xue, Dayiheng Liu, Wenqiang Lei, Jie Fu, Jian Lan, Mei Li, Baosong Yang, Jun Xie, Yidan Zhang, Dezhong Peng, Jiancheng Lv |  |
| 2047 |  |  [The Benefits of Label-Description Training for Zero-Shot Text Classification](https://doi.org/10.18653/v1/2023.emnlp-main.853) |  | 0 | Pretrained language models have improved zero-shot text classification by allowing the transfer of semantic knowledge from the training data in order to classify among specific label sets in downstream tasks. We propose a simple way to further improve zero-shot accuracies with minimal effort. We curate small finetuning datasets intended to describe the labels for a task. Unlike typical finetuning data, which has texts annotated with labels, our data simply describes the labels in language,... | Lingyu Gao, Debanjan Ghosh, Kevin Gimpel |  |
| 2048 |  |  [Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer](https://doi.org/10.18653/v1/2023.emnlp-main.854) |  | 0 | We introduce and demonstrate how to effectively train multilingual machine translation models with pixel representations. We experiment with two different data settings with a variety of language and script coverage, demonstrating improved performance compared to subword embeddings. We explore various properties of pixel representations such as parameter sharing within and across scripts to better understand where they lead to positive transfer. We observe that these properties not only enable... | Elizabeth Salesky, Neha Verma, Philipp Koehn, Matt Post |  |
| 2049 |  |  [Finding Authentic Counterhate Arguments: A Case Study with Public Figures](https://doi.org/10.18653/v1/2023.emnlp-main.855) |  | 0 | We explore authentic counterhate arguments for online hateful content toward individuals. Previous efforts are limited to counterhate to fight against hateful content toward groups. Thus, we present a corpus of 54,816 hateful tweet-paragraph pairs, where the paragraphs are candidate counterhate arguments. The counterhate arguments are retrieved from 2,500 online articles from multiple sources. We propose a methodology that assures the authenticity of the counter argument and its specificity to... | Abdullah Albanyan, Ahmed Hassan, Eduardo Blanco |  |
| 2050 |  |  [Can We Edit Multimodal Large Language Models?](https://doi.org/10.18653/v1/2023.emnlp-main.856) |  | 0 | In this paper, we focus on editing multimodal Large Language Models (LLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing... | Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, Ningyu Zhang |  |
| 2051 |  |  [Exploring Discourse Structure in Document-level Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.857) |  | 0 | Neural machine translation has achieved great success in the past few years with the help of transformer architectures and large-scale bilingual corpora. However, when the source text gradually grows into an entire document, the performance of current methods for document-level machine translation (DocMT) is less satisfactory. Although the context is beneficial to the translation in general, it is difficult for traditional methods to utilize such long-range information. Previous studies on... | Xinyu Hu, Xiaojun Wan |  |
| 2052 |  |  [ClusterLLM: Large Language Models as a Guide for Text Clustering](https://doi.org/10.18653/v1/2023.emnlp-main.858) |  | 0 | We introduce ClusterLLM, a novel text clustering framework that leverages feedback from an instruction-tuned large language model, such as ChatGPT. Compared with traditional unsupervised methods that builds upon “small” embedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the emergent capability of LLM even if its embeddings are inaccessible; and (2) it understands the user’s preference on clustering through textual instruction and/or a few annotated data. First, we prompt... | Yuwei Zhang, Zihan Wang, Jingbo Shang |  |
| 2053 |  |  [CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code](https://doi.org/10.18653/v1/2023.emnlp-main.859) |  | 0 | Since the rise of neural natural-language-to-code models (NL→Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the... | Shuyan Zhou, Uri Alon, Sumit Agarwal, Graham Neubig |  |
| 2054 |  |  [Learn and Consolidate: Continual Adaptation for Zero-Shot and Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.860) |  | 0 | Although existing multilingual neural machine translation (MNMT) models have demonstrated remarkable performance to handle multiple translation directions in a single model and achieved zero-shot translation between language pairs unseen in training, they still suffer from relatively poor translation qualities for some language pairs. A practical scenario is that how to continually update MNMT models for both supervised and zero-shot translations when limited new data arrives. To this end, we... | Kaiyu Huang, Peng Li, Junpeng Liu, Maosong Sun, Yang Liu |  |
| 2055 |  |  [e-THERAPIST: I suggest you to cultivate a mindset of positivity and nurture uplifting thoughts](https://doi.org/10.18653/v1/2023.emnlp-main.861) |  | 0 | The shortage of therapists for mental health patients emphasizes the importance of globally accessible dialogue systems alleviating their issues. To have effective interpersonal psychotherapy, these systems must exhibit politeness and empathy when needed. However, these factors may vary as per the user’s gender, age, persona, and sentiment. Hence, in order to establish trust and provide a personalized cordial experience, it is essential that generated responses should be tailored to individual... | Kshitij Mishra, Priyanshu Priya, Manisha Burja, Asif Ekbal |  |
| 2056 |  |  [AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages](https://doi.org/10.18653/v1/2023.emnlp-main.862) |  | 0 | Africa is home to over 2,000 languages from over six language families and has the highest linguistic diversity among all continents. This includes 75 languages with at least one million speakers each. Yet, there is little NLP research conducted on African languages. Crucial in enabling such research is the availability of high-quality annotated datasets. In this paper, we introduce AfriSenti, a sentiment analysis benchmark that contains a total of >110,000 tweets in 14 African languages... | Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Abinew Ali Ayele, Nedjma Ousidhoum, David Ifeoluwa Adelani, Seid Muhie Yimam, Ibrahim Said Ahmad, Meriem Beloucif, Saif M. Mohammad, Sebastian Ruder, Oumaima Hourrane, Alípio Jorge, Pavel Brazdil, Felermino Dário Mário António Ali, Davis David, Salomey Osei, Bello Shehu Bello, Falalu Ibrahim Lawan, Tajuddeen Gwadabe, Samuel Rutunda, Tadesse Destaw Belay, Wendimu Baye Messelle, Hailu Beshada Balcha, Sisay Adugna Chala, Hagos Tesfahun Gebremichael, Bernard Opoku, Stephen Arthur |  |
| 2057 |  |  [Quantifying Character Similarity with Vision Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.863) |  | 0 | Record linkage is a bedrock of quantitative social science, as analyses often require linking data from multiple, noisy sources. Off-the-shelf string matching methods are widely used, as they are straightforward and cheap to implement and scale. Not all character substitutions are equally probable, and for some settings there are widely used handcrafted lists denoting which string substitutions are more likely, that improve the accuracy of string matching. However, such lists do not exist for... | Xinmei Yang, Abhishek Arora, ShaoYu Jheng, Melissa Dell |  |
| 2058 |  |  [Syllogistic Reasoning for Legal Judgment Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.864) |  | 0 | Legal judgment assistants are developing fast due to impressive progress of large language models (LLMs). However, people can hardly trust the results generated by a model without reliable analysis of legal judgement. For legal practitioners, it is common practice to utilize syllogistic reasoning to select and evaluate the arguments of the parties as part of the legal decision-making process. But the development of syllogistic reasoning for legal judgment analysis is hindered by the lack of... | Wentao Deng, Jiahuan Pei, Keyi Kong, Zhe Chen, Furu Wei, Yujun Li, Zhaochun Ren, Zhumin Chen, Pengjie Ren |  |
| 2059 |  |  [Improving Transformer-based Program Repair Model through False Behavior Diagnosis](https://doi.org/10.18653/v1/2023.emnlp-main.865) |  | 0 | Research on automated program repairs using transformer-based models has recently gained considerable attention. The comprehension of the erroneous behavior of a model enables the identification of its inherent capacity and provides insights for improvement. However, the current landscape of research on program repair models lacks an investigation of their false behavior. Thus, we propose a methodology for diagnosing and treating the false behaviors of transformer-based program repair models.... | Youngkyoung Kim, Misoo Kim, Eunseok Lee |  |
| 2060 |  |  [SUT: Active Defects Probing for Transcompiler Models](https://doi.org/10.18653/v1/2023.emnlp-main.866) |  | 0 | Automatic Program translation has enormous application value and hence has been attracting significant interest from AI researchers. However, we observe that current program translation models still make elementary syntax errors, particularly, when the target language does not have syntax elements in the source language. Metrics like BLUE, CodeBLUE and computation accuracy may not expose these issues. In this paper we introduce a new metrics for programming language translation and these... | Mengnan Qi, Yufan Huang, Maoquan Wang, Yongqiang Yao, Zihan Liu, Bin Gu, Colin B. Clement, Neel Sundaresan |  |
| 2061 |  |  [KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection](https://doi.org/10.18653/v1/2023.emnlp-main.867) |  | 0 | Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the \*hallucination\* problem, poses a significant risk to their deployment. A common approach to address this issue is to retrieve relevant knowledge and fine-tune the LLM with the knowledge in its input. Unfortunately, this method incurs high training costs and may cause catastrophic forgetting for multi-tasking... | Sehyun Choi, Tianqing Fang, Zhaowei Wang, Yangqiu Song |  |
| 2062 |  |  [CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL](https://doi.org/10.18653/v1/2023.emnlp-main.868) |  | 0 | Existing Text-to-SQL generators require the entire schema to be encoded with the user text. This is expensive or impractical for large databases with tens of thousands of columns. Standard dense retrieval techniques are inadequate for schema subsetting of a large structured database, where the correct semantics of retrieval demands that we rank sets of schema elements rather than individual documents. In response, we propose a two-stage process for effective coverage during retrieval. First, we... | Mayank Kothyari, Dhruva Dhingra, Sunita Sarawagi, Soumen Chakrabarti |  |
| 2063 |  |  [This Reads Like That: Deep Learning for Interpretable Natural Language Processing](https://doi.org/10.18653/v1/2023.emnlp-main.869) |  | 0 | Prototype learning, a popular machine learning method designed for inherently interpretable decisions, leverages similarities to learned prototypes for classifying new data. While it is mainly applied in computer vision, in this work, we build upon prior research and further explore the extension of prototypical networks to natural language processing. We introduce a learned weighted similarity measure that enhances the similarity computation by focusing on informative dimensions of pre-trained... | Claudio Fanconi, Moritz Vandenhirtz, Severin Husmann, Julia E. Vogt |  |
| 2064 |  |  [Incorporating Structured Representations into Pretrained Vision & Language Models Using Scene Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.870) |  | 0 | Vision and language models (VLMs) have demonstrated remarkable zero-shot (ZS) performance in a variety of tasks. However, recent works have shown that even the best VLMs struggle to capture aspects of compositional scene understanding, such as object attributes, relations, and action states. In contrast, obtaining structured annotations, such as scene graphs (SGs), that could improve these models is time-consuming and costly, and thus cannot be used on a large scale. Here we ask whether small... | Roei Herzig, Alon Mendelson, Leonid Karlinsky, Assaf Arbelle, Rogério Feris, Trevor Darrell, Amir Globerson |  |
| 2065 |  |  [TLM: Token-Level Masking for Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.871) |  | 0 | Structured dropout approaches, such as attention dropout and DropHead, have been investigated to regularize the multi-head attention mechanism in Transformers. In this paper, we propose a new regularization scheme based on token-level rather than structure-level to reduce overfitting. Specifically, we devise a novel Token-Level Masking (TLM) training strategy for Transformers to regularize the connections of self-attention, which consists of two masking techniques that are effective and easy to... | Yangjun Wu, Kebin Fang, Dongxiang Zhang, Han Wang, Hao Zhang, Gang Chen |  |
| 2066 |  |  [Addressing NER Annotation Noises with Uncertainty-Guided Tree-Structured CRFs](https://doi.org/10.18653/v1/2023.emnlp-main.872) |  | 0 | Real-world named entity recognition (NER) datasets are notorious for their noisy nature, attributed to annotation errors, inconsistencies, and subjective interpretations. Such noises present a substantial challenge for traditional supervised learning methods. In this paper, we present a new and unified approach to tackle annotation noises for NER. Our method considers NER as a constituency tree parsing problem, utilizing a tree-structured Conditional Random Fields (CRFs) with uncertainty... | Jian Liu, Weichang Liu, Yufeng Chen, Jinan Xu, Zhe Zhao |  |
| 2067 |  |  [Hi Guys or Hi Folks? Benchmarking Gender-Neutral Machine Translation with the GeNTE Corpus](https://doi.org/10.18653/v1/2023.emnlp-main.873) |  | 0 | Gender inequality is embedded in our communication practices and perpetuated in translation technologies. This becomes particularly apparent when translating into grammatical gender languages, where machine translation (MT) often defaults to masculine and stereotypical representations by making undue binary gender assumptions. Our work addresses the rising demand for inclusive language by focusing head-on on gender-neutral translation from English to Italian. We start from the essentials:... | Andrea Piergentili, Beatrice Savoldi, Dennis Fucci, Matteo Negri, Luisa Bentivogli |  |
| 2068 |  |  [Multilingual Holistic Bias: Extending Descriptors and Patterns to Unveil Demographic Biases in Languages at Scale](https://doi.org/10.18653/v1/2023.emnlp-main.874) |  | 0 | We introduce a multilingual extension of the HolisticBias dataset, the largest English template-based taxonomy of textual people references: Multilingual HolisticBias. This extension consists of 20,459 sentences in 50 languages distributed across 13 demographic axes. Source sentences are built from combinations of 118 demographic descriptors and three patterns, excluding nonsensical combinations. Multilingual translations include alternatives for gendered languages that cover gendered... | Marta R. Costajussà, Pierre Andrews, Eric Michael Smith, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Daniel Licht, Carleigh Wood |  |
| 2069 |  |  [GlobalBench: A Benchmark for Global Progress in Natural Language Processing](https://doi.org/10.18653/v1/2023.emnlp-main.875) |  | 0 | Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding... | Yueqi Song, Simran Khanuja, Pengfei Liu, Fahim Faisal, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig |  |
| 2070 |  |  [DetGPT: Detect What You Need via Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.876) |  | 0 | In recent years, the field of computer vision has seen significant advancements thanks to the development of large language models (LLMs). These models have enabled more effective and sophisticated interactions between humans and machines, paving the way for novel techniques that blur the lines between human and machine intelligence. In this paper, we introduce a new paradigm for object detection that we call reasoning-based object detection. Unlike conventional object detection methods that... | Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, Lingpeng Kong, Tong Zhang |  |
| 2071 |  |  [Language Models with Rationality](https://doi.org/10.18653/v1/2023.emnlp-main.877) |  | 0 | While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent “beliefs”. This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs and their inferential relationships explicit, and to resolve inconsistencies that may exist, so that answers are supported by interpretable chains of reasoning drawn from a consistent network of... | Nora Kassner, Oyvind Tafjord, Ashish Sabharwal, Kyle Richardson, Hinrich Schütze, Peter Clark |  |
| 2072 |  |  [Self-Improvement of Non-autoregressive Model via Sequence-Level Distillation](https://doi.org/10.18653/v1/2023.emnlp-main.878) |  | 0 | Although Non-autoregressive Transformer (NAT) models have achieved great success in terms of fast inference speed, this speedup comes with a performance drop due to the inherent multi-modality problem of the NAT model. Previous works commonly alleviate this problem by replacing the target side of the raw data with distilled data generated by Autoregressive Transformer (AT) models. However, the multi-modality problem in the distilled data is still significant and thus limits further improvement... | Yusheng Liao, Shuyang Jiang, Yiqi Li, Yu Wang, Yanfeng Wang |  |
| 2073 |  |  [Mitigating Temporal Misalignment by Discarding Outdated Facts](https://doi.org/10.18653/v1/2023.emnlp-main.879) |  | 0 | While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having only been trained on data collected in the past. To mitigate the effects of temporal misalignment, we propose fact duration prediction: the task of predicting how long a given fact will... | Michael J. Q. Zhang, Eunsol Choi |  |
| 2074 |  |  [Open-world Semi-supervised Generalized Relation Discovery Aligned in a Real-world Setting](https://doi.org/10.18653/v1/2023.emnlp-main.880) |  | 0 | Open-world Relation Extraction (OpenRE) has recently garnered significant attention. However, existing approaches tend to oversimplify the problem by assuming that all instances of unlabeled data belong to novel classes, thereby limiting the practicality of these methods. We argue that the OpenRE setting should be more aligned with the characteristics of real-world data. Specifically, we propose two key improvements: (a) unlabeled data should encompass known and novel classes, including... | William Hogan, Jiacheng Li, Jingbo Shang |  |
| 2075 |  |  [IEKG: A Commonsense Knowledge Graph for Idiomatic Expressions](https://doi.org/10.18653/v1/2023.emnlp-main.881) |  | 0 | Idiomatic expression (IE) processing and comprehension have challenged pre-trained language models (PTLMs) because their meanings are non-compositional. Unlike prior works that enable IE comprehension through fine-tuning PTLMs with sentences containing IEs, in this work, we construct IEKG, a commonsense knowledge graph for figurative interpretations of IEs. This extends the established ATOMIC2020 converting PTLMs into knowledge models (KMs) that encode and infer commonsense knowledge related to... | Ziheng Zeng, Kellen Tan Cheng, Srihari Venkat Nanniyur, Jianing Zhou, Suma Bhat |  |
| 2076 |  |  [Bias Neutralization in Non-Parallel Texts: A Cyclic Approach with Auxiliary Guidance](https://doi.org/10.18653/v1/2023.emnlp-main.882) |  | 0 | Objectivity is a goal for Wikipedia and many news sites, as well as a guiding principle of many large language models. Indeed, several methods have recently been developed for automatic subjective bias neutralization. These methods, however, typically rely on parallel text for training (i.e. a biased sentence coupled with a non-biased sentence), demonstrate poor transfer to new domains, and can lose important bias-independent context. Toward expanding the reach of bias neutralization, we... | Karthic Madanagopal, James Caverlee |  |
| 2077 |  |  [Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation](https://doi.org/10.18653/v1/2023.emnlp-main.883) |  | 0 | Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (\*.i.e, generating large-scale harmful and misleading content\*). To combat this emerging risk of LLMs, we propose a novel “\*\*\*Fighting Fire with Fire\*\*\*” (F3) strategy that harnesses modern LLMs’ generative and emergent reasoning capabilities to counter human-written and LLM-generated disinformation. First, we leverage GPT-3.5-turbo to synthesize authentic and... | Jason Samuel Lucas, Adaku Uchendu, Michiharu Yamashita, Jooyoung Lee, Shaurya Rohatgi, Dongwon Lee |  |
| 2078 |  |  [SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts](https://doi.org/10.18653/v1/2023.emnlp-main.884) |  | 0 | Prompt tuning has emerged as a successful parameter-efficient alternative to the full fine-tuning of language models. However, prior works on prompt tuning often utilize long soft prompts of up to 100 tokens to improve performance, overlooking the inefficiency associated with extended inputs. In this paper, we propose a novel prompt tuning method SMoP (Sparse Mixture-of-Prompts) that utilizes short soft prompts for efficient training and inference while maintaining performance gains typically... | JoonYoung Choi, Junho Kim, JunHyung Park, WingLam Mok, SangKeun Lee |  |
| 2079 |  |  [BRAINTEASER: Lateral Thinking Puzzles for Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.885) |  | 0 | The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BrainTeaser: a multiple-choice Question Answering task designed to test the model’s ability to exhibit lateral thinking and defy default commonsense associations. We design a... | Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati |  |
| 2080 |  |  [When are Lemons Purple? The Concept Association Bias of Vision-Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.886) |  | 0 | Large-scale vision-language models such as CLIP have shown impressive performance on zero-shot image classification and image-to-text retrieval. However, such performance does not realize in tasks that require a finer-grained correspondence between vision and language, such as Visual Question Answering (VQA). We investigate why this is the case, and report an interesting phenomenon of vision-language models, which we call the Concept Association Bias (CAB), as a potential cause of the... | Yingtian Tang, Yutaro Yamada, Yoyo Zhang, Ilker Yildirim |  |
| 2081 |  |  [What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability](https://doi.org/10.18653/v1/2023.emnlp-main.887) |  | 0 | In Natural Language Generation (NLG) tasks, for any input, multiple communicative goals are plausible, and any goal can be put into words, or produced, in multiple ways. We characterise the extent to which human production varies lexically, syntactically, and semantically across four NLG tasks, connecting human production variability to aleatoric or data uncertainty. We then inspect the space of output strings shaped by a generation system’s predicted probability distribution and decoding... | Mario Giulianelli, Joris Baan, Wilker Aziz, Raquel Fernández, Barbara Plank |  |
| 2082 |  |  [Text Representation Distillation via Information Bottleneck Principle](https://doi.org/10.18653/v1/2023.emnlp-main.888) |  | 0 | Pre-trained language models (PLMs) have recently shown great success in text representation field. However, the high computational cost and high-dimensional representation of PLMs pose significant challenges for practical applications. To make models more accessible, an effective method is to distill large models into smaller representation models. In order to relieve the issue of performance degradation after distillation, we propose a novel Knowledge Distillation method called IBKD. This... | Yanzhao Zhang, Dingkun Long, Zehan Li, Pengjun Xie |  |
| 2083 |  |  [Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation](https://doi.org/10.18653/v1/2023.emnlp-main.889) |  | 0 | In this paper, we present a novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models. Our approach is designed to consider the student model’s weaknesses and foster a tailored learning experience by generating targeted exercises aligned with educational science principles, such as knowledge tracing and personalized learning. Concretely, we let GPT-3 be a math tutor and run two steps iteratively: 1)... | Zhenwen Liang, Wenhao Yu, Tanmay Rajpurohit, Peter Clark, Xiangliang Zhang, Ashwin Kalyan |  |
| 2084 |  |  [FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions](https://doi.org/10.18653/v1/2023.emnlp-main.890) |  | 0 | Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand... | Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim, Yejin Choi, Maarten Sap |  |
| 2085 |  |  [Exploring the Boundaries of GPT-4 in Radiology](https://doi.org/10.18653/v1/2023.emnlp-main.891) |  | 0 | The recent success of general-domain large language models (LLMs) has significantly changed the natural language processing paradigm towards a unified foundation model across domains and applications. In this paper, we focus on assessing the performance of GPT-4, the most capable LLM so far, on the text-based applications for radiology reports, comparing against state-of-the-art (SOTA) radiology-specific models. Exploring various prompting strategies, we evaluated GPT-4 on a diverse range of... | Qianchu Liu, Stephanie L. Hyland, Shruthi Bannur, Kenza Bouzid, Daniel C. Castro, Maria Wetscherek, Robert Tinn, Harshita Sharma, Fernando PérezGarcía, Anton Schwaighofer, Pranav Rajpurkar, Sameer Tajdin Khanna, Hoifung Poon, Naoto Usuyama, Anja Thieme, Aditya V. Nori, Matthew P. Lungren, Ozan Oktay, Javier AlvarezValle |  |
| 2086 |  |  [A Frustratingly Easy Post-Training Quantization Scheme for LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.892) |  | 0 | Efficient inference has become crucial for hyper-scale AI models, including large language models, as their parameter count continues to increase for enhanced performance. This necessity holds true regardless of the computing environment, whether it be mobile devices or cloud servers. Quantization emerges as a solution to alleviate the computational burden during inference. By representing models with a reduced bit-width, quantization minimizes the frequency of DRAM access while fully... | Yongkweon Jeon, Chungman Lee, Kyungphil Park, HoYoung Kim |  |
| 2087 |  |  [A Comprehensive Evaluation of Biomedical Entity Linking Models](https://doi.org/10.18653/v1/2023.emnlp-main.893) |  | 0 | Biomedical entity linking (BioEL) is the process of connecting entities referenced in documents to entries in biomedical databases such as the Unified Medical Language System (UMLS) or Medical Subject Headings (MeSH). The study objective was to comprehensively evaluate nine recent state-of-the-art biomedical entity linking models under a unified framework. We compare these models along axes of (1) accuracy, (2) speed, (3) ease of use, (4) generalization, and (5) adaptability to new ontologies... | David Kartchner, Jennifer Deng, Shubham Lohiya, Tejasri Kopparthi, Prasanth Bathala, Daniel DomingoFernández, Cassie S. Mitchell |  |
| 2088 |  |  [Exploring Jiu-Jitsu Argumentation for Writing Peer Review Rebuttals](https://doi.org/10.18653/v1/2023.emnlp-main.894) |  | 0 | In many domains of argumentation, people’s arguments are driven by so-called attitude roots, i.e., underlying beliefs and world views, and their corresponding attitude themes. Given the strength of these latent drivers of arguments, recent work in psychology suggests that instead of directly countering surface-level reasoning (e.g., falsifying the premises), one should follow an argumentation style inspired by the Jiu-Jitsu “soft” combat system: first, identify an arguer’s attitude roots and... | Sukannya Purkayastha, Anne Lauscher, Iryna Gurevych |  |
| 2089 |  |  [LIMIT: Language Identification, Misidentification, and Translation using Hierarchical Models in 350+ Languages](https://doi.org/10.18653/v1/2023.emnlp-main.895) |  | 0 | Knowing the language of an input text/audio is a necessary first step for using almost every NLP tool such as taggers, parsers, or translation systems. Language identification is a well-studied problem, sometimes even considered solved; in reality, due to lack of data and computational challenges, current systems cannot accurately identify most of the world’s 7000 languages. To tackle this bottleneck, we first compile a corpus, MCS-350, of 50K multilingual and parallel children’s stories in... | Milind Agarwal, Md Mahfuz Ibn Alam, Antonios Anastasopoulos |  |
| 2090 |  |  [FreeAL: Towards Human-Free Active Learning in the Era of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.896) |  | 0 | Collecting high-quality labeled data for model training is notoriously time-consuming and labor-intensive for various NLP tasks. While copious solutions, such as active learning for small language models (SLMs) and prevalent in-context learning in the era of large language models (LLMs), have been proposed and alleviate the labeling burden to some extent, their performances are still subject to human intervention. It is still underexplored how to reduce the annotation cost in the LLMs era. To... | Ruixuan Xiao, Yiwen Dong, Junbo Zhao, Runze Wu, Minmin Lin, Gang Chen, Haobo Wang |  |
| 2091 |  |  [API-Assisted Code Generation for Question Answering on Varied Table Structures](https://doi.org/10.18653/v1/2023.emnlp-main.897) |  | 0 | A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation for structured tables as multi-index Pandas data frames, (2) uses Python as a powerful querying language, and (3) uses few-shot prompting to translate NL questions into Python programs, which are... | Yihan Cao, Shuyi Chen, Ryan Liu, Zhiruo Wang, Daniel Fried |  |
| 2092 |  |  [Data Factors for Better Compositional Generalization](https://doi.org/10.18653/v1/2023.emnlp-main.898) |  | 0 | Recent diagnostic datasets on compositional generalization, such as SCAN (Lake and Baroni, 2018) and COGS (Kim and Linzen, 2020), expose severe problems in models trained from scratch on these datasets. However, in contrast to this poor performance, state-of-the-art models trained on larger and more general datasets show better generalization ability. In this work, to reconcile this inconsistency, we conduct an empirical analysis by training Transformer models on a variety of training sets with... | Xiang Zhou, Yichen Jiang, Mohit Bansal |  |
| 2093 |  |  [ChatEdit: Towards Multi-turn Interactive Facial Image Editing via Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.899) |  | 0 | This paper explores interactive facial image editing through dialogue and presents the ChatEdit benchmark dataset for evaluating image editing and conversation abilities in this context. ChatEdit is constructed from the CelebA-HQ dataset, incorporating annotated multi-turn dialogues corresponding to user editing requests on the images. The dataset is challenging, as it requires the system to dynamically track and edit images based on user requests, while generating appropriate natural language... | Xing Cui, Zekun Li, Pei Li, Yibo Hu, Hailin Shi, Chunshui Cao, Zhaofeng He |  |
| 2094 |  |  [Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations](https://doi.org/10.18653/v1/2023.emnlp-main.900) |  | 0 | Traditional sentence embedding models encode sentences into vector representations to capture useful properties such as the semantic similarity between sentences. However, in addition to similarity, sentence semantics can also be interpreted via compositional operations such as sentence fusion or difference. It is unclear whether the compositional semantics of sentences can be directly reflected as compositional operations in the embedding space. To more effectively bridge the continuous... | James Y. Huang, Wenlin Yao, Kaiqiang Song, Hongming Zhang, Muhao Chen, Dong Yu |  |
| 2095 |  |  [Outlier Dimensions Encode Task Specific Knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.901) |  | 0 | Representations from large language models (LLMs) are known to be dominated by a small subset of dimensions with exceedingly high variance. Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings. In this study, we investigate how fine-tuning impacts outlier dimensions and show that 1) outlier dimensions that occur in pre-training persist in... | William Rudman, Catherine Chen, Carsten Eickhoff |  |
| 2096 |  |  [Hi-ArG: Exploring the Integration of Hierarchical Argumentation Graphs in Language Pretraining](https://doi.org/10.18653/v1/2023.emnlp-main.902) |  | 0 | The knowledge graph is a structure to store and represent knowledge, and recent studies have discussed its capability to assist language models for various applications. Some variations of knowledge graphs aim to record arguments and their relations for computational argumentation tasks. However, many must simplify semantic types to fit specific schemas, thus losing flexibility and expression ability. In this paper, we propose the \*\*Hi\*\*erarchical \*\*Ar\*\*gumentation \*\*G\*\*raph... | Jingcong Liang, Rong Ye, Meng Han, Qi Zhang, Ruofei Lai, Xinyu Zhang, Zhao Cao, Xuanjing Huang, Zhongyu Wei |  |
| 2097 |  |  [Biomedical Named Entity Recognition via Dictionary-based Synonym Generalization](https://doi.org/10.18653/v1/2023.emnlp-main.903) |  | 0 | Biomedical named entity recognition is one of the core tasks in biomedical natural language processing (BioNLP). To tackle this task, numerous supervised/distantly supervised approaches have been proposed. Despite their remarkable success, these approaches inescapably demand laborious human effort. To alleviate the need of human effort, dictionary-based approaches have been proposed to extract named entities simply based on a given dictionary. However, one downside of existing dictionary-based... | Zihao Fu, Yixuan Su, Zaiqiao Meng, Nigel Collier |  |
| 2098 |  |  [GNAT: A General Narrative Alignment Tool](https://doi.org/10.18653/v1/2023.emnlp-main.904) |  | 0 | Algorithmic sequence alignment identifies similar segments shared between pairs of documents, and is fundamental to many NLP tasks. But it is difficult to recognize similarities between distant versions of narratives such as translations and retellings, particularly for summaries and abridgements which are much shorter than the original novels. We develop a general approach to narrative alignment coupling the Smith-Waterman algorithm from bioinformatics with modern text similarity metrics. We... | Tanzir Pial, Steven Skiena |  |
| 2099 |  |  [Self-Ensemble of N-best Generation Hypotheses by Lexically Constrained Decoding](https://doi.org/10.18653/v1/2023.emnlp-main.905) |  | 0 | We propose a method that ensembles N-best hypotheses to improve natural language generation. Previous studies have achieved notable improvements in generation quality by explicitly reranking N-best candidates. These studies assume that there exists a hypothesis of higher quality. We expand the assumption to be more practical as there exist partly higher quality hypotheses in the N-best yet they may be imperfect as the entire sentences. By merging these high-quality fragments, we can obtain a... | Ryota Miyano, Tomoyuki Kajiwara, Yuki Arase |  |
| 2100 |  |  [UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.906) |  | 0 | Charts are widely used for data analysis, providing visual representations and insights into complex data. To facilitate chart-based data analysis using natural language, several downstream tasks have been introduced recently such as chart question answering and chart summarization. However, existing methods for these tasks often rely on pretraining on language or vision-language tasks, neglecting the explicit modeling of chart structures (e.g., how chart elements are related to each other). To... | Ahmed Masry, Parsa Kavehzadeh, Do Xuan Long, Enamul Hoque, Shafiq Joty |  |
| 2101 |  |  [Merging Experts into One: Improving Computational Efficiency of Mixture of Experts](https://doi.org/10.18653/v1/2023.emnlp-main.907) |  | 0 | Scaling the size of language models usually leads to remarkable advancements in NLP tasks. But it often comes with a price of growing computational cost. Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a small subset of parameters (e.g., one expert) for each input, its computation escalates significantly if increasing the number of activated experts, limiting its practical utility. Can we retain the advantages of adding more experts without substantially increasing... | Shwai He, RunZe Fan, Liang Ding, Li Shen, Tianyi Zhou, Dacheng Tao |  |
| 2102 |  |  [Distance-Based Propagation for Efficient Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.908) |  | 0 | Knowledge graph completion (KGC) aims to predict unseen edges in knowledge graphs (KGs), resulting in the discovery of new facts. A new class of methods have been proposed to tackle this problem by aggregating path information. These methods have shown tremendous ability in the task of KGC. However they are plagued by efficiency issues. Though there are a few recent attempts to address this through learnable path pruning, they often sacrifice the performance to gain efficiency. In this work, we... | Harry Shomer, Yao Ma, Juanhui Li, Bo Wu, Charu C. Aggarwal, Jiliang Tang |  |
| 2103 |  |  [What to Read in a Contract? Party-Specific Summarization of Legal Obligations, Entitlements, and Prohibitions](https://doi.org/10.18653/v1/2023.emnlp-main.909) |  | 0 | Reviewing and comprehending key obligations, entitlements, and prohibitions in legal contracts can be a tedious task due to their length and domain-specificity. Furthermore, the key rights and duties requiring review vary for each contracting party. In this work, we propose a new task of party-specific extractive summarization for legal contracts to facilitate faster reviewing and improved comprehension of rights and duties. To facilitate this, we curate a dataset comprising of party-specific... | Abhilasha Sancheti, Aparna Garimella, Balaji Vasan Srinivasan, Rachel Rudinger |  |
| 2104 |  |  [Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization](https://doi.org/10.18653/v1/2023.emnlp-main.910) |  | 0 | Large Language Models (LLMs) are proficient in natural language processing tasks, but their deployment is often restricted by extensive parameter sizes and computational demands. This paper focuses on post-training quantization (PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8) quantization, to enhance computational efficiency—a topic less explored compared to weight-only quantization. We present two innovative techniques: activation-quantization-aware scaling (AQAS) and... | Janghwan Lee, Minsoo Kim, Seungcheol Baek, Seok Joong Hwang, Wonyong Sung, Jungwook Choi |  |
| 2105 |  |  [CP-BCS: Binary Code Summarization Guided by Control Flow Graph and Pseudo Code](https://doi.org/10.18653/v1/2023.emnlp-main.911) |  | 0 | Automatically generating function summaries for binaries is an extremely valuable but challenging task, since it involves translating the execution behavior and semantics of the low-level language (assembly code) into human-readable natural language. However, most current works on understanding assembly code are oriented towards generating function names, which involve numerous abbreviations that make them still confusing. To bridge this gap, we focus on generating complete summaries for binary... | Tong Ye, Lingfei Wu, Tengfei Ma, Xuhong Zhang, Yangkai Du, Peiyu Liu, Shouling Ji, Wenhai Wang |  |
| 2106 |  |  [Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism](https://doi.org/10.18653/v1/2023.emnlp-main.912) |  | 0 | Large language models (LLMs) take advantage of step-by-step reasoning instructions, e.g., chain-of-thought (CoT) prompting. Building on this, their ability to perform CoT-style reasoning robustly is of interest from a probing perspective. In this study, we inspect the step-by-step reasoning ability of LLMs with a focus on negation, which is a core linguistic phenomenon that is difficult to process. In particular, we introduce several controlled settings (e.g., reasoning in case of fictional... | Mengyu Ye, Tatsuki Kuribayashi, Jun Suzuki, Goro Kobayashi, Hiroaki Funayama |  |
| 2107 |  |  [Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.913) |  | 0 | Chain-of-Thought (CoT) is a technique that guides Large Language Models (LLMs) to decompose complex tasks into multi-step reasoning through intermediate steps in natural language form. Briefly, CoT enables LLMs to think step by step. However, although many Natural Language Understanding (NLU) tasks also require thinking step by step, LLMs perform less well than small-scale Masked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose Chain-of-Thought Tuning (CoTT), a two-step... | Caoyun Fan, Jidong Tian, Yitian Li, Wenqing Chen, Hao He, Yaohui Jin |  |
| 2108 |  |  [Large Language Models are Complex Table Parsers](https://doi.org/10.18653/v1/2023.emnlp-main.914) |  | 0 | With the Generative Pre-trained Transformer 3.5 (GPT-3.5) exhibiting remarkable reasoning and comprehension abilities in Natural Language Processing (NLP), most Question Answering (QA) research has primarily centered around general QA tasks based on GPT, neglecting the specific challenges posed by Complex Table QA. In this paper, we propose to incorporate GPT-3.5 to address such challenges, in which complex tables are reconstructed into tuples and specific prompt designs are employed for... | Bowen Zhao, Changkai Ji, Yuejie Zhang, Wen He, Yingwen Wang, Qing Wang, Rui Feng, Xiaobo Zhang |  |
| 2109 |  |  [R2H: Building Multimodal Navigation Helpers that Respond to Help Requests](https://doi.org/10.18653/v1/2023.emnlp-main.915) |  | 0 | Intelligent navigation-helper agents are critical as they can navigate users in unknown areas through environmental awareness and conversational ability, serving as potential accessibility tools for individuals with disabilities. In this work, we first introduce a novel benchmark, Respond to Help Requests (R2H), to promote the development of multi-modal navigation helpers capable of responding to requests for help, utilizing existing dialog-based embodied datasets. R2H mainly includes two... | Yue Fan, Jing Gu, Kaizhi Zheng, Xin Wang |  |
| 2110 |  |  [Speech-enriched Memory for Inference-time Adaptation of ASR Models to Word Dictionaries](https://doi.org/10.18653/v1/2023.emnlp-main.916) |  | 0 | Despite the impressive performance of ASR models on mainstream benchmarks, their performance on rare words is unsatisfactory. In enterprise settings, often a focused list of entities (such as locations, names, etc) are available which can be used to adapt the model to the terminology of specific domains. In this paper, we present a novel inference algorithm that improves the prediction of state-of-the-art ASR models using nearest-neighbor-based matching on an inference-time word list. We... | Ashish R. Mittal, Sunita Sarawagi, Preethi Jyothi, George Saon, Gakuto Kurata |  |
| 2111 |  |  [Generative Table Pre-training Empowers Models for Tabular Prediction](https://doi.org/10.18653/v1/2023.emnlp-main.917) |  | 0 | Recently, the topic of table pre-training has attracted considerable research interest. However, how to employ table pre-training to boost the performance of tabular prediction remains an open challenge. In this paper, we propose TapTap, the first attempt that leverages table pre-training to empower models for tabular prediction. After pre-training on a large corpus of real-world tabular data, TapTap can generate high-quality synthetic tables to support various applications on tabular data,... | Tianping Zhang, Shaowen Wang, Shuicheng Yan, Li Jian, Qian Liu |  |
| 2112 |  |  [Learning to Describe for Predicting Zero-shot Drug-Drug Interactions](https://doi.org/10.18653/v1/2023.emnlp-main.918) |  | 0 | Adverse drug-drug interactions (DDIs) can compromise the effectiveness of concurrent drug administration, posing a significant challenge in healthcare. As the development of new drugs continues, the potential for unknown adverse effects resulting from DDIs becomes a growing concern. Traditional computational methods for DDI prediction may fail to capture interactions for new drugs due to the lack of knowledge. In this paper, we introduce a new problem setup as zero-shot DDI prediction that... | Fangqi Zhu, Yongqi Zhang, Lei Chen, Bing Qin, Ruifeng Xu |  |
| 2113 |  |  [A Simple Baseline for Knowledge-Based Visual Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.919) |  | 0 | This paper is on the problem of Knowledge-Based Visual Question Answering (KB-VQA). Recent works have emphasized the significance of incorporating both explicit (through external databases) and implicit (through LLMs) knowledge to answer questions requiring external knowledge effectively. A common limitation of such approaches is that they consist of relatively complicated pipelines and often heavily rely on accessing GPT-3 API. Our main contribution in this paper is to propose a much simpler... | Alexandros Xenos, Themos Stafylakis, Ioannis Patras, Georgios Tzimiropoulos |  |
| 2114 |  |  [Unveiling the Essence of Poetry: Introducing a Comprehensive Dataset and Benchmark for Poem Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.920) |  | 0 | While research in natural language processing has progressed significantly in creative language generation, the question of whether language models can interpret the intended meaning of creative language largely remains unanswered. Poetry as a creative art form has existed for generations, and summarization of such content requires deciphering the figurative patterns to find out the actual intent and message of the poet. This task can provide the researchers an opportunity to evaluate the... | Ridwan Mahbub, Ifrad Khan, Samiha Anuva, Md Shahriar, Md. Tahmid Rahman Laskar, Sabbir Ahmed |  |
| 2115 |  |  [Privacy Implications of Retrieval-Based Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.921) |  | 0 | Retrieval-based language models (LMs) have demonstrated improved interpretability, factuality, and adaptability compared to their parametric counterparts by incorporating retrieved text from external datastores. While it is well known that parametric models are prone to leaking private data, it remains unclear how the addition of a retrieval datastore impacts model privacy. In this work, we present the first study of privacy risks in retrieval-based LMs, particularly kNN-LMs. Our goal is to... | Yangsibo Huang, Samyak Gupta, Zexuan Zhong, Kai Li, Danqi Chen |  |
| 2116 |  |  [IMTLab: An Open-Source Platform for Building, Evaluating, and Diagnosing Interactive Machine Translation Systems](https://doi.org/10.18653/v1/2023.emnlp-main.922) |  | 0 | We present IMTLab, an open-source end-to-end interactive machine translation (IMT) system platform that enables researchers to quickly build IMT systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. IMTLab treats the whole interactive translation process as a task-oriented dialogue with a human-in-the-loop setting, in which human interventions can be explicitly incorporated to produce high-quality, error-free translations. To this end, a... | Xu Huang, Zhirui Zhang, Ruize Gao, Yichao Du, Lemao Liu, Guoping Huang, Shuming Shi, Jiajun Chen, Shujian Huang |  |
| 2117 |  |  [Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents](https://doi.org/10.18653/v1/2023.emnlp-main.923) |  | 0 | Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR.... | Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren |  |
| 2118 |  |  [DiNeR: A Large Realistic Dataset for Evaluating Compositional Generalization](https://doi.org/10.18653/v1/2023.emnlp-main.924) |  | 0 | Most of the existing compositional generalization datasets are synthetically-generated, resulting in a lack of natural language variation. While there have been recent attempts to introduce non-synthetic datasets for compositional generalization, they suffer from either limited data scale or a lack of diversity in the forms of combinations. To better investigate compositional generalization with more linguistic phenomena and compositional diversity, we propose the DIsh NamE Recognition (DiNeR)... | Chengang Hu, Xiao Liu, Yansong Feng |  |
| 2119 |  |  [Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?](https://doi.org/10.18653/v1/2023.emnlp-main.925) |  | 0 | Pre-trained vision and language models have demonstrated state-of-the-art capabilities over existing tasks involving images and texts, including visual question answering. However, it remains unclear whether these models possess the capability to answer questions that are not only querying visual content but knowledge-intensive and information-seeking. In this study, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered... | Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, MingWei Chang |  |
| 2120 |  |  [EDeR: Towards Understanding Dependency Relations Between Events](https://doi.org/10.18653/v1/2023.emnlp-main.926) |  | 0 | Relation extraction is a crucial task in natural language processing (NLP) and information retrieval (IR). Previous work on event relation extraction mainly focuses on hierarchical, temporal and causal relations. Such relationships consider two events to be independent in terms of syntax and semantics, but they fail to recognize the interdependence between events. To bridge this gap, we introduce a human-annotated Event Dependency Relation dataset (EDeR). The annotation is done on a sample of... | Ruiqi Li, Patrik Haslum, Leyang Cui |  |
| 2121 |  |  [It Ain't Over: A Multi-aspect Diverse Math Word Problem Dataset](https://doi.org/10.18653/v1/2023.emnlp-main.927) |  | 0 | The math word problem (MWP) is a complex task that requires natural language understanding and logical reasoning to extract key knowledge from natural language narratives. Previous studies have provided various MWP datasets but lack diversity in problem types, lexical usage patterns, languages, and annotations for intermediate solutions. To address these limitations, we introduce a new MWP dataset, named DMath (Diverse Math Word Problems), offering a wide range of diversity in problem types,... | Jiwoo Kim, Youngbin Kim, Ilwoong Baek, JinYeong Bak, Jongwuk Lee |  |
| 2122 |  |  [Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness](https://doi.org/10.18653/v1/2023.emnlp-main.928) |  | 0 | This paper investigates the significant impact different prompts have on the behaviour of ChatGPT when used for health information seeking. As people more and more depend on generative large language models (LLMs) like ChatGPT, it is critical to understand model behaviour under different conditions, especially for domains where incorrect answers can have serious consequences such as health. Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness... | Bevan Koopman, Guido Zuccon |  |
| 2123 |  |  [kNN-LM Does Not Improve Open-ended Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.929) |  | 0 | In this paper, we study the generation quality of interpolation-based retrieval-augmented language models (LMs). These methods, best exemplified by the kNN-LM, interpolate the LM’s predicted distribution of the next word with a distribution formed from the most relevant retrievals for a given prefix. While the kNN-LM and related methods yield impressive decreases in perplexity, we discover that they do not exhibit corresponding improvements in open-ended generation quality, as measured by both... | Shufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun Manjunatha, Mohit Iyyer |  |
| 2124 |  |  [Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model](https://doi.org/10.18653/v1/2023.emnlp-main.930) |  | 0 | Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for pretraining large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while keeping training and inference costs (in FLOPs) fixed. In this work, we analyzed two major design choices of S-FFN: the memory block (a.k.a. expert) size and the memory block selection method under a... | Zeyu Liu, Tim Dettmers, Xi Lin, Veselin Stoyanov, Xian Li |  |
| 2125 |  |  [Exploring the Impact of Model Scaling on Parameter-Efficient Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.931) |  | 0 | Parameter-efficient tuning (PET) methods can effectively drive extremely large pre-trained language models (PLMs) by training only minimal parameters. Different PET methods utilize different manually designed tunable modules. In small PLMs, there are usually noticeable performance differences among PET methods. Nevertheless, as the model scale increases, the performance differences become marginal. Hence, we hypothesize that model scaling mitigates the impact of design differences on PET... | Yusheng Su, ChiMin Chan, Jiali Cheng, Yujia Qin, Yankai Lin, Shengding Hu, Zonghan Yang, Ning Ding, Xingzhi Sun, Guotong Xie, Zhiyuan Liu, Maosong Sun |  |
| 2126 |  |  [STAIR: Learning Sparse Text and Image Representation in Grounded Tokens](https://doi.org/10.18653/v1/2023.emnlp-main.932) |  | 0 | Image and text retrieval is one of the foundational tasks in the vision and language domain with multiple real-world applications. State-of-the-art contrastive approaches, e.g. CLIP, ALIGN, represent images and texts as dense embeddings and calculate the similarity in the dense embedding space as the matching score. On the other hand, sparse semantic features like bag-of-words models are more interpretable, but believed to suffer from inferior accuracy than dense representations. In this work,... | Chen Chen, Bowen Zhang, Liangliang Cao, Jiguang Shen, Tom Gunter, Albin Madappally Jose, Alexander Toshev, Yantao Zheng, Ruoming Pang, Yinfei Yang |  |
| 2127 |  |  [Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting](https://doi.org/10.18653/v1/2023.emnlp-main.933) |  | 0 | Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization of idiomatic translation and related issues. This allows us to conduct a synthetic experiment revealing a tipping point at which transformer-based machine translation models correctly default to... | Emmy Liu, Aditi Chaudhary, Graham Neubig |  |
| 2128 |  |  [CoRec: An Easy Approach for Coordination Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.934) |  | 0 | In this paper, we observe and address the challenges of the coordination recognition task. Most existing methods rely on syntactic parsers to identify the coordinators in a sentence and detect the coordination boundaries. However, state-of-the-art syntactic parsers are slow and suffer from errors, especially for long and complicated sentences. To better solve the problems, we propose a pipeline model COordination RECognizer (CoRec). It consists of two components: coordinator identifier and... | Qing Wang, Haojie Jia, Wenfei Song, Qi Li |  |
| 2129 |  |  [A linear time approximation of Wasserstein distance with word embedding selection](https://doi.org/10.18653/v1/2023.emnlp-main.935) |  | 0 | Wasserstein distance, which can be computed by solving the optimal transport problem, is a powerful method for measuring the dissimilarity between documents. In the NLP community, it is referred to as word mover’s distance (WMD). One of the key challenges of Wasserstein distance is its computational cost since it needs cubic time. Although the Sinkhorn algorithm is a powerful tool to speed up to compute the Wasserstein distance, it still requires square time. Recently, a linear time... | Sho Otao, Makoto Yamada |  |
| 2130 |  |  [Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication](https://doi.org/10.18653/v1/2023.emnlp-main.936) |  | 0 | Large Language Models (LLMs) have recently made significant strides in complex reasoning tasks through the Chain-of-Thought technique. Despite this progress, their reasoning is often constrained by their intrinsic understanding, lacking external insights. To address this, we propose Exchange-of-Thought (EoT), a novel framework that enables cross-model communication during problem-solving. Drawing inspiration from network topology, EoT integrates four unique communication paradigms: Memory,... | Zhangyue Yin, Qiushi Sun, Cheng Chang, Qipeng Guo, Junqi Dai, Xuanjing Huang, Xipeng Qiu |  |
| 2131 |  |  [Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction](https://doi.org/10.18653/v1/2023.emnlp-main.937) |  | 0 | Emotion recognition is a crucial task for human conversation understanding. It becomes more challenging with the notion of multimodal data, e.g., language, voice, and facial expressions. As a typical solution, the global- and the local context information are exploited to predict the emotional label for every single sentence, i.e., utterance, in the dialogue. Specifically, the global representation could be captured via modeling of cross-modal interactions at the conversation level. The local... | CamVan Thi Nguyen, AnhTuan Mai, TheSon Le, HaiDang Kieu, DucTrong Le |  |
| 2132 |  |  [Connecting degree and polarity: An artificial language learning study](https://doi.org/10.18653/v1/2023.emnlp-main.938) |  | 0 | We investigate a new linguistic generalisation in pre-trained language models (taking BERT Devlin et al. 2019 as a case study). We focus on degree modifiers (expressions like slightly, very, rather, extremely) and test the hypothesis that the degree expressed by a modifier (low, medium or high degree) is related to the modifier’s sensitivity to sentence polarity (whether it shows preference for affirmative or negative sentences or neither). To probe this connection, we apply the Artificial... | Lisa Bylinina, Alexey Tikhonov, Ekaterina Garmash |  |
| 2133 |  |  [Prompting with Pseudo-Code Instructions](https://doi.org/10.18653/v1/2023.emnlp-main.939) |  | 0 | Prompting with natural language instructions has recently emerged as a popular method of harnessing the capabilities of large language models (LLM). Given the inherent ambiguity present in natural language, it is intuitive to consider the possible advantages of prompting with less ambiguous prompt styles, like pseudo-code. In this paper, we explore if prompting via pseudo-code instructions helps improve the performance of pre-trained language models. We manually create a dataset of pseudo-code... | Mayank Mishra, Prince Kumar, Riyaz A. Bhat, Rudra Murthy V, Danish Contractor, Srikanth Tamilselvam |  |
| 2134 |  |  [CRAB: Assessing the Strength of Causal Relationships Between Real-world Events](https://doi.org/10.18653/v1/2023.emnlp-main.940) |  | 0 | Understanding narratives requires reasoning about the cause-and-effect relationships between events mentioned in the text. While existing foundation models yield impressive results in many NLP tasks requiring reasoning, it is unclear whether they understand the complexity of the underlying network of causal relationships of events in narratives. In this work, we present CRAB, a new Causal Reasoning Assessment Benchmark designed to evaluate causal understanding of events in real-world... | Angelika Romanou, Syrielle Montariol, Debjit Paul, Léo Laugier, Karl Aberer, Antoine Bosselut |  |
| 2135 |  |  [NORMSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly](https://doi.org/10.18653/v1/2023.emnlp-main.941) |  | 0 | Knowledge of norms is needed to understand and reason about acceptable behavior in human communication and interactions across sociocultural scenarios. Most computational research on norms has focused on a single culture, and manually built datasets, from non-conversational settings. We address these limitations by proposing a new framework, NormSage, to automatically extract culture-specific norms from multi-lingual conversations. NormSage uses GPT-3 prompting to 1) extract candidate norms... | Yi Fung, Tuhin Chakrabarty, Hao Guo, Owen Rambow, Smaranda Muresan, Heng Ji |  |
| 2136 |  |  [A State-Vector Framework for Dataset Effects](https://doi.org/10.18653/v1/2023.emnlp-main.942) |  | 0 | The impressive success of recent deep neural network (DNN)-based systems is significantly influenced by the high-quality datasets used in training. However, the effects of the datasets, especially how they interact with each other, remain underexplored. We propose a state-vector framework to enable rigorous studies in this direction. This framework uses idealized probing test results as the bases of a vector space. This framework allows us to quantify the effects of both standalone and... | Esmat Sahak, Zining Zhu, Frank Rudzicz |  |
| 2137 |  |  [Challenges in Context-Aware Neural Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.943) |  | 0 | Context-aware neural machine translation, a paradigm that involves leveraging information beyond sentence-level context to resolve inter-sentential discourse dependencies and improve document-level translation quality, has given rise to a number of recent techniques. However, despite well-reasoned intuitions, most context-aware translation models show only modest improvements over sentence-level systems. In this work, we investigate and present several core challenges that impede progress... | Linghao Jin, Jacqueline He, Jonathan May, Xuezhe Ma |  |
| 2138 |  |  [Task-Adaptive Tokenization: Enhancing Long-Form Text Generation Efficacy in Mental Health and Beyond](https://doi.org/10.18653/v1/2023.emnlp-main.944) |  | 0 | We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the... | Siyang Liu, Naihao Deng, Sahand Sabour, Yilin Jia, Minlie Huang, Rada Mihalcea |  |
| 2139 |  |  [FACTIFY3M: A benchmark for multimodal fact verification with explainability through 5W Question-Answering](https://doi.org/10.18653/v1/2023.emnlp-main.945) |  | 0 | Combating disinformation is one of the burning societal crises - about 67% of the American population believes that disinformation produces a lot of uncertainty, and 10% of them knowingly propagate disinformation. Evidence shows that disinformation can manipulate democratic processes and public opinion, causing disruption in the share market, panic and anxiety in society, and even death during crises. Therefore, disinformation should be identified promptly and, if possible, mitigated. With... | Megha Chakraborty, Khushbu Pahwa, Anku Rani, Shreyas Chatterjee, Dwip Dalal, Harshit Dave, Ritvik G, Preethi Gurumurthy, Adarsh Mahor, Samahriti Mukherjee, Aditya Pakala, Ishan Paul, Janvita Reddy, Arghya Sarkar, Kinjal Sensharma, Aman Chadha, Amit P. Sheth, Amitava Das |  |
| 2140 |  |  [Building Multi-domain Dialog State Trackers from Single-domain Dialogs](https://doi.org/10.18653/v1/2023.emnlp-main.946) |  | 0 | Existing multi-domain dialog state tracking (DST) models are developed based on multi-domain dialogs, which require significant manual effort to define domain relations and collect data. This process can be challenging and expensive, particularly when numerous domains are involved. In this paper, we propose a divide-and-conquer (DAC) DST paradigm and a multi-domain dialog synthesis framework, which makes building multi-domain DST models from single-domain dialogs possible. The DAC paradigm... | Qi Zhu, Zheng Zhang, Xiaoyan Zhu, Minlie Huang |  |
| 2141 |  |  [Specialist or Generalist? Instruction Tuning for Specific NLP Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.947) |  | 0 | The potential of large language models (LLMs) to simultaneously perform a wide range of natural language processing (NLP) tasks has been the subject of extensive research. Although instruction tuning has proven to be a data-efficient method for transforming LLMs into such generalist models, their performance still lags behind specialist models trained exclusively for specific tasks. In this paper, we investigate whether incorporating broadcoverage generalist instruction tuning can contribute to... | Chufan Shi, Yixuan Su, Cheng Yang, Yujiu Yang, Deng Cai |  |
| 2142 |  |  [Making Large Language Models Better Data Creators](https://doi.org/10.18653/v1/2023.emnlp-main.948) |  | 0 | Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the preferred option in some cases. However, these models still require human-labeled data for optimal performance, which is expensive and time-consuming to obtain. In order to address this issue, several techniques to reduce... | DongHo Lee, Jay Pujara, Mohit Sewak, Ryen White, Sujay Kumar Jauhar |  |
| 2143 |  |  [Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation](https://doi.org/10.18653/v1/2023.emnlp-main.949) |  | 0 | Large Language Models (LLMs) have made remarkable advancements in the field of natural language generation. However, the propensity of LLMs to generate inaccurate or non-factual content, termed “hallucinations”, remains a significant challenge. Current hallucination detection methods often necessitate the retrieval of great numbers of relevant evidence, thereby increasing response times. We introduce a unique framework that leverages statistical decision theory and Bayesian sequential analysis... | Xiaohua Wang, Yuliang Yan, Longtao Huang, Xiaoqing Zheng, Xuanjing Huang |  |
| 2144 |  |  [Guideline Learning for In-Context Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.950) |  | 0 | Large language models (LLMs) can perform a new task by merely conditioning on task instructions and a few input-output examples, without optimizing any parameters. This is called In-Context Learning (ICL). In-context Information Extraction (IE) has recently garnered attention in the research community. However, the performance of In-context IE generally lags behind the state-of-the-art supervised expert models. We highlight a key reason for this shortfall: underspecified task description. The... | Chaoxu Pang, Yixuan Cao, Qiang Ding, Ping Luo |  |
| 2145 |  |  [Open Information Extraction via Chunks](https://doi.org/10.18653/v1/2023.emnlp-main.951) |  | 0 | Open Information Extraction (OIE) aims to extract relational tuples from open-domain sentences. Existing OIE systems split a sentence into tokens and recognize token spans as tuple relations and arguments. We instead propose Sentence as Chunk sequence (SaC) and recognize chunk spans as tuple relations and arguments. We argue that SaC has better properties for OIE than sentence as token sequence, and evaluate four choices of chunks (i.e., CoNLL chunks, OIA simple phrases, noun phrases, and spans... | Kuicai Dong, Aixin Sun, JungJae Kim, Xiaoli Li |  |
| 2146 |  |  [Rethinking Word-Level Auto-Completion in Computer-Aided Translation](https://doi.org/10.18653/v1/2023.emnlp-main.952) |  | 0 | Word-level auto-completion (WLAC) plays a crucial role in Computer-Assisted Translation. While previous studies have primarily focused on designing complex model architectures, this paper takes a different perspective by rethinking the fundamental question: what kind of words are good auto-completions? We introduce a measurable criterion to address this question and discover that existing WLAC models often fail to meet this criterion. Building upon this observation, we propose an effective... | Xingyu Chen, Lemao Liu, Guoping Huang, Zhirui Zhang, Mingming Yang, Shuming Shi, Rui Wang |  |
| 2147 |  |  [Automatic Transcription of Handwritten Old Occitan Language](https://doi.org/10.18653/v1/2023.emnlp-main.953) |  | 0 | While existing neural network-based approaches have shown promising results in Handwritten Text Recognition (HTR) for high-resource languages and standardized/machine-written text, their application to low-resource languages often presents challenges, resulting in reduced effectiveness. In this paper, we propose an innovative HTR approach that leverages the Transformer architecture for recognizing handwritten Old Occitan language. Given the limited availability of data, which comprises only... | Esteban Garces Arias, Vallari Pai, Matthias Schöffel, Christian Heumann, Matthias Aßenmacher |  |
| 2148 |  |  [CorefPrompt: Prompt-based Event Coreference Resolution by Measuring Event Type and Argument Compatibilities](https://doi.org/10.18653/v1/2023.emnlp-main.954) |  | 0 | Event coreference resolution (ECR) aims to group event mentions referring to the same real-world event into clusters. Most previous studies adopt the “encoding first, then scoring” framework, making the coreference judgment rely on event encoding. Furthermore, current methods struggle to leverage human-summarized ECR rules, e.g., coreferential events should have the same event type, to guide the model. To address these two issues, we propose a prompt-based approach, CorefPrompt, to transform... | Sheng Xu, Peifeng Li, Qiaoming Zhu |  |
| 2149 |  |  [Anaphor Assisted Document-Level Relation Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.955) |  | 0 | Document-level relation extraction (DocRE) involves identifying relations between entities distributed in multiple sentences within a document. Existing methods focus on building a heterogeneous document graph to model the internal structure of an entity and the external interaction between entities. However, there are two drawbacks in existing methods. On one hand, anaphor plays an important role in reasoning to identify relations between entities but is ignored by these methods. On the other... | Chonggang Lu, Richong Zhang, Kai Sun, Jaein Kim, Cunwang Zhang, Yongyi Mao |  |
| 2150 |  |  [FinEntity: Entity-level Sentiment Classification for Financial Texts](https://doi.org/10.18653/v1/2023.emnlp-main.956) |  | 0 | In the financial domain, conducting entity-level sentiment analysis is crucial for accurately assessing the sentiment directed toward a specific financial entity. To our knowledge, no publicly available dataset currently exists for this purpose. In this work, we introduce an entity-level sentiment classification dataset, called FinEntity, that annotates financial entity spans and their sentiment (positive, neutral, and negative) in financial news. We document the dataset construction process in... | Yixuan Tang, Yi Yang, Allen Huang, Andy Tam, Justin Z. Tang |  |
| 2151 |  |  [All Things Considered: Detecting Partisan Events from News Media with Cross-Article Comparison](https://doi.org/10.18653/v1/2023.emnlp-main.957) |  | 0 | Public opinion is shaped by the information news media provide, and that information in turn may be shaped by the ideological preferences of media outlets. But while much attention has been devoted to media bias via overt ideological language or topic selection, a more unobtrusive way in which the media shape opinion is via the strategic inclusion or omission of partisan events that may support one side or the other. We develop a latent variable-based framework to predict the ideology of news... | Yujian Liu, Xinliang Frederick Zhang, Kaijian Zou, Ruihong Huang, Nicholas Beauchamp, Lu Wang |  |
| 2152 |  |  [Rationale-Enhanced Language Models are Better Continual Relation Learners](https://doi.org/10.18653/v1/2023.emnlp-main.958) |  | 0 | Continual relation extraction (CRE) aims to solve the problem of catastrophic forgetting when learning a sequence of newly emerging relations. Recent CRE studies have found that catastrophic forgetting arises from the model’s lack of robustness against future analogous relations. To address the issue, we introduce rationale, i.e., the explanations of relation classification results generated by Large Language Models (LLM), into CRE task. Specifically, we design the multi-task rationale tuning... | Weimin Xiong, Yifan Song, Peiyi Wang, Sujian Li |  |
| 2153 |  |  [BanglaAbuseMeme: A Dataset for Bengali Abusive Meme Classification](https://doi.org/10.18653/v1/2023.emnlp-main.959) |  | 0 | The dramatic increase in the use of social media platforms for information sharing has also fueled a steep growth in online abuse. A simple yet effective way of abusing individuals or communities is by creating memes, which often integrate an image with a short piece of text layered on top of it. Such harmful elements are in rampant use and are a threat to online safety. Hence it is necessary to develop efficient models to detect and flag abusive memes. The problem becomes more challenging in a... | Mithun Das, Animesh Mukherjee |  |
| 2154 |  |  [ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts](https://doi.org/10.18653/v1/2023.emnlp-main.960) |  | 0 | Eye movements in reading play a crucial role in psycholinguistic research studying the cognitive mechanisms underlying human language processing. More recently, the tight coupling between eye movements and cognition has also been leveraged for language-related machine learning tasks such as the interpretability, enhancement, and pre-training of language models, as well as the inference of reader- and text-specific properties. However, scarcity of eye movement data and its unavailability at... | Lena S. Bolliger, David R. Reich, Patrick Haller, Deborah N. Jakobi, Paul Prasse, Lena A. Jäger |  |
| 2155 |  |  [From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.961) |  | 0 | Being able to predict people’s opinions on issues and behaviors in realistic scenarios can be helpful in various domains, such as politics and marketing. However, conducting large-scale surveys like the European Social Survey to solicit people’s opinions on individual issues can incur prohibitive costs. Leveraging prior research showing influence of core human values on individual decisions and actions, we propose to use value-injected large language models (LLM) to predict opinions and... | Dongjun Kang, Joonsuk Park, Yohan Jo, JinYeong Bak |  |
| 2156 |  |  [Analyzing Film Adaptation through Narrative Alignment](https://doi.org/10.18653/v1/2023.emnlp-main.962) |  | 0 | Novels are often adapted into feature films, but the differences between the two media usually require dropping sections of the source text from the movie script. Here we study this screen adaptation process by constructing narrative alignments using the Smith-Waterman local alignment algorithm coupled with SBERT embedding distance to quantify text similarity between scenes and book units. We use these alignments to perform an automated analysis of 40 adaptations, revealing insights into the... | Tanzir Pial, Shahreen Salim Aunti, Charuta Pethe, Allen Kim, Steven Skiena |  |
| 2157 |  |  [Inverse Scaling Can Become U-Shaped](https://doi.org/10.18653/v1/2023.emnlp-main.963) |  | 0 | Scaling up language models has been empirically shown to improve performance on a wide range of downstream tasks. However, if we were to observe worse performance as a function of scale (inverse scaling) on certain tasks, this would indicate that scaling can also encourage behaviors that are misaligned with human preferences. The Inverse Scaling Prize (McKenzie et al. 2023) identified eleven such inverse scaling tasks, evaluated on models of up to 280B parameters and up to 500 zettaFLOPs of... | Jason Wei, Najoung Kim, Yi Tay, Quoc V. Le |  |
| 2158 |  |  [Nearest Neighbor Machine Translation is Meta-Optimizer on Output Projection Layer](https://doi.org/10.18653/v1/2023.emnlp-main.964) |  | 0 | Nearest Neighbor Machine Translation (kNN-MT) has achieved great success in domain adaptation tasks by integrating pre-trained Neural Machine Translation (NMT) models with domain-specific token-level retrieval. However, the reasons underlying its success have not been thoroughly investigated. In this paper, we comprehensively analyze kNN-MT through theoretical and empirical studies. Initially, we provide new insights into the working mechanism of kNN-MT as an efficient technique to implicitly... | Ruize Gao, Zhirui Zhang, Yichao Du, Lemao Liu, Rui Wang |  |
| 2159 |  |  [Variance Matters: Detecting Semantic Differences without Corpus/Word Alignment](https://doi.org/10.18653/v1/2023.emnlp-main.965) |  | 0 | In this paper, we propose methods for discovering semantic differences in words appearing in two corpora. The key idea is to measure the coverage of meanings of a word in a corpus through the norm of its mean word vector, which is equivalent to examining a kind of variance of the word vector distribution. The proposed methods do not require alignments between words and/or corpora for comparison that previous methods do. All they require are to compute variance (or norms of mean word vectors)... | Ryo Nagata, Hiroya Takamura, Naoki Otani, Yoshifumi Kawasaki |  |
| 2160 |  |  [MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter](https://doi.org/10.18653/v1/2023.emnlp-main.966) |  | 0 | Language Models (LMs) have demonstrated impressive molecule understanding ability on various 1D text-related tasks. However, they inherently lack 2D graph perception — a critical ability of human professionals in comprehending molecules’ topological structures. To bridge this gap, we propose MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. MolCA enables an LM (i.e., Galactica) to understand both text- and graph-based molecular contents via the... | Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, TatSeng Chua |  |
| 2161 |  |  [A Training-Free Debiasing Framework with Counterfactual Reasoning for Conversational Emotion Detection](https://doi.org/10.18653/v1/2023.emnlp-main.967) |  | 0 | Unintended dataset biases typically exist in existing Emotion Recognition in Conversations (ERC) datasets, including label bias, where models favor the majority class due to imbalanced training data, as well as the speaker and neutral word bias, where models make unfair predictions because of excessive correlations between specific neutral words or speakers and classes. However, previous studies in ERC generally focus on capturing context-sensitive and speaker-sensitive dependencies, ignoring... | Geng Tu, Ran Jing, Bin Liang, Min Yang, KamFai Wong, Ruifeng Xu |  |
| 2162 |  |  [Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations](https://doi.org/10.18653/v1/2023.emnlp-main.968) |  | 0 | Large language models (LLMs) have exhibited striking in-context learning (ICL) ability to adapt to target tasks with a few input-output demonstrations. For better ICL, different methods are proposed to select representative demonstrations from existing training corpora. However, such settings are not aligned with real-world practices, as end-users usually query LMs without access to demonstration pools. In this work, we introduce Self-ICL—a simple framework which bootstraps LMs’ intrinsic... | WeiLin Chen, ChengKuang Wu, YunNung Chen, HsinHsi Chen |  |
| 2163 |  |  [Learning Knowledge-Enhanced Contextual Language Representations for Domain Natural Language Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.969) |  | 0 | Knowledge-Enhanced Pre-trained Language Models (KEPLMs) improve the performance of various downstream NLP tasks by injecting knowledge facts from large-scale Knowledge Graphs (KGs). However, existing methods for pre-training KEPLMs with relational triples are difficult to be adapted to close domains due to the lack of sufficient domain graph semantics. In this paper, we propose a Knowledge-enhanced language representation learning framework for various closed domains (KANGAROO) via capturing... | Taolin Zhang, Ruyao Xu, Chengyu Wang, Zhongjie Duan, Cen Chen, Minghui Qiu, Dawei Cheng, Xiaofeng He, Weining Qian |  |
| 2164 |  |  [ScdNER: Span-Based Consistency-Aware Document-Level Named Entity Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.970) |  | 0 | Document-level NER approaches use global information via word-based key-value memory for accurate and consistent predictions. However, such global information on word level can introduce noise when the same word appears in different token sequences and has different labels. This work proposes a two-stage document-level NER model, ScdNER, for more accurate and consistent predictions via adaptive span-level global feature fusion. In the first stage, ScdNER trains a binary classifier to predict if... | Ying Wei, Qi Li |  |
| 2165 |  |  [MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions](https://doi.org/10.18653/v1/2023.emnlp-main.971) |  | 0 | The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model’s related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should... | Zexuan Zhong, Zhengxuan Wu, Christopher D. Manning, Christopher Potts, Danqi Chen |  |
| 2166 |  |  [Stance Detection on Social Media with Background Knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.972) |  | 0 | Identifying users’ stances regarding specific targets/topics is a significant route to learning public opinion from social media platforms. Most existing studies of stance detection strive to learn stance information about specific targets from the context, in order to determine the user’s stance on the target. However, in real-world scenarios, we usually have a certain understanding of a target when we express our stance on it. In this paper, we investigate stance detection from a novel... | Ang Li, Bin Liang, Jingqian Zhao, Bowen Zhang, Min Yang, Ruifeng Xu |  |
| 2167 |  |  [Vision-Enhanced Semantic Entity Recognition in Document Images via Visually-Asymmetric Consistency Learning](https://doi.org/10.18653/v1/2023.emnlp-main.973) |  | 0 | Extracting meaningful entities belonging to predefined categories from Visually-rich Form-like Documents (VFDs) is a challenging task. Visual and layout features such as font, background, color, and bounding box location and size provide important cues for identifying entities of the same type. However, existing models commonly train a visual encoder with weak cross-modal supervision signals, resulting in a limited capacity to capture these non-textual features and suboptimal performance. In... | Hao Wang, Xiahua Chen, Rui Wang, Chenhui Chu |  |
| 2168 |  |  [NormDial: A Comparable Bilingual Synthetic Dialog Dataset for Modeling Social Norm Adherence and Violation](https://doi.org/10.18653/v1/2023.emnlp-main.974) |  | 0 | Social norms fundamentally shape interpersonal communication. We present NormDial, a high-quality dyadic dialogue dataset with turn-by-turn annotations of social norm adherences and violations for Chinese and American cultures. Introducing the task of social norm observance detection, our dataset is synthetically generated in both Chinese and English using a human-in-the-loop pipeline by prompting large language models with a small collection of expert-annotated social norms. We show that our... | Oliver Li, Mallika Subramanian, Arkadiy Saakyan, Sky CHWang, Smaranda Muresan |  |
| 2169 |  |  [ClimateBERT-NetZero: Detecting and Assessing Net Zero and Reduction Targets](https://doi.org/10.18653/v1/2023.emnlp-main.975) |  | 0 | Public and private actors struggle to assess the vast amounts of information about sustainability commitments made by various institutions. To address this problem, we create a novel tool for automatically detecting corporate and national net zero and reduction targets in three steps. First, we introduce an expert-annotated data set with 3.5K text samples. Second, we train and release ClimateBERT-NetZero, a natural language classifier to detect whether a text contains a net zero or reduction... | Tobias Schimanski, Julia Anna Bingler, Mathias Kraus, Camilla Hyslop, Markus Leippold |  |
| 2170 |  |  [Leap-of-Thought: Accelerating Transformers via Dynamic Token Routing](https://doi.org/10.18653/v1/2023.emnlp-main.976) |  | 0 | Computational inefficiency in transformers has been a long-standing challenge, hindering the deployment in resource-constrained or real-time applications. One promising approach to mitigate this limitation is to progressively remove less significant tokens, given that the sequence length strongly contributes to the inefficiency. However, this approach entails a potential risk of losing crucial information due to the irrevocable nature of token removal. In this paper, we introduce... | Yeachan Kim, Junho Kim, JunHyung Park, Mingyu Lee, SangKeun Lee |  |
| 2171 |  |  [Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning](https://doi.org/10.18653/v1/2023.emnlp-main.977) |  | 0 | Query-focused Summarization (QfS) deals with systems that generate summaries from document(s) based on a query. Motivated by the insight that Reinforcement Learning (RL) provides a generalization to Supervised Learning (SL) for Natural Language Generation, and thereby performs better (empirically) than SL, we use an RL-based approach for this task of QfS. Additionally, we also resolve the conflict of employing RL in Transformers with Teacher Forcing. We develop multiple Policy Gradient... | Swaroop Nath, Pushpak Bhattacharyya, Harshad Khadilkar |  |
| 2172 |  |  [Fair Text Classification with Wasserstein Independence](https://doi.org/10.18653/v1/2023.emnlp-main.978) |  | 0 | Group fairness is a central research topic in text classification, where reaching fair treatment between sensitive groups (e.g. women vs. men) remains an open challenge. This paper presents a novel method for mitigating biases in neural text classification, agnostic to the model architecture. Considering the difficulty to distinguish fair from unfair information in a text encoder, we take inspiration from adversarial training to induce Wasserstein independence between representations learned to... | Thibaud Leteno, Antoine Gourru, Charlotte Laclau, Rémi Emonet, Christophe Gravier |  |
| 2173 |  |  [TacoPrompt: A Collaborative Multi-Task Prompt Learning Method for Self-Supervised Taxonomy Completion](https://doi.org/10.18653/v1/2023.emnlp-main.979) |  | 0 | Automatic taxonomy completion aims to attach the emerging concept to an appropriate pair of hypernym and hyponym in the existing taxonomy. Existing methods suffer from the overfitting to leaf-only problem caused by imbalanced leaf and non-leaf samples when training the newly initialized classification head. Besides, they only leverage subtasks, namely attaching the concept to its hypernym or hyponym, as auxiliary supervision for representation learning yet neglect the effects of subtask results... | Hongyuan Xu, Ciyi Liu, Yuhang Niu, Yunong Chen, Xiangrui Cai, Yanlong Wen, Xiaojie Yuan |  |
| 2174 |  |  [An Attribution Method for Siamese Encoders](https://doi.org/10.18653/v1/2023.emnlp-main.980) |  | 0 | Despite the success of Siamese encoder models such as sentence transformers (ST), little is known about the aspects of inputs they pay attention to. A barrier is that their predictions cannot be attributed to individual features, as they compare two inputs rather than processing a single one. This paper derives a local attribution method for Siamese encoders by generalizing the principle of integrated gradients to models with multiple inputs. The output takes the form of feature-pair... | Lucas Möller, Dmitry Nikolaev, Sebastian Padó |  |
| 2175 |  |  [Global Voices, Local Biases: Socio-Cultural Prejudices across Languages](https://doi.org/10.18653/v1/2023.emnlp-main.981) |  | 0 | Human biases are ubiquitous but not uniform: disparities exist across linguistic, cultural, and societal borders. As large amounts of recent literature suggest, language models (LMs) trained on human data can reflect and often amplify the effects of these social biases. However, the vast majority of existing studies on bias are heavily skewed towards Western and European languages. In this work, we scale the Word Embedding Association Test (WEAT) to 24 languages, enabling broader studies and... | Anjishnu Mukherjee, Chahat Raj, Ziwei Zhu, Antonios Anastasopoulos |  |
| 2176 |  |  [Graph vs. Sequence: An Empirical Study on Knowledge Forms for Knowledge-Grounded Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.982) |  | 0 | Knowledge-grounded dialogue is a task of gener- ating an informative response based on both the dialogue history and external knowledge source. In general, there are two forms of knowledge: manu- ally annotated knowledge graphs and knowledge text from website. From various evaluation viewpoints, each type of knowledge has advantages and downsides. To further distinguish the principles and determinants from the intricate factors, we conduct a thorough experiment and study on the task to answer... | Yizhe Yang, Heyan Huang, Yuhang Liu, Yang Gao |  |
| 2177 |  |  [Are Compressed Language Models Less Subgroup Robust?](https://doi.org/10.18653/v1/2023.emnlp-main.983) |  | 0 | To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression... | Leonidas Gee, Andrea Zugarini, Novi Quadrianto |  |
| 2178 |  |  [Length Does Matter: Summary Length can Bias Summarization Metrics](https://doi.org/10.18653/v1/2023.emnlp-main.984) |  | 0 | Establishing the characteristics of an effective summary is a complicated and often subjective endeavor. Consequently, the development of metrics for the summarization task has become a dynamic area of research within natural language processing. In this paper, we reveal that existing summarization metrics exhibit a bias toward the length of generated summaries. Our thorough experiments, conducted on a variety of datasets, metrics, and models, substantiate these findings. The results indicate... | Xiaobo Guo, Soroush Vosoughi |  |
| 2179 |  |  [NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.985) |  | 0 | Temporal Logic (TL) can be used to rigorously specify complex high-level specification for systems in many engineering applications. The translation between natural language (NL) and TL has been under-explored due to the lack of dataset and generalizable model across different application domains. In this paper, we propose an accurate and generalizable transformation framework of English instructions from NL to TL, exploring the use of Large Language Models (LLMs) at multiple stages. Our... | Yongchao Chen, Rujul Gandhi, Yang Zhang, Chuchu Fan |  |
| 2180 |  |  [Reformulating NLP tasks to Capture Longitudinal Manifestation of Language Disorders in People with Dementia](https://doi.org/10.18653/v1/2023.emnlp-main.986) |  | 0 | Dementia is associated with language disorders which impede communication. Here, we automatically learn linguistic disorder patterns by making use of a moderately-sized pre-trained language model and forcing it to focus on reformulated natural language processing (NLP) tasks and associated linguistic patterns. Our experiments show that NLP tasks that encapsulate contextual information and enhance the gradient signal with linguistic patterns benefit performance. We then use the probability... | Dimitris Gkoumas, Matthew Purver, Maria Liakata |  |
| 2181 |  |  [Elevating Code-mixed Text Handling through Auditory Information of Words](https://doi.org/10.18653/v1/2023.emnlp-main.987) |  | 0 | With the growing popularity of code-mixed data, there is an increasing need for better handling of this type of data, which poses a number of challenges, such as dealing with spelling variations, multiple languages, different scripts, and a lack of resources. Current language models face difficulty in effectively handling code-mixed data as they primarily focus on the semantic representation of words and ignore the auditory phonetic features. This leads to difficulties in handling spelling... | Mamta, Zishan Ahmad, Asif Ekbal |  |
| 2182 |  |  [Predict and Use: Harnessing Predicted Gaze to Improve Multimodal Sarcasm Detection](https://doi.org/10.18653/v1/2023.emnlp-main.988) |  | 0 | Sarcasm is a complex linguistic construct with incongruity at its very core. Detecting sarcasm depends on the actual content spoken and tonality, facial expressions, the context of an utterance, and personal traits like language proficiency and cognitive capabilities. In this paper, we propose the utilization of synthetic gaze data to improve the task performance for multimodal sarcasm detection in a conversational setting. We enrich an existing multimodal conversational dataset, i.e.,... | Divyank Tiwari, Diptesh Kanojia, Anupama Ray, Apoorva Nunna, Pushpak Bhattacharyya |  |
| 2183 |  |  [Fine-grained Medical Vision-Language Representation Learning for Radiology Report Generation](https://doi.org/10.18653/v1/2023.emnlp-main.989) |  | 0 | Given the input radiology images, the objective of radiology report generation is to produce accurate and comprehensive medical reports, which typically include multiple descriptive clinical sentences associated with different phenotypes. Most existing works have relied on a pre-trained vision encoder to extract the visual representations of the images. In this study, we propose a phenotype-driven medical vision-language representation learning framework to efficiently bridge the gap between... | Siyuan Wang, Bo Peng, Yichao Liu, Qi Peng |  |
| 2184 |  |  [ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer](https://doi.org/10.18653/v1/2023.emnlp-main.990) |  | 0 | Text-to-speech(TTS) has undergone remarkable improvements in performance, particularly with the advent of Denoising Diffusion Probabilistic Models (DDPMs). However, the perceived quality of audio depends not solely on its content, pitch, rhythm, and energy, but also on the physical environment.In this work, we propose ViT-TTS, the first visual TTS model with scalable diffusion transformers. ViT-TTS complement the phoneme sequence with the visual information to generate high-perceived audio,... | Huadai Liu, Rongjie Huang, Xuan Lin, Wenqiang Xu, Maozong Zheng, Hong Chen, Jinzheng He, Zhou Zhao |  |
| 2185 |  |  [Consistency Analysis of ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.991) |  | 0 | ChatGPT has gained a huge popularity since its introduction. Its positive aspects have been reported through many media platforms, and some analyses even showed that ChatGPT achieved a decent grade in professional exams, adding extra support to the claim that AI can now assist and even replace humans in industrial fields. Others, however, doubt its reliability and trustworthiness. This paper investigates the trustworthiness of ChatGPT and GPT-4 regarding logically consistent behaviour, focusing... | Myeongjun Jang, Thomas Lukasiewicz |  |
| 2186 |  |  [Do Differences in Values Influence Disagreements in Online Discussions?](https://doi.org/10.18653/v1/2023.emnlp-main.992) |  | 0 | Disagreements are common in online discussions. Disagreement may foster collaboration and improve the quality of a discussion under some conditions. Although there exist methods for recognizing disagreement, a deeper understanding of factors that influence disagreement is lacking in the literature. We investigate a hypothesis that differences in personal values are indicative of disagreement in online discussions. We show how state-of-the-art models can be used for estimating values in online... | Michiel van der Meer, Piek Vossen, Catholijn M. Jonker, Pradeep K. Murukannaiah |  |
| 2187 |  |  [Automated Fact-Checking in Dialogue: Are Specialized Models Needed?](https://doi.org/10.18653/v1/2023.emnlp-main.993) |  | 0 | Prior research has shown that typical fact-checking models for stand-alone claims struggle with claims made in conversation. As a solution, fine-tuning these models on dialogue data has been proposed. However, creating separate models for each use case is impractical, and we show that fine-tuning models for dialogue results in poor performance on typical fact-checking. To overcome this challenge, we present techniques that allow us to use the same models for both dialogue and typical... | Eric Chamoun, Marzieh Saeidi, Andreas Vlachos |  |
| 2188 |  |  [A Digital Language Coherence Marker for Monitoring Dementia](https://doi.org/10.18653/v1/2023.emnlp-main.994) |  | 0 | The use of spontaneous language to derive appropriate digital markers has become an emergent, promising and non-intrusive method to diagnose and monitor dementia. Here we propose methods to capture language coherence as a cost-effective, human-interpretable digital marker for monitoring cognitive changes in people with dementia. We introduce a novel task to learn the temporal logical consistency of utterances in short transcribed narratives and investigate a range of neural approaches. We... | Dimitris Gkoumas, Adam Tsakalidis, Maria Liakata |  |
| 2189 |  |  [Detecting Spoilers in Movie Reviews with External Movie Knowledge and User Networks](https://doi.org/10.18653/v1/2023.emnlp-main.995) |  | 0 | Online movie review platforms are providing crowdsourced feedback for the film industry and the general public, while spoiler reviews greatly compromise user experience. Although preliminary research efforts were made to automatically identify spoilers, they merely focus on the review content itself, while robust spoiler detection requires putting the review into the context of facts and knowledge regarding movies, user behavior on film review platforms, and more. In light of these challenges,... | Heng Wang, Wenqian Zhang, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Qinghua Zheng, Minnan Luo |  |
| 2190 |  |  [Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimoda Emotion Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.996) |  | 0 | Multimodal emotion recognition aims to recognize emotions for each utterance from multiple modalities, which has received increasing attention for its application in human-machine interaction. Current graph-based methods fail to simultaneously depict global contextual features and local diverse uni-modal features in a dialogue. Furthermore, with the number of graph layers increasing, they easily fall into over-smoothing. In this paper, we propose a method for joint modality fusion and graph... | Dongyuan Li, Yusong Wang, Kotaro Funakoshi, Manabu Okumura |  |
| 2191 |  |  [HyperRank: Hyperbolic Ranking Model for Unsupervised Keyphrase Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.997) |  | 0 | Given the exponential growth in the number of documents on the web in recent years, there is an increasing demand for accurate models to extract keyphrases from such documents. Keyphrase extraction is the task of automatically identifying representative keyphrases from the source document. Typically, candidate keyphrases exhibit latent hierarchical structures embedded with intricate syntactic and semantic information. Moreover, the relationships between candidate keyphrases and the document... | Mingyang Song, Huafeng Liu, Liping Jing |  |
| 2192 |  |  [Assessing the influence of attractor-verb distance on grammatical agreement in humans and language models](https://doi.org/10.18653/v1/2023.emnlp-main.998) |  | 0 | Subject-verb agreement in the presence of an attractor noun located between the main noun and the verb elicits complex behavior: judgments of grammaticality are modulated by the grammatical features of the attractor. For example, in the sentence \`\`The girl near the boys likes climbing'', the attractor (boys) disagrees in grammatical number with the verb (likes), creating a locally implausible transition probability. Here, we parametrically modulate the distance between the attractor and the... | ChristosNikolaos Zacharopoulos, Théo Desbordes, Mathias SabléMeyer |  |
| 2193 |  |  [Federated Meta-Learning for Emotion and Sentiment Aware Multi-modal Complaint Identification](https://doi.org/10.18653/v1/2023.emnlp-main.999) |  | 0 | Automatic detection of consumers’ complaints about items or services they buy can be critical for organizations and online merchants. Previous studies on complaint identification are limited to text. Images along with the reviews can provide cues to identify complaints better, thus emphasizing the importance of incorporating multi-modal inputs into the process. Generally, the customer’s emotional state significantly impacts the complaint expression; thus, the effect of emotion and sentiment on... | Apoorva Singh, Siddarth Chandrasekar, Sriparna Saha, Tanmay Sen |  |
| 2194 |  |  [Semantic Similarity Models for Depression Severity Estimation](https://doi.org/10.18653/v1/2023.emnlp-main.1000) |  | 0 | Depressive disorders constitute a severe public health issue worldwide. However, public health systems have limited capacity for case detection and diagnosis. In this regard, the widespread use of social media has opened up a way to access public information on a large scale. Computational methods can serve as support tools for rapid screening by exploiting this user-generated social media content. This paper presents an efficient semantic pipeline to study depression severity in individuals... | Anxo Pérez, Neha Warikoo, Kexin Wang, Javier Parapar, Iryna Gurevych |  |
| 2195 |  |  [Hop, Union, Generate: Explainable Multi-hop Reasoning without Rationale Supervision](https://doi.org/10.18653/v1/2023.emnlp-main.1001) |  | 0 | Explainable multi-hop question answering (QA) not only predicts answers but also identifies rationales, i. e. subsets of input sentences used to derive the answers. Existing methods rely on supervision for both answers and rationales. This problem has been extensively studied under the supervised setting, where both answer and rationale annotations are given. Because rationale annotations are expensive to collect and not always available, recent efforts have been devoted to developing methods... | Wenting Zhao, Justin T. Chiu, Claire Cardie, Alexander M. Rush |  |
| 2196 |  |  [To Split or Not to Split: Composing Compounds in Contextual Vector Spaces](https://doi.org/10.18653/v1/2023.emnlp-main.1002) |  | 0 | We investigate the effect of sub-word tokenization on representations of German noun compounds: single orthographic words which are composed of two or more constituents but often tokenized into units that are not morphologically motivated or meaningful. Using variants of BERT models and tokenization strategies on domain-specific restricted diachronic data, we introduce a suite of evaluations relying on the masked language modelling task and compositionality prediction. We obtain the most... | Christopher Jenkins, Filip Miletic, Sabine Schulte im Walde |  |
| 2197 |  |  [ToolWriter: Question Specific Tool Synthesis for Tabular Data](https://doi.org/10.18653/v1/2023.emnlp-main.1003) |  | 0 | Tabular question answering (TQA) presents a challenging setting for neural systems by requiring joint reasoning of natural language with large amounts of semi-structured data. Unlike humans who use programmatic tools like filters to transform data before processing, language models in TQA process tables directly, resulting in information loss as table size increases. In this paper we propose ToolWriter to generate query specific programs and detect when to apply them to transform tables and... | Carlos Gemmell, Jeff Dalton |  |
| 2198 |  |  [Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations](https://doi.org/10.18653/v1/2023.emnlp-main.1004) |  | 0 | Relational databases play an important role in business, science, and more. However, many users cannot fully unleash the analytical power of relational databases, because they are not familiar with database languages such as SQL. Many techniques have been proposed to automatically generate SQL from natural language, but they suffer from two issues: (1) they still make many mistakes, particularly for complex queries, and (2) they do not provide a flexible way for non-expert users to validate and... | Yuan Tian, Zheng Zhang, Zheng Ning, Toby JiaJun Li, Jonathan K. Kummerfeld, Tianyi Zhang |  |
| 2199 |  |  [CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Low Resource With Contrastive Learning](https://doi.org/10.18653/v1/2023.emnlp-main.1005) |  | 0 | Machine-Generated Text (MGT) detection, a task that discriminates MGT from Human-Written Text (HWT), plays a crucial role in preventing misuse of text generative models, which excel in mimicking human writing style recently. Latest proposed detectors usually take coarse text sequences as input and fine-tune pretrained models with standard cross-entropy loss. However, these methods fail to consider the linguistic structure of texts. Moreover, they lack the ability to handle the low-resource... | Xiaoming Liu, Zhaohan Zhang, Yichen Wang, Hang Pu, Yu Lan, Chao Shen |  |
| 2200 |  |  [AnyTOD: A Programmable Task-Oriented Dialog System](https://doi.org/10.18653/v1/2023.emnlp-main.1006) |  | 0 | We propose AnyTOD, an end-to-end, zero-shot task-oriented dialog (TOD) system capable of zero-shot adaptation onto unseen tasks or domains. We view TOD as a program executed by a language model (LM), where program logic and ontology is provided by a designer as a schema. To enable generalization to unseen schemas and programs without prior training, AnyTOD adopts a neuro-symbolic approach. A neural LM keeps track of events that occur during a conversation, and a symbolic program implementing... | Jeffrey Zhao, Yuan Cao, Raghav Gupta, Harrison Lee, Abhinav Rastogi, Mingqiu Wang, Hagen Soltau, Izhak Shafran, Yonghui Wu |  |
| 2201 |  |  [Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.1007) |  | 0 | Recent pre-trained language models (PLMs) achieve promising results in existing abstractive summarization datasets. However, existing summarization benchmarks overlap in time with the standard pre-training corpora and finetuning datasets. Hence, the strong performance of PLMs may rely on the parametric knowledge that is memorized during pre-training and fine-tuning. Moreover, the knowledge memorized by PLMs may quickly become outdated, which affects the generalization performance of PLMs on... | Chi Seng Cheang, Hou Pong Chan, Derek F. Wong, Xuebo Liu, Zhaocong Li, Yanming Sun, Shudong Liu, Lidia S. Chao |  |
| 2202 |  |  [Zero-Shot Multi-Label Topic Inference with Sentence Encoders and LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.1008) |  | 0 | In this paper, we conducted a comprehensive study with the latest Sentence Encoders and Large Language Models (LLMs) on the challenging task of “definition-wild zero-shot topic inference”, where users define or provide the topics of interest in real-time. Through extensive experimentation on seven diverse data sets, we observed that LLMs, such as ChatGPT-3.5 and PaLM, demonstrated superior generality compared to other LLMs, e.g., BLOOM and GPT-NeoX. Furthermore, Sentence-BERT, a BERT-based... | Souvika Sarkar, Dongji Feng, Shubhra Kanti Karmaker Santu |  |
| 2203 |  |  [TaskDiff: A Similarity Metric for Task-Oriented Conversations](https://doi.org/10.18653/v1/2023.emnlp-main.1009) |  | 0 | The popularity of conversational digital assistants has resulted in the availability of large amounts of conversational data which can be utilized for improved user experience and personalized response generation. Building these assistants using popular large language models like ChatGPT also require additional emphasis on prompt engineering and evaluation methods. Textual similarity metrics are a key ingredient for such analysis and evaluations. While many similarity metrics have been proposed... | Ankita Bhaumik, Praveen Venkateswaran, Yara Rizk, Vatche Isahagian |  |
| 2204 |  |  [Not all Fake News is Written: A Dataset and Analysis of Misleading Video Headlines](https://doi.org/10.18653/v1/2023.emnlp-main.1010) |  | 0 | Polarization and the marketplace for impressions have conspired to make navigating information online difficult for users, and while there has been a significant effort to detect false or misleading text, multimodal datasets have received considerably less attention. To complement existing resources, we present multimodal Video Misleading Headline (VMH), a dataset that consists of videos and whether annotators believe the headline is representative of the video’s contents. After collecting and... | Yoo Yeon Sung, Jordan L. BoydGraber, Naeemul Hassan |  |
| 2205 |  |  [Learning From Free-Text Human Feedback - Collect New Datasets Or Extend Existing Ones?](https://doi.org/10.18653/v1/2023.emnlp-main.1011) |  | 0 | Learning from free-text human feedback is essential for dialog systems, but annotated data is scarce and usually covers only a small fraction of error types known in conversational AI. Instead of collecting and annotating new datasets from scratch, recent advances in synthetic dialog generation could be used to augment existing dialog datasets with the necessary annotations. However, to assess the feasibility of such an effort, it is important to know the types and frequency of free-text human... | Dominic Petrak, Nafise Sadat Moosavi, Ye Tian, Nikolai Rozanov, Iryna Gurevych |  |
| 2206 |  |  [Euphemistic Abuse - A New Dataset and Classification Experiments for Implicitly Abusive Language](https://doi.org/10.18653/v1/2023.emnlp-main.1012) |  | 0 | We address the task of identifying euphemistic abuse (e.g. “You inspire me to fall asleep”) paraphrasing simple explicitly abusive utterances (e.g. “You are boring”). For this task, we introduce a novel dataset that has been created via crowdsourcing. Special attention has been paid to the generation of appropriate negative (non-abusive) data. We report on classification experiments showing that classifiers trained on previous datasets are less capable of detecting such abuse. Best automatic... | Michael Wiegand, Jana Kampfmeier, Elisabeth Eder, Josef Ruppenhofer |  |
| 2207 |  |  [Exploring Distributional Shifts in Large Language Models for Code Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.1013) |  | 0 | We systematically study how three large language models with code capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data. We consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. We establish that samples from each new domain present all the models with a significant challenge of distribution shift. We study... | Shushan Arakelyan, Rocktim Jyoti Das, Yi Mao, Xiang Ren |  |
| 2208 |  |  [ATHENA: Mathematical Reasoning with Thought Expansion](https://doi.org/10.18653/v1/2023.emnlp-main.1014) |  | 0 | Solving math word problems depends on how to articulate the problems, the lens through which models view human linguistic expressions. Real-world settings count on such a method even more due to the diverse practices of the same mathematical operations. Earlier works constrain available thinking processes by limited prediction strategies without considering their significance in acquiring mathematical knowledge. We introduce Attention-based THought Expansion Network Architecture (ATHENA) to... | JB. Kim, Hazel Kim, Joonghyuk Hahn, YoSub Han |  |
| 2209 |  |  [A Benchmark for Reasoning with Spatial Prepositions](https://doi.org/10.18653/v1/2023.emnlp-main.1015) |  | 0 | Spatial reasoning is a fundamental building block of human cognition, used in representing, grounding, and reasoning about physical and abstract concepts. We propose a novel benchmark focused on assessing inferential properties of statements with spatial prepositions. The benchmark includes original datasets in English and Romanian and aims to probe the limits of reasoning about spatial relations in large language models. We use prompt engineering to study the performance of two families of... | Iulia M. Comsa, Srini Narayanan |  |
| 2210 |  |  [TIMELINE: Exhaustive Annotation of Temporal Relations Supporting the Automatic Ordering of Events in News Articles](https://doi.org/10.18653/v1/2023.emnlp-main.1016) |  | 0 | Temporal relation extraction models have thus far been hindered by a number of issues in existing temporal relation-annotated news datasets, including: (1) low inter-annotator agreement due to the lack of specificity of their annotation guidelines in terms of what counts as a temporal relation; (2) the exclusion of long-distance relations within a given document (those spanning across different paragraphs); and (3) the exclusion of events that are not centred on verbs. This paper aims to... | Sarah Alsayyahi, Riza BatistaNavarro |  |
| 2211 |  |  [Mitigating Over-Generation for Unsupervised Keyphrase Extraction with Heterogeneous Centrality Detection](https://doi.org/10.18653/v1/2023.emnlp-main.1017) |  | 0 | Over-generation errors occur when a keyphrase extraction model correctly determines a candidate keyphrase as a keyphrase because it contains a word that frequently appears in the document but at the same time erroneously outputs other candidates as keyphrases because they contain the same word. To mitigate this issue, we propose a new heterogeneous centrality detection approach (CentralityRank), which extracts keyphrases by simultaneously identifying both implicit and explicit centrality within... | Mingyang Song, Pengyu Xu, Yi Feng, Huafeng Liu, Liping Jing |  |
| 2212 |  |  [Towards Interpretable and Efficient Automatic Reference-Based Summarization Evaluation](https://doi.org/10.18653/v1/2023.emnlp-main.1018) |  | 0 | Interpretability and efficiency are two important considerations for the adoption of neural automatic metrics. In this work, we develop strong-performing automatic metrics for reference-based summarization evaluation, based on a two-stage evaluation pipeline that first extracts basic information units from one text sequence and then checks the extracted units in another sequence. The metrics we developed include two-stage metrics that can provide high interpretability at both the fine-grained... | Yixin Liu, Alexander R. Fabbri, Yilun Zhao, Pengfei Liu, Shafiq Joty, ChienSheng Wu, Caiming Xiong, Dragomir Radev |  |
| 2213 |  |  [MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.1019) |  | 0 | Reading comprehension of legal text can be a particularly challenging task due to the length and complexity of legal clauses and a shortage of expert-annotated datasets. To address this challenge, we introduce the Merger Agreement Understanding Dataset (MAUD), an expert-annotated reading comprehension dataset based on the American Bar Association’s 2021 Public Target Deal Points Study, with over 39,000 examples and over 47,000 total annotations. Our fine-tuned Transformer baselines show... | Steven H. Wang, Antoine Scardigli, Leonard Tang, Wei Chen, Dimitry Levkin, Anya Chen, Spencer Ball, Thomas Woodside, Oliver Zhang, Dan Hendrycks |  |
| 2214 |  |  [PK-ICR: Persona-Knowledge Interactive Multi-Context Retrieval for Grounded Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.1020) |  | 0 | Identifying relevant persona or knowledge for conversational systems is critical to grounded dialogue response generation. However, each grounding has been mostly researched in isolation with more practical multi-context dialogue tasks introduced in recent works. We define Persona and Knowledge Dual Context Identification as the task to identify persona and knowledge jointly for a given dialogue, which could be of elevated importance in complex multi-context dialogue settings. We develop a... | Minsik Oh, Joosung Lee, Jiwei Li, Guoyin Wang |  |
| 2215 |  |  [More Than Spoken Words: Nonverbal Message Extraction and Generation](https://doi.org/10.18653/v1/2023.emnlp-main.1021) |  | 0 | Nonverbal messages (NM) such as speakers’ facial expressions and speed of speech are essential for face-to-face communication, and they can be regarded as implicit knowledge as they are usually not included in existing dialogue understanding or generation tasks. This paper introduces the task of extracting NMs in written text and generating NMs for spoken text. Previous studies merely focus on extracting NMs from relatively small-scale well-structured corpora such as movie scripts wherein NMs... | Dian Yu, Xiaoyang Wang, Wanshun Chen, Nan Du, Longyue Wang, Haitao Mi, Dong Yu |  |
| 2216 |  |  [Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance](https://doi.org/10.18653/v1/2023.emnlp-main.1022) |  | 0 | While analogies are a common way to evaluate word embeddings in NLP, it is also of interest to investigate whether or not analogical reasoning is a task in itself that can be learned. In this paper, we test several ways to learn basic analogical reasoning, specifically focusing on analogies that are more typical of what is used to evaluate analogical reasoning in humans than those in commonly used NLP benchmarks. Our experiments find that models are able to learn analogical reasoning, even with... | Molly R. Petersen, Lonneke van der Plas |  |
| 2217 |  |  [FAME: Flexible, Scalable Analogy Mappings Engine](https://doi.org/10.18653/v1/2023.emnlp-main.1023) |  | 0 | Analogy is one of the core capacities of human cognition; when faced with new situations, we often transfer prior experience from other domains. Most work on computational analogy relies heavily on complex, manually crafted input. In this work, we relax the input requirements, requiring only names of entities to be mapped. We automatically extract commonsense representations and use them to identify a mapping between the entities. Unlike previous works, our framework can handle partial... | Shahar Jacob, Chen Shani, Dafna Shahaf |  |
| 2218 |  |  [A Self-training Framework for Automated Medical Report Generation](https://doi.org/10.18653/v1/2023.emnlp-main.1024) |  | 0 | Medical report generation, focusing on automatically generating accurate clinical findings from medical images, is an important medical artificial intelligence task. It reduces the workload of physicians in writing reports. Many of the current methods depend heavily on labeled datasets that include a large amount of image-report pairs, but such datasets labeled by physicians are hard to acquire in clinical practice. To this end, in this paper, we introduce a self-training framework named REMOTE... | Siyuan Wang, Zheng Liu, Bo Peng |  |
| 2219 |  |  [A Picture is Worth a Thousand Words: Language Models Plan from Pixels](https://doi.org/10.18653/v1/2023.emnlp-main.1025) |  | 0 | Planning is an important capability of artificial agents that perform long-horizon tasks in real-world environments. In this work, we explore the use of pre-trained language models (PLMs) to reason about plan sequences from text instructions in embodied visual environments. Prior PLM based approaches for planning either assume observations are available in the form of text by a captioning model, reason about plans from the instruction alone, or incorporate information about the visual... | Anthony Z. Liu, Lajanugen Logeswaran, Sungryull Sohn, Honglak Lee |  |
| 2220 |  |  [Interpreting and Exploiting Functional Specialization in Multi-Head Attention under Multi-task Learning](https://doi.org/10.18653/v1/2023.emnlp-main.1026) |  | 0 | Transformer-based models, even though achieving super-human performance on several downstream tasks, are often regarded as a black box and used as a whole. It is still unclear what mechanisms they have learned, especially their core module: multi-head attention. Inspired by functional specialization in the human brain, which helps to efficiently handle multiple tasks, this work attempts to figure out whether the multi-head attention module will evolve similar function separation under... | Chong Li, Shaonan Wang, Yunhao Zhang, Jiajun Zhang, Chengqing Zong |  |
| 2221 |  |  [Multilingual Previously Fact-Checked Claim Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.1027) |  | 0 | Fact-checkers are often hampered by the sheer amount of online content that needs to be fact-checked. NLP can help them by retrieving already existing fact-checks relevant to the content being investigated. This paper introduces a new multilingual dataset for previously fact-checked claim retrieval. We collected 28k posts in 27 languages from social media, 206k fact-checks in 39 languages written by professional fact-checkers, as well as 31k connections between these two groups. This is the... | Matús Pikuliak, Ivan Srba, Róbert Móro, Timo Hromadka, Timotej Smolen, Martin Melisek, Ivan Vykopal, Jakub Simko, Juraj Podrouzek, Mária Bieliková |  |
| 2222 |  |  [ALCAP: Alignment-Augmented Music Captioner](https://doi.org/10.18653/v1/2023.emnlp-main.1028) |  | 0 | Music captioning has gained significant attention in the wake of the rising prominence of streaming media platforms. Traditional approaches often prioritize either the audio or lyrics aspect of the music, inadvertently ignoring the intricate interplay between the two. However, a comprehensive understanding of music necessitates the integration of both these elements. In this study, we delve into this overlooked realm by introducing a method to systematically learn multimodal alignment between... | Zihao He, Weituo Hao, Wei Tsung Lu, Changyou Chen, Kristina Lerman, Xuchen Song |  |
| 2223 |  |  [Do Transformers Parse while Predicting the Masked Word?](https://doi.org/10.18653/v1/2023.emnlp-main.1029) |  | 0 | Pre-trained language models have been shown to encode linguistic structures like parse trees in their embeddings while being trained unsupervised. Some doubts have been raised whether the models are doing parsing or only some computation weakly correlated with it. Concretely: (a) Is it possible to explicitly describe transformers with realistic embedding dimensions, number of heads, etc. that are capable of doing parsing — or even approximate parsing? (b) Why do pre-trained models capture... | Haoyu Zhao, Abhishek Panigrahi, Rong Ge, Sanjeev Arora |  |
| 2224 |  |  [Composable Text Controls in Latent Space with ODEs](https://doi.org/10.18653/v1/2023.emnlp-main.1030) |  | 0 | Real-world text applications often involve composing a wide range of text control operations, such as editing the text w.r.t. an attribute, manipulating keywords and structure, and generating new text of desired properties. Prior work typically learns/finetunes a language model (LM) to perform individual or specific subsets of operations. Recent research has studied combining operations in a plug-and-play manner, often with costly search or optimization in the complex sequence space. This paper... | Guangyi Liu, Zeyu Feng, Yuan Gao, Zichao Yang, Xiaodan Liang, Junwei Bao, Xiaodong He, Shuguang Cui, Zhen Li, Zhiting Hu |  |
| 2225 |  |  [P5: Plug-and-Play Persona Prompting for Personalized Response Selection](https://doi.org/10.18653/v1/2023.emnlp-main.1031) |  | 0 | The use of persona-grounded retrieval-based chatbots is crucial for personalized conversations, but there are several challenges that need to be addressed. 1) In general, collecting persona-grounded corpus is very expensive. 2) The chatbot system does not always respond in consideration of persona at real applications. To address these challenges, we propose a plug-and-play persona prompting method. Our system can function as a standard open-domain chatbot if persona information is not... | Joosung Lee, Minsik Oh, Donghun Lee |  |
| 2226 |  |  [Reader: Model-based language-instructed reinforcement learning](https://doi.org/10.18653/v1/2023.emnlp-main.1032) |  | 0 | We explore how we can build accurate world models, which are partially specified by language, and how we can plan with them in the face of novelty and uncertainty. We propose the first model-based reinforcement learning approach to tackle the environment Read To Fight Monsters (Zhong et al., 2019), a grounded policy learning problem. In RTFM an agent has to reason over a set of rules and a goal, both described in a language manual, and the observations, while taking into account the uncertainty... | Nicola Dainese, Pekka Marttinen, Alexander Ilin |  |
| 2227 |  |  [Adapting Offline Speech Translation Models for Streaming with Future-Aware Distillation and Inference](https://doi.org/10.18653/v1/2023.emnlp-main.1033) |  | 0 | A popular approach to streaming speech translation is to employ a single offline model with a wait-k policy to support different latency requirements, which is simpler than training multiple online models with different latency constraints. However, there is a mismatch problem in using a model trained with complete utterances for streaming inference with partial input. We demonstrate that speech representations extracted at the end of a streaming input are significantly different from those... | Biao Fu, Minpeng Liao, Kai Fan, Zhongqiang Huang, Boxing Chen, Yidong Chen, Xiaodong Shi |  |
| 2228 |  |  [Relation-aware Ensemble Learning for Knowledge Graph Embedding](https://doi.org/10.18653/v1/2023.emnlp-main.1034) |  | 0 | Knowledge graph (KG) embedding is a fundamental task in natural language processing, and various methods have been proposed to explore semantic patterns in distinctive ways. In this paper, we propose to learn an ensemble by leveraging existing methods in a relation-aware manner. However, exploring these semantics using relation-aware ensemble leads to a much larger search space than general ensemble methods. To address this issue, we propose a divide-search-combine algorithm RelEns-DSC that... | Ling Yue, Yongqi Zhang, Quanming Yao, Yong Li, Xian Wu, Ziheng Zhang, Zhenxi Lin, Yefeng Zheng |  |
| 2229 |  |  [GenEx: A Commonsense-aware Unified Generative Framework for Explainable Cyberbullying Detection](https://doi.org/10.18653/v1/2023.emnlp-main.1035) |  | 0 | With the rise of social media and online communication, the issue of cyberbullying has gained significant prominence. While extensive research is being conducted to develop more effective models for detecting cyberbullying in monolingual languages, a significant gap exists in understanding code-mixed languages and the need for explainability in this context. To address this gap, we have introduced a novel benchmark dataset named BullyExplain for explainable cyberbullying detection in code-mixed... | Krishanu Maity, Raghav Jain, Prince Jha, Sriparna Saha, Pushpak Bhattacharyya |  |
| 2230 |  |  [Document-Level Machine Translation with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.1036) |  | 0 | Large language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs’ ability on discourse modeling. The study focuses on three aspects: 1) Effects of Context-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2)... | Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, Zhaopeng Tu |  |
| 2231 |  |  [Multilingual Simplification of Medical Texts](https://doi.org/10.18653/v1/2023.emnlp-main.1037) |  | 0 | Automated text simplification aims to produce simple versions of complex texts. This task is especially useful in the medical domain, where the latest medical findings are typically communicated via complex and technical articles. This creates barriers for laypeople seeking access to up-to-date medical findings, consequently impeding progress on health literacy. Most existing work on medical text simplification has focused on monolingual settings, with the result that such evidence would be... | Sebastian Joseph, Kathryn Kazanas, Keziah Reina, Vishnesh J. Ramanathan, Wei Xu, Byron C. Wallace, Junyi Jessy Li |  |
| 2232 |  |  [When Reviewers Lock Horns: Finding Disagreements in Scientific Peer Reviews](https://doi.org/10.18653/v1/2023.emnlp-main.1038) |  | 0 | To this date, the efficacy of the scientific publishing enterprise fundamentally rests on the strength of the peer review process. The journal editor or the conference chair primarily relies on the expert reviewers’ assessment, identify points of agreement and disagreement and try to reach a consensus to make a fair and informed decision on whether to accept or reject a paper. However, with the escalating number of submissions requiring review, especially in top-tier Artificial Intelligence... | Sandeep Kumar, Tirthankar Ghosal, Asif Ekbal |  |
| 2233 |  |  [Argue with Me Tersely: Towards Sentence-Level Counter-Argument Generation](https://doi.org/10.18653/v1/2023.emnlp-main.1039) |  | 0 | Counter-argument generation—a captivating area in computational linguistics—seeks to craft statements that offer opposing views. While most research has ventured into paragraph-level generation, sentence-level counter-argument generation beckons with its unique constraints and brevity-focused challenges. Furthermore, the diverse nature of counter-arguments poses challenges for evaluating model performance solely based on n-gram-based metrics. In this paper, we present the ArgTersely benchmark... | Jiayu Lin, Rong Ye, Meng Han, Qi Zhang, Ruofei Lai, Xinyu Zhang, Zhao Cao, Xuanjing Huang, Zhongyu Wei |  |
| 2234 |  |  [JASMINE: Arabic GPT Models for Few-Shot Learning](https://doi.org/10.18653/v1/2023.emnlp-main.1040) |  | 0 | Scholarship on generative pretraining (GPT) remains acutely Anglocentric, leaving serious gaps in our understanding of the whole class of autoregressive models. For example, we have little knowledge about the potential of these models and their societal impacts in diverse linguistic and cultural settings. We alleviate this issue for Arabic, a wide collection of languages and dialectal varieties with more than 400 million population, by introducing JASMINE. JASMINE is a suite of powerful Arabic... | El Moatez Billah Nagoudi, Muhammad AbdulMageed, AbdelRahim A. Elmadany, Alcides Alcoba Inciarte, Md Tawkat Islam Khondaker |  |
| 2235 |  |  [NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports](https://doi.org/10.18653/v1/2023.emnlp-main.1041) |  | 0 | How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual... | Maël Jullien, Marco Valentino, Hannah Frost, Paul O'Regan, Dónal Landers, André Freitas |  |
| 2236 |  |  [Addressing Linguistic Bias through a Contrastive Analysis of Academic Writing in the NLP Domain](https://doi.org/10.18653/v1/2023.emnlp-main.1042) |  | 0 | It has been well documented that a reviewer’s opinion of the nativeness of expression in an academic paper affects the likelihood of it being accepted for publication. Previous works have also shone a light on the stress and anxiety authors who are non-native English speakers experience when attempting to publish in international venues. We explore how this might be a concern in the field of Natural Language Processing (NLP) through conducting a comprehensive statistical analysis of NLP paper... | Robert Ridley, Zhen Wu, Jianbing Zhang, Shujian Huang, Xinyu Dai |  |
| 2237 |  |  [RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation](https://doi.org/10.18653/v1/2023.emnlp-main.1043) |  | 0 | Grammatical Error Correction (GEC) systems play a vital role in assisting people with their daily writing tasks. However, users may sometimes come across a GEC system that initially performs well but fails to correct errors when the inputs are slightly modified. To ensure an ideal user experience, a reliable GEC system should have the ability to provide consistent and accurate suggestions when encountering irrelevant context perturbations, which we refer to as context robustness. In this paper,... | Yue Zhang, Leyang Cui, Enbo Zhao, Wei Bi, Shuming Shi |  |
| 2238 |  |  [Detecting Propaganda Techniques in Code-Switched Social Media Text](https://doi.org/10.18653/v1/2023.emnlp-main.1044) |  | 0 | Propaganda is a form of communication intended to influence the opinions and the mindset of the public to promote a particular agenda. With the rise of social media, propaganda has spread rapidly, leading to the need for automatic propaganda detection systems. Most work on propaganda detection has focused on high-resource languages, such as English, and little effort has been made to detect propaganda for low-resource languages. Yet, it is common to find a mix of multiple languages in social... | Muhammad Umar Salman, Asif Hanif, Shady Shehata, Preslav Nakov |  |
| 2239 |  |  [Speech Recognition and Meaning Interpretation: Towards Disambiguation of Structurally Ambiguous Spoken Utterances in Indonesian](https://doi.org/10.18653/v1/2023.emnlp-main.1045) |  | 0 | Despite being the world’s fourth-most populous country, the development of spoken language technologies in Indonesia still needs improvement. Most automatic speech recognition (ASR) systems that have been developed are still limited to transcribing the exact word-by-word, which, in many cases, consists of ambiguous sentences. In fact, speakers use prosodic characteristics of speech to convey different interpretations, which, unfortunately, these systems often ignore. In this study, we attempt... | Ruhiyah Widiaputri, Ayu Purwarianti, Dessi Puji Lestari, Kurniawati Azizah, Dipta Tanaya, Sakriani Sakti |  |
| 2240 |  |  [Target-Agnostic Gender-Aware Contrastive Learning for Mitigating Bias in Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.1046) |  | 0 | Gender bias is a significant issue in machine translation, leading to ongoing research efforts in developing bias mitigation techniques. However, most works focus on debiasing bilingual models without much consideration for multilingual systems. In this paper, we specifically target the gender bias issue of multilingual machine translation models for unambiguous cases where there is a single correct translation, and propose a bias mitigation method based on a novel approach. Specifically, we... | Minwoo Lee, Hyukhun Koh, Kangil Lee, Dongdong Zhang, Minsung Kim, Kyomin Jung |  |
| 2241 |  |  [Code-Switching Metrics Using Intonation Units](https://doi.org/10.18653/v1/2023.emnlp-main.1047) |  | 0 | Code-switching (CS) metrics in NLP that are based on word-level units are misaligned with true bilingual CS behavior. Crucially, CS is not equally likely between any two words, but follows syntactic and prosodic rules. We adapt two metrics, multilinguality and CS probability, and apply them to transcribed bilingual speech, for the first time putting forward Intonation Units (IUs) – prosodic speech segments – as basic tokens for NLP tasks. In addition, we calculate these two metrics separately... | Rebecca Pattichis, Dora LaCasse, Sonya Trawick, Rena Cacoullos |  |
