# EMNLP2023

## 会议论文列表

本会议共有 2241 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Frontmatter](https://aclanthology.org/2023.emnlp-demo.0) |  | 0 |  |  |  |
| 2 |  |  [Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs](https://doi.org/10.18653/v1/2023.emnlp-demo.1) |  | 0 | Most NLP tasks are modeled as supervised learning and thus require labeled training data to train effective models. However, manually producing such data at sufficient quality and quantity is known to be costly and time-intensive. Current research addresses this bottleneck by exploring a novel paradigm called zero-shot learning via dataset generation. Here, a powerful LLM is prompted with a task description to generate labeled data that can be used to train a downstream NLP model. For instance, an LLM might be prompted to “generate 500 movie reviews with positive overall sentiment, and another 500 with negative sentiment.” The generated data could then be used to train a binary sentiment classifier, effectively leveraging an LLM as a teacher to a smaller student model. With this demo, we introduce Fabricator, an open-source Python toolkit for dataset generation. Fabricator implements common dataset generation workflows, supports a wide range of downstream NLP tasks (such as text classification, question answering, and entity recognition), and is integrated with well-known libraries to facilitate quick experimentation. With Fabricator, we aim to support researchers in conducting reproducible dataset generation experiments using LLMs and help practitioners apply this approach to train models for downstream tasks. | Jonas Golde, Patrick Haller, Felix Hamborg, Julian Risch, Alan Akbik |  |
| 3 |  |  [End-to-End Evaluation for Low-Latency Simultaneous Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-demo.2) |  | 0 | The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches. In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components. Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework allows to automatically evaluate the translation quality as well as latency and also provides a web interface to show the low-latency model outputs to the user. | Christian Huber, Tu Anh Dinh, Carlos Mullov, NgocQuan Pham, Thai Binh Nguyen, Fabian Retkowski, Stefan Constantin, Enes Yavuz Ugan, Danni Liu, Zhaolin Li, Sai Koneru, Jan Niehues, Alexander Waibel |  |
| 4 |  |  [CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools](https://doi.org/10.18653/v1/2023.emnlp-demo.3) |  | 0 | In the face of climate change, are companies really taking substantial steps toward more sustainable operations? A comprehensive answer lies in the dense, information-rich landscape of corporate sustainability reports. However, the sheer volume and complexity of these reports make human analysis very costly. Therefore, only a few entities worldwide have the resources to analyze these reports at scale, which leads to a lack of transparency in sustainability reporting. Empowering stakeholders with LLM-based automatic analysis tools can be a promising way to democratize sustainability report analysis. However, developing such tools is challenging due to (1) the hallucination of LLMs and (2) the inefficiency of bringing domain experts into the AI development loop. In this paper, we introduce ChatReport, a novel LLM-based system to automate the analysis of corporate sustainability reports, addressing existing challenges by (1) making the answers traceable to reduce the harm of hallucination and (2) actively involving domain experts in the development loop. We make our methodology, annotated datasets, and generated analyses of 1015 reports publicly available. Video Introduction: https://www.youtube.com/watch?v=Q5AzaKzPE4M Github: https://github.com/EdisonNi-hku/chatreport Live web app: reports.chatclimate.ai | Jingwei Ni, Julia Anna Bingler, Chiara Colesanti Senni, Mathias Kraus, Glen Gostlow, Tobias Schimanski, Dominik Stammbach, Saeid Ashraf Vaghefi, Qian Wang, Nicolas Webersinke, Tobias Wekhof, Tingyu Yu, Markus Leippold |  |
| 5 |  |  [RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.4) |  | 0 | Retrieval-augmented large language models (R-LLMs) combine pre-trained large language models (LLMs) with information retrieval systems to improve the accuracy of factual question-answering. However, current libraries for building R-LLMs provide high-level abstractions without sufficient transparency for evaluating and optimizing prompts within specific inference processes such as retrieval and generation. To address this gap, we present RaLLe, an open-source framework designed to facilitate the development, evaluation, and optimization of R-LLMs for knowledge-intensive tasks. With RaLLe, developers can easily develop and evaluate R-LLMs, improving hand-crafted prompts, assessing individual inference processes, and objectively measuring overall system performance quantitatively. By leveraging these features, developers can enhance the performance and accuracy of their R-LLMs in knowledge-intensive generation tasks. | Yasuto Hoshi, Daisuke Miyashita, Youyang Ng, Kento Tatsuno, Yasuhiro Morioka, Osamu Torii, Jun Deguchi |  |
| 6 |  |  [VIST5: An Adaptive, Retrieval-Augmented Language Model for Visualization-oriented Dialog](https://doi.org/10.18653/v1/2023.emnlp-demo.5) |  | 0 | The advent of large language models has brought about new ways of interacting with data intuitively via natural language. In recent years, a variety of visualization systems have explored the use of natural language to create and modify visualizations through visualization-oriented dialog. However, the majority of these systems rely on tailored dialog agents to analyze domain-specific data and operate domain-specific visualization tools and libraries. This is a major challenge when trying to transfer functionalities between dialog interfaces of different visualization applications. To address this issue, we propose VIST5, a visualization-oriented dialog system that focuses on easy adaptability to an application domain as well as easy transferability of language-controllable visualization library functions between applications. Its architecture is based on a retrieval-augmented T5 language model that leverages few-shot learning capabilities to enable a rapid adaptation of the system. | Henrik Voigt, Nuno Carvalhais, Monique Meuschke, Markus Reichstein, Sina Zarrieß, Kai Lawonn |  |
| 7 |  |  [H2O Open Ecosystem for State-of-the-art Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.6) |  | 0 | Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7 to 70 Billion parameters. We also introduce H2O LLM Studio, a framework and no-code GUI designed for efficient fine-tuning, evaluation, and deployment of LLMs using the most recent state-of-the-art techniques. Our code and models are licensed under fully permissive Apache 2.0 licenses. We believe open-source language models help to boost AI development and make it more accessible and trustworthy. Our demo is available at: https://gpt.h2o.ai/ | Arno Candel, Jon McKinney, Philipp Singer, Pascal Pfeiffer, Maximilian Jeblick, Chun Ming Lee, Marcos V. Conde |  |
| 8 |  |  [Koala: An Index for Quantifying Overlaps with Pre-training Corpora](https://doi.org/10.18653/v1/2023.emnlp-demo.7) |  | 0 | In very recent years more attention has been placed on probing the role of pre-training data in Large Language Models (LLMs) downstream behaviour. Despite the importance, there is no public tool that supports such analysis of pre-training corpora at large scale. To help research in this space, we launch Koala, a searchable index over large pre-training corpora using lossless compressed suffix arrays with highly efficient compression rate and search support. In its first release we index the public proportion of OPT 175B, GPT-3, GPT-Neo, GPT-Neo, LLaMA, BERT, ELECTRA, RoBERTA, XLNet pre-training corpora. Koala provides a framework to do forensic analysis on the current and future benchmarks as well as to assess the degree of memorization in the output from the LLMs. Koala is available for public use at https://koala-index.erc.monash.edu/. | ThuyTrang Vu, Xuanli He, Gholamreza Haffari, Ehsan Shareghi |  |
| 9 |  |  [Sudowoodo: A Chinese Lyric Imitation System with Source Lyrics](https://doi.org/10.18653/v1/2023.emnlp-demo.8) |  | 0 | Lyrics generation is a well-known application in natural language generation research, with several previous studies focusing on generating accurate lyrics using precise control such as keywords, rhymes, etc. However, lyrics imitation, which involves writing new lyrics by imitating the style and content of the source lyrics, remains a challenging task due to the lack of a parallel corpus. In this paper, we introduce Sudowoodo, a Chinese lyrics imitation system that can generate new lyrics based on the text of source lyrics. To address the issue of lacking a parallel training corpus for lyrics imitation, we propose a novel framework to construct a parallel corpus based on a keyword-based lyrics model from source lyrics. Then the pairs (new lyrics, source lyrics) are used to train the lyrics imitation model. During the inference process, we utilize a post-processing module to filter and rank the generated lyrics, selecting the highest-quality ones. We incorporated audio information and aligned the lyrics with the audio to form the songs as a bonus. The human evaluation results show that our framework can perform better lyric imitation. Meanwhile, the Sudowoodo system and demo video of the system is available at Sudowoodo and https://youtu.be/u5BBT\_j1L5M | Yongzhu Chang, Rongsheng Zhang, Lin Jiang, Qihang Chen, Le Zhang, Jiashu Pu |  |
| 10 |  |  [ConvLab-3: A Flexible Dialogue System Toolkit Based on a Unified Data Format](https://doi.org/10.18653/v1/2023.emnlp-demo.9) |  | 0 | Task-oriented dialogue (TOD) systems function as digital assistants, guiding users through various tasks such as booking flights or finding restaurants. Existing toolkits for building TOD systems often fall short in delivering comprehensive arrays of data, model, and experimental environments with a user-friendly experience. We introduce ConvLab-3: a multifaceted dialogue system toolkit crafted to bridge this gap. Our unified data format simplifies the integration of diverse datasets and models, significantly reducing complexity and cost for studying generalization and transfer. Enhanced with robust reinforcement learning (RL) tools, featuring a streamlined training process, in-depth evaluation tools, and a selection of user simulators, ConvLab-3 supports the rapid development and evaluation of robust dialogue policies. Through an extensive study, we demonstrate the efficacy of transfer learning and RL and showcase that ConvLab-3 is not only a powerful tool for seasoned researchers but also an accessible platform for newcomers. | Qi Zhu, Christian Geishauser, HsienChin Lin, Carel van Niekerk, Baolin Peng, Zheng Zhang, Shutong Feng, Michael Heck, Nurul Lubis, Dazhen Wan, Xiaochen Zhu, Jianfeng Gao, Milica Gasic, Minlie Huang |  |
| 11 |  |  [FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge](https://doi.org/10.18653/v1/2023.emnlp-demo.10) |  | 0 | Detecting factual errors of textual information, whether generated by large language models (LLM) or curated by humans, is crucial for making informed decisions. LLMs’ inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses. Humans, too, are prone to factual errors in their writing. Since manual detection and correction of factual er- rors is labor-intensive, developing an automatic approach can greatly reduce human effort. We present a prototype tool that automatically extracts factual claims from text, gathers evidence from external knowledge sources, evaluates the factuality of each claim, and suggests revisions for identified errors using the collected evidence. Initial empirical evaluation on fact error detection (77-85% F1) shows the potential of our tool. | Farima Fatahi Bayat, Kun Qian, Benjamin Han, Yisi Sang, Anton Belyi, Samira Khorshidi, Fei Wu, Ihab F. Ilyas, Yunyao Li |  |
| 12 |  |  [YATO: Yet Another deep learning based Text analysis Open toolkit](https://doi.org/10.18653/v1/2023.emnlp-demo.11) |  | 0 | We introduce YATO, an open-source, easy-to-use toolkit for text analysis with deep learning. Different from existing heavily engineered toolkits and platforms, YATO is lightweight and user-friendly for researchers from cross-disciplinary areas. Designed in a hierarchical structure, YATO supports free combinations of three types of widely used features including 1) traditional neural networks (CNN, RNN, etc.); 2) pre-trained language models (BERT, RoBERTa, ELECTRA, etc.); and 3) user-customized neural features via a simple configurable file. Benefiting from the advantages of flexibility and ease of use, YATO can facilitate fast reproduction and refinement of state-of-the-art NLP models, and promote the cross-disciplinary applications of NLP techniques. The code, examples, and documentation are publicly available at https://github.com/jiesutd/YATO. A demo video is also available at https://www.youtube.com/playlist?list=PLJ0mhzMcRuDUlTkzBfAftOqiJRxYTTjXH. | Zeqiang Wang, Yile Wang, Jiageng Wu, Zhiyang Teng, Jie Yang |  |
| 13 |  |  [Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face](https://doi.org/10.18653/v1/2023.emnlp-demo.12) |  | 0 | We present Spacerini, a tool that integrates the Pyserini toolkit for reproducible information retrieval research with Hugging Face to enable the seamless construction and deployment of interactive search engines. Spacerini makes state-of-the-art sparse and dense retrieval models more accessible to non-IR practitioners while minimizing deployment effort. This is useful for NLP researchers who want to better understand and validate their research by performing qualitative analyses of training corpora, for IR researchers who want to demonstrate new retrieval models integrated into the growing Pyserini ecosystem, and for third parties reproducing the work of other researchers. Spacerini is open source and includes utilities for loading, preprocessing, indexing, and deploying search engines locally and remotely. We demonstrate a portfolio of 13 search engines created with Spacerini for different use cases. | Christopher Akiki, Odunayo Ogundepo, Aleksandra Piktus, Xinyu Zhang, Akintunde Oladipo, Jimmy Lin, Martin Potthast |  |
| 14 |  |  [Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning](https://doi.org/10.18653/v1/2023.emnlp-demo.13) |  | 0 | We introduce Adapters, an open-source library that unifies parameter-efficient and modular transfer learning in large language models. By integrating 10 diverse adapter methods into a unified interface, Adapters offers ease of use and flexible configuration. Our library allows researchers and practitioners to leverage adapter modularity through composition blocks, enabling the design of complex adapter setups. We demonstrate the library’s efficacy by evaluating its performance against full fine-tuning on various NLP tasks. Adapters provides a powerful tool for addressing the challenges of conventional fine-tuning paradigms and promoting more efficient and modular transfer learning. The library is available via https://adapterhub.ml/adapters. | Clifton Poth, Hannah Sterz, Indraneil Paul, Sukannya Purkayastha, Leon Engländer, Timo Imhof, Ivan Vulic, Sebastian Ruder, Iryna Gurevych, Jonas Pfeiffer |  |
| 15 |  |  [INTELMO: Enhancing Models' Adoption of Interactive Interfaces](https://doi.org/10.18653/v1/2023.emnlp-demo.14) |  | 0 | This paper presents INTELMO, an easy-to-use library to help model developers adopt user-faced interactive interfaces and articles from real-time RSS sources for their language models. The library categorizes common NLP tasks and provides default style patterns, streamlining the process of creating interfaces with minimal code modifications while ensuring an intuitive user experience. Moreover, INTELMO employs a multi-granular hierarchical abstraction to provide developers with fine-grained and flexible control over user interfaces. INTELMO is under active development, with document available at https://intelmo.github.io. | Chunxu Yang, ChienSheng Wu, Lidiya Murakhovs'ka, Philippe Laban, Xiang Chen |  |
| 16 |  |  [Humanoid Agents: Platform for Simulating Human-like Generative Agents](https://doi.org/10.18653/v1/2023.emnlp-demo.15) |  | 0 | Just as computational simulations of atoms, molecules and cells have shaped the way we study the sciences, true-to-life simulations of human-like agents can be valuable tools for studying human behavior. We propose Humanoid Agents, a system that guides Generative Agents to behave more like humans by introducing three elements of System 1 processing: Basic needs (e.g. hunger, health and energy), Emotion and Closeness in Relationships. Humanoid Agents are able to use these dynamic elements to adapt their daily activities and conversations with other agents, as supported with empirical experiments. Our system is designed to be extensible to various settings, three of which we demonstrate, as well as to other elements influencing human behavior (e.g. empathy, moral values and cultural background). Our platform also includes a Unity WebGL game interface for visualization and an interactive analytics dashboard to show agent statuses over time. Our platform is available on https://www.humanoidagents.com/ and code is on https://github.com/HumanoidAgents/HumanoidAgents | Zhilin Wang, Yu Ying Chiu, Yu Cheung Chiu |  |
| 17 |  |  [TP-Detector: Detecting Turning Points in the Engineering Process of Large-scale Projects](https://doi.org/10.18653/v1/2023.emnlp-demo.16) |  | 0 | This paper introduces a novel task of detecting turning points in the engineering process of large-scale projects, wherein the turning points signify significant transitions occurring between phases. Given the complexities involving diverse critical events and limited comprehension in individual news reports, we approach the problem by treating the sequence of related news streams as a window with multiple instances. To capture the evolution of changes effectively, we adopt a deep Multiple Instance Learning (MIL) framework and employ the multiple instance ranking loss to discern the transition patterns exhibited in the turning point window. Extensive experiments consistently demonstrate the effectiveness of our proposed approach on the constructed dataset compared to baseline methods. We deployed the proposed mode and provided a demonstration video to illustrate its functionality. The code and dataset are available on GitHub. | Qi Wu, WenHan Chao, Xian Zhou, Zhunchen Luo |  |
| 18 |  |  [CLEVA: Chinese Language Models EVAluation Platform](https://doi.org/10.18653/v1/2023.emnlp-demo.17) |  | 0 | With the continuous emergence of Chinese Large Language Models (LLMs), how to evaluate a model’s capabilities has become an increasingly significant issue. The absence of a comprehensive Chinese benchmark that thoroughly assesses a model’s performance, the unstandardized and incomparable prompting procedure, and the prevalent risk of contamination pose major challenges in the current evaluation of Chinese LLMs. We present CLEVA, a user-friendly platform crafted to holistically evaluate Chinese LLMs. Our platform employs a standardized workflow to assess LLMs’ performance across various dimensions, regularly updating a competitive leaderboard. To alleviate contamination, CLEVA curates a significant proportion of new data and develops a sampling strategy that guarantees a unique subset for each leaderboard round. Empowered by an easy-to-use interface that requires just a few mouse clicks and a model API, users can conduct a thorough evaluation with minimal coding. Large-scale experiments featuring 23 Chinese LLMs have validated CLEVA’s efficacy. | Yanyang Li, Jianqiao Zhao, Duo Zheng, ZiYuan Hu, Zhi Chen, Xiaohui Su, Yongfeng Huang, Shijia Huang, Dahua Lin, Michael R. Lyu, Liwei Wang |  |
| 19 |  |  [DOPA METER - A Tool Suite for Metrical Document Profiling and Aggregation](https://doi.org/10.18653/v1/2023.emnlp-demo.18) |  | 0 | We present DOPA METER, a tool suite for the metrical investigation of written language, that provides diagnostic means for its division into discourse categories, such as registers, genres, and style. The quantitative basis of our system are 120 metrics covering a wide range of lexical, syntactic, and semantic features relevant for language profiling. The scores can be summarized, compared, and aggregated using visualization tools that can be tailored according to the users’ needs. We also showcase an application scenario for DOPA METER. | Christina Lohr, Udo Hahn |  |
| 20 |  |  [Muted: Multilingual Targeted Offensive Speech Identification and Visualization](https://doi.org/10.18653/v1/2023.emnlp-demo.19) |  | 0 | Offensive language such as hate, abuse, and profanity (HAP) occurs in various content on the web. While previous work has mostly dealt with sentence level annotations, there have been a few recent attempts to identify offensive spans as well. We build upon this work and introduce MUTED, a system to identify multilingual HAP content by displaying offensive arguments and their targets using heat maps to indicate their intensity. MUTED can leverage any transformer-based HAP-classification model and its attention mechanism out-of-the-box to identify toxic spans, without further fine-tuning. In addition, we use the spaCy library to identify the specific targets and arguments for the words predicted by the attention heatmaps. We present the model’s performance on identifying offensive spans and their targets in existing datasets and present new annotations on German text. Finally, we demonstrate our proposed visualization tool on multilingual inputs. | Christoph Tillmann, Aashka Trivedi, Sara Rosenthal, Santosh Borse, Rong Zhang, Avirup Sil, Bishwaranjan Bhattacharjee |  |
| 21 |  |  [Gentopia.AI: A Collaborative Platform for Tool-Augmented LLMs](https://doi.org/10.18653/v1/2023.emnlp-demo.20) |  | 0 | Augmented Language Models (ALMs) empower large language models with the ability to use tools, transforming them into intelligent agents for real-world interactions. However, most existing frameworks for ALMs, to varying degrees, are deficient in the following critical features: flexible customization, collaborative democratization, and holistic evaluation. This paper proposes Gentopia, a lightweight and extensible framework for ALMs. Gentopia allows the flexible customization of agents through simple configurations, seamlessly integrating various language models, task formats, prompting modules, and plugins into a unified paradigm. Furthermore, we establish Gentpool, a public platform enabling the registration and sharing of user-customized agents. Agents registered in Gentpool are composable such that they can be assembled together for agent collaboration, advancing the democratization of artificial intelligence. To ensure high-quality agents, Gentbench, an integral component of Gentpool, is designed to thoroughly evaluate user-customized agents across diverse aspects such as safety, robustness, efficiency, etc. We release Gentopia on Github and will continuously move forward. | Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, Dongkuan Xu |  |
| 22 |  |  [MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.21) |  | 0 | AI-empowered music processing is a diverse feld that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classifcation). For developers and amateurs, it is very diffcult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements. Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements. More specifically, we build 1) toolset that collects tools from diverse sources, including Hugging Face, GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g., ChatGPT) to organize these tools and automatically decompose user requests into multiple sub-tasks and invoke corresponding music tools. The primary goal of this system is to free users from the intricacies of AI-music tools, enabling them to concentrate on the creative aspect. By granting users the freedom to effortlessly combine tools, the system offers a seamless and enriching music experience. The code is available on GitHub along with a brief instructional video. | Dingyao Yu, Kaitao Song, Peiling Lu, Tianyu He, Xu Tan, Wei Ye, Shikun Zhang, Jiang Bian |  |
| 23 |  |  [SentAlign: Accurate and Scalable Sentence Alignment](https://doi.org/10.18653/v1/2023.emnlp-demo.22) |  | 0 | We present SentAlign, an accurate sentence alignment tool designed to handle very large parallel document pairs. Given user-defined parameters, the alignment algorithm evaluates all possible alignment paths in fairly large documents of thousands of sentences and uses a divide-and-conquer approach to align documents containing tens of thousands of sentences. The scoring function is based on LaBSE bilingual sentence representations. SentAlign outperforms five other sentence alignment tools when evaluated on two different evaluation sets, German-French and English-Icelandic, and on a downstream machine translation task. | Steinþór Steingrímsson, Hrafn Loftsson, Andy Way |  |
| 24 |  |  [QACheck: A Demonstration System for Question-Guided Multi-Hop Fact-Checking](https://doi.org/10.18653/v1/2023.emnlp-demo.23) |  | 0 | Fact-checking real-world claims often requires intricate, multi-step reasoning due to the absence of direct evidence to support or refute them. However, existing fact-checking systems often lack transparency in their decision-making, making it challenging for users to comprehend their reasoning process. To address this, we propose the Question-guided Multi-hop Fact-Checking (QACheck) system, which guides the model’s reasoning process by asking a series of questions critical for verifying a claim. QACheck has five key modules: a claim verifier, a question generator, a question-answering module, a QA validator, and a reasoner. Users can input a claim into QACheck, which then predicts its veracity and provides a comprehensive report detailing its reasoning process, guided by a sequence of (question, answer) pairs. QACheck also provides the source of evidence supporting each question, fostering a transparent, explainable, and user-friendly fact-checking process. | Liangming Pan, Xinyuan Lu, MinYen Kan, Preslav Nakov |  |
| 25 |  |  [RobustQA: A Framework for Adversarial Text Generation Analysis on Question Answering Systems](https://doi.org/10.18653/v1/2023.emnlp-demo.24) |  | 0 | Question answering (QA) systems have reached human-level accuracy; however, these systems are not robust enough and are vulnerable to adversarial examples. Recently, adversarial attacks have been widely investigated in text classification. However, there have been few research efforts on this topic in QA. In this article, we have modified the attack algorithms widely used in text classification to fit those algorithms for QA systems. We have evaluated the impact of various attack methods on QA systems at character, word, and sentence levels. Furthermore, we have developed a new framework, named RobustQA, as the first open-source toolkit for investigating textual adversarial attacks in QA systems. RobustQA consists of seven modules: Tokenizer, Victim Model, Goals, Metrics, Attacker, Attack Selector, and Evaluator. It currently supports six different attack algorithms. Furthermore, the framework simplifies the development of new attack algorithms in QA. The source code and documentation of RobustQA are available at https://github.com/mirbostani/RobustQA. | Yasaman Boreshban, Seyed Morteza Mirbostani, Seyedeh Fatemeh Ahmadi, Gita Shojaee, Fatemeh Kamani, Gholamreza GhassemSani, Seyed Abolghasem Mirroshandel |  |
| 26 |  |  [Kandinsky: An Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion](https://doi.org/10.18653/v1/2023.emnlp-demo.25) |  | 0 | Text-to-image generation is a significant domain in modern computer vision and achieved substantial improvements through the evolution of generative architectures. Among these, diffusion-based models demonstrated essential quality enhancements. These models generally split into two categories: pixel-level and latent-level approaches. We present Kandinsky – a novel exploration of latent diffusion architecture, combining the principles of image prior models with latent diffusion techniques. The image prior model, is trained separately to map CLIP text and image embeddings. Another distinct feature of the proposed model is the modified MoVQ implementation, which serves as the image autoencoder component. Overall the designed model contains 3.3B parameters. We also deployed a user-friendly demo system that supports diverse generative modes such as text-to-image generation, image fusion, text and image fusion, image variations generation and text-guided inpainting/outpainting. Additionally we released the source code and checkpoints for Kandinsky models. Experimental evaluations demonstrate FID score of 8.03 on the COCO-30K dataset, marking our model as the top open source performer in terms of measurable image generation quality. | Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov, Angelina Kuts, Alexander Panchenko, Andrey Kuznetsov, Denis Dimitrov |  |
| 27 |  |  [NewsRecLib: A PyTorch-Lightning Library for Neural News Recommendation](https://doi.org/10.18653/v1/2023.emnlp-demo.26) |  | 0 | NewsRecLib is an open-source library based on Pytorch-Lightning and Hydra developed for training and evaluating neural news recommendation models. The foremost goals of NewsRecLib are to promote reproducible research and rigorous experimental evaluation by (i) providing a unified and highly configurable framework for exhaustive experimental studies and (ii) enabling a thorough analysis of the performance contribution of different model architecture components and training regimes. NewsRecLib is highly modular, allows specifying experiments in a single configuration file, and includes extensive logging facilities. Moreover, NewsRecLib provides out-of-the-box implementations of several prominent neural models, training methods, standard evaluation benchmarks, and evaluation metrics for news recommendation. | Andreea Iana, Goran Glavas, Heiko Paulheim |  |
| 28 |  |  [MiniChain: A Small Library for Coding with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.27) |  | 0 | Programming augmented by large language models (LLMs) opens up many new application areas, but also requires care. LLMs are accurate enough, on average, to replace core functionality, yet make basic mistakes that demonstrate a lack of robustness. An ecosystem of prompting tools, from intelligent agents to new programming languages, have emerged with different solutions for patching LLMs with other tools. In this work, we introduce MiniChain, an opinionated tool for LLM augmented programming, with the design goals of ease-of-use of prototyping, transparency through automatic visualization, and a minimalistic approach to advanced features. The MiniChain library provides core primitives for coding LLM calls, separating out prompt templates, and capturing program structure. The library includes demo implementations of the main applications papers in the area, including chat-bots, code generation, retrieval-based question answering, and complex information extraction. The library is open-source and available at https://github.com/srush/MiniChain, with code demos available at https://srush-minichain.hf.space/, and video demo at https://www.youtube.com/watch?v=VszZ1VnO7sk. | Alexander M. Rush |  |
| 29 |  |  [Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback](https://doi.org/10.18653/v1/2023.emnlp-demo.28) |  | 0 | A key technology for large language models (LLMs) involves instruction tuning that helps align the models’ responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are applied to produce the best commercial LLMs. To improve the accessibility of LLMs, various instruction-tuned open-source LLMs have also been introduced recently. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their accessibility to many other languages in the world. In addition, SFT has been used as the only approach to instruction-tune open-source LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework with created resources, fine-tuned LLMs, interaction scripts are released at https://github.com/nlp-uoregon/Okapi. A demo video to show our framework can also be found at: https://youtu.be/QFV2fkPwvi0. | Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A. Rossi, Thien Huu Nguyen |  |
| 30 |  |  [SAGEViz: SchemA GEneration and Visualization](https://doi.org/10.18653/v1/2023.emnlp-demo.29) |  | 0 | Schema induction involves creating a graph representation depicting how events unfold in a scenario. We present SAGEViz, an intuitive and modular tool that utilizes human-AI collaboration to create and update complex schema graphs efficiently, where multiple annotators (humans and models) can work simultaneously on a schema graph from any domain. The tool consists of two components: (1) a curation component powered by plug-and-play event language models to create and expand event sequences while human annotators validate and enrich the sequences to build complex hierarchical schemas, and (2) an easy-to-use visualization component to visualize schemas at varying levels of hierarchy. Using supervised and few-shot approaches, our event language models can continually predict relevant events starting from a seed event. We conduct a user study and show that users need less effort in terms of interaction steps with SAGEViz to generate schemas of better quality. We also include a video demonstrating the system. | Sugam Devare, Mahnaz Koupaee, Gautham Gunapati, Sayontan Ghosh, Sai Vallurupalli, Yash Kumar Lal, Francis Ferraro, Nathanael Chambers, Greg Durrett, Raymond J. Mooney, Katrin Erk, Niranjan Balasubramanian |  |
| 31 |  |  [Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained Text Evaluation](https://doi.org/10.18653/v1/2023.emnlp-demo.30) |  | 0 | Fine-grained, span-level human evaluation has emerged as a reliable and robust method for evaluating text generation tasks such as summarization, simplification, machine translation and news generation, and the derived annotations have been useful for training automatic metrics and improving language models. However, existing annotation tools implemented for these evaluation frameworks lack the adaptability to be extended to different domains or languages, or modify annotation settings according to user needs; and, the absence of a unified annotated data format inhibits the research in multi-task learning. In this paper, we introduce Thresh, a unified, customizable and deployable platform for fine-grained evaluation. With a single YAML configuration file, users can build and test an annotation interface for any framework within minutes – all in one web browser window. To facilitate collaboration and sharing, Thresh provides a community hub that hosts a collection of fine-grained frameworks and corresponding annotations made and collected by the community, covering a wide range of NLP tasks. For deployment, Thresh offers multiple options for any scale of annotation projects from small manual inspections to large crowdsourcing ones. Additionally, we introduce a Python library to streamline the entire process from typology design and deployment to annotation processing. Thresh is publicly accessible at https://thresh.tools. | David Heineman, Yao Dou, Wei Xu |  |
| 32 |  |  [InsightPilot: An LLM-Empowered Automated Data Exploration System](https://doi.org/10.18653/v1/2023.emnlp-demo.31) |  | 0 | Exploring data is crucial in data analysis, as it helps users understand and interpret the data more effectively. However, performing effective data exploration requires in-depth knowledge of the dataset, the user intent and expertise in data analysis techniques. Not being familiar with either can create obstacles that make the process time-consuming and overwhelming. To address this issue, we introduce InsightPilot, an LLM (Large Language Model)-based, automated data exploration system designed to simplify the data exploration process. InsightPilot features a set of carefully designed analysis actions that streamline the data exploration process. Given a natural language question, InsightPilot collaborates with the LLM to issue a sequence of analysis actions, explore the data and generate insights. We demonstrate the effectiveness of InsightPilot in a user study and a case study, showing how it can help users gain valuable insights from their datasets. | Pingchuan Ma, Rui Ding, Shuai Wang, Shi Han, Dongmei Zhang |  |
| 33 |  |  [SynJax: Structured Probability Distributions for JAX](https://doi.org/10.18653/v1/2023.emnlp-demo.32) |  | 0 | The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form. SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. This is done by exploiting the connection between algorithms for automatic differentiation and probabilistic inference. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https://github.com/google-deepmind/synjax | Milos Stanojevic, Laurent Sartran |  |
| 34 |  |  [RESIN-EDITOR: A Schema-guided Hierarchical Event Graph Visualizer and Editor](https://doi.org/10.18653/v1/2023.emnlp-demo.33) |  | 0 | In this paper, we present RESIN-EDITOR, an interactive event graph visualizer and editor designed for analyzing complex events. Our RESIN-EDITOR system allows users to render and freely edit hierarchical event graphs extracted from multimedia and multi-document news clusters with guidance from human-curated event schemas. RESIN-EDITOR’s unique features include hierarchical graph visualization, comprehensive source tracing, and interactive user editing, which significantly outperforms existing Information Extraction (IE) visualization tools in both IE result analysis and general model improvements. In our evaluation of RESIN-EDITOR, we demonstrate ways in which our tool is effective in understanding complex events and enhancing system performances. The source code, a video demonstration, and a live website for RESIN-EDITOR have been made publicly available. | Khanh Duy Nguyen, Zixuan Zhang, Reece Suchocki, Sha Li, Martha Palmer, Susan Windisch Brown, Jiawei Han, Heng Ji |  |
| 35 |  |  [DRGCoder: Explainable Clinical Coding for the Early Prediction of Diagnostic-Related Groups](https://doi.org/10.18653/v1/2023.emnlp-demo.34) |  | 0 | Medical claim coding is the process of transforming medical records, usually presented as free texts written by clinicians, or discharge summaries, into structured codes in a classification system such as ICD-10 (International Classification of Diseases, Tenth Revision) or DRG (Diagnosis-Related Group) codes. This process is essential for medical billing and transitional care; however, manual coding is time-consuming, error-prone, and expensive. To solve these issues, we propose DRGCoder, an explainability-enhanced clinical claim coding system for the early prediction of medical severity DRGs (MS-DRGs), a classification system that categorizes patients’ hospital stays into various DRG groups based on the severity of illness and mortality risk. The DRGCoder framework introduces a novel multi-task Transformer model for MS-DRG prediction, modeling both the DRG labels of the discharge summaries and the important, or salient words within he discharge summaries. We allow users to inspect DRGCoder’s reasoning by visualizing the weights for each word of the input. Additionally, DRGCoder allows users to identify diseases within discharge summaries and compare across multiple discharge summaries. Our demo is available at https://huggingface.co/spaces/danielhajialigol/DRGCoder. A video demonstrating the demo can be found at https://www.youtube.com/watch?v=pcdiG6VwqlA | Daniel Hajialigol, Derek Kaknes, Tanner Barbour, Daphne Yao, Chris North, Jimeng Sun, David Liem, Xuan Wang |  |
| 36 |  |  [CAMRA: Copilot for AMR Annotation](https://doi.org/10.18653/v1/2023.emnlp-demo.35) |  | 0 | In this paper, we introduce CAMRA (Copilot for AMR Annotatations), a cutting-edge web-based tool designed for constructing Abstract Meaning Representation (AMR) from natural language text. CAMRA offers a novel approach to deep lexical semantics annotation such as AMR, treating AMR annotation akin to coding in programming languages. Leveraging the familiarity of programming paradigms, CAMRA encompasses all essential features of existing AMR editors, including example lookup, while going a step further by integrating Propbank roleset lookup as an autocomplete feature within the tool. Notably, CAMRA incorporates AMR parser models as coding co-pilots, greatly enhancing the efficiency and accuracy of AMR annotators. | Jon Z. Cai, Shafiuddin Rehan Ahmed, Julia Bonn, Kristin WrightBettner, Martha Palmer, James H. Martin |  |
| 37 |  |  [Reaction Miner: An Integrated System for Chemical Reaction Extraction from Textual Data](https://doi.org/10.18653/v1/2023.emnlp-demo.36) |  | 0 | Chemical reactions, as a core entity in the realm of chemistry, hold crucial implications in diverse areas ranging from hands-on laboratory research to advanced computational drug design. Despite a burgeoning interest in employing NLP techniques to extract these reactions, aligning this task with the real-world requirements of chemistry practitioners remains an ongoing challenge. In this paper, we present Reaction Miner, a system specifically designed to interact with raw scientific literature, delivering precise and more informative chemical reactions. Going beyond mere extraction, Reaction Miner integrates a holistic workflow: it accepts PDF files as input, bypassing the need for pre-processing and bolstering user accessibility. Subsequently, a text segmentation module ensures that the refined text encapsulates complete chemical reactions, augmenting the accuracy of extraction. Moreover, Reaction Miner broadens the scope of existing pre-defined reaction roles, including vital attributes previously neglected, thereby offering a more comprehensive depiction of chemical reactions. Evaluations conducted by chemistry domain users highlight the efficacy of each module in our system, demonstrating Reaction Miner as a powerful tool in this field. | Ming Zhong, Siru Ouyang, Yizhu Jiao, Priyanka Kargupta, Leo Luo, Yanzhen Shen, Bobby Zhou, Xianrui Zhong, Xuan Liu, Hongxiang Li, Jinfeng Xiao, Minhao Jiang, Vivian Hu, Xuan Wang, Heng Ji, Martin D. Burke, Huimin Zhao, Jiawei Han |  |
| 38 |  |  [CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies](https://doi.org/10.18653/v1/2023.emnlp-demo.37) |  | 0 | Various NLP tasks require a complex hierarchical structure over nodes, where each node is a cluster of items. Examples include generating entailment graphs, hierarchical cross-document coreference resolution, annotating event and subevent relations, etc. To enable efficient annotation of such hierarchical structures, we release CHAMP, an open source tool allowing to incrementally construct both clusters and hierarchy simultaneously over any type of texts. This incremental approach significantly reduces annotation time compared to the common pairwise annotation approach and also guarantees maintaining transitivity at the cluster and hierarchy levels. Furthermore, CHAMP includes a consolidation mode, where an adjudicator can easily compare multiple cluster hierarchy annotations and resolve disagreements. | Arie Cattan, Tom Hope, Doug Downey, Roy BarHaim, Lilach Eden, Yoav Kantor, Ido Dagan |  |
| 39 |  |  [Prompt2Model: Generating Deployable Models from Natural Language Instructions](https://doi.org/10.18653/v1/2023.emnlp-demo.38) |  | 0 | Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model. Our demo video is posted at youtu.be/LYYQ_EhGd-Q. | Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu, Graham Neubig |  |
| 40 |  |  [NewsSense: Reference-free Verification via Cross-document Comparison](https://doi.org/10.18653/v1/2023.emnlp-demo.39) |  | 0 | We present NewsSense, a novel sensemaking tool and reading interface designed to collect and integrate information from multiple news articles on a central topic. NewsSense provides “reference-free verification,” augmenting a central grounding article of the user’s choice by: (1) linking to related articles from different sources; and (2) providing inline highlights on how specific claims are either supported or contradicted by information from other articles. Using NewsSense, users can seamlessly digest and cross-check multiple information sources without disturbing their natural reading flow. Our pilot study shows that NewsSense has the potential to help users identify key information, verify the credibility of news articles, explore different perspectives, and understand what content is supported, contradicted, or missing. | Jeremiah Milbauer, Ziqi Ding, Zhijin Wu, Tongshuang Wu |  |
| 41 |  |  [NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails](https://doi.org/10.18653/v1/2023.emnlp-demo.40) |  | 0 | NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment. Using a runtime inspired from dialogue management, NeMo Guardrails provides a different approach by allowing developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable. Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails. | Traian Rebedea, Razvan Dinu, Makesh Narsimhan Sreedhar, Christopher Parisien, Jonathan Cohen |  |
| 42 |  |  [LM-Polygraph: Uncertainty Estimation for Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.41) |  | 0 | Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields. However, a significant challenge arises as these models often “hallucinate”, i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods are one path to safer, more responsible, and more effective use of LLMs. However, to date, research on UE methods for LLMs has been focused primarily on theoretical rather than engineering contributions. In this work, we tackle this issue by introducing LM-Polygraph, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python. Additionally, it introduces an extendable benchmark for consistent evaluation of UE techniques by researchers, and a demo web application that enriches the standard chat dialog with confidence scores, empowering end-users to discern unreliable responses. LM-Polygraph is compatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and GPT-4, and is designed to support future releases of similarly-styled LMs. | Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, Timothy Baldwin, Artem Shelmanov |  |
| 43 |  |  [Descriptive Knowledge Graph in Biomedical Domain](https://doi.org/10.18653/v1/2023.emnlp-demo.42) |  | 0 | We present a novel system that automatically extracts and generates informative and descriptive sentences from the biomedical corpus and facilitates the efficient search for relational knowledge. Unlike previous search engines or exploration systems that retrieve unconnected passages, our system organizes descriptive sentences as a relational graph, enabling researchers to explore closely related biomedical entities (e.g., diseases treated by a chemical) or indirectly connected entities (e.g., potential drugs for treating a disease). Our system also uses ChatGPT and a fine-tuned relation synthesis model to generate concise and reliable descriptive sentences from retrieved information, reducing the need for extensive human reading effort. With our system, researchers can easily obtain both high-level knowledge and detailed references and interactively steer to the information of interest. We spotlight the application of our system in COVID-19 research, illustrating its utility in areas such as drug repurposing and literature curation. | Kerui Zhu, Jie Huang, Kevin ChenChuan Chang |  |
| 44 |  |  [Prompterator: Iterate Efficiently towards More Effective Prompts](https://doi.org/10.18653/v1/2023.emnlp-demo.43) |  | 0 | With the advent of Large Language Models (LLMs) the process known as prompting, which entices the LLM to solve an arbitrary language processing task without the need for finetuning, has risen to prominence. Finding well-performing prompts, however, is a non-trivial task which requires experimentation in order to arrive at a prompt that solves a specific task. When a given task does not readily reduce to one that can be easily measured with well established metrics, human evaluation of the results obtained by prompting is often necessary. In this work we present prompterator, a tool that helps the user interactively iterate over various potential prompts and choose the best performing one based on human feedback. It is distributed as an open source package with out-of-the-box support for various LLM providers and was designed to be easily extensible. | Samuel Sucik, Daniel Skala, Andrej Svec, Peter Hraska, Marek Suppa |  |
| 45 |  |  [ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.44) |  | 0 | The unprecedented performance of LLMs requires comprehensive and accurate evaluation. We argue that for LLMs evaluation, benchmarks need to be comprehensive and systematic. To this end, we propose the Zhujiu benchmark, which has the following strengths: (1) Multi-dimensional ability coverage: We comprehensively evaluate LLMs across 7 ability dimensions covering 51 tasks. Especially, we also propose a new benchmark that focus on knowledge ability of LLMs. (2) Multi-faceted evaluation methods collaboration: We use 3 different yet complementary evaluation methods to comprehensively evaluate LLMs, which can ensure the authority and accuracy of the evaluation results. (3) Comprehensive Chinese benchmark: ZhuJiu is the pioneering benchmark that fully assesses LLMs in Chinese, while also providing equally robust evaluation abilities in English. (4) Avoiding potential data leakage: To avoid data leakage, we construct evaluation data specifically for 37 tasks. We evaluate 10 current mainstream LLMs, and conduct an in-depth discussion and analysis of their results. The ZhuJiu benchmark and open-participation leaderboard are publicly released at http://www.zhujiu-benchmark.com and we also provide a demo video at https://youtu.be/qypkJ89L1Ic. | Baoli Zhang, Haining Xie, Pengfan Du, Junhao Chen, Pengfei Cao, Yubo Chen, Shengping Liu, Kang Liu, Jun Zhao |  |
| 46 |  |  [PaperMage: A Unified Toolkit for Processing, Representing, and Manipulating Visually-Rich Scientific Documents](https://doi.org/10.18653/v1/2023.emnlp-demo.45) |  | 0 | Despite growing interest in applying natural language processing (NLP) and computer vision (CV) models to the scholarly domain, scientific documents remain challenging to work with. They’re often in difficult-to-use PDF formats, and the ecosystem of models to process them is fragmented and incomplete. We introduce PaperMage, an open-source Python toolkit for analyzing and processing visually-rich, structured scientific documents. PaperMage offers clean and intuitive abstractions for seamlessly representing and manipulating both textual and visual document elements. PaperMage achieves this by integrating disparate state-of-the-art NLP and CV models into a unified framework, and provides turn-key recipes for common scientific document processing use-cases. PaperMage has powered multiple research prototypes of AI applications over scientific documents, along with Semantic Scholar’s large-scale production system for processing millions of PDFs. GitHub: https://github.com/allenai/papermage | Kyle Lo, Zejiang Shen, Benjamin Newman, Joseph Chee Chang, Russell Authur, Erin Bransom, Stefan Candra, Yoganand Chandrasekhar, Regan Huff, Bailey Kuehl, Amanpreet Singh, Chris Wilhelm, Angele Zamarron, Marti A. Hearst, Daniel S. Weld, Doug Downey, Luca Soldaini |  |
| 47 |  |  [OmniEvent: A Comprehensive, Fair, and Easy-to-Use Toolkit for Event Understanding](https://doi.org/10.18653/v1/2023.emnlp-demo.46) |  | 0 | Event understanding aims at understanding the content and relationship of events within texts, which covers multiple complicated information extraction tasks: event detection, event argument extraction, and event relation extraction. To facilitate related research and application, we present an event understanding toolkit OmniEvent, which features three desiderata: (1) Comprehensive. OmniEvent supports mainstream modeling paradigms of all the event understanding tasks and the processing of 15 widely-used English and Chinese datasets. (2) Fair. OmniEvent carefully handles the inconspicuous evaluation pitfalls reported in Peng et al. (2023), which ensures fair comparisons between different models. (3) Easy-to-use. OmniEvent is designed to be easily used by users with varying needs. We provide off-the-shelf models that can be directly deployed as web services. The modular framework also enables users to easily implement and evaluate new event understanding models with OmniEvent. The toolkit is publicly released along with the demonstration website and video. | Hao Peng, Xiaozhi Wang, Feng Yao, Zimu Wang, Chuzhao Zhu, Kaisheng Zeng, Lei Hou, Juanzi Li |  |
| 48 |  |  [CocoSciSum: A Scientific Summarization Toolkit with Compositional Controllability](https://doi.org/10.18653/v1/2023.emnlp-demo.47) |  | 0 | We present a novel toolkit for controlled summarization of scientific documents, designed for the specific needs of the scientific community. Our system generates summaries based on user preferences, adjusting key attributes specifically of length and keyword inclusion. A distinguishing feature is its ability to manage multiple attributes concurrently, demonstrating Compositional Controllability for Scientific Summarization (CocoSciSum). Benchmarked against the strong Flan-T5 baseline, CocoSciSum exhibits superior performance on both the quality of summaries generated and the control over single and multiple attributes. Moreover, CocoSciSum is a user-centric toolkit, supporting user preferences expressed in natural language instructions, and accommodating diverse input document formats. CocoSciSum is available on GitHub (https://github.com/WING-NUS/SciAssist/tree/CocoSciSum) with an introduction video (https://youtu.be/YC1YDeEjAbQ). | Yixi Ding, Yanxia Qin, Qian Liu, MinYen Kan |  |
| 49 |  |  [CoLLiE: Collaborative Training of Large Language Models in an Efficient Way](https://doi.org/10.18653/v1/2023.emnlp-demo.48) |  | 0 | Large language models (LLMs) are increasingly pivotal in a wide range of natural language processing tasks. Access to pre-trained models, courtesy of the open-source community, has made it possible to adapt these models to specific applications for enhanced performance. However, the substantial resources required for training these models necessitate efficient solutions. This paper introduces CoLLiE, an efficient library that facilitates collaborative training of large language models using 3D parallelism, parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion, Adan, Sophia, and LOMO. With its modular design and comprehensive functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and customization. CoLLiE has proven superior training efficiency in comparison with prevalent solutions in pre-training and fine-tuning scenarios. Furthermore, we provide an empirical evaluation of the correlation between model size and GPU memory consumption under different optimization methods, as well as an analysis of the throughput. Lastly, we carry out a comprehensive comparison of various optimizers and PEFT methods within the instruction-tuning context. CoLLiE is available at https://github.com/OpenLMLab/collie. | Kai Lv, Shuo Zhang, Tianle Gu, Shuhao Xing, Jiawei Hong, Keyu Chen, Xiaoran Liu, Yuqing Yang, Honglin Guo, Tengxiao Liu, Yu Sun, Qipeng Guo, Hang Yan, Xipeng Qiu |  |
| 50 |  |  [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://doi.org/10.18653/v1/2023.emnlp-demo.49) |  | 0 | We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual & audio encoders with LLM’s embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos. | Hang Zhang, Xin Li, Lidong Bing |  |
| 51 |  |  [SummHelper: Collaborative Human-Computer Summarization](https://doi.org/10.18653/v1/2023.emnlp-demo.50) |  | 0 | Current approaches for text summarization are predominantly automatic, with rather limited space for human intervention and control over the process. In this paper, we introduce SummHelper, and screencast demo at https://www.youtube.com/watch?v=nGcknJwGhxk a 2-phase summarization assistant designed to foster human-machine collaboration. The initial phase involves content selection, where the system recommends potential content, allowing users to accept, modify, or introduce additional selections. The subsequent phase, content consolidation, involves SummHelper generating a coherent summary from these selections, which users can then refine using visual mappings between the summary and the source text. Small-scale user studies reveal the effectiveness of our application, with participants being especially appreciative of the balance between automated guidance and opportunities for personal input. | Aviv Slobodkin, Niv Nachum, Shmuel Amar, Ori Shapira, Ido Dagan |  |
| 52 |  |  [ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.51) |  | 0 | Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent frameworks that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with a customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent online demo, library are now publicly available. | Chenliang Li, He Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, Hongzhu Shi, Ji Zhang, Fei Huang, Jingren Zhou |  |
| 53 |  |  [EfficientOCR: An Extensible, Open-Source Package for Efficiently Digitizing World Knowledge](https://doi.org/10.18653/v1/2023.emnlp-demo.52) |  | 0 | Billions of public domain documents remain trapped in hard copy or lack an accurate digitization. Modern natural language processing methods cannot be used to index, retrieve, and summarize their texts; conduct computational textual analyses; or extract information for statistical analyses, and these texts cannot be incorporated into language model training. Given the diversity and sheer quantity of public domain texts, liberating them at scale requires optical character recognition (OCR) that is accurate, extremely cheap to deploy, and sample-efficient to customize to novel collections, languages, and character sets. Existing OCR engines, largely designed for small-scale commercial applications in high resource languages, often fall short of these requirements. EffOCR (EfficientOCR), a novel open-source OCR package, meets both the computational and sample efficiency requirements for liberating texts at scale by abandoning the sequence-to-sequence architecture typically used for OCR, which takes representations from a learned vision model as inputs to a learned language model. Instead, EffOCR models OCR as a character or word-level image retrieval problem. EffOCR is cheap and sample efficient to train, as the model only needs to learn characters’ visual appearance and not how they are used in sequence to form language. Models in the EffOCR model zoo can be deployed off-the-shelf with only a few lines of code and include lightweight models designed for mobile phones that are extremely cheap to deploy. Importantly, EffOCR also allows for easy, sample efficient customization with a simple model training interface and minimal labeling requirements due to its sample efficiency. We illustrate the utility of EffOCR by cheaply and accurately digitizing 20 million historical U.S. newspaper scans, evaluating zero-shot performance on randomly selected documents from the U.S. National Archives, and accurately digitizing a Japanese document collection for which all other OCR solutions failed. | Tom Bryan, Jacob Carlson, Abhishek Arora, Melissa Dell |  |
| 54 |  |  [Frontmatter](https://aclanthology.org/2023.emnlp-industry.0) |  | 0 |  |  |  |
| 55 |  |  [BeautifulPrompt: Towards Automatic Prompt Engineering for Text-to-Image Synthesis](https://doi.org/10.18653/v1/2023.emnlp-industry.1) |  | 0 | Recently, diffusion-based deep generative models (e.g., Stable Diffusion) have shown impressive results in text-to-image synthesis. However, current text-to-image models often require multiple passes of prompt engineering by humans in order to produce satisfactory results for real-world applications. We propose BeautifulPrompt, a deep generative model to produce high-quality prompts from very simple raw descriptions, which enables diffusion-based models to generate more beautiful images. In our work, we first fine-tuned the BeautifulPrompt model over low-quality and high-quality collecting prompt pairs. Then, to ensure that our generated prompts can generate more beautiful images, we further propose a Reinforcement Learning with Visual AI Feedback technique to fine-tune our model to maximize the reward values of the generated prompts, where the reward values are calculated based on the PickScore and the Aesthetic Scores. Our results demonstrate that learning from visual AI feedback promises the potential to improve the quality of generated prompts and images significantly. We further showcase the integration of BeautifulPrompt to a cloud-native AI platform to provide better text-to-image generation service in the cloud. | Tingfeng Cao, Chengyu Wang, Bingyan Liu, Ziheng Wu, Jinhui Zhu, Jun Huang |  |
| 56 |  |  [Enhancing Language Model with Unit Test Techniques for Efficient Regular Expression Generation](https://doi.org/10.18653/v1/2023.emnlp-industry.2) |  | 0 | Recent research has investigated the use of generative language models to produce regular expressions with semantic-based approaches. However, these approaches have shown shortcomings in practical applications, particularly in terms of functional correctness, which refers to the ability to reproduce the intended function inputs by the user. To address this issue, we present a novel method called Unit-Test Driven Reinforcement Learning (UTD-RL). Our approach differs from previous methods by taking into account the crucial aspect of functional correctness and transforming it into a differentiable gradient feedback using policy gradient techniques. In which functional correctness can be evaluated through Unit Tests, a testing method that ensures regular expressions meets its design and performs as intended. Experiments conducted on three public datasets demonstrate the effectiveness of the proposed method in generating regular expressions. This method has been employed in a regulatory scenario where regular expressions can be utilized to ensure that all online content is free from non-compliant elements, thereby significantly reducing the workload of relevant personnel. | Chenhui Mao, Xiexiong Lin, Xin Jin, Xin Zhang |  |
| 57 |  |  [A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models](https://doi.org/10.18653/v1/2023.emnlp-industry.3) |  | 0 | Large language models have become a vital component in modern NLP, achieving state of the art performance in a variety of tasks. However, they are often inefficient for real-world deployment due to their expensive inference costs. Knowledge distillation is a promising technique to improve their efficiency while retaining most of their effectiveness. In this paper, we reproduce, compare and analyze several representative methods for task-agnostic (general-purpose) distillation of Transformer language models. Our target of study includes Output Distribution (OD) transfer, Hidden State (HS) transfer with various layer mapping strategies, and Multi-Head Attention (MHA) transfer based on MiniLMv2. Through our extensive experiments, we study the effectiveness of each method for various student architectures in both monolingual (English) and multilingual settings. Overall, we show that MHA transfer based on MiniLMv2 is generally the best option for distillation and explain the potential reasons behind its success. Moreover, we show that HS transfer remains as a competitive baseline, especially under a sophisticated layer mapping strategy, while OD transfer consistently lags behind other approaches. Findings from this study helped us deploy efficient yet effective student models for latency-critical applications. | Takuma Udagawa, Aashka Trivedi, Michele Merler, Bishwaranjan Bhattacharjee |  |
| 58 |  |  [Towards Effective Automatic Debt Collection with Persona Awareness](https://doi.org/10.18653/v1/2023.emnlp-industry.4) |  | 0 | Understanding debtor personas is crucial for collectors to empathize with debtors and develop more effective collection strategies. In this paper, we take the first step towards comprehensively investigating the significance of debtor personas and present a successful commercial practice on automatic debt collection agents. Specifically, we organize the debtor personas into a taxonomy and construct a persona-aware conversation dataset. Building upon it, we implement a simple yet effective persona-aware agent called PAD. After two-month online testing, PAD increases the recovery rate by 3.31% and collects an additional ~100K RMB. Our commercial practice brings inspiration to the debt collection industry by providing an effective automatic solution. | Tong Zhang, Junhong Liu, Chen Huang, Jia Liu, Hongru Liang, Zujie Wen, Wenqiang Lei |  |
| 59 |  |  [Gatekeeper to save COGS and improve efficiency of Text Prediction](https://doi.org/10.18653/v1/2023.emnlp-industry.5) |  | 0 | The text prediction (TP) workflow calls a Large Language Model (LLM), almost, after every character to get subsequent sequence of characters, till user accepts a suggestion. The confidence score of the prediction is commonly used for filtering the results to ensure that only correct predictions are shown to user. As LLMs require massive amounts of computation and storage, such an approach incurs network and high execution cost. So, we propose a Model gatekeeper (GK) to stop the LLM calls that will result in incorrect predictions at client application level itself. This way a GK can save cost of model inference and improve user experience by not showing the incorrect predictions. We demonstrate that use of a model gatekeeper saved approx 46.6% of COGS for TP, at the cost of approx 4.5% loss in character saving. Use of GK also improved the efficiency (suggestion rate) of TP model by 73%. | Nidhi Tiwari, Sneha Kola, Milos Milunovic, Siqing Chen, Marjan Slavkovski |  |
| 60 |  |  [Efficient Transformer Knowledge Distillation: A Performance Review](https://doi.org/10.18653/v1/2023.emnlp-industry.6) |  | 0 | As pretrained transformer language models continue to achieve state-of-the-art performance, the Natural Language Processing community has pushed for advances in model compression and efficient attention mechanisms to address high computational requirements and limited input sequence length. Despite these separate efforts, no investigation has been done into the intersection of these two fields. In this work, we provide an evaluation of model compression via knowledge distillation on efficient attention transformers. We provide cost-performance trade-offs for the compression of state-of-the-art efficient attention architectures and the gains made in performance in comparison to their full attention counterparts. Furthermore, we introduce a new long-context Named Entity Recognition dataset, GONERD, to train and test the performance of NER models on long sequences. We find that distilled efficient attention transformers can preserve a significant amount of original model performance, preserving up to 98.6% across short-context tasks (GLUE, SQUAD, CoNLL-2003), up to 94.6% across long-context Question-and-Answering tasks (HotpotQA, TriviaQA), and up to 98.8% on long-context Named Entity Recognition (GONERD), while decreasing inference times by up to 57.8%. We find that, for most models on most tasks, performing knowledge distillation is an effective method to yield high-performing efficient attention models with low costs. | Nathan Brown, Ashton Williamson, Tahj Anderson, Logan Lawrence |  |
| 61 |  |  [CDD: A Large Scale Dataset for Legal Intelligence Research](https://doi.org/10.18653/v1/2023.emnlp-industry.7) |  | 0 | As an important application of Artificial Intelligence, legal intelligence has recently attracted the attention of many researchers. Previous works investigated diverse issues like predicting crimes, predicting outcomes of judicial debates, or extracting information/knowledge from various kinds of legal documents. Although many advances have been made, the research on supporting prediction of court judgments remains relatively scarce, while the lack of large-scale data resources limits the development of this research.In this paper, we present a novel, large-size Court Debate Dataset (CDD), which includes 30,481 court cases, totaling 1,144,425 utterances. CDD contains real-world conversations involving judges, plaintiffs and defendants in court trials. To construct this dataset we have invited experienced judges to design appropriate labels for data records. We then asked law school students to provide annotations based on the defined labels. The dataset can be applied to several downstream tasks, such as text summarization, dialogue generation, text classification, etc. We introduce the details of the different tasks in the rapidly developing field of legal intelligence, the research of which can be fostered thanks to our dataset, and we provide the corresponding benchmark performance. | Changzhen Ji, Yating Zhang, Adam Jatowt, Haipang Wu |  |
| 62 |  |  [MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning](https://doi.org/10.18653/v1/2023.emnlp-industry.8) |  | 0 | In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online (https://github.com/noetits/MUST_P-SRL) that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-related fields. | Noé Tits |  |
| 63 |  |  [Personalized Dense Retrieval on Global Index for Voice-enabled Conversational Systems](https://doi.org/10.18653/v1/2023.emnlp-industry.9) |  | 0 | Voice-controlled AI dialogue systems are susceptible to noise from phonetic variations and failure to resolve ambiguous entities. Typically, personalized entity resolution (ER) and/or query rewrites (QR) are deployed to recover from these error modes. Previous work in this field achieves personalization by constraining retrieval search space to personalized indices built from user’s historical interactions with the device. While constrained retrieval achieves high precision, predictions are limited to entities in recent user history, which offers low coverage of future requests. Further, maintaining individual indices for millions of users is memory intensive and difficult to scale. In this work, we propose a personalized entity retrieval system that is robust to phonetic noise and ambiguity but is not limited to a personalized index. We achieve this by embedding user listening preferences into a contextual query embedding used in retrieval. We demonstrate our model’s ability to correct multiple error modes and show 91% improvement over baseline on the entity retrieval task. Finally, we optimize the end-to-end approach to fit within online latency constraints while maintaining gains in performance. | Masha Belyi, Charlotte Dzialo, Chaitanya Dwivedi, Prajit Muppidi, Kanna Shimizu |  |
| 64 |  |  [Text2Topic: Multi-Label Text Classification System for Efficient Topic Detection in User Generated Content with Zero-Shot Capabilities](https://doi.org/10.18653/v1/2023.emnlp-industry.10) |  | 0 | Multi-label text classification is a critical task in the industry. It helps to extract structured information from large amount of textual data. We propose Text to Topic (Text2Topic), which achieves high multi-label classification performance by employing a Bi-Encoder Transformer architecture that utilizes concatenation, subtraction, and multiplication of embeddings on both text and topic. Text2Topic also supports zero-shot predictions, produces domain-specific text embeddings, and enables production-scale batch-inference with high throughput. The final model achieves accurate and comprehensive results compared to state-of-the-art baselines, including large language models (LLMs). In this study, a total of 239 topics are defined, and around 1.6 million text-topic pairs annotations (in which 200K are positive) are collected on approximately 120K texts from 3 main data sources on Booking.com. The data is collected with optimized smart sampling and partial labeling. The final Text2Topic model is deployed on a real-world stream processing platform, and it outperforms other models with 92.9% micro mAP, as well as a 75.8% macro mAP score. We summarize the modeling choices which are extensively tested through ablation studies, and share detailed in-production decision-making steps. | Fengjun Wang, Moran Beladev, Ofri Kleinfeld, Elina Frayerman, Tal Shachar, Eran Fainman, Karen Lastmann Assaraf, Sarai Mizrachi, Benjamin Wang |  |
| 65 |  |  [Deep Metric Learning to Hierarchically Rank - An Application in Product Retrieval](https://doi.org/10.18653/v1/2023.emnlp-industry.11) |  | 0 | Most e-commerce search engines use customer behavior signals to augment lexical matching and improve search relevance. Many e-commerce companies like Amazon, Alibaba, Ebay etc. operate in multiple countries with country specific stores. However, customer behavior data is sparse in newer stores. To compensate for sparsity of behavioral data in low traffic stores, search engines often use cross-listed products in some form. However, cross-listing across stores is not uniform and in many cases itself sparse. In this paper, we develop a model to identify duplicate and near-duplicate products across stores. Such a model can be used to unify product catalogs worldwide, improve product meta-data or as in our case, use near-duplicate products across multiple to improve search relevance. To capture the product similarity hierarchy, we develop an approach that integrates retrieval and ranking tasks across multiple languages in a single step based on a novel Hierarchical Ranked Multi Similarity (HRMS) Loss that combines Multi-Similarity (MS) loss and Hierarchical Triplet Loss to learn a hierarchical metric space. Our method outperforms strong baselines in terms of catalog coverage and precision of the mappings. We also show via online A/B tests that the product mappings found by our method are successful at improving search quality in low traffic stores, measured in rate of searches with at least one click, significantly by 0.8% and improving cold start product engagement measured as new product clicks significantly by 1.72% in established stores. | Kee Kiat Koo, Ashutosh Joshi, Nishaanth Reddy, Karim Bouyarmane, Ismail B. Tutar, Vaclav Petricek, Changhe Yuan |  |
| 66 |  |  [A Pretrained Language Model for Cyber Threat Intelligence](https://doi.org/10.18653/v1/2023.emnlp-industry.12) |  | 0 | We present a new BERT model for the cybersecurity domain, CTI-BERT, which can improve the accuracy of cyber threat intelligence (CTI) extraction, enabling organizations to better defend against potential cyber threats. We provide detailed information about the domain corpus collection, the training methodology and its effectiveness for a variety of NLP tasks for the cybersecurity domain. The experiments show that CTI-BERT significantly outperforms several general-domain and security-domain models for these cybersecurity applications indicating that the training data and methodology have a significant impact on the model performance. | Youngja Park, Weiqiu You |  |
| 67 |  |  [SAMP: A Model Inference Toolkit of Post-Training Quantization for Text Processing via Self-Adaptive Mixed-Precision](https://doi.org/10.18653/v1/2023.emnlp-industry.13) |  | 0 | The latest industrial inference engines, such as FasterTransformer and TurboTransformers, have verified that half-precision floating point (FP16) and 8-bit integer (INT8) quantization can greatly improve model inference speed. However, the existing INT8 quantization methods are too complicated, and improper usage will lead to model performance damage greatly. In this paper, we develop a toolkit for users to easily quantize their models for inference, in which Self-Adaptive Mixed-Precision (SAMP) is proposed to automatically control quantization rate by a mixed-precision architecture to balance model accuracy and efficiency. Experimental results show that our SAMP toolkit has a higher speedup than PyTorch and FasterTransformer while ensuring the required accuracy. In addition, SAMP is based on a modular design, decoupling the tokenizer, embedding, encoder and target layers, which allows users to handle various downstream tasks and can be seamlessly integrated into PyTorch. | Rong Tian, Zijing Zhao, Weijie Liu, Haoyan Liu, Weiquan Mao, Zhe Zhao, Kan Zhou |  |
| 68 |  |  [KD-Boost: Boosting Real-Time Semantic Matching in E-commerce with Knowledge Distillation](https://doi.org/10.18653/v1/2023.emnlp-industry.14) |  | 0 | Real-time semantic matching is vital to web and product search. Transformer-based models have shown to be highly effective at encoding queries into an embedding space where semantically similar entities (queries or results) are in close proximity. However, the computational complexity of large transformer models limits their utilization for real-time matching. In this paper, we propose KD-Boost, a novel knowledge distillation algorithm designed for real-time semantic matching. KD-Boost trains low latency accurate student models by leveraging soft labels from a teacher model as well as ground truth via pairwise query-product and query-query signal derived from direct audits, user behavior, and taxonomy-based data using custom loss functions. Experiments on internal and external e-commerce datasets demonstrate an improvement of 2-3% ROC-AUC compared to training student models directly, outperforming teacher and SOTA knowledge distillation benchmarks. Simulated online A/B tests using KD-Boost for automated Query Reformulation (QR) indicate a 6.31% increase in query-to-query matching, 2.76% increase in product coverage, and a 2.19% improvement in relevance. | Sanjay Agrawal, Vivek Sembium, Ankith M. S |  |
| 69 |  |  [Multi-teacher Distillation for Multilingual Spelling Correction](https://doi.org/10.18653/v1/2023.emnlp-industry.15) |  | 0 | Accurate spelling correction is a critical step in modern search interfaces, especially in an era of mobile devices and speech-to-text interfaces. For services that are deployed around the world, this poses a significant challenge for multilingual NLP: spelling errors need to be caught and corrected in all languages, and even in queries that use multiple languages. In this paper, we tackle this challenge using multi-teacher distillation. On our approach, a monolingual teacher model is trained for each language/locale, and these individual models are distilled into a single multilingual student model intended to serve all languages/locales. In experiments using open-source data as well as customer data from a worldwide search service, we show that this leads to highly effective spelling correction models that can meet the tight latency requirements of deployed services. | Jingfen Zhang, Xuan Guo, Sravan Bodapati, Christopher Potts |  |
| 70 |  |  [Does Named Entity Recognition Truly Not Scale Up to Real-world Product Attribute Extraction?](https://doi.org/10.18653/v1/2023.emnlp-industry.16) |  | 0 | The key challenge in the attribute-value extraction (AVE) task from e-commerce sites is the scalability to diverse attributes for a large number of products in real-world e-commerce sites. To make AVE scalable to diverse attributes, recent researchers adopted a question-answering (QA)-based approach that additionally inputs the target attribute as a query to extract its values, and confirmed its advantage over a classical approach based on named-entity recognition (NER) on real-word e-commerce datasets. In this study, we argue the scalability of the NER-based approach compared to the QA-based approach, since researchers have compared BERT-based QA-based models to only a weak BiLSTM-based NER baseline trained from scratch in terms of only accuracy on datasets designed to evaluate the QA-based approach. Experimental results using a publicly available real-word dataset revealed that, under a fair setting, BERT-based NER models rival BERT-based QA models in terms of the accuracy, and their inference is faster than the QA model that processes the same product text several times to handle multiple target attributes. | WeiTe Chen, Keiji Shinzato, Naoki Yoshinaga, Yandi Xia |  |
| 71 |  |  [Investigating Table-to-Text Generation Capabilities of Large Language Models in Real-World Information Seeking Scenarios](https://doi.org/10.18653/v1/2023.emnlp-industry.17) |  | 0 | Tabular data is prevalent across various industries, necessitating significant time and effort for users to understand and manipulate for their information-seeking purposes. The advancements in large language models (LLMs) have shown enormous potential to improve user efficiency. However, the adoption of LLMs in real-world applications for table information seeking remains underexplored. In this paper, we investigate the table-to-text capabilities of different LLMs using four datasets within two real-world information seeking scenarios. These include the LogicNLG and our newly-constructed LoTNLG datasets for data insight generation, along with the FeTaQA and our newly-constructed F2WTQ datasets for query-based generation. We structure our investigation around three research questions, evaluating the performance of LLMs in table-to-text generation, automated evaluation, and feedback generation, respectively. Experimental results indicate that the current high-performing LLM, specifically GPT-4, can effectively serve as a table-to-text generator, evaluator, and feedback generator, facilitating users’ information seeking purposes in real-world scenarios. However, a significant performance gap still exists between other open-sourced LLMs (e.g., Vicuna and LLaMA-2) and GPT-4 models. Our data and code are publicly available at https://github.com/yale-nlp/LLM-T2T. | Yilun Zhao, Haowei Zhang, Shengyun Si, Linyong Nan, Xiangru Tang, Arman Cohan |  |
| 72 |  |  [TMID: A Comprehensive Real-world Dataset for Trademark Infringement Detection in E-Commerce](https://doi.org/10.18653/v1/2023.emnlp-industry.18) |  | 0 | Annually, e-commerce platforms incur substantial financial losses due to trademark infringements, making it crucial to identify and mitigate potential legal risks tied to merchant information registered to the platforms. However, the absence of high-quality datasets hampers research in this area. To address this gap, our study introduces TMID, a novel dataset to detect trademark infringement in merchant registrations. This is a real-world dataset sourced directly from Alipay, one of the world’s largest e-commerce and digital payment platforms. As infringement detection is a legal reasoning task requiring an understanding of the contexts and legal rules, we offer a thorough collection of legal rules and merchant and trademark-related contextual information with annotations from legal experts. We ensure the data quality by performing an extensive statistical analysis. Furthermore, we conduct an empirical study on this dataset to highlight its value and the key challenges. Through this study, we aim to contribute valuable resources to advance research into legal compliance related to trademark infringement within the e-commerce sphere. | Tongxin Hu, Zhuang Li, Xin Jin, Lizhen Qu, Xin Zhang |  |
| 73 |  |  [Joint Dialogue Topic Segmentation and Categorization: A Case Study on Clinical Spoken Conversations](https://doi.org/10.18653/v1/2023.emnlp-industry.19) |  | 0 | Utilizing natural language processing techniques in clinical conversations is effective to improve the efficiency of health management workflows for medical staff and patients. Dialogue segmentation and topic categorization are two fundamental steps for processing verbose spoken conversations and highlighting informative spans for downstream tasks. However, in practical use cases, due to the variety of segmentation granularity and topic definition, and the lack of diverse annotated corpora, no generic models are readily applicable for domain-specific applications. In this work, we introduce and adopt a joint model for dialogue segmentation and topic categorization, and conduct a case study on healthcare follow-up calls for diabetes management; we provide insights from both data and model perspectives toward performance and robustness. | Zhengyuan Liu, Siti Umairah Md. Salleh, Hong Choon Oh, Pavitra Krishnaswamy, Nancy F. Chen |  |
| 74 |  |  [AdapterDistillation: Non-Destructive Task Composition with Knowledge Distillation](https://doi.org/10.18653/v1/2023.emnlp-industry.20) |  | 0 | Leveraging knowledge from multiple tasks through introducing a small number of task specific parameters into each transformer layer, also known as adapters, receives much attention recently. However, adding an extra fusion layer to implement knowledge composition not only increases the inference time but also is non-scalable for some applications. To avoid these issues, we propose a two-stage knowledge distillation algorithm called AdapterDistillation. In the first stage, we extract task specific knowledge by using local data to train a student adapter. In the second stage, we distill the knowledge from the existing teacher adapters into the student adapter to help its inference. Extensive experiments on frequently asked question retrieval in task-oriented dialog systems validate the efficiency of AdapterDistillation. We show that AdapterDistillation outperforms existing algorithms in terms of accuracy, resource consumption and inference time. | Junjie Wang, Yicheng Chen, Wangshu Zhang, Sen Hu, Teng Xu, Jing Zheng |  |
| 75 |  |  [PROMINET: Prototype-based Multi-View Network for Interpretable Email Response Prediction](https://doi.org/10.18653/v1/2023.emnlp-industry.21) |  | 0 | Email is a widely used tool for business communication, and email marketing has emerged as a cost-effective strategy for enterprises. While previous studies have examined factors affecting email marketing performance, limited research has focused on understanding email response behavior by considering email content and metadata. This study proposes a Prototype-based Multi-view Network (PROMINET) that incorporates semantic and structural information from email data. By utilizing prototype learning, the PROMINET model generates latent exemplars, enabling interpretable email response prediction. The model maps learned semantic and structural exemplars to observed samples in the training data at different levels of granularity, such as document, sentence, or phrase. The approach is evaluated on two real-world email datasets: the Enron corpus and an in-house Email Marketing corpus. Experimental results demonstrate that the PROMINET model outperforms baseline models, achieving a ~3% improvement in F1 score on both datasets. Additionally, the model provides interpretability through prototypes at different granularity levels while maintaining comparable performance to non-interpretable models. The learned prototypes also show potential for generating suggestions to enhance email text editing and improve the likelihood of effective email responses. This research contributes to enhancing sender-receiver communication and customer engagement in email interactions. | Yuqing Wang, Prashanth Vijayaraghavan, Ehsan Degan |  |
| 76 |  |  [Retrieval-Enhanced Dual Encoder Training for Product Matching](https://doi.org/10.18653/v1/2023.emnlp-industry.22) |  | 0 | Product matching is the task of matching a seller-listed item to an appropriate product. It is a critical task for an e-commerce platform, and the approach needs to be efficient to run in a large-scale setting. A dual encoder approach has been a common practice for product matching recently, due to its high performance and computation efficiency. In this paper, we propose a two-stage training for the dual encoder model. Stage 1 trained a dual encoder to identify the more informative training data. Stage 2 then train on the more informative data to get a better dual encoder model. This technique is a learned approach for building training data. We evaluate the retrieval-enhanced training on two different datasets: a publicly available Large-Scale Product Matching dataset and a real-world e-commerce dataset containing 47 million products. Experiment results show that our approach improved by 2% F1 on the public dataset and 9% F1 on the real-world e-commerce dataset. | Justin Chiu |  |
| 77 |  |  [WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-industry.23) |  | 0 | This paper introduces WordArt Designer, a user-driven framework for artistic typography synthesis, relying on the Large Language Model (LLM). The system incorporates four key modules: the LLM Engine, SemTypo, StyTypo, and TexTypo modules. 1) The LLM Engine, empowered by the LLM (e.g. GPT-3.5), interprets user inputs and generates actionable prompts for the other modules, thereby transforming abstract concepts into tangible designs. 2) The SemTypo module optimizes font designs using semantic concepts, striking a balance between artistic transformation and readability. 3) Building on the semantic layout provided by the SemTypo module, the StyTypo module creates smooth, refined images. 4) The TexTypo module further enhances the design’s aesthetics through texture rendering, enabling the generation of inventive textured fonts. Notably, WordArt Designer highlights the fusion of generative AI with artistic typography. Experience its capabilities on ModelScope: https://www.modelscope.cn/studios/WordArt/WordArt. | JunYan He, ZhiQi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Xianhui Lin, Xiaoyang Kang, Zengke Jin, Yusen Hu, Bin Luo, Yifeng Geng, Xuansong Xie |  |
| 78 |  |  [Lattice Path Edit Distance: A Romanization-aware Edit Distance for Extracting Misspelling-Correction Pairs from Japanese Search Query Logs](https://doi.org/10.18653/v1/2023.emnlp-industry.24) |  | 0 | Edit distance has been successfully used to extract training data, i.e., misspelling-correction pairs, of spelling correction models from search query logs in languages including English. However, the success does not readily apply to Japanese, where misspellings are often dissimilar to correct spellings due to the romanization-based input methods. To address this problem, we introduce lattice path edit distance, which utilizes romanization lattices to efficiently consider all possible romanized forms of input strings. Empirical experiments using Japanese search query logs demonstrated that the lattice path edit distance outperformed baseline methods including the standard edit distance combined with an existing transliterator and morphological analyzer. A training data collection pipeline that uses the lattice path edit distance has been deployed in production at our search engine for over a year. | Nobuhiro Kaji |  |
| 79 |  |  [Learning Multilingual Sentence Representations with Cross-lingual Consistency Regularization](https://doi.org/10.18653/v1/2023.emnlp-industry.25) |  | 0 | Multilingual sentence representations are the foundation for similarity-based bitext mining, which is crucial for scaling multilingual neural machine translation (NMT) system to more languages. In this paper, we introduce MuSR: a one-for-all Multilingual Sentence Representation model that supports 223 languages. Leveraging billions of English-centric parallel corpora, we train a multilingual Transformer encoder, coupled with an auxiliary Transformer decoder, by adopting a multilingual NMT framework with CrossConST, a cross-lingual consistency regularization technique proposed in Gao et al. (2023). Experimental results on multilingual similarity search and bitext mining tasks show the effectiveness of our approach. Specifically, MuSR achieves superior performance over LASER3 (Heffernan et al., 2022) which consists of 148 independent multilingual sentence encoders. | Pengzhi Gao, Liwen Zhang, Zhongjun He, Hua Wu, Haifeng Wang |  |
| 80 |  |  [Unveiling Identity Biases in Toxicity Detection : A Game-Focused Dataset and Reactivity Analysis Approach](https://doi.org/10.18653/v1/2023.emnlp-industry.26) |  | 0 | Identity biases arise commonly from annotated datasets, can be propagated in language models and can cause further harm to marginal groups. Existing bias benchmarking datasets are mainly focused on gender or racial biases and are made to pinpoint which class the model is biased towards. They also are not designed for the gaming industry, a concern for models built for toxicity detection in videogames’ chat. We propose a dataset and a method to highlight oversensitive terms using reactivity analysis and the model’s performance. We test our dataset against ToxBuster, a language model developed by Ubisoft fine-tuned for toxicity detection on multiplayer videogame’s written chat, and Perspective API. We find that these toxicity models often automatically tag terms related to a community’s identity as toxic, which prevents members of already marginalized groups to make their presence known or have a mature / normal conversation. Through this process, we have generated an interesting list of terms that trigger the models to varying degrees, along with insights on establishing a baseline through human annotations. | Josiane Van Dorpe, Zachary Yang, Nicolas GrenonGodbout, Grégoire Winterstein |  |
| 81 |  |  [ORANGE: Text-video Retrieval via Watch-time-aware Heterogeneous Graph Contrastive Learning](https://doi.org/10.18653/v1/2023.emnlp-industry.27) |  | 0 | With the explosive growth of short-video data on industrial video-sharing platforms such as TikTok and YouTube, text-video retrieval techniques have become increasingly important. Most existing works for text-video retrieval focus on designing informative representation learning methods and delicate matching mechanisms, which leverage the content information of queries and videos themselves (i.e., textual information of queries and multimodal information of videos). However, real-world scenarios often involve brief, ambiguous queries and low-quality videos, making content-based retrieval less effective. In order to accommodate various search requirements and enhance user satisfaction, this study introduces a novel Text-video Retrieval method via Watch-time-aware Heterogeneous Graph Contrastive Learning (termed ORANGE). This approach aims to learn informative embeddings for queries and videos by leveraging both content information and the abundant relational information present in video-search scenarios. Specifically, we first construct a heterogeneous information graph where nodes represent domain objects (e.g., query, video, tag) and edges represent rich relations among these objects. Afterwards, a meta-path-guided heterogeneous graph attention encoder with the awareness of video watch time is devised to encode various semantic aspects of query and video nodes. To train our model, we introduce a meta-path-wise contrastive learning paradigm that facilitates capturing dependencies across multiple semantic relations, thereby enhancing the obtained embeddings. Finally, when deployed online, for new queries non-existent in the constructed graph, a bert-based query encoder distilled from our ORANGE is employed. Offline experiments conducted on a real-world dataset demonstrate the effectiveness of our ORANGE. Moreover, it has been implemented in the matching stage of an industrial online video-search service, where it exhibited statistically significant improvements over the online baseline in an A/B test. | Yucheng Lin, Tim Chang, Yaning Chang, Jianqiang Ma, Donghui Li, Ting Peng, Zang Li, Zhiyi Zhou, Feng Wang |  |
| 82 |  |  [Compute-Efficient Churn Reduction for Conversational Agents](https://doi.org/10.18653/v1/2023.emnlp-industry.28) |  | 0 | Model churn occurs when re-training a model yields different predictions despite using the same data and hyper-parameters. Churn reduction is crucial for industry conversational systems where users expect consistent results for the same queries. In this setting, compute resources are often limited due to latency requirements during serving and overall time constraints during re-training. To address this issue, we propose a compute-efficient method that mitigates churn without requiring extra resources for training or inference. Our approach involves a lightweight data pre-processing step that pairs semantic parses based on their “function call signature” and encourages similarity through an additional loss based on Jensen-Shannon Divergence. We validate the effectiveness of our method in three scenarios: academic (+3.93 percent improvement on average in a churn reduction metric), simulated noisy data (+8.09), and industry (+5.28) settings. | Christopher Hidey, Sarthak Sarthak |  |
| 83 |  |  [Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering](https://doi.org/10.18653/v1/2023.emnlp-industry.29) |  | 0 | Large Language Model (LLM) has gained popularity and achieved remarkable results in open-domain tasks, but its performance in real industrial domain-specific scenarios is average due to its lack of specific domain knowledge. This issue has attracted widespread attention, but there are few relevant benchmarks available. In this paper, we provide a benchmark Question Answering (QA) dataset named MSQA, centered around Microsoft products and IT technical problems encountered by customers. This dataset contains industry cloud-specific QA knowledge, an area not extensively covered in general LLMs, making it well-suited for evaluating methods aiming to enhance LLMs’ domain-specific capabilities. In addition, we propose a new model interaction paradigm that can empower LLM to achieve better performance on domain-specific tasks where it is not proficient. Extensive experiments demonstrate that the approach following our method outperforms the commonly used LLM with retrieval methods. We make our source code and sample data available at: https://aka.ms/Microsoft_QA. | Fangkai Yang, Pu Zhao, Zezhong Wang, Lu Wang, Bo Qiao, Jue Zhang, Mohit Garg, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang |  |
| 84 |  |  [Enhancing Extreme Multi-Label Text Classification: Addressing Challenges in Model, Data, and Evaluation](https://doi.org/10.18653/v1/2023.emnlp-industry.30) |  | 0 | Extreme multi-label text classification is a prevalent task in industry, but it frequently encounters challenges in terms of machine learning perspectives, including model limitations, data scarcity, and time-consuming evaluation. This paper aims to mitigate these issues by introducing novel approaches. Firstly, we propose a label ranking model as an alternative to the conventional SciBERT-based classification model, enabling efficient handling of large-scale labels and accommodating new labels. Secondly, we present an active learning-based pipeline that addresses the data scarcity of new labels during the update of a classification system. Finally, we introduce ChatGPT to assist with model evaluation. Our experiments demonstrate the effectiveness of these techniques in enhancing the extreme multi-label text classification task. | Dan Li, Zi Long Zhu, Janneke van de Loo, Agnes Masip Gomez, Vikrant Yadav, Georgios Tsatsaronis, Zubair Afzal |  |
| 85 |  |  [Query-aware Multi-modal based Ranking Relevance in Video Search](https://doi.org/10.18653/v1/2023.emnlp-industry.31) |  | 0 | Relevance ranking system plays a crucial role in video search on streaming platforms. Most relevance ranking methods focus on text modality, incapable of fully exploiting cross-modal cues present in video. Recent multi-modal models have demonstrated promise in various vision-language tasks but provide limited help for downstream query-video relevance tasks due to the discrepency between relevance ranking-agnostic pre-training objectives and the real video search scenarios that demand comprehensive relevance modeling. To address these challenges, we propose a QUery-Aware pre-training model with multi-modaLITY (QUALITY) that incorporates hard-mined query information as alignment targets and utilizes video tag information for guidance. QUALITY is integrated into our relevance ranking model, which leverages multi-modal knowledge and improves ranking optimization method based on ordinal regression. Extensive experiments show our proposed model significantly enhances video search performance. | Chengcan Ye, Ting Peng, Tim Chang, Zhiyi Zhou, Feng Wang |  |
| 86 |  |  [Coordinated Replay Sample Selection for Continual Federated Learning](https://doi.org/10.18653/v1/2023.emnlp-industry.32) |  | 0 | Continual Federated Learning (CFL) combines Federated Learning (FL), the decentralized learning of a central model on a number of client devices that may not communicate their data, and Continual Learning (CL), the learning of a model from a continual stream of data without keeping the entire history. In CL, the main challenge is forgetting what was learned from past data. While replay-based algorithms that keep a small pool of past training data are effective to reduce forgetting, only simple replay sample selection strategies have been applied to CFL in prior work, and no previous work has explored coordination among clients for better sample selection. To bridge this gap, we adapt a replay sample selection objective based on loss gradient diversity to CFL and propose a new relaxation-based selection of samples to optimize the objective. Next, we propose a practical algorithm to coordinate gradient-based replay sample selection across clients without communicating private data. We benchmark our coordinated and uncoordinated replay sample selection algorithms against random sampling-based baselines with language models trained on a large scale de-identified real-world text dataset. We show that gradient-based sample selection methods both boost performance and reduce forgetting compared to random sampling methods, with our coordination method showing gains early in the low replay size regime (when the budget for storing past data is small). | Jack Good, Jimit Majmudar, Christophe Dupuy, Jixuan Wang, Charith Peris, Clement Chung, Richard S. Zemel, Rahul Gupta |  |
| 87 |  |  [Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective](https://doi.org/10.18653/v1/2023.emnlp-industry.33) |  | 0 | This paper studies how to effectively build meeting summarization systems for real-world usage using large language models (LLMs). For this purpose, we conduct an extensive evaluation and comparison of various closed-source and open-source LLMs, namely, GPT-4, GPT-3.5, PaLM-2, and LLaMA-2. Our findings reveal that most closed-source LLMs are generally better in terms of performance. However, much smaller open-source models like LLaMA-2 (7B and 13B) could still achieve performance comparable to the large closed-source models even in zero-shot scenarios. Considering the privacy concerns of closed-source models for only being accessible via API, alongside the high cost associated with using fine-tuned versions of the closed-source models, the opensource models that can achieve competitive performance are more advantageous for industrial use. Balancing performance with associated costs and privacy concerns, the LLaMA-2-7B model looks more promising for industrial usage. In sum, this paper offers practical insights on using LLMs for real-world business meeting summarization, shedding light on the trade-offs between performance and cost. | Md. Tahmid Rahman Laskar, XueYong Fu, Cheng Chen, Shashi Bhushan TN |  |
| 88 |  |  [Creator Context for Tweet Recommendation](https://doi.org/10.18653/v1/2023.emnlp-industry.34) |  | 0 | When discussing a tweet, people usually not only refer to the content it delivers, but also to the person behind the tweet. In other words, grounding the interpretation of the tweet in the context of its creator plays an important role in deciphering the true intent and the importance of the tweet. In this paper, we attempt to answer the question of how creator context should be used to advance tweet understanding. Specifically, we investigate the usefulness of different types of creator context, and examine different model structures for incorporating creator context in tweet modeling. We evaluate our tweet understanding models on a practical use case – recommending relevant tweets to news articles. This use case already exists in popular news apps, and can also serve as a useful assistive tool for journalists. We discover that creator context is essential for tweet understanding, and can improve application metrics by a large margin. However, we also observe that not all creator contexts are equal. Creator context can be time sensitive and noisy. Careful creator context selection and deliberate model structure design play an important role in creator context effectiveness. | Spurthi Amba Hombaiah, Tao Chen, Mingyang Zhang, Michael Bendersky, Marc Najork, Matt Colen, Sergey Levi, Vladimir Ofitserov, Tanvir Amin |  |
| 89 |  |  [AdaBERT-CTC: Leveraging BERT-CTC for Text-Only Domain Adaptation in ASR](https://doi.org/10.18653/v1/2023.emnlp-industry.35) |  | 0 | End-to-end (E2E) automatic speech recognition (ASR) models are becoming increasingly popular in commercial applications, such as virtual assistants, closed captioning, and dictation systems. The accuracy of the ASR is crucial to their success. However, E2E models still struggle to recognize out-of-domain words such as proper nouns and domain-specific terms. In this paper we introduce AdaBERT-CTC, a domain adaptation technique that relies solely on textual data. Our method allows for text-only adaptation by fine-tuning a pre-trained self-supervised text encoder model. Additionally, we show that our method can be made parameter-efficient by adding bottleneck adapters to the pre-trained model. This allows for adaptation with less than a 5% increase in parameters and minimal computational overhead during inference. We demonstrate that our approach outperforms the base BERT-CTC model by up to 14% relative word error rate improvement on several out-of-domain, publicly available datasets. | Tyler Vuong, Karel Mundnich, Dhanush Bekal, Veera Raghavendra Elluru, Srikanth Ronanki, Sravan Bodapati |  |
| 90 |  |  [Conversing with databases: Practical Natural Language Querying](https://doi.org/10.18653/v1/2023.emnlp-industry.36) |  | 0 | In this work, we designed, developed and released in production DataQue – a hybrid NLQ (Natural Language Querying) system for conversational DB querying. We address multiple practical problems that are not accounted for in public Text-to-SQL solutions – numerous complex implied conditions in user questions, jargon and abbreviations, custom calculations, non-SQL operations, a need to inject all those into pipeline fast and to have guaranteed parsing results for demanding users, cold-start problem. The DataQue processing pipeline for Text-to-SQL translation consists of 10-15 model-based and rule-based components that allows to tightly control the processing. | Denis Kochedykov, Fenglin Yin, Sreevidya Khatravath |  |
| 91 |  |  [AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications](https://doi.org/10.18653/v1/2023.emnlp-industry.37) |  | 0 | Adversarially testing large language models (LLMs) is crucial for their safe and responsible deployment in practice. We introduce an AI-assisted approach for automated generation of adversarial evaluation datasets to test the safety of LLM generations on new downstream applications. We call it AART AI-assisted Red-Teaming - an automated alternative to current manual red-teaming efforts. AART offers a data generation and augmentation pipeline of reusable and customizable recipes that reduce significantly human effort and enable integration of adversarial testing earlier in new product development. AART generates evaluation datasets with high diversity of content characteristics critical for effective adversarial testing (e.g. sensitive and harmful concepts, specific to a wide range of cultural and geographic regions and application scenarios). The data generation is steered by AI-assisted recipes to define, scope and prioritize diversity within a new application context. This feeds into a structured LLM-generation process that scales up evaluation priorities. This provides transparency of developers evaluation intentions and enables quick adaptation to new use cases and newly discovered model weaknesses. Compared to some of the state-of-the-art tools AART shows promising results in terms of concept coverage and data quality. | Bhaktipriya Radharapu, Kevin Robinson, Lora Aroyo, Preethi Lahoti |  |
| 92 |  |  [Speakerly: A Voice-based Writing Assistant for Text Composition](https://doi.org/10.18653/v1/2023.emnlp-industry.38) |  | 0 | We present Speakerly, a new real-time voice-based writing assistance system that helps users with text composition across various use cases such as emails, instant messages, and notes. The user can interact with the system through instructions or dictation, and the system generates a well-formatted and coherent document. We describe the system architecture and detail how we address the various challenges while building and deploying such a system at scale. More specifically, our system uses a combination of small, task-specific models as well as pre-trained language models for fast and effective text composition while supporting a variety of input modes for better usability. | Dhruv Kumar, Vipul Raheja, Alice KaiserSchatzlein, Robyn Perry, Apurva Joshi, Justin HuguesNuger, Samuel Lou, Navid Chowdhury |  |
| 93 |  |  [Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks](https://doi.org/10.18653/v1/2023.emnlp-industry.39) |  | 0 | The most recent large language models (LLMs) such as ChatGPT and GPT-4 have shown exceptional capabilities of generalist models, achieving state-of-the-art performance on a wide range of NLP tasks with little or no adaptation. How effective are such models in the finance domain? Understanding this basic question would have a significant impact on many downstream financial analytical tasks. In this paper, we conduct empirical studies and provide experimental evidences of their performance on a wide variety of financial text analytical problems, using eight benchmark datasets from five categories of tasks. We report both the strengths and limitations of the current models by comparing them to the state-of-the-art fine-tuned approaches and the recently released domain-specific pretrained models. We hope our study can help to understand the capability of the existing models in the financial domain and facilitate further improvements. | Xianzhi Li, Samuel Chan, Xiaodan Zhu, Yulong Pei, Zhiqiang Ma, Xiaomo Liu, Sameena Shah |  |
| 94 |  |  [CL-QR: Cross-Lingual Enhanced Query Reformulation for Multi-lingual Conversational AI Agents](https://doi.org/10.18653/v1/2023.emnlp-industry.40) |  | 0 | The growing popularity of conversational AI agents such as Alexa, Google Assistant, and Siri rely on accurate spoken language comprehension. The query reformulation (QR) method, which reformulates defective user queries, has been broadly adopted to mitigate the challenges posed by understanding user’s intent from imperfect spoken recognition result. However, due to the scarcity of non-English QR labels, providing high-quality QR for non-English users still remains a challenge. This work proposes a novel cross-lingual QR framework, CL-QR, to leverage the abundant reformulation resources in English to improve non-English QR performance. The proposed work also proposes a Module-wise Mutually-supervised Feedback learning (MMF) algorithm to enable the continually self-improving of the CL-QR, which alleviates the lack of cross-lingual QR training data and enhances the delivery of high-quality reformulations learned in English for multilingual queries. Both offline evaluation and online A/B testing demonstrates the effectiveness of the proposed method. | Zhongkai Sun, Zhengyang Zhao, Sixing Lu, Chengyuan Ma, Xiaohu Liu, Xing Fan, Wei Shen, Chenlei Guo |  |
| 95 |  |  [Improving Contextual Query Rewrite for Conversational AI Agents through User-preference Feedback Learning](https://doi.org/10.18653/v1/2023.emnlp-industry.41) |  | 0 | Contextual query rewriting (CQR) is a crucial component in Conversational AI agents, leveraging the contextual information from previous user-agent conversations to improve the comprehension of current user intent. However, traditional CQR methods often concentrate on supervised fine-tuning only, neglecting the opportunities to learn from user feedback to align with user preferences. Inspired by recent advances in learning from human feedback (LHF), this paper proposes a novel Preference Aligned Contextual Query Rewriting (PA-CQR) framework to enhance the CQR model’s capability in generating user preference-aligned rewrites. This paper also investigates the efficacy of various state-of-the-art feedback learning algorithms on the CQR task, and proposes a novel Dynamic Direct Preference Optimization (Dynamic DPO) algorithm to better adapt the DPO algorithm to large-scale CQR training. Experiments on large-scale real-world CQR data set demonstrate the superiority of the proposed PA-CQR framework and the Dynamic DPO. | Zhongkai Sun, Yingxue Zhou, Jie Hao, Xing Fan, Yanbin Lu, Chengyuan Ma, Wei Shen, Chenlei Guo |  |
| 96 |  |  [Scaling Neural ITN for Numbers and Temporal Expressions in Tamil: Findings for an Agglutinative Low-resource Language](https://doi.org/10.18653/v1/2023.emnlp-industry.42) |  | 0 | ITN involves rewriting the verbalised form of text from spoken transcripts to its corresponding written form. The task inherently expects challenges in identifying ITN entries due to spelling variations in words arising out of dialects, transcription errors etc. Additionally, in Tamil, word boundaries between adjacent words in a sentence often get obscured due to Punarchi, i.e. phonetic transformation of these boundaries. Being morphologically rich, the words in Tamil show a high degree of agglutination due to inflection and clitics. The combination of such factors leads to a high degree of surface-form variations, making scalability with pure rule-based approaches difficult. Instead, we experiment with fine-tuning three pre-trained neural LMs, consisting of a seq2seq model (s2s), a non-autoregressive text editor (NAR) and a sequence tagger + rules combination (tagger). While the tagger approach works best in a fully-supervised setting, s2s performs the best (98.05 F-Score) when augmented with additional data, via bootstrapping and data augmentation (DA&B). S2S reports a cumulative percentage improvement of 20.1 %, and statistically significant gains for all our models with DA&B. Compared to a fully supervised setup, bootstrapping alone reports a percentage improvement as high as 14.12 %, even with a small seed set of 324 ITN entries. | Bhavuk Singhal, Sindhuja Gopalan, Amrith Krishna, Malolan Chetlur |  |
| 97 |  |  [EELBERT: Tiny Models through Dynamic Embeddings](https://doi.org/10.18653/v1/2023.emnlp-industry.43) |  | 0 | We introduce EELBERT, an approach for compression of transformer-based models (e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is achieved by replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations. Since the input embedding layer occupies a large portion of the model size, especially for the smaller BERT variants, replacing this layer with an embedding computation function helps us reduce the model size significantly. Empirical evaluation on the GLUE benchmark shows that our BERT variants (EELBERT) suffer minimal regression compared to the traditional BERT models. Through this approach, we are able to develop our smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully trained BERT-tiny, while being 15x smaller (1.2 MB) in size. | Gabrielle Cohn, Rishika Agarwal, Deepanshu Gupta, Siddharth Patwardhan |  |
| 98 |  |  [Gold Standard Bangla OCR Dataset: An In-Depth Look at Data Preprocessing and Annotation Processes](https://doi.org/10.18653/v1/2023.emnlp-industry.44) |  | 0 | This research paper focuses on developing an improved Bangla Optical Character Recognition (OCR) system, addressing the challenges posed by the complexity of Bangla text structure, diverse handwriting styles, and the scarcity of comprehensive datasets. Leveraging recent advancements in Deep Learning and OCR techniques, we anticipate a significant enhancement in the performance of Bangla OCR by utilizing a large and diverse collection of labeled Bangla text image datasets. This study introduces the most extensive gold standard corpus for Bangla characters and words, comprising over 4 million human-annotated images. Our dataset encompasses various document types, such as Computer Compose, Letterpress, Typewriters, Outdoor Banner-Poster, and Handwritten documents, gathered from diverse sources. The entire corpus has undergone meticulous human annotation, employing a controlled annotation procedure consisting of three-step annotation and one-step validation, ensuring adherence to gold standard criteria. This paper provides a comprehensive overview of the complete data collection procedure. The ICT Division, Government of the People’s Republic of Bangladesh, will make the dataset publicly available, facilitating further research and development in Bangla OCR and related domains. | Hasmot Ali, AKM Shahariar Azad Rabby, Md. Majedul Islam, A. k. m Mahamud, Nazmul Hasan, Fuad Rahman |  |
| 99 |  |  [PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching](https://doi.org/10.18653/v1/2023.emnlp-industry.45) |  | 0 | Instruction fine-tuning has conventionally been employed to adapt Large Language Models (LLMs) to a variety of diverse tasks. Nonetheless, this technique often necessitates substantial computational resources, making it impractical for deployment by individuals or small-scale entities. Recently, Low-Rank Adaptation (LoRA) has become a promising alternative, offering tuning capabilities with reduced resource overhead. However, attaining satisfactory performance through the fine-tuning of LoRA is a non-trivial challenge. In this paper, we propose PILLOW, which aims to improve LoRA’s performance by leveraging LLM’s in-context learning capability through prompt matching via reinforcement learning in resource-constrained environments. Specifically, PILLOW incorporates a matching network that selects prompts from a user-defined pool, concatenates the optimal prompts given the user instruction, and performs inference using the LoRA-fine-tuned LLMs. Compared with typical instruction fine-tuning methods, PILLOW exhibits commensurate performance on various evaluation metrics, utilizing only consumer-grade GPU resources and exhibiting a large increase in training efficiency. | Zhenting Qi, Xiaoyu Tan, Shaojie Shi, Chao Qu, Yinghui Xu, Yuan Qi |  |
| 100 |  |  [Welcome to the Real World: Efficient, Incremental and Scalable Key Point Analysis](https://doi.org/10.18653/v1/2023.emnlp-industry.46) |  | 0 | Key Point Analysis (KPA) is an emerging summarization framework, which extracts the main points from a collection of opinions, and quantifies their prevalence. It has been successfully applied to diverse types of data, including arguments, user reviews and survey responses. Despite the growing academic interest in KPA, little attention has been given to the practical challenges of implementing a KPA system in production. This work presents a deployed KPA system, which regularly serves multiple teams in our organization. We discuss the main challenges we faced while building a real-world KPA system, as well as the architecture and algorithmic improvements we developed to address these challenges. Specifically, we focus on efficient matching of sentences to key points, incremental processing, scalability and resiliency. The value of our contributions is demonstrated in an extensive set of experiments, over five existing and novel datasets. Finally, we describe several use cases of the deployed system, which illustrate its practical value. | Lilach Eden, Yoav Kantor, Matan Orbach, Yoav Katz, Noam Slonim, Roy BarHaim |  |
| 101 |  |  [Automatic Linking of Judgements to UK Supreme Court Hearings](https://doi.org/10.18653/v1/2023.emnlp-industry.47) |  | 0 | One the most important archived legal material in the UK is the Supreme Court published judgements and video recordings of court sittings for the decided cases. The impact of Supreme Court published material extends far beyond the parties involved in any given case as it provides landmark rulings on arguable points of law of the greatest public and constitutional importance. However, the recordings of a case are usually very long which makes it both time and effort consuming for legal professionals to study the critical arguments in the legal deliberations. In this research, we summarise the second part of a combined research-industrial project for building an automated tool designed specifically to link segments in the text judgement to semantically relevant timespans in the videos of the hearings. The tool is employed as a User-Interface (UI) platform that provides a better access to justice by bookmarking the timespans in the videos which contributed to the final judgement of the case. We explain how we employ AI generative technology to retrieve the relevant links and show that the customisation of the GPT text embeddings to our dataset achieves the best accuracy for our automatic linking system. | Hadeel Saadany, Constantin Orasan |  |
| 102 |  |  [Automatic Marketing Theme and Commodity Construction System for E-commerce](https://doi.org/10.18653/v1/2023.emnlp-industry.48) |  | 0 | When consumers’ shopping needs are concentrated, they are more interested in the collection of commodities under the specific marketing theme. Therefore, mining marketing themes and their commodities collections can help customers save shopping costs and improve user clicks and purchases for recommendation system. However, the current system invites experts to write marketing themes and select the relevant commodities, which suffer from difficulty in mass production, poor timeliness and low online indicators. Therefore, we propose a automatic marketing theme and commodity construction system, which can not only generate popular marketing themes and select the relevant commodities automatically, but also improve the theme online effectiveness in the recommendation system. Specifically, we firstly utilize the pretrained language model to generate the marketing themes. And then, we utilize the theme-commodity consistency module to select the relevant commodities for the above generative theme. What’s more, we also build the indicator simulator to evaluate the effectiveness of the above generative theme. When the indicator is lower, the above selective commodities will be input into the theme-rewriter module to generate more efficient marketing themes. Finally, we utilize the human screening to control the system quality. Both the offline experiments and online A/B test demonstrate the superior performance of our proposed system compared with state-of-the-art methods. | Zhiping Wang, Peng Lin, Hainan Zhang, Hongshen Chen, Tianhao Li, Zhuoye Ding, Sulong Xu, Jinghe Hu |  |
| 103 |  |  [Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures](https://doi.org/10.18653/v1/2023.emnlp-industry.49) |  | 0 | This paper introduces a new IncidentAI dataset for safety prevention. Different from prior corpora that usually contain a single task, our dataset comprises three tasks: named entity recognition, cause-effect extraction, and information retrieval. The dataset is annotated by domain experts who have at least six years of practical experience as high-pressure gas conservation managers. We validate the contribution of the dataset in the scenario of safety prevention. Preliminary results on the three tasks show that NLP techniques are beneficial for analyzing incident reports to prevent future failures. The dataset facilitates future research in NLP and incident management communities. The access to the dataset is also provided (The IncidentAI dataset is available at: https://github.com/Cinnamon/incident-ai-dataset). | Shumpei Inoue, MinhTien Nguyen, Hiroki Mizokuchi, TuanAnh D. Nguyen, HuuHiep Nguyen, Dung Le |  |
| 104 |  |  [An Auxiliary Task Boosted Multi-task Learning Method for Service Account Retrieval with Limited Human Annotation](https://doi.org/10.18653/v1/2023.emnlp-industry.50) |  | 0 | Service accounts, including organizations’ official accounts and mini-programs, provide various convenient services for users, and have become crucial components of a number of applications. Therefore, retrieving service accounts quickly and accurately is vital. However, this task suffers from the problem of limited human annotation, i.e., manually assessing account functionality and assigning ratings based on user experience is both labor-intensive and time-consuming. To this end, this paper proposes a novel approach, the Auxiliary task Boosted Multi-Task Learning method (AuxBoost-MTL). Specifically, the proposed method introduces multiple auxiliary tasks, which is able to utilized the log data from our application as supervision, and enhance the performance of the main task, service account retrieval. Furthermore, we introduce an Adaptive Hierarchical Fusion Module (AHF module) into our approach. This module is designed to adaptively perform hierarchical fusion of embeddings from auxiliary tasks into the main task, thereby enhancing the model efficacy. Experiments on two real-world industrial datasets demonstrate the effectiveness of our proposed approach. | Yuanzhou Yao, Zhao Zhang, Kaijia Yang, Huasheng Liang, Qiang Yan, Yongjun Xu |  |
| 105 |  |  [VKIE: The Application of Key Information Extraction on Video Text](https://doi.org/10.18653/v1/2023.emnlp-industry.51) |  | 0 | Extracting structured information from videos is critical for numerous downstream applications in the industry. In this paper, we define a significant task of extracting hierarchical key information from visual texts on videos. To fulfill this task, we decouple it into four subtasks and introduce two implementation solutions called PipVKIE and UniVKIE. PipVKIE sequentially completes the four subtasks in continuous stages, while UniVKIE is improved by unifying all the subtasks into one backbone. Both PipVKIE and UniVKIE leverage multimodal information from vision, text, and coordinates for feature representation. Extensive experiments on one well-defined dataset demonstrate that our solutions can achieve remarkable performance and efficient inference speed. | Siyu An, Ye Liu, Haoyuan Peng, Di Yin |  |
| 106 |  |  [Investigating the Role and Impact of Disfluency on Summarization](https://doi.org/10.18653/v1/2023.emnlp-industry.52) |  | 0 | Contact centers handle both chat and voice calls for the same domain. As part of their workflow, it is a standard practice to summarize the conversations once they conclude. A significant distinction between chat and voice communication lies in the presence of disfluencies in voice calls, such as repetitions, restarts, and replacements. These disfluencies are generally considered noise for downstream natural language understanding (NLU) tasks. While a separate summarization model for voice calls can be trained in addition to chat specific model for the same domain, it requires manual annotations for both the channels and adds complexity arising due to maintaining two models. Therefore, it’s crucial to investigate if a model trained on fluent data can handle disfluent data effectively. While previous research explored impact of disfluency on question-answering and intent detection, its influence on summarization is inadequately studied. Our experiments reveal up to 6.99-point degradation in Rouge-L score, along with reduced fluency, consistency, and relevance when a fluent-trained model handles disfluent data. Replacement disfluencies have the highest negative impact. To mitigate this, we examine Fused-Fine Tuning by training the model with a combination of fluent and disfluent data, resulting in improved performance on both public and real-life datasets. Our work highlights the significance of incorporating disfluency in training summarization models and its advantages in an industrial setting. | Varun Nathan, Ayush Kumar, Jithendra Vepa |  |
| 107 |  |  [InsightNet : Structured Insight Mining from Customer Feedback](https://doi.org/10.18653/v1/2023.emnlp-industry.53) |  | 0 | We propose InsightNet, a novel approach for the automated extraction of structured insights from customer reviews. Our end-to-end machine learning framework is designed to overcome the limitations of current solutions, including the absence of structure for identified topics, non-standard aspect names, and lack of abundant training data. The proposed solution builds a semi-supervised multi-level taxonomy from raw reviews, a semantic similarity heuristic approach to generate labelled data and employs a multi-task insight extraction architecture by fine-tuning an LLM. InsightNet identifies granular actionable topics with customer sentiments and verbatim for each topic. Evaluations on real-world customer review data show that InsightNet performs better than existing solutions in terms of structure, hierarchy and completeness. We empirically demonstrate that InsightNet outperforms the current state-of-the-art methods in multi-label topic classification, achieving an F1 score of 0.85, which is an improvement of 11% F1-score over the previous best results. Additionally, InsightNet generalises well for unseen aspects and suggests new topics to be added to the taxonomy. | Sandeep Sricharan Mukku, Manan Soni, Chetan Aggarwal, Jitenkumar Rana, Promod Yenigalla, Rashmi Patange, Shyam Mohan |  |
| 108 |  |  [E2E Spoken Entity Extraction for Virtual Agents](https://doi.org/10.18653/v1/2023.emnlp-industry.54) |  | 0 | In human-computer conversations, extracting entities such as names, street addresses and email addresses from speech is a challenging task. In this paper, we study the impact of fine-tuning pre-trained speech encoders on extracting spoken entities in human-readable form directly from speech without the need for text transcription. We illustrate that such a direct approach optimizes the encoder to transcribe only the entity relevant portions of speech ignoring the superfluous portions such as carrier phrases, or spell name entities. In the context of dialog from an enterprise virtual agent, we demonstrate that the 1-step approach outperforms the typical 2-step approach which first generates lexical transcriptions followed by text-based entity extraction for identifying spoken entities. | Karan Singla, YeonJun Kim, Srinivas Bangalore |  |
| 109 |  |  [Generative Models for Product Attribute Extraction](https://doi.org/10.18653/v1/2023.emnlp-industry.55) |  | 0 | Product attribute extraction is an emerging field in information extraction and e-commerce, with applications including knowledge base construction, product recommendation, and enhancing customer experiences. In this work, we explore the use of generative models for product attribute extraction. We analyze their utility with hard and soft prompting methods, and demonstrate their ability to generate implicit attribute values, which state-of-the-art sequence tagging models are unable to extract. We perform a wide range of experiments on Amazon and MAVE product attribute datasets, and are the first to present results on multilingual attribute extraction. Our results show that generative models can outperform state- of-the-art tagging models for explicit product attribute extraction while having greater data efficiency, that they have the unique ability to perform implicit attribute extraction, and that in certain settings large language models can perform competitively with finetuned models with as little as two in-context examples. | Ansel Blume, Nasser Zalmout, Heng Ji, Xian Li |  |
| 110 |  |  [CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering](https://doi.org/10.18653/v1/2023.emnlp-industry.56) |  | 0 | Large language models (LLMs) have demonstrated remarkable performance by following natural language instructions without fine-tuning them on domain-specific tasks and data. However, leveraging LLMs for domain-specific question answering suffers from severe limitations. The generated answer tends to hallucinate due to the training data collection time (when using off-the-shelf), complex user utterance and wrong retrieval (in retrieval-augmented generation). Furthermore, due to the lack of awareness about the domain and expected output, such LLMs may generate unexpected and unsafe answers that are not tailored to the target domain. In this paper, we propose CarExpert, an in-car retrieval-augmented conversational question-answering system leveraging LLMs for different tasks. Specifically, CarExpert employs LLMs to control the input, provide domain-specific documents to the extractive and generative answering components, and controls the output to ensure safe and domain-specific answers. A comprehensive empirical evaluation exhibits that CarExpert outperforms state-of-the-art LLMs in generating natural, safe and car-specific answers. | Md. Rashad Al Hasan Rony, Christian Suess, Sinchana Ramakanth Bhat, Viju Sudhi, Julia Schneider, Maximilian Vogel, Roman Teucher, Ken E. Friedl, Soumya R. Sahoo |  |
| 111 |  |  [BUSTER: a "BUSiness Transaction Entity Recognition" dataset](https://doi.org/10.18653/v1/2023.emnlp-industry.57) |  | 0 | Albeit Natural Language Processing has seen major breakthroughs in the last few years, transferring such advances into real-world business cases can be challenging. One of the reasons resides in the displacement between popular benchmarks and actual data. Lack of supervision, unbalanced classes, noisy data and long documents often affect real problems in vertical domains such as finance, law and health. To support industry-oriented research, we present BUSTER, a BUSiness Transaction Entity Recognition dataset. The dataset consists of 3779 manually annotated documents on financial transactions. We establish several baselines exploiting both general-purpose and domain-specific language models. The best performing model is also used to automatically annotate 6196 documents, which we release as an additional silver corpus to BUSTER. | Andrea Zugarini, Andrew Zamai, Marco Ernandes, Leonardo Rigutini |  |
| 112 |  |  [Multi-word Tokenization for Sequence Compression](https://doi.org/10.18653/v1/2023.emnlp-industry.58) |  | 0 | Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this paper, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation. | Leonidas Gee, Leonardo Rigutini, Marco Ernandes, Andrea Zugarini |  |
| 113 |  |  [JarviX: A LLM No code Platform for Tabular Data Analysis and Optimization](https://doi.org/10.18653/v1/2023.emnlp-industry.59) |  | 0 | In this study, we introduce JarviX, a sophisticated data analytics framework. JarviX is designed to employ Large Language Models (LLMs) to facilitate an automated guide and execute high-precision data analyzes on tabular datasets. This framework emphasizes the significance of varying column types, capitalizing on state-of-the-art LLMs to generate concise data insight summaries, propose relevant analysis inquiries, visualize data effectively, and provide comprehensive explanations for results drawn from an extensive data analysis pipeline. Moreover, JarviX incorporates an automated machine learning (AutoML) pipeline for predictive modeling. This integration forms a comprehensive and automated optimization cycle, which proves particularly advantageous for optimizing machine configuration. The efficacy and adaptability of JarviX are substantiated through a series of practical use case studies. | Shangching Liu, Shengkun Wang, Tsungyao Chang, Wenqi Lin, ChungWei Hsiung, YiChen Hsieh, YuPing Cheng, SianHong Luo, Jianwei Zhang |  |
| 114 |  |  [Retrieve and Copy: Scaling ASR Personalization to Large Catalogs](https://doi.org/10.18653/v1/2023.emnlp-industry.60) |  | 0 | Personalization of automatic speech recognition (ASR) models is a widely studied topic because of its many practical applications. Most recently, attention-based contextual biasing techniques are used to improve the recognition of rare words and/or domain specific entities. However, due to performance constraints, the biasing is often limited to a few thousand entities, restricting real-world usability. To address this, we first propose a “Retrieve and Copy” mechanism to improve latency while retaining the accuracy even when scaled to a large catalog. We also propose a training strategy to overcome the degradation in recall at such scale due to an increased number of confusing entities. Overall, our approach achieves up to 6% more Word Error Rate reduction (WERR) and 3.6% absolute improvement in F1 when compared to a strong baseline. Our method also allows for large catalog sizes of up to 20K without significantly affecting WER and F1-scores, while achieving at least 20% inference speedup per acoustic frame. | Sai Muralidhar Jayanthi, Devang Kulshreshtha, Saket Dingliwal, Srikanth Ronanki, Sravan Bodapati |  |
| 115 |  |  [STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants](https://doi.org/10.18653/v1/2023.emnlp-industry.61) |  | 0 | In the context of a voice assistant system, steering refers to the phenomenon in which a user issues a follow-up command attempting to direct or clarify a previous turn. We propose STEER, a steering detection model that predicts whether a follow-up turn is a user’s attempt to steer the previous command. Constructing a training dataset for steering use cases poses challenges due to the cold-start problem. To overcome this, we developed heuristic rules to sample opt-in usage data, approximating positive and negative samples without any annotation. Our experimental results show promising performance in identifying steering intent, with over 95% accuracy on our sampled data. Moreover, STEER, in conjunction with our sampling strategy, aligns effectively with real-world steering scenarios, as evidenced by its strong zero-shot performance on a human-graded evaluation set. In addition to relying solely on user transcripts as input, we introduce STEER+, an enhanced version of the model. STEER+ utilizes a semantic parse tree to provide more context on out-of-vocabulary words, such as named entities that often occur at the sentence boundary. This further improves model performance, reducing error rate in domains where entities frequently appear, such as messaging. Lastly, we present a data analysis that highlights the improvement in user experience when voice assistants support steering use cases. | Leon Liyang Zhang, Jiarui Lu, Joel Ruben Antony Moniz, Aditya Kulkarni, Dhivya Piraviperumal, Tien Dung Tran, Nick Tzou, Hong Yu |  |
| 116 |  |  [Self-Criticism: Aligning Large Language Models with their Understanding of Helpfulness, Honesty, and Harmlessness](https://doi.org/10.18653/v1/2023.emnlp-industry.62) |  | 0 | Recently, there has been a notable surge in the significance of large language models (LLMs) that engage in conversational-style interactions, such as ChatGPT and Claude, as they contribute significantly to the progress of artificial general intelligence (AGI). Typically, these models undergo a two-phase fine-tuning process: instruction fine-tuning (IF) and reinforcement learning from human feedback (RLHF). These methods aim to align the LLMs to be helpful, honest, and harmless (HHH). However, RLHF, which incorporates independent reward models trained on high-quality human feedback datasets, incurs high costs in terms of hardware resources and human efforts. Therefore, we explore the possibility of aligning LLMs with their own understanding of HHH through IF and in-context learning (ICL). In this study, we propose a novel framework called Self-Criticism, which allows LLMs to align themselves with HHH based on the definition they learned from a large-scale text corpus. We begin by employing IF on a given instruction set and learning HHH discrimination through few-shot ICL. Subsequently, the LLMs evaluate their own generated responses and learn to produce “better” responses based on self-judgment. Finally, the model is retrained based on the self-generated responses to distill the whole process. By analyzing our proposed method, we also find interesting connections between Self-Criticism and goal-conditioned reinforcement learning, and pseudo-labeling. Experimental results demonstrate that this method achieves nearly identical performance to RLHF in terms of both human evaluation and evaluation by other LLMs, with only a minimal alignment tax. | Xiaoyu Tan, Shaojie Shi, Xihe Qiu, Chao Qu, Zhenting Qi, Yinghui Xu, Yuan Qi |  |
| 117 |  |  [InstructPTS: Instruction-Tuning LLMs for Product Title Summarization](https://doi.org/10.18653/v1/2023.emnlp-industry.63) |  | 0 | E-commerce product catalogs contain billions of items. Most products have lengthy titles, as sellers pack them with product attributes to improve retrieval, and highlight key product aspects. This results in a gap between such unnatural products titles, and how customers refer to them. It also limits how e-commerce stores can use these seller-provided titles for recommendation, QA, or review summarization. Inspired by recent work on instruction-tuned LLMs, we present InstructPTS, a controllable approach for the task of Product Title Summarization (PTS). Trained using a novel instruction fine-tuning strategy, our approach is able to summarize product titles according to various criteria (e.g. number of words in a summary, inclusion of specific phrases, etc.). Extensive evaluation on a real-world e-commerce catalog shows that compared to simple fine-tuning of LLMs, our proposed approach can generate more accurate product name summaries, with an improvement of over 14 and 8 BLEU and ROUGE points, respectively. | Besnik Fetahu, Zhiyu Chen, Oleg Rokhlenko, Shervin Malmasi |  |
| 118 |  |  [LLM4Vis: Explainable Visualization Recommendation using ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-industry.64) |  | 0 | Data visualization is a powerful tool for exploring and communicating insights in various domains. To automate visualization choice for datasets, a task known as visualization recommendation has been proposed. Various machine-learning-based approaches have been developed for this purpose, but they often require a large corpus of dataset-visualization pairs for training and lack natural explanations for their results. To address this research gap, we propose LLM4Vis, a novel ChatGPT-based prompting approach to perform visualization recommendation and return human-like explanations using very few demonstration examples. Our approach involves feature description, demonstration example selection, explanation generation, demonstration example construction, and inference steps. To obtain demonstration examples with high-quality explanations, we propose a new explanation generation bootstrapping to iteratively refine generated explanations by considering the previous generation and template-based hint. Evaluations on the VizML dataset show that LLM4Vis outperforms or performs similarly to supervised learning models like Random Forest, Decision Tree, and MLP, in both few-shot and zero-shot settings. The qualitative evaluation also shows the effectiveness of explanations generated by LLM4Vis. | Lei Wang, Songheng Zhang, Yun Wang, EePeng Lim, Yong Wang |  |
| 119 |  |  [DUBLIN: Visual Document Understanding By Language-Image Network](https://doi.org/10.18653/v1/2023.emnlp-industry.65) |  | 0 | In this paper, we present DUBLIN, a pixel-based model for visual document understanding that does not rely on OCR. DUBLIN can process both images and texts in documents just by the pixels and handle diverse document types and tasks. DUBLIN is pretrained on a large corpus of document images with novel tasks that enhance its visual and linguistic abilities. We evaluate DUBLIN on various benchmarks and show that it achieves state-of-the-art performance on extractive tasks such as DocVQA, InfoVQA, AI2D, OCR-VQA, RefExp, and CORD, as well as strong performance on abstraction datasets such as VisualMRC and text captioning. Our model demonstrates the potential of OCR-free document processing and opens new avenues for applications and research. | Kriti Aggarwal, Aditi Khandelwal, Kumar Tanmay, Owais Khan Mohammed, Qiang Liu, Monojit Choudhury, Hardik Hansrajbhai Chauhan, Subhojit Som, Vishrav Chaudhary, Saurabh Tiwary |  |
| 120 |  |  [DocumentNet: Bridging the Data Gap in Document Pre-training](https://doi.org/10.18653/v1/2023.emnlp-industry.66) |  | 0 | Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-training for both classic and few-shot learning settings. With the recent emergence of large language models (LLMs), DocumentNet provides a large data source to extend their multimodal capabilities for VDER. | Lijun Yu, Jin Miao, Xiaoyu Sun, Jiayi Chen, Alexander G. Hauptmann, Hanjun Dai, Wei Wei |  |
| 121 |  |  [Relevance-assisted Generation for Robust Zero-shot Retrieval](https://doi.org/10.18653/v1/2023.emnlp-industry.67) |  | 0 | Zero-shot retrieval tasks such as the BEIR benchmark reveal out-of-domain generalization as a key weakness of high-performance dense retrievers. As a solution, domain adaptation for dense retrievers has been actively studied. A notable approach is synthesizing domain-specific data, by generating pseudo queries (PQ), for fine-tuning with domain-specific relevance between PQ and documents. Our contribution is showing that key biases can cause sampled PQ to be irrelevant, negatively contributing to generalization. We propose to preempt their generation, by dividing the generation into simpler subtasks, of generating relevance explanations and guiding the generation to avoid negative generalization. Experiment results show that our proposed approach is more robust to domain shifts, validated on challenging BEIR zero-shot retrieval tasks. | Jihyuk Kim, Minsoo Kim, Joonsuk Park, Seungwon Hwang |  |
| 122 |  |  [Too much of product information : Don't worry, let's look for evidence!](https://doi.org/10.18653/v1/2023.emnlp-industry.68) |  | 0 | Product question answering (PQA) aims to provide an instant response to customer questions posted on shopping message boards, social media, brand websites and retail stores. In this paper, we propose a distantly supervised solution to answer customer questions by using product information. Auto-answering questions using product information poses two main challenges:(i) labelled data is not readily available (ii)lengthy product information requires attending to various parts of the text to answer the question. To this end, we first propose a novel distant supervision based NLI model to prepare training data without any manual efforts. To deal with lengthy context, we factorize answer generation into two sub-problems. First, given product information, model extracts evidence spans relevant to question. Then, model leverages evidence spans to generate answer. Further, we propose two novelties in fine-tuning approach: (i) First, we jointly fine-tune model for both the tasks in end-to-end manner and showcase that it outperforms standard multi-task fine-tuning. (ii) Next, we introduce an auxiliary contrastive loss for evidence extraction. We show that combination of these two ideas achieves an absolute improvement of 6% in accuracy (human evaluation) over baselines. | Aryan Jain, Jitenkumar Rana, Chetan Aggarwal |  |
| 123 |  |  [Harnessing LLMs for Temporal Data - A Study on Explainable Financial Time Series Forecasting](https://doi.org/10.18653/v1/2023.emnlp-industry.69) |  | 0 | Applying machine learning to financial time series has been an active area of industrial research enabling innovation in market insights, risk management, strategic decision-making, and policy formation. This paper explores the novel use of Large Language Models (LLMs) for explainable financial time series forecasting, addressing challenges in cross-sequence reasoning, multi-modal data integration, and result interpretation that are inherent in traditional approaches. Focusing on NASDAQ-100 stocks, we utilize public historical stock data, company metadata, and economic/financial news. Our experiments employ GPT-4 for zero-shot/few-shot inference and Open LLaMA for instruction-based fine-tuning. The study demonstrates LLMs’ ability to generate well-reasoned decisions by leveraging cross-sequence information and extracting insights from text and price time series. We show that our LLM-based approach outperforms classic ARMA-GARCH and gradient-boosting tree models. Furthermore, fine-tuned public LLMs, such as Open-LLaMA, can generate reasonable and explainable forecasts, although they underperform compared to GPT-4. | Xinli Yu, Zheng Chen, Yanbin Lu |  |
| 124 |  |  [ViGPTQA - State-of-the-Art LLMs for Vietnamese Question Answering: System Overview, Core Models Training, and Evaluations](https://doi.org/10.18653/v1/2023.emnlp-industry.70) |  | 0 | Large language models (LLMs) and their applications in low-resource languages (such as in Vietnamese) are limited due to lack of training data and benchmarking datasets. This paper introduces a practical real-world implementation of a question answering system for Vietnamese, called ViGPTQA, leveraging the power of LLM. Since there is no effective LLM in Vietnamese to date, we also propose, evaluate, and open-source an instruction-tuned LLM for Vietnamese, named ViGPT. ViGPT demonstrates exceptional performances, especially on real-world scenarios. We curate a new set of benchmark datasets that encompass both AI and human-generated data, providing a comprehensive evaluation framework for Vietnamese LLMs. By achieving state-of-the-art results and approaching other multilingual LLMs, our instruction-tuned LLM underscores the need for dedicated Vietnamese-specific LLMs. Our open-source model supports customized and privacy-fulfilled Vietnamese language processing systems. | Minh Thuan Nguyen, KhanhTung Tran, NhuVan Nguyen, XuanSon Vu |  |
| 125 |  |  [An Integrated Search System for Korea Weather Data](https://doi.org/10.18653/v1/2023.emnlp-industry.71) |  | 0 | We introduce WeatherSearch, an integrated search system deployed at the Korea Meteorological Administration (KMA). WeatherSearch enables users to retrieve all the relevant data for weather forecasting from a massive weather database with simple natural language queries. We carefully design and conduct multiple expert surveys and interviews for template creation and apply data augmentation techniques including template filling to collect 4 million data points with minimal human labors. We then finetune mT5 on the collected dataset and achieve an average MRR of 0.66 and an average Recall of 0.82. We also discuss weather-data-specific characteristics that should be taken into account for creating such a system. We hope our paper serves as a simple and effective guideline for those designing similar systems in other regions of the world. | Jinkyung Jo, Dayeon Ki, Soyoung Yoon, Minjoon Seo |  |
| 126 |  |  [Adaptive Hyper-parameter Learning for Deep Semantic Retrieval](https://doi.org/10.18653/v1/2023.emnlp-industry.72) |  | 0 | Deep semantic retrieval has achieved remarkable success in online E-commerce applications. The majority of methods aim to distinguish positive items and negative items for each query by utilizing margin loss or softmax loss. Despite their decent performance, these methods are highly sensitive to hyper-parameters, i.e., margin and temperature 𝜏, which measure the similarity of negative pairs and affect the distribution of items in metric space. How to design and choose adaptively parameters for different pairs is still an open challenge. Recently several methods have attempted to alleviate the above problem by learning each parameter through trainable/statistical methods in the recommendation. We argue that those are not suitable for retrieval scenarios, due to the agnosticism and diversity of the queries. To fully overcome this limitation, we propose a novel adaptive metric learning method that designs a simple and universal hyper-parameter-free learning method to improve the performance of retrieval. Specifically, we first propose a method that adaptive obtains the hyper-parameters by relying on the batch similarity without fixed or extra-trainable hyper-parameters. Subsequently, we adopt a symmetric metric learning method to mitigate model collapse issues. Furthermore, the proposed method is general and sheds a highlight on other fields. Extensive experiments demonstrate our method significantly outperforms previous methods on a real-world dataset, highlighting the superiority and effectiveness of our method. This method has been successfully deployed on an online E-commerce search platform and brought substantial economic benefits. | Mingming Li, Chunyuan Yuan, Huimu Wang, Peng Wang, Jingwei Zhuo, Binbin Wang, Lin Liu, Sulong Xu |  |
| 127 |  |  [On Sample-Efficient Code Generation](https://doi.org/10.18653/v1/2023.emnlp-industry.73) |  | 0 | Large language models often struggle to predict runtime behavior in code generation tasks, leading to a reliance on rejection sampling (best-of-n) to generate multiple code snippets then select the best. Our distinction is reducing sampling costs, without compromising generation quality. We introduce EFFICODE, a novel framework that prioritizes sampling on test problems that models can solve. We show how EFFICODE estimates solvability to optimize computational costs during multiple sampling. Based on empirical evidence, EFFICODE consistently demonstrates reduced sampling budgets while maintaining comparable code generation performance, especially when problems are challenging. In addition, utilizing EFFICODE to rank sampled code snippets also shows its effectiveness in answer code selection for reducing temporal costs, by not requiring any execution or test case generation. | Hojae Han, Yu Jin Kim, Byoungjip Kim, Youngwon Lee, Kyungjae Lee, Kyungmin Lee, Moontae Lee, Kyunghoon Bae, Seungwon Hwang |  |
| 128 |  |  [Batch Prompting: Efficient Inference with Large Language Model APIs](https://doi.org/10.18653/v1/2023.emnlp-industry.74) |  | 0 | Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use. We propose batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs decrease almost inverse linearly with the number of samples in each batch. We extensively validate the effectiveness of batch prompting on ten datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch prompting significantly (up to 5× with six samples in batch) reduces the LLM (Codex) inference token and time costs while achieving better or comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5 and GPT-4, we show the benefits of batch prompting also hold. Further analysis shows that the number of samples in each batch and the complexity of tasks affect its performance. Moreover, batch prompting can be applied across different reasoning methods using LLMs. Our code is released at the site https://github.com/xlang-ai/batch-prompting. | Zhoujun Cheng, Jungo Kasai, Tao Yu |  |
| 129 |  |  [Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding](https://doi.org/10.18653/v1/2023.emnlp-industry.75) |  | 0 | A Personalized Query Rewriting system strives to minimize defective queries to ensure robust conversational functionality by considering individual user behavior and preferences. It’s designed as a search-based system, maintaining a user index of past successful interactions with the conversational AI. However, this method faces challenges with unseen interactions, which refers to novel user interactions not covered by the user’s historical index. This paper introduces our Collaborative Query Rewriting approach, which utilizes underlying topological information to assist in rewriting defective queries arising from unseen user interactions. This approach begins by constructing a “User Feedback Interaction Graph” (FIG) using historical user-entity interactions. Subsequently, we traverse through the graph edges to establish an enhanced user index, referred to as the “collaborative user index”. This paper then further explores the use of Large Language Models (LLMs) in conjunction with graph traversal, leading to a significant increase in index coverage for unseen interactions. The effectiveness of our proposed approach has been proven through experiments on a large-scale real-world dataset and online A/B experiments. | Zheng Chen, Ziyan Jiang, Fan Yang, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Aram Galstyan |  |
| 130 |  |  [DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues](https://doi.org/10.18653/v1/2023.emnlp-industry.76) |  | 0 | Controversy is a reflection of our zeitgeist, and an important aspect to any discourse. The rise of large language models (LLMs) as conversational systems has increased public reliance on these systems for answers to their various questions. Consequently, it is crucial to systematically examine how these models respond to questions that pertaining to ongoing debates. However, few such datasets exist in providing human-annotated labels reflecting the contemporary discussions. To foster research in this area, we propose a novel construction of a controversial questions dataset, expanding upon the publicly released Quora Question Pairs Dataset. This dataset presents challenges concerning knowledge recency, safety, fairness, and bias. We evaluate different LLMs using a subset of this dataset, illuminating how they handle controversial issues and the stances they adopt. This research ultimately contributes to our understanding of LLMs’ interaction with controversial issues, paving the way for improvements in their comprehension and handling of complex societal debates. | David Q. Sun, Artem Abzaliev, Hadas Kotek, Christopher Klein, Zidi Xiu, Jason D. Williams |  |
| 131 |  |  [Angel: Enterprise Search System for the Non-Profit Industry](https://doi.org/10.18653/v1/2023.emnlp-industry.77) |  | 0 | Non-profit industry need a system for accurately matching fund-seekers (e.g., AMERICAN NATIONAL RED CROSS) with fund-givers (e.g., BILL AND MELINDA GATES FOUNDATION) aligned in cause (e.g., cancer) and target beneficiary group (e.g., children). In this paper, we create an enterprise search system “ANGEL” for the non-profit industry that takes a fund-giver’s mission description as input and returns a ranked list of fund-seekers as output, and vice-versa. ANGEL employs ColBERT, a neural information retrieval model, which we enhance by exploiting the two techniques of (a) Syntax-aware local attention (SLA) to combine syntactic information in the mission description with multi-head self-attention and (b) Dense Pseudo Relevance Feedback (DPRF) for augmentation of short mission descriptions. We create a mapping dictionary “non-profit-dict” to curate a “non-profit-search database” containing information on 594K fund-givers and 194K fund-seekers from IRS-990 filings for the non-profit industry search engines . We also curate a “non-profit-evaluation” dataset containing scored matching between 463 fund-givers and 100 fund-seekers. The research is in collaboration with a philanthropic startup that identifies itself as an “AI matching platform, fundraising assistant, and philanthropy search base.” Domain experts at the philanthropic startup annotate the non-profit evaluation dataset and continuously evaluate the performance of ANGEL. ANGEL achieves an improvement of 0.14 MAP@10 and 0.16 MRR@10 over the state-of-the-art baseline on the non-profit evaluation dataset. To the best of our knowledge, ours is the first effort at building an enterprise search engine based on neural information retrieval for the non-profit industry. | Saiful Haq, Ashutosh Sharma, Pushpak Bhattacharyya |  |
| 132 |  |  [Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023](https://aclanthology.org/volumes/2023.findings-emnlp/) |  | 0 |  | Houda Bouamor, Juan Pino, Kalika Bali |  |
| 133 |  |  [Frontmatter](https://aclanthology.org/2023.findings-emnlp.0) |  | 0 |  |  |  |
| 134 |  |  [Multi Document Summarization Evaluation in the Presence of Damaging Content](https://doi.org/10.18653/v1/2023.findings-emnlp.1) |  | 0 | In the Multi-document summarization (MDS) task, a summary is produced for a given set of documents. A recent line of research introduced the concept of damaging documents, denoting documents that should not be exposed to readers due to various reasons. In the presence of damaging documents, a summarizer is ideally expected to exclude damaging content in its output. Existing metrics evaluate a summary based on aspects such as relevance and consistency with the source documents. We propose to additionally measure the ability of MDS systems to properly handle damaging documents in their input set. To that end, we offer two novel metrics based on lexical similarity and language model likelihood. A set of experiments demonstrates the effectiveness of our metrics in measuring the ability of MDS systems to summarize a set of documents while eliminating damaging content from their summaries. | Avshalom Manevich, David Carmel, Nachshon Cohen, Elad Kravi, Ori Shapira |  |
| 135 |  |  [Guiding AMR Parsing with Reverse Graph Linearization](https://doi.org/10.18653/v1/2023.findings-emnlp.2) |  | 0 | Abstract Meaning Representation (AMR) parsing aims to extract an abstract semantic graph from a given sentence. The sequence-to-sequence approaches, which linearize the semantic graph into a sequence of nodes and edges and generate the linearized graph directly, have achieved good performance. However, we observed that these approaches suffer from structure loss accumulation during the decoding process, leading to a much lower F1-score for nodes and edges decoded later compared to those decoded earlier. To address this issue, we propose a novel Reverse Graph Linearization (RGL) enhanced framework. RGL defines both default and reverse linearization orders of an AMR graph, where most structures at the back part of the default order appear at the front part of the reversed order and vice versa. RGL incorporates the reversed linearization to the original AMR parser through a two-pass self-distillation mechanism, which guides the model when generating the default linearizations. Our analysis shows that our proposed method significantly mitigates the problem of structure loss accumulation, outperforming the previously best AMR parsing model by 0.8 and 0.5 Smatch scores on the AMR 2.0 and AMR 3.0 dataset, respectively. The code are available at https://github.com/pkunlp-icler/AMR_reverse_graph_linearization. | Bofei Gao, Liang Chen, Peiyi Wang, Zhifang Sui, Baobao Chang |  |
| 136 |  |  [Translate the Beauty in Songs: Jointly Learning to Align Melody and Translate Lyrics](https://doi.org/10.18653/v1/2023.findings-emnlp.3) |  | 0 | Song translation requires both translation of lyrics and alignment of music notes so that the resulting verse can be sung to the accompanying melody, which is a challenging problem that has attracted some interests in different aspects of the translation process. In this paper, we propose Lyrics-Melody Translation with Adaptive Grouping (LTAG), a holistic solution to automatic song translation by jointly modeling lyric translation and lyrics-melody alignment. It is a novel encoder-decoder framework that can simultaneously translate the source lyrics and determine the number of aligned notes at each decoding step through an adaptive note grouping module. To address data scarcity, we commissioned a small amount of training data annotated specifically for this task and used large amounts of automatic training data through back-translation. Experiments conducted on an English-Chinese song translation data set show the effectiveness of our model in both automatic and human evaluations. | Chengxi Li, Kai Fan, Jiajun Bu, Boxing Chen, Zhongqiang Huang, Zhi Yu |  |
| 137 |  |  [Aksharantar: Open Indic-language Transliteration datasets and models for the Next Billion Users](https://doi.org/10.18653/v1/2023.findings-emnlp.4) |  | 0 | Transliteration is very important in the Indian language context due to the usage of multiple scripts and the widespread use of romanized inputs. However, few training and evaluation sets are publicly available. We introduce Aksharantar, the largest publicly available transliteration dataset for Indian languages created by mining from monolingual and parallel corpora, as well as collecting data from human annotators. The dataset contains 26 million transliteration pairs for 21 Indic languages from 3 language families using 12 scripts. Aksharantar is 21 times larger than existing datasets and is the first publicly available dataset for 7 languages and 1 language family. We also introduce a test set of 103k word pairs for 19 languages that enables a fine-grained analysis of transliteration models on native origin words, foreign words, frequent words, and rare words. Using the training set, we trained IndicXlit, a multilingual transliteration model that improves accuracy by 15% on the Dakshina test set, and establishes strong baselines on the Aksharantar testset introduced in this work. The models, mining scripts, transliteration guidelines, and datasets are available at https://github.com/AI4Bharat/IndicXlit under open-source licenses. | Yash Madhani, Sushane Parthan, Priyanka Bedekar, Gokul NC, Ruchi Khapra, Anoop Kunchukuttan, Pratyush Kumar, Mitesh M. Khapra |  |
| 138 |  |  [Pretraining Without Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.5) |  | 0 | Transformers have been essential to pretraining success in NLP. While other architectures have been used, downstream accuracy is either significantly worse, or requires attention layers to match standard benchmarks such as GLUE. This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs). Our proposed model, Bidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures. The model learns static layers that do not consider pair-wise interactions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE and can be extended to long-form pretraining of 4096 tokens without approximation. Analysis shows that while the models have similar average accuracy, the approach has different inductive biases than BERT and scales more efficiently to longer sequences. | Junxiong Wang, Jing Nathan Yan, Albert Gu, Alexander M. Rush |  |
| 139 |  |  [Time-Aware Representation Learning for Time-Sensitive Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.6) |  | 0 | Time is one of the crucial factors in real-world question answering (QA) problems. However, language models have difficulty understanding the relationships between time specifiers, such as ‘after’ and ‘before’, and numbers, since existing QA datasets do not include sufficient time expressions. To address this issue, we propose a Time-Context aware Question Answering (TCQA) framework. We suggest a Time-Context dependent Span Extraction (TCSE) task, and build a time-context dependent data generation framework for model training. Moreover, we present a metric to evaluate the time awareness of the QA model using TCSE. The TCSE task consists of a question and four sentence candidates classified as correct or incorrect based on time and context. The model is trained to extract the answer span from the sentence that is both correct in time and context. The model trained with TCQA outperforms baseline models up to 8.5 of the F1-score in the TimeQA dataset. Our dataset and code are available at https://github.com/sonjbin/TCQA | Jungbin Son, Alice Oh |  |
| 140 |  |  [EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics](https://doi.org/10.18653/v1/2023.findings-emnlp.7) |  | 0 | Efficiency is a key property to foster inclusiveness and reduce environmental costs, especially in an era of LLMs. In this work, we provide a comprehensive evaluation of efficiency for MT evaluation metrics. Our approach involves replacing computation-intensive transformers with lighter alternatives and employing linear and quadratic approximations for alignment algorithms on top of LLM representations. We evaluate six (reference-free and reference-based) metrics across three MT datasets and examine 16 lightweight transformers. In addition, we look into the training efficiency of metrics like COMET by utilizing adapters. Our results indicate that (a) TinyBERT provides the optimal balance between quality and efficiency, (b) CPU speed-ups are more substantial than those on GPU; (c) WMD approximations yield no efficiency gains while reducing quality and (d) adapters enhance training efficiency (regarding backward pass speed and memory requirements) as well as, in some cases, metric quality. These findings can help to strike a balance between evaluation speed and quality, which is essential for effective NLG systems. Furthermore, our research contributes to the ongoing efforts to optimize NLG evaluation metrics with minimal impact on performance. To our knowledge, ours is the most comprehensive analysis of different aspects of efficiency for MT metrics conducted so far. | Daniil Larionov, Jens Grünwald, Christoph Leiter, Steffen Eger |  |
| 141 |  |  [Unsupervised Opinion Summarization Using Approximate Geodesics](https://doi.org/10.18653/v1/2023.findings-emnlp.8) |  | 0 | Opinion summarization is the task of creating summaries capturing popular opinions from user reviews. In this paper, we introduce Geodesic Summarizer (GeoSumm), a novel system to perform unsupervised extractive opinion summarization. GeoSumm consists of an encoder-decoder based representation learning model that generates topical representations of texts. These representations capture the underlying semantics of the text as a distribution over learnable latent units. GeoSumm generates these topical representations by performing dictionary learning over pre-trained text representations at multiple layers of the decoder. We then use these topical representations to quantify the importance of review sentences using a novel approximate geodesic distance-based scoring mechanism. We use the importance scores to identify popular opinions in order to compose general and aspect-specific summaries. Our proposed model, GeoSumm, achieves strong performance on three opinion summarization datasets. We perform additional experiments to analyze the functioning of our model and showcase the generalization ability of GeoSumm across different domains. | Somnath Basu Roy Chowdhury, Nicholas Monath, Avinava Dubey, Amr Ahmed, Snigdha Chaturvedi |  |
| 142 |  |  [Investigating the Frequency Distortion of Word Embeddings and Its Impact on Bias Metrics](https://doi.org/10.18653/v1/2023.findings-emnlp.9) |  | 0 | Recent research has shown that static word embeddings can encode words’ frequencies. However, little has been studied about this behavior. In the present work, we study how frequency and semantic similarity relate to one another in static word embeddings, and we assess the impact of this relationship on embedding-based bias metrics. We find that Skip-gram, GloVe and FastText embeddings tend to produce higher similarity between high-frequency words than between other frequency combinations. We show that the association between frequency and similarity also appears when words are randomly shuffled, and holds for different hyperparameter settings. This proves that the patterns we find are neither due to real semantic associations nor to specific parameters choices, and are an artifact produced by the word embeddings. To illustrate how frequencies can affect the measurement of biases related to gender, ethnicity, and affluence, we carry out a controlled experiment that shows that biases can even change sign or reverse their order when word frequencies change. | Francisco Valentini, Juan Sosa, Diego Fernández Slezak, Edgar Altszyler |  |
| 143 |  |  [Improving Classifier Robustness through Active Generative Counterfactual Data Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.10) |  | 0 | Counterfactual Data Augmentation (CDA) is a commonly used technique for improving robustness in natural language classifiers. However, one fundamental challenge is how to discover meaningful counterfactuals and efficiently label them, with minimal human labeling cost. Most existing methods either completely rely on human-annotated labels, an expensive process which limits the scale of counterfactual data, or implicitly assume label invariance, which may mislead the model with incorrect labels. In this paper, we present a novel framework that utilizes counterfactual generative models to generate a large number of diverse counterfactuals by actively sampling from regions of uncertainty, and then automatically label them with a learned auxiliary classifier. Our key insight is that we can more correctly label the generated counterfactuals by training a pairwise classifier that interpolates the relationship between the original example and the counterfactual. We demonstrate that with a small amount of human-annotated counterfactual data (10%), we can generate a counterfactual augmentation dataset with learned labels, that provides an 18-20% improvement in robustness and a 14-21% reduction in errors on 6 out-of-domain datasets, comparable to that of a fully human-annotated counterfactual dataset for both sentiment classification and question paraphrase tasks. | Ananth Balashankar, Xuezhi Wang, Yao Qin, Ben Packer, Nithum Thain, Ed H. Chi, Jilin Chen, Alex Beutel |  |
| 144 |  |  [Data Augmentation Techniques for Machine Translation of Code-Switched Texts: A Comparative Study](https://doi.org/10.18653/v1/2023.findings-emnlp.11) |  | 0 | Code-switching (CSW) text generation has been receiving increasing attention as a solution to address data scarcity. In light of this growing interest, we need more comprehensive studies comparing different augmentation approaches. In this work, we compare three popular approaches: lexical replacements, linguistic theories, and back-translation (BT), in the context of Egyptian Arabic-English CSW. We assess the effectiveness of the approaches on machine translation and the quality of augmentations through human evaluation. We show that BT and CSW predictive-based lexical replacement, being trained on CSW parallel data, perform best on both tasks. Linguistic theories and random lexical replacement prove to be effective in the lack of CSW parallel data, where both approaches achieve similar results. | Injy Hamed, Nizar Habash, Thang Vu |  |
| 145 |  |  [On the Relation between Sensitivity and Accuracy in In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.12) |  | 0 | In-context learning (ICL) suffers from oversensitivity to the prompt, making it unreliable in real-world scenarios. We study the sensitivity of ICL with respect to multiple perturbation types. First, we find that label bias obscures the true sensitivity, and therefore prior work may have significantly underestimated ICL sensitivity. Second, we observe a strong negative correlation between ICL sensitivity and accuracy: predictions sensitive to perturbations are less likely to be correct. Motivated by these findings, we propose SenSel, a few-shot selective prediction method that abstains from sensitive predictions. Experiments on ten classification datasets show that SenSel consistently outperforms two commonly used confidence-based and entropy-based baselines on abstention decisions. | Yanda Chen, Chen Zhao, Zhou Yu, Kathleen R. McKeown, He He |  |
| 146 |  |  [Self-distilled Transitive Instance Weighting for Denoised Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.13) |  | 0 | The widespread existence of wrongly labeled instances is a challenge to distantly supervised relation extraction. Most of the previous works are trained in a bag-level setting to alleviate such noise. However, sentence-level training better utilizes the information than bag-level training, as long as combined with effective noise alleviation. In this work, we propose a novel Transitive Instance Weighting mechanism integrated with the self-distilled BERT backbone, utilizing information in the intermediate outputs to generate dynamic instance weights for denoised sentence-level training. By down-weighting wrongly labeled instances and discounting the weights of easy-to-fit ones, our method can effectively tackle wrongly labeled instances and prevent overfitting. Experiments on both held-out and manual datasets indicate that our method achieves state-of-the-art performance and consistent improvements over the baselines. | Xiangyu Lin, Weijia Jia, Zhiguo Gong |  |
| 147 |  |  [MWE as WSD: Solving Multiword Expression Identification with Word Sense Disambiguation](https://doi.org/10.18653/v1/2023.findings-emnlp.14) |  | 0 | Recent approaches to word sense disambiguation (WSD) utilize encodings of the sense gloss (definition), in addition to the input context, to improve performance. In this work we demonstrate that this approach can be adapted for use in multiword expression (MWE) identification by training models which use gloss and context information to filter MWE candidates produced by a rule-based extraction pipeline. Our approach substantially improves precision, outperforming the state-of-the-art in MWE identification on the DiMSUM dataset by up to 1.9 F1 points and achieving competitive results on the PARSEME 1.1 English dataset. Our models also retain most of their WSD performance, showing that a single model can be used for both tasks. Finally, building on similar approaches using Bi-encoders for WSD, we introduce a novel Poly-encoder architecture which improves MWE identification performance. | Joshua Tanner, Jacob Hoffman |  |
| 148 |  |  [Dual Contrastive Learning Framework for Incremental Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.15) |  | 0 | Incremental learning plays a pivotal role in the context of online knowledge discovery, as it encourages large models (LM) to learn and refresh knowledge continuously. Many approaches have been proposed to simultaneously preserve knowledge from previous tasks while learning new concepts in online NLP applications. In this paper, we primarily focus on learning a more generalized embedding space that could be better transferred to various downstream sequence tasks. The key idea is to learn from both task-agnostic and task-specific embedding aspects so that the inherent challenge of catastrophic forgetting that arises in incremental learning scenarios can be addressed with a more generalized solution. We propose a dual contrastive learning (DCL) based framework to foster the transferability of representations across different tasks, it consists of two key components: firstly, we utilize global contrastive learning that intertwines a task-agnostic strategy for promoting a generalized embedding space; secondly, considering the domain shift from unseen distributions can compromise the quality of learned embeddings. We further incorporate a task-specific attention mechanism to enhance the adaptability of task-specific weight for various emerging tasks and ultimately reduce errors in generic representations. Experiments over various text datasets demonstrate that our work achieves superior performance and outperforms the current state-of-the-art methods. | Yigong Wang, Zhuoyi Wang, Yu Lin, Jinghui Guo, Sadaf Md. Halim, Latifur Khan |  |
| 149 |  |  [Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards](https://doi.org/10.18653/v1/2023.findings-emnlp.16) |  | 0 | Community Question-Answering (CQA) portals serve as a valuable tool for helping users within an organization. However, making them accessible to non-English-speaking users continues to be a challenge. Translating questions can broaden the community’s reach, benefiting individuals with similar inquiries in various languages. Translating questions using Neural Machine Translation (NMT) poses more challenges, especially in noisy environments, where the grammatical correctness of the questions is not monitored. These questions may be phrased as statements by non-native speakers, with incorrect subject-verb order and sometimes even missing question marks. Creating a synthetic parallel corpus from such data is also difficult due to its noisy nature. To address this issue, we propose a training methodology that fine-tunes the NMT system only using source-side data. Our approach balances adequacy and fluency by utilizing a loss function that combines BERTScore and Masked Language Model (MLM) Score. Our method surpasses the conventional Maximum Likelihood Estimation (MLE) based fine-tuning approach, which relies on synthetic target data, by achieving a 1.9 BLEU score improvement. Our model exhibits robustness while we add noise to our baseline, and still achieve 1.1 BLEU improvement and large improvements on TER and BLEURT metrics. Our proposed methodology is model-agnostic and is only necessary during the training phase. We make the codes and datasets publicly available at https://www.iitp.ac.in/~ai-nlp-ml/resources.html#DomainAdapt for facilitating further research. | Baban Gain, Ramakrishna Appicharla, Soumya Chennabasavaraj, Nikesh Garera, Asif Ekbal, Muthusamy Chelliah |  |
| 150 |  |  [Filtered Semi-Markov CRF](https://doi.org/10.18653/v1/2023.findings-emnlp.17) |  | 0 | Semi-Markov CRF has been proposed as an alternative to the traditional Linear Chain CRF for text segmentation tasks such as Named Entity Recognition (NER). Unlike CRF, which treats text segmentation as token-level prediction, Semi-CRF considers segments as the basic unit, making it more expressive. However, Semi-CRF suffers from two major drawbacks: (1) quadratic complexity over sequence length, as it operates on every span of the input sequence, and (2) inferior performance compared to CRF for sequence labeling tasks like NER. In this paper, we introduce Filtered Semi-Markov CRF, a variant of Semi-CRF that addresses these issues by incorporating a filtering step to eliminate irrelevant segments, reducing complexity and search space. Our approach is evaluated on several NER benchmarks, where it outperforms both CRF and Semi-CRF while being significantly faster. The implementation of our method is available on Github. | Urchade Zaratiana, Nadi Tomeh, Niama El Khbir, Pierre Holat, Thierry Charnois |  |
| 151 |  |  [Data Pruning for Efficient Model Pruning in Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.18) |  | 0 | Model pruning methods reduce memory requirements and inference time of large-scale pre-trained language models after deployment. However, the actual pruning procedure is computationally intensive, involving repeated training and pruning until the required sparsity is achieved. This paper combines data pruning with movement pruning for Neural Machine Translation (NMT) to enable efficient fine-pruning. We design a dataset pruning strategy by leveraging cross-entropy scores of individual training instances. We conduct pruning experiments on the task of machine translation from Romanian-to-English and Turkish-to-English, and demonstrate that selecting hard-to-learn examples (top-k) based on training cross-entropy scores outperforms other dataset pruning methods. We empirically demonstrate that data pruning reduces the overall steps required for convergence and the training time of movement pruning. Finally, we perform a series of experiments to tease apart the role of training data during movement pruning and uncover new insights to understand the interplay between data and model pruning in the context of NMT. | Abdul Hameed Azeemi, Ihsan Ayyub Qazi, Agha Ali Raza |  |
| 152 |  |  [Long-Form Speech Translation through Segmentation with Finite-State Decoding Constraints on Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.19) |  | 0 | One challenge in speech translation is that plenty of spoken content is long-form, but short units are necessary for obtaining high-quality translations. To address this mismatch, we adapt large language models (LLMs) to split long ASR transcripts into segments that can be independently translated so as to maximize the overall translation quality. We overcome the tendency of hallucination in LLMs by incorporating finite-state constraints during decoding; these eliminate invalid outputs without requiring additional training. We discover that LLMs are adaptable to transcripts containing ASR errors through prompt-tuning or fine-tuning. Relative to a state-of-the-art automatic punctuation baseline, our best LLM improves the average BLEU by 2.9 points for English–German, English–Spanish, and English–Arabic TED talk translation in 9 test sets, just by improving segmentation. | Arya McCarthy, Hao Zhang, Shankar Kumar, Felix Stahlberg, Ke Wu |  |
| 153 |  |  [Re-Temp: Relation-Aware Temporal Representation Learning for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.findings-emnlp.20) |  | 0 | Temporal Knowledge Graph Completion (TKGC) under the extrapolation setting aims to predict the missing entity from a fact in the future, posing a challenge that aligns more closely with real-world prediction problems. Existing research mostly encodes entities and relations using sequential graph neural networks applied to recent snapshots. However, these approaches tend to overlook the ability to skip irrelevant snapshots according to entity-related relations in the query and disregard the importance of explicit temporal information. To address this, we propose our model, Re-Temp (Relation-Aware Temporal Representation Learning), which leverages explicit temporal embedding as input and incorporates skip information flow after each timestamp to skip unnecessary information for prediction. Additionally, we introduce a two-phase forward propagation method to prevent information leakage. Through the evaluation on six TKGC (extrapolation) datasets, we demonstrate that our model outperforms all eight recent state-of-the-art models by a significant margin. | Kunze Wang, Soyeon Caren Han, Josiah Poon |  |
| 154 |  |  [RethinkingTMSC: An Empirical Study for Target-Oriented Multimodal Sentiment Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.21) |  | 0 | Recently, Target-oriented Multimodal Sentiment Classification (TMSC) has gained significant attention among scholars. However, current multimodal models have reached a performance bottleneck. To investigate the causes of this problem, we perform extensive empirical evaluation and in-depth analysis of the datasets to answer the following questions: \*\*Q1\*\*: Are the modalities equally important for TMSC? \*\*Q2\*\*: Which multimodal fusion modules are more effective? \*\*Q3\*\*: Do existing datasets adequately support the research? Our experiments and analyses reveal that the current TMSC systems primarily rely on the textual modality, as most of targets’ sentiments can be determined \*solely\* by text. Consequently, we point out several directions to work on for the TMSC task in terms of model design and dataset construction. The code and data can be found in https://github.com/Junjie-Ye/RethinkingTMSC. | Junjie Ye, Jie Zhou, Junfeng Tian, Rui Wang, Qi Zhang, Tao Gui, Xuanjing Huang |  |
| 155 |  |  [Lexical Entrainment for Conversational Systems](https://doi.org/10.18653/v1/2023.findings-emnlp.22) |  | 0 | Conversational agents have become ubiquitous in assisting with daily tasks, and are expected to possess human-like features. One such feature is lexical entrainment (LE), a phenomenon in which speakers in human-human conversations tend to naturally and subconsciously align their lexical choices with those of their interlocutors, leading to more successful and engaging conversations. As an example, if a digital assistant replies “Your appointment for Jinling Noodle Pub is at 7 pm” to the question “When is my reservation for Jinling Noodle Bar today?”, it may feel as though the assistant is trying to correct the speaker, whereas a response of “Your reservation for Jinling Noodle Baris at 7 pm” would likely be perceived as more positive. This highlights the importance of LE in establishing a shared terminology for maximum clarity and reducing ambiguity in conversations. However, we demonstrate in this work that current response generation models do not adequately address this crucial human-like phenomenon. To address this, we propose a new dataset, named MultiWOZ-ENTR, and a measure for LE for conversational systems. Additionally, we suggest a way to explicitly integrate LE into conversational systems with two new tasks, a LE extraction task and a LE generation task. We also present two baseline approaches for the LE extraction task, which aim to detect LE expressions from dialogue contexts | Zhengxiang Shi, Procheta Sen, Aldo Lipani |  |
| 156 |  |  [AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies](https://doi.org/10.18653/v1/2023.findings-emnlp.23) |  | 0 | We show that dialogue models can detect errors in their own messages, by calculating the likelihood of replies that are indicative of poor messages. For example, if an agent believes its partner is likely to respond “I don’t understand” to a candidate message, that message may not make sense, so an alternative message should be chosen. We evaluate our approach on a dataset from the game Diplomacy, which contains long dialogues richly grounded in the game state, on which existing models make many errors. We first show that hand-crafted replies can be effective for the task of detecting nonsense in applications as complex as Diplomacy. We then design AutoReply, an algorithm to search for such discriminative replies automatically, given a small number of annotated dialogue examples. We find that AutoReply-generated replies outperform handcrafted replies and perform on par with supervised learning approaches. | Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis |  |
| 157 |  |  [Follow-on Question Suggestion via Voice Hints for Voice Assistants](https://doi.org/10.18653/v1/2023.findings-emnlp.24) |  | 0 | The adoption of voice assistants like Alexa or Siri has grown rapidly, allowing users to instantly access information via voice search. Query suggestion is a standard feature of screen-based search experiences, allowing users to explore additional topics. However, this is not trivial to implement in voice-based settings. To enable this, we tackle the novel task of suggesting questions with compact and natural voice hints to allow users to ask follow-up questions. We define the task, ground it in syntactic theory and outline linguistic desiderata for spoken hints. We propose baselines and an approach using sequence-to-sequence Transformers to generate spoken hints from a list of questions. Using a new dataset of 6681 input questions and human written hints, we evaluated the models with automatic metrics and human evaluation. Results show that a naive approach of concatenating suggested questions creates poor voice hints. Our approach, which applies a linguistically-motivated pretraining task was strongly preferred by humans for producing the most natural hints. | Besnik Fetahu, Pedro Faustini, Anjie Fang, Giuseppe Castellucci, Oleg Rokhlenko, Shervin Malmasi |  |
| 158 |  |  [Bidirectional Masked Self-attention and N-gram Span Attention for Constituency Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.25) |  | 0 | Attention mechanisms have become a crucial aspect of deep learning, particularly in natural language processing (NLP) tasks. However, in tasks such as constituency parsing, attention mechanisms can lack the directional information needed to form sentence spans. To address this issue, we propose a Bidirectional masked and N-gram span Attention (BNA) model, which is designed by modifying the attention mechanisms to capture the explicit dependencies between each word and enhance the representation of the output span vectors. The proposed model achieves state-of-the-art performance on the Penn Treebank and Chinese Penn Treebank datasets, with F1 scores of 96.47 and 94.15, respectively. Ablation studies and analysis show that our proposed BNA model effectively captures sentence structure by contextualizing each word in a sentence through bidirectional dependencies and enhancing span representation. | Soohyeong Kim, Whanhee Cho, Minji Kim, Yong Choi |  |
| 159 |  |  [CR-COPEC: Causal Rationale of Corporate Performance Changes to learn from Financial Reports](https://doi.org/10.18653/v1/2023.findings-emnlp.26) |  | 0 | In this paper, we introduce CR-COPEC called Causal Rationale of Corporate Performance Changes from financial reports. This is a comprehensive large-scale domain-adaptation causal sentence dataset to detect financial performance changes of corporate. CR-COPEC contributes to two major achievements. First, it detects causal rationale from 10-K annual reports of the U.S. companies, which contain experts’ causal analysis following accounting standards in a formal manner. This dataset can be widely used by both individual investors and analysts as material information resources for investing and decision-making without tremendous effort to read through all the documents. Second, it carefully considers different characteristics which affect the financial performance of companies in twelve industries. As a result, CR-COPEC can distinguish causal sentences in various industries by taking unique narratives in each industry into consideration. We also provide an extensive analysis of how well CR-COPEC dataset is constructed and suited for classifying target sentences as causal ones with respect to industry characteristics. | Ye Eun Chun, Sunjae Kwon, Kyunghwan Sohn, Nakwon Sung, Junyoup Lee, Byoung Seo, Kevin Compher, Seungwon Hwang, Jaesik Choi |  |
| 160 |  |  [Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT](https://doi.org/10.18653/v1/2023.findings-emnlp.27) |  | 0 | The goal of this paper is to explore how Transformer language models process semantic knowledge, especially regarding the plausibility of noun-verb relations. First, I demonstrate GPT2 exhibits a higher degree of similarity with humans in plausibility processing compared to other Transformer language models. Next, I delve into how knowledge of plausibility is contained within attention heads of GPT2 and how these heads causally contribute to GPT2’s plausibility processing ability. Through several experiments, it was found that: i) GPT2 has a number of attention heads that detect plausible noun-verb relationships; ii) these heads collectively contribute to the Transformer’s ability to process plausibility, albeit to varying degrees; and iii) attention heads’ individual performance in detecting plausibility does not necessarily correlate with how much they contribute to GPT2’s plausibility processing ability. | Soo Ryu |  |
| 161 |  |  [Automatic Unit Test Data Generation and Actor-Critic Reinforcement Learning for Code Synthesis](https://doi.org/10.18653/v1/2023.findings-emnlp.28) |  | 0 | The advent of large pre-trained language models in the domain of Code Synthesis has shown remarkable performance on various benchmarks, treating the problem of Code Generation in a fashion similar to Natural Language Generation, trained with a Language Modelling (LM) objective. In addition, the property of programming language code being precisely evaluable with respect to its semantics – through the use of Unit Tests to check its functional correctness – lends itself to using Reinforcement Learning (RL) as a further training paradigm. Previous work has shown that RL can be applied as such to improve models’ coding capabilities; however, such RL-based methods rely on a reward signal based on defined Unit Tests, which are much harder to obtain compared to the huge crawled code datasets used in LM objectives. In this work, we present a novel approach to automatically obtain data consisting of function signatures and associated Unit Tests, suitable for RL training of Code Synthesis models. We also introduce a straightforward, simple yet effective Actor-Critic RL training scheme and show that it, in conjunction with automatically generated training data, leads to improvement of a pre-trained code language model’s performance by up to 9.9% improvement over the original underlying code synthesis LM, and up to 4.3% over RL-based models trained with standard PPO or CodeRL. | Philip John Gorinski, Matthieu Zimmer, Gerasimos Lampouras, DerrickGohXin Deik, Ignacio Iacobacci |  |
| 162 |  |  [Unlocking the Heterogeneous Landscape of Big Data NLP with DUUI](https://doi.org/10.18653/v1/2023.findings-emnlp.29) |  | 0 | Automatic analysis of large corpora is a complex task, especially in terms of time efficiency. This complexity is increased by the fact that flexible, extensible text analysis requires the continuous integration of ever new tools. Since there are no adequate frameworks for these purposes in the field of NLP, and especially in the context of UIMA, that are not outdated or unusable for security reasons, we present a new approach to address the latter task: Docker Unified UIMA Interface (DUUI), a scalable, flexible, lightweight, and feature-rich framework for automatic distributed analysis of text corpora that leverages Big Data experience and virtualization with Docker. We evaluate DUUI’s communication approach against a state-of-the-art approach and demonstrate its outstanding behavior in terms of time efficiency, enabling the analysis of big text data. | Alexander Leonhardt, Giuseppe Abrami, Daniel Baumartz, Alexander Mehler |  |
| 163 |  |  [Towards Agile Text Classifiers for Everyone](https://doi.org/10.18653/v1/2023.findings-emnlp.30) |  | 0 | Text-based safety classifiers are widely used for content moderation and increasingly to tune generative language model behavior - a topic of growing concern for the safety of digital assistants and chatbots. However, different policies require different classifiers, and safety policies themselves improve from iteration and adaptation. This paper introduces and evaluates methods for agile text classification, whereby classifiers are trained using small, targeted datasets that can be quickly developed for a particular policy. Experimenting with 7 datasets from three safety-related domains, comprising 15 annotation schemes, led to our key finding: prompt-tuning large language models, like PaLM 62B, with a labeled dataset of as few as 80 examples can achieve state-of-the-art performance. We argue that this enables a paradigm shift for text classification, especially for models supporting safer online discourse. Instead of collecting millions of examples to attempt to create universal safety classifiers over months or years, classifiers could be tuned using small datasets, created by individuals or small organizations, tailored for specific use cases, and iterated on and adapted in the time-span of a day. | Maximilian Mozes, Jessica Hoffmann, Katrin Tomanek, Muhamed Kouate, Nithum Thain, Ann Yuan, Tolga Bolukbasi, Lucas Dixon |  |
| 164 |  |  [Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good](https://doi.org/10.18653/v1/2023.findings-emnlp.31) |  | 0 | With the recent advances in natural language processing (NLP), a vast number of applications have emerged across various use cases. Among the plethora of NLP applications, many academic researchers are motivated to do work that has a positive social impact, in line with the recent initiatives of NLP for Social Good (NLP4SG). However, it is not always obvious to researchers how their research efforts are tackling today’s big social problems. Thus, in this paper, we introduce NLP4SGPapers, a scientific dataset with three associated tasks that can help identify NLP4SG papers and characterize the NLP4SG landscape by: (1) identifying the papers that address a social problem, (2) mapping them to the corresponding UN Sustainable Development Goals (SDGs), and (3) identifying the task they are solving and the methods they are using. Using state-of-the-art NLP models, we address each of these tasks and use them on the entire ACL Anthology, resulting in a visualization workspace that gives researchers a comprehensive overview of the field of NLP4SG. Our website is available at https://nlp4sg.vercel.app . We released our data at https://huggingface.co/datasets/feradauto/NLP4SGPapers and code at https://github.com/feradauto/nlp4sg | Fernando Gonzalez Adauto, Zhijing Jin, Bernhard Schölkopf, Tom Hope, Mrinmaya Sachan, Rada Mihalcea |  |
| 165 |  |  [PAXQA: Generating Cross-lingual Question Answering Examples at Training Scale](https://doi.org/10.18653/v1/2023.findings-emnlp.32) |  | 0 | Existing question answering (QA) systems owe much of their success to large, high-quality training data. Such annotation efforts are costly, and the difficulty compounds in the cross-lingual setting. Therefore, prior cross-lingual QA work has focused on releasing evaluation datasets, and then applying zero-shot methods as baselines. This work proposes a synthetic data generation method for cross-lingual QA which leverages indirect supervision from existing parallel corpora. Our method termed PAXQA (Projecting annotations for cross-lingual (x) QA) decomposes cross-lingual QA into two stages. First, we apply a question generation (QG) model to the English side. Second, we apply annotation projection to translate both the questions and answers. To better translate questions, we propose a novel use of lexically-constrained machine translation, in which constrained entities are extracted from the parallel bitexts. We apply PAXQA to generate cross-lingual QA examples in 4 languages (662K examples total), and perform human evaluation on a subset to create validation and test splits. We then show that models fine-tuned on these datasets outperform prior synthetic data generation models over several extractive QA datasets. The largest performance gains are for directions with non-English questions and English contexts. Ablation studies show that our dataset generation method is relatively robust to noise from automatic word alignments, showing the sufficient quality of our generations. To facilitate follow-up work, we release our code and datasets at https://github.com/manestay/paxqa. | Bryan Li, Chris CallisonBurch |  |
| 166 |  |  [Sharing, Teaching and Aligning: Knowledgeable Transfer Learning for Cross-Lingual Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.33) |  | 0 | In cross-lingual language understanding, machine translation is often utilized to enhance the transferability of models across languages, either by translating the training data from the source language to the target, or from the target to the source to aid inference. However, in cross-lingual machine reading comprehension (MRC), it is difficult to perform a deep level of assistance to enhance cross-lingual transfer because of the variation of answer span positions in different languages. In this paper, we propose X-STA, a new approach for cross-lingual MRC. Specifically, we leverage an attentive teacher to subtly transfer the answer spans of the source language to the answer output space of the target. A Gradient-Disentangled Knowledge Sharing technique is proposed as an improved cross-attention block. In addition, we force the model to learn semantic alignments from multiple granularities and calibrate the model outputs with teacher guidance to enhance cross-lingual transferability. Experiments on three multi-lingual MRC datasets show the effectiveness of our method, outperforming state-of-the-art approaches. | Tingfeng Cao, Chengyu Wang, Chuanqi Tan, Jun Huang, Jinhui Zhu |  |
| 167 |  |  [BERT Goes Off-Topic: Investigating the Domain Transfer Challenge using Genre Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.34) |  | 0 | While performance of many text classification tasks has been recently improved due to Pretrained Language Models (PLMs), in this paper we show that they still suffer from a performance gap when the underlying distribution of topics changes. For example, a genre classifier trained on political topics often fails when tested on documents in the same genre, but about sport or medicine. In this work, we quantify this phenomenon empirically with a large corpus and a large set of topics. Thus, we verify that domain transfer remains challenging both for classic PLMs, such as BERT, and for modern large models (LLMs), such as GPT. We develop a data augmentation approach by generating texts in any desired genre and on any desired topic, even when there are no documents in the training corpus that are both in that particular genre and on that particular topic. When we augment the training dataset with the topically-controlled synthetic texts, F1 improves up to 50% for some topics, approaching on-topic training, while showing no or next to no improvement for other topics. While our empirical results focus on genre classification, our methodology is applicable to other classification tasks such as gender, authorship, or sentiment classification. | Dmitri Roussinov, Serge Sharoff |  |
| 168 |  |  [Toward Stronger Textual Attack Detectors](https://doi.org/10.18653/v1/2023.findings-emnlp.35) |  | 0 | The landscape of available textual adversarial attacks keeps growing, posing severe threats and raising concerns regarding deep NLP systems integrity. However, the crucial problem of defending against malicious attacks has only drawn few attention in the NLP community. The latter is nonetheless instrumental to develop robust and trustworthy systems. This paper makes two important contributions in this line of search: (i) we introduce LAROUSSE, a new framework to detect textual adversarial attacks and (ii) we introduce STAKEOUT, an extended benchmark composed of nine popular attack methods, three datasets and two pre-trained models. LAROUSSE is ready-to-use in production as it is unsupervised, hyperparameter free and non-differentiable, protecting it against gradient-based methods. Our new benchmark STAKEOUT allows for a robust evaluation framework: we conduct extensive numerical experiments which demonstrate that LAROUSSE outperforms previous methods, and which allows to identify interesting factor of detection rate variations. | Pierre Colombo, Marine Picot, Nathan Noiry, Guillaume Staerman, Pablo Piantanida |  |
| 169 |  |  [MEAL: Stable and Active Learning for Few-Shot Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.36) |  | 0 | Few-shot classification has made great strides due to foundation models that, through priming and prompting, are highly effective few-shot learners. However, this approach has high variance both across different sets of few shots (\*data selection\*) and across different finetuning runs (\*run variability\*). This is problematic not only because it impedes the fair comparison of different approaches, but especially because it makes few-shot learning too unreliable for many real-world applications. To alleviate these issues, we make two contributions for more stable and effective few-shot learning: First, we propose novel ensembling methods and show that they substantially reduce \*run variability\*. Second, we introduce a new active learning (AL) criterion for \*data selection\* and present the first AL-based approach specifically tailored towards prompt-based learning. In our experiments, we show that our combined method, MEAL (\*\*M\*\*ultiprompt finetuning and prediction \*\*E\*\*nsembling with \*\*A\*\*ctive \*\*L\*\*earning), improves overall performance of prompt-based finetuning by 2.3 points on five diverse tasks. We publicly share our code and data splits in https://github.com/akoksal/MEAL. | Abdullatif Köksal, Timo Schick, Hinrich Schütze |  |
| 170 |  |  [Structure and Label Constrained Data Augmentation for Cross-domain Few-shot NER](https://doi.org/10.18653/v1/2023.findings-emnlp.37) |  | 0 | Cross-domain few-shot named entity recognition (NER) is a challenging task that aims to recognize entities in target domains with limited labeled data by leveraging relevant knowledge from source domains. However, domain gaps limit the effect of knowledge transfer and harm the performance of NER models. In this paper, we analyze those domain gaps from two new perspectives, i.e., entity annotations and entity structures and leverage word-to-tag and word-to-word relations to model them, respectively. Moreover, we propose a novel method called Structure and Label Constrained Data Augmentation (SLC-DA) for Cross-domain Few-shot NER, which novelly design a label constrained pre-train task and a structure constrained optimization objectives in the data augmentation process to generate domain-specific augmented data to help NER models smoothly transition from source to target domains. We evaluate our approach on several standard datasets and achieve state-of-the-art or competitive results, demonstrating the effectiveness of our method in cross-domain few-shot NER. | Jingyi Zhang, Ying Zhang, Yufeng Chen, Jinan Xu |  |
| 171 |  |  [Weakly-supervised Deep Cognate Detection Framework for Low-Resourced Languages Using Morphological Knowledge of Closely-Related Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.38) |  | 0 | Exploiting cognates for transfer learning in under-resourced languages is an exciting opportunity for language understanding tasks, including unsupervised machine translation, named entity recognition and information retrieval. Previous approaches mainly focused on supervised cognate detection tasks based on orthographic, phonetic or state-of-the-art contextual language models, which under-perform for most under-resourced languages. This paper proposes a novel language-agnostic weakly-supervised deep cognate detection framework for under-resourced languages using morphological knowledge from closely related languages. We train an encoder to gain morphological knowledge of a language and transfer the knowledge to perform unsupervised and weakly-supervised cognate detection tasks with and without the pivot language for the closely-related languages. While unsupervised, it overcomes the need for hand-crafted annotation of cognates. We performed experiments on different published cognate detection datasets across language families and observed not only significant improvement over the state-of-the-art but also our method outperformed the state-of-the-art supervised and unsupervised methods. Our model can be extended to a wide range of languages from any language family as it overcomes the requirement of the annotation of the cognate pairs for training. | Koustava Goswami, Priya Rani, Theodorus Fransen, John P. McCrae |  |
| 172 |  |  [SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data](https://doi.org/10.18653/v1/2023.findings-emnlp.39) |  | 0 | Text-to-SQL aims to automate the process of generating SQL queries on a database from natural language text. In this work, we propose “SQLPrompt”, tailored to improve the few-shot prompting capabilities of Text-to-SQL for Large Language Models (LLMs). Our methods include innovative prompt design, execution-based consistency decoding strategy which selects the SQL with the most consistent execution outcome among other SQL proposals, and a method that aims to improve performance by diversifying the SQL proposals during consistency selection with different prompt designs (“MixPrompt”) and foundation models (“MixLLMs”). We show that SQLPrompt outperforms previous approaches for in-context learning with zero labeled data by a large margin, closing the gap with finetuning state-of-the-art with thousands of labeled data. | Ruoxi Sun, Sercan Ö. Arik, Rajarishi Sinha, Hootan Nakhost, Hanjun Dai, Pengcheng Yin, Tomas Pfister |  |
| 173 |  |  [Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.40) |  | 0 | Foundation models or pre-trained models have substantially improved the performance of various language, vision, and vision-language understanding tasks. However, existing foundation models can only perform the best in one type of tasks, namely language, vision, or vision-language. It is still an open question whether it is possible to construct a general foundation model performing the best for all the understanding tasks. In this paper, we propose a new method for training the general foundation model, X-FM (the X-Foundation Model). X-FM has one language encoder, one vision encoder, and one fusion encoder, as well as a new training method. The training method includes two new techniques for learning X-FM from text, image, and image-text pair data. One is to stop gradients from the vision-language training when learning the language encoder. The other is to leverage the vision-language training to guide the learning of the vision encoder. Extensive experiments on benchmark datasets show that X-FM can significantly outperform existing general foundation models and perform better than or comparable to existing foundation models specifically for language, vision, or vision-language understanding. Code and pre-trained models are released at https://github.com/zhangxinsong-nlp/XFM. | Xinsong Zhang, Yan Zeng, Jipeng Zhang, Hang Li |  |
| 174 |  |  [Trigger Warnings: Bootstrapping a Violence Detector for Fan Fiction](https://doi.org/10.18653/v1/2023.findings-emnlp.41) |  | 0 | We present the first dataset and evaluation results on a newly defined task: assigning trigger warnings. We introduce a labeled corpus of narrative fiction from Archive of Our Own (AO3), a popular fan fiction site, and define a document-level classification task to determine whether or not to assign a trigger warning to an English story. We focus on the most commonly assigned trigger type “violence’ using the warning labels provided by AO3 authors as ground-truth labels. We trained SVM, BERT, and Longfomer models on three datasets sampled from the corpus and achieve F1 scores between 0.8 and 0.9, indicating that assigning trigger warnings for violence is feasible. | Magdalena Wolska, Matti Wiegmann, Christopher Schröder, Ole Borchardt, Benno Stein, Martin Potthast |  |
| 175 |  |  [Pass-Tuning: Towards Structure-Aware Parameter-Efficient Tuning for Code Representation Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.42) |  | 0 | Code pre-trained models (CodePTMs) have recently become the de-facto paradigm for various tasks in the domain of code intelligence. To achieve excellent performance, the widely used strategy is to fine-tune all the parameters of CodePTMs. However, as the model size increases along with the number of downstream tasks, this strategy becomes excessively expensive. There are also some prior works that utilize Parameter-Efficient Learning (PEL) methods for model tuning in natural language processing to mitigate similar problems, but applying them directly to CodePTMs fails to capture the inherent structural characteristics of codes. To address the problem, in this paper, we propose Pass-Tuning for structure-aware Parameter-Efficient code representation learning. Specifically, a plug-and-play graph neural network module that can learn from Abstract Syntax Tree (AST) is employed as a tunable prefix. On the one hand, Pass-Tuning can further exploit the structural information of source code. On the other hand, it could serve as a replacement for full fine-tuning. We evaluate our method on multiple tasks across eight programming languages, including code understanding and generation. These results demonstrate the effectiveness, robustness, and universality of our method. | Nuo Chen, Qiushi Sun, Jianing Wang, Xiang Li, Ming Gao |  |
| 176 |  |  [Counterfactual Augmentation for Multimodal Learning Under Presentation Bias](https://doi.org/10.18653/v1/2023.findings-emnlp.43) |  | 0 | In real-world machine learning systems, labels are often derived from user behaviors that the system wishes to encourage. Over time, new models must be trained as new training examples and features become available. However, feedback loops between users and models can bias future user behavior, inducing a \*presentation bias\* in the labels that compromises the ability to train new models. In this paper, we propose \*counterfactual augmentation\*, a novel causal method for correcting presentation bias using generated counterfactual labels. Our empirical evaluations demonstrate that counterfactual augmentation yields better downstream performance compared to both uncorrected models and existing bias-correction methods. Model analyses further indicate that the generated counterfactuals align closely with true counterfactuals in an oracle setting. | Victoria Lin, LouisPhilippe Morency, Dimitrios Dimitriadis, Srinagesh Sharma |  |
| 177 |  |  [A Table-to-Text Framework with Heterogeneous Multidominance Attention and Self-Evaluated Multi-Pass Deliberation](https://doi.org/10.18653/v1/2023.findings-emnlp.44) |  | 0 | Though big progress in table-to-text works, effectively leveraging table structure signals, e.g., hierarchical structure, remains challenging. Besides, deliberating generated descriptions proves to be effective for table-to-text. However, determining the appropriate outcome when encountering multi-pass candidates is another challenge. To this end, we propose a novel table-to-text approach on top of Self-evaluated multi-pass Generation and Heterogenous Multidominance Attention, namely SG-HMA. Specifically, we formulate the table structure into a multidominance (MD) structure and devise a heterogenous multidominance attention (HMA) to comprehensively explore the complex interactions encoded in the hierarchical structure, which can further deliver rich signals for text generation with the help of pre-trained language models (PLMs). Afterward, a contrastive loss is introduced to align the generation objective with evaluation metrics, so the more faithful generated descriptions can be guaranteed. We conduct extensive experiments on three public datasets, demonstrating that SG-HMA outperforms several SOTA methods quantitatively and qualitatively. | Xi Chen, Xinjiang Lu, Haoran Xin, Wenjun Peng, Haoyang Duan, Feihu Jiang, Jingbo Zhou, Hui Xiong |  |
| 178 |  |  [Crossing the Aisle: Unveiling Partisan and Counter-Partisan Events in News Reporting](https://doi.org/10.18653/v1/2023.findings-emnlp.45) |  | 0 | News media is expected to uphold unbiased reporting. Yet they may still affect public opinion by selectively including or omitting events that support or contradict their ideological positions. Prior work in NLP has only studied media bias via linguistic style and word usage. In this paper, we study to which degree media balances news reporting and affects consumers through event inclusion or omission. We first introduce the task of detecting both partisan and counter-partisan events: events that support or oppose the author’s political ideology. To conduct our study, we annotate a high-quality dataset, PAC, containing 8,511 (counter-)partisan event annotations in 304 news articles from ideologically diverse media outlets. We benchmark PAC to highlight the challenges of this task. Our findings highlight both the ways in which the news subtly shapes opinion and the need for large language models that better understand events within a broader context. Our dataset can be found at https://github.com/launchnlp/Partisan-Event-Dataset. | Kaijian Zou, Xinliang Frederick Zhang, Winston Wu, Nicholas Beauchamp, Lu Wang |  |
| 179 |  |  [Video-Text Retrieval by Supervised Sparse Multi-Grained Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.46) |  | 0 | While recent progress in video-text retrieval has been advanced by the exploration of better representation learning, in this paper, we present a novel multi-grained sparse learning framework, S3MA, to learn an aligned sparse space shared between the video and the text for video-text retrieval. The shared sparse space is initialized with a finite number of sparse concepts, each of which refers to a number of words. With the text data at hand, we learn and update the shared sparse space in a supervised manner using the proposed similarity and alignment losses. Moreover, to enable multi-grained alignment, we incorporate frame representations for better modeling the video modality and calculating fine-grained and coarse-grained similarities. Benefiting from the learned shared sparse space and multi-grained similarities, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of S3MA over existing methods. | Yimu Wang, Peng Shi |  |
| 180 |  |  [Zero-Shot-BERT-Adapters: a Zero-Shot Pipeline for Unknown Intent Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.47) |  | 0 | Intent discovery is a crucial task in natural language processing, and it is increasingly relevant for various of industrial applications. Identifying novel, unseen intents from user inputs remains one of the biggest challenges in this field. Herein, we propose Zero-Shot-BERT-Adapters, a two-stage method for multilingual intent discovery relying on a Transformer architecture, fine-tuned with Adapters. We train the model for Natural Language Inference (NLI) and later perform unknown intent classification in a zero-shot setting for multiple languages. In our evaluation, we first analyze the quality of the model after adaptive fine-tuning on known classes. Secondly, we evaluate its performance in casting intent classification as an NLI task. Lastly, we test the zero-shot performance of the model on unseen classes, showing how Zero-Shot-BERT-Adapters can effectively perform intent discovery by generating semantically similar intents, if not equal, to the ground-truth ones. Our experiments show how Zero-Shot-BERT-Adapters outperforms various baselines in two zero-shot settings: known intent classification and unseen intent discovery. The proposed pipeline holds the potential for broad application in customer care. It enables automated dynamic triage using a lightweight model that can be easily deployed and scaled in various business scenarios, unlike large language models. Zero-Shot-BERT-Adapters represents an innovative multi-language approach for intent discovery, enabling the online generation of novel intents. A Python package implementing the pipeline and the new datasets we compiled are available at the following link: https://github.com/GT4SD/zero-shot-bert-adapters. | Daniele Comi, Dimitrios Christofidellis, Pier Francesco Piazza, Matteo Manica |  |
| 181 |  |  [ReFSQL: A Retrieval-Augmentation Framework for Text-to-SQL Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.48) |  | 0 | Text-to-SQL is the task that aims at translating natural language questions into SQL queries. Existing methods directly align the natural language with SQL Language and train one encoder-decoder-based model to fit all questions. However, they underestimate the inherent structural characteristics of SQL, as well as the gap between specific structure knowledge and general knowledge. This leads to structure errors in the generated SQL. To address the above challenges, we propose a retrieval-argument framework, namely ReFSQL. It contains two parts, structure-enhanced retriever and the generator. Structure-enhanced retriever is designed to identify samples with comparable specific knowledge in an unsupervised way. Subsequently, we incorporate the retrieved samples’ SQL into the input, enabling the model to acquire prior knowledge of similar SQL grammar. To further bridge the gap between specific and general knowledge, we present a mahalanobis contrastive learning method, which facilitates the transfer of the sample toward the specific knowledge distribution constructed by the retrieved samples. Experimental results on five datasets verify the effectiveness of our approach in improving the accuracy and robustness of Text-to-SQL generation. Our framework has achieved improved performance when combined with many other backbone models (including the 11B flan-T5) and also achieved state-of-the-art performance when compared to existing methods that employ the fine-tuning approach. | Kun Zhang, Xiexiong Lin, Yuanzhuo Wang, Xin Zhang, Fei Sun, Jianhe Cen, Hexiang Tan, Xuhui Jiang, Huawei Shen |  |
| 182 |  |  [Approximating Two-Layer Feedforward Networks for Efficient Transformers](https://doi.org/10.18653/v1/2023.findings-emnlp.49) |  | 0 | How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that \*unifies\* various methods to \*approximate two-layer NNs\* (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the \*compute-equal\* condition, our evaluation condition is \*parameter-equal\*, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the \*dense\* Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-efficient LMs. Our code is public. | Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber |  |
| 183 |  |  [Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.50) |  | 0 | Adapting a large language model for multiple-attribute text style transfer via fine-tuning can be challenging due to the substantial amount of computational resources and labeled data required for the specific downstream task. In this paper, we address this challenge by introducing Adapter-TST, a framework that freezes the pre-trained model’s original parameters and enables the development of a multiple-attribute text style transfer model. Using BART as the backbone model, Adapter-TST utilizes different neural adapters to model different types of attribute information, similar to a plug-in connected to BART. Our method allows control over multiple attributes (e.g. sentiment, tense, active or passive voice) and configures the adapters’ architecture to generate multiple outputs in respect to attributes or compositional editing on the same sentence. We evaluate the proposed model on both traditional sentiment transfer and multiple-attribute transfer tasks. The experiment results demonstrate that Adapter-TST outperforms all the state-of-the-art baselines with significantly less computational resources. We have also empirically shown that each adapter is able to characterize specific stylistic attributes effectively and can be configured to perform compositional editing. | Zhiqiang Hu, Nancy F. Chen, Roy KaWei Lee |  |
| 184 |  |  [Solving the Right Problem is Key for Translational NLP: A Case Study in UMLS Vocabulary Insertion](https://doi.org/10.18653/v1/2023.findings-emnlp.51) |  | 0 | As the immense opportunities enabled by large language models become more apparent, NLP systems will be increasingly expected to excel in real-world settings. However, in many instances, powerful models alone will not yield translational NLP solutions, especially if the formulated problem is not well aligned with the real-world task. In this work, we study the case of UMLS vocabulary insertion, an important real-world task in which hundreds of thousands of new terms, referred to as atoms, are added to the UMLS, one of the most comprehensive open-source biomedical knowledge bases. Previous work aimed to develop an automated NLP system to make this time-consuming, costly, and error-prone task more efficient. Nevertheless, practical progress in this direction has been difficult to achieve due to a problem formulation and evaluation gap between research output and the real-world task. In order to address this gap, we introduce a new formulation for UMLS vocabulary insertion which mirrors the real-world task, datasets which faithfully represent it and several strong baselines we developed through re-purposing existing solutions. Additionally, we propose an effective rule-enhanced biomedical language model which enables important new model behavior, outperforms all strong baselines and provides measurable qualitative improvements to editors who carry out the UVI task. We hope this case study provides insight into the considerable importance of problem formulation for the success of translational NLP solutions. | Bernal Jimenez Gutierrez, Yuqing Mao, Vinh Nguyen, Kin Wah Fung, Yu Su, Olivier Bodenreider |  |
| 185 |  |  [Improving Cross-lingual Transfer through Subtree-aware Word Reordering](https://doi.org/10.18653/v1/2023.findings-emnlp.52) |  | 0 | Despite the impressive growth of the abilities of multilingual language models, such as XLM-R and mT5, it has been shown that they still face difficulties when tackling typologically-distant languages, particularly in the low-resource setting. One obstacle for effective cross-lingual transfer is variability in word-order patterns. It can be potentially mitigated via source- or target-side word reordering, and numerous approaches to reordering have been proposed. However, they rely on language-specific rules, work on the level of POS tags, or only target the main clause, leaving subordinate clauses intact. To address these limitations, we present a new powerful reordering method, defined in terms of Universal Dependencies, that is able to learn fine-grained word-order patterns conditioned on the syntactic context from a small amount of annotated data and can be applied at all levels of the syntactic tree. We conduct experiments on a diverse set of tasks and show that our method consistently outperforms strong baselines over different language pairs and model architectures. This performance advantage holds true in both zero-shot and few-shot scenarios. | Ofir Arviv, Dmitry Nikolaev, Taelin Karidi, Omri Abend |  |
| 186 |  |  [Novel Slot Detection With an Incremental Setting](https://doi.org/10.18653/v1/2023.findings-emnlp.53) |  | 0 |  | Chen Liang, Hongliang Li, Changhao Guan, Qingbin Liu, Jian Liu, Jinan Xu, Zhe Zhao |  |
| 187 |  |  [Self-supervised Post-processing Method to Enrich Pretrained Word Vectors](https://doi.org/10.18653/v1/2023.findings-emnlp.54) |  | 0 |  | Hwiyeol Jo |  |
| 188 |  |  [Automatic Model Selection with Large Language Models for Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.55) |  | 0 |  | James Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, Michael Qizhe Xie |  |
| 189 |  |  [ARKitSceneRefer: Text-based Localization of Small Objects in Diverse Real-World 3D Indoor Scenes](https://doi.org/10.18653/v1/2023.findings-emnlp.56) |  | 0 |  | Shunya Kato, Shuhei Kurita, Chenhui Chu, Sadao Kurohashi |  |
| 190 |  |  [Improving Question Generation with Multi-level Content Planning](https://doi.org/10.18653/v1/2023.findings-emnlp.57) |  | 0 |  | Zehua Xia, Qi Gou, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li, CamTu Nguyen |  |
| 191 |  |  [Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing](https://doi.org/10.18653/v1/2023.findings-emnlp.58) |  | 0 |  | Yue Guo, Zian Xu, Yi Yang |  |
| 192 |  |  [DelucionQA: Detecting Hallucinations in Domain-specific Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.59) |  | 0 |  | Mobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun Araki, Arsalan Gundroo, Bingqing Wang, Rakesh R. Menon, Md. Rizwan Parvez, Zhe Feng |  |
| 193 |  |  [InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution](https://doi.org/10.18653/v1/2023.findings-emnlp.60) |  | 0 |  | Xiangru Jian, Yimu Wang |  |
| 194 |  |  [Dissecting In-Context Learning of Translations in GPT-3](https://doi.org/10.18653/v1/2023.findings-emnlp.61) |  | 0 |  | Vikas Raunak, Arul Menezes, Hany Hassan Awadalla |  |
| 195 |  |  [Social Commonsense-Guided Search Query Generation for Open-Domain Knowledge-Powered Conversations](https://doi.org/10.18653/v1/2023.findings-emnlp.62) |  | 0 |  | Revanth Gangi Reddy, Hao Bai, Wentao Yao, Sharath Chandra Etagi Suresh, Heng Ji, ChengXiang Zhai |  |
| 196 |  |  [MixTEA: Semi-supervised Entity Alignment with Mixture Teaching](https://doi.org/10.18653/v1/2023.findings-emnlp.63) |  | 0 |  | Feng Xie, Xin Song, Xiang Zeng, Xuechen Zhao, Lei Tian, Bin Zhou, Yusong Tan |  |
| 197 |  |  [EZ-STANCE: A Large Dataset for Zero-Shot Stance Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.64) |  | 0 |  | Chenye Zhao, Cornelia Caragea |  |
| 198 |  |  [Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.65) |  | 0 |  | Fan Jiang, Qiongkai Xu, Tom Drummond, Trevor Cohn |  |
| 199 |  |  [TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.66) |  | 0 |  | Shuhuai Ren, Sishuo Chen, Shicheng Li, Xu Sun, Lu Hou |  |
| 200 |  |  [Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.67) |  | 0 |  | Xin Su, Phillip Howard, Nagib Hakim, Steven Bethard |  |
| 201 |  |  [The Internal State of an LLM Knows When It's Lying](https://doi.org/10.18653/v1/2023.findings-emnlp.68) |  | 0 |  | Amos Azaria, Tom M. Mitchell |  |
| 202 |  |  [Factual Relation Discrimination for Factuality-oriented Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.69) |  | 0 |  | Zhiguang Gao, Peifeng Li, Feng Jiang, Xiaomin Chu, Qiaoming Zhu |  |
| 203 |  |  [Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment](https://doi.org/10.18653/v1/2023.findings-emnlp.70) |  | 0 |  | Qian Li, Cheng Ji, Shu Guo, Zhaoji Liang, Lihong Wang, Jianxin Li |  |
| 204 |  |  [Is a Prestigious Job the same as a Prestigious Country? A Case Study on Multilingual Sentence Embeddings and European Countries](https://doi.org/10.18653/v1/2023.findings-emnlp.71) |  | 0 |  | Jindrich Libovický |  |
| 205 |  |  [Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.72) |  | 0 |  | Ziqiao Ma, Jacob Sansom, Run Peng, Joyce Chai |  |
| 206 |  |  [Text Augmented Spatial Aware Zero-shot Referring Image Segmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.73) |  | 0 |  | Yucheng Suo, Linchao Zhu, Yi Yang |  |
| 207 |  |  [IRFL: Image Recognition of Figurative Language](https://doi.org/10.18653/v1/2023.findings-emnlp.74) |  | 0 |  | Ron Yosef, Yonatan Bitton, Dafna Shahaf |  |
| 208 |  |  [Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization](https://doi.org/10.18653/v1/2023.findings-emnlp.75) |  | 0 |  | Kaihang Pan, Juncheng Li, Hongye Song, Jun Lin, Xiaozhong Liu, Siliang Tang |  |
| 209 |  |  [An Adaptive Prompt Generation Framework for Task-oriented Dialogue System](https://doi.org/10.18653/v1/2023.findings-emnlp.76) |  | 0 |  | Jun Gao, Liuyu Xiang, Huijia Wu, Han Zhao, Yiqi Tong, Zhaofeng He |  |
| 210 |  |  [Temporal Knowledge Graph Reasoning Based on N-tuple Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.77) |  | 0 |  | Zhongni Hou, Xiaolong Jin, Zixuan Li, Long Bai, Saiping Guan, Yutao Zeng, Jiafeng Guo, Xueqi Cheng |  |
| 211 |  |  [Make Your Decision Convincing! A Unified Two-Stage Framework: Self-Attribution and Decision-Making](https://doi.org/10.18653/v1/2023.findings-emnlp.78) |  | 0 |  | Yanrui Du, Sendong Zhao, Haochun Wang, Yuhan Chen, Rui Bai, Zewen Qiang, Muzhen Cai, Bing Qin |  |
| 212 |  |  [Adaptive Structure Induction for Aspect-based Sentiment Analysis with Spectral Perspective](https://doi.org/10.18653/v1/2023.findings-emnlp.79) |  | 0 |  | Hao Niu, Yun Xiong, Xiaosu Wang, Wenjing Yu, Yao Zhang, Zhonglei Guo |  |
| 213 |  |  [NovaCOMET: Open Commonsense Foundation Models with Symbolic Knowledge Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.80) |  | 0 |  | Peter West, Ronan Le Bras, Taylor Sorensen, Bill Yuchen Lin, Liwei Jiang, Ximing Lu, Khyathi Raghavi Chandu, Jack Hessel, Ashutosh Baheti, Chandra Bhagavatula, Yejin Choi |  |
| 214 |  |  [In-Context Demonstration Selection with Cross Entropy Difference](https://doi.org/10.18653/v1/2023.findings-emnlp.81) |  | 0 |  | Dan Iter, Reid Pryzant, Ruochen Xu, Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu |  |
| 215 |  |  [The Past, Present, and Future of Typological Databases in NLP](https://doi.org/10.18653/v1/2023.findings-emnlp.82) |  | 0 |  | Emi Baylor, Esther Ploeger, Johannes Bjerva |  |
| 216 |  |  [SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations](https://doi.org/10.18653/v1/2023.findings-emnlp.83) |  | 0 |  | Yirong Chen, Xiaofen Xing, Jingkai Lin, Huimin Zheng, Zhenyu Wang, Qi Liu, Xiangmin Xu |  |
| 217 |  |  [Can ChatGPT Assess Human Personalities? A General Evaluation Framework](https://doi.org/10.18653/v1/2023.findings-emnlp.84) |  | 0 |  | Haocong Rao, Cyril Leung, Chunyan Miao |  |
| 218 |  |  [MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model](https://doi.org/10.18653/v1/2023.findings-emnlp.85) |  | 0 |  | Le Zhang, Yihong Wu, Fengran Mo, JianYun Nie, Aishwarya Agrawal |  |
| 219 |  |  [Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search](https://doi.org/10.18653/v1/2023.findings-emnlp.86) |  | 0 |  | Kelong Mao, Zhicheng Dou, Fengran Mo, Jiewen Hou, Haonan Chen, Hongjin Qian |  |
| 220 |  |  [DocAsRef: An Empirical Study on Repurposing Reference-based Summary Quality Metrics as Reference-free Metrics](https://doi.org/10.18653/v1/2023.findings-emnlp.87) |  | 0 |  | Forrest Sheng Bao, Ruixuan Tu, Ge Luo, Yinfei Yang, Hebi Li, Minghui Qiu, Youbiao He, Cen Chen |  |
| 221 |  |  [Toxicity in chatgpt: Analyzing persona-assigned language models](https://doi.org/10.18653/v1/2023.findings-emnlp.88) |  | 0 |  | Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan |  |
| 222 |  |  [Execution-Based Evaluation for Open-Domain Code Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.89) |  | 0 |  | Zhiruo Wang, Shuyan Zhou, Daniel Fried, Graham Neubig |  |
| 223 |  |  [Syntax-Aware Retrieval Augmented Code Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.90) |  | 0 |  | Xiangyu Zhang, Yu Zhou, Guang Yang, Taolue Chen |  |
| 224 |  |  [Selecting Key Views for Zero-Shot Entity Linking](https://doi.org/10.18653/v1/2023.findings-emnlp.91) |  | 0 |  | Xuhui Sui, Ying Zhang, Kehui Song, Baohang Zhou, Xiaojie Yuan, Wensheng Zhang |  |
| 225 |  |  [Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term](https://doi.org/10.18653/v1/2023.findings-emnlp.92) |  | 0 |  | YiLi Hsu, ShihChieh Dai, Aiping Xiong, LunWei Ku |  |
| 226 |  |  [Improving the Robustness of Summarization Models by Detecting and Removing Input Noise](https://doi.org/10.18653/v1/2023.findings-emnlp.93) |  | 0 |  | Kundan Krishna, Yao Zhao, Jie Ren, Balaji Lakshminarayanan, Jiaming Luo, Mohammad Saleh, Peter J. Liu |  |
| 227 |  |  [How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts](https://doi.org/10.18653/v1/2023.findings-emnlp.94) |  | 0 |  | Tharindu Kumarage, Paras Sheth, Raha Moraffah, Joshua Garland, Huan Liu |  |
| 228 |  |  [Knowledge is a Region in Weight Space for Fine-tuned Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.95) |  | 0 |  | Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, Leshem Choshen |  |
| 229 |  |  [Unveiling the Multi-Annotation Process: Examining the Influence of Annotation Quantity and Instance Difficulty on Model Performance](https://doi.org/10.18653/v1/2023.findings-emnlp.96) |  | 0 |  | Pritam Kadasi, Mayank Singh |  |
| 230 |  |  [On the Risk of Misinformation Pollution with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.97) |  | 0 |  | Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, MinYen Kan, William Yang Wang |  |
| 231 |  |  [Dolphin: A Challenging and Diverse Benchmark for Arabic NLG](https://doi.org/10.18653/v1/2023.findings-emnlp.98) |  | 0 |  | El Moatez Billah Nagoudi, AbdelRahim A. Elmadany, Ahmed Oumar ElShangiti, Muhammad AbdulMageed |  |
| 232 |  |  [Hierarchical Enhancement Framework for Aspect-based Argument Mining](https://doi.org/10.18653/v1/2023.findings-emnlp.99) |  | 0 |  | Yujie Fu, Yang Li, Suge Wang, Xiaoli Li, Deyu Li, Jian Liao, Jianxing Zheng |  |
| 233 |  |  [MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.100) |  | 0 |  | Yifan Wei, Yisong Su, Huanhuan Ma, Xiaoyan Yu, Fangyu Lei, Yuanzhe Zhang, Jun Zhao, Kang Liu |  |
| 234 |  |  [What Makes Chain-of-Thought Prompting Effective? A Counterfactual Study](https://doi.org/10.18653/v1/2023.findings-emnlp.101) |  | 0 |  | Aman Madaan, Katherine Hermann, Amir Yazdanbakhsh |  |
| 235 |  |  [Perceptual Structure in the absence of grounding: the impact of abstractedness and subjectivity in color language for LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.102) |  | 0 |  | Pablo Loyola, Edison MarreseTaylor, Andrés Hoyos Idrobo |  |
| 236 |  |  [A Dataset for Investigating the Impact of Context for Offensive Language Detection in Tweets](https://doi.org/10.18653/v1/2023.findings-emnlp.103) |  | 0 |  | Musa Ihtiyar, Ömer Özdemir, Mustafa Erengül, Arzucan Özgür |  |
| 237 |  |  [Remember what you did so you know what to do next](https://doi.org/10.18653/v1/2023.findings-emnlp.104) |  | 0 |  | Manuel R. Ciosici, Alex Hedges, Yash Kankanampati, Justin Martin, Marjorie Freedman, Ralph M. Weischedel |  |
| 238 |  |  [An Empirical Study of Multimodal Model Merging](https://doi.org/10.18653/v1/2023.findings-emnlp.105) |  | 0 |  | YiLin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, Lijuan Wang |  |
| 239 |  |  [Learning to Abstract with Nonparametric Variational Information Bottleneck](https://doi.org/10.18653/v1/2023.findings-emnlp.106) |  | 0 |  | Melika Behjati, Fabio Fehr, James Henderson |  |
| 240 |  |  [Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document](https://doi.org/10.18653/v1/2023.findings-emnlp.107) |  | 0 |  | Xiangnan Chen, Qian Xiao, Juncheng Li, Duo Dong, Jun Lin, Xiaozhong Liu, Siliang Tang |  |
| 241 |  |  [Learning to Compose Representations of Different Encoder Layers towards Improving Compositional Generalization](https://doi.org/10.18653/v1/2023.findings-emnlp.108) |  | 0 |  | Lei Lin, Shuangtao Li, Yafang Zheng, Biao Fu, Shan Liu, Yidong Chen, Xiaodong Shi |  |
| 242 |  |  [SelectNoise: Unsupervised Noise Injection to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.109) |  | 0 |  | Maharaj Brahma, Kaushal Maurya, Maunendra Sankar Desarkar |  |
| 243 |  |  [Breaking Boundaries in Retrieval Systems: Unsupervised Domain Adaptation with Denoise-Finetuning](https://doi.org/10.18653/v1/2023.findings-emnlp.110) |  | 0 |  | Che Chen, Ching Yang, ChunYi Lin, HungYu Kao |  |
| 244 |  |  [Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach](https://doi.org/10.18653/v1/2023.findings-emnlp.111) |  | 0 |  | Zheyuan Zhang, Jifan Yu, Juanzi Li, Lei Hou |  |
| 245 |  |  [Simpler neural networks prefer subregular languages](https://doi.org/10.18653/v1/2023.findings-emnlp.112) |  | 0 |  | Charles Torres, Richard Futrell |  |
| 246 |  |  [Simple Hardware-Efficient PCFGs with Independent Left and Right Productions](https://doi.org/10.18653/v1/2023.findings-emnlp.113) |  | 0 |  | Wei Liu, Songlin Yang, Yoon Kim, Kewei Tu |  |
| 247 |  |  [R³ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context](https://doi.org/10.18653/v1/2023.findings-emnlp.114) |  | 0 |  | Qingyuan Tian, Hanlun Zhu, Lei Wang, Yang Li, Yunshi Lan |  |
| 248 |  |  [Quality Estimation-Assisted Automatic Post-Editing](https://doi.org/10.18653/v1/2023.findings-emnlp.115) |  | 0 |  | Sourabh Dattatray Deoghare, Diptesh Kanojia, Frédéric Blain, Tharindu Ranasinghe, Pushpak Bhattacharyya |  |
| 249 |  |  [Adapter Pruning using Tropical Characterization](https://doi.org/10.18653/v1/2023.findings-emnlp.116) |  | 0 |  | Rishabh Bhardwaj, Tushar Vaidya, Soujanya Poria |  |
| 250 |  |  [Self-Supervised Rule Learning to Link Text Segments to Relational Elements of Structured Knowledge](https://doi.org/10.18653/v1/2023.findings-emnlp.117) |  | 0 |  | Shajith Ikbal, Udit Sharma, Hima Karanam, Sumit Neelam, Ronny Luss, Dheeraj Sreedhar, Pavan Kapanipathi, Naweed Khan, Kyle Erwin, Ndivhuwo Makondo, Ibrahim Abdelaziz, Achille Fokoue, Alexander Gray, Maxwell Crouse, Subhajit Chaudhury, Chitra Subramanian |  |
| 251 |  |  [TaTA: A Multilingual Table-to-Text Dataset for African Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.118) |  | 0 |  | Sebastian Gehrmann, Sebastian Ruder, Vitaly Nikolaev, Jan A. Botha, Michael Chavinda, Ankur P. Parikh, Clara Rivera |  |
| 252 |  |  [Explain-then-translate: an analysis on improving program translation with self-generated explanations](https://doi.org/10.18653/v1/2023.findings-emnlp.119) |  | 0 |  | Zilu Tang, Mayank Agarwal, Alexander Shypula, Bailin Wang, Derry Wijaya, Jie Chen, Yoon Kim |  |
| 253 |  |  [Can Brain Signals Reveal Inner Alignment with Human Languages?](https://doi.org/10.18653/v1/2023.findings-emnlp.120) |  | 0 |  | Jielin Qiu, William Han, Jiacheng Zhu, Mengdi Xu, Douglas Weber, Bo Li, Ding Zhao |  |
| 254 |  |  [DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource Event Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.121) |  | 0 |  | Gang Zhao, Xiaocheng Gong, Xinjie Yang, Guanting Dong, Shudong Lu, Si Li |  |
| 255 |  |  [GLGR: Question-aware Global-to-Local Graph Reasoning for Multi-party Dialogue Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.122) |  | 0 |  | Yanling Li, Bowei Zou, Yifan Fan, Xibo Li, Ai Ti Aw, Yu Hong |  |
| 256 |  |  [Towards Mitigating LLM Hallucination via Self Reflection](https://doi.org/10.18653/v1/2023.findings-emnlp.123) |  | 0 |  | Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung |  |
| 257 |  |  [Making Body Movement in Sign Language Corpus Accessible for Linguists and Machines with Three-Dimensional Normalization of MediaPipe](https://doi.org/10.18653/v1/2023.findings-emnlp.124) |  | 0 |  | Victor Skobov, Mayumi Bono |  |
| 258 |  |  [XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.125) |  | 0 |  | Sebastian Ruder, Jonathan H. Clark, Alexander Gutkin, Mihir Kale, Min Ma, Massimo Nicosia, Shruti Rijhwani, Parker Riley, Jean Michel A. Sarr, Xinyi Wang, John Wieting, Nitish Gupta, Anna Katanova, Christo Kirov, Dana L. Dickinson, Brian Roark, Bidisha Samanta, Connie Tao, David Ifeoluwa Adelani, Vera Axelrod, Isaac Caswell, Colin Cherry, Dan Garrette, R. Reeve Ingle, Melvin Johnson, Dmitry Panteleev, Partha Talukdar |  |
| 259 |  |  [DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models](https://doi.org/10.18653/v1/2023.findings-emnlp.126) |  | 0 |  | Shengguang Wu, Mei Yuan, Qi Su |  |
| 260 |  |  [DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias](https://doi.org/10.18653/v1/2023.findings-emnlp.127) |  | 0 |  | Mahdi Zakizadeh, Kaveh Eskandari Miandoab, Mohammad Taher Pilehvar |  |
| 261 |  |  [Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens](https://doi.org/10.18653/v1/2023.findings-emnlp.128) |  | 0 |  | ByungDoh Oh, William Schuler |  |
| 262 |  |  [ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination](https://doi.org/10.18653/v1/2023.findings-emnlp.129) |  | 0 |  | Dongfang Li, Jindi Yu, Baotian Hu, Zhenran Xu, Min Zhang |  |
| 263 |  |  [CLASS: A Design Framework for Building Intelligent Tutoring Systems Based on Learning Science principles](https://doi.org/10.18653/v1/2023.findings-emnlp.130) |  | 0 |  | Shashank Sonkar, Naiming Liu, Debshila Basu Mallick, Richard G. Baraniuk |  |
| 264 |  |  [Normal-Abnormal Decoupling Memory for Medical Report Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.131) |  | 0 |  | Guosheng Zhao, Yan Yan, Zijian Zhao |  |
| 265 |  |  [mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations](https://doi.org/10.18653/v1/2023.findings-emnlp.132) |  | 0 |  | Jonas Pfeiffer, Francesco Piccinno, Massimo Nicosia, Xinyi Wang, Machel Reid, Sebastian Ruder |  |
| 266 |  |  [ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories](https://doi.org/10.18653/v1/2023.findings-emnlp.133) |  | 0 |  | Heming Xia, Qingxiu Dong, Lei Li, Jingjing Xu, Tianyu Liu, Ziwei Qin, Zhifang Sui |  |
| 267 |  |  [MultiCoNER v2: a Large Multilingual dataset for Fine-grained and Noisy Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.134) |  | 0 |  | Besnik Fetahu, Zhiyu Chen, Sudipta Kar, Oleg Rokhlenko, Shervin Malmasi |  |
| 268 |  |  [A Query-Parallel Machine Reading Comprehension Framework for Low-resource NER](https://doi.org/10.18653/v1/2023.findings-emnlp.135) |  | 0 |  | Yuhao Zhang, Yongliang Wang |  |
| 269 |  |  [BiSPN: Generating Entity Set and Relation Set Coherently in One Pass](https://doi.org/10.18653/v1/2023.findings-emnlp.136) |  | 0 |  | Yuxin He, Buzhou Tang |  |
| 270 |  |  [MEEP: Is this Engaging? Prompting Large Language Models for Dialogue Evaluation in Multilingual Settings](https://doi.org/10.18653/v1/2023.findings-emnlp.137) |  | 0 |  | Amila Ferron, Amber Shore, Ekata Mitra, Ameeta Agrawal |  |
| 271 |  |  [Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.138) |  | 0 |  | Jaeyoung Choe, Keonwoong Noh, Nayeon Kim, Seyun Ahn, Woohwan Jung |  |
| 272 |  |  [LLMDet: A Third Party Large Language Models Generated Text Detection Tool](https://doi.org/10.18653/v1/2023.findings-emnlp.139) |  | 0 |  | Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng, TatSeng Chua |  |
| 273 |  |  [RECAP: Towards Precise Radiology Report Generation via Dynamic Disease Progression Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.140) |  | 0 |  | Wenjun Hou, Yi Cheng, Kaishuai Xu, Wenjie Li, Jiang Liu |  |
| 274 |  |  [Causal Intervention for Abstractive Related Work Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.141) |  | 0 |  | Jiachang Liu, Qi Zhang, Chongyang Shi, Usman Naseem, Shoujin Wang, Liang Hu, Ivor W. Tsang |  |
| 275 |  |  [G-SPEED: General SParse Efficient Editing MoDel](https://doi.org/10.18653/v1/2023.findings-emnlp.142) |  | 0 |  | Haoke Zhang, Yue Wang, Juntao Li, Xiabing Zhou, Min Zhang |  |
| 276 |  |  [Attack Prompt Generation for Red Teaming and Defending Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.143) |  | 0 |  | Boyi Deng, Wenjie Wang, Fuli Feng, Yang Deng, Qifan Wang, Xiangnan He |  |
| 277 |  |  [Smart "Chef": Verifying the Effect of Role-based Paraphrasing for Aspect Term Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.144) |  | 0 |  | Jiaxiang Chen, Yu Hong, Qingting Xu, Jianmin Yao |  |
| 278 |  |  [Multi-Defendant Legal Judgment Prediction via Hierarchical Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.145) |  | 0 |  | Yougang Lyu, Jitai Hao, Zihan Wang, Kai Zhao, Shen Gao, Pengjie Ren, Zhumin Chen, Fang Wang, Zhaochun Ren |  |
| 279 |  |  [Interpreting Indirect Answers to Yes-No Questions in Multiple Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.146) |  | 0 |  | Zijie Wang, Md Mosharaf Hossain, Shivam Mathur, Terry Cruz Melo, Kadir Bulut Özler, Keun Hee Park, Jacob Quintero, MohammadHossein Rezaei, Shreya Nupur Shakya, Md Nayem Uddin, Eduardo Blanco |  |
| 280 |  |  [Generalizing Few-Shot Named Entity Recognizers to Unseen Domains with Type-Related Features](https://doi.org/10.18653/v1/2023.findings-emnlp.147) |  | 0 |  | Zihan Wang, Ziqi Zhao, Zhumin Chen, Pengjie Ren, Maarten de Rijke, Zhaochun Ren |  |
| 281 |  |  [Intervention-Based Alignment of Code Search with Execution Feedback](https://doi.org/10.18653/v1/2023.findings-emnlp.148) |  | 0 |  | Hojae Han, Minsoo Kim, Seungwon Hwang, Nan Duan, Shuai Lu |  |
| 282 |  |  [Enhancing Neural Machine Translation with Semantic Units](https://doi.org/10.18653/v1/2023.findings-emnlp.149) |  | 0 |  | Langlin Huang, Shuhao Gu, Zhuocheng Zhang, Yang Feng |  |
| 283 |  |  [DRAFT: Dense Retrieval Augmented Few-shot Topic classifier Framework](https://doi.org/10.18653/v1/2023.findings-emnlp.150) |  | 0 |  | Keonwoo Kim, Younggun Lee |  |
| 284 |  |  [A Framework for Exploring Player Perceptions of LLM-Generated Dialogue in Commercial Video Games](https://doi.org/10.18653/v1/2023.findings-emnlp.151) |  | 0 |  | Nader Akoury, Qian Yang, Mohit Iyyer |  |
| 285 |  |  [Generative Calibration for In-context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.152) |  | 0 |  | Zhongtao Jiang, Yuanzhe Zhang, Cao Liu, Jun Zhao, Kang Liu |  |
| 286 |  |  [Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.153) |  | 0 |  | Xilai Ma, Jing Li, Min Zhang |  |
| 287 |  |  [AdaTranS: Adapting with Boundary-based Shrinking for End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.154) |  | 0 |  | Xingshan Zeng, Liangyou Li, Qun Liu |  |
| 288 |  |  [No offence, Bert - I insult only humans! Multilingual sentence-level attack on toxicity detection networks](https://doi.org/10.18653/v1/2023.findings-emnlp.155) |  | 0 |  | Sergey Berezin, Reza Farahbakhsh, Noël Crespi |  |
| 289 |  |  [Manipulating the Perceived Personality Traits of Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.156) |  | 0 |  | Graham Caron, Shashank Srivastava |  |
| 290 |  |  [WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia](https://doi.org/10.18653/v1/2023.findings-emnlp.157) |  | 0 |  | Sina J. Semnani, Violet Z. Yao, Heidi C. Zhang, Monica S. Lam |  |
| 291 |  |  [Automated Few-Shot Classification with Instruction-Finetuned Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.158) |  | 0 |  | Rami Aly, Xingjian Shi, Kaixiang Lin, Aston Zhang, Andrew Gordon Wilson |  |
| 292 |  |  [Meta-Learning of Prompt Generation for Lightweight Prompt Engineering on Language-Model-as-a-Service](https://doi.org/10.18653/v1/2023.findings-emnlp.159) |  | 0 |  | Hyeonmin Ha, Jihye Lee, Wookje Han, ByungGon Chun |  |
| 293 |  |  [Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction](https://doi.org/10.18653/v1/2023.findings-emnlp.160) |  | 0 |  | Siyu Yuan, Jiangjie Chen, Xuyang Ge, Yanghua Xiao, Deqing Yang |  |
| 294 |  |  [HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings](https://doi.org/10.18653/v1/2023.findings-emnlp.161) |  | 0 |  | Zhuofeng Wu, Chaowei Xiao, V. G. Vinod Vydiswaran |  |
| 295 |  |  [Density-Aware Prototypical Network for Few-Shot Relation Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.162) |  | 0 |  | Jianfeng Wu, Mengting Hu, Yike Wu, Bingzhe Wu, Yalan Xie, Mingming Liu, Renhong Cheng |  |
| 296 |  |  [Improved Training of Deep Text Clustering](https://doi.org/10.18653/v1/2023.findings-emnlp.163) |  | 0 |  | Zonghao Yang, Wenpeng Hu, Yushan Tan, Zhunchen Luo |  |
| 297 |  |  [RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.164) |  | 0 |  | Jingcheng Deng, Liang Pang, Huawei Shen, Xueqi Cheng |  |
| 298 |  |  [RefGPT: Dialogue Generation of GPT, by GPT, and for GPT](https://doi.org/10.18653/v1/2023.findings-emnlp.165) |  | 0 |  | Dongjie Yang, Ruifeng Yuan, Yuantao Fan, Yifei Yang, Zili Wang, Shusen Wang, Hai Zhao |  |
| 299 |  |  [INA: An Integrative Approach for Enhancing Negotiation Strategies with Reward-Based Dialogue Agent](https://doi.org/10.18653/v1/2023.findings-emnlp.166) |  | 0 |  | Zishan Ahmad, Suman Saurabh, Vaishakh Sreekanth Menon, Asif Ekbal, Roshni R. Ramnani, Anutosh Maitra |  |
| 300 |  |  [Large Language Models are Better Reasoners with Self-Verification](https://doi.org/10.18653/v1/2023.findings-emnlp.167) |  | 0 |  | Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao |  |
| 301 |  |  [Multi-Granularity Information Interaction Framework for Incomplete Utterance Rewriting](https://doi.org/10.18653/v1/2023.findings-emnlp.168) |  | 0 |  | Haowei Du, Dinghao Zhang, Chen Li, Yang Li, Dongyan Zhao |  |
| 302 |  |  [Accuracy is not enough: Evaluating Personalization in Summarizers](https://doi.org/10.18653/v1/2023.findings-emnlp.169) |  | 0 |  | Rahul Vansh, Darsh Rank, Sourish Dasgupta, Tanmoy Chakraborty |  |
| 303 |  |  [For Generated Text, Is NLI-Neutral Text the Best Text?](https://doi.org/10.18653/v1/2023.findings-emnlp.170) |  | 0 |  | Michail Mersinias, Kyle Mahowald |  |
| 304 |  |  [Combining Counting Processes and Classification Improves a Stopping Rule for Technology Assisted Review](https://doi.org/10.18653/v1/2023.findings-emnlp.171) |  | 0 |  | Reem Bin Hezam, Mark Stevenson |  |
| 305 |  |  [Complexity-Guided Curriculum Learning for Text Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.172) |  | 0 |  | Nidhi Vakil, Hadi Amiri |  |
| 306 |  |  [CoVariance-based Causal Debiasing for Entity and Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.173) |  | 0 |  | Lin Ren, Yongbin Liu, Yixin Cao, Chunping Ouyang |  |
| 307 |  |  [Multi-label and Multi-target Sampling of Machine Annotation for Computational Stance Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.174) |  | 0 |  | Zhengyuan Liu, Hai Leong Chieu, Nancy F. Chen |  |
| 308 |  |  [In What Languages are Generative Language Models the Most Formal? Analyzing Formality Distribution across Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.175) |  | 0 |  | Asim Ersoy, Gerson Vizcarra, Tasmiah Tahsin Mayeesha, Benjamin Muller |  |
| 309 |  |  [MaXM: Towards Multilingual Visual Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.176) |  | 0 |  | Soravit Changpinyo, Linting Xue, Michal Yarom, Ashish V. Thapliyal, Idan Szpektor, Julien Amelot, Xi Chen, Radu Soricut |  |
| 310 |  |  [Efficient Latent Variable Modeling for Knowledge-Grounded Dialogue Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.177) |  | 0 |  | Gunsoo Han, Daejin Jo, Daniel Wontae Nam, Eunseop Yoon, Taehwan Kwon, Seungeun Rho, KyoungWoon On, Chang Dong Yoo, Sungwoong Kim |  |
| 311 |  |  [Ask To The Point: Open-Domain Entity-Centric Question Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.178) |  | 0 |  | Yuxiang Liu, Jie Huang, Kevin ChenChuan Chang |  |
| 312 |  |  [Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.179) |  | 0 |  | Jinyuan Wang, Junlong Li, Hai Zhao |  |
| 313 |  |  [CASE: Commonsense-Augmented Score with an Expanded Answer Space](https://doi.org/10.18653/v1/2023.findings-emnlp.180) |  | 0 |  | Wenkai Chen, Sahithya Ravi, Vered Shwartz |  |
| 314 |  |  [GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.181) |  | 0 |  | Yichuan Li, Kaize Ding, Kyumin Lee |  |
| 315 |  |  [Sources of Hallucination by Large Language Models on Inference Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.182) |  | 0 |  | Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, Mark Steedman |  |
| 316 |  |  [Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer](https://doi.org/10.18653/v1/2023.findings-emnlp.183) |  | 0 |  | Qingru Zhang, Dhananjay Ram, Cole Hawkins, Sheng Zha, Tuo Zhao |  |
| 317 |  |  [Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with Auxiliary Refined Knowledge](https://doi.org/10.18653/v1/2023.findings-emnlp.184) |  | 0 |  | Jinyuan Li, Han Li, Zhuo Pan, Di Sun, Jiahao Wang, Wenkun Zhang, Gang Pan |  |
| 318 |  |  [Understanding HTML with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.185) |  | 0 |  | Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin V. Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, Aleksandra Faust |  |
| 319 |  |  [The PEACE-Reviews dataset: Modeling Cognitive Appraisals in Emotion Text Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.186) |  | 0 |  | Gerard Yeo, Kokil Jaidka |  |
| 320 |  |  [UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model](https://doi.org/10.18653/v1/2023.findings-emnlp.187) |  | 0 |  | Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Lin, Fei Huang |  |
| 321 |  |  [Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback](https://doi.org/10.18653/v1/2023.findings-emnlp.188) |  | 0 |  | Wei Shen, Rui Zheng, WenYu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 322 |  |  [Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions](https://doi.org/10.18653/v1/2023.findings-emnlp.189) |  | 0 |  | Ziyue Wang, Chi Chen, Peng Li, Yang Liu |  |
| 323 |  |  [Take a Closer Look at Multilinguality! Improve Multilingual Pre-Training Using Monolingual Corpora Only](https://doi.org/10.18653/v1/2023.findings-emnlp.190) |  | 0 |  | Jinliang Lu, Yu Lu, Jiajun Zhang |  |
| 324 |  |  [LogiCoT: Logical Chain-of-Thought Instruction Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.191) |  | 0 |  | Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, Yue Zhang |  |
| 325 |  |  [Hiding in Plain Sight: Tweets with Hate Speech Masked by Homoglyphs](https://doi.org/10.18653/v1/2023.findings-emnlp.192) |  | 0 |  | Portia Cooper, Mihai Surdeanu, Eduardo Blanco |  |
| 326 |  |  [Reducing Spurious Correlations in Aspect-based Sentiment Analysis with Explanation from Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.193) |  | 0 |  | Qianlong Wang, Keyang Ding, Bin Liang, Min Yang, Ruifeng Xu |  |
| 327 |  |  [High-quality argumentative information in low resources approaches improve counter-narrative generation](https://doi.org/10.18653/v1/2023.findings-emnlp.194) |  | 0 |  | Damián Ariel Furman, Pablo Torres, José A. Rodríguez, Diego Letzen, Maria Vanina Martinez, Laura Alonso Alemany |  |
| 328 |  |  [A Reference-free Segmentation Quality Index (SegReFree)](https://doi.org/10.18653/v1/2023.findings-emnlp.195) |  | 0 |  | Evan Lucas, Dylan Kangas, Timothy C. Havens |  |
| 329 |  |  [In-context Learning for Few-shot Multimodal Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.196) |  | 0 |  | Chenran Cai, Qianlong Wang, Bin Liang, Bing Qin, Min Yang, KamFai Wong, Ruifeng Xu |  |
| 330 |  |  [On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study](https://doi.org/10.18653/v1/2023.findings-emnlp.197) |  | 0 |  | Polina Zablotskaia, Du Phan, Joshua Maynez, Shashi Narayan, Jie Ren, Jeremiah Z. Liu |  |
| 331 |  |  [Handshape-Aware Sign Language Recognition: Extended Datasets and Exploration of Handshape-Inclusive Methods](https://doi.org/10.18653/v1/2023.findings-emnlp.198) |  | 0 |  | Xuan Zhang, Kevin Duh |  |
| 332 |  |  [SimCKP: Simple Contrastive Learning of Keyphrase Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.199) |  | 0 |  | Minseok Choi, Chaeheon Gwak, Seho Kim, Si Hyeong Kim, Jaegul Choo |  |
| 333 |  |  [LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain](https://doi.org/10.18653/v1/2023.findings-emnlp.200) |  | 0 |  | Joel Niklaus, Veton Matoshi, Pooja Rani, Andrea Galassi, Matthias Stürmer, Ilias Chalkidis |  |
| 334 |  |  [Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.201) |  | 0 |  | AnZi Yen, WeiLing Hsu |  |
| 335 |  |  [Simultaneous Machine Translation with Tailored Reference](https://doi.org/10.18653/v1/2023.findings-emnlp.202) |  | 0 |  | Shoutao Guo, Shaolei Zhang, Yang Feng |  |
| 336 |  |  [Dynamic Voting for Efficient Reasoning in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.203) |  | 0 |  | Mingfeng Xue, Dayiheng Liu, Wenqiang Lei, Xingzhang Ren, Baosong Yang, Jun Xie, Yidan Zhang, Dezhong Peng, Jiancheng Lv |  |
| 337 |  |  [On Surgical Fine-tuning for Language Encoders](https://doi.org/10.18653/v1/2023.findings-emnlp.204) |  | 0 |  | Abhilasha Lodha, Gayatri Belapurkar, Saloni Chalkapurkar, Yuanming Tao, Reshmi Ghosh, Samyadeep Basu, Dmitrii Petrov, Soundararajan Srinivasan |  |
| 338 |  |  [AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.205) |  | 0 |  | Siqi Ouyang, Lei Li |  |
| 339 |  |  [Measuring Faithful and Plausible Visual Grounding in VQA](https://doi.org/10.18653/v1/2023.findings-emnlp.206) |  | 0 |  | Daniel Reich, Felix Putze, Tanja Schultz |  |
| 340 |  |  [Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.207) |  | 0 |  | Sukmin Cho, Jeongyeon Seo, Soyeong Jeong, Jong C. Park |  |
| 341 |  |  [Can you Summarize my learnings? Towards Perspective-based Educational Dialogue Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.208) |  | 0 |  | Raghav Jain, Tulika Saha, Jhagrut Lalwani, Sriparna Saha |  |
| 342 |  |  [Adaptive Textual Label Noise Learning based on Pre-trained Models](https://doi.org/10.18653/v1/2023.findings-emnlp.209) |  | 0 |  | Shaohuan Cheng, Wenyu Chen, Mingsheng Fu, Xuanting Xie, Hong Qu |  |
| 343 |  |  [Towards Informative Open-ended Text Generation with Dynamic Knowledge Triples](https://doi.org/10.18653/v1/2023.findings-emnlp.210) |  | 0 |  | Zixuan Ren, Yang Zhao, Chengqing Zong |  |
| 344 |  |  [Novel Relation Detection: Discovering Unknown Relation Types via Multi-Strategy Self-Supervised Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.211) |  | 0 |  | Qingbin Liu, Yin Kung, Yanchao Hao, Dianbo Sui, Siyuan Cheng, Xi Chen, Ningyu Zhang, Jiaoyan Chen |  |
| 345 |  |  [Ask Language Model to Clean Your Noisy Translation Data](https://doi.org/10.18653/v1/2023.findings-emnlp.212) |  | 0 |  | Quinten Bolding, Baohao Liao, Brandon James Denis, Jun Luo, Christof Monz |  |
| 346 |  |  [Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users](https://doi.org/10.18653/v1/2023.findings-emnlp.213) |  | 0 |  | Yohan Jo, Xinyan Zhao, Arijit Biswas, Nikoletta Basiou, Vincent Auvray, Nikolaos Malandrakis, Angeliki Metallinou, Alexandros Potamianos |  |
| 347 |  |  [Extractive Summarization via ChatGPT for Faithful Summary Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.214) |  | 0 |  | Haopeng Zhang, Xiao Liu, Jiawei Zhang |  |
| 348 |  |  [MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization](https://doi.org/10.18653/v1/2023.findings-emnlp.215) |  | 0 |  | Yuyan Chen, Zhihao Wen, Ge Fan, Zhengyu Chen, Wei Wu, Dayiheng Liu, Zhixu Li, Bang Liu, Yanghua Xiao |  |
| 349 |  |  [PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.216) |  | 0 |  | Tao Yang, Tianyuan Shi, Fanqi Wan, Xiaojun Quan, Qifan Wang, Bingzhe Wu, Jiaxiang Wu |  |
| 350 |  |  [Harnessing the power of LLMs: Evaluating human-AI text co-creation through the lens of news headline generation](https://doi.org/10.18653/v1/2023.findings-emnlp.217) |  | 0 |  | Zijian Ding, Alison SmithRenner, Wenjuan Zhang, Joel R. Tetreault, Alejandro Jaimes |  |
| 351 |  |  [NERetrieve: Dataset for Next Generation Named Entity Recognition and Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.218) |  | 0 |  | Uri Katz, Matan Vetzler, Amir David Nissan Cohen, Yoav Goldberg |  |
| 352 |  |  [SWEET - Weakly Supervised Person Name Extraction for Fighting Human Trafficking](https://doi.org/10.18653/v1/2023.findings-emnlp.219) |  | 0 |  | Javin Liu, Hao Yu, Vidya Sujaya, Pratheeksha Nair, Kellin Pelrine, Reihaneh Rabbany |  |
| 353 |  |  [Watermarking LLMs with Weight Quantization](https://doi.org/10.18653/v1/2023.findings-emnlp.220) |  | 0 |  | Linyang Li, Botian Jiang, Pengyu Wang, Ke Ren, Hang Yan, Xipeng Qiu |  |
| 354 |  |  [Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.221) |  | 0 |  | Roshanak Mirzaee, Parisa Kordjamshidi |  |
| 355 |  |  [PsyAttention: Psychological Attention Model for Personality Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.222) |  | 0 |  | Baohua Zhang, Yongyi Huang, Wenyao Cui, Huaping Zhang, Jianyun Shang |  |
| 356 |  |  [RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training](https://doi.org/10.18653/v1/2023.findings-emnlp.223) |  | 0 |  | Jaehyung Kim, Yuning Mao, Rui Hou, Hanchao Yu, Davis Liang, Pascale Fung, Qifan Wang, Fuli Feng, Lifu Huang, Madian Khabsa |  |
| 357 |  |  [The Law and NLP: Bridging Disciplinary Disconnects](https://doi.org/10.18653/v1/2023.findings-emnlp.224) |  | 0 |  | Robert Mahari, Dominik Stammbach, Elliott Ash, Alex Pentland |  |
| 358 |  |  [Symbolization, Prompt, and Classification: A Framework for Implicit Speaker Identification in Novels](https://doi.org/10.18653/v1/2023.findings-emnlp.225) |  | 0 |  | Yue Chen, TianWei He, Hongbin Zhou, JiaChen Gu, Heng Lu, ZhenHua Ling |  |
| 359 |  |  [Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.226) |  | 0 |  | Gabriel Sarch, Yue Wu, Michael J. Tarr, Katerina Fragkiadaki |  |
| 360 |  |  [ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought](https://doi.org/10.18653/v1/2023.findings-emnlp.227) |  | 0 |  | Hanchong Zhang, Ruisheng Cao, Lu Chen, Hongshen Xu, Kai Yu |  |
| 361 |  |  [Manifold-Preserving Transformers are Effective for Short-Long Range Encoding](https://doi.org/10.18653/v1/2023.findings-emnlp.228) |  | 0 |  | Ayan Sengupta, Md. Shad Akhtar, Tanmoy Chakraborty |  |
| 362 |  |  [ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.229) |  | 0 |  | Martin Vejvar, Yasutaka Fujimoto |  |
| 363 |  |  [Detecting Syntactic Change with Pre-trained Transformer Models](https://doi.org/10.18653/v1/2023.findings-emnlp.230) |  | 0 |  | Liwen Hou, David Smith |  |
| 364 |  |  [A Word Sense Distribution-based approach for Semantic Change Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.231) |  | 0 |  | Xiaohang Tang, Yi Zhou, Taichi Aida, Procheta Sen, Danushka Bollegala |  |
| 365 |  |  [Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.232) |  | 0 |  | Zheye Deng, Weiqi Wang, Zhaowei Wang, Xin Liu, Yangqiu Song |  |
| 366 |  |  [Improving Conversational Recommendation Systems via Bias Analysis and Language-Model-Enhanced Data Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.233) |  | 0 |  | Xi Wang, Hossein A. Rahmani, Jiqun Liu, Emine Yilmaz |  |
| 367 |  |  [Exploring Graph Pre-training for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.234) |  | 0 |  | Xiaoyi Bao, Zhongqing Wang, Guodong Zhou |  |
| 368 |  |  [DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding](https://doi.org/10.18653/v1/2023.findings-emnlp.235) |  | 0 |  | Thong Nguyen, Xiaobao Wu, Xinshuai Dong, CongDuy Nguyen, SeeKiong Ng, Anh Tuan Luu |  |
| 369 |  |  [Test-time Augmentation for Factual Probing](https://doi.org/10.18653/v1/2023.findings-emnlp.236) |  | 0 |  | Go Kamoda, Benjamin Heinzerling, Keisuke Sakaguchi, Kentaro Inui |  |
| 370 |  |  [Methodological Insights in Detecting Subtle Semantic Shifts with Contextualized and Static Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.237) |  | 0 |  | Sanne Hoeken, Özge Alaçam, Antske Fokkens, Pia Sommerauer |  |
| 371 |  |  [Disfluent Cues for Enhanced Speech Understanding in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.238) |  | 0 |  | Morteza Rohanian, Farhad Nooralahzadeh, Omid Rohanian, David A. Clifton, Michael Krauthammer |  |
| 372 |  |  [Watermarking PLMs on Classification Tasks by Combining Contrastive Learning with Weight Perturbation](https://doi.org/10.18653/v1/2023.findings-emnlp.239) |  | 0 |  | Chenxi Gu, Xiaoqing Zheng, Jianhan Xu, Muling Wu, Cenyuan Zhang, Chengsong Huang, Hua Cai, Xuanjing Huang |  |
| 373 |  |  [BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla Lemmatizer](https://doi.org/10.18653/v1/2023.findings-emnlp.240) |  | 0 |  | Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, Md. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed |  |
| 374 |  |  [Exploring the Sensitivity of LLMs' Decision-Making Capabilities: Insights from Prompt Variations and Hyperparameters](https://doi.org/10.18653/v1/2023.findings-emnlp.241) |  | 0 |  | Manikanta Loya, Divya Sinha, Richard Futrell |  |
| 375 |  |  [Search Augmented Instruction Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.242) |  | 0 |  | Hongyin Luo, Tianhua Zhang, YungSung Chuang, Yuan Gong, Yoon Kim, Xixin Wu, Helen Meng, James R. Glass |  |
| 376 |  |  ["Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters](https://doi.org/10.18653/v1/2023.findings-emnlp.243) |  | 0 |  | Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, KaiWei Chang, Nanyun Peng |  |
| 377 |  |  [TextMixer: Mixing Multiple Inputs for Privacy-Preserving Inference](https://doi.org/10.18653/v1/2023.findings-emnlp.244) |  | 0 |  | Xin Zhou, Yi Lu, Ruotian Ma, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 378 |  |  [FinePrompt: Unveiling the Role of Finetuned Inductive Bias on Compositional Reasoning in GPT-4](https://doi.org/10.18653/v1/2023.findings-emnlp.245) |  | 0 |  | Jeonghwan Kim, Giwon Hong, SungHyon Myaeng, Joyce Jiyoung Whang |  |
| 379 |  |  [Teacher Perception of Automatically Extracted Grammar Concepts for L2 Language Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.246) |  | 0 |  | Aditi Chaudhary, Arun Sampath, Ashwin Sheshadri, Antonios Anastasopoulos, Graham Neubig |  |
| 380 |  |  [Allies: Prompting Large Language Model with Beam Search](https://doi.org/10.18653/v1/2023.findings-emnlp.247) |  | 0 |  | Hao Sun, Xiao Liu, Yeyun Gong, Yan Zhang, Daxin Jiang, Linjun Yang, Nan Duan |  |
| 381 |  |  [Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.248) |  | 0 |  | Liangming Pan, Alon Albalak, Xinyi Wang, William Yang Wang |  |
| 382 |  |  [SiMFy: A Simple Yet Effective Approach for Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.249) |  | 0 |  | Zhengtao Liu, Lei Tan, Mengfan Li, Yao Wan, Hai Jin, Xuanhua Shi |  |
| 383 |  |  [Understanding Translationese in Cross-Lingual Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.250) |  | 0 |  | Jiaan Wang, Fandong Meng, Yunlong Liang, Tingyi Zhang, Jiarong Xu, Zhixu Li, Jie Zhou |  |
| 384 |  |  [The Truth, The Whole Truth, and Nothing but the Truth: A New Benchmark Dataset for Hebrew Text Credibility Assessment](https://doi.org/10.18653/v1/2023.findings-emnlp.251) |  | 0 |  | Ben Hagag, Reut Tsarfaty |  |
| 385 |  |  [IndiSocialFT: Multilingual Word Representation for Indian languages in code-mixed environment](https://doi.org/10.18653/v1/2023.findings-emnlp.252) |  | 0 |  | Saurabh Kumar, Sanasam Ranbir Singh, Sukumar Nandi |  |
| 386 |  |  [Adaptive Hinge Balance Loss for Document-Level Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.253) |  | 0 |  | Jize Wang, Xinyi Le, Xiaodi Peng, Cailian Chen |  |
| 387 |  |  [Answer-state Recurrent Relational Network (AsRRN) for Constructed Response Assessment and Feedback Grouping](https://doi.org/10.18653/v1/2023.findings-emnlp.254) |  | 0 |  | Zhaohui Li, Susan Lloyd, Matthew Beckman, Rebecca J. Passonneau |  |
| 388 |  |  [Low-Resource Comparative Opinion Quintuple Extraction by Data Augmentation with Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.255) |  | 0 |  | Qingting Xu, Yu Hong, Fubang Zhao, Kaisong Song, Yangyang Kang, Jiaxiang Chen, Guodong Zhou |  |
| 389 |  |  [A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.256) |  | 0 |  | Shiping Yang, Renliang Sun, Xiaojun Wan |  |
| 390 |  |  [Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.257) |  | 0 |  | Heming Xia, Tao Ge, Peiyi Wang, SiQing Chen, Furu Wei, Zhifang Sui |  |
| 391 |  |  [APP: Adaptive Prototypical Pseudo-Labeling for Few-shot OOD Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.258) |  | 0 |  | Pei Wang, Keqing He, Yutao Mou, Xiaoshuai Song, Yanan Wu, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu |  |
| 392 |  |  [2INER: Instructive and In-Context Learning on Few-Shot Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.259) |  | 0 |  | Jiasheng Zhang, Xikai Liu, Xinyi Lai, Yan Gao, Shusen Wang, Yao Hu, Yiqing Lin |  |
| 393 |  |  [Generative Emotion Cause Triplet Extraction in Conversations with Commonsense Knowledge](https://doi.org/10.18653/v1/2023.findings-emnlp.260) |  | 0 |  | Fanfan Wang, Jianfei Yu, Rui Xia |  |
| 394 |  |  [Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.261) |  | 0 |  | Sean Xie, Soroush Vosoughi, Saeed Hassanpour |  |
| 395 |  |  [GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence](https://doi.org/10.18653/v1/2023.findings-emnlp.262) |  | 0 |  | Zhihua Wen, Zhiliang Tian, Wei Wu, Yuxin Yang, Yanqi Shi, Zhen Huang, Dongsheng Li |  |
| 396 |  |  [KAPALM: Knowledge grAPh enhAnced Language Models for Fake News Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.263) |  | 0 |  | Jing Ma, Chen Chen, Chunyan Hou, Xiaojie Yuan |  |
| 397 |  |  [Comparing the Evaluation and Production of Loophole Behavior in Humans and Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.264) |  | 0 |  | Sonia K. Murthy, Kiera Parece, Sophie Bridgers, Peng Qian, Tomer D. Ullman |  |
| 398 |  |  [InstructExcel: A Benchmark for Natural Language Instruction in Excel](https://doi.org/10.18653/v1/2023.findings-emnlp.265) |  | 0 |  | Justin Payan, Swaroop Mishra, Mukul Singh, Carina Negreanu, Christian Pölitz, Chitta Baral, Subhro Roy, Rasika Chakravarthy, Benjamin Van Durme, Elnaz Nouri |  |
| 399 |  |  [Hallucination Detection for Grounded Instruction Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.266) |  | 0 |  | Lingjun Zhao, Khanh Nguyen, Hal Daumé III |  |
| 400 |  |  [Definitions Matter: Guiding GPT for Multi-label Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.267) |  | 0 |  | Youri Peskine, Damir Korencic, Ivan Grubisic, Paolo Papotti, Raphaël Troncy, Paolo Rosso |  |
| 401 |  |  [ECHo: A Visio-Linguistic Dataset for Event Causality Inference via Human-Centric Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.268) |  | 0 |  | Yuxi Xie, Guanzhen Li, MinYen Kan |  |
| 402 |  |  [An Empirical Study of Instruction-tuning Large Language Models in Chinese](https://doi.org/10.18653/v1/2023.findings-emnlp.269) |  | 0 |  | Qingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, Weiping Wang |  |
| 403 |  |  [Debiasing Multimodal Models via Causal Information Minimization](https://doi.org/10.18653/v1/2023.findings-emnlp.270) |  | 0 |  | Vaidehi Patil, Adyasha Maharana, Mohit Bansal |  |
| 404 |  |  [Evaluating Emotion Arcs Across Languages: Bridging the Global Divide in Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.271) |  | 0 |  | Daniela Teodorescu, Saif M. Mohammad |  |
| 405 |  |  [Multi-step Jailbreaking Privacy Attacks on ChatGPT](https://doi.org/10.18653/v1/2023.findings-emnlp.272) |  | 0 |  | Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, Yangqiu Song |  |
| 406 |  |  [Chain-of-Thought Embeddings for Stance Detection on Social Media](https://doi.org/10.18653/v1/2023.findings-emnlp.273) |  | 0 |  | Joseph Gatto, Omar Sharif, Sarah Preum |  |
| 407 |  |  [Using LLM for Improving Key Event Discovery: Temporal-Guided News Stream Clustering with Event Summaries](https://doi.org/10.18653/v1/2023.findings-emnlp.274) |  | 0 |  | Nishanth Sridhar Nakshatri, Siyi Liu, Sihao Chen, Dan Roth, Dan Goldwasser, Daniel Hopkins |  |
| 408 |  |  [Descriptive Prompt Paraphrasing for Target-Oriented Multimodal Sentiment Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.275) |  | 0 |  | Dan Liu, Lin Li, Xiaohui Tao, Jian Cui, Qing Xie |  |
| 409 |  |  [Joint Semantic and Strategy Matching for Persuasive Dialogue](https://doi.org/10.18653/v1/2023.findings-emnlp.276) |  | 0 |  | Chuhao Jin, Yutao Zhu, Lingzhen Kong, Shijie Li, Xiao Zhang, Ruihua Song, Xu Chen, Huan Chen, Yuchong Sun, Yu Chen, Jun Xu |  |
| 410 |  |  [Non-Autoregressive Sentence Ordering](https://doi.org/10.18653/v1/2023.findings-emnlp.277) |  | 0 |  | Yi Bin, Wenhao Shi, Bin Ji, Jipeng Zhang, Yujuan Ding, Yang Yang |  |
| 411 |  |  [Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.278) |  | 0 |  | Chenhui Shen, Liying Cheng, XuanPhi Nguyen, Yang You, Lidong Bing |  |
| 412 |  |  [Women Wearing Lipstick: Measuring the Bias Between an Object and Its Related Gender](https://doi.org/10.18653/v1/2023.findings-emnlp.279) |  | 0 |  | Ahmed Sabir, Lluís Padró |  |
| 413 |  |  [FREDSum: A Dialogue Summarization Corpus for French Political Debates](https://doi.org/10.18653/v1/2023.findings-emnlp.280) |  | 0 |  | Virgile Rennard, Guokan Shang, Damien Grari, Julie Hunter, Michalis Vazirgiannis |  |
| 414 |  |  [Towards Zero-shot Relation Extraction in Web Mining: A Multimodal Approach with Relative XML Path](https://doi.org/10.18653/v1/2023.findings-emnlp.281) |  | 0 |  | Zilong Wang, Jingbo Shang |  |
| 415 |  |  [Narrative Style and the Spread of Health Misinformation on Twitter](https://doi.org/10.18653/v1/2023.findings-emnlp.282) |  | 0 |  | Achyutarama R. Ganti, Eslam Ali Hassan Hussein, Steven R. Wilson, Zexin Ma, Xinyan Zhao |  |
| 416 |  |  [HadSkip: Homotopic and Adaptive Layer Skipping of Pre-trained Language Models for Efficient Inference](https://doi.org/10.18653/v1/2023.findings-emnlp.283) |  | 0 |  | Haoyu Wang, Yaqing Wang, Tianci Liu, Tuo Zhao, Jing Gao |  |
| 417 |  |  [Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.284) |  | 0 |  | Zhiyu Chen, Yujie Lu, William Yang Wang |  |
| 418 |  |  [Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.285) |  | 0 |  | Amirhossein Kazemnejad, Mehdi Rezagholizadeh, Prasanna Parthasarathi, Sarath Chandar |  |
| 419 |  |  [Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.286) |  | 0 |  | Jianing Zhou, Ziheng Zeng, Hongyu Gong, Suma Bhat |  |
| 420 |  |  [Information Extraction from Legal Wills: How Well Does GPT-4 Do?](https://doi.org/10.18653/v1/2023.findings-emnlp.287) |  | 0 |  | Alice Saebom Kwak, Cheonkam Jeong, Gaetano Forte, Derek E. Bambauer, Clayton T. Morrison, Mihai Surdeanu |  |
| 421 |  |  [Transparency at the Source: Evaluating and Interpreting Language Models With Access to the True Distribution](https://doi.org/10.18653/v1/2023.findings-emnlp.288) |  | 0 |  | Jaap Jumelet, Willem H. Zuidema |  |
| 422 |  |  [Continual Generalized Intent Discovery: Marching Towards Dynamic and Open-world Intent Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.289) |  | 0 |  | Xiaoshuai Song, Yutao Mou, Keqing He, Yueyan Qiu, Jinxu Zhao, Pei Wang, Weiran Xu |  |
| 423 |  |  [Frugal Prompting for Dialog Models](https://doi.org/10.18653/v1/2023.findings-emnlp.290) |  | 0 |  | Bishal Santra, Sakya Basak, Abhinandan De, Manish Gupta, Pawan Goyal |  |
| 424 |  |  [The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.291) |  | 0 |  | Mutian He, Philip N. Garner |  |
| 425 |  |  [MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space](https://doi.org/10.18653/v1/2023.findings-emnlp.292) |  | 0 |  | Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, Xueqi Cheng, TatSeng Chua |  |
| 426 |  |  [HPE: Answering Complex Questions over Text by Hybrid Question Parsing and Execution](https://doi.org/10.18653/v1/2023.findings-emnlp.293) |  | 0 |  | Ye Liu, Semih Yavuz, Rui Meng, Dragomir Radev, Caiming Xiong, Shafiq Joty, Yingbo Zhou |  |
| 427 |  |  [Length-Adaptive Distillation: Customizing Small Language Model for Dynamic Token Pruning](https://doi.org/10.18653/v1/2023.findings-emnlp.294) |  | 0 |  | Chang Liu, Chongyang Tao, Jianxin Liang, Jiazhan Feng, Tao Shen, Quzhe Huang, Dongyan Zhao |  |
| 428 |  |  [Toxicity, Morality, and Speech Act Guided Stance Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.295) |  | 0 |  | Apoorva Upadhyaya, Marco Fisichella, Wolfgang Nejdl |  |
| 429 |  |  [Reasoning about Ambiguous Definite Descriptions](https://doi.org/10.18653/v1/2023.findings-emnlp.296) |  | 0 |  | Stefan F. Schouten, Peter Bloem, Ilia Markov, Piek Vossen |  |
| 430 |  |  [A Framework for Bidirectional Decoding: Case Study in Morphological Inflection](https://doi.org/10.18653/v1/2023.findings-emnlp.297) |  | 0 |  | Marc E. Canby, Julia Hockenmaier |  |
| 431 |  |  [Text-guided 3D Human Generation from 2D Collections](https://doi.org/10.18653/v1/2023.findings-emnlp.298) |  | 0 |  | TsuJui Fu, Wenhan Xiong, Yixin Nie, Jingyu Liu, Barlas Oguz, William Wang |  |
| 432 |  |  [Statistically Profiling Biases in Natural Language Reasoning Datasets and Models](https://doi.org/10.18653/v1/2023.findings-emnlp.299) |  | 0 |  | Shanshan Huang, Kenny Q. Zhu |  |
| 433 |  |  [Verb Conjugation in Transformers Is Determined by Linear Encodings of Subject Number](https://doi.org/10.18653/v1/2023.findings-emnlp.300) |  | 0 |  | Sophie Hao, Tal Linzen |  |
| 434 |  |  [MUX-PLMs: Data Multiplexing for High-throughput Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.301) |  | 0 |  | Vishvak Murahari, Ameet Deshpande, Carlos E. Jimenez, Izhak Shafran, Mingqiu Wang, Yuan Cao, Karthik Narasimhan |  |
| 435 |  |  [That was the last straw, we need more: Are Translation Systems Sensitive to Disambiguating Context?](https://doi.org/10.18653/v1/2023.findings-emnlp.302) |  | 0 |  | Jaechan Lee, Alisa Liu, Orevaoghene Ahia, Hila Gonen, Noah A. Smith |  |
| 436 |  |  [MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic](https://doi.org/10.18653/v1/2023.findings-emnlp.303) |  | 0 |  | Damien Sileo, Antoine Lernould |  |
| 437 |  |  [LATENTLOGIC: Learning Logic Rules in Latent Space over Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.304) |  | 0 |  | Junnan Liu, Qianren Mao, Chenghua Lin, Yangqiu Song, Jianxin Li |  |
| 438 |  |  [RobustEmbed: Robust Sentence Embeddings Using Self-Supervised Contrastive Pre-Training](https://doi.org/10.18653/v1/2023.findings-emnlp.305) |  | 0 |  | Javad Rafiei Asl, Eduardo Blanco, Daniel Takabi |  |
| 439 |  |  [More than Votes? Voting and Language based Partisanship in the US Supreme Court](https://doi.org/10.18653/v1/2023.findings-emnlp.306) |  | 0 |  | Biaoyan Fang, Trevor Cohn, Timothy Baldwin, Lea Frermann |  |
| 440 |  |  [Automatic Evaluation of Attribution by Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.307) |  | 0 |  | Xiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, Huan Sun |  |
| 441 |  |  [Modeling Highlighting of Metaphors in Multitask Contrastive Learning Paradigms](https://doi.org/10.18653/v1/2023.findings-emnlp.308) |  | 0 |  | Meghdut Sengupta, Milad Alshomary, Ingrid Scharlau, Henning Wachsmuth |  |
| 442 |  |  [LDM²: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement](https://doi.org/10.18653/v1/2023.findings-emnlp.309) |  | 0 |  | Xingjin Wang, Linjing Li, Daniel Dajun Zeng |  |
| 443 |  |  [ZARA: Improving Few-Shot Self-Rationalization for Small Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.310) |  | 0 |  | WeiLin Chen, AnZi Yen, ChengKuang Wu, HenHsen Huang, HsinHsi Chen |  |
| 444 |  |  [ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation](https://doi.org/10.18653/v1/2023.findings-emnlp.311) |  | 0 |  | Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, Jingbo Shang |  |
| 445 |  |  [Mind the Gap: Automated Corpus Creation for Enthymeme Detection and Reconstruction in Learner Arguments](https://doi.org/10.18653/v1/2023.findings-emnlp.312) |  | 0 |  | Maja Stahl, Nick Düsterhus, MeiHua Chen, Henning Wachsmuth |  |
| 446 |  |  [Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.313) |  | 0 |  | Tianyu Yang, Thy Thy Tran, Iryna Gurevych |  |
| 447 |  |  [Retrieving Multimodal Information for Augmented Generation: A Survey](https://doi.org/10.18653/v1/2023.findings-emnlp.314) |  | 0 |  | Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Do Xuan Long, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, Shafiq Joty |  |
| 448 |  |  [Improving Contrastive Learning of Sentence Embeddings with Focal InfoNCE](https://doi.org/10.18653/v1/2023.findings-emnlp.315) |  | 0 |  | Pengyue Hou, Xingyu Li |  |
| 449 |  |  [The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.316) |  | 0 |  | Dung Nguyen Manh, Le Nam Hai, Anh T. V. Dau, Anh Minh Nguyen, Khanh Nghiem, Jin Guo, Nghi D. Q. Bui |  |
| 450 |  |  [SDOH-NLI: a Dataset for Inferring Social Determinants of Health from Clinical Notes](https://doi.org/10.18653/v1/2023.findings-emnlp.317) |  | 0 |  | Ádám D. Lelkes, Eric Loreaux, Tal Schuster, MingJun Chen, Alvin Rajkomar |  |
| 451 |  |  [On the Zero-Shot Generalization of Machine-Generated Text Detectors](https://doi.org/10.18653/v1/2023.findings-emnlp.318) |  | 0 |  | Xiao Pu, Jingyu Zhang, Xiaochuang Han, Yulia Tsvetkov, Tianxing He |  |
| 452 |  |  [Complex Event Schema Induction with Knowledge-Enriched Diffusion Model](https://doi.org/10.18653/v1/2023.findings-emnlp.319) |  | 0 |  | Yupu Hao, Pengfei Cao, Yubo Chen, Kang Liu, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Jun Zhao |  |
| 453 |  |  [Exploiting Emotion-Semantic Correlations for Empathetic Response Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.320) |  | 0 |  | Zhou Yang, Zhaochun Ren, Yufeng Wang, Xiaofei Zhu, Zhihao Chen, Tiecheng Cai, Yunbing Wu, Yisong Su, Sibo Ju, Xiangwen Liao |  |
| 454 |  |  [Long-Range Language Modeling with Selective Cache](https://doi.org/10.18653/v1/2023.findings-emnlp.321) |  | 0 |  | Xinting Huang, Nora Hollenstein |  |
| 455 |  |  [Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding](https://doi.org/10.18653/v1/2023.findings-emnlp.322) |  | 0 |  | Lorenzo Jaime Yu Flores, Heyuan Huang, Kejian Shi, Sophie Chheang, Arman Cohan |  |
| 456 |  |  [FaLA: Fast Linear Adaptation for Replacing Backbone Models on Edge Devices](https://doi.org/10.18653/v1/2023.findings-emnlp.323) |  | 0 |  | Shuo Huang, Lizhen Qu, Xingliang Yuan, Chunyang Chen |  |
| 457 |  |  [Intuitive Multilingual Audio-Visual Speech Recognition with a Single-Trained Model](https://doi.org/10.18653/v1/2023.findings-emnlp.324) |  | 0 |  | Joanna Hong, Se Jin Park, Yong Man Ro |  |
| 458 |  |  [Controllable Chest X-Ray Report Generation from Longitudinal Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.325) |  | 0 |  | Francesco Dalla Serra, Chaoyang Wang, Fani Deligianni, Jeff Dalton, Alison O'Neil |  |
| 459 |  |  [Is ChatGPT a Good Multi-Party Conversation Solver?](https://doi.org/10.18653/v1/2023.findings-emnlp.326) |  | 0 |  | ChaoHong Tan, JiaChen Gu, ZhenHua Ling |  |
| 460 |  |  [Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis](https://doi.org/10.18653/v1/2023.findings-emnlp.327) |  | 0 |  | Jianqiao Lu, Wenyong Huang, Nianzu Zheng, Xingshan Zeng, Yu Ting Yeung, Xiao Chen |  |
| 461 |  |  [Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders](https://doi.org/10.18653/v1/2023.findings-emnlp.328) |  | 0 |  | Qianren Mao, Shaobo Zhao, Jiarui Li, Xiaolei Gu, Shizhu He, Bo Li, Jianxin Li |  |
| 462 |  |  [Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.329) |  | 0 |  | Haeju Lee, Minchan Jeong, SeYoung Yun, KeeEung Kim |  |
| 463 |  |  [CCIM: Cross-modal Cross-lingual Interactive Image Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.330) |  | 0 |  | Cong Ma, Yaping Zhang, Mei Tu, Yang Zhao, Yu Zhou, Chengqing Zong |  |
| 464 |  |  [TRAMS: Training-free Memory Selection for Long-range Language Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.331) |  | 0 |  | Haofei Yu, Cunxiang Wang, Yue Zhang, Wei Bi |  |
| 465 |  |  [A Critical Analysis of Document Out-of-Distribution Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.332) |  | 0 |  | Jiuxiang Gu, Yifei Ming, Yi Zhou, Jason Kuen, Vlad I. Morariu, Handong Zhao, Ruiyi Zhang, Nikolaos Barmpalios, Anqi Liu, Yixuan Li, Tong Sun, Ani Nenkova |  |
| 466 |  |  [Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.333) |  | 0 |  | Ke Wang, Jun Xie, Yuqi Zhang, Yu Zhao |  |
| 467 |  |  [Active Learning Principles for In-Context Learning with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.334) |  | 0 |  | Katerina Margatina, Timo Schick, Nikolaos Aletras, Jane DwivediYu |  |
| 468 |  |  [InteMATs: Integrating Granularity-Specific Multilingual Adapters for Cross-Lingual Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.335) |  | 0 |  | Meizhen Liu, Xu Guo, Jiakai He, Jianye Chen, Fengyu Zhou, Siu Cheung Hui |  |
| 469 |  |  [PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.336) |  | 0 |  | Chengfeng Dou, Zhi Jin, Wenpin Jiao, Haiyan Zhao, Yongqiang Zhao, Zhengwei Tao |  |
| 470 |  |  [CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.337) |  | 0 |  | Weixiang Yan, Yuchen Tian, Yunzhe Li, Qian Chen, Wen Wang |  |
| 471 |  |  [impact of sample selection on in-context learning for entity extraction from scientific writing](https://doi.org/10.18653/v1/2023.findings-emnlp.338) |  | 0 |  | Necva Bölücü, Maciej Rybinski, Stephen Wan |  |
| 472 |  |  [Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models](https://doi.org/10.18653/v1/2023.findings-emnlp.339) |  | 0 |  | Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker |  |
| 473 |  |  [Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks](https://doi.org/10.18653/v1/2023.findings-emnlp.340) |  | 0 |  | Yichen Huang, Timothy Baldwin |  |
| 474 |  |  [Time-Considerable Dialogue Models via Reranking by Time Dependency](https://doi.org/10.18653/v1/2023.findings-emnlp.341) |  | 0 |  | Yuiko Tsunomori, Masakazu Ishihata, Hiroaki Sugiyama |  |
| 475 |  |  [Non-Compositionality in Sentiment: New Data and Analyses](https://doi.org/10.18653/v1/2023.findings-emnlp.342) |  | 0 |  | Verna Dankers, Christopher Lucas |  |
| 476 |  |  [MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.343) |  | 0 |  | Guoxin Chen, Yiming Qian, Bowen Wang, Liangzhi Li |  |
| 477 |  |  [DocTrack: A Visually-Rich Document Dataset Really Aligned with Human Eye Movement for Machine Reading](https://doi.org/10.18653/v1/2023.findings-emnlp.344) |  | 0 |  | Hao Wang, Qingxuan Wang, Yue Li, Changqing Wang, Chenhui Chu, Rui Wang |  |
| 478 |  |  [Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.345) |  | 0 |  | Jiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan Ö. Arik, Tomas Pfister, Somesh Jha |  |
| 479 |  |  [Bi-Drop: Enhancing Fine-tuning Generalization via Synchronous sub-net Estimation and Optimization](https://doi.org/10.18653/v1/2023.findings-emnlp.346) |  | 0 |  | Shoujie Tong, Heming Xia, Damai Dai, Runxin Xu, Tianyu Liu, Binghuai Lin, Yunbo Cao, Zhifang Sui |  |
| 480 |  |  [ClozEx: A Task toward Generation of English Cloze Explanation](https://doi.org/10.18653/v1/2023.findings-emnlp.347) |  | 0 |  | Zizheng Zhang, Masato Mita, Mamoru Komachi |  |
| 481 |  |  [Is Probing All You Need? Indicator Tasks as an Alternative to Probing Embedding Spaces](https://doi.org/10.18653/v1/2023.findings-emnlp.348) |  | 0 |  | Tal Levy, Omer Goldman, Reut Tsarfaty |  |
| 482 |  |  [The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.349) |  | 0 |  | Satya Sai Srinath Namburi, Makesh Sreedhar, Srinath Srinivasan, Frederic Sala |  |
| 483 |  |  [CoEdIT: Text Editing by Task-Specific Instruction Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.350) |  | 0 |  | Vipul Raheja, Dhruv Kumar, Ryan Koo, Dongyeop Kang |  |
| 484 |  |  [Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.351) |  | 0 |  | Yi Dai, Hao Lang, Kaisheng Zeng, Fei Huang, Yongbin Li |  |
| 485 |  |  [Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information](https://doi.org/10.18653/v1/2023.findings-emnlp.352) |  | 0 |  | Alla Chepurova, Aydar Bulatov, Yuri Kuratov, Mikhail Burtsev |  |
| 486 |  |  [DeltaScore: Fine-Grained Story Evaluation with Perturbations](https://doi.org/10.18653/v1/2023.findings-emnlp.353) |  | 0 |  | Zhuohan Xie, Miao Li, Trevor Cohn, Jey Han Lau |  |
| 487 |  |  [MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields](https://doi.org/10.18653/v1/2023.findings-emnlp.354) |  | 0 |  | Jiaying Lu, Yongchen Qian, Shifan Zhao, Yuanzhe Xi, Carl Yang |  |
| 488 |  |  [Don't waste a single annotation: improving single-label classifiers through soft labels](https://doi.org/10.18653/v1/2023.findings-emnlp.355) |  | 0 |  | Ben Wu, Yue Li, Yida Mu, Carolina Scarton, Kalina Bontcheva, Xingyi Song |  |
| 489 |  |  [Black-Box Tuning of Vision-Language Models with Effective Gradient Approximation](https://doi.org/10.18653/v1/2023.findings-emnlp.356) |  | 0 |  | Zixian Guo, Yuxiang Wei, Ming Liu, Zhilong Ji, Jinfeng Bai, Yiwen Guo, Wangmeng Zuo |  |
| 490 |  |  [How to Determine the Most Powerful Pre-trained Language Model without Brute Force Fine-tuning? An Empirical Survey](https://doi.org/10.18653/v1/2023.findings-emnlp.357) |  | 0 |  | Jun Bai, Xiaofeng Zhang, Chen Li, Hanhua Hong, Xi Xu, Chenghua Lin, Wenge Rong |  |
| 491 |  |  [Licon: A Diverse, Controllable and Challenging Linguistic Concept Learning Benchmark](https://doi.org/10.18653/v1/2023.findings-emnlp.358) |  | 0 |  | Shenglong Yu, Ying Zhang, Wenya Guo, Zhengkun Zhang, Ru Zhou, Xiaojie Yuan |  |
| 492 |  |  [InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations](https://doi.org/10.18653/v1/2023.findings-emnlp.359) |  | 0 |  | Nils Feldhus, Qianli Wang, Tatiana Anikina, Sahil Chopra, Cennet Oguz, Sebastian Möller |  |
| 493 |  |  [INVITE: a Testbed of Automatically Generated Invalid Questions to Evaluate Large Language Models for Hallucinations](https://doi.org/10.18653/v1/2023.findings-emnlp.360) |  | 0 |  | Anil Ramakrishna, Rahul Gupta, Jens Lehmann, Morteza Ziyadi |  |
| 494 |  |  [Multimodal Automated Fact-Checking: A Survey](https://doi.org/10.18653/v1/2023.findings-emnlp.361) |  | 0 |  | Mubashara Akhtar, Michael Sejr Schlichtkrull, Zhijiang Guo, Oana Cocarascu, Elena Simperl, Andreas Vlachos |  |
| 495 |  |  [PROTEGE: Prompt-based Diverse Question Generation from Web Articles](https://doi.org/10.18653/v1/2023.findings-emnlp.362) |  | 0 |  | Vinayak Puranik, Anirban Majumder, Vineet Chaoji |  |
| 496 |  |  [GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions](https://doi.org/10.18653/v1/2023.findings-emnlp.363) |  | 0 |  | TingYao Hsu, ChiehYang Huang, Ryan A. Rossi, Sungchul Kim, C. Lee Giles, TingHao Kenneth Huang |  |
| 497 |  |  [Mulan: A Multi-Level Alignment Model for Video Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.364) |  | 0 |  | Yu Fu, Cong Cao, Yuling Yang, Yuhai Lu, Fangfang Yuan, Dakui Wang, Yanbing Liu |  |
| 498 |  |  [HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.365) |  | 0 |  | Yongjin Yang, Joonkee Kim, Yujin Kim, Namgyu Ho, James Thorne, SeYoung Yun |  |
| 499 |  |  [ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.366) |  | 0 |  | Yaorui Shi, An Zhang, Enzhi Zhang, Zhiyuan Liu, Xiang Wang |  |
| 500 |  |  [Decomposing Complex Queries for Tip-of-the-tongue Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.367) |  | 0 |  | Kevin Lin, Kyle Lo, Joseph Gonzalez, Dan Klein |  |
| 501 |  |  [Values, Ethics, Morals? On the Use of Moral Concepts in NLP Research](https://doi.org/10.18653/v1/2023.findings-emnlp.368) |  | 0 |  | Karina Vida, Judith Simon, Anne Lauscher |  |
| 502 |  |  [Self-Supervised Behavior Cloned Transformers are Path Crawlers for Text Games](https://doi.org/10.18653/v1/2023.findings-emnlp.369) |  | 0 |  | Ruoyao Wang, Peter A. Jansen |  |
| 503 |  |  [Adapting Pretrained Text-to-Text Models for Long Text Sequences](https://doi.org/10.18653/v1/2023.findings-emnlp.370) |  | 0 |  | Wenhan Xiong, Anchit Gupta, Shubham Toshniwal, Yashar Mehdad, Scott Yih |  |
| 504 |  |  [xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark](https://doi.org/10.18653/v1/2023.findings-emnlp.371) |  | 0 |  | Chen Zhang, Luis F. D'Haro, Chengguang Tang, Ke Shi, Guohua Tang, Haizhou Li |  |
| 505 |  |  [MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems](https://doi.org/10.18653/v1/2023.findings-emnlp.372) |  | 0 |  | Jakub Macina, Nico Daheim, Sankalan Pal Chowdhury, Tanmay Sinha, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan |  |
| 506 |  |  [Towards Making the Most of ChatGPT for Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.373) |  | 0 |  | Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang, Dacheng Tao |  |
| 507 |  |  [Enhancing Reasoning Capabilities by Instruction Learning and Chain-of-Thoughts for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.374) |  | 0 |  | Yuxiang Lu, Yu Hong, Zhipang Wang, Guodong Zhou |  |
| 508 |  |  [Large-Scale and Multi-Perspective Opinion Summarization with Diverse Review Subsets](https://doi.org/10.18653/v1/2023.findings-emnlp.375) |  | 0 |  | Han Jiang, Rui Wang, Zhihua Wei, Yu Li, Xinpeng Wang |  |
| 509 |  |  [Topic-Informed Dialogue Summarization using Topic Distribution and Prompt-based Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.376) |  | 0 |  | Jaeah You, Youngjoong Ko |  |
| 510 |  |  [Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy](https://doi.org/10.18653/v1/2023.findings-emnlp.377) |  | 0 |  | Jiwoo Hong, Yejin Cho, Jiyoung Han, Jaemin Jung, James Thorne |  |
| 511 |  |  [Measuring and Narrowing the Compositionality Gap in Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.378) |  | 0 |  | Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, Mike Lewis |  |
| 512 |  |  [Unsupervised Candidate Answer Extraction through Differentiable Masker-Reconstructor Model](https://doi.org/10.18653/v1/2023.findings-emnlp.379) |  | 0 |  | Zhuoer Wang, Yicheng Wang, Ziwei Zhu, James Caverlee |  |
| 513 |  |  [HoneyBee: Progressive Instruction Finetuning of Large Language Models for Materials Science](https://doi.org/10.18653/v1/2023.findings-emnlp.380) |  | 0 |  | Yu Song, Santiago Miret, Huan Zhang, Bang Liu |  |
| 514 |  |  [Prompt-Based Editing for Text Style Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.381) |  | 0 |  | Guoqing Luo, Yutong Han, Lili Mou, Mauajama Firdaus |  |
| 515 |  |  [Representativeness as a Forgotten Lesson for Multilingual and Code-switched Data Collection and Preparation](https://doi.org/10.18653/v1/2023.findings-emnlp.382) |  | 0 |  | A. Seza Dogruöz, Sunayana Sitaram, Zheng Xin Yong |  |
| 516 |  |  [NERvous About My Health: Constructing a Bengali Medical Named Entity Recognition Dataset](https://doi.org/10.18653/v1/2023.findings-emnlp.383) |  | 0 |  | Alvi Khan, Fida Kamal, Nuzhat Nower, Tasnim Ahmed, Sabbir Ahmed, Tareque Chowdhury |  |
| 517 |  |  [Sparse Black-Box Multimodal Attack for Vision-Language Adversary Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.384) |  | 0 |  | Zhen Yu, Zhou Qin, Zhenhua Chen, Meihui Lian, Haojun Fu, Weigao Wen, Hui Xue, Kun He |  |
| 518 |  |  [Towards a Unified Framework for Reference Retrieval and Related Work Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.385) |  | 0 |  | Zhengliang Shi, Shen Gao, Zhen Zhang, Xiuying Chen, Zhumin Chen, Pengjie Ren, Zhaochun Ren |  |
| 519 |  |  [Visual Storytelling with Question-Answer Plans](https://doi.org/10.18653/v1/2023.findings-emnlp.386) |  | 0 |  | Danyang Liu, Mirella Lapata, Frank Keller |  |
| 520 |  |  [Investigating Online Community Engagement through Stancetaking](https://doi.org/10.18653/v1/2023.findings-emnlp.387) |  | 0 |  | Jai Aggarwal, Brian Diep, Julia Watson, Suzanne Stevenson |  |
| 521 |  |  [ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.388) |  | 0 |  | Alex Mei, Sharon Levy, William Yang Wang |  |
| 522 |  |  [Learning to Correct Noisy Labels for Fine-Grained Entity Typing via Co-Prediction Prompt Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.389) |  | 0 |  | Minghao Tang, Yongquan He, Yongxiu Xu, Hongbo Xu, Wenyuan Zhang, Yang Lin |  |
| 523 |  |  [Co²PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.390) |  | 0 |  | Xiangjue Dong, Ziwei Zhu, Zhuoer Wang, Maria Teleki, James Caverlee |  |
| 524 |  |  [A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.391) |  | 0 |  | Chenhui Shen, Liying Cheng, XuanPhi Nguyen, Yang You, Lidong Bing |  |
| 525 |  |  [Universal Domain Adaptation for Robust Handling of Distributional Shifts in NLP](https://doi.org/10.18653/v1/2023.findings-emnlp.392) |  | 0 |  | Hyuhng Joon Kim, Hyunsoo Cho, SangWoo Lee, Junyeob Kim, Choonghyun Park, Sanggoo Lee, Kang Min Yoo, Taeuk Kim |  |
| 526 |  |  [Aligning Language Models to User Opinions](https://doi.org/10.18653/v1/2023.findings-emnlp.393) |  | 0 |  | EunJeong Hwang, Bodhisattwa Prasad Majumder, Niket Tandon |  |
| 527 |  |  [CCSRD: Content-Centric Speech Representation Disentanglement Learning for End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.394) |  | 0 |  | Xiaohu Zhao, Haoran Sun, Yikun Lei, Shaolin Zhu, Deyi Xiong |  |
| 528 |  |  [Miracle: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control](https://doi.org/10.18653/v1/2023.findings-emnlp.395) |  | 0 |  | Zhenyi Lu, Wei Wei, Xiaoye Qu, XianLing Mao, Dangyang Chen, Jixiong Chen |  |
| 529 |  |  [Towards Multilingual Interlinear Morphological Glossing](https://doi.org/10.18653/v1/2023.findings-emnlp.396) |  | 0 |  | Shu Okabe, François Yvon |  |
| 530 |  |  [Transformer Working Memory Enables Regular Language Reasoning And Natural Language Length Extrapolation](https://doi.org/10.18653/v1/2023.findings-emnlp.397) |  | 0 |  | TaChung Chi, TingHan Fan, Alexander Rudnicky, Peter J. Ramadge |  |
| 531 |  |  [Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting](https://doi.org/10.18653/v1/2023.findings-emnlp.398) |  | 0 |  | Fanghua Ye, Meng Fang, Shenghui Li, Emine Yilmaz |  |
| 532 |  |  [Distilling ChatGPT for Explainable Automated Student Answer Assessment](https://doi.org/10.18653/v1/2023.findings-emnlp.399) |  | 0 |  | Jiazheng Li, Lin Gui, Yuxiang Zhou, David West, Cesare Aloisi, Yulan He |  |
| 533 |  |  [Grammatical Error Correction via Mixed-Grained Weighted Training](https://doi.org/10.18653/v1/2023.findings-emnlp.400) |  | 0 |  | Jiahao Li, Quan Wang, Chiwei Zhu, Zhendong Mao, Yongdong Zhang |  |
| 534 |  |  [A Unified Framework for Synaesthesia Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.401) |  | 0 |  | Kun Sheng, Zhongqing Wang, Qingqing Zhao, Xiaotong Jiang, Guodong Zhou |  |
| 535 |  |  [Domain Private Transformers for Multi-Domain Dialog Systems](https://doi.org/10.18653/v1/2023.findings-emnlp.402) |  | 0 |  | Anmol Kabra, Ethan R. Elenberg |  |
| 536 |  |  [Visual Elements Mining as Prompts for Instruction Learning for Target-Oriented Multimodal Sentiment Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.403) |  | 0 |  | Bin Yang, Jinlong Li |  |
| 537 |  |  [NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.404) |  | 0 |  | Jongwoo Ko, Seungjoon Park, Yujin Kim, Sumyeong Ahn, DuSeong Chang, Euijai Ahn, SeYoung Yun |  |
| 538 |  |  [GBT: Generative Boosting Training Approach for Paraphrase Identification](https://doi.org/10.18653/v1/2023.findings-emnlp.405) |  | 0 |  | Rui Peng, Zhiling Jin, Yu Hong |  |
| 539 |  |  [DeCrisisMB: Debiased Semi-Supervised Learning for Crisis Tweet Classification via Memory Bank](https://doi.org/10.18653/v1/2023.findings-emnlp.406) |  | 0 |  | Henry Peng Zou, Yue Zhou, Weizhi Zhang, Cornelia Caragea |  |
| 540 |  |  [Probing LLMs for hate speech detection: strengths and vulnerabilities](https://doi.org/10.18653/v1/2023.findings-emnlp.407) |  | 0 |  | Sarthak Roy, Ashish Harshavardhan, Animesh Mukherjee, Punyajoy Saha |  |
| 541 |  |  [From Simple to Complex: A Progressive Framework for Document-level Informative Argument Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.408) |  | 0 |  | Quzhe Huang, Yanxi Zhang, Dongyan Zhao |  |
| 542 |  |  [MultiCMET: A Novel Chinese Benchmark for Understanding Multimodal Metaphor](https://doi.org/10.18653/v1/2023.findings-emnlp.409) |  | 0 |  | Dongyu Zhang, Jingwei Yu, Senyuan Jin, Liang Yang, Hongfei Lin |  |
| 543 |  |  [GlotLID: Language Identification for Low-Resource Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.410) |  | 0 |  | Amir Hossein Kargaran, Ayyoob Imani, François Yvon, Hinrich Schütze |  |
| 544 |  |  [Finding Support Examples for In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.411) |  | 0 |  | Xiaonan Li, Xipeng Qiu |  |
| 545 |  |  [Uncovering the Root of Hate Speech: A Dataset for Identifying Hate Instigating Speech](https://doi.org/10.18653/v1/2023.findings-emnlp.412) |  | 0 |  | Hyoungjun Park, Ho Shim, Kyuhan Lee |  |
| 546 |  |  [Responsible AI Considerations in Text Summarization Research: A Review of Current Practices](https://doi.org/10.18653/v1/2023.findings-emnlp.413) |  | 0 |  | Yu Lu Liu, Meng Cao, Su Lin Blodgett, Jackie Chi Kit Cheung, Alexandra Olteanu, Adam Trischler |  |
| 547 |  |  [Improving Speech Translation by Fusing Speech and Text](https://doi.org/10.18653/v1/2023.findings-emnlp.414) |  | 0 |  | Wenbiao Yin, Zhicheng Liu, Chengqi Zhao, Tao Wang, Jian Tong, Rong Ye |  |
| 548 |  |  [Narrative Order Aware Story Generation via Bidirectional Pretraining Model with Optimal Transport Reward](https://doi.org/10.18653/v1/2023.findings-emnlp.415) |  | 0 |  | Zhicong Lu, Li Jin, Guangluan Xu, Linmei Hu, Nayu Liu, Xiaoyu Li, Xian Sun, Zequn Zhang, Kaiwen Wei |  |
| 549 |  |  [Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.416) |  | 0 |  | Haoran Wang, Kai Shu |  |
| 550 |  |  [Strong and Efficient Baselines for Open Domain Conversational Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.417) |  | 0 |  | Andrei C. Coman, Gianni Barlacchi, Adrià de Gispert |  |
| 551 |  |  [Efficient Continue Training of Temporal Language Model with Structural Information](https://doi.org/10.18653/v1/2023.findings-emnlp.418) |  | 0 |  | Zhaochen Su, Juntao Li, Zikang Zhang, Zihan Zhou, Min Zhang |  |
| 552 |  |  [Retrieval-Augmented Parsing for Complex Graphs by Exploiting Structure and Uncertainty](https://doi.org/10.18653/v1/2023.findings-emnlp.419) |  | 0 |  | Zi Lin, Quan Yuan, Panupong Pasupat, Jeremiah Z. Liu, Jingbo Shang |  |
| 553 |  |  [When it Rains, it Pours: Modeling Media Storms and the News Ecosystem](https://doi.org/10.18653/v1/2023.findings-emnlp.420) |  | 0 |  | Benjamin Litterer, David Jurgens, Dallas Card |  |
| 554 |  |  [Intra-Event and Inter-Event Dependency-Aware Graph Network for Event Argument Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.421) |  | 0 |  | Hao Li, Yanan Cao, Yubing Ren, Fang Fang, Lanxue Zhang, Yingjie Li, Shi Wang |  |
| 555 |  |  [From Relevance to Utility: Evidence Retrieval with Feedback for Fact Verification](https://doi.org/10.18653/v1/2023.findings-emnlp.422) |  | 0 |  | Hengran Zhang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng |  |
| 556 |  |  [How to Train Your Dragon: Diverse Augmentation Towards Generalizable Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.423) |  | 0 |  | ShengChieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wentau Yih, Xilun Chen |  |
| 557 |  |  [Discovering Highly Influential Shortcut Reasoning: An Automated Template-Free Approach](https://doi.org/10.18653/v1/2023.findings-emnlp.424) |  | 0 |  | Daichi Haraguchi, Kiyoaki Shirai, Naoya Inoue, Natthawut Kertkeidkachorn |  |
| 558 |  |  [Schema-adaptable Knowledge Graph Construction](https://doi.org/10.18653/v1/2023.findings-emnlp.425) |  | 0 |  | Hongbin Ye, Honghao Gui, Xin Xu, Xi Chen, Huajun Chen, Ningyu Zhang |  |
| 559 |  |  [Evaluating the Knowledge Base Completion Potential of GPT](https://doi.org/10.18653/v1/2023.findings-emnlp.426) |  | 0 |  | Blerta Veseli, Simon Razniewski, JanChristoph Kalo, Gerhard Weikum |  |
| 560 |  |  [Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset](https://doi.org/10.18653/v1/2023.findings-emnlp.427) |  | 0 |  | Haoyi Wu, Wenyang Hui, Yezeng Chen, Weiqi Wu, Kewei Tu, Yi Zhou |  |
| 561 |  |  [DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text](https://doi.org/10.18653/v1/2023.findings-emnlp.428) |  | 0 |  | Shuaiyi Li, Yang Deng, Wai Lam |  |
| 562 |  |  [TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised Intent Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.429) |  | 0 |  | Nicholas Botzer, David Vasquez, Tim Weninger, Issam H. Laradji |  |
| 563 |  |  [Late Fusion of Transformers for Sentiment Analysis of Code-Switched Data](https://doi.org/10.18653/v1/2023.findings-emnlp.430) |  | 0 |  | Gagan Sharma, R. Chinmay, Raksha Sharma |  |
| 564 |  |  [Inductive Relation Inference of Knowledge Graph Enhanced by Ontology Information](https://doi.org/10.18653/v1/2023.findings-emnlp.431) |  | 0 |  | Wentao Zhou, Jun Zhao, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 565 |  |  [Dynamic Stance: Modeling Discussions by Labeling the Interactions](https://doi.org/10.18653/v1/2023.findings-emnlp.432) |  | 0 |  | Blanca Calvo Figueras, Irene Baucells de la Peña, Tommaso Caselli |  |
| 566 |  |  [Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements](https://doi.org/10.18653/v1/2023.findings-emnlp.433) |  | 0 |  | Yushan Qian, Weinan Zhang, Ting Liu |  |
| 567 |  |  [GPT Deciphering Fedspeak: Quantifying Dissent Among Hawks and Doves](https://doi.org/10.18653/v1/2023.findings-emnlp.434) |  | 0 |  | Denis Peskoff, Adam Visokay, Sander Schulhoff, Benjamin Wachspress, Alan Blinder, Brandon M. Stewart |  |
| 568 |  |  [DialogQAE: N-to-N Question Answer Pair Extraction from Customer Service Chatlog](https://doi.org/10.18653/v1/2023.findings-emnlp.435) |  | 0 |  | Xin Zheng, Tianyu Liu, Haoran Meng, Xu Wang, Yufan Jiang, MengLiang Rao, Binghuai Lin, Yunbo Cao, Zhifang Sui |  |
| 569 |  |  [Inverse Reinforcement Learning for Text Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.436) |  | 0 |  | Yu Fu, Deyi Xiong, Yue Dong |  |
| 570 |  |  [MM-Reasoner: A Multi-Modal Knowledge-Aware Framework for Knowledge-Based Visual Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.437) |  | 0 |  | Mahmoud Khademi, Ziyi Yang, Felipe Frujeri, Chenguang Zhu |  |
| 571 |  |  [Toward Joint Language Modeling for Speech Units and Text](https://doi.org/10.18653/v1/2023.findings-emnlp.438) |  | 0 |  | JuChieh Chou, ChungMing Chien, WeiNing Hsu, Karen Livescu, Arun Babu, Alexis Conneau, Alexei Baevski, Michael Auli |  |
| 572 |  |  [From Chaos to Clarity: Claim Normalization to Empower Fact-Checking](https://doi.org/10.18653/v1/2023.findings-emnlp.439) |  | 0 |  | Megha Sundriyal, Tanmoy Chakraborty, Preslav Nakov |  |
| 573 |  |  [Mitigating Biases in Hate Speech Detection from A Causal Perspective](https://doi.org/10.18653/v1/2023.findings-emnlp.440) |  | 0 |  | Zhehao Zhang, Jiaao Chen, Diyi Yang |  |
| 574 |  |  [Unmasking the Hidden Meaning: Bridging Implicit and Explicit Hate Speech Embedding Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.441) |  | 0 |  | Nicolás Benjamín Ocampo, Elena Cabrio, Serena Villata |  |
| 575 |  |  [PerturbScore: Connecting Discrete and Continuous Perturbations in NLP](https://doi.org/10.18653/v1/2023.findings-emnlp.442) |  | 0 |  | Linyang Li, Ke Ren, Yunfan Shao, Pengyu Wang, Xipeng Qiu |  |
| 576 |  |  [InstructoR: Instructing Unsupervised Conversational Dense Retrieval with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.443) |  | 0 |  | Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao |  |
| 577 |  |  [The Iron(ic) Melting Pot: Reviewing Human Evaluation in Humour, Irony and Sarcasm Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.444) |  | 0 |  | Tyler Loakman, Aaron Maladry, Chenghua Lin |  |
| 578 |  |  [INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.445) |  | 0 |  | H. S. V. N. S. Kowndinya Renduchintala, Krishnateja Killamsetty, Sumit Bhatia, Milan Aggarwal, Ganesh Ramakrishnan, Rishabh K. Iyer, Balaji Krishnamurthy |  |
| 579 |  |  [Towards General Error Diagnosis via Behavioral Testing in Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.446) |  | 0 |  | Junjie Wu, Lemao Liu, DitYan Yeung |  |
| 580 |  |  [Retrieval-Augmented Few-shot Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.447) |  | 0 |  | Guoxin Yu, Lemao Liu, Haiyun Jiang, Shuming Shi, Xiang Ao |  |
| 581 |  |  [Temporal Extrapolation and Knowledge Transfer for Lifelong Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.448) |  | 0 |  | Zhongwu Chen, Chengjin Xu, Fenglong Su, Zhen Huang, Yong Dou |  |
| 582 |  |  [Comparing Prompt-Based and Standard Fine-Tuning for Urdu Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.449) |  | 0 |  | Faizad Ullah, Ubaid Azam, Ali Faheem, Faisal Kamiran, Asim Karim |  |
| 583 |  |  [Explore the Way: Exploring Reasoning Path by Bridging Entities for Effective Cross-Document Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.450) |  | 0 |  | Junyoung Son, Jinsung Kim, Jungwoo Lim, Yoonna Jang, Heuiseok Lim |  |
| 584 |  |  [The student becomes the master: Outperforming GPT3 on Scientific Factual Error Correction](https://doi.org/10.18653/v1/2023.findings-emnlp.451) |  | 0 |  | Dhananjay Ashok, Atharva Kulkarni, Hai Pham, Barnabás Póczos |  |
| 585 |  |  [Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.452) |  | 0 |  | Ruosen Li, Xinya Du |  |
| 586 |  |  [Hierarchical Catalogue Generation for Literature Review: A Benchmark](https://doi.org/10.18653/v1/2023.findings-emnlp.453) |  | 0 |  | Kun Zhu, Xiaocheng Feng, Xiachong Feng, Yingsheng Wu, Bing Qin |  |
| 587 |  |  [MCC-KD: Multi-CoT Consistent Knowledge Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.454) |  | 0 |  | Hongzhan Chen, Siyue Wu, Xiaojun Quan, Rui Wang, Ming Yan, Ji Zhang |  |
| 588 |  |  [An Empirical Study of Frame Selection for Text-to-Video Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.455) |  | 0 |  | Mengxia Wu, Min Cao, Yang Bai, Ziyin Zeng, Chen Chen, Liqiang Nie, Min Zhang |  |
| 589 |  |  [Conditional Natural Language Inference](https://doi.org/10.18653/v1/2023.findings-emnlp.456) |  | 0 |  | Youngwoo Kim, Razieh Rahimi, James Allan |  |
| 590 |  |  [Contrastive Distant Supervision for Debiased and Denoised Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.457) |  | 0 |  | Ning Bian, Hongyu Lin, Xianpei Han, Ben He, Le Sun |  |
| 591 |  |  [KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness](https://doi.org/10.18653/v1/2023.findings-emnlp.458) |  | 0 |  | Yichuan Li, Jialong Han, Kyumin Lee, Chengyuan Ma, Benjamin Z. Yao, Xiaohu Liu |  |
| 592 |  |  [Revisiting Large Language Models as Zero-shot Relation Extractors](https://doi.org/10.18653/v1/2023.findings-emnlp.459) |  | 0 |  | Guozheng Li, Peng Wang, Wenjun Ke |  |
| 593 |  |  [Multi-Stage Pre-training Enhanced by ChatGPT for Multi-Scenario Multi-Domain Dialogue Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.460) |  | 0 |  | Weixiao Zhou, Gengyao Li, Xianfu Cheng, Xinnian Liang, Junnan Zhu, Feifei Zhai, Zhoujun Li |  |
| 594 |  |  [Towards large language model-based personal agents in the enterprise: Current trends and open problems](https://doi.org/10.18653/v1/2023.findings-emnlp.461) |  | 0 |  | Vinod Muthusamy, Yara Rizk, Kiran Kate, Praveen Venkateswaran, Vatche Isahagian, Ashu Gulati, Parijat Dube |  |
| 595 |  |  [CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.462) |  | 0 |  | Cheng Qian, Chi Han, Yi Ren Fung, Yujia Qin, Zhiyuan Liu, Heng Ji |  |
| 596 |  |  [Query-based Image Captioning from Multi-context 360cdegree Images](https://doi.org/10.18653/v1/2023.findings-emnlp.463) |  | 0 |  | Koki Maeda, Shuhei Kurita, Taiki Miyanishi, Naoaki Okazaki |  |
| 597 |  |  [Auto Search Indexer for End-to-End Document Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.464) |  | 0 |  | Tianchi Yang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang |  |
| 598 |  |  ['Person' == Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion](https://doi.org/10.18653/v1/2023.findings-emnlp.465) |  | 0 |  | Sourojit Ghosh, Aylin Caliskan |  |
| 599 |  |  [Task-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.466) |  | 0 |  | Yuliang Cai, Jesse Thomason, Mohammad Rostami |  |
| 600 |  |  [Evaluating Verifiability in Generative Search Engines](https://doi.org/10.18653/v1/2023.findings-emnlp.467) |  | 0 |  | Nelson F. Liu, Tianyi Zhang, Percy Liang |  |
| 601 |  |  [Enhancing Abstractiveness of Summarization Models through Calibrated Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.468) |  | 0 |  | Hwanjun Song, Igor Shalyminov, Hang Su, Siffi Singh, Kaisheng Yao, Saab Mansour |  |
| 602 |  |  [Visually Grounded Continual Language Learning with Selective Specialization](https://doi.org/10.18653/v1/2023.findings-emnlp.469) |  | 0 |  | Kyra Ahrens, Lennart Bengtson, Jae Hee Lee, Stefan Wermter |  |
| 603 |  |  [RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.470) |  | 0 |  | Victor Zhong, Weijia Shi, Wentau Yih, Luke Zettlemoyer |  |
| 604 |  |  [Leveraging Multiple Teachers for Test-Time Adaptation of Language-Guided Classifiers](https://doi.org/10.18653/v1/2023.findings-emnlp.471) |  | 0 |  | Kangda Wei, Sayan Ghosh, Rakesh R. Menon, Shashank Srivastava |  |
| 605 |  |  [Summarizing Multiple Documents with Conversational Structure for Meta-Review Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.472) |  | 0 |  | Miao Li, Eduard H. Hovy, Jey Han Lau |  |
| 606 |  |  [VIPHY: Probing "Visible" Physical Commonsense Knowledge](https://doi.org/10.18653/v1/2023.findings-emnlp.473) |  | 0 |  | Shikhar Singh, Ehsan Qasemi, Muhao Chen |  |
| 607 |  |  [Two Directions for Clinical Data Generation with Large Language Models: Data-to-Label and Label-to-Data](https://doi.org/10.18653/v1/2023.findings-emnlp.474) |  | 0 |  | Rumeng Li, Xun Wang, Hong Yu |  |
| 608 |  |  [Stylized Dialogue Generation with Feature-Guided Knowledge Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.475) |  | 0 |  | Jinpeng Li, Zekai Zhang, Xiuying Chen, Dongyan Zhao, Rui Yan |  |
| 609 |  |  [Probing LLMs for Joint Encoding of Linguistic Categories](https://doi.org/10.18653/v1/2023.findings-emnlp.476) |  | 0 |  | Giulio Starace, Konstantinos Papakostas, Rochelle Choenni, Apostolos Panagiotopoulos, Matteo Rosati, Alina Leidinger, Ekaterina Shutova |  |
| 610 |  |  [On Robustness of Finetuned Transformer-based NLP Models](https://doi.org/10.18653/v1/2023.findings-emnlp.477) |  | 0 |  | Pavan Kalyan Reddy Neerudu, Subba Reddy Oota, Mounika Marreddy, Venkateswara Rao Kagita, Manish Gupta |  |
| 611 |  |  [Measuring and Mitigating Constraint Violations of In-Context Learning for Utterance-to-API Semantic Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.478) |  | 0 |  | Shufan Wang, Sébastien Jean, Sailik Sengupta, James Gung, Nikolaos Pappas, Yi Zhang |  |
| 612 |  |  [Entity Disambiguation on a Tight Labeling Budget](https://doi.org/10.18653/v1/2023.findings-emnlp.479) |  | 0 |  | Audi Primadhanty, Ariadna Quattoni |  |
| 613 |  |  [Topic-DPR: Topic-based Prompts for Dense Passage Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.480) |  | 0 |  | Qingfa Xiao, Shuangyin Li, Lei Chen |  |
| 614 |  |  [Quantifying the Dialect Gap and its Correlates Across Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.481) |  | 0 |  | Anjali Kantharuban, Ivan Vulic, Anna Korhonen |  |
| 615 |  |  [RECAL: Sample-Relation Guided Confidence Calibration over Tabular Data](https://doi.org/10.18653/v1/2023.findings-emnlp.482) |  | 0 |  | Haotian Wang, Zhen Zhang, Mengting Hu, Qichao Wang, Liang Chen, Yatao Bian, Bingzhe Wu |  |
| 616 |  |  [Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment](https://doi.org/10.18653/v1/2023.findings-emnlp.483) |  | 0 |  | Zhen Zhang, Jialu Wang, Xin Eric Wang |  |
| 617 |  |  [Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries](https://doi.org/10.18653/v1/2023.findings-emnlp.484) |  | 0 |  | Prafulla Kumar Choubey, Alexander R. Fabbri, Caiming Xiong, ChienSheng Wu |  |
| 618 |  |  [Pseudointelligence: A Unifying Lens on Language Model Evaluation](https://doi.org/10.18653/v1/2023.findings-emnlp.485) |  | 0 |  | Shikhar Murty, Orr Paradise, Pratyusha Sharma |  |
| 619 |  |  [GDA: Grammar-based Data Augmentation for Text Classification using Slot Information](https://doi.org/10.18653/v1/2023.findings-emnlp.486) |  | 0 |  | Joonghyuk Hahn, Hyunjoon Cheon, Elizabeth Orwig, SuHyeon Kim, SangKi Ko, YoSub Han |  |
| 620 |  |  [Implicit Sense-labeled Connective Recognition as Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.487) |  | 0 |  | Yui Oka, Tsutomu Hirao |  |
| 621 |  |  [VISTA: Visual-Textual Knowledge Graph Representation Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.488) |  | 0 |  | Jaejun Lee, Chanyoung Chung, Hochang Lee, Sungho Jo, Joyce Jiyoung Whang |  |
| 622 |  |  [Dynamic Stashing Quantization for Efficient Transformer Training](https://doi.org/10.18653/v1/2023.findings-emnlp.489) |  | 0 |  | Guo Yang, Daniel Lo, Robert D. Mullins, Yiren Zhao |  |
| 623 |  |  [A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.490) |  | 0 |  | Ruihao Shui, Yixin Cao, Xiang Wang, TatSeng Chua |  |
| 624 |  |  [A Lightweight Method to Generate Unanswerable Questions in English](https://doi.org/10.18653/v1/2023.findings-emnlp.491) |  | 0 |  | Vagrant Gautam, Miaoran Zhang, Dietrich Klakow |  |
| 625 |  |  [Automatic Evaluate Dialogue Appropriateness by Using Dialogue Act](https://doi.org/10.18653/v1/2023.findings-emnlp.492) |  | 0 |  | Bao Chen, Yuanjie Wang, Zeming Liu, Yuhang Guo |  |
| 626 |  |  [TabPrompt: Graph-based Pre-training and Prompting for Few-shot Table Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.493) |  | 0 |  | Rihui Jin, Jianan Wang, Wei Tan, Yongrui Chen, Guilin Qi, Wang Hao |  |
| 627 |  |  [Towards Formality-Aware Neural Machine Translation by Leveraging Context Information](https://doi.org/10.18653/v1/2023.findings-emnlp.494) |  | 0 |  | Dohee Kim, Yujin Baek, Soyoung Yang, Jaegul Choo |  |
| 628 |  |  [Improving Seq2Seq Grammatical Error Correction via Decoding Interventions](https://doi.org/10.18653/v1/2023.findings-emnlp.495) |  | 0 |  | Houquan Zhou, Yumeng Liu, Zhenghua Li, Min Zhang, Bo Zhang, Chen Li, Ji Zhang, Fei Huang |  |
| 629 |  |  [Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses](https://doi.org/10.18653/v1/2023.findings-emnlp.496) |  | 0 |  | Aysa Xuemo Fan, Haoran Zhang, Luc Paquette, Rui Zhang |  |
| 630 |  |  [Learning Easily Updated General Purpose Text Representations with Adaptable Task-Specific Prefix](https://doi.org/10.18653/v1/2023.findings-emnlp.497) |  | 0 |  | KuanHao Huang, Liang Tan, Rui Hou, Sinong Wang, Amjad Almahairi, Ruty Rinott |  |
| 631 |  |  [Good Meta-tasks Make A Better Cross-lingual Meta-transfer Learning for Low-resource Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.498) |  | 0 |  | Linjuan Wu, Zongyi Guo, Baoliang Cui, Haihong Tang, Weiming Lu |  |
| 632 |  |  [Reasoning Makes Good Annotators : An Automatic Task-specific Rules Distilling Framework for Low-resource Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.499) |  | 0 |  | Yilin Lu, Juncheng Li, Xiaoqiang Wang, Haochen Shi, Tao Chen, Siliang Tang |  |
| 633 |  |  [Co-training and Co-distillation for Quality Improvement and Compression of Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.500) |  | 0 |  | Hayeon Lee, Rui Hou, Jongpil Kim, Davis Liang, Hongbo Zhang, Sung Ju Hwang, Alexander Min |  |
| 634 |  |  [ReadPrompt: A Readable Prompting Method for Reliable Knowledge Probing](https://doi.org/10.18653/v1/2023.findings-emnlp.501) |  | 0 |  | Zezhong Wang, Luyao Ye, Hongru Wang, WaiChung Kwan, David Ho, KamFai Wong |  |
| 635 |  |  [Coherent Entity Disambiguation via Modeling Topic and Categorical Dependency](https://doi.org/10.18653/v1/2023.findings-emnlp.502) |  | 0 |  | Zilin Xiao, Linjun Shou, Xingyao Zhang, Jie Wu, Ming Gong, Daxin Jiang |  |
| 636 |  |  [How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench](https://doi.org/10.18653/v1/2023.findings-emnlp.503) |  | 0 |  | Qinyuan Ye, Harvey Yiyun Fu, Xiang Ren, Robin Jia |  |
| 637 |  |  [POSQA: Probe the World Models of LLMs with Size Comparisons](https://doi.org/10.18653/v1/2023.findings-emnlp.504) |  | 0 |  | Chang Shu, Jiuzhou Han, Fangyu Liu, Ehsan Shareghi, Nigel Collier |  |
| 638 |  |  [Hierarchical Fusion for Online Multimodal Dialog Act Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.505) |  | 0 |  | Md Messal Monem Miah, Adarsh Pyarelal, Ruihong Huang |  |
| 639 |  |  [STEER: Unified Style Transfer with Expert Reinforcement](https://doi.org/10.18653/v1/2023.findings-emnlp.506) |  | 0 |  | Skyler Hallinan, Faeze Brahman, Ximing Lu, Jaehun Jung, Sean Welleck, Yejin Choi |  |
| 640 |  |  [Enhancing Argument Structure Extraction with Efficient Leverage of Contextual Information](https://doi.org/10.18653/v1/2023.findings-emnlp.507) |  | 0 |  | Yun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Jie Zhou, Yue Zhang |  |
| 641 |  |  [Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate](https://doi.org/10.18653/v1/2023.findings-emnlp.508) |  | 0 |  | Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, Bing Qin |  |
| 642 |  |  [Culturally Aware Natural Language Inference](https://doi.org/10.18653/v1/2023.findings-emnlp.509) |  | 0 |  | Jing Huang, Diyi Yang |  |
| 643 |  |  [End-to-End Autoregressive Retrieval via Bootstrapping for Smart Reply Systems](https://doi.org/10.18653/v1/2023.findings-emnlp.510) |  | 0 |  | Benjamin Towle, Ke Zhou |  |
| 644 |  |  [Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness](https://doi.org/10.18653/v1/2023.findings-emnlp.511) |  | 0 |  | Zichao Li, Ines Arous, Siva Reddy, Jackie Chi Kit Cheung |  |
| 645 |  |  [Effects of Human Adversarial and Affable Samples on BERT Generalizability](https://doi.org/10.18653/v1/2023.findings-emnlp.512) |  | 0 |  | Aparna Elangovan, Estrid He, Yuan Li, Karin Verspoor |  |
| 646 |  |  [Logic Unveils Truth, While Disguise Obscures It: Transition Logic Augmented Response Selection for Multi-Turn Dialogue](https://doi.org/10.18653/v1/2023.findings-emnlp.513) |  | 0 |  | Tingchen Fu, Xueliang Zhao, Lemao Liu, Rui Yan |  |
| 647 |  |  [Are Language Models Worse than Humans at Following Prompts? It's Complicated](https://doi.org/10.18653/v1/2023.findings-emnlp.514) |  | 0 |  | Albert Webson, Alyssa Marie Loo, Qinan Yu, Ellie Pavlick |  |
| 648 |  |  [A Sequence-to-Structure Approach to Document-level Targeted Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.515) |  | 0 |  | Nan Song, Hongjie Cai, Rui Xia, Jianfei Yu, Zhen Wu, Xinyu Dai |  |
| 649 |  |  [Generating Extractive Answers: Gated Recurrent Memory Reader for Conversational Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.516) |  | 0 |  | Xuanyu Zhang, Qing Yang |  |
| 650 |  |  [Text2Tree: Aligning Text Representation to the Label Tree Hierarchy for Imbalanced Medical Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.517) |  | 0 |  | Jiahuan Yan, Haojun Gao, Kai Zhang, Weize Liu, Danny Chen, Jian Wu, Jintai Chen |  |
| 651 |  |  [Impact of Co-occurrence on Factual Knowledge of Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.518) |  | 0 |  | Cheongwoong Kang, Jaesik Choi |  |
| 652 |  |  [CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.519) |  | 0 |  | Aswanth M., Ratish Puduppully, Raj Dabre, Anoop Kunchukuttan |  |
| 653 |  |  [Swap and Predict - Predicting the Semantic Changes in Words across Corpora by Context Swapping](https://doi.org/10.18653/v1/2023.findings-emnlp.520) |  | 0 |  | Taichi Aida, Danushka Bollegala |  |
| 654 |  |  [Beyond Layout Embedding: Layout Attention with Gaussian Biases for Structured Document Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.521) |  | 0 |  | Xi Zhu, Xue Han, Shuyuan Peng, Shuo Lei, Chao Deng, Junlan Feng |  |
| 655 |  |  [ESPVR: Entity Spans Position Visual Regions for Multimodal Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.522) |  | 0 |  | Xiujiao Li, Guanglu Sun, Xinyu Liu |  |
| 656 |  |  [Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency](https://doi.org/10.18653/v1/2023.findings-emnlp.523) |  | 0 |  | Lingfeng Shen, Weiting Tan, Boyuan Zheng, Daniel Khashabi |  |
| 657 |  |  [Detecting Erroneously Recognized Handwritten Byzantine Text](https://doi.org/10.18653/v1/2023.findings-emnlp.524) |  | 0 |  | John Pavlopoulos, Vasiliki Kougia, Paraskevi Platanou, Holger Essler |  |
| 658 |  |  [Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment](https://doi.org/10.18653/v1/2023.findings-emnlp.525) |  | 0 |  | Boyang Xue, Weichao Wang, Hongru Wang, Fei Mi, Rui Wang, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, KamFai Wong |  |
| 659 |  |  [TRIP: Accelerating Document-level Multilingual Pre-training via Triangular Document-level Pre-training on Parallel Data Triplets](https://doi.org/10.18653/v1/2023.findings-emnlp.526) |  | 0 |  | Hongyuan Lu, Haoyang Huang, Shuming Ma, Dongdong Zhang, Wai Lam, Zhaochuan Gao, Anthony Aue, Arul Menezes, Furu Wei |  |
| 660 |  |  [Frequency Balanced Datasets Lead to Better Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.527) |  | 0 |  | Rodolfo Zevallos, Mireia Farrús, Núria Bel |  |
| 661 |  |  [Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.528) |  | 0 |  | Jianing Wang, Qiushi Sun, Nuo Chen, Chengyu Wang, Jun Huang, Ming Gao, Xiang Li |  |
| 662 |  |  [TR-Rules: Rule-based Model for Link Forecasting on Temporal Knowledge Graph Considering Temporal Redundancy](https://doi.org/10.18653/v1/2023.findings-emnlp.529) |  | 0 |  | Ningyuan Li, Haihong E, Shi Li, Mingzhi Sun, Tianyu Yao, Meina Song, Yong Wang, Haoran Luo |  |
| 663 |  |  [On the Transferability of Visually Grounded PCFGs](https://doi.org/10.18653/v1/2023.findings-emnlp.530) |  | 0 |  | Yanpeng Zhao, Ivan Titov |  |
| 664 |  |  [Analysis of Style-Shifting on Social Media: Using Neural Language Model Conditioned by Social Meanings](https://doi.org/10.18653/v1/2023.findings-emnlp.531) |  | 0 |  | Seiya Kawano, Shota Kanezaki, Angel Fernando Garcia Contreras, Akishige Yuguchi, Marie Katsurai, Koichiro Yoshino |  |
| 665 |  |  [Linguistic Compression in Single-Sentence Human-Written Summaries](https://doi.org/10.18653/v1/2023.findings-emnlp.532) |  | 0 |  | Fangcong Yin, Marten van Schijndel |  |
| 666 |  |  [MCLF: A Multi-grained Contrastive Learning Framework for ASR-robust Spoken Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.533) |  | 0 |  | Zhiqi Huang, Dongsheng Chen, Zhihong Zhu, Xuxin Cheng |  |
| 667 |  |  [Beyond Candidates : Adaptive Dialogue Agent Utilizing Persona and Knowledge](https://doi.org/10.18653/v1/2023.findings-emnlp.534) |  | 0 |  | Jungwoo Lim, Myunghoon Kang, Jinsung Kim, Jeongwook Kim, Yuna Hur, Heuiseok Lim |  |
| 668 |  |  [SmartSpanNER: Making SpanNER Robust in Low Resource Scenarios](https://doi.org/10.18653/v1/2023.findings-emnlp.535) |  | 0 |  | Min Zhang, Xiaosong Qiao, Yanqing Zhao, Shimin Tao, Hao Yang |  |
| 669 |  |  [ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.536) |  | 0 |  | Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, Omer Levy |  |
| 670 |  |  [Data Selection Curriculum for Abstractive Text Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.537) |  | 0 |  | Shichao Sun, Ruifeng Yuan, Jianfei He, Ziqiang Cao, Wenjie Li, Xiaohua Jia |  |
| 671 |  |  [Romanization-based Large-scale Adaptation of Multilingual Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.538) |  | 0 |  | Sukannya Purkayastha, Sebastian Ruder, Jonas Pfeiffer, Iryna Gurevych, Ivan Vulic |  |
| 672 |  |  [Measuring bias in Instruction-Following models with P-AT](https://doi.org/10.18653/v1/2023.findings-emnlp.539) |  | 0 |  | Dario Onorati, Elena Sofia Ruzzetti, Davide Venditti, Leonardo Ranaldi, Fabio Massimo Zanzotto |  |
| 673 |  |  [Open-ended Commonsense Reasoning with Unrestricted Answer Candidates](https://doi.org/10.18653/v1/2023.findings-emnlp.540) |  | 0 |  | Chen Ling, Xuchao Zhang, Xujiang Zhao, Yanchi Liu, Wei Cheng, Mika Oishi, Takao Osaki, Katsushi Matsuda, Haifeng Chen, Liang Zhao |  |
| 674 |  |  [Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units](https://doi.org/10.18653/v1/2023.findings-emnlp.541) |  | 0 |  | Gallil Maimon, Yossi Adi |  |
| 675 |  |  [Knowledge-Selective Pretraining for Attribute Value Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.542) |  | 0 |  | Hui Liu, Qingyu Yin, Zhengyang Wang, Chenwei Zhang, Haoming Jiang, Yifan Gao, Zheng Li, Xian Li, Chao Zhang, Bing Yin, William Wang, Xiaodan Zhu |  |
| 676 |  |  [New Datasets and Controllable Iterative Data Augmentation Method for Code-switching ASR Error Correction](https://doi.org/10.18653/v1/2023.findings-emnlp.543) |  | 0 |  | Zhaohong Wan, Xiaojun Wan, Wei Peng, Rongjun Li |  |
| 677 |  |  [Efficient k-NN Search with Cross-Encoders using Adaptive Multi-Round CUR Decomposition](https://doi.org/10.18653/v1/2023.findings-emnlp.544) |  | 0 |  | Nishant Yadav, Nicholas Monath, Manzil Zaheer, Andrew McCallum |  |
| 678 |  |  [Isotropic Representation Can Improve Zero-Shot Cross-Lingual Transfer on Multilingual Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.545) |  | 0 |  | Yixin Ji, Jikai Wang, Juntao Li, Hai Ye, Min Zhang |  |
| 679 |  |  [Blackbird language matrices (BLM), a new task for rule-like generalization in neural networks: Can Large Language Models pass the test?](https://doi.org/10.18653/v1/2023.findings-emnlp.546) |  | 0 |  | Paola Merlo |  |
| 680 |  |  [DistillCSE: Distilled Contrastive Learning for Sentence Embeddings](https://doi.org/10.18653/v1/2023.findings-emnlp.547) |  | 0 |  | Jiahao Xu, Wei Shao, Lihui Chen, Lemao Liu |  |
| 681 |  |  [GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets](https://doi.org/10.18653/v1/2023.findings-emnlp.548) |  | 0 |  | Wolfgang Otto, Matthäus Zloch, Lu Gan, Saurav Karmakar, Stefan Dietze |  |
| 682 |  |  [Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.549) |  | 0 |  | John M. Giorgi, Luca Soldaini, Bo Wang, Gary D. Bader, Kyle Lo, Lucy Lu Wang, Arman Cohan |  |
| 683 |  |  [Few-shot Unified Question Answering: Tuning Models or Prompts?](https://doi.org/10.18653/v1/2023.findings-emnlp.550) |  | 0 |  | Srijan Bansal, Semih Yavuz, Bo Pang, Meghana Bhat, Yingbo Zhou |  |
| 684 |  |  [Finding Common Ground: Annotating and Predicting Common Ground in Spoken Conversations](https://doi.org/10.18653/v1/2023.findings-emnlp.551) |  | 0 |  | Magdalena Markowska, Mohammad Taghizadeh, Adil Soubki, Seyed Abolghasem Mirroshandel, Owen Rambow |  |
| 685 |  |  [Getting MoRE out of Mixture of Language Model Reasoning Experts](https://doi.org/10.18653/v1/2023.findings-emnlp.552) |  | 0 |  | Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, Jordan L. BoydGraber |  |
| 686 |  |  ["You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract Meaning Representation](https://doi.org/10.18653/v1/2023.findings-emnlp.553) |  | 0 |  | Allyson Ettinger, Jena D. Hwang, Valentina Pyatkin, Chandra Bhagavatula, Yejin Choi |  |
| 687 |  |  [Zero-Shot Data Maps. Efficient Dataset Cartography Without Model Training](https://doi.org/10.18653/v1/2023.findings-emnlp.554) |  | 0 |  | Angelo Basile, Marc FrancoSalvador, Paolo Rosso |  |
| 688 |  |  [Isotropy-Enhanced Conditional Masked Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.555) |  | 0 |  | Pei Guo, Yisheng Xiao, Juntao Li, Yixin Ji, Min Zhang |  |
| 689 |  |  [Scaling Law for Document Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.556) |  | 0 |  | Zhuocheng Zhang, Shuhao Gu, Min Zhang, Yang Feng |  |
| 690 |  |  [Automatic Pronunciation Assessment - A Review](https://doi.org/10.18653/v1/2023.findings-emnlp.557) |  | 0 |  | Yassine El Kheir, Ahmed Ali, Shammur Absar Chowdhury |  |
| 691 |  |  [Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model](https://doi.org/10.18653/v1/2023.findings-emnlp.558) |  | 0 |  | Yinghan Long, Sayeed Shafayet Chowdhury, Kaushik Roy |  |
| 692 |  |  [PUNR: Pre-training with User Behavior Modeling for News Recommendation](https://doi.org/10.18653/v1/2023.findings-emnlp.559) |  | 0 |  | Guangyuan Ma, Hongtao Liu, Xing Wu, Wanhui Qian, Zhepeng Lv, Qing Yang, Songlin Hu |  |
| 693 |  |  [Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design](https://doi.org/10.18653/v1/2023.findings-emnlp.560) |  | 0 |  | Henry Sprueill, Carl Edwards, Mariefel V. Olarte, Udishnu Sanyal, Heng Ji, Sutanay Choudhury |  |
| 694 |  |  [Measure Children's Mindreading Ability with Machine Reading](https://doi.org/10.18653/v1/2023.findings-emnlp.561) |  | 0 |  | Yuliang Yan, Xiaohua Wang, Xiang Zhou, Xiaoqing Zheng, Xuanjing Huang |  |
| 695 |  |  [Crosslingual Transfer Learning for Low-Resource Languages Based on Multilingual Colexification Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.562) |  | 0 |  | Yihong Liu, Haotian Ye, Leonie Weissweiler, Renhao Pei, Hinrich Schütze |  |
| 696 |  |  [Injecting structural hints: Using language models to study inductive biases in language learning](https://doi.org/10.18653/v1/2023.findings-emnlp.563) |  | 0 |  | Isabel Papadimitriou, Dan Jurafsky |  |
| 697 |  |  [Machine Reading Comprehension using Case-based Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.564) |  | 0 |  | Dung Thai, Dhruv Agarwal, Mudit Chaudhary, Wenlong Zhao, Rajarshi Das, JayYoon Lee, Hannaneh Hajishirzi, Manzil Zaheer, Andrew McCallum |  |
| 698 |  |  [Unleashing the Power of Language Models in Text-Attributed Graph](https://doi.org/10.18653/v1/2023.findings-emnlp.565) |  | 0 |  | Haoyu Kuang, Jiarong Xu, Haozhe Zhang, Zuyu Zhao, Qi Zhang, Xuanjing Huang, Zhongyu Wei |  |
| 699 |  |  [Locally Differentially Private Document Generation Using Zero Shot Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.566) |  | 0 |  | Saiteja Utpala, Sara Hooker, PinYu Chen |  |
| 700 |  |  [Contrastive Deterministic Autoencoders For Language Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.567) |  | 0 |  | Amur Ghose, Pascal Poupart |  |
| 701 |  |  [CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.568) |  | 0 |  | Denis Jered McInerney, Geoffrey S. Young, JanWillem van de Meent, Byron C. Wallace |  |
| 702 |  |  [Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers](https://doi.org/10.18653/v1/2023.findings-emnlp.569) |  | 0 |  | Mosh Levy, Shauli Ravfogel, Yoav Goldberg |  |
| 703 |  |  [Large Language Models Meet Harry Potter: A Dataset for Aligning Dialogue Agents with Characters](https://doi.org/10.18653/v1/2023.findings-emnlp.570) |  | 0 |  | Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan Li, Ziyang Chen, Longyue Wang, Jia Li |  |
| 704 |  |  [Quick Back-Translation for Unsupervised Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.571) |  | 0 |  | Benjamin Brimacombe, Jiawei Zhou |  |
| 705 |  |  [SIR-ABSC: Incorporating Syntax into RoBERTa-based Sentiment Analysis Models with a Special Aggregator Token](https://doi.org/10.18653/v1/2023.findings-emnlp.572) |  | 0 |  | Ikhyun Cho, Yoonhwa Jung, Julia Hockenmaier |  |
| 706 |  |  [Citance-Contextualized Summarization of Scientific Papers](https://doi.org/10.18653/v1/2023.findings-emnlp.573) |  | 0 |  | Shahbaz Syed, Ahmad Dawar Hakimi, Khalid Al Khatib, Martin Potthast |  |
| 707 |  |  [SegAugment: Maximizing the Utility of Speech Translation Data with Segmentation-based Augmentations](https://doi.org/10.18653/v1/2023.findings-emnlp.574) |  | 0 |  | Ioannis Tsiamas, José A. R. Fonollosa, Marta R. Costajussà |  |
| 708 |  |  [Intersectional Stereotypes in Large Language Models: Dataset and Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.575) |  | 0 |  | Weicheng Ma, Brian Chiang, Tong Wu, Lili Wang, Soroush Vosoughi |  |
| 709 |  |  [Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond](https://doi.org/10.18653/v1/2023.findings-emnlp.576) |  | 0 |  | Zhecan Wang, Long Chen, Haoxuan You, Keyang Xu, Yicheng He, Wenhao Li, Noel Codella, KaiWei Chang, ShihFu Chang |  |
| 710 |  |  [The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who](https://doi.org/10.18653/v1/2023.findings-emnlp.577) |  | 0 |  | Michael Sejr Schlichtkrull, Nedjma Ousidhoum, Andreas Vlachos |  |
| 711 |  |  [Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression](https://doi.org/10.18653/v1/2023.findings-emnlp.578) |  | 0 |  | Jiduan Liu, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Dongyan Zhao, Ran Wang, Rui Yan |  |
| 712 |  |  [COUNT: COntrastive UNlikelihood Text Style Transfer for Text Detoxification](https://doi.org/10.18653/v1/2023.findings-emnlp.579) |  | 0 |  | Mohammad Mahdi Abdollah Pour, Parsa Farinneya, Manasa Bharadwaj, Nikhil Verma, Ali Pesaranghader, Scott Sanner |  |
| 713 |  |  [KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.findings-emnlp.580) |  | 0 |  | Yanbin Wei, Qiushi Huang, Yu Zhang, James T. Kwok |  |
| 714 |  |  [Show, Write, and Retrieve: Entity-aware Article Generation and Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.581) |  | 0 |  | Zhongping Zhang, Yiwen Gu, Bryan A. Plummer |  |
| 715 |  |  [A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing](https://doi.org/10.18653/v1/2023.findings-emnlp.582) |  | 0 |  | William Timkey, Tal Linzen |  |
| 716 |  |  [Annotations Are Not All You Need: A Cross-modal Knowledge Transfer Network for Unsupervised Temporal Sentence Grounding](https://doi.org/10.18653/v1/2023.findings-emnlp.583) |  | 0 |  | Xiang Fang, Daizong Liu, Wanlong Fang, Pan Zhou, Yu Cheng, Keke Tang, Kai Zou |  |
| 717 |  |  [Parameter Efficient Multi-task Fine-tuning by Learning to Transfer Token-wise Prompts](https://doi.org/10.18653/v1/2023.findings-emnlp.584) |  | 0 |  | Muling Wu, Wenhao Liu, Jianhan Xu, Changze Lv, Zixuan Ling, Tianlong Li, Longtao Huang, Xiaoqing Zheng, Xuanjing Huang |  |
| 718 |  |  [A Rewriting Approach for Gender Inclusivity in Portuguese](https://doi.org/10.18653/v1/2023.findings-emnlp.585) |  | 0 |  | Leonor Veloso, Luísa Coheur, Rui Ribeiro |  |
| 719 |  |  [EARA: Improving Biomedical Semantic Textual Similarity with Entity-Aligned Attention and Retrieval Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.586) |  | 0 |  | Ying Xiong, Xin Yang, Linjing Liu, KaChun Wong, Qingcai Chen, Yang Xiang, Buzhou Tang |  |
| 720 |  |  [Neuro-Symbolic Sentiment Analysis with Dynamic Word Sense Disambiguation](https://doi.org/10.18653/v1/2023.findings-emnlp.587) |  | 0 |  | Xulang Zhang, Rui Mao, Kai He, Erik Cambria |  |
| 721 |  |  [Role of Context in Unsupervised Sentence Representation Learning: the Case of Dialog Act Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.588) |  | 0 |  | Rastislav Hronsky, Emmanuel Keuleers |  |
| 722 |  |  [CLMSM: A Multi-Task Learning Framework for Pre-training on Procedural Text](https://doi.org/10.18653/v1/2023.findings-emnlp.589) |  | 0 |  | Abhilash Nandy, Manav Nitin Kapadnis, Pawan Goyal, Niloy Ganguly |  |
| 723 |  |  [Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking](https://doi.org/10.18653/v1/2023.findings-emnlp.590) |  | 0 |  | Shengyao Zhuang, Bing Liu, Bevan Koopman, Guido Zuccon |  |
| 724 |  |  [On General Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.591) |  | 0 |  | David Schlangen |  |
| 725 |  |  [USB: A Unified Summarization Benchmark Across Tasks and Domains](https://doi.org/10.18653/v1/2023.findings-emnlp.592) |  | 0 |  | Kundan Krishna, Prakhar Gupta, Sanjana Ramprasad, Byron C. Wallace, Jeffrey P. Bigham, Zachary C. Lipton |  |
| 726 |  |  [tagE: Enabling an Embodied Agent to Understand Human Instructions](https://doi.org/10.18653/v1/2023.findings-emnlp.593) |  | 0 |  | Chayan Sarkar, Avik Mitra, Pradip Pramanick, Tapas Nayak |  |
| 727 |  |  [Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.594) |  | 0 |  | Simon Chi Lok U, Jie He, Víctor GutiérrezBasulto, Jeff Z. Pan |  |
| 728 |  |  [Uncovering Limitations in Text-to-Image Generation: A Contrastive Approach with Structured Semantic Alignment](https://doi.org/10.18653/v1/2023.findings-emnlp.595) |  | 0 |  | Qianyu Feng, Yulei Sui, Hongyu Zhang |  |
| 729 |  |  [An Intent-based and Annotation-free Method for Duplicate Question Detection in CQA Forums](https://doi.org/10.18653/v1/2023.findings-emnlp.596) |  | 0 |  | Yubo Shu, Hansu Gu, Peng Zhang, Tun Lu, Ning Gu |  |
| 730 |  |  [Accelerating Multiple Intent Detection and Slot Filling via Targeted Knowledge Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.597) |  | 0 |  | Xuxin Cheng, Zhihong Zhu, Wanshi Xu, Yaowei Li, Hongxiang Li, Yuexian Zou |  |
| 731 |  |  [Type-Aware Decomposed Framework for Few-Shot Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.598) |  | 0 |  | Yongqi Li, Yu Yu, Tieyun Qian |  |
| 732 |  |  [A Closer Look into Using Large Language Models for Automatic Evaluation](https://doi.org/10.18653/v1/2023.findings-emnlp.599) |  | 0 |  | David ChengHan Chiang, Hungyi Lee |  |
| 733 |  |  [Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification using Graph Neural Networks?](https://doi.org/10.18653/v1/2023.findings-emnlp.600) |  | 0 |  | Margarita Bugueño, Gerard de Melo |  |
| 734 |  |  [Natural Language Annotations for Reasoning about Program Semantics](https://doi.org/10.18653/v1/2023.findings-emnlp.601) |  | 0 |  | Marco Zocca |  |
| 735 |  |  [Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate to Speech Emotion Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.602) |  | 0 |  | Isaac Slaughter, Craig Greenberg, Reva Schwartz, Aylin Caliskan |  |
| 736 |  |  [Text Classification via Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.603) |  | 0 |  | Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, Guoyin Wang |  |
| 737 |  |  [On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.604) |  | 0 |  | Jiayi Chen, Hanjun Dai, Bo Dai, Aidong Zhang, Wei Wei |  |
| 738 |  |  [Semi-Structured Object Sequence Encoders](https://doi.org/10.18653/v1/2023.findings-emnlp.605) |  | 0 |  | Rudra Murthy V, Riyaz A. Bhat, R. Chulaka Gunasekara, Siva Sankalp Patel, Hui Wan, Tejas I. Dhamecha, Danish Contractor, Marina Danilevsky |  |
| 739 |  |  [DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM](https://doi.org/10.18653/v1/2023.findings-emnlp.606) |  | 0 |  | Weijie Xu, Wenxiang Hu, Fanyou Wu, Srinivasan H. Sengamedu |  |
| 740 |  |  [Energy and Carbon Considerations of Fine-Tuning BERT](https://doi.org/10.18653/v1/2023.findings-emnlp.607) |  | 0 |  | Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni |  |
| 741 |  |  [Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models](https://doi.org/10.18653/v1/2023.findings-emnlp.608) |  | 0 |  | Sumuk Shashidhar, Abhinav Chinta, Vaibhav Sahai, Zhenhailong Wang, Heng Ji |  |
| 742 |  |  [Chinese Metaphorical Relation Extraction: Dataset and Models](https://doi.org/10.18653/v1/2023.findings-emnlp.609) |  | 0 |  | Guihua Chen, Tiantian Wu, MiaoMiao Cheng, Xu Han, Jiefu Gong, Shijin Wang, Wei Song |  |
| 743 |  |  [Example-based Hypernetworks for Multi-source Adaptation to Unseen Domains](https://doi.org/10.18653/v1/2023.findings-emnlp.610) |  | 0 |  | Tomer Volk, Eyal BenDavid, Ohad Amosy, Gal Chechik, Roi Reichart |  |
| 744 |  |  [Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.611) |  | 0 |  | Hongzhan Lin, Ziyang Luo, Jing Ma, Long Chen |  |
| 745 |  |  [Domain Adaptation for Conversational Query Production with the RAG Model Feedback](https://doi.org/10.18653/v1/2023.findings-emnlp.612) |  | 0 |  | Ante Wang, Linfeng Song, Ge Xu, Jinsong Su |  |
| 746 |  |  [LEGO: A Multi-agent Collaborative Framework with Role-playing and Iterative Feedback for Causality Explanation Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.613) |  | 0 |  | Zhitao He, Pengfei Cao, Yubo Chen, Kang Liu, Ruopeng Li, Mengshu Sun, Jun Zhao |  |
| 747 |  |  [Ranking LLM-Generated Loop Invariants for Program Verification](https://doi.org/10.18653/v1/2023.findings-emnlp.614) |  | 0 |  | Saikat Chakraborty, Shuvendu K. Lahiri, Sarah Fakhoury, Akash Lal, Madanlal Musuvathi, Aseem Rastogi, Aditya Senthilnathan, Rahul Sharma, Nikhil Swamy |  |
| 748 |  |  [WordNet Is All You Need: A Surprisingly Effective Unsupervised Method for Graded Lexical Entailment](https://doi.org/10.18653/v1/2023.findings-emnlp.615) |  | 0 |  | Joseph Renner, Pascal Denis, Rémi Gilleron |  |
| 749 |  |  [Knowledge Corpus Error in Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.616) |  | 0 |  | Yejoon Lee, Philhoon Oh, James Thorne |  |
| 750 |  |  [Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.617) |  | 0 |  | Markus Freitag, Behrooz Ghorbani, Patrick Fernandes |  |
| 751 |  |  [The language of prompting: What linguistic properties make a prompt successful?](https://doi.org/10.18653/v1/2023.findings-emnlp.618) |  | 0 |  | Alina Leidinger, Robert van Rooij, Ekaterina Shutova |  |
| 752 |  |  [When and Why Does Bias Mitigation Work?](https://doi.org/10.18653/v1/2023.findings-emnlp.619) |  | 0 |  | Abhilasha Ravichander, Joe Stacey, Marek Rei |  |
| 753 |  |  [Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy](https://doi.org/10.18653/v1/2023.findings-emnlp.620) |  | 0 |  | Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen |  |
| 754 |  |  [Dynamic Low-rank Estimation for Transformer-based Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.621) |  | 0 |  | Ting Hua, Xiao Li, Shangqian Gao, YenChang Hsu, Yilin Shen, Hongxia Jin |  |
| 755 |  |  [Non-parallel Accent Transfer based on Fine-grained Controllable Accent Modelling](https://doi.org/10.18653/v1/2023.findings-emnlp.622) |  | 0 |  | Linqin Wang, Zhengtao Yu, Yuanzhang Yang, Shengxiang Gao, Cunli Mao, Yuxin Huang |  |
| 756 |  |  [Compositional Generalization for Data-to-Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.623) |  | 0 |  | Xinnuo Xu, Ivan Titov, Mirella Lapata |  |
| 757 |  |  [In-Context Learning Creates Task Vectors](https://doi.org/10.18653/v1/2023.findings-emnlp.624) |  | 0 |  | Roee Hendel, Mor Geva, Amir Globerson |  |
| 758 |  |  [TalkUp: Paving the Way for Understanding Empowering Language](https://doi.org/10.18653/v1/2023.findings-emnlp.625) |  | 0 |  | Lucille Njoo, Chan Young Park, Octavia Stappart, Marvin Thielk, Yi Chu, Yulia Tsvetkov |  |
| 759 |  |  [Unifying Text, Tables, and Images for Multimodal Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.626) |  | 0 |  | Haohao Luo, Ying Shen, Yang Deng |  |
| 760 |  |  [Unsupervised Lexical Simplification with Context Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.627) |  | 0 |  | Takashi Wada, Timothy Baldwin, Jey Han Lau |  |
| 761 |  |  [mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences](https://doi.org/10.18653/v1/2023.findings-emnlp.628) |  | 0 |  | David C. Uthus, Santiago Ontañón, Joshua Ainslie, Mandy Guo |  |
| 762 |  |  [Multilingual Lottery Tickets to Pretrain Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.629) |  | 0 |  | Jaeseong Lee, Seungwon Hwang |  |
| 763 |  |  [Target-Aware Spatio-Temporal Reasoning via Answering Questions in Dynamic Audio-Visual Scenarios](https://doi.org/10.18653/v1/2023.findings-emnlp.630) |  | 0 |  | Yuanyuan Jiang, Jianqin Yin |  |
| 764 |  |  [KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.631) |  | 0 |  | Jiho Kim, Yeonsu Kwon, Yohan Jo, Edward Choi |  |
| 765 |  |  [Breaking the Language Barrier: Improving Cross-Lingual Reasoning with Structured Self-Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.632) |  | 0 |  | Negar Foroutan, Mohammadreza Banaei, Karl Aberer, Antoine Bosselut |  |
| 766 |  |  [CITB: A Benchmark for Continual Instruction Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.633) |  | 0 |  | Zihan Zhang, Meng Fang, Ling Chen, MohammadReza NamaziRad |  |
| 767 |  |  [Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.634) |  | 0 |  | Raymond Li, Gabriel Murray, Giuseppe Carenini |  |
| 768 |  |  [Towards Better Representations for Multi-Label Text Classification with Multi-granularity Information](https://doi.org/10.18653/v1/2023.findings-emnlp.635) |  | 0 |  | Fangfang Li, Puzhen Su, Junwen Duan, Weidong Xiao |  |
| 769 |  |  [PCMID: Multi-Intent Detection through Supervised Prototypical Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.636) |  | 0 |  | Yurun Song, Junchen Zhao, Spencer Koehler, Amir Abdullah, Ian G. Harris |  |
| 770 |  |  [Is GPT-4 a Good Data Analyst?](https://doi.org/10.18653/v1/2023.findings-emnlp.637) |  | 0 |  | Liying Cheng, Xingxuan Li, Lidong Bing |  |
| 771 |  |  [DiffusionRet: Diffusion-Enhanced Generative Retriever using Constrained Decoding](https://doi.org/10.18653/v1/2023.findings-emnlp.638) |  | 0 |  | Shanbao Qiao, Xuebing Liu, SeungHoon Na |  |
| 772 |  |  [Estimating Large Language Model Capabilities without Labeled Test Data](https://doi.org/10.18653/v1/2023.findings-emnlp.639) |  | 0 |  | Harvey Yiyun Fu, Qinyuan Ye, Albert Xu, Xiang Ren, Robin Jia |  |
| 773 |  |  [A Novel Contrastive Learning Method for Clickbait Detection on RoCliCo: A Romanian Clickbait Corpus of News Articles](https://doi.org/10.18653/v1/2023.findings-emnlp.640) |  | 0 |  | DariaMihaela Broscoteanu, Radu Tudor Ionescu |  |
| 774 |  |  [Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogues](https://doi.org/10.18653/v1/2023.findings-emnlp.641) |  | 0 |  | Hongru Wang, Minda Hu, Yang Deng, Rui Wang, Fei Mi, Weichao Wang, Yasheng Wang, WaiChung Kwan, Irwin King, KamFai Wong |  |
| 775 |  |  [Toxicity in Multilingual Machine Translation at Scale](https://doi.org/10.18653/v1/2023.findings-emnlp.642) |  | 0 |  | Marta R. Costajussà, Eric Michael Smith, Christophe Ropers, Daniel Licht, Jean Maillard, Javier Ferrando, Carlos Escolano |  |
| 776 |  |  [Conversational Recommender System and Large Language Model Are Made for Each Other in E-commerce Pre-sales Dialogue](https://doi.org/10.18653/v1/2023.findings-emnlp.643) |  | 0 |  | Yuanxing Liu, Weinan Zhang, Yifan Chen, Yuchi Zhang, Haopeng Bai, Fan Feng, Hengbin Cui, Yongbin Li, Wanxiang Che |  |
| 777 |  |  [VIP5: Towards Multimodal Foundation Models for Recommendation](https://doi.org/10.18653/v1/2023.findings-emnlp.644) |  | 0 |  | Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, Yongfeng Zhang |  |
| 778 |  |  [A Spectral Viewpoint on Continual Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.645) |  | 0 |  | Huy Nguyen, Chien Nguyen, Linh Ngo Van, Anh Tuan Luu, Thien Huu Nguyen |  |
| 779 |  |  [Learning to Follow Object-Centric Image Editing Instructions Faithfully](https://doi.org/10.18653/v1/2023.findings-emnlp.646) |  | 0 |  | Tuhin Chakrabarty, Kanishk Singh, Arkadiy Saakyan, Smaranda Muresan |  |
| 780 |  |  [Zero-shot Topical Text Classification with LLMs - an Experimental Study](https://doi.org/10.18653/v1/2023.findings-emnlp.647) |  | 0 |  | Shai Gretz, Alon Halfon, Ilya Shnayderman, Orith ToledoRonen, Artem Spector, Lena Dankin, Yannis Katsis, Ofir Arviv, Yoav Katz, Noam Slonim, Liat EinDor |  |
| 781 |  |  [Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems](https://doi.org/10.18653/v1/2023.findings-emnlp.648) |  | 0 |  | Yixin Wan, Jieyu Zhao, Aman Chadha, Nanyun Peng, KaiWei Chang |  |
| 782 |  |  [A Black-Box Attack on Code Models via Representation Nearest Neighbor Search](https://doi.org/10.18653/v1/2023.findings-emnlp.649) |  | 0 |  | Jie Zhang, Wei Ma, Qiang Hu, Shangqing Liu, Xiaofei Xie, Yves Le Traon, Yang Liu |  |
| 783 |  |  [How Well Do Text Embedding Models Understand Syntax?](https://doi.org/10.18653/v1/2023.findings-emnlp.650) |  | 0 |  | Yan Zhang, Zhaopeng Feng, Zhiyang Teng, Zuozhu Liu, Haizhou Li |  |
| 784 |  |  [CASSI: Contextual and Semantic Structure-based Interpolation Augmentation for Low-Resource NER](https://doi.org/10.18653/v1/2023.findings-emnlp.651) |  | 0 |  | Tanmay Surana, ThiNga Ho, Kyaw Zin Tun, Eng Siong Chng |  |
| 785 |  |  [NEWTON: Are Large Language Models Capable of Physical Reasoning?](https://doi.org/10.18653/v1/2023.findings-emnlp.652) |  | 0 |  | Yi Ru Wang, Jiafei Duan, Dieter Fox, Siddhartha S. Srinivasa |  |
| 786 |  |  [Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language](https://doi.org/10.18653/v1/2023.findings-emnlp.653) |  | 0 |  | Jimin Mun, Emily Allaway, Akhila Yerukola, Laura Vianna, SarahJane Leslie, Maarten Sap |  |
| 787 |  |  [On the Calibration of Large Language Models and Alignment](https://doi.org/10.18653/v1/2023.findings-emnlp.654) |  | 0 |  | Chiwei Zhu, Benfeng Xu, Quan Wang, Yongdong Zhang, Zhendong Mao |  |
| 788 |  |  [TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction](https://doi.org/10.18653/v1/2023.findings-emnlp.655) |  | 0 |  | Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, Yiming Qian |  |
| 789 |  |  [Identifying Conspiracy Theories News based on Event Relation Graph](https://doi.org/10.18653/v1/2023.findings-emnlp.656) |  | 0 |  | Yuanyuan Lei, Ruihong Huang |  |
| 790 |  |  [Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems](https://doi.org/10.18653/v1/2023.findings-emnlp.657) |  | 0 |  | Lidiya Murakhovs'ka, Philippe Laban, Tian Xie, Caiming Xiong, ChienSheng Wu |  |
| 791 |  |  [Dynamic Open-book Prompt for Conversational Recommender System](https://doi.org/10.18653/v1/2023.findings-emnlp.658) |  | 0 |  | Xuan Ma, Tieyun Qian, Ke Sun |  |
| 792 |  |  [Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.659) |  | 0 |  | Zhihan Zhang, Shuohang Wang, Wenhao Yu, Yichong Xu, Dan Iter, Qingkai Zeng, Yang Liu, Chenguang Zhu, Meng Jiang |  |
| 793 |  |  [DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models](https://doi.org/10.18653/v1/2023.findings-emnlp.660) |  | 0 |  | Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, Lingpeng Kong |  |
| 794 |  |  [M2C: Towards Automatic Multimodal Manga Complement](https://doi.org/10.18653/v1/2023.findings-emnlp.661) |  | 0 |  | Hongcheng Guo, Boyang Wang, Jiaqi Bai, Jiaheng Liu, Jian Yang, Zhoujun Li |  |
| 795 |  |  [Learn Your Tokens: Word-Pooled Tokenization for Language Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.662) |  | 0 |  | Avijit Thawani, Saurabh Ghanekar, Xiaoyuan Zhu, Jay Pujara |  |
| 796 |  |  [Towards Detecting Contextual Real-Time Toxicity for In-Game Chat](https://doi.org/10.18653/v1/2023.findings-emnlp.663) |  | 0 |  | Zachary Yang, Nicolas GrenonGodbout, Reihaneh Rabbany |  |
| 797 |  |  [JWSign: A Highly Multilingual Corpus of Bible Translations for more Diversity in Sign Language Processing](https://doi.org/10.18653/v1/2023.findings-emnlp.664) |  | 0 |  | Shester Gueuwou, Sophie Siake, Colin Leong, Mathias Müller |  |
| 798 |  |  [Do Stochastic Parrots have Feelings Too? Improving Neural Detection of Synthetic Text via Emotion Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.665) |  | 0 |  | Alan Cowap, Yvette Graham, Jennifer Foster |  |
| 799 |  |  [Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules](https://doi.org/10.18653/v1/2023.findings-emnlp.666) |  | 0 |  | Chaojun Xiao, Yuqi Luo, Wenbin Zhang, Pengle Zhang, Xu Han, Yankai Lin, Zhengyan Zhang, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou |  |
| 800 |  |  [PivotFEC: Enhancing Few-shot Factual Error Correction with a Pivot Task Approach using Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.667) |  | 0 |  | Xingwei He, ALong Jin, Jun Ma, Yuan Yuan, Siu Ming Yiu |  |
| 801 |  |  [Semantic Similarity Covariance Matrix Shrinkage](https://doi.org/10.18653/v1/2023.findings-emnlp.668) |  | 0 |  | Guillaume Becquin, Saher Esmeir |  |
| 802 |  |  [LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.669) |  | 0 |  | ShihChieh Dai, Aiping Xiong, LunWei Ku |  |
| 803 |  |  [LLM aided semi-supervision for efficient Extractive Dialog Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.670) |  | 0 |  | Nishant Mishra, Gaurav Sahu, Iacer Calixto, Ameen AbuHanna, Issam H. Laradji |  |
| 804 |  |  [Investigating Multilingual Coreference Resolution by Universal Annotations](https://doi.org/10.18653/v1/2023.findings-emnlp.671) |  | 0 |  | Haixia Chai, Michael Strube |  |
| 805 |  |  [FactSpotter: Evaluating the Factual Faithfulness of Graph-to-Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.672) |  | 0 |  | Kun Zhang, Oana Balalau, Ioana Manolescu |  |
| 806 |  |  [LayoutDIT: Layout-Aware End-to-End Document Image Translation with Multi-Step Conductive Decoder](https://doi.org/10.18653/v1/2023.findings-emnlp.673) |  | 0 |  | Zhiyang Zhang, Yaping Zhang, Yupu Liang, Lu Xiang, Yang Zhao, Yu Zhou, Chengqing Zong |  |
| 807 |  |  [Balaur: Language Model Pretraining with Lexical Semantic Relations](https://doi.org/10.18653/v1/2023.findings-emnlp.674) |  | 0 |  | Andrei Mircea, Jackie C. K. Cheung |  |
| 808 |  |  [Exploring In-Context Learning for Knowledge Grounded Dialog Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.675) |  | 0 |  | Qinyu Chen, Wenhao Wu, Sujian Li |  |
| 809 |  |  [Towards Enhancing Relational Rules for Knowledge Graph Link Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.676) |  | 0 |  | Shuhan Wu, Huaiyu Wan, Wei Chen, Yuting Wu, Junfeng Shen, Youfang Lin |  |
| 810 |  |  [Are NLP Models Good at Tracing Thoughts: An Overview of Narrative Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.677) |  | 0 |  | Lixing Zhu, Runcong Zhao, Lin Gui, Yulan He |  |
| 811 |  |  [Who is Speaking? Speaker-Aware Multiparty Dialogue Act Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.678) |  | 0 |  | Ayesha Qamar, Adarsh Pyarelal, Ruihong Huang |  |
| 812 |  |  [Demystifying Prompts in Language Models via Perplexity Estimation](https://doi.org/10.18653/v1/2023.findings-emnlp.679) |  | 0 |  | Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith, Luke Zettlemoyer |  |
| 813 |  |  [C2D2 Dataset: A Resource for the Cognitive Distortion Analysis and Its Impact on Mental Health](https://doi.org/10.18653/v1/2023.findings-emnlp.680) |  | 0 |  | Bichen Wang, Pengfei Deng, Yanyan Zhao, Bing Qin |  |
| 814 |  |  [MixEdit: Revisiting Data Augmentation and Beyond for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.findings-emnlp.681) |  | 0 |  | Jingheng Ye, Yinghui Li, Yangning Li, HaiTao Zheng |  |
| 815 |  |  [CCEval: A Representative Evaluation Benchmark for the Chinese-centric Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.682) |  | 0 |  | Lianzhang Lou, Xi Yin, Yutao Xie, Yang Xiang |  |
| 816 |  |  [ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual Common Sense](https://doi.org/10.18653/v1/2023.findings-emnlp.683) |  | 0 |  | Kankan Zhou, Eason Lai, Wei Bin Au Yeong, Kyriakos Mouratidis, Jing Jiang |  |
| 817 |  |  [Automatic Analysis of Substantiation in Scientific Peer Reviews](https://doi.org/10.18653/v1/2023.findings-emnlp.684) |  | 0 |  | Yanzhu Guo, Guokan Shang, Virgile Rennard, Michalis Vazirgiannis, Chloé Clavel |  |
| 818 |  |  [Hierarchical Prompting Assists Large Language Model on Web Navigation](https://doi.org/10.18653/v1/2023.findings-emnlp.685) |  | 0 |  | Robert Lo, Abishek Sridhar, Frank F. Xu, Hao Zhu, Shuyan Zhou |  |
| 819 |  |  [Can Large Language Models Fix Data Annotation Errors? An Empirical Study Using Debatepedia for Query-Focused Text Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.686) |  | 0 |  | Md. Tahmid Rahman Laskar, Mizanur Rahman, Israt Jahan, Enamul Hoque, Jimmy Xiangji Huang |  |
| 820 |  |  [TSTR: Target Similarity Tuning Meets the Real World](https://doi.org/10.18653/v1/2023.findings-emnlp.687) |  | 0 |  | Anirudh Khatry, Sumit Gulwani, Priyanshu Gupta, Vu Le, Mukul Singh, Ananya Singha, Gust Verbruggen |  |
| 821 |  |  [RealBehavior: A Framework for Faithfully Characterizing Foundation Models' Human-like Behavior Mechanisms](https://doi.org/10.18653/v1/2023.findings-emnlp.688) |  | 0 |  | Enyu Zhou, Rui Zheng, Zhiheng Xi, Songyang Gao, Xiaoran Fan, Zichu Fei, Jingting Ye, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 822 |  |  [Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance](https://doi.org/10.18653/v1/2023.findings-emnlp.689) |  | 0 |  | Thiemo Wambsganss, Xiaotian Su, Vinitra Swamy, Seyed Parsa Neshaei, Roman Rietsche, Tanja Käser |  |
| 823 |  |  [VERVE: Template-based ReflectiVE Rewriting for MotiVational IntErviewing](https://doi.org/10.18653/v1/2023.findings-emnlp.690) |  | 0 |  | Do June Min, Verónica PérezRosas, Ken Resnicow, Rada Mihalcea |  |
| 824 |  |  [Self-Knowledge Guided Retrieval Augmentation for Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.691) |  | 0 |  | Yile Wang, Peng Li, Maosong Sun, Yang Liu |  |
| 825 |  |  [Pretraining Language Models with Text-Attributed Heterogeneous Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.692) |  | 0 |  | Tao Zou, Le Yu, Yifei Huang, Leilei Sun, Bowen Du |  |
| 826 |  |  [CReTIHC: Designing Causal Reasoning Tasks about Temporal Interventions and Hallucinated Confoundings](https://doi.org/10.18653/v1/2023.findings-emnlp.693) |  | 0 |  | Changwoo Chun, Songeun Lee, Jaehyung Seo, Heuiseok Lim |  |
| 827 |  |  [On the Dimensionality of Sentence Embeddings](https://doi.org/10.18653/v1/2023.findings-emnlp.694) |  | 0 |  | Hongwei Wang, Hongming Zhang, Dong Yu |  |
| 828 |  |  [Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.695) |  | 0 |  | Huiyin Xue, Nikolaos Aletras |  |
| 829 |  |  [Entity-Based Evaluation of Political Bias in Automatic Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.696) |  | 0 |  | Karen Zhou, Chenhao Tan |  |
| 830 |  |  [StyleBART: Decorate Pretrained Model with Style Adapters for Unsupervised Stylistic Headline Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.697) |  | 0 |  | Hanqing Wang, Yajing Luo, Boya Xiong, Guanhua Chen, Yun Chen |  |
| 831 |  |  [RSVP: Customer Intent Detection via Agent Response Contrastive and Generative Pre-Training](https://doi.org/10.18653/v1/2023.findings-emnlp.698) |  | 0 |  | YuChien Tang, WeiYao Wang, AnZi Yen, WenChih Peng |  |
| 832 |  |  [Improving Low-resource Question Answering by Augmenting Question Information](https://doi.org/10.18653/v1/2023.findings-emnlp.699) |  | 0 |  | Andong Chen, Yuan Sun, Xiaobing Zhao, Rosella P. Galindo Esparza, Kehai Chen, Yang Xiang, Tiejun Zhao, Min Zhang |  |
| 833 |  |  [InstructSafety: A Unified Framework for Building Multidimensional and Explainable Safety Detector through Instruction Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.700) |  | 0 |  | Zhexin Zhang, Jiale Cheng, Hao Sun, Jiawen Deng, Minlie Huang |  |
| 834 |  |  ["A Tale of Two Movements': Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.701) |  | 0 |  | Shamik Roy, Dan Goldwasser |  |
| 835 |  |  [ClusterPrompt: Cluster Semantic Enhanced Prompt Learning for New Intent Discovery](https://doi.org/10.18653/v1/2023.findings-emnlp.702) |  | 0 |  | Jinggui Liang, Lizi Liao |  |
| 836 |  |  [Investigating the Effect of Pre-finetuning BERT Models on NLI Involving Presuppositions](https://doi.org/10.18653/v1/2023.findings-emnlp.703) |  | 0 |  | Jad Kabbara, Jackie Chi Kit Cheung |  |
| 837 |  |  [MRRL: Modifying the Reference via Reinforcement Learning for Non-Autoregressive Joint Multiple Intent Detection and Slot Filling](https://doi.org/10.18653/v1/2023.findings-emnlp.704) |  | 0 |  | Xuxin Cheng, Zhihong Zhu, Bowen Cao, Qichen Ye, Yuexian Zou |  |
| 838 |  |  [DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy Slot Filling Task](https://doi.org/10.18653/v1/2023.findings-emnlp.705) |  | 0 |  | Guanting Dong, Tingfeng Hui, Zhuoma Gongque, Jinxu Zhao, Daichi Guo, Gang Zhao, Keqing He, Weiran Xu |  |
| 839 |  |  [SHARCS: Efficient Transformers Through Routing with Dynamic Width Sub-networks](https://doi.org/10.18653/v1/2023.findings-emnlp.706) |  | 0 |  | Mohammadreza Salehi, Sachin Mehta, Aditya Kusupati, Ali Farhadi, Hannaneh Hajishirzi |  |
| 840 |  |  [Always the Best Fit: Adaptive Domain Gap Filling from Causal Perspective for Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.707) |  | 0 |  | Ge Bai, Chenji Lu, Jiaxiang Geng, Shilong Li, Yidong Shi, Xiyan Liu, Ying Liu, Zhang Zhang, Ruifang Liu |  |
| 841 |  |  [MEGClass: Extremely Weakly Supervised Text Classification via Mutually-Enhancing Text Granularities](https://doi.org/10.18653/v1/2023.findings-emnlp.708) |  | 0 |  | Priyanka Kargupta, Tanay Komarlu, Susik Yoon, Xuan Wang, Jiawei Han |  |
| 842 |  |  [Causal Inference from Text: Unveiling Interactions between Variables](https://doi.org/10.18653/v1/2023.findings-emnlp.709) |  | 0 |  | Yuxiang Zhou, Yulan He |  |
| 843 |  |  [Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!](https://doi.org/10.18653/v1/2023.findings-emnlp.710) |  | 0 |  | Yubo Ma, Yixin Cao, Yong Hong, Aixin Sun |  |
| 844 |  |  [Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration](https://doi.org/10.18653/v1/2023.findings-emnlp.711) |  | 0 |  | Yang Deng, Lizi Liao, Liang Chen, Hongru Wang, Wenqiang Lei, TatSeng Chua |  |
| 845 |  |  [Ecologically Valid Explanations for Label Variation in NLI](https://doi.org/10.18653/v1/2023.findings-emnlp.712) |  | 0 |  | NanJiang Jiang, Chenhao Tan, MarieCatherine de Marneffe |  |
| 846 |  |  [A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.713) |  | 0 |  | Adrian Kochsiek, Rainer Gemulla |  |
| 847 |  |  [SummIt: Iterative Text Summarization via ChatGPT](https://doi.org/10.18653/v1/2023.findings-emnlp.714) |  | 0 |  | Haopeng Zhang, Xiao Liu, Jiawei Zhang |  |
| 848 |  |  [Orthogonal Subspace Learning for Language Model Continual Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.715) |  | 0 |  | Xiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong Bao, Rui Zheng, Qi Zhang, Tao Gui, Xuanjing Huang |  |
| 849 |  |  [Attention-Enhancing Backdoor Attacks Against BERT-based Models](https://doi.org/10.18653/v1/2023.findings-emnlp.716) |  | 0 |  | Weimin Lyu, Songzhu Zheng, Lu Pang, Haibin Ling, Chao Chen |  |
| 850 |  |  [Hi-ToM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.717) |  | 0 |  | Yufan Wu, Yinghui He, Yilin Jia, Rada Mihalcea, Yulong Chen, Naihao Deng |  |
| 851 |  |  [Image and Text: Fighting the same Battle? Super Resolution Learning for Imbalanced Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.718) |  | 0 |  | Romain Meunier, Farah Benamara, Véronique Moriceau, Patricia Stolf |  |
| 852 |  |  [SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank](https://doi.org/10.18653/v1/2023.findings-emnlp.719) |  | 0 |  | Dheeraj Mekala, Adithya Samavedhi, Chengyu Dong, Jingbo Shang |  |
| 853 |  |  [Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.720) |  | 0 |  | Qiang Zhang, Jason Naradowsky, Yusuke Miyao |  |
| 854 |  |  [A Structure-Aware Generative Adversarial Network for Bilingual Lexicon Induction](https://doi.org/10.18653/v1/2023.findings-emnlp.721) |  | 0 |  | Bocheng Han, Qian Tao, Lusi Li, Zhihao Xiong |  |
| 855 |  |  [NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark](https://doi.org/10.18653/v1/2023.findings-emnlp.722) |  | 0 |  | Oscar Sainz, Jon Ander Campos, Iker GarcíaFerrero, Julen Etxaniz, Oier Lopez de Lacalle, Eneko Agirre |  |
| 856 |  |  [Improving Pacing in Long-Form Story Planning](https://doi.org/10.18653/v1/2023.findings-emnlp.723) |  | 0 |  | Yichen Wang, Kevin Yang, Xiaoming Liu, Dan Klein |  |
| 857 |  |  [Argument mining as a multi-hop generative machine reading comprehension task](https://doi.org/10.18653/v1/2023.findings-emnlp.724) |  | 0 |  | Boyang Liu, Viktor Schlegel, Riza BatistaNavarro, Sophia Ananiadou |  |
| 858 |  |  [HuatuoGPT, Towards Taming Language Model to Be a Doctor](https://doi.org/10.18653/v1/2023.findings-emnlp.725) |  | 0 |  | Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Guiming Chen, Jianquan Li, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, Haizhou Li |  |
| 859 |  |  [Debias NLU Datasets via Training-free Perturbations](https://doi.org/10.18653/v1/2023.findings-emnlp.726) |  | 0 |  | Qi Guo, Yuanhang Tang, Yawen Ouyang, Zhen Wu, Xinyu Dai |  |
| 860 |  |  [Aspect-to-Scope Oriented Multi-view Contrastive Learning for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.727) |  | 0 |  | Heyan Chai, Ziyi Yao, Siyu Tang, Ye Wang, Liqiang Nie, Binxing Fang, Qing Liao |  |
| 861 |  |  [Robustness of Named-Entity Replacements for In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.728) |  | 0 |  | Saeed Goodarzi, Nikhil Kagita, Dennis Minn, Shufan Wang, Roberto Dessì, Shubham Toshniwal, Adina Williams, Jack Lanchantin, Koustuv Sinha |  |
| 862 |  |  [Contrastive Learning-based Sentence Encoders Implicitly Weight Informative Words](https://doi.org/10.18653/v1/2023.findings-emnlp.729) |  | 0 |  | Hiroto Kurita, Goro Kobayashi, Sho Yokoi, Kentaro Inui |  |
| 863 |  |  [Legally Enforceable Hate Speech Detection for Public Forums](https://doi.org/10.18653/v1/2023.findings-emnlp.730) |  | 0 |  | Chu Fei Luo, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu |  |
| 864 |  |  [ConPrompt: Pre-training a Language Model with Machine-Generated Data for Implicit Hate Speech Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.731) |  | 0 |  | Youngwook Kim, Shinwoo Park, Youngsoo Namgoong, YoSub Han |  |
| 865 |  |  [Incorporating Syntactic Knowledge into Pre-trained Language Model using Optimization for Overcoming Catastrophic Forgetting](https://doi.org/10.18653/v1/2023.findings-emnlp.732) |  | 0 |  | Ran Iwamoto, Issei Yoshida, Hiroshi Kanayama, Takuya Ohko, Masayasu Muraoka |  |
| 866 |  |  [Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?](https://doi.org/10.18653/v1/2023.findings-emnlp.733) |  | 0 |  | Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, Luke Zettlemoyer |  |
| 867 |  |  [Chain-of-Thought Reasoning in Tabular Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.734) |  | 0 |  | Mingyu Zheng, Hao Yang, Wenbin Jiang, Zheng Lin, Yajuan Lyu, Qiaoqiao She, Weiping Wang |  |
| 868 |  |  [Diffusion Language Model with Query-Document Relevance for Query-Focused Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.735) |  | 0 |  | Shaoyao Huang, Luozheng Qin, Ziqiang Cao |  |
| 869 |  |  [Grounded and well-rounded: a methodological approach to the study of cross-modal and cross-lingual grounding](https://doi.org/10.18653/v1/2023.findings-emnlp.736) |  | 0 |  | Timothee Mickus, Elaine Zosa, Denis Paperno |  |
| 870 |  |  [EMO-KNOW: A Large Scale Dataset on Emotion-Cause](https://doi.org/10.18653/v1/2023.findings-emnlp.737) |  | 0 |  | Mia Huong Nguyen, Yasith Samaradivakara, Prasanth Sasikumar, Chitralekha Gupta, Suranga Nanayakkara |  |
| 871 |  |  [Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.738) |  | 0 |  | Weize Chen, Xiaoyue Xu, Xu Han, Yankai Lin, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou |  |
| 872 |  |  [Natural Response Generation for Chinese Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.739) |  | 0 |  | Nuo Chen, Hongguang Li, Yinan Bao, Baoyuan Wang, Jia Li |  |
| 873 |  |  [Treepiece: Faster Semantic Parsing via Tree Tokenization](https://doi.org/10.18653/v1/2023.findings-emnlp.740) |  | 0 |  | Sid Wang, Akshat Shrivastava, Aleksandr Livshits |  |
| 874 |  |  [Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking](https://doi.org/10.18653/v1/2023.findings-emnlp.741) |  | 0 |  | Yuxiang Wu, Guanting Dong, Weiran Xu |  |
| 875 |  |  [Mitigating Framing Bias with Polarity Minimization Loss](https://doi.org/10.18653/v1/2023.findings-emnlp.742) |  | 0 |  | Yejin Bang, Nayeon Lee, Pascale Fung |  |
| 876 |  |  [Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation](https://doi.org/10.18653/v1/2023.findings-emnlp.743) |  | 0 |  | Jinglong Gao, Xiao Ding, Bing Qin, Ting Liu |  |
| 877 |  |  [Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.744) |  | 0 |  | Duarte M. Alves, Nuno Miguel Guerreiro, João Alves, José Pombal, Ricardo Rei, José Guilherme Camargo de Souza, Pierre Colombo, André F. T. Martins |  |
| 878 |  |  [How Many Demonstrations Do You Need for In-context Learning?](https://doi.org/10.18653/v1/2023.findings-emnlp.745) |  | 0 |  | Jiuhai Chen, Lichang Chen, Chen Zhu, Tianyi Zhou |  |
| 879 |  |  [Improving word mover's distance by leveraging self-attention matrix](https://doi.org/10.18653/v1/2023.findings-emnlp.746) |  | 0 |  | Hiroaki Yamagiwa, Sho Yokoi, Hidetoshi Shimodaira |  |
| 880 |  |  [Improving Span Representation by Efficient Span-Level Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.747) |  | 0 |  | Pengyu Ji, Songlin Yang, Kewei Tu |  |
| 881 |  |  [Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.748) |  | 0 |  | Simon Stepputtis, Joseph Campbell, Yaqi Xie, Zhengyang Qi, Wenxin Sharon Zhang, Ruiyi Wang, Sanketh Rangreji, Charles Lewis, Katia P. Sycara |  |
| 882 |  |  [Improving Sequential Model Editing with Fact Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.749) |  | 0 |  | Xiaoqi Han, Ru Li, Hongye Tan, Yuanlong Wang, Qinghua Chai, Jeff Z. Pan |  |
| 883 |  |  [Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT - A Text-to-SQL Parsing Comparison](https://doi.org/10.18653/v1/2023.findings-emnlp.750) |  | 0 |  | Shuo Sun, Yuchen Zhang, Jiahuan Yan, Yuze Gao, Donovan Ong, Bin Chen, Jian Su |  |
| 884 |  |  [KBioXLM: A Knowledge-anchored Biomedical Multilingual Pretrained Language Model](https://doi.org/10.18653/v1/2023.findings-emnlp.751) |  | 0 |  | Lei Geng, Xu Yan, Ziqiang Cao, Juntao Li, Wenjie Li, Sujian Li, Xinjie Zhou, Yang Yang, Jun Zhang |  |
| 885 |  |  [Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?](https://doi.org/10.18653/v1/2023.findings-emnlp.752) |  | 0 |  | Sathvik Nair, Philip Resnik |  |
| 886 |  |  [A Zero-Shot Language Agent for Computer Control with Structured Reflection](https://doi.org/10.18653/v1/2023.findings-emnlp.753) |  | 0 |  | Tao Li, Gang Li, Zhiwei Deng, Bryan Wang, Yang Li |  |
| 887 |  |  [SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF](https://doi.org/10.18653/v1/2023.findings-emnlp.754) |  | 0 |  | Yi Dong, Zhilin Wang, Makesh Narsimhan Sreedhar, Xianchao Wu, Oleksii Kuchaiev |  |
| 888 |  |  [IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.755) |  | 0 |  | Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad A. Ayyubi, KaiWei Chang, ShihFu Chang |  |
| 889 |  |  [GRI: Graph-based Relative Isomorphism of Word Embedding Spaces](https://doi.org/10.18653/v1/2023.findings-emnlp.756) |  | 0 |  | Muhammad Asif Ali, Yan Hu, Jianbin Qin, Di Wang |  |
| 890 |  |  [PersonaLM: Language Model Personalization via Domain-distributed Span Aggregated K-Nearest N-gram Retrieval Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.757) |  | 0 |  | Puneet Mathur, Zhe Liu, Ke Li, Yingyi Ma, Gil Keren, Zeeshan Ahmed, Dinesh Manocha, Xuedong Zhang |  |
| 891 |  |  [Scaling Vision-Language Models with Sparse Mixture of Experts](https://doi.org/10.18653/v1/2023.findings-emnlp.758) |  | 0 |  | Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He |  |
| 892 |  |  [Aspect-Category Enhanced Learning with a Neural Coherence Model for Implicit Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.759) |  | 0 |  | Jin Cui, Fumiyo Fukumoto, Xinfeng Wang, Yoshimi Suzuki, Jiyi Li, Wanzeng Kong |  |
| 893 |  |  [End-to-end Adversarial Sample Generation for Data Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.760) |  | 0 |  | Tianyuan Liu, Yuqing Sun |  |
| 894 |  |  [Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries over Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.761) |  | 0 |  | Yao Xu, Shizhu He, Cunguang Wang, Li Cai, Kang Liu, Jun Zhao |  |
| 895 |  |  [Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement](https://doi.org/10.18653/v1/2023.findings-emnlp.762) |  | 0 |  | Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Jia Liu, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 896 |  |  [Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection](https://doi.org/10.18653/v1/2023.findings-emnlp.763) |  | 0 |  | Jianwei Li, Weizhi Gao, Qi Lei, Dongkuan Xu |  |
| 897 |  |  [Eyes Show the Way: Modelling Gaze Behaviour for Hallucination Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.764) |  | 0 |  | Kishan Maharaj, Ashita Saxena, Raja Kumar, Abhijit Mishra, Pushpak Bhattacharyya |  |
| 898 |  |  [Noisy Pair Corrector for Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.765) |  | 0 |  | Hang Zhang, Yeyun Gong, Xingwei He, Dayiheng Liu, Daya Guo, Jiancheng Lv, Jian Guo |  |
| 899 |  |  [Enhancing Accessible Communication: from European Portuguese to Portuguese Sign Language](https://doi.org/10.18653/v1/2023.findings-emnlp.766) |  | 0 |  | Catarina Sousa, Luísa Coheur, Mara Moita |  |
| 900 |  |  [Diversifying language models for lesser-studied languages and language-usage contexts: A case of second language Korean](https://doi.org/10.18653/v1/2023.findings-emnlp.767) |  | 0 |  | Hakyung Sung, GyuHo Shin |  |
| 901 |  |  [Improving generalization in large langue model by learning prefix subspaces](https://doi.org/10.18653/v1/2023.findings-emnlp.768) |  | 0 |  | Louis Falissard, Vincent Guigue, Laure Soulier |  |
| 902 |  |  [Domain Adaptation for Sentiment Analysis Using Robust Internal Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.769) |  | 0 |  | Mohammad Rostami, Digbalay Bose, Shrikanth Narayanan, Aram Galstyan |  |
| 903 |  |  [KeFVP: Knowledge-enhanced Financial Volatility Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.770) |  | 0 |  | Hao Niu, Yun Xiong, Xiaosu Wang, Wenjing Yu, Yao Zhang, Weizu Yang |  |
| 904 |  |  [A Frustratingly Easy Plug-and-Play Detection-and-Reasoning Module for Chinese Spelling Check](https://doi.org/10.18653/v1/2023.findings-emnlp.771) |  | 0 |  | Haojing Huang, Jingheng Ye, Qingyu Zhou, Yinghui Li, Yangning Li, Feng Zhou, HaiTao Zheng |  |
| 905 |  |  [Asking Clarification Questions to Handle Ambiguity in Open-Domain QA](https://doi.org/10.18653/v1/2023.findings-emnlp.772) |  | 0 |  | Dongryeol Lee, Segwang Kim, Minwoo Lee, Hwanhee Lee, Joonsuk Park, SangWoo Lee, Kyomin Jung |  |
| 906 |  |  [Addressing the Length Bias Challenge in Document-Level Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.773) |  | 0 |  | Zhuocheng Zhang, Shuhao Gu, Min Zhang, Yang Feng |  |
| 907 |  |  [EconBERTa: Towards Robust Extraction of Named Entities in Economics](https://doi.org/10.18653/v1/2023.findings-emnlp.774) |  | 0 |  | Karim Lasri, Pedro Vitor Quinta de Castro, Mona Schirmer, Luis Eduardo San Martin, Linxi Wang, Tomás Dulka, Haaya Naushan, John PouguéBiyong, Arianna Legovini, Samuel Fraiberger |  |
| 908 |  |  [Consonant is all you need: a compact representation of English text for efficient NLP](https://doi.org/10.18653/v1/2023.findings-emnlp.775) |  | 0 |  | Maged Saeed AlShaibani, Irfan Ahmad |  |
| 909 |  |  [Detrimental Contexts in Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.776) |  | 0 |  | Philhoon Oh, James Thorne |  |
| 910 |  |  [PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for Languages in India](https://doi.org/10.18653/v1/2023.findings-emnlp.777) |  | 0 |  | Ashok Urlana, Pinzhen Chen, Zheng Zhao, Shay B. Cohen, Manish Shrivastava, Barry Haddow |  |
| 911 |  |  [Beyond Labels: Empowering Human Annotators with Natural Language Explanations through a Novel Active-Learning Architecture](https://doi.org/10.18653/v1/2023.findings-emnlp.778) |  | 0 |  | Bingsheng Yao, Ishan Jindal, Lucian Popa, Yannis Katsis, Sayan Ghosh, Lihong He, Yuxuan Lu, Shashank Srivastava, Yunyao Li, James A. Hendler, Dakuo Wang |  |
| 912 |  |  [Decoding Stumpers: Large Language Models vs. Human Problem-Solvers](https://doi.org/10.18653/v1/2023.findings-emnlp.779) |  | 0 |  | Alon Goldstein, Miriam Havin, Roi Reichart, Ariel Goldstein |  |
| 913 |  |  [Efficient Cross-Task Prompt Tuning for Few-Shot Conversational Emotion Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.780) |  | 0 |  | Yige Xu, Zhiwei Zeng, Zhiqi Shen |  |
| 914 |  |  [SYMPTOMIFY: Transforming Symptom Annotations with Language Model Knowledge Harvesting](https://doi.org/10.18653/v1/2023.findings-emnlp.781) |  | 0 |  | Bosung Kim, Ndapa Nakashole |  |
| 915 |  |  [TokenDrop + BucketSampler: Towards Efficient Padding-free Fine-tuning of Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.782) |  | 0 |  | Amrit Nagarajan, Anand Raghunathan |  |
| 916 |  |  [Unified Representation for Non-compositional and Compositional Expressions](https://doi.org/10.18653/v1/2023.findings-emnlp.783) |  | 0 |  | Ziheng Zeng, Suma Bhat |  |
| 917 |  |  [Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.784) |  | 0 |  | Kosuke Akimoto, Kunihiro Takeoka, Masafumi Oyamada |  |
| 918 |  |  [Error Detection for Text-to-SQL Semantic Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.785) |  | 0 |  | Shijie Chen, Ziru Chen, Huan Sun, Yu Su |  |
| 919 |  |  [Ultra-Fine Entity Typing with Prior Knowledge about Labels: A Simple Clustering Based Strategy](https://doi.org/10.18653/v1/2023.findings-emnlp.786) |  | 0 |  | Na Li, Zied Bouraoui, Steven Schockaert |  |
| 920 |  |  [Multilingual Coarse Political Stance Classification of Media. The Editorial Line of a ChatGPT and Bard Newspaper](https://doi.org/10.18653/v1/2023.findings-emnlp.787) |  | 0 |  | Cristina EspañaBonet |  |
| 921 |  |  [Do "English" Named Entity Recognizers Work Well on Global Englishes?](https://doi.org/10.18653/v1/2023.findings-emnlp.788) |  | 0 |  | Alexander Shan, John Bauer, Riley Carlson, Christopher D. Manning |  |
| 922 |  |  [Affective and Dynamic Beam Search for Story Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.789) |  | 0 |  | Tenghao Huang, Ehsan Qasemi, Bangzheng Li, He Wang, Faeze Brahman, Muhao Chen, Snigdha Chaturvedi |  |
| 923 |  |  [Multiview Clickbait Detection via Jointly Modeling Subjective and Objective Preference](https://doi.org/10.18653/v1/2023.findings-emnlp.790) |  | 0 |  | Chongyang Shi, Yijun Yin, Qi Zhang, Liang Xiao, Usman Naseem, Shoujin Wang, Liang Hu |  |
| 924 |  |  [Let's Synthesize Step by Step: Iterative Dataset Synthesis with Large Language Models by Extrapolating Errors from Small Models](https://doi.org/10.18653/v1/2023.findings-emnlp.791) |  | 0 |  | Ruida Wang, Wangchunshu Zhou, Mrinmaya Sachan |  |
| 925 |  |  [Identifying Early Maladaptive Schemas from Mental Health Question Texts](https://doi.org/10.18653/v1/2023.findings-emnlp.792) |  | 0 |  | Sujatha Das Gollapalli, Beng Heng Ang, SeeKiong Ng |  |
| 926 |  |  [Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning](https://doi.org/10.18653/v1/2023.findings-emnlp.793) |  | 0 |  | Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, DeAn Huang, Linxi Fan, Zhiding Yu, Shiyi Lan, Bo Li, Mohammad Shoeybi, MingYu Liu, Yuke Zhu, Bryan Catanzaro, Chaowei Xiao, Anima Anandkumar |  |
| 927 |  |  [Syntax Matters: Towards Spoken Language Understanding via Syntax-Aware Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.794) |  | 0 |  | Yifeng Xie, Zhihong Zhu, Xuxin Cheng, Zhiqi Huang, Dongsheng Chen |  |
| 928 |  |  [Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate](https://doi.org/10.18653/v1/2023.findings-emnlp.795) |  | 0 |  | Boshi Wang, Xiang Yue, Huan Sun |  |
| 929 |  |  [Using In-Context Learning to Improve Dialogue Safety](https://doi.org/10.18653/v1/2023.findings-emnlp.796) |  | 0 |  | Nicholas Meade, Spandana Gella, Devamanyu Hazarika, Prakhar Gupta, Di Jin, Siva Reddy, Yang Liu, Dilek HakkaniTur |  |
| 930 |  |  [HEAR: Hearing Enhanced Audio Response for Video-grounded Dialogue](https://doi.org/10.18653/v1/2023.findings-emnlp.797) |  | 0 |  | Sunjae Yoon, Dahyun Kim, Eunseop Yoon, Hee Suk Yoon, Junyeong Kim, Chang Dong Yoo |  |
| 931 |  |  [Improving Consistency for Text Summarization with Energy Functions](https://doi.org/10.18653/v1/2023.findings-emnlp.798) |  | 0 |  | Qi Zeng, Qingyu Yin, Zheng Li, Yifan Gao, Sreyashi Nag, Zhengyang Wang, Bing Yin, Heng Ji, Chao Zhang |  |
| 932 |  |  [Defining a New NLP Playground](https://doi.org/10.18653/v1/2023.findings-emnlp.799) |  | 0 |  | Sha Li, Chi Han, Pengfei Yu, Carl Edwards, Manling Li, Xingyao Wang, Yi Ren Fung, Charles Yu, Joel R. Tetreault, Eduard H. Hovy, Heng Ji |  |
| 933 |  |  [UPTON: Preventing Authorship Leakage from Public Text Release via Data Poisoning](https://doi.org/10.18653/v1/2023.findings-emnlp.800) |  | 0 |  | Ziyao Wang, Thai Le, Dongwon Lee |  |
| 934 |  |  [IAEval: A Comprehensive Evaluation of Instance Attribution on Natural Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.801) |  | 0 |  | Peijian Gu, Yaozong Shen, Lijie Wang, Quan Wang, Hua Wu, Zhendong Mao |  |
| 935 |  |  [Scene Graph Enhanced Pseudo-Labeling for Referring Expression Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.802) |  | 0 |  | Cantao Wu, Yi Cai, Liuwu Li, Jiexin Wang |  |
| 936 |  |  [Noisy Self-Training with Synthetic Queries for Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.803) |  | 0 |  | Fan Jiang, Tom Drummond, Trevor Cohn |  |
| 937 |  |  [Leveraging GPT-4 for Automatic Translation Post-Editing](https://doi.org/10.18653/v1/2023.findings-emnlp.804) |  | 0 |  | Vikas Raunak, Amr Sharaf, Yiren Wang, Hany Hassan Awadalla, Arul Menezes |  |
| 938 |  |  [Uniform Complexity for Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.805) |  | 0 |  | Joseph Marvin Imperial, Harish Tayyar Madabushi |  |
| 939 |  |  [Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.806) |  | 0 |  | Hongru Wang, Rui Wang, Fei Mi, Yang Deng, Zezhong Wang, Bin Liang, Ruifeng Xu, KamFai Wong |  |
| 940 |  |  [CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.807) |  | 0 |  | Rajdeep Mukherjee, Nithish Kannen, Saurabh Kumar Pandey, Pawan Goyal |  |
| 941 |  |  [Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompts](https://doi.org/10.18653/v1/2023.findings-emnlp.808) |  | 0 |  | Gangwei Jiang, Caigao Jiang, Siqiao Xue, James Zhang, Jun Zhou, Defu Lian, Ying Wei |  |
| 942 |  |  [Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts](https://doi.org/10.18653/v1/2023.findings-emnlp.809) |  | 0 |  | Deepanway Ghosal, Navonil Majumder, Roy KaWei Lee, Rada Mihalcea, Soujanya Poria |  |
| 943 |  |  [XLS-R fine-tuning on noisy word boundaries for unsupervised speech segmentation into words](https://doi.org/10.18653/v1/2023.findings-emnlp.810) |  | 0 |  | Robin Algayres, Pablo DiegoSimon, Benoît Sagot, Emmanuel Dupoux |  |
| 944 |  |  [Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data](https://doi.org/10.18653/v1/2023.findings-emnlp.811) |  | 0 |  | Kashun Shum, Shizhe Diao, Tong Zhang |  |
| 945 |  |  [What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations](https://doi.org/10.18653/v1/2023.findings-emnlp.812) |  | 0 |  | Kavel Rao, Liwei Jiang, Valentina Pyatkin, Yuling Gu, Niket Tandon, Nouha Dziri, Faeze Brahman, Yejin Choi |  |
| 946 |  |  [An Empirical Study on Multiple Knowledge from ChatGPT for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2023.findings-emnlp.813) |  | 0 |  | Geng Tu, Bin Liang, Bing Qin, KamFai Wong, Ruifeng Xu |  |
| 947 |  |  [Exploiting Contrastive Learning and Numerical Evidence for Confusing Legal Judgment Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.814) |  | 0 |  | Leilei Gan, Baokui Li, Kun Kuang, Yating Zhang, Lei Wang, Anh Tuan Luu, Yi Yang, Fei Wu |  |
| 948 |  |  [One For All & All For One: Bypassing Hyperparameter Tuning with Model Averaging for Cross-Lingual Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.815) |  | 0 |  | Fabian David Schmidt, Ivan Vulic, Goran Glavas |  |
| 949 |  |  [Dimensions of Online Conflict: Towards Modeling Agonism](https://doi.org/10.18653/v1/2023.findings-emnlp.816) |  | 0 |  | Matt Canute, Mali Jin, hannah holtzclaw, Alberto Lusoli, Philippa R. Adams, Mugdha Pandya, Maite Taboada, Diana Maynard, Wendy Hui Kyong Chun |  |
| 950 |  |  [Learning under Label Proportions for Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.817) |  | 0 |  | Jatin Chauhan, Xiaoxuan Wang, Wei Wang |  |
| 951 |  |  [MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition](https://doi.org/10.18653/v1/2023.findings-emnlp.818) |  | 0 |  | Guangyue Xu, Parisa Kordjamshidi, Joyce Chai |  |
| 952 |  |  [PR-MCS: Perturbation Robust Metric for MultiLingual Image Captioning](https://doi.org/10.18653/v1/2023.findings-emnlp.819) |  | 0 |  | Yongil Kim, Yerin Hwang, Hyeongu Yun, Seunghyun Yoon, Trung Bui, Kyomin Jung |  |
| 953 |  |  [Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.820) |  | 0 |  | Yu Zhang, Hao Cheng, Zhihong Shen, Xiaodong Liu, YeYi Wang, Jianfeng Gao |  |
| 954 |  |  [BLM-s/lE: A structured dataset of English spray-load verb alternations for testing generalization in LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.821) |  | 0 |  | Giuseppe Samo, Vivi Nastase, Chunyang Jiang, Paola Merlo |  |
| 955 |  |  [Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt](https://doi.org/10.18653/v1/2023.findings-emnlp.822) |  | 0 |  | Seonghyeon Ye, Joel Jang, Doyoung Kim, Yongrae Jo, Minjoon Seo |  |
| 956 |  |  [Geographical Erasure in Language Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.823) |  | 0 |  | Pola Schwöbel, Jacek Golebiowski, Michele Donini, Cédric Archambeau, Danish Pruthi |  |
| 957 |  |  [Can Foundation Models Watch, Talk and Guide You Step by Step to Make a Cake?](https://doi.org/10.18653/v1/2023.findings-emnlp.824) |  | 0 |  | Yuwei Bao, Keunwoo Peter Yu, Yichi Zhang, Shane Storks, Itamar BarYossef, Alexander De La Iglesia, Megan Su, XiaoLin Zheng, Joyce Chai |  |
| 958 |  |  [Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?](https://doi.org/10.18653/v1/2023.findings-emnlp.825) |  | 0 |  | Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q. Tran, Dani Yogatama, Donald Metzler |  |
| 959 |  |  [Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.826) |  | 0 |  | Haoyang Huang, Tianyi Tang, Dongdong Zhang, Xin Zhao, Ting Song, Yan Xia, Furu Wei |  |
| 960 |  |  [DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text](https://doi.org/10.18653/v1/2023.findings-emnlp.827) |  | 0 |  | Jinyan Su, Terry Yue Zhuo, Di Wang, Preslav Nakov |  |
| 961 |  |  [From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with Small Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.828) |  | 0 |  | Junbing Yan, Chengyu Wang, Taolin Zhang, Xiaofeng He, Jun Huang, Wei Zhang |  |
| 962 |  |  [Macedon: Minimizing Representation Coding Rate Reduction for Cross-Lingual Natural Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.829) |  | 0 |  | Haoyu Wang, Yaqing Wang, Huaxiu Yao, Jing Gao |  |
| 963 |  |  [Adversarial Robustness for Large Language NER models using Disentanglement and Word Attributions](https://doi.org/10.18653/v1/2023.findings-emnlp.830) |  | 0 |  | Xiaomeng Jin, Bhanukiran Vinzamuri, Sriram Venkatapathy, Heng Ji, Pradeep Natarajan |  |
| 964 |  |  [LLMs - the Good, the Bad or the Indispensable?: A Use Case on Legal Statute Prediction and Legal Judgment Prediction on Indian Court Cases](https://doi.org/10.18653/v1/2023.findings-emnlp.831) |  | 0 |  | Shaurya Vats, Atharva Zope, Somsubhra De, Anurag Sharma, Upal Bhattacharya, Shubham Kumar Nigam, Shouvik Kumar Guha, Koustav Rudra, Kripabandhu Ghosh |  |
| 965 |  |  [You Are What You Annotate: Towards Better Models through Annotator Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.832) |  | 0 |  | Naihao Deng, Xinliang Frederick Zhang, Siyang Liu, Winston Wu, Lu Wang, Rada Mihalcea |  |
| 966 |  |  [Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers](https://doi.org/10.18653/v1/2023.findings-emnlp.833) |  | 0 |  | Wencong You, Zayd Hammoudeh, Daniel Lowd |  |
| 967 |  |  [Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance](https://doi.org/10.18653/v1/2023.findings-emnlp.834) |  | 0 |  | Song Wang, Zhen Tan, Ruocheng Guo, Jundong Li |  |
| 968 |  |  [Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions](https://doi.org/10.18653/v1/2023.findings-emnlp.835) |  | 0 |  | Shulin Cao, Jiajie Zhang, Jiaxin Shi, Xin Lv, Zijun Yao, Qi Tian, Lei Hou, Juanzi Li |  |
| 969 |  |  [Ensemble-Instruct: Instruction Tuning Data Generation with a Heterogeneous Mixture of LMs](https://doi.org/10.18653/v1/2023.findings-emnlp.836) |  | 0 |  | YoungSuk Lee, Md. Arafat Sultan, Yousef ElKurdi, Tahira Naseem, Asim Munawar, Radu Florian, Salim Roukos, Ramón Fernandez Astudillo |  |
| 970 |  |  [The Less the Merrier? Investigating Language Representation in Multilingual Models](https://doi.org/10.18653/v1/2023.findings-emnlp.837) |  | 0 |  | Hellina Nigatu, Atnafu Lambebo Tonja, Jugal Kalita |  |
| 971 |  |  [SuperTweetEval: A Challenging, Unified and Heterogeneous Benchmark for Social Media NLP Research](https://doi.org/10.18653/v1/2023.findings-emnlp.838) |  | 0 |  | Dimosthenis Antypas, Asahi Ushio, Francesco Barbieri, Leonardo Neves, Kiamehr Rezaee, Luis Espinosa Anke, Jiaxin Pei, José CamachoCollados |  |
| 972 |  |  [Enabling Unsupervised Neural Machine Translation with Word-level Visual Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.839) |  | 0 |  | Chengpeng Fu, Xiaocheng Feng, Yichong Huang, Wenshuai Huo, Hui Wang, Bing Qin, Ting Liu |  |
| 973 |  |  [Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches](https://doi.org/10.18653/v1/2023.findings-emnlp.840) |  | 0 |  | Daniel Fried, Nicholas Tomlin, Jennifer Hu, Roma Patel, Aida Nematzadeh |  |
| 974 |  |  [MISCA: A Joint Model for Multiple Intent Detection and Slot Filling with Intent-Slot Co-Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.841) |  | 0 |  | Thinh Pham, Tran Chi, Dat Quoc Nguyen |  |
| 975 |  |  [Enhancing Emotion Recognition in Conversation via Multi-view Feature Alignment and Memorization](https://doi.org/10.18653/v1/2023.findings-emnlp.842) |  | 0 |  | Guiyang Hou, Yongliang Shen, Wenqi Zhang, Wei Xue, Weiming Lu |  |
| 976 |  |  [Mandarin classifier systems optimize to accommodate communicative pressures](https://doi.org/10.18653/v1/2023.findings-emnlp.843) |  | 0 |  | Yamei Wang, Géraldine Walther |  |
| 977 |  |  [Probing Representations for Document-level Event Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.844) |  | 0 |  | Barry Wang, Xinya Du, Claire Cardie |  |
| 978 |  |  [Cultural Compass: Predicting Transfer Learning Success in Offensive Language Detection with Cultural Features](https://doi.org/10.18653/v1/2023.findings-emnlp.845) |  | 0 |  | Li Zhou, Antonia Karamolegkou, Wenyu Chen, Daniel Hershcovich |  |
| 979 |  |  [Linguistically Motivated Sign Language Segmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.846) |  | 0 |  | Amit Moryossef, Zifan Jiang, Mathias Müller, Sarah Ebling, Yoav Goldberg |  |
| 980 |  |  [Re-weighting Tokens: A Simple and Effective Active Learning Strategy for Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.847) |  | 0 |  | Haocheng Luo, Wei Tan, Ngoc Dang Nguyen, Lan Du |  |
| 981 |  |  [Language-Agnostic Bias Detection in Language Models with Bias Probing](https://doi.org/10.18653/v1/2023.findings-emnlp.848) |  | 0 |  | Abdullatif Köksal, Omer Faruk Yalcin, Ahmet Akbiyik, M. Tahir Kilavuz, Anna Korhonen, Hinrich Schütze |  |
| 982 |  |  [CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.849) |  | 0 |  | Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang |  |
| 983 |  |  [Improving Multi-Criteria Chinese Word Segmentation through Learning Sentence Representation](https://doi.org/10.18653/v1/2023.findings-emnlp.850) |  | 0 |  | Chun Lin, YingJia Lin, ChiaJen Yeh, YiTing Li, Ching Yang, HungYu Kao |  |
| 984 |  |  [A Joint Matrix Factorization Analysis of Multilingual Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.851) |  | 0 |  | Zheng Zhao, Yftah Ziser, Bonnie Webber, Shay B. Cohen |  |
| 985 |  |  [Don't Add, don't Miss: Effective Content Preserving Generation from Pre-Selected Text Spans](https://doi.org/10.18653/v1/2023.findings-emnlp.852) |  | 0 |  | Aviv Slobodkin, Avi Caciularu, Eran Hirsch, Ido Dagan |  |
| 986 |  |  [A Computational Interface to Translate Strategic Intent from Unstructured Language in a Low-Data Setting](https://doi.org/10.18653/v1/2023.findings-emnlp.853) |  | 0 |  | Pradyumna Tambwekar, Lakshita Dodeja, Nathan Vaska, Wei Xu, Matthew C. Gombolay |  |
| 987 |  |  [HFMRE: Constructing Huffman Tree in Bags to Find Excellent Instances for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.854) |  | 0 |  | Min Li, Cong Shao, Gang Li, Mingle Zhou |  |
| 988 |  |  [DISCO: A Large Scale Human Annotated Corpus for Disfluency Correction in Indo-European Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.855) |  | 0 |  | Vineet Bhat, Preethi Jyothi, Pushpak Bhattacharyya |  |
| 989 |  |  [Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity](https://doi.org/10.18653/v1/2023.findings-emnlp.856) |  | 0 |  | Haoran Xu, Maha Elbayad, Kenton Murray, Jean Maillard, Vedanuj Goswami |  |
| 990 |  |  [Misery Loves Complexity: Exploring Linguistic Complexity in the Context of Emotion Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.857) |  | 0 |  | Pranaydeep Singh, Luna De Bruyne, Orphée De Clercq, Els Lefever |  |
| 991 |  |  [Probing the "Creativity" of Large Language Models: Can models produce divergent semantic association?](https://doi.org/10.18653/v1/2023.findings-emnlp.858) |  | 0 |  | Honghua Chen, Nai Ding |  |
| 992 |  |  [Code-Switching with Word Senses for Pretraining in Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.859) |  | 0 |  | Vivek Iyer, Edoardo Barba, Alexandra Birch, Jeff Z. Pan, Roberto Navigli |  |
| 993 |  |  [DiffusionSL: Sequence Labeling via Tag Diffusion Process](https://doi.org/10.18653/v1/2023.findings-emnlp.860) |  | 0 |  | Ziyang Huang, Pengfei Cao, Jun Zhao, Kang Liu |  |
| 994 |  |  [COMET-M: Reasoning about Multiple Events in Complex Sentences](https://doi.org/10.18653/v1/2023.findings-emnlp.861) |  | 0 |  | Sahithya Ravi, Raymond T. Ng, Vered Shwartz |  |
| 995 |  |  [On Event Individuation for Document-Level Information Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.862) |  | 0 |  | William Gantt, Reno Kriz, Yunmo Chen, Siddharth Vashishtha, Aaron Steven White |  |
| 996 |  |  [AniEE: A Dataset of Animal Experimental Literature for Event Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.863) |  | 0 |  | Dohee Kim, Ra Yoo, Soyoung Yang, Hee Yang, Jaegul Choo |  |
| 997 |  |  [From Words to Wires: Generating Functioning Electronic Devices from Natural Language Descriptions](https://doi.org/10.18653/v1/2023.findings-emnlp.864) |  | 0 |  | Peter Jansen |  |
| 998 |  |  [Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training](https://doi.org/10.18653/v1/2023.findings-emnlp.865) |  | 0 |  | Zhisong Zhang, Emma Strubell, Eduard H. Hovy |  |
| 999 |  |  [Explicit Alignment and Many-to-many Entailment Based Reasoning for Conversational Machine Reading](https://doi.org/10.18653/v1/2023.findings-emnlp.866) |  | 0 |  | Yangyang Luo, Shiyu Tian, Caixia Yuan, Xiaojie Wang |  |
| 1000 |  |  [Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers](https://doi.org/10.18653/v1/2023.findings-emnlp.867) |  | 0 |  | Osman Batur Ince, Tanin Zeraati, Semih Yagcioglu, Yadollah Yaghoobzadeh, Erkut Erdem, Aykut Erdem |  |
| 1001 |  |  [Roles of Scaling and Instruction Tuning in Language Perception: Model vs. Human Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.868) |  | 0 |  | Changjiang Gao, Shujian Huang, Jixing Li, Jiajun Chen |  |
| 1002 |  |  [Efficient Data Learning for Open Information Extraction with Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.869) |  | 0 |  | Zhiyuan Fan, Shizhu He |  |
| 1003 |  |  [Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning](https://doi.org/10.18653/v1/2023.findings-emnlp.870) |  | 0 |  | Han Zhou, Xingchen Wan, Ivan Vulic, Anna Korhonen |  |
| 1004 |  |  [Towards Zero-shot Learning for End-to-end Cross-modal Translation Models](https://doi.org/10.18653/v1/2023.findings-emnlp.871) |  | 0 |  | Jichen Yang, Kai Fan, Minpeng Liao, Boxing Chen, Zhongqiang Huang |  |
| 1005 |  |  [LLMaAA: Making Large Language Models as Active Annotators](https://doi.org/10.18653/v1/2023.findings-emnlp.872) |  | 0 |  | Ruoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, Lei Zou |  |
| 1006 |  |  [NLMs: Augmenting Negation in Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.873) |  | 0 |  | Rituraj Singh, Rahul Kumar, Vivek Sridhar |  |
| 1007 |  |  [Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated Neural Text Retrievers](https://doi.org/10.18653/v1/2023.findings-emnlp.874) |  | 0 |  | Weng Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Jiahua Liu, Tao Li, Yuxiao Dong, Jie Tang |  |
| 1008 |  |  [X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity](https://doi.org/10.18653/v1/2023.findings-emnlp.875) |  | 0 |  | Taejun Yun, Jinhyeon Kim, Deokyeong Kang, Seong Hoon Lim, Jihoon Kim, Taeuk Kim |  |
| 1009 |  |  [Noise-Robust Semi-Supervised Learning for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.876) |  | 0 |  | Xin Sun, Qiang Liu, Shu Wu, Zilei Wang, Liang Wang |  |
| 1010 |  |  [Towards Concept-Aware Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.877) |  | 0 |  | Chen Shani, Jilles Vreeken, Dafna Shahaf |  |
| 1011 |  |  [ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.878) |  | 0 |  | Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, Thien Huu Nguyen |  |
| 1012 |  |  [Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training](https://doi.org/10.18653/v1/2023.findings-emnlp.879) |  | 0 |  | Max MüllerEberstein, Rob van der Goot, Barbara Plank, Ivan Titov |  |
| 1013 |  |  [Not All Demonstration Examples are Equally Beneficial: Reweighting Demonstration Examples for In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.880) |  | 0 |  | Zhe Yang, Damai Dai, Peiyi Wang, Zhifang Sui |  |
| 1014 |  |  [Difference-Masking: Choosing What to Mask in Continued Pretraining](https://doi.org/10.18653/v1/2023.findings-emnlp.881) |  | 0 |  | Alex Wilf, Syeda Nahida Akter, Leena Mathur, Paul Pu Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, LouisPhilippe Morency |  |
| 1015 |  |  [Learn From One Specialized Sub-Teacher: One-to-One Mapping for Feature-Based Knowledge Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.882) |  | 0 |  | Khouloud Saadi, Jelena Mitrovic, Michael Granitzer |  |
| 1016 |  |  [IMU2CLIP: Language-grounded Motion Sensor Translation with Multimodal Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.883) |  | 0 |  | Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Aparajita Saraf, Amy Bearman, Babak Damavandi |  |
| 1017 |  |  [Conditioning on Dialog Acts improves Empathy Style Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.884) |  | 0 |  | Renyi Qu, Lyle H. Ungar, João Sedoc |  |
| 1018 |  |  [Systematic Assessment of Factual Knowledge in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.885) |  | 0 |  | Linhao Luo, ThuyTrang Vu, Dinh Q. Phung, Gholamreza Haffari |  |
| 1019 |  |  [From Speculation Detection to Trustworthy Relational Tuples in Information Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.886) |  | 0 |  | Kuicai Dong, Aixin Sun, JungJae Kim, Xiaoli Li |  |
| 1020 |  |  [Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.887) |  | 0 |  | Kaiser Sun, Peng Qi, Yuhao Zhang, Lan Liu, William Yang Wang, Zhiheng Huang |  |
| 1021 |  |  [Dialogue Medical Information Extraction with Medical-Item Graph and Dialogue-Status Enriched Representation](https://doi.org/10.18653/v1/2023.findings-emnlp.888) |  | 0 |  | Lei Gao, Xinnan Zhang, Xian Wu, Shen Ge, Yefeng Zheng |  |
| 1022 |  |  [LogicAttack: Adversarial Attacks for Evaluating Logical Consistency of Natural Language Inference](https://doi.org/10.18653/v1/2023.findings-emnlp.889) |  | 0 |  | Mutsumi Nakamura, Santosh Mashetty, Mihir Parmar, Neeraj Varshney, Chitta Baral |  |
| 1023 |  |  [Decomposed Prompt Tuning via Low-Rank Reparameterization](https://doi.org/10.18653/v1/2023.findings-emnlp.890) |  | 0 |  | Yao Xiao, Lu Xu, Jiaxi Li, Wei Lu, Xiaoli Li |  |
| 1024 |  |  [SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.891) |  | 0 |  | Xiaoying Zhang, Baolin Peng, Kun Li, Jingyan Zhou, Helen Meng |  |
| 1025 |  |  [Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.892) |  | 0 |  | Abhinav Rao, Aditi Khandelwal, Kumar Tanmay, Utkarsh Agarwal, Monojit Choudhury |  |
| 1026 |  |  [Vector-Quantized Prompt Learning for Paraphrase Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.893) |  | 0 |  | Haotian Luo, Yixin Liu, Peidong Liu, Xianggen Liu |  |
| 1027 |  |  [Rethinking the Construction of Effective Metrics for Understanding the Mechanisms of Pretrained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.894) |  | 0 |  | You Li, Jinhui Yin, Yuming Lin |  |
| 1028 |  |  [PARROT: Zero-Shot Narrative Reading Comprehension via Parallel Reading](https://doi.org/10.18653/v1/2023.findings-emnlp.895) |  | 0 |  | Chao Zhao, Anvesh Rao Vijjini, Snigdha Chaturvedi |  |
| 1029 |  |  [BioDEX: Large-Scale Biomedical Adverse Drug Event Extraction for Real-World Pharmacovigilance](https://doi.org/10.18653/v1/2023.findings-emnlp.896) |  | 0 |  | Karel D'Oosterlinck, François Remy, Johannes Deleu, Thomas Demeester, Chris Develder, Klim Zaporojets, Aneiss Ghodsi, Simon Ellershaw, Jack Collins, Christopher Potts |  |
| 1030 |  |  [Coarse-to-Fine Dual Encoders are Better Frame Identification Learners](https://doi.org/10.18653/v1/2023.findings-emnlp.897) |  | 0 |  | Kaikai An, Ce Zheng, Bofei Gao, Haozhe Zhao, Baobao Chang |  |
| 1031 |  |  [Sound of Story: Multi-modal Storytelling with Audio](https://doi.org/10.18653/v1/2023.findings-emnlp.898) |  | 0 |  | Jaeyeon Bae, Seokhoon Jeong, Seokun Kang, Namgi Han, JaeYon Lee, Hyounghun Kim, Taehwan Kim |  |
| 1032 |  |  [Synthesize, if you do not have: Effective Synthetic Dataset Creation Strategies for Self-Supervised Opinion Summarization in E-commerce](https://doi.org/10.18653/v1/2023.findings-emnlp.899) |  | 0 |  | Tejpalsingh Siledar, Suman Banerjee, Amey Patil, Sudhanshu Singh, Muthusamy Chelliah, Nikesh Garera, Pushpak Bhattacharyya |  |
| 1033 |  |  [Leveraging Contrastive Learning and Knowledge Distillation for Incomplete Modality Rumor Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.900) |  | 0 |  | Fan Xu, Pinyun Fu, Qi Huang, Bowei Zou, AiTi Aw, Mingwen Wang |  |
| 1034 |  |  [Beyond Testers' Biases: Guiding Model Testing with Knowledge Bases using LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.901) |  | 0 |  | Chenyang Yang, Rishabh Rustogi, Rachel A. BrowerSinning, Grace A. Lewis, Christian Kästner, Tongshuang Wu |  |
| 1035 |  |  [CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.902) |  | 0 |  | Weiqi Wang, Tianqing Fang, Wenxuan Ding, Baixuan Xu, Xin Liu, Yangqiu Song, Antoine Bosselut |  |
| 1036 |  |  [kNN-CM: A Non-parametric Inference-Phase Adaptation of Parametric Text Classifiers](https://doi.org/10.18653/v1/2023.findings-emnlp.903) |  | 0 |  | Rishabh Bhardwaj, Yingting Li, Navonil Majumder, Bo Cheng, Soujanya Poria |  |
| 1037 |  |  [Cross-modality Data Augmentation for End-to-End Sign Language Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.904) |  | 0 |  | Jinhui Ye, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Hui Xiong |  |
| 1038 |  |  [Consistency is Key: On Data-Efficient Modality Transfer in Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.905) |  | 0 |  | Hojin Lee, Changmin Lee, Seungwon Hwang |  |
| 1039 |  |  [Relation-Aware Question Answering for Heterogeneous Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.906) |  | 0 |  | Haowei Du, Quzhe Huang, Chen Li, Chen Zhang, Yang Li, Dongyan Zhao |  |
| 1040 |  |  [InstOptima: Evolutionary Multi-objective Instruction Optimization via Large Language Model-based Instruction Operators](https://doi.org/10.18653/v1/2023.findings-emnlp.907) |  | 0 |  | Heng Yang, Ke Li |  |
| 1041 |  |  [Less than One-shot: Named Entity Recognition via Extremely Weak Supervision](https://doi.org/10.18653/v1/2023.findings-emnlp.908) |  | 0 |  | Letian Peng, Zihan Wang, Jingbo Shang |  |
| 1042 |  |  [Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.909) |  | 0 |  | Jungmin Yun, Mihyeon Kim, Youngbin Kim |  |
| 1043 |  |  [Semantic Decomposition of Question and SQL for Text-to-SQL Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.910) |  | 0 |  | Ben Eyal, Moran Mahabi, Ophir Haroche, Amir Bachar, Michael Elhadad |  |
| 1044 |  |  [Time-Aware Language Modeling for Historical Text Dating](https://doi.org/10.18653/v1/2023.findings-emnlp.911) |  | 0 |  | Han Ren, Hai Wang, Yajie Zhao, Yafeng Ren |  |
| 1045 |  |  [A Read-and-Select Framework for Zero-shot Entity Linking](https://doi.org/10.18653/v1/2023.findings-emnlp.912) |  | 0 |  | Zhenran Xu, Yulin Chen, Baotian Hu, Min Zhang |  |
| 1046 |  |  [Multi-Task Learning of Query Generation and Classification for Generative Conversational Question Rewriting](https://doi.org/10.18653/v1/2023.findings-emnlp.913) |  | 0 |  | Sarawoot Kongyoung, Craig MacDonald, Iadh Ounis |  |
| 1047 |  |  [DepNeCTI: Dependency-based Nested Compound Type Identification for Sanskrit](https://doi.org/10.18653/v1/2023.findings-emnlp.914) |  | 0 |  | Jivnesh Sandhan, Yaswanth Narsupalli, Sreevatsa Muppirala, Sriram Krishnan, Pavankumar Satuluri, Amba P. Kulkarni, Pawan Goyal |  |
| 1048 |  |  [HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark](https://doi.org/10.18653/v1/2023.findings-emnlp.915) |  | 0 |  | Amir David Nissan Cohen, Hilla Merhav, Yoav Goldberg, Reut Tsarfaty |  |
| 1049 |  |  [HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.916) |  | 0 |  | Nafis Irtiza Tripto, Adaku Uchendu, Thai Le, Mattia Setzu, Fosca Giannotti, Dongwon Lee |  |
| 1050 |  |  [Data Augmentation for Code Translation with Comparable Corpora and Multiple References](https://doi.org/10.18653/v1/2023.findings-emnlp.917) |  | 0 |  | Yiqing Xie, Atharva Naik, Daniel Fried, Carolyn P. Rosé |  |
| 1051 |  |  [Multilingual Generation and Answering of Questions from Texts and Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.918) |  | 0 |  | Kelvin Han, Claire Gardent |  |
| 1052 |  |  [InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.919) |  | 0 |  | Renzhi Wang, Jing Li, Piji Li |  |
| 1053 |  |  [Enhancing Scalability of Pre-trained Language Models via Efficient Parameter Sharing](https://doi.org/10.18653/v1/2023.findings-emnlp.920) |  | 0 |  | Peiyu Liu, ZeFeng Gao, Yushuo Chen, Xin Zhao, JiRong Wen |  |
| 1054 |  |  [Boosting Prompt-Based Self-Training With Mapping-Free Automatic Verbalizer for Multi-Class Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.921) |  | 0 |  | Yookyung Kho, Jaehee Kim, Pilsung Kang |  |
| 1055 |  |  [On the Impact of Cross-Domain Data on German Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.922) |  | 0 |  | Amin Dada, Aokun Chen, Cheng Peng, Kaleb E. Smith, Ahmad IdrissiYaghir, Constantin Seibold, Jianning Li, Lars Heiliger, Christoph M. Friedrich, Daniel Truhn, Jan Egger, Jiang Bian, Jens Kleesiek, Yonghui Wu |  |
| 1056 |  |  [Dialect-to-Standard Normalization: A Large-Scale Multilingual Evaluation](https://doi.org/10.18653/v1/2023.findings-emnlp.923) |  | 0 |  | Olli Kuparinen, Aleksandra Miletic, Yves Scherrer |  |
| 1057 |  |  [Re-Examining Summarization Evaluation across Multiple Quality Criteria](https://doi.org/10.18653/v1/2023.findings-emnlp.924) |  | 0 |  | Ori Ernst, Ori Shapira, Ido Dagan, Ran Levy |  |
| 1058 |  |  [A Parallel Corpus for Vietnamese Central-Northern Dialect Text Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.925) |  | 0 |  | Thang Le, Anh Tuan Luu |  |
| 1059 |  |  [A Comprehensive Evaluation of Tool-Assisted Generation Strategies](https://doi.org/10.18653/v1/2023.findings-emnlp.926) |  | 0 |  | Alon Jacovi, Avi Caciularu, Jonathan Herzig, Roee Aharoni, Bernd Bohnet, Mor Geva |  |
| 1060 |  |  [InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT](https://doi.org/10.18653/v1/2023.findings-emnlp.927) |  | 0 |  | Yichong Xu, Ruochen Xu, Dan Iter, Yang Liu, Shuohang Wang, Chenguang Zhu, Michael Zeng |  |
| 1061 |  |  [Learning to love diligent trolls: Accounting for rater effects in the dialogue safety task](https://doi.org/10.18653/v1/2023.findings-emnlp.928) |  | 0 |  | Michael John Ilagan |  |
| 1062 |  |  [Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal Scenarios Like a Lawyer?](https://doi.org/10.18653/v1/2023.findings-emnlp.929) |  | 0 |  | Xiaoxi Kang, Lizhen Qu, LayKi Soon, Adnan Trakic, Terry Yue Zhuo, Patrick Charles Emerton, Genevieve Grant |  |
| 1063 |  |  [Coverage-based Example Selection for In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.930) |  | 0 |  | Shivanshu Gupta, Matt Gardner, Sameer Singh |  |
| 1064 |  |  [Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization](https://doi.org/10.18653/v1/2023.findings-emnlp.931) |  | 0 |  | Ningyu Xu, Qi Zhang, Jingting Ye, Menghan Zhang, Xuanjing Huang |  |
| 1065 |  |  [Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing](https://doi.org/10.18653/v1/2023.findings-emnlp.932) |  | 0 |  | LucieAimée Kaffee, Arnav Arora, Zeerak Talat, Isabelle Augenstein |  |
| 1066 |  |  [BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions](https://doi.org/10.18653/v1/2023.findings-emnlp.933) |  | 0 |  | Arth Bohra, Govert Verkes, Artem Harutyunyan, Pascal Weinberger, Giovanni Campagna |  |
| 1067 |  |  [Approximating CKY with Transformers](https://doi.org/10.18653/v1/2023.findings-emnlp.934) |  | 0 |  | Ghazal Khalighinejad, Ollie Liu, Sam Wiseman |  |
| 1068 |  |  [DialGuide: Aligning Dialogue Model Behavior with Developer Guidelines](https://doi.org/10.18653/v1/2023.findings-emnlp.935) |  | 0 |  | Prakhar Gupta, Yang Liu, Di Jin, Behnam Hedayatnia, Spandana Gella, Sijia Liu, Patrick Lange, Julia Hirschberg, Dilek HakkaniTur |  |
| 1069 |  |  [RWKV: Reinventing RNNs for the Transformer Era](https://doi.org/10.18653/v1/2023.findings-emnlp.936) |  | 0 |  | Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan S. Wind, Stanislaw Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, RuiJie Zhu |  |
| 1070 |  |  [Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification](https://doi.org/10.18653/v1/2023.findings-emnlp.937) |  | 0 |  | ChiaYu Hung, Zhiqiang Hu, Yujia Hu, Roy KaWei Lee |  |
| 1071 |  |  [Transitioning Representations between Languages for Cross-lingual Event Detection via Langevin Dynamics](https://doi.org/10.18653/v1/2023.findings-emnlp.938) |  | 0 |  | Chien Nguyen, Huy Nguyen, Franck Dernoncourt, Thien Huu Nguyen |  |
| 1072 |  |  [VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers](https://doi.org/10.18653/v1/2023.findings-emnlp.939) |  | 0 |  | Shahar Katz, Yonatan Belinkov |  |
| 1073 |  |  [Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?](https://doi.org/10.18653/v1/2023.findings-emnlp.940) |  | 0 |  | Leiyu Pan, Supryadi, Deyi Xiong |  |
| 1074 |  |  [Arabic Mini-ClimateGPT : A Climate Change and Sustainability Tailored Arabic LLM](https://doi.org/10.18653/v1/2023.findings-emnlp.941) |  | 0 |  | Sahal Shaji Mullappilly, Abdelrahman M. Shaker, Omkar Thawakar, Hisham Cholakkal, Rao Muhammad Anwer, Salman H. Khan, Fahad Shahbaz Khan |  |
| 1075 |  |  [Interpreting Answers to Yes-No Questions in User-Generated Content](https://doi.org/10.18653/v1/2023.findings-emnlp.942) |  | 0 |  | Shivam Mathur, Keun Hee Park, Dhivya Chinnappa, Saketh Kotamraju, Eduardo Blanco |  |
| 1076 |  |  [Task-Aware Self-Supervised Framework for Dialogue Discourse Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.943) |  | 0 |  | Wei Li, Luyao Zhu, Wei Shao, Zonglin Yang, Erik Cambria |  |
| 1077 |  |  [Selective Demonstrations for Cross-domain Text-to-SQL](https://doi.org/10.18653/v1/2023.findings-emnlp.944) |  | 0 |  | Shuaichen Chang, Eric FoslerLussier |  |
| 1078 |  |  [DocSplit: Simple Contrastive Pretraining for Large Document Embeddings](https://doi.org/10.18653/v1/2023.findings-emnlp.945) |  | 0 |  | Yujie Wang, Mike Izbicki |  |
| 1079 |  |  [TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.946) |  | 0 |  | Shubhra Kanti Karmaker Santu, Dongji Feng |  |
| 1080 |  |  [IntenDD: A Unified Contrastive Learning Approach for Intent Detection and Discovery](https://doi.org/10.18653/v1/2023.findings-emnlp.947) |  | 0 |  | Bhavuk Singhal, Ashim Gupta, Shivasankaran V. P, Amrith Krishna |  |
| 1081 |  |  [INarIG: Iterative Non-autoregressive Instruct Generation Model For Word-Level Auto Completion](https://doi.org/10.18653/v1/2023.findings-emnlp.948) |  | 0 |  | Hengchao Shang, Zongyao Li, Daimeng Wei, Jiaxin Guo, Minghan Wang, Xiaoyu Chen, Lizhi Lei, Hao Yang |  |
| 1082 |  |  [Is the Answer in the Text? Challenging ChatGPT with Evidence Retrieval from Instructive Text](https://doi.org/10.18653/v1/2023.findings-emnlp.949) |  | 0 |  | Sophie Henning, Talita Anthonio, Wei Zhou, Heike Adel, Mohsen Mesgar, Annemarie Friedrich |  |
| 1083 |  |  [PaRaDe: Passage Ranking using Demonstrations with LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.950) |  | 0 |  | Andrew Drozdov, Honglei Zhuang, Zhuyun Dai, Zhen Qin, Razieh Rahimi, Xuanhui Wang, Dana Alon, Mohit Iyyer, Andrew McCallum, Donald Metzler, Kai Hui |  |
| 1084 |  |  [Learning Dynamic Representations for Discourse Dependency Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.951) |  | 0 |  | Tianyi Liu, Yansong Feng, Dongyan Zhao |  |
| 1085 |  |  [K-HATERS: A Hate Speech Detection Corpus in Korean with Target-Specific Ratings](https://doi.org/10.18653/v1/2023.findings-emnlp.952) |  | 0 |  | Chaewon Park, Soohwan Kim, Kyubyong Park, Kunwoo Park |  |
| 1086 |  |  [Mitigating Data Imbalance and Representation Degeneration in Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.953) |  | 0 |  | Wen Lai, Alexandra Chronopoulou, Alexander Fraser |  |
| 1087 |  |  [BotPercent: Estimating Bot Populations in Twitter Communities](https://doi.org/10.18653/v1/2023.findings-emnlp.954) |  | 0 |  | Zhaoxuan Tan, Shangbin Feng, Melanie Sclar, Herun Wan, Minnan Luo, Yejin Choi, Yulia Tsvetkov |  |
| 1088 |  |  [The Locality and Symmetry of Positional Encodings](https://doi.org/10.18653/v1/2023.findings-emnlp.955) |  | 0 |  | Lihu Chen, Gaël Varoquaux, Fabian M. Suchanek |  |
| 1089 |  |  [Towards a Deep Understanding of Multilingual End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.956) |  | 0 |  | Haoran Sun, Xiaohu Zhao, Yikun Lei, Shaolin Zhu, Deyi Xiong |  |
| 1090 |  |  [An Empirical Investigation of Implicit and Explicit Knowledge-Enhanced Methods for Ad Hoc Dataset Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.957) |  | 0 |  | Weiqing Luo, Qiaosheng Chen, Zhiyang Zhang, Zixian Huang, Gong Cheng |  |
| 1091 |  |  [A Multi-Modal Multilingual Benchmark for Document Image Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.958) |  | 0 |  | Yoshinari Fujinuma, Siddharth Varia, Nishant Sankaran, Srikar Appalaraju, Bonan Min, Yogarshi Vyas |  |
| 1092 |  |  [Unnatural language processing: How do language models handle machine-generated prompts?](https://doi.org/10.18653/v1/2023.findings-emnlp.959) |  | 0 |  | Corentin Kervadec, Francesca Franzon, Marco Baroni |  |
| 1093 |  |  [Investigating the Effectiveness of Multiple Expert Models Collaboration](https://doi.org/10.18653/v1/2023.findings-emnlp.960) |  | 0 |  | Ikumi Ito, Takumi Ito, Jun Suzuki, Kentaro Inui |  |
| 1094 |  |  [Gradually Excavating External Knowledge for Implicit Complex Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.961) |  | 0 |  | Chang Liu, Xiaoguang Li, Lifeng Shang, Xin Jiang, Qun Liu, Edmund Y. Lam, Ngai Wong |  |
| 1095 |  |  [Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.962) |  | 0 |  | Hongli Zhan, Desmond C. Ong, Junyi Jessy Li |  |
| 1096 |  |  [Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.963) |  | 0 |  | Elena Sofia Ruzzetti, Federico Ranaldi, Felicia Logozzo, Michele Mastromattei, Leonardo Ranaldi, Fabio Massimo Zanzotto |  |
| 1097 |  |  [Discourse Sense Flows: Modelling the Rhetorical Style of Documents across Various Domains](https://doi.org/10.18653/v1/2023.findings-emnlp.964) |  | 0 |  | René Knaebel, Manfred Stede |  |
| 1098 |  |  [HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework for Cross-Domain Zero-Shot Slot Filling](https://doi.org/10.18653/v1/2023.findings-emnlp.965) |  | 0 |  | Junwen Zhang, Yin Zhang |  |
| 1099 |  |  [A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing](https://doi.org/10.18653/v1/2023.findings-emnlp.966) |  | 0 |  | Carlos GómezRodríguez, Paul Williams |  |
| 1100 |  |  [1-PAGER: One Pass Answer Generation and Evidence Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.967) |  | 0 |  | Palak Jain, Livio Soares, Tom Kwiatkowski |  |
| 1101 |  |  [Context-faithful Prompting for Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.968) |  | 0 |  | Wenxuan Zhou, Sheng Zhang, Hoifung Poon, Muhao Chen |  |
| 1102 |  |  [InfoCL: Alleviating Catastrophic Forgetting in Continual Text Classification from An Information Theoretic Perspective](https://doi.org/10.18653/v1/2023.findings-emnlp.969) |  | 0 |  | Yifan Song, Peiyi Wang, Weimin Xiong, Dawei Zhu, Tianyu Liu, Zhifang Sui, Sujian Li |  |
| 1103 |  |  [Sparse Frame Grouping Network with Action Centered for Untrimmed Video Paragraph Captioning](https://doi.org/10.18653/v1/2023.findings-emnlp.970) |  | 0 |  | Guorui Yu, Yimin Hu, Yuejie Zhang, Rui Feng, Tao Zhang, Shang Gao |  |
| 1104 |  |  [Unsupervised Binary Code Translation with Application to Code Clone Detection and Vulnerability Discovery](https://doi.org/10.18653/v1/2023.findings-emnlp.971) |  | 0 |  | Iftakhar Ahmad, Lannan Luo |  |
| 1105 |  |  [Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.972) |  | 0 |  | Inderjeet Nair, Shwetha Somasundaram, Apoorv Saxena, Koustava Goswami |  |
| 1106 |  |  [Emergent Inabilities? Inverse Scaling Over the Course of Pretraining](https://doi.org/10.18653/v1/2023.findings-emnlp.973) |  | 0 |  | James A. Michaelov, Ben Bergen |  |
| 1107 |  |  [Alignment Precedes Fusion: Open-Vocabulary Named Entity Recognition as Context-Type Semantic Matching](https://doi.org/10.18653/v1/2023.findings-emnlp.974) |  | 0 |  | Zhuoran Jin, Pengfei Cao, Zhitao He, Yubo Chen, Kang Liu, Jun Zhao |  |
| 1108 |  |  [Representation Projection Invariance Mitigates Representation Collapse](https://doi.org/10.18653/v1/2023.findings-emnlp.975) |  | 0 |  | Anastasia Razdaibiedina, Ashish Khetan, Zohar Karnin, Daniel Khashabi, Vivek Madan |  |
| 1109 |  |  [Tunable Soft Prompts are Messengers in Federated Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.976) |  | 0 |  | Chenhe Dong, Yuexiang Xie, Bolin Ding, Ying Shen, Yaliang Li |  |
| 1110 |  |  [Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.977) |  | 0 |  | Benjamin Yan, Ruochen Liu, David E. Kuo, Subathra Adithan, Eduardo Pontes Reis, Stephen Kwak, Vasantha Kumar Venugopal, Chloe O'Connell, Agustina Saenz, Pranav Rajpurkar, Michael Moor |  |
| 1111 |  |  [Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs](https://doi.org/10.18653/v1/2023.findings-emnlp.978) |  | 0 |  | Yuxin Zuo, Bei Li, Chuanhao Lv, Tong Zheng, Tong Xiao, JingBo Zhu |  |
| 1112 |  |  [GenKIE: Robust Generative Multimodal Document Key Information Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.979) |  | 0 |  | Panfeng Cao, Ye Wang, Qiang Zhang, Zaiqiao Meng |  |
| 1113 |  |  [Improving Multimodal Sentiment Analysis: Supervised Angular margin-based Contrastive Learning for Enhanced Fusion Representation](https://doi.org/10.18653/v1/2023.findings-emnlp.980) |  | 0 |  | CongDuy Nguyen, Thong Nguyen, Duc Anh Vu, Anh Tuan Luu |  |
| 1114 |  |  [Efficient Multilingual Language Model Compression through Vocabulary Trimming](https://doi.org/10.18653/v1/2023.findings-emnlp.981) |  | 0 |  | Asahi Ushio, Yi Zhou, José CamachoCollados |  |
| 1115 |  |  [ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.982) |  | 0 |  | Guojun Wu |  |
| 1116 |  |  [GTA: Gated Toxicity Avoidance for LM Performance Preservation](https://doi.org/10.18653/v1/2023.findings-emnlp.983) |  | 0 |  | Heegyu Kim, Hyunsouk Cho |  |
| 1117 |  |  [LMGQS: A Large-scale Dataset for Query-focused Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.984) |  | 0 |  | Ruochen Xu, Song Wang, Yang Liu, Shuohang Wang, Yichong Xu, Dan Iter, Pengcheng He, Chenguang Zhu, Michael Zeng |  |
| 1118 |  |  [ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.985) |  | 0 |  | Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Xin Zhao, JiRong Wen |  |
| 1119 |  |  [Non-Autoregressive Document-Level Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.986) |  | 0 |  | Guangsheng Bao, Zhiyang Teng, Hao Zhou, Jianhao Yan, Yue Zhang |  |
| 1120 |  |  [Exploring the Effectiveness of Multi-Lingual Commonsense Knowledge-Aware Open-Domain Dialogue Response Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.987) |  | 0 |  | Sixing Wu, Jiong Yu, Tianshi Che, Yang Zhou, Wei Zhou |  |
| 1121 |  |  [Mixture of Soft Prompts for Controllable Data Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.988) |  | 0 |  | Derek Chen, Celine Lee, Yunan Lu, Domenic Rosati, Zhou Yu |  |
| 1122 |  |  [A Boundary Offset Prediction Network for Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.989) |  | 0 |  | Minghao Tang, Yongquan He, Yongxiu Xu, Hongbo Xu, Wenyuan Zhang, Yang Lin |  |
| 1123 |  |  [Prefix-Tuning Based Unsupervised Text Style Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.990) |  | 0 |  | Huiyu Mai, Wenhao Jiang, ZhiHong Deng |  |
| 1124 |  |  [Evaluating and Enhancing the Robustness of Code Pre-trained Models through Structure-Aware Adversarial Samples Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.991) |  | 0 |  | Nuo Chen, Qiushi Sun, Jianing Wang, Ming Gao, Xiaoli Li, Xiang Li |  |
| 1125 |  |  [Annotation Sensitivity: Training Data Collection Methods Affect Model Performance](https://doi.org/10.18653/v1/2023.findings-emnlp.992) |  | 0 |  | Christoph Kern, Stephanie Eckman, Jacob Beck, Rob Chew, Bolei Ma, Frauke Kreuter |  |
| 1126 |  |  [Qualitative Code Suggestion: A Human-Centric Approach to Qualitative Coding](https://doi.org/10.18653/v1/2023.findings-emnlp.993) |  | 0 |  | Cesare Spinoso Di Piano, Samira Abbasgholizadeh Rahimi, Jackie Chi Kit Cheung |  |
| 1127 |  |  [D²TV: Dual Knowledge Distillation and Target-oriented Vision Modeling for Many-to-Many Multimodal Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.994) |  | 0 |  | Yunlong Liang, Fandong Meng, Jiaan Wang, Jinan Xu, Yufeng Chen, Jie Zhou |  |
| 1128 |  |  [Improving Input-label Mapping with Demonstration Replay for In-context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.995) |  | 0 |  | Zhuocheng Gong, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui Yan |  |
| 1129 |  |  [Enhancing Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies](https://doi.org/10.18653/v1/2023.findings-emnlp.996) |  | 0 |  | Linyong Nan, Yilun Zhao, Weijin Zou, Narutatsu Ri, Jaesung Tae, Ellen Zhang, Arman Cohan, Dragomir Radev |  |
| 1130 |  |  [Cross-lingual Open-Retrieval Question Answering for African Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.997) |  | 0 |  | Odunayo Ogundepo, Tajuddeen Gwadabe, Clara Rivera, Jonathan H. Clark, Sebastian Ruder, David Ifeoluwa Adelani, Bonaventure Dossou, Abdou Aziz Diop, Claytone Sikasote, Gilles Hacheme, Happy Buzaaba, Ignatius Ezeani, Rooweither Mabuya, Salomey Osei, Chris Emezue, Albert Kahira, Shamsuddeen Hassan Muhammad, Akintunde Oladipo, Abraham Toluwase Owodunni, Atnafu Lambebo Tonja, Iyanuoluwa Shode, Akari Asai, Aremu Anuoluwapo, Ayodele Awokoya, Bernard Opoku, Chiamaka Chukwuneke, Christine Mwase, Clemencia Siro, Stephen Arthur, Tunde Ajayi, Verrah Otiende, Andre Niyongabo Rubungo, Boyd Sinkala, Daniel A. Ajisafe, Emeka Onwuegbuzia, Falalu Ibrahim Lawan, Ibrahim Said Ahmad, Jesujoba O. Alabi, Chinedu E. Mbonu, Mofetoluwa Adeyemi, Mofya Phiri, Orevaoghene Ahia, Ruqayya Nasir Iro, Sonia Adhiambo |  |
| 1131 |  |  [Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens](https://doi.org/10.18653/v1/2023.findings-emnlp.998) |  | 0 |  | David Stap, Vlad Niculae, Christof Monz |  |
| 1132 |  |  [Aligning Predictive Uncertainty with Clarification Questions in Grounded Dialog](https://doi.org/10.18653/v1/2023.findings-emnlp.999) |  | 0 |  | Kata Naszádi, Putra Manggala, Christof Monz |  |
| 1133 |  |  [Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.1000) |  | 0 |  | Ilias Stogiannidis, Stavros Vassos, Prodromos Malakasiotis, Ion Androutsopoulos |  |
| 1134 |  |  [ParroT: Translating during Chat using Large Language Models tuned with Human Translation and Feedback](https://doi.org/10.18653/v1/2023.findings-emnlp.1001) |  | 0 |  | Wenxiang Jiao, Jentse Huang, Wenxuan Wang, Zhiwei He, Tian Liang, Xing Wang, Shuming Shi, Zhaopeng Tu |  |
| 1135 |  |  [Dense Retrieval as Indirect Supervision for Large-space Decision Making](https://doi.org/10.18653/v1/2023.findings-emnlp.1002) |  | 0 |  | Nan Xu, Fei Wang, Mingtao Dong, Muhao Chen |  |
| 1136 |  |  [One-Model-Connects-All: A Unified Graph Pre-Training Model for Online Community Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.1003) |  | 0 |  | Ruoxue Ma, Jiarong Xu, Xinnong Zhang, Haozhe Zhang, Zuyu Zhao, Qi Zhang, Xuanjing Huang, Zhongyu Wei |  |
| 1137 |  |  [In-Image Neural Machine Translation with Segmented Pixel Sequence-to-Sequence Model](https://doi.org/10.18653/v1/2023.findings-emnlp.1004) |  | 0 |  | Yanzhi Tian, Xiang Li, Zeming Liu, Yuhang Guo, Bin Wang |  |
| 1138 |  |  [NarrativeXL: a Large-scale Dataset for Long-Term Memory Models](https://doi.org/10.18653/v1/2023.findings-emnlp.1005) |  | 0 |  | Arsenii Moskvichev, KyVinh Mai |  |
| 1139 |  |  [Dialogue Act-Aided Backchannel Prediction Using Multi-Task Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.1006) |  | 0 |  | Wencke Liermann, YoHan Park, YongSeok Choi, KongJoo Lee |  |
| 1140 |  |  [mReFinED: An Efficient End-to-End Multilingual Entity Linking System](https://doi.org/10.18653/v1/2023.findings-emnlp.1007) |  | 0 |  | Peerat Limkonchotiwat, Weiwei Cheng, Christos Christodoulopoulos, Amir Saffari, Jens Lehmann |  |
| 1141 |  |  [Sub-network Discovery and Soft-masking for Continual Learning of Mixed Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.1008) |  | 0 |  | Zixuan Ke, Bing Liu, Wenhan Xiong, Asli Celikyilmaz, Haoran Li |  |
| 1142 |  |  [PIVOINE: Instruction Tuning for Open-world Entity Profiling](https://doi.org/10.18653/v1/2023.findings-emnlp.1009) |  | 0 |  | Keming Lu, Xiaoman Pan, Kaiqiang Song, Hongming Zhang, Dong Yu, Jianshu Chen |  |
| 1143 |  |  [DiQAD: A Benchmark Dataset for Open-domain Dialogue Quality Assessment](https://doi.org/10.18653/v1/2023.findings-emnlp.1010) |  | 0 |  | Yukun Zhao, Lingyong Yan, Weiwei Sun, Chong Meng, Shuaiqiang Wang, Zhicong Cheng, Zhaochun Ren, Dawei Yin |  |
| 1144 |  |  [Tuna: Instruction Tuning using Feedback from Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.1011) |  | 0 |  | Haoran Li, Yiran Liu, Xingxing Zhang, Wei Lu, Furu Wei |  |
| 1145 |  |  [Emptying the Ocean with a Spoon: Should We Edit Models?](https://doi.org/10.18653/v1/2023.findings-emnlp.1012) |  | 0 |  | Yuval Pinter, Michael Elhadad |  |
| 1146 |  |  [A Causal View of Entity Bias in (Large) Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.1013) |  | 0 |  | Fei Wang, Wenjie Mo, Yiwei Wang, Wenxuan Zhou, Muhao Chen |  |
| 1147 |  |  [T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics](https://doi.org/10.18653/v1/2023.findings-emnlp.1014) |  | 0 |  | Yiwei Qin, Weizhe Yuan, Graham Neubig, Pengfei Liu |  |
| 1148 |  |  [T-Projection: High Quality Annotation Projection for Sequence Labeling Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.1015) |  | 0 |  | Iker GarcíaFerrero, Rodrigo Agerri, German Rigau |  |
| 1149 |  |  [MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document](https://doi.org/10.18653/v1/2023.findings-emnlp.1016) |  | 0 |  | Zheng Chu, Zekun Wang, Jiafeng Liang, Ming Liu, Bing Qin |  |
| 1150 |  |  [MSCFFN: A New FFN with Multi-Space Cross to Accelerate Transformer](https://doi.org/10.18653/v1/2023.findings-emnlp.1017) |  | 0 |  | Tang Dongge, Qing Yang |  |
| 1151 |  |  [Dialect Transfer for Swiss German Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.1018) |  | 0 |  | Claudio Paonessa, Yanick Schraner, Jan Deriu, Manuela Hürlimann, Manfred Vogel, Mark Cieliebak |  |
| 1152 |  |  [Masked Path Modeling for Vision-and-Language Navigation](https://doi.org/10.18653/v1/2023.findings-emnlp.1019) |  | 0 |  | ZiYi Dou, Feng Gao, Nanyun Peng |  |
| 1153 |  |  [Learning Interpretable Style Embeddings via Prompting LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.1020) |  | 0 |  | Ajay Patel, Delip Rao, Ansh Kothary, Kathleen R. McKeown, Chris CallisonBurch |  |
| 1154 |  |  [Exploring Context-Aware Evaluation Metrics for Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.1021) |  | 0 |  | Xinyu Hu, Xunjian Yin, Xiaojun Wan |  |
| 1155 |  |  [GRACE: Discriminator-Guided Chain-of-Thought Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.1022) |  | 0 |  | Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Lu Wang |  |
| 1156 |  |  [QADYNAMICS: Training Dynamics-Driven Synthetic QA Diagnostic for Zero-Shot Commonsense Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.1023) |  | 0 |  | Haochen Shi, Weiqi Wang, Tianqing Fang, Baixuan Xu, Wenxuan Ding, Xin Liu, Yangqiu Song |  |
| 1157 |  |  [RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.1024) |  | 0 |  | Chengyuan Liu, Fubang Zhao, Yangyang Kang, Jingyuan Zhang, Xiang Zhou, Changlong Sun, Kun Kuang, Fei Wu |  |
| 1158 |  |  [PromptARA: Improving Deep Representation in Hybrid Automatic Readability Assessment with Prompt and Orthogonal Projection](https://doi.org/10.18653/v1/2023.findings-emnlp.1025) |  | 0 |  | Jinshan Zeng, Xianglong Yu, Xianchao Tong, Wenyan Xiao |  |
| 1159 |  |  [Does Listener Gaze in Face-to-Face Interaction Follow the Entropy Rate Constancy Principle: An Empirical Study](https://doi.org/10.18653/v1/2023.findings-emnlp.1026) |  | 0 |  | Yu Wang, Hendrik Buschmeier |  |
| 1160 |  |  [Incorporating Object-Level Visual Context for Multimodal Fine-Grained Entity Typing](https://doi.org/10.18653/v1/2023.findings-emnlp.1027) |  | 0 |  | Ying Zhang, Wenbo Fan, Kehui Song, Yu Zhao, Xuhui Sui, Xiaojie Yuan |  |
| 1161 |  |  [Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data](https://doi.org/10.18653/v1/2023.findings-emnlp.1028) |  | 0 |  | Mubashara Akhtar, Abhilash Reddy Shankarampeta, Vivek Gupta, Arpit Patil, Oana Cocarascu, Elena Simperl |  |
| 1162 |  |  [Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.1029) |  | 0 |  | Ruixiang Tang, Gord Lueck, Rodolfo Quispe, Huseyin A. Inan, Janardhan Kulkarni, Xia Hu |  |
| 1163 |  |  [BERT Has More to Offer: BERT Layers Combination Yields Better Sentence Embeddings](https://doi.org/10.18653/v1/2023.findings-emnlp.1030) |  | 0 |  | Seyyed MohammadSaleh Hosseini, Munawara Munia, Latifur Khan |  |
| 1164 |  |  [Extrapolating Multilingual Understanding Models as Multilingual Generators](https://doi.org/10.18653/v1/2023.findings-emnlp.1031) |  | 0 |  | Bohong Wu, Fei Yuan, Hai Zhao, Lei Li, Jingjing Xu |  |
| 1165 |  |  [SAC³: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency](https://doi.org/10.18653/v1/2023.findings-emnlp.1032) |  | 0 |  | Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A. Malin, Kumar Sricharan |  |
| 1166 |  |  [Test-Time Self-Adaptive Small Language Models for Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.1033) |  | 0 |  | Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong Park |  |
| 1167 |  |  [ExpNote: Black-box Large Language Models are better Task Solvers with Experience Notebook](https://doi.org/10.18653/v1/2023.findings-emnlp.1034) |  | 0 |  | Wangtao Sun, Xuanqing Yu, Shizhu He, Jun Zhao, Kang Liu |  |
| 1168 |  |  [Evaluating Parameter-Efficient Finetuning Approaches for Pre-trained Models on the Financial Domain](https://doi.org/10.18653/v1/2023.findings-emnlp.1035) |  | 0 |  | Isabella Olariu, Cedric Lothritz, Jacques Klein, Tegawendé F. Bissyandé, Siwen Guo, Shohreh Haddadan |  |
| 1169 |  |  [Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model](https://doi.org/10.18653/v1/2023.findings-emnlp.1036) |  | 0 |  | Parishad BehnamGhader, Santiago Miret, Siva Reddy |  |
| 1170 |  |  [BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text](https://doi.org/10.18653/v1/2023.findings-emnlp.1037) |  | 0 |  | Aarohi Srivastava, David Chiang |  |
| 1171 |  |  [Closed Boundary Learning for Classification Tasks with the Universum Class](https://doi.org/10.18653/v1/2023.findings-emnlp.1038) |  | 0 |  | Hanzhang Zhou, Zijian Feng, Kezhi Mao |  |
| 1172 |  |  [Revisiting Entropy Rate Constancy in Text](https://doi.org/10.18653/v1/2023.findings-emnlp.1039) |  | 0 |  | Vivek Verma, Nicholas Tomlin, Dan Klein |  |
| 1173 |  |  [Calibrated Seq2seq Models for Efficient and Generalizable Ultra-fine Entity Typing](https://doi.org/10.18653/v1/2023.findings-emnlp.1040) |  | 0 |  | Yanlin Feng, Adithya Pratapa, David R. Mortensen |  |
| 1174 |  |  [Learning Semantic Role Labeling from Compatible Label Sequences](https://doi.org/10.18653/v1/2023.findings-emnlp.1041) |  | 0 |  | Tao Li, Ghazaleh Kazeminejad, Susan Windisch Brown, Vivek Srikumar, Martha Palmer |  |
| 1175 |  |  [QUADRo: Dataset and Models for QUestion-Answer Database Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.1042) |  | 0 |  | Stefano Campese, Ivano Lauriola, Alessandro Moschitti |  |
| 1176 |  |  [Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.1043) |  | 0 |  | Paul Youssef, Osman Alperen Koras, Meijie Li, Jörg Schlötterer, Christin Seifert |  |
| 1177 |  |  [Is ChatGPT the ultimate Data Augmentation Algorithm?](https://doi.org/10.18653/v1/2023.findings-emnlp.1044) |  | 0 |  | Frédéric Piedboeuf, Philippe Langlais |  |
| 1178 |  |  [Enhanced Simultaneous Machine Translation with Word-level Policies](https://doi.org/10.18653/v1/2023.findings-emnlp.1045) |  | 0 |  | Kang Kim, Hankyu Cho |  |
| 1179 |  |  [Causal Intervention-based Few-Shot Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.1046) |  | 0 |  | Zhen Yang, Yongbin Liu, Chunping Ouyang |  |
| 1180 |  |  [TADI: Topic-aware Attention and Powerful Dual-encoder Interaction for Recall in News Recommendation](https://doi.org/10.18653/v1/2023.findings-emnlp.1047) |  | 0 |  | Junxiang Jiang |  |
| 1181 |  |  [Unveiling the Power of Argument Arrangement in Online Persuasive Discussions](https://doi.org/10.18653/v1/2023.findings-emnlp.1048) |  | 0 |  | Nailia Mirzakhmedova, Johannes Kiesel, Khalid Al Khatib, Benno Stein |  |
| 1182 |  |  [FFAEval: Evaluating Dialogue System via Free-For-All Ranking](https://doi.org/10.18653/v1/2023.findings-emnlp.1049) |  | 0 |  | Zeyao Ma, Zijun Yao, Jing Zhang, Jifan Yu, Xiaohan Zhang, Juanzi Li, Jie Tang |  |
| 1183 |  |  [Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.1050) |  | 0 |  | Nuo Chen, Hongguang Li, Junqing He, Yinan Bao, Xinshi Lin, Qi Yang, Jianfeng Liu, Ruyi Gan, Jiaxing Zhang, Baoyuan Wang, Jia Li |  |
| 1184 |  |  [VER: Unifying Verbalizing Entities and Relations](https://doi.org/10.18653/v1/2023.findings-emnlp.1051) |  | 0 |  | Jie Huang, Kevin Chang |  |
| 1185 |  |  [The Linearity of the Effect of Surprisal on Reading Times across Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.1052) |  | 0 |  | Weijie Xu, Jason Chon, Tianran Liu, Richard Futrell |  |
| 1186 |  |  [Adversarial Text Generation by Search and Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.1053) |  | 0 |  | Guoyi Li, Bingkang Shi, Zongzhen Liu, Dehan Kong, Yulei Wu, Xiaodan Zhang, Longtao Huang, Honglei Lyu |  |
| 1187 |  |  [Measuring Pointwise \mathcalV-Usable Information In-Context-ly](https://doi.org/10.18653/v1/2023.findings-emnlp.1054) |  | 0 |  | Sheng Lu, Shan Chen, Yingya Li, Danielle S. Bitterman, Guergana Savova, Iryna Gurevych |  |
| 1188 |  |  [SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities](https://doi.org/10.18653/v1/2023.findings-emnlp.1055) |  | 0 |  | Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, Xipeng Qiu |  |
| 1189 |  |  [Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance via Probability Calibration](https://doi.org/10.18653/v1/2023.findings-emnlp.1056) |  | 0 |  | Ercong Nie, Helmut Schmid, Hinrich Schütze |  |
| 1190 |  |  [A Thorough Examination on Zero-shot Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.1057) |  | 0 |  | Ruiyang Ren, Yingqi Qu, Jing Liu, Xin Zhao, Qifei Wu, Yuchen Ding, Hua Wu, Haifeng Wang, JiRong Wen |  |
| 1191 |  |  [Contrastive Pre-training for Personalized Expert Finding](https://doi.org/10.18653/v1/2023.findings-emnlp.1058) |  | 0 |  | Qiyao Peng, Hongtao Liu, Zhepeng Lv, Qing Yang, Wenjun Wang |  |
| 1192 |  |  [Mitigating Intrinsic Named Entity-Related Hallucinations of Abstractive Text Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.1059) |  | 0 |  | Jianbin Shen, Junyu Xuan, Christy Jie Liang |  |
| 1193 |  |  [Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.1060) |  | 0 |  | Hongfu Liu, Ye Wang |  |
| 1194 |  |  [Frontmatter](https://aclanthology.org/2023.emnlp-main.0) |  | 0 |  |  |  |
| 1195 |  |  [IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions](https://doi.org/10.18653/v1/2023.emnlp-main.1) |  | 0 |  | Zhebin Zhang, Xinyu Zhang, Yuanhang Ren, Saijiang Shi, Meng Han, Yongkang Wu, Ruofei Lai, Zhao Cao |  |
| 1196 |  |  [Absolute Position Embedding Learns Sinusoid-like Waves for Attention Based on Relative Position](https://doi.org/10.18653/v1/2023.emnlp-main.2) |  | 0 |  | Yuji Yamamoto, Takuya Matsuzaki |  |
| 1197 |  |  [Chinese Lexical Substitution: Dataset and Method](https://doi.org/10.18653/v1/2023.emnlp-main.3) |  | 0 |  | Jipeng Qiang, Kang Liu, Ying Li, Yun Li, Yi Zhu, YunHao Yuan, Xiaocheng Hu, Xiaoye Ouyang |  |
| 1198 |  |  [Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting](https://doi.org/10.18653/v1/2023.emnlp-main.4) |  | 0 |  | Chenkai Sun, Jinning Li, Yi Ren Fung, Hou Pong Chan, Tarek F. Abdelzaher, ChengXiang Zhai, Heng Ji |  |
| 1199 |  |  [Fine-grained Conversational Decoding via Isotropic and Proximal Search](https://doi.org/10.18653/v1/2023.emnlp-main.5) |  | 0 |  | Yuxuan Yao, Han Wu, Qiling Xu, Linqi Song |  |
| 1200 |  |  [Holistic Inter-Annotator Agreement and Corpus Coherence Estimation in a Large-scale Multilingual Annotation Campaign](https://doi.org/10.18653/v1/2023.emnlp-main.6) |  | 0 |  | Nicolas Stefanovitch, Jakub Piskorski |  |
| 1201 |  |  [PHD: Pixel-Based Language Modeling of Historical Documents](https://doi.org/10.18653/v1/2023.emnlp-main.7) |  | 0 |  | Nadav Borenstein, Phillip Rust, Desmond Elliott, Isabelle Augenstein |  |
| 1202 |  |  [Primacy Effect of ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.8) |  | 0 |  | Yiwei Wang, Yujun Cai, Muhao Chen, Yuxuan Liang, Bryan Hooi |  |
| 1203 |  |  [Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension](https://doi.org/10.18653/v1/2023.emnlp-main.9) |  | 0 |  | Akira Kawabata, Saku Sugawara |  |
| 1204 |  |  [Evaluating and Modeling Attribution for Cross-Lingual Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.10) |  | 0 |  | Benjamin Muller, John Wieting, Jonathan H. Clark, Tom Kwiatkowski, Sebastian Ruder, Livio Soares, Roee Aharoni, Jonathan Herzig, Xinyi Wang |  |
| 1205 |  |  [Better Quality Pre-training Data and T5 Models for African Languages](https://doi.org/10.18653/v1/2023.emnlp-main.11) |  | 0 |  | Akintunde Oladipo, Mofetoluwa Adeyemi, Orevaoghene Ahia, Abraham Toluwase Owodunni, Odunayo Ogundepo, David Ifeoluwa Adelani, Jimmy Lin |  |
| 1206 |  |  [Sparse Universal Transformer](https://doi.org/10.18653/v1/2023.emnlp-main.12) |  | 0 |  | Shawn Tan, Yikang Shen, Zhenfang Chen, Aaron C. Courville, Chuang Gan |  |
| 1207 |  |  [Theory of Mind for Multi-Agent Collaboration via Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.13) |  | 0 |  | Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Charles Lewis, Katia P. Sycara |  |
| 1208 |  |  [Establishing Trustworthiness: Rethinking Tasks and Model Evaluation](https://doi.org/10.18653/v1/2023.emnlp-main.14) |  | 0 |  | Robert Litschko, Max MüllerEberstein, Rob van der Goot, Leon WeberGenzel, Barbara Plank |  |
| 1209 |  |  [Let's Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought](https://doi.org/10.18653/v1/2023.emnlp-main.15) |  | 0 |  | Vaishnavi Himakunthala, Andy Ouyang, Daniel Rose, Ryan He, Alex Mei, Yujie Lu, Chinmay Sonar, Michael Saxon, William Yang Wang |  |
| 1210 |  |  [GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP](https://doi.org/10.18653/v1/2023.emnlp-main.16) |  | 0 |  | Md Tawkat Islam Khondaker, Abdul Waheed, El Moatez Billah Nagoudi, Muhammad AbdulMageed |  |
| 1211 |  |  [Dual-Channel Span for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.17) |  | 0 |  | Pan Li, Ping Li, Kai Zhang |  |
| 1212 |  |  [Cultural Concept Adaptation on Multimodal Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.18) |  | 0 |  | Zhi Li, Yin Zhang |  |
| 1213 |  |  [Understanding Compositional Data Augmentation in Typologically Diverse Morphological Inflection](https://doi.org/10.18653/v1/2023.emnlp-main.19) |  | 0 |  | Farhan Samir, Miikka Silfverberg |  |
| 1214 |  |  [Evaluating Object Hallucination in Large Vision-Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.20) |  | 0 |  | Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, JiRong Wen |  |
| 1215 |  |  [Event Ontology Completion with Hierarchical Structure Evolution Networks](https://doi.org/10.18653/v1/2023.emnlp-main.21) |  | 0 |  | Pengfei Cao, Yupu Hao, Yubo Chen, Kang Liu, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Jun Zhao |  |
| 1216 |  |  [Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients](https://doi.org/10.18653/v1/2023.emnlp-main.22) |  | 0 |  | Feihu Jin, Jiajun Zhang, Chengqing Zong |  |
| 1217 |  |  [Discourse Structures Guided Fine-grained Propaganda Identification](https://doi.org/10.18653/v1/2023.emnlp-main.23) |  | 0 |  | Yuanyuan Lei, Ruihong Huang |  |
| 1218 |  |  [CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.24) |  | 0 |  | Benjamin Minixhofer, Jonas Pfeiffer, Ivan Vulic |  |
| 1219 |  |  [Improving Image Captioning via Predicting Structured Concepts](https://doi.org/10.18653/v1/2023.emnlp-main.25) |  | 0 |  | Ting Wang, Weidong Chen, Yuanhe Tian, Yan Song, Zhendong Mao |  |
| 1220 |  |  [GATITOS: Using a New Multilingual Lexicon for Low-resource Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.26) |  | 0 |  | Alexander Jones, Isaac Caswell, Orhan Firat, Ishank Saxena |  |
| 1221 |  |  [Continually Improving Extractive QA via Human Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.27) |  | 0 |  | Ge Gao, HungTing Chen, Yoav Artzi, Eunsol Choi |  |
| 1222 |  |  [Using Interpretation Methods for Model Enhancement](https://doi.org/10.18653/v1/2023.emnlp-main.28) |  | 0 |  | Zhuo Chen, Chengyue Jiang, Kewei Tu |  |
| 1223 |  |  [An Expression Tree Decoding Strategy for Mathematical Equation Generation](https://doi.org/10.18653/v1/2023.emnlp-main.29) |  | 0 |  | Wenqi Zhang, Yongliang Shen, Qingpeng Nong, Zeqi Tan, Yanna Ma, Weiming Lu |  |
| 1224 |  |  [Bootstrapping Small & High Performance Language Models with Unmasking-Removal Training Policy](https://doi.org/10.18653/v1/2023.emnlp-main.30) |  | 0 |  | Yahan Yang, Elior Sulem, Insup Lee, Dan Roth |  |
| 1225 |  |  [Diversity Enhanced Narrative Question Generation for Storybooks](https://doi.org/10.18653/v1/2023.emnlp-main.31) |  | 0 |  | Hokeun Yoon, JinYeong Bak |  |
| 1226 |  |  [Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification](https://doi.org/10.18653/v1/2023.emnlp-main.32) |  | 0 |  | Chengyu Dong, Zihan Wang, Jingbo Shang |  |
| 1227 |  |  [How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.33) |  | 0 |  | Hang Chen, Xinyu Yang, Jing Luo, Wenjing Zhu |  |
| 1228 |  |  [Compressing and Debiasing Vision-Language Pre-Trained Models for Visual Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.34) |  | 0 |  | Qingyi Si, Yuanxin Liu, Zheng Lin, Peng Fu, Yanan Cao, Weiping Wang |  |
| 1229 |  |  [Selectively Answering Ambiguous Questions](https://doi.org/10.18653/v1/2023.emnlp-main.35) |  | 0 |  | Jeremy R. Cole, Michael J. Q. Zhang, Daniel Gillick, Julian Eisenschlos, Bhuwan Dhingra, Jacob Eisenstein |  |
| 1230 |  |  [Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.36) |  | 0 |  | DongHo Lee, Kian Ahrabian, Woojeong Jin, Fred Morstatter, Jay Pujara |  |
| 1231 |  |  [Knowledge Graph Compression Enhances Diverse Commonsense Generation](https://doi.org/10.18653/v1/2023.emnlp-main.37) |  | 0 |  | Eunjeong Hwang, Veronika Thost, Vered Shwartz, Tengfei Ma |  |
| 1232 |  |  [Pragmatic Reasoning Unlocks Quantifier Semantics for Foundation Models](https://doi.org/10.18653/v1/2023.emnlp-main.38) |  | 0 |  | Yiyuan Li, Rakesh R. Menon, Sayan Ghosh, Shashank Srivastava |  |
| 1233 |  |  [LLM-FP4: 4-Bit Floating-Point Quantized Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.39) |  | 0 |  | ShihYang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, KwangTing Cheng |  |
| 1234 |  |  [Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers](https://doi.org/10.18653/v1/2023.emnlp-main.40) |  | 0 |  | Chen Tang, Shun Wang, Tomas Goldsack, Chenghua Lin |  |
| 1235 |  |  [Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting](https://doi.org/10.18653/v1/2023.emnlp-main.41) |  | 0 |  | Xi Ye, Greg Durrett |  |
| 1236 |  |  [HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.42) |  | 0 |  | David Dale, Elena Voita, Janice Lam, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Loïc Barrault, Marta R. Costajussà |  |
| 1237 |  |  [Gradient-based Gradual Pruning for Language-Specific Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.43) |  | 0 |  | Dan He, MinhQuang Pham, ThanhLe Ha, Marco Turchi |  |
| 1238 |  |  [LLM-powered Data Augmentation for Enhanced Cross-lingual Performance](https://doi.org/10.18653/v1/2023.emnlp-main.44) |  | 0 |  | Chenxi Whitehouse, Monojit Choudhury, Alham Fikri Aji |  |
| 1239 |  |  [Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.45) |  | 0 |  | Chenxu Wang, Ping Jian, Mu Huang |  |
| 1240 |  |  [VLIS: Unimodal Language Models Guide Multimodal Language Generation](https://doi.org/10.18653/v1/2023.emnlp-main.46) |  | 0 |  | Jiwan Chung, Youngjae Yu |  |
| 1241 |  |  [Conceptual structure coheres in human cognition but not in large language models](https://doi.org/10.18653/v1/2023.emnlp-main.47) |  | 0 |  | Siddharth Suresh, Kushin Mukherjee, Xizheng Yu, WeiChun Huang, Lisa Padua, Timothy T. Rogers |  |
| 1242 |  |  [Towards LLM-driven Dialogue State Tracking](https://doi.org/10.18653/v1/2023.emnlp-main.48) |  | 0 |  | Yujie Feng, Zexin Lu, Bo Liu, Liming Zhan, XiaoMing Wu |  |
| 1243 |  |  [Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.49) |  | 0 |  | Haoyu Zhang, Yu Wang, Guanghao Yin, Kejun Liu, Yuanyuan Liu, Tianshu Yu |  |
| 1244 |  |  [Multitask Multimodal Prompted Training for Interactive Embodied Task Completion](https://doi.org/10.18653/v1/2023.emnlp-main.50) |  | 0 |  | Georgios Pantazopoulos, Malvina Nikandrou, Amit Parekh, Bhathiya Hemanthage, Arash Eshghi, Ioannis Konstas, Verena Rieser, Oliver Lemon, Alessandro Suglia |  |
| 1245 |  |  [We're Afraid Language Models Aren't Modeling Ambiguity](https://doi.org/10.18653/v1/2023.emnlp-main.51) |  | 0 |  | Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah A. Smith, Yejin Choi |  |
| 1246 |  |  [Linear-Time Modeling of Linguistic Structure: An Order-Theoretic Perspective](https://doi.org/10.18653/v1/2023.emnlp-main.52) |  | 0 |  | Tianyu Liu, Afra Amini, Mrinmaya Sachan, Ryan Cotterell |  |
| 1247 |  |  [GEMINI: Controlling The Sentence-Level Summary Style in Abstractive Text Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.53) |  | 0 |  | Guangsheng Bao, Zebin Ou, Yue Zhang |  |
| 1248 |  |  [Fidelity-Enriched Contrastive Search: Reconciling the Faithfulness-Diversity Trade-Off in Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.54) |  | 0 |  | WeiLin Chen, ChengKuang Wu, HsinHsi Chen, ChungChi Chen |  |
| 1249 |  |  [Analyzing Norm Violations in Live-Stream Chat](https://doi.org/10.18653/v1/2023.emnlp-main.55) |  | 0 |  | Jihyung Moon, DongHo Lee, Hyundong Cho, Woojeong Jin, Chan Young Park, Minwoo Kim, Jonathan May, Jay Pujara, Sungjoon Park |  |
| 1250 |  |  [Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality](https://doi.org/10.18653/v1/2023.emnlp-main.56) |  | 0 |  | Harman Singh, Pengchuan Zhang, Qifan Wang, Mengjiao Wang, Wenhan Xiong, Jingfei Du, Yu Chen |  |
| 1251 |  |  [Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms](https://doi.org/10.18653/v1/2023.emnlp-main.57) |  | 0 |  | Seungju Han, Junhyeok Kim, Jack Hessel, Liwei Jiang, Jiwan Chung, Yejin Son, Yejin Choi, Youngjae Yu |  |
| 1252 |  |  [Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus](https://doi.org/10.18653/v1/2023.emnlp-main.58) |  | 0 |  | Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng, Yue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing Wang, Luoyi Fu |  |
| 1253 |  |  [FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.59) |  | 0 |  | Shangbin Feng, Vidhisha Balachandran, Yuyang Bai, Yulia Tsvetkov |  |
| 1254 |  |  [Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation](https://doi.org/10.18653/v1/2023.emnlp-main.60) |  | 0 |  | Xuanli He, Qiongkai Xu, Jun Wang, Benjamin I. P. Rubinstein, Trevor Cohn |  |
| 1255 |  |  [Symbol tuning improves in-context learning in language models](https://doi.org/10.18653/v1/2023.emnlp-main.61) |  | 0 |  | Jerry W. Wei, Le Hou, Andrew K. Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, Quoc V. Le |  |
| 1256 |  |  [The neural dynamics of word recognition and integration](https://doi.org/10.18653/v1/2023.emnlp-main.62) |  | 0 |  | Jon Gauthier, Roger Levy |  |
| 1257 |  |  [Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.63) |  | 0 |  | Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, Jaewoo Kang |  |
| 1258 |  |  [Incorporating Worker Perspectives into MTurk Annotation Practices for NLP](https://doi.org/10.18653/v1/2023.emnlp-main.64) |  | 0 |  | Olivia Huang, Eve Fleisig, Dan Klein |  |
| 1259 |  |  [Predict the Future from the Past? On the Temporal Data Distribution Shift in Financial Sentiment Classifications](https://doi.org/10.18653/v1/2023.emnlp-main.65) |  | 0 |  | Yue Guo, Chenxi Hu, Yi Yang |  |
| 1260 |  |  [Look-back Decoding for Open-Ended Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.66) |  | 0 |  | Nan Xu, Chunting Zhou, Asli Celikyilmaz, Xuezhe Ma |  |
| 1261 |  |  [Large Language Models Can Self-Improve](https://doi.org/10.18653/v1/2023.emnlp-main.67) |  | 0 |  | Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han |  |
| 1262 |  |  [CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://doi.org/10.18653/v1/2023.emnlp-main.68) |  | 0 |  | Yue Wang, Hung Le, Akhilesh Gotmare, Nghi D. Q. Bui, Junnan Li, Steven C. H. Hoi |  |
| 1263 |  |  [Structural generalization in COGS: Supertagging is (almost) all you need](https://doi.org/10.18653/v1/2023.emnlp-main.69) |  | 0 |  | Alban Petit, Caio F. Corro, François Yvon |  |
| 1264 |  |  [BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations](https://doi.org/10.18653/v1/2023.emnlp-main.70) |  | 0 |  | Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, Rui Yan |  |
| 1265 |  |  [Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings](https://doi.org/10.18653/v1/2023.emnlp-main.71) |  | 0 |  | Andrea W. WenYi, David Mimno |  |
| 1266 |  |  [Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation](https://doi.org/10.18653/v1/2023.emnlp-main.72) |  | 0 |  | Jian Wang, Yi Cheng, Dongding Lin, Chak Tou Leong, Wenjie Li |  |
| 1267 |  |  [SeqXGPT: Sentence-Level AI-Generated Text Detection](https://doi.org/10.18653/v1/2023.emnlp-main.73) |  | 0 |  | Pengyu Wang, Linyang Li, Ke Ren, Botian Jiang, Dong Zhang, Xipeng Qiu |  |
| 1268 |  |  [QTSumm: Query-Focused Summarization over Tabular Data](https://doi.org/10.18653/v1/2023.emnlp-main.74) |  | 0 |  | Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin Liu, Weijin Zou, Simeng Han, Ruizhe Chen, Xiangru Tang, Yumo Xu, Dragomir Radev, Arman Cohan |  |
| 1269 |  |  [From Wrong To Right: A Recursive Approach Towards Vision-Language Explanation](https://doi.org/10.18653/v1/2023.emnlp-main.75) |  | 0 |  | Jiaxin Ge, Sanjay Subramanian, Trevor Darrell, Boyi Li |  |
| 1270 |  |  ['Don't Get Too Technical with Me': A Discourse Structure-Based Framework for Automatic Science Journalism](https://doi.org/10.18653/v1/2023.emnlp-main.76) |  | 0 |  | Ronald Cardenas, Bingsheng Yao, Dakuo Wang, Yufang Hou |  |
| 1271 |  |  [LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following](https://doi.org/10.18653/v1/2023.emnlp-main.77) |  | 0 |  | ChengFu Yang, YenChun Chen, Jianwei Yang, Xiyang Dai, Lu Yuan, YuChiang Frank Wang, KaiWei Chang |  |
| 1272 |  |  [Penalty Decoding: Well Suppress the Self-Reinforcement Effect in Open-Ended Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.78) |  | 0 |  | Wenhong Zhu, Hongkun Hao, Rui Wang |  |
| 1273 |  |  [Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.79) |  | 0 |  | Jianwei Li, Qi Lei, Wei Cheng, Dongkuan Xu |  |
| 1274 |  |  [Clinical Contradiction Detection](https://doi.org/10.18653/v1/2023.emnlp-main.80) |  | 0 |  | Dave Makhervaks, Plia Gillis, Kira Radinsky |  |
| 1275 |  |  [Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements](https://doi.org/10.18653/v1/2023.emnlp-main.81) |  | 0 |  | Jiacheng Liu, Wenya Wang, Dianzhuo Wang, Noah A. Smith, Yejin Choi, Hannaneh Hajishirzi |  |
| 1276 |  |  [Text-Transport: Toward Learning Causal Effects of Natural Language](https://doi.org/10.18653/v1/2023.emnlp-main.82) |  | 0 |  | Victoria Lin, LouisPhilippe Morency, Eli BenMichael |  |
| 1277 |  |  [How Does Generative Retrieval Scale to Millions of Passages?](https://doi.org/10.18653/v1/2023.emnlp-main.83) |  | 0 |  | Ronak Pradeep, Kai Hui, Jai Gupta, Ádám D. Lelkes, Honglei Zhuang, Jimmy Lin, Donald Metzler, Vinh Q. Tran |  |
| 1278 |  |  [Unveiling the Implicit Toxicity in Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.84) |  | 0 |  | Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, Minlie Huang |  |
| 1279 |  |  [Is ChatGPT a General-Purpose Natural Language Processing Task Solver?](https://doi.org/10.18653/v1/2023.emnlp-main.85) |  | 0 |  | Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang |  |
| 1280 |  |  [Length is a Curse and a Blessing for Document-level Semantics](https://doi.org/10.18653/v1/2023.emnlp-main.86) |  | 0 |  | Chenghao Xiao, Yizhi Li, G. Thomas Hudson, Chenghua Lin, Noura Al Moubayed |  |
| 1281 |  |  [ALCUNA: Large Language Models Meet New Knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.87) |  | 0 |  | Xunjian Yin, Baizhou Huang, Xiaojun Wan |  |
| 1282 |  |  [Location-Aware Visual Question Generation with Lightweight Models](https://doi.org/10.18653/v1/2023.emnlp-main.88) |  | 0 |  | Nicholas Collin Suwono, Justin ChihYao Chen, TunMin Hung, TingHao (Kenneth) Huang, IBin Liao, YungHui Li, LunWei Ku, ShaoHua Sun |  |
| 1283 |  |  [MemeCap: A Dataset for Captioning and Interpreting Memes](https://doi.org/10.18653/v1/2023.emnlp-main.89) |  | 0 |  | Eunjeong Hwang, Vered Shwartz |  |
| 1284 |  |  [Where to start? Analyzing the potential value of intermediate models](https://doi.org/10.18653/v1/2023.emnlp-main.90) |  | 0 |  | Leshem Choshen, Elad Venezian, Shachar DonYehiya, Noam Slonim, Yoav Katz |  |
| 1285 |  |  [Transcending Scaling Laws with 0.1% Extra Compute](https://doi.org/10.18653/v1/2023.emnlp-main.91) |  | 0 |  | Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q. Tran, David R. So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, Denny Zhou, Donald Metzler, Slav Petrov, Neil Houlsby, Quoc V. Le, Mostafa Dehghani |  |
| 1286 |  |  [CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation](https://doi.org/10.18653/v1/2023.emnlp-main.92) |  | 0 |  | Minzhi Li, Taiwei Shi, Caleb Ziems, MinYen Kan, Nancy F. Chen, Zhengyuan Liu, Diyi Yang |  |
| 1287 |  |  [Optimizing Retrieval-augmented Reader Models via Token Elimination](https://doi.org/10.18653/v1/2023.emnlp-main.93) |  | 0 |  | Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, Moshe Wasserblat |  |
| 1288 |  |  [WSDMS: Debunk Fake News via Weakly Supervised Detection of Misinforming Sentences with Contextualized Social Wisdom](https://doi.org/10.18653/v1/2023.emnlp-main.94) |  | 0 |  | Ruichao Yang, Wei Gao, Jing Ma, Hongzhan Lin, Zhiwei Yang |  |
| 1289 |  |  [Robust Prompt Optimization for Large Language Models Against Distribution Shifts](https://doi.org/10.18653/v1/2023.emnlp-main.95) |  | 0 |  | Moxin Li, Wenjie Wang, Fuli Feng, Yixin Cao, Jizhi Zhang, TatSeng Chua |  |
| 1290 |  |  [Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.96) |  | 0 |  | Martin Josifoski, Marija Sakota, Maxime Peyrard, Robert West |  |
| 1291 |  |  [Condensing Multilingual Knowledge with Lightweight Language-Specific Modules](https://doi.org/10.18653/v1/2023.emnlp-main.97) |  | 0 |  | Haoran Xu, Weiting Tan, Shuyue Stella Li, Yunmo Chen, Benjamin Van Durme, Philipp Koehn, Kenton Murray |  |
| 1292 |  |  [The Framework Tax: Disparities Between Inference Efficiency in NLP Research and Deployment](https://doi.org/10.18653/v1/2023.emnlp-main.98) |  | 0 |  | Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell |  |
| 1293 |  |  [Evaluating Cross-Domain Text-to-SQL Models and Benchmarks](https://doi.org/10.18653/v1/2023.emnlp-main.99) |  | 0 |  | Mohammadreza Pourreza, Davood Rafiei |  |
| 1294 |  |  [Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.100) |  | 0 |  | Simone Conia, Min Li, Daniel Lee, Umar Farooq Minhas, Ihab F. Ilyas, Yunyao Li |  |
| 1295 |  |  [Memory-Based Invariance Learning for Out-of-Domain Text Classification](https://doi.org/10.18653/v1/2023.emnlp-main.101) |  | 0 |  | Chen Jia, Yue Zhang |  |
| 1296 |  |  [Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling](https://doi.org/10.18653/v1/2023.emnlp-main.102) |  | 0 |  | Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong Liu |  |
| 1297 |  |  [Three Stream Based Multi-level Event Contrastive Learning for Text-Video Event Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.103) |  | 0 |  | Jiaqi Li, Chuanyi Zhang, Miaozeng Du, Dehai Min, Yongrui Chen, Guilin Qi |  |
| 1298 |  |  [Diversify Question Generation with Retrieval-Augmented Style Transfer](https://doi.org/10.18653/v1/2023.emnlp-main.104) |  | 0 |  | Qi Gou, Zehua Xia, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li, CamTu Nguyen |  |
| 1299 |  |  [Fast and Accurate Factual Inconsistency Detection Over Long Documents](https://doi.org/10.18653/v1/2023.emnlp-main.105) |  | 0 |  | Barrett Martin Lattimer, Patrick Chen, Xinyuan Zhang, Yi Yang |  |
| 1300 |  |  [Interpreting Embedding Spaces by Conceptualization](https://doi.org/10.18653/v1/2023.emnlp-main.106) |  | 0 |  | Adi Simhi, Shaul Markovitch |  |
| 1301 |  |  [Knowledge-Augmented Language Model Verification](https://doi.org/10.18653/v1/2023.emnlp-main.107) |  | 0 |  | Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C. Park, Sung Ju Hwang |  |
| 1302 |  |  [A Generation-based Deductive Method for Math Word Problems](https://doi.org/10.18653/v1/2023.emnlp-main.108) |  | 0 |  | Yuxuan Hu, Jing Zhang, Haoyang Li, Cuiping Li, Hong Chen |  |
| 1303 |  |  [Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation](https://doi.org/10.18653/v1/2023.emnlp-main.109) |  | 0 |  | Zeyuan Yang, Peng Li, Yang Liu |  |
| 1304 |  |  [Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning](https://doi.org/10.18653/v1/2023.emnlp-main.110) |  | 0 |  | Ryan Shea, Zhou Yu |  |
| 1305 |  |  [Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories](https://doi.org/10.18653/v1/2023.emnlp-main.111) |  | 0 |  | Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett |  |
| 1306 |  |  [Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.112) |  | 0 |  | PoNien Kung, Fan Yin, Di Wu, KaiWei Chang, Nanyun Peng |  |
| 1307 |  |  [Towards Example-Based NMT with Multi-Levenshtein Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.113) |  | 0 |  | Maxime Bouthors, Josep Maria Crego, François Yvon |  |
| 1308 |  |  [DUnE: Dataset for Unified Editing](https://doi.org/10.18653/v1/2023.emnlp-main.114) |  | 0 |  | Afra Feyza Akyürek, Eric Pan, Garry Kuwanto, Derry Wijaya |  |
| 1309 |  |  ["Fifty Shades of Bias": Normative Ratings of Gender Bias in GPT Generated English Text](https://doi.org/10.18653/v1/2023.emnlp-main.115) |  | 0 |  | Rishav Hada, Agrima Seth, Harshita Diddee, Kalika Bali |  |
| 1310 |  |  [Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.116) |  | 0 |  | Peitian Zhang, Zheng Liu, Shitao Xiao, Zhicheng Dou, Jing Yao |  |
| 1311 |  |  [ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness](https://doi.org/10.18653/v1/2023.emnlp-main.117) |  | 0 |  | Ján Cegin, Jakub Simko, Peter Brusilovsky |  |
| 1312 |  |  [Query-as-context Pre-training for Dense Passage Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.118) |  | 0 |  | Xing Wu, Guangyuan Ma, Wanhui Qian, Zijia Lin, Songlin Hu |  |
| 1313 |  |  [A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.119) |  | 0 |  | Andrea Burns, Krishna Srinivasan, Joshua Ainslie, Geoff Brown, Bryan A. Plummer, Kate Saenko, Jianmo Ni, Mandy Guo |  |
| 1314 |  |  [Democratizing Reasoning Ability: Tailored Learning from Large Language Model](https://doi.org/10.18653/v1/2023.emnlp-main.120) |  | 0 |  | Zhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang |  |
| 1315 |  |  [OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.121) |  | 0 |  | Shmuel Amar, Liat Schiff, Ori Ernst, Asi Shefer, Ori Shapira, Ido Dagan |  |
| 1316 |  |  [PEFTDebias : Capturing debiasing information using PEFTs](https://doi.org/10.18653/v1/2023.emnlp-main.122) |  | 0 |  | Sumit Agarwal, Aditya Srikanth Veerubhotla, Srijan Bansal |  |
| 1317 |  |  [Byte Pair Encoding for Symbolic Music](https://doi.org/10.18653/v1/2023.emnlp-main.123) |  | 0 |  | Nathan Fradet, Nicolas Gutowski, Fabien Chhel, JeanPierre Briot |  |
| 1318 |  |  [Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models](https://doi.org/10.18653/v1/2023.emnlp-main.124) |  | 0 |  | Alejo LopezAvila, Víctor SuárezPaniagua |  |
| 1319 |  |  [Self-Influence Guided Data Reweighting for Language Model Pre-training](https://doi.org/10.18653/v1/2023.emnlp-main.125) |  | 0 |  | Megh Thakkar, Tolga Bolukbasi, Sriram Ganapathy, Shikhar Vashishth, Sarath Chandar, Partha Talukdar |  |
| 1320 |  |  [ACTOR: Active Learning with Annotator-specific Classification Heads to Embrace Human Label Variation](https://doi.org/10.18653/v1/2023.emnlp-main.126) |  | 0 |  | Xinpeng Wang, Barbara Plank |  |
| 1321 |  |  [TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.127) |  | 0 |  | Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, Idan Szpektor |  |
| 1322 |  |  [VivesDebate-Speech: A Corpus of Spoken Argumentation to Leverage Audio Features for Argument Mining](https://doi.org/10.18653/v1/2023.emnlp-main.128) |  | 0 |  | Ramon RuizDolz, Javier Sanchez |  |
| 1323 |  |  [Tagging-Assisted Generation Model with Encoder and Decoder Supervision for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.129) |  | 0 |  | Xianlong Luo, Meng Yang, Yihao Wang |  |
| 1324 |  |  [Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.130) |  | 0 |  | Namrata Shivagunde, Vladislav Lialin, Anna Rumshisky |  |
| 1325 |  |  [Norm of Word Embedding Encodes Information Gain](https://doi.org/10.18653/v1/2023.emnlp-main.131) |  | 0 |  | Momose Oyama, Sho Yokoi, Hidetoshi Shimodaira |  |
| 1326 |  |  [CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data](https://doi.org/10.18653/v1/2023.emnlp-main.132) |  | 0 |  | Zhehao Zhang, Xitao Li, Yan Gao, JianGuang Lou |  |
| 1327 |  |  [Promoting Topic Coherence and Inter-Document Consorts in Multi-Document Summarization via Simplicial Complex and Sheaf Graph](https://doi.org/10.18653/v1/2023.emnlp-main.133) |  | 0 |  | Yash Kumar Atri, Arun Iyer, Tanmoy Chakraborty, Vikram Goyal |  |
| 1328 |  |  [MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations](https://doi.org/10.18653/v1/2023.emnlp-main.134) |  | 0 |  | Arkil Patel, Satwik Bhattamishra, Siva Reddy, Dzmitry Bahdanau |  |
| 1329 |  |  [Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency](https://doi.org/10.18653/v1/2023.emnlp-main.135) |  | 0 |  | Eric Zelikman, Wanjing Anya Ma, Jasmine E. Tran, Diyi Yang, Jason D. Yeatman, Nick Haber |  |
| 1330 |  |  [Counter Turing Test (CT2): AI-Generated Text Detection is Not as Easy as You May Think - Introducing AI Detectability Index (ADI)](https://doi.org/10.18653/v1/2023.emnlp-main.136) |  | 0 |  | Megha Chakraborty, S. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman, Shreya Gautam, Tanay Kumar, Krish Sharma, Niyar R. Barman, Chandan Gupta, Vinija Jain, Aman Chadha, Amit P. Sheth, Amitava Das |  |
| 1331 |  |  [Revisiting the Optimality of Word Lengths](https://doi.org/10.18653/v1/2023.emnlp-main.137) |  | 0 |  | Tiago Pimentel, Clara Meister, Ethan Wilcox, Kyle Mahowald, Ryan Cotterell |  |
| 1332 |  |  [Document-level Relationship Extraction by Bidirectional Constraints of Beta Rules](https://doi.org/10.18653/v1/2023.emnlp-main.138) |  | 0 |  | Yichun Liu, Zizhong Zhu, Xiaowang Zhang, Zhiyong Feng, Daoqi Chen, Yaxin Li |  |
| 1333 |  |  [Instructed Language Models with Retrievers Are Powerful Entity Linkers](https://doi.org/10.18653/v1/2023.emnlp-main.139) |  | 0 |  | Zilin Xiao, Ming Gong, Jie Wu, Xingyao Zhang, Linjun Shou, Daxin Jiang |  |
| 1334 |  |  [Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text](https://doi.org/10.18653/v1/2023.emnlp-main.140) |  | 0 |  | Xiang Li, Jinglu Wang, Xiaohao Xu, Muqiao Yang, Fan Yang, Yizhou Zhao, Rita Singh, Bhiksha Raj |  |
| 1335 |  |  [PROSE: A Pronoun Omission Solution for Chinese-English Spoken Language Translation](https://doi.org/10.18653/v1/2023.emnlp-main.141) |  | 0 |  | Ke Wang, Xiutian Zhao, Yanghui Li, Wei Peng |  |
| 1336 |  |  [A Diachronic Analysis of Paradigm Shifts in NLP Research: When, How, and Why?](https://doi.org/10.18653/v1/2023.emnlp-main.142) |  | 0 |  | Aniket Pramanick, Yufang Hou, Saif M. Mohammad, Iryna Gurevych |  |
| 1337 |  |  [Does the Correctness of Factual Knowledge Matter for Factual Knowledge-Enhanced Pre-trained Language Models?](https://doi.org/10.18653/v1/2023.emnlp-main.143) |  | 0 |  | Boxi Cao, Qiaoyu Tang, Hongyu Lin, Xianpei Han, Le Sun |  |
| 1338 |  |  [Syntactic Substitutability as Unsupervised Dependency Syntax](https://doi.org/10.18653/v1/2023.emnlp-main.144) |  | 0 |  | Jasper Jian, Siva Reddy |  |
| 1339 |  |  [MProto: Multi-Prototype Network with Denoised Optimal Transport for Distantly Supervised Named Entity Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.145) |  | 0 |  | Shuhui Wu, Yongliang Shen, Zeqi Tan, Wenqi Ren, Jietian Guo, Shiliang Pu, Weiming Lu |  |
| 1340 |  |  [The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions](https://doi.org/10.18653/v1/2023.emnlp-main.146) |  | 0 |  | Siru Ouyang, Shuohang Wang, Yang Liu, Ming Zhong, Yizhu Jiao, Dan Iter, Reid Pryzant, Chenguang Zhu, Heng Ji, Jiawei Han |  |
| 1341 |  |  [Learning the Visualness of Text Using Large Vision-Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.147) |  | 0 |  | Gaurav Verma, Ryan A. Rossi, Christopher Tensmeyer, Jiuxiang Gu, Ani Nenkova |  |
| 1342 |  |  [The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values](https://doi.org/10.18653/v1/2023.emnlp-main.148) |  | 0 |  | Hannah Kirk, Andrew M. Bean, Bertie Vidgen, Paul Röttger, Scott Hale |  |
| 1343 |  |  [TempTabQA: Temporal Question Answering for Semi-Structured Tables](https://doi.org/10.18653/v1/2023.emnlp-main.149) |  | 0 |  | Vivek Gupta, Pranshu Kandoi, Mahek Bhavesh Vora, Shuo Zhang, Yujie He, Ridho Reinanda, Vivek Srikumar |  |
| 1344 |  |  [Task-Level Thinking Steps Help Large Language Models for Challenging Classification Task](https://doi.org/10.18653/v1/2023.emnlp-main.150) |  | 0 |  | Chunhui Du, Jidong Tian, Haoran Liao, Jindou Chen, Hao He, Yaohui Jin |  |
| 1345 |  |  [RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation](https://doi.org/10.18653/v1/2023.emnlp-main.151) |  | 0 |  | Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, JianGuang Lou, Weizhu Chen |  |
| 1346 |  |  [Influence Scores at Scale for Efficient Language Data Sampling](https://doi.org/10.18653/v1/2023.emnlp-main.152) |  | 0 |  | Nikhil Anand, Joshua Tan, Maria Minakova |  |
| 1347 |  |  [G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment](https://doi.org/10.18653/v1/2023.emnlp-main.153) |  | 0 |  | Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu |  |
| 1348 |  |  [Learning Retrieval Augmentation for Personalized Dialogue Generation](https://doi.org/10.18653/v1/2023.emnlp-main.154) |  | 0 |  | Qiushi Huang, Shuai Fu, Xubo Liu, Wenwu Wang, Tom Ko, Yu Zhang, Lilian Tang |  |
| 1349 |  |  [The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations](https://doi.org/10.18653/v1/2023.emnlp-main.155) |  | 0 |  | Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S. M. Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das |  |
| 1350 |  |  [NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders](https://doi.org/10.18653/v1/2023.emnlp-main.156) |  | 0 |  | Livio Soares, Daniel Gillick, Jeremy R. Cole, Tom Kwiatkowski |  |
| 1351 |  |  [Analyzing Modular Approaches for Visual Question Decomposition](https://doi.org/10.18653/v1/2023.emnlp-main.157) |  | 0 |  | Apoorv Khandelwal, Ellie Pavlick, Chen Sun |  |
| 1352 |  |  [Improving Summarization with Human Edits](https://doi.org/10.18653/v1/2023.emnlp-main.158) |  | 0 |  | Zonghai Yao, Benjamin J. Schloss, Sai P. Selvaraj |  |
| 1353 |  |  [Did You Mean...? Confidence-based Trade-offs in Semantic Parsing](https://doi.org/10.18653/v1/2023.emnlp-main.159) |  | 0 |  | Elias StengelEskin, Benjamin Van Durme |  |
| 1354 |  |  [The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages](https://doi.org/10.18653/v1/2023.emnlp-main.160) |  | 0 |  | Chiyu Zhang, Khai Duy Doan, Qisheng Liao, Muhammad AbdulMageed |  |
| 1355 |  |  [Understanding the Effect of Model Compression on Social Bias in Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.161) |  | 0 |  | Gustavo Gonçalves, Emma Strubell |  |
| 1356 |  |  [BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology](https://doi.org/10.18653/v1/2023.emnlp-main.162) |  | 0 |  | Odhran O'Donoghue, Aleksandar Shtedritski, John Ginger, Ralph Abboud, Ali Essa Ghareeb, Samuel G. Rodriques |  |
| 1357 |  |  [Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages](https://doi.org/10.18653/v1/2023.emnlp-main.163) |  | 0 |  | Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, Wanxiang Che |  |
| 1358 |  |  [FinGPT: Large Generative Models for a Small Language](https://doi.org/10.18653/v1/2023.emnlp-main.164) |  | 0 |  | Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, HannaMari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, Thomas Wang, Nouamane Tazi, Teven Le Scao, Thomas Wolf, Osma Suominen, Samuli Sairanen, Mikko Merioksa, Jyrki Heinonen, Aija Vahtola, Samuel Antao, Sampo Pyysalo |  |
| 1359 |  |  [Boosting Summarization with Normalizing Flows and Aggressive Training](https://doi.org/10.18653/v1/2023.emnlp-main.165) |  | 0 |  | Yu Yang, Xiaotong Shen |  |
| 1360 |  |  [Indicative Summarization of Long Discussions](https://doi.org/10.18653/v1/2023.emnlp-main.166) |  | 0 |  | Shahbaz Syed, Dominik Schwabe, Khalid Al Khatib, Martin Potthast |  |
| 1361 |  |  [A Framework for Vision-Language Warm-up Tasks in Multimodal Dialogue Models](https://doi.org/10.18653/v1/2023.emnlp-main.167) |  | 0 |  | Jaewook Lee, Seongsik Park, SeongHeum Park, Hongjin Kim, Harksoo Kim |  |
| 1362 |  |  [Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair Modeling](https://doi.org/10.18653/v1/2023.emnlp-main.168) |  | 0 |  | Yuanhang Yang, Shiyi Qi, Chuanyi Liu, Qifan Wang, Cuiyun Gao, Zenglin Xu |  |
| 1363 |  |  [Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts](https://doi.org/10.18653/v1/2023.emnlp-main.169) |  | 0 |  | Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang |  |
| 1364 |  |  [GLEN: General-Purpose Event Detection for Thousands of Types](https://doi.org/10.18653/v1/2023.emnlp-main.170) |  | 0 |  | Sha Li, Qiusi Zhan, Kathryn Conger, Martha Palmer, Heng Ji, Jiawei Han |  |
| 1365 |  |  [Hierarchical Pretraining on Multimodal Electronic Health Records](https://doi.org/10.18653/v1/2023.emnlp-main.171) |  | 0 |  | Xiaochen Wang, Junyu Luo, Jiaqi Wang, Ziyi Yin, Suhan Cui, Yuan Zhong, Yaqing Wang, Fenglong Ma |  |
| 1366 |  |  [Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.172) |  | 0 |  | Mateusz Lango, Ondrej Dusek |  |
| 1367 |  |  [Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.173) |  | 0 |  | Wenyu Guo, Qingkai Fang, Dong Yu, Yang Feng |  |
| 1368 |  |  [DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.174) |  | 0 |  | Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, Deyi Xiong |  |
| 1369 |  |  [Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques](https://doi.org/10.18653/v1/2023.emnlp-main.175) |  | 0 |  | Manon Reusens, Philipp Borchert, Margot Mieskes, Jochen De Weerdt, Bart Baesens |  |
| 1370 |  |  [Can Language Models Laugh at YouTube Short-form Videos?](https://doi.org/10.18653/v1/2023.emnlp-main.176) |  | 0 |  | Dayoon Ko, Sangho Lee, Gunhee Kim |  |
| 1371 |  |  [Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation](https://doi.org/10.18653/v1/2023.emnlp-main.177) |  | 0 |  | Jiaang Li, Quan Wang, Yi Liu, Licheng Zhang, Zhendong Mao |  |
| 1372 |  |  [Exploring All-In-One Knowledge Distillation Framework for Neural Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.178) |  | 0 |  | Zhongjian Miao, Wen Zhang, Jinsong Su, Xiang Li, Jian Luan, Yidong Chen, Bin Wang, Min Zhang |  |
| 1373 |  |  [HistAlign: Improving Context Dependency in Language Generation by Aligning with History](https://doi.org/10.18653/v1/2023.emnlp-main.179) |  | 0 |  | David Wan, Shiyue Zhang, Mohit Bansal |  |
| 1374 |  |  [CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models](https://doi.org/10.18653/v1/2023.emnlp-main.180) |  | 0 |  | Aitor Ormazabal, Mikel Artetxe, Eneko Agirre |  |
| 1375 |  |  [Image Manipulation via Multi-Hop Instructions - A New Dataset and Weakly-Supervised Neuro-Symbolic Approach](https://doi.org/10.18653/v1/2023.emnlp-main.181) |  | 0 |  | Harman Singh, Poorva Garg, Mohit Gupta, Kevin Shah, Ashish Goswami, Satyam Modi, Arnab Kumar Mondal, Dinesh Khandelwal, Dinesh Garg, Parag Singla |  |
| 1376 |  |  [Generative Spoken Language Model based on continuous word-sized audio tokens](https://doi.org/10.18653/v1/2023.emnlp-main.182) |  | 0 |  | Robin Algayres, Yossi Adi, Tu Anh Nguyen, Jade Copet, Gabriel Synnaeve, Benoît Sagot, Emmanuel Dupoux |  |
| 1377 |  |  [Enhancing Chat Language Models by Scaling High-quality Instructional Conversations](https://doi.org/10.18653/v1/2023.emnlp-main.183) |  | 0 |  | Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, Bowen Zhou |  |
| 1378 |  |  [Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining](https://doi.org/10.18653/v1/2023.emnlp-main.184) |  | 0 |  | Emanuele Bugliarello, Aida Nematzadeh, Lisa Anne Hendricks |  |
| 1379 |  |  [Unsupervised Grammatical Error Correction Rivaling Supervised Methods](https://doi.org/10.18653/v1/2023.emnlp-main.185) |  | 0 |  | Hannan Cao, Liping Yuan, Yuchen Zhang, Hwee Tou Ng |  |
| 1380 |  |  [S2abEL: A Dataset for Entity Linking from Scientific Tables](https://doi.org/10.18653/v1/2023.emnlp-main.186) |  | 0 |  | Yuze Lou, Bailey Kuehl, Erin Bransom, Sergey Feldman, Aakanksha Naik, Doug Downey |  |
| 1381 |  |  [API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.187) |  | 0 |  | Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li |  |
| 1382 |  |  [Language and Mental Health: Measures of Emotion Dynamics from Text as Linguistic Biosocial Markers](https://doi.org/10.18653/v1/2023.emnlp-main.188) |  | 0 |  | Daniela Teodorescu, Tiffany Cheng, Alona Fyshe, Saif M. Mohammad |  |
| 1383 |  |  [Lion: Adversarial Distillation of Proprietary Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.189) |  | 0 |  | Yuxin Jiang, Chunkit Chan, Mingyang Chen, Wei Wang |  |
| 1384 |  |  [Evaluating Large Language Models on Controlled Generation Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.190) |  | 0 |  | Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John Frederick Wieting, Nanyun Peng, Xuezhe Ma |  |
| 1385 |  |  [DeSIQ: Towards an Unbiased, Challenging Benchmark for Social Intelligence Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.191) |  | 0 |  | Xiaoyu Guo, YuanFang Li, Gholamreza Haffari |  |
| 1386 |  |  [Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation](https://doi.org/10.18653/v1/2023.emnlp-main.192) |  | 0 |  | Adam Bouyamourn |  |
| 1387 |  |  [A Question Answering Framework for Decontextualizing User-facing Snippets from Scientific Documents](https://doi.org/10.18653/v1/2023.emnlp-main.193) |  | 0 |  | Benjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, Kyle Lo |  |
| 1388 |  |  [SLOG: A Structural Generalization Benchmark for Semantic Parsing](https://doi.org/10.18653/v1/2023.emnlp-main.194) |  | 0 |  | Bingzhi Li, Lucia Donatelli, Alexander Koller, Tal Linzen, Yuekun Yao, Najoung Kim |  |
| 1389 |  |  [Pushdown Layers: Encoding Recursive Structure in Transformer Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.195) |  | 0 |  | Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher D. Manning |  |
| 1390 |  |  [Can LLMs Facilitate Interpretation of Pre-trained Language Models?](https://doi.org/10.18653/v1/2023.emnlp-main.196) |  | 0 |  | Basel Mousi, Nadir Durrani, Fahim Dalvi |  |
| 1391 |  |  [Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets](https://doi.org/10.18653/v1/2023.emnlp-main.197) |  | 0 |  | Su Ah Lee, Seokjin Oh, Woohwan Jung |  |
| 1392 |  |  [Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies](https://doi.org/10.18653/v1/2023.emnlp-main.198) |  | 0 |  | Zhengxuan Wu, Alex Tamkin, Isabel Papadimitriou |  |
| 1393 |  |  [Non-Autoregressive Math Word Problem Solver with Unified Tree Structure](https://doi.org/10.18653/v1/2023.emnlp-main.199) |  | 0 |  | Yi Bin, Mengqun Han, Wenhao Shi, Lei Wang, Yang Yang, SeeKiong Ng, Heng Tao Shen |  |
| 1394 |  |  [Improving Chinese Pop Song and Hokkien Gezi Opera Singing Voice Synthesis by Enhancing Local Modeling](https://doi.org/10.18653/v1/2023.emnlp-main.200) |  | 0 |  | Peng Bai, Yue Zhou, Meizhen Zheng, Wujin Sun, Xiaodong Shi |  |
| 1395 |  |  [What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on QA Systems](https://doi.org/10.18653/v1/2023.emnlp-main.201) |  | 0 |  | Navita Goyal, Eleftheria Briakou, Amanda Liu, Connor Baumler, Claire Bonial, Jeffrey Micher, Clare R. Voss, Marine Carpuat, Hal Daumé III |  |
| 1396 |  |  [GROOViST: A Metric for Grounding Objects in Visual Storytelling](https://doi.org/10.18653/v1/2023.emnlp-main.202) |  | 0 |  | Aditya K. Surikuchi, Sandro Pezzelle, Raquel Fernández |  |
| 1397 |  |  [VIBE: Topic-Driven Temporal Adaptation for Twitter Classification](https://doi.org/10.18653/v1/2023.emnlp-main.203) |  | 0 |  | Yuji Zhang, Jing Li, Wenjie Li |  |
| 1398 |  |  [TOD-Flow: Modeling the Structure of Task-Oriented Dialogues](https://doi.org/10.18653/v1/2023.emnlp-main.204) |  | 0 |  | Sungryull Sohn, Yiwei Lyu, Anthony Z. Liu, Lajanugen Logeswaran, DongKi Kim, Dongsub Shim, Honglak Lee |  |
| 1399 |  |  [TopWORDS-Poetry: Simultaneous Text Segmentation and Word Discovery for Classical Chinese Poetry via Bayesian Inference](https://doi.org/10.18653/v1/2023.emnlp-main.205) |  | 0 |  | Changzai Pan, Feiyue Li, Ke Deng |  |
| 1400 |  |  [Knowledge Rumination for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.206) |  | 0 |  | Yunzhi Yao, Peng Wang, Shengyu Mao, Chuanqi Tan, Fei Huang, Huajun Chen, Ningyu Zhang |  |
| 1401 |  |  [Struct-XLM: A Structure Discovery Multilingual Language Model for Enhancing Cross-lingual Transfer through Reinforcement Learning](https://doi.org/10.18653/v1/2023.emnlp-main.207) |  | 0 |  | Linjuan Wu, Weiming Lu |  |
| 1402 |  |  [AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification](https://doi.org/10.18653/v1/2023.emnlp-main.208) |  | 0 |  | Yongxin Huang, Kexin Wang, Sourav Dutta, Raj Nath Patel, Goran Glavas, Iryna Gurevych |  |
| 1403 |  |  [Interview Evaluation: A Novel Approach for Automatic Evaluation of Conversational Question Answering Models](https://doi.org/10.18653/v1/2023.emnlp-main.209) |  | 0 |  | Xibo Li, Bowei Zou, Yifan Fan, Yanling Li, Ai Ti Aw, Yu Hong |  |
| 1404 |  |  [TCFLE-8: a Corpus of Learner Written Productions for French as a Foreign Language and its Application to Automated Essay Scoring](https://doi.org/10.18653/v1/2023.emnlp-main.210) |  | 0 |  | Rodrigo Wilkens, Alice Pintard, David Alfter, Vincent Folny, Thomas François |  |
| 1405 |  |  [Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA](https://doi.org/10.18653/v1/2023.emnlp-main.211) |  | 0 |  | David Heineman, Yao Dou, Mounica Maddela, Wei Xu |  |
| 1406 |  |  [Confidence-based Ensembling of Perspective-aware Models](https://doi.org/10.18653/v1/2023.emnlp-main.212) |  | 0 |  | Silvia Casola, Soda Marem Lo, Valerio Basile, Simona Frenda, Alessandra Teresa Cignarella, Viviana Patti, Cristina Bosco |  |
| 1407 |  |  [ToViLaG: Your Visual-Language Generative Model is Also An Evildoer](https://doi.org/10.18653/v1/2023.emnlp-main.213) |  | 0 |  | Xinpeng Wang, Xiaoyuan Yi, Han Jiang, Shanlin Zhou, Zhihua Wei, Xing Xie |  |
| 1408 |  |  [GPT-RE: In-context Learning for Relation Extraction using Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.214) |  | 0 |  | Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song, Jiwei Li, Sadao Kurohashi |  |
| 1409 |  |  [Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment](https://doi.org/10.18653/v1/2023.emnlp-main.215) |  | 0 |  | Sky CHWang, Arkadiy Saakyan, Oliver Li, Zhou Yu, Smaranda Muresan |  |
| 1410 |  |  [INFORM : Information eNtropy based multi-step reasoning FOR large language Models](https://doi.org/10.18653/v1/2023.emnlp-main.216) |  | 0 |  | Chuyue Zhou, Wangjie You, Juntao Li, Jing Ye, Kehai Chen, Min Zhang |  |
| 1411 |  |  [Adaptive Gating in Mixture-of-Experts based Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.217) |  | 0 |  | Jiamin Li, Qiang Su, Yitao Yang, Yimin Jiang, Cong Wang, Hong Xu |  |
| 1412 |  |  [On the Automatic Generation and Simplification of Children's Stories](https://doi.org/10.18653/v1/2023.emnlp-main.218) |  | 0 |  | Maria R. Valentini, Jennifer Weber, Jesus Salcido, Téa Wright, Eliana Colunga, Katharina von der Wense |  |
| 1413 |  |  [When Do Decompositions Help for Machine Reading?](https://doi.org/10.18653/v1/2023.emnlp-main.219) |  | 0 |  | Kangda Wei, Dawn J. Lawrie, Benjamin Van Durme, Yunmo Chen, Orion Weller |  |
| 1414 |  |  [The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.220) |  | 0 |  | Aviv Slobodkin, Omer Goldman, Avi Caciularu, Ido Dagan, Shauli Ravfogel |  |
| 1415 |  |  [Identifying Informational Sources in News Articles](https://doi.org/10.18653/v1/2023.emnlp-main.221) |  | 0 |  | Alexander Spangher, Nanyun Peng, Emilio Ferrara, Jonathan May |  |
| 1416 |  |  [Retrofitting Light-weight Language Models for Emotions using Supervised Contrastive Learning](https://doi.org/10.18653/v1/2023.emnlp-main.222) |  | 0 |  | Sapan Shah, Sreedhar Reddy, Pushpak Bhattacharyya |  |
| 1417 |  |  [Longtriever: a Pre-trained Long Text Encoder for Dense Document Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.223) |  | 0 |  | Junhan Yang, Zheng Liu, Chaozhuo Li, Guangzhong Sun, Xing Xie |  |
| 1418 |  |  [Revisiting De-Identification of Electronic Medical Records: Evaluation of Within- and Cross-Hospital Generalization](https://doi.org/10.18653/v1/2023.emnlp-main.224) |  | 0 |  | Yiyang Liu, Jinpeng Li, Enwei Zhu |  |
| 1419 |  |  [Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.225) |  | 0 |  | Gurusha Juneja, Subhabrata Dutta, Soumen Chakrabarti, Sunny Manchanda, Tanmoy Chakraborty |  |
| 1420 |  |  [Language Representation Projection: Can We Transfer Factual Knowledge across Languages in Multilingual Language Models?](https://doi.org/10.18653/v1/2023.emnlp-main.226) |  | 0 |  | Shaoyang Xu, Junzhuo Li, Deyi Xiong |  |
| 1421 |  |  [Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.227) |  | 0 |  | James A. Michaelov, Catherine Arnett, Tyler A. Chang, Ben Bergen |  |
| 1422 |  |  [ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph](https://doi.org/10.18653/v1/2023.emnlp-main.228) |  | 0 |  | Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yaliang Li, JiRong Wen |  |
| 1423 |  |  [Deep Natural Language Feature Learning for Interpretable Prediction](https://doi.org/10.18653/v1/2023.emnlp-main.229) |  | 0 |  | Felipe Urrutia, Cristian Buc Calderon, Valentin Barrière |  |
| 1424 |  |  [ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.230) |  | 0 |  | David Esiobu, Xiaoqing Ellen Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude Fernandes, Jane DwivediYu, Eleonora Presani, Adina Williams, Eric Michael Smith |  |
| 1425 |  |  [Enhancing Task-oriented Dialogue Systems with Generative Post-processing Networks](https://doi.org/10.18653/v1/2023.emnlp-main.231) |  | 0 |  | Atsumoto Ohashi, Ryuichiro Higashinaka |  |
| 1426 |  |  [Adapting Language Models to Compress Contexts](https://doi.org/10.18653/v1/2023.emnlp-main.232) |  | 0 |  | Alexis Chevalier, Alexander Wettig, Anirudh Ajith, Danqi Chen |  |
| 1427 |  |  [Selective Labeling: How to Radically Lower Data-Labeling Costs for Document Extraction Models](https://doi.org/10.18653/v1/2023.emnlp-main.233) |  | 0 |  | Yichao Zhou, James B. Wendt, Navneet Potti, Jing Xie, Sandeep Tata |  |
| 1428 |  |  [TRAVEL: Tag-Aware Conversational FAQ Retrieval via Reinforcement Learning](https://doi.org/10.18653/v1/2023.emnlp-main.234) |  | 0 |  | Yue Chen, Dingnan Jin, Chen Huang, Jia Liu, Wenqiang Lei |  |
| 1429 |  |  [Continual Dialogue State Tracking via Example-Guided Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.235) |  | 0 |  | Hyundong Cho, Andrea Madotto, Zhaojiang Lin, Khyathi Raghavi Chandu, Satwik Kottur, Jing Xu, Jonathan May, Chinnadhurai Sankar |  |
| 1430 |  |  [Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media](https://doi.org/10.18653/v1/2023.emnlp-main.236) |  | 0 |  | Shubham Mittal, Megha Sundriyal, Preslav Nakov |  |
| 1431 |  |  [COVID-19 Vaccine Misinformation in Middle Income Countries](https://doi.org/10.18653/v1/2023.emnlp-main.237) |  | 0 |  | Jongin Kim, Byeo Bak, Aditya Agrawal, Jiaxi Wu, Veronika J. Wirtz, Traci Hong, Derry Wijaya |  |
| 1432 |  |  [Contrastive Learning of Sentence Embeddings from Scratch](https://doi.org/10.18653/v1/2023.emnlp-main.238) |  | 0 |  | Junlei Zhang, Zhenzhong Lan, Junxian He |  |
| 1433 |  |  [A Rose by Any Other Name would not Smell as Sweet: Social Bias in Names Mistranslation](https://doi.org/10.18653/v1/2023.emnlp-main.239) |  | 0 |  | Sandra Sandoval, Jieyu Zhao, Marine Carpuat, Hal Daumé III |  |
| 1434 |  |  [Investigating Efficiently Extending Transformers for Long Input Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.240) |  | 0 |  | Jason Phang, Yao Zhao, Peter J. Liu |  |
| 1435 |  |  [CS2W: A Chinese Spoken-to-Written Style Conversion Dataset with Multiple Conversion Types](https://doi.org/10.18653/v1/2023.emnlp-main.241) |  | 0 |  | Zishan Guo, Linhao Yu, Minghui Xu, Renren Jin, Deyi Xiong |  |
| 1436 |  |  [Unifying Cross-Lingual Transfer across Scenarios of Resource Scarcity](https://doi.org/10.18653/v1/2023.emnlp-main.242) |  | 0 |  | Alan Ansell, Marinela Parovic, Ivan Vulic, Anna Korhonen, Edoardo M. Ponti |  |
| 1437 |  |  [A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.243) |  | 0 |  | Giuseppe Attanasio, Flor Miriam Plaza del Arco, Debora Nozza, Anne Lauscher |  |
| 1438 |  |  [DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining](https://doi.org/10.18653/v1/2023.emnlp-main.244) |  | 0 |  | Weifeng Jiang, Qianren Mao, Chenghua Lin, Jianxin Li, Ting Deng, Weiyi Yang, Zheng Wang |  |
| 1439 |  |  [Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation](https://doi.org/10.18653/v1/2023.emnlp-main.245) |  | 0 |  | Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han, KaiWei Chang |  |
| 1440 |  |  [Are All Steps Equally Important? Benchmarking Essentiality Detection in Event Processes](https://doi.org/10.18653/v1/2023.emnlp-main.246) |  | 0 |  | Haoyu Wang, Hongming Zhang, Yueguan Wang, Yuqian Deng, Muhao Chen, Dan Roth |  |
| 1441 |  |  [Language Model is Suitable for Correction of Handwritten Mathematical Expressions Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.247) |  | 0 |  | Zui Chen, Jiaqi Han, Chaofan Yang, Yi Zhou |  |
| 1442 |  |  [Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection](https://doi.org/10.18653/v1/2023.emnlp-main.248) |  | 0 |  | Gretel Liz De la Peña Sarracén, Paolo Rosso, Robert Litschko, Goran Glavas, Simone Paolo Ponzetto |  |
| 1443 |  |  [SuperDialseg: A Large-scale Dataset for Supervised Dialogue Segmentation](https://doi.org/10.18653/v1/2023.emnlp-main.249) |  | 0 |  | Junfeng Jiang, Chengzhang Dong, Sadao Kurohashi, Akiko Aizawa |  |
| 1444 |  |  [ATFormer: A Learned Performance Model with Transfer Learning Across Devices for Deep Learning Tensor Programs](https://doi.org/10.18653/v1/2023.emnlp-main.250) |  | 0 |  | Yang Bai, Wenqian Zhao, Shuo Yin, Zixiao Wang, Bei Yu |  |
| 1445 |  |  [mRedditSum: A Multimodal Abstractive Summarization Dataset of Reddit Threads with Images](https://doi.org/10.18653/v1/2023.emnlp-main.251) |  | 0 |  | Keighley Overbay, Jaewoo Ahn, Fatemeh Pesaran Zadeh, Joonsuk Park, Gunhee Kim |  |
| 1446 |  |  [Sparse Low-rank Adaptation of Pre-trained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.252) |  | 0 |  | Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, Maosong Sun |  |
| 1447 |  |  [Human Learning by Model Feedback: The Dynamics of Iterative Prompting with Midjourney](https://doi.org/10.18653/v1/2023.emnlp-main.253) |  | 0 |  | Shachar DonYehiya, Leshem Choshen, Omri Abend |  |
| 1448 |  |  [ULF: Unsupervised Labeling Function Correction using Cross-Validation for Weak Supervision](https://doi.org/10.18653/v1/2023.emnlp-main.254) |  | 0 |  | Anastasiia Sedova, Benjamin Roth |  |
| 1449 |  |  [The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.255) |  | 0 |  | Jingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu, Di Jin, Qifan Wang, Lifu Huang |  |
| 1450 |  |  [Ideology Takes Multiple Looks: A High-Quality Dataset for Multifaceted Ideology Detection](https://doi.org/10.18653/v1/2023.emnlp-main.256) |  | 0 |  | Songtao Liu, Ziling Luo, Minghua Xu, Lixiao Wei, Ziyao Wei, Han Yu, Wei Xiang, Bang Wang |  |
| 1451 |  |  [Transductive Learning for Textual Few-Shot Classification in API-based Embedding Models](https://doi.org/10.18653/v1/2023.emnlp-main.257) |  | 0 |  | Pierre Colombo, Victor Pellegrain, Malik Boudiaf, Myriam Tami, Victor Storchan, Ismail Ben Ayed, Pablo Piantanida |  |
| 1452 |  |  [MEGA: Multilingual Evaluation of Generative AI](https://doi.org/10.18653/v1/2023.emnlp-main.258) |  | 0 |  | Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Uttama Nambi, Tanuja Ganu, Sameer Segal, Mohamed Ahmed, Kalika Bali, Sunayana Sitaram |  |
| 1453 |  |  [Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation](https://doi.org/10.18653/v1/2023.emnlp-main.259) |  | 0 |  | Xin Yuan, Jie Guo, Weidong Qiu, Zheng Huang, Shujun Li |  |
| 1454 |  |  [Video-Helpful Multimodal Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.260) |  | 0 |  | Yihang Li, Shuichiro Shimizu, Chenhui Chu, Sadao Kurohashi, Wei Li |  |
| 1455 |  |  [Large Language Models are Temporal and Causal Reasoners for Video Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.261) |  | 0 |  | Dohwan Ko, Ji Soo Lee, WooYoung Kang, Byungseok Roh, Hyunwoo Kim |  |
| 1456 |  |  [Uncertainty Guided Global Memory Improves Multi-Hop Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.262) |  | 0 |  | Alsu Sagirova, Mikhail Burtsev |  |
| 1457 |  |  [Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation](https://doi.org/10.18653/v1/2023.emnlp-main.263) |  | 0 |  | Yuanyuan Liang, Jianing Wang, Hanlun Zhu, Lei Wang, Weining Qian, Yunshi Lan |  |
| 1458 |  |  [TrojanSQL: SQL Injection against Natural Language Interface to Database](https://doi.org/10.18653/v1/2023.emnlp-main.264) |  | 0 |  | Jinchuan Zhang, Yan Zhou, Binyuan Hui, Yaxin Liu, Ziming Li, Songlin Hu |  |
| 1459 |  |  [Preserving Privacy Through Dememorization: An Unlearning Technique For Mitigating Memorization Risks In Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.265) |  | 0 |  | Aly M. Kassem, Omar Mahmoud, Sherif Saad |  |
| 1460 |  |  [MingOfficial: A Ming Official Career Dataset and a Historical Context-Aware Representation Learning Framework](https://doi.org/10.18653/v1/2023.emnlp-main.266) |  | 0 |  | YouJun Chen, HsinYi Hsieh, Yu Lin, Yingtao Tian, Bert Chan, YuSin Liu, YiHsuan Lin, Richard TzongHan Tsai |  |
| 1461 |  |  [DPP-TTS: Diversifying prosodic features of speech via determinantal point processes](https://doi.org/10.18653/v1/2023.emnlp-main.267) |  | 0 |  | Seongho Joo, Hyukhun Koh, Kyomin Jung |  |
| 1462 |  |  [Meta-Learning Online Adaptation of Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.268) |  | 0 |  | Nathan Hu, Eric Mitchell, Christopher D. Manning, Chelsea Finn |  |
| 1463 |  |  [Self-Detoxifying Language Models via Toxification Reversal](https://doi.org/10.18653/v1/2023.emnlp-main.269) |  | 0 |  | Chak Tou Leong, Yi Cheng, Jiashuo Wang, Jian Wang, Wenjie Li |  |
| 1464 |  |  [Interactive Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.270) |  | 0 |  | Felix Faltings, Michel Galley, Kianté Brantley, Baolin Peng, Weixin Cai, Yizhe Zhang, Jianfeng Gao, Bill Dolan |  |
| 1465 |  |  [Knowledge Distillation \approx Label Smoothing: Fact or Fallacy?](https://doi.org/10.18653/v1/2023.emnlp-main.271) |  | 0 |  | Md. Sultan |  |
| 1466 |  |  [Analyzing Cognitive Plausibility of Subword Tokenization](https://doi.org/10.18653/v1/2023.emnlp-main.272) |  | 0 |  | Lisa Beinborn, Yuval Pinter |  |
| 1467 |  |  [POE: Process of Elimination for Multiple Choice Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.273) |  | 0 |  | Chenkai Ma, Xinya Du |  |
| 1468 |  |  [NeuSTIP: A Neuro-Symbolic Model for Link and Time Prediction in Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.274) |  | 0 |  | Ishaan Singh, Navdeep Kaur, Garima Gaur, Mausam |  |
| 1469 |  |  [Standardizing Distress Analysis: Emotion-Driven Distress Identification and Cause Extraction (DICE) in Multimodal Online Posts](https://doi.org/10.18653/v1/2023.emnlp-main.275) |  | 0 |  | Gopendra Vikram Singh, Soumitra Ghosh, Atul Verma, Chetna Painkra, Asif Ekbal |  |
| 1470 |  |  [Out-of-Distribution Generalization in Natural Language Processing: Past, Present, and Future](https://doi.org/10.18653/v1/2023.emnlp-main.276) |  | 0 |  | Linyi Yang, Yaoxian Song, Xuan Ren, Chenyang Lyu, Yidong Wang, Jingming Zhuo, Lingqiao Liu, Jindong Wang, Jennifer Foster, Yue Zhang |  |
| 1471 |  |  [Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.277) |  | 0 |  | Hongyi Zheng, Abulhair Saparov |  |
| 1472 |  |  [Can Large Language Models Capture Dissenting Human Voices?](https://doi.org/10.18653/v1/2023.emnlp-main.278) |  | 0 |  | Noah Lee, Na An, James Thorne |  |
| 1473 |  |  [DecoMT: Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.279) |  | 0 |  | Ratish Puduppully, Anoop Kunchukuttan, Raj Dabre, Ai Ti Aw, Nancy Chen |  |
| 1474 |  |  [Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.280) |  | 0 |  | Hao Zhao, Jie Fu, Zhaofeng He |  |
| 1475 |  |  [Towards Building More Robust NER datasets: An Empirical Study on NER Dataset Bias from a Dataset Difficulty View](https://doi.org/10.18653/v1/2023.emnlp-main.281) |  | 0 |  | Ruotian Ma, Xiaolei Wang, Xin Zhou, Qi Zhang, Xuanjing Huang |  |
| 1476 |  |  [GradSim: Gradient-Based Language Grouping for Effective Multilingual Training](https://doi.org/10.18653/v1/2023.emnlp-main.282) |  | 0 |  | Mingyang Wang, Heike Adel, Lukas Lange, Jannik Strötgen, Hinrich Schütze |  |
| 1477 |  |  [Discovering Universal Geometry in Embeddings with ICA](https://doi.org/10.18653/v1/2023.emnlp-main.283) |  | 0 |  | Hiroaki Yamagiwa, Momose Oyama, Hidetoshi Shimodaira |  |
| 1478 |  |  [Toward a Critical Toponymy Framework for Named Entity Recognition: A Case Study of Airbnb in New York City](https://doi.org/10.18653/v1/2023.emnlp-main.284) |  | 0 |  | Mikael Brunila, Jack LaViolette, Sky CHWang, Priyanka Verma, Clara Féré, Grant McKenzie |  |
| 1479 |  |  [Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.285) |  | 0 |  | Lang Qin, Yao Zhang, Hongru Liang, Jun Wang, Zhenglu Yang |  |
| 1480 |  |  [Merging Generated and Retrieved Knowledge for Open-Domain QA](https://doi.org/10.18653/v1/2023.emnlp-main.286) |  | 0 |  | Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Lu Wang |  |
| 1481 |  |  [Best of Both Worlds: Towards Improving Temporal Knowledge Base Question Answering via Targeted Fact Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.287) |  | 0 |  | Nithish Kannen, Udit Sharma, Sumit Neelam, Dinesh Khandelwal, Shajith Ikbal, Hima Karanam, L. Venkata Subramaniam |  |
| 1482 |  |  [Text Fact Transfer](https://doi.org/10.18653/v1/2023.emnlp-main.288) |  | 0 |  | Nishant Balepur, Jie Huang, Kevin ChenChuan Chang |  |
| 1483 |  |  [A Cheaper and Better Diffusion Language Model with Soft-Masked Noise](https://doi.org/10.18653/v1/2023.emnlp-main.289) |  | 0 |  | Jiaao Chen, Aston Zhang, Mu Li, Alex Smola, Diyi Yang |  |
| 1484 |  |  [Mirages. On Anthropomorphism in Dialogue Systems](https://doi.org/10.18653/v1/2023.emnlp-main.290) |  | 0 |  | Gavin Abercrombie, Amanda Cercas Curry, Tanvi Dinkar, Verena Rieser, Zeerak Talat |  |
| 1485 |  |  [Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?](https://doi.org/10.18653/v1/2023.emnlp-main.291) |  | 0 |  | Kevin Liu, Stephen Casper, Dylan HadfieldMenell, Jacob Andreas |  |
| 1486 |  |  [KEBAP: Korean Error Explainable Benchmark Dataset for ASR and Post-processing](https://doi.org/10.18653/v1/2023.emnlp-main.292) |  | 0 |  | Seonmin Koo, Chanjun Park, Jinsung Kim, Jaehyung Seo, Sugyeong Eo, Hyeonseok Moon, Heuiseok Lim |  |
| 1487 |  |  [Adaptive Policy with Wait-k Model for Simultaneous Translation](https://doi.org/10.18653/v1/2023.emnlp-main.293) |  | 0 |  | Libo Zhao, Kai Fan, Wei Luo, Jing Wu, Shushu Wang, Ziqian Zeng, Zhongqiang Huang |  |
| 1488 |  |  [Cross-Document Event Coreference Resolution on Discourse Structure](https://doi.org/10.18653/v1/2023.emnlp-main.294) |  | 0 |  | Xinyu Chen, Sheng Xu, Peifeng Li, Qiaoming Zhu |  |
| 1489 |  |  [Post-hoc Utterance Refining Method by Entity Mining for Faithful Knowledge Grounded Conversations](https://doi.org/10.18653/v1/2023.emnlp-main.295) |  | 0 |  | Yoonna Jang, Suhyune Son, Jeongwoo Lee, Junyoung Son, Yuna Hur, Jungwoo Lim, Hyeonseok Moon, Kisu Yang, Heuiseok Lim |  |
| 1490 |  |  [Can We Edit Factual Knowledge by In-Context Learning?](https://doi.org/10.18653/v1/2023.emnlp-main.296) |  | 0 |  | Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, Baobao Chang |  |
| 1491 |  |  [EDIS: Entity-Driven Image Search over Multimodal Web Content](https://doi.org/10.18653/v1/2023.emnlp-main.297) |  | 0 |  | Siqi Liu, Weixi Feng, TsuJui Fu, Wenhu Chen, William Wang |  |
| 1492 |  |  [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://doi.org/10.18653/v1/2023.emnlp-main.298) |  | 0 |  | Joshua Ainslie, James LeeThorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, Sumit Sanghai |  |
| 1493 |  |  [Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.299) |  | 0 |  | Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, Mrinmaya Sachan |  |
| 1494 |  |  [BiasX: "Thinking Slow" in Toxic Content Moderation with Explanations of Implied Social Biases](https://doi.org/10.18653/v1/2023.emnlp-main.300) |  | 0 |  | Yiming Zhang, Sravani Nanduri, Liwei Jiang, Tongshuang Wu, Maarten Sap |  |
| 1495 |  |  [Text encoders bottleneck compositionality in contrastive vision-language models](https://doi.org/10.18653/v1/2023.emnlp-main.301) |  | 0 |  | Amita Kamath, Jack Hessel, KaiWei Chang |  |
| 1496 |  |  [Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition](https://doi.org/10.18653/v1/2023.emnlp-main.302) |  | 0 |  | Sander Schulhoff, Jeremy Pinto, Anaum Khan, LouisFrançois Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, Christopher Carnahan, Jordan L. BoydGraber |  |
| 1497 |  |  [MMNMT: Modularizing Multilingual Neural Machine Translation with Flexibly Assembled MoE and Dense Blocks](https://doi.org/10.18653/v1/2023.emnlp-main.303) |  | 0 |  | Shangjie Li, Xiangpeng Wei, Shaolin Zhu, Jun Xie, Baosong Yang, Deyi Xiong |  |
| 1498 |  |  [Localizing Active Objects from Egocentric Vision with Symbolic World Knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.304) |  | 0 |  | TeLin Wu, Yu Zhou, Nanyun Peng |  |
| 1499 |  |  [Introducing Rhetorical Parallelism Detection: A New Task with Datasets, Metrics, and Baselines](https://doi.org/10.18653/v1/2023.emnlp-main.305) |  | 0 |  | Stephen Bothwell, Justin DeBenedetto, Theresa Crnkovich, Hildegund Müller, David Chiang |  |
| 1500 |  |  [Prompting is not a substitute for probability measurements in large language models](https://doi.org/10.18653/v1/2023.emnlp-main.306) |  | 0 |  | Jennifer Hu, Roger Levy |  |
| 1501 |  |  [Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings](https://doi.org/10.18653/v1/2023.emnlp-main.307) |  | 0 |  | Josip Jukic, Jan Snajder |  |
| 1502 |  |  [Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks](https://doi.org/10.18653/v1/2023.emnlp-main.308) |  | 0 |  | Alon Jacovi, Avi Caciularu, Omer Goldman, Yoav Goldberg |  |
| 1503 |  |  [CoLT5: Faster Long-Range Transformers with Conditional Computation](https://doi.org/10.18653/v1/2023.emnlp-main.309) |  | 0 |  | Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontañón, Siddhartha Brahma, Yury Zemlyanskiy, David C. Uthus, Mandy Guo, James LeeThorp, Yi Tay, YunHsuan Sung, Sumit Sanghai |  |
| 1504 |  |  [DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.310) |  | 0 |  | Praveen Venkateswaran, Evelyn Duesterwald, Vatche Isahagian |  |
| 1505 |  |  [Cross-Cultural Analysis of Human Values, Morals, and Biases in Folk Tales](https://doi.org/10.18653/v1/2023.emnlp-main.311) |  | 0 |  | Winston Wu, Lu Wang, Rada Mihalcea |  |
| 1506 |  |  [Non-Programmers Can Label Programs Indirectly via Active Examples: A Case Study with Text-to-SQL](https://doi.org/10.18653/v1/2023.emnlp-main.312) |  | 0 |  | Ruiqi Zhong, Charlie Snell, Dan Klein, Jason Eisner |  |
| 1507 |  |  [LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers](https://doi.org/10.18653/v1/2023.emnlp-main.313) |  | 0 |  | Theo Olausson, Alex Gu, Benjamin Lipkin, Cedegao E. Zhang, Armando SolarLezama, Joshua B. Tenenbaum, Roger Levy |  |
| 1508 |  |  [Non-autoregressive Streaming Transformer for Simultaneous Translation](https://doi.org/10.18653/v1/2023.emnlp-main.314) |  | 0 |  | Zhengrui Ma, Shaolei Zhang, Shoutao Guo, Chenze Shao, Min Zhang, Yang Feng |  |
| 1509 |  |  [ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing](https://doi.org/10.18653/v1/2023.emnlp-main.315) |  | 0 |  | Nam Nguyen, Thang Phan, DucVu Nguyen, Kiet Van Nguyen |  |
| 1510 |  |  [RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.316) |  | 0 |  | Shiao Meng, Xuming Hu, Aiwei Liu, Shuang Li, Fukun Ma, Yawen Yang, Lijie Wen |  |
| 1511 |  |  [GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.317) |  | 0 |  | Zekun Li, Wenxuan Zhou, YaoYi Chiang, Muhao Chen |  |
| 1512 |  |  [Cross-Modal Conceptualization in Bottleneck Models](https://doi.org/10.18653/v1/2023.emnlp-main.318) |  | 0 |  | Danis Alukaev, Semen Kiselev, Ilya Pershin, Bulat Ibragimov, Vladimir Ivanov, Alexey Kornaev, Ivan Titov |  |
| 1513 |  |  [LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.319) |  | 0 |  | Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, EePeng Lim, Lidong Bing, Xing Xu, Soujanya Poria, Roy KaWei Lee |  |
| 1514 |  |  [DREAM: Deployment of Recombination and Ensembles in Argument Mining](https://doi.org/10.18653/v1/2023.emnlp-main.320) |  | 0 |  | Florian Ruosch, Cristina Sarasua, Abraham Bernstein |  |
| 1515 |  |  [MILDSum: A Novel Benchmark Dataset for Multilingual Summarization of Indian Legal Case Judgments](https://doi.org/10.18653/v1/2023.emnlp-main.321) |  | 0 |  | Debtanu Datta, Shubham Soni, Rajdeep Mukherjee, Saptarshi Ghosh |  |
| 1516 |  |  [Query Rewriting in Retrieval-Augmented Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.322) |  | 0 |  | Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan |  |
| 1517 |  |  [PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation](https://doi.org/10.18653/v1/2023.emnlp-main.323) |  | 0 |  | Gaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, Issam H. Laradji |  |
| 1518 |  |  [COHESENTIA: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts](https://doi.org/10.18653/v1/2023.emnlp-main.324) |  | 0 |  | Aviya Maimon, Reut Tsarfaty |  |
| 1519 |  |  [QUDeval: The Evaluation of Questions Under Discussion Discourse Parsing](https://doi.org/10.18653/v1/2023.emnlp-main.325) |  | 0 |  | Yating Wu, Ritika Mangla, Greg Durrett, Junyi Jessy Li |  |
| 1520 |  |  [PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter](https://doi.org/10.18653/v1/2023.emnlp-main.326) |  | 0 |  | Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, Jing Xiao |  |
| 1521 |  |  [Exploring Chain of Thought Style Prompting for Text-to-SQL](https://doi.org/10.18653/v1/2023.emnlp-main.327) |  | 0 |  | ChangYu Tai, Ziru Chen, Tianshu Zhang, Xiang Deng, Huan Sun |  |
| 1522 |  |  [Efficient Algorithms for Recognizing Weighted Tree-Adjoining Languages](https://doi.org/10.18653/v1/2023.emnlp-main.328) |  | 0 |  | Alexandra Butoi, Tim Vieira, Ryan Cotterell, David Chiang |  |
| 1523 |  |  [Harnessing Black-Box Control to Boost Commonsense in LM's Generation](https://doi.org/10.18653/v1/2023.emnlp-main.329) |  | 0 |  | Yufei Tian, Felix Zhang, Nanyun Peng |  |
| 1524 |  |  [Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.330) |  | 0 |  | Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher D. Manning |  |
| 1525 |  |  [Representative Demonstration Selection for In-Context Learning with Two-Stage Determinantal Point Process](https://doi.org/10.18653/v1/2023.emnlp-main.331) |  | 0 |  | Zhao Yang, Yuanzhe Zhang, Dianbo Sui, Cao Liu, Jun Zhao, Kang Liu |  |
| 1526 |  |  [The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.332) |  | 0 |  | Lovisa Hagström, Denitsa Saynova, Tobias Norlund, Moa Johansson, Richard Johansson |  |
| 1527 |  |  [ViPE: Visualise Pretty-much Everything](https://doi.org/10.18653/v1/2023.emnlp-main.333) |  | 0 |  | Hassan Shahmohammadi, Adhiraj Ghosh, Hendrik P. A. Lensch |  |
| 1528 |  |  [Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.334) |  | 0 |  | Junpeng Li, Zixia Jia, Zilong Zheng |  |
| 1529 |  |  [Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.335) |  | 0 |  | Kaitlyn Zhou, Dan Jurafsky, Tatsunori Hashimoto |  |
| 1530 |  |  [Elaborative Simplification as Implicit Questions Under Discussion](https://doi.org/10.18653/v1/2023.emnlp-main.336) |  | 0 |  | Yating Wu, William Sheffield, Kyle Mahowald, Junyi Jessy Li |  |
| 1531 |  |  [EntSUMv2: Dataset, Models and Evaluation for More Abstractive Entity-Centric Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.337) |  | 0 |  | Dhruv Mehra, Lingjue Xie, Ella HofmannCoyle, Mayank Kulkarni, Daniel PreotiucPietro |  |
| 1532 |  |  [SciRepEval: A Multi-Format Benchmark for Scientific Document Representations](https://doi.org/10.18653/v1/2023.emnlp-main.338) |  | 0 |  | Amanpreet Singh, Mike D'Arcy, Arman Cohan, Doug Downey, Sergey Feldman |  |
| 1533 |  |  [A Diachronic Perspective on User Trust in AI under Uncertainty](https://doi.org/10.18653/v1/2023.emnlp-main.339) |  | 0 |  | Shehzaad Dhuliawala, Vilém Zouhar, Mennatallah ElAssady, Mrinmaya Sachan |  |
| 1534 |  |  [CT-GAT: Cross-Task Generative Adversarial Attack based on Transferability](https://doi.org/10.18653/v1/2023.emnlp-main.340) |  | 0 |  | Minxuan Lv, Chengwei Dai, Kun Li, Wei Zhou, Songlin Hu |  |
| 1535 |  |  [Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling](https://doi.org/10.18653/v1/2023.emnlp-main.341) |  | 0 |  | Hai Yu, Chong Deng, Qinglin Zhang, Jiaqing Liu, Qian Chen, Wen Wang |  |
| 1536 |  |  [Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents](https://doi.org/10.18653/v1/2023.emnlp-main.342) |  | 0 |  | Hyungjoo Chae, Yongho Song, Kai Tzuiunn Ong, Taeyoon Kwon, Minjin Kim, Youngjae Yu, Dongha Lee, Dongyeop Kang, Jinyoung Yeo |  |
| 1537 |  |  [Information Value: Measuring Utterance Predictability as Distance from Plausible Alternatives](https://doi.org/10.18653/v1/2023.emnlp-main.343) |  | 0 |  | Mario Giulianelli, Sarenne Wallbridge, Raquel Fernández |  |
| 1538 |  |  [Generating Commonsense Counterfactuals for Stable Relation Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.344) |  | 0 |  | Xin Miao, Yongqi Li, Tieyun Qian |  |
| 1539 |  |  [C-STS: Conditional Semantic Textual Similarity](https://doi.org/10.18653/v1/2023.emnlp-main.345) |  | 0 |  | Ameet Deshpande, Carlos E. Jimenez, Howard Chen, Vishvak Murahari, Victoria Graf, Tanmay Rajpurohit, Ashwin Kalyan, Danqi Chen, Karthik Narasimhan |  |
| 1540 |  |  [Cross-lingual Transfer Can Worsen Bias in Sentiment Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.346) |  | 0 |  | Seraphina GoldfarbTarrant, Björn Ross, Adam Lopez |  |
| 1541 |  |  [Rumor Detection on Social Media with Crowd Intelligence and ChatGPT-Assisted Networks](https://doi.org/10.18653/v1/2023.emnlp-main.347) |  | 0 |  | Chang Yang, Peng Zhang, Wenbo Qiao, Hui Gao, Jiaming Zhao |  |
| 1542 |  |  [Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?](https://doi.org/10.18653/v1/2023.emnlp-main.348) |  | 0 |  | Yichi Zhang, Jiayi Pan, Yuchen Zhou, Rui Pan, Joyce Chai |  |
| 1543 |  |  [Analysing State-Backed Propaganda Websites: a New Dataset and Linguistic Study](https://doi.org/10.18653/v1/2023.emnlp-main.349) |  | 0 |  | Freddy Heppell, Kalina Bontcheva, Carolina Scarton |  |
| 1544 |  |  [Controllable Contrastive Generation for Multilingual Biomedical Entity Linking](https://doi.org/10.18653/v1/2023.emnlp-main.350) |  | 0 |  | Tiantian Zhu, Yang Qin, Qingcai Chen, Xin Mu, Changlong Yu, Yang Xiang |  |
| 1545 |  |  [HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts](https://doi.org/10.18653/v1/2023.emnlp-main.351) |  | 0 |  | Truong Do, Le Khiem, Quang Pham, TrungTin Nguyen, ThanhNam Doan, Binh Nguyen, Chenghao Liu, Savitha Ramasamy, Xiaoli Li, Steven C. H. Hoi |  |
| 1546 |  |  [MediaHG: Rethinking Eye-catchy Features in Social Media Headline Generation](https://doi.org/10.18653/v1/2023.emnlp-main.352) |  | 0 |  | Boning Zhang, Yang Yang |  |
| 1547 |  |  [Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata](https://doi.org/10.18653/v1/2023.emnlp-main.353) |  | 0 |  | Silei Xu, Shicheng Liu, Theo Culhane, Elizaveta Pertseva, MengHsi Wu, Sina J. Semnani, Monica S. Lam |  |
| 1548 |  |  [ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.354) |  | 0 |  | Dheeraj Mekala, Jason Andrew Wolfe, Subhro Roy |  |
| 1549 |  |  [Efficient Grammatical Error Correction Via Multi-Task Training and Optimized Training Schedule](https://doi.org/10.18653/v1/2023.emnlp-main.355) |  | 0 |  | Andrey Bout, Alexander Podolskiy, Sergey I. Nikolenko, Irina Piontkovskaya |  |
| 1550 |  |  [The BLA Benchmark: Investigating Basic Language Abilities of Pre-Trained Multimodal Models](https://doi.org/10.18653/v1/2023.emnlp-main.356) |  | 0 |  | Xinyi Chen, Raquel Fernández, Sandro Pezzelle |  |
| 1551 |  |  [RainProof: An Umbrella to Shield Text Generator from Out-Of-Distribution Data](https://doi.org/10.18653/v1/2023.emnlp-main.357) |  | 0 |  | Maxime Darrin, Pablo Piantanida, Pierre Colombo |  |
| 1552 |  |  [KEPL: Knowledge Enhanced Prompt Learning for Chinese Hypernym-Hyponym Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.358) |  | 0 |  | Ningchen Ma, Dong Wang, Hongyun Bao, Lei He, Suncong Zheng |  |
| 1553 |  |  [Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings](https://doi.org/10.18653/v1/2023.emnlp-main.359) |  | 0 |  | Qian Chen, Wen Wang, Qinglin Zhang, Siqi Zheng, Chong Deng, Hai Yu, Jiaqing Liu, Yukun Ma, Chong Zhang |  |
| 1554 |  |  [Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.360) |  | 0 |  | Ji Qi, Chuchun Zhang, Xiaozhi Wang, Kaisheng Zeng, Jifan Yu, Jinxin Liu, Lei Hou, Juanzi Li, Xu Bin |  |
| 1555 |  |  [Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions](https://doi.org/10.18653/v1/2023.emnlp-main.361) |  | 0 |  | LucieAimée Kaffee, Arnav Arora, Isabelle Augenstein |  |
| 1556 |  |  [Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding](https://doi.org/10.18653/v1/2023.emnlp-main.362) |  | 0 |  | Sangmin Bae, Jongwoo Ko, Hwanjun Song, SeYoung Yun |  |
| 1557 |  |  [End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions](https://doi.org/10.18653/v1/2023.emnlp-main.363) |  | 0 |  | Libo Qin, Wenbo Pan, Qiguang Chen, Lizi Liao, Zhou Yu, Yue Zhang, Wanxiang Che, Min Li |  |
| 1558 |  |  [Answering Questions by Meta-Reasoning over Multiple Chains of Thought](https://doi.org/10.18653/v1/2023.emnlp-main.364) |  | 0 |  | Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, Jonathan Berant |  |
| 1559 |  |  [INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.365) |  | 0 |  | Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, Lei Li |  |
| 1560 |  |  [Multi-level Contrastive Learning for Script-based Character Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.366) |  | 0 |  | Dawei Li, Hengyuan Zhang, Yanran Li, Shiping Yang |  |
| 1561 |  |  [CHEF in the Language Kitchen: A Generative Data Augmentation Leveraging Korean Morpheme Ingredients](https://doi.org/10.18653/v1/2023.emnlp-main.367) |  | 0 |  | Jaehyung Seo, Hyeonseok Moon, Jaewook Lee, Sugyeong Eo, Chanjun Park, Heuiseok Lim |  |
| 1562 |  |  [Automatic Debate Evaluation with Argumentation Semantics and Natural Language Argument Graph Networks](https://doi.org/10.18653/v1/2023.emnlp-main.368) |  | 0 |  | Ramon RuizDolz, Stella Heras, Ana GarcíaFornes |  |
| 1563 |  |  [Transfer-Free Data-Efficient Multilingual Slot Labeling](https://doi.org/10.18653/v1/2023.emnlp-main.369) |  | 0 |  | Evgeniia Razumovskaia, Ivan Vulic, Anna Korhonen |  |
| 1564 |  |  [Towards Interpretable Mental Health Analysis with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.370) |  | 0 |  | Kailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, Ziyan Kuang, Sophia Ananiadou |  |
| 1565 |  |  [Learning to Rank Generation with Pairwise Partial Rewards](https://doi.org/10.18653/v1/2023.emnlp-main.371) |  | 0 |  | Youngwon Lee, Jinu Lee, Seungwon Hwang |  |
| 1566 |  |  [GreedyCAS: Unsupervised Scientific Abstract Segmentation with Normalized Mutual Information](https://doi.org/10.18653/v1/2023.emnlp-main.372) |  | 0 |  | Yingqiang Gao, Jessica Lam, Nianlong Gu, Richard H. R. Hahnloser |  |
| 1567 |  |  [Spoiler Detection as Semantic Text Matching](https://doi.org/10.18653/v1/2023.emnlp-main.373) |  | 0 |  | Ryan Tran, Canwen Xu, Julian J. McAuley |  |
| 1568 |  |  [Multimodal Embodied Plan Prediction Augmented with Synthetic Embodied Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.374) |  | 0 |  | Aishwarya Padmakumar, Mert Inan, Spandana Gella, Patrick Lange, Dilek HakkaniTur |  |
| 1569 |  |  [GEM: Gestalt Enhanced Markup Language Model for Web Understanding via Render Tree](https://doi.org/10.18653/v1/2023.emnlp-main.375) |  | 0 |  | Zirui Shao, Feiyu Gao, Zhongda Qi, Hangdi Xing, Jiajun Bu, Zhi Yu, Qi Zheng, Xiaozhong Liu |  |
| 1570 |  |  [Abstractive Open Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.376) |  | 0 |  | Kevin Pei, Ishan Jindal, Kevin ChenChuan Chang |  |
| 1571 |  |  [CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network](https://doi.org/10.18653/v1/2023.emnlp-main.377) |  | 0 |  | Sreyan Ghosh, Manan Suri, Purva Chiniya, Utkarsh Tyagi, Sonal Kumar, Dinesh Manocha |  |
| 1572 |  |  [CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.emnlp-main.378) |  | 0 |  | Jingheng Ye, Yinghui Li, Qingyu Zhou, Yangning Li, Shirong Ma, HaiTao Zheng, Ying Shen |  |
| 1573 |  |  [Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods](https://doi.org/10.18653/v1/2023.emnlp-main.379) |  | 0 |  | Jonathan Kamp, Lisa Beinborn, Antske Fokkens |  |
| 1574 |  |  [SentiStream: A Co-Training Framework for Adaptive Online Sentiment Analysis in Evolving Data Streams](https://doi.org/10.18653/v1/2023.emnlp-main.380) |  | 0 |  | Yuhao Wu, Karthick Sharma, Chun Seah, Shuhao Zhang |  |
| 1575 |  |  [HyperNetwork-based Decoupling to Improve Model Generalization for Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.381) |  | 0 |  | Liang Zhang, Chulun Zhou, Fandong Meng, Jinsong Su, Yidong Chen, Jie Zhou |  |
| 1576 |  |  [Solving Hard Analogy Questions with Relation Embedding Chains](https://doi.org/10.18653/v1/2023.emnlp-main.382) |  | 0 |  | Nitesh Kumar, Steven Schockaert |  |
| 1577 |  |  [Modeling Empathic Similarity in Personal Narratives](https://doi.org/10.18653/v1/2023.emnlp-main.383) |  | 0 |  | Jocelyn Shen, Maarten Sap, Pedro ColonHernandez, Hae Park, Cynthia Breazeal |  |
| 1578 |  |  [Tree Prompting: Efficient Task Adaptation without Fine-Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.384) |  | 0 |  | Chandan Singh, John X. Morris, Alexander M. Rush, Jianfeng Gao, Yuntian Deng |  |
| 1579 |  |  [Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data](https://doi.org/10.18653/v1/2023.emnlp-main.385) |  | 0 |  | Canwen Xu, Daya Guo, Nan Duan, Julian J. McAuley |  |
| 1580 |  |  [Empathy Intent Drives Empathy Detection](https://doi.org/10.18653/v1/2023.emnlp-main.386) |  | 0 |  | Liting Jiang, Di Wu, Bohui Mao, Yanbing Li, Wushour Slamu |  |
| 1581 |  |  [Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling](https://doi.org/10.18653/v1/2023.emnlp-main.387) |  | 0 |  | Yuanjun Shi, Linzhi Wu, Minglai Shao |  |
| 1582 |  |  [BasahaCorpus: An Expanded Linguistic Resource for Readability Assessment in Central Philippine Languages](https://doi.org/10.18653/v1/2023.emnlp-main.388) |  | 0 |  | Joseph Marvin Imperial, Ekaterina Kochmar |  |
| 1583 |  |  [ReTAG: Reasoning Aware Table to Analytic Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.389) |  | 0 |  | Deepanway Ghosal, Preksha Nema, Aravindan Raghuveer |  |
| 1584 |  |  [Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators](https://doi.org/10.18653/v1/2023.emnlp-main.390) |  | 0 |  | Liang Chen, Yang Deng, Yatao Bian, Zeyu Qin, Bingzhe Wu, TatSeng Chua, KamFai Wong |  |
| 1585 |  |  [Compressing Context to Enhance Inference Efficiency of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.391) |  | 0 |  | Yucheng Li, Bo Dong, Frank Guerin, Chenghua Lin |  |
| 1586 |  |  [MoT: Memory-of-Thought Enables ChatGPT to Self-Improve](https://doi.org/10.18653/v1/2023.emnlp-main.392) |  | 0 |  | Xiaonan Li, Xipeng Qiu |  |
| 1587 |  |  [4 and 7-bit Labeling for Projective and Non-Projective Dependency Trees](https://doi.org/10.18653/v1/2023.emnlp-main.393) |  | 0 |  | Carlos GómezRodríguez, Diego Roca, David Vilares |  |
| 1588 |  |  [Can You Follow Me? Testing Situational Understanding for ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.394) |  | 0 |  | Chenghao Yang, Allyson Ettinger |  |
| 1589 |  |  [Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4](https://doi.org/10.18653/v1/2023.emnlp-main.395) |  | 0 |  | Kellin Pelrine, Anne Imouza, Camille Thibault, Meilina Reksoprodjo, Caleb Gupta, Joel Christoph, JeanFrançois Godbout, Reihaneh Rabbany |  |
| 1590 |  |  [Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation](https://doi.org/10.18653/v1/2023.emnlp-main.396) |  | 0 |  | Bashar Alhafni, Go Inoue, Christian Khairallah, Nizar Habash |  |
| 1591 |  |  [HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.397) |  | 0 |  | Junyi Li, Xiaoxue Cheng, Xin Zhao, JianYun Nie, JiRong Wen |  |
| 1592 |  |  [Enabling Large Language Models to Generate Text with Citations](https://doi.org/10.18653/v1/2023.emnlp-main.398) |  | 0 |  | Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen |  |
| 1593 |  |  [Revisiting Machine Translation for Cross-lingual Classification](https://doi.org/10.18653/v1/2023.emnlp-main.399) |  | 0 |  | Mikel Artetxe, Vedanuj Goswami, Shruti Bhosale, Angela Fan, Luke Zettlemoyer |  |
| 1594 |  |  [Pre-Trained Language Models Augmented with Synthetic Scanpaths for Natural Language Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.400) |  | 0 |  | Shuwen Deng, Paul Prasse, David R. Reich, Tobias Scheffer, Lena A. Jäger |  |
| 1595 |  |  [Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model](https://doi.org/10.18653/v1/2023.emnlp-main.401) |  | 0 |  | Leonie Weissweiler, Valentin Hofmann, Anjali Kantharuban, Anna Cai, Ritam Dutt, Amey Hengle, Anubha Kabra, Atharva Kulkarni, Abhishek Vijayakumar, Haofei Yu, Hinrich Schütze, Kemal Oflazer, David R. Mortensen |  |
| 1596 |  |  [Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.402) |  | 0 |  | Quanyu Long, Wenya Wang, Sinno Jialin Pan |  |
| 1597 |  |  [Understanding the Inner-workings of Language Models Through Representation Dissimilarity](https://doi.org/10.18653/v1/2023.emnlp-main.403) |  | 0 |  | Davis Brown, Charles Godfrey, Nicholas Konz, Jonathan H. Tu, Henry Kvinge |  |
| 1598 |  |  [Efficient Classification of Long Documents via State-Space Models](https://doi.org/10.18653/v1/2023.emnlp-main.404) |  | 0 |  | Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, Ivan Kobyzev |  |
| 1599 |  |  [Dual-Feedback Knowledge Retrieval for Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2023.emnlp-main.405) |  | 0 |  | Tianyuan Shi, Liangzhi Li, Zijian Lin, Tao Yang, Xiaojun Quan, Qifan Wang |  |
| 1600 |  |  [Construction Artifacts in Metaphor Identification Datasets](https://doi.org/10.18653/v1/2023.emnlp-main.406) |  | 0 |  | Joanne Boisson, Luis Espinosa Anke, José CamachoCollados |  |
| 1601 |  |  [MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.407) |  | 0 |  | Deepak Nathani, David Wang, Liangming Pan, William Yang Wang |  |
| 1602 |  |  [Granularity Matters: Pathological Graph-driven Cross-modal Alignment for Brain CT Report Generation](https://doi.org/10.18653/v1/2023.emnlp-main.408) |  | 0 |  | Yanzhao Shi, Junzhong Ji, Xiaodan Zhang, Liangqiong Qu, Ying Liu |  |
| 1603 |  |  [Enhancing Structured Evidence Extraction for Fact Verification](https://doi.org/10.18653/v1/2023.emnlp-main.409) |  | 0 |  | Zirui Wu, Nan Hu, Yansong Feng |  |
| 1604 |  |  [Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models](https://doi.org/10.18653/v1/2023.emnlp-main.410) |  | 0 |  | Di Wu, Wasi Uddin Ahmad, KaiWei Chang |  |
| 1605 |  |  [A Fair and In-Depth Evaluation of Existing End-to-End Entity Linking Systems](https://doi.org/10.18653/v1/2023.emnlp-main.411) |  | 0 |  | Hannah Bast, Matthias Hertel, Natalie Prange |  |
| 1606 |  |  [A Multi-Task Dataset for Assessing Discourse Coherence in Chinese Essays: Structure, Theme, and Logic Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.412) |  | 0 |  | Hongyi Wu, Xinshu Shen, Man Lan, Shaoguang Mao, Xiaopeng Bai, Yuanbin Wu |  |
| 1607 |  |  [SKD-NER: Continual Named Entity Recognition via Span-based Knowledge Distillation with Reinforcement Learning](https://doi.org/10.18653/v1/2023.emnlp-main.413) |  | 0 |  | Yi Chen, Liang He |  |
| 1608 |  |  [Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation](https://doi.org/10.18653/v1/2023.emnlp-main.414) |  | 0 |  | Chengwei Qin, Chen Chen, Shafiq Joty |  |
| 1609 |  |  [When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.415) |  | 0 |  | Eve Fleisig, Rediet Abebe, Dan Klein |  |
| 1610 |  |  [Lazy-k Decoding: Constrained Decoding for Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.416) |  | 0 |  | Arthur Hemmer, Mickaël Coustaty, Nicola Bartolo, Jérôme Brachat, JeanMarc Ogier |  |
| 1611 |  |  [Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation](https://doi.org/10.18653/v1/2023.emnlp-main.417) |  | 0 |  | Hailin Chen, Amrita Saha, Steven ChuHong Hoi, Shafiq Joty |  |
| 1612 |  |  [Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.418) |  | 0 |  | Raghav Jain, Daivik Sojitra, Arkadeep Acharya, Sriparna Saha, Adam Jatowt, Sandipan Dandapat |  |
| 1613 |  |  [Comparing Styles across Languages](https://doi.org/10.18653/v1/2023.emnlp-main.419) |  | 0 |  | Shreya Havaldar, Matthew Pressimone, Eric Wong, Lyle H. Ungar |  |
| 1614 |  |  [Event Causality Extraction via Implicit Cause-Effect Interactions](https://doi.org/10.18653/v1/2023.emnlp-main.420) |  | 0 |  | Jintao Liu, Zequn Zhang, Kaiwen Wei, Zhi Guo, Xian Sun, Li Jin, Xiaoyu Li |  |
| 1615 |  |  [Evaluation of African American Language Bias in Natural Language Generation](https://doi.org/10.18653/v1/2023.emnlp-main.421) |  | 0 |  | Nicholas Deas, Jessica Grieser, Shana Kleiner, Desmond Patton, Elsbeth Turcan, Kathleen R. McKeown |  |
| 1616 |  |  [A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2023.emnlp-main.422) |  | 0 |  | Songbo Hu, Han Zhou, Moy Yuan, Milan Gritta, Guchun Zhang, Ignacio Iacobacci, Anna Korhonen, Ivan Vulic |  |
| 1617 |  |  [Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction](https://doi.org/10.18653/v1/2023.emnlp-main.423) |  | 0 |  | V. S. D. S. Mahesh Akavarapu, Arnab Bhattacharya |  |
| 1618 |  |  [Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning](https://doi.org/10.18653/v1/2023.emnlp-main.424) |  | 0 |  | Ximing Lu, Faeze Brahman, Peter West, Jaehun Jung, Khyathi Raghavi Chandu, Abhilasha Ravichander, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, Nouha Dziri, Jillian Fisher, Bill Y. Lin, Skyler Hallinan, Lianhui Qin, Xiang Ren, Sean Welleck, Yejin Choi |  |
| 1619 |  |  [Weakly Supervised Semantic Parsing with Execution-based Spurious Program Filtering](https://doi.org/10.18653/v1/2023.emnlp-main.425) |  | 0 |  | Kangil Lee, Segwang Kim, Kyomin Jung |  |
| 1620 |  |  [Taxonomy Expansion for Named Entity Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.426) |  | 0 |  | Karthikeyan K, Yogarshi Vyas, Jie Ma, Giovanni Paolini, Neha Anna John, Shuai Wang, Yassine Benajiba, Vittorio Castelli, Dan Roth, Miguel Ballesteros |  |
| 1621 |  |  [Rather a Nurse than a Physician - Contrastive Explanations under Investigation](https://doi.org/10.18653/v1/2023.emnlp-main.427) |  | 0 |  | Oliver Eberle, Ilias Chalkidis, Laura Cabello, Stephanie Brandl |  |
| 1622 |  |  [EtiCor: Corpus for Analyzing LLMs for Etiquettes](https://doi.org/10.18653/v1/2023.emnlp-main.428) |  | 0 |  | Ashutosh Dwivedi, Pradhyumna Lavania, Ashutosh Modi |  |
| 1623 |  |  [An Investigation of LLMs' Inefficacy in Understanding Converse Relations](https://doi.org/10.18653/v1/2023.emnlp-main.429) |  | 0 |  | Chengwen Qi, Bowen Li, Binyuan Hui, Bailin Wang, Jinyang Li, Jinwang Wu, Yuanjun Laili |  |
| 1624 |  |  [Towards Low-Resource Automatic Program Repair with Meta-Learning and Pretrained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.430) |  | 0 |  | Weishi Wang, Yue Wang, Steven C. H. Hoi, Shafiq Joty |  |
| 1625 |  |  [ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters](https://doi.org/10.18653/v1/2023.emnlp-main.431) |  | 0 |  | Vipul Rathore, Rajdeep Dhingra, Parag Singla, Mausam |  |
| 1626 |  |  [Log-FGAER: Logic-Guided Fine-Grained Address Entity Recognition from Multi-Turn Spoken Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.432) |  | 0 |  | Xue Han, Yitong Wang, Qian Hu, Pengwei Hu, Chao Deng, Junlan Feng |  |
| 1627 |  |  [Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning](https://doi.org/10.18653/v1/2023.emnlp-main.433) |  | 0 |  | Sarkar Snigdha Sarathi Das, Haoran Zhang, Peng Shi, Wenpeng Yin, Rui Zhang |  |
| 1628 |  |  [On the Representational Capacity of Recurrent Neural Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.434) |  | 0 |  | Franz Nowak, Anej Svete, Li Du, Ryan Cotterell |  |
| 1629 |  |  [A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.435) |  | 0 |  | Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan |  |
| 1630 |  |  [Benchmarking and Improving Text-to-SQL Generation under Ambiguity](https://doi.org/10.18653/v1/2023.emnlp-main.436) |  | 0 |  | Adithya Bhaskar, Tushar Tomar, Ashutosh Sathe, Sunita Sarawagi |  |
| 1631 |  |  [Non-autoregressive Text Editing with Copy-aware Latent Alignments](https://doi.org/10.18653/v1/2023.emnlp-main.437) |  | 0 |  | Yu Zhang, Yue Zhang, Leyang Cui, Guohong Fu |  |
| 1632 |  |  [Translating away Translationese without Parallel Data](https://doi.org/10.18653/v1/2023.emnlp-main.438) |  | 0 |  | Rricha Jalota, Koel Dutta Chowdhury, Cristina EspañaBonet, Josef van Genabith |  |
| 1633 |  |  [Prompt-Based Monte-Carlo Tree Search for Goal-oriented Dialogue Policy Planning](https://doi.org/10.18653/v1/2023.emnlp-main.439) |  | 0 |  | Xiao Yu, Maximillian Chen, Zhou Yu |  |
| 1634 |  |  [UniMath: A Foundational and Multimodal Mathematical Reasoner](https://doi.org/10.18653/v1/2023.emnlp-main.440) |  | 0 |  | Zhenwen Liang, Tianyu Yang, Jipeng Zhang, Xiangliang Zhang |  |
| 1635 |  |  [CaseEncoder: A Knowledge-enhanced Pre-trained Model for Legal Case Encoding](https://doi.org/10.18653/v1/2023.emnlp-main.441) |  | 0 |  | Yixiao Ma, Yueyue Wu, Weihang Su, Qingyao Ai, Yiqun Liu |  |
| 1636 |  |  [HiddenTables and PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies](https://doi.org/10.18653/v1/2023.emnlp-main.442) |  | 0 |  | William Watson, Nicole Cho, Tucker Balch, Manuela Veloso |  |
| 1637 |  |  [Causal Document-Grounded Dialogue Pre-training](https://doi.org/10.18653/v1/2023.emnlp-main.443) |  | 0 |  | Yingxiu Zhao, Bowen Yu, Bowen Li, Haiyang Yu, Jinyang Li, Chao Wang, Fei Huang, Yongbin Li, Nevin L. Zhang |  |
| 1638 |  |  [Accented Speech Recognition With Accent-specific Codebooks](https://doi.org/10.18653/v1/2023.emnlp-main.444) |  | 0 |  | Darshan Prabhu, Preethi Jyothi, Sriram Ganapathy, Vinit Unni |  |
| 1639 |  |  [Linking Surface Facts to Large-Scale Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.445) |  | 0 |  | Gorjan Radevski, Kiril Gashteovski, ChiaChien Hung, Carolin Lawrence, Goran Glavas |  |
| 1640 |  |  [Sentiment Analysis on Streaming User Reviews via Dual-Channel Dynamic Graph Neural Network](https://doi.org/10.18653/v1/2023.emnlp-main.446) |  | 0 |  | Xin Zhang, Linhai Zhang, Deyu Zhou |  |
| 1641 |  |  [DUMB: A Dutch Model Benchmark](https://doi.org/10.18653/v1/2023.emnlp-main.447) |  | 0 |  | Wietse de Vries, Martijn Wieling, Malvina Nissim |  |
| 1642 |  |  [OssCSE: Overcoming Surface Structure Bias in Contrastive Learning for Unsupervised Sentence Embedding](https://doi.org/10.18653/v1/2023.emnlp-main.448) |  | 0 |  | Zhan Shi, Guoyin Wang, Ke Bai, Jiwei Li, Xiang Li, Qingjun Cui, Belinda Zeng, Trishul Chilimbi, Xiaodan Zhu |  |
| 1643 |  |  [End-to-End Single-Channel Speaker-Turn Aware Conversational Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-main.449) |  | 0 |  | Juan Pablo ZuluagaGomez, Zhaocheng Huang, Xing Niu, Rohit Paturi, Sundararajan Srinivasan, Prashant Mathur, Brian Thompson, Marcello Federico |  |
| 1644 |  |  [A Fine-Grained Taxonomy of Replies to Hate Speech](https://doi.org/10.18653/v1/2023.emnlp-main.450) |  | 0 |  | Xinchen Yu, Ashley Zhao, Eduardo Blanco, Lingzi Hong |  |
| 1645 |  |  [JointMatch: A Unified Approach for Diverse and Collaborative Pseudo-Labeling to Semi-Supervised Text Classification](https://doi.org/10.18653/v1/2023.emnlp-main.451) |  | 0 |  | Henry Peng Zou, Cornelia Caragea |  |
| 1646 |  |  [Simple Temporal Adaptation to Changing Label Sets: Hashtag Prediction via Dense KNN](https://doi.org/10.18653/v1/2023.emnlp-main.452) |  | 0 |  | Niloofar Mireshghallah, Nikolai Vogler, Junxian He, Omar Florez, Ahmed ElKishky, Taylor BergKirkpatrick |  |
| 1647 |  |  [Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4](https://doi.org/10.18653/v1/2023.emnlp-main.453) |  | 0 |  | Kent K. Chang, Mackenzie Cramer, Sandeep Soni, David Bamman |  |
| 1648 |  |  [A Study on Accessing Linguistic Information in Pre-Trained Language Models by Using Prompts](https://doi.org/10.18653/v1/2023.emnlp-main.454) |  | 0 |  | Marion Di Marco, Katharina Hämmerl, Alexander Fraser |  |
| 1649 |  |  [CiteBench: A Benchmark for Scientific Citation Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.455) |  | 0 |  | Martin Funkquist, Ilia Kuznetsov, Yufang Hou, Iryna Gurevych |  |
| 1650 |  |  [From Heuristic to Analytic: Cognitively Motivated Strategies for Coherent Physical Commonsense Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.456) |  | 0 |  | Zheyuan Zhang, Shane Storks, Fengyuan Hu, Sungryull Sohn, Moontae Lee, Honglak Lee, Joyce Chai |  |
| 1651 |  |  [A Challenging Multimodal Video Summary: Simultaneously Extracting and Generating Keyframe-Caption Pairs from Video](https://doi.org/10.18653/v1/2023.emnlp-main.457) |  | 0 |  | Keito Kudo, Haruki Nagasawa, Jun Suzuki, Nobuyuki Shimizu |  |
| 1652 |  |  [Copyright Violations and Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.458) |  | 0 |  | Antonia Karamolegkou, Jiaang Li, Li Zhou, Anders Søgaard |  |
| 1653 |  |  [Effects of sub-word segmentation on performance of transformer language models](https://doi.org/10.18653/v1/2023.emnlp-main.459) |  | 0 |  | Jue Hou, Anisia Katinskaia, AnhDuc Vu, Roman Yangarber |  |
| 1654 |  |  [Symbolic Planning and Code Generation for Grounded Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.460) |  | 0 |  | Justin T. Chiu, Wenting Zhao, Derek Chen, Saujas Vaduguru, Alexander M. Rush, Daniel Fried |  |
| 1655 |  |  [Universal Self-Adaptive Prompting](https://doi.org/10.18653/v1/2023.emnlp-main.461) |  | 0 |  | Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Hanjun Dai, Julian Eisenschlos, Sercan Ö. Arik, Tomas Pfister |  |
| 1656 |  |  [Somali Information Retrieval Corpus: Bridging the Gap between Query Translation and Dedicated Language Resources](https://doi.org/10.18653/v1/2023.emnlp-main.462) |  | 0 |  | Abdisalam Badel, Ting Zhong, Wenxin Tai, Fan Zhou |  |
| 1657 |  |  [Beat LLMs at Their Own Game: Zero-Shot LLM-Generated Text Detection via Querying ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.463) |  | 0 |  | Biru Zhu, Lifan Yuan, Ganqu Cui, Yangyi Chen, Chong Fu, Bingxiang He, Yangdong Deng, Zhiyuan Liu, Maosong Sun, Ming Gu |  |
| 1658 |  |  [Faithful Model Evaluation for Model-Based Metrics](https://doi.org/10.18653/v1/2023.emnlp-main.464) |  | 0 |  | Qian Hu, Palash Goyal, Rahul Gupta |  |
| 1659 |  |  [Content- and Topology-Aware Representation Learning for Scientific Multi-Literature](https://doi.org/10.18653/v1/2023.emnlp-main.465) |  | 0 |  | Kai Zhang, Kaisong Song, Yangyang Kang, Xiaozhong Liu |  |
| 1660 |  |  [Language Model Quality Correlates with Psychometric Predictive Power in Multiple Languages](https://doi.org/10.18653/v1/2023.emnlp-main.466) |  | 0 |  | Ethan Wilcox, Clara Meister, Ryan Cotterell, Tiago Pimentel |  |
| 1661 |  |  [Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks](https://doi.org/10.18653/v1/2023.emnlp-main.467) |  | 0 |  | Zhaohui Yan, Songlin Yang, Wei Liu, Kewei Tu |  |
| 1662 |  |  [Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.468) |  | 0 |  | Daman Arora, Himanshu Gaurav Singh, Mausam |  |
| 1663 |  |  [StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure](https://doi.org/10.18653/v1/2023.emnlp-main.469) |  | 0 |  | Mattia Opper, Victor Prokhorov, Siddharth Narayanaswamy |  |
| 1664 |  |  [WiCE: Real-World Entailment for Claims in Wikipedia](https://doi.org/10.18653/v1/2023.emnlp-main.470) |  | 0 |  | Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, Greg Durrett |  |
| 1665 |  |  [Natural Disaster Tweets Classification Using Multimodal Data](https://doi.org/10.18653/v1/2023.emnlp-main.471) |  | 0 |  | Mohammad Basit, Bashir Alam, Zubaida Fatima, Salman Shaikh |  |
| 1666 |  |  [On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research](https://doi.org/10.18653/v1/2023.emnlp-main.472) |  | 0 |  | Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker |  |
| 1667 |  |  [RoBoCoP: A Comprehensive ROmance BOrrowing COgnate Package and Benchmark for Multilingual Cognate Identification](https://doi.org/10.18653/v1/2023.emnlp-main.473) |  | 0 |  | Liviu P. Dinu, Ana Sabina Uban, Alina Maria Cristea, Anca Dinu, IoanBogdan Iordache, Simona Georgescu, Laurentiu Zoicas |  |
| 1668 |  |  [Instructive Dialogue Summarization with Query Aggregations](https://doi.org/10.18653/v1/2023.emnlp-main.474) |  | 0 |  | Bin Wang, Zhengyuan Liu, Nancy F. Chen |  |
| 1669 |  |  [Semantic matching for text classification with complex class descriptions](https://doi.org/10.18653/v1/2023.emnlp-main.475) |  | 0 |  | Brian de Silva, KuanWen Huang, Gwang Lee, Karen Hovsepian, Yan Xu, Mingwei Shen |  |
| 1670 |  |  [MADNet: Maximizing Addressee Deduction Expectation for Multi-Party Conversation Generation](https://doi.org/10.18653/v1/2023.emnlp-main.476) |  | 0 |  | JiaChen Gu, ChaoHong Tan, Caiyuan Chu, ZhenHua Ling, Chongyang Tao, Quan Liu, Cong Liu |  |
| 1671 |  |  [GLEN: Generative Retrieval via Lexical Index Learning](https://doi.org/10.18653/v1/2023.emnlp-main.477) |  | 0 |  | Sunkyung Lee, Minjin Choi, Jongwuk Lee |  |
| 1672 |  |  [Turn-Level Active Learning for Dialogue State Tracking](https://doi.org/10.18653/v1/2023.emnlp-main.478) |  | 0 |  | Zihan Zhang, Meng Fang, Fanghua Ye, Ling Chen, MohammadReza NamaziRad |  |
| 1673 |  |  [ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.479) |  | 0 |  | Haoqin Tu, Yitong Li, Fei Mi, Zhongliang Yang |  |
| 1674 |  |  [Modeling Conceptual Attribute Likeness and Domain Inconsistency for Metaphor Detection](https://doi.org/10.18653/v1/2023.emnlp-main.480) |  | 0 |  | Yuan Tian, Nan Xu, Wenji Mao, Daniel Zeng |  |
| 1675 |  |  [Referring Image Segmentation via Joint Mask Contextual Embedding Learning and Progressive Alignment Network](https://doi.org/10.18653/v1/2023.emnlp-main.481) |  | 0 |  | Ziling Huang, Shin'ichi Satoh |  |
| 1676 |  |  [Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study](https://doi.org/10.18653/v1/2023.emnlp-main.482) |  | 0 |  | Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, Anima Anandkumar, Bryan Catanzaro |  |
| 1677 |  |  [SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables](https://doi.org/10.18653/v1/2023.emnlp-main.483) |  | 0 |  | Xinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov, MinYen Kan |  |
| 1678 |  |  [Training Simultaneous Speech Translation with Robust and Random Wait-k-Tokens Strategy](https://doi.org/10.18653/v1/2023.emnlp-main.484) |  | 0 |  | Linlin Zhang, Kai Fan, Jiajun Bu, Zhongqiang Huang |  |
| 1679 |  |  [SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples](https://doi.org/10.18653/v1/2023.emnlp-main.485) |  | 0 |  | Deqing Fu, Ameya Godbole, Robin Jia |  |
| 1680 |  |  [Enhancing Code-Switching for Cross-lingual SLU: A Unified View of Semantic and Grammatical Coherence](https://doi.org/10.18653/v1/2023.emnlp-main.486) |  | 0 |  | Zhihong Zhu, Xuxin Cheng, Zhiqi Huang, Dongsheng Chen, Yuexian Zou |  |
| 1681 |  |  [Task-Agnostic Low-Rank Adapters for Unseen English Dialects](https://doi.org/10.18653/v1/2023.emnlp-main.487) |  | 0 |  | Zedian Xiao, William Held, Yanchen Liu, Diyi Yang |  |
| 1682 |  |  [Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization](https://doi.org/10.18653/v1/2023.emnlp-main.488) |  | 0 |  | Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor S. Sheng, Huaiyu Dai, Dejing Dou |  |
| 1683 |  |  [TheoremQA: A Theorem-driven Question Answering Dataset](https://doi.org/10.18653/v1/2023.emnlp-main.489) |  | 0 |  | Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, Tony Xia |  |
| 1684 |  |  [Scalable-DSC: A Structural Template Prompt Approach to Scalable Dialogue State Correction](https://doi.org/10.18653/v1/2023.emnlp-main.490) |  | 0 |  | Haoxiang Su, Hongyan Xie, Hao Huang, Shuangyong Song, Ruiyu Fang, Xiaomeng Huang, Sijie Feng |  |
| 1685 |  |  [Don't Trust ChatGPT when your Question is not in English: A Study of Multilingual Abilities and Types of LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.491) |  | 0 |  | Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, Grzegorz Kondrak |  |
| 1686 |  |  [M³Seg: A Maximum-Minimum Mutual Information Paradigm for Unsupervised Topic Segmentation in ASR Transcripts](https://doi.org/10.18653/v1/2023.emnlp-main.492) |  | 0 |  | Ke Wang, Xiutian Zhao, Yanghui Li, Wei Peng |  |
| 1687 |  |  [Empirical Study of Zero-Shot NER with ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.493) |  | 0 |  | Tingyu Xie, Qi Li, Jian Zhang, Yan Zhang, Zuozhu Liu, Hongwei Wang |  |
| 1688 |  |  [Automatic Prompt Optimization with "Gradient Descent" and Beam Search](https://doi.org/10.18653/v1/2023.emnlp-main.494) |  | 0 |  | Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng |  |
| 1689 |  |  [Active Retrieval Augmented Generation](https://doi.org/10.18653/v1/2023.emnlp-main.495) |  | 0 |  | Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane DwivediYu, Yiming Yang, Jamie Callan, Graham Neubig |  |
| 1690 |  |  [GD-COMET: A Geo-Diverse Commonsense Inference Model](https://doi.org/10.18653/v1/2023.emnlp-main.496) |  | 0 |  | Mehar Bhatia, Vered Shwartz |  |
| 1691 |  |  [Multi-level Adaptive Contrastive Learning for Knowledge Internalization in Dialogue Generation](https://doi.org/10.18653/v1/2023.emnlp-main.497) |  | 0 |  | Chenxu Yang, Zheng Lin, Lanrui Wang, Chong Tian, Liang Pang, Jiangnan Li, Qirong Ho, Yanan Cao, Weiping Wang |  |
| 1692 |  |  [Enhancing Biomedical Lay Summarisation with External Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.498) |  | 0 |  | Tomas Goldsack, Zhihao Zhang, Chen Tang, Carolina Scarton, Chenghua Lin |  |
| 1693 |  |  [A Diffusion Weighted Graph Framework for New Intent Discovery](https://doi.org/10.18653/v1/2023.emnlp-main.499) |  | 0 |  | Wenkai Shi, Wenbin An, Feng Tian, Qinghua Zheng, Qianying Wang, Ping Chen |  |
| 1694 |  |  [A Self-enhancement Multitask Framework for Unsupervised Aspect Category Detection](https://doi.org/10.18653/v1/2023.emnlp-main.500) |  | 0 |  | ThiNhung Nguyen, Hoang Ngo, KiemHieu Nguyen, TuanDung Cao |  |
| 1695 |  |  [DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.501) |  | 0 |  | Chengcheng Han, Xiaowei Du, Che Zhang, Yixin Lian, Xiang Li, Ming Gao, Baoyuan Wang |  |
| 1696 |  |  [Recurrent Neural Language Models as Probabilistic Finite-state Automata](https://doi.org/10.18653/v1/2023.emnlp-main.502) |  | 0 |  | Anej Svete, Ryan Cotterell |  |
| 1697 |  |  [Revisiting Source Context in Nearest Neighbor Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.503) |  | 0 |  | Xuanhong Li, Peng Li, Po Hu |  |
| 1698 |  |  [Find-2-Find: Multitask Learning for Anaphora Resolution and Object Localization](https://doi.org/10.18653/v1/2023.emnlp-main.504) |  | 0 |  | Cennet Oguz, Pascal Denis, Emmanuel Vincent, Simon Ostermann, Josef van Genabith |  |
| 1699 |  |  [Background Summarization of Event Timelines](https://doi.org/10.18653/v1/2023.emnlp-main.505) |  | 0 |  | Adithya Pratapa, Kevin Small, Markus Dreyer |  |
| 1700 |  |  [Superlim: A Swedish Language Understanding Evaluation Benchmark](https://doi.org/10.18653/v1/2023.emnlp-main.506) |  | 0 |  | Aleksandrs Berdicevskis, Gerlof Bouma, Robin Kurtz, Felix Morger, Joey Öhman, Yvonne Adesam, Lars Borin, Dana Dannélls, Markus Forsberg, Tim Isbister, Anna Lindahl, Martin Malmsten, Faton Rekathati, Magnus Sahlgren, Elena Volodina, Love Börjeson, Simon Hengchen, Nina Tahmasebi |  |
| 1701 |  |  [Reasoning with Language Model is Planning with World Model](https://doi.org/10.18653/v1/2023.emnlp-main.507) |  | 0 |  | Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu |  |
| 1702 |  |  [LLM-enhanced Self-training for Cross-domain Constituency Parsing](https://doi.org/10.18653/v1/2023.emnlp-main.508) |  | 0 |  | Jianling Li, Meishan Zhang, Peiming Guo, Min Zhang, Yue Zhang |  |
| 1703 |  |  [Continual Named Entity Recognition without Catastrophic Forgetting](https://doi.org/10.18653/v1/2023.emnlp-main.509) |  | 0 |  | Duzhen Zhang, Wei Cong, Jiahua Dong, Yahan Yu, Xiuyi Chen, Yonggang Zhang, Zhen Fang |  |
| 1704 |  |  [DSI++: Updating Transformer Memory with New Documents](https://doi.org/10.18653/v1/2023.emnlp-main.510) |  | 0 |  | Sanket Vaibhav Mehta, Jai Gupta, Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Jinfeng Rao, Marc Najork, Emma Strubell, Donald Metzler |  |
| 1705 |  |  [Editing Common Sense in Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.511) |  | 0 |  | Anshita Gupta, Debanjan Mondal, Akshay Krishna Sheshadri, Wenlong Zhao, Xiang Li, Sarah Wiegreffe, Niket Tandon |  |
| 1706 |  |  [Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.512) |  | 0 |  | Tianqi Zhong, Quan Wang, Jingxuan Han, Yongdong Zhang, Zhendong Mao |  |
| 1707 |  |  [Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.513) |  | 0 |  | Hosein Mohebbi, Grzegorz Chrupala, Willem H. Zuidema, Afra Alishahi |  |
| 1708 |  |  [Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System](https://doi.org/10.18653/v1/2023.emnlp-main.514) |  | 0 |  | Weizhou Shen, Yingqi Gao, Canbin Huang, Fanqi Wan, Xiaojun Quan, Wei Bi |  |
| 1709 |  |  [IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions](https://doi.org/10.18653/v1/2023.emnlp-main.515) |  | 0 |  | Wenhao Yu, Meng Jiang, Peter Clark, Ashish Sabharwal |  |
| 1710 |  |  [How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances](https://doi.org/10.18653/v1/2023.emnlp-main.516) |  | 0 |  | Zihan Zhang, Meng Fang, Ling Chen, MohammadReza NamaziRad, Jun Wang |  |
| 1711 |  |  [PreWoMe: Exploiting Presuppositions as Working Memory for Long Form Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.517) |  | 0 |  | Wookje Han, Jinsol Park, Kyungjae Lee |  |
| 1712 |  |  [Memorisation Cartography: Mapping out the Memorisation-Generalisation Continuum in Neural Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.518) |  | 0 |  | Verna Dankers, Ivan Titov, Dieuwke Hupkes |  |
| 1713 |  |  [DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4](https://doi.org/10.18653/v1/2023.emnlp-main.519) |  | 0 |  | Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Hassan Foroosh, Fei Liu |  |
| 1714 |  |  [Gender Biases in Automatic Evaluation Metrics for Image Captioning](https://doi.org/10.18653/v1/2023.emnlp-main.520) |  | 0 |  | Haoyi Qiu, ZiYi Dou, Tianlu Wang, Asli Celikyilmaz, Nanyun Peng |  |
| 1715 |  |  [QA-NatVer: Question Answering for Natural Logic-based Fact Verification](https://doi.org/10.18653/v1/2023.emnlp-main.521) |  | 0 |  | Rami Aly, Marek Strong, Andreas Vlachos |  |
| 1716 |  |  [Increasing Probability Mass on Answer Choices Does Not Always Improve Accuracy](https://doi.org/10.18653/v1/2023.emnlp-main.522) |  | 0 |  | Sarah Wiegreffe, Matthew Finlayson, Oyvind Tafjord, Peter Clark, Ashish Sabharwal |  |
| 1717 |  |  [Generating Data for Symbolic Language with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.523) |  | 0 |  | Jiacheng Ye, Chengzu Li, Lingpeng Kong, Tao Yu |  |
| 1718 |  |  [IDTraffickers: An Authorship Attribution Dataset to link and connect Potential Human-Trafficking Operations on Text Escort Advertisements](https://doi.org/10.18653/v1/2023.emnlp-main.524) |  | 0 |  | Vageesh Saxena, Benjamin Bashpole, Gijs van Dijck, Gerasimos Spanakis |  |
| 1719 |  |  [Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.525) |  | 0 |  | Laura Cabello, Emanuele Bugliarello, Stephanie Brandl, Desmond Elliott |  |
| 1720 |  |  [Improving Dialogue Discourse Parsing via Reply-to Structures of Addressee Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.526) |  | 0 |  | Yaxin Fan, Feng Jiang, Peifeng Li, Fang Kong, Qiaoming Zhu |  |
| 1721 |  |  [Improving Language Models' Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary](https://doi.org/10.18653/v1/2023.emnlp-main.527) |  | 0 |  | Myeongjun Jang, Thomas Lukasiewicz |  |
| 1722 |  |  [DALE: Generative Data Augmentation for Low-Resource Legal NLP](https://doi.org/10.18653/v1/2023.emnlp-main.528) |  | 0 |  | Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Ramaneswaran S., Sakshi Singh, Utkarsh Tyagi, Dinesh Manocha |  |
| 1723 |  |  [FedID: Federated Interactive Distillation for Large-Scale Pretraining Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.529) |  | 0 |  | Xinge Ma, Jiangming Liu, Jin Wang, Xuejie Zhang |  |
| 1724 |  |  [trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.530) |  | 0 |  | Alexander Havrilla, Maksym Zhuravinskyi, Duy Phung, Aman Tiwari, Jonathan Tow, Stella Biderman, Quentin Anthony, Louis Castricato |  |
| 1725 |  |  [This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.531) |  | 0 |  | Iker GarcíaFerrero, Begoña Altuna, Javier Álvez, Itziar GonzalezDios, German Rigau |  |
| 1726 |  |  [MT2: Towards a Multi-Task Machine Translation Model with Translation-Specific In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.532) |  | 0 |  | Chunyou Li, Mingtong Liu, Hongxiao Zhang, Yufeng Chen, Jinan Xu, Ming Zhou |  |
| 1727 |  |  [CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset](https://doi.org/10.18653/v1/2023.emnlp-main.533) |  | 0 |  | Susanna Rücker, Alan Akbik |  |
| 1728 |  |  [Disentangling Transformer Language Models as Superposed Topic Models](https://doi.org/10.18653/v1/2023.emnlp-main.534) |  | 0 |  | Jia Peng Lim, Hady W. Lauw |  |
| 1729 |  |  [Conversational Semantic Parsing using Dynamic Context Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.535) |  | 0 |  | Parag Jain, Mirella Lapata |  |
| 1730 |  |  [Not all quantifiers are equal: Probing Transformer-based language models' understanding of generalised quantifiers](https://doi.org/10.18653/v1/2023.emnlp-main.536) |  | 0 |  | Tharindu Madusanka, Iqra Zahid, Hao Li, Ian PrattHartmann, Riza BatistaNavarro |  |
| 1731 |  |  [Structure-aware Knowledge Graph-to-text Generation with Planning Selection and Similarity Distinction](https://doi.org/10.18653/v1/2023.emnlp-main.537) |  | 0 |  | Feng Zhao, Hongzhi Zou, Cheng Yan |  |
| 1732 |  |  [SOUL: Towards Sentiment and Opinion Understanding of Language](https://doi.org/10.18653/v1/2023.emnlp-main.538) |  | 0 |  | Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing |  |
| 1733 |  |  [Regulation and NLP (RegNLP): Taming Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.539) |  | 0 |  | Catalina Goanta, Nikolaos Aletras, Ilias Chalkidis, Sofia Ranchordás, Gerasimos Spanakis |  |
| 1734 |  |  [MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation](https://doi.org/10.18653/v1/2023.emnlp-main.540) |  | 0 |  | Zexue He, Yu Wang, An Yan, Yao Liu, Eric Y. Chang, Amilcare Gentili, Julian J. McAuley, ChunNan Hsu |  |
| 1735 |  |  [Seeing through the mess: evolutionary dynamics of lexical polysemy](https://doi.org/10.18653/v1/2023.emnlp-main.541) |  | 0 |  | Andreas Baumann, Andreas Stephan, Benjamin Roth |  |
| 1736 |  |  [Are Embedded Potatoes Still Vegetables? On the Limitations of WordNet Embeddings for Lexical Semantics](https://doi.org/10.18653/v1/2023.emnlp-main.542) |  | 0 |  | Xuyou Cheng, Michael Sejr Schlichtkrull, Guy Emerson |  |
| 1737 |  |  [Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.543) |  | 0 |  | Andrea Sottana, Bin Liang, Kai Zou, Zheng Yuan |  |
| 1738 |  |  [Event-Location Tracking in Narratives: A Case Study on Holocaust Testimonies](https://doi.org/10.18653/v1/2023.emnlp-main.544) |  | 0 |  | Eitan Wagner, Renana Keydar, Omri Abend |  |
| 1739 |  |  [Dialogizer: Context-aware Conversational-QA Dataset Generation from Textual Sources](https://doi.org/10.18653/v1/2023.emnlp-main.545) |  | 0 |  | Yerin Hwang, Yongil Kim, Hyunkyung Bae, Hwanhee Lee, Jeesoo Bang, Kyomin Jung |  |
| 1740 |  |  [Learning to Predict Task Transferability via Soft Prompt](https://doi.org/10.18653/v1/2023.emnlp-main.546) |  | 0 |  | Lingyun Feng |  |
| 1741 |  |  [Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.547) |  | 0 |  | Wang Zhu, Jesse Thomason, Robin Jia |  |
| 1742 |  |  [Mirror: A Universal Framework for Various Information Extraction Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.548) |  | 0 |  | Tong Zhu, Junfei Ren, Zijian Yu, Mengsong Wu, Guoliang Zhang, Xiaoye Qu, Wenliang Chen, Zhefeng Wang, Baoxing Huai, Min Zhang |  |
| 1743 |  |  ["Mistakes Help Us Grow": Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms](https://doi.org/10.18653/v1/2023.emnlp-main.549) |  | 0 |  | Kunal Handa, Margaret Clapper, Jessica Boyle, Rose E. Wang, Diyi Yang, David S. Yeager, Dorottya Demszky |  |
| 1744 |  |  [Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text](https://doi.org/10.18653/v1/2023.emnlp-main.550) |  | 0 |  | Qi Cao, Takeshi Kojima, Yutaka Matsuo, Yusuke Iwasawa |  |
| 1745 |  |  [Detecting and Mitigating Hallucinations in Multilingual Summarisation](https://doi.org/10.18653/v1/2023.emnlp-main.551) |  | 0 |  | Yifu Qiu, Yftah Ziser, Anna Korhonen, Edoardo Maria Ponti, Shay B. Cohen |  |
| 1746 |  |  [Exploring Linguistic Probes for Morphological Inflection](https://doi.org/10.18653/v1/2023.emnlp-main.552) |  | 0 |  | Jordan Kodner, Salam Khalifa, Sarah Ruth Brogden Payne |  |
| 1747 |  |  [AMR Parsing with Causal Hierarchical Attention and Pointers](https://doi.org/10.18653/v1/2023.emnlp-main.553) |  | 0 |  | Chao Lou, Kewei Tu |  |
| 1748 |  |  [FLatS: Principled Out-of-Distribution Detection with Feature-Based Likelihood Ratio Score](https://doi.org/10.18653/v1/2023.emnlp-main.554) |  | 0 |  | Haowei Lin, Yuntian Gu |  |
| 1749 |  |  [Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.555) |  | 0 |  | Haoqi Zheng, Qihuang Zhong, Liang Ding, Zhiliang Tian, Xin Niu, Changjian Wang, Dongsheng Li, Dacheng Tao |  |
| 1750 |  |  [IC3: Image Captioning by Committee Consensus](https://doi.org/10.18653/v1/2023.emnlp-main.556) |  | 0 |  | David Chan, Austin Myers, Sudheendra Vijayanarasimhan, David A. Ross, John F. Canny |  |
| 1751 |  |  [SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.557) |  | 0 |  | Potsawee Manakul, Adian Liusie, Mark J. F. Gales |  |
| 1752 |  |  [Fair Without Leveling Down: A New Intersectional Fairness Definition](https://doi.org/10.18653/v1/2023.emnlp-main.558) |  | 0 |  | Gaurav Maheshwari, Aurélien Bellet, Pascal Denis, Mikaela Keller |  |
| 1753 |  |  [Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications](https://doi.org/10.18653/v1/2023.emnlp-main.559) |  | 0 |  | Manuel Faysse, Gautier Viaud, Céline Hudelot, Pierre Colombo |  |
| 1754 |  |  [CLAD-ST: Contrastive Learning with Adversarial Data for Robust Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-main.560) |  | 0 |  | Sathish Indurthi, Shamil Chollampatt, Ravi Agrawal, Marco Turchi |  |
| 1755 |  |  [M2DF: Multi-grained Multi-curriculum Denoising Framework for Multimodal Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.561) |  | 0 |  | Fei Zhao, Chunhui Li, Zhen Wu, Yawen Ouyang, Jianbing Zhang, Xinyu Dai |  |
| 1756 |  |  [Detection of Multiple Mental Disorders from Social Media with Two-Stream Psychiatric Experts](https://doi.org/10.18653/v1/2023.emnlp-main.562) |  | 0 |  | Siyuan Chen, Zhiling Zhang, Mengyue Wu, Kenny Q. Zhu |  |
| 1757 |  |  [Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?](https://doi.org/10.18653/v1/2023.emnlp-main.563) |  | 0 |  | Ahmed Alajrami, Katerina Margatina, Nikolaos Aletras |  |
| 1758 |  |  [Improved Unsupervised Chinese Word Segmentation Using Pre-trained Knowledge and Pseudo-labeling Transfer](https://doi.org/10.18653/v1/2023.emnlp-main.564) |  | 0 |  | HsiuWen Li, YingJia Lin, YiTing Li, Chun Lin, HungYu Kao |  |
| 1759 |  |  [EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.565) |  | 0 |  | Hanlin Tang, Yifu Sun, Decheng Wu, Kai Liu, Jianchen Zhu, Zhanhui Kang |  |
| 1760 |  |  [Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings](https://doi.org/10.18653/v1/2023.emnlp-main.566) |  | 0 |  | Mattia Atzeni, Mikhail Plekhanov, Frédéric A. Dreyer, Nora Kassner, Simone Merello, Louis Martin, Nicola Cancedda |  |
| 1761 |  |  [APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.567) |  | 0 |  | Qifan Wang, Yuning Mao, Jingang Wang, Hanchao Yu, Shaoliang Nie, Sinong Wang, Fuli Feng, Lifu Huang, Xiaojun Quan, Zenglin Xu, Dongfang Liu |  |
| 1762 |  |  [What's "up" with vision-language models? Investigating their struggle with spatial reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.568) |  | 0 |  | Amita Kamath, Jack Hessel, KaiWei Chang |  |
| 1763 |  |  [IBADR: an Iterative Bias-Aware Dataset Refinement Framework for Debiasing NLU models](https://doi.org/10.18653/v1/2023.emnlp-main.569) |  | 0 |  | Xiaoyue Wang, Xin Liu, Lijie Wang, Yaoxiang Wang, Jinsong Su, Hua Wu |  |
| 1764 |  |  [Learning Preference Model for LLMs via Automatic Preference Data Generation](https://doi.org/10.18653/v1/2023.emnlp-main.570) |  | 0 |  | Shijia Huang, Jianqiao Zhao, Yanyang Li, Liwei Wang |  |
| 1765 |  |  [Multilingual k-Nearest-Neighbor Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.571) |  | 0 |  | David Stap, Christof Monz |  |
| 1766 |  |  [Understanding Computational Models of Semantic Change: New Insights from the Speech Community](https://doi.org/10.18653/v1/2023.emnlp-main.572) |  | 0 |  | Filip Miletic, Anne PrzewoznyDesriaux, Ludovic Tanguy |  |
| 1767 |  |  [Causal Reasoning through Two Cognition Layers for Improving Generalization in Visual Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.573) |  | 0 |  | Trang Nguyen, Naoaki Okazaki |  |
| 1768 |  |  [StructGPT: A General Framework for Large Language Model to Reason over Structured Data](https://doi.org/10.18653/v1/2023.emnlp-main.574) |  | 0 |  | Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, JiRong Wen |  |
| 1769 |  |  [Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement](https://doi.org/10.18653/v1/2023.emnlp-main.575) |  | 0 |  | Rosamond Elizabeth Thalken, Edward H. Stiglitz, David Mimno, Matthew Wilkens |  |
| 1770 |  |  [Model-tuning Via Prompts Makes NLP Models Adversarially Robust](https://doi.org/10.18653/v1/2023.emnlp-main.576) |  | 0 |  | Mrigank Raman, Pratyush Maini, J. Zico Kolter, Zachary C. Lipton, Danish Pruthi |  |
| 1771 |  |  [Learning Co-Speech Gesture for Multimodal Aphasia Type Detection](https://doi.org/10.18653/v1/2023.emnlp-main.577) |  | 0 |  | Daeun Lee, Sejung Son, Hyolim Jeon, Seungbae Kim, Jinyoung Han |  |
| 1772 |  |  [STINMatch: Semi-Supervised Semantic-Topological Iteration Network for Financial Risk Detection via News Label Diffusion](https://doi.org/10.18653/v1/2023.emnlp-main.578) |  | 0 |  | Xurui Li, Yue Qin, Rui Zhu, Tianqianjin Lin, Yongming Fan, Yangyang Kang, Kaisong Song, Fubang Zhao, Changlong Sun, Haixu Tang, Xiaozhong Liu |  |
| 1773 |  |  [Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection](https://doi.org/10.18653/v1/2023.emnlp-main.579) |  | 0 |  | Vyoma Raman, Eve Fleisig, Dan Klein |  |
| 1774 |  |  [Describe Me an Auklet: Generating Grounded Perceptual Category Descriptions](https://doi.org/10.18653/v1/2023.emnlp-main.580) |  | 0 |  | Bill Noble, Nikolai Ilinykh |  |
| 1775 |  |  [Revisiting Automated Topic Model Evaluation with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.581) |  | 0 |  | Dominik Stammbach, Vilém Zouhar, Alexander Miserlis Hoyle, Mrinmaya Sachan, Elliott Ash |  |
| 1776 |  |  [ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection and Argumentative Dialogue Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.582) |  | 0 |  | Xiutian Zhao, Ke Wang, Wei Peng |  |
| 1777 |  |  [On the Benefits of Learning to Route in Mixture-of-Experts Models](https://doi.org/10.18653/v1/2023.emnlp-main.583) |  | 0 |  | Nishanth Dikkala, Nikhil Ghosh, Raghu Meka, Rina Panigrahy, Nikhil Vyas, Xin Wang |  |
| 1778 |  |  [SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation](https://doi.org/10.18653/v1/2023.emnlp-main.584) |  | 0 |  | Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua Maynez, Roee Aharoni, Vitaly Nikolaev, Thibault Sellam, Aditya Siddhant, Dipanjan Das, Ankur P. Parikh |  |
| 1779 |  |  [Query2doc: Query Expansion with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.585) |  | 0 |  | Liang Wang, Nan Yang, Furu Wei |  |
| 1780 |  |  [We Need to Talk About Reproducibility in NLP Model Comparison](https://doi.org/10.18653/v1/2023.emnlp-main.586) |  | 0 |  | Yan Xue, Xuefei Cao, Xingli Yang, Yu Wang, Ruibo Wang, Jihong Li |  |
| 1781 |  |  [Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration](https://doi.org/10.18653/v1/2023.emnlp-main.587) |  | 0 |  | Fanqi Wan, Xinting Huang, Tao Yang, Xiaojun Quan, Wei Bi, Shuming Shi |  |
| 1782 |  |  [Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions](https://doi.org/10.18653/v1/2023.emnlp-main.588) |  | 0 |  | Kazuki Irie, Róbert Csordás, Jürgen Schmidhuber |  |
| 1783 |  |  [InterFair: Debiasing with Natural Language Feedback for Fair Interpretable Predictions](https://doi.org/10.18653/v1/2023.emnlp-main.589) |  | 0 |  | Bodhisattwa Prasad Majumder, Zexue He, Julian J. McAuley |  |
| 1784 |  |  [Just Adjust One Prompt: Enhancing In-Context Dialogue Scoring via Constructing the Optimal Subgraph of Demonstrations and Prompts](https://doi.org/10.18653/v1/2023.emnlp-main.590) |  | 0 |  | Jiashu Pu, Ling Cheng, Lu Fan, Tangjie Lv, Rongsheng Zhang |  |
| 1785 |  |  [Multilingual estimation of political-party positioning: From label aggregation to long-input Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.591) |  | 0 |  | Dmitry Nikolaev, Tanise Ceron, Sebastian Padó |  |
| 1786 |  |  [ART: rule bAsed futuRe-inference deducTion](https://doi.org/10.18653/v1/2023.emnlp-main.592) |  | 0 |  | Mengze Li, Tianqi Zhao, Jionghao Bai, Baoyi He, Jiaxu Miao, Wei Ji, Zheqi Lv, Zhou Zhao, Shengyu Zhang, Wenqiao Zhang, Fei Wu |  |
| 1787 |  |  [EpiK-Eval: Evaluation for Language Models as Epistemic Models](https://doi.org/10.18653/v1/2023.emnlp-main.593) |  | 0 |  | Gabriele Prato, Jerry Huang, Prasanna Parthasarathi, Shagun Sodhani, Sarath Chandar |  |
| 1788 |  |  [From Dissonance to Insights: Dissecting Disagreements in Rationale Construction for Case Outcome Classification](https://doi.org/10.18653/v1/2023.emnlp-main.594) |  | 0 |  | Shanshan Xu, T. Y. S. S. Santosh, Oana Ichim, Isabella Risini, Barbara Plank, Matthias Grabmair |  |
| 1789 |  |  [On Bilingual Lexicon Induction with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.595) |  | 0 |  | Yaoyiran Li, Anna Korhonen, Ivan Vulic |  |
| 1790 |  |  [Statistical Depth for Ranking and Characterizing Transformer-Based Text Embeddings](https://doi.org/10.18653/v1/2023.emnlp-main.596) |  | 0 |  | Parker Seegmiller, Sarah Preum |  |
| 1791 |  |  [CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model](https://doi.org/10.18653/v1/2023.emnlp-main.597) |  | 0 |  | Kaiyan Zhang, Ning Ding, Biqing Qi, Xuekai Zhu, Xinwei Long, Bowen Zhou |  |
| 1792 |  |  [From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues](https://doi.org/10.18653/v1/2023.emnlp-main.598) |  | 0 |  | Shivani Kumar, Ramaneswaran S., Md. Shad Akhtar, Tanmoy Chakraborty |  |
| 1793 |  |  [Large Language Models are biased to overestimate profoundness](https://doi.org/10.18653/v1/2023.emnlp-main.599) |  | 0 |  | Eugenio HerreraBerg, Tomás Vergara Browne, Pablo LeónVillagrá, MarcLluís Vives, Cristian Buc Calderon |  |
| 1794 |  |  [SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.600) |  | 0 |  | Philippe Laban, Wojciech Kryscinski, Divyansh Agarwal, Alexander R. Fabbri, Caiming Xiong, Shafiq Joty, ChienSheng Wu |  |
| 1795 |  |  [DIVE: Towards Descriptive and Diverse Visual Commonsense Generation](https://doi.org/10.18653/v1/2023.emnlp-main.601) |  | 0 |  | JunHyung Park, Hyuntae Park, Youjin Kang, Eojin Jeon, SangKeun Lee |  |
| 1796 |  |  [Towards Conceptualization of "Fair Explanation": Disparate Impacts of anti-Asian Hate Speech Explanations on Content Moderators](https://doi.org/10.18653/v1/2023.emnlp-main.602) |  | 0 |  | Tin Nguyen, Jiannan Xu, Aayushi Roy, Hal Daumé III, Marine Carpuat |  |
| 1797 |  |  [Bridging Background Knowledge Gaps in Translation with Automatic Explicitation](https://doi.org/10.18653/v1/2023.emnlp-main.603) |  | 0 |  | HyoJung Han, Jordan L. BoydGraber, Marine Carpuat |  |
| 1798 |  |  [A Quality-based Syntactic Template Retriever for Syntactically-Controlled Paraphrase Generation](https://doi.org/10.18653/v1/2023.emnlp-main.604) |  | 0 |  | Xue Zhang, Songming Zhang, Yunlong Liang, Yufeng Chen, Jian Liu, Wenjuan Han, Jinan Xu |  |
| 1799 |  |  [Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.605) |  | 0 |  | Di Wu, Christof Monz |  |
| 1800 |  |  [Quantifying the redundancy between prosody and text](https://doi.org/10.18653/v1/2023.emnlp-main.606) |  | 0 |  | Lukas Wolf, Tiago Pimentel, Evelina Fedorenko, Ryan Cotterell, Alex Warstadt, Ethan Wilcox, Tamar Regev |  |
| 1801 |  |  [CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.607) |  | 0 |  | Mete Ismayilzada, Debjit Paul, Syrielle Montariol, Mor Geva, Antoine Bosselut |  |
| 1802 |  |  [A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot](https://doi.org/10.18653/v1/2023.emnlp-main.608) |  | 0 |  | Aanisha Bhattacharyya, Yaman Singla, Balaji Krishnamurthy, Rajiv Ratn Shah, Changyou Chen |  |
| 1803 |  |  [Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.609) |  | 0 |  | Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun |  |
| 1804 |  |  [Prompting Scientific Names for Zero-Shot Species Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.610) |  | 0 |  | Shubham Parashar, Zhiqiu Lin, Yanan Li, Shu Kong |  |
| 1805 |  |  [Active Learning for Natural Language Generation](https://doi.org/10.18653/v1/2023.emnlp-main.611) |  | 0 |  | Yotam Perlitz, Ariel Gera, Michal ShmueliScheuer, Dafna Sheinwald, Noam Slonim, Liat EinDor |  |
| 1806 |  |  [Re³Dial: Retrieve, Reorganize and Rescale Conversations for Long-Turn Open-Domain Dialogue Pre-training](https://doi.org/10.18653/v1/2023.emnlp-main.612) |  | 0 |  | Jiaxin Wen, Hao Zhou, Jian Guan, Jie Zhou, Minlie Huang |  |
| 1807 |  |  [MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup](https://doi.org/10.18653/v1/2023.emnlp-main.613) |  | 0 |  | Hua Shen, Vicky Zayats, Johann C. Rocholl, Daniel D. Walker, Dirk Padfield |  |
| 1808 |  |  [Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.614) |  | 0 |  | Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, Yulia Tsvetkov |  |
| 1809 |  |  [Characterizing Mechanisms for Factual Recall in Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.615) |  | 0 |  | Qinan Yu, Jack Merullo, Ellie Pavlick |  |
| 1810 |  |  [MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark](https://doi.org/10.18653/v1/2023.emnlp-main.616) |  | 0 |  | Dominik Macko, Róbert Móro, Adaku Uchendu, Jason Samuel Lucas, Michiharu Yamashita, Matús Pikuliak, Ivan Srba, Thai Le, Dongwon Lee, Jakub Simko, Mária Bieliková |  |
| 1811 |  |  [Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?](https://doi.org/10.18653/v1/2023.emnlp-main.617) |  | 0 |  | Cheng Zhang, Jianyi Cheng, Ilia Shumailov, George A. Constantinides, Yiren Zhao |  |
| 1812 |  |  [Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.618) |  | 0 |  | Srijith Radhakrishnan, ChaoHan Huck Yang, Sumeer Ahmad Khan, Rohit Kumar, Narsis A. Kiani, David GomezCabrero, Jesper Tegnér |  |
| 1813 |  |  [Reducing Sequence Length by Predicting Edit Spans with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.619) |  | 0 |  | Masahiro Kaneko, Naoaki Okazaki |  |
| 1814 |  |  [Instruct and Extract: Instruction Tuning for On-Demand Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.620) |  | 0 |  | Yizhu Jiao, Ming Zhong, Sha Li, Ruining Zhao, Siru Ouyang, Heng Ji, Jiawei Han |  |
| 1815 |  |  [Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.621) |  | 0 |  | Xiaolei Wang, Xinyu Tang, Xin Zhao, Jingyuan Wang, JiRong Wen |  |
| 1816 |  |  [ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness](https://doi.org/10.18653/v1/2023.emnlp-main.622) |  | 0 |  | Archiki Prasad, Swarnadeep Saha, Xiang Zhou, Mohit Bansal |  |
| 1817 |  |  [Expand, Highlight, Generate: RL-driven Document Generation for Passage Reranking](https://doi.org/10.18653/v1/2023.emnlp-main.623) |  | 0 |  | Arian Askari, Mohammad Aliannejadi, Chuan Meng, Evangelos Kanoulas, Suzan Verberne |  |
| 1818 |  |  [Transformer-based Live Update Generation for Soccer Matches from Microblog Posts](https://doi.org/10.18653/v1/2023.emnlp-main.624) |  | 0 |  | Masashi Oshika, Kosuke Yamada, Ryohei Sasano, Koichi Takeda |  |
| 1819 |  |  [Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets](https://doi.org/10.18653/v1/2023.emnlp-main.625) |  | 0 |  | Irina Bejan, Artem Sokolov, Katja Filippova |  |
| 1820 |  |  [Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews](https://doi.org/10.18653/v1/2023.emnlp-main.626) |  | 0 |  | Hye Sun Yun, Iain James Marshall, Thomas A. Trikalinos, Byron C. Wallace |  |
| 1821 |  |  [PromptST: Abstract Prompt Learning for End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-main.627) |  | 0 |  | Tengfei Yu, Liang Ding, Xuebo Liu, Kehai Chen, Meishan Zhang, Dacheng Tao, Min Zhang |  |
| 1822 |  |  [Text Rendering Strategies for Pixel Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.628) |  | 0 |  | Jonas F. Lotz, Elizabeth Salesky, Phillip Rust, Desmond Elliott |  |
| 1823 |  |  [APoLLo : Unified Adapter and Prompt Learning for Vision Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.629) |  | 0 |  | Sanjoy Chowdhury, Sayan Nag, Dinesh Manocha |  |
| 1824 |  |  [SAMRank: Unsupervised Keyphrase Extraction using Self-Attention Map in BERT and GPT-2](https://doi.org/10.18653/v1/2023.emnlp-main.630) |  | 0 |  | Byungha Kang, Youhyun Shin |  |
| 1825 |  |  [Contrastive Learning for Inference in Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.631) |  | 0 |  | Etsuko Ishii, Yan Xu, Bryan Wilie, Ziwei Ji, Holy Lovenia, Willy Chung, Pascale Fung |  |
| 1826 |  |  [Editing Large Language Models: Problems, Methods, and Opportunities](https://doi.org/10.18653/v1/2023.emnlp-main.632) |  | 0 |  | Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang |  |
| 1827 |  |  [MarkQA: A large scale KBQA dataset with numerical reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.633) |  | 0 |  | Xiang Huang, Sitao Cheng, Yuheng Bao, Shanshan Huang, Yuzhong Qu |  |
| 1828 |  |  [Comparing Biases and the Impact of Multilingual Training across Multiple Languages](https://doi.org/10.18653/v1/2023.emnlp-main.634) |  | 0 |  | Sharon Levy, Neha Anna John, Ling Liu, Yogarshi Vyas, Jie Ma, Yoshinari Fujinuma, Miguel Ballesteros, Vittorio Castelli, Dan Roth |  |
| 1829 |  |  [HutCRS: Hierarchical User-Interest Tracking for Conversational Recommender System](https://doi.org/10.18653/v1/2023.emnlp-main.635) |  | 0 |  | Mingjie Qian, Yongsen Zheng, Jinghui Qin, Liang Lin |  |
| 1830 |  |  [Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.636) |  | 0 |  | Xiaoshuai Song, Keqing He, Pei Wang, Guanting Dong, Yutao Mou, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu |  |
| 1831 |  |  [The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining](https://doi.org/10.18653/v1/2023.emnlp-main.637) |  | 0 |  | TingRui Chiang, Dani Yogatama |  |
| 1832 |  |  [Simple and Effective Input Reformulations for Translation](https://doi.org/10.18653/v1/2023.emnlp-main.638) |  | 0 |  | Brian Yu, Hansen Lillemark, Kurt Keutzer |  |
| 1833 |  |  [Pointwise Mutual Information Based Metric and Decoding Strategy for Faithful Generation in Document Grounded Dialogs](https://doi.org/10.18653/v1/2023.emnlp-main.639) |  | 0 |  | Yatin Nandwani, Vineet Kumar, Dinesh Raghu, Sachindra Joshi, Luis A. Lastras |  |
| 1834 |  |  [The ACL OCL Corpus: Advancing Open Science in Computational Linguistics](https://doi.org/10.18653/v1/2023.emnlp-main.640) |  | 0 |  | Shaurya Rohatgi, Yanxia Qin, Benjamin Aw, Niranjana Unnithan, MinYen Kan |  |
| 1835 |  |  [Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.641) |  | 0 |  | Lina Conti, Guillaume Wisniewski |  |
| 1836 |  |  [Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset](https://doi.org/10.18653/v1/2023.emnlp-main.642) |  | 0 |  | Arthur Amalvy, Vincent Labatut, Richard Dufour |  |
| 1837 |  |  [Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting](https://doi.org/10.18653/v1/2023.emnlp-main.643) |  | 0 |  | Preethi Lahoti, Nicholas Blumm, Xiao Ma, Raghavendra Kotikalapudi, Sahitya Potluri, Qijun Tan, Hansa Srinivasan, Ben Packer, Ahmad Beirami, Alex Beutel, Jilin Chen |  |
| 1838 |  |  [Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection](https://doi.org/10.18653/v1/2023.emnlp-main.644) |  | 0 |  | Xinlin Peng, Ying Zhou, Ben He, Le Sun, Yingfei Sun |  |
| 1839 |  |  [Contextual Interaction for Argument Post Quality Assessment](https://doi.org/10.18653/v1/2023.emnlp-main.645) |  | 0 |  | Yiran Wang, Xuanang Chen, Ben He, Le Sun |  |
| 1840 |  |  [Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent Classification](https://doi.org/10.18653/v1/2023.emnlp-main.646) |  | 0 |  | Mujeen Sung, James Gung, Elman Mansimov, Nikolaos Pappas, Raphael Shu, Salvatore Romeo, Yi Zhang, Vittorio Castelli |  |
| 1841 |  |  [Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations](https://doi.org/10.18653/v1/2023.emnlp-main.647) |  | 0 |  | Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, Ming Yin |  |
| 1842 |  |  [GazeVQA: A Video Question Answering Dataset for Multiview Eye-Gaze Task-Oriented Collaborations](https://doi.org/10.18653/v1/2023.emnlp-main.648) |  | 0 |  | Muhammet Furkan Ilaslan, Chenan Song, Joya Chen, Difei Gao, Weixian Lei, Qianli Xu, Joo Lim, Mike Zheng Shou |  |
| 1843 |  |  [People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection](https://doi.org/10.18653/v1/2023.emnlp-main.649) |  | 0 |  | Indira Sen, Dennis Assenmacher, Mattia Samory, Isabelle Augenstein, Wil M. P. van der Aalst, Claudia Wagner |  |
| 1844 |  |  [Unraveling Feature Extraction Mechanisms in Neural Networks](https://doi.org/10.18653/v1/2023.emnlp-main.650) |  | 0 |  | Xiaobing Sun, Jiaxi Li, Wei Lu |  |
| 1845 |  |  [CAPSTONE: Curriculum Sampling for Dense Retrieval with Document Expansion](https://doi.org/10.18653/v1/2023.emnlp-main.651) |  | 0 |  | Xingwei He, Yeyun Gong, ALong Jin, Hang Zhang, Anlei Dong, Jian Jiao, SiuMing Yiu, Nan Duan |  |
| 1846 |  |  [Balance Act: Mitigating Hubness in Cross-Modal Retrieval with Query and Gallery Banks](https://doi.org/10.18653/v1/2023.emnlp-main.652) |  | 0 |  | Yimu Wang, Xiangru Jian, Bo Xue |  |
| 1847 |  |  [E-CORE: Emotion Correlation Enhanced Empathetic Dialogue Generation](https://doi.org/10.18653/v1/2023.emnlp-main.653) |  | 0 |  | Fengyi Fu, Lei Zhang, Quan Wang, Zhendong Mao |  |
| 1848 |  |  [What do Deck Chairs and Sun Hats Have in Common? Uncovering Shared Properties in Large Concept Vocabularies](https://doi.org/10.18653/v1/2023.emnlp-main.654) |  | 0 |  | Amit Gajbhiye, Zied Bouraoui, Na Li, Usashi Chatterjee, Luis Espinosa Anke, Steven Schockaert |  |
| 1849 |  |  [ALDi: Quantifying the Arabic Level of Dialectness of Text](https://doi.org/10.18653/v1/2023.emnlp-main.655) |  | 0 |  | Amr Keleg, Sharon Goldwater, Walid Magdy |  |
| 1850 |  |  [3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding](https://doi.org/10.18653/v1/2023.emnlp-main.656) |  | 0 |  | Zehan Wang, Haifeng Huang, Yang Zhao, Linjun Li, Xize Cheng, Yichen Zhu, Aoxiong Yin, Zhou Zhao |  |
| 1851 |  |  [Goal-Driven Explainable Clustering via Language Descriptions](https://doi.org/10.18653/v1/2023.emnlp-main.657) |  | 0 |  | Zihan Wang, Jingbo Shang, Ruiqi Zhong |  |
| 1852 |  |  [Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.658) |  | 0 |  | Jirui Qi, Raquel Fernández, Arianna Bisazza |  |
| 1853 |  |  [Learning from Mistakes via Cooperative Study Assistant for Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.659) |  | 0 |  | Danqing Wang, Lei Li |  |
| 1854 |  |  [Bridging the Digital Divide: Performance Variation across Socio-Economic Factors in Vision-Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.660) |  | 0 |  | Joan Nwatu, Oana Ignat, Rada Mihalcea |  |
| 1855 |  |  [Conceptor-Aided Debiasing of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.661) |  | 0 |  | Yifei Li, Lyle H. Ungar, João Sedoc |  |
| 1856 |  |  [AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing Evaluation Suite](https://doi.org/10.18653/v1/2023.emnlp-main.662) |  | 0 |  | Jonas Groschwitz, Shay B. Cohen, Lucia Donatelli, Meaghan Fowlie |  |
| 1857 |  |  [Rethinking and Improving Multi-task Learning for End-to-end Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-main.663) |  | 0 |  | Yuhao Zhang, Chen Xu, Bei Li, Hao Chen, Tong Xiao, Chunliang Zhang, Jingbo Zhu |  |
| 1858 |  |  [AD-NLP: A Benchmark for Anomaly Detection in Natural Language Processing](https://doi.org/10.18653/v1/2023.emnlp-main.664) |  | 0 |  | Matei Bejan, Andrei Manolache, Marius Popescu |  |
| 1859 |  |  [Enhancing the Ranking Context of Dense Retrieval through Reciprocal Nearest Neighbors](https://doi.org/10.18653/v1/2023.emnlp-main.665) |  | 0 |  | George Zerveas, Navid Rekabsaz, Carsten Eickhoff |  |
| 1860 |  |  [Cross-Lingual Cross-Target Stance Detection with Dual Knowledge Distillation Framework](https://doi.org/10.18653/v1/2023.emnlp-main.666) |  | 0 |  | Ruike Zhang, Hanxuan Yang, Wenji Mao |  |
| 1861 |  |  [PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs](https://doi.org/10.18653/v1/2023.emnlp-main.667) |  | 0 |  | Rahul Goel, Waleed Ammar, Aditya Gupta, Siddharth Vashishtha, Motoki Sano, Faiz Surani, Max Chang, HyunJeong Choe, David Greene, Chuan He, Rattima Nitisaroj, Anna Trukhina, Shachi Paul, Pararth Shah, Rushin Shah, Zhou Yu |  |
| 1862 |  |  [An Iteratively Parallel Generation Method with the Pre-Filling Strategy for Document-level Event Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.668) |  | 0 |  | Guanhua Huang, Runxin Xu, Ying Zeng, Jiaze Chen, Zhouwang Yang, Weinan E |  |
| 1863 |  |  [CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations](https://doi.org/10.18653/v1/2023.emnlp-main.669) |  | 0 |  | Myra Cheng, Tiziano Piccardi, Diyi Yang |  |
| 1864 |  |  [Reduce Human Labor On Evaluating Conversational Information Retrieval System: A Human-Machine Collaboration Approach](https://doi.org/10.18653/v1/2023.emnlp-main.670) |  | 0 |  | Chen Huang, Peixin Qin, Wenqiang Lei, Jiancheng Lv |  |
| 1865 |  |  [BERTie Bott's Every Flavor Labels: A Tasty Introduction to Semantic Role Labeling for Galician](https://doi.org/10.18653/v1/2023.emnlp-main.671) |  | 0 |  | Micaella Bruton, Meriem Beloucif |  |
| 1866 |  |  [Program Translation via Code Distillation](https://doi.org/10.18653/v1/2023.emnlp-main.672) |  | 0 |  | Yufan Huang, Mengnan Qi, Yongqiang Yao, Maoquan Wang, Bin Gu, Colin B. Clement, Neel Sundaresan |  |
| 1867 |  |  [FaMeSumm: Investigating and Improving Faithfulness of Medical Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.673) |  | 0 |  | Nan Zhang, Yusen Zhang, Wu Guo, Prasenjit Mitra, Rui Zhang |  |
| 1868 |  |  [Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning](https://doi.org/10.18653/v1/2023.emnlp-main.674) |  | 0 |  | Saibo Geng, Martin Josifoski, Maxime Peyrard, Robert West |  |
| 1869 |  |  [Systematic word meta-sense extension](https://doi.org/10.18653/v1/2023.emnlp-main.675) |  | 0 |  | Lei Yu |  |
| 1870 |  |  [Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory](https://doi.org/10.18653/v1/2023.emnlp-main.676) |  | 0 |  | Ziang Xiao, Susu Zhang, Vivian Lai, Q. Vera Liao |  |
| 1871 |  |  [Revisiting the Knowledge Injection Frameworks](https://doi.org/10.18653/v1/2023.emnlp-main.677) |  | 0 |  | Peng Fu, Yiming Zhang, Haobo Wang, Weikang Qiu, Junbo Zhao |  |
| 1872 |  |  [We Are What We Repeatedly Do: Inducing and Deploying Habitual Schemas in Persona-Based Responses](https://doi.org/10.18653/v1/2023.emnlp-main.678) |  | 0 |  | Benjamin Kane, Lenhart K. Schubert |  |
| 1873 |  |  [Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model](https://doi.org/10.18653/v1/2023.emnlp-main.679) |  | 0 |  | Qi Jia, Siyu Ren, Yizhu Liu, Kenny Q. Zhu |  |
| 1874 |  |  [TaskWeb: Selecting Better Source Tasks for Multi-task NLP](https://doi.org/10.18653/v1/2023.emnlp-main.680) |  | 0 |  | Joongwon Kim, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi |  |
| 1875 |  |  [Improving Bias Mitigation through Bias Experts in Natural Language Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.681) |  | 0 |  | Eojin Jeon, Mingyu Lee, Juhyeong Park, Yeachan Kim, WingLam Mok, SangKeun Lee |  |
| 1876 |  |  [Semi-supervised multimodal coreference resolution in image narrations](https://doi.org/10.18653/v1/2023.emnlp-main.682) |  | 0 |  | Arushi Goel, Basura Fernando, Frank Keller, Hakan Bilen |  |
| 1877 |  |  [A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.683) |  | 0 |  | Yi Zhou, José CamachoCollados, Danushka Bollegala |  |
| 1878 |  |  [Argument-based Detection and Classification of Fallacies in Political Debates](https://doi.org/10.18653/v1/2023.emnlp-main.684) |  | 0 |  | Pierpaolo Goffredo, Mariana Espinoza, Serena Villata, Elena Cabrio |  |
| 1879 |  |  [Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation](https://doi.org/10.18653/v1/2023.emnlp-main.685) |  | 0 |  | Wanrong Zhu, Xinyi Wang, Yujie Lu, TsuJui Fu, Xin Wang, Miguel P. Eckstein, William Wang |  |
| 1880 |  |  [SpEL: Structured Prediction for Entity Linking](https://doi.org/10.18653/v1/2023.emnlp-main.686) |  | 0 |  | Hassan Shavarani, Anoop Sarkar |  |
| 1881 |  |  [Architectural Sweet Spots for Modeling Human Label Variation by the Example of Argument Quality: It's Best to Relate Perspectives!](https://doi.org/10.18653/v1/2023.emnlp-main.687) |  | 0 |  | Philipp Heinisch, Matthias Orlikowski, Julia Romberg, Philipp Cimiano |  |
| 1882 |  |  [Explicit Planning Helps Language Models in Logical Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.688) |  | 0 |  | Hongyu Zhao, Kangrui Wang, Mo Yu, Hongyuan Mei |  |
| 1883 |  |  [clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents](https://doi.org/10.18653/v1/2023.emnlp-main.689) |  | 0 |  | Kranti Chalamalasetti, Jana Götze, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, David Schlangen |  |
| 1884 |  |  [Explaining with Contrastive Phrasal Highlighting: A Case Study in Assisting Humans to Detect Translation Differences](https://doi.org/10.18653/v1/2023.emnlp-main.690) |  | 0 |  | Eleftheria Briakou, Navita Goyal, Marine Carpuat |  |
| 1885 |  |  [Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in Foundation Models](https://doi.org/10.18653/v1/2023.emnlp-main.691) |  | 0 |  | Tim Schott, Daniel Furman, Shreshta Bhat |  |
| 1886 |  |  [Anchoring Fine-tuning of Sentence Transformer with Semantic Label Information for Efficient Truly Few-shot Classification](https://doi.org/10.18653/v1/2023.emnlp-main.692) |  | 0 |  | Amalie Brogaard Pauli, Leon Derczynski, Ira Assent |  |
| 1887 |  |  [UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers](https://doi.org/10.18653/v1/2023.emnlp-main.693) |  | 0 |  | Jon SaadFalcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md. Arafat Sultan, Christopher Potts |  |
| 1888 |  |  [TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings](https://doi.org/10.18653/v1/2023.emnlp-main.694) |  | 0 |  | Hans W. A. Hanley, Zakir Durumeric |  |
| 1889 |  |  [Data Similarity is Not Enough to Explain Language Model Performance](https://doi.org/10.18653/v1/2023.emnlp-main.695) |  | 0 |  | Gregory Yauney, Emily Reif, David Mimno |  |
| 1890 |  |  [Zero-shot Sharpness-Aware Quantization for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.696) |  | 0 |  | Miaoxi Zhu, Qihuang Zhong, Li Shen, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao |  |
| 1891 |  |  [Deciphering Stereotypes in Pre-Trained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.697) |  | 0 |  | Weicheng Ma, Henry Scheible, Brian Wang, Goutham Veeramachaneni, Pratim Chowdhary, Alan Sun, Andrew Koulogeorge, Lili Wang, Diyi Yang, Soroush Vosoughi |  |
| 1892 |  |  [An "Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives"](https://doi.org/10.18653/v1/2023.emnlp-main.698) |  | 0 |  | Young Min Cho, Sunny Rai, Lyle H. Ungar, João Sedoc, Sharath Chandra Guntuku |  |
| 1893 |  |  [Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark](https://doi.org/10.18653/v1/2023.emnlp-main.699) |  | 0 |  | Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, David Jurgens |  |
| 1894 |  |  [Interventional Rationalization](https://doi.org/10.18653/v1/2023.emnlp-main.700) |  | 0 |  | Linan Yue, Qi Liu, Li Wang, Yanqing An, Yichao Du, Zhenya Huang |  |
| 1895 |  |  [Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting](https://doi.org/10.18653/v1/2023.emnlp-main.701) |  | 0 |  | Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap |  |
| 1896 |  |  [Axiomatic Preference Modeling for Longform Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.702) |  | 0 |  | Corby Rosset, Guoqing Zheng, Victor Dibia, Ahmed Awadallah, Paul N. Bennett |  |
| 1897 |  |  [Countering Misinformation via Emotional Response Generation](https://doi.org/10.18653/v1/2023.emnlp-main.703) |  | 0 |  | Daniel Russo, Shane P. KaszefskiYaschuk, Jacopo Staiano, Marco Guerini |  |
| 1898 |  |  [Seq2seq is All You Need for Coreference Resolution](https://doi.org/10.18653/v1/2023.emnlp-main.704) |  | 0 |  | Wenzheng Zhang, Sam Wiseman, Karl Stratos |  |
| 1899 |  |  [Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection](https://doi.org/10.18653/v1/2023.emnlp-main.705) |  | 0 |  | Dennis Fucci, Marco Gaido, Sara Papi, Mauro Cettolo, Matteo Negri, Luisa Bentivogli |  |
| 1900 |  |  [StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.706) |  | 0 |  | Cheng Jiayang, Lin Qiu, Tsz Ho Chan, Tianqing Fang, Weiqi Wang, Chunkit Chan, Dongyu Ru, Qipeng Guo, Hongming Zhang, Yangqiu Song, Yue Zhang, Zheng Zhang |  |
| 1901 |  |  [Beyond Detection: A Defend-and-Summarize Strategy for Robust and Interpretable Rumor Analysis on Social Media](https://doi.org/10.18653/v1/2023.emnlp-main.707) |  | 0 |  | YiTing Chang, YunZhu Song, YiSyuan Chen, HongHan Shuai |  |
| 1902 |  |  [Crystal: Introspective Reasoners Reinforced with Self-Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.708) |  | 0 |  | Jiacheng Liu, Ramakanth Pasunuru, Hannaneh Hajishirzi, Yejin Choi, Asli Celikyilmaz |  |
| 1903 |  |  [DiffS2UT: A Semantic Preserving Diffusion Model for Textless Direct Speech-to-Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-main.709) |  | 0 |  | Yongxin Zhu, Zhujin Gao, Xinyuan Zhou, Zhongyi Ye, Linli Xu |  |
| 1904 |  |  [BioFEG: Generate Latent Features for Biomedical Entity Linking](https://doi.org/10.18653/v1/2023.emnlp-main.710) |  | 0 |  | Xuhui Sui, Ying Zhang, Xiangrui Cai, Kehui Song, Baohang Zhou, Xiaojie Yuan, Wensheng Zhang |  |
| 1905 |  |  [TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.711) |  | 0 |  | Jing Xiong, Jianhao Shen, Ye Yuan, Haiming Wang, Yichun Yin, Zhengying Liu, Lin Li, Zhijiang Guo, Qingxing Cao, Yinya Huang, Chuanyang Zheng, Xiaodan Liang, Ming Zhang, Qun Liu |  |
| 1906 |  |  [Physician Detection of Clinical Harm in Machine Translation: Quality Estimation Aids in Reliance and Backtranslation Identifies Critical Errors](https://doi.org/10.18653/v1/2023.emnlp-main.712) |  | 0 |  | Nikita Mehandru, Sweta Agrawal, Yimin Xiao, Ge Gao, Elaine C. Khoong, Marine Carpuat, Niloufar Salehi |  |
| 1907 |  |  [Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive](https://doi.org/10.18653/v1/2023.emnlp-main.713) |  | 0 |  | Tharindu Cyril Weerasooriya, Sujan Dutta, Tharindu Ranasinghe, Marcos Zampieri, Christopher Homan, Ashiqur R. KhudaBukhsh |  |
| 1908 |  |  [Generating Summaries with Controllable Readability Levels](https://doi.org/10.18653/v1/2023.emnlp-main.714) |  | 0 |  | Leonardo F. R. Ribeiro, Mohit Bansal, Markus Dreyer |  |
| 1909 |  |  [mAggretriever: A Simple yet Effective Approach to Zero-Shot Multilingual Dense Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.715) |  | 0 |  | ShengChieh Lin, Amin Ahmad, Jimmy Lin |  |
| 1910 |  |  [CodeFusion: A Pre-trained Diffusion Model for Code Generation](https://doi.org/10.18653/v1/2023.emnlp-main.716) |  | 0 |  | Mukul Singh, José Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Gust Verbruggen |  |
| 1911 |  |  [CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs](https://doi.org/10.18653/v1/2023.emnlp-main.717) |  | 0 |  | Taha Aksu, Devamanyu Hazarika, Shikib Mehri, Seokhwan Kim, Dilek HakkaniTur, Yang Liu, Mahdi Namazifar |  |
| 1912 |  |  [VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights](https://doi.org/10.18653/v1/2023.emnlp-main.718) |  | 0 |  | Shanshan Xu, Leon Staufer, T. Y. S. S. Santosh, Oana Ichim, Corina Heri, Matthias Grabmair |  |
| 1913 |  |  [ACQUIRED: A Dataset for Answering Counterfactual Questions In Real-Life Videos](https://doi.org/10.18653/v1/2023.emnlp-main.719) |  | 0 |  | TeLin Wu, ZiYi Dou, Qingyuan Hu, Yu Hou, Nischal Reddy Chandra, Marjorie Freedman, Ralph M. Weischedel, Nanyun Peng |  |
| 1914 |  |  [From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base](https://doi.org/10.18653/v1/2023.emnlp-main.720) |  | 0 |  | Wangzhen Guo, Linyin Luo, Hanjiang Lai, Jian Yin |  |
| 1915 |  |  [Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model](https://doi.org/10.18653/v1/2023.emnlp-main.721) |  | 0 |  | Haikang Deng, Colin Raffel |  |
| 1916 |  |  [CORE: A Few-Shot Company Relation Classification Dataset for Robust Domain Adaptation](https://doi.org/10.18653/v1/2023.emnlp-main.722) |  | 0 |  | Philipp Borchert, Jochen De Weerdt, Kristof Coussement, Arno De Caigny, MarieFrancine Moens |  |
| 1917 |  |  [Models See Hallucinations: Evaluating the Factuality in Video Captioning](https://doi.org/10.18653/v1/2023.emnlp-main.723) |  | 0 |  | Hui Liu, Xiaojun Wan |  |
| 1918 |  |  [Back Transcription as a Method for Evaluating Robustness of Natural Language Understanding Models to Speech Recognition Errors](https://doi.org/10.18653/v1/2023.emnlp-main.724) |  | 0 |  | Marek Kubis, Pawel Skórzewski, Marcin Sowanski, Tomasz Zietkiewicz |  |
| 1919 |  |  [Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models for Learning Conceptual Spaces](https://doi.org/10.18653/v1/2023.emnlp-main.725) |  | 0 |  | Usashi Chatterjee, Amit Gajbhiye, Steven Schockaert |  |
| 1920 |  |  [Can Language Models Understand Physical Concepts?](https://doi.org/10.18653/v1/2023.emnlp-main.726) |  | 0 |  | Lei Li, Jingjing Xu, Qingxiu Dong, Ce Zheng, Xu Sun, Lingpeng Kong, Qi Liu |  |
| 1921 |  |  [SPT: Learning to Selectively Insert Prompts for Better Prompt Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.727) |  | 0 |  | Wei Zhu, Ming Tan |  |
| 1922 |  |  [Once Upon a Time in Graph: Relative-Time Pretraining for Complex Temporal Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.728) |  | 0 |  | Sen Yang, Xin Li, Lidong Bing, Wai Lam |  |
| 1923 |  |  [Expository Text Generation: Imitate, Retrieve, Paraphrase](https://doi.org/10.18653/v1/2023.emnlp-main.729) |  | 0 |  | Nishant Balepur, Jie Huang, Kevin ChenChuan Chang |  |
| 1924 |  |  [Large-scale similarity search with Optimal Transport](https://doi.org/10.18653/v1/2023.emnlp-main.730) |  | 0 |  | Cléa Laouar, Yuki Takezawa, Makoto Yamada |  |
| 1925 |  |  [Enhancing Textbooks with Visuals from the Web for Improved Learning](https://doi.org/10.18653/v1/2023.emnlp-main.731) |  | 0 |  | Janvijay Singh, Vilém Zouhar, Mrinmaya Sachan |  |
| 1926 |  |  [Continual Event Extraction with Semantic Confusion Rectification](https://doi.org/10.18653/v1/2023.emnlp-main.732) |  | 0 |  | Zitao Wang, Xinyi Wang, Wei Hu |  |
| 1927 |  |  [An Empirical Study of Translation Hypothesis Ensembling with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.733) |  | 0 |  | António Farinhas, José Guilherme Camargo de Souza, André F. T. Martins |  |
| 1928 |  |  [FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning](https://doi.org/10.18653/v1/2023.emnlp-main.734) |  | 0 |  | Jaemin Shin, Hyungjun Yoon, Seungjoo Lee, Sungjoon Park, Yunxin Liu, Jinho D. Choi, SungJu Lee |  |
| 1929 |  |  [Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.735) |  | 0 |  | Geewook Kim, Hodong Lee, Daehee Kim, Haeji Jung, Sanghee Park, Yoonsik Kim, Sangdoo Yun, Taeho Kil, Bado Lee, Seunghyun Park |  |
| 1930 |  |  [Continual Learning for Multilingual Neural Machine Translation via Dual Importance-based Model Division](https://doi.org/10.18653/v1/2023.emnlp-main.736) |  | 0 |  | Junpeng Liu, Kaiyu Huang, Hao Yu, Jiuyi Li, Jinsong Su, Degen Huang |  |
| 1931 |  |  [SimCSE++: Improving Contrastive Learning for Sentence Embeddings from Two Perspectives](https://doi.org/10.18653/v1/2023.emnlp-main.737) |  | 0 |  | Jiahao Xu, Wei Shao, Lihui Chen, Lemao Liu |  |
| 1932 |  |  [Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.738) |  | 0 |  | Jiaao Chen, Diyi Yang |  |
| 1933 |  |  [Simplicity Level Estimate (SLE): A Learned Reference-Less Metric for Sentence Simplification](https://doi.org/10.18653/v1/2023.emnlp-main.739) |  | 0 |  | Liam Cripwell, Joël Legrand, Claire Gardent |  |
| 1934 |  |  [Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration](https://doi.org/10.18653/v1/2023.emnlp-main.740) |  | 0 |  | Yiquan Wu, Siying Zhou, Yifei Liu, Weiming Lu, Xiaozhong Liu, Yating Zhang, Changlong Sun, Fei Wu, Kun Kuang |  |
| 1935 |  |  [FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.741) |  | 0 |  | Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wentau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi |  |
| 1936 |  |  [Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems](https://doi.org/10.18653/v1/2023.emnlp-main.742) |  | 0 |  | Marek Kadlcík, Michal Stefánik, Ondrej Sotolár, Vlastimil Martinek |  |
| 1937 |  |  [CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.743) |  | 0 |  | Hoang Nguyen, Ye Liu, Chenwei Zhang, Tao Zhang, Philip S. Yu |  |
| 1938 |  |  [When Language Models Fall in Love: Animacy Processing in Transformer Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.744) |  | 0 |  | Michael Hanna, Yonatan Belinkov, Sandro Pezzelle |  |
| 1939 |  |  [Improving Unsupervised Relation Extraction by Augmenting Diverse Sentence Pairs](https://doi.org/10.18653/v1/2023.emnlp-main.745) |  | 0 |  | Qing Wang, Kang Zhou, Qiao Qiao, Yuepei Li, Qi Li |  |
| 1940 |  |  [Paraphrase Types for Generation and Detection](https://doi.org/10.18653/v1/2023.emnlp-main.746) |  | 0 |  | Jan Philip Wahle, Bela Gipp, Terry Ruas |  |
| 1941 |  |  [Target-to-Source Augmentation for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.747) |  | 0 |  | Yice Zhang, Yifan Yang, Meng Li, Bin Liang, Shiwei Chen, Ruifeng Xu |  |
| 1942 |  |  [PAC-tuning: Fine-tuning Pre-trained Language Models with PAC-driven Perturbed Gradient Descent](https://doi.org/10.18653/v1/2023.emnlp-main.748) |  | 0 |  | Guangliang Liu, Zhiyu Xue, Xitong Zhang, Kristen Marie Johnson, Rongrong Wang |  |
| 1943 |  |  [Emergence of Abstract State Representations in Embodied Sequence Modeling](https://doi.org/10.18653/v1/2023.emnlp-main.749) |  | 0 |  | Tian Yun, Zilai Zeng, Kunal Handa, Ashish V. Thapliyal, Bo Pang, Ellie Pavlick, Chen Sun |  |
| 1944 |  |  [Accelerating Toeplitz Neural Network with Constant-time Inference Complexity](https://doi.org/10.18653/v1/2023.emnlp-main.750) |  | 0 |  | Zhen Qin, Yiran Zhong |  |
| 1945 |  |  [Dissecting Recall of Factual Associations in Auto-Regressive Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.751) |  | 0 |  | Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson |  |
| 1946 |  |  [StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.752) |  | 0 |  | Sullam Jeoung, Yubin Ge, Jana Diesner |  |
| 1947 |  |  [Select, Prompt, Filter: Distilling Large Language Models for Summarizing Conversations](https://doi.org/10.18653/v1/2023.emnlp-main.753) |  | 0 |  | MinhQuang Pham, Sathish Indurthi, Shamil Chollampatt, Marco Turchi |  |
| 1948 |  |  [Human Raters Cannot Distinguish English Translations from Original English Texts](https://doi.org/10.18653/v1/2023.emnlp-main.754) |  | 0 |  | Shira Wein |  |
| 1949 |  |  [Impressions: Visual Semiotics and Aesthetic Impact Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.755) |  | 0 |  | Julia Kruk, Caleb Ziems, Diyi Yang |  |
| 1950 |  |  [DNA: Denoised Neighborhood Aggregation for Fine-grained Category Discovery](https://doi.org/10.18653/v1/2023.emnlp-main.756) |  | 0 |  | Wenbin An, Feng Tian, Wenkai Shi, Yan Chen, Qinghua Zheng, Qianying Wang, Ping Chen |  |
| 1951 |  |  [Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.757) |  | 0 |  | Shuai Zhao, Jinming Wen, Anh Tuan Luu, Junbo Zhao, Jie Fu |  |
| 1952 |  |  [UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation](https://doi.org/10.18653/v1/2023.emnlp-main.758) |  | 0 |  | Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Weiwei Deng, Qi Zhang |  |
| 1953 |  |  [KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning](https://doi.org/10.18653/v1/2023.emnlp-main.759) |  | 0 |  | Xiao Yu, Qingyang Wu, Kun Qian, Zhou Yu |  |
| 1954 |  |  [Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU](https://doi.org/10.18653/v1/2023.emnlp-main.760) |  | 0 |  | Fajri Koto, Nurul Aisyah, Haonan Li, Timothy Baldwin |  |
| 1955 |  |  [Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.761) |  | 0 |  | Pranjal Aggarwal, Aman Madaan, Yiming Yang, Mausam |  |
| 1956 |  |  [Bridging Information-Theoretic and Geometric Compression in Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.762) |  | 0 |  | Emily Cheng, Corentin Kervadec, Marco Baroni |  |
| 1957 |  |  [Pre-training Language Models for Comparative Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.763) |  | 0 |  | Mengxia Yu, Zhihan Zhang, Wenhao Yu, Meng Jiang |  |
| 1958 |  |  [Improved Pseudo Data for Machine Translation Quality Estimation with Constrained Beam Search](https://doi.org/10.18653/v1/2023.emnlp-main.764) |  | 0 |  | Xiang Geng, Yu Zhang, Zhejian Lai, Shuaijie She, Wei Zou, Shimin Tao, Hao Yang, Jiajun Chen, Shujian Huang |  |
| 1959 |  |  [Text Embeddings Reveal (Almost) As Much As Text](https://doi.org/10.18653/v1/2023.emnlp-main.765) |  | 0 |  | John X. Morris, Volodymyr Kuleshov, Vitaly Shmatikov, Alexander M. Rush |  |
| 1960 |  |  [AutoTrial: Prompting Language Models for Clinical Trial Design](https://doi.org/10.18653/v1/2023.emnlp-main.766) |  | 0 |  | Zifeng Wang, Cao Xiao, Jimeng Sun |  |
| 1961 |  |  [Faster Minimum Bayes Risk Decoding with Confidence-based Pruning](https://doi.org/10.18653/v1/2023.emnlp-main.767) |  | 0 |  | Julius Cheng, Andreas Vlachos |  |
| 1962 |  |  [Enhancing Generative Retrieval with Reinforcement Learning from Relevance Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.768) |  | 0 |  | Yujia Zhou, Zhicheng Dou, JiRong Wen |  |
| 1963 |  |  [Multi-Source Probing for Open-Domain Conversational Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.769) |  | 0 |  | Yuanxi Li, Hao Zhou, Jie Zhou, Minlie Huang |  |
| 1964 |  |  [Hallucination Mitigation in Natural Language Generation from Large-Scale Open-Domain Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.770) |  | 0 |  | Xiao Shi, Zhengyuan Zhu, Zeyu Zhang, Chengkai Li |  |
| 1965 |  |  [Multi-Source Multi-Type Knowledge Exploration and Exploitation for Dialogue Generation](https://doi.org/10.18653/v1/2023.emnlp-main.771) |  | 0 |  | Xuanfan Ni, Hongliang Dai, Zhaochun Ren, Piji Li |  |
| 1966 |  |  [Focus Your Attention (with Adaptive IIR Filters)](https://doi.org/10.18653/v1/2023.emnlp-main.772) |  | 0 |  | Shahar Lutati, Itamar Zimerman, Lior Wolf |  |
| 1967 |  |  [Identifying Statements Crucial for Awareness of Interpretive Nonsense to Prevent Communication Breakdowns](https://doi.org/10.18653/v1/2023.emnlp-main.773) |  | 0 |  | Tomoyuki Maekawa, Michita Imai |  |
| 1968 |  |  [Multilingual Large Language Models Are Not (Yet) Code-Switchers](https://doi.org/10.18653/v1/2023.emnlp-main.774) |  | 0 |  | Ruochen Zhang, Samuel Cahyawijaya, Jan Christian Blaise Cruz, Genta Indra Winata, Alham Fikri Aji |  |
| 1969 |  |  [Reinforced Target-driven Conversational Promotion](https://doi.org/10.18653/v1/2023.emnlp-main.775) |  | 0 |  | Huy Dao, Lizi Liao, Dung D. Le, Yuxiang Nie |  |
| 1970 |  |  [Identification of Multimodal Stance Towards Frames of Communication](https://doi.org/10.18653/v1/2023.emnlp-main.776) |  | 0 |  | Maxwell A. Weinzierl, Sanda M. Harabagiu |  |
| 1971 |  |  [Unsupervised Sounding Pixel Learning](https://doi.org/10.18653/v1/2023.emnlp-main.777) |  | 0 |  | Yining Zhang, Yanli Ji, Yang Yang |  |
| 1972 |  |  [LM vs LM: Detecting Factual Errors via Cross Examination](https://doi.org/10.18653/v1/2023.emnlp-main.778) |  | 0 |  | Roi Cohen, May Hamri, Mor Geva, Amir Globerson |  |
| 1973 |  |  [Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.779) |  | 0 |  | Bram van Dijk, Tom Kouwenhoven, Marco Spruit, Max Johannes van Duijn |  |
| 1974 |  |  [PIEClass: Weakly-Supervised Text Classification with Prompting and Noise-Robust Iterative Ensemble Training](https://doi.org/10.18653/v1/2023.emnlp-main.780) |  | 0 |  | Yunyi Zhang, Minhao Jiang, Yu Meng, Yu Zhang, Jiawei Han |  |
| 1975 |  |  [MeaeQ: Mount Model Extraction Attacks with Efficient Queries](https://doi.org/10.18653/v1/2023.emnlp-main.781) |  | 0 |  | Chengwei Dai, Minxuan Lv, Kun Li, Wei Zhou |  |
| 1976 |  |  [The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.782) |  | 0 |  | Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, Minjoon Seo |  |
| 1977 |  |  [Explaining Interactions Between Text Spans](https://doi.org/10.18653/v1/2023.emnlp-main.783) |  | 0 |  | Sagnik Ray Choudhury, Pepa Atanasova, Isabelle Augenstein |  |
| 1978 |  |  [Predictive Chemistry Augmented with Text Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.784) |  | 0 |  | Yujie Qian, Zhening Li, Zhengkai Tu, Connor W. Coley, Regina Barzilay |  |
| 1979 |  |  [System Combination via Quality Estimation for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.emnlp-main.785) |  | 0 |  | Muhammad Reza Qorib, Hwee Tou Ng |  |
| 1980 |  |  [Rethinking Negative Pairs in Code Search](https://doi.org/10.18653/v1/2023.emnlp-main.786) |  | 0 |  | Haochen Li, Xin Zhou, Anh Tuan Luu, Chunyan Miao |  |
| 1981 |  |  [Question Answering as Programming for Solving Time-Sensitive Questions](https://doi.org/10.18653/v1/2023.emnlp-main.787) |  | 0 |  | Xinyu Zhu, Cheng Yang, Bei Chen, Siheng Li, JianGuang Lou, Yujiu Yang |  |
| 1982 |  |  [Joint Geometrical and Statistical Domain Adaptation for Cross-domain Code Vulnerability Detection](https://doi.org/10.18653/v1/2023.emnlp-main.788) |  | 0 |  | Qianjin Du, Shiji Zhou, Xiaohui Kuang, Gang Zhao, Jidong Zhai |  |
| 1983 |  |  [Revisiting Sparse Retrieval for Few-shot Entity Linking](https://doi.org/10.18653/v1/2023.emnlp-main.789) |  | 0 |  | Yulin Chen, Zhenran Xu, Baotian Hu, Min Zhang |  |
| 1984 |  |  [Controlling Pre-trained Language Models for Grade-Specific Text Simplification](https://doi.org/10.18653/v1/2023.emnlp-main.790) |  | 0 |  | Sweta Agrawal, Marine Carpuat |  |
| 1985 |  |  [CLEVR-Implicit: A Diagnostic Dataset for Implicit Reasoning in Referring Expression Comprehension](https://doi.org/10.18653/v1/2023.emnlp-main.791) |  | 0 |  | Jingwei Zhang, Xin Wu, Yi Cai |  |
| 1986 |  |  ["Are Your Explanations Reliable?" Investigating the Stability of LIME in Explaining Text Classifiers by Marrying XAI and Adversarial Attack](https://doi.org/10.18653/v1/2023.emnlp-main.792) |  | 0 |  | Christopher Burger, Lingwei Chen, Thai Le |  |
| 1987 |  |  [CQE: A Comprehensive Quantity Extractor](https://doi.org/10.18653/v1/2023.emnlp-main.793) |  | 0 |  | Satya Almasian, Vivian Kazakova, Philip Göldner, Michael Gertz |  |
| 1988 |  |  [Context Compression for Auto-regressive Transformers with Sentinel Tokens](https://doi.org/10.18653/v1/2023.emnlp-main.794) |  | 0 |  | Siyu Ren, Qi Jia, Kenny Q. Zhu |  |
| 1989 |  |  [A Unified View of Evaluation Metrics for Structured Prediction](https://doi.org/10.18653/v1/2023.emnlp-main.795) |  | 0 |  | Yunmo Chen, William Gantt, Tongfei Chen, Aaron Steven White, Benjamin Van Durme |  |
| 1990 |  |  [A Deeper (Autoregressive) Approach to Non-Convergent Discourse Parsing](https://doi.org/10.18653/v1/2023.emnlp-main.796) |  | 0 |  | Oren Tsur, Yoav Tulpan |  |
| 1991 |  |  [We are Who We Cite: Bridges of Influence Between Natural Language Processing and Other Academic Fields](https://doi.org/10.18653/v1/2023.emnlp-main.797) |  | 0 |  | Jan Philip Wahle, Terry Ruas, Mohamed Abdalla, Bela Gipp, Saif M. Mohammad |  |
| 1992 |  |  [Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration](https://doi.org/10.18653/v1/2023.emnlp-main.798) |  | 0 |  | Daniel Deutsch, George F. Foster, Markus Freitag |  |
| 1993 |  |  [SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization](https://doi.org/10.18653/v1/2023.emnlp-main.799) |  | 0 |  | Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, Yejin Choi |  |
| 1994 |  |  [Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.800) |  | 0 |  | Zhiwei Hu, Víctor GutiérrezBasulto, Zhiliang Xiang, Ru Li, Jeff Z. Pan |  |
| 1995 |  |  [MailEx: Email Event and Argument Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.801) |  | 0 |  | Saurabh Srivastava, Gaurav Singh, Shou Matsumoto, Ali K. Raz, Paulo C. G. Costa, Joshua Poore, Ziyu Yao |  |
| 1996 |  |  [Optimized Tokenization for Transcribed Error Correction](https://doi.org/10.18653/v1/2023.emnlp-main.802) |  | 0 |  | Tomer Wullach, Shlomo E. Chazan |  |
| 1997 |  |  [Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.803) |  | 0 |  | Yi Su, Yixin Ji, Juntao Li, Hai Ye, Min Zhang |  |
| 1998 |  |  [Generative Adversarial Training with Perturbed Token Detection for Model Robustness](https://doi.org/10.18653/v1/2023.emnlp-main.804) |  | 0 |  | Jiahao Zhao, Wenji Mao |  |
| 1999 |  |  [Multi-Task Knowledge Distillation with Embedding Constraints for Scholarly Keyphrase Boundary Classification](https://doi.org/10.18653/v1/2023.emnlp-main.805) |  | 0 |  | Seo Park, Cornelia Caragea |  |
| 2000 |  |  [Set Learning for Generative Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.806) |  | 0 |  | Jiangnan Li, Yice Zhang, Bin Liang, KamFai Wong, Ruifeng Xu |  |
| 2001 |  |  [Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation](https://doi.org/10.18653/v1/2023.emnlp-main.807) |  | 0 |  | Anastasia Kritharoula, Maria Lymperaiou, Giorgos Stamou |  |
| 2002 |  |  [Be Selfish, But Wisely: Investigating the Impact of Agent Personality in Mixed-Motive Human-Agent Interactions](https://doi.org/10.18653/v1/2023.emnlp-main.808) |  | 0 |  | Kushal Chawla, Ian Wu, Yu Rong, Gale M. Lucas, Jonathan Gratch |  |
| 2003 |  |  [Doolittle: Benchmarks and Corpora for Academic Writing Formalization](https://doi.org/10.18653/v1/2023.emnlp-main.809) |  | 0 |  | Shizhe Diao, Yongyu Lei, Liangming Pan, Tianqing Fang, Wangchunshu Zhou, Sedrick Scott Keh, MinYen Kan, Tong Zhang |  |
| 2004 |  |  [Token Prediction as Implicit Classification to Identify LLM-Generated Text](https://doi.org/10.18653/v1/2023.emnlp-main.810) |  | 0 |  | Yutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, Bhiksha Raj |  |
| 2005 |  |  [On Evaluation of Bangla Word Analogies](https://doi.org/10.18653/v1/2023.emnlp-main.811) |  | 0 |  | Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu |  |
| 2006 |  |  [Reconstruct Before Summarize: An Efficient Two-Step Framework for Condensing and Summarizing Meeting Transcripts](https://doi.org/10.18653/v1/2023.emnlp-main.812) |  | 0 |  | Haochen Tan, Han Wu, Wei Shao, Xinyun Zhang, Mingjie Zhan, Zhaohui Hou, Ding Liang, Linqi Song |  |
| 2007 |  |  [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.813) |  | 0 |  | Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa |  |
| 2008 |  |  [Character-LLM: A Trainable Agent for Role-Playing](https://doi.org/10.18653/v1/2023.emnlp-main.814) |  | 0 |  | Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu |  |
| 2009 |  |  [Natural Language Decompositions of Implicit Content Enable Better Text Representations](https://doi.org/10.18653/v1/2023.emnlp-main.815) |  | 0 |  | Alexander Miserlis Hoyle, Rupak Sarkar, Pranav Goel, Philip Resnik |  |
| 2010 |  |  [A Scalable Framework for Table of Contents Extraction from Complex ESG Annual Reports](https://doi.org/10.18653/v1/2023.emnlp-main.816) |  | 0 |  | Xinyu Wang, Lin Gui, Yulan He |  |
| 2011 |  |  [Semantic Space Grounded Weighted Decoding for Multi-Attribute Controllable Dialogue Generation](https://doi.org/10.18653/v1/2023.emnlp-main.817) |  | 0 |  | Zhiling Zhang, Mengyue Wu, Kenny Q. Zhu |  |
| 2012 |  |  [How do languages influence each other? Studying cross-lingual data sharing during LM fine-tuning](https://doi.org/10.18653/v1/2023.emnlp-main.818) |  | 0 |  | Rochelle Choenni, Dan Garrette, Ekaterina Shutova |  |
| 2013 |  |  [COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation](https://doi.org/10.18653/v1/2023.emnlp-main.819) |  | 0 |  | Nan Wang, Qifan Wang, YiChia Wang, Maziar Sanjabi, Jingzhou Liu, Hamed Firooz, Hongning Wang, Shaoliang Nie |  |
| 2014 |  |  [NameGuess: Column Name Expansion for Tabular Data](https://doi.org/10.18653/v1/2023.emnlp-main.820) |  | 0 |  | Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Shen Wang, Huzefa Rangwala, George Karypis |  |
| 2015 |  |  [BLESS: Benchmarking Large Language Models on Sentence Simplification](https://doi.org/10.18653/v1/2023.emnlp-main.821) |  | 0 |  | Tannon Kew, Alison Chi, Laura VásquezRodríguez, Sweta Agrawal, Dennis Aumiller, Fernando AlvaManchego, Matthew Shardlow |  |
| 2016 |  |  [To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing](https://doi.org/10.18653/v1/2023.emnlp-main.822) |  | 0 |  | Sireesh Gururaja, Amanda Bertsch, Clara Na, David Gray Widder, Emma Strubell |  |
| 2017 |  |  [PALS: Personalized Active Learning for Subjective Tasks in NLP](https://doi.org/10.18653/v1/2023.emnlp-main.823) |  | 0 |  | Kamil Kanclerz, Konrad Karanowski, Julita Bielaniewicz, Marcin Gruza, Piotr Milkowski, Jan Kocon, Przemyslaw Kazienko |  |
| 2018 |  |  [ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation](https://doi.org/10.18653/v1/2023.emnlp-main.824) |  | 0 |  | Yangyi Chen, Xingyao Wang, Manling Li, Derek Hoiem, Heng Ji |  |
| 2019 |  |  [LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.825) |  | 0 |  | Huiqiang Jiang, Qianhui Wu, ChinYew Lin, Yuqing Yang, Lili Qiu |  |
| 2020 |  |  [EXPLAIN, EDIT, GENERATE: Rationale-Sensitive Counterfactual Data Augmentation for Multi-hop Fact Verification](https://doi.org/10.18653/v1/2023.emnlp-main.826) |  | 0 |  | Yingjie Zhu, Jiasheng Si, Yibo Zhao, Haiyang Zhu, Deyu Zhou, Yulan He |  |
| 2021 |  |  [An Exploration of Left-Corner Transformations](https://doi.org/10.18653/v1/2023.emnlp-main.827) |  | 0 |  | Andreas Opedal, Eleftheria Tsipidi, Tiago Pimentel, Ryan Cotterell, Tim Vieira |  |
| 2022 |  |  [Characterizing and Verifying Scientific Claims: Qualitative Causal Structure is All You Need](https://doi.org/10.18653/v1/2023.emnlp-main.828) |  | 0 |  | Jinxuan Wu, Wenhan Chao, Xian Zhou, Zhunchen Luo |  |
| 2023 |  |  [FOCUS: Effective Embedding Initialization for Monolingual Specialization of Multilingual Models](https://doi.org/10.18653/v1/2023.emnlp-main.829) |  | 0 |  | Konstantin Dobler, Gerard de Melo |  |
| 2024 |  |  [ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games](https://doi.org/10.18653/v1/2023.emnlp-main.830) |  | 0 |  | Ruoyao Wang, Graham Todd, Xingdi Yuan, Ziang Xiao, MarcAlexandre Côté, Peter A. Jansen |  |
| 2025 |  |  [Skill-Based Few-Shot Selection for In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.831) |  | 0 |  | Shengnan An, Bo Zhou, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Weizhu Chen, JianGuang Lou |  |
| 2026 |  |  [MaNtLE: Model-agnostic Natural Language Explainer](https://doi.org/10.18653/v1/2023.emnlp-main.832) |  | 0 |  | Rakesh R. Menon, Kerem Zaman, Shashank Srivastava |  |
| 2027 |  |  [PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer](https://doi.org/10.18653/v1/2023.emnlp-main.833) |  | 0 |  | Lichang Chen, Jiuhai Chen, Heng Huang, Minhao Cheng |  |
| 2028 |  |  [Ling-CL: Understanding NLP Models through Linguistic Curricula](https://doi.org/10.18653/v1/2023.emnlp-main.834) |  | 0 |  | Mohamed Elgaar, Hadi Amiri |  |
| 2029 |  |  [Towards Unsupervised Recognition of Token-level Semantic Differences in Related Documents](https://doi.org/10.18653/v1/2023.emnlp-main.835) |  | 0 |  | Jannis Vamvas, Rico Sennrich |  |
| 2030 |  |  [Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance](https://doi.org/10.18653/v1/2023.emnlp-main.836) |  | 0 |  | Shaomu Tan, Christof Monz |  |
| 2031 |  |  [SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA](https://doi.org/10.18653/v1/2023.emnlp-main.837) |  | 0 |  | Jonathan Tonglet, Manon Reusens, Philipp Borchert, Bart Baesens |  |
| 2032 |  |  [Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations](https://doi.org/10.18653/v1/2023.emnlp-main.838) |  | 0 |  | Jihyoung Jang, Minseong Boo, Hyounghun Kim |  |
| 2033 |  |  [DueT: Image-Text Contrastive Transfer Learning with Dual-adapter Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.839) |  | 0 |  | Taku Hasegawa, Kyosuke Nishida, Koki Maeda, Kuniko Saito |  |
| 2034 |  |  [Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation](https://doi.org/10.18653/v1/2023.emnlp-main.840) |  | 0 |  | Yeongseo Jung, Eunseo Jung, Lei Chen |  |
| 2035 |  |  [CLAIR: Evaluating Image Captions with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.841) |  | 0 |  | David M. Chan, Suzanne Petryk, Joseph Gonzalez, Trevor Darrell, John F. Canny |  |
| 2036 |  |  [MoPe: Model Perturbation based Privacy Attacks on Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.842) |  | 0 |  | Marvin Li, Jason Wang, Jeffrey G. Wang, Seth Neel |  |
| 2037 |  |  [q2d: Turning Questions into Dialogs to Teach Models How to Search](https://doi.org/10.18653/v1/2023.emnlp-main.843) |  | 0 |  | Yonatan Bitton, Shlomi CohenGanor, Ido Hakimi, Yoad Lewenberg, Roee Aharoni, Enav Weinreb |  |
| 2038 |  |  [Aligning Large Language Models through Synthetic Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.844) |  | 0 |  | Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, Minjoon Seo |  |
| 2039 |  |  [You Told Me That Joke Twice: A Systematic Investigation of Transferability and Robustness of Humor Detection Models](https://doi.org/10.18653/v1/2023.emnlp-main.845) |  | 0 |  | Alexander Baranov, Vladimir Kniazhevsky, Pavel Braslavski |  |
| 2040 |  |  [Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction](https://doi.org/10.18653/v1/2023.emnlp-main.846) |  | 0 |  | Chong Zhang, Ya Guo, Yi Tu, Huan Chen, Jinyang Tang, Huijia Zhu, Qi Zhang, Tao Gui |  |
| 2041 |  |  [Empower Nested Boolean Logic via Self-Supervised Curriculum Learning](https://doi.org/10.18653/v1/2023.emnlp-main.847) |  | 0 |  | Hongqiu Wu, Linfeng Liu, Hai Zhao, Min Zhang |  |
| 2042 |  |  [The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.848) |  | 0 |  | Pranav Venkit, Mukund Srinath, Sanjana Gautam, Saranya Venkatraman, Vipul Gupta, Rebecca J. Passonneau, Shomir Wilson |  |
| 2043 |  |  [Poisoning Retrieval Corpora by Injecting Adversarial Passages](https://doi.org/10.18653/v1/2023.emnlp-main.849) |  | 0 |  | Zexuan Zhong, Ziqing Huang, Alexander Wettig, Danqi Chen |  |
| 2044 |  |  [DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules](https://doi.org/10.18653/v1/2023.emnlp-main.850) |  | 0 |  | Yanchen Liu, William Held, Diyi Yang |  |
| 2045 |  |  [Clustering Pseudo Language Family in Multilingual Translation Models with Fisher Information Matrix](https://doi.org/10.18653/v1/2023.emnlp-main.851) |  | 0 |  | Xinyu Ma, Xuebo Liu, Min Zhang |  |
| 2046 |  |  [Unifying Discrete and Continuous Representations for Unsupervised Paraphrase Generation](https://doi.org/10.18653/v1/2023.emnlp-main.852) |  | 0 |  | Mingfeng Xue, Dayiheng Liu, Wenqiang Lei, Jie Fu, Jian Lan, Mei Li, Baosong Yang, Jun Xie, Yidan Zhang, Dezhong Peng, Jiancheng Lv |  |
| 2047 |  |  [The Benefits of Label-Description Training for Zero-Shot Text Classification](https://doi.org/10.18653/v1/2023.emnlp-main.853) |  | 0 |  | Lingyu Gao, Debanjan Ghosh, Kevin Gimpel |  |
| 2048 |  |  [Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer](https://doi.org/10.18653/v1/2023.emnlp-main.854) |  | 0 |  | Elizabeth Salesky, Neha Verma, Philipp Koehn, Matt Post |  |
| 2049 |  |  [Finding Authentic Counterhate Arguments: A Case Study with Public Figures](https://doi.org/10.18653/v1/2023.emnlp-main.855) |  | 0 |  | Abdullah Albanyan, Ahmed Hassan, Eduardo Blanco |  |
| 2050 |  |  [Can We Edit Multimodal Large Language Models?](https://doi.org/10.18653/v1/2023.emnlp-main.856) |  | 0 |  | Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, Ningyu Zhang |  |
| 2051 |  |  [Exploring Discourse Structure in Document-level Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.857) |  | 0 |  | Xinyu Hu, Xiaojun Wan |  |
| 2052 |  |  [ClusterLLM: Large Language Models as a Guide for Text Clustering](https://doi.org/10.18653/v1/2023.emnlp-main.858) |  | 0 |  | Yuwei Zhang, Zihan Wang, Jingbo Shang |  |
| 2053 |  |  [CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code](https://doi.org/10.18653/v1/2023.emnlp-main.859) |  | 0 |  | Shuyan Zhou, Uri Alon, Sumit Agarwal, Graham Neubig |  |
| 2054 |  |  [Learn and Consolidate: Continual Adaptation for Zero-Shot and Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.860) |  | 0 |  | Kaiyu Huang, Peng Li, Junpeng Liu, Maosong Sun, Yang Liu |  |
| 2055 |  |  [e-THERAPIST: I suggest you to cultivate a mindset of positivity and nurture uplifting thoughts](https://doi.org/10.18653/v1/2023.emnlp-main.861) |  | 0 |  | Kshitij Mishra, Priyanshu Priya, Manisha Burja, Asif Ekbal |  |
| 2056 |  |  [AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages](https://doi.org/10.18653/v1/2023.emnlp-main.862) |  | 0 |  | Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Abinew Ali Ayele, Nedjma Ousidhoum, David Ifeoluwa Adelani, Seid Muhie Yimam, Ibrahim Said Ahmad, Meriem Beloucif, Saif M. Mohammad, Sebastian Ruder, Oumaima Hourrane, Alípio Jorge, Pavel Brazdil, Felermino Dário Mário António Ali, Davis David, Salomey Osei, Bello Shehu Bello, Falalu Ibrahim Lawan, Tajuddeen Gwadabe, Samuel Rutunda, Tadesse Destaw Belay, Wendimu Baye Messelle, Hailu Beshada Balcha, Sisay Adugna Chala, Hagos Tesfahun Gebremichael, Bernard Opoku, Stephen Arthur |  |
| 2057 |  |  [Quantifying Character Similarity with Vision Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.863) |  | 0 |  | Xinmei Yang, Abhishek Arora, ShaoYu Jheng, Melissa Dell |  |
| 2058 |  |  [Syllogistic Reasoning for Legal Judgment Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.864) |  | 0 |  | Wentao Deng, Jiahuan Pei, Keyi Kong, Zhe Chen, Furu Wei, Yujun Li, Zhaochun Ren, Zhumin Chen, Pengjie Ren |  |
| 2059 |  |  [Improving Transformer-based Program Repair Model through False Behavior Diagnosis](https://doi.org/10.18653/v1/2023.emnlp-main.865) |  | 0 |  | Youngkyoung Kim, Misoo Kim, Eunseok Lee |  |
| 2060 |  |  [SUT: Active Defects Probing for Transcompiler Models](https://doi.org/10.18653/v1/2023.emnlp-main.866) |  | 0 |  | Mengnan Qi, Yufan Huang, Maoquan Wang, Yongqiang Yao, Zihan Liu, Bin Gu, Colin B. Clement, Neel Sundaresan |  |
| 2061 |  |  [KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection](https://doi.org/10.18653/v1/2023.emnlp-main.867) |  | 0 |  | Sehyun Choi, Tianqing Fang, Zhaowei Wang, Yangqiu Song |  |
| 2062 |  |  [CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL](https://doi.org/10.18653/v1/2023.emnlp-main.868) |  | 0 |  | Mayank Kothyari, Dhruva Dhingra, Sunita Sarawagi, Soumen Chakrabarti |  |
| 2063 |  |  [This Reads Like That: Deep Learning for Interpretable Natural Language Processing](https://doi.org/10.18653/v1/2023.emnlp-main.869) |  | 0 |  | Claudio Fanconi, Moritz Vandenhirtz, Severin Husmann, Julia E. Vogt |  |
| 2064 |  |  [Incorporating Structured Representations into Pretrained Vision & Language Models Using Scene Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.870) |  | 0 |  | Roei Herzig, Alon Mendelson, Leonid Karlinsky, Assaf Arbelle, Rogério Feris, Trevor Darrell, Amir Globerson |  |
| 2065 |  |  [TLM: Token-Level Masking for Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.871) |  | 0 |  | Yangjun Wu, Kebin Fang, Dongxiang Zhang, Han Wang, Hao Zhang, Gang Chen |  |
| 2066 |  |  [Addressing NER Annotation Noises with Uncertainty-Guided Tree-Structured CRFs](https://doi.org/10.18653/v1/2023.emnlp-main.872) |  | 0 |  | Jian Liu, Weichang Liu, Yufeng Chen, Jinan Xu, Zhe Zhao |  |
| 2067 |  |  [Hi Guys or Hi Folks? Benchmarking Gender-Neutral Machine Translation with the GeNTE Corpus](https://doi.org/10.18653/v1/2023.emnlp-main.873) |  | 0 |  | Andrea Piergentili, Beatrice Savoldi, Dennis Fucci, Matteo Negri, Luisa Bentivogli |  |
| 2068 |  |  [Multilingual Holistic Bias: Extending Descriptors and Patterns to Unveil Demographic Biases in Languages at Scale](https://doi.org/10.18653/v1/2023.emnlp-main.874) |  | 0 |  | Marta R. Costajussà, Pierre Andrews, Eric Michael Smith, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Daniel Licht, Carleigh Wood |  |
| 2069 |  |  [GlobalBench: A Benchmark for Global Progress in Natural Language Processing](https://doi.org/10.18653/v1/2023.emnlp-main.875) |  | 0 |  | Yueqi Song, Simran Khanuja, Pengfei Liu, Fahim Faisal, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig |  |
| 2070 |  |  [DetGPT: Detect What You Need via Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.876) |  | 0 |  | Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, Lingpeng Kong, Tong Zhang |  |
| 2071 |  |  [Language Models with Rationality](https://doi.org/10.18653/v1/2023.emnlp-main.877) |  | 0 |  | Nora Kassner, Oyvind Tafjord, Ashish Sabharwal, Kyle Richardson, Hinrich Schütze, Peter Clark |  |
| 2072 |  |  [Self-Improvement of Non-autoregressive Model via Sequence-Level Distillation](https://doi.org/10.18653/v1/2023.emnlp-main.878) |  | 0 |  | Yusheng Liao, Shuyang Jiang, Yiqi Li, Yu Wang, Yanfeng Wang |  |
| 2073 |  |  [Mitigating Temporal Misalignment by Discarding Outdated Facts](https://doi.org/10.18653/v1/2023.emnlp-main.879) |  | 0 |  | Michael J. Q. Zhang, Eunsol Choi |  |
| 2074 |  |  [Open-world Semi-supervised Generalized Relation Discovery Aligned in a Real-world Setting](https://doi.org/10.18653/v1/2023.emnlp-main.880) |  | 0 |  | William Hogan, Jiacheng Li, Jingbo Shang |  |
| 2075 |  |  [IEKG: A Commonsense Knowledge Graph for Idiomatic Expressions](https://doi.org/10.18653/v1/2023.emnlp-main.881) |  | 0 |  | Ziheng Zeng, Kellen Tan Cheng, Srihari Venkat Nanniyur, Jianing Zhou, Suma Bhat |  |
| 2076 |  |  [Bias Neutralization in Non-Parallel Texts: A Cyclic Approach with Auxiliary Guidance](https://doi.org/10.18653/v1/2023.emnlp-main.882) |  | 0 |  | Karthic Madanagopal, James Caverlee |  |
| 2077 |  |  [Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation](https://doi.org/10.18653/v1/2023.emnlp-main.883) |  | 0 |  | Jason Samuel Lucas, Adaku Uchendu, Michiharu Yamashita, Jooyoung Lee, Shaurya Rohatgi, Dongwon Lee |  |
| 2078 |  |  [SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts](https://doi.org/10.18653/v1/2023.emnlp-main.884) |  | 0 |  | JoonYoung Choi, Junho Kim, JunHyung Park, WingLam Mok, SangKeun Lee |  |
| 2079 |  |  [BRAINTEASER: Lateral Thinking Puzzles for Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.885) |  | 0 |  | Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati |  |
| 2080 |  |  [When are Lemons Purple? The Concept Association Bias of Vision-Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.886) |  | 0 |  | Yingtian Tang, Yutaro Yamada, Yoyo Zhang, Ilker Yildirim |  |
| 2081 |  |  [What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability](https://doi.org/10.18653/v1/2023.emnlp-main.887) |  | 0 |  | Mario Giulianelli, Joris Baan, Wilker Aziz, Raquel Fernández, Barbara Plank |  |
| 2082 |  |  [Text Representation Distillation via Information Bottleneck Principle](https://doi.org/10.18653/v1/2023.emnlp-main.888) |  | 0 |  | Yanzhao Zhang, Dingkun Long, Zehan Li, Pengjun Xie |  |
| 2083 |  |  [Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation](https://doi.org/10.18653/v1/2023.emnlp-main.889) |  | 0 |  | Zhenwen Liang, Wenhao Yu, Tanmay Rajpurohit, Peter Clark, Xiangliang Zhang, Ashwin Kalyan |  |
| 2084 |  |  [FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions](https://doi.org/10.18653/v1/2023.emnlp-main.890) |  | 0 |  | Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim, Yejin Choi, Maarten Sap |  |
| 2085 |  |  [Exploring the Boundaries of GPT-4 in Radiology](https://doi.org/10.18653/v1/2023.emnlp-main.891) |  | 0 |  | Qianchu Liu, Stephanie L. Hyland, Shruthi Bannur, Kenza Bouzid, Daniel C. Castro, Maria Wetscherek, Robert Tinn, Harshita Sharma, Fernando PérezGarcía, Anton Schwaighofer, Pranav Rajpurkar, Sameer Tajdin Khanna, Hoifung Poon, Naoto Usuyama, Anja Thieme, Aditya V. Nori, Matthew P. Lungren, Ozan Oktay, Javier AlvarezValle |  |
| 2086 |  |  [A Frustratingly Easy Post-Training Quantization Scheme for LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.892) |  | 0 |  | Yongkweon Jeon, Chungman Lee, Kyungphil Park, HoYoung Kim |  |
| 2087 |  |  [A Comprehensive Evaluation of Biomedical Entity Linking Models](https://doi.org/10.18653/v1/2023.emnlp-main.893) |  | 0 |  | David Kartchner, Jennifer Deng, Shubham Lohiya, Tejasri Kopparthi, Prasanth Bathala, Daniel DomingoFernández, Cassie S. Mitchell |  |
| 2088 |  |  [Exploring Jiu-Jitsu Argumentation for Writing Peer Review Rebuttals](https://doi.org/10.18653/v1/2023.emnlp-main.894) |  | 0 |  | Sukannya Purkayastha, Anne Lauscher, Iryna Gurevych |  |
| 2089 |  |  [LIMIT: Language Identification, Misidentification, and Translation using Hierarchical Models in 350+ Languages](https://doi.org/10.18653/v1/2023.emnlp-main.895) |  | 0 |  | Milind Agarwal, Md Mahfuz Ibn Alam, Antonios Anastasopoulos |  |
| 2090 |  |  [FreeAL: Towards Human-Free Active Learning in the Era of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.896) |  | 0 |  | Ruixuan Xiao, Yiwen Dong, Junbo Zhao, Runze Wu, Minmin Lin, Gang Chen, Haobo Wang |  |
| 2091 |  |  [API-Assisted Code Generation for Question Answering on Varied Table Structures](https://doi.org/10.18653/v1/2023.emnlp-main.897) |  | 0 |  | Yihan Cao, Shuyi Chen, Ryan Liu, Zhiruo Wang, Daniel Fried |  |
| 2092 |  |  [Data Factors for Better Compositional Generalization](https://doi.org/10.18653/v1/2023.emnlp-main.898) |  | 0 |  | Xiang Zhou, Yichen Jiang, Mohit Bansal |  |
| 2093 |  |  [ChatEdit: Towards Multi-turn Interactive Facial Image Editing via Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.899) |  | 0 |  | Xing Cui, Zekun Li, Pei Li, Yibo Hu, Hailin Shi, Chunshui Cao, Zhaofeng He |  |
| 2094 |  |  [Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations](https://doi.org/10.18653/v1/2023.emnlp-main.900) |  | 0 |  | James Y. Huang, Wenlin Yao, Kaiqiang Song, Hongming Zhang, Muhao Chen, Dong Yu |  |
| 2095 |  |  [Outlier Dimensions Encode Task Specific Knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.901) |  | 0 |  | William Rudman, Catherine Chen, Carsten Eickhoff |  |
| 2096 |  |  [Hi-ArG: Exploring the Integration of Hierarchical Argumentation Graphs in Language Pretraining](https://doi.org/10.18653/v1/2023.emnlp-main.902) |  | 0 |  | Jingcong Liang, Rong Ye, Meng Han, Qi Zhang, Ruofei Lai, Xinyu Zhang, Zhao Cao, Xuanjing Huang, Zhongyu Wei |  |
| 2097 |  |  [Biomedical Named Entity Recognition via Dictionary-based Synonym Generalization](https://doi.org/10.18653/v1/2023.emnlp-main.903) |  | 0 |  | Zihao Fu, Yixuan Su, Zaiqiao Meng, Nigel Collier |  |
| 2098 |  |  [GNAT: A General Narrative Alignment Tool](https://doi.org/10.18653/v1/2023.emnlp-main.904) |  | 0 |  | Tanzir Pial, Steven Skiena |  |
| 2099 |  |  [Self-Ensemble of N-best Generation Hypotheses by Lexically Constrained Decoding](https://doi.org/10.18653/v1/2023.emnlp-main.905) |  | 0 |  | Ryota Miyano, Tomoyuki Kajiwara, Yuki Arase |  |
| 2100 |  |  [UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.906) |  | 0 |  | Ahmed Masry, Parsa Kavehzadeh, Do Xuan Long, Enamul Hoque, Shafiq Joty |  |
| 2101 |  |  [Merging Experts into One: Improving Computational Efficiency of Mixture of Experts](https://doi.org/10.18653/v1/2023.emnlp-main.907) |  | 0 |  | Shwai He, RunZe Fan, Liang Ding, Li Shen, Tianyi Zhou, Dacheng Tao |  |
| 2102 |  |  [Distance-Based Propagation for Efficient Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.908) |  | 0 |  | Harry Shomer, Yao Ma, Juanhui Li, Bo Wu, Charu C. Aggarwal, Jiliang Tang |  |
| 2103 |  |  [What to Read in a Contract? Party-Specific Summarization of Legal Obligations, Entitlements, and Prohibitions](https://doi.org/10.18653/v1/2023.emnlp-main.909) |  | 0 |  | Abhilasha Sancheti, Aparna Garimella, Balaji Vasan Srinivasan, Rachel Rudinger |  |
| 2104 |  |  [Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization](https://doi.org/10.18653/v1/2023.emnlp-main.910) |  | 0 |  | Janghwan Lee, Minsoo Kim, Seungcheol Baek, Seok Joong Hwang, Wonyong Sung, Jungwook Choi |  |
| 2105 |  |  [CP-BCS: Binary Code Summarization Guided by Control Flow Graph and Pseudo Code](https://doi.org/10.18653/v1/2023.emnlp-main.911) |  | 0 |  | Tong Ye, Lingfei Wu, Tengfei Ma, Xuhong Zhang, Yangkai Du, Peiyu Liu, Shouling Ji, Wenhai Wang |  |
| 2106 |  |  [Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism](https://doi.org/10.18653/v1/2023.emnlp-main.912) |  | 0 |  | Mengyu Ye, Tatsuki Kuribayashi, Jun Suzuki, Goro Kobayashi, Hiroaki Funayama |  |
| 2107 |  |  [Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.913) |  | 0 |  | Caoyun Fan, Jidong Tian, Yitian Li, Wenqing Chen, Hao He, Yaohui Jin |  |
| 2108 |  |  [Large Language Models are Complex Table Parsers](https://doi.org/10.18653/v1/2023.emnlp-main.914) |  | 0 |  | Bowen Zhao, Changkai Ji, Yuejie Zhang, Wen He, Yingwen Wang, Qing Wang, Rui Feng, Xiaobo Zhang |  |
| 2109 |  |  [R2H: Building Multimodal Navigation Helpers that Respond to Help Requests](https://doi.org/10.18653/v1/2023.emnlp-main.915) |  | 0 |  | Yue Fan, Jing Gu, Kaizhi Zheng, Xin Wang |  |
| 2110 |  |  [Speech-enriched Memory for Inference-time Adaptation of ASR Models to Word Dictionaries](https://doi.org/10.18653/v1/2023.emnlp-main.916) |  | 0 |  | Ashish R. Mittal, Sunita Sarawagi, Preethi Jyothi, George Saon, Gakuto Kurata |  |
| 2111 |  |  [Generative Table Pre-training Empowers Models for Tabular Prediction](https://doi.org/10.18653/v1/2023.emnlp-main.917) |  | 0 |  | Tianping Zhang, Shaowen Wang, Shuicheng Yan, Li Jian, Qian Liu |  |
| 2112 |  |  [Learning to Describe for Predicting Zero-shot Drug-Drug Interactions](https://doi.org/10.18653/v1/2023.emnlp-main.918) |  | 0 |  | Fangqi Zhu, Yongqi Zhang, Lei Chen, Bing Qin, Ruifeng Xu |  |
| 2113 |  |  [A Simple Baseline for Knowledge-Based Visual Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.919) |  | 0 |  | Alexandros Xenos, Themos Stafylakis, Ioannis Patras, Georgios Tzimiropoulos |  |
| 2114 |  |  [Unveiling the Essence of Poetry: Introducing a Comprehensive Dataset and Benchmark for Poem Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.920) |  | 0 |  | Ridwan Mahbub, Ifrad Khan, Samiha Anuva, Md Shahriar, Md. Tahmid Rahman Laskar, Sabbir Ahmed |  |
| 2115 |  |  [Privacy Implications of Retrieval-Based Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.921) |  | 0 |  | Yangsibo Huang, Samyak Gupta, Zexuan Zhong, Kai Li, Danqi Chen |  |
| 2116 |  |  [IMTLab: An Open-Source Platform for Building, Evaluating, and Diagnosing Interactive Machine Translation Systems](https://doi.org/10.18653/v1/2023.emnlp-main.922) |  | 0 |  | Xu Huang, Zhirui Zhang, Ruize Gao, Yichao Du, Lemao Liu, Guoping Huang, Shuming Shi, Jiajun Chen, Shujian Huang |  |
| 2117 |  |  [Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents](https://doi.org/10.18653/v1/2023.emnlp-main.923) |  | 0 |  | Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren |  |
| 2118 |  |  [DiNeR: A Large Realistic Dataset for Evaluating Compositional Generalization](https://doi.org/10.18653/v1/2023.emnlp-main.924) |  | 0 |  | Chengang Hu, Xiao Liu, Yansong Feng |  |
| 2119 |  |  [Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?](https://doi.org/10.18653/v1/2023.emnlp-main.925) |  | 0 |  | Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, MingWei Chang |  |
| 2120 |  |  [EDeR: Towards Understanding Dependency Relations Between Events](https://doi.org/10.18653/v1/2023.emnlp-main.926) |  | 0 |  | Ruiqi Li, Patrik Haslum, Leyang Cui |  |
| 2121 |  |  [It Ain't Over: A Multi-aspect Diverse Math Word Problem Dataset](https://doi.org/10.18653/v1/2023.emnlp-main.927) |  | 0 |  | Jiwoo Kim, Youngbin Kim, Ilwoong Baek, JinYeong Bak, Jongwuk Lee |  |
| 2122 |  |  [Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness](https://doi.org/10.18653/v1/2023.emnlp-main.928) |  | 0 |  | Bevan Koopman, Guido Zuccon |  |
| 2123 |  |  [kNN-LM Does Not Improve Open-ended Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.929) |  | 0 |  | Shufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun Manjunatha, Mohit Iyyer |  |
| 2124 |  |  [Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model](https://doi.org/10.18653/v1/2023.emnlp-main.930) |  | 0 |  | Zeyu Liu, Tim Dettmers, Xi Lin, Veselin Stoyanov, Xian Li |  |
| 2125 |  |  [Exploring the Impact of Model Scaling on Parameter-Efficient Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.931) |  | 0 |  | Yusheng Su, ChiMin Chan, Jiali Cheng, Yujia Qin, Yankai Lin, Shengding Hu, Zonghan Yang, Ning Ding, Xingzhi Sun, Guotong Xie, Zhiyuan Liu, Maosong Sun |  |
| 2126 |  |  [STAIR: Learning Sparse Text and Image Representation in Grounded Tokens](https://doi.org/10.18653/v1/2023.emnlp-main.932) |  | 0 |  | Chen Chen, Bowen Zhang, Liangliang Cao, Jiguang Shen, Tom Gunter, Albin Madappally Jose, Alexander Toshev, Yantao Zheng, Ruoming Pang, Yinfei Yang |  |
| 2127 |  |  [Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting](https://doi.org/10.18653/v1/2023.emnlp-main.933) |  | 0 |  | Emmy Liu, Aditi Chaudhary, Graham Neubig |  |
| 2128 |  |  [CoRec: An Easy Approach for Coordination Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.934) |  | 0 |  | Qing Wang, Haojie Jia, Wenfei Song, Qi Li |  |
| 2129 |  |  [A linear time approximation of Wasserstein distance with word embedding selection](https://doi.org/10.18653/v1/2023.emnlp-main.935) |  | 0 |  | Sho Otao, Makoto Yamada |  |
| 2130 |  |  [Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication](https://doi.org/10.18653/v1/2023.emnlp-main.936) |  | 0 |  | Zhangyue Yin, Qiushi Sun, Cheng Chang, Qipeng Guo, Junqi Dai, Xuanjing Huang, Xipeng Qiu |  |
| 2131 |  |  [Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction](https://doi.org/10.18653/v1/2023.emnlp-main.937) |  | 0 |  | CamVan Thi Nguyen, AnhTuan Mai, TheSon Le, HaiDang Kieu, DucTrong Le |  |
| 2132 |  |  [Connecting degree and polarity: An artificial language learning study](https://doi.org/10.18653/v1/2023.emnlp-main.938) |  | 0 |  | Lisa Bylinina, Alexey Tikhonov, Ekaterina Garmash |  |
| 2133 |  |  [Prompting with Pseudo-Code Instructions](https://doi.org/10.18653/v1/2023.emnlp-main.939) |  | 0 |  | Mayank Mishra, Prince Kumar, Riyaz A. Bhat, Rudra Murthy V, Danish Contractor, Srikanth Tamilselvam |  |
| 2134 |  |  [CRAB: Assessing the Strength of Causal Relationships Between Real-world Events](https://doi.org/10.18653/v1/2023.emnlp-main.940) |  | 0 |  | Angelika Romanou, Syrielle Montariol, Debjit Paul, Léo Laugier, Karl Aberer, Antoine Bosselut |  |
| 2135 |  |  [NORMSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly](https://doi.org/10.18653/v1/2023.emnlp-main.941) |  | 0 |  | Yi Fung, Tuhin Chakrabarty, Hao Guo, Owen Rambow, Smaranda Muresan, Heng Ji |  |
| 2136 |  |  [A State-Vector Framework for Dataset Effects](https://doi.org/10.18653/v1/2023.emnlp-main.942) |  | 0 |  | Esmat Sahak, Zining Zhu, Frank Rudzicz |  |
| 2137 |  |  [Challenges in Context-Aware Neural Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.943) |  | 0 |  | Linghao Jin, Jacqueline He, Jonathan May, Xuezhe Ma |  |
| 2138 |  |  [Task-Adaptive Tokenization: Enhancing Long-Form Text Generation Efficacy in Mental Health and Beyond](https://doi.org/10.18653/v1/2023.emnlp-main.944) |  | 0 |  | Siyang Liu, Naihao Deng, Sahand Sabour, Yilin Jia, Minlie Huang, Rada Mihalcea |  |
| 2139 |  |  [FACTIFY3M: A benchmark for multimodal fact verification with explainability through 5W Question-Answering](https://doi.org/10.18653/v1/2023.emnlp-main.945) |  | 0 |  | Megha Chakraborty, Khushbu Pahwa, Anku Rani, Shreyas Chatterjee, Dwip Dalal, Harshit Dave, Ritvik G, Preethi Gurumurthy, Adarsh Mahor, Samahriti Mukherjee, Aditya Pakala, Ishan Paul, Janvita Reddy, Arghya Sarkar, Kinjal Sensharma, Aman Chadha, Amit P. Sheth, Amitava Das |  |
| 2140 |  |  [Building Multi-domain Dialog State Trackers from Single-domain Dialogs](https://doi.org/10.18653/v1/2023.emnlp-main.946) |  | 0 |  | Qi Zhu, Zheng Zhang, Xiaoyan Zhu, Minlie Huang |  |
| 2141 |  |  [Specialist or Generalist? Instruction Tuning for Specific NLP Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.947) |  | 0 |  | Chufan Shi, Yixuan Su, Cheng Yang, Yujiu Yang, Deng Cai |  |
| 2142 |  |  [Making Large Language Models Better Data Creators](https://doi.org/10.18653/v1/2023.emnlp-main.948) |  | 0 |  | DongHo Lee, Jay Pujara, Mohit Sewak, Ryen White, Sujay Kumar Jauhar |  |
| 2143 |  |  [Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation](https://doi.org/10.18653/v1/2023.emnlp-main.949) |  | 0 |  | Xiaohua Wang, Yuliang Yan, Longtao Huang, Xiaoqing Zheng, Xuanjing Huang |  |
| 2144 |  |  [Guideline Learning for In-Context Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.950) |  | 0 |  | Chaoxu Pang, Yixuan Cao, Qiang Ding, Ping Luo |  |
| 2145 |  |  [Open Information Extraction via Chunks](https://doi.org/10.18653/v1/2023.emnlp-main.951) |  | 0 |  | Kuicai Dong, Aixin Sun, JungJae Kim, Xiaoli Li |  |
| 2146 |  |  [Rethinking Word-Level Auto-Completion in Computer-Aided Translation](https://doi.org/10.18653/v1/2023.emnlp-main.952) |  | 0 |  | Xingyu Chen, Lemao Liu, Guoping Huang, Zhirui Zhang, Mingming Yang, Shuming Shi, Rui Wang |  |
| 2147 |  |  [Automatic Transcription of Handwritten Old Occitan Language](https://doi.org/10.18653/v1/2023.emnlp-main.953) |  | 0 |  | Esteban Garces Arias, Vallari Pai, Matthias Schöffel, Christian Heumann, Matthias Aßenmacher |  |
| 2148 |  |  [CorefPrompt: Prompt-based Event Coreference Resolution by Measuring Event Type and Argument Compatibilities](https://doi.org/10.18653/v1/2023.emnlp-main.954) |  | 0 |  | Sheng Xu, Peifeng Li, Qiaoming Zhu |  |
| 2149 |  |  [Anaphor Assisted Document-Level Relation Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.955) |  | 0 |  | Chonggang Lu, Richong Zhang, Kai Sun, Jaein Kim, Cunwang Zhang, Yongyi Mao |  |
| 2150 |  |  [FinEntity: Entity-level Sentiment Classification for Financial Texts](https://doi.org/10.18653/v1/2023.emnlp-main.956) |  | 0 |  | Yixuan Tang, Yi Yang, Allen Huang, Andy Tam, Justin Z. Tang |  |
| 2151 |  |  [All Things Considered: Detecting Partisan Events from News Media with Cross-Article Comparison](https://doi.org/10.18653/v1/2023.emnlp-main.957) |  | 0 |  | Yujian Liu, Xinliang Frederick Zhang, Kaijian Zou, Ruihong Huang, Nicholas Beauchamp, Lu Wang |  |
| 2152 |  |  [Rationale-Enhanced Language Models are Better Continual Relation Learners](https://doi.org/10.18653/v1/2023.emnlp-main.958) |  | 0 |  | Weimin Xiong, Yifan Song, Peiyi Wang, Sujian Li |  |
| 2153 |  |  [BanglaAbuseMeme: A Dataset for Bengali Abusive Meme Classification](https://doi.org/10.18653/v1/2023.emnlp-main.959) |  | 0 |  | Mithun Das, Animesh Mukherjee |  |
| 2154 |  |  [ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts](https://doi.org/10.18653/v1/2023.emnlp-main.960) |  | 0 |  | Lena S. Bolliger, David R. Reich, Patrick Haller, Deborah N. Jakobi, Paul Prasse, Lena A. Jäger |  |
| 2155 |  |  [From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.961) |  | 0 |  | Dongjun Kang, Joonsuk Park, Yohan Jo, JinYeong Bak |  |
| 2156 |  |  [Analyzing Film Adaptation through Narrative Alignment](https://doi.org/10.18653/v1/2023.emnlp-main.962) |  | 0 |  | Tanzir Pial, Shahreen Salim Aunti, Charuta Pethe, Allen Kim, Steven Skiena |  |
| 2157 |  |  [Inverse Scaling Can Become U-Shaped](https://doi.org/10.18653/v1/2023.emnlp-main.963) |  | 0 |  | Jason Wei, Najoung Kim, Yi Tay, Quoc V. Le |  |
| 2158 |  |  [Nearest Neighbor Machine Translation is Meta-Optimizer on Output Projection Layer](https://doi.org/10.18653/v1/2023.emnlp-main.964) |  | 0 |  | Ruize Gao, Zhirui Zhang, Yichao Du, Lemao Liu, Rui Wang |  |
| 2159 |  |  [Variance Matters: Detecting Semantic Differences without Corpus/Word Alignment](https://doi.org/10.18653/v1/2023.emnlp-main.965) |  | 0 |  | Ryo Nagata, Hiroya Takamura, Naoki Otani, Yoshifumi Kawasaki |  |
| 2160 |  |  [MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter](https://doi.org/10.18653/v1/2023.emnlp-main.966) |  | 0 |  | Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, TatSeng Chua |  |
| 2161 |  |  [A Training-Free Debiasing Framework with Counterfactual Reasoning for Conversational Emotion Detection](https://doi.org/10.18653/v1/2023.emnlp-main.967) |  | 0 |  | Geng Tu, Ran Jing, Bin Liang, Min Yang, KamFai Wong, Ruifeng Xu |  |
| 2162 |  |  [Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations](https://doi.org/10.18653/v1/2023.emnlp-main.968) |  | 0 |  | WeiLin Chen, ChengKuang Wu, YunNung Chen, HsinHsi Chen |  |
| 2163 |  |  [Learning Knowledge-Enhanced Contextual Language Representations for Domain Natural Language Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.969) |  | 0 |  | Taolin Zhang, Ruyao Xu, Chengyu Wang, Zhongjie Duan, Cen Chen, Minghui Qiu, Dawei Cheng, Xiaofeng He, Weining Qian |  |
| 2164 |  |  [ScdNER: Span-Based Consistency-Aware Document-Level Named Entity Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.970) |  | 0 |  | Ying Wei, Qi Li |  |
| 2165 |  |  [MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions](https://doi.org/10.18653/v1/2023.emnlp-main.971) |  | 0 |  | Zexuan Zhong, Zhengxuan Wu, Christopher D. Manning, Christopher Potts, Danqi Chen |  |
| 2166 |  |  [Stance Detection on Social Media with Background Knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.972) |  | 0 |  | Ang Li, Bin Liang, Jingqian Zhao, Bowen Zhang, Min Yang, Ruifeng Xu |  |
| 2167 |  |  [Vision-Enhanced Semantic Entity Recognition in Document Images via Visually-Asymmetric Consistency Learning](https://doi.org/10.18653/v1/2023.emnlp-main.973) |  | 0 |  | Hao Wang, Xiahua Chen, Rui Wang, Chenhui Chu |  |
| 2168 |  |  [NormDial: A Comparable Bilingual Synthetic Dialog Dataset for Modeling Social Norm Adherence and Violation](https://doi.org/10.18653/v1/2023.emnlp-main.974) |  | 0 |  | Oliver Li, Mallika Subramanian, Arkadiy Saakyan, Sky CHWang, Smaranda Muresan |  |
| 2169 |  |  [ClimateBERT-NetZero: Detecting and Assessing Net Zero and Reduction Targets](https://doi.org/10.18653/v1/2023.emnlp-main.975) |  | 0 |  | Tobias Schimanski, Julia Anna Bingler, Mathias Kraus, Camilla Hyslop, Markus Leippold |  |
| 2170 |  |  [Leap-of-Thought: Accelerating Transformers via Dynamic Token Routing](https://doi.org/10.18653/v1/2023.emnlp-main.976) |  | 0 |  | Yeachan Kim, Junho Kim, JunHyung Park, Mingyu Lee, SangKeun Lee |  |
| 2171 |  |  [Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning](https://doi.org/10.18653/v1/2023.emnlp-main.977) |  | 0 |  | Swaroop Nath, Pushpak Bhattacharyya, Harshad Khadilkar |  |
| 2172 |  |  [Fair Text Classification with Wasserstein Independence](https://doi.org/10.18653/v1/2023.emnlp-main.978) |  | 0 |  | Thibaud Leteno, Antoine Gourru, Charlotte Laclau, Rémi Emonet, Christophe Gravier |  |
| 2173 |  |  [TacoPrompt: A Collaborative Multi-Task Prompt Learning Method for Self-Supervised Taxonomy Completion](https://doi.org/10.18653/v1/2023.emnlp-main.979) |  | 0 |  | Hongyuan Xu, Ciyi Liu, Yuhang Niu, Yunong Chen, Xiangrui Cai, Yanlong Wen, Xiaojie Yuan |  |
| 2174 |  |  [An Attribution Method for Siamese Encoders](https://doi.org/10.18653/v1/2023.emnlp-main.980) |  | 0 |  | Lucas Möller, Dmitry Nikolaev, Sebastian Padó |  |
| 2175 |  |  [Global Voices, Local Biases: Socio-Cultural Prejudices across Languages](https://doi.org/10.18653/v1/2023.emnlp-main.981) |  | 0 |  | Anjishnu Mukherjee, Chahat Raj, Ziwei Zhu, Antonios Anastasopoulos |  |
| 2176 |  |  [Graph vs. Sequence: An Empirical Study on Knowledge Forms for Knowledge-Grounded Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.982) |  | 0 |  | Yizhe Yang, Heyan Huang, Yuhang Liu, Yang Gao |  |
| 2177 |  |  [Are Compressed Language Models Less Subgroup Robust?](https://doi.org/10.18653/v1/2023.emnlp-main.983) |  | 0 |  | Leonidas Gee, Andrea Zugarini, Novi Quadrianto |  |
| 2178 |  |  [Length Does Matter: Summary Length can Bias Summarization Metrics](https://doi.org/10.18653/v1/2023.emnlp-main.984) |  | 0 |  | Xiaobo Guo, Soroush Vosoughi |  |
| 2179 |  |  [NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.985) |  | 0 |  | Yongchao Chen, Rujul Gandhi, Yang Zhang, Chuchu Fan |  |
| 2180 |  |  [Reformulating NLP tasks to Capture Longitudinal Manifestation of Language Disorders in People with Dementia](https://doi.org/10.18653/v1/2023.emnlp-main.986) |  | 0 |  | Dimitris Gkoumas, Matthew Purver, Maria Liakata |  |
| 2181 |  |  [Elevating Code-mixed Text Handling through Auditory Information of Words](https://doi.org/10.18653/v1/2023.emnlp-main.987) |  | 0 |  | Mamta, Zishan Ahmad, Asif Ekbal |  |
| 2182 |  |  [Predict and Use: Harnessing Predicted Gaze to Improve Multimodal Sarcasm Detection](https://doi.org/10.18653/v1/2023.emnlp-main.988) |  | 0 |  | Divyank Tiwari, Diptesh Kanojia, Anupama Ray, Apoorva Nunna, Pushpak Bhattacharyya |  |
| 2183 |  |  [Fine-grained Medical Vision-Language Representation Learning for Radiology Report Generation](https://doi.org/10.18653/v1/2023.emnlp-main.989) |  | 0 |  | Siyuan Wang, Bo Peng, Yichao Liu, Qi Peng |  |
| 2184 |  |  [ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer](https://doi.org/10.18653/v1/2023.emnlp-main.990) |  | 0 |  | Huadai Liu, Rongjie Huang, Xuan Lin, Wenqiang Xu, Maozong Zheng, Hong Chen, Jinzheng He, Zhou Zhao |  |
| 2185 |  |  [Consistency Analysis of ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.991) |  | 0 |  | Myeongjun Jang, Thomas Lukasiewicz |  |
| 2186 |  |  [Do Differences in Values Influence Disagreements in Online Discussions?](https://doi.org/10.18653/v1/2023.emnlp-main.992) |  | 0 |  | Michiel van der Meer, Piek Vossen, Catholijn M. Jonker, Pradeep K. Murukannaiah |  |
| 2187 |  |  [Automated Fact-Checking in Dialogue: Are Specialized Models Needed?](https://doi.org/10.18653/v1/2023.emnlp-main.993) |  | 0 |  | Eric Chamoun, Marzieh Saeidi, Andreas Vlachos |  |
| 2188 |  |  [A Digital Language Coherence Marker for Monitoring Dementia](https://doi.org/10.18653/v1/2023.emnlp-main.994) |  | 0 |  | Dimitris Gkoumas, Adam Tsakalidis, Maria Liakata |  |
| 2189 |  |  [Detecting Spoilers in Movie Reviews with External Movie Knowledge and User Networks](https://doi.org/10.18653/v1/2023.emnlp-main.995) |  | 0 |  | Heng Wang, Wenqian Zhang, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Qinghua Zheng, Minnan Luo |  |
| 2190 |  |  [Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimoda Emotion Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.996) |  | 0 |  | Dongyuan Li, Yusong Wang, Kotaro Funakoshi, Manabu Okumura |  |
| 2191 |  |  [HyperRank: Hyperbolic Ranking Model for Unsupervised Keyphrase Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.997) |  | 0 |  | Mingyang Song, Huafeng Liu, Liping Jing |  |
| 2192 |  |  [Assessing the influence of attractor-verb distance on grammatical agreement in humans and language models](https://doi.org/10.18653/v1/2023.emnlp-main.998) |  | 0 |  | ChristosNikolaos Zacharopoulos, Théo Desbordes, Mathias SabléMeyer |  |
| 2193 |  |  [Federated Meta-Learning for Emotion and Sentiment Aware Multi-modal Complaint Identification](https://doi.org/10.18653/v1/2023.emnlp-main.999) |  | 0 |  | Apoorva Singh, Siddarth Chandrasekar, Sriparna Saha, Tanmay Sen |  |
| 2194 |  |  [Semantic Similarity Models for Depression Severity Estimation](https://doi.org/10.18653/v1/2023.emnlp-main.1000) |  | 0 |  | Anxo Pérez, Neha Warikoo, Kexin Wang, Javier Parapar, Iryna Gurevych |  |
| 2195 |  |  [Hop, Union, Generate: Explainable Multi-hop Reasoning without Rationale Supervision](https://doi.org/10.18653/v1/2023.emnlp-main.1001) |  | 0 |  | Wenting Zhao, Justin T. Chiu, Claire Cardie, Alexander M. Rush |  |
| 2196 |  |  [To Split or Not to Split: Composing Compounds in Contextual Vector Spaces](https://doi.org/10.18653/v1/2023.emnlp-main.1002) |  | 0 |  | Christopher Jenkins, Filip Miletic, Sabine Schulte im Walde |  |
| 2197 |  |  [ToolWriter: Question Specific Tool Synthesis for Tabular Data](https://doi.org/10.18653/v1/2023.emnlp-main.1003) |  | 0 |  | Carlos Gemmell, Jeff Dalton |  |
| 2198 |  |  [Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations](https://doi.org/10.18653/v1/2023.emnlp-main.1004) |  | 0 |  | Yuan Tian, Zheng Zhang, Zheng Ning, Toby JiaJun Li, Jonathan K. Kummerfeld, Tianyi Zhang |  |
| 2199 |  |  [CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Low Resource With Contrastive Learning](https://doi.org/10.18653/v1/2023.emnlp-main.1005) |  | 0 |  | Xiaoming Liu, Zhaohan Zhang, Yichen Wang, Hang Pu, Yu Lan, Chao Shen |  |
| 2200 |  |  [AnyTOD: A Programmable Task-Oriented Dialog System](https://doi.org/10.18653/v1/2023.emnlp-main.1006) |  | 0 |  | Jeffrey Zhao, Yuan Cao, Raghav Gupta, Harrison Lee, Abhinav Rastogi, Mingqiu Wang, Hagen Soltau, Izhak Shafran, Yonghui Wu |  |
| 2201 |  |  [Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.1007) |  | 0 |  | Chi Seng Cheang, Hou Pong Chan, Derek F. Wong, Xuebo Liu, Zhaocong Li, Yanming Sun, Shudong Liu, Lidia S. Chao |  |
| 2202 |  |  [Zero-Shot Multi-Label Topic Inference with Sentence Encoders and LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.1008) |  | 0 |  | Souvika Sarkar, Dongji Feng, Shubhra Kanti Karmaker Santu |  |
| 2203 |  |  [TaskDiff: A Similarity Metric for Task-Oriented Conversations](https://doi.org/10.18653/v1/2023.emnlp-main.1009) |  | 0 |  | Ankita Bhaumik, Praveen Venkateswaran, Yara Rizk, Vatche Isahagian |  |
| 2204 |  |  [Not all Fake News is Written: A Dataset and Analysis of Misleading Video Headlines](https://doi.org/10.18653/v1/2023.emnlp-main.1010) |  | 0 |  | Yoo Yeon Sung, Jordan L. BoydGraber, Naeemul Hassan |  |
| 2205 |  |  [Learning From Free-Text Human Feedback - Collect New Datasets Or Extend Existing Ones?](https://doi.org/10.18653/v1/2023.emnlp-main.1011) |  | 0 |  | Dominic Petrak, Nafise Sadat Moosavi, Ye Tian, Nikolai Rozanov, Iryna Gurevych |  |
| 2206 |  |  [Euphemistic Abuse - A New Dataset and Classification Experiments for Implicitly Abusive Language](https://doi.org/10.18653/v1/2023.emnlp-main.1012) |  | 0 |  | Michael Wiegand, Jana Kampfmeier, Elisabeth Eder, Josef Ruppenhofer |  |
| 2207 |  |  [Exploring Distributional Shifts in Large Language Models for Code Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.1013) |  | 0 |  | Shushan Arakelyan, Rocktim Jyoti Das, Yi Mao, Xiang Ren |  |
| 2208 |  |  [ATHENA: Mathematical Reasoning with Thought Expansion](https://doi.org/10.18653/v1/2023.emnlp-main.1014) |  | 0 |  | JB. Kim, Hazel Kim, Joonghyuk Hahn, YoSub Han |  |
| 2209 |  |  [A Benchmark for Reasoning with Spatial Prepositions](https://doi.org/10.18653/v1/2023.emnlp-main.1015) |  | 0 |  | Iulia M. Comsa, Srini Narayanan |  |
| 2210 |  |  [TIMELINE: Exhaustive Annotation of Temporal Relations Supporting the Automatic Ordering of Events in News Articles](https://doi.org/10.18653/v1/2023.emnlp-main.1016) |  | 0 |  | Sarah Alsayyahi, Riza BatistaNavarro |  |
| 2211 |  |  [Mitigating Over-Generation for Unsupervised Keyphrase Extraction with Heterogeneous Centrality Detection](https://doi.org/10.18653/v1/2023.emnlp-main.1017) |  | 0 |  | Mingyang Song, Pengyu Xu, Yi Feng, Huafeng Liu, Liping Jing |  |
| 2212 |  |  [Towards Interpretable and Efficient Automatic Reference-Based Summarization Evaluation](https://doi.org/10.18653/v1/2023.emnlp-main.1018) |  | 0 |  | Yixin Liu, Alexander R. Fabbri, Yilun Zhao, Pengfei Liu, Shafiq Joty, ChienSheng Wu, Caiming Xiong, Dragomir Radev |  |
| 2213 |  |  [MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.1019) |  | 0 |  | Steven H. Wang, Antoine Scardigli, Leonard Tang, Wei Chen, Dimitry Levkin, Anya Chen, Spencer Ball, Thomas Woodside, Oliver Zhang, Dan Hendrycks |  |
| 2214 |  |  [PK-ICR: Persona-Knowledge Interactive Multi-Context Retrieval for Grounded Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.1020) |  | 0 |  | Minsik Oh, Joosung Lee, Jiwei Li, Guoyin Wang |  |
| 2215 |  |  [More Than Spoken Words: Nonverbal Message Extraction and Generation](https://doi.org/10.18653/v1/2023.emnlp-main.1021) |  | 0 |  | Dian Yu, Xiaoyang Wang, Wanshun Chen, Nan Du, Longyue Wang, Haitao Mi, Dong Yu |  |
| 2216 |  |  [Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance](https://doi.org/10.18653/v1/2023.emnlp-main.1022) |  | 0 |  | Molly R. Petersen, Lonneke van der Plas |  |
| 2217 |  |  [FAME: Flexible, Scalable Analogy Mappings Engine](https://doi.org/10.18653/v1/2023.emnlp-main.1023) |  | 0 |  | Shahar Jacob, Chen Shani, Dafna Shahaf |  |
| 2218 |  |  [A Self-training Framework for Automated Medical Report Generation](https://doi.org/10.18653/v1/2023.emnlp-main.1024) |  | 0 |  | Siyuan Wang, Zheng Liu, Bo Peng |  |
| 2219 |  |  [A Picture is Worth a Thousand Words: Language Models Plan from Pixels](https://doi.org/10.18653/v1/2023.emnlp-main.1025) |  | 0 |  | Anthony Z. Liu, Lajanugen Logeswaran, Sungryull Sohn, Honglak Lee |  |
| 2220 |  |  [Interpreting and Exploiting Functional Specialization in Multi-Head Attention under Multi-task Learning](https://doi.org/10.18653/v1/2023.emnlp-main.1026) |  | 0 |  | Chong Li, Shaonan Wang, Yunhao Zhang, Jiajun Zhang, Chengqing Zong |  |
| 2221 |  |  [Multilingual Previously Fact-Checked Claim Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.1027) |  | 0 |  | Matús Pikuliak, Ivan Srba, Róbert Móro, Timo Hromadka, Timotej Smolen, Martin Melisek, Ivan Vykopal, Jakub Simko, Juraj Podrouzek, Mária Bieliková |  |
| 2222 |  |  [ALCAP: Alignment-Augmented Music Captioner](https://doi.org/10.18653/v1/2023.emnlp-main.1028) |  | 0 |  | Zihao He, Weituo Hao, Wei Tsung Lu, Changyou Chen, Kristina Lerman, Xuchen Song |  |
| 2223 |  |  [Do Transformers Parse while Predicting the Masked Word?](https://doi.org/10.18653/v1/2023.emnlp-main.1029) |  | 0 |  | Haoyu Zhao, Abhishek Panigrahi, Rong Ge, Sanjeev Arora |  |
| 2224 |  |  [Composable Text Controls in Latent Space with ODEs](https://doi.org/10.18653/v1/2023.emnlp-main.1030) |  | 0 |  | Guangyi Liu, Zeyu Feng, Yuan Gao, Zichao Yang, Xiaodan Liang, Junwei Bao, Xiaodong He, Shuguang Cui, Zhen Li, Zhiting Hu |  |
| 2225 |  |  [P5: Plug-and-Play Persona Prompting for Personalized Response Selection](https://doi.org/10.18653/v1/2023.emnlp-main.1031) |  | 0 |  | Joosung Lee, Minsik Oh, Donghun Lee |  |
| 2226 |  |  [Reader: Model-based language-instructed reinforcement learning](https://doi.org/10.18653/v1/2023.emnlp-main.1032) |  | 0 |  | Nicola Dainese, Pekka Marttinen, Alexander Ilin |  |
| 2227 |  |  [Adapting Offline Speech Translation Models for Streaming with Future-Aware Distillation and Inference](https://doi.org/10.18653/v1/2023.emnlp-main.1033) |  | 0 |  | Biao Fu, Minpeng Liao, Kai Fan, Zhongqiang Huang, Boxing Chen, Yidong Chen, Xiaodong Shi |  |
| 2228 |  |  [Relation-aware Ensemble Learning for Knowledge Graph Embedding](https://doi.org/10.18653/v1/2023.emnlp-main.1034) |  | 0 |  | Ling Yue, Yongqi Zhang, Quanming Yao, Yong Li, Xian Wu, Ziheng Zhang, Zhenxi Lin, Yefeng Zheng |  |
| 2229 |  |  [GenEx: A Commonsense-aware Unified Generative Framework for Explainable Cyberbullying Detection](https://doi.org/10.18653/v1/2023.emnlp-main.1035) |  | 0 |  | Krishanu Maity, Raghav Jain, Prince Jha, Sriparna Saha, Pushpak Bhattacharyya |  |
| 2230 |  |  [Document-Level Machine Translation with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.1036) |  | 0 |  | Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, Zhaopeng Tu |  |
| 2231 |  |  [Multilingual Simplification of Medical Texts](https://doi.org/10.18653/v1/2023.emnlp-main.1037) |  | 0 |  | Sebastian Joseph, Kathryn Kazanas, Keziah Reina, Vishnesh J. Ramanathan, Wei Xu, Byron C. Wallace, Junyi Jessy Li |  |
| 2232 |  |  [When Reviewers Lock Horns: Finding Disagreements in Scientific Peer Reviews](https://doi.org/10.18653/v1/2023.emnlp-main.1038) |  | 0 |  | Sandeep Kumar, Tirthankar Ghosal, Asif Ekbal |  |
| 2233 |  |  [Argue with Me Tersely: Towards Sentence-Level Counter-Argument Generation](https://doi.org/10.18653/v1/2023.emnlp-main.1039) |  | 0 |  | Jiayu Lin, Rong Ye, Meng Han, Qi Zhang, Ruofei Lai, Xinyu Zhang, Zhao Cao, Xuanjing Huang, Zhongyu Wei |  |
| 2234 |  |  [JASMINE: Arabic GPT Models for Few-Shot Learning](https://doi.org/10.18653/v1/2023.emnlp-main.1040) |  | 0 |  | El Moatez Billah Nagoudi, Muhammad AbdulMageed, AbdelRahim A. Elmadany, Alcides Alcoba Inciarte, Md Tawkat Islam Khondaker |  |
| 2235 |  |  [NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports](https://doi.org/10.18653/v1/2023.emnlp-main.1041) |  | 0 |  | Maël Jullien, Marco Valentino, Hannah Frost, Paul O'Regan, Dónal Landers, André Freitas |  |
| 2236 |  |  [Addressing Linguistic Bias through a Contrastive Analysis of Academic Writing in the NLP Domain](https://doi.org/10.18653/v1/2023.emnlp-main.1042) |  | 0 |  | Robert Ridley, Zhen Wu, Jianbing Zhang, Shujian Huang, Xinyu Dai |  |
| 2237 |  |  [RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation](https://doi.org/10.18653/v1/2023.emnlp-main.1043) |  | 0 |  | Yue Zhang, Leyang Cui, Enbo Zhao, Wei Bi, Shuming Shi |  |
| 2238 |  |  [Detecting Propaganda Techniques in Code-Switched Social Media Text](https://doi.org/10.18653/v1/2023.emnlp-main.1044) |  | 0 |  | Muhammad Umar Salman, Asif Hanif, Shady Shehata, Preslav Nakov |  |
| 2239 |  |  [Speech Recognition and Meaning Interpretation: Towards Disambiguation of Structurally Ambiguous Spoken Utterances in Indonesian](https://doi.org/10.18653/v1/2023.emnlp-main.1045) |  | 0 |  | Ruhiyah Widiaputri, Ayu Purwarianti, Dessi Puji Lestari, Kurniawati Azizah, Dipta Tanaya, Sakriani Sakti |  |
| 2240 |  |  [Target-Agnostic Gender-Aware Contrastive Learning for Mitigating Bias in Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.1046) |  | 0 |  | Minwoo Lee, Hyukhun Koh, Kangil Lee, Dongdong Zhang, Minsung Kim, Kyomin Jung |  |
| 2241 |  |  [Code-Switching Metrics Using Intonation Units](https://doi.org/10.18653/v1/2023.emnlp-main.1047) |  | 0 |  | Rebecca Pattichis, Dora LaCasse, Sonya Trawick, Rena Cacoullos |  |
