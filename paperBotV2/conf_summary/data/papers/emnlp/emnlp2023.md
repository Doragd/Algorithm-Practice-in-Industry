# EMNLP2023

## 会议论文列表

本会议共有 2241 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Frontmatter](https://aclanthology.org/2023.emnlp-demo.0) |  | 0 |  |  |  |
| 2 |  |  [Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs](https://doi.org/10.18653/v1/2023.emnlp-demo.1) |  | 0 | Most NLP tasks are modeled as supervised learning and thus require labeled training data to train effective models. However, manually producing such data at sufficient quality and quantity is known to be costly and time-intensive. Current research addresses this bottleneck by exploring a novel... | Alan Akbik, Felix Hamborg, Jonas Golde, Julian Risch, Patrick Haller |  |
| 3 |  |  [End-to-End Evaluation for Low-Latency Simultaneous Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-demo.2) |  | 0 | The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of... | Alexander Waibel, Carlos Mullov, Christian Huber, Danni Liu, Enes Yavuz Ugan, Fabian Retkowski, Jan Niehues, NgocQuan Pham, Sai Koneru, Stefan Constantin, Thai Binh Nguyen, Tu Anh Dinh, Zhaolin Li |  |
| 4 |  |  [CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools](https://doi.org/10.18653/v1/2023.emnlp-demo.3) |  | 0 | In the face of climate change, are companies really taking substantial steps toward more sustainable operations? A comprehensive answer lies in the dense, information-rich landscape of corporate sustainability reports. However, the sheer volume and complexity of these reports make human analysis... | Chiara Colesanti Senni, Dominik Stammbach, Glen Gostlow, Jingwei Ni, Julia Anna Bingler, Markus Leippold, Mathias Kraus, Nicolas Webersinke, Qian Wang, Saeid Ashraf Vaghefi, Tingyu Yu, Tobias Schimanski, Tobias Wekhof |  |
| 5 |  |  [RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.4) |  | 0 | Retrieval-augmented large language models (R-LLMs) combine pre-trained large language models (LLMs) with information retrieval systems to improve the accuracy of factual question-answering. However, current libraries for building R-LLMs provide high-level abstractions without sufficient... | Daisuke Miyashita, Jun Deguchi, Kento Tatsuno, Osamu Torii, Yasuhiro Morioka, Yasuto Hoshi, Youyang Ng |  |
| 6 |  |  [VIST5: An Adaptive, Retrieval-Augmented Language Model for Visualization-oriented Dialog](https://doi.org/10.18653/v1/2023.emnlp-demo.5) |  | 0 | The advent of large language models has brought about new ways of interacting with data intuitively via natural language. In recent years, a variety of visualization systems have explored the use of natural language to create and modify visualizations through visualization-oriented dialog. However,... | Henrik Voigt, Kai Lawonn, Markus Reichstein, Monique Meuschke, Nuno Carvalhais, Sina Zarrieß |  |
| 7 |  |  [H2O Open Ecosystem for State-of-the-art Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.6) |  | 0 | Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing... | Arno Candel, Chun Ming Lee, Jon McKinney, Marcos V. Conde, Maximilian Jeblick, Pascal Pfeiffer, Philipp Singer |  |
| 8 |  |  [Koala: An Index for Quantifying Overlaps with Pre-training Corpora](https://doi.org/10.18653/v1/2023.emnlp-demo.7) |  | 0 | In very recent years more attention has been placed on probing the role of pre-training data in Large Language Models (LLMs) downstream behaviour. Despite the importance, there is no public tool that supports such analysis of pre-training corpora at large scale. To help research in this space, we... | Ehsan Shareghi, Gholamreza Haffari, ThuyTrang Vu, Xuanli He |  |
| 9 |  |  [Sudowoodo: A Chinese Lyric Imitation System with Source Lyrics](https://doi.org/10.18653/v1/2023.emnlp-demo.8) |  | 0 | Lyrics generation is a well-known application in natural language generation research, with several previous studies focusing on generating accurate lyrics using precise control such as keywords, rhymes, etc. However, lyrics imitation, which involves writing new lyrics by imitating the style and... | Jiashu Pu, Le Zhang, Lin Jiang, Qihang Chen, Rongsheng Zhang, Yongzhu Chang |  |
| 10 |  |  [ConvLab-3: A Flexible Dialogue System Toolkit Based on a Unified Data Format](https://doi.org/10.18653/v1/2023.emnlp-demo.9) |  | 0 | Task-oriented dialogue (TOD) systems function as digital assistants, guiding users through various tasks such as booking flights or finding restaurants. Existing toolkits for building TOD systems often fall short in delivering comprehensive arrays of data, model, and experimental environments with... | Baolin Peng, Carel van Niekerk, Christian Geishauser, Dazhen Wan, HsienChin Lin, Jianfeng Gao, Michael Heck, Milica Gasic, Minlie Huang, Nurul Lubis, Qi Zhu, Shutong Feng, Xiaochen Zhu, Zheng Zhang |  |
| 11 |  |  [FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge](https://doi.org/10.18653/v1/2023.emnlp-demo.10) |  | 0 | Detecting factual errors of textual information, whether generated by large language models (LLM) or curated by humans, is crucial for making informed decisions. LLMs’ inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their... | Anton Belyi, Benjamin Han, Farima Fatahi Bayat, Fei Wu, Ihab F. Ilyas, Kun Qian, Samira Khorshidi, Yisi Sang, Yunyao Li |  |
| 12 |  |  [YATO: Yet Another deep learning based Text analysis Open toolkit](https://doi.org/10.18653/v1/2023.emnlp-demo.11) |  | 0 | We introduce YATO, an open-source, easy-to-use toolkit for text analysis with deep learning. Different from existing heavily engineered toolkits and platforms, YATO is lightweight and user-friendly for researchers from cross-disciplinary areas. Designed in a hierarchical structure, YATO supports... | Jiageng Wu, Jie Yang, Yile Wang, Zeqiang Wang, Zhiyang Teng |  |
| 13 |  |  [Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face](https://doi.org/10.18653/v1/2023.emnlp-demo.12) |  | 0 | We present Spacerini, a tool that integrates the Pyserini toolkit for reproducible information retrieval research with Hugging Face to enable the seamless construction and deployment of interactive search engines. Spacerini makes state-of-the-art sparse and dense retrieval models more accessible to... | Akintunde Oladipo, Aleksandra Piktus, Christopher Akiki, Jimmy Lin, Martin Potthast, Odunayo Ogundepo, Xinyu Zhang |  |
| 14 |  |  [Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning](https://doi.org/10.18653/v1/2023.emnlp-demo.13) |  | 0 | We introduce Adapters, an open-source library that unifies parameter-efficient and modular transfer learning in large language models. By integrating 10 diverse adapter methods into a unified interface, Adapters offers ease of use and flexible configuration. Our library allows researchers and... | Clifton Poth, Hannah Sterz, Indraneil Paul, Iryna Gurevych, Ivan Vulic, Jonas Pfeiffer, Leon Engländer, Sebastian Ruder, Sukannya Purkayastha, Timo Imhof |  |
| 15 |  |  [INTELMO: Enhancing Models' Adoption of Interactive Interfaces](https://doi.org/10.18653/v1/2023.emnlp-demo.14) |  | 0 | This paper presents INTELMO, an easy-to-use library to help model developers adopt user-faced interactive interfaces and articles from real-time RSS sources for their language models. The library categorizes common NLP tasks and provides default style patterns, streamlining the process of creating... | ChienSheng Wu, Chunxu Yang, Lidiya Murakhovs'ka, Philippe Laban, Xiang Chen |  |
| 16 |  |  [Humanoid Agents: Platform for Simulating Human-like Generative Agents](https://doi.org/10.18653/v1/2023.emnlp-demo.15) |  | 0 | Just as computational simulations of atoms, molecules and cells have shaped the way we study the sciences, true-to-life simulations of human-like agents can be valuable tools for studying human behavior. We propose Humanoid Agents, a system that guides Generative Agents to behave more like humans... | Yu Cheung Chiu, Yu Ying Chiu, Zhilin Wang |  |
| 17 |  |  [TP-Detector: Detecting Turning Points in the Engineering Process of Large-scale Projects](https://doi.org/10.18653/v1/2023.emnlp-demo.16) |  | 0 | This paper introduces a novel task of detecting turning points in the engineering process of large-scale projects, wherein the turning points signify significant transitions occurring between phases. Given the complexities involving diverse critical events and limited comprehension in individual... | Qi Wu, WenHan Chao, Xian Zhou, Zhunchen Luo |  |
| 18 |  |  [CLEVA: Chinese Language Models EVAluation Platform](https://doi.org/10.18653/v1/2023.emnlp-demo.17) |  | 0 | With the continuous emergence of Chinese Large Language Models (LLMs), how to evaluate a model’s capabilities has become an increasingly significant issue. The absence of a comprehensive Chinese benchmark that thoroughly assesses a model’s performance, the unstandardized and incomparable prompting... | Dahua Lin, Duo Zheng, Jianqiao Zhao, Liwei Wang, Michael R. Lyu, Shijia Huang, Xiaohui Su, Yanyang Li, Yongfeng Huang, Zhi Chen, ZiYuan Hu |  |
| 19 |  |  [DOPA METER - A Tool Suite for Metrical Document Profiling and Aggregation](https://doi.org/10.18653/v1/2023.emnlp-demo.18) |  | 0 | We present DOPA METER, a tool suite for the metrical investigation of written language, that provides diagnostic means for its division into discourse categories, such as registers, genres, and style. The quantitative basis of our system are 120 metrics covering a wide range of lexical, syntactic,... | Christina Lohr, Udo Hahn |  |
| 20 |  |  [Muted: Multilingual Targeted Offensive Speech Identification and Visualization](https://doi.org/10.18653/v1/2023.emnlp-demo.19) |  | 0 | Offensive language such as hate, abuse, and profanity (HAP) occurs in various content on the web. While previous work has mostly dealt with sentence level annotations, there have been a few recent attempts to identify offensive spans as well. We build upon this work and introduce MUTED, a system to... | Aashka Trivedi, Avirup Sil, Bishwaranjan Bhattacharjee, Christoph Tillmann, Rong Zhang, Santosh Borse, Sara Rosenthal |  |
| 21 |  |  [Gentopia.AI: A Collaborative Platform for Tool-Augmented LLMs](https://doi.org/10.18653/v1/2023.emnlp-demo.20) |  | 0 | Augmented Language Models (ALMs) empower large language models with the ability to use tools, transforming them into intelligent agents for real-world interactions. However, most existing frameworks for ALMs, to varying degrees, are deficient in the following critical features: flexible... | Binfeng Xu, Dongkuan Xu, Hua Shen, Murong Yue, Xukun Liu, Yuchen Liu, Yuhan Li, Zeyu Han, Zhiyuan Peng, Ziyu Yao |  |
| 22 |  |  [MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.21) |  | 0 | AI-empowered music processing is a diverse feld that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classifcation). For developers and amateurs, it is very diffcult to grasp all of these task to satisfy their requirements in... | Dingyao Yu, Jiang Bian, Kaitao Song, Peiling Lu, Shikun Zhang, Tianyu He, Wei Ye, Xu Tan |  |
| 23 |  |  [SentAlign: Accurate and Scalable Sentence Alignment](https://doi.org/10.18653/v1/2023.emnlp-demo.22) |  | 0 | We present SentAlign, an accurate sentence alignment tool designed to handle very large parallel document pairs. Given user-defined parameters, the alignment algorithm evaluates all possible alignment paths in fairly large documents of thousands of sentences and uses a divide-and-conquer approach... | Andy Way, Hrafn Loftsson, Steinþór Steingrímsson |  |
| 24 |  |  [QACheck: A Demonstration System for Question-Guided Multi-Hop Fact-Checking](https://doi.org/10.18653/v1/2023.emnlp-demo.23) |  | 0 | Fact-checking real-world claims often requires intricate, multi-step reasoning due to the absence of direct evidence to support or refute them. However, existing fact-checking systems often lack transparency in their decision-making, making it challenging for users to comprehend their reasoning... | Liangming Pan, MinYen Kan, Preslav Nakov, Xinyuan Lu |  |
| 25 |  |  [RobustQA: A Framework for Adversarial Text Generation Analysis on Question Answering Systems](https://doi.org/10.18653/v1/2023.emnlp-demo.24) |  | 0 | Question answering (QA) systems have reached human-level accuracy; however, these systems are not robust enough and are vulnerable to adversarial examples. Recently, adversarial attacks have been widely investigated in text classification. However, there have been few research efforts on this topic... | Fatemeh Kamani, Gholamreza GhassemSani, Gita Shojaee, Seyed Abolghasem Mirroshandel, Seyed Morteza Mirbostani, Seyedeh Fatemeh Ahmadi, Yasaman Boreshban |  |
| 26 |  |  [Kandinsky: An Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion](https://doi.org/10.18653/v1/2023.emnlp-demo.25) |  | 0 | Text-to-image generation is a significant domain in modern computer vision and achieved substantial improvements through the evolution of generative architectures. Among these, diffusion-based models demonstrated essential quality enhancements. These models generally split into two categories:... | Alexander Panchenko, Anastasia Maltseva, Andrey Kuznetsov, Angelina Kuts, Anton Razzhigaev, Arseniy Shakhmatov, Denis Dimitrov, Igor Pavlov, Ilya Ryabov, Vladimir Arkhipkin |  |
| 27 |  |  [NewsRecLib: A PyTorch-Lightning Library for Neural News Recommendation](https://doi.org/10.18653/v1/2023.emnlp-demo.26) |  | 0 | NewsRecLib is an open-source library based on Pytorch-Lightning and Hydra developed for training and evaluating neural news recommendation models. The foremost goals of NewsRecLib are to promote reproducible research and rigorous experimental evaluation by (i) providing a unified and highly... | Andreea Iana, Goran Glavas, Heiko Paulheim |  |
| 28 |  |  [MiniChain: A Small Library for Coding with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.27) |  | 0 | Programming augmented by large language models (LLMs) opens up many new application areas, but also requires care. LLMs are accurate enough, on average, to replace core functionality, yet make basic mistakes that demonstrate a lack of robustness. An ecosystem of prompting tools, from intelligent... | Alexander M. Rush |  |
| 29 |  |  [Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback](https://doi.org/10.18653/v1/2023.emnlp-demo.28) |  | 0 | A key technology for large language models (LLMs) involves instruction tuning that helps align the models’ responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning... | Chien Van Nguyen, Franck Dernoncourt, Nghia Trung Ngo, Ryan A. Rossi, Thien Huu Nguyen, Thuat Nguyen, Viet Dac Lai |  |
| 30 |  |  [SAGEViz: SchemA GEneration and Visualization](https://doi.org/10.18653/v1/2023.emnlp-demo.29) |  | 0 | Schema induction involves creating a graph representation depicting how events unfold in a scenario. We present SAGEViz, an intuitive and modular tool that utilizes human-AI collaboration to create and update complex schema graphs efficiently, where multiple annotators (humans and models) can work... | Francis Ferraro, Gautham Gunapati, Greg Durrett, Katrin Erk, Mahnaz Koupaee, Nathanael Chambers, Niranjan Balasubramanian, Raymond J. Mooney, Sai Vallurupalli, Sayontan Ghosh, Sugam Devare, Yash Kumar Lal |  |
| 31 |  |  [Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained Text Evaluation](https://doi.org/10.18653/v1/2023.emnlp-demo.30) |  | 0 | Fine-grained, span-level human evaluation has emerged as a reliable and robust method for evaluating text generation tasks such as summarization, simplification, machine translation and news generation, and the derived annotations have been useful for training automatic metrics and improving... | David Heineman, Wei Xu, Yao Dou |  |
| 32 |  |  [InsightPilot: An LLM-Empowered Automated Data Exploration System](https://doi.org/10.18653/v1/2023.emnlp-demo.31) |  | 0 | Exploring data is crucial in data analysis, as it helps users understand and interpret the data more effectively. However, performing effective data exploration requires in-depth knowledge of the dataset, the user intent and expertise in data analysis techniques. Not being familiar with either can... | Dongmei Zhang, Pingchuan Ma, Rui Ding, Shi Han, Shuai Wang |  |
| 33 |  |  [SynJax: Structured Probability Distributions for JAX](https://doi.org/10.18653/v1/2023.emnlp-demo.32) |  | 0 | The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited... | Laurent Sartran, Milos Stanojevic |  |
| 34 |  |  [RESIN-EDITOR: A Schema-guided Hierarchical Event Graph Visualizer and Editor](https://doi.org/10.18653/v1/2023.emnlp-demo.33) |  | 0 | In this paper, we present RESIN-EDITOR, an interactive event graph visualizer and editor designed for analyzing complex events. Our RESIN-EDITOR system allows users to render and freely edit hierarchical event graphs extracted from multimedia and multi-document news clusters with guidance from... | Heng Ji, Jiawei Han, Khanh Duy Nguyen, Martha Palmer, Reece Suchocki, Sha Li, Susan Windisch Brown, Zixuan Zhang |  |
| 35 |  |  [DRGCoder: Explainable Clinical Coding for the Early Prediction of Diagnostic-Related Groups](https://doi.org/10.18653/v1/2023.emnlp-demo.34) |  | 0 | Medical claim coding is the process of transforming medical records, usually presented as free texts written by clinicians, or discharge summaries, into structured codes in a classification system such as ICD-10 (International Classification of Diseases, Tenth Revision) or DRG (Diagnosis-Related... | Chris North, Daniel Hajialigol, Daphne Yao, David Liem, Derek Kaknes, Jimeng Sun, Tanner Barbour, Xuan Wang |  |
| 36 |  |  [CAMRA: Copilot for AMR Annotation](https://doi.org/10.18653/v1/2023.emnlp-demo.35) |  | 0 | In this paper, we introduce CAMRA (Copilot for AMR Annotatations), a cutting-edge web-based tool designed for constructing Abstract Meaning Representation (AMR) from natural language text. CAMRA offers a novel approach to deep lexical semantics annotation such as AMR, treating AMR annotation akin... | James H. Martin, Jon Z. Cai, Julia Bonn, Kristin WrightBettner, Martha Palmer, Shafiuddin Rehan Ahmed |  |
| 37 |  |  [Reaction Miner: An Integrated System for Chemical Reaction Extraction from Textual Data](https://doi.org/10.18653/v1/2023.emnlp-demo.36) |  | 0 | Chemical reactions, as a core entity in the realm of chemistry, hold crucial implications in diverse areas ranging from hands-on laboratory research to advanced computational drug design. Despite a burgeoning interest in employing NLP techniques to extract these reactions, aligning this task with... | Bobby Zhou, Heng Ji, Hongxiang Li, Huimin Zhao, Jiawei Han, Jinfeng Xiao, Leo Luo, Martin D. Burke, Ming Zhong, Minhao Jiang, Priyanka Kargupta, Siru Ouyang, Vivian Hu, Xianrui Zhong, Xuan Liu, Xuan Wang, Yanzhen Shen, Yizhu Jiao |  |
| 38 |  |  [CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies](https://doi.org/10.18653/v1/2023.emnlp-demo.37) |  | 0 | Various NLP tasks require a complex hierarchical structure over nodes, where each node is a cluster of items. Examples include generating entailment graphs, hierarchical cross-document coreference resolution, annotating event and subevent relations, etc. To enable efficient annotation of such... | Arie Cattan, Doug Downey, Ido Dagan, Lilach Eden, Roy BarHaim, Tom Hope, Yoav Kantor |  |
| 39 |  |  [Prompt2Model: Generating Deployable Models from Natural Language Instructions](https://doi.org/10.18653/v1/2023.emnlp-demo.38) |  | 0 | Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they... | Amanda Bertsch, Chenyang Zhao, Graham Neubig, Tongshuang Wu, Vijay Viswanathan |  |
| 40 |  |  [NewsSense: Reference-free Verification via Cross-document Comparison](https://doi.org/10.18653/v1/2023.emnlp-demo.39) |  | 0 | We present NewsSense, a novel sensemaking tool and reading interface designed to collect and integrate information from multiple news articles on a central topic. NewsSense provides “reference-free verification,” augmenting a central grounding article of the user’s choice by: (1) linking to related... | Jeremiah Milbauer, Tongshuang Wu, Zhijin Wu, Ziqi Ding |  |
| 41 |  |  [NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails](https://doi.org/10.18653/v1/2023.emnlp-demo.40) |  | 0 | NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path,... | Christopher Parisien, Jonathan Cohen, Makesh Narsimhan Sreedhar, Razvan Dinu, Traian Rebedea |  |
| 42 |  |  [LM-Polygraph: Uncertainty Estimation for Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.41) |  | 0 | Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields. However, a significant challenge arises as these models often “hallucinate”, i.e., fabricate facts without providing users an apparent means to... | Akim Tsvigun, Alexander Panchenko, Artem Shelmanov, Artem Vazhentsev, Daniil Vasilev, Ekaterina Fadeeva, Elizaveta Goncharova, Kirill Fedyanin, Maxim Panov, Roman Vashurin, Sergey Petrakov, Timothy Baldwin |  |
| 43 |  |  [Descriptive Knowledge Graph in Biomedical Domain](https://doi.org/10.18653/v1/2023.emnlp-demo.42) |  | 0 | We present a novel system that automatically extracts and generates informative and descriptive sentences from the biomedical corpus and facilitates the efficient search for relational knowledge. Unlike previous search engines or exploration systems that retrieve unconnected passages, our system... | Jie Huang, Kerui Zhu, Kevin ChenChuan Chang |  |
| 44 |  |  [Prompterator: Iterate Efficiently towards More Effective Prompts](https://doi.org/10.18653/v1/2023.emnlp-demo.43) |  | 0 | With the advent of Large Language Models (LLMs) the process known as prompting, which entices the LLM to solve an arbitrary language processing task without the need for finetuning, has risen to prominence. Finding well-performing prompts, however, is a non-trivial task which requires... | Andrej Svec, Daniel Skala, Marek Suppa, Peter Hraska, Samuel Sucik |  |
| 45 |  |  [ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.44) |  | 0 | The unprecedented performance of LLMs requires comprehensive and accurate evaluation. We argue that for LLMs evaluation, benchmarks need to be comprehensive and systematic. To this end, we propose the Zhujiu benchmark, which has the following strengths: (1) Multi-dimensional ability coverage: We... | Baoli Zhang, Haining Xie, Jun Zhao, Junhao Chen, Kang Liu, Pengfan Du, Pengfei Cao, Shengping Liu, Yubo Chen |  |
| 46 |  |  [PaperMage: A Unified Toolkit for Processing, Representing, and Manipulating Visually-Rich Scientific Documents](https://doi.org/10.18653/v1/2023.emnlp-demo.45) |  | 0 | Despite growing interest in applying natural language processing (NLP) and computer vision (CV) models to the scholarly domain, scientific documents remain challenging to work with. They’re often in difficult-to-use PDF formats, and the ecosystem of models to process them is fragmented and... | Amanpreet Singh, Angele Zamarron, Bailey Kuehl, Benjamin Newman, Chris Wilhelm, Daniel S. Weld, Doug Downey, Erin Bransom, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Marti A. Hearst, Regan Huff, Russell Authur, Stefan Candra, Yoganand Chandrasekhar, Zejiang Shen |  |
| 47 |  |  [OmniEvent: A Comprehensive, Fair, and Easy-to-Use Toolkit for Event Understanding](https://doi.org/10.18653/v1/2023.emnlp-demo.46) |  | 0 | Event understanding aims at understanding the content and relationship of events within texts, which covers multiple complicated information extraction tasks: event detection, event argument extraction, and event relation extraction. To facilitate related research and application, we present an... | Chuzhao Zhu, Feng Yao, Hao Peng, Juanzi Li, Kaisheng Zeng, Lei Hou, Xiaozhi Wang, Zimu Wang |  |
| 48 |  |  [CocoSciSum: A Scientific Summarization Toolkit with Compositional Controllability](https://doi.org/10.18653/v1/2023.emnlp-demo.47) |  | 0 | We present a novel toolkit for controlled summarization of scientific documents, designed for the specific needs of the scientific community. Our system generates summaries based on user preferences, adjusting key attributes specifically of length and keyword inclusion. A distinguishing feature is... | MinYen Kan, Qian Liu, Yanxia Qin, Yixi Ding |  |
| 49 |  |  [CoLLiE: Collaborative Training of Large Language Models in an Efficient Way](https://doi.org/10.18653/v1/2023.emnlp-demo.48) |  | 0 | Large language models (LLMs) are increasingly pivotal in a wide range of natural language processing tasks. Access to pre-trained models, courtesy of the open-source community, has made it possible to adapt these models to specific applications for enhanced performance. However, the substantial... | Hang Yan, Honglin Guo, Jiawei Hong, Kai Lv, Keyu Chen, Qipeng Guo, Shuhao Xing, Shuo Zhang, Tengxiao Liu, Tianle Gu, Xiaoran Liu, Xipeng Qiu, Yu Sun, Yuqing Yang |  |
| 50 |  |  [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://doi.org/10.18653/v1/2023.emnlp-demo.49) |  | 0 | We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike... | Hang Zhang, Lidong Bing, Xin Li |  |
| 51 |  |  [SummHelper: Collaborative Human-Computer Summarization](https://doi.org/10.18653/v1/2023.emnlp-demo.50) |  | 0 | Current approaches for text summarization are predominantly automatic, with rather limited space for human intervention and control over the process. In this paper, we introduce SummHelper, and screencast demo at https://www.youtube.com/watch?v=nGcknJwGhxk a 2-phase summarization assistant designed... | Aviv Slobodkin, Ido Dagan, Niv Nachum, Ori Shapira, Shmuel Amar |  |
| 52 |  |  [ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-demo.51) |  | 0 | Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent frameworks that equips... | Chen Cheng, Chenliang Li, Fei Huang, Haiyang Xu, He Chen, Hongzhu Shi, Ji Zhang, Jingren Zhou, Ming Yan, Weizhou Shen, Wenmeng Zhou, Yingda Chen, Zhicheng Zhang, Zhikai Wu |  |
| 53 |  |  [EfficientOCR: An Extensible, Open-Source Package for Efficiently Digitizing World Knowledge](https://doi.org/10.18653/v1/2023.emnlp-demo.52) |  | 0 | Billions of public domain documents remain trapped in hard copy or lack an accurate digitization. Modern natural language processing methods cannot be used to index, retrieve, and summarize their texts; conduct computational textual analyses; or extract information for statistical analyses, and... | Abhishek Arora, Jacob Carlson, Melissa Dell, Tom Bryan |  |
| 54 |  |  [Frontmatter](https://aclanthology.org/2023.emnlp-industry.0) |  | 0 |  |  |  |
| 55 |  |  [BeautifulPrompt: Towards Automatic Prompt Engineering for Text-to-Image Synthesis](https://doi.org/10.18653/v1/2023.emnlp-industry.1) |  | 0 | Recently, diffusion-based deep generative models (e.g., Stable Diffusion) have shown impressive results in text-to-image synthesis. However, current text-to-image models often require multiple passes of prompt engineering by humans in order to produce satisfactory results for real-world... | Bingyan Liu, Chengyu Wang, Jinhui Zhu, Jun Huang, Tingfeng Cao, Ziheng Wu |  |
| 56 |  |  [Enhancing Language Model with Unit Test Techniques for Efficient Regular Expression Generation](https://doi.org/10.18653/v1/2023.emnlp-industry.2) |  | 0 | Recent research has investigated the use of generative language models to produce regular expressions with semantic-based approaches. However, these approaches have shown shortcomings in practical applications, particularly in terms of functional correctness, which refers to the ability to... | Chenhui Mao, Xiexiong Lin, Xin Jin, Xin Zhang |  |
| 57 |  |  [A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models](https://doi.org/10.18653/v1/2023.emnlp-industry.3) |  | 0 | Large language models have become a vital component in modern NLP, achieving state of the art performance in a variety of tasks. However, they are often inefficient for real-world deployment due to their expensive inference costs. Knowledge distillation is a promising technique to improve their... | Aashka Trivedi, Bishwaranjan Bhattacharjee, Michele Merler, Takuma Udagawa |  |
| 58 |  |  [Towards Effective Automatic Debt Collection with Persona Awareness](https://doi.org/10.18653/v1/2023.emnlp-industry.4) |  | 0 | Understanding debtor personas is crucial for collectors to empathize with debtors and develop more effective collection strategies. In this paper, we take the first step towards comprehensively investigating the significance of debtor personas and present a successful commercial practice on... | Chen Huang, Hongru Liang, Jia Liu, Junhong Liu, Tong Zhang, Wenqiang Lei, Zujie Wen |  |
| 59 |  |  [Gatekeeper to save COGS and improve efficiency of Text Prediction](https://doi.org/10.18653/v1/2023.emnlp-industry.5) |  | 0 | The text prediction (TP) workflow calls a Large Language Model (LLM), almost, after every character to get subsequent sequence of characters, till user accepts a suggestion. The confidence score of the prediction is commonly used for filtering the results to ensure that only correct predictions are... | Marjan Slavkovski, Milos Milunovic, Nidhi Tiwari, Siqing Chen, Sneha Kola |  |
| 60 |  |  [Efficient Transformer Knowledge Distillation: A Performance Review](https://doi.org/10.18653/v1/2023.emnlp-industry.6) |  | 0 | As pretrained transformer language models continue to achieve state-of-the-art performance, the Natural Language Processing community has pushed for advances in model compression and efficient attention mechanisms to address high computational requirements and limited input sequence length. Despite... | Ashton Williamson, Logan Lawrence, Nathan Brown, Tahj Anderson |  |
| 61 |  |  [CDD: A Large Scale Dataset for Legal Intelligence Research](https://doi.org/10.18653/v1/2023.emnlp-industry.7) |  | 0 | As an important application of Artificial Intelligence, legal intelligence has recently attracted the attention of many researchers. Previous works investigated diverse issues like predicting crimes, predicting outcomes of judicial debates, or extracting information/knowledge from various kinds of... | Adam Jatowt, Changzhen Ji, Haipang Wu, Yating Zhang |  |
| 62 |  |  [MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning](https://doi.org/10.18653/v1/2023.emnlp-industry.8) |  | 0 | In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our... | Noé Tits |  |
| 63 |  |  [Personalized Dense Retrieval on Global Index for Voice-enabled Conversational Systems](https://doi.org/10.18653/v1/2023.emnlp-industry.9) |  | 0 | Voice-controlled AI dialogue systems are susceptible to noise from phonetic variations and failure to resolve ambiguous entities. Typically, personalized entity resolution (ER) and/or query rewrites (QR) are deployed to recover from these error modes. Previous work in this field achieves... | Chaitanya Dwivedi, Charlotte Dzialo, Kanna Shimizu, Masha Belyi, Prajit Muppidi |  |
| 64 |  |  [Text2Topic: Multi-Label Text Classification System for Efficient Topic Detection in User Generated Content with Zero-Shot Capabilities](https://doi.org/10.18653/v1/2023.emnlp-industry.10) |  | 0 | Multi-label text classification is a critical task in the industry. It helps to extract structured information from large amount of textual data. We propose Text to Topic (Text2Topic), which achieves high multi-label classification performance by employing a Bi-Encoder Transformer architecture that... | Benjamin Wang, Elina Frayerman, Eran Fainman, Fengjun Wang, Karen Lastmann Assaraf, Moran Beladev, Ofri Kleinfeld, Sarai Mizrachi, Tal Shachar |  |
| 65 |  |  [Deep Metric Learning to Hierarchically Rank - An Application in Product Retrieval](https://doi.org/10.18653/v1/2023.emnlp-industry.11) |  | 0 | Most e-commerce search engines use customer behavior signals to augment lexical matching and improve search relevance. Many e-commerce companies like Amazon, Alibaba, Ebay etc. operate in multiple countries with country specific stores. However, customer behavior data is sparse in newer stores. To... | Ashutosh Joshi, Changhe Yuan, Ismail B. Tutar, Karim Bouyarmane, Kee Kiat Koo, Nishaanth Reddy, Vaclav Petricek |  |
| 66 |  |  [A Pretrained Language Model for Cyber Threat Intelligence](https://doi.org/10.18653/v1/2023.emnlp-industry.12) |  | 0 | We present a new BERT model for the cybersecurity domain, CTI-BERT, which can improve the accuracy of cyber threat intelligence (CTI) extraction, enabling organizations to better defend against potential cyber threats. We provide detailed information about the domain corpus collection, the training... | Weiqiu You, Youngja Park |  |
| 67 |  |  [SAMP: A Model Inference Toolkit of Post-Training Quantization for Text Processing via Self-Adaptive Mixed-Precision](https://doi.org/10.18653/v1/2023.emnlp-industry.13) |  | 0 | The latest industrial inference engines, such as FasterTransformer and TurboTransformers, have verified that half-precision floating point (FP16) and 8-bit integer (INT8) quantization can greatly improve model inference speed. However, the existing INT8 quantization methods are too complicated, and... | Haoyan Liu, Kan Zhou, Rong Tian, Weijie Liu, Weiquan Mao, Zhe Zhao, Zijing Zhao |  |
| 68 |  |  [KD-Boost: Boosting Real-Time Semantic Matching in E-commerce with Knowledge Distillation](https://doi.org/10.18653/v1/2023.emnlp-industry.14) |  | 0 | Real-time semantic matching is vital to web and product search. Transformer-based models have shown to be highly effective at encoding queries into an embedding space where semantically similar entities (queries or results) are in close proximity. However, the computational complexity of large... | Ankith M. S, Sanjay Agrawal, Vivek Sembium |  |
| 69 |  |  [Multi-teacher Distillation for Multilingual Spelling Correction](https://doi.org/10.18653/v1/2023.emnlp-industry.15) |  | 0 | Accurate spelling correction is a critical step in modern search interfaces, especially in an era of mobile devices and speech-to-text interfaces. For services that are deployed around the world, this poses a significant challenge for multilingual NLP: spelling errors need to be caught and... | Christopher Potts, Jingfen Zhang, Sravan Bodapati, Xuan Guo |  |
| 70 |  |  [Does Named Entity Recognition Truly Not Scale Up to Real-world Product Attribute Extraction?](https://doi.org/10.18653/v1/2023.emnlp-industry.16) |  | 0 | The key challenge in the attribute-value extraction (AVE) task from e-commerce sites is the scalability to diverse attributes for a large number of products in real-world e-commerce sites. To make AVE scalable to diverse attributes, recent researchers adopted a question-answering (QA)-based... | Keiji Shinzato, Naoki Yoshinaga, WeiTe Chen, Yandi Xia |  |
| 71 |  |  [Investigating Table-to-Text Generation Capabilities of Large Language Models in Real-World Information Seeking Scenarios](https://doi.org/10.18653/v1/2023.emnlp-industry.17) |  | 0 | Tabular data is prevalent across various industries, necessitating significant time and effort for users to understand and manipulate for their information-seeking purposes. The advancements in large language models (LLMs) have shown enormous potential to improve user efficiency. However, the... | Arman Cohan, Haowei Zhang, Linyong Nan, Shengyun Si, Xiangru Tang, Yilun Zhao |  |
| 72 |  |  [TMID: A Comprehensive Real-world Dataset for Trademark Infringement Detection in E-Commerce](https://doi.org/10.18653/v1/2023.emnlp-industry.18) |  | 0 | Annually, e-commerce platforms incur substantial financial losses due to trademark infringements, making it crucial to identify and mitigate potential legal risks tied to merchant information registered to the platforms. However, the absence of high-quality datasets hampers research in this area.... | Lizhen Qu, Tongxin Hu, Xin Jin, Xin Zhang, Zhuang Li |  |
| 73 |  |  [Joint Dialogue Topic Segmentation and Categorization: A Case Study on Clinical Spoken Conversations](https://doi.org/10.18653/v1/2023.emnlp-industry.19) |  | 0 | Utilizing natural language processing techniques in clinical conversations is effective to improve the efficiency of health management workflows for medical staff and patients. Dialogue segmentation and topic categorization are two fundamental steps for processing verbose spoken conversations and... | Hong Choon Oh, Nancy F. Chen, Pavitra Krishnaswamy, Siti Umairah Md. Salleh, Zhengyuan Liu |  |
| 74 |  |  [AdapterDistillation: Non-Destructive Task Composition with Knowledge Distillation](https://doi.org/10.18653/v1/2023.emnlp-industry.20) |  | 0 | Leveraging knowledge from multiple tasks through introducing a small number of task specific parameters into each transformer layer, also known as adapters, receives much attention recently. However, adding an extra fusion layer to implement knowledge composition not only increases the inference... | Jing Zheng, Junjie Wang, Sen Hu, Teng Xu, Wangshu Zhang, Yicheng Chen |  |
| 75 |  |  [PROMINET: Prototype-based Multi-View Network for Interpretable Email Response Prediction](https://doi.org/10.18653/v1/2023.emnlp-industry.21) |  | 0 | Email is a widely used tool for business communication, and email marketing has emerged as a cost-effective strategy for enterprises. While previous studies have examined factors affecting email marketing performance, limited research has focused on understanding email response behavior by... | Ehsan Degan, Prashanth Vijayaraghavan, Yuqing Wang |  |
| 76 |  |  [Retrieval-Enhanced Dual Encoder Training for Product Matching](https://doi.org/10.18653/v1/2023.emnlp-industry.22) |  | 0 | Product matching is the task of matching a seller-listed item to an appropriate product. It is a critical task for an e-commerce platform, and the approach needs to be efficient to run in a large-scale setting. A dual encoder approach has been a common practice for product matching recently, due to... | Justin Chiu |  |
| 77 |  |  [WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-industry.23) |  | 0 | This paper introduces WordArt Designer, a user-driven framework for artistic typography synthesis, relying on the Large Language Model (LLM). The system incorporates four key modules: the LLM Engine, SemTypo, StyTypo, and TexTypo modules. 1) The LLM Engine, empowered by the LLM (e.g. GPT-3.5),... | Bin Luo, Chenyang Li, Jingdong Sun, JunYan He, Wangmeng Xiang, Xianhui Lin, Xiaoyang Kang, Xuansong Xie, Yifeng Geng, Yusen Hu, Zengke Jin, ZhiQi Cheng |  |
| 78 |  |  [Lattice Path Edit Distance: A Romanization-aware Edit Distance for Extracting Misspelling-Correction Pairs from Japanese Search Query Logs](https://doi.org/10.18653/v1/2023.emnlp-industry.24) |  | 0 | Edit distance has been successfully used to extract training data, i.e., misspelling-correction pairs, of spelling correction models from search query logs in languages including English. However, the success does not readily apply to Japanese, where misspellings are often dissimilar to correct... | Nobuhiro Kaji |  |
| 79 |  |  [Learning Multilingual Sentence Representations with Cross-lingual Consistency Regularization](https://doi.org/10.18653/v1/2023.emnlp-industry.25) |  | 0 | Multilingual sentence representations are the foundation for similarity-based bitext mining, which is crucial for scaling multilingual neural machine translation (NMT) system to more languages. In this paper, we introduce MuSR: a one-for-all Multilingual Sentence Representation model that supports... | Haifeng Wang, Hua Wu, Liwen Zhang, Pengzhi Gao, Zhongjun He |  |
| 80 |  |  [Unveiling Identity Biases in Toxicity Detection : A Game-Focused Dataset and Reactivity Analysis Approach](https://doi.org/10.18653/v1/2023.emnlp-industry.26) |  | 0 | Identity biases arise commonly from annotated datasets, can be propagated in language models and can cause further harm to marginal groups. Existing bias benchmarking datasets are mainly focused on gender or racial biases and are made to pinpoint which class the model is biased towards. They also... | Grégoire Winterstein, Josiane Van Dorpe, Nicolas GrenonGodbout, Zachary Yang |  |
| 81 |  |  [ORANGE: Text-video Retrieval via Watch-time-aware Heterogeneous Graph Contrastive Learning](https://doi.org/10.18653/v1/2023.emnlp-industry.27) |  | 0 | With the explosive growth of short-video data on industrial video-sharing platforms such as TikTok and YouTube, text-video retrieval techniques have become increasingly important. Most existing works for text-video retrieval focus on designing informative representation learning methods and... | Donghui Li, Feng Wang, Jianqiang Ma, Tim Chang, Ting Peng, Yaning Chang, Yucheng Lin, Zang Li, Zhiyi Zhou |  |
| 82 |  |  [Compute-Efficient Churn Reduction for Conversational Agents](https://doi.org/10.18653/v1/2023.emnlp-industry.28) |  | 0 | Model churn occurs when re-training a model yields different predictions despite using the same data and hyper-parameters. Churn reduction is crucial for industry conversational systems where users expect consistent results for the same queries. In this setting, compute resources are often limited... | Christopher Hidey, Sarthak Sarthak |  |
| 83 |  |  [Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering](https://doi.org/10.18653/v1/2023.emnlp-industry.29) |  | 0 | Large Language Model (LLM) has gained popularity and achieved remarkable results in open-domain tasks, but its performance in real industrial domain-specific scenarios is average due to its lack of specific domain knowledge. This issue has attracted widespread attention, but there are few relevant... | Bo Qiao, Dongmei Zhang, Fangkai Yang, Jue Zhang, Lu Wang, Mohit Garg, Pu Zhao, Qingwei Lin, Saravan Rajmohan, Zezhong Wang |  |
| 84 |  |  [Enhancing Extreme Multi-Label Text Classification: Addressing Challenges in Model, Data, and Evaluation](https://doi.org/10.18653/v1/2023.emnlp-industry.30) |  | 0 | Extreme multi-label text classification is a prevalent task in industry, but it frequently encounters challenges in terms of machine learning perspectives, including model limitations, data scarcity, and time-consuming evaluation. This paper aims to mitigate these issues by introducing novel... | Agnes Masip Gomez, Dan Li, Georgios Tsatsaronis, Janneke van de Loo, Vikrant Yadav, Zi Long Zhu, Zubair Afzal |  |
| 85 |  |  [Query-aware Multi-modal based Ranking Relevance in Video Search](https://doi.org/10.18653/v1/2023.emnlp-industry.31) |  | 0 | Relevance ranking system plays a crucial role in video search on streaming platforms. Most relevance ranking methods focus on text modality, incapable of fully exploiting cross-modal cues present in video. Recent multi-modal models have demonstrated promise in various vision-language tasks but... | Chengcan Ye, Feng Wang, Tim Chang, Ting Peng, Zhiyi Zhou |  |
| 86 |  |  [Coordinated Replay Sample Selection for Continual Federated Learning](https://doi.org/10.18653/v1/2023.emnlp-industry.32) |  | 0 | Continual Federated Learning (CFL) combines Federated Learning (FL), the decentralized learning of a central model on a number of client devices that may not communicate their data, and Continual Learning (CL), the learning of a model from a continual stream of data without keeping the entire... | Charith Peris, Christophe Dupuy, Clement Chung, Jack Good, Jimit Majmudar, Jixuan Wang, Rahul Gupta, Richard S. Zemel |  |
| 87 |  |  [Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective](https://doi.org/10.18653/v1/2023.emnlp-industry.33) |  | 0 | This paper studies how to effectively build meeting summarization systems for real-world usage using large language models (LLMs). For this purpose, we conduct an extensive evaluation and comparison of various closed-source and open-source LLMs, namely, GPT-4, GPT-3.5, PaLM-2, and LLaMA-2. Our... | Cheng Chen, Md. Tahmid Rahman Laskar, Shashi Bhushan TN, XueYong Fu |  |
| 88 |  |  [Creator Context for Tweet Recommendation](https://doi.org/10.18653/v1/2023.emnlp-industry.34) |  | 0 | When discussing a tweet, people usually not only refer to the content it delivers, but also to the person behind the tweet. In other words, grounding the interpretation of the tweet in the context of its creator plays an important role in deciphering the true intent and the importance of the tweet.... | Marc Najork, Matt Colen, Michael Bendersky, Mingyang Zhang, Sergey Levi, Spurthi Amba Hombaiah, Tanvir Amin, Tao Chen, Vladimir Ofitserov |  |
| 89 |  |  [AdaBERT-CTC: Leveraging BERT-CTC for Text-Only Domain Adaptation in ASR](https://doi.org/10.18653/v1/2023.emnlp-industry.35) |  | 0 | End-to-end (E2E) automatic speech recognition (ASR) models are becoming increasingly popular in commercial applications, such as virtual assistants, closed captioning, and dictation systems. The accuracy of the ASR is crucial to their success. However, E2E models still struggle to recognize... | Dhanush Bekal, Karel Mundnich, Sravan Bodapati, Srikanth Ronanki, Tyler Vuong, Veera Raghavendra Elluru |  |
| 90 |  |  [Conversing with databases: Practical Natural Language Querying](https://doi.org/10.18653/v1/2023.emnlp-industry.36) |  | 0 | In this work, we designed, developed and released in production DataQue – a hybrid NLQ (Natural Language Querying) system for conversational DB querying. We address multiple practical problems that are not accounted for in public Text-to-SQL solutions – numerous complex implied conditions in user... | Denis Kochedykov, Fenglin Yin, Sreevidya Khatravath |  |
| 91 |  |  [AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications](https://doi.org/10.18653/v1/2023.emnlp-industry.37) |  | 0 | Adversarially testing large language models (LLMs) is crucial for their safe and responsible deployment in practice. We introduce an AI-assisted approach for automated generation of adversarial evaluation datasets to test the safety of LLM generations on new downstream applications. We call it AART... | Bhaktipriya Radharapu, Kevin Robinson, Lora Aroyo, Preethi Lahoti |  |
| 92 |  |  [Speakerly: A Voice-based Writing Assistant for Text Composition](https://doi.org/10.18653/v1/2023.emnlp-industry.38) |  | 0 | We present Speakerly, a new real-time voice-based writing assistance system that helps users with text composition across various use cases such as emails, instant messages, and notes. The user can interact with the system through instructions or dictation, and the system generates a well-formatted... | Alice KaiserSchatzlein, Apurva Joshi, Dhruv Kumar, Justin HuguesNuger, Navid Chowdhury, Robyn Perry, Samuel Lou, Vipul Raheja |  |
| 93 |  |  [Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks](https://doi.org/10.18653/v1/2023.emnlp-industry.39) |  | 0 | The most recent large language models (LLMs) such as ChatGPT and GPT-4 have shown exceptional capabilities of generalist models, achieving state-of-the-art performance on a wide range of NLP tasks with little or no adaptation. How effective are such models in the finance domain? Understanding this... | Sameena Shah, Samuel Chan, Xianzhi Li, Xiaodan Zhu, Xiaomo Liu, Yulong Pei, Zhiqiang Ma |  |
| 94 |  |  [CL-QR: Cross-Lingual Enhanced Query Reformulation for Multi-lingual Conversational AI Agents](https://doi.org/10.18653/v1/2023.emnlp-industry.40) |  | 0 | The growing popularity of conversational AI agents such as Alexa, Google Assistant, and Siri rely on accurate spoken language comprehension. The query reformulation (QR) method, which reformulates defective user queries, has been broadly adopted to mitigate the challenges posed by understanding... | Chengyuan Ma, Chenlei Guo, Sixing Lu, Wei Shen, Xiaohu Liu, Xing Fan, Zhengyang Zhao, Zhongkai Sun |  |
| 95 |  |  [Improving Contextual Query Rewrite for Conversational AI Agents through User-preference Feedback Learning](https://doi.org/10.18653/v1/2023.emnlp-industry.41) |  | 0 | Contextual query rewriting (CQR) is a crucial component in Conversational AI agents, leveraging the contextual information from previous user-agent conversations to improve the comprehension of current user intent. However, traditional CQR methods often concentrate on supervised fine-tuning only,... | Chengyuan Ma, Chenlei Guo, Jie Hao, Wei Shen, Xing Fan, Yanbin Lu, Yingxue Zhou, Zhongkai Sun |  |
| 96 |  |  [Scaling Neural ITN for Numbers and Temporal Expressions in Tamil: Findings for an Agglutinative Low-resource Language](https://doi.org/10.18653/v1/2023.emnlp-industry.42) |  | 0 | ITN involves rewriting the verbalised form of text from spoken transcripts to its corresponding written form. The task inherently expects challenges in identifying ITN entries due to spelling variations in words arising out of dialects, transcription errors etc. Additionally, in Tamil, word... | Amrith Krishna, Bhavuk Singhal, Malolan Chetlur, Sindhuja Gopalan |  |
| 97 |  |  [EELBERT: Tiny Models through Dynamic Embeddings](https://doi.org/10.18653/v1/2023.emnlp-industry.43) |  | 0 | We introduce EELBERT, an approach for compression of transformer-based models (e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is achieved by replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations. Since the input embedding... | Deepanshu Gupta, Gabrielle Cohn, Rishika Agarwal, Siddharth Patwardhan |  |
| 98 |  |  [Gold Standard Bangla OCR Dataset: An In-Depth Look at Data Preprocessing and Annotation Processes](https://doi.org/10.18653/v1/2023.emnlp-industry.44) |  | 0 | This research paper focuses on developing an improved Bangla Optical Character Recognition (OCR) system, addressing the challenges posed by the complexity of Bangla text structure, diverse handwriting styles, and the scarcity of comprehensive datasets. Leveraging recent advancements in Deep... | A. k. m Mahamud, AKM Shahariar Azad Rabby, Fuad Rahman, Hasmot Ali, Md. Majedul Islam, Nazmul Hasan |  |
| 99 |  |  [PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching](https://doi.org/10.18653/v1/2023.emnlp-industry.45) |  | 0 | Instruction fine-tuning has conventionally been employed to adapt Large Language Models (LLMs) to a variety of diverse tasks. Nonetheless, this technique often necessitates substantial computational resources, making it impractical for deployment by individuals or small-scale entities. Recently,... | Chao Qu, Shaojie Shi, Xiaoyu Tan, Yinghui Xu, Yuan Qi, Zhenting Qi |  |
| 100 |  |  [Welcome to the Real World: Efficient, Incremental and Scalable Key Point Analysis](https://doi.org/10.18653/v1/2023.emnlp-industry.46) |  | 0 | Key Point Analysis (KPA) is an emerging summarization framework, which extracts the main points from a collection of opinions, and quantifies their prevalence. It has been successfully applied to diverse types of data, including arguments, user reviews and survey responses. Despite the growing... | Lilach Eden, Matan Orbach, Noam Slonim, Roy BarHaim, Yoav Kantor, Yoav Katz |  |
| 101 |  |  [Automatic Linking of Judgements to UK Supreme Court Hearings](https://doi.org/10.18653/v1/2023.emnlp-industry.47) |  | 0 | One the most important archived legal material in the UK is the Supreme Court published judgements and video recordings of court sittings for the decided cases. The impact of Supreme Court published material extends far beyond the parties involved in any given case as it provides landmark rulings... | Constantin Orasan, Hadeel Saadany |  |
| 102 |  |  [Automatic Marketing Theme and Commodity Construction System for E-commerce](https://doi.org/10.18653/v1/2023.emnlp-industry.48) |  | 0 | When consumers’ shopping needs are concentrated, they are more interested in the collection of commodities under the specific marketing theme. Therefore, mining marketing themes and their commodities collections can help customers save shopping costs and improve user clicks and purchases for... | Hainan Zhang, Hongshen Chen, Jinghe Hu, Peng Lin, Sulong Xu, Tianhao Li, Zhiping Wang, Zhuoye Ding |  |
| 103 |  |  [Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures](https://doi.org/10.18653/v1/2023.emnlp-industry.49) |  | 0 | This paper introduces a new IncidentAI dataset for safety prevention. Different from prior corpora that usually contain a single task, our dataset comprises three tasks: named entity recognition, cause-effect extraction, and information retrieval. The dataset is annotated by domain experts who have... | Dung Le, Hiroki Mizokuchi, HuuHiep Nguyen, MinhTien Nguyen, Shumpei Inoue, TuanAnh D. Nguyen |  |
| 104 |  |  [An Auxiliary Task Boosted Multi-task Learning Method for Service Account Retrieval with Limited Human Annotation](https://doi.org/10.18653/v1/2023.emnlp-industry.50) |  | 0 | Service accounts, including organizations’ official accounts and mini-programs, provide various convenient services for users, and have become crucial components of a number of applications. Therefore, retrieving service accounts quickly and accurately is vital. However, this task suffers from the... | Huasheng Liang, Kaijia Yang, Qiang Yan, Yongjun Xu, Yuanzhou Yao, Zhao Zhang |  |
| 105 |  |  [VKIE: The Application of Key Information Extraction on Video Text](https://doi.org/10.18653/v1/2023.emnlp-industry.51) |  | 0 | Extracting structured information from videos is critical for numerous downstream applications in the industry. In this paper, we define a significant task of extracting hierarchical key information from visual texts on videos. To fulfill this task, we decouple it into four subtasks and introduce... | Di Yin, Haoyuan Peng, Siyu An, Ye Liu |  |
| 106 |  |  [Investigating the Role and Impact of Disfluency on Summarization](https://doi.org/10.18653/v1/2023.emnlp-industry.52) |  | 0 | Contact centers handle both chat and voice calls for the same domain. As part of their workflow, it is a standard practice to summarize the conversations once they conclude. A significant distinction between chat and voice communication lies in the presence of disfluencies in voice calls, such as... | Ayush Kumar, Jithendra Vepa, Varun Nathan |  |
| 107 |  |  [InsightNet : Structured Insight Mining from Customer Feedback](https://doi.org/10.18653/v1/2023.emnlp-industry.53) |  | 0 | We propose InsightNet, a novel approach for the automated extraction of structured insights from customer reviews. Our end-to-end machine learning framework is designed to overcome the limitations of current solutions, including the absence of structure for identified topics, non-standard aspect... | Chetan Aggarwal, Jitenkumar Rana, Manan Soni, Promod Yenigalla, Rashmi Patange, Sandeep Sricharan Mukku, Shyam Mohan |  |
| 108 |  |  [E2E Spoken Entity Extraction for Virtual Agents](https://doi.org/10.18653/v1/2023.emnlp-industry.54) |  | 0 | In human-computer conversations, extracting entities such as names, street addresses and email addresses from speech is a challenging task. In this paper, we study the impact of fine-tuning pre-trained speech encoders on extracting spoken entities in human-readable form directly from speech without... | Karan Singla, Srinivas Bangalore, YeonJun Kim |  |
| 109 |  |  [Generative Models for Product Attribute Extraction](https://doi.org/10.18653/v1/2023.emnlp-industry.55) |  | 0 | Product attribute extraction is an emerging field in information extraction and e-commerce, with applications including knowledge base construction, product recommendation, and enhancing customer experiences. In this work, we explore the use of generative models for product attribute extraction. We... | Ansel Blume, Heng Ji, Nasser Zalmout, Xian Li |  |
| 110 |  |  [CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering](https://doi.org/10.18653/v1/2023.emnlp-industry.56) |  | 0 | Large language models (LLMs) have demonstrated remarkable performance by following natural language instructions without fine-tuning them on domain-specific tasks and data. However, leveraging LLMs for domain-specific question answering suffers from severe limitations. The generated answer tends to... | Christian Suess, Julia Schneider, Ken E. Friedl, Maximilian Vogel, Md. Rashad Al Hasan Rony, Roman Teucher, Sinchana Ramakanth Bhat, Soumya R. Sahoo, Viju Sudhi |  |
| 111 |  |  [BUSTER: a "BUSiness Transaction Entity Recognition" dataset](https://doi.org/10.18653/v1/2023.emnlp-industry.57) |  | 0 | Albeit Natural Language Processing has seen major breakthroughs in the last few years, transferring such advances into real-world business cases can be challenging. One of the reasons resides in the displacement between popular benchmarks and actual data. Lack of supervision, unbalanced classes,... | Andrea Zugarini, Andrew Zamai, Leonardo Rigutini, Marco Ernandes |  |
| 112 |  |  [Multi-word Tokenization for Sequence Compression](https://doi.org/10.18653/v1/2023.emnlp-industry.58) |  | 0 | Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this paper, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word... | Andrea Zugarini, Leonardo Rigutini, Leonidas Gee, Marco Ernandes |  |
| 113 |  |  [JarviX: A LLM No code Platform for Tabular Data Analysis and Optimization](https://doi.org/10.18653/v1/2023.emnlp-industry.59) |  | 0 | In this study, we introduce JarviX, a sophisticated data analytics framework. JarviX is designed to employ Large Language Models (LLMs) to facilitate an automated guide and execute high-precision data analyzes on tabular datasets. This framework emphasizes the significance of varying column types,... | ChungWei Hsiung, Jianwei Zhang, Shangching Liu, Shengkun Wang, SianHong Luo, Tsungyao Chang, Wenqi Lin, YiChen Hsieh, YuPing Cheng |  |
| 114 |  |  [Retrieve and Copy: Scaling ASR Personalization to Large Catalogs](https://doi.org/10.18653/v1/2023.emnlp-industry.60) |  | 0 | Personalization of automatic speech recognition (ASR) models is a widely studied topic because of its many practical applications. Most recently, attention-based contextual biasing techniques are used to improve the recognition of rare words and/or domain specific entities. However, due to... | Devang Kulshreshtha, Sai Muralidhar Jayanthi, Saket Dingliwal, Sravan Bodapati, Srikanth Ronanki |  |
| 115 |  |  [STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants](https://doi.org/10.18653/v1/2023.emnlp-industry.61) |  | 0 | In the context of a voice assistant system, steering refers to the phenomenon in which a user issues a follow-up command attempting to direct or clarify a previous turn. We propose STEER, a steering detection model that predicts whether a follow-up turn is a user’s attempt to steer the previous... | Aditya Kulkarni, Dhivya Piraviperumal, Hong Yu, Jiarui Lu, Joel Ruben Antony Moniz, Leon Liyang Zhang, Nick Tzou, Tien Dung Tran |  |
| 116 |  |  [Self-Criticism: Aligning Large Language Models with their Understanding of Helpfulness, Honesty, and Harmlessness](https://doi.org/10.18653/v1/2023.emnlp-industry.62) |  | 0 | Recently, there has been a notable surge in the significance of large language models (LLMs) that engage in conversational-style interactions, such as ChatGPT and Claude, as they contribute significantly to the progress of artificial general intelligence (AGI). Typically, these models undergo a... | Chao Qu, Shaojie Shi, Xiaoyu Tan, Xihe Qiu, Yinghui Xu, Yuan Qi, Zhenting Qi |  |
| 117 |  |  [InstructPTS: Instruction-Tuning LLMs for Product Title Summarization](https://doi.org/10.18653/v1/2023.emnlp-industry.63) |  | 0 | E-commerce product catalogs contain billions of items. Most products have lengthy titles, as sellers pack them with product attributes to improve retrieval, and highlight key product aspects. This results in a gap between such unnatural products titles, and how customers refer to them. It also... | Besnik Fetahu, Oleg Rokhlenko, Shervin Malmasi, Zhiyu Chen |  |
| 118 |  |  [LLM4Vis: Explainable Visualization Recommendation using ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-industry.64) |  | 0 | Data visualization is a powerful tool for exploring and communicating insights in various domains. To automate visualization choice for datasets, a task known as visualization recommendation has been proposed. Various machine-learning-based approaches have been developed for this purpose, but they... | EePeng Lim, Lei Wang, Songheng Zhang, Yong Wang, Yun Wang |  |
| 119 |  |  [DUBLIN: Visual Document Understanding By Language-Image Network](https://doi.org/10.18653/v1/2023.emnlp-industry.65) |  | 0 | In this paper, we present DUBLIN, a pixel-based model for visual document understanding that does not rely on OCR. DUBLIN can process both images and texts in documents just by the pixels and handle diverse document types and tasks. DUBLIN is pretrained on a large corpus of document images with... | Aditi Khandelwal, Hardik Hansrajbhai Chauhan, Kriti Aggarwal, Kumar Tanmay, Monojit Choudhury, Owais Khan Mohammed, Qiang Liu, Saurabh Tiwary, Subhojit Som, Vishrav Chaudhary |  |
| 120 |  |  [DocumentNet: Bridging the Data Gap in Document Pre-training](https://doi.org/10.18653/v1/2023.emnlp-industry.66) |  | 0 | Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and... | Alexander G. Hauptmann, Hanjun Dai, Jiayi Chen, Jin Miao, Lijun Yu, Wei Wei, Xiaoyu Sun |  |
| 121 |  |  [Relevance-assisted Generation for Robust Zero-shot Retrieval](https://doi.org/10.18653/v1/2023.emnlp-industry.67) |  | 0 | Zero-shot retrieval tasks such as the BEIR benchmark reveal out-of-domain generalization as a key weakness of high-performance dense retrievers. As a solution, domain adaptation for dense retrievers has been actively studied. A notable approach is synthesizing domain-specific data, by generating... | Jihyuk Kim, Joonsuk Park, Minsoo Kim, Seungwon Hwang |  |
| 122 |  |  [Too much of product information : Don't worry, let's look for evidence!](https://doi.org/10.18653/v1/2023.emnlp-industry.68) |  | 0 | Product question answering (PQA) aims to provide an instant response to customer questions posted on shopping message boards, social media, brand websites and retail stores. In this paper, we propose a distantly supervised solution to answer customer questions by using product information.... | Aryan Jain, Chetan Aggarwal, Jitenkumar Rana |  |
| 123 |  |  [Harnessing LLMs for Temporal Data - A Study on Explainable Financial Time Series Forecasting](https://doi.org/10.18653/v1/2023.emnlp-industry.69) |  | 0 | Applying machine learning to financial time series has been an active area of industrial research enabling innovation in market insights, risk management, strategic decision-making, and policy formation. This paper explores the novel use of Large Language Models (LLMs) for explainable financial... | Xinli Yu, Yanbin Lu, Zheng Chen |  |
| 124 |  |  [ViGPTQA - State-of-the-Art LLMs for Vietnamese Question Answering: System Overview, Core Models Training, and Evaluations](https://doi.org/10.18653/v1/2023.emnlp-industry.70) |  | 0 | Large language models (LLMs) and their applications in low-resource languages (such as in Vietnamese) are limited due to lack of training data and benchmarking datasets. This paper introduces a practical real-world implementation of a question answering system for Vietnamese, called ViGPTQA,... | KhanhTung Tran, Minh Thuan Nguyen, NhuVan Nguyen, XuanSon Vu |  |
| 125 |  |  [An Integrated Search System for Korea Weather Data](https://doi.org/10.18653/v1/2023.emnlp-industry.71) |  | 0 | We introduce WeatherSearch, an integrated search system deployed at the Korea Meteorological Administration (KMA). WeatherSearch enables users to retrieve all the relevant data for weather forecasting from a massive weather database with simple natural language queries. We carefully design and... | Dayeon Ki, Jinkyung Jo, Minjoon Seo, Soyoung Yoon |  |
| 126 |  |  [Adaptive Hyper-parameter Learning for Deep Semantic Retrieval](https://doi.org/10.18653/v1/2023.emnlp-industry.72) |  | 0 | Deep semantic retrieval has achieved remarkable success in online E-commerce applications. The majority of methods aim to distinguish positive items and negative items for each query by utilizing margin loss or softmax loss. Despite their decent performance, these methods are highly sensitive to... | Binbin Wang, Chunyuan Yuan, Huimu Wang, Jingwei Zhuo, Lin Liu, Mingming Li, Peng Wang, Sulong Xu |  |
| 127 |  |  [On Sample-Efficient Code Generation](https://doi.org/10.18653/v1/2023.emnlp-industry.73) |  | 0 | Large language models often struggle to predict runtime behavior in code generation tasks, leading to a reliance on rejection sampling (best-of-n) to generate multiple code snippets then select the best. Our distinction is reducing sampling costs, without compromising generation quality. We... | Byoungjip Kim, Hojae Han, Kyunghoon Bae, Kyungjae Lee, Kyungmin Lee, Moontae Lee, Seungwon Hwang, Youngwon Lee, Yu Jin Kim |  |
| 128 |  |  [Batch Prompting: Efficient Inference with Large Language Model APIs](https://doi.org/10.18653/v1/2023.emnlp-industry.74) |  | 0 | Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use. We propose batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one... | Jungo Kasai, Tao Yu, Zhoujun Cheng |  |
| 129 |  |  [Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding](https://doi.org/10.18653/v1/2023.emnlp-industry.75) |  | 0 | A Personalized Query Rewriting system strives to minimize defective queries to ensure robust conversational functionality by considering individual user behavior and preferences. It’s designed as a search-based system, maintaining a user index of past successful interactions with the conversational... | Aram Galstyan, Eunah Cho, Fan Yang, Xiaojiang Huang, Xing Fan, Yanbin Lu, Zheng Chen, Ziyan Jiang |  |
| 130 |  |  [DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues](https://doi.org/10.18653/v1/2023.emnlp-industry.76) |  | 0 | Controversy is a reflection of our zeitgeist, and an important aspect to any discourse. The rise of large language models (LLMs) as conversational systems has increased public reliance on these systems for answers to their various questions. Consequently, it is crucial to systematically examine how... | Artem Abzaliev, Christopher Klein, David Q. Sun, Hadas Kotek, Jason D. Williams, Zidi Xiu |  |
| 131 |  |  [Angel: Enterprise Search System for the Non-Profit Industry](https://doi.org/10.18653/v1/2023.emnlp-industry.77) |  | 0 | Non-profit industry need a system for accurately matching fund-seekers (e.g., AMERICAN NATIONAL RED CROSS) with fund-givers (e.g., BILL AND MELINDA GATES FOUNDATION) aligned in cause (e.g., cancer) and target beneficiary group (e.g., children). In this paper, we create an enterprise search system... | Ashutosh Sharma, Pushpak Bhattacharyya, Saiful Haq |  |
| 132 |  |  [Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023](https://aclanthology.org/volumes/2023.findings-emnlp/) |  | 0 |  | Houda Bouamor, Juan Pino, Kalika Bali |  |
| 133 |  |  [Frontmatter](https://aclanthology.org/2023.findings-emnlp.0) |  | 0 |  |  |  |
| 134 |  |  [Multi Document Summarization Evaluation in the Presence of Damaging Content](https://doi.org/10.18653/v1/2023.findings-emnlp.1) |  | 0 | In the Multi-document summarization (MDS) task, a summary is produced for a given set of documents. A recent line of research introduced the concept of damaging documents, denoting documents that should not be exposed to readers due to various reasons. In the presence of damaging documents, a... | Avshalom Manevich, David Carmel, Elad Kravi, Nachshon Cohen, Ori Shapira |  |
| 135 |  |  [Guiding AMR Parsing with Reverse Graph Linearization](https://doi.org/10.18653/v1/2023.findings-emnlp.2) |  | 0 | Abstract Meaning Representation (AMR) parsing aims to extract an abstract semantic graph from a given sentence. The sequence-to-sequence approaches, which linearize the semantic graph into a sequence of nodes and edges and generate the linearized graph directly, have achieved good performance.... | Baobao Chang, Bofei Gao, Liang Chen, Peiyi Wang, Zhifang Sui |  |
| 136 |  |  [Translate the Beauty in Songs: Jointly Learning to Align Melody and Translate Lyrics](https://doi.org/10.18653/v1/2023.findings-emnlp.3) |  | 0 | Song translation requires both translation of lyrics and alignment of music notes so that the resulting verse can be sung to the accompanying melody, which is a challenging problem that has attracted some interests in different aspects of the translation process. In this paper, we propose... | Boxing Chen, Chengxi Li, Jiajun Bu, Kai Fan, Zhi Yu, Zhongqiang Huang |  |
| 137 |  |  [Aksharantar: Open Indic-language Transliteration datasets and models for the Next Billion Users](https://doi.org/10.18653/v1/2023.findings-emnlp.4) |  | 0 | Transliteration is very important in the Indian language context due to the usage of multiple scripts and the widespread use of romanized inputs. However, few training and evaluation sets are publicly available. We introduce Aksharantar, the largest publicly available transliteration dataset for... | Anoop Kunchukuttan, Gokul NC, Mitesh M. Khapra, Pratyush Kumar, Priyanka Bedekar, Ruchi Khapra, Sushane Parthan, Yash Madhani |  |
| 138 |  |  [Pretraining Without Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.5) |  | 0 | Transformers have been essential to pretraining success in NLP. While other architectures have been used, downstream accuracy is either significantly worse, or requires attention layers to match standard benchmarks such as GLUE. This work explores pretraining without attention by using recent... | Albert Gu, Alexander M. Rush, Jing Nathan Yan, Junxiong Wang |  |
| 139 |  |  [Time-Aware Representation Learning for Time-Sensitive Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.6) |  | 0 | Time is one of the crucial factors in real-world question answering (QA) problems. However, language models have difficulty understanding the relationships between time specifiers, such as ‘after’ and ‘before’, and numbers, since existing QA datasets do not include sufficient time expressions. To... | Alice Oh, Jungbin Son |  |
| 140 |  |  [EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics](https://doi.org/10.18653/v1/2023.findings-emnlp.7) |  | 0 | Efficiency is a key property to foster inclusiveness and reduce environmental costs, especially in an era of LLMs. In this work, we provide a comprehensive evaluation of efficiency for MT evaluation metrics. Our approach involves replacing computation-intensive transformers with lighter... | Christoph Leiter, Daniil Larionov, Jens Grünwald, Steffen Eger |  |
| 141 |  |  [Unsupervised Opinion Summarization Using Approximate Geodesics](https://doi.org/10.18653/v1/2023.findings-emnlp.8) |  | 0 | Opinion summarization is the task of creating summaries capturing popular opinions from user reviews. In this paper, we introduce Geodesic Summarizer (GeoSumm), a novel system to perform unsupervised extractive opinion summarization. GeoSumm consists of an encoder-decoder based representation... | Amr Ahmed, Avinava Dubey, Nicholas Monath, Snigdha Chaturvedi, Somnath Basu Roy Chowdhury |  |
| 142 |  |  [Investigating the Frequency Distortion of Word Embeddings and Its Impact on Bias Metrics](https://doi.org/10.18653/v1/2023.findings-emnlp.9) |  | 0 | Recent research has shown that static word embeddings can encode words’ frequencies. However, little has been studied about this behavior. In the present work, we study how frequency and semantic similarity relate to one another in static word embeddings, and we assess the impact of this... | Diego Fernández Slezak, Edgar Altszyler, Francisco Valentini, Juan Sosa |  |
| 143 |  |  [Improving Classifier Robustness through Active Generative Counterfactual Data Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.10) |  | 0 | Counterfactual Data Augmentation (CDA) is a commonly used technique for improving robustness in natural language classifiers. However, one fundamental challenge is how to discover meaningful counterfactuals and efficiently label them, with minimal human labeling cost. Most existing methods either... | Alex Beutel, Ananth Balashankar, Ben Packer, Ed H. Chi, Jilin Chen, Nithum Thain, Xuezhi Wang, Yao Qin |  |
| 144 |  |  [Data Augmentation Techniques for Machine Translation of Code-Switched Texts: A Comparative Study](https://doi.org/10.18653/v1/2023.findings-emnlp.11) |  | 0 | Code-switching (CSW) text generation has been receiving increasing attention as a solution to address data scarcity. In light of this growing interest, we need more comprehensive studies comparing different augmentation approaches. In this work, we compare three popular approaches: lexical... | Injy Hamed, Nizar Habash, Thang Vu |  |
| 145 |  |  [On the Relation between Sensitivity and Accuracy in In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.12) |  | 0 | In-context learning (ICL) suffers from oversensitivity to the prompt, making it unreliable in real-world scenarios. We study the sensitivity of ICL with respect to multiple perturbation types. First, we find that label bias obscures the true sensitivity, and therefore prior work may have... | Chen Zhao, He He, Kathleen R. McKeown, Yanda Chen, Zhou Yu |  |
| 146 |  |  [Self-distilled Transitive Instance Weighting for Denoised Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.13) |  | 0 | The widespread existence of wrongly labeled instances is a challenge to distantly supervised relation extraction. Most of the previous works are trained in a bag-level setting to alleviate such noise. However, sentence-level training better utilizes the information than bag-level training, as long... | Weijia Jia, Xiangyu Lin, Zhiguo Gong |  |
| 147 |  |  [MWE as WSD: Solving Multiword Expression Identification with Word Sense Disambiguation](https://doi.org/10.18653/v1/2023.findings-emnlp.14) |  | 0 | Recent approaches to word sense disambiguation (WSD) utilize encodings of the sense gloss (definition), in addition to the input context, to improve performance. In this work we demonstrate that this approach can be adapted for use in multiword expression (MWE) identification by training models... | Jacob Hoffman, Joshua Tanner |  |
| 148 |  |  [Dual Contrastive Learning Framework for Incremental Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.15) |  | 0 | Incremental learning plays a pivotal role in the context of online knowledge discovery, as it encourages large models (LM) to learn and refresh knowledge continuously. Many approaches have been proposed to simultaneously preserve knowledge from previous tasks while learning new concepts in online... | Jinghui Guo, Latifur Khan, Sadaf Md. Halim, Yigong Wang, Yu Lin, Zhuoyi Wang |  |
| 149 |  |  [Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards](https://doi.org/10.18653/v1/2023.findings-emnlp.16) |  | 0 | Community Question-Answering (CQA) portals serve as a valuable tool for helping users within an organization. However, making them accessible to non-English-speaking users continues to be a challenge. Translating questions can broaden the community’s reach, benefiting individuals with similar... | Asif Ekbal, Baban Gain, Muthusamy Chelliah, Nikesh Garera, Ramakrishna Appicharla, Soumya Chennabasavaraj |  |
| 150 |  |  [Filtered Semi-Markov CRF](https://doi.org/10.18653/v1/2023.findings-emnlp.17) |  | 0 | Semi-Markov CRF has been proposed as an alternative to the traditional Linear Chain CRF for text segmentation tasks such as Named Entity Recognition (NER). Unlike CRF, which treats text segmentation as token-level prediction, Semi-CRF considers segments as the basic unit, making it more expressive.... | Nadi Tomeh, Niama El Khbir, Pierre Holat, Thierry Charnois, Urchade Zaratiana |  |
| 151 |  |  [Data Pruning for Efficient Model Pruning in Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.18) |  | 0 | Model pruning methods reduce memory requirements and inference time of large-scale pre-trained language models after deployment. However, the actual pruning procedure is computationally intensive, involving repeated training and pruning until the required sparsity is achieved. This paper combines... | Abdul Hameed Azeemi, Agha Ali Raza, Ihsan Ayyub Qazi |  |
| 152 |  |  [Long-Form Speech Translation through Segmentation with Finite-State Decoding Constraints on Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.19) |  | 0 | One challenge in speech translation is that plenty of spoken content is long-form, but short units are necessary for obtaining high-quality translations. To address this mismatch, we adapt large language models (LLMs) to split long ASR transcripts into segments that can be independently translated... | Arya McCarthy, Felix Stahlberg, Hao Zhang, Ke Wu, Shankar Kumar |  |
| 153 |  |  [Re-Temp: Relation-Aware Temporal Representation Learning for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.findings-emnlp.20) |  | 0 | Temporal Knowledge Graph Completion (TKGC) under the extrapolation setting aims to predict the missing entity from a fact in the future, posing a challenge that aligns more closely with real-world prediction problems. Existing research mostly encodes entities and relations using sequential graph... | Josiah Poon, Kunze Wang, Soyeon Caren Han |  |
| 154 |  |  [RethinkingTMSC: An Empirical Study for Target-Oriented Multimodal Sentiment Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.21) |  | 0 | Recently, Target-oriented Multimodal Sentiment Classification (TMSC) has gained significant attention among scholars. However, current multimodal models have reached a performance bottleneck. To investigate the causes of this problem, we perform extensive empirical evaluation and in-depth analysis... | Jie Zhou, Junfeng Tian, Junjie Ye, Qi Zhang, Rui Wang, Tao Gui, Xuanjing Huang |  |
| 155 |  |  [Lexical Entrainment for Conversational Systems](https://doi.org/10.18653/v1/2023.findings-emnlp.22) |  | 0 | Conversational agents have become ubiquitous in assisting with daily tasks, and are expected to possess human-like features. One such feature is lexical entrainment (LE), a phenomenon in which speakers in human-human conversations tend to naturally and subconsciously align their lexical choices... | Aldo Lipani, Procheta Sen, Zhengxiang Shi |  |
| 156 |  |  [AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies](https://doi.org/10.18653/v1/2023.findings-emnlp.23) |  | 0 | We show that dialogue models can detect errors in their own messages, by calculating the likelihood of replies that are indicative of poor messages. For example, if an agent believes its partner is likely to respond “I don’t understand” to a candidate message, that message may not make sense, so an... | Adi Renduchintala, Athul Paul Jacob, Daniel Fried, Emily Dinan, Mike Lewis, Weiyan Shi, Zhou Yu |  |
| 157 |  |  [Follow-on Question Suggestion via Voice Hints for Voice Assistants](https://doi.org/10.18653/v1/2023.findings-emnlp.24) |  | 0 | The adoption of voice assistants like Alexa or Siri has grown rapidly, allowing users to instantly access information via voice search. Query suggestion is a standard feature of screen-based search experiences, allowing users to explore additional topics. However, this is not trivial to implement... | Anjie Fang, Besnik Fetahu, Giuseppe Castellucci, Oleg Rokhlenko, Pedro Faustini, Shervin Malmasi |  |
| 158 |  |  [Bidirectional Masked Self-attention and N-gram Span Attention for Constituency Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.25) |  | 0 | Attention mechanisms have become a crucial aspect of deep learning, particularly in natural language processing (NLP) tasks. However, in tasks such as constituency parsing, attention mechanisms can lack the directional information needed to form sentence spans. To address this issue, we propose a... | Minji Kim, Soohyeong Kim, Whanhee Cho, Yong Choi |  |
| 159 |  |  [CR-COPEC: Causal Rationale of Corporate Performance Changes to learn from Financial Reports](https://doi.org/10.18653/v1/2023.findings-emnlp.26) |  | 0 | In this paper, we introduce CR-COPEC called Causal Rationale of Corporate Performance Changes from financial reports. This is a comprehensive large-scale domain-adaptation causal sentence dataset to detect financial performance changes of corporate. CR-COPEC contributes to two major achievements.... | Byoung Seo, Jaesik Choi, Junyoup Lee, Kevin Compher, Kyunghwan Sohn, Nakwon Sung, Seungwon Hwang, Sunjae Kwon, Ye Eun Chun |  |
| 160 |  |  [Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT](https://doi.org/10.18653/v1/2023.findings-emnlp.27) |  | 0 | The goal of this paper is to explore how Transformer language models process semantic knowledge, especially regarding the plausibility of noun-verb relations. First, I demonstrate GPT2 exhibits a higher degree of similarity with humans in plausibility processing compared to other Transformer... | Soo Ryu |  |
| 161 |  |  [Automatic Unit Test Data Generation and Actor-Critic Reinforcement Learning for Code Synthesis](https://doi.org/10.18653/v1/2023.findings-emnlp.28) |  | 0 | The advent of large pre-trained language models in the domain of Code Synthesis has shown remarkable performance on various benchmarks, treating the problem of Code Generation in a fashion similar to Natural Language Generation, trained with a Language Modelling (LM) objective. In addition, the... | DerrickGohXin Deik, Gerasimos Lampouras, Ignacio Iacobacci, Matthieu Zimmer, Philip John Gorinski |  |
| 162 |  |  [Unlocking the Heterogeneous Landscape of Big Data NLP with DUUI](https://doi.org/10.18653/v1/2023.findings-emnlp.29) |  | 0 | Automatic analysis of large corpora is a complex task, especially in terms of time efficiency. This complexity is increased by the fact that flexible, extensible text analysis requires the continuous integration of ever new tools. Since there are no adequate frameworks for these purposes in the... | Alexander Leonhardt, Alexander Mehler, Daniel Baumartz, Giuseppe Abrami |  |
| 163 |  |  [Towards Agile Text Classifiers for Everyone](https://doi.org/10.18653/v1/2023.findings-emnlp.30) |  | 0 | Text-based safety classifiers are widely used for content moderation and increasingly to tune generative language model behavior - a topic of growing concern for the safety of digital assistants and chatbots. However, different policies require different classifiers, and safety policies themselves... | Ann Yuan, Jessica Hoffmann, Katrin Tomanek, Lucas Dixon, Maximilian Mozes, Muhamed Kouate, Nithum Thain, Tolga Bolukbasi |  |
| 164 |  |  [Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good](https://doi.org/10.18653/v1/2023.findings-emnlp.31) |  | 0 | With the recent advances in natural language processing (NLP), a vast number of applications have emerged across various use cases. Among the plethora of NLP applications, many academic researchers are motivated to do work that has a positive social impact, in line with the recent initiatives of... | Bernhard Schölkopf, Fernando Gonzalez Adauto, Mrinmaya Sachan, Rada Mihalcea, Tom Hope, Zhijing Jin |  |
| 165 |  |  [PAXQA: Generating Cross-lingual Question Answering Examples at Training Scale](https://doi.org/10.18653/v1/2023.findings-emnlp.32) |  | 0 | Existing question answering (QA) systems owe much of their success to large, high-quality training data. Such annotation efforts are costly, and the difficulty compounds in the cross-lingual setting. Therefore, prior cross-lingual QA work has focused on releasing evaluation datasets, and then... | Bryan Li, Chris CallisonBurch |  |
| 166 |  |  [Sharing, Teaching and Aligning: Knowledgeable Transfer Learning for Cross-Lingual Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.33) |  | 0 | In cross-lingual language understanding, machine translation is often utilized to enhance the transferability of models across languages, either by translating the training data from the source language to the target, or from the target to the source to aid inference. However, in cross-lingual... | Chengyu Wang, Chuanqi Tan, Jinhui Zhu, Jun Huang, Tingfeng Cao |  |
| 167 |  |  [BERT Goes Off-Topic: Investigating the Domain Transfer Challenge using Genre Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.34) |  | 0 | While performance of many text classification tasks has been recently improved due to Pretrained Language Models (PLMs), in this paper we show that they still suffer from a performance gap when the underlying distribution of topics changes. For example, a genre classifier trained on political... | Dmitri Roussinov, Serge Sharoff |  |
| 168 |  |  [Toward Stronger Textual Attack Detectors](https://doi.org/10.18653/v1/2023.findings-emnlp.35) |  | 0 | The landscape of available textual adversarial attacks keeps growing, posing severe threats and raising concerns regarding deep NLP systems integrity. However, the crucial problem of defending against malicious attacks has only drawn few attention in the NLP community. The latter is nonetheless... | Guillaume Staerman, Marine Picot, Nathan Noiry, Pablo Piantanida, Pierre Colombo |  |
| 169 |  |  [MEAL: Stable and Active Learning for Few-Shot Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.36) |  | 0 | Few-shot classification has made great strides due to foundation models that, through priming and prompting, are highly effective few-shot learners. However, this approach has high variance both across different sets of few shots (\*data selection\*) and across different finetuning runs (\*run... | Abdullatif Köksal, Hinrich Schütze, Timo Schick |  |
| 170 |  |  [Structure and Label Constrained Data Augmentation for Cross-domain Few-shot NER](https://doi.org/10.18653/v1/2023.findings-emnlp.37) |  | 0 | Cross-domain few-shot named entity recognition (NER) is a challenging task that aims to recognize entities in target domains with limited labeled data by leveraging relevant knowledge from source domains. However, domain gaps limit the effect of knowledge transfer and harm the performance of NER... | Jinan Xu, Jingyi Zhang, Ying Zhang, Yufeng Chen |  |
| 171 |  |  [Weakly-supervised Deep Cognate Detection Framework for Low-Resourced Languages Using Morphological Knowledge of Closely-Related Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.38) |  | 0 | Exploiting cognates for transfer learning in under-resourced languages is an exciting opportunity for language understanding tasks, including unsupervised machine translation, named entity recognition and information retrieval. Previous approaches mainly focused on supervised cognate detection... | John P. McCrae, Koustava Goswami, Priya Rani, Theodorus Fransen |  |
| 172 |  |  [SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data](https://doi.org/10.18653/v1/2023.findings-emnlp.39) |  | 0 | Text-to-SQL aims to automate the process of generating SQL queries on a database from natural language text. In this work, we propose “SQLPrompt”, tailored to improve the few-shot prompting capabilities of Text-to-SQL for Large Language Models (LLMs). Our methods include innovative prompt design,... | Hanjun Dai, Hootan Nakhost, Pengcheng Yin, Rajarishi Sinha, Ruoxi Sun, Sercan Ö. Arik, Tomas Pfister |  |
| 173 |  |  [Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.40) |  | 0 | Foundation models or pre-trained models have substantially improved the performance of various language, vision, and vision-language understanding tasks. However, existing foundation models can only perform the best in one type of tasks, namely language, vision, or vision-language. It is still an... | Hang Li, Jipeng Zhang, Xinsong Zhang, Yan Zeng |  |
| 174 |  |  [Trigger Warnings: Bootstrapping a Violence Detector for Fan Fiction](https://doi.org/10.18653/v1/2023.findings-emnlp.41) |  | 0 | We present the first dataset and evaluation results on a newly defined task: assigning trigger warnings. We introduce a labeled corpus of narrative fiction from Archive of Our Own (AO3), a popular fan fiction site, and define a document-level classification task to determine whether or not to... | Benno Stein, Christopher Schröder, Magdalena Wolska, Martin Potthast, Matti Wiegmann, Ole Borchardt |  |
| 175 |  |  [Pass-Tuning: Towards Structure-Aware Parameter-Efficient Tuning for Code Representation Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.42) |  | 0 | Code pre-trained models (CodePTMs) have recently become the de-facto paradigm for various tasks in the domain of code intelligence. To achieve excellent performance, the widely used strategy is to fine-tune all the parameters of CodePTMs. However, as the model size increases along with the number... | Jianing Wang, Ming Gao, Nuo Chen, Qiushi Sun, Xiang Li |  |
| 176 |  |  [Counterfactual Augmentation for Multimodal Learning Under Presentation Bias](https://doi.org/10.18653/v1/2023.findings-emnlp.43) |  | 0 | In real-world machine learning systems, labels are often derived from user behaviors that the system wishes to encourage. Over time, new models must be trained as new training examples and features become available. However, feedback loops between users and models can bias future user behavior,... | Dimitrios Dimitriadis, LouisPhilippe Morency, Srinagesh Sharma, Victoria Lin |  |
| 177 |  |  [A Table-to-Text Framework with Heterogeneous Multidominance Attention and Self-Evaluated Multi-Pass Deliberation](https://doi.org/10.18653/v1/2023.findings-emnlp.44) |  | 0 | Though big progress in table-to-text works, effectively leveraging table structure signals, e.g., hierarchical structure, remains challenging. Besides, deliberating generated descriptions proves to be effective for table-to-text. However, determining the appropriate outcome when encountering... | Feihu Jiang, Haoran Xin, Haoyang Duan, Hui Xiong, Jingbo Zhou, Wenjun Peng, Xi Chen, Xinjiang Lu |  |
| 178 |  |  [Crossing the Aisle: Unveiling Partisan and Counter-Partisan Events in News Reporting](https://doi.org/10.18653/v1/2023.findings-emnlp.45) |  | 0 | News media is expected to uphold unbiased reporting. Yet they may still affect public opinion by selectively including or omitting events that support or contradict their ideological positions. Prior work in NLP has only studied media bias via linguistic style and word usage. In this paper, we... | Kaijian Zou, Lu Wang, Nicholas Beauchamp, Winston Wu, Xinliang Frederick Zhang |  |
| 179 |  |  [Video-Text Retrieval by Supervised Sparse Multi-Grained Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.46) |  | 0 | While recent progress in video-text retrieval has been advanced by the exploration of better representation learning, in this paper, we present a novel multi-grained sparse learning framework, S3MA, to learn an aligned sparse space shared between the video and the text for video-text retrieval. The... | Peng Shi, Yimu Wang |  |
| 180 |  |  [Zero-Shot-BERT-Adapters: a Zero-Shot Pipeline for Unknown Intent Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.47) |  | 0 | Intent discovery is a crucial task in natural language processing, and it is increasingly relevant for various of industrial applications. Identifying novel, unseen intents from user inputs remains one of the biggest challenges in this field. Herein, we propose Zero-Shot-BERT-Adapters, a two-stage... | Daniele Comi, Dimitrios Christofidellis, Matteo Manica, Pier Francesco Piazza |  |
| 181 |  |  [ReFSQL: A Retrieval-Augmentation Framework for Text-to-SQL Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.48) |  | 0 | Text-to-SQL is the task that aims at translating natural language questions into SQL queries. Existing methods directly align the natural language with SQL Language and train one encoder-decoder-based model to fit all questions. However, they underestimate the inherent structural characteristics of... | Fei Sun, Hexiang Tan, Huawei Shen, Jianhe Cen, Kun Zhang, Xiexiong Lin, Xin Zhang, Xuhui Jiang, Yuanzhuo Wang |  |
| 182 |  |  [Approximating Two-Layer Feedforward Networks for Efficient Transformers](https://doi.org/10.18653/v1/2023.findings-emnlp.49) |  | 0 | How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general... | Jürgen Schmidhuber, Kazuki Irie, Róbert Csordás |  |
| 183 |  |  [Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.50) |  | 0 | Adapting a large language model for multiple-attribute text style transfer via fine-tuning can be challenging due to the substantial amount of computational resources and labeled data required for the specific downstream task. In this paper, we address this challenge by introducing Adapter-TST, a... | Nancy F. Chen, Roy KaWei Lee, Zhiqiang Hu |  |
| 184 |  |  [Solving the Right Problem is Key for Translational NLP: A Case Study in UMLS Vocabulary Insertion](https://doi.org/10.18653/v1/2023.findings-emnlp.51) |  | 0 | As the immense opportunities enabled by large language models become more apparent, NLP systems will be increasingly expected to excel in real-world settings. However, in many instances, powerful models alone will not yield translational NLP solutions, especially if the formulated problem is not... | Bernal Jimenez Gutierrez, Kin Wah Fung, Olivier Bodenreider, Vinh Nguyen, Yu Su, Yuqing Mao |  |
| 185 |  |  [Improving Cross-lingual Transfer through Subtree-aware Word Reordering](https://doi.org/10.18653/v1/2023.findings-emnlp.52) |  | 0 | Despite the impressive growth of the abilities of multilingual language models, such as XLM-R and mT5, it has been shown that they still face difficulties when tackling typologically-distant languages, particularly in the low-resource setting. One obstacle for effective cross-lingual transfer is... | Dmitry Nikolaev, Ofir Arviv, Omri Abend, Taelin Karidi |  |
| 186 |  |  [Novel Slot Detection With an Incremental Setting](https://doi.org/10.18653/v1/2023.findings-emnlp.53) |  | 0 | Current dialogue systems face diverse user requests and rapid change domains, making quickly adapt to scenarios with previous unseen slot types become a major challenge. Recently, researchers have introduced novel slot detection (NSD) to discover potential new types. However, dialogue system with... | Changhao Guan, Chen Liang, Hongliang Li, Jian Liu, Jinan Xu, Qingbin Liu, Zhe Zhao |  |
| 187 |  |  [Self-supervised Post-processing Method to Enrich Pretrained Word Vectors](https://doi.org/10.18653/v1/2023.findings-emnlp.54) |  | 0 | Retrofitting techniques, which inject external resources into word representations, have compensated for the weakness of distributed representations in semantic and relational knowledge between words. However, the previous methods require additional external resources and strongly depend on the... | Hwiyeol Jo |  |
| 188 |  |  [Automatic Model Selection with Large Language Models for Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.55) |  | 0 | Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two distinct reasoning methods, each with its own strengths. CoT employs natural language, offering flexibility and interpretability, while PAL utilizes programming language, yielding more structured and rigorous logic. We... | James Xu Zhao, Junxian He, Kenji Kawaguchi, Michael Qizhe Xie, Yuxi Xie |  |
| 189 |  |  [ARKitSceneRefer: Text-based Localization of Small Objects in Diverse Real-World 3D Indoor Scenes](https://doi.org/10.18653/v1/2023.findings-emnlp.56) |  | 0 | 3D referring expression comprehension is a task to ground text representations onto objects in 3D scenes. It is a crucial task for indoor household robots or augmented reality devices to localize objects referred to in user instructions. However, existing indoor 3D referring expression... | Chenhui Chu, Sadao Kurohashi, Shuhei Kurita, Shunya Kato |  |
| 190 |  |  [Improving Question Generation with Multi-level Content Planning](https://doi.org/10.18653/v1/2023.findings-emnlp.57) |  | 0 | This paper addresses the problem of generating questions from a given context and an answer, specifically focusing on questions that require multi-hop reasoning across an extended context. Previous studies have suggested that key phrase selection is essential for question generation (QG), yet it is... | Bowen Yu, CamTu Nguyen, Fei Huang, Haiyang Yu, Qi Gou, Yongbin Li, Zehua Xia |  |
| 191 |  |  [Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing](https://doi.org/10.18653/v1/2023.findings-emnlp.58) |  | 0 | The emergence of Large Language Models (LLMs), such as ChatGPT, has revolutionized general natural language preprocessing (NLP) tasks. However, their expertise in the financial domain lacks a comprehensive evaluation. To assess the ability of LLMs to solve financial NLP tasks, we present FinLMEval,... | Yi Yang, Yue Guo, Zian Xu |  |
| 192 |  |  [DelucionQA: Detecting Hallucinations in Domain-specific Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.59) |  | 0 | Hallucination is a well-known phenomenon in text generated by large language models (LLMs). The existence of hallucinatory responses is found in almost all application scenarios e.g., summarization, question-answering (QA) etc. For applications requiring high reliability (e.g., customer-facing... | Arsalan Gundroo, Bingqing Wang, Jun Araki, Lukas Lange, Md. Rizwan Parvez, Mobashir Sadat, Rakesh R. Menon, Zhe Feng, Zhengyu Zhou |  |
| 193 |  |  [InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution](https://doi.org/10.18653/v1/2023.findings-emnlp.60) |  | 0 | Over recent decades, significant advancements in cross-modal retrieval is mainly driven by breakthroughs in visual and linguistic modeling. However, a recent study shows that multi-modal data representations tend to cluster within a limited convex cone (as representation degeneration problem),... | Xiangru Jian, Yimu Wang |  |
| 194 |  |  [Dissecting In-Context Learning of Translations in GPT-3](https://doi.org/10.18653/v1/2023.findings-emnlp.61) |  | 0 | Most of the recent work in leveraging Large Language Models (LLMs) such as GPT-3 for Machine Translation (MT) has focused on selecting the few-shot samples for prompting. In this work, we try to better understand the role of demonstration attributes for the in-context learning of translations... | Arul Menezes, Hany Hassan Awadalla, Vikas Raunak |  |
| 195 |  |  [Social Commonsense-Guided Search Query Generation for Open-Domain Knowledge-Powered Conversations](https://doi.org/10.18653/v1/2023.findings-emnlp.62) |  | 0 | Open-domain dialog involves generating search queries that help obtain relevant knowledge for holding informative conversations. However, it can be challenging to determine what information to retrieve when the user is passive and does not express a clear need or request. To tackle this issue, we... | ChengXiang Zhai, Hao Bai, Heng Ji, Revanth Gangi Reddy, Sharath Chandra Etagi Suresh, Wentao Yao |  |
| 196 |  |  [MixTEA: Semi-supervised Entity Alignment with Mixture Teaching](https://doi.org/10.18653/v1/2023.findings-emnlp.63) |  | 0 | Semi-supervised entity alignment (EA) is a practical and challenging task because of the lack of adequate labeled mappings as training data. Most works address this problem by generating pseudo mappings for unlabeled entities. However, they either suffer from the erroneous (noisy) pseudo mappings... | Bin Zhou, Feng Xie, Lei Tian, Xiang Zeng, Xin Song, Xuechen Zhao, Yusong Tan |  |
| 197 |  |  [EZ-STANCE: A Large Dataset for Zero-Shot Stance Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.64) |  | 0 |  | Chenye Zhao, Cornelia Caragea |  |
| 198 |  |  [Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.65) |  | 0 | Neural ‘dense’ retrieval models are state of the art for many datasets, however these models often exhibit limited domain transfer ability. Existing approaches to adaptation are unwieldy, such as requiring explicit supervision, complex model architectures, or massive external models. We present... | Fan Jiang, Qiongkai Xu, Tom Drummond, Trevor Cohn |  |
| 199 |  |  [TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.66) |  | 0 | Large-scale video-language pre-training has made remarkable strides in advancing video-language understanding tasks. However, the heavy computational burden of video encoding remains a formidable efficiency bottleneck, particularly for long-form videos. These videos contain massive visual tokens... | Lu Hou, Shicheng Li, Shuhuai Ren, Sishuo Chen, Xu Sun |  |
| 200 |  |  [Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.67) |  | 0 | Answering time-sensitive questions from long documents requires temporal reasoning over the times in questions and documents. An important open question is whether large language models can perform such reasoning solely using a provided text document, or whether they can benefit from additional... | Nagib Hakim, Phillip Howard, Steven Bethard, Xin Su |  |
| 201 |  |  [The Internal State of an LLM Knows When It's Lying](https://doi.org/10.18653/v1/2023.findings-emnlp.68) |  | 0 | While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM’s internal state can be used to reveal the truthfulness... | Amos Azaria, Tom M. Mitchell |  |
| 202 |  |  [Factual Relation Discrimination for Factuality-oriented Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.69) |  | 0 | Most neural abstractive summarization models are capable of producing high-quality summaries. However, they still frequently contain factual errors. Existing factuality-oriented abstractive summarization models only consider the integration of factual information and ignore the causes of factual... | Feng Jiang, Peifeng Li, Qiaoming Zhu, Xiaomin Chu, Zhiguang Gao |  |
| 203 |  |  [Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment](https://doi.org/10.18653/v1/2023.findings-emnlp.70) |  | 0 | Multi-Modal Entity Alignment (MMEA) is a critical task that aims to identify equivalent entity pairs across multi-modal knowledge graphs (MMKGs). However, this task faces challenges due to the presence of different types of information, including neighboring entities, multi-modal attributes, and... | Cheng Ji, Jianxin Li, Lihong Wang, Qian Li, Shu Guo, Zhaoji Liang |  |
| 204 |  |  [Is a Prestigious Job the same as a Prestigious Country? A Case Study on Multilingual Sentence Embeddings and European Countries](https://doi.org/10.18653/v1/2023.findings-emnlp.71) |  | 0 | We study how multilingual sentence representations capture European countries and occupations and how this differs across European languages. We prompt the models with templated sentences that we machine-translate into 12 European languages and analyze the most prominent dimensions in the... | Jindrich Libovický |  |
| 205 |  |  [Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.72) |  | 0 | Large Language Models (LLMs) have generated considerable interest and debate regarding their potential emergence of Theory of Mind (ToM). Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones primarily focus on... | Jacob Sansom, Joyce Chai, Run Peng, Ziqiao Ma |  |
| 206 |  |  [Text Augmented Spatial Aware Zero-shot Referring Image Segmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.73) |  | 0 | In this paper, we study a challenging task of zero-shot referring image segmentation. This task aims to identify the instance mask that is most related to a referring expression without training on pixel-level annotations. Previous research takes advantage of pre-trained cross-modal models, e.g.,... | Linchao Zhu, Yi Yang, Yucheng Suo |  |
| 207 |  |  [IRFL: Image Recognition of Figurative Language](https://doi.org/10.18653/v1/2023.findings-emnlp.74) |  | 0 | Figures of speech such as metaphors, similes, and idioms are integral parts of human communication. They are ubiquitous in many forms of discourse, allowing people to convey complex, abstract ideas and evoke emotion. As figurative forms are often conveyed through multiple modalities (e.g., both... | Dafna Shahaf, Ron Yosef, Yonatan Bitton |  |
| 208 |  |  [Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization](https://doi.org/10.18653/v1/2023.findings-emnlp.75) |  | 0 | Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it... | Hongye Song, Jun Lin, Juncheng Li, Kaihang Pan, Siliang Tang, Xiaozhong Liu |  |
| 209 |  |  [An Adaptive Prompt Generation Framework for Task-oriented Dialogue System](https://doi.org/10.18653/v1/2023.findings-emnlp.76) |  | 0 | The de facto way of utilizing black-box large language models (LLMs) to perform various downstream tasks is prompting. However, obtaining suitable prompts for specific tasks is still a challenging problem. While existing LLM-based methods demonstrate promising performance in task-oriented dialogue... | Han Zhao, Huijia Wu, Jun Gao, Liuyu Xiang, Yiqi Tong, Zhaofeng He |  |
| 210 |  |  [Temporal Knowledge Graph Reasoning Based on N-tuple Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.77) |  | 0 | Reasoning over Temporal Knowledge Graphs (TKGs) that predicts temporal facts (e.g., events) in the future is crucial for many applications. The temporal facts in existing TKGs only contain their core entities (i.e., the entities playing core roles therein) and formulate them as quadruples, i.e.,... | Jiafeng Guo, Long Bai, Saiping Guan, Xiaolong Jin, Xueqi Cheng, Yutao Zeng, Zhongni Hou, Zixuan Li |  |
| 211 |  |  [Make Your Decision Convincing! A Unified Two-Stage Framework: Self-Attribution and Decision-Making](https://doi.org/10.18653/v1/2023.findings-emnlp.78) |  | 0 | Explaining black-box model behavior with natural language has achieved impressive results in various NLP tasks. Recent research has explored the utilization of subsequences from the input text as a rationale, providing users with evidence to support the model decision. Although existing frameworks... | Bing Qin, Haochun Wang, Muzhen Cai, Rui Bai, Sendong Zhao, Yanrui Du, Yuhan Chen, Zewen Qiang |  |
| 212 |  |  [Adaptive Structure Induction for Aspect-based Sentiment Analysis with Spectral Perspective](https://doi.org/10.18653/v1/2023.findings-emnlp.79) |  | 0 | Recently, incorporating structure information (e.g. dependency syntactic tree) can enhance the performance of aspect-based sentiment analysis (ABSA). However, this structure information is obtained from off-the-shelf parsers, which is often sub-optimal and cumbersome. Thus, automatically learning... | Hao Niu, Wenjing Yu, Xiaosu Wang, Yao Zhang, Yun Xiong, Zhonglei Guo |  |
| 213 |  |  [NovaCOMET: Open Commonsense Foundation Models with Symbolic Knowledge Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.80) |  | 0 | We present NovaCOMET, an open commonsense knowledge model, that combines the best aspects of knowledge and general task models. Compared to previous knowledge models, NovaCOMET allows open-format relations enabling direct application to reasoning tasks; compared to general task models like Flan-T5,... | Ashutosh Baheti, Bill Yuchen Lin, Chandra Bhagavatula, Jack Hessel, Khyathi Raghavi Chandu, Liwei Jiang, Peter West, Ronan Le Bras, Taylor Sorensen, Ximing Lu, Yejin Choi |  |
| 214 |  |  [In-Context Demonstration Selection with Cross Entropy Difference](https://doi.org/10.18653/v1/2023.findings-emnlp.81) |  | 0 | Large language models (LLMs) can use in-context demonstrations to improve performance on zero-shot tasks. However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples. We present a cross-entropy difference (CED) method... | Chenguang Zhu, Dan Iter, Reid Pryzant, Ruochen Xu, Shuohang Wang, Yang Liu, Yichong Xu |  |
| 215 |  |  [The Past, Present, and Future of Typological Databases in NLP](https://doi.org/10.18653/v1/2023.findings-emnlp.82) |  | 0 | Typological information has the potential to be beneficial in the development of NLP models, particularly for low-resource languages. Unfortunately, current large-scale typological databases, notably WALS and Grambank, are inconsistent both with each other and with other sources of typological... | Emi Baylor, Esther Ploeger, Johannes Bjerva |  |
| 216 |  |  [SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations](https://doi.org/10.18653/v1/2023.findings-emnlp.83) |  | 0 | Large language models (LLMs) have been widely applied in various fields due to their excellent capability for memorizing knowledge and chain of thought (CoT). When these language models are applied in the field of psychological counseling, they often rush to provide universal advice. However, when... | Huimin Zheng, Jingkai Lin, Qi Liu, Xiangmin Xu, Xiaofen Xing, Yirong Chen, Zhenyu Wang |  |
| 217 |  |  [Can ChatGPT Assess Human Personalities? A General Evaluation Framework](https://doi.org/10.18653/v1/2023.findings-emnlp.84) |  | 0 | Large Language Models (LLMs) especially ChatGPT have produced impressive results in various areas, but their potential human-like psychology is still largely unexplored. Existing works study the virtual personalities of LLMs but rarely explore the possibility of analyzing human personalities via... | Chunyan Miao, Cyril Leung, Haocong Rao |  |
| 218 |  |  [MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model](https://doi.org/10.18653/v1/2023.findings-emnlp.85) |  | 0 | Multi-modal open-domain question answering typically requires evidence retrieval from databases across diverse modalities, such as images, tables, passages, etc. Even Large Language Models (LLMs) like GPT-4 fall short in this task. To enable LLMs to tackle the task in a zero-shot manner, we... | Aishwarya Agrawal, Fengran Mo, JianYun Nie, Le Zhang, Yihong Wu |  |
| 219 |  |  [Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search](https://doi.org/10.18653/v1/2023.findings-emnlp.86) |  | 0 | Precisely understanding users’ contextual search intent has been an important challenge for conversational search. As conversational search sessions are much more diverse and long-tailed, existing methods trained on limited data still show unsatisfactory effectiveness and robustness to handle real... | Fengran Mo, Haonan Chen, Hongjin Qian, Jiewen Hou, Kelong Mao, Zhicheng Dou |  |
| 220 |  |  [DocAsRef: An Empirical Study on Repurposing Reference-based Summary Quality Metrics as Reference-free Metrics](https://doi.org/10.18653/v1/2023.findings-emnlp.87) |  | 0 | Automated summary quality assessment falls into two categories: reference-based and reference-free. Reference-based metrics, historically deemed more accurate due to the additional information provided by human-written references, are limited by their reliance on human input. In this paper, we... | Cen Chen, Forrest Sheng Bao, Ge Luo, Hebi Li, Minghui Qiu, Ruixuan Tu, Yinfei Yang, Youbiao He |  |
| 221 |  |  [Toxicity in chatgpt: Analyzing persona-assigned language models](https://doi.org/10.18653/v1/2023.findings-emnlp.88) |  | 0 | Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students... | Ameet Deshpande, Ashwin Kalyan, Karthik Narasimhan, Tanmay Rajpurohit, Vishvak Murahari |  |
| 222 |  |  [Execution-Based Evaluation for Open-Domain Code Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.89) |  | 0 | To extend the scope of coding queries to more realistic settings, we propose ODEX, the first Open-Domain EXecution-based natural language (NL) to Python code generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse libraries, along with 1,707 human-written test cases for execution. Our... | Daniel Fried, Graham Neubig, Shuyan Zhou, Zhiruo Wang |  |
| 223 |  |  [Syntax-Aware Retrieval Augmented Code Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.90) |  | 0 | Neural code generation models are nowadays widely adopted to generate code from natural language descriptions automatically. Recently, pre-trained neural models equipped with token-level retrieval capabilities have exhibited great potentials in neural machine translation. However, applying them... | Guang Yang, Taolue Chen, Xiangyu Zhang, Yu Zhou |  |
| 224 |  |  [Selecting Key Views for Zero-Shot Entity Linking](https://doi.org/10.18653/v1/2023.findings-emnlp.91) |  | 0 | Entity linking, which aligns mentions in the text to entities in knowledge bases, is essential for many natural language processing tasks. Considering the real-world scenarios, recent research hotspot of entity linking has focused on the zero-shot setting, where mentions need to link to unseen... | Baohang Zhou, Kehui Song, Wensheng Zhang, Xiaojie Yuan, Xuhui Sui, Ying Zhang |  |
| 225 |  |  [Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term](https://doi.org/10.18653/v1/2023.findings-emnlp.92) |  | 0 | With advancements in natural language processing (NLP) models, automatic explanation generation has been proposed to mitigate misinformation on social media platforms in addition to adding warning labels to identified fake news. While many researchers have focused on generating good explanations,... | Aiping Xiong, LunWei Ku, ShihChieh Dai, YiLi Hsu |  |
| 226 |  |  [Improving the Robustness of Summarization Models by Detecting and Removing Input Noise](https://doi.org/10.18653/v1/2023.findings-emnlp.93) |  | 0 | The evaluation of abstractive summarization models typically uses test data that is identically distributed as training data. In real-world practice, documents to be summarized may contain input noise caused by text extraction artifacts or data pipeline bugs. The robustness of model performance... | Balaji Lakshminarayanan, Jiaming Luo, Jie Ren, Kundan Krishna, Mohammad Saleh, Peter J. Liu, Yao Zhao |  |
| 227 |  |  [How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts](https://doi.org/10.18653/v1/2023.findings-emnlp.94) |  | 0 | In recent years, there has been a rapid proliferation of AI-generated text, primarily driven by the release of powerful pre-trained language models (PLMs). To address the issue of misuse associated with AI-generated text, various high-performing detectors have been developed, including the OpenAI... | Huan Liu, Joshua Garland, Paras Sheth, Raha Moraffah, Tharindu Kumarage |  |
| 228 |  |  [Knowledge is a Region in Weight Space for Fine-tuned Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.95) |  | 0 | Research on neural networks has focused on understanding a single model trained on a single dataset. However, relatively little is known about the relationships between different models, particularly those trained or tested on different datasets. We address this by studying how the weight space and... | Almog Gueta, Colin Raffel, Elad Venezian, Leshem Choshen, Noam Slonim, Yoav Katz |  |
| 229 |  |  [Unveiling the Multi-Annotation Process: Examining the Influence of Annotation Quantity and Instance Difficulty on Model Performance](https://doi.org/10.18653/v1/2023.findings-emnlp.96) |  | 0 | The NLP community has long advocated for the construction of multi-annotator datasets to better capture the nuances of language interpretation, subjectivity, and ambiguity. This paper conducts a retrospective study to show how performance scores can vary when a dataset expands from a single... | Mayank Singh, Pritam Kadasi |  |
| 230 |  |  [On the Risk of Misinformation Pollution with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.97) |  | 0 | We investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential... | Liangming Pan, MinYen Kan, Preslav Nakov, Wenhu Chen, William Yang Wang, Yikang Pan |  |
| 231 |  |  [Dolphin: A Challenging and Diverse Benchmark for Arabic NLG](https://doi.org/10.18653/v1/2023.findings-emnlp.98) |  | 0 | We present Dolphin, a novel benchmark that addresses the need for a natural language generation (NLG) evaluation framework dedicated to the wide collection of Arabic languages and varieties. The proposed benchmark encompasses a broad range of 13 different NLG tasks, including dialogue generation,... | AbdelRahim A. Elmadany, Ahmed Oumar ElShangiti, El Moatez Billah Nagoudi, Muhammad AbdulMageed |  |
| 232 |  |  [Hierarchical Enhancement Framework for Aspect-based Argument Mining](https://doi.org/10.18653/v1/2023.findings-emnlp.99) |  | 0 | Aspect-Based Argument Mining (ABAM) is a critical task in computational argumentation. Existing methods have primarily treated ABAM as a nested named entity recognition problem, overlooking the need for tailored strategies to effectively address the specific challenges of ABAM tasks. To this end,... | Deyu Li, Jian Liao, Jianxing Zheng, Suge Wang, Xiaoli Li, Yang Li, Yujie Fu |  |
| 233 |  |  [MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.100) |  | 0 | Large language models (LLMs) have shown nearly saturated performance on many natural language processing (NLP) tasks. As a result, it is natural for people to believe that LLMs have also mastered abilities such as time understanding and reasoning. However, research on the temporal sensitivity of... | Fangyu Lei, Huanhuan Ma, Jun Zhao, Kang Liu, Xiaoyan Yu, Yifan Wei, Yisong Su, Yuanzhe Zhang |  |
| 234 |  |  [What Makes Chain-of-Thought Prompting Effective? A Counterfactual Study](https://doi.org/10.18653/v1/2023.findings-emnlp.101) |  | 0 | The effectiveness of Chain-of-thought prompting (CoT) has been widely recognized, but the underlying mechanisms behind its success, the reason why it just works for a wide range of tasks, remains an open question. To investigate this, we employ a counterfactual prompting approach, systematically... | Aman Madaan, Amir Yazdanbakhsh, Katherine Hermann |  |
| 235 |  |  [Perceptual Structure in the absence of grounding: the impact of abstractedness and subjectivity in color language for LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.102) |  | 0 | The need for grounding in language understanding is an active research topic. Previous work has suggested that color perception and color language appear as a suitable test bed to empirically study the problem, given its cognitive significance and showing that there is considerable alignment... | Andrés Hoyos Idrobo, Edison MarreseTaylor, Pablo Loyola |  |
| 236 |  |  [A Dataset for Investigating the Impact of Context for Offensive Language Detection in Tweets](https://doi.org/10.18653/v1/2023.findings-emnlp.103) |  | 0 | Offensive language detection is crucial in natural language processing (NLP). We investigated the importance of context for detecting such language in reply tweets on Twitter, where the use of offensive language is widespread. We collected a Turkish tweet dataset where the target group was... | Arzucan Özgür, Musa Ihtiyar, Mustafa Erengül, Ömer Özdemir |  |
| 237 |  |  [Remember what you did so you know what to do next](https://doi.org/10.18653/v1/2023.findings-emnlp.104) |  | 0 | We explore using the 6B parameter GPT-J language model to create a plan for a simulated robot to achieve 30 classes of goals in ScienceWorld, a text game simulator for elementary science experiments and for which previously published empirical work has shown large language models (LLM)s to be a... | Alex Hedges, Justin Martin, Manuel R. Ciosici, Marjorie Freedman, Ralph M. Weischedel, Yash Kankanampati |  |
| 238 |  |  [An Empirical Study of Multimodal Model Merging](https://doi.org/10.18653/v1/2023.findings-emnlp.105) |  | 0 | Model merging (e.g., via interpolation or task arithmetic) fuses multiple models trained on different tasks to generate a multi-task solution. The technique has been proven successful in previous studies, where the models are trained on similar tasks and with the same initialization. In this paper,... | Kevin Lin, Lijuan Wang, Linjie Li, Mohit Bansal, YiLin Sung, Zhe Gan |  |
| 239 |  |  [Learning to Abstract with Nonparametric Variational Information Bottleneck](https://doi.org/10.18653/v1/2023.findings-emnlp.106) |  | 0 | Learned representations at the level of characters, sub-words, words, and sentences, have each contributed to advances in understanding different NLP tasks and linguistic phenomena. However, learning textual embeddings is costly as they are tokenization specific and require different models to be... | Fabio Fehr, James Henderson, Melika Behjati |  |
| 240 |  |  [Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document](https://doi.org/10.18653/v1/2023.findings-emnlp.107) |  | 0 | Visual Relation Extraction (VRE) is a powerful means of discovering relationships between entities within visually-rich documents. Existing methods often focus on manipulating entity features to find pairwise relations, yet neglect the more fundamental structural information that links disparate... | Duo Dong, Jun Lin, Juncheng Li, Qian Xiao, Siliang Tang, Xiangnan Chen, Xiaozhong Liu |  |
| 241 |  |  [Learning to Compose Representations of Different Encoder Layers towards Improving Compositional Generalization](https://doi.org/10.18653/v1/2023.findings-emnlp.108) |  | 0 | Recent studies have shown that sequence-to-sequence (seq2seq) models struggle with compositional generalization (CG), i.e., the ability to systematically generalize to unseen compositions of seen components. There is mounting evidence that one of the reasons hindering CG is the representation of... | Biao Fu, Lei Lin, Shan Liu, Shuangtao Li, Xiaodong Shi, Yafang Zheng, Yidong Chen |  |
| 242 |  |  [SelectNoise: Unsupervised Noise Injection to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.109) |  | 0 | In this work, we focus on the task of machine translation (MT) from extremely low-resource language (ELRLs) to English. The unavailability of parallel data, lack of representation from large multilingual pre-trained models, and limited monolingual data hinder the development of MT systems for... | Kaushal Maurya, Maharaj Brahma, Maunendra Sankar Desarkar |  |
| 243 |  |  [Breaking Boundaries in Retrieval Systems: Unsupervised Domain Adaptation with Denoise-Finetuning](https://doi.org/10.18653/v1/2023.findings-emnlp.110) |  | 0 | Dense retrieval models have exhibited remarkable effectiveness, but they rely on abundant labeled data and face challenges when applied to different domains. Previous domain adaptation methods have employed generative models to generate pseudo queries, creating pseudo datasets to enhance the... | Che Chen, Ching Yang, ChunYi Lin, HungYu Kao |  |
| 244 |  |  [Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach](https://doi.org/10.18653/v1/2023.findings-emnlp.111) |  | 0 | Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence. Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains. However,... | Jifan Yu, Juanzi Li, Lei Hou, Zheyuan Zhang |  |
| 245 |  |  [Simpler neural networks prefer subregular languages](https://doi.org/10.18653/v1/2023.findings-emnlp.112) |  | 0 | We apply a continuous relaxation of L0 regularization (Louizos et al., 2017), which induces sparsity, to study the inductive biases of LSTMs. In particular, we are interested in the patterns of formal languages which are readily learned and expressed by LSTMs. Across a wide range of tests we find... | Charles Torres, Richard Futrell |  |
| 246 |  |  [Simple Hardware-Efficient PCFGs with Independent Left and Right Productions](https://doi.org/10.18653/v1/2023.findings-emnlp.113) |  | 0 | Scaling dense PCFGs to thousands of nonterminals via low-rank parameterizations of the rule probability tensor has been shown to be beneficial for unsupervised parsing. However, PCFGs scaled this way still perform poorly as a language model, and even underperform similarly-sized HMMs. This work... | Kewei Tu, Songlin Yang, Wei Liu, Yoon Kim |  |
| 247 |  |  [R³ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context](https://doi.org/10.18653/v1/2023.findings-emnlp.114) |  | 0 | With the help of Chain-of-Thought (CoT) prompting, Large Language Models (LLMs) have achieved remarkable performance on various reasoning tasks. However, most of them have been evaluated under noise-free context and the dilemma for LLMs to produce inaccurate results under the noisy context has not... | Hanlun Zhu, Lei Wang, Qingyuan Tian, Yang Li, Yunshi Lan |  |
| 248 |  |  [Quality Estimation-Assisted Automatic Post-Editing](https://doi.org/10.18653/v1/2023.findings-emnlp.115) |  | 0 | Automatic Post-Editing (APE) systems are prone to over-correction of the Machine Translation (MT) outputs. While Word-level Quality Estimation (QE) system can provide a way to curtail the over-correction, a significant performance gain has not been observed thus far by utilizing existing APE and QE... | Diptesh Kanojia, Frédéric Blain, Pushpak Bhattacharyya, Sourabh Dattatray Deoghare, Tharindu Ranasinghe |  |
| 249 |  |  [Adapter Pruning using Tropical Characterization](https://doi.org/10.18653/v1/2023.findings-emnlp.116) |  | 0 | Adapters are widely popular parameter-efficient transfer learning approaches in natural language processing that insert trainable modules in between layers of a pre-trained language model. Apart from several heuristics, however, there has been a lack of studies analyzing the optimal number of... | Rishabh Bhardwaj, Soujanya Poria, Tushar Vaidya |  |
| 250 |  |  [Self-Supervised Rule Learning to Link Text Segments to Relational Elements of Structured Knowledge](https://doi.org/10.18653/v1/2023.findings-emnlp.117) |  | 0 | We present a neuro-symbolic approach to self-learn rules that serve as interpretable knowledge to perform relation linking in knowledge base question answering systems. These rules define natural language text predicates as a weighted mixture of knowledge base paths. The weights learned during... | Achille Fokoue, Alexander Gray, Chitra Subramanian, Dheeraj Sreedhar, Hima Karanam, Ibrahim Abdelaziz, Kyle Erwin, Maxwell Crouse, Naweed Khan, Ndivhuwo Makondo, Pavan Kapanipathi, Ronny Luss, Shajith Ikbal, Subhajit Chaudhury, Sumit Neelam, Udit Sharma |  |
| 251 |  |  [TaTA: A Multilingual Table-to-Text Dataset for African Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.118) |  | 0 | Existing data-to-text generation datasets are mostly limited to English. To address this lack of data, we create Table-to-Text in African languages (TaTA), the first large multilingual table-to-text dataset with a focus on African languages. We created TaTA by transcribing figures and accompanying... | Ankur P. Parikh, Clara Rivera, Jan A. Botha, Michael Chavinda, Sebastian Gehrmann, Sebastian Ruder, Vitaly Nikolaev |  |
| 252 |  |  [Explain-then-translate: an analysis on improving program translation with self-generated explanations](https://doi.org/10.18653/v1/2023.findings-emnlp.119) |  | 0 | This work explores the use of self-generated natural language explanations as an intermediate step for code-to-code translation with language models. Across three types of explanations and 19 programming languages constructed from the MultiPL-E dataset, we find the explanations to be particularly... | Alexander Shypula, Bailin Wang, Derry Wijaya, Jie Chen, Mayank Agarwal, Yoon Kim, Zilu Tang |  |
| 253 |  |  [Can Brain Signals Reveal Inner Alignment with Human Languages?](https://doi.org/10.18653/v1/2023.findings-emnlp.120) |  | 0 | Brain Signals, such as Electroencephalography (EEG), and human languages have been widely explored independently for many downstream tasks, however, the connection between them has not been well explored. In this study, we explore the relationship and dependency between EEG and language. To study... | Bo Li, Ding Zhao, Douglas Weber, Jiacheng Zhu, Jielin Qiu, Mengdi Xu, William Han |  |
| 254 |  |  [DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource Event Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.121) |  | 0 | Most current Event Extraction (EE) methods focus on the high-resource scenario, which requires a large amount of annotated data and can hardly be applied to low-resource domains. To address EE more effectively with limited resources, we propose the Demonstration-enhanced Schema-guided Generation... | Gang Zhao, Guanting Dong, Shudong Lu, Si Li, Xiaocheng Gong, Xinjie Yang |  |
| 255 |  |  [GLGR: Question-aware Global-to-Local Graph Reasoning for Multi-party Dialogue Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.122) |  | 0 | Graph reasoning contributes to the integration of discretely-distributed attentive information (clues) for Multi-party Dialogue Reading Comprehension (MDRC). This is attributed primarily to multi-hop reasoning over global conversational structures. However, existing approaches barely apply... | Ai Ti Aw, Bowei Zou, Xibo Li, Yanling Li, Yifan Fan, Yu Hong |  |
| 256 |  |  [Towards Mitigating LLM Hallucination via Self Reflection](https://doi.org/10.18653/v1/2023.findings-emnlp.123) |  | 0 | Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of “hallucination”, where models generate plausible-sounding but unfaithful or... | Etsuko Ishii, Nayeon Lee, Pascale Fung, Tiezheng Yu, Yan Xu, Ziwei Ji |  |
| 257 |  |  [Making Body Movement in Sign Language Corpus Accessible for Linguists and Machines with Three-Dimensional Normalization of MediaPipe](https://doi.org/10.18653/v1/2023.findings-emnlp.124) |  | 0 | Linguists can access movement in the sign language video corpus through manual annotation or computational methods. The first relies on a predefinition of features, and the second requires technical knowledge. Methods like MediaPipe and OpenPose are now more often used in sign language processing.... | Mayumi Bono, Victor Skobov |  |
| 258 |  |  [XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.125) |  | 0 | Data scarcity is a crucial issue for the development of highly multilingual NLP systems. Yet for many under-represented languages (ULs) — languages for which NLP research is particularly far behind in meeting user needs — it is feasible to annotate small amounts of data. Motivated by this, we... | Alexander Gutkin, Anna Katanova, Bidisha Samanta, Brian Roark, Christo Kirov, Colin Cherry, Connie Tao, Dan Garrette, Dana L. Dickinson, David Ifeoluwa Adelani, Dmitry Panteleev, Isaac Caswell, Jean Michel A. Sarr, John Wieting, Jonathan H. Clark, Massimo Nicosia, Melvin Johnson, Mihir Kale, Min Ma, Nitish Gupta, Parker Riley, Partha Talukdar, R. Reeve Ingle, Sebastian Ruder, Shruti Rijhwani, Vera Axelrod, Xinyi Wang |  |
| 259 |  |  [DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models](https://doi.org/10.18653/v1/2023.findings-emnlp.126) |  | 0 | Recent advances in image and video creation, especially AI-based image synthesis, have led to the production of numerous visual scenes that exhibit a high level of abstractness and diversity. Consequently, Visual Storytelling (VST), a task that involves generating meaningful and coherent narratives... | Mei Yuan, Qi Su, Shengguang Wu |  |
| 260 |  |  [DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias](https://doi.org/10.18653/v1/2023.findings-emnlp.127) |  | 0 | Numerous debiasing techniques have been proposed to mitigate the gender bias that is prevalent in pretrained language models. These are often evaluated on datasets that check the extent to which the model is gender-neutral in its predictions. Importantly, this evaluation protocol overlooks the... | Kaveh Eskandari Miandoab, Mahdi Zakizadeh, Mohammad Taher Pilehvar |  |
| 261 |  |  [Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens](https://doi.org/10.18653/v1/2023.findings-emnlp.128) |  | 0 | Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and... | ByungDoh Oh, William Schuler |  |
| 262 |  |  [ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination](https://doi.org/10.18653/v1/2023.findings-emnlp.129) |  | 0 | In the field of Large Language Models (LLMs), researchers are increasingly exploring their effectiveness across a wide range of tasks. However, a critical area that requires further investigation is the interpretability of these models, particularly the ability to generate rational explanations for... | Baotian Hu, Dongfang Li, Jindi Yu, Min Zhang, Zhenran Xu |  |
| 263 |  |  [CLASS: A Design Framework for Building Intelligent Tutoring Systems Based on Learning Science principles](https://doi.org/10.18653/v1/2023.findings-emnlp.130) |  | 0 | We present a design framework called Conversational Learning with Analytical Step-by-Step Strategies (CLASS) for building advanced Intelligent Tutoring Systems (ITS) powered by high-performance Large Language Models (LLMs). The CLASS framework empowers ITS with two key capabilities. First, through... | Debshila Basu Mallick, Naiming Liu, Richard G. Baraniuk, Shashank Sonkar |  |
| 264 |  |  [Normal-Abnormal Decoupling Memory for Medical Report Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.131) |  | 0 | The automatic generation of medical reports plays a crucial role in clinical automation. In contrast to natural images, radiological images exhibit a high degree of similarity, while medical data are prone to data bias and complex noise, posing challenges for existing methods in capturing nuanced... | Guosheng Zhao, Yan Yan, Zijian Zhao |  |
| 265 |  |  [mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations](https://doi.org/10.18653/v1/2023.findings-emnlp.132) |  | 0 | Multilingual sequence-to-sequence models perform poorly with increased language coverage and fail to consistently generate text in the correct target language in few-shot settings. To address these challenges, we propose mmT5, a modular multilingual sequence-to-sequence model. mmT5 utilizes... | Francesco Piccinno, Jonas Pfeiffer, Machel Reid, Massimo Nicosia, Sebastian Ruder, Xinyi Wang |  |
| 266 |  |  [ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories](https://doi.org/10.18653/v1/2023.findings-emnlp.133) |  | 0 | Recently, Large Language Models (LLMs) have been serving as general-purpose interfaces, posing a significant demand for comprehensive visual knowledge. However, it remains unclear how well current LLMs and their visually augmented counterparts (VaLMs) can master visual commonsense knowledge. To... | Heming Xia, Jingjing Xu, Lei Li, Qingxiu Dong, Tianyu Liu, Zhifang Sui, Ziwei Qin |  |
| 267 |  |  [MultiCoNER v2: a Large Multilingual dataset for Fine-grained and Noisy Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.134) |  | 0 | We present MULTICONER V2, a dataset for fine-grained Named Entity Recognition covering 33 entity classes across 12 languages, in both monolingual and multilingual settings. This dataset aims to tackle the following practical challenges in NER: (i) effective handling of fine-grained classes that... | Besnik Fetahu, Oleg Rokhlenko, Shervin Malmasi, Sudipta Kar, Zhiyu Chen |  |
| 268 |  |  [A Query-Parallel Machine Reading Comprehension Framework for Low-resource NER](https://doi.org/10.18653/v1/2023.findings-emnlp.135) |  | 0 | Named entity recognition (NER) is a fundamental task in natural language processing. Recently, NER has been formulated as a machine reading comprehension (MRC) task, in which manually-crafted queries are used to extract entities of different types. However, current MRC-based NER techniques are... | Yongliang Wang, Yuhao Zhang |  |
| 269 |  |  [BiSPN: Generating Entity Set and Relation Set Coherently in One Pass](https://doi.org/10.18653/v1/2023.findings-emnlp.136) |  | 0 | By modeling the interaction among instances and avoiding error propagation, Set Prediction Networks (SPNs) achieve state-of-the-art performance on the tasks of named entity recognition and relation triple extraction respectively. However, how to jointly extract entities and relation triples via... | Buzhou Tang, Yuxin He |  |
| 270 |  |  [MEEP: Is this Engaging? Prompting Large Language Models for Dialogue Evaluation in Multilingual Settings](https://doi.org/10.18653/v1/2023.findings-emnlp.137) |  | 0 | As dialogue systems become more popular, evaluation of their response quality gains importance. Engagingness highly correlates with overall quality and creates a sense of connection that gives human participants a more fulfilling experience. Although qualities like coherence and fluency are readily... | Amber Shore, Ameeta Agrawal, Amila Ferron, Ekata Mitra |  |
| 271 |  |  [Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.138) |  | 0 | Over the past few years, various domain-specific pretrained language models (PLMs) have been proposed and have outperformed general-domain PLMs in specialized areas such as biomedical, scientific, and clinical domains. In addition, financial PLMs have been studied because of the high economic... | Jaeyoung Choe, Keonwoong Noh, Nayeon Kim, Seyun Ahn, Woohwan Jung |  |
| 272 |  |  [LLMDet: A Third Party Large Language Models Generated Text Detection Tool](https://doi.org/10.18653/v1/2023.findings-emnlp.139) |  | 0 | Generated texts from large language models (LLMs) are remarkably close to high-quality human-authored text, raising concerns about their potential misuse in spreading false information and academic misconduct. Consequently, there is an urgent need for a highly practical detection tool capable of... | Huawei Shen, Kangxi Wu, Liang Pang, TatSeng Chua, Xueqi Cheng |  |
| 273 |  |  [RECAP: Towards Precise Radiology Report Generation via Dynamic Disease Progression Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.140) |  | 0 | Automating radiology report generation can significantly alleviate radiologists’ workloads. Previous research has primarily focused on realizing highly concise observations while neglecting the precise attributes that determine the severity of diseases (e.g., small pleural effusion). Since... | Jiang Liu, Kaishuai Xu, Wenjie Li, Wenjun Hou, Yi Cheng |  |
| 274 |  |  [Causal Intervention for Abstractive Related Work Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.141) |  | 0 | Abstractive related work generation has attracted increasing attention in generating coherent related work that helps readers grasp the current research. However, most existing models ignore the inherent causality during related work generation, leading to spurious correlations which downgrade the... | Chongyang Shi, Ivor W. Tsang, Jiachang Liu, Liang Hu, Qi Zhang, Shoujin Wang, Usman Naseem |  |
| 275 |  |  [G-SPEED: General SParse Efficient Editing MoDel](https://doi.org/10.18653/v1/2023.findings-emnlp.142) |  | 0 | Large Language Models (LLMs) have demonstrated incredible capabilities in understanding, generating, and manipulating languages. Through human-model interactions, LLMs can automatically understand human-issued instructions and output the expected contents, which can significantly increase working... | Haoke Zhang, Juntao Li, Min Zhang, Xiabing Zhou, Yue Wang |  |
| 276 |  |  [Attack Prompt Generation for Red Teaming and Defending Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.143) |  | 0 | Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose... | Boyi Deng, Fuli Feng, Qifan Wang, Wenjie Wang, Xiangnan He, Yang Deng |  |
| 277 |  |  [Smart "Chef": Verifying the Effect of Role-based Paraphrasing for Aspect Term Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.144) |  | 0 | We tackle Aspect Term Extraction (ATE), a task of automatically extracting aspect terms from sentences. The current Pretrained Language Model (PLM) based extractors have achieved significant improvements. They primarily benefit from context-aware encoding. However, a considerable number of... | Jianmin Yao, Jiaxiang Chen, Qingting Xu, Yu Hong |  |
| 278 |  |  [Multi-Defendant Legal Judgment Prediction via Hierarchical Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.145) |  | 0 | Multiple defendants in a criminal fact description generally exhibit complex interactions, and cannot be well handled by existing Legal Judgment Prediction (LJP) methods which focus on predicting judgment results (e.g., law articles, charges, and terms of penalty) for single-defendant cases. To... | Fang Wang, Jitai Hao, Kai Zhao, Pengjie Ren, Shen Gao, Yougang Lyu, Zhaochun Ren, Zhumin Chen, Zihan Wang |  |
| 279 |  |  [Interpreting Indirect Answers to Yes-No Questions in Multiple Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.146) |  | 0 | Yes-no questions expect a yes or no for an answer, but people often skip polar keywords. Instead, they answer with long explanations that must be interpreted. In this paper, we focus on this challenging problem and release new benchmarks in eight languages. We present a distant supervision approach... | Eduardo Blanco, Jacob Quintero, Kadir Bulut Özler, Keun Hee Park, Md Mosharaf Hossain, Md Nayem Uddin, MohammadHossein Rezaei, Shivam Mathur, Shreya Nupur Shakya, Terry Cruz Melo, Zijie Wang |  |
| 280 |  |  [Generalizing Few-Shot Named Entity Recognizers to Unseen Domains with Type-Related Features](https://doi.org/10.18653/v1/2023.findings-emnlp.147) |  | 0 | Few-shot named entity recognition (NER) has shown remarkable progress in identifying entities in low-resource domains. However, few-shot NER methods still struggle with out-of-domain (OOD) examples due to their reliance on manual labeling for the target domain. To address this limitation, recent... | Maarten de Rijke, Pengjie Ren, Zhaochun Ren, Zhumin Chen, Zihan Wang, Ziqi Zhao |  |
| 281 |  |  [Intervention-Based Alignment of Code Search with Execution Feedback](https://doi.org/10.18653/v1/2023.findings-emnlp.148) |  | 0 | One of the fundamental goals in code search is to retrieve a functionally correct code for a given natural language query. As annotating for correctness requires executing test cases (i.e. obtaining execution feedback), existing code search training datasets approximate text-code co-occurrences as... | Hojae Han, Minsoo Kim, Nan Duan, Seungwon Hwang, Shuai Lu |  |
| 282 |  |  [Enhancing Neural Machine Translation with Semantic Units](https://doi.org/10.18653/v1/2023.findings-emnlp.149) |  | 0 | Conventional neural machine translation (NMT) models typically use subwords and words as the basic units for model input and comprehension. However, complete words and phrases composed of several tokens are often the fundamental units for expressing semantics, referred to as semantic units. To... | Langlin Huang, Shuhao Gu, Yang Feng, Zhuocheng Zhang |  |
| 283 |  |  [DRAFT: Dense Retrieval Augmented Few-shot Topic classifier Framework](https://doi.org/10.18653/v1/2023.findings-emnlp.150) |  | 0 | With the growing volume of diverse information, the demand for classifying arbitrary topics has become increasingly critical. To address this challenge, we introduce DRAFT, a simple framework designed to train a classifier for few-shot topic classification. DRAFT uses a few examples of a specific... | Keonwoo Kim, Younggun Lee |  |
| 284 |  |  [A Framework for Exploring Player Perceptions of LLM-Generated Dialogue in Commercial Video Games](https://doi.org/10.18653/v1/2023.findings-emnlp.151) |  | 0 | The growing capabilities of large language models (LLMs) have inspired recent efforts to integrate LLM-generated dialogue into video games. However, evaluation remains a major challenge: how do we assess the player experience in a commercial game augmented with LLM-generated dialogue? To explore... | Mohit Iyyer, Nader Akoury, Qian Yang |  |
| 285 |  |  [Generative Calibration for In-context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.152) |  | 0 | As one of the most exciting features of large language models (LLMs), in-context learning is a mixed blessing. While it allows users to fast-prototype a task solver with only a few training examples, the performance is generally sensitive to various configurations of the prompt such as the choice... | Cao Liu, Jun Zhao, Kang Liu, Yuanzhe Zhang, Zhongtao Jiang |  |
| 286 |  |  [Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.153) |  | 0 | Few-shot relation extraction involves identifying the type of relationship between two specific entities within a text, using a limited number of annotated samples. A variety of solutions to this problem have emerged by applying meta-learning and neural graph techniques which typically necessitate... | Jing Li, Min Zhang, Xilai Ma |  |
| 287 |  |  [AdaTranS: Adapting with Boundary-based Shrinking for End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.154) |  | 0 | To alleviate the data scarcity problem in End-to-end speech translation (ST), pre-training on data for speech recognition and machine translation is considered as an important technique. However, the modality gap between speech and text prevents the ST model from efficiently inheriting knowledge... | Liangyou Li, Qun Liu, Xingshan Zeng |  |
| 288 |  |  [No offence, Bert - I insult only humans! Multilingual sentence-level attack on toxicity detection networks](https://doi.org/10.18653/v1/2023.findings-emnlp.155) |  | 0 | We introduce a simple yet efficient sentence-level attack on black-box toxicity detector models. By adding several positive words or sentences to the end of a hateful message, we are able to change the prediction of a neural network and pass the toxicity detection system check. This approach is... | Noël Crespi, Reza Farahbakhsh, Sergey Berezin |  |
| 289 |  |  [Manipulating the Perceived Personality Traits of Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.156) |  | 0 | Psychology research has long explored aspects of human personality like extroversion, agreeableness and emotional stability, three of the personality traits that make up the ‘Big Five’. Categorizations like the ‘Big Five’ are commonly used to assess and diagnose personality types. In this work, we... | Graham Caron, Shashank Srivastava |  |
| 290 |  |  [WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia](https://doi.org/10.18653/v1/2023.findings-emnlp.157) |  | 0 | This paper presents the first few-shot LLM-based chatbot that almost never hallucinates and has high conversationality and low latency. WikiChat is grounded on the English Wikipedia, the largest curated free-text corpus. WikiChat generates a response from an LLM, retains only the grounded facts,... | Heidi C. Zhang, Monica S. Lam, Sina J. Semnani, Violet Z. Yao |  |
| 291 |  |  [Automated Few-Shot Classification with Instruction-Finetuned Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.158) |  | 0 | A particularly successful class of approaches for few-shot learning combines language models with prompts - hand-crafted task descriptions that complement data samples. However, designing prompts by hand for each task commonly requires domain knowledge and substantial guesswork. We observe, in the... | Andrew Gordon Wilson, Aston Zhang, Kaixiang Lin, Rami Aly, Xingjian Shi |  |
| 292 |  |  [Meta-Learning of Prompt Generation for Lightweight Prompt Engineering on Language-Model-as-a-Service](https://doi.org/10.18653/v1/2023.findings-emnlp.159) |  | 0 | Recently, many companies have been providing the capabilities of large language models as services. These Language-Model-as-a-Service (LMaaS) offerings support a variety of user tasks through in-context learning from prompts, which include instructions and demonstrations of the task. However, for... | ByungGon Chun, Hyeonmin Ha, Jihye Lee, Wookje Han |  |
| 293 |  |  [Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction](https://doi.org/10.18653/v1/2023.findings-emnlp.160) |  | 0 | The vital role of analogical reasoning in human cognition allows us to grasp novel concepts by linking them with familiar ones through shared relational structures. Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often... | Deqing Yang, Jiangjie Chen, Siyu Yuan, Xuyang Ge, Yanghua Xiao |  |
| 294 |  |  [HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings](https://doi.org/10.18653/v1/2023.findings-emnlp.161) |  | 0 | In this paper, we propose a hierarchical contrastive learning framework, HiCL, which considers local segment-level and global sequence-level relationships to improve training efficiency and effectiveness. Traditional methods typically encode a sequence in its entirety for contrast with others,... | Chaowei Xiao, V. G. Vinod Vydiswaran, Zhuofeng Wu |  |
| 295 |  |  [Density-Aware Prototypical Network for Few-Shot Relation Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.162) |  | 0 | In recent years, few-shot relation classification has evoked many research interests. Yet a more challenging problem, i.e. none-of-the-above (NOTA), is under-explored. Existing works mainly regard NOTA as an extra class and treat it the same as known relations. However, such a solution ignores the... | Bingzhe Wu, Jianfeng Wu, Mengting Hu, Mingming Liu, Renhong Cheng, Yalan Xie, Yike Wu |  |
| 296 |  |  [Improved Training of Deep Text Clustering](https://doi.org/10.18653/v1/2023.findings-emnlp.163) |  | 0 | The classical deep clustering optimization methods basically leverage information such as clustering centers, mutual information, and distance metrics to construct implicit generalized labels to establish information feedback (weak supervision) and thus optimize the deep model. However, the... | Wenpeng Hu, Yushan Tan, Zhunchen Luo, Zonghao Yang |  |
| 297 |  |  [RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.164) |  | 0 | Retrieval-augmented language models show promise in addressing issues like outdated information and hallucinations in language models (LMs). However, current research faces two main problems: 1) determining what information to retrieve, and 2) effectively combining retrieved information during... | Huawei Shen, Jingcheng Deng, Liang Pang, Xueqi Cheng |  |
| 298 |  |  [RefGPT: Dialogue Generation of GPT, by GPT, and for GPT](https://doi.org/10.18653/v1/2023.findings-emnlp.165) |  | 0 | Large Language Models (LLMs) have attained the impressive capability to resolve a wide range of NLP tasks by fine-tuning high-quality instruction data. However, collecting human-written data of high quality, especially multi-turn dialogues, is expensive and unattainable for most people. Though... | Dongjie Yang, Hai Zhao, Ruifeng Yuan, Shusen Wang, Yifei Yang, Yuantao Fan, Zili Wang |  |
| 299 |  |  [INA: An Integrative Approach for Enhancing Negotiation Strategies with Reward-Based Dialogue Agent](https://doi.org/10.18653/v1/2023.findings-emnlp.166) |  | 0 | In this paper, we propose a novel negotiation agent designed for the online marketplace. Our dialogue agent is integrative in nature i.e, it possesses the capability to negotiate on price as well as other factors, such as the addition or removal of items from a deal bundle, thereby offering a more... | Anutosh Maitra, Asif Ekbal, Roshni R. Ramnani, Suman Saurabh, Vaishakh Sreekanth Menon, Zishan Ahmad |  |
| 300 |  |  [Large Language Models are Better Reasoners with Self-Verification](https://doi.org/10.18653/v1/2023.findings-emnlp.167) |  | 0 | Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and... | Bin Li, Bin Sun, Fei Xia, Jun Zhao, Kang Liu, Minjun Zhu, Shengping Liu, Shizhu He, Yixuan Weng |  |
| 301 |  |  [Multi-Granularity Information Interaction Framework for Incomplete Utterance Rewriting](https://doi.org/10.18653/v1/2023.findings-emnlp.168) |  | 0 | Recent approaches in Incomplete Utterance Rewriting (IUR) fail to capture the source of important words, which is crucial to edit the incomplete utterance, and introduce words from irrelevant utterances. We propose a novel and effective multi-task information interaction framework including context... | Chen Li, Dinghao Zhang, Dongyan Zhao, Haowei Du, Yang Li |  |
| 302 |  |  [Accuracy is not enough: Evaluating Personalization in Summarizers](https://doi.org/10.18653/v1/2023.findings-emnlp.169) |  | 0 | Text summarization models are evaluated in terms of their accuracy and quality using various measures such as ROUGE, BLEU, METEOR, BERTScore, PYRAMID, readability, and several other recently proposed ones. The central objective of all accuracy measures is to evaluate the model’s ability to capture... | Darsh Rank, Rahul Vansh, Sourish Dasgupta, Tanmoy Chakraborty |  |
| 303 |  |  [For Generated Text, Is NLI-Neutral Text the Best Text?](https://doi.org/10.18653/v1/2023.findings-emnlp.170) |  | 0 | We explore incorporating natural language inference (NLI) into the text generative pipeline by using a pre-trained NLI model to assess whether a generated sentence entails, contradicts, or is neutral to the prompt and preceding text. First, we show that the NLI task is predictive of generation... | Kyle Mahowald, Michail Mersinias |  |
| 304 |  |  [Combining Counting Processes and Classification Improves a Stopping Rule for Technology Assisted Review](https://doi.org/10.18653/v1/2023.findings-emnlp.171) |  | 0 | Technology Assisted Review (TAR) stopping rules aim to reduce the cost of manually assessing documents for relevance by minimising the number of documents that need to be examined to ensure a desired level of recall. This paper extends an effective stopping rule using information derived from a... | Mark Stevenson, Reem Bin Hezam |  |
| 305 |  |  [Complexity-Guided Curriculum Learning for Text Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.172) |  | 0 | Curriculum learning provides a systematic approach to training. It refines training progressively, tailors training to task requirements, and improves generalization through exposure to diverse examples. We present a curriculum learning approach that builds on existing knowledge about text and... | Hadi Amiri, Nidhi Vakil |  |
| 306 |  |  [CoVariance-based Causal Debiasing for Entity and Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.173) |  | 0 | Joint entity and relation extraction tasks aim to recognize named entities and extract relations simultaneously. Suffering from a variety of data biases, such as data selection bias, and distribution bias (out of distribution, long-tail distribution), serious concerns can be witnessed to threaten... | Chunping Ouyang, Lin Ren, Yixin Cao, Yongbin Liu |  |
| 307 |  |  [Multi-label and Multi-target Sampling of Machine Annotation for Computational Stance Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.174) |  | 0 | Data collection from manual labeling provides domain-specific and task-aligned supervision for data-driven approaches, and a critical mass of well-annotated resources is required to achieve reasonable performance in natural language processing tasks. However, manual annotations are often... | Hai Leong Chieu, Nancy F. Chen, Zhengyuan Liu |  |
| 308 |  |  [In What Languages are Generative Language Models the Most Formal? Analyzing Formality Distribution across Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.175) |  | 0 | Multilingual generative language models (LMs) are increasingly fluent in a large variety of languages. Trained on the concatenation of corpora in multiple languages, they enable powerful transfer from high-resource languages to low-resource ones. However, it is still unknown what cultural biases... | Asim Ersoy, Benjamin Muller, Gerson Vizcarra, Tasmiah Tahsin Mayeesha |  |
| 309 |  |  [MaXM: Towards Multilingual Visual Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.176) |  | 0 | Visual Question Answering (VQA) has been primarily studied through the lens of the English language. Yet, tackling VQA in other languages in the same manner would require a considerable amount of resources. In this paper, we propose scalable solutions to multilingual visual question answering... | Ashish V. Thapliyal, Idan Szpektor, Julien Amelot, Linting Xue, Michal Yarom, Radu Soricut, Soravit Changpinyo, Xi Chen |  |
| 310 |  |  [Efficient Latent Variable Modeling for Knowledge-Grounded Dialogue Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.177) |  | 0 | Knowledge-grounded dialogue generation requires first retrieving appropriate external knowledge based on a conversational context and then generating a response grounded on the retrieved knowledge. In general, these two sequential modules, a knowledge retriever and a response generator, have been... | Chang Dong Yoo, Daejin Jo, Daniel Wontae Nam, Eunseop Yoon, Gunsoo Han, KyoungWoon On, Seungeun Rho, Sungwoong Kim, Taehwan Kwon |  |
| 311 |  |  [Ask To The Point: Open-Domain Entity-Centric Question Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.178) |  | 0 | We introduce a new task called \*entity-centric question generation\* (ECQG), motivated by real-world applications such as topic-specific learning, assisted reading, and fact-checking. The task aims to generate questions from an entity perspective. To solve ECQG, we propose a coherent PLM-based... | Jie Huang, Kevin ChenChuan Chang, Yuxiang Liu |  |
| 312 |  |  [Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.179) |  | 0 | In open-domain question-answering (ODQA), most existing questions require single-hop reasoning on commonsense. To further extend this task, we officially introduce open-domain multi-hop reasoning (ODMR) by answering multi-hop questions with explicit reasoning steps in open-domain setting. Recently,... | Hai Zhao, Jinyuan Wang, Junlong Li |  |
| 313 |  |  [CASE: Commonsense-Augmented Score with an Expanded Answer Space](https://doi.org/10.18653/v1/2023.findings-emnlp.180) |  | 0 | LLMs have demonstrated impressive zero-shot performance on NLP tasks thanks to the knowledge they acquired in their training. In multiple-choice QA tasks, the LM probabilities are used as an imperfect measure of the plausibility of each answer choice. One of the major limitations of the basic score... | Sahithya Ravi, Vered Shwartz, Wenkai Chen |  |
| 314 |  |  [GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.181) |  | 0 | Self-supervised representation learning on text-attributed graphs, which aims to create expressive and generalizable representations for various downstream tasks, has received increasing research attention lately. However, existing methods either struggle to capture the full extent of structural... | Kaize Ding, Kyumin Lee, Yichuan Li |  |
| 315 |  |  [Sources of Hallucination by Large Language Models on Inference Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.182) |  | 0 | Large Language Models (LLMs) are claimed to be capable of Natural Language Inference (NLI), necessary for applied tasks like question answering and summarization. We present a series of behavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled... | Liang Cheng, Mark Johnson, Mark Steedman, Mohammad Javad Hosseini, Nick McKenna, Tianyi Li |  |
| 316 |  |  [Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer](https://doi.org/10.18653/v1/2023.findings-emnlp.183) |  | 0 | Pretrained transformer models have demonstrated remarkable performance across various natural language processing tasks. These models leverage the attention mechanism to capture long- and short-range dependencies in the sequence. However, the (full) attention mechanism incurs high computational... | Cole Hawkins, Dhananjay Ram, Qingru Zhang, Sheng Zha, Tuo Zhao |  |
| 317 |  |  [Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with Auxiliary Refined Knowledge](https://doi.org/10.18653/v1/2023.findings-emnlp.184) |  | 0 | Multimodal Named Entity Recognition (MNER) on social media aims to enhance textual entity prediction by incorporating image-based clues. Existing studies mainly focus on maximizing the utilization of pertinent image information or incorporating external knowledge from explicit knowledge bases.... | Di Sun, Gang Pan, Han Li, Jiahao Wang, Jinyuan Li, Wenkun Zhang, Zhuo Pan |  |
| 318 |  |  [Understanding HTML with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.185) |  | 0 | Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding – i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval – have not... | Aakanksha Chowdhery, Aleksandra Faust, Austin V. Huang, Izzeddin Gur, Mustafa Safdari, Noah Fiedel, Ofir Nachum, Sharan Narang, Yingjie Miao |  |
| 319 |  |  [The PEACE-Reviews dataset: Modeling Cognitive Appraisals in Emotion Text Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.186) |  | 0 | Cognitive appraisal plays a pivotal role in deciphering emotions. Recent studies have delved into its significance, yet the interplay between various forms of cognitive appraisal and specific emotions, such as joy and anger, remains an area of exploration in consumption contexts. Our research... | Gerard Yeo, Kokil Jaidka |  |
| 320 |  |  [UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model](https://doi.org/10.18653/v1/2023.findings-emnlp.187) |  | 0 | Text is ubiquitous in our visual world, conveying crucial information, such as in documents, websites, and everyday photographs. In this work, we propose UReader, a first exploration of universal OCR-free visually-situated language understanding based on the Multimodal Large Language Model (MLLM).... | Anwen Hu, Chenliang Li, Fei Huang, Guohai Xu, Haiyang Xu, Ji Zhang, Jiabo Ye, Junfeng Tian, Liang He, Ming Yan, Qi Qian, Qin Jin, Qinghao Ye, Xin Lin |  |
| 321 |  |  [Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback](https://doi.org/10.18653/v1/2023.findings-emnlp.188) |  | 0 | Reinforcement learning from human feedback serves as a crucial bridge, aligning large language models with human and societal values. This alignment requires a vast corpus of human feedback to learn a reward model, which is subsequently used to finetune language models. However, we have identified... | Jun Zhao, Qi Zhang, Rui Zheng, Shihan Dou, Tao Gui, Wei Shen, WenYu Zhan, Xuanjing Huang |  |
| 322 |  |  [Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions](https://doi.org/10.18653/v1/2023.findings-emnlp.189) |  | 0 | Large Language Models (LLMs) demonstrate impressive reasoning ability and the maintenance of world knowledge not only in natural language tasks, but also in some vision-language tasks such as open-domain knowledge-based visual question answering (OK-VQA). As images are invisible to LLMs,... | Chi Chen, Peng Li, Yang Liu, Ziyue Wang |  |
| 323 |  |  [Take a Closer Look at Multilinguality! Improve Multilingual Pre-Training Using Monolingual Corpora Only](https://doi.org/10.18653/v1/2023.findings-emnlp.190) |  | 0 | Recent studies have revealed the remarkable cross-lingual capability of multilingual pre-trained language models (mPLMs), even when pre-trained without parallel corpora (mono-mPLMs). Intuitively, semantic alignments may be the reason behind such capability but remain under-explored. In this work,... | Jiajun Zhang, Jinliang Lu, Yu Lu |  |
| 324 |  |  [LogiCoT: Logical Chain-of-Thought Instruction Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.191) |  | 0 | Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive chain-of-thought reasoning ability. Recent work on self-instruction tuning, such as Alpaca, has focused on enhancing the general proficiency of models. These instructions enable the model to achieve performance comparable to... | Chaoli Zhang, Hanmeng Liu, Leyang Cui, Qiji Zhou, Yue Zhang, Zhiyang Teng |  |
| 325 |  |  [Hiding in Plain Sight: Tweets with Hate Speech Masked by Homoglyphs](https://doi.org/10.18653/v1/2023.findings-emnlp.192) |  | 0 | To avoid detection by current NLP monitoring applications, progenitors of hate speech often replace one or more letters in offensive words with homoglyphs, visually similar Unicode characters. Harvesting real-world hate speech containing homoglyphs is challenging due to the vast replacement... | Eduardo Blanco, Mihai Surdeanu, Portia Cooper |  |
| 326 |  |  [Reducing Spurious Correlations in Aspect-based Sentiment Analysis with Explanation from Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.193) |  | 0 | Recently, aspect-based sentiment analysis (ABSA) models have yielded promising results. However, they are susceptible to learning spurious correlations between certain words of the input text and output labels while modeling the sentiment feature of the aspect. This spurious correlation will... | Bin Liang, Keyang Ding, Min Yang, Qianlong Wang, Ruifeng Xu |  |
| 327 |  |  [High-quality argumentative information in low resources approaches improve counter-narrative generation](https://doi.org/10.18653/v1/2023.findings-emnlp.194) |  | 0 | It has been shown that high quality fine-tuning boosts the performance of language models, even if the size of the fine-tuning is small. In this work we show how highly targeted fine-tuning improves the task of hate speech counter-narrative generation in user-generated text, even for very small... | Damián Ariel Furman, Diego Letzen, José A. Rodríguez, Laura Alonso Alemany, Maria Vanina Martinez, Pablo Torres |  |
| 328 |  |  [A Reference-free Segmentation Quality Index (SegReFree)](https://doi.org/10.18653/v1/2023.findings-emnlp.195) |  | 0 | Topic segmentation, in the context of natural language processing, is the process of finding boundaries in a sequence of sentences that separate groups of adjacent sentences at shifts in semantic meaning. Currently, assessing the quality of a segmentation is done by comparing segmentation... | Dylan Kangas, Evan Lucas, Timothy C. Havens |  |
| 329 |  |  [In-context Learning for Few-shot Multimodal Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.196) |  | 0 | Thanks in part to the availability of copious annotated resources for some entity categories, existing studies have achieved superior performance in multimodal named entity recognition (MNER). However, in the real-world scenario, it is infeasible to enumerate all entity categories in advance.... | Bin Liang, Bing Qin, Chenran Cai, KamFai Wong, Min Yang, Qianlong Wang, Ruifeng Xu |  |
| 330 |  |  [On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study](https://doi.org/10.18653/v1/2023.findings-emnlp.197) |  | 0 | Modern deep models for summarization attains impressive benchmark performance, but they are prone to generating miscalibrated predictive uncertainty. This means that they assign high confidence to low-quality predictions, leading to compromised reliability and trustworthiness in real-world... | Du Phan, Jeremiah Z. Liu, Jie Ren, Joshua Maynez, Polina Zablotskaia, Shashi Narayan |  |
| 331 |  |  [Handshape-Aware Sign Language Recognition: Extended Datasets and Exploration of Handshape-Inclusive Methods](https://doi.org/10.18653/v1/2023.findings-emnlp.198) |  | 0 | The majority of existing work on sign language recognition encodes signed videos without explicitly acknowledging the phonological attributes of signs. Given that handshape is a vital parameter in sign languages, we explore the potential of handshape-aware sign language recognition. We augment the... | Kevin Duh, Xuan Zhang |  |
| 332 |  |  [SimCKP: Simple Contrastive Learning of Keyphrase Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.199) |  | 0 | Keyphrase generation (KG) aims to generate a set of summarizing words or phrases given a source document, while keyphrase extraction (KE) aims to identify them from the text. Because the search space is much smaller in KE, it is often combined with KG to predict keyphrases that may or may not exist... | Chaeheon Gwak, Jaegul Choo, Minseok Choi, Seho Kim, Si Hyeong Kim |  |
| 333 |  |  [LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain](https://doi.org/10.18653/v1/2023.findings-emnlp.200) |  | 0 | Lately, propelled by phenomenal advances around the transformer architecture, the legal NLP field has enjoyed spectacular growth. To measure progress, well-curated and challenging benchmarks are crucial. Previous efforts have produced numerous benchmarks for general NLP models, typically based on... | Andrea Galassi, Ilias Chalkidis, Joel Niklaus, Matthias Stürmer, Pooja Rani, Veton Matoshi |  |
| 334 |  |  [Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.201) |  | 0 | Due to the remarkable language understanding and generation abilities of large language models (LLMs), their use in educational applications has been explored. However, little work has been done on investigating the pedagogical ability of LLMs in helping students to learn mathematics. In this... | AnZi Yen, WeiLing Hsu |  |
| 335 |  |  [Simultaneous Machine Translation with Tailored Reference](https://doi.org/10.18653/v1/2023.findings-emnlp.202) |  | 0 | Simultaneous machine translation (SiMT) generates translation while reading the whole source sentence. However, existing SiMT models are typically trained using the same reference disregarding the varying amounts of available source information at different latency. Training the model with... | Shaolei Zhang, Shoutao Guo, Yang Feng |  |
| 336 |  |  [Dynamic Voting for Efficient Reasoning in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.203) |  | 0 | Multi-path voting methods like Self-consistency have been used to mitigate reasoning errors in large language models caused by factual errors and illusion generation. However, these methods require excessive computing resources as they generate numerous reasoning paths for each problem. And our... | Baosong Yang, Dayiheng Liu, Dezhong Peng, Jiancheng Lv, Jun Xie, Mingfeng Xue, Wenqiang Lei, Xingzhang Ren, Yidan Zhang |  |
| 337 |  |  [On Surgical Fine-tuning for Language Encoders](https://doi.org/10.18653/v1/2023.findings-emnlp.204) |  | 0 | Fine-tuning all the layers of a pre-trained neural language encoder (either using all the parameters or using parameter-efficient methods) is often the de-facto way of adapting it to a new task. We show evidence that for different downstream language tasks, fine-tuning only a subset of layers is... | Abhilasha Lodha, Dmitrii Petrov, Gayatri Belapurkar, Reshmi Ghosh, Saloni Chalkapurkar, Samyadeep Basu, Soundararajan Srinivasan, Yuanming Tao |  |
| 338 |  |  [AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.205) |  | 0 | Recent large language models (LLMs) are promising for making decisions in grounded environments. However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment. Existing methods require either... | Lei Li, Siqi Ouyang |  |
| 339 |  |  [Measuring Faithful and Plausible Visual Grounding in VQA](https://doi.org/10.18653/v1/2023.findings-emnlp.206) |  | 0 | Metrics for Visual Grounding (VG) in Visual Question Answering (VQA) systems primarily aim to measure a system’s reliance on relevant parts of the image when inferring an answer to the given question. Lack of VG has been a common problem among state-of-the-art VQA systems and can manifest in... | Daniel Reich, Felix Putze, Tanja Schultz |  |
| 340 |  |  [Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.207) |  | 0 | Large language models (LLMs) enable zero-shot approaches in open-domain question answering (ODQA), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cost and the need for... | Jeongyeon Seo, Jong C. Park, Soyeong Jeong, Sukmin Cho |  |
| 341 |  |  [Can you Summarize my learnings? Towards Perspective-based Educational Dialogue Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.208) |  | 0 | The steady increase in the utilization of Virtual Tutors (VT) over recent years has allowed for a more efficient, personalized, and interactive AI-based learning experiences. A vital aspect in these educational chatbots is summarizing the conversations between the VT and the students, as it is... | Jhagrut Lalwani, Raghav Jain, Sriparna Saha, Tulika Saha |  |
| 342 |  |  [Adaptive Textual Label Noise Learning based on Pre-trained Models](https://doi.org/10.18653/v1/2023.findings-emnlp.209) |  | 0 | The label noise in real-world scenarios is unpredictable and can even be a mixture of different types of noise. To meet this challenge, we develop an adaptive textual label noise learning framework based on pre-trained models, which consists of an adaptive warm-up stage and a hybrid training stage.... | Hong Qu, Mingsheng Fu, Shaohuan Cheng, Wenyu Chen, Xuanting Xie |  |
| 343 |  |  [Towards Informative Open-ended Text Generation with Dynamic Knowledge Triples](https://doi.org/10.18653/v1/2023.findings-emnlp.210) |  | 0 | Pretrained language models (PLMs), especially large language models (LLMs) demonstrate impressive capabilities in open-ended text generation. While our statistical results show that LLMs often suffer from over-concentrated information, where the generated texts overly focus on the given prompt and... | Chengqing Zong, Yang Zhao, Zixuan Ren |  |
| 344 |  |  [Novel Relation Detection: Discovering Unknown Relation Types via Multi-Strategy Self-Supervised Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.211) |  | 0 | Conventional approaches to relation extraction can only recognize predefined relation types. In the real world, new or out-of-scope relation types may keep challenging the deployed models. In this paper, we formalize such a challenging problem as Novel Relation Detection (NRD), which aims to... | Dianbo Sui, Jiaoyan Chen, Ningyu Zhang, Qingbin Liu, Siyuan Cheng, Xi Chen, Yanchao Hao, Yin Kung |  |
| 345 |  |  [Ask Language Model to Clean Your Noisy Translation Data](https://doi.org/10.18653/v1/2023.findings-emnlp.212) |  | 0 | TTransformer models have demonstrated remarkable performance in neural machine translation (NMT). However, their vulnerability to noisy input poses a significant challenge in practical implementation, where generating clean output from noisy input is crucial. The MTNT dataset is widely used as a... | Baohao Liao, Brandon James Denis, Christof Monz, Jun Luo, Quinten Bolding |  |
| 346 |  |  [Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users](https://doi.org/10.18653/v1/2023.findings-emnlp.213) |  | 0 | While most task-oriented dialogues assume conversations between the agent and one user at a time, dialogue systems are increasingly expected to communicate with multiple users simultaneously who make decisions collaboratively. To facilitate development of such systems, we release the Multi-User... | Alexandros Potamianos, Angeliki Metallinou, Arijit Biswas, Nikolaos Malandrakis, Nikoletta Basiou, Vincent Auvray, Xinyan Zhao, Yohan Jo |  |
| 347 |  |  [Extractive Summarization via ChatGPT for Faithful Summary Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.214) |  | 0 | Extractive summarization is a crucial task in natural language processing that aims to condense long documents into shorter versions by directly extracting sentences. The recent introduction of large language models has attracted significant interest in the NLP community due to its remarkable... | Haopeng Zhang, Jiawei Zhang, Xiao Liu |  |
| 348 |  |  [MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization](https://doi.org/10.18653/v1/2023.findings-emnlp.215) |  | 0 | Prompt engineering, as an efficient and effective way to leverage Large Language Models (LLM), has drawn a lot of attention from the research community. The existing research primarily emphasizes the importance of adapting prompts to specific tasks, rather than specific LLMs. However, a good prompt... | Bang Liu, Dayiheng Liu, Ge Fan, Wei Wu, Yanghua Xiao, Yuyan Chen, Zhengyu Chen, Zhihao Wen, Zhixu Li |  |
| 349 |  |  [PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.216) |  | 0 | Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual’s personality from their written texts, remains largely... | Bingzhe Wu, Fanqi Wan, Jiaxiang Wu, Qifan Wang, Tao Yang, Tianyuan Shi, Xiaojun Quan |  |
| 350 |  |  [Harnessing the power of LLMs: Evaluating human-AI text co-creation through the lens of news headline generation](https://doi.org/10.18653/v1/2023.findings-emnlp.217) |  | 0 | To explore how humans can best leverage LLMs for writing and how interacting with these models affects feelings of ownership and trust in the writing process, we compared common human-AI interaction types (e.g., guiding system, selecting from system outputs, post-editing outputs) in the context of... | Alejandro Jaimes, Alison SmithRenner, Joel R. Tetreault, Wenjuan Zhang, Zijian Ding |  |
| 351 |  |  [NERetrieve: Dataset for Next Generation Named Entity Recognition and Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.218) |  | 0 | Recognizing entities in texts is a central need in many information-seeking scenarios, and indeed, Named Entity Recognition (NER) is arguably one of the most successful examples of a widely adopted NLP task and corresponding NLP technology. Recent advances in large language models (LLMs) appear to... | Amir David Nissan Cohen, Matan Vetzler, Uri Katz, Yoav Goldberg |  |
| 352 |  |  [SWEET - Weakly Supervised Person Name Extraction for Fighting Human Trafficking](https://doi.org/10.18653/v1/2023.findings-emnlp.219) |  | 0 | In this work, we propose a weak supervision pipeline SWEET: Supervise Weakly for Entity Extraction to fight Trafficking for extracting person names from noisy escort advertisements. Our method combines the simplicity of rule-matching (through antirules, i.e., negated rules) and the generalizability... | Hao Yu, Javin Liu, Kellin Pelrine, Pratheeksha Nair, Reihaneh Rabbany, Vidya Sujaya |  |
| 353 |  |  [Watermarking LLMs with Weight Quantization](https://doi.org/10.18653/v1/2023.findings-emnlp.220) |  | 0 | Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models. This paper proposes a novel watermarking strategy... | Botian Jiang, Hang Yan, Ke Ren, Linyang Li, Pengyu Wang, Xipeng Qiu |  |
| 354 |  |  [Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.221) |  | 0 | Spatial reasoning over text is challenging as the models not only need to extract the direct spatial information from the text but also reason over those and infer implicit spatial relations. Recent studies highlight the struggles even large language models encounter when it comes to performing... | Parisa Kordjamshidi, Roshanak Mirzaee |  |
| 355 |  |  [PsyAttention: Psychological Attention Model for Personality Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.222) |  | 0 | Work on personality detection has tended to incorporate psychological features from different personality models, such as BigFive and MBTI. There are more than 900 psychological features, each of which is helpful for personality detection. However, when used in combination, the application of... | Baohua Zhang, Huaping Zhang, Jianyun Shang, Wenyao Cui, Yongyi Huang |  |
| 356 |  |  [RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training](https://doi.org/10.18653/v1/2023.findings-emnlp.223) |  | 0 | Fine-tuning pre-trained language models (LMs) has become the de facto standard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to robustness issues, such as adversarial robustness and model calibration. Several perspectives of robustness for LMs have been studied independently, but... | Davis Liang, Fuli Feng, Hanchao Yu, Jaehyung Kim, Lifu Huang, Madian Khabsa, Pascale Fung, Qifan Wang, Rui Hou, Yuning Mao |  |
| 357 |  |  [The Law and NLP: Bridging Disciplinary Disconnects](https://doi.org/10.18653/v1/2023.findings-emnlp.224) |  | 0 | Legal practice is intrinsically rooted in the fabric of language, yet legal practitioners and scholars have been slow to adopt tools from natural language processing (NLP). At the same time, the legal system is experiencing an access to justice crisis, which could be partially alleviated with NLP.... | Alex Pentland, Dominik Stammbach, Elliott Ash, Robert Mahari |  |
| 358 |  |  [Symbolization, Prompt, and Classification: A Framework for Implicit Speaker Identification in Novels](https://doi.org/10.18653/v1/2023.findings-emnlp.225) |  | 0 | Speaker identification in novel dialogues can be widely applied to various downstream tasks, such as producing multi-speaker audiobooks and converting novels into scripts. However, existing state-of-the-art methods are limited to handling explicit narrative patterns like “Tom said, '...'", unable... | Heng Lu, Hongbin Zhou, JiaChen Gu, TianWei He, Yue Chen, ZhenHua Ling |  |
| 359 |  |  [Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.226) |  | 0 | Pre-trained and frozen LLMs can effectively map simple scene re-arrangement instructions to programs over a robot’s visuomotor functions through appropriate few-shot example prompting. To parse open-domain natural language and adapt to a user’s idiosyncratic procedures, not known during prompt... | Gabriel Sarch, Katerina Fragkiadaki, Michael J. Tarr, Yue Wu |  |
| 360 |  |  [ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought](https://doi.org/10.18653/v1/2023.findings-emnlp.227) |  | 0 | Recently Large Language Models (LLMs) have been proven to have strong abilities in various domains and tasks. We study the problem of prompt designing in the text-to-SQL task and attempt to improve the LLMs’ reasoning ability when generating SQL queries. Besides the trivial few-shot in-context... | Hanchong Zhang, Hongshen Xu, Kai Yu, Lu Chen, Ruisheng Cao |  |
| 361 |  |  [Manifold-Preserving Transformers are Effective for Short-Long Range Encoding](https://doi.org/10.18653/v1/2023.findings-emnlp.228) |  | 0 | Multi-head self-attention-based Transformers have shown promise in different learning tasks. Albeit these models exhibit significant improvement in understanding short-term and long-term contexts from sequences, encoders of Transformers and their variants fail to preserve layer-wise contextual... | Ayan Sengupta, Md. Shad Akhtar, Tanmoy Chakraborty |  |
| 362 |  |  [ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.229) |  | 0 | We present ASPIRO, an approach for structured data verbalisation into short template sentences in zero to few-shot settings. Unlike previous methods, our approach prompts Large Language Models (LLMs) to directly produce entity-agnostic templates, rather than relying on LLMs to faithfully copy the... | Martin Vejvar, Yasutaka Fujimoto |  |
| 363 |  |  [Detecting Syntactic Change with Pre-trained Transformer Models](https://doi.org/10.18653/v1/2023.findings-emnlp.230) |  | 0 | We investigate the ability of Transformer-based language models to find syntactic differences between the English of the early 1800s and that of the late 1900s. First, we show that a fine-tuned BERT model can distinguish between text from these two periods using syntactic information only; to show... | David Smith, Liwen Hou |  |
| 364 |  |  [A Word Sense Distribution-based approach for Semantic Change Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.231) |  | 0 | Semantic Change Detection of words is an important task for various NLP applications that must make time-sensitive predictions. Some words are used over time in novel ways to express new meanings, and these new meanings establish themselves as novel senses of existing words. On the other hand, Word... | Danushka Bollegala, Procheta Sen, Taichi Aida, Xiaohang Tang, Yi Zhou |  |
| 365 |  |  [Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.232) |  | 0 | Commonsense Knowledge Graphs (CSKGs) are crucial for commonsense reasoning, yet constructing them through human annotations can be costly. As a result, various automatic methods have been proposed to construct CSKG with larger semantic coverage. However, these unsupervised approaches introduce... | Weiqi Wang, Xin Liu, Yangqiu Song, Zhaowei Wang, Zheye Deng |  |
| 366 |  |  [Improving Conversational Recommendation Systems via Bias Analysis and Language-Model-Enhanced Data Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.233) |  | 0 | Conversational Recommendation System (CRS) is a rapidly growing research area that has gained significant attention alongside advancements in language modelling techniques. However, the current state of conversational recommendation faces numerous challenges due to its relative novelty and limited... | Emine Yilmaz, Hossein A. Rahmani, Jiqun Liu, Xi Wang |  |
| 367 |  |  [Exploring Graph Pre-training for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.234) |  | 0 | Existing studies tend to extract the sentiment elements in a generative manner in order to avoid complex modeling. Despite their effectiveness, they ignore importance of the relationships between sentiment elements that could be crucial, making the large pre-trained generative models sub-optimal... | Guodong Zhou, Xiaoyi Bao, Zhongqing Wang |  |
| 368 |  |  [DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding](https://doi.org/10.18653/v1/2023.findings-emnlp.235) |  | 0 | Temporal Language Grounding seeks to localize video moments that semantically correspond to a natural language query. Recent advances employ the attention mechanism to learn the relations between video moments and the text query. However, naive attention might not be able to appropriately capture... | Anh Tuan Luu, CongDuy Nguyen, SeeKiong Ng, Thong Nguyen, Xiaobao Wu, Xinshuai Dong |  |
| 369 |  |  [Test-time Augmentation for Factual Probing](https://doi.org/10.18653/v1/2023.findings-emnlp.236) |  | 0 | Factual probing is a method that uses prompts to test if a language model “knows” certain world knowledge facts. A problem in factual probing is that small changes to the prompt can lead to large changes in model output. Previous work aimed to alleviate this problem by optimizing prompts via text... | Benjamin Heinzerling, Go Kamoda, Keisuke Sakaguchi, Kentaro Inui |  |
| 370 |  |  [Methodological Insights in Detecting Subtle Semantic Shifts with Contextualized and Static Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.237) |  | 0 | In this paper, we investigate automatic detection of subtle semantic shifts between social communities of different political convictions in Dutch and English. We perform a methodological study comparing methods using static and contextualized language models. We investigate the impact of... | Antske Fokkens, Pia Sommerauer, Sanne Hoeken, Özge Alaçam |  |
| 371 |  |  [Disfluent Cues for Enhanced Speech Understanding in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.238) |  | 0 | In computational linguistics, the common practice is to “clean” disfluent content from spontaneous speech. However, we hypothesize that these disfluencies might serve as more than mere noise, potentially acting as informative cues. We use a range of pre-trained models for a reading comprehension... | David A. Clifton, Farhad Nooralahzadeh, Michael Krauthammer, Morteza Rohanian, Omid Rohanian |  |
| 372 |  |  [Watermarking PLMs on Classification Tasks by Combining Contrastive Learning with Weight Perturbation](https://doi.org/10.18653/v1/2023.findings-emnlp.239) |  | 0 | Large pre-trained language models (PLMs) have achieved remarkable success, making them highly valuable intellectual property due to their expensive training costs. Consequently, model watermarking, a method developed to protect the intellectual property of neural models, has emerged as a crucial... | Cenyuan Zhang, Chengsong Huang, Chenxi Gu, Hua Cai, Jianhan Xu, Muling Wu, Xiaoqing Zheng, Xuanjing Huang |  |
| 373 |  |  [BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla Lemmatizer](https://doi.org/10.18653/v1/2023.findings-emnlp.240) |  | 0 | Lemmatization holds significance in both natural language processing (NLP) and linguistics, as it effectively decreases data density and aids in comprehending contextual meaning. However, due to the highly inflected nature and morphological richness, lemmatization in Bangla text poses a complex... | Faisal Ahamed Khan, Hakim Arif, Labib Imam Chowdhury, Massud Forkan, Md. Ekramul Islam, Md. Motahar Mahtab, Md. Shahad Mahmud Chowdhury, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed, Nazifa Nuha Chowdhury, Neelima Kundu, Sadia Afrin |  |
| 374 |  |  [Exploring the Sensitivity of LLMs' Decision-Making Capabilities: Insights from Prompt Variations and Hyperparameters](https://doi.org/10.18653/v1/2023.findings-emnlp.241) |  | 0 | The advancement of Large Language Models (LLMs) has led to their widespread use across a broad spectrum of tasks, including decision-making. Prior studies have compared the decision-making abilities of LLMs with those of humans from a psychological perspective. However, these studies have not... | Divya Sinha, Manikanta Loya, Richard Futrell |  |
| 375 |  |  [Search Augmented Instruction Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.242) |  | 0 | Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and... | Helen Meng, Hongyin Luo, James R. Glass, Tianhua Zhang, Xixin Wu, Yoon Kim, Yuan Gong, YungSung Chuang |  |
| 376 |  |  ["Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters](https://doi.org/10.18653/v1/2023.findings-emnlp.243) |  | 0 | Large Language Models (LLMs) have recently emerged as an effective tool to assist individuals in writing various types of content, including professional documents such as recommendation letters. Though bringing convenience, this application also introduces unprecedented fairness concerns.... | Aparna Garimella, George Pu, Jiao Sun, KaiWei Chang, Nanyun Peng, Yixin Wan |  |
| 377 |  |  [TextMixer: Mixing Multiple Inputs for Privacy-Preserving Inference](https://doi.org/10.18653/v1/2023.findings-emnlp.244) |  | 0 | Pre-trained language models (PLMs) are often deployed as cloud services, enabling users to upload textual data and perform inference remotely. However, users’ personal text often contains sensitive information, and sharing such data directly with the service providers can lead to serious privacy... | Qi Zhang, Ruotian Ma, Tao Gui, Xin Zhou, Xuanjing Huang, Yi Lu |  |
| 378 |  |  [FinePrompt: Unveiling the Role of Finetuned Inductive Bias on Compositional Reasoning in GPT-4](https://doi.org/10.18653/v1/2023.findings-emnlp.245) |  | 0 | Compositional reasoning across texts has been a long-standing challenge in natural language processing. With large language models like GPT-4 taking over the field, prompting techniques such as chain-of-thought (CoT) were proposed to unlock compositional, multi-step reasoning capabilities of LLMs.... | Giwon Hong, Jeonghwan Kim, Joyce Jiyoung Whang, SungHyon Myaeng |  |
| 379 |  |  [Teacher Perception of Automatically Extracted Grammar Concepts for L2 Language Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.246) |  | 0 | One of the challenges in language teaching is how best to organize rules regarding syntax, semantics, or phonology in a meaningful manner. This not only requires content creators to have pedagogical skills, but also have that language’s deep understanding. While comprehensive materials to develop... | Aditi Chaudhary, Antonios Anastasopoulos, Arun Sampath, Ashwin Sheshadri, Graham Neubig |  |
| 380 |  |  [Allies: Prompting Large Language Model with Beam Search](https://doi.org/10.18653/v1/2023.findings-emnlp.247) |  | 0 | With the advance of large language models (LLMs), the research field of LLM applications becomes more and more popular and the idea of constructing pipelines to accomplish complex tasks by stacking LLM API calls come true. However, this kind of methods face two limitations: narrow information... | Daxin Jiang, Hao Sun, Linjun Yang, Nan Duan, Xiao Liu, Yan Zhang, Yeyun Gong |  |
| 381 |  |  [Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.248) |  | 0 | Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a... | Alon Albalak, Liangming Pan, William Yang Wang, Xinyi Wang |  |
| 382 |  |  [SiMFy: A Simple Yet Effective Approach for Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.249) |  | 0 | Temporal Knowledge Graph (TKG) reasoning, which focuses on leveraging temporal information to infer future facts in knowledge graphs, plays a vital role in knowledge graph completion. Typically, existing works for this task design graph neural networks and recurrent neural networks to respectively... | Hai Jin, Lei Tan, Mengfan Li, Xuanhua Shi, Yao Wan, Zhengtao Liu |  |
| 383 |  |  [Understanding Translationese in Cross-Lingual Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.250) |  | 0 | Given a document in a source language, cross-lingual summarization (CLS) aims at generating a concise summary in a different target language. Unlike monolingual summarization (MS), naturally occurring source-language documents paired with target-language summaries are rare. To collect large-scale... | Fandong Meng, Jiaan Wang, Jiarong Xu, Jie Zhou, Tingyi Zhang, Yunlong Liang, Zhixu Li |  |
| 384 |  |  [The Truth, The Whole Truth, and Nothing but the Truth: A New Benchmark Dataset for Hebrew Text Credibility Assessment](https://doi.org/10.18653/v1/2023.findings-emnlp.251) |  | 0 | In the age of information overload, it is more important than ever to discern fact from fiction. From the internet to traditional media, we are constantly confronted with a deluge of information, much of which comes from politicians and other public figures who wield significant influence. In this... | Ben Hagag, Reut Tsarfaty |  |
| 385 |  |  [IndiSocialFT: Multilingual Word Representation for Indian languages in code-mixed environment](https://doi.org/10.18653/v1/2023.findings-emnlp.252) |  | 0 | The increasing number of Indian language users on the internet necessitates the development of Indian language technologies. In response to this demand, our paper presents a generalized representation vector for diverse text characteristics, including native scripts, transliterated text,... | Sanasam Ranbir Singh, Saurabh Kumar, Sukumar Nandi |  |
| 386 |  |  [Adaptive Hinge Balance Loss for Document-Level Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.253) |  | 0 | Document-Level Relation Extraction aims at predicting relations between entities from multiple sentences. A common practice is to select multi-label classification thresholds to decide whether a relation exists between an entity pair. However, in the document-level task, most entity pairs do not... | Cailian Chen, Jize Wang, Xiaodi Peng, Xinyi Le |  |
| 387 |  |  [Answer-state Recurrent Relational Network (AsRRN) for Constructed Response Assessment and Feedback Grouping](https://doi.org/10.18653/v1/2023.findings-emnlp.254) |  | 0 | STEM educators must trade off the ease of assessing selected response (SR) questions, like multiple choice, with constructed response (CR) questions, where students articulate their own reasoning. Our work addresses a CR type new to NLP but common in college STEM, consisting of multiple questions... | Matthew Beckman, Rebecca J. Passonneau, Susan Lloyd, Zhaohui Li |  |
| 388 |  |  [Low-Resource Comparative Opinion Quintuple Extraction by Data Augmentation with Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.255) |  | 0 | Comparative Opinion Quintuple Extraction (COQE) aims to predict comparative opinion quintuples from comparative sentences. These quintuples include subject, object, shareable aspect, comparative opinion, and preference. The existing pipeline-based COQE method fails in error propagation. In... | Fubang Zhao, Guodong Zhou, Jiaxiang Chen, Kaisong Song, Qingting Xu, Yangyang Kang, Yu Hong |  |
| 389 |  |  [A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.256) |  | 0 | Large Language Models (LLMs) have shown their ability to collaborate effectively with humans in real-world scenarios. However, LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical... | Renliang Sun, Shiping Yang, Xiaojun Wan |  |
| 390 |  |  [Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.257) |  | 0 | We propose Speculative Decoding (SpecDec), for the first time ever, to formally study exploiting the idea of speculative execution to accelerate autoregressive (AR) decoding. Speculative Decoding has two innovations: Spec-Drafter – an independent model specially optimized for efficient and accurate... | Furu Wei, Heming Xia, Peiyi Wang, SiQing Chen, Tao Ge, Zhifang Sui |  |
| 391 |  |  [APP: Adaptive Prototypical Pseudo-Labeling for Few-shot OOD Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.258) |  | 0 | Detecting out-of-domain (OOD) intents from user queries is essential for a task-oriented dialogue system. Previous OOD detection studies generally work on the assumption that plenty of labeled IND intents exist. In this paper, we focus on a more practical few-shot OOD setting where there are only a... | Jingang Wang, Keqing He, Pei Wang, Weiran Xu, Xiaoshuai Song, Xunliang Cai, Yanan Wu, Yunsen Xian, Yutao Mou |  |
| 392 |  |  [2INER: Instructive and In-Context Learning on Few-Shot Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.259) |  | 0 | Prompt-based learning has emerged as a powerful technique in natural language processing (NLP) due to its ability to leverage pre-training knowledge for downstream few-shot tasks. In this paper, we propose 2INER, a novel text-to-text framework for Few-Shot Named Entity Recognition (NER) tasks. Our... | Jiasheng Zhang, Shusen Wang, Xikai Liu, Xinyi Lai, Yan Gao, Yao Hu, Yiqing Lin |  |
| 393 |  |  [Generative Emotion Cause Triplet Extraction in Conversations with Commonsense Knowledge](https://doi.org/10.18653/v1/2023.findings-emnlp.260) |  | 0 | Emotion Cause Triplet Extraction in Conversations (ECTEC) aims to simultaneously extract emotion utterances, emotion categories, and cause utterances from conversations. However, existing studies mainly decompose the ECTEC task into multiple subtasks and solve them in a pipeline manner. Moreover,... | Fanfan Wang, Jianfei Yu, Rui Xia |  |
| 394 |  |  [Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.261) |  | 0 | Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such as their focus on... | Saeed Hassanpour, Sean Xie, Soroush Vosoughi |  |
| 395 |  |  [GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence](https://doi.org/10.18653/v1/2023.findings-emnlp.262) |  | 0 | Conditional story generation is significant in human-machine interaction, particularly in producing stories with complex plots. While Large language models (LLMs) perform well on multiple NLP tasks, including story generation, it is challenging to generate stories with both complex and creative... | Dongsheng Li, Wei Wu, Yanqi Shi, Yuxin Yang, Zhen Huang, Zhihua Wen, Zhiliang Tian |  |
| 396 |  |  [KAPALM: Knowledge grAPh enhAnced Language Models for Fake News Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.263) |  | 0 | Social media has not only facilitated news consumption, but also led to the wide spread of fake news. Because news articles in social media is usually condensed and full of knowledge entities, existing methods of fake news detection use external entity knowledge. However, majority of these methods... | Chen Chen, Chunyan Hou, Jing Ma, Xiaojie Yuan |  |
| 397 |  |  [Comparing the Evaluation and Production of Loophole Behavior in Humans and Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.264) |  | 0 | In law, lore, and everyday life, loopholes are commonplace. When people exploit a loophole, they understand the intended meaning or goal of another person, but choose to go with a different interpretation. Past and current AI research has shown that artificial intelligence engages in what seems... | Kiera Parece, Peng Qian, Sonia K. Murthy, Sophie Bridgers, Tomer D. Ullman |  |
| 398 |  |  [InstructExcel: A Benchmark for Natural Language Instruction in Excel](https://doi.org/10.18653/v1/2023.findings-emnlp.265) |  | 0 | With the evolution of Large Language Models (LLMs) we can solve increasingly more complex NLP tasks across various domains, including spreadsheets. This work investigates whether LLMs can generate code (Excel OfficeScripts, a TypeScript API for executing many tasks in Excel) that solves Excel... | Benjamin Van Durme, Carina Negreanu, Chitta Baral, Christian Pölitz, Elnaz Nouri, Justin Payan, Mukul Singh, Rasika Chakravarthy, Subhro Roy, Swaroop Mishra |  |
| 399 |  |  [Hallucination Detection for Grounded Instruction Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.266) |  | 0 | We investigate the problem of generating instructions to guide humans to navigate in simulated residential environments. A major issue with current models is hallucination: they generate references to actions or objects that are inconsistent with what a human follower would perform or encounter... | Hal Daumé III, Khanh Nguyen, Lingjun Zhao |  |
| 400 |  |  [Definitions Matter: Guiding GPT for Multi-label Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.267) |  | 0 | Large language models have recently risen in popularity due to their ability to perform many natural language tasks without requiring any fine-tuning. In this work, we focus on two novel ideas: (1) generating definitions from examples and using them for zero-shot classification, and (2)... | Damir Korencic, Ivan Grubisic, Paolo Papotti, Paolo Rosso, Raphaël Troncy, Youri Peskine |  |
| 401 |  |  [ECHo: A Visio-Linguistic Dataset for Event Causality Inference via Human-Centric Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.268) |  | 0 | We introduce ECHo (Event Causality Inference via Human-Centric Reasoning), a diagnostic dataset of event causality inference grounded in visio-linguistic social scenarios. ECHo employs real-world human-centric deductive information building on a television crime drama. ECHo requires the... | Guanzhen Li, MinYen Kan, Yuxi Xie |  |
| 402 |  |  [An Empirical Study of Instruction-tuning Large Language Models in Chinese](https://doi.org/10.18653/v1/2023.findings-emnlp.269) |  | 0 | The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the open-source community’s interest in instruction-tuning, which is deemed to accelerate ChatGPT’s replication process. However,... | Qingyi Si, Tong Wang, Weiping Wang, Xu Zhang, Yanan Cao, Zheng Lin |  |
| 403 |  |  [Debiasing Multimodal Models via Causal Information Minimization](https://doi.org/10.18653/v1/2023.findings-emnlp.270) |  | 0 | Most existing debiasing methods for multimodal models, including causal intervention and inference methods, utilize approximate heuristics to represent the biases, such as shallow features from early stages of training or unimodal features for multimodal tasks like VQA, etc., which may not be... | Adyasha Maharana, Mohit Bansal, Vaidehi Patil |  |
| 404 |  |  [Evaluating Emotion Arcs Across Languages: Bridging the Global Divide in Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.271) |  | 0 | Emotion arcs capture how an individual (or a population) feels over time. They are widely used in industry and research; however, there is little work on evaluating the automatically generated arcs. This is because of the difficulty of establishing the true (gold) emotion arc. Our work, for the... | Daniela Teodorescu, Saif M. Mohammad |  |
| 405 |  |  [Multi-step Jailbreaking Privacy Attacks on ChatGPT](https://doi.org/10.18653/v1/2023.findings-emnlp.272) |  | 0 | With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given appropriate prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content... | Dadi Guo, Fanpu Meng, Haoran Li, Jie Huang, Mingshi Xu, Wei Fan, Yangqiu Song |  |
| 406 |  |  [Chain-of-Thought Embeddings for Stance Detection on Social Media](https://doi.org/10.18653/v1/2023.findings-emnlp.273) |  | 0 | Stance detection on social media is challenging for Large Language Models (LLMs), as emerging slang and colloquial language in online conversations often contain deeply implicit stance labels. Chain-of-Thought (COT) prompting has recently been shown to improve performance on stance detection tasks... | Joseph Gatto, Omar Sharif, Sarah Preum |  |
| 407 |  |  [Using LLM for Improving Key Event Discovery: Temporal-Guided News Stream Clustering with Event Summaries](https://doi.org/10.18653/v1/2023.findings-emnlp.274) |  | 0 | Understanding and characterizing the discus- sions around key events in news streams is important for analyzing political discourse. In this work, we study the problem of identification of such key events and the news articles associated with those events from news streams. We propose a generic... | Dan Goldwasser, Dan Roth, Daniel Hopkins, Nishanth Sridhar Nakshatri, Sihao Chen, Siyi Liu |  |
| 408 |  |  [Descriptive Prompt Paraphrasing for Target-Oriented Multimodal Sentiment Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.275) |  | 0 | Target-Oriented Multimodal Sentiment Classification (TMSC) aims to perform sentiment polarity on a target jointly considering its corresponding multiple modalities including text, image, and others. Current researches mainly work on either of two types of targets in a decentralized manner. One type... | Dan Liu, Jian Cui, Lin Li, Qing Xie, Xiaohui Tao |  |
| 409 |  |  [Joint Semantic and Strategy Matching for Persuasive Dialogue](https://doi.org/10.18653/v1/2023.findings-emnlp.276) |  | 0 | Persuasive dialogue aims to persuade users to achieve some targets by conversations. While previous persuasion models have achieved notable successes, they mostly base themselves on utterance semantic matching, and an important aspect has been ignored, that is, the strategy of the conversations,... | Chuhao Jin, Huan Chen, Jun Xu, Lingzhen Kong, Ruihua Song, Shijie Li, Xiao Zhang, Xu Chen, Yu Chen, Yuchong Sun, Yutao Zhu |  |
| 410 |  |  [Non-Autoregressive Sentence Ordering](https://doi.org/10.18653/v1/2023.findings-emnlp.277) |  | 0 | Existing sentence ordering approaches generally employ encoder-decoder frameworks with the pointer net to recover the coherence by recurrently predicting each sentence step-by-step. Such an autoregressive manner only leverages unilateral dependencies during decoding and cannot fully explore the... | Bin Ji, Jipeng Zhang, Wenhao Shi, Yang Yang, Yi Bin, Yujuan Ding |  |
| 411 |  |  [Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.278) |  | 0 | With the recent undeniable advancement in reasoning abilities in large language models (LLMs) like ChatGPT and GPT-4, there is a growing trend for using LLMs on various tasks. One area where LLMs can be employed is as an alternative evaluation metric for complex generative tasks, which generally... | Chenhui Shen, Lidong Bing, Liying Cheng, XuanPhi Nguyen, Yang You |  |
| 412 |  |  [Women Wearing Lipstick: Measuring the Bias Between an Object and Its Related Gender](https://doi.org/10.18653/v1/2023.findings-emnlp.279) |  | 0 | In this paper, we investigate the impact of objects on gender bias in image captioning systems. Our results show that only gender-specific objects have a strong gender bias (e.g., women-lipstick). In addition, we propose a visual semantic-based gender score that measures the degree of bias and can... | Ahmed Sabir, Lluís Padró |  |
| 413 |  |  [FREDSum: A Dialogue Summarization Corpus for French Political Debates](https://doi.org/10.18653/v1/2023.findings-emnlp.280) |  | 0 | Recent advances in deep learning, and especially the invention of encoder-decoder architectures, have significantly improved the performance of abstractive summarization systems. While the majority of research has focused on written documents, we have observed an increasing interest in the... | Damien Grari, Guokan Shang, Julie Hunter, Michalis Vazirgiannis, Virgile Rennard |  |
| 414 |  |  [Towards Zero-shot Relation Extraction in Web Mining: A Multimodal Approach with Relative XML Path](https://doi.org/10.18653/v1/2023.findings-emnlp.281) |  | 0 | The rapid growth of web pages and the increasing complexity of their structure poses a challenge for web mining models. Web mining models are required to understand semi-structured web pages, particularly when little is known about the subject or template of a new page. Current methods migrate... | Jingbo Shang, Zilong Wang |  |
| 415 |  |  [Narrative Style and the Spread of Health Misinformation on Twitter](https://doi.org/10.18653/v1/2023.findings-emnlp.282) |  | 0 | Using a narrative style is an effective way to communicate health information both on and off social media. Given the amount of misinformation being spread online and its potential negative effects, it is crucial to investigate the interplay between narrative communication style and misinformative... | Achyutarama R. Ganti, Eslam Ali Hassan Hussein, Steven R. Wilson, Xinyan Zhao, Zexin Ma |  |
| 416 |  |  [HadSkip: Homotopic and Adaptive Layer Skipping of Pre-trained Language Models for Efficient Inference](https://doi.org/10.18653/v1/2023.findings-emnlp.283) |  | 0 | Pre-trained language models (LMs) have brought remarkable performance on numerous NLP tasks. However, they require significant resources and entail high computational costs for inference, making them challenging to deploy in real-world and real-time systems. Existing early exiting methods aim to... | Haoyu Wang, Jing Gao, Tianci Liu, Tuo Zhao, Yaqing Wang |  |
| 417 |  |  [Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.284) |  | 0 | Mental illness remains one of the most critical public health issues of our time, due to the severe scarcity and accessibility limit of professionals. Psychotherapy requires high-level expertise to conduct deep, complex reasoning and analysis on the cognition modeling of the patients. In the era of... | William Yang Wang, Yujie Lu, Zhiyu Chen |  |
| 418 |  |  [Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.285) |  | 0 | While pre-trained language models (PLMs) have shown evidence of acquiring vast amounts of knowledge, it remains unclear how much of this parametric knowledge is actually usable in performing downstream tasks. We propose a systematic framework to measure parametric knowledge utilization in PLMs. Our... | Amirhossein Kazemnejad, Mehdi Rezagholizadeh, Prasanna Parthasarathi, Sarath Chandar |  |
| 419 |  |  [Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.286) |  | 0 | Non-compositional expressions, by virtue of their non-compositionality, are a classic ‘pain in the neck’ for NLP systems. Different from the general language modeling and generation tasks that are primarily compositional, generating non-compositional expressions is more challenging for current... | Hongyu Gong, Jianing Zhou, Suma Bhat, Ziheng Zeng |  |
| 420 |  |  [Information Extraction from Legal Wills: How Well Does GPT-4 Do?](https://doi.org/10.18653/v1/2023.findings-emnlp.287) |  | 0 | This work presents a manually annotated dataset for Information Extraction (IE) from legal wills, and relevant in-context learning experiments on the dataset. The dataset consists of entities, binary relations between the entities (e.g., relations between testator and beneficiary), and n-ary events... | Alice Saebom Kwak, Cheonkam Jeong, Clayton T. Morrison, Derek E. Bambauer, Gaetano Forte, Mihai Surdeanu |  |
| 421 |  |  [Transparency at the Source: Evaluating and Interpreting Language Models With Access to the True Distribution](https://doi.org/10.18653/v1/2023.findings-emnlp.288) |  | 0 | We present a setup for training, evaluating and interpreting neural language models, that uses artificial, language-like data. The data is generated using a massive probabilistic grammar (based on state-split PCFGs), that is itself derived from a large natural language corpus, but also provides us... | Jaap Jumelet, Willem H. Zuidema |  |
| 422 |  |  [Continual Generalized Intent Discovery: Marching Towards Dynamic and Open-world Intent Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.289) |  | 0 | In a practical dialogue system, users may input out-of-domain (OOD) queries. The Generalized Intent Discovery (GID) task aims to discover OOD intents from OOD queries and extend them to the in-domain (IND) classifier. However, GID only considers one stage of OOD learning, and needs to utilize the... | Jinxu Zhao, Keqing He, Pei Wang, Weiran Xu, Xiaoshuai Song, Yueyan Qiu, Yutao Mou |  |
| 423 |  |  [Frugal Prompting for Dialog Models](https://doi.org/10.18653/v1/2023.findings-emnlp.290) |  | 0 | The use of large language models (LLMs) in natural language processing (NLP) tasks is rapidly increasing, leading to changes in how researchers approach problems in the field. To fully utilize these models’ abilities, a better understanding of their behavior for different input protocols is... | Abhinandan De, Bishal Santra, Manish Gupta, Pawan Goyal, Sakya Basak |  |
| 424 |  |  [The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.291) |  | 0 | End-to-end spoken language understanding (SLU) remains elusive even with current large pretrained language models on text and speech, especially in multilingual cases. Machine translation has been established as a powerful pretraining objective on text as it enables the model to capture high-level... | Mutian He, Philip N. Garner |  |
| 425 |  |  [MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space](https://doi.org/10.18653/v1/2023.findings-emnlp.292) |  | 0 | Multi-aspect controllable text generation aims to generate fluent sentences that possess multiple desired attributes simultaneously. Traditional methods either require expensive iteration / searching within the discrete text space during the decoding stage, or train separate controllers for each... | Hanxing Ding, Huawei Shen, Liang Pang, TatSeng Chua, Xueqi Cheng, Zihao Wei |  |
| 426 |  |  [HPE: Answering Complex Questions over Text by Hybrid Question Parsing and Execution](https://doi.org/10.18653/v1/2023.findings-emnlp.293) |  | 0 | The dominant paradigm of textual question answering systems is based on end-to-end neural networks, which excels at answering natural language questions but falls short on complex ones. This stands in contrast to the broad adaptation of semantic parsing approaches over structured data sources... | Caiming Xiong, Dragomir Radev, Rui Meng, Semih Yavuz, Shafiq Joty, Ye Liu, Yingbo Zhou |  |
| 427 |  |  [Length-Adaptive Distillation: Customizing Small Language Model for Dynamic Token Pruning](https://doi.org/10.18653/v1/2023.findings-emnlp.294) |  | 0 | Pre-trained language models greatly improve the performance of various tasks but at a cost of high computation overhead. To facilitate practical applications, there are mainly two lines of research to accelerate model inference: model compression and dynamic computation (e.g., dynamic token... | Chang Liu, Chongyang Tao, Dongyan Zhao, Jianxin Liang, Jiazhan Feng, Quzhe Huang, Tao Shen |  |
| 428 |  |  [Toxicity, Morality, and Speech Act Guided Stance Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.295) |  | 0 | In this work, we focus on the task of determining the public attitude toward various social issues discussed on social media platforms. Platforms such as Twitter, however, are often used to spread misinformation, fake news through polarizing views. Existing literature suggests that higher levels of... | Apoorva Upadhyaya, Marco Fisichella, Wolfgang Nejdl |  |
| 429 |  |  [Reasoning about Ambiguous Definite Descriptions](https://doi.org/10.18653/v1/2023.findings-emnlp.296) |  | 0 | Natural language reasoning plays an increasingly important role in improving language models’ ability to solve complex language understanding tasks. An interesting use case for reasoning is the resolution of context-dependent ambiguity. But no resources exist to evaluate how well Large Language... | Ilia Markov, Peter Bloem, Piek Vossen, Stefan F. Schouten |  |
| 430 |  |  [A Framework for Bidirectional Decoding: Case Study in Morphological Inflection](https://doi.org/10.18653/v1/2023.findings-emnlp.297) |  | 0 | Transformer-based encoder-decoder models that generate outputs in a left-to-right fashion have become standard for sequence-to-sequence tasks. In this paper, we propose a framework for decoding that produces sequences from the “outside-in”: at each step, the model chooses to generate a token on the... | Julia Hockenmaier, Marc E. Canby |  |
| 431 |  |  [Text-guided 3D Human Generation from 2D Collections](https://doi.org/10.18653/v1/2023.findings-emnlp.298) |  | 0 | 3D human modeling has been widely used for engaging interaction in gaming, film, and animation. The customization of these characters is crucial for creativity and scalability, which highlights the importance of controllability. In this work, we introduce Text-guided 3D Human Generation (T3H),... | Barlas Oguz, Jingyu Liu, TsuJui Fu, Wenhan Xiong, William Wang, Yixin Nie |  |
| 432 |  |  [Statistically Profiling Biases in Natural Language Reasoning Datasets and Models](https://doi.org/10.18653/v1/2023.findings-emnlp.299) |  | 0 | Recent studies have shown that many natural language understanding and reasoning datasets contain statistical cues that can be exploited by NLP models, resulting in an overestimation of their capabilities. Existing methods, such as “hypothesis-only” tests and CheckList, are limited in identifying... | Kenny Q. Zhu, Shanshan Huang |  |
| 433 |  |  [Verb Conjugation in Transformers Is Determined by Linear Encodings of Subject Number](https://doi.org/10.18653/v1/2023.findings-emnlp.300) |  | 0 | Deep architectures such as Transformers are sometimes criticized for having uninterpretable “black-box” representations. We use causal intervention analysis to show that, in fact, some linguistic features are represented in a linear, interpretable format. Specifically, we show that BERT’s ability... | Sophie Hao, Tal Linzen |  |
| 434 |  |  [MUX-PLMs: Data Multiplexing for High-throughput Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.301) |  | 0 | The widespread adoption of large language models such as ChatGPT and Bard has led to unprecedented demand for these technologies. The burgeoning cost of inference for ever-increasing model sizes coupled with hardware shortages has limited affordable access and poses a pressing need for efficiency... | Ameet Deshpande, Carlos E. Jimenez, Izhak Shafran, Karthik Narasimhan, Mingqiu Wang, Vishvak Murahari, Yuan Cao |  |
| 435 |  |  [That was the last straw, we need more: Are Translation Systems Sensitive to Disambiguating Context?](https://doi.org/10.18653/v1/2023.findings-emnlp.302) |  | 0 | The translation of ambiguous text presents a challenge for translation systems, as it requires using the surrounding context to disambiguate the intended meaning as much as possible. While prior work has studied ambiguities that result from different grammatical features of the source and target... | Alisa Liu, Hila Gonen, Jaechan Lee, Noah A. Smith, Orevaoghene Ahia |  |
| 436 |  |  [MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic](https://doi.org/10.18653/v1/2023.findings-emnlp.303) |  | 0 | Theory of Mind (ToM) is a critical component of intelligence but its assessment remains the subject of heated debates. Prior research applied human ToM assessments to natural language processing models using either human-created standardized tests or rule-based templates. However, these methods... | Antoine Lernould, Damien Sileo |  |
| 437 |  |  [LATENTLOGIC: Learning Logic Rules in Latent Space over Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.304) |  | 0 | Learning logic rules for knowledge graph reasoning is essential as such rules provide interpretable explanations for reasoning and can be generalized to different domains. However, existing methods often face challenges such as searching in a vast search space (e.g., enumeration of relational paths... | Chenghua Lin, Jianxin Li, Junnan Liu, Qianren Mao, Yangqiu Song |  |
| 438 |  |  [RobustEmbed: Robust Sentence Embeddings Using Self-Supervised Contrastive Pre-Training](https://doi.org/10.18653/v1/2023.findings-emnlp.305) |  | 0 | Pre-trained language models (PLMs) have demonstrated their exceptional performance across a wide range of natural language processing tasks. The utilization of PLM-based sentence embeddings enables the generation of contextual representations that capture rich semantic information. However, despite... | Daniel Takabi, Eduardo Blanco, Javad Rafiei Asl |  |
| 439 |  |  [More than Votes? Voting and Language based Partisanship in the US Supreme Court](https://doi.org/10.18653/v1/2023.findings-emnlp.306) |  | 0 | Understanding the prevalence and dynamics of justice partisanship and ideology in the US Supreme Court is critical in studying jurisdiction. Most research quantifies partisanship based on voting behavior, and oral arguments in the courtroom — the last essential procedure before the final case... | Biaoyan Fang, Lea Frermann, Timothy Baldwin, Trevor Cohn |  |
| 440 |  |  [Automatic Evaluation of Attribution by Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.307) |  | 0 | A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited... | Boshi Wang, Huan Sun, Kai Zhang, Xiang Yue, Yu Su, Ziru Chen |  |
| 441 |  |  [Modeling Highlighting of Metaphors in Multitask Contrastive Learning Paradigms](https://doi.org/10.18653/v1/2023.findings-emnlp.308) |  | 0 | Metaphorical language, such as “spending time together”, projects meaning from a source domain (here, money) to a target domain (time). Thereby, it highlights certain aspects of the target domain, such as the effort behind the time investment. Highlighting aspects with metaphors (while hiding... | Henning Wachsmuth, Ingrid Scharlau, Meghdut Sengupta, Milad Alshomary |  |
| 442 |  |  [LDM²: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement](https://doi.org/10.18653/v1/2023.findings-emnlp.309) |  | 0 | With the rapid development of large language models (LLMs), it is highly demanded that LLMs can be adopted to make decisions to enable the artificial general intelligence. Most approaches leverage manually crafted examples to prompt the LLMs to imitate the decision process of human. However,... | Daniel Dajun Zeng, Linjing Li, Xingjin Wang |  |
| 443 |  |  [ZARA: Improving Few-Shot Self-Rationalization for Small Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.310) |  | 0 | Language models (LMs) that jointly generate end-task answers as well as free-text rationales are known as self-rationalization models. Recent works demonstrate great performance gain for self-rationalization by few-shot prompting LMs with rationale-augmented exemplars. However, the ability to... | AnZi Yen, ChengKuang Wu, HenHsen Huang, HsinHsi Chen, WeiLin Chen |  |
| 444 |  |  [ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation](https://doi.org/10.18653/v1/2023.findings-emnlp.311) |  | 0 | Despite remarkable advances that large language models have achieved in chatbots nowadays, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social... | Jingbo Shang, Yangkun Wang, Yongqi Tong, Yujia Wang, Yuxin Guo, Zi Lin, Zihan Wang |  |
| 445 |  |  [Mind the Gap: Automated Corpus Creation for Enthymeme Detection and Reconstruction in Learner Arguments](https://doi.org/10.18653/v1/2023.findings-emnlp.312) |  | 0 | Writing strong arguments can be challenging for learners. It requires to select and arrange multiple argumentative discourse units (ADUs) in a logical and coherent way as well as to decide which ADUs to leave implicit, so called enthymemes. However, when important ADUs are missing, readers might... | Henning Wachsmuth, Maja Stahl, MeiHua Chen, Nick Düsterhus |  |
| 446 |  |  [Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.313) |  | 0 | Current variational dialog models have employed pre-trained language models (PLMs) to parameterize the likelihood and posterior distributions. However, the Gaussian assumption made on the prior distribution is incompatible with these distributions, thus restricting the diversity of generated... | Iryna Gurevych, Thy Thy Tran, Tianyu Yang |  |
| 447 |  |  [Retrieving Multimodal Information for Augmented Generation: A Survey](https://doi.org/10.18653/v1/2023.findings-emnlp.314) |  | 0 | As Large Language Models (LLMs) become popular, there emerged an important trend of using multimodality to augment the LLMs’ generation ability, which enables LLMs to better interact with the world. However, there lacks a unified perception of at which stage and how to incorporate different... | Bosheng Ding, Chengwei Qin, Do Xuan Long, Fangkai Jiao, Hailin Chen, Minzhi Li, Ruochen Zhao, Shafiq Joty, Weishi Wang, Xiaobao Guo, Xingxuan Li |  |
| 448 |  |  [Improving Contrastive Learning of Sentence Embeddings with Focal InfoNCE](https://doi.org/10.18653/v1/2023.findings-emnlp.315) |  | 0 | The recent success of SimCSE has greatly advanced state-of-the-art sentence representations. However, the original formulation of SimCSE does not fully exploit the potential of hard negative samples in contrastive learning. This study introduces an unsupervised contrastive learning framework that... | Pengyue Hou, Xingyu Li |  |
| 449 |  |  [The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.316) |  | 0 | We present The Vault, an open-source dataset of high quality code-text pairs in multiple programming languages for training large language models to understand and generate code. We propose methods for thoroughly extracting samples that use both rules and deep learning to ensure that they contain... | Anh Minh Nguyen, Anh T. V. Dau, Dung Nguyen Manh, Jin Guo, Khanh Nghiem, Le Nam Hai, Nghi D. Q. Bui |  |
| 450 |  |  [SDOH-NLI: a Dataset for Inferring Social Determinants of Health from Clinical Notes](https://doi.org/10.18653/v1/2023.findings-emnlp.317) |  | 0 | Social and behavioral determinants of health (SDOH) play a significant role in shaping health outcomes, and extracting these determinants from clinical notes is a first step to help healthcare providers systematically identify opportunities to provide appropriate care and address disparities.... | Alvin Rajkomar, Eric Loreaux, MingJun Chen, Tal Schuster, Ádám D. Lelkes |  |
| 451 |  |  [On the Zero-Shot Generalization of Machine-Generated Text Detectors](https://doi.org/10.18653/v1/2023.findings-emnlp.318) |  | 0 | The rampant proliferation of large language models, fluent enough to generate text indistinguishable from human-written language, gives unprecedented importance to the detection of machine-generated text. This work is motivated by an important research question: How will the detectors of... | Jingyu Zhang, Tianxing He, Xiao Pu, Xiaochuang Han, Yulia Tsvetkov |  |
| 452 |  |  [Complex Event Schema Induction with Knowledge-Enriched Diffusion Model](https://doi.org/10.18653/v1/2023.findings-emnlp.319) |  | 0 | The concept of a complex event schema pertains to the graph structure that represents real-world knowledge of events and their multi-dimensional relationships. However, previous studies on event schema induction have been hindered by challenges such as error propagation and data quality issues. To... | Huaijun Li, Jiexin Xu, Jun Zhao, Kang Liu, Pengfei Cao, Xiaojian Jiang, Yubo Chen, Yupu Hao |  |
| 453 |  |  [Exploiting Emotion-Semantic Correlations for Empathetic Response Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.320) |  | 0 | Empathetic response generation aims to generate empathetic responses by understanding the speaker’s emotional feelings from the language of dialogue. Recent methods capture emotional words in the language of communicators and construct them as static vectors to perceive nuanced emotions. However,... | Sibo Ju, Tiecheng Cai, Xiangwen Liao, Xiaofei Zhu, Yisong Su, Yufeng Wang, Yunbing Wu, Zhaochun Ren, Zhihao Chen, Zhou Yang |  |
| 454 |  |  [Long-Range Language Modeling with Selective Cache](https://doi.org/10.18653/v1/2023.findings-emnlp.321) |  | 0 | The computational cost of transformer-based language models grows quadratically with the sequence length. In this paper, we introduce the selective cache, which stores the selected key-value pairs from the previous context. By selecting important key-value pairs the model makes better use of the... | Nora Hollenstein, Xinting Huang |  |
| 455 |  |  [Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding](https://doi.org/10.18653/v1/2023.findings-emnlp.322) |  | 0 | Text simplification has emerged as an increasingly useful application of AI for bridging the communication gap in specialized fields such as medicine, where the lexicon is often dominated by technical jargon and complex constructs. Despite notable progress, methods in medical simplification... | Arman Cohan, Heyuan Huang, Kejian Shi, Lorenzo Jaime Yu Flores, Sophie Chheang |  |
| 456 |  |  [FaLA: Fast Linear Adaptation for Replacing Backbone Models on Edge Devices](https://doi.org/10.18653/v1/2023.findings-emnlp.323) |  | 0 | In this work, we study the language model backbone replacement problem for personalized downstream tasks in a non-stationary on-device scenario. In real world, company may periodically update the knowledge and architectures of backbones to keep the competitive in the market, meanwhile, to... | Chunyang Chen, Lizhen Qu, Shuo Huang, Xingliang Yuan |  |
| 457 |  |  [Intuitive Multilingual Audio-Visual Speech Recognition with a Single-Trained Model](https://doi.org/10.18653/v1/2023.findings-emnlp.324) |  | 0 | We present a novel approach to multilingual audio-visual speech recognition tasks by introducing a single model on a multilingual dataset. Motivated by a human cognitive system where humans can intuitively distinguish different languages without any conscious effort or guidance, we propose a model... | Joanna Hong, Se Jin Park, Yong Man Ro |  |
| 458 |  |  [Controllable Chest X-Ray Report Generation from Longitudinal Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.325) |  | 0 | Radiology reports are detailed text descriptions of the content of medical scans. Each report describes the presence/absence and location of relevant clinical findings, commonly including comparison with prior exams of the same patient to describe how they evolved. Radiology reporting is a... | Alison O'Neil, Chaoyang Wang, Fani Deligianni, Francesco Dalla Serra, Jeff Dalton |  |
| 459 |  |  [Is ChatGPT a Good Multi-Party Conversation Solver?](https://doi.org/10.18653/v1/2023.findings-emnlp.326) |  | 0 | Large Language Models (LLMs) have emerged as influential instruments within the realm of natural language processing; nevertheless, their capacity to handle multi-party conversations (MPCs) – a scenario marked by the presence of multiple interlocutors involved in intricate information exchanges –... | ChaoHong Tan, JiaChen Gu, ZhenHua Ling |  |
| 460 |  |  [Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis](https://doi.org/10.18653/v1/2023.findings-emnlp.327) |  | 0 | Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We... | Jianqiao Lu, Nianzu Zheng, Wenyong Huang, Xiao Chen, Xingshan Zeng, Yu Ting Yeung |  |
| 461 |  |  [Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders](https://doi.org/10.18653/v1/2023.findings-emnlp.328) |  | 0 | Pre-trained sentence representations are crucial for identifying significant sentences in unsupervised document extractive summarization. However, the traditional two-step paradigm of pre-training and sentence-ranking, creates a gap due to differing optimization objectives. To address this issue,... | Bo Li, Jianxin Li, Jiarui Li, Qianren Mao, Shaobo Zhao, Shizhu He, Xiaolei Gu |  |
| 462 |  |  [Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.329) |  | 0 | Prompt tuning, in which prompts are optimized to adapt large-scale pre-trained language models to downstream tasks instead of fine-tuning the full model parameters, has been shown to be particularly effective when the prompts are trained in the multi-task transfer learning setting. These methods... | Haeju Lee, KeeEung Kim, Minchan Jeong, SeYoung Yun |  |
| 463 |  |  [CCIM: Cross-modal Cross-lingual Interactive Image Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.330) |  | 0 | Text image machine translation (TIMT) which translates source language text images into target language texts has attracted intensive attention in recent years. Although the end-to-end TIMT model directly generates target translation from encoded text image features with an efficient architecture,... | Chengqing Zong, Cong Ma, Mei Tu, Yang Zhao, Yaping Zhang, Yu Zhou |  |
| 464 |  |  [TRAMS: Training-free Memory Selection for Long-range Language Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.331) |  | 0 | The Transformer architecture is crucial for numerous AI models, but it still faces challenges in long-range language modeling. Though several specific transformer architectures have been designed to tackle issues of long-range dependencies, existing methods like Transformer-XL are plagued by a high... | Cunxiang Wang, Haofei Yu, Wei Bi, Yue Zhang |  |
| 465 |  |  [A Critical Analysis of Document Out-of-Distribution Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.332) |  | 0 | Large-scale pre-training is widely used in recent document understanding tasks. During deployment, one may expect that models should trigger a conservative fallback policy when encountering out-of-distribution (OOD) samples, which highlights the importance of OOD detection. However, most existing... | Ani Nenkova, Anqi Liu, Handong Zhao, Jason Kuen, Jiuxiang Gu, Nikolaos Barmpalios, Ruiyi Zhang, Tong Sun, Vlad I. Morariu, Yi Zhou, Yifei Ming, Yixuan Li |  |
| 466 |  |  [Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.333) |  | 0 | Improving neural machine translation (NMT) systems with prompting has achieved significant progress in recent years. In this work, we focus on how to integrate multi-knowledge, multiple types of knowledge, into NMT models to enhance the performance with prompting. We propose a unified framework,... | Jun Xie, Ke Wang, Yu Zhao, Yuqi Zhang |  |
| 467 |  |  [Active Learning Principles for In-Context Learning with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.334) |  | 0 | The remarkable advancements in large language models (LLMs) have significantly enhanced predictive performance in few-shot learning settings. By using only a small number of labeled examples, referred to as demonstrations, LLMs can effectively perform the task at hand through in-context learning.... | Jane DwivediYu, Katerina Margatina, Nikolaos Aletras, Timo Schick |  |
| 468 |  |  [InteMATs: Integrating Granularity-Specific Multilingual Adapters for Cross-Lingual Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.335) |  | 0 | Multilingual language models (MLLMs) have achieved remarkable success in various cross-lingual transfer tasks. However, they suffer poor performance in zero-shot low-resource languages, particularly when dealing with longer contexts. Existing research mainly relies on full-model fine-tuning on... | Fengyu Zhou, Jiakai He, Jianye Chen, Meizhen Liu, Siu Cheung Hui, Xu Guo |  |
| 469 |  |  [PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.336) |  | 0 | The patient-centered medical dialogue systems strive to offer diagnostic interpretation services to users who are less knowledgeable about medical knowledge, through emphasizing the importance of providing responses specific to the patients. It is difficult for the large language models (LLMs) to... | Chengfeng Dou, Haiyan Zhao, Wenpin Jiao, Yongqiang Zhao, Zhengwei Tao, Zhi Jin |  |
| 470 |  |  [CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.337) |  | 0 | Recent code translation techniques exploit neural machine translation models to translate source code from one programming language to another to satisfy production compatibility or to improve efficiency of codebase maintenance. Most existing code translation datasets only focus on a single pair of... | Qian Chen, Weixiang Yan, Wen Wang, Yuchen Tian, Yunzhe Li |  |
| 471 |  |  [impact of sample selection on in-context learning for entity extraction from scientific writing](https://doi.org/10.18653/v1/2023.findings-emnlp.338) |  | 0 | Prompt-based usage of Large Language Models (LLMs) is an increasingly popular way to tackle many well-known natural language problems. This trend is due, in part, to the appeal of the In-Context Learning (ICL) prompt set-up, in which a few selected training examples are provided along with the... | Maciej Rybinski, Necva Bölücü, Stephen Wan |  |
| 472 |  |  [Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models](https://doi.org/10.18653/v1/2023.findings-emnlp.339) |  | 0 | Considerable effort has been dedicated to mitigating toxicity, but existing methods often require drastic modifications to model parameters or the use of computationally intensive auxiliary models. Furthermore, previous approaches have often neglected the crucial factor of language’s evolving... | Beyza Ermis, Luiza Pozzobon, Patrick Lewis, Sara Hooker |  |
| 473 |  |  [Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks](https://doi.org/10.18653/v1/2023.findings-emnlp.340) |  | 0 | We investigate MT evaluation metric performance on adversarially-synthesized texts, to shed light on metric robustness. We experiment with word- and character-level attacks on three popular machine translation metrics: BERTScore, BLEURT, and COMET. Our human experiments validate that automatic... | Timothy Baldwin, Yichen Huang |  |
| 474 |  |  [Time-Considerable Dialogue Models via Reranking by Time Dependency](https://doi.org/10.18653/v1/2023.findings-emnlp.341) |  | 0 | In the last few years, generative dialogue models have shown excellent performance and have been used for various applications. As chatbots become more prevalent in our daily lives, more and more people expect them to behave more like humans, but existing dialogue models do not consider the time... | Hiroaki Sugiyama, Masakazu Ishihata, Yuiko Tsunomori |  |
| 475 |  |  [Non-Compositionality in Sentiment: New Data and Analyses](https://doi.org/10.18653/v1/2023.findings-emnlp.342) |  | 0 | When natural language phrases are combined, their meaning is often more than the sum of their parts. In the context of NLP tasks such as sentiment analysis, where the meaning of a phrase is its sentiment, that still applies. Many NLP studies on sentiment analysis, however, focus on the fact that... | Christopher Lucas, Verna Dankers |  |
| 476 |  |  [MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.343) |  | 0 | The large language models have achieved superior performance on various natural language tasks. One major drawback of such approaches is they are resource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a resource-efficient solution to fine-tune the pre-trained language models... | Bowen Wang, Guoxin Chen, Liangzhi Li, Yiming Qian |  |
| 477 |  |  [DocTrack: A Visually-Rich Document Dataset Really Aligned with Human Eye Movement for Machine Reading](https://doi.org/10.18653/v1/2023.findings-emnlp.344) |  | 0 | The use of visually-rich documents in various fields has created a demand for Document AI models that can read and comprehend documents like humans, which requires the overcoming of technical, linguistic, and cognitive barriers. Unfortunately, the lack of appropriate datasets has significantly... | Changqing Wang, Chenhui Chu, Hao Wang, Qingxuan Wang, Rui Wang, Yue Li |  |
| 478 |  |  [Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.345) |  | 0 | Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. \*Selective prediction\* is a technique that... | Jiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan Ö. Arik, Somesh Jha, Tomas Pfister |  |
| 479 |  |  [Bi-Drop: Enhancing Fine-tuning Generalization via Synchronous sub-net Estimation and Optimization](https://doi.org/10.18653/v1/2023.findings-emnlp.346) |  | 0 | Pretrained language models have achieved remarkable success in natural language understanding. However, fine-tuning pretrained models on limited training data tends to overfit and thus diminish performance. This paper presents Bi-Drop, a fine-tuning strategy that selectively updates model... | Binghuai Lin, Damai Dai, Heming Xia, Runxin Xu, Shoujie Tong, Tianyu Liu, Yunbo Cao, Zhifang Sui |  |
| 480 |  |  [ClozEx: A Task toward Generation of English Cloze Explanation](https://doi.org/10.18653/v1/2023.findings-emnlp.347) |  | 0 | Providing explanations for cloze questions in language assessment (LA) has been recognized as a valuable approach to enhancing the language proficiency of learners. However, there is a noticeable absence of dedicated tasks and datasets specifically designed for generating language learner... | Mamoru Komachi, Masato Mita, Zizheng Zhang |  |
| 481 |  |  [Is Probing All You Need? Indicator Tasks as an Alternative to Probing Embedding Spaces](https://doi.org/10.18653/v1/2023.findings-emnlp.348) |  | 0 | The ability to identify and control different kinds of linguistic information encoded in vector representations of words has many use cases, especially for explainability and bias removal. This is usually done via a set of simple classification tasks, termed probes, to evaluate the information... | Omer Goldman, Reut Tsarfaty, Tal Levy |  |
| 482 |  |  [The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.349) |  | 0 | Compressing large language models (LLMs), often consisting of billions of parameters, provides faster inference, smaller memory footprints, and enables local deployment. The standard compression techniques are pruning and quantization, with the former eliminating redundant connections in model... | Frederic Sala, Makesh Sreedhar, Satya Sai Srinath Namburi, Srinath Srinivasan |  |
| 483 |  |  [CoEdIT: Text Editing by Task-Specific Instruction Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.350) |  | 0 | We introduce CoEdIT, a state-of-the-art text editing system for writing assistance. CoEdIT takes instructions from the user specifying the attributes of the desired text, such as “Make the sentence simpler” or “Write it in a more neutral style,” and outputs the edited text. We present a large... | Dhruv Kumar, Dongyeop Kang, Ryan Koo, Vipul Raheja |  |
| 484 |  |  [Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.351) |  | 0 | Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes.... | Fei Huang, Hao Lang, Kaisheng Zeng, Yi Dai, Yongbin Li |  |
| 485 |  |  [Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information](https://doi.org/10.18653/v1/2023.findings-emnlp.352) |  | 0 | Real-world Knowledge Graphs (KGs) often suffer from incompleteness, which limits their potential performance. Knowledge Graph Completion (KGC) techniques aim to address this issue. However, traditional KGC methods are computationally intensive and impractical for large-scale KGs, necessitating the... | Alla Chepurova, Aydar Bulatov, Mikhail Burtsev, Yuri Kuratov |  |
| 486 |  |  [DeltaScore: Fine-Grained Story Evaluation with Perturbations](https://doi.org/10.18653/v1/2023.findings-emnlp.353) |  | 0 | Numerous evaluation metrics have been developed for natural language generation tasks, but their effectiveness in evaluating stories is limited as they are not specifically tailored to assess intricate aspects of storytelling, such as fluency and interestingness. In this paper, we introduce... | Jey Han Lau, Miao Li, Trevor Cohn, Zhuohan Xie |  |
| 487 |  |  [MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields](https://doi.org/10.18653/v1/2023.findings-emnlp.354) |  | 0 | Previous research has demonstrated the advantages of integrating data from multiple sources over traditional unimodal data, leading to the emergence of numerous novel multimodal applications. We propose a multimodal classification benchmark MuG with eight datasets that allows researchers to... | Carl Yang, Jiaying Lu, Shifan Zhao, Yongchen Qian, Yuanzhe Xi |  |
| 488 |  |  [Don't waste a single annotation: improving single-label classifiers through soft labels](https://doi.org/10.18653/v1/2023.findings-emnlp.355) |  | 0 | In this paper, we address the limitations of the common data annotation and training methods for objective single-label classification tasks. Typically, when annotating such tasks annotators are only asked to provide a single label for each sample and annotator disagreement is discarded when a... | Ben Wu, Carolina Scarton, Kalina Bontcheva, Xingyi Song, Yida Mu, Yue Li |  |
| 489 |  |  [Black-Box Tuning of Vision-Language Models with Effective Gradient Approximation](https://doi.org/10.18653/v1/2023.findings-emnlp.356) |  | 0 | Parameter-efficient fine-tuning (PEFT) methods have provided an effective way for adapting large vision-language models to specific tasks or scenarios. Typically, they learn a very small scale of parameters for pre-trained models in a white-box formulation, which assumes model architectures to be... | Jinfeng Bai, Ming Liu, Wangmeng Zuo, Yiwen Guo, Yuxiang Wei, Zhilong Ji, Zixian Guo |  |
| 490 |  |  [How to Determine the Most Powerful Pre-trained Language Model without Brute Force Fine-tuning? An Empirical Survey](https://doi.org/10.18653/v1/2023.findings-emnlp.357) |  | 0 | Transferability estimation has been attached to great attention in the computer vision fields. Researchers try to estimate with low computational cost the performance of a model when transferred from a source task to a given target task. Considering the effectiveness of such estimations, the... | Chen Li, Chenghua Lin, Hanhua Hong, Jun Bai, Wenge Rong, Xi Xu, Xiaofeng Zhang |  |
| 491 |  |  [Licon: A Diverse, Controllable and Challenging Linguistic Concept Learning Benchmark](https://doi.org/10.18653/v1/2023.findings-emnlp.358) |  | 0 | Concept Learning requires learning the definition of a general category from given training examples. Most of the existing methods focus on learning concepts from images. However, the visual information cannot present abstract concepts exactly, which struggles the introduction of novel concepts... | Ru Zhou, Shenglong Yu, Wenya Guo, Xiaojie Yuan, Ying Zhang, Zhengkun Zhang |  |
| 492 |  |  [InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations](https://doi.org/10.18653/v1/2023.findings-emnlp.359) |  | 0 | While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations... | Cennet Oguz, Nils Feldhus, Qianli Wang, Sahil Chopra, Sebastian Möller, Tatiana Anikina |  |
| 493 |  |  [INVITE: a Testbed of Automatically Generated Invalid Questions to Evaluate Large Language Models for Hallucinations](https://doi.org/10.18653/v1/2023.findings-emnlp.360) |  | 0 | Recent advancements in Large language models (LLMs) have enabled them to hold free form conversations over multiple turns, but they exhibit a tendency to make unfounded and incorrect statements, commonly known as hallucinations. In particular, LLMs hallucinate frequently when given invalid... | Anil Ramakrishna, Jens Lehmann, Morteza Ziyadi, Rahul Gupta |  |
| 494 |  |  [Multimodal Automated Fact-Checking: A Survey](https://doi.org/10.18653/v1/2023.findings-emnlp.361) |  | 0 | Misinformation is often conveyed in multiple modalities, e.g. a miscaptioned image. Multimodal misinformation is perceived as more credible by humans, and spreads faster than its text-only counterparts. While an increasing body of research investigates automated fact-checking (AFC), previous... | Andreas Vlachos, Elena Simperl, Michael Sejr Schlichtkrull, Mubashara Akhtar, Oana Cocarascu, Zhijiang Guo |  |
| 495 |  |  [PROTEGE: Prompt-based Diverse Question Generation from Web Articles](https://doi.org/10.18653/v1/2023.findings-emnlp.362) |  | 0 | Rich and diverse knowledge bases (KB) are foundational building blocks for online knowledge sharing communities such as StackOverflow and Quora, and applications such as conversational assistants (aka chatbots). A popular format for knowledge bases is question-answer pairs (or FAQs), where... | Anirban Majumder, Vinayak Puranik, Vineet Chaoji |  |
| 496 |  |  [GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions](https://doi.org/10.18653/v1/2023.findings-emnlp.363) |  | 0 | There is growing interest in systems that generate captions for scientific figures. However, assessing these systems’ output poses a significant challenge. Human evaluation requires academic expertise and is costly, while automatic evaluation depends on often low-quality author-written captions.... | C. Lee Giles, ChiehYang Huang, Ryan A. Rossi, Sungchul Kim, TingHao Kenneth Huang, TingYao Hsu |  |
| 497 |  |  [Mulan: A Multi-Level Alignment Model for Video Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.364) |  | 0 | Video Question Answering (VideoQA) aims to answer questions about the visual content of a video. Current methods mainly focus on improving joint representations of video and text. However, these methods pay little attention to the fine-grained semantic interaction between video and text. In this... | Cong Cao, Dakui Wang, Fangfang Yuan, Yanbing Liu, Yu Fu, Yuhai Lu, Yuling Yang |  |
| 498 |  |  [HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.365) |  | 0 | With the proliferation of social media, accurate detection of hate speech has become critical to ensure safety online. To combat nuanced forms of hate speech, it is important to identify and thoroughly explain hate speech to help users understand its harmful effects. Recent benchmarks have... | James Thorne, Joonkee Kim, Namgyu Ho, SeYoung Yun, Yongjin Yang, Yujin Kim |  |
| 499 |  |  [ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.366) |  | 0 | Predicting chemical reactions, a fundamental challenge in chemistry, involves forecasting the resulting products from a given reaction process. Conventional techniques, notably those employing Graph Neural Networks (GNNs), are often limited by insufficient training data and their inability to... | An Zhang, Enzhi Zhang, Xiang Wang, Yaorui Shi, Zhiyuan Liu |  |
| 500 |  |  [Decomposing Complex Queries for Tip-of-the-tongue Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.367) |  | 0 | When re-finding items, users who forget or are uncertain about identifying details often rely on creative strategies for expressing their information needs—complex queries that describe content elements (e.g., book characters or events), information beyond the document text (e.g., descriptions of... | Dan Klein, Joseph Gonzalez, Kevin Lin, Kyle Lo |  |
| 501 |  |  [Values, Ethics, Morals? On the Use of Moral Concepts in NLP Research](https://doi.org/10.18653/v1/2023.findings-emnlp.368) |  | 0 | With language technology increasingly affecting individuals’ lives, many recent works have investigated the ethical aspects of NLP. Among other topics, researchers focused on the notion of morality, investigating, for example, which moral judgements language models make. However, there has been... | Anne Lauscher, Judith Simon, Karina Vida |  |
| 502 |  |  [Self-Supervised Behavior Cloned Transformers are Path Crawlers for Text Games](https://doi.org/10.18653/v1/2023.findings-emnlp.369) |  | 0 | In this work, we introduce a self-supervised behavior cloning transformer for text games, which are challenging benchmarks for multi-step reasoning in virtual environments. Traditionally, Behavior Cloning Transformers excel in such tasks but rely on supervised training data. Our approach... | Peter A. Jansen, Ruoyao Wang |  |
| 503 |  |  [Adapting Pretrained Text-to-Text Models for Long Text Sequences](https://doi.org/10.18653/v1/2023.findings-emnlp.370) |  | 0 | We present an empirical study of adapting an existing pretrained text-to-text model for long-sequence inputs. Through a comprehensive study along three axes of the pretraining pipeline – model architecture, optimization objective, and pretraining corpus, we propose an effective recipe to build... | Anchit Gupta, Scott Yih, Shubham Toshniwal, Wenhan Xiong, Yashar Mehdad |  |
| 504 |  |  [xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark](https://doi.org/10.18653/v1/2023.findings-emnlp.371) |  | 0 | Recent advancements in reference-free learned metrics for open-domain dialogue evaluation have been driven by the progress in pre-trained language models and the availability of dialogue data with high-quality human annotations. However, current studies predominantly concentrate on English... | Chen Zhang, Chengguang Tang, Guohua Tang, Haizhou Li, Ke Shi, Luis F. D'Haro |  |
| 505 |  |  [MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems](https://doi.org/10.18653/v1/2023.findings-emnlp.372) |  | 0 | While automatic dialogue tutors hold great potential in making education personalized and more accessible, research on such systems has been hampered by a lack of sufficiently large and high-quality datasets. Collecting such datasets remains challenging, as recording tutoring sessions raises... | Iryna Gurevych, Jakub Macina, Manu Kapur, Mrinmaya Sachan, Nico Daheim, Sankalan Pal Chowdhury, Tanmay Sinha |  |
| 506 |  |  [Towards Making the Most of ChatGPT for Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.373) |  | 0 | ChatGPT shows remarkable capabilities for machine translation (MT). Several prior studies have shown that it achieves comparable results to commercial systems for high-resource languages, but lags behind in complex tasks, e.g, low-resource and distant-language-pairs translation. However, they... | Dacheng Tao, Keqin Peng, Li Shen, Liang Ding, Min Zhang, Qihuang Zhong, Xuebo Liu, Yuanxin Ouyang |  |
| 507 |  |  [Enhancing Reasoning Capabilities by Instruction Learning and Chain-of-Thoughts for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.374) |  | 0 | The aim of implicit discourse relation recognition is to comprehend the sense of connection between two arguments. In this work, we present a classification method that is solely based on generative models. Our proposed approach employs a combination of instruction templates and in-context learning... | Guodong Zhou, Yu Hong, Yuxiang Lu, Zhipang Wang |  |
| 508 |  |  [Large-Scale and Multi-Perspective Opinion Summarization with Diverse Review Subsets](https://doi.org/10.18653/v1/2023.findings-emnlp.375) |  | 0 | Opinion summarization is expected to digest larger review sets and provide summaries from different perspectives. However, most existing solutions are deficient in epitomizing extensive reviews and offering opinion summaries from various angles due to the lack of designs for information selection.... | Han Jiang, Rui Wang, Xinpeng Wang, Yu Li, Zhihua Wei |  |
| 509 |  |  [Topic-Informed Dialogue Summarization using Topic Distribution and Prompt-based Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.376) |  | 0 | Dealing with multiple topics should be considered an important issue in dialogue summarization, because dialogues, unlike documents, are prone to topic drift. Thus, we propose a new dialogue summarization model that reflects dialogue topic distribution to consider all topics present in the... | Jaeah You, Youngjoong Ko |  |
| 510 |  |  [Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy](https://doi.org/10.18653/v1/2023.findings-emnlp.377) |  | 0 | We address an important gap in detecting political bias in news articles. Previous works that perform document classification can be influenced by the writing style of each news outlet, leading to overfitting and limited generalizability. Our approach overcomes this limitation by considering both... | Jaemin Jung, James Thorne, Jiwoo Hong, Jiyoung Han, Yejin Cho |  |
| 511 |  |  [Measuring and Narrowing the Compositionality Gap in Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.378) |  | 0 | We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the... | Ludwig Schmidt, Mike Lewis, Muru Zhang, Noah A. Smith, Ofir Press, Sewon Min |  |
| 512 |  |  [Unsupervised Candidate Answer Extraction through Differentiable Masker-Reconstructor Model](https://doi.org/10.18653/v1/2023.findings-emnlp.379) |  | 0 | Question generation is a widely used data augmentation approach with extensive applications, and extracting qualified candidate answers from context passages is a critical step for most question generation systems. However, existing methods for candidate answer extraction are reliant on linguistic... | James Caverlee, Yicheng Wang, Zhuoer Wang, Ziwei Zhu |  |
| 513 |  |  [HoneyBee: Progressive Instruction Finetuning of Large Language Models for Materials Science](https://doi.org/10.18653/v1/2023.findings-emnlp.380) |  | 0 | We propose an instruction-based process for trustworthy data curation in materials science (MatSci-Instruct), which we then apply to finetune a LLaMa-based language model targeted for materials science (HoneyBee). MatSci-Instruct helps alleviate the scarcity of relevant, high-quality materials... | Bang Liu, Huan Zhang, Santiago Miret, Yu Song |  |
| 514 |  |  [Prompt-Based Editing for Text Style Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.381) |  | 0 | Prompting approaches have been recently explored in text style transfer, where a textual prompt is used to query a pretrained language model (PLM) to generate style-transferred texts word by word in an autoregressive manner. However, such a generation process is less controllable and early... | Guoqing Luo, Lili Mou, Mauajama Firdaus, Yutong Han |  |
| 515 |  |  [Representativeness as a Forgotten Lesson for Multilingual and Code-switched Data Collection and Preparation](https://doi.org/10.18653/v1/2023.findings-emnlp.382) |  | 0 | Multilingualism is widespread around the world and code-switching (CSW) is a common practice among different language pairs/tuples across locations and regions. However, there is still not much progress in building successful CSW systems, despite the recent advances in Massive Multilingual Language... | A. Seza Dogruöz, Sunayana Sitaram, Zheng Xin Yong |  |
| 516 |  |  [NERvous About My Health: Constructing a Bengali Medical Named Entity Recognition Dataset](https://doi.org/10.18653/v1/2023.findings-emnlp.383) |  | 0 | The ability to identify important entities in a text, known as Named Entity Recognition (NER), is useful in a large variety of downstream tasks in the biomedical domain. This is a considerably difficult task when working with Consumer Health Questions (CHQs), which consist of informal language used... | Alvi Khan, Fida Kamal, Nuzhat Nower, Sabbir Ahmed, Tareque Chowdhury, Tasnim Ahmed |  |
| 517 |  |  [Sparse Black-Box Multimodal Attack for Vision-Language Adversary Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.384) |  | 0 | Deep neural networks have been widely applied in real-world scenarios, such as product restrictions on e-commerce and hate speech monitoring on social media, to ensure secure governance of various platforms. However, illegal merchants often deceive the detection models by adding large-scale... | Haojun Fu, Hui Xue, Kun He, Meihui Lian, Weigao Wen, Zhen Yu, Zhenhua Chen, Zhou Qin |  |
| 518 |  |  [Towards a Unified Framework for Reference Retrieval and Related Work Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.385) |  | 0 | The task of related work generation aims to generate a comprehensive survey of related research topics automatically, saving time and effort for authors. Existing methods simplify this task by using human-annotated references in a large-scale scientific corpus as information sources, which is time-... | Pengjie Ren, Shen Gao, Xiuying Chen, Zhaochun Ren, Zhen Zhang, Zhengliang Shi, Zhumin Chen |  |
| 519 |  |  [Visual Storytelling with Question-Answer Plans](https://doi.org/10.18653/v1/2023.findings-emnlp.386) |  | 0 | Visual storytelling aims to generate compelling narratives from image sequences. Existing models often focus on enhancing the representation of the image sequence, e.g., with external knowledge sources or advanced graph structures. Despite recent progress, the stories are often repetitive,... | Danyang Liu, Frank Keller, Mirella Lapata |  |
| 520 |  |  [Investigating Online Community Engagement through Stancetaking](https://doi.org/10.18653/v1/2023.findings-emnlp.387) |  | 0 | Much work has explored lexical and semantic variation in online communities, and drawn connections to community identity and user engagement patterns. Communities also express identity through the sociolinguistic concept of stancetaking. Large-scale computational work on stancetaking has explored... | Brian Diep, Jai Aggarwal, Julia Watson, Suzanne Stevenson |  |
| 521 |  |  [ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.388) |  | 0 | As large language models are integrated into society, robustness toward a suite of prompts is increasingly important to maintain reliability in a high-variance environment.Robustness evaluations must comprehensively encapsulate the various settings in which a user may invoke an intelligent system.... | Alex Mei, Sharon Levy, William Yang Wang |  |
| 522 |  |  [Learning to Correct Noisy Labels for Fine-Grained Entity Typing via Co-Prediction Prompt Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.389) |  | 0 | Fine-grained entity typing (FET) is an essential task in natural language processing that aims to assign semantic types to entities in text. However, FET poses a major challenge known as the noise labeling problem, whereby current methods rely on estimating noise distribution to identify noisy... | Hongbo Xu, Minghao Tang, Wenyuan Zhang, Yang Lin, Yongquan He, Yongxiu Xu |  |
| 523 |  |  [Co²PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.390) |  | 0 | Pre-trained Language Models are widely used in many important real-world applications. However, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications. To address this challenge, we propose Co2PT, an... | James Caverlee, Maria Teleki, Xiangjue Dong, Zhuoer Wang, Ziwei Zhu |  |
| 524 |  |  [A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.391) |  | 0 | Pre-trained language models (PLMs) have achieved outstanding achievements in abstractive single-document summarization (SDS). However, such benefits may not fully extend to multi-document summarization (MDS), where the handling of cross-document information is more complex. Previous works either... | Chenhui Shen, Lidong Bing, Liying Cheng, XuanPhi Nguyen, Yang You |  |
| 525 |  |  [Universal Domain Adaptation for Robust Handling of Distributional Shifts in NLP](https://doi.org/10.18653/v1/2023.findings-emnlp.392) |  | 0 | When deploying machine learning systems to the wild, it is highly desirable for them to effectively leverage prior knowledge to the unfamiliar domain while also firing alarms to anomalous inputs. In order to address these requirements, Universal Domain Adaptation (UniDA) has emerged as a novel... | Choonghyun Park, Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Kang Min Yoo, SangWoo Lee, Sanggoo Lee, Taeuk Kim |  |
| 526 |  |  [Aligning Language Models to User Opinions](https://doi.org/10.18653/v1/2023.findings-emnlp.393) |  | 0 | An important aspect of developing LLMs that interact with humans is to align models’ behavior to their users. It is possible to prompt an LLM into behaving as a certain persona, especially a user group or ideological persona the model captured during its pertaining stage. But, how to best align an... | Bodhisattwa Prasad Majumder, EunJeong Hwang, Niket Tandon |  |
| 527 |  |  [CCSRD: Content-Centric Speech Representation Disentanglement Learning for End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.394) |  | 0 | Deep neural networks have demonstrated their capacity in extracting features from speech inputs. However, these features may include non-linguistic speech factors such as timbre and speaker identity, which are not directly related to translation. In this paper, we propose a content-centric speech... | Deyi Xiong, Haoran Sun, Shaolin Zhu, Xiaohu Zhao, Yikun Lei |  |
| 528 |  |  [Miracle: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control](https://doi.org/10.18653/v1/2023.findings-emnlp.395) |  | 0 | Personalized dialogue systems aim to endow the chatbot agent with more anthropomorphic traits for human-like interactions. Previous approaches have explored explicitly user profile modeling using text descriptions, implicit derivation of user embeddings, or utilizing handicraft prompts for... | Dangyang Chen, Jixiong Chen, Wei Wei, XianLing Mao, Xiaoye Qu, Zhenyi Lu |  |
| 529 |  |  [Towards Multilingual Interlinear Morphological Glossing](https://doi.org/10.18653/v1/2023.findings-emnlp.396) |  | 0 | Interlinear Morphological Glosses are annotations produced in the context of language documentation. Their goal is to identify morphs occurring in an L1 sentence and to explicit their function and meaning, with the further support of an associated translation in L2. We study here the task of... | François Yvon, Shu Okabe |  |
| 530 |  |  [Transformer Working Memory Enables Regular Language Reasoning And Natural Language Length Extrapolation](https://doi.org/10.18653/v1/2023.findings-emnlp.397) |  | 0 | Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and... | Alexander Rudnicky, Peter J. Ramadge, TaChung Chi, TingHan Fan |  |
| 531 |  |  [Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting](https://doi.org/10.18653/v1/2023.findings-emnlp.398) |  | 0 | Query rewriting plays a vital role in enhancing conversational search by transforming context-dependent user queries into standalone forms. Existing approaches primarily leverage human-rewritten queries as labels to train query rewriting models. However, human rewrites may lack sufficient... | Emine Yilmaz, Fanghua Ye, Meng Fang, Shenghui Li |  |
| 532 |  |  [Distilling ChatGPT for Explainable Automated Student Answer Assessment](https://doi.org/10.18653/v1/2023.findings-emnlp.399) |  | 0 | Providing explainable and faithful feedback is crucial for automated student answer assessment. In this paper, we introduce a novel framework that explores using ChatGPT, a cutting-edge large language model, for the concurrent tasks of student answer scoring and rationale generation. We identify... | Cesare Aloisi, David West, Jiazheng Li, Lin Gui, Yulan He, Yuxiang Zhou |  |
| 533 |  |  [Grammatical Error Correction via Mixed-Grained Weighted Training](https://doi.org/10.18653/v1/2023.findings-emnlp.400) |  | 0 | The task of Grammatical Error Correction (GEC) aims to automatically correct grammatical errors in natural texts. Almost all previous works treat annotated training data equally, but inherent discrepancies in data are neglected. In this paper, the inherent discrepancies are manifested in two... | Chiwei Zhu, Jiahao Li, Quan Wang, Yongdong Zhang, Zhendong Mao |  |
| 534 |  |  [A Unified Framework for Synaesthesia Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.401) |  | 0 | Synaesthesia refers to the description of perceptions in one sensory modality through concepts from other modalities. It involves not only a linguistic phenomenon, but also a cognitive phenomenon structuring human thought and action, which makes understanding it challenging. As a means of... | Guodong Zhou, Kun Sheng, Qingqing Zhao, Xiaotong Jiang, Zhongqing Wang |  |
| 535 |  |  [Domain Private Transformers for Multi-Domain Dialog Systems](https://doi.org/10.18653/v1/2023.findings-emnlp.402) |  | 0 | Large, general purpose language models have demonstrated impressive performance across many different conversational domains. While multi-domain language models achieve low overall perplexity, their outputs are not guaranteed to stay within the domain of a given input prompt. This paper proposes... | Anmol Kabra, Ethan R. Elenberg |  |
| 536 |  |  [Visual Elements Mining as Prompts for Instruction Learning for Target-Oriented Multimodal Sentiment Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.403) |  | 0 | Target-oriented Multimodal Sentiment Classification (TMSC) aims to incorporate visual modality with text modality to identify the sentiment polarity towards a specific target within a sentence. To address this task, we propose a Visual Elements Mining as Prompts (VEMP) method, which describes the... | Bin Yang, Jinlong Li |  |
| 537 |  |  [NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.404) |  | 0 | Structured pruning methods have proven effective in reducing the model size and accelerating inference speed in various network architectures such as Transformers. Despite the versatility of encoder-decoder models in numerous NLP tasks, the structured pruning methods on such models are relatively... | DuSeong Chang, Euijai Ahn, Jongwoo Ko, SeYoung Yun, Seungjoon Park, Sumyeong Ahn, Yujin Kim |  |
| 538 |  |  [GBT: Generative Boosting Training Approach for Paraphrase Identification](https://doi.org/10.18653/v1/2023.findings-emnlp.405) |  | 0 | Paraphrase Identification (PI), a task of determining whether a pair of sentences express the same meaning, is widely applied in Information Retrieval and Question Answering. Data Augmentation (DA) is proven effective in tackling the PI task. However, the majority of DA methods still suffer from... | Rui Peng, Yu Hong, Zhiling Jin |  |
| 539 |  |  [DeCrisisMB: Debiased Semi-Supervised Learning for Crisis Tweet Classification via Memory Bank](https://doi.org/10.18653/v1/2023.findings-emnlp.406) |  | 0 | During crisis events, people often use social media platforms such as Twitter to disseminate information about the situation, warnings, advice, and support. Emergency relief organizations leverage such information to acquire timely crisis circumstances and expedite rescue operations. While existing... | Cornelia Caragea, Henry Peng Zou, Weizhi Zhang, Yue Zhou |  |
| 540 |  |  [Probing LLMs for hate speech detection: strengths and vulnerabilities](https://doi.org/10.18653/v1/2023.findings-emnlp.407) |  | 0 | Recently efforts have been made by social media platforms as well as researchers to detect hateful or toxic language using large language models. However, none of these works aim to use explanation, additional context and victim community information in the detection process. We utilise different... | Animesh Mukherjee, Ashish Harshavardhan, Punyajoy Saha, Sarthak Roy |  |
| 541 |  |  [From Simple to Complex: A Progressive Framework for Document-level Informative Argument Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.408) |  | 0 | Document-level Event Argument Extraction (EAE) requires the model to extract arguments of multiple events from a single document. Considering the underlying dependencies between these events, recent efforts leverage the idea of “memory”, where the results of already predicted events are cached and... | Dongyan Zhao, Quzhe Huang, Yanxi Zhang |  |
| 542 |  |  [MultiCMET: A Novel Chinese Benchmark for Understanding Multimodal Metaphor](https://doi.org/10.18653/v1/2023.findings-emnlp.409) |  | 0 | Metaphor is a pervasive aspect of human communication, and its presence in multimodal forms has become more prominent with the progress of mass media. However, there is limited research on multimodal metaphor resources beyond the English language. Furthermore, the existing work in natural language... | Dongyu Zhang, Hongfei Lin, Jingwei Yu, Liang Yang, Senyuan Jin |  |
| 543 |  |  [GlotLID: Language Identification for Low-Resource Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.410) |  | 0 | Several recent papers have published good solutions for language identification (LID) for about 300 high-resource and medium-resource languages. However, there is no LID available that (i) covers a wide range of low-resource languages, (ii) is rigorously evaluated and reliable and (iii) efficient... | Amir Hossein Kargaran, Ayyoob Imani, François Yvon, Hinrich Schütze |  |
| 544 |  |  [Finding Support Examples for In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.411) |  | 0 | In-context learning is a new learning paradigm where a language model observes a few examples and directly outputs the test input’s prediction. Previous works have shown that it is sensitive to the provided examples and randomly sampled examples probably cause inferior performance. In this paper,... | Xiaonan Li, Xipeng Qiu |  |
| 545 |  |  [Uncovering the Root of Hate Speech: A Dataset for Identifying Hate Instigating Speech](https://doi.org/10.18653/v1/2023.findings-emnlp.412) |  | 0 | While many prior studies have applied computational approaches, such as machine learning, to detect and moderate hate speech, only scant attention has been paid to the task of identifying the underlying cause of hate speech. In this study, we introduce the concept of hate instigating speech, which... | Ho Shim, Hyoungjun Park, Kyuhan Lee |  |
| 546 |  |  [Responsible AI Considerations in Text Summarization Research: A Review of Current Practices](https://doi.org/10.18653/v1/2023.findings-emnlp.413) |  | 0 | AI and NLP publication venues have increasingly encouraged researchers to reflect on possible ethical considerations, adverse impacts, and other responsible AI issues their work might engender. However, for specific NLP tasks our understanding of how prevalent such issues are, or when and why these... | Adam Trischler, Alexandra Olteanu, Jackie Chi Kit Cheung, Meng Cao, Su Lin Blodgett, Yu Lu Liu |  |
| 547 |  |  [Improving Speech Translation by Fusing Speech and Text](https://doi.org/10.18653/v1/2023.findings-emnlp.414) |  | 0 | In speech translation, leveraging multimodal data to improve model performance and address limitations of individual modalities has shown significant effectiveness. In this paper, we harness the complementary strengths of speech and text to improve speech translation. However, speech and text are... | Chengqi Zhao, Jian Tong, Rong Ye, Tao Wang, Wenbiao Yin, Zhicheng Liu |  |
| 548 |  |  [Narrative Order Aware Story Generation via Bidirectional Pretraining Model with Optimal Transport Reward](https://doi.org/10.18653/v1/2023.findings-emnlp.415) |  | 0 | To create a captivating story, a writer often plans a sequence of logically coherent events and ingeniously manipulates the narrative order to generate flashback in place. However, existing storytelling systems suffer from both insufficient understanding of event correlations and inadequate... | Guangluan Xu, Kaiwen Wei, Li Jin, Linmei Hu, Nayu Liu, Xian Sun, Xiaoyu Li, Zequn Zhang, Zhicong Lu |  |
| 549 |  |  [Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.416) |  | 0 | Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to... | Haoran Wang, Kai Shu |  |
| 550 |  |  [Strong and Efficient Baselines for Open Domain Conversational Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.417) |  | 0 | Unlike the Open Domain Question Answering (ODQA) setting, the conversational (ODConvQA) domain has received limited attention when it comes to reevaluating baselines for both efficiency and effectiveness. In this paper, we study the State-of-the-Art (SotA) Dense Passage Retrieval (DPR) retriever... | Adrià de Gispert, Andrei C. Coman, Gianni Barlacchi |  |
| 551 |  |  [Efficient Continue Training of Temporal Language Model with Structural Information](https://doi.org/10.18653/v1/2023.findings-emnlp.418) |  | 0 | Current language models are mainly trained on snap-shots of data gathered at a particular time, which decreases their capability to generalize over time and model language change. To model the time variable, existing works have explored temporal language models (e.g., TempoBERT) by directly... | Juntao Li, Min Zhang, Zhaochen Su, Zihan Zhou, Zikang Zhang |  |
| 552 |  |  [Retrieval-Augmented Parsing for Complex Graphs by Exploiting Structure and Uncertainty](https://doi.org/10.18653/v1/2023.findings-emnlp.419) |  | 0 | Retrieval augmentation enhances generative language models by retrieving informative exemplars relevant for output prediction. However, in realistic graph parsing problems where the output space is large and complex, classic retrieval methods based on input-sentence similarity can fail to identify... | Jeremiah Z. Liu, Jingbo Shang, Panupong Pasupat, Quan Yuan, Zi Lin |  |
| 553 |  |  [When it Rains, it Pours: Modeling Media Storms and the News Ecosystem](https://doi.org/10.18653/v1/2023.findings-emnlp.420) |  | 0 | Most events in the world receive at most brief coverage by the news media. Occasionally, however, an event will trigger a media storm, with voluminous and widespread coverage lasting for weeks instead of days. In this work, we develop and apply a pairwise article similarity model, allowing us to... | Benjamin Litterer, Dallas Card, David Jurgens |  |
| 554 |  |  [Intra-Event and Inter-Event Dependency-Aware Graph Network for Event Argument Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.421) |  | 0 | Event argument extraction is critical to various natural language processing tasks for providing structured information. Existing works usually extract the event arguments one by one, and mostly neglect to build dependency information among event argument roles, especially from the perspective of... | Fang Fang, Hao Li, Lanxue Zhang, Shi Wang, Yanan Cao, Yingjie Li, Yubing Ren |  |
| 555 |  |  [From Relevance to Utility: Evidence Retrieval with Feedback for Fact Verification](https://doi.org/10.18653/v1/2023.findings-emnlp.422) |  | 0 | Retrieval-enhanced methods have become a primary approach in fact verification (FV); it requires reasoning over multiple retrieved pieces of evidence to verify the integrity of a claim. To retrieve evidence, existing work often employs off-the-shelf retrieval models whose design is based on the... | Hengran Zhang, Jiafeng Guo, Maarten de Rijke, Ruqing Zhang, Xueqi Cheng, Yixing Fan |  |
| 556 |  |  [How to Train Your Dragon: Diverse Augmentation Towards Generalizable Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.423) |  | 0 | Various techniques have been developed in recent years to improve dense retrieval (DR), such as unsupervised contrastive learning and pseudo-query generation. Existing DRs, however, often suffer from effectiveness tradeoffs between supervised and zero-shot retrieval, which some argue was due to the... | Akari Asai, Barlas Oguz, Jimmy Lin, Minghan Li, ShengChieh Lin, Wentau Yih, Xilun Chen, Yashar Mehdad |  |
| 557 |  |  [Discovering Highly Influential Shortcut Reasoning: An Automated Template-Free Approach](https://doi.org/10.18653/v1/2023.findings-emnlp.424) |  | 0 | Shortcut reasoning is an irrational process of inference, which degrades the robustness of an NLP model. While a number of previous work has tackled the identification of shortcut reasoning, there are still two major limitations: (i) a method for quantifying the severity of the discovered shortcut... | Daichi Haraguchi, Kiyoaki Shirai, Naoya Inoue, Natthawut Kertkeidkachorn |  |
| 558 |  |  [Schema-adaptable Knowledge Graph Construction](https://doi.org/10.18653/v1/2023.findings-emnlp.425) |  | 0 | Conventional Knowledge Graph Construction (KGC) approaches typically follow the static information extraction paradigm with a closed set of pre-defined schema. As a result, such approaches fall short when applied to dynamic scenarios or domains, whereas a new type of knowledge emerges. This... | Hongbin Ye, Honghao Gui, Huajun Chen, Ningyu Zhang, Xi Chen, Xin Xu |  |
| 559 |  |  [Evaluating the Knowledge Base Completion Potential of GPT](https://doi.org/10.18653/v1/2023.findings-emnlp.426) |  | 0 | Structured knowledge bases (KBs) are an asset for search engines and other applications but are inevitably incomplete. Language models (LMs) have been proposed for unsupervised knowledge base completion (KBC), yet, their ability to do this at scale and with high accuracy remains an open question.... | Blerta Veseli, Gerhard Weikum, JanChristoph Kalo, Simon Razniewski |  |
| 560 |  |  [Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset](https://doi.org/10.18653/v1/2023.findings-emnlp.427) |  | 0 | Mathematical understanding and reasoning are crucial tasks for assessing the capabilities of artificial intelligence (AI). However, existing benchmarks either require just a few steps of reasoning, or only contain a small amount of data in one specific topic, making it hard to analyse AI’s... | Haoyi Wu, Kewei Tu, Weiqi Wu, Wenyang Hui, Yezeng Chen, Yi Zhou |  |
| 561 |  |  [DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text](https://doi.org/10.18653/v1/2023.findings-emnlp.428) |  | 0 | Spatial reasoning in text plays a crucial role in various real-world applications. Existing approaches for spatial reasoning typically infer spatial relations from pure text, which overlook the gap between natural language and symbolic structures. Graph neural networks (GNNs) have showcased... | Shuaiyi Li, Wai Lam, Yang Deng |  |
| 562 |  |  [TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised Intent Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.429) |  | 0 | The ability to detect intent in dialogue systems has become increasingly important in modern technology. These systems often generate a large amount of unlabeled data, and manually labeling this data requires substantial human effort. Semi-supervised methods attempt to remedy this cost by using a... | David Vasquez, Issam H. Laradji, Nicholas Botzer, Tim Weninger |  |
| 563 |  |  [Late Fusion of Transformers for Sentiment Analysis of Code-Switched Data](https://doi.org/10.18653/v1/2023.findings-emnlp.430) |  | 0 | Code-switching is a common phenomenon in multilingual communities and is often used on social media. However, sentiment analysis of code-switched data is a challenging yet less explored area of research. This paper aims to develop a sentiment analysis system for code-switched data. In this paper,... | Gagan Sharma, R. Chinmay, Raksha Sharma |  |
| 564 |  |  [Inductive Relation Inference of Knowledge Graph Enhanced by Ontology Information](https://doi.org/10.18653/v1/2023.findings-emnlp.431) |  | 0 | The inductive inference of the knowledge graph aims to complete the potential relations between the new unknown entities in the graph. Most existing methods are based on entity-independent features such as graph structure information and relationship information to inference. However, the... | Jun Zhao, Qi Zhang, Tao Gui, Wentao Zhou, Xuanjing Huang |  |
| 565 |  |  [Dynamic Stance: Modeling Discussions by Labeling the Interactions](https://doi.org/10.18653/v1/2023.findings-emnlp.432) |  | 0 | Stance detection is an increasingly popular task that has been mainly modeled as a static task, by assigning the expressed attitude of a text toward a given topic. Such a framing presents limitations, with trained systems showing poor generalization capabilities and being strongly topic-dependent.... | Blanca Calvo Figueras, Irene Baucells de la Peña, Tommaso Caselli |  |
| 566 |  |  [Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements](https://doi.org/10.18653/v1/2023.findings-emnlp.433) |  | 0 | Empathetic dialogue is an indispensable part of building harmonious social relationships and contributes to the development of a helpful AI. Previous approaches are mainly based on fine small-scale language models. With the advent of ChatGPT, the application effect of large language models (LLMs)... | Ting Liu, Weinan Zhang, Yushan Qian |  |
| 567 |  |  [GPT Deciphering Fedspeak: Quantifying Dissent Among Hawks and Doves](https://doi.org/10.18653/v1/2023.findings-emnlp.434) |  | 0 | Markets and policymakers around the world hang on the consequential monetary policy decisions made by the Federal Open Market Committee (FOMC). Publicly available textual documentation of their meetings provides insight into members’ attitudes about the economy. We use GPT-4 to quantify dissent... | Adam Visokay, Alan Blinder, Benjamin Wachspress, Brandon M. Stewart, Denis Peskoff, Sander Schulhoff |  |
| 568 |  |  [DialogQAE: N-to-N Question Answer Pair Extraction from Customer Service Chatlog](https://doi.org/10.18653/v1/2023.findings-emnlp.435) |  | 0 | Harvesting question-answer (QA) pairs from customer service chatlog in the wild is an efficient way to enrich the knowledge base for customer service chatbots in the cold start or continuous integration scenarios. Prior work attempts to obtain 1-to-1 QA pairs from growing customer service chatlog,... | Binghuai Lin, Haoran Meng, MengLiang Rao, Tianyu Liu, Xin Zheng, Xu Wang, Yufan Jiang, Yunbo Cao, Zhifang Sui |  |
| 569 |  |  [Inverse Reinforcement Learning for Text Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.436) |  | 0 | We introduce inverse reinforcement learning (IRL) as an effective paradigm for training abstractive summarization models, imitating human summarization behaviors. Our IRL model estimates the reward function using a suite of important sub-rewards for summarization and concurrently optimizes the... | Deyi Xiong, Yu Fu, Yue Dong |  |
| 570 |  |  [MM-Reasoner: A Multi-Modal Knowledge-Aware Framework for Knowledge-Based Visual Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.437) |  | 0 | Thanks to the strong reasoning capabilities of Large Language Models (LLMs), recent approaches to knowledge-based visual question answering (KVQA) utilize LLMs with a global caption of an input image to answer a question. However, these approaches may miss key visual information that is not... | Chenguang Zhu, Felipe Frujeri, Mahmoud Khademi, Ziyi Yang |  |
| 571 |  |  [Toward Joint Language Modeling for Speech Units and Text](https://doi.org/10.18653/v1/2023.findings-emnlp.438) |  | 0 | Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language... | Alexei Baevski, Alexis Conneau, Arun Babu, ChungMing Chien, JuChieh Chou, Karen Livescu, Michael Auli, WeiNing Hsu |  |
| 572 |  |  [From Chaos to Clarity: Claim Normalization to Empower Fact-Checking](https://doi.org/10.18653/v1/2023.findings-emnlp.439) |  | 0 | With the proliferation of social media platforms, users are exposed to vast information, including posts containing misleading claims. However, the pervasive noise inherent in these posts presents a challenge in identifying precise and prominent claims that require verification. Extracting the core... | Megha Sundriyal, Preslav Nakov, Tanmoy Chakraborty |  |
| 573 |  |  [Mitigating Biases in Hate Speech Detection from A Causal Perspective](https://doi.org/10.18653/v1/2023.findings-emnlp.440) |  | 0 | Nowadays, many hate speech detectors are built to automatically detect hateful content. However, their training sets are sometimes skewed towards certain stereotypes (e.g., race or religion-related). As a result, the detectors are prone to depend on some shortcuts for predictions. Previous works... | Diyi Yang, Jiaao Chen, Zhehao Zhang |  |
| 574 |  |  [Unmasking the Hidden Meaning: Bridging Implicit and Explicit Hate Speech Embedding Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.441) |  | 0 | Research on automatic hate speech (HS) detection has mainly focused on identifying explicit forms of hateful expressions on user-generated content. Recently, a few works have started to investigate methods to address more implicit and subtle abusive content. However, despite these efforts,... | Elena Cabrio, Nicolás Benjamín Ocampo, Serena Villata |  |
| 575 |  |  [PerturbScore: Connecting Discrete and Continuous Perturbations in NLP](https://doi.org/10.18653/v1/2023.findings-emnlp.442) |  | 0 | With the rapid development of neural network applications in NLP, model robustness problem is gaining more attention. Different from computer vision, the discrete nature of texts makes it more challenging to explore robustness in NLP. Therefore, in this paper, we aim to connect discrete... | Ke Ren, Linyang Li, Pengyu Wang, Xipeng Qiu, Yunfan Shao |  |
| 576 |  |  [InstructoR: Instructing Unsupervised Conversational Dense Retrieval with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.443) |  | 0 | Compared to traditional single-turn ad-hoc retrieval, conversational retrieval needs to handle the multi-turn conversation and understand the user’s real query intent. However, most existing methods simply fine-tune the pre-trained ad-hoc retriever on limited supervised data, making it challenging... | Jun Zhao, Kang Liu, Pengfei Cao, Yubo Chen, Zhuoran Jin |  |
| 577 |  |  [The Iron(ic) Melting Pot: Reviewing Human Evaluation in Humour, Irony and Sarcasm Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.444) |  | 0 | Human evaluation in often considered to be the gold standard method of evaluating a Natural Language Generation system. However, whilst its importance is accepted by the community at large, the quality of its execution is often brought into question. In this position paper, we argue that the... | Aaron Maladry, Chenghua Lin, Tyler Loakman |  |
| 578 |  |  [INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.445) |  | 0 | A salient characteristic of pre-trained language models (PTLMs) is a remarkable improvement in their generalization capability and emergence of new capabilities with increasing model capacity and pre-training dataset size. Consequently, we are witnessing the development of enormous models pushing... | Balaji Krishnamurthy, Ganesh Ramakrishnan, H. S. V. N. S. Kowndinya Renduchintala, Krishnateja Killamsetty, Milan Aggarwal, Rishabh K. Iyer, Sumit Bhatia |  |
| 579 |  |  [Towards General Error Diagnosis via Behavioral Testing in Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.446) |  | 0 | Behavioral testing offers a crucial means of diagnosing linguistic errors and assessing capabilities of NLP models. However, applying behavioral testing to machine translation (MT) systems is challenging as it generally requires human efforts to craft references for evaluating the translation... | DitYan Yeung, Junjie Wu, Lemao Liu |  |
| 580 |  |  [Retrieval-Augmented Few-shot Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.447) |  | 0 | Retrieval-augmented methods are successful in the standard scenario where the retrieval space is sufficient; whereas in the few-shot scenario with limited retrieval space, this paper shows it is non-trivial to put them into practice. First, it is impossible to retrieve semantically similar examples... | Guoxin Yu, Haiyun Jiang, Lemao Liu, Shuming Shi, Xiang Ao |  |
| 581 |  |  [Temporal Extrapolation and Knowledge Transfer for Lifelong Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.448) |  | 0 | Real-world Temporal Knowledge Graphs keep growing with time and new entities and facts emerge continually, necessitating a model that can extrapolate to future timestamps and transfer knowledge for new components. Therefore, our work first dives into this more realistic issue, lifelong TKG... | Chengjin Xu, Fenglong Su, Yong Dou, Zhen Huang, Zhongwu Chen |  |
| 582 |  |  [Comparing Prompt-Based and Standard Fine-Tuning for Urdu Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.449) |  | 0 | Recent advancements in natural language processing have demonstrated the efficacy of pre-trained language models for various downstream tasks through prompt-based fine-tuning. In contrast to standard fine-tuning, which relies solely on labeled examples, prompt-based fine-tuning combines a few... | Ali Faheem, Asim Karim, Faisal Kamiran, Faizad Ullah, Ubaid Azam |  |
| 583 |  |  [Explore the Way: Exploring Reasoning Path by Bridging Entities for Effective Cross-Document Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.450) |  | 0 | Cross-document relation extraction (CodRED) task aims to infer the relation between two entities mentioned in different documents within a reasoning path. Previous studies have concentrated on merely capturing implicit relations between the entities. However, humans usually utilize explicit... | Heuiseok Lim, Jinsung Kim, Jungwoo Lim, Junyoung Son, Yoonna Jang |  |
| 584 |  |  [The student becomes the master: Outperforming GPT3 on Scientific Factual Error Correction](https://doi.org/10.18653/v1/2023.findings-emnlp.451) |  | 0 | Due to the prohibitively high cost of creating error correction datasets, most Factual Claim Correction methods rely on a powerful verification model to guide the correction process. This leads to a significant drop in performance in domains like Scientific Claim Correction, where good verification... | Atharva Kulkarni, Barnabás Póczos, Dhananjay Ashok, Hai Pham |  |
| 585 |  |  [Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.452) |  | 0 | Neural models, including large language models (LLMs), achieve superior performance on multi-hop question-answering. To elicit reasoning capabilities from LLMs, recent works propose using the chain-of-thought (CoT) mechanism to generate both the reasoning chain and the answer, which enhances the... | Ruosen Li, Xinya Du |  |
| 586 |  |  [Hierarchical Catalogue Generation for Literature Review: A Benchmark](https://doi.org/10.18653/v1/2023.findings-emnlp.453) |  | 0 | Scientific literature review generation aims to extract and organize important information from an abundant collection of reference papers and produces corresponding reviews while lacking a clear and logical hierarchy. We observe that a high-quality catalogue-guided generation process can... | Bing Qin, Kun Zhu, Xiachong Feng, Xiaocheng Feng, Yingsheng Wu |  |
| 587 |  |  [MCC-KD: Multi-CoT Consistent Knowledge Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.454) |  | 0 | Large language models (LLMs) have showcased remarkable capabilities in complex reasoning through chain of thought (CoT) prompting. Recently, there has been a growing interest in transferring these reasoning abilities from LLMs to smaller models. However, achieving both the diversity and consistency... | Hongzhan Chen, Ji Zhang, Ming Yan, Rui Wang, Siyue Wu, Xiaojun Quan |  |
| 588 |  |  [An Empirical Study of Frame Selection for Text-to-Video Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.455) |  | 0 | Text-to-video retrieval (TVR) aims to find the most relevant video in a large video gallery given a query text. The intricate and abundant context of the video challenges the performance and efficiency of TVR. To handle the serialized video contexts, existing methods typically select a subset of... | Chen Chen, Liqiang Nie, Mengxia Wu, Min Cao, Min Zhang, Yang Bai, Ziyin Zeng |  |
| 589 |  |  [Conditional Natural Language Inference](https://doi.org/10.18653/v1/2023.findings-emnlp.456) |  | 0 | To properly explain sentence pairs that provide contradictory (different) information for different conditions, we introduce the task of conditional natural language inference (Cond-NLI) and focus on automatically extracting contradictory aspects and their conditions from a sentence pair. Cond-NLI... | James Allan, Razieh Rahimi, Youngwoo Kim |  |
| 590 |  |  [Contrastive Distant Supervision for Debiased and Denoised Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.457) |  | 0 | Distant Supervision (DS) is a promising learning approach for MRC by leveraging easily-obtained question-answer pairs. Unfortunately, the heuristically annotated dataset will inevitably lead to mislabeled instances, resulting in answer bias and context noise problems. To learn debiased and denoised... | Ben He, Hongyu Lin, Le Sun, Ning Bian, Xianpei Han |  |
| 591 |  |  [KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness](https://doi.org/10.18653/v1/2023.findings-emnlp.458) |  | 0 | In recent years, Pre-trained Language Models (PLMs) have shown their superiority by pre-training on unstructured text corpus and then fine-tuning on downstream tasks. On entity-rich textual resources like Wikipedia, Knowledge-Enhanced PLMs (KEPLMs) incorporate the interactions between tokens and... | Benjamin Z. Yao, Chengyuan Ma, Jialong Han, Kyumin Lee, Xiaohu Liu, Yichuan Li |  |
| 592 |  |  [Revisiting Large Language Models as Zero-shot Relation Extractors](https://doi.org/10.18653/v1/2023.findings-emnlp.459) |  | 0 | Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility... | Guozheng Li, Peng Wang, Wenjun Ke |  |
| 593 |  |  [Multi-Stage Pre-training Enhanced by ChatGPT for Multi-Scenario Multi-Domain Dialogue Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.460) |  | 0 | Dialogue summarization involves a wide range of scenarios and domains. However, existing methods generally only apply to specific scenarios or domains. In this study, we propose a new pre-trained model specifically designed for multi-scenario multi-domain dialogue summarization. It adopts a... | Feifei Zhai, Gengyao Li, Junnan Zhu, Weixiao Zhou, Xianfu Cheng, Xinnian Liang, Zhoujun Li |  |
| 594 |  |  [Towards large language model-based personal agents in the enterprise: Current trends and open problems](https://doi.org/10.18653/v1/2023.findings-emnlp.461) |  | 0 | There is an emerging trend to use large language models (LLMs) to reason about complex goals and orchestrate a set of pluggable tools or APIs to accomplish a goal. This functionality could, among other use cases, be used to build personal assistants for knowledge workers. While there are impressive... | Ashu Gulati, Kiran Kate, Parijat Dube, Praveen Venkateswaran, Vatche Isahagian, Vinod Muthusamy, Yara Rizk |  |
| 595 |  |  [CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.462) |  | 0 | Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved. To overcome these limitations, we propose CREATOR, a novel... | Cheng Qian, Chi Han, Heng Ji, Yi Ren Fung, Yujia Qin, Zhiyuan Liu |  |
| 596 |  |  [Query-based Image Captioning from Multi-context 360cdegree Images](https://doi.org/10.18653/v1/2023.findings-emnlp.463) |  | 0 | A 360-degree image captures the entire scene without the limitations of a camera’s field of view, which makes it difficult to describe all the contexts in a single caption. We propose a novel task called Query-based Image Captioning (QuIC) for 360-degree images, where a query (words or short... | Koki Maeda, Naoaki Okazaki, Shuhei Kurita, Taiki Miyanishi |  |
| 597 |  |  [Auto Search Indexer for End-to-End Document Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.464) |  | 0 | Generative retrieval, which is a new advanced paradigm for document retrieval, has recently attracted research interests, since it encodes all documents into the model and directly generates the retrieved documents. However, its power is still underutilized since it heavily relies on the... | Feng Sun, Haizhen Huang, Minghui Song, Qi Zhang, Tianchi Yang, Weiwei Deng, Zihan Zhang |  |
| 598 |  |  ['Person' == Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion](https://doi.org/10.18653/v1/2023.findings-emnlp.465) |  | 0 | We study stereotypes embedded within one of the most popular text-to-image generators: Stable Diffusion. We answer the question: what stereotypes of gender and nationality/continental identity does Stable Diffusion display in the absence of such information i.e. what gender and... | Aylin Caliskan, Sourojit Ghosh |  |
| 599 |  |  [Task-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.466) |  | 0 | The size and the computational load of fine-tuning large-scale pre-trained neural network are becoming two major obstacles in adopting machine learning in many applications. Continual learning (CL) can serve as a remedy through enabling knowledge-transfer across sequentially arriving tasks which... | Jesse Thomason, Mohammad Rostami, Yuliang Cai |  |
| 600 |  |  [Evaluating Verifiability in Generative Search Engines](https://doi.org/10.18653/v1/2023.findings-emnlp.467) |  | 0 | Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and... | Nelson F. Liu, Percy Liang, Tianyi Zhang |  |
| 601 |  |  [Enhancing Abstractiveness of Summarization Models through Calibrated Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.468) |  | 0 | In this paper, we propose a novel approach named DisCal to enhance the level of abstractiveness (measured by n-gram overlap) without sacrificing the informativeness (measured by ROUGE) of generated summaries. DisCal exposes diverse pseudo summaries with two supervision to the student model.... | Hang Su, Hwanjun Song, Igor Shalyminov, Kaisheng Yao, Saab Mansour, Siffi Singh |  |
| 602 |  |  [Visually Grounded Continual Language Learning with Selective Specialization](https://doi.org/10.18653/v1/2023.findings-emnlp.469) |  | 0 | A desirable trait of an artificial agent acting in the visual world is to continually learn a sequence of language-informed tasks while striking a balance between sufficiently specializing in each task and building a generalized knowledge for transfer. Selective specialization, i.e., a careful... | Jae Hee Lee, Kyra Ahrens, Lennart Bengtson, Stefan Wermter |  |
| 603 |  |  [RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.470) |  | 0 | We introduce RoMQA, the first benchmark for robust, multi-evidence, multi-answer question answering (QA). RoMQA contains clusters of questions that are derived from related constraints mined from the Wikidata knowledge graph. RoMQA evaluates robustness of QA models to varying constraints by... | Luke Zettlemoyer, Victor Zhong, Weijia Shi, Wentau Yih |  |
| 604 |  |  [Leveraging Multiple Teachers for Test-Time Adaptation of Language-Guided Classifiers](https://doi.org/10.18653/v1/2023.findings-emnlp.471) |  | 0 | Recent approaches have explored language- guided classifiers capable of classifying examples from novel tasks when provided with task-specific natural language explanations, instructions or prompts (Sanh et al., 2022; R. Menon et al., 2022). While these classifiers can generalize in zero-shot... | Kangda Wei, Rakesh R. Menon, Sayan Ghosh, Shashank Srivastava |  |
| 605 |  |  [Summarizing Multiple Documents with Conversational Structure for Meta-Review Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.472) |  | 0 | We present PeerSum, a novel dataset for generating meta-reviews of scientific papers. The meta-reviews can be interpreted as abstractive summaries of reviews, multi-turn discussions and the paper abstract. These source documents have a rich inter-document relationship with an explicit hierarchical... | Eduard H. Hovy, Jey Han Lau, Miao Li |  |
| 606 |  |  [VIPHY: Probing "Visible" Physical Commonsense Knowledge](https://doi.org/10.18653/v1/2023.findings-emnlp.473) |  | 0 | Vision-language models (VLMs) have shown remarkable performance on visual reasoning tasks (e.g. attributes, location). While such tasks measure the requisite knowledge to ground and reason over a given visual instance, they do not, however, measure the ability of VLMs to retain and generalize such... | Ehsan Qasemi, Muhao Chen, Shikhar Singh |  |
| 607 |  |  [Two Directions for Clinical Data Generation with Large Language Models: Data-to-Label and Label-to-Data](https://doi.org/10.18653/v1/2023.findings-emnlp.474) |  | 0 | Large language models (LLMs) can generate natural language texts for various domains and tasks, but their potential for clinical text mining, a domain with scarce, sensitive, and imbalanced medical data, is under-explored. We investigate whether LLMs can augment clinical data for detecting... | Hong Yu, Rumeng Li, Xun Wang |  |
| 608 |  |  [Stylized Dialogue Generation with Feature-Guided Knowledge Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.475) |  | 0 | Stylized dialogue generation systems aim to produce coherent and context-aware dialogues while effectively emulating the desired style. Generating stylized dialogue is valuable yet challenging due to the scarce parallel data. Existing methods often synthesize pseudo data through back translation,... | Dongyan Zhao, Jinpeng Li, Rui Yan, Xiuying Chen, Zekai Zhang |  |
| 609 |  |  [Probing LLMs for Joint Encoding of Linguistic Categories](https://doi.org/10.18653/v1/2023.findings-emnlp.476) |  | 0 | Large Language Models (LLMs) exhibit impressive performance on a range of NLP tasks, due to the general-purpose linguistic knowledge acquired during pretraining. Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower... | Alina Leidinger, Apostolos Panagiotopoulos, Ekaterina Shutova, Giulio Starace, Konstantinos Papakostas, Matteo Rosati, Rochelle Choenni |  |
| 610 |  |  [On Robustness of Finetuned Transformer-based NLP Models](https://doi.org/10.18653/v1/2023.findings-emnlp.477) |  | 0 | Transformer-based pretrained models like BERT, GPT-2 and T5 have been finetuned for a large number of natural language processing (NLP) tasks, and have been shown to be very effective. However, while finetuning, what changes across layers in these models with respect to pretrained checkpoints is... | Manish Gupta, Mounika Marreddy, Pavan Kalyan Reddy Neerudu, Subba Reddy Oota, Venkateswara Rao Kagita |  |
| 611 |  |  [Measuring and Mitigating Constraint Violations of In-Context Learning for Utterance-to-API Semantic Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.478) |  | 0 | In executable task-oriented semantic parsing, the system aims to translate users’ utterances in natural language to machine-interpretable programs (API calls) that can be executed according to pre-defined API specifications. With the popularity of Large Language Models (LLMs), in-context learning... | James Gung, Nikolaos Pappas, Sailik Sengupta, Shufan Wang, Sébastien Jean, Yi Zhang |  |
| 612 |  |  [Entity Disambiguation on a Tight Labeling Budget](https://doi.org/10.18653/v1/2023.findings-emnlp.479) |  | 0 | Many real-world NLP applications face the challenge of training an entity disambiguation model for a specific domain with a small labeling budget. In this setting there is often access to a large unlabeled pool of documents. It is then natural to ask the question: which samples should be selected... | Ariadna Quattoni, Audi Primadhanty |  |
| 613 |  |  [Topic-DPR: Topic-based Prompts for Dense Passage Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.480) |  | 0 | Prompt-based learning’s efficacy across numerous natural language processing tasks has led to its integration into dense passage retrieval. Prior research has mainly focused on enhancing the semantic understanding of pre-trained language models by optimizing a single vector as a continuous prompt.... | Lei Chen, Qingfa Xiao, Shuangyin Li |  |
| 614 |  |  [Quantifying the Dialect Gap and its Correlates Across Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.481) |  | 0 | Historically, researchers and consumers have noticed a decrease in quality when applying NLP tools to minority variants of languages (i.e. Puerto Rican Spanish or Swiss German), but studies exploring this have been limited to a select few languages. Additionally, past studies have mainly been... | Anjali Kantharuban, Anna Korhonen, Ivan Vulic |  |
| 615 |  |  [RECAL: Sample-Relation Guided Confidence Calibration over Tabular Data](https://doi.org/10.18653/v1/2023.findings-emnlp.482) |  | 0 | Tabular-format data is widely adopted in various real-world applications. Various machine learning models have achieved remarkable success in both industrial applications and data-science competitions. Despite these successes, most current machine learning methods for tabular data lack accurate... | Bingzhe Wu, Haotian Wang, Liang Chen, Mengting Hu, Qichao Wang, Yatao Bian, Zhen Zhang |  |
| 616 |  |  [Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment](https://doi.org/10.18653/v1/2023.findings-emnlp.483) |  | 0 | Pre-trained vision and language models such as CLIP have witnessed remarkable success in connecting images and texts with a primary focus on English texts. Despite recent efforts to extend CLIP to support other languages, disparities in performance among different languages have been observed due... | Jialu Wang, Xin Eric Wang, Zhen Zhang |  |
| 617 |  |  [Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries](https://doi.org/10.18653/v1/2023.findings-emnlp.484) |  | 0 | Ideal summarization models should generalize to novel summary-worthy content without remembering reference training summaries by rote. However, a single average performance score on the entire test set is inadequate in determining such model competencies. We propose a fine-grained evaluation... | Alexander R. Fabbri, Caiming Xiong, ChienSheng Wu, Prafulla Kumar Choubey |  |
| 618 |  |  [Pseudointelligence: A Unifying Lens on Language Model Evaluation](https://doi.org/10.18653/v1/2023.findings-emnlp.485) |  | 0 | With large language models surpassing human performance on an increasing number of benchmarks, we must take a principled approach for targeted evaluation of model capabilities. Inspired by pseudorandomness, we propose pseudointelligence, which captures the maxim that “(perceived) intelligence lies... | Orr Paradise, Pratyusha Sharma, Shikhar Murty |  |
| 619 |  |  [GDA: Grammar-based Data Augmentation for Text Classification using Slot Information](https://doi.org/10.18653/v1/2023.findings-emnlp.486) |  | 0 | Recent studies propose various data augmentation approaches to resolve the low-resource problem in natural language processing tasks. Data augmentation is a successful solution to this problem and recent strategies give variation on sentence structures to boost performance. However, these... | Elizabeth Orwig, Hyunjoon Cheon, Joonghyuk Hahn, SangKi Ko, SuHyeon Kim, YoSub Han |  |
| 620 |  |  [Implicit Sense-labeled Connective Recognition as Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.487) |  | 0 | Implicit Discourse Relation Recognition (IDRR) involves identifying the sense label of an implicit connective between adjacent text spans. This has traditionally been approached as a classification task. However, some downstream tasks require more than just a sense label as well as the specific... | Tsutomu Hirao, Yui Oka |  |
| 621 |  |  [VISTA: Visual-Textual Knowledge Graph Representation Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.488) |  | 0 | Knowledge graphs represent human knowledge using triplets composed of entities and relations. While most existing knowledge graph embedding methods only consider the structure of a knowledge graph, a few recently proposed multimodal methods utilize images or text descriptions of entities in a... | Chanyoung Chung, Hochang Lee, Jaejun Lee, Joyce Jiyoung Whang, Sungho Jo |  |
| 622 |  |  [Dynamic Stashing Quantization for Efficient Transformer Training](https://doi.org/10.18653/v1/2023.findings-emnlp.489) |  | 0 | Large Language Models (LLMs) have demonstrated impressive performance on a range of Natural Language Processing (NLP) tasks. Unfortunately, the immense amount of computations and memory accesses required for LLM training makes them prohibitively expensive in terms of hardware cost, and thus... | Daniel Lo, Guo Yang, Robert D. Mullins, Yiren Zhao |  |
| 623 |  |  [A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.490) |  | 0 | Large language models (LLMs) have demonstrated great potential for domain-specific applications, such as the law domain. However, recent disputes over GPT-4’s law evaluation raise questions concerning their performance in real-world legal tasks. To systematically investigate their competency in the... | Ruihao Shui, TatSeng Chua, Xiang Wang, Yixin Cao |  |
| 624 |  |  [A Lightweight Method to Generate Unanswerable Questions in English](https://doi.org/10.18653/v1/2023.findings-emnlp.491) |  | 0 | If a question cannot be answered with the available information, robust systems for question answering (QA) should know \*not\* to answer. One way to build QA models that do this is with additional training data comprised of unanswerable questions, created either by employing annotators or through... | Dietrich Klakow, Miaoran Zhang, Vagrant Gautam |  |
| 625 |  |  [Automatic Evaluate Dialogue Appropriateness by Using Dialogue Act](https://doi.org/10.18653/v1/2023.findings-emnlp.492) |  | 0 | Evaluation of dialogue systems requires assessing various aspects, among which appropriateness holds significance as a core element of communicative language competence. However, current evaluations heavily rely on human judgments, which are time-consuming, labor-intensive, prone to biases, and... | Bao Chen, Yuanjie Wang, Yuhang Guo, Zeming Liu |  |
| 626 |  |  [TabPrompt: Graph-based Pre-training and Prompting for Few-shot Table Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.493) |  | 0 | Table Understanding (TU) is a crucial aspect of information extraction that enables machines to comprehend the semantics behind tabular data. However, existing methods of TU cannot deal with the scarcity of labeled tabular data. In addition, these methods primarily focus on the textual content... | Guilin Qi, Jianan Wang, Rihui Jin, Wang Hao, Wei Tan, Yongrui Chen |  |
| 627 |  |  [Towards Formality-Aware Neural Machine Translation by Leveraging Context Information](https://doi.org/10.18653/v1/2023.findings-emnlp.494) |  | 0 | Formality is one of the most important linguistic properties to determine the naturalness of translation. Although a target-side context contains formality-related tokens, the sparsity within the context makes it difficult for context-aware neural machine translation (NMT) models to properly... | Dohee Kim, Jaegul Choo, Soyoung Yang, Yujin Baek |  |
| 628 |  |  [Improving Seq2Seq Grammatical Error Correction via Decoding Interventions](https://doi.org/10.18653/v1/2023.findings-emnlp.495) |  | 0 | The sequence-to-sequence (Seq2Seq) approach has recently been widely used in grammatical error correction (GEC) and shows promising performance. However, the Seq2Seq GEC approach still suffers from two issues. First, a Seq2Seq GEC model can only be trained on parallel data, which, in GEC task, is... | Bo Zhang, Chen Li, Fei Huang, Houquan Zhou, Ji Zhang, Min Zhang, Yumeng Liu, Zhenghua Li |  |
| 629 |  |  [Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses](https://doi.org/10.18653/v1/2023.findings-emnlp.496) |  | 0 | In this paper, we explore the application of large language models (LLMs) for generating code-tracing questions in introductory programming courses. We designed targeted prompts for GPT4, guiding it to generate code-tracing questions based on code snippets and descriptions. We established a set of... | Aysa Xuemo Fan, Haoran Zhang, Luc Paquette, Rui Zhang |  |
| 630 |  |  [Learning Easily Updated General Purpose Text Representations with Adaptable Task-Specific Prefix](https://doi.org/10.18653/v1/2023.findings-emnlp.497) |  | 0 | Many real-world applications require making multiple predictions from the same text. Fine-tuning a large pre-trained language model for each downstream task causes computational burdens in the inference time due to several times of forward passes. To amortize the computational cost, freezing the... | Amjad Almahairi, KuanHao Huang, Liang Tan, Rui Hou, Ruty Rinott, Sinong Wang |  |
| 631 |  |  [Good Meta-tasks Make A Better Cross-lingual Meta-transfer Learning for Low-resource Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.498) |  | 0 | Model-agnostic meta-learning has garnered attention as a promising technique for enhancing few-shot cross-lingual transfer learning in low-resource scenarios. However, little attention was paid to the impact of data selection strategies on this cross-lingual meta-transfer method, particularly the... | Baoliang Cui, Haihong Tang, Linjuan Wu, Weiming Lu, Zongyi Guo |  |
| 632 |  |  [Reasoning Makes Good Annotators : An Automatic Task-specific Rules Distilling Framework for Low-resource Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.499) |  | 0 | Relation extraction is often challenged by insufficient labeled data. Previous methods exploit knowledge from unlabeled data by generating pseudo labels in a self-training pipeline, which suffers a gradual drift problem. Logic rules, a transferable and explainable form of expert knowledge, have... | Haochen Shi, Juncheng Li, Siliang Tang, Tao Chen, Xiaoqiang Wang, Yilin Lu |  |
| 633 |  |  [Co-training and Co-distillation for Quality Improvement and Compression of Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.500) |  | 0 | Knowledge Distillation (KD) compresses computationally expensive pre-trained language models (PLMs) by transferring their knowledge to smaller models, allowing their use in resource-constrained or real-time settings. However, most smaller models fail to surpass the performance of the original... | Alexander Min, Davis Liang, Hayeon Lee, Hongbo Zhang, Jongpil Kim, Rui Hou, Sung Ju Hwang |  |
| 634 |  |  [ReadPrompt: A Readable Prompting Method for Reliable Knowledge Probing](https://doi.org/10.18653/v1/2023.findings-emnlp.501) |  | 0 | Knowledge probing is a task to assess the knowledge encoded within pre-trained language models (PLMs) by having the PLM complete prompts such as “Italy is located in __,”. The model’s prediction precision serves as a lower bound for the amount of knowledge it contains. Subsequent works explore... | David Ho, Hongru Wang, KamFai Wong, Luyao Ye, WaiChung Kwan, Zezhong Wang |  |
| 635 |  |  [Coherent Entity Disambiguation via Modeling Topic and Categorical Dependency](https://doi.org/10.18653/v1/2023.findings-emnlp.502) |  | 0 | Previous entity disambiguation (ED) methods adopt a discriminative paradigm, where prediction is made based on matching scores between mention context and candidate entities using length-limited encoders. However, these methods often struggle to capture explicit discourse-level dependencies,... | Daxin Jiang, Jie Wu, Linjun Shou, Ming Gong, Xingyao Zhang, Zilin Xiao |  |
| 636 |  |  [How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench](https://doi.org/10.18653/v1/2023.findings-emnlp.503) |  | 0 | We investigate the predictability of large language model (LLM) capabilities: given records of past experiments using different model families, numbers of parameters, tasks, and numbers of in-context examples, can we accurately predict LLM performance on new experiment configurations? Answering... | Harvey Yiyun Fu, Qinyuan Ye, Robin Jia, Xiang Ren |  |
| 637 |  |  [POSQA: Probe the World Models of LLMs with Size Comparisons](https://doi.org/10.18653/v1/2023.findings-emnlp.504) |  | 0 | Embodied language comprehension emphasizes that language understanding is not solely a matter of mental processing in the brain but also involves interactions with the physical and social environment. With the explosive growth of Large Language Models (LLMs) and their already ubiquitous presence in... | Chang Shu, Ehsan Shareghi, Fangyu Liu, Jiuzhou Han, Nigel Collier |  |
| 638 |  |  [Hierarchical Fusion for Online Multimodal Dialog Act Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.505) |  | 0 | We propose a framework for online multimodal dialog act (DA) classification based on raw audio and ASR-generated transcriptions of current and past utterances. Existing multimodal DA classification approaches are limited by ineffective audio modeling and late-stage fusion. We showcase significant... | Adarsh Pyarelal, Md Messal Monem Miah, Ruihong Huang |  |
| 639 |  |  [STEER: Unified Style Transfer with Expert Reinforcement](https://doi.org/10.18653/v1/2023.findings-emnlp.506) |  | 0 | While text style transfer has many applications across natural language processing, the core premise of transferring from a single source style is unrealistic in a real-world setting. In this work, we focus on arbitrary style transfer: rewriting a text from an arbitrary, unknown style to a target... | Faeze Brahman, Jaehun Jung, Sean Welleck, Skyler Hallinan, Ximing Lu, Yejin Choi |  |
| 640 |  |  [Enhancing Argument Structure Extraction with Efficient Leverage of Contextual Information](https://doi.org/10.18653/v1/2023.findings-emnlp.507) |  | 0 | Argument structure extraction (ASE) aims to identify the discourse structure of arguments within documents. Previous research has demonstrated that contextual information is crucial for developing an effective ASE model. However, we observe that merely concatenating sentences in a contextual window... | Fandong Meng, Jie Zhou, Yingjie Li, Yue Zhang, Yun Luo, Zhen Yang |  |
| 641 |  |  [Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate](https://doi.org/10.18653/v1/2023.findings-emnlp.508) |  | 0 | Large Language Models (LLMs) have shown impressive capabilities in various applications, but they still face various inconsistency issues. Existing works primarily focus on the inconsistency issues within a single LLM, while we complementarily explore the inter-consistency among multiple LLMs for... | Bing Qin, Kai Xiong, Ting Liu, Xiao Ding, Yixin Cao |  |
| 642 |  |  [Culturally Aware Natural Language Inference](https://doi.org/10.18653/v1/2023.findings-emnlp.509) |  | 0 | Humans produce and consume language in a particular cultural context, which includes knowledge about specific norms and practices. A listener’s awareness of the cultural context is critical for interpreting the speaker’s meaning. A simple expression like \*I didn’t leave a tip\* implies a strong... | Diyi Yang, Jing Huang |  |
| 643 |  |  [End-to-End Autoregressive Retrieval via Bootstrapping for Smart Reply Systems](https://doi.org/10.18653/v1/2023.findings-emnlp.510) |  | 0 | Reply suggestion systems represent a staple component of many instant messaging and email systems. However, the requirement to produce sets of replies, rather than individual replies, makes the task poorly suited for out-of-the-box retrieval architectures, which only consider individual... | Benjamin Towle, Ke Zhou |  |
| 644 |  |  [Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness](https://doi.org/10.18653/v1/2023.findings-emnlp.511) |  | 0 | The potential of using a large language model (LLM) as a knowledge base (KB) has sparked significant interest. To maintain the knowledge acquired by LLMs, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge. Existing... | Ines Arous, Jackie Chi Kit Cheung, Siva Reddy, Zichao Li |  |
| 645 |  |  [Effects of Human Adversarial and Affable Samples on BERT Generalizability](https://doi.org/10.18653/v1/2023.findings-emnlp.512) |  | 0 | BERT-based models have had strong performance on leaderboards, yet have been demonstrably worse in real-world settings requiring generalization. Limited quantities of training data is considered a key impediment to achieving generalizability in machine learning. In this paper, we examine the impact... | Aparna Elangovan, Estrid He, Karin Verspoor, Yuan Li |  |
| 646 |  |  [Logic Unveils Truth, While Disguise Obscures It: Transition Logic Augmented Response Selection for Multi-Turn Dialogue](https://doi.org/10.18653/v1/2023.findings-emnlp.513) |  | 0 | Multi-turn response selection aims to retrieve a response for a dialogue context from a candidate pool and negative sampling is the key to its retrieval performance. However, previous methods of negative samples tend to yield false negatives due to the one-to-many property in open-domain dialogue,... | Lemao Liu, Rui Yan, Tingchen Fu, Xueliang Zhao |  |
| 647 |  |  [Are Language Models Worse than Humans at Following Prompts? It's Complicated](https://doi.org/10.18653/v1/2023.findings-emnlp.514) |  | 0 | Prompts have been the center of progress in advancing language models’ zero-shot and few-shot performance. However, recent work finds that models can perform surprisingly well when given intentionally irrelevant or misleading prompts. Such results may be interpreted as evidence that model behavior... | Albert Webson, Alyssa Marie Loo, Ellie Pavlick, Qinan Yu |  |
| 648 |  |  [A Sequence-to-Structure Approach to Document-level Targeted Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.515) |  | 0 | Most previous studies on aspect-based sentiment analysis (ABSA) were carried out at the sentence level, while the research of document-level ABSA has not received enough attention. In this work, we focus on the document-level targeted sentiment analysis task, which aims to extract the opinion... | Hongjie Cai, Jianfei Yu, Nan Song, Rui Xia, Xinyu Dai, Zhen Wu |  |
| 649 |  |  [Generating Extractive Answers: Gated Recurrent Memory Reader for Conversational Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.516) |  | 0 | Conversational question answering (CQA) is a more complicated task than traditional single-turn machine reading comprehension (MRC). Different from large language models (LLMs) like ChatGPT, the models of CQA need to extract answers from given contents to answer follow-up questions according to... | Qing Yang, Xuanyu Zhang |  |
| 650 |  |  [Text2Tree: Aligning Text Representation to the Label Tree Hierarchy for Imbalanced Medical Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.517) |  | 0 | Deep learning approaches exhibit promising performances on various text tasks. However, they are still struggling on medical text classification since samples are often extremely imbalanced and scarce. Different from existing mainstream approaches that focus on supplementary semantics with external... | Danny Chen, Haojun Gao, Jiahuan Yan, Jian Wu, Jintai Chen, Kai Zhang, Weize Liu |  |
| 651 |  |  [Impact of Co-occurrence on Factual Knowledge of Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.518) |  | 0 | Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results... | Cheongwoong Kang, Jaesik Choi |  |
| 652 |  |  [CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.519) |  | 0 | Large language models have demonstrated the capability to perform on machine translation when the input is prompted with a few examples (in-context learning). Translation quality depends on various features of the selected examples, such as their quality and relevance, but previous work has... | Anoop Kunchukuttan, Aswanth M., Raj Dabre, Ratish Puduppully |  |
| 653 |  |  [Swap and Predict - Predicting the Semantic Changes in Words across Corpora by Context Swapping](https://doi.org/10.18653/v1/2023.findings-emnlp.520) |  | 0 | Meanings of words change over time and across domains. Detecting the semantic changes of words is an important task for various NLP applications that must make time-sensitive predictions. We consider the problem of predicting whether a given target word, w, changes its meaning between two different... | Danushka Bollegala, Taichi Aida |  |
| 654 |  |  [Beyond Layout Embedding: Layout Attention with Gaussian Biases for Structured Document Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.521) |  | 0 | Effectively encoding layout information is a central problem in structured document understanding. Most existing methods rely heavily on millions of trainable parameters to learn the layout features of each word from Cartesian coordinates. However, two unresolved questions remain: (1) Is the... | Chao Deng, Junlan Feng, Shuo Lei, Shuyuan Peng, Xi Zhu, Xue Han |  |
| 655 |  |  [ESPVR: Entity Spans Position Visual Regions for Multimodal Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.522) |  | 0 | Multimodal Named Entity Recognition (MNER) uses visual information to improve the performance of text-only Named Entity Recognition (NER). However, existing methods for acquiring local visual information suffer from certain limitations: (1) using an attention-based method to extract visual regions... | Guanglu Sun, Xinyu Liu, Xiujiao Li |  |
| 656 |  |  [Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency](https://doi.org/10.18653/v1/2023.findings-emnlp.523) |  | 0 | With growing capabilities of large language models, prompting them has become the dominant way to access them. This has motivated the development of strategies for automatically selecting effective language prompts. In this paper, we introduce \*\*pFlat\*\* (prompt flatness), a new metric to... | Boyuan Zheng, Daniel Khashabi, Lingfeng Shen, Weiting Tan |  |
| 657 |  |  [Detecting Erroneously Recognized Handwritten Byzantine Text](https://doi.org/10.18653/v1/2023.findings-emnlp.524) |  | 0 | Handwritten text recognition (HTR) yields textual output that comprises errors, which are considerably more compared to that of recognised printed (OCRed) text. Post-correcting methods can eliminate such errors but may also introduce errors. In this study, we investigate the issues arising from... | Holger Essler, John Pavlopoulos, Paraskevi Platanou, Vasiliki Kougia |  |
| 658 |  |  [Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment](https://doi.org/10.18653/v1/2023.findings-emnlp.525) |  | 0 | Pretrained language models (PLMs) based knowledge-grounded dialogue systems are prone to generate responses that are factually inconsistent with the provided knowledge source. In such inconsistent responses, the dialogue models fail to accurately express the external factual knowledge they rely... | Boyang Xue, Fei Mi, Hongru Wang, KamFai Wong, Lifeng Shang, Qun Liu, Rui Wang, Weichao Wang, Xin Jiang, Yasheng Wang |  |
| 659 |  |  [TRIP: Accelerating Document-level Multilingual Pre-training via Triangular Document-level Pre-training on Parallel Data Triplets](https://doi.org/10.18653/v1/2023.findings-emnlp.526) |  | 0 | Despite the success of multilingual sequence-to-sequence pre-training, most existing approaches rely on document-level monolingual corpora in many different languages, sentence-level bilingual corpora, and sometimes synthetic document-level bilingual corpora. This hampers the performance with... | Anthony Aue, Arul Menezes, Dongdong Zhang, Furu Wei, Haoyang Huang, Hongyuan Lu, Shuming Ma, Wai Lam, Zhaochuan Gao |  |
| 660 |  |  [Frequency Balanced Datasets Lead to Better Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.527) |  | 0 | This paper reports on the experiments aimed to improve our understanding of the role of the amount of data required for training attention-based transformer language models. Specifically, we investigate the impact of reducing the immense amounts of required pre-training data through sampling... | Mireia Farrús, Núria Bel, Rodolfo Zevallos |  |
| 661 |  |  [Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.528) |  | 0 | The recent success of large pre-trained language models (PLMs) heavily hinges on massive labeled data, which typically produces inferior performance in low-resource scenarios. To remedy this dilemma, we study self-training as one of the predominant semi-supervised learning (SSL) approaches, which... | Chengyu Wang, Jianing Wang, Jun Huang, Ming Gao, Nuo Chen, Qiushi Sun, Xiang Li |  |
| 662 |  |  [TR-Rules: Rule-based Model for Link Forecasting on Temporal Knowledge Graph Considering Temporal Redundancy](https://doi.org/10.18653/v1/2023.findings-emnlp.529) |  | 0 | Temporal knowledge graph (TKG) has been proved to be an effective way for modeling dynamic facts in real world. Many efforts have been devoted into predicting future events i.e. extrapolation, on TKGs. Recently, rule-based knowledge graph completion methods which are considered to be more... | Haihong E, Haoran Luo, Meina Song, Mingzhi Sun, Ningyuan Li, Shi Li, Tianyu Yao, Yong Wang |  |
| 663 |  |  [On the Transferability of Visually Grounded PCFGs](https://doi.org/10.18653/v1/2023.findings-emnlp.530) |  | 0 | There has been a significant surge of interest in visually grounded grammar induction in recent times. While a variety of models have been developed for the task and have demonstrated impressive performance, they have not been evaluated on text domains that are different from the training domain,... | Ivan Titov, Yanpeng Zhao |  |
| 664 |  |  [Analysis of Style-Shifting on Social Media: Using Neural Language Model Conditioned by Social Meanings](https://doi.org/10.18653/v1/2023.findings-emnlp.531) |  | 0 | In this paper, we propose a novel framework for evaluating style-shifting in social media conversations. Our proposed framework captures changes in an individual’s conversational style based on surprisals predicted by a personalized neural language model for individuals. Our personalized language... | Akishige Yuguchi, Angel Fernando Garcia Contreras, Koichiro Yoshino, Marie Katsurai, Seiya Kawano, Shota Kanezaki |  |
| 665 |  |  [Linguistic Compression in Single-Sentence Human-Written Summaries](https://doi.org/10.18653/v1/2023.findings-emnlp.532) |  | 0 | Summarizing texts involves significant cognitive efforts to compress information. While advances in automatic summarization systems have drawn attention from the NLP and linguistics communities to this topic, there is a lack of computational studies of linguistic patterns in human-written... | Fangcong Yin, Marten van Schijndel |  |
| 666 |  |  [MCLF: A Multi-grained Contrastive Learning Framework for ASR-robust Spoken Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.533) |  | 0 | Enhancing the robustness towards Automatic Speech Recognition (ASR) errors is of great importance for Spoken Language Understanding (SLU). Trending ASR-robust SLU systems have witnessed impressive improvements through global contrastive learning. However, although most ASR errors occur only at... | Dongsheng Chen, Xuxin Cheng, Zhihong Zhu, Zhiqi Huang |  |
| 667 |  |  [Beyond Candidates : Adaptive Dialogue Agent Utilizing Persona and Knowledge](https://doi.org/10.18653/v1/2023.findings-emnlp.534) |  | 0 | To build ultimate dialogue agents, previous studies suggest models that ground both persona and knowledge. However, applying the dialogue system directly to the usual conversation is still limited because the system requires a complete sentence-formed persona and knowledge candidate sets from the... | Heuiseok Lim, Jeongwook Kim, Jinsung Kim, Jungwoo Lim, Myunghoon Kang, Yuna Hur |  |
| 668 |  |  [SmartSpanNER: Making SpanNER Robust in Low Resource Scenarios](https://doi.org/10.18653/v1/2023.findings-emnlp.535) |  | 0 | Named Entity Recognition (NER) is one of the most fundamental tasks in natural language processing. Span-level prediction (SpanNER) is more naturally suitable for nested NER than sequence labeling (SeqLab). However, according to our experiments, the SpanNER method is more sensitive to the amount of... | Hao Yang, Min Zhang, Shimin Tao, Xiaosong Qiao, Yanqing Zhao |  |
| 669 |  |  [ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.536) |  | 0 | We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test and small validation sets, without training data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such... | Avia Efrat, Jonathan Berant, Maor Ivgi, Omer Levy, Uri Shaham |  |
| 670 |  |  [Data Selection Curriculum for Abstractive Text Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.537) |  | 0 | Abstractive Text Summarization (ATS) models are commonly trained using large-scale data that is randomly shuffled. However, the impact of data selection and data ordering on ATS models remains a relatively unexplored research area, where a significant challenge lies in accurately assessing the... | Jianfei He, Ruifeng Yuan, Shichao Sun, Wenjie Li, Xiaohua Jia, Ziqiang Cao |  |
| 671 |  |  [Romanization-based Large-scale Adaptation of Multilingual Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.538) |  | 0 | Large multilingual pretrained language models (mPLMs) have become the de facto state of the art for cross-lingual transfer in NLP. However, their large-scale deployment to many languages, besides pretraining data scarcity, is also hindered by the increase in vocabulary size and limitations in their... | Iryna Gurevych, Ivan Vulic, Jonas Pfeiffer, Sebastian Ruder, Sukannya Purkayastha |  |
| 672 |  |  [Measuring bias in Instruction-Following models with P-AT](https://doi.org/10.18653/v1/2023.findings-emnlp.539) |  | 0 | Instruction-Following Language Models (IFLMs) are promising and versatile tools for solving many downstream, information-seeking tasks. Given their success, there is an urgent need to have a shared resource to determine whether existing and new IFLMs are prone to produce biased language... | Dario Onorati, Davide Venditti, Elena Sofia Ruzzetti, Fabio Massimo Zanzotto, Leonardo Ranaldi |  |
| 673 |  |  [Open-ended Commonsense Reasoning with Unrestricted Answer Candidates](https://doi.org/10.18653/v1/2023.findings-emnlp.540) |  | 0 | Open-ended Commonsense Reasoning is defined as solving a commonsense question without providing 1) a short list of answer candidates and 2) a pre-defined answer scope. Conventional ways of formulating the commonsense question into a question-answering form or utilizing external knowledge to learn... | Chen Ling, Haifeng Chen, Katsushi Matsuda, Liang Zhao, Mika Oishi, Takao Osaki, Wei Cheng, Xuchao Zhang, Xujiang Zhao, Yanchi Liu |  |
| 674 |  |  [Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units](https://doi.org/10.18653/v1/2023.findings-emnlp.541) |  | 0 | We introduce DISSC, a novel, lightweight method that converts the rhythm, pitch contour and timbre of a recording to a target speaker in a textless manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on timbre, and ignore people’s unique speaking style (prosody). The proposed... | Gallil Maimon, Yossi Adi |  |
| 675 |  |  [Knowledge-Selective Pretraining for Attribute Value Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.542) |  | 0 | Attribute Value Extraction (AVE) aims to retrieve the values of attributes from the product profiles. The state-of-the-art methods tackle the AVE task through a question-answering (QA) paradigm, where the value is predicted from the context (i.e. product profile) given a query (i.e. attributes).... | Bing Yin, Chao Zhang, Chenwei Zhang, Haoming Jiang, Hui Liu, Qingyu Yin, William Wang, Xian Li, Xiaodan Zhu, Yifan Gao, Zheng Li, Zhengyang Wang |  |
| 676 |  |  [New Datasets and Controllable Iterative Data Augmentation Method for Code-switching ASR Error Correction](https://doi.org/10.18653/v1/2023.findings-emnlp.543) |  | 0 | With the wide use of automatic speech recognition(ASR) systems, researchers pay more attention to the ASR error correction task to improve the quality of recognition results. In particular, ASR in bilingual or multilingual settings, namely code-switching ASR, has greater challenges and research... | Rongjun Li, Wei Peng, Xiaojun Wan, Zhaohong Wan |  |
| 677 |  |  [Efficient k-NN Search with Cross-Encoders using Adaptive Multi-Round CUR Decomposition](https://doi.org/10.18653/v1/2023.findings-emnlp.544) |  | 0 | Cross-encoder models, which jointly encode and score a query-item pair, are prohibitively expensive for direct k-nearest neighbor (k-NN) search. Consequently, k-NN search typically employs a fast approximate retrieval (e.g. using BM25 or dual-encoder vectors), followed by reranking with a... | Andrew McCallum, Manzil Zaheer, Nicholas Monath, Nishant Yadav |  |
| 678 |  |  [Isotropic Representation Can Improve Zero-Shot Cross-Lingual Transfer on Multilingual Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.545) |  | 0 | With the development of multilingual pre-trained language models (mPLMs), zero-shot cross-lingual transfer shows great potential. To further improve the performance of cross-lingual transfer, many studies have explored representation misalignment caused by morphological differences but neglected... | Hai Ye, Jikai Wang, Juntao Li, Min Zhang, Yixin Ji |  |
| 679 |  |  [Blackbird language matrices (BLM), a new task for rule-like generalization in neural networks: Can Large Language Models pass the test?](https://doi.org/10.18653/v1/2023.findings-emnlp.546) |  | 0 | How do we evaluate Large Language Models (LLMs) and determine the aspects and limits of their intelligent behaviour? It is currently conjectured that shortcomings of LLMs in multi-linguality and reasoning are due to a lack of ability to generalize. It has been argued that, instead, humans are... | Paola Merlo |  |
| 680 |  |  [DistillCSE: Distilled Contrastive Learning for Sentence Embeddings](https://doi.org/10.18653/v1/2023.findings-emnlp.547) |  | 0 | This paper proposes the DistillCSE framework, which performs contrastive learning under the self-training paradigm with knowledge distillation. The potential advantage of DistillCSE is its self-enhancing feature: using a base model to provide additional supervision signals, a stronger model may be... | Jiahao Xu, Lemao Liu, Lihui Chen, Wei Shao |  |
| 681 |  |  [GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets](https://doi.org/10.18653/v1/2023.findings-emnlp.548) |  | 0 | Named Entity Recognition (NER) models play a crucial role in various NLP tasks, including information extraction (IE) and text understanding. In academic writing, references to machine learning models and datasets are fundamental components of various computer science publications and necessitate... | Lu Gan, Matthäus Zloch, Saurav Karmakar, Stefan Dietze, Wolfgang Otto |  |
| 682 |  |  [Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.549) |  | 0 | Multi-document summarization (MDS) assumes a set of topic-related documents are provided as input. In practice, this document set is not always available; it would need to be retrieved given an information need, i.e. a question or topic statement, a setting we dub “open-domain’ MDS. We study this... | Arman Cohan, Bo Wang, Gary D. Bader, John M. Giorgi, Kyle Lo, Luca Soldaini, Lucy Lu Wang |  |
| 683 |  |  [Few-shot Unified Question Answering: Tuning Models or Prompts?](https://doi.org/10.18653/v1/2023.findings-emnlp.550) |  | 0 | Question-answering (QA) tasks often investigate specific question types, knowledge domains, or reasoning skills, leading to specialized models catering to specific categories of QA tasks. While recent research has explored the idea of unified QA models, such models are usually explored for... | Bo Pang, Meghana Bhat, Semih Yavuz, Srijan Bansal, Yingbo Zhou |  |
| 684 |  |  [Finding Common Ground: Annotating and Predicting Common Ground in Spoken Conversations](https://doi.org/10.18653/v1/2023.findings-emnlp.551) |  | 0 | When we communicate with other humans, we do not simply generate a sequence of words. Rather, we use our cognitive state (beliefs, desires, intentions) and our model of the audience’s cognitive state to create utterances that affect the audience’s cognitive state in the intended manner. An... | Adil Soubki, Magdalena Markowska, Mohammad Taghizadeh, Owen Rambow, Seyed Abolghasem Mirroshandel |  |
| 685 |  |  [Getting MoRE out of Mixture of Language Model Reasoning Experts](https://doi.org/10.18653/v1/2023.findings-emnlp.552) |  | 0 | While recent large language models (LLMs) improve on various question answering (QA) datasets, it remains difficult for a single model to generalize across question types that require distinct reasoning abilities. We provide empirical evidence that state-of-the-art LLMs suffer from poor... | Chen Zhao, Chenglei Si, Jordan L. BoydGraber, Luke Zettlemoyer, Weijia Shi |  |
| 686 |  |  ["You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract Meaning Representation](https://doi.org/10.18653/v1/2023.findings-emnlp.553) |  | 0 | Large language models (LLMs) demonstrate an amazing proficiency and fluency in the use of language. Does that mean that they have also acquired insightful linguistic knowledge about the language, to an extent that they can serve as an “expert linguistic annotator’? In this paper, we examine the... | Allyson Ettinger, Chandra Bhagavatula, Jena D. Hwang, Valentina Pyatkin, Yejin Choi |  |
| 687 |  |  [Zero-Shot Data Maps. Efficient Dataset Cartography Without Model Training](https://doi.org/10.18653/v1/2023.findings-emnlp.554) |  | 0 | Data Maps (Swayamdipta, et al. 2020) have emerged as a powerful tool for diagnosing large annotated datasets. Given a model fitted on a dataset, these maps show each data instance from the dataset in a 2-dimensional space defined by a) the model’s confidence in the true class and b) the variability... | Angelo Basile, Marc FrancoSalvador, Paolo Rosso |  |
| 688 |  |  [Isotropy-Enhanced Conditional Masked Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.555) |  | 0 | Non-autoregressive models have been widely used for various text generation tasks to accelerate the inference process but at the cost of generation quality to some extent. To achieve a good balance between inference speedup and generation quality, iterative NAR models like CMLM and Disco are... | Juntao Li, Min Zhang, Pei Guo, Yisheng Xiao, Yixin Ji |  |
| 689 |  |  [Scaling Law for Document Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.556) |  | 0 | The scaling laws of language models have played a significant role in advancing large language models. In order to promote the development of document translation, we systematically examine the scaling laws in this field. In this paper, we carry out an in-depth analysis of the influence of three... | Min Zhang, Shuhao Gu, Yang Feng, Zhuocheng Zhang |  |
| 690 |  |  [Automatic Pronunciation Assessment - A Review](https://doi.org/10.18653/v1/2023.findings-emnlp.557) |  | 0 | Pronunciation assessment and its application in computer-aided pronunciation training (CAPT) have seen impressive progress in recent years. With the rapid growth in language processing and deep learning over the past few years, there is a need for an updated review. In this paper, we review methods... | Ahmed Ali, Shammur Absar Chowdhury, Yassine El Kheir |  |
| 691 |  |  [Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model](https://doi.org/10.18653/v1/2023.findings-emnlp.558) |  | 0 | Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the... | Kaushik Roy, Sayeed Shafayet Chowdhury, Yinghan Long |  |
| 692 |  |  [PUNR: Pre-training with User Behavior Modeling for News Recommendation](https://doi.org/10.18653/v1/2023.findings-emnlp.559) |  | 0 | News recommendation aims to predict click behaviors based on user behaviors. How to effectively model the user representations is the key to recommending preferred news. Existing works are mostly focused on improvements in the supervised fine-tuning stage. However, there is still a lack of... | Guangyuan Ma, Hongtao Liu, Qing Yang, Songlin Hu, Wanhui Qian, Xing Wu, Zhepeng Lv |  |
| 693 |  |  [Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design](https://doi.org/10.18653/v1/2023.findings-emnlp.560) |  | 0 | Discovering novel catalysts requires complex reasoning involving multiple chemical properties and resultant trade-offs, leading to a combinatorial growth in the search space. While large language models (LLM) have demonstrated novel capabilities for chemistry through complex instruction following... | Carl Edwards, Heng Ji, Henry Sprueill, Mariefel V. Olarte, Sutanay Choudhury, Udishnu Sanyal |  |
| 694 |  |  [Measure Children's Mindreading Ability with Machine Reading](https://doi.org/10.18653/v1/2023.findings-emnlp.561) |  | 0 | Recently, much research in psychology has benefited from the advances in machine learning techniques. Some recent studies showed that it is possible to build automated scoring models for children’s mindreading. These models were trained on a set of manually-labeled question-response pairs, which... | Xiang Zhou, Xiaohua Wang, Xiaoqing Zheng, Xuanjing Huang, Yuliang Yan |  |
| 695 |  |  [Crosslingual Transfer Learning for Low-Resource Languages Based on Multilingual Colexification Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.562) |  | 0 | In comparative linguistics, colexification refers to the phenomenon of a lexical form conveying two or more distinct meanings. Existing work on colexification patterns relies on annotated word lists, limiting scalability and usefulness in NLP. In contrast, we identify colexification patterns of... | Haotian Ye, Hinrich Schütze, Leonie Weissweiler, Renhao Pei, Yihong Liu |  |
| 696 |  |  [Injecting structural hints: Using language models to study inductive biases in language learning](https://doi.org/10.18653/v1/2023.findings-emnlp.563) |  | 0 | Both humans and transformer language models are able to learn language without explicit structural supervision. What cognitive inductive biases make this learning possible? Here, we examine the effect of different inductive learning biases by actively controlling the inductive biases of artificial... | Dan Jurafsky, Isabel Papadimitriou |  |
| 697 |  |  [Machine Reading Comprehension using Case-based Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.564) |  | 0 | We present an accurate and interpretable method for answer extraction in machine reading comprehension that is reminiscent of case-based reasoning (CBR) from classical AI. Our method (CBR-MRC) builds upon the hypothesis that contextualized answers to similar questions share semantic similarities... | Andrew McCallum, Dhruv Agarwal, Dung Thai, Hannaneh Hajishirzi, JayYoon Lee, Manzil Zaheer, Mudit Chaudhary, Rajarshi Das, Wenlong Zhao |  |
| 698 |  |  [Unleashing the Power of Language Models in Text-Attributed Graph](https://doi.org/10.18653/v1/2023.findings-emnlp.565) |  | 0 | Representation learning on graph has been demonstrated to be a powerful tool for solving real-world problems. Text-attributed graph carries both semantic and structural information among different types of graphs. Existing works have paved the way for knowledge extraction of this type of data by... | Haoyu Kuang, Haozhe Zhang, Jiarong Xu, Qi Zhang, Xuanjing Huang, Zhongyu Wei, Zuyu Zhao |  |
| 699 |  |  [Locally Differentially Private Document Generation Using Zero Shot Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.566) |  | 0 | Numerous studies have highlighted the privacy risks associated with large language models. Our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called... | PinYu Chen, Saiteja Utpala, Sara Hooker |  |
| 700 |  |  [Contrastive Deterministic Autoencoders For Language Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.567) |  | 0 | Variational autoencoders (VAEs) are a popular family of generative models with wide applicability. Training VAEs, especially for text, often runs into the issue of posterior collapse, resulting in loss of representation quality. Deterministic autoencoders avoid this issue, and have been explored... | Amur Ghose, Pascal Poupart |  |
| 701 |  |  [CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.568) |  | 0 | We propose CHiLL (Crafting High-Level Latents), an approach for natural-language specification of features for linear models. CHiLL prompts LLMs with expert-crafted queries to generate interpretable features from health records. The resulting noisy labels are then used to train a simple linear... | Byron C. Wallace, Denis Jered McInerney, Geoffrey S. Young, JanWillem van de Meent |  |
| 702 |  |  [Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers](https://doi.org/10.18653/v1/2023.findings-emnlp.569) |  | 0 | Recent applications of LLMs in Machine Reading Comprehension (MRC) systems have shown impressive results, but the use of shortcuts, mechanisms triggered by features spuriously correlated to the true label, has emerged as a potential threat to their reliability. We analyze the problem from two... | Mosh Levy, Shauli Ravfogel, Yoav Goldberg |  |
| 703 |  |  [Large Language Models Meet Harry Potter: A Dataset for Aligning Dialogue Agents with Characters](https://doi.org/10.18653/v1/2023.findings-emnlp.570) |  | 0 | In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT and GPT4 have demonstrated immense potential in constructing open-domain dialogue agents. However, aligning these agents with specific characters or individuals remains a considerable challenge due to the complexities of... | Deng Cai, Haiyun Jiang, Jia Li, Longyue Wang, Nuo Chen, Yan Wang, Yuhan Li, Ziyang Chen |  |
| 704 |  |  [Quick Back-Translation for Unsupervised Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.571) |  | 0 | The field of unsupervised machine translation has seen significant advancement from the marriage of the Transformer and the back-translation algorithm. The Transformer is a powerful generative model, and back-translation leverages Transformer’s high-quality translations for iterative... | Benjamin Brimacombe, Jiawei Zhou |  |
| 705 |  |  [SIR-ABSC: Incorporating Syntax into RoBERTa-based Sentiment Analysis Models with a Special Aggregator Token](https://doi.org/10.18653/v1/2023.findings-emnlp.572) |  | 0 | We present a simple, but effective method to incorporate syntactic dependency information directly into transformer-based language models (e.g. RoBERTa) for tasks such as Aspect-Based Sentiment Classification (ABSC), where the desired output depends on specific input tokens. In contrast to prior... | Ikhyun Cho, Julia Hockenmaier, Yoonhwa Jung |  |
| 706 |  |  [Citance-Contextualized Summarization of Scientific Papers](https://doi.org/10.18653/v1/2023.findings-emnlp.573) |  | 0 | Current approaches to automatic summarization of scientific papers generate informative summaries in the form of abstracts. However, abstracts are not intended to show the relationship between a paper and the references cited in it. We propose a new contextualized summarization approach that can... | Ahmad Dawar Hakimi, Khalid Al Khatib, Martin Potthast, Shahbaz Syed |  |
| 707 |  |  [SegAugment: Maximizing the Utility of Speech Translation Data with Segmentation-based Augmentations](https://doi.org/10.18653/v1/2023.findings-emnlp.574) |  | 0 | End-to-end Speech Translation is hindered by a lack of available data resources. While most of them are based on documents, a sentence-level version is available, which is however single and static, potentially impeding the usefulness of the data. We propose a new data augmentation strategy,... | Ioannis Tsiamas, José A. R. Fonollosa, Marta R. Costajussà |  |
| 708 |  |  [Intersectional Stereotypes in Large Language Models: Dataset and Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.575) |  | 0 | Despite many stereotypes targeting intersectional demographic groups, prior studies on stereotypes within Large Language Models (LLMs) primarily focus on broader, individual categories. This research bridges this gap by introducing a novel dataset of intersectional stereotypes, curated with the... | Brian Chiang, Lili Wang, Soroush Vosoughi, Tong Wu, Weicheng Ma |  |
| 709 |  |  [Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond](https://doi.org/10.18653/v1/2023.findings-emnlp.576) |  | 0 | Vision-language (VL) understanding tasks evaluate models’ comprehension of complex visual scenes through multiple-choice questions. However, we have identified two dataset biases that models can exploit as shortcuts to resolve various VL tasks correctly without proper understanding. The first type... | Haoxuan You, KaiWei Chang, Keyang Xu, Long Chen, Noel Codella, ShihFu Chang, Wenhao Li, Yicheng He, Zhecan Wang |  |
| 710 |  |  [The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who](https://doi.org/10.18653/v1/2023.findings-emnlp.577) |  | 0 | Automated fact-checking is often presented as an epistemic tool that fact-checkers, social media consumers, and other stakeholders can use to fight misinformation. Nevertheless, few papers thoroughly discuss how. We document this by analysing 100 highly-cited papers, and annotating epistemic... | Andreas Vlachos, Michael Sejr Schlichtkrull, Nedjma Ousidhoum |  |
| 711 |  |  [Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression](https://doi.org/10.18653/v1/2023.findings-emnlp.578) |  | 0 | Large-scale pre-trained language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, the massive size of these models poses huge challenges for their deployment in real-world applications. While numerous model compression techniques... | Dongyan Zhao, Jiahao Liu, Jiduan Liu, Jingang Wang, Qifan Wang, Ran Wang, Rui Yan, Xunliang Cai |  |
| 712 |  |  [COUNT: COntrastive UNlikelihood Text Style Transfer for Text Detoxification](https://doi.org/10.18653/v1/2023.findings-emnlp.579) |  | 0 | Offensive and toxic text on social media platforms can lead to polarization and divisiveness within online communities and hinders constructive dialogue. Text detoxification is a crucial task in natural language processing to ensure the generation of non-toxic and safe text. Text detoxification is... | Ali Pesaranghader, Manasa Bharadwaj, Mohammad Mahdi Abdollah Pour, Nikhil Verma, Parsa Farinneya, Scott Sanner |  |
| 713 |  |  [KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.findings-emnlp.580) |  | 0 | Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC and they can be categorized into two main classes, including triple-based and test-based approaches. Triple-based methods struggle... | James T. Kwok, Qiushi Huang, Yanbin Wei, Yu Zhang |  |
| 714 |  |  [Show, Write, and Retrieve: Entity-aware Article Generation and Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.581) |  | 0 | Article comprehension is an important challenge in natural language processing with many applications such as article generation or image-to-article retrieval. Prior work typically encodes all tokens in articles uniformly using pretrained language models. However, in many applications, such as... | Bryan A. Plummer, Yiwen Gu, Zhongping Zhang |  |
| 715 |  |  [A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing](https://doi.org/10.18653/v1/2023.findings-emnlp.582) |  | 0 | Two of the central factors believed to underpin human sentence processing difficulty are expectations and retrieval from working memory. A recent attempt to create a unified cognitive model integrating these two factors have relied on the parallels between the self-attention mechanism of... | Tal Linzen, William Timkey |  |
| 716 |  |  [Annotations Are Not All You Need: A Cross-modal Knowledge Transfer Network for Unsupervised Temporal Sentence Grounding](https://doi.org/10.18653/v1/2023.findings-emnlp.583) |  | 0 | This paper addresses the task of temporal sentence grounding (TSG). Although many respectable works have made decent achievements in this important topic, they severely rely on massive expensive video-query paired annotations, which require a tremendous amount of human effort to collect in... | Daizong Liu, Kai Zou, Keke Tang, Pan Zhou, Wanlong Fang, Xiang Fang, Yu Cheng |  |
| 717 |  |  [Parameter Efficient Multi-task Fine-tuning by Learning to Transfer Token-wise Prompts](https://doi.org/10.18653/v1/2023.findings-emnlp.584) |  | 0 | Prompt tuning has been proven to be successful on various tasks by incorporating a small number of trainable parameters while freezing large pre-trained language models (PLMs). However, it is still unsettled how to generate more proper prompts for any individual examples and how to extend prompt... | Changze Lv, Jianhan Xu, Longtao Huang, Muling Wu, Tianlong Li, Wenhao Liu, Xiaoqing Zheng, Xuanjing Huang, Zixuan Ling |  |
| 718 |  |  [A Rewriting Approach for Gender Inclusivity in Portuguese](https://doi.org/10.18653/v1/2023.findings-emnlp.585) |  | 0 | In recent years, there has been a notable rise in research interest regarding the integration of gender-inclusive and gender-neutral language in natural language processing models. A specific area of focus that has gained practical and academic significant interest is gender-neutral rewriting,... | Leonor Veloso, Luísa Coheur, Rui Ribeiro |  |
| 719 |  |  [EARA: Improving Biomedical Semantic Textual Similarity with Entity-Aligned Attention and Retrieval Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.586) |  | 0 | Measuring Semantic Textual Similarity (STS) is a fundamental task in biomedical text processing, which aims at quantifying the similarity between two input biomedical sentences. Unfortunately, the STS datasets in the biomedical domain are relatively smaller but more complex in semantics than common... | Buzhou Tang, KaChun Wong, Linjing Liu, Qingcai Chen, Xin Yang, Yang Xiang, Ying Xiong |  |
| 720 |  |  [Neuro-Symbolic Sentiment Analysis with Dynamic Word Sense Disambiguation](https://doi.org/10.18653/v1/2023.findings-emnlp.587) |  | 0 | Sentiment analysis is a task that highly depends on the understanding of word senses. Traditional neural network models are black boxes that represent word senses as vectors that are uninterpretable for humans. On the other hand, the application of Word Sense Disambiguation (WSD) systems in... | Erik Cambria, Kai He, Rui Mao, Xulang Zhang |  |
| 721 |  |  [Role of Context in Unsupervised Sentence Representation Learning: the Case of Dialog Act Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.588) |  | 0 | Unsupervised learning of word representations involves capturing the contextual information surrounding word occurrences, which can be grounded in the observation that word form is largely disconnected from word meaning. While there are fewer reasons to believe that the same holds for sentences,... | Emmanuel Keuleers, Rastislav Hronsky |  |
| 722 |  |  [CLMSM: A Multi-Task Learning Framework for Pre-training on Procedural Text](https://doi.org/10.18653/v1/2023.findings-emnlp.589) |  | 0 | In this paper, we propose \*\*\*CLMSM\*\*\*, a domain-specific, continual pre-training framework, that learns from a large set of procedural recipes. \*\*\*CLMSM\*\*\* uses a Multi-Task Learning Framework to optimize two objectives - a) Contrastive Learning using hard triplets to learn fine-grained... | Abhilash Nandy, Manav Nitin Kapadnis, Niloy Ganguly, Pawan Goyal |  |
| 723 |  |  [Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking](https://doi.org/10.18653/v1/2023.findings-emnlp.590) |  | 0 | In the field of information retrieval, Query Likelihood Models (QLMs) rank documents based on the probability of generating the query given the content of a document. Recently, advanced large language models (LLMs) have emerged as effective QLMs, showcasing promising ranking capabilities. This... | Bevan Koopman, Bing Liu, Guido Zuccon, Shengyao Zhuang |  |
| 724 |  |  [On General Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.591) |  | 0 | Natural Language Processing prides itself to be an empirically-minded, if not outright empiricist field, and yet lately it seems to get itself into essentialist debates on issues of meaning and measurement (“Do Large Language Models Understand Language, And If So, How Much?”). This is not by... | David Schlangen |  |
| 725 |  |  [USB: A Unified Summarization Benchmark Across Tasks and Domains](https://doi.org/10.18653/v1/2023.findings-emnlp.592) |  | 0 | While the NLP community has produced numerous summarization benchmarks, none provide the rich annotations required to simultaneously address many important problems related to control and reliability. We introduce a Wikipedia-derived benchmark, complemented by a rich set of crowd-sourced... | Byron C. Wallace, Jeffrey P. Bigham, Kundan Krishna, Prakhar Gupta, Sanjana Ramprasad, Zachary C. Lipton |  |
| 726 |  |  [tagE: Enabling an Embodied Agent to Understand Human Instructions](https://doi.org/10.18653/v1/2023.findings-emnlp.593) |  | 0 | Natural language serves as the primary mode of communication when an intelligent agent with a physical presence engages with human beings. While a plethora of research focuses on natural language understanding (NLU), encompassing endeavors such as sentiment analysis, intent prediction, question... | Avik Mitra, Chayan Sarkar, Pradip Pramanick, Tapas Nayak |  |
| 727 |  |  [Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.594) |  | 0 | Hierarchical multi-label text classification (HMTC) aims at utilizing a label hierarchy in multi-label classification. Recent approaches to HMTC deal with the problem of imposing an overconstrained premise on the output space by using contrastive learning on generated samples in a semi-supervised... | Jeff Z. Pan, Jie He, Simon Chi Lok U, Víctor GutiérrezBasulto |  |
| 728 |  |  [Uncovering Limitations in Text-to-Image Generation: A Contrastive Approach with Structured Semantic Alignment](https://doi.org/10.18653/v1/2023.findings-emnlp.595) |  | 0 | Despite significant advancements in text-to-image generation models, they still face challenges when it comes to producing highly detailed or complex images based on textual descriptions. In order to explore these limitations, we propose a Structured Semantic Alignment (SSA) method for evaluating... | Hongyu Zhang, Qianyu Feng, Yulei Sui |  |
| 729 |  |  [An Intent-based and Annotation-free Method for Duplicate Question Detection in CQA Forums](https://doi.org/10.18653/v1/2023.findings-emnlp.596) |  | 0 | With the advent of large language models (LLMs), Community Question Answering (CQA) forums offer well-curated questions and answers that can be utilized for instruction-tuning, effectively training LLMs to be aligned with human intents. However, the issue of duplicate questions arises as the volume... | Hansu Gu, Ning Gu, Peng Zhang, Tun Lu, Yubo Shu |  |
| 730 |  |  [Accelerating Multiple Intent Detection and Slot Filling via Targeted Knowledge Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.597) |  | 0 | Recent non-autoregressive Spoken Language Understanding (SLU) models have attracted increasing attention because of their encouraging inference speed. However, most of existing methods (1) suffer from the multi-modality problem since they have little prior knowledge about the reference during... | Hongxiang Li, Wanshi Xu, Xuxin Cheng, Yaowei Li, Yuexian Zou, Zhihong Zhu |  |
| 731 |  |  [Type-Aware Decomposed Framework for Few-Shot Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.598) |  | 0 | Despite the recent success achieved by several two-stage prototypical networks in few-shot named entity recognition (NER) task, the over-detected false spans at span detection stage and the inaccurate and unstable prototypes at type classification stage remain to be challenging problems. In this... | Tieyun Qian, Yongqi Li, Yu Yu |  |
| 732 |  |  [A Closer Look into Using Large Language Models for Automatic Evaluation](https://doi.org/10.18653/v1/2023.findings-emnlp.599) |  | 0 | Using large language models (LLMs) to evaluate text quality has recently gained popularity. Some existing prior works explore the idea of using LLMs for evaluation, while they differ in some details of the evaluation process. In this paper, we analyze \*LLM evaluation\* and \*G-Eval\*, and we... | David ChengHan Chiang, Hungyi Lee |  |
| 733 |  |  [Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification using Graph Neural Networks?](https://doi.org/10.18653/v1/2023.findings-emnlp.600) |  | 0 | Given the success of Graph Neural Networks (GNNs) for structure-aware machine learning, many studies have explored their use for text classification, but mostly in specific domains with limited data characteristics. Moreover, some strategies prior to GNNs relied on graph mining and classical... | Gerard de Melo, Margarita Bugueño |  |
| 734 |  |  [Natural Language Annotations for Reasoning about Program Semantics](https://doi.org/10.18653/v1/2023.findings-emnlp.601) |  | 0 | By grounding natural language inference in code (and vice versa), researchers aim to create programming assistants that explain their work, are “coachable” and can surface any gaps in their reasoning. Can we deduce automatically interesting properties of programs from their syntax and common-sense... | Marco Zocca |  |
| 735 |  |  [Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate to Speech Emotion Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.602) |  | 0 | Previous work has established that a person’s demographics and speech style affect how well speech processing models perform for them. But where does this bias come from? In this work, we present the Speech Embedding Association Test (SpEAT), a method for detecting bias in one type of model used... | Aylin Caliskan, Craig Greenberg, Isaac Slaughter, Reva Schwartz |  |
| 736 |  |  [Text Classification via Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.603) |  | 0 | Despite the remarkable success of large-scale Language Models (LLMs) such as GPT-3, their performances still significantly underperform fine-tuned models in the task of text classification.This is due to (1) the lack of reasoning ability in addressing complex linguistic phenomena (e.g.,... | Fei Wu, Guoyin Wang, Jiwei Li, Shangwei Guo, Tianwei Zhang, Xiaofei Sun, Xiaoya Li |  |
| 737 |  |  [On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.604) |  | 0 | Visually-rich document entity retrieval (VDER), which extracts key information (e.g. date, address) from document images like invoices and receipts, has become an important topic in industrial NLP applications. The emergence of new document types at a constant pace, each with its unique entity... | Aidong Zhang, Bo Dai, Hanjun Dai, Jiayi Chen, Wei Wei |  |
| 738 |  |  [Semi-Structured Object Sequence Encoders](https://doi.org/10.18653/v1/2023.findings-emnlp.605) |  | 0 | In this paper we explore the task of modeling semi-structured object sequences; in particular, we focus our attention on the problem of developing a structure-aware input representation for such sequences. Examples of such data include user activity on websites, machine logs, and many others. This... | Danish Contractor, Hui Wan, Marina Danilevsky, R. Chulaka Gunasekara, Riyaz A. Bhat, Rudra Murthy V, Siva Sankalp Patel, Tejas I. Dhamecha |  |
| 739 |  |  [DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM](https://doi.org/10.18653/v1/2023.findings-emnlp.606) |  | 0 | In the burgeoning field of natural language processing, Neural Topic Models (NTMs) and Large Language Models (LLMs) have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic... | Fanyou Wu, Srinivasan H. Sengamedu, Weijie Xu, Wenxiang Hu |  |
| 740 |  |  [Energy and Carbon Considerations of Fine-Tuning BERT](https://doi.org/10.18653/v1/2023.findings-emnlp.607) |  | 0 | Despite the popularity of the pre-train then fine-tune paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning,... | Clara Na, Emma Strubell, Sasha Luccioni, Sorelle A. Friedler, Xiaorong Wang |  |
| 741 |  |  [Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models](https://doi.org/10.18653/v1/2023.findings-emnlp.608) |  | 0 | The dominance of proprietary LLMs has led to restricted access and raised information privacy concerns. The SoTA open-source alternatives are crucial for information-sensitive and high-volume applications but often lag behind in performance. To address this gap, we propose (1) A generalized variant... | Abhinav Chinta, Heng Ji, Sumuk Shashidhar, Vaibhav Sahai, Zhenhailong Wang |  |
| 742 |  |  [Chinese Metaphorical Relation Extraction: Dataset and Models](https://doi.org/10.18653/v1/2023.findings-emnlp.609) |  | 0 | Metaphor identification is usually formulated as a sequence labeling or a syntactically related word-pair classification problem. In this paper, we propose a novel formulation of metaphor identification as a relation extraction problem. We introduce metaphorical relations, which are links between... | Guihua Chen, Jiefu Gong, MiaoMiao Cheng, Shijin Wang, Tiantian Wu, Wei Song, Xu Han |  |
| 743 |  |  [Example-based Hypernetworks for Multi-source Adaptation to Unseen Domains](https://doi.org/10.18653/v1/2023.findings-emnlp.610) |  | 0 | As Natural Language Processing (NLP) algorithms continually achieve new milestones, out-of-distribution generalization remains a significant challenge. This paper addresses the issue of multi-source adaptation for unfamiliar domains: We leverage labeled data from multiple source domains to... | Eyal BenDavid, Gal Chechik, Ohad Amosy, Roi Reichart, Tomer Volk |  |
| 744 |  |  [Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.611) |  | 0 | The age of social media is rife with memes. Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image. However, existing harmful meme detection approaches only recognize superficial... | Hongzhan Lin, Jing Ma, Long Chen, Ziyang Luo |  |
| 745 |  |  [Domain Adaptation for Conversational Query Production with the RAG Model Feedback](https://doi.org/10.18653/v1/2023.findings-emnlp.612) |  | 0 | Conversational query production is an emerging fundamental task for the dialogue system, where search queries are generated to explore the vast and continually updating knowledge from a search engine. To accelerate this line of research, previous studies have released several datasets with... | Ante Wang, Ge Xu, Jinsong Su, Linfeng Song |  |
| 746 |  |  [LEGO: A Multi-agent Collaborative Framework with Role-playing and Iterative Feedback for Causality Explanation Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.613) |  | 0 | Causality Explanation Generation refers to generate an explanation in natural language given an initial cause-effect pair. It demands rigorous explicit rationales to demonstrate the acquisition of implicit commonsense knowledge, which is unlikely to be easily memorized, making it challenging for... | Jun Zhao, Kang Liu, Mengshu Sun, Pengfei Cao, Ruopeng Li, Yubo Chen, Zhitao He |  |
| 747 |  |  [Ranking LLM-Generated Loop Invariants for Program Verification](https://doi.org/10.18653/v1/2023.findings-emnlp.614) |  | 0 | Synthesizing inductive loop invariants is fundamental to automating program verification. In this work we observe that Large Language Models (such as gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of programs in a 0-shot setting, yet require several samples to generate... | Aditya Senthilnathan, Akash Lal, Aseem Rastogi, Madanlal Musuvathi, Nikhil Swamy, Rahul Sharma, Saikat Chakraborty, Sarah Fakhoury, Shuvendu K. Lahiri |  |
| 748 |  |  [WordNet Is All You Need: A Surprisingly Effective Unsupervised Method for Graded Lexical Entailment](https://doi.org/10.18653/v1/2023.findings-emnlp.615) |  | 0 | We propose a simple unsupervised approach which exclusively relies on WordNet (Miller,1995) for predicting graded lexical entailment (GLE) in English. Inspired by the seminal work of Resnik (1995), our method models GLE as the sum of two information-theoretic scores: a symmetric semantic similarity... | Joseph Renner, Pascal Denis, Rémi Gilleron |  |
| 749 |  |  [Knowledge Corpus Error in Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.616) |  | 0 | Recent works in open-domain question answering (QA) have explored generating context passages from large language models (LLMs), replacing the traditional retrieval step in the QA pipeline. However, it is not well understood why generated passages can be more effective than retrieved ones. This... | James Thorne, Philhoon Oh, Yejoon Lee |  |
| 750 |  |  [Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.617) |  | 0 | Recent advances in machine translation (MT) have shown that Minimum Bayes Risk (MBR) decoding can be a powerful alternative to beam search decoding, especially when combined with neural-based utility functions. However, the performance of MBR decoding depends heavily on how and how many candidates... | Behrooz Ghorbani, Markus Freitag, Patrick Fernandes |  |
| 751 |  |  [The language of prompting: What linguistic properties make a prompt successful?](https://doi.org/10.18653/v1/2023.findings-emnlp.618) |  | 0 | The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt... | Alina Leidinger, Ekaterina Shutova, Robert van Rooij |  |
| 752 |  |  [When and Why Does Bias Mitigation Work?](https://doi.org/10.18653/v1/2023.findings-emnlp.619) |  | 0 | Neural models have been shown to exploit shallow surface features to perform language understanding tasks, rather than learning the deeper language understanding and reasoning skills that practitioners desire. Previous work has developed debiasing techniques to pressure models away from spurious... | Abhilasha Ravichander, Joe Stacey, Marek Rei |  |
| 753 |  |  [Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy](https://doi.org/10.18653/v1/2023.findings-emnlp.620) |  | 0 | Retrieval-augmented generation has raise extensive attention as it is promising to address the limitations of large language models including outdated knowledge and hallucinations. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work... | Minlie Huang, Nan Duan, Weizhu Chen, Yelong Shen, Yeyun Gong, Zhihong Shao |  |
| 754 |  |  [Dynamic Low-rank Estimation for Transformer-based Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.621) |  | 0 | Matrix decomposition methods, such as Singular Value Decomposition (SVD) and its importance-weighted variants, have been widely used for compressing Transformer-based language models. While importance-weighted decomposition methods alleviate the strong assumption of equal importance for each... | Hongxia Jin, Shangqian Gao, Ting Hua, Xiao Li, YenChang Hsu, Yilin Shen |  |
| 755 |  |  [Non-parallel Accent Transfer based on Fine-grained Controllable Accent Modelling](https://doi.org/10.18653/v1/2023.findings-emnlp.622) |  | 0 | Existing accent transfer works rely on parallel data or speech recognition models. This paper focuses on the practical application of accent transfer and aims to implement accent transfer using non-parallel datasets. The study has encountered the challenge of speech representation disentanglement... | Cunli Mao, Linqin Wang, Shengxiang Gao, Yuanzhang Yang, Yuxin Huang, Zhengtao Yu |  |
| 756 |  |  [Compositional Generalization for Data-to-Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.623) |  | 0 | Data-to-text generation involves transforming structured data, often represented as predicate-argument tuples, into coherent textual descriptions. Despite recent advances, systems still struggle when confronted with unseen combinations of predicates, producing unfaithful descriptions... | Ivan Titov, Mirella Lapata, Xinnuo Xu |  |
| 757 |  |  [In-Context Learning Creates Task Vectors](https://doi.org/10.18653/v1/2023.findings-emnlp.624) |  | 0 | In-context learning (ICL) in Large Language Models (LLMs) has emerged as a powerful new learning paradigm. However, its underlying mechanism is still not well understood. In particular, it is challenging to map it to the “standard’ machine learning framework, where one uses a training set S to find... | Amir Globerson, Mor Geva, Roee Hendel |  |
| 758 |  |  [TalkUp: Paving the Way for Understanding Empowering Language](https://doi.org/10.18653/v1/2023.findings-emnlp.625) |  | 0 | Empowering language is important in many real-world contexts, from education to workplace dynamics to healthcare. Though language technologies are growing more prevalent in these contexts, empowerment has seldom been studied in NLP, and moreover, it is inherently challenging to operationalize... | Chan Young Park, Lucille Njoo, Marvin Thielk, Octavia Stappart, Yi Chu, Yulia Tsvetkov |  |
| 759 |  |  [Unifying Text, Tables, and Images for Multimodal Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.626) |  | 0 | Multimodal question answering (MMQA), which aims to derive the answer from multiple knowledge modalities (e.g., text, tables, and images), has received increasing attention due to its board applications. Current approaches to MMQA often rely on single-modal or bi-modal QA models, which limits their... | Haohao Luo, Yang Deng, Ying Shen |  |
| 760 |  |  [Unsupervised Lexical Simplification with Context Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.627) |  | 0 | We propose a new unsupervised lexical simplification method that uses only monolingual data and pre-trained language models. Given a target word and its context, our method generates substitutes based on the target context and also additional contexts sampled from monolingual data. We conduct... | Jey Han Lau, Takashi Wada, Timothy Baldwin |  |
| 761 |  |  [mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences](https://doi.org/10.18653/v1/2023.findings-emnlp.628) |  | 0 | We present our work on developing a multilingual, efficient text-to-text transformer that is suitable for handling long inputs. This model, called mLongT5, builds upon the architecture of LongT5, while leveraging the multilingual datasets used for pretraining mT5 and the pretraining tasks of UL2.... | David C. Uthus, Joshua Ainslie, Mandy Guo, Santiago Ontañón |  |
| 762 |  |  [Multilingual Lottery Tickets to Pretrain Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.629) |  | 0 | The curse of multilinguality in training multilingual pretrained language models (mPLMs) refers to the negative interference between languages, especially when the capacity is limited. While increasing the capacity may appear intuitive for overcoming this curse, it negatively affects both training... | Jaeseong Lee, Seungwon Hwang |  |
| 763 |  |  [Target-Aware Spatio-Temporal Reasoning via Answering Questions in Dynamic Audio-Visual Scenarios](https://doi.org/10.18653/v1/2023.findings-emnlp.630) |  | 0 | Audio-visual question answering (AVQA) is a challenging task that requires multistep spatio-temporal reasoning over multimodal contexts. Recent works rely on elaborate target-agnostic parsing of audio-visual scenes for spatial grounding while mistreating audio and video as separate entities for... | Jianqin Yin, Yuanyuan Jiang |  |
| 764 |  |  [KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.631) |  | 0 | While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address... | Edward Choi, Jiho Kim, Yeonsu Kwon, Yohan Jo |  |
| 765 |  |  [Breaking the Language Barrier: Improving Cross-Lingual Reasoning with Structured Self-Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.632) |  | 0 | In this work, we study whether multilingual language models (MultiLMs) can transfer logical reasoning abilities to other languages when they are fine-tuned for reasoning in a different language. We evaluate the cross-lingual reasoning abilities of MultiLMs in two schemes: (1) where the language of... | Antoine Bosselut, Karl Aberer, Mohammadreza Banaei, Negar Foroutan |  |
| 766 |  |  [CITB: A Benchmark for Continual Instruction Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.633) |  | 0 | Continual learning (CL) is a paradigm that aims to replicate the human ability to learn and accumulate knowledge continually without forgetting previous knowledge and transferring it to new tasks. Recent instruction tuning (IT) involves fine-tuning models to make them more adaptable to solving NLP... | Ling Chen, Meng Fang, MohammadReza NamaziRad, Zihan Zhang |  |
| 767 |  |  [Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.634) |  | 0 | In this work, we propose a method that combines two popular research areas by injecting linguistic structures into pre-trained language models in the parameter-efficient fine-tuning (PEFT) setting. In our approach, parallel adapter modules encoding different linguistic structures are combined using... | Gabriel Murray, Giuseppe Carenini, Raymond Li |  |
| 768 |  |  [Towards Better Representations for Multi-Label Text Classification with Multi-granularity Information](https://doi.org/10.18653/v1/2023.findings-emnlp.635) |  | 0 | Multi-label text classification (MLTC) aims to assign multiple labels to a given text. Previous works have focused on text representation learning and label correlations modeling using pre-trained language models (PLMs). However, studies have shown that PLMs generate word frequency-oriented text... | Fangfang Li, Junwen Duan, Puzhen Su, Weidong Xiao |  |
| 769 |  |  [PCMID: Multi-Intent Detection through Supervised Prototypical Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.636) |  | 0 | Intent detection is a major task in Natural Language Understanding (NLU) and is the component of dialogue systems for interpreting users’ intentions based on their utterances. Many works have explored detecting intents by assuming that each utterance represents only a single intent. Such systems... | Amir Abdullah, Ian G. Harris, Junchen Zhao, Spencer Koehler, Yurun Song |  |
| 770 |  |  [Is GPT-4 a Good Data Analyst?](https://doi.org/10.18653/v1/2023.findings-emnlp.637) |  | 0 | As large language models (LLMs) have demonstrated their powerful capabilities in plenty of domains and tasks, including context understanding, code generation, language generation, data storytelling, etc., many data analysts may raise concerns if their jobs will be replaced by artificial... | Lidong Bing, Liying Cheng, Xingxuan Li |  |
| 771 |  |  [DiffusionRet: Diffusion-Enhanced Generative Retriever using Constrained Decoding](https://doi.org/10.18653/v1/2023.findings-emnlp.638) |  | 0 | Generative retrieval, which maps from a query to its relevant document identifiers (docids), has recently emerged as a new information retrieval (IR) paradigm, however, having suffered from 1) the lack of the intermediate reasoning step, caused by the manner of merely using a query to perform the... | SeungHoon Na, Shanbao Qiao, Xuebing Liu |  |
| 772 |  |  [Estimating Large Language Model Capabilities without Labeled Test Data](https://doi.org/10.18653/v1/2023.findings-emnlp.639) |  | 0 | Large Language Models (LLMs) have exhibited an impressive ability to perform in-context learning (ICL) from only a few examples, but the success of ICL varies widely from task to task. Thus, it is important to quickly determine whether ICL is applicable to a new task, but directly evaluating ICL... | Albert Xu, Harvey Yiyun Fu, Qinyuan Ye, Robin Jia, Xiang Ren |  |
| 773 |  |  [A Novel Contrastive Learning Method for Clickbait Detection on RoCliCo: A Romanian Clickbait Corpus of News Articles](https://doi.org/10.18653/v1/2023.findings-emnlp.640) |  | 0 | To increase revenue, news websites often resort to using deceptive news titles, luring users into clicking on the title and reading the full news. Clickbait detection is the task that aims to automatically detect this form of false advertisement and avoid wasting the precious time of online users.... | DariaMihaela Broscoteanu, Radu Tudor Ionescu |  |
| 774 |  |  [Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogues](https://doi.org/10.18653/v1/2023.findings-emnlp.641) |  | 0 | Open-domain dialogue system usually requires different sources of knowledge to generate more informative and evidential responses. However, existing knowledge-grounded dialogue systems either focus on a single knowledge source or overlook the dependency between multiple sources of knowledge, which... | Fei Mi, Hongru Wang, Irwin King, KamFai Wong, Minda Hu, Rui Wang, WaiChung Kwan, Weichao Wang, Yang Deng, Yasheng Wang |  |
| 775 |  |  [Toxicity in Multilingual Machine Translation at Scale](https://doi.org/10.18653/v1/2023.findings-emnlp.642) |  | 0 | Machine Translation systems can produce different types of errors, some of which are characterized as critical or catastrophic due to the specific negative impact that they can have on users. In this paper we focus on one type of critical error: added toxicity. We evaluate and analyze added... | Carlos Escolano, Christophe Ropers, Daniel Licht, Eric Michael Smith, Javier Ferrando, Jean Maillard, Marta R. Costajussà |  |
| 776 |  |  [Conversational Recommender System and Large Language Model Are Made for Each Other in E-commerce Pre-sales Dialogue](https://doi.org/10.18653/v1/2023.findings-emnlp.643) |  | 0 | E-commerce pre-sales dialogue aims to understand and elicit user needs and preferences for the items they are seeking so as to provide appropriate recommendations. Conversational recommender systems (CRSs) learn user representation and provide accurate recommendations based on dialogue context, but... | Fan Feng, Haopeng Bai, Hengbin Cui, Wanxiang Che, Weinan Zhang, Yifan Chen, Yongbin Li, Yuanxing Liu, Yuchi Zhang |  |
| 777 |  |  [VIP5: Towards Multimodal Foundation Models for Recommendation](https://doi.org/10.18653/v1/2023.findings-emnlp.644) |  | 0 | Computer Vision (CV), Natural Language Processing (NLP), and Recommender Systems (RecSys) are three prominent AI applications that have traditionally developed independently, resulting in disparate modeling and engineering methodologies. This has impeded the ability for these fields to directly... | Juntao Tan, Shijie Geng, Shuchang Liu, Yongfeng Zhang, Zuohui Fu |  |
| 778 |  |  [A Spectral Viewpoint on Continual Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.645) |  | 0 | Continual Relation Extraction (CRE) aims to continuously train a model to learn new relations while preserving its ability on previously learned relations. Similar to other continual learning problems, in CRE, models experience representation shift, where learned deep space changes in the continual... | Anh Tuan Luu, Chien Nguyen, Huy Nguyen, Linh Ngo Van, Thien Huu Nguyen |  |
| 779 |  |  [Learning to Follow Object-Centric Image Editing Instructions Faithfully](https://doi.org/10.18653/v1/2023.findings-emnlp.646) |  | 0 | Natural language instructions are a powerful interface for editing the outputs of text-to-image diffusion models. However, several challenges need to be addressed: 1) underspecification (the need to model the implicit meaning of instructions) 2) grounding (the need to localize where the edit has to... | Arkadiy Saakyan, Kanishk Singh, Smaranda Muresan, Tuhin Chakrabarty |  |
| 780 |  |  [Zero-shot Topical Text Classification with LLMs - an Experimental Study](https://doi.org/10.18653/v1/2023.findings-emnlp.647) |  | 0 | Topical Text Classification (TTC) is an ancient, yet timely research area in natural language processing, with many practical applications. The recent dramatic advancements in large LMs raise the question of how well these models can perform in this task in a zero-shot scenario. Here, we share a... | Alon Halfon, Artem Spector, Ilya Shnayderman, Lena Dankin, Liat EinDor, Noam Slonim, Ofir Arviv, Orith ToledoRonen, Shai Gretz, Yannis Katsis, Yoav Katz |  |
| 781 |  |  [Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems](https://doi.org/10.18653/v1/2023.findings-emnlp.648) |  | 0 | Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. We define generic personas to represent demographic groups, such as “an Asian person”, whereas specific personas may take the... | Aman Chadha, Jieyu Zhao, KaiWei Chang, Nanyun Peng, Yixin Wan |  |
| 782 |  |  [A Black-Box Attack on Code Models via Representation Nearest Neighbor Search](https://doi.org/10.18653/v1/2023.findings-emnlp.649) |  | 0 | Existing methods for generating adversarial code examples face several challenges: limted availability of substitute variables, high verification costs for these substitutes, and the creation of adversarial samples with noticeable perturbations. To address these concerns, our proposed approach,... | Jie Zhang, Qiang Hu, Shangqing Liu, Wei Ma, Xiaofei Xie, Yang Liu, Yves Le Traon |  |
| 783 |  |  [How Well Do Text Embedding Models Understand Syntax?](https://doi.org/10.18653/v1/2023.findings-emnlp.650) |  | 0 | Text embedding models have significantly contributed to advancements in natural language processing by adeptly capturing semantic properties of textual data. However, the ability of these models to generalize across a wide range of syntactic contexts remains under-explored. In this paper, we first... | Haizhou Li, Yan Zhang, Zhaopeng Feng, Zhiyang Teng, Zuozhu Liu |  |
| 784 |  |  [CASSI: Contextual and Semantic Structure-based Interpolation Augmentation for Low-Resource NER](https://doi.org/10.18653/v1/2023.findings-emnlp.651) |  | 0 | While text augmentation methods have been successful in improving performance in the low-resource setting, they suffer from annotation corruption for a token-level task like NER. Moreover, existing methods cannot reliably add context diversity to the dataset, which has been shown to be crucial for... | Eng Siong Chng, Kyaw Zin Tun, Tanmay Surana, ThiNga Ho |  |
| 785 |  |  [NEWTON: Are Large Language Models Capable of Physical Reasoning?](https://doi.org/10.18653/v1/2023.findings-emnlp.652) |  | 0 | Large Language Models (LLMs), through their contextualized representations, have been empirically proven to encapsulate syntactic, semantic, word sense, and common-sense knowledge. However, there has been limited exploration of their physical reasoning abilities, specifically concerning the crucial... | Dieter Fox, Jiafei Duan, Siddhartha S. Srinivasa, Yi Ru Wang |  |
| 786 |  |  [Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language](https://doi.org/10.18653/v1/2023.findings-emnlp.653) |  | 0 | Counterspeech, i.e., responses to counteract potential harms of hateful speech, has become an increasingly popular solution to address online hate speech without censorship. However, properly countering hateful language requires countering and dispelling the underlying inaccurate stereotypes... | Akhila Yerukola, Emily Allaway, Jimin Mun, Laura Vianna, Maarten Sap, SarahJane Leslie |  |
| 787 |  |  [On the Calibration of Large Language Models and Alignment](https://doi.org/10.18653/v1/2023.findings-emnlp.654) |  | 0 | As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and... | Benfeng Xu, Chiwei Zhu, Quan Wang, Yongdong Zhang, Zhendong Mao |  |
| 788 |  |  [TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction](https://doi.org/10.18653/v1/2023.findings-emnlp.655) |  | 0 | Since ChatGPT released its API for public use, the number of applications built on top of commercial large language models (LLMs) increase exponentially. One popular usage of such models is leveraging its in-context learning ability and generating responses given user queries leveraging knowledge... | Bowen Wang, Junyi Liu, Liangzhi Li, Tong Xiang, Yiming Qian |  |
| 789 |  |  [Identifying Conspiracy Theories News based on Event Relation Graph](https://doi.org/10.18653/v1/2023.findings-emnlp.656) |  | 0 | Conspiracy theories, as a type of misinformation, are narratives that explains an event or situation in an irrational or malicious manner. While most previous work examined conspiracy theory in social media short texts, limited attention was put on such misinformation in long news documents. In... | Ruihong Huang, Yuanyuan Lei |  |
| 790 |  |  [Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems](https://doi.org/10.18653/v1/2023.findings-emnlp.657) |  | 0 | Making big purchases requires consumers to research or consult a salesperson to gain domain expertise. However, existing conversational recommender systems (CRS) often overlook users’ lack of background knowledge, focusing solely on gathering preferences. In this work, we define a new problem space... | Caiming Xiong, ChienSheng Wu, Lidiya Murakhovs'ka, Philippe Laban, Tian Xie |  |
| 791 |  |  [Dynamic Open-book Prompt for Conversational Recommender System](https://doi.org/10.18653/v1/2023.findings-emnlp.658) |  | 0 | Conversational Recommender System (CRS) aims to deliver personalized recommendations through interactive dialogues. Recent advances in prompt learning have shed light on this task. However, the performance of existing methods is confined by the limited context within ongoing conversations.... | Ke Sun, Tieyun Qian, Xuan Ma |  |
| 792 |  |  [Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.659) |  | 0 | Large language models (LLMs) can perform a wide range of tasks by following natural language instructions, without the necessity of task-specific fine-tuning. Unfortunately, the performance of LLMs is greatly influenced by the quality of these instructions, and manually writing effective... | Chenguang Zhu, Dan Iter, Meng Jiang, Qingkai Zeng, Shuohang Wang, Wenhao Yu, Yang Liu, Yichong Xu, Zhihan Zhang |  |
| 793 |  |  [DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models](https://doi.org/10.18653/v1/2023.findings-emnlp.660) |  | 0 | Diffusion models have gained prominence in generating high-quality sequences of text. Nevertheless, current approaches predominantly represent discrete text within a continuous diffusion space, which incurs substantial computational overhead during training and results in slower sampling speeds. In... | Jiangtao Feng, Lingpeng Kong, Mukai Li, Shansan Gong, Zhiyong Wu |  |
| 794 |  |  [M2C: Towards Automatic Multimodal Manga Complement](https://doi.org/10.18653/v1/2023.findings-emnlp.661) |  | 0 | Multimodal manga analysis focuses on enhancing manga understanding with visual and textual features, which has attracted considerable attention from both natural language processing and computer vision communities. Currently, most comics are hand-drawn and prone to problems such as missing pages,... | Boyang Wang, Hongcheng Guo, Jiaheng Liu, Jian Yang, Jiaqi Bai, Zhoujun Li |  |
| 795 |  |  [Learn Your Tokens: Word-Pooled Tokenization for Language Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.662) |  | 0 | Language models typically tokenize text into subwords, using a deterministic, hand-engineered heuristic of combining characters into longer surface-level strings such as ‘ing’ or whole words. Recent literature has repeatedly shown the limitations of such a tokenization strategy, particularly for... | Avijit Thawani, Jay Pujara, Saurabh Ghanekar, Xiaoyuan Zhu |  |
| 796 |  |  [Towards Detecting Contextual Real-Time Toxicity for In-Game Chat](https://doi.org/10.18653/v1/2023.findings-emnlp.663) |  | 0 | Real-time toxicity detection in online environments poses a significant challenge, due to the increasing prevalence of social media and gaming platforms. We introduce ToxBuster, a simple and scalable model that reliably detects toxic content in real-time for a line of chat by including chat history... | Nicolas GrenonGodbout, Reihaneh Rabbany, Zachary Yang |  |
| 797 |  |  [JWSign: A Highly Multilingual Corpus of Bible Translations for more Diversity in Sign Language Processing](https://doi.org/10.18653/v1/2023.findings-emnlp.664) |  | 0 | Advancements in sign language processing have been hindered by a lack of sufficient data, impeding progress in recognition, translation, and production tasks. The absence of comprehensive sign language datasets across the world’s sign languages has widened the gap in this field, resulting in a few... | Colin Leong, Mathias Müller, Shester Gueuwou, Sophie Siake |  |
| 798 |  |  [Do Stochastic Parrots have Feelings Too? Improving Neural Detection of Synthetic Text via Emotion Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.665) |  | 0 | Recent developments in generative AI have shone a spotlight on high-performance synthetic text generation technologies. The now wide availability and ease of use of such models highlights the urgent need to provide equally powerful technologies capable of identifying synthetic text. With this in... | Alan Cowap, Jennifer Foster, Yvette Graham |  |
| 799 |  |  [Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules](https://doi.org/10.18653/v1/2023.findings-emnlp.666) |  | 0 | Large language models (LLMs) have achieved remarkable results on NLP tasks but at the expense of huge parameter sizes and the consequent computational costs. In this paper, we propose Variator, a parameter-efficient acceleration method that enhances computational efficiency through plug-and-play... | Chaojun Xiao, Jie Zhou, Maosong Sun, Pengle Zhang, Ruobing Xie, Wenbin Zhang, Xu Han, Yankai Lin, Yuqi Luo, Zhengyan Zhang, Zhiyuan Liu |  |
| 800 |  |  [PivotFEC: Enhancing Few-shot Factual Error Correction with a Pivot Task Approach using Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.667) |  | 0 | Factual Error Correction (FEC) aims to rectify false claims by making minimal revisions to align them more accurately with supporting evidence. However, the lack of datasets containing false claims and their corresponding corrections has impeded progress in this field. Existing distantly supervised... | ALong Jin, Jun Ma, Siu Ming Yiu, Xingwei He, Yuan Yuan |  |
| 801 |  |  [Semantic Similarity Covariance Matrix Shrinkage](https://doi.org/10.18653/v1/2023.findings-emnlp.668) |  | 0 | An accurate estimation of the covariance matrix is a critical component of many applications in finance, including portfolio optimization. The sample covariance suffers from the curse of dimensionality when the number of observations is in the same order or lower than the number of variables. This... | Guillaume Becquin, Saher Esmeir |  |
| 802 |  |  [LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.669) |  | 0 | Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen... | Aiping Xiong, LunWei Ku, ShihChieh Dai |  |
| 803 |  |  [LLM aided semi-supervision for efficient Extractive Dialog Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.670) |  | 0 | Generating high-quality summaries for chat dialogs often requires large labeled datasets. We propose a method to efficiently use unlabeled data for extractive summarization of customer-agent dialogs. In our method, we frame summarization as a question-answering problem and use state-of-the-art... | Ameen AbuHanna, Gaurav Sahu, Iacer Calixto, Issam H. Laradji, Nishant Mishra |  |
| 804 |  |  [Investigating Multilingual Coreference Resolution by Universal Annotations](https://doi.org/10.18653/v1/2023.findings-emnlp.671) |  | 0 | Multilingual coreference resolution (MCR) has been a long-standing and challenging task. With the newly proposed multilingual coreference dataset, CorefUD (Nedoluzhko et al., 2022), we conduct an investigation into the task by using its harmonized universal morphosyntactic and coreference... | Haixia Chai, Michael Strube |  |
| 805 |  |  [FactSpotter: Evaluating the Factual Faithfulness of Graph-to-Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.672) |  | 0 | Graph-to-text (G2T) generation takes a graph as input and aims to generate a fluent and faith- ful textual representation of the information in the graph. The task has many applications, such as dialogue generation and question an- swering. In this work, we investigate to what extent the G2T... | Ioana Manolescu, Kun Zhang, Oana Balalau |  |
| 806 |  |  [LayoutDIT: Layout-Aware End-to-End Document Image Translation with Multi-Step Conductive Decoder](https://doi.org/10.18653/v1/2023.findings-emnlp.673) |  | 0 | Document image translation (DIT) aims to translate text embedded in images from one language to another. It is a challenging task that needs to understand visual layout with text semantics simultaneously. However, existing methods struggle to capture the crucial visual layout in real-world complex... | Chengqing Zong, Lu Xiang, Yang Zhao, Yaping Zhang, Yu Zhou, Yupu Liang, Zhiyang Zhang |  |
| 807 |  |  [Balaur: Language Model Pretraining with Lexical Semantic Relations](https://doi.org/10.18653/v1/2023.findings-emnlp.674) |  | 0 | Lexical semantic relations (LSRs) characterize meaning relationships between words and play an important role in systematic generalization on lexical inference tasks. Notably, several tasks that require knowledge of hypernymy still pose a challenge for pretrained language models (LMs) such as BERT,... | Andrei Mircea, Jackie C. K. Cheung |  |
| 808 |  |  [Exploring In-Context Learning for Knowledge Grounded Dialog Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.675) |  | 0 | Large neural-based dialog generation models have been applied in many real-life scenarios, yet they are prone to hallucination and tend to produce factually inaccurate outputs which raise great concerns. To alleviate this problem, we propose a plug-and-play retrieval-based framework IKA, which... | Qinyu Chen, Sujian Li, Wenhao Wu |  |
| 809 |  |  [Towards Enhancing Relational Rules for Knowledge Graph Link Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.676) |  | 0 | Graph neural networks (GNNs) have shown promising performance for knowledge graph reasoning. A recent variant of GNN called progressive relational graph neural network (PRGNN), utilizes relational rules to infer missing knowledge in relational digraphs and achieves notable results. However, during... | Huaiyu Wan, Junfeng Shen, Shuhan Wu, Wei Chen, Youfang Lin, Yuting Wu |  |
| 810 |  |  [Are NLP Models Good at Tracing Thoughts: An Overview of Narrative Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.677) |  | 0 | Narrative understanding involves capturing the author’s cognitive processes, providing insights into their knowledge, intentions, beliefs, and desires. Although large language models (LLMs) excel in generating grammatically coherent text, their ability to comprehend the author’s thoughts remains... | Lin Gui, Lixing Zhu, Runcong Zhao, Yulan He |  |
| 811 |  |  [Who is Speaking? Speaker-Aware Multiparty Dialogue Act Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.678) |  | 0 | Utterances do not occur in isolation in dialogues; it is essential to have the information of who the speaker of an utterance is to be able to recover the speaker’s intention with respect to the surrounding context. Beyond simply capturing speaker switches, identifying how speakers interact with... | Adarsh Pyarelal, Ayesha Qamar, Ruihong Huang |  |
| 812 |  |  [Demystifying Prompts in Language Models via Perplexity Estimation](https://doi.org/10.18653/v1/2023.findings-emnlp.679) |  | 0 | Language models can be prompted to perform a wide variety of tasks with zero- and few-shot in-context learning. However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens. In this paper, we analyze the factors that contribute to this variance... | Hila Gonen, Luke Zettlemoyer, Noah A. Smith, Srini Iyer, Terra Blevins |  |
| 813 |  |  [C2D2 Dataset: A Resource for the Cognitive Distortion Analysis and Its Impact on Mental Health](https://doi.org/10.18653/v1/2023.findings-emnlp.680) |  | 0 | Cognitive distortions refer to patterns of irrational thinking that can lead to distorted perceptions of reality and mental health problems in individuals. Despite previous attempts to detect cognitive distortion through language, progress has been slow due to the lack of appropriate data. In this... | Bichen Wang, Bing Qin, Pengfei Deng, Yanyan Zhao |  |
| 814 |  |  [MixEdit: Revisiting Data Augmentation and Beyond for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.findings-emnlp.681) |  | 0 | Data Augmentation through generating pseudo data has been proven effective in mitigating the challenge of data scarcity in the field of Grammatical Error Correction (GEC). Various augmentation strategies have been widely explored, most of which are motivated by two heuristics, i.e., increasing the... | HaiTao Zheng, Jingheng Ye, Yangning Li, Yinghui Li |  |
| 815 |  |  [CCEval: A Representative Evaluation Benchmark for the Chinese-centric Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.682) |  | 0 | The Chinese-centric Multilingual Machine Translation (MMT) has gained more importance recently due to increasing demands from international business development and cross-cultural exchanges. However, an important factor that limits the progress of this area is the lack of highly representative and... | Lianzhang Lou, Xi Yin, Yang Xiang, Yutao Xie |  |
| 816 |  |  [ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual Common Sense](https://doi.org/10.18653/v1/2023.findings-emnlp.683) |  | 0 | Humans possess a strong capability for reasoning beyond common sense. For example, given an unconventional image of a goldfish laying on the table next to an empty fishbowl, a human would effortlessly determine that the fish is not inside the fishbowl. The case, however, may be different for a... | Eason Lai, Jing Jiang, Kankan Zhou, Kyriakos Mouratidis, Wei Bin Au Yeong |  |
| 817 |  |  [Automatic Analysis of Substantiation in Scientific Peer Reviews](https://doi.org/10.18653/v1/2023.findings-emnlp.684) |  | 0 | With the increasing amount of problematic peer reviews in top AI conferences, the community is urgently in need of automatic quality control measures. In this paper, we restrict our attention to substantiation — one popular quality aspect indicating whether the claims in a review are sufficiently... | Chloé Clavel, Guokan Shang, Michalis Vazirgiannis, Virgile Rennard, Yanzhu Guo |  |
| 818 |  |  [Hierarchical Prompting Assists Large Language Model on Web Navigation](https://doi.org/10.18653/v1/2023.findings-emnlp.685) |  | 0 | Large language models (LLMs) struggle on processing complicated observations in interactive decision making. To alleviate this issue, we propose a simple hierarchical prompting approach. Diverging from previous prompting approaches that always put the full observation (a web page) to the prompt, we... | Abishek Sridhar, Frank F. Xu, Hao Zhu, Robert Lo, Shuyan Zhou |  |
| 819 |  |  [Can Large Language Models Fix Data Annotation Errors? An Empirical Study Using Debatepedia for Query-Focused Text Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.686) |  | 0 | Debatepedia is a publicly available dataset consisting of arguments and counter-arguments on controversial topics that has been widely used for the single-document query-focused abstractive summarization task in recent years. However, it has been recently found that this dataset is limited by noise... | Enamul Hoque, Israt Jahan, Jimmy Xiangji Huang, Md. Tahmid Rahman Laskar, Mizanur Rahman |  |
| 820 |  |  [TSTR: Target Similarity Tuning Meets the Real World](https://doi.org/10.18653/v1/2023.findings-emnlp.687) |  | 0 | Target similarity tuning (TST) is a method of selecting relevant examples in natural language (NL) to code generation through large language models (LLMs) to improve performance. Its goal is to adapt a sentence embedding model to have the similarity between two NL inputs match the similarity... | Ananya Singha, Anirudh Khatry, Gust Verbruggen, Mukul Singh, Priyanshu Gupta, Sumit Gulwani, Vu Le |  |
| 821 |  |  [RealBehavior: A Framework for Faithfully Characterizing Foundation Models' Human-like Behavior Mechanisms](https://doi.org/10.18653/v1/2023.findings-emnlp.688) |  | 0 | Reports of human-like behaviors in foundation models are growing, with psychological theories providing enduring tools to investigate these behaviors. However, current research tends to directly apply these human-oriented tools without verifying the faithfulness of their outcomes. In this paper, we... | Enyu Zhou, Jingting Ye, Qi Zhang, Rui Zheng, Songyang Gao, Tao Gui, Xiaoran Fan, Xuanjing Huang, Zhiheng Xi, Zichu Fei |  |
| 822 |  |  [Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance](https://doi.org/10.18653/v1/2023.findings-emnlp.689) |  | 0 | Large Language Models (LLMs) are increasingly utilized in educational tasks such as providing writing suggestions to students. Despite their potential, LLMs are known to harbor inherent biases which may negatively impact learners. Previous studies have investigated bias in models and data... | Roman Rietsche, Seyed Parsa Neshaei, Tanja Käser, Thiemo Wambsganss, Vinitra Swamy, Xiaotian Su |  |
| 823 |  |  [VERVE: Template-based ReflectiVE Rewriting for MotiVational IntErviewing](https://doi.org/10.18653/v1/2023.findings-emnlp.690) |  | 0 | Reflective listening is a fundamental skill that counselors must acquire to achieve proficiency in motivational interviewing (MI). It involves responding in a manner that acknowledges and explores the meaning of what the client has expressed in the conversation. In this work, we introduce the task... | Do June Min, Ken Resnicow, Rada Mihalcea, Verónica PérezRosas |  |
| 824 |  |  [Self-Knowledge Guided Retrieval Augmentation for Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.691) |  | 0 | Large language models (LLMs) have shown superior performance without task-specific fine-tuning. Despite the success, the knowledge stored in the parameters of LLMs could still be incomplete and difficult to update due to the computational costs. As complementary, retrieval-based methods can offer... | Maosong Sun, Peng Li, Yang Liu, Yile Wang |  |
| 825 |  |  [Pretraining Language Models with Text-Attributed Heterogeneous Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.692) |  | 0 | In many real-world scenarios (e.g., academic networks, social platforms), different types of entities are not only associated with texts but also connected by various relationships, which can be abstracted as Text-Attributed Heterogeneous Graphs (TAHGs). Current pretraining tasks for Language... | Bowen Du, Le Yu, Leilei Sun, Tao Zou, Yifei Huang |  |
| 826 |  |  [CReTIHC: Designing Causal Reasoning Tasks about Temporal Interventions and Hallucinated Confoundings](https://doi.org/10.18653/v1/2023.findings-emnlp.693) |  | 0 | Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their ability to establish causal relationships, particularly in the context of temporal interventions and language hallucinations, remains challenging. This paper presents CReTIHC, a... | Changwoo Chun, Heuiseok Lim, Jaehyung Seo, Songeun Lee |  |
| 827 |  |  [On the Dimensionality of Sentence Embeddings](https://doi.org/10.18653/v1/2023.findings-emnlp.694) |  | 0 | Learning sentence embeddings is a fundamental problem in natural language processing. While existing research primarily focuses on enhancing the quality of sentence embeddings, the exploration of sentence embedding dimensions is limited. Here we present a comprehensive and empirical analysis of the... | Dong Yu, Hongming Zhang, Hongwei Wang |  |
| 828 |  |  [Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.695) |  | 0 | Scaling pre-trained language models has resulted in large performance gains in various natural language processing tasks but comes with a large cost in memory requirements. Inspired by the position embeddings in transformers, we aim to simplify and reduce the memory footprint of the multi-head... | Huiyin Xue, Nikolaos Aletras |  |
| 829 |  |  [Entity-Based Evaluation of Political Bias in Automatic Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.696) |  | 0 | Growing literature has shown that NLP systems may encode social biases; however, the \*political\* bias of summarization models remains relatively unknown. In this work, we use an entity replacement method to investigate the portrayal of politicians in automatically generated summaries of news... | Chenhao Tan, Karen Zhou |  |
| 830 |  |  [StyleBART: Decorate Pretrained Model with Style Adapters for Unsupervised Stylistic Headline Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.697) |  | 0 | Stylistic headline generation is the task to generate a headline that not only summarizes the content of an article, but also reflects a desired style that attracts users. As style-specific article-headline pairs are scarce, previous researches focus on unsupervised approaches with a standard... | Boya Xiong, Guanhua Chen, Hanqing Wang, Yajing Luo, Yun Chen |  |
| 831 |  |  [RSVP: Customer Intent Detection via Agent Response Contrastive and Generative Pre-Training](https://doi.org/10.18653/v1/2023.findings-emnlp.698) |  | 0 | The dialogue systems in customer services have been developed with neural models to provide users with precise answers and round-the-clock support in task-oriented conversations by detecting customer intents based on their utterances. Existing intent detection approaches have highly relied on... | AnZi Yen, WeiYao Wang, WenChih Peng, YuChien Tang |  |
| 832 |  |  [Improving Low-resource Question Answering by Augmenting Question Information](https://doi.org/10.18653/v1/2023.findings-emnlp.699) |  | 0 | In the era of large models, low-resource question-answering tasks lag, emphasizing the importance of data augmentation - a key research avenue in natural language processing. The main challenges include leveraging the large model’s internal knowledge for data augmentation, determining which QA data... | Andong Chen, Kehai Chen, Min Zhang, Rosella P. Galindo Esparza, Tiejun Zhao, Xiaobing Zhao, Yang Xiang, Yuan Sun |  |
| 833 |  |  [InstructSafety: A Unified Framework for Building Multidimensional and Explainable Safety Detector through Instruction Tuning](https://doi.org/10.18653/v1/2023.findings-emnlp.700) |  | 0 | Safety detection has been an increasingly important topic in recent years and it has become even more necessary to develop reliable safety detection systems with the rapid development of large language models. However, currently available safety detection systems have limitations in terms of their... | Hao Sun, Jiale Cheng, Jiawen Deng, Minlie Huang, Zhexin Zhang |  |
| 834 |  |  ["A Tale of Two Movements': Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.701) |  | 0 | Social media has become a major driver of social change, by facilitating the formation of online social movements. Automatically understanding the perspectives driving the movement and the voices opposing it, is a challenging task as annotated data is difficult to obtain. We propose a weakly... | Dan Goldwasser, Shamik Roy |  |
| 835 |  |  [ClusterPrompt: Cluster Semantic Enhanced Prompt Learning for New Intent Discovery](https://doi.org/10.18653/v1/2023.findings-emnlp.702) |  | 0 | The discovery of new intent categories from user utterances is a crucial task in expanding agent skills. The key lies in how to efficiently solicit semantic evidence from utterances and properly transfer knowledge from existing intents to new intents. However, previous methods laid too much... | Jinggui Liang, Lizi Liao |  |
| 836 |  |  [Investigating the Effect of Pre-finetuning BERT Models on NLI Involving Presuppositions](https://doi.org/10.18653/v1/2023.findings-emnlp.703) |  | 0 | We explore the connection between presupposition, discourse and sarcasm and propose to leverage that connection in a transfer learning scenario with the goal of improving the performance of NLI models on cases involving presupposition. We exploit advances in training transformer-based models that... | Jackie Chi Kit Cheung, Jad Kabbara |  |
| 837 |  |  [MRRL: Modifying the Reference via Reinforcement Learning for Non-Autoregressive Joint Multiple Intent Detection and Slot Filling](https://doi.org/10.18653/v1/2023.findings-emnlp.704) |  | 0 | With the rise of non-autoregressive approach, some non-autoregressive models for joint multiple intent detection and slot filling have obtained the promising inference speed. However, most existing SLU models (1) suffer from the multi-modality problem that leads to reference intents and slots may... | Bowen Cao, Qichen Ye, Xuxin Cheng, Yuexian Zou, Zhihong Zhu |  |
| 838 |  |  [DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy Slot Filling Task](https://doi.org/10.18653/v1/2023.findings-emnlp.705) |  | 0 | Recently, prompt-based generative frameworks have shown impressive capabilities in sequence labeling tasks. However, in practical dialogue scenarios, relying solely on simplistic templates and traditional corpora presents a challenge for these methods in generalizing to unknown input perturbations.... | Daichi Guo, Gang Zhao, Guanting Dong, Jinxu Zhao, Keqing He, Tingfeng Hui, Weiran Xu, Zhuoma Gongque |  |
| 839 |  |  [SHARCS: Efficient Transformers Through Routing with Dynamic Width Sub-networks](https://doi.org/10.18653/v1/2023.findings-emnlp.706) |  | 0 | We introduce SHARCS for adaptive inference that takes into account the hardness of input samples. SHARCS can train a router on any transformer network, enabling the model to direct different samples to sub-networks with varying widths. Our experiments demonstrate that: (1) SHARCS outperforms or... | Aditya Kusupati, Ali Farhadi, Hannaneh Hajishirzi, Mohammadreza Salehi, Sachin Mehta |  |
| 840 |  |  [Always the Best Fit: Adaptive Domain Gap Filling from Causal Perspective for Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.707) |  | 0 | Cross-domain Relation Extraction aims to transfer knowledge from a source domain to a different target domain to address low-resource challenges. However, the semantic gap caused by data bias between domains is a major challenge, especially in few-shot scenarios. Previous work has mainly focused on... | Chenji Lu, Ge Bai, Jiaxiang Geng, Ruifang Liu, Shilong Li, Xiyan Liu, Yidong Shi, Ying Liu, Zhang Zhang |  |
| 841 |  |  [MEGClass: Extremely Weakly Supervised Text Classification via Mutually-Enhancing Text Granularities](https://doi.org/10.18653/v1/2023.findings-emnlp.708) |  | 0 | Text classification is essential for organizing unstructured text. Traditional methods rely on human annotations or, more recently, a set of class seed words for supervision, which can be costly, particularly for specialized or emerging domains. To address this, using class surface names alone as... | Jiawei Han, Priyanka Kargupta, Susik Yoon, Tanay Komarlu, Xuan Wang |  |
| 842 |  |  [Causal Inference from Text: Unveiling Interactions between Variables](https://doi.org/10.18653/v1/2023.findings-emnlp.709) |  | 0 | Adjusting for latent covariates is crucial for estimating causal effects from observational textual data. Most existing methods only account for confounding covariates that affect both treatment and outcome, potentially leading to biased causal effects. This bias arises from insufficient... | Yulan He, Yuxiang Zhou |  |
| 843 |  |  [Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!](https://doi.org/10.18653/v1/2023.findings-emnlp.710) |  | 0 | Large Language Models (LLMs) have made remarkable strides in various tasks. Whether LLMs are competitive few-shot solvers for information extraction (IE) tasks, however, remains an open problem. In this work, we aim to provide a thorough answer to this question. Through extensive experiments on... | Aixin Sun, Yixin Cao, Yong Hong, Yubo Ma |  |
| 844 |  |  [Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration](https://doi.org/10.18653/v1/2023.findings-emnlp.711) |  | 0 | Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, they still possess limitations, such as failing to ask clarifying questions to ambiguous queries or refuse users’ unreasonable... | Hongru Wang, Liang Chen, Lizi Liao, TatSeng Chua, Wenqiang Lei, Yang Deng |  |
| 845 |  |  [Ecologically Valid Explanations for Label Variation in NLI](https://doi.org/10.18653/v1/2023.findings-emnlp.712) |  | 0 | Human label variation, or annotation disagreement, exists in many natural language processing (NLP) tasks, including natural language inference (NLI). To gain direct evidence of how NLI label variation arises, we build LiveNLI, an English dataset of 1,415 ecologically valid explanations (annotators... | Chenhao Tan, MarieCatherine de Marneffe, NanJiang Jiang |  |
| 846 |  |  [A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.713) |  | 0 | Semi-inductive link prediction (LP) in knowledge graphs (KG) is the task of predicting facts for new, previously unseen entities based on context information. Although new entities can be integrated by retraining the model from scratch in principle, such an approach is infeasible for large-scale... | Adrian Kochsiek, Rainer Gemulla |  |
| 847 |  |  [SummIt: Iterative Text Summarization via ChatGPT](https://doi.org/10.18653/v1/2023.findings-emnlp.714) |  | 0 | Existing text summarization systems have made significant progress in recent years, but typically generate summaries in a single step. The one-shot summarization setting is sometimes inadequate, however, as the generated summary may contain hallucinations or overlook important details related to... | Haopeng Zhang, Jiawei Zhang, Xiao Liu |  |
| 848 |  |  [Orthogonal Subspace Learning for Language Model Continual Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.715) |  | 0 | Benefiting from massive corpora and advanced hardware, large language models (LLMs) exhibit remarkable capabilities in language understanding and generation. However, their performance degrades in scenarios where multiple tasks are encountered sequentially, also known as catastrophic forgetting. In... | Han Xia, Qi Zhang, Qiming Ge, Rong Bao, Rui Zheng, Tao Gui, Tianze Chen, Xiao Wang, Xuanjing Huang |  |
| 849 |  |  [Attention-Enhancing Backdoor Attacks Against BERT-based Models](https://doi.org/10.18653/v1/2023.findings-emnlp.716) |  | 0 | Recent studies have revealed that Backdoor Attacks can threaten the safety of natural language processing (NLP) models. Investigating the strategies of backdoor attacks will help to understand the model’s vulnerability. Most existing textual backdoor attacks focus on generating stealthy triggers or... | Chao Chen, Haibin Ling, Lu Pang, Songzhu Zheng, Weimin Lyu |  |
| 850 |  |  [Hi-ToM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.717) |  | 0 | Theory of Mind (ToM) is the ability to reason about one’s own and others’ mental states. ToM plays a critical role in the development of intelligence, language understanding, and cognitive processes. While previous work has primarily focused on first and second-order ToM, we explore higher-order... | Naihao Deng, Rada Mihalcea, Yilin Jia, Yinghui He, Yufan Wu, Yulong Chen |  |
| 851 |  |  [Image and Text: Fighting the same Battle? Super Resolution Learning for Imbalanced Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.718) |  | 0 | In this paper, we propose SRL4NLP, a new approach for data augmentation by drawing an analogy between image and text processing: Super-resolution learning. This method is based on using high-resolution images to overcome the problem of low resolution images. While this technique is a common usage... | Farah Benamara, Patricia Stolf, Romain Meunier, Véronique Moriceau |  |
| 852 |  |  [SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank](https://doi.org/10.18653/v1/2023.findings-emnlp.719) |  | 0 | Deep neural classifiers trained with cross-entropy loss (CE loss) often suffer from poor calibration, necessitating the task of out-of-distribution (OOD) detection. Traditional supervised OOD detection methods require expensive manual annotation of in-distribution and OOD samples. To address the... | Adithya Samavedhi, Chengyu Dong, Dheeraj Mekala, Jingbo Shang |  |
| 853 |  |  [Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.720) |  | 0 | Knowing how to end and resume conversations over time is a natural part of communication, allowing for discussions to span weeks, months, or years. The duration of gaps between conversations dictates which topics are relevant and which questions to ask, and dialogue systems which do not explicitly... | Jason Naradowsky, Qiang Zhang, Yusuke Miyao |  |
| 854 |  |  [A Structure-Aware Generative Adversarial Network for Bilingual Lexicon Induction](https://doi.org/10.18653/v1/2023.findings-emnlp.721) |  | 0 | Bilingual lexicon induction (BLI) is the task of inducing word translations with a learned mapping function that aligns monolingual word embedding spaces in two different languages. However, most previous methods treat word embeddings as isolated entities and fail to jointly consider both the... | Bocheng Han, Lusi Li, Qian Tao, Zhihao Xiong |  |
| 855 |  |  [NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark](https://doi.org/10.18653/v1/2023.findings-emnlp.722) |  | 0 | In this position paper we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the... | Eneko Agirre, Iker GarcíaFerrero, Jon Ander Campos, Julen Etxaniz, Oier Lopez de Lacalle, Oscar Sainz |  |
| 856 |  |  [Improving Pacing in Long-Form Story Planning](https://doi.org/10.18653/v1/2023.findings-emnlp.723) |  | 0 | Existing LLM-based systems for writing long-form stories or story outlines frequently suffer from unnatural pacing, whether glossing over important events or over-elaborating on insignificant details, resulting in a jarring experience for the reader. We propose a \*\*CONC\*\*rete \*\*O\*\*utline... | Dan Klein, Kevin Yang, Xiaoming Liu, Yichen Wang |  |
| 857 |  |  [Argument mining as a multi-hop generative machine reading comprehension task](https://doi.org/10.18653/v1/2023.findings-emnlp.724) |  | 0 | Argument mining (AM) is a natural language processing task that aims to generate an argumentative graph given an unstructured argumentative text. An argumentative graph that consists of argumentative components and argumentative relations contains completed information of an argument and exhibits... | Boyang Liu, Riza BatistaNavarro, Sophia Ananiadou, Viktor Schlegel |  |
| 858 |  |  [HuatuoGPT, Towards Taming Language Model to Be a Doctor](https://doi.org/10.18653/v1/2023.findings-emnlp.725) |  | 0 | In this paper, we present HuatuoGPT, a Large Language Model (LLM) for medical consultation. The core recipe of HuatuoGPT is to leverage both distilled data from \*\*ChatGPT\*\* and real-world data from \*\*doctors\*\* in the supervised fine-tuning stage. This is not only because purely using... | Benyou Wang, Fei Yu, Feng Jiang, Guiming Chen, Haizhou Li, Hongbo Zhang, Jianquan Li, Junying Chen, Qingying Xiao, Xiang Wan, Xiangbo Wu, Zhihong Chen, Zhiyi Zhang |  |
| 859 |  |  [Debias NLU Datasets via Training-free Perturbations](https://doi.org/10.18653/v1/2023.findings-emnlp.726) |  | 0 | Several recent studies have shown that advanced models for natural language understanding (NLU) are prone to capture biased features that are independent of the task but spuriously correlated to labels. Such models often perform well on in-distribution (ID) datasets but fail to generalize to... | Qi Guo, Xinyu Dai, Yawen Ouyang, Yuanhang Tang, Zhen Wu |  |
| 860 |  |  [Aspect-to-Scope Oriented Multi-view Contrastive Learning for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.727) |  | 0 | Aspect-based sentiment analysis (ABSA) aims to align aspects and corresponding sentiment expressions, so as to identify the sentiment polarities of specific aspects. Most existing ABSA methods focus on mining syntactic or semantic information, which still suffers from noisy interference introduced... | Binxing Fang, Heyan Chai, Liqiang Nie, Qing Liao, Siyu Tang, Ye Wang, Ziyi Yao |  |
| 861 |  |  [Robustness of Named-Entity Replacements for In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.728) |  | 0 | A key feature of modern large language models (LLMs) is their ability to perform in-context learning, a prompting technique where query- answer demonstrations are shown before the final query. This allows for generalization to novel distributions at inference time where the LLM can learn new rules... | Adina Williams, Dennis Minn, Jack Lanchantin, Koustuv Sinha, Nikhil Kagita, Roberto Dessì, Saeed Goodarzi, Shubham Toshniwal, Shufan Wang |  |
| 862 |  |  [Contrastive Learning-based Sentence Encoders Implicitly Weight Informative Words](https://doi.org/10.18653/v1/2023.findings-emnlp.729) |  | 0 | The performance of sentence encoders can be significantly improved through the simple practice of fine-tuning using contrastive loss. A natural question arises: what characteristics do models acquire during contrastive learning? This paper theoretically and experimentally shows that... | Goro Kobayashi, Hiroto Kurita, Kentaro Inui, Sho Yokoi |  |
| 863 |  |  [Legally Enforceable Hate Speech Detection for Public Forums](https://doi.org/10.18653/v1/2023.findings-emnlp.730) |  | 0 | Hate speech causes widespread and deep-seated societal issues. Proper enforcement of hate speech laws is key for protecting groups of people against harmful and discriminatory language. However, determining what constitutes hate speech is a complex task that is highly open to subjective... | Chu Fei Luo, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu |  |
| 864 |  |  [ConPrompt: Pre-training a Language Model with Machine-Generated Data for Implicit Hate Speech Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.731) |  | 0 | Implicit hate speech detection is a challenging task in text classification since no explicit cues (e.g., swear words) exist in the text. While some pre-trained language models have been developed for hate speech detection, they are not specialized in implicit hate speech. Recently, an implicit... | Shinwoo Park, YoSub Han, Youngsoo Namgoong, Youngwook Kim |  |
| 865 |  |  [Incorporating Syntactic Knowledge into Pre-trained Language Model using Optimization for Overcoming Catastrophic Forgetting](https://doi.org/10.18653/v1/2023.findings-emnlp.732) |  | 0 | Syntactic knowledge is invaluable information for many tasks which handle complex or long sentences, but typical pre-trained language models do not contain sufficient syntactic knowledge. Thus it results in failures in downstream tasks that require syntactic knowledge. In this paper, we explore... | Hiroshi Kanayama, Issei Yoshida, Masayasu Muraoka, Ran Iwamoto, Takuya Ohko |  |
| 866 |  |  [Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?](https://doi.org/10.18653/v1/2023.findings-emnlp.733) |  | 0 | Large language models can perform downstream tasks in a zero-shot fashion, given natural language prompts that specify the desired behavior. Such prompts are typically hand engineered, but can also be learned with gradient-based methods from labeled data. However, it is underexplored what factors... | Ari Holtzman, Hila Gonen, Luke Zettlemoyer, Weijia Shi, Xiaochuang Han, Yulia Tsvetkov |  |
| 867 |  |  [Chain-of-Thought Reasoning in Tabular Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.734) |  | 0 | Tabular mathematical reasoning task requires models to perform multi-step operations including information look-up and numerical calculation, based on heterogeneous data from tables and questions. Existing solutions tend to extend chain-of-thought (CoT) reasoning into powerful large language models... | Hao Yang, Mingyu Zheng, Qiaoqiao She, Weiping Wang, Wenbin Jiang, Yajuan Lyu, Zheng Lin |  |
| 868 |  |  [Diffusion Language Model with Query-Document Relevance for Query-Focused Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.735) |  | 0 | Query-Focused Summarization (QFS) aims to generate summaries from source documents that can answer specific queries. Although the QFS task has gained increasing attention recently, its development is constrained by the fact that mainstream QFS models are BART variants, which are autoregressive and... | Luozheng Qin, Shaoyao Huang, Ziqiang Cao |  |
| 869 |  |  [Grounded and well-rounded: a methodological approach to the study of cross-modal and cross-lingual grounding](https://doi.org/10.18653/v1/2023.findings-emnlp.736) |  | 0 | Grounding has been argued to be a crucial component towards the development of more complete and truly semantically competent artificial intelligence systems. Literature has divided into two camps: While some argue that grounding allows for qualitatively different generalizations, others believe it... | Denis Paperno, Elaine Zosa, Timothee Mickus |  |
| 870 |  |  [EMO-KNOW: A Large Scale Dataset on Emotion-Cause](https://doi.org/10.18653/v1/2023.findings-emnlp.737) |  | 0 | Emotion-Cause analysis has attracted the attention of researchers in recent years. However, most existing datasets are limited in size and number of emotion categories. They often focus on extracting parts of the document that contain the emotion cause and fail to provide more abstractive,... | Chitralekha Gupta, Mia Huong Nguyen, Prasanth Sasikumar, Suranga Nanayakkara, Yasith Samaradivakara |  |
| 871 |  |  [Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.738) |  | 0 | Parameter-shared pre-trained language models (PLMs) have emerged as a successful approach in resource-constrained environments, enabling substantial reductions in model storage and memory costs without significant performance compromise. However, it is important to note that parameter sharing does... | Jie Zhou, Maosong Sun, Ruobing Xie, Weize Chen, Xiaoyue Xu, Xu Han, Yankai Lin, Zhiyuan Liu |  |
| 872 |  |  [Natural Response Generation for Chinese Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.739) |  | 0 | Machine reading comprehension (MRC) is an important area of conversation agents and draws a lot of attention. However, there is a notable limitation to current MRC benchmarks: The labeled answers are mostly either spans extracted from the target corpus or the choices of the given candidates,... | Baoyuan Wang, Hongguang Li, Jia Li, Nuo Chen, Yinan Bao |  |
| 873 |  |  [Treepiece: Faster Semantic Parsing via Tree Tokenization](https://doi.org/10.18653/v1/2023.findings-emnlp.740) |  | 0 | Autoregressive (AR) encoder-decoder neural networks have proved successful in many NLP problems, including Semantic Parsing – a task that translates natural language to machine-readable parse trees. However, the sequential prediction process of AR models can be slow. To accelerate AR for semantic... | Akshat Shrivastava, Aleksandr Livshits, Sid Wang |  |
| 874 |  |  [Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking](https://doi.org/10.18653/v1/2023.findings-emnlp.741) |  | 0 | Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time-consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations... | Guanting Dong, Weiran Xu, Yuxiang Wu |  |
| 875 |  |  [Mitigating Framing Bias with Polarity Minimization Loss](https://doi.org/10.18653/v1/2023.findings-emnlp.742) |  | 0 | Framing bias plays a significant role in exacerbating political polarization by distorting the perception of actual events. Media outlets with divergent political stances often use polarized language in their reporting of the same event. We propose a new loss function that encourages the model to... | Nayeon Lee, Pascale Fung, Yejin Bang |  |
| 876 |  |  [Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation](https://doi.org/10.18653/v1/2023.findings-emnlp.743) |  | 0 | Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT’s causal reasoning... | Bing Qin, Jinglong Gao, Ting Liu, Xiao Ding |  |
| 877 |  |  [Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.744) |  | 0 | Large language models (LLMs) are a promising avenue for machine translation (MT). However, current LLM-based MT systems are brittle: their effectiveness highly depends on the choice of few-shot examples and they often require extra post-processing due to overgeneration. Alternatives such as... | André F. T. Martins, Duarte M. Alves, José Guilherme Camargo de Souza, José Pombal, João Alves, Nuno Miguel Guerreiro, Pierre Colombo, Ricardo Rei |  |
| 878 |  |  [How Many Demonstrations Do You Need for In-context Learning?](https://doi.org/10.18653/v1/2023.findings-emnlp.745) |  | 0 | Large language models (LLMs) are capable to perform complex reasoning by in-context learning (ICL) when provided with a few input-output demonstrations (demos) and more powerful when intermediate reasoning steps (chain of thoughts (CoT)) of the demos are given. Is it necessary to use multi-demo in... | Chen Zhu, Jiuhai Chen, Lichang Chen, Tianyi Zhou |  |
| 879 |  |  [Improving word mover's distance by leveraging self-attention matrix](https://doi.org/10.18653/v1/2023.findings-emnlp.746) |  | 0 | Measuring the semantic similarity between two sentences is still an important task. The word mover’s distance (WMD) computes the similarity via the optimal alignment between the sets of word embeddings. However, WMD does not utilize word order, making it challenging to distinguish sentences with... | Hidetoshi Shimodaira, Hiroaki Yamagiwa, Sho Yokoi |  |
| 880 |  |  [Improving Span Representation by Efficient Span-Level Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.747) |  | 0 | High-quality span representations are crucial to natural language processing tasks involving span prediction and classification. Most existing methods derive a span representation by aggregation of token representations within the span. In contrast, we aim to improve span representations by... | Kewei Tu, Pengyu Ji, Songlin Yang |  |
| 881 |  |  [Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.748) |  | 0 | Deception and persuasion play a critical role in long-horizon dialogues between multiple parties, especially when the interests, goals, and motivations of the participants are not aligned. Such complex tasks pose challenges for current Large Language Models (LLM) as deception and persuasion can... | Charles Lewis, Joseph Campbell, Katia P. Sycara, Ruiyi Wang, Sanketh Rangreji, Simon Stepputtis, Wenxin Sharon Zhang, Yaqi Xie, Zhengyang Qi |  |
| 882 |  |  [Improving Sequential Model Editing with Fact Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.749) |  | 0 | The task of sequential model editing is to fix erroneous knowledge in Pre-trained Language Models (PLMs) efficiently, precisely and continuously. Although existing methods can deal with a small number of modifications, these methods experience a performance decline or require additional annotated... | Hongye Tan, Jeff Z. Pan, Qinghua Chai, Ru Li, Xiaoqi Han, Yuanlong Wang |  |
| 883 |  |  [Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT - A Text-to-SQL Parsing Comparison](https://doi.org/10.18653/v1/2023.findings-emnlp.750) |  | 0 | The success of ChatGPT has ignited an AI race, with researchers striving to develop new large language models (LLMs) that can match or surpass the language understanding and generation abilities of commercial ones. In recent times, a number of models have emerged, claiming performance near that of... | Bin Chen, Donovan Ong, Jiahuan Yan, Jian Su, Shuo Sun, Yuchen Zhang, Yuze Gao |  |
| 884 |  |  [KBioXLM: A Knowledge-anchored Biomedical Multilingual Pretrained Language Model](https://doi.org/10.18653/v1/2023.findings-emnlp.751) |  | 0 | Most biomedical pretrained language models are monolingual and cannot handle the growing cross-lingual requirements. The scarcity of non-English domain corpora, not to mention parallel data, poses a significant hurdle in training multilingual biomedical models. Since knowledge forms the core of... | Jun Zhang, Juntao Li, Lei Geng, Sujian Li, Wenjie Li, Xinjie Zhou, Xu Yan, Yang Yang, Ziqiang Cao |  |
| 885 |  |  [Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?](https://doi.org/10.18653/v1/2023.findings-emnlp.752) |  | 0 | An important assumption that comes with using LLMs on psycholinguistic data has gone unverified. LLM-based predictions are based on subword tokenization, not decomposition of words into morphemes. Does that matter? We carefully test this by comparing surprisal estimates using orthographic,... | Philip Resnik, Sathvik Nair |  |
| 886 |  |  [A Zero-Shot Language Agent for Computer Control with Structured Reflection](https://doi.org/10.18653/v1/2023.findings-emnlp.753) |  | 0 | Large language models (LLMs) have shown increasing capacity at planning and executing a high-level goal in a live computer environment (e.g. MiniWoB++). To perform a task, recent works often require a model to learn from trace examples of the task via either supervised learning or few/many-shot... | Bryan Wang, Gang Li, Tao Li, Yang Li, Zhiwei Deng |  |
| 887 |  |  [SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF](https://doi.org/10.18653/v1/2023.findings-emnlp.754) |  | 0 | Model alignment with human preferences is an essential step in making Large Language Models (LLMs) helpful and consistent with human values. It typically consists of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) stages. However, RLHF faces inherent limitations... | Makesh Narsimhan Sreedhar, Oleksii Kuchaiev, Xianchao Wu, Yi Dong, Zhilin Wang |  |
| 888 |  |  [IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.755) |  | 0 | The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a... | Gengyu Wang, Hammad A. Ayyubi, Haoxuan You, KaiWei Chang, Long Chen, Rui Sun, ShihFu Chang, Zhecan Wang |  |
| 889 |  |  [GRI: Graph-based Relative Isomorphism of Word Embedding Spaces](https://doi.org/10.18653/v1/2023.findings-emnlp.756) |  | 0 | Automated construction of bi-lingual dictionaries using monolingual embedding spaces is a core challenge in machine translation. The end performance of these dictionaries relies on the geometric similarity of individual spaces, i.e., their degree of isomorphism. Existing attempts aimed at... | Di Wang, Jianbin Qin, Muhammad Asif Ali, Yan Hu |  |
| 890 |  |  [PersonaLM: Language Model Personalization via Domain-distributed Span Aggregated K-Nearest N-gram Retrieval Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.757) |  | 0 | We introduce PersonaLM - Domain-distributed Span-Aggregated K-nearest N-gram retrieval augmentation to improve language modeling for Automatic Speech Recognition (ASR) personalization. PersonaLM leverages contextually similar n-gram word frequencies for recognizing rare word patterns associated... | Dinesh Manocha, Gil Keren, Ke Li, Puneet Mathur, Xuedong Zhang, Yingyi Ma, Zeeshan Ahmed, Zhe Liu |  |
| 891 |  |  [Scaling Vision-Language Models with Sparse Mixture of Experts](https://doi.org/10.18653/v1/2023.findings-emnlp.758) |  | 0 | The field of natural language processing (NLP) has made significant strides in recent years, particularly in the development of large-scale vision-language models (VLMs). These models aim to bridge the gap between text and visual information, enabling a more comprehensive understanding of... | Chunyuan Li, Kurt Keutzer, Sheng Shen, Trevor Darrell, Yuxiong He, Zhewei Yao |  |
| 892 |  |  [Aspect-Category Enhanced Learning with a Neural Coherence Model for Implicit Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.759) |  | 0 | Aspect-based sentiment analysis (ABSA) has been widely studied since the explosive growth of social networking services. However, the recognition of implicit sentiments that do not contain obvious opinion words remains less explored. In this paper, we propose aspect-category enhanced learning with... | Fumiyo Fukumoto, Jin Cui, Jiyi Li, Wanzeng Kong, Xinfeng Wang, Yoshimi Suzuki |  |
| 893 |  |  [End-to-end Adversarial Sample Generation for Data Augmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.760) |  | 0 | Adversarial samples pose a significant challenge to neural inference models. In this paper, we propose a novel enhancing approach A3 for the robustness of the neural NLP models, which combines the adversarial training and data augmentation. We propose an adversarial sample generator that consists... | Tianyuan Liu, Yuqing Sun |  |
| 894 |  |  [Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries over Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.761) |  | 0 | Complex Query Answering (CQA) is a challenge task of Knowledge Graph (KG). Due to the incompleteness of KGs, query embedding (QE) methods have been proposed to encode queries and entities into the same embedding space, and treat logical operators as neural set operators to obtain answers. However,... | Cunguang Wang, Jun Zhao, Kang Liu, Li Cai, Shizhu He, Yao Xu |  |
| 895 |  |  [Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement](https://doi.org/10.18653/v1/2023.findings-emnlp.762) |  | 0 | To enhance the multi-step reasoning capabilities of large language models, researchers have extensively explored prompting methods, notably the Chain-of-Thought (CoT) method which explicitly elicits human-like rationales. However, they have inadvertently overlooked the potential of enhancing model... | Jia Liu, Qi Zhang, Rui Zheng, Senjie Jin, Songyang Gao, Tao Gui, Xuanjing Huang, Yuhao Zhou, Zhiheng Xi |  |
| 896 |  |  [Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection](https://doi.org/10.18653/v1/2023.findings-emnlp.763) |  | 0 | It is widely acknowledged that large and sparse models have higher accuracy than small and dense models under the same model size constraints. This motivates us to train a large model and then remove its redundant neurons or weights by pruning. Most existing works pruned the networks in a... | Dongkuan Xu, Jianwei Li, Qi Lei, Weizhi Gao |  |
| 897 |  |  [Eyes Show the Way: Modelling Gaze Behaviour for Hallucination Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.764) |  | 0 | Detecting hallucinations in natural language processing (NLP) is a critical undertaking that demands a deep understanding of both the semantic and pragmatic aspects of languages. Cognitive approaches that leverage users’ behavioural signals, such as gaze, have demonstrated effectiveness in... | Abhijit Mishra, Ashita Saxena, Kishan Maharaj, Pushpak Bhattacharyya, Raja Kumar |  |
| 898 |  |  [Noisy Pair Corrector for Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.765) |  | 0 | Most dense retrieval models contain an implicit assumption: the training query-document pairs are exactly matched. Since it is expensive to annotate the corpus manually, training pairs in real-world applications are usually collected automatically, which inevitably introduces mismatched-pair noise.... | Daya Guo, Dayiheng Liu, Hang Zhang, Jian Guo, Jiancheng Lv, Xingwei He, Yeyun Gong |  |
| 899 |  |  [Enhancing Accessible Communication: from European Portuguese to Portuguese Sign Language](https://doi.org/10.18653/v1/2023.findings-emnlp.766) |  | 0 | Portuguese Sign Language (LGP) is the official language in deaf education in Portugal. Current approaches in developing a translation system between European Portuguese and LGP rely on hand-crafted rules. In this paper, we present a fully automatic corpora-driven rule-based machine translation... | Catarina Sousa, Luísa Coheur, Mara Moita |  |
| 900 |  |  [Diversifying language models for lesser-studied languages and language-usage contexts: A case of second language Korean](https://doi.org/10.18653/v1/2023.findings-emnlp.767) |  | 0 | This study investigates the extent to which currently available morpheme parsers/taggers apply to lesser-studied languages and language-usage contexts, with a focus on second language (L2) Korean. We pursue this inquiry by (1) training a neural-network model (pre-trained on first language [L1]... | GyuHo Shin, Hakyung Sung |  |
| 901 |  |  [Improving generalization in large langue model by learning prefix subspaces](https://doi.org/10.18653/v1/2023.findings-emnlp.768) |  | 0 | This article focuses on large language models (LLMs) fine-tuning in the scarce data regime (also known as “few-shot learning setting”). We propose a method to increase the generalization capabilities of LLMs based on neural network subspaces. This optimization method, recently introduced in... | Laure Soulier, Louis Falissard, Vincent Guigue |  |
| 902 |  |  [Domain Adaptation for Sentiment Analysis Using Robust Internal Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.769) |  | 0 | Sentiment analysis is a costly yet necessary task for enterprises to study the opinions of their customers to improve their products and to determine optimal marketing strategies. Due to the existence of a wide range of domains across different products and services, cross-domain sentiment analysis... | Aram Galstyan, Digbalay Bose, Mohammad Rostami, Shrikanth Narayanan |  |
| 903 |  |  [KeFVP: Knowledge-enhanced Financial Volatility Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.770) |  | 0 | Financial volatility prediction is vital for indicating a company’s risk profile. Transcripts of companies’ earnings calls are important unstructured data sources to be utilized to access companies’ performance and risk profiles. However, current works ignore the role of financial metrics knowledge... | Hao Niu, Weizu Yang, Wenjing Yu, Xiaosu Wang, Yao Zhang, Yun Xiong |  |
| 904 |  |  [A Frustratingly Easy Plug-and-Play Detection-and-Reasoning Module for Chinese Spelling Check](https://doi.org/10.18653/v1/2023.findings-emnlp.771) |  | 0 | In recent years, Chinese Spelling Check (CSC) has been greatly improved by designing task-specific pre-training methods or introducing auxiliary tasks, which mostly solve this task in an end-to-end fashion. In this paper, we propose to decompose the CSC workflow into detection, reasoning, and... | Feng Zhou, HaiTao Zheng, Haojing Huang, Jingheng Ye, Qingyu Zhou, Yangning Li, Yinghui Li |  |
| 905 |  |  [Asking Clarification Questions to Handle Ambiguity in Open-Domain QA](https://doi.org/10.18653/v1/2023.findings-emnlp.772) |  | 0 | Ambiguous questions persist in open-domain question answering, because formulating a precise question with a unique answer is often challenging. Previous works have tackled this issue by asking disambiguated questions for all possible interpretations of the ambiguous question. Instead, we propose... | Dongryeol Lee, Hwanhee Lee, Joonsuk Park, Kyomin Jung, Minwoo Lee, SangWoo Lee, Segwang Kim |  |
| 906 |  |  [Addressing the Length Bias Challenge in Document-Level Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.773) |  | 0 | Document-level neural machine translation (DNMT) has shown promising results by incorporating context information through increased maximum lengths of source and target sentences. However, this approach also introduces a length bias problem, whereby DNMT suffers from significant translation quality... | Min Zhang, Shuhao Gu, Yang Feng, Zhuocheng Zhang |  |
| 907 |  |  [EconBERTa: Towards Robust Extraction of Named Entities in Economics](https://doi.org/10.18653/v1/2023.findings-emnlp.774) |  | 0 | Adapting general-purpose language models has proven to be effective in tackling downstream tasks within specific domains. In this paper, we address the task of extracting entities from the economics literature on impact evaluation. To this end, we release EconBERTa, a large language model... | Arianna Legovini, Haaya Naushan, John PouguéBiyong, Karim Lasri, Linxi Wang, Luis Eduardo San Martin, Mona Schirmer, Pedro Vitor Quinta de Castro, Samuel Fraiberger, Tomás Dulka |  |
| 908 |  |  [Consonant is all you need: a compact representation of English text for efficient NLP](https://doi.org/10.18653/v1/2023.findings-emnlp.775) |  | 0 | In natural language processing (NLP), the representation of text plays a crucial role in various tasks such as language modeling, sentiment analysis, and machine translation. The standard approach is to represent text in the same way as we, as humans, read and write. In this paper, we propose a... | Irfan Ahmad, Maged Saeed AlShaibani |  |
| 909 |  |  [Detrimental Contexts in Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.776) |  | 0 | For knowledge intensive NLP tasks, it has been widely accepted that accessing more information is a contributing factor to improvements in the model’s end-to-end performance. However, counter-intuitively, too much context can have a negative impact on the model when evaluated on common question... | James Thorne, Philhoon Oh |  |
| 910 |  |  [PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for Languages in India](https://doi.org/10.18653/v1/2023.findings-emnlp.777) |  | 0 | This paper introduces PMIndiaSum, a multilingual and massively parallel summarization corpus focused on languages in India. Our corpus provides a training and testing ground for four language families, 14 languages, and the largest to date with 196 language pairs. We detail our construction... | Ashok Urlana, Barry Haddow, Manish Shrivastava, Pinzhen Chen, Shay B. Cohen, Zheng Zhao |  |
| 911 |  |  [Beyond Labels: Empowering Human Annotators with Natural Language Explanations through a Novel Active-Learning Architecture](https://doi.org/10.18653/v1/2023.findings-emnlp.778) |  | 0 | Real-world domain experts (e.g., doctors) rarely annotate only a decision label in their day-to-day workflow without providing explanations. Yet, existing low-resource learning techniques, such as Active Learning (AL), that aim to support human annotators mostly focus on the label while neglecting... | Bingsheng Yao, Dakuo Wang, Ishan Jindal, James A. Hendler, Lihong He, Lucian Popa, Sayan Ghosh, Shashank Srivastava, Yannis Katsis, Yunyao Li, Yuxuan Lu |  |
| 912 |  |  [Decoding Stumpers: Large Language Models vs. Human Problem-Solvers](https://doi.org/10.18653/v1/2023.findings-emnlp.779) |  | 0 | This paper investigates the problem-solving capabilities of Large Language Models (LLMs) by evaluating their performance on stumpers, unique single-step intuition problems that pose challenges for human solvers but are easily verifiable. We compare the performance of four state-of-the-art LLMs... | Alon Goldstein, Ariel Goldstein, Miriam Havin, Roi Reichart |  |
| 913 |  |  [Efficient Cross-Task Prompt Tuning for Few-Shot Conversational Emotion Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.780) |  | 0 | Emotion Recognition in Conversation (ERC) has been widely studied due to its importance in developing emotion-aware empathetic machines. The rise of pre-trained language models (PLMs) has further pushed the limit of ERC performance. However, most recent works on ERC using PLMs are heavily... | Yige Xu, Zhiqi Shen, Zhiwei Zeng |  |
| 914 |  |  [SYMPTOMIFY: Transforming Symptom Annotations with Language Model Knowledge Harvesting](https://doi.org/10.18653/v1/2023.findings-emnlp.781) |  | 0 | Given the high-stakes nature of healthcare decision-making, we aim to improve the efficiency of human annotators rather than replacing them with fully automated solutions. We introduce a new comprehensive resource, SYMPTOMIFY, a dataset of annotated vaccine adverse reaction reports detailing... | Bosung Kim, Ndapa Nakashole |  |
| 915 |  |  [TokenDrop + BucketSampler: Towards Efficient Padding-free Fine-tuning of Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.782) |  | 0 | The great success of Language Models (LMs) for various Natural Language Processing (NLP) tasks is accompanied by computational challenges during both pre-training and fine-tuning. Pre-training has attracted significant attention due to its huge computational footprint. We focus on the fine-tuning... | Amrit Nagarajan, Anand Raghunathan |  |
| 916 |  |  [Unified Representation for Non-compositional and Compositional Expressions](https://doi.org/10.18653/v1/2023.findings-emnlp.783) |  | 0 | Accurate processing of non-compositional language relies on generating good representations for such expressions. In this work, we study the representation of language non-compositionality by proposing a language model, PIER+, that builds on BART and can create semantically meaningful and... | Suma Bhat, Ziheng Zeng |  |
| 917 |  |  [Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.784) |  | 0 | Retrieval-augmented generation models augment knowledge encoded in a language model by providing additional relevant external knowledge (context) during generation. Although it has been shown that the quantity and quality of context impact the performance of retrieval-augmented generation models... | Kosuke Akimoto, Kunihiro Takeoka, Masafumi Oyamada |  |
| 918 |  |  [Error Detection for Text-to-SQL Semantic Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.785) |  | 0 | Despite remarkable progress in text-to-SQL semantic parsing in recent years, the performance of existing parsers is still far from perfect. Specifically, modern text-to-SQL parsers based on deep learning are often over-confident, thus casting doubt on their trustworthiness when deployed for real... | Huan Sun, Shijie Chen, Yu Su, Ziru Chen |  |
| 919 |  |  [Ultra-Fine Entity Typing with Prior Knowledge about Labels: A Simple Clustering Based Strategy](https://doi.org/10.18653/v1/2023.findings-emnlp.786) |  | 0 | Ultra-fine entity typing (UFET) is the task of inferring the semantic types from a large set of fine-grained candidates that apply to a given entity mention. This task is especially challenging because we only have a small number of training examples for many types, even with distant supervision... | Na Li, Steven Schockaert, Zied Bouraoui |  |
| 920 |  |  [Multilingual Coarse Political Stance Classification of Media. The Editorial Line of a ChatGPT and Bard Newspaper](https://doi.org/10.18653/v1/2023.findings-emnlp.787) |  | 0 | Neutrality is difficult to achieve and, in politics, subjective. Traditional media typically adopt an editorial line that can be used by their potential readers as an indicator of the media bias. Several platforms currently rate news outlets according to their political bias. The editorial line and... | Cristina EspañaBonet |  |
| 921 |  |  [Do "English" Named Entity Recognizers Work Well on Global Englishes?](https://doi.org/10.18653/v1/2023.findings-emnlp.788) |  | 0 | The vast majority of the popular English named entity recognition (NER) datasets contain American or British English data, despite the existence of many global varieties of English. As such, it is unclear whether they generalize for analyzing use of English globally. To test this, we build a... | Alexander Shan, Christopher D. Manning, John Bauer, Riley Carlson |  |
| 922 |  |  [Affective and Dynamic Beam Search for Story Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.789) |  | 0 | Storytelling’s captivating potential makes it a fascinating research area, with implications for entertainment, education, therapy, and cognitive studies. In this paper, we propose Affective Story Generator (AffGen) for generating interesting narratives. AffGen introduces ‘intriguing twists’ in... | Bangzheng Li, Ehsan Qasemi, Faeze Brahman, He Wang, Muhao Chen, Snigdha Chaturvedi, Tenghao Huang |  |
| 923 |  |  [Multiview Clickbait Detection via Jointly Modeling Subjective and Objective Preference](https://doi.org/10.18653/v1/2023.findings-emnlp.790) |  | 0 | Clickbait posts tend to spread inaccurate or misleading information to manipulate people’s attention and emotions, which greatly harms the credibility of social media. Existing clickbait detection models rely on analyzing the objective semantics in posts or correlating posts with article content... | Chongyang Shi, Liang Hu, Liang Xiao, Qi Zhang, Shoujin Wang, Usman Naseem, Yijun Yin |  |
| 924 |  |  [Let's Synthesize Step by Step: Iterative Dataset Synthesis with Large Language Models by Extrapolating Errors from Small Models](https://doi.org/10.18653/v1/2023.findings-emnlp.791) |  | 0 | \*Data Synthesis\* is a promising way to train a small model with very little labeled data. One approach for data synthesis is to leverage the rich knowledge from large language models to synthesize pseudo training examples for small models, making it possible to achieve both data and compute... | Mrinmaya Sachan, Ruida Wang, Wangchunshu Zhou |  |
| 925 |  |  [Identifying Early Maladaptive Schemas from Mental Health Question Texts](https://doi.org/10.18653/v1/2023.findings-emnlp.792) |  | 0 | In Psychotherapy, maladaptive schemas– negative perceptions that an individual has of the self, others, or the world that endure despite objective reality–often lead to resistance to treatments and relapse of mental health issues such as depression, anxiety, panic attacks etc. Identification of... | Beng Heng Ang, SeeKiong Ng, Sujatha Das Gollapalli |  |
| 926 |  |  [Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning](https://doi.org/10.18653/v1/2023.findings-emnlp.793) |  | 0 | Augmenting pretrained language models (LMs) with a vision encoder (e.g., Flamingo) has obtained state-of-the-art results in image-to-text generation. However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual... | Anima Anandkumar, Bo Li, Bryan Catanzaro, Chaowei Xiao, DeAn Huang, Linxi Fan, MingYu Liu, Mohammad Shoeybi, Shiyi Lan, Vijay Korthikanti, Wei Ping, Weili Nie, Yuke Zhu, Zhiding Yu, Zhuolin Yang, Zihan Liu |  |
| 927 |  |  [Syntax Matters: Towards Spoken Language Understanding via Syntax-Aware Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.794) |  | 0 | Spoken Language Understanding (SLU), a crucial component of task-oriented dialogue systems, has consistently garnered attention from both academic and industrial communities. Although incorporating syntactic information into models has the potential to enhance the comprehension of user utterances... | Dongsheng Chen, Xuxin Cheng, Yifeng Xie, Zhihong Zhu, Zhiqi Huang |  |
| 928 |  |  [Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate](https://doi.org/10.18653/v1/2023.findings-emnlp.795) |  | 0 | Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive performance in complex reasoning tasks. However, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial... | Boshi Wang, Huan Sun, Xiang Yue |  |
| 929 |  |  [Using In-Context Learning to Improve Dialogue Safety](https://doi.org/10.18653/v1/2023.findings-emnlp.796) |  | 0 | While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, often perpetuating social biases or stereotypes. We investigate a... | Devamanyu Hazarika, Di Jin, Dilek HakkaniTur, Nicholas Meade, Prakhar Gupta, Siva Reddy, Spandana Gella, Yang Liu |  |
| 930 |  |  [HEAR: Hearing Enhanced Audio Response for Video-grounded Dialogue](https://doi.org/10.18653/v1/2023.findings-emnlp.797) |  | 0 | Video-grounded Dialogue (VGD) aims to answer questions regarding a given multi-modal input comprising video, audio, and dialogue history. Although there have been numerous efforts in developing VGD systems to improve the quality of their responses, existing systems are competent only to incorporate... | Chang Dong Yoo, Dahyun Kim, Eunseop Yoon, Hee Suk Yoon, Junyeong Kim, Sunjae Yoon |  |
| 931 |  |  [Improving Consistency for Text Summarization with Energy Functions](https://doi.org/10.18653/v1/2023.findings-emnlp.798) |  | 0 | Current abstractive summarization models often generate inconsistent content, i.e. texts that are not directly inferable from the source document, are not consistent with respect to world knowledge, or are self-contradictory. These inconsistencies motivate a new consistency taxonomy that we define... | Bing Yin, Chao Zhang, Heng Ji, Qi Zeng, Qingyu Yin, Sreyashi Nag, Yifan Gao, Zheng Li, Zhengyang Wang |  |
| 932 |  |  [Defining a New NLP Playground](https://doi.org/10.18653/v1/2023.findings-emnlp.799) |  | 0 | The recent explosion of performance of large language models (LLMs) has changed the field of Natural Language Processing (NLP) more abruptly and seismically than any other shift in the field’s 80 year history. This has resulted in concerns that the field will become homogenized and... | Carl Edwards, Charles Yu, Chi Han, Eduard H. Hovy, Heng Ji, Joel R. Tetreault, Manling Li, Pengfei Yu, Sha Li, Xingyao Wang, Yi Ren Fung |  |
| 933 |  |  [UPTON: Preventing Authorship Leakage from Public Text Release via Data Poisoning](https://doi.org/10.18653/v1/2023.findings-emnlp.800) |  | 0 | Consider a scenario where an author (e.g., activist, whistle-blower) with many public writings wishes to write “anonymously” when attackers may have already built an authorship attribution (AA) model based off of public writings including those of the author. To enable her wish, we ask a question... | Dongwon Lee, Thai Le, Ziyao Wang |  |
| 934 |  |  [IAEval: A Comprehensive Evaluation of Instance Attribution on Natural Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.801) |  | 0 | Instance attribution (IA) aims to identify the training instances leading to the prediction of a test example, helping researchers understand the dataset better and optimize data processing. While many IA methods have been proposed recently, how to evaluate them still remains open. Previous... | Hua Wu, Lijie Wang, Peijian Gu, Quan Wang, Yaozong Shen, Zhendong Mao |  |
| 935 |  |  [Scene Graph Enhanced Pseudo-Labeling for Referring Expression Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.802) |  | 0 | Referring Expression Comprehension (ReC) is a task that involves localizing objects in images based on natural language expressions. Most ReC methods typically approach the task as a supervised learning problem. However, the need for costly annotations, such as clear image-text pairs or region-text... | Cantao Wu, Jiexin Wang, Liuwu Li, Yi Cai |  |
| 936 |  |  [Noisy Self-Training with Synthetic Queries for Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.803) |  | 0 | Although existing neural retrieval models reveal promising results when training data is abundant and the performance keeps improving as training data increases, collecting high-quality annotated data is prohibitively costly. To this end, we introduce a novel noisy self-training framework combined... | Fan Jiang, Tom Drummond, Trevor Cohn |  |
| 937 |  |  [Leveraging GPT-4 for Automatic Translation Post-Editing](https://doi.org/10.18653/v1/2023.findings-emnlp.804) |  | 0 | While Neural Machine Translation (NMT) represents the leading approach to Machine Translation (MT), the outputs of NMT models still require translation post-editing to rectify errors and enhance quality under critical settings. In this work, we formalize the task of direct translation post-editing... | Amr Sharaf, Arul Menezes, Hany Hassan Awadalla, Vikas Raunak, Yiren Wang |  |
| 938 |  |  [Uniform Complexity for Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.805) |  | 0 | Large language models (LLMs) have shown promising results in a wide array of generative NLP tasks, such as summarization and machine translation. In the context of narrative generation, however, existing models still do not capture factors that contribute to producing consistent text. For instance,... | Harish Tayyar Madabushi, Joseph Marvin Imperial |  |
| 939 |  |  [Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.806) |  | 0 | Large Language Models (LLMs), such as ChatGPT, greatly empower dialogue systems with strong language understanding and generation capabilities. However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic... | Bin Liang, Fei Mi, Hongru Wang, KamFai Wong, Rui Wang, Ruifeng Xu, Yang Deng, Zezhong Wang |  |
| 940 |  |  [CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.807) |  | 0 | Existing works on Aspect Sentiment Triplet Extraction (ASTE) explicitly focus on developing more efficient fine-tuning techniques for the task. Instead, our motivation is to come up with a generic approach that can improve the downstream performances of multiple ABSA tasks simultaneously. Towards... | Nithish Kannen, Pawan Goyal, Rajdeep Mukherjee, Saurabh Kumar Pandey |  |
| 941 |  |  [Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompts](https://doi.org/10.18653/v1/2023.findings-emnlp.808) |  | 0 | Continual pre-training has been urgent for adapting a pre-trained model to a multitude of domains and tasks in the fast-evolving world. In practice, a continually pre-trained model is expected to demonstrate not only greater capacity when fine-tuned on pre-trained domains but also a non-decreasing... | Caigao Jiang, Defu Lian, Gangwei Jiang, James Zhang, Jun Zhou, Siqiao Xue, Ying Wei |  |
| 942 |  |  [Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts](https://doi.org/10.18653/v1/2023.findings-emnlp.809) |  | 0 | Visual question answering (VQA) is the task of answering questions about an image. The task assumes an understanding of both the image and the question to provide a natural language answer. VQA has gained popularity in recent years due to its potential applications in a wide range of fields,... | Deepanway Ghosal, Navonil Majumder, Rada Mihalcea, Roy KaWei Lee, Soujanya Poria |  |
| 943 |  |  [XLS-R fine-tuning on noisy word boundaries for unsupervised speech segmentation into words](https://doi.org/10.18653/v1/2023.findings-emnlp.810) |  | 0 | Due to the absence of explicit word boundaries in the speech stream, the task of segmenting spoken sentences into word units without text supervision is particularly challenging. In this work, we leverage the most recent self-supervised speech models that have proved to quickly adapt to new tasks... | Benoît Sagot, Emmanuel Dupoux, Pablo DiegoSimon, Robin Algayres |  |
| 944 |  |  [Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data](https://doi.org/10.18653/v1/2023.findings-emnlp.811) |  | 0 | Chain-of-thought (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in complex reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications... | Kashun Shum, Shizhe Diao, Tong Zhang |  |
| 945 |  |  [What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations](https://doi.org/10.18653/v1/2023.findings-emnlp.812) |  | 0 | Moral or ethical judgments rely heavily on the specific contexts in which they occur. Understanding varying shades of defeasible contextualizations (i.e., additional information that strengthens or attenuates the moral acceptability of an action) is critical to accurately represent the subtlety and... | Faeze Brahman, Kavel Rao, Liwei Jiang, Niket Tandon, Nouha Dziri, Valentina Pyatkin, Yejin Choi, Yuling Gu |  |
| 946 |  |  [An Empirical Study on Multiple Knowledge from ChatGPT for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2023.findings-emnlp.813) |  | 0 | Multiple knowledge (e.g., co-reference, topics, emotional causes, etc) has been demonstrated effective for emotion detection. However, exploring this knowledge in Emotion Recognition in Conversations (ERC) is currently a blank slate due to the lack of annotated data and the high cost involved in... | Bin Liang, Bing Qin, Geng Tu, KamFai Wong, Ruifeng Xu |  |
| 947 |  |  [Exploiting Contrastive Learning and Numerical Evidence for Confusing Legal Judgment Prediction](https://doi.org/10.18653/v1/2023.findings-emnlp.814) |  | 0 | Given the fact description text of a legal case, legal judgment prediction (LJP) aims to predict the case’s charge, applicable law article, and term of penalty. A core problem of LJP is distinguishing confusing legal cases where only subtle text differences exist. Previous studies fail to... | Anh Tuan Luu, Baokui Li, Fei Wu, Kun Kuang, Lei Wang, Leilei Gan, Yating Zhang, Yi Yang |  |
| 948 |  |  [One For All & All For One: Bypassing Hyperparameter Tuning with Model Averaging for Cross-Lingual Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.815) |  | 0 | Multilingual language models enable zero-shot cross-lingual transfer (ZS-XLT): fine-tuned on sizable source-language task data, they perform the task in target languages without labeled instances. The effectiveness of ZS-XLT hinges on the linguistic proximity between languages and the amount of... | Fabian David Schmidt, Goran Glavas, Ivan Vulic |  |
| 949 |  |  [Dimensions of Online Conflict: Towards Modeling Agonism](https://doi.org/10.18653/v1/2023.findings-emnlp.816) |  | 0 | Agonism plays a vital role in democratic dialogue by fostering diverse perspectives and robust discussions. Within the realm of online conflict there is another type: hateful antagonism, which undermines constructive dialogue. Detecting conflict online is central to platform moderation and... | Alberto Lusoli, Diana Maynard, Maite Taboada, Mali Jin, Matt Canute, Mugdha Pandya, Philippa R. Adams, Wendy Hui Kyong Chun, hannah holtzclaw |  |
| 950 |  |  [Learning under Label Proportions for Text Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.817) |  | 0 | We present one of the preliminary NLP works under the challenging setup of Learning from Label Proportions (LLP), where the data is provided in an aggregate form called bags and only the proportion of samples in each class as the ground truth. This setup is inline with the desired characteristics... | Jatin Chauhan, Wei Wang, Xiaoxuan Wang |  |
| 951 |  |  [MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition](https://doi.org/10.18653/v1/2023.findings-emnlp.818) |  | 0 | Humans have the ability to learn novel compositional concepts by recalling primitive concepts acquired from past experience and generalizing these primitive concepts to novel compositions. Inspired by the above human’s compositional learning procedure, in this paper, we propose MetaReVision, a... | Guangyue Xu, Joyce Chai, Parisa Kordjamshidi |  |
| 952 |  |  [PR-MCS: Perturbation Robust Metric for MultiLingual Image Captioning](https://doi.org/10.18653/v1/2023.findings-emnlp.819) |  | 0 | Vulnerability to lexical perturbation is a critical weakness of automatic evaluation metrics for image captioning. This paper proposes Perturbation Robust Multi-Lingual CLIPScore(PR-MCS), which exhibits robustness to such perturbations, as a novel reference-free image captioning metric applicable... | Hyeongu Yun, Kyomin Jung, Seunghyun Yoon, Trung Bui, Yerin Hwang, Yongil Kim |  |
| 953 |  |  [Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.820) |  | 0 | Scientific literature understanding tasks have gained significant attention due to their potential to accelerate scientific discovery. Pre-trained language models (LMs) have shown effectiveness in these tasks, especially when tuned via contrastive learning. However, jointly utilizing pre-training... | Hao Cheng, Jianfeng Gao, Xiaodong Liu, YeYi Wang, Yu Zhang, Zhihong Shen |  |
| 954 |  |  [BLM-s/lE: A structured dataset of English spray-load verb alternations for testing generalization in LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.821) |  | 0 | Current NLP models appear to be achieving performance comparable to human capabilities on well-established benchmarks. New benchmarks are now necessary to test deeper layers of understanding of natural languages by these models. Blackbird’s Language Matrices are a recently developed framework that... | Chunyang Jiang, Giuseppe Samo, Paola Merlo, Vivi Nastase |  |
| 955 |  |  [Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt](https://doi.org/10.18653/v1/2023.findings-emnlp.822) |  | 0 | Enhancing the zero-shot performance of instruction-following models requires heavy computation, either by scaling the total number of training datasets or the model size. In this work, we explore how retrieval of soft prompts obtained through prompt tuning can efficiently assist hard prompts in... | Doyoung Kim, Joel Jang, Minjoon Seo, Seonghyeon Ye, Yongrae Jo |  |
| 956 |  |  [Geographical Erasure in Language Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.823) |  | 0 | Large language models (LLMs) encode vast amounts of world knowledge. However, since these models are trained on large swaths of internet data, they are at risk of inordinately capturing information about dominant groups. This imbalance can propagate into generated language. In this work, we study... | Cédric Archambeau, Danish Pruthi, Jacek Golebiowski, Michele Donini, Pola Schwöbel |  |
| 957 |  |  [Can Foundation Models Watch, Talk and Guide You Step by Step to Make a Cake?](https://doi.org/10.18653/v1/2023.findings-emnlp.824) |  | 0 | Despite tremendous advances in AI, it remains a significant challenge to develop interactive task guidance systems that can offer situated, personalized guidance and assist humans in various tasks. These systems need to have a sophisticated understanding of the user as well as the environment, and... | Alexander De La Iglesia, Itamar BarYossef, Joyce Chai, Keunwoo Peter Yu, Megan Su, Shane Storks, XiaoLin Zheng, Yichi Zhang, Yuwei Bao |  |
| 958 |  |  [Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?](https://doi.org/10.18653/v1/2023.findings-emnlp.825) |  | 0 | There have been a lot of interest in the scaling properties of Transformer models. However, not much has been done on the front of investigating the effect of scaling properties of different inductive biases and model architectures. Do model architectures scale differently? If so, how does... | Dani Yogatama, Donald Metzler, Hyung Won Chung, Jinfeng Rao, Mostafa Dehghani, Samira Abnar, Sharan Narang, Vinh Q. Tran, William Fedus, Yi Tay |  |
| 959 |  |  [Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.826) |  | 0 | Large language models (LLMs) demonstrate impressive multilingual capability, but their performance varies substantially across different languages. In this work, we introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to systematically improve the multilingual... | Dongdong Zhang, Furu Wei, Haoyang Huang, Tianyi Tang, Ting Song, Xin Zhao, Yan Xia |  |
| 960 |  |  [DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text](https://doi.org/10.18653/v1/2023.findings-emnlp.827) |  | 0 | With the rapid progress of Large language models (LLMs) and the huge amount of text they generate, it becomes impractical to manually distinguish whether a text is machine-generated. The growing use of LLMs in social media and education, prompts us to develop methods to detect machine-generated... | Di Wang, Jinyan Su, Preslav Nakov, Terry Yue Zhuo |  |
| 961 |  |  [From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with Small Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.828) |  | 0 | Reasoning is a distinctive human capacity, enabling us to address complex problems by breaking them down into a series of manageable cognitive steps. Yet, complex logical reasoning is still cumbersome for language models. Based on the dual process theory in cognitive science, we are the first to... | Chengyu Wang, Jun Huang, Junbing Yan, Taolin Zhang, Wei Zhang, Xiaofeng He |  |
| 962 |  |  [Macedon: Minimizing Representation Coding Rate Reduction for Cross-Lingual Natural Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.829) |  | 0 | Cross-lingual natural language understanding(NLU) is one of the fundamental tasks of NLP. The goal is to learn a model which can generalize well on both high-resource and low-resource language data. Recent pre-trained multilingual language models, e.g., multilingual BERT, XLM, have shown impressive... | Haoyu Wang, Huaxiu Yao, Jing Gao, Yaqing Wang |  |
| 963 |  |  [Adversarial Robustness for Large Language NER models using Disentanglement and Word Attributions](https://doi.org/10.18653/v1/2023.findings-emnlp.830) |  | 0 | Large language models (LLM’s) have been widely used for several applications such as question answering, text classification and clustering. While the preliminary results across the aforementioned tasks looks promising, recent work has dived deep into LLM’s performing poorly for complex Named... | Bhanukiran Vinzamuri, Heng Ji, Pradeep Natarajan, Sriram Venkatapathy, Xiaomeng Jin |  |
| 964 |  |  [LLMs - the Good, the Bad or the Indispensable?: A Use Case on Legal Statute Prediction and Legal Judgment Prediction on Indian Court Cases](https://doi.org/10.18653/v1/2023.findings-emnlp.831) |  | 0 | The Large Language Models (LLMs) have impacted many real-life tasks. To examine the efficacy of LLMs in a high-stake domain like law, we have applied state-of-the-art LLMs for two popular tasks: Statute Prediction and Judgment Prediction, on Indian Supreme Court cases. We see that while LLMs... | Anurag Sharma, Atharva Zope, Koustav Rudra, Kripabandhu Ghosh, Shaurya Vats, Shouvik Kumar Guha, Shubham Kumar Nigam, Somsubhra De, Upal Bhattacharya |  |
| 965 |  |  [You Are What You Annotate: Towards Better Models through Annotator Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.832) |  | 0 | Annotator disagreement is ubiquitous in natural language processing (NLP) tasks. There are multiple reasons for such disagreements, including the subjectivity of the task, difficult cases, unclear guidelines, and so on. Rather than simply aggregating labels to obtain data annotations, we instead... | Lu Wang, Naihao Deng, Rada Mihalcea, Siyang Liu, Winston Wu, Xinliang Frederick Zhang |  |
| 966 |  |  [Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers](https://doi.org/10.18653/v1/2023.findings-emnlp.833) |  | 0 | Backdoor attacks manipulate model predictions by inserting innocuous triggers into training and test data. We focus on more realistic and more challenging clean-label attacks where the adversarial training examples are correctly labeled. Our attack, LLMBkd, leverages language models to... | Daniel Lowd, Wencong You, Zayd Hammoudeh |  |
| 967 |  |  [Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance](https://doi.org/10.18653/v1/2023.findings-emnlp.834) |  | 0 | Adopting a two-stage paradigm of pretraining followed by fine-tuning, Pretrained Language Models (PLMs) have achieved substantial advancements in the field of natural language processing. However, in real-world scenarios, data labels are often noisy due to the complex annotation process, making it... | Jundong Li, Ruocheng Guo, Song Wang, Zhen Tan |  |
| 968 |  |  [Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions](https://doi.org/10.18653/v1/2023.findings-emnlp.835) |  | 0 | Large language models (LLMs) are capable of answering knowledge-intensive complex questions with chain-of-thought (CoT) reasoning. However, they tend to generate factually incorrect reasoning steps when the required knowledge is not available or up-to-date in models’ parameters. Recent works turn... | Jiajie Zhang, Jiaxin Shi, Juanzi Li, Lei Hou, Qi Tian, Shulin Cao, Xin Lv, Zijun Yao |  |
| 969 |  |  [Ensemble-Instruct: Instruction Tuning Data Generation with a Heterogeneous Mixture of LMs](https://doi.org/10.18653/v1/2023.findings-emnlp.836) |  | 0 | Using in-context learning (ICL) for data generation, techniques such as Self-Instruct (Wang et al., 2023) or the follow-up Alpaca (Taori et al., 2023) can train strong conversational agents with only a small amount of human supervision. One limitation of these approaches is that they resort to very... | Asim Munawar, Md. Arafat Sultan, Radu Florian, Ramón Fernandez Astudillo, Salim Roukos, Tahira Naseem, YoungSuk Lee, Yousef ElKurdi |  |
| 970 |  |  [The Less the Merrier? Investigating Language Representation in Multilingual Models](https://doi.org/10.18653/v1/2023.findings-emnlp.837) |  | 0 | Multilingual Language Models offer a way to incorporate multiple languages in one model and utilize cross-language transfer learning to improve performance for different Natural Language Processing (NLP) tasks. Despite progress in multilingual models, not all languages are supported as well,... | Atnafu Lambebo Tonja, Hellina Nigatu, Jugal Kalita |  |
| 971 |  |  [SuperTweetEval: A Challenging, Unified and Heterogeneous Benchmark for Social Media NLP Research](https://doi.org/10.18653/v1/2023.findings-emnlp.838) |  | 0 | Despite its relevance, the maturity of NLP for social media pales in comparison with general-purpose models, metrics and benchmarks. This fragmented landscape makes it hard for the community to know, for instance, given a task, which is the best performing model and how it compares with others. To... | Asahi Ushio, Dimosthenis Antypas, Francesco Barbieri, Jiaxin Pei, José CamachoCollados, Kiamehr Rezaee, Leonardo Neves, Luis Espinosa Anke |  |
| 972 |  |  [Enabling Unsupervised Neural Machine Translation with Word-level Visual Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.839) |  | 0 | Unsupervised neural machine translation has recently made remarkable strides, achieving impressive results with the exclusive use of monolingual corpora. Nonetheless, these methods still exhibit fundamental flaws, such as confusing similar words. A straightforward remedy to rectify this drawback is... | Bing Qin, Chengpeng Fu, Hui Wang, Ting Liu, Wenshuai Huo, Xiaocheng Feng, Yichong Huang |  |
| 973 |  |  [Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches](https://doi.org/10.18653/v1/2023.findings-emnlp.840) |  | 0 | People rely heavily on context to enrich meaning beyond what is literally said, enabling concise but effective communication. To interact successfully and naturally with people, user-facing artificial intelligence systems will require similar skills in pragmatics: relying on various types of... | Aida Nematzadeh, Daniel Fried, Jennifer Hu, Nicholas Tomlin, Roma Patel |  |
| 974 |  |  [MISCA: A Joint Model for Multiple Intent Detection and Slot Filling with Intent-Slot Co-Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.841) |  | 0 | The research study of detecting multiple intents and filling slots is becoming more popular because of its relevance to complicated real-world situations. Recent advanced approaches, which are joint models based on graphs, might still face two potential issues: (i) the uncertainty introduced by... | Dat Quoc Nguyen, Thinh Pham, Tran Chi |  |
| 975 |  |  [Enhancing Emotion Recognition in Conversation via Multi-view Feature Alignment and Memorization](https://doi.org/10.18653/v1/2023.findings-emnlp.842) |  | 0 | Emotion recognition in conversation (ERC) has attracted increasing attention in natural language processing community. Previous work commonly first extract semantic-view features via fine-tuning PLMs, then models context-view features based on the obtained semantic-view features by various graph... | Guiyang Hou, Wei Xue, Weiming Lu, Wenqi Zhang, Yongliang Shen |  |
| 976 |  |  [Mandarin classifier systems optimize to accommodate communicative pressures](https://doi.org/10.18653/v1/2023.findings-emnlp.843) |  | 0 | Previous work on noun classification implies that gender systems are inherently optimized to accommodate communicative pressures on human language learning and processing (Dye. et al 2017, 2018). They state that languages make use of either grammatical (e.g., gender) or probabilistic (pre-nominal... | Géraldine Walther, Yamei Wang |  |
| 977 |  |  [Probing Representations for Document-level Event Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.844) |  | 0 | The probing classifiers framework has been employed for interpreting deep neural network models for a variety of natural language processing (NLP) applications. Studies, however, have largely focused on sentencelevel NLP tasks. This work is the first to apply the probing paradigm to representations... | Barry Wang, Claire Cardie, Xinya Du |  |
| 978 |  |  [Cultural Compass: Predicting Transfer Learning Success in Offensive Language Detection with Cultural Features](https://doi.org/10.18653/v1/2023.findings-emnlp.845) |  | 0 | The increasing ubiquity of language technology necessitates a shift towards considering cultural diversity in the machine learning realm, particularly for subjective tasks that rely heavily on cultural nuances, such as Offensive Language Detection (OLD). Current understanding underscores that these... | Antonia Karamolegkou, Daniel Hershcovich, Li Zhou, Wenyu Chen |  |
| 979 |  |  [Linguistically Motivated Sign Language Segmentation](https://doi.org/10.18653/v1/2023.findings-emnlp.846) |  | 0 | Sign language segmentation is a crucial task in sign language processing systems. It enables downstream tasks such as sign recognition, transcription, and machine translation. In this work, we consider two kinds of segmentation: segmentation into individual signs and segmentation into phrases,... | Amit Moryossef, Mathias Müller, Sarah Ebling, Yoav Goldberg, Zifan Jiang |  |
| 980 |  |  [Re-weighting Tokens: A Simple and Effective Active Learning Strategy for Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.847) |  | 0 | Active learning, a widely adopted technique for enhancing machine learning models in text and image classification tasks with limited annotation resources, has received relatively little attention in the domain of Named Entity Recognition (NER). The challenge of data imbalance in NER has hindered... | Haocheng Luo, Lan Du, Ngoc Dang Nguyen, Wei Tan |  |
| 981 |  |  [Language-Agnostic Bias Detection in Language Models with Bias Probing](https://doi.org/10.18653/v1/2023.findings-emnlp.848) |  | 0 | Pretrained language models (PLMs) are key components in NLP, but they contain strong social biases. Quantifying these biases is challenging because current methods focusing on fill-the-mask objectives are sensitive to slight changes in input. To address this, we propose a bias probing technique... | Abdullatif Köksal, Ahmet Akbiyik, Anna Korhonen, Hinrich Schütze, M. Tahir Kilavuz, Omer Faruk Yalcin |  |
| 982 |  |  [CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.849) |  | 0 | How much success in Knowledge Graph Completion (KGC) would translate into the performance enhancement in downstream tasks is an important question that has not been studied in depth. In this paper, we introduce a novel benchmark, namely CompleQA, to comprehensively assess the influence of... | Chenyan Xiong, Donghan Yu, Yiming Yang, Yu Gu |  |
| 983 |  |  [Improving Multi-Criteria Chinese Word Segmentation through Learning Sentence Representation](https://doi.org/10.18653/v1/2023.findings-emnlp.850) |  | 0 | Recent Chinese word segmentation (CWS) models have shown competitive performance with pre-trained language models’ knowledge. However, these models tend to learn the segmentation knowledge through in-vocabulary words rather than understanding the meaning of the entire context. To address this... | ChiaJen Yeh, Ching Yang, Chun Lin, HungYu Kao, YiTing Li, YingJia Lin |  |
| 984 |  |  [A Joint Matrix Factorization Analysis of Multilingual Representations](https://doi.org/10.18653/v1/2023.findings-emnlp.851) |  | 0 | We present an analysis tool based on joint matrix factorization for comparing latent representations of multilingual and monolingual models. An alternative to probing, this tool allows us to analyze multiple sets of representations in a joint manner. Using this tool, we study to what extent and how... | Bonnie Webber, Shay B. Cohen, Yftah Ziser, Zheng Zhao |  |
| 985 |  |  [Don't Add, don't Miss: Effective Content Preserving Generation from Pre-Selected Text Spans](https://doi.org/10.18653/v1/2023.findings-emnlp.852) |  | 0 | The recently introduced Controlled Text Reduction (CTR) task isolates the text generation step within typical summarization-style tasks. It does so by challenging models to generate coherent text conforming to pre-selected content within the input text (“highlights”). This framing enables increased... | Avi Caciularu, Aviv Slobodkin, Eran Hirsch, Ido Dagan |  |
| 986 |  |  [A Computational Interface to Translate Strategic Intent from Unstructured Language in a Low-Data Setting](https://doi.org/10.18653/v1/2023.findings-emnlp.853) |  | 0 | Many real-world tasks involve a mixed-initiative setup, wherein humans and AI systems collaboratively perform a task. While significant work has been conducted towards enabling humans to specify, through language, exactly how an agent should complete a task (i.e., low-level specification), prior... | Lakshita Dodeja, Matthew C. Gombolay, Nathan Vaska, Pradyumna Tambwekar, Wei Xu |  |
| 987 |  |  [HFMRE: Constructing Huffman Tree in Bags to Find Excellent Instances for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.854) |  | 0 | Since the introduction of distantly supervised relation extraction methods, numerous approaches have been developed, the most representative of which is multi-instance learning (MIL). To find reliable features that are most representative of multi-instance bags, aggregation strategies such as AVG... | Cong Shao, Gang Li, Min Li, Mingle Zhou |  |
| 988 |  |  [DISCO: A Large Scale Human Annotated Corpus for Disfluency Correction in Indo-European Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.855) |  | 0 | Disfluency correction (DC) is the process of removing disfluent elements like fillers, repetitions and corrections from spoken utterances to create readable and interpretable text. DC is a vital post-processing step applied to Automatic Speech Recognition (ASR) outputs, before subsequent processing... | Preethi Jyothi, Pushpak Bhattacharyya, Vineet Bhat |  |
| 989 |  |  [Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity](https://doi.org/10.18653/v1/2023.findings-emnlp.856) |  | 0 | Mixture-of-experts (MoE) models that employ sparse activation have demonstrated effectiveness in significantly increasing the number of parameters while maintaining low computational requirements per token. However, recent studies have established that MoE models are inherently... | Haoran Xu, Jean Maillard, Kenton Murray, Maha Elbayad, Vedanuj Goswami |  |
| 990 |  |  [Misery Loves Complexity: Exploring Linguistic Complexity in the Context of Emotion Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.857) |  | 0 | Given the omnipresence of social media in our society, thoughts and opinions are being shared online in an unprecedented manner. This means that both positive and negative emotions can be equally and freely expressed. However, the negativity bias posits that human beings are inherently drawn to and... | Els Lefever, Luna De Bruyne, Orphée De Clercq, Pranaydeep Singh |  |
| 991 |  |  [Probing the "Creativity" of Large Language Models: Can models produce divergent semantic association?](https://doi.org/10.18653/v1/2023.findings-emnlp.858) |  | 0 | Large language models possess remarkable capacity for processing language, but it remains unclear whether these models can further generate creative content. The present study aims to investigate the creative thinking of large language models through a cognitive perspective. We utilize the... | Honghua Chen, Nai Ding |  |
| 992 |  |  [Code-Switching with Word Senses for Pretraining in Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.859) |  | 0 | Lexical ambiguity is a significant and pervasive challenge in Neural Machine Translation (NMT), with many state-of-the-art (SOTA) NMT systems struggling to handle polysemous words (Campolungo et al., 2022). The same holds for the NMT pretraining paradigm of denoising synthetic “code-switched” text... | Alexandra Birch, Edoardo Barba, Jeff Z. Pan, Roberto Navigli, Vivek Iyer |  |
| 993 |  |  [DiffusionSL: Sequence Labeling via Tag Diffusion Process](https://doi.org/10.18653/v1/2023.findings-emnlp.860) |  | 0 | Sequence Labeling (SL) is long-standing in Natural Language Processing (NLP). Traditionally, discriminative models have been widely used to capture the conditional distribution of sequence tags, rather than generative models. In this paper, we present DiffusionSL, a framework that utilizes a... | Jun Zhao, Kang Liu, Pengfei Cao, Ziyang Huang |  |
| 994 |  |  [COMET-M: Reasoning about Multiple Events in Complex Sentences](https://doi.org/10.18653/v1/2023.findings-emnlp.861) |  | 0 | Understanding the speaker’s intended meaning often involves drawing commonsense inferences to reason about what is not stated explicitly. In multi-event sentences, it requires understanding the relationships between events based on contextual knowledge. We propose COMET-M (Multi-Event), an... | Raymond T. Ng, Sahithya Ravi, Vered Shwartz |  |
| 995 |  |  [On Event Individuation for Document-Level Information Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.862) |  | 0 | As information extraction (IE) systems have grown more adept at processing whole documents, the classic task of \*template filling\* has seen renewed interest as a benchmark for document-level IE. In this position paper, we call into question the suitability of template filling for this purpose. We... | Aaron Steven White, Reno Kriz, Siddharth Vashishtha, William Gantt, Yunmo Chen |  |
| 996 |  |  [AniEE: A Dataset of Animal Experimental Literature for Event Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.863) |  | 0 | Event extraction (EE), as a crucial information extraction (IE) task, aims to identify event triggers and their associated arguments from unstructured text, subsequently classifying them into pre-defined types and roles. In the biomedical domain, EE is widely used to extract complex structures... | Dohee Kim, Hee Yang, Jaegul Choo, Ra Yoo, Soyoung Yang |  |
| 997 |  |  [From Words to Wires: Generating Functioning Electronic Devices from Natural Language Descriptions](https://doi.org/10.18653/v1/2023.findings-emnlp.864) |  | 0 | In this work, we show that contemporary language models have a previously unknown skill – the capacity for electronic circuit design from high-level textual descriptions, akin to code generation. We introduce two benchmarks: PINS100, assessing model knowledge of electrical components, and MICRO25,... | Peter Jansen |  |
| 998 |  |  [Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training](https://doi.org/10.18653/v1/2023.findings-emnlp.865) |  | 0 | In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also... | Eduard H. Hovy, Emma Strubell, Zhisong Zhang |  |
| 999 |  |  [Explicit Alignment and Many-to-many Entailment Based Reasoning for Conversational Machine Reading](https://doi.org/10.18653/v1/2023.findings-emnlp.866) |  | 0 | Conversational Machine Reading (CMR) requires answering a user’s initial question through multi-turn dialogue interactions based on a given document. Although there exist many effective methods, they largely neglected the alignment between the document and the user-provided information, which... | Caixia Yuan, Shiyu Tian, Xiaojie Wang, Yangyang Luo |  |
| 1000 |  |  [Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers](https://doi.org/10.18653/v1/2023.findings-emnlp.867) |  | 0 | Neural networks have revolutionized language modeling and excelled in various downstream tasks. However, the extent to which these models achieve compositional generalization comparable to human cognitive abilities remains a topic of debate. While existing approaches in the field have mainly... | Aykut Erdem, Erkut Erdem, Osman Batur Ince, Semih Yagcioglu, Tanin Zeraati, Yadollah Yaghoobzadeh |  |
| 1001 |  |  [Roles of Scaling and Instruction Tuning in Language Perception: Model vs. Human Attention](https://doi.org/10.18653/v1/2023.findings-emnlp.868) |  | 0 | Recent large language models (LLMs) have revealed strong abilities to understand natural language. Since most of them share the same basic structure, i.e. the transformer block, possible contributors to their success in the training process are scaling and instruction tuning. However, how these... | Changjiang Gao, Jiajun Chen, Jixing Li, Shujian Huang |  |
| 1002 |  |  [Efficient Data Learning for Open Information Extraction with Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.869) |  | 0 | Open Information Extraction (OpenIE) is a fundamental yet challenging task in Natural Language Processing, which involves extracting all triples (subject, predicate, object) from a given sentence. While labelling-based methods have their merits, generation-based techniques offer unique advantages,... | Shizhu He, Zhiyuan Fan |  |
| 1003 |  |  [Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning](https://doi.org/10.18653/v1/2023.findings-emnlp.870) |  | 0 | Prompt-based learning has been an effective paradigm for large pretrained language models (LLM), enabling few-shot or even zero-shot learning. Black-box prompt search has received growing interest recently for its distinctive properties of gradient-free optimization, proven particularly useful and... | Anna Korhonen, Han Zhou, Ivan Vulic, Xingchen Wan |  |
| 1004 |  |  [Towards Zero-shot Learning for End-to-end Cross-modal Translation Models](https://doi.org/10.18653/v1/2023.findings-emnlp.871) |  | 0 | One of the main problems in speech translation is the mismatches between different modalities. The second problem, scarcity of parallel data covering multiple modalities, means that the end-to-end multi-modal models tend to perform worse than cascade models, although there are exceptions under... | Boxing Chen, Jichen Yang, Kai Fan, Minpeng Liao, Zhongqiang Huang |  |
| 1005 |  |  [LLMaAA: Making Large Language Models as Active Annotators](https://doi.org/10.18653/v1/2023.findings-emnlp.872) |  | 0 | Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has... | Lei Zou, Ming Zhou, Ruoyu Zhang, Yanzeng Li, Yongliang Ma |  |
| 1006 |  |  [NLMs: Augmenting Negation in Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.873) |  | 0 | Negation is the fundamental component in a natural language that reverses the semantic meaning of a sentence. It plays an extremely important role across a wide range of applications, yet they are underrepresented in pre-trained language models (LMs), resulting often in wrong inferences. In this... | Rahul Kumar, Rituraj Singh, Vivek Sridhar |  |
| 1007 |  |  [Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated Neural Text Retrievers](https://doi.org/10.18653/v1/2023.findings-emnlp.874) |  | 0 | Prompt tuning attempts to update few task-specific parameters in pre-trained models. It has achieved comparable performance to fine-tuning of the full parameter set on both language understanding and generation tasks. In this work, we study the problem of prompt tuning for neural text retrievers.... | Jiahua Liu, Jie Tang, Kaixuan Ji, Lilong Xue, Tao Li, Weng Tam, Xiao Liu, Yuxiao Dong |  |
| 1008 |  |  [X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity](https://doi.org/10.18653/v1/2023.findings-emnlp.875) |  | 0 | Cross-lingual transfer (XLT) is an emergent ability of multilingual language models that preserves their performance on a task to a significant extent when evaluated in languages that were not included in the fine-tuning process. While English, due to its widespread usage, is typically regarded as... | Deokyeong Kang, Jihoon Kim, Jinhyeon Kim, Seong Hoon Lim, Taejun Yun, Taeuk Kim |  |
| 1009 |  |  [Noise-Robust Semi-Supervised Learning for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.876) |  | 0 | Distantly supervised relation extraction (DSRE) aims to extract relational facts from texts but suffers from noisy instances. To mitigate the influence of noisy labels, current methods typically use the Multi-Instance-Learning framework to extract relations for each bag. However, these approaches... | Liang Wang, Qiang Liu, Shu Wu, Xin Sun, Zilei Wang |  |
| 1010 |  |  [Towards Concept-Aware Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.877) |  | 0 | Concepts play a pivotal role in various human cognitive functions, including learning, reasoning and communication. However, there is very little work on endowing machines with the ability to form and reason with concepts. In particular, state-of-the-art large language models (LLMs) work at the... | Chen Shani, Dafna Shahaf, Jilles Vreeken |  |
| 1011 |  |  [ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.878) |  | 0 | Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase... | Amir Pouran Ben Veyseh, Franck Dernoncourt, Hieu Man, Nghia Trung Ngo, Thien Huu Nguyen, Trung Bui, Viet Dac Lai |  |
| 1012 |  |  [Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training](https://doi.org/10.18653/v1/2023.findings-emnlp.879) |  | 0 | Representational spaces learned via language modeling are fundamental to Natural Language Processing (NLP), however there has been limited understanding regarding how and when during training various types of linguistic information emerge and interact. Leveraging a novel information theoretic... | Barbara Plank, Ivan Titov, Max MüllerEberstein, Rob van der Goot |  |
| 1013 |  |  [Not All Demonstration Examples are Equally Beneficial: Reweighting Demonstration Examples for In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.880) |  | 0 | Large Language Models (LLMs) have recently gained the In-Context Learning (ICL) ability with the models scaling up, allowing them to quickly adapt to downstream tasks with only a few demonstration examples prepended in the input sequence. Nonetheless, the current practice of ICL treats all... | Damai Dai, Peiyi Wang, Zhe Yang, Zhifang Sui |  |
| 1014 |  |  [Difference-Masking: Choosing What to Mask in Continued Pretraining](https://doi.org/10.18653/v1/2023.findings-emnlp.881) |  | 0 | The self-supervised objective of masked prediction has led to promising performance gains on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition that deciding what to mask can substantially improve learning outcomes. We investigate this in... | Alex Wilf, Eric Nyberg, Leena Mathur, LouisPhilippe Morency, Mengrou Shou, Paul Pu Liang, Sheryl Mathew, Syeda Nahida Akter |  |
| 1015 |  |  [Learn From One Specialized Sub-Teacher: One-to-One Mapping for Feature-Based Knowledge Distillation](https://doi.org/10.18653/v1/2023.findings-emnlp.882) |  | 0 | Knowledge distillation is known as an effective technique for compressing over-parameterized language models. In this work, we propose to break down the global feature distillation task into N local sub-tasks. In this new framework, we consider each neuron in the last hidden layer of the teacher... | Jelena Mitrovic, Khouloud Saadi, Michael Granitzer |  |
| 1016 |  |  [IMU2CLIP: Language-grounded Motion Sensor Translation with Multimodal Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.883) |  | 0 | We present IMU2CLIP, a novel pre-training approach to align Inertial Measurement Unit (IMU) motion sensor recordings with text and video, by projecting them into the joint representation space of Contrastive Language-Image Pre-training (CLIP). The proposed approach allows IMU2CLIP to translate... | Amy Bearman, Andrea Madotto, Aparajita Saraf, Babak Damavandi, Seungwhan Moon, Zhaojiang Lin |  |
| 1017 |  |  [Conditioning on Dialog Acts improves Empathy Style Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.884) |  | 0 | We explore the role of dialog acts in style transfer, specifically empathy style transfer – rewriting a sentence to make it more empathetic without changing its meaning. Specifically, we use two novel few-shot prompting strategies: target prompting, which only uses examples of the target style... | João Sedoc, Lyle H. Ungar, Renyi Qu |  |
| 1018 |  |  [Systematic Assessment of Factual Knowledge in Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.885) |  | 0 | Previous studies have relied on existing question-answering benchmarks to evaluate the knowledge stored in large language models (LLMs). However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data.... | Dinh Q. Phung, Gholamreza Haffari, Linhao Luo, ThuyTrang Vu |  |
| 1019 |  |  [From Speculation Detection to Trustworthy Relational Tuples in Information Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.886) |  | 0 | Speculation detection is an important NLP task to identify text factuality. However, the extracted speculative information (e.g., speculative polarity, cue, and scope) lacks structure and poses challenges for direct utilization in downstream tasks. Open Information Extraction (OIE), on the other... | Aixin Sun, JungJae Kim, Kuicai Dong, Xiaoli Li |  |
| 1020 |  |  [Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.887) |  | 0 | Generative models have been widely applied to solve extractive tasks, where parts of the input is extracted to form the desired output, and achieved significant success. For example, in extractive question answering (QA), generative models have constantly yielded state-of-the-art results. In this... | Kaiser Sun, Lan Liu, Peng Qi, William Yang Wang, Yuhao Zhang, Zhiheng Huang |  |
| 1021 |  |  [Dialogue Medical Information Extraction with Medical-Item Graph and Dialogue-Status Enriched Representation](https://doi.org/10.18653/v1/2023.findings-emnlp.888) |  | 0 | The multi-turn doctor-patient dialogue includes rich medical knowledge, like the symptoms of the patient, the diagnosis and medication suggested by the doctor. If mined and represented properly, such medical knowledge can benefit a large range of clinical applications, including diagnosis... | Lei Gao, Shen Ge, Xian Wu, Xinnan Zhang, Yefeng Zheng |  |
| 1022 |  |  [LogicAttack: Adversarial Attacks for Evaluating Logical Consistency of Natural Language Inference](https://doi.org/10.18653/v1/2023.findings-emnlp.889) |  | 0 | Recently Large Language Models (LLMs) such as GPT-3, ChatGPT, and FLAN have led to impressive progress in Natural Language Inference (NLI) tasks. However, these models may rely on simple heuristics or artifacts in the evaluation data to achieve their high performance, which suggests that they still... | Chitta Baral, Mihir Parmar, Mutsumi Nakamura, Neeraj Varshney, Santosh Mashetty |  |
| 1023 |  |  [Decomposed Prompt Tuning via Low-Rank Reparameterization](https://doi.org/10.18653/v1/2023.findings-emnlp.890) |  | 0 | While prompt tuning approaches have achieved competitive performance with high efficiency, we observe that they invariably employ the same initialization process, wherein the soft prompt is either randomly initialized or derived from an existing embedding vocabulary. In contrast to these... | Jiaxi Li, Lu Xu, Wei Lu, Xiaoli Li, Yao Xiao |  |
| 1024 |  |  [SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.891) |  | 0 | Building and maintaining end-to-end task bots using minimal human effort is a long-standing challenge in dialog research. In this work, we introduce SGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems effortlessly based on large language models (LLMs). Utilizing the... | Baolin Peng, Helen Meng, Jingyan Zhou, Kun Li, Xiaoying Zhang |  |
| 1025 |  |  [Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.892) |  | 0 | In this position paper, we argue that instead of morally aligning LLMs to specific set of ethical principles, we should infuse generic ethical reasoning capabilities into them so that they can handle value pluralism at a global scale. When provided with an ethical policy, an LLM should be capable... | Abhinav Rao, Aditi Khandelwal, Kumar Tanmay, Monojit Choudhury, Utkarsh Agarwal |  |
| 1026 |  |  [Vector-Quantized Prompt Learning for Paraphrase Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.893) |  | 0 | Deep generative modeling of natural languages has achieved many successes, such as producing fluent sentences and translating from one language into another. However, the development of generative modeling techniques for paraphrase generation still lags behind largely due to the challenges in... | Haotian Luo, Peidong Liu, Xianggen Liu, Yixin Liu |  |
| 1027 |  |  [Rethinking the Construction of Effective Metrics for Understanding the Mechanisms of Pretrained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.894) |  | 0 | Pretrained language models are expected to effectively map input text to a set of vectors while preserving the inherent relationships within the text. Consequently, designing a white-box model to compute metrics that reflect the presence of specific internal relations in these vectors has become a... | Jinhui Yin, You Li, Yuming Lin |  |
| 1028 |  |  [PARROT: Zero-Shot Narrative Reading Comprehension via Parallel Reading](https://doi.org/10.18653/v1/2023.findings-emnlp.895) |  | 0 | Narrative comprehension is a challenging task that requires a deep understanding of the foundational elements of narratives. Acquiring this skill requires extensive annotated data. To mitigate the burden of data annotation, we present Parrot, a zero-shot approach for narrative reading comprehension... | Anvesh Rao Vijjini, Chao Zhao, Snigdha Chaturvedi |  |
| 1029 |  |  [BioDEX: Large-Scale Biomedical Adverse Drug Event Extraction for Real-World Pharmacovigilance](https://doi.org/10.18653/v1/2023.findings-emnlp.896) |  | 0 | Timely and accurate extraction of Adverse Drug Events (ADE) from biomedical literature is paramount for public safety, but involves slow and costly manual labor. We set out to improve drug safety monitoring (pharmacovigilance, PV) through the use of Natural Language Processing (NLP). We introduce... | Aneiss Ghodsi, Chris Develder, Christopher Potts, François Remy, Jack Collins, Johannes Deleu, Karel D'Oosterlinck, Klim Zaporojets, Simon Ellershaw, Thomas Demeester |  |
| 1030 |  |  [Coarse-to-Fine Dual Encoders are Better Frame Identification Learners](https://doi.org/10.18653/v1/2023.findings-emnlp.897) |  | 0 | Frame identification aims to find semantic frames associated with target words in a sentence. Recent researches measure the similarity or matching score between targets and candidate frames by modeling frame definitions. However, they either lack sufficient representation learning of the... | Baobao Chang, Bofei Gao, Ce Zheng, Haozhe Zhao, Kaikai An |  |
| 1031 |  |  [Sound of Story: Multi-modal Storytelling with Audio](https://doi.org/10.18653/v1/2023.findings-emnlp.898) |  | 0 | Storytelling is multi-modal in the real world. When one tells a story, one may use all of the visualizations and sounds along with the story itself. However, prior studies on storytelling datasets and tasks have paid little attention to sound even though sound also conveys meaningful semantics of... | Hyounghun Kim, JaeYon Lee, Jaeyeon Bae, Namgi Han, Seokhoon Jeong, Seokun Kang, Taehwan Kim |  |
| 1032 |  |  [Synthesize, if you do not have: Effective Synthetic Dataset Creation Strategies for Self-Supervised Opinion Summarization in E-commerce](https://doi.org/10.18653/v1/2023.findings-emnlp.899) |  | 0 | In e-commerce, opinion summarization is the process of condensing the opinions presented in product reviews. However, the absence of large amounts of supervised datasets presents challenges in generating both aspect-specific and general opinion summaries. Existing approaches have attempted to... | Amey Patil, Muthusamy Chelliah, Nikesh Garera, Pushpak Bhattacharyya, Sudhanshu Singh, Suman Banerjee, Tejpalsingh Siledar |  |
| 1033 |  |  [Leveraging Contrastive Learning and Knowledge Distillation for Incomplete Modality Rumor Detection](https://doi.org/10.18653/v1/2023.findings-emnlp.900) |  | 0 | Rumors spread rapidly through online social microblogs at a relatively low cost, causing substantial economic losses and negative consequences in our daily lives. Existing rumor detection models often neglect the underlying semantic coherence between text and image components in multimodal posts,... | AiTi Aw, Bowei Zou, Fan Xu, Mingwen Wang, Pinyun Fu, Qi Huang |  |
| 1034 |  |  [Beyond Testers' Biases: Guiding Model Testing with Knowledge Bases using LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.901) |  | 0 | Current model testing work has mostly focused on creating test cases. Identifying what to test is a step that is largely ignored and poorly supported. We propose Weaver, an interactive tool that supports requirements elicitation for guiding model testing. Weaver uses large language models to... | Chenyang Yang, Christian Kästner, Grace A. Lewis, Rachel A. BrowerSinning, Rishabh Rustogi, Tongshuang Wu |  |
| 1035 |  |  [CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.902) |  | 0 | The task of zero-shot commonsense question answering evaluates models on their capacity to reason about general scenarios beyond those presented in specific datasets. Existing approaches for tackling this task leverage external knowledge from CommonSense Knowledge Bases (CSKBs) by pre-training the... | Antoine Bosselut, Baixuan Xu, Tianqing Fang, Weiqi Wang, Wenxuan Ding, Xin Liu, Yangqiu Song |  |
| 1036 |  |  [kNN-CM: A Non-parametric Inference-Phase Adaptation of Parametric Text Classifiers](https://doi.org/10.18653/v1/2023.findings-emnlp.903) |  | 0 | Semi-parametric models exhibit the properties of both parametric and non-parametric modeling and have been shown to be effective in the next-word prediction language modeling task. However, there is a lack of studies on the text-discriminating properties of such models. We propose an... | Bo Cheng, Navonil Majumder, Rishabh Bhardwaj, Soujanya Poria, Yingting Li |  |
| 1037 |  |  [Cross-modality Data Augmentation for End-to-End Sign Language Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.904) |  | 0 | End-to-end sign language translation (SLT) aims to directly convert sign language videos into spoken language texts without intermediate representations. It has been challenging due to the data scarcity of labeled data and the modality gap between sign videos and texts. To tackle these challenges,... | Hui Xiong, Jinhui Ye, Wenxiang Jiao, Xing Wang, Zhaopeng Tu |  |
| 1038 |  |  [Consistency is Key: On Data-Efficient Modality Transfer in Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.905) |  | 0 | End-to-end approaches have shown promising results for speech translation (ST), but they suffer from its data scarcity compared to machine translation (MT). To address this, progressive training has become a common practice, of using external MT data during the fine-tuning phase. Despite of its... | Changmin Lee, Hojin Lee, Seungwon Hwang |  |
| 1039 |  |  [Relation-Aware Question Answering for Heterogeneous Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.906) |  | 0 | Multi-hop Knowledge Base Question Answering(KBQA) aims to find the answer entity in a knowledge graph (KG), which requires multiple steps of reasoning. Existing retrieval-based approaches solve this task by concentrating on the specific relation at different hops and predicting the intermediate... | Chen Li, Chen Zhang, Dongyan Zhao, Haowei Du, Quzhe Huang, Yang Li |  |
| 1040 |  |  [InstOptima: Evolutionary Multi-objective Instruction Optimization via Large Language Model-based Instruction Operators](https://doi.org/10.18653/v1/2023.findings-emnlp.907) |  | 0 | Instruction-based language modeling has received significant attention in pretrained language models. However, the efficiency of instruction engineering remains low and hinders the development of instruction studies. Recent studies have focused on automating instruction generation, but they... | Heng Yang, Ke Li |  |
| 1041 |  |  [Less than One-shot: Named Entity Recognition via Extremely Weak Supervision](https://doi.org/10.18653/v1/2023.findings-emnlp.908) |  | 0 | We study the named entity recognition (NER) problem under the extremely weak supervision (XWS) setting, where only one example entity per type is given in a context-free way. While one can see that XWS is lighter than one-shot in terms of the amount of supervision, we propose a novel method X-NER... | Jingbo Shang, Letian Peng, Zihan Wang |  |
| 1042 |  |  [Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.909) |  | 0 | Transformer-based models have achieved dominant performance in numerous NLP tasks. Despite their remarkable successes, pre-trained transformers such as BERT suffer from a computationally expensive self-attention mechanism that interacts with all tokens, including the ones unfavorable to... | Jungmin Yun, Mihyeon Kim, Youngbin Kim |  |
| 1043 |  |  [Semantic Decomposition of Question and SQL for Text-to-SQL Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.910) |  | 0 | Text-to-SQL semantic parsing faces challenges in generalizing to cross-domain and complex queries. Recent research has employed a question decomposition strategy to enhance the parsing of complex SQL queries.However, this strategy encounters two major obstacles: (1) existing datasets lack question... | Amir Bachar, Ben Eyal, Michael Elhadad, Moran Mahabi, Ophir Haroche |  |
| 1044 |  |  [Time-Aware Language Modeling for Historical Text Dating](https://doi.org/10.18653/v1/2023.findings-emnlp.911) |  | 0 | Automatic text dating(ATD) is a challenging task since explicit temporal mentions usually do not appear in texts. Existing state-of-the-art approaches learn word representations via language models, whereas most of them ignore diachronic change of words, which may affect the efforts of text... | Hai Wang, Han Ren, Yafeng Ren, Yajie Zhao |  |
| 1045 |  |  [A Read-and-Select Framework for Zero-shot Entity Linking](https://doi.org/10.18653/v1/2023.findings-emnlp.912) |  | 0 | Zero-shot entity linking (EL) aims at aligning entity mentions to unseen entities to challenge the generalization ability. Previous methods largely focus on the candidate retrieval stage and ignore the essential candidate ranking stage, which disambiguates among entities and makes the final linking... | Baotian Hu, Min Zhang, Yulin Chen, Zhenran Xu |  |
| 1046 |  |  [Multi-Task Learning of Query Generation and Classification for Generative Conversational Question Rewriting](https://doi.org/10.18653/v1/2023.findings-emnlp.913) |  | 0 | In conversational search settings, users ask questions and receive answers as part of a conversation. The ambiguity in the questions is a common challenge, which can be effectively addressed by leveraging contextual information from the conversation history. In this context, determining topic... | Craig MacDonald, Iadh Ounis, Sarawoot Kongyoung |  |
| 1047 |  |  [DepNeCTI: Dependency-based Nested Compound Type Identification for Sanskrit](https://doi.org/10.18653/v1/2023.findings-emnlp.914) |  | 0 | Multi-component compounding is a prevalent phenomenon in Sanskrit, and understanding the implicit structure of a compound’s components is crucial for deciphering its meaning. Earlier approaches in Sanskrit have focused on binary compounds and neglected the multi-component compound setting. This... | Amba P. Kulkarni, Jivnesh Sandhan, Pavankumar Satuluri, Pawan Goyal, Sreevatsa Muppirala, Sriram Krishnan, Yaswanth Narsupalli |  |
| 1048 |  |  [HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark](https://doi.org/10.18653/v1/2023.findings-emnlp.915) |  | 0 | Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly on morpho-syntactic tasks, neglecting the semantic dimension of language understanding. To bridge this gap, we set out to deliver a Hebrew Machine Reading Comprehension (MRC) dataset, where MRC is to be realized as... | Amir David Nissan Cohen, Hilla Merhav, Reut Tsarfaty, Yoav Goldberg |  |
| 1049 |  |  [HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis](https://doi.org/10.18653/v1/2023.findings-emnlp.916) |  | 0 | Authorship Analysis, also known as stylometry, has been an essential aspect of Natural Language Processing (NLP) for a long time. Likewise, the recent advancement of Large Language Models (LLMs) has made authorship analysis increasingly crucial for distinguishing between human-written and... | Adaku Uchendu, Dongwon Lee, Fosca Giannotti, Mattia Setzu, Nafis Irtiza Tripto, Thai Le |  |
| 1050 |  |  [Data Augmentation for Code Translation with Comparable Corpora and Multiple References](https://doi.org/10.18653/v1/2023.findings-emnlp.917) |  | 0 | One major challenge of translating code between programming languages is that parallel training data is often limited. To overcome this challenge, we present two data augmentation techniques, one that builds comparable corpora (i.e., code pairs with similar functionality), and another that augments... | Atharva Naik, Carolyn P. Rosé, Daniel Fried, Yiqing Xie |  |
| 1051 |  |  [Multilingual Generation and Answering of Questions from Texts and Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-emnlp.918) |  | 0 | The ability to bridge Question Generation (QG) and Question Answering (QA) across structured and unstructured modalities has the potential for aiding different NLP applications. One key application is in QA-based methods that have recently been shown to be useful for automatically evaluating... | Claire Gardent, Kelvin Han |  |
| 1052 |  |  [InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.919) |  | 0 | Diffusion models have garnered considerable interest in the field of text generation. Several studies have explored text diffusion models with different structures and applied them to various tasks, including named entity recognition and summarization. However, there exists a notable disparity... | Jing Li, Piji Li, Renzhi Wang |  |
| 1053 |  |  [Enhancing Scalability of Pre-trained Language Models via Efficient Parameter Sharing](https://doi.org/10.18653/v1/2023.findings-emnlp.920) |  | 0 | In this paper, we propose a highly parameter-efficient approach to scaling pre-trained language models (PLMs) to a deeper model depth. Unlike prior work that shares all parameters or uses extra blocks, we design a more capable parameter-sharing architecture based on matrix product operator (MPO),... | JiRong Wen, Peiyu Liu, Xin Zhao, Yushuo Chen, ZeFeng Gao |  |
| 1054 |  |  [Boosting Prompt-Based Self-Training With Mapping-Free Automatic Verbalizer for Multi-Class Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.921) |  | 0 | Recently, prompt-based fine-tuning has garnered considerable interest as a core technique for few-shot text classification task. This approach reformulates the fine-tuning objective to align with the Masked Language Modeling (MLM) objective. Leveraging unlabeled data, prompt-based self-training has... | Jaehee Kim, Pilsung Kang, Yookyung Kho |  |
| 1055 |  |  [On the Impact of Cross-Domain Data on German Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.922) |  | 0 | Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over... | Ahmad IdrissiYaghir, Amin Dada, Aokun Chen, Cheng Peng, Christoph M. Friedrich, Constantin Seibold, Daniel Truhn, Jan Egger, Jens Kleesiek, Jiang Bian, Jianning Li, Kaleb E. Smith, Lars Heiliger, Yonghui Wu |  |
| 1056 |  |  [Dialect-to-Standard Normalization: A Large-Scale Multilingual Evaluation](https://doi.org/10.18653/v1/2023.findings-emnlp.923) |  | 0 | Text normalization methods have been commonly applied to historical language or user-generated content, but less often to dialectal transcriptions. In this paper, we introduce dialect-to-standard normalization – i.e., mapping phonetic transcriptions from different dialects to the orthographic norm... | Aleksandra Miletic, Olli Kuparinen, Yves Scherrer |  |
| 1057 |  |  [Re-Examining Summarization Evaluation across Multiple Quality Criteria](https://doi.org/10.18653/v1/2023.findings-emnlp.924) |  | 0 | The common practice for assessing automatic evaluation metrics is to measure the correlation between their induced system rankings and those obtained by reliable human evaluation, where a higher correlation indicates a better metric. Yet, an intricate setting arises when an NLP task is evaluated by... | Ido Dagan, Ori Ernst, Ori Shapira, Ran Levy |  |
| 1058 |  |  [A Parallel Corpus for Vietnamese Central-Northern Dialect Text Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.925) |  | 0 | The Vietnamese language embodies dialectal variants closely attached to the nation’s three macro-regions: the Northern, Central and Southern regions. As the northern dialect forms the basis of the standard language, it’s considered the prestige dialect. While the northern dialect differs from the... | Anh Tuan Luu, Thang Le |  |
| 1059 |  |  [A Comprehensive Evaluation of Tool-Assisted Generation Strategies](https://doi.org/10.18653/v1/2023.findings-emnlp.926) |  | 0 | A growing area of research investigates augmenting language models with tools (e.g., search engines, calculators) to overcome their shortcomings (e.g., missing or incorrect knowledge, incorrect logical inferences). Various few-shot tool-usage strategies have been proposed. However, there is no... | Alon Jacovi, Avi Caciularu, Bernd Bohnet, Jonathan Herzig, Mor Geva, Roee Aharoni |  |
| 1060 |  |  [InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT](https://doi.org/10.18653/v1/2023.findings-emnlp.927) |  | 0 | While large models such as GPT-3 demonstrate exceptional performance in zeroshot and fewshot summarization tasks, their extensive serving and fine-tuning costs hinder their utilization in various applications. Conversely, previous studies have found that although automatic metrics tend to favor... | Chenguang Zhu, Dan Iter, Michael Zeng, Ruochen Xu, Shuohang Wang, Yang Liu, Yichong Xu |  |
| 1061 |  |  [Learning to love diligent trolls: Accounting for rater effects in the dialogue safety task](https://doi.org/10.18653/v1/2023.findings-emnlp.928) |  | 0 | Chatbots have the risk of generating offensive utterances, which must be avoided. Post-deployment, one way for a chatbot to continuously improve is to source utterance/label pairs from feedback by live users. However, among users are trolls, who provide training examples with incorrect labels. To... | Michael John Ilagan |  |
| 1062 |  |  [Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal Scenarios Like a Lawyer?](https://doi.org/10.18653/v1/2023.findings-emnlp.929) |  | 0 | Large Language Models (LLMs), such as ChatGPT, have drawn a lot of attentions recently in the legal domain due to its emergent ability to tackle a variety of legal tasks. However, it is still unknown if LLMs are able to analyze a legal case and perform reasoning in the same manner as lawyers.... | Adnan Trakic, Genevieve Grant, LayKi Soon, Lizhen Qu, Patrick Charles Emerton, Terry Yue Zhuo, Xiaoxi Kang |  |
| 1063 |  |  [Coverage-based Example Selection for In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.930) |  | 0 | In-context learning (ICL), the ability of large language models to perform novel tasks by conditioning on a prompt with a few task examples, requires these examples to be informative about the test instance. The standard approach of independently ranking and selecting the most similar examples... | Matt Gardner, Sameer Singh, Shivanshu Gupta |  |
| 1064 |  |  [Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization](https://doi.org/10.18653/v1/2023.findings-emnlp.931) |  | 0 | Large language models (LLMs) have exhibited considerable cross-lingual generalization abilities, whereby they implicitly transfer knowledge across languages. However, the transfer is not equally successful for all languages, especially for low-resource ones, which poses an ongoing challenge. It is... | Jingting Ye, Menghan Zhang, Ningyu Xu, Qi Zhang, Xuanjing Huang |  |
| 1065 |  |  [Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing](https://doi.org/10.18653/v1/2023.findings-emnlp.932) |  | 0 | Dual use, the intentional, harmful reuse of technology and scientific artefacts, is an ill-defined problem within the context of Natural Language Processing (NLP). As large language models (LLMs) have advanced in their capabilities and become more accessible, the risk of their intentional misuse... | Arnav Arora, Isabelle Augenstein, LucieAimée Kaffee, Zeerak Talat |  |
| 1066 |  |  [BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions](https://doi.org/10.18653/v1/2023.findings-emnlp.933) |  | 0 | Text classification is a well-studied and versatile building block for many NLP applications. Yet, existing approaches require either large annotated corpora to train a model with or, when using large language models as a base, require carefully crafting the prompt as well as using a long context... | Artem Harutyunyan, Arth Bohra, Giovanni Campagna, Govert Verkes, Pascal Weinberger |  |
| 1067 |  |  [Approximating CKY with Transformers](https://doi.org/10.18653/v1/2023.findings-emnlp.934) |  | 0 | We investigate the ability of transformer models to approximate the CKY algorithm, using them to directly predict a sentence’s parse and thus avoid the CKY algorithm’s cubic dependence on sentence length. We find that on standard constituency parsing benchmarks this approach achieves competitive or... | Ghazal Khalighinejad, Ollie Liu, Sam Wiseman |  |
| 1068 |  |  [DialGuide: Aligning Dialogue Model Behavior with Developer Guidelines](https://doi.org/10.18653/v1/2023.findings-emnlp.935) |  | 0 | Dialogue models are able to generate coherent and fluent responses, but they can still be challenging to control and may produce non-engaging, unsafe results. This unpredictability diminishes user trust and can hinder the use of the models in the real world. To address this, we introduce DialGuide,... | Behnam Hedayatnia, Di Jin, Dilek HakkaniTur, Julia Hirschberg, Patrick Lange, Prakhar Gupta, Sijia Liu, Spandana Gella, Yang Liu |  |
| 1069 |  |  [RWKV: Reinventing RNNs for the Transformer Era](https://doi.org/10.18653/v1/2023.findings-emnlp.936) |  | 0 | Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but... | Alon Albalak, Atsushi Saito, Bartlomiej Koptyra, Bo Peng, Eric Alcaide, Ferdinand Mom, Guangyu Song, Haowen Hou, Hayden Lau, Huanqi Cao, Jan Kocon, Jiaju Lin, Jiaming Kong, Jian Zhu, Johan S. Wind, Kranthi Kiran GV, Krishna Sri Ipsit Mantri, Leon Derczynski, Matteo Grella, Michael Chung, Przemyslaw Kazienko, Qinghua Zhou, Quentin Anthony, RuiJie Zhu, Samuel Arcadinho, Stanislaw Wozniak, Stella Biderman, Xiangru Tang, Xin Cheng, Xingjian Du, Xuzheng He, Zhenyuan Zhang |  |
| 1070 |  |  [Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification](https://doi.org/10.18653/v1/2023.findings-emnlp.937) |  | 0 | Authorship verification (AV) is a fundamental task in natural language processing (NLP) and computational linguistics, with applications in forensic analysis, plagiarism detection, and identification of deceptive content. Existing AV techniques, including traditional stylometric and deep learning... | ChiaYu Hung, Roy KaWei Lee, Yujia Hu, Zhiqiang Hu |  |
| 1071 |  |  [Transitioning Representations between Languages for Cross-lingual Event Detection via Langevin Dynamics](https://doi.org/10.18653/v1/2023.findings-emnlp.938) |  | 0 | Cross-lingual transfer learning (CLTL) for event detection (ED) aims to develop models in high-resource source languages that can be directly applied to produce effective performance for lower-resource target languages. Previous research in this area has focused on representation matching methods... | Chien Nguyen, Franck Dernoncourt, Huy Nguyen, Thien Huu Nguyen |  |
| 1072 |  |  [VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers](https://doi.org/10.18653/v1/2023.findings-emnlp.939) |  | 0 | Recent advances in interpretability suggest we can project weights and hidden states of transformer-based language models (LMs) to their vocabulary, a transformation that makes them more human interpretable. In this paper, we investigate LM attention heads and memory values, the vectors the models... | Shahar Katz, Yonatan Belinkov |  |
| 1073 |  |  [Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?](https://doi.org/10.18653/v1/2023.findings-emnlp.940) |  | 0 | Robustness, the ability of models to maintain performance in the face of perturbations, is critical for developing reliable NLP systems. Recent studies have shown promising results in improving the robustness of models through adversarial training and data augmentation. However, in machine... | Deyi Xiong, Leiyu Pan, Supryadi |  |
| 1074 |  |  [Arabic Mini-ClimateGPT : A Climate Change and Sustainability Tailored Arabic LLM](https://doi.org/10.18653/v1/2023.findings-emnlp.941) |  | 0 | Climate change is one of the most significant challenges we face together as a society. Creating awareness and educating policy makers the wide-ranging impact of climate change is an essential step towards a sustainable future. Recently, Large Language Models (LLMs) like ChatGPT and Bard have shown... | Abdelrahman M. Shaker, Fahad Shahbaz Khan, Hisham Cholakkal, Omkar Thawakar, Rao Muhammad Anwer, Sahal Shaji Mullappilly, Salman H. Khan |  |
| 1075 |  |  [Interpreting Answers to Yes-No Questions in User-Generated Content](https://doi.org/10.18653/v1/2023.findings-emnlp.942) |  | 0 | Interpreting answers to yes-no questions in social media is difficult. Yes and no keywords are uncommon, and the few answers that include them are rarely to be interpreted what the keywords suggest. In this paper, we present a new corpus of 4,442 yes-no question-answer pairs from Twitter. We... | Dhivya Chinnappa, Eduardo Blanco, Keun Hee Park, Saketh Kotamraju, Shivam Mathur |  |
| 1076 |  |  [Task-Aware Self-Supervised Framework for Dialogue Discourse Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.943) |  | 0 | Dialogue discourse parsing is a fundamental natural language processing task. It can benefit a series of conversation-related downstream tasks including dialogue summarization and emotion recognition in conversations. However, existing parsing approaches are constrained by predefined relation... | Erik Cambria, Luyao Zhu, Wei Li, Wei Shao, Zonglin Yang |  |
| 1077 |  |  [Selective Demonstrations for Cross-domain Text-to-SQL](https://doi.org/10.18653/v1/2023.findings-emnlp.944) |  | 0 | Large language models (LLMs) with in-context learning have demonstrated impressive generalization capabilities in the cross-domain text-to-SQL task, without the use of in-domain annotations. However, incorporating in-domain demonstration examples has been found to greatly enhance LLMs’ performance.... | Eric FoslerLussier, Shuaichen Chang |  |
| 1078 |  |  [DocSplit: Simple Contrastive Pretraining for Large Document Embeddings](https://doi.org/10.18653/v1/2023.findings-emnlp.945) |  | 0 | Existing model pretraining methods only consider local information. For example, in the popular token masking strategy, the words closer to the masked token are more important for prediction than words far away. This results in pretrained models that generate high-quality sentence embeddings, but... | Mike Izbicki, Yujie Wang |  |
| 1079 |  |  [TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.946) |  | 0 | While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied and yet to be benchmarked. However, conducting such benchmarking studies is challenging because of the... | Dongji Feng, Shubhra Kanti Karmaker Santu |  |
| 1080 |  |  [IntenDD: A Unified Contrastive Learning Approach for Intent Detection and Discovery](https://doi.org/10.18653/v1/2023.findings-emnlp.947) |  | 0 | Identifying intents from dialogue utterances forms an integral component of task-oriented dialogue systems. Intent-related tasks are typically formulated either as a classification task, where the utterances are classified into predefined categories or as a clustering task when new and previously... | Amrith Krishna, Ashim Gupta, Bhavuk Singhal, Shivasankaran V. P |  |
| 1081 |  |  [INarIG: Iterative Non-autoregressive Instruct Generation Model For Word-Level Auto Completion](https://doi.org/10.18653/v1/2023.findings-emnlp.948) |  | 0 | Computer-aided translation (CAT) aims to enhance human translation efficiency and is still important in scenarios where machine translation cannot meet quality requirements. One fundamental task within this field is Word-Level Auto Completion (WLAC). WLAC predicts a target word given a source... | Daimeng Wei, Hao Yang, Hengchao Shang, Jiaxin Guo, Lizhi Lei, Minghan Wang, Xiaoyu Chen, Zongyao Li |  |
| 1082 |  |  [Is the Answer in the Text? Challenging ChatGPT with Evidence Retrieval from Instructive Text](https://doi.org/10.18653/v1/2023.findings-emnlp.949) |  | 0 | Generative language models have recently shown remarkable success in generating answers to questions in a given textual context. However, these answers may suffer from hallucination, wrongly cite evidence, and spread misleading information. In this work, we address this problem by employing... | Annemarie Friedrich, Heike Adel, Mohsen Mesgar, Sophie Henning, Talita Anthonio, Wei Zhou |  |
| 1083 |  |  [PaRaDe: Passage Ranking using Demonstrations with LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.950) |  | 0 | Recent studies show that large language models (LLMs) can be instructed to effectively perform zero-shot passage re-ranking, in which the results of a first stage retrieval method, such as BM25, are rated and reordered to improve relevance. In this work, we improve LLM-based re-ranking by... | Andrew Drozdov, Andrew McCallum, Dana Alon, Donald Metzler, Honglei Zhuang, Kai Hui, Mohit Iyyer, Razieh Rahimi, Xuanhui Wang, Zhen Qin, Zhuyun Dai |  |
| 1084 |  |  [Learning Dynamic Representations for Discourse Dependency Parsing](https://doi.org/10.18653/v1/2023.findings-emnlp.951) |  | 0 | Transition systems have been widely used for the discourse dependency parsing task. Existing works often characterize transition states by examining a certain number of elementary discourse units (EDUs), while neglecting the arcs obtained from the transition history. In this paper, we propose to... | Dongyan Zhao, Tianyi Liu, Yansong Feng |  |
| 1085 |  |  [K-HATERS: A Hate Speech Detection Corpus in Korean with Target-Specific Ratings](https://doi.org/10.18653/v1/2023.findings-emnlp.952) |  | 0 | Numerous datasets have been proposed to combat the spread of online hate. Despite these efforts, a majority of these resources are English-centric, primarily focusing on overt forms of hate. This research gap calls for developing high-quality corpora in diverse languages that also encapsulate more... | Chaewon Park, Kunwoo Park, Kyubyong Park, Soohwan Kim |  |
| 1086 |  |  [Mitigating Data Imbalance and Representation Degeneration in Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.953) |  | 0 | Despite advances in multilingual neural machine translation (MNMT), we argue that there are still two major challenges in this area: data imbalance and representation degeneration. The data imbalance problem refers to the imbalance in the amount of parallel corpora for all language pairs,... | Alexander Fraser, Alexandra Chronopoulou, Wen Lai |  |
| 1087 |  |  [BotPercent: Estimating Bot Populations in Twitter Communities](https://doi.org/10.18653/v1/2023.findings-emnlp.954) |  | 0 | Twitter bot detection is vital in combating misinformation and safeguarding the integrity of social media discourse. While malicious bots are becoming more and more sophisticated and personalized, standard bot detection approaches are still agnostic to social environments (henceforth, communities)... | Herun Wan, Melanie Sclar, Minnan Luo, Shangbin Feng, Yejin Choi, Yulia Tsvetkov, Zhaoxuan Tan |  |
| 1088 |  |  [The Locality and Symmetry of Positional Encodings](https://doi.org/10.18653/v1/2023.findings-emnlp.955) |  | 0 | Positional Encodings (PEs) are used to inject word-order information into transformer-based language models. While they can significantly enhance the quality of sentence representations, their specific contribution to language models is not fully understood, especially given recent findings that... | Fabian M. Suchanek, Gaël Varoquaux, Lihu Chen |  |
| 1089 |  |  [Towards a Deep Understanding of Multilingual End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.956) |  | 0 | In this paper, we employ Singular Value Canonical Correlation Analysis (SVCCA) to analyze representations learnt in a multilingual end-to-end speech translation model trained over 22 languages. SVCCA enables us to estimate representational similarity across languages and layers, enhancing our... | Deyi Xiong, Haoran Sun, Shaolin Zhu, Xiaohu Zhao, Yikun Lei |  |
| 1090 |  |  [An Empirical Investigation of Implicit and Explicit Knowledge-Enhanced Methods for Ad Hoc Dataset Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.957) |  | 0 | Ad hoc dataset retrieval has become an important way of finding data on the Web, where the underlying problem is how to measure the relevance of a dataset to a query. State-of-the-art solutions for this task are still lexical methods, which cannot capture semantic similarity. Semantics-aware... | Gong Cheng, Qiaosheng Chen, Weiqing Luo, Zhiyang Zhang, Zixian Huang |  |
| 1091 |  |  [A Multi-Modal Multilingual Benchmark for Document Image Classification](https://doi.org/10.18653/v1/2023.findings-emnlp.958) |  | 0 | Document image classification is different from plain-text document classification and consists of classifying a document by understanding the content and structure of documents such as forms, emails, and other such documents. We show that the only existing dataset for this task (Lewis et al.,... | Bonan Min, Nishant Sankaran, Siddharth Varia, Srikar Appalaraju, Yogarshi Vyas, Yoshinari Fujinuma |  |
| 1092 |  |  [Unnatural language processing: How do language models handle machine-generated prompts?](https://doi.org/10.18653/v1/2023.findings-emnlp.959) |  | 0 | Language model prompt optimization research has shown that semantically and grammatically well-formed manually crafted prompts are routinely outperformed by automatically generated token sequences with no apparent meaning or syntactic structure, including sequences of vectors from a model’s... | Corentin Kervadec, Francesca Franzon, Marco Baroni |  |
| 1093 |  |  [Investigating the Effectiveness of Multiple Expert Models Collaboration](https://doi.org/10.18653/v1/2023.findings-emnlp.960) |  | 0 | This paper aims to investigate the effectiveness of several machine translation (MT) models and aggregation methods in a multi-domain setting under fair conditions and explore a direction for tackling multi-domain MT. We mainly compare the performance of the single model approach by jointly... | Ikumi Ito, Jun Suzuki, Kentaro Inui, Takumi Ito |  |
| 1094 |  |  [Gradually Excavating External Knowledge for Implicit Complex Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.961) |  | 0 | Recently, large language models (LLMs) have gained much attention for the emergence of human-comparable capabilities and huge potential. However, for open-domain implicit question-answering problems, LLMs may not be the ultimate solution due to the reasons of: 1) uncovered or out-of-date domain... | Chang Liu, Edmund Y. Lam, Lifeng Shang, Ngai Wong, Qun Liu, Xiaoguang Li, Xin Jiang |  |
| 1095 |  |  [Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.962) |  | 0 | The emotions we experience involve complex processes; besides physiological aspects, research in psychology has studied cognitive appraisals where people assess their situations subjectively, according to their own values (Scherer, 2005). Thus, the same situation can often result in different... | Desmond C. Ong, Hongli Zhan, Junyi Jessy Li |  |
| 1096 |  |  [Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.963) |  | 0 | The impressive achievements of transformers force NLP researchers to delve into how these models represent the underlying structure of natural language. In this paper, we propose a novel standpoint to investigate the above issue: using typological similarities among languages to observe how their... | Elena Sofia Ruzzetti, Fabio Massimo Zanzotto, Federico Ranaldi, Felicia Logozzo, Leonardo Ranaldi, Michele Mastromattei |  |
| 1097 |  |  [Discourse Sense Flows: Modelling the Rhetorical Style of Documents across Various Domains](https://doi.org/10.18653/v1/2023.findings-emnlp.964) |  | 0 | Recent research on shallow discourse parsing has given renewed attention to the role of discourse relation signals, in particular explicit connectives and so-called alternative lexicalizations. In our work, we first develop new models for extracting signals and classifying their senses, both for... | Manfred Stede, René Knaebel |  |
| 1098 |  |  [HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework for Cross-Domain Zero-Shot Slot Filling](https://doi.org/10.18653/v1/2023.findings-emnlp.965) |  | 0 | In task-oriented dialogue scenarios, cross-domain zero-shot slot filling plays a vital role in leveraging source domain knowledge to learn a model with high generalization ability in unknown target domain where annotated data is unavailable. However, the existing state-of-the-art zero-shot slot... | Junwen Zhang, Yin Zhang |  |
| 1099 |  |  [A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing](https://doi.org/10.18653/v1/2023.findings-emnlp.966) |  | 0 | We evaluate a range of recent LLMs on English creative writing, a challenging and complex task that requires imagination, coherence, and style. We use a difficult, open-ended scenario chosen to avoid training data reuse: an epic narration of a single combat between Ignatius J. Reilly, the... | Carlos GómezRodríguez, Paul Williams |  |
| 1100 |  |  [1-PAGER: One Pass Answer Generation and Evidence Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.967) |  | 0 | We present 1-Pager the first system that answers a question and retrieves evidence using a single Transformer-based model and decoding process. 1-Pager incrementally partitions the retrieval corpus using constrained decoding to select a document and answer string, and we show that this is... | Livio Soares, Palak Jain, Tom Kwiatkowski |  |
| 1101 |  |  [Context-faithful Prompting for Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.968) |  | 0 | Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks... | Hoifung Poon, Muhao Chen, Sheng Zhang, Wenxuan Zhou |  |
| 1102 |  |  [InfoCL: Alleviating Catastrophic Forgetting in Continual Text Classification from An Information Theoretic Perspective](https://doi.org/10.18653/v1/2023.findings-emnlp.969) |  | 0 | Continual learning (CL) aims to constantly learn new knowledge over time while avoiding catastrophic forgetting on old tasks. We focus on continual text classification under the class-incremental setting. Recent CL studies have identified the severe performance decrease on analogous classes as a... | Dawei Zhu, Peiyi Wang, Sujian Li, Tianyu Liu, Weimin Xiong, Yifan Song, Zhifang Sui |  |
| 1103 |  |  [Sparse Frame Grouping Network with Action Centered for Untrimmed Video Paragraph Captioning](https://doi.org/10.18653/v1/2023.findings-emnlp.970) |  | 0 | Generating paragraph captions for untrimmed videos without event annotations is challenging, especially when aiming to enhance precision and minimize repetition at the same time. To address this challenge, we propose a module called Sparse Frame Grouping (SFG). It dynamically groups event... | Guorui Yu, Rui Feng, Shang Gao, Tao Zhang, Yimin Hu, Yuejie Zhang |  |
| 1104 |  |  [Unsupervised Binary Code Translation with Application to Code Clone Detection and Vulnerability Discovery](https://doi.org/10.18653/v1/2023.findings-emnlp.971) |  | 0 | Binary code analysis has immense importance in the research domain of software security. Today, software is very often compiled for various Instruction Set Architectures (ISAs). As a result, cross-architecture binary code analysis has become an emerging problem. Recently, deep learning-based binary... | Iftakhar Ahmad, Lannan Luo |  |
| 1105 |  |  [Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.972) |  | 0 | We address the task of evidence retrieval for long document question answering, which involves locating relevant paragraphs within a document to answer a question. We aim to assess the applicability of large language models (LLMs) in the task of zero-shot long document evidence retrieval, owing to... | Apoorv Saxena, Inderjeet Nair, Koustava Goswami, Shwetha Somasundaram |  |
| 1106 |  |  [Emergent Inabilities? Inverse Scaling Over the Course of Pretraining](https://doi.org/10.18653/v1/2023.findings-emnlp.973) |  | 0 | Does inverse scaling only occur as a function of model size, or can it also occur over the course of training? We carry out an exploratory study investigating whether the performance of language models on specific tasks can decrease (while general performance remains high) during training on the... | Ben Bergen, James A. Michaelov |  |
| 1107 |  |  [Alignment Precedes Fusion: Open-Vocabulary Named Entity Recognition as Context-Type Semantic Matching](https://doi.org/10.18653/v1/2023.findings-emnlp.974) |  | 0 | Despite the significant progress in developing named entity recognition models, scaling to novel-emerging types still remains challenging in real-world scenarios. Continual learning and zero-shot learning approaches have been explored to handle novel-emerging types with less human supervision, but... | Jun Zhao, Kang Liu, Pengfei Cao, Yubo Chen, Zhitao He, Zhuoran Jin |  |
| 1108 |  |  [Representation Projection Invariance Mitigates Representation Collapse](https://doi.org/10.18653/v1/2023.findings-emnlp.975) |  | 0 | Fine-tuning contextualized representations learned by pre-trained language models remains a prevalent practice in NLP. However, fine-tuning can lead to representation degradation (also known as representation collapse), which may result in instability, sub-optimal performance, and weak... | Anastasia Razdaibiedina, Ashish Khetan, Daniel Khashabi, Vivek Madan, Zohar Karnin |  |
| 1109 |  |  [Tunable Soft Prompts are Messengers in Federated Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.976) |  | 0 | Federated learning (FL) enables multiple participants to collaboratively train machine learning models using decentralized data sources, alleviating privacy concerns that arise from directly sharing local data. However, the lack of model privacy protection in FL becomes an unneglectable challenge,... | Bolin Ding, Chenhe Dong, Yaliang Li, Ying Shen, Yuexiang Xie |  |
| 1110 |  |  [Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting](https://doi.org/10.18653/v1/2023.findings-emnlp.977) |  | 0 | Automatically generated reports from medical images promise to improve the workflow of radiologists. Existing methods consider an image-to-report modeling task by directly generating a fully-fledged report from an image. However, this conflates the content of the report (e.g., findings and their... | Agustina Saenz, Benjamin Yan, Chloe O'Connell, David E. Kuo, Eduardo Pontes Reis, Michael Moor, Pranav Rajpurkar, Ruochen Liu, Stephen Kwak, Subathra Adithan, Vasantha Kumar Venugopal |  |
| 1111 |  |  [Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs](https://doi.org/10.18653/v1/2023.findings-emnlp.978) |  | 0 | This paper presents an in-depth study of multimodal machine translation (MMT), examining the prevailing understanding that MMT systems exhibit decreased sensitivity to visual information when text inputs are complete. Instead, we attribute this phenomenon to insufficient cross-modal interaction,... | Bei Li, Chuanhao Lv, JingBo Zhu, Tong Xiao, Tong Zheng, Yuxin Zuo |  |
| 1112 |  |  [GenKIE: Robust Generative Multimodal Document Key Information Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.979) |  | 0 | Key information extraction (KIE) from scanned documents has gained increasing attention because of its applications in various domains. Although promising results have been achieved by some recent KIE approaches, they are usually built based on discriminative models, which lack the ability to... | Panfeng Cao, Qiang Zhang, Ye Wang, Zaiqiao Meng |  |
| 1113 |  |  [Improving Multimodal Sentiment Analysis: Supervised Angular margin-based Contrastive Learning for Enhanced Fusion Representation](https://doi.org/10.18653/v1/2023.findings-emnlp.980) |  | 0 | The effectiveness of a model is heavily reliant on the quality of the fusion representation of multiple modalities in multimodal sentiment analysis. Moreover, each modality is extracted from raw input and integrated with the rest to construct a multimodal representation. Although previous methods... | Anh Tuan Luu, CongDuy Nguyen, Duc Anh Vu, Thong Nguyen |  |
| 1114 |  |  [Efficient Multilingual Language Model Compression through Vocabulary Trimming](https://doi.org/10.18653/v1/2023.findings-emnlp.981) |  | 0 | Multilingual language models (LMs) have become a powerful tool in NLP, especially for non-English languages. Nevertheless, model parameters of multilingual LMs remain large due to the larger embedding matrix of the vocabulary covering tokens in different languages. Instead, monolingual LMs can be... | Asahi Ushio, José CamachoCollados, Yi Zhou |  |
| 1115 |  |  [ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding](https://doi.org/10.18653/v1/2023.findings-emnlp.982) |  | 0 | Most multilingual vision-and-language (V&L) research aims to accomplish multilingual and multimodal capabilities within one model. However, the scarcity of multilingual captions for images has hindered the development. To overcome this obstacle, we propose ICU, Image Caption Understanding, which... | Guojun Wu |  |
| 1116 |  |  [GTA: Gated Toxicity Avoidance for LM Performance Preservation](https://doi.org/10.18653/v1/2023.findings-emnlp.983) |  | 0 | Caution: This paper includes offensive words that could potentially cause unpleasantness. The fast-paced evolution of generative language models such as GPT-4 has demonstrated outstanding results in various NLP generation tasks. However, due to the potential generation of offensive words related to... | Heegyu Kim, Hyunsouk Cho |  |
| 1117 |  |  [LMGQS: A Large-scale Dataset for Query-focused Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.984) |  | 0 | Query-focused summarization (QFS) aims to extract or generate a summary of an input document that directly answers or is relevant to a given query. The lack of large-scale datasets in the form of documents, queries, and summaries has hindered model development in this area. In contrast, multiple... | Chenguang Zhu, Dan Iter, Michael Zeng, Pengcheng He, Ruochen Xu, Shuohang Wang, Song Wang, Yang Liu, Yichong Xu |  |
| 1118 |  |  [ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.985) |  | 0 | Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning. To improve the reasoning abilities, we propose ChatCoT, a tool-augmented... | Beichen Zhang, JiRong Wen, Kun Zhou, Xin Zhao, Zheng Gong, Zhipeng Chen |  |
| 1119 |  |  [Non-Autoregressive Document-Level Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.986) |  | 0 | Non-autoregressive translation (NAT) models achieve comparable performance and superior speed compared to auto-regressive translation (AT) models in the context of sentence-level machine translation (MT). However, their abilities are unexplored in document-level MT, hindering their usage in real... | Guangsheng Bao, Hao Zhou, Jianhao Yan, Yue Zhang, Zhiyang Teng |  |
| 1120 |  |  [Exploring the Effectiveness of Multi-Lingual Commonsense Knowledge-Aware Open-Domain Dialogue Response Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.987) |  | 0 | Prior works have shown the promising results of commonsense knowledge-aware models in improving informativeness while reducing the hallucination issue. Nonetheless, prior works often can only use monolingual knowledge whose language is consistent with the dialogue context. Except for a few... | Jiong Yu, Sixing Wu, Tianshi Che, Wei Zhou, Yang Zhou |  |
| 1121 |  |  [Mixture of Soft Prompts for Controllable Data Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.988) |  | 0 | Large language models (LLMs) effectively generate fluent text when the target output follows natural language patterns. However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such... | Celine Lee, Derek Chen, Domenic Rosati, Yunan Lu, Zhou Yu |  |
| 1122 |  |  [A Boundary Offset Prediction Network for Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.989) |  | 0 | Named entity recognition (NER) is a fundamental task in natural language processing that aims to identify and classify named entities in text. However, span-based methods for NER typically assign entity types to text spans, resulting in an imbalanced sample space and neglecting the connections... | Hongbo Xu, Minghao Tang, Wenyuan Zhang, Yang Lin, Yongquan He, Yongxiu Xu |  |
| 1123 |  |  [Prefix-Tuning Based Unsupervised Text Style Transfer](https://doi.org/10.18653/v1/2023.findings-emnlp.990) |  | 0 | Unsupervised text style transfer aims at training a generative model that can alter the style of the input sentence while preserving its content without using any parallel data. In this paper, we employ powerful pre-trained large language models and present a new prefix-tuning-based method for... | Huiyu Mai, Wenhao Jiang, ZhiHong Deng |  |
| 1124 |  |  [Evaluating and Enhancing the Robustness of Code Pre-trained Models through Structure-Aware Adversarial Samples Generation](https://doi.org/10.18653/v1/2023.findings-emnlp.991) |  | 0 | Code pre-trained models (CodePTMs) have significantly advanced the field of neural code intelligence. Despite their capabilities, these models are susceptible to adversarial attacks that subtly modify the model inputs, resulting in incorrect outputs or predictions. Previous methods of robustness... | Jianing Wang, Ming Gao, Nuo Chen, Qiushi Sun, Xiang Li, Xiaoli Li |  |
| 1125 |  |  [Annotation Sensitivity: Training Data Collection Methods Affect Model Performance](https://doi.org/10.18653/v1/2023.findings-emnlp.992) |  | 0 | When training data are collected from human annotators, the design of the annotation instrument, the instructions given to annotators, the characteristics of the annotators, and their interactions can impact training data. This study demonstrates that design choices made when creating an annotation... | Bolei Ma, Christoph Kern, Frauke Kreuter, Jacob Beck, Rob Chew, Stephanie Eckman |  |
| 1126 |  |  [Qualitative Code Suggestion: A Human-Centric Approach to Qualitative Coding](https://doi.org/10.18653/v1/2023.findings-emnlp.993) |  | 0 | Qualitative coding is a content analysis method in which researchers read through a text corpus and assign descriptive labels or qualitative codes to passages. It is an arduous and manual process which human-computer interaction (HCI) studies have shown could greatly benefit from NLP techniques to... | Cesare Spinoso Di Piano, Jackie Chi Kit Cheung, Samira Abbasgholizadeh Rahimi |  |
| 1127 |  |  [D²TV: Dual Knowledge Distillation and Target-oriented Vision Modeling for Many-to-Many Multimodal Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.994) |  | 0 | Many-to-many multimodal summarization (M3S) task aims to generate summaries in any language with document inputs in any language and the corresponding image sequence, which essentially comprises of multimodal monolingual summarization (MMS) and multimodal cross-lingual summarization (MXLS) tasks.... | Fandong Meng, Jiaan Wang, Jie Zhou, Jinan Xu, Yufeng Chen, Yunlong Liang |  |
| 1128 |  |  [Improving Input-label Mapping with Demonstration Replay for In-context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.995) |  | 0 | In-context learning (ICL) is an emerging capability of large autoregressive language models where a few input-label demonstrations are appended to the input to enhance the model’s understanding of downstream NLP tasks, without directly adjusting the model parameters. The effectiveness of ICL can be... | Dongyan Zhao, Jiahao Liu, Jingang Wang, Qifan Wang, Rui Yan, Xunliang Cai, Zhuocheng Gong |  |
| 1129 |  |  [Enhancing Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies](https://doi.org/10.18653/v1/2023.findings-emnlp.996) |  | 0 | In-context learning (ICL) has emerged as a new approach to various natural language processing tasks, utilizing large language models (LLMs) to make predictions based on context that has been supplemented with a few examples or task-specific instructions. In this paper, we aim to extend this method... | Arman Cohan, Dragomir Radev, Ellen Zhang, Jaesung Tae, Linyong Nan, Narutatsu Ri, Weijin Zou, Yilun Zhao |  |
| 1130 |  |  [Cross-lingual Open-Retrieval Question Answering for African Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.997) |  | 0 | African languages have far less in-language content available digitally, making it challenging for question answering systems to satisfy the information needs of users. Cross-lingual open-retrieval question answering (XOR QA) systems – those that retrieve answer content from other languages while... | Abdou Aziz Diop, Abraham Toluwase Owodunni, Akari Asai, Akintunde Oladipo, Albert Kahira, Andre Niyongabo Rubungo, Aremu Anuoluwapo, Atnafu Lambebo Tonja, Ayodele Awokoya, Bernard Opoku, Bonaventure Dossou, Boyd Sinkala, Chiamaka Chukwuneke, Chinedu E. Mbonu, Chris Emezue, Christine Mwase, Clara Rivera, Claytone Sikasote, Clemencia Siro, Daniel A. Ajisafe, David Ifeoluwa Adelani, Emeka Onwuegbuzia, Falalu Ibrahim Lawan, Gilles Hacheme, Happy Buzaaba, Ibrahim Said Ahmad, Ignatius Ezeani, Iyanuoluwa Shode, Jesujoba O. Alabi, Jonathan H. Clark, Mofetoluwa Adeyemi, Mofya Phiri, Odunayo Ogundepo, Orevaoghene Ahia, Rooweither Mabuya, Ruqayya Nasir Iro, Salomey Osei, Sebastian Ruder, Shamsuddeen Hassan Muhammad, Sonia Adhiambo, Stephen Arthur, Tajuddeen Gwadabe, Tunde Ajayi, Verrah Otiende |  |
| 1131 |  |  [Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens](https://doi.org/10.18653/v1/2023.findings-emnlp.998) |  | 0 | We argue that translation quality alone is not a sufficient metric for measuring knowledge transfer in multilingual neural machine translation. To support this claim, we introduce Representational Transfer Potential (RTP), which measures representational similarities between languages. We show that... | Christof Monz, David Stap, Vlad Niculae |  |
| 1132 |  |  [Aligning Predictive Uncertainty with Clarification Questions in Grounded Dialog](https://doi.org/10.18653/v1/2023.findings-emnlp.999) |  | 0 | Asking for clarification is fundamental to effective collaboration. An interactive artificial agent must know when to ask a human instructor for more information in order to ascertain their goals. Previous work bases the timing of questions on supervised models learned from interactions between... | Christof Monz, Kata Naszádi, Putra Manggala |  |
| 1133 |  |  [Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.1000) |  | 0 | Prompting Large Language Models (LLMs) performs impressively in zero- and few-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot afford the cost of creating large task-specific training datasets, but also the cost of pretraining their own LLMs, are increasingly turning to... | Ilias Stogiannidis, Ion Androutsopoulos, Prodromos Malakasiotis, Stavros Vassos |  |
| 1134 |  |  [ParroT: Translating during Chat using Large Language Models tuned with Human Translation and Feedback](https://doi.org/10.18653/v1/2023.findings-emnlp.1001) |  | 0 | Large language models (LLMs) like ChatGPT have exhibited remarkable abilities on a wide range of natural language processing (NLP) tasks, including various machine translation abilities accomplished during chat. However, these models are only accessible through restricted APIs, which creates... | Jentse Huang, Shuming Shi, Tian Liang, Wenxiang Jiao, Wenxuan Wang, Xing Wang, Zhaopeng Tu, Zhiwei He |  |
| 1135 |  |  [Dense Retrieval as Indirect Supervision for Large-space Decision Making](https://doi.org/10.18653/v1/2023.findings-emnlp.1002) |  | 0 | Many discriminative natural language understanding (NLU) tasks have large label spaces. Learning such a process of large-space decision making is particularly challenging due to the lack of training instances per label and the difficulty of selection among many fine-grained labels. Inspired by... | Fei Wang, Mingtao Dong, Muhao Chen, Nan Xu |  |
| 1136 |  |  [One-Model-Connects-All: A Unified Graph Pre-Training Model for Online Community Modeling](https://doi.org/10.18653/v1/2023.findings-emnlp.1003) |  | 0 | Online community is composed of communities, users, and user-generated textual content, with rich information that can help us solve social problems. Previous research hasn’t fully utilized these three components and the relationship among them. What’s more, they can’t adapt to a wide range of... | Haozhe Zhang, Jiarong Xu, Qi Zhang, Ruoxue Ma, Xinnong Zhang, Xuanjing Huang, Zhongyu Wei, Zuyu Zhao |  |
| 1137 |  |  [In-Image Neural Machine Translation with Segmented Pixel Sequence-to-Sequence Model](https://doi.org/10.18653/v1/2023.findings-emnlp.1004) |  | 0 | In-Image Machine Translation (IIMT) aims to convert images containing texts from one language to another. Traditional approaches for this task are cascade methods, which utilize optical character recognition (OCR) followed by neural machine translation (NMT) and text rendering. However, the cascade... | Bin Wang, Xiang Li, Yanzhi Tian, Yuhang Guo, Zeming Liu |  |
| 1138 |  |  [NarrativeXL: a Large-scale Dataset for Long-Term Memory Models](https://doi.org/10.18653/v1/2023.findings-emnlp.1005) |  | 0 | We propose a new large-scale (nearly a million questions) ultra-long-context (more than 50,000 words average document length) reading comprehension dataset. Using GPT 3.5, we summarized each scene in 1,500 hand-curated fiction books from Project Gutenberg, which resulted in approximately 150... | Arsenii Moskvichev, KyVinh Mai |  |
| 1139 |  |  [Dialogue Act-Aided Backchannel Prediction Using Multi-Task Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.1006) |  | 0 | Produced in the form of small injections such as “Yeah!” or “Uh-Huh” by listeners in a conversation, supportive verbal feedback (i.e., backchanneling) is essential for natural dialogue. Highlighting its tight relation to speaker intent and utterance type, we propose a multi-task learning approach... | KongJoo Lee, Wencke Liermann, YoHan Park, YongSeok Choi |  |
| 1140 |  |  [mReFinED: An Efficient End-to-End Multilingual Entity Linking System](https://doi.org/10.18653/v1/2023.findings-emnlp.1007) |  | 0 | End-to-end multilingual entity linking (MEL) is concerned with identifying multilingual entity mentions and their corresponding entity IDs in a knowledge base. Existing works assumed that entity mentions were given and skipped the entity mention detection step due to a lack of high-quality... | Amir Saffari, Christos Christodoulopoulos, Jens Lehmann, Peerat Limkonchotiwat, Weiwei Cheng |  |
| 1141 |  |  [Sub-network Discovery and Soft-masking for Continual Learning of Mixed Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.1008) |  | 0 | Continual learning (CL) has two main objectives: preventing catastrophic forgetting (CF) and encouraging knowledge transfer (KT). The existing literature mainly focused on overcoming CF. Some work has also been done on KT when the tasks are similar. To our knowledge, only one method has been... | Asli Celikyilmaz, Bing Liu, Haoran Li, Wenhan Xiong, Zixuan Ke |  |
| 1142 |  |  [PIVOINE: Instruction Tuning for Open-world Entity Profiling](https://doi.org/10.18653/v1/2023.findings-emnlp.1009) |  | 0 | This work considers the problem of Open-world Entity Profiling, a sub-domain of Open-world Information Extraction (Open-world IE). Unlike the conventional closed-world IE, Open-world IE is considered a more general situation where entities and relations could be beyond a predefined ontology. We... | Dong Yu, Hongming Zhang, Jianshu Chen, Kaiqiang Song, Keming Lu, Xiaoman Pan |  |
| 1143 |  |  [DiQAD: A Benchmark Dataset for Open-domain Dialogue Quality Assessment](https://doi.org/10.18653/v1/2023.findings-emnlp.1010) |  | 0 | Dialogue assessment plays a critical role in the development of open-domain dialogue systems. Existing work are uncapable of providing an end-to-end and human-epistemic assessment dataset, while they only provide sub-metrics like coherence or the dialogues are conversed between annotators far from... | Chong Meng, Dawei Yin, Lingyong Yan, Shuaiqiang Wang, Weiwei Sun, Yukun Zhao, Zhaochun Ren, Zhicong Cheng |  |
| 1144 |  |  [Tuna: Instruction Tuning using Feedback from Large Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.1011) |  | 0 | Instruction tuning of open-source large language models (LLMs) like LLaMA, using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4, has proven to be a cost-effective way to align model behaviors with human preferences. However, the instruction-tuned model has only seen one... | Furu Wei, Haoran Li, Wei Lu, Xingxing Zhang, Yiran Liu |  |
| 1145 |  |  [Emptying the Ocean with a Spoon: Should We Edit Models?](https://doi.org/10.18653/v1/2023.findings-emnlp.1012) |  | 0 | We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations. We contrast model editing with three similar but distinct approaches that pursue better defined objectives: (1) retrieval-based architectures, which decouple... | Michael Elhadad, Yuval Pinter |  |
| 1146 |  |  [A Causal View of Entity Bias in (Large) Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.1013) |  | 0 | Entity bias widely affects pretrained (large) language models, causing them to rely on (biased) parametric knowledge to make unfaithful predictions. Although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying... | Fei Wang, Muhao Chen, Wenjie Mo, Wenxuan Zhou, Yiwei Wang |  |
| 1147 |  |  [T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics](https://doi.org/10.18653/v1/2023.findings-emnlp.1014) |  | 0 | Modern embedding-based metrics for evaluation of generated text generally fall into one of two paradigms: discriminative metrics that are trained to directly predict which outputs are of higher quality according to supervised human annotations, and generative metrics that are trained to evaluate... | Graham Neubig, Pengfei Liu, Weizhe Yuan, Yiwei Qin |  |
| 1148 |  |  [T-Projection: High Quality Annotation Projection for Sequence Labeling Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.1015) |  | 0 | In the absence of readily available labeled data for a given sequence labeling task and language, annotation projection has been proposed as one of the possible strategies to automatically generate annotated data. Annotation projection has often been formulated as the task of transporting, on... | German Rigau, Iker GarcíaFerrero, Rodrigo Agerri |  |
| 1149 |  |  [MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document](https://doi.org/10.18653/v1/2023.findings-emnlp.1016) |  | 0 | The facts and time in the document are intricately intertwined, making temporal reasoning over documents challenging. Previous work models time implicitly, making it difficult to handle such complex relationships. To address this issue, we propose MTGER, a novel Multi-view Temporal Graph Enhanced... | Bing Qin, Jiafeng Liang, Ming Liu, Zekun Wang, Zheng Chu |  |
| 1150 |  |  [MSCFFN: A New FFN with Multi-Space Cross to Accelerate Transformer](https://doi.org/10.18653/v1/2023.findings-emnlp.1017) |  | 0 | Transformer models have achieved impressive success in various natural language processing tasks. But it is also limited used in some areas and the heavy computation complexity is one of the main limitations. Many model structures have been proposed to reduce the computation complexity and some are... | Qing Yang, Tang Dongge |  |
| 1151 |  |  [Dialect Transfer for Swiss German Speech Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.1018) |  | 0 | This paper investigates the challenges in building Swiss German speech translation systems, specifically focusing on the impact of dialect diversity and differences between Swiss German and Standard German. Swiss German is a spoken language with no formal writing system, it comprises many diverse... | Claudio Paonessa, Jan Deriu, Manfred Vogel, Manuela Hürlimann, Mark Cieliebak, Yanick Schraner |  |
| 1152 |  |  [Masked Path Modeling for Vision-and-Language Navigation](https://doi.org/10.18653/v1/2023.findings-emnlp.1019) |  | 0 | Vision-and-language navigation (VLN) agents are trained to navigate in real-world environments based on natural language instructions. A major challenge in VLN is the limited available training data, which hinders the models’ ability to generalize effectively. Previous approaches have attempted to... | Feng Gao, Nanyun Peng, ZiYi Dou |  |
| 1153 |  |  [Learning Interpretable Style Embeddings via Prompting LLMs](https://doi.org/10.18653/v1/2023.findings-emnlp.1020) |  | 0 | Style representation learning builds content-independent representations of author style in text. To date, no large dataset of texts with stylometric annotations on a wide range of style dimensions has been compiled, perhaps because the linguistic expertise to perform such annotation would be... | Ajay Patel, Ansh Kothary, Chris CallisonBurch, Delip Rao, Kathleen R. McKeown |  |
| 1154 |  |  [Exploring Context-Aware Evaluation Metrics for Machine Translation](https://doi.org/10.18653/v1/2023.findings-emnlp.1021) |  | 0 | Previous studies on machine translation evaluation mostly focused on the quality of individual sentences, while overlooking the important role of contextual information. Although WMT Metrics Shared Tasks have introduced context content into the human annotations of translation evaluation since... | Xiaojun Wan, Xinyu Hu, Xunjian Yin |  |
| 1155 |  |  [GRACE: Discriminator-Guided Chain-of-Thought Reasoning](https://doi.org/10.18653/v1/2023.findings-emnlp.1022) |  | 0 | In the context of multi-step reasoning, e.g., with chain-of-thought, language models (LMs) can easily assign a high likelihood to incorrect steps. As a result, decoding strategies that optimize for solution likelihood often yield incorrect solutions. To address this issue, we propose Guiding... | Honglak Lee, Lajanugen Logeswaran, Lu Wang, Moontae Lee, Muhammad Khalifa |  |
| 1156 |  |  [QADYNAMICS: Training Dynamics-Driven Synthetic QA Diagnostic for Zero-Shot Commonsense Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.1023) |  | 0 | Zero-shot commonsense Question-Answering (QA) requires models to reason about general situations beyond specific benchmarks. State-of-the-art approaches fine-tune language models on QA pairs constructed from CommonSense Knowledge Bases (CSKBs) to equip the models with more commonsense knowledge in... | Baixuan Xu, Haochen Shi, Tianqing Fang, Weiqi Wang, Wenxuan Ding, Xin Liu, Yangqiu Song |  |
| 1157 |  |  [RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction](https://doi.org/10.18653/v1/2023.findings-emnlp.1024) |  | 0 | Universal Information Extraction (UIE) is an area of interest due to the challenges posed by varying targets, heterogeneous structures, and demand-specific schemas. Previous works have achieved success by unifying a few tasks, such as Named Entity Recognition (NER) and Relation Extraction (RE),... | Changlong Sun, Chengyuan Liu, Fei Wu, Fubang Zhao, Jingyuan Zhang, Kun Kuang, Xiang Zhou, Yangyang Kang |  |
| 1158 |  |  [PromptARA: Improving Deep Representation in Hybrid Automatic Readability Assessment with Prompt and Orthogonal Projection](https://doi.org/10.18653/v1/2023.findings-emnlp.1025) |  | 0 | Readability assessment aims to automatically classify texts based on readers’ reading levels. The hybrid automatic readability assessment (ARA) models using both deep and linguistic features have attracted rising attention in recent years due to their impressive performance. However, deep features... | Jinshan Zeng, Wenyan Xiao, Xianchao Tong, Xianglong Yu |  |
| 1159 |  |  [Does Listener Gaze in Face-to-Face Interaction Follow the Entropy Rate Constancy Principle: An Empirical Study](https://doi.org/10.18653/v1/2023.findings-emnlp.1026) |  | 0 | It is generally assumed that language (written and spoken) follows the entropy rate constancy (ERC) principle, which states that the information density of a text is constant over time. Recently, this has also been found for nonverbal gestures used in monologue, but it is still unclear whether the... | Hendrik Buschmeier, Yu Wang |  |
| 1160 |  |  [Incorporating Object-Level Visual Context for Multimodal Fine-Grained Entity Typing](https://doi.org/10.18653/v1/2023.findings-emnlp.1027) |  | 0 | Fine-grained entity typing (FGET) aims to assign appropriate fine-grained types to entity mentions within their context, which is an important foundational task in natural language processing. Previous approaches for FGET only utilized textual context information. However, in the form of short... | Kehui Song, Wenbo Fan, Xiaojie Yuan, Xuhui Sui, Ying Zhang, Yu Zhao |  |
| 1161 |  |  [Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data](https://doi.org/10.18653/v1/2023.findings-emnlp.1028) |  | 0 | Numerical data plays a crucial role in various real-world domains like finance, economics, and science. Thus, understanding and reasoning with numbers are essential in these fields. Recent benchmarks have assessed the numerical reasoning abilities of language models, revealing their limitations in... | Abhilash Reddy Shankarampeta, Arpit Patil, Elena Simperl, Mubashara Akhtar, Oana Cocarascu, Vivek Gupta |  |
| 1162 |  |  [Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks](https://doi.org/10.18653/v1/2023.findings-emnlp.1029) |  | 0 | Large language models have revolutionized the field of NLP by achieving state-of-the-art performance on various tasks. However, there is a concern that these models may disclose information in the training data. In this study, we focus on the summarization task and investigate the membership... | Gord Lueck, Huseyin A. Inan, Janardhan Kulkarni, Rodolfo Quispe, Ruixiang Tang, Xia Hu |  |
| 1163 |  |  [BERT Has More to Offer: BERT Layers Combination Yields Better Sentence Embeddings](https://doi.org/10.18653/v1/2023.findings-emnlp.1030) |  | 0 | Obtaining sentence representations from BERT-based models as feature extractors is invaluable as it takes much less time to pre-compute a one-time representation of the data and then use it for the downstream tasks, rather than fine-tune the whole BERT. Most previous works acquire a sentence’s... | Latifur Khan, Munawara Munia, Seyyed MohammadSaleh Hosseini |  |
| 1164 |  |  [Extrapolating Multilingual Understanding Models as Multilingual Generators](https://doi.org/10.18653/v1/2023.findings-emnlp.1031) |  | 0 | Multilingual understanding models (or encoder-based), pre-trained via masked language modeling, have achieved promising results on many language understanding tasks (e.g., mBERT). However, these models are not capable of generating high-quality text compared with decoder-based causal language... | Bohong Wu, Fei Yuan, Hai Zhao, Jingjing Xu, Lei Li |  |
| 1165 |  |  [SAC³: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency](https://doi.org/10.18653/v1/2023.findings-emnlp.1032) |  | 0 | Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2)... | Bradley A. Malin, Jiaxin Zhang, Kamalika Das, Kumar Sricharan, Zhuohang Li |  |
| 1166 |  |  [Test-Time Self-Adaptive Small Language Models for Question Answering](https://doi.org/10.18653/v1/2023.findings-emnlp.1033) |  | 0 | Recent instruction-finetuned large language models (LMs) have achieved notable performances in various tasks, such as question-answering (QA). However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their... | Jinheon Baek, Jong Park, Soyeong Jeong, Sukmin Cho, Sung Ju Hwang |  |
| 1167 |  |  [ExpNote: Black-box Large Language Models are better Task Solvers with Experience Notebook](https://doi.org/10.18653/v1/2023.findings-emnlp.1034) |  | 0 | Black-box Large Language Models (LLMs) have shown great power in solving various tasks and are considered general problem solvers. However, LLMs still fail in many specific tasks although understand the task instruction. In this paper, we focus on the problem of boosting the ability of black-box... | Jun Zhao, Kang Liu, Shizhu He, Wangtao Sun, Xuanqing Yu |  |
| 1168 |  |  [Evaluating Parameter-Efficient Finetuning Approaches for Pre-trained Models on the Financial Domain](https://doi.org/10.18653/v1/2023.findings-emnlp.1035) |  | 0 | Large-scale language models with millions, billions, or trillions of trainable parameters are becoming increasingly popular. However, they risk becoming rapidly over-parameterized and the adaptation cost of fully fine-tuning them increases significantly. Storing them becomes progressively... | Cedric Lothritz, Isabella Olariu, Jacques Klein, Shohreh Haddadan, Siwen Guo, Tegawendé F. Bissyandé |  |
| 1169 |  |  [Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model](https://doi.org/10.18653/v1/2023.findings-emnlp.1036) |  | 0 | Augmenting pretrained language models with retrievers has shown promise in effectively solving common NLP problems, such as language modeling and question answering. In this paper, we evaluate the strengths and weaknesses of popular retriever-augmented language models, namely kNN-LM, REALM, DPR +... | Parishad BehnamGhader, Santiago Miret, Siva Reddy |  |
| 1170 |  |  [BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text](https://doi.org/10.18653/v1/2023.findings-emnlp.1037) |  | 0 | Real-world NLP applications often deal with nonstandard text (e.g., dialectal, informal, or misspelled text). However, language models like BERT deteriorate in the face of dialect variation or noise. How do we push BERT’s modeling capabilities to encompass nonstandard text? Fine-tuning helps, but... | Aarohi Srivastava, David Chiang |  |
| 1171 |  |  [Closed Boundary Learning for Classification Tasks with the Universum Class](https://doi.org/10.18653/v1/2023.findings-emnlp.1038) |  | 0 | The Universum class, often known as the \*other\* class or the\*miscellaneous\* class, is defined as a collection of samples that do not belong to any class of interest. It is a typical class that exists in many classification-based tasks in NLP, such as relation extraction, named entity... | Hanzhang Zhou, Kezhi Mao, Zijian Feng |  |
| 1172 |  |  [Revisiting Entropy Rate Constancy in Text](https://doi.org/10.18653/v1/2023.findings-emnlp.1039) |  | 0 | The uniform information density (UID) hypothesis states that humans tend to distribute information roughly evenly across an utterance or discourse. Early evidence in support of the UID hypothesis came from Genzel and Charniak (2002), which proposed an entropy rate constancy principle based on the... | Dan Klein, Nicholas Tomlin, Vivek Verma |  |
| 1173 |  |  [Calibrated Seq2seq Models for Efficient and Generalizable Ultra-fine Entity Typing](https://doi.org/10.18653/v1/2023.findings-emnlp.1040) |  | 0 | Ultra-fine entity typing plays a crucial role in information extraction by predicting fine-grained semantic types for entity mentions in text. However, this task poses significant challenges due to the massive number of entity types in the output space. The current state-of-the-art approaches,... | Adithya Pratapa, David R. Mortensen, Yanlin Feng |  |
| 1174 |  |  [Learning Semantic Role Labeling from Compatible Label Sequences](https://doi.org/10.18653/v1/2023.findings-emnlp.1041) |  | 0 | Semantic role labeling (SRL) has multiple disjoint label sets, e.g., VerbNet and PropBank. Creating these datasets is challenging, therefore a natural question is how to use each one to help the other. Prior work has shown that cross-task interaction helps, but only explored multitask learning so... | Ghazaleh Kazeminejad, Martha Palmer, Susan Windisch Brown, Tao Li, Vivek Srikumar |  |
| 1175 |  |  [QUADRo: Dataset and Models for QUestion-Answer Database Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.1042) |  | 0 | An effective approach to design automated Question Answering (QA) systems is to efficiently retrieve answers from pre-computed databases containing question/answer pairs. One of the main challenges to this design is the lack of training/testing data. Existing resources are limited in size and... | Alessandro Moschitti, Ivano Lauriola, Stefano Campese |  |
| 1176 |  |  [Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-emnlp.1043) |  | 0 | Pre-trained Language Models (PLMs) are trained on vast unlabeled data, rich in world knowledge. This fact has sparked the interest of the community in quantifying the amount of factual knowledge present in PLMs, as this explains their performance on downstream tasks, and potentially justifies their... | Christin Seifert, Jörg Schlötterer, Meijie Li, Osman Alperen Koras, Paul Youssef |  |
| 1177 |  |  [Is ChatGPT the ultimate Data Augmentation Algorithm?](https://doi.org/10.18653/v1/2023.findings-emnlp.1044) |  | 0 | In the aftermath of GPT-3.5, commonly known as ChatGPT, research have attempted to assess its capacity for lowering annotation cost, either by doing zero-shot learning, generating new data, or replacing human annotators. Some studies have also investigated its use for data augmentation (DA), but... | Frédéric Piedboeuf, Philippe Langlais |  |
| 1178 |  |  [Enhanced Simultaneous Machine Translation with Word-level Policies](https://doi.org/10.18653/v1/2023.findings-emnlp.1045) |  | 0 | Recent years have seen remarkable advances in the field of Simultaneous Machine Translation (SiMT) due to the introduction of innovative policies that dictate whether to READ or WRITE at each step of the translation process. However, a common assumption in many existing studies is that operations... | Hankyu Cho, Kang Kim |  |
| 1179 |  |  [Causal Intervention-based Few-Shot Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-emnlp.1046) |  | 0 | Few-shot named entity recognition (NER) systems aim to recognize new classes of entities with limited labeled samples. However, these systems face a significant challenge of overfitting compared to tasks with abundant samples. This overfitting is mainly caused by the spurious correlation resulting... | Chunping Ouyang, Yongbin Liu, Zhen Yang |  |
| 1180 |  |  [TADI: Topic-aware Attention and Powerful Dual-encoder Interaction for Recall in News Recommendation](https://doi.org/10.18653/v1/2023.findings-emnlp.1047) |  | 0 | News recommendation is one of the widest commercialization in natural language processing research area, which aims to recommend news according to user interests. New recall plays an important role in news recommendation. It is to recall candidates from a very large news database. Recent researches... | Junxiang Jiang |  |
| 1181 |  |  [Unveiling the Power of Argument Arrangement in Online Persuasive Discussions](https://doi.org/10.18653/v1/2023.findings-emnlp.1048) |  | 0 | Previous research on argumentation in online discussions has largely focused on examining individual comments and neglected the interactive nature of discussions. In line with previous work, we represent individual comments as sequences of semantic argumentative unit types. However, because it is... | Benno Stein, Johannes Kiesel, Khalid Al Khatib, Nailia Mirzakhmedova |  |
| 1182 |  |  [FFAEval: Evaluating Dialogue System via Free-For-All Ranking](https://doi.org/10.18653/v1/2023.findings-emnlp.1049) |  | 0 | Evaluating open-domain dialogue systems is currently an open question. Automatic evaluation metrics have shown poor correlation with human assessment in dialogue generation tasks. Human evaluation, which involves annotators for multi-dimension scoring, is trustworthy but time-consuming. In this... | Jie Tang, Jifan Yu, Jing Zhang, Juanzi Li, Xiaohan Zhang, Zeyao Ma, Zijun Yao |  |
| 1183 |  |  [Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-emnlp.1050) |  | 0 | The conversational machine reading comprehension (CMRC) task aims to answer questions in conversations, which has been a hot research topic in recent years because of its wide applications. However, existing CMRC benchmarks in which each conversation is assigned a static passage are inconsistent... | Baoyuan Wang, Hongguang Li, Jia Li, Jianfeng Liu, Jiaxing Zhang, Junqing He, Nuo Chen, Qi Yang, Ruyi Gan, Xinshi Lin, Yinan Bao |  |
| 1184 |  |  [VER: Unifying Verbalizing Entities and Relations](https://doi.org/10.18653/v1/2023.findings-emnlp.1051) |  | 0 | Entities and relationships between entities are vital in the real world. Essentially, we understand the world by understanding entities and relations. For instance, to understand a field, e.g., computer science, we need to understand the relevant concepts, e.g., machine learning, and the... | Jie Huang, Kevin Chang |  |
| 1185 |  |  [The Linearity of the Effect of Surprisal on Reading Times across Languages](https://doi.org/10.18653/v1/2023.findings-emnlp.1052) |  | 0 | In psycholinguistics, surprisal theory posits that the amount of online processing effort expended by a human comprehender per word positively correlates with the surprisal of that word given its preceding context. In addition to this overall correlation, more importantly, the specific quantitative... | Jason Chon, Richard Futrell, Tianran Liu, Weijie Xu |  |
| 1186 |  |  [Adversarial Text Generation by Search and Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.1053) |  | 0 | Recent research has shown that evaluating the robustness of natural language processing models using textual attack methods is significant. However, most existing text attack methods only use heuristic replacement strategies or language models to generate replacement words at the word level. The... | Bingkang Shi, Dehan Kong, Guoyi Li, Honglei Lyu, Longtao Huang, Xiaodan Zhang, Yulei Wu, Zongzhen Liu |  |
| 1187 |  |  [Measuring Pointwise \mathcalV-Usable Information In-Context-ly](https://doi.org/10.18653/v1/2023.findings-emnlp.1054) |  | 0 | In-context learning (ICL) is a new learning paradigm that has gained popularity along with the development of large language models. In this work, we adapt a recently proposed hardness metric, pointwise 𝒱-usable information (PVI), to an in-context version (in-context PVI). Compared to the original... | Danielle S. Bitterman, Guergana Savova, Iryna Gurevych, Shan Chen, Sheng Lu, Yingya Li |  |
| 1188 |  |  [SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities](https://doi.org/10.18653/v1/2023.findings-emnlp.1055) |  | 0 | Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge... | Dong Zhang, Jun Zhan, Pengyu Wang, Shimin Li, Xin Zhang, Xipeng Qiu, Yaqian Zhou |  |
| 1189 |  |  [Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance via Probability Calibration](https://doi.org/10.18653/v1/2023.findings-emnlp.1056) |  | 0 | Pretrained multilingual encoder models can directly perform zero-shot multilingual tasks or linguistic probing by reformulating the input examples into cloze-style prompts. This is accomplished by predicting the probabilities of the label words at the masked token position, without requiring any... | Ercong Nie, Helmut Schmid, Hinrich Schütze |  |
| 1190 |  |  [A Thorough Examination on Zero-shot Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.1057) |  | 0 | Recent years have witnessed the significant advance in dense retrieval (DR) based on powerful pre-trained language models (PLM). DR models have achieved excellent performance in several benchmark datasets, while they are shown to be not as competitive as traditional sparse retrieval models (e.g.,... | Haifeng Wang, Hua Wu, JiRong Wen, Jing Liu, Qifei Wu, Ruiyang Ren, Xin Zhao, Yingqi Qu, Yuchen Ding |  |
| 1191 |  |  [Contrastive Pre-training for Personalized Expert Finding](https://doi.org/10.18653/v1/2023.findings-emnlp.1058) |  | 0 | Expert finding could help route questions to potential suitable users to answer in Community Question Answering (CQA) platforms. Hence it is essential to learn accurate representations of experts and questions according to the question text articles. Recently the pre-training and fine-tuning... | Hongtao Liu, Qing Yang, Qiyao Peng, Wenjun Wang, Zhepeng Lv |  |
| 1192 |  |  [Mitigating Intrinsic Named Entity-Related Hallucinations of Abstractive Text Summarization](https://doi.org/10.18653/v1/2023.findings-emnlp.1059) |  | 0 | Abstractive text summarization (ATS) is both important and challenging. Recent studies have shown that ATS still faces various forms of hallucination. Our study also indicates that a significant portion of hallucinations is named entity-related. They might appear in different forms, such as... | Christy Jie Liang, Jianbin Shen, Junyu Xuan |  |
| 1193 |  |  [Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning](https://doi.org/10.18653/v1/2023.findings-emnlp.1060) |  | 0 | Large Language models (LLMs) possess the capability to engage In-context Learning (ICL) by leveraging a few demonstrations pertaining to a new downstream task as conditions. However, this particular learning paradigm suffers from high instability stemming from substantial variances induced by... | Hongfu Liu, Ye Wang |  |
| 1194 |  |  [Frontmatter](https://aclanthology.org/2023.emnlp-main.0) |  | 0 |  |  |  |
| 1195 |  |  [IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions](https://doi.org/10.18653/v1/2023.emnlp-main.1) |  | 0 | Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for open-domain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making... | Meng Han, Ruofei Lai, Saijiang Shi, Xinyu Zhang, Yongkang Wu, Yuanhang Ren, Zhao Cao, Zhebin Zhang |  |
| 1196 |  |  [Absolute Position Embedding Learns Sinusoid-like Waves for Attention Based on Relative Position](https://doi.org/10.18653/v1/2023.emnlp-main.2) |  | 0 | Attention weight is a clue to interpret how a Transformer-based model makes an inference. In some attention heads, the attention focuses on the neighbors of each token. This allows the output vector of each token to depend on the surrounding tokens and contributes to make the inference... | Takuya Matsuzaki, Yuji Yamamoto |  |
| 1197 |  |  [Chinese Lexical Substitution: Dataset and Method](https://doi.org/10.18653/v1/2023.emnlp-main.3) |  | 0 | Existing lexical substitution (LS) benchmarks were collected by asking human annotators to think of substitutes from memory, resulting in benchmarks with limited coverage and relatively small scales. To overcome this problem, we propose a novel annotation method to construct an LS dataset based on... | Jipeng Qiang, Kang Liu, Xiaocheng Hu, Xiaoye Ouyang, Yi Zhu, Ying Li, Yun Li, YunHao Yuan |  |
| 1198 |  |  [Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting](https://doi.org/10.18653/v1/2023.emnlp-main.4) |  | 0 | Automatic response forecasting for news media plays a crucial role in enabling content producers to efficiently predict the impact of news releases and prevent unexpected negative outcomes such as social conflict and moral injury. To effectively forecast responses, it is essential to develop... | ChengXiang Zhai, Chenkai Sun, Heng Ji, Hou Pong Chan, Jinning Li, Tarek F. Abdelzaher, Yi Ren Fung |  |
| 1199 |  |  [Fine-grained Conversational Decoding via Isotropic and Proximal Search](https://doi.org/10.18653/v1/2023.emnlp-main.5) |  | 0 | General-purpose text decoding approaches are usually adopted for dialogue response generation. Although the quality of the generated responses can be improved with dialogue-specific encoding methods, conversational decoding methods are still under-explored. Inspired by SimDRC that a good dialogue... | Han Wu, Linqi Song, Qiling Xu, Yuxuan Yao |  |
| 1200 |  |  [Holistic Inter-Annotator Agreement and Corpus Coherence Estimation in a Large-scale Multilingual Annotation Campaign](https://doi.org/10.18653/v1/2023.emnlp-main.6) |  | 0 | In this paper we report on the complexity of persuasion technique annotation in the context of a large multilingual annotation campaign involving 6 languages and approximately 40 annotators. We highlight the techniques that appear to be difficult for humans to annotate and elaborate on our findings... | Jakub Piskorski, Nicolas Stefanovitch |  |
| 1201 |  |  [PHD: Pixel-Based Language Modeling of Historical Documents](https://doi.org/10.18653/v1/2023.emnlp-main.7) |  | 0 | The digitisation of historical documents has provided historians with unprecedented research opportunities. Yet, the conventional approach to analysing historical documents involves converting them from images to text using OCR, a process that overlooks the potential benefits of treating them as... | Desmond Elliott, Isabelle Augenstein, Nadav Borenstein, Phillip Rust |  |
| 1202 |  |  [Primacy Effect of ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.8) |  | 0 | Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The... | Bryan Hooi, Muhao Chen, Yiwei Wang, Yujun Cai, Yuxuan Liang |  |
| 1203 |  |  [Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension](https://doi.org/10.18653/v1/2023.emnlp-main.9) |  | 0 | To precisely evaluate a language model’s capability for logical reading comprehension, we present a dataset for testing the understanding of the rationale behind critical reasoning. For questions taken from an existing multiple-choice logical reading comprehension dataset, we crowdsource rationale... | Akira Kawabata, Saku Sugawara |  |
| 1204 |  |  [Evaluating and Modeling Attribution for Cross-Lingual Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.10) |  | 0 | Trustworthy answer content is abundant in many high-resource languages and is instantly accessible through question answering systems — yet this content can be hard to access for those that do not speak these languages. The leap forward in cross-lingual modeling quality offered by generative... | Benjamin Muller, John Wieting, Jonathan H. Clark, Jonathan Herzig, Livio Soares, Roee Aharoni, Sebastian Ruder, Tom Kwiatkowski, Xinyi Wang |  |
| 1205 |  |  [Better Quality Pre-training Data and T5 Models for African Languages](https://doi.org/10.18653/v1/2023.emnlp-main.11) |  | 0 | In this study, we highlight the importance of enhancing the quality of pretraining data in multilingual language models. Existing web crawls have demonstrated quality issues, particularly in the context of low-resource languages. Consequently, we introduce a new multilingual pretraining corpus for... | Abraham Toluwase Owodunni, Akintunde Oladipo, David Ifeoluwa Adelani, Jimmy Lin, Mofetoluwa Adeyemi, Odunayo Ogundepo, Orevaoghene Ahia |  |
| 1206 |  |  [Sparse Universal Transformer](https://doi.org/10.18653/v1/2023.emnlp-main.12) |  | 0 | The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers and is Turing-complete under certain assumptions. Empirical evidence also shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The... | Aaron C. Courville, Chuang Gan, Shawn Tan, Yikang Shen, Zhenfang Chen |  |
| 1207 |  |  [Theory of Mind for Multi-Agent Collaboration via Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.13) |  | 0 | While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference... | Charles Lewis, Dana Hughes, Huao Li, Joseph Campbell, Katia P. Sycara, Simon Stepputtis, Yu Quan Chong |  |
| 1208 |  |  [Establishing Trustworthiness: Rethinking Tasks and Model Evaluation](https://doi.org/10.18653/v1/2023.emnlp-main.14) |  | 0 | Language understanding is a multi-faceted cognitive capability, which the Natural Language Processing (NLP) community has striven to model computationally for decades. Traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and... | Barbara Plank, Leon WeberGenzel, Max MüllerEberstein, Rob van der Goot, Robert Litschko |  |
| 1209 |  |  [Let's Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought](https://doi.org/10.18653/v1/2023.emnlp-main.15) |  | 0 | Despite exciting recent results showing vision-language systems’ capacity to reason about images using natural language, their capacity for video reasoning remains underexplored. We motivate framing video reasoning as the sequential understanding of a small number of keyframes, thereby leveraging... | Alex Mei, Andy Ouyang, Chinmay Sonar, Daniel Rose, Michael Saxon, Ryan He, Vaishnavi Himakunthala, William Yang Wang, Yujie Lu |  |
| 1210 |  |  [GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP](https://doi.org/10.18653/v1/2023.emnlp-main.16) |  | 0 | ChatGPT’s emergence heralds a transformative phase in NLP, particularly demonstrated through its excellent performance on many English benchmarks. However, the model’s efficacy across diverse linguistic contexts remains largely uncharted territory. This work aims to bridge this knowledge gap, with... | Abdul Waheed, El Moatez Billah Nagoudi, Md Tawkat Islam Khondaker, Muhammad AbdulMageed |  |
| 1211 |  |  [Dual-Channel Span for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.17) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) is one of the compound tasks of fine-grained aspect-based sentiment analysis (ABSA), aiming at extracting the triplets of aspect terms, corresponding opinion terms and the associated sentiment orientation. Recent efforts in exploiting span-level semantic... | Kai Zhang, Pan Li, Ping Li |  |
| 1212 |  |  [Cultural Concept Adaptation on Multimodal Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.18) |  | 0 | Developing cultural adaptation methods is important, which can improve the model performance on the low-resource ones and provide more equitable opportunities for everyone to benefit from advanced technology. Past methods primarily focused on multilingual and multimodal capabilities, and the... | Yin Zhang, Zhi Li |  |
| 1213 |  |  [Understanding Compositional Data Augmentation in Typologically Diverse Morphological Inflection](https://doi.org/10.18653/v1/2023.emnlp-main.19) |  | 0 | Data augmentation techniques are widely used in low-resource automatic morphological inflection to address the issue of data sparsity. However, the full implications of these techniques remain poorly understood. In this study, we aim to shed light on the theoretical aspects of the data augmentation... | Farhan Samir, Miikka Silfverberg |  |
| 1214 |  |  [Evaluating Object Hallucination in Large Vision-Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.20) |  | 0 | Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently proposed by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that they suffer from... | JiRong Wen, Jinpeng Wang, Kun Zhou, Wayne Xin Zhao, Yifan Du, Yifan Li |  |
| 1215 |  |  [Event Ontology Completion with Hierarchical Structure Evolution Networks](https://doi.org/10.18653/v1/2023.emnlp-main.21) |  | 0 | Traditional event detection methods require predefined event schemas. However, manually defining event schemas is expensive and the coverage of schemas is limited. To this end, some works study the event type induction (ETI) task, which discovers new event types via clustering. However, the setting... | Huaijun Li, Jiexin Xu, Jun Zhao, Kang Liu, Pengfei Cao, Xiaojian Jiang, Yubo Chen, Yupu Hao |  |
| 1216 |  |  [Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients](https://doi.org/10.18653/v1/2023.emnlp-main.22) |  | 0 | Fine-tuning all parameters of large language models (LLMs) requires significant computational resources and is time-consuming. Recent parameter-efficient tuning methods such as Adapter tuning, Prefix tuning, and LoRA allow for updating a small subset of parameters in large language models. However,... | Chengqing Zong, Feihu Jin, Jiajun Zhang |  |
| 1217 |  |  [Discourse Structures Guided Fine-grained Propaganda Identification](https://doi.org/10.18653/v1/2023.emnlp-main.23) |  | 0 | Propaganda is a form of deceptive narratives that instigate or mislead the public, usually with a political purpose. In this paper, we aim to identify propaganda in political news at two fine-grained levels: sentence-level and token-level. We observe that propaganda content is more likely to be... | Ruihong Huang, Yuanyuan Lei |  |
| 1218 |  |  [CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.24) |  | 0 | While many languages possess processes of joining two or more words to create compound words, previous studies have been typically limited only to languages with excessively productive compound formation (e.g., German, Dutch) and there is no public dataset containing compound and non-compound words... | Benjamin Minixhofer, Ivan Vulic, Jonas Pfeiffer |  |
| 1219 |  |  [Improving Image Captioning via Predicting Structured Concepts](https://doi.org/10.18653/v1/2023.emnlp-main.25) |  | 0 | Having the difficulty of solving the semantic gap between images and texts for the image captioning task, conventional studies in this area paid some attention to treating semantic concepts as a bridge between the two modalities and improved captioning performance accordingly. Although promising... | Ting Wang, Weidong Chen, Yan Song, Yuanhe Tian, Zhendong Mao |  |
| 1220 |  |  [GATITOS: Using a New Multilingual Lexicon for Low-resource Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.26) |  | 0 | Modern machine translation models and language models are able to translate without having been trained on parallel data, greatly expanding the set of languages that they can serve. However, these models still struggle in a variety of predictable ways, a problem that cannot be overcome without at... | Alexander Jones, Isaac Caswell, Ishank Saxena, Orhan Firat |  |
| 1221 |  |  [Continually Improving Extractive QA via Human Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.27) |  | 0 | We study continually improving an extractive question answering (QA) system via human user feedback. We design and deploy an iterative approach, where information-seeking users ask questions, receive model-predicted answers, and provide feedback. We conduct experiments involving thousands of user... | Eunsol Choi, Ge Gao, HungTing Chen, Yoav Artzi |  |
| 1222 |  |  [Using Interpretation Methods for Model Enhancement](https://doi.org/10.18653/v1/2023.emnlp-main.28) |  | 0 | In the age of neural natural language processing, there are plenty of works trying to derive interpretations of neural models. Intuitively, when gold rationales exist during training, one can additionally train the model to match its interpretation with the rationales. However, this intuitive idea... | Chengyue Jiang, Kewei Tu, Zhuo Chen |  |
| 1223 |  |  [An Expression Tree Decoding Strategy for Mathematical Equation Generation](https://doi.org/10.18653/v1/2023.emnlp-main.29) |  | 0 | Generating mathematical equations from natural language requires an accurate understanding of the relations among math expressions. Existing approaches can be broadly categorized into token-level and expression-level generation. The former treats equations as a mathematical language, sequentially... | Qingpeng Nong, Weiming Lu, Wenqi Zhang, Yanna Ma, Yongliang Shen, Zeqi Tan |  |
| 1224 |  |  [Bootstrapping Small & High Performance Language Models with Unmasking-Removal Training Policy](https://doi.org/10.18653/v1/2023.emnlp-main.30) |  | 0 | BabyBERTa, a language model trained on small-scale child-directed speech while none of the words are unmasked during training, has been shown to achieve a level of grammaticality comparable to that of RoBERTa-base, which is trained on 6,000 times more words and 15 times more parameters. Relying on... | Dan Roth, Elior Sulem, Insup Lee, Yahan Yang |  |
| 1225 |  |  [Diversity Enhanced Narrative Question Generation for Storybooks](https://doi.org/10.18653/v1/2023.emnlp-main.31) |  | 0 | Question generation (QG) from a given context can enhance comprehension, engagement, assessment, and overall efficacy in learning or conversational environments. Despite recent advancements in QG, the challenge of enhancing or measuring the diversity of generated questions often remains... | Hokeun Yoon, JinYeong Bak |  |
| 1226 |  |  [Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification](https://doi.org/10.18653/v1/2023.emnlp-main.32) |  | 0 | Recent advances in weakly supervised text classification mostly focus on designing sophisticated methods to turn high-level human heuristics into quality pseudo-labels. In this paper, we revisit the seed matching-based method, which is arguably the simplest way to generate pseudo-labels, and show... | Chengyu Dong, Jingbo Shang, Zihan Wang |  |
| 1227 |  |  [How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.33) |  | 0 | Our investigation into the Affective Reasoning in Conversation (ARC) task highlights the challenge of causal discrimination. Almost all existing models, including large language models (LLMs), excel at capturing semantic correlations within utterance embeddings but fall short in determining the... | Hang Chen, Jing Luo, Wenjing Zhu, Xinyu Yang |  |
| 1228 |  |  [Compressing and Debiasing Vision-Language Pre-Trained Models for Visual Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.34) |  | 0 | Despite the excellent performance of vision-language pre-trained models (VLPs) on conventional VQA task, they still suffer from two problems: First, VLPs tend to rely on language biases in datasets and fail to generalize to out-of-distribution (OOD) data. Second, they are inefficient in terms of... | Peng Fu, Qingyi Si, Weiping Wang, Yanan Cao, Yuanxin Liu, Zheng Lin |  |
| 1229 |  |  [Selectively Answering Ambiguous Questions](https://doi.org/10.18653/v1/2023.emnlp-main.35) |  | 0 | Trustworthy language models should abstain from answering questions when they do not know the answer. However, the answer to a question can be unknown for a variety of reasons. Prior research has focused on the case in which the question is clear and the answer is unambiguous but possibly unknown.... | Bhuwan Dhingra, Daniel Gillick, Jacob Eisenstein, Jeremy R. Cole, Julian Eisenschlos, Michael J. Q. Zhang |  |
| 1230 |  |  [Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.36) |  | 0 | Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts. In this paper, we develop an approach to use in-context learning (ICL) with large language models (LLMs) for TKG forecasting. Our extensive evaluation compares diverse... | DongHo Lee, Fred Morstatter, Jay Pujara, Kian Ahrabian, Woojeong Jin |  |
| 1231 |  |  [Knowledge Graph Compression Enhances Diverse Commonsense Generation](https://doi.org/10.18653/v1/2023.emnlp-main.37) |  | 0 | Generating commonsense explanations requires reasoning about commonsense knowledge beyond what is explicitly mentioned in the context. Existing models use commonsense knowledge graphs such as ConceptNet to extract a subgraph of relevant knowledge pertaining to concepts in the input. However, due to... | Eunjeong Hwang, Tengfei Ma, Vered Shwartz, Veronika Thost |  |
| 1232 |  |  [Pragmatic Reasoning Unlocks Quantifier Semantics for Foundation Models](https://doi.org/10.18653/v1/2023.emnlp-main.38) |  | 0 | Generalized quantifiers (e.g., few, most) are used to indicate the proportions predicates satisfy (for example, some apples are red). One way to interpret quantifier semantics is to explicitly bind these satisfactions with percentage scopes (e.g., 30%-40% of apples are red). This approach can be... | Rakesh R. Menon, Sayan Ghosh, Shashank Srivastava, Yiyuan Li |  |
| 1233 |  |  [LLM-FP4: 4-Bit Floating-Point Quantized Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.39) |  | 0 | We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits. Compared to... | KwangTing Cheng, Pingcheng Dong, ShihYang Liu, Xijie Huang, Zechun Liu |  |
| 1234 |  |  [Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers](https://doi.org/10.18653/v1/2023.emnlp-main.40) |  | 0 | Abstracts derived from biomedical literature possess distinct domain-specific characteristics, including specialised writing styles and biomedical terminologies, which necessitate a deep understanding of the related literature. As a result, existing language models struggle to generate technical... | Chen Tang, Chenghua Lin, Shun Wang, Tomas Goldsack |  |
| 1235 |  |  [Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting](https://doi.org/10.18653/v1/2023.emnlp-main.41) |  | 0 | Recent work has shown how to prompt large language models with explanations to obtain strong performance on textual reasoning tasks, i.e., the chain-of-thought paradigm. However, subtly different explanations can yield widely varying downstream task accuracy. Explanations that have not been “tuned”... | Greg Durrett, Xi Ye |  |
| 1236 |  |  [HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.42) |  | 0 | Hallucinations in machine translation are translations that contain information completely unrelated to the input. Omissions are translations that do not include some of the input information. While both cases tend to be catastrophic errors undermining user trust, annotated data with these types of... | Christophe Ropers, Cynthia Gao, David Dale, Elahe Kalbassi, Elena Voita, Janice Lam, Loïc Barrault, Marta R. Costajussà, Prangthip Hansanti |  |
| 1237 |  |  [Gradient-based Gradual Pruning for Language-Specific Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.43) |  | 0 | Multilingual neural machine translation (MNMT) offers the convenience of translating between multiple languages with a single model. However, MNMT often suffers from performance degradation in high-resource languages compared to bilingual counterparts. This degradation is commonly attributed to... | Dan He, Marco Turchi, MinhQuang Pham, ThanhLe Ha |  |
| 1238 |  |  [LLM-powered Data Augmentation for Enhanced Cross-lingual Performance](https://doi.org/10.18653/v1/2023.emnlp-main.44) |  | 0 | This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to... | Alham Fikri Aji, Chenxi Whitehouse, Monojit Choudhury |  |
| 1239 |  |  [Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.45) |  | 0 | Implicit Discourse Relation Recognition (IDRR), which infers discourse relations without the help of explicit connectives, is still a crucial and challenging task for discourse parsing. Recent works tend to exploit the hierarchical structure information from the annotated senses, which demonstrate... | Chenxu Wang, Mu Huang, Ping Jian |  |
| 1240 |  |  [VLIS: Unimodal Language Models Guide Multimodal Language Generation](https://doi.org/10.18653/v1/2023.emnlp-main.46) |  | 0 | Multimodal language generation, which leverages the synergy of language and vision, is a rapidly expanding field. However, existing vision-language models face challenges in tasks that require complex linguistic understanding. To address this issue, we introduce Visual-Language models as Importance... | Jiwan Chung, Youngjae Yu |  |
| 1241 |  |  [Conceptual structure coheres in human cognition but not in large language models](https://doi.org/10.18653/v1/2023.emnlp-main.47) |  | 0 | Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior... | Kushin Mukherjee, Lisa Padua, Siddharth Suresh, Timothy T. Rogers, WeiChun Huang, Xizheng Yu |  |
| 1242 |  |  [Towards LLM-driven Dialogue State Tracking](https://doi.org/10.18653/v1/2023.emnlp-main.48) |  | 0 | Dialogue State Tracking (DST) is of paramount importance in ensuring accurate tracking of user goals and system actions within task-oriented dialogue systems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT has sparked considerable interest in assessing their efficacy across... | Bo Liu, Liming Zhan, XiaoMing Wu, Yujie Feng, Zexin Lu |  |
| 1243 |  |  [Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.49) |  | 0 | Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich information from multiple sources (\*e.g.,\* language, video, and audio), the potential sentiment-irrelevant and conflicting information across modalities may hinder the performance from being further improved. To... | Guanghao Yin, Haoyu Zhang, Kejun Liu, Tianshu Yu, Yu Wang, Yuanyuan Liu |  |
| 1244 |  |  [Multitask Multimodal Prompted Training for Interactive Embodied Task Completion](https://doi.org/10.18653/v1/2023.emnlp-main.50) |  | 0 | Interactive and embodied tasks pose at least two fundamental challenges to existing Vision & Language (VL) models, including 1) grounding language in trajectories of actions and observations, and 2) referential disambiguation. To tackle these challenges, we propose an Embodied MultiModal Agent... | Alessandro Suglia, Amit Parekh, Arash Eshghi, Bhathiya Hemanthage, Georgios Pantazopoulos, Ioannis Konstas, Malvina Nikandrou, Oliver Lemon, Verena Rieser |  |
| 1245 |  |  [We're Afraid Language Models Aren't Modeling Ambiguity](https://doi.org/10.18653/v1/2023.emnlp-main.51) |  | 0 | Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models are increasingly employed as dialogue interfaces and... | Alane Suhr, Alexander Koller, Alisa Liu, Julian Michael, Noah A. Smith, Peter West, Swabha Swayamdipta, Yejin Choi, Zhaofeng Wu |  |
| 1246 |  |  [Linear-Time Modeling of Linguistic Structure: An Order-Theoretic Perspective](https://doi.org/10.18653/v1/2023.emnlp-main.52) |  | 0 | Tasks that model the relation between pairs of tokens in a string are a vital part of understanding natural language. Such tasks, in general, require exhaustive pair-wise comparisons of tokens, thus having a quadratic runtime complexity in the length of the string. We show that these exhaustive... | Afra Amini, Mrinmaya Sachan, Ryan Cotterell, Tianyu Liu |  |
| 1247 |  |  [GEMINI: Controlling The Sentence-Level Summary Style in Abstractive Text Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.53) |  | 0 | Human experts write summaries using different techniques, including extracting a sentence from the document and rewriting it, or fusing various information from the document to abstract it. These techniques are flexible and thus difficult to be imitated by any single method. To address this issue,... | Guangsheng Bao, Yue Zhang, Zebin Ou |  |
| 1248 |  |  [Fidelity-Enriched Contrastive Search: Reconciling the Faithfulness-Diversity Trade-Off in Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.54) |  | 0 | In this paper, we address the hallucination problem commonly found in natural language generation tasks. Language models often generate fluent and convincing content but can lack consistency with the provided source, resulting in potential inaccuracies. We propose a new decoding method called... | ChengKuang Wu, ChungChi Chen, HsinHsi Chen, WeiLin Chen |  |
| 1249 |  |  [Analyzing Norm Violations in Live-Stream Chat](https://doi.org/10.18653/v1/2023.emnlp-main.55) |  | 0 | Toxic language, such as hate speech, can deter users from participating in online communities and enjoying popular platforms. Previous approaches to detecting toxic language and norm violations have been primarily concerned with conversations from online forums and social media, such as Reddit and... | Chan Young Park, DongHo Lee, Hyundong Cho, Jay Pujara, Jihyung Moon, Jonathan May, Minwoo Kim, Sungjoon Park, Woojeong Jin |  |
| 1250 |  |  [Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality](https://doi.org/10.18653/v1/2023.emnlp-main.56) |  | 0 | Contrastively trained vision-language models have achieved remarkable progress in vision and language representation learning. However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations.... | Harman Singh, Jingfei Du, Mengjiao Wang, Pengchuan Zhang, Qifan Wang, Wenhan Xiong, Yu Chen |  |
| 1251 |  |  [Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms](https://doi.org/10.18653/v1/2023.emnlp-main.57) |  | 0 | Commonsense norms are defeasible by context: reading books is usually great, but not when driving a car. While contexts can be explicitly described in language, in embodied scenarios, contexts are often provided visually. This type of visually grounded reasoning about defeasible commonsense norms... | Jack Hessel, Jiwan Chung, Junhyeok Kim, Liwei Jiang, Seungju Han, Yejin Choi, Yejin Son, Youngjae Yu |  |
| 1252 |  |  [Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus](https://doi.org/10.18653/v1/2023.emnlp-main.58) |  | 0 | Large Language Models (LLMs) have gained significant popularity for their impressive performance across diverse fields. However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications. Existing works for detecting... | Cheng Deng, Chenghu Zhou, Lin Qiu, Luoyi Fu, Qipeng Guo, Tianhang Zhang, Xinbing Wang, Yue Zhang, Zheng Zhang |  |
| 1253 |  |  [FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.59) |  | 0 | Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains.... | Shangbin Feng, Vidhisha Balachandran, Yulia Tsvetkov, Yuyang Bai |  |
| 1254 |  |  [Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation](https://doi.org/10.18653/v1/2023.emnlp-main.60) |  | 0 | Modern NLP models are often trained over large untrusted datasets, raising the potential for a malicious adversary to compromise model behaviour. For instance, backdoors can be implanted through crafting training instances with a specific textual trigger and a target label. This paper posits that... | Benjamin I. P. Rubinstein, Jun Wang, Qiongkai Xu, Trevor Cohn, Xuanli He |  |
| 1255 |  |  [Symbol tuning improves in-context learning in language models](https://doi.org/10.18653/v1/2023.emnlp-main.61) |  | 0 | We present symbol tuning - finetuning language models on in-context input-label pairs where natural language labels (e.g., “positive/negative sentiment”) are replaced with arbitrary symbols (e.g., “foo/bar”). Symbol tuning leverages the intuition that when a model cannot use instructions or natural... | Andrew K. Lampinen, Da Huang, Denny Zhou, Jerry W. Wei, Le Hou, Quoc V. Le, Tengyu Ma, Xiangning Chen, Xinyun Chen, Yi Tay, Yifeng Lu |  |
| 1256 |  |  [The neural dynamics of word recognition and integration](https://doi.org/10.18653/v1/2023.emnlp-main.62) |  | 0 | Listeners recognize and integrate words in rapid and noisy everyday speech by combining expectations about upcoming content with incremental sensory evidence. We present a computational model of word recognition which formalizes this perceptual process in Bayesian decision theory. We fit this model... | Jon Gauthier, Roger Levy |  |
| 1257 |  |  [Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.63) |  | 0 | Questions in open-domain question answering are often ambiguous, allowing multiple interpretations. One approach to handling them is to identify all possible interpretations of the ambiguous question (AQ) and to generate a long-form answer addressing them all, as suggested by Stelmakh et al.,... | Byeongguk Jeon, Gangwoo Kim, Jaewoo Kang, Joonsuk Park, Sungdong Kim |  |
| 1258 |  |  [Incorporating Worker Perspectives into MTurk Annotation Practices for NLP](https://doi.org/10.18653/v1/2023.emnlp-main.64) |  | 0 | Current practices regarding data collection for natural language processing on Amazon Mechanical Turk (MTurk) often rely on a combination of studies on data quality and heuristics shared among NLP researchers. However, without considering the perspectives of MTurk workers, these approaches are... | Dan Klein, Eve Fleisig, Olivia Huang |  |
| 1259 |  |  [Predict the Future from the Past? On the Temporal Data Distribution Shift in Financial Sentiment Classifications](https://doi.org/10.18653/v1/2023.emnlp-main.65) |  | 0 | Temporal data distribution shift is prevalent in the financial text. How can a financial sentiment analysis system be trained in a volatile market environment that can accurately infer sentiment and be robust to temporal data distribution shifts? In this paper, we conduct an empirical study on the... | Chenxi Hu, Yi Yang, Yue Guo |  |
| 1260 |  |  [Look-back Decoding for Open-Ended Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.66) |  | 0 | Given a prefix (context), open-ended generation aims to decode texts that are coherent, which do not abruptly drift from previous topics, and informative, which do not suffer from undesired repetitions. In this paper, we propose Look-back, an improved decoding algorithm that leverages the... | Asli Celikyilmaz, Chunting Zhou, Nan Xu, Xuezhe Ma |  |
| 1261 |  |  [Large Language Models Can Self-Improve](https://doi.org/10.18653/v1/2023.emnlp-main.67) |  | 0 | Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also... | Hongkun Yu, Jiawei Han, Jiaxin Huang, Le Hou, Shixiang Gu, Xuezhi Wang, Yuexin Wu |  |
| 1262 |  |  [CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://doi.org/10.18653/v1/2023.emnlp-main.68) |  | 0 | Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for... | Akhilesh Gotmare, Hung Le, Junnan Li, Nghi D. Q. Bui, Steven C. H. Hoi, Yue Wang |  |
| 1263 |  |  [Structural generalization in COGS: Supertagging is (almost) all you need](https://doi.org/10.18653/v1/2023.emnlp-main.69) |  | 0 | In many Natural Language Processing applications, neural networks have been found to fail to generalize on out-of-distribution examples. In particular, several recent semantic parsing datasets have put forward important limitations of neural networks in cases where compositional generalization is... | Alban Petit, Caio F. Corro, François Yvon |  |
| 1264 |  |  [BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations](https://doi.org/10.18653/v1/2023.emnlp-main.70) |  | 0 | Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal... | Jinhua Zhu, Kaiyuan Gao, Kehan Wu, Lijun Wu, Qizhi Pei, Rui Yan, Wei Zhang, Yingce Xia |  |
| 1265 |  |  [Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings](https://doi.org/10.18653/v1/2023.emnlp-main.71) |  | 0 | Cross-lingual transfer learning is an important property of multilingual large language models (LLMs). But how do LLMs represent relationships between languages? Every language model has an input layer that maps tokens to vectors. This ubiquitous layer of language models is often overlooked. We... | Andrea W. WenYi, David Mimno |  |
| 1266 |  |  [Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation](https://doi.org/10.18653/v1/2023.emnlp-main.72) |  | 0 | Target-oriented dialogue systems, designed to proactively steer conversations toward predefined targets or accomplish specific system-side goals, are an exciting area in conversational AI. In this work, by formulating a <dialogue act, topic> pair as the conversation target, we explore a novel... | Chak Tou Leong, Dongding Lin, Jian Wang, Wenjie Li, Yi Cheng |  |
| 1267 |  |  [SeqXGPT: Sentence-Level AI-Generated Text Detection](https://doi.org/10.18653/v1/2023.emnlp-main.73) |  | 0 | Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs. Therefore, it is important to build strong AI-generated text (AIGT) detectors. Current works only consider document-level AIGT detection, therefore, in this paper, we first... | Botian Jiang, Dong Zhang, Ke Ren, Linyang Li, Pengyu Wang, Xipeng Qiu |  |
| 1268 |  |  [QTSumm: Query-Focused Summarization over Tabular Data](https://doi.org/10.18653/v1/2023.emnlp-main.74) |  | 0 | People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users’ information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new... | Arman Cohan, Boyu Mi, Dragomir Radev, Linyong Nan, Ruizhe Chen, Simeng Han, Weijin Zou, Xiangru Tang, Yilun Zhao, Yixin Liu, Yumo Xu, Zhenting Qi |  |
| 1269 |  |  [From Wrong To Right: A Recursive Approach Towards Vision-Language Explanation](https://doi.org/10.18653/v1/2023.emnlp-main.75) |  | 0 | Addressing the challenge of adapting pre-trained vision-language models for generating insightful explanations for visual reasoning tasks with limited annotations, we present ReVisE: a Recursive Visual Explanation algorithm. Our method iteratively computes visual features (conditioned on the text... | Boyi Li, Jiaxin Ge, Sanjay Subramanian, Trevor Darrell |  |
| 1270 |  |  ['Don't Get Too Technical with Me': A Discourse Structure-Based Framework for Automatic Science Journalism](https://doi.org/10.18653/v1/2023.emnlp-main.76) |  | 0 | Science journalism refers to the task of reporting technical findings of a scientific paper as a less technical news article to the general public audience. We aim to design an automated system to support this real-world task (i.e., automatic science journalism ) by 1) introducing a... | Bingsheng Yao, Dakuo Wang, Ronald Cardenas, Yufang Hou |  |
| 1271 |  |  [LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following](https://doi.org/10.18653/v1/2023.emnlp-main.77) |  | 0 | End-to-end Transformers have demonstrated an impressive success rate for Embodied Instruction Following when the environment has been seen in training. However, they tend to struggle when deployed in an unseen environment. This lack of generalizability is due to the agent’s insensitivity to subtle... | ChengFu Yang, Jianwei Yang, KaiWei Chang, Lu Yuan, Xiyang Dai, YenChun Chen, YuChiang Frank Wang |  |
| 1272 |  |  [Penalty Decoding: Well Suppress the Self-Reinforcement Effect in Open-Ended Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.78) |  | 0 | The decoding algorithm is critical for open-ended text generation, transforming latent representations into coherent and meaningful outputs. This paper investigates the self-reinforcement effect in text generation and the effectiveness of a repetition penalty to mitigate it. However, determining... | Hongkun Hao, Rui Wang, Wenhong Zhu |  |
| 1273 |  |  [Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.79) |  | 0 | The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the... | Dongkuan Xu, Jianwei Li, Qi Lei, Wei Cheng |  |
| 1274 |  |  [Clinical Contradiction Detection](https://doi.org/10.18653/v1/2023.emnlp-main.80) |  | 0 | Detecting contradictions in text is essential in determining the validity of the literature and sources that we consume. Medical corpora are riddled with conflicting statements. This is due to the large throughput of new studies and the difficulty in replicating experiments, such as clinical... | Dave Makhervaks, Kira Radinsky, Plia Gillis |  |
| 1275 |  |  [Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements](https://doi.org/10.18653/v1/2023.emnlp-main.81) |  | 0 | Today’s language models can be remarkably intelligent yet still produce text that contains trivial commonsense errors. Therefore, we seek a retrospective verification approach that can reflect on the commonsense plausibility of the machine text, and introduce Vera, a general-purpose model that... | Dianzhuo Wang, Hannaneh Hajishirzi, Jiacheng Liu, Noah A. Smith, Wenya Wang, Yejin Choi |  |
| 1276 |  |  [Text-Transport: Toward Learning Causal Effects of Natural Language](https://doi.org/10.18653/v1/2023.emnlp-main.82) |  | 0 | As language technologies gain prominence in real-world settings, it is important to understand \*how\* changes to language affect reader perceptions. This can be formalized as the \*causal effect\* of varying a linguistic attribute (e.g., sentiment) on a reader’s response to the text. In this... | Eli BenMichael, LouisPhilippe Morency, Victoria Lin |  |
| 1277 |  |  [How Does Generative Retrieval Scale to Millions of Passages?](https://doi.org/10.18653/v1/2023.emnlp-main.83) |  | 0 | The emerging paradigm of generative retrieval re-frames the classic information retrieval problem into a sequence-to-sequence modeling task, forgoing external indices and encoding an entire document corpus within a single Transformer. Although many different approaches have been proposed to improve... | Donald Metzler, Honglei Zhuang, Jai Gupta, Jimmy Lin, Kai Hui, Ronak Pradeep, Vinh Q. Tran, Ádám D. Lelkes |  |
| 1278 |  |  [Unveiling the Implicit Toxicity in Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.84) |  | 0 | The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use. While recent studies primarily focus on probing toxic outputs that can be easily detected with existing toxicity classifiers, we show... | Chengfei Li, Hao Sun, Jiaxin Wen, Jinfeng Bai, Minlie Huang, Pei Ke, Zhexin Zhang |  |
| 1279 |  |  [Is ChatGPT a General-Purpose Natural Language Processing Task Solver?](https://doi.org/10.18653/v1/2023.emnlp-main.85) |  | 0 | Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot—i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural... | Aston Zhang, Chengwei Qin, Diyi Yang, Jiaao Chen, Michihiro Yasunaga, Zhuosheng Zhang |  |
| 1280 |  |  [Length is a Curse and a Blessing for Document-level Semantics](https://doi.org/10.18653/v1/2023.emnlp-main.86) |  | 0 | In recent years, contrastive learning (CL) has been extensively utilized to recover sentence and document-level encoding capability from pre-trained language models. In this work, we question the length generalizability of CL-based models, i.e., their vulnerability towards length-induced semantic... | Chenghao Xiao, Chenghua Lin, G. Thomas Hudson, Noura Al Moubayed, Yizhi Li |  |
| 1281 |  |  [ALCUNA: Large Language Models Meet New Knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.87) |  | 0 | With the rapid development of NLP, large-scale language models (LLMs) excel in various tasks across multiple domains now. However, existing benchmarks may not adequately measure these models’ capabilities, especially when faced with new knowledge. In this paper, we address the lack of benchmarks to... | Baizhou Huang, Xiaojun Wan, Xunjian Yin |  |
| 1282 |  |  [Location-Aware Visual Question Generation with Lightweight Models](https://doi.org/10.18653/v1/2023.emnlp-main.88) |  | 0 | This work introduces a novel task, location-aware visual question generation (LocaVQG), which aims to generate engaging questions from data relevant to a particular geographical location. Specifically, we represent such location-aware information with surrounding images and a GPS coordinate. To... | IBin Liao, Justin ChihYao Chen, LunWei Ku, Nicholas Collin Suwono, ShaoHua Sun, TingHao (Kenneth) Huang, TunMin Hung, YungHui Li |  |
| 1283 |  |  [MemeCap: A Dataset for Captioning and Interpreting Memes](https://doi.org/10.18653/v1/2023.emnlp-main.89) |  | 0 | Memes are a widely popular tool for web users to express their thoughts using visual metaphors. Understanding memes requires recognizing and interpreting visual metaphors with respect to the text inside or around the meme, often while employing background knowledge and reasoning abilities. We... | Eunjeong Hwang, Vered Shwartz |  |
| 1284 |  |  [Where to start? Analyzing the potential value of intermediate models](https://doi.org/10.18653/v1/2023.emnlp-main.90) |  | 0 | Previous studies observed that finetuned models may be better base models than the vanilla pretrained model. Such a model, finetuned on some source dataset, may provide a better starting point for a new finetuning process on a desired target dataset. Here, we perform a systematic analysis of this... | Elad Venezian, Leshem Choshen, Noam Slonim, Shachar DonYehiya, Yoav Katz |  |
| 1285 |  |  [Transcending Scaling Laws with 0.1% Extra Compute](https://doi.org/10.18653/v1/2023.emnlp-main.91) |  | 0 | Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a... | Aakanksha Chowdhery, David R. So, Denny Zhou, Donald Metzler, Huaixiu Steven Zheng, Hyung Won Chung, Jason Wei, Jinfeng Rao, Mostafa Dehghani, Neil Houlsby, Quoc V. Le, Siamak Shakeri, Slav Petrov, Vinh Q. Tran, Xavier Garcia, Yi Tay |  |
| 1286 |  |  [CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation](https://doi.org/10.18653/v1/2023.emnlp-main.92) |  | 0 | Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even... | Caleb Ziems, Diyi Yang, MinYen Kan, Minzhi Li, Nancy F. Chen, Taiwei Shi, Zhengyuan Liu |  |
| 1287 |  |  [Optimizing Retrieval-augmented Reader Models via Token Elimination](https://doi.org/10.18653/v1/2023.emnlp-main.93) |  | 0 | Fusion-in-Decoder (FiD) is an effective retrieval-augmented language model applied across a variety of open-domain tasks, such as question answering, fact checking, etc. In FiD, supporting passages are first retrieved and then processed using a generative model (Reader), which can cause a... | Avi Caciularu, Ido Dagan, Moshe Berchansky, Moshe Wasserblat, Peter Izsak |  |
| 1288 |  |  [WSDMS: Debunk Fake News via Weakly Supervised Detection of Misinforming Sentences with Contextualized Social Wisdom](https://doi.org/10.18653/v1/2023.emnlp-main.94) |  | 0 | Fake news debunking primarily focuses on determining the truthfulness of news articles, which oversimplifies the issue as fake news often combines elements of both truth and falsehood. Thus, it becomes crucial to identify specific instances of misinformation within the articles. In this research,... | Hongzhan Lin, Jing Ma, Ruichao Yang, Wei Gao, Zhiwei Yang |  |
| 1289 |  |  [Robust Prompt Optimization for Large Language Models Against Distribution Shifts](https://doi.org/10.18653/v1/2023.emnlp-main.95) |  | 0 | Large Language Model (LLM) has demonstrated significant ability in various Natural Language Processing tasks. However, their effectiveness is highly dependent on the phrasing of the task prompt, leading to research on automatic prompt optimization using labeled task data. We reveal that these... | Fuli Feng, Jizhi Zhang, Moxin Li, TatSeng Chua, Wenjie Wang, Yixin Cao |  |
| 1290 |  |  [Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.96) |  | 0 | Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the... | Marija Sakota, Martin Josifoski, Maxime Peyrard, Robert West |  |
| 1291 |  |  [Condensing Multilingual Knowledge with Lightweight Language-Specific Modules](https://doi.org/10.18653/v1/2023.emnlp-main.97) |  | 0 | Incorporating language-specific (LS) modules or Mixture-of-Experts (MoE) are proven methods to boost performance in multilingual model performance, but the scalability of these approaches to hundreds of languages or experts tends to be hard to manage. We present Language-specific Matrix Synthesis... | Benjamin Van Durme, Haoran Xu, Kenton Murray, Philipp Koehn, Shuyue Stella Li, Weiting Tan, Yunmo Chen |  |
| 1292 |  |  [The Framework Tax: Disparities Between Inference Efficiency in NLP Research and Deployment](https://doi.org/10.18653/v1/2023.emnlp-main.98) |  | 0 | Increased focus on the computational efficiency of systems in natural language processing has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point... | Clara Na, Emma Strubell, Jacob Kahn, Jared Fernandez, Yonatan Bisk |  |
| 1293 |  |  [Evaluating Cross-Domain Text-to-SQL Models and Benchmarks](https://doi.org/10.18653/v1/2023.emnlp-main.99) |  | 0 | Text-to-SQL benchmarks play a crucial role in evaluating the progress made in the field and the ranking of different models. However, accurately matching a model-generated SQL query to a reference SQL query in a benchmark fails for various reasons, such as underspecified natural language queries,... | Davood Rafiei, Mohammadreza Pourreza |  |
| 1294 |  |  [Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.100) |  | 0 | Recent work in Natural Language Processing and Computer Vision has been using textual information – e.g., entity names and descriptions – available in knowledge graphs to ground neural models to high-quality structured data. However, when it comes to non-English languages, the quantity and quality... | Daniel Lee, Ihab F. Ilyas, Min Li, Simone Conia, Umar Farooq Minhas, Yunyao Li |  |
| 1295 |  |  [Memory-Based Invariance Learning for Out-of-Domain Text Classification](https://doi.org/10.18653/v1/2023.emnlp-main.101) |  | 0 | We investigate the task of out-of-domain (OOD) text classification with the aim of extending a classification model, trained on multiple source domains, to an unseen target domain. Recent studies have shown that learning invariant representations can enhance the performance of OOD generalization.... | Chen Jia, Yue Zhang |  |
| 1296 |  |  [Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling](https://doi.org/10.18653/v1/2023.emnlp-main.102) |  | 0 | Post-training quantization (PTQ) of transformer language models faces significant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are concentrated in specific channels and are asymmetric across channels. To address this issue, we propose the... | Jinyang Guo, Ruihao Gong, Xiangguo Zhang, Xianglong Liu, Xiuying Wei, Yuhang Li, Yunchen Zhang |  |
| 1297 |  |  [Three Stream Based Multi-level Event Contrastive Learning for Text-Video Event Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.103) |  | 0 | Text-video based multimodal event extraction refers to identifying event information from the given text-video pairs. Existing methods predominantly utilize video appearance features (VAF) and text sequence features (TSF) as input information. Some of them employ contrastive learning to align VAF... | Chuanyi Zhang, Dehai Min, Guilin Qi, Jiaqi Li, Miaozeng Du, Yongrui Chen |  |
| 1298 |  |  [Diversify Question Generation with Retrieval-Augmented Style Transfer](https://doi.org/10.18653/v1/2023.emnlp-main.104) |  | 0 | Given a textual passage and an answer, humans are able to ask questions with various expressions, but this ability is still challenging for most question generation (QG) systems. Existing solutions mainly focus on the internal knowledge within the given passage or the semantic word space for... | Bowen Yu, CamTu Nguyen, Fei Huang, Haiyang Yu, Qi Gou, Yongbin Li, Zehua Xia |  |
| 1299 |  |  [Fast and Accurate Factual Inconsistency Detection Over Long Documents](https://doi.org/10.18653/v1/2023.emnlp-main.105) |  | 0 | Generative AI models exhibit remarkable potential; however, hallucinations across various tasks present a significant challenge, particularly for longer inputs that current approaches struggle to address effectively. We introduce SCALE (Source Chunking Approach for Large-scale inconsistency... | Barrett Martin Lattimer, Patrick Chen, Xinyuan Zhang, Yi Yang |  |
| 1300 |  |  [Interpreting Embedding Spaces by Conceptualization](https://doi.org/10.18653/v1/2023.emnlp-main.106) |  | 0 | One of the main methods for computational interpretation of a text is mapping it into a vector in some embedding space. Such vectors can then be used for a variety of textual processing tasks. Recently, most embedding spaces are a product of training large language models (LLMs). One major drawback... | Adi Simhi, Shaul Markovitch |  |
| 1301 |  |  [Knowledge-Augmented Language Model Verification](https://doi.org/10.18653/v1/2023.emnlp-main.107) |  | 0 | Recent Language Models (LMs) have shown impressive capabilities in generating texts with the knowledge internalized in parameters. Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated. To address this... | Jinheon Baek, Jong C. Park, Minki Kang, Soyeong Jeong, Sung Ju Hwang |  |
| 1302 |  |  [A Generation-based Deductive Method for Math Word Problems](https://doi.org/10.18653/v1/2023.emnlp-main.108) |  | 0 | Math word problems (MWP) involving advanced operators such as linear equation solver cannot be easily tackled by earlier MWP methods, because the existing generation methods suffer from repeated sub-expression generation and deductive methods are restricted to dealing with binary operations. This... | Cuiping Li, Haoyang Li, Hong Chen, Jing Zhang, Yuxuan Hu |  |
| 1303 |  |  [Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation](https://doi.org/10.18653/v1/2023.emnlp-main.109) |  | 0 | Large Language Models (LLMs) have showcased impressive performance. However, due to their inability to capture relationships among samples, these frozen LLMs inevitably keep repeating similar mistakes. In this work, we propose our Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in... | Peng Li, Yang Liu, Zeyuan Yang |  |
| 1304 |  |  [Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning](https://doi.org/10.18653/v1/2023.emnlp-main.110) |  | 0 | Maintaining a consistent persona is a key quality for any open domain dialogue system. Current state-of-the-art systems do this by training agents with supervised learning or online reinforcement learning (RL). However, systems trained with supervised learning often lack consistency as they are... | Ryan Shea, Zhou Yu |  |
| 1305 |  |  [Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories](https://doi.org/10.18653/v1/2023.emnlp-main.111) |  | 0 | In this paper we improve the zero-shot generalization ability of language models via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora (external memories), with the option to “plug in” unseen memory at inference time. We... | Arnold Overwijk, Chenyan Xiong, Corby Rosset, Jiawei Han, Paul Bennett, Suyu Ge |  |
| 1306 |  |  [Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.112) |  | 0 | Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions. However, how to select new tasks to improve the performance and generalizability of IT models remains an open question.... | Di Wu, Fan Yin, KaiWei Chang, Nanyun Peng, PoNien Kung |  |
| 1307 |  |  [Towards Example-Based NMT with Multi-Levenshtein Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.113) |  | 0 | Retrieval-Augmented Machine Translation (RAMT) is attracting growing attention. This is because RAMT not only improves translation metrics, but is also assumed to implement some form of domain adaptation. In this contribution, we study another salient trait of RAMT, its ability to make translation... | François Yvon, Josep Maria Crego, Maxime Bouthors |  |
| 1308 |  |  [DUnE: Dataset for Unified Editing](https://doi.org/10.18653/v1/2023.emnlp-main.114) |  | 0 | Even the most advanced language models remain susceptible to errors necessitating to modify these models without initiating a comprehensive retraining process. Model editing refers to the modification of a model’s knowledge or representations in a manner that produces the desired outcomes. Prior... | Afra Feyza Akyürek, Derry Wijaya, Eric Pan, Garry Kuwanto |  |
| 1309 |  |  ["Fifty Shades of Bias": Normative Ratings of Gender Bias in GPT Generated English Text](https://doi.org/10.18653/v1/2023.emnlp-main.115) |  | 0 | Language serves as a powerful tool for the manifestation of societal belief systems. In doing so, it also perpetuates the prevalent biases in our society. Gender bias is one of the most pervasive biases in our society and is seen in online and offline discourses. With LLMs increasingly gaining... | Agrima Seth, Harshita Diddee, Kalika Bali, Rishav Hada |  |
| 1310 |  |  [Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.116) |  | 0 | Inverted file structure is a common technique for accelerating dense retrieval. It clusters documents based on their embeddings; during searching, it probes nearby clusters w.r.t. an input query and only evaluates documents within them by subsequent codecs, thus avoiding the expensive cost from... | Jing Yao, Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou |  |
| 1311 |  |  [ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness](https://doi.org/10.18653/v1/2023.emnlp-main.117) |  | 0 | The emergence of generative large language models (LLMs) raises the question: what will be its impact on crowdsourcing? Traditionally, crowdsourcing has been used for acquiring solutions to a wide variety of human-intelligence tasks, including ones involving text generation, modification or... | Jakub Simko, Ján Cegin, Peter Brusilovsky |  |
| 1312 |  |  [Query-as-context Pre-training for Dense Passage Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.118) |  | 0 | Recently, methods have been developed to improve the performance of dense passage retrieval by using context-supervised pre-training. These methods simply consider two passages from the same document to be relevant, without taking into account the potential negative impacts of weakly correlated... | Guangyuan Ma, Songlin Hu, Wanhui Qian, Xing Wu, Zijia Lin |  |
| 1313 |  |  [A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.119) |  | 0 | Webpages have been a rich, scalable resource for vision-language and language only tasks. Yet only pieces of webpages are kept in existing datasets: image-caption pairs, long text articles, or raw HTML, never all in one place. Webpage tasks have resultingly received little attention and structured... | Andrea Burns, Bryan A. Plummer, Geoff Brown, Jianmo Ni, Joshua Ainslie, Kate Saenko, Krishna Srinivasan, Mandy Guo |  |
| 1314 |  |  [Democratizing Reasoning Ability: Tailored Learning from Large Language Model](https://doi.org/10.18653/v1/2023.emnlp-main.120) |  | 0 | Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature. Recent research on advancing open-source smaller LMs by distilling knowledge from black-box LLMs... | Feng Sun, Furu Wei, Haizhen Huang, Jiahai Wang, Minghui Song, Qi Zhang, Shaohan Huang, Weiwei Deng, Yuxuan Liu, Zhaoyang Wang, Zihan Zhang |  |
| 1315 |  |  [OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.121) |  | 0 | The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspect-based summarization setting... | Asi Shefer, Ido Dagan, Liat Schiff, Ori Ernst, Ori Shapira, Shmuel Amar |  |
| 1316 |  |  [PEFTDebias : Capturing debiasing information using PEFTs](https://doi.org/10.18653/v1/2023.emnlp-main.122) |  | 0 | The increasing use of foundation models highlights the urgent need to address and eliminate implicit biases present in them that arise during pretraining. In this paper, we introduce PEFTDebias, a novel approach that employs parameter-efficient fine-tuning (PEFT) to mitigate the biases within... | Aditya Srikanth Veerubhotla, Srijan Bansal, Sumit Agarwal |  |
| 1317 |  |  [Byte Pair Encoding for Symbolic Music](https://doi.org/10.18653/v1/2023.emnlp-main.123) |  | 0 | When used with deep learning, the symbolic music modality is often coupled with language model architectures. To do so, the music needs to be tokenized, i.e. converted into a sequence of discrete tokens. This can be achieved by different approaches, as music can be composed of simultaneous tracks,... | Fabien Chhel, JeanPierre Briot, Nathan Fradet, Nicolas Gutowski |  |
| 1318 |  |  [Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models](https://doi.org/10.18653/v1/2023.emnlp-main.124) |  | 0 | Recently, using large pre-trained Transformer models for transfer learning tasks has evolved to the point where they have become one of the flagship trends in the Natural Language Processing (NLP) community, giving rise to various outlooks such as prompt-based, adapters, or combinations with... | Alejo LopezAvila, Víctor SuárezPaniagua |  |
| 1319 |  |  [Self-Influence Guided Data Reweighting for Language Model Pre-training](https://doi.org/10.18653/v1/2023.emnlp-main.125) |  | 0 | Language Models (LMs) pre-trained with selfsupervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training.... | Megh Thakkar, Partha Talukdar, Sarath Chandar, Shikhar Vashishth, Sriram Ganapathy, Tolga Bolukbasi |  |
| 1320 |  |  [ACTOR: Active Learning with Annotator-specific Classification Heads to Embrace Human Label Variation](https://doi.org/10.18653/v1/2023.emnlp-main.126) |  | 0 | Label aggregation such as majority voting is commonly used to resolve annotator disagreement in dataset creation. However, this may disregard minority values and opinions. Recent studies indicate that learning from individual annotations outperforms learning from aggregated labels, though they... | Barbara Plank, Xinpeng Wang |  |
| 1321 |  |  [TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.127) |  | 0 | Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written... | Chen Elkind, Idan Szpektor, Jonathan Herzig, Roee Aharoni, Zorik Gekhman |  |
| 1322 |  |  [VivesDebate-Speech: A Corpus of Spoken Argumentation to Leverage Audio Features for Argument Mining](https://doi.org/10.18653/v1/2023.emnlp-main.128) |  | 0 | In this paper, we describe VivesDebate-Speech, a corpus of spoken argumentation created to leverage audio features for argument mining tasks. The creation of this corpus represents an important contribution to the intersection of speech processing and argument mining communities, and one of the... | Javier Sanchez, Ramon RuizDolz |  |
| 1323 |  |  [Tagging-Assisted Generation Model with Encoder and Decoder Supervision for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.129) |  | 0 | ASTE (Aspect Sentiment Triplet Extraction) has gained increasing attention. Recent advancements in the ASTE task have been primarily driven by Natural Language Generation-based (NLG) approaches. However, most NLG methods overlook the supervision of the encoder-decoder hidden representations and... | Meng Yang, Xianlong Luo, Yihao Wang |  |
| 1324 |  |  [Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.130) |  | 0 | Language model probing is often used to test specific capabilities of models. However, conclusions from such studies may be limited when the probing benchmarks are small and lack statistical power. In this work, we introduce new, larger datasets for negation (NEG-1500-SIMP) and role reversal... | Anna Rumshisky, Namrata Shivagunde, Vladislav Lialin |  |
| 1325 |  |  [Norm of Word Embedding Encodes Information Gain](https://doi.org/10.18653/v1/2023.emnlp-main.131) |  | 0 | Distributed representations of words encode lexical semantic information, but what type of information is encoded and how? Focusing on the skip-gram with negative-sampling method, we found that the squared norm of static word embedding encodes the information gain conveyed by the word; the... | Hidetoshi Shimodaira, Momose Oyama, Sho Yokoi |  |
| 1326 |  |  [CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data](https://doi.org/10.18653/v1/2023.emnlp-main.132) |  | 0 | Large language models (LLMs) show powerful reasoning abilities on various text-based tasks. However, their reasoning capability on structured data such as tables has not been systematically explored. In this work, we first establish a comprehensive taxonomy of reasoning and operation types for... | JianGuang Lou, Xitao Li, Yan Gao, Zhehao Zhang |  |
| 1327 |  |  [Promoting Topic Coherence and Inter-Document Consorts in Multi-Document Summarization via Simplicial Complex and Sheaf Graph](https://doi.org/10.18653/v1/2023.emnlp-main.133) |  | 0 | Multi-document Summarization (MDS) characterizes compressing information from multiple source documents to its succinct summary. An ideal summary should encompass all topics and accurately model cross-document relations expounded upon in the source documents. However, existing systems either impose... | Arun Iyer, Tanmoy Chakraborty, Vikram Goyal, Yash Kumar Atri |  |
| 1328 |  |  [MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations](https://doi.org/10.18653/v1/2023.emnlp-main.134) |  | 0 | Humans possess a remarkable ability to assign novel interpretations to linguistic expressions, enabling them to learn new words and understand community-specific connotations. However, Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly. Therefore, it is... | Arkil Patel, Dzmitry Bahdanau, Satwik Bhattamishra, Siva Reddy |  |
| 1329 |  |  [Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency](https://doi.org/10.18653/v1/2023.emnlp-main.135) |  | 0 | Developing an educational test can be expensive and time-consuming, as each item must be written by experts and then evaluated by collecting hundreds of student responses. Moreover, many tests require multiple distinct sets of questions administered throughout the school year to closely monitor... | Diyi Yang, Eric Zelikman, Jasmine E. Tran, Jason D. Yeatman, Nick Haber, Wanjing Anya Ma |  |
| 1330 |  |  [Counter Turing Test (CT2): AI-Generated Text Detection is Not as Easy as You May Think - Introducing AI Detectability Index (ADI)](https://doi.org/10.18653/v1/2023.emnlp-main.136) |  | 0 | With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. This triggered a series of events, including an open letter, signed by thousands of researchers and tech leaders in March 2023, demanding a six-month moratorium on the training of AI systems... | Aman Chadha, Amit P. Sheth, Amitava Das, Chandan Gupta, Krish Sharma, Megha Chakraborty, Niyar R. Barman, S. M. Mehedi Zaman, S. M. Towhidul Islam Tonmoy, Shreya Gautam, Tanay Kumar, Vinija Jain |  |
| 1331 |  |  [Revisiting the Optimality of Word Lengths](https://doi.org/10.18653/v1/2023.emnlp-main.137) |  | 0 | Zipf (1935) posited that wordforms are optimized to minimize utterances’ communicative costs. Under the assumption that cost is given by an utterance’s length, he supported this claim by showing that words’ lengths are inversely correlated with their frequencies. Communicative cost, however, can be... | Clara Meister, Ethan Wilcox, Kyle Mahowald, Ryan Cotterell, Tiago Pimentel |  |
| 1332 |  |  [Document-level Relationship Extraction by Bidirectional Constraints of Beta Rules](https://doi.org/10.18653/v1/2023.emnlp-main.138) |  | 0 | Document-level Relation Extraction (DocRE) aims to extract relations among entity pairs in documents. Some works introduce logic constraints into DocRE, addressing the issues of opacity and weak logic in original DocRE models. However, they only focus on forward logic constraints and the rules... | Daoqi Chen, Xiaowang Zhang, Yaxin Li, Yichun Liu, Zhiyong Feng, Zizhong Zhu |  |
| 1333 |  |  [Instructed Language Models with Retrievers Are Powerful Entity Linkers](https://doi.org/10.18653/v1/2023.emnlp-main.139) |  | 0 | Generative approaches powered by large language models (LLMs) have demonstrated emergent abilities in tasks that require complex reasoning abilities. Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking... | Daxin Jiang, Jie Wu, Linjun Shou, Ming Gong, Xingyao Zhang, Zilin Xiao |  |
| 1334 |  |  [Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text](https://doi.org/10.18653/v1/2023.emnlp-main.140) |  | 0 | Linguistic communication is prevalent in Human-Computer Interaction (HCI). Speech (spoken language) serves as a convenient yet potentially ambiguous form due to noise and accents, exposing a gap compared to text. In this study, we investigate the prominent HCI task, Referring Video Object... | Bhiksha Raj, Fan Yang, Jinglu Wang, Muqiao Yang, Rita Singh, Xiang Li, Xiaohao Xu, Yizhou Zhao |  |
| 1335 |  |  [PROSE: A Pronoun Omission Solution for Chinese-English Spoken Language Translation](https://doi.org/10.18653/v1/2023.emnlp-main.141) |  | 0 | Neural Machine Translation (NMT) systems encounter a significant challenge when translating a pro-drop (‘pronoun-dropping’) language (e.g., Chinese) to a non-pro-drop one (e.g., English), since the pro-drop phenomenon demands NMT systems to recover omitted pronouns. This unique and crucial task,... | Ke Wang, Wei Peng, Xiutian Zhao, Yanghui Li |  |
| 1336 |  |  [A Diachronic Analysis of Paradigm Shifts in NLP Research: When, How, and Why?](https://doi.org/10.18653/v1/2023.emnlp-main.142) |  | 0 | Understanding the fundamental concepts and trends in a scientific field is crucial for keeping abreast of its continuous advancement. In this study, we propose a systematic framework for analyzing the evolution of research topics in a scientific field using causal discovery and inference... | Aniket Pramanick, Iryna Gurevych, Saif M. Mohammad, Yufang Hou |  |
| 1337 |  |  [Does the Correctness of Factual Knowledge Matter for Factual Knowledge-Enhanced Pre-trained Language Models?](https://doi.org/10.18653/v1/2023.emnlp-main.143) |  | 0 | In recent years, the injection of factual knowledge has been observed to have a significant positive correlation to the downstream task performance of pre-trained language models. However, existing work neither demonstrates that pre-trained models successfully learn the injected factual knowledge... | Boxi Cao, Hongyu Lin, Le Sun, Qiaoyu Tang, Xianpei Han |  |
| 1338 |  |  [Syntactic Substitutability as Unsupervised Dependency Syntax](https://doi.org/10.18653/v1/2023.emnlp-main.144) |  | 0 | Syntax is a latent hierarchical structure which underpins the robust and compositional nature of human language. In this work, we explore the hypothesis that syntactic dependencies can be represented in language model attention distributions and propose a new method to induce these structures... | Jasper Jian, Siva Reddy |  |
| 1339 |  |  [MProto: Multi-Prototype Network with Denoised Optimal Transport for Distantly Supervised Named Entity Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.145) |  | 0 | Distantly supervised named entity recognition (DS-NER) aims to locate entity mentions and classify their types with only knowledge bases or gazetteers and unlabeled corpus. However, distant annotations are noisy and degrade the performance of NER models. In this paper, we propose a noise-robust... | Jietian Guo, Shiliang Pu, Shuhui Wu, Weiming Lu, Wenqi Ren, Yongliang Shen, Zeqi Tan |  |
| 1340 |  |  [The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions](https://doi.org/10.18653/v1/2023.emnlp-main.146) |  | 0 | Recent progress in Large Language Models (LLMs) has produced models that exhibit remarkable performance across a variety of NLP tasks. However, it remains unclear whether the existing focus of NLP research accurately captures the genuine requirements of human users. This paper provides a... | Chenguang Zhu, Dan Iter, Heng Ji, Jiawei Han, Ming Zhong, Reid Pryzant, Shuohang Wang, Siru Ouyang, Yang Liu, Yizhu Jiao |  |
| 1341 |  |  [Learning the Visualness of Text Using Large Vision-Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.147) |  | 0 | Visual text evokes an image in a person’s mind, while non-visual text fails to do so. A method to automatically detect visualness in text will enable text-to-image retrieval and generation models to augment text with relevant images. This is particularly challenging with long-form text as... | Ani Nenkova, Christopher Tensmeyer, Gaurav Verma, Jiuxiang Gu, Ryan A. Rossi |  |
| 1342 |  |  [The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values](https://doi.org/10.18653/v1/2023.emnlp-main.148) |  | 0 | Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey... | Andrew M. Bean, Bertie Vidgen, Hannah Kirk, Paul Röttger, Scott Hale |  |
| 1343 |  |  [TempTabQA: Temporal Question Answering for Semi-Structured Tables](https://doi.org/10.18653/v1/2023.emnlp-main.149) |  | 0 | Semi-structured data, such as Infobox tables, often include temporal information about entities, either implicitly or explicitly. Can current NLP systems reason about such information in semi-structured tables? To tackle this question, we introduce the task of temporal question answering on... | Mahek Bhavesh Vora, Pranshu Kandoi, Ridho Reinanda, Shuo Zhang, Vivek Gupta, Vivek Srikumar, Yujie He |  |
| 1344 |  |  [Task-Level Thinking Steps Help Large Language Models for Challenging Classification Task](https://doi.org/10.18653/v1/2023.emnlp-main.150) |  | 0 | Large language models (LLMs) have shown incredible performance on many tasks such as dialogue generation, commonsense reasoning and question answering. In-context learning (ICL) is an important paradigm for adapting LLMs to the downstream tasks by prompting few demonstrations. However, the... | Chunhui Du, Hao He, Haoran Liao, Jidong Tian, Jindou Chen, Yaohui Jin |  |
| 1345 |  |  [RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation](https://doi.org/10.18653/v1/2023.emnlp-main.151) |  | 0 | The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic,... | Bei Chen, Daoguang Zan, Fengji Zhang, Jacky Keung, JianGuang Lou, Jin Liu, Weizhu Chen, Yi Mao, Yue Zhang |  |
| 1346 |  |  [Influence Scores at Scale for Efficient Language Data Sampling](https://doi.org/10.18653/v1/2023.emnlp-main.152) |  | 0 | Modern ML systems ingest data aggregated from diverse sources, such as synthetic, human-annotated, and live customer traffic. Understanding which examples are important to the performance of a learning algorithm is crucial for efficient model training. Recently, a growing body of literature has... | Joshua Tan, Maria Minakova, Nikhil Anand |  |
| 1347 |  |  [G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment](https://doi.org/10.18653/v1/2023.emnlp-main.153) |  | 0 | The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and... | Chenguang Zhu, Dan Iter, Ruochen Xu, Shuohang Wang, Yang Liu, Yichong Xu |  |
| 1348 |  |  [Learning Retrieval Augmentation for Personalized Dialogue Generation](https://doi.org/10.18653/v1/2023.emnlp-main.154) |  | 0 | Personalized dialogue generation, focusing on generating highly tailored responses by leveraging persona profiles and dialogue context, has gained significant attention in conversational AI applications. However, persona profiles, a prevalent setting in current personalized dialogue datasets,... | Lilian Tang, Qiushi Huang, Shuai Fu, Tom Ko, Wenwu Wang, Xubo Liu, Yu Zhang |  |
| 1349 |  |  [The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations](https://doi.org/10.18653/v1/2023.emnlp-main.155) |  | 0 | The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and... | Agnibh Pathak, Aman Chadha, Amit P. Sheth, Amitava Das, Anubhav Sarkar, S. M. Towhidul Islam Tonmoy, Swagata Chakraborty, Vipula Rawte |  |
| 1350 |  |  [NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders](https://doi.org/10.18653/v1/2023.emnlp-main.156) |  | 0 | Neural document rerankers are extremely effective in terms of accuracy. However, the best models require dedicated hardware for serving, which is costly and often not feasible. To avoid this servingtime requirement, we present a method of capturing up to 86% of the gains of a Transformer... | Daniel Gillick, Jeremy R. Cole, Livio Soares, Tom Kwiatkowski |  |
| 1351 |  |  [Analyzing Modular Approaches for Visual Question Decomposition](https://doi.org/10.18653/v1/2023.emnlp-main.157) |  | 0 | Modular neural networks without additional training have recently been shown to surpass end-to-end neural networks on challenging vision–language tasks. The latest such methods simultaneously introduce LLM-based code generation to build programs and a number of skill-specific, task-oriented modules... | Apoorv Khandelwal, Chen Sun, Ellie Pavlick |  |
| 1352 |  |  [Improving Summarization with Human Edits](https://doi.org/10.18653/v1/2023.emnlp-main.158) |  | 0 | Recent work has shown the promise of learning with human feedback paradigms to produce human-determined high-quality text. Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional... | Benjamin J. Schloss, Sai P. Selvaraj, Zonghai Yao |  |
| 1353 |  |  [Did You Mean...? Confidence-based Trade-offs in Semantic Parsing](https://doi.org/10.18653/v1/2023.emnlp-main.159) |  | 0 | We illustrate how a calibrated model can help balance common trade-offs in task-oriented parsing. In a simulated annotator-in-the-loop experiment, we show that well-calibrated confidence scores allow us to balance cost with annotator load, improving accuracy with a small number of interactions. We... | Benjamin Van Durme, Elias StengelEskin |  |
| 1354 |  |  [The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages](https://doi.org/10.18653/v1/2023.emnlp-main.160) |  | 0 | Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate remarkable performance in a wide range of tasks. Despite numerous recent studies that examine the performance of instruction-tuned LLMs on various NLP benchmarks, there remains a lack of comprehensive investigation into... | Chiyu Zhang, Khai Duy Doan, Muhammad AbdulMageed, Qisheng Liao |  |
| 1355 |  |  [Understanding the Effect of Model Compression on Social Bias in Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.161) |  | 0 | Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model’s predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to... | Emma Strubell, Gustavo Gonçalves |  |
| 1356 |  |  [BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology](https://doi.org/10.18653/v1/2023.emnlp-main.162) |  | 0 | The ability to automatically generate accurate protocols for scientific experiments would represent a major step towards the automation of science. Large Language Models (LLMs) have impressive capabilities on a wide range of tasks, such as question answering and the generation of coherent text and... | Aleksandar Shtedritski, Ali Essa Ghareeb, John Ginger, Odhran O'Donoghue, Ralph Abboud, Samuel G. Rodriques |  |
| 1357 |  |  [Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages](https://doi.org/10.18653/v1/2023.emnlp-main.163) |  | 0 | Chain-of-thought (CoT) is capable of eliciting models to explicitly generate reasoning paths, thus promoting reasoning accuracy and attracting increasing attention. Specifically, zero-shot CoT achieves remarkable improvements in a wide range of reasoning tasks by simply instructing the LLM with the... | Fuxuan Wei, Libo Qin, Qiguang Chen, Shijue Huang, Wanxiang Che |  |
| 1358 |  |  [FinGPT: Large Generative Models for a Small Language](https://doi.org/10.18653/v1/2023.emnlp-main.164) |  | 0 | Large language models (LLMs) excel in many tasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the challenges of creating LLMs for... | Aija Vahtola, Aleksandra Piktus, Anni Eskelinen, Filip Ginter, HannaMari Kupari, Jenna Kanerva, Jouni Luoma, Jyrki Heinonen, Mikko Merioksa, Niklas Muennighoff, Nouamane Tazi, Osma Suominen, Risto Luukkonen, Sampo Pyysalo, Samuel Antao, Samuli Sairanen, Teven Le Scao, Thomas Wang, Thomas Wolf, Veronika Laippala, Ville Komulainen |  |
| 1359 |  |  [Boosting Summarization with Normalizing Flows and Aggressive Training](https://doi.org/10.18653/v1/2023.emnlp-main.165) |  | 0 | This paper presents FlowSUM, a normalizing flows-based variational encoder-decoder framework for Transformer-based summarization. Our approach tackles two primary challenges in variational summarization: insufficient semantic information in latent representations and posterior collapse during... | Xiaotong Shen, Yu Yang |  |
| 1360 |  |  [Indicative Summarization of Long Discussions](https://doi.org/10.18653/v1/2023.emnlp-main.166) |  | 0 | Online forums encourage the exchange and discussion of different stances on many topics. Not only do they provide an opportunity to present one’s own arguments, but may also gather a broad cross-section of others’ arguments. However, the resulting long discussions are difficult to overview. This... | Dominik Schwabe, Khalid Al Khatib, Martin Potthast, Shahbaz Syed |  |
| 1361 |  |  [A Framework for Vision-Language Warm-up Tasks in Multimodal Dialogue Models](https://doi.org/10.18653/v1/2023.emnlp-main.167) |  | 0 | Most research on multimodal open-domain dialogue agents has focused on pretraining and multi-task learning using additional rich datasets beyond a given target dataset. However, methods for exploiting these additional datasets can be quite limited in real-world settings, creating a need for more... | Harksoo Kim, Hongjin Kim, Jaewook Lee, SeongHeum Park, Seongsik Park |  |
| 1362 |  |  [Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair Modeling](https://doi.org/10.18653/v1/2023.emnlp-main.168) |  | 0 | Transformer-based models have achieved great success on sentence pair modeling tasks, such as answer selection and natural language inference (NLI). These models generally perform cross-attention over input pairs, leading to prohibitive computational cost. Recent studies propose dual-encoder and... | Chuanyi Liu, Cuiyun Gao, Qifan Wang, Shiyi Qi, Yuanhang Yang, Zenglin Xu |  |
| 1363 |  |  [Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts](https://doi.org/10.18653/v1/2023.emnlp-main.169) |  | 0 | As large language models (LLMs) have shown effectiveness with different prompting methods, such as Chain of Thought, Program of Thought, we find that these methods have formed a great complementarity to each other on math reasoning tasks. In this work, we propose XoT, an integrated problem solving... | Qipeng Guo, Tengxiao Liu, Xiangkun Hu, Xipeng Qiu, Yue Zhang, Yuqing Yang, Zheng Zhang |  |
| 1364 |  |  [GLEN: General-Purpose Event Detection for Thousands of Types](https://doi.org/10.18653/v1/2023.emnlp-main.170) |  | 0 | The progress of event extraction research has been hindered by the absence of wide-coverage, large-scale datasets. To make event extraction systems more accessible, we build a general-purpose event detection dataset GLEN, which covers 205K event mentions with 3,465 different types, making it more... | Heng Ji, Jiawei Han, Kathryn Conger, Martha Palmer, Qiusi Zhan, Sha Li |  |
| 1365 |  |  [Hierarchical Pretraining on Multimodal Electronic Health Records](https://doi.org/10.18653/v1/2023.emnlp-main.171) |  | 0 | Pretraining has proven to be a powerful technique in natural language processing (NLP), exhibiting remarkable success in various NLP downstream tasks. However, in the medical domain, existing pretrained models on electronic health records (EHR) fail to capture the hierarchical nature of EHR data,... | Fenglong Ma, Jiaqi Wang, Junyu Luo, Suhan Cui, Xiaochen Wang, Yaqing Wang, Yuan Zhong, Ziyi Yin |  |
| 1366 |  |  [Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.172) |  | 0 | Hallucination of text ungrounded in the input is a well-known problem in neural data-to-text generation. Many methods have been proposed to mitigate it, but they typically require altering model architecture or collecting additional data, and thus cannot be easily applied to an existing model. In... | Mateusz Lango, Ondrej Dusek |  |
| 1367 |  |  [Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.173) |  | 0 | Multimodal machine translation (MMT) simultaneously takes the source sentence and a relevant image as input for translation. Since there is no paired image available for the input sentence in most cases, recent studies suggest utilizing powerful text-to-image generation models to provide image... | Dong Yu, Qingkai Fang, Wenyu Guo, Yang Feng |  |
| 1368 |  |  [DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.174) |  | 0 | Pretrained language models have learned a vast amount of human knowledge from large-scale corpora, but their powerful memorization capability also brings the risk of data leakage. Some risks may only be discovered after the model training is completed, such as the model memorizing a specific phone... | Chao Bian, Deyi Xiong, Junzhuo Li, Minghui Xu, Shuangzhi Wu, Weilong Dong, Xinwei Wu |  |
| 1369 |  |  [Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques](https://doi.org/10.18653/v1/2023.emnlp-main.175) |  | 0 | This paper investigates the transferability of debiasing techniques across different languages within multilingual models. We examine the applicability of these techniques in English, French, German, and Dutch. Using multilingual BERT (mBERT), we demonstrate that cross-lingual transfer of debiasing... | Bart Baesens, Jochen De Weerdt, Manon Reusens, Margot Mieskes, Philipp Borchert |  |
| 1370 |  |  [Can Language Models Laugh at YouTube Short-form Videos?](https://doi.org/10.18653/v1/2023.emnlp-main.176) |  | 0 | As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans. Unfortunately, previous video humor datasets target specific domains such as speeches or sitcoms, and mostly focus on verbal cues. We... | Dayoon Ko, Gunhee Kim, Sangho Lee |  |
| 1371 |  |  [Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation](https://doi.org/10.18653/v1/2023.emnlp-main.177) |  | 0 | Representation Learning on Knowledge Graphs (KGs) is essential for downstream tasks. The dominant approach, KG Embedding (KGE), represents entities with independent vectors and faces the scalability challenge. Recent studies propose an alternative way for parameter efficiency, which represents... | Jiaang Li, Licheng Zhang, Quan Wang, Yi Liu, Zhendong Mao |  |
| 1372 |  |  [Exploring All-In-One Knowledge Distillation Framework for Neural Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.178) |  | 0 | Conventional knowledge distillation(KD) approaches are commonly employed to compress neural machine translation(NMT) models. However, they only obtain one lightweight student each time. Consequently, we have to conduct KD multiple times when different students are required at the same time, which... | Bin Wang, Jian Luan, Jinsong Su, Min Zhang, Wen Zhang, Xiang Li, Yidong Chen, Zhongjian Miao |  |
| 1373 |  |  [HistAlign: Improving Context Dependency in Language Generation by Aligning with History](https://doi.org/10.18653/v1/2023.emnlp-main.179) |  | 0 | Language models (LMs) can generate hallucinations and incoherent outputs, which highlights their weak context dependency. Cache-LMs, which augment LMs with a memory of recent history, can increase context dependency and have shown remarkable performance in diverse language generation tasks.... | David Wan, Mohit Bansal, Shiyue Zhang |  |
| 1374 |  |  [CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models](https://doi.org/10.18653/v1/2023.emnlp-main.180) |  | 0 | Methods for adapting language models (LMs) to new tasks and domains have traditionally assumed white-box access to the model, and work by modifying its parameters. However, this is incompatible with a recent trend in the field, where the highest quality models are only available as black-boxes... | Aitor Ormazabal, Eneko Agirre, Mikel Artetxe |  |
| 1375 |  |  [Image Manipulation via Multi-Hop Instructions - A New Dataset and Weakly-Supervised Neuro-Symbolic Approach](https://doi.org/10.18653/v1/2023.emnlp-main.181) |  | 0 | We are interested in image manipulation via natural language text – a task that is useful for multiple AI applications but requires complex reasoning over multi-modal spaces. We extend recently proposed Neuro Symbolic Concept Learning (NSCL), which has been quite effective for the task of Visual... | Arnab Kumar Mondal, Ashish Goswami, Dinesh Garg, Dinesh Khandelwal, Harman Singh, Kevin Shah, Mohit Gupta, Parag Singla, Poorva Garg, Satyam Modi |  |
| 1376 |  |  [Generative Spoken Language Model based on continuous word-sized audio tokens](https://doi.org/10.18653/v1/2023.emnlp-main.182) |  | 0 | In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a... | Benoît Sagot, Emmanuel Dupoux, Gabriel Synnaeve, Jade Copet, Robin Algayres, Tu Anh Nguyen, Yossi Adi |  |
| 1377 |  |  [Enhancing Chat Language Models by Scaling High-quality Instructional Conversations](https://doi.org/10.18653/v1/2023.emnlp-main.183) |  | 0 | Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to push the... | Bokai Xu, Bowen Zhou, Maosong Sun, Ning Ding, Shengding Hu, Yujia Qin, Yulin Chen, Zhiyuan Liu |  |
| 1378 |  |  [Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining](https://doi.org/10.18653/v1/2023.emnlp-main.184) |  | 0 | Recent work in vision-and-language pretraining has investigated supervised signals from object detection data to learn better, fine-grained multimodal representations. In this work, we take a step further and explore how we can tap into supervision from small-scale visual relation data. In... | Aida Nematzadeh, Emanuele Bugliarello, Lisa Anne Hendricks |  |
| 1379 |  |  [Unsupervised Grammatical Error Correction Rivaling Supervised Methods](https://doi.org/10.18653/v1/2023.emnlp-main.185) |  | 0 | State-of-the-art grammatical error correction (GEC) systems rely on parallel training data (ungrammatical sentences and their manually corrected counterparts), which are expensive to construct. In this paper, we employ the Break-It-Fix-It (BIFI) method to build an unsupervised GEC system. The BIFI... | Hannan Cao, Hwee Tou Ng, Liping Yuan, Yuchen Zhang |  |
| 1380 |  |  [S2abEL: A Dataset for Entity Linking from Scientific Tables](https://doi.org/10.18653/v1/2023.emnlp-main.186) |  | 0 | Entity linking (EL) is the task of linking a textual mention to its corresponding entry in a knowledge base, and is critical for many knowledge-intensive NLP applications. When applied to tables in scientific papers, EL is a step toward large-scale scientific knowledge bases that could enable... | Aakanksha Naik, Bailey Kuehl, Doug Downey, Erin Bransom, Sergey Feldman, Yuze Lou |  |
| 1381 |  |  [API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.187) |  | 0 | Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs’ ability to utilize tools? (3) What... | Bowen Yu, Fei Huang, Feifan Song, Haiyang Yu, Hangyu Li, Minghao Li, Yingxiu Zhao, Yongbin Li, Zhoujun Li |  |
| 1382 |  |  [Language and Mental Health: Measures of Emotion Dynamics from Text as Linguistic Biosocial Markers](https://doi.org/10.18653/v1/2023.emnlp-main.188) |  | 0 | Research in psychopathology has shown that, at an aggregate level, the patterns of emotional change over time—emotion dynamics—are indicators of one’s mental health. One’s patterns of emotion change have traditionally been determined through self-reports of emotions; however, there are known issues... | Alona Fyshe, Daniela Teodorescu, Saif M. Mohammad, Tiffany Cheng |  |
| 1383 |  |  [Lion: Adversarial Distillation of Proprietary Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.189) |  | 0 | The practice of transferring knowledge from a sophisticated, proprietary large language model (LLM) to a compact, open-source LLM has garnered considerable attention. Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those... | Chunkit Chan, Mingyang Chen, Wei Wang, Yuxin Jiang |  |
| 1384 |  |  [Evaluating Large Language Models on Controlled Generation Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.190) |  | 0 | While recent studies have looked into the abilities of large language models in various benchmark tasks, including question generation, reading comprehension, multilingual and etc, there have been few studies looking into the controllability of large language models on generation tasks. We present... | Jiao Sun, John Frederick Wieting, Nan Xu, Nanyun Peng, Qian Hu, Rahul Gupta, Wangchunshu Zhou, Xuezhe Ma, Yufei Tian |  |
| 1385 |  |  [DeSIQ: Towards an Unbiased, Challenging Benchmark for Social Intelligence Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.191) |  | 0 | Social intelligence is essential for understanding and reasoning about human expressions, intents and interactions. One representative benchmark for its study is Social Intelligence Queries (Social-IQ), a dataset of multiple-choice questions on videos of complex social interactions. We define a... | Gholamreza Haffari, Xiaoyu Guo, YuanFang Li |  |
| 1386 |  |  [Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation](https://doi.org/10.18653/v1/2023.emnlp-main.192) |  | 0 | We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence: a condition that we call evidential closure. Information about the truth or falsity of sentences is not statistically identified in the standard neural language... | Adam Bouyamourn |  |
| 1387 |  |  [A Question Answering Framework for Decontextualizing User-facing Snippets from Scientific Documents](https://doi.org/10.18653/v1/2023.emnlp-main.193) |  | 0 | Many real-world applications (e.g., note taking, search) require extracting a sentence or paragraph from a document and showing that snippet to a human outside of the source document. Yet, users may find snippets difficult to understand as they lack context from the original document. In this work,... | Arman Cohan, Benjamin Newman, Kyle Lo, Luca Soldaini, Raymond Fok |  |
| 1388 |  |  [SLOG: A Structural Generalization Benchmark for Semantic Parsing](https://doi.org/10.18653/v1/2023.emnlp-main.194) |  | 0 | The goal of compositional generalization benchmarks is to evaluate how well models generalize to new complex linguistic expressions. Existing benchmarks often focus on lexical generalization, the interpretation of novel lexical items in syntactic structures familiar from training; structural... | Alexander Koller, Bingzhi Li, Lucia Donatelli, Najoung Kim, Tal Linzen, Yuekun Yao |  |
| 1389 |  |  [Pushdown Layers: Encoding Recursive Structure in Transformer Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.195) |  | 0 | Recursion is a prominent feature of human language, and fundamentally challenging for self-attention due to the lack of an explicit recursive-state tracking mechanism. Consequently, Transformer language models poorly capture long-tail recursive structure and exhibit sample-inefficient syntactic... | Christopher D. Manning, Jacob Andreas, Pratyusha Sharma, Shikhar Murty |  |
| 1390 |  |  [Can LLMs Facilitate Interpretation of Pre-trained Language Models?](https://doi.org/10.18653/v1/2023.emnlp-main.196) |  | 0 | Work done to uncover the knowledge encoded within pre-trained language models rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation. We propose using a large language model, ChatGPT, as an annotator to... | Basel Mousi, Fahim Dalvi, Nadir Durrani |  |
| 1391 |  |  [Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets](https://doi.org/10.18653/v1/2023.emnlp-main.197) |  | 0 | Named Entity Recognition (NER) frequently suffers from the problem of insufficient labeled data, particularly in fine-grained NER scenarios. Although K-shot learning techniques can be applied, their performance tends to saturate when the number of annotations exceeds several tens of labels. To... | Seokjin Oh, Su Ah Lee, Woohwan Jung |  |
| 1392 |  |  [Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies](https://doi.org/10.18653/v1/2023.emnlp-main.198) |  | 0 | When we transfer a pretrained language model to a new language, there are many axes of variation that change at once. To disentangle the impact of different factors like syntactic similarity and vocabulary similarity, we propose a set of controlled transfer studies: we systematically transform the... | Alex Tamkin, Isabel Papadimitriou, Zhengxuan Wu |  |
| 1393 |  |  [Non-Autoregressive Math Word Problem Solver with Unified Tree Structure](https://doi.org/10.18653/v1/2023.emnlp-main.199) |  | 0 | Existing MWP solvers employ sequence or binary tree to present the solution expression and decode it from given problem description. However, such structures fail to handle the variants that can be derived via mathematical manipulation, e.g., (a1+a2)\*a3 and a1 \* a3+a2 \* a3 can both be possible... | Heng Tao Shen, Lei Wang, Mengqun Han, SeeKiong Ng, Wenhao Shi, Yang Yang, Yi Bin |  |
| 1394 |  |  [Improving Chinese Pop Song and Hokkien Gezi Opera Singing Voice Synthesis by Enhancing Local Modeling](https://doi.org/10.18653/v1/2023.emnlp-main.200) |  | 0 | Singing Voice Synthesis (SVS) strives to synthesize pleasing vocals based on music scores and lyrics. The current acoustic models based on Transformer usually process the entire sequence globally and use a simple L1 loss. However, this approach overlooks the significance of local modeling within... | Meizhen Zheng, Peng Bai, Wujin Sun, Xiaodong Shi, Yue Zhou |  |
| 1395 |  |  [What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on QA Systems](https://doi.org/10.18653/v1/2023.emnlp-main.201) |  | 0 | NLP systems have shown impressive performance at answering questions by retrieving relevant context. However, with the increasingly large models, it is impossible and often undesirable to constrain models’ knowledge or reasoning to only the retrieved context. This leads to a mismatch between the... | Amanda Liu, Claire Bonial, Clare R. Voss, Connor Baumler, Eleftheria Briakou, Hal Daumé III, Jeffrey Micher, Marine Carpuat, Navita Goyal |  |
| 1396 |  |  [GROOViST: A Metric for Grounding Objects in Visual Storytelling](https://doi.org/10.18653/v1/2023.emnlp-main.202) |  | 0 | A proper evaluation of stories generated for a sequence of images—the task commonly referred to as visual storytelling—must consider multiple aspects, such as coherence, grammatical correctness, and visual grounding. In this work, we focus on evaluating the degree of grounding, that is, the extent... | Aditya K. Surikuchi, Raquel Fernández, Sandro Pezzelle |  |
| 1397 |  |  [VIBE: Topic-Driven Temporal Adaptation for Twitter Classification](https://doi.org/10.18653/v1/2023.emnlp-main.203) |  | 0 | Language features are evolving in real-world social media, resulting in the deteriorating performance of text classification in dynamics. To address this challenge, we study temporal adaptation, where models trained on past data are tested in the future. Most prior work focused on continued... | Jing Li, Wenjie Li, Yuji Zhang |  |
| 1398 |  |  [TOD-Flow: Modeling the Structure of Task-Oriented Dialogues](https://doi.org/10.18653/v1/2023.emnlp-main.204) |  | 0 | Task-Oriented Dialogue (TOD) systems have become crucial components in interactive artificial intelligence applications. While recent advances have capitalized on pre-trained language models (PLMs), they exhibit limitations regarding transparency and controllability. To address these challenges, we... | Anthony Z. Liu, DongKi Kim, Dongsub Shim, Honglak Lee, Lajanugen Logeswaran, Sungryull Sohn, Yiwei Lyu |  |
| 1399 |  |  [TopWORDS-Poetry: Simultaneous Text Segmentation and Word Discovery for Classical Chinese Poetry via Bayesian Inference](https://doi.org/10.18653/v1/2023.emnlp-main.205) |  | 0 | As a precious cultural heritage of human beings, classical Chinese poetry has a very unique writing style and often contains special words that rarely appear in general Chinese texts, posting critical challenges for natural language processing. Little effort has been made in the literature for... | Changzai Pan, Feiyue Li, Ke Deng |  |
| 1400 |  |  [Knowledge Rumination for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.206) |  | 0 | Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may... | Chuanqi Tan, Fei Huang, Huajun Chen, Ningyu Zhang, Peng Wang, Shengyu Mao, Yunzhi Yao |  |
| 1401 |  |  [Struct-XLM: A Structure Discovery Multilingual Language Model for Enhancing Cross-lingual Transfer through Reinforcement Learning](https://doi.org/10.18653/v1/2023.emnlp-main.207) |  | 0 | Cross-lingual transfer learning heavily relies on well-aligned cross-lingual representations. The syntactic structure is recognized as beneficial for cross-lingual transfer, but limited researches utilize it for aligning representation in multilingual pre-trained language models (PLMs).... | Linjuan Wu, Weiming Lu |  |
| 1402 |  |  [AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification](https://doi.org/10.18653/v1/2023.emnlp-main.208) |  | 0 | Recent work has found that few-shot sentence classification based on pre-trained Sentence Encoders (SEs) is efficient, robust, and effective. In this work, we investigate strategies for domain-specialization in the context of few-shot sentence classification with SEs. We first establish that... | Goran Glavas, Iryna Gurevych, Kexin Wang, Raj Nath Patel, Sourav Dutta, Yongxin Huang |  |
| 1403 |  |  [Interview Evaluation: A Novel Approach for Automatic Evaluation of Conversational Question Answering Models](https://doi.org/10.18653/v1/2023.emnlp-main.209) |  | 0 | Conversational Question Answering (CQA) aims to provide natural language answers to users in information-seeking dialogues. Existing CQA benchmarks often evaluate models using pre-collected human-human conversations. However, replacing the model-predicted dialogue history with ground truth... | Ai Ti Aw, Bowei Zou, Xibo Li, Yanling Li, Yifan Fan, Yu Hong |  |
| 1404 |  |  [TCFLE-8: a Corpus of Learner Written Productions for French as a Foreign Language and its Application to Automated Essay Scoring](https://doi.org/10.18653/v1/2023.emnlp-main.210) |  | 0 | Automated Essay Scoring (AES) aims to automatically assess the quality of essays. Automation enables large-scale assessment, improvements in consistency, reliability, and standardization. Those characteristics are of particular relevance in the context of language certification exams. However, a... | Alice Pintard, David Alfter, Rodrigo Wilkens, Thomas François, Vincent Folny |  |
| 1405 |  |  [Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA](https://doi.org/10.18653/v1/2023.emnlp-main.211) |  | 0 | Large language models (e.g., GPT-4) are uniquely capable of producing highly rated text simplification, yet current human evaluation methods fail to provide a clear understanding of systems’ specific strengths and weaknesses. To address this limitation, we introduce SALSA, an edit-based human... | David Heineman, Mounica Maddela, Wei Xu, Yao Dou |  |
| 1406 |  |  [Confidence-based Ensembling of Perspective-aware Models](https://doi.org/10.18653/v1/2023.emnlp-main.212) |  | 0 | Research in the field of NLP has recently focused on the variability that people show in selecting labels when performing an annotation task. Exploiting disagreements in annotations has been shown to offer advantages for accurate modelling and fair evaluation. In this paper, we propose a strongly... | Alessandra Teresa Cignarella, Cristina Bosco, Silvia Casola, Simona Frenda, Soda Marem Lo, Valerio Basile, Viviana Patti |  |
| 1407 |  |  [ToViLaG: Your Visual-Language Generative Model is Also An Evildoer](https://doi.org/10.18653/v1/2023.emnlp-main.213) |  | 0 | Recent large-scale Visual-Language Generative Models (VLGMs) have achieved unprecedented improvement in multimodal image/text generation. However, these models might also generate toxic content, e.g., offensive text and pornography images, raising significant ethical risks. Despite exhaustive... | Han Jiang, Shanlin Zhou, Xiaoyuan Yi, Xing Xie, Xinpeng Wang, Zhihua Wei |  |
| 1408 |  |  [GPT-RE: In-context Learning for Relation Extraction using Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.214) |  | 0 | In spite of the potential for ground-breaking achievements offered by large language models (LLMs) (e.g., GPT-3) via in-context learning (ICL), they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major... | Fei Cheng, Haiyue Song, Jiwei Li, Qianying Liu, Sadao Kurohashi, Zhen Wan, Zhuoyuan Mao |  |
| 1409 |  |  [Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment](https://doi.org/10.18653/v1/2023.emnlp-main.215) |  | 0 | Designing systems that can reason across cultures requires that they are grounded in the norms of the contexts in which they operate. However, current research on developing computational models of social norms has primarily focused on American society. Here, we propose a novel approach to discover... | Arkadiy Saakyan, Oliver Li, Sky CHWang, Smaranda Muresan, Zhou Yu |  |
| 1410 |  |  [INFORM : Information eNtropy based multi-step reasoning FOR large language Models](https://doi.org/10.18653/v1/2023.emnlp-main.216) |  | 0 | Large language models (LLMs) have demonstrated exceptional performance in reasoning tasks with dedicated Chain-of-Thought (CoT) prompts. Further enhancing CoT prompts with exquisite exemplars can significantly improve reasoning performance.However, the effectiveness of CoT prompts may fluctuate... | Chuyue Zhou, Jing Ye, Juntao Li, Kehai Chen, Min Zhang, Wangjie You |  |
| 1411 |  |  [Adaptive Gating in Mixture-of-Experts based Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.217) |  | 0 | Large language models have demonstrated exceptional language understanding capabilities in many NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE models adopt a... | Cong Wang, Hong Xu, Jiamin Li, Qiang Su, Yimin Jiang, Yitao Yang |  |
| 1412 |  |  [On the Automatic Generation and Simplification of Children's Stories](https://doi.org/10.18653/v1/2023.emnlp-main.218) |  | 0 | With recent advances in large language models (LLMs), the concept of automatically generating children’s educational materials has become increasingly realistic. Working toward the goal of age-appropriate simplicity in generated educational texts, we first examine the ability of several popular... | Eliana Colunga, Jennifer Weber, Jesus Salcido, Katharina von der Wense, Maria R. Valentini, Téa Wright |  |
| 1413 |  |  [When Do Decompositions Help for Machine Reading?](https://doi.org/10.18653/v1/2023.emnlp-main.219) |  | 0 | Answering complex questions often requires multi-step reasoning in order to obtain the final answer. Most research into decompositions of complex questions involves open-domain systems, which have shown success in using these decompositions for improved retrieval. In the machine reading setting,... | Benjamin Van Durme, Dawn J. Lawrie, Kangda Wei, Orion Weller, Yunmo Chen |  |
| 1414 |  |  [The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.220) |  | 0 | Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior... | Avi Caciularu, Aviv Slobodkin, Ido Dagan, Omer Goldman, Shauli Ravfogel |  |
| 1415 |  |  [Identifying Informational Sources in News Articles](https://doi.org/10.18653/v1/2023.emnlp-main.221) |  | 0 | News articles are driven by the informational sources journalists use in reporting. Modeling when, how and why sources get used together in stories can help us better understand the information we consume and even help journalists with the task of producing it. In this work, we take steps toward... | Alexander Spangher, Emilio Ferrara, Jonathan May, Nanyun Peng |  |
| 1416 |  |  [Retrofitting Light-weight Language Models for Emotions using Supervised Contrastive Learning](https://doi.org/10.18653/v1/2023.emnlp-main.222) |  | 0 | We present a novel retrofitting method to induce emotion aspects into pre-trained language models (PLMs) such as BERT and RoBERTa. Our method updates pre-trained network weights using contrastive learning so that the text fragments exhibiting similar emotions are encoded nearby in the... | Pushpak Bhattacharyya, Sapan Shah, Sreedhar Reddy |  |
| 1417 |  |  [Longtriever: a Pre-trained Long Text Encoder for Dense Document Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.223) |  | 0 | Pre-trained language models (PLMs) have achieved the preeminent position in dense retrieval due to their powerful capacity in modeling intrinsic semantics. However, most existing PLM-based retrieval models encounter substantial computational costs and are infeasible for processing long documents.... | Chaozhuo Li, Guangzhong Sun, Junhan Yang, Xing Xie, Zheng Liu |  |
| 1418 |  |  [Revisiting De-Identification of Electronic Medical Records: Evaluation of Within- and Cross-Hospital Generalization](https://doi.org/10.18653/v1/2023.emnlp-main.224) |  | 0 | The de-identification task aims to detect and remove the protected health information from electronic medical records (EMRs). Previous studies generally focus on the within-hospital setting and achieve great successes, while the cross-hospital setting has been overlooked. This study introduces a... | Enwei Zhu, Jinpeng Li, Yiyang Liu |  |
| 1419 |  |  [Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.225) |  | 0 | Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A... | Gurusha Juneja, Soumen Chakrabarti, Subhabrata Dutta, Sunny Manchanda, Tanmoy Chakraborty |  |
| 1420 |  |  [Language Representation Projection: Can We Transfer Factual Knowledge across Languages in Multilingual Language Models?](https://doi.org/10.18653/v1/2023.emnlp-main.226) |  | 0 | Multilingual pretrained language models serve as repositories of multilingual factual knowledge. Nevertheless, a substantial performance gap of factual knowledge probing exists between high-resource languages and low-resource languages, suggesting limited implicit factual knowledge transfer across... | Deyi Xiong, Junzhuo Li, Shaoyang Xu |  |
| 1421 |  |  [Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.227) |  | 0 | Abstract grammatical knowledge—of parts of speech and grammatical patterns—is key to the capacity for linguistic generalization in humans. But how abstract is grammatical knowledge in large language models? In the human literature, compelling evidence for grammatical abstraction comes from... | Ben Bergen, Catherine Arnett, James A. Michaelov, Tyler A. Chang |  |
| 1422 |  |  [ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph](https://doi.org/10.18653/v1/2023.emnlp-main.228) |  | 0 | Question Answering over Knowledge Graph (KGQA) aims to seek answer entities for the natural language question from a large-scale Knowledge Graph (KG). To better perform reasoning on KG, recent work typically adopts a pre-trained language model (PLM) to model the question, and a graph neural network... | JiRong Wen, Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yaliang Li |  |
| 1423 |  |  [Deep Natural Language Feature Learning for Interpretable Prediction](https://doi.org/10.18653/v1/2023.emnlp-main.229) |  | 0 | We propose a general method to break down a main complex task into a set of intermediary easier sub-tasks, which are formulated in natural language as binary questions related to the final target task. Our method allows for representing each example by a vector consisting of the answers to these... | Cristian Buc Calderon, Felipe Urrutia, Valentin Barrière |  |
| 1424 |  |  [ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.230) |  | 0 | As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing... | Adina Williams, David Esiobu, Eleonora Presani, Eric Michael Smith, Jane DwivediYu, Jude Fernandes, Megan Ung, Saghar Hosseini, Xiaoqing Ellen Tan, Yuchen Zhang |  |
| 1425 |  |  [Enhancing Task-oriented Dialogue Systems with Generative Post-processing Networks](https://doi.org/10.18653/v1/2023.emnlp-main.231) |  | 0 | Recently, post-processing networks (PPNs), which modify the outputs of arbitrary modules including non-differentiable ones in task-oriented dialogue systems, have been proposed. PPNs have successfully improved the dialogue performance by post-processing natural language understanding (NLU),... | Atsumoto Ohashi, Ryuichiro Higashinaka |  |
| 1426 |  |  [Adapting Language Models to Compress Contexts](https://doi.org/10.18653/v1/2023.emnlp-main.232) |  | 0 | Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models... | Alexander Wettig, Alexis Chevalier, Anirudh Ajith, Danqi Chen |  |
| 1427 |  |  [Selective Labeling: How to Radically Lower Data-Labeling Costs for Document Extraction Models](https://doi.org/10.18653/v1/2023.emnlp-main.233) |  | 0 | Building automatic extraction models for visually rich documents like invoices, receipts, bills, tax forms, etc. has received significant attention lately. A key bottleneck in developing extraction models for new document types is the cost of acquiring the several thousand high-quality labeled... | James B. Wendt, Jing Xie, Navneet Potti, Sandeep Tata, Yichao Zhou |  |
| 1428 |  |  [TRAVEL: Tag-Aware Conversational FAQ Retrieval via Reinforcement Learning](https://doi.org/10.18653/v1/2023.emnlp-main.234) |  | 0 | Efficiently retrieving FAQ questions that match users’ intent is essential for online customer service. Existing methods aim to fully utilize the dynamic conversation context to enhance the semantic association between the user query and FAQ questions. However, the conversation context contains... | Chen Huang, Dingnan Jin, Jia Liu, Wenqiang Lei, Yue Chen |  |
| 1429 |  |  [Continual Dialogue State Tracking via Example-Guided Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.235) |  | 0 | Dialogue systems are frequently updated to accommodate new services, but naively updating them by continually training with data for new services in diminishing performance on previously learnt services. Motivated by the insight that dialogue state tracking (DST), a crucial component of dialogue... | Andrea Madotto, Chinnadhurai Sankar, Hyundong Cho, Jing Xu, Jonathan May, Khyathi Raghavi Chandu, Satwik Kottur, Zhaojiang Lin |  |
| 1430 |  |  [Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media](https://doi.org/10.18653/v1/2023.emnlp-main.236) |  | 0 | Claim span identification (CSI) is an important step in fact-checking pipelines, aiming to identify text segments that contain a check-worthy claim or assertion in a social media post. Despite its importance to journalists and human fact-checkers, it remains a severely understudied problem, and the... | Megha Sundriyal, Preslav Nakov, Shubham Mittal |  |
| 1431 |  |  [COVID-19 Vaccine Misinformation in Middle Income Countries](https://doi.org/10.18653/v1/2023.emnlp-main.237) |  | 0 | This paper introduces a multilingual dataset of COVID-19 vaccine misinformation, consisting of annotated tweets from three middle-income countries: Brazil, Indonesia, and Nigeria. The expertly curated dataset includes annotations for 5,952 tweets, assessing their relevance to COVID-19 vaccines,... | Aditya Agrawal, Byeo Bak, Derry Wijaya, Jiaxi Wu, Jongin Kim, Traci Hong, Veronika J. Wirtz |  |
| 1432 |  |  [Contrastive Learning of Sentence Embeddings from Scratch](https://doi.org/10.18653/v1/2023.emnlp-main.238) |  | 0 | Contrastive learning has been the dominant approach to train state-of-the-art sentence embeddings. Previous studies have typically learned sentence embeddings either through the use of human-annotated natural language inference (NLI) data or via large-scale unlabeled sentences in an unsupervised... | Junlei Zhang, Junxian He, Zhenzhong Lan |  |
| 1433 |  |  [A Rose by Any Other Name would not Smell as Sweet: Social Bias in Names Mistranslation](https://doi.org/10.18653/v1/2023.emnlp-main.239) |  | 0 | We ask the question: Are there widespread disparities in machine translations of names across race/ethnicity, and gender? We hypothesize that the translation quality of names and surrounding context will be lower for names associated with US racial and ethnic minorities due to these systems’... | Hal Daumé III, Jieyu Zhao, Marine Carpuat, Sandra Sandoval |  |
| 1434 |  |  [Investigating Efficiently Extending Transformers for Long Input Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.240) |  | 0 | While large pretrained Transformer models have proven highly capable at tackling natural language tasks, handling long sequence inputs still poses a significant challenge. One such task is long input summarization, where inputs are longer than the maximum input context of most models. Through an... | Jason Phang, Peter J. Liu, Yao Zhao |  |
| 1435 |  |  [CS2W: A Chinese Spoken-to-Written Style Conversion Dataset with Multiple Conversion Types](https://doi.org/10.18653/v1/2023.emnlp-main.241) |  | 0 | Spoken texts (either manual or automatic transcriptions from automatic speech recognition (ASR)) often contain disfluencies and grammatical errors, which pose tremendous challenges to downstream tasks. Converting spoken into written language is hence desirable. Unfortunately, the availability of... | Deyi Xiong, Linhao Yu, Minghui Xu, Renren Jin, Zishan Guo |  |
| 1436 |  |  [Unifying Cross-Lingual Transfer across Scenarios of Resource Scarcity](https://doi.org/10.18653/v1/2023.emnlp-main.242) |  | 0 | The scarcity of data in many of the world’s languages necessitates the transfer of knowledge from other, resource-rich languages. However, the level of scarcity varies significantly across multiple dimensions, including: i) the amount of task-specific data available in the source and target... | Alan Ansell, Anna Korhonen, Edoardo M. Ponti, Ivan Vulic, Marinela Parovic |  |
| 1437 |  |  [A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.243) |  | 0 | Recent instruction fine-tuned models can solve multiple NLP tasks when prompted to do so, with machine translation (MT) being a prominent use case. However, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. In MT, this... | Anne Lauscher, Debora Nozza, Flor Miriam Plaza del Arco, Giuseppe Attanasio |  |
| 1438 |  |  [DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining](https://doi.org/10.18653/v1/2023.emnlp-main.244) |  | 0 | Many text mining models are constructed by fine-tuning a large deep pre-trained language model (PLM) in downstream tasks. However, a significant challenge that arises nowadays is how to maintain performance when we use a lightweight model with limited labeled samples. We present DisCo, a... | Chenghua Lin, Jianxin Li, Qianren Mao, Ting Deng, Weifeng Jiang, Weiyi Yang, Zheng Wang |  |
| 1439 |  |  [Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation](https://doi.org/10.18653/v1/2023.emnlp-main.245) |  | 0 | Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) to comprehend instructions and generate appropriate responses. Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook... | Da Yin, Fan Yin, Hritik Bansal, Jiawei Han, KaiWei Chang, Ming Zhong, Xiao Liu |  |
| 1440 |  |  [Are All Steps Equally Important? Benchmarking Essentiality Detection in Event Processes](https://doi.org/10.18653/v1/2023.emnlp-main.246) |  | 0 | Natural language often describes events in different granularities, such that more coarse-grained (goal) events can often be decomposed into fine-grained sequences of (step) events. A critical but overlooked challenge in understanding an event process lies in the fact that the step events are not... | Dan Roth, Haoyu Wang, Hongming Zhang, Muhao Chen, Yueguan Wang, Yuqian Deng |  |
| 1441 |  |  [Language Model is Suitable for Correction of Handwritten Mathematical Expressions Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.247) |  | 0 | Handwritten mathematical expression recognition (HMER) is a multidisciplinary task that generates LaTeX sequences from images. Existing approaches, employing tree decoders within attention-based encoder-decoder architectures, aim to capture the hierarchical tree structure, but are limited by CFGs... | Chaofan Yang, Jiaqi Han, Yi Zhou, Zui Chen |  |
| 1442 |  |  [Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection](https://doi.org/10.18653/v1/2023.emnlp-main.248) |  | 0 | Cross-lingual transfer learning from high-resource to medium and low-resource languages has shown encouraging results. However, the scarcity of resources in target languages remains a challenge. In this work, we resort to data augmentation and continual pre-training for domain adaptation to improve... | Goran Glavas, Gretel Liz De la Peña Sarracén, Paolo Rosso, Robert Litschko, Simone Paolo Ponzetto |  |
| 1443 |  |  [SuperDialseg: A Large-scale Dataset for Supervised Dialogue Segmentation](https://doi.org/10.18653/v1/2023.emnlp-main.249) |  | 0 | Dialogue segmentation is a crucial task for dialogue systems allowing a better understanding of conversational texts. Despite recent progress in unsupervised dialogue segmentation methods, their performances are limited by the lack of explicit supervised signals for training. Furthermore, the... | Akiko Aizawa, Chengzhang Dong, Junfeng Jiang, Sadao Kurohashi |  |
| 1444 |  |  [ATFormer: A Learned Performance Model with Transfer Learning Across Devices for Deep Learning Tensor Programs](https://doi.org/10.18653/v1/2023.emnlp-main.250) |  | 0 | The training and inference efficiency of ever-larger deep neural networks highly rely on the performance of tensor operators on specific hardware platforms. Therefore, a compilation-based optimization flow with automatic tensor generation and parameter tuning is necessary for efficient model... | Bei Yu, Shuo Yin, Wenqian Zhao, Yang Bai, Zixiao Wang |  |
| 1445 |  |  [mRedditSum: A Multimodal Abstractive Summarization Dataset of Reddit Threads with Images](https://doi.org/10.18653/v1/2023.emnlp-main.251) |  | 0 | The growing number of multimodal online discussions necessitates automatic summarization to save time and reduce content overload. However, existing summarization datasets are not suitable for this purpose, as they either do not cover discussions, multiple modalities, or both. To this end, we... | Fatemeh Pesaran Zadeh, Gunhee Kim, Jaewoo Ahn, Joonsuk Park, Keighley Overbay |  |
| 1446 |  |  [Sparse Low-rank Adaptation of Pre-trained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.252) |  | 0 | Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. The popular method of low-rank adaptation (LoRA) offers a notable approach, hypothesizing that the adaptation process is intrinsically low-dimensional. Although LoRA... | Bowen Zhou, Maosong Sun, Ning Ding, Qiaosen Wang, Xingtai Lv, Yulin Chen, Zhiyuan Liu |  |
| 1447 |  |  [Human Learning by Model Feedback: The Dynamics of Iterative Prompting with Midjourney](https://doi.org/10.18653/v1/2023.emnlp-main.253) |  | 0 | Generating images with a Text-to-Image model often requires multiple trials, where human users iteratively update their prompt based on feedback, namely the output image. Taking inspiration from cognitive work on reference games and dialogue alignment, this paper analyzes the dynamics of the user... | Leshem Choshen, Omri Abend, Shachar DonYehiya |  |
| 1448 |  |  [ULF: Unsupervised Labeling Function Correction using Cross-Validation for Weak Supervision](https://doi.org/10.18653/v1/2023.emnlp-main.254) |  | 0 | A cost-effective alternative to manual data labeling is weak supervision (WS), where data samples are automatically annotated using a predefined set of labeling functions (LFs), rule-based mechanisms that generate artificial labels for the associated classes. In this work, we investigate noise... | Anastasiia Sedova, Benjamin Roth |  |
| 1449 |  |  [The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.255) |  | 0 | Chain-of-Thought (CoT) prompting enables large language models to solve complex reasoning problems by generating intermediate steps. However, confined by its inherent single-pass and sequential generation process, CoT heavily relies on the initial decisions, causing errors in early steps to... | Di Jin, Jingyuan Qi, Lifu Huang, Minqian Liu, Qifan Wang, Ying Shen, Zhiyang Xu |  |
| 1450 |  |  [Ideology Takes Multiple Looks: A High-Quality Dataset for Multifaceted Ideology Detection](https://doi.org/10.18653/v1/2023.emnlp-main.256) |  | 0 | Ideology detection (ID) is important for gaining insights about peoples’ opinions and stances on our world and society, which can find many applications in politics, economics and social sciences. It is not uncommon that a piece of text can contain descriptions of various issues. It is also widely... | Bang Wang, Han Yu, Lixiao Wei, Minghua Xu, Songtao Liu, Wei Xiang, Ziling Luo, Ziyao Wei |  |
| 1451 |  |  [Transductive Learning for Textual Few-Shot Classification in API-based Embedding Models](https://doi.org/10.18653/v1/2023.emnlp-main.257) |  | 0 | Proprietary and closed APIs are becoming increasingly common to process natural language, and are impacting the practical applications of natural language processing, including few-shot classification. Few-shot classification involves training a model to perform a new classification task with a... | Ismail Ben Ayed, Malik Boudiaf, Myriam Tami, Pablo Piantanida, Pierre Colombo, Victor Pellegrain, Victor Storchan |  |
| 1452 |  |  [MEGA: Multilingual Evaluation of Generative AI](https://doi.org/10.18653/v1/2023.emnlp-main.258) |  | 0 | Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that... | Akshay Uttama Nambi, Harshita Diddee, Kabir Ahuja, Kalika Bali, Krithika Ramesh, Millicent Ochieng, Mohamed Ahmed, Prachi Jain, Rishav Hada, Sameer Segal, Sunayana Sitaram, Tanuja Ganu |  |
| 1453 |  |  [Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation](https://doi.org/10.18653/v1/2023.emnlp-main.259) |  | 0 | Mis- and disinformation online have become a major societal problem as major sources of online harms of different kinds. One common form of mis- and disinformation is out-of-context (OOC) information, where different pieces of information are falsely associated, e.g., a real image combined with a... | Jie Guo, Shujun Li, Weidong Qiu, Xin Yuan, Zheng Huang |  |
| 1454 |  |  [Video-Helpful Multimodal Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.260) |  | 0 | Existing multimodal machine translation (MMT) datasets consist of images and video captions or instructional video subtitles, which rarely contain linguistic ambiguity, making visual information ineffective in generating appropriate translations. Recent work has constructed an ambiguous subtitles... | Chenhui Chu, Sadao Kurohashi, Shuichiro Shimizu, Wei Li, Yihang Li |  |
| 1455 |  |  [Large Language Models are Temporal and Causal Reasoners for Video Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.261) |  | 0 | Large Language Models (LLMs) have shown remarkable performances on a wide range of natural language understanding and generation tasks. We observe that the LLMs provide effective priors in exploiting linguistic shortcuts for temporal and causal reasoning in Video Question Answering (VideoQA).... | Byungseok Roh, Dohwan Ko, Hyunwoo Kim, Ji Soo Lee, WooYoung Kang |  |
| 1456 |  |  [Uncertainty Guided Global Memory Improves Multi-Hop Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.262) |  | 0 | Transformers have become the gold standard for many natural language processing tasks and, in particular, for multi-hop question answering (MHQA). This task includes processing a long document and reasoning over the multiple parts of it. The landscape of MHQA approaches can be classified into two... | Alsu Sagirova, Mikhail Burtsev |  |
| 1457 |  |  [Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation](https://doi.org/10.18653/v1/2023.emnlp-main.263) |  | 0 | The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods... | Hanlun Zhu, Jianing Wang, Lei Wang, Weining Qian, Yuanyuan Liang, Yunshi Lan |  |
| 1458 |  |  [TrojanSQL: SQL Injection against Natural Language Interface to Database](https://doi.org/10.18653/v1/2023.emnlp-main.264) |  | 0 | The technology of text-to-SQL has significantly enhanced the efficiency of accessing and manipulating databases. However, limited research has been conducted to study its vulnerabilities emerging from malicious user interaction. By proposing TrojanSQL, a backdoor-based SQL injection framework for... | Binyuan Hui, Jinchuan Zhang, Songlin Hu, Yan Zhou, Yaxin Liu, Ziming Li |  |
| 1459 |  |  [Preserving Privacy Through Dememorization: An Unlearning Technique For Mitigating Memorization Risks In Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.265) |  | 0 | Large Language models (LLMs) are trained on vast amounts of data, including sensitive information that poses a risk to personal privacy if exposed. LLMs have shown the ability to memorize and reproduce portions of their training data when prompted by adversaries. Prior research has focused on... | Aly M. Kassem, Omar Mahmoud, Sherif Saad |  |
| 1460 |  |  [MingOfficial: A Ming Official Career Dataset and a Historical Context-Aware Representation Learning Framework](https://doi.org/10.18653/v1/2023.emnlp-main.266) |  | 0 | In Chinese studies, understanding the nuanced traits of historical figures, often not explicitly evident in biographical data, has been a key interest. However, identifying these traits can be challenging due to the need for domain expertise, specialist knowledge, and context-specific insights,... | Bert Chan, HsinYi Hsieh, Richard TzongHan Tsai, YiHsuan Lin, Yingtao Tian, YouJun Chen, Yu Lin, YuSin Liu |  |
| 1461 |  |  [DPP-TTS: Diversifying prosodic features of speech via determinantal point processes](https://doi.org/10.18653/v1/2023.emnlp-main.267) |  | 0 | With the rapid advancement in deep generative models, recent neural Text-To-Speech(TTS) models have succeeded in synthesizing human-like speech. There have been some efforts to generate speech with various prosody beyond monotonous prosody patterns. However, previous works have several limitations.... | Hyukhun Koh, Kyomin Jung, Seongho Joo |  |
| 1462 |  |  [Meta-Learning Online Adaptation of Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.268) |  | 0 | Large language models encode impressively broad world knowledge in their parameters. However, the knowledge in static language models falls out of date, limiting the model’s effective “shelf life.” While online fine-tuning can reduce this degradation, we find that naively fine-tuning on a stream of... | Chelsea Finn, Christopher D. Manning, Eric Mitchell, Nathan Hu |  |
| 1463 |  |  [Self-Detoxifying Language Models via Toxification Reversal](https://doi.org/10.18653/v1/2023.emnlp-main.269) |  | 0 | Language model detoxification aims to minimize the risk of generating offensive or harmful content in pretrained language models (PLMs) for safer deployment. Existing methods can be roughly categorized as finetuning-based and decoding-based. However, the former is often resource-intensive, while... | Chak Tou Leong, Jian Wang, Jiashuo Wang, Wenjie Li, Yi Cheng |  |
| 1464 |  |  [Interactive Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.270) |  | 0 | Users interact with text, image, code, or other editors on a daily basis. However, machine learning models are rarely trained in the settings that reflect the interactivity between users and their editor. This is understandable as training AI models with real users is not only slow and costly, but... | Baolin Peng, Bill Dolan, Felix Faltings, Jianfeng Gao, Kianté Brantley, Michel Galley, Weixin Cai, Yizhe Zhang |  |
| 1465 |  |  [Knowledge Distillation \approx Label Smoothing: Fact or Fallacy?](https://doi.org/10.18653/v1/2023.emnlp-main.271) |  | 0 | Originally proposed as a method for knowledge transfer from one model to another, some recent studies have suggested that knowledge distillation (KD) is in fact a form of regularization. Perhaps the strongest argument of all for this new perspective comes from its apparent similarities with label... | Md. Sultan |  |
| 1466 |  |  [Analyzing Cognitive Plausibility of Subword Tokenization](https://doi.org/10.18653/v1/2023.emnlp-main.272) |  | 0 | Subword tokenization has become the de-facto standard for tokenization although comparative evaluations of their quality across languages are scarce. Existing evaluation studies focus on the effect of a tokenization algorithm on the performance in downstream tasks, or on engineering criteria such... | Lisa Beinborn, Yuval Pinter |  |
| 1467 |  |  [POE: Process of Elimination for Multiple Choice Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.273) |  | 0 | Language models (LMs) are capable of conducting in-context learning for multiple choice reasoning tasks, but the options in these tasks are treated equally. As humans often first eliminate wrong options before picking the final correct answer, we argue a similar two-step strategy can make LMs... | Chenkai Ma, Xinya Du |  |
| 1468 |  |  [NeuSTIP: A Neuro-Symbolic Model for Link and Time Prediction in Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.274) |  | 0 | Neuro-symbolic (NS) models for knowledge graph completion (KGC) combine the benefits of symbolic models (interpretable inference) with those of distributed representations (parameter sharing, high accuracy). While several NS models exist for KGs with static facts, there is limited work on temporal... | Garima Gaur, Ishaan Singh, Mausam, Navdeep Kaur |  |
| 1469 |  |  [Standardizing Distress Analysis: Emotion-Driven Distress Identification and Cause Extraction (DICE) in Multimodal Online Posts](https://doi.org/10.18653/v1/2023.emnlp-main.275) |  | 0 | Due to its growing impact on public opinion, hate speech on social media has garnered increased attention. While automated methods for identifying hate speech have been presented in the past, they have mostly been limited to analyzing textual content. The interpretability of such models has... | Asif Ekbal, Atul Verma, Chetna Painkra, Gopendra Vikram Singh, Soumitra Ghosh |  |
| 1470 |  |  [Out-of-Distribution Generalization in Natural Language Processing: Past, Present, and Future](https://doi.org/10.18653/v1/2023.emnlp-main.276) |  | 0 | Machine learning (ML) systems in natural language processing (NLP) face significant challenges in generalizing to out-of-distribution (OOD) data, where the test distribution differs from the training data distribution. This poses important questions about the robustness of NLP models and their high... | Chenyang Lyu, Jennifer Foster, Jindong Wang, Jingming Zhuo, Lingqiao Liu, Linyi Yang, Xuan Ren, Yaoxian Song, Yidong Wang, Yue Zhang |  |
| 1471 |  |  [Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.277) |  | 0 | Recent advances in prompt engineering enable large language models (LLMs) to solve multi-hop logical reasoning problems with impressive accuracy. However, there is little existing work investigating the robustness of LLMs with few-shot prompting techniques. Therefore, we introduce a systematic... | Abulhair Saparov, Hongyi Zheng |  |
| 1472 |  |  [Can Large Language Models Capture Dissenting Human Voices?](https://doi.org/10.18653/v1/2023.emnlp-main.278) |  | 0 | Large language models (LLMs) have shown impressive achievements in solving a broad range of tasks. Augmented by instruction fine-tuning, LLMs have also been shown to generalize in zero-shot settings as well. However, whether LLMs closely align with the human disagreement distribution has not been... | James Thorne, Na An, Noah Lee |  |
| 1473 |  |  [DecoMT: Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.279) |  | 0 | This study investigates machine translation between related languages i.e., languages within the same family that share linguistic characteristics such as word order and lexical similarity. Machine translation through few-shot prompting leverages a small set of translation pair examples to generate... | Ai Ti Aw, Anoop Kunchukuttan, Nancy Chen, Raj Dabre, Ratish Puduppully |  |
| 1474 |  |  [Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.280) |  | 0 | Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pre-trained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer... | Hao Zhao, Jie Fu, Zhaofeng He |  |
| 1475 |  |  [Towards Building More Robust NER datasets: An Empirical Study on NER Dataset Bias from a Dataset Difficulty View](https://doi.org/10.18653/v1/2023.emnlp-main.281) |  | 0 | Recently, many studies have illustrated the robustness problem of Named Entity Recognition (NER) systems: the NER models often rely on superficial entity patterns for predictions, without considering evidence from the context. Consequently, even state-of-the-art NER models generalize poorly to... | Qi Zhang, Ruotian Ma, Xiaolei Wang, Xin Zhou, Xuanjing Huang |  |
| 1476 |  |  [GradSim: Gradient-Based Language Grouping for Effective Multilingual Training](https://doi.org/10.18653/v1/2023.emnlp-main.282) |  | 0 | Most languages of the world pose low-resource challenges to natural language processing models. With multilingual training, knowledge can be shared among languages. However, not all languages positively influence each other and it is an open research question how to select the most suitable set of... | Heike Adel, Hinrich Schütze, Jannik Strötgen, Lukas Lange, Mingyang Wang |  |
| 1477 |  |  [Discovering Universal Geometry in Embeddings with ICA](https://doi.org/10.18653/v1/2023.emnlp-main.283) |  | 0 | This study utilizes Independent Component Analysis (ICA) to unveil a consistent semantic structure within embeddings of words or images. Our approach extracts independent semantic components from the embeddings of a pre-trained model by leveraging anisotropic information that remains after the... | Hidetoshi Shimodaira, Hiroaki Yamagiwa, Momose Oyama |  |
| 1478 |  |  [Toward a Critical Toponymy Framework for Named Entity Recognition: A Case Study of Airbnb in New York City](https://doi.org/10.18653/v1/2023.emnlp-main.284) |  | 0 | Critical toponymy examines the dynamics of power, capital, and resistance through place names and the sites to which they refer. Studies here have traditionally focused on the semantic content of toponyms and the top-down institutional processes that produce them. However, they have generally... | Clara Féré, Grant McKenzie, Jack LaViolette, Mikael Brunila, Priyanka Verma, Sky CHWang |  |
| 1479 |  |  [Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.285) |  | 0 | Accurate knowledge selection is critical in knowledge-grounded dialogue systems. Towards a closer look at it, we offer a novel perspective to organize existing literature, i.e., knowledge selection coupled with, after, and before generation. We focus on the third under-explored category of study,... | Hongru Liang, Jun Wang, Lang Qin, Yao Zhang, Zhenglu Yang |  |
| 1480 |  |  [Merging Generated and Retrieved Knowledge for Open-Domain QA](https://doi.org/10.18653/v1/2023.emnlp-main.286) |  | 0 | Open-domain question answering (QA) systems are often built with retrieval modules. However, retrieving passages from a given source is known to suffer from insufficient knowledge coverage. Alternatively, prompting large language models (LLMs) to generate contextual passages based on their... | Honglak Lee, Lajanugen Logeswaran, Lu Wang, Moontae Lee, Muhammad Khalifa, Yunxiang Zhang |  |
| 1481 |  |  [Best of Both Worlds: Towards Improving Temporal Knowledge Base Question Answering via Targeted Fact Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.287) |  | 0 | Temporal question answering (QA) is a special category of complex question answering task that requires reasoning over facts asserting time intervals of events. Previous works have predominately relied on Knowledge Base Question Answering (KBQA) for temporal QA. One of the major challenges faced by... | Dinesh Khandelwal, Hima Karanam, L. Venkata Subramaniam, Nithish Kannen, Shajith Ikbal, Sumit Neelam, Udit Sharma |  |
| 1482 |  |  [Text Fact Transfer](https://doi.org/10.18653/v1/2023.emnlp-main.288) |  | 0 | Text style transfer is a prominent task that aims to control the style of text without inherently changing its factual content. To cover more text modification applications, such as adapting past news for current events and repurposing educational materials, we propose the task of text fact... | Jie Huang, Kevin ChenChuan Chang, Nishant Balepur |  |
| 1483 |  |  [A Cheaper and Better Diffusion Language Model with Soft-Masked Noise](https://doi.org/10.18653/v1/2023.emnlp-main.289) |  | 0 | Diffusion models that are based on iterative denoising have been recently proposed and leveraged in various generation tasks like image generation. Whereas, as a way inherently built for continuous data, existing diffusion models still have some limitations in modeling discrete data, e.g.,... | Alex Smola, Aston Zhang, Diyi Yang, Jiaao Chen, Mu Li |  |
| 1484 |  |  [Mirages. On Anthropomorphism in Dialogue Systems](https://doi.org/10.18653/v1/2023.emnlp-main.290) |  | 0 | Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism is inevitable, conscious and unconscious design choices can guide users to personify them to varying degrees. Encouraging users to relate to automated... | Amanda Cercas Curry, Gavin Abercrombie, Tanvi Dinkar, Verena Rieser, Zeerak Talat |  |
| 1485 |  |  [Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?](https://doi.org/10.18653/v1/2023.emnlp-main.291) |  | 0 | Neural language models (LMs) can be used to evaluate the truth of factual statements in two ways: they can be either queried for statement probabilities, or probed for internal representations of truthfulness. Past work has found that these two procedures sometimes disagree, and that probes tend to... | Dylan HadfieldMenell, Jacob Andreas, Kevin Liu, Stephen Casper |  |
| 1486 |  |  [KEBAP: Korean Error Explainable Benchmark Dataset for ASR and Post-processing](https://doi.org/10.18653/v1/2023.emnlp-main.292) |  | 0 | Automatic Speech Recognition (ASR) systems are instrumental across various applications, with their performance being critically tied to user satisfaction. Conventional evaluation metrics for ASR systems produce a singular aggregate score, which is insufficient for understanding specific system... | Chanjun Park, Heuiseok Lim, Hyeonseok Moon, Jaehyung Seo, Jinsung Kim, Seonmin Koo, Sugyeong Eo |  |
| 1487 |  |  [Adaptive Policy with Wait-k Model for Simultaneous Translation](https://doi.org/10.18653/v1/2023.emnlp-main.293) |  | 0 | Simultaneous machine translation (SiMT) requires a robust read/write policy in conjunction with a high-quality translation model. Traditional methods rely on either a fixed wait-k policy coupled with a standalone wait-k translation model, or an adaptive policy jointly trained with the translation... | Jing Wu, Kai Fan, Libo Zhao, Shushu Wang, Wei Luo, Zhongqiang Huang, Ziqian Zeng |  |
| 1488 |  |  [Cross-Document Event Coreference Resolution on Discourse Structure](https://doi.org/10.18653/v1/2023.emnlp-main.294) |  | 0 | Cross-document event coreference resolution (CD-ECR) is a task of clustering event mentions across multiple documents that refer to the same real-world events. Previous studies usually model the CD-ECR task as a pairwise similarity comparison problem by using different event mention features, and... | Peifeng Li, Qiaoming Zhu, Sheng Xu, Xinyu Chen |  |
| 1489 |  |  [Post-hoc Utterance Refining Method by Entity Mining for Faithful Knowledge Grounded Conversations](https://doi.org/10.18653/v1/2023.emnlp-main.295) |  | 0 | Despite the striking advances in recent language generation performance, model-generated responses have suffered from the chronic problem of hallucinations that are either untrue or unfaithful to a given source. Especially in the task of knowledge grounded conversation, the models are required to... | Heuiseok Lim, Hyeonseok Moon, Jeongwoo Lee, Jungwoo Lim, Junyoung Son, Kisu Yang, Suhyune Son, Yoonna Jang, Yuna Hur |  |
| 1490 |  |  [Can We Edit Factual Knowledge by In-Context Learning?](https://doi.org/10.18653/v1/2023.emnlp-main.296) |  | 0 | Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or outdated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However,... | Baobao Chang, Ce Zheng, Jingjing Xu, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu |  |
| 1491 |  |  [EDIS: Entity-Driven Image Search over Multimodal Web Content](https://doi.org/10.18653/v1/2023.emnlp-main.297) |  | 0 | Making image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce Entity-Driven Image Search (EDIS), a challenging dataset for cross-modal image search in... | Siqi Liu, TsuJui Fu, Weixi Feng, Wenhu Chen, William Wang |  |
| 1492 |  |  [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://doi.org/10.18653/v1/2023.emnlp-main.298) |  | 0 | Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing... | Federico Lebrón, James LeeThorp, Joshua Ainslie, Michiel de Jong, Sumit Sanghai, Yury Zemlyanskiy |  |
| 1493 |  |  [Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.299) |  | 0 | Recent work has shown that language models (LMs) have strong multi-step (i.e., procedural) reasoning capabilities. However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism. In this paper, we try to... | Alessandro Stolfo, Antoine Bosselut, Guangtao Zeng, Jiaoda Li, Mrinmaya Sachan, Wangchunshu Zhou, Yifan Hou, Yu Fei |  |
| 1494 |  |  [BiasX: "Thinking Slow" in Toxic Content Moderation with Explanations of Implied Social Biases](https://doi.org/10.18653/v1/2023.emnlp-main.300) |  | 0 | Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text... | Liwei Jiang, Maarten Sap, Sravani Nanduri, Tongshuang Wu, Yiming Zhang |  |
| 1495 |  |  [Text encoders bottleneck compositionality in contrastive vision-language models](https://doi.org/10.18653/v1/2023.emnlp-main.301) |  | 0 | Performant vision-language (VL) models like CLIP represent captions using a single vector. How much information about language is lost in this bottleneck? We first curate CompPrompts, a set of increasingly compositional image captions that VL models should be able to capture (e.g., single object,... | Amita Kamath, Jack Hessel, KaiWei Chang |  |
| 1496 |  |  [Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition](https://doi.org/10.18653/v1/2023.emnlp-main.302) |  | 0 | Large Language Models (LLMs) are increasingly being deployed in interactive contexts that involve direct user engagement, such as chatbots and writing assistants. These deployments are increasingly plagued by prompt injection and jailbreaking (collectively, prompt hacking), in which models are... | Anaum Khan, Anson Liu Kost, Chenglei Si, Christopher Carnahan, Jeremy Pinto, Jordan L. BoydGraber, LouisFrançois Bouchard, Sander Schulhoff, Svetlina Anati, Valen Tagliabue |  |
| 1497 |  |  [MMNMT: Modularizing Multilingual Neural Machine Translation with Flexibly Assembled MoE and Dense Blocks](https://doi.org/10.18653/v1/2023.emnlp-main.303) |  | 0 | Mixture-of-Experts (MoE) based sparse architectures can significantly increase model capacity with sublinear computational overhead, which are hence widely used in massively multilingual neural machine translation (MNMT). However, they are prone to overfitting on low-resource language translation.... | Baosong Yang, Deyi Xiong, Jun Xie, Shangjie Li, Shaolin Zhu, Xiangpeng Wei |  |
| 1498 |  |  [Localizing Active Objects from Egocentric Vision with Symbolic World Knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.304) |  | 0 | The ability to actively ground task instructions from an egocentric view is crucial for AI agents to accomplish tasks or assist humans virtually. One important step towards this goal is to localize and track key active objects that undergo major state change as a consequence of human... | Nanyun Peng, TeLin Wu, Yu Zhou |  |
| 1499 |  |  [Introducing Rhetorical Parallelism Detection: A New Task with Datasets, Metrics, and Baselines](https://doi.org/10.18653/v1/2023.emnlp-main.305) |  | 0 | Rhetoric, both spoken and written, involves not only content but also style. One common stylistic tool is parallelism: the juxtaposition of phrases which have the same sequence of linguistic (e.g., phonological, syntactic, semantic) features. Despite the ubiquity of parallelism, the field of... | David Chiang, Hildegund Müller, Justin DeBenedetto, Stephen Bothwell, Theresa Crnkovich |  |
| 1500 |  |  [Prompting is not a substitute for probability measurements in large language models](https://doi.org/10.18653/v1/2023.emnlp-main.306) |  | 0 | Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models’ probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby... | Jennifer Hu, Roger Levy |  |
| 1501 |  |  [Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings](https://doi.org/10.18653/v1/2023.emnlp-main.307) |  | 0 | Pre-trained language models (PLMs) have ignited a surge in demand for effective fine-tuning techniques, particularly in low-resource domains and languages. Active learning (AL), a set of algorithms designed to decrease labeling costs by minimizing label complexity, has shown promise in confronting... | Jan Snajder, Josip Jukic |  |
| 1502 |  |  [Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks](https://doi.org/10.18653/v1/2023.emnlp-main.308) |  | 0 | Data contamination has become prevalent and challenging with the rise of models pretrained on large automatically-crawled corpora. For closed models, the training data becomes a trade secret, and even for open models, it is not trivial to detect contamination. Strategies such as leaderboards with... | Alon Jacovi, Avi Caciularu, Omer Goldman, Yoav Goldberg |  |
| 1503 |  |  [CoLT5: Faster Long-Range Transformers with Conditional Computation](https://doi.org/10.18653/v1/2023.emnlp-main.309) |  | 0 | Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive – not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important,... | David C. Uthus, James LeeThorp, Joshua Ainslie, Mandy Guo, Michiel de Jong, Santiago Ontañón, Siddhartha Brahma, Sumit Sanghai, Tao Lei, Yi Tay, YunHsuan Sung, Yury Zemlyanskiy |  |
| 1504 |  |  [DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.310) |  | 0 | Dialogue State Tracking (DST), a key component of task-oriented conversation systems, represents user intentions by determining the values of pre-defined slots in an ongoing dialogue. Existing approaches use hand-crafted templates and additional slot information to fine-tune and prompt large... | Evelyn Duesterwald, Praveen Venkateswaran, Vatche Isahagian |  |
| 1505 |  |  [Cross-Cultural Analysis of Human Values, Morals, and Biases in Folk Tales](https://doi.org/10.18653/v1/2023.emnlp-main.311) |  | 0 | Folk tales are strong cultural and social influences in children’s lives, and they are known to teach morals and values. However, existing studies on folk tales are largely limited to European tales. In our study, we compile a large corpus of over 1,900 tales originating from 27 diverse cultures... | Lu Wang, Rada Mihalcea, Winston Wu |  |
| 1506 |  |  [Non-Programmers Can Label Programs Indirectly via Active Examples: A Case Study with Text-to-SQL](https://doi.org/10.18653/v1/2023.emnlp-main.312) |  | 0 | Can non-programmers annotate natural language utterances with complex programs that represent their meaning? We introduce APEL, a framework in which non-programmers select among candidate programs generated by a seed semantic parser (e.g., Codex). Since they cannot understand the candidate... | Charlie Snell, Dan Klein, Jason Eisner, Ruiqi Zhong |  |
| 1507 |  |  [LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers](https://doi.org/10.18653/v1/2023.emnlp-main.313) |  | 0 | Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language... | Alex Gu, Armando SolarLezama, Benjamin Lipkin, Cedegao E. Zhang, Joshua B. Tenenbaum, Roger Levy, Theo Olausson |  |
| 1508 |  |  [Non-autoregressive Streaming Transformer for Simultaneous Translation](https://doi.org/10.18653/v1/2023.emnlp-main.314) |  | 0 | Simultaneous machine translation (SiMT) models are trained to strike a balance between latency and translation quality. However, training these models to achieve high quality while maintaining low latency often leads to a tendency for aggressive anticipation. We argue that such issue stems from the... | Chenze Shao, Min Zhang, Shaolei Zhang, Shoutao Guo, Yang Feng, Zhengrui Ma |  |
| 1509 |  |  [ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing](https://doi.org/10.18653/v1/2023.emnlp-main.315) |  | 0 | English and Chinese, known as resource-rich languages, have witnessed the strong development of transformer-based language models for natural language processing tasks. Although Vietnam has approximately 100M people speaking Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and... | DucVu Nguyen, Kiet Van Nguyen, Nam Nguyen, Thang Phan |  |
| 1510 |  |  [RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.316) |  | 0 | How to identify semantic relations among entities in a document when only a few labeled documents are available? Few-shot document-level relation extraction (FSDLRE) is crucial for addressing the pervasive data scarcity problem in real-world scenarios. Metric-based meta-learning is an effective... | Aiwei Liu, Fukun Ma, Lijie Wen, Shiao Meng, Shuang Li, Xuming Hu, Yawen Yang |  |
| 1511 |  |  [GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.317) |  | 0 | Humans subconsciously engage in geospatial reasoning when reading articles. We recognize place names and their spatial relations in text and mentally associate them with their physical locations on Earth. Although pretrained language models can mimic this cognitive process using linguistic context,... | Muhao Chen, Wenxuan Zhou, YaoYi Chiang, Zekun Li |  |
| 1512 |  |  [Cross-Modal Conceptualization in Bottleneck Models](https://doi.org/10.18653/v1/2023.emnlp-main.318) |  | 0 | Concept Bottleneck Models (CBMs) assume that training examples (e.g., x-ray images) are annotated with high-level concepts (e.g., types of abnormalities), and perform classification by first predicting the concepts, followed by predicting the label relying on these concepts. However, the primary... | Alexey Kornaev, Bulat Ibragimov, Danis Alukaev, Ilya Pershin, Ivan Titov, Semen Kiselev, Vladimir Ivanov |  |
| 1513 |  |  [LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.319) |  | 0 | The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various... | EePeng Lim, Lei Wang, Lidong Bing, Roy KaWei Lee, Soujanya Poria, Wanyu Xu, Xing Xu, Yihuai Lan, Zhiqiang Hu |  |
| 1514 |  |  [DREAM: Deployment of Recombination and Ensembles in Argument Mining](https://doi.org/10.18653/v1/2023.emnlp-main.320) |  | 0 | Current approaches to Argument Mining (AM) tend to take a holistic or black-box view of the overall pipeline. This paper, in contrast, aims to provide a solution to achieve increased performance based on current components instead of independent all-new solutions. To that end, it presents the... | Abraham Bernstein, Cristina Sarasua, Florian Ruosch |  |
| 1515 |  |  [MILDSum: A Novel Benchmark Dataset for Multilingual Summarization of Indian Legal Case Judgments](https://doi.org/10.18653/v1/2023.emnlp-main.321) |  | 0 | Automatic summarization of legal case judgments is a practically important problem that has attracted substantial research efforts in many countries. In the context of the Indian judiciary, there is an additional complexity – Indian legal case judgments are mostly written in complex English, but a... | Debtanu Datta, Rajdeep Mukherjee, Saptarshi Ghosh, Shubham Soni |  |
| 1516 |  |  [Query Rewriting in Retrieval-Augmented Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.322) |  | 0 | Large Language Models (LLMs) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the... | Hai Zhao, Nan Duan, Pengcheng He, Xinbei Ma, Yeyun Gong |  |
| 1517 |  |  [PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation](https://doi.org/10.18653/v1/2023.emnlp-main.323) |  | 0 | Data augmentation is a widely used technique to address the problem of text classification when there is a limited amount of training data. Recent work often tackles this problem using large language models (LLMs) like GPT3 that can generate new examples given already available ones. In this work,... | Dzmitry Bahdanau, Gaurav Sahu, Issam H. Laradji, Olga Vechtomova |  |
| 1518 |  |  [COHESENTIA: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts](https://doi.org/10.18653/v1/2023.emnlp-main.324) |  | 0 | Coherence is a linguistic term that refers to the relations between small textual units (sentences, propositions), which make the text logically consistent and meaningful to the reader. With the advances of generative foundational models in NLP, there is a pressing need to automatically assess the... | Aviya Maimon, Reut Tsarfaty |  |
| 1519 |  |  [QUDeval: The Evaluation of Questions Under Discussion Discourse Parsing](https://doi.org/10.18653/v1/2023.emnlp-main.325) |  | 0 | Questions Under Discussion (QUD) is a versatile linguistic framework in which discourse progresses as continuously asking questions and answering them. Automatic parsing of a discourse to produce a QUD structure thus entails a complex question generation task: given a document and an answer... | Greg Durrett, Junyi Jessy Li, Ritika Mangla, Yating Wu |  |
| 1520 |  |  [PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter](https://doi.org/10.18653/v1/2023.emnlp-main.326) |  | 0 | The Retrieval Question Answering (ReQA) task employs the retrieval-augmented framework, composed of a retriever and generator. The generators formulate the answer based on the documents retrieved by the retriever. Incorporating Large Language Models (LLMs) as generators is beneficial due to their... | Haoyan Yang, Jianzong Wang, Jing Xiao, Ming Li, Ning Cheng, Yong Zhang, Zhitao Li |  |
| 1521 |  |  [Exploring Chain of Thought Style Prompting for Text-to-SQL](https://doi.org/10.18653/v1/2023.emnlp-main.327) |  | 0 | In-context learning with large language models (LLMs) has recently caught increasing attention due to its superior few-shot performance on various tasks. However, its performance on text-to-SQL parsing still has much room for improvement. In this paper, we hypothesize that a crucial aspect of LLMs... | ChangYu Tai, Huan Sun, Tianshu Zhang, Xiang Deng, Ziru Chen |  |
| 1522 |  |  [Efficient Algorithms for Recognizing Weighted Tree-Adjoining Languages](https://doi.org/10.18653/v1/2023.emnlp-main.328) |  | 0 | The class of tree-adjoining languages can be characterized by various two-level formalisms, consisting of a context-free grammar (CFG) or pushdown automaton (PDA) controlling another CFG or PDA. These four formalisms are equivalent to tree-adjoining grammars (TAG), linear indexed grammars (LIG),... | Alexandra Butoi, David Chiang, Ryan Cotterell, Tim Vieira |  |
| 1523 |  |  [Harnessing Black-Box Control to Boost Commonsense in LM's Generation](https://doi.org/10.18653/v1/2023.emnlp-main.329) |  | 0 | Large language models (LLMs) such as GPT-3 have demonstrated a strong capability to generate coherent and contextually relevant text. However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times. Moreover, fine-tuning the entire LLM towards more... | Felix Zhang, Nanyun Peng, Yufei Tian |  |
| 1524 |  |  [Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.330) |  | 0 | A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that... | Allan Zhou, Archit Sharma, Chelsea Finn, Christopher D. Manning, Eric Mitchell, Huaxiu Yao, Katherine Tian, Rafael Rafailov |  |
| 1525 |  |  [Representative Demonstration Selection for In-Context Learning with Two-Stage Determinantal Point Process](https://doi.org/10.18653/v1/2023.emnlp-main.331) |  | 0 | Although In-Context Learning has proven effective across a broad array of tasks, its efficiency is noticeably influenced by the selection of demonstrations. Existing methods tend to select different demonstrations for each test instance, which is time-consuming and poses limitations in practical... | Cao Liu, Dianbo Sui, Jun Zhao, Kang Liu, Yuanzhe Zhang, Zhao Yang |  |
| 1526 |  |  [The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.332) |  | 0 | Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might supply the answer “Edinburgh” to “Anne Redpath passed away in X.” and... | Denitsa Saynova, Lovisa Hagström, Moa Johansson, Richard Johansson, Tobias Norlund |  |
| 1527 |  |  [ViPE: Visualise Pretty-much Everything](https://doi.org/10.18653/v1/2023.emnlp-main.333) |  | 0 | Figurative and non-literal expressions are profoundly integrated in human communication. Visualising such expressions allow us to convey our creative thoughts, and evoke nuanced emotions. Recent text-to-image models like Stable Diffusion, on the other hand, struggle to depict non-literal... | Adhiraj Ghosh, Hassan Shahmohammadi, Hendrik P. A. Lensch |  |
| 1528 |  |  [Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.334) |  | 0 | Document-level Relation Extraction (DocRE), which aims to extract relations from a long context, is a critical challenge in achieving fine-grained structural comprehension and generating interpretable document representations. Inspired by recent advances in in-context learning capabilities emergent... | Junpeng Li, Zilong Zheng, Zixia Jia |  |
| 1529 |  |  [Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.335) |  | 0 | The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs. Here, we study an aspect of model... | Dan Jurafsky, Kaitlyn Zhou, Tatsunori Hashimoto |  |
| 1530 |  |  [Elaborative Simplification as Implicit Questions Under Discussion](https://doi.org/10.18653/v1/2023.emnlp-main.336) |  | 0 | Automated text simplification, a technique useful for making text more accessible to people such as children and emergent bilinguals, is often thought of as a monolingual translation task from complex sentences to simplified sentences using encoder-decoder models. This view fails to account for... | Junyi Jessy Li, Kyle Mahowald, William Sheffield, Yating Wu |  |
| 1531 |  |  [EntSUMv2: Dataset, Models and Evaluation for More Abstractive Entity-Centric Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.337) |  | 0 | Entity-centric summarization is a form of controllable summarization that aims to generate a summary for a specific entity given a document. Concise summaries are valuable in various real-life applications, as they enable users to quickly grasp the main points of the document focusing on an entity... | Daniel PreotiucPietro, Dhruv Mehra, Ella HofmannCoyle, Lingjue Xie, Mayank Kulkarni |  |
| 1532 |  |  [SciRepEval: A Multi-Format Benchmark for Scientific Document Representations](https://doi.org/10.18653/v1/2023.emnlp-main.338) |  | 0 | Learned representations of scientific documents can serve as valuable input features for downstream tasks without further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first... | Amanpreet Singh, Arman Cohan, Doug Downey, Mike D'Arcy, Sergey Feldman |  |
| 1533 |  |  [A Diachronic Perspective on User Trust in AI under Uncertainty](https://doi.org/10.18653/v1/2023.emnlp-main.339) |  | 0 | In human-AI collaboration, users typically form a mental model of the AI system, which captures the user’s beliefs about when the system performs well and when it does not. The construction of this mental model is guided by both the system’s veracity as well as the system output presented to the... | Mennatallah ElAssady, Mrinmaya Sachan, Shehzaad Dhuliawala, Vilém Zouhar |  |
| 1534 |  |  [CT-GAT: Cross-Task Generative Adversarial Attack based on Transferability](https://doi.org/10.18653/v1/2023.emnlp-main.340) |  | 0 | Neural network models are vulnerable to adversarial examples, and adversarial transferability further increases the risk of adversarial attacks. Current methods based on transferability often rely on substitute models, which can be impractical and costly in real-world scenarios due to the... | Chengwei Dai, Kun Li, Minxuan Lv, Songlin Hu, Wei Zhou |  |
| 1535 |  |  [Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling](https://doi.org/10.18653/v1/2023.emnlp-main.341) |  | 0 | Topic segmentation is critical for obtaining structured documents and improving down- stream tasks such as information retrieval. Due to its ability of automatically exploring clues of topic shift from abundant labeled data, recent supervised neural models have greatly promoted the development of... | Chong Deng, Hai Yu, Jiaqing Liu, Qian Chen, Qinglin Zhang, Wen Wang |  |
| 1536 |  |  [Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents](https://doi.org/10.18653/v1/2023.emnlp-main.342) |  | 0 | Human-like chatbots necessitate the use of commonsense reasoning in order to effectively comprehend and respond to implicit information present within conversations. Achieving such coherence and informativeness in responses, however, is a non-trivial task. Even for large language models (LLMs), the... | Dongha Lee, Dongyeop Kang, Hyungjoo Chae, Jinyoung Yeo, Kai Tzuiunn Ong, Minjin Kim, Taeyoon Kwon, Yongho Song, Youngjae Yu |  |
| 1537 |  |  [Information Value: Measuring Utterance Predictability as Distance from Plausible Alternatives](https://doi.org/10.18653/v1/2023.emnlp-main.343) |  | 0 | We present information value, a measure which quantifies the predictability of an utterance relative to a set of plausible alternatives. We introduce a method to obtain interpretable estimates of information value using neural text generators, and exploit their psychometric predictive power to... | Mario Giulianelli, Raquel Fernández, Sarenne Wallbridge |  |
| 1538 |  |  [Generating Commonsense Counterfactuals for Stable Relation Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.344) |  | 0 | Recent studies on counterfactual augmented data have achieved great success in the coarse-grained natural language processing tasks. However, existing methods encounter two major problems when dealing with the fine-grained relation extraction tasks. One is that they struggle to accurately identify... | Tieyun Qian, Xin Miao, Yongqi Li |  |
| 1539 |  |  [C-STS: Conditional Semantic Textual Similarity](https://doi.org/10.18653/v1/2023.emnlp-main.345) |  | 0 | Semantic textual similarity (STS) has been a cornerstone task in NLP that measures the degree of similarity between a pair of sentences, with applications in information retrieval, question answering, and embedding methods. However, it is an inherently ambiguous task, with the sentence similarity... | Ameet Deshpande, Ashwin Kalyan, Carlos E. Jimenez, Danqi Chen, Howard Chen, Karthik Narasimhan, Tanmay Rajpurohit, Victoria Graf, Vishvak Murahari |  |
| 1540 |  |  [Cross-lingual Transfer Can Worsen Bias in Sentiment Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.346) |  | 0 | Sentiment analysis (SA) systems are widely deployed in many of the world’s languages, and there is well-documented evidence of demographic bias in these systems. In languages beyond English, scarcer training data is often supplemented with transfer learning using pre-trained models, including... | Adam Lopez, Björn Ross, Seraphina GoldfarbTarrant |  |
| 1541 |  |  [Rumor Detection on Social Media with Crowd Intelligence and ChatGPT-Assisted Networks](https://doi.org/10.18653/v1/2023.emnlp-main.347) |  | 0 | In the era of widespread dissemination through social media, the task of rumor detection plays a pivotal role in establishing a trustworthy and reliable information environment. Nonetheless, existing research on rumor detection confronts several challenges: the limited expressive power of text... | Chang Yang, Hui Gao, Jiaming Zhao, Peng Zhang, Wenbo Qiao |  |
| 1542 |  |  [Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?](https://doi.org/10.18653/v1/2023.emnlp-main.348) |  | 0 | Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human’s perception of reality isn’t always faithful to the physical world. This raises a key question: do VLMs have the similar kind of... | Jiayi Pan, Joyce Chai, Rui Pan, Yichi Zhang, Yuchen Zhou |  |
| 1543 |  |  [Analysing State-Backed Propaganda Websites: a New Dataset and Linguistic Study](https://doi.org/10.18653/v1/2023.emnlp-main.349) |  | 0 | This paper analyses two hitherto unstudied sites sharing state-backed disinformation, Reliable Recent News (rrn.world) and WarOnFakes (waronfakes.com), which publish content in Arabic, Chinese, English, French, German, and Spanish. We describe our content acquisition methodology and perform... | Carolina Scarton, Freddy Heppell, Kalina Bontcheva |  |
| 1544 |  |  [Controllable Contrastive Generation for Multilingual Biomedical Entity Linking](https://doi.org/10.18653/v1/2023.emnlp-main.350) |  | 0 | Multilingual biomedical entity linking (MBEL) aims to map language-specific mentions in the biomedical text to standardized concepts in a multilingual knowledge base (KB) such as Unified Medical Language System (UMLS). In this paper, we propose Con2GEN, a prompt-based controllable contrastive... | Changlong Yu, Qingcai Chen, Tiantian Zhu, Xin Mu, Yang Qin, Yang Xiang |  |
| 1545 |  |  [HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts](https://doi.org/10.18653/v1/2023.emnlp-main.351) |  | 0 | By routing input tokens to only a few split experts, Sparse Mixture-of-Experts has enabled efficient training of large language models. Recent findings suggest that fixing the routers can achieve competitive performance by alleviating the collapsing problem, where all experts eventually learn... | Binh Nguyen, Chenghao Liu, Le Khiem, Quang Pham, Savitha Ramasamy, Steven C. H. Hoi, ThanhNam Doan, TrungTin Nguyen, Truong Do, Xiaoli Li |  |
| 1546 |  |  [MediaHG: Rethinking Eye-catchy Features in Social Media Headline Generation](https://doi.org/10.18653/v1/2023.emnlp-main.352) |  | 0 | An attractive blog headline on social media platforms can immediately grab readers and trigger more clicks. However, a good headline shall not only contract the main content but also be eye-catchy with domain platform features, which are decided by the website’s users and objectives. With effective... | Boning Zhang, Yang Yang |  |
| 1547 |  |  [Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata](https://doi.org/10.18653/v1/2023.emnlp-main.353) |  | 0 | While large language models (LLMs) can answer many questions correctly, they can also hallucinate and give wrong answers. Wikidata, with its over 12 billion facts, can be used to ground LLMs to improve their factuality. This paper presents WikiWebQuestions, a high-quality question answering... | Elizaveta Pertseva, MengHsi Wu, Monica S. Lam, Shicheng Liu, Silei Xu, Sina J. Semnani, Theo Culhane |  |
| 1548 |  |  [ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.354) |  | 0 | We explore the use of large language models (LLMs) for zero-shot semantic parsing. Semantic parsing involves mapping natural language utterances to task-specific meaning representations. LLMs are generally trained on publicly available text and code and cannot be expected to directly generalize to... | Dheeraj Mekala, Jason Andrew Wolfe, Subhro Roy |  |
| 1549 |  |  [Efficient Grammatical Error Correction Via Multi-Task Training and Optimized Training Schedule](https://doi.org/10.18653/v1/2023.emnlp-main.355) |  | 0 | Progress in neural grammatical error correction (GEC) is hindered by the lack of annotated training data. Sufficient amounts of high-quality manually annotated data are not available, so recent research has relied on generating synthetic data, pretraining on it, and then fine-tuning on real... | Alexander Podolskiy, Andrey Bout, Irina Piontkovskaya, Sergey I. Nikolenko |  |
| 1550 |  |  [The BLA Benchmark: Investigating Basic Language Abilities of Pre-Trained Multimodal Models](https://doi.org/10.18653/v1/2023.emnlp-main.356) |  | 0 | Despite the impressive performance achieved by pre-trained language-and-vision models in downstream tasks, it remains an open question whether this reflects a proper understanding of image-text interaction. In this work, we explore to what extent they handle basic linguistic... | Raquel Fernández, Sandro Pezzelle, Xinyi Chen |  |
| 1551 |  |  [RainProof: An Umbrella to Shield Text Generator from Out-Of-Distribution Data](https://doi.org/10.18653/v1/2023.emnlp-main.357) |  | 0 | Implementing effective control mechanisms to ensure the proper functioning and security of deployed NLP models, from translation to chatbots, is essential. A key ingredient to ensure safe system behaviour is Out-Of-Distribution (OOD) detection, which aims to detect whether an input sample is... | Maxime Darrin, Pablo Piantanida, Pierre Colombo |  |
| 1552 |  |  [KEPL: Knowledge Enhanced Prompt Learning for Chinese Hypernym-Hyponym Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.358) |  | 0 | Modeling hypernym-hyponym (“is-a”) relations is very important for many natural language processing (NLP) tasks, such as classification, natural language inference and relation extraction. Existing work on is-a relation extraction is mostly in the English language environment. Due to the... | Dong Wang, Hongyun Bao, Lei He, Ningchen Ma, Suncong Zheng |  |
| 1553 |  |  [Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings](https://doi.org/10.18653/v1/2023.emnlp-main.359) |  | 0 | Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual... | Chong Deng, Chong Zhang, Hai Yu, Jiaqing Liu, Qian Chen, Qinglin Zhang, Siqi Zheng, Wen Wang, Yukun Ma |  |
| 1554 |  |  [Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.360) |  | 0 | The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial validation of... | Chuchun Zhang, Ji Qi, Jifan Yu, Jinxin Liu, Juanzi Li, Kaisheng Zeng, Lei Hou, Xiaozhi Wang, Xu Bin |  |
| 1555 |  |  [Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions](https://doi.org/10.18653/v1/2023.emnlp-main.361) |  | 0 | The moderation of content on online platforms is usually non-transparent. On Wikipedia, however, this discussion is carried out publicly and editors are encouraged to use the content moderation policies as explanations for making moderation decisions. Currently, only a few comments explicitly... | Arnav Arora, Isabelle Augenstein, LucieAimée Kaffee |  |
| 1556 |  |  [Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding](https://doi.org/10.18653/v1/2023.emnlp-main.362) |  | 0 | To tackle the high inference latency exhibited by autoregressive language models, previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings,... | Hwanjun Song, Jongwoo Ko, Sangmin Bae, SeYoung Yun |  |
| 1557 |  |  [End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions](https://doi.org/10.18653/v1/2023.emnlp-main.363) |  | 0 | End-to-end task-oriented dialogue (EToD) can directly generate responses in an end-to-end fashion without modular training, which attracts escalating popularity. The advancement of deep neural networks, especially the successful use of large pre-trained models, has further led to significant... | Libo Qin, Lizi Liao, Min Li, Qiguang Chen, Wanxiang Che, Wenbo Pan, Yue Zhang, Zhou Yu |  |
| 1558 |  |  [Answering Questions by Meta-Reasoning over Multiple Chains of Thought](https://doi.org/10.18653/v1/2023.emnlp-main.364) |  | 0 | Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the... | Ben Bogin, Daniel Deutch, Jonathan Berant, Ori Yoran, Tomer Wolfson, Uri Katz |  |
| 1559 |  |  [INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.365) |  | 0 | Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics do not provide explicit explanation of their verdict, nor associate the scores with defects in the generated text. To address this... | Danqing Wang, Lei Li, Liangming Pan, Markus Freitag, Wenda Xu, William Wang, Zhenqiao Song |  |
| 1560 |  |  [Multi-level Contrastive Learning for Script-based Character Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.366) |  | 0 | In this work, we tackle the scenario of understanding characters in scripts, which aims to learn the characters’ personalities and identities from their utterances. We begin by analyzing several challenges in this scenario, and then propose a multi-level contrastive learning framework to capture... | Dawei Li, Hengyuan Zhang, Shiping Yang, Yanran Li |  |
| 1561 |  |  [CHEF in the Language Kitchen: A Generative Data Augmentation Leveraging Korean Morpheme Ingredients](https://doi.org/10.18653/v1/2023.emnlp-main.367) |  | 0 | Korean morphological variations present unique opportunities and challenges in natural language processing (NLP), necessitating an advanced understanding of morpheme-based sentence construction. The complexity of morphological variations allows for diverse sentence forms based on the... | Chanjun Park, Heuiseok Lim, Hyeonseok Moon, Jaehyung Seo, Jaewook Lee, Sugyeong Eo |  |
| 1562 |  |  [Automatic Debate Evaluation with Argumentation Semantics and Natural Language Argument Graph Networks](https://doi.org/10.18653/v1/2023.emnlp-main.368) |  | 0 | The lack of annotated data on professional argumentation and complete argumentative debates has led to the oversimplification and the inability of approaching more complex natural language processing tasks. Such is the case of the automatic evaluation of complete professional argumentative debates.... | Ana GarcíaFornes, Ramon RuizDolz, Stella Heras |  |
| 1563 |  |  [Transfer-Free Data-Efficient Multilingual Slot Labeling](https://doi.org/10.18653/v1/2023.emnlp-main.369) |  | 0 | Slot labeling (SL) is a core component of task-oriented dialogue (TOD) systems, where slots and corresponding values are usually language-, task- and domain-specific. Therefore, extending the system to any new language-domain-task configuration requires (re)running an expensive and... | Anna Korhonen, Evgeniia Razumovskaia, Ivan Vulic |  |
| 1564 |  |  [Towards Interpretable Mental Health Analysis with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.370) |  | 0 | The latest large language models (LLMs) such as ChatGPT, exhibit strong capabilities in automated mental health analysis. However, existing relevant studies bear several limitations, including inadequate evaluations, lack of prompting strategies, and ignorance of exploring LLMs for explainability.... | Kailai Yang, Qianqian Xie, Shaoxiong Ji, Sophia Ananiadou, Tianlin Zhang, Ziyan Kuang |  |
| 1565 |  |  [Learning to Rank Generation with Pairwise Partial Rewards](https://doi.org/10.18653/v1/2023.emnlp-main.371) |  | 0 | This paper studies the use of reinforcement learning for conditional text generation, which overcomes the limitation of the prevalent supervised maximum likelihood estimation approach. However, it still suffers from challenges including the large action space and the delayed reward, as the reward... | Jinu Lee, Seungwon Hwang, Youngwon Lee |  |
| 1566 |  |  [GreedyCAS: Unsupervised Scientific Abstract Segmentation with Normalized Mutual Information](https://doi.org/10.18653/v1/2023.emnlp-main.372) |  | 0 | The abstracts of scientific papers typically contain both premises (e.g., background and observations) and conclusions. Although conclusion sentences are highlighted in structured abstracts, in non-structured abstracts the concluding information is not explicitly marked, which makes the automatic... | Jessica Lam, Nianlong Gu, Richard H. R. Hahnloser, Yingqiang Gao |  |
| 1567 |  |  [Spoiler Detection as Semantic Text Matching](https://doi.org/10.18653/v1/2023.emnlp-main.373) |  | 0 | Engaging with discussion of TV shows online often requires individuals to refrain from consuming show-related content for extended periods to avoid spoilers. While existing research on spoiler detection shows promising results in safeguarding viewers from general spoilers, it fails to address the... | Canwen Xu, Julian J. McAuley, Ryan Tran |  |
| 1568 |  |  [Multimodal Embodied Plan Prediction Augmented with Synthetic Embodied Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.374) |  | 0 | Embodied task completion is a challenge where an agent in a simulated environment must predict environment actions to complete tasks based on natural language instructions and ego-centric visual observations. We propose a variant of this problem where the agent predicts actions at a higher level of... | Aishwarya Padmakumar, Dilek HakkaniTur, Mert Inan, Patrick Lange, Spandana Gella |  |
| 1569 |  |  [GEM: Gestalt Enhanced Markup Language Model for Web Understanding via Render Tree](https://doi.org/10.18653/v1/2023.emnlp-main.375) |  | 0 | Inexhaustible web content carries abundant perceptible information beyond text. Unfortunately, most prior efforts in pre-trained Language Models (LMs) ignore such cyber-richness, while few of them only employ plain HTMLs, and crucial information in the rendered web, such as visual, layout, and... | Feiyu Gao, Hangdi Xing, Jiajun Bu, Qi Zheng, Xiaozhong Liu, Zhi Yu, Zhongda Qi, Zirui Shao |  |
| 1570 |  |  [Abstractive Open Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.376) |  | 0 | Open Information Extraction (OpenIE) is a traditional NLP task that extracts structured information from unstructured text to be used for other downstream applications. Traditionally, OpenIE focuses on extracting the surface forms of relations as they appear in the raw text, which we term... | Ishan Jindal, Kevin ChenChuan Chang, Kevin Pei |  |
| 1571 |  |  [CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network](https://doi.org/10.18653/v1/2023.emnlp-main.377) |  | 0 | The tremendous growth of social media users interacting in online conversations has led to significant growth in hate speech affecting people from various demographics. Most of the prior works focus on detecting explicit hate speech, which is overt and leverages hateful phrases, with very little... | Dinesh Manocha, Manan Suri, Purva Chiniya, Sonal Kumar, Sreyan Ghosh, Utkarsh Tyagi |  |
| 1572 |  |  [CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.emnlp-main.378) |  | 0 | Evaluating the performance of Grammatical Error Correction (GEC) systems is a challenging task due to its subjectivity. Designing an evaluation metric that is as objective as possible is crucial to the development of GEC task. However, mainstream evaluation metrics, i.e., reference-based metrics,... | HaiTao Zheng, Jingheng Ye, Qingyu Zhou, Shirong Ma, Yangning Li, Ying Shen, Yinghui Li |  |
| 1573 |  |  [Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods](https://doi.org/10.18653/v1/2023.emnlp-main.379) |  | 0 | Feature attribution scores are used for explaining the prediction of a text classifier to users by highlighting a k number of tokens. In this work, we propose a way to determine the number of optimal k tokens that should be displayed from sequential properties of the attribution scores. Our... | Antske Fokkens, Jonathan Kamp, Lisa Beinborn |  |
| 1574 |  |  [SentiStream: A Co-Training Framework for Adaptive Online Sentiment Analysis in Evolving Data Streams](https://doi.org/10.18653/v1/2023.emnlp-main.380) |  | 0 | Online sentiment analysis has emerged as a crucial component in numerous data-driven applications, including social media monitoring, customer feedback analysis, and online reputation management. Despite their importance, current methodologies falter in effectively managing the continuously... | Chun Seah, Karthick Sharma, Shuhao Zhang, Yuhao Wu |  |
| 1575 |  |  [HyperNetwork-based Decoupling to Improve Model Generalization for Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.381) |  | 0 | Few-shot relation extraction (FSRE) aims to train a model that can deal with new relations using only a few labeled examples. Most existing studies employ Prototypical Networks for FSRE, which usually overfits the relation classes in the training set and cannot generalize well to unseen relations.... | Chulun Zhou, Fandong Meng, Jie Zhou, Jinsong Su, Liang Zhang, Yidong Chen |  |
| 1576 |  |  [Solving Hard Analogy Questions with Relation Embedding Chains](https://doi.org/10.18653/v1/2023.emnlp-main.382) |  | 0 | Modelling how concepts are related is a central topic in Lexical Semantics. A common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to model the relation between two concepts as a set of paths. However, KGs are limited to a fixed set of relation types, and they are incomplete... | Nitesh Kumar, Steven Schockaert |  |
| 1577 |  |  [Modeling Empathic Similarity in Personal Narratives](https://doi.org/10.18653/v1/2023.emnlp-main.383) |  | 0 | The most meaningful connections between people are often fostered through expression of shared vulnerability and emotional experiences in personal narratives. We introduce a new task of identifying similarity in personal stories based on empathic resonance, i.e., the extent to which two people... | Cynthia Breazeal, Hae Park, Jocelyn Shen, Maarten Sap, Pedro ColonHernandez |  |
| 1578 |  |  [Tree Prompting: Efficient Task Adaptation without Fine-Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.384) |  | 0 | Prompting language models (LMs) is the main interface for applying them to new tasks. However, for smaller LMs, prompting provides low accuracy compared to gradient-based fine-tuning. Tree Prompting is an approach to prompting which builds a decision tree of prompts, linking multiple prompt-LM... | Alexander M. Rush, Chandan Singh, Jianfeng Gao, John X. Morris, Yuntian Deng |  |
| 1579 |  |  [Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data](https://doi.org/10.18653/v1/2023.emnlp-main.385) |  | 0 | Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically... | Canwen Xu, Daya Guo, Julian J. McAuley, Nan Duan |  |
| 1580 |  |  [Empathy Intent Drives Empathy Detection](https://doi.org/10.18653/v1/2023.emnlp-main.386) |  | 0 | Empathy plays an important role in the human dialogue. Detecting the empathetic direction expressed by the user is necessary for empathetic dialogue systems because it is highly relevant to understanding the user’s needs. Several studies have shown that empathy intent information improves the... | Bohui Mao, Di Wu, Liting Jiang, Wushour Slamu, Yanbing Li |  |
| 1581 |  |  [Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling](https://doi.org/10.18653/v1/2023.emnlp-main.387) |  | 0 | Recently slot filling has witnessed great development thanks to deep learning and the availability of large-scale annotated data. However, it poses a critical challenge to handle a novel domain whose samples are never seen during training. The recognition performance might be greatly degraded due... | Linzhi Wu, Minglai Shao, Yuanjun Shi |  |
| 1582 |  |  [BasahaCorpus: An Expanded Linguistic Resource for Readability Assessment in Central Philippine Languages](https://doi.org/10.18653/v1/2023.emnlp-main.388) |  | 0 | Current research on automatic readability assessment (ARA) has focused on improving the performance of models in high-resource languages such as English. In this work, we introduce and release BasahaCorpus as part of an initiative aimed at expanding available corpora and baseline models for... | Ekaterina Kochmar, Joseph Marvin Imperial |  |
| 1583 |  |  [ReTAG: Reasoning Aware Table to Analytic Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.389) |  | 0 | The task of table summarization involves generating text that both succinctly and accurately represents the table or a specific set of highlighted cells within a table. While significant progress has been made in table to text generation techniques, models still mostly generate descriptive... | Aravindan Raghuveer, Deepanway Ghosal, Preksha Nema |  |
| 1584 |  |  [Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators](https://doi.org/10.18653/v1/2023.emnlp-main.390) |  | 0 | Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of... | Bingzhe Wu, KamFai Wong, Liang Chen, TatSeng Chua, Yang Deng, Yatao Bian, Zeyu Qin |  |
| 1585 |  |  [Compressing Context to Enhance Inference Efficiency of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.391) |  | 0 | Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when... | Bo Dong, Chenghua Lin, Frank Guerin, Yucheng Li |  |
| 1586 |  |  [MoT: Memory-of-Thought Enables ChatGPT to Self-Improve](https://doi.org/10.18653/v1/2023.emnlp-main.392) |  | 0 | Large Language Models (LLMs) have shown impressive abilities on various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, humans can easily improve themselves by self-thinking and memory, without external... | Xiaonan Li, Xipeng Qiu |  |
| 1587 |  |  [4 and 7-bit Labeling for Projective and Non-Projective Dependency Trees](https://doi.org/10.18653/v1/2023.emnlp-main.393) |  | 0 | We introduce an encoding for parsing as sequence labeling that can represent any projective dependency tree as a sequence of 4-bit labels, one per word. The bits in each word’s label represent (1) whether it is a right or left dependent, (2) whether it is the outermost (left/right) dependent of its... | Carlos GómezRodríguez, David Vilares, Diego Roca |  |
| 1588 |  |  [Can You Follow Me? Testing Situational Understanding for ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.394) |  | 0 | Understanding sentence meanings and updating information states appropriately across time—what we call “situational understanding” (SU)—is a critical ability for human-like AI agents. SU is essential in particular for chat models, such as ChatGPT, to enable consistent, coherent, and effective... | Allyson Ettinger, Chenghao Yang |  |
| 1589 |  |  [Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4](https://doi.org/10.18653/v1/2023.emnlp-main.395) |  | 0 | Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in... | Anne Imouza, Caleb Gupta, Camille Thibault, JeanFrançois Godbout, Joel Christoph, Kellin Pelrine, Meilina Reksoprodjo, Reihaneh Rabbany |  |
| 1590 |  |  [Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation](https://doi.org/10.18653/v1/2023.emnlp-main.396) |  | 0 | Grammatical error correction (GEC) is a well-explored problem in English with many existing models and datasets. However, research on GEC in morphologically rich languages has been limited due to challenges such as data scarcity and language complexity. In this paper, we present the first results... | Bashar Alhafni, Christian Khairallah, Go Inoue, Nizar Habash |  |
| 1591 |  |  [HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.397) |  | 0 | Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination... | JiRong Wen, JianYun Nie, Junyi Li, Xiaoxue Cheng, Xin Zhao |  |
| 1592 |  |  [Enabling Large Language Models to Generate Text with Citations](https://doi.org/10.18653/v1/2023.emnlp-main.398) |  | 0 | Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies... | Danqi Chen, Howard Yen, Jiatong Yu, Tianyu Gao |  |
| 1593 |  |  [Revisiting Machine Translation for Cross-lingual Classification](https://doi.org/10.18653/v1/2023.emnlp-main.399) |  | 0 | Machine Translation (MT) has been widely used for cross-lingual classification, either by translating the test set into English and running inference with a monolingual model (translate-test), or translating the training set into the target languages and finetuning a multilingual model... | Angela Fan, Luke Zettlemoyer, Mikel Artetxe, Shruti Bhosale, Vedanuj Goswami |  |
| 1594 |  |  [Pre-Trained Language Models Augmented with Synthetic Scanpaths for Natural Language Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.400) |  | 0 | Human gaze data offer cognitive information that reflects natural language comprehension. Indeed, augmenting language models with human scanpaths has proven beneficial for a range of NLP tasks, including language understanding. However, the applicability of this approach is hampered because the... | David R. Reich, Lena A. Jäger, Paul Prasse, Shuwen Deng, Tobias Scheffer |  |
| 1595 |  |  [Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model](https://doi.org/10.18653/v1/2023.emnlp-main.401) |  | 0 | Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that... | Abhishek Vijayakumar, Amey Hengle, Anjali Kantharuban, Anna Cai, Anubha Kabra, Atharva Kulkarni, David R. Mortensen, Haofei Yu, Hinrich Schütze, Kemal Oflazer, Leonie Weissweiler, Ritam Dutt, Valentin Hofmann |  |
| 1596 |  |  [Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.402) |  | 0 | Large language models (LLMs) have showcased their capability with few-shot inference known as in-context learning. However, in-domain demonstrations are not always readily available in real scenarios, leading to cross-domain in-context learning. Besides, LLMs are still facing challenges in... | Quanyu Long, Sinno Jialin Pan, Wenya Wang |  |
| 1597 |  |  [Understanding the Inner-workings of Language Models Through Representation Dissimilarity](https://doi.org/10.18653/v1/2023.emnlp-main.403) |  | 0 | As language models are applied to an increasing number of real-world applications, understanding their inner workings has become an important issue in model trust, interpretability, and transparency. In this work we show that representation dissimilarity measures, which are functions that measure... | Charles Godfrey, Davis Brown, Henry Kvinge, Jonathan H. Tu, Nicholas Konz |  |
| 1598 |  |  [Efficient Classification of Long Documents via State-Space Models](https://doi.org/10.18653/v1/2023.emnlp-main.404) |  | 0 | Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity... | Bang Liu, Ivan Kobyzev, Mehdi Rezagholizadeh, Peng Lu, Suyuchen Wang |  |
| 1599 |  |  [Dual-Feedback Knowledge Retrieval for Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2023.emnlp-main.405) |  | 0 | Efficient knowledge retrieval plays a pivotal role in ensuring the success of end-to-end task-oriented dialogue systems by facilitating the selection of relevant information necessary to fulfill user requests. However, current approaches generally integrate knowledge retrieval and response... | Liangzhi Li, Qifan Wang, Tao Yang, Tianyuan Shi, Xiaojun Quan, Zijian Lin |  |
| 1600 |  |  [Construction Artifacts in Metaphor Identification Datasets](https://doi.org/10.18653/v1/2023.emnlp-main.406) |  | 0 | Metaphor identification aims at understanding whether a given expression is used figuratively in context. However, in this paper we show how existing metaphor identification datasets can be gamed by fully ignoring the potential metaphorical expression or the context in which it occurs. We test this... | Joanne Boisson, José CamachoCollados, Luis Espinosa Anke |  |
| 1601 |  |  [MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.407) |  | 0 | Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has... | David Wang, Deepak Nathani, Liangming Pan, William Yang Wang |  |
| 1602 |  |  [Granularity Matters: Pathological Graph-driven Cross-modal Alignment for Brain CT Report Generation](https://doi.org/10.18653/v1/2023.emnlp-main.408) |  | 0 | The automatic Brain CT reports generation can improve the efficiency and accuracy of diagnosing cranial diseases. However, current methods are limited by 1) coarse-grained supervision: the training data in image-text format lacks detailed supervision for recognizing subtle abnormalities, and 2)... | Junzhong Ji, Liangqiong Qu, Xiaodan Zhang, Yanzhao Shi, Ying Liu |  |
| 1603 |  |  [Enhancing Structured Evidence Extraction for Fact Verification](https://doi.org/10.18653/v1/2023.emnlp-main.409) |  | 0 | Open-domain fact verification is the task of verifying claims in natural language texts against extracted evidence. FEVEROUS is a benchmark that requires extracting and integrating both unstructured and structured evidence to verify a given claim. Previous models suffer from low recall of... | Nan Hu, Yansong Feng, Zirui Wu |  |
| 1604 |  |  [Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models](https://doi.org/10.18653/v1/2023.emnlp-main.410) |  | 0 | Keyphrase Generation (KPG) is a longstanding task in NLP with widespread applications. The advent of sequence-to-sequence (seq2seq) pre-trained language models (PLMs) has ushered in a transformative era for KPG, yielding promising performance improvements. However, many design decisions remain... | Di Wu, KaiWei Chang, Wasi Uddin Ahmad |  |
| 1605 |  |  [A Fair and In-Depth Evaluation of Existing End-to-End Entity Linking Systems](https://doi.org/10.18653/v1/2023.emnlp-main.411) |  | 0 | Existing evaluations of entity linking systems often say little about how the system is going to perform for a particular application. There are two fundamental reasons for this. One is that many evaluations only use aggregate measures (like precision, recall, and F1 score), without a detailed... | Hannah Bast, Matthias Hertel, Natalie Prange |  |
| 1606 |  |  [A Multi-Task Dataset for Assessing Discourse Coherence in Chinese Essays: Structure, Theme, and Logic Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.412) |  | 0 | This paper introduces the Chinese Essay Discourse Coherence Corpus (CEDCC), a multi-task dataset for assessing discourse coherence. Existing research tends to focus on isolated dimensions of discourse coherence, a gap which the CEDCC addresses by integrating coherence grading, topical continuity,... | Hongyi Wu, Man Lan, Shaoguang Mao, Xiaopeng Bai, Xinshu Shen, Yuanbin Wu |  |
| 1607 |  |  [SKD-NER: Continual Named Entity Recognition via Span-based Knowledge Distillation with Reinforcement Learning](https://doi.org/10.18653/v1/2023.emnlp-main.413) |  | 0 | Continual learning for named entity recognition (CL-NER) aims to enable models to continuously learn new entity types while retaining the ability to recognize previously learned ones. However, the current strategies fall short of effectively addressing the catastrophic forgetting of previously... | Liang He, Yi Chen |  |
| 1608 |  |  [Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation](https://doi.org/10.18653/v1/2023.emnlp-main.414) |  | 0 | Lifelong sequence generation (LSG), a problem in continual learning, aims to continually train a model on a sequence of generation tasks to learn constantly emerging new generation patterns while avoiding the forgetting of previous knowledge. Existing LSG methods mainly focus on maintaining old... | Chen Chen, Chengwei Qin, Shafiq Joty |  |
| 1609 |  |  [When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.415) |  | 0 | Though majority vote among annotators is typically used for ground truth labels in machine learning, annotator disagreement in tasks such as hate speech detection may reflect systematic differences in opinion across groups, not noise. Thus, a crucial problem in hate speech detection is determining... | Dan Klein, Eve Fleisig, Rediet Abebe |  |
| 1610 |  |  [Lazy-k Decoding: Constrained Decoding for Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.416) |  | 0 | We explore the possibility of improving probabilistic models in structured prediction. Specifically, we combine the models with constrained decoding approaches in the context of token classification for information extraction. The decoding methods search for constraint-satisfying label-assignments... | Arthur Hemmer, JeanMarc Ogier, Jérôme Brachat, Mickaël Coustaty, Nicola Bartolo |  |
| 1611 |  |  [Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation](https://doi.org/10.18653/v1/2023.emnlp-main.417) |  | 0 | With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model... | Amrita Saha, Hailin Chen, Shafiq Joty, Steven ChuHong Hoi |  |
| 1612 |  |  [Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.418) |  | 0 | Temporal reasoning represents a vital component of human communication and understanding, yet remains an underexplored area within the context of Large Language Models (LLMs). Despite LLMs demonstrating significant proficiency in a range of tasks, a comprehensive, large-scale analysis of their... | Adam Jatowt, Arkadeep Acharya, Daivik Sojitra, Raghav Jain, Sandipan Dandapat, Sriparna Saha |  |
| 1613 |  |  [Comparing Styles across Languages](https://doi.org/10.18653/v1/2023.emnlp-main.419) |  | 0 | Understanding how styles differ across languages is advantageous for training both humans and computers to generate culturally appropriate text. We introduce an explanation framework to extract stylistic differences from multilingual LMs and compare styles across languages. Our framework (1)... | Eric Wong, Lyle H. Ungar, Matthew Pressimone, Shreya Havaldar |  |
| 1614 |  |  [Event Causality Extraction via Implicit Cause-Effect Interactions](https://doi.org/10.18653/v1/2023.emnlp-main.420) |  | 0 | Event Causality Extraction (ECE) aims to extract the cause-effect event pairs from the given text, which requires the model to possess a strong reasoning ability to capture event causalities. However, existing works have not adequately exploited the interactions between the cause and effect event... | Jintao Liu, Kaiwen Wei, Li Jin, Xian Sun, Xiaoyu Li, Zequn Zhang, Zhi Guo |  |
| 1615 |  |  [Evaluation of African American Language Bias in Natural Language Generation](https://doi.org/10.18653/v1/2023.emnlp-main.421) |  | 0 | While biases disadvantaging African American Language (AAL) have been uncovered in models for tasks such as speech recognition and toxicity detection, there has been little investigation of these biases for language generation models like ChatGPT. We evaluate how well LLMs understand AAL in... | Desmond Patton, Elsbeth Turcan, Jessica Grieser, Kathleen R. McKeown, Nicholas Deas, Shana Kleiner |  |
| 1616 |  |  [A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2023.emnlp-main.422) |  | 0 | Achieving robust language technologies that can perform well across the world’s many languages is a central goal of multilingual NLP. In this work, we take stock of and empirically analyse task performance disparities that exist between multilingual task-oriented dialogue (ToD) systems. We first... | Anna Korhonen, Guchun Zhang, Han Zhou, Ignacio Iacobacci, Ivan Vulic, Milan Gritta, Moy Yuan, Songbo Hu |  |
| 1617 |  |  [Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction](https://doi.org/10.18653/v1/2023.emnlp-main.423) |  | 0 | Phonological reconstruction is one of the central problems in historical linguistics where a proto-word of an ancestral language is determined from the observed cognate words of daughter languages. Computational approaches to historical linguistics attempt to automate the task by learning models on... | Arnab Bhattacharya, V. S. D. S. Mahesh Akavarapu |  |
| 1618 |  |  [Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning](https://doi.org/10.18653/v1/2023.emnlp-main.424) |  | 0 | While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be... | Abhilasha Ravichander, Bill Y. Lin, Faeze Brahman, Jaehun Jung, Jillian Fisher, Khyathi Raghavi Chandu, Lianhui Qin, Liwei Jiang, Nouha Dziri, Peter West, Prithviraj Ammanabrolu, Sahana Ramnath, Sean Welleck, Skyler Hallinan, Xiang Ren, Ximing Lu, Yejin Choi |  |
| 1619 |  |  [Weakly Supervised Semantic Parsing with Execution-based Spurious Program Filtering](https://doi.org/10.18653/v1/2023.emnlp-main.425) |  | 0 | The problem of spurious programs is a longstanding challenge when training a semantic parser from weak supervision. To eliminate such programs that have wrong semantics but correct denotation, existing methods focus on exploiting similarities between examples based on domain-specific knowledge. In... | Kangil Lee, Kyomin Jung, Segwang Kim |  |
| 1620 |  |  [Taxonomy Expansion for Named Entity Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.426) |  | 0 | Training a Named Entity Recognition (NER) model often involves fixing a taxonomy of entity types. However, requirements evolve and we might need the NER model to recognize additional entity types. A simple approach is to re-annotate entire dataset with both existing and additional entity types and... | Dan Roth, Giovanni Paolini, Jie Ma, Karthikeyan K, Miguel Ballesteros, Neha Anna John, Shuai Wang, Vittorio Castelli, Yassine Benajiba, Yogarshi Vyas |  |
| 1621 |  |  [Rather a Nurse than a Physician - Contrastive Explanations under Investigation](https://doi.org/10.18653/v1/2023.emnlp-main.427) |  | 0 | Contrastive explanations, where one decision is explained \*in contrast to another\*, are supposed to be closer to how humans explain a decision than non-contrastive explanations, where the decision is not necessarily referenced to an alternative. This claim has never been empirically validated. We... | Ilias Chalkidis, Laura Cabello, Oliver Eberle, Stephanie Brandl |  |
| 1622 |  |  [EtiCor: Corpus for Analyzing LLMs for Etiquettes](https://doi.org/10.18653/v1/2023.emnlp-main.428) |  | 0 | Etiquettes are an essential ingredient of day-to-day interactions among people. Moreover, etiquettes are region-specific, and etiquettes in one region might contradict those in other regions. In this paper, we propose EtiCor, an Etiquettes Corpus, having texts about social norms from five different... | Ashutosh Dwivedi, Ashutosh Modi, Pradhyumna Lavania |  |
| 1623 |  |  [An Investigation of LLMs' Inefficacy in Understanding Converse Relations](https://doi.org/10.18653/v1/2023.emnlp-main.429) |  | 0 | Large Language Models (LLMs) have achieved remarkable success in many formal language oriented tasks, such as structural data-to-text and semantic parsing. However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do... | Bailin Wang, Binyuan Hui, Bowen Li, Chengwen Qi, Jinwang Wu, Jinyang Li, Yuanjun Laili |  |
| 1624 |  |  [Towards Low-Resource Automatic Program Repair with Meta-Learning and Pretrained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.430) |  | 0 | Automatic program repair (APR) has gained increasing attention as an essential technique in software development to reduce manual debugging efforts and boost developers’ productivity. Recent advances in deep learning (DL) based models have demonstrated promising results by learning from large-scale... | Shafiq Joty, Steven C. H. Hoi, Weishi Wang, Yue Wang |  |
| 1625 |  |  [ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters](https://doi.org/10.18653/v1/2023.emnlp-main.431) |  | 0 | We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via the use of language adapters (LAs). Most of the earlier works have explored training with adapter of a single source (often English), and testing either using the target LA or LA of another related language. Training target... | Mausam, Parag Singla, Rajdeep Dhingra, Vipul Rathore |  |
| 1626 |  |  [Log-FGAER: Logic-Guided Fine-Grained Address Entity Recognition from Multi-Turn Spoken Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.432) |  | 0 | Fine-grained address entity recognition (FGAER) from multi-turn spoken dialogues is particularly challenging. The major reason lies in that a full address is often formed through a conversation process. Different parts of an address are distributed through multiple turns of a dialogue with spoken... | Chao Deng, Junlan Feng, Pengwei Hu, Qian Hu, Xue Han, Yitong Wang |  |
| 1627 |  |  [Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning](https://doi.org/10.18653/v1/2023.emnlp-main.433) |  | 0 | Unified Sequence Labeling that articulates different sequence labeling problems such as Named Entity Recognition, Relation Extraction, Semantic Role Labeling, etc. in a generalized sequence-to-sequence format opens up the opportunity to make the maximum utilization of large language model knowledge... | Haoran Zhang, Peng Shi, Rui Zhang, Sarkar Snigdha Sarathi Das, Wenpeng Yin |  |
| 1628 |  |  [On the Representational Capacity of Recurrent Neural Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.434) |  | 0 | This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings... | Anej Svete, Franz Nowak, Li Du, Ryan Cotterell |  |
| 1629 |  |  [A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.435) |  | 0 | Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect... | Alessandro Stolfo, Mrinmaya Sachan, Yonatan Belinkov |  |
| 1630 |  |  [Benchmarking and Improving Text-to-SQL Generation under Ambiguity](https://doi.org/10.18653/v1/2023.emnlp-main.436) |  | 0 | Research in Text-to-SQL conversion has been largely benchmarked against datasets where each text query corresponds to one correct SQL. However, natural language queries over real-life databases frequently involve significant ambiguity about the intended SQL due to overlapping schema names and... | Adithya Bhaskar, Ashutosh Sathe, Sunita Sarawagi, Tushar Tomar |  |
| 1631 |  |  [Non-autoregressive Text Editing with Copy-aware Latent Alignments](https://doi.org/10.18653/v1/2023.emnlp-main.437) |  | 0 | Recent work has witnessed a paradigm shift from Seq2Seq to Seq2Edit in the field of text editing, with the aim of addressing the slow autoregressive inference problem posed by the former. Despite promising results, Seq2Edit approaches still face several challenges such as inflexibility in... | Guohong Fu, Leyang Cui, Yu Zhang, Yue Zhang |  |
| 1632 |  |  [Translating away Translationese without Parallel Data](https://doi.org/10.18653/v1/2023.emnlp-main.438) |  | 0 | Translated texts exhibit systematic linguistic differences compared to original texts in the same language, and these differences are referred to as translationese. Translationese has effects on various cross-lingual natural language processing tasks, potentially leading to biased results. In this... | Cristina EspañaBonet, Josef van Genabith, Koel Dutta Chowdhury, Rricha Jalota |  |
| 1633 |  |  [Prompt-Based Monte-Carlo Tree Search for Goal-oriented Dialogue Policy Planning](https://doi.org/10.18653/v1/2023.emnlp-main.439) |  | 0 | Planning for goal-oriented dialogue often requires simulating future dialogue interactions and estimating task progress. Many approaches thus consider training neural networks to perform look-ahead search algorithms such as A\* search and Monte Carlo Tree Search (MCTS). However, this training often... | Maximillian Chen, Xiao Yu, Zhou Yu |  |
| 1634 |  |  [UniMath: A Foundational and Multimodal Mathematical Reasoner](https://doi.org/10.18653/v1/2023.emnlp-main.440) |  | 0 | While significant progress has been made in natural language processing (NLP), existing methods exhibit limitations in effectively interpreting and processing diverse mathematical modalities. Therefore, we introduce UniMath, a versatile and unified system designed for multimodal mathematical... | Jipeng Zhang, Tianyu Yang, Xiangliang Zhang, Zhenwen Liang |  |
| 1635 |  |  [CaseEncoder: A Knowledge-enhanced Pre-trained Model for Legal Case Encoding](https://doi.org/10.18653/v1/2023.emnlp-main.441) |  | 0 | Legal case retrieval is a critical process for modern legal information systems. While recent studies have utilized pre-trained language models (PLMs) based on the general domain self-supervised pre-training paradigm to build models for legal case retrieval, there are limitations in using general... | Qingyao Ai, Weihang Su, Yiqun Liu, Yixiao Ma, Yueyue Wu |  |
| 1636 |  |  [HiddenTables and PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies](https://doi.org/10.18653/v1/2023.emnlp-main.442) |  | 0 | A myriad of different Large Language Models (LLMs) face a common challenge in contextually analyzing table question-answering tasks. These challenges are engendered from (1) finite context windows for large tables, (2) multi-faceted discrepancies amongst tokenization patterns against cell... | Manuela Veloso, Nicole Cho, Tucker Balch, William Watson |  |
| 1637 |  |  [Causal Document-Grounded Dialogue Pre-training](https://doi.org/10.18653/v1/2023.emnlp-main.443) |  | 0 | The goal of document-grounded dialogue (DocGD) is to generate a response by anchoring the evidence in a supporting document in accordance with the dialogue context. This entails four causally interconnected variables. While task-specific pre-training has significantly enhanced performances on... | Bowen Li, Bowen Yu, Chao Wang, Fei Huang, Haiyang Yu, Jinyang Li, Nevin L. Zhang, Yingxiu Zhao, Yongbin Li |  |
| 1638 |  |  [Accented Speech Recognition With Accent-specific Codebooks](https://doi.org/10.18653/v1/2023.emnlp-main.444) |  | 0 | Speech accents pose a significant challenge to state-of-the-art automatic speech recognition (ASR) systems. Degradation in performance across underrepresented accents is a severe deterrent to the inclusive adoption of ASR. In this work, we propose a novel accent adaptation approach for end-to-end... | Darshan Prabhu, Preethi Jyothi, Sriram Ganapathy, Vinit Unni |  |
| 1639 |  |  [Linking Surface Facts to Large-Scale Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.445) |  | 0 | Open Information Extraction (OIE) methods extract facts from natural language text in the form of (“subject”; “relation”; “object”) triples. These facts are, however, merely surface forms, the ambiguity of which impedes their downstream usage; e.g., the surface phrase “Michael Jordan” may refer to... | Carolin Lawrence, ChiaChien Hung, Goran Glavas, Gorjan Radevski, Kiril Gashteovski |  |
| 1640 |  |  [Sentiment Analysis on Streaming User Reviews via Dual-Channel Dynamic Graph Neural Network](https://doi.org/10.18653/v1/2023.emnlp-main.446) |  | 0 | Sentiment analysis on user reviews has achieved great success thanks to the rapid growth of deep learning techniques. The large number of online streaming reviews also provides the opportunity to model temporal dynamics for users and products on the timeline. However, existing methods model users... | Deyu Zhou, Linhai Zhang, Xin Zhang |  |
| 1641 |  |  [DUMB: A Dutch Model Benchmark](https://doi.org/10.18653/v1/2023.emnlp-main.447) |  | 0 | We introduce the Dutch Model Benchmark: DUMB. The benchmark includes a diverse set of datasets for low-, medium- and high-resource tasks. The total set of nine tasks includes four tasks that were previously not available in Dutch. Instead of relying on a mean score across tasks, we propose Relative... | Malvina Nissim, Martijn Wieling, Wietse de Vries |  |
| 1642 |  |  [OssCSE: Overcoming Surface Structure Bias in Contrastive Learning for Unsupervised Sentence Embedding](https://doi.org/10.18653/v1/2023.emnlp-main.448) |  | 0 | Contrastive learning has been demonstrated effective in unsupervised sentence representation learning. Given one sentence, positive pairs are obtained by passing the sentence to the encoder twice using the different dropout masks, and negative pairs are obtained by taking another sentence in the... | Belinda Zeng, Guoyin Wang, Jiwei Li, Ke Bai, Qingjun Cui, Trishul Chilimbi, Xiang Li, Xiaodan Zhu, Zhan Shi |  |
| 1643 |  |  [End-to-End Single-Channel Speaker-Turn Aware Conversational Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-main.449) |  | 0 | Conventional speech-to-text translation (ST) systems are trained on single-speaker utterances, and they may not generalize to real-life scenarios where the audio contains conversations by multiple speakers. In this paper, we tackle single-channel multi-speaker conversational ST with an end-to-end... | Brian Thompson, Juan Pablo ZuluagaGomez, Marcello Federico, Prashant Mathur, Rohit Paturi, Sundararajan Srinivasan, Xing Niu, Zhaocheng Huang |  |
| 1644 |  |  [A Fine-Grained Taxonomy of Replies to Hate Speech](https://doi.org/10.18653/v1/2023.emnlp-main.450) |  | 0 | Countering rather than censoring hate speech has emerged as a promising strategy to address hatred. There are many types of counterspeech in user-generated content: addressing the hateful content or its author, generic requests, well-reasoned counter arguments, insults, etc. The effectiveness of... | Ashley Zhao, Eduardo Blanco, Lingzi Hong, Xinchen Yu |  |
| 1645 |  |  [JointMatch: A Unified Approach for Diverse and Collaborative Pseudo-Labeling to Semi-Supervised Text Classification](https://doi.org/10.18653/v1/2023.emnlp-main.451) |  | 0 | Semi-supervised text classification (SSTC) has gained increasing attention due to its ability to leverage unlabeled data. However, existing approaches based on pseudo-labeling suffer from the issues of pseudo-label bias and error accumulation. In this paper, we propose JointMatch, a holistic... | Cornelia Caragea, Henry Peng Zou |  |
| 1646 |  |  [Simple Temporal Adaptation to Changing Label Sets: Hashtag Prediction via Dense KNN](https://doi.org/10.18653/v1/2023.emnlp-main.452) |  | 0 | User-generated social media data is constantly changing as new trends influence online discussion and personal information is deleted due to privacy concerns. However, traditional NLP models rely on fixed training datasets, which means they are unable to adapt to temporal change—both test... | Ahmed ElKishky, Junxian He, Nikolai Vogler, Niloofar Mireshghallah, Omar Florez, Taylor BergKirkpatrick |  |
| 1647 |  |  [Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4](https://doi.org/10.18653/v1/2023.emnlp-main.453) |  | 0 | In this work, we carry out a data archaeology to infer books that are known to ChatGPT and GPT-4 using a name cloze membership inference query. We find that OpenAI models have memorized a wide collection of copyrighted materials, and that the degree of memorization is tied to the frequency with... | David Bamman, Kent K. Chang, Mackenzie Cramer, Sandeep Soni |  |
| 1648 |  |  [A Study on Accessing Linguistic Information in Pre-Trained Language Models by Using Prompts](https://doi.org/10.18653/v1/2023.emnlp-main.454) |  | 0 | We study whether linguistic information in pre-trained multilingual language models can be accessed by human language: So far, there is no easy method to directly obtain linguistic information and gain insights into the linguistic principles encoded in such models. We use the technique of prompting... | Alexander Fraser, Katharina Hämmerl, Marion Di Marco |  |
| 1649 |  |  [CiteBench: A Benchmark for Scientific Citation Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.455) |  | 0 | Science progresses by building upon the prior body of knowledge documented in scientific publications. The acceleration of research makes it hard to stay up-to-date with the recent developments and to summarize the ever-growing body of prior work. To address this, the task of citation text... | Ilia Kuznetsov, Iryna Gurevych, Martin Funkquist, Yufang Hou |  |
| 1650 |  |  [From Heuristic to Analytic: Cognitively Motivated Strategies for Coherent Physical Commonsense Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.456) |  | 0 | Pre-trained language models (PLMs) have shown impressive performance in various language tasks. However, they are prone to spurious correlations, and often generate illusory information. In real-world applications, PLMs should justify decisions with formalized, coherent reasoning chains, but this... | Fengyuan Hu, Honglak Lee, Joyce Chai, Moontae Lee, Shane Storks, Sungryull Sohn, Zheyuan Zhang |  |
| 1651 |  |  [A Challenging Multimodal Video Summary: Simultaneously Extracting and Generating Keyframe-Caption Pairs from Video](https://doi.org/10.18653/v1/2023.emnlp-main.457) |  | 0 | This paper proposes a practical multimodal video summarization task setting and a dataset to train and evaluate the task. The target task involves summarizing a given video into a predefined number of keyframe-caption pairs and displaying them in a listable format to grasp the video content... | Haruki Nagasawa, Jun Suzuki, Keito Kudo, Nobuyuki Shimizu |  |
| 1652 |  |  [Copyright Violations and Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.458) |  | 0 | Language models may memorize more than just facts, including entire chunks of texts seen during training. Fair use exemptions to copyright laws typically allow for limited use of copyrighted material without permission from the copyright holder, but typically for extraction of information from... | Anders Søgaard, Antonia Karamolegkou, Jiaang Li, Li Zhou |  |
| 1653 |  |  [Effects of sub-word segmentation on performance of transformer language models](https://doi.org/10.18653/v1/2023.emnlp-main.459) |  | 0 | Language modeling is a fundamental task in natural language processing, which has been thoroughly explored with various architectures and hyperparameters. However, few studies focus on the effect of sub-word segmentation on the performance of language models (LMs). In this paper, we compare GPT and... | AnhDuc Vu, Anisia Katinskaia, Jue Hou, Roman Yangarber |  |
| 1654 |  |  [Symbolic Planning and Code Generation for Grounded Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.460) |  | 0 | Large language models (LLMs) excel at processing and generating text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded... | Alexander M. Rush, Daniel Fried, Derek Chen, Justin T. Chiu, Saujas Vaduguru, Wenting Zhao |  |
| 1655 |  |  [Universal Self-Adaptive Prompting](https://doi.org/10.18653/v1/2023.emnlp-main.461) |  | 0 | A hallmark of modern large language models (LLMs) is their impressive general zero-shot and few-shot abilities, often elicited through in-context learning (ICL) via prompting. However, while highly coveted and being the most general, zero-shot performances in LLMs are still typically weaker due to... | Hanjun Dai, Hootan Nakhost, Julian Eisenschlos, Ruoxi Sun, Sercan Ö. Arik, Tomas Pfister, Xingchen Wan |  |
| 1656 |  |  [Somali Information Retrieval Corpus: Bridging the Gap between Query Translation and Dedicated Language Resources](https://doi.org/10.18653/v1/2023.emnlp-main.462) |  | 0 | Despite the growing use of the Somali language in various online domains, research on Somali language information retrieval remains limited and primarily relies on query translation due to the lack of a dedicated corpus. To address this problem, we collaborated with language experts and natural... | Abdisalam Badel, Fan Zhou, Ting Zhong, Wenxin Tai |  |
| 1657 |  |  [Beat LLMs at Their Own Game: Zero-Shot LLM-Generated Text Detection via Querying ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.463) |  | 0 | Large language models (LLMs), e.g., ChatGPT, have revolutionized the domain of natural language processing because of their excellent performance on various tasks. Despite their great potential, LLMs also incur serious concerns as they are likely to be misused. There are already reported cases of... | Bingxiang He, Biru Zhu, Chong Fu, Ganqu Cui, Lifan Yuan, Maosong Sun, Ming Gu, Yangdong Deng, Yangyi Chen, Zhiyuan Liu |  |
| 1658 |  |  [Faithful Model Evaluation for Model-Based Metrics](https://doi.org/10.18653/v1/2023.emnlp-main.464) |  | 0 | Statistical significance testing is used in natural language processing (NLP) to determine whether the results of a study or experiment are likely to be due to chance or if they reflect a genuine relationship. A key step in significance testing is the estimation of confidence interval which is a... | Palash Goyal, Qian Hu, Rahul Gupta |  |
| 1659 |  |  [Content- and Topology-Aware Representation Learning for Scientific Multi-Literature](https://doi.org/10.18653/v1/2023.emnlp-main.465) |  | 0 | Representation learning forms an essential building block in the development of natural language processing architectures. To date, mainstream approaches focus on learning textual information at the sentence- or document-level, unfortunately, overlooking the inter-document connections. This... | Kai Zhang, Kaisong Song, Xiaozhong Liu, Yangyang Kang |  |
| 1660 |  |  [Language Model Quality Correlates with Psychometric Predictive Power in Multiple Languages](https://doi.org/10.18653/v1/2023.emnlp-main.466) |  | 0 | Surprisal theory (Hale, 2001; Levy, 2008) posits that a word’s reading time is proportional to its surprisal (i.e., to its negative log probability given the proceeding context). Since we are unable to access a word’s ground-truth probability, surprisal theory has been empirically tested using... | Clara Meister, Ethan Wilcox, Ryan Cotterell, Tiago Pimentel |  |
| 1661 |  |  [Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks](https://doi.org/10.18653/v1/2023.emnlp-main.467) |  | 0 | Entity and Relation Extraction (ERE) is an important task in information extraction. Recent marker-based pipeline models achieve state-of-the-art performance, but still suffer from the error propagation issue. Also, most of current ERE models do not take into account higher-order interactions... | Kewei Tu, Songlin Yang, Wei Liu, Zhaohui Yan |  |
| 1662 |  |  [Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.468) |  | 0 | The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging... | Daman Arora, Himanshu Gaurav Singh, Mausam |  |
| 1663 |  |  [StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure](https://doi.org/10.18653/v1/2023.emnlp-main.469) |  | 0 | This work presents StrAE: a Structured Autoencoder framework that through strict adherence to explicit structure, and use of a novel contrastive objective over tree-structured representations, enables effective learning of multi-level representations. Through comparison over different forms of... | Mattia Opper, Siddharth Narayanaswamy, Victor Prokhorov |  |
| 1664 |  |  [WiCE: Real-World Entailment for Claims in Wikipedia](https://doi.org/10.18653/v1/2023.emnlp-main.470) |  | 0 | Textual entailment models are increasingly applied in settings like fact-checking, presupposition verification in question answering, or summary evaluation. However, these represent a significant domain shift from existing entailment datasets, and models underperform as a result. We propose WiCE, a... | Greg Durrett, Juan Diego Rodriguez, Ryo Kamoi, Tanya Goyal |  |
| 1665 |  |  [Natural Disaster Tweets Classification Using Multimodal Data](https://doi.org/10.18653/v1/2023.emnlp-main.471) |  | 0 | Social media platforms are extensively used for expressing opinions or conveying information. The information available on such platforms can be used for various humanitarian and disaster-related tasks as distributing messages in different formats through social media is quick and easy. Often this... | Bashir Alam, Mohammad Basit, Salman Shaikh, Zubaida Fatima |  |
| 1666 |  |  [On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research](https://doi.org/10.18653/v1/2023.emnlp-main.472) |  | 0 | Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We... | Beyza Ermis, Luiza Pozzobon, Patrick Lewis, Sara Hooker |  |
| 1667 |  |  [RoBoCoP: A Comprehensive ROmance BOrrowing COgnate Package and Benchmark for Multilingual Cognate Identification](https://doi.org/10.18653/v1/2023.emnlp-main.473) |  | 0 | The identification of cognates is a fundamental process in historical linguistics, on which any further research is based. Even though there are several cognate databases for Romance languages, they are rather scattered, incomplete, noisy, contain unreliable information, or have uncertain... | Alina Maria Cristea, Ana Sabina Uban, Anca Dinu, IoanBogdan Iordache, Laurentiu Zoicas, Liviu P. Dinu, Simona Georgescu |  |
| 1668 |  |  [Instructive Dialogue Summarization with Query Aggregations](https://doi.org/10.18653/v1/2023.emnlp-main.474) |  | 0 | Conventional dialogue summarization methods directly generate summaries and do not consider user’s specific interests. This poses challenges in cases where the users are more focused on particular topics or aspects. With the advancement of instruction-finetuned language models, we introduce... | Bin Wang, Nancy F. Chen, Zhengyuan Liu |  |
| 1669 |  |  [Semantic matching for text classification with complex class descriptions](https://doi.org/10.18653/v1/2023.emnlp-main.475) |  | 0 | Text classifiers are an indispensable tool for machine learning practitioners, but adapting them to new classes is expensive. To reduce the cost of new classes, previous work exploits class descriptions and/or labels from existing classes. However, these approaches leave a gap in the model... | Brian de Silva, Gwang Lee, Karen Hovsepian, KuanWen Huang, Mingwei Shen, Yan Xu |  |
| 1670 |  |  [MADNet: Maximizing Addressee Deduction Expectation for Multi-Party Conversation Generation](https://doi.org/10.18653/v1/2023.emnlp-main.476) |  | 0 | Modeling multi-party conversations (MPCs) with graph neural networks has been proven effective at capturing complicated and graphical information flows. However, existing methods rely heavily on the necessary addressee labels and can only be applied to an ideal setting where each utterance must be... | Caiyuan Chu, ChaoHong Tan, Chongyang Tao, Cong Liu, JiaChen Gu, Quan Liu, ZhenHua Ling |  |
| 1671 |  |  [GLEN: Generative Retrieval via Lexical Index Learning](https://doi.org/10.18653/v1/2023.emnlp-main.477) |  | 0 | Generative retrieval shed light on a new paradigm of document retrieval, aiming to directly generate the identifier of a relevant document for a query. While it takes advantage of bypassing the construction of auxiliary index structures, existing studies face two significant challenges: (i) the... | Jongwuk Lee, Minjin Choi, Sunkyung Lee |  |
| 1672 |  |  [Turn-Level Active Learning for Dialogue State Tracking](https://doi.org/10.18653/v1/2023.emnlp-main.478) |  | 0 | Dialogue state tracking (DST) plays an important role in task-oriented dialogue systems. However, collecting a large amount of turn-by-turn annotated dialogue data is costly and inefficient. In this paper, we propose a novel turn-level active learning framework for DST to actively select turns in... | Fanghua Ye, Ling Chen, Meng Fang, MohammadReza NamaziRad, Zihan Zhang |  |
| 1673 |  |  [ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.479) |  | 0 | Incorporating visual knowledge into text-only dialogue systems has become a potential direction to imitate the way humans think, imagine, and communicate. However, existing multimodal dialogue systems are either confined by the scale and quality of available datasets or the coarse concept of visual... | Fei Mi, Haoqin Tu, Yitong Li, Zhongliang Yang |  |
| 1674 |  |  [Modeling Conceptual Attribute Likeness and Domain Inconsistency for Metaphor Detection](https://doi.org/10.18653/v1/2023.emnlp-main.480) |  | 0 | Metaphor detection is an important and challenging task in natural language processing, which aims to distinguish between metaphorical and literal expressions in text. Previous studies mainly leverage the incongruity of source and target domains and contextual clues for detection, neglecting... | Daniel Zeng, Nan Xu, Wenji Mao, Yuan Tian |  |
| 1675 |  |  [Referring Image Segmentation via Joint Mask Contextual Embedding Learning and Progressive Alignment Network](https://doi.org/10.18653/v1/2023.emnlp-main.481) |  | 0 | Referring image segmentation is a task that aims to predict pixel-wise masks corresponding to objects in an image described by natural language expressions. Previous methods for referring image segmentation employ a cascade framework to break down complex problems into multiple stages. However, its... | Shin'ichi Satoh, Ziling Huang |  |
| 1676 |  |  [Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study](https://doi.org/10.18653/v1/2023.emnlp-main.482) |  | 0 | Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To... | Anima Anandkumar, Bo Li, Boxin Wang, Bryan Catanzaro, Chaowei Xiao, Lawrence McAfee, Mohammad Shoeybi, Oleksii Kuchaiev, Peng Xu, Wei Ping, Yi Dong, Zihan Liu |  |
| 1677 |  |  [SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables](https://doi.org/10.18653/v1/2023.emnlp-main.483) |  | 0 | Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from... | Liangming Pan, MinYen Kan, Preslav Nakov, Qian Liu, Xinyuan Lu |  |
| 1678 |  |  [Training Simultaneous Speech Translation with Robust and Random Wait-k-Tokens Strategy](https://doi.org/10.18653/v1/2023.emnlp-main.484) |  | 0 | Simultaneous Speech Translation (SimulST) is a task focused on ensuring high-quality translation of speech in low-latency situations. Despite this, the modality gap (e.g., unknown word boundaries) between audio and text presents a challenge. This gap hinders the effective application of policies... | Jiajun Bu, Kai Fan, Linlin Zhang, Zhongqiang Huang |  |
| 1679 |  |  [SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples](https://doi.org/10.18653/v1/2023.emnlp-main.485) |  | 0 | Detecting negatives (such as non-entailment relationships, unanswerable questions, and false claims) is an important and challenging aspect of many natural language understanding tasks. Though manually collecting challenging negative examples can help models detect them, it is both costly and... | Ameya Godbole, Deqing Fu, Robin Jia |  |
| 1680 |  |  [Enhancing Code-Switching for Cross-lingual SLU: A Unified View of Semantic and Grammatical Coherence](https://doi.org/10.18653/v1/2023.emnlp-main.486) |  | 0 | Despite the success of spoken language understanding (SLU) in high-resource languages, achieving similar performance in low-resource settings, such as zero-shot scenarios, remains challenging due to limited labeled training data. To improve zero-shot cross-lingual SLU, recent studies have explored... | Dongsheng Chen, Xuxin Cheng, Yuexian Zou, Zhihong Zhu, Zhiqi Huang |  |
| 1681 |  |  [Task-Agnostic Low-Rank Adapters for Unseen English Dialects](https://doi.org/10.18653/v1/2023.emnlp-main.487) |  | 0 | Large Language Models (LLMs) are trained on corpora disproportionally weighted in favor of Standard American English. As a result, speakers of other dialects experience significantly more failures when interacting with these technologies. In practice, these speakers often accommodate their speech... | Diyi Yang, William Held, Yanchen Liu, Zedian Xiao |  |
| 1682 |  |  [Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization](https://doi.org/10.18653/v1/2023.emnlp-main.488) |  | 0 | Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in... | Dejing Dou, Huaiyu Dai, Ji Liu, Jiaxiang Ren, Jiwen Zhou, Tianshi Che, Victor S. Sheng, Yang Zhou |  |
| 1683 |  |  [TheoremQA: A Theorem-driven Question Answering Dataset](https://doi.org/10.18653/v1/2023.emnlp-main.489) |  | 0 | The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated.... | Jianyu Xu, Max Ku, Ming Yin, Pan Lu, Tony Xia, Wenhu Chen, Xinyi Wang, Xueguang Ma, Yixin Wan |  |
| 1684 |  |  [Scalable-DSC: A Structural Template Prompt Approach to Scalable Dialogue State Correction](https://doi.org/10.18653/v1/2023.emnlp-main.490) |  | 0 | Dialogue state error correction has recently been proposed to correct wrong slot values in predicted dialogue states, thereby mitigating the error propagation problem for dialogue state tracking (DST). These approaches, though effective, are heavily intertwined with specific DST models, limiting... | Hao Huang, Haoxiang Su, Hongyan Xie, Ruiyu Fang, Shuangyong Song, Sijie Feng, Xiaomeng Huang |  |
| 1685 |  |  [Don't Trust ChatGPT when your Question is not in English: A Study of Multilingual Abilities and Types of LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.491) |  | 0 | Large language models (LLMs) have demonstrated exceptional natural language understanding abilities, and have excelled in a variety of natural language processing (NLP) tasks. Despite the fact that most LLMs are trained predominantly on English, multiple studies have demonstrated their capabilities... | Bradley Hauer, Grzegorz Kondrak, Ning Shi, Senyu Li, Xiang Zhang |  |
| 1686 |  |  [M³Seg: A Maximum-Minimum Mutual Information Paradigm for Unsupervised Topic Segmentation in ASR Transcripts](https://doi.org/10.18653/v1/2023.emnlp-main.492) |  | 0 | Topic segmentation aims to detect topic boundaries and split automatic speech recognition transcriptions (e.g., meeting transcripts) into segments that are bounded by thematic meanings. In this work, we propose M3Seg, a novel Maximum-Minimum Mutual information paradigm for linear topic segmentation... | Ke Wang, Wei Peng, Xiutian Zhao, Yanghui Li |  |
| 1687 |  |  [Empirical Study of Zero-Shot NER with ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.493) |  | 0 | Large language models (LLMs) exhibited powerful capability in various natural language processing tasks. This work focuses on exploring LLM performance on zero-shot information extraction, with a focus on the ChatGPT and named entity recognition (NER) task. Inspired by the remarkable reasoning... | Hongwei Wang, Jian Zhang, Qi Li, Tingyu Xie, Yan Zhang, Zuozhu Liu |  |
| 1688 |  |  [Automatic Prompt Optimization with "Gradient Descent" and Beam Search](https://doi.org/10.18653/v1/2023.emnlp-main.494) |  | 0 | Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Prompt Optimization with... | Chenguang Zhu, Dan Iter, Jerry Li, Michael Zeng, Reid Pryzant, Yin Tat Lee |  |
| 1689 |  |  [Active Retrieval Augmented Generation](https://doi.org/10.18653/v1/2023.emnlp-main.495) |  | 0 | Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing... | Frank F. Xu, Graham Neubig, Jamie Callan, Jane DwivediYu, Luyu Gao, Qian Liu, Yiming Yang, Zhengbao Jiang, Zhiqing Sun |  |
| 1690 |  |  [GD-COMET: A Geo-Diverse Commonsense Inference Model](https://doi.org/10.18653/v1/2023.emnlp-main.496) |  | 0 | With the increasing integration of AI into everyday life, it’s becoming crucial to design AI systems to serve users from diverse backgrounds by making them culturally aware. In this paper, we present GD-COMET, a geo-diverse version of the COMET commonsense inference model. GD-COMET goes beyond... | Mehar Bhatia, Vered Shwartz |  |
| 1691 |  |  [Multi-level Adaptive Contrastive Learning for Knowledge Internalization in Dialogue Generation](https://doi.org/10.18653/v1/2023.emnlp-main.497) |  | 0 | Knowledge-grounded dialogue generation aims to mitigate the issue of text degeneration by incorporating external knowledge to supplement the context. However, the model often fails to internalize this information into responses in a human-like manner. Instead, it simply inserts segments of the... | Chenxu Yang, Chong Tian, Jiangnan Li, Lanrui Wang, Liang Pang, Qirong Ho, Weiping Wang, Yanan Cao, Zheng Lin |  |
| 1692 |  |  [Enhancing Biomedical Lay Summarisation with External Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.498) |  | 0 | Previous approaches for automatic lay summarisation are exclusively reliant on the source article that, given it is written for a technical audience (e.g., researchers), is unlikely to explicitly define all technical concepts or state all of the background information that is relevant for a lay... | Carolina Scarton, Chen Tang, Chenghua Lin, Tomas Goldsack, Zhihao Zhang |  |
| 1693 |  |  [A Diffusion Weighted Graph Framework for New Intent Discovery](https://doi.org/10.18653/v1/2023.emnlp-main.499) |  | 0 | New Intent Discovery (NID) aims to recognize both new and known intents from unlabeled data with the aid of limited labeled data containing only known intents. Without considering structure relationships between samples, previous methods generate noisy supervisory signals which cannot strike a... | Feng Tian, Ping Chen, Qianying Wang, Qinghua Zheng, Wenbin An, Wenkai Shi |  |
| 1694 |  |  [A Self-enhancement Multitask Framework for Unsupervised Aspect Category Detection](https://doi.org/10.18653/v1/2023.emnlp-main.500) |  | 0 | Our work addresses the problem of unsupervised Aspect Category Detection using a small set of seed words. Recent works have focused on learning embedding spaces for seed words and sentences to establish similarities between sentences and aspects. However, aspect representations are limited by the... | Hoang Ngo, KiemHieu Nguyen, ThiNhung Nguyen, TuanDung Cao |  |
| 1695 |  |  [DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.501) |  | 0 | Chain-of-Thought (CoT) prompting has successfully enhanced the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective, or even detrimental, to the performance on reasoning tasks in Smaller Language Models (SLMs) with less than 10... | Baoyuan Wang, Che Zhang, Chengcheng Han, Ming Gao, Xiang Li, Xiaowei Du, Yixin Lian |  |
| 1696 |  |  [Recurrent Neural Language Models as Probabilistic Finite-state Automata](https://doi.org/10.18653/v1/2023.emnlp-main.502) |  | 0 | Studying language models (LMs) in terms of well-understood formalisms allows us to precisely characterize their abilities and limitations. Previous work has investigated the expressive power of recurrent neural network (RNN) LMs in terms of their capacity to recognize unweighted formal languages.... | Anej Svete, Ryan Cotterell |  |
| 1697 |  |  [Revisiting Source Context in Nearest Neighbor Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.503) |  | 0 | Nearest neighbor machine translation (kNN-MT), which interpolates target token probabilities with estimates derived from additional examples, has achieved significant improvements and attracted extensive interest in recent years. However, existing research does not explicitly consider the source... | Peng Li, Po Hu, Xuanhong Li |  |
| 1698 |  |  [Find-2-Find: Multitask Learning for Anaphora Resolution and Object Localization](https://doi.org/10.18653/v1/2023.emnlp-main.504) |  | 0 | In multimodal understanding tasks, visual and linguistic ambiguities can arise. Visual ambiguity can occur when visual objects require a model to ground a referring expression in a video without strong supervision, while linguistic ambiguity can occur from changes in entities in action flows. As an... | Cennet Oguz, Emmanuel Vincent, Josef van Genabith, Pascal Denis, Simon Ostermann |  |
| 1699 |  |  [Background Summarization of Event Timelines](https://doi.org/10.18653/v1/2023.emnlp-main.505) |  | 0 | Generating concise summaries of news events is a challenging natural language processing task. While journalists often curate timelines to highlight key sub-events, newcomers to a news event face challenges in catching up on its historical context. In this paper, we address this need by introducing... | Adithya Pratapa, Kevin Small, Markus Dreyer |  |
| 1700 |  |  [Superlim: A Swedish Language Understanding Evaluation Benchmark](https://doi.org/10.18653/v1/2023.emnlp-main.506) |  | 0 | We present Superlim, a multi-task NLP benchmark and analysis platform for evaluating Swedish language models, a counterpart to the English-language (Super)GLUE suite. We describe the dataset, the tasks, the leaderboard and report the baseline results yielded by a reference implementation. The... | Aleksandrs Berdicevskis, Anna Lindahl, Dana Dannélls, Elena Volodina, Faton Rekathati, Felix Morger, Gerlof Bouma, Joey Öhman, Lars Borin, Love Börjeson, Magnus Sahlgren, Markus Forsberg, Martin Malmsten, Nina Tahmasebi, Robin Kurtz, Simon Hengchen, Tim Isbister, Yvonne Adesam |  |
| 1701 |  |  [Reasoning with Language Model is Planning with World Model](https://doi.org/10.18653/v1/2023.emnlp-main.507) |  | 0 | Large language models (LLMs) have shown remarkable reasoning capabilities, particularly with Chain-of-Thought-style prompts. However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning.... | Daisy Zhe Wang, Haodi Ma, Joshua Jiahua Hong, Shibo Hao, Yi Gu, Zhen Wang, Zhiting Hu |  |
| 1702 |  |  [LLM-enhanced Self-training for Cross-domain Constituency Parsing](https://doi.org/10.18653/v1/2023.emnlp-main.508) |  | 0 | Self-training has proven to be an effective approach for cross-domain tasks, and in this study, we explore its application to cross-domain constituency parsing. Traditional self-training methods rely on limited and potentially low-quality raw corpora. To overcome this limitation, we propose... | Jianling Li, Meishan Zhang, Min Zhang, Peiming Guo, Yue Zhang |  |
| 1703 |  |  [Continual Named Entity Recognition without Catastrophic Forgetting](https://doi.org/10.18653/v1/2023.emnlp-main.509) |  | 0 | Continual Named Entity Recognition (CNER) is a burgeoning area, which involves updating an existing model by incorporating new entity types sequentially. Nevertheless, continual learning approaches are often severely afflicted by catastrophic forgetting. This issue is intensified in CNER due to the... | Duzhen Zhang, Jiahua Dong, Wei Cong, Xiuyi Chen, Yahan Yu, Yonggang Zhang, Zhen Fang |  |
| 1704 |  |  [DSI++: Updating Transformer Memory with New Documents](https://doi.org/10.18653/v1/2023.emnlp-main.510) |  | 0 | Differentiable Search Indices (DSIs) encode a corpus of documents in the parameters of a model and use the same model to map queries directly to relevant document identifiers. Despite the solid performance of DSI models, successfully deploying them in scenarios where document corpora change with... | Donald Metzler, Emma Strubell, Jai Gupta, Jinfeng Rao, Marc Najork, Mostafa Dehghani, Sanket Vaibhav Mehta, Vinh Q. Tran, Yi Tay |  |
| 1705 |  |  [Editing Common Sense in Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.511) |  | 0 | Editing model parameters directly in Transformers makes updating open-source transformer-based models possible without re-training. However, these editing methods have only been evaluated on statements about encyclopedic knowledge with a single correct answer. Commonsense knowledge with multiple... | Akshay Krishna Sheshadri, Anshita Gupta, Debanjan Mondal, Niket Tandon, Sarah Wiegreffe, Wenlong Zhao, Xiang Li |  |
| 1706 |  |  [Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.512) |  | 0 | Controllable text generation (CTG) aims to generate text with desired attributes, and decoding-time-based methods have shown promising performance on this task. However, in this paper, we identify the phenomenon of Attribute Collapse for the first time. It causes the fluency of generated text to... | Jingxuan Han, Quan Wang, Tianqi Zhong, Yongdong Zhang, Zhendong Mao |  |
| 1707 |  |  [Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.513) |  | 0 | Transformers have become a key architecture in speech processing, but our understanding of how they build up representations of acoustic and linguistic structure is limited. In this study, we address this gap by investigating how measures of ‘context-mixing’ developed for text models can be adapted... | Afra Alishahi, Grzegorz Chrupala, Hosein Mohebbi, Willem H. Zuidema |  |
| 1708 |  |  [Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System](https://doi.org/10.18653/v1/2023.emnlp-main.514) |  | 0 | Developing an efficient retriever to retrieve knowledge from a large-scale knowledge base (KB) is critical for task-oriented dialogue systems to effectively handle localized and specialized tasks. However, widely used generative models such as T5 and ChatGPT often struggle to differentiate subtle... | Canbin Huang, Fanqi Wan, Wei Bi, Weizhou Shen, Xiaojun Quan, Yingqi Gao |  |
| 1709 |  |  [IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions](https://doi.org/10.18653/v1/2023.emnlp-main.515) |  | 0 | Although counterfactual reasoning is a fundamental aspect of intelligence, the lack of large-scale counterfactual open-domain question-answering (QA) benchmarks makes it difficult to evaluate and improve models on this ability. To address this void, we introduce the first such dataset, named IfQA,... | Ashish Sabharwal, Meng Jiang, Peter Clark, Wenhao Yu |  |
| 1710 |  |  [How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances](https://doi.org/10.18653/v1/2023.emnlp-main.516) |  | 0 | Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era. This paper provides a comprehensive review of recent advances in aligning deployed LLMs with... | Jun Wang, Ling Chen, Meng Fang, MohammadReza NamaziRad, Zihan Zhang |  |
| 1711 |  |  [PreWoMe: Exploiting Presuppositions as Working Memory for Long Form Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.517) |  | 0 | Information-seeking questions in long-form question answering (LFQA) often prove misleading due to ambiguity or false presupposition in the question. While many existing approaches handle misleading questions, they are tailored to limited questions, which are insufficient in a real-world setting... | Jinsol Park, Kyungjae Lee, Wookje Han |  |
| 1712 |  |  [Memorisation Cartography: Mapping out the Memorisation-Generalisation Continuum in Neural Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.518) |  | 0 | When training a neural network, it will quickly memorise some source-target mappings from your dataset but never learn some others. Yet, memorisation is not easily expressed as a binary feature that is good or bad: individual datapoints lie on a memorisation-generalisation continuum. What... | Dieuwke Hupkes, Ivan Titov, Verna Dankers |  |
| 1713 |  |  [DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4](https://doi.org/10.18653/v1/2023.emnlp-main.519) |  | 0 | Human preference judgments are pivotal in guiding large language models (LLMs) to produce outputs that align with human values. Human evaluations are also used in summarization tasks to compare outputs from various systems, complementing existing automatic metrics. Despite their significance,... | Fei Liu, Hassan Foroosh, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Yebowen Hu |  |
| 1714 |  |  [Gender Biases in Automatic Evaluation Metrics for Image Captioning](https://doi.org/10.18653/v1/2023.emnlp-main.520) |  | 0 | Model-based evaluation metrics (e.g., CLIPScore and GPTScore) have demonstrated decent correlations with human judgments in various language generation tasks. However, their impact on fairness remains largely unexplored. It is widely recognized that pretrained models can inadvertently encode... | Asli Celikyilmaz, Haoyi Qiu, Nanyun Peng, Tianlu Wang, ZiYi Dou |  |
| 1715 |  |  [QA-NatVer: Question Answering for Natural Logic-based Fact Verification](https://doi.org/10.18653/v1/2023.emnlp-main.521) |  | 0 | Fact verification systems assess a claim’s veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural... | Andreas Vlachos, Marek Strong, Rami Aly |  |
| 1716 |  |  [Increasing Probability Mass on Answer Choices Does Not Always Improve Accuracy](https://doi.org/10.18653/v1/2023.emnlp-main.522) |  | 0 | When pretrained language models (LMs) are applied to discriminative tasks such as multiple-choice questions, they place probability mass on vocabulary tokens that aren’t among the given answer choices. Spreading probability mass across multiple surface forms with identical meaning (such as “bath”... | Ashish Sabharwal, Matthew Finlayson, Oyvind Tafjord, Peter Clark, Sarah Wiegreffe |  |
| 1717 |  |  [Generating Data for Symbolic Language with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.523) |  | 0 | While large language models (LLMs) bring not only performance but also complexity, recent work has started to turn LLMs into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily... | Chengzu Li, Jiacheng Ye, Lingpeng Kong, Tao Yu |  |
| 1718 |  |  [IDTraffickers: An Authorship Attribution Dataset to link and connect Potential Human-Trafficking Operations on Text Escort Advertisements](https://doi.org/10.18653/v1/2023.emnlp-main.524) |  | 0 | Human trafficking (HT) is a pervasive global issue affecting vulnerable individuals, violating their fundamental human rights. Investigations reveal that a significant number of HT cases are associated with online advertisements (ads), particularly in escort markets. Consequently, identifying and... | Benjamin Bashpole, Gerasimos Spanakis, Gijs van Dijck, Vageesh Saxena |  |
| 1719 |  |  [Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.525) |  | 0 | Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. Therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance... | Desmond Elliott, Emanuele Bugliarello, Laura Cabello, Stephanie Brandl |  |
| 1720 |  |  [Improving Dialogue Discourse Parsing via Reply-to Structures of Addressee Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.526) |  | 0 | Dialogue discourse parsing aims to reflect the relation-based structure of dialogue by establishing discourse links according to discourse relations. To alleviate data sparsity, previous studies have adopted multitasking approaches to jointly learn dialogue discourse parsing with related tasks... | Fang Kong, Feng Jiang, Peifeng Li, Qiaoming Zhu, Yaxin Fan |  |
| 1721 |  |  [Improving Language Models' Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary](https://doi.org/10.18653/v1/2023.emnlp-main.527) |  | 0 | The non-humanlike behaviour of contemporary pre-trained language models (PLMs) is a leading cause undermining their trustworthiness. A striking phenomenon of such faulty behaviours is the generation of inconsistent predictions, which produces logically contradictory results, such as generating... | Myeongjun Jang, Thomas Lukasiewicz |  |
| 1722 |  |  [DALE: Generative Data Augmentation for Low-Resource Legal NLP](https://doi.org/10.18653/v1/2023.emnlp-main.528) |  | 0 | We present DALE, a novel and effective generative Data Augmentation framework for low-resource LEgal NLP. DALE addresses the challenges existing frameworks pose in generating effective data augmentations of legal documents - legal language, with its specialized vocabulary and complex semantics,... | Chandra Kiran Reddy Evuru, Dinesh Manocha, Ramaneswaran S., Sakshi Singh, Sonal Kumar, Sreyan Ghosh, Utkarsh Tyagi |  |
| 1723 |  |  [FedID: Federated Interactive Distillation for Large-Scale Pretraining Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.529) |  | 0 | The growing concerns and regulations surrounding the protection of user data privacy have necessitated decentralized training paradigms. To this end, federated learning (FL) is widely studied in user-related natural language processing (NLP). However, it suffers from several critical limitations... | Jiangming Liu, Jin Wang, Xinge Ma, Xuejie Zhang |  |
| 1724 |  |  [trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.530) |  | 0 | Reinforcement learning from human feedback (RLHF) utilizes human feedback to better align large language models with human preferences via online optimization against a learned reward model. Current RLHF paradigms rely on Proximal Policy Optimization (PPO), which quickly becomes a challenge to... | Alexander Havrilla, Aman Tiwari, Duy Phung, Jonathan Tow, Louis Castricato, Maksym Zhuravinskyi, Quentin Anthony, Stella Biderman |  |
| 1725 |  |  [This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.531) |  | 0 | Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing. We try to clarify the reasons for the sub-optimal performance of LLMs... | Begoña Altuna, German Rigau, Iker GarcíaFerrero, Itziar GonzalezDios, Javier Álvez |  |
| 1726 |  |  [MT2: Towards a Multi-Task Machine Translation Model with Translation-Specific In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.532) |  | 0 | Sentence-level translation, document-level translation, translation memory, and terminology constrained translation play an important role in machine translation. Most of the previous work uses separate models or methods to solve these tasks, which is not conducive to knowledge transfer of... | Chunyou Li, Hongxiao Zhang, Jinan Xu, Ming Zhou, Mingtong Liu, Yufeng Chen |  |
| 1727 |  |  [CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset](https://doi.org/10.18653/v1/2023.emnlp-main.533) |  | 0 | The CoNLL-03 corpus is arguably the most well-known and utilized benchmark dataset for named entity recognition (NER). However, prior works found significant numbers of annotation errors, incompleteness, and inconsistencies in the data. This poses challenges to objectively comparing NER approaches... | Alan Akbik, Susanna Rücker |  |
| 1728 |  |  [Disentangling Transformer Language Models as Superposed Topic Models](https://doi.org/10.18653/v1/2023.emnlp-main.534) |  | 0 | Topic Modelling is an established research area where the quality of a given topic is measured using coherence metrics. Often, we infer topics from Neural Topic Models (NTM) by interpreting their decoder weights, consisting of top-activated words projected from individual neurons. Transformer-based... | Hady W. Lauw, Jia Peng Lim |  |
| 1729 |  |  [Conversational Semantic Parsing using Dynamic Context Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.535) |  | 0 | In this paper we consider the task of conversational semantic parsing over general purpose knowledge graphs (KGs) with millions of entities, and thousands of relation-types. We focus on models which are capable of interactively mapping user utterances into executable logical forms (e.g., Sparql) in... | Mirella Lapata, Parag Jain |  |
| 1730 |  |  [Not all quantifiers are equal: Probing Transformer-based language models' understanding of generalised quantifiers](https://doi.org/10.18653/v1/2023.emnlp-main.536) |  | 0 | How do different generalised quantifiers affect the behaviour of transformer-based language models (TLMs)? The recent popularity of TLMs and the central role generalised quantifiers have traditionally played in linguistics and logic bring this question into particular focus. The current research... | Hao Li, Ian PrattHartmann, Iqra Zahid, Riza BatistaNavarro, Tharindu Madusanka |  |
| 1731 |  |  [Structure-aware Knowledge Graph-to-text Generation with Planning Selection and Similarity Distinction](https://doi.org/10.18653/v1/2023.emnlp-main.537) |  | 0 | The knowledge graph-to-text (KG-to-text) generation task aims to synthesize coherent and engaging sentences that accurately convey the complex information derived from an input knowledge graph. One of the primary challenges in this task is bridging the gap between the diverse structures of the KG... | Cheng Yan, Feng Zhao, Hongzhi Zou |  |
| 1732 |  |  [SOUL: Towards Sentiment and Opinion Understanding of Language](https://doi.org/10.18653/v1/2023.emnlp-main.538) |  | 0 | Sentiment analysis is a well-established natural language processing task, with sentiment polarity classification being one of its most popular and representative tasks. However, despite the success of pre-trained language models in this area, they often fall short of capturing the broader... | Lidong Bing, Sinno Jialin Pan, Wenxuan Zhang, Yue Deng |  |
| 1733 |  |  [Regulation and NLP (RegNLP): Taming Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.539) |  | 0 | The scientific innovation in Natural Language Processing (NLP) and more broadly in artificial intelligence (AI) is at its fastest pace to date. As large language models (LLMs) unleash a new era of automation, important debates emerge regarding the benefits and risks of their development, deployment... | Catalina Goanta, Gerasimos Spanakis, Ilias Chalkidis, Nikolaos Aletras, Sofia Ranchordás |  |
| 1734 |  |  [MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation](https://doi.org/10.18653/v1/2023.emnlp-main.540) |  | 0 | Curated datasets for healthcare are often limited due to the need of human annotations from experts. In this paper, we present MedEval, a multi-level, multi-task, and multi-domain medical benchmark to facilitate the development of language models for healthcare. MedEval is comprehensive and... | Amilcare Gentili, An Yan, ChunNan Hsu, Eric Y. Chang, Julian J. McAuley, Yao Liu, Yu Wang, Zexue He |  |
| 1735 |  |  [Seeing through the mess: evolutionary dynamics of lexical polysemy](https://doi.org/10.18653/v1/2023.emnlp-main.541) |  | 0 | Evidently, words can have multiple senses. For example, the word mess refers to a place to have food or to a confusing situation. How exactly multiple senses emerge is less clear. In this work, we propose and analyze a mathematical model of the evolution of lexical meaning to investigate mechanisms... | Andreas Baumann, Andreas Stephan, Benjamin Roth |  |
| 1736 |  |  [Are Embedded Potatoes Still Vegetables? On the Limitations of WordNet Embeddings for Lexical Semantics](https://doi.org/10.18653/v1/2023.emnlp-main.542) |  | 0 | Knowledge Base Embedding (KBE) models have been widely used to encode structured information from knowledge bases, including WordNet. However, the existing literature has predominantly focused on link prediction as the evaluation task, often neglecting exploration of the models’ semantic... | Guy Emerson, Michael Sejr Schlichtkrull, Xuyou Cheng |  |
| 1737 |  |  [Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.543) |  | 0 | Large Language Models (LLMs) evaluation is a patchy and inconsistent landscape, and it is becoming clear that the quality of automatic evaluation metrics is not keeping up with the pace of development of generative models. We aim to improve the understanding of current models’ performance by... | Andrea Sottana, Bin Liang, Kai Zou, Zheng Yuan |  |
| 1738 |  |  [Event-Location Tracking in Narratives: A Case Study on Holocaust Testimonies](https://doi.org/10.18653/v1/2023.emnlp-main.544) |  | 0 | This work focuses on the spatial dimension of narrative understanding and presents the task of event-location tracking in narrative texts. The task intends to extract the sequence of locations where the narrative is set through its progression. We present several architectures for the task that... | Eitan Wagner, Omri Abend, Renana Keydar |  |
| 1739 |  |  [Dialogizer: Context-aware Conversational-QA Dataset Generation from Textual Sources](https://doi.org/10.18653/v1/2023.emnlp-main.545) |  | 0 | To address the data scarcity issue in Conversational question answering (ConvQA), a dialog inpainting method, which utilizes documents to generate ConvQA datasets, has been proposed. However, the original dialog inpainting model is trained solely on the dialog reconstruction task, resulting in the... | Hwanhee Lee, Hyunkyung Bae, Jeesoo Bang, Kyomin Jung, Yerin Hwang, Yongil Kim |  |
| 1740 |  |  [Learning to Predict Task Transferability via Soft Prompt](https://doi.org/10.18653/v1/2023.emnlp-main.546) |  | 0 | Fine-tuning pretrained language models on helpful intermediate tasks often greatly improves the performance of target tasks. However, how to efficiently find the source tasks that can successfully transfer still remains under-explored. In this work, we propose to learn an affinity scoring function... | Lingyun Feng |  |
| 1741 |  |  [Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.547) |  | 0 | We propose Chain-of-Questions, a framework that trains a model to robustly answer multistep questions by generating and answering sub-questions. We obtain supervision for sub-questions from human-annotated question decomposition meaning representation (QDMR), but QDMR does not include annotated... | Jesse Thomason, Robin Jia, Wang Zhu |  |
| 1742 |  |  [Mirror: A Universal Framework for Various Information Extraction Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.548) |  | 0 | Sharing knowledge between information extraction tasks has always been a challenge due to the diverse data formats and task variations. Meanwhile, this divergence leads to information waste and increases difficulties in building complex applications in real scenarios. Recent studies often formulate... | Baoxing Huai, Guoliang Zhang, Junfei Ren, Mengsong Wu, Min Zhang, Tong Zhu, Wenliang Chen, Xiaoye Qu, Zhefeng Wang, Zijian Yu |  |
| 1743 |  |  ["Mistakes Help Us Grow": Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms](https://doi.org/10.18653/v1/2023.emnlp-main.549) |  | 0 | Teachers’ growth mindset supportive language (GMSL)—rhetoric emphasizing that one’s skills can be improved over time—has been shown to significantly reduce disparities in academic achievement and enhance students’ learning outcomes. Although teachers espouse growth mindset principles, most find it... | David S. Yeager, Diyi Yang, Dorottya Demszky, Jessica Boyle, Kunal Handa, Margaret Clapper, Rose E. Wang |  |
| 1744 |  |  [Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text](https://doi.org/10.18653/v1/2023.emnlp-main.550) |  | 0 | While Large Language Models (LLMs) have achieved remarkable performance in many tasks, much about their inner workings remains unclear. In this study, we present novel experimental insights into the resilience of LLMs, particularly GPT-4, when subjected to extensive character-level permutations. To... | Qi Cao, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo |  |
| 1745 |  |  [Detecting and Mitigating Hallucinations in Multilingual Summarisation](https://doi.org/10.18653/v1/2023.emnlp-main.551) |  | 0 | Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource languages, where... | Anna Korhonen, Edoardo Maria Ponti, Shay B. Cohen, Yftah Ziser, Yifu Qiu |  |
| 1746 |  |  [Exploring Linguistic Probes for Morphological Inflection](https://doi.org/10.18653/v1/2023.emnlp-main.552) |  | 0 | Modern work on the cross-linguistic computational modeling of morphological inflection has typically employed language-independent data splitting algorithms. In this paper, we supplement that approach with language-specific probes designed to test aspects of morphological generalization. Testing... | Jordan Kodner, Salam Khalifa, Sarah Ruth Brogden Payne |  |
| 1747 |  |  [AMR Parsing with Causal Hierarchical Attention and Pointers](https://doi.org/10.18653/v1/2023.emnlp-main.553) |  | 0 | Translation-based AMR parsers have recently gained popularity due to their simplicity and effectiveness. They predict linearized graphs as free texts, avoiding explicit structure modeling. However, this simplicity neglects structural locality in AMR graphs and introduces unnecessary tokens to... | Chao Lou, Kewei Tu |  |
| 1748 |  |  [FLatS: Principled Out-of-Distribution Detection with Feature-Based Likelihood Ratio Score](https://doi.org/10.18653/v1/2023.emnlp-main.554) |  | 0 | Detecting out-of-distribution (OOD) instances is crucial for NLP models in practical applications. Although numerous OOD detection methods exist, most of them are empirical. Backed by theoretical analysis, this paper advocates for the measurement of the “OOD-ness” of a test case x through the... | Haowei Lin, Yuntian Gu |  |
| 1749 |  |  [Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.555) |  | 0 | Text classification tasks often encounter few-shot scenarios with limited labeled data, and addressing data scarcity is crucial. Data augmentation with mixup merges sample pairs to generate new pseudos, which can relieve the data deficiency issue in text classification. However, the quality of... | Changjian Wang, Dacheng Tao, Dongsheng Li, Haoqi Zheng, Liang Ding, Qihuang Zhong, Xin Niu, Zhiliang Tian |  |
| 1750 |  |  [IC3: Image Captioning by Committee Consensus](https://doi.org/10.18653/v1/2023.emnlp-main.556) |  | 0 | If you ask a human to describe an image, they might do so in a thousand different ways. Traditionally, image captioning models are trained to generate a single “best’ (most like a reference) image caption. Unfortunately, doing so encourages captions that are “informationally impoverished,’ and... | Austin Myers, David A. Ross, David Chan, John F. Canny, Sudheendra Vijayanarasimhan |  |
| 1751 |  |  [SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.557) |  | 0 | Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either... | Adian Liusie, Mark J. F. Gales, Potsawee Manakul |  |
| 1752 |  |  [Fair Without Leveling Down: A New Intersectional Fairness Definition](https://doi.org/10.18653/v1/2023.emnlp-main.558) |  | 0 | In this work, we consider the problem of intersectional group fairness in the classification setting, where the objective is to learn discrimination-free models in the presence of several intersecting sensitive groups. First, we illustrate various shortcomings of existing fairness measures commonly... | Aurélien Bellet, Gaurav Maheshwari, Mikaela Keller, Pascal Denis |  |
| 1753 |  |  [Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications](https://doi.org/10.18653/v1/2023.emnlp-main.559) |  | 0 | Instruction Fine-Tuning (IFT) is a powerful paradigm that strengthens the zero-shot capabilities of Large Language Models (LLMs), but in doing so induces new evaluation metric requirements. We show LLM-based metrics to be well adapted to these requirements, and leverage them to conduct an... | Céline Hudelot, Gautier Viaud, Manuel Faysse, Pierre Colombo |  |
| 1754 |  |  [CLAD-ST: Contrastive Learning with Adversarial Data for Robust Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-main.560) |  | 0 | The cascaded approach continues to be the most popular choice for speech translation (ST). This approach consists of an automatic speech recognition (ASR) model and a machine translation (MT) model that are used in a pipeline to translate speech in one language to text in another language. MT... | Marco Turchi, Ravi Agrawal, Sathish Indurthi, Shamil Chollampatt |  |
| 1755 |  |  [M2DF: Multi-grained Multi-curriculum Denoising Framework for Multimodal Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.561) |  | 0 | Multimodal Aspect-based Sentiment Analysis (MABSA) is a fine-grained Sentiment Analysis task, which has attracted growing research interests recently. Existing work mainly utilizes image information to improve the performance of MABSA task. However, most of the studies overestimate the importance... | Chunhui Li, Fei Zhao, Jianbing Zhang, Xinyu Dai, Yawen Ouyang, Zhen Wu |  |
| 1756 |  |  [Detection of Multiple Mental Disorders from Social Media with Two-Stream Psychiatric Experts](https://doi.org/10.18653/v1/2023.emnlp-main.562) |  | 0 | Existing Mental Disease Detection (MDD) research largely studies the detection of a single disorder, overlooking the fact that mental diseases might occur in tandem. Many approaches are not backed by domain knowledge (e.g., psychiatric symptoms) and thus fail to produce interpretable results. To... | Kenny Q. Zhu, Mengyue Wu, Siyuan Chen, Zhiling Zhang |  |
| 1757 |  |  [Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?](https://doi.org/10.18653/v1/2023.emnlp-main.563) |  | 0 | Understanding how and what pre-trained language models (PLMs) learn about language is an open challenge in natural language processing. Previous work has focused on identifying whether they capture semantic and syntactic information, and how the data or the pre-training objective affects their... | Ahmed Alajrami, Katerina Margatina, Nikolaos Aletras |  |
| 1758 |  |  [Improved Unsupervised Chinese Word Segmentation Using Pre-trained Knowledge and Pseudo-labeling Transfer](https://doi.org/10.18653/v1/2023.emnlp-main.564) |  | 0 | Unsupervised Chinese word segmentation (UCWS) has made progress by incorporating linguistic knowledge from pre-trained language models using parameter-free probing techniques. However, such approaches suffer from increased training time due to the need for multiple inferences using a pre-trained... | Chun Lin, HsiuWen Li, HungYu Kao, YiTing Li, YingJia Lin |  |
| 1759 |  |  [EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.565) |  | 0 | Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most... | Decheng Wu, Hanlin Tang, Jianchen Zhu, Kai Liu, Yifu Sun, Zhanhui Kang |  |
| 1760 |  |  [Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings](https://doi.org/10.18653/v1/2023.emnlp-main.566) |  | 0 | Entity linking methods based on dense retrieval are widely adopted in large-scale applications for their efficiency, but they can fall short of generative models, as they are sensitive to the structure of the embedding space. To address this issue, this paper introduces DUCK, an approach to... | Frédéric A. Dreyer, Louis Martin, Mattia Atzeni, Mikhail Plekhanov, Nicola Cancedda, Nora Kassner, Simone Merello |  |
| 1761 |  |  [APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.567) |  | 0 | With the continuous growth of large language models, the process of fine-tuning these models for new tasks has become increasingly parameter-intensive. Prompt tuning, a method that involves tuning a small set of soft prompts, has emerged as an effective and efficient approach for adapting large... | Dongfang Liu, Fuli Feng, Hanchao Yu, Jingang Wang, Lifu Huang, Qifan Wang, Shaoliang Nie, Sinong Wang, Xiaojun Quan, Yuning Mao, Zenglin Xu |  |
| 1762 |  |  [What's "up" with vision-language models? Investigating their struggle with spatial reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.568) |  | 0 | Recent vision-language (VL) models are powerful, but can they reliably distinguish “right” from “left”? We curate three new corpora to quantify model comprehension of such basic spatial relations. These tests isolate spatial reasoning more precisely than existing datasets like VQAv2, e.g., our... | Amita Kamath, Jack Hessel, KaiWei Chang |  |
| 1763 |  |  [IBADR: an Iterative Bias-Aware Dataset Refinement Framework for Debiasing NLU models](https://doi.org/10.18653/v1/2023.emnlp-main.569) |  | 0 | As commonly-used methods for debiasing natural language understanding (NLU) models, dataset refinement approaches heavily rely on manual data analysis, and thus maybe unable to cover all the potential biased features. In this paper, we propose IBADR, an Iterative Bias-Aware Dataset Refinement... | Hua Wu, Jinsong Su, Lijie Wang, Xiaoyue Wang, Xin Liu, Yaoxiang Wang |  |
| 1764 |  |  [Learning Preference Model for LLMs via Automatic Preference Data Generation](https://doi.org/10.18653/v1/2023.emnlp-main.570) |  | 0 | Despite the advanced capacities of the state-of-the-art large language models (LLMs), they suffer from issues of hallucination, stereotype, etc. Preference models play an important role in LLM alignment, yet training preference models predominantly rely on human-annotated data. This reliance limits... | Jianqiao Zhao, Liwei Wang, Shijia Huang, Yanyang Li |  |
| 1765 |  |  [Multilingual k-Nearest-Neighbor Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.571) |  | 0 | k-nearest-neighbor machine translation has demonstrated remarkable improvements in machine translation quality by creating a datastore of cached examples. However, these improvements have been limited to high-resource language pairs, with large datastores, and remain a challenge for low-resource... | Christof Monz, David Stap |  |
| 1766 |  |  [Understanding Computational Models of Semantic Change: New Insights from the Speech Community](https://doi.org/10.18653/v1/2023.emnlp-main.572) |  | 0 | We investigate the descriptive relevance of widely used semantic change models in linguistic descriptions of present-day speech communities. We focus on the sociolinguistic issue of contact-induced semantic shifts in Quebec English, and analyze 40 target words using type-level and token-level word... | Anne PrzewoznyDesriaux, Filip Miletic, Ludovic Tanguy |  |
| 1767 |  |  [Causal Reasoning through Two Cognition Layers for Improving Generalization in Visual Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.573) |  | 0 | Generalization in Visual Question Answering (VQA) requires models to answer questions about images with contexts beyond the training distribution. Existing attempts primarily refine unimodal aspects, overlooking enhancements in multimodal aspects. Besides, diverse interpretations of the input lead... | Naoaki Okazaki, Trang Nguyen |  |
| 1768 |  |  [StructGPT: A General Framework for Large Language Model to Reason over Structured Data](https://doi.org/10.18653/v1/2023.emnlp-main.574) |  | 0 | In this paper, we aim to improve the reasoning ability of large language models (LLMs) over structured data in a unified way. Inspired by the studies on tool augmentation for LLMs, we develop an Iterative Reading-then-Reasoning (IRR) framework to solve question answering tasks based on structured... | JiRong Wen, Jinhao Jiang, Keming Ye, Kun Zhou, Xin Zhao, Zican Dong |  |
| 1769 |  |  [Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement](https://doi.org/10.18653/v1/2023.emnlp-main.575) |  | 0 | Generative language models (LMs) are increasingly used for document class-prediction tasks and promise enormous improvements in cost and efficiency. Existing research often examines simple classification tasks, but the capability of LMs to classify on complex or specialized tasks is less well... | David Mimno, Edward H. Stiglitz, Matthew Wilkens, Rosamond Elizabeth Thalken |  |
| 1770 |  |  [Model-tuning Via Prompts Makes NLP Models Adversarially Robust](https://doi.org/10.18653/v1/2023.emnlp-main.576) |  | 0 | In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token’s hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a... | Danish Pruthi, J. Zico Kolter, Mrigank Raman, Pratyush Maini, Zachary C. Lipton |  |
| 1771 |  |  [Learning Co-Speech Gesture for Multimodal Aphasia Type Detection](https://doi.org/10.18653/v1/2023.emnlp-main.577) |  | 0 | Aphasia, a language disorder resulting from brain damage, requires accurate identification of specific aphasia types, such as Broca’s and Wernicke’s aphasia, for effective treatment. However, little attention has been paid to developing methods to detect different types of aphasia. Recognizing the... | Daeun Lee, Hyolim Jeon, Jinyoung Han, Sejung Son, Seungbae Kim |  |
| 1772 |  |  [STINMatch: Semi-Supervised Semantic-Topological Iteration Network for Financial Risk Detection via News Label Diffusion](https://doi.org/10.18653/v1/2023.emnlp-main.578) |  | 0 | Commercial news provide rich semantics and timely information for automated financial risk detection. However, unaffordable large-scale annotation as well as training data sparseness barrier the full exploitation of commercial news in risk detection. To address this problem, we propose a... | Changlong Sun, Fubang Zhao, Haixu Tang, Kaisong Song, Rui Zhu, Tianqianjin Lin, Xiaozhong Liu, Xurui Li, Yangyang Kang, Yongming Fan, Yue Qin |  |
| 1773 |  |  [Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection](https://doi.org/10.18653/v1/2023.emnlp-main.579) |  | 0 | The impact of AI models on marginalized communities has traditionally been measured by identifying performance differences between specified demographic subgroups. Though this approach aims to center vulnerable groups, it risks obscuring patterns of harm faced by intersectional subgroups or shared... | Dan Klein, Eve Fleisig, Vyoma Raman |  |
| 1774 |  |  [Describe Me an Auklet: Generating Grounded Perceptual Category Descriptions](https://doi.org/10.18653/v1/2023.emnlp-main.580) |  | 0 | Human speakers can generate descriptions of perceptual concepts, abstracted from the instance-level. Moreover, such descriptions can be used by other speakers to learn provisional representations of those concepts. Learning and using abstract perceptual concepts is under-investigated in the... | Bill Noble, Nikolai Ilinykh |  |
| 1775 |  |  [Revisiting Automated Topic Model Evaluation with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.581) |  | 0 | Topic models help us make sense of large text collections. Automatically evaluating their output and determining the optimal number of topics are both longstanding challenges, with no effective automated solutions to date. This paper proposes using large language models (LLMs) for these tasks. We... | Alexander Miserlis Hoyle, Dominik Stammbach, Elliott Ash, Mrinmaya Sachan, Vilém Zouhar |  |
| 1776 |  |  [ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection and Argumentative Dialogue Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.582) |  | 0 | Dialogue agents have been receiving increasing attention for years, and this trend has been further boosted by the recent progress of large language models (LLMs). Stance detection and dialogue summarization are two core tasks of dialogue agents in application scenarios that involve argumentative... | Ke Wang, Wei Peng, Xiutian Zhao |  |
| 1777 |  |  [On the Benefits of Learning to Route in Mixture-of-Experts Models](https://doi.org/10.18653/v1/2023.emnlp-main.583) |  | 0 | Mixture-of-Expert (MoE) Transformer models, such as the Switch Transformer, allow us to successfully scale up model sizes while keeping the amount of compute time fixed. Prior work has established the computational efficiency benefits of using these models. A core component of these models is a... | Nikhil Ghosh, Nikhil Vyas, Nishanth Dikkala, Raghu Meka, Rina Panigrahy, Xin Wang |  |
| 1778 |  |  [SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation](https://doi.org/10.18653/v1/2023.emnlp-main.584) |  | 0 | Reliable automatic evaluation of summarization systems is challenging due to the multifaceted and subjective nature of the task. This is especially the case for languages other than English, where human evaluations are scarce. In this work, we introduce SEAHORSE, a dataset for multilingual,... | Aditya Siddhant, Ankur P. Parikh, Dipanjan Das, Elizabeth Clark, Joshua Maynez, Roee Aharoni, Sebastian Gehrmann, Shruti Rijhwani, Thibault Sellam, Vitaly Nikolaev |  |
| 1779 |  |  [Query2doc: Query Expansion with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.585) |  | 0 | This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo... | Furu Wei, Liang Wang, Nan Yang |  |
| 1780 |  |  [We Need to Talk About Reproducibility in NLP Model Comparison](https://doi.org/10.18653/v1/2023.emnlp-main.586) |  | 0 | NLPers frequently face reproducibility crisis in a comparison of various models of a real-world NLP task. Many studies have empirically showed that the standard splits tend to produce low reproducible and unreliable conclusions, and they attempted to improve the splits by using more random... | Jihong Li, Ruibo Wang, Xingli Yang, Xuefei Cao, Yan Xue, Yu Wang |  |
| 1781 |  |  [Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration](https://doi.org/10.18653/v1/2023.emnlp-main.587) |  | 0 | Instruction-tuning can be substantially optimized through enhanced diversity, resulting in models capable of handling a broader spectrum of tasks. However, existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension... | Fanqi Wan, Shuming Shi, Tao Yang, Wei Bi, Xiaojun Quan, Xinting Huang |  |
| 1782 |  |  [Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions](https://doi.org/10.18653/v1/2023.emnlp-main.588) |  | 0 | Recent studies of the computational power of recurrent neural networks (RNNs) reveal a hierarchy of RNN architectures, given real-time and finite-precision assumptions. Here we study auto-regressive Transformers with linearised attention, a.k.a. linear Transformers (LTs) or Fast Weight Programmers... | Jürgen Schmidhuber, Kazuki Irie, Róbert Csordás |  |
| 1783 |  |  [InterFair: Debiasing with Natural Language Feedback for Fair Interpretable Predictions](https://doi.org/10.18653/v1/2023.emnlp-main.589) |  | 0 | Debiasing methods in NLP models traditionally focus on isolating information related to a sensitive attribute (e.g., gender or race). We instead argue that a favorable debiasing method should use sensitive information ‘fairly,’ with explanations, rather than blindly eliminating it. This fair... | Bodhisattwa Prasad Majumder, Julian J. McAuley, Zexue He |  |
| 1784 |  |  [Just Adjust One Prompt: Enhancing In-Context Dialogue Scoring via Constructing the Optimal Subgraph of Demonstrations and Prompts](https://doi.org/10.18653/v1/2023.emnlp-main.590) |  | 0 | The use of modern Large Language Models (LLMs) as chatbots still has some problems such as hallucinations and lack of empathy. Identifying these issues can help improve chatbot performance. The community has been continually iterating on reference-free dialogue evaluation methods based on large... | Jiashu Pu, Ling Cheng, Lu Fan, Rongsheng Zhang, Tangjie Lv |  |
| 1785 |  |  [Multilingual estimation of political-party positioning: From label aggregation to long-input Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.591) |  | 0 | Scaling analysis is a technique in computational political science that assigns a political actor (e.g. politician or party) a score on a predefined scale based on a (typically long) body of text (e.g. a parliamentary speech or an election manifesto). For example, political scientists have often... | Dmitry Nikolaev, Sebastian Padó, Tanise Ceron |  |
| 1786 |  |  [ART: rule bAsed futuRe-inference deducTion](https://doi.org/10.18653/v1/2023.emnlp-main.592) |  | 0 | Deductive reasoning is a crucial cognitive ability of humanity, allowing us to derive valid conclusions from premises and observations. However, existing works mainly focus on language-based premises and generally neglect deductive reasoning from visual observations. In this work, we introduce rule... | Baoyi He, Fei Wu, Jiaxu Miao, Jionghao Bai, Mengze Li, Shengyu Zhang, Tianqi Zhao, Wei Ji, Wenqiao Zhang, Zheqi Lv, Zhou Zhao |  |
| 1787 |  |  [EpiK-Eval: Evaluation for Language Models as Epistemic Models](https://doi.org/10.18653/v1/2023.emnlp-main.593) |  | 0 | In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents—a crucial ability in numerous applications—remains unexplored. This paper... | Gabriele Prato, Jerry Huang, Prasanna Parthasarathi, Sarath Chandar, Shagun Sodhani |  |
| 1788 |  |  [From Dissonance to Insights: Dissecting Disagreements in Rationale Construction for Case Outcome Classification](https://doi.org/10.18653/v1/2023.emnlp-main.594) |  | 0 | In legal NLP, Case Outcome Classification (COC) must not only be accurate but also trustworthy and explainable. Existing work in explainable COC has been limited to annotations by a single expert. However, it is well-known that lawyers may disagree in their assessment of case facts. We hence... | Barbara Plank, Isabella Risini, Matthias Grabmair, Oana Ichim, Shanshan Xu, T. Y. S. S. Santosh |  |
| 1789 |  |  [On Bilingual Lexicon Induction with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.595) |  | 0 | Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs... | Anna Korhonen, Ivan Vulic, Yaoyiran Li |  |
| 1790 |  |  [Statistical Depth for Ranking and Characterizing Transformer-Based Text Embeddings](https://doi.org/10.18653/v1/2023.emnlp-main.596) |  | 0 | The popularity of transformer-based text embeddings calls for better statistical tools for measuring distributions of such embeddings. One such tool would be a method for ranking texts within a corpus by centrality, i.e. assigning each text a number signifying how representative that text is of the... | Parker Seegmiller, Sarah Preum |  |
| 1791 |  |  [CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model](https://doi.org/10.18653/v1/2023.emnlp-main.597) |  | 0 | Instruction tuning has recently been recognized as an effective way of aligning Large Language Models (LLMs) to enhance their generalization ability across various tasks. However, when tuning publicly accessible, centralized LLMs with private instruction data, privacy concerns are inevitable. While... | Biqing Qi, Bowen Zhou, Kaiyan Zhang, Ning Ding, Xinwei Long, Xuekai Zhu |  |
| 1792 |  |  [From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues](https://doi.org/10.18653/v1/2023.emnlp-main.598) |  | 0 | Understanding emotions during conversation is a fundamental aspect of human communication, driving NLP research for Emotion Recognition in Conversation (ERC). While considerable research has focused on discerning emotions of individual speakers in monolingual dialogues, understanding the emotional... | Md. Shad Akhtar, Ramaneswaran S., Shivani Kumar, Tanmoy Chakraborty |  |
| 1793 |  |  [Large Language Models are biased to overestimate profoundness](https://doi.org/10.18653/v1/2023.emnlp-main.599) |  | 0 | Recent advancements in natural language processing by large language models (LLMs), such as GPT-4, have been suggested to approach Artificial General Intelligence. And yet, it is still under dispute whether LLMs possess similar reasoning abilities to humans. This study evaluates GPT-4 and various... | Cristian Buc Calderon, Eugenio HerreraBerg, MarcLluís Vives, Pablo LeónVillagrá, Tomás Vergara Browne |  |
| 1794 |  |  [SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.600) |  | 0 | With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large... | Alexander R. Fabbri, Caiming Xiong, ChienSheng Wu, Divyansh Agarwal, Philippe Laban, Shafiq Joty, Wojciech Kryscinski |  |
| 1795 |  |  [DIVE: Towards Descriptive and Diverse Visual Commonsense Generation](https://doi.org/10.18653/v1/2023.emnlp-main.601) |  | 0 | Towards human-level visual understanding, visual commonsense generation has been introduced to generate commonsense inferences beyond images. However, current research on visual commonsense generation has overlooked an important human cognitive ability: generating descriptive and diverse... | Eojin Jeon, Hyuntae Park, JunHyung Park, SangKeun Lee, Youjin Kang |  |
| 1796 |  |  [Towards Conceptualization of "Fair Explanation": Disparate Impacts of anti-Asian Hate Speech Explanations on Content Moderators](https://doi.org/10.18653/v1/2023.emnlp-main.602) |  | 0 | Recent research at the intersection of AI explainability and fairness has focused on how explanations can improve human-plus-AI task performance as assessed by fairness measures. We propose to characterize what constitutes an explanation that is itself “fair” – an explanation that does not... | Aayushi Roy, Hal Daumé III, Jiannan Xu, Marine Carpuat, Tin Nguyen |  |
| 1797 |  |  [Bridging Background Knowledge Gaps in Translation with Automatic Explicitation](https://doi.org/10.18653/v1/2023.emnlp-main.603) |  | 0 | Translations help people understand content written in another language. However, even correct literal translations do not fulfill that goal when people lack the necessary background to understand them. Professional translators incorporate explicitations to explain the missing context by... | HyoJung Han, Jordan L. BoydGraber, Marine Carpuat |  |
| 1798 |  |  [A Quality-based Syntactic Template Retriever for Syntactically-Controlled Paraphrase Generation](https://doi.org/10.18653/v1/2023.emnlp-main.604) |  | 0 | Existing syntactically-controlled paraphrase generation (SPG) models perform promisingly with human-annotated or well-chosen syntactic templates. However, the difficulty of obtaining such templates actually hinders the practical application of SPG models. For one thing, the prohibitive cost makes... | Jian Liu, Jinan Xu, Songming Zhang, Wenjuan Han, Xue Zhang, Yufeng Chen, Yunlong Liang |  |
| 1799 |  |  [Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.605) |  | 0 | Using a shared vocabulary is common practice in Multilingual Neural Machine Translation (MNMT). In addition to its simple design, shared tokens play an important role in positive knowledge transfer, which manifests naturally when the shared tokens refer to similar meanings across languages.... | Christof Monz, Di Wu |  |
| 1800 |  |  [Quantifying the redundancy between prosody and text](https://doi.org/10.18653/v1/2023.emnlp-main.606) |  | 0 | Prosody—the suprasegmental component of speech, including pitch, loudness, and tempo—carries critical aspects of meaning. However, the relationship between the information conveyed by prosody vs. by the words themselves remains poorly understood. We use large language models (LLMs) to estimate how... | Alex Warstadt, Ethan Wilcox, Evelina Fedorenko, Lukas Wolf, Ryan Cotterell, Tamar Regev, Tiago Pimentel |  |
| 1801 |  |  [CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.607) |  | 0 | Recent efforts in natural language processing (NLP) commonsense reasoning research have yielded a considerable number of new datasets and benchmarks. However, most of these datasets formulate commonsense reasoning challenges in artificial scenarios that are not reflective of the tasks which... | Antoine Bosselut, Debjit Paul, Mete Ismayilzada, Mor Geva, Syrielle Montariol |  |
| 1802 |  |  [A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot](https://doi.org/10.18653/v1/2023.emnlp-main.608) |  | 0 | Multimedia content, such as advertisements and story videos, exhibit a rich blend of creativity and multiple modalities. They incorporate elements like text, visuals, audio, and storytelling techniques, employing devices like emotions, symbolism, and slogans to convey meaning. There is a dearth of... | Aanisha Bhattacharyya, Balaji Krishnamurthy, Changyou Chen, Rajiv Ratn Shah, Yaman Singla |  |
| 1803 |  |  [Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.609) |  | 0 | In-context learning (ICL) emerges as a promising capability of large language models (LLMs) by providing them with demonstration examples to perform diverse tasks. However, the underlying mechanism of how LLMs learn from the provided context remains under-explored. In this paper, we investigate the... | Damai Dai, Deli Chen, Fandong Meng, Hao Zhou, Jie Zhou, Lean Wang, Lei Li, Xu Sun |  |
| 1804 |  |  [Prompting Scientific Names for Zero-Shot Species Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.610) |  | 0 | Trained on web-scale image-text pairs, Vision-Language Models (VLMs) such as CLIP can recognize images of common objects in a zero-shot fashion. However, it is underexplored how to use CLIP for zero-shot recognition of highly specialized concepts, e.g., species of birds, plants, and animals, for... | Shu Kong, Shubham Parashar, Yanan Li, Zhiqiu Lin |  |
| 1805 |  |  [Active Learning for Natural Language Generation](https://doi.org/10.18653/v1/2023.emnlp-main.611) |  | 0 | The field of Natural Language Generation (NLG) suffers from a severe shortage of labeled data due to the extremely expensive and time-consuming process involved in manual annotation. A natural approach for coping with this problem is active learning (AL), a well-known machine learning technique for... | Ariel Gera, Dafna Sheinwald, Liat EinDor, Michal ShmueliScheuer, Noam Slonim, Yotam Perlitz |  |
| 1806 |  |  [Re³Dial: Retrieve, Reorganize and Rescale Conversations for Long-Turn Open-Domain Dialogue Pre-training](https://doi.org/10.18653/v1/2023.emnlp-main.612) |  | 0 | Pre-training on large-scale open-domain dialogue data can substantially improve the performance of dialogue models. However, the pre-trained dialogue model’s ability to utilize long-range context is limited due to the scarcity of long-turn dialogue sessions. Most dialogues in existing pre-training... | Hao Zhou, Jian Guan, Jiaxin Wen, Jie Zhou, Minlie Huang |  |
| 1807 |  |  [MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup](https://doi.org/10.18653/v1/2023.emnlp-main.613) |  | 0 | Current disfluency detection models focus on individual utterances each from a single speaker. However, numerous discontinuity phenomena in spoken conversational transcripts occur across multiple turns, which can not be identified by disfluency detection models. This study addresses these phenomena... | Daniel D. Walker, Dirk Padfield, Hua Shen, Johann C. Rocholl, Vicky Zayats |  |
| 1808 |  |  [Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.614) |  | 0 | Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of “tokens” processed... | David R. Mortensen, Hila Gonen, Jungo Kasai, Noah A. Smith, Orevaoghene Ahia, Sachin Kumar, Yulia Tsvetkov |  |
| 1809 |  |  [Characterizing Mechanisms for Factual Recall in Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.615) |  | 0 | Language Models (LMs) often must integrate facts they memorized in pretraining with new information that appears in a given context. These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict. On a dataset that queries for knowledge... | Ellie Pavlick, Jack Merullo, Qinan Yu |  |
| 1810 |  |  [MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark](https://doi.org/10.18653/v1/2023.emnlp-main.616) |  | 0 | There is a lack of research into capabilities of recent LLMs to generate convincing text in languages other than English and into performance of detectors of machine-generated text in multilingual settings. This is also reflected in the available benchmarks which lack authentic texts in languages... | Adaku Uchendu, Dominik Macko, Dongwon Lee, Ivan Srba, Jakub Simko, Jason Samuel Lucas, Matús Pikuliak, Michiharu Yamashita, Mária Bieliková, Róbert Móro, Thai Le |  |
| 1811 |  |  [Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?](https://doi.org/10.18653/v1/2023.emnlp-main.617) |  | 0 | The inference of Large language models (LLMs) requires immense computation and memory resources. To curtail these costs, quantisation has emerged as a promising solution, but existing LLM quantisation mainly focuses on 8-bit. In this work, we explore the statistical and learning properties of the... | Cheng Zhang, George A. Constantinides, Ilia Shumailov, Jianyi Cheng, Yiren Zhao |  |
| 1812 |  |  [Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.618) |  | 0 | We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR). Our methodology leverages both acoustic information and external linguistic representations to generate accurate speech transcription contexts. This marks a step towards a... | ChaoHan Huck Yang, David GomezCabrero, Jesper Tegnér, Narsis A. Kiani, Rohit Kumar, Srijith Radhakrishnan, Sumeer Ahmad Khan |  |
| 1813 |  |  [Reducing Sequence Length by Predicting Edit Spans with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.619) |  | 0 | Large Language Models (LLMs) have demonstrated remarkable performance in various tasks and gained significant attention. LLMs are also used for local sequence transduction tasks, including grammatical error correction (GEC) and formality style transfer, where most tokens in a source text are kept... | Masahiro Kaneko, Naoaki Okazaki |  |
| 1814 |  |  [Instruct and Extract: Instruction Tuning for On-Demand Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.620) |  | 0 | Large language models with instruction-following capabilities open the door to a wider group of users. However, when it comes to information extraction – a classic task in natural language processing – most task-specific systems cannot align well with long-tail ad hoc extraction use cases for... | Heng Ji, Jiawei Han, Ming Zhong, Ruining Zhao, Sha Li, Siru Ouyang, Yizhu Jiao |  |
| 1815 |  |  [Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.621) |  | 0 | The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs. In this paper, we embark on an investigation into the utilization of ChatGPT for CRSs,... | JiRong Wen, Jingyuan Wang, Xiaolei Wang, Xin Zhao, Xinyu Tang |  |
| 1816 |  |  [ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness](https://doi.org/10.18653/v1/2023.emnlp-main.622) |  | 0 | Multi-step reasoning ability is fundamental to many natural language tasks, yet it is unclear what constitutes a good reasoning chain and how to evaluate them. Most existing methods focus solely on whether the reasoning chain leads to the correct conclusion, but this answer-oriented view may... | Archiki Prasad, Mohit Bansal, Swarnadeep Saha, Xiang Zhou |  |
| 1817 |  |  [Expand, Highlight, Generate: RL-driven Document Generation for Passage Reranking](https://doi.org/10.18653/v1/2023.emnlp-main.623) |  | 0 | Generating synthetic training data based on large language models (LLMs) for ranking models has gained attention recently. Prior studies use LLMs to build pseudo query-document pairs by generating synthetic queries from documents in a corpus. In this paper, we propose a new perspective of data... | Arian Askari, Chuan Meng, Evangelos Kanoulas, Mohammad Aliannejadi, Suzan Verberne |  |
| 1818 |  |  [Transformer-based Live Update Generation for Soccer Matches from Microblog Posts](https://doi.org/10.18653/v1/2023.emnlp-main.624) |  | 0 | It has been known to be difficult to generate adequate sports updates from a sequence of vast amounts of diverse live tweets, although the live sports viewing experience with tweets is gaining the popularity. In this paper, we focus on soccer matches and work on building a system to generate live... | Koichi Takeda, Kosuke Yamada, Masashi Oshika, Ryohei Sasano |  |
| 1819 |  |  [Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets](https://doi.org/10.18653/v1/2023.emnlp-main.625) |  | 0 | Increasingly larger datasets have become a standard ingredient to advancing the state-of-the-art in NLP. However, data quality might have already become the bottleneck to unlock further gains. Given the diversity and the sizes of modern datasets, standard data filtering is not straight-forward to... | Artem Sokolov, Irina Bejan, Katja Filippova |  |
| 1820 |  |  [Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews](https://doi.org/10.18653/v1/2023.emnlp-main.626) |  | 0 | Medical systematic reviews play a vital role in healthcare decision making and policy. However, their production is time-consuming, limiting the availability of high-quality and up-to-date evidence summaries. Recent advancements in LLMs offer the potential to automatically generate literature... | Byron C. Wallace, Hye Sun Yun, Iain James Marshall, Thomas A. Trikalinos |  |
| 1821 |  |  [PromptST: Abstract Prompt Learning for End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-main.627) |  | 0 | An end-to-end speech-to-text (S2T) translation model is usually initialized from a pre-trained speech recognition encoder and a pre-trained text-to-text (T2T) translation decoder. Although this straightforward setting has been shown empirically successful, there do not exist clear answers to the... | Dacheng Tao, Kehai Chen, Liang Ding, Meishan Zhang, Min Zhang, Tengfei Yu, Xuebo Liu |  |
| 1822 |  |  [Text Rendering Strategies for Pixel Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.628) |  | 0 | Pixel-based language models process text rendered as images, which allows them to handle any script, making them a promising approach to open vocabulary language modelling. However, recent approaches use text renderers that produce a large set of almost-equivalent input patches, which may prove... | Desmond Elliott, Elizabeth Salesky, Jonas F. Lotz, Phillip Rust |  |
| 1823 |  |  [APoLLo : Unified Adapter and Prompt Learning for Vision Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.629) |  | 0 | The choice of input text prompt plays a critical role in the performance of Vision-Language Pretrained (VLP) models such as CLIP. We present APoLLo, a unified multi-modal approach that combines Adapter and Prompt learning for Vision-Language models. Our method is designed to substantially improve... | Dinesh Manocha, Sanjoy Chowdhury, Sayan Nag |  |
| 1824 |  |  [SAMRank: Unsupervised Keyphrase Extraction using Self-Attention Map in BERT and GPT-2](https://doi.org/10.18653/v1/2023.emnlp-main.630) |  | 0 | We propose a novel unsupervised keyphrase extraction approach, called SAMRank, which uses only a self-attention map in a pre-trained language model (PLM) to determine the importance of phrases. Most recent approaches for unsupervised keyphrase extraction mainly utilize contextualized embeddings to... | Byungha Kang, Youhyun Shin |  |
| 1825 |  |  [Contrastive Learning for Inference in Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.631) |  | 0 | Inference, especially those derived from inductive processes, is a crucial component in our conversation to complement the information implicitly or explicitly conveyed by a speaker. While recent large language models show remarkable advances in inference tasks, their performance in inductive... | Bryan Wilie, Etsuko Ishii, Holy Lovenia, Pascale Fung, Willy Chung, Yan Xu, Ziwei Ji |  |
| 1826 |  |  [Editing Large Language Models: Problems, Methods, and Opportunities](https://doi.org/10.18653/v1/2023.emnlp-main.632) |  | 0 | Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to alter the behavior of LLMs efficiently within a... | Bozhong Tian, Huajun Chen, Ningyu Zhang, Peng Wang, Shumin Deng, Siyuan Cheng, Yunzhi Yao, Zhoubo Li |  |
| 1827 |  |  [MarkQA: A large scale KBQA dataset with numerical reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.633) |  | 0 | While question answering over knowledge bases (KBQA) has shown progress in addressing factoid questions, KBQA with numerical reasoning remains relatively unexplored. In this paper, we focus on the complex numerical reasoning in KBQA, and propose a new task, NR-KBQA, which necessitates the ability... | Shanshan Huang, Sitao Cheng, Xiang Huang, Yuheng Bao, Yuzhong Qu |  |
| 1828 |  |  [Comparing Biases and the Impact of Multilingual Training across Multiple Languages](https://doi.org/10.18653/v1/2023.emnlp-main.634) |  | 0 | Studies in bias and fairness in natural language processing have primarily examined social biases within a single language and/or across few attributes (e.g. gender, race). However, biases can manifest differently across various languages for individual attributes. As a result, it is critical to... | Dan Roth, Jie Ma, Ling Liu, Miguel Ballesteros, Neha Anna John, Sharon Levy, Vittorio Castelli, Yogarshi Vyas, Yoshinari Fujinuma |  |
| 1829 |  |  [HutCRS: Hierarchical User-Interest Tracking for Conversational Recommender System](https://doi.org/10.18653/v1/2023.emnlp-main.635) |  | 0 | Conversational Recommender System (CRS) aims to explicitly acquire user preferences towards items and attributes through natural language conversations. However, existing CRS methods ask users to provide explicit answers (yes/no) for each attribute they require, regardless of users’ knowledge or... | Jinghui Qin, Liang Lin, Mingjie Qian, Yongsen Zheng |  |
| 1830 |  |  [Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.636) |  | 0 | The tasks of out-of-domain (OOD) intent discovery and generalized intent discovery (GID) aim to extend a closed intent classifier to open-world intent sets, which is crucial to task-oriented dialogue (TOD) systems. Previous methods address them by fine-tuning discriminative models. Recently,... | Guanting Dong, Jingang Wang, Keqing He, Pei Wang, Weiran Xu, Xiaoshuai Song, Xunliang Cai, Yunsen Xian, Yutao Mou |  |
| 1831 |  |  [The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining](https://doi.org/10.18653/v1/2023.emnlp-main.637) |  | 0 | We analyze the masked language modeling pretraining objective function from the perspective of the Distributional Hypothesis. We investigate whether the better sample efficiency and the better generalization capability of models pretrained with masked language modeling can be attributed to the... | Dani Yogatama, TingRui Chiang |  |
| 1832 |  |  [Simple and Effective Input Reformulations for Translation](https://doi.org/10.18653/v1/2023.emnlp-main.638) |  | 0 | Foundation language models learn from their finetuning input context in different ways. In this paper, we reformulate inputs during finetuning for challenging translation tasks, leveraging model strengths from pretraining in novel ways to improve downstream performance. These reformulations are... | Brian Yu, Hansen Lillemark, Kurt Keutzer |  |
| 1833 |  |  [Pointwise Mutual Information Based Metric and Decoding Strategy for Faithful Generation in Document Grounded Dialogs](https://doi.org/10.18653/v1/2023.emnlp-main.639) |  | 0 | A major concern in using deep learning based generative models for document-grounded dialogs is the potential generation of responses that are not faithful to the underlying document. Existing automated metrics used for evaluating the faithfulness of response with respect to the grounding document... | Dinesh Raghu, Luis A. Lastras, Sachindra Joshi, Vineet Kumar, Yatin Nandwani |  |
| 1834 |  |  [The ACL OCL Corpus: Advancing Open Science in Computational Linguistics](https://doi.org/10.18653/v1/2023.emnlp-main.640) |  | 0 | We present ACL OCL, a scholarly corpus derived from the ACL Anthology to assist Open scientific research in the Computational Linguistics domain. Integrating and enhancing the previous versions of the ACL Anthology, the ACL OCL contributes metadata, PDF files, citation graphs and additional... | Benjamin Aw, MinYen Kan, Niranjana Unnithan, Shaurya Rohatgi, Yanxia Qin |  |
| 1835 |  |  [Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.641) |  | 0 | Numerous studies have demonstrated the ability of neural language models to learn various linguistic properties without direct supervision. This work takes an initial step towards exploring the less researched topic of how neural models discover linguistic properties of words, such as gender, as... | Guillaume Wisniewski, Lina Conti |  |
| 1836 |  |  [Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset](https://doi.org/10.18653/v1/2023.emnlp-main.642) |  | 0 | While recent pre-trained transformer-based models can perform named entity recognition (NER) with great accuracy, their limited range remains an issue when applied to long documents such as whole novels. To alleviate this issue, a solution is to retrieve relevant context at the document level.... | Arthur Amalvy, Richard Dufour, Vincent Labatut |  |
| 1837 |  |  [Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting](https://doi.org/10.18653/v1/2023.emnlp-main.643) |  | 0 | A crucial challenge for generative large language models (LLMs) is diversity: when a user’s prompt is under-specified, models may follow implicit assumptions while generating a response, which may result in homogenization of the responses, as well as certain demographic groups being... | Ahmad Beirami, Alex Beutel, Ben Packer, Hansa Srinivasan, Jilin Chen, Nicholas Blumm, Preethi Lahoti, Qijun Tan, Raghavendra Kotikalapudi, Sahitya Potluri, Xiao Ma |  |
| 1838 |  |  [Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection](https://doi.org/10.18653/v1/2023.emnlp-main.644) |  | 0 | Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises. Although several detectors... | Ben He, Le Sun, Xinlin Peng, Ying Zhou, Yingfei Sun |  |
| 1839 |  |  [Contextual Interaction for Argument Post Quality Assessment](https://doi.org/10.18653/v1/2023.emnlp-main.645) |  | 0 | Recently, there has been an increased emphasis on assessing the quality of natural language arguments. Existing approaches primarily focus on evaluating the quality of individual argument posts. However, they often fall short when it comes to effectively distinguishing arguments that possess a... | Ben He, Le Sun, Xuanang Chen, Yiran Wang |  |
| 1840 |  |  [Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent Classification](https://doi.org/10.18653/v1/2023.emnlp-main.646) |  | 0 | Intent classification (IC) plays an important role in task-oriented dialogue systems. However, IC models often generalize poorly when training without sufficient annotated examples for each user intent. We propose a novel pre-training method for text encoders that uses contrastive learning with... | Elman Mansimov, James Gung, Mujeen Sung, Nikolaos Pappas, Raphael Shu, Salvatore Romeo, Vittorio Castelli, Yi Zhang |  |
| 1841 |  |  [Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations](https://doi.org/10.18653/v1/2023.emnlp-main.647) |  | 0 | The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate... | Hangxiao Zhu, Ming Yin, Zhuoran Lu, Zhuoyan Li |  |
| 1842 |  |  [GazeVQA: A Video Question Answering Dataset for Multiview Eye-Gaze Task-Oriented Collaborations](https://doi.org/10.18653/v1/2023.emnlp-main.648) |  | 0 | The usage of exocentric and egocentric videos in Video Question Answering (VQA) is a new endeavor in human-robot interaction and collaboration studies. Particularly for egocentric videos, one may leverage eye-gaze information to understand human intentions during the task. In this paper, we build a... | Chenan Song, Difei Gao, Joo Lim, Joya Chen, Mike Zheng Shou, Muhammet Furkan Ilaslan, Qianli Xu, Weixian Lei |  |
| 1843 |  |  [People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection](https://doi.org/10.18653/v1/2023.emnlp-main.649) |  | 0 | NLP models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. Therefore, it is imperative that these models are robust to spurious features. Past work has attempted to tackle such spurious features using training data... | Claudia Wagner, Dennis Assenmacher, Indira Sen, Isabelle Augenstein, Mattia Samory, Wil M. P. van der Aalst |  |
| 1844 |  |  [Unraveling Feature Extraction Mechanisms in Neural Networks](https://doi.org/10.18653/v1/2023.emnlp-main.650) |  | 0 | The underlying mechanism of neural networks in capturing precise knowledge has been the subject of consistent research efforts. In this work, we propose a theoretical approach based on Neural Tangent Kernels (NTKs) to investigate such mechanisms. Specifically, considering the infinite network... | Jiaxi Li, Wei Lu, Xiaobing Sun |  |
| 1845 |  |  [CAPSTONE: Curriculum Sampling for Dense Retrieval with Document Expansion](https://doi.org/10.18653/v1/2023.emnlp-main.651) |  | 0 | The dual-encoder has become the de facto architecture for dense retrieval. Typically, it computes the latent representations of the query and document independently, thus failing to fully capture the interactions between the query and document. To alleviate this, recent research has focused on... | ALong Jin, Anlei Dong, Hang Zhang, Jian Jiao, Nan Duan, SiuMing Yiu, Xingwei He, Yeyun Gong |  |
| 1846 |  |  [Balance Act: Mitigating Hubness in Cross-Modal Retrieval with Query and Gallery Banks](https://doi.org/10.18653/v1/2023.emnlp-main.652) |  | 0 | In this work, we present a post-processing solution to address the hubness problem in cross-modal retrieval, a phenomenon where a small number of gallery data points are frequently retrieved, resulting in a decline in retrieval performance. We first theoretically demonstrate the necessity of... | Bo Xue, Xiangru Jian, Yimu Wang |  |
| 1847 |  |  [E-CORE: Emotion Correlation Enhanced Empathetic Dialogue Generation](https://doi.org/10.18653/v1/2023.emnlp-main.653) |  | 0 | Achieving empathy is a crucial step toward humanized dialogue systems. Current approaches for empathetic dialogue generation mainly perceive an emotional label to generate an empathetic response conditioned on it, which simply treat emotions independently, but ignore the intrinsic emotion... | Fengyi Fu, Lei Zhang, Quan Wang, Zhendong Mao |  |
| 1848 |  |  [What do Deck Chairs and Sun Hats Have in Common? Uncovering Shared Properties in Large Concept Vocabularies](https://doi.org/10.18653/v1/2023.emnlp-main.654) |  | 0 | Concepts play a central role in many applications. This includes settings where concepts have to be modelled in the absence of sentence context. Previous work has therefore focused on distilling decontextualised concept embeddings from language models. But concepts can be modelled from different... | Amit Gajbhiye, Luis Espinosa Anke, Na Li, Steven Schockaert, Usashi Chatterjee, Zied Bouraoui |  |
| 1849 |  |  [ALDi: Quantifying the Arabic Level of Dialectness of Text](https://doi.org/10.18653/v1/2023.emnlp-main.655) |  | 0 | Transcribed speech and user-generated text in Arabic typically contain a mixture of Modern Standard Arabic (MSA), the standardized language taught in schools, and Dialectal Arabic (DA), used in daily communications. To handle this variation, previous work in Arabic NLP has focused on Dialect... | Amr Keleg, Sharon Goldwater, Walid Magdy |  |
| 1850 |  |  [3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding](https://doi.org/10.18653/v1/2023.emnlp-main.656) |  | 0 | 3D visual grounding aims to localize the target object in a 3D point cloud by a free-form language description. Typically, the sentences describing the target object tend to provide information about its relative relation between other objects and its position within the whole scene. In this work,... | Aoxiong Yin, Haifeng Huang, Linjun Li, Xize Cheng, Yang Zhao, Yichen Zhu, Zehan Wang, Zhou Zhao |  |
| 1851 |  |  [Goal-Driven Explainable Clustering via Language Descriptions](https://doi.org/10.18653/v1/2023.emnlp-main.657) |  | 0 | Unsupervised clustering is widely used to explore large corpora, but existing formulations neither consider the users’ goals nor explain clusters’ meanings. We propose a new task formulation, “Goal-Driven Clustering with Explanations” (GoalEx), which represents both the goal and the explanations as... | Jingbo Shang, Ruiqi Zhong, Zihan Wang |  |
| 1852 |  |  [Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.658) |  | 0 | Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the... | Arianna Bisazza, Jirui Qi, Raquel Fernández |  |
| 1853 |  |  [Learning from Mistakes via Cooperative Study Assistant for Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.659) |  | 0 | Large language models (LLMs) have demonstrated their potential to refine their generation based on their own feedback. However, the feedback from LLM itself is often inaccurate, thereby limiting its benefits. In this paper, we propose Study Assistant for Large LAnguage Model (SALAM), a novel... | Danqing Wang, Lei Li |  |
| 1854 |  |  [Bridging the Digital Divide: Performance Variation across Socio-Economic Factors in Vision-Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.660) |  | 0 | Despite the impressive performance of current AI models reported across various tasks, performance reports often do not include evaluations of how these models perform on the specific groups that will be impacted by these technologies. Among the minority groups under-represented in AI, data from... | Joan Nwatu, Oana Ignat, Rada Mihalcea |  |
| 1855 |  |  [Conceptor-Aided Debiasing of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.661) |  | 0 | Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use \*conceptors\*–a soft projection method–to identify and remove the... | João Sedoc, Lyle H. Ungar, Yifei Li |  |
| 1856 |  |  [AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing Evaluation Suite](https://doi.org/10.18653/v1/2023.emnlp-main.662) |  | 0 | We present the Granular AMR Parsing Evaluation Suite (GrAPES), a challenge set for Abstract Meaning Representation (AMR) parsing with accompanying evaluation metrics. AMR parsers now obtain high scores on the standard AMR evaluation metric Smatch, close to or even above reported inter-annotator... | Jonas Groschwitz, Lucia Donatelli, Meaghan Fowlie, Shay B. Cohen |  |
| 1857 |  |  [Rethinking and Improving Multi-task Learning for End-to-end Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-main.663) |  | 0 | Significant improvements in end-to-end speech translation (ST) have been achieved through the application of multi-task learning. However, the extent to which auxiliary tasks are highly consistent with the ST task, and how much this approach truly helps, have not been thoroughly studied. In this... | Bei Li, Chen Xu, Chunliang Zhang, Hao Chen, Jingbo Zhu, Tong Xiao, Yuhao Zhang |  |
| 1858 |  |  [AD-NLP: A Benchmark for Anomaly Detection in Natural Language Processing](https://doi.org/10.18653/v1/2023.emnlp-main.664) |  | 0 | Deep learning models have reignited the interest in Anomaly Detection research in recent years. Methods for Anomaly Detection in text have shown strong empirical results on ad-hoc anomaly setups that are usually made by downsampling some classes of a labeled dataset. This can lead to... | Andrei Manolache, Marius Popescu, Matei Bejan |  |
| 1859 |  |  [Enhancing the Ranking Context of Dense Retrieval through Reciprocal Nearest Neighbors](https://doi.org/10.18653/v1/2023.emnlp-main.665) |  | 0 | Sparse annotation poses persistent challenges to training dense retrieval models; for example, it distorts the training signal when unlabeled relevant documents are used spuriously as negatives in contrastive learning. To alleviate this problem, we introduce evidence-based label smoothing, a novel,... | Carsten Eickhoff, George Zerveas, Navid Rekabsaz |  |
| 1860 |  |  [Cross-Lingual Cross-Target Stance Detection with Dual Knowledge Distillation Framework](https://doi.org/10.18653/v1/2023.emnlp-main.666) |  | 0 | Stance detection aims to identify the user’s attitude toward specific targets from text, which is an important research area in text mining and benefits a variety of application domains. Existing studies on stance detection were conducted mainly in English. Due to the low-resource problem in most... | Hanxuan Yang, Ruike Zhang, Wenji Mao |  |
| 1861 |  |  [PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs](https://doi.org/10.18653/v1/2023.emnlp-main.667) |  | 0 | Research interest in task-oriented dialogs has increased as systems such as Google Assistant, Alexa and Siri have become ubiquitous in everyday life. However, the impact of academic research in this area has been limited by the lack of datasets that realistically capture the wide array of user pain... | Aditya Gupta, Anna Trukhina, Chuan He, David Greene, Faiz Surani, HyunJeong Choe, Max Chang, Motoki Sano, Pararth Shah, Rahul Goel, Rattima Nitisaroj, Rushin Shah, Shachi Paul, Siddharth Vashishtha, Waleed Ammar, Zhou Yu |  |
| 1862 |  |  [An Iteratively Parallel Generation Method with the Pre-Filling Strategy for Document-level Event Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.668) |  | 0 | In document-level event extraction (DEE) tasks, a document typically contains many event records with multiple event roles. Therefore, accurately extracting all event records is a big challenge since the number of event records is not given. Previous works present the entity-based directed acyclic... | Guanhua Huang, Jiaze Chen, Runxin Xu, Weinan E, Ying Zeng, Zhouwang Yang |  |
| 1863 |  |  [CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations](https://doi.org/10.18653/v1/2023.emnlp-main.669) |  | 0 | Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM... | Diyi Yang, Myra Cheng, Tiziano Piccardi |  |
| 1864 |  |  [Reduce Human Labor On Evaluating Conversational Information Retrieval System: A Human-Machine Collaboration Approach](https://doi.org/10.18653/v1/2023.emnlp-main.670) |  | 0 | Evaluating conversational information retrieval (CIR) systems is a challenging task that requires a significant amount of human labor for annotation. It is imperative to invest significant effort into researching more labor-effective methods for evaluating CIR systems. To touch upon this challenge,... | Chen Huang, Jiancheng Lv, Peixin Qin, Wenqiang Lei |  |
| 1865 |  |  [BERTie Bott's Every Flavor Labels: A Tasty Introduction to Semantic Role Labeling for Galician](https://doi.org/10.18653/v1/2023.emnlp-main.671) |  | 0 | In this paper, we leverage existing corpora, WordNet, and dependency parsing to build the first Galician dataset for training semantic role labeling systems in an effort to expand available NLP resources. Additionally, we introduce verb indexing, a new pre-processing method, which helps increase... | Meriem Beloucif, Micaella Bruton |  |
| 1866 |  |  [Program Translation via Code Distillation](https://doi.org/10.18653/v1/2023.emnlp-main.672) |  | 0 | Software version migration and program translation are an important and costly part of the lifecycle of large codebases. Traditional machine translation relies on parallel corpora for supervised translation, which is not feasible for program translation due to a dearth of aligned data. Recent... | Bin Gu, Colin B. Clement, Maoquan Wang, Mengnan Qi, Neel Sundaresan, Yongqiang Yao, Yufan Huang |  |
| 1867 |  |  [FaMeSumm: Investigating and Improving Faithfulness of Medical Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.673) |  | 0 | Summaries of medical text shall be faithful by being consistent and factual with source inputs, which is an important but understudied topic for safety and efficiency in healthcare. In this paper, we investigate and improve faithfulness in summarization on a broad range of medical summarization... | Nan Zhang, Prasenjit Mitra, Rui Zhang, Wu Guo, Yusen Zhang |  |
| 1868 |  |  [Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning](https://doi.org/10.18653/v1/2023.emnlp-main.674) |  | 0 | Despite their impressive performance, large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly. To address this issue, grammar-constrained decoding (GCD) can be used to control the generation of... | Martin Josifoski, Maxime Peyrard, Robert West, Saibo Geng |  |
| 1869 |  |  [Systematic word meta-sense extension](https://doi.org/10.18653/v1/2023.emnlp-main.675) |  | 0 | The meaning of polysemous words often varies in a highly productive yet predictable way. Generalizing the regularity between conventional senses to derive novel word meaning is crucial for automated processing of non-literal language uses such as figurative expressions. We introduce a novel task... | Lei Yu |  |
| 1870 |  |  [Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory](https://doi.org/10.18653/v1/2023.emnlp-main.676) |  | 0 | We address a fundamental challenge in Natural Language Generation (NLG) model evaluation—the design and evaluation of evaluation metrics. Recognizing the limitations of existing automatic metrics and noises from how current human evaluation was conducted, we propose MetricEval, a framework informed... | Q. Vera Liao, Susu Zhang, Vivian Lai, Ziang Xiao |  |
| 1871 |  |  [Revisiting the Knowledge Injection Frameworks](https://doi.org/10.18653/v1/2023.emnlp-main.677) |  | 0 | In recent years, large language models (LLMs), such as GPTs, have attained great impact worldwide. However, how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved. Indeed, there have emerged a few works on this line... | Haobo Wang, Junbo Zhao, Peng Fu, Weikang Qiu, Yiming Zhang |  |
| 1872 |  |  [We Are What We Repeatedly Do: Inducing and Deploying Habitual Schemas in Persona-Based Responses](https://doi.org/10.18653/v1/2023.emnlp-main.678) |  | 0 | Many practical applications of dialogue technology require the generation of responses according to a particular developer-specified persona. While a variety of personas can be elicited from recent large language models, the opaqueness and unpredictability of these models make it desirable to be... | Benjamin Kane, Lenhart K. Schubert |  |
| 1873 |  |  [Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model](https://doi.org/10.18653/v1/2023.emnlp-main.679) |  | 0 | Despite tremendous improvements in natural language generation, summarization models still suffer from the unfaithfulness issue. Previous work evaluates faithfulness either using models trained on the other tasks or in-domain synthetic data, or prompting a large model such as ChatGPT. This paper... | Kenny Q. Zhu, Qi Jia, Siyu Ren, Yizhu Liu |  |
| 1874 |  |  [TaskWeb: Selecting Better Source Tasks for Multi-task NLP](https://doi.org/10.18653/v1/2023.emnlp-main.680) |  | 0 | Recent work in NLP has shown promising results in training models on large amounts of tasks to achieve better generalization. However, it is not well-understood how tasks are related, and how helpful training tasks can be chosen for a new task. In this work, we investigate whether knowing task... | Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, Joongwon Kim |  |
| 1875 |  |  [Improving Bias Mitigation through Bias Experts in Natural Language Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.681) |  | 0 | Biases in the dataset often enable the model to achieve high performance on in-distribution data, while poorly performing on out-of-distribution data. To mitigate the detrimental effect of the bias on the networks, previous works have proposed debiasing methods that down-weight the biased examples... | Eojin Jeon, Juhyeong Park, Mingyu Lee, SangKeun Lee, WingLam Mok, Yeachan Kim |  |
| 1876 |  |  [Semi-supervised multimodal coreference resolution in image narrations](https://doi.org/10.18653/v1/2023.emnlp-main.682) |  | 0 | In this paper, we study multimodal coreference resolution, specifically where a longer descriptive text, i.e., a narration is paired with an image. This poses significant challenges due to fine-grained image-text alignment, inherent ambiguity present in narrative language, and unavailability of... | Arushi Goel, Basura Fernando, Frank Keller, Hakan Bilen |  |
| 1877 |  |  [A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.683) |  | 0 | Various types of social biases have been reported with pretrained Masked Language Models (MLMs) in prior work. However, multiple underlying factors are associated with an MLM such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled,... | Danushka Bollegala, José CamachoCollados, Yi Zhou |  |
| 1878 |  |  [Argument-based Detection and Classification of Fallacies in Political Debates](https://doi.org/10.18653/v1/2023.emnlp-main.684) |  | 0 | Fallacies are arguments that employ faulty reasoning. Given their persuasive and seemingly valid nature, fallacious arguments are often used in political debates. Employing these misleading arguments in politics can have detrimental consequences for society, since they can lead to inaccurate... | Elena Cabrio, Mariana Espinoza, Pierpaolo Goffredo, Serena Villata |  |
| 1879 |  |  [Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation](https://doi.org/10.18653/v1/2023.emnlp-main.685) |  | 0 | The field of text-to-image (T2I) generation has garnered significant attention both within the research community and among everyday users. Despite the advancements of T2I models, a common issue encountered by users is the need for repetitive editing of input prompts in order to receive a... | Miguel P. Eckstein, TsuJui Fu, Wanrong Zhu, William Wang, Xin Wang, Xinyi Wang, Yujie Lu |  |
| 1880 |  |  [SpEL: Structured Prediction for Entity Linking](https://doi.org/10.18653/v1/2023.emnlp-main.686) |  | 0 | Entity linking is a prominent thread of research focused on structured data creation by linking spans of text to an ontology or knowledge source. We revisit the use of structured prediction for entity linking which classifies each individual input token as an entity, and aggregates the token... | Anoop Sarkar, Hassan Shavarani |  |
| 1881 |  |  [Architectural Sweet Spots for Modeling Human Label Variation by the Example of Argument Quality: It's Best to Relate Perspectives!](https://doi.org/10.18653/v1/2023.emnlp-main.687) |  | 0 | Many annotation tasks in natural language processing are highly subjective in that there can be different valid and justified perspectives on what is a proper label for a given example. This also applies to the judgment of argument quality, where the assignment of a single ground truth is often... | Julia Romberg, Matthias Orlikowski, Philipp Cimiano, Philipp Heinisch |  |
| 1882 |  |  [Explicit Planning Helps Language Models in Logical Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.688) |  | 0 | Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose LEAP, a novel system that uses language models to perform multi-step logical reasoning and incorporates explicit planning into the inference procedure. Explicit... | Hongyu Zhao, Hongyuan Mei, Kangrui Wang, Mo Yu |  |
| 1883 |  |  [clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents](https://doi.org/10.18653/v1/2023.emnlp-main.689) |  | 0 | Recent work has proposed a methodology for the systematic evaluation of “Situated Language Understanding Agents” — agents that operate in rich linguistic and non-linguistic contexts — through testing them in carefully constructed interactive settings. Other recent work has argued that Large... | Brielen Madureira, David Schlangen, Jana Götze, Kranti Chalamalasetti, Philipp Sadler, Sherzod Hakimov |  |
| 1884 |  |  [Explaining with Contrastive Phrasal Highlighting: A Case Study in Assisting Humans to Detect Translation Differences](https://doi.org/10.18653/v1/2023.emnlp-main.690) |  | 0 | Explainable NLP techniques primarily explain by answering “Which tokens in the input are responsible for this prediction?”. We argue that for NLP models that make predictions by comparing two input texts, it is more useful to explain by answering “What differences between the two inputs explain... | Eleftheria Briakou, Marine Carpuat, Navita Goyal |  |
| 1885 |  |  [Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in Foundation Models](https://doi.org/10.18653/v1/2023.emnlp-main.691) |  | 0 | In this work, we assess the ability of foundation models to recall encyclopedic knowledge across a wide range of linguistic contexts. To support this, we: 1) produce a 20-language dataset that contains 303k factual associations paired with counterfactuals, 2) evaluate 5 models in a multilingual... | Daniel Furman, Shreshta Bhat, Tim Schott |  |
| 1886 |  |  [Anchoring Fine-tuning of Sentence Transformer with Semantic Label Information for Efficient Truly Few-shot Classification](https://doi.org/10.18653/v1/2023.emnlp-main.692) |  | 0 | Few-shot classification is a powerful technique, but training requires substantial computing power and data. We propose an efficient method with small model sizes and less training data with only 2-8 training instances per class. Our proposed method, AncSetFit, targets low data scenarios by... | Amalie Brogaard Pauli, Ira Assent, Leon Derczynski |  |
| 1887 |  |  [UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers](https://doi.org/10.18653/v1/2023.emnlp-main.693) |  | 0 | Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large... | Avirup Sil, Christopher Potts, Jon SaadFalcon, Keshav Santhanam, Martin Franz, Md. Arafat Sultan, Omar Khattab, Radu Florian, Salim Roukos |  |
| 1888 |  |  [TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings](https://doi.org/10.18653/v1/2023.emnlp-main.694) |  | 0 | Stance detection is important for understanding different attitudes and beliefs on the Internet. However, given that a passage’s stance toward a given topic is often highly dependent on that topic, building a stance detection model that generalizes to unseen topics is difficult. In this work, we... | Hans W. A. Hanley, Zakir Durumeric |  |
| 1889 |  |  [Data Similarity is Not Enough to Explain Language Model Performance](https://doi.org/10.18653/v1/2023.emnlp-main.695) |  | 0 | Large language models achieve high performance on many but not all downstream tasks. The interaction between pretraining data and task data is commonly assumed to determine this variance: a task with data that is more similar to a model’s pretraining data is assumed to be easier for that model. We... | David Mimno, Emily Reif, Gregory Yauney |  |
| 1890 |  |  [Zero-shot Sharpness-Aware Quantization for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.696) |  | 0 | Quantization is a promising approach for reducing memory overhead and accelerating inference, especially in large pre-trained language model (PLM) scenarios. While having no access to original training data due to security and privacy concerns has emerged the demand for zero-shot quantization. Most... | Bo Du, Dacheng Tao, Juhua Liu, Li Shen, Liang Ding, Miaoxi Zhu, Qihuang Zhong |  |
| 1891 |  |  [Deciphering Stereotypes in Pre-Trained Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.697) |  | 0 | Warning: This paper contains content that is stereotypical and may be upsetting. This paper addresses the issue of demographic stereotypes present in Transformer-based pre-trained language models (PLMs) and aims to deepen our understanding of how these biases are encoded in these models. To... | Alan Sun, Andrew Koulogeorge, Brian Wang, Diyi Yang, Goutham Veeramachaneni, Henry Scheible, Lili Wang, Pratim Chowdhary, Soroush Vosoughi, Weicheng Ma |  |
| 1892 |  |  [An "Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives"](https://doi.org/10.18653/v1/2023.emnlp-main.698) |  | 0 | Mental health conversational agents (a.k.a. chatbots) are widely studied for their potential to offer accessible support to those experiencing mental health challenges. Previous surveys on the topic primarily consider papers published in either computer science or medicine, leading to a divide in... | João Sedoc, Lyle H. Ungar, Sharath Chandra Guntuku, Sunny Rai, Young Min Cho |  |
| 1893 |  |  [Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark](https://doi.org/10.18653/v1/2023.emnlp-main.699) |  | 0 | Large language models (LLMs) have been shown to perform well at a variety of syntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well LLMs understand... | Chang Shu, David Jurgens, Jiaxin Pei, Minje Choi, Sagar Kumar |  |
| 1894 |  |  [Interventional Rationalization](https://doi.org/10.18653/v1/2023.emnlp-main.700) |  | 0 | Selective rationalizations improve the explainability of neural networks by selecting a subsequence of the input (i.e., rationales) to explain the prediction results. Although existing methods have achieved promising results, they still suffer from adopting the spurious correlations in data (aka.,... | Li Wang, Linan Yue, Qi Liu, Yanqing An, Yichao Du, Zhenya Huang |  |
| 1895 |  |  [Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting](https://doi.org/10.18653/v1/2023.emnlp-main.701) |  | 0 | Most existing stylistic text rewriting methods and evaluation metrics operate on a sentence level, but ignoring the broader context of the text can lead to preferring generic, ambiguous, and incoherent rewrites. In this paper, we investigate integrating the preceding textual context into both the... | Akhila Yerukola, Elizabeth Clark, Maarten Sap, Xuhui Zhou |  |
| 1896 |  |  [Axiomatic Preference Modeling for Longform Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.702) |  | 0 | The remarkable abilities of large language models (LLMs) like ChatGPT and GPT-4 partially stem from the post-training processes involving human preferences encoded within a reward model as part of a Reinforcement Learning from Human Feedback (RLHF) regimen. These reward models (RMs) often lack... | Ahmed Awadallah, Corby Rosset, Guoqing Zheng, Paul N. Bennett, Victor Dibia |  |
| 1897 |  |  [Countering Misinformation via Emotional Response Generation](https://doi.org/10.18653/v1/2023.emnlp-main.703) |  | 0 | The proliferation of misinformation on social media platforms (SMPs) poses a significant danger to public health, social cohesion and ultimately democracy. Previous research has shown how social correction can be an effective way to curb misinformation, by engaging directly in a constructive... | Daniel Russo, Jacopo Staiano, Marco Guerini, Shane P. KaszefskiYaschuk |  |
| 1898 |  |  [Seq2seq is All You Need for Coreference Resolution](https://doi.org/10.18653/v1/2023.emnlp-main.704) |  | 0 | Existing works on coreference resolution suggest that task-specific models are necessary to achieve state-of-the-art performance. In this work, we present compelling evidence that such models are not necessary. We finetune a pretrained seq2seq transformer to map an input document to a tagged... | Karl Stratos, Sam Wiseman, Wenzheng Zhang |  |
| 1899 |  |  [Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection](https://doi.org/10.18653/v1/2023.emnlp-main.705) |  | 0 | When translating words referring to the speaker, speech translation (ST) systems should not resort to default masculine generics nor rely on potentially misleading vocal traits. Rather, they should assign gender according to the speakers’ preference. The existing solutions to do so, though... | Dennis Fucci, Luisa Bentivogli, Marco Gaido, Matteo Negri, Mauro Cettolo, Sara Papi |  |
| 1900 |  |  [StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.706) |  | 0 | Analogy-making between narratives is crucial for human reasoning. In this paper, we evaluate the ability to identify and generate analogies by constructing a first-of-its-kind large-scale story-level analogy corpus, StoryAnalogy, which contains 24K story pairs from diverse domains with human... | Cheng Jiayang, Chunkit Chan, Dongyu Ru, Hongming Zhang, Lin Qiu, Qipeng Guo, Tianqing Fang, Tsz Ho Chan, Weiqi Wang, Yangqiu Song, Yue Zhang, Zheng Zhang |  |
| 1901 |  |  [Beyond Detection: A Defend-and-Summarize Strategy for Robust and Interpretable Rumor Analysis on Social Media](https://doi.org/10.18653/v1/2023.emnlp-main.707) |  | 0 | As the impact of social media gradually escalates, people are more likely to be exposed to indistinguishable fake news. Therefore, numerous studies have attempted to detect rumors on social media by analyzing the textual content and propagation paths. However, fewer works on rumor detection tasks... | HongHan Shuai, YiSyuan Chen, YiTing Chang, YunZhu Song |  |
| 1902 |  |  [Crystal: Introspective Reasoners Reinforced with Self-Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.708) |  | 0 | Extensive work has shown that the performance and interpretability of commonsense reasoning can be improved via knowledge-augmented reasoning methods, where the knowledge that underpins the reasoning process is explicitly verbalized and utilized. However, existing implementations, including... | Asli Celikyilmaz, Hannaneh Hajishirzi, Jiacheng Liu, Ramakanth Pasunuru, Yejin Choi |  |
| 1903 |  |  [DiffS2UT: A Semantic Preserving Diffusion Model for Textless Direct Speech-to-Speech Translation](https://doi.org/10.18653/v1/2023.emnlp-main.709) |  | 0 | While Diffusion Generative Models have achieved great success on image generation tasks, how to efficiently and effectively incorporate them into speech generation especially translation tasks remains a non-trivial problem. Specifically, due to the low information density of speech data, the... | Linli Xu, Xinyuan Zhou, Yongxin Zhu, Zhongyi Ye, Zhujin Gao |  |
| 1904 |  |  [BioFEG: Generate Latent Features for Biomedical Entity Linking](https://doi.org/10.18653/v1/2023.emnlp-main.710) |  | 0 | Biomedical entity linking is an essential task in biomedical text processing, which aims to map entity mentions in biomedical text, such as clinical notes, to standard terms in a given knowledge base. However, this task is challenging due to the rarity of many biomedical entities in real-world... | Baohang Zhou, Kehui Song, Wensheng Zhang, Xiangrui Cai, Xiaojie Yuan, Xuhui Sui, Ying Zhang |  |
| 1905 |  |  [TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.711) |  | 0 | Automated theorem proving (ATP) has become an appealing domain for exploring the reasoning ability of the recent successful generative language models. However, current ATP benchmarks are mainly focus on symbolic inference, but rarely involve the understanding of complex number combination... | Chuanyang Zheng, Haiming Wang, Jianhao Shen, Jing Xiong, Lin Li, Ming Zhang, Qingxing Cao, Qun Liu, Xiaodan Liang, Ye Yuan, Yichun Yin, Yinya Huang, Zhengying Liu, Zhijiang Guo |  |
| 1906 |  |  [Physician Detection of Clinical Harm in Machine Translation: Quality Estimation Aids in Reliance and Backtranslation Identifies Critical Errors](https://doi.org/10.18653/v1/2023.emnlp-main.712) |  | 0 | A major challenge in the practical use of Machine Translation (MT) is that users lack information on translation quality to make informed decisions about how to rely on outputs. Progress in quality estimation research provides techniques to automatically assess MT quality, but these techniques have... | Elaine C. Khoong, Ge Gao, Marine Carpuat, Nikita Mehandru, Niloufar Salehi, Sweta Agrawal, Yimin Xiao |  |
| 1907 |  |  [Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive](https://doi.org/10.18653/v1/2023.emnlp-main.713) |  | 0 | Offensive speech detection is a key component of content moderation. However, what is offensive can be highly subjective. This paper investigates how machine and human moderators disagree on what is offensive when it comes to real-world social web political discourse. We show that (1) there is... | Ashiqur R. KhudaBukhsh, Christopher Homan, Marcos Zampieri, Sujan Dutta, Tharindu Cyril Weerasooriya, Tharindu Ranasinghe |  |
| 1908 |  |  [Generating Summaries with Controllable Readability Levels](https://doi.org/10.18653/v1/2023.emnlp-main.714) |  | 0 | Readability refers to how easily a reader can understand a written text. Several factors affect the readability level, such as the complexity of the text, its subject matter, and the reader’s background knowledge. Generating summaries based on different readability levels is critical for enabling... | Leonardo F. R. Ribeiro, Markus Dreyer, Mohit Bansal |  |
| 1909 |  |  [mAggretriever: A Simple yet Effective Approach to Zero-Shot Multilingual Dense Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.715) |  | 0 | Multilingual information retrieval (MLIR) is a crucial yet challenging task due to the need for human annotations in multiple languages, making training data creation labor-intensive. In this paper, we introduce mAggretriever, which effectively leverages semantic and lexical features from... | Amin Ahmad, Jimmy Lin, ShengChieh Lin |  |
| 1910 |  |  [CodeFusion: A Pre-trained Diffusion Model for Code Generation](https://doi.org/10.18653/v1/2023.emnlp-main.716) |  | 0 | Imagine a developer who can only change their last line of code—how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens... | Carina Negreanu, Gust Verbruggen, José Cambronero, Mukul Singh, Sumit Gulwani, Vu Le |  |
| 1911 |  |  [CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs](https://doi.org/10.18653/v1/2023.emnlp-main.717) |  | 0 | Instruction-based multitasking has played a critical role in the success of large language models (LLMs) in multi-turn dialog applications. While publicly available LLMs have shown promising performance, when exposed to complex instructions with multiple constraints, they lag against... | Devamanyu Hazarika, Dilek HakkaniTur, Mahdi Namazifar, Seokhwan Kim, Shikib Mehri, Taha Aksu, Yang Liu |  |
| 1912 |  |  [VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights](https://doi.org/10.18653/v1/2023.emnlp-main.718) |  | 0 | Recognizing vulnerability is crucial for understanding and implementing targeted support to empower individuals in need. This is especially important at the European Court of Human Rights (ECtHR), where the court adapts Convention standards to meet actual individual needs and thus to ensure... | Corina Heri, Leon Staufer, Matthias Grabmair, Oana Ichim, Shanshan Xu, T. Y. S. S. Santosh |  |
| 1913 |  |  [ACQUIRED: A Dataset for Answering Counterfactual Questions In Real-Life Videos](https://doi.org/10.18653/v1/2023.emnlp-main.719) |  | 0 | Multimodal counterfactual reasoning is a vital yet challenging ability for AI systems. It involves predicting the outcomes of hypothetical circumstances based on vision and language inputs, which enables AI models to learn from failures and explore hypothetical scenarios. Despite its importance,... | Marjorie Freedman, Nanyun Peng, Nischal Reddy Chandra, Qingyuan Hu, Ralph M. Weischedel, TeLin Wu, Yu Hou, ZiYi Dou |  |
| 1914 |  |  [From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base](https://doi.org/10.18653/v1/2023.emnlp-main.720) |  | 0 | Parsing questions into executable logical forms has showed impressive results for knowledge-base question answering (KBQA). However, complex KBQA is a more challenging task that requires to perform complex multi-step reasoning. Recently, a new semantic parser called KoPL has been proposed to... | Hanjiang Lai, Jian Yin, Linyin Luo, Wangzhen Guo |  |
| 1915 |  |  [Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model](https://doi.org/10.18653/v1/2023.emnlp-main.721) |  | 0 | While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward... | Colin Raffel, Haikang Deng |  |
| 1916 |  |  [CORE: A Few-Shot Company Relation Classification Dataset for Robust Domain Adaptation](https://doi.org/10.18653/v1/2023.emnlp-main.722) |  | 0 | We introduce CORE, a dataset for few-shot relation classification (RC) focused on company relations and business entities. CORE includes 4,708 instances of 12 relation types with corresponding textual evidence extracted from company Wikipedia pages. Company names and business entities pose a... | Arno De Caigny, Jochen De Weerdt, Kristof Coussement, MarieFrancine Moens, Philipp Borchert |  |
| 1917 |  |  [Models See Hallucinations: Evaluating the Factuality in Video Captioning](https://doi.org/10.18653/v1/2023.emnlp-main.723) |  | 0 | Video captioning aims to describe events in a video with natural language. In recent years, many works have focused on improving captioning models’ performance. However, like other text generation tasks, it risks introducing factual errors not supported by the input video. Factual errors can... | Hui Liu, Xiaojun Wan |  |
| 1918 |  |  [Back Transcription as a Method for Evaluating Robustness of Natural Language Understanding Models to Speech Recognition Errors](https://doi.org/10.18653/v1/2023.emnlp-main.724) |  | 0 | In a spoken dialogue system, an NLU model is preceded by a speech recognition system that can deteriorate the performance of natural language understanding. This paper proposes a method for investigating the impact of speech recognition errors on the performance of natural language understanding... | Marcin Sowanski, Marek Kubis, Pawel Skórzewski, Tomasz Zietkiewicz |  |
| 1919 |  |  [Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models for Learning Conceptual Spaces](https://doi.org/10.18653/v1/2023.emnlp-main.725) |  | 0 | The theory of Conceptual Spaces is an influential cognitive-linguistic framework for representing the meaning of concepts. Conceptual spaces are constructed from a set of quality dimensions, which essentially correspond to primitive perceptual features (e.g. hue or size). These quality dimensions... | Amit Gajbhiye, Steven Schockaert, Usashi Chatterjee |  |
| 1920 |  |  [Can Language Models Understand Physical Concepts?](https://doi.org/10.18653/v1/2023.emnlp-main.726) |  | 0 | Language models (LMs) gradually become general-purpose interfaces in the interactive and embodied world, where the understanding of physical concepts is an essential prerequisite. However, it is unclear whether LMs can understand physical concepts in the human world. To investigate this, we design... | Ce Zheng, Jingjing Xu, Lei Li, Lingpeng Kong, Qi Liu, Qingxiu Dong, Xu Sun |  |
| 1921 |  |  [SPT: Learning to Selectively Insert Prompts for Better Prompt Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.727) |  | 0 | Prompt tuning prepends a soft prompt to the input embeddings or hidden states and only optimizes the prompt to adapt pretrained models (PTMs) to downstream tasks. The previous work manually selects prompt layers which are far from optimal and failed to exploit the potential of prompt tuning. In... | Ming Tan, Wei Zhu |  |
| 1922 |  |  [Once Upon a Time in Graph: Relative-Time Pretraining for Complex Temporal Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.728) |  | 0 | Our physical world is constantly evolving over time, rendering challenges for pre-trained language models to understand and reason over the temporal contexts of texts. Existing work focuses on strengthening the direct association between a piece of text and its time-stamp. However, the... | Lidong Bing, Sen Yang, Wai Lam, Xin Li |  |
| 1923 |  |  [Expository Text Generation: Imitate, Retrieve, Paraphrase](https://doi.org/10.18653/v1/2023.emnlp-main.729) |  | 0 | Expository documents are vital resources for conveying complex information to readers. Despite their usefulness, writing expository text by hand is a challenging process that requires careful content planning, obtaining facts from multiple sources, and the ability to clearly synthesize these facts.... | Jie Huang, Kevin ChenChuan Chang, Nishant Balepur |  |
| 1924 |  |  [Large-scale similarity search with Optimal Transport](https://doi.org/10.18653/v1/2023.emnlp-main.730) |  | 0 | Wasserstein distance is a powerful tool for comparing probability distributions and is widely used for document classification and retrieval tasks in NLP. In particular, it is known as the word mover’s distance (WMD) in the NLP community. WMD exhibits excellent performance for various NLP tasks;... | Cléa Laouar, Makoto Yamada, Yuki Takezawa |  |
| 1925 |  |  [Enhancing Textbooks with Visuals from the Web for Improved Learning](https://doi.org/10.18653/v1/2023.emnlp-main.731) |  | 0 | Textbooks are one of the main mediums for delivering high-quality education to students. In particular, explanatory and illustrative visuals play a key role in retention, comprehension and general transfer of knowledge. However, many textbooks lack these interesting visuals to support student... | Janvijay Singh, Mrinmaya Sachan, Vilém Zouhar |  |
| 1926 |  |  [Continual Event Extraction with Semantic Confusion Rectification](https://doi.org/10.18653/v1/2023.emnlp-main.732) |  | 0 | We study continual event extraction, which aims to extract incessantly emerging event information while avoiding forgetting. We observe that the semantic confusion on event types stems from the annotations of the same text being updated over time. The imbalance between event types even aggravates... | Wei Hu, Xinyi Wang, Zitao Wang |  |
| 1927 |  |  [An Empirical Study of Translation Hypothesis Ensembling with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.733) |  | 0 | Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output. In this paper, we investigate how hypothesis ensembling can improve the quality of the generated text for the specific problem of LLM-based machine translation. We... | André F. T. Martins, António Farinhas, José Guilherme Camargo de Souza |  |
| 1928 |  |  [FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning](https://doi.org/10.18653/v1/2023.emnlp-main.734) |  | 0 | Psychiatrists diagnose mental disorders via the linguistic use of patients. Still, due to data privacy, existing passive mental health monitoring systems use alternative features such as activity, app usage, and location via mobile devices. We propose FedTherapist, a mobile mental health monitoring... | Hyungjun Yoon, Jaemin Shin, Jinho D. Choi, Seungjoo Lee, SungJu Lee, Sungjoon Park, Yunxin Liu |  |
| 1929 |  |  [Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.735) |  | 0 | Recent advances in Large Language Models (LLMs) have stimulated a surge of research aimed at extending their applications to the visual domain. While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images... | Bado Lee, Daehee Kim, Geewook Kim, Haeji Jung, Hodong Lee, Sangdoo Yun, Sanghee Park, Seunghyun Park, Taeho Kil, Yoonsik Kim |  |
| 1930 |  |  [Continual Learning for Multilingual Neural Machine Translation via Dual Importance-based Model Division](https://doi.org/10.18653/v1/2023.emnlp-main.736) |  | 0 | A persistent goal of multilingual neural machine translation (MNMT) is to continually adapt the model to support new language pairs or improve some current language pairs without accessing the previous training data. To achieve this, the existing methods primarily focus on preventing catastrophic... | Degen Huang, Hao Yu, Jinsong Su, Jiuyi Li, Junpeng Liu, Kaiyu Huang |  |
| 1931 |  |  [SimCSE++: Improving Contrastive Learning for Sentence Embeddings from Two Perspectives](https://doi.org/10.18653/v1/2023.emnlp-main.737) |  | 0 | This paper improves contrastive learning for sentence embeddings from two perspectives: handling dropout noise and addressing feature corruption. Specifically, for the first perspective, we identify that the dropout noise from negative pairs affects the model’s performance. Therefore, we propose a... | Jiahao Xu, Lemao Liu, Lihui Chen, Wei Shao |  |
| 1932 |  |  [Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.738) |  | 0 | Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to... | Diyi Yang, Jiaao Chen |  |
| 1933 |  |  [Simplicity Level Estimate (SLE): A Learned Reference-Less Metric for Sentence Simplification](https://doi.org/10.18653/v1/2023.emnlp-main.739) |  | 0 | Automatic evaluation for sentence simplification remains a challenging problem. Most popular evaluation metrics require multiple high-quality references – something not readily available for simplification – which makes it difficult to test performance on unseen domains. Furthermore, most existing... | Claire Gardent, Joël Legrand, Liam Cripwell |  |
| 1934 |  |  [Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration](https://doi.org/10.18653/v1/2023.emnlp-main.740) |  | 0 | Legal Judgment Prediction (LJP) has become an increasingly crucial task in Legal AI, i.e., predicting the judgment of the case in terms of case fact description. Precedents are the previous legal cases with similar facts, which are the basis for the judgment of the subsequent case in national legal... | Changlong Sun, Fei Wu, Kun Kuang, Siying Zhou, Weiming Lu, Xiaozhong Liu, Yating Zhang, Yifei Liu, Yiquan Wu |  |
| 1935 |  |  [FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.741) |  | 0 | Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and... | Hannaneh Hajishirzi, Kalpesh Krishna, Luke Zettlemoyer, Mike Lewis, Mohit Iyyer, Pang Wei Koh, Sewon Min, Wentau Yih, Xinxi Lyu |  |
| 1936 |  |  [Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems](https://doi.org/10.18653/v1/2023.emnlp-main.742) |  | 0 | Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation. We address this deficiency by creating Calc-X, a collection of datasets that demonstrates the appropriate use of a calculator in reasoning... | Marek Kadlcík, Michal Stefánik, Ondrej Sotolár, Vlastimil Martinek |  |
| 1937 |  |  [CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.743) |  | 0 | While Chain-of-Thought prompting is popular in reasoning tasks, its application to Large Language Models (LLMs) in Natural Language Understanding (NLU) is under-explored. Motivated by multi-step reasoning of LLMs, we propose Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU... | Chenwei Zhang, Hoang Nguyen, Philip S. Yu, Tao Zhang, Ye Liu |  |
| 1938 |  |  [When Language Models Fall in Love: Animacy Processing in Transformer Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.744) |  | 0 | Animacy—whether an entity is alive and sentient—is fundamental to cognitive processing, impacting areas such as memory, vision, and language. However, animacy is not always expressed directly in language: in English it often manifests indirectly, in the form of selectional constraints on verbs and... | Michael Hanna, Sandro Pezzelle, Yonatan Belinkov |  |
| 1939 |  |  [Improving Unsupervised Relation Extraction by Augmenting Diverse Sentence Pairs](https://doi.org/10.18653/v1/2023.emnlp-main.745) |  | 0 | Unsupervised relation extraction (URE) aims to extract relations between named entities from raw text without requiring manual annotations or pre-existing knowledge bases. In recent studies of URE, researchers put a notable emphasis on contrastive learning strategies for acquiring relation... | Kang Zhou, Qi Li, Qiao Qiao, Qing Wang, Yuepei Li |  |
| 1940 |  |  [Paraphrase Types for Generation and Detection](https://doi.org/10.18653/v1/2023.emnlp-main.746) |  | 0 | Current approaches in paraphrase generation and detection heavily rely on a single general similarity score, ignoring the intricate linguistic properties of language. This paper introduces two new tasks to address this shortcoming by considering paraphrase types - specific linguistic perturbations... | Bela Gipp, Jan Philip Wahle, Terry Ruas |  |
| 1941 |  |  [Target-to-Source Augmentation for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.747) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) is an important task in sentiment analysis, aiming to extract aspect-level opinions and sentiments from user-generated reviews. The fine-grained nature of ASTE incurs a high annotation cost, while the scarcity of annotated data limits the performance of... | Bin Liang, Meng Li, Ruifeng Xu, Shiwei Chen, Yice Zhang, Yifan Yang |  |
| 1942 |  |  [PAC-tuning: Fine-tuning Pre-trained Language Models with PAC-driven Perturbed Gradient Descent](https://doi.org/10.18653/v1/2023.emnlp-main.748) |  | 0 | Fine-tuning pretrained language models (PLMs) for downstream tasks is a large-scale optimization problem, in which the choice of the training algorithm critically determines how well the trained model can generalize to unseen test data, especially in the context of few-shot learning. To achieve... | Guangliang Liu, Kristen Marie Johnson, Rongrong Wang, Xitong Zhang, Zhiyu Xue |  |
| 1943 |  |  [Emergence of Abstract State Representations in Embodied Sequence Modeling](https://doi.org/10.18653/v1/2023.emnlp-main.749) |  | 0 | Decision making via sequence modeling aims to mimic the success of language models, where actions taken by an embodied agent are modeled as tokens to predict. Despite their promising performance, it remains unclear if embodied sequence modeling leads to the emergence of internal representations... | Ashish V. Thapliyal, Bo Pang, Chen Sun, Ellie Pavlick, Kunal Handa, Tian Yun, Zilai Zeng |  |
| 1944 |  |  [Accelerating Toeplitz Neural Network with Constant-time Inference Complexity](https://doi.org/10.18653/v1/2023.emnlp-main.750) |  | 0 | Toeplitz Neural Networks (TNNs) have exhibited outstanding performance in various sequence modeling tasks. They outperform commonly used Transformer-based models while benefiting from log-linear space-time complexities. On the other hand, State Space Models (SSMs) achieve lower performance than... | Yiran Zhong, Zhen Qin |  |
| 1945 |  |  [Dissecting Recall of Factual Associations in Auto-Regressive Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.751) |  | 0 | Transformer-based language models (LMs) are known to capture factual knowledge in their parameters. While previous work looked into where factual associations are stored, only little is known about how they are retrieved internally during inference. We investigate this question through the lens of... | Amir Globerson, Jasmijn Bastings, Katja Filippova, Mor Geva |  |
| 1946 |  |  [StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.752) |  | 0 | Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data. We propose a theoretically grounded framework called StereoMap to gain insights into their perceptions of how demographic groups have been viewed by society. The framework is... | Jana Diesner, Sullam Jeoung, Yubin Ge |  |
| 1947 |  |  [Select, Prompt, Filter: Distilling Large Language Models for Summarizing Conversations](https://doi.org/10.18653/v1/2023.emnlp-main.753) |  | 0 | Large language models (LLMs) like ChatGPT can be expensive to train, deploy, and use for specific natural language generation tasks such as text summarization and for certain domains. A promising alternative is to fine-tune relatively smaller language models (LMs) on a particular task using... | Marco Turchi, MinhQuang Pham, Sathish Indurthi, Shamil Chollampatt |  |
| 1948 |  |  [Human Raters Cannot Distinguish English Translations from Original English Texts](https://doi.org/10.18653/v1/2023.emnlp-main.754) |  | 0 | The term translationese describes the set of linguistic features unique to translated texts, which appear regardless of translation quality. Though automatic classifiers designed to distinguish translated texts achieve high accuracy and prior work has identified common hallmarks of translationese,... | Shira Wein |  |
| 1949 |  |  [Impressions: Visual Semiotics and Aesthetic Impact Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.755) |  | 0 | Is aesthetic impact different from beauty? Is visual salience a reflection of its capacity for effective communication? We present Impressions, a novel dataset through which to investigate the semiotics of images, and how specific visual features and design choices can elicit specific emotions,... | Caleb Ziems, Diyi Yang, Julia Kruk |  |
| 1950 |  |  [DNA: Denoised Neighborhood Aggregation for Fine-grained Category Discovery](https://doi.org/10.18653/v1/2023.emnlp-main.756) |  | 0 | Discovering fine-grained categories from coarsely labeled data is a practical and challenging task, which can bridge the gap between the demand for fine-grained analysis and the high annotation cost. Previous works mainly focus on instance-level discrimination to learn low-level features, but... | Feng Tian, Ping Chen, Qianying Wang, Qinghua Zheng, Wenbin An, Wenkai Shi, Yan Chen |  |
| 1951 |  |  [Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.757) |  | 0 | The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor... | Anh Tuan Luu, Jie Fu, Jinming Wen, Junbo Zhao, Shuai Zhao |  |
| 1952 |  |  [UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation](https://doi.org/10.18653/v1/2023.emnlp-main.758) |  | 0 | Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and... | Daixuan Cheng, Furu Wei, Hao Sun, Jianfeng Liu, Junyu Bi, Qi Zhang, Shaohan Huang, Weiwei Deng, Yuefeng Zhan, Yujing Wang |  |
| 1953 |  |  [KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning](https://doi.org/10.18653/v1/2023.emnlp-main.759) |  | 0 | In task-oriented dialogs (TOD), reinforcement learning (RL) algorithms train a model to directly optimize response for task-related metrics. However, RL often needs to perform exploration, which can be time-consuming due to the slow auto-regressive sequence generation process. We investigate an... | Kun Qian, Qingyang Wu, Xiao Yu, Zhou Yu |  |
| 1954 |  |  [Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU](https://doi.org/10.18653/v1/2023.emnlp-main.760) |  | 0 | Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable... | Fajri Koto, Haonan Li, Nurul Aisyah, Timothy Baldwin |  |
| 1955 |  |  [Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.761) |  | 0 | A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better... | Aman Madaan, Mausam, Pranjal Aggarwal, Yiming Yang |  |
| 1956 |  |  [Bridging Information-Theoretic and Geometric Compression in Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.762) |  | 0 | For a language model (LM) to faithfully model human language, it must compress vast, potentially infinite information into relatively few dimensions. We propose analyzing compression in (pre-trained) LMs from two points of view: geometric and information-theoretic. We demonstrate that the two views... | Corentin Kervadec, Emily Cheng, Marco Baroni |  |
| 1957 |  |  [Pre-training Language Models for Comparative Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.763) |  | 0 | Comparative reasoning is a process of comparing objects, concepts, or entities to draw conclusions, which constitutes a fundamental cognitive ability. In this paper, we propose a novel framework to pre-train language models for enhancing their abilities of comparative reasoning over texts. While... | Meng Jiang, Mengxia Yu, Wenhao Yu, Zhihan Zhang |  |
| 1958 |  |  [Improved Pseudo Data for Machine Translation Quality Estimation with Constrained Beam Search](https://doi.org/10.18653/v1/2023.emnlp-main.764) |  | 0 | Machine translation (MT) quality estimation (QE) is a crucial task to estimate the quality of MT outputs when reference translations are unavailable. Many studies focus on generating pseudo data using large parallel corpus and achieve remarkable success in the supervised setting. However, pseudo... | Hao Yang, Jiajun Chen, Shimin Tao, Shuaijie She, Shujian Huang, Wei Zou, Xiang Geng, Yu Zhang, Zhejian Lai |  |
| 1959 |  |  [Text Embeddings Reveal (Almost) As Much As Text](https://doi.org/10.18653/v1/2023.emnlp-main.765) |  | 0 | How much private information do text embeddings reveal about the original text? We investigate the problem of embedding inversion, reconstructing the full text represented in dense text embeddings. We frame the problem as controlled generation: generating text that, when reembedded, is close to a... | Alexander M. Rush, John X. Morris, Vitaly Shmatikov, Volodymyr Kuleshov |  |
| 1960 |  |  [AutoTrial: Prompting Language Models for Clinical Trial Design](https://doi.org/10.18653/v1/2023.emnlp-main.766) |  | 0 | Clinical trials are critical for drug development. Constructing the appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for patient recruitment) is essential for the trial’s success. Proper design of clinical trial protocols should consider similar precedent trials and their... | Cao Xiao, Jimeng Sun, Zifeng Wang |  |
| 1961 |  |  [Faster Minimum Bayes Risk Decoding with Confidence-based Pruning](https://doi.org/10.18653/v1/2023.emnlp-main.767) |  | 0 | Minimum Bayes risk (MBR) decoding outputs the hypothesis with the highest expected utility over the model distribution for some utility function. It has been shown to improve accuracy over beam search in conditional language generation problems and especially neural machine translation, in both... | Andreas Vlachos, Julius Cheng |  |
| 1962 |  |  [Enhancing Generative Retrieval with Reinforcement Learning from Relevance Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.768) |  | 0 | The recent advent of end-to-end generative retrieval marks a significant shift in document retrieval methods, leveraging differentiable search indexes to directly produce relevant document identifiers (docids) in response to a specific query. Nevertheless, this approach faces two fundamental... | JiRong Wen, Yujia Zhou, Zhicheng Dou |  |
| 1963 |  |  [Multi-Source Probing for Open-Domain Conversational Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.769) |  | 0 | Dialogue comprehension and generation are vital to the success of open-domain dialogue systems. Although pre-trained generative conversation models have made significant progress in generating fluent responses, people have difficulty judging whether they understand and efficiently model the... | Hao Zhou, Jie Zhou, Minlie Huang, Yuanxi Li |  |
| 1964 |  |  [Hallucination Mitigation in Natural Language Generation from Large-Scale Open-Domain Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.770) |  | 0 | In generating natural language descriptions for knowledge graph triples, prior works used either small-scale, human-annotated datasets or datasets with limited variety of graph shapes, e.g., those having mostly star graphs. Graph-to-text models trained and evaluated on such datasets are largely not... | Chengkai Li, Xiao Shi, Zeyu Zhang, Zhengyuan Zhu |  |
| 1965 |  |  [Multi-Source Multi-Type Knowledge Exploration and Exploitation for Dialogue Generation](https://doi.org/10.18653/v1/2023.emnlp-main.771) |  | 0 | Open-domain multi-turn dialogue generation encounters the significant challenge of lacking various types of knowledge from diverse sources. Existing models typically focus on identifying specific types of dialogue knowledge and utilize corresponding datasets for training. However, this approach... | Hongliang Dai, Piji Li, Xuanfan Ni, Zhaochun Ren |  |
| 1966 |  |  [Focus Your Attention (with Adaptive IIR Filters)](https://doi.org/10.18653/v1/2023.emnlp-main.772) |  | 0 | We present a new layer in which dynamic (i.e., input-dependent) Infinite Impulse Response (IIR) filters of order two are used to process the input sequence prior to applying conventional attention. The input is split into chunks, and the coefficients of these filters are determined based on... | Itamar Zimerman, Lior Wolf, Shahar Lutati |  |
| 1967 |  |  [Identifying Statements Crucial for Awareness of Interpretive Nonsense to Prevent Communication Breakdowns](https://doi.org/10.18653/v1/2023.emnlp-main.773) |  | 0 | During remote conversations, communication breakdowns often occur when a listener misses certain statements. Our objective is to prevent such breakdowns by identifying Statements Crucial for Awareness of Interpretive Nonsense (SCAINs). If a listener misses a SCAIN, s/he may interpret subsequent... | Michita Imai, Tomoyuki Maekawa |  |
| 1968 |  |  [Multilingual Large Language Models Are Not (Yet) Code-Switchers](https://doi.org/10.18653/v1/2023.emnlp-main.774) |  | 0 | Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of... | Alham Fikri Aji, Genta Indra Winata, Jan Christian Blaise Cruz, Ruochen Zhang, Samuel Cahyawijaya |  |
| 1969 |  |  [Reinforced Target-driven Conversational Promotion](https://doi.org/10.18653/v1/2023.emnlp-main.775) |  | 0 | The ability to proactively engage with users towards pitching products is highly desired for conversational assistants. However, existing conversational recommendation methods overemphasize on acquiring user preferences while ignore the strategic planning for nudging users towards accepting a... | Dung D. Le, Huy Dao, Lizi Liao, Yuxiang Nie |  |
| 1970 |  |  [Identification of Multimodal Stance Towards Frames of Communication](https://doi.org/10.18653/v1/2023.emnlp-main.776) |  | 0 | Frames of communication are often evoked in multimedia documents. When an author decides to add an image to a text, one or both of the modalities may evoke a communication frame. Moreover, when evoking the frame, the author also conveys her/his stance towards the frame. Until now, determining if... | Maxwell A. Weinzierl, Sanda M. Harabagiu |  |
| 1971 |  |  [Unsupervised Sounding Pixel Learning](https://doi.org/10.18653/v1/2023.emnlp-main.777) |  | 0 | Sounding source localization is a challenging cross-modal task due to the difficulty of cross-modal alignment. Although supervised cross-modal methods achieve encouraging performance, heavy manual annotations are expensive and inefficient. Thus it is valuable and meaningful to develop unsupervised... | Yang Yang, Yanli Ji, Yining Zhang |  |
| 1972 |  |  [LM vs LM: Detecting Factual Errors via Cross Examination](https://doi.org/10.18653/v1/2023.emnlp-main.778) |  | 0 | A prominent weakness of modern language models (LMs) is their tendency to generate factually incorrect text, which hinders their usability. A natural question is whether such factual errors can be detected automatically. Inspired by truth-seeking mechanisms in law, we propose a factuality... | Amir Globerson, May Hamri, Mor Geva, Roi Cohen |  |
| 1973 |  |  [Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.779) |  | 0 | Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the debate and... | Bram van Dijk, Marco Spruit, Max Johannes van Duijn, Tom Kouwenhoven |  |
| 1974 |  |  [PIEClass: Weakly-Supervised Text Classification with Prompting and Noise-Robust Iterative Ensemble Training](https://doi.org/10.18653/v1/2023.emnlp-main.780) |  | 0 | Weakly-supervised text classification trains a classifier using the label name of each target class as the only supervision, which largely reduces human annotation efforts. Most existing methods first use the label names as static keyword-based features to generate pseudo labels, which are then... | Jiawei Han, Minhao Jiang, Yu Meng, Yu Zhang, Yunyi Zhang |  |
| 1975 |  |  [MeaeQ: Mount Model Extraction Attacks with Efficient Queries](https://doi.org/10.18653/v1/2023.emnlp-main.781) |  | 0 | We study model extraction attacks in natural language processing (NLP) where attackers aim to steal victim models by repeatedly querying the open Application Programming Interfaces (APIs). Recent works focus on limited-query budget settings and adopt random sampling or active learning-based... | Chengwei Dai, Kun Li, Minxuan Lv, Wei Zhou |  |
| 1976 |  |  [The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.782) |  | 0 | Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In... | Doyoung Kim, Jamin Shin, Joel Jang, Minjoon Seo, Se June Joo, Seonghyeon Ye, Seungone Kim |  |
| 1977 |  |  [Explaining Interactions Between Text Spans](https://doi.org/10.18653/v1/2023.emnlp-main.783) |  | 0 | Reasoning over spans of tokens from different parts of the input is essential for natural language understanding (NLU) tasks such as fact-checking (FC), machine reading comprehension (MRC) or natural language inference (NLI). However, existing highlight-based explanations primarily focus on... | Isabelle Augenstein, Pepa Atanasova, Sagnik Ray Choudhury |  |
| 1978 |  |  [Predictive Chemistry Augmented with Text Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.784) |  | 0 | This paper focuses on using natural language descriptions to enhance predictive models in the chemistry field. Conventionally, chemoinformatics models are trained with extensive structured data manually extracted from the literature. In this paper, we introduce TextReact, a novel method that... | Connor W. Coley, Regina Barzilay, Yujie Qian, Zhengkai Tu, Zhening Li |  |
| 1979 |  |  [System Combination via Quality Estimation for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.emnlp-main.785) |  | 0 | Quality estimation models have been developed to assess the corrections made by grammatical error correction (GEC) models when the reference or gold-standard corrections are not available. An ideal quality estimator can be utilized to combine the outputs of multiple GEC systems by choosing the best... | Hwee Tou Ng, Muhammad Reza Qorib |  |
| 1980 |  |  [Rethinking Negative Pairs in Code Search](https://doi.org/10.18653/v1/2023.emnlp-main.786) |  | 0 | Recently, contrastive learning has become a key component in fine-tuning code search models for software development efficiency and effectiveness. It pulls together positive code snippets while pushing negative samples away given search queries. Among contrastive learning, InfoNCE is the most... | Anh Tuan Luu, Chunyan Miao, Haochen Li, Xin Zhou |  |
| 1981 |  |  [Question Answering as Programming for Solving Time-Sensitive Questions](https://doi.org/10.18653/v1/2023.emnlp-main.787) |  | 0 | Question answering plays a pivotal role in human daily life because it involves our acquisition of knowledge about the world. However, due to the dynamic and ever-changing nature of real-world facts, the answer can be completely different when the time constraint in the question changes. Recently,... | Bei Chen, Cheng Yang, JianGuang Lou, Siheng Li, Xinyu Zhu, Yujiu Yang |  |
| 1982 |  |  [Joint Geometrical and Statistical Domain Adaptation for Cross-domain Code Vulnerability Detection](https://doi.org/10.18653/v1/2023.emnlp-main.788) |  | 0 | In code vulnerability detection tasks, a detector trained on a label-rich source domain fails to provide accurate prediction on new or unseen target domains due to the lack of labeled training data on target domains. Previous studies mainly utilize domain adaptation to perform cross-domain... | Gang Zhao, Jidong Zhai, Qianjin Du, Shiji Zhou, Xiaohui Kuang |  |
| 1983 |  |  [Revisiting Sparse Retrieval for Few-shot Entity Linking](https://doi.org/10.18653/v1/2023.emnlp-main.789) |  | 0 | Entity linking aims to link ambiguous mentions to their corresponding entities in a knowledge base. One of the key challenges comes from insufficient labeled data for specific domains. Although dense retrievers have achieved excellent performance on several benchmarks, their performance decreases... | Baotian Hu, Min Zhang, Yulin Chen, Zhenran Xu |  |
| 1984 |  |  [Controlling Pre-trained Language Models for Grade-Specific Text Simplification](https://doi.org/10.18653/v1/2023.emnlp-main.790) |  | 0 | Text simplification systems rewrite text to make it more readable while preserving its content. However, what makes a text easy to read depends on the intended readers. Recent work has shown that pre-trained language models can simplify text using a wealth of techniques to control output... | Marine Carpuat, Sweta Agrawal |  |
| 1985 |  |  [CLEVR-Implicit: A Diagnostic Dataset for Implicit Reasoning in Referring Expression Comprehension](https://doi.org/10.18653/v1/2023.emnlp-main.791) |  | 0 | Recently, pre-trained vision-language (VL) models have achieved remarkable success in various cross-modal tasks, including referring expression comprehension (REC). These models are pre-trained on the large-scale image-text pairs to learn the alignment between words in textual descriptions and... | Jingwei Zhang, Xin Wu, Yi Cai |  |
| 1986 |  |  ["Are Your Explanations Reliable?" Investigating the Stability of LIME in Explaining Text Classifiers by Marrying XAI and Adversarial Attack](https://doi.org/10.18653/v1/2023.emnlp-main.792) |  | 0 | LIME has emerged as one of the most commonly referenced tools in explainable AI (XAI) frameworks that is integrated into critical machine learning applications (e.g., healthcare and finance). However, its stability remains little explored, especially in the context of text data, due to the unique... | Christopher Burger, Lingwei Chen, Thai Le |  |
| 1987 |  |  [CQE: A Comprehensive Quantity Extractor](https://doi.org/10.18653/v1/2023.emnlp-main.793) |  | 0 | Quantities are essential in documents to describe factual information. They are ubiquitous in application domains such as finance, business, medicine, and science in general. Compared to other information extraction approaches, interestingly only a few works exist that describe methods for a proper... | Michael Gertz, Philip Göldner, Satya Almasian, Vivian Kazakova |  |
| 1988 |  |  [Context Compression for Auto-regressive Transformers with Sentinel Tokens](https://doi.org/10.18653/v1/2023.emnlp-main.794) |  | 0 | The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this... | Kenny Q. Zhu, Qi Jia, Siyu Ren |  |
| 1989 |  |  [A Unified View of Evaluation Metrics for Structured Prediction](https://doi.org/10.18653/v1/2023.emnlp-main.795) |  | 0 | We present a conceptual framework that unifies a variety of evaluation metrics for different structured prediction tasks (e.g. event and relation extraction, syntactic and semantic parsing). Our framework requires representing the outputs of these tasks as objects of certain data types, and derives... | Aaron Steven White, Benjamin Van Durme, Tongfei Chen, William Gantt, Yunmo Chen |  |
| 1990 |  |  [A Deeper (Autoregressive) Approach to Non-Convergent Discourse Parsing](https://doi.org/10.18653/v1/2023.emnlp-main.796) |  | 0 | Online social platforms provide a bustling arena for information-sharing and for multi-party discussions. Various frameworks for dialogic discourse parsing were developed and used for the processing of discussions and for predicting the productivity of a dialogue. However, most of these frameworks... | Oren Tsur, Yoav Tulpan |  |
| 1991 |  |  [We are Who We Cite: Bridges of Influence Between Natural Language Processing and Other Academic Fields](https://doi.org/10.18653/v1/2023.emnlp-main.797) |  | 0 | Natural Language Processing (NLP) is poised to substantially influence the world. However, significant progress comes hand-in-hand with substantial risks. Addressing them requires broad engagement with various fields of study. Yet, little empirical work examines the state of such engagement (past... | Bela Gipp, Jan Philip Wahle, Mohamed Abdalla, Saif M. Mohammad, Terry Ruas |  |
| 1992 |  |  [Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration](https://doi.org/10.18653/v1/2023.emnlp-main.798) |  | 0 | Kendall’s tau is frequently used to meta-evaluate how well machine translation (MT) evaluation metrics score individual translations. Its focus on pairwise score comparisons is intuitive but raises the question of how ties should be handled, a gray area that has motivated different variants in the... | Daniel Deutsch, George F. Foster, Markus Freitag |  |
| 1993 |  |  [SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization](https://doi.org/10.18653/v1/2023.emnlp-main.799) |  | 0 | Data scarcity has been a long standing issue in the field of open-domain social dialogue. To quench this thirst, we present SODA: the first publicly available, million-scale high-quality social dialogue dataset. By contextualizing social commonsense knowledge from a knowledge graph, we are able to... | Gunhee Kim, Hyunwoo Kim, Jack Hessel, Liwei Jiang, Maarten Sap, Malihe Alikhani, Pei Zhou, Peter West, Ronan Le Bras, Ximing Lu, Yejin Choi, Youngjae Yu |  |
| 1994 |  |  [Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.800) |  | 0 | Knowledge graph entity typing (KGET) aims at inferring plausible types of entities in knowledge graphs. Existing approaches to KGET focus on how to better encode the knowledge provided by the neighbors and types of an entity into its representation. However, they ignore the semantic knowledge... | Jeff Z. Pan, Ru Li, Víctor GutiérrezBasulto, Zhiliang Xiang, Zhiwei Hu |  |
| 1995 |  |  [MailEx: Email Event and Argument Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.801) |  | 0 | In this work, we present the first dataset, MailEx, for performing event extraction from conversational email threads. To this end, we first proposed a new taxonomy covering 10 event types and 76 arguments in the email domain. Our final dataset includes 1.5K email threads and ~4K emails, which are... | Ali K. Raz, Gaurav Singh, Joshua Poore, Paulo C. G. Costa, Saurabh Srivastava, Shou Matsumoto, Ziyu Yao |  |
| 1996 |  |  [Optimized Tokenization for Transcribed Error Correction](https://doi.org/10.18653/v1/2023.emnlp-main.802) |  | 0 | The challenges facing speech recognition systems, such as variations in pronunciations, adverse audio conditions, and the scarcity of labeled data, emphasize the necessity for a post-processing step that corrects recurring errors. Previous research has shown the advantages of employing dedicated... | Shlomo E. Chazan, Tomer Wullach |  |
| 1997 |  |  [Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.803) |  | 0 | Although pre-trained language models (PLM) have achieved great success in question answering (QA), their robustness is still insufficient to support their practical applications, especially in the face of distribution shifts. Recently, test-time adaptation (TTA) has shown great potential for... | Hai Ye, Juntao Li, Min Zhang, Yi Su, Yixin Ji |  |
| 1998 |  |  [Generative Adversarial Training with Perturbed Token Detection for Model Robustness](https://doi.org/10.18653/v1/2023.emnlp-main.804) |  | 0 | Adversarial training is the dominant strategy towards model robustness. Current adversarial training methods typically apply perturbations to embedding representations, whereas actual text-based attacks introduce perturbations as discrete tokens. Thus there exists a gap between the continuous... | Jiahao Zhao, Wenji Mao |  |
| 1999 |  |  [Multi-Task Knowledge Distillation with Embedding Constraints for Scholarly Keyphrase Boundary Classification](https://doi.org/10.18653/v1/2023.emnlp-main.805) |  | 0 | The task of scholarly keyphrase boundary classification aims at identifying keyphrases from scientific papers and classifying them with their types from a set of predefined classes (e.g., task, process, or material). Despite the importance of keyphrases and their types in many downstream... | Cornelia Caragea, Seo Park |  |
| 2000 |  |  [Set Learning for Generative Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.806) |  | 0 | Recent efforts have endeavored to employ the sequence-to-sequence (Seq2Seq) model in Information Extraction (IE) due to its potential to tackle multiple IE tasks in a unified manner. Under this formalization, multiple structured objects are concatenated as the target sequence in a predefined order.... | Bin Liang, Jiangnan Li, KamFai Wong, Ruifeng Xu, Yice Zhang |  |
| 2001 |  |  [Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation](https://doi.org/10.18653/v1/2023.emnlp-main.807) |  | 0 | Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the goal of retrieving an image among a set of candidates, which better represents the meaning of an ambiguous word within a given context. In this paper, we make a substantial step towards unveiling this interesting task by... | Anastasia Kritharoula, Giorgos Stamou, Maria Lymperaiou |  |
| 2002 |  |  [Be Selfish, But Wisely: Investigating the Impact of Agent Personality in Mixed-Motive Human-Agent Interactions](https://doi.org/10.18653/v1/2023.emnlp-main.808) |  | 0 | A natural way to design a negotiation dialogue system is via self-play RL: train an agent that learns to maximize its performance by interacting with a simulated user that has been designed to imitate human-human dialogue data. Although this procedure has been adopted in prior work, we find that it... | Gale M. Lucas, Ian Wu, Jonathan Gratch, Kushal Chawla, Yu Rong |  |
| 2003 |  |  [Doolittle: Benchmarks and Corpora for Academic Writing Formalization](https://doi.org/10.18653/v1/2023.emnlp-main.809) |  | 0 | Improving the quality of academic writing is a meaningful but challenging task. Conventional methods of language refinement focus on narrow, specific linguistic features within isolated sentences, such as grammatical errors and improper word use. We propose a more general task, Academic Writing... | Liangming Pan, MinYen Kan, Sedrick Scott Keh, Shizhe Diao, Tianqing Fang, Tong Zhang, Wangchunshu Zhou, Yongyu Lei |  |
| 2004 |  |  [Token Prediction as Implicit Classification to Identify LLM-Generated Text](https://doi.org/10.18653/v1/2023.emnlp-main.810) |  | 0 | This paper introduces a novel approach for identifying the possible large language models (LLMs) involved in text generation. Instead of adding an additional classification layer to a base LM, we reframe the classification task as a next-token prediction task and directly fine-tune the base LM to... | Bhiksha Raj, Hao Kang, Liangze Li, Rita Singh, Vivian Zhai, Yutian Chen |  |
| 2005 |  |  [On Evaluation of Bangla Word Analogies](https://doi.org/10.18653/v1/2023.emnlp-main.811) |  | 0 | This paper presents a benchmark dataset of Bangla word analogies for evaluating the quality of existing Bangla word embeddings. Despite being the 7th largest spoken language in the world, Bangla is still a low-resource language and popular NLP models often struggle to perform well on Bangla data... | Mousumi Akter, Shubhra Kanti Karmaker Santu, Souvika Sarkar |  |
| 2006 |  |  [Reconstruct Before Summarize: An Efficient Two-Step Framework for Condensing and Summarizing Meeting Transcripts](https://doi.org/10.18653/v1/2023.emnlp-main.812) |  | 0 | Meetings typically involve multiple participants and lengthy conversations, resulting in redundant and trivial content. To overcome these challenges, we propose a two-step framework, Reconstruct before Summarize (RbS), for effective and efficient meeting summarization. RbS first leverages a... | Ding Liang, Han Wu, Haochen Tan, Linqi Song, Mingjie Zhan, Wei Shao, Xinyun Zhang, Zhaohui Hou |  |
| 2007 |  |  [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.813) |  | 0 | Large multilingual language models typically rely on a single vocabulary shared across 100+ languages. As these models have increased in parameter count and depth, vocabulary size has remained largely unchanged. This vocabulary bottleneck limits the representational capabilities of multilingual... | Davis Liang, Hila Gonen, Luke Zettlemoyer, Madian Khabsa, Marjan Ghazvininejad, Naman Goyal, Rui Hou, Yuning Mao |  |
| 2008 |  |  [Character-LLM: A Trainable Agent for Role-Playing](https://doi.org/10.18653/v1/2023.emnlp-main.814) |  | 0 | Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human... | Junqi Dai, Linyang Li, Xipeng Qiu, Yunfan Shao |  |
| 2009 |  |  [Natural Language Decompositions of Implicit Content Enable Better Text Representations](https://doi.org/10.18653/v1/2023.emnlp-main.815) |  | 0 | When people interpret text, they rely on inferences that go beyond the observed language itself. Inspired by this observation, we introduce a method for the analysis of text that takes implicitly communicated content explicitly into account. We use a large language model to produce sets of... | Alexander Miserlis Hoyle, Philip Resnik, Pranav Goel, Rupak Sarkar |  |
| 2010 |  |  [A Scalable Framework for Table of Contents Extraction from Complex ESG Annual Reports](https://doi.org/10.18653/v1/2023.emnlp-main.816) |  | 0 | Table of contents (ToC) extraction centres on structuring documents in a hierarchical manner. In this paper, we propose a new dataset, ESGDoc, comprising 1,093 ESG annual reports from 563 companies spanning from 2001 to 2022. These reports pose significant challenges due to their diverse structures... | Lin Gui, Xinyu Wang, Yulan He |  |
| 2011 |  |  [Semantic Space Grounded Weighted Decoding for Multi-Attribute Controllable Dialogue Generation](https://doi.org/10.18653/v1/2023.emnlp-main.817) |  | 0 | Controlling chatbot utterance generation with multiple attributes such as personalities, emotions and dialogue acts is a practically useful but under-studied problem. We propose a novel framework called DASC that possesses strong controllability with a weighted decoding paradigm, while improving... | Kenny Q. Zhu, Mengyue Wu, Zhiling Zhang |  |
| 2012 |  |  [How do languages influence each other? Studying cross-lingual data sharing during LM fine-tuning](https://doi.org/10.18653/v1/2023.emnlp-main.818) |  | 0 | Multilingual language models (MLMs) are jointly trained on data from many different languages such that representation of individual languages can benefit from other languages’ data. Impressive performance in zero-shot cross-lingual transfer shows that these models are able to exploit this... | Dan Garrette, Ekaterina Shutova, Rochelle Choenni |  |
| 2013 |  |  [COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation](https://doi.org/10.18653/v1/2023.emnlp-main.819) |  | 0 | As language models become increasingly integrated into our digital lives, Personalized Text Generation (PTG) has emerged as a pivotal component with a wide range of applications. However, the bias inherent in user written text, often used for PTG model training, can inadvertently associate... | Hamed Firooz, Hongning Wang, Jingzhou Liu, Maziar Sanjabi, Nan Wang, Qifan Wang, Shaoliang Nie, YiChia Wang |  |
| 2014 |  |  [NameGuess: Column Name Expansion for Tabular Data](https://doi.org/10.18653/v1/2023.emnlp-main.820) |  | 0 | Recent advances in large language models have revolutionized many sectors, including the database industry. One common challenge when dealing with large volumes of tabular data is the pervasive use of abbreviated column names, which can negatively impact performance on various data search, access,... | Balasubramaniam Srinivasan, George Karypis, Huzefa Rangwala, Jiani Zhang, Shen Wang, Zhengyuan Shen |  |
| 2015 |  |  [BLESS: Benchmarking Large Language Models on Sentence Simplification](https://doi.org/10.18653/v1/2023.emnlp-main.821) |  | 0 | We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art Large Language Models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture,... | Alison Chi, Dennis Aumiller, Fernando AlvaManchego, Laura VásquezRodríguez, Matthew Shardlow, Sweta Agrawal, Tannon Kew |  |
| 2016 |  |  [To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing](https://doi.org/10.18653/v1/2023.emnlp-main.822) |  | 0 | NLP is in a period of disruptive change that is impacting our methodologies, funding sources, and public perception. In this work, we seek to understand how to shape our future by better understanding our past. We study factors that shape NLP as a field, including culture, incentives, and... | Amanda Bertsch, Clara Na, David Gray Widder, Emma Strubell, Sireesh Gururaja |  |
| 2017 |  |  [PALS: Personalized Active Learning for Subjective Tasks in NLP](https://doi.org/10.18653/v1/2023.emnlp-main.823) |  | 0 | For subjective NLP problems, such as classification of hate speech, aggression, or emotions, personalized solutions can be exploited. Then, the learned models infer about the perception of the content independently for each reader. To acquire training data, texts are commonly randomly assigned to... | Jan Kocon, Julita Bielaniewicz, Kamil Kanclerz, Konrad Karanowski, Marcin Gruza, Piotr Milkowski, Przemyslaw Kazienko |  |
| 2018 |  |  [ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation](https://doi.org/10.18653/v1/2023.emnlp-main.824) |  | 0 | State-of-the-art vision-language models (VLMs) still have limited performance in structural knowledge extraction, such as relations between objects. In this work, we present ViStruct, a training framework to learn VLMs for effective visual structural knowledge extraction. Two novel designs are... | Derek Hoiem, Heng Ji, Manling Li, Xingyao Wang, Yangyi Chen |  |
| 2019 |  |  [LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.825) |  | 0 | Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of... | ChinYew Lin, Huiqiang Jiang, Lili Qiu, Qianhui Wu, Yuqing Yang |  |
| 2020 |  |  [EXPLAIN, EDIT, GENERATE: Rationale-Sensitive Counterfactual Data Augmentation for Multi-hop Fact Verification](https://doi.org/10.18653/v1/2023.emnlp-main.826) |  | 0 | Automatic multi-hop fact verification task has gained significant attention in recent years. Despite impressive results, these well-designed models perform poorly on out-of-domain data. One possible solution is to augment the training data with counterfactuals, which are generated by minimally... | Deyu Zhou, Haiyang Zhu, Jiasheng Si, Yibo Zhao, Yingjie Zhu, Yulan He |  |
| 2021 |  |  [An Exploration of Left-Corner Transformations](https://doi.org/10.18653/v1/2023.emnlp-main.827) |  | 0 | The left-corner transformation (Rosenkrantz and Lewis, 1970) is used to remove left recursion from context-free grammars, which is an important step towards making the grammar parsable top-down with simple techniques. This paper generalizes prior left-corner transformations to support... | Andreas Opedal, Eleftheria Tsipidi, Ryan Cotterell, Tiago Pimentel, Tim Vieira |  |
| 2022 |  |  [Characterizing and Verifying Scientific Claims: Qualitative Causal Structure is All You Need](https://doi.org/10.18653/v1/2023.emnlp-main.828) |  | 0 | A scientific claim typically begins with the formulation of a research question or hypothesis, which is a tentative statement or proposition about a phenomenon or relationship between variables. Within the realm of scientific claim verification, considerable research efforts have been dedicated to... | Jinxuan Wu, Wenhan Chao, Xian Zhou, Zhunchen Luo |  |
| 2023 |  |  [FOCUS: Effective Embedding Initialization for Monolingual Specialization of Multilingual Models](https://doi.org/10.18653/v1/2023.emnlp-main.829) |  | 0 | Using model weights pretrained on a high-resource language as a warm start can reduce the need for data and compute to obtain high-quality language models for other, especially low-resource, languages. However, if we want to use a new tokenizer specialized for the target language, we cannot... | Gerard de Melo, Konstantin Dobler |  |
| 2024 |  |  [ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games](https://doi.org/10.18653/v1/2023.emnlp-main.830) |  | 0 | In this work we investigate the capacity of language models to generate explicit, interpretable, and interactive world models of scientific and common-sense reasoning tasks. We operationalize this as a task of generating text games, expressed as hundreds of lines of Python code. To facilitate this... | Graham Todd, MarcAlexandre Côté, Peter A. Jansen, Ruoyao Wang, Xingdi Yuan, Ziang Xiao |  |
| 2025 |  |  [Skill-Based Few-Shot Selection for In-Context Learning](https://doi.org/10.18653/v1/2023.emnlp-main.831) |  | 0 | \*In-context learning\* is the paradigm that adapts large language models to downstream tasks by providing a few examples. \*Few-shot selection\*—selecting appropriate examples for each test instance separately—is important for in-context learning. In this paper, we propose \*\*Skill-KNN\*\*, a... | Bei Chen, Bo Zhou, JianGuang Lou, Nanning Zheng, Qiang Fu, Shengnan An, Weizhu Chen, Zeqi Lin |  |
| 2026 |  |  [MaNtLE: Model-agnostic Natural Language Explainer](https://doi.org/10.18653/v1/2023.emnlp-main.832) |  | 0 | Understanding the internal reasoning behind the predictions of machine learning systems is increasingly vital, given their rising adoption and acceptance. While previous approaches, such as LIME generate algorithmic explanations by attributing importance to input features for individual examples,... | Kerem Zaman, Rakesh R. Menon, Shashank Srivastava |  |
| 2027 |  |  [PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer](https://doi.org/10.18653/v1/2023.emnlp-main.833) |  | 0 | Recent studies show that prompt tuning can better leverage the power of large language models than fine-tuning on downstream natural language understanding tasks. However, the existing prompt tuning methods have training instability issues, as the variance of scores under different random seeds is... | Heng Huang, Jiuhai Chen, Lichang Chen, Minhao Cheng |  |
| 2028 |  |  [Ling-CL: Understanding NLP Models through Linguistic Curricula](https://doi.org/10.18653/v1/2023.emnlp-main.834) |  | 0 | We employ a characterization of linguistic complexity from psycholinguistic and language acquisition research to develop data-driven curricula to understand the underlying linguistic knowledge that models learn to address NLP tasks. The novelty of our approach is in the development of linguistic... | Hadi Amiri, Mohamed Elgaar |  |
| 2029 |  |  [Towards Unsupervised Recognition of Token-level Semantic Differences in Related Documents](https://doi.org/10.18653/v1/2023.emnlp-main.835) |  | 0 | Automatically highlighting words that cause semantic differences between two documents could be useful for a wide range of applications. We formulate recognizing semantic differences (RSD) as a token-level regression task and study three unsupervised approaches that rely on a masked language model.... | Jannis Vamvas, Rico Sennrich |  |
| 2030 |  |  [Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance](https://doi.org/10.18653/v1/2023.emnlp-main.836) |  | 0 | Multilingual Neural Machine Translation (MNMT) facilitates knowledge sharing but often suffers from poor zero-shot (ZS) translation qualities. While prior work has explored the causes of overall low zero-shot translation qualities, our work introduces a fresh perspective: the presence of... | Christof Monz, Shaomu Tan |  |
| 2031 |  |  [SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA](https://doi.org/10.18653/v1/2023.emnlp-main.837) |  | 0 | Question answering over hybrid contexts is a complex task, which requires the combination of information extracted from unstructured texts and structured tables in various ways. Recently, In-Context Learning demonstrated significant performance advances for reasoning tasks. In this paradigm, a... | Bart Baesens, Jonathan Tonglet, Manon Reusens, Philipp Borchert |  |
| 2032 |  |  [Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations](https://doi.org/10.18653/v1/2023.emnlp-main.838) |  | 0 | In the field of natural language processing, open-domain chatbots have emerged as an important research topic. However, a major limitation of existing open-domain chatbot research is its singular focus on short single-session dialogue, neglecting the potential need for understanding contextual... | Hyounghun Kim, Jihyoung Jang, Minseong Boo |  |
| 2033 |  |  [DueT: Image-Text Contrastive Transfer Learning with Dual-adapter Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.839) |  | 0 | This paper presents DueT, a novel transfer learning method for vision and language models built by contrastive learning. In DueT, adapters are inserted into the image and text encoders, which have been initialized using models pre-trained on uni-modal corpora and then frozen. By training only these... | Koki Maeda, Kuniko Saito, Kyosuke Nishida, Taku Hasegawa |  |
| 2034 |  |  [Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation](https://doi.org/10.18653/v1/2023.emnlp-main.840) |  | 0 | In Conversational Recommendation System (CRS), an agent is asked to recommend a set of items to users within natural language conversations. To address the need for both conversational capability and personalized recommendations, prior works have utilized separate recommendation and dialogue... | Eunseo Jung, Lei Chen, Yeongseo Jung |  |
| 2035 |  |  [CLAIR: Evaluating Image Captions with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.841) |  | 0 | The evaluation of machine-generated image captions poses an interesting yet persistent challenge. Effective evaluation measures must consider numerous dimensions of similarity, including semantic relevance, visual structure, object interactions, caption diversity, and specificity. Existing... | David M. Chan, John F. Canny, Joseph Gonzalez, Suzanne Petryk, Trevor Darrell |  |
| 2036 |  |  [MoPe: Model Perturbation based Privacy Attacks on Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.842) |  | 0 | Recent work has shown that Large Language Models (LLMs) can unintentionally leak sensitive information present in their training data. In this paper, we present Model Perturbations (MoPe), a new method to identify with high confidence if a given text is in the training data of a pre-trained... | Jason Wang, Jeffrey G. Wang, Marvin Li, Seth Neel |  |
| 2037 |  |  [q2d: Turning Questions into Dialogs to Teach Models How to Search](https://doi.org/10.18653/v1/2023.emnlp-main.843) |  | 0 | One of the exciting capabilities of recent language models for dialog is their ability to independently search for relevant information to ground a given dialog response. However, obtaining training data to teach models how to issue search queries is time and resource consuming. In this work, we... | Enav Weinreb, Ido Hakimi, Roee Aharoni, Shlomi CohenGanor, Yoad Lewenberg, Yonatan Bitton |  |
| 2038 |  |  [Aligning Large Language Models through Synthetic Feedback](https://doi.org/10.18653/v1/2023.emnlp-main.844) |  | 0 | Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT. In this work, we propose a novel alignment... | Donghyun Kwak, Jamin Shin, Kang Min Yoo, Minjoon Seo, Sanghwan Bae, Soyoung Kang, Sungdong Kim |  |
| 2039 |  |  [You Told Me That Joke Twice: A Systematic Investigation of Transferability and Robustness of Humor Detection Models](https://doi.org/10.18653/v1/2023.emnlp-main.845) |  | 0 | In this study, we focus on automatic humor detection, a highly relevant task for conversational AI. To date, there are several English datasets for this task, but little research on how models trained on them generalize and behave in the wild. To fill this gap, we carefully analyze existing... | Alexander Baranov, Pavel Braslavski, Vladimir Kniazhevsky |  |
| 2040 |  |  [Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction](https://doi.org/10.18653/v1/2023.emnlp-main.846) |  | 0 | Recent advances in multimodal pre-trained models have significantly improved information extraction from visually-rich documents (VrDs), in which named entity recognition (NER) is treated as a sequence-labeling task of predicting the BIO entity tags for tokens, following the typical setting of NLP.... | Chong Zhang, Huan Chen, Huijia Zhu, Jinyang Tang, Qi Zhang, Tao Gui, Ya Guo, Yi Tu |  |
| 2041 |  |  [Empower Nested Boolean Logic via Self-Supervised Curriculum Learning](https://doi.org/10.18653/v1/2023.emnlp-main.847) |  | 0 | Beyond the great cognitive powers showcased by language models, it is crucial to scrutinize whether their reasoning capabilities stem from strong generalization or merely exposure to relevant data. As opposed to constructing increasingly complex logic, this paper probes into the boolean logic, the... | Hai Zhao, Hongqiu Wu, Linfeng Liu, Min Zhang |  |
| 2042 |  |  [The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.848) |  | 0 | We conduct an inquiry into the sociotechnical aspects of sentiment analysis (SA) by critically examining 189 peer-reviewed papers on their applications, models, and datasets. Our investigation stems from the recognition that SA has become an integral component of diverse sociotechnical systems,... | Mukund Srinath, Pranav Venkit, Rebecca J. Passonneau, Sanjana Gautam, Saranya Venkatraman, Shomir Wilson, Vipul Gupta |  |
| 2043 |  |  [Poisoning Retrieval Corpora by Injecting Adversarial Passages](https://doi.org/10.18653/v1/2023.emnlp-main.849) |  | 0 | Dense retrievers have achieved state-of-the-art performance in various information retrieval tasks, but to what extent can they be safely deployed in real-world applications? In this work, we propose a novel attack for dense retrieval systems in which a malicious user generates a small number of... | Alexander Wettig, Danqi Chen, Zexuan Zhong, Ziqing Huang |  |
| 2044 |  |  [DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules](https://doi.org/10.18653/v1/2023.emnlp-main.850) |  | 0 | Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects. While existing mitigations tackle discrepancies for individual target dialects, they assume access to high-accuracy... | Diyi Yang, William Held, Yanchen Liu |  |
| 2045 |  |  [Clustering Pseudo Language Family in Multilingual Translation Models with Fisher Information Matrix](https://doi.org/10.18653/v1/2023.emnlp-main.851) |  | 0 | In multilingual translation research, the comprehension and utilization of language families are of paramount importance. Nevertheless, clustering languages based solely on their ancestral families can yield suboptimal results due to variations in the datasets employed during the model’s training... | Min Zhang, Xinyu Ma, Xuebo Liu |  |
| 2046 |  |  [Unifying Discrete and Continuous Representations for Unsupervised Paraphrase Generation](https://doi.org/10.18653/v1/2023.emnlp-main.852) |  | 0 | Unsupervised paraphrase generation is a challenging task that benefits a variety of downstream NLP applications. Current unsupervised methods for paraphrase generation typically employ round-trip translation or denoising, which require translation corpus and result in paraphrases overly similar to... | Baosong Yang, Dayiheng Liu, Dezhong Peng, Jian Lan, Jiancheng Lv, Jie Fu, Jun Xie, Mei Li, Mingfeng Xue, Wenqiang Lei, Yidan Zhang |  |
| 2047 |  |  [The Benefits of Label-Description Training for Zero-Shot Text Classification](https://doi.org/10.18653/v1/2023.emnlp-main.853) |  | 0 | Pretrained language models have improved zero-shot text classification by allowing the transfer of semantic knowledge from the training data in order to classify among specific label sets in downstream tasks. We propose a simple way to further improve zero-shot accuracies with minimal effort. We... | Debanjan Ghosh, Kevin Gimpel, Lingyu Gao |  |
| 2048 |  |  [Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer](https://doi.org/10.18653/v1/2023.emnlp-main.854) |  | 0 | We introduce and demonstrate how to effectively train multilingual machine translation models with pixel representations. We experiment with two different data settings with a variety of language and script coverage, demonstrating improved performance compared to subword embeddings. We explore... | Elizabeth Salesky, Matt Post, Neha Verma, Philipp Koehn |  |
| 2049 |  |  [Finding Authentic Counterhate Arguments: A Case Study with Public Figures](https://doi.org/10.18653/v1/2023.emnlp-main.855) |  | 0 | We explore authentic counterhate arguments for online hateful content toward individuals. Previous efforts are limited to counterhate to fight against hateful content toward groups. Thus, we present a corpus of 54,816 hateful tweet-paragraph pairs, where the paragraphs are candidate counterhate... | Abdullah Albanyan, Ahmed Hassan, Eduardo Blanco |  |
| 2050 |  |  [Can We Edit Multimodal Large Language Models?](https://doi.org/10.18653/v1/2023.emnlp-main.856) |  | 0 | In this paper, we focus on editing multimodal Large Language Models (LLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we... | Bozhong Tian, Huajun Chen, Ningyu Zhang, Qingbin Liu, Siyuan Cheng, Xi Chen, Yongheng Wang |  |
| 2051 |  |  [Exploring Discourse Structure in Document-level Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.857) |  | 0 | Neural machine translation has achieved great success in the past few years with the help of transformer architectures and large-scale bilingual corpora. However, when the source text gradually grows into an entire document, the performance of current methods for document-level machine translation... | Xiaojun Wan, Xinyu Hu |  |
| 2052 |  |  [ClusterLLM: Large Language Models as a Guide for Text Clustering](https://doi.org/10.18653/v1/2023.emnlp-main.858) |  | 0 | We introduce ClusterLLM, a novel text clustering framework that leverages feedback from an instruction-tuned large language model, such as ChatGPT. Compared with traditional unsupervised methods that builds upon “small” embedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the... | Jingbo Shang, Yuwei Zhang, Zihan Wang |  |
| 2053 |  |  [CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code](https://doi.org/10.18653/v1/2023.emnlp-main.859) |  | 0 | Since the rise of neural natural-language-to-code models (NL→Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code... | Graham Neubig, Shuyan Zhou, Sumit Agarwal, Uri Alon |  |
| 2054 |  |  [Learn and Consolidate: Continual Adaptation for Zero-Shot and Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.860) |  | 0 | Although existing multilingual neural machine translation (MNMT) models have demonstrated remarkable performance to handle multiple translation directions in a single model and achieved zero-shot translation between language pairs unseen in training, they still suffer from relatively poor... | Junpeng Liu, Kaiyu Huang, Maosong Sun, Peng Li, Yang Liu |  |
| 2055 |  |  [e-THERAPIST: I suggest you to cultivate a mindset of positivity and nurture uplifting thoughts](https://doi.org/10.18653/v1/2023.emnlp-main.861) |  | 0 | The shortage of therapists for mental health patients emphasizes the importance of globally accessible dialogue systems alleviating their issues. To have effective interpersonal psychotherapy, these systems must exhibit politeness and empathy when needed. However, these factors may vary as per the... | Asif Ekbal, Kshitij Mishra, Manisha Burja, Priyanshu Priya |  |
| 2056 |  |  [AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages](https://doi.org/10.18653/v1/2023.emnlp-main.862) |  | 0 | Africa is home to over 2,000 languages from over six language families and has the highest linguistic diversity among all continents. This includes 75 languages with at least one million speakers each. Yet, there is little NLP research conducted on African languages. Crucial in enabling such... | Abinew Ali Ayele, Alípio Jorge, Bello Shehu Bello, Bernard Opoku, David Ifeoluwa Adelani, Davis David, Falalu Ibrahim Lawan, Felermino Dário Mário António Ali, Hagos Tesfahun Gebremichael, Hailu Beshada Balcha, Ibrahim Said Ahmad, Idris Abdulmumin, Meriem Beloucif, Nedjma Ousidhoum, Oumaima Hourrane, Pavel Brazdil, Saif M. Mohammad, Salomey Osei, Samuel Rutunda, Sebastian Ruder, Seid Muhie Yimam, Shamsuddeen Hassan Muhammad, Sisay Adugna Chala, Stephen Arthur, Tadesse Destaw Belay, Tajuddeen Gwadabe, Wendimu Baye Messelle |  |
| 2057 |  |  [Quantifying Character Similarity with Vision Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.863) |  | 0 | Record linkage is a bedrock of quantitative social science, as analyses often require linking data from multiple, noisy sources. Off-the-shelf string matching methods are widely used, as they are straightforward and cheap to implement and scale. Not all character substitutions are equally probable,... | Abhishek Arora, Melissa Dell, ShaoYu Jheng, Xinmei Yang |  |
| 2058 |  |  [Syllogistic Reasoning for Legal Judgment Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.864) |  | 0 | Legal judgment assistants are developing fast due to impressive progress of large language models (LLMs). However, people can hardly trust the results generated by a model without reliable analysis of legal judgement. For legal practitioners, it is common practice to utilize syllogistic reasoning... | Furu Wei, Jiahuan Pei, Keyi Kong, Pengjie Ren, Wentao Deng, Yujun Li, Zhaochun Ren, Zhe Chen, Zhumin Chen |  |
| 2059 |  |  [Improving Transformer-based Program Repair Model through False Behavior Diagnosis](https://doi.org/10.18653/v1/2023.emnlp-main.865) |  | 0 | Research on automated program repairs using transformer-based models has recently gained considerable attention. The comprehension of the erroneous behavior of a model enables the identification of its inherent capacity and provides insights for improvement. However, the current landscape of... | Eunseok Lee, Misoo Kim, Youngkyoung Kim |  |
| 2060 |  |  [SUT: Active Defects Probing for Transcompiler Models](https://doi.org/10.18653/v1/2023.emnlp-main.866) |  | 0 | Automatic Program translation has enormous application value and hence has been attracting significant interest from AI researchers. However, we observe that current program translation models still make elementary syntax errors, particularly, when the target language does not have syntax elements... | Bin Gu, Colin B. Clement, Maoquan Wang, Mengnan Qi, Neel Sundaresan, Yongqiang Yao, Yufan Huang, Zihan Liu |  |
| 2061 |  |  [KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection](https://doi.org/10.18653/v1/2023.emnlp-main.867) |  | 0 | Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the \*hallucination\* problem, poses a significant risk to their deployment. A common approach to address this issue is... | Sehyun Choi, Tianqing Fang, Yangqiu Song, Zhaowei Wang |  |
| 2062 |  |  [CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL](https://doi.org/10.18653/v1/2023.emnlp-main.868) |  | 0 | Existing Text-to-SQL generators require the entire schema to be encoded with the user text. This is expensive or impractical for large databases with tens of thousands of columns. Standard dense retrieval techniques are inadequate for schema subsetting of a large structured database, where the... | Dhruva Dhingra, Mayank Kothyari, Soumen Chakrabarti, Sunita Sarawagi |  |
| 2063 |  |  [This Reads Like That: Deep Learning for Interpretable Natural Language Processing](https://doi.org/10.18653/v1/2023.emnlp-main.869) |  | 0 | Prototype learning, a popular machine learning method designed for inherently interpretable decisions, leverages similarities to learned prototypes for classifying new data. While it is mainly applied in computer vision, in this work, we build upon prior research and further explore the extension... | Claudio Fanconi, Julia E. Vogt, Moritz Vandenhirtz, Severin Husmann |  |
| 2064 |  |  [Incorporating Structured Representations into Pretrained Vision & Language Models Using Scene Graphs](https://doi.org/10.18653/v1/2023.emnlp-main.870) |  | 0 | Vision and language models (VLMs) have demonstrated remarkable zero-shot (ZS) performance in a variety of tasks. However, recent works have shown that even the best VLMs struggle to capture aspects of compositional scene understanding, such as object attributes, relations, and action states. In... | Alon Mendelson, Amir Globerson, Assaf Arbelle, Leonid Karlinsky, Roei Herzig, Rogério Feris, Trevor Darrell |  |
| 2065 |  |  [TLM: Token-Level Masking for Transformers](https://doi.org/10.18653/v1/2023.emnlp-main.871) |  | 0 | Structured dropout approaches, such as attention dropout and DropHead, have been investigated to regularize the multi-head attention mechanism in Transformers. In this paper, we propose a new regularization scheme based on token-level rather than structure-level to reduce overfitting. Specifically,... | Dongxiang Zhang, Gang Chen, Han Wang, Hao Zhang, Kebin Fang, Yangjun Wu |  |
| 2066 |  |  [Addressing NER Annotation Noises with Uncertainty-Guided Tree-Structured CRFs](https://doi.org/10.18653/v1/2023.emnlp-main.872) |  | 0 | Real-world named entity recognition (NER) datasets are notorious for their noisy nature, attributed to annotation errors, inconsistencies, and subjective interpretations. Such noises present a substantial challenge for traditional supervised learning methods. In this paper, we present a new and... | Jian Liu, Jinan Xu, Weichang Liu, Yufeng Chen, Zhe Zhao |  |
| 2067 |  |  [Hi Guys or Hi Folks? Benchmarking Gender-Neutral Machine Translation with the GeNTE Corpus](https://doi.org/10.18653/v1/2023.emnlp-main.873) |  | 0 | Gender inequality is embedded in our communication practices and perpetuated in translation technologies. This becomes particularly apparent when translating into grammatical gender languages, where machine translation (MT) often defaults to masculine and stereotypical representations by making... | Andrea Piergentili, Beatrice Savoldi, Dennis Fucci, Luisa Bentivogli, Matteo Negri |  |
| 2068 |  |  [Multilingual Holistic Bias: Extending Descriptors and Patterns to Unveil Demographic Biases in Languages at Scale](https://doi.org/10.18653/v1/2023.emnlp-main.874) |  | 0 | We introduce a multilingual extension of the HolisticBias dataset, the largest English template-based taxonomy of textual people references: Multilingual HolisticBias. This extension consists of 20,459 sentences in 50 languages distributed across 13 demographic axes. Source sentences are built from... | Carleigh Wood, Christophe Ropers, Cynthia Gao, Daniel Licht, Elahe Kalbassi, Eric Michael Smith, Marta R. Costajussà, Pierre Andrews, Prangthip Hansanti |  |
| 2069 |  |  [GlobalBench: A Benchmark for Global Progress in Natural Language Processing](https://doi.org/10.18653/v1/2023.emnlp-main.875) |  | 0 | Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of... | Alham Fikri Aji, Alissa Ostapenko, Antonios Anastasopoulos, Fahim Faisal, Genta Indra Winata, Graham Neubig, Pengfei Liu, Samuel Cahyawijaya, Simran Khanuja, Yueqi Song, Yulia Tsvetkov |  |
| 2070 |  |  [DetGPT: Detect What You Need via Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.876) |  | 0 | In recent years, the field of computer vision has seen significant advancements thanks to the development of large language models (LLMs). These models have enabled more effective and sophisticated interactions between humans and machines, paving the way for novel techniques that blur the lines... | Hang Xu, Hanze Dong, Jiahui Gao, Jianhua Han, Jipeng Zhang, Lewei Yao, Lingpeng Kong, Renjie Pi, Rui Pan, Shizhe Diao, Tong Zhang |  |
| 2071 |  |  [Language Models with Rationality](https://doi.org/10.18653/v1/2023.emnlp-main.877) |  | 0 | While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent “beliefs”. This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs... | Ashish Sabharwal, Hinrich Schütze, Kyle Richardson, Nora Kassner, Oyvind Tafjord, Peter Clark |  |
| 2072 |  |  [Self-Improvement of Non-autoregressive Model via Sequence-Level Distillation](https://doi.org/10.18653/v1/2023.emnlp-main.878) |  | 0 | Although Non-autoregressive Transformer (NAT) models have achieved great success in terms of fast inference speed, this speedup comes with a performance drop due to the inherent multi-modality problem of the NAT model. Previous works commonly alleviate this problem by replacing the target side of... | Shuyang Jiang, Yanfeng Wang, Yiqi Li, Yu Wang, Yusheng Liao |  |
| 2073 |  |  [Mitigating Temporal Misalignment by Discarding Outdated Facts](https://doi.org/10.18653/v1/2023.emnlp-main.879) |  | 0 | While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present,... | Eunsol Choi, Michael J. Q. Zhang |  |
| 2074 |  |  [Open-world Semi-supervised Generalized Relation Discovery Aligned in a Real-world Setting](https://doi.org/10.18653/v1/2023.emnlp-main.880) |  | 0 | Open-world Relation Extraction (OpenRE) has recently garnered significant attention. However, existing approaches tend to oversimplify the problem by assuming that all instances of unlabeled data belong to novel classes, thereby limiting the practicality of these methods. We argue that the OpenRE... | Jiacheng Li, Jingbo Shang, William Hogan |  |
| 2075 |  |  [IEKG: A Commonsense Knowledge Graph for Idiomatic Expressions](https://doi.org/10.18653/v1/2023.emnlp-main.881) |  | 0 | Idiomatic expression (IE) processing and comprehension have challenged pre-trained language models (PTLMs) because their meanings are non-compositional. Unlike prior works that enable IE comprehension through fine-tuning PTLMs with sentences containing IEs, in this work, we construct IEKG, a... | Jianing Zhou, Kellen Tan Cheng, Srihari Venkat Nanniyur, Suma Bhat, Ziheng Zeng |  |
| 2076 |  |  [Bias Neutralization in Non-Parallel Texts: A Cyclic Approach with Auxiliary Guidance](https://doi.org/10.18653/v1/2023.emnlp-main.882) |  | 0 | Objectivity is a goal for Wikipedia and many news sites, as well as a guiding principle of many large language models. Indeed, several methods have recently been developed for automatic subjective bias neutralization. These methods, however, typically rely on parallel text for training (i.e. a... | James Caverlee, Karthic Madanagopal |  |
| 2077 |  |  [Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation](https://doi.org/10.18653/v1/2023.emnlp-main.883) |  | 0 | Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (\*.i.e, generating large-scale harmful and misleading content\*). To combat this emerging risk of LLMs, we propose a novel “\*\*\*Fighting Fire with Fire\*\*\*” (F3)... | Adaku Uchendu, Dongwon Lee, Jason Samuel Lucas, Jooyoung Lee, Michiharu Yamashita, Shaurya Rohatgi |  |
| 2078 |  |  [SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts](https://doi.org/10.18653/v1/2023.emnlp-main.884) |  | 0 | Prompt tuning has emerged as a successful parameter-efficient alternative to the full fine-tuning of language models. However, prior works on prompt tuning often utilize long soft prompts of up to 100 tokens to improve performance, overlooking the inefficiency associated with extended inputs. In... | JoonYoung Choi, JunHyung Park, Junho Kim, SangKeun Lee, WingLam Mok |  |
| 2079 |  |  [BRAINTEASER: Lateral Thinking Puzzles for Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.885) |  | 0 | The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To... | Filip Ilievski, Kaixin Ma, Yifan Jiang, Zhivar Sourati |  |
| 2080 |  |  [When are Lemons Purple? The Concept Association Bias of Vision-Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.886) |  | 0 | Large-scale vision-language models such as CLIP have shown impressive performance on zero-shot image classification and image-to-text retrieval. However, such performance does not realize in tasks that require a finer-grained correspondence between vision and language, such as Visual Question... | Ilker Yildirim, Yingtian Tang, Yoyo Zhang, Yutaro Yamada |  |
| 2081 |  |  [What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability](https://doi.org/10.18653/v1/2023.emnlp-main.887) |  | 0 | In Natural Language Generation (NLG) tasks, for any input, multiple communicative goals are plausible, and any goal can be put into words, or produced, in multiple ways. We characterise the extent to which human production varies lexically, syntactically, and semantically across four NLG tasks,... | Barbara Plank, Joris Baan, Mario Giulianelli, Raquel Fernández, Wilker Aziz |  |
| 2082 |  |  [Text Representation Distillation via Information Bottleneck Principle](https://doi.org/10.18653/v1/2023.emnlp-main.888) |  | 0 | Pre-trained language models (PLMs) have recently shown great success in text representation field. However, the high computational cost and high-dimensional representation of PLMs pose significant challenges for practical applications. To make models more accessible, an effective method is to... | Dingkun Long, Pengjun Xie, Yanzhao Zhang, Zehan Li |  |
| 2083 |  |  [Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation](https://doi.org/10.18653/v1/2023.emnlp-main.889) |  | 0 | In this paper, we present a novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models. Our approach is designed to consider the student model’s weaknesses and foster a tailored learning experience by generating... | Ashwin Kalyan, Peter Clark, Tanmay Rajpurohit, Wenhao Yu, Xiangliang Zhang, Zhenwen Liang |  |
| 2084 |  |  [FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions](https://doi.org/10.18653/v1/2023.emnlp-main.890) |  | 0 | Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon... | Gunhee Kim, Hyunwoo Kim, Maarten Sap, Melanie Sclar, Ronan Le Bras, Xuhui Zhou, Yejin Choi |  |
| 2085 |  |  [Exploring the Boundaries of GPT-4 in Radiology](https://doi.org/10.18653/v1/2023.emnlp-main.891) |  | 0 | The recent success of general-domain large language models (LLMs) has significantly changed the natural language processing paradigm towards a unified foundation model across domains and applications. In this paper, we focus on assessing the performance of GPT-4, the most capable LLM so far, on the... | Aditya V. Nori, Anja Thieme, Anton Schwaighofer, Daniel C. Castro, Fernando PérezGarcía, Harshita Sharma, Hoifung Poon, Javier AlvarezValle, Kenza Bouzid, Maria Wetscherek, Matthew P. Lungren, Naoto Usuyama, Ozan Oktay, Pranav Rajpurkar, Qianchu Liu, Robert Tinn, Sameer Tajdin Khanna, Shruthi Bannur, Stephanie L. Hyland |  |
| 2086 |  |  [A Frustratingly Easy Post-Training Quantization Scheme for LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.892) |  | 0 | Efficient inference has become crucial for hyper-scale AI models, including large language models, as their parameter count continues to increase for enhanced performance. This necessity holds true regardless of the computing environment, whether it be mobile devices or cloud servers. Quantization... | Chungman Lee, HoYoung Kim, Kyungphil Park, Yongkweon Jeon |  |
| 2087 |  |  [A Comprehensive Evaluation of Biomedical Entity Linking Models](https://doi.org/10.18653/v1/2023.emnlp-main.893) |  | 0 | Biomedical entity linking (BioEL) is the process of connecting entities referenced in documents to entries in biomedical databases such as the Unified Medical Language System (UMLS) or Medical Subject Headings (MeSH). The study objective was to comprehensively evaluate nine recent state-of-the-art... | Cassie S. Mitchell, Daniel DomingoFernández, David Kartchner, Jennifer Deng, Prasanth Bathala, Shubham Lohiya, Tejasri Kopparthi |  |
| 2088 |  |  [Exploring Jiu-Jitsu Argumentation for Writing Peer Review Rebuttals](https://doi.org/10.18653/v1/2023.emnlp-main.894) |  | 0 | In many domains of argumentation, people’s arguments are driven by so-called attitude roots, i.e., underlying beliefs and world views, and their corresponding attitude themes. Given the strength of these latent drivers of arguments, recent work in psychology suggests that instead of directly... | Anne Lauscher, Iryna Gurevych, Sukannya Purkayastha |  |
| 2089 |  |  [LIMIT: Language Identification, Misidentification, and Translation using Hierarchical Models in 350+ Languages](https://doi.org/10.18653/v1/2023.emnlp-main.895) |  | 0 | Knowing the language of an input text/audio is a necessary first step for using almost every NLP tool such as taggers, parsers, or translation systems. Language identification is a well-studied problem, sometimes even considered solved; in reality, due to lack of data and computational challenges,... | Antonios Anastasopoulos, Md Mahfuz Ibn Alam, Milind Agarwal |  |
| 2090 |  |  [FreeAL: Towards Human-Free Active Learning in the Era of Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.896) |  | 0 | Collecting high-quality labeled data for model training is notoriously time-consuming and labor-intensive for various NLP tasks. While copious solutions, such as active learning for small language models (SLMs) and prevalent in-context learning in the era of large language models (LLMs), have been... | Gang Chen, Haobo Wang, Junbo Zhao, Minmin Lin, Ruixuan Xiao, Runze Wu, Yiwen Dong |  |
| 2091 |  |  [API-Assisted Code Generation for Question Answering on Varied Table Structures](https://doi.org/10.18653/v1/2023.emnlp-main.897) |  | 0 | A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation... | Daniel Fried, Ryan Liu, Shuyi Chen, Yihan Cao, Zhiruo Wang |  |
| 2092 |  |  [Data Factors for Better Compositional Generalization](https://doi.org/10.18653/v1/2023.emnlp-main.898) |  | 0 | Recent diagnostic datasets on compositional generalization, such as SCAN (Lake and Baroni, 2018) and COGS (Kim and Linzen, 2020), expose severe problems in models trained from scratch on these datasets. However, in contrast to this poor performance, state-of-the-art models trained on larger and... | Mohit Bansal, Xiang Zhou, Yichen Jiang |  |
| 2093 |  |  [ChatEdit: Towards Multi-turn Interactive Facial Image Editing via Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.899) |  | 0 | This paper explores interactive facial image editing through dialogue and presents the ChatEdit benchmark dataset for evaluating image editing and conversation abilities in this context. ChatEdit is constructed from the CelebA-HQ dataset, incorporating annotated multi-turn dialogues corresponding... | Chunshui Cao, Hailin Shi, Pei Li, Xing Cui, Yibo Hu, Zekun Li, Zhaofeng He |  |
| 2094 |  |  [Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations](https://doi.org/10.18653/v1/2023.emnlp-main.900) |  | 0 | Traditional sentence embedding models encode sentences into vector representations to capture useful properties such as the semantic similarity between sentences. However, in addition to similarity, sentence semantics can also be interpreted via compositional operations such as sentence fusion or... | Dong Yu, Hongming Zhang, James Y. Huang, Kaiqiang Song, Muhao Chen, Wenlin Yao |  |
| 2095 |  |  [Outlier Dimensions Encode Task Specific Knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.901) |  | 0 | Representations from large language models (LLMs) are known to be dominated by a small subset of dimensions with exceedingly high variance. Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are... | Carsten Eickhoff, Catherine Chen, William Rudman |  |
| 2096 |  |  [Hi-ArG: Exploring the Integration of Hierarchical Argumentation Graphs in Language Pretraining](https://doi.org/10.18653/v1/2023.emnlp-main.902) |  | 0 | The knowledge graph is a structure to store and represent knowledge, and recent studies have discussed its capability to assist language models for various applications. Some variations of knowledge graphs aim to record arguments and their relations for computational argumentation tasks. However,... | Jingcong Liang, Meng Han, Qi Zhang, Rong Ye, Ruofei Lai, Xinyu Zhang, Xuanjing Huang, Zhao Cao, Zhongyu Wei |  |
| 2097 |  |  [Biomedical Named Entity Recognition via Dictionary-based Synonym Generalization](https://doi.org/10.18653/v1/2023.emnlp-main.903) |  | 0 | Biomedical named entity recognition is one of the core tasks in biomedical natural language processing (BioNLP). To tackle this task, numerous supervised/distantly supervised approaches have been proposed. Despite their remarkable success, these approaches inescapably demand laborious human effort.... | Nigel Collier, Yixuan Su, Zaiqiao Meng, Zihao Fu |  |
| 2098 |  |  [GNAT: A General Narrative Alignment Tool](https://doi.org/10.18653/v1/2023.emnlp-main.904) |  | 0 | Algorithmic sequence alignment identifies similar segments shared between pairs of documents, and is fundamental to many NLP tasks. But it is difficult to recognize similarities between distant versions of narratives such as translations and retellings, particularly for summaries and abridgements... | Steven Skiena, Tanzir Pial |  |
| 2099 |  |  [Self-Ensemble of N-best Generation Hypotheses by Lexically Constrained Decoding](https://doi.org/10.18653/v1/2023.emnlp-main.905) |  | 0 | We propose a method that ensembles N-best hypotheses to improve natural language generation. Previous studies have achieved notable improvements in generation quality by explicitly reranking N-best candidates. These studies assume that there exists a hypothesis of higher quality. We expand the... | Ryota Miyano, Tomoyuki Kajiwara, Yuki Arase |  |
| 2100 |  |  [UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.906) |  | 0 | Charts are widely used for data analysis, providing visual representations and insights into complex data. To facilitate chart-based data analysis using natural language, several downstream tasks have been introduced recently such as chart question answering and chart summarization. However,... | Ahmed Masry, Do Xuan Long, Enamul Hoque, Parsa Kavehzadeh, Shafiq Joty |  |
| 2101 |  |  [Merging Experts into One: Improving Computational Efficiency of Mixture of Experts](https://doi.org/10.18653/v1/2023.emnlp-main.907) |  | 0 | Scaling the size of language models usually leads to remarkable advancements in NLP tasks. But it often comes with a price of growing computational cost. Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a small subset of parameters (e.g., one expert) for each input, its... | Dacheng Tao, Li Shen, Liang Ding, RunZe Fan, Shwai He, Tianyi Zhou |  |
| 2102 |  |  [Distance-Based Propagation for Efficient Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2023.emnlp-main.908) |  | 0 | Knowledge graph completion (KGC) aims to predict unseen edges in knowledge graphs (KGs), resulting in the discovery of new facts. A new class of methods have been proposed to tackle this problem by aggregating path information. These methods have shown tremendous ability in the task of KGC. However... | Bo Wu, Charu C. Aggarwal, Harry Shomer, Jiliang Tang, Juanhui Li, Yao Ma |  |
| 2103 |  |  [What to Read in a Contract? Party-Specific Summarization of Legal Obligations, Entitlements, and Prohibitions](https://doi.org/10.18653/v1/2023.emnlp-main.909) |  | 0 | Reviewing and comprehending key obligations, entitlements, and prohibitions in legal contracts can be a tedious task due to their length and domain-specificity. Furthermore, the key rights and duties requiring review vary for each contracting party. In this work, we propose a new task of... | Abhilasha Sancheti, Aparna Garimella, Balaji Vasan Srinivasan, Rachel Rudinger |  |
| 2104 |  |  [Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization](https://doi.org/10.18653/v1/2023.emnlp-main.910) |  | 0 | Large Language Models (LLMs) are proficient in natural language processing tasks, but their deployment is often restricted by extensive parameter sizes and computational demands. This paper focuses on post-training quantization (PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8)... | Janghwan Lee, Jungwook Choi, Minsoo Kim, Seok Joong Hwang, Seungcheol Baek, Wonyong Sung |  |
| 2105 |  |  [CP-BCS: Binary Code Summarization Guided by Control Flow Graph and Pseudo Code](https://doi.org/10.18653/v1/2023.emnlp-main.911) |  | 0 | Automatically generating function summaries for binaries is an extremely valuable but challenging task, since it involves translating the execution behavior and semantics of the low-level language (assembly code) into human-readable natural language. However, most current works on understanding... | Lingfei Wu, Peiyu Liu, Shouling Ji, Tengfei Ma, Tong Ye, Wenhai Wang, Xuhong Zhang, Yangkai Du |  |
| 2106 |  |  [Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism](https://doi.org/10.18653/v1/2023.emnlp-main.912) |  | 0 | Large language models (LLMs) take advantage of step-by-step reasoning instructions, e.g., chain-of-thought (CoT) prompting. Building on this, their ability to perform CoT-style reasoning robustly is of interest from a probing perspective. In this study, we inspect the step-by-step reasoning ability... | Goro Kobayashi, Hiroaki Funayama, Jun Suzuki, Mengyu Ye, Tatsuki Kuribayashi |  |
| 2107 |  |  [Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.913) |  | 0 | Chain-of-Thought (CoT) is a technique that guides Large Language Models (LLMs) to decompose complex tasks into multi-step reasoning through intermediate steps in natural language form. Briefly, CoT enables LLMs to think step by step. However, although many Natural Language Understanding (NLU) tasks... | Caoyun Fan, Hao He, Jidong Tian, Wenqing Chen, Yaohui Jin, Yitian Li |  |
| 2108 |  |  [Large Language Models are Complex Table Parsers](https://doi.org/10.18653/v1/2023.emnlp-main.914) |  | 0 | With the Generative Pre-trained Transformer 3.5 (GPT-3.5) exhibiting remarkable reasoning and comprehension abilities in Natural Language Processing (NLP), most Question Answering (QA) research has primarily centered around general QA tasks based on GPT, neglecting the specific challenges posed by... | Bowen Zhao, Changkai Ji, Qing Wang, Rui Feng, Wen He, Xiaobo Zhang, Yingwen Wang, Yuejie Zhang |  |
| 2109 |  |  [R2H: Building Multimodal Navigation Helpers that Respond to Help Requests](https://doi.org/10.18653/v1/2023.emnlp-main.915) |  | 0 | Intelligent navigation-helper agents are critical as they can navigate users in unknown areas through environmental awareness and conversational ability, serving as potential accessibility tools for individuals with disabilities. In this work, we first introduce a novel benchmark, Respond to Help... | Jing Gu, Kaizhi Zheng, Xin Wang, Yue Fan |  |
| 2110 |  |  [Speech-enriched Memory for Inference-time Adaptation of ASR Models to Word Dictionaries](https://doi.org/10.18653/v1/2023.emnlp-main.916) |  | 0 | Despite the impressive performance of ASR models on mainstream benchmarks, their performance on rare words is unsatisfactory. In enterprise settings, often a focused list of entities (such as locations, names, etc) are available which can be used to adapt the model to the terminology of specific... | Ashish R. Mittal, Gakuto Kurata, George Saon, Preethi Jyothi, Sunita Sarawagi |  |
| 2111 |  |  [Generative Table Pre-training Empowers Models for Tabular Prediction](https://doi.org/10.18653/v1/2023.emnlp-main.917) |  | 0 | Recently, the topic of table pre-training has attracted considerable research interest. However, how to employ table pre-training to boost the performance of tabular prediction remains an open challenge. In this paper, we propose TapTap, the first attempt that leverages table pre-training to... | Li Jian, Qian Liu, Shaowen Wang, Shuicheng Yan, Tianping Zhang |  |
| 2112 |  |  [Learning to Describe for Predicting Zero-shot Drug-Drug Interactions](https://doi.org/10.18653/v1/2023.emnlp-main.918) |  | 0 | Adverse drug-drug interactions (DDIs) can compromise the effectiveness of concurrent drug administration, posing a significant challenge in healthcare. As the development of new drugs continues, the potential for unknown adverse effects resulting from DDIs becomes a growing concern. Traditional... | Bing Qin, Fangqi Zhu, Lei Chen, Ruifeng Xu, Yongqi Zhang |  |
| 2113 |  |  [A Simple Baseline for Knowledge-Based Visual Question Answering](https://doi.org/10.18653/v1/2023.emnlp-main.919) |  | 0 | This paper is on the problem of Knowledge-Based Visual Question Answering (KB-VQA). Recent works have emphasized the significance of incorporating both explicit (through external databases) and implicit (through LLMs) knowledge to answer questions requiring external knowledge effectively. A common... | Alexandros Xenos, Georgios Tzimiropoulos, Ioannis Patras, Themos Stafylakis |  |
| 2114 |  |  [Unveiling the Essence of Poetry: Introducing a Comprehensive Dataset and Benchmark for Poem Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.920) |  | 0 | While research in natural language processing has progressed significantly in creative language generation, the question of whether language models can interpret the intended meaning of creative language largely remains unanswered. Poetry as a creative art form has existed for generations, and... | Ifrad Khan, Md Shahriar, Md. Tahmid Rahman Laskar, Ridwan Mahbub, Sabbir Ahmed, Samiha Anuva |  |
| 2115 |  |  [Privacy Implications of Retrieval-Based Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.921) |  | 0 | Retrieval-based language models (LMs) have demonstrated improved interpretability, factuality, and adaptability compared to their parametric counterparts by incorporating retrieved text from external datastores. While it is well known that parametric models are prone to leaking private data, it... | Danqi Chen, Kai Li, Samyak Gupta, Yangsibo Huang, Zexuan Zhong |  |
| 2116 |  |  [IMTLab: An Open-Source Platform for Building, Evaluating, and Diagnosing Interactive Machine Translation Systems](https://doi.org/10.18653/v1/2023.emnlp-main.922) |  | 0 | We present IMTLab, an open-source end-to-end interactive machine translation (IMT) system platform that enables researchers to quickly build IMT systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. IMTLab treats the whole interactive... | Guoping Huang, Jiajun Chen, Lemao Liu, Ruize Gao, Shujian Huang, Shuming Shi, Xu Huang, Yichao Du, Zhirui Zhang |  |
| 2117 |  |  [Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents](https://doi.org/10.18653/v1/2023.emnlp-main.923) |  | 0 | Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy... | Dawei Yin, Lingyong Yan, Pengjie Ren, Shuaiqiang Wang, Weiwei Sun, Xinyu Ma, Zhaochun Ren, Zhumin Chen |  |
| 2118 |  |  [DiNeR: A Large Realistic Dataset for Evaluating Compositional Generalization](https://doi.org/10.18653/v1/2023.emnlp-main.924) |  | 0 | Most of the existing compositional generalization datasets are synthetically-generated, resulting in a lack of natural language variation. While there have been recent attempts to introduce non-synthetic datasets for compositional generalization, they suffer from either limited data scale or a lack... | Chengang Hu, Xiao Liu, Yansong Feng |  |
| 2119 |  |  [Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?](https://doi.org/10.18653/v1/2023.emnlp-main.925) |  | 0 | Pre-trained vision and language models have demonstrated state-of-the-art capabilities over existing tasks involving images and texts, including visual question answering. However, it remains unclear whether these models possess the capability to answer questions that are not only querying visual... | Alan Ritter, Haitian Sun, Hexiang Hu, MingWei Chang, Soravit Changpinyo, Yang Chen, Yi Luan |  |
| 2120 |  |  [EDeR: Towards Understanding Dependency Relations Between Events](https://doi.org/10.18653/v1/2023.emnlp-main.926) |  | 0 | Relation extraction is a crucial task in natural language processing (NLP) and information retrieval (IR). Previous work on event relation extraction mainly focuses on hierarchical, temporal and causal relations. Such relationships consider two events to be independent in terms of syntax and... | Leyang Cui, Patrik Haslum, Ruiqi Li |  |
| 2121 |  |  [It Ain't Over: A Multi-aspect Diverse Math Word Problem Dataset](https://doi.org/10.18653/v1/2023.emnlp-main.927) |  | 0 | The math word problem (MWP) is a complex task that requires natural language understanding and logical reasoning to extract key knowledge from natural language narratives. Previous studies have provided various MWP datasets but lack diversity in problem types, lexical usage patterns, languages, and... | Ilwoong Baek, JinYeong Bak, Jiwoo Kim, Jongwuk Lee, Youngbin Kim |  |
| 2122 |  |  [Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness](https://doi.org/10.18653/v1/2023.emnlp-main.928) |  | 0 | This paper investigates the significant impact different prompts have on the behaviour of ChatGPT when used for health information seeking. As people more and more depend on generative large language models (LLMs) like ChatGPT, it is critical to understand model behaviour under different... | Bevan Koopman, Guido Zuccon |  |
| 2123 |  |  [kNN-LM Does Not Improve Open-ended Text Generation](https://doi.org/10.18653/v1/2023.emnlp-main.929) |  | 0 | In this paper, we study the generation quality of interpolation-based retrieval-augmented language models (LMs). These methods, best exemplified by the kNN-LM, interpolate the LM’s predicted distribution of the next word with a distribution formed from the most relevant retrievals for a given... | Andrew Drozdov, Aparna Garimella, Mohit Iyyer, Shufan Wang, Varun Manjunatha, Yixiao Song |  |
| 2124 |  |  [Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model](https://doi.org/10.18653/v1/2023.emnlp-main.930) |  | 0 | Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for pretraining large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while... | Tim Dettmers, Veselin Stoyanov, Xi Lin, Xian Li, Zeyu Liu |  |
| 2125 |  |  [Exploring the Impact of Model Scaling on Parameter-Efficient Tuning](https://doi.org/10.18653/v1/2023.emnlp-main.931) |  | 0 | Parameter-efficient tuning (PET) methods can effectively drive extremely large pre-trained language models (PLMs) by training only minimal parameters. Different PET methods utilize different manually designed tunable modules. In small PLMs, there are usually noticeable performance differences among... | ChiMin Chan, Guotong Xie, Jiali Cheng, Maosong Sun, Ning Ding, Shengding Hu, Xingzhi Sun, Yankai Lin, Yujia Qin, Yusheng Su, Zhiyuan Liu, Zonghan Yang |  |
| 2126 |  |  [STAIR: Learning Sparse Text and Image Representation in Grounded Tokens](https://doi.org/10.18653/v1/2023.emnlp-main.932) |  | 0 | Image and text retrieval is one of the foundational tasks in the vision and language domain with multiple real-world applications. State-of-the-art contrastive approaches, e.g. CLIP, ALIGN, represent images and texts as dense embeddings and calculate the similarity in the dense embedding space as... | Albin Madappally Jose, Alexander Toshev, Bowen Zhang, Chen Chen, Jiguang Shen, Liangliang Cao, Ruoming Pang, Tom Gunter, Yantao Zheng, Yinfei Yang |  |
| 2127 |  |  [Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting](https://doi.org/10.18653/v1/2023.emnlp-main.933) |  | 0 | Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization... | Aditi Chaudhary, Emmy Liu, Graham Neubig |  |
| 2128 |  |  [CoRec: An Easy Approach for Coordination Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.934) |  | 0 | In this paper, we observe and address the challenges of the coordination recognition task. Most existing methods rely on syntactic parsers to identify the coordinators in a sentence and detect the coordination boundaries. However, state-of-the-art syntactic parsers are slow and suffer from errors,... | Haojie Jia, Qi Li, Qing Wang, Wenfei Song |  |
| 2129 |  |  [A linear time approximation of Wasserstein distance with word embedding selection](https://doi.org/10.18653/v1/2023.emnlp-main.935) |  | 0 | Wasserstein distance, which can be computed by solving the optimal transport problem, is a powerful method for measuring the dissimilarity between documents. In the NLP community, it is referred to as word mover’s distance (WMD). One of the key challenges of Wasserstein distance is its... | Makoto Yamada, Sho Otao |  |
| 2130 |  |  [Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication](https://doi.org/10.18653/v1/2023.emnlp-main.936) |  | 0 | Large Language Models (LLMs) have recently made significant strides in complex reasoning tasks through the Chain-of-Thought technique. Despite this progress, their reasoning is often constrained by their intrinsic understanding, lacking external insights. To address this, we propose... | Cheng Chang, Junqi Dai, Qipeng Guo, Qiushi Sun, Xipeng Qiu, Xuanjing Huang, Zhangyue Yin |  |
| 2131 |  |  [Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction](https://doi.org/10.18653/v1/2023.emnlp-main.937) |  | 0 | Emotion recognition is a crucial task for human conversation understanding. It becomes more challenging with the notion of multimodal data, e.g., language, voice, and facial expressions. As a typical solution, the global- and the local context information are exploited to predict the emotional... | AnhTuan Mai, CamVan Thi Nguyen, DucTrong Le, HaiDang Kieu, TheSon Le |  |
| 2132 |  |  [Connecting degree and polarity: An artificial language learning study](https://doi.org/10.18653/v1/2023.emnlp-main.938) |  | 0 | We investigate a new linguistic generalisation in pre-trained language models (taking BERT Devlin et al. 2019 as a case study). We focus on degree modifiers (expressions like slightly, very, rather, extremely) and test the hypothesis that the degree expressed by a modifier (low, medium or high... | Alexey Tikhonov, Ekaterina Garmash, Lisa Bylinina |  |
| 2133 |  |  [Prompting with Pseudo-Code Instructions](https://doi.org/10.18653/v1/2023.emnlp-main.939) |  | 0 | Prompting with natural language instructions has recently emerged as a popular method of harnessing the capabilities of large language models (LLM). Given the inherent ambiguity present in natural language, it is intuitive to consider the possible advantages of prompting with less ambiguous prompt... | Danish Contractor, Mayank Mishra, Prince Kumar, Riyaz A. Bhat, Rudra Murthy V, Srikanth Tamilselvam |  |
| 2134 |  |  [CRAB: Assessing the Strength of Causal Relationships Between Real-world Events](https://doi.org/10.18653/v1/2023.emnlp-main.940) |  | 0 | Understanding narratives requires reasoning about the cause-and-effect relationships between events mentioned in the text. While existing foundation models yield impressive results in many NLP tasks requiring reasoning, it is unclear whether they understand the complexity of the underlying network... | Angelika Romanou, Antoine Bosselut, Debjit Paul, Karl Aberer, Léo Laugier, Syrielle Montariol |  |
| 2135 |  |  [NORMSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly](https://doi.org/10.18653/v1/2023.emnlp-main.941) |  | 0 | Knowledge of norms is needed to understand and reason about acceptable behavior in human communication and interactions across sociocultural scenarios. Most computational research on norms has focused on a single culture, and manually built datasets, from non-conversational settings. We address... | Hao Guo, Heng Ji, Owen Rambow, Smaranda Muresan, Tuhin Chakrabarty, Yi Fung |  |
| 2136 |  |  [A State-Vector Framework for Dataset Effects](https://doi.org/10.18653/v1/2023.emnlp-main.942) |  | 0 | The impressive success of recent deep neural network (DNN)-based systems is significantly influenced by the high-quality datasets used in training. However, the effects of the datasets, especially how they interact with each other, remain underexplored. We propose a state-vector framework to enable... | Esmat Sahak, Frank Rudzicz, Zining Zhu |  |
| 2137 |  |  [Challenges in Context-Aware Neural Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.943) |  | 0 | Context-aware neural machine translation, a paradigm that involves leveraging information beyond sentence-level context to resolve inter-sentential discourse dependencies and improve document-level translation quality, has given rise to a number of recent techniques. However, despite well-reasoned... | Jacqueline He, Jonathan May, Linghao Jin, Xuezhe Ma |  |
| 2138 |  |  [Task-Adaptive Tokenization: Enhancing Long-Form Text Generation Efficacy in Mental Health and Beyond](https://doi.org/10.18653/v1/2023.emnlp-main.944) |  | 0 | We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes,... | Minlie Huang, Naihao Deng, Rada Mihalcea, Sahand Sabour, Siyang Liu, Yilin Jia |  |
| 2139 |  |  [FACTIFY3M: A benchmark for multimodal fact verification with explainability through 5W Question-Answering](https://doi.org/10.18653/v1/2023.emnlp-main.945) |  | 0 | Combating disinformation is one of the burning societal crises - about 67% of the American population believes that disinformation produces a lot of uncertainty, and 10% of them knowingly propagate disinformation. Evidence shows that disinformation can manipulate democratic processes and public... | Adarsh Mahor, Aditya Pakala, Aman Chadha, Amit P. Sheth, Amitava Das, Anku Rani, Arghya Sarkar, Dwip Dalal, Harshit Dave, Ishan Paul, Janvita Reddy, Khushbu Pahwa, Kinjal Sensharma, Megha Chakraborty, Preethi Gurumurthy, Ritvik G, Samahriti Mukherjee, Shreyas Chatterjee |  |
| 2140 |  |  [Building Multi-domain Dialog State Trackers from Single-domain Dialogs](https://doi.org/10.18653/v1/2023.emnlp-main.946) |  | 0 | Existing multi-domain dialog state tracking (DST) models are developed based on multi-domain dialogs, which require significant manual effort to define domain relations and collect data. This process can be challenging and expensive, particularly when numerous domains are involved. In this paper,... | Minlie Huang, Qi Zhu, Xiaoyan Zhu, Zheng Zhang |  |
| 2141 |  |  [Specialist or Generalist? Instruction Tuning for Specific NLP Tasks](https://doi.org/10.18653/v1/2023.emnlp-main.947) |  | 0 | The potential of large language models (LLMs) to simultaneously perform a wide range of natural language processing (NLP) tasks has been the subject of extensive research. Although instruction tuning has proven to be a data-efficient method for transforming LLMs into such generalist models, their... | Cheng Yang, Chufan Shi, Deng Cai, Yixuan Su, Yujiu Yang |  |
| 2142 |  |  [Making Large Language Models Better Data Creators](https://doi.org/10.18653/v1/2023.emnlp-main.948) |  | 0 | Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the preferred option in... | DongHo Lee, Jay Pujara, Mohit Sewak, Ryen White, Sujay Kumar Jauhar |  |
| 2143 |  |  [Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation](https://doi.org/10.18653/v1/2023.emnlp-main.949) |  | 0 | Large Language Models (LLMs) have made remarkable advancements in the field of natural language generation. However, the propensity of LLMs to generate inaccurate or non-factual content, termed “hallucinations”, remains a significant challenge. Current hallucination detection methods often... | Longtao Huang, Xiaohua Wang, Xiaoqing Zheng, Xuanjing Huang, Yuliang Yan |  |
| 2144 |  |  [Guideline Learning for In-Context Information Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.950) |  | 0 | Large language models (LLMs) can perform a new task by merely conditioning on task instructions and a few input-output examples, without optimizing any parameters. This is called In-Context Learning (ICL). In-context Information Extraction (IE) has recently garnered attention in the research... | Chaoxu Pang, Ping Luo, Qiang Ding, Yixuan Cao |  |
| 2145 |  |  [Open Information Extraction via Chunks](https://doi.org/10.18653/v1/2023.emnlp-main.951) |  | 0 | Open Information Extraction (OIE) aims to extract relational tuples from open-domain sentences. Existing OIE systems split a sentence into tokens and recognize token spans as tuple relations and arguments. We instead propose Sentence as Chunk sequence (SaC) and recognize chunk spans as tuple... | Aixin Sun, JungJae Kim, Kuicai Dong, Xiaoli Li |  |
| 2146 |  |  [Rethinking Word-Level Auto-Completion in Computer-Aided Translation](https://doi.org/10.18653/v1/2023.emnlp-main.952) |  | 0 | Word-level auto-completion (WLAC) plays a crucial role in Computer-Assisted Translation. While previous studies have primarily focused on designing complex model architectures, this paper takes a different perspective by rethinking the fundamental question: what kind of words are good... | Guoping Huang, Lemao Liu, Mingming Yang, Rui Wang, Shuming Shi, Xingyu Chen, Zhirui Zhang |  |
| 2147 |  |  [Automatic Transcription of Handwritten Old Occitan Language](https://doi.org/10.18653/v1/2023.emnlp-main.953) |  | 0 | While existing neural network-based approaches have shown promising results in Handwritten Text Recognition (HTR) for high-resource languages and standardized/machine-written text, their application to low-resource languages often presents challenges, resulting in reduced effectiveness. In this... | Christian Heumann, Esteban Garces Arias, Matthias Aßenmacher, Matthias Schöffel, Vallari Pai |  |
| 2148 |  |  [CorefPrompt: Prompt-based Event Coreference Resolution by Measuring Event Type and Argument Compatibilities](https://doi.org/10.18653/v1/2023.emnlp-main.954) |  | 0 | Event coreference resolution (ECR) aims to group event mentions referring to the same real-world event into clusters. Most previous studies adopt the “encoding first, then scoring” framework, making the coreference judgment rely on event encoding. Furthermore, current methods struggle to leverage... | Peifeng Li, Qiaoming Zhu, Sheng Xu |  |
| 2149 |  |  [Anaphor Assisted Document-Level Relation Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.955) |  | 0 | Document-level relation extraction (DocRE) involves identifying relations between entities distributed in multiple sentences within a document. Existing methods focus on building a heterogeneous document graph to model the internal structure of an entity and the external interaction between... | Chonggang Lu, Cunwang Zhang, Jaein Kim, Kai Sun, Richong Zhang, Yongyi Mao |  |
| 2150 |  |  [FinEntity: Entity-level Sentiment Classification for Financial Texts](https://doi.org/10.18653/v1/2023.emnlp-main.956) |  | 0 | In the financial domain, conducting entity-level sentiment analysis is crucial for accurately assessing the sentiment directed toward a specific financial entity. To our knowledge, no publicly available dataset currently exists for this purpose. In this work, we introduce an entity-level sentiment... | Allen Huang, Andy Tam, Justin Z. Tang, Yi Yang, Yixuan Tang |  |
| 2151 |  |  [All Things Considered: Detecting Partisan Events from News Media with Cross-Article Comparison](https://doi.org/10.18653/v1/2023.emnlp-main.957) |  | 0 | Public opinion is shaped by the information news media provide, and that information in turn may be shaped by the ideological preferences of media outlets. But while much attention has been devoted to media bias via overt ideological language or topic selection, a more unobtrusive way in which the... | Kaijian Zou, Lu Wang, Nicholas Beauchamp, Ruihong Huang, Xinliang Frederick Zhang, Yujian Liu |  |
| 2152 |  |  [Rationale-Enhanced Language Models are Better Continual Relation Learners](https://doi.org/10.18653/v1/2023.emnlp-main.958) |  | 0 | Continual relation extraction (CRE) aims to solve the problem of catastrophic forgetting when learning a sequence of newly emerging relations. Recent CRE studies have found that catastrophic forgetting arises from the model’s lack of robustness against future analogous relations. To address the... | Peiyi Wang, Sujian Li, Weimin Xiong, Yifan Song |  |
| 2153 |  |  [BanglaAbuseMeme: A Dataset for Bengali Abusive Meme Classification](https://doi.org/10.18653/v1/2023.emnlp-main.959) |  | 0 | The dramatic increase in the use of social media platforms for information sharing has also fueled a steep growth in online abuse. A simple yet effective way of abusing individuals or communities is by creating memes, which often integrate an image with a short piece of text layered on top of it.... | Animesh Mukherjee, Mithun Das |  |
| 2154 |  |  [ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts](https://doi.org/10.18653/v1/2023.emnlp-main.960) |  | 0 | Eye movements in reading play a crucial role in psycholinguistic research studying the cognitive mechanisms underlying human language processing. More recently, the tight coupling between eye movements and cognition has also been leveraged for language-related machine learning tasks such as the... | David R. Reich, Deborah N. Jakobi, Lena A. Jäger, Lena S. Bolliger, Patrick Haller, Paul Prasse |  |
| 2155 |  |  [From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.961) |  | 0 | Being able to predict people’s opinions on issues and behaviors in realistic scenarios can be helpful in various domains, such as politics and marketing. However, conducting large-scale surveys like the European Social Survey to solicit people’s opinions on individual issues can incur prohibitive... | Dongjun Kang, JinYeong Bak, Joonsuk Park, Yohan Jo |  |
| 2156 |  |  [Analyzing Film Adaptation through Narrative Alignment](https://doi.org/10.18653/v1/2023.emnlp-main.962) |  | 0 | Novels are often adapted into feature films, but the differences between the two media usually require dropping sections of the source text from the movie script. Here we study this screen adaptation process by constructing narrative alignments using the Smith-Waterman local alignment algorithm... | Allen Kim, Charuta Pethe, Shahreen Salim Aunti, Steven Skiena, Tanzir Pial |  |
| 2157 |  |  [Inverse Scaling Can Become U-Shaped](https://doi.org/10.18653/v1/2023.emnlp-main.963) |  | 0 | Scaling up language models has been empirically shown to improve performance on a wide range of downstream tasks. However, if we were to observe worse performance as a function of scale (inverse scaling) on certain tasks, this would indicate that scaling can also encourage behaviors that are... | Jason Wei, Najoung Kim, Quoc V. Le, Yi Tay |  |
| 2158 |  |  [Nearest Neighbor Machine Translation is Meta-Optimizer on Output Projection Layer](https://doi.org/10.18653/v1/2023.emnlp-main.964) |  | 0 | Nearest Neighbor Machine Translation (kNN-MT) has achieved great success in domain adaptation tasks by integrating pre-trained Neural Machine Translation (NMT) models with domain-specific token-level retrieval. However, the reasons underlying its success have not been thoroughly investigated. In... | Lemao Liu, Rui Wang, Ruize Gao, Yichao Du, Zhirui Zhang |  |
| 2159 |  |  [Variance Matters: Detecting Semantic Differences without Corpus/Word Alignment](https://doi.org/10.18653/v1/2023.emnlp-main.965) |  | 0 | In this paper, we propose methods for discovering semantic differences in words appearing in two corpora. The key idea is to measure the coverage of meanings of a word in a corpus through the norm of its mean word vector, which is equivalent to examining a kind of variance of the word vector... | Hiroya Takamura, Naoki Otani, Ryo Nagata, Yoshifumi Kawasaki |  |
| 2160 |  |  [MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter](https://doi.org/10.18653/v1/2023.emnlp-main.966) |  | 0 | Language Models (LMs) have demonstrated impressive molecule understanding ability on various 1D text-related tasks. However, they inherently lack 2D graph perception — a critical ability of human professionals in comprehending molecules’ topological structures. To bridge this gap, we propose MolCA:... | Hao Fei, Kenji Kawaguchi, Sihang Li, TatSeng Chua, Xiang Wang, Yanchen Luo, Yixin Cao, Zhiyuan Liu |  |
| 2161 |  |  [A Training-Free Debiasing Framework with Counterfactual Reasoning for Conversational Emotion Detection](https://doi.org/10.18653/v1/2023.emnlp-main.967) |  | 0 | Unintended dataset biases typically exist in existing Emotion Recognition in Conversations (ERC) datasets, including label bias, where models favor the majority class due to imbalanced training data, as well as the speaker and neutral word bias, where models make unfair predictions because of... | Bin Liang, Geng Tu, KamFai Wong, Min Yang, Ran Jing, Ruifeng Xu |  |
| 2162 |  |  [Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations](https://doi.org/10.18653/v1/2023.emnlp-main.968) |  | 0 | Large language models (LLMs) have exhibited striking in-context learning (ICL) ability to adapt to target tasks with a few input-output demonstrations. For better ICL, different methods are proposed to select representative demonstrations from existing training corpora. However, such settings are... | ChengKuang Wu, HsinHsi Chen, WeiLin Chen, YunNung Chen |  |
| 2163 |  |  [Learning Knowledge-Enhanced Contextual Language Representations for Domain Natural Language Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.969) |  | 0 | Knowledge-Enhanced Pre-trained Language Models (KEPLMs) improve the performance of various downstream NLP tasks by injecting knowledge facts from large-scale Knowledge Graphs (KGs). However, existing methods for pre-training KEPLMs with relational triples are difficult to be adapted to close... | Cen Chen, Chengyu Wang, Dawei Cheng, Minghui Qiu, Ruyao Xu, Taolin Zhang, Weining Qian, Xiaofeng He, Zhongjie Duan |  |
| 2164 |  |  [ScdNER: Span-Based Consistency-Aware Document-Level Named Entity Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.970) |  | 0 | Document-level NER approaches use global information via word-based key-value memory for accurate and consistent predictions. However, such global information on word level can introduce noise when the same word appears in different token sequences and has different labels. This work proposes a... | Qi Li, Ying Wei |  |
| 2165 |  |  [MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions](https://doi.org/10.18653/v1/2023.emnlp-main.971) |  | 0 | The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited,... | Christopher D. Manning, Christopher Potts, Danqi Chen, Zexuan Zhong, Zhengxuan Wu |  |
| 2166 |  |  [Stance Detection on Social Media with Background Knowledge](https://doi.org/10.18653/v1/2023.emnlp-main.972) |  | 0 | Identifying users’ stances regarding specific targets/topics is a significant route to learning public opinion from social media platforms. Most existing studies of stance detection strive to learn stance information about specific targets from the context, in order to determine the user’s stance... | Ang Li, Bin Liang, Bowen Zhang, Jingqian Zhao, Min Yang, Ruifeng Xu |  |
| 2167 |  |  [Vision-Enhanced Semantic Entity Recognition in Document Images via Visually-Asymmetric Consistency Learning](https://doi.org/10.18653/v1/2023.emnlp-main.973) |  | 0 | Extracting meaningful entities belonging to predefined categories from Visually-rich Form-like Documents (VFDs) is a challenging task. Visual and layout features such as font, background, color, and bounding box location and size provide important cues for identifying entities of the same type.... | Chenhui Chu, Hao Wang, Rui Wang, Xiahua Chen |  |
| 2168 |  |  [NormDial: A Comparable Bilingual Synthetic Dialog Dataset for Modeling Social Norm Adherence and Violation](https://doi.org/10.18653/v1/2023.emnlp-main.974) |  | 0 | Social norms fundamentally shape interpersonal communication. We present NormDial, a high-quality dyadic dialogue dataset with turn-by-turn annotations of social norm adherences and violations for Chinese and American cultures. Introducing the task of social norm observance detection, our dataset... | Arkadiy Saakyan, Mallika Subramanian, Oliver Li, Sky CHWang, Smaranda Muresan |  |
| 2169 |  |  [ClimateBERT-NetZero: Detecting and Assessing Net Zero and Reduction Targets](https://doi.org/10.18653/v1/2023.emnlp-main.975) |  | 0 | Public and private actors struggle to assess the vast amounts of information about sustainability commitments made by various institutions. To address this problem, we create a novel tool for automatically detecting corporate and national net zero and reduction targets in three steps. First, we... | Camilla Hyslop, Julia Anna Bingler, Markus Leippold, Mathias Kraus, Tobias Schimanski |  |
| 2170 |  |  [Leap-of-Thought: Accelerating Transformers via Dynamic Token Routing](https://doi.org/10.18653/v1/2023.emnlp-main.976) |  | 0 | Computational inefficiency in transformers has been a long-standing challenge, hindering the deployment in resource-constrained or real-time applications. One promising approach to mitigate this limitation is to progressively remove less significant tokens, given that the sequence length strongly... | JunHyung Park, Junho Kim, Mingyu Lee, SangKeun Lee, Yeachan Kim |  |
| 2171 |  |  [Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning](https://doi.org/10.18653/v1/2023.emnlp-main.977) |  | 0 | Query-focused Summarization (QfS) deals with systems that generate summaries from document(s) based on a query. Motivated by the insight that Reinforcement Learning (RL) provides a generalization to Supervised Learning (SL) for Natural Language Generation, and thereby performs better (empirically)... | Harshad Khadilkar, Pushpak Bhattacharyya, Swaroop Nath |  |
| 2172 |  |  [Fair Text Classification with Wasserstein Independence](https://doi.org/10.18653/v1/2023.emnlp-main.978) |  | 0 | Group fairness is a central research topic in text classification, where reaching fair treatment between sensitive groups (e.g. women vs. men) remains an open challenge. This paper presents a novel method for mitigating biases in neural text classification, agnostic to the model architecture.... | Antoine Gourru, Charlotte Laclau, Christophe Gravier, Rémi Emonet, Thibaud Leteno |  |
| 2173 |  |  [TacoPrompt: A Collaborative Multi-Task Prompt Learning Method for Self-Supervised Taxonomy Completion](https://doi.org/10.18653/v1/2023.emnlp-main.979) |  | 0 | Automatic taxonomy completion aims to attach the emerging concept to an appropriate pair of hypernym and hyponym in the existing taxonomy. Existing methods suffer from the overfitting to leaf-only problem caused by imbalanced leaf and non-leaf samples when training the newly initialized... | Ciyi Liu, Hongyuan Xu, Xiangrui Cai, Xiaojie Yuan, Yanlong Wen, Yuhang Niu, Yunong Chen |  |
| 2174 |  |  [An Attribution Method for Siamese Encoders](https://doi.org/10.18653/v1/2023.emnlp-main.980) |  | 0 | Despite the success of Siamese encoder models such as sentence transformers (ST), little is known about the aspects of inputs they pay attention to. A barrier is that their predictions cannot be attributed to individual features, as they compare two inputs rather than processing a single one. This... | Dmitry Nikolaev, Lucas Möller, Sebastian Padó |  |
| 2175 |  |  [Global Voices, Local Biases: Socio-Cultural Prejudices across Languages](https://doi.org/10.18653/v1/2023.emnlp-main.981) |  | 0 | Human biases are ubiquitous but not uniform: disparities exist across linguistic, cultural, and societal borders. As large amounts of recent literature suggest, language models (LMs) trained on human data can reflect and often amplify the effects of these social biases. However, the vast majority... | Anjishnu Mukherjee, Antonios Anastasopoulos, Chahat Raj, Ziwei Zhu |  |
| 2176 |  |  [Graph vs. Sequence: An Empirical Study on Knowledge Forms for Knowledge-Grounded Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.982) |  | 0 | Knowledge-grounded dialogue is a task of gener- ating an informative response based on both the dialogue history and external knowledge source. In general, there are two forms of knowledge: manu- ally annotated knowledge graphs and knowledge text from website. From various evaluation viewpoints,... | Heyan Huang, Yang Gao, Yizhe Yang, Yuhang Liu |  |
| 2177 |  |  [Are Compressed Language Models Less Subgroup Robust?](https://doi.org/10.18653/v1/2023.emnlp-main.983) |  | 0 | To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18... | Andrea Zugarini, Leonidas Gee, Novi Quadrianto |  |
| 2178 |  |  [Length Does Matter: Summary Length can Bias Summarization Metrics](https://doi.org/10.18653/v1/2023.emnlp-main.984) |  | 0 | Establishing the characteristics of an effective summary is a complicated and often subjective endeavor. Consequently, the development of metrics for the summarization task has become a dynamic area of research within natural language processing. In this paper, we reveal that existing summarization... | Soroush Vosoughi, Xiaobo Guo |  |
| 2179 |  |  [NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.985) |  | 0 | Temporal Logic (TL) can be used to rigorously specify complex high-level specification for systems in many engineering applications. The translation between natural language (NL) and TL has been under-explored due to the lack of dataset and generalizable model across different application domains.... | Chuchu Fan, Rujul Gandhi, Yang Zhang, Yongchao Chen |  |
| 2180 |  |  [Reformulating NLP tasks to Capture Longitudinal Manifestation of Language Disorders in People with Dementia](https://doi.org/10.18653/v1/2023.emnlp-main.986) |  | 0 | Dementia is associated with language disorders which impede communication. Here, we automatically learn linguistic disorder patterns by making use of a moderately-sized pre-trained language model and forcing it to focus on reformulated natural language processing (NLP) tasks and associated... | Dimitris Gkoumas, Maria Liakata, Matthew Purver |  |
| 2181 |  |  [Elevating Code-mixed Text Handling through Auditory Information of Words](https://doi.org/10.18653/v1/2023.emnlp-main.987) |  | 0 | With the growing popularity of code-mixed data, there is an increasing need for better handling of this type of data, which poses a number of challenges, such as dealing with spelling variations, multiple languages, different scripts, and a lack of resources. Current language models face difficulty... | Asif Ekbal, Mamta, Zishan Ahmad |  |
| 2182 |  |  [Predict and Use: Harnessing Predicted Gaze to Improve Multimodal Sarcasm Detection](https://doi.org/10.18653/v1/2023.emnlp-main.988) |  | 0 | Sarcasm is a complex linguistic construct with incongruity at its very core. Detecting sarcasm depends on the actual content spoken and tonality, facial expressions, the context of an utterance, and personal traits like language proficiency and cognitive capabilities. In this paper, we propose the... | Anupama Ray, Apoorva Nunna, Diptesh Kanojia, Divyank Tiwari, Pushpak Bhattacharyya |  |
| 2183 |  |  [Fine-grained Medical Vision-Language Representation Learning for Radiology Report Generation](https://doi.org/10.18653/v1/2023.emnlp-main.989) |  | 0 | Given the input radiology images, the objective of radiology report generation is to produce accurate and comprehensive medical reports, which typically include multiple descriptive clinical sentences associated with different phenotypes. Most existing works have relied on a pre-trained vision... | Bo Peng, Qi Peng, Siyuan Wang, Yichao Liu |  |
| 2184 |  |  [ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer](https://doi.org/10.18653/v1/2023.emnlp-main.990) |  | 0 | Text-to-speech(TTS) has undergone remarkable improvements in performance, particularly with the advent of Denoising Diffusion Probabilistic Models (DDPMs). However, the perceived quality of audio depends not solely on its content, pitch, rhythm, and energy, but also on the physical environment.In... | Hong Chen, Huadai Liu, Jinzheng He, Maozong Zheng, Rongjie Huang, Wenqiang Xu, Xuan Lin, Zhou Zhao |  |
| 2185 |  |  [Consistency Analysis of ChatGPT](https://doi.org/10.18653/v1/2023.emnlp-main.991) |  | 0 | ChatGPT has gained a huge popularity since its introduction. Its positive aspects have been reported through many media platforms, and some analyses even showed that ChatGPT achieved a decent grade in professional exams, adding extra support to the claim that AI can now assist and even replace... | Myeongjun Jang, Thomas Lukasiewicz |  |
| 2186 |  |  [Do Differences in Values Influence Disagreements in Online Discussions?](https://doi.org/10.18653/v1/2023.emnlp-main.992) |  | 0 | Disagreements are common in online discussions. Disagreement may foster collaboration and improve the quality of a discussion under some conditions. Although there exist methods for recognizing disagreement, a deeper understanding of factors that influence disagreement is lacking in the literature.... | Catholijn M. Jonker, Michiel van der Meer, Piek Vossen, Pradeep K. Murukannaiah |  |
| 2187 |  |  [Automated Fact-Checking in Dialogue: Are Specialized Models Needed?](https://doi.org/10.18653/v1/2023.emnlp-main.993) |  | 0 | Prior research has shown that typical fact-checking models for stand-alone claims struggle with claims made in conversation. As a solution, fine-tuning these models on dialogue data has been proposed. However, creating separate models for each use case is impractical, and we show that fine-tuning... | Andreas Vlachos, Eric Chamoun, Marzieh Saeidi |  |
| 2188 |  |  [A Digital Language Coherence Marker for Monitoring Dementia](https://doi.org/10.18653/v1/2023.emnlp-main.994) |  | 0 | The use of spontaneous language to derive appropriate digital markers has become an emergent, promising and non-intrusive method to diagnose and monitor dementia. Here we propose methods to capture language coherence as a cost-effective, human-interpretable digital marker for monitoring cognitive... | Adam Tsakalidis, Dimitris Gkoumas, Maria Liakata |  |
| 2189 |  |  [Detecting Spoilers in Movie Reviews with External Movie Knowledge and User Networks](https://doi.org/10.18653/v1/2023.emnlp-main.995) |  | 0 | Online movie review platforms are providing crowdsourced feedback for the film industry and the general public, while spoiler reviews greatly compromise user experience. Although preliminary research efforts were made to automatically identify spoilers, they merely focus on the review content... | Heng Wang, Minnan Luo, Qinghua Zheng, Shangbin Feng, Wenqian Zhang, Yuyang Bai, Zhaoxuan Tan |  |
| 2190 |  |  [Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimoda Emotion Recognition](https://doi.org/10.18653/v1/2023.emnlp-main.996) |  | 0 | Multimodal emotion recognition aims to recognize emotions for each utterance from multiple modalities, which has received increasing attention for its application in human-machine interaction. Current graph-based methods fail to simultaneously depict global contextual features and local diverse... | Dongyuan Li, Kotaro Funakoshi, Manabu Okumura, Yusong Wang |  |
| 2191 |  |  [HyperRank: Hyperbolic Ranking Model for Unsupervised Keyphrase Extraction](https://doi.org/10.18653/v1/2023.emnlp-main.997) |  | 0 | Given the exponential growth in the number of documents on the web in recent years, there is an increasing demand for accurate models to extract keyphrases from such documents. Keyphrase extraction is the task of automatically identifying representative keyphrases from the source document.... | Huafeng Liu, Liping Jing, Mingyang Song |  |
| 2192 |  |  [Assessing the influence of attractor-verb distance on grammatical agreement in humans and language models](https://doi.org/10.18653/v1/2023.emnlp-main.998) |  | 0 | Subject-verb agreement in the presence of an attractor noun located between the main noun and the verb elicits complex behavior: judgments of grammaticality are modulated by the grammatical features of the attractor. For example, in the sentence \`\`The girl near the boys likes climbing'', the... | ChristosNikolaos Zacharopoulos, Mathias SabléMeyer, Théo Desbordes |  |
| 2193 |  |  [Federated Meta-Learning for Emotion and Sentiment Aware Multi-modal Complaint Identification](https://doi.org/10.18653/v1/2023.emnlp-main.999) |  | 0 | Automatic detection of consumers’ complaints about items or services they buy can be critical for organizations and online merchants. Previous studies on complaint identification are limited to text. Images along with the reviews can provide cues to identify complaints better, thus emphasizing the... | Apoorva Singh, Siddarth Chandrasekar, Sriparna Saha, Tanmay Sen |  |
| 2194 |  |  [Semantic Similarity Models for Depression Severity Estimation](https://doi.org/10.18653/v1/2023.emnlp-main.1000) |  | 0 | Depressive disorders constitute a severe public health issue worldwide. However, public health systems have limited capacity for case detection and diagnosis. In this regard, the widespread use of social media has opened up a way to access public information on a large scale. Computational methods... | Anxo Pérez, Iryna Gurevych, Javier Parapar, Kexin Wang, Neha Warikoo |  |
| 2195 |  |  [Hop, Union, Generate: Explainable Multi-hop Reasoning without Rationale Supervision](https://doi.org/10.18653/v1/2023.emnlp-main.1001) |  | 0 | Explainable multi-hop question answering (QA) not only predicts answers but also identifies rationales, i. e. subsets of input sentences used to derive the answers. Existing methods rely on supervision for both answers and rationales. This problem has been extensively studied under the supervised... | Alexander M. Rush, Claire Cardie, Justin T. Chiu, Wenting Zhao |  |
| 2196 |  |  [To Split or Not to Split: Composing Compounds in Contextual Vector Spaces](https://doi.org/10.18653/v1/2023.emnlp-main.1002) |  | 0 | We investigate the effect of sub-word tokenization on representations of German noun compounds: single orthographic words which are composed of two or more constituents but often tokenized into units that are not morphologically motivated or meaningful. Using variants of BERT models and... | Christopher Jenkins, Filip Miletic, Sabine Schulte im Walde |  |
| 2197 |  |  [ToolWriter: Question Specific Tool Synthesis for Tabular Data](https://doi.org/10.18653/v1/2023.emnlp-main.1003) |  | 0 | Tabular question answering (TQA) presents a challenging setting for neural systems by requiring joint reasoning of natural language with large amounts of semi-structured data. Unlike humans who use programmatic tools like filters to transform data before processing, language models in TQA process... | Carlos Gemmell, Jeff Dalton |  |
| 2198 |  |  [Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations](https://doi.org/10.18653/v1/2023.emnlp-main.1004) |  | 0 | Relational databases play an important role in business, science, and more. However, many users cannot fully unleash the analytical power of relational databases, because they are not familiar with database languages such as SQL. Many techniques have been proposed to automatically generate SQL from... | Jonathan K. Kummerfeld, Tianyi Zhang, Toby JiaJun Li, Yuan Tian, Zheng Ning, Zheng Zhang |  |
| 2199 |  |  [CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Low Resource With Contrastive Learning](https://doi.org/10.18653/v1/2023.emnlp-main.1005) |  | 0 | Machine-Generated Text (MGT) detection, a task that discriminates MGT from Human-Written Text (HWT), plays a crucial role in preventing misuse of text generative models, which excel in mimicking human writing style recently. Latest proposed detectors usually take coarse text sequences as input and... | Chao Shen, Hang Pu, Xiaoming Liu, Yichen Wang, Yu Lan, Zhaohan Zhang |  |
| 2200 |  |  [AnyTOD: A Programmable Task-Oriented Dialog System](https://doi.org/10.18653/v1/2023.emnlp-main.1006) |  | 0 | We propose AnyTOD, an end-to-end, zero-shot task-oriented dialog (TOD) system capable of zero-shot adaptation onto unseen tasks or domains. We view TOD as a program executed by a language model (LM), where program logic and ontology is provided by a designer as a schema. To enable generalization to... | Abhinav Rastogi, Hagen Soltau, Harrison Lee, Izhak Shafran, Jeffrey Zhao, Mingqiu Wang, Raghav Gupta, Yonghui Wu, Yuan Cao |  |
| 2201 |  |  [Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization](https://doi.org/10.18653/v1/2023.emnlp-main.1007) |  | 0 | Recent pre-trained language models (PLMs) achieve promising results in existing abstractive summarization datasets. However, existing summarization benchmarks overlap in time with the standard pre-training corpora and finetuning datasets. Hence, the strong performance of PLMs may rely on the... | Chi Seng Cheang, Derek F. Wong, Hou Pong Chan, Lidia S. Chao, Shudong Liu, Xuebo Liu, Yanming Sun, Zhaocong Li |  |
| 2202 |  |  [Zero-Shot Multi-Label Topic Inference with Sentence Encoders and LLMs](https://doi.org/10.18653/v1/2023.emnlp-main.1008) |  | 0 | In this paper, we conducted a comprehensive study with the latest Sentence Encoders and Large Language Models (LLMs) on the challenging task of “definition-wild zero-shot topic inference”, where users define or provide the topics of interest in real-time. Through extensive experimentation on seven... | Dongji Feng, Shubhra Kanti Karmaker Santu, Souvika Sarkar |  |
| 2203 |  |  [TaskDiff: A Similarity Metric for Task-Oriented Conversations](https://doi.org/10.18653/v1/2023.emnlp-main.1009) |  | 0 | The popularity of conversational digital assistants has resulted in the availability of large amounts of conversational data which can be utilized for improved user experience and personalized response generation. Building these assistants using popular large language models like ChatGPT also... | Ankita Bhaumik, Praveen Venkateswaran, Vatche Isahagian, Yara Rizk |  |
| 2204 |  |  [Not all Fake News is Written: A Dataset and Analysis of Misleading Video Headlines](https://doi.org/10.18653/v1/2023.emnlp-main.1010) |  | 0 | Polarization and the marketplace for impressions have conspired to make navigating information online difficult for users, and while there has been a significant effort to detect false or misleading text, multimodal datasets have received considerably less attention. To complement existing... | Jordan L. BoydGraber, Naeemul Hassan, Yoo Yeon Sung |  |
| 2205 |  |  [Learning From Free-Text Human Feedback - Collect New Datasets Or Extend Existing Ones?](https://doi.org/10.18653/v1/2023.emnlp-main.1011) |  | 0 | Learning from free-text human feedback is essential for dialog systems, but annotated data is scarce and usually covers only a small fraction of error types known in conversational AI. Instead of collecting and annotating new datasets from scratch, recent advances in synthetic dialog generation... | Dominic Petrak, Iryna Gurevych, Nafise Sadat Moosavi, Nikolai Rozanov, Ye Tian |  |
| 2206 |  |  [Euphemistic Abuse - A New Dataset and Classification Experiments for Implicitly Abusive Language](https://doi.org/10.18653/v1/2023.emnlp-main.1012) |  | 0 | We address the task of identifying euphemistic abuse (e.g. “You inspire me to fall asleep”) paraphrasing simple explicitly abusive utterances (e.g. “You are boring”). For this task, we introduce a novel dataset that has been created via crowdsourcing. Special attention has been paid to the... | Elisabeth Eder, Jana Kampfmeier, Josef Ruppenhofer, Michael Wiegand |  |
| 2207 |  |  [Exploring Distributional Shifts in Large Language Models for Code Analysis](https://doi.org/10.18653/v1/2023.emnlp-main.1013) |  | 0 | We systematically study how three large language models with code capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data. We consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an... | Rocktim Jyoti Das, Shushan Arakelyan, Xiang Ren, Yi Mao |  |
| 2208 |  |  [ATHENA: Mathematical Reasoning with Thought Expansion](https://doi.org/10.18653/v1/2023.emnlp-main.1014) |  | 0 | Solving math word problems depends on how to articulate the problems, the lens through which models view human linguistic expressions. Real-world settings count on such a method even more due to the diverse practices of the same mathematical operations. Earlier works constrain available thinking... | Hazel Kim, JB. Kim, Joonghyuk Hahn, YoSub Han |  |
| 2209 |  |  [A Benchmark for Reasoning with Spatial Prepositions](https://doi.org/10.18653/v1/2023.emnlp-main.1015) |  | 0 | Spatial reasoning is a fundamental building block of human cognition, used in representing, grounding, and reasoning about physical and abstract concepts. We propose a novel benchmark focused on assessing inferential properties of statements with spatial prepositions. The benchmark includes... | Iulia M. Comsa, Srini Narayanan |  |
| 2210 |  |  [TIMELINE: Exhaustive Annotation of Temporal Relations Supporting the Automatic Ordering of Events in News Articles](https://doi.org/10.18653/v1/2023.emnlp-main.1016) |  | 0 | Temporal relation extraction models have thus far been hindered by a number of issues in existing temporal relation-annotated news datasets, including: (1) low inter-annotator agreement due to the lack of specificity of their annotation guidelines in terms of what counts as a temporal relation; (2)... | Riza BatistaNavarro, Sarah Alsayyahi |  |
| 2211 |  |  [Mitigating Over-Generation for Unsupervised Keyphrase Extraction with Heterogeneous Centrality Detection](https://doi.org/10.18653/v1/2023.emnlp-main.1017) |  | 0 | Over-generation errors occur when a keyphrase extraction model correctly determines a candidate keyphrase as a keyphrase because it contains a word that frequently appears in the document but at the same time erroneously outputs other candidates as keyphrases because they contain the same word. To... | Huafeng Liu, Liping Jing, Mingyang Song, Pengyu Xu, Yi Feng |  |
| 2212 |  |  [Towards Interpretable and Efficient Automatic Reference-Based Summarization Evaluation](https://doi.org/10.18653/v1/2023.emnlp-main.1018) |  | 0 | Interpretability and efficiency are two important considerations for the adoption of neural automatic metrics. In this work, we develop strong-performing automatic metrics for reference-based summarization evaluation, based on a two-stage evaluation pipeline that first extracts basic information... | Alexander R. Fabbri, Caiming Xiong, ChienSheng Wu, Dragomir Radev, Pengfei Liu, Shafiq Joty, Yilun Zhao, Yixin Liu |  |
| 2213 |  |  [MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding](https://doi.org/10.18653/v1/2023.emnlp-main.1019) |  | 0 | Reading comprehension of legal text can be a particularly challenging task due to the length and complexity of legal clauses and a shortage of expert-annotated datasets. To address this challenge, we introduce the Merger Agreement Understanding Dataset (MAUD), an expert-annotated reading... | Antoine Scardigli, Anya Chen, Dan Hendrycks, Dimitry Levkin, Leonard Tang, Oliver Zhang, Spencer Ball, Steven H. Wang, Thomas Woodside, Wei Chen |  |
| 2214 |  |  [PK-ICR: Persona-Knowledge Interactive Multi-Context Retrieval for Grounded Dialogue](https://doi.org/10.18653/v1/2023.emnlp-main.1020) |  | 0 | Identifying relevant persona or knowledge for conversational systems is critical to grounded dialogue response generation. However, each grounding has been mostly researched in isolation with more practical multi-context dialogue tasks introduced in recent works. We define Persona and Knowledge... | Guoyin Wang, Jiwei Li, Joosung Lee, Minsik Oh |  |
| 2215 |  |  [More Than Spoken Words: Nonverbal Message Extraction and Generation](https://doi.org/10.18653/v1/2023.emnlp-main.1021) |  | 0 | Nonverbal messages (NM) such as speakers’ facial expressions and speed of speech are essential for face-to-face communication, and they can be regarded as implicit knowledge as they are usually not included in existing dialogue understanding or generation tasks. This paper introduces the task of... | Dian Yu, Dong Yu, Haitao Mi, Longyue Wang, Nan Du, Wanshun Chen, Xiaoyang Wang |  |
| 2216 |  |  [Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance](https://doi.org/10.18653/v1/2023.emnlp-main.1022) |  | 0 | While analogies are a common way to evaluate word embeddings in NLP, it is also of interest to investigate whether or not analogical reasoning is a task in itself that can be learned. In this paper, we test several ways to learn basic analogical reasoning, specifically focusing on analogies that... | Lonneke van der Plas, Molly R. Petersen |  |
| 2217 |  |  [FAME: Flexible, Scalable Analogy Mappings Engine](https://doi.org/10.18653/v1/2023.emnlp-main.1023) |  | 0 | Analogy is one of the core capacities of human cognition; when faced with new situations, we often transfer prior experience from other domains. Most work on computational analogy relies heavily on complex, manually crafted input. In this work, we relax the input requirements, requiring only names... | Chen Shani, Dafna Shahaf, Shahar Jacob |  |
| 2218 |  |  [A Self-training Framework for Automated Medical Report Generation](https://doi.org/10.18653/v1/2023.emnlp-main.1024) |  | 0 | Medical report generation, focusing on automatically generating accurate clinical findings from medical images, is an important medical artificial intelligence task. It reduces the workload of physicians in writing reports. Many of the current methods depend heavily on labeled datasets that include... | Bo Peng, Siyuan Wang, Zheng Liu |  |
| 2219 |  |  [A Picture is Worth a Thousand Words: Language Models Plan from Pixels](https://doi.org/10.18653/v1/2023.emnlp-main.1025) |  | 0 | Planning is an important capability of artificial agents that perform long-horizon tasks in real-world environments. In this work, we explore the use of pre-trained language models (PLMs) to reason about plan sequences from text instructions in embodied visual environments. Prior PLM based... | Anthony Z. Liu, Honglak Lee, Lajanugen Logeswaran, Sungryull Sohn |  |
| 2220 |  |  [Interpreting and Exploiting Functional Specialization in Multi-Head Attention under Multi-task Learning](https://doi.org/10.18653/v1/2023.emnlp-main.1026) |  | 0 | Transformer-based models, even though achieving super-human performance on several downstream tasks, are often regarded as a black box and used as a whole. It is still unclear what mechanisms they have learned, especially their core module: multi-head attention. Inspired by functional... | Chengqing Zong, Chong Li, Jiajun Zhang, Shaonan Wang, Yunhao Zhang |  |
| 2221 |  |  [Multilingual Previously Fact-Checked Claim Retrieval](https://doi.org/10.18653/v1/2023.emnlp-main.1027) |  | 0 | Fact-checkers are often hampered by the sheer amount of online content that needs to be fact-checked. NLP can help them by retrieving already existing fact-checks relevant to the content being investigated. This paper introduces a new multilingual dataset for previously fact-checked claim... | Ivan Srba, Ivan Vykopal, Jakub Simko, Juraj Podrouzek, Martin Melisek, Matús Pikuliak, Mária Bieliková, Róbert Móro, Timo Hromadka, Timotej Smolen |  |
| 2222 |  |  [ALCAP: Alignment-Augmented Music Captioner](https://doi.org/10.18653/v1/2023.emnlp-main.1028) |  | 0 | Music captioning has gained significant attention in the wake of the rising prominence of streaming media platforms. Traditional approaches often prioritize either the audio or lyrics aspect of the music, inadvertently ignoring the intricate interplay between the two. However, a comprehensive... | Changyou Chen, Kristina Lerman, Wei Tsung Lu, Weituo Hao, Xuchen Song, Zihao He |  |
| 2223 |  |  [Do Transformers Parse while Predicting the Masked Word?](https://doi.org/10.18653/v1/2023.emnlp-main.1029) |  | 0 | Pre-trained language models have been shown to encode linguistic structures like parse trees in their embeddings while being trained unsupervised. Some doubts have been raised whether the models are doing parsing or only some computation weakly correlated with it. Concretely: (a) Is it possible to... | Abhishek Panigrahi, Haoyu Zhao, Rong Ge, Sanjeev Arora |  |
| 2224 |  |  [Composable Text Controls in Latent Space with ODEs](https://doi.org/10.18653/v1/2023.emnlp-main.1030) |  | 0 | Real-world text applications often involve composing a wide range of text control operations, such as editing the text w.r.t. an attribute, manipulating keywords and structure, and generating new text of desired properties. Prior work typically learns/finetunes a language model (LM) to perform... | Guangyi Liu, Junwei Bao, Shuguang Cui, Xiaodan Liang, Xiaodong He, Yuan Gao, Zeyu Feng, Zhen Li, Zhiting Hu, Zichao Yang |  |
| 2225 |  |  [P5: Plug-and-Play Persona Prompting for Personalized Response Selection](https://doi.org/10.18653/v1/2023.emnlp-main.1031) |  | 0 | The use of persona-grounded retrieval-based chatbots is crucial for personalized conversations, but there are several challenges that need to be addressed. 1) In general, collecting persona-grounded corpus is very expensive. 2) The chatbot system does not always respond in consideration of persona... | Donghun Lee, Joosung Lee, Minsik Oh |  |
| 2226 |  |  [Reader: Model-based language-instructed reinforcement learning](https://doi.org/10.18653/v1/2023.emnlp-main.1032) |  | 0 | We explore how we can build accurate world models, which are partially specified by language, and how we can plan with them in the face of novelty and uncertainty. We propose the first model-based reinforcement learning approach to tackle the environment Read To Fight Monsters (Zhong et al., 2019),... | Alexander Ilin, Nicola Dainese, Pekka Marttinen |  |
| 2227 |  |  [Adapting Offline Speech Translation Models for Streaming with Future-Aware Distillation and Inference](https://doi.org/10.18653/v1/2023.emnlp-main.1033) |  | 0 | A popular approach to streaming speech translation is to employ a single offline model with a wait-k policy to support different latency requirements, which is simpler than training multiple online models with different latency constraints. However, there is a mismatch problem in using a model... | Biao Fu, Boxing Chen, Kai Fan, Minpeng Liao, Xiaodong Shi, Yidong Chen, Zhongqiang Huang |  |
| 2228 |  |  [Relation-aware Ensemble Learning for Knowledge Graph Embedding](https://doi.org/10.18653/v1/2023.emnlp-main.1034) |  | 0 | Knowledge graph (KG) embedding is a fundamental task in natural language processing, and various methods have been proposed to explore semantic patterns in distinctive ways. In this paper, we propose to learn an ensemble by leveraging existing methods in a relation-aware manner. However, exploring... | Ling Yue, Quanming Yao, Xian Wu, Yefeng Zheng, Yong Li, Yongqi Zhang, Zhenxi Lin, Ziheng Zhang |  |
| 2229 |  |  [GenEx: A Commonsense-aware Unified Generative Framework for Explainable Cyberbullying Detection](https://doi.org/10.18653/v1/2023.emnlp-main.1035) |  | 0 | With the rise of social media and online communication, the issue of cyberbullying has gained significant prominence. While extensive research is being conducted to develop more effective models for detecting cyberbullying in monolingual languages, a significant gap exists in understanding... | Krishanu Maity, Prince Jha, Pushpak Bhattacharyya, Raghav Jain, Sriparna Saha |  |
| 2230 |  |  [Document-Level Machine Translation with Large Language Models](https://doi.org/10.18653/v1/2023.emnlp-main.1036) |  | 0 | Large language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs’ ability on discourse... | Chenyang Lyu, Dian Yu, Longyue Wang, Shuming Shi, Tianbo Ji, Zhaopeng Tu, Zhirui Zhang |  |
| 2231 |  |  [Multilingual Simplification of Medical Texts](https://doi.org/10.18653/v1/2023.emnlp-main.1037) |  | 0 | Automated text simplification aims to produce simple versions of complex texts. This task is especially useful in the medical domain, where the latest medical findings are typically communicated via complex and technical articles. This creates barriers for laypeople seeking access to up-to-date... | Byron C. Wallace, Junyi Jessy Li, Kathryn Kazanas, Keziah Reina, Sebastian Joseph, Vishnesh J. Ramanathan, Wei Xu |  |
| 2232 |  |  [When Reviewers Lock Horns: Finding Disagreements in Scientific Peer Reviews](https://doi.org/10.18653/v1/2023.emnlp-main.1038) |  | 0 | To this date, the efficacy of the scientific publishing enterprise fundamentally rests on the strength of the peer review process. The journal editor or the conference chair primarily relies on the expert reviewers’ assessment, identify points of agreement and disagreement and try to reach a... | Asif Ekbal, Sandeep Kumar, Tirthankar Ghosal |  |
| 2233 |  |  [Argue with Me Tersely: Towards Sentence-Level Counter-Argument Generation](https://doi.org/10.18653/v1/2023.emnlp-main.1039) |  | 0 | Counter-argument generation—a captivating area in computational linguistics—seeks to craft statements that offer opposing views. While most research has ventured into paragraph-level generation, sentence-level counter-argument generation beckons with its unique constraints and brevity-focused... | Jiayu Lin, Meng Han, Qi Zhang, Rong Ye, Ruofei Lai, Xinyu Zhang, Xuanjing Huang, Zhao Cao, Zhongyu Wei |  |
| 2234 |  |  [JASMINE: Arabic GPT Models for Few-Shot Learning](https://doi.org/10.18653/v1/2023.emnlp-main.1040) |  | 0 | Scholarship on generative pretraining (GPT) remains acutely Anglocentric, leaving serious gaps in our understanding of the whole class of autoregressive models. For example, we have little knowledge about the potential of these models and their societal impacts in diverse linguistic and cultural... | AbdelRahim A. Elmadany, Alcides Alcoba Inciarte, El Moatez Billah Nagoudi, Md Tawkat Islam Khondaker, Muhammad AbdulMageed |  |
| 2235 |  |  [NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports](https://doi.org/10.18653/v1/2023.emnlp-main.1041) |  | 0 | How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical... | André Freitas, Dónal Landers, Hannah Frost, Marco Valentino, Maël Jullien, Paul O'Regan |  |
| 2236 |  |  [Addressing Linguistic Bias through a Contrastive Analysis of Academic Writing in the NLP Domain](https://doi.org/10.18653/v1/2023.emnlp-main.1042) |  | 0 | It has been well documented that a reviewer’s opinion of the nativeness of expression in an academic paper affects the likelihood of it being accepted for publication. Previous works have also shone a light on the stress and anxiety authors who are non-native English speakers experience when... | Jianbing Zhang, Robert Ridley, Shujian Huang, Xinyu Dai, Zhen Wu |  |
| 2237 |  |  [RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation](https://doi.org/10.18653/v1/2023.emnlp-main.1043) |  | 0 | Grammatical Error Correction (GEC) systems play a vital role in assisting people with their daily writing tasks. However, users may sometimes come across a GEC system that initially performs well but fails to correct errors when the inputs are slightly modified. To ensure an ideal user experience,... | Enbo Zhao, Leyang Cui, Shuming Shi, Wei Bi, Yue Zhang |  |
| 2238 |  |  [Detecting Propaganda Techniques in Code-Switched Social Media Text](https://doi.org/10.18653/v1/2023.emnlp-main.1044) |  | 0 | Propaganda is a form of communication intended to influence the opinions and the mindset of the public to promote a particular agenda. With the rise of social media, propaganda has spread rapidly, leading to the need for automatic propaganda detection systems. Most work on propaganda detection has... | Asif Hanif, Muhammad Umar Salman, Preslav Nakov, Shady Shehata |  |
| 2239 |  |  [Speech Recognition and Meaning Interpretation: Towards Disambiguation of Structurally Ambiguous Spoken Utterances in Indonesian](https://doi.org/10.18653/v1/2023.emnlp-main.1045) |  | 0 | Despite being the world’s fourth-most populous country, the development of spoken language technologies in Indonesia still needs improvement. Most automatic speech recognition (ASR) systems that have been developed are still limited to transcribing the exact word-by-word, which, in many cases,... | Ayu Purwarianti, Dessi Puji Lestari, Dipta Tanaya, Kurniawati Azizah, Ruhiyah Widiaputri, Sakriani Sakti |  |
| 2240 |  |  [Target-Agnostic Gender-Aware Contrastive Learning for Mitigating Bias in Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.emnlp-main.1046) |  | 0 | Gender bias is a significant issue in machine translation, leading to ongoing research efforts in developing bias mitigation techniques. However, most works focus on debiasing bilingual models without much consideration for multilingual systems. In this paper, we specifically target the gender bias... | Dongdong Zhang, Hyukhun Koh, Kangil Lee, Kyomin Jung, Minsung Kim, Minwoo Lee |  |
| 2241 |  |  [Code-Switching Metrics Using Intonation Units](https://doi.org/10.18653/v1/2023.emnlp-main.1047) |  | 0 | Code-switching (CS) metrics in NLP that are based on word-level units are misaligned with true bilingual CS behavior. Crucially, CS is not equally likely between any two words, but follows syntactic and prosodic rules. We adapt two metrics, multilinguality and CS probability, and apply them to... | Dora LaCasse, Rebecca Pattichis, Rena Cacoullos, Sonya Trawick |  |
