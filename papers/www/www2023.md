# WWW2023 Paper List

|论文|作者|组织|摘要|翻译|代码|引用数|
|---|---|---|---|---|---|---|
|[Automated Ontology Evaluation: Evaluating Coverage and Correctness using a Domain Corpus](https://doi.org/10.1145/3543873.3587617)|Antonio Zaitoun, Tomer Sagi, Katja Hose|Aalborg University, Denmark; University of Haifa, Israel; Aalborg University, Denmark and TU Wien, Austria|Ontologies conceptualize domains and are a crucial part of web semantics and information systems. However, re-using an existing ontology for a new task requires a detailed evaluation of the candidate ontology as it may cover only a subset of the domain concepts, contain information that is redundant or misleading, and have inaccurate relations and hierarchies between concepts. Manual evaluation of large and complex ontologies is a tedious task. Thus, a few approaches have been proposed for automated evaluation, ranging from concept coverage to ontology generation from a corpus. Existing approaches, however, are limited by their dependence on external structured knowledge sources, such as a thesaurus, as well as by their inability to evaluate semantic relationships. In this paper, we propose a novel framework to automatically evaluate the domain coverage and semantic correctness of existing ontologies based on domain information derived from text. The approach uses a domain-tuned named-entity-recognition model to extract phrasal concepts. The extracted concepts are then used as a representation of the domain against which we evaluate the candidate ontology’s concepts. We further employ a domain-tuned language model to determine the semantic correctness of the candidate ontology’s relations. We demonstrate our automated approach on several large ontologies from the oceanographic domain and show its agreement with a manual evaluation by domain experts and its superiority over the state-of-the-art.|本体概念化领域，是网络语义和信息系统的重要组成部分。然而，在新任务中重新使用现有的本体需要对候选本体进行详细的评估，因为它可能只涉及领域概念的一个子集，包含冗余或误导的信息，并且概念之间的关系和层次结构不准确。手工评估大型和复杂的本体是一项繁琐的任务。因此，提出了一些自动评估的方法，从概念覆盖到从语料库中生成本体。然而，现有的方法受到依赖于外部结构化知识源(如同义词表)以及无法评估语义关系的限制。本文提出了一种基于文本的领域信息自动评估现有本体的领域覆盖率和语义正确性的框架。该方法使用一个域调整的命名实体识别模型来提取短语概念。然后将提取的概念用作领域的表示，我们根据这个表示来评估候选本体的概念。我们进一步使用领域调优的语言模型来确定候选本体关系的语义正确性。我们展示了我们的自动化方法从海洋学领域的几个大的本体论，并表明其与领域专家的手工评估的一致性及其优越性的最新水平。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automated+Ontology+Evaluation:+Evaluating+Coverage+and+Correctness+using+a+Domain+Corpus)|2|
|[Reinforcement Learning-based Counter-Misinformation Response Generation: A Case Study of COVID-19 Vaccine Misinformation](https://doi.org/10.1145/3543507.3583388)|Bing He, Mustaque Ahamad, Srijan Kumar|Georgia Institute of Technology, USA|The spread of online misinformation threatens public health, democracy, and the broader society. While professional fact-checkers form the first line of defense by fact-checking popular false claims, they do not engage directly in conversations with misinformation spreaders. On the other hand, non-expert ordinary users act as eyes-on-the-ground who proactively counter misinformation -- recent research has shown that 96% counter-misinformation responses are made by ordinary users. However, research also found that 2/3 times, these responses are rude and lack evidence. This work seeks to create a counter-misinformation response generation model to empower users to effectively correct misinformation. This objective is challenging due to the absence of datasets containing ground-truth of ideal counter-misinformation responses, and the lack of models that can generate responses backed by communication theories. In this work, we create two novel datasets of misinformation and counter-misinformation response pairs from in-the-wild social media and crowdsourcing from college-educated students. We annotate the collected data to distinguish poor from ideal responses that are factual, polite, and refute misinformation. We propose MisinfoCorrect, a reinforcement learning-based framework that learns to generate counter-misinformation responses for an input misinformation post. The model rewards the generator to increase the politeness, factuality, and refutation attitude while retaining text fluency and relevancy. Quantitative and qualitative evaluation shows that our model outperforms several baselines by generating high-quality counter-responses. This work illustrates the promise of generative text models for social good -- here, to help create a safe and reliable information ecosystem. The code and data is accessible on https://github.com/claws-lab/MisinfoCorrect.|网上虚假信息的传播威胁着公众健康、民主政体以及更广泛的社会。虽然专业的事实核查人员通过事实核查流行的虚假说法构成了第一道防线，但他们并不直接参与与错误信息传播者的对话。另一方面，非专家的普通用户充当了主动反错误信息的实地眼睛——最近的研究表明，96% 的反错误信息反应是由普通用户做出的。然而，研究还发现，有2/3的情况下，这些回答是粗鲁的，缺乏证据。这项工作旨在创建一个反错误信息响应生成模型，使用户能够有效地纠正错误信息。这一目标具有挑战性，因为缺乏包含理想的反错误信息反应的地面真相的数据集，以及缺乏能够产生由传播理论支持的反应的模型。在这项工作中，我们创建了两个新的数据集的错误信息和反错误信息的反应对野生社会媒体和大学生众包。我们对收集到的数据进行注释，以区分差异和理想的反应，这些反应是事实的、礼貌的和反驳错误信息的。我们提出一个基于强化学习的错误信息纠正框架，学习为输入错误信息的帖子产生反错误信息响应。该模型在保持文本流畅性和相关性的同时，奖励生成者增加礼貌、事实和反驳态度。定量和定性评估表明，我们的模型通过产生高质量的反响优于几个基线。这项工作说明了生成文本模型对社会公益的承诺——在这里，帮助创建一个安全可靠的信息生态系统。代码和数据可在 https://github.com/claws-lab/misinfocorrect 上查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reinforcement+Learning-based+Counter-Misinformation+Response+Generation:+A+Case+Study+of+COVID-19+Vaccine+Misinformation)|2|
|[A Concept Knowledge Graph for User Next Intent Prediction at Alipay](https://doi.org/10.1145/3543873.3587308)|Yacheng He, Qianghuai Jia, Lin Yuan, Ruopeng Li, Yixin Ou, Ningyu Zhang|Ant Group, China; Zhejiang University, China|This paper illustrates the technologies of user next intent prediction with a concept knowledge graph. The system has been deployed on the Web at Alipay, serving more than 100 million daily active users. To explicitly characterize user intent, we propose AlipayKG, which is an offline concept knowledge graph in the Life-Service domain modeling the historical behaviors of users, the rich content interacted by users and the relations between them. We further introduce a Transformer-based model which integrates expert rules from the knowledge graph to infer the online user's next intent. Experimental results demonstrate that the proposed system can effectively enhance the performance of the downstream tasks while retaining explainability.|本文用概念知识图说明了用户下一意图预测技术。该系统已经部署在支付宝的网站上，为超过1亿日活跃用户提供服务。为了明确表征用户意图，本文提出了 AlipayKG，它是生活服务领域中的一个离线概念知识图，对用户的历史行为、用户交互的丰富内容以及用户之间的关系进行建模。我们进一步介绍了一个基于 Transformer 的模型，该模型集成了来自知识图的专家规则，以推断在线用户的下一个意图。实验结果表明，该系统能够有效地提高下游任务的性能，同时保持可解释性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Concept+Knowledge+Graph+for+User+Next+Intent+Prediction+at+Alipay)|1|
|[Interaction-level Membership Inference Attack Against Federated Recommender Systems](https://doi.org/10.1145/3543507.3583359)|Wei Yuan, Chaoqun Yang, Quoc Viet Hung Nguyen, Lizhen Cui, Tieke He, Hongzhi Yin|Griffith University, Australia; Nanjing University, China; Shandong University, China; The University of Queensland, Australia|The marriage of federated learning and recommender system (FedRec) has been widely used to address the growing data privacy concerns in personalized recommendation services. In FedRecs, users' attribute information and behavior data (i.e., user-item interaction data) are kept locally on their personal devices, therefore, it is considered a fairly secure approach to protect user privacy. As a result, the privacy issue of FedRecs is rarely explored. Unfortunately, several recent studies reveal that FedRecs are vulnerable to user attribute inference attacks, highlighting the privacy concerns of FedRecs. In this paper, we further investigate the privacy problem of user behavior data (i.e., user-item interactions) in FedRecs. Specifically, we perform the first systematic study on interaction-level membership inference attacks on FedRecs. An interaction-level membership inference attacker is first designed, and then the classical privacy protection mechanism, Local Differential Privacy (LDP), is adopted to defend against the membership inference attack. Unfortunately, the empirical analysis shows that LDP is not effective against such new attacks unless the recommendation performance is largely compromised. To mitigate the interaction-level membership attack threats, we design a simple yet effective defense method to significantly reduce the attacker's inference accuracy without losing recommendation performance. Extensive experiments are conducted with two widely used FedRecs (Fed-NCF and Fed-LightGCN) on three real-world recommendation datasets (MovieLens-100K, Steam-200K, and Amazon Cell Phone), and the experimental results show the effectiveness of our solutions.|联邦学习与推荐系统的结合(FedRec)已被广泛用于解决个性化推荐服务中日益增长的数据隐私问题。在 FedRecs 中，用户的属性信息和行为数据(即用户项交互数据)保存在他们的个人设备上，因此，它被认为是保护用户隐私的一种相当安全的方法。因此，FedRecs 的隐私问题很少被探讨。不幸的是，最近的一些研究表明，联邦医疗记录系统容易受到用户属性推理攻击，突出了联邦医疗记录系统的隐私问题。在本文中，我们进一步研究了 FedRecs 中用户行为数据(即用户项交互)的隐私问题。具体来说，我们对 FedRecs 的交互层次成员推理攻击进行了第一次系统研究。首先设计了一个交互级别的成员推断攻击，然后采用经典的隐私保护机制，即本地差分隐私(lDP)来抵御成员推断攻击。遗憾的是，实证分析表明，除非推荐性能受到很大影响，否则 LDP 无法有效地抵抗这种新的攻击。为了减轻交互级别的成员攻击威胁，我们设计了一种简单而有效的防御方法，在不损失推荐性能的前提下显著降低攻击者的推断精度。在三个真实世界的推荐数据集(MovieLens-100K，Stream-200K 和 Amazon Cell Phone)上，用两个广泛使用的 FedRecs (Fed-NCF 和 Fed-LightGCN)进行了广泛的实验，实验结果显示了我们的解决方案的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interaction-level+Membership+Inference+Attack+Against+Federated+Recommender+Systems)|1|
|[Learning with Exposure Constraints in Recommendation Systems](https://doi.org/10.1145/3543507.3583320)|Omer BenPorat, Rotem Torkan|Faculty of Data and Decision Sciences, Technion - Israel Institute of Technology, Israel|Recommendation systems are dynamic economic systems that balance the needs of multiple stakeholders. A recent line of work studies incentives from the content providers' point of view. Content providers, e.g., vloggers and bloggers, contribute fresh content and rely on user engagement to create revenue and finance their operations. In this work, we propose a contextual multi-armed bandit setting to model the dependency of content providers on exposure. In our model, the system receives a user context in every round and has to select one of the arms. Every arm is a content provider who must receive a minimum number of pulls every fixed time period (e.g., a month) to remain viable in later rounds; otherwise, the arm departs and is no longer available. The system aims to maximize the users' (content consumers) welfare. To that end, it should learn which arms are vital and ensure they remain viable by subsidizing arm pulls if needed. We develop algorithms with sub-linear regret, as well as a lower bound that demonstrates that our algorithms are optimal up to logarithmic factors.|推荐系统是平衡多个利益相关者需求的动态经济系统。最近的一项工作是从内容提供商的角度研究激励机制。内容提供商，例如，视频博客和博客，贡献新的内容，并依靠用户参与来创造收入和资助他们的业务。在这项工作中，我们提出了一个上下文多臂老虎机设置来模拟内容提供者对曝光的依赖。在我们的模型中，系统在每一轮中接收一个用户上下文，并且必须选择一个武器。每只手臂都是一个内容提供者，它必须在每个固定的时间段(例如，一个月)接受最少数量的拉动，以便在以后的回合中保持活力; 否则，这只手臂就会离开，不再可用。该系统旨在最大化用户(内容消费者)的福利。为此，它应该了解哪些武器是至关重要的，并确保他们保持可行的补贴，如果需要的手臂拉。我们开发的算法与次线性遗憾，以及一个下限，表明我们的算法是最优的对数因素。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+with+Exposure+Constraints+in+Recommendation+Systems)|1|
|[On How Zero-Knowledge Proof Blockchain Mixers Improve, and Worsen User Privacy](https://doi.org/10.1145/3543507.3583217)|Zhipeng Wang, Stefanos Chaliasos, Kaihua Qin, Liyi Zhou, Lifeng Gao, Pascal Berrang, Benjamin Livshits, Arthur Gervais|University of Birmingham, United Kingdom; UCL, United Kingdom and UC Berkeley, USA; Imperial College London, United Kingdom|Zero-knowledge proof (ZKP) mixers are one of the most widely-used blockchain privacy solutions, operating on top of smart contract-enabled blockchains. We find that ZKP mixers are tightly intertwined with the growing number of Decentralized Finance (DeFi) attacks and Blockchain Extractable Value (BEV) extractions. Through coin flow tracing, we discover that 205 blockchain attackers and 2,595 BEV extractors leverage mixers as their source of funds, while depositing a total attack revenue of 412.87M USD. Moreover, the US OFAC sanctions against the largest ZKP mixer, Tornado.Cash, have reduced the mixer's daily deposits by more than 80%. Further, ZKP mixers advertise their level of privacy through a so-called anonymity set size, which similarly to k-anonymity allows a user to hide among a set of k other users. Through empirical measurements, we, however, find that these anonymity set claims are mostly inaccurate. For the most popular mixers on Ethereum (ETH) and Binance Smart Chain (BSC), we show how to reduce the anonymity set size on average by 27.34% and 46.02% respectively. Our empirical evidence is also the first to suggest a differing privacy-predilection of users on ETH and BSC. State-of-the-art ZKP mixers are moreover interwoven with the DeFi ecosystem by offering anonymity mining (AM) incentives, i.e., users receive monetary rewards for mixing coins. However, contrary to the claims of related work, we find that AM does not necessarily improve the quality of a mixer's anonymity set. Our findings indicate that AM attracts privacy-ignorant users, who then do not contribute to improving the privacy of other mixer users.|零知识证明(ZKP)混频器是最广泛使用的区块链隐私解决方案之一，运行在智能合同启用的区块链之上。我们发现 ZKP 混频器与不断增加的分散金融(DeFi)攻击和区块链可提取值(BEV)提取紧密相关。通过硬币流追踪，我们发现205个区块链攻击者和2595个 BEV 提取者利用混合器作为他们的资金来源，同时存储总攻击收入为412.87万美元。此外，美国海外资产管制办公室制裁了 ZKP 最大的搅拌机龙卷风。现金，减少了搅拌机的每日存款超过80% 。此外，ZKP 混频器通过所谓的匿名集大小来宣传他们的隐私级别，这与 k 匿名类似，允许用户隐藏在 k 其他用户集中。然而，通过实证测量，我们发现这些匿名集索赔大多是不准确的。针对 Etherum (ETH)和 Binance 智能链(BSC)上最常用的混频器，我们分别给出了如何平均减少匿名集大小27.34% 和46.02% 的方法。我们的经验证明也是第一个提出不同隐私偏好的用户。最先进的 ZKP 混合器还通过提供匿名挖掘(AM)奖励与 DeFi 生态系统交织在一起，也就是说，用户通过混合硬币获得金钱奖励。然而，与相关工作的主张相反，我们发现 AM 并不一定改善混频器匿名集的质量。我们的研究结果表明，AM 吸引了那些对隐私无知的用户，而这些用户并没有帮助改善其他混频器用户的隐私。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+How+Zero-Knowledge+Proof+Blockchain+Mixers+Improve,+and+Worsen+User+Privacy)|1|
|[To Store or Not? Online Data Selection for Federated Learning with Limited Storage](https://doi.org/10.1145/3543507.3583426)|Chen Gong, Zhenzhe Zheng, Fan Wu, Yunfeng Shao, Bingshuai Li, Guihai Chen|; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China|Machine learning models have been deployed in mobile networks to deal with massive data from different layers to enable automated network management and intelligence on devices. To overcome high communication cost and severe privacy concerns of centralized machine learning, federated learning (FL) has been proposed to achieve distributed machine learning among networked devices. While the computation and communication limitation has been widely studied, the impact of on-device storage on the performance of FL is still not explored. Without an effective data selection policy to filter the massive streaming data on devices, classical FL can suffer from much longer model training time ($4\times$) and significant inference accuracy reduction ($7\%$), observed in our experiments. In this work, we take the first step to consider the online data selection for FL with limited on-device storage. We first define a new data valuation metric for data evaluation and selection in FL with theoretical guarantees for speeding up model convergence and enhancing final model accuracy, simultaneously. We further design {\ttfamily ODE}, a framework of \textbf{O}nline \textbf{D}ata s\textbf{E}lection for FL, to coordinate networked devices to store valuable data samples. Experimental results on one industrial dataset and three public datasets show the remarkable advantages of {\ttfamily ODE} over the state-of-the-art approaches. Particularly, on the industrial dataset, {\ttfamily ODE} achieves as high as $2.5\times$ speedup of training time and $6\%$ increase in inference accuracy, and is robust to various factors in practical environments.|机器学习模型已经部署在移动网络中，用于处理来自不同层次的大量数据，以实现设备上的自动网络管理和智能化。为了克服集中式机器学习的高通信成本和严重的隐私问题，提出了联邦学习(FL)来实现网络设备之间的分布式机器学习。虽然计算和通信的局限性已经被广泛研究，但是在设备上存储对 FL 性能的影响还没有被探讨。在我们的实验中观察到，如果没有有效的数据选择策略来过滤设备上的大量流数据，经典的 FL 可能会遭受更长的模型训练时间(4倍 $)和显着的推断准确性降低(7% $)。在这项工作中，我们采取的第一步考虑在线数据选择的 FL 与有限的设备上的存储。我们首先定义了一个新的用于 FL 中数据评估和选择的数据估值度量，同时为加快模型收敛速度和提高最终模型精度提供了理论保证。进一步设计了一个基于 textbf { O } nline textbf { D } ata’s textbf { E }选项的框架{ ttfamily ODE } ，用于协调网络设备以存储有价值的数据样本。在一个工业数据集和三个公共数据集上的实验结果表明，{ ttfamily ODE }方法比现有的方法具有显著的优势。特别是在工业数据集上，{ ttfamily ODE }达到了2.5倍的训练时间加速和6% 的推理精度提高，并且对实际环境中的各种因素具有鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=To+Store+or+Not?+Online+Data+Selection+for+Federated+Learning+with+Limited+Storage)|1|
|[Chain of Explanation: New Prompting Method to Generate Quality Natural Language Explanation for Implicit Hate Speech](https://doi.org/10.1145/3543873.3587320)|Fan Huang, Haewoon Kwak, Jisun An|Luddy School of Informatics, Computing, and Engineering, Indiana University Bloomington, USA|Recent studies have exploited advanced generative language models to generate Natural Language Explanations (NLE) for why a certain text could be hateful. We propose the Chain of Explanation (CoE) Prompting method, using the heuristic words and target group, to generate high-quality NLE for implicit hate speech. We improved the BLUE score from 44.0 to 62.3 for NLE generation by providing accurate target information. We then evaluate the quality of generated NLE using various automatic metrics and human annotations of informativeness and clarity scores.|最近的研究已经开发了先进的生成语言模型来产生自然语言解释(NLE)为什么某个文本可能是可恨的。提出了一种基于启发式词语和目标群的解释链提示方法，用于生成高质量的非线性语言环境。通过提供准确的目标信息，我们将 NLE 生成的 BLUE 评分从44.0提高到62.3。然后，我们使用各种自动指标和信息性和清晰度评分的人工注释来评估生成的 NLE 的质量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Chain+of+Explanation:+New+Prompting+Method+to+Generate+Quality+Natural+Language+Explanation+for+Implicit+Hate+Speech)|1|
|[NeuKron: Constant-Size Lossy Compression of Sparse Reorderable Matrices and Tensors](https://doi.org/10.1145/3543507.3583226)|Taehyung Kwon, Jihoon Ko, Jinhong Jung, Kijung Shin|Kim Jaechul Graduate School of AI, Korea Advanced Institute of Science and Technology, Republic of Korea; Department of Computer Science & Engineering, Jeonbuk National University, Republic of Korea|Many real-world data are naturally represented as a sparse reorderable matrix, whose rows and columns can be arbitrarily ordered (e.g., the adjacency matrix of a bipartite graph). Storing a sparse matrix in conventional ways requires an amount of space linear in the number of non-zeros, and lossy compression of sparse matrices (e.g., Truncated SVD) typically requires an amount of space linear in the number of rows and columns. In this work, we propose NeuKron for compressing a sparse reorderable matrix into a constant-size space. NeuKron generalizes Kronecker products using a recurrent neural network with a constant number of parameters. NeuKron updates the parameters so that a given matrix is approximated by the product and reorders the rows and columns of the matrix to facilitate the approximation. The updates take time linear in the number of non-zeros in the input matrix, and the approximation of each entry can be retrieved in logarithmic time. We also extend NeuKron to compress sparse reorderable tensors (e.g. multi-layer graphs), which generalize matrices. Through experiments on ten real-world datasets, we show that NeuKron is (a) Compact: requiring up to five orders of magnitude less space than its best competitor with similar approximation errors, (b) Accurate: giving up to 10x smaller approximation error than its best competitors with similar size outputs, and (c) Scalable: successfully compressing a matrix with over 230 million non-zero entries.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NeuKron:+Constant-Size+Lossy+Compression+of+Sparse+Reorderable+Matrices+and+Tensors)|1|
|[Hierarchical Knowledge Graph Learning Enabled Socioeconomic Indicator Prediction in Location-Based Social Network](https://doi.org/10.1145/3543507.3583239)|Zhilun Zhou, Yu Liu, Jingtao Ding, Depeng Jin, Yong Li|Tsinghua University, China|Socioeconomic indicators reflect location status from various aspects such as demographics, economy, crime and land usage, which play an important role in the understanding of location-based social networks (LBSNs). Especially, several existing works leverage multi-source data for socioeconomic indicator prediction in LBSNs, which however fail to capture semantic information as well as distil comprehensive knowledge therein. On the other hand, knowledge graph (KG), which distils semantic knowledge from multi-source data, has been popular in recent LBSN research, which inspires us to introduce KG for socioeconomic indicator prediction in LBSNs. Specifically, we first construct a location-based KG (LBKG) to integrate various kinds of knowledge from heterogeneous LBSN data, including locations and other related elements like point of interests (POIs), business areas as well as various relationships between them, such as spatial proximity and functional similarity. Then we propose a hierarchical KG learning model to capture both global knowledge from LBKG and domain knowledge from several sub-KGs. Extensive experiments on three datasets demonstrate our model’s superiority over state-of-the-art methods in socioeconomic indicators prediction. Our code is released at: https://github.com/tsinghua-fib-lab/KG-socioeconomic-indicator-prediction.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Knowledge+Graph+Learning+Enabled+Socioeconomic+Indicator+Prediction+in+Location-Based+Social+Network)|1|
|[Characterization of Simplicial Complexes by Counting Simplets Beyond Four Nodes](https://doi.org/10.1145/3543507.3583332)|Hyunju Kim, Jihoon Ko, Fanchen Bu, Kijung Shin|Kim Jaechul Graduate School of AI, Korea Advanced Institute of Science and Technology, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Republic of Korea|Simplicial complexes are higher-order combinatorial structures which have been used to represent real-world complex systems. In this paper, we concentrate on the local patterns in simplicial complexes called simplets, a generalization of graphlets. We formulate the problem of counting simplets of a given size in a given simplicial complex. For this problem, we extend a sampling algorithm based on color coding from graphs to simplicial complexes, with essential technical novelty. We theoretically analyze our proposed algorithm named SC3, showing its correctness, unbiasedness, convergence, and time/space complexity. Through the extensive experiments on sixteen real-world datasets, we show the superiority of SC3 in terms of accuracy, speed, and scalability, compared to the baseline methods. Finally, we use the counts given by SC3 for simplicial complex analysis, especially for characterization, which is further used for simplicial complex clustering, where SC3 shows a strong ability of characterization with domain-based similarity.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Characterization+of+Simplicial+Complexes+by+Counting+Simplets+Beyond+Four+Nodes)|1|
|[KRACL: Contrastive Learning with Graph Context Modeling for Sparse Knowledge Graph Completion](https://doi.org/10.1145/3543507.3583412)|Zhaoxuan Tan, Zilong Chen, Shangbin Feng, Qingyue Zhang, Qinghua Zheng, Jundong Li, Minnan Luo|Tsinghua University, China; University of Washington, USA; Xi'an Jiaotong University, China; University of Virginia, USA|Knowledge Graph Embeddings (KGE) aim to map entities and relations to low dimensional spaces and have become the \textit{de-facto} standard for knowledge graph completion. Most existing KGE methods suffer from the sparsity challenge, where it is harder to predict entities that appear less frequently in knowledge graphs. In this work, we propose a novel framework KRACL to alleviate the widespread sparsity in KGs with graph context and contrastive learning. Firstly, we propose the Knowledge Relational Attention Network (KRAT) to leverage the graph context by simultaneously projecting neighboring triples to different latent spaces and jointly aggregating messages with the attention mechanism. KRAT is capable of capturing the subtle semantic information and importance of different context triples as well as leveraging multi-hop information in knowledge graphs. Secondly, we propose the knowledge contrastive loss by combining the contrastive loss with cross entropy loss, which introduces more negative samples and thus enriches the feedback to sparse entities. Our experiments demonstrate that KRACL achieves superior results across various standard knowledge graph benchmarks, especially on WN18RR and NELL-995 which have large numbers of low in-degree entities. Extensive experiments also bear out KRACL's effectiveness in handling sparse knowledge graphs and robustness against noisy triples.|知识图嵌入技术(KGE)旨在将实体和关系映射到低维空间，已经成为知识图形完成的文本标准。大多数现有的 KGE 方法都受到稀疏性挑战的影响，在稀疏性挑战中，很难预测知识图中出现频率较低的实体。在这项工作中，我们提出了一个新的框架 KRACL，以缓解广泛的稀疏性幼儿园图形上下文和对比学习。首先，我们提出了知识关系注意网络(KRAT) ，通过将相邻的三元组同时投影到不同的潜在空间，并利用注意机制联合聚集信息，从而利用图的上下文。KRAT 能够捕捉不同上下文三元组的微妙语义信息和重要性，并利用知识图表中的多跳信息。其次，将对比损失和交叉熵损失相结合，提出了知识对比损失，引入了更多的负样本，丰富了对稀疏实体的反馈。我们的实验表明，KRACL 在不同的标准知识图基准测试中，特别是在 WN18RR 和 NELL-995上，获得了优越的结果，这两个基准测试都有大量的低度实体。广泛的实验也证明了 KRACL 在处理稀疏知识图和抗噪声三元组鲁棒性方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KRACL:+Contrastive+Learning+with+Graph+Context+Modeling+for+Sparse+Knowledge+Graph+Completion)|1|
|[Migration Reframed? A multilingual analysis on the stance shift in Europe during the Ukrainian crisis](https://doi.org/10.1145/3543507.3583442)|Sergej Wildemann, Claudia Niederée, Erick Elejalde|L3S Research Center, Leibniz Universität Hannover, Germany|The war in Ukraine seems to have positively changed the attitude toward the critical societal topic of migration in Europe -- at least towards refugees from Ukraine. We investigate whether this impression is substantiated by how the topic is reflected in online news and social media, thus linking the representation of the issue on the Web to its perception in society. For this purpose, we combine and adapt leading-edge automatic text processing for a novel multilingual stance detection approach. Starting from 5.5M Twitter posts published by 565 European news outlets in one year, beginning September 2021, plus replies, we perform a multilingual analysis of migration-related media coverage and associated social media interaction for Europe and selected European countries. The results of our analysis show that there is actually a reframing of the discussion illustrated by the terminology change, e.g., from "migrant" to "refugee", often even accentuated with phrases such as "real refugees". However, concerning a stance shift in public perception, the picture is more diverse than expected. All analyzed cases show a noticeable temporal stance shift around the start of the war in Ukraine. Still, there are apparent national differences in the size and stability of this shift.|乌克兰的战争似乎积极地改变了人们对欧洲移民这一关键社会话题的态度——至少是对乌克兰难民的态度。我们调查这种印象是否被在线新闻和社交媒体所反映的主题所证实，从而将问题在网络上的表现与其在社会中的感知联系起来。为此，我们将前沿自动文本处理技术结合起来，提出了一种新的多语言姿态检测方法。从2021年9月开始，565家欧洲新闻机构在一年内发布了550万条 Twitter 帖子，再加上回复，我们对欧洲和选定的欧洲国家的移民相关媒体报道和相关社交媒体互动进行了多语言分析。我们的分析结果表明，术语的变化，例如从“移民”到“难民”，甚至常常用“真正的难民”这样的短语来强调，实际上是对讨论的重新构建。然而，关于公众看法的立场转变，情况比预期的更加多样化。所有分析的案例都表明，在乌克兰战争开始前后，人们的立场发生了明显的时间转变。尽管如此，各国在这种转变的规模和稳定性方面仍存在明显差异。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Migration+Reframed?+A+multilingual+analysis+on+the+stance+shift+in+Europe+during+the+Ukrainian+crisis)|1|
|[Multitask Peer Prediction With Task-dependent Strategies](https://doi.org/10.1145/3543507.3583292)|Yichi Zhang, Grant Schoenebeck|University of Michigan, USA|Peer prediction aims to incentivize truthful reports from agents whose reports cannot be assessed with any objective ground truthful information. In the multi-task setting where each agent is asked multiple questions, a sequence of mechanisms have been proposed which are truthful — truth-telling is guaranteed to be an equilibrium, or even better, informed truthful — truth-telling is guaranteed to be one of the best-paid equilibria. However, these guarantees assume agents’ strategies are restricted to be task-independent: an agent’s report on a task is not affected by her information about other tasks. We provide the first discussion on how to design (informed) truthful mechanisms for task-dependent strategies, which allows the agents to report based on all her information on the assigned tasks. We call such stronger mechanisms (informed) omni-truthful. In particular, we propose the joint-disjoint task framework, a new paradigm which builds upon the previous penalty-bonus task framework. First, we show a natural reduction from mechanisms in the penalty-bonus task framework to mechanisms in the joint-disjoint task framework that maps every truthful mechanism to an omni-truthful mechanism. Such a reduction is non-trivial as we show that current penalty-bonus task mechanisms are not, in general, omni-truthful. Second, for a stronger truthful guarantee, we design the matching agreement (MA) mechanism which is informed omni-truthful. Finally, for the MA mechanism in the detail-free setting where no prior knowledge is assumed, we show how many tasks are required to (approximately) retain the truthful guarantees.|同伴预测的目的是激励那些报告不能用任何客观的真实信息进行评估的代理人的真实报告。在多任务环境中，每个主体被问到多个问题，一系列的机制被提出，这些机制是真实的ーー讲真话被保证是一种均衡，甚至更好，知情的真话ーー讲真话被保证是一种收入最高的均衡。然而，这些保证假设代理的策略被限制为独立于任务: 代理关于任务的报告不受她关于其他任务的信息的影响。我们首先讨论了如何为任务依赖策略设计(知情的)真实机制，使得代理人能够根据她对指定任务的所有信息进行报告。我们称这种更强大的机制(知情的)为全面真实。特别地，我们提出了联合-分离任务框架，这是一个建立在以前惩罚-奖励任务框架基础上的新范式。首先，我们展示了从惩罚-奖励任务框架中的机制到联合-不相交任务框架中的机制的自然还原，该机制映射每个真实机制到一个全真机制。这种减少是不平凡的，因为我们表明，目前的惩罚奖励任务机制，一般来说，不是全面真实的。其次，为了获得更强的真实性保证，我们设计了全真信息匹配协议(MA)机制。最后，对于无细节设置且不假设先验知识的 MA 机制，我们展示了需要多少任务才能(近似地)保持真实性保证。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multitask+Peer+Prediction+With+Task-dependent+Strategies)|1|
|[High-Effort Crowds: Limited Liability via Tournaments](https://doi.org/10.1145/3543507.3583334)|Yichi Zhang, Grant Schoenebeck|School of Information, University of Michigan, USA|We consider the crowdsourcing setting where, in response to the assigned tasks, agents strategically decide both how much effort to exert (from a continuum) and whether to manipulate their reports. The goal is to design payment mechanisms that (1) satisfy limited liability (all payments are non-negative), (2) reduce the principal’s cost of budget, (3) incentivize effort and (4) incentivize truthful responses. In our framework, the payment mechanism composes a performance measurement, which noisily evaluates agents’ effort based on their reports, and a payment function, which converts the scores output by the performance measurement to payments. Previous literature suggests applying a peer prediction mechanism combined with a linear payment function. This method can achieve either (1), (3) and (4), or (2), (3) and (4) in the binary effort setting. In this paper, we suggest using a rank-order payment function (tournament). Assuming Gaussian noise, we analytically optimize the rank-order payment function, and identify a sufficient statistic, sensitivity, which serves as a metric for optimizing the performance measurements. This helps us obtain (1), (2) and (3) simultaneously. Additionally, we show that adding noise to agents’ scores can preserve the truthfulness of the performance measurements under the non-linear tournament, which gives us all four objectives. Our real-data estimated agent-based model experiments show that our method can greatly reduce the payment of effort elicitation while preserving the truthfulness of the performance measurement. In addition, we empirically evaluate several commonly used performance measurements in terms of their sensitivities and strategic robustness.|我们考虑众包的设置，在这个设置中，为了响应分配的任务，代理人战略性地决定要付出多少努力(从一个连续体)和是否操纵他们的报告。我们的目标是设计支付机制: (1)满足有限责任(所有支付都是非负数的) ，(2)降低本金的预算成本，(3)激励努力，(4)激励诚实的回应。在我们的框架中，支付机制包括一个绩效度量，它基于代理人的报告对代理人的努力进行嘈杂的评估，以及一个支付函数，它将绩效度量的得分输出转换为支付。以前的文献建议应用同行预测机制结合线性支付函数。该方法可以在二进制努力设置中实现(1)、(3)和(4)或(2)、(3)和(4)。在本文中，我们建议使用等级顺序支付函数(锦标赛)。假设高斯噪声，我们分析优化秩序支付函数，并确定了一个充分的统计，灵敏度，作为一个度量优化性能测量。这有助于我们同时获得(1)、(2)和(3)。此外，我们还发现，在非线性竞争下，给代理人的分数增加噪声可以保持性能测量的真实性，这给了我们四个目标。我们的实际数据估计个体为本模型实验表明，我们的方法可以大大减少努力诱发的支付，同时保持绩效测量的真实性。此外，我们根据灵敏度和战略稳健性对几个常用的性能测量进行了实证评估。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=High-Effort+Crowds:+Limited+Liability+via+Tournaments)|1|
|[Knowledge-infused Contrastive Learning for Urban Imagery-based Socioeconomic Prediction](https://doi.org/10.1145/3543507.3583876)|Yu Liu, Xin Zhang, Jingtao Ding, Yanxin Xi, Yong Li|Tsinghua University, China; University of Helsinki, Finland|Monitoring sustainable development goals requires accurate and timely socioeconomic statistics, while ubiquitous and frequently-updated urban imagery in web like satellite/street view images has emerged as an important source for socioeconomic prediction. Especially, recent studies turn to self-supervised contrastive learning with manually designed similarity metrics for urban imagery representation learning and further socioeconomic prediction, which however suffers from effectiveness and robustness issues. To address such issues, in this paper, we propose a Knowledge-infused Contrastive Learning (KnowCL) model for urban imagery-based socioeconomic prediction. Specifically, we firstly introduce knowledge graph (KG) to effectively model the urban knowledge in spatiality, mobility, etc., and then build neural network based encoders to learn representations of an urban image in associated semantic and visual spaces, respectively. Finally, we design a cross-modality based contrastive learning framework with a novel image-KG contrastive loss, which maximizes the mutual information between semantic and visual representations for knowledge infusion. Extensive experiments of applying the learnt visual representations for socioeconomic prediction on three datasets demonstrate the superior performance of KnowCL with over 30\% improvements on $R^2$ compared with baselines. Especially, our proposed KnowCL model can apply to both satellite and street imagery with both effectiveness and transferability achieved, which provides insights into urban imagery-based socioeconomic prediction.|监测可持续发展目标需要准确和及时的社会经济统计数据，而无处不在和经常更新的城市图像，如卫星/街景图像，已成为社会经济预测的一个重要来源。特别是近年来，城市图像表征学习和进一步的社会经济预测的研究主要集中在自监督对比学习和人工设计的相似度量上，但这种方法存在有效性和鲁棒性的问题。为了解决这些问题，本文提出了一个基于知识注入的对比学习(KnowCL)模型，用于基于图像的城市社会经济预测。首先引入知识图(KG)对城市知识进行空间、流动性等方面的有效建模，然后构建基于神经网络的编码器，分别学习相关语义空间和视觉空间中城市图像的表示。最后，我们设计了一个基于交叉模态的对比学习框架，该框架具有一种新的图像对比度损失—— KG 对比度损失，最大化了语义表征和视觉表征之间的相互信息，实现了知识的输入。在三个数据集上应用学习可视化表示进行社会经济预测的广泛实验表明，与基线相比，KnowCL 的性能优越，在 $R ^ 2 $上有超过30% 的改善。特别是，我们提出的 KnowCL 模型可以同时应用于卫星和街道图像，实现了有效性和可转移性，为基于图像的城市社会经济预测提供了见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge-infused+Contrastive+Learning+for+Urban+Imagery-based+Socioeconomic+Prediction)|1|
|[Dynamic Embedding-based Retrieval for Personalized Item Recommendations at Instacart](https://doi.org/10.1145/3543873.3587668)|Chuanwei Ruan, Allan Stewart, Han Li, Ryan Ye, David Vengerov, Haixun Wang|Instacart, USA|Personalization is essential in e-commerce, with item recommendation as a critical task. In this paper, we describe a hybrid embedding-based retrieval system for real-time personalized item recommendations on Instacart. Our system addresses unique challenges in the multi-source retrieval system, and includes several key components to make it highly personalized and dynamic. Specifically, our system features a hybrid embedding model that includes a long-term user interests embedding model and a real-time session-based model, which are combined to capture users’ immediate intents and historical interactions. Additionally, we have developed a contextual bandit solution to dynamically adjust the number of candidates from each source and optimally allocate retrieval slots given a limited computational budget. Our modeling and system optimization efforts have enabled us to provide highly personalized item recommendations in real-time at scale to all our customers, including new and long-standing users.|个性化在电子商务中是必不可少的，项目推荐是一项关键任务。本文描述了一个基于嵌入的混合检索系统，用于 Instacart 上的实时个性化项目推荐。我们的系统解决了多源检索系统中的独特挑战，并包含了几个关键组件，使其具有高度的个性化和动态性。具体来说，我们的系统采用混合嵌入模型，包括长期用户兴趣嵌入模型和基于实时会话的嵌入模型，它们结合起来捕获用户的直接意图和历史交互。此外，我们已经开发了一个上下文盗贼解决方案来动态调整每个来源的候选人数量，并在有限的计算预算下优化分配检索时隙。我们的建模和系统优化工作，使我们能够提供高度个性化的项目推荐的实时规模，我们的所有客户，包括新的和长期的用户。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Embedding-based+Retrieval+for+Personalized+Item+Recommendations+at+Instacart)|0|
|[A Multi-Granularity Matching Attention Network for Query Intent Classification in E-commerce Retrieval](https://doi.org/10.1145/3543873.3584639)|Chunyuan Yuan, Yiming Qiu, Mingming Li, Haiqing Hu, Songlin Wang, Sulong Xu|JD.com, Beijing, China, China|Query intent classification, which aims at assisting customers to find desired products, has become an essential component of the e-commerce search. Existing query intent classification models either design more exquisite models to enhance the representation learning of queries or explore label-graph and multi-task to facilitate models to learn external information. However, these models cannot capture multi-granularity matching features from queries and categories, which makes them hard to mitigate the gap in the expression between informal queries and categories. This paper proposes a Multi-granularity Matching Attention Network (MMAN), which contains three modules: a self-matching module, a char-level matching module, and a semantic-level matching module to comprehensively extract features from the query and a query-category interaction matrix. In this way, the model can eliminate the difference in expression between queries and categories for query intent classification. We conduct extensive offline and online A/B experiments, and the results show that the MMAN significantly outperforms the strong baselines, which shows the superiority and effectiveness of MMAN. MMAN has been deployed in production and brings great commercial value for our company.|查询意图分类已经成为电子商务搜索的一个重要组成部分，其目的是帮助客户找到期望的产品。现有的查询意图分类模型要么设计更精细的模型来增强查询的表示学习，要么探索标签图和多任务来促进模型学习外部信息。然而，这些模型不能从查询和类别中捕获多粒度匹配特性，这使得它们很难缩小非正式查询和类别之间的表达差距。提出了一种多粒度匹配注意网络(MMAN)模型，该模型包括三个模块: 自匹配模块、字符级匹配模块和语义级匹配模块。通过这种方式，该模型可以消除查询和类别之间在查询意图分类方面的表达差异。我们进行了大量的离线和在线 A/B 实验，结果表明，MMAN 的性能明显优于强基线，显示了 MMAN 的优越性和有效性。MMAN 已投入生产，为公司带来了巨大的商业价值。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Multi-Granularity+Matching+Attention+Network+for+Query+Intent+Classification+in+E-commerce+Retrieval)|0|
|[Divide and Conquer: Towards Better Embedding-based Retrieval for Recommender Systems from a Multi-task Perspective](https://doi.org/10.1145/3543873.3584629)|Yuan Zhang, Xue Dong, Weijie Ding, Biao Li, Peng Jiang, Kun Gai|Shandong University, China; Kuaishou Technology, China; Unaffiliated, China|Embedding-based retrieval (EBR) methods are widely used in modern recommender systems thanks to its simplicity and effectiveness. However, along the journey of deploying and iterating on EBR in production, we still identify some fundamental issues in existing methods. First, when dealing with large corpus of candidate items, EBR models often have difficulties in balancing the performance on distinguishing highly relevant items (positives) from both irrelevant ones (easy negatives) and from somewhat related yet not competitive ones (hard negatives). Also, we have little control in the diversity and fairness of the retrieval results because of the ``greedy'' nature of nearest vector search. These issues compromise the performance of EBR methods in large-scale industrial scenarios. This paper introduces a simple and proven-in-production solution to overcome these issues. The proposed solution takes a divide-and-conquer approach: the whole set of candidate items are divided into multiple clusters and we run EBR to retrieve relevant candidates from each cluster in parallel; top candidates from each cluster are then combined by some controllable merging strategies. This approach allows our EBR models to only concentrate on discriminating positives from mostly hard negatives. It also enables further improvement from a multi-tasking learning (MTL) perspective: retrieval problems within each cluster can be regarded as individual tasks; inspired by recent successes in prompting and prefix-tuning, we propose an efficient task adaption technique further boosting the retrieval performance within each cluster with negligible overheads.|嵌入式检索方法以其简单有效的特点在现代推荐系统中得到了广泛的应用。然而，在生产中部署和迭代 EBR 的过程中，我们仍然发现了现有方法中的一些基本问题。首先，在处理大量候选项目时，EBR 模型往往难以平衡区分高度相关项目(正面)和无关项目(简单负面)以及有些相关但没有竞争性的项目(硬负面)。此外，由于最近向量搜索的“贪婪”特性，我们对检索结果的多样性和公平性几乎没有控制。这些问题影响了 EBR 方法在大规模工业场景中的性能。本文介绍了一个简单且已经在生产中得到验证的解决方案来克服这些问题。该解决方案采用分而治之的方法: 将整个候选项集划分为多个集群，并运行 EBR 并行地从每个集群中检索相关候选项; 然后通过一些可控的合并策略将每个集群中的最优候选项集合起来。这种方法允许我们的 EBR 模型只集中于区分正面和大多数硬负面。它还能从多任务学习(MTL)的角度进一步改进: 每个集群中的检索问题可以被视为单个任务; 受最近在提示和前缀调优方面的成功启发，我们提出了一种有效的任务适应技术，进一步提高了每个集群中的检索性能，开销可以忽略不计。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Divide+and+Conquer:+Towards+Better+Embedding-based+Retrieval+for+Recommender+Systems+from+a+Multi-task+Perspective)|0|
|[Expressive user embedding from churn and recommendation multi-task learning](https://doi.org/10.1145/3543873.3587306)|Huajun Bai, Davide Liu, Thomas Hirtz, Alexandre Boulenger|Tsinghua University, China; Genify, China; Genify, United Arab Emirates|In this paper, we present a Multi-Task model for Recommendation and Churn prediction (MT) in the retail banking industry. The model leverages a hard parameter-sharing framework and consists of a shared multi-stack encoder with multi-head self-attention and two fully connected task heads. It is trained to achieve two multi-class classification tasks: predicting product churn and identifying the next-best products (NBP) for users, individually. Our experiments demonstrate the superiority of the multi-task model compared to its single-task versions, reaching top-1 precision at 78.1% and 77.6%, for churn and NBP prediction respectively. Moreover, we find that the model learns a coherent and expressive high-level representation reflecting user intentions related to both tasks. There is a clear separation between users with acquisitions and users with churn. In addition, acquirers are more tightly clustered compared to the churners. The gradual separability of churning and acquiring users, who diverge in intent, is a desirable property. It provides a basis for model explainability, critical to industry adoption, and also enables other downstream applications. These potential additional benefits, beyond reducing customer attrition and increasing product use–two primary concerns of businesses, make such a model even more valuable.|本文提出了一个零售银行业推荐和流失预测的多任务模型。该模型利用一个硬参数共享框架，由一个具有多头自注意的共享多栈编码器和两个完全连接的任务头组成。它被训练以完成两个多类别的分类任务: 预测产品流失和为用户分别识别次优产品(NBP)。我们的实验证明了多任务模型相对于单任务模型的优越性，在流失预测和 NBP 预测方面分别达到了78.1% 和77.6% 的 Top-1精度。此外，我们发现该模型学习了一个连贯的和表达的高层次表示，反映了与两个任务相关的用户意图。并购用户和流失用户之间有明显的区别。此外，与搅拌器相比，收购者更紧密地聚集在一起。搅动用户和获取用户的逐渐可分性，这是一个可取的特性，因为用户的意图不同。它为模型的可解释性提供了基础，对于工业的采用至关重要，并且还支持其他下游应用程序。这些潜在的额外好处，除了减少客户流失和增加产品使用(企业的两个主要关注点)之外，使得这种模式更加有价值。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Expressive+user+embedding+from+churn+and+recommendation+multi-task+learning)|0|
|[Continual Transfer Learning for Cross-Domain Click-Through Rate Prediction at Taobao](https://doi.org/10.1145/3543873.3584625)|Lixin Liu, Yanling Wang, Tianming Wang, Dong Guan, Jiawei Wu, Jingxu Chen, Rong Xiao, Wenxiang Zhu, Fei Fang|Alibaba Group, China; Renmin University of China, China; Alibaba group, China|As one of the largest e-commerce platforms in the world, Taobao's recommendation systems (RSs) serve the demands of shopping for hundreds of millions of customers. Click-Through Rate (CTR) prediction is a core component of the RS. One of the biggest characteristics in CTR prediction at Taobao is that there exist multiple recommendation domains where the scales of different domains vary significantly. Therefore, it is crucial to perform cross-domain CTR prediction to transfer knowledge from large domains to small domains to alleviate the data sparsity issue. However, existing cross-domain CTR prediction methods are proposed for static knowledge transfer, ignoring that all domains in real-world RSs are continually time-evolving. In light of this, we present a necessary but novel task named Continual Transfer Learning (CTL), which transfers knowledge from a time-evolving source domain to a time-evolving target domain. In this work, we propose a simple and effective CTL model called CTNet to solve the problem of continual cross-domain CTR prediction at Taobao, and CTNet can be trained efficiently. Particularly, CTNet considers an important characteristic in the industry that models has been continually well-trained for a very long time. So CTNet aims to fully utilize all the well-trained model parameters in both source domain and target domain to avoid losing historically acquired knowledge, and only needs incremental target domain data for training to guarantee efficiency. Extensive offline experiments and online A/B testing at Taobao demonstrate the efficiency and effectiveness of CTNet. CTNet is now deployed online in the recommender systems of Taobao, serving the main traffic of hundreds of millions of active users.|作为世界上最大的电子商务平台之一，淘宝的推荐系统(RS)为数以亿计的顾客提供购物服务。点进率预测是遥感的核心组成部分。淘宝网点击率预测的最大特点之一是存在多个推荐域，不同域的规模差异很大。因此，进行跨域 CTR 预测，将知识从大域转移到小域，以缓解数据稀疏性问题至关重要。然而，现有的跨域 CTR 预测方法都是针对静态知识转移而提出的，忽略了现实 RSS 中的所有域都是不断时间演化的。鉴于此，我们提出了一个必要的，但新颖的任务称为连续转移学习(CTL) ，它将知识从一个时间演化的源领域转移到一个时间演化的目标领域。本文提出了一种简单有效的 CTL 模型 CTNet 来解决淘宝网连续跨域 CTR 预测问题，可以有效地训练 CTNet。特别是，CTNet 认为模特行业的一个重要特征是模特长期以来一直受到良好的培训。因此，CTNet 的目标是充分利用源域和目标域中所有训练有素的模型参数，避免丢失历史获得的知识，只需要增量的目标域数据进行训练，以保证训练效率。在淘宝上的大量离线实验和在线 A/B 测试证明了 CTNet 的效率和有效性。CTNet 现已部署在淘宝网的推荐系统中，为数亿活跃用户的主要流量提供服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Continual+Transfer+Learning+for+Cross-Domain+Click-Through+Rate+Prediction+at+Taobao)|0|
|[MAKE: Vision-Language Pre-training based Product Retrieval in Taobao Search](https://doi.org/10.1145/3543873.3584627)|Xiaoyang Zheng, Zilong Wang, Sen Li, Ke Xu, Tao Zhuang, Qingwen Liu, Xiaoyi Zeng|; Alibaba Group, China|Taobao Search consists of two phases: the retrieval phase and the ranking phase. Given a user query, the retrieval phase returns a subset of candidate products for the following ranking phase. Recently, the paradigm of pre-training and fine-tuning has shown its potential in incorporating visual clues into retrieval tasks. In this paper, we focus on solving the problem of text-to-multimodal retrieval in Taobao Search. We consider that users' attention on titles or images varies on products. Hence, we propose a novel Modal Adaptation module for cross-modal fusion, which helps assigns appropriate weights on texts and images across products. Furthermore, in e-commerce search, user queries tend to be brief and thus lead to significant semantic imbalance between user queries and product titles. Therefore, we design a separate text encoder and a Keyword Enhancement mechanism to enrich the query representations and improve text-to-multimodal matching. To this end, we present a novel vision-language (V+L) pre-training methods to exploit the multimodal information of (user query, product title, product image). Extensive experiments demonstrate that our retrieval-specific pre-training model (referred to as MAKE) outperforms existing V+L pre-training methods on the text-to-multimodal retrieval task. MAKE has been deployed online and brings major improvements on the retrieval system of Taobao Search.|淘宝搜索包括两个阶段: 检索阶段和排名阶段。给定一个用户查询，检索阶段返回下一个排序阶段的候选产品的子集。最近，预先训练和微调的范式已经显示了其在将视觉线索纳入检索任务方面的潜力。本文主要研究淘宝搜索中文本到多模式检索的问题。我们认为用户对标题或图片的关注因产品而异。因此，我们提出了一个新的模态适应模块的跨模态融合，这有助于分配适当的权重的文本和图像跨产品。此外，在电子商务搜索中，用户查询往往是简短的，从而导致用户查询和产品标题之间的语义严重失衡。因此，我们设计了一个单独的文本编码器和一个关键字增强机制，以丰富查询表示和改善文本到多模式匹配。为此，我们提出了一种新的视觉语言(V + L)预训练方法来利用多模态信息(用户查询、产品标题、产品图像)。大量的实验表明，我们的检索特定的预训练模型(简称 MAKE)在文本到多模态检索任务上优于现有的 V + L 预训练方法。MAKE 已经在线部署，并对淘宝搜索的检索系统进行了重大改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MAKE:+Vision-Language+Pre-training+based+Product+Retrieval+in+Taobao+Search)|0|
|[HAPENS: Hardness-Personalized Negative Sampling for Implicit Collaborative Filtering](https://doi.org/10.1145/3543873.3584631)|Haoxin Liu, Pu Zhao, Si Qin, Yong Shi, Mirror Xu, Qingwei Lin, Dongmei Zhang|Microsoft Bing, China; Microsoft Research, China|For training implicit collaborative filtering (ICF) models, hard negative sampling (HNS) has become a state-of-the-art solution for obtaining negative signals from massive uninteracted items. However, selecting appropriate hardness levels for personalized recommendations remains a fundamental, yet underexplored, problem. Previous HNS works have primarily adjusted the hardness level by tuning a single hyperparameter. However, applying the same hardness level to each user is unsuitable due to varying user behavioral characteristics, the quantity and quality of user records, and different consistencies of models’ inductive biases. Moreover, increasing the number of hyperparameters is not practical due to the massive number of users. To address this important and challenging problem, we propose a model-agnostic and practical approach called hardness-personalized negative sampling (HAPENS). HAPENS uses a two-stage approach: in stage one, it trains the ICF model with a customized objective function that optimizes its worst performance on each user’s interacted item set. In stage two, it utilizes these worst performances as personalized hardness levels with a well-designed sampling distribution, and trains the final model with the same architecture. We evaluated HAPENS on the collected Bing advertising dataset and one public dataset, and the comprehensive experimental results demonstrate its robustness and superiority. Moreover, HAPENS has delivered significant benefits to the Bing advertising system. To the best of our knowledge, we are the first to study this important and challenging problem.|对于训练内隐协同过滤模型(ICF) ，硬负采样(hNS)已成为从大量未交互项目中获取负信号的最新解决方案。然而，为个性化推荐选择合适的硬度水平仍然是一个基本的、尚未得到充分探索的问题。以往的 HNS 工作主要是通过调整单个超参数来调整硬度水平。然而，由于不同的用户行为特征、用户记录的数量和质量以及模型归纳偏差的不同一致性，对每个用户应用相同的硬度水平是不合适的。此外，由于用户数量庞大，增加超参数的数量是不切实际的。为了解决这一重要而具有挑战性的问题，我们提出了一种模型不可知的实用方法，称为硬度个性化阴性采样(HAPENS)。HAPENS 使用两阶段的方法: 在第一阶段，它使用一个定制的目标函数来训练 ICF 模型，该目标函数在每个用户的交互项集上优化其最差的性能。在第二阶段，它利用这些最差的性能作为个性化的硬度水平，具有设计良好的采样分布，并训练最终模型具有相同的架构。我们对搜集到的 Bing 广告数据集和一个公共数据集进行了 HAPENS 评估，综合实验结果表明了 HAPENS 的鲁棒性和优越性。此外，HAPENS 为必应广告系统带来了巨大的好处。据我们所知，我们是第一个研究这个重要而富有挑战性的问题的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HAPENS:+Hardness-Personalized+Negative+Sampling+for+Implicit+Collaborative+Filtering)|0|
|[Que2Engage: Embedding-based Retrieval for Relevant and Engaging Products at Facebook Marketplace](https://doi.org/10.1145/3543873.3584633)|Yunzhong He, Yuxin Tian, Mengjiao Wang, Feier Chen, Licheng Yu, Maolong Tang, Congcong Chen, Ning Zhang, Bin Kuang, Arul Prakash|Meta, USA; University of California, Merced, USA|Embedding-based Retrieval (EBR) in e-commerce search is a powerful search retrieval technique to address semantic matches between search queries and products. However, commercial search engines like Facebook Marketplace Search are complex multi-stage systems optimized for multiple business objectives. At Facebook Marketplace, search retrieval focuses on matching search queries with relevant products, while search ranking puts more emphasis on contextual signals to up-rank the more engaging products. As a result, the end-to-end searcher experience is a function of both relevance and engagement, and the interaction between different stages of the system. This presents challenges to EBR systems in order to optimize for better searcher experiences. In this paper we presents Que2Engage, a search EBR system built towards bridging the gap between retrieval and ranking for end-to-end optimizations. Que2Engage takes a multimodal & multitask approach to infuse contextual information into the retrieval stage and to balance different business objectives. We show the effectiveness of our approach via a multitask evaluation framework and thorough baseline comparisons and ablation studies. Que2Engage is deployed on Facebook Marketplace Search and shows significant improvements in searcher engagement in two weeks of A/B testing.|电子商务搜索中的嵌入式检索(EBR)是解决搜索查询与产品之间语义匹配的一种强有力的检索技术。然而，像 Facebook Marketplace Search 这样的商业搜索引擎是为多个业务目标而优化的复杂的多阶段系统。在 Facebook Marketplace，搜索检索侧重于将搜索查询与相关产品进行匹配，而搜索排名更侧重于上下文信号，以提升更具吸引力的产品的排名。因此，端到端的搜索体验是相关性和参与度的函数，以及系统不同阶段之间的相互作用。这对 EBR 系统提出了挑战，以便优化更好的搜索体验。本文介绍了 Que2Engage，这是一个搜索 EBR 系统，旨在弥合检索和排序之间的差距，以实现端到端的优化。Que2Engage 采用多模态和多任务的方法将上下文信息注入检索阶段，并平衡不同的业务目标。我们通过一个多任务评估框架和彻底的基线比较和消融研究来展示我们的方法的有效性。Que2Engage 部署在 Facebook Marketplace Search 上，并在两周的 A/B 测试中显示出搜索者参与度的显著改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Que2Engage:+Embedding-based+Retrieval+for+Relevant+and+Engaging+Products+at+Facebook+Marketplace)|0|
|[Learning Multi-Stage Multi-Grained Semantic Embeddings for E-Commerce Search](https://doi.org/10.1145/3543873.3584638)|Binbin Wang, Mingming Li, Zhixiong Zeng, Jingwei Zhuo, Songlin Wang, Sulong Xu, Bo Long, Weipeng Yan|JD.com, China|Retrieving relevant items that match users' queries from billion-scale corpus forms the core of industrial e-commerce search systems, in which embedding-based retrieval (EBR) methods are prevailing. These methods adopt a two-tower framework to learn embedding vectors for query and item separately and thus leverage efficient approximate nearest neighbor (ANN) search to retrieve relevant items. However, existing EBR methods usually ignore inconsistent user behaviors in industrial multi-stage search systems, resulting in insufficient retrieval efficiency with a low commercial return. To tackle this challenge, we propose to improve EBR methods by learning Multi-level Multi-Grained Semantic Embeddings(MMSE). We propose the multi-stage information mining to exploit the ordered, clicked, unclicked and random sampled items in practical user behavior data, and then capture query-item similarity via a post-fusion strategy. We then propose multi-grained learning objectives that integrate the retrieval loss with global comparison ability and the ranking loss with local comparison ability to generate semantic embeddings. Both experiments on a real-world billion-scale dataset and online A/B tests verify the effectiveness of MMSE in achieving significant performance improvements on metrics such as offline recall and online conversion rate (CVR).|基于嵌入式检索(EBR)方法是工业电子商务搜索系统的核心，它可以从数十亿规模的语料库中检索出与用户查询相匹配的相关项目。这些方法采用双塔架构，分别学习查询和项目的嵌入向量，从而利用有效的近似最近邻(ANN)搜索来检索相关项目。然而，现有的 EBR 方法往往忽略了工业多阶段搜索系统中不一致的用户行为，导致检索效率不足，商业收益较低。为了解决这一问题，我们提出通过学习多级多粒度语义嵌入(MMSE)来改进 EBR 方法。提出了一种基于多阶段信息挖掘的方法，利用实际用户行为数据中的有序、点击、未点击和随机抽样条目，通过后融合策略获取查询条目的相似性。然后提出多粒度学习目标，将检索损失与全局比较能力、排序损失与局部比较能力相结合，生成语义嵌入。在真实世界的十亿级数据集上的实验和在线 A/B 测试都验证了 MMSE 在离线召回率和在线转换率(CVR)等指标上实现显著性能改进的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Multi-Stage+Multi-Grained+Semantic+Embeddings+for+E-Commerce+Search)|0|
|[CAM2: Conformity-Aware Multi-Task Ranking Model for Large-Scale Recommender Systems](https://doi.org/10.1145/3543873.3584657)|Ameya Raul, Amey Porobo Dharwadker, Brad Schumitsch|Meta Inc., USA|Learning large-scale industrial recommender system models by fitting them to historical user interaction data makes them vulnerable to conformity bias. This may be due to a number of factors, including the fact that user interests may be difficult to determine and that many items are often interacted with based on ecosystem factors other than their relevance to the individual user. In this work, we introduce CAM2, a conformity-aware multi-task ranking model to serve relevant items to users on one of the largest industrial recommendation platforms. CAM2 addresses these challenges systematically by leveraging causal modeling to disentangle users' conformity to popular items from their true interests. This framework is generalizable and can be scaled to support multiple representations of conformity and user relevance in any large-scale recommender system. We provide deeper practical insights and demonstrate the effectiveness of the proposed model through improvements in offline evaluation metrics compared to our production multi-task ranking model. We also show through online experiments that the CAM2 model results in a significant 0.50% increase in aggregated user engagement, coupled with a 0.21% increase in daily active users on Facebook Watch, a popular video discovery and sharing platform serving billions of users.|通过将大规模工业推荐系统模型与历史用户交互数据进行拟合，使其容易受到一致性偏差的影响。这可能是由于若干因素，包括用户的兴趣可能难以确定，而且许多项目往往基于生态系统因素而不是它们与个别用户的相关性进行交互。在这项工作中，我们介绍了 CAM2，一个整合意识的多任务排序模型，以服务于相关项目的用户在一个最大的行业推荐平台。CAM2通过利用因果建模系统地解决这些挑战，从用户的真实兴趣中分离出用户对流行项目的一致性。这个框架是可以推广的，可以扩展到支持任何大规模推荐系统中的多种一致性和用户相关性的表示。我们提供了更深入的实践见解，并证明了该模型的有效性，通过改进离线评估指标相比，我们的生产多任务排序模型。我们还通过在线实验表明，CAM2模型显著增加了0.50% 的聚合用户参与度，同时 Facebook Watch 的日常活跃用户增加了0.21% 。 Facebook Watch 是一个流行的视频发现和分享平台，服务于数十亿用户。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CAM2:+Conformity-Aware+Multi-Task+Ranking+Model+for+Large-Scale+Recommender+Systems)|0|
|[A Deep Behavior Path Matching Network for Click-Through Rate Prediction](https://doi.org/10.1145/3543873.3584662)|Jian Dong, Yisong Yu, Yapeng Zhang, Yimin Lv, Shuli Wang, Beihong Jin, Yongkang Wang, Xingxing Wang, Dong Wang|Meituan Ltd, China; Institute of Software, Chinese Academy of Sciences, China and University of Chinese Academy of Sciences, China; Meituan Ltd., China|User behaviors on an e-commerce app not only contain different kinds of feedback on items but also sometimes imply the cognitive clue of the user's decision-making. For understanding the psychological procedure behind user decisions, we present the behavior path and propose to match the user's current behavior path with historical behavior paths to predict user behaviors on the app. Further, we design a deep neural network for behavior path matching and solve three difficulties in modeling behavior paths: sparsity, noise interference, and accurate matching of behavior paths. In particular, we leverage contrastive learning to augment user behavior paths, provide behavior path self-activation to alleviate the effect of noise, and adopt a two-level matching mechanism to identify the most appropriate candidate. Our model shows excellent performance on two real-world datasets, outperforming the state-of-the-art CTR model. Moreover, our model has been deployed on the Meituan food delivery platform and has accumulated 1.6% improvement in CTR and 1.8% improvement in advertising revenue.|用户在电子商务应用程序上的行为不仅包含对项目的不同类型的反馈，而且有时还意味着用户决策的认知线索。为了理解用户决策背后的心理过程，我们提出了行为路径，并建议匹配用户的当前行为路径和历史行为路径，以预测用户在应用程序上的行为。进一步，我们设计了一个用于行为路径匹配的深层神经网络，解决了行为路径建模中的三个难点: 稀疏性、噪声干扰和行为路径的精确匹配。特别地，我们利用对比学习来增强用户的行为路径，提供行为路径自激活来减轻噪声的影响，并采用两级匹配机制来确定最合适的候选者。我们的模型在两个真实世界的数据集上显示了出色的性能，优于最先进的 CTR 模型。此外，我们的模型已经部署在美团食品配送平台上，点击率累计提高了1.6% ，广告收入累计提高了1.8% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Deep+Behavior+Path+Matching+Network+for+Click-Through+Rate+Prediction)|0|
|[Cross-lingual Search for e-Commerce based on Query Translatability and Mixed-Domain Fine-Tuning](https://doi.org/10.1145/3543873.3587660)|Jesus PerezMartin, Jorge GomezRobles, Asier GutiérrezFandiño, Pankaj Adsul, Sravanthi Rajanala, Leonardo Lezcano|Walmart Global Tech, USA|Online stores in the US offer a unique scenario for Cross-Lingual Information Retrieval (CLIR) due to the mix of Spanish and English in user queries. Machine Translation (MT) provides an opportunity to lift relevance by translating the Spanish queries to English before delivering them to the search engine. However, polysemy-derived problems, high latency and context scarcity in product search, make generic MT an impractical solution. The wide diversity of products in marketplaces injects non-translatable entities, loanwords, ambiguous morphemes, cross-language ambiguity and a variety of Spanish dialects in the communication between buyers and sellers, posing a thread to the accuracy of MT. In this work, we leverage domain adaptation on a simplified architecture of Neural Machine Translation (NMT) to make both latency and accuracy suitable for e-commerce search. Our NMT model is fine-tuned on a mixed-domain corpus based on engagement data expanded with catalog back-translation techniques. Beyond accuracy, and given that translation is not the goal but the means to relevant results, the problem of Query Translatability is addressed by a classifier on whether the translation should be automatic or explicitly requested. We assembled these models into a query translation system that we tested and launched at Walmart.com , with a statistically significant lift in Spanish GMV and an nDCG gain for Spanish queries of +70%.|在美国的网上商店为跨语言信息检索(CLIR)提供了一个独特的场景，因为在用户查询中混合了西班牙语和英语。机器翻译(MT)提供了一个机会，通过将西班牙语查询翻译成英语，然后再将其传递给搜索引擎，从而提高相关性。然而，产品搜索中的多义性问题、高延迟性和上下文稀缺性使得通用机器翻译成为一种不切实际的解决方案。市场中产品的广泛多样性在买卖双方的交流中注入了不可翻译的实体、外来词、模棱两可的语素、跨语言模糊性和各种西班牙方言，这为神经机器翻译(NMT)的准确性提供了一条线索。在这项工作中，我们利用神经机器翻译(NMT)的简化架构的领域适应性，使延迟和准确性都适用于电子商务搜索。我们的 NMT 模型是在基于使用目录反向翻译技术扩展的参与数据的混合域语料库上进行微调的。除了准确性之外，考虑到翻译不是目标，而是获得相关结果的手段，查询可翻译性问题由分类器处理，分类器决定翻译应该是自动的还是显式的。我们将这些模型组合成一个查询翻译系统，并在 Walmart.com 上进行了测试和推出，西班牙语 GMV 的统计学显著提升，西班牙语查询的 nDCG 增幅为 + 70% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-lingual+Search+for+e-Commerce+based+on+Query+Translatability+and+Mixed-Domain+Fine-Tuning)|0|
|[Enhancing User Personalization in Conversational Recommenders](https://doi.org/10.1145/3543507.3583192)|Allen Lin, Ziwei Zhu, Jianling Wang, James Caverlee|George Mason University, USA; Texas A&M University, USA|Conversational recommenders are emerging as a powerful tool to personalize a user's recommendation experience. Through a back-and-forth dialogue, users can quickly hone in on just the right items. Many approaches to conversational recommendation, however, only partially explore the user preference space and make limiting assumptions about how user feedback can be best incorporated, resulting in long dialogues and poor recommendation performance. In this paper, we propose a novel conversational recommendation framework with two unique features: (i) a greedy NDCG attribute selector, to enhance user personalization in the interactive preference elicitation process by prioritizing attributes that most effectively represent the actual preference space of the user; and (ii) a user representation refiner, to effectively fuse together the user preferences collected from the interactive elicitation process to obtain a more personalized understanding of the user. Through extensive experiments on four frequently used datasets, we find the proposed framework not only outperforms all the state-of-the-art conversational recommenders (in terms of both recommendation performance and conversation efficiency), but also provides a more personalized experience for the user under the proposed multi-groundtruth multi-round conversational recommendation setting.|对话式推荐正在成为个性化用户推荐体验的强大工具。通过反复的对话，用户可以快速找到正确的项目。然而，许多会话推荐方法只是部分地探索了用户偏好空间，并对如何最好地整合用户反馈进行了有限的假设，导致了冗长的对话和糟糕的推荐性能。本文提出了一种新的会话推荐框架，该框架具有两个独特的特征: (1)贪婪的 NDCG 属性选择器，通过对最有效地表示用户实际偏好空间的属性进行优先排序，增强交互式偏好启发过程中的用户个性化; (2)用户表示细化器，有效地融合交互式启发过程中收集到的用户偏好，以获得对用户更个性化的理解。通过对四个常用数据集的大量实验，我们发现该框架不仅在推荐性能和会话效率方面优于所有最先进的会话推荐器，而且在提出的多地面真相多轮会话推荐设置下为用户提供了更加个性化的体验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+User+Personalization+in+Conversational+Recommenders)|0|
|[Dual-interest Factorization-heads Attention for Sequential Recommendation](https://doi.org/10.1145/3543507.3583278)|Guanyu Lin, Chen Gao, Yu Zheng, Jianxin Chang, Yanan Niu, Yang Song, Zhiheng Li, Depeng Jin, Yong Li|Tsinghua University, China; Department of Electronic Engineering, Tsinghua University, China; kuaishou, China|Accurate user interest modeling is vital for recommendation scenarios. One of the effective solutions is the sequential recommendation that relies on click behaviors, but this is not elegant in the video feed recommendation where users are passive in receiving the streaming contents and return skip or no-skip behaviors. Here skip and no-skip behaviors can be treated as negative and positive feedback, respectively. With the mixture of positive and negative feedback, it is challenging to capture the transition pattern of behavioral sequence. To do so, FeedRec has exploited a shared vanilla Transformer, which may be inelegant because head interaction of multi-heads attention does not consider different types of feedback. In this paper, we propose Dual-interest Factorization-heads Attention for Sequential Recommendation (short for DFAR) consisting of feedback-aware encoding layer, dual-interest disentangling layer and prediction layer. In the feedback-aware encoding layer, we first suppose each head of multi-heads attention can capture specific feedback relations. Then we further propose factorization-heads attention which can mask specific head interaction and inject feedback information so as to factorize the relation between different types of feedback. Additionally, we propose a dual-interest disentangling layer to decouple positive and negative interests before performing disentanglement on their representations. Finally, we evolve the positive and negative interests by corresponding towers whose outputs are contrastive by BPR loss. Experiments on two real-world datasets show the superiority of our proposed method against state-of-the-art baselines. Further ablation study and visualization also sustain its effectiveness. We release the source code here: https://github.com/tsinghua-fib-lab/WWW2023-DFAR.|准确的用户兴趣建模对于推荐场景至关重要。其中一个有效的解决方案是依赖于点击行为的顺序推荐，但是在视频提要推荐中这并不优雅，因为用户在接收流内容和返回跳过或不跳过行为时是被动的。在这里，跳过和不跳过行为可以分别视为负反馈和正反馈。由于正反馈和负反馈的混合，捕捉行为序列的转换模式具有挑战性。为此，FeedRec 利用了一个共享的香草变压器，这可能是不雅的，因为多头注意的头部交互没有考虑不同类型的反馈。本文提出了由反馈感知编码层、双兴趣分解层和预测层组成的双兴趣分解顺序推荐系统。在反馈感知编码层，我们首先假设多头注意的每个头都能捕获特定的反馈关系。然后进一步提出因子分解-头注意，它可以掩盖特定的头交互，并注入反馈信息，从而对不同类型的反馈之间的关系进行因子分解。此外，我们提出了一个双利益解缠层，以解耦正面和负面的利益之前，执行解缠的表示。最后，我们通过相应的塔进行正负利益演化，其输出由于业务流程重组损失而具有对比性。在两个实际数据集上的实验表明了我们提出的方法对最先进的基线的优越性。进一步的消融研究和可视化也支持其有效性。我们在这里发布源代码:  https://github.com/tsinghua-fib-lab/www2023-dfar。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual-interest+Factorization-heads+Attention+for+Sequential+Recommendation)|0|
|[A Cross-Media Retrieval System for Web-SNS-Map Using Suggested Keywords Generating and Ranking Method Based on Search Characteristics](https://doi.org/10.1145/3543873.3587344)|Da Li, Masaki Sugihashi, Tadahiko Kumamoto, Yukiko Kawai|Chiba Institute of Technology, Japan; Fukuoka University, Japan; Kyoto Sangyo University, Japan|The research on multimedia retrieval has lasted for several decades. However, past efforts generally focused on single-media retrieval, where the queries and retrieval results belong to the same media (platform) type, such as social media platforms or search engines. In single-media retrieval, users have to select search media or options based on search characteristics such as contents, time, or spatial distance, they might be unable to retrieve correct results mixed in other media if they carelessly forget to select. In this study, we propose a cross-media retrieval system using suggestion generation methods to integrate three search characteristics of the Web (textual content-based retrieval), SNS (timeliness), and map (spatial distance-aware retrieval). In our previous research, we attempted to improve search efficiency using clustering methods to provide search results to users through related terms, etc. In this paper, we focus on the search efficiency of multiple search media. We utilize Google search engine to obtain the retrieval content from the Web, Twitter to obtain timely information from SNSs, and Google Maps to get geographical information from maps. We apply the obtained retrieval results to analyze the similarities between them by clustering. Then, we generate relevant suggestions and provide them to users. Moreover, we validate the effectiveness of the search results generated by our proposed system.|多媒体检索的研究已经持续了几十年。然而，过去的努力通常集中在单媒体检索，其中查询和检索结果属于相同的媒体(平台)类型，如社会媒体平台或搜索引擎。在单媒体检索中，用户必须根据内容、时间或空间距离等搜索特征选择搜索媒体或选项，如果不小心忘记选择，可能无法检索混合在其他媒体中的正确结果。在本研究中，我们提出一个跨媒体检索系统，利用建议产生的方法来整合网页(文本内容检索)、 SNS (及时性)和地图(空间距离感知检索)的三个搜索特性。在我们以前的研究中，我们尝试使用聚类方法来提高搜索效率，通过相关词汇等为用户提供搜索结果。本文主要研究多种搜索媒体的搜索效率。我们利用谷歌搜索引擎从网络中获取检索内容，利用 Twitter 从 SNS 中获取及时信息，利用谷歌地图从地图中获取地理信息。我们应用所得到的检索结果，通过聚类分析它们之间的相似性。然后，我们生成相关的建议并提供给用户。此外，我们还验证了该系统所产生的搜索结果的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Cross-Media+Retrieval+System+for+Web-SNS-Map+Using+Suggested+Keywords+Generating+and+Ranking+Method+Based+on+Search+Characteristics)|0|
|[A Knowledge Enhanced Hierarchical Fusion Network for CTR Prediction under Account Search Scenario in WeChat](https://doi.org/10.1145/3543873.3584650)|Yuanzhou Yao, Zhao Zhang, Kaijia Yang, Huasheng Liang, Qiang Yan, Fuzheng Zhuang, Yongjun Xu, Boyu Diao, Chao Li|Institute of Artificial Intelligence, Beihang University, China; Zhejiang Lab, China; WeChat, Tencent, China; Institute of Computing Technology, Chinese Academy of Sciences, China|Click-through rate (CTR) estimation plays as a pivotal function module in various online services. Previous studies mainly apply CTR models to the field of recommendation or online advertisement. Indeed, CTR is also critical in information retrieval, since the CTR probability can serve as a valuable feature for a query-document pair. In this paper, we study the CTR task under account search scenario in WeChat, where users search official accounts or mini programs corresponding to an organization. Despite the large number of CTR models, directly applying them to our task is inappropriate since the account retrieval task has a number of specific characteristics. E.g., different from traditional user-centric CTR models, in our task, CTR prediction is query-centric and does not model user information. In addition, queries and accounts are short texts, and heavily rely on prior knowledge and semantic understanding. These characteristics require us to specially design a CTR model for the task. To this end, we propose a novel CTR prediction model named Knowledge eNhanced hIerarchical Fusion nEtwork (KNIFE). Specifically, to tackle the prior information problem, we mine the knowledge graph of accounts as side information; to enhance the representations of queries, we construct a bipartite graph for queries and accounts. In addition, a hierarchical network structure is proposed to fuse the representations of different information in a fine-grained manner. Finally, the representations of queries and accounts are obtained from this hierarchical network and fed into the CTR model together with other features for prediction. We conduct extensive experiments against 12 existing models across two industrial datasets. Both offline and online A/B test results indicate the effectiveness of KNIFE.|在各种网上服务中，点进率评估是一个关键的功能模块。以往的研究主要将点击率模型应用于推荐或在线广告领域。实际上，点击率在信息检索中也很关键，因为点击率可以作为查询-文档对的一个有价值的特性。本文研究了微信中用户搜索官方账号或与组织对应的小程序的帐号搜索情景下的点击率任务。尽管有大量的点击率检索模型，但由于账户检索任务具有许多特殊性，直接将其应用于我们的任务是不合适的。例如，与传统的以用户为中心的 CTR 模型不同，在我们的任务中，CTR 预测是以查询为中心的，不对用户信息建模。此外，查询和帐户是简短的文本，并且严重依赖于先前的知识和语义理解。这些特性要求我们为任务专门设计一个 CTR 模型。为此，我们提出了一种新的 CTR 预测模型——知识增强分层融合网络(KNIFE)。具体来说，为了解决先验信息问题，我们挖掘帐户的知识图作为边信息; 为了增强查询的表示，我们为查询和帐户构造一个二分图。此外，提出了一种分层网络结构，以细粒度的方式融合不同信息的表示。最后，从这个层次网络中获得查询和帐户的表示，并将其与其他用于预测的特征一起反馈到 CTR 模型中。我们对两个工业数据集中的12个现有模型进行了广泛的实验。离线和在线 A/B 测试结果均表明了 KNIFE 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Knowledge+Enhanced+Hierarchical+Fusion+Network+for+CTR+Prediction+under+Account+Search+Scenario+in+WeChat)|0|
|[Multi-Objective Ranking to Boost Navigational Suggestions in eCommerce AutoComplete](https://doi.org/10.1145/3543873.3584649)|Sonali Singh, Sachin Farfade, Prakash Mandayam Comar|Amazon, India|Query AutoComplete (QAC) helps customers complete their search queries quickly by suggesting completed queries. QAC on eCommerce sites usually employ Learning to Rank (LTR) approaches based on customer behaviour signals such as clicks and conversion rates to optimize business metrics. However, they do not exclusively optimize for the quality of suggested queries which results in lack of navigational suggestions like product categories and attributes, e.g., "sports shoes" and "white shoes" for query "shoes". We propose to improve the quality of query suggestions by introducing navigational suggestions without impacting the business metrics. For this purpose, we augment the customer behaviour (CB) based objective with Query-Quality (QQ) objective and assemble them with trainable mixture weights to define multi-objective optimization function. We propose to optimize this multi-objective function by implementing ALMO algorithm to obtain a model robust against any mixture weight. We show that this formulation improves query relevance on an eCommerce QAC dataset by at least 13% over the baseline Deep Pairwise LTR (DeepPLTR) with minimal impact on MRR and results in a lift of 0.26% in GMV in an online A/B test. We also evaluated our approach on public search logs datasets and got improvement in query relevance by using query coherence as QQ objective.|QueryAutoComplete (QAC)通过建议已完成的查询，帮助客户快速完成搜索查询。电子商务网站上的 QAC 通常采用基于客户行为信号(如点击率和转换率)的学习排名(LTR)方法来优化业务指标。然而，它们并不专门针对建议查询的质量进行优化，这会导致缺乏像产品类别和属性这样的导航建议，例如，“运动鞋”和查询“鞋子”的“白鞋子”。我们建议通过引入导航建议而不影响业务度量来提高查询建议的质量。为此，我们将基于顾客行为(CB)的目标与查询质量(QQ)目标相结合，并用可训练的混合权重组合它们来定义多目标优化函数。我们提出通过实现 ALMO 算法来优化这个多目标函数，以获得对任意混合权重的鲁棒模型。我们表明，这种制定方法使电子商务 QAC 数据集的查询相关性比基线 Deep Pairwise LTR (DeepPLTR)至少提高了13% ，对 MRR 的影响最小，并且在线 A/B 测试中导致 GMV 升高0.26% 。对公共检索日志数据集的检索方法进行了评估，并以查询一致性为 QQ 目标，提高了查询相关性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Objective+Ranking+to+Boost+Navigational+Suggestions+in+eCommerce+AutoComplete)|0|
|[Personalization and Recommendations in Search](https://doi.org/10.1145/3543873.3589749)|Sudarshan Lamkhede, Anlei Dong, Moumita Bhattacharya, Hongning Wang|Microsoft Bing, USA; Dept. of Computer Science, University of Virginia, USA; Netflix Research, USA|The utility of a search system for its users can be further enhanced by providing personalized results and recommendations within the search context. However, the research discussions around these aspects of search remain fragmented across different conferences and workshops. Hence, this workshop aims to bring together researchers and practitioners from industry and academia to engage in the discussions of algorithmic and system challenges in search personalization and effectively recommending within search context.|通过在搜索上下文中提供个性化的结果和建议，可以进一步加强搜索系统对用户的效用。然而，围绕搜索这些方面的研究讨论在不同的会议和研讨会上仍然支离破碎。因此，这个研讨会的目的是聚集业界和学术界的研究人员和从业人员，参与讨论在搜索个性化和有效推荐搜索背景下的算法和系统挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalization+and+Recommendations+in+Search)|0|
|[Cooperative Retriever and Ranker in Deep Recommenders](https://doi.org/10.1145/3543507.3583422)|Xu Huang, Defu Lian, Jin Chen, Liu Zheng, Xing Xie, Enhong Chen|University of Science and Technology of China, China; ; University of Electronic Science and Technology of China, China; Microsoft Research Asia, China|Deep recommender systems (DRS) are intensively applied in modern web services. To deal with the massive web contents, DRS employs a two-stage workflow: retrieval and ranking, to generate its recommendation results. The retriever aims to select a small set of relevant candidates from the entire items with high efficiency; while the ranker, usually more precise but time-consuming, is supposed to further refine the best items from the retrieved candidates. Traditionally, the two components are trained either independently or within a simple cascading pipeline, which is prone to poor collaboration effect. Though some latest works suggested to train retriever and ranker jointly, there still exist many severe limitations: item distribution shift between training and inference, false negative, and misalignment of ranking order. As such, it remains to explore effective collaborations between retriever and ranker.|深度推荐系统(DRS)在现代 Web 服务中得到了广泛的应用。为了处理海量的网络内容，DRS 采用了两个阶段的工作流程: 检索和排名，以生成其推荐结果。检索器的目标是从整个项目中高效地选择一小部分相关候选项; 而排名器通常更精确但更耗时，应该从检索到的候选项中进一步提炼出最好的项目。传统上，这两个组件要么单独训练，要么在一个简单的级联管道中训练，这样容易产生较差的协作效果。虽然最新的一些研究提出了联合训练检索器和排序器，但仍然存在很多严重的局限性: 训练和推理之间的项目分布转移、错误否定和排序顺序不一致。因此，仍然需要探索检索器和排名器之间的有效协作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cooperative+Retriever+and+Ranker+in+Deep+Recommenders)|0|
|[Modeling Temporal Positive and Negative Excitation for Sequential Recommendation](https://doi.org/10.1145/3543507.3583463)|Chengkai Huang, Shoujin Wang, Xianzhi Wang, Lina Yao|University of Technology Sydney, Australia; The University of New South Wales, Australia; CSIRO's Data 61, Australia and The University of New South Wales, Australia|Sequential recommendation aims to predict the next item which interests users via modeling their interest in items over time. Most of the existing works on sequential recommendation model users’ dynamic interest in specific items while overlooking users’ static interest revealed by some static attribute information of items, e.g., category, brand. Moreover, existing works often only consider the positive excitation of a user’s historical interactions on his/her next choice on candidate items while ignoring the commonly existing negative excitation, resulting in insufficiently modeling dynamic interest. The overlook of static interest and negative excitation will lead to incomplete interest modeling and thus impedes the recommendation performance. To this end, in this paper, we propose modeling both static interest and negative excitation for dynamic interest to further improve the recommendation performance. Accordingly, we design a novel Static-Dynamic Interest Learning (SDIL) framework featured with a novel Temporal Positive and Negative Excitation Modeling (TPNE) module for accurate sequential recommendation. TPNE is specially designed for comprehensively modeling dynamic interest based on temporal positive and negative excitation learning. Extensive experiments on three real-world datasets show that SDIL can effectively capture both static and dynamic interest and outperforms state-of-the-art baselines.|序贯推荐旨在通过建立用户对项目的兴趣模型来预测下一个用户感兴趣的项目。现有的序贯推荐模型大多是建立在用户对特定项目的动态兴趣的基础上，忽略了项目的静态属性信息(如类别、品牌等)所揭示的用户的静态兴趣。此外，现有的作品往往只考虑用户的历史交互作用对他/她的下一个选择的候选项的正激励，而忽略了普遍存在的负激励，导致不足的建模动态兴趣。忽视静态兴趣和负激励会导致兴趣建模的不完整，从而影响推荐性能。为此，本文提出了静态兴趣模型和动态兴趣的负激励模型，以进一步提高推荐性能。因此，我们设计了一个新颖的静态-动态兴趣学习(SDIL)框架，该框架具有一个新颖的时态正负激励建模(TPNE)模块，用于准确的顺序推荐。TPNE 是一种基于时间正负激励学习的动态兴趣综合建模方法。在三个实际数据集上的大量实验表明，SDIL 能够有效地捕获静态和动态兴趣，并且性能优于最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+Temporal+Positive+and+Negative+Excitation+for+Sequential+Recommendation)|0|
|[Beyond Two-Tower: Attribute Guided Representation Learning for Candidate Retrieval](https://doi.org/10.1145/3543507.3583254)|Hongyu Shan, Qishen Zhang, Zhongyi Liu, Guannan Zhang, Chenliang Li|Wuhan University, China; antgroup, China|Candidate retrieval is a key part of the modern search engines whose goal is to find candidate items that are semantically related to the query from a large item pool. The core difference against the later ranking stage is the requirement of low latency. Hence, two-tower structure with two parallel yet independent encoder for both query and item is prevalent in many systems. In these efforts, the semantic information of a query and a candidate item is fed into the corresponding encoder and then use their representations for retrieval. With the popularity of pre-trained semantic models, the state-of-the-art for semantic retrieval tasks has achieved the significant performance gain. However, the capacity of learning relevance signals is still limited by the isolation between the query and the item. The interaction-based modeling between the query and the item has been widely validated to be useful for the ranking stage, where more computation cost is affordable. Here, we are quite initerested in an demanding question: how to exploiting query-item interaction-based learning to enhance candidate retrieval and still maintain the low computation cost. Note that an item usually contain various heteorgeneous attributes which could help us understand the item characteristics more precisely. To this end, we propose a novel attribute guided representation learning framework (named AGREE) to enhance the candidate retrieval by exploiting query-attribute relevance. The key idea is to couple the query and item representation learning together during the training phase, but also enable easy decoupling for efficient inference. Specifically, we introduce an attribute fusion layer in the item side to identify most relevant item features for item representation. On the query side, an attribute-aware learning process is introduced to better infer the search intent also from these attributes. After model training, we then decouple the attribute information away from the query encoder, which guarantees the low latency for the inference phase. Extensive experiments over two real-world large-scale datasets demonstrate the superiority of the proposed AGREE against several state-of-the-art technical alternatives. Further online A/B test from AliPay search servise also show that AGREE achieves substantial performance gain over four business metrics. Currently, the proposed AGREE has been deployed online in AliPay for serving major traffic.|候选检索是现代搜索引擎的一个关键部分，其目标是从一个大的项目池中查找与查询语义相关的候选项。与后期排名阶段的核心区别在于对低延迟的要求。因此，双塔结构的两个并行但独立的编码器的查询和项目是普遍存在的许多系统。在这些工作中，查询和候选项的语义信息被输入到相应的编码器中，然后使用它们的表示进行检索。随着预训练语义模型的普及，语义检索任务的性能得到了显著提高。然而，相关信号的学习能力仍然受到查询与项目之间隔离的限制。基于交互的查询和项目之间的建模已被广泛验证是有用的排名阶段，其中更多的计算成本是负担得起的。如何利用基于查询项交互的学习来提高候选检索的效率，同时保持较低的计算成本，是本文研究的热点问题。注意，项目通常包含各种异构属性，这些属性可以帮助我们更精确地理解项目特征。为此，我们提出了一种新的属性引导表示学习框架(AGREE) ，利用查询-属性相关性来增强候选检索。其核心思想是在训练阶段将查询和项目表示学习耦合在一起，同时也为有效的推理提供了简单的解耦。具体来说，我们在项目端引入一个属性融合层来识别项目表示中最相关的项目特征。在查询方面，引入了一个感知属性的学习过程，以更好地从这些属性中推断出搜索意图。经过模型训练后，将属性信息与查询编码器解耦，保证了推理阶段的低延迟。通过两个现实世界大规模数据集的大量实验证明了所提议的 AGREE 相对于几种最先进的技术选择的优越性。支付宝搜索服务的进一步在线 A/B 测试也表明，AGREE 在四个业务指标上取得了显著的性能提升。目前，拟议的《支付宝协议》已在支付宝网上部署，以服务主要流量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beyond+Two-Tower:+Attribute+Guided+Representation+Learning+for+Candidate+Retrieval)|0|
|[Improving Content Retrievability in Search with Controllable Query Generation](https://doi.org/10.1145/3543507.3583261)|Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, Hugues Bouchard|Spotify, Spain; Spotify, Italy; Spotify, USA; Spotify, Netherlands|An important goal of online platforms is to enable content discovery, i.e. allow users to find a catalog entity they were not familiar with. A pre-requisite to discover an entity, e.g. a book, with a search engine is that the entity is retrievable, i.e. there are queries for which the system will surface such entity in the top results. However, machine-learned search engines have a high retrievability bias, where the majority of the queries return the same entities. This happens partly due to the predominance of narrow intent queries, where users create queries using the title of an already known entity, e.g. in book search 'harry potter'. The amount of broad queries where users want to discover new entities, e.g. in music search 'chill lyrical electronica with an atmospheric feeling to it', and have a higher tolerance to what they might find, is small in comparison. We focus here on two factors that have a negative impact on the retrievability of the entities (I) the training data used for dense retrieval models and (II) the distribution of narrow and broad intent queries issued in the system. We propose CtrlQGen, a method that generates queries for a chosen underlying intent-narrow or broad. We can use CtrlQGen to improve factor (I) by generating training data for dense retrieval models comprised of diverse synthetic queries. CtrlQGen can also be used to deal with factor (II) by suggesting queries with broader intents to users. Our results on datasets from the domains of music, podcasts, and books reveal that we can significantly decrease the retrievability bias of a dense retrieval model when using CtrlQGen. First, by using the generated queries as training data for dense models we make 9% of the entities retrievable (go from zero to non-zero retrievability). Second, by suggesting broader queries to users, we can make 12% of the entities retrievable in the best case.|在线平台的一个重要目标是支持内容发现，即允许用户找到他们不熟悉的目录实体。使用搜索引擎发现一个实体(例如一本书)的先决条件是该实体是可检索的，也就是说，有一些查询系统将在顶部结果中显示该实体。然而，机器学习搜索引擎有很高的可检索性偏差，其中大多数查询返回相同的实体。这部分是由于狭义意图查询的优势，用户使用已知实体的标题创建查询，例如在图书搜索“哈利波特”。用户希望发现新实体的广泛查询的数量，例如在音乐搜索“寒冷的抒情电子乐与大气的感觉”，并有一个更高的容忍度，他们可能会发现，相比之下是小的。这里我们重点讨论对实体的可检索性有负面影响的两个因素(I)用于密集检索模型的训练数据和(II)系统中发出的狭义和广义意图查询的分布。我们提出了 CtrlQGen，一种为选定的底层意图生成查询的方法——狭义的或者广义的。我们可以使用 CtrlQGen 通过为由不同合成查询组成的密集检索模型生成训练数据来改进 factor (I)。CtrlQGen 还可以通过向用户建议具有更广泛意图的查询来处理 factor (II)。我们对音乐、播客和书籍领域的数据集的研究结果表明，当使用 CtrlQGen 时，我们可以显著降低密集检索模型的可检索性偏差。首先，通过使用生成的查询作为密集模型的训练数据，我们使9% 的实体可检索(从零到非零可检索性)。其次，通过向用户建议更广泛的查询，我们可以使12% 的实体在最佳情况下可检索。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Content+Retrievability+in+Search+with+Controllable+Query+Generation)|0|
|[PIE: Personalized Interest Exploration for Large-Scale Recommender Systems](https://doi.org/10.1145/3543873.3584656)|Khushhall Chandra Mahajan, Amey Porobo Dharwadker, Romil Shah, Simeng Qu, Gaurav Bang, Brad Schumitsch|Meta Inc., USA|Recommender systems are increasingly successful in recommending personalized content to users. However, these systems often capitalize on popular content. There is also a continuous evolution of user interests that need to be captured, but there is no direct way to systematically explore users' interests. This also tends to affect the overall quality of the recommendation pipeline as training data is generated from the candidates presented to the user. In this paper, we present a framework for exploration in large-scale recommender systems to address these challenges. It consists of three parts, first the user-creator exploration which focuses on identifying the best creators that users are interested in, second the online exploration framework and third a feed composition mechanism that balances explore and exploit to ensure optimal prevalence of exploratory videos. Our methodology can be easily integrated into an existing large-scale recommender system with minimal modifications. We also analyze the value of exploration by defining relevant metrics around user-creator connections and understanding how this helps the overall recommendation pipeline with strong online gains in creator and ecosystem value. In contrast to the regression on user engagement metrics generally seen while exploring, our method is able to achieve significant improvements of 3.50% in strong creator connections and 0.85% increase in novel creator connections. Moreover, our work has been deployed in production on Facebook Watch, a popular video discovery and sharing platform serving billions of users.|推荐系统在向用户推荐个性化内容方面越来越成功。然而，这些系统往往利用流行的内容。用户兴趣的不断演变也需要被捕捉，但是没有直接的方法来系统地探索用户的兴趣。这也往往影响推荐管道的总体质量，因为培训数据是从向用户提供的候选人中产生的。在本文中，我们提出了一个大规模推荐系统的探索框架，以解决这些挑战。它由三部分组成，第一部分是用户创建者探索，侧重于确定用户感兴趣的最佳创建者，第二部分是在线探索框架，第三部分是平衡探索和利用的馈送组合机制，以确保探索性视频的最佳流行。我们的方法可以很容易地集成到一个现有的大规模推荐系统中，只需要做很少的修改。我们还通过定义用户-创建者连接的相关度量来分析探索的价值，并了解这如何帮助整个推荐流水线在创建者和生态系统价值方面获得强大的在线收益。与探索过程中常见的用户参与度指标的回归相比，我们的方法能够在强创作者关系中获得3.50% 的显著提高，在新创作者关系中获得0.85% 的显著提高。此外，我们的工作已经部署在 Facebook Watch 上，这是一个流行的视频发现和分享平台，为数十亿用户提供服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PIE:+Personalized+Interest+Exploration+for+Large-Scale+Recommender+Systems)|0|
|[Improving Product Search with Season-Aware Query-Product Semantic Similarity](https://doi.org/10.1145/3543873.3587625)|Haoming Chen, Yetian Chen, Jingjing Meng, Yang Jiao, Yikai Ni, Yan Gao, Michinari Momma, Yi Sun|Amazon.com, USA; Harvard University, USA|Product search for online shopping should be season-aware, i.e., presenting seasonally relevant products to customers. In this paper, we propose a simple yet effective solution to improve seasonal relevance in product search by incorporating seasonality into language models for semantic matching. We first identify seasonal queries and products by analyzing implicit seasonal contexts through time-series analysis over the past year. Then we introduce explicit seasonal contexts by enhancing the query representation with a season token according to when the query is issued. A new season-enhanced BERT model (SE-BERT) is also proposed to learn the semantic similarity between the resulting seasonal queries and products. SE-BERT utilizes Multi-modal Adaption Gate (MAG) to augment the season-enhanced semantic embedding with other contextual information such as product price and review counts for robust relevance prediction. To better align with the ranking objective, a listwise loss function (neural NDCG) is used to regularize learning. Experimental results validate the effectiveness of the proposed method, which outperforms existing solutions for query-product relevance prediction in terms of NDCG and Price Weighted Purchases (PWP).|网上购物的产品搜寻应具有季节性，即向顾客展示季节性相关的产品。在本文中，我们提出了一个简单而有效的解决方案，以改善产品搜索的季节性相关性，将季节性纳入语义匹配的语言模型。我们首先通过对过去一年的时间序列分析，分析隐含的季节性背景，识别出季节性查询和产品。然后根据查询发出的时间，使用季节标记增强查询表示，从而引入明确的季节上下文。提出了一种新的季节增强 BERT 模型(SE-BERT) ，用于学习产生的季节查询与产品之间的语义相似性。该算法利用多模态自适应门(MAG)增强季节增强语义嵌入，并结合产品价格和评论计数等上下文信息进行鲁棒相关性预测。为了更好地与排名目标保持一致，一个列表损失函数(神经 NDCG)被用来规范学习。实验结果验证了该方法的有效性，在 NDCG 和价格加权购买(PWP)方面优于现有的查询产品相关性预测方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Product+Search+with+Season-Aware+Query-Product+Semantic+Similarity)|0|
|[Blend and Match: Distilling Semantic Search Models with Different Inductive Biases and Model Architectures](https://doi.org/10.1145/3543873.3587629)|Hamed Bonab, Ashutosh Joshi, Ravi Bhatia, Ankit Gandhi, Vijay Huddar, Juhi Naik, Mutasem AlDarabsah, Choon Hui Teo, Jonathan May, Tarun Agarwal, Vaclav Petricek|Amazon, USA; Amazon, India; Amazon, USA and USC Information Sciences Institute, USA|Commercial search engines use different semantic models to augment lexical matches. These models provide candidate items for a user’s query from a target space of millions to billions of items. Models with different inductive biases provide relatively different predictions, making it desirable to launch multiple semantic models in production. However, latency and resource constraints make simultaneously deploying multiple models impractical. In this paper, we introduce a distillation approach, called Blend and Match (BM), to unify two different semantic search models into a single model. We use a Bi-encoder semantic matching model as our primary model and propose a novel loss function to incorporate eXtreme Multi-label Classification (XMC) predictions as the secondary model. Our experiments conducted on two large-scale datasets, collected from a popular e-commerce store, show that our proposed approach significantly improves the recall of the primary Bi-encoder model by 11% to 17% with a minimal loss in precision. We show that traditional knowledge distillation approaches result in a sub-optimal performance for our problem setting, and our BM approach yields comparable rankings with strong Rank Fusion (RF) methods used only if one could deploy multiple models.|商业搜索引擎使用不同的语义模型来增加词汇匹配。这些模型为用户的查询提供从数百万到数十亿的候选项。具有不同归纳偏差的模型提供了相对不同的预测，因此在生产环境中启动多个语义模型是可取的。然而，延迟和资源限制使得同时部署多个模型不切实际。在本文中，我们引入了一种称为“混合与匹配”(Blend and Match，BM)的提取方法，将两个不同的语义搜索模型统一到一个单一的模型中。我们使用一个双编码器语义匹配模型作为我们的主要模型，并提出了一个新的损失函数合并 eXtreme 多标签分类(XMC)预测作为次要模型。我们在两个大规模数据集上进行的实验，从一个流行的电子商务商店收集，表明我们提出的方法显着提高了11% 至17% 的主要双编码器模型的召回率，最小的精度损失。我们表明，传统的知识提取方法导致次优的性能为我们的问题设置，和我们的 BM 方法产生可比的排名与强秩融合(RF)方法只有当一个人可以部署多个模型使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Blend+and+Match:+Distilling+Semantic+Search+Models+with+Different+Inductive+Biases+and+Model+Architectures)|0|
|[Joint Internal Multi-Interest Exploration and External Domain Alignment for Cross Domain Sequential Recommendation](https://doi.org/10.1145/3543507.3583366)|Weiming Liu, Xiaolin Zheng, Chaochao Chen, Jiajie Su, Xinting Liao, Mengling Hu, Yanchao Tan|Fuzhou Univerisity, China; Zhejiang University, China|Sequential Cross-Domain Recommendation (CDR) has been popularly studied to utilize different domain knowledge and users’ historical behaviors for the next-item prediction. In this paper, we focus on the cross-domain sequential recommendation problem. This commonly exist problem is rather challenging from two perspectives, i.e., the implicit user historical rating sequences are difficult in modeling and the users/items on different domains are mostly non-overlapped. Most previous sequential CDR approaches cannot solve the cross-domain sequential recommendation problem well, since (1) they cannot sufficiently depict the users’ actual preferences, (2) they cannot leverage and transfer useful knowledge across domains. To tackle the above issues, we propose joint Internal multi-interest exploration and External domain alignment for cross domain Sequential Recommendation model (IESRec). IESRec includes two main modules, i.e., internal multi-interest exploration module and external domain alignment module. To reflect the users’ diverse characteristics with multi-interests evolution, we first propose internal temporal optimal transport method in the internal multi-interest exploration module. We further propose external alignment optimal transport method in the external domain alignment module to reduce domain discrepancy for the item embeddings. Our empirical studies on Amazon datasets demonstrate that IESRec significantly outperforms the state-of-the-art models.|序贯跨域推荐(CDR)是一种利用不同领域知识和用户历史行为进行下一个项目预测的方法。本文主要研究跨域序列推荐问题。这个常见的问题从两个方面来看都是相当具有挑战性的，即隐式用户历史评分序列难以建模，而且不同领域的用户/项目大多是非重叠的。以往的顺序 CDR 方法不能很好地解决跨域顺序推荐问题，因为(1)它们不能充分描述用户的实际偏好，(2)它们不能利用和跨域传递有用的知识。为了解决上述问题，我们提出了跨域序列推荐模型(IESRec)的内部多利益探索和外部域对齐的联合方法。IESRec 主要包括两个模块，即内部多兴趣探索模块和外部域对齐模块。为了反映用户的多样性特征和多种利益的演化，我们首先在内部多种利益探索模块中提出了内部时间最优传输方法。进一步提出了外域对齐模块中的外域对齐最优传输方法，以减少项目嵌入时的域差异。我们对亚马逊数据集的实证研究表明，IESRec 明显优于最先进的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Joint+Internal+Multi-Interest+Exploration+and+External+Domain+Alignment+for+Cross+Domain+Sequential+Recommendation)|0|
|[Latent User Intent Modeling for Sequential Recommenders](https://doi.org/10.1145/3543873.3584641)|Bo Chang, Alexandros Karatzoglou, Yuyan Wang, Can Xu, Ed H. Chi, Minmin Chen|Google, USA|Sequential recommender models are essential components of modern industrial recommender systems. These models learn to predict the next items a user is likely to interact with based on his/her interaction history on the platform. Most sequential recommenders however lack a higher-level understanding of user intents, which often drive user behaviors online. Intent modeling is thus critical for understanding users and optimizing long-term user experience. We propose a probabilistic modeling approach and formulate user intent as latent variables, which are inferred based on user behavior signals using variational autoencoders (VAE). The recommendation policy is then adjusted accordingly given the inferred user intent. We demonstrate the effectiveness of the latent user intent modeling via offline analyses as well as live experiments on a large-scale industrial recommendation platform.|序贯推荐模型是现代工业推荐系统的重要组成部分。这些模型学习根据用户在平台上的交互历史来预测用户可能与之交互的下一个项目。然而，大多数顺序推荐系统缺乏对用户意图的更高层次的理解，这往往会驱动用户在线行为。因此，意图建模对于理解用户和优化长期用户体验至关重要。提出了一种基于变分自动编码器(VAE)的基于用户行为信号的概率建模方法，并将用户意图表示为潜变量。然后根据推断出的用户意图相应地调整推荐策略。通过离线分析以及在大规模工业推荐平台上的实验，验证了潜在用户意图建模的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Latent+User+Intent+Modeling+for+Sequential+Recommenders)|0|
|[Deep Neural Network with LinUCB: A Contextual Bandit Approach for Personalized Recommendation](https://doi.org/10.1145/3543873.3587684)|Qicai Shi, Feng Xiao, Douglas Pickard, Inga Chen, Liang Chen|Disneystreaming, USA; Disneystreaming, China|Recommender systems are widely used in many Web applications to recommend items which are relevant to a user’s preferences. However, focusing on exploiting user preferences while ignoring exploration will lead to biased feedback and hurt the user’s experience in the long term. The Mutli-Armed Bandit (MAB) is introduced to balance the tradeoff between exploitation and exploration. By utilizing context information in the reward function, contextual bandit algorithms lead to better performance compared to context-free bandit algorithms. However, existing contextual bandit algorithms either assume a linear relation between the expected reward and context features, whose representation power gets limited, or use a deep neural network in the reward function which is impractical in implementation. In this paper, we propose a new contextual bandit algorithm, DeepLinUCB, which leverages the representation power of deep neural network to transform the raw context features in the reward function. Specifically, this deep neural network is dedicated to the recommender system, which is efficient and practical in real-world applications. Furthermore, we conduct extensive experiments in our online recommender system using requests from real-world scenarios and show that DeepLinUCB is efficient and outperforms other bandit algorithms.|在许多 Web 应用程序中，推荐系统被广泛用于推荐与用户首选项相关的项目。然而，只关注用户偏好而忽视探索将导致偏见的反馈，从长远来看会损害用户的体验。为了平衡开发与勘探之间的权衡，引进了多臂匪。通过在奖励函数中利用上下文信息，上下文盗贼算法比无上下文盗贼算法具有更好的性能。然而，现有的上下文盗贼算法要么假定期望奖励与上下文特征之间存在线性关系，其表示能力受到限制，要么在奖励函数中使用深度神经网络，这在实现上是不切实际的。本文提出了一种新的上下文盗贼算法 DeepLinUCB，该算法利用深层神经网络的表示能力来转换奖励函数中的原始上下文特征。具体来说，这种深层神经网络专门用于推荐系统，在实际应用中非常有效和实用。此外，我们使用来自现实场景的请求，在我们的在线推荐系统中进行了大量的实验，结果表明 DeepLinUCB 是高效的，并且优于其他盗贼算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Neural+Network+with+LinUCB:+A+Contextual+Bandit+Approach+for+Personalized+Recommendation)|0|
|[Contrastive Collaborative Filtering for Cold-Start Item Recommendation](https://doi.org/10.1145/3543507.3583286)|Zhihui Zhou, Lilin Zhang, Ning Yang|Sichuan University, China|The cold-start problem is a long-standing challenge in recommender systems. As a promising solution, content-based generative models usually project a cold-start item's content onto a warm-start item embedding to capture collaborative signals from item content so that collaborative filtering can be applied. However, since the training of the cold-start recommendation models is conducted on warm datasets, the existent methods face the issue that the collaborative embeddings of items will be blurred, which significantly degenerates the performance of cold-start item recommendation. To address this issue, we propose a novel model called Contrastive Collaborative Filtering for Cold-start item Recommendation (CCFCRec), which capitalizes on the co-occurrence collaborative signals in warm training data to alleviate the issue of blurry collaborative embeddings for cold-start item recommendation. In particular, we devise a contrastive collaborative filtering (CF) framework, consisting of a content CF module and a co-occurrence CF module to generate the content-based collaborative embedding and the co-occurrence collaborative embedding for a training item, respectively. During the joint training of the two CF modules, we apply a contrastive learning between the two collaborative embeddings, by which the knowledge about the co-occurrence signals can be indirectly transferred to the content CF module, so that the blurry collaborative embeddings can be rectified implicitly by the memorized co-occurrence collaborative signals during the applying phase. Together with the sound theoretical analysis, the extensive experiments conducted on real datasets demonstrate the superiority of the proposed model. The codes and datasets are available on https://github.com/zzhin/CCFCRec.|在推荐系统中，冷启动问题是一个长期存在的挑战。作为一种有前途的解决方案，基于内容的生成模型通常将一个冷启动项目的内容投射到一个嵌入的热启动项目上，以从项目内容中捕获协作信号，从而可以应用协同过滤。然而，由于冷启动推荐模型的训练是在暖数据集上进行的，现有的方法面临着项目协同嵌入模糊的问题，这严重影响了冷启动项目推荐的性能。为了解决这个问题，我们提出了一个新的模型，称为冷启动项目推荐对比协同过滤(CCFCrec) ，它利用共现协作信号在暖培训数据，以减轻问题模糊的协作嵌入冷启动项目推荐。特别地，我们设计了一个对比协同过滤(CF)框架，由一个内容 CF 模块和一个共现 CF 模块组成，分别为一个培训项目生成基于内容的协同嵌入和共现协同嵌入。在两个 CF 模块的联合训练中，我们对两个协同嵌入进行了对比学习，通过对比学习可以将关于共现信号的知识间接转移到内容 CF 模块中，从而在应用阶段可以通过记忆共现协同信号来隐式纠正模糊的协同嵌入。通过在实际数据集上的大量实验，结合理论分析，证明了该模型的优越性。代码和数据集可在 https://github.com/zzhin/ccfcrec 上获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Collaborative+Filtering+for+Cold-Start+Item+Recommendation)|0|
|[ColdNAS: Search to Modulate for User Cold-Start Recommendation](https://doi.org/10.1145/3543507.3583344)|Shiguang Wu, Yaqing Wang, Qinghe Jing, Daxiang Dong, Dejing Dou, Quanming Yao|Baidu Inc., China; Electronic Engineering, Tsinghua University, China|Making personalized recommendation for cold-start users, who only have a few interaction histories, is a challenging problem in recommendation systems. Recent works leverage hypernetworks to directly map user interaction histories to user-specific parameters, which are then used to modulate predictor by feature-wise linear modulation function. These works obtain the state-of-the-art performance. However, the physical meaning of scaling and shifting in recommendation data is unclear. Instead of using a fixed modulation function and deciding modulation position by expertise, we propose a modulation framework called ColdNAS for user cold-start problem, where we look for proper modulation structure, including function and position, via neural architecture search. We design a search space which covers broad models and theoretically prove that this search space can be transformed to a much smaller space, enabling an efficient and robust one-shot search algorithm. Extensive experimental results on benchmark datasets show that ColdNAS consistently performs the best. We observe that different modulation functions lead to the best performance on different datasets, which validates the necessity of designing a searching-based method. Codes are available at https://github.com/LARS-research/ColdNAS.|在推荐系统中，为只有少量交互历史的冷启动用户进行个性化推荐是一个具有挑战性的问题。最近的研究利用超网络将用户交互历史直接映射到用户特定的参数，然后用特征线性调制函数对预测器进行调制。这些作品获得了最先进的表演水平。然而，推荐数据的缩放和转移的物理意义尚不清楚。为了解决用户冷启动问题，我们提出了一种称为 ColdNAS 的调制框架，该框架通过神经结构搜索寻找合适的调制结构，包括功能和位置，而不是使用固定的调制函数来确定调制位置。我们设计了一个覆盖广泛模型的搜索空间，并从理论上证明了这个搜索空间可以转换成更小的空间，从而实现了一种高效、鲁棒的一次性搜索算法。在基准数据集上的大量实验结果表明，ColdNAS 始终表现最好。我们观察到不同的调制函数对不同的数据集产生最佳的性能，这验证了设计一种基于搜索的方法的必要性。密码可在 https://github.com/lars-research/coldnas 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ColdNAS:+Search+to+Modulate+for+User+Cold-Start+Recommendation)|0|
|[Improving the Relevance of Product Search for Queries with Negations](https://doi.org/10.1145/3543873.3587319)|Felice Antonio Merra, Omar Zaidan, Fabricio de Sousa Nascimento|Amazon, Japan; Amazon, Germany|Product search engines (PSEs) play an essential role in retail websites as they make it easier for users to retrieve relevant products within large catalogs. Despite the continuous progress that has led to increasingly accurate search engines, a limited focus has been given to their performance on queries with negations. Indeed, while we would expect to retrieve different products for the queries “iPhone 13 cover with ring” and “iPhone 13 cover without ring”, this does not happen in popular PSEs with the latter query containing results with the unwanted ring component. The limitation of modern PSEs in understanding negations motivates the need for further investigation. In this work, we start by defining the negation intent in users queries. Then, we design a transformer-based model, named Negation Detector for Queries (ND4Q), that reaches optimal performance in negation detection (+95% on accuracy metrics). Finally, having built the first negation detector for product search queries, we propose a negation-aware filtering strategy, named Filtering Irrelevant Products (FIP). The promising experimental results in improve the PSE relevance performance using FIP (+9.41% on [email protected] for queries where the negation starts with "without") pave the way to additional research effort towards negation-aware PSEs.|产品搜索引擎(PSE)在零售网站中发挥着重要作用，因为它们使用户更容易在大型目录中检索相关产品。尽管不断取得进展，导致搜索引擎越来越准确，但对否定查询的性能关注有限。事实上，虽然我们期望检索不同的产品的查询“ iPhone13盖有戒指”和“ iPhone13盖无戒指”，这不会发生在流行的 PSE 与后者的查询包含不想要的戒指组件的结果。现代 PSE 在理解否定方面的局限性促使了进一步研究的必要性。在这项工作中，我们首先定义用户查询中的否定意图。然后，我们设计了一个基于变压器的模型，称为查询否定检测器(ND4Q) ，它在否定检测中达到了最佳的性能(在准确性指标上 + 95%)。最后，在构建了产品搜索查询的第一个否定检测器的基础上，提出了一种基于否定感知的过滤策略——过滤不相关产品(FIP)。使用 FIP (对于否定以“无”开头的查询，[ email protected ]增加9.41%)改善 PSE 相关性的有希望的实验结果为针对具有否定意识的 PSE 的额外研究努力铺平了道路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+the+Relevance+of+Product+Search+for+Queries+with+Negations)|0|
|[Movie Ticket, Popcorn, and Another Movie Next Weekend: Time-Aware Service Sequential Recommendation for User Retention](https://doi.org/10.1145/3543873.3584628)|Xiaoyan Yang, Dong Wang, Binbin Hu, Dan Yang, Yue Shen, Jinjie Gu, Zhiqiang Zhang, Shiwei Lyu, Haipeng Zhang, Guannan Zhang|Ant Group, China; ShanghaiTech University, China|When a customer sees a movie recommendation, she may buy the ticket right away, which is the immediate feedback that helps improve the recommender system. Alternatively, she may choose to come back later and this long-term feedback is also modeled to promote user retention. However, the long-term feedback comes with non-trivial challenges in understanding user retention: the complicated correlation between current demands and follow-up demands, coupled with the periodicity of services. For instance, before the movie, the customer buys popcorn through the App, which temporally correlates with the initial movie recommendation. Days later, she checks the App for new movies, as a weekly routine. To address this complexity in a more fine-grained revisit modeling, we propose Time Aware Service Sequential Recommendation (TASSR) for user retention, which is equipped with a multi-task design and an In-category TimeSeqBlock module. Large-scale online and offline experiments demonstrate its significant advantages over competitive baselines.|当顾客看到一部电影的推荐信时，她可能会马上买票，这是一种即时的反馈，有助于提高推荐系统。或者，她可以选择以后再来，这种长期的反馈也被建模以促进用户保留。然而，长期的反馈在理解用户保留方面带来了重大挑战: 当前需求和后续需求之间的复杂关系，以及服务的周期性。例如，在看电影之前，客户通过 App 购买爆米花，这在时间上与最初的电影推荐相关。几天后，她每周例行检查应用程序是否有新电影。为了在更细粒度的再访问建模中解决这一复杂性，我们提出了用于用户保持的时间感知服务序列推荐(TASSR) ，该推荐配备了多任务设计和同类 TimeSeqBlock 模块。大规模的在线和离线实验证明了它相对于竞争基线的显著优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Movie+Ticket,+Popcorn,+and+Another+Movie+Next+Weekend:+Time-Aware+Service+Sequential+Recommendation+for+User+Retention)|0|
|[Unified Vision-Language Representation Modeling for E-Commerce Same-style Products Retrieval](https://doi.org/10.1145/3543873.3584632)|Ben Chen, Linbo Jin, Xinxin Wang, Dehong Gao, Wen Jiang, Wei Ning|Alibaba Group, China; Aliaba Group, China|Same-style products retrieval plays an important role in e-commerce platforms, aiming to identify the same products which may have different text descriptions or images. It can be used for similar products retrieval from different suppliers or duplicate products detection of one supplier. Common methods use the image as the detected object, but they only consider the visual features and overlook the attribute information contained in the textual descriptions, and perform weakly for products in image less important industries like machinery, hardware tools and electronic component, even if an additional text matching module is added. In this paper, we propose a unified vision-language modeling method for e-commerce same-style products retrieval, which is designed to represent one product with its textual descriptions and visual contents. It contains one sampling skill to collect positive pairs from user click log with category and relevance constrained, and a novel contrastive loss unit to model the image, text, and image+text representations into one joint embedding space. It is capable of cross-modal product-to-product retrieval, as well as style transfer and user-interactive search. Offline evaluations on annotated data demonstrate its superior retrieval performance, and online testings show it can attract more clicks and conversions. Moreover, this model has already been deployed online for similar products retrieval in alibaba.com, the largest B2B e-commerce platform in the world.|同类产品检索在电子商务平台中起着重要作用，其目的是识别具有不同文本描述或图像的同类产品。它可用于从不同供应商检索相似产品或检测一个供应商的重复产品。一般的检测方法都是以图像作为检测对象，但它们只考虑视觉特征，忽略了文本描述中的属性信息，对于机械、硬件工具和电子元件等图像不太重要的行业的产品，即使增加了额外的文本匹配模块，检测效果也很差。本文提出了一种统一的电子商务同类产品检索的视觉语言建模方法。它包含一种采样技巧，用于从类别和相关性受限的用户点击日志中收集正对，以及一种新的对比度损失单元，用于将图像、文本和图像 + 文本表示建模为一个联合嵌入空间。它能够进行跨模式的产品对产品检索，以及样式转移和用户交互式搜索。对注释数据的离线评估表明它具有优越的检索性能，在线测试表明它可以吸引更多的点击和转换。此外，该模型已经在全球最大的 B2B 电子商务平台阿里巴巴网站(alibaba.com)的类似产品检索中得到应用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unified+Vision-Language+Representation+Modeling+for+E-Commerce+Same-style+Products+Retrieval)|0|
|[Task Adaptive Multi-learner Network for Joint CTR and CVR Estimation](https://doi.org/10.1145/3543873.3584653)|Xiaofan Liu, Qinglin Jia, Chuhan Wu, Jingjie Li, Quanyu Dai, Lin Bo, Rui Zhang, Ruiming Tang|Beijing University of Posts and Telecommunications, China; Renmin University of China, China; Huawei Noah's Ark Lab, China; ruizhang.info, China|CTR and CVR are critical factors in personalized applications, and many methods jointly estimate them via multi-task learning to alleviate the ultra-sparsity of conversion behaviors. However, it is still difficult to predict CVR accurately and robustly due to the limited and even biased knowledge extracted by the single model tower optimized on insufficient conversion samples. In this paper, we propose a task adaptive multi-learner (TAML) framework for joint CTR and CVR prediction. We design a hierarchical task adaptive knowledge representation module with different experts to capture knowledge in different granularities, which can effectively exploit the commonalities between CTR and CVR estimation tasks meanwhile keeping their unique characteristics. We apply multiple learners to extract data knowledge from various views and fuse their predictions to obtain accurate and robust scores. To facilitate knowledge sharing across learners, we further perform self-distillation that uses the fused scores to teach different learners. Thorough offline and online experiments show the superiority of TAML in different Ad ranking tasks, and we have deployed it in Huawei’s online advertising platform to serve the main traffic.|CTR 和 CVR 是个性化应用中的关键因素，多种方法通过多任务学习来联合估计它们，以减轻转换行为的超稀疏性。然而，由于单模型塔在转换样本不足的情况下进行了优化，提取的知识有限，甚至有偏差，因此仍然难以准确、稳健地预测 CVR。本文提出了一个任务自适应多学习器(TAML)框架，用于联合 CTR 和 CVR 预测。设计了一个分层任务自适应知识表示模块，采用不同的专家来获取不同粒度的知识，有效地利用了 CTR 和 CVR 估计任务的共性，同时保持了它们的独特性。我们应用多个学习者从不同的角度提取数据知识，并融合他们的预测，以获得准确和稳健的分数。为了促进学习者之间的知识共享，我们进一步使用融合分数来教授不同的学习者。通过线下和线上的实验，我们发现了 TAML 在不同广告排名任务中的优势，并将其应用于华为的在线广告平台，为主要流量提供服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Task+Adaptive+Multi-learner+Network+for+Joint+CTR+and+CVR+Estimation)|0|
|[Deep Intention-Aware Network for Click-Through Rate Prediction](https://doi.org/10.1145/3543873.3584661)|Yaxian Xia, Yi Cao, Sihao Hu, Tong Liu, Lingling Lu|Alibaba Group, China; Georgia Institute of Technology, USA; Zhejiang University, China|E-commerce platforms provide entrances for customers to enter mini-apps that can meet their specific shopping requirements. Trigger items displayed on entrance icons can attract more entering. However, conventional Click-Through-Rate (CTR) prediction models, which ignore user instant interest in trigger item, fail to be applied to the new recommendation scenario dubbed Trigger-Induced Recommendation in Mini-Apps (TIRA). Moreover, due to the high stickiness of customers to mini-apps, we argue that existing trigger-based methods that over-emphasize the importance of trigger items, are undesired for TIRA, since a large portion of customer entries are because of their routine shopping habits instead of triggers. We identify that the key to TIRA is to extract customers' personalized entering intention and weigh the impact of triggers based on this intention. To achieve this goal, we convert CTR prediction for TIRA into a separate estimation form, and present Deep Intention-Aware Network (DIAN) with three key elements: 1) Intent Net that estimates user's entering intention, i.e., whether he/she is affected by the trigger or by the habits; 2) Trigger-Aware Net and 3) Trigger-Free Net that estimate CTRs given user's intention is to the trigger-item and the mini-app respectively. Following a joint learning way, DIAN can both accurately predict user intention and dynamically balance the results of trigger-free and trigger-based recommendations based on the estimated intention. Experiments show that DIAN advances state-of-the-art performance in a large real-world dataset, and brings a 9.39% lift of online Item Page View and 4.74% CTR for Juhuasuan, a famous mini-app of Taobao.|电子商务平台为客户提供了进入迷你应用程序，可以满足他们的具体购物需求。触发项目显示在入口图标可以吸引更多的进入。然而，传统的点击率(Click-Through-Rate，CTR)预测模型忽略了用户对触发条目的即时兴趣，无法应用于被称为微型应用程序中的触发诱导推荐(Trigger-)的新推荐场景。此外，由于客户对迷你应用程序的高粘性，我们认为，现有的基于触发器的方法，过分强调触发项目的重要性，是不希望 TIRA，因为大部分客户进入是因为他们的日常购物习惯，而不是触发器。我们认为，TIRA 的关键是提取顾客的个性化进入意图，并根据这一意图权衡触发因素的影响。为了实现这一目标，我们将 TIRA 的 CTR 预测转化为一个单独的估计形式，并提出深度意图感知网络(DIAN)的三个关键要素: 1)意图网络，估计用户的进入意图，即他/她是否受到触发器或习惯的影响; 2)触发感知网络和3)无触发网络，估计给定用户意图的 CTR 分别是触发项目和迷你应用程序。DIAN 采用联合学习的方法，既能准确预测用户意图，又能根据预测意图动态平衡无触发和基于触发的推荐结果。实验表明，DIAN 在一个大型现实数据集中提升了最先进的性能，使在线项目页面查看率提高了9.39% ，淘宝著名小应用聚花酸的点击率提高了4.74% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Intention-Aware+Network+for+Click-Through+Rate+Prediction)|0|
|[Search Personalization at Netflix](https://doi.org/10.1145/3543873.3587675)|Vito Ostuni, Christoph Kofler, Manjesh Nilange, Sudarshan Lamkhede, Dan Zylberglejd|Netflix Inc., USA|At Netflix, personalization plays a key role in several aspects of our user experience, from ranking titles to constructing an optimal Homepage. Although personalization is a well established research field, its application to search presents unique problems and opportunities. In this paper, we describe the evolution of Search personalization at Netflix, its unique challenges, and provide a high level overview of relevant solutions.|在 Netflix，个性化在我们的用户体验的几个方面起着关键作用，从排名标题到建立一个最佳的主页。虽然个性化是一个成熟的研究领域，但是它在搜索中的应用却带来了独特的问题和机遇。在本文中，我们描述了在 Netflix 搜索个性化的演变，其独特的挑战，并提供了相关解决方案的高层次概述。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Search+Personalization+at+Netflix)|0|
|[Pretrained Embeddings for E-commerce Machine Learning: When it Fails and Why?](https://doi.org/10.1145/3543873.3587669)|Da Xu, Bo Yang|Amazon, USA; LinkedIn, USA|The use of pretrained embeddings has become widespread in modern e-commerce machine learning (ML) systems. In practice, however, we have encountered several key issues when using pretrained embedding in a real-world production system, many of which cannot be fully explained by current knowledge. Unfortunately, we find that there is a lack of a thorough understanding of how pre-trained embeddings work, especially their intrinsic properties and interactions with downstream tasks. Consequently, it becomes challenging to make interactive and scalable decisions regarding the use of pre-trained embeddings in practice. Our investigation leads to two significant discoveries about using pretrained embeddings in e-commerce applications. Firstly, we find that the design of the pretraining and downstream models, particularly how they encode and decode information via embedding vectors, can have a profound impact. Secondly, we establish a principled perspective of pre-trained embeddings via the lens of kernel analysis, which can be used to evaluate their predictability, interactively and scalably. These findings help to address the practical challenges we faced and offer valuable guidance for successful adoption of pretrained embeddings in real-world production. Our conclusions are backed by solid theoretical reasoning, benchmark experiments, as well as online testings.|在现代电子商务机器学习(ML)系统中，预训练嵌入技术已经得到了广泛的应用。然而，在实践中，我们遇到了几个关键问题，当使用预训练嵌入在一个真实的生产系统，其中许多不能完全解释现有的知识。不幸的是，我们发现缺乏对预先训练的嵌入如何工作的透彻理解，特别是它们的内在属性和与下游任务的交互。因此，在实践中使用预先训练的嵌入方法时，做出交互式和可扩展的决策变得具有挑战性。我们的调查导致两个重要的发现，使用预训练嵌入在电子商务应用程序。首先，我们发现预训练和下游模型的设计，特别是它们如何通过嵌入向量对信息进行编码和解码，会产生深远的影响。其次，通过核分析的视角，建立了预训练嵌入的原则性视角，可以用来评估预训练嵌入的可预测性、交互性和可扩展性。这些发现有助于解决我们面临的实际挑战，并为在现实生产中成功采用预先培训的嵌入提供了宝贵的指导。我们的结论得到了可靠的理论推理、基准实验以及在线测试的支持。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pretrained+Embeddings+for+E-commerce+Machine+Learning:+When+it+Fails+and+Why?)|0|
|[GELTOR: A Graph Embedding Method based on Listwise Learning to Rank](https://doi.org/10.1145/3543507.3583193)|Masoud Reyhani Hamedani, JinSu Ryu, SangWook Kim|Hanyang University, Republic of Korea|Similarity-based embedding methods have introduced a new perspective on graph embedding by conforming the similarity distribution of latent vectors in the embedding space to that of nodes in the graph; they show significant effectiveness over conventional embedding methods in various machine learning tasks. In this paper, we first point out the three drawbacks of existing similarity-based embedding methods: inaccurate similarity computation, conflicting optimization goal, and impairing in/out-degree distributions. Then, motivated by these drawbacks, we propose AdaSim*, a novel similarity measure for graphs that is conducive to the similarity-based graph embedding. We finally propose GELTOR, an effective embedding method that employs AdaSim* as a node similarity measure and the concept of learning-to-rank in the embedding process. Contrary to existing methods, GELTOR does not learn the similarity scores distribution; instead, for any target node, GELTOR conforms the ranks of its top-t similar nodes in the embedding space to their original ranks based on AdaSim* scores. We conduct extensive experiments with six real-world datasets to evaluate the effectiveness of GELTOR in graph reconstruction, link prediction, and node classification tasks. Our experimental results show that (1) AdaSim* outperforms AdaSim, RWR, and MCT in computing nodes similarity in graphs, (2) our GETLOR outperforms existing state-of-the-arts and conventional embedding methods in most cases of the above machine learning tasks, thereby implying that learning-to-rank is beneficial to graph embedding.|基于相似性的嵌入方法通过调整嵌入空间中潜在向量与图中节点的相似性分布，为图的嵌入提供了一个新的视角，它们在各种机器学习任务中显示出比传统的嵌入方法更为有效的效果。本文首先指出了现有的基于相似度的嵌入方法存在的三个缺点: 相似度计算不准确、优化目标冲突和损伤内外度分布。然后，基于这些缺点，我们提出了 AdaSim * ，这是一种新的图的相似性度量，有利于基于相似性的图嵌入。最后提出了一种有效的嵌入方法 GELTOR，该方法采用 AdaSim * 作为节点相似性度量，并在嵌入过程中引入了学习排序的概念。与现有的方法相反，GELTOR 不学习相似度分数分布; 相反，对于任何目标节点，GELTOR 根据 AdaSim * 分数将其嵌入空间中的顶部 -t 相似节点的排名与其原始排名保持一致。我们使用六个真实世界的数据集进行了广泛的实验，以评估 GELTOR 在图重建、链路预测和节点分类任务中的有效性。我们的实验结果表明: (1) AdaSim * 在计算图中节点相似度方面优于 AdaSim，RWR 和 MCT; (2)在上述机器学习任务的大多数情况下，我们的 GETLOR 优于现有的最先进的和传统的嵌入方法，从而意味着学习排序有利于图嵌入。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GELTOR:+A+Graph+Embedding+Method+based+on+Listwise+Learning+to+Rank)|0|
|[On the Theories Behind Hard Negative Sampling for Recommendation](https://doi.org/10.1145/3543507.3583223)|Wentao Shi, Jiawei Chen, Fuli Feng, Jizhi Zhang, Junkang Wu, Chongming Gao, Xiangnan He|University of Science and Technology of China, China; Zhejiang University, China|Negative sampling has been heavily used to train recommender models on large-scale data, wherein sampling hard examples usually not only accelerates the convergence but also improves the model accuracy. Nevertheless, the reasons for the effectiveness of Hard Negative Sampling (HNS) have not been revealed yet. In this work, we fill the research gap by conducting thorough theoretical analyses on HNS. Firstly, we prove that employing HNS on the Bayesian Personalized Ranking (BPR) learner is equivalent to optimizing One-way Partial AUC (OPAUC). Concretely, the BPR equipped with Dynamic Negative Sampling (DNS) is an exact estimator, while with softmax-based sampling is a soft estimator. Secondly, we prove that OPAUC has a stronger connection with Top-K evaluation metrics than AUC and verify it with simulation experiments. These analyses establish the theoretical foundation of HNS in optimizing Top-K recommendation performance for the first time. On these bases, we offer two insightful guidelines for effective usage of HNS: 1) the sampling hardness should be controllable, e.g., via pre-defined hyper-parameters, to adapt to different Top-K metrics and datasets; 2) the smaller the $K$ we emphasize in Top-K evaluation metrics, the harder the negative samples we should draw. Extensive experiments on three real-world benchmarks verify the two guidelines.|负抽样已经被广泛用于大规模数据的推荐模型训练，而硬实例抽样不仅可以加快模型的收敛速度，而且可以提高模型的精度。然而，硬性负样本(HNS)有效性的原因尚未被揭示。本文通过对 HNS 进行深入的理论分析，填补了研究空白。首先，我们证明了对贝叶斯个性化排序(BPR)学习者使用 HNS 等价于优化单向部分 AUC (OPAUC)。具体来说，装有动态负抽样(DNS)的 BPR 是一个精确估计量，而基于软最大抽样的 BPR 是一个软估计量。其次，我们证明了 OPAUC 与 Top-K 评价指标之间的联系比 AUC 更强，并通过仿真实验进行了验证。这些分析首次为 HNS 优化 Top-K 推荐性能奠定了理论基础。在此基础上，我们为有效使用 HNS 提供了两个有见地的指导方针: 1)抽样硬度应该是可控的，例如，通过预定义的超参数，以适应不同的 Top-K 指标和数据集; 2)我们在 Top-K 评估指标中强调的 $K $越小，我们应该抽取的负面样本就越难。在三个真实世界的基准上进行的大量实验验证了这两条准则。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Theories+Behind+Hard+Negative+Sampling+for+Recommendation)|0|
|[A Counterfactual Collaborative Session-based Recommender System](https://doi.org/10.1145/3543507.3583321)|Wenzhuo Song, Shoujin Wang, Yan Wang, Kunpeng Liu, Xueyan Liu, Minghao Yin|Portland State University, USA; University of Technology Sydney, Australia; Macquarie University, Australia; Jilin University, China; Northeast Normal University, China|Most session-based recommender systems (SBRSs) focus on extracting information from the observed items in the current session of a user to predict a next item, ignoring the causes outside the session (called outer-session causes, OSCs) that influence the user's selection of items. However, these causes widely exist in the real world, and few studies have investigated their role in SBRSs. In this work, we analyze the causalities and correlations of the OSCs in SBRSs from the perspective of causal inference. We find that the OSCs are essentially the confounders in SBRSs, which leads to spurious correlations in the data used to train SBRS models. To address this problem, we propose a novel SBRS framework named COCO-SBRS (COunterfactual COllaborative Session-Based Recommender Systems) to learn the causality between OSCs and user-item interactions in SBRSs. COCO-SBRS first adopts a self-supervised approach to pre-train a recommendation model by designing pseudo-labels of causes for each user's selection of the item in data to guide the training process. Next, COCO-SBRS adopts counterfactual inference to recommend items based on the outputs of the pre-trained recommendation model considering the causalities to alleviate the data sparsity problem. As a result, COCO-SBRS can learn the causalities in data, preventing the model from learning spurious correlations. The experimental results of our extensive experiments conducted on three real-world datasets demonstrate the superiority of our proposed framework over ten representative SBRSs.|大多数基于会话的推荐系统(SBS)专注于从用户当前会话中观察到的项目中提取信息来预测下一个项目，而忽略了会话之外影响用户选择项目的原因(称为外部会话原因，OSC)。然而，这些原因在现实世界中普遍存在，很少有研究探讨它们在 SBRS 中的作用。本文从因果推理的角度分析了 SBRS 中 OSCs 的因果关系及其相关性。我们发现 OSC 本质上是 SBRS 中的混杂因素，这导致了用于训练 SBRS 模型的数据中存在虚假的相关性。为了解决这一问题，我们提出了一种新的 SBRS 框架 COCO-SBRS (COCO-SBRS，非事实协作的基于会话的推荐系统)来了解在 SBRS 中 OSC 和用户项目交互之间的因果关系。COCO-SBRS 首先采用自我监督的方法对推荐模型进行预训练，为每个用户选择数据中项目的原因设计伪标签，以指导训练过程。其次，COCO-SBRS 采用反事实推理方法，根据预训练推荐模型的输出结果进行推荐，考虑因果关系，以缓解数据稀疏问题。因此，COCO-SBRS 模型可以学习数据中的因果关系，防止模型学习虚假的相关性。我们在三个实际数据集上进行的大量实验结果表明，我们提出的框架优于十个具有代表性的 SBRS。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Counterfactual+Collaborative+Session-based+Recommender+System)|0|
|[Debiased Contrastive Learning for Sequential Recommendation](https://doi.org/10.1145/3543507.3583361)|Yuhao Yang, Chao Huang, Lianghao Xia, Chunzhen Huang, Da Luo, Kangyi Lin||Current sequential recommender systems are proposed to tackle the dynamic user preference learning with various neural techniques, such as Transformer and Graph Neural Networks (GNNs). However, inference from the highly sparse user behavior data may hinder the representation ability of sequential pattern encoding. To address the label shortage issue, contrastive learning (CL) methods are proposed recently to perform data augmentation in two fashions: (i) randomly corrupting the sequence data (e.g. stochastic masking, reordering); (ii) aligning representations across pre-defined contrastive views. Although effective, we argue that current CL-based methods have limitations in addressing popularity bias and disentangling of user conformity and real interest. In this paper, we propose a new Debiased Contrastive learning paradigm for Recommendation (DCRec) that unifies sequential pattern encoding with global collaborative relation modeling through adaptive conformity-aware augmentation. This solution is designed to tackle the popularity bias issue in recommendation systems. Our debiased contrastive learning framework effectively captures both the patterns of item transitions within sequences and the dependencies between users across sequences. Our experiments on various real-world datasets have demonstrated that DCRec significantly outperforms state-of-the-art baselines, indicating its efficacy for recommendation. To facilitate reproducibility of our results, we make our implementation of DCRec publicly available at: https://github.com/HKUDS/DCRec.|目前的顺序推荐系统主要采用变压器和图形神经网络(GNN)等多种神经网络技术来解决动态用户偏好学习问题。然而，从高度稀疏的用户行为数据中进行推断可能会阻碍序列模式编码的表示能力。为了解决标签短缺问题，最近提出了对比学习(CL)方法，以两种方式进行数据增强: (i)随机破坏序列数据(例如随机掩蔽，重新排序) ; (ii)跨预定义的对比视图对齐表示。虽然有效，但我们认为目前基于 CL 的方法在解决流行偏差和用户一致性与真实兴趣的分离方面存在局限性。本文提出了一种新的无偏对比推荐学习范式，它通过自适应整合意识增强将序列模式编码与全局协作关系建模相结合。该解决方案旨在解决推荐系统中的流行偏差问题。我们的去偏差对比学习框架有效地捕获了序列中的项目转换模式和用户之间的依赖关系。我们在各种真实世界数据集上的实验表明，DCREc 显著优于最先进的基线，表明其推荐功效。为了便于重复我们的结果，我们将我们的 DCRec 的实现公布于以下 https://github.com/hkuds/DCRec。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Debiased+Contrastive+Learning+for+Sequential+Recommendation)|0|
|[Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders](https://doi.org/10.1145/3543507.3583434)|Yupeng Hou, Zhankui He, Julian J. McAuley, Wayne Xin Zhao|UC San Diego, USA; Beijing Key Laboratory of Big Data Management and Analysis Methods, Renmin University of China, China; Renmin University of China, China|Recently, the generality of natural language text has been leveraged to develop transferable recommender systems. The basic idea is to employ pre-trained language models~(PLM) to encode item text into item representations. Despite the promising transferability, the binding between item text and item representations might be too tight, leading to potential problems such as over-emphasizing the effect of text features and exaggerating the negative impact of domain gap. To address this issue, this paper proposes VQ-Rec, a novel approach to learning Vector-Quantized item representations for transferable sequential Recommenders. The main novelty of our approach lies in the new item representation scheme: it first maps item text into a vector of discrete indices (called item code), and then employs these indices to lookup the code embedding table for deriving item representations. Such a scheme can be denoted as "text $\Longrightarrow$ code $\Longrightarrow$ representation". Based on this representation scheme, we further propose an enhanced contrastive pre-training approach, using semi-synthetic and mixed-domain code representations as hard negatives. Furthermore, we design a new cross-domain fine-tuning method based on a differentiable permutation-based network. Extensive experiments conducted on six public benchmarks demonstrate the effectiveness of the proposed approach, in both cross-domain and cross-platform settings. Code and pre-trained model are available at: https://github.com/RUCAIBox/VQ-Rec.|近年来，人们利用自然语言文本的通用性来开发可转移的推荐系统。其基本思想是使用预先训练好的语言模型 ~ (PLM)将项目文本编码成项目表示。项目文本与项目表征之间的联系过于紧密，可能导致过分强调文本特征的作用，夸大领域差距的负面影响等问题。为了解决这一问题，本文提出了一种新的学习矢量量化项目表示的方法 VQ-Rec。该方法的主要创新点在于新的项目表示方案: 它首先将项目文本映射到一个离散索引的向量(称为项目代码) ，然后使用这些索引查找代码嵌入表以获得项目表示。这样的方案可以表示为“ text $Longrightarrow $code $Longrightarrow $代表”。基于这种表示方案，我们进一步提出了一种增强的对比预训练方法，使用半合成和混合域代码表示作为硬负数。在此基础上，设计了一种新的基于可微置换网络的跨域微调方法。在六个公共基准上进行的大量实验证明了该方法在跨领域和跨平台环境中的有效性。代码和预先训练的模型可在以下 https://github.com/rucaibox/vq-rec 找到:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Vector-Quantized+Item+Representation+for+Transferable+Sequential+Recommenders)|0|
|[KAE-Informer: A Knowledge Auto-Embedding Informer for Forecasting Long-Term Workloads of Microservices](https://doi.org/10.1145/3543507.3583288)|Qin Hua, Dingyu Yang, Shiyou Qian, Hanwen Hu, Jian Cao, Guangtao Xue|Shanghai Jiao Tong University, China; Alibaba Group, China|Accurately forecasting workloads in terms of throughput that is quantified as queries per second (QPS) is essential for microservices to elastically adjust their resource allocations. However, long-term QPS prediction is challenging in two aspects: 1) generality across various services with different temporal patterns, 2) characterization of intricate QPS sequences which are entangled by multiple components. In this paper, we propose a knowledge auto-embedding Informer network (KAE-Informer) for forecasting the long-term QPS sequences of microservices. By analyzing a large number of microservice traces, we discover that there are two main decomposable and predictable components in QPS sequences, namely global trend & dominant periodicity (TP) and low-frequency residual patterns with long-range dependencies. These two components are important for accurately forecasting long-term QPS. First, KAE-Informer embeds the knowledge of TP components through mathematical modeling. Second, KAE-Informer designs a convolution ProbSparse self-attention mechanism and a multi-layer event discrimination scheme to extract and embed the knowledge of local context awareness and event regression effect implied in residual components, respectively. We conduct experiments based on three real datasets including a QPS dataset collected from 40 microservices. The experiment results show that KAE-Informer achieves a reduction of MAPE, MAE and RMSE by about 16.6%, 17.6% and 23.1% respectively, compared to the state-of-the-art models.|根据每秒查询(QPS)量化的吞吐量准确预测工作负载对于微服务弹性调整其资源分配至关重要。然而，长期的 QPS 预测在两个方面具有挑战性: 1)不同时间模式的服务之间的一般性，2)被多个组件纠缠在一起的复杂的 QPS 序列的角色塑造。本文提出了一种基于知识自动嵌入的信息网络(KAE-Informer)来预测微服务的长期 QPS 序列。通过对大量微服务跟踪的分析，发现 QPS 序列中存在两个主要的可分解和可预测成分，即全局趋势和主周期(TP)和具有长程依赖性的低频残差模式。这两个组成部分是准确预测长期 QPS 的重要组成部分。首先，KAE-Informer 通过数学建模嵌入 TP 元件的知识。其次，KAE-Informer 分别设计了一种卷积 Probse 自注意机制和一种多层次事件识别方案来提取和嵌入残差分量中隐含的局部上下文感知和事件回归效应的知识。我们基于三个实际数据集进行实验，其中包括从40个微服务中收集的 QPS 数据集。实验结果表明，与现有的模型相比，KAE-Informer 的 MAPE、 MAE 和 RMSE 分别降低了约16.6% 、17.6% 和23.1% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KAE-Informer:+A+Knowledge+Auto-Embedding+Informer+for+Forecasting+Long-Term+Workloads+of+Microservices)|0|
|[Propaganda Política Pagada: Exploring U.S. Political Facebook Ads en Español](https://doi.org/10.1145/3543507.3583425)|Bruno Coelho, Tobias Lauinger, Laura Edelson, Ian Goldstein, Damon McCoy|New York University, USA|In 2021, the U.S. Hispanic population totaled 62.5 million people, 68% of whom spoke Spanish in their homes. To date, it is unclear which political advertisers address this audience in their preferred language, and whether they do so differently than for English-speaking audiences. In this work, we study differences between political Facebook ads in English and Spanish during 2020, the latest U.S. presidential election. Political advertisers spent $ 1.48 B in English, but only $ 28.8 M in Spanish, disproportionately little compared to the share of Spanish speakers in the population. We further find a lower proportion of election-related advertisers (which additionally are more liberal-leaning than in the English set), and a higher proportion of government agencies in the set of Spanish ads. We perform multilingual topic classification, finding that the most common ad topics in English were also present in Spanish, but to a different extent, and with a different composition of advertisers. Thus, Spanish speakers are served different types of ads from different types of advertisers than English speakers, and in lower amounts; these results raise the question of whether political communication through Facebook ads may be inequitable and effectively disadvantaging the sizeable minority of Spanish speakers in the U.S. population.|2021年，美国西班牙裔人口总数为6250万，其中68% 的人在家里说西班牙语。到目前为止，还不清楚哪些政治广告主用自己喜欢的语言向这些受众发表演讲，以及他们的做法是否与英语受众不同。在这项工作中，我们研究了2020年美国总统大选期间 Facebook 上英语和西班牙语的政治广告之间的差异。政治广告客户在英语广告上花费了14.8亿美元，但在西班牙语广告上只花费了2880万美元，与说西班牙语的人口比例相比，这个数字不成比例。我们进一步发现，与选举有关的广告客户比例较低(此外，这些广告客户比英语广告客户更倾向于自由派) ，而在西班牙语广告客户中，政府机构的比例较高。我们进行了多语言话题分类，发现英语中最常见的广告话题也出现在西班牙语中，但程度不同，广告主的构成也不同。因此，说西班牙语的人比说英语的人得到了不同类型的广告，而且数量较少; 这些结果提出了一个问题: 通过 Facebook 广告进行的政治交流是否不公平，是否有效地损害了美国人口中说西班牙语的少数人的利益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Propaganda+Política+Pagada:+Exploring+U.S.+Political+Facebook+Ads+en+Español)|0|
|[Learning Denoised and Interpretable Session Representation for Conversational Search](https://doi.org/10.1145/3543507.3583265)|Kelong Mao, Hongjin Qian, Fengran Mo, Zhicheng Dou, Bang Liu, Xiaohua Cheng, Zhao Cao|Renmin University of China, China; Huawei Poisson Lab, China; Université de Montréal, Canada; RALI & Mila, Université de Montréal, Canada|Conversational search supports multi-turn user-system interactions to solve complex information needs. Compared with the traditional single-turn ad-hoc search, conversational search faces a more complex search intent understanding problem because a conversational search session is much longer and contains many noisy tokens. However, existing conversational dense retrieval solutions simply fine-tune the pre-trained ad-hoc query encoder on limited conversational search data, which are hard to achieve satisfactory performance in such a complex conversational search scenario. Meanwhile, the learned latent representation also lacks interpretability that people cannot perceive how the model understands the session. To tackle the above drawbacks, we propose a sparse Lexical-based Conversational REtriever (LeCoRE), which extends the SPLADE model with two well-matched multi-level denoising methods uniformly based on knowledge distillation and external query rewrites to generate denoised and interpretable lexical session representation. Extensive experiments on four public conversational search datasets in both normal and zero-shot evaluation settings demonstrate the strong performance of LeCoRE towards more effective and interpretable conversational search.|会话搜索支持多回合的用户-系统交互，以解决复杂的信息需求。与传统的单向自组织搜索相比，会话搜索面临着更复杂的搜索意图理解问题，因为会话搜索会话更长，且包含大量噪声标记。然而，现有的会话密集检索解决方案只是在有限的会话搜索数据上对预先训练好的自组织查询编码器进行微调，难以在如此复杂的会话搜索场景中获得令人满意的性能。同时，习得的潜在表征也缺乏可解释性，人们无法感知模型是如何理解会话的。针对上述缺点，本文提出了一种基于稀疏词汇的会话检索(Conversational REtriever，LeCoRE)算法，该算法扩展了 SPLADE 模型，采用基于知识提取和外部查询重写的两种匹配性较好的多级去噪方法，均匀地生成去噪和可解释的词汇会话表示。对四个公共会话搜索数据集在正常和零拍评估环境下的大量实验表明，LeCoRE 在更有效和可解释的会话搜索方面具有很强的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Denoised+and+Interpretable+Session+Representation+for+Conversational+Search)|0|
|[Fairly Adaptive Negative Sampling for Recommendations](https://doi.org/10.1145/3543507.3583355)|Xiao Chen, Wenqi Fan, Jingfan Chen, Haochen Liu, Zitao Liu, Zhaoxiang Zhang, Qing Li||Pairwise learning strategies are prevalent for optimizing recommendation models on implicit feedback data, which usually learns user preference by discriminating between positive (i.e., clicked by a user) and negative items (i.e., obtained by negative sampling). However, the size of different item groups (specified by item attribute) is usually unevenly distributed. We empirically find that the commonly used uniform negative sampling strategy for pairwise algorithms (e.g., BPR) can inherit such data bias and oversample the majority item group as negative instances, severely countering group fairness on the item side. In this paper, we propose a Fairly adaptive Negative sampling approach (FairNeg), which improves item group fairness via adaptively adjusting the group-level negative sampling distribution in the training process. In particular, it first perceives the model's unfairness status at each step and then adjusts the group-wise sampling distribution with an adaptive momentum update strategy for better facilitating fairness optimization. Moreover, a negative sampling distribution Mixup mechanism is proposed, which gracefully incorporates existing importance-aware sampling techniques intended for mining informative negative samples, thus allowing for achieving multiple optimization purposes. Extensive experiments on four public datasets show our proposed method's superiority in group fairness enhancement and fairness-utility tradeoff.|成对学习策略普遍用于优化隐性反馈数据的推荐模型，它通常通过区分正面(即用户点击)和负面(即通过负面抽样获得)来学习用户偏好。但是，不同项目组(由项目属性指定)的大小通常是不均匀分布的。实证结果表明，成对算法中常用的一致负抽样策略(如 BPR)会继承这种数据偏差，并将多数项目组作为负实例过度抽样，严重影响项目方的群体公平性。本文提出了一种公平自适应负抽样方法(FairNeg) ，该方法通过在训练过程中自适应调整组级负抽样分布来提高项目组的公平性。特别地，它首先在每个步骤中感知模型的不公平状态，然后利用自适应动量更新策略调整分组抽样分布，以更好地促进公平性优化。此外，提出了负抽样分布混合机制，它优雅地结合了现有的重要性感知抽样技术，旨在挖掘信息负样本，从而实现多种优化目的。在四个公共数据集上的大量实验表明，该方法在增强群体公平性和公平-效用权衡方面具有优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairly+Adaptive+Negative+Sampling+for+Recommendations)|0|
|[CNSVRE: A Query Reformulated Search System with Explainable Summarization for Virtual Research Environment](https://doi.org/10.1145/3543873.3587360)|Na Li, Yangjun Zhang, Zhiming Zhao|University of Amsterdam, Netherlands|Computational notebook environments have drawn broad attention in data-centric research applications, e.g., virtual research environment, for exploratory data analysis and algorithm prototyping. Vanilla computational notebook search solutions have been proposed but they do not pay much attention to the information needs of scientific researchers. Previous studies either treat computational notebook search as a code search problem or focus on content-based computational notebook search. The queries being considered are neither research-concerning nor diversified whereas researchers’ information needs are highly specialized and complex. Moreover, relevance evaluation for computational notebooks is tricky and unreliable since computational notebooks contain fragments of text and code and are usually poorly organized. To solve the above challenges, we propose a computational notebook search system for virtual research environment (VRE), i.e., CNSVRE, with scientific query reformulation and computational notebook summarization. We conduct a user study to demonstrate the effectiveness, efficiency, and satisfaction with the system.|计算机笔记本环境在以数据为中心的研究应用中引起了广泛的关注，例如用于探索性数据分析和算法原型的虚拟研究环境。香草计算笔记本搜索解决方案已经提出，但他们没有太多的关注科学研究人员的信息需求。以往的研究要么将计算笔记本搜索视为一个代码搜索问题，要么将重点放在基于内容的计算笔记本搜索上。被考虑的查询既不涉及研究，也不多样化，而研究人员的信息需求是高度专业化和复杂化的。此外，计算笔记本的相关性评估是棘手和不可靠的，因为计算笔记本包含文本和代码片段，通常组织不良。为了解决上述挑战，我们提出了一个虚拟研究环境(即 CNSVRE)的计算笔记本搜索系统，该系统具有科学的查询重构和计算笔记本摘要。我们进行了用户研究，以证明系统的有效性、效率和满意度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CNSVRE:+A+Query+Reformulated+Search+System+with+Explainable+Summarization+for+Virtual+Research+Environment)|0|
|[Personalized style recommendation via reinforcement learning](https://doi.org/10.1145/3543873.3587367)|Jiyun Luo, Kurchi Subhra Hazra, Wenyu Huo, Rui Li, Abhijit Mahabal|Pinterest Inc., USA|Pinterest fashion and home decor searchers often have different style tastes. Some existing work adopts users’ past engagement to infer style preference. These methods cannot help users discover new styles. Other work requires users to provide text or visual signals to describe their style preference, but users often are not familiar with style terms and do not have the right image to start with. In this paper, we propose a reinforcement learning (RL) method to help users explore and exploit style space without requiring extra user input. Experimental results show that our method improves the success rate of Pinterest fashion and home decor searches by 34.8%.|Pinterest 时尚和家居装饰搜索往往有不同的风格品味。现有的一些工作采用用户过去的接触来推断风格偏好。这些方法不能帮助用户发现新样式。其他工作需要用户提供文本或视觉信号来描述他们的风格偏好，但用户往往不熟悉风格术语，并没有正确的图像开始。在这篇文章中，我们提出了一个强化学习(RL)方法来帮助用户探索和开发样式空间，而不需要额外的用户输入。实验结果表明，该方法提高了 Pinterest 时装和家居装饰搜索的成功率34.8% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+style+recommendation+via+reinforcement+learning)|0|
|[HierCat: Hierarchical Query Categorization from Weakly Supervised Data at Facebook Marketplace](https://doi.org/10.1145/3543873.3584622)|Yunzhong He, Cong Zhang, Ruoyan Kong, Chaitanya Kulkarni, Qing Liu, Ashish Gandhe, Amit Nithianandan, Arul Prakash|Meta, USA; University of Minnesota Twin Cities, USA|Query categorization at customer-to-customer e-commerce platforms like Facebook Marketplace is challenging due to the vagueness of search intent, noise in real-world data, and imbalanced training data across languages. Its deployment also needs to consider challenges in scalability and downstream integration in order to translate modeling advances into better search result relevance. In this paper we present HierCat, the query categorization system at Facebook Marketplace. HierCat addresses these challenges by leveraging multi-task pre-training of dual-encoder architectures with a hierarchical inference step to effectively learn from weakly supervised training data mined from searcher engagement. We show that HierCat not only outperforms popular methods in offline experiments, but also leads to 1.4% improvement in NDCG and 4.3% increase in searcher engagement at Facebook Marketplace Search in online A/B testing.|像 Facebook Marketplace 这样的客户对客户的电子商务平台，由于搜索意图的模糊性、现实世界数据中的噪音以及跨语言的不平衡训练数据，查询分类是一个挑战。它的部署还需要考虑可伸缩性和下游集成方面的挑战，以便将建模进展转化为更好的搜索结果相关性。本文介绍了 Facebook Marketplace 的查询分类系统 HierCat。HierCat 通过利用双重编码器架构的多任务预训练和分层推理步骤来解决这些挑战，以有效地学习从搜索引擎参与中挖掘的弱监督训练数据。我们发现 HierCat 不仅在离线实验中表现优于流行的方法，而且在线 A/B 测试中导致 NDCG 改善1.4% ，Facebook Marketplace Search 的搜索者参与度提高4.3% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HierCat:+Hierarchical+Query+Categorization+from+Weakly+Supervised+Data+at+Facebook+Marketplace)|0|
|[Search-based Recommendation: the Case for Difficult Predictions](https://doi.org/10.1145/3543873.3587374)|Ghazaleh Haratinezhad Torbati, Gerhard Weikum, Andrew Yates|University of Amsterdam, Netherlands; Max Planck Institute for Informatics, Germany|Recommender systems have achieved impressive results on benchmark datasets. However, the numbers are often influenced by assumptions made on the data and evaluation mode. This work questions and revises these assumptions, to study and improve the quality, particularly for the difficult case of search-based recommendations. Users start with a personally liked item as a query and look for similar items that match their tastes. User satisfaction requires discovering truly unknown items: new authors of books rather than merely more books of known writers. We propose a unified system architecture that combines interaction-based and content-based signals and leverages language models for Transformer-powered predictions. We present new techniques for selecting negative training samples, and investigate their performance in the underexplored search-based evaluation mode.|推荐系统在基准数据集上取得了令人印象深刻的成果。然而，这些数字往往受到对数据和评估模式的假设的影响。本文对这些假设进行了质疑和修正，以研究和提高质量，特别是针对困难案例的基于搜索的推荐。用户从一个个人喜欢的项目开始查询，然后寻找与他们口味相符的类似项目。用户满意度要求发现真正未知的项目: 书籍的新作者，而不仅仅是知名作家的书籍。我们提出了一个统一的系统体系结构，它结合了基于交互和基于内容的信号，并利用语言模型进行基于 Transformer 的预测。我们提出了选择负训练样本的新技术，并研究了它们在基于搜索的评估模式中的表现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Search-based+Recommendation:+the+Case+for+Difficult+Predictions)|0|
|[Reweighting Clicks with Dwell Time in Recommendation](https://doi.org/10.1145/3543873.3584624)|Ruobing Xie, Lin Ma, Shaoliang Zhang, Feng Xia, Leyu Lin|WeChat, Tencent, China|The click behavior is the most widely-used user positive feedback in recommendation. However, simply considering each click equally in training may suffer from clickbaits and title-content mismatching, and thus fail to precisely capture users' real satisfaction on items. Dwell time could be viewed as a high-quality quantitative indicator of user preferences on each click, while existing recommendation models do not fully explore the modeling of dwell time. In this work, we focus on reweighting clicks with dwell time in recommendation. Precisely, we first define a new behavior named valid read, which helps to select high-quality click instances for different users and items via dwell time. Next, we propose a normalized dwell time function to reweight click signals in training for recommendation. The Click reweighting model achieves significant improvements on both offline and online evaluations in real-world systems.|点击行为是推荐中使用最广泛的用户正面反馈。然而，在培训中仅仅考虑每一次点击的平等性可能会遭受点击诱惑和标题内容不匹配的问题，因此不能准确地捕捉用户对项目的真正满意度。停留时间可以被视为每次点击时用户偏好的高质量定量指标，而现有的推荐模型并没有充分探索停留时间的建模。在这项工作中，我们将重点放在用推荐中的停留时间重新加权点击。确切地说，我们首先定义一个名为有效读的新行为，它有助于通过停留时间为不同的用户和项目选择高质量的单击实例。接下来，我们提出了一个规范化的停留时间函数来重新加权点击信号的训练推荐。Click 重新加权模型在现实世界系统的离线和在线评估方面都取得了显著的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reweighting+Clicks+with+Dwell+Time+in+Recommendation)|0|
|[Disentangled Causal Embedding With Contrastive Learning For Recommender System](https://doi.org/10.1145/3543873.3584637)|Weiqi Zhao, Dian Tang, Xin Chen, Dawei Lv, Daoli Ou, Biao Li, Peng Jiang, Kun Gai|Kuaishou Technology, China; Unaffiliated, China|Recommender systems usually rely on observed user interaction data to build personalized recommendation models, assuming that the observed data reflect user interest. However, user interacting with an item may also due to conformity, the need to follow popular items. Most previous studies neglect user's conformity and entangle interest with it, which may cause the recommender systems fail to provide satisfying results. Therefore, from the cause-effect view, disentangling these interaction causes is a crucial issue. It also contributes to OOD problems, where training and test data are out-of-distribution. Nevertheless, it is quite challenging as we lack the signal to differentiate interest and conformity. The data sparsity of pure cause and the items' long-tail problem hinder disentangled causal embedding. In this paper, we propose DCCL, a framework that adopts contrastive learning to disentangle these two causes by sample augmentation for interest and conformity respectively. Futhermore, DCCL is model-agnostic, which can be easily deployed in any industrial online system. Extensive experiments are conducted over two real-world datasets and DCCL outperforms state-of-the-art baselines on top of various backbone models in various OOD environments. We also demonstrate the performance improvements by online A/B testing on Kuaishou, a billion-user scale short-video recommender system.|推荐系统通常依赖于观察到的用户交互数据来建立个性化的推荐模型，假设观察到的数据反映了用户的兴趣。然而，用户与一个项目的互动也可能是由于一致性，需要遵循流行的项目。以往的大多数研究忽视了用户的一致性，并与之产生利益纠葛，这可能导致推荐系统不能提供令人满意的结果。因此，从因果观点来看，解开这些相互作用的原因是一个至关重要的问题。它还会导致面向对象设计(OOD)问题，即培训和测试数据分布不均。然而，这是相当具有挑战性的，因为我们缺乏区分兴趣和一致性的信号。纯因果关系的数据稀疏性和项目的长尾问题阻碍了因果关系的解纠缠嵌入。在本文中，我们提出了 DCCL，一个采用对比学习的框架，分别通过兴趣和从众的样本增加来解决这两个原因。此外，DCCL 是模型无关的，可以很容易地部署在任何工业在线系统。在两个真实世界的数据集上进行了广泛的实验，DCCL 在各种面向对象设计(OOD)环境中的各种骨干模型上的表现优于最先进的基线。我们还通过在 Kuaishou 的在线 A/B 测试展示了性能改进，这是一个拥有10亿用户规模的短视频推荐系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disentangled+Causal+Embedding+With+Contrastive+Learning+For+Recommender+System)|0|
|[Confidence Ranking for CTR Prediction](https://doi.org/10.1145/3543873.3584643)|Jian Zhu, Congcong Liu, Pei Wang, Xiwei Zhao, Zhangang Lin, Jingping Shao|JD.com, China|Model evolution and data updating are two common phenomena in large-scale real-world machine learning applications, e.g. ads and recommendation systems. To adapt, the real-world system typically retrain with all available data and online learn with recently available data to update the models periodically with the goal of better serving performance. In this paper, we propose a novel framework, named Confidence Ranking, which designs the optimization objective as a ranking function with two different models. Our confidence ranking loss allows direct optimization of the logits output for different convex surrogate functions of metrics, e.g. AUC and Accuracy depending on the target task and dataset. Armed with our proposed methods, our experiments show that the introduction of confidence ranking loss can outperform all baselines on the CTR prediction tasks of public and industrial datasets. This framework has been deployed in the advertisement system of JD.com to serve the main traffic in the fine-rank stage.|模型演化和数据更新是广告和推荐系统等大规模真实世界机器学习应用中的两种常见现象。为了适应这种情况，现实世界中的系统通常使用所有可用的数据进行再培训，并使用最近可用的数据进行在线学习，以便定期更新模型，从而更好地服务于性能。在本文中，我们提出了一个新的框架，称为置信排序，设计的优化目标为一个排序函数与两个不同的模型。我们的置信度排序损失允许直接优化不同凸性度量替代函数的 logit 输出，例如 AUC 和精度取决于目标任务和数据集。实验结果表明，在公共数据集和工业数据集的 CTR 预测任务中，置信度排序损失的引入能够优于所有基线。该框架已经部署在京东的广告系统中，服务于精品阶段的主流流量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Confidence+Ranking+for+CTR+Prediction)|0|
|[Personalised Search in E-Comm Groceries](https://doi.org/10.1145/3543873.3587588)|Ramprabhu Murugesan, Anuja Sharan|Walmart labs, India; Walmart Labs, India|Personalized Search(henceforth called P10d Search) focuses to deliver user-specific search results based on the previous purchases. Search engine retrieves the result based on the defined relevancy algorithm. When a user searches a keyword, search engine constructs the search query based on the defined searchable fields/attributes along with configured relevancy algorithm. Position of the item retrieved in search results is determined by the search algorithm based on the search term. The results are further refined or ranked based on different click stream signals, product features, market data to provide much relevant results. Personalisation provides the ranked the list of items for a given user based on past purchases. Personalisation is agnostic of search query and takes user id, cart additions, site taxonomy and user’s shopping history as input signals. In summary, search engine queries data based on relevancy and personalisation engine retrieves based purely on purchases. Goal of personalised search is to enhance the search results by adding personalised results without affecting the search relevance.|个性化检索(以下简称 P10d 搜索)的重点是提供基于以前购买的特定用户的搜索结果。搜索引擎根据定义的相关算法检索结果。当用户搜索关键字时，搜索引擎根据定义的可搜索字段/属性以及配置的相关性算法构造搜索查询。在搜索结果中检索到的项的位置由基于搜索项的搜索算法确定。根据不同的点击流信号、产品特点、市场数据对结果进行进一步细化或排序，以提供更多相关的结果。个性化为给定用户提供了基于过去购买的商品的排名列表。个性化是不可知的搜索查询，并采取用户 ID，购物车添加，网站分类和用户的购物历史作为输入信号。总之，搜索引擎查询数据的相关性和个性化引擎检索纯粹基于购买。个性化搜索的目标是在不影响搜索相关性的情况下，通过添加个性化搜索结果来提高搜索结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalised+Search+in+E-Comm+Groceries)|0|
|[Graph Embedding for Mapping Interdisciplinary Research Networks](https://doi.org/10.1145/3543873.3587570)|Eoghan Cunningham, Derek Greene|University College Dublin, Ireland|Representation learning is the first step in automating tasks such as research paper recommendation, classification, and retrieval. Due to the accelerating rate of research publication, together with the recognised benefits of interdisciplinary research, systems that facilitate researchers in discovering and understanding relevant works from beyond their immediate school of knowledge are vital. This work explores different methods of research paper representation (or document embedding), to identify those methods that are capable of preserving the interdisciplinary implications of research papers in their embeddings. In addition to evaluating state of the art methods of document embedding in a interdisciplinary citation prediction task, we propose a novel Graph Neural Network architecture designed to preserve the key interdisciplinary implications of research articles in citation network node embeddings. Our proposed method outperforms other GNN-based methods in interdisciplinary citation prediction, without compromising overall citation prediction performance.|表示学习是研究论文推荐、分类和检索等任务自动化的第一步。由于研究发表的速度加快，再加上科际整合的公认好处，有助研究人员发现和理解其直接学校知识以外的相关著作的系统是至关重要的。本文探讨了研究论文表示(或文档嵌入)的不同方法，以确定哪些方法能够在其嵌入过程中保持研究论文的跨学科含义。除了评估在跨学科引文预测任务中嵌入文档的最新方法之外，我们还提出了一种新的图形神经网络体系结构，旨在保存引文网络节点嵌入中研究论文的关键跨学科含义。我们提出的方法在跨学科引文预测方面优于其他基于 GNN 的方法，而不影响整体的引文预测性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Embedding+for+Mapping+Interdisciplinary+Research+Networks)|0|
|[Deep Passage Retrieval in E-Commerce](https://doi.org/10.1145/3543873.3587624)|Vinay Rao Dandin, Ozan Ersoy, Kyung Hyuk Kim|Flipkart US R&D Center, USA|We have developed a conversational assistant called the Decision Assistant (DA) to help customers make purchase decisions. To answer customer queries successfully, we use a question and answering (QnA) system that retrieves data on product pages and extracts answers. With various data sources available on the product pages, we deal with unique challenges such as different terminologies and data formats for successful answer retrieval. In this paper, we propose two different bi-encoder architectures for retrieving data from each of the two data sources considered – product descriptions and specifications. The proposed architectures beat the baseline approaches while maintaining a high recall and low latency in production. We envision that the proposed approaches can be widely applicable to other e-commerce QnA systems.|我们已经开发了一个称为决策助理(DA)的会话助理来帮助客户做出购买决策。为了成功地回答客户的查询，我们使用一个问答(QnA)系统来检索产品页面上的数据并提取答案。随着各种数据源可在产品页面，我们处理独特的挑战，如不同的术语和数据格式，以成功的答案检索。在本文中，我们提出了两种不同的双编码器体系结构来检索数据从每个两个数据源考虑-产品描述和规格。所提出的体系结构打破了基线方法，同时在生产中保持了较高的召回率和较低的延迟。我们设想所提出的方法可以广泛应用于其他电子商务 QnA 系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Passage+Retrieval+in+E-Commerce)|0|
|[Quantize Sequential Recommenders Without Private Data](https://doi.org/10.1145/3543507.3583351)|Lingfeng Shi, Yuang Liu, Jun Wang, Wei Zhang|East China Normal University, China|Deep neural networks have achieved great success in sequential recommendation systems. While maintaining high competence in user modeling and next-item recommendation, these models have long been plagued by the numerous parameters and computation, which inhibit them to be deployed on resource-constrained mobile devices. Model quantization, as one of the main paradigms for compression techniques, converts float parameters to low-bit values to reduce parameter redundancy and accelerate inference. To avoid drastic performance degradation, it usually requests a fine-tuning phase with an original dataset. However, the training set of user-item interactions is not always available due to transmission limits or privacy concerns. In this paper, we propose a novel framework to quantize sequential recommenders without access to any real private data. A generator is employed in the framework to synthesize fake sequence samples to feed the quantized sequential recommendation model and minimize the gap with a full-precision sequential recommendation model. The generator and the quantized model are optimized with a min-max game — alternating discrepancy estimation and knowledge transfer. Moreover, we devise a two-level discrepancy modeling strategy to transfer information between the quantized model and the full-precision model. The extensive experiments of various recommendation networks on three public datasets demonstrate the effectiveness of the proposed framework.|深层神经网络在序贯推荐系统中取得了巨大的成功。尽管这些模型在用户建模和下一个项目推荐方面保持了很高的能力，但长期以来，这些模型一直受到众多参数和计算的困扰，这些参数和计算阻碍了它们被部署到资源受限的移动设备上。模型量化作为压缩技术的主要范式之一，将浮点参数转换为低位值，以减少参数冗余，加速推理。为了避免严重的性能下降，它通常要求对原始数据集进行微调。然而，由于传输限制或隐私问题，用户项交互的训练集并不总是可用的。在本文中，我们提出了一个新的框架，量化顺序推荐没有访问任何真正的私有数据。该框架采用生成器对伪序列样本进行合成，以满足量化序列推荐模型的要求，同时采用全精度序列推荐模型使推荐间隔最小化。利用最小-最大对策-交替差异估计和知识转移对生成器和量化模型进行优化。此外，我们还设计了一个两层差异建模策略来传递量化模型和全精度模型之间的信息。在三个公共数据集上对各种推荐网络进行了广泛的实验，证明了该框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quantize+Sequential+Recommenders+Without+Private+Data)|0|
|[Adap-τ : Adaptively Modulating Embedding Magnitude for Recommendation](https://doi.org/10.1145/3543507.3583363)|Jiawei Chen, Junkang Wu, Jiancan Wu, Xuezhi Cao, Sheng Zhou, Xiangnan He|University of Science and Technology of China, China; ; Zhejiang University, China|Recent years have witnessed the great successes of embedding-based methods in recommender systems. Despite their decent performance, we argue one potential limitation of these methods — the embedding magnitude has not been explicitly modulated, which may aggravate popularity bias and training instability, hindering the model from making a good recommendation. It motivates us to leverage the embedding normalization in recommendation. By normalizing user/item embeddings to a specific value, we empirically observe impressive performance gains (9% on average) on four real-world datasets. Although encouraging, we also reveal a serious limitation when applying normalization in recommendation — the performance is highly sensitive to the choice of the temperature τ which controls the scale of the normalized embeddings. To fully foster the merits of the normalization while circumvent its limitation, this work studied on how to adaptively set the proper τ. Towards this end, we first make a comprehensive analyses of τ to fully understand its role on recommendation. We then accordingly develop an adaptive fine-grained strategy Adap-τ for the temperature with satisfying four desirable properties including adaptivity, personalized, efficiency and model-agnostic. Extensive experiments have been conducted to validate the effectiveness of the proposal. The code is available at https://github.com/junkangwu/Adap_tau.|近年来，基于嵌入的方法在推荐系统中取得了巨大成功。尽管这些方法表现出色，但我们发现其存在一个潜在缺陷——嵌入向量的模长未被显式调控，这可能加剧流行度偏差和训练不稳定性，从而阻碍模型生成优质推荐。这一发现促使我们探索归一化技术在推荐系统中的应用。通过将用户/物品嵌入向量归一化至特定数值，我们在四个真实数据集上实证观察到了显著的性能提升（平均提升9%）。虽然结果令人鼓舞，但我们也揭示了推荐场景中应用归一化的严重局限——模型性能对控制归一化嵌入尺度的温度系数τ高度敏感。为充分发挥归一化优势同时规避其局限，本研究致力于探索如何自适应地确定最佳τ值。为此，我们首先对τ展开了全面分析以深入理解其在推荐中的作用机制，进而提出满足四大特性（自适应性、个性化、高效性、模型无关性）的细粒度温度调节策略Adap-τ。大量实验验证了该方案的有效性，相关代码已开源在https://github.com/junkangwu/Adap_tau。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adap-τ+:+Adaptively+Modulating+Embedding+Magnitude+for+Recommendation)|0|
|[Clustered Embedding Learning for Recommender Systems](https://doi.org/10.1145/3543507.3583362)|Yizhou Chen, Guangda Huzhang, Anxiang Zeng, Qingtao Yu, Hui Sun, HengYi Li, Jingyi Li, Yabo Ni, Han Yu, Zhiming Zhou|Shanghai University of Finance and Economics, China; SCSE, Nanyang Technological University, Singapore; Shopee Pte Ltd., Singapore|In recent years, recommender systems have advanced rapidly, where embedding learning for users and items plays a critical role. A standard method learns a unique embedding vector for each user and item. However, such a method has two important limitations in real-world applications: 1) it is hard to learn embeddings that generalize well for users and items with rare interactions on their own; and 2) it may incur unbearably high memory costs when the number of users and items scales up. Existing approaches either can only address one of the limitations or have flawed overall performances. In this paper, we propose Clustered Embedding Learning (CEL) as an integrated solution to these two problems. CEL is a plug-and-play embedding learning framework that can be combined with any differentiable feature interaction model. It is capable of achieving improved performance, especially for cold users and items, with reduced memory cost. CEL enables automatic and dynamic clustering of users and items in a top-down fashion, where clustered entities jointly learn a shared embedding. The accelerated version of CEL has an optimal time complexity, which supports efficient online updates. Theoretically, we prove the identifiability and the existence of a unique optimal number of clusters for CEL in the context of nonnegative matrix factorization. Empirically, we validate the effectiveness of CEL on three public datasets and one business dataset, showing its consistently superior performance against current state-of-the-art methods. In particular, when incorporating CEL into the business model, it brings an improvement of $+0.6\%$ in AUC, which translates into a significant revenue gain; meanwhile, the size of the embedding table gets $2650$ times smaller.|近年来，推荐系统发展迅速，其中用户和项目的嵌入式学习起着至关重要的作用。标准方法为每个用户和项学习唯一的嵌入向量。然而，这种方法在实际应用中有两个重要的局限性: 1)很难学习嵌入式技术，这种技术可以很好地适用于用户和具有罕见交互的项目; 2)当用户和项目的数量增加时，它可能会产生难以忍受的高内存成本。现有的方法要么只能解决其中的一个限制，要么具有有缺陷的整体性能。在本文中，我们提出了集群嵌入式学习(CEL)作为这两个问题的综合解决方案。CEL 是一个即插即用的嵌入式学习框架，可以与任何可微的特征交互模型相结合。它能够提高性能，特别是对于冷用户和项目，同时降低内存成本。CEL 以自顶向下的方式支持用户和项目的自动和动态集群，集群实体可以在这种方式下联合学习共享嵌入。CEL 的加速版本具有最佳的时间复杂度，支持高效的在线更新。理论上，我们证明了在非负矩阵分解的情况下，CEL 的可识别性和唯一最优簇数的存在性。通过实验，我们验证了 CEL 在三个公共数据集和一个业务数据集上的有效性，显示了与当前最先进的方法相比，CEL 始终具有优越的性能。特别是，当将 CEL 融入到商业模式中时，它在 AUC 中带来了 $+ 0.6% 的改进，这意味着显著的收入增长; 与此同时，嵌入表的大小变小了2650美元。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Clustered+Embedding+Learning+for+Recommender+Systems)|0|
|[MMMLP: Multi-modal Multilayer Perceptron for Sequential Recommendations](https://doi.org/10.1145/3543507.3583378)|Jiahao Liang, Xiangyu Zhao, Muyang Li, Zijian Zhang, Wanyu Wang, Haochen Liu, Zitao Liu|Jilin University, China and City University of Hong Kong, Hong Kong; Michigan State University, USA; Guangdong Institute of Smart Education, Jinan University, China; University of Sydney, Australia; City University of Hong Kong, Hong Kong|Sequential recommendation aims to offer potentially interesting products to users by capturing their historical sequence of interacted items. Although it has facilitated extensive physical scenarios, sequential recommendation for multi-modal sequences has long been neglected. Multi-modal data that depicts a user’s historical interactions exists ubiquitously, such as product pictures, textual descriptions, and interacted item sequences, providing semantic information from multiple perspectives that comprehensively describe a user’s preferences. However, existing sequential recommendation methods either fail to directly handle multi-modality or suffer from high computational complexity. To address this, we propose a novel Multi-Modal Multi-Layer Perceptron (MMMLP) for maintaining multi-modal sequences for sequential recommendation. MMMLP is a purely MLP-based architecture that consists of three modules - the Feature Mixer Layer, Fusion Mixer Layer, and Prediction Layer - and has an edge on both efficacy and efficiency. Extensive experiments show that MMMLP achieves state-of-the-art performance with linear complexity. We also conduct ablating analysis to verify the contribution of each component. Furthermore, compatible experiments are devised, and the results show that the multi-modal representation learned by our proposed model generally benefits other recommendation models, emphasizing our model’s ability to handle multi-modal information. We have made our code available online to ease reproducibility1.|顺序推荐旨在通过获取用户交互项的历史顺序，为用户提供潜在有趣的产品。虽然它促进了广泛的物理场景，多模态序列的顺序推荐长期以来被忽视。描述用户历史交互的多模态数据无处不在，比如产品图片、文本描述和交互式项目序列，从多个角度提供语义信息，全面描述用户的偏好。然而，现有的顺序推荐方法要么不能直接处理多模态问题，要么计算复杂度较高。为了解决这个问题，我们提出了一种新的多模态多层感知器(MMMLP)来维护多模态序列的顺序推荐。MMLP 是一个纯粹基于 MLP 的架构，它由三个模块组成——特征混合层、融合混合层和预测层——并且在功效和效率方面都有优势。大量的实验表明，MMLP 在线性复杂度方面达到了最先进的性能。我们还进行了烧蚀分析，以验证每个组分的贡献。此外，设计了相容性实验，结果表明，我们提出的模型学习的多模态表示一般有利于其他推荐模型，强调我们的模型的能力，处理多模态信息。我们已经在网上提供了我们的代码，以便于重现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MMMLP:+Multi-modal+Multilayer+Perceptron+for+Sequential+Recommendations)|0|
|[AutoMLP: Automated MLP for Sequential Recommendations](https://doi.org/10.1145/3543507.3583440)|Muyang Li, Zijian Zhang, Xiangyu Zhao, Wanyu Wang, Minghao Zhao, Runze Wu, Ruocheng Guo|City University of Hong Kong, Hong Kong and Jilin University, China; Fuxi AI Lab, NetEase, China; Bytedance AI Lab UK, United Kingdom; City University of Hong Kong, Hong Kong and University of Sydney, Australia; City University of Hong Kong, Hong Kong|Sequential recommender systems aim to predict users' next interested item given their historical interactions. However, a long-standing issue is how to distinguish between users' long/short-term interests, which may be heterogeneous and contribute differently to the next recommendation. Existing approaches usually set pre-defined short-term interest length by exhaustive search or empirical experience, which is either highly inefficient or yields subpar results. The recent advanced transformer-based models can achieve state-of-the-art performances despite the aforementioned issue, but they have a quadratic computational complexity to the length of the input sequence. To this end, this paper proposes a novel sequential recommender system, AutoMLP, aiming for better modeling users' long/short-term interests from their historical interactions. In addition, we design an automated and adaptive search algorithm for preferable short-term interest length via end-to-end optimization. Through extensive experiments, we show that AutoMLP has competitive performance against state-of-the-art methods, while maintaining linear computational complexity.|顺序推荐系统的目的是预测用户的下一个感兴趣的项目给予他们的历史交互。然而，一个长期存在的问题是如何区分用户的长期和短期利益，这可能是不同的，并作出不同的贡献下一个建议。现有方法通常通过穷举搜索或实证经验来设定预先确定的短期利率长度，这种方法要么效率极低，要么效果不佳。尽管存在上述问题，最近的先进的基于变压器的模型能够实现最先进的性能，但是它们对于输入序列的长度具有二次计算复杂度。为此，本文提出了一种新的顺序推荐系统—— AutoMLP，旨在从用户的历史交互中更好地建立用户的长期/短期兴趣模型。此外，我们设计了一个自动化和自适应的搜索算法，通过端到端优化较好的短期兴趣长度。通过大量的实验，我们发现 AutoMLP 在保持线性计算复杂度的同时，具有与最先进的方法相竞争的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoMLP:+Automated+MLP+for+Sequential+Recommendations)|0|
|[NASRec: Weight Sharing Neural Architecture Search for Recommender Systems](https://doi.org/10.1145/3543507.3583446)|Tunhou Zhang, Dehua Cheng, Yuchen He, Zhengxing Chen, Xiaoliang Dai, Liang Xiong, Feng Yan, Hai Li, Yiran Chen, Wei Wen|Meta AI, USA; Duke University, USA; University of Houston, USA|The rise of deep neural networks offers new opportunities in optimizing recommender systems. However, optimizing recommender systems using deep neural networks requires delicate architecture fabrication. We propose NASRec, a paradigm that trains a single supernet and efficiently produces abundant models/sub-architectures by weight sharing. To overcome the data multi-modality and architecture heterogeneity challenges in the recommendation domain, NASRec establishes a large supernet (i.e., search space) to search the full architectures. The supernet incorporates versatile choice of operators and dense connectivity to minimize human efforts for finding priors. The scale and heterogeneity in NASRec impose several challenges, such as training inefficiency, operator-imbalance, and degraded rank correlation. We tackle these challenges by proposing single-operator any-connection sampling, operator-balancing interaction modules, and post-training fine-tuning. Our crafted models, NASRecNet, show promising results on three Click-Through Rates (CTR) prediction benchmarks, indicating that NASRec outperforms both manually designed models and existing NAS methods with state-of-the-art performance. Our work is publicly available at https://github.com/facebookresearch/NasRec.|深层神经网络的兴起为优化推荐系统提供了新的机会。然而，使用深层神经网络优化推荐系统需要精细的架构制作。我们提出 NASRec，一个训练单个超级网络并通过权重分享有效地产生丰富的模型/子架构的范例。为了克服推荐域中的数据多态性和体系结构异构性挑战，NASRec 建立了一个大型超网(即搜索空间)来搜索完整的体系结构。超级网结合了多种操作员的选择和密集的连接，以最大限度地减少人的努力，找到前科。NASRec 的规模和异质性带来了一些挑战，如培训效率低下、操作员失衡和等级相关性降低。我们通过提出单操作者任意连接采样、操作者平衡交互模块和训练后微调来应对这些挑战。我们精心设计的模型 NASRecNet 在三个点击率(Click-Through Rate，CTR)预测基准上显示出有希望的结果，表明 NASRecc 的性能优于手工设计的模型和现有的 NAS 方法，具有最先进的性能。我们的工作 https://github.com/facebookresearch/nasrec 公开。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NASRec:+Weight+Sharing+Neural+Architecture+Search+for+Recommender+Systems)|0|
|[Membership Inference Attacks Against Sequential Recommender Systems](https://doi.org/10.1145/3543507.3583447)|Zhihao Zhu, Chenwang Wu, Rui Fan, Defu Lian, Enhong Chen|University of Science and Technology of China, China|Recent studies have demonstrated the vulnerability of recommender systems to membership inference attacks, which determine whether a user’s historical data was utilized for model training, posing serious privacy leakage issues. Existing works assumed that member and non-member users follow different recommendation modes, and then infer membership based on the difference vector between the user’s historical behaviors and the recommendation list. The previous frameworks are invalid against inductive recommendations, such as sequential recommendations, since the disparities of difference vectors constructed by the recommendations between members and non-members become imperceptible. This motivates us to dig deeper into the target model. In addition, most MIA frameworks assume that they can obtain some in-distribution data from the same distribution of the target data, which is hard to gain in recommender system. To address these difficulties, we propose a Membership Inference Attack framework against sequential recommenders based on Model Extraction(ME-MIA). Specifically, we train a surrogate model to simulate the target model based on two universal loss functions. For a given behavior sequence, the loss functions ensure the recommended items and corresponding rank of the surrogate model are consistent with the target model’s recommendation. Due to the special training mode of the surrogate model, it is hard to judge which user is its member(non-member). Therefore, we establish a shadow model and use shadow model’s members(non-members) to train the attack model later. Next, we build a user feature generator to construct representative feature vectors from the shadow(surrogate) model. The crafting feature vectors are finally input into the attack model to identify users’ membership. Furthermore, to tackle the high cost of obtaining in-distribution data, we develop two variants of ME-MIA, realizing data-efficient and even data-free MIA by fabricating authentic in-distribution data. Notably, the latter is impossible in the previous works. Finally, we evaluate ME-MIA against multiple sequential recommendation models on three real-world datasets. Experimental results show that ME-MIA and its variants can achieve efficient extraction and outperform state-of-the-art algorithms in terms of attack performance.|最近的研究表明，推荐系统容易受到成员推断攻击，这决定了用户的历史数据是否被用于模型训练，造成严重的隐私泄露问题。现有的研究假设成员用户和非成员用户遵循不同的推荐模式，然后根据用户历史行为和推荐列表之间的差异向量推断成员关系。以前的框架对于归纳推荐(如顺序推荐)是无效的，因为成员和非成员之间由推荐构造的差异向量的差异变得不可察觉。这促使我们更深入地研究目标模型。此外，大多数 MIA 框架都假定它们可以从目标数据的同一分布中获得一些分布内数据，而这在推荐系统中是很难获得的。为了解决这些问题，我们提出了一个基于模型提取(ME-MIA)的针对顺序推荐的成员推理攻击框架。具体来说，我们训练了一个代理模型来模拟目标模型基于两个通用的损失函数。对于给定的行为序列，损失函数保证代理模型的推荐项和相应的等级与目标模型的推荐一致。由于代理模型的特殊训练模式，很难判断哪个用户是它的成员(非成员)。因此，我们建立了一个阴影模型，然后利用阴影模型的成员(非成员)来训练攻击模型。接下来，我们构建一个用户特征生成器来从阴影(代理)模型中构造具有代表性的特征向量。最后将特征向量输入到攻击模型中，识别用户的隶属关系。此外，为了解决获取内部分发数据的高成本问题，我们开发了两种不同的 ME-MIA，通过制作真实的内部分发数据来实现数据高效甚至无数据的 MIA。值得注意的是，后者在前面的作品中是不可能的。最后，我们在三个实际数据集上对多个顺序推荐模型进行 ME-MIA 评估。实验结果表明，ME-MIA 算法及其变体能够实现有效的提取，并且在攻击性能方面优于目前最先进的算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Membership+Inference+Attacks+Against+Sequential+Recommender+Systems)|0|
|[Communicative MARL-based Relevance Discerning Network for Repetition-Aware Recommendation](https://doi.org/10.1145/3543507.3583459)|Kaiyuan Li, Pengfei Wang, Haitao Wang, Qiang Liu, Xingxing Wang, Dong Wang, Shangguang Wang|Beijing University of Posts and Telecommunications, China; Meituan, China|The repeated user-item interaction now is becoming a common phenomenon in the e-commerce scenario. Due to its potential economic profit, various models are emerging to predict which item will be re-interacted based on the user-item interactions. In this specific scenario, item relevance is a critical factor that needs to be concerned, which tends to have different effects on the succeeding re-interacted one (i.e., stimulating or delaying its emergence). It is necessary to make a detailed discernment of item relevance for a better repetition-aware recommendation. Unfortunately, existing works usually mixed all these types, which may disturb the learning process and result in poor performance. In this paper, we introduce a novel Communicative MARL-based Relevance Discerning Network (CARDfor short) to automatically discern the item relevance for a better repetition-aware recommendation. Specifically, CARDformalizes the item relevance discerning problem into a communication selection process in MARL. CARDtreats each unique interacted item as an agent and defines three different communication types over agents, which are stimulative, inhibitive, and noisy respectively. After this, CARDutilizes a Gumbel-enhanced classifier to distinguish the communication types among agents, and an attention-based Reactive Point Process is further designed to transmit the well-discerned stimulative and inhibitive incentives separately among all agents to make an effective collaboration for repetition decisions. Experimental results on two real-world e-commerce datasets show that our proposed method outperforms the state-of-the-art recommendation methods in terms of both sequential and repetition-aware recommenders. Furthermore, CARDis also deployed in the online sponsored search advertising system in Meituan, obtaining a performance improvement of over 1.5% and 1.2% in CTR and effective Cost Per Mille (eCPM) respectively, which is significant to the business.|重复的用户-项目交互现在正在成为电子商务场景中的一个普遍现象。由于其潜在的经济利益，各种模型正在出现，以预测哪些项目将重新交互的基础上，用户项目的交互。在这个特定的场景中，项目相关性是一个需要关注的关键因素，它往往对后续的重新相互作用有不同的影响(即，刺激或延迟其出现)。有必要对项目的相关性进行详细的识别，以便更好地提出有重复意识的建议。不幸的是，现有的作品往往混合了所有这些类型，这可能会干扰学习过程，导致较差的表现。本文介绍了一种新的基于交际 MARL 的关联识别网络(CARD) ，该网络可以自动识别项目的相关性，从而获得更好的重复感知推荐。特别地，CARD 将项目相关性识别问题形式化为 MARL 中的通信选择过程。CARD 将每个独特的交互项目视为一个代理，并定义了代理上的三种不同的通信类型，分别是刺激性、抑制性和噪声性。此后，CARD 利用 Gumbel 增强的分类器来区分代理人之间的通信类型，并进一步设计基于注意力的反应点过程，以在所有代理人之间分别传递明确的刺激和抑制激励，以便为重复决策进行有效的协作。在两个实际电子商务数据集上的实验结果表明，该方法在顺序推荐和重复推荐方面都优于目前最先进的推荐方法。此外，CARD 还部署在在线赞助的搜索广告系统中，美团点击率和有效每公里成本(eCPM)分别提高了1.5% 和1.2% ，这对业务具有重要意义。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Communicative+MARL-based+Relevance+Discerning+Network+for+Repetition-Aware+Recommendation)|0|
|[Personalized Graph Signal Processing for Collaborative Filtering](https://doi.org/10.1145/3543507.3583466)|Jiahao Liu, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, Li Shang, Ning Gu|Microsoft Research Asia, China; School of Computer Science, Fudan University, China and Shanghai Key Laboratory of Data Science, Fudan University, China; Amazon, USA|The collaborative filtering (CF) problem with only user-item interaction information can be solved by graph signal processing (GSP), which uses low-pass filters to smooth the observed interaction signals on the similarity graph to obtain the prediction signals. However, the interaction signal may not be sufficient to accurately characterize user interests and the low-pass filters may ignore the useful information contained in the high-frequency component of the observed signals, resulting in suboptimal accuracy. To this end, we propose a personalized graph signal processing (PGSP) method for collaborative filtering. Firstly, we design the personalized graph signal containing richer user information and construct an augmented similarity graph containing more graph topology information, to more effectively characterize user interests. Secondly, we devise a mixed-frequency graph filter to introduce useful information in the high-frequency components of the observed signals by combining an ideal low-pass filter that smooths signals globally and a linear low-pass filter that smooths signals locally. Finally, we combine the personalized graph signal, the augmented similarity graph and the mixed-frequency graph filter by proposing a pipeline consisting of three key steps: pre-processing, graph convolution and post-processing. Extensive experiments show that PGSP can achieve superior accuracy compared with state-of-the-art CF methods and, as a nonparametric method, PGSP has very high training efficiency.|图形信号处理(gSP)可以解决只有用户-项目交互信息的协同过滤(CF)问题，它使用低通滤波器平滑相似图上观察到的交互信号，以获得预测信号。然而，交互信号可能不足以准确地表征用户的兴趣，而且低通滤波器可能会忽略观测信号的高频分量中包含的有用信息，从而导致次优精度。为此，我们提出了一个个性化的图形信号处理(PgSP)方法来处理协同过滤。首先，设计了包含更丰富用户信息的个性化图形信号，构造了包含更多图形拓扑信息的增广相似度图，以更有效地刻画用户兴趣。其次，我们设计了一个混合频率图形滤波器，通过结合理想的低通滤波器对信号进行全局平滑和线性低通滤波器对信号进行局部平滑，从而在观测信号的高频成分中引入有用的信息。最后，结合个性化图形信号、增强相似图和混合频率图滤波，提出了一种由预处理、图卷积和后处理三个关键步骤组成的流水线。大量的实验表明，与现有的 CF 方法相比，PGSP 具有更高的精度，并且作为一种非参数方法，PGSP 具有很高的训练效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Graph+Signal+Processing+for+Collaborative+Filtering)|0|
|[Multi-Task Recommendations with Reinforcement Learning](https://doi.org/10.1145/3543507.3583467)|Ziru Liu, Jiejie Tian, Qingpeng Cai, Xiangyu Zhao, Jingtong Gao, Shuchang Liu, Dayou Chen, Tonghao He, Dong Zheng, Peng Jiang, Kun Gai|Kuaishou, China; City University of Hong Kong, China; Unaffiliated, China|In recent years, Multi-task Learning (MTL) has yielded immense success in Recommender System (RS) applications. However, current MTL-based recommendation models tend to disregard the session-wise patterns of user-item interactions because they are predominantly constructed based on item-wise datasets. Moreover, balancing multiple objectives has always been a challenge in this field, which is typically avoided via linear estimations in existing works. To address these issues, in this paper, we propose a Reinforcement Learning (RL) enhanced MTL framework, namely RMTL, to combine the losses of different recommendation tasks using dynamic weights. To be specific, the RMTL structure can address the two aforementioned issues by (i) constructing an MTL environment from session-wise interactions and (ii) training multi-task actor-critic network structure, which is compatible with most existing MTL-based recommendation models, and (iii) optimizing and fine-tuning the MTL loss function using the weights generated by critic networks. Experiments on two real-world public datasets demonstrate the effectiveness of RMTL with a higher AUC against state-of-the-art MTL-based recommendation models. Additionally, we evaluate and validate RMTL's compatibility and transferability across various MTL models.|近年来，多任务学习在推荐系统应用方面取得了巨大的成功。然而，目前基于 MTL 的推荐模型倾向于忽略用户项目交互的会话模式，因为它们主要是基于项目数据集构建的。此外，平衡多个目标一直是这个领域的一个挑战，这通常是通过现有工作中的线性估计来避免的。为了解决这些问题，在本文中，我们提出了一个强化学习增强的 MTL 框架，即 RMTL，它使用动态权重来组合不同推荐任务的丢失。具体来说，RMTL 结构可以解决上述两个问题: (1)通过会话交互构建 MTL 环境; (2)训练与大多数基于 MTL 的推荐模型兼容的多任务参与者-评论者网络结构; (3)利用评论者网络生成的权重优化和微调 MTL 损失函数。在两个真实世界的公共数据集上的实验证明了具有较高 AUC 的 RMTL 对基于最新 MTL 的推荐模型的有效性。此外，我们评估和验证 RMTL 的兼容性和跨各种 MTL 模型的可转移性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Task+Recommendations+with+Reinforcement+Learning)|0|
|[A Self-Correcting Sequential Recommender](https://doi.org/10.1145/3543507.3583479)|Yujie Lin, Chenyang Wang, Zhumin Chen, Zhaochun Ren, Xin Xin, Qiang Yan, Maarten de Rijke, Xiuzhen Cheng, Pengjie Ren|University of Amsterdam, Netherlands; Shandong University, China; WeChat, Tencent, China|Sequential recommendations aim to capture users' preferences from their historical interactions so as to predict the next item that they will interact with. Sequential recommendation methods usually assume that all items in a user's historical interactions reflect her/his preferences and transition patterns between items. However, real-world interaction data is imperfect in that (i) users might erroneously click on items, i.e., so-called misclicks on irrelevant items, and (ii) users might miss items, i.e., unexposed relevant items due to inaccurate recommendations. To tackle the two issues listed above, we propose STEAM, a Self-correcTing sEquentiAl recoMmender. STEAM first corrects an input item sequence by adjusting the misclicked and/or missed items. It then uses the corrected item sequence to train a recommender and make the next item prediction.We design an item-wise corrector that can adaptively select one type of operation for each item in the sequence. The operation types are 'keep', 'delete' and 'insert.' In order to train the item-wise corrector without requiring additional labeling, we design two self-supervised learning mechanisms: (i) deletion correction (i.e., deleting randomly inserted items), and (ii) insertion correction (i.e., predicting randomly deleted items). We integrate the corrector with the recommender by sharing the encoder and by training them jointly. We conduct extensive experiments on three real-world datasets and the experimental results demonstrate that STEAM outperforms state-of-the-art sequential recommendation baselines. Our in-depth analyses confirm that STEAM benefits from learning to correct the raw item sequences.|序贯推荐旨在从用户的历史交互中获取他们的偏好，从而预测他们将要交互的下一个项目。顺序推荐方法通常假设用户历史交互中的所有项目都反映了用户的偏好和项目之间的转换模式。然而，真实世界的交互数据是不完美的，因为(i)用户可能会错误地点击项目，即所谓的不相关项目的错误点击，以及(ii)用户可能会错过项目，即由于不准确的推荐而未公开的相关项目。为了解决上面列出的两个问题，我们提出 STEAM，一个自我修正的 sEquentiAl 推荐器。STEAM 首先通过调整错误点击和/或错过的项目来更正输入项目序列。然后使用校正后的项目序列来训练推荐者并对下一个项目进行预测。我们设计了一个项目校正器，它可以自适应地为序列中的每个项目选择一种操作类型。操作类型为“ keep”、“ delete”和“ insert”。为了训练项目校正器而不需要额外的标签，我们设计了两个自我监督学习机制: (i)删除校正(即删除随机插入的项目)和(ii)插入校正(即预测随机删除的项目)。我们通过共享编码器和共同训练，将校正器和推荐器结合起来。我们在三个真实世界的数据集上进行了广泛的实验，实验结果表明 STEAM 的性能优于最先进的顺序推荐基线。我们的深入分析证实，STEAM 受益于学习纠正原始项目序列。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Self-Correcting+Sequential+Recommender)|0|
|[Confident Action Decision via Hierarchical Policy Learning for Conversational Recommendation](https://doi.org/10.1145/3543507.3583536)|Heeseon Kim, Hyeongjun Yang, KyongHo Lee|Department of Computer Science, Yonsei University, Republic of Korea|Conversational recommender systems (CRS) aim to acquire a user’s dynamic interests for a successful recommendation. By asking about his/her preferences, CRS explore current needs of a user and recommend items of interest. However, previous works may not determine a proper action in a timely manner which leads to the insufficient information gathering and the waste of conversation turns. Since they learn a single decision policy, it is difficult for them to address the general decision problems in CRS. Besides, existing methods do not distinguish whether the past behaviors inferred from the historical interactions are closely related to the user’s current preference. To address these issues, we propose a novel Hierarchical policy learning based Conversational Recommendation framework (HiCR). HiCR formulates the multi-round decision making process as a hierarchical policy learning scheme, which consists of both a high-level policy and a low-level policy. In detail, the high-level policy aims to determine what type of action to take, such as a recommendation or a query, by observing the comprehensive conversation information. According to the decided action type, the low-level policy selects a specific action, such as which attribute to ask or which item to recommend. The hierarchical conversation policy enables CRS to decide an optimal action, resulting in reducing the unnecessary consumption of conversation turns and the continuous failure of recommendations. Furthermore, in order to filter out the unnecessary historical information when enriching the current user preference, we extract and utilize the informative past behaviors that are attentive to the current needs. Empirical experiments on four real-world datasets show the superiority of our approach against the current state-of-the-art methods.|会话推荐系统(CRS)的目标是获取用户的动态兴趣，从而实现成功的推荐。通过询问用户的偏好，CRS 探索用户当前的需求并推荐感兴趣的项目。然而，以往的作品不能及时确定适当的行动，导致信息收集不足和谈话的浪费。由于他们只学习单一的决策策略，因此很难解决 CRS 中的一般决策问题。此外，现有的方法不能区分从历史交互中推断出的过去行为是否与用户当前的偏好密切相关。为了解决这些问题，我们提出了一种新的基于层次策略学习的会话推荐框架(HiCR)。HiCR 将多轮决策过程描述为一个分层的决策学习方案，该方案由高层决策和低层决策两部分组成。具体来说，高级策略旨在通过观察全面的会话信息来确定采取何种类型的操作，比如推荐或查询。根据确定的操作类型，底层策略选择一个特定的操作，比如询问哪个属性或推荐哪个项目。分层对话策略使 CRS 能够决定一个最优的操作，从而减少不必要的话轮消耗和建议的持续失败。此外，为了在丰富当前用户偏好时过滤掉不必要的历史信息，我们提取并利用了关注当前需求的信息性过去行为。在四个真实世界数据集上的实验表明了我们的方法相对于当前最先进的方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Confident+Action+Decision+via+Hierarchical+Policy+Learning+for+Conversational+Recommendation)|0|
|[Mutual Wasserstein Discrepancy Minimization for Sequential Recommendation](https://doi.org/10.1145/3543507.3583529)|Ziwei Fan, Zhiwei Liu, Hao Peng, Philip S. Yu|Beihang University, USA; Salesforce AI Research, USA; University of Illinois Chicago, USA|Self-supervised sequential recommendation significantly improves recommendation performance by maximizing mutual information with well-designed data augmentations. However, the mutual information estimation is based on the calculation of Kullback Leibler divergence with several limitations, including asymmetrical estimation, the exponential need of the sample size, and training instability. Also, existing data augmentations are mostly stochastic and can potentially break sequential correlations with random modifications. These two issues motivate us to investigate an alternative robust mutual information measurement capable of modeling uncertainty and alleviating KL divergence limitations. To this end, we propose a novel self-supervised learning framework based on Mutual WasserStein discrepancy minimization MStein for the sequential recommendation. We propose the Wasserstein Discrepancy Measurement to measure the mutual information between augmented sequences. Wasserstein Discrepancy Measurement builds upon the 2-Wasserstein distance, which is more robust, more efficient in small batch sizes, and able to model the uncertainty of stochastic augmentation processes. We also propose a novel contrastive learning loss based on Wasserstein Discrepancy Measurement. Extensive experiments on four benchmark datasets demonstrate the effectiveness of MStein over baselines. More quantitative analyses show the robustness against perturbations and training efficiency in batch size. Finally, improvements analysis indicates better representations of popular users or items with significant uncertainty. The source code is at https://github.com/zfan20/MStein.|自监督顺序推荐通过设计良好的数据增强最大化互信息，显著提高了推荐性能。然而，互信息估计是基于 Kullback Leibler 散度的计算，具有不对称估计、样本量的指数需求和训练不稳定性等局限性。此外，现有的数据扩充大多是随机的，并可能打破随机修改顺序相关性。这两个问题促使我们研究一种可替代的鲁棒互信息测量方法，该方法能够对不确定性进行建模并减轻 KL 发散的限制。为此，我们提出了一种新的基于 Mutual WasserStein 差异最小化 MStein 的自监督学习框架，用于顺序推荐。我们提出了 Wasserstein 差异度量来度量增广序列之间的互信息。Wasserstein 误差测度建立在2-Wasserstein 距离的基础上，该距离在小批量情况下具有更强的鲁棒性和更高的效率，能够对随机增量过程的不确定性进行建模。我们还提出了一种新的基于 Wasserstein 差异度量的对比学习损失。在四个基准数据集上的大量实验证明了 MStein 在基线上的有效性。进一步的定量分析表明，该算法具有较强的抗干扰能力，并且在批量情况下具有较高的训练效率。最后，改进分析表明，流行用户或具有显著不确定性的项目的表示更好。源代码在 https://github.com/zfan20/mstein。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mutual+Wasserstein+Discrepancy+Minimization+for+Sequential+Recommendation)|0|
|[Automatic Feature Selection By One-Shot Neural Architecture Search In Recommendation Systems](https://doi.org/10.1145/3543507.3583444)|He Wei, Yuekui Yang, Haiyang Wu, Yangyang Tang, Meixi Liu, Jianfeng Li|Machine learning platform department, TEG, Tencent, China and Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology, Tsinghua University, China; Machine learning platform department, TEG, Tencent, China|Feature selection is crucial in large-scale recommendation system, which can not only reduce the computational cost, but also improve the recommendation efficiency. Most existing works rank the features and then select the top-k ones as the final feature subset. However, they assess feature importance individually and ignore the interrelationship between features. Consequently, multiple features with high relevance may be selected simultaneously, resulting in sub-optimal result. In this work, we solve this problem by proposing an AutoML-based feature selection framework that can automatically search the optimal feature subset. Specifically, we first embed the search space into a weight-sharing Supernet. Then, a two-stage neural architecture search method is employed to evaluate the feature quality. In the first stage, a well-designed sampling method considering feature convergence fairness is applied to train the Supernet. In the second stage, a reinforcement learning method is used to search for the optimal feature subset efficiently. The Experimental results on two real datasets demonstrate the superior performance of new framework over other solutions. Our proposed method obtain significant improvement with a 20% reduction in the amount of features on the Criteo. More validation experiments demonstrate the ability and robustness of the framework.|特征选择是大规模推荐系统的关键，它不仅可以降低计算量，而且可以提高推荐效率。大多数已有的作品对特征进行排序，然后选择最上面的 k 个特征作为最终的特征子集。然而，他们单独评估特征的重要性，而忽略了特征之间的相互关系。因此，可以同时选择多个高相关性的特征，从而导致次优结果。针对这一问题，本文提出了一种基于 AutoML 的特征选择框架，该框架可以自动搜索最优特征子集。具体来说，我们首先将搜索空间嵌入到一个权重共享的超级网络中。然后，采用两阶段神经网络结构搜索方法对特征质量进行评价。在第一阶段，采用一种设计良好的考虑特征收敛公平性的抽样方法对超网进行训练。在第二阶段，使用强化学习方法有效地搜索最优特征子集。在两个实际数据集上的实验结果表明，新框架的性能优于其他解决方案。我们提出的方法获得了显着的改进，在标准的数量减少了20% 的特征。更多的验证实验证明了该框架的能力和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Feature+Selection+By+One-Shot+Neural+Architecture+Search+In+Recommendation+Systems)|0|
|[Catch: Collaborative Feature Set Search for Automated Feature Engineering](https://doi.org/10.1145/3543507.3583527)|Guoshan Lu, Haobo Wang, Saisai Yang, Jing Yuan, Guozheng Yang, Cheng Zang, Gang Chen, Junbo Zhao|Zheshang Bank Co., Ltd., China; Zhejiang University, China; Institute of Computing Innovation, Zhejiang University, China|Feature engineering often plays a crucial role in building mining systems for tabular data, which traditionally requires experienced human experts to perform. Thanks to the rapid advances in reinforcement learning, it has offered an automated alternative, i.e. automated feature engineering (AutoFE). In this work, through scrutiny of the prior AutoFE methods, we characterize several research challenges that remained in this regime, concerning system-wide efficiency, efficacy, and practicality toward production. We then propose Catch, a full-fledged new AutoFE framework that comprehensively addresses the aforementioned challenges. The core to Catch composes a hierarchical-policy reinforcement learning scheme that manifests a collaborative feature engineering exploration and exploitation grounded on the granularity of the whole feature set. At a higher level of the hierarchy, a decision-making module controls the post-processing of the attained feature engineering transformation. We extensively experiment with Catch on 26 academic standardized tabular datasets and 9 industrialized real-world datasets. Measured by numerous metrics and analyses, Catch establishes a new state-of-the-art, from perspectives performance, latency as well as its practicality towards production. Source code1 can be found at https://github.com/1171000709/Catch.|在构建表格数据挖掘系统时，特征工程往往起着至关重要的作用，表格数据挖掘传统上需要有经验的人类专家来完成。由于强化学习的快速发展，它提供了一种自动化的替代方案，即自动化特征工程(AutoFE)。在这项工作中，通过审查以前的自动有限元方法，我们描述了几个研究挑战，仍然在这个制度，关于系统的效率，效率和实用性的生产。然后，我们建议使用 Catch，这是一个成熟的新的 AutoFE 框架，可以全面解决上述挑战。Core to Catch 组成了一个层次化的策略强化学习方案，体现了基于整个特性集粒度的协同特性工程探索和开发。在层次结构的更高层次上，决策模块控制所获得的特征工程变换的后处理。我们对26个学术标准化表格数据集和9个工业化真实世界数据集进行了广泛的实验。通过大量的度量和分析，Catch 从性能、延迟以及对生产的实用性的角度建立了一个新的最先进的状态。源代码1可在 https://github.com/1171000709/catch 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Catch:+Collaborative+Feature+Set+Search+for+Automated+Feature+Engineering)|0|
|[The Hitchhiker's Guide to Facebook Web Tracking with Invisible Pixels and Click IDs](https://doi.org/10.1145/3543507.3583311)|Paschalis Bekos, Panagiotis Papadopoulos, Evangelos P. Markatos, Nicolas Kourtellis|University of Crete/FORTH, Greece; Telefonica Research, Spain; FORTH, Greece|Over the past years, advertisement companies have used various tracking methods to persistently track users across the web. Such tracking methods usually include first and third-party cookies, cookie synchronization, as well as a variety of fingerprinting mechanisms. Facebook (FB) recently introduced a new tagging mechanism that attaches a one-time tag as a URL parameter (FBCLID) on outgoing links to other websites. Although such a tag does not seem to have enough information to persistently track users, we demonstrate that despite its ephemeral nature, when combined with FB Pixel, it can aid in persistently monitoring user browsing behavior across i) different websites, ii) different actions on each website, iii) time, i.e., both in the past as well as in the future. We refer to this online monitoring of users as FB web tracking. We find that FB Pixel tracks a wide range of user activities on websites with alarming detail, especially on websites classified as sensitive categories under GDPR. Also, we show how the FBCLID tag can be used to match, and thus de-anonymize, activities of online users performed in the distant past (even before those users had a FB account) tracked by FB Pixel. In fact, by combining this tag with cookies that have rolling expiration dates, FB can also keep track of users' browsing activities in the future as well. Our experimental results suggest that 23% of the 10k most popular websites have adopted this technology, and can contribute to this activity tracking on the web. Furthermore, our longitudinal study shows that this type of user activity tracking can go as far back as 2015. Simply said, if a user creates for the first time a FB account today, FB could, under some conditions, match their anonymously collected past web browsing activity to their newly created FB profile, from as far back as 2015 and continue tracking their activity in the future.|在过去的几年里，广告公司使用了各种各样的跟踪方法来持续跟踪网络上的用户。这种跟踪方法通常包括第一方和第三方 cookie、 cookie 同步以及各种指纹识别机制。Facebook (FB)最近推出了一种新的标签机制，它将一次性标签作为 URL 参数(FBCLID)附加到其他网站的外向链接上。虽然这样的标签似乎没有足够的信息来持续跟踪用户，我们证明，尽管它的短暂性质，当结合 FB 像素，它可以帮助持续监测用户浏览行为在 i)不同的网站，ii)不同的行动在每个网站，iii)时间，即在过去和未来。我们把这种对用户的在线监控称为 FB 网络跟踪。我们发现，FB 像素跟踪广泛的用户活动的网站具有惊人的细节，特别是在网站归类为敏感类别下的 GDPR。此外，我们展示了如何使用 FBCLID 标签来匹配，从而去匿名，在线用户的活动执行在遥远的过去(甚至在那些用户有一个 FB 帐户之前)由 FB 像素跟踪。事实上，通过将这个标签与具有滚动过期日期的 cookie 相结合，FB 还可以跟踪用户未来的浏览活动。我们的实验结果表明，23% 的10k 最受欢迎的网站已经采用了这项技术，并可以有助于在网上跟踪这项活动。此外，我们的追踪研究显示，这种类型的用户活动跟踪可以追溯到2015年。简单地说，如果一个用户今天第一次创建一个 FB 帐户，FB 可以，在某些条件下，匹配他们的匿名收集过去的网页浏览活动和他们新创建的 FB 配置文件，从2015年开始，并在未来继续跟踪他们的活动。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Hitchhiker's+Guide+to+Facebook+Web+Tracking+with+Invisible+Pixels+and+Click+IDs)|0|
|[Atrapos: Real-time Evaluation of Metapath Query Workloads](https://doi.org/10.1145/3543507.3583322)|Serafeim Chatzopoulos, Thanasis Vergoulis, Dimitrios Skoutas, Theodore Dalamagas, Christos Tryfonopoulos, Panagiotis Karras|IMSI, Athena RC, Greece; University of the Peloponnese, Greece; Aarhus University, Denmark; University of the Peloponnese, Greece and IMSI, Athena RC, Greece|Heterogeneous information networks (HINs) represent different types of entities and relationships between them. Exploring and mining HINs relies on metapath queries that identify pairs of entities connected by relationships of diverse semantics. While the real-time evaluation of metapath query workloads on large, web-scale HINs is highly demanding in computational cost, current approaches do not exploit interrelationships among the queries. In this paper, we present Atrapos, a new approach for the real-time evaluation of metapath query workloads that leverages a combination of efficient sparse matrix multiplication and intermediate result caching. Atrapos selects intermediate results to cache and reuse by detecting frequent sub-metapaths among workload queries in real time, using a tailor-made data structure, the Overlap Tree, and an associated caching policy. Our experimental study on real data shows that Atrapos accelerates exploratory data analysis and mining on HINs, outperforming off-the-shelf caching approaches and state-of-the-art research prototypes in all examined scenarios.|异构信息网络(HIN)表示不同类型的实体以及它们之间的关系。探索和挖掘 HIN 依赖于元路径查询，这些查询标识由不同语义关系连接的实体对。尽管对大型 Web 规模 HIN 上的元路径查询工作负载进行实时评估对计算成本要求很高，但目前的方法没有利用查询之间的相互关系。在这篇文章中，我们介绍了一种新的实时评估元路径查询工作负载的方法—— Arapos，它结合了高效的稀疏矩阵乘法和中间结果缓存。Apapos 通过实时检测工作负载查询之间频繁的子元路径，使用量身定制的数据结构、重叠树和相关的缓存策略来选择缓存和重用的中间结果。我们对真实数据的实验研究表明，在所有经过检验的场景中，阿特波斯加速了 HIN 的探索性数据分析和挖掘，表现优于现成的缓存方法和最先进的研究原型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Atrapos:+Real-time+Evaluation+of+Metapath+Query+Workloads)|0|
|[TRAVERS: A Diversity-Based Dynamic Approach to Iterative Relevance Search over Knowledge Graphs](https://doi.org/10.1145/3543507.3583429)|Ziyang Li, Yu Gu, Yulin Shen, Wei Hu, Gong Cheng|Ohio State University, USA; State Key Laboratory for Novel Software Technology, Nanjing University, China|Relevance search over knowledge graphs seeks top-ranked answer entities that are most relevant to a query entity. Since the semantics of relevance varies with the user need and its formalization is difficult for non-experts, existing methods infer semantics from user-provided example answer entities. However, a user may provide very few examples, even none at the beginning of interaction, thereby limiting the effectiveness of such methods. In this paper, we vision a more practical scenario called labeling-based iterative relevance search: instead of effortfully inputting example answer entities, the user effortlessly (e.g., implicitly) labels current answer entities, and is rewarded with improved answer entities in the next iteration. To realize the scenario, our approach TRAVERS incorporates two rankers: a diversity-oriented ranker for supporting cold start and avoiding converging to sub-optimum caused by noisy labels, and a relevance-oriented ranker capable of handling unbalanced labels. Moreover, the two rankers and their combination dynamically evolve over iterations. TRAVERS outperformed a variety of baselines in experiments with simulated and real user behavior.|基于知识图的相关性搜索寻找与查询实体最相关的排名最高的答案实体。由于相关性的语义随用户需求而变化，而且对于非专家来说，相关性的形式化很困难，现有的方法都是从用户提供的示例答案实体中推断语义。然而，用户可能只提供很少的例子，甚至在交互开始时没有例子，从而限制了这些方法的有效性。在本文中，我们设想了一个更实际的场景，叫做基于标签的迭代相关性搜索: 用户不必费力地输入示例答案实体，而是毫不费力地(例如，隐式地)标记当前答案实体，并在下一次迭代中得到改进的答案实体。为了实现该方案，我们的方法 TRAVERS 包含两个排序器: 一个面向多样性的排序器支持冷启动，避免收敛到次优由于噪声标签，和一个相关性导向的排序器能够处理不平衡的标签。此外，这两个排名及其组合在迭代中动态演化。在模拟和真实用户行为的实验中，TRAVERS 的表现优于各种基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TRAVERS:+A+Diversity-Based+Dynamic+Approach+to+Iterative+Relevance+Search+over+Knowledge+Graphs)|0|
|[Message Function Search for Knowledge Graph Embedding](https://doi.org/10.1145/3543507.3583546)|Shimin Di, Lei Chen|The Hong Kong University of Science and Technology (Guangzhou), China; The Hong Kong University of Secience and Technology, China|Recently, many promising embedding models have been proposed to embed knowledge graphs (KGs) and their more general forms, such as n-ary relational data (NRD) and hyper-relational KG (HKG). To promote the data adaptability and performance of embedding models, KG searching methods propose to search for suitable models for a given KG data set. But they are restricted to a single KG form, and the searched models are restricted to a single type of embedding model. To tackle such issues, we propose to build a search space for the message function in graph neural networks (GNNs). However, it is a non-trivial task. Existing message function designs fix the structures and operators, which makes them difficult to handle different KG forms and data sets. Therefore, we first design a novel message function space, which enables both structures and operators to be searched for the given KG form (including KG, NRD, and HKG) and data. The proposed space can flexibly take different KG forms as inputs and is expressive to search for different types of embedding models. Especially, some existing message function designs and some classic KG embedding models can be instantiated as special cases of our space. We empirically show that the searched message functions are data-dependent, and can achieve leading performance on benchmark KGs, NRD, and HKGs.|近年来，人们提出了许多有前途的嵌入模型来嵌入知识图(KG)及其更一般的形式，如 n 元关系数据(NRD)和超关系 KG (HKG)。为了提高嵌入模型的数据适应性和性能，KG 搜索方法提出为给定的 KG 数据集寻找合适的模型。但是它们仅限于单一的 KG 形式，所搜索的模型仅限于单一类型的嵌入模型。为了解决这些问题，我们提出在图神经网络(GNN)中建立一个消息函数的搜索空间。然而，这是一项非常重要的任务。现有的消息功能设计固定了结构和操作符，使得它们难以处理不同的 KG 表单和数据集。因此，我们首先设计一个新的消息函数空间，它允许搜索给定的 KG 表单(包括 KG、 NRD 和 HKG)和数据的结构和操作符。该空间可以灵活地采用不同的 KG 形式作为输入，具有表达性，可以搜索不同类型的嵌入模型。特别是，现有的一些消息函数设计和一些经典的 KG 嵌入模型可以作为我们空间的特例进行实例化。实验结果表明，搜索消息函数具有数据依赖性，可以在基准幼儿园、 NRD 幼儿园和 HKG 幼儿园中取得领先的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Message+Function+Search+for+Knowledge+Graph+Embedding)|0|
|[FINGER: Fast Inference for Graph-based Approximate Nearest Neighbor Search](https://doi.org/10.1145/3543507.3583318)|Patrick H. Chen, WeiCheng Chang, JyunYu Jiang, HsiangFu Yu, Inderjit S. Dhillon, ChoJui Hsieh||Approximate K-Nearest Neighbor Search (AKNNS) has now become ubiquitous in modern applications, for example, as a fast search procedure with two tower deep learning models. Graph-based methods for AKNNS in particular have received great attention due to their superior performance. These methods rely on greedy graph search to traverse the data points as embedding vectors in a database. Under this greedy search scheme, we make a key observation: many distance computations do not influence search updates so these computations can be approximated without hurting performance. As a result, we propose FINGER, a fast inference method to achieve efficient graph search. FINGER approximates the distance function by estimating angles between neighboring residual vectors with low-rank bases and distribution matching. The approximated distance can be used to bypass unnecessary computations, which leads to faster searches. Empirically, accelerating a popular graph-based method named HNSW by FINGER is shown to outperform existing graph-based methods by 20%-60% across different benchmark datasets.|近似 K 最近邻搜索(AKNNS)已经成为现代应用中普遍存在的问题，例如，作为一种具有两个塔式深度学习模型的快速搜索过程。基于图的 AKNNS 方法由于其优越的性能而受到了广泛的关注。这些方法依赖于贪婪图搜索，以嵌入向量的形式遍历数据库中的数据点。在这种贪婪的搜索方案下，我们做了一个关键的观察: 许多距离计算不影响搜索更新，所以这些计算可以近似而不损害性能。因此，我们提出了 FINGER，一种快速的推理方法来实现有效的图搜索。FINGER 通过估计低秩基相邻残差向量之间的夹角和分布匹配来逼近距离函数。近似距离可以用来绕过不必要的计算，从而导致更快的搜索。经验表明，通过 FINGER 加速一种流行的基于图的方法 HNSW，在不同的基准数据集上比现有的基于图的方法的性能提高了20% -60% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FINGER:+Fast+Inference+for+Graph-based+Approximate+Nearest+Neighbor+Search)|0|
|[Match4Match: Enhancing Text-Video Retrieval by Maximum Flow with Minimum Cost](https://doi.org/10.1145/3543507.3583365)|Zhongjie Duan, Chengyu Wang, Cen Chen, Wenmeng Zhou, Jun Huang, Weining Qian|East China Normal University, China; Alibaba Group, China|With the explosive growth of video and text data on the web, text-video retrieval has become a vital task for online video platforms. Recently, text-video retrieval methods based on pre-trained models have attracted a lot of attention. However, existing methods cannot effectively capture the fine-grained information in videos, and typically suffer from the hubness problem where a collection of similar videos are retrieved by a large number of different queries. In this paper, we propose Match4Match, a new text-video retrieval method based on CLIP (Contrastive Language-Image Pretraining) and graph optimization theories. To balance calculation efficiency and model accuracy, Match4Match seamlessly supports three inference modes for different application scenarios. In fast vector retrieval mode, we embed texts and videos in the same space and employ a vector retrieval engine to obtain the top K videos. In fine-grained alignment mode, our method fully utilizes the pre-trained knowledge of the CLIP model to align words with corresponding video frames, and uses the fine-grained information to compute text-video similarity more accurately. In flow-style matching mode, to alleviate the detrimental impact of the hubness problem, we model the retrieval problem as a combinatorial optimization problem and solve it using maximum flow with minimum cost algorithm. To demonstrate the effectiveness of our method, we conduct experiments on five public text-video datasets. The overall performance of our proposed method outperforms state-of-the-art methods. Additionally, we evaluate the computational efficiency of Match4Match. Benefiting from the three flexible inference modes, Match4Match can respond to a large number of query requests with low latency or achieve high recall with acceptable time consumption.|随着网络视频和文本数据的爆炸式增长，文本视频检索已经成为在线视频平台的一项重要任务。近年来，基于预训练模型的文本视频检索方法引起了人们的广泛关注。然而，现有的方法不能有效地捕获视频中的细粒度信息，通常会遇到集线器问题，即大量不同的查询检索相似的视频集合。本文提出了一种基于对比语言-图像预训练(CLIP)和图形优化理论的文本-视频检索方法 Match4Match。为了平衡计算效率和模型精度，Match4Match 无缝支持针对不同应用场景的三种推理模式。在快速矢量检索模式下，我们将文本和视频嵌入到同一空间中，并使用矢量检索引擎获取最高 K 视频。在细粒度对齐模式下，该方法充分利用 CLIP 模型的预训练知识对相应的视频帧进行单词对齐，并利用细粒度信息更准确地计算文本-视频的相似度。在流式匹配模式下，为了减轻中继问题的不利影响，我们将检索问题建模为一个组合优化问题，并使用最大流和最小成本算法解决该问题。为了验证该方法的有效性，我们在五个公共文本视频数据集上进行了实验。我们提出的方法的总体性能优于最先进的方法。此外，我们还评估了 Match4Match 的计算效率。Match4Match 得益于这三种灵活的推理模式，可以以较低的延迟响应大量查询请求，或者以可接受的时间消耗实现高召回率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Match4Match:+Enhancing+Text-Video+Retrieval+by+Maximum+Flow+with+Minimum+Cost)|0|
|[Zero-shot Clarifying Question Generation for Conversational Search](https://doi.org/10.1145/3543507.3583420)|Zhenduo Wang, Yuancheng Tu, Corby Rosset, Nick Craswell, Ming Wu, Qingyao Ai|Microsoft Corp, USA; University of Utah, USA; Tsinghua University, China; GitHub Inc, USA|A long-standing challenge for search and conversational assistants is query intention detection in ambiguous queries. Asking clarifying questions in conversational search has been widely studied and considered an effective solution to resolve query ambiguity. Existing work have explored various approaches for clarifying question ranking and generation. However, due to the lack of real conversational search data, they have to use artificial datasets for training, which limits their generalizability to real-world search scenarios. As a result, the industry has shown reluctance to implement them in reality, further suspending the availability of real conversational search interaction data. The above dilemma can be formulated as a cold start problem of clarifying question generation and conversational search in general. Furthermore, even if we do have large-scale conversational logs, it is not realistic to gather training data that can comprehensively cover all possible queries and topics in open-domain search scenarios. The risk of fitting bias when training a clarifying question retrieval/generation model on incomprehensive dataset is thus another important challenge. In this work, we innovatively explore generating clarifying questions in a zero-shot setting to overcome the cold start problem and we propose a constrained clarifying question generation system which uses both question templates and query facets to guide the effective and precise question generation. The experiment results show that our method outperforms existing state-of-the-art zero-shot baselines by a large margin. Human annotations to our model outputs also indicate our method generates 25.2\% more natural questions, 18.1\% more useful questions, 6.1\% less unnatural and 4\% less useless questions.|模糊查询中的查询意图检测一直是搜索和会话助手面临的挑战。在会话搜索中提出澄清问题被广泛研究，被认为是解决查询歧义的有效方法。现有的工作已经探索了各种方法来澄清问题的排序和生成。然而，由于缺乏真实的会话搜索数据，他们不得不使用人工数据集进行训练，这限制了他们对真实世界搜索场景的普遍性。结果，业界表现出不愿意在现实中实现它们，进一步暂停了真正的会话搜索交互数据的可用性。上述困境可以概括为澄清问题生成和一般会话搜索的冷启动问题。此外，即使我们有大规模的会话日志，收集能够全面涵盖开放域搜索场景中所有可能的查询和主题的培训数据也是不现实的。因此，在不全面的数据集上训练澄清问题检索/生成模型时，拟合偏差的风险是另一个重要的挑战。在这项工作中，我们创新性地探索了在零拍环境下生成澄清问题来克服冷启动问题，并提出了一个有约束的澄清问题生成系统，该系统使用问题模板和查询面来指导有效和准确的问题生成。实验结果表明，我们的方法比现有的最先进的零拍摄基线有很大的优势。我们的模型输出的人工注释也表明我们的方法产生了25.2% 更自然的问题，18.1% 更有用的问题，6.1% 更少的非自然的和4% 更少的无用的问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Zero-shot+Clarifying+Question+Generation+for+Conversational+Search)|0|
|[Everything Evolves in Personalized PageRank](https://doi.org/10.1145/3543507.3583474)|Zihao Li, Dongqi Fu, Jingrui He|University of Illinois at Urbana-Champaign, USA|Personalized PageRank, as a graphical model, has been proven as an effective solution in many applications such as web page search, recommendation, etc. However, in the real world, the setting of personalized PageRank is usually dynamic like the evolving World Wide Web. On the one hand, the outdated PageRank solution can be sub-optimal for ignoring the evolution pattern. On the other hand, solving the solution from the scratch at each timestamp causes costly computation complexity. Hence, in this paper, we aim to solve the Personalized PageRank effectively and efficiently in a fully dynamic setting, i.e., every component in the Personalized PageRank formula is dependent on time. To this end, we propose the EvePPR method that can track the exact personalized PageRank solution at each timestamp in the fully dynamic setting, and we theoretically and empirically prove the accuracy and time complexity of EvePPR. Moreover, we apply EvePPR to solve the dynamic knowledge graph alignment task, where a fully dynamic setting is necessary but complex. The experiments show that EvePPR outperforms the state-of-the-art baselines for similar nodes retrieval across graphs.|个性化 PageRank 作为一种图形化模型，已被证明是网页搜索、推荐等应用中的一种有效解决方案。然而，在现实世界中，个性化 PageRank 的设置通常是动态的，就像不断发展的万维网一样。一方面，过时的 PageRank 解决方案可能是次优的，因为它忽略了进化模式。另一方面，在每个时间戳从零开始求解解决方案会导致昂贵的计算复杂度。因此，本文的目标是在一个完全动态的环境下有效地解决个性化 PageRank 问题，也就是说，个性化 PageRank 公式中的每个组成部分都依赖于时间。为此，我们提出了 EvePPR 方法，该方法可以在完全动态的环境下精确跟踪每个时间戳的个性化 PageRank 解，并从理论和实验上证明了 EvePPR 方法的准确性和时间复杂度。此外，我们应用 EvePPR 来解决动态知识图对齐任务，其中一个完全动态的设置是必要的，但是复杂的。实验表明，EvePPR 在跨图检索相似节点时优于最新的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Everything+Evolves+in+Personalized+PageRank)|0|
|[Incorporating Explicit Subtopics in Personalized Search](https://doi.org/10.1145/3543507.3583488)|Shuting Wang, Zhicheng Dou, Jing Yao, Yujia Zhou, JiRong Wen|Renmin University of China, China; Social Computing Group, Microsoft Research Asia, China; Renmin University of China, China and Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Educationf Education, China|The key to personalized search is modeling user intents to tailor returned results for different users. Existing personalized methods mainly focus on learning implicit user interest vectors. In this paper, we propose ExpliPS, a personalized search model that explicitly incorporates query subtopics into personalization. It models the user’s current intent by estimating the user’s preference over the subtopics of the current query and personalizes the results over the weighted subtopics. We think that in such a way, personalized search could be more explainable and stable. Specifically, we first employ a semantic encoder to learn the representations of the user’s historical behaviours. Then with the historical behaviour representations, a subtopic preference encoder is devised to predict the user’s subtopic preferences on the current query. Finally, we rerank the candidates via a subtopic-aware ranker that prioritizes the documents relevant to the user-preferred subtopics. Experimental results show our model ExpliPS outperforms the state-of-the-art personalized web search models with explainable and stable results.|个性化检索的关键是建立用户意图模型，为不同的用户定制返回的结果。现有的个性化方法主要侧重于学习隐式用户兴趣向量。在这篇文章中，我们提出了 expliPS，一个明确地将查询子主题合并到个性化中的个性化检索模型。它通过估计用户对当前查询的子主题的偏好来建模用户的当前意图，并对加权子主题的结果进行个性化处理。我们认为，通过这种方式，个性化检索可以更容易解释，也更稳定。具体来说，我们首先使用一个语义编码器来学习用户历史行为的表示。然后结合历史行为表示，设计了一种子主题偏好编码器来预测用户对当前查询的子主题偏好。最后，我们通过一个子主题感知排名器对候选人进行重新排名，该排名器对与用户首选子主题相关的文档进行优先排序。实验结果表明，该模型的性能优于目前最先进的个性化网络搜索模型，结果具有可解释性和稳定性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Incorporating+Explicit+Subtopics+in+Personalized+Search)|0|
|[Optimizing Feature Set for Click-Through Rate Prediction](https://doi.org/10.1145/3543507.3583545)|Fuyuan Lyu, Xing Tang, Dugang Liu, Liang Chen, Xiuqiang He, Xue Liu|McGill University, Canada; Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), China; FiT, Tencent, China|Click-through prediction (CTR) models transform features into latent vectors and enumerate possible feature interactions to improve performance based on the input feature set. Therefore, when selecting an optimal feature set, we should consider the influence of both feature and its interaction. However, most previous works focus on either feature field selection or only select feature interaction based on the fixed feature set to produce the feature set. The former restricts search space to the feature field, which is too coarse to determine subtle features. They also do not filter useless feature interactions, leading to higher computation costs and degraded model performance. The latter identifies useful feature interaction from all available features, resulting in many redundant features in the feature set. In this paper, we propose a novel method named OptFS to address these problems. To unify the selection of feature and its interaction, we decompose the selection of each feature interaction into the selection of two correlated features. Such a decomposition makes the model end-to-end trainable given various feature interaction operations. By adopting feature-level search space, we set a learnable gate to determine whether each feature should be within the feature set. Because of the large-scale search space, we develop a learning-by-continuation training scheme to learn such gates. Hence, OptFS generates the feature set only containing features which improve the final prediction results. Experimentally, we evaluate OptFS on three public datasets, demonstrating OptFS can optimize feature sets which enhance the model performance and further reduce both the storage and computational cost.|点击预测(CTR)模型将特征转换为潜在向量，并列举可能的特征交互，以提高基于输入特征集的性能。因此，在选择最优特征集时，应同时考虑特征及其相互作用的影响。然而，以往的工作主要集中在特征字段的选择或者仅仅基于固定特征集选择特征交互来产生特征集。前者将搜索空间限制在特征域内，特征域太粗，无法确定细微的特征。它们也不过滤无用的特征交互，导致更高的计算成本和降低模型性能。后者从所有可用的特征中识别出有用的特征交互，从而导致特征集中的许多冗余特征。在本文中，我们提出了一种新的方法称为 OptFS 来解决这些问题。为了统一特征选择和特征交互，将每个特征交互的选择分解为两个相关特征的选择。这样的分解使得模型在给定各种特征交互操作的情况下可以进行端到端的训练。通过采用特征级搜索空间，我们设置了一个可学习的门来确定每个特征是否应该在特征集中。由于搜索空间较大，我们提出了一种基于连续学习的训练方案来学习这类门。因此，OptFS 生成的特征集仅包含改善最终预测结果的特征。在实验上，我们对三个公共数据集上的 OptFS 进行了评估，结果表明 OptFS 可以优化特征集，从而提高模型的性能，进一步降低存储和计算成本。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Feature+Set+for+Click-Through+Rate+Prediction)|0|
|[Filtered-DiskANN: Graph Algorithms for Approximate Nearest Neighbor Search with Filters](https://doi.org/10.1145/3543507.3583552)|Siddharth Gollapudi, Neel Karia, Varun Sivashankar, Ravishankar Krishnaswamy, Nikit Begwani, Swapnil Raz, Yiyong Lin, Yin Zhang, Neelam Mahapatro, Premkumar Srinivasan, Amit Singh, Harsha Vardhan Simhadri|Microsoft Research, USA; Microsoft, USA; Columbia University, USA; Microsoft, India; Microsoft Research, India|As Approximate Nearest Neighbor Search (ANNS)-based dense retrieval becomes ubiquitous for search and recommendation scenarios, efficiently answering filtered ANNS queries has become a critical requirement. Filtered ANNS queries ask for the nearest neighbors of a query’s embedding from the points in the index that match the query’s labels such as date, price range, language. There has been little prior work on algorithms that use label metadata associated with vector data to build efficient indices for filtered ANNS queries. Consequently, current indices have high search latency or low recall which is not practical in interactive web-scenarios. We present two algorithms with native support for faster and more accurate filtered ANNS queries: one with streaming support, and another based on batch construction. Central to our algorithms is the construction of a graph-structured index which forms connections not only based on the geometry of the vector data, but also the associated label set. On real-world data with natural labels, both algorithms are an order of magnitude or more efficient for filtered queries than the current state of the art algorithms. The generated indices also be queried from an SSD and support thousands of queries per second at over [email protected]|随着基于近似最近邻搜索(ANNS)的密集检索在搜索和推荐场景中的普及，有效地回答经过滤的 ANNS 查询已成为一个关键要求。经过过滤的 ANNS 查询要求查询嵌入的最近邻居从索引中匹配查询标签的点，如日期，价格范围，语言。使用与矢量数据相关联的标签元数据为经过过滤的 ANNS 查询构建高效索引的算法之前几乎没有研究。因此，当前的索引具有较高的搜索延迟或较低的召回率，这在交互式网络场景中是不实用的。我们提出了两个算法与本地支持更快，更准确的过滤 ANNS 查询: 一个流支持，另一个基于批量构造。我们算法的核心是构造一个图结构索引，它不仅根据矢量数据的几何形状，而且根据相关的标签集形成连接。对于带有自然标签的真实世界数据，这两种算法对于过滤查询来说都是一种数量级，或者比目前最先进的算法效率更高。生成的索引也可以从 SSD 查询，并支持每秒在 over [ email protected ]处的数千个查询|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Filtered-DiskANN:+Graph+Algorithms+for+Approximate+Nearest+Neighbor+Search+with+Filters)|0|
|[P-MMF: Provider Max-min Fairness Re-ranking in Recommender System](https://doi.org/10.1145/3543507.3583296)|Chen Xu, Sirui Chen, Jun Xu, Weiran Shen, Xiao Zhang, Gang Wang, Zhenhua Dong||In this paper, we address the issue of recommending fairly from the aspect of providers, which has become increasingly essential in multistakeholder recommender systems. Existing studies on provider fairness usually focused on designing proportion fairness (PF) metrics that first consider systematic fairness. However, sociological researches show that to make the market more stable, max-min fairness (MMF) is a better metric. The main reason is that MMF aims to improve the utility of the worst ones preferentially, guiding the system to support the providers in weak market positions. When applying MMF to recommender systems, how to balance user preferences and provider fairness in an online recommendation scenario is still a challenging problem. In this paper, we proposed an online re-ranking model named Provider Max-min Fairness Re-ranking (P-MMF) to tackle the problem. Specifically, P-MMF formulates provider fair recommendation as a resource allocation problem, where the exposure slots are considered the resources to be allocated to providers and the max-min fairness is used as the regularizer during the process. We show that the problem can be further represented as a regularized online optimizing problem and solved efficiently in its dual space. During the online re-ranking phase, a momentum gradient descent method is designed to conduct the dynamic re-ranking. Theoretical analysis showed that the regret of P-MMF can be bounded. Experimental results on four public recommender datasets demonstrated that P-MMF can outperformed the state-of-the-art baselines. Experimental results also show that P-MMF can retain small computationally costs on a corpus with the large number of items.|在本文中，我们从提供者的角度讨论了公平推荐的问题，这在多利益相关者推荐系统中已经变得越来越重要。现有的关于提供者公平性的研究通常集中在设计比例公平性(PF)指标时首先考虑系统公平性。然而，社会学研究表明，为了使市场更加稳定，极大极小公平(MMF)是一个更好的衡量标准。主要原因在于，货币市场基金旨在优先提高最差的基金的效用，引导金融体系支持处于弱势市场地位的基金提供者。在将 MMF 应用于推荐系统时，如何在在线推荐场景中平衡用户偏好和提供者公平性仍然是一个具有挑战性的问题。在这篇文章中，我们提出了一个在线重新排序模型——提供者极大极小公平重新排序(P-MMF)来解决这个问题。具体来说，P-MMF 将提供商公平推荐制定为一个资源分配问题，其中风险承担时段被视为将分配给提供商的资源，而极大极小公平则被用作过程中的规范者。证明了该问题可以进一步表示为正则化在线优化问题，并在其对偶空间中有效地求解。在在线重新排名阶段，动量梯度下降法方法被设计用于进行动态重新排名。理论分析表明，P-MMF 的遗憾是有限的。对四个公共推荐数据集的实验结果表明，P-MMF 能够优于最先进的基线。实验结果还表明，P-MMF 能够在项目数量较多的语料库上保持较小的计算开销。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=P-MMF:+Provider+Max-min+Fairness+Re-ranking+in+Recommender+System)|0|
|[Dual Intent Enhanced Graph Neural Network for Session-based New Item Recommendation](https://doi.org/10.1145/3543507.3583526)|Di Jin, Luzhi Wang, Yizhen Zheng, Guojie Song, Fei Jiang, Xiang Li, Wei Lin, Shirui Pan|School of Intelligence Science and Technology, Peking University, China; School of Information and Communication Technology, Griffith University, Australia; Department of Data Science and AI, Faculty of IT, Monash University, Australia; Meituan, China; Professional, China; College of Intelligence and Computing, Tianjin University, China|Recommender systems are essential to various fields, e.g., e-commerce, e-learning, and streaming media. At present, graph neural networks (GNNs) for session-based recommendations normally can only recommend items existing in users' historical sessions. As a result, these GNNs have difficulty recommending items that users have never interacted with (new items), which leads to a phenomenon of information cocoon. Therefore, it is necessary to recommend new items to users. As there is no interaction between new items and users, we cannot include new items when building session graphs for GNN session-based recommender systems. Thus, it is challenging to recommend new items for users when using GNN-based methods. We regard this challenge as '\textbf{G}NN \textbf{S}ession-based \textbf{N}ew \textbf{I}tem \textbf{R}ecommendation (GSNIR)'. To solve this problem, we propose a dual-intent enhanced graph neural network for it. Due to the fact that new items are not tied to historical sessions, the users' intent is difficult to predict. We design a dual-intent network to learn user intent from an attention mechanism and the distribution of historical data respectively, which can simulate users' decision-making process in interacting with a new item. To solve the challenge that new items cannot be learned by GNNs, inspired by zero-shot learning (ZSL), we infer the new item representation in GNN space by using their attributes. By outputting new item probabilities, which contain recommendation scores of the corresponding items, the new items with higher scores are recommended to users. Experiments on two representative real-world datasets show the superiority of our proposed method. The case study from the real-world verifies interpretability benefits brought by the dual-intent module and the new item reasoning module. The code is available at Github: https://github.com/Ee1s/NirGNN|推荐系统对于电子商务、电子学习和流媒体等各个领域都是必不可少的。目前，基于会话推荐的图神经网络(GNN)通常只能推荐用户历史会话中存在的项目。因此，这些 GNN 很难推荐用户从未接触过的项目(新项目) ，从而导致信息茧现象。因此，有必要向用户推荐新项目。由于新项目和用户之间没有交互，所以在为基于 GNN 会话的推荐系统构建会话图时，我们不能包含新项目。因此，在使用基于 GNN 的方法时，向用户推荐新项目是一个挑战。我们将此挑战视为“ textbf { G } NN textbf { S } session-based textbf { N } ew textbf { I } tem textbf { R }推荐(GSNIR)”。为了解决这一问题，我们提出了一种双意图增强的图神经网络。由于新条目不与历史会话相关联，因此很难预测用户的意图。设计了一个双意图网络，分别从注意机制和历史数据分布中学习用户意图，模拟用户在与新项目交互时的决策过程。为了解决 GNN 无法学习新项目的问题，受零点学习(ZSL)的启发，我们利用 GNN 空间中新项目的属性来推断新项目的表示。通过输出新项目概率，其中包含相应项目的推荐分数，新项目的分数越高，推荐给用户。在两个具有代表性的实际数据集上的实验表明了该方法的优越性。通过实际案例分析，验证了双意图模块和新的项目推理模块所带来的可解释性优势。代码可以在 Github:  https://Github.com/ee1s/nirgnn 上找到|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual+Intent+Enhanced+Graph+Neural+Network+for+Session-based+New+Item+Recommendation)|0|
|[Cross-domain recommendation via user interest alignment](https://doi.org/10.1145/3543507.3583263)|Chuang Zhao, Hongke Zhao, Ming HE, Jian Zhang, Jianping Fan|College of Management and Economics, Tianjin University, China; School of Cyberspace Security, Hangzhou Dianzi University, China; AI Lab at Lenovo Research, China; College of Management and Economics, Tianjin University, China and AI Lab at Lenovo Research, China|Cross-domain recommendation aims to leverage knowledge from multiple domains to alleviate the data sparsity and cold-start problems in traditional recommender systems. One popular paradigm is to employ overlapping user representations to establish domain connections, thereby improving recommendation performance in all scenarios. Nevertheless, the general practice of this approach is to train user embeddings in each domain separately and then aggregate them in a plain manner, often ignoring potential cross-domain similarities between users and items. Furthermore, considering that their training objective is recommendation task-oriented without specific regularizations, the optimized embeddings disregard the interest alignment among user's views, and even violate the user's original interest distribution. To address these challenges, we propose a novel cross-domain recommendation framework, namely COAST, to improve recommendation performance on dual domains by perceiving the cross-domain similarity between entities and aligning user interests. Specifically, we first construct a unified cross-domain heterogeneous graph and redefine the message passing mechanism of graph convolutional networks to capture high-order similarity of users and items across domains. Targeted at user interest alignment, we develop deep insights from two more fine-grained perspectives of user-user and user-item interest invariance across domains by virtue of affluent unsupervised and semantic signals. We conduct intensive experiments on multiple tasks, constructed from two large recommendation data sets. Extensive results show COAST consistently and significantly outperforms state-of-the-art cross-domain recommendation algorithms as well as classic single-domain recommendation methods.|跨域推荐的目的是利用来自多个域的知识来缓解传统推荐系统中的数据稀疏和冷启动问题。一个流行的范例是使用重叠的用户表示来建立域连接，从而在所有场景中提高推荐性能。然而，这种方法的一般实践是分别训练每个域中的用户嵌入，然后以简单的方式聚合它们，通常忽略用户和项目之间潜在的跨域相似性。此外，考虑到其训练目标是面向推荐任务的，没有具体的规范化，优化嵌入无视用户视图之间的兴趣一致性，甚至违背了用户原有的兴趣分布。为了应对这些挑战，我们提出了一种新的跨域推荐框架，即 COAST，通过感知实体之间的跨域相似性和调整用户兴趣来提高双域推荐的性能。具体来说，我们首先构建一个统一的跨域异构图，重新定义图卷积网络的消息传递机制，以获取跨域用户和项目的高阶相似性。针对用户兴趣对齐，我们从用户-用户和用户项目兴趣不变性的两个更细粒度的角度，通过丰富的无监督和语义信号，开发深刻的见解。我们对两个大型推荐数据集构建的多个任务进行了深入的实验。广泛的结果表明，COAST 始终如一地显著优于最先进的跨域推荐算法和经典的单域推荐方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-domain+recommendation+via+user+interest+alignment)|0|
|[A Semantic Partitioning Method for Large-Scale Training of Knowledge Graph Embeddings](https://doi.org/10.1145/3543873.3587537)|Yuhe Bai|Sorbonne University, France|In recent years, knowledge graph embeddings have achieved great success. Many methods have been proposed and achieved state-of-the-art results in various tasks. However, most of the current methods present one or more of the following problems: (i) They only consider fact triplets, while ignoring the ontology information of knowledge graphs. (ii) The obtained embeddings do not contain much semantic information. Therefore, using these embeddings for semantic tasks is problematic. (iii) They do not enable large-scale training. In this paper, we propose a new algorithm that incorporates the ontology of knowledge graphs and partitions the knowledge graph based on classes to include more semantic information for parallel training of large-scale knowledge graph embeddings. Our preliminary results show that our algorithm performs well on several popular benchmarks.|近年来，知识图嵌入技术取得了很大的成功。已经提出了许多方法，并在各种任务中取得了最新的成果。然而，目前的大多数方法都存在以下一个或多个问题: (i)它们只考虑事实三元组，而忽略了知识图的本体信息。(ii)所得的嵌入资料并无太多语义信息。因此，将这些嵌入用于语义任务是有问题的。(iii)不能进行大规模培训。在本文中，我们提出了一个新的算法，它结合了知识图的本体和基于类的知识图划分，以包括更多的语义信息并行训练大规模的知识图嵌入。我们的初步结果表明，我们的算法在几个流行的基准测试中表现良好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Semantic+Partitioning+Method+for+Large-Scale+Training+of+Knowledge+Graph+Embeddings)|0|
|[Intent-Aware Propensity Estimation via Click Pattern Stratification](https://doi.org/10.1145/3543873.3587610)|Ehsan Ebrahimzadeh, Alex Cozzi, Abraham Bagherjeiran|Search Ranking and Monetization, eBay, USA|Counterfactual learning to rank via inverse propensity weighting is the most popular approach to train ranking models using biased implicit user feedback from logged search data. Standard click propensity estimation techniques rely on simple models of user browsing behavior that primarily account for the attributes of the presentation context that affect whether the relevance of an item to the search context is observed. Most notably, the inherent effect of the listwise presentation of the items on users’ propensity for engagement is captured in the position of the presented items on the search result page. In this work, we enrich this position bias based click propensity model by proposing an observation model that further incorporates the underlying search intent, as reflected in the user’s click pattern in the search context. Our approach does not require an intent prediction model based on the content of the search context. Instead, we rely on a simple, yet effective, non-causal estimate of the user’s browsing intent from the number of click events in the search context. We empirically characterize the distinct rank decay patterns of the estimated click propensities in the characterized intent classes. In particular, we demonstrate a sharper decay of click propensities in top ranks for the intent class identified by sparse user clicks and the higher likelihood of observing clicks in lower ranks for the intent class identified by higher number of user clicks. We show that the proposed intent-aware propensity estimation technique helps with training ranking models with more effective personalization and generalization power through empirical results for a ranking task in a major e-commerce platform.|通过逆倾向加权反事实学习排序是最流行的方法来训练排序模型使用有偏见的隐式用户反馈的日志搜索数据。标准的点击倾向评估技术依赖于用户浏览行为的简单模型，这些模型主要解释了表示上下文的属性，这些属性影响了项目与搜索上下文的相关性是否被观察到。最值得注意的是，在搜索结果页面上呈现的项目的位置捕捉到了项目列表方式对用户参与倾向的内在影响。在这项工作中，我们丰富了这个基于位置偏差的点击倾向模型，通过提出一个观察模型，进一步结合潜在的搜索意图，如反映在用户的点击模式在搜索上下文。我们的方法不需要基于搜索上下文内容的意图预测模型。相反，我们依赖于从搜索上下文中的点击事件数量对用户的浏览意图进行简单而有效的非因果估计。我们经验性地描述了特征意图类中估计的点击倾向的不同秩衰减模式。特别是，我们证明了通过稀疏的用户点击识别的意图类别的顶级点击倾向的更强烈的衰减，以及通过更高数量的用户点击识别的意图类别在较低级别中观察到点击的可能性更高。通过对一个大型电子商务平台的排序任务进行实证分析，我们发现提出的意图感知倾向估计技术有助于训练排序模型，使其具有更有效的个性化和泛化能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Intent-Aware+Propensity+Estimation+via+Click+Pattern+Stratification)|0|
|[Disentangling Degree-related Biases and Interest for Out-of-Distribution Generalized Directed Network Embedding](https://doi.org/10.1145/3543507.3583271)|Hyunsik Yoo, YeonChang Lee, Kijung Shin, SangWook Kim|Korea Advanced Institute of Science and Technology, Republic of Korea; Georgia Institute of Technology, USA; Hanyang University, Republic of Korea|The goal of directed network embedding is to represent the nodes in a given directed network as embeddings that preserve the asymmetric relationships between nodes. While a number of directed network embedding methods have been proposed, we empirically show that the existing methods lack out-of-distribution generalization abilities against degree-related distributional shifts. To mitigate this problem, we propose ODIN (Out-of-Distribution Generalized Directed Network Embedding), a new directed NE method where we model multiple factors in the formation of directed edges. Then, for each node, ODIN learns multiple embeddings, each of which preserves its corresponding factor, by disentangling interest factors and biases related to in- and out-degrees of nodes. Our experiments on four real-world directed networks demonstrate that disentangling multiple factors enables ODIN to yield out-of-distribution generalized embeddings that are consistently effective under various degrees of shifts in degree distributions. Specifically, ODIN universally outperforms 9 state-of-the-art competitors in 2 LP tasks on 4 real-world datasets under both identical distribution (ID) and non-ID settings. The code is available at https://github.com/hsyoo32/odin.|有向网络嵌入的目的是将给定有向网络中的节点表示为保持节点间不对称关系的嵌入。虽然已经提出了一些有向网络嵌入方法，但是实验表明，现有的方法缺乏对度相关分布偏移的分布外泛化能力。为了解决这一问题，我们提出了一种新的有向网络嵌入方法 ODIN (Out-of-Distribution Generalization Directed Network Embedding) ，该方法对有向边的形成过程中的多个因素进行建模。然后，对于每个节点，ODIN 通过分离与节点内外度相关的兴趣因子和偏差来学习多个嵌入，每个嵌入保留相应的因子。我们在四个真实世界的定向网络上的实验表明，解开多个因素使 ODIN 能够产生分布外的广义嵌入，在度分布的不同程度的转移下一致有效。具体而言，ODIN 在4个真实世界数据集的2个 LP 任务中，在相同的分布(ID)和非 ID 设置下，普遍优于9个最先进的竞争对手。密码可在 https://github.com/hsyoo32/odin 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disentangling+Degree-related+Biases+and+Interest+for+Out-of-Distribution+Generalized+Directed+Network+Embedding)|0|
|[Fine-tuning Partition-aware Item Similarities for Efficient and Scalable Recommendation](https://doi.org/10.1145/3543507.3583240)|Tianjun Wei, Jianghong Ma, Tommy W. S. Chow|Harbin Institute of Technology, China; City University of Hong Kong, Hong Kong|Collaborative filtering (CF) is widely searched in recommendation with various types of solutions. Recent success of Graph Convolution Networks (GCN) in CF demonstrates the effectiveness of modeling high-order relationships through graphs, while repetitive graph convolution and iterative batch optimization limit their efficiency. Instead, item similarity models attempt to construct direct relationships through efficient interaction encoding. Despite their great performance, the growing item numbers result in quadratic growth in similarity modeling process, posing critical scalability problems. In this paper, we investigate the graph sampling strategy adopted in latest GCN model for efficiency improving, and identify the potential item group structure in the sampled graph. Based on this, we propose a novel item similarity model which introduces graph partitioning to restrict the item similarity modeling within each partition. Specifically, we show that the spectral information of the original graph is well in preserving global-level information. Then, it is added to fine-tune local item similarities with a new data augmentation strategy acted as partition-aware prior knowledge, jointly to cope with the information loss brought by partitioning. Experiments carried out on 4 datasets show that the proposed model outperforms state-of-the-art GCN models with 10x speed-up and item similarity models with 95\% parameter storage savings.|协同过滤(CF)在推荐中被广泛搜索，提供了各种类型的解决方案。最近，图卷积网络(GCN)在 CF 中的成功证明了通过图建立高阶关系的有效性，而重复图卷积和迭代批处理优化限制了它们的效率。相反，项目相似性模型试图通过有效的交互编码来构建直接关系。尽管它们具有很好的性能，但是在相似性建模过程中，项目数量的增长会导致二次增长，从而产生关键的可扩展性问题。本文研究了最新 GCN 模型中为提高效率而采用的图抽样策略，并识别了抽样图中潜在的项目组结构。在此基础上，提出了一种新的项目相似度模型，该模型引入图划分来约束项目相似度建模。具体地说，我们证明了原始图的光谱信息在保持全局水平信息方面是很好的。然后加入一种新的数据增强策略作为分区感知的先验知识，对局部项相似性进行微调，共同应对分区带来的信息丢失。在4个数据集上进行的实验表明，该模型比最新的 GCN 模型具有10倍的加速度和项目相似度，节省了95% 的参数存储空间。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fine-tuning+Partition-aware+Item+Similarities+for+Efficient+and+Scalable+Recommendation)|0|
|[Multi-Behavior Recommendation with Cascading Graph Convolution Networks](https://doi.org/10.1145/3543507.3583439)|Zhiyong Cheng, Sai Han, Fan Liu, Lei Zhu, Zan Gao, Yuxin Peng|School of Computing, National University of Singapore, Singapore; School of Information Science and Engineering, Shandong Normal University, China; Shandong Artificial Intelligence Institute, Qilu University of Technology (Shandong Academy of Sciences), China; Wangxuan Institute of Computer Technology, Peking University, China and Peng Cheng Laboratory, China|Multi-behavior recommendation, which exploits auxiliary behaviors (e.g., click and cart) to help predict users' potential interactions on the target behavior (e.g., buy), is regarded as an effective way to alleviate the data sparsity or cold-start issues in recommendation. Multi-behaviors are often taken in certain orders in real-world applications (e.g., click>cart>buy). In a behavior chain, a latter behavior usually exhibits a stronger signal of user preference than the former one does. Most existing multi-behavior models fail to capture such dependencies in a behavior chain for embedding learning. In this work, we propose a novel multi-behavior recommendation model with cascading graph convolution networks (named MB-CGCN). In MB-CGCN, the embeddings learned from one behavior are used as the input features for the next behavior's embedding learning after a feature transformation operation. In this way, our model explicitly utilizes the behavior dependencies in embedding learning. Experiments on two benchmark datasets demonstrate the effectiveness of our model on exploiting multi-behavior data. It outperforms the best baseline by 33.7% and 35.9% on average over the two datasets in terms of Recall@10 and NDCG@10, respectively.|多行为推荐利用辅助行为(如点击和购物车)来帮助预测用户在目标行为(如购买)上的潜在交互，被认为是缓解推荐中数据稀疏或冷启动问题的有效方法。在实际应用程序中，多行为通常按照特定的顺序执行(例如，单击 > 购物车 > 购买)。在行为链中，后一种行为通常比前一种行为表现出更强的用户偏好信号。大多数现有的多行为模型无法在嵌入式学习的行为链中捕获这种依赖关系。提出了一种新的具有级联图卷积网络的多行为推荐模型(MB-CGCN)。在 MB-CGCN 中，从一个行为中学习的嵌入作为特征转换操作后下一个行为的嵌入学习的输入特征。通过这种方式，我们的模型明确地利用了嵌入式学习中的行为依赖。在两个基准数据集上的实验证明了该模型对多行为数据的有效性。以 Recall@10和 NDCG@10计算，该方法比最佳基线的平均值分别高出33.7% 和35.9% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Behavior+Recommendation+with+Cascading+Graph+Convolution+Networks)|0|
|[Cross-domain Recommendation with Behavioral Importance Perception](https://doi.org/10.1145/3543507.3583494)|Hong Chen, Xin Wang, Ruobing Xie, Yuwei Zhou, Wenwu Zhu|Department of Computer Science and Technology, Tsinghua University, China; WeChat Search Application Department, Tencent, China|Cross-domain recommendation (CDR) aims to leverage the source domain information to provide better recommendation for the target domain, which is widely adopted in recommender systems to alleviate the data sparsity and cold-start problems. However, existing CDR methods mostly focus on designing effective model architectures to transfer the source domain knowledge, ignoring the behavior-level effect during the loss optimization process, where behaviors regarding different aspects in the source domain may have different importance for the CDR model optimization. The ignorance of the behavior-level effect will cause the carefully designed model architectures ending up with sub-optimal parameters, which limits the recommendation performance. To tackle the problem, we propose a generic behavioral importance-aware optimization framework for cross-domain recommendation (BIAO). Specifically, we propose a behavioral perceptron which predicts the importance of each source behavior according to the corresponding item’s global impact and local user-specific impact. The joint optimization process of the CDR model and the behavioral perceptron is formulated as a bi-level optimization problem. In the lower optimization, only the CDR model is updated with weighted source behavior loss and the target domain loss, while in the upper optimization, the behavioral perceptron is updated with implicit gradient from a developing dataset obtained through the proposed reorder-and-reuse strategy. Extensive experiments show that our proposed optimization framework consistently improves the performance of different cross-domain recommendation models in 7 cross-domain scenarios, demonstrating that our method can serve as a generic and powerful tool for cross-domain recommendation1.|跨域推荐(CDR)是利用源域信息为目标域提供更好的推荐，在推荐系统中被广泛采用以缓解数据稀疏和冷启动问题。然而，现有的 CDR 方法大多侧重于设计有效的模型结构来传递源域知识，忽略了损失优化过程中的行为级效应，其中源域中不同方面的行为对 CDR 模型优化的重要性不同。对行为级效应的忽视将导致精心设计的模型结构最终得到次优参数，从而限制了推荐性能。为了解决这个问题，我们提出了一个通用的跨域推荐的行为重要性感知优化框架(BIAO)。具体来说，我们提出了一种行为感知器，它根据相应项目的全局影响和局部用户特定影响来预测每个源行为的重要性。CDR 模型和行为感知器的联合优化过程是一个双层次的最佳化问题。在下层优化中，只对 CDR 模型进行加权源行为丢失和目标域丢失的更新，而在上层优化中，行为感知器通过提出的重排序和重用策略从一个正在发展的数据集中获得隐式梯度更新。大量的实验表明，我们提出的优化框架在7个跨领域场景中始终如一地提高了不同跨领域推荐模型的性能，表明我们的方法可以作为跨领域推荐的一个通用和强大的工具1。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-domain+Recommendation+with+Behavioral+Importance+Perception)|0|
|[Multi-Lingual Multi-Partite Product Title Matching](https://doi.org/10.1145/3543873.3587322)|HuanLin Tay, WeiJie Tay, Hady W. Lauw|Singapore Management University, Singapore|In a globalized marketplace, one could access products or services from almost anywhere. However, resolving which product in one language corresponds to another product in a different language remains an under-explored problem. We explore this from two perspectives. First, given two products of different languages, how to assess their similarity that could signal a potential match. Second, given products from various languages, how to arrive at a multi-partite clustering that respects cardinality constraints efficiently. We describe algorithms for each perspective and integrate them into a promising solution validated on real-world datasets.|在一个全球化的市场中，人们几乎可以从任何地方获得产品或服务。然而，解决一种语言中的哪种产品对应于另一种语言中的另一种产品仍然是一个尚未得到充分探讨的问题。我们从两个角度来探讨这个问题。首先，给定两种不同语言的产品，如何评估它们的相似性，这可能标志着潜在的匹配。其次，给定来自不同语言的产品，如何有效地得到一个尊重基数约束的多部分聚类。我们描述每个视角的算法，并将它们集成到一个在真实世界数据集上验证的有希望的解决方案中。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Lingual+Multi-Partite+Product+Title+Matching)|0|
|[Multi-interest Recommendation on Shopping for Others](https://doi.org/10.1145/3543873.3587341)|Shuang Li, Yaokun Liu, Xiaowang Zhang, Yuexian Hou, Zhiyong Feng|Tianjin University, China, China; Tianjin University, China|Existing recommendation methods based on multi-interest frameworks effectively model users from multiple aspects to represent complex user interests. However, more research still needs to be done on the behavior of users shopping for others. We propose a Multi-Demander Recommendation (MDR) model to learn different people’s interests from a sequence of actions. We first decouple the feature embeddings of items to learn the static preferences of different demanders. Next, a weighted directed global graph is constructed to model the associations among item categories. We partition short sequences by time intervals and look up category embeddings from the graph to capture dynamic intents. Finally, preferences and intentions are combined with learning the interests of different demanders. The conducted experiments demonstrate that our model improves the accuracy of recommendations.|现有的基于多兴趣框架的推荐方法能够有效地从多个方面对用户进行建模，以表达复杂的用户兴趣。然而，还需要对用户为他人购物的行为进行更多的研究。我们提出了一个多需求推荐(MDR)模型，从一系列的行动中了解不同人的兴趣。首先解耦项目的特征嵌入，学习不同需求者的静态偏好。然后，构造一个加权有向全局图来模拟项目类别之间的关联。我们根据时间间隔对短序列进行划分，并从图中查找类别嵌入以捕获动态意图。最后，将偏好和意图与学习不同需求者的兴趣结合起来。实验表明，该模型提高了推荐的准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-interest+Recommendation+on+Shopping+for+Others)|0|
|[Explicit and Implicit Semantic Ranking Framework](https://doi.org/10.1145/3543873.3584621)|Xiaofeng Zhu, Thomas Lin, Vishal Anand, Matthew Calderwood, Eric ClausenBrown, Gord Lueck, Wenwai Yim, Cheng Wu|Nuance Communications, USA; Microsoft Corporation, USA|The core challenge in numerous real-world applications is to match an inquiry to the best document from a mutable and finite set of candidates. Existing industry solutions, especially latency-constrained services, often rely on similarity algorithms that sacrifice quality for speed. In this paper we introduce a generic semantic learning-to-rank framework, Self-training Semantic Cross-attention Ranking (sRank). This transformer-based framework uses linear pairwise loss with mutable training batch sizes and achieves quality gains and high efficiency, and has been applied effectively to show gains on two industry tasks at Microsoft over real-world large-scale data sets: Smart Reply (SR) and Ambient Clinical Intelligence (ACI). In Smart Reply, $sRank$ assists live customers with technical support by selecting the best reply from predefined solutions based on consumer and support agent messages. It achieves 11.7% gain in offline top-one accuracy on the SR task over the previous system, and has enabled 38.7% time reduction in composing messages in telemetry recorded since its general release in January 2021. In the ACI task, sRank selects relevant historical physician templates that serve as guidance for a text summarization model to generate higher quality medical notes. It achieves 35.5% top-one accuracy gain, along with 46% relative ROUGE-L gain in generated medical notes.|在许多实际应用程序中的核心挑战是将查询与来自一组可变且有限的候选文档的最佳文档匹配。现有的行业解决方案，尤其是延迟受限的服务，通常依赖于牺牲质量以提高速度的相似性算法。本文介绍了一个通用的语义学习排序框架——自训练语义交叉注意排序(sRank)。这种基于变压器的框架使用线性成对损失和可变的训练批量大小，实现了质量增益和高效率，并已有效地应用于显示在两个行业任务中的收益在微软超过现实世界的大规模数据集: 智能应答(SR)和环境临床智能(ACI)。在智能答复中，$sRank $通过从基于消费者和支持代理消息的预定义解决方案中选择最佳答复，为现场客户提供技术支持。与之前的系统相比，它在离线状态下获得了11.7% 的最高准确率，并且自2021年1月发布以来，在遥测信息的合成方面减少了38.7% 的时间。在 ACI 任务中，sRank 选择相关的历史医生模板作为文本摘要模型的指导，以生成更高质量的医疗笔记。它实现了35.5% 的最高一级准确性增益，以及46% 的相对 ROUGE-L 增益在生成的医疗记录。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explicit+and+Implicit+Semantic+Ranking+Framework)|0|
|[MPKGAC: Multimodal Product Attribute Completion in E-commerce](https://doi.org/10.1145/3543873.3584623)|Kai Wang, Jianzhi Shao, Tao Zhang, Qijin Chen, Chengfu Huo|Alibaba Group, China|Product attributes can display the selling points of products, helping users find their desired products in search results. However, product attributes are typically incomplete. In e-commerce, products have multimodal features, including original attributes, images, and texts. How to make full use of the multimodal data to complete the missing attributes is the key challenge. To this end, we propose MPKGAC, a powerful three-stream framework that handles multimodal product data for attribute completion. We build a multimodal product knowledge graph (KG) from the multimodal features, and then convert the attribute completion problem into a multimodal KG completion task. MPKGAC encodes each modality separately, fuses them adaptively, and integrates multimodal decoders for prediction. Experiments show that MPKGAC outperforms the best baseline by 6.2% in [email protected] MPKGAC is employed to enrich selling points of the women’s clothing industry at Alibaba.com.cn and improves the click-through rate (CTR) by a relative 2.14%.|产品属性可以显示产品的销售点，帮助用户在搜索结果中找到他们想要的产品。但是，产品属性通常是不完整的。在电子商务中，产品具有多模态特征，包括原始属性、图像和文本。如何充分利用多模态数据来完成缺失的属性是一个关键的挑战。为此，我们提出了 MPKGAC，一个强大的三流框架，处理多通道产品数据的属性完成。首先根据多模态特征构造多模态产品知识图，然后将属性完成问题转化为多模态产品完成任务。MPKGAC 对每种模式分别进行编码，自适应地进行融合，并集成多模式解码器进行预测。实验表明，在阿里巴巴网站上，MPKGAC 的表现优于最佳基线6.2% ，提高了女装行业的销售点，提高了相对2.14% 的点进率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MPKGAC:+Multimodal+Product+Attribute+Completion+in+E-commerce)|0|
|[Bootstrapping Contrastive Learning Enhanced Music Cold-Start Matching](https://doi.org/10.1145/3543873.3584626)|Xinping Zhao, Ying Zhang, Qiang Xiao, Yuming Ren, Yingchun Yang|Zhejiang University, China and NetEase Cloud Music, NetEase Inc., China; NetEase Cloud Music, NetEase Inc., China; Zhejiang University, China|We study a particular matching task we call Music Cold-Start Matching. In short, given a cold-start song request, we expect to retrieve songs with similar audiences and then fastly push the cold-start song to the audiences of the retrieved songs to warm up it. However, there are hardly any studies done on this task. Therefore, in this paper, we will formalize the problem of Music Cold-Start Matching detailedly and give a scheme. During the offline training, we attempt to learn high-quality song representations based on song content features. But, we find supervision signals typically follow power-law distribution causing skewed representation learning. To address this issue, we propose a novel contrastive learning paradigm named Bootstrapping Contrastive Learning (BCL) to enhance the quality of learned representations by exerting contrastive regularization. During the online serving, to locate the target audiences more accurately, we propose Clustering-based Audience Targeting (CAT) that clusters audience representations to acquire a few cluster centroids and then locate the target audiences by measuring the relevance between the audience representations and the cluster centroids. Extensive experiments on the offline dataset and online system demonstrate the effectiveness and efficiency of our method. Currently, we have deployed it on NetEase Cloud Music, affecting millions of users.|我们研究一个特殊的匹配任务，我们称之为音乐冷启动匹配。简而言之，给定一个冷启动歌曲请求，我们期望检索具有相似受众的歌曲，然后快速将冷启动歌曲推送给被检索歌曲的受众进行预热。然而，几乎没有任何关于这项任务的研究。因此，本文将音乐冷启动匹配问题进行了详细的形式化描述，并给出了一个解决方案。在离线训练中，我们尝试根据歌曲的内容特征来学习高质量的歌曲表现。但是，我们发现监督信号具有典型的幂律分布特征，从而导致了偏态表征学习。为了解决这一问题，我们提出了一种新的对比学习范式——自举对比学习(BCL) ，通过运用对比正则化来提高学习表征的质量。在在线服务过程中，为了更准确地定位目标受众，我们提出了基于聚类的受众定位(CAT)方法，即通过聚类获取受众表征的几个聚类中心，然后通过测量受众表征与聚类中心之间的相关性来定位目标受众。在离线数据集和在线系统上的大量实验证明了该方法的有效性和高效性。目前，我们已经在网易云音乐上部署了它，影响了数百万用户。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bootstrapping+Contrastive+Learning+Enhanced+Music+Cold-Start+Matching)|0|
|[Reinforcing User Retention in a Billion Scale Short Video Recommender System](https://doi.org/10.1145/3543873.3584640)|Qingpeng Cai, Shuchang Liu, Xueliang Wang, Tianyou Zuo, Wentao Xie, Bin Yang, Dong Zheng, Peng Jiang, Kun Gai|Kuaishou Technology, China; Unaffiliated, China|Recently, short video platforms have achieved rapid user growth by recommending interesting content to users. The objective of the recommendation is to optimize user retention, thereby driving the growth of DAU (Daily Active Users). Retention is a long-term feedback after multiple interactions of users and the system, and it is hard to decompose retention reward to each item or a list of items. Thus traditional point-wise and list-wise models are not able to optimize retention. In this paper, we choose reinforcement learning methods to optimize the retention as they are designed to maximize the long-term performance. We formulate the problem as an infinite-horizon request-based Markov Decision Process, and our objective is to minimize the accumulated time interval of multiple sessions, which is equal to improving the app open frequency and user retention. However, current reinforcement learning algorithms can not be directly applied in this setting due to uncertainty, bias, and long delay time incurred by the properties of user retention. We propose a novel method, dubbed RLUR, to address the aforementioned challenges. Both offline and live experiments show that RLUR can significantly improve user retention. RLUR has been fully launched in Kuaishou app for a long time, and achieves consistent performance improvement on user retention and DAU.|最近，短视频平台通过向用户推荐有趣的内容实现了用户的快速增长。推荐的目的是优化用户保持率，从而推动 DAU (每日活跃用户)的增长。保留是用户和系统进行多次交互后的长期反馈，很难将保留奖励分解为每个项目或一个项目列表。因此，传统的点模型和列表模型不能优化保留。在本文中，我们选择强化学习方法来优化保留，因为它们旨在最大限度地提高长期绩效。我们把这个问题表述为一个基于无限期请求的马可夫决策过程，我们的目标是最小化多个会话的累积时间间隔，这相当于提高应用程序的开放频率和用户保持率。然而，由于用户保持特性的不确定性、偏差和长时间延迟，现有的强化学习算法不能直接应用于这种设置。我们提出了一种新的方法，称为 RLUR，以解决上述挑战。离线和现场实验都表明，RLUR 可以显著提高用户保持率。RLUR 已经在 Kuaishou 应用程序中全面推出很长时间了，并且在用户保留和 DAU 方面取得了持续的性能改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reinforcing+User+Retention+in+a+Billion+Scale+Short+Video+Recommender+System)|0|
|[Jointly modeling products and resource pages for task-oriented recommendation](https://doi.org/10.1145/3543873.3584642)|Brendan Duncan, Surya Kallumadi, Taylor BergKirkpatrick, Julian J. McAuley|UC San Diego, USA; Lowe's Companies, Inc., USA|Modeling high-level user intent in recommender systems can improve performance, although it is often difficult to obtain a ground truth measure of this intent. In this paper, we investigate a novel way to obtain such an intent signal by leveraging resource pages associated with a particular task. We jointly model product interactions and resource page interactions to create a system which can recommend both products and resource pages to users. Our experiments consider the domain of home improvement product recommendation, where resource pages are DIY (do-it-yourself) project pages from Lowes.com. Each DIY page provides a list of tools, materials, and step-by-step instructions to complete a DIY project, such as building a deck, installing cabinets, and fixing a leaking pipe. We use this data as an indicator of the intended project, which is a natural high-level intent signal for home improvement shoppers. We then extend a state-of-the-art system to incorporate this new intent data, and show a significant improvement in the ability of the system to recommend products. We further demonstrate that our system can be used to successfully recommend DIY project pages to users. We have taken initial steps towards deploying our method for project recommendation in production on the Lowe’s website and for recommendations through marketing emails.|在推荐系统中建模高级用户意图可以提高性能，尽管通常很难获得这种意图的地面真实度量。在本文中，我们研究了一种通过利用与特定任务相关联的资源页来获得这种意图信号的新方法。我们联合对产品交互和资源页交互进行建模，以创建一个可以向用户推荐产品和资源页的系统。我们的实验考虑了家装产品推荐领域，其中的资源页面是来自 Lowes.com 的 DIY (DIY-it-yourself)项目页面。每一个 DIY 页面都提供了一系列的工具、材料和一步一步的指导来完成一个 DIY 项目，例如建造一个甲板、安装橱柜和修复一个泄漏的管道。我们使用这些数据作为预期项目的指标，这对于家装购物者来说是一个自然的高层次的意图信号。然后，我们扩展了一个最先进的系统来合并这些新的意图数据，并显示系统推荐产品的能力有了显著的提高。我们进一步演示了我们的系统可以用来成功地向用户推荐 DIY 项目页面。我们已经采取了初步步骤，部署我们的方法，项目推荐生产在劳的网站上，并通过营销电子邮件的建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Jointly+modeling+products+and+resource+pages+for+task-oriented+recommendation)|0|
|[Meta-Generator Enhanced Multi-Domain Recommendation](https://doi.org/10.1145/3543873.3584652)|Yingyi Zhang, Xianneng Li, Yahe Yu, Jian Tang, Huanfang Deng, Junya Lu, Yeyin Zhang, Qiancheng Jiang, Yunsen Xian, Liqian Yu, Han Liu|Dalian University of Technology, China; Meituan-Dianping Group, China; Meituan, China|Large-scale e-commercial platforms usually contain multiple business fields, which require industrial algorithms to characterize user intents across multiple domains. Numerous efforts have been made in user multi-domain intent modeling to achieve state-of-the-art performance. However, existing methods mainly focus on the domains having rich user information, which makes implementation to domains with sparse or rare user behavior meet with mixed success. Hence, in this paper, we propose a novel method named Meta-generator enhanced multi-Domain model (MetaDomain) to address the above issue. MetaDomain mainly includes two steps, 1) users’ multi-domain intent representation and 2) users’ multi-domain intent fusion. Specifically, in users’ multi-domain intent representation, we use the gradient information from a domain intent extractor to train the domain intent meta-generator, where the domain intent extractor has the input of users’ sequence feature and domain meta-generator has the input of users’ basic feature, hence the capability of generating users’ intent with sparse behavior. Afterward, in users’ multi-domain intent fusion, a domain graph is used to represent the high-order multi-domain connectivity. Extensive experiments have been carried out under a real-world industrial platform named Meituan. Both offline and rigorous online A/B tests under the billion-level data scale demonstrate the superiority of the proposed MetaDomain method over the state-of-the-art baselines. Furthermore comparing with the method using multi-domain sequence features, MetaDomain can reduce the serving latency by 20%. Currently, MetaDomain has been deployed in Meituan one of the largest worldwide Online-to-Offline(O2O) platforms.|大型电子商务平台通常包含多个业务字段，这需要工业算法来描述跨多个域的用户意图。在用户多领域意图建模方面做了大量工作，以实现最先进的性能。然而，现有的方法主要集中在具有丰富用户信息的域上，这使得对于用户行为稀疏或罕见的域的实现成败参半。为此，本文提出了一种新的元生成器增强型多域模型(MetaDomain)来解决上述问题。元域主要包括两个步骤: 1)用户的多域意图表示和2)用户的多域意图融合。具体来说，在用户的多领域意图表示中，我们利用领域意图提取器的梯度信息来训练领域意图元生成器，其中领域意图提取器输入用户的序列特征，领域元生成器输入用户的基本特征，从而具有生成稀疏行为的用户意图的能力。然后，在用户的多域意图融合中，使用一个域图来表示高阶多域连通性。在一个名为“美团”的真实工业平台下，人们进行了广泛的实验。在十亿级数据规模下的离线和严格的在线 A/B 测试都证明了提出的 MetaDomain 方法相对于最先进的基线的优越性。此外，与采用多域序列特征的方法相比，元域可以减少20% 的服务延迟。目前，MetaDomain 已经部署在全球最大的在线到离线(o2O)平台之一的美团上。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Meta-Generator+Enhanced+Multi-Domain+Recommendation)|0|
|[Integrated Ranking for News Feed with Reinforcement Learning](https://doi.org/10.1145/3543873.3584651)|Menghui Zhu, Wei Xia, Weiwen Liu, Yifan Liu, Ruiming Tang, Weinan Zhang|Shanghai Jiao Tong University, China; Huawei Noah?s Ark Lab, China|With the development of recommender systems, it becomes an increasingly common need to mix multiple item sequences from different sources. Therefore, the integrated ranking stage is proposed to be responsible for this task with re-ranking models. However, existing methods ignore the relation between the sequences, thus resulting in local optimum over the interaction session. To resolve this challenge, in this paper, we propose a new model named NFIRank (News Feed Integrated Ranking with reinforcement learning) and formulate the whole interaction session as a MDP (Markov Decision Process). Sufficient offline experiments are provided to verify the effectiveness of our model. In addition, we deployed our model on Huawei Browser and gained 1.58% improvements in CTR compared with the baseline in online A/B test. Code will be available at https://gitee.com/mindspore/models/tree/master/research/recommend/NFIRank.|随着推荐系统的发展，混合来自不同来源的多个项目序列的需求变得越来越普遍。因此，提出综合排序阶段负责这一任务的重新排序模型。然而，现有的方法忽略了序列之间的关系，从而导致局部最优的交互会话。为了解决这个问题，在本文中，我们提出了一个新的模型 NFIRank (带强化学习的新闻源综合排名) ，并将整个交互会话表示为一个 MDP (马可夫决策过程)。通过离线实验验证了模型的有效性。此外，我们在华为浏览器上部署了我们的模型，与在线 A/B 测试的基线相比，点击率提高了1.58% 。密码将在 https://gitee.com/mindspore/models/tree/master/research/recommend/nfirank 公布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Integrated+Ranking+for+News+Feed+with+Reinforcement+Learning)|0|
|[Measuring e-Commerce Metric Changes in Online Experiments](https://doi.org/10.1145/3543873.3584654)|C. H. Bryan Liu, Emma J. McCoy|London School of Economics and Political Science, United Kingdom; ASOS.com, United Kingdom and Imperial College London, United Kingdom|Digital technology organizations routinely use online experiments (e.g. A/B tests) to guide their product and business decisions. In e-commerce, we often measure changes to transaction- or item-based business metrics such as Average Basket Value (ABV), Average Basket Size (ABS), and Average Selling Price (ASP); yet it remains a common pitfall to ignore the dependency between the value/size of transactions/items during experiment design and analysis. We present empirical evidence on such dependency, its impact on measurement uncertainty, and practical implications on A/B test outcomes if left unmitigated. By making the evidence available, we hope to drive awareness of the pitfall among experimenters in e-commerce and hence encourage the adoption of established mitigation approaches. We also share lessons learned when incorporating selected mitigation approaches into our experimentation analysis platform currently in production.|数字技术组织经常使用在线实验(例如 A/B 测试)来指导他们的产品和商业决策。在电子商务中，我们经常衡量基于交易或项目的业务指标的变化，如平均篮子价值(ABV)、平均篮子大小(ABS)和平均销售价格(ASP) ; 然而，在实验设计和分析过程中忽视交易/项目的价值/大小之间的依赖性仍然是一个常见的陷阱。我们介绍了这种依赖性的经验证明，它对测量不确定性的影响，以及如果不加以缓解的话对 A/B 测试结果的实际影响。通过提供证据，我们希望提高电子商务实验者对这一陷阱的认识，从而鼓励采用既定的缓解办法。我们还分享了将选定的缓解方法纳入我们目前正在生产的实验分析平台的经验教训。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Measuring+e-Commerce+Metric+Changes+in+Online+Experiments)|0|
|[Improve retrieval of regional titles in streaming services with dense retrieval](https://doi.org/10.1145/3543873.3587619)|Bhargav Upadhyay, Tejas Khairnar, Anup Kotalwar|Amazon, India|Customers search for movie and series titles released across the world on streaming services like primevideo.com (PV), netflix.com (Netflix). In non-English speaking countries like India, Nepal and many others, the regional titles are transliterated from native language to English and are being searched in English. Given that there can be multiple transliterations possible for almost all the titles, searching for a regional title can be a very frustrating customer experience if these nuances are not handled correctly by the search system. Typing errors make the problem even more challenging. Streaming services uses spell correction and auto-suggestions/auto-complete features to address this issue up to certain extent. Auto-suggest fails when user searches keywords not in scope of the auto-suggest. Spell correction is effective at correcting common typing errors but as these titles doesn’t follow strict grammar rules and new titles constantly added to the catalog, spell correction have limited success. With recent progress in deep learning (DL), embedding vectors based dense retrieval is being used extensively to retrieve semantically relevant documents for a given query. In this work, we have used dense retrieval to address the noise introduced by transliteration variations and typing errors to improve retrieval of regional media titles. In the absent of any relevant dataset to test our hypothesis, we created a new dataset of 40K query title pairs from PV search logs. We also created a baseline by bench-marking PV’s performance on test data. We present an extensive study on the impact of 1. pre-training, 2. data augmentation, 3. positive to negative sample ratio, and 4. choice of loss function on retrieval performance. Our best model has shown 51.24% improvement in [email protected] over PV baseline.|客户可以通过流媒体服务搜索世界各地发行的电影和剧集，比如 primeideo.com (PV)、 Netflix.com (Netflix)。在非英语国家，如印度、尼泊尔和许多其他国家，地区标题被从母语音译成英语，并用英语进行搜索。鉴于几乎所有标题都可能有多种音译，如果搜索系统不能正确处理这些细微差别，那么搜索地区标题可能是一种非常令人沮丧的客户体验。键入错误使问题更具挑战性。流媒体服务使用拼写修正和自动建议/自动完成功能在一定程度上解决了这个问题。当用户搜索不在自动建议范围内的关键字时，自动建议失败。拼写纠正在纠正常见的打字错误方面是有效的，但是由于这些标题没有遵循严格的语法规则，而且新标题不断地添加到目录中，拼写纠正的成功有限。随着深度学习(DL)技术的发展，基于嵌入向量的密集检索技术被广泛应用于给定查询的语义相关文档检索。在本研究中，我们利用密集检索来解决音译变异和打字错误所引起的噪音问题，以提高地区性媒体标题的检索效率。在缺乏相关数据集来检验我们的假设的情况下，我们从 PV 搜索日志中创建了一个40K 查询标题对的新数据集。我们还通过在测试数据上标记 PV 的性能来创建基线。我们提出了一个广泛的研究影响1。训练前2分钟。数据增强，3。正负样本比率，以及4。损失函数对检索性能的选择。我们最好的模型已经显示了51.24% 的改善[电子邮件保护]超过 PV 基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improve+retrieval+of+regional+titles+in+streaming+services+with+dense+retrieval)|0|
|[hp-frac: An index to determine Awarded Researchers](https://doi.org/10.1145/3543873.3587597)|Aashay Singhal, Kamalakar Karlapalem|International Institute of Information Technology, Hyderabad, India|In order to advance academic research, it is important to assess and evaluate the academic influence of researchers and the findings they produce. Citation metrics are universally used methods to evaluate researchers. Amongst the several variations of citation metrics, the h-index proposed by Hirsch has become the leading measure. Recent work shows that h-index is not an effective measure to determine scientific impact - due to changing authorship patterns. This can be mitigated by using h-index of a paper to compute h-index of an author. We show that using fractional allocation of h-index gives better results. In this work, we reapply two indices based on the h-index of a single paper. The indices are referred to as: hp-index and hp-frac-index. We run large-scale experiments in three different fields with about a million publications and 3,000 authors. Our experiments show that hp-frac-index provides a unique ranking when compared to h-index. It also performs better than h-index in providing higher ranks to the awarded researcher.|为了推进学术研究，评估和评估研究人员的学术影响力和他们的发现是非常重要的。引文指标是评价科研人员的普遍方法。在引文指标的众多变化中，赫希提出的 h 指标已经成为引文指标的主导指标。最近的研究表明，由于作者模式的改变，h 指数不是确定科学影响的有效方法。这可以通过使用论文的 h- 索引来计算作者的 h- 索引来减轻。我们表明，使用分数分配的 h 指标给出了更好的结果。在这项工作中，我们重新应用两个指标的基础上的单一文件的 h-索引。这些指数被称为: hp-index 和 hp-frac-index。我们在三个不同的领域进行了大规模的实验，发表了大约一百万篇论文，有3000名作者。我们的实验表明，与 h 指数相比，hp-frac 指数提供了一个唯一的排名。在为获奖研究人员提供更高的排名方面，它也比 h-index 表现得更好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=hp-frac:+An+index+to+determine+Awarded+Researchers)|0|
|[Application of an ontology for model cards to generate computable artifacts for linking machine learning information from biomedical research](https://doi.org/10.1145/3543873.3587601)|Muhammad Amith, Licong Cui, Kirk Roberts, Cui Tao|Department of Information Science, University of North Texas, USA; School of Biomedical Informatics, University of Texas Health Science Center at Houston, USA; School of Biomedical Informatics, The University of Texas Health Science Center at Houston, USA|Model card reports provide a transparent description of machine learning models which includes information about their evaluation, limitations, intended use, etc. Federal health agencies have expressed an interest in model cards report for research studies using machine-learning based AI. Previously, we have developed an ontology model for model card reports to structure and formalize these reports. In this paper, we demonstrate a Java-based library (OWL API, FaCT++) that leverages our ontology to publish computable model card reports. We discuss future directions and other use cases that highlight applicability and feasibility of ontology-driven systems to support FAIR challenges.|模型卡片报告提供了机器学习模型的透明描述，包括它们的评估、限制、预期用途等信息。联邦卫生机构对使用基于机器学习的人工智能的研究报告模型卡表示了兴趣。在此之前，我们已经开发了一个模型卡片报告的本体模型来构造和形式化这些报告。在本文中，我们演示了一个基于 Java 的库(OWL API，FaCT + +) ，它利用我们的本体来发布可计算模型卡报告。我们讨论未来的方向和其他用例，突出本体驱动的系统的适用性和可行性，以支持 FAIR 的挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Application+of+an+ontology+for+model+cards+to+generate+computable+artifacts+for+linking+machine+learning+information+from+biomedical+research)|0|
|[Stance Inference in Twitter through Graph Convolutional Collaborative Filtering Networks with Minimal Supervision](https://doi.org/10.1145/3543873.3587640)|Zhiwei Zhou, Erick Elejalde|Leibniz Universität Hannover, L3S Research Center, Germany|Social Media (SM) has become a stage for people to share thoughts, emotions, opinions, and almost every other aspect of their daily lives. This abundance of human interaction makes SM particularly attractive for social sensing. Especially during polarizing events such as political elections or referendums, users post information and encourage others to support their side, using symbols such as hashtags to represent their attitudes. However, many users choose not to attach hashtags to their messages, use a different language, or show their position only indirectly. Thus, automatically identifying their opinions becomes a more challenging task. To uncover these implicit perspectives, we propose a collaborative filtering model based on Graph Convolutional Networks that exploits the textual content in messages and the rich connections between users and topics. Moreover, our approach only requires a small annotation effort compared to state-of-the-art solutions. Nevertheless, the proposed model achieves competitive performance in predicting individuals' stances. We analyze users' attitudes ahead of two constitutional referendums in Chile in 2020 and 2022. Using two large Twitter datasets, our model achieves improvements of 3.4% in recall and 3.6% in accuracy over the baselines.|社交媒体(SM)已经成为人们分享思想、情感、观点以及日常生活中几乎所有其他方面的一个舞台。这种丰富的人际互动使 SM 对社会感知特别有吸引力。特别是在政治选举或公民投票等两极分化的活动中，用户发布信息并鼓励其他人支持他们的立场，使用标签等符号来表达他们的态度。但是，许多用户选择不在消息中附加标签，不使用其他语言，或者只间接显示自己的位置。因此，自动识别他们的观点成为一项更具挑战性的任务。为了揭示这些隐含的观点，我们提出了一个基于图形卷积网络的协同过滤模型，该模型利用消息中的文本内容以及用户和主题之间的丰富联系。此外，与最先进的解决方案相比，我们的方法只需要很少的注释工作。然而，所提出的模型在预测个体的立场方面达到了竞争性的表现。我们分析了2020年和2022年智利两次宪法公投前用户的态度。使用两个大型的 Twitter 数据集，我们的模型比基线数据集提高了3.4% 的召回率和3.6% 的准确率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Stance+Inference+in+Twitter+through+Graph+Convolutional+Collaborative+Filtering+Networks+with+Minimal+Supervision)|0|
|[Retrieving false claims on Twitter during the Russia-Ukraine conflict](https://doi.org/10.1145/3543873.3587571)|Valerio La Gatta, Chiyu Wei, Luca Luceri, Francesco Pierri, Emilio Ferrara|Politecnico di Milano, Italy and Information Sciences Institute, University of Southern California, USA; Information Sciences Institute, University of Southern California, USA; Information Sciences Institute, University of Southern California, USA and University of Naples Federico II, Italy|Nowadays, false and unverified information on social media sway individuals' perceptions during major geo-political events and threaten the quality of the whole digital information ecosystem. Since the Russian invasion of Ukraine, several fact-checking organizations have been actively involved in verifying stories related to the conflict that circulated online. In this paper, we leverage a public repository of fact-checked claims to build a methodological framework for automatically identifying false and unsubstantiated claims spreading on Twitter in February 2022. Our framework consists of two sequential models: First, the claim detection model identifies whether tweets incorporate a (false) claim among those considered in our collection. Then, the claim retrieval model matches the tweets with fact-checked information by ranking verified claims according to their relevance with the input tweet. Both models are based on pre-trained language models and fine-tuned to perform a text classification task and an information retrieval task, respectively. In particular, to validate the effectiveness of our methodology, we consider 83 verified false claims that spread on Twitter during the first week of the invasion, and manually annotate 5,872 tweets according to the claim(s) they report. Our experiments show that our proposed methodology outperforms standard baselines for both claim detection and claim retrieval. Overall, our results highlight how social media providers could effectively leverage semi-automated approaches to identify, track, and eventually moderate false information that spreads on their platforms.|如今，社交媒体上的虚假和未经证实的信息在重大地缘政治事件中左右着人们的看法，威胁着整个数字信息生态系统的质量。自从俄罗斯入侵乌克兰以来，几个事实核查组织一直积极参与核实在网上流传的与冲突有关的故事。在本文中，我们利用一个事实核查索赔的公共数据库，建立一个方法框架，自动识别2022年2月在 Twitter 上传播的虚假和未经证实的索赔。我们的框架由两个顺序模型组成: 首先，索赔检测模型确定 tweet 是否在我们的集合中考虑的索赔中包含(虚假)索赔。然后，索赔检索模型根据索赔与输入索赔的相关性对索赔进行排序，从而将索赔与事实核查信息进行匹配。这两种模型都是基于预先训练好的语言模型，经过微调后分别执行文本分类任务和信息检索分类任务。特别是，为了验证我们的方法的有效性，我们考虑了在入侵的第一周在 Twitter 上传播的83个经过验证的虚假声明，并根据他们报告的声明手动注释了5,872条推文。我们的实验表明，我们提出的方法在索赔检测和索赔检索方面都优于标准基线。总的来说，我们的研究结果强调了社交媒体提供商如何有效地利用半自动化方法来识别、跟踪并最终控制在他们的平台上传播的虚假信息。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retrieving+false+claims+on+Twitter+during+the+Russia-Ukraine+conflict)|0|
|[Enhancing Hierarchy-Aware Graph Networks with Deep Dual Clustering for Session-based Recommendation](https://doi.org/10.1145/3543507.3583247)|Jiajie Su, Chaochao Chen, Weiming Liu, Fei Wu, Xiaolin Zheng, Haoming Lyu|Zhejiang University, China|Session-based Recommendation aims at predicting the next interacted item based on short anonymous behavior sessions. However, existing solutions neglect to model two inherent properties of sequential representing distributions, i.e., hierarchy structures resulted from item popularity and collaborations existing in both intra- and inter-session. Tackling with these two factors at the same time is challenging. On the one hand, traditional Euclidean space utilized in previous studies fails to capture hierarchy structures due to a restricted representation ability. On the other hand, the intuitive apply of hyperbolic geometry could extract hierarchical patterns but more emphasis on degree distribution weakens intra- and inter-session collaborations. To address the challenges, we propose a Hierarchy-Aware Dual Clustering Graph Network (HADCG) model for session-based recommendation. Towards the first challenge, we design the hierarchy-aware graph modeling module which converts sessions into hyperbolic session graphs, adopting hyperbolic geometry in propagation and attention mechanism so as to integrate chronological and hierarchical information. As for the second challenge, we introduce the deep dual clustering module which develops a two-level clustering strategy, i.e., information regularizer for intra-session clustering and contrastive learner for inter-session clustering, to enhance hyperbolic representation learning from collaborative perspectives and further promote recommendation performance. Extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed HADCG.|基于会话的推荐是基于短的匿名行为会话来预测下一个交互项。然而，现有的解决方案忽视了顺序表示分布的两个固有属性，即由于项目流行和会话内和会话间存在的协作而产生的层次结构。同时处理这两个因素是具有挑战性的。一方面，传统的欧氏空间由于表示能力的限制，无法捕捉层次结构;。另一方面，双曲几何的直观应用可以提取等级模式，但更强调学位分配会削弱会话内部和会话间的协作。针对这一挑战，我们提出了一种基于会话的层次感知双聚类图网络(HADCG)模型。针对第一个挑战，我们设计了层次感知的图形建模模块，它将会话转换为双曲会话图，在传播和注意机制中采用双曲几何，以便整合时间和层次信息。针对第二个挑战，我们引入了深度双聚类模型，提出了一种两级聚类策略，即会话内聚类的信息调整器和会话间聚类的对比学习器，从协作的角度提高双曲表示学习，进一步提高推荐性能。在三个实际数据集上的大量实验证明了所提出的 HADCG 算法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Hierarchy-Aware+Graph+Networks+with+Deep+Dual+Clustering+for+Session-based+Recommendation)|0|
|[Intra and Inter Domain HyperGraph Convolutional Network for Cross-Domain Recommendation](https://doi.org/10.1145/3543507.3583402)|Zhongxuan Han, Xiaolin Zheng, Chaochao Chen, Wenjie Cheng, Yang Yao|Zhejiang Lab, China; Zhejiang University, China|Cross-Domain Recommendation (CDR) aims to solve the data sparsity problem by integrating the strengths of different domains. Though researchers have proposed various CDR methods to effectively transfer knowledge across domains, they fail to address the following key issues, i.e., (1) they cannot model high-order correlations among users and items in every single domain to obtain more accurate representations; (2) they cannot model the correlations among items across different domains. To tackle the above issues, we propose a novel Intra and Inter Domain HyperGraph Convolutional Network (II-HGCN) framework, which includes two main layers in the modeling process, i.e., the intra-domain layer and the inter-domain layer. In the intra-domain layer, we design a user hypergraph and an item hypergraph to model high-order correlations inside every single domain. Thus we can address the data sparsity problem better and learn high-quality representations of users and items. In the inter-domain layer, we propose an inter-domain hypergraph structure to explore correlations among items from different domains based on their interactions with common users. Therefore we can not only transfer the knowledge of users but also combine embeddings of items across domains. Comprehensive experiments on three widely used benchmark datasets demonstrate that II-HGCN outperforms other state-of-the-art methods, especially when datasets are extremely sparse.|跨域推荐(CDR)旨在通过整合不同域的优势来解决数据稀疏性问题。尽管研究人员已经提出了各种 CDR 方法来有效地跨领域传递知识，但他们未能解决以下关键问题，即: (1)他们不能模拟每个领域中用户和项目之间的高阶相关性以获得更准确的表示; (2)他们不能模拟不同领域中项目之间的相关性。为了解决上述问题，我们提出了一种新的域内和域间超图卷积网络(II-HGCN)框架，它包括建模过程中的两个主要层次，即域内层和域间层。在域内层，我们设计了一个用户超图和一个项目超图来模拟每个域内的高阶相关性。因此，我们可以更好地解决数据稀疏问题，并学习用户和项目的高质量表示。在域间层，我们提出了一个域间超图结构来探索来自不同领域的项目之间的相关性，基于它们与公共用户的交互。因此，不仅可以实现用户知识的传递，还可以实现跨域嵌入的组合。在三个广泛使用的基准数据集上的综合实验表明，II-HGCN 优于其他最先进的方法，特别是在数据集极其稀疏的情况下。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Intra+and+Inter+Domain+HyperGraph+Convolutional+Network+for+Cross-Domain+Recommendation)|0|
|[Generating Counterfactual Hard Negative Samples for Graph Contrastive Learning](https://doi.org/10.1145/3543507.3583499)|Haoran Yang, Hongxu Chen, Sixiao Zhang, Xiangguo Sun, Qian Li, Xiangyu Zhao, Guandong Xu|Curtin University, Australia; University of Technology Sydney, Australia; City University of Hong Kong, Hong Kong; The Chinese University of Hong Kong, Hong Kong|Graph contrastive learning has emerged as a powerful tool for unsupervised graph representation learning. The key to the success of graph contrastive learning is to acquire high-quality positive and negative samples as contrasting pairs for the purpose of learning underlying structural semantics of the input graph. Recent works usually sample negative samples from the same training batch with the positive samples, or from an external irrelevant graph. However, a significant limitation lies in such strategies, which is the unavoidable problem of sampling false negative samples. In this paper, we propose a novel method to utilize \textbf{C}ounterfactual mechanism to generate artificial hard negative samples for \textbf{G}raph \textbf{C}ontrastive learning, namely \textbf{CGC}, which has a different perspective compared to those sampling-based strategies. We utilize counterfactual mechanism to produce hard negative samples, which ensures that the generated samples are similar to, but have labels that different from the positive sample. The proposed method achieves satisfying results on several datasets compared to some traditional unsupervised graph learning methods and some SOTA graph contrastive learning methods. We also conduct some supplementary experiments to give an extensive illustration of the proposed method, including the performances of CGC with different hard negative samples and evaluations for hard negative samples generated with different similarity measurements.|图形对比学习已经成为无监督图形表示学习的有力工具。图形对比学习成功的关键是获取高质量的正负样本作为对比对，从而学习输入图的结构语义。最近的作品通常从同一训练批次的正样本中抽取负样本，或者从一个外部不相关的图中抽取负样本。然而，这种策略存在一个显著的局限性，这就是不可避免的采样假阴性样本的问题。本文提出了一种新的利用 textbf { C }反事实机制生成 textbf { G } raph textbf { C }对比学习人工硬负样本的方法，即 textbf { CGC }。我们利用反事实机制生成硬负样本，确保所生成的样本与正样本相似，但有不同于正样本的标签。与传统的无监督图形学习方法和 SOTA 图形对比学习方法相比，该方法在多个数据集上取得了令人满意的效果。我们还进行了一些补充实验，对所提出的方法进行了广泛的说明，包括对不同硬负样本的 CGC 性能和对不同相似度测量产生的硬负样本的评价。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generating+Counterfactual+Hard+Negative+Samples+for+Graph+Contrastive+Learning)|0|
|[Toward Degree Bias in Embedding-Based Knowledge Graph Completion](https://doi.org/10.1145/3543507.3583544)|Harry Shomer, Wei Jin, Wentao Wang, Jiliang Tang|Computer Science, Michigan State University, USA|A fundamental task for knowledge graphs (KGs) is knowledge graph completion (KGC). It aims to predict unseen edges by learning representations for all the entities and relations in a KG. A common concern when learning representations on traditional graphs is degree bias. It can affect graph algorithms by learning poor representations for lower-degree nodes, often leading to low performance on such nodes. However, there has been limited research on whether there exists degree bias for embedding-based KGC and how such bias affects the performance of KGC. In this paper, we validate the existence of degree bias in embedding-based KGC and identify the key factor to degree bias. We then introduce a novel data augmentation method, KG-Mixup, to generate synthetic triples to mitigate such bias. Extensive experiments have demonstrated that our method can improve various embedding-based KGC methods and outperform other methods tackling the bias problem on multiple benchmark datasets.|知识图的一个基本任务是知识图的完成。它的目的是通过学习 KG 中所有实体和关系的表示来预测看不见的边。学习传统图表示的一个常见问题是度偏差。它通过学习低度节点的差表示来影响图算法，经常导致低度节点的性能下降。然而，关于嵌入式 KGC 是否存在程度偏差以及这种偏差如何影响 KGC 性能的研究还很有限。在本文中，我们验证了基于嵌入的 KGC 中存在程度偏差，并找出了影响程度偏差的关键因素。然后，我们引入一种新的数据增强方法，KG 混合，产生合成三元组，以减轻这种偏差。大量的实验表明，该方法可以改进各种基于嵌入的 KGC 方法，并优于其他处理多基准数据集偏差问题的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Toward+Degree+Bias+in+Embedding-Based+Knowledge+Graph+Completion)|0|
|[LINet: A Location and Intention-Aware Neural Network for Hotel Group Recommendation](https://doi.org/10.1145/3543507.3583202)|Ruitao Zhu, Detao Lv, Yao Yu, Ruihao Zhu, Zhenzhe Zheng, Ke Bu, Quan Lu, Fan Wu|Cornell University, USA; Shanghai Jiao Tong University, China; Alibaba Group, China|Motivated by the collaboration with Fliggy1, a leading Online Travel Platform (OTP), we investigate an important but less explored research topic about optimizing the quality of hotel supply, namely selecting potential profitable hotels in advance to build up adequate room inventory. We formulate a WWW problem, i.e., within a specific time period (When) and potential travel area (Where), which hotels should be recommended to a certain group of users with similar travel intentions (Why). We identify three critical challenges in solving the WWW problem: user groups generation, travel data sparsity and utilization of hotel recommendation information (e.g., period, location and intention). To this end, we propose LINet, a Location and Intention-aware neural Network for hotel group recommendation. Specifically, LINet first identifies user travel intentions for user groups generalization, and then characterizes the group preferences by jointly considering historical user-hotel interaction and spatio-temporal features of hotels. For data sparsity, we develop a graph neural network, which employs long-term data, and further design an auxiliary loss function of location that efficiently exploits data within the same and across different locations. Both offline and online experiments demonstrate the effectiveness of LINet when compared with state-of-the-art methods. LINet has been successfully deployed on Fliggy to retrieve high quality hotels for business development, serving hundreds of hotel operation scenarios and thousands of hotel operators.|受到与领先的在线旅游平台(OTP) Fliggy1合作的启发，我们研究了一个重要但探索较少的优化酒店供应质量的研究课题，即提前选择潜在的盈利酒店，以建立足够的客房库存。我们制定了一个 WWW 问题，即在一个特定的时间段(何时)和潜在的旅游区域(何地) ，哪些酒店应该被推荐给具有相似旅游意图的特定用户群体(为什么)。我们确定了解决 WWW 问题的三个关键挑战: 用户群生成、旅游数据稀疏和酒店推荐信息的利用(例如，时间、地点和意图)。为此，我们提出了 LINet，一个位置和意图感知的神经网络，用于酒店集团推荐。具体来说，LINet 首先通过用户群的概括来识别用户的旅游意图，然后联合考虑历史上用户与酒店的交互和酒店的时空特征来表征用户的群体偏好。针对数据稀疏性问题，提出了一种基于长期数据的图形神经网络，并进一步设计了位置辅助损失函数，有效地利用同一位置和不同位置的数据。离线和在线实验都证明了与最先进的方法相比，LINet 的有效性。LINet 已经成功地部署在 Fliggy 上，为业务发展检索高质量的酒店，为数百家酒店运营方案和数千家酒店运营商提供服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LINet:+A+Location+and+Intention-Aware+Neural+Network+for+Hotel+Group+Recommendation)|0|
|[Distillation from Heterogeneous Models for Top-K Recommendation](https://doi.org/10.1145/3543507.3583209)|SeongKu Kang, Wonbin Kweon, Dongha Lee, Jianxun Lian, Xing Xie, Hwanjo Yu|Microsoft Research Asia, China; Yonsei University, Republic of Korea; Pohang University of Science and Technology, Republic of Korea|Recent recommender systems have shown remarkable performance by using an ensemble of heterogeneous models. However, it is exceedingly costly because it requires resources and inference latency proportional to the number of models, which remains the bottleneck for production. Our work aims to transfer the ensemble knowledge of heterogeneous teachers to a lightweight student model using knowledge distillation (KD), to reduce the huge inference costs while retaining high accuracy. Through an empirical study, we find that the efficacy of distillation severely drops when transferring knowledge from heterogeneous teachers. Nevertheless, we show that an important signal to ease the difficulty can be obtained from the teacher's training trajectory. This paper proposes a new KD framework, named HetComp, that guides the student model by transferring easy-to-hard sequences of knowledge generated from the teachers' trajectories. To provide guidance according to the student's learning state, HetComp uses dynamic knowledge construction to provide progressively difficult ranking knowledge and adaptive knowledge transfer to gradually transfer finer-grained ranking information. Our comprehensive experiments show that HetComp significantly improves the distillation quality and the generalization of the student model.|最近的推荐系统通过使用一系列异构模型显示了显著的性能。然而，它的成本非常高，因为它需要的资源和推理延迟与模型的数量成正比，这仍然是生产的瓶颈。我们的工作旨在利用知识精馏(KD)将异构教师的集成知识转移到一个轻量级的学生模型，以减少庞大的推理成本，同时保持较高的推理精度。通过实证研究发现，异质型教师传授知识时，蒸馏效果严重下降。然而，我们表明，一个重要的信号，缓解困难可以从教师的培训轨迹。提出了一种新的知识发现框架 HetComp，该框架通过传递由教师轨迹生成的易于生成的知识序列来指导学生模型。为了根据学生的学习状态提供指导，HetComp 使用动态知识结构来提供逐步难以排序的知识，并使用自适应知识转移来逐步传递更细粒度的排序信息。我们的综合实验表明，HetComp 显著提高了蒸馏质量和学生模型的推广。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distillation+from+Heterogeneous+Models+for+Top-K+Recommendation)|0|
|[Exploration and Regularization of the Latent Action Space in Recommendation](https://doi.org/10.1145/3543507.3583244)|Shuchang Liu, Qingpeng Cai, Bowen Sun, Yuhao Wang, Ji Jiang, Dong Zheng, Peng Jiang, Kun Gai, Xiangyu Zhao, Yongfeng Zhang|; Peking University, China; City University of Hong Kong, China; Rutgers University, USA; Kuaishou Technology, China|In recommender systems, reinforcement learning solutions have effectively boosted recommendation performance because of their ability to capture long-term user-system interaction. However, the action space of the recommendation policy is a list of items, which could be extremely large with a dynamic candidate item pool. To overcome this challenge, we propose a hyper-actor and critic learning framework where the policy decomposes the item list generation process into a hyper-action inference step and an effect-action selection step. The first step maps the given state space into a vectorized hyper-action space, and the second step selects the item list based on the hyper-action. In order to regulate the discrepancy between the two action spaces, we design an alignment module along with a kernel mapping function for items to ensure inference accuracy and include a supervision module to stabilize the learning process. We build simulated environments on public datasets and empirically show that our framework is superior in recommendation compared to standard RL baselines.|在推荐系统中，强化学习解决方案有效地提高了推荐性能，因为它们能够捕捉长期的用户系统交互。但是，推荐策略的操作空间是一个项目列表，对于动态候选项目池，这个列表可能非常大。为了克服这一挑战，我们提出了一个超行为者和批评者学习框架，其中策略将项目表生成过程分解为一个超行为推理步骤和一个效果-行为选择步骤。第一步将给定的状态空间映射到向量化的超动作空间，第二步根据超动作选择项目列表。为了调节两个动作空间之间的差异，我们设计了一个对齐模块和一个项目的核映射函数来保证推理的准确性，并包括一个监督模块来稳定学习过程。我们在公共数据集上建立了模拟环境，并且经验表明我们的框架在推荐方面优于标准 RL 基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploration+and+Regularization+of+the+Latent+Action+Space+in+Recommendation)|0|
|[Compressed Interaction Graph based Framework for Multi-behavior Recommendation](https://doi.org/10.1145/3543507.3583312)|Wei Guo, Chang Meng, Enming Yuan, Zhicheng He, Huifeng Guo, Yingxue Zhang, Bo Chen, Yaochen Hu, Ruiming Tang, Xiu Li, Rui Zhang|Huawei Technologies, Canada; Shenzhen International Graduate School, Tsinghua University, China; ruizhang.info, China; Institute for Interdisciplinary Information Sciences, Tsinghua University, China; Huawei Noah's Ark Lab, China|Multi-types of user behavior data (e.g., clicking, adding to cart, and purchasing) are recorded in most real-world recommendation scenarios, which can help to learn users' multi-faceted preferences. However, it is challenging to explore multi-behavior data due to the unbalanced data distribution and sparse target behavior, which lead to the inadequate modeling of high-order relations when treating multi-behavior data ''as features'' and gradient conflict in multitask learning when treating multi-behavior data ''as labels''. In this paper, we propose CIGF, a Compressed Interaction Graph based Framework, to overcome the above limitations. Specifically, we design a novel Compressed Interaction Graph Convolution Network (CIGCN) to model instance-level high-order relations explicitly. To alleviate the potential gradient conflict when treating multi-behavior data ''as labels'', we propose a Multi-Expert with Separate Input (MESI) network with separate input on the top of CIGCN for multi-task learning. Comprehensive experiments on three large-scale real-world datasets demonstrate the superiority of CIGF. Ablation studies and in-depth analysis further validate the effectiveness of our proposed model in capturing high-order relations and alleviating gradient conflict. The source code and datasets are available at https://github.com/MC-CV/CIGF.|多种类型的用户行为数据(例如，点击、添加到购物车和购买)记录在大多数真实世界的推荐场景中，这有助于了解用户的多方面偏好。然而，由于多行为数据分布不均衡，目标行为稀疏，导致多任务学习中将多行为数据“作为特征”的高阶关系建模不足，将多行为数据“作为标签”的多任务学习中存在梯度冲突。本文提出了一种基于压缩交互图的 CIGF 框架，以克服上述局限性。具体来说，我们设计了一个新的压缩交互图卷积网络(CIGCN)来显式地建模实例级的高阶关系。为了缓解多行为数据“作为标签”时潜在的梯度冲突，本文提出了一种在 CIGCN 顶部具有独立输入的多专家网络，用于多任务学习。通过对三个大规模实际数据集的综合实验，验证了 CIGF 算法的优越性。烧蚀研究和深入分析进一步验证了我们提出的模型在捕获高阶关系和缓解梯度冲突方面的有效性。源代码和数据集可在 https://github.com/mc-cv/cigf 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Compressed+Interaction+Graph+based+Framework+for+Multi-behavior+Recommendation)|0|
|[Correlative Preference Transfer with Hierarchical Hypergraph Network for Multi-Domain Recommendation](https://doi.org/10.1145/3543507.3583331)|Zixuan Xu, Penghui Wei, Shaoguo Liu, Weimin Zhang, Liang Wang, Bo Zheng|Alibaba Group, China|Advanced recommender systems usually involve multiple domains (such as scenarios or categories) for various marketing strategies, and users interact with them to satisfy diverse demands. The goal of multi-domain recommendation (MDR) is to improve the recommendation performance of all domains simultaneously. Conventional graph neural network based methods usually deal with each domain separately, or train a shared model to serve all domains. The former fails to leverage users' cross-domain behaviors, making the behavior sparseness issue a great obstacle. The latter learns shared user representation with respect to all domains, which neglects users' domain-specific preferences. In this paper we propose $\mathsf{H^3Trans}$, a hierarchical hypergraph network based correlative preference transfer framework for MDR, which represents multi-domain user-item interactions into a unified graph to help preference transfer. $\mathsf{H^3Trans}$ incorporates two hyperedge-based modules, namely dynamic item transfer (Hyper-I) and adaptive user aggregation (Hyper-U). Hyper-I extracts correlative information from multi-domain user-item feedbacks for eliminating domain discrepancy of item representations. Hyper-U aggregates users' scattered preferences in multiple domains and further exploits the high-order (not only pair-wise) connections to improve user representations. Experiments on both public and production datasets verify the superiority of $\mathsf{H^3Trans}$ for MDR.|高级推荐系统通常涉及多个领域(如场景或类别)的不同营销策略，用户与他们互动，以满足不同的需求。多域推荐(MDR)的目标是同时提高所有域的推荐性能。传统的基于图神经网络的方法通常分别处理各个领域，或者训练一个共享模型来服务于所有领域。前者未能充分利用用户的跨域行为，使得行为稀疏性问题成为一大障碍。后者学习所有域的共享用户表示，这忽略了用户特定域的首选项。本文提出了一种基于层次超图网络的 MDR 相关偏好传递框架 $mathsf { H ^ 3Trans } $，它将多领域用户-项目交互表示为一个统一的图，以帮助偏好传递。$mathsf { H ^ 3Trans } $包含两个基于超边界的模块，即动态项传输(Hyper-I)和自适应用户聚合(Hyper-U)。Hyper-I 从多领域用户项目反馈中提取相关信息，消除项目表示的领域差异。Hyper-U 聚合用户在多个域中的分散偏好，并进一步利用高阶(不仅仅是成对)连接来改善用户表示。在公共数据集和生产数据集上的实验验证了 $mathsf { H ^ 3Trans } $用于 MDR 的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Correlative+Preference+Transfer+with+Hierarchical+Hypergraph+Network+for+Multi-Domain+Recommendation)|0|
|[User Retention-oriented Recommendation with Decision Transformer](https://doi.org/10.1145/3543507.3583418)|Kesen Zhao, Lixin Zou, Xiangyu Zhao, Maolin Wang, Dawei Yin|Baidu Inc., China; Wuhan University, China; City University of Hong Kong, Hong Kong|Improving user retention with reinforcement learning~(RL) has attracted increasing attention due to its significant importance in boosting user engagement. However, training the RL policy from scratch without hurting users' experience is unavoidable due to the requirement of trial-and-error searches. Furthermore, the offline methods, which aim to optimize the policy without online interactions, suffer from the notorious stability problem in value estimation or unbounded variance in counterfactual policy evaluation. To this end, we propose optimizing user retention with Decision Transformer~(DT), which avoids the offline difficulty by translating the RL as an autoregressive problem. However, deploying the DT in recommendation is a non-trivial problem because of the following challenges: (1) deficiency in modeling the numerical reward value; (2) data discrepancy between the policy learning and recommendation generation; (3) unreliable offline performance evaluation. In this work, we, therefore, contribute a series of strategies for tackling the exposed issues. We first articulate an efficient reward prompt by weighted aggregation of meta embeddings for informative reward embedding. Then, we endow a weighted contrastive learning method to solve the discrepancy between training and inference. Furthermore, we design two robust offline metrics to measure user retention. Finally, the significant improvement in the benchmark datasets demonstrates the superiority of the proposed method.|使用强化学习 ~ (RL)提高用户保持率已经引起了越来越多的关注，因为它在提高用户参与度方面具有重要意义。然而，由于试错检索的要求，在不损害用户体验的情况下从头开始训练 RL 策略是不可避免的。此外，离线方法的目的是优化政策没有在线交互，受到臭名昭著的稳定性问题的价值估计或无界方差的反事实政策评估。为此，我们提出了利用决策转换器 ~ (DT)来优化用户保留，通过将 RL 转换为一个自回归问题来避免离线困难。然而，在推荐中部署 DT 是一个非常重要的问题，因为它面临以下挑战: (1)数值奖励值建模不足; (2)策略学习和推荐生成之间的数据差异; (3)不可靠的离线性能评估。在这项工作中，我们，因此，贡献了一系列的战略，以解决暴露的问题。我们首先通过元嵌入的加权聚合提出了一个有效的信息嵌入奖励提示。然后，我们提出了一种加权对比学习方法来解决训练和推理之间的差异。此外，我们还设计了两个健壮的离线度量来衡量用户保持率。最后，基准数据集的显著改进证明了该方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User+Retention-oriented+Recommendation+with+Decision+Transformer)|0|
|[Balancing Unobserved Confounding with a Few Unbiased Ratings in Debiased Recommendations](https://doi.org/10.1145/3543507.3583495)|Haoxuan Li, Yanghao Xiao, Chunyuan Zheng, Peng Wu|University of California, San Diego, USA; Peking University, China; University of Chinese Academy of Sciences, China; Beijing Technology and Business University, China|Recommender systems are seen as an effective tool to address information overload, but it is widely known that the presence of various biases makes direct training on large-scale observational data result in sub-optimal prediction performance. In contrast, unbiased ratings obtained from randomized controlled trials or A/B tests are considered to be the golden standard, but are costly and small in scale in reality. To exploit both types of data, recent works proposed to use unbiased ratings to correct the parameters of the propensity or imputation models trained on the biased dataset. However, the existing methods fail to obtain accurate predictions in the presence of unobserved confounding or model misspecification. In this paper, we propose a theoretically guaranteed model-agnostic balancing approach that can be applied to any existing debiasing method with the aim of combating unobserved confounding and model misspecification. The proposed approach makes full use of unbiased data by alternatively correcting model parameters learned with biased data, and adaptively learning balance coefficients of biased samples for further debiasing. Extensive real-world experiments are conducted along with the deployment of our proposal on four representative debiasing methods to demonstrate the effectiveness.|推荐系统被视为解决信息超载问题的有效工具，但众所周知，各种偏差的存在使得对大规模观测数据的直接训练导致次优预测性能。相比之下，通过随机对照试验或 A/B 测试获得的无偏评分被认为是黄金标准，但实际上成本高，规模小。为了利用这两种类型的数据，最近的工作建议使用无偏评级来修正倾向或插补模型的参数训练偏向的数据集。然而，现有的方法不能获得准确的预测存在未观察到的混杂或模型错误说明。本文提出了一种理论保证的模型无关平衡方法，该方法可以应用于任何现有的去偏方法，以消除未观察到的混淆和模型不确定性。该方法充分利用无偏数据，通过交替校正有偏数据学习的模型参数，以及有偏样本的自适应学习平衡系数进一步消除偏差。随着我们的建议在四个有代表性的去偏方法上的部署，广泛的现实世界的实验被进行，以证明有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Balancing+Unobserved+Confounding+with+a+Few+Unbiased+Ratings+in+Debiased+Recommendations)|0|
|[Denoising and Prompt-Tuning for Multi-Behavior Recommendation](https://doi.org/10.1145/3543507.3583513)|Chi Zhang, Rui Chen, Xiangyu Zhao, Qilong Han, Li Li|Harbin Engineering University, China; University of Delaware, USA; City University of Hong Kong, Hong Kong|In practical recommendation scenarios, users often interact with items under multi-typed behaviors (e.g., click, add-to-cart, and purchase). Traditional collaborative filtering techniques typically assume that users only have a single type of behavior with items, making it insufficient to utilize complex collaborative signals to learn informative representations and infer actual user preferences. Consequently, some pioneer studies explore modeling multi-behavior heterogeneity to learn better representations and boost the performance of recommendations for a target behavior. However, a large number of auxiliary behaviors (i.e., click and add-to-cart) could introduce irrelevant information to recommenders, which could mislead the target behavior (i.e., purchase) recommendation, rendering two critical challenges: (i) denoising auxiliary behaviors and (ii) bridging the semantic gap between auxiliary and target behaviors. Motivated by the above observation, we propose a novel framework-Denoising and Prompt-Tuning (DPT) with a three-stage learning paradigm to solve the aforementioned challenges. In particular, DPT is equipped with a pattern-enhanced graph encoder in the first stage to learn complex patterns as prior knowledge in a data-driven manner to guide learning informative representation and pinpointing reliable noise for subsequent stages. Accordingly, we adopt different lightweight tuning approaches with effectiveness and efficiency in the following stages to further attenuate the influence of noise and alleviate the semantic gap among multi-typed behaviors. Extensive experiments on two real-world datasets demonstrate the superiority of DPT over a wide range of state-of-the-art methods. The implementation code is available online at https://github.com/zc-97/DPT.|在实际的推荐场景中，用户经常在多类型行为下与项目交互(例如，单击、添加到购物车和购买)。传统的协同过滤技术通常假设用户只有单一类型的行为与项目，使其不足以利用复杂的协作信号来学习信息表示和推断实际的用户偏好。因此，一些先驱研究探索建立多行为异质性模型，以学习更好的表示方法，并提高针对目标行为的建议的性能。然而，大量的辅助行为(即点击和添加到购物车)可能会向推荐者引入不相关的信息，这可能会误导目标行为(即购买)推荐，造成两个关键的挑战: (i)去噪辅助行为和(ii)桥接辅助行为和目标行为之间的语义差距。基于上述观察，我们提出了一个新的框架-去噪和及时调整(DPT)与三个阶段的学习范式，以解决上述挑战。特别是，DPT 在第一阶段配备了模式增强图形编码器，以数据驱动的方式学习复杂的模式作为先验知识，以指导学习信息表示和确定后续阶段的可靠噪声。相应地，我们在接下来的阶段采用了不同的轻量化方法，以进一步减小噪声的影响，缓解多类型行为之间的语义鸿沟。在两个实际数据集上的大量实验证明了 DPT 相对于一系列最先进的方法的优越性。实施守则可于网上 https://github.com/zc-97/dpt 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Denoising+and+Prompt-Tuning+for+Multi-Behavior+Recommendation)|0|
|[CAMUS: Attribute-Aware Counterfactual Augmentation for Minority Users in Recommendation](https://doi.org/10.1145/3543507.3583538)|Yuxin Ying, Fuzhen Zhuang, Yongchun Zhu, Deqing Wang, Hongwei Zheng|Beijing Academy of Blockchain and Edge Computing, China; Institute of Computing Technology, Chinese Academy of Sciences, China; School of Computer Science and Engineering, Beihang University, China; Institute of Artificial Intelligence, Beihang University, China|Embedding-based methods currently achieved impressive success in recommender systems. However, such methods are more likely to suffer from bias in data distribution, especially the attribute bias problem. For example, when a certain type of user, like the elderly, occupies the mainstream, the recommendation results of minority users would be seriously affected by the mainstream users’ attributes. To address this problem, most existing methods are proposed from the perspective of fairness, which focuses on eliminating unfairness but deteriorates the recommendation performance. Unlike these methods, in this paper, we focus on improving the recommendation performance for minority users of biased attributes. Along this line, we propose a novel attribute-aware Counterfactual Augmentation framework for Minority Users(CAMUS). Specifically, the CAMUS consists of a counterfactual augmenter, a confidence estimator, and a recommender. The counterfactual augmenter conducts data augmentation for the minority group by utilizing the interactions of mainstream users based on a universal counterfactual assumption. Besides, a tri-training-based confidence estimator is applied to ensure the effectiveness of augmentation. Extensive experiments on three real-world datasets have demonstrated the superior performance of the proposed methods. Further case studies verify the universality of the proposed CAMUS framework on different data sparsity, attributes, and models.|基于嵌入的方法目前在推荐系统中取得了令人印象深刻的成功。然而，这些方法更容易受到数据分布偏差的影响，特别是属性偏差问题。例如，当某种类型的用户，如老年人，占据主流时，少数用户的推荐结果会受到主流用户属性的严重影响。为了解决这个问题，现有的方法大多是从公平的角度出发，着重于消除不公平性，但是会降低推荐性能。与这些方法不同的是，本文主要研究如何提高偏向属性的少数用户的推荐性能。在此基础上，我们提出了一种新的面向少数用户的属性感知反事实增强框架(CAMUS)。具体来说，CAMUS 由一个反事实增强器、一个置信度估计器和一个推荐器组成。反事实增强器利用主流用户基于普遍反事实假设的交互作用对少数群体进行数据增强。此外，采用基于三训练的置信估计来保证增广的有效性。在三个实际数据集上的大量实验证明了该方法的优越性能。进一步的案例研究验证了所提出的 CAMUS 框架在不同的数据稀疏性、属性和模型上的通用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CAMUS:+Attribute-Aware+Counterfactual+Augmentation+for+Minority+Users+in+Recommendation)|0|
|[Dynamically Expandable Graph Convolution for Streaming Recommendation](https://doi.org/10.1145/3543507.3583237)|Bowei He, Xu He, Yingxue Zhang, Ruiming Tang, Chen Ma|Department of Computer Science, City University of Hong Kong, Hong Kong; Huawei Noah's Ark Lab, China; City University of Hong Kong, Hong Kong; Huawei Noah's Ark Lab Montreal, Canada|Personalized recommender systems have been widely studied and deployed to reduce information overload and satisfy users' diverse needs. However, conventional recommendation models solely conduct a one-time training-test fashion and can hardly adapt to evolving demands, considering user preference shifts and ever-increasing users and items in the real world. To tackle such challenges, the streaming recommendation is proposed and has attracted great attention recently. Among these, continual graph learning is widely regarded as a promising approach for the streaming recommendation by academia and industry. However, existing methods either rely on the historical data replay which is often not practical under increasingly strict data regulations, or can seldom solve the \textit{over-stability} issue. To overcome these difficulties, we propose a novel \textbf{D}ynamically \textbf{E}xpandable \textbf{G}raph \textbf{C}onvolution (DEGC) algorithm from a \textit{model isolation} perspective for the streaming recommendation which is orthogonal to previous methods. Based on the motivation of disentangling outdated short-term preferences from useful long-term preferences, we design a sequence of operations including graph convolution pruning, refining, and expanding to only preserve beneficial long-term preference-related parameters and extract fresh short-term preferences. Moreover, we model the temporal user preference, which is utilized as user embedding initialization, for better capturing the individual-level preference shifts. Extensive experiments on the three most representative GCN-based recommendation models and four industrial datasets demonstrate the effectiveness and robustness of our method.|个性化推荐系统已被广泛研究和部署，以减少信息超载和满足用户的不同需求。然而，考虑到用户偏好的变化以及现实世界中不断增加的用户和项目，传统的推荐模型只能进行一次性的训练测试，很难适应不断变化的需求。为了应对这些挑战，流媒体推荐被提出并引起了人们的广泛关注。其中，连续图学习被学术界和工业界广泛认为是一种很有前途的流推荐方法。然而，现有的方法要么依赖于历史数据的重放，这在日益严格的数据规则下往往是不切实际的，要么很少能解决文本的过稳定性问题。为了克服这些困难，我们从文本{模型隔离}的角度提出了一种新的动态 textbf { D }可扩展 textbf { E } Raph textbf { C }内卷(DEGC)算法用于流式推荐，该算法与以前的方法是正交的。基于将过时的短期偏好与有用的长期偏好分离的动机，我们设计了一系列操作，包括图卷积修剪，细化和扩展，以仅保留有益的长期偏好相关参数并提取新的短期偏好。此外，为了更好地捕获个体层次的偏好变化，我们建立了时态用户偏好模型，并将其用于用户嵌入初始化。在三个最具代表性的基于 GCN 的推荐模型和四个工业数据集上的大量实验证明了该方法的有效性和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamically+Expandable+Graph+Convolution+for+Streaming+Recommendation)|0|
|[CTRLStruct: Dialogue Structure Learning for Open-Domain Response Generation](https://doi.org/10.1145/3543507.3583285)|Congchi Yin, Piji Li, Zhaochun Ren|Nanjing University of Aeronautics and Astronautics, China; Shandong University, China|Dialogue structure discovery is essential in dialogue generation. Well-structured topic flow can leverage background information and predict future topics to help generate controllable and explainable responses. However, most previous work focused on dialogue structure learning in task-oriented dialogue other than open-domain dialogue which is more complicated and challenging. In this paper, we present a new framework CTRLStruct for dialogue structure learning to effectively explore topic-level dialogue clusters as well as their transitions with unlabelled information. Precisely, dialogue utterances encoded by bi-directional Transformer are further trained through a special designed contrastive learning task to improve representation. Then we perform clustering to utterance-level representations and form topic-level clusters that can be considered as vertices in dialogue structure graph. The edges in the graph indicating transition probability between vertices are calculated by mimicking expert behavior in datasets. Finally, dialogue structure graph is integrated into dialogue model to perform controlled response generation. Experiments on two popular open-domain dialogue datasets show our model can generate more coherent responses compared to some excellent dialogue models, as well as outperform some typical sentence embedding methods in dialogue utterance representation. Code is available in GitHub.|对话结构的发现是对话生成的基础。结构良好的主题流可以利用背景信息并预测未来的主题，从而帮助产生可控的和可解释的反应。然而，以往的研究大多侧重于面向任务的对话中的对话结构学习，而开放领域的对话更为复杂和具有挑战性。本文提出了一种新的对话结构学习框架 CTRLstruct，以有效地探索话题层次的对话群及其与未标记信息的过渡。准确地说，双向变压器编码的对话话语通过特别设计的对比学习任务进一步训练，以提高表征能力。然后对话语层面的表征进行聚类，形成话题层面的聚类，这些聚类可以看作是对话结构图中的顶点。通过模拟数据集中的专家行为，计算图中表示顶点间转移概率的边。最后，将对话结构图集成到对话模型中进行受控响应的生成。实验结果表明，与一些优秀的对话模型相比，该模型能够产生更加连贯的对话响应，并且在对话话语表征中优于一些典型的句子嵌入方法。代码可在 GitHub 中获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CTRLStruct:+Dialogue+Structure+Learning+for+Open-Domain+Response+Generation)|0|
|[BlinkViz: Fast and Scalable Approximate Visualization on Very Large Datasets using Neural-Enhanced Mixed Sum-Product Networks](https://doi.org/10.1145/3543507.3583411)|Yimeng Qiao, Yinan Jing, Hanbing Zhang, Zhenying He, Kai Zhang, X. Sean Wang|Shanghai Key Laboratory of Data Science, School of Software, Fudan University, China; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, China|Web-based online interactive visual analytics enjoys popularity in recent years. Traditionally, visualizations are produced directly from querying the underlying data. However, for a very large dataset, this way is so time-consuming that it cannot meet the low-latency requirements of interactive visual analytics. In this paper, we propose a learning-based visualization approach called BlinkViz, which uses a learned model to produce approximate visualizations by leveraging mixed sum-product networks to learn the distribution of the original data. In such a way, it makes visualization faster and more scalable by decoupling visualization and data. In addition, to improve the accuracy of approximate visualizations, we propose an enhanced model by incorporating a neural network with residual structures, which can refine prediction results, especially for visual requests with low selectivity. Extensive experiments show that BlinkViz is extremely fast even on a large dataset with hundreds of millions of data records (over 30GB), responding in sub-seconds (from 2ms to less than 500ms for different requests) while keeping a low error rate. Furthermore, our approach remains scalable on latency and memory footprint size regardless of data size.|基于 Web 的在线交互式可视化分析近年来很受欢迎。传统上，可视化是通过查询底层数据直接生成的。然而，对于一个非常大的数据集，这种方法非常耗时，不能满足交互式可视化分析的低延迟要求。本文提出了一种基于学习的可视化方法 BlinkViz，该方法利用学习模型，通过混合和积网络来学习原始数据的分布情况，从而产生近似可视化效果。通过这种方式，可视化和数据解耦，使得可视化更快、更具可伸缩性。此外，为了提高近似可视化的准确性，我们提出了一个增强的模型，通过结合残差结构的神经网络，可以细化预测结果，特别是对低选择性的可视化请求。大量的实验表明，BlinkViz 即使在拥有数亿条数据记录(超过30GB)的大型数据集上也是极其快速的，响应时间在亚秒(对于不同的请求，响应时间从2毫秒到不到500毫秒) ，同时保持较低的错误率。此外，无论数据大小如何，我们的方法在延迟和内存占用大小上都是可伸缩的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BlinkViz:+Fast+and+Scalable+Approximate+Visualization+on+Very+Large+Datasets+using+Neural-Enhanced+Mixed+Sum-Product+Networks)|0|
|[Semi-supervised Adversarial Learning for Complementary Item Recommendation](https://doi.org/10.1145/3543507.3583462)|Koby Bibas, Oren Sar Shalom, Dietmar Jannach|Amazon, Israel; Meta, Israel; University of Klagenfurt, Austria|Complementary item recommendations are a ubiquitous feature of modern e-commerce sites. Such recommendations are highly effective when they are based on collaborative signals like co-purchase statistics. In certain online marketplaces, however, e.g., on online auction sites, constantly new items are added to the catalog. In such cases, complementary item recommendations are often based on item side-information due to a lack of interaction data. In this work, we propose a novel approach that can leverage both item side-information and labeled complementary item pairs to generate effective complementary recommendations for cold items, i.e., for items for which no co-purchase statistics yet exist. Given that complementary items typically have to be of a different category than the seed item, we technically maintain a latent space for each item category. Simultaneously, we learn to project distributed item representations into these category spaces to determine suitable recommendations. The main learning process in our architecture utilizes labeled pairs of complementary items. In addition, we adopt ideas from Cycle Generative Adversarial Networks (CycleGAN) to leverage available item information even in case no labeled data exists for a given item and category. Experiments on three e-commerce datasets show that our method is highly effective.|互补商品推荐是现代电子商务网站的一个普遍特征。当这些建议基于合作信号(如共同购买统计数据)时，它们是非常有效的。然而，在某些在线市场，例如在线拍卖网站，不断有新物品被添加到目录中。在这种情况下，由于缺乏交互数据，补充项目推荐通常基于项目侧信息。在这项工作中，我们提出了一个新颖的方法，可以利用项目侧信息和标记的互补项目对，以产生有效的补充建议，冷的项目，即项目，共同购买统计数据尚不存在。鉴于补充项目通常必须是一个与种子项目不同的类别，我们在技术上为每个项目类别保持一个潜在的空间。同时，我们学习将分布式项表示投射到这些类别空间中，以确定合适的建议。在我们的建筑中，主要的学习过程是使用标记成对的互补项目。此外，我们采用循环生成对抗网络(CycleGAN)的思想来利用可用的项目信息，即使在给定的项目和类别没有标记数据存在的情况下。在三个电子商务数据集上的实验表明，该方法是高效的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semi-supervised+Adversarial+Learning+for+Complementary+Item+Recommendation)|0|
|[MaSS: Model-agnostic, Semantic and Stealthy Data Poisoning Attack on Knowledge Graph Embedding](https://doi.org/10.1145/3543507.3583203)|Xiaoyu You, Beina Sheng, Daizong Ding, Mi Zhang, Xudong Pan, Min Yang, Fuli Feng|University of Science and Technology of China, CCCD Key Lab of Ministry of Culture and Tourism, China; Fudan University, School of Computer Science, China|Open-source knowledge graphs are attracting increasing attention. Nevertheless, the openness also raises the concern of data poisoning attacks, that is, the attacker could submit malicious facts to bias the prediction of knowledge graph embedding (KGE) models. Existing studies on such attacks adopt a clear-box setting and neglect the semantic information of the generated facts, making them fail to attack in real-world scenarios. In this work, we consider a more rigorous setting and propose a model-agnostic, semantic, and stealthy data poisoning attack on KGE models from a practical perspective. The main design of our work is to inject indicative paths to make the infected model predict certain malicious facts. With the aid of the proposed opaque-box path injection theory, we theoretically reveal that the attack success rate under the opaque-box setting is determined by the plausibility of triplets on the indicative path. Based on this, we develop a novel and efficient algorithm to search paths that maximize the attack goal, satisfy certain semantic constraints, and preserve certain stealthiness, i.e., the normal functionality of the target KGE will not be influenced although it predicts wrong facts given certain queries. Through extensive evaluation of benchmark datasets and 6 typical knowledge graph embedding models as the victims, we validate the effectiveness in terms of attack success rate (ASR) under opaque-box setting and stealthiness. For example, on FB15k-237, our attack achieves a ASR on DeepPath, with an average ASR over when attacking various KGE models under the opaque-box setting.|开源知识图表越来越受到人们的关注。然而，这种开放性也引起了人们对数据中毒攻击的担忧，即攻击者可能会提交恶意事实来偏向知识图嵌入(KGE)模型的预测。现有的关于此类攻击的研究采用了清晰框设置，忽视了所生成事实的语义信息，使得它们无法在现实世界中进行攻击。在这项工作中，我们考虑了一个更严格的设置，并提出了一个模型无关，语义，隐秘的数据中毒攻击的 KGE 模型从实用的角度。我们工作的主要设计是注入指示性路径，使被感染的模型能够预测某些恶意事实。借助所提出的不透明盒路径注入理论，我们从理论上揭示了在不透明盒设置下的攻击成功率取决于指示路径上三联体的合理性。在此基础上，提出了一种新的高效的路径搜索算法，该算法能够使攻击目标最大化，满足一定的语义约束，并保持一定的隐蔽性。通过对基准数据集和6种典型的知识图嵌入模型作为受害者的广泛评估，验证了在不透明框设置和隐蔽性条件下的攻击成功率(ASR)的有效性。例如，在 FB15k-237上，我们的攻击在 DeepPath 上达到 ASR，在不透明盒子设置下攻击各种 KGE 模型时达到平均 ASR。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MaSS:+Model-agnostic,+Semantic+and+Stealthy+Data+Poisoning+Attack+on+Knowledge+Graph+Embedding)|0|
|[TaxoComplete: Self-Supervised Taxonomy Completion Leveraging Position-Enhanced Semantic Matching](https://doi.org/10.1145/3543507.3583342)|Ines Arous, Ljiljana Dolamic, Philippe CudréMauroux|University of Fribourg, Switzerland; armasuisse, Switzerland|Taxonomies are used to organize knowledge in many applications, including recommender systems, content browsing, or web search. With the emergence of new concepts, static taxonomies become obsolete as they fail to capture up-to-date knowledge. Several approaches have been proposed to address the problem of maintaining taxonomies automatically. These approaches typically rely on a limited set of neighbors to represent a given node in the taxonomy. However, considering distant nodes could improve the representation of some portions of the taxonomy, especially for those nodes situated in the periphery or in sparse regions of the taxonomy. In this work, we propose TaxoComplete, a self-supervised taxonomy completion framework that learns the representation of nodes leveraging their position in the taxonomy. TaxoComplete uses a self-supervision generation process that selects some nodes and associates each of them with an anchor set, which is a set composed of nodes in the close and distant neighborhood of the selected node. Using self-supervision data, TaxoComplete learns a position-enhanced node representation using two components: (1) a query-anchor semantic matching mechanism, which encodes pairs of nodes and matches their semantic distance to their graph distance, such that nodes that are close in the taxonomy are placed closely in the shared embedding space while distant nodes are placed further apart; (2) a direction-aware propagation module, which embeds the direction of edges in node representation, such that we discriminate <node, parent> relation from other taxonomic relations. Our approach allows the representation of nodes to encapsulate information from a large neighborhood while being aware of the distance separating pairs of nodes in the taxonomy. Extensive experiments on four real-world and large-scale datasets show that TaxoComplete is substantially more effective than state-of-the-art methods (2x more effective in terms of [email protected] ).|分类法用于在许多应用程序中组织知识，包括推荐系统、内容浏览或网络搜索。随着新概念的出现，静态分类法变得过时，因为它们无法捕获最新的知识。已经提出了几种方法来解决自动维护分类法的问题。这些方法通常依赖于一组有限的邻居来表示分类法中的给定节点。然而，考虑远处的节点可以改善分类学的某些部分的表示，特别是对于那些位于分类学的边缘或稀疏区域的节点。在这项工作中，我们提出了 TaxoComplete，一个自我监督的分类完成框架，它学习利用节点在分类中的位置来表示节点。TaxoComplete 使用一个自我监督的生成过程，它选择一些节点，并将它们与锚集相关联，锚集是由所选节点的近邻和远邻的节点组成的集合。TaxoComplete 利用自我监督数据学习位置增强的节点表示，它使用两个组件: (1)查询锚语义匹配机制，对节点对进行编码，并将它们的语义距离匹配到它们的图形距离上，这样在分类学中相近的节点被紧密地放置在共享嵌入空间中，而远处的节点被放置得更远; (2)方向感知传播模块，这种模块在节点表示中嵌入边的方向，这样我们可以区分 < 节点，父节点 > 关系和其他分类关系。我们的方法允许节点的表示来封装来自大邻居的信息，同时知道分类法中节点对之间的距离。在四个真实世界和大规模数据集上的大量实验表明，TaxoComplete 实质上比最先进的方法更有效(在[ email protected ]方面效率高出2倍)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TaxoComplete:+Self-Supervised+Taxonomy+Completion+Leveraging+Position-Enhanced+Semantic+Matching)|0|
|[Bipartite Graph Convolutional Hashing for Effective and Efficient Top-N Search in Hamming Space](https://doi.org/10.1145/3543507.3583219)|Yankai Chen, Yixiang Fang, Yifei Zhang, Irwin King|The Chinese University of Hong Kong, Shenzhen, China; The Chinese University of Hong Kong, Hong Kong|Searching on bipartite graphs is basal and versatile to many real-world Web applications, e.g., online recommendation, database retrieval, and query-document searching. Given a query node, the conventional approaches rely on the similarity matching with the vectorized node embeddings in the continuous Euclidean space. To efficiently manage intensive similarity computation, developing hashing techniques for graph structured data has recently become an emerging research direction. Despite the retrieval efficiency in Hamming space, prior work is however confronted with catastrophic performance decay. In this work, we investigate the problem of hashing with Graph Convolutional Network on bipartite graphs for effective Top-N search. We propose an end-to-end Bipartite Graph Convolutional Hashing approach, namely BGCH, which consists of three novel and effective modules: (1) adaptive graph convolutional hashing, (2) latent feature dispersion, and (3) Fourier serialized gradient estimation. Specifically, the former two modules achieve the substantial retention of the structural information against the inevitable information loss in hash encoding; the last module develops Fourier Series decomposition to the hashing function in the frequency domain mainly for more accurate gradient estimation. The extensive experiments on six real-world datasets not only show the performance superiority over the competing hashing-based counterparts, but also demonstrate the effectiveness of all proposed model components contained therein.|对于许多实际的 Web 应用程序，如在线推荐、数据库检索和查询文档搜索，二部图搜索是基础的和通用的。给定一个查询节点，传统的方法依赖于与连续欧氏空间中向量化节点嵌入的相似性匹配。为了有效地管理密集型相似度计算，开发图结构化数据的哈希技术已成为一个新兴的研究方向。尽管在汉明空间中反演效率很高，但是先前的工作却面临着灾难性的性能衰减。在本文中，我们研究了用图卷积网络对二部图进行散列以获得有效的 Top-N 搜索的问题。提出了一种端到端的二部图卷积哈希方法，即 BGCH，它由三个新颖有效的模块组成: (1)自适应图卷积哈希，(2)潜在特征分散，(3)傅里叶序列化梯度估计。具体来说，前两个模块针对哈希编码中不可避免的信息损失实现了结构信息的实质性保留，最后一个模块对频域中的哈希函数进行了傅里叶级数分解，以便更准确地进行梯度估计。在六个实际数据集上进行的大量实验不仅表明了该模型的性能优于竞争对手的散列模型，而且证明了其中所有模型组件的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bipartite+Graph+Convolutional+Hashing+for+Effective+and+Efficient+Top-N+Search+in+Hamming+Space)|0|
|[LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval](https://doi.org/10.1145/3543507.3583294)|Kai Zhang, Chongyang Tao, Tao Shen, Can Xu, Xiubo Geng, Binxing Jiao, Daxin Jiang|The Ohio State University, USA; University of Technology Sydney, Australia; Microsoft, China|Retrieval models based on dense representations in semantic space have become an indispensable branch for first-stage retrieval. These retrievers benefit from surging advances in representation learning towards compressive global sequence-level embeddings. However, they are prone to overlook local salient phrases and entity mentions in texts, which usually play pivot roles in first-stage retrieval. To mitigate this weakness, we propose to make a dense retriever align a well-performing lexicon-aware representation model. The alignment is achieved by weakened knowledge distillations to enlighten the retriever via two aspects -- 1) a lexicon-augmented contrastive objective to challenge the dense encoder and 2) a pair-wise rank-consistent regularization to make dense model's behavior incline to the other. We evaluate our model on three public benchmarks, which shows that with a comparable lexicon-aware retriever as the teacher, our proposed dense one can bring consistent and significant improvements, and even outdo its teacher. In addition, we found our improvement on the dense retriever is complementary to the standard ranker distillation, which can further lift state-of-the-art performance.|基于语义空间密集表示的检索模型已经成为第一阶段检索不可或缺的分支。这些检索器受益于表示学习向压缩全局序列级嵌入方向的飞速发展。然而，它们往往忽视文本中的局部显著短语和实体提及，而这些短语和实体提及在第一阶段的检索中起着关键作用。为了减轻这个弱点，我们建议使一个密集检索对齐一个良好的表现词典感知表示模型。该方法通过弱化知识提取，从两个方面启发检索者: 1)词典增强对比目标，挑战密集编码器; 2)成对秩一致正则化，使密集模型的行为倾向于另一方。我们在三个公共基准上评估了我们的模型，结果表明，与一个具有可比性的词汇感知检索器作为教师，我们提出的密集型可以带来一致和重大的改进，甚至超过它的老师。此外，我们发现我们对稠密检索器的改进是对标准等级精馏的补充，它可以进一步提高最先进的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LED:+Lexicon-Enlightened+Dense+Retriever+for+Large-Scale+Retrieval)|0|
|[A Passage-Level Reading Behavior Model for Mobile Search](https://doi.org/10.1145/3543507.3583343)|Zhijing Wu, Jiaxin Mao, Kedi Xu, Dandan Song, Heyan Huang|School of Computer Science, Carnegie Mellon University, USA; Gaoling School of Artificial Intelligence, Renmin University of China, China; School of Computer Science and Technology, Beijing Institute of Technology, China|Reading is a vital and complex cognitive activity during users’ information-seeking process. Several studies have focused on understanding users’ reading behavior in desktop search. Their findings greatly contribute to the design of information retrieval models. However, little is known about how users read a result in mobile search, although search currently happens more frequently in mobile scenarios. In this paper, we conduct a lab-based user study to investigate users’ fine-grained reading behavior patterns in mobile search. We find that users’ reading attention allocation is strongly affected by several behavior biases, such as position and selection biases. Inspired by these findings, we propose a probabilistic generative model, the Passage-level Reading behavior Model (PRM), to model users’ reading behavior in mobile search. The PRM utilizes observable passage-level exposure and viewport duration events to infer users’ unobserved skimming event, reading event, and satisfaction perception during the reading process. Besides fitting the passage-level reading behavior, we utilize the fitted parameters of PRM to estimate the passage-level and document-level relevance. Experimental results show that PRM outperforms existing unsupervised relevance estimation models. PRM has strong interpretability and provides valuable insights into the understanding of how users seek and perceive useful information in mobile search.|阅读是用户信息搜索过程中一种重要而复杂的认知活动。一些研究集中在了解用户在桌面搜索中的阅读行为。他们的发现极大地促进了信息检索模型的设计。然而，尽管目前搜索在移动场景中出现的频率更高，但人们对用户在移动搜索中如何阅读结果知之甚少。本文以实验室为基础，对移动搜索中用户的细粒度阅读行为模式进行了研究。研究发现，位置偏差、选择偏差等行为偏差对用户的阅读注意分配有显著影响。受这些发现的启发，我们提出了一个概率生成模型——短文水平阅读行为模型(PRM) ，来模拟用户在移动搜索中的阅读行为。PRM 利用可观察到的文章水平暴露和视窗持续时间事件来推断用户在阅读过程中未观察到的略读事件、阅读事件和满意度感知。除了拟合文章阅读行为外，我们还利用 PRM 的拟合参数来估计文章阅读水平和文档阅读水平的相关性。实验结果表明，PRM 算法的性能优于现有的无监督相关估计模型。PRM 具有很强的可解释性，为理解用户在移动搜索中如何寻找和感知有用信息提供了有价值的见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Passage-Level+Reading+Behavior+Model+for+Mobile+Search)|0|
|[PROD: Progressive Distillation for Dense Retrieval](https://doi.org/10.1145/3543507.3583421)|Zhenghao Lin, Yeyun Gong, Xiao Liu, Hang Zhang, Chen Lin, Anlei Dong, Jian Jiao, Jingwen Lu, Daxin Jiang, Rangan Majumder, Nan Duan|Microsoft Research Asia, China; Microsoft, China; School of Informatics, Xiamen University, China; Microsoft, USA|Knowledge distillation is an effective way to transfer knowledge from a strong teacher to an efficient student model. Ideally, we expect the better the teacher is, the better the student. However, this expectation does not always come true. It is common that a better teacher model results in a bad student via distillation due to the nonnegligible gap between teacher and student. To bridge the gap, we propose PROD, a PROgressive Distillation method, for dense retrieval. PROD consists of a teacher progressive distillation and a data progressive distillation to gradually improve the student. We conduct extensive experiments on five widely-used benchmarks, MS MARCO Passage, TREC Passage 19, TREC Document 19, MS MARCO Document and Natural Questions, where PROD achieves the state-of-the-art within the distillation methods for dense retrieval. The code and models will be released.|知识提取是将知识从一个强有力的教师转化为一个有效的学生模型的有效途径。理想情况下，我们期望老师越好，学生越好。然而，这种期望并不总是能够实现。由于教师与学生之间存在着不可忽视的差距，一个较好的教师模型常常通过精馏的方法导致不好的学生。为了填补这一空白，我们提出了一种逐步精馏方法 PROD，用于密集提取。PROD 由一个教师的逐步升华和一个数据的逐步升华组成，以逐步提高学生的素质。我们对五个广泛使用的基准进行了广泛的实验，即 MS MARCO Passage，TREC Passage 19，TREC Document 19，MS MARCO Document and Natural Questions，PROD 在这些基准上实现了用于密集检索的蒸馏方法的最先进水平。代码和模型将被发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PROD:+Progressive+Distillation+for+Dense+Retrieval)|0|
|[Ad Auction Design with Coupon-Dependent Conversion Rate in the Auto-bidding World](https://doi.org/10.1145/3543507.3583230)|Bonan Ni, Xun Wang, Qi Zhang, Pingzhong Tang, Zhourong Chen, Tianjiu Yin, Liangni Lu, Xiaobing Liu, Kewu Sun, Zhe Ma|ByteDance, China; Intelligent Science & Technology Academy of CASIC, China and Scientific Research Key Laboratory of Aerospace Defence Intelligent Systems and Technology, China; Institute for Interdisciplinary Information Sciences, Tsinghua University, China; TuringSense, China and Institute for Interdisciplinary Information Sciences, Tsinghua University, China|Online advertising has become a dominant source of revenue of the Internet. In classic auction theory, only the auctioneer (i.e., the platform) and buyers (i.e., the advertisers) are involved, while the advertising audiences are ignored. For ecommerce advertising, however, the platform can provide coupons for the advertising audiences and nudge them into purchasing more products at lower prices (e.g., 2 dollars off the regular price). Such promotions can lead to an increase in amount and value of purchases. In this paper, we jointly design the coupon value computation, slot allocation, and payment of online advertising in an auto-bidding world. Firstly, we propose the auction mechanism, named CFA-auction (i.e., Coupon-For-the-Audiences-auction), which takes advertising audiences into account in the auction design. We prove the existence of pacing equilibrium, and show that CFA-auction satisfies the IC (incentive compatibility), IR (individual rationality) constraints. Then, we study the optimality of CFA-auction, and prove it can maintain an approximation of the optimal. Finally, experimental evaluation results on both offline dataset as well as online A/B test demonstrate the effectiveness of CFA-auction.|在线广告已成为互联网收入的主要来源。在传统的拍卖理论中，只有拍卖商(即平台)和买家(即广告商)参与，而广告受众被忽略。然而，对于电子商务广告来说，这个平台可以为广告受众提供优惠券，推动他们以更低的价格购买更多的产品(例如，比正常价格低2美元)。此类促销活动可能导致购买数量和价值的增加。在这篇论文中，我们共同设计了自动竞价世界中的优惠券价值计算、时段分配和在线广告支付。首先，我们提出了一种拍卖机制，即 CFA 拍卖(即为受众提供优惠券的拍卖) ，该机制在拍卖设计中考虑了广告受众。我们证明了节奏均衡的存在，并且证明了 CFA 拍卖满足集成激励相容(IC)、个人理性(IR)约束。然后，我们研究了 CFA 拍卖的最优性，并证明了它可以保持最优的近似。最后，通过对离线数据集和在线 A/B 测试的实验结果验证了 CFA 拍卖的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ad+Auction+Design+with+Coupon-Dependent+Conversion+Rate+in+the+Auto-bidding+World)|0|
|[A Reference-Dependent Model for Web Search Evaluation: Understanding and Measuring the Experience of Boundedly Rational Users](https://doi.org/10.1145/3543507.3583551)|Nuo Chen, Jiqun Liu, Tetsuya Sakai|The University of Oklahoma, USA; Waseda University, Japan|Previous researches demonstrate that users’ actions in search interaction are associated with relative gains and losses to reference points, known as the reference dependence effect. However, this widely confirmed effect is not represented in most user models underpinning existing search evaluation metrics. In this study, we propose a new evaluation metric framework, namely Reference Dependent Metric (ReDeM), for assessing query-level search by incorporating the effect of reference dependence into the modelling of user search behavior. To test the overall effectiveness of the proposed framework, (1) we evaluate the performance, in terms of correlation with user satisfaction, of ReDeMs built upon different reference points against that of the widely-used metrics on three search datasets; (2) we examine the performance of ReDeMs under different task states, like task difficulty and task urgency; and (3) we analyze the statistical reliability of ReDeMs in terms of discriminative power. Experimental results indicate that: (1) ReDeMs integrated with a proper reference point achieve better correlations with user satisfaction than most of the existing metrics, like Discounted Cumulative Gain (DCG) and Rank-Biased Precision (RBP), even though their parameters have already been well-tuned; (2) ReDeMs reach relatively better performance compared to existing metrics when the task triggers a high-level cognitive load; (3) the discriminative power of ReDeMs is far stronger than Expected Reciprocal Rank (ERR), slightly stronger than Precision and similar to DCG, RBP and INST. To our knowledge, this study is the first to explicitly incorporate the reference dependence effect into the user browsing model and offline evaluation metrics. Our work illustrates a promising approach to leveraging the insights about user biases from cognitive psychology in better evaluating user search experience and enhancing user models.|以往的研究表明，用户在搜索交互中的行为与参考点的相对收益和相对损失有关，称为参考依赖效应。然而，这种被广泛证实的效果在大多数支持现有搜索评估指标的用户模型中并没有体现出来。在这项研究中，我们提出了一个新的评估度量框架，即参考依赖度量(ReDeM) ，通过将参考依赖的影响纳入用户搜索行为的建模来评估查询级搜索。为了测试提出的框架的整体有效性，(1)我们评估建立在不同参考点上的 ReDeM 与用户满意度的相关性，与三个搜索数据集上广泛使用的指标的相关性; (2)我们检查 ReDeM 在不同任务状态下的表现，如任务难度和任务紧迫性; (3)我们分析 ReDeM 在区分能力方面的统计可靠性。实验结果表明: (1)与合适的参考点相结合的 ReDeMs 与用户满意度的相关性优于大多数现有指标，如 DCG 和 RBP，尽管它们的参数已经得到了很好的调整; (2)当任务触发高水平的认知负荷时，与现有指标相比，ReDeMs 获得了相对更好的性能; (3) ReDeMs 的区分能力远远强于期望互惠秩序(ERR) ，略强于精度，类似于 DCG、 RBP 和 INST。据我们所知，这项研究是第一个明确地将参考依赖效应纳入用户浏览模型和离线评价指标。我们的工作说明了一种有前途的方法，利用认知心理学对用户偏见的洞察力，更好地评估用户搜索体验和增强用户模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Reference-Dependent+Model+for+Web+Search+Evaluation:+Understanding+and+Measuring+the+Experience+of+Boundedly+Rational+Users)|0|
|[Maximizing Submodular Functions for Recommendation in the Presence of Biases](https://doi.org/10.1145/3543507.3583195)|Anay Mehrotra, Nisheeth K. Vishnoi||Subset selection tasks, arise in recommendation systems and search engines and ask to select a subset of items that maximize the value for the user. The values of subsets often display diminishing returns, and hence, submodular functions have been used to model them. If the inputs defining the submodular function are known, then existing algorithms can be used. In many applications, however, inputs have been observed to have social biases that reduce the utility of the output subset. Hence, interventions to improve the utility are desired. Prior works focus on maximizing linear functions -- a special case of submodular functions -- and show that fairness constraint-based interventions can not only ensure proportional representation but also achieve near-optimal utility in the presence of biases. We study the maximization of a family of submodular functions that capture functions arising in the aforementioned applications. Our first result is that, unlike linear functions, constraint-based interventions cannot guarantee any constant fraction of the optimal utility for this family of submodular functions. Our second result is an algorithm for submodular maximization. The algorithm provably outputs subsets that have near-optimal utility for this family under mild assumptions and that proportionally represent items from each group. In empirical evaluation, with both synthetic and real-world data, we observe that this algorithm improves the utility of the output subset for this family of submodular functions over baselines.|子集选择任务，出现在推荐系统和搜索引擎，并要求选择一个子集的项目，最大限度地为用户的价值。子集的值经常显示报酬递减，因此，子模块函数被用来建模它们。如果定义子模函数的输入是已知的，那么可以使用现有的算法。然而，在许多应用中，输入已被观察到具有社会偏差，从而降低了输出子集的效用。因此，需要采取干预措施来改善效用。先前的研究集中在最大化线性函数(次模函数的一个特例) ，并且表明基于公平约束的干预不仅可以确保比例代表制，而且在存在偏差的情况下还可以实现接近最优的效用。我们研究了一类子模函数的最大化问题，这类子模函数捕获上述应用中出现的函数。我们的第一个结果是，与线性函数不同，基于约束的干预不能保证这个子模函数族的最优效用的任何常数部分。我们的第二个结果是一个次模最大化算法。该算法可证明输出的子集具有接近最优的效用为这个家庭在温和的假设和比例代表项目从每组。在实证评价中，我们观察到，无论是合成的还是真实的数据，这个算法都提高了这个子模函数族的输出子集在基线上的效用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Maximizing+Submodular+Functions+for+Recommendation+in+the+Presence+of+Biases)|0|
|[Facility Relocation Search For Good: When Facility Exposure Meets User Convenience](https://doi.org/10.1145/3543507.3583859)|Hui Luo, Zhifeng Bao, J. Shane Culpepper, Mingzhao Li, Yanchang Zhao|CSIRO, Australia; RMIT University, Australia|In this paper, we propose a novel facility relocation problem where facilities (and their services) are portable, which is a combinatorial search problem with many practical applications. Given a set of users, a set of existing facilities, and a set of potential sites, we decide which of the existing facilities to relocate to potential sites, such that two factors are satisfied: (1) facility exposure: facilities after relocation have balanced exposure, namely serving equivalent numbers of users; (2) user convenience: it is convenient for users to access the nearest facility, which provides services with shorter travel distance. This problem is motivated by applications such as dynamically redistributing vaccine resources to align supply with demand for different vaccination centers, and relocating the bike sharing sites daily to improve the transportation efficiency. We first prove that this problem is NP-hard, and then we propose two algorithms: a non-learning best response algorithm () and a reinforcement learning algorithm (). In particular, the best response algorithm finds a Nash equilibrium to balance the facility-related and the user-related goals. To avoid being confined to only one Nash equilibrium, as found in the method, we also propose the reinforcement learning algorithm for long-term benefits, where each facility is an agent and we determine whether a facility needs to be relocated or not. To verify the effectiveness of our methods, we adopt multiple metrics to evaluate not only our objective, but also several other facility exposure equity and user convenience metrics to understand the benefits after facility relocation. Finally, comprehensive experiments using real-world datasets provide insights into the effectiveness of the two algorithms in practice.|在本文中，我们提出了一个新的设施搬迁问题，其中设施(及其服务)是可移植的，这是一个组合搜索问题与许多实际应用。根据一组使用者、一组现有设施及一组可供选择的用地，我们会决定哪些现有设施须迁往可供选择的用地，以满足以下两个因素: (1)设施接触量: 迁移后的设施接触量均衡，即服务相等数目的使用者; (2)使用者方便: 使用者可方便地前往最近的设施，而该设施提供的服务距离较短。这个问题的动机是应用程序，如动态重新分配疫苗资源，以调整供应与不同的疫苗接种中心的需求，并重新安置自行车共享站点，以提高运输效率每天。我们首先证明了这个问题是 NP 难的，然后我们提出了两个算法: 非学习最佳响应算法()和强化学习算法()。特别是，最佳响应算法会找到一个平衡设施相关目标和用户相关目标的纳什均衡点。为了避免只局限于一个纳什均衡点，正如方法中所发现的那样，我们还提出了长期利益的强化学习算法，即每个设施都是一个代理人，我们决定是否需要重新安置一个设施。为了验证我们的方法的有效性，我们不仅采用了多个指标来评估我们的目标，而且还采用了其他一些设施暴露公平性和用户便利性指标来了解设施搬迁后的好处。最后，利用真实世界数据集进行综合实验，验证了这两种算法在实际应用中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Facility+Relocation+Search+For+Good:+When+Facility+Exposure+Meets+User+Convenience)|0|
|[Detecting and Limiting Negative User Experiences in Social Media Platforms](https://doi.org/10.1145/3543507.3583883)|Lluís Garcia Pueyo, Vinodh Kumar Sunkara, Prathyusha Senthil Kumar, Mohit Diwan, Qian Ge, Behrang Javaherian, Vasilis Verroios|Meta Platforms, Inc., USA|Item ranking is important to a social media platform’s success. The order in which posts, videos, messages, comments, ads, used products, notifications are presented to a user greatly affects the time spent on the platform, how often they visit it, how much they interact with each other, and the quantity and quality of the content they post. To this end, item ranking algorithms use models that predict the likelihood of different events, e.g., the user liking, sharing, commenting on a video, clicking/converting on an ad, or opening the platform’s app from a notification. Unfortunately, by solely relying on such event-prediction models, social media platforms tend to over optimize for short-term objectives and ignore the long-term effects. In this paper, we propose an approach that aims at improving item ranking long-term impact. The approach primarily relies on an ML model that predicts negative user experiences. The model utilizes all available UI events: the details of an action can reveal how positive or negative the user experience has been; for example, a user writing a lengthy report asking for a given video to be taken down, likely had a very negative experience. Furthermore, the model takes into account detected integrity (e.g., hostile speech or graphic violence) and quality (e.g., click or engagement bait) issues with the content. Note that those issues can be perceived very differently from different users. Therefore, developing a personalized model, where a prediction refers to a specific user for a specific piece of content at a specific point in time, is a fundamental design choice in our approach. Besides the personalized ML model, our approach consists of two more pieces: (a) the way the personalized model is integrated with an item ranking algorithm and (b) the metrics, methodology, and success criteria for the long term impact of detecting and limiting negative user experiences. Our evaluation process uses extensive A/B testing on the Facebook platform: we compare the impact of our approach in treatment groups against production control groups. The AB test results indicate a 5% to 50% reduction in hides, reports, and submitted feedback. Furthermore, we compare against a baseline that does not include some of the crucial elements of our approach: the comparison shows our approach has a 100x to 30x lower False Positive Ratio than a baseline. Lastly, we present the results from a large scale survey, where we observe a statistically significant improvement of 3 to 6 percent in users’ sentiment regarding content suffering from nudity, clickbait, false / misleading, witnessing-hate, and violence issues.|项目排名对社交媒体平台的成功至关重要。发布、视频、信息、评论、广告、二手产品、通知的顺序对用户在平台上花费的时间、访问频率、互动程度以及发布内容的数量和质量有很大影响。为此，项目排名算法使用模型来预测不同事件的可能性，例如，用户喜欢，分享，评论视频，点击/转换广告，或从通知中打开平台的应用程序。不幸的是，仅仅依靠这种事件预测模型，社交媒体平台往往会过度优化短期目标而忽视长期影响。在本文中，我们提出了一种方法，旨在提高项目排名的长期影响。这种方法主要依靠机器学习模型来预测负面的用户体验。该模型利用了所有可用的 UI 事件: 一个动作的细节可以揭示用户体验的积极或消极程度; 例如，一个用户写了一份长篇报告，要求删除一个给定的视频，很可能有一个非常消极的体验。此外，该模型还考虑了检测到的完整性(例如，敌意言论或暴力画面)和质量(例如，点击或参与诱饵)问题。请注意，这些问题可以从不同的用户看到非常不同。因此，开发个性化的模型，其中预测指的是在特定时间点的特定内容的特定用户，是我们方法中的一个基本设计选择。除了个性化机器学习模型，我们的方法还包括两个部分: (a)个性化模型与项目排序算法的整合方式; (b)检测和限制负面用户体验的长期影响的指标、方法和成功标准。我们的评估过程在 Facebook 平台上使用了广泛的 A/B 测试: 我们比较我们在治疗组和生产控制组中的方法的影响。AB 测试结果表明，皮革、报告和提交的反馈减少了5% 到50% 。此外，我们比较了不包括我们方法的一些关键要素的基线: 比较显示我们的方法比基线低100到30倍的错误阳性率。最后，我们展示了一项大规模调查的结果，我们观察到用户对于遭受裸露、点击诱饵、虚假/误导、目击仇恨和暴力问题的内容的情绪有3% 到6% 的统计显著改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+and+Limiting+Negative+User+Experiences+in+Social+Media+Platforms)|0|
|[On Detecting Policy-Related Political Ads: An Exploratory Analysis of Meta Ads in 2022 French Election](https://doi.org/10.1145/3543507.3583875)|Vera Sosnovik, Romaissa Kessi, Maximin Coavoux, Oana Goga|CNRS, France and LIG, Université Grenoble Alpes, Grenoble INP, France; CNRS, France and LIX, Inria, Ecole Polytechnique, Institut Polytechnique de Paris, France|Online political advertising has become the cornerstone of political campaigns. The budget spent solely on political advertising in the U.S. has increased by more than 100% from \$700 million during the 2017-2018 U.S. election cycle to \$1.6 billion during the 2020 U.S. presidential elections. Naturally, the capacity offered by online platforms to micro-target ads with political content has been worrying lawmakers, journalists, and online platforms, especially after the 2016 U.S. presidential election, where Cambridge Analytica has targeted voters with political ads congruent with their personality To curb such risks, both online platforms and regulators (through the DSA act proposed by the European Commission) have agreed that researchers, journalists, and civil society need to be able to scrutinize the political ads running on large online platforms. Consequently, online platforms such as Meta and Google have implemented Ad Libraries that contain information about all political ads running on their platforms. This is the first step on a long path. Due to the volume of available data, it is impossible to go through these ads manually, and we now need automated methods and tools to assist in the scrutiny of political ads. In this paper, we focus on political ads that are related to policy. Understanding which policies politicians or organizations promote and to whom is essential in determining dishonest representations. This paper proposes automated methods based on pre-trained models to classify ads in 14 main policy groups identified by the Comparative Agenda Project (CAP). We discuss several inherent challenges that arise. Finally, we analyze policy-related ads featured on Meta platforms during the 2022 French presidential elections period.|在线政治广告已经成为政治运动的基石。美国政治广告预算从2017-2018年美国大选期间的7亿美元增加到2020年美国总统大选期间的16亿美元，增幅超过100% 。自然，在线平台提供的针对政治内容的微观广告的能力一直令立法者、记者和在线平台感到担忧，尤其是在2016年美国总统大选之后，剑桥分析公司(Cambridge Analytica)针对选民的政治广告符合他们的个性。为了遏制这种风险，在线平台和监管机构(通过欧盟委员会提出的 DSA 法案)已经同意，研究人员、记者和公民社会需要能够审查在大型在线平台上运行的政治广告。因此，像 Meta 和 Google 这样的在线平台已经实现了广告库，其中包含了在其平台上运行的所有政治广告的信息。这是漫长道路上的第一步。由于可获得的数据量很大，手动浏览这些广告是不可能的，我们现在需要自动化的方法和工具来协助审查政治广告。本文主要研究与政策相关的政治广告。了解哪些政治家或组织提倡哪些政策，以及向谁提出这些政策，对于确定不诚实陈述至关重要。本文提出了一种基于预训练模型的广告自动分类方法，用于比较议程项目(CAP)确定的14个主要政策组的广告分类。我们将讨论出现的几个内在挑战。最后，我们分析了2022年法国总统大选期间 Meta 平台上的政策相关广告。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Detecting+Policy-Related+Political+Ads:+An+Exploratory+Analysis+of+Meta+Ads+in+2022+French+Election)|0|
|[A ML-based Approach for HTML-based Style Recommendation](https://doi.org/10.1145/3543873.3587300)|Ryan Aponte, Ryan A. Rossi, Shunan Guo, Jane Hoffswell, Nedim Lipka, Chang Xiao, Gromit YeukYin Chan, Eunyee Koh, Nesreen K. Ahmed|Intel Labs, USA; Adobe, USA; CMU, USA; Adobe Research, USA|Given a large corpus of HTML-based emails (or websites, posters, documents) collected from the web, how can we train a model capable of learning from such rich heterogeneous data for HTML-based style recommendation tasks such as recommending useful design styles or suggesting alternative HTML designs? To address this new learning task, we first decompose each HTML document in the corpus into a sequence of smaller HTML fragments where each fragment may consist of a set of HTML entities such as buttons, images, textual content (titles, paragraphs) and stylistic entities such as background-style, font-style, button-style, among others. From these HTML fragments, we then derive a single large heterogeneous hypergraph that captures the higher-order dependencies between HTML fragments and entities in such fragments, both within the same HTML document as well as across the HTML documents in the corpus. We then formulate this new HTML style recommendation task as a hypergraph representation learning problem and propose an approach to solve it. Our approach is able to learn effective low-dimensional representations of the higher-order fragments that consist of sets of heterogeneous entities as well as low-dimensional representations of the individual entities themselves. We demonstrate the effectiveness of the approach across several design style recommendation tasks. To the best of our knowledge, this work is the first to develop an ML-based model for the task of HTML-based email style recommendation.|鉴于从网上收集的大量基于 HTML 的电子邮件(或网站、海报、文档) ，我们如何才能培养一个模型，使其能够从这些丰富的异构数据中学习基于 HTML 的风格推荐任务，如推荐有用的设计风格或建议替代 HTML 设计？为了解决这个新的学习任务，我们首先将语料库中的每个 HTML 文档分解为一系列较小的 HTML 片段，其中每个片段可能包含一组 HTML 实体，如按钮、图像、文本内容(标题、段落)和风格实体，如背景风格、字体风格、按钮风格等。然后，从这些 HTML 片段中，我们得到一个单一的大型异构超图，它捕获这些片段中 HTML 片段和实体之间的高阶依赖关系，这些依赖关系既存在于同一 HTML 文档中，也存在于语料库中的 HTML 文档之间。然后将这个新的 HTML 风格推荐任务表示为一个超图表示学习问题，并提出了一种解决方法。我们的方法能够学习高阶片段的有效低维表示，这些片段由异构实体集合以及单个实体本身的低维表示组成。我们在几个设计风格的推荐任务中演示了该方法的有效性。据我们所知，这项工作是第一个开发基于机器学习的任务的基于 HTML 的电子邮件样式推荐模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+ML-based+Approach+for+HTML-based+Style+Recommendation)|0|
|[Graph-Level Embedding for Time-Evolving Graphs](https://doi.org/10.1145/3543873.3587299)|Lili Wang, Chenghan Huang, Xinyuan Cao, Weicheng Ma, Soroush Vosoughi|Georgia Institute of Technology, USA; Jefferies Financial Group LLC, USA; Dartmouth College, USA|Graph representation learning (also known as network embedding) has been extensively researched with varying levels of granularity, ranging from nodes to graphs. While most prior work in this area focuses on node-level representation, limited research has been conducted on graph-level embedding, particularly for dynamic or temporal networks. However, learning low-dimensional graph-level representations for dynamic networks is critical for various downstream graph retrieval tasks such as temporal graph similarity ranking, temporal graph isomorphism, and anomaly detection. In this paper, we present a novel method for temporal graph-level embedding that addresses this gap. Our approach involves constructing a multilayer graph and using a modified random walk with temporal backtracking to generate temporal contexts for the graph’s nodes. We then train a “document-level’’ language model on these contexts to generate graph-level embeddings. We evaluate our proposed model on five publicly available datasets for the task of temporal graph similarity ranking, and our model outperforms baseline methods. Our experimental results demonstrate the effectiveness of our method in generating graph-level embeddings for dynamic networks.|图表示学习(也称为网络嵌入)已经被广泛研究与不同的粒度级别，从节点到图。虽然该领域的大多数工作集中在节点级表示，但是对图级嵌入的研究很有限，特别是对于动态或时态网络。然而，学习动态网络的低维图级表示对于各种下游图检索任务(如时间图相似性排序、时间图同构和异常检测)至关重要。在本文中，我们提出了一种新的时间图级嵌入方法来解决这一问题。我们的方法包括构造一个多层图，并使用一个修改过的随机游走和时间回溯来为图的节点生成时间上下文。然后，我们在这些上下文上训练一个“文档级”语言模型来生成图级嵌入。我们评估了我们提出的模型在五个公开可用的数据集的时间图相似性排序的任务，我们的模型优于基线方法。实验结果表明了该方法在动态网络图级嵌入生成中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph-Level+Embedding+for+Time-Evolving+Graphs)|0|
|[SpotLight: Visual Insight Recommendation](https://doi.org/10.1145/3543873.3587302)|Camille Harris, Ryan A. Rossi, Sana Malik, Jane Hoffswell, Fan Du, Tak Yeon Lee, Eunyee Koh, Handong Zhao|Georgia Tech, USA; KAIST, Republic of Korea; Adobe Research, USA|Visualization recommendation systems make understanding data more accessible to users of all skill levels by automatically generating visualizations for users to explore. However, most existing visualization recommendation systems focus on ranking all possible visualizations based on the attributes or encodings, which makes it difficult to find the most relevant insights. We therefore introduce a novel class of insight-based visualization recommendation systems that automatically rank and recommend groups of related insights as well as the most important insights within each group. Our approach combines results from different learning-based methods to discover insights automatically and generalizes to a variety of attribute types (e.g., categorical, numerical, and temporal), including non-trivial combinations of these attribute types. To demonstrate the utility of this approach, we implemented a insight-centric visualization recommendation system, SpotLight, and conducted a user study with twelve participants, which showed that users are able to quickly find and understand relevant insights in unfamiliar data.|可视化推荐系统通过自动生成供用户探索的可视化，使所有技能水平的用户更容易理解数据。然而，现有的可视化推荐系统大多侧重于基于属性或编码对所有可能的可视化进行排序，这使得很难找到最相关的见解。因此，我们引入了一类新颖的基于洞察力的可视化推荐系统，该系统可以自动对相关洞察力以及每个组内最重要的洞察力进行排名和推荐。我们的方法结合了来自不同的基于学习的方法的结果，自动发现见解，并推广到各种属性类型(例如，分类，数字和时间) ，包括这些属性类型的非平凡组合。为了证明这种方法的实用性，我们实施了一个以洞察力为中心的可视化推荐系统 SpotLight，并对12名参与者进行了用户研究，结果显示用户能够快速找到并理解不熟悉数据中的相关见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SpotLight:+Visual+Insight+Recommendation)|0|
|[DataExpo: A One-Stop Dataset Service for Open Science Research](https://doi.org/10.1145/3543873.3587305)|Bin Lu, Lyuwen Wu, Lina Yang, Chenxing Sun, Wei Liu, Xiaoying Gan, Shiyu Liang, Luoyi Fu, Xinbing Wang, Chenghu Zhou|Shanghai Jiao Tong University, China; Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, China|The large volumes of data on the Internet provides new opportunities for scientific discovery, especially promoting data-driven open science research. However, due to lack of accurate semantic markups, finding relevant data is still difficult. To address this problem, we develop a one-stop dataset service called DataExpo and propose a deep learning method for automatic metadata ingestion. In this demo paper, we describe the system architecture, and how DataExpo facilitates dataset discovery, search and recommendation. Up till now, DataExpo has indexed over 960,000 datasets from more than 27,000 repositories in the context of Deep-time Digital Earth Program. Demo visitors can explore our service via https://dataexpo.acemap.info.|互联网上的大量数据为科学发现提供了新的机会，特别是促进了数据驱动的开放科学研究。然而，由于缺乏准确的语义标记，找到相关数据仍然很困难。为了解决这个问题，我们开发了一个名为 DataExpo 的一站式数据集服务，并提出了一种自动元数据摄取的深度学习方法。在本演示文章中，我们描述了系统的体系结构，以及 DataExpo 如何促进数据集的发现、搜索和推荐。到目前为止，数据博览会已经从超过27,000个数据库中索引了超过960,000个数据集。示范观众可透过 https://dataexpo.acemap.info 探索我们的服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DataExpo:+A+One-Stop+Dataset+Service+for+Open+Science+Research)|0|
|[Mirror: A Natural Language Interface for Data Querying, Summarization, and Visualization](https://doi.org/10.1145/3543873.3587309)|Canwen Xu, Julian J. McAuley, Penghan Wang|UC San Diego, USA; Cisco, USA|We present Mirror, an open-source platform for data exploration and analysis powered by large language models. Mirror offers an intuitive natural language interface for querying databases, and automatically generates executable SQL commands to retrieve relevant data and summarize it in natural language. In addition, users can preview and manually edit the generated SQL commands to ensure the accuracy of their queries. Mirror also generates visualizations to facilitate understanding of the data. Designed with flexibility and human input in mind, Mirror is suitable for both experienced data analysts and non-technical professionals looking to gain insights from their data.|我们介绍了一个基于大型语言模型的开源数据探索和分析平台—— Mirror。Mirror 为查询数据库提供了一个直观的自然语言界面，并自动生成可执行的 SQL 命令来检索相关数据并用自然语言对其进行汇总。此外，用户还可以预览和手动编辑生成的 SQL 命令，以确保查询的准确性。Mirror 还生成可视化，以便于理解数据。设计具有灵活性和人工输入的头脑，镜子是适合于有经验的数据分析师和非技术专业人士寻求获得洞察力从他们的数据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mirror:+A+Natural+Language+Interface+for+Data+Querying,+Summarization,+and+Visualization)|0|
|[Is the Impression Log Beneficial to Effective Model Training in News Recommender Systems? No, It's NOT](https://doi.org/10.1145/3543873.3587312)|Jeewon Ahn, HongKyun Bae, SangWook Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+the+Impression+Log+Beneficial+to+Effective+Model+Training+in+News+Recommender+Systems?+No,+It's+NOT)|0|
|[Incorporating Embedding to Topic Modeling for More Effective Short Text Analysis](https://doi.org/10.1145/3543873.3587316)|Junaid Rashid, Jungeun Kim, Usman Naseem|School of Computer Science, The University of Sydney, Sydney, Australia, Australia; Department of Data Science, Sejong University, Seoul, Republic of Korea, Republic of Korea; Department of Software, Kongju National University, Cheonan, Republic of Korea, Republic of Korea|With the growing abundance of short text content on websites, analyzing and comprehending these short texts has become a crucial task. Topic modeling is a widely used technique for analyzing short text documents and uncovering the underlying topics. However, traditional topic models face difficulties in accurately extracting topics from short texts due to limited content and their sparse nature. To address these issues, we propose an Embedding-based topic modeling (EmTM) approach that incorporates word embedding and hierarchical clustering to identify significant topics. Experimental results demonstrate the effectiveness of EmTM on two datasets comprising web short texts, Snippet and News. The results indicate a superiority of EmTM over baseline topic models by its exceptional performance in both classification accuracy and topic coherence metrics.|随着网站上短文内容的日益丰富，分析和理解这些短文已经成为一项重要的任务。主题建模是一种广泛使用的分析短文本文档和揭示潜在主题的技术。然而，传统的话题模型由于内容有限和稀疏的特点，很难准确地从短文中提取话题。为了解决这些问题，我们提出了一种基于嵌入的主题建模(EmTM)方法，该方法结合了单词嵌入和层次聚类来识别重要的主题。实验结果表明，该方法能够有效地处理包括网络短文本、片段和新闻在内的两个数据集。结果表明，与基线主题模型相比，EmTM 在分类精度和主题一致性度量方面具有优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Incorporating+Embedding+to+Topic+Modeling+for+More+Effective+Short+Text+Analysis)|0|
|[EnhancE: Enhanced Entity and Relation Embedding for Knowledge Hypergraph Link Prediction](https://doi.org/10.1145/3543873.3587326)|Chenxu Wang, Zhao Li, Xin Wang, Zirui Chen|Tianjin University, China|Knowledge Hypergraphs, as the generalization of knowledge graphs, have attracted increasingly widespread attention due to their friendly compatibility with real-world facts. However, link prediction in knowledge hypergraph is still an underexplored field despite the ubiquity of n-ary facts in the real world. Several recent representative embedding-based knowledge hypergraph link prediction methods have proven to be effective in a series of benchmarks, however, they only consider the position (or role) information, ignoring the neighborhood structure among entities and rich semantic information within each fact. To this end, we propose a model named EnhancE for effective link prediction in knowledge hypergraphs. On the one hand, a more expressive entity representation is obtained with both position and neighborhood information added to the initial embedding. On the other hand, rich semantic information of the involved entities within each tuple is incorporated into relation embedding for enhanced representation. Extensive experimental results over real datasets of both knowledge hypergraph and knowledge graph demonstrate the excellent performance of EnhancE compared with a variety of state-of-the-art baselines.|知识超图作为知识图的一种推广，由于其与现实世界事实的友好兼容性而引起了人们越来越广泛的关注。然而，尽管在现实世界中 n 元事实的普遍存在，知识超图中的链接预测仍然是一个未被充分探索的领域。最近几种有代表性的嵌入式知识超图链接预测方法已经被证明在一系列的基准测试中是有效的，但是它们只考虑位置(或角色)信息，忽略了实体间的邻域结构和每个事实中丰富的语义信息。为此，我们提出了一种基于知识超图的有效链接预测模型——增强 E。一方面，在初始嵌入的基础上加入位置信息和邻域信息，得到更具表现力的实体表示;。另一方面，每个元组中所涉及的实体的丰富语义信息被合并到关系嵌入中以增强表示。在知识超图和知识图的实际数据集上进行的大量实验结果表明，与各种最先进的基线相比，增强 E 具有优异的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EnhancE:+Enhanced+Entity+and+Relation+Embedding+for+Knowledge+Hypergraph+Link+Prediction)|0|
|[An Analogical Reasoning Method Based on Multi-task Learning with Relational Clustering](https://doi.org/10.1145/3543873.3587333)|Shuyi Li, Shaojuan Wu, Xiaowang Zhang, Zhiyong Feng|College of Intelligence and Computing, Tianjin University, China|Analogical QA task is a challenging natural language processing problem. When two word pairs are similar in their relationships, we refer to their relations as analogous. Although the analogy method based on word embedding is well developed, the analogy reasoning is far beyond this scope. At present, the methods based on pre-trained language models have explored only the tip of the iceberg. In this paper, we proposed a multi-task learning method for analogical QA task. First, we obtain word-pair representations by leveraging the output embeddings of the [MASK] token in the pre-trained language model. The representations are prepared for two tasks. The first task aims to train an analogical classifier by supervised learning. The second task is an auxiliary task based on relation clustering to generate relation pseudo-labels for word pairs and train relation classifier. Our method guides the model to analyze the relation similarity in analogical reasoning without relation labels. The experiments show that our method achieve excellent performance on four analogical reasoning datasets without the help of external corpus and knowledge. In the most difficult data set E-KAR, it has increased by at least 4%.|类比 QA 任务是一个具有挑战性的自然语言处理问题。当两个词对在关系上相似时，我们把它们的关系称为相似。基于嵌入词的类比推理方法虽然已经得到了很好的发展，但是类比推理远远超出了这个范围。目前，基于预训练语言模型的方法仅仅探索了冰山一角。本文提出了一种类比 QA 任务的多任务学习方法。首先，我们利用预训练语言模型中[ MASK ]令牌的输出嵌入获得词对表示。这些表示准备用于两个任务。第一个任务是通过监督式学习训练一个类比分类器。第二个任务是一个基于关系聚类的辅助任务，用于生成词对的关系伪标签和训练关系分类器。该方法引导模型分析无关联标签的类比推理中的关联相似性。实验结果表明，该方法在不借助外部语料库和知识的情况下，对四个类比推理数据集取得了良好的性能。在最困难的数据集 E-KAR 中，它至少增加了4% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Analogical+Reasoning+Method+Based+on+Multi-task+Learning+with+Relational+Clustering)|0|
|[Templet: A Collaborative System for Knowledge Graph Question Answering over Wikidata](https://doi.org/10.1145/3543873.3587335)|Francisca Suárez, Aidan Hogan|DCC, Universidad de Chile, Chile; DCC, Universidad de Chile, Chile and Instituto Milenio Fundamentos de los Datos (IMFD), Chile|We present Templet: an online question answering (QA) system for Wikidata. Templet is based on the collaboratively-edited repository QAWiki, which collects questions in multiple natural languages along with their corresponding structured queries. Templet generates templates from question–query pairs on QAWiki by replacing key entities with identifiers. Using autocompletion, the user can type a question in natural language, select a template, and again using autocompletion, select the entities they wish to insert into the template’s placeholders, generating a concrete question, query and results. The main objectives of Templet are: (i) to enable users to answer potentially complex questions over Wikidata using natural language templates and autocompletion; (ii) to encourage users to collaboratively create new templates via QAWiki, which in turn can benefit not only Templet, but other QA systems.|我们提出的模板: 一个在线问题回答(QA)系统的 Wikidata。Templet 基于协作编辑的存储库 QAWiki，该存储库用多种自然语言收集问题以及相应的结构化查询。Templet 通过使用标识符替换关键实体，从 QAWiki 上的问题-查询对生成模板。使用自动补全，用户可以用自然语言键入一个问题，选择一个模板，然后再次使用自动补全，选择他们希望插入到模板占位符中的实体，生成一个具体的问题、查询和结果。Templet 的主要目标是: (i)使用户能够通过使用自然语言模板和自动完成来回答 Wikidata 上潜在的复杂问题; (ii)鼓励用户通过 QAWiki 合作创建新的模板，这反过来不仅可以使 Templet 受益，还可以使其他 QA 系统受益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Templet:+A+Collaborative+System+for+Knowledge+Graph+Question+Answering+over+Wikidata)|0|
|[OptiRef: Query Optimization for Knowledge Bases](https://doi.org/10.1145/3543873.3587342)|Wafaa El Husseini, Cheikh Brahim El Vaigh, François Goasdoué, Hélène Jaudoin|Univ. Bourgogne, France; Univ. Rennes, France|Ontology-mediated query answering (OMQA) consists in asking database queries on a knowledge base (KB); a KB is a set of facts, the KB’s database, described by domain knowledge, the KB’s ontology. FOL-rewritability is the main OMQA technique: it reformulates a query w.r.t. the KB’s ontology so that the evaluation of the reformulated query on the KB’s database computes the correct answers. However, because this technique embeds the domain knowledge relevant to the query into the reformulated query, a reformulated query may be complex and its optimization is the crux of efficiency. We showcase OptiRef that implements a novel, general optimization framework for efficient query answering on datalog ±, description logic, existential rules, OWL and RDF/S KBs. OptiRef optimizes reformulated queries by rapidly computing, based on a KB’s database summary, simpler (contained) queries with the same answers. We demonstrate OptiRef’s effectiveness on well-established benchmarks: performance is significantly improved in general, up to several orders of magnitude in the best cases!|本体介导的查询回答(OMQA)包括在知识库(KB)上询问数据库查询; 知识库是一组事实，知识库的数据库，由领域知识描述，知识库的本体。FOL 可重写性是主要的 OMQA 技术: 它重新规范查询 w.r.t. 知识库的本体，以便在知识库的数据库上计算重新规范查询的正确答案。然而，由于这种技术将与查询相关的领域知识嵌入到重构查询中，因此重构查询可能比较复杂，其优化是提高查询效率的关键。我们展示了 OptiRef，它实现了一个新颖的、通用的优化框架，用于在数据目录 ± 、描述逻辑、存在规则、 OWL 和 RDF/S 知识库上进行有效的查询应答。OptiRef 通过快速计算优化重新配置的查询，基于知识库的数据库摘要，使用具有相同答案的更简单(包含)的查询。我们展示了 OptiRef 在完善的基准测试上的有效性: 性能总体上得到了显著的改善，在最好的情况下达到了几个数量级！|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OptiRef:+Query+Optimization+for+Knowledge+Bases)|0|
|[Learning Topical Structured Interfaces from Medical Research Literature](https://doi.org/10.1145/3543873.3587353)|Maitry Chauhan, Anna Pyayt, Michael N. Gubanov|University of South Florida, USA; Florida State University, USA|Accessing large-scale structured datasets such as WDC or CORD-191 is very challenging. Even if one topic (e.g. COVID-19 vaccine efficacy) is of interest, all topical tables in different sources/papers have hundreds of different schemas, depending on the authors, which significantly complicates both finding and querying them. Here we demonstrate a scalable Meta-profiler system, capable of constructing a structured standardized interface to a topic of interest in large-scale (semi-)structured datasets. This interface, that we call Meta-profile represents a multi-dimensional meta-data summary for a selected topic of interest, accumulating all differently structured representations of the topical tables in the dataset. Such Meta-profiles can be used as a rich visualization as well as a robust structural query interface simplifying access to large-scale (semi-)structured data for different user segments, such as data scientists and end users.|访问大规模的结构化数据集，如 WDC 或 CORD-191是非常具有挑战性的。即使一个主题(例如2019冠状病毒疾病疫苗效力)是有趣的，不同来源/论文中的所有主题表格都有数百种不同的模式，这取决于作者，这使得查找和查询这些模式变得非常复杂。在这里，我们演示了一个可伸缩的元分析器系统，它能够为大规模(半)结构化数据集中感兴趣的主题构建一个结构化的标准化接口。我们称之为 Meta-profile 的这个接口表示一个选定主题的多维元数据摘要，它积累了数据集中主题表的所有不同结构的表示。这样的元概要文件可以用作丰富的可视化以及健壮的结构化查询界面，简化不同用户段(如数据科学家和最终用户)对大规模(半)结构化数据的访问。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Topical+Structured+Interfaces+from+Medical+Research+Literature)|0|
|[DGBCT: A Scalable Distributed Gradient Boosting Causal Tree at Alipay](https://doi.org/10.1145/3543873.3584645)|Jun Zhou, Caizhi Tang, Qing Cui, Yi Ding, Longfei Li, Fei Wu|Ant Group, China; College of Computer Science and Technology, Zhejiang University, China and Ant Group, China; College of Computer Science and Technology, Zhejiang University, China|Causal effect estimation has been increasingly emphasized in the past few years. To handle this problem, tree-based causal methods have been widely used due to their robustness and explainability. However, most of the existing methods are limited to running on a single machine, making it difficult to scale up to hundreds of millions of data in typical industrial scenarios. This paper proposes DGBCT, a Distributed Gradient Boosting Causal Tree to tackle such problem, and the contribution of this paper is three folds. First, we extend the original GBCT method to a multi-treatment setting and take the monotonic constraints into consideration, so that more typical industrial necessities can be resolved with our framework. Moreover, we implement DGBCT based on the ‘Controller-Coordinator-Worker’ framework, in which dual failover mechanism is achieved, and commendable flexibility is ensured. In addition, empirical results show that DGBCT significantly outperforms the state-of-the-art causal trees, and has a near-linear speedup as the number of workers grows. The system is currently deployed in Alipay1 to support the daily business tasks that involve hundreds of millions of users.|因果效应估计在过去的几年中越来越受到重视。为了解决这个问题，基于树的因果关系方法由于其鲁棒性和可解释性而得到了广泛的应用。然而，现有的大多数方法仅限于在单台机器上运行，因此在典型的工业场景中很难扩展到数亿个数据。本文提出了分布式梯度提升因果树 DGBCT 来解决这个问题，本文的贡献有三个方面。首先，我们将原来的 GBCT 方法扩展到一个多处理环境，并且考虑了单调约束，使得我们的框架能够解决更多的典型工业需求。此外，我们在“控制器-协调器-工作者”框架的基础上实现了 DGBCT，实现了双重故障转移机制，并保证了值得称赞的灵活性。此外，实证结果显示，DGBCT 的表现明显优于最先进的因果树，并且随着工作人员数量的增加有近线性的加速效应。该系统目前部署在支付宝1中，以支持涉及数亿用户的日常业务任务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DGBCT:+A+Scalable+Distributed+Gradient+Boosting+Causal+Tree+at+Alipay)|0|
|[What Image do You Need? A Two-stage Framework for Image Selection in E-commerce](https://doi.org/10.1145/3543873.3584646)|Sheng You, Chao Wang, Baohua Wu, Jingping Liu, Quan Lu, Guanzhou Han, Yanghua Xiao|East China University of Science and Technology, China; Alibaba Group, China; Fudan University, China; Shanghai University, China|In e-commerce, images are widely used to display more intuitive information about items. Image selection significantly affects the user’s click-through rate (CTR). Most existing work considers the CTR as the target to find an appropriate image. However, these methods are challenging to deploy online efficiently. Also, the selected images may not relate to the item but are profitable to CTR, resulting in the undesirable phenomenon of enticing users to click on the item. To address these issues, we propose a novel two-stage pipeline method with content-based recall model and CTR-based ranking model. The first is realized as a joint method based on the title-image matching model and multi-modal knowledge graph embedding learning model. The second is a CTR-based visually aware scoring model, incorporating the entity textual information and entity images. Experimental results show the effectiveness and efficiency of our method in offline evaluations. After a month of online A/B testing on a travel platform Fliggy, the relative improvement of our method is 5% with respect to seller selection on CTCVR in the searching scenario, and our method further improves pCTR from 3.48% of human pick to 3.53% in the recommendation scenario.|在电子商务中，图像被广泛用于显示更直观的商品信息。图像选择会显著影响用户的点进率。大多数现有的工作都将 CTR 作为寻找合适图像的目标。然而，这些方法在线有效部署是具有挑战性的。此外，所选图像可能与该项目无关，但有利于点击率，导致不良现象的诱惑用户点击该项目。为了解决这些问题，我们提出了一种新的基于内容的召回模型和基于点击率的排序模型的两阶段流水线方法。第一种是基于标题-图像匹配模型和多模态知识图嵌入学习模型的联合方法。第二种是基于 CTR 的视觉感知评分模型，结合了实体文本信息和实体图像。实验结果表明了该方法在离线评估中的有效性和有效性。在旅游平台 Fliggy 上进行了一个月的在线 A/B 测试后，在搜索场景中，我们的方法相对于 CTCVR 上的卖方选择的相对改善为5% ，并且我们的方法进一步将 pCTR 从人类选择的3.48% 提高到推荐场景中的3.53% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What+Image+do+You+Need?+A+Two-stage+Framework+for+Image+Selection+in+E-commerce)|0|
|[Learning Geolocation by Accurately Matching Customer Addresses via Graph based Active Learning](https://doi.org/10.1145/3543873.3584647)|Saket Maheshwary, Saurabh Sohoney|Amazon, India|We propose a novel adaptation of graph-based active learning for customer address resolution or de-duplication, with the aim to determine if two addresses represent the same physical building or not. For delivery systems, improving address resolution positively impacts multiple downstream systems such as geocoding, route planning and delivery time estimations, leading to an efficient and reliable delivery experience, both for customers as well as delivery agents. Our proposed approach jointly leverages address text, past delivery information and concepts from graph theory to retrieve informative and diverse record pairs to label. We empirically show the effectiveness of our approach on manually curated dataset across addresses from India (IN) and United Arab Emirates (UAE). We achieved absolute improvement in recall on average across IN and UAE while preserving precision over the existing production system. We also introduce delivery point (DP) geocode learning for cold-start addresses as a downstream application of address resolution. In addition to offline evaluation, we also performed online A/B experiments which show that when the production model is augmented with active learnt record pairs, the delivery precision improved by and delivery defects reduced by on an average across shipments from IN and UAE.|我们提出了一种新的基于图的主动学习的客户地址解析或去重复，目的是确定是否两个地址代表相同的物理建筑物。对于送货系统，提高地址分辨率会对多个下游系统产生积极影响，例如地理编码、路径规划和送货时间估计，从而为客户和送货代理带来高效和可靠的送货体验。我们提出的方法共同利用地址文本，过去的传递信息和概念从图论检索信息和不同的记录对标签。我们通过实验证明了我们的方法在人工管理来自印度(IN)和阿拉伯联合酋长国(UAE)的数据集方面的有效性。在保持现有生产系统精度的同时，我们在 IN 和阿联酋的平均召回率上取得了绝对的提高。我们还介绍了用于冷启动地址的交付点(DP)地理编码学习作为地址解析的下游应用。除了离线评估之外，我们还进行了在线 A/B 实验，结果表明，当生产模型增加了主动学习记录对时，从 IN 和阿联酋发货的交付精度提高了，交付缺陷平均减少了。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Geolocation+by+Accurately+Matching+Customer+Addresses+via+Graph+based+Active+Learning)|0|
|[CAViaR: Context Aware Video Recommendations](https://doi.org/10.1145/3543873.3584658)|Khushhall Chandra Mahajan, Aditya Palnitkar, Ameya Raul, Brad Schumitsch|Meta Inc., USA|Many recommendation systems rely on point-wise models, which score items individually. However, point-wise models generating scores for a video are unable to account for other videos being recommended in a query. Due to this, diversity has to be introduced through the application of heuristic-based rules, which are not able to capture user preferences, or make balanced trade-offs in terms of diversity and item relevance. In this paper, we propose a novel method which introduces diversity by modeling the impact of low diversity on user's engagement on individual items, thus being able to account for both diversity and relevance to adjust item scores. The proposed method is designed to be easily pluggable into existing large-scale recommender systems, while introducing minimal changes in the recommendations stack. Our models show significant improvements in offline metrics based on the normalized cross entropy loss compared to production point-wise models. Our approach also shows a substantial increase of 1.7% in topline engagements coupled with a 1.5% increase in daily active users in an A/B test with live traffic on Facebook Watch, which translates into an increase of millions in the number of daily active users for the product.|许多推荐系统依赖于逐点模型，这种模型对项目进行单独评分。然而，为视频生成分数的点模型无法解释在查询中推荐的其他视频。因此，必须通过应用启发式规则来引入多样性，这些规则不能捕捉用户的偏好，也不能在多样性和项目相关性方面做出平衡的权衡。本文提出了一种引入多样性的新方法，该方法通过建立低多样性对用户参与个别项目的影响模型，从而能够同时考虑多样性和相关性来调整项目得分。所提出的方法被设计成可以很容易地插入到现有的大规模推荐系统中，同时在推荐堆栈中引入最小的变化。与生产点模型相比，我们的模型显示了基于归一化交叉熵损失的离线指标的显著改进。我们的方法还显示，顶线参与度大幅增加了1.7% ，在 Facebook Watch 上的实时流量的 A/B 测试中，每日活跃用户增加了1.5% ，这意味着产品的每日活跃用户数量增加了数百万。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CAViaR:+Context+Aware+Video+Recommendations)|0|
|[Towards Building a Mobile App for People on the Spectrum](https://doi.org/10.1145/3543873.3587533)|Victoria Firsanova|Department of Mathematical Linguistics, Saint Petersburg State University, Russian Federation|The inclusion of autistic people can be augmented by a mobile app that provides information without a human mediator making information perception more liberating for people in the spectrum. This paper is an overview of a doctoral work dedicated to the development of a web-based mobile tool for supporting the inclusion of people on the autism spectrum. The work includes UX/UI research conducted with psychiatry experts, web information retrieval study and neural question-answering research. Currently, the study results comprise several mobile app layouts, a retriever-reader model design and fine-tuned neural network for extractive question-answering. Source code and other resources are available at https://github.com/vifirsanova/empi.|自闭症患者的包容性可以通过移动应用程序得到加强，这个程序可以在没有人类中介的情况下提供信息，从而使自闭症患者的信息感知更加自由。这篇文章是一篇博士论文的概述，该论文致力于开发一种基于网络的移动工具，以支持孤独症患者的融入。这项工作包括由精神病学专家进行的用户体验/用户界面研究、网络信息检索研究和神经问答研究。目前，研究结果包括几个移动应用程序的布局，一个检索-阅读器模型设计和微调神经网络提取问题回答。源代码和其他资源可在 https://github.com/vifirsanova/empi 获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Building+a+Mobile+App+for+People+on+the+Spectrum)|0|
|[Multi-turn mediated solutions for Conversational Artificial Intelligent systems leveraging graph-based techniques](https://doi.org/10.1145/3543873.3587540)|Riya Naik|Computer Science & Information Systems, Birla Institute Of Technology And Science, Pilani, India|The current era is dominated by intelligent Question Answering (QA) systems that can instantly answer almost all their questions, saving users search time and increasing the throughput and precision in the applied domain. A vast amount of work is being carried out in QA systems to deliver better content satisfying users’ information needs [2]. Since QA systems are ascending the cycle of emerging technologies, there are potential research gaps that can be explored. QA systems form a significant part of Conversational Artificial Intelligent systems giving rise to a new research pathway, i.e., Conversational Question Answering (CQA) systems [32]. We propose to design and develop a CQA system leveraging Hypergraph-based techniques. The approach focuses on the multi-turn conversation and multi-context to gauge users’ exact information needs and deliver better answers. We further aim to address "supporting evidence-based retrieval" for fact-based responsible answer generation. Since the QA system requires a large amount of data and processing, we also intend to investigate hardware performance for effective system utilization.|当今时代的主流是智能问答(QA)系统，它可以即时回答几乎所有的问题，节省用户搜索时间，提高应用领域的吞吐量和精度。为了提供更好的内容以满足用户的信息需求，QA 系统正在进行大量的工作[2]。由于 QA 系统正在提升新兴技术的周期，因此存在可以探索的潜在研究差距。问答系统构成了会话人工智能系统的重要组成部分，从而产生了一种新的研究途径，即会话问答(CQA)系统[32]。我们建议利用基于 Hypergraph 的技术设计和开发一个 CQA 系统。该方法侧重于多回合会话和多上下文，以衡量用户的确切信息需求和提供更好的答案。我们进一步的目标是解决“支持基于证据的检索”的事实为基础的负责任的答案生成。由于 QA 系统需要大量的数据和处理，因此我们还打算研究硬件性能，以便有效地利用系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-turn+mediated+solutions+for+Conversational+Artificial+Intelligent+systems+leveraging+graph-based+techniques)|0|
|[Graph and Embedding based Approach for Text Clustering: Topic Detection in a Large Multilingual Public Consultation](https://doi.org/10.1145/3543873.3587627)|Nicolas Stefanovitch, Guillaume Jacquet, Bertrand De Longueville|European Commission - Joint Research Centre, Italy|We present a novel algorithm for multilingual text clustering built upon two well studied techniques: multilingual aligned embedding and community detection in graphs. The aim of our algorithm is to discover underlying topics in a multilingual dataset using clustering. We present both a numerical evaluation using silhouette and V-measure metrics, and a qualitative evaluation for which we propose a new systematic approach. Our algorithm presents robust overall performance and its results were empirically evaluated by an analyst. The work we present was done in the context of a large multilingual public consultation, for which our new algorithm was deployed and used on a daily basis.|本文提出了一种新的多语言文本聚类算法，该算法基于两种已经得到广泛研究的技术: 多语言对齐嵌入和图中的社区检测。我们的算法的目的是使用聚类来发现多语言数据集中的基本主题。我们提出了一个数值评估使用轮廓和 V 测量度量，和一个定性的评估，我们提出了一个新的系统方法。我们的算法提出了稳健的整体性能，其结果是经验性的分析评价。我们介绍的工作是在大规模多语种公众协商的背景下完成的，我们的新算法每天都得到部署和使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+and+Embedding+based+Approach+for+Text+Clustering:+Topic+Detection+in+a+Large+Multilingual+Public+Consultation)|0|
|[Dual-grained Text-Image Olfactory Matching Model with Mutual Promotion Stages](https://doi.org/10.1145/3543873.3587649)|Yi Shao, Jiande Sun, Ye Jiang, Jing Li|Qingdao University of Science and Technology, China; Shandong Management University, China; Shandong Normal University, China|Olfactory experience has great advantages in awakening human memories and emotions, which may even surpass vision in some cases. Studies have proved that olfactory scene descriptions in images and text content can also arouse human olfactory imagination, but there are still few studies on solving related problems from the perspective of computer vision and NLP. This paper proposes a multimodal model that can detect similar olfactory experience in paired text-image samples. The model builds two stages, coarse-grained and fine-grained. The model adopts the feature fusion method based on pre-trained CLIP for coarse-grained matching training to obtain a preliminary feature extractor to promote fine-grained matching training, and then uses the similarity calculation method based on stacked cross attention for fine-grained matching training to obtain the final feature extractor which in turn promotes coarse-grained matching training. Finally, we manually build an approximate olfactory nouns list during fine-grained matching training, which not only yields significantly better performance when fed back to the fine-grained matching process, but this noun list can be used for future research. Experiments on the MUSTI task dataset of MediaEval2022 prove that the coarse-grained and fine-grained matching stages in proposed model both perform well, and both F1 measures exceed the existing baseline models.|嗅觉经验在唤醒人类记忆和情感方面有很大的优势，在某些情况下甚至可能超越视觉。研究表明，图像和文本内容中的嗅觉场景描述也能激发人类的嗅觉想象，但从计算机视觉和自然语言处理的角度解决相关问题的研究还很少。本文提出了一个多模态模型，可以检测相似的嗅觉经验配对文本图像样本。该模型分为粗粒度和细粒度两个阶段。该模型采用基于预训练 CLIP 的特征融合方法进行粗粒度匹配训练，得到初步的特征提取器以促进细粒度匹配训练，然后采用基于叠加交叉注意的相似度计算方法进行细粒度匹配训练，得到最终的特征提取器以促进粗粒度匹配训练。最后，我们在细粒度匹配训练过程中手动构建了一个近似的嗅觉名词列表，这不仅在反馈到细粒度匹配过程时产生了明显更好的性能，而且这个名词列表可以用于未来的研究。在 MediaEval2022的 MUSTI 任务数据集上的实验证明，所提出的模型中的粗粒度和细粒度匹配阶段都表现良好，而且两种 F1测度都超过了现有的基线模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual-grained+Text-Image+Olfactory+Matching+Model+with+Mutual+Promotion+Stages)|0|
|[MEMER - Multimodal Encoder for Multi-signal Early-stage Recommendations](https://doi.org/10.1145/3543873.3587679)|Mohit Agarwal, Srijan Saket, Rishabh Mehrotra|ShareChat, India|Millions of content gets created daily on platforms like YouTube, Facebook, TikTok etc. Most of such large scale recommender systems are data demanding, thus taking substantial time for content embedding to mature. This problem is aggravated when there is no behavioral data available for new content. Poor quality recommendation for these items lead to user dissatisfaction and short content shelf-life. In this paper we propose a solution MEMER (Multimodal Encoder for Multi-signal Early-stage Recommendations), that utilises the multimodal semantic information of content and uses it to generate better quality embeddings for early-stage items. We demonstrate the flexibility of the framework by extending it to various explicit and implicit user actions. Using these learnt embeddings, we conduct offline and online experiments to verify its effectiveness. The predicted embeddings show significant gains in online early-stage experiments for both videos and images (videos: 44% relative gain in click through rate, 46% relative gain in explicit engagements, 9% relative gain in successful video play, 20% relative reduction in skips, images: 56% relative gain in explicit engagements). This also compares well against the performance of mature embeddings (83.3% RelaImpr (RI) [18] in Successful Video Play, 97.8% RelaImpr in Clicks).|每天都有数以百万计的内容在 YouTube、 Facebook、 TikTok 等平台上被创造出来。大多数这样的大规模推荐系统都需要大量的数据，因此内容嵌入需要大量的时间才能成熟。当没有可用于新内容的行为数据时，这个问题就更加严重了。这些项目的低质量推荐导致用户不满意和内容保质期短。在本文中，我们提出了一个解决方案 MEMER (多信号早期推荐的多模式编码器) ，利用多模式内容语义信息，并使用它为早期项目产生更好的质量嵌入。我们通过将框架扩展到各种显式和隐式用户操作来演示框架的灵活性。使用这些学习嵌入，我们进行离线和在线实验，以验证其有效性。预测的嵌入在视频和图像的在线早期实验中都显示出显著的增益(视频: 点击率相对增益44% ，显性参与相对增益46% ，成功视频播放相对增益9% ，跳过相对减少20% ，图像: 显性参与相对增益56%)。这也与成熟嵌入的性能相当(83.3% RelaImpr (RI)[18]在成功的视频播放，97.8% RelaImpr 在点击)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MEMER+-+Multimodal+Encoder+for+Multi-signal+Early-stage+Recommendations)|0|
|[Social Re-Identification Assisted RTO Detection for E-Commerce](https://doi.org/10.1145/3543873.3587620)|Hitkul Jangra, Abinaya K, Soham Saha, Satyajit Banerjee, Muthusamy Chelliah, Ponnurangam Kumaraguru|IIIT Hyderabad, India; Flipkart, India; IIIT Delhi, India|E-commerce features like easy cancellations, returns, and refunds can be exploited by bad actors or uninformed customers, leading to revenue loss for organization. One such problem faced by e-commerce platforms is Return To Origin (RTO), where the user cancels an order while it is in transit for delivery. In such a scenario platform faces logistics and opportunity costs. Traditionally, models trained on historical trends are used to predict the propensity of an order becoming RTO. Sociology literature has highlighted clear correlations between socio-economic indicators and users’ tendency to exploit systems to gain financial advantage. Social media profiles have information about location, education, and profession which have been shown to be an estimator of socio-economic condition. We believe combining social media data with e-commerce information can lead to improvements in a variety of tasks like RTO, recommendation, fraud detection, and credit modeling. In our proposed system, we find the public social profile of an e-commerce user and extract socio-economic features. Internal data fused with extracted social features are used to train a RTO order detection model. Our system demonstrates a performance improvement in RTO detection of 3.1% and 19.9% on precision and recall, respectively. Our system directly impacts the bottom line revenue and shows the applicability of social re-identification in e-commerce.|电子商务的特点，如容易取消，退货和退款可以利用不良行为者或不知情的客户，导致收入损失的组织。电子商务平台面临的一个这样的问题是返还原产地(RTO) ，即用户在运输途中取消订单。在这种情况下，平台面临物流和机会成本。传统上，根据历史趋势训练的模型被用来预测订单成为 RTO 的倾向。社会学文献强调了社会经济指标与用户利用系统获取金融优势的倾向之间的明确相关性。社交媒体档案包含有关地理位置、教育和职业的信息，这些信息已被证明是社会经济状况的估计值。我们相信，将社交媒体数据与电子商务信息结合起来，可以改进诸如 RTO、推荐、欺诈检测和信用建模等多种任务。在我们提出的系统中，我们找到电子商务用户的公共社会轮廓，并提取社会经济特征。利用内部数据与提取的社会特征融合，建立了 RTO 订单检测模型。我们的系统在检测准确率召回率方面的性能改善分别为3.1% 和19.9% 。我们的系统直接影响到底线收入，显示了社会再认同在电子商务中的适用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Social+Re-Identification+Assisted+RTO+Detection+for+E-Commerce)|0|
|[Contextual Response Interpretation for Automated Structured Interviews: A Case Study in Market Research](https://doi.org/10.1145/3543873.3587657)|Harshita Sahijwani, Kaustubh D. Dhole, Ankur P. Purwar, Venugopal Vasudevan, Eugene Agichtein|Procter & Gamble, USA; Emory University, USA; Procter & Gamble, Singapore|Structured interviews are used in many settings, importantly in market research on topics such as brand perception, customer habits, or preferences, which are critical to product development, marketing, and e-commerce at large. Such interviews generally consist of a series of questions that are asked to a participant. These interviews are typically conducted by skilled interviewers, who interpret the responses from the participants and can adapt the interview accordingly. Using automated conversational agents to conduct such interviews would enable reaching a much larger and potentially more diverse group of participants than currently possible. However, the technical challenges involved in building such a conversational system are relatively unexplored. To learn more about these challenges, we convert a market research multiple-choice questionnaire to a conversational format and conduct a user study. We address the key task of conducting structured interviews, namely interpreting the participant's response, for example, by matching it to one or more predefined options. Our findings can be applied to improve response interpretation for the information elicitation phase of conversational recommender systems.|结构化访谈用于许多场合，重要的是用于市场调查，如品牌认知、客户习惯或偏好，这些对产品开发、市场营销和电子商务至关重要。这种面试通常包括向参与者提出的一系列问题。这些面试通常由技术熟练的面试官进行，他们解释参与者的回答，并能相应地调整面试。使用自动对话代理进行这种访谈将能够接触到比目前可能的更多、可能更多样化的参与者群体。然而，构建这样一个会话系统所涉及的技术挑战相对而言还没有得到探索。为了更多地了解这些挑战，我们将市场调查多项选择问卷转换为会话形式，并进行用户研究。我们解决的关键任务是进行结构化访谈，即解释参与者的反应，例如，通过匹配一个或多个预先定义的选项。我们的研究结果可以用于改善会话推荐系统中信息激发阶段的响应解释。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contextual+Response+Interpretation+for+Automated+Structured+Interviews:+A+Case+Study+in+Market+Research)|0|
|[Knowledge Graph-Enhanced Neural Query Rewriting](https://doi.org/10.1145/3543873.3587678)|Shahla Farzana, Qunzhi Zhou, Petar Ristoski|eBay Inc, USA; University of Illinois Chicago, USA; eBay Inc., USA|The main task of an e-commerce search engine is to semantically match the user query to the product inventory and retrieve the most relevant items that match the user’s intent. This task is not trivial as often there can be a mismatch between the user’s intent and the product inventory for various reasons, the most prevalent being: (i) the buyers and sellers use different vocabularies, which leads to a mismatch; (ii) the inventory doesn’t contain products that match the user’s intent. To build a successful e-commerce platform it is of paramount importance to be able to address both of these challenges. To do so, query rewriting approaches are used, which try to bridge the semantic gap between the user’s intent and the available product inventory. Such approaches use a combination of query token dropping, replacement and expansion. In this work we introduce a novel Knowledge Graph-enhanced neural query rewriting in the e-commerce domain. We use a relationship-rich product Knowledge Graph to infuse auxiliary knowledge in a transformer-based query rewriting deep neural network. Experiments on two tasks, query pruning and complete query rewriting, show that our proposed approach significantly outperforms a baseline BERT-based query rewriting solution.|电子商务搜索引擎的主要任务是在语义上将用户查询与产品库存相匹配，并检索与用户意图相匹配的最相关项目。这个任务并不是微不足道的，因为在用户的意图和产品库存之间经常会因为各种原因而产生不匹配，最普遍的原因是: (i)买家和卖家使用不同的词汇，这会导致不匹配; (ii)库存不包含符合用户意图的产品。要建立一个成功的电子商务平台，最重要的是能够应对这两个挑战。为此，使用了查询重写方法，这些方法试图弥合用户意图和可用产品目录之间的语义差距。这种方法结合使用查询标记删除、替换和扩展。在这项工作中，我们介绍了一种新的知识图增强神经查询重写在电子商务领域。在基于变压器的查询重写深度神经网络中，我们使用一个关系丰富的产品知识图来注入辅助知识。通过对查询裁剪和完全查询重写两个任务的实验表明，该方法的性能明显优于基于 BERT 的基线查询重写方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Graph-Enhanced+Neural+Query+Rewriting)|0|
|[Fairness-aware Differentially Private Collaborative Filtering](https://doi.org/10.1145/3543873.3587577)|Zhenhuan Yang, Yingqiang Ge, Congzhe Su, Dingxian Wang, Xiaoting Zhao, Yiming Ying|University at Albany, SUNY, USA; Etsy, USA; Rutgers University, USA|Recently, there has been an increasing adoption of differential privacy guided algorithms for privacy-preserving machine learning tasks. However, the use of such algorithms comes with trade-offs in terms of algorithmic fairness, which has been widely acknowledged. Specifically, we have empirically observed that the classical collaborative filtering method, trained by differentially private stochastic gradient descent (DP-SGD), results in a disparate impact on user groups with respect to different user engagement levels. This, in turn, causes the original unfair model to become even more biased against inactive users. To address the above issues, we propose \textbf{DP-Fair}, a two-stage framework for collaborative filtering based algorithms. Specifically, it combines differential privacy mechanisms with fairness constraints to protect user privacy while ensuring fair recommendations. The experimental results, based on Amazon datasets, and user history logs collected from Etsy, one of the largest e-commerce platforms, demonstrate that our proposed method exhibits superior performance in terms of both overall accuracy and user group fairness on both shallow and deep recommendation models compared to vanilla DP-SGD.|最近，在保护隐私的机器学习任务中越来越多地采用差分隐私指导算法。然而，这种算法的使用伴随着算法公平性方面的权衡，这已经得到了广泛的认可。具体来说，我们已经经验性地观察到，由差异私人协同过滤(DP-sgd)训练的经典随机梯度下降方法，在不同的用户参与水平方面对用户组产生了不同的影响。这反过来又导致原来的不公平模型对非活动用户变得更加有偏见。为了解决上述问题，我们提出 textbf { DP-fair } ，一个基于协同过滤的算法的两阶段框架。具体来说，它结合了差分隐私机制和公平约束，以保护用户隐私，同时确保公平推荐。基于 Amazon 数据集的实验结果，以及从最大的电子商务平台之一 Etsy 收集的用户历史记录表明，与普通的 DP-SGD 相比，我们提出的方法在浅层和深层推荐模型的总体准确性和用户组公平性方面表现出更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness-aware+Differentially+Private+Collaborative+Filtering)|0|
|[Psychotherapy AI Companion with Reinforcement Learning Recommendations and Interpretable Policy Dynamics](https://doi.org/10.1145/3543873.3587623)|Baihan Lin, Guillermo A. Cecchi, Djallel Bouneffouf|IBM TJ Watson Research Center, USA; Columbia University, USA|We introduce a Reinforcement Learning Psychotherapy AI Companion that generates topic recommendations for therapists based on patient responses. The system uses Deep Reinforcement Learning (DRL) to generate multi-objective policies for four different psychiatric conditions: anxiety, depression, schizophrenia, and suicidal cases. We present our experimental results on the accuracy of recommended topics using three different scales of working alliance ratings: task, bond, and goal. We show that the system is able to capture the real data (historical topics discussed by the therapists) relatively well, and that the best performing models vary by disorder and rating scale. To gain interpretable insights into the learned policies, we visualize policy trajectories in a 2D principal component analysis space and transition matrices. These visualizations reveal distinct patterns in the policies trained with different reward signals and trained on different clinical diagnoses. Our system's success in generating DIsorder-Specific Multi-Objective Policies (DISMOP) and interpretable policy dynamics demonstrates the potential of DRL in providing personalized and efficient therapeutic recommendations.|我们介绍一个强化学习的心理治疗 AI 指南，根据患者的反应为治疗师提供主题建议。该系统使用深度强化学习(DRL)为四种不同的精神疾病(焦虑症、抑郁症、精神分裂症和自杀病例)制定多目标政策。我们使用三种不同的工作联盟评分尺度(任务、联系和目标)对推荐话题的准确性进行了实验研究。我们表明，该系统能够相对较好地捕获真实数据(治疗师讨论的历史主题) ，并且表现最好的模型因障碍和评分尺度而异。为了获得对所学政策的可解释的见解，我们在二维主成分分析空间和转换矩阵中可视化政策轨迹。这些可视化显示了不同奖励信号训练和不同临床诊断训练的政策的不同模式。我们的系统在产生疾病特异性多目标政策(DISMOP)和可解释的政策动态方面的成功表明 DRL 在提供个性化和有效的治疗建议方面的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Psychotherapy+AI+Companion+with+Reinforcement+Learning+Recommendations+and+Interpretable+Policy+Dynamics)|0|
|[Investigating Action-Space Generalization in Reinforcement Learning for Recommendation Systems](https://doi.org/10.1145/3543873.3587661)|Abhishek Naik, Bo Chang, Alexandros Karatzoglou, Martin Mladenov, Ed H. Chi, Minmin Chen|University of Alberta, Canada and Alberta Machine Intelligence Institute (Amii), Canada; Google Research, USA|Recommender systems are used to suggest items to users based on the users’ preferences. Such systems often deal with massive item sets and incredibly sparse user-item interactions, which makes it very challenging to generate high-quality personalized recommendations. Reinforcement learning (RL) is a framework for sequential decision making and naturally formulates recommender-system tasks: recommending items as actions in different user and context states to maximize long-term user experience. We investigate two RL policy parameterizations that generalize sparse user-items interactions by leveraging the relationships between actions: parameterizing the policy over action features as a softmax or Gaussian distribution. Our experiments on synthetic problems suggest that the Gaussian parameterization—which is not commonly used on recommendation tasks—is more robust to the set of action features than the softmax parameterization. Based on these promising results, we propose a more thorough investigation of the theoretical properties and empirical benefits of the Gaussian parameterization for recommender systems.|推荐系统用于根据用户的喜好向用户推荐项目。这样的系统经常处理大量的项目集和难以置信的稀疏的用户-项目交互，这使得生成高质量的个性化推荐非常具有挑战性。推荐强化学习(RL)是一个连续决策的框架，它自然而然地制定了推荐系统的任务: 在不同的用户和上下文状态下，推荐项目作为行动，以最大限度地提高长期用户体验。我们研究了两种 RL 策略参数化，它们通过利用操作之间的关系来推广稀疏的用户-项目交互: 将策略参数化为 softmax 或者正态分布。我们在合成问题上的实验表明，高斯参数化(在推荐任务中并不常用)比 softmax 参量化对动作特征集的鲁棒性更强。基于这些有希望的结果，我们建议对高斯参量化推荐系统的理论特性和经验效益进行更深入的研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Investigating+Action-Space+Generalization+in+Reinforcement+Learning+for+Recommendation+Systems)|0|
|[Conversion of Legal Agreements into Smart Legal Contracts using NLP](https://doi.org/10.1145/3543873.3587554)|Eason Chen, Niall Roche, YuenHsien Tseng, Walter Hernández, Jiangbo Shangguan, Alastair Moore|HSBC Business School, Peking University, United Kingdom; National Taiwan Normal University, Taiwan; University College London, United Kingdom|A Smart Legal Contract (SLC) is a specialized digital agreement comprising natural language and computable components. The Accord Project provides an open-source SLC framework containing three main modules: Cicero, Concerto, and Ergo. Currently, we need lawyers, programmers, and clients to work together with great effort to create a usable SLC using the Accord Project. This paper proposes a pipeline to automate the SLC creation process with several Natural Language Processing (NLP) models to convert law contracts to the Accord Project's Concerto model. After evaluating the proposed pipeline, we discovered that our NER pipeline accurately detects CiceroMark from Accord Project template text with an accuracy of 0.8. Additionally, our Question Answering method can extract one-third of the Concerto variables from the template text. We also delve into some limitations and possible future research for the proposed pipeline. Finally, we describe a web interface enabling users to build SLCs. This interface leverages the proposed pipeline to convert text documents to Smart Legal Contracts by using NLP models.|智能法律合同(SLC)是一种专门的数字协议，包括自然语言和可计算组件。Accord Project 提供了一个开源的 SLC 框架，其中包含三个主要模块: Cicero、 Concerto 和 Ergo。目前，我们需要律师、程序员和客户共同努力，使用 AccordProject 创建一个可用的 SLC。本文提出了一种使用多种自然语言处理(NLP)模型实现 SLC 创建过程自动化的流水线，将法律合同转换为 Accord Project 的 Concerto 模型。经过评估后，我们发现我们的 NER 流水线能够准确地从雅阁项目模板文本中检测到 CiceroMark，准确率为0.8。此外，我们的问题回答方法可以提取三分之一的协奏曲变量从模板文本。我们还深入探讨了一些局限性和可能的未来研究的建议管道。最后，我们描述了一个允许用户构建 SLC 的 Web 界面。该接口利用拟议的管道，通过使用 NLP 模型将文本文档转换为智能法律合同。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conversion+of+Legal+Agreements+into+Smart+Legal+Contracts+using+NLP)|0|
|[Query-Driven Knowledge Graph Construction using Question Answering and Multimodal Fusion](https://doi.org/10.1145/3543873.3587567)|Yang Peng|University of Florida, USA|Over recent years, large knowledge bases have been constructed to store massive knowledge graphs. However, these knowledge graphs are highly incomplete. To solve this problem, we propose a web-based question answering system with multimodal fusion of unstructured and structured information, to fill in missing information for knowledge bases. To utilize unstructured information from the Web for knowledge graph construction, we design multimodal features and question templates to extract missing facts, which can achieve good quality with very few questions. The question answering system also employs structured information from knowledge bases, such as entity types and entity-to-entity relatedness, to help improve extraction quality. To improve system efficiency, we utilize a few query-driven techniques for web-based question answering to reduce the runtime and provide fast responses to user queries. Extensive experiments have been conducted to demonstrate the effectiveness and efficiency of our system.|近年来，人们建立了大型知识库来存储海量的知识图表。然而，这些知识图是非常不完整的。为了解决这一问题，我们提出了一种基于网络的非结构化和结构化信息多模态融合的问答系统，以填补知识库中缺失的信息。为了利用 Web 中的非结构化信息进行知识图的构造，我们设计了多模态特征和问题模板来提取缺失的事实，这样可以在很少的问题下获得很好的质量。问答系统还利用知识库中的结构化信息，如实体类型和实体间的相关性，以提高抽取质量。为了提高系统的效率，我们采用了一些基于查询驱动的网络问答技术，以减少运行时间，并提供快速响应用户的查询。通过大量的实验验证了该系统的有效性和高效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query-Driven+Knowledge+Graph+Construction+using+Question+Answering+and+Multimodal+Fusion)|0|
|[Decoding Prompt Syntax: Analysing its Impact on Knowledge Retrieval in Large Language Models](https://doi.org/10.1145/3543873.3587655)|Stephan Linzbach, Tim Tressel, Laura Kallmeyer, Stefan Dietze, Hajira Jabeen|GESIS Leibniz Institute for Social Sciences, Germany; GESIS Leibniz Institut für Sozialwissenschaften, Germany; Heinrich Heine University, Germany; GESIS Leibniz Institute for Social Sciences, Germany and Heinrich Heine University, Germany|Large Language Models (LLMs), with their advanced architectures and training on massive language datasets, contain unexplored knowledge. One method to infer this knowledge is through the use of cloze-style prompts. Typically, these prompts are manually designed because the phrasing of these prompts impacts the knowledge retrieval performance, even if the LLM encodes the desired information. In this paper, we study the impact of prompt syntax on the knowledge retrieval capacity of LLMs. We use a template-based approach to paraphrase simple prompts into prompts with a more complex grammatical structure. We then analyse the LLM performance for these structurally different but semantically equivalent prompts. Our study reveals that simple prompts work better than complex forms of sentences. The performance across the syntactical variations for simple relations (1:1) remains best, with a marginal decrease across different typologies. These results reinforce that simple prompt structures are more effective for knowledge retrieval in LLMs and motivate future research into the impact of prompt syntax on various tasks.|大型语言模型(LLM)具有先进的体系结构和对大量语言数据集的训练，包含了未开发的知识。一种推断这种知识的方法是通过使用完形填空式的提示。通常，这些提示是手动设计的，因为这些提示的措辞会影响知识检索性能，即使 LLM 对所需的信息进行了编码。本文研究了提示语法对 LLM 知识检索能力的影响。我们使用基于模板的方法将简单的提示转述为具有更复杂语法结构的提示。然后，我们分析这些结构不同但语义相等的提示符的 LLM 性能。我们的研究表明，简单的提示语比复杂的句子形式更有效。简单关系(1:1)的句法变化的表现仍然是最好的，不同类型之间的表现略有下降。这些结果强调了简单的提示结构对于 LLM 中的知识检索更有效，并且激发了对提示句法对各种任务的影响的进一步研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decoding+Prompt+Syntax:+Analysing+its+Impact+on+Knowledge+Retrieval+in+Large+Language+Models)|0|
|[CS-TGN: Community Search via Temporal Graph Neural Networks](https://doi.org/10.1145/3543873.3587654)|Farnoosh Hashemi, Ali Behrouz, Milad Rezaei Hajidehi|University of British Columbia, Canada|Searching for local communities is an important research challenge that allows for personalized community discovery and supports advanced data analysis in various complex networks, such as the World Wide Web, social networks, and brain networks. The evolution of these networks over time has motivated several recent studies to identify local communities in temporal networks. Given any query nodes, Community Search aims to find a densely connected subgraph containing query nodes. However, existing community search approaches in temporal networks have two main limitations: (1) they adopt pre-defined subgraph patterns to model communities, which cannot find communities that do not conform to these patterns in real-world networks, and (2) they only use the aggregation of disjoint structural information to measure quality, missing the dynamic of connections and temporal properties. In this paper, we propose a query-driven Temporal Graph Convolutional Network (CS-TGN) that can capture flexible community structures by learning from the ground-truth communities in a data-driven manner. CS-TGN first combines the local query-dependent structure and the global graph embedding in each snapshot of the network and then uses a GRU cell with contextual attention to learn the dynamics of interactions and update node embeddings over time. We demonstrate how this model can be used for interactive community search in an online setting, allowing users to evaluate the found communities and provide feedback. Experiments on real-world temporal graphs with ground-truth communities validate the superior quality of the solutions obtained and the efficiency of our model in both temporal and interactive static settings.|搜索当地社区是一个重要的研究挑战，它允许个性化的社区发现，并支持各种复杂网络中的先进数据分析，如万维网、社交网络和大脑网络。随着时间的推移，这些网络的演变促使最近几项研究在时间网络中识别局部群落。给定任何查询节点，Community Search 的目标是找到一个包含查询节点的密集连接子图。然而，现有的时态网络中的社区搜索方法存在两个主要的局限性: (1)它们采用预定义的子图模式来模拟社区，在现实网络中不能找到不符合这些模式的社区; (2)它们只使用不相交的结构信息的聚合来度量质量，缺乏连接的动态性和时态性。本文提出了一种基于查询驱动的时态图卷积网络(CS-TGN) ，该网络通过以数据驱动的方式从地面真相社区中学习知识，可以捕获灵活的社区结构。TGN 首先将本地查询依赖结构和全局图嵌入到网络的每个快照中，然后利用一个具有上下文关注的 GRU 单元来学习交互的动态性，并随着时间的推移更新节点嵌入。我们展示了这个模型如何在在线环境中用于交互式社区搜索，允许用户评估找到的社区并提供反馈。在真实时间图上的实验结果验证了该模型在时间静态和交互静态环境下的优越性和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CS-TGN:+Community+Search+via+Temporal+Graph+Neural+Networks)|0|
|[Learned Temporal Aggregations for Fraud Classification on E-Commerce Platforms](https://doi.org/10.1145/3543873.3587632)|Xiao Ling, David Yan, Bilal Alsallakh, Ashutosh Pandey, Manan Bakshi, Pamela Bhattacharya|Meta, USA; Meta, Canada; North Carolina State University, USA; Voxel AI, USA|Fraud and other types of adversarial behavior are serious problems on customer-to-customer (C2C) e-commerce platforms, where harmful behaviors by bad actors erode user trust and safety. Many modern e-commerce integrity systems utilize machine learning (ML) to detect fraud and bad actors. We discuss the practical problems faced by integrity systems which utilize data associated with user interactions with the platform. Specifically, we focus on the challenge of representing the user interaction events, and aggregating their features. We compare the performance of two paradigms to handle the feature temporality when training the ML models: hand-engineered temporal aggregation and a learned aggregation using a sequence encoder. We show that a model which learns a time-aggregation using a sequence encoder outperforms models trained on handcrafted aggregations on the fraud classification task with a real-world dataset.|欺诈和其他类型的对抗行为是 C2C 电子商务平台上的严重问题，不良行为者的有害行为侵蚀了用户的信任和安全。许多现代电子商务完整性系统利用机器学习(ML)来检测欺诈和不良行为者。我们讨论完整性系统所面临的实际问题，这些系统利用与平台的用户交互相关的数据。具体来说，我们关注的挑战是表示用户交互事件，并聚合它们的特性。在训练机器学习模型时，我们比较了两种模式处理特征时间的性能: 手工时间聚合和使用序列编码器的学习聚合。我们展示了一个使用序列编码器学习时间聚合的模型优于使用真实世界数据集进行欺诈分类任务的手工聚合训练的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learned+Temporal+Aggregations+for+Fraud+Classification+on+E-Commerce+Platforms)|0|
|[Decency and Decentralisation: Verifiable Decentralised Knowledge Graph Querying](https://doi.org/10.1145/3543873.3587635)|Aisling Third, John Domingue|Knowledge Media Institute, The Open University, United Kingdom|Increasing interest in decentralisation for data and processing on the Web brings with it the need to re-examine methods for verifying data and behaviour for scalable multi-party interactions. We consider factors relevant to verification of querying activity on knowledge graphs in a Trusted Decentralised Web, and set out ideas for future research in this area.|随着人们对数据地方分权和网络处理的兴趣日益增长，人们需要重新审视可扩展的多方交互的数据和行为验证方法。我们考虑了与可信分布式网络中知识图表查询活动验证相关的因素，并为这一领域的未来研究提出了一些想法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decency+and+Decentralisation:+Verifiable+Decentralised+Knowledge+Graph+Querying)|0|
|[Towards a Decentralized Data Hub and Query System for Federated Dynamic Data Spaces](https://doi.org/10.1145/3543873.3587646)|Danh Le Phuoc, Sonja Schimmler, Anh LeTuan, Uwe A. Kuehn, Manfred Hauswirth|TU Berlin, Germany; Fraunhofer Institute for Open Communication Systems, Berlin, Germany|This position paper proposes a hybrid architecture for secure and efficient data sharing and processing across dynamic data spaces. On the one hand, current centralized approaches are plagued by issues such as lack of privacy and control for users, high costs, and bad performance, making these approaches unsuitable for the decentralized data spaces prevalent in Europe and various industries (decentralized on the conceptual and physical levels while centralized in the underlying implementation). On the other hand, decentralized systems face challenges with limited knowledge of/control over the global system, fair resource utilization, and data provenance. Our proposed Semantic Data Ledger (SDL) approach combines the advantages of both architectures to overcome their limitations. SDL allows users to choose the best combination of centralized and decentralized features, providing a decentralized infrastructure for the publication of structured data with machine-readable semantics. It supports expressive structured queries, secure data sharing, and payment mechanisms based on an underlying autonomous ledger, enabling the implementation of economic models and fair-use strategies.|本文提出了一种跨动态数据空间的安全有效的数据共享和处理的混合体系结构。一方面，当前的集中式方法受到诸如用户缺乏隐私和控制、高成本和性能差等问题的困扰，使得这些方法不适合在欧洲和各种行业盛行的分散式数据空间(在概念和物理层面上分散，而在底层实现中集中)。另一方面，分散系统面临的挑战是对全球系统的了解和控制有限，资源利用不公平，数据来源不明确。我们提出的语义数据分类账(SDL)方法结合了两种体系结构的优点，克服了它们的局限性。SDL 允许用户选择集中和分散特性的最佳组合，为具有机器可读语义的结构化数据的发布提供分散的基础设施。它支持表达式结构化查询、安全数据共享和基于底层自治分类账的支付机制，使经济模型和合理使用策略的实施成为可能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+a+Decentralized+Data+Hub+and+Query+System+for+Federated+Dynamic+Data+Spaces)|0|
|[What are "personal data spaces"?](https://doi.org/10.1145/3543873.3587656)|Viivi Lähteenoja|University of Helsinki, Finland and Aalto University, Finland|While the concept of “data spaces” is no longer new, its specific application to individuals and personal data management is still undeveloped. This short paper presents a vision for “personal data spaces” in the shape of a work-in-progress description of them and some of the conceptual and implementation features envisioned. It is offered for discussion, debate, and improvement by professionals, policymakers, and researchers operating in the intersection of data spaces and personal data management.|虽然“数据空间”的概念不再是新的，但它在个人和个人数据管理方面的具体应用仍然没有得到发展。这篇简短的论文提出了一个“个人数据空间”的愿景，其形式是一个在建的数据空间描述，以及所设想的一些概念和实现特征。它提供了讨论，辩论和改进的专业人士，决策者和研究人员在数据空间和个人数据管理的交叉运作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What+are+"personal+data+spaces"?)|0|
|[TAPP: Defining standard provenance information for clinical research data and workflows - Obstacles and opportunities](https://doi.org/10.1145/3543873.3587562)|Kerstin Gierend, Judith A. H. Wodke, Sascha Genehr, Robert Gött, Ron Henkel, Frank Krüger, Markus Mandalka, Lea Michaelis, Alexander Scheuerlein, Max Schröder, Atinkut Zeleke, Dagmar Waltemath|Medical Informatics Laboratory, MeDaX Group, University Medicine Greifswald, Germany; Rostock University Library, University of Rostock, Germany; Department of Biomedical Informatics, Center for Preventive Medicine and Digital Health, Medical Faculty Mannheim, Heidelberg University, Germany; Core Unit Data Integration Center, University Medicine Greifswald, Germany; Institute of Communications Engineering, University of Rostock, Germany; Institute for Data Science, University of Greifswald, Germany; Medical Informatics Laboratory, University Medicine Greifswald, Germany; Faculty of Engineering, Wismar University of Applied Sciences, Germany|Data provenance has raised much attention across disciplines lately, as it has been shown that enrichment of data with provenance information leads to better credibility, renders data more FAIR fostering data reuse. Also, the biomedical domain has recognised the potential of provenance capture. However, several obstacles prevent efficient, automated, and machine-interpretable enrichment of biomedical data with provenance information, such as data heterogeneity, complexity, and sensitivity. Here, we explain how in Germany clinical data are transferred from hospital information systems into a data integration centre to enable secondary use of patient data and how it can be reused as research data. Considering the complex data infrastructures in hospitals, we indicate obstacles and opportunities when collecting provenance information along heterogeneous data processing pipelines. To express provenance data, we indicate the usage of the Fast Healthcare Interoperability Resource (FHIR) provenance resource for healthcare data. In addition, we consider already existing approaches from other research fields and standard communities. As a solution towards high-quality standardised clinical research data, we propose to develop a ’MInimal Requirements for Automated Provenance Information Enrichment’ (MIRAPIE) guideline. As a community project, MIRAPIE should generalise provenance information concepts to allow its world-wide applicability, possibly beyond the health care sector.|数据来源最近引起了跨学科的广泛关注，因为已经表明，用来源信息丰富数据可以提高可信度，使数据更加公平，促进数据重用。此外，生物医学领域已经认识到种源捕获的潜力。然而，一些障碍阻碍了生物医学数据的有效、自动化和机器可解释的来源信息的丰富，例如数据异构性、复杂性和敏感性。在这里，我们解释在德国如何将临床数据从医院信息系统转移到数据集成中心，以便能够对患者数据进行二次使用，以及如何将其重用为研究数据。考虑到医院复杂的数据基础设施，我们指出了沿着异构数据处理管道收集起源信息的障碍和机会。为了表示来源数据，我们指出了 Fast Healthcare Interoperability Resource (FHIR)来源资源对医疗数据的使用。此外，我们还考虑了来自其他研究领域和标准社区的已有方法。作为高质量标准化临床研究数据的解决方案，我们建议制定一个“自动起源信息丰富的最低要求”(MIRAPIE)指南。作为一个社区项目，MIRAPIE 应该推广起源信息的概念，使其在世界范围内适用，可能超出卫生保健部门。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TAPP:+Defining+standard+provenance+information+for+clinical+research+data+and+workflows+-+Obstacles+and+opportunities)|0|
|[ProSA: A provenance system for reproducing query results](https://doi.org/10.1145/3543873.3587563)|Tanja Auge|Faculty of Informatics and Data Science, University of Regensburg, Germany|Good scientific work requires comprehensible, transparent and reproducible research. One way to ensure this is to include all data relevant to a study or evaluation when publishing an article. This data should be at least aggregated or anonymized, at best compact and complete, but always resilient. In this paper we present ProSA, a system for calculating the minimal necessary data set, called sub-database. For this, we combine the Chase — a set of algorithms for transforming databases — with additional provenance information. We display the implementation of provenance guided by the ProSA pipeline and show its use to generate an optimized sub-database. Furhter, we demonstrate how the ProSA GUI looks like and present some applications and extensions.|好的科学工作需要可理解、透明和可重复的研究。确保这一点的一个方法是在发表文章时包括与研究或评估相关的所有数据。这些数据应该至少是聚合或匿名的，充其量是紧凑和完整的，但总是具有弹性。本文介绍了 ProSA，一个计算最小必要数据集的系统，称为子数据库。为此，我们将 Chase (一组转换数据库的算法)与其他来源信息结合起来。我们展示了 ProSA 流水线引导的起源实现，并展示了它用于生成优化的子数据库。进一步，我们将演示 ProSA GUI 的外观，并展示一些应用程序和扩展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ProSA:+A+provenance+system+for+reproducing+query+results)|0|
|[Hybrid Query and Instance Explanations and Repairs](https://doi.org/10.1145/3543873.3587565)|Seokki Lee, Boris Glavic, Adriane Chapman, Bertram Ludäscher|University of Cincinnati, USA; University of Southampton, United Kingdom; Illinois Institute of Technology, USA; University of Illinois at Urbana-Champaign, USA|Prior work on explaining missing (unexpected) query results identifies which parts of the query or data are responsible for the erroneous result or repairs the query or data to fix such errors. The problem of generating repairs is typically expressed as an optimization problem, i.e., a single repair is returned that is optimal wrt. to some criterion such as minimizing the repair’s side effects. However, such an optimization objective may not concretely model a user’s (often hard to formalize) notion of which repair is “correct”. In this paper, we motivate hybrid explanations and repairs, i.e., that fix both the query and the data. Instead of returning one “optimal” repair, we argue for an approach that empowers the user to explore the space of possible repairs effectively. We also present a proof-of-concept implementation and outline open research problems.|先前解释丢失(意外)查询结果的工作确定了查询或数据的哪些部分对错误结果负责，或者修复查询或数据以修复此类错误。产生维修的问题通常表示为一个最佳化问题，也就是说，一个单一的维修被返回，这是最优的书面意见，以某些标准，如最小化维修的副作用。然而，这样的优化目标可能无法具体地模拟用户(通常很难形式化)的哪种修复是“正确的”概念。在本文中，我们激励混合解释和修复，即，修复查询和数据。与返回一个“最佳”修复相反，我们主张采用一种方法，使用户能够有效地探索可能的修复空间。我们还提出了一个概念验证实现，并概述了开放式研究问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hybrid+Query+and+Instance+Explanations+and+Repairs)|0|
|[Querying Container Provenance](https://doi.org/10.1145/3543873.3587568)|Aniket Modi, Moaz Reyad, Tanu Malik, Ashish Gehani|SRI International, USA; College of Computing and Digital Media, DePaul University, USA; College of Computing and Digital Media, DePaul University, USA and Department of Computer Science and Engineering, IIT Delhi, India|Containers are lightweight mechanisms for the isolation of operating system resources. They are realized by activating a set of namespaces. Given the use of containers in scientific computing, tracking and managing provenance within and across containers is becoming essential for debugging and reproducibility. In this work, we examine the properties of container provenance graphs that result from auditing containerized applications. We observe that the generated container provenance graphs are hypergraphs because one resource may belong to one or more namespaces. We examine the hierarchical behavior of PID, mount, and user namespaces, that are more commonly activated and show that even when represented as hypergraphs, the resulting container provenance graphs are acyclic. We experiment with recently published container logs and identify hypergraph properties.|容器是用于隔离操作系统资源的轻量级机制。它们是通过激活一组名称空间来实现的。鉴于容器在科学计算中的使用，跟踪和管理容器内部和跨容器的出处对于调试和再现性变得至关重要。在本文中，我们研究了审计容器化应用程序所产生的容器起源图的属性。我们注意到，生成的容器起源图是超图，因为一个资源可能属于一个或多个名称空间。我们研究了 PID、 mount 和用户名称空间的分层行为，这些名称空间通常被激活，并且表明即使用超图表示，最终的容器起源图也是无环的。我们使用最近发布的容器日志进行实验，并识别超图属性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Querying+Container+Provenance)|0|
|[Graph-less Collaborative Filtering](https://doi.org/10.1145/3543507.3583196)|Lianghao Xia, Chao Huang, Jiao Shi, Yong Xu|South China University of Technology, China; The University of Hong Kong, Hong Kong|Graph neural networks (GNNs) have shown the power in representation learning over graph-structured user-item interaction data for collaborative filtering (CF) task. However, with their inherently recursive message propagation among neighboring nodes, existing GNN-based CF models may generate indistinguishable and inaccurate user (item) representations due to the over-smoothing and noise effect with low-pass Laplacian smoothing operators. In addition, the recursive information propagation with the stacked aggregators in the entire graph structures may result in poor scalability in practical applications. Motivated by these limitations, we propose a simple and effective collaborative filtering model (SimRec) that marries the power of knowledge distillation and contrastive learning. In SimRec, adaptive transferring knowledge is enabled between the teacher GNN model and a lightweight student network, to not only preserve the global collaborative signals, but also address the over-smoothing issue with representation recalibration. Empirical results on public datasets show that SimRec archives better efficiency while maintaining superior recommendation performance compared with various strong baselines. Our implementations are publicly available at: https://github.com/HKUDS/SimRec.|图形神经网络(GNN)已经显示了在表示学习方面的能力，超过了图形结构的用户-项目交互数据的协同过滤(CF)任务。然而，由于现有的基于 GNN 的 CF 模型固有的相邻节点之间的递归消息传播特性，由于低通拉普拉斯平滑算子的过平滑和噪声效应，可能产生难以区分和不准确的用户(项)表示。此外，在整个图结构中，叠加聚合器的递归信息传播可能导致实际应用中的可扩展性较差。基于这些局限性，我们提出了一个简单有效的协同过滤模型(SimRec) ，它将知识提取和对比学习的力量结合在一起。在 SimRec 中，在教师 GNN 模型和轻量级学生网络之间实现了自适应知识传递，不仅保留了全局协作信号，而且通过表示重校正解决了过于平滑的问题。对公共数据集的实证结果表明，与各种强基线相比，SimRec 在保持优异推荐性能的同时提高了存档效率。我们的实施方案可以在以下 https://github.com/hkuds/simrec 公开获得:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph-less+Collaborative+Filtering)|0|
|[Collaboration-Aware Graph Convolutional Network for Recommender Systems](https://doi.org/10.1145/3543507.3583229)|Yu Wang, Yuying Zhao, Yi Zhang, Tyler Derr|Vanderbilt university, USA|Graph Neural Networks (GNNs) have been successfully adopted in recommender systems by virtue of the message-passing that implicitly captures collaborative effect. Nevertheless, most of the existing message-passing mechanisms for recommendation are directly inherited from GNNs without scrutinizing whether the captured collaborative effect would benefit the prediction of user preferences. In this paper, we first analyze how message-passing captures the collaborative effect and propose a recommendation-oriented topological metric, Common Interacted Ratio (CIR), which measures the level of interaction between a specific neighbor of a node with the rest of its neighbors. After demonstrating the benefits of leveraging collaborations from neighbors with higher CIR, we propose a recommendation-tailored GNN, Collaboration-Aware Graph Convolutional Network (CAGCN), that goes beyond 1-Weisfeiler-Lehman(1-WL) test in distinguishing non-bipartite-subgraph-isomorphic graphs. Experiments on six benchmark datasets show that the best CAGCN variant outperforms the most representative GNN-based recommendation model, LightGCN, by nearly 10% in Recall@20 and also achieves around 80% speedup. Our code is publicly available at https://github.com/YuWVandy/CAGCN.|图形神经网络(GNN)通过隐式捕捉协作效果的消息传递，已成功地应用于推荐系统中。尽管如此，大多数现有的推荐信息传递机制都是直接从 GNN 继承而来的，没有仔细检查所捕获的协作效应是否有利于预测用户的偏好。在本文中，我们首先分析了消息传递是如何捕获协作效果的，并提出了一个面向推荐的拓扑度量，公共交互比(CIR) ，它测量节点的特定邻居与其他邻居之间的交互水平。在证明了利用具有较高 CIR 的邻居的协作的好处之后，我们提出了一种推荐量身定制的 GNN，协作感知图卷积网络(CAGCN) ，其超越了1-Weisfeiler-Lehman (1-WL)检验在区分非二部子图-同构图。在六个基准数据集上的实验表明，在 Recall@20中，最好的 CAGCN 变体比最具代表性的基于 GNN 的推荐模型 LightGCN 的性能提高了近10% ，并且还实现了约80% 的加速比。我们的代码可以在 https://github.com/yuwvandy/cagcn 上公开获取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaboration-Aware+Graph+Convolutional+Network+for+Recommender+Systems)|0|
|[HyConvE: A Novel Embedding Model for Knowledge Hypergraph Link Prediction with Convolutional Neural Networks](https://doi.org/10.1145/3543507.3583256)|Chenxu Wang, Xin Wang, Zhao Li, Zirui Chen, Jianxin Li|Deakin University, Australia; Tianjin University, China|Knowledge hypergraph embedding, which projects entities and n-ary relations into a low-dimensional continuous vector space to predict missing links, remains a challenging area to be explored despite the ubiquity of n-ary relational facts in the real world. Currently, knowledge hypergraph link prediction methods are essentially simple extensions of those used in knowledge graphs, where n-ary relational facts are decomposed into different subelements. Convolutional neural networks have been shown to have remarkable information extraction capabilities in previous work on knowledge graph link prediction. In this paper, we propose a novel embedding-based knowledge hypergraph link prediction model named HyConvE, which exploits the powerful learning ability of convolutional neural networks for effective link prediction. Specifically, we employ 3D convolution to capture the deep interactions of entities and relations to efficiently extract explicit and implicit knowledge in each n-ary relational fact without compromising its translation property. In addition, appropriate relation and position-aware filters are utilized sequentially to perform two-dimensional convolution operations to capture the intrinsic patterns and position information in each n-ary relation, respectively. Extensive experimental results on real datasets of knowledge hypergraphs and knowledge graphs demonstrate the superior performance of HyConvE compared with state-of-the-art baselines.|知识超图嵌入将实体和 n 元关系投影到低维连续向量空间中以预测缺失的链接，尽管 n 元关系事实在现实世界中无处不在，但仍然是一个有待探索的挑战领域。目前，知识超图链接预测方法基本上是知识图的简单扩展，其中 n 元关系事实被分解为不同的子元素。卷积神经网络已被证明具有显著的信息抽取能力在以前的工作中的知识图链接预测。本文提出了一种新的基于嵌入的知识超图链接预测模型 HyConve，该模型利用卷积神经网络强大的学习能力进行有效的链接预测。具体来说，我们使用三维卷积来捕捉实体和关系的深层交互作用，以有效地提取每个 n 元关系事实中的显性和隐性知识，而不损害其翻译性质。此外，还利用合适的关系和位置感知滤波器，分别进行二维卷积运算，捕获每个 n 元关系中的内在模式和位置信息。在知识超图和知识图的实际数据集上进行的大量实验结果表明，与最先进的基线相比，HyConve 方法具有更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HyConvE:+A+Novel+Embedding+Model+for+Knowledge+Hypergraph+Link+Prediction+with+Convolutional+Neural+Networks)|0|
|[Efficient Approximation Algorithms for the Diameter-Bounded Max-Coverage Group Steiner Tree Problem](https://doi.org/10.1145/3543507.3583257)|Ke Zhang, Xiaoqing Wang, Gong Cheng|State Key Laboratory for Novel Software Technology, Nanjing University, China|The Diameter-bounded max-Coverage Group Steiner Tree (DCGST) problem has recently been proposed as an expressive way of formulating keyword-based search and exploration of knowledge graphs. It aims at finding a diameter-bounded tree which covers the most given groups of vertices and has the minimum weight. In contrast to its specialization—the classic Group Steiner Tree (GST) problem which has been extensively studied, the emerging DCGST problem still lacks an efficient algorithm. In this paper, we propose Cba, the first approximation algorithm for the DCGST problem, and we prove its worst-case approximation ratio. Furthermore, we incorporate a best-first search strategy with two pruning methods into PrunedCBA, an improved approximation algorithm. Our extensive experiments on real and synthetic graphs demonstrate the effectiveness and efficiency of PrunedCBA.|直径有界的最大覆盖群 Steiner 树(DCGST)问题是近年来提出的一种基于关键字搜索和知识图探索的表示方法。它的目标是找到一个直径有界的树，它覆盖了最多给定的顶点群，并具有最小的权重。与经典的群 Steiner 树(GST)问题相比，新出现的 DCGST 问题仍然缺乏一种有效的算法。在这篇文章中，我们提出了 Cba，这是 DCGST 问题的第一个近似演算法，并且证明了它的最坏情况逼近比。此外，我们将最佳优先搜索策略和两种修剪方法结合到一个改进的近似演算法 PrunedCBA 中。我们在实图和合成图上的大量实验证明了 PrunedCBA 的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Approximation+Algorithms+for+the+Diameter-Bounded+Max-Coverage+Group+Steiner+Tree+Problem)|0|
|[ConsRec: Learning Consensus Behind Interactions for Group Recommendation](https://doi.org/10.1145/3543507.3583277)|Xixi Wu, Yun Xiong, Yao Zhang, Yizhu Jiao, Jiawei Zhang, Yangyong Zhu, Philip S. Yu|University of Illinois at Chicago, USA; IFM Lab, Department of Computer Science, University of California, Davis, USA; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, China; University of Illinois at Urbana-Champaign, USA|Since group activities have become very common in daily life, there is an urgent demand for generating recommendations for a group of users, referred to as group recommendation task. Existing group recommendation methods usually infer groups' preferences via aggregating diverse members' interests. Actually, groups' ultimate choice involves compromises between members, and finally, an agreement can be reached. However, existing individual information aggregation lacks a holistic group-level consideration, failing to capture the consensus information. Besides, their specific aggregation strategies either suffer from high computational costs or become too coarse-grained to make precise predictions. To solve the aforementioned limitations, in this paper, we focus on exploring consensus behind group behavior data. To comprehensively capture the group consensus, we innovatively design three distinct views which provide mutually complementary information to enable multi-view learning, including member-level aggregation, item-level tastes, and group-level inherent preferences. To integrate and balance the multi-view information, an adaptive fusion component is further proposed. As to member-level aggregation, different from existing linear or attentive strategies, we design a novel hypergraph neural network that allows for efficient hypergraph convolutional operations to generate expressive member-level aggregation. We evaluate our ConsRec on two real-world datasets and experimental results show that our model outperforms state-of-the-art methods. An extensive case study also verifies the effectiveness of consensus modeling.|由于小组活动在日常生活中已经非常普遍，因此迫切需要为一组用户提供建议，称为小组推荐任务。现有的群体推荐方法通常通过聚合不同成员的兴趣来推断群体的偏好。实际上，团体的最终选择包括成员之间的妥协，最终可以达成协议。然而，现有的个人信息聚合缺乏整体的群体层面的考虑，未能捕获共识信息。此外，他们特定的聚合策略要么计算成本高，要么过于粗粒度，无法做出精确的预测。为了解决上述局限性，本文重点探讨群体行为数据背后的共识。为了全面捕捉群体共识，我们创新性地设计了三种不同的视图，提供相互补充的信息，使多视图学习成为可能，包括成员层面的聚合、项目层面的品味和群体层面的固有偏好。为了对多视点信息进行集成和平衡，进一步提出了一种自适应融合构件。对于成员级聚集，不同于现有的线性或注意策略，我们设计了一种新的超图神经网络，该网络允许有效的超图卷积操作来产生具有表达能力的成员级聚集。我们在两个真实世界的数据集上评估了我们的 ConsRec，实验结果表明我们的模型优于最先进的方法。一个广泛的案例研究也验证了一致性建模的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ConsRec:+Learning+Consensus+Behind+Interactions+for+Group+Recommendation)|0|
|[Graph Neural Networks with Diverse Spectral Filtering](https://doi.org/10.1145/3543507.3583324)|Jingwei Guo, Kaizhu Huang, Xinping Yi, Rui Zhang|Xi'an Jiaotong-Liverpool University; The University of Liverpool, China; The University of Liverpool, United Kingdom; Duke Kunshan University, China; Xi'an Jiaotong-Liverpool University, China|Spectral Graph Neural Networks (GNNs) have achieved tremendous success in graph machine learning, with polynomial filters applied for graph convolutions, where all nodes share the identical filter weights to mine their local contexts. Despite the success, existing spectral GNNs usually fail to deal with complex networks (e.g., WWW) due to such homogeneous spectral filtering setting that ignores the regional heterogeneity as typically seen in real-world networks. To tackle this issue, we propose a novel diverse spectral filtering (DSF) framework, which automatically learns node-specific filter weights to exploit the varying local structure properly. Particularly, the diverse filter weights consist of two components — A global one shared among all nodes, and a local one that varies along network edges to reflect node difference arising from distinct graph parts — to balance between local and global information. As such, not only can the global graph characteristics be captured, but also the diverse local patterns can be mined with awareness of different node positions. Interestingly, we formulate a novel optimization problem to assist in learning diverse filters, which also enables us to enhance any spectral GNNs with our DSF framework. We showcase the proposed framework on three state-of-the-arts including GPR-GNN, BernNet, and JacobiConv. Extensive experiments over 10 benchmark datasets demonstrate that our framework can consistently boost model performance by up to 4.92% in node classification tasks, producing diverse filters with enhanced interpretability.|谱图神经网络(GNN)在图形机器学习中取得了巨大的成功，其中多项式滤波器应用于图卷积，其中所有节点共享相同的滤波器权重来挖掘它们的局部上下文。尽管已有的光谱 GNN 在处理复杂网络(如 WWW)时取得了一定的成功，但由于均匀的光谱滤波设置忽略了现实网络中典型的区域异质性，导致 GNN 无法处理复杂网络(如 WWW)。针对这一问题，我们提出了一种新的多谱段滤波(DSF)框架，该框架能够自动学习节点特定的滤波器权值，以适当地利用变化的局部结构。特别地，不同的滤波器权重由两部分组成: 一部分是所有节点共享的全局权重，另一部分是沿网络边缘变化的局部权重，以反映不同图部分产生的节点差异，从而平衡局部和全局信息。因此，不仅可以捕获全局图的特征，而且可以挖掘不同节点位置的不同局部模式。有趣的是，我们制定了一个新的最佳化问题，以帮助学习不同的过滤器，这也使我们能够增强任何光谱 GNN 与我们的 dSF 框架。我们在 GPR-GNN、 BernNet 和 JacobiConv 这三种最新技术的基础上展示了提议的框架。通过对10个基准数据集的大量实验表明，我们的框架可以在节点分类任务中始终如一地将模型性能提高高达4.92% ，产生具有增强可解释性的多样化过滤器。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Neural+Networks+with+Diverse+Spectral+Filtering)|0|
|[Semi-decentralized Federated Ego Graph Learning for Recommendation](https://doi.org/10.1145/3543507.3583337)|Liang Qu, Ningzhi Tang, Ruiqi Zheng, Quoc Viet Hung Nguyen, Zi Huang, Yuhui Shi, Hongzhi Yin|Griffith University, Australia; The University of Queensland, Australia; Southern University of Science and Technology, China|Collaborative filtering (CF) based recommender systems are typically trained based on personal interaction data (e.g., clicks and purchases) that could be naturally represented as ego graphs. However, most existing recommendation methods collect these ego graphs from all users to compose a global graph to obtain high-order collaborative information between users and items, and these centralized CF recommendation methods inevitably lead to a high risk of user privacy leakage. Although recently proposed federated recommendation systems can mitigate the privacy problem, they either restrict the on-device local training to an isolated ego graph or rely on an additional third-party server to access other ego graphs resulting in a cumbersome pipeline, which is hard to work in practice. In addition, existing federated recommendation systems require resource-limited devices to maintain the entire embedding tables resulting in high communication costs. In light of this, we propose a semi-decentralized federated ego graph learning framework for on-device recommendations, named SemiDFEGL, which introduces new device-to-device collaborations to improve scalability and reduce communication costs and innovatively utilizes predicted interacted item nodes to connect isolated ego graphs to augment local subgraphs such that the high-order user-item collaborative information could be used in a privacy-preserving manner. Furthermore, the proposed framework is model-agnostic, meaning that it could be seamlessly integrated with existing graph neural network-based recommendation methods and privacy protection techniques. To validate the effectiveness of the proposed SemiDFEGL, extensive experiments are conducted on three public datasets, and the results demonstrate the superiority of the proposed SemiDFEGL compared to other federated recommendation methods.|基于协同过滤(CF)的推荐系统通常基于个人交互数据(例如点击和购买)进行培训，这些数据可以自然地表示为自我图表。然而，现有的大多数推荐方法都是从所有用户中收集这些自我图来构成一个全局图，以获得用户和项目之间的高阶协同信息，而这些集中式的 CF 推荐方法不可避免地会导致用户隐私泄露的高风险。虽然最近提出的联邦推荐系统可以缓解隐私问题，但是它们要么将设备上的本地培训限制在一个孤立的自我图上，要么依赖于另外一个第三方服务器来访问其他自我图，从而产生一个繁琐的管道，这在实践中很难实现。此外，现有的联邦推荐系统需要资源有限的设备来维护整个嵌入表，从而导致高通信成本。鉴于此，我们提出了一个半分散的联邦自我图学习框架 SemiDFEGL，该框架引入了新的设备间协作以提高可扩展性和降低通信成本，并创新地利用预测的交互项节点连接孤立的自我图以增强局部子图，从而可以以保护隐私的方式使用高阶用户项协作信息。此外，提出的框架是模型无关的，这意味着它可以与现有的基于图神经网络的推荐方法和隐私保护技术无缝集成。为了验证半 DFEGL 的有效性，在三个公共数据集上进行了广泛的实验，实验结果表明了半 DFEGL 相对于其他联邦推荐方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semi-decentralized+Federated+Ego+Graph+Learning+for+Recommendation)|0|
|[SINCERE: Sequential Interaction Networks representation learning on Co-Evolving RiEmannian manifolds](https://doi.org/10.1145/3543507.3583353)|Junda Ye, Zhongbao Zhang, Li Sun, Yang Yan, Feiyang Wang, Fuxin Ren|North China Electric Power University, China; Beijing University of Posts and Telecommunications, China|Sequential interaction networks (SIN) have been commonly adopted in many applications such as recommendation systems, search engines and social networks to describe the mutual influence between users and items/products. Efforts on representing SIN are mainly focused on capturing the dynamics of networks in Euclidean space, and recently plenty of work has extended to hyperbolic geometry for implicit hierarchical learning. Previous approaches which learn the embedding trajectories of users and items achieve promising results. However, there are still a range of fundamental issues remaining open. For example, is it appropriate to place user and item nodes in one identical space regardless of their inherent discrepancy? Instead of residing in a single fixed curvature space, how will the representation spaces evolve when new interaction occurs? To explore these issues for sequential interaction networks, we propose SINCERE, a novel method representing Sequential Interaction Networks on Co-Evolving RiEmannian manifolds. SIN- CERE not only takes the user and item embedding trajectories in respective spaces into account, but also emphasizes on the space evolvement that how curvature changes over time. Specifically, we introduce a fresh cross-geometry aggregation which allows us to propagate information across different Riemannian manifolds without breaking conformal invariance, and a curvature estimator which is delicately designed to predict global curvatures effectively according to current local Ricci curvatures. Extensive experiments on several real-world datasets demonstrate the promising performance of SINCERE over the state-of-the-art sequential interaction prediction methods.|在推荐系统、搜索引擎、社交网络等应用中，用户与产品之间的相互影响通常采用序贯交互网络(SIN)来描述。表示 SIN 的努力主要集中在捕捉欧几里得空间中网络的动态，最近大量的工作已经延伸到了隐含双曲几何的深度学习。以往的方法通过学习用户和项目的嵌入轨迹，取得了良好的效果。然而，仍有一系列基本问题悬而未决。例如，是否应该将用户和项目节点放在一个相同的空间中，而不管它们的固有差异？当发生新的相互作用时，表示空间将如何演化，而不是驻留在一个单一的固定曲率空间中？为了研究序贯相互作用网络的这些问题，我们提出了一种新的方法 SINCARE，它在共进化黎曼流形上表示序贯相互作用网络。SIN-CERE 不仅考虑了用户和项目在各自空间中的嵌入轨迹，而且强调了曲率随时间变化的空间演化。具体地说，我们引入了一个新的交叉几何聚合，它允许我们在不破坏共形不变性的情况下在不同的黎曼流形上传播信息，以及一个精心设计的曲率估计器，它可以根据当前的局部 Ricci 曲率有效地预测全局曲率。在几个真实世界数据集上的大量实验表明，SINCARE 相对于最先进的顺序交互预测方法具有很好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SINCERE:+Sequential+Interaction+Networks+representation+learning+on+Co-Evolving+RiEmannian+manifolds)|0|
|[TIGER: Temporal Interaction Graph Embedding with Restarts](https://doi.org/10.1145/3543507.3583433)|Yao Zhang, Yun Xiong, Yongxiang Liao, Yiheng Sun, Yucheng Jin, Xuehao Zheng, Yangyong Zhu|Tencent Weixin Group, China; Fudan University, China|Temporal interaction graphs (TIGs), consisting of sequences of timestamped interaction events, are prevalent in fields like e-commerce and social networks. To better learn dynamic node embeddings that vary over time, researchers have proposed a series of temporal graph neural networks for TIGs. However, due to the entangled temporal and structural dependencies, existing methods have to process the sequence of events chronologically and consecutively to ensure node representations are up-to-date. This prevents existing models from parallelization and reduces their flexibility in industrial applications. To tackle the above challenge, in this paper, we propose TIGER, a TIG embedding model that can restart at any timestamp. We introduce a restarter module that generates surrogate representations acting as the warm initialization of node representations. By restarting from multiple timestamps simultaneously, we divide the sequence into multiple chunks and naturally enable the parallelization of the model. Moreover, in contrast to previous models that utilize a single memory unit, we introduce a dual memory module to better exploit neighborhood information and alleviate the staleness problem. Extensive experiments on four public datasets and one industrial dataset are conducted, and the results verify both the effectiveness and the efficiency of our work.|时间交互图(TIGs)由时间戳交互事件序列组成，在电子商务和社交网络等领域非常普遍。为了更好地学习随时间变化的动态节点嵌入，研究人员提出了一系列针对 TIG 的时间图神经网络。然而，由于时间和结构的依赖性，现有的方法必须按时间顺序和连续地处理事件序列，以确保节点表示是最新的。这阻止了现有模型的并行化，并降低了它们在工业应用程序中的灵活性。为了应对上述挑战，本文提出了一种 TIG 嵌入模型 TIGER，它可以在任意时间戳重新启动。我们引入了一个 restarter 模块，它生成代理表示作为节点表示的温初始化。通过同时从多个时间戳重新开始，我们将序列划分为多个块，自然而然地实现了模型的并行化。此外，相对于以往的单一存储器模型，我们引入了双存储器模块，以更好地利用邻域信息和缓解过时问题。对四个公共数据集和一个工业数据集进行了广泛的实验，实验结果验证了本文工作的有效性和高效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TIGER:+Temporal+Interaction+Graph+Embedding+with+Restarts)|0|
|[Expressive and Efficient Representation Learning for Ranking Links in Temporal Graphs](https://doi.org/10.1145/3543507.3583476)|Susheel Suresh, Mayank Shrivastava, Arko Mukherjee, Jennifer Neville, Pan Li|Microsoft Research, USA; Purdue University, USA; Microsoft, USA|Temporal graph representation learning (T-GRL) aims to learn representations that model how graph edges evolve over time. While recent works on T-GRL have improved link prediction accuracy in temporal settings, their methods optimize a point-wise loss function independently over future links rather than optimize jointly over a candidate set per node. In applications where resources (e.g., attention) are allocated based on ranking links by likelihood, the use of a ranking loss is preferred. However it is not straightforward to develop a T-GRL method to optimize a ranking loss due to a tradeoff between model expressivity and scalability. In this work, we address these issues and propose a Temporal Graph network for Ranking (TGRank), which significantly improves performance for link prediction tasks by (i) optimizing a list-wise loss for improved ranking, and (ii) incorporating a labeling approach designed to allow for efficient inference over the candidate set jointly, while provably boosting expressivity. We extensively evaluate TGRank over six real networks. TGRank outperforms the state-of-the-art baselines on average by 14.21%↑ (transductive) and 16.25% ↑ (inductive) in ranking metrics while being more efficient (up-to 65 × speed-up) to make inference on large networks.|时态图表示学习(T-GRL)的目的是学习模拟图边如何随时间演化的表示。尽管最近在 T-GRL 上的工作在时间设置上提高了链路预测的精度，但是他们的方法在未来链路上独立优化点损失函数，而不是在每个节点的候选集上联合优化。在应用程序中，资源(如注意力)的分配是基于可能性的排名链接，使用排名损失是首选。然而，由于模型表达性和可伸缩性之间的权衡，开发 T-GRL 方法来优化排名损失并不容易。在这项工作中，我们解决了这些问题，并提出了排名时态图网络(TGRank) ，它通过(i)优化改善排名的列表损失，以及(ii)结合标记方法，以便对候选集合进行有效的推理，同时可证明地提高表现力，从而显着改善链接预测任务的性能。我们广泛评估 TGRank 在六个实际网络。TGRank 在排序指标方面平均比最先进的基线表现出14.21% 惊(转导)和16.25% 惊(归纳)的优势，同时在大型网络上进行推理的效率更高(高达65倍加速)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Expressive+and+Efficient+Representation+Learning+for+Ranking+Links+in+Temporal+Graphs)|0|
|[Semi-Supervised Embedding of Attributed Multiplex Networks](https://doi.org/10.1145/3543507.3583485)|Ylli Sadikaj, Justus Rass, Yllka Velaj, Claudia Plant|Faculty of Computer Science, University of Vienna, Austria and ds:Univie, University of Vienna, Austria; Faculty of Computer Science, University of Vienna, Austria; Faculty of Computer Science, University of Vienna, Austria and UniVie Doctoral School Computer Science, University of Vienna, Austria|Complex information can be represented as networks (graphs) characterized by a large number of nodes, multiple types of nodes, and multiple types of relationships between them, i.e. multiplex networks. Additionally, these networks are enriched with different types of node features. We propose a Semi-supervised Embedding approach for Attributed Multiplex Networks (SSAMN), to jointly embed nodes, node attributes, and node labels of multiplex networks in a low dimensional space. Network embedding techniques have garnered research attention for real-world applications. However, most existing techniques solely focus on learning the node embeddings, and only a few learn class label embeddings. Our method assumes that we have different classes of nodes and that we know the class label of some, very few nodes for every class. Guided by this type of supervision, SSAMN learns a low-dimensional representation incorporating all information in a large labeled multiplex network. SSAMN integrates techniques from Spectral Embedding and Homogeneity Analysis to improve the embedding of nodes, node attributes, and node labels. Our experiments demonstrate that we only need very few labels per class in order to have a final embedding that preservers the information of the graph. To evaluate the performance of SSAMN, we run experiments on four real-world datasets. The results show that our approach outperforms state-of-the-art methods for downstream tasks such as semi-supervised node classification and node clustering.|复杂的信息可以用网络(图形)来表示，拥有属性包括大量的节点、多种类型的节点以及它们之间多种类型的关系，即多路网络。此外，这些网络丰富了不同类型的节点特征。提出了一种基于半监督嵌入的属性化多路网络(SSAMN)方法，在低维空间中联合嵌入多路网络的节点、节点属性和节点标签。网络嵌入技术已经成为现实应用领域的研究热点。然而，大多数现有的技术只关注于学习节点嵌入，只有少数学习类标签嵌入。我们的方法假设我们有不同的节点类，并且我们知道每个类的一些非常少的节点的类标签。在这种类型的监督指导下，SSAMN 学习了一种低维表示，将所有信息整合到一个大的标记多路网络中。SSAMN 集成了谱嵌入和均匀性分析技术，改进了节点、节点属性和节点标签的嵌入。我们的实验表明，我们只需要非常少的标签每个类，以便有一个最终的嵌入，保存图的信息。为了评估 SSAMN 的表现，我们在四个真实世界的数据集上进行了实验。结果表明，该方法在处理半监督节点分类和节点聚类等下游任务时，性能优于现有方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semi-Supervised+Embedding+of+Attributed+Multiplex+Networks)|0|
|[Search to Capture Long-range Dependency with Stacking GNNs for Graph Classification](https://doi.org/10.1145/3543507.3583486)|Lanning Wei, Zhiqiang He, Huan Zhao, Quanming Yao|4Paradigm. Inc, China; Institute of Computing Technology, Chinese Academy of Sciences, China and University of Chinese Academy of Sciences, China; Institute of Computing Technology, Chinese Academy of Science, China and Lenovo, China; Department of Electronic Engineering, Tsinghua University, China|In recent years, Graph Neural Networks (GNNs) have been popular in the graph classification task. Currently, shallow GNNs are more common due to the well-known over-smoothing problem facing deeper GNNs. However, they are sub-optimal without utilizing the information from distant nodes, i.e., the long-range dependencies. The mainstream methods in the graph classification task can extract the long-range dependencies either by designing the pooling operations or incorporating the higher-order neighbors, while they have evident drawbacks by modifying the original graph structure, which may result in information loss in graph structure learning. In this paper, by justifying the smaller influence of the over-smoothing problem in the graph classification task, we evoke the importance of stacking-based GNNs and then employ them to capture the long-range dependencies without modifying the original graph structure. To achieve this, two design needs are given for stacking-based GNNs, i.e., sufficient model depth and adaptive skip-connection schemes. By transforming the two design needs into designing data-specific inter-layer connections, we propose a novel approach with the help of neural architecture search (NAS), which is dubbed LRGNN (Long-Range Graph Neural Networks). Extensive experiments on five datasets show that the proposed LRGNN can achieve the best performance, and obtained data-specific GNNs with different depth and skip-connection schemes, which can better capture the long-range dependencies.|近年来，图形神经网络(GNN)在图形分类任务中得到了广泛的应用。目前，浅 GNN 更常见，由于众所周知的过度平滑问题所面临的深 GNN。然而，如果没有利用来自远程节点的信息(即远程依赖) ，它们就是次优的。图分类任务中的主流方法可以通过设计合并操作或合并高阶邻居来提取远程依赖关系，但通过修改原有的图结构存在明显的缺陷，可能导致图结构学习中的信息丢失。本文通过证明过平滑问题在图分类任务中的影响较小，引出了基于叠加的 GNN 的重要性，并利用它们在不改变原始图结构的情况下捕获长程依赖关系。为了实现这一目标，给出了基于叠加的 GNN 的两种设计需求，即充分的模型深度和自适应跳跃连接方案。通过将这两种设计需求转化为设计数据特定的层间连接，我们提出了一种神经结构搜索(NAS)的新方法，称为长程图形神经网络(LRGNN)。通过对5个数据集的大量实验表明，本文提出的 LRGNN 能够获得最好的性能，并且能够获得具有不同深度和跳跃连接方案的数据特定 GNN，能够更好地捕获远程依赖关系。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Search+to+Capture+Long-range+Dependency+with+Stacking+GNNs+for+Graph+Classification)|0|
|[Cut-matching Games for Generalized Hypergraph Ratio Cuts](https://doi.org/10.1145/3543507.3583539)|Nate Veldt|Texas A&M University, USA|Hypergraph clustering is a basic algorithmic primitive for analyzing complex datasets and systems characterized by multiway interactions, such as group email conversations, groups of co-purchased retail products, and co-authorship data. This paper presents a practical $O(\log n)$-approximation algorithm for a broad class of hypergraph ratio cut clustering objectives. This includes objectives involving generalized hypergraph cut functions, which allow a user to penalize cut hyperedges differently depending on the number of nodes in each cluster. Our method is a generalization of the cut-matching framework for graph ratio cuts, and relies only on solving maximum s-t flow problems in a special reduced graph. It is significantly faster than existing hypergraph ratio cut algorithms, while also solving a more general problem. In numerical experiments on various types of hypergraphs, we show that it quickly finds ratio cut solutions within a small factor of optimality.|Hypergraph 聚类是一种基本的算法原理，用于分析复杂的数据集和多拥有属性交互的系统，比如群组电子邮件对话、共同购买的零售产品组和合著者数据。本文提出了一个实用的 $o (log n) $- 近似演算法，用于一类广泛的超图比率削减聚类目标。这包括涉及广义超图割函数的目标，它允许用户根据每个簇中节点的数量对割超边进行不同的惩罚。该方法是图比割的割匹配框架的推广，仅依赖于求解特殊简化图中的最大 s-t 流问题。它明显快于现有的超图比率割算法，同时也解决了更一般的问题。通过对不同类型超图的数值实验，我们发现它能在一个小的最优性因子内快速找到比率割分解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cut-matching+Games+for+Generalized+Hypergraph+Ratio+Cuts)|0|
|[ApeGNN: Node-Wise Adaptive Aggregation in GNNs for Recommendation](https://doi.org/10.1145/3543507.3583530)|Dan Zhang, Yifan Zhu, Yuxiao Dong, Yuandong Wang, Wenzheng Feng, Evgeny Kharlamov, Jie Tang|Tsinghua University, China; Bosch Center for Artificial Intelligence, Germany|In recent years, graph neural networks (GNNs) have made great progress in recommendation. The core mechanism of GNNs-based recommender system is to iteratively aggregate neighboring information on the user-item interaction graph. However, existing GNNs treat users and items equally and cannot distinguish diverse local patterns of each node, which makes them suboptimal in the recommendation scenario. To resolve this challenge, we present a node-wise adaptive graph neural network framework ApeGNN. ApeGNN develops a node-wise adaptive diffusion mechanism for information aggregation, in which each node is enabled to adaptively decide its diffusion weights based on the local structure (e.g., degree). We perform experiments on six widely-used recommendation datasets. The experimental results show that the proposed ApeGNN is superior to the most advanced GNN-based recommender methods (up to 48.94%), demonstrating the effectiveness of node-wise adaptive aggregation.|近年来，图神经网络在推荐方面取得了很大的进展。基于 GNN 的推荐系统的核心机制是在用户-项目交互图上迭代地聚合相邻信息。然而，现有的 GNN 对用户和项目一视同仁，不能区分每个节点的不同本地模式，这使得它们在推荐场景中处于次优状态。为了解决这一问题，我们提出了一种节点自适应图神经网络框架 ApeGNN。ApeGNN 提出了一种基于节点的自适应信息聚合扩散机制，该机制允许每个节点根据局部结构(如度)自适应确定其扩散权重。我们在六个广泛使用的推荐数据集上进行实验。实验结果表明，该算法优于目前最先进的基于 GNN 的推荐方法(48.94%) ，证明了节点自适应聚集的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ApeGNN:+Node-Wise+Adaptive+Aggregation+in+GNNs+for+Recommendation)|0|
|[Multi-Modal Self-Supervised Learning for Recommendation](https://doi.org/10.1145/3543507.3583206)|Wei Wei, Chao Huang, Lianghao Xia, Chuxu Zhang|University of Hong Kong, Hong Kong; Brandeis University, USA; The University of Hong Kong, Hong Kong|The online emergence of multi-modal sharing platforms (eg, TikTok, Youtube) is powering personalized recommender systems to incorporate various modalities (eg, visual, textual and acoustic) into the latent user representations. While existing works on multi-modal recommendation exploit multimedia content features in enhancing item embeddings, their model representation capability is limited by heavy label reliance and weak robustness on sparse user behavior data. Inspired by the recent progress of self-supervised learning in alleviating label scarcity issue, we explore deriving self-supervision signals with effectively learning of modality-aware user preference and cross-modal dependencies. To this end, we propose a new Multi-Modal Self-Supervised Learning (MMSSL) method which tackles two key challenges. Specifically, to characterize the inter-dependency between the user-item collaborative view and item multi-modal semantic view, we design a modality-aware interactive structure learning paradigm via adversarial perturbations for data augmentation. In addition, to capture the effects that user's modality-aware interaction pattern would interweave with each other, a cross-modal contrastive learning approach is introduced to jointly preserve the inter-modal semantic commonality and user preference diversity. Experiments on real-world datasets verify the superiority of our method in offering great potential for multimedia recommendation over various state-of-the-art baselines. The implementation is released at: https://github.com/HKUDS/MMSSL.|多模式共享平台(如 TikTok、 Youtube)的在线出现为个性化推荐系统提供了动力，将各种模式(如视觉、文本和声学)纳入潜在用户表示。现有的多模态推荐方法利用多媒体内容特征增强项目嵌入，但其模型表示能力受到严重的标签依赖和对稀疏用户行为数据鲁棒性较差的限制。受近年来自我监督学习在缓解标签稀缺问题上的进展的启发，我们探讨了如何通过有效地学习模式感知的用户偏好和跨模式依赖来获得自我监督信号。为此，我们提出了一种新的多模态自主学习(MMSSL)方法，解决了两个关键的挑战。为了刻画用户项目协作视图和项目多模态语义视图之间的相互依赖关系，我们设计了一个基于模态感知的交互式结构学习范式。此外，为了捕捉用户感知情态的交互模式相互交织的影响，引入了一种跨情态对比学习方法，以共同保持多情态语义共性和用户偏好多样性。在现实世界数据集上的实验验证了该方法的优越性，在各种最先进的基线上为多媒体推荐提供了巨大的潜力。实施 https://github.com/hkuds/mmssl 如下:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Modal+Self-Supervised+Learning+for+Recommendation)|0|
|[Bootstrap Latent Representations for Multi-modal Recommendation](https://doi.org/10.1145/3543507.3583251)|Xin Zhou, Hongyu Zhou, Yong Liu, Zhiwei Zeng, Chunyan Miao, Pengwei Wang, Yuan You, Feijun Jiang|Nanyang Technological University, Singapore; Alibaba, China|This paper studies the multi-modal recommendation problem, where the item multi-modality information (e.g., images and textual descriptions) is exploited to improve the recommendation accuracy. Besides the user-item interaction graph, existing state-of-the-art methods usually use auxiliary graphs (e.g., user-user or item-item relation graph) to augment the learned representations of users and/or items. These representations are often propagated and aggregated on auxiliary graphs using graph convolutional networks, which can be prohibitively expensive in computation and memory, especially for large graphs. Moreover, existing multi-modal recommendation methods usually leverage randomly sampled negative examples in Bayesian Personalized Ranking (BPR) loss to guide the learning of user/item representations, which increases the computational cost on large graphs and may also bring noisy supervision signals into the training process. To tackle the above issues, we propose a novel self-supervised multi-modal recommendation model, dubbed BM3, which requires neither augmentations from auxiliary graphs nor negative samples. Specifically, BM3 first bootstraps latent contrastive views from the representations of users and items with a simple dropout augmentation. It then jointly optimizes three multi-modal objectives to learn the representations of users and items by reconstructing the user-item interaction graph and aligning modality features under both inter- and intra-modality perspectives. BM3 alleviates both the need for contrasting with negative examples and the complex graph augmentation from an additional target network for contrastive view generation. We show BM3 outperforms prior recommendation models on three datasets with number of nodes ranging from 20K to 200K, while achieving a 2-9X reduction in training time. Our code is available at https://github.com/enoche/BM3.|本文研究了多模态推荐问题，该问题利用项目的多模态信息(如图像和文本描述)来提高推荐的准确性。除了用户-项目交互图，现有的方法通常使用辅助图(例如，用户-用户或项目-项目关系图)来增强用户和/或项目的学习表示。这些表示通常使用图卷积网络在辅助图上进行传播和聚合，这在计算和存储方面是非常昂贵的，特别是对于大图。此外，现有的多模态推荐方法通常利用贝叶斯个性化排序(BPR)损失中随机抽样的负例子来指导用户/项目表示的学习，这增加了大图上的计算成本，也可能使噪声监督信号进入训练过程。为了解决上述问题，我们提出了一种新的自监督多模态推荐模型，称为 BM3，它不需要辅助图和负样本的增广。具体来说，BM3首先通过简单的辍学增强从用户和项目的表示中引导潜在的对比视图。然后，通过重构用户-项目交互图和在情态间和情态内对齐情态特征，联合优化三个多模态目标来学习用户和项目的表征。BM3减轻了与负面例子对比的需要，也减轻了从一个额外的目标网络生成对比视图的复杂图形增强。我们发现 BM3在三个数据集上的节点数从20K 到200K 不等，优于先前的推荐模型，同时实现了2-9倍的训练时间缩短。我们的代码可以在 https://github.com/enoche/bm3找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bootstrap+Latent+Representations+for+Multi-modal+Recommendation)|0|
|[Recommendation with Causality enhanced Natural Language Explanations](https://doi.org/10.1145/3543507.3583260)|Jingsen Zhang, Xu Chen, Jiakai Tang, Weiqi Shao, Quanyu Dai, Zhenhua Dong, Rui Zhang|Renmin University of China, China; Huawei Noah's Ark Lab, China; www.ruizhang.info, China|Explainable recommendation has recently attracted increasing attention from both academic and industry communities. Among different explainable strategies, generating natural language explanations is an important method, which can deliver more informative, flexible and readable explanations to facilitate better user decisions. Despite the effectiveness, existing models are mostly optimized based on the observed datasets, which can be skewed due to the selection or exposure bias. To alleviate this problem, in this paper, we formulate the task of explainable recommendation with a causal graph, and design a causality enhanced framework to generate unbiased explanations. More specifically, we firstly define an ideal unbiased learning objective, and then derive a tractable loss for the observational data based on the inverse propensity score (IPS), where the key is a sample re-weighting strategy for equalizing the loss and ideal objective in expectation. Considering that the IPS estimated from the sparse and noisy recommendation datasets can be inaccurate, we introduce a fault tolerant mechanism by minimizing the maximum loss induced by the sample weights near the IPS. For more comprehensive modeling, we further analyze and infer the potential latent confounders induced by the complex and diverse user personalities. We conduct extensive experiments by comparing with the state-of-the-art methods based on three real-world datasets to demonstrate the effectiveness of our method.|可解释的建议最近引起了学术界和工业界越来越多的关注。在不同的解释策略中，生成自然语言解释是一种重要的方法，它可以提供更多的信息，灵活和可读的解释，以便于更好的用户决策。尽管有效，现有的模型大多是优化的基础上观察数据集，这可能会由于选择或曝光偏差。为了解决这一问题，本文利用因果图构造了可解释推荐任务，并设计了一个因果增强框架来生成无偏解释。更具体地说，我们首先定义一个理想的无偏学习目标，然后推导出一个基于逆倾向评分(IPS)的观测数据易处理的损失，其中的关键是一个样本重新加权策略来均衡损失和期望的理想目标。针对由稀疏和噪声推荐数据集估计的 IPS 可能不准确的问题，我们引入了一种容错机制，使 IPS 附近的样本权重引起的最大损失最小。为了更全面的建模，我们进一步分析和推断潜在的潜在混杂因素引起的复杂和多样的用户个性。为了验证该方法的有效性，我们在三个实际数据集上进行了广泛的实验，并与现有的方法进行了比较。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recommendation+with+Causality+enhanced+Natural+Language+Explanations)|0|
|[Two-Stage Constrained Actor-Critic for Short Video Recommendation](https://doi.org/10.1145/3543507.3583259)|Qingpeng Cai, Zhenghai Xue, Chi Zhang, Wanqi Xue, Shuchang Liu, Ruohan Zhan, Xueliang Wang, Tianyou Zuo, Wentao Xie, Dong Zheng, Peng Jiang, Kun Gai|Hong Kong University of Science and Technology, China; Kuaishou Technology, China; Unaffiliated, China|The wide popularity of short videos on social media poses new opportunities and challenges to optimize recommender systems on the video-sharing platforms. Users sequentially interact with the system and provide complex and multi-faceted responses, including watch time and various types of interactions with multiple videos. One the one hand, the platforms aims at optimizing the users' cumulative watch time (main goal) in long term, which can be effectively optimized by Reinforcement Learning. On the other hand, the platforms also needs to satisfy the constraint of accommodating the responses of multiple user interactions (auxiliary goals) such like, follow, share etc. In this paper, we formulate the problem of short video recommendation as a Constrained Markov Decision Process (CMDP). We find that traditional constrained reinforcement learning algorithms can not work well in this setting. We propose a novel two-stage constrained actor-critic method: At stage one, we learn individual policies to optimize each auxiliary signal. At stage two, we learn a policy to (i) optimize the main signal and (ii) stay close to policies learned at the first stage, which effectively guarantees the performance of this main policy on the auxiliaries. Through extensive offline evaluations, we demonstrate effectiveness of our method over alternatives in both optimizing the main goal as well as balancing the others. We further show the advantage of our method in live experiments of short video recommendations, where it significantly outperforms other baselines in terms of both watch time and interactions. Our approach has been fully launched in the production system to optimize user experiences on the platform.|短视频在社交媒体上的广泛流行为优化视频共享平台上的推荐系统带来了新的机遇和挑战。用户按顺序与系统互动，并提供复杂和多方面的反应，包括观看时间和与多个视频的各种类型的互动。一方面，这些平台旨在长期优化用户的累计观看时间(主要目标) ，这可以通过强化学习有效地优化。另一方面，平台还需要满足适应多用户交互(辅助目标)的响应约束，如跟踪、共享等。在这篇文章中，我们将短视频推荐问题描述为一个约束马可夫决策过程(CMDP)。我们发现传统的约束强化学习算法在这种情况下不能很好地工作。我们提出了一种新的两阶段约束行为者-评论方法: 在第一阶段，我们学习个体策略来优化每个辅助信号。在第二阶段，我们学习了一个策略来(i)优化主信号和(ii)紧跟在第一阶段学到的策略，这有效地保证了这个主策略在辅助系统上的性能。通过广泛的离线评估，我们证明了我们的方法在优化主要目标和平衡其他方面的有效性。我们进一步展示了我们的方法在短视频推荐的现场实验中的优势，在观看时间和交互方面显著优于其他基准。我们的方法已经在生产系统中全面推出，以优化平台上的用户体验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Two-Stage+Constrained+Actor-Critic+for+Short+Video+Recommendation)|0|
|[Robust Recommendation with Adversarial Gaussian Data Augmentation](https://doi.org/10.1145/3543507.3583273)|Zhenlei Wang, Xu Chen|Gaoling School of Artificial Intelligence, Renmin university of China, China|Recommender system holds the promise of accurately understanding and estimating the user preferences. However, due to the extremely sparse user-item interactions, the learned recommender models can be less robust and sensitive to the highly dynamic user preferences and easily changed recommendation environments. To alleviate this problem, in this paper, we propose a simple yet effective robust recommender framework by generating additional samples from the Gaussian distributions. In specific, we design two types of data augmentation strategies. For the first one, we directly produce the data based on the original samples, where we simulate the generation process in the latent space. For the second one, we firstly change the original samples towards the direction of maximizing the loss function, and then produce the data based on the altered samples to make more effective explorations. Based on both of the above strategies, we leverage adversarial training to optimize the recommender model with the generated data which can achieve the largest losses. In addition, we theoretically analyze our framework, and find that the above two data augmentation strategies equal to impose a gradient based regularization on the original recommender models. We conduct extensive experiments based on six real-world datasets to demonstrate the effectiveness of our framework.|推荐系统有希望准确理解和估计用户的偏好。然而，由于用户与项目之间的交互非常稀少，所学习的推荐模型可能对高度动态的用户偏好和容易更改的推荐环境不太健壮和敏感。为了解决这个问题，本文提出了一个简单而有效的鲁棒推荐框架，通过从高斯分布生成额外的样本。具体来说，我们设计了两种类型的数据增强策略。对于第一种方法，我们直接在原始样本的基础上生成数据，模拟潜在空间中的生成过程。第二种方法首先将原始样本向损失函数最大化方向改变，然后根据改变后的样本生成数据，进行更有效的探索。基于上述两种策略，我们利用对抗性训练来优化推荐模型，生成的数据可以达到最大的损失。此外，我们还从理论上分析了我们的框架，发现上述两种数据增强策略相当于在原有的推荐模型上加入了基于梯度的正则化。我们基于六个真实世界的数据集进行了广泛的实验，以证明我们的框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Recommendation+with+Adversarial+Gaussian+Data+Augmentation)|0|
|[Anti-FakeU: Defending Shilling Attacks on Graph Neural Network based Recommender Model](https://doi.org/10.1145/3543507.3583289)|Xiaoyu You, Chi Li, Daizong Ding, Mi Zhang, Fuli Feng, Xudong Pan, Min Yang|University of Science and Technology of China, CCCD Key Lab of Ministry of Culture and Tourism, China; Fudan University, School of Computer Science, China|Graph neural network (GNN) based recommendation models are observed to be more vulnerable against carefully-designed malicious records injected into the system, i.e., shilling attacks, which manipulate the recommendation to common users and therefore impair user trust. In this paper, we for the first time conduct a systematic study on the vulnerability of GNN based recommendation model against the shilling attack. With the aid of theoretical analysis, we attribute the root cause of the vulnerability to its neighborhood aggregation mechanism, which could make the negative impact of attacks propagate rapidly in the system. To restore the robustness of GNN based recommendation model, the key factor lies in detecting malicious records in the system and preventing the propagation of misinformation. To this end, we construct a user-user graph to capture the patterns of malicious behaviors and design a novel GNN based detector to identify fake users. Furthermore, we develop a data augmentation strategy and a joint learning paradigm to train the recommender model and the proposed detector. Extensive experiments on benchmark datasets validate the enhanced robustness of the proposed method in resisting various types of shilling attacks and identifying fake users, e.g., our proposed method fully mitigating the impact of popularity attacks on target items up to , and improving the accuracy of detecting fake users on the Gowalla dataset by .|基于图神经网络(GNN)的推荐模型更容易受到注入系统的精心设计的恶意记录(即先令攻击)的攻击，这些恶意记录操纵普通用户的推荐，从而损害用户的信任。本文首次对基于 GNN 的推荐模型在面对先令攻击时的脆弱性进行了系统的研究。在理论分析的基础上，将易受攻击的根本原因归结为其邻域聚合机制，使得攻击的负面影响在系统中迅速传播。要恢复基于 GNN 的推荐模型的鲁棒性，关键在于检测系统中的恶意记录，防止错误信息的传播。为此，我们构造了一个用户-用户图来捕捉恶意行为的模式，并设计了一种新的基于 GNN 的检测器来识别虚假用户。此外，我们发展了一个数据增强策略和一个联合学习范式来训练推荐模型和建议的检测器。基准数据集的大量实验验证了该方法在抵御各种先令攻击和识别假用户方面的增强鲁棒性，例如，我们提出的方法充分减轻了流行攻击对目标项的影响，并通过以下方法提高了 Gowalla 数据集检测假用户的准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Anti-FakeU:+Defending+Shilling+Attacks+on+Graph+Neural+Network+based+Recommender+Model)|0|
|[Automated Self-Supervised Learning for Recommendation](https://doi.org/10.1145/3543507.3583336)|Lianghao Xia, Chao Huang, Chunzhen Huang, Kangyi Lin, Tao Yu, Ben Kao|Tencent, China; The University of Hong Kong, Hong Kong|Graph neural networks (GNNs) have emerged as the state-of-the-art paradigm for collaborative filtering (CF). To improve the representation quality over limited labeled data, contrastive learning has attracted attention in recommendation and benefited graph-based CF model recently. However, the success of most contrastive methods heavily relies on manually generating effective contrastive views for heuristic-based data augmentation. This does not generalize across different datasets and downstream recommendation tasks, which is difficult to be adaptive for data augmentation and robust to noise perturbation. To fill this crucial gap, this work proposes a unified Automated Collaborative Filtering (AutoCF) to automatically perform data augmentation for recommendation. Specifically, we focus on the generative self-supervised learning framework with a learnable augmentation paradigm that benefits the automated distillation of important self-supervised signals. To enhance the representation discrimination ability, our masked graph autoencoder is designed to aggregate global information during the augmentation via reconstructing the masked subgraph structures. Experiments and ablation studies are performed on several public datasets for recommending products, venues, and locations. Results demonstrate the superiority of AutoCF against various baseline methods. We release the model implementation at https://github.com/HKUDS/AutoCF.|图形神经网络(GNN)已经成为最先进的协同过滤(CF)模式。为了提高有限标记数据的表示质量，对比学习近年来受到推荐界的关注，并受益于基于图的 CF 模型。然而，大多数对比方法的成功在很大程度上依赖于手工生成有效的对比视图，用于基于启发式的数据增强。这不能在不同的数据集和下游推荐任务之间推广，这对于数据增强和抗噪声干扰是很难自适应的。为了填补这个关键的空白，这项工作提出了一个统一的自动化协同过滤(AutoCF)来自动执行数据增强的推荐。具体来说，我们重点研究了具有可学习增强范式的生成式自监督学习框架，该框架有利于自动提取重要的自监督信号。为了提高表示识别能力，我们设计了掩码自动编码器，通过重构掩码子图结构来聚集增强过程中的全局信息。实验和烧蚀研究进行了几个公共数据集推荐产品，场所和地点。结果表明，AutoCF 方法与各种基线方法相比具有优越性。我们在 https://github.com/hkuds/autocf 发布模型实现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automated+Self-Supervised+Learning+for+Recommendation)|0|
|[AutoDenoise: Automatic Data Instance Denoising for Recommendations](https://doi.org/10.1145/3543507.3583339)|Weilin Lin, Xiangyu Zhao, Yejing Wang, Yuanshao Zhu, Wanyu Wang|City University of Hong Kong, Hong Kong and Southern University of Science and Technology, China; City University of Hong Kong, Hong Kong|Historical user-item interaction datasets are essential in training modern recommender systems for predicting user preferences. However, the arbitrary user behaviors in most recommendation scenarios lead to a large volume of noisy data instances being recorded, which cannot fully represent their true interests. While a large number of denoising studies are emerging in the recommender system community, all of them suffer from highly dynamic data distributions. In this paper, we propose a Deep Reinforcement Learning (DRL) based framework, AutoDenoise, with an Instance Denoising Policy Network, for denoising data instances with an instance selection manner in deep recommender systems. To be specific, AutoDenoise serves as an agent in DRL to adaptively select noise-free and predictive data instances, which can then be utilized directly in training representative recommendation models. In addition, we design an alternate two-phase optimization strategy to train and validate the AutoDenoise properly. In the searching phase, we aim to train the policy network with the capacity of instance denoising; in the validation phase, we find out and evaluate the denoised subset of data instances selected by the trained policy network, so as to validate its denoising ability. We conduct extensive experiments to validate the effectiveness of AutoDenoise combined with multiple representative recommender system models.|历史用户项目交互数据集对于培训现代推荐系统来预测用户偏好是必不可少的。然而，在大多数推荐场景中，任意的用户行为会导致大量有噪声的数据实例被记录下来，而这些数据实例并不能完全代表用户的真实兴趣。虽然在推荐系统社区中出现了大量的去噪研究，但所有这些研究都受到高度动态数据分布的影响。在这篇文章中，我们提出了一个基于深度强化学习的框架，AutoDenoise，和一个实例去噪策略网络，用于在深度推荐系统中用实例选择的方式去除数据实例。具体来说，自动去噪作为 DRL 中的一个代理，自适应地选择无噪声和预测数据实例，然后可以直接用于训练代表性的推荐模型。此外，我们还设计了一个交替的两阶段优化策略来训练和验证自动去噪的正确性。在搜索阶段，我们的目标是训练具有实例去噪能力的策略网络，在验证阶段，我们找出并评估训练后的策略网络选择的数据实例的去噪子集，以验证其去噪能力。我们进行了广泛的实验，以验证自动去噪与多个具有代表性的推荐系统模型相结合的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoDenoise:+Automatic+Data+Instance+Denoising+for+Recommendations)|0|
|[AutoS2AE: Automate to Regularize Sparse Shallow Autoencoders for Recommendation](https://doi.org/10.1145/3543507.3583349)|Rui Fan, Yuanhao Pu, Jin Chen, Zhihao Zhu, Defu Lian, Enhong Chen|University of Electronic Science and Technology of China, China; School of Computer Science, School of Data Science, University of Science and Technology of China, China and State Key Laboratory of Cognitive Intelligence, China; School of Data Science, University of Science and Technology of China, China; School of Computer Science, University of Science and Technology of China, China|The Embarrassingly Shallow Autoencoders (EASE and SLIM) are strong recommendation methods based on implicit feedback, compared to competing methods like iALS and VAE-CF. However, EASE suffers from several major shortcomings. First, the training and inference of EASE can not scale with the increasing number of items since it requires storing and inverting a large dense matrix; Second, though its optimization objective – the square loss– can yield a closed-form solution, it is not consistent with recommendation goal – predicting a personalized ranking on a set of items, so that its performance is far from optimal w.r.t ranking-oriented recommendation metrics. Finally, the regularization coefficients are sensitive w.r.t recommendation accuracy and vary a lot across different datasets, so the fine-tuning of these parameters is important yet time-consuming. To improve training and inference efficiency, we propose a Similarity-Structure Aware Shallow Autoencoder on top of three similarity structures, including Co-Occurrence, KNN and NSW. We then optimize the model with a weighted square loss, which is proven effective for ranking-based recommendation but still capable of deriving closed-form solutions. However, the weight in the loss can not be learned in the training set and is similarly sensitive w.r.t the accuracy to regularization coefficients. To automatically tune the hyperparameters, we design two validation losses on the validation set for guidance, and update the hyperparameters with the gradient of the validation losses. We finally evaluate the proposed method on multiple real-world datasets and show that it outperforms seven competing baselines remarkably, and verify the effectiveness of each part in the proposed method.|令人尴尬的浅层自动编码器(EASE 和 SLIM)是基于隐式反馈的强有力的推荐方法，与 iALS 和 VAE-CF 等竞争方法相比。然而，EASE 有几个主要的缺点。首先，EASE 的训练和推理不能随着项目数量的增加而扩展，因为它需要存储和反演一个大的密集矩阵; 其次，虽然它的优化目标-平方损失-可以产生一个封闭形式的解决方案，但它不符合推荐目标-预测一组项目的个性化排名，因此它的性能远远不是最优的面向网络排名的推荐指标。最后，正则化系数对推荐精度非常敏感，并且在不同的数据集上有很大的差异，因此对这些参数进行微调非常重要，但也非常耗时。为了提高训练和推理效率，本文提出了一种基于共现、 KNN 和 NSW 三种相似结构的相似结构感知浅层自动编码器。然后，我们用加权平方损失优化模型，这被证明是有效的排名为基础的推荐，但仍然能够导出闭合形式的解决方案。然而，损失中的权重不能在训练集中学习，并且对正则化系数的精度同样敏感。为了自动调整超参数，我们在验证集上设计了两个验证损失作为指导，并用验证损失的梯度来更新超参数。最后，我们对该方法在多个实际数据集上的性能进行了评估，结果表明该方法明显优于7个竞争基线，并验证了该方法各部分的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoS2AE:+Automate+to+Regularize+Sparse+Shallow+Autoencoders+for+Recommendation)|0|
|[Improving Recommendation Fairness via Data Augmentation](https://doi.org/10.1145/3543507.3583341)|Lei Chen, Le Wu, Kun Zhang, Richang Hong, Defu Lian, Zhiqiang Zhang, Jun Zhou, Meng Wang|University of Science and Technology of China, China; Ant Group, China; Hefei University of Technology, China; Hefei University of Technology, China and Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, China|Collaborative filtering based recommendation learns users' preferences from all users' historical behavior data, and has been popular to facilitate decision making. R Recently, the fairness issue of recommendation has become more and more essential. A recommender system is considered unfair when it does not perform equally well for different user groups according to users' sensitive attributes~(e.g., gender, race). Plenty of methods have been proposed to alleviate unfairness by optimizing a predefined fairness goal or changing the distribution of unbalanced training data. However, they either suffered from the specific fairness optimization metrics or relied on redesigning the current recommendation architecture. In this paper, we study how to improve recommendation fairness from the data augmentation perspective. The recommendation model amplifies the inherent unfairness of imbalanced training data. We augment imbalanced training data towards balanced data distribution to improve fairness. The proposed framework is generally applicable to any embedding-based recommendation, and does not need to pre-define a fairness metric. Extensive experiments on two real-world datasets clearly demonstrate the superiority of our proposed framework. We publish the source code at https://github.com/newlei/FDA.|基于协同过滤的推荐从所有用户的历史行为数据中了解用户的偏好，并且已经流行起来以促进决策制定。近年来，推荐的公平性问题变得越来越重要。根据用户的敏感属性 ~ (如性别、种族) ，一个推荐系统在不同用户组中的表现不尽相同，这被认为是不公平的。通过优化预定义的公平目标或改变不平衡训练数据的分布，已经提出了许多缓解不公平现象的方法。然而，它们要么受到特定公平性优化指标的影响，要么依赖于重新设计当前的推荐体系结构。本文从数据增强的角度研究如何提高推荐公平性。推荐模型放大了不平衡训练数据固有的不公平性。为了提高训练数据的公平性，我们对不平衡的训练数据进行扩充以达到平衡的数据分布。提出的框架通常适用于任何基于嵌入的建议，并且不需要预先定义公平性度量。在两个实际数据集上的大量实验清楚地表明了我们提出的框架的优越性。我们在 https://github.com/newlei/fda 公布源代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Recommendation+Fairness+via+Data+Augmentation)|0|
|[Robust Preference-Guided Denoising for Graph based Social Recommendation](https://doi.org/10.1145/3543507.3583374)|Yuhan Quan, Jingtao Ding, Chen Gao, Lingling Yi, Depeng Jin, Yong Li|Tsinghua University, China; Tencent, China|Graph Neural Network(GNN) based social recommendation models improve the prediction accuracy of user preference by leveraging GNN in exploiting preference similarity contained in social relations. However, in terms of both effectiveness and efficiency of recommendation, a large portion of social relations can be redundant or even noisy, e.g., it is quite normal that friends share no preference in a certain domain. Existing models do not fully solve this problem of relation redundancy and noise, as they directly characterize social influence over the full social network. In this paper, we instead propose to improve graph based social recommendation by only retaining the informative social relations to ensure an efficient and effective influence diffusion, i.e., graph denoising. Our designed denoising method is preference-guided to model social relation confidence and benefits user preference learning in return by providing a denoised but more informative social graph for recommendation models. Moreover, to avoid interference of noisy social relations, it designs a self-correcting curriculum learning module and an adaptive denoising strategy, both favoring highly-confident samples. Experimental results on three public datasets demonstrate its consistent capability of improving two state-of-the-art social recommendation models by robustly removing 10-40% of original relations. We release the source code at https://github.com/tsinghua-fib-lab/Graph-Denoising-SocialRec.|基于图神经网络(GNN)的社会推荐模型通过利用社会关系中包含的偏好相似性来提高用户偏好的预测精度。然而，就推荐的有效性和效率而言，很大一部分社会关系可能是冗余的，甚至是嘈杂的，例如，朋友在某个领域没有共同的偏好是很正常的。现有的模型并没有完全解决关系冗余和噪声的问题，因为它们直接表征了社会对整个社会网络的影响。在本文中，我们提出改进基于图的社会推荐，只保留信息性的社会关系，以确保有效和有效的影响扩散，即图去噪。我们设计的去噪方法是偏好引导的社会关系模型的信心和有益的用户偏好学习的回报，提供了一个去噪，但更多的信息社会图的推荐模型。同时，为了避免社会关系噪声的干扰，设计了自校正课程学习模块和自适应去噪策略，两者都有利于高自信样本。在三个公共数据集上的实验结果表明，该算法能够通过鲁棒地去除10-40% 的原始关系来改进两个最新的社会推荐模型。我们在 https://github.com/tsinghua-fib-lab/graph-denoising-socialrec 公布源代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Preference-Guided+Denoising+for+Graph+based+Social+Recommendation)|0|
|[Few-shot News Recommendation via Cross-lingual Transfer](https://doi.org/10.1145/3543507.3583383)|Taicheng Guo, Lu Yu, Basem Shihada, Xiangliang Zhang||The cold-start problem has been commonly recognized in recommendation systems and studied by following a general idea to leverage the abundant interaction records of warm users to infer the preference of cold users. However, the performance of these solutions is limited by the amount of records available from warm users to use. Thus, building a recommendation system based on few interaction records from a few users still remains a challenging problem for unpopular or early-stage recommendation platforms. This paper focuses on solving the few-shot recommendation problem for news recommendation based on two observations. First, news at diferent platforms (even in diferent languages) may share similar topics.Second, the user preference over these topics is transferable across diferent platforms. Therefore, we propose to solve the few-shot news recommendation problem by transferring the user-news preference from a many-shot source domain to a few-shot target domain. To bridge two domainsthat are even in diferent languages and without any overlapping users and news, we propose a novel unsupervised cross-lingual transfer model as the news encoder that aligns semantically similar news in two domains. A user encoder is constructed on top of the aligned news encoding and transfers the user preference from the source to target domain. Experimental results on two real-world news recommendation datasets show the superior performance of our proposed method on addressing few-shot news recommendation, comparing to the baselines. The source code can be found at https://github.com/taichengguo/Few-shot-NewsRec .|冷启动问题在推荐系统中已经得到了广泛的认可，并且通过利用热用户丰富的交互记录来推断冷用户的偏好这一基本思想进行了研究。但是，这些解决方案的性能受限于可供暖用户使用的记录数量。因此，对于不受欢迎或处于早期阶段的推荐平台来说，建立一个基于少量用户交互记录的推荐系统仍然是一个具有挑战性的问题。本文主要研究基于两个观察值的新闻推荐中的少镜头推荐问题。首先，不同平台的新闻(甚至是不同语言的新闻)可能会有相似的话题。其次，用户对这些主题的偏好可以跨不同的平台传递。因此，我们提出通过将用户新闻偏好从多镜头源域转移到少镜头目标域来解决少镜头新闻推荐问题。为了在两个不同语言的领域之间架起一座桥梁，并且没有任何重叠的用户和新闻，我们提出了一种新的无监督跨语言传输模型作为新闻编码器，它将两个领域中语义相似的新闻进行对齐。用户编码器构造在对齐的新闻编码之上，并将用户首选项从源传输到目标域。在两个实际新闻推荐数据集上的实验结果表明，与基线相比，本文提出的方法在处理少镜头新闻推荐方面具有更好的性能。源代码可以在 https://github.com/taichengguo/few-shot-newsrec 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Few-shot+News+Recommendation+via+Cross-lingual+Transfer)|0|
|[Show Me The Best Outfit for A Certain Scene: A Scene-aware Fashion Recommender System](https://doi.org/10.1145/3543507.3583435)|Tangwei Ye, Liang Hu, Qi Zhang, Zhong Yuan Lai, Usman Naseem, Dora D. Liu|Tongji University, China and DeepBlue Academy of Sciences, China; DeepBlue Academy of Sciences, China; University of Sydney, Australia; DeepBlue Academy of Sciences, China and BirenTech Research, China; University of Technology Sydney, Australia and DeepBlue Academy of Sciences, China|Fashion recommendation (FR) has received increasing attention in the research of new types of recommender systems. Existing fashion recommender systems (FRSs) typically focus on clothing item suggestions for users in three scenarios: 1) how to best recommend fashion items preferred by users; 2) how to best compose a complete outfit, and 3) how to best complete a clothing ensemble. However, current FRSs often overlook an important aspect when making FR, that is, the compatibility of the clothing item or outfit recommendations is highly dependent on the scene context. To this end, we propose the scene-aware fashion recommender system (SAFRS), which uncovers a hitherto unexplored avenue where scene information is taken into account when constructing the FR model. More specifically, our SAFRS addresses this problem by encoding scene and outfit information in separation attention encoders and then fusing the resulting feature embeddings via a novel scene-aware compatibility score function. Extensive qualitative and quantitative experiments are conducted to show that our SAFRS model outperforms all baselines for every evaluated metric.|时尚推荐(FR)在新型推荐系统的研究中受到越来越多的关注。现有的时尚推荐系统(FRSs)主要集中在三个场景中为用户提供服装项目建议: 1)如何最好地推荐用户喜欢的时尚项目; 2)如何最好地组合一套完整的服装; 3)如何最好地完成一套服装。然而，目前的 FRS 在制作 FR 时往往忽略了一个重要方面，即服装项目或服装的兼容性建议高度依赖于场景上下文。为此，我们提出了场景感知时尚推荐系统(SAFRS) ，它揭示了一个迄今为止尚未探索的途径，在构建 FR 模型时，场景信息被考虑在内。更具体地说，我们的 SAFRS 通过在分离注意力编码器中编码场景和装备信息，然后通过一种新颖的场景感知兼容性评分函数融合所得到的特征嵌入来解决这个问题。广泛的定性和定量实验表明，我们的 SAFRS 模型优于所有基线的每一个评估指标。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Show+Me+The+Best+Outfit+for+A+Certain+Scene:+A+Scene-aware+Fashion+Recommender+System)|0|
|[Invariant Collaborative Filtering to Popularity Distribution Shift](https://doi.org/10.1145/3543507.3583461)|An Zhang, Jingnan Zheng, Xiang Wang, Yancheng Yuan, TatSeng Chua|Sea-NExT Joint Lab, National University of Singapore, Singapore; The Hong Kong Polytechnic University, Hong Kong; University of Science and Technology of China, China and Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, China; National University of Singapore, Singapore|Collaborative Filtering (CF) models, despite their great success, suffer from severe performance drops due to popularity distribution shifts, where these changes are ubiquitous and inevitable in real-world scenarios. Unfortunately, most leading popularity debiasing strategies, rather than tackling the vulnerability of CF models to varying popularity distributions, require prior knowledge of the test distribution to identify the degree of bias and further learn the popularity-entangled representations to mitigate the bias. Consequently, these models result in significant performance benefits in the target test set, while dramatically deviating the recommendation from users' true interests without knowing the popularity distribution in advance. In this work, we propose a novel learning framework, Invariant Collaborative Filtering (InvCF), to discover disentangled representations that faithfully reveal the latent preference and popularity semantics without making any assumption about the popularity distribution. At its core is the distillation of unbiased preference representations (i.e., user preference on item property), which are invariant to the change of popularity semantics, while filtering out the popularity feature that is unstable or outdated. Extensive experiments on five benchmark datasets and four evaluation settings (i.e., synthetic long-tail, unbiased, temporal split, and out-of-distribution evaluations) demonstrate that InvCF outperforms the state-of-the-art baselines in terms of popularity generalization ability on real recommendations. Visualization studies shed light on the advantages of InvCF for disentangled representation learning. Our codes are available at https://github.com/anzhang314/InvCF.|协同过滤(CF)模型尽管取得了巨大的成功，但由于受欢迎程度的分布变化，性能严重下降，这些变化在现实世界中无处不在，也是不可避免的。不幸的是，大多数领先的流行去偏策略，而不是解决 CF 模型对不同流行分布的脆弱性，需要事先了解测试分布以确定偏倚程度，并进一步学习流行纠缠表示以减轻偏倚。因此，这些模型在目标测试集中产生了显著的性能效益，同时在不事先知道用户流行度分布的情况下，大大偏离了用户的真实兴趣。在这项工作中，我们提出了一个新的学习框架，不变协同过滤(InvCF) ，发现分离的表征，忠实地揭示潜在的偏好和流行语义，而不作任何假设的流行分布。其核心是无偏好的偏好表示(即，用户对项目属性的偏好)的精华，这些偏好对流行语义的变化是不变的，同时过滤掉不稳定或过时的流行特征。对五个基准数据集和四个评估设置(即合成长尾，无偏见，时间分割和分布外评估)的广泛实验表明，InvCF 在真实推荐的普及概括能力方面优于最先进的基线。可视化研究揭示了 InvCF 在分离表征学习中的优势。我们的密码可以在 https://github.com/anzhang314/invcf 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Invariant+Collaborative+Filtering+to+Popularity+Distribution+Shift)|0|
|[Code Recommendation for Open Source Software Developers](https://doi.org/10.1145/3543507.3583503)|Yiqiao Jin, Yunsheng Bai, Yanqiao Zhu, Yizhou Sun, Wei Wang|Georgia Institute of Technology, USA; University of California, Los Angeles, USA|Open Source Software (OSS) is forming the spines of technology infrastructures, attracting millions of talents to contribute. Notably, it is challenging and critical to consider both the developers' interests and the semantic features of the project code to recommend appropriate development tasks to OSS developers. In this paper, we formulate the novel problem of code recommendation, whose purpose is to predict the future contribution behaviors of developers given their interaction history, the semantic features of source code, and the hierarchical file structures of projects. Considering the complex interactions among multiple parties within the system, we propose CODER, a novel graph-based code recommendation framework for open source software developers. CODER jointly models microscopic user-code interactions and macroscopic user-project interactions via a heterogeneous graph and further bridges the two levels of information through aggregation on file-structure graphs that reflect the project hierarchy. Moreover, due to the lack of reliable benchmarks, we construct three large-scale datasets to facilitate future research in this direction. Extensive experiments show that our CODER framework achieves superior performance under various experimental settings, including intra-project, cross-project, and cold-start recommendation. We will release all the datasets, code, and utilities for data retrieval upon the acceptance of this work.|开源软件(OSS)正在形成技术基础设施的脊梁，吸引了数以百万计的人才贡献。值得注意的是，同时考虑开发人员的兴趣和项目代码的语义特性，以便向 OSS 开发人员推荐适当的开发任务，这是一项具有挑战性和关键性的工作。本文提出了一个新的代码推荐问题，其目的是根据开发人员的交互历史、源代码的语义特征以及项目的层次化文件结构来预测开发人员未来的贡献行为。考虑到系统中多方之间的复杂交互，我们提出了一种新的基于图的开源软件开发者代码推荐框架 CODER。CODER 通过异构图联合建模微观用户-代码交互和宏观用户-项目交互，并通过聚合反映项目层次结构的文件结构图进一步桥接两个层次的信息。此外，由于缺乏可靠的基准，我们建立了三个大规模的数据集，以方便未来在这方面的研究。大量的实验表明，我们的 CODER 框架在不同的实验环境下，包括项目内、项目间和冷启动推荐，都取得了较好的性能。我们将发布所有的数据集，代码和实用程序的数据检索后，接受这项工作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Code+Recommendation+for+Open+Source+Software+Developers)|0|
|[pFedPrompt: Learning Personalized Prompt for Vision-Language Models in Federated Learning](https://doi.org/10.1145/3543507.3583518)|Tao Guo, Song Guo, Junxiao Wang|The Hong Kong Polytechnic University, Hong Kong|Pre-trained vision-language models like CLIP show great potential in learning representations that capture latent characteristics of users. A recently proposed method called Contextual Optimization (CoOp) introduces the concept of training prompt for adapting pre-trained vision-language models. Given the lightweight nature of this method, researchers have migrated the paradigm from centralized to decentralized system to innovate the collaborative training framework of Federated Learning (FL). However, current prompt training in FL mainly focuses on modeling user consensus and lacks the adaptation to user characteristics, leaving the personalization of prompt largely under-explored. Researches over the past few years have applied personalized FL (pFL) approaches to customizing models for heterogeneous users. Unfortunately, we find that with the variation of modality and training behavior, directly applying the pFL methods to prompt training leads to insufficient personalization and performance. To bridge the gap, we present pFedPrompt, which leverages the unique advantage of multimodality in vision-language models by learning user consensus from linguistic space and adapting to user characteristics in visual space in a non-parametric manner. Through this dual collaboration, the learned prompt will be fully personalized and aligned to the user’s local characteristics. We conduct extensive experiments across various datasets under the FL setting with statistical heterogeneity. The results demonstrate the superiority of our pFedPrompt against the alternative approaches with robust performance.|像 CLIP 这样预先训练好的视觉语言模型在捕捉用户潜在特征的学习表示方面显示出巨大的潜力。最近提出的上下文优化(CoOp)方法引入了训练提示符的概念来适应预先训练好的视觉语言模型。考虑到该方法的轻量级特性，研究人员将该模式从集中式系统迁移到分散式系统，以创新联邦学习(FL)的协同训练框架。然而，目前外语快速教学主要侧重于建立用户共识模型，缺乏对用户特征的适应性，使得个性化快速教学在很大程度上缺乏探索。过去几年的研究已经将个性化 FL (pFL)方法应用于异构用户的定制模型。不幸的是，我们发现，随着模式和训练行为的变化，直接应用 pFL 方法促进训练导致个性化和绩效的不足。为了弥合这一差距，我们提出了 pFedPrompt，它通过从语言空间学习用户共识并以非参数方式适应视觉空间中的用户特征，从而利用了视觉语言模型中多模态的独特优势。通过这种双重协作，学到的提示符将完全个性化，并与用户的本地特征保持一致。我们在统计异质性的 FL 环境下对不同的数据集进行了广泛的实验。实验结果表明，本文提出的 pFedPrompt 算法与其他具有鲁棒性能的方法相比具有优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=pFedPrompt:+Learning+Personalized+Prompt+for+Vision-Language+Models+in+Federated+Learning)|0|
|[Word Sense Disambiguation by Refining Target Word Embedding](https://doi.org/10.1145/3543507.3583191)|Xuefeng Zhang, Richong Zhang, Xiaoyang Li, Fanshuang Kong, Junfan Chen, Samuel Mensah, Yongyi Mao|University of Ottawa, Canada; SKLSDE, School of Computer Science and Engineering, Beihang University, China; The University of Sheffield, United Kingdom|Word Sense Disambiguation (WSD) which aims to identify the correct sense of a target word appearing in a specific context is essential for web text analysis. The use of glosses has been explored as a means for WSD. However, only a few works model the correlation between the target context and gloss. We add to the body of literature by presenting a model that employs a multi-head attention mechanism on deep contextual features of the target word and candidate glosses to refine the target word embedding. Furthermore, to encourage the model to learn the relevant part of target features that align with the correct gloss, we recursively alternate attention on target word features and that of candidate glosses to gradually extract the relevant contextual features of the target word, refining its representation and strengthening the final disambiguation results. Empirical studies on the five most commonly used benchmark datasets show that our proposed model is effective and achieves state-of-the-art results.|词义消歧(WSD)是识别特定语境中目标词的正确意义，是网络文本分析的基础。水务署已研究使用注释作为一种方法。然而，只有少数作品模拟了目标语境和注释之间的相关性。本文提出了一种基于目标词深层语境特征和候选修饰语的多目标注意机制来完善目标词嵌入的模型，并对文献进行了补充。此外，为了鼓励模型学习与正确的注释相一致的目标特征的相关部分，我们递归地交替关注目标词特征和候选注释的特征，以逐渐提取目标词的相关上下文特征，完善其表示并加强最终的消歧结果。对五个最常用的基准数据集的实证研究表明，我们提出的模型是有效的，并取得了最先进的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Word+Sense+Disambiguation+by+Refining+Target+Word+Embedding)|0|
|[Dual Policy Learning for Aggregation Optimization in Graph Neural Network-based Recommender Systems](https://doi.org/10.1145/3543507.3583241)|Heesoo Jung, Sangpil Kim, Hogun Park|Dept. of Artificial Intelligence, Korea University, Republic of Korea; Dept. of Electrical and Computer Engineering, Sungkyunkwan University, Republic of Korea and Dept. of Artificial Intelligence, Sungkyunkwan University, Republic of Korea; Dept. of Artificial Intelligence, Sungkyunkwan University, Republic of Korea|Graph Neural Networks (GNNs) provide powerful representations for recommendation tasks. GNN-based recommendation systems capture the complex high-order connectivity between users and items by aggregating information from distant neighbors and can improve the performance of recommender systems. Recently, Knowledge Graphs (KGs) have also been incorporated into the user-item interaction graph to provide more abundant contextual information; they are exploited to address cold-start problems and enable more explainable aggregation in GNN-based recommender systems (GNN-Rs). However, due to the heterogeneous nature of users and items, developing an effective aggregation strategy that works across multiple GNN-Rs, such as LightGCN and KGAT, remains a challenge. In this paper, we propose a novel reinforcement learning-based message passing framework for recommender systems, which we call DPAO (Dual Policy framework for Aggregation Optimization). This framework adaptively determines high-order connectivity to aggregate users and items using dual policy learning. Dual policy learning leverages two Deep-Q-Network models to exploit the user- and item-aware feedback from a GNN-R and boost the performance of the target GNN-R. Our proposed framework was evaluated with both non-KG-based and KG-based GNN-R models on six real-world datasets, and their results show that our proposed framework significantly enhances the recent base model, improving nDCG and Recall by up to 63.7% and 42.9%, respectively. Our implementation code is available at https://github.com/steve30572/DPAO/.|图形神经网络(GNN)为推荐任务提供了强有力的表示。基于 GNN 的推荐系统通过聚合来自远邻的信息来捕获用户和项目之间复杂的高阶连通性，从而提高推荐系统的性能。最近，知识图(KGs)也被纳入到用户项目交互图中，以提供更丰富的上下文信息; 它们被用来解决冷启动问题，并能够在基于 GNN 的推荐系统(GNN-Rs)中实现更可解释的聚合。然而，由于用户和项目的异构性，开发一个跨多个 GNN-R (如 LightGCN 和 KGAT)的有效聚合策略仍然是一个挑战。本文提出了一种新的基于强化学习的推荐系统消息传递框架，称之为聚合优化的双策略框架(DPAO)。该框架使用双策略学习自适应地确定与聚合用户和项目的高阶连通性。双策略学习利用两个 Deep-Q 网络模型来利用来自 GNN-R 的用户和项目感知反馈，提高目标 GNN-R 的性能。我们提出的框架在六个实际数据集上用非 KG 和基于 KG 的 GNN-R 模型进行了评估，结果表明，我们提出的框架显着增强了最近的基础模型，使 nDCG 和 Recall 分别提高了63.7% 和42.9% 。我们的实施守则可于 https://github.com/steve30572/dpao/索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual+Policy+Learning+for+Aggregation+Optimization+in+Graph+Neural+Network-based+Recommender+Systems)|0|
|[Addressing Heterophily in Graph Anomaly Detection: A Perspective of Graph Spectrum](https://doi.org/10.1145/3543507.3583268)|Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, Yongdong Zhang|University of Science and Technology of China, China; Zhejiang University, China; Beijing Electronic Science And Technology Institute, China|Graph anomaly detection (GAD) suffers from heterophily — abnormal nodes are sparse so that they are connected to vast normal nodes. The current solutions upon Graph Neural Networks (GNNs) blindly smooth the representation of neiboring nodes, thus undermining the discriminative information of the anomalies. To alleviate the issue, recent studies identify and discard inter-class edges through estimating and comparing the node-level representation similarity. However, the representation of a single node can be misleading when the prediction error is high, thus hindering the performance of the edge indicator. In graph signal processing, the smoothness index is a widely adopted metric which plays the role of frequency in classical spectral analysis. Considering the ground truth Y to be a signal on graph, the smoothness index is equivalent to the value of the heterophily ratio. From this perspective, we aim to address the heterophily problem in the spectral domain. First, we point out that heterophily is positively associated with the frequency of a graph. Towards this end, we could prune inter-class edges by simply emphasizing and delineating the high-frequency components of the graph. Recall that graph Laplacian is a high-pass filter, we adopt it to measure the extent of 1-hop label changing of the center node and indicate high-frequency components. As GAD can be formulated as a semi-supervised binary classification problem, only part of the nodes are labeled. As an alternative, we use the prediction of the nodes to estimate it. Through our analysis, we show that prediction errors are less likely to affect the identification process. Extensive empirical evaluations on four benchmarks demonstrate the effectiveness of the indicator over popular homophilic, heterophilic, and tailored fraud detection methods. Our proposed indicator can effectively reduce the heterophily degree of the graph, thus boosting the overall GAD performance. Codes are open-sourced in https://github.com/blacksingular/GHRN.|图形异常检测(GAD)存在异质性ーー异常节点稀疏，因此它们连接到巨大的正常节点。现有的基于图神经网络(GNN)的解决方案盲目地平滑邻近节点的表示，从而破坏了异常的判别信息。为了缓解这一问题，最近的研究通过估计和比较节点级表示相似度来识别和丢弃类间边缘。然而，当预测误差较大时，单个节点的表示可能会产生误导，从而影响边缘指示器的性能。在图形信号处理中，平滑度指数是一个被广泛采用的度量指标，在经典的谱分析中起着频率的作用。考虑到地面真值 Y 是图上的一个信号，光滑度指标等价于异质比的值。从这个角度出发，我们的目标是解决谱域中的异质性问题。首先，我们指出异质性与图的频率成正相关。为了达到这个目的，我们可以通过简单地强调和描述图的高频成分来修剪类间边缘。回想一下，图拉普拉斯是一个高通滤波器，我们采用它来测量中心节点的1跳标签变化的程度，并指示高频分量。由于 GAD 可以表述为一个半监督的二进制分类问题，所以只对部分节点进行标记。作为一种替代方法，我们使用节点的预测来估计它。通过我们的分析，我们表明，预测错误不太可能影响识别过程。对四个基准的广泛的实证评估证明了指标的有效性超过流行的同质性，异质性和量身定制的欺诈检测方法。我们提出的指标可以有效地降低图的异构度，从而提高整体的 GAD 性能。代码在 https://github.com/blacksingular/ghrn 中是开源的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Addressing+Heterophily+in+Graph+Anomaly+Detection:+A+Perspective+of+Graph+Spectrum)|0|
|[Ginver: Generative Model Inversion Attacks Against Collaborative Inference](https://doi.org/10.1145/3543507.3583306)|Yupeng Yin, Xianglong Zhang, Huanle Zhang, Feng Li, Yue Yu, Xiuzhen Cheng, Pengfei Hu|National University of Defense Technology, China and Peng Cheng Laboratory, China; School of Computer Science and Technology, Shandong University, China|Deep Learning (DL) has been widely adopted in almost all domains, from threat recognition to medical diagnosis. Albeit its supreme model accuracy, DL imposes a heavy burden on devices as it incurs overwhelming system overhead to execute DL models, especially on Internet-of-Things (IoT) and edge devices. Collaborative inference is a promising approach to supporting DL models, by which the data owner (the victim) runs the first layers of the model on her local device and then a cloud provider (the adversary) runs the remaining layers of the model. Compared to offloading the entire model to the cloud, the collaborative inference approach is more data privacy-preserving as the owner’s model input is not exposed to outsiders. However, we show in this paper that the adversary can restore the victim’s model input by exploiting the output of the victim’s local model. Our attack is dubbed Ginver 1: Generative model inversion attacks against collaborative inference. Once trained, Ginver can infer the victim’s unseen model inputs without remaking the inversion attack model and thus has the generative capability. We extensively evaluate Ginver under different settings (e.g., white-box and black-box of the victim’s local model) and applications (e.g., CIFAR10 and FaceScrub datasets). The experimental results show that Ginver recovers high-quality images from the victims.|从威胁识别到医学诊断，深度学习已被广泛应用于几乎所有的领域。尽管 DL 的模型精确度最高，但它对设备造成了沉重的负担，因为它在执行 DL 模型时会产生巨大的系统开销，特别是在物联网(IoT)和边缘设备上。协作推理是支持 DL 模型的一种有前途的方法，通过这种方法，数据所有者(受害者)在其本地设备上运行模型的第一层，然后云提供者(对手)运行模型的其余层。与将整个模型卸载到云中相比，协作推理方法更能保护数据隐私，因为所有者的模型输入不会暴露给外部人员。然而，本文证明了对手可以通过利用被害人局部模型的输出来恢复被害人的模型输入。我们的攻击被称为 Ginver 1: 针对协作推理的生成模型反转攻击。一旦被训练，Ginver 可以推断出受害者看不见的模型输入，而无需重建反转攻击模型，因此具有生成能力。我们在不同的设置(例如，受害者本地模型的白盒和黑盒)和应用程序(例如，CIFAR10和 FaceScrub 数据集)下广泛评估 Ginver。实验结果表明，Ginver 可以从受害者身上恢复出高质量的图像。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ginver:+Generative+Model+Inversion+Attacks+Against+Collaborative+Inference)|0|
|[All Your Shops Are Belong to Us: Security Weaknesses in E-commerce Platforms](https://doi.org/10.1145/3543507.3583319)|Rohan Pagey, Mohammad Mannan, Amr M. Youssef|Concordia Institute for Information Systems Engineering, Concordia University, Canada|Software as a Service (SaaS) e-commerce platforms for merchants allow individual business owners to set up their online stores almost instantly. Prior work has shown that the checkout flows and payment integration of some e-commerce applications are vulnerable to logic bugs with serious financial consequences, e.g., allowing “shopping for free”. Apart from checkout and payment integration, vulnerabilities in other e-commerce operations have remained largely unexplored, even though they can have far more serious consequences, e.g., enabling “store takeover”. In this work, we design and implement a security evaluation framework to uncover security vulnerabilities in e-commerce operations beyond checkout/payment integration. We use this framework to analyze 32 representative e-commerce platforms, including web services of 24 commercial SaaS platforms and 15 associated Android apps, and 8 open source platforms; these platforms host over 10 million stores as approximated through Google dorks. We uncover several new vulnerabilities with serious consequences, e.g., allowing an attacker to take over all stores under a platform, and listing illegal products at a victim’s store—in addition to “shopping for free” bugs, without exploiting the checkout/payment process. We found 12 platforms vulnerable to store takeover (affecting 41000+ stores) and 6 platforms vulnerable to shopping for free (affecting 19000+ stores, approximated via Google dorks on Oct. 8, 2022). We have responsibly disclosed the vulnerabilities to all affected parties, and requested four CVEs (three assigned, and one is pending review).|软件即服务(SaaS)商家电子商务平台允许个体企业主几乎立即建立他们的在线商店。先前的研究已经表明，一些电子商务应用程序的结帐流程和支付集成容易受到逻辑错误的影响，这些逻辑错误会带来严重的财务后果，例如，允许“免费购物”。除了结账和支付集成，其他电子商务操作中的漏洞基本上还没有得到探索，尽管它们可能产生更为严重的后果，例如“商店接管”。在这项工作中，我们设计和实现了一个安全评估框架，以揭示电子商务运作中的安全漏洞超越结帐/支付集成。我们使用这个框架来分析32个具有代表性的电子商务平台，包括24个商业 SaaS 平台和15个相关的 Android 应用程序的网络服务，以及8个开源平台; 这些平台拥有超过1000万个商店，大致相当于 Google 呆子的数量。我们发现了几个具有严重后果的新漏洞，例如，允许攻击者在一个平台下接管所有商店，以及在受害者的商店中列出非法产品ーー除了“免费购物”的漏洞之外，还没有利用结帐/付款过程。我们发现有12个平台容易被商店接管(影响到41000多家商店) ，6个平台容易被免费购物(影响到19000多家商店，大约在2022年10月8日通过谷歌书呆子)。我们已经负责任地向所有受影响的方面披露了漏洞，并要求四个 CVE (三个分配，一个正在等待审查)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=All+Your+Shops+Are+Belong+to+Us:+Security+Weaknesses+in+E-commerce+Platforms)|0|
|[An Empirical Study of the Usage of Checksums for Web Downloads](https://doi.org/10.1145/3543507.3583326)|Gaël Bernard, Rémi Coudert, Bertil Chapuis, Kévin Huguenin|University of Applied Sciences Western Switzerland, Switzerland; EPFL, Switzerland; Department of Information Systems, University of Lausanne, Switzerland|Checksums, typically provided on webpages and generated from cryptographic hash functions (e.g., MD5, SHA256) or signature schemes (e.g., PGP), are commonly used on websites to enable users to verify that the files they download have not been tampered with when stored on possibly untrusted servers. In this paper, we elucidate the current practices regarding the usage of checksums for web downloads (hash functions used, visibility and validity of checksums, type of websites and files, etc.), as this has been mostly overlooked so far. Using a snowball-sampling strategy for the 200000 most popular domains of the Web, we first crawled a dataset of 8.5M webpages, from which we built, through an active-learning approach, a unique dataset of 277 diverse webpages that contain checksums. Our analysis of these webpages reveals interesting findings about the usage of checksums. For instance, it shows that checksums are used mostly to verify program files, that weak hash functions are frequently used, and that a non-negligible proportion of the checksums provided on webpages do not match that of their associated files. Finally, we complement our analysis with a survey of the webmasters of the considered webpages (N = 26), thus shedding light on the reasons behind the checksum-related choices they make.|校验和通常在网页上提供，由加密散列函数(例如 MD5、 SHA256)或签名方案(例如 PGP)产生，通常在网站上使用，使用户能够验证他们下载的文件在存储在可能不受信任的服务器上时没有被篡改。在这篇文章中，我们阐述了目前使用校验和进行网页下载的做法(使用的散列函数，校验和的可见性和有效性，网站和文件的类型等) ，因为这是迄今为止大多数被忽视的。使用滚雪球抽样策略对200000个最流行的网络领域，我们首先抓取了850万个网页的数据集，从中，我们通过一个主动学习的方法，建立了一个包含校验和的277个不同网页的独特数据集。我们对这些网页的分析揭示了校验和使用的有趣发现。例如，它表明校验和主要用于验证程序文件，经常使用弱散列函数，网页上提供的校验和中有不可忽视的比例与相关文件的校验和不匹配。最后，我们通过对所考虑的网页(N = 26)的网站管理员进行调查来补充我们的分析，从而阐明他们做出校验和相关选择背后的原因。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Empirical+Study+of+the+Usage+of+Checksums+for+Web+Downloads)|0|
|[Quantifying and Defending against Privacy Threats on Federated Knowledge Graph Embedding](https://doi.org/10.1145/3543507.3583450)|Yuke Hu, Wei Liang, Ruofan Wu, Kai Xiao, Weiqiang Wang, Xiaochen Li, Jinfei Liu, Zhan Qin|Ant Group, China; Zhejiang University, China and HIC-ZJU, China|Knowledge Graph Embedding (KGE) is a fundamental technique that extracts expressive representation from knowledge graph (KG) to facilitate diverse downstream tasks. The emerging federated KGE (FKGE) collaboratively trains from distributed KGs held among clients while avoiding exchanging clients' sensitive raw KGs, which can still suffer from privacy threats as evidenced in other federated model trainings (e.g., neural networks). However, quantifying and defending against such privacy threats remain unexplored for FKGE which possesses unique properties not shared by previously studied models. In this paper, we conduct the first holistic study of the privacy threat on FKGE from both attack and defense perspectives. For the attack, we quantify the privacy threat by proposing three new inference attacks, which reveal substantial privacy risk by successfully inferring the existence of the KG triple from victim clients. For the defense, we propose DP-Flames, a novel differentially private FKGE with private selection, which offers a better privacy-utility tradeoff by exploiting the entity-binding sparse gradient property of FKGE and comes with a tight privacy accountant by incorporating the state-of-the-art private selection technique. We further propose an adaptive privacy budget allocation policy to dynamically adjust defense magnitude across the training procedure. Comprehensive evaluations demonstrate that the proposed defense can successfully mitigate the privacy threat by effectively reducing the success rate of inference attacks from $83.1\%$ to $59.4\%$ on average with only a modest utility decrease.|知识图嵌入(KGE)是一种从知识图中提取表达式的基本技术，可以方便地完成不同的下游任务。新兴的联邦 KGE (FKGE)协同培训客户之间的分布式幼儿园，同时避免交换客户敏感的原始幼儿园，这些幼儿园仍然可能受到隐私威胁，这在其他联邦模型培训(例如，神经网络)中得到了证明。然而，量化和防御这种隐私威胁仍然没有探索的 FKGE，具有独特的性质没有共享以前的研究模型。本文首次从攻击和防御两个角度对 FKGE 隐私威胁进行了全面的研究。对于这种攻击，我们提出了三种新的推理攻击来量化隐私威胁，通过成功地从受害客户端推断出 KG 三元组的存在，揭示了巨大的隐私风险。对于辩方，我们提出 DP-Flames，一种具有私有选择的新型差异私有 FKGE，它通过利用 FKGE 的实体绑定稀疏梯度特性提供了更好的隐私-效用权衡，并通过结合最先进的私有选择技术提供了一个严密的隐私会计。我们进一步提出了一个自适应的隐私预算分配策略，以动态调整整个训练过程中的防御大小。综合评估表明，提出的防御能够成功地减轻隐私威胁，有效地减少推理攻击的成功率从83.1% $至59.4% $平均只有适度的效用减少。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quantifying+and+Defending+against+Privacy+Threats+on+Federated+Knowledge+Graph+Embedding)|0|
|[Sanitizing Sentence Embeddings (and Labels) for Local Differential Privacy](https://doi.org/10.1145/3543507.3583512)|Minxin Du, Xiang Yue, Sherman S. M. Chow, Huan Sun|Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Ohio State University, USA|Differentially private (DP) learning, notably DP stochastic gradient descent (DP-SGD), has limited applicability in fine-tuning gigantic pre-trained language models (LMs) for natural language processing tasks. The culprit is the perturbation of gradients (as gigantic as entire models), leading to significant efficiency and accuracy drops. We show how to achieve metric-based local DP (LDP) by sanitizing (high-dimensional) sentence embedding, extracted by LMs and much smaller than gradients. For potential utility improvement, we impose a consistency constraint on the sanitization. We explore two approaches: One is brand new and can directly output consistent noisy embeddings; the other is an upgradation with post-processing. To further mitigate “the curse of dimensionality,” we introduce two trainable linear maps for mediating dimensions without hurting privacy or utility. Our protection can effectively defend against privacy threats on embeddings. It also naturally extends to inference. Our experiments1 show that we reach the non-private accuracy under properly configured parameters, e.g., 0.92 for SST-2 with a privacy budget ϵ = 10 and the reduced dimension as 16. We also sanitize the label for LDP (with another small privacy budget) with limited accuracy losses to fully protect every sequence-label pair.|差异私有(DP)学习，特别是 DP 随机梯度下降(DP-sgd) ，在为自然语言处理任务微调庞大的预训练语言模型(LMs)方面的适用性有限。罪魁祸首是梯度的扰动(像整个模型一样巨大) ，导致了显著的效率和精度下降。我们展示了如何通过消毒(高维)句子嵌入、利用 LMs 提取和比梯度小得多的方法来实现基于度量的局部 DP (LDP)。对于潜在的效用改进，我们对消毒施加一致性约束。我们探索了两种方法: 一种是全新的，可以直接输出一致的噪声嵌入; 另一种是后处理的升级。为了进一步减轻“维数灾难”，我们引入了两个可训练的线性映射，用于在不损害隐私或效用的情况下调节维度。我们的保护可以有效地防御嵌入式系统的隐私威胁。它也自然地延伸到推理。我们的实验表明，在适当的参数配置下，我们达到了非私有精度，例如，0.92的 SST-2与私有预算 ε = 10和降维为16。我们还消毒的标签为 LDP (与另一个小的隐私预算)与有限的准确性损失，以充分保护每个序列标签对。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sanitizing+Sentence+Embeddings+(and+Labels)+for+Local+Differential+Privacy)|0|
|[Heterogeneous Federated Knowledge Graph Embedding Learning and Unlearning](https://doi.org/10.1145/3543507.3583305)|Xiangrong Zhu, Guangyao Li, Wei Hu|State Key Laboratory for Novel Software Technology, Nanjing University, China and National Institute of Healthcare Data Science, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China|Federated Learning (FL) recently emerges as a paradigm to train a global machine learning model across distributed clients without sharing raw data. Knowledge Graph (KG) embedding represents KGs in a continuous vector space, serving as the backbone of many knowledge-driven applications. As a promising combination, federated KG embedding can fully take advantage of knowledge learned from different clients while preserving the privacy of local data. However, realistic problems such as data heterogeneity and knowledge forgetting still remain to be concerned. In this paper, we propose FedLU, a novel FL framework for heterogeneous KG embedding learning and unlearning. To cope with the drift between local optimization and global convergence caused by data heterogeneity, we propose mutual knowledge distillation to transfer local knowledge to global, and absorb global knowledge back. Moreover, we present an unlearning method based on cognitive neuroscience, which combines retroactive interference and passive decay to erase specific knowledge from local clients and propagate to the global model by reusing knowledge distillation. We construct new datasets for assessing realistic performance of the state-of-the-arts. Extensive experiments show that FedLU achieves superior results in both link prediction and knowledge forgetting.|联邦学习(Federated Learning，FL)最近作为一种模式出现，它可以在不共享原始数据的情况下跨分布式客户机训练全局机器学习模型。知识图(KG)嵌入表示连续向量空间中的 KG，作为许多知识驱动应用程序的骨干。作为一种有前途的组合，联邦 KG 嵌入可以充分利用从不同客户端获得的知识，同时保护本地数据的隐私。然而，数据异构性和知识遗忘等现实问题仍然值得关注。本文提出了一种适用于异构 KG 嵌入学习和非学习的 FL 框架 FedLU。针对数据异构导致的局部优化和全局收敛之间的漂移问题，提出了互知识提取方法，将局部知识转化为全局知识，并将全局知识吸收回来。此外，我们还提出了一种基于认知神经科学的去学习方法，它结合了追溯干扰和被动衰减，从本地客户删除特定的知识，并通过重复使用知识提取传播到全球模型。我们建立了新的数据集来评估现实性能的最新技术。大量实验表明，FedLU 在链路预测和知识遗忘方面都取得了较好的效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Heterogeneous+Federated+Knowledge+Graph+Embedding+Learning+and+Unlearning)|0|
|[A Single Vector Is Not Enough: Taxonomy Expansion via Box Embeddings](https://doi.org/10.1145/3543507.3583310)|Song Jiang, Qiyue Yao, Qifan Wang, Yizhou Sun|Meta AI, USA; University of California, Los Angeles, USA|Taxonomies, which organize knowledge hierarchically, support various practical web applications such as product navigation in online shopping and user profile tagging on social platforms. Given the continued and rapid emergence of new entities, maintaining a comprehensive taxonomy in a timely manner through human annotation is prohibitively expensive. Therefore, expanding a taxonomy automatically with new entities is essential. Most existing methods for expanding taxonomies encode entities into vector embeddings (i.e., single points). However, we argue that vectors are insufficient to model the “is-a” hierarchy in taxonomy (asymmetrical relation), because two points can only represent pairwise similarity (symmetrical relation). To this end, we propose to project taxonomy entities into boxes (i.e., hyperrectangles). Two boxes can be "contained", "disjoint" and "intersecting", thus naturally representing an asymmetrical taxonomic hierarchy. Upon box embeddings, we propose a novel model BoxTaxo for taxonomy expansion. The core of BoxTaxo is to learn boxes for entities to capture their child-parent hierarchies. To achieve this, BoxTaxo optimizes the box embeddings from a joint view of geometry and probability. BoxTaxo also offers an easy and natural way for inference: examine whether the box of a given new entity is fully enclosed inside the box of a candidate parent from the existing taxonomy. Extensive experiments on two benchmarks demonstrate the effectiveness of BoxTaxo compared to vector based models.|分类法按层次组织知识，支持各种实际的网络应用程序，如在线购物中的产品导航和社交平台上的用户配置文件标签。鉴于新实体的持续和快速出现，通过人工注释及时维护全面的分类是非常昂贵的。因此，使用新实体自动扩展分类法是必不可少的。大多数现有的分类扩展方法都将实体编码为向量嵌入(即单点)。然而，我们认为向量不足以模拟分类学中的“ is-a”层次结构(非对称关系) ，因为两点只能表示成对的相似性(对称关系)。为此，我们建议将分类实体投影到盒子(即超矩形)中。两个盒子可以“包含”、“不相交”和“交叉”，因此自然地代表了一个不对称的分类层次。在盒子嵌入的基础上，提出了一种新的分类扩展模型 BoxTaxo。BoxTaxo 的核心是为实体学习用于捕获其子-父层次结构的框。为了实现这一点，BoxTaxo 从几何和概率的联合视图优化了盒子嵌入。BoxTaxo 还提供了一种简单而自然的推理方法: 检查给定新实体的框是否完全封闭在现有分类法的候选父类的框中。在两个基准上的大量实验证明了 BoxTaxo 与基于向量的模型相比的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Single+Vector+Is+Not+Enough:+Taxonomy+Expansion+via+Box+Embeddings)|0|
|[Knowledge Graph Question Answering with Ambiguous Query](https://doi.org/10.1145/3543507.3583316)|Lihui Liu, Yuzhong Chen, Mahashweta Das, Hao Yang, Hanghang Tong|visa research, USA; Department of Computer Science, University of Illinois at Urbana Champaign, USA|Knowledge graph question answering aims to identify answers of the query according to the facts in the knowledge graph. In the vast majority of the existing works, the input queries are considered perfect and can precisely express the user’s query intention. However, in reality, input queries might be ambiguous and elusive which only contain a limited amount of information. Directly answering these ambiguous queries may yield unwanted answers and deteriorate user experience. In this paper, we propose PReFNet which focuses on answering ambiguous queries with pseudo relevance feedback on knowledge graphs. In order to leverage the hidden (pseudo) relevance information existed in the results that are initially returned from a given query, PReFNet treats the top-k returned candidate answers as a set of most relevant answers, and uses variational Bayesian inference to infer user’s query intention. To boost the quality of the inferred queries, a neighborhood embedding based VGAE model is used to prune inferior inferred queries. The inferred high quality queries will be returned to the users to help them search with ease. Moreover, all the high-quality candidate nodes will be re-ranked according to the inferred queries. The experiment results show that our proposed method can recommend high-quality query graphs to users and improve the question answering accuracy.|知识图问答的目的是根据知识图中的事实来识别问题的答案。在现有的大多数工作中，输入查询被认为是完美的，可以准确地表达用户的查询意图。然而，在现实中，输入查询可能是模糊和难以捉摸的，只包含有限数量的信息。直接回答这些模棱两可的问题可能会得到不想要的答案，并损害用户体验。在这篇文章中，我们提出了一种基于知识图的伪关联反馈回答模糊查询的方法。为了利用最初从给定查询返回的结果中存在的隐藏(伪)相关信息，prefNet 将返回的候选答案视为一组最相关的答案，并使用变异贝叶斯推断来推断用户的查询意图。为了提高推理查询的质量，提出了一种基于邻域嵌入的 VGAE 模型来裁剪劣质推理查询。推断出的高质量查询将返回给用户，以帮助他们轻松搜索。此外，所有高质量的候选节点将根据推断的查询进行重新排序。实验结果表明，该方法可以向用户推荐高质量的查询图，提高问答的准确率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Graph+Question+Answering+with+Ambiguous+Query)|0|
|[Hierarchical Self-Attention Embedding for Temporal Knowledge Graph Completion](https://doi.org/10.1145/3543507.3583397)|Xin Ren, Luyi Bai, Qianwen Xiao, Xiangxi Meng|Northeastern University, China|Temporal Knowledge Graph (TKG) is composed of a series of facts related to timestamps in the real world and has become the basis of many artificial intelligence applications. However, the existing TKG is usually incomplete. It has become a hot research task to infer missing facts based on existing facts in a TKG; namely, Temporal Knowledge Graph Completion (TKGC). The current mainstream TKGC models are embedded models that predict missing facts by representing entities, relations and timestamps as low-dimensional vectors. In order to deal with the TKG structure information, there are some models that try to introduce attention mechanism into the embedding process. But they only consider the structure information of entities or relations, and ignore the structure information of the whole TKG. Moreover, most of them usually treat timestamps as a general feature and cannot take advantage of the potential time series information of the timestamp. To solve these problems, wo propose a new Hierarchical Self-Attention Embedding (HSAE) model which inspired by self-attention mechanism and diachronic embedding technique. For structure information of the whole TKG, we divide the TKG into two layers: entity layer and relation layer, and then apply the self-attention mechanism to the entity layer and relation layer respectively to capture the structure information. For time series information of the timestamp, we capture them by combining positional encoding and diachronic embedding technique into the above two self-attention layers. Finally, we can get the embedded representation vectors of entities, relations and timestamps, which can be combined with other models for better results. We evaluate our model on three TKG datasets: ICEWS14, ICEWS05-15 and GDELT. Experimental results on the TKGC (interpolation) task demonstrate that our model achieves state-of-the-art results.|时间知识图(TKG)由现实世界中与时间戳相关的一系列事实组成，已成为许多人工智能应用的基础。然而，现有的 TKG 通常是不完整的。基于 TKG 中已有事实推断缺失事实，即时态知识图完成(TKGC) ，已成为一个热门的研究课题。目前主流的 TKGC 模型是嵌入式模型，通过将实体、关系和时间戳表示为低维向量来预测缺失事实。为了处理 TKG 的结构信息，有一些模型尝试在嵌入过程中引入注意机制。但他们只考虑实体或关系的结构信息，而忽略了整个 TKG 的结构信息。此外，它们中的大多数通常将时间戳视为一个通用特性，不能利用时间戳的潜在时间序列信息。为了解决这些问题，我们提出了一种新的分层自我注意嵌入(HSAE)模型，该模型受自我注意机制和历时嵌入技术的启发。对于整个 TKG 的结构信息，我们将 TKG 分为实体层和关系层两个层次，然后将自注意机制分别应用于实体层和关系层，以获取 TKG 的结构信息。对于时间戳的时间序列信息，我们将位置编码和历时嵌入技术结合到上述两个自我注意层中来获取它们。最后，我们可以得到实体、关系和时间戳的嵌入式表示向量，这些表示向量可以与其他模型相结合以获得更好的结果。我们在三个 TKG 数据集上评估我们的模型: ICEWS14，ICEWS05-15和 GDELT。在 TKGC (插值)任务上的实验结果表明，我们的模型达到了最先进的效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Self-Attention+Embedding+for+Temporal+Knowledge+Graph+Completion)|0|
|[Link Prediction with Attention Applied on Multiple Knowledge Graph Embedding Models](https://doi.org/10.1145/3543507.3583358)|Cosimo Gregucci, Mojtaba Nayyeri, Daniel Hernández, Steffen Staab|University of Stuttgart, Germany; University of Stuttgart, Germany and University of Southampton, United Kingdom|Predicting missing links between entities in a knowledge graph is a fundamental task to deal with the incompleteness of data on the Web. Knowledge graph embeddings map nodes into a vector space to predict new links, scoring them according to geometric criteria. Relations in the graph may follow patterns that can be learned, e.g., some relations might be symmetric and others might be hierarchical. However, the learning capability of different embedding models varies for each pattern and, so far, no single model can learn all patterns equally well. In this paper, we combine the query representations from several models in a unified one to incorporate patterns that are independently captured by each model. Our combination uses attention to select the most suitable model to answer each query. The models are also mapped onto a non-Euclidean manifold, the Poincar\'e ball, to capture structural patterns, such as hierarchies, besides relational patterns, such as symmetry. We prove that our combination provides a higher expressiveness and inference power than each model on its own. As a result, the combined model can learn relational and structural patterns. We conduct extensive experimental analysis with various link prediction benchmarks showing that the combined model outperforms individual models, including state-of-the-art approaches.|预测知识图中实体之间的缺失链接是处理网络数据不完整性的基本任务。知识图嵌入将节点映射到向量空间中，预测新的链接，并根据几何标准对其进行评分。图中的关系可能遵循可以学习的模式，例如，一些关系可能是对称的，另一些可能是等级的。然而，不同嵌入模型的学习能力因模式的不同而异，到目前为止，还没有一个单独的模型能够同样好地学习所有的模式。在本文中，我们将来自多个模型的查询表示结合在一个统一的模型中，以合并由每个模型独立捕获的模式。我们的组合使用注意力来选择最合适的模型来回答每个查询。这些模型还被映射到一个非欧几里德流形，即 Poincar‘ e 球，以捕获结构模式，如层次结构，以及关系模式，如对称性。我们证明，我们的组合提供了更高的表达能力和推理能力比每个模型本身。因此，组合模型可以学习关系模式和结构模式。我们进行了广泛的实验分析与各种链接预测基准表明，组合模型优于个别模型，包括最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Link+Prediction+with+Attention+Applied+on+Multiple+Knowledge+Graph+Embedding+Models)|0|
|[SeqCare: Sequential Training with External Medical Knowledge Graph for Diagnosis Prediction in Healthcare Data](https://doi.org/10.1145/3543507.3583543)|Yongxin Xu, Xu Chu, Kai Yang, Zhiyuan Wang, Peinie Zou, Hongxin Ding, Junfeng Zhao, Yasha Wang, Bing Xie|Tsinghua University, China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, China and Peking University, China; Zhongguancun Laboratory, China|Deep learning techniques are capable of capturing complex input-output relationships, and have been widely applied to the diagnosis prediction task based on web-based patient electronic health records (EHR) data. To improve the prediction and interpretability of pure data-driven deep learning with only a limited amount of labeled data, a pervasive trend is to assist the model training with knowledge priors from online medical knowledge graphs. However, they marginally investigated the label imbalance and the task-irrelevant noise in the external knowledge graph. The imbalanced label distribution would bias the learning and knowledge extraction towards the majority categories. The task-irrelevant noise introduces extra uncertainty to the model performance. To this end, aiming at by-passing the bias-variance trade-off dilemma, we introduce a new sequential learning framework, dubbed SeqCare, for diagnosis prediction with online medical knowledge graphs. Concretely, in the first step, SeqCare learns a bias-reduced space through a self-supervised graph contrastive learning task. Secondly, SeqCare reduces the learning uncertainty by refining the supervision signal and the graph structure of the knowledge graph simultaneously. Lastly, SeqCare trains the model in the bias-variance reduced space with a self-distillation to further filter out irrelevant information in the data. Experimental evaluations on two real-world datasets show that SeqCare outperforms state-of-the-art approaches. Case studies exemplify the interpretability of SeqCare. Moreover, the medical findings discovered by SeqCare are consistent with experts and medical literature.|深度学习技术能够捕捉复杂的输入输出关系，已广泛应用于基于网络病人电子健康记录(EHR)数据的诊断预测任务中。为了提高纯数据驱动的深度学习的预测性和可解释性，通过在线医学知识图的知识先验来辅助模型训练是一个普遍的趋势。然而，他们对外部知识图中的标签不平衡和任务不相关噪声的研究很少。不均衡的标签分布会使学习和知识抽取偏向于大多数类别。任务无关噪声给模型性能带来额外的不确定性。为此，针对偏差-方差权衡困境，我们引入了一个新的序列学习框架，称为 SeqCare，用于在线医学知识图的诊断预测。具体地说，在第一步中，SeqCare 通过一个自监督的图形对比学习任务学习一个减少偏差的空间。其次，SeqCare 通过同时细化监督信号和知识图的图形结构来降低学习的不确定性。最后，SeqCare 在偏差-方差缩减空间中用自精馏的方法对模型进行训练，以进一步滤除数据中的不相关信息。对两个真实世界数据集的实验评估表明，SeqCare 的性能优于最先进的方法。案例研究例证了 SeqCare 的可解释性。此外，SeqCare 发现的医学发现与专家和医学文献一致。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SeqCare:+Sequential+Training+with+External+Medical+Knowledge+Graph+for+Diagnosis+Prediction+in+Healthcare+Data)|0|
|[The Thin Ideology of Populist Advertising on Facebook during the 2019 EU Elections](https://doi.org/10.1145/3543507.3583267)|Arthur Capozzi, Gianmarco De Francisci Morales, Yelena Mejova, Corrado Monti, André Panisson|Computer Science, Universita' di Torino, Italy; Centai, Italy; ISI Foundation, Italy|Social media has been an important tool in the expansion of the populist message, and it is thought to have contributed to the electoral success of populist parties in the past decade. This study compares how populist parties advertised on Facebook during the 2019 European Parliamentary election. In particular, we examine commonalities and differences in which audiences they reach and on which issues they focus. By using data from Meta (previously Facebook) Ad Library, we analyze 45k ad campaigns by 39 parties, both populist and mainstream, in Germany, United Kingdom, Italy, Spain, and Poland. While populist parties represent just over 20% of the total expenditure on political ads, they account for 40% of the total impressions$\unicode{x2013}$most of which from Eurosceptic and far-right parties$\unicode{x2013}$thus hinting at a competitive advantage for populist parties on Facebook. We further find that ads posted by populist parties are more likely to reach male audiences, and sometimes much older ones. In terms of issues, populist politicians focus on monetary policy, state bureaucracy and reforms, and security, while the focus on EU and Brexit is on par with non-populist, mainstream parties. However, issue preferences are largely country-specific, thus supporting the view in political science that populism is a "thin ideology", that does not have a universal, coherent policy agenda. This study illustrates the usefulness of publicly available advertising data for monitoring the populist outreach to, and engagement with, millions of potential voters, while outlining the limitations of currently available data.|社交媒体一直是传播民粹主义信息的重要工具，据认为它在过去十年中为民粹主义政党在选举中取得成功做出了贡献。这项研究比较了2019年欧洲议会选举期间民粹主义政党在 Facebook 上的广告。特别是，我们研究了它们所接触到的受众以及它们所关注的问题的共性和差异。通过使用来自 Meta (以前的 Facebook)广告库的数据，我们分析了来自德国、英国、意大利、西班牙和波兰的39个民粹主义和主流政党的45000个广告活动。尽管民粹主义政党仅占政治广告总支出的20% 多一点，但他们占了总印象的40% ，其中大部分来自欧洲怀疑论者和极右翼政党，因此暗示了民粹主义政党在 Facebook 上的竞争优势。我们进一步发现，民粹主义政党发布的广告更有可能触及男性受众，有时甚至是年龄更大的受众。就问题而言，民粹主义政客关注的是货币政策、国家官僚机构和改革以及安全，而关注欧盟和 Brexit 的政党，与非民粹主义的主流政党不相上下。然而，问题的偏好在很大程度上是针对具体国家的，因此支持了政治科学中的观点，即民粹主义是一种“薄弱的意识形态”，没有一个普遍的、连贯的政策议程。这项研究说明了公开可用的广告数据在监测民粹主义者与数百万潜在选民的接触和接触方面的有用性，同时概述了目前可用数据的局限性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Thin+Ideology+of+Populist+Advertising+on+Facebook+during+the+2019+EU+Elections)|0|
|[FlexiFed: Personalized Federated Learning for Edge Clients with Heterogeneous Model Architectures](https://doi.org/10.1145/3543507.3583347)|Kaibin Wang, Qiang He, Feifei Chen, Chunyang Chen, Faliang Huang, Hai Jin, Yun Yang|Nanning Normal University, China; Deakin University, Australia; Huazhong University of Science and Technology, China and Swinburne University of Technology, Australia; Swinburne University of Technology, Australia; Monash University, Australia; Huazhong University of Science and Technology, China|Mobile and Web-of-Things (WoT) devices at the network edge account for more than half of the world’s web traffic, making a great data source for various machine learning (ML) applications, particularly federated learning (FL) which offers a promising solution to privacy-preserving ML feeding on these data. FL allows edge mobile and WoT devices to train a shared global ML model under the orchestration of a central parameter server. In the real world, due to resource heterogeneity, these edge devices often train different versions of models (e.g., VGG-16 and VGG-19) or different ML models (e.g., VGG and ResNet) for the same ML task (e.g., computer vision and speech recognition). Existing FL schemes have assumed that participating edge devices share a common model architecture, and thus cannot facilitate FL across edge devices with heterogeneous ML model architectures. We explored this architecture heterogeneity challenge and found that FL can and should accommodate these edge devices to improve model accuracy and accelerate model training. This paper presents our findings and FlexiFed, a novel scheme for FL across edge devices with heterogeneous model architectures, and three model aggregation strategies for accommodating architecture heterogeneity under FlexiFed. Experiments with four widely-used ML models on four public datasets demonstrate 1) the usefulness of FlexiFed; and 2) that compared with the state-of-the-art FL scheme, FlexiFed improves model accuracy by 2.6%-9.7% and accelerates model convergence by 1.24 × -4.04 ×.|处于网络边缘的移动和物联网(WoT)设备占据了世界网络流量的一半以上，为各种机器学习(ML)应用提供了一个巨大的数据源，特别是联邦学习(FL) ，它为依靠这些数据保护隐私的机器学习提供了一个有前途的解决方案。FL 允许边缘移动和 WoT 设备在一个中心参数服务器的协调下训练一个共享的全球 ML 模型。在现实世界中，由于资源的异质性，这些边缘设备经常为相同的机器学习任务训练不同版本的模型(例如，VGG-16和 VGG-19)或不同的机器学习模型(例如，VGG 和 ResNet)(例如，计算机视觉和语音识别)。现有的 FL 方案假定参与的边缘设备共享一个共同的模型架构，因此不能促进具有异构机器学习模型架构的边缘设备之间的 FL。我们探讨了这种体系结构异构性的挑战，发现 FL 可以而且应该适应这些边缘设备，以提高模型精度和加速模型训练。本文介绍了我们的研究结果和 FlexiFed，这是一个跨具有异构模型结构的边缘设备的 FL 的新方案，以及在 FlexiFed 下适应结构异构性的三种模型聚合策略。在四个公共数据集上对四个广泛使用的机器学习模型进行的实验表明: 1) FlexiFed 的有用性; 2)与最先进的 FL 方案相比，FlexiFed 将模型精度提高了2.6% -9.7% ，并加速了模型收敛1.24 × -4.04 × 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FlexiFed:+Personalized+Federated+Learning+for+Edge+Clients+with+Heterogeneous+Model+Architectures)|0|
|[PipeEdge: A Trusted Pipelining Collaborative Edge Training based on Blockchain](https://doi.org/10.1145/3543507.3583413)|Liang Yuan, Qiang He, Feifei Chen, Ruihan Dou, Hai Jin, Yun Yang|Deakin University, Australia; Huazhong University of Science and Technology, China and Swinburne University of Technology, Australia; Swinburne University of Technology, Australia; University of Waterloo, Canada; Huazhong University of Science and Technology, China|Powered by the massive data generated by the blossom of mobile and Web-of-Things (WoT) devices, Deep Neural Networks (DNNs) have developed both in accuracy and size in recent years. Conventional cloud-based DNN training incurs rapidly-increasing data and model transmission overheads as well as privacy issues. Mobile edge computing (MEC) provides a promising solution by facilitating DNN model training on edge servers at the network edge. However, edge servers often suffer from constrained resources and need to collaborate on DNN training. Unfortunately, managed by different telecoms, edge servers cannot properly collaborate with each other without incentives and trust. In this paper, we introduce PipeEdge, a scheme that promotes collaborative edge training between edge servers by introducing incentives and trust based on blockchain. Under the PipeEdge scheme, edge servers can hire trustworthy workers for pipelined DNN training tasks based on model parallelism. We implement PipeEdge and evaluate it comprehensively with four different DNN models. The results show that it outperforms state-of-the-art schemes by up to 173.98% with negligible overheads.|深度神经网络(DNN)由移动设备和物联网(WoT)设备的蓬勃发展所产生的大量数据所驱动，近年来在精度和规模上都有所发展。传统的基于云的 DNN 培训会带来快速增长的数据和模型传输开销以及隐私问题。移动边缘计算(MEC)为网络边缘服务器上的 DNN 模型训练提供了一种有前途的解决方案。然而，边缘服务器经常受到资源的限制，需要协作进行 DNN 培训。不幸的是，由不同电信公司管理的边缘服务器如果没有激励和信任，就无法正确地相互协作。本文介绍了 PipeEdge 方案，该方案通过引入基于区块链的激励和信任来促进边缘服务器之间的协同边缘训练。在 PipeEdge 方案下，边缘服务器可以雇佣值得信赖的工人，根据模型并行性执行流水线 DNN 培训任务。我们实现了 PipeEdge，并使用四种不同的 DNN 模型对其进行了综合评估。结果表明，该方案的性能优于最先进的方案达173.98% ，开销可以忽略不计。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PipeEdge:+A+Trusted+Pipelining+Collaborative+Edge+Training+based+on+Blockchain)|0|
|[ELASTIC: Edge Workload Forecasting based on Collaborative Cloud-Edge Deep Learning](https://doi.org/10.1145/3543507.3583436)|Yanan Li, Haitao Yuan, Zhe Fu, Xiao Ma, Mengwei Xu, Shangguang Wang|Tsinghua University, China; Beijing University of Posts and Telecommunications, China; Nanyang Technological University, Singapore|With the rapid development of edge computing in the post-COVID19 pandemic period, precise workload forecasting is considered the basis for making full use of the edge limited resources, and both edge service providers (ESPs) and edge service consumers (ESCs) can benefit significantly from it. Existing paradigms of workload forecasting (i.e., edge-only or cloud-only) are improper, due to failing to consider the inter-site correlations and might suffer from significant data transmission delays. With the increasing adoption of edge platforms by web services, it is critical to balance both accuracy and efficiency in workload forecasting. In this paper, we propose ELASTIC, which is the first study that leverages a cloud-edge collaborative paradigm for edge workload forecasting with multi-view graphs. Specifically, at the global stage, we design a learnable aggregation layer on each edge site to reduce the time consumption while capturing the inter-site correlation. Additionally, at the local stage, we design a disaggregation layer combining both the intra-site correlation and inter-site correlation to improve the prediction accuracy. Extensive experiments on realistic edge workload datasets collected from China’s largest edge service provider show that ELASTIC outperforms state-of-the-art methods, decreases time consumption, and reduces communication cost.|随着后 COVID19大流行时期边缘计算的快速发展，精确的工作量预测被认为是充分利用边缘有限资源的基础，边缘服务提供商(ESP)和边缘服务消费者(ESCs)都可以从中受益匪浅。现有的工作量预测模式(即仅边缘预测或仅云预测)是不适当的，因为没有考虑到站点间的相关性，并可能遭受显著的数据传输延迟。随着 Web 服务越来越多地采用边缘平台，在工作负载预测中平衡准确性和效率至关重要。在本文中，我们提出了 ELASTIC，这是第一个利用云端协作范式进行多视图边缘工作负荷预测的研究。具体来说，在全局阶段，我们在每个边缘站点上设计一个可学习的聚合层，以减少时间消耗，同时捕获站点间的相关性。此外，在局部阶段，我们设计了一个解体层，将场内相关性和场间相关性结合起来，以提高预测的准确性。对中国最大的边缘服务提供商收集的真实边缘工作负载数据集进行的大量实验表明，ELASTIC 优于最先进的方法，减少了时间消耗，降低了通信成本。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ELASTIC:+Edge+Workload+Forecasting+based+on+Collaborative+Cloud-Edge+Deep+Learning)|0|
|[DUET: A Tuning-Free Device-Cloud Collaborative Parameters Generation Framework for Efficient Device Model Generalization](https://doi.org/10.1145/3543507.3583451)|Zheqi Lv, Wenqiao Zhang, Shengyu Zhang, Kun Kuang, Feng Wang, Yongwei Wang, Zhengyu Chen, Tao Shen, Hongxia Yang, Beng Chin Ooi, Fei Wu|Alibaba Group, China; Zhejiang University, China; National University of Singapore, Singapore|Device Model Generalization (DMG) is a practical yet under-investigated research topic for on-device machine learning applications. It aims to improve the generalization ability of pre-trained models when deployed on resource-constrained devices, such as improving the performance of pre-trained cloud models on smart mobiles. While quite a lot of works have investigated the data distribution shift across clouds and devices, most of them focus on model fine-tuning on personalized data for individual devices to facilitate DMG. Despite their promising, these approaches require on-device re-training, which is practically infeasible due to the overfitting problem and high time delay when performing gradient calculation on real-time data. In this paper, we argue that the computational cost brought by fine-tuning can be rather unnecessary. We consequently present a novel perspective to improving DMG without increasing computational cost, i.e., device-specific parameter generation which directly maps data distribution to parameters. Specifically, we propose an efficient Device-cloUd collaborative parametErs generaTion framework DUET. DUET is deployed on a powerful cloud server that only requires the low cost of forwarding propagation and low time delay of data transmission between the device and the cloud. By doing so, DUET can rehearse the device-specific model weight realizations conditioned on the personalized real-time data for an individual device. Importantly, our DUET elegantly connects the cloud and device as a 'duet' collaboration, frees the DMG from fine-tuning, and enables a faster and more accurate DMG paradigm. We conduct an extensive experimental study of DUET on three public datasets, and the experimental results confirm our framework's effectiveness and generalisability for different DMG tasks.|设备模型综合(DMG)是设备上机器学习应用中一个实用但尚未得到充分研究的课题。它旨在提高预先训练的模型在资源有限的设备上部署时的泛化能力，例如提高预先训练的云模型在智能手机上的性能。虽然许多工作已经研究了跨云和设备的数据分布转移，但大多数工作集中在针对个人设备的个性化数据的模型微调上，以促进 DMG。尽管这些方法很有前景，但是需要在设备上进行再训练，这在实际中是不可行的，因为在对实时数据进行梯度计算时，存在过拟合问题和高时延。在本文中，我们认为微调带来的计算成本可能是相当不必要的。因此，我们提出了一个新的视角来改善 DMG 而不增加计算成本，即，设备特定的参数生成，直接映射数据分布到参数。具体地说，我们提出了一种高效的设备云协同参数生成框架 DUET。DUET 部署在一个强大的云服务器上，只需要较低的转发传播成本和较低的设备与云之间的数据传输延迟。通过这样做，DUET 可以预演设备特定的模型权重实现条件下的个性化实时数据为单个设备。重要的是，我们的 DUET 作为一个“二重奏”协作优雅地连接了云和设备，从微调中解放了 DMG，并支持更快、更准确的 DMG 范例。我们对三个公共数据集上的 DUET 进行了广泛的实验研究，实验结果证实了我们的框架对不同 DMG 任务的有效性和通用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DUET:+A+Tuning-Free+Device-Cloud+Collaborative+Parameters+Generation+Framework+for+Efficient+Device+Model+Generalization)|0|
|[RL-MPCA: A Reinforcement Learning Based Multi-Phase Computation Allocation Approach for Recommender Systems](https://doi.org/10.1145/3543507.3583313)|Jiahong Zhou, Shunhui Mao, Guoliang Yang, Bo Tang, Qianlong Xie, Lebin Lin, Xingxing Wang, Dong Wang|Meituan, China|Recommender systems aim to recommend the most suitable items to users from a large number of candidates. Their computation cost grows as the number of user requests and the complexity of services (or models) increases. Under the limitation of computation resources (CRs), how to make a trade-off between computation cost and business revenue becomes an essential question. The existing studies focus on dynamically allocating CRs in queue truncation scenarios (i.e., allocating the size of candidates), and formulate the CR allocation problem as an optimization problem with constraints. Some of them focus on single-phase CR allocation, and others focus on multi-phase CR allocation but introduce some assumptions about queue truncation scenarios. However, these assumptions do not hold in other scenarios, such as retrieval channel selection and prediction model selection. Moreover, existing studies ignore the state transition process of requests between different phases, limiting the effectiveness of their approaches. This paper proposes a Reinforcement Learning (RL) based Multi-Phase Computation Allocation approach (RL-MPCA), which aims to maximize the total business revenue under the limitation of CRs. RL-MPCA formulates the CR allocation problem as a Weakly Coupled MDP problem and solves it with an RL-based approach. Specifically, RL-MPCA designs a novel deep Q-network to adapt to various CR allocation scenarios, and calibrates the Q-value by introducing multiple adaptive Lagrange multipliers (adaptive-λ) to avoid violating the global CR constraints. Finally, experiments on the offline simulation environment and online real-world recommender system validate the effectiveness of our approach.|推荐系统的目的是从大量的候选人中向用户推荐最合适的项目。它们的计算成本随着用户请求的数量和服务(或模型)的复杂性的增加而增加。在计算资源有限的情况下，如何在计算成本和业务收益之间取得平衡成为一个必须解决的问题。现有的研究集中于在队列截断情况下(即分配候选人的人数)动态分配登记册编号，并将登记册编号分配问题制订为一个有约束的最佳化问题。其中一些关注于单相 CR 分配，另一些关注于多相 CR 分配，但是引入了一些关于队列截断场景的假设。但是，这些假设在其他场景中不成立，例如检索通道选择和预测模型选择。此外，现有的研究忽视了请求在不同阶段之间的状态转换过程，限制了这些方法的有效性。本文提出了一种基于强化学习的多阶段计算分配方法(RL-MPCA) ，其目标是在客户关系的限制下实现企业总收入的最大化。RL-MPCA 将 CR 分配问题表示为弱耦合 MDP 问题，并用基于 RL 的方法求解。具体来说，RL-MPCA 设计了一种新的深层 Q 网络来适应各种 CR 分配场景，并通过引入多个自适应拉格朗日乘子(Adaptive-λ)来校准 Q 值，以避免违反全局 CR 约束。最后，离线仿真环境和在线真实世界推荐系统的实验验证了我们方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RL-MPCA:+A+Reinforcement+Learning+Based+Multi-Phase+Computation+Allocation+Approach+for+Recommender+Systems)|0|
|[Learning To Rank Resources with GNN](https://doi.org/10.1145/3543507.3583360)|Ulugbek Ergashev, Eduard C. Dragut, Weiyi Meng|Computer Science, Binghamton University, USA; Computer and Information Sciences, Temple University, USA|As the content on the Internet continues to grow, many new dynamically changing and heterogeneous sources of data constantly emerge. A conventional search engine cannot crawl and index at the same pace as the expansion of the Internet. Moreover, a large portion of the data on the Internet is not accessible to traditional search engines. Distributed Information Retrieval (DIR) is a viable solution to this as it integrates multiple shards (resources) and provides a unified access to them. Resource selection is a key component of DIR systems. There is a rich body of literature on resource selection approaches for DIR. A key limitation of the existing approaches is that they primarily use term-based statistical features and do not generally model resource-query and resource-resource relationships. In this paper, we propose a graph neural network (GNN) based approach to learning-to-rank that is capable of modeling resource-query and resource-resource relationships. Specifically, we utilize a pre-trained language model (PTLM) to obtain semantic information from queries and resources. Then, we explicitly build a heterogeneous graph to preserve structural information of query-resource relationships and employ GNN to extract structural information. In addition, the heterogeneous graph is enriched with resource-resource type of edges to further enhance the ranking accuracy. Extensive experiments on benchmark datasets show that our proposed approach is highly effective in resource selection. Our method outperforms the state-of-the-art by 6.4% to 42% on various performance metrics.|随着 Internet 上的内容不断增长，许多新的动态变化和异构的数据源不断出现。传统的搜索引擎无法以与互联网扩展同样的速度爬行和索引。此外，互联网上的大部分数据不能被传统的搜索引擎访问。分布式信息检索(DIR)是一个可行的解决方案，因为它集成了多个碎片(资源) ，并提供了对它们的统一访问。资源选择是 DIR 系统的关键组成部分。关于 DIR 的资源选择方法有大量的文献。现有方法的一个主要局限性在于，它们主要使用基于术语的统计特征，并且一般不对资源-查询和资源-资源关系建模。本文提出了一种基于图神经网络(GNN)的排序学习方法，该方法能够对资源-查询和资源-资源关系进行建模。具体来说，我们利用预先训练的语言模型(PTLM)从查询和资源中获取语义信息。然后，我们显式地构建一个异构图来保存查询-资源关系的结构信息，并使用 GNN 来提取结构信息。此外，对异构图进行了资源-资源类型边的丰富，进一步提高了排序的准确性。对基准数据集的大量实验表明，该方法在资源选择方面是非常有效的。在各种性能指标上，我们的方法比最先进的方法表现好6.4% 到42% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+To+Rank+Resources+with+GNN)|0|
|[CgAT: Center-Guided Adversarial Training for Deep Hashing-Based Retrieval](https://doi.org/10.1145/3543507.3583369)|Xunguang Wang, Yiqun Lin, Xiaomeng Li|The Hong Kong University of Science and Technology, China and The Hong Kong University of Science and Technology Shenzhen Research Institute, China; The Hong Kong University of Science and Technology, China|Deep hashing has been extensively utilized in massive image retrieval because of its efficiency and effectiveness. However, deep hashing models are vulnerable to adversarial examples, making it essential to develop adversarial defense methods for image retrieval. Existing solutions achieved limited defense performance because of using weak adversarial samples for training and lacking discriminative optimization objectives to learn robust features. In this paper, we present a min-max based Center-guided Adversarial Training, namely CgAT, to improve the robustness of deep hashing networks through worst adversarial examples. Specifically, we first formulate the center code as a semantically-discriminative representative of the input image content, which preserves the semantic similarity with positive samples and dissimilarity with negative examples. We prove that a mathematical formula can calculate the center code immediately. After obtaining the center codes in each optimization iteration of the deep hashing network, they are adopted to guide the adversarial training process. On the one hand, CgAT generates the worst adversarial examples as augmented data by maximizing the Hamming distance between the hash codes of the adversarial examples and the center codes. On the other hand, CgAT learns to mitigate the effects of adversarial samples by minimizing the Hamming distance to the center codes. Extensive experiments on the benchmark datasets demonstrate the effectiveness of our adversarial training algorithm in defending against adversarial attacks for deep hashing-based retrieval. Compared with the current state-of-the-art defense method, we significantly improve the defense performance by an average of 18.61\%, 12.35\%, and 11.56\% on FLICKR-25K, NUS-WIDE, and MS-COCO, respectively. The code is available at https://github.com/xunguangwang/CgAT.|深度散列由于其高效性和有效性，在海量图像检索中得到了广泛的应用。然而，深度散列模型很容易受到敌对实例的影响，因此开发图像检索的敌对防御方法是非常必要的。现有的解决方案由于使用弱对手样本进行训练，缺乏区分性优化目标来学习鲁棒特征，因此防御性能有限。本文提出了一种基于最小-最大中心引导的对抗训练方法，即 CgAT，通过最坏的对抗实例来提高深度哈希网络的鲁棒性。具体来说，我们首先将中心代码表示为输入图像内容的语义识别代表，保留了正样本的语义相似性和负样本的语义不相似性。我们证明了一个数学公式可以立即计算中心码。在深度哈希网络的每次优化迭代中获得中心码后，采用中心码来指导对抗训练过程。一方面，通过最大化对手的散列码和中心代码之间的汉明距离，CgAT 生成最坏的对手的例子作为增强数据。另一方面，CgAT 学会了通过最小化对中心代码的汉明距离来减轻对抗性样本的影响。在基准数据集上的大量实验证明了我们的对抗性训练算法在防御基于深度散列检索的对抗性攻击方面的有效性。与目前最先进的防御方法相比，FLICKR-25K、 NUS-WIDE 和 MS-COCO 的防御性能分别平均提高了18.61% 、12.35% 和11.56% 。密码可在 https://github.com/xunguangwang/cgat 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CgAT:+Center-Guided+Adversarial+Training+for+Deep+Hashing-Based+Retrieval)|0|
|[Algorithmic Vibe in Information Retrieval](https://doi.org/10.1145/3543507.3583384)|Ali Montazeralghaem, Nick Craswell, Ryen W. White, Ahmed Hassan Awadallah, Byungki Byun|Microsoft, USA; University of Massachusetts Amherst, USA|When information retrieval systems return a ranked list of results in response to a query, they may be choosing from a large set of candidate results that are equally useful and relevant. This means we might be able to identify a difference between rankers A and B, where ranker A systematically prefers a certain type of relevant results. Ranker A may have this systematic difference (different “vibe”) without having systematically better or worse results according to standard information retrieval metrics. We first show that a vibe difference can exist, comparing two publicly available rankers, where the one that is trained on health-related queries will systematically prefer health-related results, even for non-health queries. We define a vibe metric that lets us see the words that a ranker prefers. We investigate the vibe of search engine clicks vs. human labels. We perform an initial study into correcting for vibe differences to make ranker A more like ranker B via changes in negative sampling during training.|当信息检索系统对一个查询返回一个排名结果列表时，它们可能会从一大堆同样有用和相关的候选结果中进行选择。这意味着我们可能能够识别排名 A 和 B 之间的差异，其中排名 A 系统地偏好某种类型的相关结果。排名 a 可能有这种系统性差异(不同的“内心感应”) ，而没有根据标准的信息检索指标得出系统性更好或更差的结果。我们首先展示了内心感应差异的存在，比较两个公开可用的排名，其中受过健康相关查询培训的人会系统地偏好与健康相关的结果，即使对于非健康查询也是如此。我们定义一个内心感应度量，让我们看到一个排名喜欢的词。我们调查了搜索引擎点击与人类标签之间的关系。我们进行了一个初步的研究，以纠正内心感应的差异，使排名 A 更像排名 B 通过改变负面抽样在训练期间。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Algorithmic+Vibe+in+Information+Retrieval)|0|
|[Geographic Information Retrieval Using Wikipedia Articles](https://doi.org/10.1145/3543507.3583469)|Amir Krause, Sara Cohen|The Rachel and Selim Benin School of Computer Science and Engineering, The Hebrew University, Israel|Assigning semantically relevant, real-world locations to documents opens new possibilities to perform geographic information retrieval. We propose a novel approach to automatically determine the latitude-longitude coordinates of appropriate Wikipedia articles with high accuracy, leveraging both text and metadata in the corpus. By examining articles whose base-truth coordinates are known, we show that our method attains a substantial improvement over state of the art works. We subsequently demonstrate how our approach could yield two benefits: (1) detecting significant geolocation errors in Wikipedia; and (2) proposing approximated coordinates for hundreds of thousands of articles which are not traditionally considered to be locations (such as events, ideas or people), opening new possibilities for conceptual geographic retrievals over Wikipedia.|为文档分配语义相关的、现实世界中的位置，为执行地理信息检索开辟了新的可能性。我们提出了一种新的方法，利用语料库中的文本和元数据，高精度地自动确定适当 Wikipedia 文章的经纬度坐标。通过检查文章的基础-真理坐标已知，我们表明，我们的方法取得了实质性的改善状态的艺术作品。我们随后展示了我们的方法如何产生两个好处: (1)检测维基百科中的重大地理定位错误; (2)提出几十万个传统上不被认为是地点的文章(如事件、想法或人)的大致坐标，为维基百科的概念地理检索开辟了新的可能性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Geographic+Information+Retrieval+Using+Wikipedia+Articles)|0|
|[Optimizing Guided Traversal for Fast Learned Sparse Retrieval](https://doi.org/10.1145/3543507.3583497)|Yifan Qiao, Yingrui Yang, Haixin Lin, Tao Yang|Department of Computer Science, University of California at Santa Barbara, USA; Department of Computer Science, University of California, Santa Barbara, USA|Recent studies show that BM25-driven dynamic index skipping can greatly accelerate MaxScore-based document retrieval based on the learned sparse representation derived by DeepImpact. This paper investigates the effectiveness of such a traversal guidance strategy during top k retrieval when using other models such as SPLADE and uniCOIL, and finds that unconstrained BM25-driven skipping could have a visible relevance degradation when the BM25 model is not well aligned with a learned weight model or when retrieval depth k is small. This paper generalizes the previous work and optimizes the BM25 guided index traversal with a two-level pruning control scheme and model alignment for fast retrieval using a sparse representation. Although there can be a cost of increased latency, the proposed scheme is much faster than the original MaxScore method without BM25 guidance while retaining the relevance effectiveness. This paper analyzes the competitiveness of this two-level pruning scheme, and evaluates its tradeoff in ranking relevance and time efficiency when searching several test datasets.|最近的研究表明，BM25驱动的动态指数跳跃可以大大加快基于深度影响学习稀疏表示的基于 MaxScore 的文献检索。本文研究了使用 SPLADE 和 uniCOIL 等其他模型进行 top k 检索时，这种遍历指导策略的有效性，发现当 BM25模型与学习权重模型不匹配或检索深度 k 较小时，无约束 BM25驱动的跳跃可能具有明显的相关性退化。本文在总结前人工作的基础上，采用两级剪枝控制策略和稀疏表示模型对齐方法对 BM25引导的索引遍历进行了优化，实现了快速检索。虽然可能会增加延迟的代价，提出的方案比原来的 MaxScore 方法快得多没有 BM25的指导，同时保留了相关性的有效性。本文分析了这种两级剪枝方案的竞争力，并在搜索多个测试数据集时，对其在排序相关性和时间效率方面的权衡进行了评估。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Guided+Traversal+for+Fast+Learned+Sparse+Retrieval)|0|
|[Stability and Efficiency of Personalised Cultural Markets](https://doi.org/10.1145/3543507.3583315)|Haiqing Zhu, Yun Kuen Cheung, Lexing Xie|Royal Holloway University of London, United Kingdom; Australian National University, Australia|This work is concerned with the dynamics of online cultural markets, namely, attention allocation of many users on a set of digital goods with infinite supply. Such dynamic is important in shaping processes and outcomes in society, from trending items in entertainment, collective knowledge creation, to election outcomes. The outcomes of online cultural markets are susceptible to intricate social influence dynamics, particularly so when the community comprises consumers with heterogeneous interests. This has made formal analysis of these markets improbable. In this paper, we remedy this by establishing robust connections between influence dynamics and optimization processes, in trial-offer markets where the consumer preferences are modelled by multinomial logit. Among other results, we show that the proportional-response-esque influence dynamic is equivalent to stochastic mirror descent on a convex objective function, thus leading to a stable and predictable outcome. When all consumers are homogeneous, the objective function has a natural interpretation as a weighted sum of efficiency and diversity of the culture market. In simulations driven by real-world preferences collected from a large-scale recommender system, we observe that ranking strategies aligned with the underlying heterogeneous preferences are more stable, and achieves higher efficiency and diversity. In simulations driven by real-world preferences collected from a large-scale recommender system, we observe that ranking strategies aligned with the underlying heterogeneous preferences are more stable, and achieves higher efficiency and diversity.|本文研究的是在线文化市场的动态变化，即在无限供应的数字商品上，许多用户的注意力分配。这种动态对于塑造社会的进程和结果至关重要，从娱乐的趋势项目、集体知识创造到选举结果。在线文化市场的结果容易受到错综复杂的社会影响力动态的影响，特别是当社区由具有不同兴趣的消费者组成时。这使得对这些市场的正式分析成为不可能。在本文中，我们通过建立影响力动态和最优化过程之间的强有力的联系来纠正这个问题，在试销市场中，消费者的偏好是由多项式 logit 建模的。在其他结果中，我们表明，比例响应方式的影响动态相当于随机镜下降的凸目标函数，从而导致一个稳定和可预测的结果。当所有消费者都是同质的时候，目标函数就自然地被解释为文化市场效率和多样性的加权和。在从大规模推荐系统中收集的现实世界偏好驱动的模拟中，我们观察到与潜在的异质偏好相一致的排序策略更加稳定，并且实现更高的效率和多样性。在从大规模推荐系统中收集的现实世界偏好驱动的模拟中，我们观察到与潜在的异质偏好相一致的排序策略更加稳定，并且实现更高的效率和多样性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Stability+and+Efficiency+of+Personalised+Cultural+Markets)|0|
|[Eligibility Mechanisms: Auctions Meet Information Retrieval](https://doi.org/10.1145/3543507.3583478)|Gagan Goel, Renato Paes Leme, Jon Schneider, David Thompson, Hanrui Zhang|Google, USA; Carnegie Mellon University, USA|The design of internet advertisement systems is both an auction design problem and an information retrieval (IR) problem. As an auction, the designer needs to take the participants incentives into account. As an information retrieval problem, it needs to identify the ad that it is the most relevant to a user out of an enormous set of ad candidates. Those aspects are combined by first having an IR system narrow down the initial set of ad candidates to a manageable size followed by an auction that ranks and prices those candidates. If the IR system uses information about bids, agents could in principle manipulate the system by manipulating the IR stage even when the subsequent auction is truthful. In this paper we investigate the design of truthful IR mechanisms, which we term eligibility mechanisms. We model it as a truthful version of the stochastic probing problem. We show that there is a constant gap between the truthful and non-truthful versions of the stochastic probing problem and exhibit a constant approximation algorithm. En route, we also characterize the set of eligibility mechanisms, which provides necessary and sufficient conditions for an IR system to be truthful.|互联网广告系统的设计既是一个拍卖设计问题，也是一个信息检索(IR)问题。作为一种拍卖，设计师需要考虑参与者的激励因素。作为一个信息检索问题，它需要从大量的候选广告中找出与用户最相关的广告。将这些方面结合起来，首先通过投资者关系系统将最初的广告候选人缩小到一个可管理的规模，然后通过拍卖对这些候选人进行排名和定价。如果 IR 系统使用关于出价的信息，即使随后的拍卖是真实的，代理人原则上也可以通过操纵 IR 阶段来操纵系统。本文研究了真实信息检索机制的设计，我们称之为资格机制。我们将其建模为随机探测问题的真实版本。我们证明了随机探测问题的真实版本和非真实版本之间存在一个恒定的差距，并呈现出一个恒定的近似演算法。在此过程中，我们还刻画了一组资格机制，它为 IR 系统的真实性提供了充分的必要条件。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Eligibility+Mechanisms:+Auctions+Meet+Information+Retrieval)|0|
|[Scoping Fairness Objectives and Identifying Fairness Metrics for Recommender Systems: The Practitioners' Perspective](https://doi.org/10.1145/3543507.3583204)|Jessie J. Smith, Lex Beattie, Henriette Cramer|Spotify, USA; University of Colorado, Boulder, USA|Measuring and assessing the impact and “fairness’’ of recommendation algorithms is central to responsible recommendation efforts. However, the complexity of fairness definitions and the proliferation of fairness metrics in research literature have led to a complex decision-making space. This environment makes it challenging for practitioners to operationalize and pick metrics that work within their unique context. This suggests that practitioners require more decision-making support, but it is not clear what type of support would be beneficial. We conducted a literature review of 24 papers to gather metrics introduced by the research community for measuring fairness in recommendation and ranking systems. We organized these metrics into a ‘decision-tree style’ support framework designed to help practitioners scope fairness objectives and identify fairness metrics relevant to their recommendation domain and application context. To explore the feasibility of this approach, we conducted 15 semi-structured interviews using this framework to assess which challenges practitioners may face when scoping fairness objectives and metrics for their system, and which further support may be needed beyond such tools.|衡量和评估推荐算法的影响和“公平性”是负责任的推荐工作的核心。然而，公平定义的复杂性和研究文献中公平度量标准的泛滥导致了决策空间的复杂性。这种环境使得从业人员很难操作和挑选在其独特上下文中工作的度量。这表明从业人员需要更多的决策支持，但不清楚哪种类型的支持将是有益的。我们对24篇论文进行了文献回顾，收集了研究团体引入的衡量推荐和排名系统公平性的指标。我们将这些指标组织成一个“决策树风格”的支持框架，旨在帮助从业人员确定公平目标，并确定与其推荐领域和应用程序上下文相关的公平指标。为了探索这种方法的可行性，我们使用这个框架进行了15次半结构化访谈，以评估从业者在确定公平目标和系统指标时可能面临的挑战，以及这些工具之外可能还需要哪些进一步的支持。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scoping+Fairness+Objectives+and+Identifying+Fairness+Metrics+for+Recommender+Systems:+The+Practitioners'+Perspective)|0|
|[Same Same, But Different: Conditional Multi-Task Learning for Demographic-Specific Toxicity Detection](https://doi.org/10.1145/3543507.3583290)|Soumyajit Gupta, Sooyong Lee, Maria DeArteaga, Matthew Lease||Algorithmic bias often arises as a result of differential subgroup validity, in which predictive relationships vary across groups. For example, in toxic language detection, comments targeting different demographic groups can vary markedly across groups. In such settings, trained models can be dominated by the relationships that best fit the majority group, leading to disparate performance. We propose framing toxicity detection as multi-task learning (MTL), allowing a model to specialize on the relationships that are relevant to each demographic group while also leveraging shared properties across groups. With toxicity detection, each task corresponds to identifying toxicity against a particular demographic group. However, traditional MTL requires labels for all tasks to be present for every data point. To address this, we propose Conditional MTL (CondMTL), wherein only training examples relevant to the given demographic group are considered by the loss function. This lets us learn group specific representations in each branch which are not cross contaminated by irrelevant labels. Results on synthetic and real data show that using CondMTL improves predictive recall over various baselines in general and for the minority demographic group in particular, while having similar overall accuracy.|算法偏差通常由于不同的子群效度而产生，其中预测关系因组而异。例如，在毒性语言检测中，针对不同人口群体的评论可能因群体而有显著差异。在这种情况下，训练有素的模型可以被最适合大多数群体的关系所主导，从而导致不同的表现。我们建议将毒性检测框架为多任务学习(MTL) ，允许模型专门处理与每个人口组相关的关系，同时利用跨组的共享属性。通过毒性检测，每项任务都对应于确定针对特定人口群体的毒性。但是，传统的 MTL 要求为每个数据点显示所有任务的标签。为了解决这个问题，我们提出了条件 MTL (CondMTL) ，其中只有与给定人口组相关的训练实例被损失函数考虑。这使我们能够了解每个分支中不受不相关标签交叉污染的特定分组表示。对合成和真实数据的研究结果表明，使用 CondMTL 提高了对各种基线的预测性回忆，特别是对少数人口群体，同时具有相似的整体准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Same+Same,+But+Different:+Conditional+Multi-Task+Learning+for+Demographic-Specific+Toxicity+Detection)|0|
|[Towards Explainable Collaborative Filtering with Taste Clusters Learning](https://doi.org/10.1145/3543507.3583303)|Yuntao Du, Jianxun Lian, Jing Yao, Xiting Wang, Mingqi Wu, Lu Chen, Yunjun Gao, Xing Xie||Collaborative Filtering (CF) is a widely used and effective technique for recommender systems. In recent decades, there have been significant advancements in latent embedding-based CF methods for improved accuracy, such as matrix factorization, neural collaborative filtering, and LightGCN. However, the explainability of these models has not been fully explored. Adding explainability to recommendation models can not only increase trust in the decisionmaking process, but also have multiple benefits such as providing persuasive explanations for item recommendations, creating explicit profiles for users and items, and assisting item producers in design improvements. In this paper, we propose a neat and effective Explainable Collaborative Filtering (ECF) model that leverages interpretable cluster learning to achieve the two most demanding objectives: (1) Precise - the model should not compromise accuracy in the pursuit of explainability; and (2) Self-explainable - the model's explanations should truly reflect its decision-making process, not generated from post-hoc methods. The core of ECF is mining taste clusters from user-item interactions and item profiles.We map each user and item to a sparse set of taste clusters, and taste clusters are distinguished by a few representative tags. The user-item preference, users/items' cluster affiliations, and the generation of taste clusters are jointly optimized in an end-to-end manner. Additionally, we introduce a forest mechanism to ensure the model's accuracy, explainability, and diversity. To comprehensively evaluate the explainability quality of taste clusters, we design several quantitative metrics, including in-cluster item coverage, tag utilization, silhouette, and informativeness. Our model's effectiveness is demonstrated through extensive experiments on three real-world datasets.|协同过滤(CF)是推荐系统中广泛使用的有效技术。近几十年来，基于潜在嵌入的 CF 方法在提高准确性方面取得了重大进展，例如矩阵分解、神经协同过滤和 LightGCN。然而，这些模型的可解释性还没有得到充分的探索。为推荐模型增加可解释性不仅可以增加对决策过程的信任，而且还有多种好处，例如为项目推荐提供有说服力的解释，为用户和项目创建明确的配置文件，以及帮助项目生产者改进设计。在本文中，我们提出了一个简洁而有效的可解释协同过滤(ECF)模型，它利用可解释的聚类学习来实现两个最严格的目标: (1)精确——模型在追求可解释性的过程中不应该损害准确性; (2)自我解释——模型的解释应该真实地反映其决策过程，而不是由事后方法产生。ECF 的核心是从用户-项目交互和项目配置文件中挖掘味觉集群。我们将每个用户和项目映射到一个稀疏的味觉集群，味觉集群通过几个代表性的标签来区分。用户项偏好、用户/项目的集群附属关系以及味道集群的生成都以端到端的方式进行了联合优化。此外，我们还引入了森林机制来保证模型的准确性、可解释性和多样性。为了全面评价味觉集群的可解释性质量，我们设计了几个量化指标，包括集群内项目覆盖率、标签利用率、轮廓和信息量。通过对三个实际数据集的大量实验，验证了该模型的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Explainable+Collaborative+Filtering+with+Taste+Clusters+Learning)|0|
|[Towards Fair Allocation in Social Commerce Platforms](https://doi.org/10.1145/3543507.3583398)|Anjali Gupta, Shreyans J. Nagori, Abhijnan Chakraborty, Rohit Vaish, Sayan Ranu, Prajit Prashant Sinai Nadkarni, Narendra Varma Dasararaju, Muthusamy Chelliah|Indian Institute of Technology Delhi, India; Flipkart Internet Pvt. Ltd., India|Social commerce platforms are emerging businesses where producers sell products through re-sellers who advertise the products to other customers in their social network. Due to the increasing popularity of this business model, thousands of small producers and re-sellers are starting to depend on these platforms for their livelihood; thus, it is important to provide fair earning opportunities to them. The enormous product space in such platforms prohibits manual search, and motivates the need for recommendation algorithms to effectively allocate product exposure and, consequently, earning opportunities. In this work, we focus on the fairness of such allocations in social commerce platforms and formulate the problem of assigning products to re-sellers as a fair division problem with indivisible items under two-sided cardinality constraints, wherein each product must be given to at least a certain number of re-sellers and each re-seller must get a certain number of products. Our work systematically explores various well-studied benchmarks of fairness—including Nash social welfare, envy-freeness up to one item (EF1), and equitability up to one item (EQ1)—from both theoretical and experimental perspectives. We find that the existential and computational guarantees of these concepts known from the unconstrained setting do not extend to our constrained model. To address this limitation, we develop a mixed-integer linear program and other scalable heuristics that provide near-optimal approximation of Nash social welfare in simulated and real social commerce datasets. Overall, our work takes the first step towards achieving provable fairness alongside reasonable revenue guarantees on social commerce platforms.|社交商务平台是新兴企业，生产商通过转销商向社交网络中的其他客户推销产品。由于这种商业模式日益流行，成千上万的小生产商和转售商开始依赖这些平台谋生; 因此，必须为他们提供公平的赚钱机会。这些平台中巨大的产品空间禁止手工搜索，并激发了对推荐算法的需求，以有效地分配产品曝光率，从而获得机会。本文研究了社会商务平台中这种分配的公平性问题，将产品分配给转销商的问题转化为具有双侧基数约束的不可分项的公平分配问题，其中每个产品必须分配给至少一定数量的转销商，每个转销商必须得到一定数量的产品。我们的研究从理论和实验两个角度系统地探讨了各种已经得到充分研究的公平标准，包括纳什社会福利标准、一个项目下的嫉妒自由标准(EF1)和一个项目下的公平标准(EQ1)。我们发现，这些概念的存在性和计算保证已知的无约束设置不扩展到我们的约束模型。为了解决这一局限性，我们开发了一个混合整数线性规划和其他可扩展的启发式算法，在模拟和真实的社会商务数据集中提供纳什社会福利的近似最优近似值。总体而言，我们的工作朝着实现可证明的公平迈出了第一步，同时在社交商务平台上实现了合理的收入保障。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Fair+Allocation+in+Social+Commerce+Platforms)|0|
|[Fairness-Aware Clique-Preserving Spectral Clustering of Temporal Graphs](https://doi.org/10.1145/3543507.3583423)|Dongqi Fu, Dawei Zhou, Ross Maciejewski, Arie Croitoru, Marcus Boyd, Jingrui He|Arizona State University, USA; Virginia Tech, USA; University of Illinois at Urbana-Champaign, USA; George Mason University, USA; University of Maryland, College Park, USA|With the widespread development of algorithmic fairness, there has been a surge of research interest that aims to generalize the fairness notions from the attributed data to the relational data (graphs). The vast majority of existing work considers the fairness measure in terms of the low-order connectivity patterns (e.g., edges), while overlooking the higher-order patterns (e.g., k-cliques) and the dynamic nature of real-world graphs. For example, preserving triangles from graph cuts during clustering is the key to detecting compact communities; however, if the clustering algorithm only pays attention to triangle-based compactness, then the returned communities lose the fairness guarantee for each group in the graph. Furthermore, in practice, when the graph (e.g., social networks) topology constantly changes over time, one natural question is how can we ensure the compactness and demographic parity at each timestamp efficiently. To address these problems, we start from the static setting and propose a spectral method that preserves clique connections and incorporates demographic fairness constraints in returned clusters at the same time. To make this static method fit for the dynamic setting, we propose two core techniques, Laplacian Update via Edge Filtering and Searching and Eigen-Pairs Update with Singularity Avoided. Finally, all proposed components are combined into an end-to-end clustering framework named F-SEGA, and we conduct extensive experiments to demonstrate the effectiveness, efficiency, and robustness of F-SEGA.|随着算法公平性的广泛发展，将属性数据的公平性概念推广到关系数据(图)的研究兴趣日益高涨。绝大多数现有的工作考虑了低阶连通性模式(例如边)的公平性度量，而忽略了高阶模式(例如 k-cliques)和真实世界图的动态性质。例如，在聚类过程中保留图切割中的三角形是检测紧密群体的关键，但是如果聚类算法只关注基于三角形的紧密性，那么返回的群体就失去了对图中每个群体的公平性保证。此外，在实践中，当图形(例如，社交网络)拓扑随着时间不断变化时，一个自然的问题是我们如何有效地确保每个时间戳的紧凑性和人口平等性。为了解决这些问题，我们从静态设置开始，并提出了一个谱方法，保留了派系连接，同时在返回的集群中引入了人口公平约束。为了使这种静态方法适合于动态设置，我们提出了两个核心技术: 通过边缘滤波和搜索的拉普拉斯更新和避免奇异性的特征对更新。最后，将所有提出的构件组合成一个端到端的聚类框架 F-SEGA，并进行了广泛的实验来验证 F-SEGA 的有效性、高效性和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness-Aware+Clique-Preserving+Spectral+Clustering+of+Temporal+Graphs)|0|
|[HybridEval: A Human-AI Collaborative Approach for Evaluating Design Ideas at Scale](https://doi.org/10.1145/3543507.3583496)|Sepideh Mesbah, Ines Arous, Jie Yang, Alessandro Bozzon|Booking, Netherlands; University of Fribourg, Switzerland; Delft University of Technology, Netherlands|Evaluating design ideas is necessary to predict their success and assess their impact early on in the process. Existing methods rely either on metrics computed by systems that are effective but subject to errors and bias, or experts’ ratings, which are accurate but expensive and long to collect. Crowdsourcing offers a compelling way to evaluate a large number of design ideas in a short amount of time while being cost-effective. Workers’ evaluation is, however, less reliable and might substantially differ from experts’ evaluation. In this work, we investigate workers’ rating behavior and compare it with experts. First, we instrument a crowdsourcing study where we asked workers to evaluate design ideas from three innovation challenges. We show that workers share similar insights with experts but tend to rate more generously and weigh certain criteria more importantly. Next, we develop a hybrid human-AI approach that combines a machine learning model with crowdsourcing to evaluate ideas. Our approach models workers’ reliability and bias while leveraging ideas’ textual content to train a machine learning model. It is able to incorporate experts’ ratings whenever available, to supervise the model training and infer worker performance. Results show that our framework outperforms baseline methods and requires significantly less training data from experts, thus providing a viable solution for evaluating ideas at scale.|评估设计想法是必要的，以预测他们的成功和评估他们的影响早在过程中。现有的方法要么依赖于有效但容易出错和偏差的系统计算出的指标，要么依赖于专家的评分，这些评分准确但昂贵，而且需要很长时间才能收集到。众包提供了一个引人注目的方式来评估大量的设计想法在短时间内，同时具有成本效益。然而，工人的评估不太可靠，可能与专家的评估大不相同。在这项工作中，我们调查工人的评分行为，并与专家进行比较。首先，我们进行了一项众包研究，要求工人评估来自三个创新挑战的设计理念。我们的研究表明，员工与专家有着相似的见解，但他们倾向于更慷慨地给出评价，更重视某些标准。接下来，我们开发了一种混合的人工智能方法，它结合了机器学习模型和众包来评估想法。我们的方法模拟工人的可靠性和偏见，同时利用想法的文本内容来训练机器学习模型。它能够在任何时候合并专家的评分，以监督模型培训和推断工人的表现。结果表明，我们的框架优于基线方法，需要的专家培训数据明显减少，从而为大规模评估想法提供了一个可行的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HybridEval:+A+Human-AI+Collaborative+Approach+for+Evaluating+Design+Ideas+at+Scale)|0|
|[A Multi-task Model for Emotion and Offensive Aided Stance Detection of Climate Change Tweets](https://doi.org/10.1145/3543507.3583860)|Apoorva Upadhyaya, Marco Fisichella, Wolfgang Nejdl|L3S Research Center, Leibniz University Hannover, Germany|In this work, we address the United Nations Sustainable Development Goal 13: Climate Action by focusing on identifying public attitudes toward climate change on social media platforms such as Twitter. Climate change is threatening the health of the planet and humanity. Public engagement is critical to address climate change. However, climate change conversations on Twitter tend to polarize beliefs, leading to misinformation and fake news that influence public attitudes, often dividing them into climate change believers and deniers. Our paper proposes an approach to classify the attitude of climate change tweets (believe/deny/ambiguous) to identify denier statements on Twitter. Most existing approaches for detecting stances and classifying climate change tweets either overlook deniers’ tweets or do not have a suitable architecture. The relevant literature suggests that emotions and higher levels of toxicity are prevalent in climate change Twitter conversations, leading to a delay in appropriate climate action. Therefore, our work focuses on learning stance detection (main task) while exploiting the auxiliary tasks of recognizing emotions and offensive utterances. We propose a multimodal multitasking framework MEMOCLiC that captures the input data using different embedding techniques and attention frameworks, and then incorporates the learned emotional and offensive expressions to obtain an overall representation of the features relevant to the stance of the input tweet. Extensive experiments conducted on a novel curated climate change dataset and two benchmark stance detection datasets (SemEval-2016 and ClimateStance-2022) demonstrate the effectiveness of our approach.|在这项工作中，我们致力于实现联合国可持续发展目标13: 气候行动，重点是在 Twitter 等社交媒体平台上确定公众对气候变化的态度。气候变化正威胁着地球和人类的健康。公众参与对应对气候变化至关重要。然而，Twitter 上关于气候变化的讨论往往会导致信仰的两极分化，导致错误信息和假新闻，从而影响公众态度，往往将公众分为气候变化信徒和否认者。我们的论文提出了一种方法来分类气候变化推文的态度(相信/否认/模棱两可) ，以确定否认者的声明在 Twitter 上。大多数现有的检测立场和分类气候变化推文的方法要么忽略否认者的推文，要么没有一个合适的架构。相关文献表明，在气候变化的 Twitter 对话中，情绪和较高的毒性水平普遍存在，导致适当的气候行动出现延误。因此，我们的工作集中在学习姿势检测(主要任务) ，同时利用辅助任务的识别情绪和攻击性话语。我们提出了一个多模式多任务框架 MEMOCLiC，它使用不同的嵌入技术和注意力框架捕获输入数据，然后结合所学到的情绪和攻击性表达来获得与输入 tweet 立场相关的特征的整体表示。在一个新的策划气候变化数据集和两个基准姿态检测数据集(SemEval-2016和 ClimateStance-2022)上进行的大量实验证明了我们方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Multi-task+Model+for+Emotion+and+Offensive+Aided+Stance+Detection+of+Climate+Change+Tweets)|0|
|[Cross-center Early Sepsis Recognition by Medical Knowledge Guided Collaborative Learning for Data-scarce Hospitals](https://doi.org/10.1145/3543507.3583989)|Ruiqing Ding, Fangjie Rong, Xiao Han, Leye Wang|Peking University, China; Shanghai University of Finance and Economics, China|There are significant regional inequities in health resources around the world. It has become one of the most focused topics to improve health services for data-scarce hospitals and promote health equity through knowledge sharing among medical institutions. Because electronic medical records (EMRs) contain sensitive personal information, privacy protection is unavoidable and essential for multi-hospital collaboration. In this paper, for a common disease in ICU patients, sepsis, we propose a novel cross-center collaborative learning framework guided by medical knowledge, SofaNet, to achieve early recognition of this disease. The Sepsis-3 guideline, published in 2016, defines that sepsis can be diagnosed by satisfying both suspicion of infection and Sequential Organ Failure Assessment (SOFA) greater than or equal to 2. Based on this knowledge, SofaNet adopts a multi-channel GRU structure to predict SOFA values of different systems, which can be seen as an auxiliary task to generate better health status representations for sepsis recognition. Moreover, we only achieve feature distribution alignment in the hidden space during cross-center collaborative learning, which ensures secure and compliant knowledge transfer without raw data exchange. Extensive experiments on two open clinical datasets, MIMIC-III and Challenge, demonstrate that SofaNet can benefit early sepsis recognition when hospitals only have limited EMRs.|世界各地在卫生资源方面存在着严重的区域不平等。通过医疗机构之间的知识共享，改善数据稀缺的医院的卫生服务，促进卫生公平，已成为最重点的议题之一。由于电子病历(EMR)包含敏感的个人信息，隐私保护是不可避免的，也是多医院合作的必要条件。在这篇文章中，我们针对 ICU 患者常见的一种疾病，败血症，提出了一种新的跨中心合作学习框架，以医学知识为指导，SofaNet，以实现对这种疾病的早期识别。2016年发布的脓毒症3指南规定，脓毒症可以通过满足感染的怀疑和大于或等于2的序贯性器官衰竭评估(SOFA)来诊断。在此基础上，SofaNet 采用多通道 GRU 结构来预测不同系统的 SOFA 值，这可以看作是一个辅助任务，以产生更好的脓毒症识别健康状态表示。此外，我们只有在跨中心合作学习时才能在隐藏空间中实现特征分布对齐，从而确保在没有原始数据交换的情况下安全和兼容的知识传输。在两个开放的临床数据集 MIMIC-III 和 Challenge 上的大量实验表明，当医院只有有限的 EMR 时，SofaNet 可以有利于早期脓毒症识别。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-center+Early+Sepsis+Recognition+by+Medical+Knowledge+Guided+Collaborative+Learning+for+Data-scarce+Hospitals)|0|
|[Breaking Filter Bubble: A Reinforcement Learning Framework of Controllable Recommender System](https://doi.org/10.1145/3543507.3583856)|Zhenyang Li, Yancheng Dong, Chen Gao, Yizhou Zhao, Dong Li, Jianye Hao, Kai Zhang, Yong Li, Zhi Wang|Tsinghua Shenzhen International Graduate School, Tsinghua University, China and Research Institute of Tsinghua, Pearl River Delta, China; Tsinghua University, China; Carnegie Mellon University, USA; Tsinghua-Berkeley Shenzhen Institute, Tsinghua Shenzhen International Graduate School, China and Peng Cheng Laboratory, China; Tsinghua University, China and Huawei Noah's Ark Lab, China; Huawei Noah's Ark Lab, China|In the information-overloaded era of the Web, recommender systems that provide personalized content filtering are now the mainstream portal for users to access Web information. Recommender systems deploy machine learning models to learn users’ preferences from collected historical data, leading to more centralized recommendation results due to the feedback loop. As a result, it will harm the ranking of content outside the narrowed scope and limit the options seen by users. In this work, we first conduct data analysis from a graph view to observe that the users’ feedback is restricted to limited items, verifying the phenomenon of centralized recommendation. We further develop a general simulation framework to derive the procedure of the recommender system, including data collection, model learning, and item exposure, which forms a loop. To address the filter bubble issue under the feedback loop, we then propose a general and easy-to-use reinforcement learning-based method, which can adaptively select few but effective connections between nodes from different communities as the exposure list. We conduct extensive experiments in the simulation framework based on large-scale real-world datasets. The results demonstrate that our proposed reinforcement learning-based control method can serve as an effective solution to alleviate the filter bubble and the separated communities induced by it. We believe the proposed framework of controllable recommendation in this work can inspire not only the researchers of recommender systems, but also a broader community concerned with artificial intelligence algorithms’ impact on humanity, especially for those vulnerable populations on the Web.|在信息过载的 Web 时代，提供个性化内容过滤的推荐系统现在已经成为用户访问 Web 信息的主流门户。推荐系统部署机器学习模型，从收集的历史数据中了解用户的偏好，由于反馈回路的存在，推荐结果更加集中。因此，它将损害内容在狭窄范围之外的排名，并限制用户看到的选项。本文首先从图的角度进行数据分析，发现用户的反馈仅限于有限的项目，验证了集中推荐的现象。我们进一步开发了一个通用的模拟框架来推导推荐系统的过程，包括数据收集、模型学习和项目曝光，形成一个循环。为了解决反馈回路下的过滤泡问题，提出了一种通用的、易于使用的强化学习方法，该方法可以自适应地选择不同社区节点之间少量但有效的连接作为暴露列表。我们在基于大规模真实世界数据集的仿真框架中进行了广泛的实验。结果表明，本文提出的基于强化学习的控制方法可以作为一种有效的解决方案，以减轻过滤气泡及其引起的社区分离。我们相信，这项工作中提出的可控推荐框架不仅可以激励推荐系统的研究人员，而且可以激励更广泛的社区关注人工智能算法对人类的影响，特别是对那些网络上的弱势群体。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Breaking+Filter+Bubble:+A+Reinforcement+Learning+Framework+of+Controllable+Recommender+System)|0|
|[CollabEquality: A Crowd-AI Collaborative Learning Framework to Address Class-wise Inequality in Web-based Disaster Response](https://doi.org/10.1145/3543507.3583871)|Yang Zhang, Lanyu Shang, Ruohan Zong, Huimin Zeng, Zhenrui Yue, Dong Wang|School of Information Sciences, University of Illinois Urbana-Champaign, USA|Web-based disaster response (WebDR) is emerging as a pervasive approach to acquire real-time situation awareness of disaster events by collecting timely observations from the Web (e.g., social media). This paper studies a class-wise inequality problem in WebDR applications where the objective is to address the limitation of current WebDR solutions that often have imbalanced classification performance across different classes. To address such a limitation, this paper explores the collaborative strengths of the diversified yet complementary biases of AI and crowdsourced human intelligence to ensure a more balanced and accurate performance for WebDR applications. However, two critical challenges exist: 1) it is difficult to identify the imbalanced AI results without knowing the ground-truth WebDR labels a priori; ii) it is non-trivial to address the class-wise inequality problem using potentially imperfect crowd labels. To address the above challenges, we develop CollabEquality, an inequality-aware crowd-AI collaborative learning framework that carefully models the inequality bias of both AI and human intelligence from crowdsourcing systems into a principled learning framework. Extensive experiments on two real-world WebDR applications demonstrate that CollabEquality consistently outperforms the state-of-the-art baselines by significantly reducing class-wise inequality while improving the WebDR classification accuracy.|基于 Web 的灾难响应(WebDR)正在成为一种普遍的方法，通过从 Web (例如，社交媒体)收集及时的观察结果来获得对灾难事件的实时情况感知。本文研究了 WebDR 应用程序中的类别不等式问题，其目的是解决目前 WebDR 解决方案的局限性，这些解决方案通常在不同类别之间具有不平衡的分类性能。为了解决这一局限性，本文探讨了人工智能和众包人类智能的多样化但互补的偏见的协作优势，以确保更平衡和准确的 WebDR 应用程序的性能。然而，存在两个关键的挑战: 1)在不知道基本事实 WebDR 标签的情况下很难识别不平衡的 AI 结果; 2)使用潜在不完美的群体标签来解决类别不平等问题是不平凡的。为了应对上述挑战，我们开发了 Collabequity，这是一个意识到不平等的群体人工智能合作学习框架，它仔细地将人工智能和人类智能的不平等偏见从众包系统建模成一个有原则的学习框架。在两个真实世界的 WebDR 应用程序上进行的大量实验表明，CollabEquity 通过显著减少类别不平等，同时提高 WebDR 分类准确性，始终优于最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CollabEquality:+A+Crowd-AI+Collaborative+Learning+Framework+to+Address+Class-wise+Inequality+in+Web-based+Disaster+Response)|0|
|[MoleRec: Combinatorial Drug Recommendation with Substructure-Aware Molecular Representation Learning](https://doi.org/10.1145/3543507.3583872)|Nianzu Yang, Kaipeng Zeng, Qitian Wu, Junchi Yan|Shanghai Jiao Tong University, China|Combinatorial drug recommendation involves recommending a personalized combination of medication (drugs) to a patient over his/her longitudinal history, which essentially aims at solving a combinatorial optimization problem that pursues high accuracy under the safety constraint. Among existing learning-based approaches, the association between drug substructures (i.e., a sub-graph of the molecule that contributes to certain chemical effect) and the target disease is largely overlooked, though the function of drugs in fact exhibits strong relevance with particular substructures. To address this issue, we propose a molecular substructure-aware encoding method entitled MoleRec that entails a hierarchical architecture aimed at modeling inter-substructure interactions and individual substructures’ impact on patient’s health condition, in order to identify those substructures that really contribute to healing patients. Specifically, MoleRec learns to attentively pooling over substructure representations which will be element-wisely re-scaled by the model’s inferred relevancy with a patient’s health condition to obtain a prior-knowledge-informed drug representation. We further design a weight annealing strategy for drug-drug-interaction (DDI) objective to adaptively control the balance between accuracy and safety criteria throughout training. Experiments on the MIMIC-III dataset demonstrate that our approach achieves new state-of-the-art performance w.r.t. four accuracy and safety metrics. Our source code is publicly available at https://github.com/yangnianzu0515/MoleRec.|组合药物推荐包括在病人的纵向病史中向病人推荐个性化的药物组合(药物) ，其主要目的是解决在安全约束下追求高准确性的组合优化问题。在现有的基于学习的方法中，药物子结构(即有助于某些化学效应的分子的子图)与目标疾病之间的关联在很大程度上被忽视，尽管药物的功能实际上与特定的子结构显示出强烈的相关性。为了解决这个问题，我们提出了一个名为 MoleRec 的分子子结构感知编码方法，它需要一个层次结构，旨在建模子结构间的相互作用和个体子结构对患者健康状况的影响，以确定那些真正有助于治愈患者的子结构。具体而言，MoleRec 学习仔细汇集子结构表示，这些子结构表示将通过模型与患者健康状况的推断相关性进行元素智能重新缩放，以获得事先知情的药物表示。我们进一步设计了药物-药物相互作用(DDI)目标的权重退火策略，以自适应地控制整个训练过程中准确性和安全性标准之间的平衡。在 MIMIC-III 数据集上的实验表明，我们的方法实现了新的最先进的性能和四个准确性和安全性指标。我们的源代码可以在 https://github.com/yangnianzu0515/molerec 上公开。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MoleRec:+Combinatorial+Drug+Recommendation+with+Substructure-Aware+Molecular+Representation+Learning)|0|
|[Moral Narratives Around the Vaccination Debate on Facebook](https://doi.org/10.1145/3543507.3583865)|Mariano Gastón Beiró, Jacopo D'Ignazi, Victoria Perez Bustos, Maria Florencia Prado, Kyriaki Kalimeri|; Universidad de Buenos Aires. Facultad de Ingeniería, Paseo Colón 850, C1063ACV, Argentina and CONICET, Universidad de Buenos Aires, INTECIN, Paseo Colón 850, C1063ACV, Argentina; ISI Foundation, Italy|Vaccine hesitancy is a complex issue with psychological, cultural, and even societal factors entangled in the decision-making process. The narrative around this process is captured in our everyday interactions; social media data offer a direct and spontaneous view of peoples' argumentation. Here, we analysed more than 500,000 public posts and comments from Facebook Pages dedicated to the topic of vaccination to study the role of moral values and, in particular, the understudied role of the Liberty moral foundation from the actual user-generated text. We operationalise morality by employing the Moral Foundations Theory, while our proposed framework is based on recurrent neural network classifiers with a short memory and entity linking information. Our findings show that the principal moral narratives around the vaccination debate focus on the values of Liberty, Care, and Authority. Vaccine advocates urge compliance with the authorities as prosocial behaviour to protect society. On the other hand, vaccine sceptics mainly build their narrative around the value of Liberty, advocating for the right to choose freely whether to adhere or not to the vaccination. We contribute to the automatic understanding of vaccine hesitancy drivers emerging from user-generated text, providing concrete insights into the moral framing around vaccination decision-making. Especially in emergencies such as the Covid-19 pandemic, contrary to traditional surveys, these insights can be provided contemporary to the event, helping policymakers craft communication campaigns that adequately address the concerns of the hesitant population.|疫苗犹豫不决是一个复杂的问题，心理，文化，甚至社会因素纠缠在决策过程中。围绕这一过程的叙述被捕捉在我们日常的互动中; 社交媒体数据提供了人们争论的直接和自发的视角。在这里，我们分析了超过500,000个来自 Facebook 页面的公开帖子和评论，这些帖子和评论专门针对疫苗接种这一主题，研究道德价值观的作用，特别是从实际的用户生成的文本中研究自由道德基础的被忽视的作用。我们运用道德基础理论来操作道德，而我们提出的框架是基于短记忆的递归神经网络分类器和连接信息的实体。我们的研究结果表明，围绕疫苗接种辩论的主要道德叙事集中在自由、关怀和权威的价值观上。疫苗倡导者敦促当局遵守规定，以此作为保护社会的亲社会行为。另一方面，疫苗怀疑论者主要围绕自由的价值建立他们的叙述，倡导自由选择是否坚持接种疫苗的权利。我们有助于自动理解疫苗犹豫驱动程序出现从用户生成的文本，提供具体的见解围绕疫苗接种决策的道德框架。特别是在2019冠状病毒疾病大流行这样的紧急情况下，与传统的调查不同，这些见解可以在事件发生时提供，帮助政策制定者制定沟通运动，充分解决犹豫不决的民众的关切。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Moral+Narratives+Around+the+Vaccination+Debate+on+Facebook)|0|
|[Exploration of Framing Biases in Polarized Online Content Consumption](https://doi.org/10.1145/3543873.3587534)|Markus ReiterHaas|Institute of Interactive Systems and Data Science, Graz University of Technology, Austria|The study of framing bias on the Web is crucial in our digital age, as the framing of information can influence human behavior and decision on critical issues such as health or politics. Traditional frame analysis requires a curated set of frames derived from manual content analysis by domain experts. In this work, we introduce a frame analysis approach based on pretrained Transformer models that let us capture frames in an exploratory manner beyond predefined frames. In our experiments on two public online news and social media datasets, we show that our approach lets us identify underexplored conceptualizations, such as that health-related content is framed in terms of beliefs for conspiracy media, while mainstream media is instead concerned with science. We anticipate our work to be a starting point for further research on exploratory computational framing analysis using pretrained Transformers.|在我们的数字时代，研究网络上的框架偏见是至关重要的，因为信息的框架可以影响人类的行为和决策，如健康或政治等关键问题。传统的框架分析需要一组精心策划的框架，这些框架来自于领域专家的手工内容分析。在这项工作中，我们介绍了一种基于预先训练的变压器模型的帧分析方法，使我们能够以一种探索性的方式捕获超越预先定义的帧。在我们对两个公共在线新闻和社交媒体数据集的实验中，我们表明，我们的方法让我们确定了未被充分探索的概念化，例如，健康相关的内容被框定在阴谋媒体的信念方面，而主流媒体关注的是科学。我们期望我们的工作是一个开始点，进一步研究探索性计算框架分析使用预先训练的变压器。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploration+of+Framing+Biases+in+Polarized+Online+Content+Consumption)|0|
|[On Modeling Long-Term User Engagement from Stochastic Feedback](https://doi.org/10.1145/3543873.3587626)|Guoxi Zhang, Xing Yao, Xuanji Xiao|Shopee Inc., China; Graduate School of Informatics, Kyoto University, Japan; China Central Depository & Clearing Co., Ltd., China|An ultimate goal of recommender systems (RS) is to improve user engagement. Reinforcement learning (RL) is a promising paradigm for this goal, as it directly optimizes overall performance of sequential recommendation. However, many existing RL-based approaches induce huge computational overhead, because they require not only the recommended items but also all other candidate items to be stored. This paper proposes an efficient alternative that does not require the candidate items. The idea is to model the correlation between user engagement and items directly from data. Moreover, the proposed approach consider randomness in user feedback and termination behavior, which are ubiquitous for RS but rarely discussed in RL-based prior work. With online A/B experiments on real-world RS, we confirm the efficacy of the proposed approach and the importance of modeling the two types of randomness.|推荐系统(RS)的最终目标是提高用户参与度。强化学习(RL)是这个目标的一个很有前途的范例，因为它直接优化了顺序推荐的整体性能。然而，许多现有的基于 RL 的方法会产生巨大的计算开销，因为它们不仅需要存储推荐的项，而且还需要存储所有其他候选项。本文提出了一种不需要候选项的有效方案。其思想是直接从数据中建模用户参与度和项目之间的相关性。此外，提出的方法考虑了用户反馈和终止行为的随机性，这在 RS 中是普遍存在的，但在基于 RL 的先前工作中很少讨论。通过在现实世界中的在线 A/B 实验，我们证实了该方法的有效性和建模两种类型的随机性的重要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Modeling+Long-Term+User+Engagement+from+Stochastic+Feedback)|0|
|[CaML: Carbon Footprinting of Household Products with Zero-Shot Semantic Text Similarity](https://doi.org/10.1145/3543507.3583882)|Bharathan Balaji, Venkata Sai Gargeya Vunnava, Geoffrey Guest, Jared Kramer|Amazon, USA|Products contribute to carbon emissions in each phase of their life cycle, from manufacturing to disposal. Estimating the embodied carbon in products is a key step towards understanding their impact, and undertaking mitigation actions. Precise carbon attribution is challenging at scale, requiring both domain expertise and granular supply chain data. As a first-order approximation, standard reports use Economic Input-Output based Life Cycle Assessment (EIO-LCA) which estimates carbon emissions per dollar at an industry sector level using transactions between different parts of the economy. EIO-LCA models map products to an industry sector, and uses the corresponding carbon per dollar estimates to calculate the embodied carbon footprint of a product. An LCA expert needs to map each product to one of upwards of 1000 potential industry sectors. To reduce the annotation burden, the standard practice is to group products by categories, and map categories to their corresponding industry sector. We present CaML, an algorithm to automate EIO-LCA using semantic text similarity matching by leveraging the text descriptions of the product and the industry sector. CaML uses a pre-trained sentence transformer model to rank the top-5 matches, and asks a human to check if any of them are a good match. We annotated 40K products with non-experts. Our results reveal that pre-defined product categories are heterogeneous with respect to EIO-LCA industry sectors, and lead to a large mean absolute percentage error (MAPE) of 51% in kgCO2e/$. CaML outperforms the previous manually intensive method, yielding a MAPE of 22% with no domain labels (zero-shot). We compared annotations of a small sample of 210 products with LCA experts, and find that CaML accuracy is comparable to that of annotations by non-experts.|产品在其生命周期的每个阶段(从生产到处理)都会造成碳排放。评估产品中的含碳量是了解其影响并采取缓解行动的关键一步。精确的碳归属在规模上具有挑战性，需要领域专业知识和细粒度供应链数据。作为一阶近似，标准报告使用基于经济投入产出的生命周期评估(EIO-LCA) ，该评估利用不同经济部门之间的交易，在行业部门水平上估计每美元的碳排放量。生命周期评估模型将产品映射到一个行业部门，并使用相应的每美元碳排放估计值来计算产品的碳足印。LCA 专家需要将每个产品映射到1000个以上的潜在行业部门之一。为了减少注释负担，标准实践是按类别对产品进行分组，并将类别映射到相应的行业部门。我们提出了 CaML，一种利用产品和工业部门的文本描述，使用语义文本相似性匹配实现 EIO-LCA 自动化的算法。CaML 使用一个预先训练好的句子转换模型对前5个匹配项进行排序，并要求人类检查它们中是否有一个是良好匹配的。我们用非专家注释40K 产品。我们的研究结果表明，预定义的产品类别相对于 EIO-LCA 行业部门是异构的，并导致 kgCO2e/$的大平均绝对百分比误差(MAPE)为51% 。CaML 优于以前的手动密集型方法，产生的 MAPE 为22% ，没有域标签(0-shot)。我们将210个产品的小样本注释与 LCA 专家进行了比较，发现 CaML 的准确性与非专家的注释相当。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CaML:+Carbon+Footprinting+of+Household+Products+with+Zero-Shot+Semantic+Text+Similarity)|0|
|[RDF Playground: An Online Tool for Learning about the Semantic Web](https://doi.org/10.1145/3543873.3587325)|Bastián Inostroza, Raúl Cid, Aidan Hogan|DCC, Universidad de Chile, Chile; DCC, Universidad de Chile, Chile and Instituto Milenio Fundamentos de los Datos (IMFD), Chile|We present RDF Playground: a web-based tool to assist those who wish to learn or teach about the Semantic Web. The tool integrates functionalities relating to the key features of RDF, allowing users to specify an RDF graph in Turtle syntax, visualise it as an interactive graph, query it using SPARQL, reason over it using OWL 2 RL, and to validate it using SHACL or ShEx. The tool further provides the ability to import and explore data from the Web through a graph-based Linked Data browser. We discuss the design and functionality of the tool, its implementation, and the results of a usability study considering students from a Web of Data course that used it for lab assignments. We conclude with a discussion of these results, as well as future directions that we envisage for improving the tool.|我们介绍 RDF Playground: 一个基于 Web 的工具，用于帮助那些希望学习或教授语义 Web 的人。该工具集成了与 RDF 关键特性相关的功能，允许用户用 Turtle 语法指定一个 RDF 图形，将其可视化为一个交互式图形，使用 SPARQL 查询它，使用 OWL 2 RL 推理它，并使用 SHACL 或 ShEx 验证它。该工具还提供了通过基于图形的关联数据浏览器从 Web 导入和探索数据的能力。我们讨论了该工具的设计和功能，它的实现，以及一个可用性研究的结果，该研究考虑了使用它完成实验作业的数据网络课程的学生。最后，我们讨论了这些结果以及我们设想的改进该工具的未来方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RDF+Playground:+An+Online+Tool+for+Learning+about+the+Semantic+Web)|0|
|[Locating Faulty Applications via Semantic and Topology Estimation](https://doi.org/10.1145/3543873.3584660)|Shuyi Niu, Jiawei Jin, Xiutian Huang, Yonggeng Wang, Wenhao Xu, Youyong Kong|Ant Group, China; Southeast University, China|With the explosion of Internet product users, how to locate the faulty ones from numerous back-end applications after a customer complaint has become an essential issue in improving user experience. However, existing solutions mostly rely on manual testing to infer the fault, severely limiting their efficiency. In this paper, we transform the problem of locating faulty applications into two subproblems and propose a fully automated framework. We design a scorecard model in one stage to evaluate the semantic relevance between applications and customer complaints. Then in the other stage, topology graphs that reflect the actual calling relationship and engineering connection relationship between applications are utilized to evaluate the topology relevance between applications. Specifically, we employ a multi-graph co-learning framework constrained by consistency-independence loss and an engineering-theory-driven clustering strategy for the unsupervised learning of graphs. With semantic and topology relevance, we can comprehensively locate relevant faulty applications. Experiments on the Alipay dataset show that our method gains significant improvements in both model performance and efficiency.|随着互联网产品用户的爆炸式增长，如何在用户投诉之后从众多的后端应用程序中定位出故障用户已成为提高用户体验的关键问题。然而，现有的解决方案大多依赖于人工测试来推断故障，这严重限制了它们的效率。本文将故障应用定位问题转化为两个子问题，并提出了一个全自动化的框架。我们在一个阶段中设计了一个记分卡模型来评估应用程序和客户投诉之间的语义相关性。然后在另一个阶段，利用反映实际调用关系和应用间工程连接关系的拓扑图来评估应用间的拓扑相关性。具体来说，我们采用了一个受一致性独立性损失约束的多图协同学习框架和一个工程理论驱动的聚类策略来处理图的非监督式学习。通过语义和拓扑相关性，我们可以全面定位相关的故障应用。在支付宝数据集上的实验表明，该方法在模型性能和效率方面都得到了显著的改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Locating+Faulty+Applications+via+Semantic+and+Topology+Estimation)|0|
|[Analyzing COVID-Related Social Discourse on Twitter using Emotion, Sentiment, Political Bias, Stance, Veracity and Conspiracy Theories](https://doi.org/10.1145/3543873.3587622)|Youri Peskine, Raphaël Troncy, Paolo Papotti|EURECOM, France|Online misinformation has become a major concern in recent years, and it has been further emphasized during the COVID-19 pandemic. Social media platforms, such as Twitter, can be serious vectors of misinformation online. In order to better understand the spread of these fake-news, lies, deceptions, and rumours, we analyze the correlations between the following textual features in tweets: emotion, sentiment, political bias, stance, veracity and conspiracy theories. We train several transformer-based classifiers from multiple datasets to detect these textual features and identify potential correlations using conditional distributions of the labels. Our results show that the online discourse regarding some topics, such as COVID-19 regulations or conspiracy theories, is highly controversial and reflects the actual U.S. political landscape.|近年来，网上虚假信息已成为一个主要问题，在2019冠状病毒疾病大流行期间，这一问题得到了进一步强调。像 Twitter 这样的社交媒体平台可能是网络上错误信息的严重载体。为了更好地理解这些假新闻、谎言、欺骗和谣言的传播，我们分析了以下推文文本特征之间的相关性: 情绪、情绪、政治偏见、立场、真实性和阴谋论。我们训练了多个来自多个数据集的基于转换器的分类器来检测这些文本特征，并使用标签的条件分布来识别潜在的相关性。我们的研究结果表明，网上关于某些话题的讨论，比如2019冠状病毒疾病监管或阴谋论，具有高度的争议性，反映了美国实际的政治环境。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analyzing+COVID-Related+Social+Discourse+on+Twitter+using+Emotion,+Sentiment,+Political+Bias,+Stance,+Veracity+and+Conspiracy+Theories)|0|
|[Machine Learning for Streaming Media](https://doi.org/10.1145/3543873.3589751)|Sudarshan Lamkhede, Praveen Chandar, Vladan Radosavljevic, Amit Goyal, Lan Luo|Amazon Music, USA; Netflix Research, USA; Spotify, USA; University of Southern California, USA|Streaming media has become a popular medium for consumers of all ages, with people spending several hours a day streaming videos, games, music, or podcasts across devices. Most global streaming services have introduced Machine Learning (ML) into their operations to personalize consumer experience, improve content, and further enhance the value proposition of streaming services. Despite the rapid growth, there is a need to bridge the gap between academic research and industry requirements and build connections between researchers and practitioners in the field. This workshop aims to provide a unique forum for practitioners and researchers interested in Machine Learning to get together, exchange ideas and get a pulse for the state of the art in research and burning issues in the industry.|流媒体已经成为所有年龄段消费者的流行媒体，人们每天花几个小时在各种设备上观看视频、游戏、音乐或播客。大多数全球流媒体服务已经将机器学习(ML)引入到它们的运营中，以个性化消费者体验、改善内容并进一步提高流媒体服务的价值主张。尽管增长迅速，但仍需要弥合学术研究和行业需求之间的差距，并在该领域的研究人员和从业人员之间建立联系。本研讨会旨在为对机器学习感兴趣的从业人员和研究人员提供一个独特的论坛，让他们聚集在一起，交流思想，了解行业研究的最新进展和热点问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Machine+Learning+for+Streaming+Media)|0|
|[Interleaved Online Testing in Large-Scale Systems](https://doi.org/10.1145/3543873.3587572)|Nan Bi, Bai Li, Ruoyuan Gao, Graham Edge, Sachin Ahuja|Amazon, USA|Online testing is indispensable in decision making for information retrieval systems. Interleaving emerges as an online testing method with orders of magnitude higher sensitivity than the pervading A/B testing. It merges the compared results into a single interleaved result to show to users, and attributes user actions back to the systems being tested. However, its pairwise design also brings practical challenges to real-world systems, in terms of effectively comparing multiple (more than two) systems and interpreting the magnitude of raw interleaving measurement. We present two novel methods to address these challenges that make interleaving practically applicable. The first method infers the ordering of multiple systems based on interleaving pairwise results with false discovery control. The second method estimates A/B effect size based on interleaving results using a weighted linear model that adjust for uncertainties of different measurements. We showcase the effectiveness of our methods in large-scale e-commerce experiments, reporting as many as 75 interleaving results, and provide extensive evaluations of their underlying assumptions.|在线测试对于信息检索系统的决策是不可或缺的。交错测试作为一种在线测试方法出现，其灵敏度数量级高于普遍采用的 A/B 测试。它将比较的结果合并到一个交错的结果中以显示给用户，并将用户操作归结到正在测试的系统。然而，它的成对设计也给现实世界的系统带来了实际的挑战，就有效地比较多个(两个以上)系统和解释原始交错测量的大小而言。我们提出了两种新的方法来解决这些挑战，使交织实际上适用。第一种方法是基于错误发现控制交织成对结果来推断多系统的排序。第二种方法基于交错结果估计 A/B 效应大小，使用加权线性模型来调整不同测量的不确定性。我们展示了我们的方法在大规模电子商务实验中的有效性，报告了多达75个交错的结果，并提供了对其基本假设的广泛评估。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interleaved+Online+Testing+in+Large-Scale+Systems)|0|
|[Impact of COVID-19 Pandemic on Cultural Products Interests](https://doi.org/10.1145/3543873.3587594)|Ke Li, Zhiwen Yu, Ying Zhang, Bin Guo|School of Computer Science, Northwestern Polytechnical University, China and Harbin Engineering University, China; School of Computer Science, Northwestern Polytechnical University, China|The COVID-19 pandemic has had a significant impact on human behaviors and how it influenced peoples’ interests in cultural products is an unsolved problem. While prior studies mostly adopt subjective surveys to find an answer, these methods are always suffering from high cost, limited size, and subjective bias. Inspired by the rich user-oriented data over the Internet, this work explores the possibility to leverage users’ search logs to reflect humans’ underlying cultural product interests. To further examine how the COVID-19 mobility policy might influence cultural interest changes, we propose a new regression discontinuity design that has the additional potential to predict the recovery phase of peoples’ cultural product interests. By analyzing the 1592 search interest time series in 6 countries, we found different patterns of change in interest in movies, music, and art during the COVID-19 pandemic, but a clear overall incremental increase. Across the six countries we studied, we found that changes in interest in cultural products were found to be strongly correlated with mobility and that as mobility declined, interest in movies, music, and art increased by an average of 35, 27 and 20, respectively, with these changes lasting at least eight weeks.|2019冠状病毒疾病疫情对人类行为产生了重大影响，它如何影响人们对文化产品的兴趣是一个尚未解决的问题。以往的研究大多采用主观调查的方法来寻找答案，但这些方法往往成本高、规模有限、存在主观偏差。受互联网上丰富的以用户为导向的数据的启发，这项工作探索了利用用户的搜索日志来反映人类潜在的文化产品兴趣的可能性。为了进一步研究2019冠状病毒疾病流动政策可能如何影响文化兴趣的变化，我们提出了一种新的回归不连续性设计，它具有额外的潜力来预测人们的文化产品兴趣的恢复阶段。通过分析6个国家1592年的搜索兴趣时间序列，我们发现在2019冠状病毒疾病大流行期间，人们对电影、音乐和艺术的兴趣有不同的变化模式，但总体上有明显的增长。在我们研究的六个国家中，我们发现对文化产品兴趣的变化与流动性密切相关，随着流动性的下降，对电影、音乐和艺术的兴趣分别平均增加了35、27和20，这些变化至少持续了八周。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Impact+of+COVID-19+Pandemic+on+Cultural+Products+Interests)|0|
|[Enhancing Data Space Semantic Interoperability through Machine Learning: a Visionary Perspective](https://doi.org/10.1145/3543873.3587658)|Zeyd Boukhers, Christoph Lange, Oya Beyan|Faculty of Medicine and University Hospital Cologne, University of Cologne, Germany and Fraunhofer Institute for Applied Information Technology, Germany; Fraunhofer Institute for Applied Information Technology, Germany and RWTH Aachen University, Germany; Fraunhofer Institute for Applied Information Technology, Germany and Faculty of Medicine and University Hospital Cologne, University of Cologne, Germany|Our vision paper outlines a plan to improve the future of semantic interoperability in data spaces through the application of machine learning. The use of data spaces, where data is exchanged among members in a self-regulated environment, is becoming increasingly popular. However, the current manual practices of managing metadata and vocabularies in these spaces are time-consuming, prone to errors, and may not meet the needs of all stakeholders. By leveraging the power of machine learning, we believe that semantic interoperability in data spaces can be significantly improved. This involves automatically generating and updating metadata, which results in a more flexible vocabulary that can accommodate the diverse terminologies used by different sub-communities. Our vision for the future of data spaces addresses the limitations of conventional data exchange and makes data more accessible and valuable for all members of the community.|我们的愿景文件概述了一个通过机器学习应用改善数据空间语义互操作性未来的计划。数据空间的使用正变得越来越流行，数据空间是在一个自我管理的环境中在成员之间交换数据的地方。然而，在这些空间中管理元数据和词汇表的当前手工实践非常耗时，容易出错，并且可能不能满足所有涉众的需求。通过利用机器学习的力量，我们相信数据空间的语义互操作性可以得到显著改善。这涉及到自动生成和更新元数据，从而产生更灵活的词汇表，可以适应不同子社区使用的不同术语。我们对数据空间未来的展望解决了传统数据交换的局限性，并使数据对社区的所有成员更容易获得和更有价值。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Data+Space+Semantic+Interoperability+through+Machine+Learning:+a+Visionary+Perspective)|0|
|[The PLASMA Framework: Laying the Path to Domain-Specific Semantics in Dataspaces](https://doi.org/10.1145/3543873.3587662)|Alexander Paulus, André Pomp, Tobias Meisen|Institute for Technologies and Management of Digital Transformation, University of Wuppertal, Germany|Modern data management is evolving from centralized integration-based solutions to a non-integration-based process of finding, accessing and processing data, as observed within dataspaces. Common reference dataspace architectures assume that sources publish their own domain-specific schema. These schemas, also known as semantic models, can only be partially created automatically and require oversight and refinement by human modellers. Non-expert users, such as mechanical engineers or municipal workers, often have difficulty building models because they are faced with multiple ontologies, classes, and relations, and existing tools are not designed for non-expert users. The PLASMA framework consists of a platform and auxiliary services that focus on providing non-expert users with an accessible way to create and edit semantic models, combining automation approaches and support systems such as a recommendation engine. It also provides data conversion from raw data to RDF. In this paper we highlight the main features, like the modeling interface and the data conversion engine. We discuss how PLASMA as a tool is suitable for building semantic models by non-expert users in the context of dataspaces and show some applications where PLASMA has already been used in data management projects.|现代数据管理正在从基于集中式集成的解决方案演变为基于非集成的查找、访问和处理数据的过程，正如在数据空间中观察到的那样。通用参考数据空间体系结构假设源发布自己的特定于域的模式。这些模式也称为语义模型，只能部分自动创建，需要人类建模者进行监督和细化。非专家用户，如机械工程师或市政工人，往往难以建立模型，因为他们面临着多个本体、类和关系，现有的工具不是为非专家用户设计的。PLASMA 框架由一个平台和辅助服务组成，侧重于为非专家用户提供创建和编辑语义模型的便捷方式，将自动化方法和推荐引擎等支持系统结合起来。它还提供从原始数据到 RDF 的数据转换。本文着重介绍了建模接口和数据转换引擎等主要特性。我们讨论了 PLASMA 作为一种工具是如何适用于非专家用户在数据空间上下文中建立语义模型的，并展示了 PLASMA 已经在数据管理项目中使用的一些应用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+PLASMA+Framework:+Laying+the+Path+to+Domain-Specific+Semantics+in+Dataspaces)|0|
|[Pairwise-interactions-based Bayesian Inference of Network Structure from Information Cascades](https://doi.org/10.1145/3543507.3583231)|Chao Gao, Yuchen Wang, Zhen Wang, Xianghua Li, Xuelong Li|Northwestern Polytechnical University, China; School of Artificial Intelligence, Optics and Electronics (iOPEN), Northwestern Polytechnical University, China|An explicit network structure plays an important role when analyzing and understanding diffusion processes. In many scenarios, however, the interactions between nodes in an underlying network are unavailable. Although many methods for inferring a network structure from observed cascades have been proposed, they did not perceive the relationship between pairwise interactions in a cascade. Therefore, this paper proposes a Pairwise-interactions-based Bayesian Inference method (named PBI) to infer the underlying diffusion network structure. More specifically, to get more accurate inference results, we measure the weights of each candidate pairwise interaction in different cascades and add them to the likelihood of a contagion process. In addition, a pre-pruning work is introduced for candidate edges to further improve the inference efficiency. Experiments on synthetic and real-world networks show that PBI achieves significantly better results.|显式的网络结构在分析和理解扩散过程中起着重要作用。然而，在许多场景中，底层网络中的节点之间的交互是不可用的。虽然已经提出了许多从观察到的级联推断网络结构的方法，但它们没有认识到级联中成对相互作用之间的关系。因此，本文提出了一种基于成对互动的贝叶斯推断方法(称为 PBI)来推断基础扩散网络的结构。更具体地说，为了得到更准确的推断结果，我们测量了不同级联中每个候选者成对相互作用的权重，并将它们加到传染过程的可能性上。此外，为了进一步提高推理效率，引入了候选边缘的预剪枝方法。在合成网络和真实网络上的实验表明，PBI 取得了明显的改善效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pairwise-interactions-based+Bayesian+Inference+of+Network+Structure+from+Information+Cascades)|0|
|[Graph Neural Network with Two Uplift Estimators for Label-Scarcity Individual Uplift Modeling](https://doi.org/10.1145/3543507.3583368)|Dingyuan Zhu, Daixin Wang, Zhiqiang Zhang, Kun Kuang, Yan Zhang, Yulin Kang, Jun Zhou|Ant Group, China; Zhejiang university, China|Uplift modeling aims to measure the incremental effect, which we call uplift, of a strategy or action on the users from randomized experiments or observational data. Most existing uplift methods only use individual data, which are usually not informative enough to capture the unobserved and complex hidden factors regarding the uplift. Furthermore, uplift modeling scenario usually has scarce labeled data, especially for the treatment group, which also poses a great challenge for model training. Considering that the neighbors’ features and the social relationships are very informative to characterize a user’s uplift, we propose a graph neural network-based framework with two uplift estimators, called GNUM, to learn from the social graph for uplift estimation. Specifically, we design the first estimator based on a class-transformed target. The estimator is general for all types of outcomes, and is able to comprehensively model the treatment and control group data together to approach the uplift. When the outcome is discrete, we further design the other uplift estimator based on our defined partial labels, which is able to utilize more labeled data from both the treatment and control groups, to further alleviate the label scarcity problem. Comprehensive experiments on a public dataset and two industrial datasets show a superior performance of our proposed framework over state-of-the-art methods under various evaluation metrics. The proposed algorithms have been deployed online to serve real-world uplift estimation scenarios.|提升模型的目的是通过随机实验或观察数据来衡量策略或行动对用户的增量效应，我们称之为提升。大多数现有的抬升方法只使用单独的数据，这些数据通常不足以获取关于抬升的未观测到的复杂的隐藏因素。此外，抬升模型场景通常缺乏标记数据，特别是对于治疗组，这也对模型训练提出了很大的挑战。考虑到邻居的特征和社会关系对于表征用户的提升是非常有用的，我们提出了一种基于图神经网络的提升估计框架，称为 GNUM，以学习社会图的提升估计。具体地说，我们设计了基于类转换目标的第一个估计器。该估计值对于所有类型的结果都是通用的，并且能够将治疗组和对照组的数据综合建模以接近隆起。当结果是离散的，我们进一步设计其他提升估计的基础上我们定义的部分标签，它能够利用更多的标记数据从治疗组和对照组，以进一步减轻标签稀缺问题。对一个公共数据集和两个工业数据集的综合实验表明，在各种评估指标下，我们提出的框架比最先进的方法具有更好的性能。提出的算法已经在线部署，以服务于真实世界的抬升估计场景。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Neural+Network+with+Two+Uplift+Estimators+for+Label-Scarcity+Individual+Uplift+Modeling)|0|
|[Multi-head Variational Graph Autoencoder Constrained by Sum-product Networks](https://doi.org/10.1145/3543507.3583517)|Riting Xia, Yan Zhang, Chunxu Zhang, Xueyan Liu, Bo Yang|Jilin University, China|Variational graph autoencoder (VGAE) is a promising deep probabilistic model in graph representation learning. However, most existing VGAEs adopt the mean-field assumption, and cannot characterize the graphs with noise well. In this paper, we propose a novel deep probabilistic model for graph analysis, termed Multi-head Variational Graph Autoencoder Constrained by Sum-product Networks (named SPN-MVGAE), which helps to relax the mean-field assumption and learns better latent representation with fault tolerance. Our proposed model SPN-MVGAE uses conditional sum-product networks as constraints to learn the dependencies between latent factors in an end-to-end manner. Furthermore, we introduce the superposition of the latent representations learned by multiple variational networks to represent the final latent representations of nodes. Our model is the first use sum-product networks for graph representation learning, extending the scope of sum-product networks applications. Experimental results show that compared with other baseline methods, our model has competitive advantages in link prediction, fault tolerance, node classification, and graph visualization on real datasets.|变分图自动编码器(VGAE)是图表示学习中一种很有前途的深度概率模型。然而，现有的 VGAE 大多采用平均场假设，不能很好地刻画有噪声的图。本文提出了一种新的图分析的深度概率模型，称为和积网络约束下的多头变分图自动编码器(SPN-MVGAE)。我们提出的模型 SPN-MVGAE 使用条件和积网络作为约束，以端到端的方式学习潜在因素之间的相关性。此外，我们还引入了多变分网络学习的潜在表征的叠加来表示节点的最终潜在表征。该模型首次将和积网络用于图表示学习，扩展了和积网络的应用范围。实验结果表明，与其他基线方法相比，该模型在实际数据集的链接预测、容错、节点分类和图形可视化等方面具有优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-head+Variational+Graph+Autoencoder+Constrained+by+Sum-product+Networks)|0|
|[Interactive Log Parsing via Light-weight User Feedback](https://doi.org/10.1145/3543507.3583456)|Liming Wang, Hong Xie, Ye Li, Jian Tan, John C. S. Lui|Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; College of Computer Science, Chongqing University, China; Alibaba, China; Alibaba, Hong Kong|Template mining is one of the foundational tasks to support log analysis, which supports the diagnosis and troubleshooting of large scale Web applications. This paper develops a human-in-the-loop template mining framework to support interactive log analysis, which is highly desirable in real-world diagnosis or troubleshooting of Web applications but yet previous template mining algorithms fails to support it. We formulate three types of light-weight user feedbacks and based on them we design three atomic human-in-the-loop template mining algorithms. We derive mild conditions under which the outputs of our proposed algorithms are provably correct. We also derive upper bounds on the computational complexity and query complexity of each algorithm. We demonstrate the versatility of our proposed algorithms by combining them to improve the template mining accuracy of five representative algorithms over sixteen widely used benchmark datasets.|模板挖掘是支持日志分析的基本任务之一，它支持大规模 Web 应用程序的诊断和故障排除。本文提出了一种支持交互式日志分析的半人工模板挖掘框架，该框架在 Web 应用程序的实际诊断和故障排除中非常有用，但以往的模板挖掘算法都不支持。我们提出了三种轻量级用户反馈，并在此基础上设计了三种原子人在环模板挖掘算法。我们推导出我们提出的算法输出可证明正确的温和条件。我们还推导了每种算法的计算复杂度和查询复杂度的上界。我们证明了我们提出的算法的通用性，通过结合他们来提高模板挖掘准确性的五个代表性算法超过16个广泛使用的基准数据集。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interactive+Log+Parsing+via+Light-weight+User+Feedback)|0|
|[Misbehavior and Account Suspension in an Online Financial Communication Platform](https://doi.org/10.1145/3543507.3583385)|Taro Tsuchiya, Alejandro Cuevas, Thomas Magelinski, Nicolas Christin|Carnegie Mellon University, USA|The expanding accessibility and appeal of investing have attracted millions of new retail investors. As such, investment discussion boards became the de facto communities where traders create, disseminate, and discuss investing ideas. These communities, which can provide useful information to support investors, have anecdotally also attracted a wide range of misbehavior – toxicity, spam/fraud, and reputation manipulation. This paper is the first comprehensive analysis of online misbehavior in the context of investment communities. We study TradingView, the largest online communication platform for financial trading. We collect 2.76M user profiles with their corresponding social graphs, 4.2M historical article posts, and 5.3M comments, including information on nearly 4 000 suspended accounts and 17 000 removed comments. Price fluctuations seem to drive abuse across the platform and certain types of assets, such as “meme” stocks, attract disproportionate misbehavior. Suspended user accounts tend to form more closely-knit communities than those formed by non-suspended accounts; and paying accounts are less likely to be suspended than free accounts even when posting similar levels of content violating platform policies. We conclude by offering guidelines on how to adapt content moderation efforts to fit the particularities of online investment communities.|投资的可及性和吸引力不断扩大，吸引了数以百万计的新散户投资者。因此，投资讨论委员会事实上成为了交易员创建、传播和讨论投资理念的社区。这些社区可以提供有用的信息来支持投资者，据说也吸引了大量的不良行为——毒性、垃圾邮件/欺诈和声誉操纵。本文首次全面分析了投资社区背景下的网络不良行为。我们研究了 TradingView，这是最大的金融交易在线交流平台。我们收集了276万用户的个人资料及其相应的社交图表，420万篇历史文章和530万条评论，包括近4000个被暂停的账户和17000条被删除的评论。价格波动似乎推动了整个平台的滥用，某些类型的资产，如“模因”股票，吸引了不成比例的不当行为。与非暂停用户账户相比，暂停用户账户往往形成更为紧密的社区; 即使发布了违反平台政策的类似水平的内容，付费账户被暂停的可能性也低于免费账户。最后，我们提供了关于如何调整内容审核工作以适应在线投资社区的特殊性的指导方针。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Misbehavior+and+Account+Suspension+in+an+Online+Financial+Communication+Platform)|0|
|[BiSR: Bidirectionally Optimized Super-Resolution for Mobile Video Streaming](https://doi.org/10.1145/3543507.3583519)|Qian Yu, Qing Li, Rui He, Gareth Tyson, Wanxin Shi, Jianhui Lv, Zhenhui Yuan, Peng Zhang, Yulong Lan, Zhicheng Li|Tencent, China; SUSTech, China and Peng Cheng Laboratory, China; International Graduate School, Tsinghua University, China; Northumbria University, United Kingdom; Hong Kong University of Science and Technology(GZ), China; Peng Cheng Laboratory, China|The user experience of mobile web video streaming is often impacted by insufficient and dynamic network bandwidth. In this paper, we design Bidirectionally Optimized Super-Resolution (BiSR) to improve the quality of experience (QoE) for mobile web users under limited bandwidth. BiSR exploits a deep neural network (DNN)-based model to super-resolve key frames efficiently without changing the inter-frame spatial-temporal information. We then propose a downscaling DNN and a mobile-specific optimized lightweight super-resolution DNN to enhance the performance. Finally, a novel reinforcement learning-based adaptive bitrate (ABR) algorithm is proposed to verify the performance of BiSR on real network traces. Our evaluation, using a full system implementation, shows that BiSR saves 26% of bitrate compared to the traditional H.264 codec and improves the SSIM of video by 3.7% compared to the prior state-of-the-art. Overall, BiSR enhances the user-perceived quality of experience by up to 30.6%.|移动网络视频流的用户体验往往受到网络带宽不足和动态性的影响。本文设计了双向优化超分辨率(BiSR)算法，以提高有限带宽下移动网络用户的体验质量(QoE)。BiSR 利用基于深度神经网络(DNN)的模型，在不改变帧间时空信息的情况下，有效地对关键帧进行超分辨。然后，我们提出了一个缩放 DNN 和一个移动专用的优化轻量级超分辨率 DNN，以提高性能。最后，提出了一种新的基于强化学习的自适应比特率(ABR)算法来验证 BiSR 在实际网络跟踪中的性能。我们的评估，使用一个完整的系统实现，表明 BiSR 节省26% 的比特率相比，传统的 H.264编解码器和提高了3.7% 的 SSIM 的视频相比，以前的最先进的国家。总的来说，BiSR 提高了30.6% 的用户感知体验质量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BiSR:+Bidirectionally+Optimized+Super-Resolution+for+Mobile+Video+Streaming)|0|
|[Autobidding Auctions in the Presence of User Costs](https://doi.org/10.1145/3543507.3583234)|Yuan Deng, Jieming Mao, Vahab Mirrokni, Hanrui Zhang, Song Zuo|Google Research, USA; Carnegie Mellon University, USA|We study autobidding ad auctions with user costs, where each bidder is value-maximizing subject to a return-over-investment (ROI) constraint, and the seller aims to maximize the social welfare taking into consideration the user's cost of viewing an ad. We show that in the worst case, the approximation ratio of social welfare by running the vanilla VCG auctions with user costs could as bad as 0. To improve the performance of VCG, We propose a new variant of VCG based on properly chosen cost multipliers, and prove that there exist auction-dependent and bidder-dependent cost multipliers that guarantee approximation ratios of 1/2 and 1/4 respectively in terms of the social welfare.|本文研究了具有用户成本的自动竞价广告拍卖，其中每个竞价者的价值最大化受到投资回报率(ROI)的约束，卖方的目标是最大化社会福利，同时考虑用户观看广告的成本。我们指出，在最坏的情况下，通过运行普通的 VCG 拍卖与用户成本的社会福利的近似比率可能为0。为了提高 VCG 的性能，我们提出了一种新的基于合理选择成本乘数的 VCG 变体，并证明了存在拍卖相关成本乘数和投标人相关成本乘数，它们分别保证在社会福利方面的近似比为1/2和1/4。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Autobidding+Auctions+in+the+Presence+of+User+Costs)|0|
|[Online Bidding Algorithms for Return-on-Spend Constrained Advertisers✱](https://doi.org/10.1145/3543507.3583491)|Zhe Feng, Swati Padmanabhan, Di Wang|Google Research, USA; University of Washington, Seattle, USA|Online advertising has recently grown into a highly competitive and complex multi-billion-dollar industry, with advertisers bidding for ad slots at large scales and high frequencies. This has resulted in a growing need for efficient "auto-bidding" algorithms that determine the bids for incoming queries to maximize advertisers' targets subject to their specified constraints. This work explores efficient online algorithms for a single value-maximizing advertiser under an increasingly popular constraint: Return-on-Spend (RoS). We quantify efficiency in terms of regret relative to the optimal algorithm, which knows all queries a priori. We contribute a simple online algorithm that achieves near-optimal regret in expectation while always respecting the specified RoS constraint when the input sequence of queries are i.i.d. samples from some distribution. We also integrate our results with the previous work of Balseiro, Lu, and Mirrokni [BLM20] to achieve near-optimal regret while respecting both RoS and fixed budget constraints. Our algorithm follows the primal-dual framework and uses online mirror descent (OMD) for the dual updates. However, we need to use a non-canonical setup of OMD, and therefore the classic low-regret guarantee of OMD, which is for the adversarial setting in online learning, no longer holds. Nonetheless, in our case and more generally where low-regret dynamics are applied in algorithm design, the gradients encountered by OMD can be far from adversarial but influenced by our algorithmic choices. We exploit this key insight to show our OMD setup achieves low regret in the realm of our algorithm.|最近，在线广告业已发展成为一个竞争激烈、规模高达数十亿美元的复杂行业，广告客户大规模、高频率地竞标广告位置。这就导致了对有效的“自动竞价”算法的需求日益增长，这种算法可以确定收到的查询的出价，从而最大限度地提高广告商的目标，使其受到特定的约束。这项工作探讨了一个单一的价值最大化的广告客户在一个日益流行的约束下有效的在线算法: 支出回报(RoS)。我们量化效率的遗憾相对于优化算法，它知道所有查询的先验。我们提出了一个简单的在线算法，当查询的输入序列是来自某个分布的标识样本时，该算法在期望中达到接近最优的遗憾，同时始终遵守指定的 RoS 约束。我们还将我们的研究结果与 Balseiro、 Lu 和 Mirrokni [ BLM20]的前期工作结合起来，以在尊重 RoS 和固定预算约束的情况下实现近乎最佳的遗憾。我们的算法遵循原始-对偶框架，并使用在线镜像下降(OMD)的双重更新。然而，我们需要使用一个非规范的 OMD 设置，因此 OMD 的经典的低后悔保证，这是在线学习的对抗设置，不再成立。尽管如此，在我们的案例中，以及更一般的低后悔动力学应用于算法设计的情况下，OMD 遇到的梯度可能远不是对手，而是受到我们的算法选择的影响。我们利用这个关键的洞察力来展示我们的 OMD 设置在我们的算法领域实现了低遗憾。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Bidding+Algorithms+for+Return-on-Spend+Constrained+Advertisers✱)|0|
|[EDNet: Attention-Based Multimodal Representation for Classification of Twitter Users Related to Eating Disorders](https://doi.org/10.1145/3543507.3583863)|Mohammad Abuhassan, Tarique Anwar, Chengfei Liu, Hannah K. Jarman, Matthew FullerTyszkiewicz|Deakin University, Australia; Swinburne University of Technology, Australia; University of York, United Kingdom|Social media platforms provide rich data sources in several domains. In mental health, individuals experiencing an Eating Disorder (ED) are often hesitant to seek help through conventional healthcare services. However, many people seek help with diet and body image issues on social media. To better distinguish at-risk users who may need help for an ED from those who are simply commenting on ED in social environments, highly sophisticated approaches are required. Assessment of ED risks in such a situation can be done in various ways, and each has its own strengths and weaknesses. Hence, there is a need for and potential benefit of a more complex multimodal approach. To this end, we collect historical tweets, user biographies, and online behaviours of relevant users from Twitter, and generate a reasonably large labelled benchmark dataset. Thereafter, we develop an advanced multimodal deep learning model called EDNet using these data to identify the different types of users with ED engagement (e.g., potential ED sufferers, healthcare professionals, or communicators) and distinguish them from those not experiencing EDs on Twitter. EDNet consists of five deep neural network layers. With the help of its embedding, representation and behaviour modeling layers, it effectively learns the multimodalities of social media. In our experiments, EDNet consistently outperforms all the baseline techniques by significant margins. It achieves an accuracy of up to 94.32% and F1 score of up to 93.91% F1 score. To the best of our knowledge, this is the first such study to propose a multimodal approach for user-level classification according to their engagement with ED content on social media.|社交媒体平台在多个领域提供丰富的数据源。在心理健康方面，患有饮食失调(ED)的个体往往不愿意通过传统的医疗服务寻求帮助。然而，许多人在社交媒体上寻求关于饮食和身体形象问题的帮助。为了更好地区分那些可能需要急诊帮助的高危患者和那些只是在社会环境中评论急诊的患者，需要高度复杂的方法。在这种情况下，评估教育署的风险有多种方法，每种方法各有优缺点。因此，需要一种更复杂的多模式方法，而且这种方法还有潜在的好处。为此，我们从 Twitter 收集历史推文、用户简历和相关用户的在线行为，并生成一个相当大的带标签的基准数据集。此后，我们开发了一种称为 EDNet 的高级多模式深度学习模型，使用这些数据来确定不同类型的 ED 参与用户(例如，潜在的 ED 患者，医疗保健专业人员或沟通者) ，并将他们与 Twitter 上没有经历 ED 的人区分开来。EDNet 由五个深层神经网络层组成。借助其嵌入层、表征层和行为建模层，它有效地学习了社会媒体的多重形态。在我们的实验中，EDNet 始终以显著的优势优于所有的基线技术。该算法的正确率达到94.32% ，F1得分达到93.91% 。据我们所知，这是第一个这样的研究，提出了一个多模式的方法，用户级别的分类，根据他们的参与，教育署的内容在社交媒体上。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EDNet:+Attention-Based+Multimodal+Representation+for+Classification+of+Twitter+Users+Related+to+Eating+Disorders)|0|
|[C-Affinity: A Novel Similarity Measure for Effective Data Clustering](https://doi.org/10.1145/3543873.3587307)|Jiwon Hong, SangWook Kim|Hanyang University, Republic of Korea|Clustering is widely employed in various applications as it is one of the most useful data mining techniques. In performing clustering, a similarity measure, which defines how similar a pair of data objects are, plays an important role. A similarity measure is employed by considering a target dataset’s characteristics. Current similarity measures (or distances) do not reflect the distribution of data objects in a dataset at all. From the clustering point of view, this fact may limit the clustering accuracy. In this paper, we propose c-affinity, a new notion of a similarity measure that reflects the distribution of objects in the given dataset from a clustering point of view. We design c-affinity between any two objects to have a higher value as they are more likely to belong to the same cluster by learning the data distribution. We use random walk with restart (RWR) on the k-nearest neighbor graph of the given dataset to measure (1) how similar a pair of objects are and (2) how densely other objects are distributed between them. Via extensive experiments on sixteen synthetic and real-world datasets, we verify that replacing the existing similarity measure with our c-affinity improves the clustering accuracy significantly.|聚类作为最有用的数据挖掘技术之一，被广泛应用于各种应用程序中。在进行聚类时，一个相似性度量定义了一对数据对象的相似程度，它扮演着重要的角色。通过考虑目标数据集的特征，采用相似性度量。当前的相似性度量(或距离)根本不反映数据集中数据对象的分布。从聚类的角度来看，这一事实可能会限制聚类的准确性。在这篇文章中，我们提出了一个新的概念，即从聚类的角度来反映给定数据集中对象的分布的相似性度量。通过学习数据分布，我们设计任意两个对象之间的 c 亲和关系，使其具有更高的值，因为它们更可能属于同一个集群。我们在给定数据集的 k 最近邻图上使用重启随机游动(RWR)来测量(1)一对对象有多相似，(2)其他对象在它们之间的分布有多密集。通过在16个合成和真实数据集上的大量实验，我们验证了用 c 亲和度取代现有的相似度度量可以显著提高聚类的准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=C-Affinity:+A+Novel+Similarity+Measure+for+Effective+Data+Clustering)|0|
|[Knowledge Distillation on Cross-Modal Adversarial Reprogramming for Data-Limited Attribute Inference](https://doi.org/10.1145/3543873.3587313)|Quan Li, Lingwei Chen, Shixiong Jing, Dinghao Wu|Pennsylvania State University, USA; Wright State University, USA|Social media generates a rich source of text data with intrinsic user attributes (e.g., age, gender), where different parties benefit from disclosing them. Attribute inference can be cast as a text classification problem, which, however, suffers from labeled data scarcity. To address this challenge, we propose a data-limited learning model to distill knowledge on adversarial reprogramming of a visual transformer (ViT) for attribute inferences. Not only does this novel cross-modal model transfers the powerful learning capability from ViT, but also leverages unlabeled texts to reduce the demand on labeled data. Experiments on social media datasets demonstrate the state-of-the-art performance of our model on data-limited attribute inferences.|社交媒体产生了丰富的具有内在用户属性(例如，年龄，性别)的文本数据来源，不同的方面从披露这些数据中获益。属性推理可以被看作是一个文本分类问题，但是，这个问题存在标记数据稀缺性。为了解决这个问题，我们提出了一个数据有限的学习模型，以提取知识的对抗性重编程的可视化转换器(ViT)的属性推理。这种新颖的跨模式模型不仅转移了 ViT 强大的学习能力，而且利用未标记的文本来减少对标记数据的需求。在社会媒体数据集上的实验证明了我们的模型在数据有限属性推理上的最新性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Distillation+on+Cross-Modal+Adversarial+Reprogramming+for+Data-Limited+Attribute+Inference)|0|
|[Copyright Protection and Accountability of Generative AI: Attack, Watermarking and Attribution](https://doi.org/10.1145/3543873.3587321)|Haonan Zhong, Jiamin Chang, Ziyue Yang, Tingmin Wu, Pathum Chamikara Mahawaga Arachchige, Chehara Pathmabandu, Minhui Xue||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Copyright+Protection+and+Accountability+of+Generative+AI:+Attack,+Watermarking+and+Attribution)|0|
|[How Streaming Can Improve the World (Wide Web)](https://doi.org/10.1145/3543873.3587332)|Lucas Vogel, Thomas Springer|TU Dresden, Germany|Since its beginnings, web pages have been based on files. This means that HTML, CSS, and JavaScript are transferred from server to client as files, which by default need to be fully loaded before the web page is displayed. This render-blocking procedure increases loading times significantly, leading to reduced user satisfaction and revenue loss due to lower conversion rates. We present a full implementation of a new approach for loading web pages by splitting up every component and loading the page via a text-based stream. Such a modification aligns with current trends of the HTTP protocol, which has been using streams internally since HTTP/2. It significantly improves loading times, independent of the total page size.|从一开始，网页就是基于文件的。这意味着 HTML、 CSS 和 JavaScript 作为文件从服务器传输到客户端，默认情况下需要在网页显示之前完全加载这些文件。这种渲染阻塞过程大大增加了加载时间，导致用户满意度降低和收入损失，由于较低的转换率。我们提出了一个全新的加载网页的方法，通过拆分每个组件，并通过一个基于文本的流加载网页的完整实现。这种修改符合 HTTP 协议的当前趋势，自 HTTP/2以来，HTTP 协议一直在内部使用流。它显著提高了加载时间，与总页面大小无关。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Streaming+Can+Improve+the+World+(Wide+Web))|0|
|[PyPoll: A python library automating mining of networks, discussions and polarization on Twitter](https://doi.org/10.1145/3543873.3587349)|Dimitrios Panteleimon Giakatos, Pavlos Sermpezis, Athena Vakali|Aristotle University of Thessaloniki, Greece|Today online social networks have a high impact in our society as more and more people use them for communicating with each other, express their opinions, participating in public discussions, etc. In particular, Twitter is one of the most popular social network platforms people mainly use for political discussions. This attracted the interest of many research studies that analyzed social phenomena on Twitter, by collecting data, analysing communication patterns, and exploring the structure of user networks. While previous works share many common methodologies for data collection and analysis, these are mainly re-implemented every time by researchers in a custom way. In this paper, we introduce PyPoll an open-source Python library that operationalizes common analysis tasks for Twitter discussions. With PyPoll users can perform Twitter graph mining, calculate the polarization index and generate interactive visualizations without needing third-party tools. We believe that PyPoll can help researchers automate their tasks by giving them methods that are easy to use. Also, we demonstrate the use of the library by presenting two use cases; the PyPoll visualization app, an online application for graph visualizing and sharing, and the Political Lighthouse, a Web portal for displaying the polarization in various political topics on Twitter.|今天，在线社交网络对我们的社会产生了很大的影响，因为越来越多的人使用它们进行交流、表达意见、参与公共讨论等等。特别值得一提的是，Twitter 是人们主要用于政治讨论的最流行的社交网络平台之一。这引起了许多研究的兴趣，这些研究通过收集数据、分析交流模式和探索用户网络的结构来分析 Twitter 上的社会现象。虽然以前的工作共享许多共同的方法收集和分析数据，这些主要是重新实现每次由研究人员在一个自定义的方式。在本文中，我们介绍了 PyPoll，它是一个开放源码的 Python 库，可以为 Twitter 讨论操作常见的分析任务。使用 PyPoll，用户可以执行 Twitter 图形挖掘、计算极化指数和生成交互式可视化，而不需要第三方工具。我们相信 PyPoll 可以帮助研究人员通过提供易于使用的方法来自动化他们的任务。此外，我们通过展示两个用例来演示该库的使用: PyPoll 可视化应用程序，一个用于图形可视化和共享的在线应用程序，以及 Political Lighthouse，一个用于在 Twitter 上显示各种政治主题的两极分化的门户网站。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PyPoll:+A+python+library+automating+mining+of+networks,+discussions+and+polarization+on+Twitter)|0|
|[WebSHAP: Towards Explaining Any Machine Learning Models Anywhere](https://doi.org/10.1145/3543873.3587362)|Zijie J. Wang, Duen Horng Chau|Georgia Institute of Technology, USA|As machine learning (ML) is increasingly integrated into our everyday Web experience, there is a call for transparent and explainable web-based ML. However, existing explainability techniques often require dedicated backend servers, which limit their usefulness as the Web community moves toward in-browser ML for lower latency and greater privacy. To address the pressing need for a client-side explainability solution, we present WebSHAP, the first in-browser tool that adapts the state-of-the-art model-agnostic explainability technique SHAP to the Web environment. Our open-source tool is developed with modern Web technologies such as WebGL that leverage client-side hardware capabilities and make it easy to integrate into existing Web ML applications. We demonstrate WebSHAP in a usage scenario of explaining ML-based loan approval decisions to loan applicants. Reflecting on our work, we discuss the opportunities and challenges for future research on transparent Web ML. WebSHAP is available at https://github.com/poloclub/webshap.|随着机器学习(ML)越来越多地融入我们的日常网络经验，有一个透明的和可解释的基于网络的 ML 的呼吁。然而，现有的可解释性技术通常需要专用的后端服务器，这限制了它们的有用性，因为 Web 社区正在朝着浏览器内机器学习的方向发展，以获得更低的延迟和更大的隐私。为了满足对客户端可解释性解决方案的迫切需求，我们提出了 WebSHAP，这是第一个在浏览器中使最先进的模型无关可解释性技术 SHAP 适用于 Web 环境的工具。我们的开源工具是使用现代 Web 技术(如 WebGL)开发的，这些技术利用了客户端硬件功能，并使其易于集成到现有的 Web ML 应用程序中。我们在向贷款申请者解释基于 ML 的贷款批准决策的使用场景中演示了 WebSHAP。回顾我们的工作，我们讨论了未来研究透明 Web 机器学习的机遇和挑战。「网上民政事务及安全资讯 https://github.com/poloclub/WebSHAP 」已上载至。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=WebSHAP:+Towards+Explaining+Any+Machine+Learning+Models+Anywhere)|0|
|[Privacy-Preserving Online Content Moderation: A Federated Learning Use Case](https://doi.org/10.1145/3543873.3587604)|Pantelitsa Leonidou, Nicolas Kourtellis, Nikos Salamanos, Michael Sirivianos|Telefonica Research, Spain; Cyprus University of Technology, Cyprus|Users are daily exposed to a large volume of harmful content on various social network platforms. One solution is developing online moderation tools using Machine Learning techniques. However, the processing of user data by online platforms requires compliance with privacy policies. Federated Learning (FL) is an ML paradigm where the training is performed locally on the users' devices. Although the FL framework complies, in theory, with the GDPR policies, privacy leaks can still occur. For instance, an attacker accessing the final trained model can successfully perform unwanted inference of the data belonging to the users who participated in the training process. In this paper, we propose a privacy-preserving FL framework for online content moderation that incorporates Differential Privacy (DP). To demonstrate the feasibility of our approach, we focus on detecting harmful content on Twitter - but the overall concept can be generalized to other types of misbehavior. We simulate a text classifier - in FL fashion - which can detect tweets with harmful content. We show that the performance of the proposed FL framework can be close to the centralized approach - for both the DP and non-DP FL versions. Moreover, it has a high performance even if a small number of clients (each with a small number of data points) are available for the FL training. When reducing the number of clients (from 50 to 10) or the data points per client (from 1K to 0.1K), the classifier can still achieve ~81% AUC. Furthermore, we extend the evaluation to four other Twitter datasets that capture different types of user misbehavior and still obtain a promising performance (61% - 80% AUC). Finally, we explore the overhead on the users' devices during the FL training phase and show that the local training does not introduce excessive CPU utilization and memory consumption overhead.|用户每天都会在各种社交网络平台上接触到大量的有害内容。一个解决方案是使用机器学习技术开发在线审核工具。然而，通过在线平台处理用户数据需要遵守隐私策略。联邦学习(FL)是一种机器学习范式，其中的培训是在用户的设备上本地执行的。尽管 FL 框架在理论上符合 GDPR 策略，但仍然可能发生隐私泄露。例如，访问最终训练模型的攻击者可以成功地对属于参与训练过程的用户的数据进行不必要的推断。在这篇文章中，我们提出了一个保护隐私的在线内容审查框架，该框架结合了差分隐私(DP)。为了证明我们方法的可行性，我们将重点放在检测 Twitter 上的有害内容上——但总体概念可以推广到其他类型的不当行为。我们模拟了一个文本分类器——以 FL 的方式——它可以检测带有有害内容的 tweet。我们展示了所提出的 FL 框架的性能可以接近于集中式方法-对于 DP 和非 DP FL 版本都是如此。此外，即使只有少量的客户端(每个客户端都有少量的数据点)可用于 FL 培训，它也具有很高的性能。当减少客户端数量(从50到10)或每个客户端的数据点数量(从1K 到0.1 K)时，分类器仍然可以达到约81% 的 AUC。此外，我们将评估扩展到其他四个 Twitter 数据集，这些数据集捕获了不同类型的用户不当行为，并且仍然获得了有希望的性能(61% -80% AUC)。最后，我们研究了 FL 训练阶段用户设备上的开销，结果表明本地训练不会引入过多的 CPU 利用率和内存消耗开销。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privacy-Preserving+Online+Content+Moderation:+A+Federated+Learning+Use+Case)|0|
|[Intent-based Web Page Summarization with Structure-Aware Chunking and Generative Language Models](https://doi.org/10.1145/3543873.3587372)|HuanYuan Chen, Hong Yu|College of Information and Computer Sciences, University of Massachusetts Amherst, USA; School of Computer and Information Sciences, University of Massachusetts Lowell, USA|This paper introduces a structure-aware method to segment web pages into chunks based on their web structures. We utilize large language models to select chunks correspond to a given intent and generate the abstractive summary. Experiments on a food pantry dataset developed for mitigating food insecurity show that the proposed framework is promising.|介绍了一种基于结构感知的网页分块方法。我们利用大型语言模型来选择与给定意图相对应的块，并生成抽象的摘要。为减轻粮食不安全而开发的食品储藏室数据集的实验表明，所提出的框架是有希望的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Intent-based+Web+Page+Summarization+with+Structure-Aware+Chunking+and+Generative+Language+Models)|0|
|[Measuring and Detecting Virality on Social Media: The Case of Twitter's Viral Tweets Topic](https://doi.org/10.1145/3543873.3587373)|Tugrulcan Elmas, Stephane Selim, Célia Houssiaux|Indiana University Bloomington, USA; EPFL, Switzerland|Social media posts may go viral and reach large numbers of people within a short period of time. Such posts may threaten the public dialogue if they contain misleading content, making their early detection highly crucial. Previous works proposed their own metrics to annotate if a tweet is viral or not in order to automatically detect them later. However, such metrics may not accurately represent viral tweets or may introduce too many false positives. In this work, we use the ground truth data provided by Twitter's "Viral Tweets" topic to review the current metrics and also propose our own metric. We find that a tweet is more likely to be classified as viral by Twitter if the ratio of retweets to its author's followers exceeds some threshold. We found this threshold to be 2.16 in our experiments. This rule results in less false positives although it favors smaller accounts. We also propose a transformers-based model to early detect viral tweets which reports an F1 score of 0.79. The code and the tweet ids are publicly available at: https://github.com/tugrulz/ViralTweets|社交媒体上的帖子可能会像病毒一样迅速传播，并在短时间内接触到大量人群。如果这些帖子包含误导性内容，就可能威胁到公众对话，因此及早发现非常关键。以前的作品提出了他们自己的标准来注释一条推文是否是病毒性的，以便以后自动检测到它们。然而，这样的指标可能不能准确地代表病毒推文，或者可能会引入太多的假阳性。在这项工作中，我们使用 Twitter 的“ Viral Tweets”主题提供的地面真相数据来回顾当前的指标，并提出我们自己的指标。我们发现，如果一条推文的转发与其作者的关注者的比例超过某个阈值，那么它更有可能被 Twitter 归类为病毒式传播。我们在实验中发现这个阈值是2.16。这个规则导致较少的误报，尽管它有利于较小的帐户。我们还提出了一个基于转换器的模型，以早期检测病毒鸣叫报告的 F1评分为0.79。代码和 tweet id 可以在以下 https://github.com/tugrulz/viraltweets 公开获得:|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Measuring+and+Detecting+Virality+on+Social+Media:+The+Case+of+Twitter's+Viral+Tweets+Topic)|0|
|[Anytime-Valid Confidence Sequences in an Enterprise A/B Testing Platform](https://doi.org/10.1145/3543873.3584635)|Akash Maharaj, Ritwik Sinha, David Arbour, Ian WaudbySmith, Simon Z. Liu, Moumita Sinha, Raghavendra Addanki, Aaditya Ramdas, Manas Garg, Viswanathan Swaminathan|Carnegie Mellon University, USA; Adobe, USA; Adobe Research, USA|A/B tests are the gold standard for evaluating digital experiences on the web. However, traditional "fixed-horizon" statistical methods are often incompatible with the needs of modern industry practitioners as they do not permit continuous monitoring of experiments. Frequent evaluation of fixed-horizon tests ("peeking") leads to inflated type-I error and can result in erroneous conclusions. We have released an experimentation service on the Adobe Experience Platform based on anytime-valid confidence sequences, allowing for continuous monitoring of the A/B test and data-dependent stopping. We demonstrate how we adapted and deployed asymptotic confidence sequences in a full featured A/B testing platform, describe how sample size calculations can be performed, and how alternate test statistics like "lift" can be analyzed. On both simulated data and thousands of real experiments, we show the desirable properties of using anytime-valid methods instead of traditional approaches.|A/B 测试是评估网络数字体验的黄金标准。然而，传统的“固定视野”统计方法往往不符合现代行业从业人员的需要，因为他们不允许连续监测实验。频繁地评估固定水平试验(“窥视”)会导致膨胀的 I 型误差，并可能导致错误的结论。我们已经在 Adobe 体验平台上发布了一个基于随时有效置信序列的实验服务，允许连续监控 A/B 测试和依赖数据的停止。我们展示了我们如何在一个全功能的 A/B 测试平台中调整和部署渐近置信序列，描述了如何执行样本量计算，以及如何分析像“提升”这样的替代测试统计量。在模拟数据和成千上万的实际实验中，我们显示了使用随机有效方法代替传统方法的理想性质。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Anytime-Valid+Confidence+Sequences+in+an+Enterprise+A/B+Testing+Platform)|0|
|[Contrastive Fine-tuning on Few Shot Intent Detection with Topological Intent Tree](https://doi.org/10.1145/3543873.3584648)|Wei Yuan, Martin Dimkovski, Aijun An|Department of Electrical Engineering and Computer Science, York University, Canada; Intact Financial Corporation, Canada|We present a few-shot intent detection model for an enterprise’s conversational dialogue system. The model uses an intent topological tree to guide the search for the user intent using large language models (LLMs). The intents are resolved based on semantic similarities between user utterances and the text descriptions of the internal nodes of the intent tree or the intent examples in the leaf nodes of the tree. Our results show that an off-the-shelf language model can work reasonably well in a large enterprise deployment without fine-tuning, and its performance can be further improved with fine-tuning as more domain-specific data becomes available. We also show that the fine-tuned language model meets and outperforms the state-of-the-art (SOTA) results in resolving conversation intents without training classifiers. With the use of a topological intent tree, our model provides more interpretability to cultivate people’s trust in their decisions.|针对企业会话对话系统，提出了一种少镜头意图检测模型。该模型使用一个意图拓扑树来指导使用大型语言模型(LLM)搜索用户意图。根据用户语句与意图树内部节点的文本描述或树叶节点中的意图示例之间的语义相似性来解析意图。我们的研究结果表明，现成的语言模型可以在不进行微调的情况下在大型企业部署中工作得相当好，而且随着更多特定于领域的数据可用，通过微调可以进一步提高其性能。我们还表明，经过微调的语言模型满足并优于最先进的(SOTA)结果，无需训练分类器就能解析会话意图。通过使用拓扑意图树，我们的模型提供了更多的可解释性来培养人们对其决策的信任。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Fine-tuning+on+Few+Shot+Intent+Detection+with+Topological+Intent+Tree)|0|
|[Visual Item Selection With Voice Assistants: A systems perspective](https://doi.org/10.1145/3543873.3584655)|Prashan Wanigasekara, Rafid AlHumaimidi, Turan Gojayev, Niloofar Gheissari, Achal Dave, Stephen Rawls, Fan Yang, Kechen Qin, Nalin Gupta, Spurthi Sandiri, Chevanthie Dissanayake, Zeynab Raeesy, Emre Barut, Chengwei Su|Amazon, USA; Amazon, Canada; Amazon, Germany|Interacting with voice assistants, such as Amazon Alexa to aid in day-to-day tasks has become a ubiquitous phenomenon in modern-day households. These voice assistants often have screens to provide visual content (e.g., images, videos) to their users. There is an increasing trend of users shopping or searching for products using these devices, yet, these voice assistants do not support commands or queries that contain visual references to the content shown on screen (e.g., “blue one”, “red dress”). We introduce a novel multi-modal visual shopping experience where the voice assistant is aware of the visual content shown on the screen and assists the user in item selection using natural language multi-modal interactions. We detail a practical, lightweight end-to-end system architecture spanning from model fine-tuning, deployment, to skill invocation on an Amazon Echo family device with a screen. We also define a niche “Visual Item Selection” task and evaluate whether we can effectively leverage publicly available multi-modal models, and embeddings produced from these models for the task. We show that open source contrastive embeddings like CLIP [30] and ALBEF [24] have zero-shot accuracy above for the “Visual Item Selection” task on an internally collected visual shopping dataset. By further fine-tuning the embeddings, we obtain further gains of 8.6% to 24.0% in relative accuracy improvement over a baseline. The technology that enables our visual shopping assistant is available as an Alexa Skill in the Alexa Skills store.|与诸如亚马逊 Alexa 这样的语音助手进行交互以帮助完成日常任务，已经成为现代家庭中无处不在的现象。这些语音助手通常有屏幕为用户提供视觉内容(如图像、视频)。用户使用这些设备购物或搜索产品的趋势正在增加，然而，这些语音助手不支持包含对屏幕上显示的内容的可视化参考的命令或查询(例如，“蓝色的”、“红色的裙子”)。我们介绍了一种新颖的多模态视觉购物体验，语音助手可以感知屏幕上显示的视觉内容，并利用自然语言的多模态交互协助用户选择商品。我们详细介绍了一个实用的、轻量级的端到端系统架构，从模型微调、部署到带屏幕的 Amazon Echo 系列设备上的技能调用。我们还定义了一个小型的“可视化项目选择”任务，并评估我们是否能够有效地利用公开可用的多模态模型，以及从这些模型产生的嵌入任务。我们展示了像 CLIP [30]和 ALBEF [24]这样的开源对比嵌入对于内部收集的可视化购物数据集上的“可视化项目选择”任务具有零拍摄精度。通过对嵌入进行进一步的微调，我们获得了比基线相对准确度提高8.6% 到24.0% 的进一步增益。该技术，使我们的视觉购物助理可作为一个 Alexa 技能在 Alexa 技能商店。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Visual+Item+Selection+With+Voice+Assistants:+A+systems+perspective)|0|
|[Multi-Source Domain Adaptation via Latent Domain Reconstruction](https://doi.org/10.1145/3543873.3584659)|Jun Zhou, Chilin Fu, Xiaolu Zhang|Ant Group, China; College of Computer Science and Technology, Zhejiang University, China and Ant Group, China|Multi-Source Domain Adaptation (MSDA) is widely used in various machine learning scenarios for domain shifts between labeled source domains and unlabeled target domains. Conventional MSDA methods are built on a strong hypothesis that data samples from the same source belong to the same domain with the same latent distribution. However, in practice sources and their latent domains are not necessarily one-to-one correspondence. To tackle this problem, a novel Multi-source Reconstructed Domain Adaptation (MRDA) framework for MSDA is proposed. We use an Expectation-Maximization (EM) mechanism that iteratively reconstructs the source domains to recover the latent domains and performs domain adaptation on the reconstructed domains. Specifically, in the E-step, we cluster the samples from multiple sources into different latent domains, and a soft assignment strategy is proposed to avoid cluster imbalance. In the M-step, we freeze the latent domains clustered in the E-step and optimize the objective function for domain adaptation, and a global-specific feature extractor is used to capture both domain-invariant and domain-specific features. Extensive experiments demonstrate that our approach can reconstruct source domains and perform domain adaptation on the reconstructed domains effectively, thus significantly outperforming state-of-the-art (SOTA) baselines (e.g., 1% to 3.1% absolute improvement in AUC).|多源域自适应(MSDA)广泛应用于各种机器学习场景中，用于标记源域和未标记目标域之间的域移位。传统的 MSDA 方法是建立在一个强大的假设，即来自同一来源的数据样本属于同一领域，具有相同的潜在分布。然而，在实践中，来源及其潜在领域并不一定是双射。针对这一问题，提出了一种新的多源重构域自适应(MRDA)框架。我们使用一个期望最大化(EM)机制，迭代地重构源域来恢复潜在域，并对重构域执行域适应。具体来说，在 E 步中，我们将来自多个源的样本聚类到不同的潜在域中，并提出了一种软分配策略来避免聚类不平衡。在 M 步中，我们冻结了聚集在 E 步中的潜在领域，并优化了领域适应的目标函数，并且使用了全局特征提取器来捕获领域不变和领域特定的特征。广泛的实验表明，我们的方法可以重建源域并有效地对重建域进行域适应，从而显着优于最先进的(SOTA)基线(例如，AUC 的1% 至3.1% 的绝对改善)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Source+Domain+Adaptation+via+Latent+Domain+Reconstruction)|0|
|[Human Dimensions of Animal Exploitation: Towards Understanding the International Wildlife Trade and Selfie-Tourism on Twitter](https://doi.org/10.1145/3543873.3587538)|Sean P. Rogers, Jeremiah Onaolapo|University of Vermont, USA|This study investigates statements of participation in an exploitative animal activity on social media website Twitter. The data include social posts (tweets) related to two exploited species - the sloth (N=32,119), and the elephant (N=15,160). Tweets for each of these case studies were examined and labeled. The initial results reveal several features of interaction with exploited species. Namely, there are a high number of tweets indicating that individuals participated in exploited species activities during vacations in destinations that double as native countries for the exploited species. The data also indicate that a large number of exploited species activities take place at fairs, carnivals, and circuses. These initial results shed light on the trends in human participation in activities with exploited species. These findings will offer insight to stakeholders seeking to bolster education programs and quantify the level of animal exploitation.|本研究调查了社交媒体网站 Twitter 上参与剥削动物活动的声明。这些数据包括与两个被开发的物种——树懒(N = 32,119)和大象(N = 15,160)有关的社交帖子(tweet)。这些个案研究的推文都经过了检查和标记。初步结果揭示了与被开发物种相互作用的几个特征。也就是说，有大量推文表明，个人在假期期间参加了被捕捞物种的活动，而目的地是被捕捞物种的本土国家。数据还表明，大量被开发的物种活动发生在集市、嘉年华会和马戏团。这些初步结果说明了人类参与与被捕捞物种有关的活动的趋势。这些发现将为利益相关者提供深刻的见解，以支持教育项目和量化动物剥削的水平。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Human+Dimensions+of+Animal+Exploitation:+Towards+Understanding+the+International+Wildlife+Trade+and+Selfie-Tourism+on+Twitter)|0|
|[A Bridge over the Troll: Non-Complementary Activism Online](https://doi.org/10.1145/3543873.3587541)|Emyn Dean|Knowledge Media Institute (KMi), Open University, United Kingdom|Previous research has identified phenomena such as cyberbystander intervention and various other forms of responses to aggressive or hateful behaviours online. In the online media ecosystem, some people from marginalized communities and their allies have attempted to enhance organic engagement by participating in organized activism, which is sometimes characterized as "non-complementary" or "indirect". This paper attempts to identify, recognize, and label this phenomenon, as well as provide suggestions for further research in this area.|以前的研究已经确定了一些现象，例如网络旁观者的干预和对网上攻击性或仇恨行为的各种其他形式的反应。在网络媒体生态系统中，一些来自边缘化社区及其盟友的人试图通过参与有组织的行动主义来加强有机参与，这种行动主义有时被描述为“非互补”或“间接”。本文试图对这一现象进行识别、识别和标记，并为该领域的进一步研究提供建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Bridge+over+the+Troll:+Non-Complementary+Activism+Online)|0|
|[The DEEP Sensorium: a multidimensional approach to sensory domain labelling](https://doi.org/10.1145/3543873.3587631)|Simona Corciulo, Livio Bioglio, Valerio Basile, Viviana Patti, Rossana Damiano|Dipartimento di Studi Umanistici, University of Turin, Italy; Dipartimento di Informatica, University of Turin, Italy|In this paper, we describe our intuitions about how language technologies can contribute to create new ways to enhance the accessibility of exhibits in cultural contexts by exploiting the knowledge about the history of our senses and the link between perception and language. We evaluate the performance of five multi-class classification models for the task of sensory recognition and introduce the DEEP Sensorium (Deep Engaging Experiences and Practices - Sensorium), a multidimensional dataset that combines cognitive and affective features to inform systematic methodologies for augmenting exhibits with multi-sensory stimuli. For each model, using different feature sets, we show that the features expressing the affective dimension of words combined with sub-lexical features perform better than uni-dimensional training sets.|在本文中，我们描述了我们的直觉，关于语言技术如何能够有助于创造新的方式，通过利用我们的感官历史知识和感知与语言之间的联系，提高展品在文化背景下的可及性。我们评估了五个多类分类模型在感官识别任务中的表现，并介绍了 DEEP Sensorium (DEEP Engaging Experience and Practices-Sensorium) ，这是一个结合认知和情感特征的多维数据集，以通知系统方法用多感官刺激来增强展品。对于每个模型，使用不同的特征集，我们发现结合亚词汇特征表达词的情感维度的特征比一维训练集表现得更好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+DEEP+Sensorium:+a+multidimensional+approach+to+sensory+domain+labelling)|0|
|[A Survey of General Ontologies for the Cross-Industry Domain of Circular Economy](https://doi.org/10.1145/3543873.3587613)|Huanyu Li, Mina Abd Nikooie Pour, Ying Li, Mikael Lindecrantz, Eva Blomqvist, Patrick Lambrix|Ragn-Sells AB, Sweden; Linköping University, Sweden and University of Gävle, Sweden; Linköping University, Sweden|Circular Economy has the goal to reduce value loss and avoid waste by extending the life span of materials and products, including circulating materials or product parts before they become waste. Circular economy models (e.g., circular value networks) are typically complex and networked, involving different cross-industry domains. In the context of a circular value network, multiple actors, such as suppliers, manufacturers, recyclers, and product end-users, may be involved. In addition, there may be various flows of resources, energy, information and value throughout the network. This means that we face the challenge that the data and information from cross-industry domains in a circular economy model are not built on common ground, and as a result are difficult to understand and use for both humans and machines. Using ontologies to represent domain knowledge can enable actors and stakeholders from different industries in the circular economy to communicate using a common language. The knowledge domains involved include circular economy, sustainability, materials, products, manufacturing, and logistics. The objective of this paper is to investigate the landscape of current ontologies for these domains. This will enable us to in the future explore what existing knowledge can be adapted or used to develop ontologies for circular value networks.|循环经济的目标是通过延长材料和产品的寿命来减少价值损失和避免浪费，包括在成为废物之前循环的材料或产品部件。循环经济模型(例如，循环价值网络)通常是复杂和网络化的，涉及不同的跨行业领域。在循环价值网络的背景下，可能涉及多个参与者，如供应商、制造商、回收商和产品最终用户。此外，整个网络可能有各种各样的资源、能源、信息和价值流。这意味着我们面临的挑战是，来自循环经济模型中跨行业领域的数据和信息不是建立在共同的基础之上，因此人类和机器都难以理解和使用。使用本体来表示领域知识可以使循环经济中不同行业的参与者和利益相关者使用一种共同的语言进行交流。所涉及的知识领域包括循环经济、可持续性、材料、产品、制造和物流。本文的目的是研究这些领域当前的本体论景观。这将使我们能够在未来探索什么现有的知识可以被调整或用于开发循环价值网络的本体论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Survey+of+General+Ontologies+for+the+Cross-Industry+Domain+of+Circular+Economy)|0|
|[Improving Netflix Video Quality with Neural Networks](https://doi.org/10.1145/3543873.3587553)|Christos G. Bampis, LiHeng Chen, Zhi Li|Netflix, USA|Video downscaling is an important component of adaptive video streaming, which tailors streaming to screen resolutions of different devices and optimizes picture quality under varying network conditions. With video downscaling, a high-resolution input video is downscaled into multiple lower-resolution videos. This is typically done by a conventional resampling filter like Lanczos. In this talk, we describe how we improved Netflix video quality by developing neural networks for video downscaling and deploying them at scale.|视频缩放是自适应视频流的重要组成部分，它根据不同设备的屏幕分辨率对视频流进行裁剪，并在不同网络条件下优化图像质量。随着视频缩放，高分辨率输入视频被缩放成多个低分辨率视频。这通常是由一个传统的重采样过滤器，如兰科斯。在这个演讲中，我们描述了我们如何通过开发视频缩放的神经网络和大规模部署来提高 Netflix 的视频质量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Netflix+Video+Quality+with+Neural+Networks)|0|
|[Graph2Feat: Inductive Link Prediction via Knowledge Distillation](https://doi.org/10.1145/3543873.3587596)|Ahmed E. Samy, Zekarias T. Kefato, Sarunas Girdzijauskas|KTH, Royal Institute of Technology, Sweden; KTH, Royal Institue of Technology, Sweden|Link prediction between two nodes is a critical task in graph machine learning. Most approaches are based on variants of graph neural networks (GNNs) that focus on transductive link prediction and have high inference latency. However, many real-world applications require fast inference over new nodes in inductive settings where no information on connectivity is available for these nodes. Thereby, node features provide an inevitable alternative in the latter scenario. To that end, we propose Graph2Feat, which enables inductive link prediction by exploiting knowledge distillation (KD) through the Student-Teacher learning framework. In particular, Graph2Feat learns to match the representations of a lightweight student multi-layer perceptron (MLP) with a more expressive teacher GNN while learning to predict missing links based on the node features, thus attaining both GNN’s expressiveness and MLP’s fast inference. Furthermore, our approach is general; it is suitable for transductive and inductive link predictions on different types of graphs regardless of them being homogeneous or heterogeneous, directed or undirected. We carry out extensive experiments on seven real-world datasets including homogeneous and heterogeneous graphs. Our experiments demonstrate that Graph2Feat significantly outperforms SOTA methods in terms of AUC and average precision in homogeneous and heterogeneous graphs. Finally, Graph2Feat has the minimum inference time compared to the SOTA methods, and 100x acceleration compared to GNNs. The code and datasets are available on GitHub1.|两节点之间的链路预测是图形机器学习中的一个关键问题。大多数方法是基于图神经网络(GNN)的变体，侧重于传导链接预测和具有较高的推理潜伏期。然而，许多实际应用程序需要在归纳设置中对新节点进行快速推理，因为这些节点没有关于连通性的信息。因此，在后一种情况下，节点特性提供了一种不可避免的替代方案。为此，我们提出了 Graph2Feat，它通过学生-教师学习框架利用知识提取(KD)来实现归纳链接预测。特别是，Graph2Feat 学习匹配轻量级学生多层感知器(MLP)和更具表现力的教师 GNN 的表示，同时学习基于节点特征预测缺失链接，从而实现 GNN 的表现力和 MLP 的快速推理。此外，我们的方法是通用的，它适合于对不同类型的图的传导和归纳链接预测，不管它们是同质的或异质的，有向的或无向的。我们在七个真实世界的数据集上进行了广泛的实验，包括同质和异质图。我们的实验表明，Graph2Feat 在 AUC 和均匀和异构图的平均精度方面显著优于 SOTA 方法。最后，Graph2Feat 与 SOTA 方法相比具有最短的推理时间，与 GNN 相比具有100倍的加速度。代码和数据集可以在 GitHub1上获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph2Feat:+Inductive+Link+Prediction+via+Knowledge+Distillation)|0|
|[Universal Model in Online Customer Service](https://doi.org/10.1145/3543873.3587630)|ShuTing Pi, ChengPing Hsieh, Qun Liu, Yuying Zhu|Amazon, USA|Building machine learning models can be a time-consuming process that often takes several months to implement in typical business scenarios. To ensure consistent model performance and account for variations in data distribution, regular retraining is necessary. This paper introduces a solution for improving online customer service in e-commerce by presenting a universal model for predicting labels based on customer questions, without requiring training. Our novel approach involves using machine learning techniques to tag customer questions in transcripts and create a repository of questions and corresponding labels. When a customer requests assistance, an information retrieval model searches the repository for similar questions, and statistical analysis is used to predict the corresponding label. By eliminating the need for individual model training and maintenance, our approach reduces both the model development cycle and costs. The repository only requires periodic updating to maintain accuracy.|构建机器学习模型可能是一个耗时的过程，在典型的业务场景中通常需要几个月才能实现。为了确保模型性能一致，并考虑到数据分布的差异，定期再培训是必要的。本文介绍了一种改进电子商务中在线客户服务的解决方案，提出了一种基于客户问题的通用标签预测模型，该模型不需要培训。我们的新方法包括使用机器学习技术在文本中标记客户的问题，并创建一个问题库和相应的标签。当客户要求协助时，信息检索模型会在储存库中搜索类似的问题，并使用统计分析来预测相应的标签。通过消除对单个模型的培训和维护的需要，我们的方法降低了模型开发周期和成本。存储库只需要定期更新以保持准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Universal+Model+in+Online+Customer+Service)|0|
|[Robust Stochastic Multi-Armed Bandits with Historical Data](https://doi.org/10.1145/3543873.3587653)|Sarah Boufelja Yacobi, Djallel Bouneffouf|IBM Research, USA; Imperial College London, United Kingdom|We consider the problem of Stochastic Contextual Multi-Armed Bandits (CMABs) initialised with Historical data. Initialisation with historical data is an example of data-driven regularisation which should, in theory, accelerate the convergence of CMABs. However, in practice, we have little to no control over the underlying generation process of such data, which may exhibit some pathologies, possibly impeding the convergence and the stability of the algorithm. In this paper, we focus on two main challenges: bias selection and data corruption. We propose two new algorithms to solve these specific issues: LinUCB with historical data and offline balancing (OB-HLinUCB) and Robust LinUCB with corrupted historical data (R-HLinUCB). We derive their theoretical regret bounds and discuss their computational performance using real-world datasets.|我们考虑的问题随机上下文多武装匪徒(CMABs)初始化与历史数据。使用历史数据进行初始化，是数据驱动的正规化的一个例子，理论上应能加速 CMAB 的融合。然而，在实践中，我们对这些数据的基本生成过程几乎没有控制，这可能会表现出一些病态，可能会妨碍算法的收敛和稳定性。在本文中，我们主要关注两个主要的挑战: 偏差选择和数据损坏。我们提出了两种新的算法来解决这些具体问题: 具有历史数据和离线平衡的 LinUCB (OB-HLinUCB)和具有损坏历史数据的鲁棒 LinUCB (R-HLinUCB)。我们推导了它们的理论遗憾界限，并利用实际数据集讨论了它们的计算性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Stochastic+Multi-Armed+Bandits+with+Historical+Data)|0|
|[Skill Graph Construction From Semantic Understanding](https://doi.org/10.1145/3543873.3587667)|Shiyong Lin, Yiping Yuan, Carol Jin, Yi Pan|LinkedIn, USA|LinkedIn is building a skill graph to power a skill-first talent marketplace. Constructing a skill graph from a flat list is not an trivial task, especially by human curation. In this paper, we leverage the pre-trained large language model BERT to achieve this through semantic understanding on synthetically generated texts as training data. We automatically create positive and negative labels from the seed skill graph. The training data are encoded by pre-trained language models into embeddings and they are consumed by the downstream classification module to classify the relationships between skill pairs.|LinkedIn 正在构建一个技能图表，为技能优先的人才市场提供动力。从一个平面列表中构建一个技能图表并不是一件微不足道的事情，尤其是对于人工管理来说。在本文中，我们利用预训练的大语言模型 BERT，通过对综合生成的文本作为训练数据的语义理解来实现这一点。我们自动从种子技能图中创建正面和负面的标签。训练数据通过预先训练的语言模型进行编码嵌入，然后由下游分类模块使用这些数据对技能对之间的关系进行分类。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Skill+Graph+Construction+From+Semantic+Understanding)|0|
|[Cultural Differences in Signed Ego Networks on Twitter: An Investigatory Analysis](https://doi.org/10.1145/3543873.3587641)|Jack Tacchi, Chiara Boldrini, Andrea Passarella, Marco Conti|Istituto di Informatica e Telematica - Consiglio Nazionale delle Ricerche, Italy and Scuola Normale Superiore, Italy; Istituto di Informatica e Telematica - Consiglio Nazionale delle Ricerche, Italy|Human social behaviour has been observed to adhere to certain structures. One such structure, the Ego Network Model (ENM), has been found almost ubiquitously in human society. Recently, this model has been extended to include signed connections. While the unsigned ENM has been rigorously observed for decades, the signed version is still somewhat novel and lacks the same breadth of observation. Therefore, the main aim of this paper is to examine this signed structure across various categories of individuals from a swathe of culturally distinct regions. Minor differences in the distribution of signs across the SENM can be observed between cultures. However, these can be overwhelmed when the network is centred around a specific topic. Indeed, users who are engaged with specific themes display higher levels of negativity in their networks. This effect is further supported by a significant negative correlation between the number of "general" topics discussed in a network and that network’s percentage of negative connections. These findings suggest that the negativity of communications and relationships on Twitter are very dependent on the topics being discussed and, furthermore, these relationships are more likely to be negative when they are based around a specific topic.|人类的社会行为已经被观察到遵循某些结构。其中一种结构，自我网络模型(ENM) ，在人类社会中几乎无处不在。最近，这个模型已经扩展到包括有符号连接。虽然未签名的 ENM 已经被严格观察了几十年，但是签名版本仍然有些新颖，缺乏同样广度的观察。因此，本文的主要目的是考察来自不同文化区域的不同类别的个体的这种符号结构。不同文化之间可以观察到在 SENM 中符号分布的细微差别。然而，当网络围绕某个特定主题时，这些问题可能会不堪重负。事实上，参与特定主题的用户在他们的网络中表现出更高水平的消极性。网络中讨论的“一般”主题的数量与网络负连接的百分比之间存在显著的负相关性，进一步支持了这种效应。这些发现表明，Twitter 上的交流和关系的消极性很大程度上取决于正在讨论的话题，而且，当这些关系围绕着一个特定的话题时，它们更有可能是消极的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cultural+Differences+in+Signed+Ego+Networks+on+Twitter:+An+Investigatory+Analysis)|0|
|[Don't Trust, Verify: The Case of Slashing from a Popular Ethereum Explorer](https://doi.org/10.1145/3543873.3587555)|Zhiguo He, Jiasun Li, Zhengxun Wu|Independent, USA; University of Chicago and NBER, USA; George Mason University, USA|Blockchain explorers are important tools for quick look-ups of on-chain activities. However, as centralized data providers, their reliability remains under-studied. As a case study, we investigate Beaconcha.in , a leading explorer serving Ethereum’s proof-of-stake (PoS) update. According to the explorer, we find that more than 75% of slashable Byzantine actions were not slashed. Since Ethereum relies on the “stake-and-slash" mechanism to align incentives, this finding would at its face value cause concern over Ethereum’s security. However, further investigation reveals that all the apparent unslashed incidents were erroneously recorded due to the explorer’s mishandling of consensus edge cases. Besides the usual message of using caution with centralized information providers, our findings also call for attention to improving the monitoring of blockchain systems that support high-value applications.|区块链探索器是快速查找链上活动的重要工具。然而，作为集中的数据提供者，它们的可靠性仍然没有得到充分的研究。作为一个案例研究，我们调查了 Beaconcha.in，它是一个领先的探索者，服务于以太坊的木桩证明(PoS)更新。根据探险家的说法，我们发现超过75% 的拜占庭式行为没有被砍掉。由于以太坊依靠“利害关系”机制来调整激励机制，这一发现从表面上看会引起人们对以太坊安全性的担忧。然而，进一步的调查表明，所有明显的未删除事件是错误的记录，由于探索者的错误处理共识边缘案件。除了对集中的信息提供者使用谨慎的通常信息之外，我们的研究结果还呼吁注意改进对支持高价值应用的区块链系统的监测。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Don't+Trust,+Verify:+The+Case+of+Slashing+from+a+Popular+Ethereum+Explorer)|0|
|[An Exploration on Cryptocurrency Corporations' Fiscal Opportunities](https://doi.org/10.1145/3543873.3587603)|Thomas Charest, Masarah PaquetClouston|School of Criminology, University of Montreal, Canada|As the decentralized finance industry gains traction, governments worldwide are creating or modifying legislations to regulate such financial activities. To avoid these new legislations, decentralized finance enterprises may shop for fiscally advantageous jurisdictions. This study explores global tax evasion opportunities for decentralized finance enterprises. Opportunities are identified by considering various jurisdictions’ tax laws on cryptocurrencies along with their corporate income tax rates, corporate capital gains tax rates, level of financial development and level of cryptocurrency adoption. They are visualized with the manifold approximation and projection for dimension reduction (UMAP) technique. The study results show that there exist a substantial number of tax evasion opportunities for decentralized finance enterprises through both traditional offshore jurisdictions and crypto-advantageous jurisdictions. The latter jurisdictions are usually considered high-tax fiscal regimes; but, given that they do not apply tax laws, tax evasion opportunities arise, especially in jurisdictions that have high financial development and high cryptocurrency adoption. Further research should investigate these new opportunities and how they are evolving. Understanding the global landscape surrounding tax evasion opportunities in decentralized finance represents a first step at preventing corporate capital flight of cryptocurrencies.|随着权力下放的金融业获得牵引力，世界各国政府正在制定或修订法律，以管制此类金融活动。为了避免这些新的立法，分散的金融企业可以选择财政上有利的司法管辖区。本研究探讨分散型金融企业的全球逃税机会。透过考虑不同地区有关加密货币的税务法例，以及其公司所得税税率、公司资本增值税税率、金融发展水平和加密货币的采用程度，我们可找出机会。它们是可视化的流形近似和投影维度减化(UMAP)技术。研究结果表明，无论是传统的离岸管辖区还是加密优势管辖区，分散的金融企业都有大量的逃税机会。后者通常被认为是高税收的财政体制; 但是，由于它们不适用税法，逃税机会就会出现，特别是在金融发展程度高、采用加密货币程度高的地区。进一步的研究应该调查这些新的机会以及它们是如何演变的。了解分散金融中逃税机会的全球环境，是防止加密货币企业资本外逃的第一步。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Exploration+on+Cryptocurrency+Corporations'+Fiscal+Opportunities)|0|
|[Improving the Exploration/Exploitation Trade-Off in Web Content Discovery](https://doi.org/10.1145/3543873.3587574)|Peter Schulam, Ion Muslea|Amazon Alexa, USA|New web content is published constantly, and although protocols such as RSS can notify subscribers of new pages, they are not always implemented or actively maintained. A more reliable way to discover new content is to periodically re-crawl the target sites. Designing such “content discovery crawlers” has important applications, for example, in web search, digital assistants, business, humanitarian aid, and law enforcement. Existing approaches assume that each site of interest has a relatively small set of unknown “source pages” that, when refreshed, frequently provide hyperlinks to the majority of new content. The state of the art (SOTA) uses ideas from the multi-armed bandit literature to explore candidate sources while simultaneously exploiting known good sources. We observe, however, that the SOTA uses a sub-optimal algorithm for balancing exploration and exploitation. We trace this back to a mismatch between the space of actions that the SOTA algorithm models and the space of actions that the crawler must actually choose from. Our proposed approach, the Thompson crawler (named after the Thompson sampler that drives its refresh decisions), addresses this shortcoming by more faithfully modeling the action space. On a dataset of 4,070 source pages drawn from 53 news domains over a period of 7 weeks, we show that, on average, the Thompson crawler discovers 20% more new pages, finds pages 6 hours earlier, and uses 14 fewer refreshes per 100 pages discovered than the SOTA.|新的 Web 内容不断地发布，尽管 RSS 等协议可以通知订阅者新的页面，但它们并不总是得到实现或积极维护。发现新内容的一种更可靠的方法是定期重新抓取目标站点。设计这样的“内容发现爬虫”有重要的应用，例如，在网络搜索，数字助理，商业，人道主义援助和执法。现有的方法假设每个感兴趣的站点都有一个相对较小的未知“源页面”集合，当刷新时，这些页面经常提供指向大多数新内容的超链接。国家的艺术(SOTA)使用来自多臂老虎机文学的想法来探索候选资源，同时利用已知的好资源。然而，我们观察到 SOTA 使用次优算法来平衡勘探和开发。我们追溯到 SOTA 算法建模的动作空间与爬虫必须实际选择的动作空间之间的不匹配。我们提出的方法是 Thompson 爬行器(以驱动其刷新决策的 Thompson 采样器命名) ，它通过更忠实地建模动作空间来解决这一缺陷。在7周内从53个新闻域抽取的4070个源页面的数据集中，我们发现，与 SOTA 相比，Thompson 爬虫平均多发现20% 的新页面，提前6小时发现页面，并且每发现100个页面少使用14次刷新。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+the+Exploration/Exploitation+Trade-Off+in+Web+Content+Discovery)|0|
|[SoarGraph: Numerical Reasoning over Financial Table-Text Data via Semantic-Oriented Hierarchical Graphs](https://doi.org/10.1145/3543873.3587598)|Fengbin Zhu, Moxin Li, Junbin Xiao, Fuli Feng, Chao Wang, TatSeng Chua|University of Science and Technology of China, China; 6ESTATES PTE LTD, Singapore; National University of Singapore, Singapore|Towards the intelligent understanding of table-text data in the finance domain, previous research explores numerical reasoning over table-text content with Question Answering (QA) tasks. A general framework is to extract supporting evidence from the table and text and then perform numerical reasoning over extracted evidence for inferring the answer. However, existing models are vulnerable to missing supporting evidence, which limits their performance. In this work, we propose a novel Semantic-Oriented Hierarchical Graph (SoarGraph) that models the semantic relationships and dependencies among the different elements (e.g., question, table cells, text paragraphs, quantities, and dates) using hierarchical graphs to facilitate supporting evidence extraction and enhance numerical reasoning capability. We conduct our experiments on two popular benchmarks, FinQA and TAT-QA datasets, and the results show that our SoarGraph significantly outperforms all strong baselines, demonstrating remarkable effectiveness.|针对金融领域中表格文本数据的智能理解问题，以往的研究采用问答(QA)任务对表格文本内容进行数值推理。一个通用的框架是从表格和文本中提取支持证据，然后对提取的证据进行数值推理，从而推断出答案。然而，现有的模型容易失去支持证据，这限制了它们的性能。在这项工作中，我们提出了一种新的面向语义的层次图(SoarGraph) ，它使用层次图来模拟不同元素(如问题、表格单元、文本段落、数量和日期)之间的语义关系和依赖关系，以便于支持证据提取和增强数值推理能力。我们在 FinQA 和 TAT-QA 数据集上进行了实验，结果表明我们的 SoarGraph 显著优于所有强基线，显示出显著的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SoarGraph:+Numerical+Reasoning+over+Financial+Table-Text+Data+via+Semantic-Oriented+Hierarchical+Graphs)|0|
|[Online to Offline Crossover of White Supremacist Propaganda](https://doi.org/10.1145/3543873.3587569)|Ahmad Diab, BolorErdene Jagdagdorj, Lynnette Hui Xian Ng, YuRu Lin, Michael Miller Yoder|Carnegie Mellon University, USA; University of Pittsburgh, USA|White supremacist extremist groups are a significant domestic terror threat in many Western nations. These groups harness the Internet to spread their ideology via online platforms: blogs, chat rooms, forums, and social media, which can inspire violence offline. In this work, we study the persistence and reach of white supremacist propaganda in both online and offline environments. We also study patterns in narratives that crossover from online to offline environments, or vice versa. From a geospatial analysis, we find that offline propaganda is geographically widespread in the United States, with a slight tendency toward Northeastern states. Propaganda that spreads the farthest and lasts the longest has a patriotic framing and is short, memorable, and repeatable. Through text comparison methods, we illustrate that online propaganda typically leads the appearance of the same propaganda in offline flyers, banners, and graffiti. We hope that this study sheds light on the characteristics of persistent white supremacist narratives both online and offline.|在许多西方国家，白人至上主义极端组织是一个重大的国内恐怖主义威胁。这些团体利用互联网通过在线平台传播他们的意识形态: 博客、聊天室、论坛和社交媒体，这些平台可以在线下煽动暴力。在这项工作中，我们研究了白人至上主义宣传在两个在线和离线环境中的持续性和影响范围。我们也研究从 Online To Offline线上到线下环境中交叉出来的叙述模式，反之亦然。通过地理空间分析，我们发现线下宣传在美国的地理位置上非常普遍，有轻微的美国东北部倾向。传播最广、持续时间最长的宣传具有爱国主义的框架，简短、令人难忘、可重复。通过文本比较的方法，我们说明了在线宣传通常会导致同样的宣传出现在线下传单、横幅和涂鸦中。我们希望这项研究能够揭示白人至上主义叙事的在线和离线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+to+Offline+Crossover+of+White+Supremacist+Propaganda)|0|
|[Privacy-Preserving Online Content Moderation with Federated Learning](https://doi.org/10.1145/3543873.3587366)|Pantelitsa Leonidou, Nicolas Kourtellis, Nikos Salamanos, Michael Sirivianos|Telefonica Research, Spain; Cyprus University of Technology, Cyprus|Users are daily exposed to a large volume of harmful content on various social network platforms. One solution is developing online moderation tools using Machine Learning techniques. However, the processing of user data by online platforms requires compliance with privacy policies. Federated Learning (FL) is an ML paradigm where the training is performed locally on the users' devices. Although the FL framework complies, in theory, with the GDPR policies, privacy leaks can still occur. For instance, an attacker accessing the final trained model can successfully perform unwanted inference of the data belonging to the users who participated in the training process. In this paper, we propose a privacy-preserving FL framework for online content moderation that incorporates Differential Privacy (DP). To demonstrate the feasibility of our approach, we focus on detecting harmful content on Twitter - but the overall concept can be generalized to other types of misbehavior. We simulate a text classifier - in FL fashion - which can detect tweets with harmful content. We show that the performance of the proposed FL framework can be close to the centralized approach - for both the DP and non-DP FL versions. Moreover, it has a high performance even if a small number of clients (each with a small number of data points) are available for the FL training. When reducing the number of clients (from 50 to 10) or the data points per client (from 1K to 0.1K), the classifier can still achieve ~81% AUC. Furthermore, we extend the evaluation to four other Twitter datasets that capture different types of user misbehavior and still obtain a promising performance (61% - 80% AUC). Finally, we explore the overhead on the users' devices during the FL training phase and show that the local training does not introduce excessive CPU utilization and memory consumption overhead.|用户每天都会在各种社交网络平台上接触到大量的有害内容。一个解决方案是使用机器学习技术开发在线审核工具。然而，通过在线平台处理用户数据需要遵守隐私策略。联邦学习(FL)是一种机器学习范式，其中的培训是在用户的设备上本地执行的。尽管 FL 框架在理论上符合 GDPR 策略，但仍然可能发生隐私泄露。例如，访问最终训练模型的攻击者可以成功地对属于参与训练过程的用户的数据进行不必要的推断。在这篇文章中，我们提出了一个保护隐私的在线内容审查框架，该框架结合了差分隐私(DP)。为了证明我们方法的可行性，我们将重点放在检测 Twitter 上的有害内容上——但总体概念可以推广到其他类型的不当行为。我们模拟了一个文本分类器——以 FL 的方式——它可以检测带有有害内容的 tweet。我们展示了所提出的 FL 框架的性能可以接近于集中式方法——对于 DP 和非 DP FL 版本都是如此。此外，即使只有少量的客户端(每个客户端都有少量的数据点)可用于 FL 培训，它也具有很高的性能。当减少客户端数量(从50到10)或每个客户端的数据点数量(从1K 到0.1 K)时，分类器仍然可以达到约81% 的 AUC。此外，我们将评估扩展到其他四个 Twitter 数据集，这些数据集捕获不同类型的用户不当行为，并仍然获得有希望的性能(61% -80% AUC)。最后，我们研究了 FL 训练阶段用户设备上的开销，结果表明本地训练不会引入过多的 CPU 利用率和内存消耗开销。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privacy-Preserving+Online+Content+Moderation+with+Federated+Learning)|0|
|[Graph-based Approach for Studying Spread of Radical Online Sentiment](https://doi.org/10.1145/3543873.3587634)|Le Nguyen, Nidhi Rastogi|Golisano College of Computing and Information Science, Department of Software Engineering, Rochester Institute of Technology, USA|The spread of radicalization through the Internet is a growing problem. We are witnessing a rise in online hate groups, inspiring the impressionable and vulnerable population towards extreme actions in the real world. In this paper, we study the spread of hate sentiments in online forums by collecting 1,973 long comment threads (30+ comments per thread) posted on dark-web forums and containing a combination of benign posts and radical comments on the Islamic religion. This framework allows us to leverage network analysis tools to investigate sentiment propagation through a social network. By combining sentiment analysis, social network analysis, and graph theory, we aim to shed light on the propagation of hate speech in online forums and the extent to which such speech can influence individuals. The results of the intra-thread analysis suggests that sentiment tends to cluster within comment threads, with around 75% of connected members sharing similar sentiments. They also indicate that online forums can act as echo chambers where people with similar views reinforce each other’s beliefs and opinions. On the other hand, the inter-thread shows that 64% of connected threads share similar sentiments, suggesting similarities between the ideologies present in different threads and that there likely is a wider network of individuals spreading hate speech across different forums. Finally, we plan to study this work with a larger dataset, which could provide further insights into the spread of hate speech in online forums and how to mitigate it.|通过互联网传播激进主义是一个日益严重的问题。我们正在目睹网上仇恨团体的增加，鼓励易受影响的弱势群体在现实世界中采取极端行动。在这篇论文中，我们通过收集在暗网论坛上发布的1973条长评论(每条评论超过30条) ，以及包含对伊斯兰教的善意帖子和激进评论的组合，来研究在网络论坛上仇恨情绪的传播。这个框架允许我们利用网络分析工具来调查通过社交网络的情感传播。通过结合情绪分析、社会网络分析和图论，我们旨在阐明仇恨言论在网络论坛中的传播以及这种言论对个人的影响程度。内部线程分析的结果表明，情绪倾向于聚集在评论线程中，大约75% 的连接成员共享类似的情绪。他们还指出，在线论坛可以充当回音室，在这里，持有相似观点的人们可以相互加强信念和观点。另一方面，线程之间显示，64% 的连接线程有相似的情感，表明存在于不同线程的意识形态之间的相似性，并且可能有一个更广泛的个人网络在不同的论坛上传播仇恨言论。最后，我们计划利用一个更大的数据集来研究这项工作，它可以为在线论坛中仇恨言论的传播以及如何减轻这种传播提供进一步的见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph-based+Approach+for+Studying+Spread+of+Radical+Online+Sentiment)|0|
|[Swinging in the States: Does disinformation on Twitter mirror the US presidential election system?](https://doi.org/10.1145/3543873.3587638)|Manuel Pratelli, Marinella Petrocchi, Fabio Saracco, Rocco De Nicola|IMT - School for Advanced Studies Lucca, Italy and Istituto di Informatica e Telematica (IIT) CNR, Italy; Istituto di Informatica e Telematica (IIT) CNR, Italy and IMT - School for Advanced Studies Lucca, Italy; “Enrico Fermi” Research Center (CREF), Italy and IMT - School for Advanced Studies Lucca, Italy; IMT - School for Advanced Studies Lucca, Italy|For more than a decade scholars have been investigating the disinformation flow on social media contextually to societal events, like, e.g., elections. In this paper, we analyze the Twitter traffic related to the US 2020 pre-election debate and ask whether it mirrors the electoral system. The U.S. electoral system provides that, regardless of the actual vote gap, the premier candidate who received more votes in one state `takes' that state. Criticisms of this system have pointed out that election campaigns can be more intense in particular key states to achieve victory, so-called {\it swing states}. Our intuition is that election debate may cause more traffic on Twitter-and probably be more plagued by misinformation-when associated with swing states. The results mostly confirm the intuition. About 88\% of the entire traffic can be associated with swing states, and links to non-trustworthy news are shared far more in swing-related traffic than the same type of news in safe-related traffic. Considering traffic origin instead, non-trustworthy tweets generated by automated accounts, so-called social bots, are mostly associated with swing states. Our work sheds light on the role an electoral system plays in the evolution of online debates, with, in the spotlight, disinformation and social bots.|十多年来，学者们一直在研究社交媒体上的虚假信息与社会事件的关系，比如选举。在本文中，我们分析了与美国2020年大选前辩论有关的 Twitter 流量，并询问它是否反映了选举制度。美国的选举制度规定，不管实际的选票差距如何，在一个州获得更多选票的总理候选人“接受”该州。对这种制度的批评指出，竞选活动可以更加激烈，在特定的关键州取得胜利，所谓的“摇摆州”。我们的直觉是，当选举辩论与摇摆州联系在一起时，可能会在 Twitter 上引起更多流量，而且可能更容易受到错误信息的困扰。结果大多证实了直觉。大约88% 的流量可以与摇摆州联系起来，与安全相关的流量相比，与摇摆州相关的流量中分享的不可信新闻的链接要多得多。相反，考虑到流量来源，由自动账户(即所谓的社交机器人)生成的不可信的 tweet 大多与摇摆州有关。我们的工作揭示了选举系统在网络辩论演变中所扮演的角色，聚光灯下是虚假信息和社交机器人。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Swinging+in+the+States:+Does+disinformation+on+Twitter+mirror+the+US+presidential+election+system?)|0|
|[Analyzing Activity and Suspension Patterns of Twitter Bots Attacking Turkish Twitter Trends by a Longitudinal Dataset](https://doi.org/10.1145/3543873.3587650)|Tugrulcan Elmas|Indiana University Bloomington, USA|Twitter bots amplify target content in a coordinated manner to make them appear popular, which is an astroturfing attack. Such attacks promote certain keywords to push them to Twitter trends to make them visible to a broader audience. Past work on such fake trends revealed a new astroturfing attack named ephemeral astroturfing that employs a very unique bot behavior in which bots post and delete generated tweets in a coordinated manner. As such, it is easy to mass-annotate such bots reliably, making them a convenient source of ground truth for bot research. In this paper, we detect and disclose over 212,000 such bots targeting Turkish trends, which we name astrobots. We also analyze their activity and suspension patterns. We found that Twitter purged those bots en-masse 6 times since June 2018. However, the adversaries reacted quickly and deployed new bots that were created years ago. We also found that many such bots do not post tweets apart from promoting fake trends, which makes it challenging for bot detection methods to detect them. Our work provides insights into platforms' content moderation practices and bot detection research. The dataset is publicly available at https://github.com/tugrulz/EphemeralAstroturfing.|Twitter 机器人以一种协调的方式放大目标内容，让它们看起来很受欢迎，这是一种草根攻击。这样的攻击促进了某些关键词，将它们推向 Twitter 趋势，使它们对更广泛的受众显而易见。过去对这种虚假趋势的研究揭示了一种名为“短暂星空草坪”的新型星空草坪攻击，该攻击采用了一种非常独特的机器人行为，机器人以协调的方式发布和删除生成的推文。因此，很容易可靠地对这些机器人进行大量注释，使它们成为机器人研究的一个方便的地面真相来源。在这篇论文中，我们发现并揭露了超过212,000个这样的机器人瞄准了土耳其的趋势，我们将其命名为“太空机器人”。我们还分析了它们的活动和悬浮模式。我们发现，自2018年6月以来，Twitter 共清除了这些机器人6次。然而，对手反应迅速，部署了多年前创建的新机器人。我们还发现，许多这样的机器人除了推广虚假趋势之外，不会发布推文，这使得机器人检测方法很难检测到它们。我们的工作为平台的内容审核实践和机器人检测研究提供了见解。该数据集可在 https://github.com/tugrulz/ephemeralastroturfing 公开获取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analyzing+Activity+and+Suspension+Patterns+of+Twitter+Bots+Attacking+Turkish+Twitter+Trends+by+a+Longitudinal+Dataset)|0|
|[Socio-Emotional Computational Analysis of Propaganda Campaigns on Social Media Users in the Middle East](https://doi.org/10.1145/3543873.3587677)|Zain A. Halloush, Ahmed Aleroud, Craig Albert|School of Computer and Cyber Sciences, Augusta University, Augusta University, USA; Pamplin College of Arts, Humanities, and Social Sciences, Augusta University, USA|Society has been significantly impacted by social media platforms in almost every aspect of their life. This impact has been effectively formulating people’s global mindsets and opinions on political, economic, and social events. Such waves of opinion formation are referred to as propagandas and misinformation. Online propaganda influences the emotional and psychological orientation of people. The remarkable leaps in Machine Learning models and Natural Language Processing have helped in analyzing the emotional and psychological effects of cyber social threats such as propaganda campaigns on different nations, specifically in the Middle East, where rates of disputes have risen after the Arab Spring and the ongoing crises. In this paper, we present an approach to detect propagandas and the associated emotional and psychological aspects from social media news headlines that contain such a contextualized cyber social attack. We created a new dataset of headlines containing propaganda tweets and another dataset of potential emotions that the audience might endure when being exposed to such propaganda headlines. We believe that this is the first research to address the detection of emotional reactions linked to propaganda types on social media in the Middle East.|社会几乎在其生活的各个方面都受到社交媒体平台的显著影响。这种影响有效地形成了人们对政治、经济和社会事件的全球心态和观点。这种形成意见的浪潮被称为宣传和错误信息。网络宣传影响着人们的情感和心理取向。机器学习模型和自然语言处理的显著飞跃有助于分析网络社会威胁的情感和心理影响，例如针对不同国家的宣传活动，特别是在中东，在阿拉伯之春和持续的危机之后，那里的争端比率已经上升。在本文中，我们提出了一种方法来检测宣传和相关的情绪和心理方面的社会媒体新闻标题，其中包含这样一个情境化的网络社会攻击。我们创建了一个新的标题数据集，其中包含宣传推文和另一个潜在情绪数据集，观众在接触这些宣传标题时可能会忍受这些情绪。我们认为，这是第一次研究如何在中东社交媒体上发现与宣传类型相关的情绪反应。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Socio-Emotional+Computational+Analysis+of+Propaganda+Campaigns+on+Social+Media+Users+in+the+Middle+East)|0|
|[Towards a Semantic Approach for Linked Dataspace, Model and Data Cards](https://doi.org/10.1145/3543873.3587659)|Andy Donald, Apostolos Galanopoulos, Edward Curry, Emir Muñoz, Ihsan Ullah, M. A. Waskow, Maciej Dabrowski, Manan Kalra|Genesys Cloud Services Inc., Bonham Quay, Galway, Ireland, Ireland; Insight SFI Centre for Data Analytics, Data Science Institute, University of Galway, Galway, Ireland, Ireland|The vast majority of artificial intelligence practitioners overlook the importance of documentation when building and publishing models and datasets. However, due to the recent trend in the explainability and fairness of AI models, several frameworks have been proposed such as Model Cards, and Data Cards, among others, to help in the appropriate re-usage of those models and datasets. In addition, because of the introduction of the dataspace concept for similar datasets in one place, there is potential that similar Model Cards, Data Cards, Service Cards, and Dataspace Cards can be linked to extract helpful information for better decision-making about which model and data can be used for a specific application. This paper reviews the case for considering a Semantic Web approach for exchanging Model/Data Cards as Linked Data or knowledge graphs in a dataspace, making them machine-readable. We discuss the basic concepts and propose a schema for linking Data Cards and Model Cards within a dataspace. In addition, we introduce the concept of a dataspace card which can be a starting point for extracting knowledge about models and datasets in a dataspace. This helps in building trust and reuse of models and data among companies and individuals participating as publishers or consumers of such assets.|绝大多数人工智能从业者在构建和发布模型和数据集时忽视了文档的重要性。然而，由于人工智能模型的可解释性和公平性最近的趋势，已经提出了几个框架，如模型卡和数据卡等，以帮助这些模型和数据集的适当重用。此外，由于在一个地方引入了类似数据集的数据空间概念，可以将类似的模型卡、数据卡、服务卡和数据空间卡联系起来，提取有用的信息，以便更好地决定哪些模型和数据可用于特定应用。本文回顾了在数据空间中考虑语义 Web 方法将模型/数据卡交换为链接数据或知识图的情况，从而使它们具有机器可读性。我们讨论了基本概念，并提出了一个在数据空间中连接数据卡和模型卡的模式。此外，我们还介绍了数据空间卡的概念，它可以作为提取数据空间中模型和数据集知识的起点。这有助于在作为此类资产的发布者或消费者参与的公司和个人之间建立模型和数据的信任和重用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+a+Semantic+Approach+for+Linked+Dataspace,+Model+and+Data+Cards)|0|
|[Semantics in Dataspaces: Origin and Future Directions](https://doi.org/10.1145/3543873.3587689)|Johannes TheissenLipp, Max Kocher, Christoph Lange, Stefan Decker, Alexander Paulus, André Pomp, Edward Curry|RWTH Aachen University, Germany and Fraunhofer Institute for Applied Information Technology FIT, Germany; University of Wuppertal, Germany; University of Galway, Ireland; RWTH Aachen University, Germany|The term dataspace was coined two decades ago [12] and has evolved since then. Definitions range from (i) an abstraction for data management in an identifiable scope [15] over (iii) a multi-sided data platform connecting participants in an ecosystem [21] to (iii) interlinking data towards loosely connected (global) information [17]. Many implementations and scientific notions follow different interpretations of the term dataspace, but agree on some use of semantic technologies. For example, dataspaces such as the European Open Science Cloud and the German National Research Data Infrastructure are committed to applying the FAIR principles [11, 16]. Dataspaces built on top of Gaia-X are using semantic methods for service Self-Descriptions [13]. This paper investigates ongoing dataspace efforts and aims to provide insights on the definition of the term dataspace, the usage of semantics and FAIR principles, and future directions for the role of semantics in dataspaces.|数据空间这个术语是在20年前创造出来的，并且从那时起一直在进化。定义范围从(i)可识别范围内的数据管理的抽象[15]到(iii)连接生态系统参与者的多边数据平台[21]到(iii)将数据互联到松散连接的(全球)信息[17]。许多实现和科学概念遵循对数据空间这一术语的不同解释，但对语义技术的某些使用达成了一致。例如，欧洲开放科学云和德国国家研究数据基础设施等数据空间致力于应用 FAIR 原则[11,16]。建立在 Gaia-X 之上的数据空间正在使用语义方法进行服务自我描述[13]。本文研究了正在进行的数据空间工作，旨在提供对术语数据空间的定义、语义和 FAIR 原则的使用以及语义在数据空间中的作用的未来方向的见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semantics+in+Dataspaces:+Origin+and+Future+Directions)|0|
|[Efficient Sampling for Big Provenance](https://doi.org/10.1145/3543873.3587556)|Sara Moshtaghi Largani, Seokki Lee|University of Cincinnati, USA|Provenance has been studied extensively to explain existing and missing results for many applications while focusing on scalability and usability challenges. Recently, techniques that efficiently compute a compact representation of provenance have been introduced. In this work, we introduce a practical solution that computes a sample of provenance for existing results without computing full provenance. Our technique computes a sample of provenance based on the distribution of provenance wrt the query result that is estimated from the distribution of input data while considering the correlation among the data. The preliminary evaluation demonstrates that comparing to the naive approach our method efficiently computes a sample of (large size of) provenance with low errors.|起源已被广泛研究，以解释许多应用程序的现有结果和缺失结果，同时关注可伸缩性和可用性方面的挑战。最近，有效地计算种源的紧凑表示的技术已经被引入。在这项工作中，我们介绍了一个实用的解决方案，计算现有结果的来源样本，而不计算完整的来源。该方法在考虑数据间相关性的基础上，根据来源分布计算出一个来源样本，并根据输入数据的分布估计出查询结果。初步评估表明，与初始方法相比，我们的方法有效地计算了一个样本(大规模)来源低误差。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Sampling+for+Big+Provenance)|0|
|[Provenance Tracking for End-to-End Machine Learning Pipelines](https://doi.org/10.1145/3543873.3587557)|Stefan Grafberger, Paul Groth, Sebastian Schelter|University of Amsterdam, Netherlands|No abstract available.|没有摘要。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Provenance+Tracking+for+End-to-End+Machine+Learning+Pipelines)|0|
|[SeeGera: Self-supervised Semi-implicit Graph Variational Auto-encoders with Masking](https://doi.org/10.1145/3543507.3583245)|Xiang Li, Tiandi Ye, Caihua Shan, Dongsheng Li, Ming Gao|Microsoft Research Asia, China; East China Normal University, China|Generative graph self-supervised learning (SSL) aims to learn node representations by reconstructing the input graph data. However, most existing methods focus on unsupervised learning tasks only and very few work has shown its superiority over the state-of-the-art graph contrastive learning (GCL) models, especially on the classification task. While a very recent model has been proposed to bridge the gap, its performance on unsupervised learning tasks is still unknown. In this paper, to comprehensively enhance the performance of generative graph SSL against other GCL models on both unsupervised and supervised learning tasks, we propose the SeeGera model, which is based on the family of self-supervised variational graph auto-encoder (VGAE). Specifically, SeeGera adopts the semi-implicit variational inference framework, a hierarchical variational framework, and mainly focuses on feature reconstruction and structure/feature masking. On the one hand, SeeGera co-embeds both nodes and features in the encoder and reconstructs both links and features in the decoder. Since feature embeddings contain rich semantic information on features, they can be combined with node embeddings to provide fine-grained knowledge for feature reconstruction. On the other hand, SeeGera adds an additional layer for structure/feature masking to the hierarchical variational framework, which boosts the model generalizability. We conduct extensive experiments comparing SeeGera with 9 other state-of-the-art competitors. Our results show that SeeGera can compare favorably against other state-of-the-art GCL methods in a variety of unsupervised and supervised learning tasks.|生成图自监督学习(SSL)的目的是通过重构输入图数据来学习节点表示。然而，大多数现有的方法只关注非监督式学习任务，很少有研究显示其优于最先进的图形对比学习(gCL)模型，特别是在分类任务上。虽然最近有人提出了一个模型来弥补这一差距，但它在非监督式学习任务上的表现仍然是未知的。为了全面提高生成图 SSL 在无监督任务和监督式学习任务中相对于其他 GCL 模型的性能，我们提出了基于自监督变分图自动编码器(vgAE)系列的 SeeGera 模型。具体来说，SeeGera 采用了半隐式变分推理框架，即层次变分框架，主要研究特征重构和结构/特征掩蔽。一方面，SeeGera 在编码器中共同嵌入节点和特征，并在解码器中重构链路和特征。由于特征嵌入包含丰富的特征语义信息，因此它们可以与节点嵌入相结合，为特征重构提供细粒度的知识。另一方面，SeeGera 为层次变分框架增加了一个结构/特征屏蔽层，从而提高了模型的通用性。我们进行了广泛的实验比较 SeeGera 与其他9个国家的最先进的竞争对手。我们的研究结果表明，SeeGera 可以在各种无监督和无监督式学习的任务中与其他最先进的 GCL 方法相比较。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SeeGera:+Self-supervised+Semi-implicit+Graph+Variational+Auto-encoders+with+Masking)|0|
|[Lightweight source localization for large-scale social networks](https://doi.org/10.1145/3543507.3583299)|Zhen Wang, Dongpeng Hou, Chao Gao, Xiaoyu Li, Xuelong Li|Northwestern Polytechnical University, China; School of Artificial Intelligence, Optics and Electronics (iOPEN), Northwestern Polytechnical University, China|The rapid diffusion of hazardous information in large-flow-based social media causes great economic losses and potential threats to society. It is crucial to infer the inner information source as early as possible to prevent further losses. However, existing localization methods wait until all deployed sensors obtain propagation information before starting source inference within a network, and hence the best opportunity to control propagation is missed. In this paper, we propose a new localization strategy based on finite deployed sensors, named Greedy-coverage-based Rapid Source Localization (GRSL), to rapidly, flexibly and accurately infer the source in the early propagation stage of large-scale networks. There are two phases in GRSL. In the first phase, the Greedy-based Strategy (GS) greedily deploys sensors to rapidly achieve wide area coverage at a low cost. In the second phase, when a propagation event within a network is observed by a part of the sensors, the Inference Strategy (IS) with an earlier response mechanism begins executing the source inference task in an earlier small infected area. Comprehensive experiments with the SOTA methods demonstrate the superior performance and robustness of GRSL in various application scenarios.|危险信息在基于大流量的社交媒体上的快速传播给社会造成了巨大的经济损失和潜在的威胁。为了防止进一步的损失，尽早推断出内部信息源至关重要。然而，现有的定位方法要等到所有部署的传感器获得传播信息后，才能在网络中开始源推理，因此错过了控制传播的最佳机会。提出了一种基于有限部署传感器的快速信源定位策略，即基于贪婪覆盖的快速信源定位(GRSL)策略，可以在大规模网络传播的早期阶段快速、灵活、准确地推断信源。GRSL 有两个阶段。在第一阶段，基于贪婪策略(GS)贪婪地部署传感器，以低成本快速实现广域覆盖。在第二个阶段，当一部分传感器观察到网络中的传播事件时，具有早期响应机制的推理策略(IS)开始在早期的小受感染区域中执行源推理任务。通过 SOTA 方法的综合实验，证明了 GRSL 在各种应用场景下的优越性能和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lightweight+source+localization+for+large-scale+social+networks)|0|
|[xGCN: An Extreme Graph Convolutional Network for Large-scale Social Link Prediction](https://doi.org/10.1145/3543507.3583340)|Xiran Song, Jianxun Lian, Hong Huang, Zihan Luo, Wei Zhou, Xue Lin, Mingqi Wu, Chaozhuo Li, Xing Xie, Hai Jin|National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, China; Microsoft Gaming, USA; Microsoft Research Asia, China|Graph neural networks (GNNs) have seen widespread usage across multiple real-world applications, yet in transductive learning, they still face challenges in accuracy, efficiency, and scalability, due to the extensive number of trainable parameters in the embedding table and the paradigm of stacking neighborhood aggregations. This paper presents a novel model called xGCN for large-scale network embedding, which is a practical solution for link predictions. xGCN addresses these issues by encoding graph-structure data in an extreme convolutional manner, and has the potential to push the performance of network embedding-based link predictions to a new record. Specifically, instead of assigning each node with a directly learnable embedding vector, xGCN regards node embeddings as static features. It uses a propagation operation to smooth node embeddings and relies on a Refinement neural Network (RefNet) to transform the coarse embeddings derived from the unsupervised propagation into new ones that optimize a training objective. The output of RefNet, which are well-refined embeddings, will replace the original node embeddings. This process is repeated iteratively until the model converges to a satisfying status. Experiments on three social network datasets with link prediction tasks show that xGCN not only achieves the best accuracy compared with a series of competitive baselines but also is highly efficient and scalable.|图形神经网络(GNN)在多种现实应用中得到了广泛的应用，然而在传导学习中，由于嵌入表中大量的可训练参数和邻域聚合堆叠的范式，它们在精度、效率和可扩展性方面仍然面临挑战。提出了一种新的大规模网络嵌入模型 xGCN，该模型是链路预测的一种实用解决方案。XGCN 通过以极端卷积方式编码图形结构数据来解决这些问题，并且有可能将基于网络嵌入的链接预测的性能提升到一个新的记录。具体来说，xGCN 没有为每个节点分配一个可直接学习的嵌入向量，而是将节点嵌入视为静态特征。该算法利用传播操作来平滑节点嵌入，并依靠细化神经网络(RefNet)将无监督传播产生的粗嵌入转化为优化训练目标的新嵌入。RefNet 的输出是经过良好改进的嵌入，它将取代原始节点嵌入。这个过程反复重复，直到模型收敛到一个令人满意的状态。通过对三个具有链接预测任务的社会网络数据集的实验表明，xGCN 不仅比一系列竞争性基线获得了最佳的预测精度，而且具有高效性和可扩展性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=xGCN:+An+Extreme+Graph+Convolutional+Network+for+Large-scale+Social+Link+Prediction)|0|
|[GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks](https://doi.org/10.1145/3543507.3583386)|Zemin Liu, Xingtong Yu, Yuan Fang, Xinming Zhang|University of Science and Technology of China, China; Singapore Management University, Singapore; National University of Singapore, Singapore|Graphs can model complex relationships between objects, enabling a myriad of Web applications such as online page/article classification and social recommendation. While graph neural networks(GNNs) have emerged as a powerful tool for graph representation learning, in an end-to-end supervised setting, their performance heavily rely on a large amount of task-specific supervision. To reduce labeling requirement, the "pre-train, fine-tune" and "pre-train, prompt" paradigms have become increasingly common. In particular, prompting is a popular alternative to fine-tuning in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives in a task-specific manner. However, existing study of prompting on graphs is still limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-train model in a task-specific manner. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt.|图形可以模拟对象之间的复杂关系，支持无数的 Web 应用程序，例如在线页面/文章分类和社交推荐。尽管图神经网络(GNN)已经成为图表示学习的有力工具，但在端到端的监督环境下，它们的性能很大程度上依赖于大量的任务特定监督。为了减少标签要求，“预训练，微调”和“预训练，及时”的范例已经变得越来越普遍。特别是，提示是自然语言处理中微调的一种流行替代方法，其目的是以特定于任务的方式缩小培训前和下游目标之间的差距。然而，现有的图形激励研究仍然是有限的，缺乏一个普遍的治疗呼吁不同的下游任务。本文提出了一种新的图形预训练和提示框架 GraphPrompt。GraphPrompt 不仅将预训练和下游任务统一到一个共同的任务模板中，而且还使用可学习的提示来帮助下游任务以特定于任务的方式从预训练模型中找到最相关的知识。最后，我们在五个公共数据集上进行了广泛的实验来评估和分析 GraphPrompt。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphPrompt:+Unifying+Pre-Training+and+Downstream+Tasks+for+Graph+Neural+Networks)|0|
|[FedACK: Federated Adversarial Contrastive Knowledge Distillation for Cross-Lingual and Cross-Model Social Bot Detection](https://doi.org/10.1145/3543507.3583500)|Yingguang Yang, Renyu Yang, Hao Peng, Yangyang Li, Tong Li, Yong Liao, Pengyuan Zhou|University of Science and Technology of China, China; Beihang University, China; NERC-RPP, CAEIT, China; Tsinghua University, China; University of Leeds, United Kingdom|Social bot detection is of paramount importance to the resilience and security of online social platforms. The state-of-the-art detection models are siloed and have largely overlooked a variety of data characteristics from multiple cross-lingual platforms. Meanwhile, the heterogeneity of data distribution and model architecture makes it intricate to devise an efficient cross-platform and cross-model detection framework. In this paper, we propose FedACK, a new federated adversarial contrastive knowledge distillation framework for social bot detection. We devise a GAN-based federated knowledge distillation mechanism for efficiently transferring knowledge of data distribution among clients. In particular, a global generator is used to extract the knowledge of global data distribution and distill it into each client's local model. We leverage local discriminator to enable customized model design and use local generator for data enhancement with hard-to-decide samples. Local training is conducted as multi-stage adversarial and contrastive learning to enable consistent feature spaces among clients and to constrain the optimization direction of local models, reducing the divergences between local and global models. Experiments demonstrate that FedACK outperforms the state-of-the-art approaches in terms of accuracy, communication efficiency, and feature space consistency.|社交机器人检测对于在线社交平台的弹性和安全性至关重要。最先进的检测模型是孤立的，在很大程度上忽略了来自多个跨语言平台的各种数据特征。同时，数据分布和模型结构的异构性使得设计一个有效的跨平台、跨模型检测框架变得复杂。在本文中，我们提出了一个新的联邦对抗性对比知识提取框架 FedACK，用于社会机器人检测。设计了一种基于 GAN 的联邦知识提取机制，用于在客户端之间有效地传递数据分布的知识。特别地，全局生成器用于提取全局数据分布的知识，并将其提取到每个客户机的本地模型中。我们利用局部鉴别器来实现定制的模型设计，并使用局部生成器对难以确定的样本进行数据增强。局部训练作为多阶段对抗性和对比性学习进行，以使客户之间的特征空间保持一致，并约束局部模型的优化方向，减少局部模型和全局模型之间的差异。实验结果表明，FedACK 算法在准确性、通信效率和特征空间一致性方面优于目前最先进的算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedACK:+Federated+Adversarial+Contrastive+Knowledge+Distillation+for+Cross-Lingual+and+Cross-Model+Social+Bot+Detection)|0|
|[Self-training through Classifier Disagreement for Cross-Domain Opinion Target Extraction](https://doi.org/10.1145/3543507.3583325)|Kai Sun, Richong Zhang, Samuel Mensah, Nikolaos Aletras, Yongyi Mao, Xudong Liu|; Computer Science Department, University of Sheffield, UK, United Kingdom; SKLSDE, School of Computer Science and Engineering, Beihang University, China; School of Electrical Engineering and Computer Science, University of Ottawa, Canada|Opinion target extraction (OTE) or aspect extraction (AE) is a fundamental task in opinion mining that aims to extract the targets (or aspects) on which opinions have been expressed. Recent work focus on cross-domain OTE, which is typically encountered in real-world scenarios, where the testing and training distributions differ. Most methods use domain adversarial neural networks that aim to reduce the domain gap between the labelled source and unlabelled target domains to improve target domain performance. However, this approach only aligns feature distributions and does not account for class-wise feature alignment, leading to suboptimal results. Semi-supervised learning (SSL) has been explored as a solution, but is limited by the quality of pseudo-labels generated by the model. Inspired by the theoretical foundations in domain adaptation [2], we propose a new SSL approach that opts for selecting target samples whose model output from a domain-specific teacher and student network disagree on the unlabelled target data, in an effort to boost the target domain performance. Extensive experiments on benchmark cross-domain OTE datasets show that this approach is effective and performs consistently well in settings with large domain shifts.|意见目标提取(OTE)或方面提取(AE)是意见挖掘中的一个基本任务，其目的是提取表达意见的目标(或方面)。最近的工作集中在跨域 OTE 上，这是在现实世界的场景中经常遇到的问题，其中测试和训练的分布是不同的。大多数方法使用域对抗神经网络，目的是减少标记源和未标记目标域之间的域差，以提高目标域的性能。然而，这种方法只对齐了特征分布，没有考虑类别特征对齐，导致次优结果。半监督学习(SSL)作为一种解决方案，但受到模型生成的伪标签质量的限制。受领域适应理论基础[2]的启发，我们提出了一种新的 SSL 方法，选择领域特定的教师和学生网络的模型输出与未标记的目标数据不一致的目标样本，以提高目标领域的性能。在基准跨域 OTE 数据集上的大量实验表明，该方法是有效的，并且在具有较大域移位的情况下表现一致。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-training+through+Classifier+Disagreement+for+Cross-Domain+Opinion+Target+Extraction)|0|
|[Fast and Multi-aspect Mining of Complex Time-stamped Event Streams](https://doi.org/10.1145/3543507.3583370)|Kota Nakamura, Yasuko Matsubara, Koki Kawabata, Yuhei Umeda, Yuichiro Wada, Yasushi Sakurai|SANKEN, Osaka University, Japan; AI Lab., Fujitsu, Japan; AI Lab., Fujitsu, Japan and AIP, RIKEN, Japan|Given a huge, online stream of time-evolving events with multiple attributes, such as online shopping logs: (item, price, brand, time), and local mobility activities: (pick-up and drop-off locations, time), how can we summarize large, dynamic high-order tensor streams? How can we see any hidden patterns, rules, and anomalies? Our answer is to focus on two types of patterns, i.e., ''regimes'' and ''components'', for which we present CubeScope, an efficient and effective method over high-order tensor streams. Specifically, it identifies any sudden discontinuity and recognizes distinct dynamical patterns, ''regimes'' (e.g., weekday/weekend/holiday patterns). In each regime, it also performs multi-way summarization for all attributes (e.g., item, price, brand, and time) and discovers hidden ''components'' representing latent groups (e.g., item/brand groups) and their relationship. Thanks to its concise but effective summarization, CubeScope can also detect the sudden appearance of anomalies and identify the types of anomalies that occur in practice. Our proposed method has the following properties: (a) Effective: it captures dynamical multi-aspect patterns, i.e., regimes and components, and statistically summarizes all the events; (b) General: it is practical for successful application to data compression, pattern discovery, and anomaly detection on various types of tensor streams; (c) Scalable: our algorithm does not depend on the length of the data stream and its dimensionality. Extensive experiments on real datasets demonstrate that CubeScope finds meaningful patterns and anomalies correctly, and consistently outperforms the state-of-the-art methods as regards accuracy and execution speed.|给定一个巨大的，在线时间演化事件流，具有多种属性，如在线购物日志: (项目，价格，品牌，时间) ，和本地流动性活动: (上下车地点，时间) ，我们如何总结大型，动态高阶张量流？我们怎么才能看到隐藏的模式，规则和异常呢？我们的答案是关注两种类型的模式，即“体制”和“组件”，对于这两种模式，我们提出 CubeScope，一种高阶张量流上的高效和有效的方法。具体来说，它识别任何突然的不连续性，并识别不同的动态模式，“制度”(例如，工作日/周末/假日模式)。在每个体系中，它还对所有属性(例如，商品、价格、品牌和时间)进行多方面的总结，并发现代表潜在群体(例如，商品/品牌群体)及其关系的隐藏“组成部分”。由于其简洁而有效的总结，CubeScope 还可以检测异常的突然出现并识别实际发生的异常类型。我们提出的方法具有以下特点: (a)有效: 它捕获动态的多方面模式，即制度和组成部分，并对所有事件进行统计总结; (b)一般情况: 成功应用于各种类型的张量流的数据压缩、模式发现和异常检测是可行的; (c)可扩展性: 我们的算法不依赖于数据流的长度及其维度。在真实数据集上的大量实验表明，CubeScope 能够正确地发现有意义的模式和异常，并且在准确性和执行速度方面始终优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+and+Multi-aspect+Mining+of+Complex+Time-stamped+Event+Streams)|0|
|[PDSum: Prototype-driven Continuous Summarization of Evolving Multi-document Sets Stream](https://doi.org/10.1145/3543507.3583371)|Susik Yoon, Hou Pong Chan, Jiawei Han|University of Macau, Macao; University of Illinois at Urbana-Champaign, USA|Summarizing text-rich documents has been long studied in the literature, but most of the existing efforts have been made to summarize a static and predefined multi-document set. With the rapid development of online platforms for generating and distributing text-rich documents, there arises an urgent need for continuously summarizing dynamically evolving multi-document sets where the composition of documents and sets is changing over time. This is especially challenging as the summarization should be not only effective in incorporating relevant, novel, and distinctive information from each concurrent multi-document set, but also efficient in serving online applications. In this work, we propose a new summarization problem, Evolving Multi-Document sets stream Summarization (EMDS), and introduce a novel unsupervised algorithm PDSum with the idea of prototype-driven continuous summarization. PDSum builds a lightweight prototype of each multi-document set and exploits it to adapt to new documents while preserving accumulated knowledge from previous documents. To update new summaries, the most representative sentences for each multi-document set are extracted by measuring their similarities to the prototypes. A thorough evaluation with real multi-document sets streams demonstrates that PDSum outperforms state-of-the-art unsupervised multi-document summarization algorithms in EMDS in terms of relevance, novelty, and distinctiveness and is also robust to various evaluation settings.|总结文本丰富的文档在文献中已经研究了很长时间，但现有的大多数努力都是总结一个静态的和预定义的多文档集。随着生成和分发文本丰富的文件的在线平台的迅速发展，迫切需要不断总结动态演变的多文件集，其中文件和文件集的组成随着时间的推移而变化。这尤其具有挑战性，因为摘要不仅应该有效地合并每个并发多文档集中的相关、新颖和独特信息，而且还应该有效地为在线应用程序提供服务。本文提出了一个新的文摘问题——进化多文档集流文摘(EMDS) ，并引入了一种基于原型驱动的连续文摘思想的无监督算法 PDSum。PDSum 为每个多文档集构建一个轻量级原型，并利用它来适应新文档，同时保留以前文档中积累的知识。为了更新新的摘要，通过测量每个多文档集合与原型的相似性来提取最有代表性的句子。对真实多文档集合流的全面评估表明，PDSum 在相关性、新颖性和独特性方面优于 EMDS 中最先进的无监督多文档摘要算法，并且对各种评估设置也具有鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PDSum:+Prototype-driven+Continuous+Summarization+of+Evolving+Multi-document+Sets+Stream)|0|
|[Learning Disentangled Representation via Domain Adaptation for Dialogue Summarization](https://doi.org/10.1145/3543507.3583389)|Jinpeng Li, Yingce Xia, Xin Cheng, Dongyan Zhao, Rui Yan|Wangxuan Institute of Computer Technology, Peking University, China; Microsoft Research, China; Gaoling School of Artificial Intelligence, Renmin University of China, China; Wangxuan Institute of Computer Technology, Peking University, China and National Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence, China|Dialogue summarization, which aims to generate a summary for an input dialogue, plays a vital role in intelligent dialogue systems. The end-to-end models have achieved satisfactory performance in summarization, but the success is built upon enough annotated data, which is costly to obtain, especially in the dialogue summarization. To leverage the rich external data, previous works first pre-train the model on the other domain data (e.g., the news domain), and then fine-tune it directly on the dialogue domain. The data from different domains are equally treated during the training process, while the vast differences between dialogues (usually informal, repetitive, and with multiple speakers) and conventional articles (usually formal and concise) are neglected. In this work, we propose to use a disentangled representation method to reduce the deviation between data in different domains, where the input data is disentangled into domain-invariant and domain-specific representations. The domain-invariant representation carries context information that is supposed to be the same across domains (e.g., news, dialogue) and the domain-specific representation indicates the input data belongs to a particular domain. We use adversarial learning and contrastive learning to constrain the disentangled representations to the target space. Furthermore, we propose two novel reconstruction strategies, namely backtracked and cross-track reconstructions, which aim to reduce the domain characteristics of out-of-domain data and mitigate the domain bias of the model. Experimental results on three public datasets show that our model significantly outperforms the strong baselines.|对话摘要是智能对话系统中的一个重要组成部分，其目的是为输入对话生成摘要。端到端模型在摘要方面取得了令人满意的效果，但是这种成功是建立在足够的注释数据的基础上的，而这些注释数据的获取成本很高，尤其是在对话摘要方面。为了利用丰富的外部数据，以前的工作首先在其他领域数据(例如，新闻领域)上预训练模型，然后直接在对话领域进行微调。不同领域的数据在训练过程中被平等对待，而对话(通常是非正式的、重复的、多人使用的)和传统文章(通常是正式的和简洁的)之间的巨大差异被忽略。在这项工作中，我们提出使用一个分离的表示方法，以减少不同领域的数据之间的偏差，其中输入数据被分离成领域不变和领域特定的表示。领域不变表示携带的上下文信息应该是相同的跨领域(例如，新闻，对话)和领域特定的表示表明输入数据属于一个特定的领域。我们使用对抗学习和对比学习来约束离散表征到目标空间。此外，我们提出了两种新的重建策略，即回溯重建和交叉跟踪重建，旨在减少域外数据的域特征和减轻模型的域偏差。在三个公共数据集上的实验结果表明，我们的模型明显优于强基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Disentangled+Representation+via+Domain+Adaptation+for+Dialogue+Summarization)|0|
|[Towards Understanding Consumer Healthcare Questions on the Web with Semantically Enhanced Contrastive Learning](https://doi.org/10.1145/3543507.3583449)|Shweta Yadav, Stefan Cobeli, Cornelia Caragea|University of Illinois at Chicago, USA|In recent years, seeking health information on the web has become a preferred way for healthcare consumers to support their information needs. Generally, healthcare consumers use long and detailed questions with several peripheral details to express their healthcare concerns, contributing to natural language understanding challenges. One way to address this challenge is by summarizing the questions. However, most of the existing abstractive summarization systems generate impeccably fluent yet factually incorrect summaries. In this paper, we present a semantically-enhanced contrastive learning-based framework for generating abstractive question summaries that are faithful and factually correct. We devised multiple strategies based on question semantics to generate the erroneous (negative) summaries, such that the model has the understanding of plausible and incorrect perturbations of the original summary. Our extensive experimental results on two benchmark consumer health question summarization datasets confirm the effectiveness of our proposed method by achieving state-of-the-art performance and generating factually correct and fluent summaries, as measured by human evaluation.|近年来，在网上寻找健康信息已成为医疗保健消费者支持其信息需求的首选方式。一般来说，医疗保健消费者使用长而详细的问题和几个外围细节来表达他们的医疗保健关注，有助于自然语言理解的挑战。解决这个问题的一个方法是总结问题。然而，大多数现有的抽象摘要系统生成的摘要无可挑剔地流畅，但事实上是不正确的。本文提出了一个基于语义增强的对比学习框架，用于生成抽象的、忠实的、事实正确的问题摘要。我们设计了多种基于问题语义的策略来生成错误的(否定的)总结，使得模型能够理解原始总结的合理和不正确的扰动。我们在两个基准的消费者健康问题摘要数据集上的广泛实验结果证实了我们提出的方法的有效性，通过实现最先进的性能和生成事实上正确和流畅的摘要，由人类评估测量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Understanding+Consumer+Healthcare+Questions+on+the+Web+with+Semantically+Enhanced+Contrastive+Learning)|0|
|[Modeling Dynamic Interactions over Tensor Streams](https://doi.org/10.1145/3543507.3583458)|Koki Kawabata, Yasuko Matsubara, Yasushi Sakurai|SANKEN, Osaka University, Japan|Many web applications, such as search engines and social network services, are continuously producing a huge number of events with a multi-order tensor form, {count;query, location, …, timestamp}, and so how can we discover important trends to enables us to forecast long-term future events? Can we interpret any relationships between events that determine the trends from multi-aspect perspectives? Real-world online activities can be composed of (1) many time-changing interactions that control trends, for example, competition/cooperation to gain user attention, as well as (2) seasonal patterns that covers trends. To model the shifting trends via interactions, namely dynamic interactions over tensor streams, in this paper, we propose a streaming algorithm, DISMO, that we designed to discover Dynamic Interactions and Seasonality in a Multi-Order tensor. Our approach has the following properties. (a) Interpretable: it incorporates interpretable non-linear differential equations in tensor factorization so that it can reveal latent interactive relationships and thus generate future events effectively; (b) Dynamic: it can be aware of shifting trends by switching multi-aspect factors while summarizing their characteristics incrementally; and (c) Automatic: it finds every factor automatically without losing forecasting accuracy. Extensive experiments on real datasets demonstrate that our algorithm extracts interpretable interactions between data attributes, while simultaneously providing improved forecasting accuracy and a great reduction in computational time.|许多网络应用程序，如搜索引擎和社交网络服务，不断产生大量的事件与多阶张量形式，{ count; query，location，... ，time戳} ，所以我们如何发现重要的趋势，使我们能够预测长期的未来事件？我们能否从多方面的角度解释事件之间确定趋势的任何关系？现实世界的在线活动可以包括(1)许多时间变化的交互，控制趋势，例如，竞争/合作，以获得用户的关注，以及(2)涵盖趋势的季节性模式。为了通过张量流的动态相互作用(即动态相互作用)来模拟变化趋势，在本文中，我们提出了一个流算法，DISMO，我们设计它来发现多阶张量中的动态相互作用和季节性。我们的方法具有以下属性。(a)可解释性: 在张量因子分解中加入可解释的非线性微分方程，以便能够揭示潜在的交互关系，从而有效地产生未来事件; (b)动态性: 通过切换多方面因子，逐步总结其特征，它可以意识到变化的趋势; (c)自动性: 它自动找到每个因子，而不会失去预测的准确性。在实际数据集上的大量实验表明，该算法提取了数据属性之间可解释的交互，同时提高了预测精度，大大减少了计算时间。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+Dynamic+Interactions+over+Tensor+Streams)|0|
|[Constrained Subset Selection from Data Streams for Profit Maximization](https://doi.org/10.1145/3543507.3583490)|Shuang Cui, Kai Han, Jing Tang, He Huang|School of Computer Science and Technology, Soochow University, China; The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology, China; School of Computer Science and Technology, University of Science and Technology of China, China|The problem of constrained subset selection from a large data stream for profit maximization has many applications in web data mining and machine learning, such as social advertising, team formation and recommendation systems. Such a problem can be formulated as maximizing a regularized submodular function under certain constraints. In this paper, we consider a generalized k-system constraint, which captures various requirements in real-world applications. For this problem, we propose the first streaming algorithm with provable performance bounds, leveraging a novel multitudinous distorted filter framework. The empirical performance of our algorithm is extensively evaluated in several applications including web data mining and recommendation systems, and the experimental results demonstrate the superiorities of our algorithm in terms of both effectiveness and efficiency.|面向利润最大化的大型数据流的约束子集选择问题在网络数据挖掘和机器学习中有许多应用，例如社交广告、团队组建和推荐系统。这类问题可以表述为在一定约束条件下正则化子模函数的最大化问题。在本文中，我们考虑了一个广义 k 系统约束，它能够满足实际应用中的各种需求。针对这个问题，我们提出了第一个具有可证明性能界限的流式算法，利用了一个新的大量失真过滤器框架。该算法的经验性能在网络数据挖掘和推荐系统等应用中得到了广泛的评价，实验结果表明了该算法在效率和效果方面的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Constrained+Subset+Selection+from+Data+Streams+for+Profit+Maximization)|0|
|[SCStory: Self-supervised and Continual Online Story Discovery](https://doi.org/10.1145/3543507.3583507)|Susik Yoon, Yu Meng, Dongha Lee, Jiawei Han|Yonsei University, Republic of Korea; University of Illinois at Urbana-Champaign, USA|We present a framework SCStory for online story discovery, that helps people digest rapidly published news article streams in real-time without human annotations. To organize news article streams into stories, existing approaches directly encode the articles and cluster them based on representation similarity. However, these methods yield noisy and inaccurate story discovery results because the generic article embeddings do not effectively reflect the story-indicative semantics in an article and cannot adapt to the rapidly evolving news article streams. SCStory employs self-supervised and continual learning with a novel idea of story-indicative adaptive modeling of news article streams. With a lightweight hierarchical embedding module that first learns sentence representations and then article representations, SCStory identifies story-relevant information of news articles and uses them to discover stories. The embedding module is continuously updated to adapt to evolving news streams with a contrastive learning objective, backed up by two unique techniques, confidence-aware memory replay and prioritized-augmentation, employed for label absence and data scarcity problems. Thorough experiments on real and the latest news data sets demonstrate that SCStory outperforms existing state-of-the-art algorithms for unsupervised online story discovery.|我们提出了一个用于在线故事发现的框架 SCStory，它可以帮助人们在没有人工注释的情况下实时消化快速发布的新闻文章流。为了将新闻文章流组织成故事，现有的方法直接对文章进行编码，并根据表示相似性对文章进行聚类。然而，这些方法产生的噪音和不准确的故事发现结果，因为通用文章嵌入不能有效地反映故事指示语义在一篇文章，不能适应迅速发展的新闻文章流。SCStory 采用自我监督和持续学习的思想，对新闻文章流进行故事指示性自适应建模。SCStory 使用一个轻量级的层次化嵌入模块，首先学习句子表示，然后学习文章表示，SCStory 识别新闻文章的故事相关信息，并使用它们来发现故事。嵌入模块不断更新，以适应具有对比学习目标的不断发展的新闻流，并辅以两种独特的技术: 可信度感知记忆重放和优先级增强，用于解决标签缺失和数据稀缺问题。对真实新闻和最新新闻数据集的彻底实验表明，SCStory 在无监督的在线故事发现方面优于现有的最先进算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SCStory:+Self-supervised+and+Continual+Online+Story+Discovery)|0|
|[Know Your Transactions: Real-time and Generic Transaction Semantic Representation on Blockchain & Web3 Ecosystem](https://doi.org/10.1145/3543507.3583537)|Zhiying Wu, Jieli Liu, Jiajing Wu, Zibin Zheng, Xiapu Luo, Ting Chen|Hong Kong Polytechnic University, China; University of Electronic Science and Technologyo of China, China; Sun Yat-sen University, China|Web3, based on blockchain technology, is the evolving next generation Internet of value. Massive active applications on Web3, e.g. DeFi and NFT, usually rely on blockchain transactions to achieve value transfer as well as complex and diverse custom logic and intentions. Various risky or illegal behaviors such as financial fraud, hacking, money laundering are currently rampant in the blockchain ecosystem, and it is thus important to understand the intent behind the pseudonymous transactions. To reveal the intent of transactions, much effort has been devoted to extracting some particular transaction semantics through specific expert experiences. However, the limitations of existing methods in terms of effectiveness and generalization make it difficult to extract diverse transaction semantics in the rapidly growing and evolving Web3 ecosystem. In this paper, we propose the Motif-based Transaction Semantics representation method (MoTS), which can capture the transaction semantic information in the real-time transaction data workflow. To the best of our knowledge, MoTS is the first general semantic extraction method in Web3 blockchain ecosystem. Experimental results show that MoTS can effectively distinguish different transaction semantics in real-time, and can be used for various downstream tasks, giving new insights to understand the Web3 blockchain ecosystem. Our codes are available at https://github.com/wuzhy1ng/MoTS.|基于区块链技术的 Web3是不断发展的下一代价值互联网。Web3上的大量活动应用程序，例如 DeFi 和 NFT，通常依赖区块链事务来实现价值转移以及复杂多样的定制逻辑和意图。各种风险或非法行为，如金融欺诈，黑客，洗钱目前在区块链生态系统中猖獗，因此了解这些假名交易背后的意图非常重要。为了揭示事务的意图，人们花费了大量的精力通过特定的专家经验提取特定的事务语义。然而，现有方法在有效性和泛化方面的局限性使得在快速增长和发展的 Web3生态系统中很难提取不同的事务语义。在本文中，我们提出了基于主题的事务语义表示方法(moTS) ，它可以捕获实时事务数据流中的事务语义信息。据我们所知，MoTS 是 Web3区块链生态系统中第一个通用的语义提取方法。实验结果表明，MoTS 可以有效地实时区分不同的事务语义，并可用于各种下游任务，为理解 Web3区块链生态系统提供了新的视角。我们的密码可以在 https://github.com/wuzhy1ng/mots 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Know+Your+Transactions:+Real-time+and+Generic+Transaction+Semantic+Representation+on+Blockchain+&+Web3+Ecosystem)|0|
|[Toward Open-domain Slot Filling via Self-supervised Co-training](https://doi.org/10.1145/3543507.3583541)|Adib Mosharrof, Moghis Fereidouni, A. B. Siddique|University of Kentucky, USA|Slot filling is one of the critical tasks in modern conversational systems. The majority of existing literature employs supervised learning methods, which require labeled training data for each new domain. Zero-shot learning and weak supervision approaches, among others, have shown promise as alternatives to manual labeling. Nonetheless, these learning paradigms are significantly inferior to supervised learning approaches in terms of performance. To minimize this performance gap and demonstrate the possibility of open-domain slot filling, we propose a Self-supervised Co-training framework, called SCot, that requires zero in-domain manually labeled training examples and works in three phases. Phase one acquires two sets of complementary pseudo labels automatically. Phase two leverages the power of the pre-trained language model BERT, by adapting it for the slot filling task using these sets of pseudo labels. In phase three, we introduce a self-supervised cotraining mechanism, where both models automatically select highconfidence soft labels to further improve the performance of the other in an iterative fashion. Our thorough evaluations show that SCot outperforms state-of-the-art models by 45.57% and 37.56% on SGD and MultiWoZ datasets, respectively. Moreover, our proposed framework SCot achieves comparable performance when compared to state-of-the-art fully supervised models.|时隙填充是现代会话系统的关键任务之一。现有的大多数文献采用监督式学习方法，每个新领域都需要有标签的训练数据。零拍摄学习和薄弱的监督方法，除其他外，已经显示出作为手工标签替代品的前景。尽管如此，这些学习模式在表现方面明显逊色于监督式学习方法。为了尽可能减小这种性能差距，并证明开放域时隙填充的可能性，我们提出了一种称为 SCot 的自监督协同训练框架，它需要零域内手动标记的训练例子，并分三个阶段工作。第一阶段自动获取两套互补的伪标签。第二阶段利用预先训练好的语言模型 BERT 的能力，通过使用这些伪标签集使其适应插槽填充任务。在第三阶段，我们引入一个自我监督的协同训练机制，其中两个模型自动选择高置信软标签，以进一步提高其他在迭代方式的性能。我们的全面评估表明，在 SGD 和 MultiWoZ 数据集上，SCot 的性能分别比最先进的模型高出45.57% 和37.56% 。此外，我们提出的框架 SCot 实现了可比的性能相比，国家的最先进的全监督模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Toward+Open-domain+Slot+Filling+via+Self-supervised+Co-training)|0|
|[Measuring and Evading Turkmenistan's Internet Censorship: A Case Study in Large-Scale Measurements of a Low-Penetration Country](https://doi.org/10.1145/3543507.3583189)|Sadia Nourin, Van Tran, Xi Jiang, Kevin Bock, Nick Feamster, Nguyen Phong Hoang, Dave Levin|University of Chicago, USA; University of Maryland, USA|Since 2006, Turkmenistan has been listed as one of the few Internet enemies by Reporters without Borders due to its extensively censored Internet and strictly regulated information control policies. Existing reports of filtering in Turkmenistan rely on a small number of vantage points or test a small number of websites. Yet, the country's poor Internet adoption rates and small population can make more comprehensive measurement challenging. With a population of only six million people and an Internet penetration rate of only 38%, it is challenging to either recruit in-country volunteers or obtain vantage points to conduct remote network measurements at scale. We present the largest measurement study to date of Turkmenistan's Web censorship. To do so, we developed TMC, which tests the blocking status of millions of domains across the three foundational protocols of the Web (DNS, HTTP, and HTTPS). Importantly, TMC does not require access to vantage points in the country. We apply TMC to 15.5M domains, our results reveal that Turkmenistan censors more than 122K domains, using different blocklists for each protocol. We also reverse-engineer these censored domains, identifying 6K over-blocking rules causing incidental filtering of more than 5.4M domains. Finally, we use Geneva, an open-source censorship evasion tool, to discover five new censorship evasion strategies that can defeat Turkmenistan's censorship at both transport and application layers. We will publicly release both the data collected by TMC and the code for censorship evasion.|自2006年以来，土库曼斯坦由于其广泛的互联网审查和严格的信息控制政策而被列为少数几个互联网无国界记者之一。土库曼斯坦现有的过滤报告依赖于少数有利位置或测试少数网站。然而，该国糟糕的互联网使用率和较小的人口可以使更全面的测量具有挑战性。由于人口只有600万，互联网普及率只有38% ，因此，要么征聘国内志愿人员，要么获得有利位置，进行大规模的远程网络测量，这是一个挑战。我们提出了迄今为止土库曼斯坦网络审查最大的测量研究。为此，我们开发了 TMC，它测试跨 Web 的三个基本协议(DNS、 HTTP 和 HTTPS)的数百万个域的阻塞状态。重要的是，TMC 不要求进入该国的有利位置。我们将 TMC 应用于1550万个域名，我们的结果显示，土库曼斯坦审查超过122K 域名，使用不同的区块列表为每个协议。我们还反向工程这些删除域，确定6K 过度阻塞规则导致超过5.4 M 域的附带过滤。最后，我们使用日内瓦，一个开源的审查规避工具，发现五个新的审查规避策略，可以击败土库曼斯坦的审查在传输和应用层。我们将公开发布 TMC 收集的数据和规避审查的代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Measuring+and+Evading+Turkmenistan's+Internet+Censorship:+A+Case+Study+in+Large-Scale+Measurements+of+a+Low-Penetration+Country)|0|
|[NetGuard: Protecting Commercial Web APIs from Model Inversion Attacks using GAN-generated Fake Samples](https://doi.org/10.1145/3543507.3583224)|Xueluan Gong, Ziyao Wang, Yanjiao Chen, Qian Wang, Cong Wang, Chao Shen|Xi'an Jiaotong University, China; City University of Hong Kong, China; Wuhan University, China; Zhejiang University, China|Recently more and more cloud service providers (e.g., Microsoft, Google, and Amazon) have commercialized their well-trained deep learning models by providing limited access via web API interfaces. However, it is shown that these APIs are susceptible to model inversion attacks, where attackers can recover the training data with high fidelity, which may cause serious privacy leakage.Existing defenses against model inversion attacks, however, hinder the model performance and are ineffective for more advanced attacks, e.g., Mirror [4]. In this paper, we proposed NetGuard, a novel utility-aware defense methodology against model inversion attacks (MIAs). Unlike previous works that perturb prediction outputs of the victim model, we propose to mislead the MIA effort by inserting engineered fake samples during the training process. A generative adversarial network (GAN) is carefully built to construct fake training samples to mislead the attack model without degrading the performance of the victim model. Besides, we adopt continual learning to further improve the utility of the victim model. Extensive experiments on CelebA, VGG-Face, and VGG-Face2 datasets show that NetGuard is superior to existing defenses, including DP [37] and Ad-mi [32] on state-of-the-art model inversion attacks, i.e., DMI [8], Mirror [4], Privacy [12], and Alignment [34].|最近越来越多的云服务提供商(如微软、谷歌和亚马逊)通过提供有限的 Web API 接口访问，将他们训练有素的深度学习模型商业化。然而，这些 API 容易受到模型反转攻击，攻击者可以恢复高保真的训练数据，从而导致严重的隐私泄漏。然而，针对模型反转攻击的现有防御措施阻碍了模型的性能，并且对于更高级的攻击无效，例如，Mirror [4]。在本文中，我们提出了一种新的效用感知的防御模型反转攻击(MIA)方法 NetGuard。不同于以往的工作，扰乱预测输出的受害者模型，我们建议误导 MIA 的努力，插入工程假样本在训练过程中。构造一个生成式对抗网络(GAN)来构造虚假的训练样本，在不降低受害者模型性能的前提下误导攻击模型。此外，我们采用不断学习的方法进一步提高了被害人模型的有效性。在 CelebA，VGG-Face 和 VGG-Face2数据集上的大量实验表明，NetGuard 优于现有的防御系统，包括 DP [37]和 Ad-mi [32]对最先进的模型反转攻击，即 DMI [8] ，Mirror [4] ，Privacy [12]和 Align [34]。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NetGuard:+Protecting+Commercial+Web+APIs+from+Model+Inversion+Attacks+using+GAN-generated+Fake+Samples)|0|
|[Meteor: Improved Secure 3-Party Neural Network Inference with Reducing Online Communication Costs](https://doi.org/10.1145/3543507.3583272)|Ye Dong, Xiaojun Chen, Weizhan Jing, Kaiyun Li, Weiping Wang|; Institute of Information Engineering, Chinese Academy of Sciences, China and School of Cyber Security, University of Chinese Academy of Sciences, China; Institute of Information Engineering,Chinese Academy of Sciences, China and School of Cyber Security, University of Chinese Academy of Sciences, China|Secure neural network inference has been a promising solution to private Deep-Learning-as-a-Service, which enables the service provider and user to execute neural network inference without revealing their private inputs. However, the expensive overhead of current schemes is still an obstacle when applied in real applications. In this work, we present Meteor, an online communication-efficient and fast secure 3-party computation neural network inference system aginst semi-honest adversary in honest-majority. The main contributions of Meteor are two-fold: i) We propose a new and improved 3-party secret sharing scheme stemming from the linearity of replicated secret sharing, and design efficient protocols for the basic cryptographic primitives, including linear operations, multiplication, most significant bit extraction, and multiplexer. ii) Furthermore, we build efficient and secure blocks for the widely used neural network operators such as Matrix Multiplication, ReLU, and Maxpool, along with exploiting several specific optimizations for better efficiency. Our total communication with the setup phase is a little larger than SecureNN (PoPETs’19) and Falcon (PoPETs’21), two state-of-the-art solutions, but the gap is not significant when the online phase must be optimized as a priority. Using Meteor, we perform extensive evaluations on various neural networks. Compared to SecureNN and Falcon, we reduce the online communication costs by up to 25.6 × and 1.5 ×, and improve the running-time by at most 9.8 × (resp. 8.1 ×) and 1.5 × (resp. 2.1 ×) in LAN (resp. WAN) for the online inference.|安全神经网络推理已成为私有深度学习即服务(Deep-Learning-as-a-Service)的一种有前途的解决方案，它使服务提供者和用户能够在不暴露其私有输入的情况下执行神经网络推理。然而，在实际应用中，现有方案昂贵的开销仍然是一个障碍。在这项工作中，我们提出了一个在线通信效率和快速安全的三方计算神经网络推理系统对半诚实的对手在诚实的大多数。基于复制密钥共享的线性特性，提出了一种新的改进的三方密钥共享方案，并针对基本密码原语设计了高效的协议，包括线性运算、乘法、最大位提取和多路复用。Ii)此外，我们还为广泛使用的神经网络操作器(如矩阵乘法、 relU 和 Maxpool)构建了高效、安全的模块，同时利用一些特定的优化来提高效率。我们与安装阶段的总体沟通略大于 SecureNN (PoPETs’19)和 Falcon (PoPETs’21)这两种最先进的解决方案，但当在线阶段必须优先优化时，差距并不显着。使用流星，我们对各种神经网络进行广泛的评估。与 SecureNN 和 Falcon 相比，在线通信成本分别降低了25.6 × 和1.5 × ，运行时间最多提高了9.8 × 。8.1 ×)和1.5 × (分辨率)。2.1 ×).WAN)进行在线推理。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Meteor:+Improved+Secure+3-Party+Neural+Network+Inference+with+Reducing+Online+Communication+Costs)|0|
|[IRWArt: Levering Watermarking Performance for Protecting High-quality Artwork Images](https://doi.org/10.1145/3543507.3583489)|Yuanjing Luo, Tongqing Zhou, Fang Liu, Zhiping Cai|National University of Defense Technology, China; Hunan University, China|Increasing artwork plagiarism incidents underscores the urgent need for reliable copyright protection for high-quality artwork images. Although watermarking is helpful to this issue, existing methods are limited in imperceptibility and robustness. To provide high-level protection for valuable artwork images, we propose a novel invisible robust watermarking framework, dubbed as IRWArt. In our architecture, the embedding and recovery of the watermark are treated as a pair of image transformations’ inverse problems, and can be implemented through the forward and backward processes of an invertible neural networks (INN), respectively. For high visual quality, we embed the watermark in high-frequency domains with minimal impact on artwork and supervise image reconstruction using a human visual system(HVS)-consistent deep perceptual loss. For strong plagiarism-resistant, we construct a quality enhancement module for the embedded image against possible distortions caused by plagiarism actions. Moreover, the two-stagecontrastive training strategy enables the simultaneous realization of the above two goals. Experimental results on 4 datasets demonstrate the superiority of our IRWArt over other state-of-the-art watermarking methods. Code: https://github.com/1024yy/IRWArt.|越来越多的艺术品剽窃事件突出表明，迫切需要为高质量的艺术品图像提供可靠的版权保护。虽然水印技术有助于解决这一问题，但现有的水印方法在不可感知性和鲁棒性方面存在局限性。为了对有价值的艺术品图像提供高层次的保护，我们提出了一种新的不可见的鲁棒水印框架，称为 IRWArt。在我们的体系结构中，水印的嵌入和恢复被视为一对图像变换的逆问题，可以分别通过可逆神经网络(INN)的正向和反向过程来实现。为了提高视觉质量，我们将水印嵌入到高频域中，尽量减少对作品的影响，并使用人类视觉系统(HVS)一致的深度感知损失来监督图像重建。为了提高嵌入图像的抗剽窃能力，我们构建了一个质量增强模块，用于对抗剽窃行为可能造成的图像失真。此外，两阶段对比训练策略可以同时实现上述两个目标。在4个数据集上的实验结果表明了我们的 IRWArt 相对于其他最先进的水印方法的优越性。密码:  https://github.com/1024yy/irwart。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IRWArt:+Levering+Watermarking+Performance+for+Protecting+High-quality+Artwork+Images)|0|
|[CapEnrich: Enriching Caption Semantics for Web Images via Cross-modal Pre-trained Knowledge](https://doi.org/10.1145/3543507.3583232)|Linli Yao, Weijing Chen, Qin Jin|School of Information, Renmin University of China, China|Automatically generating textual descriptions for massive unlabeled images on the web can greatly benefit realistic web applications, e.g. multimodal retrieval and recommendation. However, existing models suffer from the problem of generating ``over-generic'' descriptions, such as their tendency to generate repetitive sentences with common concepts for different images. These generic descriptions fail to provide sufficient textual semantics for ever-changing web images. Inspired by the recent success of Vision-Language Pre-training (VLP) models that learn diverse image-text concept alignment during pretraining, we explore leveraging their cross-modal pre-trained knowledge to automatically enrich the textual semantics of image descriptions. With no need for additional human annotations, we propose a plug-and-play framework, i.e CapEnrich, to complement the generic image descriptions with more semantic details. Specifically, we first propose an automatic data-building strategy to get desired training sentences, based on which we then adopt prompting strategies, i.e. learnable and template prompts, to incentivize VLP models to generate more textual details. For learnable templates, we fix the whole VLP model and only tune the prompt vectors, which leads to two advantages: 1) the pre-training knowledge of VLP models can be reserved as much as possible to describe diverse visual concepts; 2) only lightweight trainable parameters are required, so it is friendly to low data resources. Extensive experiments show that our method significantly improves the descriptiveness and diversity of generated sentences for web images. The code is available at https://github.com/yaolinli/CapEnrich.|自动生成文本描述的大量未标记的图像在网络上可以大大有益于现实的网络应用程序，例如多模式检索和推荐。然而，现有的模型存在“过度泛型”描述的问题，例如它们倾向于为不同的图像生成具有共同概念的重复句子。这些通用的描述无法为不断变化的网络图像提供足够的文本语义。受最近成功的视觉语言预训练(VLP)模型的启发，我们在预训练过程中学习了不同的图像-文本概念对齐，我们探索利用它们的跨模式预训练知识来自动丰富图像描述的文本语义。由于不需要额外的人工注释，我们提出了一个即插即用的框架，即 CapEnrich，用更多的语义细节来补充通用图像描述。具体来说，我们首先提出一个自动的数据建立策略，以获得所需的训练句子，然后在此基础上，我们采用提示策略，即可学习和模板提示，以激励 VLP 模型生成更多的文本细节。对于可学习的模板，我们修正了整个 VLP 模型，只对提示向量进行调整，这带来了两个好处: 1) VLP 模型的预训练知识可以尽可能地保留，以描述不同的视觉概念; 2)只需要轻量级的可训练参数，因此对低资源量的数据是友好的。大量实验表明，该方法显著提高了网络图像生成句子的描述性和多样性。密码可在 https://github.com/yaolinli/capenrich 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CapEnrich:+Enriching+Caption+Semantics+for+Web+Images+via+Cross-modal+Pre-trained+Knowledge)|0|
|[MLN4KB: an efficient Markov logic network engine for large-scale knowledge bases and structured logic rules](https://doi.org/10.1145/3543507.3583248)|Huang Fang, Yang Liu, Yunfeng Cai, Mingming Sun|Baidu, China|Markov logic network (MLN) is a powerful statistical modeling framework for probabilistic logic reasoning. Despite the elegancy and effectiveness of MLN, the inference of MLN is known to suffer from an efficiency issue. Even the state-of-the-art MLN engines can not scale to medium-size real-world knowledge bases in the open-world setting, i.e., all unobserved facts in the knowledge base need predictions. In this work, by focusing on a certain class of first-order logic rules that are sufficiently expressive, we develop a highly efficient MLN inference engine called MLN4KB that can leverage the sparsity of knowledge bases. MLN4KB enjoys quite strong theoretical properties; its space and time complexities can be exponentially smaller than existing MLN engines. Experiments on both synthetic and real-world knowledge bases demonstrate the effectiveness of the proposed method. MLN4KB is orders of magnitudes faster (more than 103 times faster on some datasets) than existing MLN engines in the open-world setting. Without any approximation tricks, MLN4KB can scale to real-world knowledge bases including WN-18 and YAGO3-10 and achieve decent prediction accuracy without bells and whistles. We implement MLN4KB as a Julia package called MLN4KB.jl. The package supports both maximum a posteriori (MAP) inference and learning the weights of rules. MLN4KB.jl is public available at https://github.com/baidu-research/MLN4KB .|马尔可夫逻辑网络(MLN)是一个强大的统计建模框架，用于概率逻辑推理。尽管 MLN 具有优雅性和有效性，但 MLN 的推理却存在效率问题。即使是最先进的 MLN 引擎也无法在开放世界环境下扩展到中等规模的现实世界知识库，即知识库中所有未观察到的事实都需要预测。在这项工作中，通过关注一类具有足够表现力的一阶逻辑规则，我们开发了一种高效的 MLN 推理机，称为 MLN4kB，它可以利用稀缺的知识库。MLN4KB 具有很强的理论性能，其空间和时间复杂度可以指数小于现有的 MLN 发动机。在综合知识库和现实知识库上的实验表明了该方法的有效性。MLN4KB 比开放世界中现有的 MLN 引擎快几个数量级(在一些数据集上快103倍以上)。在没有任何近似技巧的情况下，MLN4KB 可以扩展到包括 WN-18和 YAGO3-10在内的真实世界的知识库，并且不需要花哨的功夫就能获得相当高的预测精度。我们将 MLN4KB 实现为一个名为 MLN4KB.jl 的 Julia 包。该软件包支持最大后验(MAP)推理和学习规则的权重。MLn4kB.jl 可于 https://github.com/baidu-research/mln4kb 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MLN4KB:+an+efficient+Markov+logic+network+engine+for+large-scale+knowledge+bases+and+structured+logic+rules)|0|
|[Learning Long- and Short-term Representations for Temporal Knowledge Graph Reasoning](https://doi.org/10.1145/3543507.3583242)|Mengqi Zhang, Yuwei Xia, Qiang Liu, Shu Wu, Liang Wang|School of Artificial Intelligence, University of Chinese Academy of Sciences, China and CRIPAC,MAIS, Institute of Automation, Chinese Academy of Sciences, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, China and CRIPAC, MAIS, Institute of Automation, Chinese Academy of Sciences, China; School of Cyber Security, University of Chinese Academy of Sciences, China and Institute of Information Engineering, Chinese Academy of Sciences, China|Temporal Knowledge graph (TKG) reasoning aims to predict missing facts based on historical TKG data. Most of the existing methods are incapable of explicitly modeling the long-term time dependencies from history and neglect the adaptive integration of the long- and short-term information. To tackle these problems, we propose a novel method that utilizes a designed Hierarchical Relational Graph Neural Network to learn the Long- and Short-term representations for TKG reasoning, namely HGLS. Specifically, to explicitly associate entities in different timestamps, we first transform the TKG into a global graph. Based on the built graph, we design a Hierarchical Relational Graph Neural Network that executes in two levels: The sub-graph level is to capture the semantic dependencies within concurrent facts of each KG. And the global-graph level aims to model the temporal dependencies between entities. Furthermore, we design a module to extract the long- and short-term information from the output of these two levels. Finally, the long- and short-term representations are fused into a unified one by Gating Integration for entity prediction. Extensive experiments on four datasets demonstrate the effectiveness of HGLS.|时态知识图(TKG)推理的目的是根据历史 TKG 数据预测缺失事实。现有的方法大多不能从历史上明确地建立长期时间依赖关系模型，忽视了长期和短期信息的自适应集成。为了解决这些问题，我们提出了一种利用所设计的层次关系图神经网络来学习 TKG 推理的长期和短期表示的新方法，即 HGLS。具体来说，为了显式地关联不同时间戳中的实体，我们首先将 TKG 转换为一个全局图。基于构建的图，我们设计了一个分层关系图神经网络，该网络在两个层次上执行: 子图层次是在每个 KG 的并发事实中捕获语义依赖。全局图层次的目标是建立实体之间的时间依赖关系模型。此外，我们还设计了一个模块，从这两个级别的输出中提取长期和短期信息。最后，通过门限积分将长期和短期表示融合为一个统一的表示，用于实体预测。在四个数据集上的大量实验证明了 HGLS 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Long-+and+Short-term+Representations+for+Temporal+Knowledge+Graph+Reasoning)|0|
|[Mutually-paced Knowledge Distillation for Cross-lingual Temporal Knowledge Graph Reasoning](https://doi.org/10.1145/3543507.3583407)|Ruijie Wang, Zheng Li, Jingfeng Yang, Tianyu Cao, Chao Zhang, Bing Yin, Tarek F. Abdelzaher|Amazon.com Inc, USA; School of Computational Science and Engineering, Georgia Institute of Technology, USA; Department of Computer Science, University of Illinois Urbana-Champaign, USA|This paper investigates cross-lingual temporal knowledge graph reasoning problem, which aims to facilitate reasoning on Temporal Knowledge Graphs (TKGs) in low-resource languages by transfering knowledge from TKGs in high-resource ones. The cross-lingual distillation ability across TKGs becomes increasingly crucial, in light of the unsatisfying performance of existing reasoning methods on those severely incomplete TKGs, especially in low-resource languages. However, it poses tremendous challenges in two aspects. First, the cross-lingual alignments, which serve as bridges for knowledge transfer, are usually too scarce to transfer sufficient knowledge between two TKGs. Second, temporal knowledge discrepancy of the aligned entities, especially when alignments are unreliable, can mislead the knowledge distillation process. We correspondingly propose a mutually-paced knowledge distillation model MP-KD, where a teacher network trained on a source TKG can guide the training of a student network on target TKGs with an alignment module. Concretely, to deal with the scarcity issue, MP-KD generates pseudo alignments between TKGs based on the temporal information extracted by our representation module. To maximize the efficacy of knowledge transfer and control the noise caused by the temporal knowledge discrepancy, we enhance MP-KD with a temporal cross-lingual attention mechanism to dynamically estimate the alignment strength. The two procedures are mutually paced along with model training. Extensive experiments on twelve cross-lingual TKG transfer tasks in the EventKG benchmark demonstrate the effectiveness of the proposed MP-KD method.|本文研究了跨语言时态知识图的推理问题，目的是通过将知识从低资源语言的时态知识图中转移到高资源语言的时态知识图中来实现对低资源语言的时态知识图的推理。由于现有的推理方法对于严重不完备的 TKG，特别是在资源不足的 TKG 中的推理效果不理想，跨 TKG 的跨语言精馏能力变得越来越重要。然而，它在两个方面提出了巨大的挑战。首先，作为知识转移桥梁的跨语言对齐通常过于稀缺，无法在两个 TKG 之间转移足够的知识。其次，排列实体的时间知识差异，特别是排列不可靠时，会误导知识提取过程。相应地，我们提出了一个相互节奏的知识提取模型 MP-KD，其中一个在源 TKG 上训练的教师网络可以用一个对齐模块来指导学生网络在目标 TKG 上的训练。具体来说，MP-KD 基于表示模块提取的时间信息生成 TKG 之间的伪对齐，以解决缺失问题。为了最大限度地提高知识转移的效率，控制时间知识差异带来的噪声，我们采用时间跨语言注意机制增强 MP-KD，动态估计对齐强度。这两个程序与模型训练是相互配合的。通过对 EventKG 基准中12个跨语言 TKG 传递任务的大量实验，证明了所提出的 MP-KD 方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mutually-paced+Knowledge+Distillation+for+Cross-lingual+Temporal+Knowledge+Graph+Reasoning)|0|
|[Large-Scale Analysis of New Employee Network Dynamics](https://doi.org/10.1145/3543507.3583400)|Yulin Yu, Longqi Yang, Siân Lindley, Mengting Wan|School of Information, University of Michigan, USA; Microsoft Research, United Kingdom; Microsoft, USA|The COVID-19 pandemic has accelerated digital transformations across industries, but also introduced new challenges into workplaces, including the difficulties of effectively socializing with colleagues when working remotely. This challenge is exacerbated for new employees who need to develop workplace networks from the outset. In this paper, by analyzing a large-scale telemetry dataset of more than 10,000 Microsoft employees who joined the company in the first three months of 2022, we describe how new employees interact and telecommute with their colleagues during their ``onboarding'' period. Our results reveal that although new hires are gradually expanding networks over time, there still exists significant gaps between their network statistics and those of tenured employees even after the six-month onboarding phase. We also observe that heterogeneity exists among new employees in how their networks change over time, where employees whose job tasks do not necessarily require extensive and diverse connections could be at a disadvantaged position in this onboarding process. By investigating how web-based people recommendations in organizational knowledge base facilitate new employees naturally expand their networks, we also demonstrate the potential of web-based applications for addressing the aforementioned socialization challenges. Altogether, our findings provide insights on new employee network dynamics in remote and hybrid work environments, which may help guide organizational leaders and web application developers on quantifying and improving the socialization experiences of new employees in digital workplaces.|2019冠状病毒疾病疫情加速了各行各业的数字化转型，但也给工作场所带来了新的挑战，包括在远程工作时难以有效地与同事进行社交。对于从一开始就需要发展工作场所网络的新员工来说，这一挑战更加严峻。在这篇论文中，我们通过分析一个大规模的遥测数据集，这个数据集包含了2022年前三个月加入微软的10,000多名员工，我们描述了新员工在“入职”期间是如何与他们的同事进行互动和远程办公的。我们的研究结果表明，尽管新员工的人际网络随着时间的推移逐渐扩大，但即使在六个月的入职阶段之后，他们的人际网络统计数据与终身雇员之间仍然存在显著差距。我们还观察到，新员工的网络随着时间的推移如何变化存在异质性，其工作任务不一定需要广泛和多样化的联系的员工可能在这一入职过程中处于不利地位。通过调查组织知识库中基于网络的人员推荐如何促进新员工自然地扩展他们的网络，我们也展示了基于网络的应用程序在解决上述社会化挑战方面的潜力。总之，我们的研究结果提供了远程和混合工作环境中新员工网络动态的见解，这可能有助于指导组织领导者和网络应用程序开发人员量化和改善新员工在数字工作场所的社会化经验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large-Scale+Analysis+of+New+Employee+Network+Dynamics)|0|
|[MassNE: Exploring Higher-Order Interactions with Marginal Effect for Massive Battle Outcome Prediction](https://doi.org/10.1145/3543507.3583390)|Yin Gu, Kai Zhang, Qi Liu, Xin Lin, Zhenya Huang, Enhong Chen|Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, China; Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, China and Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, China|In online games, predicting massive battle outcomes is a fundamental task of many applications, such as team optimization and tactical formulation. Existing works do not pay adequate attention to the massive battle. They either seek to evaluate individuals in isolation or mine simple pair-wise interactions between individuals, neither of which effectively captures the intricate interactions between massive units (e.g., individuals). Furthermore, as the team size increases, the phenomenon of diminishing marginal utility of units emerges. Such a diminishing pattern is rarely noticed in previous work, and how to capture it from data remains a challenge. To this end, we propose a novel Massive battle outcome predictor with margiNal Effect modules, namely MassNE, which comprehensively incorporates individual effects, cooperation effects (i.e., intra-team interactions) and suppression effects (i.e., inter-team interactions) for predicting battle outcomes. Specifically, we design marginal effect modules to learn how units’ marginal utility changing respect to their number, where the monotonicity assumption is applied to ensure rationality. In addition, we evaluate the current classical models and provide mathematical proofs that MassNE is able to generalize several earlier works in massive settings. Massive battle datasets generated by StarCraft II APIs are adopted to evaluate the performances of MassNE. Extensive experiments empirically demonstrate the effectiveness of MassNE, and MassNE can reveal reasonable cooperation effects, suppression effects, and marginal utilities of combat units from the data.|在网络游戏中，预测大规模战斗的结果是许多应用程序的基本任务，如团队优化和战术制定。现有的工程对这场大规模的战斗没有给予足够的重视。他们要么试图孤立地评估个体，要么挖掘个体之间简单的成对互动，这两种方法都没有有效地捕捉到大量单位(例如个体)之间错综复杂的互动。此外，随着团队规模的扩大，单位边际效用递减的现象也随之出现。这种递减模式在以前的工作中很少被注意到，如何从数据中捕获它仍然是一个挑战。为此，我们提出了一个新的具有边际效应模块的大规模战斗结果预测器，即 MassNE，它综合了个体效应，合作效应(即团队内部相互作用)和抑制效应(即团队间相互作用)来预测战斗结果。具体来说，我们设计边际效应模块来研究单位的边际效用是如何随着数量的变化而变化的，其中使用了单调性假设来确保合理性。此外，我们评估了目前的经典模型，并提供数学证明，MassNE 能够在大规模的背景下推广几个早期的作品。采用星际争霸 II API 生成的海量战斗数据集对 MassNE 的性能进行评估。大量的实验结果表明，MassNE 和 MassNE 能够从数据中揭示作战单元的合理协同效应、抑制效应和边际效用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MassNE:+Exploring+Higher-Order+Interactions+with+Marginal+Effect+for+Massive+Battle+Outcome+Prediction)|0|
|[Online Advertising in Ukraine and Russia During the 2022 Russian Invasion](https://doi.org/10.1145/3543507.3583484)|Christina Yeung, Umar Iqbal, Yekaterina Tsipenyuk O'Neil, Tadayoshi Kohno, Franziska Roesner|Security and Privacy Research Lab, University of Washington, USA; Micro Focus, USA|Online ads are a major source of information on the web. The mass reach of online advertising is often leveraged for information dissemination, at times with an objective to influence public opinion (e.g., election misinformation). We hypothesized that online advertising, due to its reach and potential, might have been used to spread information around the 2022 Russian invasion of Ukraine. Thus, to understand the online ad ecosystem during this conflict, we conducted a five-month long large-scale measurement study of online advertising in Ukraine, Russia, and the US. We studied advertising trends of ad platforms that delivered ads in Ukraine, Russia, and the US and conducted an in-depth qualitative analysis of the conflict-related ad content. We found that prominent US-based advertisers continued to support Russian websites, and a portion of online ads were used to spread conflict-related information, including protesting the invasion, and spreading awareness, which might have otherwise potentially been censored in Russia.|在线广告是网络信息的主要来源。在线广告的广泛传播经常被用于信息传播，有时是为了影响公众舆论(例如，选举错误信息)。我们假设，由于网络广告的影响力和潜力，它可能被用来传播2022年俄罗斯入侵乌克兰前后的信息。因此，为了了解这场冲突中的在线广告生态系统，我们对乌克兰、俄罗斯和美国的在线广告进行了为期五个月的大规模测量研究。我们研究了在乌克兰、俄罗斯和美国发布广告的广告平台的广告趋势，并对与冲突相关的广告内容进行了深入的定性分析。我们发现，美国知名的广告商继续支持俄罗斯网站，一部分在线广告被用于传播与冲突有关的信息，包括抗议入侵和传播意识，否则这些信息可能会在俄罗斯受到审查。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Advertising+in+Ukraine+and+Russia+During+the+2022+Russian+Invasion)|0|
|[Understanding the Behaviors of Toxic Accounts on Reddit](https://doi.org/10.1145/3543507.3583522)|Deepak Kumar, Jeff T. Hancock, Kurt Thomas, Zakir Durumeric|Stanford University, USA; Google, USA|Toxic comments are the top form of hate and harassment experienced online. While many studies have investigated the types of toxic comments posted online, the effects that such content has on people, and the impact of potential defenses, no study has captured the behaviors of the accounts that post toxic comments or how such attacks are operationalized. In this paper, we present a measurement study of 929K accounts that post toxic comments on Reddit over an 18 month period. Combined, these accounts posted over 14 million toxic comments that encompass insults, identity attacks, threats of violence, and sexual harassment. We explore the impact that these accounts have on Reddit, the targeting strategies that abusive accounts adopt, and the distinct patterns that distinguish classes of abusive accounts. Our analysis informs the nuanced interventions needed to curb unwanted toxic behaviors online.|有害的评论是网络上最常见的仇恨和骚扰。尽管许多研究调查了网上发布的有毒评论的类型、这些内容对人们的影响以及潜在防御的影响，但没有一项研究捕捉到发布有毒评论的账户的行为或者这种攻击是如何操作的。在这篇论文中，我们提出了一个测量研究的929K 帐户后，有毒评论 Reddit 在18个月期间。这些账户总共发布了超过1400万条有毒评论，包括侮辱、身份攻击、暴力威胁和性骚扰。我们探讨了这些账户对 Reddit 的影响，滥用账户采用的定位策略，以及区分滥用账户类别的独特模式。我们的分析为控制网上不良行为提供了细致入微的干预措施。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+the+Behaviors+of+Toxic+Accounts+on+Reddit)|0|
|[Online Reviews Are Leading Indicators of Changes in K-12 School Attributes](https://doi.org/10.1145/3543507.3583531)|Linsen Li, Aron Culotta, Douglas N. Harris, Nicholas Mattei|Department of Economics, Tulane University, USA; Department of Computer Science, Tulane University, USA|School rating websites are increasingly used by parents to assess the quality and fit of U.S. K-12 schools for their children. These online reviews often contain detailed descriptions of a school’s strengths and weaknesses, which both reflect and inform perceptions of a school. Existing work on these text reviews has focused on finding words or themes that underlie these perceptions, but has stopped short of using the textual reviews as leading indicators of school performance. In this paper, we investigate to what extent the language used in online reviews of a school is predictive of changes in the attributes of that school, such as its socio-economic makeup and student test scores. Using over 300K reviews of 70K U.S. schools from a popular ratings website, we apply language processing models to predict whether schools will significantly increase or decrease in an attribute of interest over a future time horizon. We find that using the text improves predictive performance significantly over a baseline model that does not include text but only the historical time-series of the indicators themselves, suggesting that the review text carries predictive power. A qualitative analysis of the most predictive terms and phrases used in the text reviews indicates a number of topics that serve as leading indicators, such as diversity, changes in school leadership, a focus on testing, and school safety.|越来越多的家长使用学校评级网站来评估美国 K-12学校的质量和适合他们孩子的程度。这些在线评论通常包含对一所学校优势和劣势的详细描述，这些优势和劣势既反映了对一所学校的看法，也反映了对这所学校的看法。关于这些文本审查的现有工作集中在寻找构成这些观念的词汇或主题，但没有使用文本审查作为学校表现的主要指标。在这篇论文中，我们调查了在网上评论一所学校时使用的语言在多大程度上可以预测该学校的属性变化，例如其社会经济构成和学生考试成绩。我们利用一个受欢迎的评级网站对美国70000所学校的300K 评论，应用语言处理模型来预测学校在未来的时间范围内是否会显著增加或减少兴趣属性。我们发现使用文本比不包括文本但仅包括指标本身的历史时间序列的基线模型显着提高了预测性能，这表明审查文本具有预测能力。对文本评论中使用的最具预测性的术语和短语进行定性分析，可以发现一些主要指标，如多样性、学校领导层的变化、对测试的关注和学校安全。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Reviews+Are+Leading+Indicators+of+Changes+in+K-12+School+Attributes)|0|
|[Beyond Fine-Tuning: Efficient and Effective Fed-Tuning for Mobile/Web Users](https://doi.org/10.1145/3543507.3583212)|Bingyan Liu, Yifeng Cai, Hongzhe Bi, Ziqi Zhang, Ding Li, Yao Guo, Xiangqun Chen|Beijing University of Posts and Telecommunications, China; Peking University, China|Fine-tuning is a typical mechanism to achieve model adaptation for mobile/web users, where a model trained by the cloud is further retrained to fit the target user task. While traditional fine-tuning has been proved effective, it only utilizes local data to achieve adaptation, failing to take advantage of the valuable knowledge from other mobile/web users. In this paper, we attempt to extend the local-user fine-tuning to multi-user fed-tuning with the help of Federated Learning (FL). Following the new paradigm, we propose EEFT, a framework aiming to achieve Efficient and Effective Fed-Tuning for mobile/web users. The key idea is to introduce lightweight but effective adaptation modules to the pre-trained model, such that we can freeze the pre-trained model and just focus on optimizing the modules to achieve cost reduction and selective task cooperation. Extensive experiments on our constructed benchmark demonstrate the effectiveness and efficiency of the proposed framework.|微调是为移动/网络用户实现模型自适应的典型机制，其中云训练的模型将进一步再训练以适应目标用户的任务。虽然传统的微调已被证明是有效的，但它只利用本地数据来实现适应，而没有利用其他移动/网络用户的宝贵知识。本文尝试在联邦学习(FL)的帮助下，将本地用户调优扩展到多用户馈源调优。遵循这一新的范式，我们提出了 EEFT 框架，旨在为移动/网络用户实现高效和有效的馈线调整。其核心思想是在预训练模型中引入轻量级但有效的自适应模块，这样我们就可以冻结预训练模型，集中精力优化模块以达到降低成本和选择性任务协作的目的。在我们构建的基准上的大量实验证明了该框架的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beyond+Fine-Tuning:+Efficient+and+Effective+Fed-Tuning+for+Mobile/Web+Users)|0|
|[Automated WebAssembly Function Purpose Identification With Semantics-Aware Analysis](https://doi.org/10.1145/3543507.3583235)|Alan Romano, Weihang Wang|University of Southern California, USA|WebAssembly is a recent web standard built for better performance in web applications. The standard defines a binary code format to use as a compilation target for a variety of languages, such as C, C++, and Rust. The standard also defines a text representation for readability, although, WebAssembly modules are difficult to interpret by human readers, regardless of their experience level. This makes it difficult to understand and maintain any existing WebAssembly code. As a result, third-party WebAssembly modules need to be implicitly trusted by developers as verifying the functionality themselves may not be feasible. To this end, we construct WASPur, a tool to automatically identify the purposes of WebAssembly functions. To build this tool, we first construct an extensive collection of WebAssembly samples that represent the state of WebAssembly. Second, we analyze the dataset and identify the diverse use cases of the collected WebAssembly modules. We leverage the dataset of WebAssembly modules to construct semantics-aware intermediate representations (IR) of the functions in the modules. We encode the function IR for use in a machine learning classifier, and we find that this classifier can predict the similarity of a given function against known named functions with an accuracy rate of 88.07%. We hope our tool will enable inspection of optimized and minified WebAssembly modules that remove function names and most other semantic identifiers.|WebAssembly 是最近为了在 Web 应用程序中获得更好的性能而建立的 Web 标准。该标准定义了一种二进制代码格式，用作各种语言(如 C、 C + + 和 Rust)的编译目标。该标准还为可读性定义了一个文本表示，尽管 WebAssembly 模块很难被人类读者解释，不管他们的经验水平如何。这使得理解和维护任何现有的 WebAssembly 代码变得非常困难。因此，开发人员需要隐式地信任第三方 WebAssembly 模块，因为验证功能本身可能是不可行的。为此，我们构建了 WASPur，这是一个自动识别 WebAssembly 函数用途的工具。为了构建这个工具，我们首先构建一个广泛的 WebAssembly 示例集合，这些示例代表 WebAssembly 的状态。其次，我们分析数据集并识别收集的 WebAssembly 模块的不同用例。我们利用 WebAssembly 模块的数据集来构造模块中函数的语义感知中间表示(IR)。将函数 IR 编码后用于机器学习分类器，结果表明，该分类器可以预测给定函数与已知命名函数的相似度，准确率为88.07% 。我们希望我们的工具能够检查优化和缩小的 WebAssembly 模块，这些模块删除了函数名和大多数其他语义标识符。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automated+WebAssembly+Function+Purpose+Identification+With+Semantics-Aware+Analysis)|0|
|[SCTAP: Supporting Scenario-Centric Trigger-Action Programming based on Software-Defined Physical Environments](https://doi.org/10.1145/3543507.3583293)|Bingkun Sun, Liwei Shen, Xin Peng, Ziming Wang|School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China|The physical world we live in is accelerating digitalization with the vigorous development of Internet of Things (IoT). Following this trend, Web of Things (WoT) further enables fast and efficient creation of various applications that perceive and act on the physical world using standard Web technologies. A popular way for creating WoT applications is Trigger-Action Programming (TAP), which allows users to orchestrate the capabilities of IoT devices in the form of “if trigger, then action”. However, existing TAP approaches don’t support scenario-centric WoT applications which involve abstract modeling of physical environments and complex spatio-temporal dependencies between events and actions. In this paper, we propose an approach called SCTAP which supports Scenario-Centric Trigger-Action Programming based on software-defined physical environments. SCTAP defines a structured and conceptual representation for physical environments, which provides the required programming abstractions for WoT applications. Based on the representation, SCTAP defines a grammar for specifying scenario-centric WoT applications with spatio-temporal dependencies. Furthermore, we design a service-based architecture for SCTAP which supports the integration of device access, event perception, environment representation, and rule execution in a loosely-coupled and extensible way. We implement SCTAP as a WoT infrastructure and evaluate it with two case studies including a smart laboratory and a smart coffee house. The results confirm the usability, feasibility and efficiency of SCTAP and its implementation.|随着物联网的蓬勃发展，我们生活的物质世界正在加速数字化进程。遵循这一趋势，物联网(Web of Things，WoT)进一步支持使用标准 Web 技术快速高效地创建各种应用程序，这些应用程序使用物理世界来感知和操作。创建物联网应用程序的一种流行方式是触发行动编程(Trigger-Action Programming，TAP) ，它允许用户以“如果触发，那么行动”的形式编排物联网设备的功能。然而，现有的 TAP 方法不支持以场景为中心的 WoT 应用程序，这些应用程序涉及物理环境的抽象建模以及事件和操作之间复杂的时空依赖关系。在本文中，我们提出了一种支持基于软件定义的物理环境的以场景为中心的触发行为编程的 SCTAP 方法。SCTAP 定义了物理环境的结构化和概念化表示，为 WoT 应用程序提供了所需的编程抽象。基于这种表示，SCTAP 定义了一种语法，用于指定具有时空依赖性的以场景为中心的 WoT 应用程序。此外，我们还为 SCTAP 设计了一个基于服务的体系结构，该体系结构以松耦合和可扩展的方式支持设备访问、事件感知、环境表示和规则执行的集成。我们将 SCTAP 作为一个 WoT 基础设施来实施，并通过两个案例研究对其进行评估，其中包括一个智能实验室和一个智能咖啡屋。实验结果验证了 SCTAP 及其实现的可用性、可行性和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SCTAP:+Supporting+Scenario-Centric+Trigger-Action+Programming+based+on+Software-Defined+Physical+Environments)|0|
|[DeeProphet: Improving HTTP Adaptive Streaming for Low Latency Live Video by Meticulous Bandwidth Prediction](https://doi.org/10.1145/3543507.3583364)|Kefan Chen, Bo Wang, Wufan Wang, Xiaoyu Li, Fengyuan Ren|Tsinghua University, China; Beijing Institute of Technology, China; Tsinghua University, China and Zhongguancun Laboratory, China|The performance of HTTP adaptive streaming (HAS) depends heavily on the prediction of end-to-end network bandwidth. The increasingly popular low latency live streaming (LLLS) faces greater challenges since it requires accurate, short-term bandwidth prediction, compared with VOD streaming which needs long-term bandwidth prediction and has good tolerance against prediction error. Part of the challenges comes from the fact that short-term bandwidth experiences both large abrupt changes and uncertain fluctuations. Additionally, it is hard to obtain valid bandwidth measurement samples in LLLS due to its inter-chunk and intra-chunk sending idleness. In this work, we present DeeProphet, a system for accurate bandwidth prediction in LLLS to improve the performance of HAS. DeeProphet overcomes the above challenges by collecting valid measurement samples using fine-grained TCP state information to identify the packet bursting intervals, and by combining the time series model and learning-based model to predict both large change and uncertain fluctuations. Experiment results show that DeeProphet improves the overall QoE by 17.7%-359.2% compared with state-of-the-art LLLS ABR algorithms, and reduces the median bandwidth prediction error to 2.7%.|HTTP 自适应流(HAS)的性能在很大程度上取决于端到端网络带宽的预测。相对于需要长期带宽预测且对预测误差有较好承受能力的 VOD 流，低延迟直播流(LLLS)由于需要准确、短期的带宽预测而面临着更大的挑战。部分挑战来自短期带宽经历巨大的突变和不确定的波动这一事实。此外，在 LLLS 中，由于组间和组内发送空闲，很难获得有效的带宽测量样本。在这项工作中，我们提出了 DeeProphet，一个在 LLLS 中准确的带宽预测系统，以改善 HAS 的性能。DeeProphet 通过使用细粒度的 TCP 状态信息收集有效的测量样本来识别数据包的爆发间隔，并结合时间序列模型和基于学习的模型来预测大的变化和不确定的波动，克服了上述挑战。实验结果表明，与最先进的 LLLS ABR 算法相比，DeeProphet 算法的总体 QoE 提高了17.7% -359.2% ，中值带宽预测误差降低到2.7% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DeeProphet:+Improving+HTTP+Adaptive+Streaming+for+Low+Latency+Live+Video+by+Meticulous+Bandwidth+Prediction)|0|
|[Is IPFS Ready for Decentralized Video Streaming?](https://doi.org/10.1145/3543507.3583404)|Zhengyu Wu, ChengHao Ryan Yang, Santiago Vargas, Aruna Balasubramanian|Computer Science, Northeastern University, USA; Computer Science, Stony Brook University, USA|InterPlanetary File System (IPFS) is a peer-to-peer protocol for decentralized content storage and retrieval. The IPFS platform has the potential to help users evade censorship and avoid a central point of failure. IPFS is seeing increasing adoption for distributing various kinds of files, including video. However, the performance of video streaming on IPFS has not been well-studied. We conduct a measurement study with over 28,000 videos hosted on the IPFS network and find that video streaming experiences high stall rates due to relatively high Round Trip Times (RTT). Further, videos are encoded using a single static quality, because of which streaming cannot adapt to different network conditions. A natural approach is to use adaptive bitrate (ABR) algorithms for streaming, which encode videos in multiple qualities and streams according to the throughput available. However, traditional ABR algorithms perform poorly on IPFS because the throughput cannot be estimated correctly. The main problem is that video segments can be retrieved from multiple sources, making it difficult to estimate the throughput. To overcome this issue, we have designed Telescope, an IPFS-aware ABR system. We conduct experiments on the IPFS network, where IPFS video providers are geographically distributed across the globe. Our results show that Telescope significantly improves the Quality of Experience (QoE) of videos, for a diverse set of network and cache conditions, compared to traditional ABR.|行星文件系统(IPFS)是一种用于分散内容存储和检索的对等协议。IPFS 平台具有帮助用户规避审查和避免中心故障点的潜力。IPFS 越来越多地被用于发布各种文件，包括视频。然而，IPFS 上视频流的性能还没有得到很好的研究。我们对 IPFS 网络上的28,000多个视频进行了测量研究，发现由于往返时间(RTT)相对较高，视频流经历了较高的失速率。而且，视频是使用单一静态质量进行编码的，因为流不能适应不同的网络条件。一种自然的方法是使用自适应比特率(ABR)算法进行视频流编码，它根据可用的吞吐量以多种质量和流的形式对视频进行编码。然而，传统的 ABR 算法在 IPFS 上表现不佳，因为不能正确估计吞吐量。主要问题是视频片段可以从多个源检索，这使得估计吞吐量变得困难。为了克服这个问题，我们设计了望远镜，一个 IPFS 感知 ABR 系统。我们在 IPFS 网络上进行实验，IPFS 视频提供商分布在全球各地。我们的研究结果表明，与传统的 ABR 相比，Telescope 在不同的网络和缓存条件下显著提高了视频的体验质量(QoE)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+IPFS+Ready+for+Decentralized+Video+Streaming?)|0|
|[SISSI: An Architecture for Semantic Interoperable Self-Sovereign Identity-based Access Control on the Web](https://doi.org/10.1145/3543507.3583409)|Christoph H.J. Braun, Vasil Papanchev, Tobias Käfer|Karlsruhe Institute of Technology, Germany|We present an architecture for authentication and authorization on the Web that is based on the Self-Sovereign Identity paradigm. Using our architecture, we aim to achieve semantic interoperability across different approaches to SSI. We build on the underlying RDF data model of the W3C’s recommendation for Verifiable Credentials and specify semantic access control rules using SHACL. Our communication protocol for an authorization process is based on Decentralised Identifiers and extends the Hyperledger Aries Present Proof protocol. We propose a modular architecture that allows for flexible extension, e. g., for supporting more signature schemes or Decentralised Identifier Methods. For evaluation, we implemented a Proof-of-Concept: We show that a Web-based approach to SSI outperfoms a blockchain-based approach to SSI in terms of End-to-End execution time.|我们提出了一种基于自主身份认证范式的 Web 身份验证和授权体系结构。利用我们的架构，我们的目标是在不同的 SSI 方法之间实现语义互操作性。我们基于 W3C 推荐的可验证凭证的底层 RDF 数据模型，并使用 SHACL 指定语义访问控制规则。我们的授权过程的通信协议是基于分散的标识符，并扩展了 Hyperledger 白羊目前的证明协议。我们提出了一个模块化的体系结构，允许灵活的扩展，例如，支持更多的签名方案或分散的标识符方法。对于评估，我们实现了一个概念验证: 我们展示了基于 Web 的 SSI 方法在端到端执行时间方面优于基于区块链的 SSI 方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SISSI:+An+Architecture+for+Semantic+Interoperable+Self-Sovereign+Identity-based+Access+Control+on+the+Web)|0|
|[Detecting Socially Abnormal Highway Driving Behaviors via Recurrent Graph Attention Networks](https://doi.org/10.1145/3543507.3583452)|Yue Hu, Yuhang Zhang, Yanbing Wang, Daniel B. Work|Vanderbilt University, USA|With the rapid development of Internet of Things technologies, the next generation traffic monitoring infrastructures are connected via the web, to aid traffic data collection and intelligent traffic management. One of the most important tasks in traffic is anomaly detection, since abnormal drivers can reduce traffic efficiency and cause safety issues. This work focuses on detecting abnormal driving behaviors from trajectories produced by highway video surveillance systems. Most of the current abnormal driving behavior detection methods focus on a limited category of abnormal behaviors that deal with a single vehicle without considering vehicular interactions. In this work, we consider the problem of detecting a variety of socially abnormal driving behaviors, i.e., behaviors that do not conform to the behavior of other nearby drivers. This task is complicated by the variety of vehicular interactions and the spatial-temporal varying nature of highway traffic. To solve this problem, we propose an autoencoder with a Recurrent Graph Attention Network that can capture the highway driving behaviors contextualized on the surrounding cars, and detect anomalies that deviate from learned patterns. Our model is scalable to large freeways with thousands of cars. Experiments on data generated from traffic simulation software show that our model is the only one that can spot the exact vehicle conducting socially abnormal behaviors, among the state-of-the-art anomaly detection models. We further show the performance on real world HighD traffic dataset, where our model detects vehicles that violate the local driving norms.|随着物联网技术的飞速发展，下一代的交通监控基础设施通过网络连接起来，有助于交通数据的采集和智能交通管理。交通中最重要的任务之一就是异常检测，因为不正常的司机会降低交通效率并引起安全问题。本文主要研究如何从高速公路视频监控系统产生的轨迹中检测出不正常的驾驶行为。目前的异常驾驶行为检测方法大多集中在有限的一类异常驾驶行为上，这类异常驾驶行为只检测一辆车，而不考虑车辆之间的相互作用。在这项工作中，我们考虑的问题，检测各种社会异常驾驶行为，即行为不符合其他附近的驾驶员的行为。由于车辆相互作用的多样性以及高速公路交通的时空变化特性，这项任务变得更加复杂。为了解决这一问题，我们提出了一种基于循环图形注意网络的自动编码器，它可以捕获与周围车辆相关的高速公路驾驶行为，并检测偏离学习模式的异常。我们的模型可扩展到拥有数千辆汽车的大型高速公路。对交通模拟软体数据的实验表明，在最先进的异常检测模型中，我们的模型是唯一能够准确识别出行为异常的车辆的模型。我们进一步显示在真实世界的高速交通数据集，其中我们的模型检测违反当地驾驶规范的车辆的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Socially+Abnormal+Highway+Driving+Behaviors+via+Recurrent+Graph+Attention+Networks)|0|
|[GROUP: An End-to-end Multi-step-ahead Workload Prediction Approach Focusing on Workload Group Behavior](https://doi.org/10.1145/3543507.3583460)|Binbin Feng, Zhijun Ding|Tongji University, China|Accurately forecasting workloads can enable web service providers to achieve proactive runtime management for applications and ensure service quality and cost efficiency. For cloud-native applications, multiple containers collaborate to handle user requests, making each container’s workload changes influenced by workload group behavior. However, existing approaches mainly analyze the individual changes of each container and do not explicitly model the workload group evolution of containers, resulting in sub-optimal results. Therefore, we propose a workload prediction method, GROUP, which implements the shifts of workload prediction focus from individual to group, workload group behavior representation from data similarity to data correlation, and workload group behavior evolution from implicit modeling to explicit modeling. First, we model the workload group behavior and its evolution from multiple perspectives. Second, we propose a container correlation calculation algorithm that considers static and dynamic container information to represent the workload group behavior. Third, we propose an end-to-end multi-step-ahead prediction method that explicitly portrays the complex relationship between the evolution of workload group behavior and the workload changes of each container. Lastly, enough experiments on public datasets show the advantages of GROUP, which provides an effective solution to achieve workload prediction for cloud-native applications.|准确预测工作负载可以使 Web 服务提供商实现应用程序的主动运行时管理，并确保服务质量和成本效率。对于云本地应用程序，多个容器协作处理用户请求，使每个容器的工作负载变化受到工作负载组行为的影响。然而，现有的方法主要分析每个集装箱的个别变化，并没有明确的模型集装箱的工作负载组演化，导致次优结果。为此，提出了一种工作负载预测方法 GROUP，该方法实现了工作负载预测焦点从个体到群体的转移，实现了工作负载群体行为从数据相似性到数据相关性的表示，实现了工作负载群体行为从隐式建模到显式建模的演化。首先，我们从多个角度对工作负载组行为及其演化进行建模。其次，提出了一种考虑静态和动态容器信息来表示工作负载组行为的容器关联计算算法。第三，提出了一种端到端多步提前预测方法，该方法明确描述了工作负载组行为的演化与每个容器的工作负载变化之间的复杂关系。最后，在公共数据集上进行了大量的实验，验证了 GROUP 的优势，为云本地应用程序实现工作负载预测提供了有效的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GROUP:+An+End-to-end+Multi-step-ahead+Workload+Prediction+Approach+Focusing+on+Workload+Group+Behavior)|0|
|[FANS: Fast Non-Autoregressive Sequence Generation for Item List Continuation](https://doi.org/10.1145/3543507.3583430)|Qijiong Liu, Jieming Zhu, Jiahao Wu, Tiandeng Wu, Zhenhua Dong, XiaoMing Wu|The Hong Kong Polytechnic University, Hong Kong; Huawei Noah's Ark Lab, China; Huawei Technolologies Co., Ltd, China|User-curated item lists, such as video-based playlists on Youtube and book-based lists on Goodreads, have become prevalent for content sharing on online platforms. Item list continuation is proposed to model the overall trend of a list and predict subsequent items. Recently, Transformer-based models have shown promise in comprehending contextual information and capturing item relationships in a list. However, deploying them in real-time industrial applications is challenging, mainly because the autoregressive generation mechanism used in them is time-consuming. In this paper, we propose a novel fast non-autoregressive sequence generation model, namely FANS, to enhance inference efficiency and quality for item list continuation. First, we use a non-autoregressive generation mechanism to decode next $K$ items simultaneously instead of one by one in existing models. Then, we design a two-stage classifier to replace the vanilla classifier used in current transformer-based models to further reduce the decoding time. Moreover, to improve the quality of non-autoregressive generation, we employ a curriculum learning strategy to optimize training. Experimental results on four real-world item list continuation datasets including Zhihu, Spotify, AotM, and Goodreads show that our FANS model can significantly improve inference efficiency (up to 8.7x) while achieving competitive or better generation quality for item list continuation compared with the state-of-the-art autoregressive models. We also validate the efficiency of FANS in an industrial setting. Our source code and data will be available at MindSpore/models and Github.|用户策划的项目列表，如 Youtube 上基于视频的播放列表和 Goodreads 上基于图书的列表，已经成为在线平台上内容共享的流行。提出项目表延续模型来模拟项目表的总体趋势并预测后续项目。最近，基于 Transformer 的模型在理解上下文信息和捕获列表中的项目关系方面显示了前景。然而，将它们部署到实时工业应用程序中是具有挑战性的，主要是因为它们中使用的自回归生成机制非常耗时。本文提出了一种新的快速非自回归序列生成模型 FANS，以提高项目表延拓的推理效率和推理质量。首先，我们使用一个非自回归生成机制来同时解码下一个 $K $条目，而不是在现有模型中逐个解码。然后，我们设计了一个两级分类器来取代基于电流互感器的模型中使用的香草分类器，以进一步减少解码时间。此外，为了提高非自回归生成的质量，我们采用课程学习策略来优化培训。实验结果表明，与最先进的自回归模型相比，我们的 FANS 模型可以显著提高推理效率(高达8.7倍) ，同时实现项目列表延续的竞争性或更好的生成质量。我们还验证了 FANS 在工业环境中的有效性。我们的源代码和数据可以在 MindSpore/model 和 Github 上找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FANS:+Fast+Non-Autoregressive+Sequence+Generation+for+Item+List+Continuation)|0|
|[DANCE: Learning A Domain Adaptive Framework for Deep Hashing](https://doi.org/10.1145/3543507.3583445)|Haixin Wang, Jinan Sun, Xiang Wei, Shikun Zhang, Chong Chen, XianSheng Hua, Xiao Luo|Department of Computer Science, UCLA, USA; Peking University, China; Zhejiang University, China; BIGO Inc., Singapore|This paper studies unsupervised domain adaptive hashing, which aims to transfer a hashing model from a label-rich source domain to a label-scarce target domain. Current state-of-the-art approaches generally resolve the problem by integrating pseudo-labeling and domain adaptation techniques into deep hashing paradigms. Nevertheless, they usually suffer from serious class imbalance in pseudo-labels and suboptimal domain alignment caused by the neglection of the intrinsic structures of two domains. To address this issue, we propose a novel method named unbiaseD duAl hashiNg Contrastive lEarning (DANCE) for domain adaptive image retrieval. The core of our DANCE is to perform contrastive learning on hash codes from both instance level and prototype level. To begin, DANCE utilizes label information to guide instance-level hashing contrastive learning in the source domain. To generate unbiased and reliable pseudo-labels for semantic learning in the target domain, we uniformly select samples around each label embedding in the Hamming space. A momentum-update scheme is also utilized to smooth the optimization process. Additionally, we measure the semantic prototype representations in both source and target domains and incorporate them into a domain-aware prototype-level contrastive learning paradigm, which enhances domain alignment in the Hamming space while maximizing the model capacity. Experimental results on a number of well-known domain adaptive retrieval benchmarks validate the effectiveness of our proposed DANCE compared to a variety of competing baselines in different settings.|本文研究了无监督域自适应哈希算法，目的是将一个哈希模型从一个标签丰富的源域转移到一个标签稀缺的目标域。当前最先进的方法通常通过将伪标记和域适应技术集成到深度散列范例中来解决这个问题。然而，由于忽略了两个域的内在结构，它们常常会出现伪标签的类别不平衡和域对齐不理想的问题。为了解决这个问题，我们提出了一种新的无偏 DduAl hashiNg 对比度学习方法(DANCE)用于领域自适应图像检索。我们的 DANCE 的核心是从实例级和原型级对哈希码进行对比学习。首先，DANCE 利用标签信息来指导源域中的实例级散列对比学习。为了在目标域中生成无偏、可靠的语义学习伪标签，在汉明空间中对每个标签周围的样本进行统一选择。利用动量更新策略使优化过程平滑。此外，我们在源域和目标域都测量了语义原型表示，并将它们整合到领域感知的原型级对比学习范式中，该范式增强了汉明空间中的领域对齐，同时最大化了模型容量。在一些著名的领域自适应检索基准上的实验结果验证了我们提出的 DANCE 相对于不同设置下的各种竞争基准的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DANCE:+Learning+A+Domain+Adaptive+Framework+for+Deep+Hashing)|0|
|[Differentiable Optimized Product Quantization and Beyond](https://doi.org/10.1145/3543507.3583482)|Zepu Lu, Defu Lian, Jin Zhang, Zaixi Zhang, Chao Feng, Hao Wang, Enhong Chen|University of Science and Technology of China, School of Computer Science, China; University of Science and Technology of China, School of Computer Science, School of Data Science, China and State Key Laboratory of Cognitive Intelligence, China; University of Science and Technology of China, School of Data Science, China|Vector quantization techniques, such as Product Quantization (PQ), play a vital role in approximate nearest neighbor search (ANNs) and maximum inner product search (MIPS) owing to their remarkable search and storage efficiency. However, the indexes in vector quantization cannot be trained together with the inference models since data indexing is not differentiable. To this end, differentiable vector quantization approaches, such as DiffPQ and DeepPQ, have been recently proposed, but existing methods have two drawbacks. First, they do not impose any constraints on codebooks, such that the resultant codebooks lack diversity, leading to limited retrieval performance. Second, since data indexing resorts to operator, differentiability is usually achieved by either relaxation or Straight-Through Estimation (STE), which leads to biased gradient and slow convergence. To address these problems, we propose a Differentiable Optimized Product Quantization method (DOPQ) and beyond in this paper. Particularly, each data is projected into multiple orthogonal spaces, to generate multiple views of data. Thus, each codebook is learned with one view of data, guaranteeing the diversity of codebooks. Moreover, instead of simple differentiable relaxation, DOPQ optimizes the loss based on direct loss minimization, significantly reducing the gradient bias problem. Finally, DOPQ is evaluated with seven datasets of both recommendation and image search tasks. Extensive experimental results show that DOPQ outperforms state-of-the-art baselines by a large margin.|向量量化技术，例如产品量化技术，由于其卓越的搜索和存储效率，在近似最近邻搜索(ANN)和最大内部产品搜索(MIPS)方面发挥了重要作用。然而，由于数据索引是不可微的，因此不能将向量量化中的索引与推断模型一起训练。为了达到这个目的，最近提出了一些可微分的向量量化方法，比如迪普 PQ 和 DeepPQ，但是现有的方法有两个缺点。首先，它们不对代码书施加任何约束，因此得到的代码书缺乏多样性，导致检索性能有限。其次，由于数据索引依赖于算子，可微性通常是通过松弛估计或直接估计(STE)来实现的，这导致了有偏的梯度和收敛速度慢。针对这些问题，本文提出了一种可微优化产品量化方法(DOPQ)。特别是，每个数据被投影到多个正交空间，以生成多个数据视图。因此，每个代码本都是以一种数据视图来学习的，从而保证了代码本的多样性。此外，DOPQ 不再是简单的可微松弛，而是基于直接损耗最小化优化损耗，大大减少了梯度偏差问题。最后，利用推荐任务和图像搜索任务的七个数据集对 DOPQ 进行评估。广泛的实验结果表明，DOPQ 的性能大大优于最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Differentiable+Optimized+Product+Quantization+and+Beyond)|0|
|[Auctions without commitment in the auto-bidding world](https://doi.org/10.1145/3543507.3583416)|Andrés Perlroth, Aranyak Mehta||Advertisers in online ad auctions are increasingly using auto-bidding mechanisms to bid into auctions instead of directly bidding their value manually. One prominent auto-bidding format is the target cost-per-acquisition (tCPA) which maximizes the volume of conversions subject to a return-of-investment constraint. From an auction theoretic perspective however, this trend seems to go against foundational results that postulate that for profit-maximizing bidders, it is optimal to use a classic bidding system like marginal CPA (mCPA) bidding rather than using strategies like tCPA. In this paper we rationalize the adoption of such seemingly sub-optimal bidding within the canonical quasi-linear framework. The crux of the argument lies in the notion of commitment. We consider a multi-stage game where first the auctioneer declares the auction rules; then bidders select either the tCPA or mCPA bidding format and then, if the auctioneer lacks commitment, it can revisit the rules of the auction (e.g., may readjust reserve prices depending on the observed bids). Our main result is that so long as a bidder believes that the auctioneer lacks commitment to follow the rule of the declared auction then the bidder will make a higher profit by choosing the tCPA format over the mCPA format. We then explore the commitment consequences for the auctioneer. In a simplified version of the model where there is only one bidder, we show that the tCPA subgame admits a credible equilibrium while the mCPA format does not. That is, when the bidder chooses the tCPA format the auctioneer can credibly implement the auction rules announced at the beginning of the game. We also show that, under some mild conditions, the auctioneer's revenue is larger when the bidder uses the tCPA format rather than mCPA. We further quantify the value for the auctioneer to be able to commit to the declared auction rules.|在线广告拍卖中，广告商越来越多地使用自动竞价机制进行竞价，而不是直接手动竞价。一个突出的自动投标格式是每次收购的目标成本(tCPA) ，它在投资回报约束下使转换量最大化。然而，从拍卖理论的角度来看，这种趋势似乎违背了基本结论，即对于利润最大化的投标者来说，使用经典的投标系统如边际 CPA (mCPA)投标比使用 tCPA 策略更为理想。本文在正则拟线性框架下，对这种看似次优的投标方式进行了合理化处理。争论的核心在于承诺的概念。我们考虑一个多阶段的博弈，首先拍卖商声明拍卖规则; 然后竞标者选择 tCPA 或 mCPA 的出价格式，然后，如果拍卖商缺乏承诺，它可以重新审视拍卖规则(例如，可以根据观察到的出价调整底价)。我们的主要结果是，只要投标人认为拍卖商缺乏承诺，以遵守宣布的拍卖规则，那么投标人将通过选择 tCPA 格式，而不是 mCPA 格式，获得更高的利润。然后，我们探讨承诺后果的拍卖商。在只有一个投标人的模型的简化版本中，我们证明了 tCPA 子博弈允许一个可信的均衡，而 mCPA 格式不允许。也就是说，当投标人选择 tCPA 格式时，拍卖人可以可信地执行游戏开始时宣布的拍卖规则。我们还表明，在一些温和的条件下，当竞标者使用 tCPA 格式而不是 mCPA 格式时，拍卖商的收入会更大。我们进一步量化拍卖师能够遵守公开的拍卖规则的价值。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Auctions+without+commitment+in+the+auto-bidding+world)|0|
|[Online resource allocation in Markov Chains](https://doi.org/10.1145/3543507.3583428)|Jianhao Jia, Hao Li, Kai Liu, Ziqi Liu, Jun Zhou, Nikolai Gravin, Zhihao Gavin Tang|Ant Group, China; Shanghai University of Finance and Economics, China|A large body of work in Computer Science and Operations Research study online algorithms for stochastic resource allocation problems. The most common assumption is that the online requests have randomly generated i.i.d. types. This assumption is well justified for static markets and/or relatively short time periods. We consider dynamic markets, whose states evolve as a random walk in a market-specific Markov Chain. This is a new model that generalizes previous i.i.d. settings. We identify important parameters of the Markov chain that is crucial for obtaining good approximation guarantees to the expected value of the optimal offline algorithm which knows realizations of all requests in advance. We focus on a stylized single-resource setting and: (i) generalize the well-known Prophet Inequality from the optimal stopping theory (single-unit setting) to Markov Chain setting; (ii) in multi-unit setting, design a simple algorithm that is asymptotically optimal under mild assumptions on the underlying Markov chain.|计算机科学与运筹学中的大量工作研究随机资源分配问题的在线算法。最常见的假设是，联机请求随机生成 i.id 类型。对于静态市场和/或相对较短的时间段，这种假设是合理的。我们考虑动态市场，其状态演化为特定市场马尔可夫链中的随机游动。这是一个新的模型，推广了以前的 ID 设置。我们确定了马尔可夫链的重要参数，这些参数对提前知道所有请求实现的最优离线算法的期望值获得良好的近似保证至关重要。我们重点研究了一个程式化的单资源设置，并且: (i)将著名的先知不等式从最优停止理论(单单元设置)推广到马尔可夫链设置; (ii)在多单元设置中，设计了一个简单的算法，该算法在基础马尔可夫链的温和假设下是渐近最优的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+resource+allocation+in+Markov+Chains)|0|
|[Worst-Case Welfare of Item Pricing in the Tollbooth Problem](https://doi.org/10.1145/3543507.3583432)|Zihan Tan, Yifeng Teng, Mingfei Zhao|Center for Discrete Mathematics and Theoretical Computer Science, Rutgers University, USA; Google Research, USA|We study the worst-case welfare of item pricing in the \emph{tollbooth problem}. The problem was first introduced by Guruswami et al, and is a special case of the combinatorial auction in which (i) each of the $m$ items in the auction is an edge of some underlying graph; and (ii) each of the $n$ buyers is single-minded and only interested in buying all edges of a single path. We consider the competitive ratio between the hindsight optimal welfare and the optimal worst-case welfare among all item-pricing mechanisms, when the order of the arriving buyers is adversarial. We assume that buyers own the \emph{tie-breaking} power, i.e. they can choose whether or not to buy the demand path at 0 utility. We prove a tight competitive ratio of $3/2$ when the underlying graph is a single path (also known as the \emph{highway} problem), whereas item-pricing can achieve the hindsight optimal if the seller is allowed to choose a proper tie-breaking rule to maximize the welfare. Moreover, we prove an $O(1)$ upper bound of competitive ratio when the underlying graph is a tree. For general graphs, we prove an $\Omega(m^{1/8})$ lower bound of the competitive ratio. We show that an $m^{\Omega(1)}$ competitive ratio is unavoidable even if the graph is a grid, or if the capacity of every edge is augmented by a constant factor $c$. The results hold even if the seller has tie-breaking power.|研究了收费站问题中项目定价的最坏情况下的福利问题。这个问题首先由 Guruswami 等人提出，是组合拍卖的一个特例，其中(i)拍卖中的每个 $m $物品都是某个基础图的边缘; (ii)每个 $n $买家都是一心一意的，只对购买单一路径的所有边缘感兴趣。我们考虑了当购买者到达顺序为对抗性时，所有商品定价机制中事后最优福利和最坏情况下最优福利之间的竞争比。我们假设买方拥有方幂{纽带断裂}功率，也就是说，他们可以选择是否以0效用购买需求路径。当基本图是单一路径时，我们证明了 $3/2 $的激烈竞争比率(也称为高速公路问题) ，然而，如果允许卖方选择一个合适的拆分规则来使福利最大化，则项目定价可以实现事后最优。此外，我们证明了当底层图是树时，竞争比率的上界为 $O (1)。对于一般图，我们证明了竞争比率的一个 $Omega (m ^ {1/8}) $下界。我们证明了即使图是一个网格，或者如果每个边的容量增加了一个常数因子 $c $，一个 $m ^ { Omega (1)} $竞争比率也是不可避免的。即使卖方具有打破平局的能力，结果仍然成立。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Worst-Case+Welfare+of+Item+Pricing+in+the+Tollbooth+Problem)|0|
|[Learning to Bid in Contextual First Price Auctions✱](https://doi.org/10.1145/3543507.3583427)|Ashwinkumar Badanidiyuru, Zhe Feng, Guru Guruganesh|Google, USA|In this paper, we investigate the problem about how to bid in repeated contextual first price auctions. We consider a single bidder (learner) who repeatedly bids in the first price auctions: at each time $t$, the learner observes a context $x_t\in \mathbb{R}^d$ and decides the bid based on historical information and $x_t$. We assume a structured linear model of the maximum bid of all the others $m_t = \alpha_0\cdot x_t + z_t$, where $\alpha_0\in \mathbb{R}^d$ is unknown to the learner and $z_t$ is randomly sampled from a noise distribution $\mathcal{F}$ with log-concave density function $f$. We consider both \emph{binary feedback} (the learner can only observe whether she wins or not) and \emph{full information feedback} (the learner can observe $m_t$) at the end of each time $t$. For binary feedback, when the noise distribution $\mathcal{F}$ is known, we propose a bidding algorithm, by using maximum likelihood estimation (MLE) method to achieve at most $\widetilde{O}(\sqrt{\log(d) T})$ regret. Moreover, we generalize this algorithm to the setting with binary feedback and the noise distribution is unknown but belongs to a parametrized family of distributions. For the full information feedback with \emph{unknown} noise distribution, we provide an algorithm that achieves regret at most $\widetilde{O}(\sqrt{dT})$. Our approach combines an estimator for log-concave density functions and then MLE method to learn the noise distribution $\mathcal{F}$ and linear weight $\alpha_0$ simultaneously. We also provide a lower bound result such that any bidding policy in a broad class must achieve regret at least $\Omega(\sqrt{T})$, even when the learner receives the full information feedback and $\mathcal{F}$ is known.|本文研究了重复背景下第一价格拍卖中的投标问题。我们考虑一个在第一次价格拍卖中重复出价的单个投标者(学习者) : 在每次 $t $时，学习者观察一个上下文 $x _ t 在 mathbb { R } ^ d $中，并根据历史信息和 $x _ t $决定出价。我们假设一个结构化的线性模型的最大出价的所有其他 $m _ t = alpha _ 0 cdot x _ t + z _ t $，其中 $alpha _ 0在 mathbb { R } ^ d $是未知的学习者和 $z _ t $是随机抽样从噪声分布 $数学{ F } $与对数凹密度函数 $f $。我们考虑两种情况: 每次结束时的情况(学习者只能观察自己是否胜出)和每次结束时的情况(学习者可以观察 $m _ t $)。对于二进制反馈，当噪声分布已知 $数学{ F } $时，我们提出了一种竞价算法，利用最大似然估计(MLE)方法实现最大宽波长{ O }(sqrt { log (d) T }) $后悔。此外，我们将该算法推广到二进制反馈环境下，噪声分布是未知的，但属于参数化分布族。对于具有未知噪声分布的完全信息反馈，我们提出了一种最多可以实现遗憾的算法。该方法将对数凹密度函数的估计量与 MLE 方法相结合，同时学习噪声分布的数学{ F } $和线性加权 $alpha _ 0 $。我们还提供了一个下界结果，即使学习者收到完整的信息反馈并且已知 $数学{ F } $，广义类中的任何投标策略都必须达到至少后悔 $Omega (sqrt { T }) $。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Bid+in+Contextual+First+Price+Auctions✱)|0|
|[Efficiency of Non-Truthful Auctions in Auto-bidding: The Power of Randomization](https://doi.org/10.1145/3543507.3583492)|Christopher Liaw, Aranyak Mehta, Andrés Perlroth|Google, USA|Auto-bidding is now widely adopted as an interface between advertisers and internet advertising as it allows advertisers to specify high-level goals, such as maximizing value subject to a value-per-spend constraint. Prior research has mainly focused on auctions that are truthful (such as a second-price auction) because these auctions admit simple (uniform) bidding strategies and are thus simpler to analyze. The main contribution of this paper is to characterize the efficiency across the spectrum of all auctions, including non-truthful auctions for which optimal bidding may be complex. For deterministic auctions, we show a dominance result: any uniform bidding equilibrium of a second-price auction (SPA) can be mapped to an equilibrium of any other auction – for example, first price auction (FPA) – with identical outcomes. In this sense, SPA with uniform bidding is an instance-wise optimal deterministic auction. Consequently, the price of anarchy (PoA) of any deterministic auction is at least the PoA of SPA with uniform bidding, which is known to be 2. We complement this by showing that the PoA of FPA without uniform bidding is 2. Next, we show, surprisingly, that truthful pricing is not dominant in the randomized setting. There is a randomized version of FPA that achieves a strictly smaller price of anarchy than its truthful counterpart when there are two bidders per query. Furthermore, this randomized FPA achieves the best-known PoA for two bidders, thus showing the power of non-truthfulness when combined with randomization. Finally, we show that no prior-free auction (even randomized, non-truthful) can improve on a PoA bound of 2 when there are a large number of advertisers per auction. These results should be interpreted qualitatively as follows. When the auction pressure is low, randomization and non-truthfulness is beneficial. On the other hand, if the auction pressure is intense, the benefits diminishes and it is optimal to implement a second-price auction.|自动竞价现在被广泛采用作为广告商和互联网广告之间的一个界面，因为它允许广告商指定高层次的目标，例如受每次支出价值约束的价值最大化。先前的研究主要集中在真实的拍卖(如二级价格拍卖) ，因为这些拍卖采用简单(统一)的竞价策略，因此更容易分析。本文的主要贡献是描述所有拍卖的效率，包括非真实的拍卖，其中最优投标可能是复杂的。对于确定性拍卖，我们给出了一个优势结果: 第二价格拍卖(SPA)的任何统一竞价均衡可以映射到任何其他拍卖的均衡，例如第一价格拍卖(FPA) ，具有相同的结果。从这个意义上说，具有统一报价的 SPA 是一种实例最优确定性拍卖。因此，任何确定性拍卖的无政府状态价格(PoA)至少是统一竞价下的 SPA 的 PoA，即为2。我们通过显示无统一投标的 FPA 的 PoA 是2来补充这一点。接下来，我们出人意料地表明，真实定价在随机设置中并不占主导地位。有一个随机版本的 FPA，实现了一个严格的较小的无政府状态的价格比其真实的对应物时，每个查询有两个投标人。此外，这种随机 FPA 实现了两个投标人最知名的 PoA，从而显示了非真实性的力量时，结合随机。最后，我们证明了当每次拍卖中有大量的广告商时，任何先验自由拍卖(即使是随机的，非真实的)都不能改善 PoA 的界为2。这些结果应该定性地解释如下。当拍卖压力较小时，随机性和非真实性是有利的。另一方面，如果拍卖压力很大，收益就会减少，实施二级价格拍卖是最佳选择。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficiency+of+Non-Truthful+Auctions+in+Auto-bidding:+The+Power+of+Randomization)|0|
|[Platform Behavior under Market Shocks: A Simulation Framework and Reinforcement-Learning Based Study](https://doi.org/10.1145/3543507.3583523)|Xintong Wang, Gary Qiurui Ma, Alon Eden, Clara Li, Alexander Trott, Stephan Zheng, David C. Parkes|Salesforce Research, USA; Harvard University, USA; Hebrew University of Jerusalem, Israel|We study the behavior of an economic platform (e.g., Amazon, Uber Eats, Instacart) under shocks, such as COVID-19 lockdowns, and the effect of different regulation considerations. To this end, we develop a multi-agent simulation environment of a platform economy in a multi-period setting where shocks may occur and disrupt the economy. Buyers and sellers are heterogeneous and modeled as economically-motivated agents, choosing whether or not to pay fees to access the platform. We use deep reinforcement learning to model the fee-setting and matching behavior of the platform, and consider two major types of regulation frameworks: (1) taxation policies and (2) platform fee restrictions. We offer a number of simulated experiments that cover different market settings and shed light on regulatory tradeoffs. Our results show that while many interventions are ineffective with a sophisticated platform actor, we identify a particular kind of regulation—fixing fees to the optimal, no-shock fees while still allowing a platform to choose how to match buyers and sellers—as holding promise for promoting the efficiency and resilience of the economic system.|我们研究经济平台(如亚马逊、 Uber Eats、 Instacart)在2019冠状病毒疾病封锁等冲击下的行为，以及不同监管考虑因素的影响。为此，我们开发了一个平台经济的多主体仿真环境，在多个时期的设置中，可能会发生冲击和破坏经济。买家和卖家是异构的，他们被塑造成具有经济动机的代理人，选择是否支付费用来访问平台。我们使用深度强化学习来模拟平台的收费设定和匹配行为，并考虑两种主要的监管框架: (1)税收政策和(2)平台收费限制。我们提供了一些模拟实验，涵盖了不同的市场设置，并阐明了监管权衡。我们的研究结果表明，尽管许多干预措施对于一个成熟的平台参与者来说是无效的，但我们确定了一种特殊的监管——将费用固定在最优的、不会引起冲击的费用上，同时仍允许平台选择如何匹配买卖双方——有望提高经济体系的效率和弹性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Platform+Behavior+under+Market+Shocks:+A+Simulation+Framework+and+Reinforcement-Learning+Based+Study)|0|
|[Near-Optimal Experimental Design Under the Budget Constraint in Online Platforms](https://doi.org/10.1145/3543507.3583528)|Yongkang Guo, Yuan Yuan, Jinshan Zhang, Yuqing Kong, Zhihua Zhu, Zheng Cai|Tencent Technology (Shenzhen) Co., Ltd., China; Peking University, China; Zhejiang University, China; Purdue University, USA|A/B testing, or controlled experiments, is the gold standard approach to causally compare the performance of algorithms on online platforms. However, conventional Bernoulli randomization in A/B testing faces many challenges such as spillover and carryover effects. Our study focuses on another challenge, especially for A/B testing on two-sided platforms -- budget constraints. Buyers on two-sided platforms often have limited budgets, where the conventional A/B testing may be infeasible to be applied, partly because two variants of allocation algorithms may conflict and lead some buyers to exceed their budgets if they are implemented simultaneously. We develop a model to describe two-sided platforms where buyers have limited budgets. We then provide an optimal experimental design that guarantees small bias and minimum variance. Bias is lower when there is more budget and a higher supply-demand rate. We test our experimental design on both synthetic data and real-world data, which verifies the theoretical results and shows our advantage compared to Bernoulli randomization.|A/B 测试或对照实验是在线平台上对算法性能进行因果比较的黄金标准方法。然而，传统的伯努利随机化在 A/B 测试中面临着许多挑战，如溢出效应和结转效应。我们的研究侧重于另一个挑战，尤其是在双边平台上的 A/B 测试——预算约束。双边平台上的买家往往预算有限，传统的 A/B 测试可能无法应用，部分原因是两种不同的分配算法可能相互冲突，导致一些买家在同时实施时超出预算。我们开发了一个模型来描述买家预算有限的双边平台。然后，我们提供了一个最佳的实验设计，保证小偏差和最小方差。当有更多的预算和更高的供求比率时，偏差就会降低。我们在合成数据和实际数据上对实验设计进行了检验，验证了理论结果，显示了我们比伯努利随机化方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Near-Optimal+Experimental+Design+Under+the+Budget+Constraint+in+Online+Platforms)|0|
|[A Method to Assess and Explain Disparate Impact in Online Retailing](https://doi.org/10.1145/3543507.3583270)|Rafael BecerrilArreola|University of South Carolina, USA|This paper presents a method for assessing whether algorithmic decision making induces disparate impact in online retailing. The proposed method specifies a statistical design, a sampling algorithm, and a technological setup for data collection through web crawling. The statistical design reduces the dimensionality of the problem and ensures that the data collected are representative, variation-rich, and suitable for the investigation of the causes behind any observed disparities. Implementations of the method can collect data on algorithmic decisions, such as price, recommendations, and delivery fees that can be matched to website visitor demographic data from established sources such as censuses and large scale surveys. The combined data can be used to investigate the presence and causes of disparate impact, potentially helping online retailers audit their algorithms without collecting or holding the demographic data of their users. The proposed method is illustrated in the context of the automated pricing decisions of a leading retailer in the United States. A custom-built platform implemented the method to collect data for nearly 20,000 different grocery products at more than 3,000 randomly-selected zip codes. The data collected indicates that prices are higher for locations with high proportions of minority households. Although these price disparities can be partly attributed to algorithmic biases, they are mainly explained by local factors and therefore can be regarded as business necessities.|本文介绍了一种评估算法决策是否会对网络购物产生不同影响的方法。提出了一种统计设计、抽样算法和网络爬行数据采集技术。统计设计降低了问题的维度，并确保收集的数据具有代表性，变异丰富，适合于调查任何观察到的差异背后的原因。该方法的实施可以收集关于算法决策的数据，如价格、推荐和交付费用，这些数据可以与人口普查和大规模调查等现有来源的网站访问者人口统计数据相匹配。合并后的数据可以用来调查不同影响的存在和原因，有可能帮助在线零售商在不收集或持有用户人口统计数据的情况下审计他们的算法。该方法是在美国领先零售商的自动定价决策的背景下进行的。一个定制的平台实现了这种方法，可以在3000多个随机选择的邮政编码中收集近20,000种不同食品杂货的数据。所收集的数据表明，少数民族家庭比例较高的地区价格较高。虽然这些价格差异可部分归因于算法偏差，但它们主要是由当地因素造成的，因此可被视为商业必需品。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Method+to+Assess+and+Explain+Disparate+Impact+in+Online+Retailing)|0|
|[Simplistic Collection and Labeling Practices Limit the Utility of Benchmark Datasets for Twitter Bot Detection](https://doi.org/10.1145/3543507.3583214)|Chris Hays, Zachary Schutzman, Manish Raghavan, Erin Walk, Philipp Zimmer|Massachusetts Institute of Technology, USA|Accurate bot detection is necessary for the safety and integrity of online platforms. It is also crucial for research on the influence of bots in elections, the spread of misinformation, and financial market manipulation. Platforms deploy infrastructure to flag or remove automated accounts, but their tools and data are not publicly available. Thus, the public must rely on third-party bot detection. These tools employ machine learning and often achieve near perfect performance for classification on existing datasets, suggesting bot detection is accurate, reliable and fit for use in downstream applications. We provide evidence that this is not the case and show that high performance is attributable to limitations in dataset collection and labeling rather than sophistication of the tools. Specifically, we show that simple decision rules -- shallow decision trees trained on a small number of features -- achieve near-state-of-the-art performance on most available datasets and that bot detection datasets, even when combined together, do not generalize well to out-of-sample datasets. Our findings reveal that predictions are highly dependent on each dataset's collection and labeling procedures rather than fundamental differences between bots and humans. These results have important implications for both transparency in sampling and labeling procedures and potential biases in research using existing bot detection tools for pre-processing.|为了保证在线平台的安全性和完整性，精确的机器人检测是必要的。对于研究机器人对选举、错误信息传播和金融市场操纵的影响也至关重要。平台部署基础设施来标记或删除自动帐户，但是它们的工具和数据不公开。因此，公众必须依靠第三方机器人检测。这些工具采用机器学习，并经常达到近乎完美的性能分类的现有数据集，表明机器人检测是准确的，可靠的，适合在下游应用。我们提供的证据表明情况并非如此，并表明高性能是由于数据集收集和标签的局限性，而不是由于工具的复杂性。具体来说，我们展示了简单的决策规则——基于少量特征训练的浅层决策树——在大多数可用数据集上实现了接近最先进的性能，而且机器人检测数据集，即使组合在一起，也不能很好地推广到样本外的数据集。我们的研究结果表明，预测高度依赖于每个数据集的收集和标记程序，而不是机器人和人类之间的根本差异。这些结果对采样和标记过程的透明度以及使用现有的机器人检测工具进行预处理的研究中的潜在偏差都有重要的意义。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Simplistic+Collection+and+Labeling+Practices+Limit+the+Utility+of+Benchmark+Datasets+for+Twitter+Bot+Detection)|0|
|[A Dataset on Malicious Paper Bidding in Peer Review](https://doi.org/10.1145/3543507.3583424)|Steven Jecmen, Minji Yoon, Vincent Conitzer, Nihar B. Shah, Fei Fang|Carnegie Mellon University, USA|In conference peer review, reviewers are often asked to provide "bids" on each submitted paper that express their interest in reviewing that paper. A paper assignment algorithm then uses these bids (along with other data) to compute a high-quality assignment of reviewers to papers. However, this process has been exploited by malicious reviewers who strategically bid in order to unethically manipulate the paper assignment, crucially undermining the peer review process. For example, these reviewers may aim to get assigned to a friend's paper as part of a quid-pro-quo deal. A critical impediment towards creating and evaluating methods to mitigate this issue is the lack of any publicly-available data on malicious paper bidding. In this work, we collect and publicly release a novel dataset to fill this gap, collected from a mock conference activity where participants were instructed to bid either honestly or maliciously. We further provide a descriptive analysis of the bidding behavior, including our categorization of different strategies employed by participants. Finally, we evaluate the ability of each strategy to manipulate the assignment, and also evaluate the performance of some simple algorithms meant to detect malicious bidding. The performance of these detection algorithms can be taken as a baseline for future research on detecting malicious bidding.|在会议同行评审中，评审人员经常被要求对每篇提交的论文提供“出价”，以表达他们对评审该论文的兴趣。然后，论文分配算法使用这些出价(以及其他数据)来计算高质量的论文审稿人分配。然而，这一过程已被恶意审查者利用，他们策略性地投标，以便不道德地操纵论文分配，从而严重破坏同行审查过程。例如，这些评论家可能会把分配到朋友的论文作为交换条件的一部分。创建和评价缓解这一问题的方法的一个关键障碍是缺乏关于恶意纸张投标的任何公开数据。在这项工作中，我们收集并公开发布了一个新的数据集来填补这个空白，这个数据集是从一个模拟会议活动中收集的，在这个活动中，参与者被要求诚实地或恶意地出价。我们进一步提供了一个描述性分析的投标行为，包括我们的分类不同的策略采用的参与者。最后，我们评估了每个策略操纵分配的能力，并且评估了一些简单算法的性能，这些算法旨在检测恶意投标。这些检测算法的性能可以作为今后恶意竞价检测研究的基准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Dataset+on+Malicious+Paper+Bidding+in+Peer+Review)|0|
|[Exploring Social Media for Early Detection of Depression in COVID-19 Patients](https://doi.org/10.1145/3543507.3583867)|Jiageng Wu, Xian Wu, Yining Hua, Shixu Lin, Yefeng Zheng, Jie Yang|Harvard University, USA; Tencent Jarvis Lab, China; Zhejiang University, China|The COVID-19 pandemic has caused substantial damage to global health. Even though three years have passed, the world continues to struggle with the virus. Concerns are growing about the impact of COVID-19 on the mental health of infected individuals, who are more likely to experience depression, which can have long-lasting consequences for both the affected individuals and the world. Detection and intervention at an early stage can reduce the risk of depression in COVID-19 patients. In this paper, we investigated the relationship between COVID-19 infection and depression through social media analysis. Firstly, we managed a dataset of COVID-19 patients that contains information about their social media activity both before and after infection. Secondly,We conducted an extensive analysis of this dataset to investigate the characteristic of COVID-19 patients with a higher risk of depression. Thirdly, we proposed a deep neural network for early prediction of depression risk. This model considers daily mood swings as a psychiatric signal and incorporates textual and emotional characteristics via knowledge distillation. Experimental results demonstrate that our proposed framework outperforms baselines in detecting depression risk, with an AUROC of 0.9317 and an AUPRC of 0.8116. Our model has the potential to enable public health organizations to initiate prompt intervention with high-risk patients|2019冠状病毒疾病大流行对全球健康造成了严重损害。尽管三年过去了，世界仍在与病毒斗争。人们越来越担心2019冠状病毒疾病对感染者心理健康的影响，他们更有可能经历抑郁症，这可能对受影响的个人和世界产生长期后果。早期发现和干预可以降低2019冠状病毒疾病患者患抑郁症的风险。在这篇论文中，我们通过社会媒体分析调查了2019冠状病毒疾病感染和抑郁症之间的关系。首先，我们管理了一个2019冠状病毒疾病患者的数据集，其中包含了他们在感染前后的社交媒体活动信息。其次，我们对这个数据集进行了广泛的分析，以调查2019冠状病毒疾病抑郁风险较高的患者的特征。第三，我们提出了一个深层神经网络用于抑郁风险的早期预测。该模型将日常情绪波动视为一种精神信号，通过知识提取将文本特征和情绪特征结合起来。实验结果表明，我们提出的框架在检测抑郁风险方面优于基线，AUROC 为0.9317，AUPRC 为0.8116。我们的模型有潜力使公共卫生组织能够对高危患者进行及时干预|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Social+Media+for+Early+Detection+of+Depression+in+COVID-19+Patients)|0|
|[Identifying Checkworthy CURE Claims on Twitter](https://doi.org/10.1145/3543507.3583870)|Sujatha Das Gollapalli, Mingzhe Du, SeeKiong Ng|Institute of Data Science, National University of Singapore, Singapore and Nanyang Technological University, Singapore; Institute of Data Science, National University of Singapore, Singapore and Centre for Trusted Internet and Community, National University of Singapore, Singapore|Medical claims on social media, if left unchecked, have the potential to directly affect the well-being of consumers of online health information. However, existing studies on claim detection do not specifically focus on medical cure aspects, neither do they address if a cure claim is “checkworthy", an indicator of whether a claim is potentially beneficial or harmful, if unchecked. In this paper, we address these limitations by compiling CW-CURE, a novel dataset of CURE tweets, namely tweets containing claims on prevention, diagnoses, risks, treatments, and cures of medical conditions. CW-CURE contains tweets on four major health conditions, namely, Alzheimer’s disease, Cancer, Diabetes, and Depression annotated for claims, their “checkworthiness", as well as the different types of claims such as quantitative claim, correlation/causation, personal experience, and future prediction. We describe our processing pipeline for compiling CW-CURE and present classification results on CURE tweets using transformer-based models. In particular, we harness claim-type information obtained with zero-shot learning to show significant improvements in checkworthiness identification. Through CW-CURE, we hope to enable research on models for effective identification and flagging of impactful CURE content, to safeguard the public’s consumption of medical content online.|社交媒体上的医疗声明，如果不加以审查，就有可能直接影响网上健康信息消费者的福祉。然而，现有的索赔检测研究并没有特别关注医疗治疗方面，也没有涉及治疗索赔是否“值得检查”，如果未经检查，这是一个索赔是否具有潜在有益或有害的指标。在本文中，我们通过编译 CW-CURE，一个新的 CURE 推文数据集，即包含关于预防、诊断、风险、治疗和医疗条件治愈的声明的推文，来解决这些局限性。CW-CURE 包含了关于四种主要健康状况的推文，即: 阿尔茨海默氏病、癌症、糖尿病和抑郁症，它们的“检查价值”，以及不同类型的声明，如定量声明、相关性/因果关系、个人经验和未来预测。我们描述了用于编译 CW-CURE 的处理流水线，并使用基于转换器的模型给出了 CURE tweet 的分类结果。特别是，我们利用通过零拍学习获得的索赔类型信息来显示可检查性识别方面的显著改进。通过 CW-CURE，我们希望能够研究有效识别和标记影响 CURE 内容的模型，以保障公众对网上医疗内容的消费。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Identifying+Checkworthy+CURE+Claims+on+Twitter)|0|
|[The Impact of Covid-19 on Online Discussions: the Case Study of the Sanctioned Suicide Forum](https://doi.org/10.1145/3543507.3583879)|Elisa Sartori, Luca Pajola, Giovanni Da San Martino, Mauro Conti|University of Padua, Italy|The COVID-19 pandemic has been at the center of the lives of many of us for at least a couple of years, during which periods of isolation and lockdowns were common. How all that affected our mental well-being, especially the ones’ who were already in distress? To investigate the matter we analyse the online discussions on Sanctioned Suicide, a forum where users discuss suicide-related topics freely. We collected discussions starting from March 2018 (before pandemic) up to July 2022, for a total of 53K threads with 700K comments and 16K users. We investigate the impact of COVID-19 on the discussions in the forum. The data show that covid, while being present in the discussions, especially during the first lockdown, has not been the main reason why new users registered to the forum. However, covid appears to be indirectly connected to other causes of distress for the users, i.e. anxiety for the economy.|2019冠状病毒疾病大流行已经成为我们许多人生活的中心至少有几年的时间，在这期间，隔离和封锁是常见的。这一切是如何影响我们的心理健康的，尤其是那些已经处于痛苦之中的人？为了调查这个问题，我们分析了网上关于“制裁性自杀”的讨论，这是一个用户可以自由讨论自杀相关话题的论坛。我们收集了从2018年3月(大流行之前)到2022年7月的讨论，共有53K 线程和700K 评论和16K 用户。我们调查2019冠状病毒疾病对论坛讨论的影响。数据显示，尽管冠状病毒疾病参与了讨论，尤其是在第一次封锁期间，但这并不是新用户注册论坛的主要原因。然而，冠状病毒疾病似乎与使用者的其他忧虑因素(即对经济的忧虑)有间接关系。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Impact+of+Covid-19+on+Online+Discussions:+the+Case+Study+of+the+Sanctioned+Suicide+Forum)|0|
|[Learning like human annotators: Cyberbullying detection in lengthy social media sessions](https://doi.org/10.1145/3543507.3583873)|Peiling Yi, Arkaitz Zubiaga|Queen Mary University of London, United Kingdom|The inherent characteristic of cyberbullying of being a recurrent attitude calls for the investigation of the problem by looking at social media sessions as a whole, beyond just isolated social media posts. However, the lengthy nature of social media sessions challenges the applicability and performance of session-based cyberbullying detection models. This is especially true when one aims to use state-of-the-art Transformer-based pre-trained language models, which only take inputs of a limited length. In this paper, we address this limitation of transformer models by proposing a conceptually intuitive framework called LS-CB, which enables cyberbullying detection from lengthy social media sessions. LS-CB relies on the intuition that we can effectively aggregate the predictions made by transformer models on smaller sliding windows extracted from lengthy social media sessions, leading to an overall improved performance. Our extensive experiments with six transformer models on two session-based datasets show that LS-CB consistently outperforms three types of competitive baselines including state-of-the-art cyberbullying detection models. In addition, we conduct a set of qualitative analyses to validate the hypotheses that cyberbullying incidents can be detected through aggregated analysis of smaller chunks derived from lengthy social media sessions (H1), and that cyberbullying incidents can occur at different points of the session (H2), hence positing that frequently used text truncation strategies are suboptimal compared to relying on holistic views of sessions. Our research in turn opens an avenue for fine-grained cyberbullying detection within sessions in future work.|网络欺凌作为一种反复出现的态度，其固有的特征要求我们将社交媒体会议作为一个整体来研究这个问题，而不仅仅是孤立的社交媒体帖子。然而，社交媒体会话的冗长性质对基于会话的网络欺凌检测模型的适用性和性能提出了挑战。当一个人的目标是使用基于最先进变压器的预先训练的语言模型时，这种情况尤其如此，因为这种语言模型只接受有限长度的输入。在本文中，我们通过提出一个称为 LS-CB 的概念直观的框架来解决变压器模型的这种局限性，该框架能够从冗长的社交媒体会话中检测出网络欺凌。LS-CB 依赖于这样一种直觉，即我们可以有效地将变压器模型的预测聚合在从冗长的社交媒体会话中提取的较小的滑动窗口上，从而导致整体性能的提高。我们在两个基于会话的数据集上对六个变压器模型进行了广泛的实验，结果表明 LS-CB 始终优于包括最先进的网络欺凌检测模型在内的三类竞争基线。此外，我们还进行了一系列定性分析，以验证网络欺凌事件可以通过对冗长的社交媒体会话(H1)产生的较小块进行聚合分析来检测的假设，并且网络欺凌事件可以发生在会话的不同点(H2) ，因此认为经常使用的文本截断策略与依赖会话的整体观点相比是次优的。我们的研究反过来又为未来工作中的会话中细粒度的网络欺凌检测开辟了一条道路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+like+human+annotators:+Cyberbullying+detection+in+lengthy+social+media+sessions)|0|
|[Vertical Federated Knowledge Transfer via Representation Distillation for Healthcare Collaboration Networks](https://doi.org/10.1145/3543507.3583874)|Chungju Huang, Leye Wang, Xiao Han|School of Computer Science, Peking University, China; School of Information Management and Engineering, Shanghai University of Finance and Economics, China|Collaboration between healthcare institutions can significantly lessen the imbalance in medical resources across various geographic areas. However, directly sharing diagnostic information between institutions is typically not permitted due to the protection of patients' highly sensitive privacy. As a novel privacy-preserving machine learning paradigm, federated learning (FL) makes it possible to maximize the data utility among multiple medical institutions. These feature-enrichment FL techniques are referred to as vertical FL (VFL). Traditional VFL can only benefit multi-parties' shared samples, which strongly restricts its application scope. In order to improve the information-sharing capability and innovation of various healthcare-related institutions, and then to establish a next-generation open medical collaboration network, we propose a unified framework for vertical federated knowledge transfer mechanism (VFedTrans) based on a novel cross-hospital representation distillation component. Specifically, our framework includes three steps. First, shared samples' federated representations are extracted by collaboratively modeling multi-parties' joint features with current efficient vertical federated representation learning methods. Second, for each hospital, we learn a local-representation-distilled module, which can transfer the knowledge from shared samples' federated representations to enrich local samples' representations. Finally, each hospital can leverage local samples' representations enriched by the distillation module to boost arbitrary downstream machine learning tasks. The experiments on real-life medical datasets verify the knowledge transfer effectiveness of our framework.|医疗机构之间的合作可以显著减少各地区医疗资源的不平衡。然而，由于对患者高度敏感的隐私的保护，医疗机构之间通常不允许直接共享诊断信息。联邦学习(FL)作为一种新的保护隐私的机器学习范式，使得多个医疗机构之间的数据效用最大化成为可能。这些特征丰富的 FL 技术被称为垂直 FL (VFL)。传统的 VFL 只能有利于多方共享样本，这严重制约了它的应用范围。为了提高各医疗机构的信息共享能力和创新能力，进而建立下一代开放式医疗协作网络，提出了一种基于新型跨医院表示蒸馏组件的垂直联邦知识转移机制(VFedTrans)统一框架。具体来说，我们的框架包括三个步骤。首先，利用现有高效的垂直联邦表示学习方法，对多方联合特征进行协同建模，提取共享样本的联邦表示;。其次，针对每个医院，我们学习了一个局部表示提取模块，该模块可以从共享样本的联邦表示中转移知识以丰富局部样本的表示。最后，每家医院可以利用蒸馏模块丰富的本地样本表示来增强任意下游机器学习任务。在实际医学数据集上的实验验证了该框架的知识转移效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Vertical+Federated+Knowledge+Transfer+via+Representation+Distillation+for+Healthcare+Collaboration+Networks)|0|
|[On Graph Time-Series Representations for Temporal Networks](https://doi.org/10.1145/3543873.3587301)|Ryan A. Rossi, Nesreen K. Ahmed, Namyong Park|Meta, USA; Intel Labs, USA; Adobe Research, USA|Representations of temporal networks arising from a stream of edges lie at the heart of models learned on it and its performance on downstream applications. While previous work on dynamic modeling and embedding have focused on representing a stream of timestamped edges using a time-series of graphs based on a specific time-scale τ (e.g., 1 month), we introduce the notion of an ϵ -graph time-series that uses a fixed number of edges for each graph, and show its effectiveness in capturing fundamental structural graph statistics over time. The results indicate that the ϵ -graph time-series representation effectively captures the structural properties of the graphs across time whereas the commonly used τ -graph time-series representation captures the frequency of edges and temporal patterns with respect to their arrival in the application time. These results have many important implications especially on the design of new GNN-based models for temporal networks as well as for understanding existing models and their limitations.|由边流产生的时间网络的表示是在它上面学习的模型及其在下游应用中的性能的核心。以往的动态建模和嵌入工作主要集中在使用基于特定时间尺度 τ (例如，1个月)的时间序列图表示一系列带时间戳的边，我们引入了 ε 图时间序列的概念，每个图使用固定数目的边，并显示了其在获取基本结构图随时间变化的统计信息方面的有效性。结果表明，ε 图时间序列表示方法能够有效地捕捉图的跨时间结构特性，而常用的 τ 图时间序列表示方法能够捕捉边和时间模式在应用时间到达的频率。这些结果对于设计新的基于 GNN 的时态网络模型以及理解现有模型及其局限性具有重要意义。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Graph+Time-Series+Representations+for+Temporal+Networks)|0|
|[Lower Risks, Better Choices: Stock Correlation Based Portfolio Selection in Stock Markets](https://doi.org/10.1145/3543873.3587298)|Di Luo, Weiheng Liao, Rui Yan|Renmin University of China, China; Made by Data, United Kingdom and Renmin University of China, China|Over the past few years, we’ve seen a huge interest in applying AI techniques to develop investment strategies both in academia and the finance industry. However, we note that generating returns is not always the sole investment objective. Take large pension funds for example, they are considerably more risk-averse as opposed to profit-seeking. With this observation, we propose a Risk-balanced Deep Portfolio Constructor (RDPC) that takes risk into explicit consideration. RDPC is an end-to-end reinforcement learning-based transformer trained to optimize both returns and risk, with a hard attention mechanism that learns the relationship between asset pairs, imitating the powerful pairs trading strategy widely adopted by many investors. Experiments on real-world data show that RDPC achieves state-of-the-art performance not just on risk metrics such as maximum drawdown, but also on risk-adjusted returns metrics including Sharpe ratio and Calmar ratio.|在过去的几年里，我们已经看到了在学术界和金融业应用人工智能技术开发投资策略的巨大兴趣。然而，我们注意到产生回报并不总是唯一的投资目标。以大型养老基金为例，它们的风险厌恶程度远高于追逐利润的程度。根据这一观察，我们提出了一个风险平衡的深度投资组合构造函数(RDPC) ，它将风险考虑在内。RDPC 是一种基于端到端强化学习的变换器，训练用于优化收益和风险，具有硬注意机制，学习资产对之间的关系，模仿许多投资者广泛采用的强大的成对交易策略。对实际数据的实验表明，RDPC 不仅在最大提取率等风险指标上取得了最佳表现，而且在 Sharpe 比率和 Calmar 比率等风险调整后的收益指标上也取得了最佳表现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lower+Risks,+Better+Choices:+Stock+Correlation+Based+Portfolio+Selection+in+Stock+Markets)|0|
|[Creation and Analysis of a Corpus of Scam Emails Targeting Universities](https://doi.org/10.1145/3543873.3587303)|Grace Ciambrone, Shomir Wilson|Human Language Technologies Lab, Pennsylvania State University, USA|Email-based scams pose a threat to the personally identifiable information and financial safety of all email users. Within a university environment, the risks are potentially greater: traditional students (i.e., within an age range typical of college students) often lack the experience and knowledge of older email users. By understanding the topics, temporal trends, and other patterns of scam emails targeting universities, these institutions can be better equipped to reduce this threat by improving their filtering methods and educating their users. While anecdotal evidence suggests common topics and trends in these scams, the empirical evidence is limited. Observing that large universities are uniquely positioned to gather and share information about email scams, we built a corpus of 5,155 English language scam emails scraped from information security websites of five large universities in the United States. We use Latent Dirichlet Allocation (LDA) topic modelling to assess the landscape and trends of scam emails sent to university addresses. We examine themes chronologically and observe that topics vary over time, indicating changes in scammer strategies. For example, scams targeting students with disabilities have steadily risen in popularity since they first appeared in 2015, while password scams experienced a boom in 2016 but have lessened in recent years. To encourage further research to mitigate the threat of email scams, we release this corpus for others to study.|基于电子邮件的骗案对所有电子邮件用户的个人身份信息和财务安全构成威胁。在大学环境中，风险可能更大: 传统的学生(即在大学生的典型年龄范围内)往往缺乏老年电子邮件用户的经验和知识。通过了解针对大学的电子邮件诈骗的主题、时间趋势和其他模式，这些机构可以通过改进过滤方法和教育用户来更好地减少这种威胁。虽然轶事证据表明了这些骗局的共同主题和趋势，但经验证明有限。考虑到大型大学在收集和分享电子邮件诈骗信息方面的独特地位，我们建立了一个从美国五所大型大学的信息安全网站上搜集到的5,155封英语诈骗电子邮件的语料库。我们使用隐含狄利克雷分布主题模型来评估发送到大学地址的诈骗电子邮件的情况和趋势。我们按时间顺序检查主题，并观察到主题随着时间的推移而变化，表明骗子策略的变化。例如，针对残疾学生的欺诈自2015年首次出现以来，受欢迎程度稳步上升，而密码欺诈在2016年经历了一次繁荣，但近年来有所减少。为了鼓励进一步研究以减轻电子邮件诈骗的威胁，我们发布了这个语料库供其他人研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Creation+and+Analysis+of+a+Corpus+of+Scam+Emails+Targeting+Universities)|0|
|[STRUM: Extractive Aspect-Based Contrastive Summarization](https://doi.org/10.1145/3543873.3587304)|Beliz Gunel, Sandeep Tata, Marc Najork|Google, USA|Comparative decisions, such as picking between two cars or deciding between two hiking trails, require the users to visit multiple webpages and contrast the choices along relevant aspects. Given the impressive capabilities of pre-trained large language models [4, 11], we ask whether they can help automate such analysis. We refer to this task as extractive aspect-based contrastive summarization which involves constructing a structured summary that compares the choices along relevant aspects. In this paper, we propose a novel method called STRUM for this task that can generalize across domains without requiring any human-written summaries or fixed aspect list as supervision. Given a set of relevant input webpages, STRUM solves this problem using two pre-trained T5-based [11] large language models: first one fine-tuned for aspect and value extraction [14], and second one fine-tuned for natural language inference [13]. We showcase the abilities of our method across different domains, identify shortcomings, and discuss questions that we believe will be critical in this new line of research.|比较决策，例如在两辆汽车之间选择或在两条徒步路线之间选择，需要用户访问多个网页，并沿相关方面对比选择。鉴于预先训练的大型语言模型的令人印象深刻的能力[4,11] ，我们问他们是否能够帮助自动化这样的分析。我们把这个任务称为基于方面的对比总结，它包括构建一个结构化的总结，比较相关方面的选择。在本文中，我们提出了一种新的方法，称为 STRUM 的任务，可以在不需要任何人写摘要或固定的方面列表作为监督跨领域泛化。给定一组相关的输入网页，STRUM 使用两个预先训练的基于 T5的[11]大型语言模型来解决这个问题: 第一个模型针对方面和值提取进行了微调[14] ，第二个模型针对自然语言推理进行了微调[13]。我们展示我们的方法在不同领域的能力，确定缺点，并讨论问题，我们认为将是这个新的研究路线的关键。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=STRUM:+Extractive+Aspect-Based+Contrastive+Summarization)|0|
|[gDoc: Automatic Generation of Structured API Documentation](https://doi.org/10.1145/3543873.3587310)|Shujun Wang, Yongqiang Tian, Dengcheng He|Alibaba Group, China|Generating and maintaining API documentation with integrity and consistency can be time-consuming and expensive for evolving APIs. To solve this problem, several approaches have been proposed to automatically generate high-quality API documentation based on a combination of knowledge from different web sources. However, current researches are weak in handling unpopular APIs and cannot generate structured API documentation. Hence, in this poster, we propose a hybrid technique(namely \textit{gDoc}) for the automatic generation of structured API documentation. We first present a fine-grained search-based strategy to generate the description for partial API parameters via computing the relevance between various APIs, ensuring the consistency of API documentation. Then, we employ the cross-modal pretraining Seq2Seq model M6 to generate a structured API document for each API, which treats the document generation problem as a translation problem. Finally, we propose a heuristic algorithm to extract practical parameter examples from API request logs. The experiments evaluated on the online system show that this work's approach significantly improves the effectiveness and efficiency of API document generation.|生成和维护具有完整性和一致性的 API 文档对于不断发展的 API 来说是非常耗时和昂贵的。为了解决这个问题，已经提出了几种方法来自动生成高质量的 API 文档的基础上的知识组合从不同的网络来源。然而，目前的研究在处理不流行的 API 方面还很薄弱，无法生成结构化的 API 文档。因此，在这张海报中，我们提出了一种混合技术(即 texttit { gDoc }) ，用于自动生成结构化 API 文档。我们首先提出了一种基于细粒度搜索的策略，通过计算不同 API 之间的相关性来生成部分 API 参数的描述，从而确保 API 文档的一致性。然后，我们使用跨模式预训练 Seq2Seq 模型 M6为每个 API 生成一个结构化的 API 文档，它将文档生成问题作为一个翻译问题来处理。最后，提出了一种启发式算法，用于从 API 请求日志中提取实际参数示例。在线系统的实验结果表明，该方法显著提高了 API 文档生成的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=gDoc:+Automatic+Generation+of+Structured+API+Documentation)|0|
|[Reduce API Debugging Overhead via Knowledge Prepositioning](https://doi.org/10.1145/3543873.3587311)|Shujun Wang, Yongqiang Tian, Dengcheng He|Alibaba Group, China|OpenAPI indicates a behavior where producers offer Application Programming Interfaces (APIs) to help end-users access their data, resources, and services. Generally, API has many parameters that need to be entered. However, it is challenging for users to understand and document these parameters correctly. This paper develops an API workbench to help users learn and debug APIs. Based on this workbench, much exploratory work has been proposed to reduce the overhead of learning and debugging APIs. We explore the knowledge, such as parameter characteristics (e.g., enumerability) and constraints (e.g., maximum/minimum value), from the massive API call logs to narrow the range of parameter values. Then, we propose a fine-grained approach to enrich the API documentation by extracting dependency knowledge between APIs. Finally, we present a learning-based prediction method to predict API execution results before the API is called, significantly reducing user debugging cycles. The experiments evaluated on the online system show that this work's approach substantially improves the user experience of debugging OpenAPIs.|OpenAPI 指的是生产者提供应用程序编程接口(API)来帮助终端用户访问他们的数据、资源和服务的行为。通常，API 有许多需要输入的参数。然而，对于用户来说，正确理解和记录这些参数是一个挑战。本文开发了一个 API 工作台来帮助用户学习和调试 API。基于这个工作台，已经提出了许多探索性的工作，以减少学习和调试 API 的开销。我们研究了大量 API 调用日志中的知识，如参数特征(例如可枚举性)和约束(例如最大/最小值) ，以缩小参数值的范围。然后，我们提出了一种细粒度的方法，通过提取 API 之间的依赖性知识来丰富 API 文档。最后，我们提出了一种基于学习的预测方法，可以在调用 API 之前预测 API 的执行结果，从而大大缩短用户调试周期。在线系统的实验表明，该方法大大提高了用户调试 OpenAPI 的体验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reduce+API+Debugging+Overhead+via+Knowledge+Prepositioning)|0|
|[Augmenting Visualizations with Predictive and Investigative Insights to Facilitate Decision Making](https://doi.org/10.1145/3543873.3587317)|Md Main Uddin Rony, Fan Du, Ryan A. Rossi, Jane Hoffswell, Niyati Chhaya, Iftikhar Ahamath Burhanuddin, Eunyee Koh|Adobe Research, India; Adobe Research, USA; University of Maryland, USA|Many people find it difficult to comprehend basic charts on the web, let alone make effective decisions from them. To address this gap, several ML models aim to automatically detect useful insights from charts and narrate them in a simpler textual format. However, most of these solutions can only detect basic factual insights (a.k.a. descriptive insights) that are already present in the chart, which may help with chart comprehension, but not decision-making. In this work, we study whether more advanced predictive and investigative insights can help users understand what will happen next and what actions they should take. These advanced insights can help decision-makers better understand the reasons behind anomaly events, predict future unfolding trends, and recommend possible actions for optimizing business outcomes. Through a study with 18 participants, we found that predictive and investigative insights lead to more insights recorded by users on average and better effectiveness ratings.|许多人发现很难理解网上的基本图表，更不用说从中做出有效的决策了。为了弥补这一差距，一些机器学习模型的目标是从图表中自动发现有用的见解，并以更简单的文本格式叙述它们。然而，这些解决方案中的大多数只能检测出图表中已经存在的基本事实洞察力(又称描述性洞察力) ，这可能有助于图表理解，但不能帮助决策。在这项工作中，我们研究是否更先进的预测和调查洞察力可以帮助用户了解接下来会发生什么，他们应该采取什么行动。这些先进的见解可以帮助决策者更好地理解异常事件背后的原因，预测未来的发展趋势，并为优化业务结果提出可能的行动建议。通过一项有18名参与者参与的研究，我们发现预测性和调查性的洞察力可以让用户记录下更多的洞察力和更好的效率评级。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Augmenting+Visualizations+with+Predictive+and+Investigative+Insights+to+Facilitate+Decision+Making)|0|
|[OntoEval: an Automated Ontology Evaluation System](https://doi.org/10.1145/3543873.3587318)|Antonio Zaitoun, Tomer Sagi, Katja Hose|Aalborg University, Denmark; University of Haifa, Israel; Aalborg University, Denmark and TU Wien, Austria|Developing semantically-aware web services requires comprehensive and accurate ontologies. Evaluating an existing ontology or adapting it is a labor-intensive and complex task for which no automated tools exist. Nevertheless, in this paper we propose a tool that aims at making this vision come true, i.e., we present a tool for the automated evaluation of ontologies that allows one to rapidly assess an ontology’s coverage of a domain and identify specific problems in the ontology’s structure. The tool evaluates the domain coverage and correctness of parent-child relations of a given ontology based on domain information derived from a text corpus representing the domain. The tool provides both overall statistics and detailed analysis of sub-graphs of the ontology. In the demo, we show how these features can be used for the iterative improvement of an ontology.|开发语义感知的 Web 服务需要全面和准确的本体。评估一个已存在的本体或对其进行适应是一个劳动密集型和复杂的任务，没有自动化的工具可以完成。然而，在本文中，我们提出了一个工具，旨在使这个愿景成真，即，我们提出了一个工具，为本体的自动评估，允许一个人快速评估本体的覆盖范围的领域，并确定具体问题的本体的结构。该工具基于从表示领域的文本语料库中获得的领域信息，评估给定本体的领域覆盖率和父子关系的正确性。该工具提供了本体的整体统计和子图的详细分析。在演示中，我们展示了如何将这些特性用于本体的迭代改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OntoEval:+an+Automated+Ontology+Evaluation+System)|0|
|[Public Spot Instance Dataset Archive Service](https://doi.org/10.1145/3543873.3587314)|Kyunghwan Kim, Subin Park, Jaeil Hwang, Hyeonyoung Lee, Seokhyeon Kang, Kyungyong Lee|Distributed Data Processing System Lab, CS Department, Kookmin University, Republic of Korea|Spot instances offered by major cloud vendors allow users to use cloud instances cost-effectively but with the risk of sudden instance interruption. To enable efficient use of spot instances by users, cloud vendors provide various datasets that reflect the current status of spot instance services, such as savings ratio, interrupt ratio, and instant availability. However, this information is scattered, and they require distinct access mechanisms and pose query constraints. Hence, ordinary users find it difficult to use the dataset to optimize spot instance usage. To resolve this issue, we propose a multi-cloud spot instance dataset service that is publicly available. This will help cloud users and system researchers to use spot instances from multiple cloud vendors to build a cost-efficient and reliable environment expediting cloud system research.|主要云供应商提供的现货实例允许用户以具有成本效益的方式使用云实例，但存在实例突然中断的风险。为了使用户能够有效地使用现场实例，云供应商提供了反映现场实例服务当前状态的各种数据集，例如节约率、中断率和即时可用性。但是，这些信息是分散的，它们需要不同的访问机制并构成查询约束。因此，普通用户发现很难使用数据集来优化现场实例的使用。为了解决这个问题，我们提出了一个公开可用的多云点实例数据集服务。这将有助于云用户和系统研究人员使用来自多个云供应商的现场实例来构建一个成本效益高、可靠的环境，从而加快云系统研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Public+Spot+Instance+Dataset+Archive+Service)|0|
|[Towards Deeper Graph Neural Networks via Layer-Adaptive](https://doi.org/10.1145/3543873.3587323)|Bingbing Xu, Bin Xie, Huawei Shen|Tiangong University, China; Institute of Computing Technology, Chinese Academy of Sciences, China|Graph neural networks have achieved state-of-the-art performance on graph-related tasks. Previous methods observed that GNNs’ performance degrades as the number of layers increases and attributed this phenomenon to over-smoothing caused by the stacked propagation. However, we proved experimentally and theoretically that it is overfitting rather than propagation that causes performance degradation. We propose a novel framework: layer-adaptive GNN (LAGNN) consisting of two modules: adaptive layer selection and random Droplayer, which can adaptively determine the number of layers and thus alleviate overfitting. We attached this general framework to two representative GNNs and achieved consistency improvements on six representative datasets.|图形神经网络在处理与图形相关的任务时取得了最先进的性能。先前的方法观察到，GNN 的性能随着层数的增加而下降，并将这种现象归因于叠加传播引起的过平滑。然而，我们从实验和理论上证明了过拟合而不是传播会导致性能下降。我们提出了一个新的框架: 层自适应 GNN (LAGNN) ，它由两个模块组成: 自适应层选择和随机 Droplayer，可以自适应地确定层数，从而减少过拟合。我们将这个总体框架附加到两个有代表性的 GNN 上，并在六个有代表性的数据集上实现了一致性改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Deeper+Graph+Neural+Networks+via+Layer-Adaptive)|0|
|[qEndpoint: A Wikidata SPARQL endpoint on commodity hardware](https://doi.org/10.1145/3543873.3587327)|Antoine Willerval, Angela Bonifati, Dennis Diefenbach|LIRIS, University of Lyon 1, France and The QA Company, France; LIRIS, University of Lyon 1, France; The QA Company, France|In this work, we demonstrate how to setup a Wikidata SPARQL endpoint on commodity hardware resources. We achieve this by using a novel triple store called qEndpoint, which uses a read-only partition based on HDT and a write partition based on RDF4J. We show that qEndpoint can index and query the entire Wikidata dump (currently 17 billion triples) on a machine with 600GB SSD, 10 cores and 10GB of RAM, while keeping the query performance comparable with other SPARQL endpoints indexing Wikidata. In a nutshell, we present the first SPARQL endpoint over Wikidata that can run on commodity hardware while preserving the query run time of existing implementations. Our work goes in the direction of democratizing the access to Wikidata as well as to other large-scale Knowledge Graphs published on the Web. The source code of qEndpoint along with the query workloads are publicly available.|在这项工作中，我们将演示如何在商品硬件资源上建立一个维基数据查询服务。我们通过使用一种称为 qEndpoint 的新型三重存储来实现这一点，它使用基于 HDT 的只读分区和基于 RDF4J 的写分区。我们展示了 qEndpoint 可以在600GB SSD、10个核和10GB RAM 的机器上索引和查询整个 Wikidata 转储(目前是170亿个三元组) ，同时保持查询性能与其他 SPARQL 端点索引 Wikidata 相当。简而言之，我们在 Wikidata 上提供了第一个 SPARQL 端点，它可以在普通硬件上运行，同时保留现有实现的查询运行时。我们的工作是朝着民主化的方向进入 Wikidata，以及其他大规模的知识图表出版在网上。QEndpoint 的源代码以及查询工作负载都是公开的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=qEndpoint:+A+Wikidata+SPARQL+endpoint+on+commodity+hardware)|0|
|[Metadatamatic: A Web application to Create a Dataset Description](https://doi.org/10.1145/3543873.3587328)|Pierre Maillot, Olivier Corby, Catherine Faron, Fabien Gandon, Franck Michel|I3S, Univ. Cote d'Azur, CNRS, Inria, France; I3S, Univ. Cote d'Azur, Inria, CNRS, France|This article introduces Metadatamatic, an open-source, online, user-friendly tool for generating the description of a knowledge base. It supports the description of any RDF dataset via a user-friendly web form that does not require prior knowledge of the vocabularies begin used, and can enrich the description with automatically generated statistics if the dataset is accessible from a public SPARQL endpoint. We discuss the models and methods behind the tool, and present some initial results suggesting that Metadatamatic can help in increasing the visibility of public knowledge bases.|本文介绍了 Metadatamatic，这是一个开源的、在线的、用户友好的工具，用于生成知识库的描述。它支持通过用户友好的 Web 表单描述任何 RDF 数据集，不需要事先了解开始使用的词汇表，并且可以通过自动生成的统计数据来丰富描述，如果数据集可以从公共 SPARQL 端点访问的话。我们讨论了该工具背后的模型和方法，并提出了一些初步结果，表明元数据可以帮助提高公共知识库的可见性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Metadatamatic:+A+Web+application+to+Create+a+Dataset+Description)|0|
|[What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis](https://doi.org/10.1145/3543873.3587324)|Xiang Deng, Vasilisa Bashlovkina, Feng Han, Simon Baumgartner, Michael Bendersky|Google Research, USA; The Ohio State University, USA|Market sentiment analysis on social media content requires knowledge of both financial markets and social media jargon, which makes it a challenging task for human raters. The resulting lack of high-quality labeled data stands in the way of conventional supervised learning methods. Instead, we approach this problem using semi-supervised learning with a large language model (LLM). Our pipeline generates weak financial sentiment labels for Reddit posts with an LLM and then uses that data to train a small model that can be served in production. We find that prompting the LLM to produce Chain-of-Thought summaries and forcing it through several reasoning paths helps generate more stable and accurate labels, while using a regression loss further improves distillation quality. With only a handful of prompts, the final model performs on par with existing supervised models. Though production applications of our model are limited by ethical considerations, the model's competitive performance points to the great potential of using LLMs for tasks that otherwise require skill-intensive annotation.|对社交媒体内容的市场情绪分析需要金融市场和社交媒体术语的双重知识，这对人类评估者来说是一项具有挑战性的任务。由此导致的高质量标记数据的缺乏阻碍了传统的监督式学习检测方法。相反，我们使用大语言模型(LLM)的半监督学习来解决这个问题。我们的管道为 Reddit 的 LLM 帖子生成了弱的金融情绪标签，然后使用这些数据来训练一个可以在生产中使用的小型模型。我们发现，提示 LLM 生成思想链摘要并强制它通过几个推理路径有助于生成更稳定和准确的标签，同时使用回归损失进一步提高蒸馏质量。由于只有少量的提示，最终模型的表现与现有的监督模型不相上下。尽管我们的模型的生产应用受到伦理考虑的限制，但是模型的竞争性性能指出了使用 LLM 完成需要技能密集型注释的任务的巨大潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What+do+LLMs+Know+about+Financial+Markets?+A+Case+Study+on+Reddit+Market+Sentiment+Analysis)|0|
|[GAT-DNS: DNS Multivariate Time Series Prediction Model Based on Graph Attention Network](https://doi.org/10.1145/3543873.3587329)|Xiaofeng Lu, Xiaoyu Zhang, Pietro Lió|Beijing University of Posts and Telecommunications, China; University of Cambridge, United Kingdom|As one of the most basic services of the Internet, DNS has suffered a lot of attacks. Existing attack detection methods rely on the learning of malicious samples, so it is difficult to detect new attacks and long-period attacks. This paper transforms the DNS data flow into time series, and proposes a DNS anomaly detection method based on graph attention network and graph embedding (GAT-DNS). GAT-DNS establishes a multivariate time series model to depict the DNS service status. When the actual flow of a feature exceeds the predicted range, it is considered that abnormal DNS behavior is found. In this paper, vertex dependency is proposed to describe the dependency between features. The features with high vertex dependency values are deleted to achieve model compression. This improves the system efficiency. Experiments on open data sets show that compared with the latest AD-Bop and QLAD methods, GAT-DNS method not only improves the precision, recall and F1 value, but also improves the time efficiency of the model.|作为互联网最基本的服务之一，域名解析系统遭受了大量的攻击。现有的攻击检测方法依赖于对恶意样本的学习，因此很难检测到新的攻击和长周期的攻击。本文将 DNS 数据流转换为时间序列，提出了一种基于图注意网络和图嵌入(gat-DNS)的 DNS 异常检测方法。GAT-DNS 建立了描述 DNS 服务状态的多变量时间序列模型。当一个特征的实际流量超过预测范围时，就认为发现了异常的 DNS 行为。本文提出用顶点依赖来描述特征之间的依赖关系。删除顶点依赖度高的特征以实现模型压缩。这提高了系统的效率。对开放数据集的实验表明，与最新的 AD-Bop 和 QLAD 方法相比，GAT-DNS 方法不仅提高了模型的精度、召回率和 F1值，而且提高了模型的时间效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GAT-DNS:+DNS+Multivariate+Time+Series+Prediction+Model+Based+on+Graph+Attention+Network)|0|
|[Weedle: Composable Dashboard for Data-Centric NLP in Computational Notebooks](https://doi.org/10.1145/3543873.3587330)|Nahyun Kwon, Hannah Kim, Sajjadur Rahman, Dan Zhang, Estevam Hruschka|Megagon Labs, USA; Texas A&M University, USA|Data-centric NLP is a highly iterative process requiring careful exploration of text data throughout entire model development lifecycle. Unfortunately, existing data exploration tools are not suitable to support data-centric NLP because of workflow discontinuity and lack of support for unstructured text. In response, we propose Weedle, a seamless and customizable exploratory text analysis system for data-centric NLP. Weedle is equipped with built-in text transformation operations and a suite of visual analysis features. With its widget, users can compose customizable dashboards interactively and programmatically in computational notebooks.|以数据为中心的 NLP 是一个高度迭代的过程，需要在整个模型开发生命周期中仔细探索文本数据。遗憾的是，由于工作流的不连续性和缺乏对非结构化文本的支持，现有的数据探索工具不适合支持以数据为中心的自然语言处理。作为回应，我们提出了 Weedle，一个无缝和可定制的探索性文本分析系统，用于以数据为中心的自然语言处理。Weedle 配备了内置的文本转换操作和一套可视化分析功能。通过它的小部件，用户可以在计算笔记本中交互式和编程式地组合可定制的仪表板。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weedle:+Composable+Dashboard+for+Data-Centric+NLP+in+Computational+Notebooks)|0|
|[The Web Data Commons Schema.org Data Set Series](https://doi.org/10.1145/3543873.3587331)|Alexander Brinkmann, Anna Primpeli, Christian Bizer|University of Mannheim, Germany|Millions of websites have started to annotate structured data within their HTML pages using the schema.org vocabulary. Popular entity types annotated with schema.org terms are products, local businesses, events, and job postings. The Web Data Commons project has been extracting schema.org data from the Common Crawl every year since 2013 and offers the extracted data for public download in the form of the schema.org data set series. The latest release in the series consists of 106 billion RDF quads describing 3.1 billion entities. The entity descriptions originate from 12.8 million different websites. From a Web Science perspective, the data set series lays the foundation for analyzing the adoption process of schema.org annotations on the Web over the past decade. From a machine learning perspective, the annotations provide a large pool of training data for tasks such as product matching, product or job categorization, information extraction, or question answering. This poster gives an overview of the content of the Web Data Commons schema.org data set series. It highlights trends in the adoption of schema.org annotations on the Web and discusses how the annotations are being used as training data for machine learning applications.|数以百万计的网站已经开始使用 schema.org 词汇表在 HTML 页面中注释结构化数据。用 schema.org 术语注释的流行实体类型是产品、本地企业、事件和招聘信息。自2013年以来，Web Data Commons 项目每年都从 Common Crawl 中提取 schema.org 数据，并以 schema.org 数据集系列的形式提供提取的数据供公众下载。该系列的最新版本由1060亿个 RDF 四元组组成，描述了31亿个实体。实体描述来自1280万个不同的网站。从 Web 科学的角度来看，数据集系列为分析过去十年中 Schema.org 注释在 Web 上的采用过程奠定了基础。从机器学习的角度来看，注释为产品匹配、产品或工作分类、信息抽取或问题回答等任务提供了大量的培训数据。本海报概述了 Web Data Commons schema.org 数据集系列的内容。它强调了在 Web 上采用 schema.org 注释的趋势，并讨论了如何将这些注释用作机器学习应用程序的培训数据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Web+Data+Commons+Schema.org+Data+Set+Series)|0|
|[Class Cardinality Comparison as a Fermi Problem](https://doi.org/10.1145/3543873.3587334)|Shrestha Ghosh, Simon Razniewski, Gerhard Weikum|Max Planck Institute for Informatics, Germany and Saarland University, Germany; Max Planck Institute for Informatics, Germany|Questions on class cardinality comparisons are quite tricky to answer and come with its own challenges. They require some kind of reasoning since web documents and knowledge bases, indispensable sources of information, rarely store direct answers to questions, such as, ``Are there more astronauts or Physics Nobel Laureates?'' We tackle questions on class cardinality comparison by tapping into three sources for absolute cardinalities as well as the cardinalities of orthogonal subgroups of the classes. We propose novel techniques for aggregating signals with partial coverage for more reliable estimates and evaluate them on a dataset of 4005 class pairs, achieving an accuracy of 83.7%.|关于类基数比较的问题很难回答，而且有其自身的挑战。它们需要某种推理，因为网络文档和知识库是不可或缺的信息来源，很少存储对诸如“有更多的宇航员或物理学诺贝尔奖获得者吗?”我们利用类的绝对基数和正交子群的基数的三个来源来解决类基数比较的问题。我们提出了一种新的技术，用于聚集具有部分覆盖的信号，以获得更可靠的估计，并在4005个类对的数据集上对它们进行评估，实现了83.7% 的准确率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Class+Cardinality+Comparison+as+a+Fermi+Problem)|0|
|[Towards a Critical Open-Source Software Database](https://doi.org/10.1145/3543873.3587336)|Tobias Dam, Lukas Daniel Klausner, Sebastian Neumaier|St. Pölten University of Applied Sciences, Austria|Open-source software (OSS) plays a vital role in the modern software ecosystem. However, the maintenance and sustainability of OSS projects can be challenging. In this paper, we present the CrOSSD project, which aims to build a database of OSS projects and measure their current project "health" status. In the project, we will use both quantitative and qualitative metrics to evaluate the health of OSS projects. The quantitative metrics will be gathered through automated crawling of meta information such as the number of contributors, commits and lines of code. Qualitative metrics will be gathered for selected "critical" projects through manual analysis and automated tools, including aspects such as sustainability, funding, community engagement and adherence to security policies. The results of the analysis will be presented on a user-friendly web platform, which will allow users to view the health of individual OSS projects as well as the overall health of the OSS ecosystem. With this approach, the CrOSSD project provides a comprehensive and up-to-date view of the health of OSS projects, making it easier for developers, maintainers and other stakeholders to understand the health of OSS projects and make informed decisions about their use and maintenance.|开源软件在现代软件生态系统中扮演着重要的角色。然而，开放源码软件项目的维护和可持续性可能是具有挑战性的。本文介绍了 CrOSSD 项目，该项目旨在建立一个开放源码软件项目数据库，并测量其当前项目的“健康”状态。在该项目中，我们将使用定量和定性指标来评估开放源码软件项目的健康状况。定量指标将通过自动抓取元信息(如贡献者的数量、提交和代码行数)来收集。通过手工分析和自动化工具，包括可持续性、资金、社区参与和遵守安全政策等方面，将为选定的“关键”项目收集定性指标。分析结果将在一个方便用户的网络平台上公布，使用户能够查看各个开放源码软件项目的健康状况以及开放源码软件生态系统的总体健康状况。通过这种方法，CrOSSD 项目提供了开放源码软件项目健康状况的全面和最新视图，使开发人员、维护人员和其他利益攸关方更容易了解开放源码软件项目的健康状况，并就其使用和维护作出明智的决定。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+a+Critical+Open-Source+Software+Database)|0|
|[Task-Specific Data Augmentation for Zero-shot and Few-shot Stance Detection](https://doi.org/10.1145/3543873.3587337)|Jiarui Zhang, Shaojuan Wu, Xiaowang Zhang, Zhiyong Feng|College of Intelligence and Computing, Tianjin University, China|Various targets keep coming up on social media, and most of them lack labeled data. In this paper, we focus on zero-shot and few-shot stance detection, which aims to identify stances with few or even no training instances. In order to solve the lack of labeled data and implicit stance expression, we propose a self-supervised data augment approach based on coreference resolution. The method is specific for stance detection to generate more stable data and reduce the variance within and between classes to achieve a balance between validity and robustness. Considering the diversity of comments, we propose a novel multi-task stance detection framework of target-related fragment extraction and stance detection, which can enhance attention on target-related fragments and reduce the noise of other fragments. Experiments show that the proposed approach achieves state-of-the-art performance in zero-shot and few-shot stance detection.|社交媒体上不断出现各种各样的目标，其中大多数都缺乏标记数据。本文重点研究了零射击和少射击姿态检测，目的是在很少甚至没有训练实例的情况下进行姿态识别。为了解决缺乏标记数据和隐式姿态表达的问题，提出了一种基于共参考分辨率的自监督数据增强方法。该方法特别适用于姿态检测，以产生更稳定的数据，减少类内和类间的方差，实现有效性和鲁棒性之间的平衡。考虑到评论的多样性，本文提出了一种新的多任务姿态检测框架，该框架可以提高对目标相关碎片的关注度，降低其他碎片的噪声。实验表明，该方法在零射击和少射击姿态检测方面取得了较好的效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Task-Specific+Data+Augmentation+for+Zero-shot+and+Few-shot+Stance+Detection)|0|
|[Measuring Potential Performance Gains of Python Web Applications with pyUpgradeSim](https://doi.org/10.1145/3543873.3587338)|Anthony Shaw, Amin Beheshti|School of Computing, Macquarie University, Australia|Python is a popular programming language for web development. However, optimizing the performance of Python web applications is a challenging task for developers. This paper presents a new approach to measuring the potential performance gains of upgraded Python web applications. Our approach is based on the provision of an interactive service that assists developers in optimizing their Python code through changes to the underlying system. The service uses profiling and visualization techniques to identify performance bottlenecks. We demonstrate and evaluate the effectiveness of our approach through a series of experiments on real-world Python web applications, measuring performance differences in between versions and the benefits of migrating at a reduced cost. The results show promising improvement in performance without any required code changes.|Python 是一种流行的 Web 开发编程语言。然而，优化 Pythonweb 应用程序的性能对于开发人员来说是一项具有挑战性的任务。本文提出了一种新的方法来衡量升级后的 Python Web 应用程序的潜在性能收益。我们的方法基于交互式服务的提供，该服务通过对底层系统的更改帮助开发人员优化他们的 Python 代码。该服务使用分析和可视化技术来识别性能瓶颈。我们通过在真实的 Python Web 应用程序上的一系列实验来演示和评估我们的方法的有效性，测量不同版本之间的性能差异以及以较低成本迁移的好处。结果显示，在不需要任何代码更改的情况下，性能有了很大的改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Measuring+Potential+Performance+Gains+of+Python+Web+Applications+with+pyUpgradeSim)|0|
|[mStore: Schema Mining based-RDF Data Storage](https://doi.org/10.1145/3543873.3587339)|Guopeng Zheng, Tenglong Ren, Lulu Yang, Xiaowang Zhang, Zhiyong Feng|College of Intelligence and Computing, Tianjin University, China|The relationship-based approach is an efficient solution strategy for distributed RDF data management. The schema of tables can directly affect the system’s storage efficiency and query performance. Most current approaches are based on fixed schema(e.g., VP, ExtVP). When facing large-scale RDF datasets and complex SPARQL queries requiring many joins, such methods suffer from problems such as long pre-processing time and poor query performance. Schemas with Pareto Optimality between the system’s space consumption and query efficiency are needed but also hard to find. Therefore, we propose mStore, a prototype system with flexible schemas based on schema mining. The intuition behind our approach is that we want to divide the combinations of predicates with higher relevance into the same schema, which can replace costly joins with low-cost selects, improving the query performance. The results show that our system performs better on complex query workloads while reducing the pre-processing time overhead compared to systems with fixed schema partitioning strategies.|基于关系的方法是一种有效的分布式 RDF 数据管理解决方案。表的模式直接影响系统的存储效率和查询性能。大多数当前的方法都基于固定的模式(例如 VP、 ExtVP)。当面对大规模 RDF 数据集和需要许多连接的复杂 SPARQL 查询时，这类方法会遇到预处理时间长和查询性能差等问题。系统的空间消耗和查询效率之间存在帕累托最优的模式是必需的，但也很难找到。因此，我们提出了基于模式挖掘的具有灵活模式的原型系统 mStore。我们的方法背后的直觉是，我们希望将具有更高相关性的谓词组合划分到同一模式中，这样可以用低成本选择代替代价高昂的连接，从而提高查询性能。结果表明，与采用固定模式划分策略的系统相比，该系统在处理复杂查询工作负载的同时，减少了预处理时间开销。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=mStore:+Schema+Mining+based-RDF+Data+Storage)|0|
|[Identifying Topic and Cause for Sarcasm: An Unsupervised Knowledge-enhanced Prompt Method](https://doi.org/10.1145/3543873.3587343)|Minjie Yuan, Qiudan Li, Xue Mao, Daniel Dajun Zeng|State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Science, China and School of Artificial Intelligence, University of Chinese Academy of Sciences, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Science, China|Sarcasm is usually emotional and topical. Mining the characteristics of sarcasm semantics in different emotional tendencies and topic expressions helps gain insight into the sarcasm cause. Most of the existing work detect sarcasm or topic label based on a supervised learning framework, which requires heavy data annotation work. To overcome the above challenges, inspired by the multi-task learning framework, this paper proposes an unsupervised knowledge-enhanced prompt method. This method uses the similarity interaction mechanism to mine the hidden relationship between the sarcasm cause and topic, which integrates external knowledge, such as syntax and emotion, into the prompting and generation process. Additionally, it identifies the sarcasm cause and topic simultaneously. Experimental results on a real-world dataset verify the effectiveness of the proposed model.|讽刺通常是情绪化的，主题化的。挖掘不同情感倾向和话题表达的讽刺语义特征有助于深入了解讽刺的成因。现有的大多数工作都是基于一个监督式学习框架来检测讽刺或主题标签，这需要大量的数据注释工作。为了克服上述挑战，受多任务学习框架的启发，本文提出了一种无监督知识增强的提示方法。该方法利用相似交互机制挖掘话题与讽刺原因之间的隐含关系，将句法、情感等外部知识融入到讽刺的提示和生成过程中。此外，它还能同时识别讽刺的原因和主题。在实际数据集上的实验结果验证了该模型的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Identifying+Topic+and+Cause+for+Sarcasm:+An+Unsupervised+Knowledge-enhanced+Prompt+Method)|0|
|[The Community Notes Observatory: Can Crowdsourced Fact-Checking be Trusted in Practice?](https://doi.org/10.1145/3543873.3587340)|Luca Righes, Mohammed Saeed, Gianluca Demartini, Paolo Papotti|EURECOM, France; The University of Queensland, Australia|Fact-checking is an important tool in fighting online misinformation. However, it requires expert human resources, and thus does not scale well on social media because of the flow of new content. Crowdsourcing has been proposed to tackle this challenge, as it can scale with a smaller cost, but it has always been studied in controlled environments. In this demo, we present the Community Notes Observatory, an online system to evaluate the first large-scale effort of crowdsourced fact-checking deployed in practice. We let demo attendees search and analyze tweets that are fact-checked by Community Notes users and compare the crowd’s activity against professional fact-checkers. The attendees will explore evidence of i) differences in how the crowd and experts select content to be checked, ii) how the crowd and the experts retrieve different resources to fact-check, and iii) the edge the crowd shows in fact-checking scalability and efficiency as compared to expert checkers.|事实核查是打击网络虚假信息的重要工具。然而，它需要专业的人力资源，因此由于新内容的流动，在社交媒体上的扩展性不好。众包已经被提议来解决这一挑战，因为它可以以较低的成本扩大规模，但它一直在受控的环境中进行研究。在这个演示中，我们展示了“社区笔记观察站”，一个在线系统，用于评估在实践中部署的第一个大规模众包事实核查工作。我们让演示的参与者搜索和分析由 Community Notes 用户进行事实核查的 tweet，并将人群的活动与专业的事实核查者进行比较。与会者将探索以下证据: i)群体和专家如何选择要检查的内容，ii)群体和专家如何检索不同的资源以进行事实检查，iii)群体在事实检查的可扩展性和效率方面表现出与专家检查员相比的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Community+Notes+Observatory:+Can+Crowdsourced+Fact-Checking+be+Trusted+in+Practice?)|0|
|[EasySpider: A No-Code Visual System for Crawling the Web](https://doi.org/10.1145/3543873.3587345)|Naibo Wang, Wenjie Feng, Jianwei Yin, SeeKiong Ng|Zhejiang University, China; National University of Singapore, Singapore|The web is a treasure trove for data that is increasingly used by computer scientists for building large machine learning models as well as non-computer scientists for social studies or marketing analyses. As such, web-crawling is an essential tool for both computational and non-computational scientists to conduct research. However, most of the existing web crawler frameworks and software products either require professional coding skills without an easy-to-use graphic user interface or are expensive and limited in features. They are thus not friendly to newbies and inconvenient for complicated web-crawling tasks. In this paper, we present an easy-to-use visual web crawler system, EasySpider, for designing and executing web crawling tasks without coding. The workflow of a new web crawling task can be visually programmed by following EasySpider’s visual wizard on the target webpages using an intuitive point-and-click interface. The generated crawler task can then be easily invoked locally or as a web service. Our EasySpider is cross-platform and flexible to adapt to different web-resources. It also supports advanced configuration for complicated tasks and extension. The whole system is open-sourced and transparent for free-access at GitHub 1, which avoids possible privacy leakage.|网络是数据的宝库，计算机科学家越来越多地使用这些数据来建立大型机器学习模型，非计算机科学家也越来越多地使用这些数据来进行社会研究或营销分析。因此，网络爬行是计算和非计算科学家进行研究的重要工具。然而，大多数现有的网络爬虫框架和软件产品要么需要专业的编码技能，而没有易于使用的图形用户界面，要么价格昂贵，功能有限。因此，他们不友好的新手和不方便的复杂的网络爬行任务。本文介绍了一个易于使用的可视化网络爬虫系统 EasySpider，它可以在不需要编码的情况下设计和执行网络爬虫任务。一个新的网页爬行任务的工作流程，可以通过遵循 EasySpider 的视觉向导在目标网页上使用一个直观的点击界面可视化编程。生成的爬虫任务可以很容易地在本地调用或作为 Web 服务调用。我们的 EasySpider 是跨平台和灵活的，以适应不同的网络资源。它还支持复杂任务和扩展的高级配置。GitHub 1的整个系统是开源和透明的，可以免费访问，避免了可能的隐私泄露。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EasySpider:+A+No-Code+Visual+System+for+Crawling+the+Web)|0|
|[Persona Consistent Dialogue Generation via Contrastive Learning](https://doi.org/10.1145/3543873.3587346)|Zhenfeng Han, Sai Zhang, Xiaowang Zhang|College of Intelligence and Computing, Tianjin University, China|The inclusion of explicit personas in generation models has gained significant attention as a means of developing intelligent dialogue agents. However, large pretrained generation models often produce inconsistent responses with persona. We investigate the model generation behavior to identify signs of inconsistency and observe inconsistent behavior patterns. In this work, we introduce contrastive learning into persona consistent dialogue generation, building on the idea that humans learn not just from positive feedback, but also from identifying and correcting undesirable behaviors. According to the inconsistent patterns, we design two strategies to construct high-quality negative samples, which are critical for contrastive learning efficacy. Experimental results demonstrate that our method can effectively improve the consistency of the responses while improving its dialogue quality on both automatic and human evaluation.|作为开发智能对话代理的一种手段，在生成模型中包含明确的人物角色受到了极大的关注。然而，大型预训练生成模型往往产生与人物角色不一致的反应。我们研究模型生成行为，以识别不一致的迹象和观察不一致的行为模式。在这项工作中，我们将对比学习引入到人格一致性对话生成中，建立在这样一个理念上，即人类不仅从积极的反馈中学习，而且从识别和纠正不良行为中学习。根据不一致的学习模式，我们设计了两种策略来构建高质量的负样本，这两种策略对于对比学习效能是至关重要的。实验结果表明，该方法可以有效地提高响应的一致性，同时提高对话质量的自动和人为评价。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Persona+Consistent+Dialogue+Generation+via+Contrastive+Learning)|0|
|[TML: A Temporal-aware Multitask Learning Framework for Time-sensitive Question Answering](https://doi.org/10.1145/3543873.3587347)|Ziqiang Chen, Shaojuan Wu, Xiaowang Zhang, Zhiyong Feng|College of Intelligence and Computing, Tianjin University, China|Many facts change over time, Time-sensitive Question Answering(TSQA) answers questions about time evolution facts to test the model’s ability in the dimension of the time. The existing methods obtain the representations of questions and documents and then compute their similarity to find the answer spans. These methods perform well in simple moment questions, but they are difficult to solve hard duration problems that need temporal relations and temporal numeric comparisons. In this paper, we propose Temporal-aware Multitask Learning (TML) with three auxiliary tasks to tackle with them. First, we propose a temporal-aware sequence labeling task to help the model distinguish the temporal expressions by detecting temporal types of tokens in the document. Then a temporal-aware masked language modeling task is used to capture the temporal relation between events based on the context. Furthermore, temporal-aware order learning is proposed to inject the ability of numeric comparison into the model. We carried out comprehensive experiments on the TimeQA benchmark, aiming to evaluate the performance of our proposed methodology in handling temporal question answering. TML significantly outperforms the baselines by a relative 10% on the two splits of the dataset.|随着时间的推移，许多事实发生了变化，时间敏感问题回答(TSQA)回答了关于时间演化事实的问题，以检验模型在时间维度上的能力。现有的方法获得问题和文档的表示，然后计算它们的相似度来找到答案范围。这些方法在解决简单矩问题时表现良好，但在解决需要时间关系和时间数值比较的难度较大的持续时间问题时存在一定的困难。本文提出了一种基于时间感知的多任务学习(TML)方法，该方法包括三个辅助任务。首先，我们提出一个时间感知序列标记任务，通过检测文档中令牌的时间类型来帮助模型区分时间表达式。然后利用一个时间感知的掩蔽语言建模任务，根据上下文捕获事件之间的时间关系。在此基础上，提出了一种基于时间感知的顺序学习方法，将数值比较能力引入到模型中。我们在 TimeQA 基准上进行了全面的实验，旨在评估我们提出的方法在处理时间问题回答中的性能。在数据集的两个部分上，TML 显著地比基线高出10% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TML:+A+Temporal-aware+Multitask+Learning+Framework+for+Time-sensitive+Question+Answering)|0|
|[Addressing socially destructive disinformation on the web with advanced AI tools: Russia as a case study](https://doi.org/10.1145/3543873.3587348)|Florian Barbaro, Andy Skumanich|Innov8ai, USA|Today, disinformation (i.e. deliberate misinformation) is omnipresent in all web communication channels. There is a developing explosion in this socially disruptive mode of web-based information. Increasingly, we have seen various countries developing advanced methods to spread their targeted disinformation. To address this flood of disinformation will require refined strategies to capture and evaluate the messages. In this paper, we present both a quantitative and a qualitative analysis of online social and information networks to better evaluate the characteristics of the disinformation campaigns. We focus on the case of Russian-generated disinformation, which has been developed to an elevated level. We demonstrate an effective approach based on a new dataset to study the Russian campaign composed of 14497 cases of dis-information and the corresponding counter-dis-information. Although this case is of high current relevance, there is very limited published evaluation. We provide a novel analysis and present a methodology to characterize this disinformation. We based our investigation on a Spherical k-means algorithm to determine the main topics of the disinformation and to discover the key trends. We employ distilBERT algorithm and achieve a high accuracy F1-score of 98.8 demonstrating good quantitative capabilities. We propose the methodology as a template for further exploration and analysis.|今天，虚假信息(即故意的错误信息)在所有的网络沟通渠道中无处不在。这种具有社会破坏性的基于网络的信息模式正在迅速发展。我们已经看到越来越多的国家开发出先进的方法来传播他们有针对性的虚假信息。为了解决这种虚假信息的泛滥，需要改进策略来捕捉和评估这些信息。在本文中，我们提出了一个定量和定性的分析在线社会和信息网络，以更好地评估特点的虚假信息运动。我们的重点是俄罗斯产生的虚假信息的情况下，这已经发展到一个提高的水平。基于一个新的数据集，我们展示了一个有效的方法来研究由14497个虚假信息和相应的反虚假信息案例组成的俄罗斯战役。尽管这个案例目前具有很高的相关性，但公开发表的评价非常有限。我们提供了一个新颖的分析，并提出了一种方法来描述这种假信息。我们的研究基于球面 k-means 算法，以确定虚假信息的主要课题和发现的关键趋势。我们采用蒸馏 BERT 算法，实现了98.8的高精度 F1评分，显示了良好的定量能力。我们建议将该方法作为进一步探索和分析的模板。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Addressing+socially+destructive+disinformation+on+the+web+with+advanced+AI+tools:+Russia+as+a+case+study)|0|
|[Katti: An Extensive and Scalable Tool for Website Analyses](https://doi.org/10.1145/3543873.3587351)|Florian Nettersheim, Stephan Arlt, Michael Rademacher, Florian Dehling|University of Applied Sciences Bonn-Rhein-Sieg, Germany; Fraunhofer FKIE, Germany; Federal Office for Information Security, Germany|Research on web security and privacy frequently relies on tools that analyze a set of websites. One major obstacle to the judicious analysis is the employment of a rock-solid and feature-rich web crawler. For example, the automated analysis of ad-malware campaigns on websites requests crawling a vast set of domains on multiple real web browsers, while simultaneously mitigating bot detections and applying user interactions on websites. Further, the ability to attach various threat analysis frameworks lacks current tooling efforts in web crawling and analyses. In this paper we introduce Katti, which overcomes several of today’s technical hurdles in web crawling. Our tool employs a distributed task queue that efficiently and reliably handles both large crawling and threat analyses requests. Katti extensively collects all available web data through an integrated person-in-the-middle proxy. Moreover, Katti is not limited to a specific use case, allowing users to easily customize our tool to their individual research intends.|对网络安全和隐私的研究通常依赖于分析一组网站的工具。明智分析的一个主要障碍是使用坚固的特性丰富的 Web 爬虫程序。例如，对网站上的广告恶意软件活动的自动分析要求在多个真实的网络浏览器上爬行大量域名，同时减少机器人检测并在网站上应用用户交互。此外，附加各种威胁分析框架的能力缺乏目前在网络爬行和分析工具的努力。在这篇文章中，我们介绍了 Katti，它克服了当今网络爬行中的一些技术障碍。我们的工具使用了一个分布式任务队列，可以高效可靠地处理大型爬行和威胁分析请求。Katti 通过一个集成的中间人代理广泛地收集所有可用的网络数据。此外，Katti 并不局限于特定的用例，允许用户根据自己的研究意图轻松定制我们的工具。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Katti:+An+Extensive+and+Scalable+Tool+for+Website+Analyses)|0|
|[Graph Induced Transformer Network for Detection of Politeness and Formality in Text](https://doi.org/10.1145/3543873.3587352)|Tirthankar Dasgupta, Manjira Sinha, Chundru Geetha Praveen|Indian Institute of Technology Kharagpur, India; Tata Consultancy Services, India; IIT Kharagpur, India|Formality and politeness are two of the most commonly studied stylistic dimensions of language that convey, authority, amount of shared context, and social distances among the communicators and are known to affect user behavior significantly. Formality in the text refers to the type of language used in situations when the speaker is very careful about the choice of words and sentence structure. In this paper, we propose a graph-induced transformer network (GiTN) to detect formality and politeness in text automatically. The proposed model exploits the latent linguistic features present in the text to identify the aforementioned stylistic factors. The proposed model is evaluated with multiple datasets across domains. We found that the proposed model’s performance surpasses most baseline systems.|正式和礼貌是语言的两个最常被研究的文体维度，它们传达了交际者之间的信息、权威、共享语境的数量和社会距离，并且已知它们会显著地影响使用者的行为。语篇中的正式语言是指说话人在选择词语和句子结构时非常谨慎的语言类型。本文提出了一种图形感应变压器网络(GiTN)来自动检测文本中的礼貌和礼貌。该模型利用文本中潜在的语言特征来识别上述文体因素。该模型使用跨领域的多个数据集进行评估。我们发现该模型的性能优于大多数基线系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Induced+Transformer+Network+for+Detection+of+Politeness+and+Formality+in+Text)|0|
|[Visualizing How-Provenance Explanations for SPARQL Queries](https://doi.org/10.1145/3543873.3587350)|Luis Galárraga, Daniel Hernández, Anas Katim, Katja Hose|Aalborg University, Denmark and TU Wien, Austria; University of Stuttgart, Germany; INSA Rouen, France; IRISA, Inria, France|Knowledge graphs (KGs) are vast collections of machine-readable information, usually modeled in RDF and queried with SPARQL. KGs have opened the door to a plethora of applications such as Web search or smart assistants that query and process the knowledge contained in those KGs. An important, but often disregarded, aspect of querying KGs is query provenance: explanations of the data sources and transformations that made a query result possible. In this article we demonstrate, through a Web application, the capabilities of SPARQLprov, an engine-agnostic method that annotates query results with how-provenance annotations. To this end, SPARQLprov resorts to query rewriting techniques, which make it applicable to already deployed SPARQL endpoints. We describe the principles behind SPARQLprov and discuss perspectives on visualizing how-provenance explanations for SPARQL queries.|知识图(KGs)是机器可读信息的大量集合，通常以 RDF 建模并使用 SPARQL 进行查询。幼儿园已经为大量的应用程序打开了大门，例如 Web 搜索或智能助手，它们可以查询和处理这些幼儿园所包含的知识。查询 KG 的一个重要但经常被忽视的方面是查询来源: 解释使查询结果成为可能的数据源和转换。在本文中，我们通过一个 Web 应用程序演示了 SPARQLprov 的功能，这是一种不依赖引擎的方法，它使用 how-source 注释对查询结果进行注释。为此，SPARQLprov 采用查询重写技术，这使得它适用于已经部署的 SPARQL 端点。我们描述了 SPARQLprov 背后的原理，并讨论了如何可视化 SPARQL 查询的起源解释。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Visualizing+How-Provenance+Explanations+for+SPARQL+Queries)|0|
|[Counterfactual Reasoning for Decision Model Fairness Assessment](https://doi.org/10.1145/3543873.3587354)|Giandomenico Cornacchia, Vito Walter Anelli, Fedelucio Narducci, Azzurra Ragone, Eugenio Di Sciascio|Politecnico di Bari, Italy; University of Bari, Italy|The increasing application of Artificial Intelligence and Machine Learning models poses potential risks of unfair behaviour and, in the light of recent regulations, has attracted the attention of the research community. Several researchers focused on seeking new fairness definitions or developing approaches to identify biased predictions. These approaches focus solely on a discrete and limited space; only a few analyze the minimum variations required in the user characteristics to ensure a positive outcome for the individuals (counterfactuals). In that direction, the methodology proposed in this paper aims to unveil unfair model behaviors using counterfactual reasoning in the case of fairness under unawareness. The method also proposes two new metrics that analyse the (estimated) sensitive information of counterfactual samples with the help of an external oracle. Experimental results on three data sets show the effectiveness of our approach for disclosing unfair behaviour of state-of-the-art Machine Learning and debiasing models. Source code is available at https://github.com/giandos200/WWW-23-Counterfactual-Fair-Opportunity-Poster-.|越来越多的人工智能和机器学习模型的应用带来了不公平行为的潜在风险，并且，根据最近的规定，已经引起了研究界的注意。一些研究人员致力于寻找新的公平定义或发展方法来识别有偏见的预测。这些方法只集中在一个离散和有限的空间; 只有少数几个分析最小的变化需要在用户特征，以确保一个积极的结果为个人(反事实)。在这个方向上，本文提出的方法论旨在揭示在无意识的公平情况下使用反事实推理的不公平模型行为。该方法还提出了两个新的度量分析(估计)敏感信息的反事实样本的帮助下，外部预言。在三个数据集上的实验结果表明了我们的方法在揭示最先进的机器学习的不公平行为和消除模型偏差方面的有效性。源代码可在 https://github.com/giandos200/www-23-counterfactual-fair-opportunity-poster- 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Reasoning+for+Decision+Model+Fairness+Assessment)|0|
|[Wikidata Atlas: Putting Wikidata on the Map](https://doi.org/10.1145/3543873.3587356)|Benjamín Del Pino, Aidan Hogan|DCC, Universidad de Chile, Chile; DCC, Universidad de Chile, Chile and Instituto Milenio Fundamentos de los Datos (IMFD), Chile|Wikidata Atlas is an online system that allows users to explore Wikidata items on an interactive global map; for example, users can explore the global distribution of all lighthouses described by Wikidata. Designing such a system poses challenges in terms of scalability, where some classes have hundreds of thousands of instances; efficiency, where visualisations are generated live; freshness, where we want changes on Wikidata to be reflected as they happen in the system; and usability, where we aim for the system to be accessible for a broad audience. Herein we describe the design and implementation of the system in light of these challenges.|Wikidata Atlas 是一个在线系统，允许用户在交互式全球地图上探索 Wikidata 项目; 例如，用户可以探索 Wikidata 描述的所有灯塔的全球分布。设计这样一个系统在可扩展性方面提出了挑战，有些类有成千上万的实例; 在效率方面，可视化是生成实时的; 在新鲜方面，我们希望 Wikidata 的变化在系统中反映出来; 在可用性方面，我们的目标是让广大用户能够访问这个系统。在这里，我们描述了针对这些挑战的系统的设计和实现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Wikidata+Atlas:+Putting+Wikidata+on+the+Map)|0|
|[How Algorithm Awareness Impacts Algospeak Use on TikTok](https://doi.org/10.1145/3543873.3587355)|Daniel Klug, Ella Steen, Kathryn Yurechko|Gordon College, USA; Washington and Lee University, USA; Software and Societal Systems Department, Carnegie Mellon University, USA|Algospeak refers to social media users intentionally altering or substituting words when creating or sharing online content, for example, using ‘le$bean’ for ‘lesbian’. This study discusses the characteristics of algospeak as a computer-mediated language phenomenon on TikTok with regards to users’ algorithmic literacy and their awareness of how the platform’s algorithms work. We then present results from an interview study with TikTok creators on their motivations to utilize algospeak. Our results indicate that algospeak is used to oppose TikTok’s algorithmic moderation system in order to prevent unjust content violations and shadowbanning when posting about benign yet seemingly unwanted subjects on TikTok. In this, we find that although algospeak helps to prevent consequences, it often impedes the creation of quality content. We provide an adapted definition of algospeak and new insights into user-platform interactions in the context of algorithmic systems and algorithm awareness.|Algospeak 指的是社交媒体用户在创建或分享在线内容时故意修改或替换词语，例如，使用“ le $bean”表示“女同性恋”。这项研究讨论了算法语音作为 TikTok 上一种计算机中介语言现象的特点，涉及到用户的算法素养和他们对该平台的算法如何工作的认识。然后，我们展示了 TikTok 创作者关于他们使用算法演讲的动机的访谈研究结果。我们的研究结果表明，algospeak 被用来对抗 TikTok 的算法审核系统，以防止在 TikTok 上发布有关良性但似乎不受欢迎的主题时，出现不公正的内容违规和影子禁令。在这一点上，我们发现，尽管算法演讲有助于防止后果，但它往往阻碍创建高质量的内容。我们提供了一个自适应的定义和算法系统和算法意识背景下的用户-平台交互的新见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Algorithm+Awareness+Impacts+Algospeak+Use+on+TikTok)|0|
|[Computing and Visualizing Agro-Meteorological Parameters based on an Observational Weather Knowledge Graph](https://doi.org/10.1145/3543873.3587357)|Nadia Yacoubi Ayadi, Catherine Faron, Franck Michel, Fabien Gandon, Olivier Corby|Université Côte d'Azur, CNRS, I3S (UMR 7271),France, France; Université Côte d'Azur, CNRS, I3S (UMR 7271), France, France|Linked-data principles are more and more adopted to integrate and publish semantically described open data using W3C standards resulting in a large amount of available resources [7]. In particular, meteorological sensor data have been uplifted into public RDF graphs, such as WeKG-MF which offers access to a large set of meteorological variables described through spatial and temporal dimensions. Nevertheless, these resources include huge numbers of raw observations that are tedious to be explored and reused by lay users. In this paper, we leverage WeKG-MF to compute important agro-meteorological parameters and views with SPARQL queries. As a result, we deployed a LOD platform as a web application to allow users to navigate, consume and produce linked datasets of agro-meterological parameters calculated on-the-fly.|使用 W3C 标准集成和发布语义描述的开放数据时，越来越多地采用链接数据原则，从而产生了大量的可用资源[7]。特别是，气象传感器数据已被提升为公共 RDF 图，例如 WeKG-MF，它提供了通过空间和时间维度描述的大量气象变量。尽管如此，这些资源包括大量的原始观察资料，这些资料对于非专业用户来说是单调乏味的，需要进行探索和重复使用。本文利用 WeKG-MF 算法，通过 SPARQL 查询来计算重要的农业气象参数和视图。因此，我们部署了一个 LOD 平台作为 Web 应用程序，允许用户导航、使用和生成连接的数据集，这些数据集是动态计算的农业气象参数。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Computing+and+Visualizing+Agro-Meteorological+Parameters+based+on+an+Observational+Weather+Knowledge+Graph)|0|
|[Injecting data into ODRL privacy policies dynamically with RDF mappings](https://doi.org/10.1145/3543873.3587358)|Juan CanoBenito, Andrea Cimmino, Raúl GarcíaCastro|Universidad Politécnica de Madrid, Spain|The privacy of the data provided by available sources is one of the major concerns of our era. In order to address this challenge, the W3C has promoted recommendations to allow expressing privacy policies. One of these recommendations is the Open Digital Rights Language (ODRL) vocabulary. Although this standard has wide adoption, it is not suitable in domains such as IoT, Ubiquitous and Mobile Computing, or discovery. The reason behind is the fact that ODRL privacy policies are not able to cope with dynamic information that may come from external sources of data and, therefore, these policies can not define privacy restrictions upon data that is not already written in the policy beforehand. In this demo paper, a solution to this challenge is presented. It is shown how ODRL policies can overcome the aforementioned limitation by being combined with a mapping language for RDF materialisation. The article shows how ODRL policies are able to consider data coming from an external data source when they are solved, in particular, a weather forecast API that provides temperature values. The demonstration defines an ODRL policy that grants access to a resource only when the temperature of the API is above a certain value.|由可用来源提供的数据的隐私性是我们这个时代的主要关注点之一。为了应对这一挑战，W3C 提出了允许表达隐私策略的建议。其中一个建议是开放数字版权语言(ODRL)词汇表。尽管这个标准已经被广泛采用，但它并不适用于物联网、无处不在和移动计算或发现等领域。其背后的原因是 ODRL 隐私策略不能处理可能来自外部数据源的动态信息，因此，这些策略不能定义未事先在策略中编写的数据的隐私限制。在本演示文件中，提出了解决这一挑战的方案。本文展示了 ODRL 策略如何通过与用于 RDF 实现的映射语言相结合来克服上述限制。本文展示了 ODRL 策略如何能够在解决来自外部数据源的数据时考虑这些数据，特别是提供温度值的天气预报 API。该演示定义了一个 ODRL 策略，该策略仅当 API 的温度高于某个值时才授予对资源的访问权。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Injecting+data+into+ODRL+privacy+policies+dynamically+with+RDF+mappings)|0|
|[MediSage: An AI Assistant for Healthcare via Composition of Neural-Symbolic Reasoning Operators](https://doi.org/10.1145/3543873.3587361)|Sutanay Choudhury, Khushbu Agarwal, Colby Ham, Suzanne Tamang|Pacific Northwest National Laboratory, USA; Stanford University, USA|We introduce MediSage, an AI decision support assistant for medical professionals and caregivers that simplifies the way in which they interact with different modalities of electronic health records (EHRs) through a conversational interface. It provides step-by-step reasoning support to an end-user to summarize patient health, predict patient outcomes and provide comprehensive and personalized healthcare recommendations. MediSage provides these reasoning capabilities by using a knowledge graph that combines general purpose clinical knowledge resources with recent-most information from the EHR data. By combining the structured representation of knowledge with the predictive power of neural models trained over both EHR and knowledge graph data, MediSage brings explainability by construction and represents a stepping stone into the future through further integration with biomedical language models.|我们介绍了 MediSage，一个面向医疗专业人员和护理人员的人工智能决策支持助手，通过对话界面简化了他们与不同形式的电子健康记录(EHRs)交互的方式。它为最终用户提供逐步推理支持，以总结患者健康状况，预测患者结果，并提供全面和个性化的医疗保健建议。MediSage 通过使用知识图表提供这些推理能力，该知识图表结合了通用临床知识资源和来自 EHR 数据的最新信息。通过将知识的结构化表示与对 EHR 和知识图数据进行训练的神经模型的预测能力相结合，MediSage 通过构建带来了可解释性，并通过与生物医学语言模型的进一步整合代表了未来的一块垫脚石。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MediSage:+An+AI+Assistant+for+Healthcare+via+Composition+of+Neural-Symbolic+Reasoning+Operators)|0|
|[Depicting Vocabulary Summaries with Devos](https://doi.org/10.1145/3543873.3587359)|Ahmad Alobaid, Jhon Toledo, Óscar Corcho, María PovedaVillalón|Universidad Politécnica de Madrid, Spain|Communicating ontologies to potential users is still a difficult and time-consuming task. Even for small ones, users need to invest time to determine whether to reuse them. Providing diagrams together with the ontologies facilitates the task of understanding the model from a user perspective. While some tools are available for depicting ontologies, and the code could also be inspected using ontology editors’ graphical interfaces, in many cases, the diagrams are too big or complex. The main objective of this demo is to present Devos, a system to generate ontology diagrams based on different strategies for summarizing the ontology.|与潜在用户交流本体仍然是一项困难和耗时的任务。即使是对于小型项目，用户也需要投入时间来决定是否重用它们。提供图表和本体论有助于从用户角度理解模型。虽然有一些工具可用于描述本体，代码也可以使用本体编辑器的图形界面进行检查，但在许多情况下，图过大或过于复杂。本演示的主要目的是介绍 Devos，一个基于不同策略生成本体图的系统，用于总结本体。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Depicting+Vocabulary+Summaries+with+Devos)|0|
|[Child Sexual Abuse Awareness and Support Seeking on Reddit: A thematic Analysis](https://doi.org/10.1145/3543873.3587363)|Siva Sahitya Simhadri, Tatiana Ringenberg|Computer and Information Technology, Purdue University, USA; Purdue University, USA|Child sexual abuse (CSA) is a pervasive issue in both online and physical contexts. Social media has grown in popularity as a platform for offering awareness, support, and community for those seeking help or advice regarding CSA. One popular social media platform in which such communities has formed is Reddit. In this study, we use both LDA and a reflexive thematic analysis to understand the types of engagements users have with subreddits aimed at CSA awareness. Through the reflexive thematic analysis, we identified six themes including strong negative emotions and phrasing, seeking help, personal experiences and their impact, measurement strategies to prevent abuse, provisioning of support, and the problematic nature of the Omegle platform. This research has implications for those creating awareness materials around CSA safety as well as for child advocacy groups.|儿童性虐待(CSA)是一个普遍存在的问题，无论是在网络上还是在物理环境中。社交媒体作为一个平台，为那些寻求关于 CSA 的帮助或建议的人提供意识、支持和社区，已经越来越受欢迎。Reddit 是一个流行的社交媒体平台，这样的社区已经形成。在这项研究中，我们使用 LDA 和一个反思性的主题分析来理解用户对于旨在提高 CSA 意识的子版块的参与类型。通过反思性主题分析，我们确定了六个主题，包括强烈的负面情绪和措辞，寻求帮助，个人经历及其影响，防止滥用的测量策略，提供支持，以及 Omegle 平台的问题性质。这项研究对那些围绕 CSA 安全创建宣传材料的人以及儿童倡导团体具有启示意义。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Child+Sexual+Abuse+Awareness+and+Support+Seeking+on+Reddit:+A+thematic+Analysis)|0|
|[Violentometer: measuring violence on the Web in real time](https://doi.org/10.1145/3543873.3587364)|Henrique S. Xavier|Ceweb, NIC.br, Brazil|This paper describes a system for monitoring in real time the level of violence on Web platforms through the use of an artificial intelligence model to classify textual data according to their content. The system was successfully implemented and tested during the electoral campaign period of the Brazilian 2022 elections by using it to monitor the attacks directed to thousands of candidates on Twitter. We show that, despite an accurate and absolute quantification of violence is not feasible, the system yields differential measures of violence levels that can be useful for understanding human behavior online.|本文描述了一个实时监测网络平台上暴力程度的系统，该系统通过使用人工智能模型来根据文本数据的内容进行分类。该系统在巴西2022年选举竞选期间成功实施和测试，用于监测针对 Twitter 上数千名候选人的攻击。我们的研究表明，尽管对暴力行为进行准确和绝对的量化是不可行的，但该系统可以产生对暴力程度的不同衡量标准，这对于理解人类在线行为是有用的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Violentometer:+measuring+violence+on+the+Web+in+real+time)|0|
|[Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech](https://doi.org/10.1145/3543873.3587368)|Fan Huang, Haewoon Kwak, Jisun An|Luddy School of Informatics, Computing, and Engineering, Indiana University Bloomington, USA|Recent studies have alarmed that many online hate speeches are implicit. With its subtle nature, the explainability of the detection of such hateful speech has been a challenging problem. In this work, we examine whether ChatGPT can be used for providing natural language explanations (NLEs) for implicit hateful speech detection. We design our prompt to elicit concise ChatGPT-generated NLEs and conduct user studies to evaluate their qualities by comparison with human-written NLEs. We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research.|最近的研究警告说，许多在线仇恨言论是含蓄的。由于其微妙的性质，这种仇恨言论的检测的可解释性一直是一个具有挑战性的问题。在这项工作中，我们研究了 ChatGPT 是否可以用于提供自然语言解释(NLEs)隐含的仇恨语音检测。我们设计我们的提示引出简明的 ChatGPT 生成的 NLEs，并进行用户研究，通过与人写的 NLEs 的比较来评估其质量。我们讨论了 ChatGPT 在隐性仇恨言语研究中的潜力和局限性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+ChatGPT+better+than+Human+Annotators?+Potential+and+Limitations+of+ChatGPT+in+Explaining+Implicit+Hate+Speech)|0|
|[RealGraph+: A High-Performance Single-Machine-Based Graph Engine that Utilizes IO Bandwidth Effectively](https://doi.org/10.1145/3543873.3587365)|MyungHwan Jang, JeongMin Park, Ikhyeon Jo, DuckHo Bae, SangWook Kim|Samsung Electronics, Republic of Korea; Department of Computer Science, Hanyang University, Republic of Korea|This paper proposes RealGraph+, an improved version of RealGraph that processes large-scale real-world graphs efficiently in a single machine. Via a preliminary analysis, we observe that the original RealGraph does not fully utilize the IO bandwidth provided by NVMe SSDs, a state-of-the-art storage device. In order to increase the IO bandwidth, we equip RealGraph+ with three optimization strategies to issue more-frequent IO requests: (1) User-space IO, (2) Asynchronous IO, and (3) SIMD processing. Via extensive experiments with four graph algorithms and six real-world datasets, we show that (1) each of our strategies is effective in increasing the IO bandwidth, thereby reducing the execution time; (2) RealGraph+ with all of our strategies improves the original RealGraph significantly; (3) RealGraph+ outperforms state-of-the-art single-machine-based graph engines dramatically; (4) it shows performance comparable to or even better than those of other distributed-system-based graph engines.|本文提出了 RealGraph + ，它是 RealGraph 的一个改进版本，可以在单台机器上有效地处理大规模的实际图形。通过初步分析，我们观察到原来的 RealGraph 没有充分利用 NVMe SSD 提供的 IO 带宽，NVMe SSD 是一种最先进的存储设备。为了增加 IO 带宽，我们给 RealGraph + 配备了三种优化策略来发出更频繁的 IO 请求: (1)用户空间 IO、(2)异步 IO 和(3) SIMD 处理。通过对4个图形算法和6个真实世界数据集的大量实验，我们表明: (1)我们的每个策略都能有效地增加 IO 带宽，从而减少执行时间; (2) RealGraph + 与我们的所有策略一起显著改善了原来的 RealGraph; (3) RealGraph + 的性能显著优于最先进的基于单机的图形引擎; (4)它的性能与其他基于分布式系统的图形引擎相当，甚至更好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RealGraph+:+A+High-Performance+Single-Machine-Based+Graph+Engine+that+Utilizes+IO+Bandwidth+Effectively)|0|
|[Hierarchical Deep Neural Network Inference for Device-Edge-Cloud Systems](https://doi.org/10.1145/3543873.3587370)|Fatih Ilhan, Selim Furkan Tekin, Sihao Hu, Tiansheng Huang, Ka Ho Chow, Ling Liu|Georgia Institute of Technology, USA|Edge computing and cloud computing have been utilized in many AI applications in various fields, such as computer vision, NLP, autonomous driving, and smart cities. To benefit from the advantages of both paradigms, we introduce HiDEC, a hierarchical deep neural network (DNN) inference framework with three novel features. First, HiDEC enables the training of a resource-adaptive DNN through the injection of multiple early exits. Second, HiDEC provides a latency-aware inference scheduler, which determines which input samples should exit locally on an edge device based on the exit scores, enabling inference on edge devices with insufficient resources to run the full model. Third, we introduce a dual thresholding approach allowing both easy and difficult samples to exit early. Our experiments on image and text classification benchmarks show that HiDEC significantly outperforms existing solutions.|边缘计算和云计算已经在计算机视觉、自然语言处理、自主驾驶和智能城市等领域的许多人工智能应用中得到了广泛的应用。为了充分利用这两种模式的优势，我们引入了 HiDEC，这是一种具有三个新特征的层次化深度神经网络(DNN)推理框架。首先，HiDEC 通过注入多个早期出口来实现资源自适应 DNN 的训练。其次，HiDEC 提供了一个感知延迟的推理调度器，它根据退出分数来确定哪些输入样本应该在边缘设备上本地退出，从而能够在资源不足以运行完整模型的边缘设备上进行推理。第三，我们引入了双阈值方法，允许容易和困难的样本早期退出。我们对图像和文本分类基准的实验表明，HiDEC 的性能明显优于现有的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Deep+Neural+Network+Inference+for+Device-Edge-Cloud+Systems)|0|
|[A Detection System for Comfortable Locations Based on Facial Expression Analysis While Riding Bicycles](https://doi.org/10.1145/3543873.3587371)|Ryuta Yamaguchi, Panote Siriaraya, Tomoki Yoshihisa, Shinji Shimojo, Yukiko Kawai|Osaka University, Japan; Kyoto Institute of Technology, Japan; Kyoto Sangyo University, Japan|In recent years, the use of bicycle as a healthy and economical means of transportation has been promoted worldwide. In addition, with the increase in bicycle commuting due to the COVID-19, the use of bicycles are attracting attention as a last-mile means of transportation in Mobility as a Service(MaaS). To help ensure a safe and comfortable ride using a smartphone mounted on a bicycle, this study focuses on analyzing facial expressions while riding to determine potential comfort along the route with the surrounding environment and to provide a map that users can explicitly feedback(FB) after riding. Combining the emotions of facial expressions while riding and FB, we annotate comfort to different locations. Afterwards, we verify the relationship between locations with high level of comfort based on the acquired data and the surrounding environment of those locations using Google Street View(GSV).|近年来，自行车作为一种健康、经济的交通工具在世界范围内得到了推广。此外，随着自行车上下班2019冠状病毒疾病的增加，自行车作为移动即服务(Mobility as a Service，MaaS)的最后一英里交通工具正在引起人们的注意。为了确保安全和舒适的骑行，使用安装在自行车上的智能手机，这项研究侧重于分析骑行时的面部表情，以确定潜在的舒适性沿着路线与周围环境，并提供一个地图，用户可以明确反馈(FB)后，骑车。结合面部表情的情绪，而骑和 FB，我们注释舒适到不同的位置。然后，我们利用谷歌街景(Google Street View，GSV) ，根据获得的数据和周围环境，验证高舒适度地点之间的关系。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Detection+System+for+Comfortable+Locations+Based+on+Facial+Expression+Analysis+While+Riding+Bicycles)|0|
|[Efficient Fair Graph Representation Learning Using a Multi-level Framework](https://doi.org/10.1145/3543873.3587369)|Yuntian He, Saket Gurukar, Srinivasan Parthasarathy|The Ohio State University, USA|Graph representation learning models have demonstrated great capability in many real-world applications. Nevertheless, prior research reveals that these models can learn biased representations leading to unfair outcomes. A few works have been proposed to mitigate the bias in graph representations. However, most existing works require exceptional time and computing resources for training and fine-tuning. In this demonstration, we propose a framework FairMILE for efficient fair graph representation learning. FairMILE allows the user to efficiently learn fair graph representations while preserving utility. In addition, FairMILE can work in conjunction with any unsupervised embedding approach based on the user’s preference and accommodate various fairness constraints. The demonstration will introduce the methodology of FairMILE, showcase how to set up and run this framework, and demonstrate our effectiveness and efficiency to the audience through both quantitative metrics and visualization.|图表示学习模型已经在许多实际应用中显示出了巨大的能力。然而，先前的研究表明，这些模型可以学习有偏见的表征，导致不公平的结果。为了减小图表示中的偏差，人们提出了一些工作。然而，大多数现有的工作需要额外的时间和计算资源进行培训和微调。在这个演示中，我们提出了一个有效的公平图表示学习框架 FairMILE。FairMILE 允许用户在保持实用性的同时有效地学习公平图表示法。此外，FairMILE 可以与任何基于用户偏好的无监督嵌入方法协同工作，并适应各种公平性约束。演示将介绍 FairMILE 的方法，展示如何建立和运行这个框架，并通过定量度量和可视化向观众展示我们的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Fair+Graph+Representation+Learning+Using+a+Multi-level+Framework)|0|
|[Simple Multi-view Can Bring Powerful Graph Neural Network](https://doi.org/10.1145/3543873.3587375)|Bingbing Xu, Yang Li, Qi Cao, Huawei Shen|Institute of Computing Technology, Chinese Academy of Sciences, China|Graph neural networks have achieved state-of-the-art performance on graph-related tasks through layer-wise neighborhood aggregation. Previous works aim to achieve powerful capability via designing injective neighborhood aggregation functions in each layer, which is difficult to determine and numerous additional parameters make it difficult to train these models. It is the input space and the aggregation function that achieve powerful capability at the same time. Instead of designing complexity aggregation functions, we propose a simple and effective framework, namely MV-GNN, to improve the model expressive power via constructing the new input space. Precisely, MV-GNN samples multi-view subgraphs for each node, and any GNN model can be applied to these views. The representation of target node is finally obtained via aggregating all views injectively. Two typical GNNs (i.e., GCN and GAT) are adopted as base models in the proposed framework, and we demonstrate the effectiveness of MV-GNN through extensive experiments.|图形神经网络通过分层邻域聚合实现了对图形相关任务的高性能处理。以往的工作都是通过设计各层的内射邻域聚合函数来实现强大的功能，但是这种功能很难确定，而且大量的附加参数使得这些模型的训练变得非常困难。正是输入空间和聚合函数同时实现了强大的功能。为了提高模型的表达能力，我们提出了一个简单有效的 MV-GNN 框架，通过构造新的输入空间来代替设计复杂的聚合函数。准确地说，MV-GNN 为每个节点采样多视图子图，并且任何 GNN 模型都可以应用于这些视图。最后通过内射聚合所有视图得到目标节点的表示。该框架采用两种典型的 GNN (即 GCN 和 GAT)作为基本模型，并通过大量实验验证了 MV-GNN 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Simple+Multi-view+Can+Bring+Powerful+Graph+Neural+Network)|0|
|[EDITS: An Easy-to-difficult Training Strategy for Cloud Failure Prediction](https://doi.org/10.1145/3543873.3584630)|Qingwei Lin, Tianci Li, Pu Zhao, Yudong Liu, Minghua Ma, Lingling Zheng, Murali Chintalapati, Bo Liu, Paul Wang, Hongyu Zhang, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang|Microsoft Research, China; Microsoft Azure, USA; Peking University, Microsoft Research, China; Microsoft 365, USA; Chongqing University, The University of Newcastle, China; Microsoft 365, China|Cloud failures have been a major threat to the reliability of cloud services. Many failure prediction approaches have been proposed to predict cloud failures before they actually occur, so that proactive actions can be taken to ensure service reliability. In industrial practice, existing failure prediction approaches mainly focus on utilizing state-of-the-art time series models to enhance the performance of failure prediction but neglect the training strategy. However, as curriculum learning points out, models perform better when they are trained with data in an order of easy-to-difficult. In this paper, we propose EDITS, a novel training strategy for cloud failure prediction, which greatly improves the performance of the existing cloud failure prediction models. Our experimental results on industrial and public datasets show that EDITS can obviously enhance the performance of cloud failure prediction model. In addition, EDITS also outperforms other curriculum learning methods. More encouragingly, our proposed EDITS has been successfully applied to Microsoft 365 and Azure online service systems, and has obviously reduced financial losses caused by cloud failures.|云故障一直是云服务可靠性的主要威胁。已经提出了许多故障预测方法，以便在云故障实际发生之前进行预测，从而可以采取主动行动来确保服务的可靠性。在工业实践中，现有的故障预测方法主要是利用最新的时间序列模型来提高故障预测的性能，而忽视了训练策略。然而，正如课程学习所指出的那样，当模型使用数据按“易于难”的顺序进行训练时，它们的表现会更好。本文提出了一种新的云失效预测训练策略 EDITS，大大提高了现有云失效预测模型的性能。我们在工业和公共数据集上的实验结果表明，EDITS 可以明显提高云失效预测模型的性能。此外，EDITS 还优于其他课程学习方法。更令人鼓舞的是，我们提出的 EDITS 已经成功地应用于微软365和 Azure 在线服务系统，显然减少了云故障造成的财务损失。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EDITS:+An+Easy-to-difficult+Training+Strategy+for+Cloud+Failure+Prediction)|0|
|[Multi-Agent Reinforcement Learning with Shared Policy for Cloud Quota Management Problem](https://doi.org/10.1145/3543873.3584634)|Tong Cheng, Hang Dong, Lu Wang, Bo Qiao, Si Qin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Thomas Moscibroda|Microsoft Research, China; Microsoft 365, USA; Microsoft Azure, USA|Quota is often used in resource allocation and management scenarios to prevent abuse of resource and increase the efficiency of resource utilization. Quota management is usually fulfilled with a set of rules maintained by the system administrator. However, maintaining these rules usually needs deep domain knowledge. Moreover, arbitrary rules usually cannot guarantee both high resource utilization and fairness at the same time. In this paper, we propose a reinforcement learning framework to automatically respond to quota requests in cloud computing platforms with distinctive usage characteristics for users. Extensive experimental results have demonstrated the superior performance of our framework on achieving a great trade-off between efficiency and fairness.|配额经常用于资源分配和管理场景，以防止资源滥用和提高资源利用效率。配额管理通常由系统管理员维持一套规则来执行。然而，维护这些规则通常需要深入的领域知识。此外，任意规则通常不能同时保证高资源利用率和公平性。在本文中，我们建议一个强化学习架构，以自动回应用户在具有不同使用特征的云端运算平台上的配额要求。大量的实验结果表明，我们的框架在实现效率和公平之间的巨大平衡方面具有优越的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Agent+Reinforcement+Learning+with+Shared+Policy+for+Cloud+Quota+Management+Problem)|0|
|[Job Type Extraction for Service Businesses](https://doi.org/10.1145/3543873.3584636)|Cheng Li, Yaping Qi, Hayk Zakaryan, Mingyang Zhang, Michael Bendersky, Yonghua Wu, Marc Najork|Google, USA|Google My Business (GMB) is a platform that hosts business profiles, which will be displayed when a user issues a relevant query on Google Search or Google Maps. GMB businesses provide a wide variety of services, from home cleaning and repair, to legal consultation. However, the exact details of the service provided (a.k.a. job types), are often missing in business profiles. This places the burden of finding these details on the users. To alleviate this burden, we built a pipeline to automatically extract the job types from business websites. We share the various challenges we faced while developing this pipeline, and how we effectively addressed these challenges by (1) utilizing structured content to tackle the cold start problem for dataset collection; (2) exploiting context information to improve model performance without hurting scalability; and (3) formulating the extraction problem as a retrieval task to improve both generalizability, efficiency, and coverage. The pipeline has been deployed for over a year and is scalable enough to be periodically refreshed. The extracted job types are serving users of Google Search and Google Maps, with significant improvements in both precision and coverage.|Google My Business (GMB)是一个托管业务档案的平台，当用户在 Google 搜索或 Google 地图上发出相关查询时，将显示这些档案。专线小巴业务提供多种服务，从家居清洁和维修，以法律咨询。然而，所提供的服务的确切细节(也就是作业类型)在业务概要文件中经常缺失。这将查找这些详细信息的负担放在用户身上。为了减轻这种负担，我们建立了一个管道，可以自动从商业网站提取工作类型。我们分享了我们在开发这个流水线时面临的各种挑战，以及我们如何有效地解决这些挑战: (1)利用结构化内容来解决数据集收集的冷启动问题; (2)利用上下文信息来提高模型性能而不损害可扩展性; (3)将提取问题作为一个检索任务来提高概括性、效率和覆盖率。该管道已经部署了一年多，可伸缩性足以定期更新。提取出来的工作类型为谷歌搜索和谷歌地图的用户提供服务，在精确度和覆盖率方面都有显著提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Job+Type+Extraction+for+Service+Businesses)|0|
|[A Practical Rule Learning Framework for Risk Management](https://doi.org/10.1145/3543873.3584644)|Jun Zhou, Meng Li, Lu Yu, Longfei Li, Fei Wu|Ant Group, China; College of Computer Science and Technology, Zhejiang University, China and Ant Group, China; College of Computer Science and Technology, Zhejiang University, China|Identifying the fraud risk of applications on the web platform is a critical challenge with both requirements of effectiveness and interpretability. In these high-stakes web applications especially in financial scenarios, decision rules have been extensively used due to the rising requirements for explainable artificial intelligence (XAI). In this work, we develop a rule learning framework with rule mining and rule refining modules for addressing the learning efficiency and class imbalance issues while making the decision rules more broadly and simply applicable to risk management scenarios. On four benchmark data-sets and two large-scale data-sets, the classification performance, interpretability, and scalability of the framework have been proved, achieving at least a 26.2% relative improvement over the state-of-the-art (SOTA) models. The system is currently being used by hundreds of millions of users and dealing with an enormous number of transactions in Ant Group, which is one of the largest mobile payment platforms in the world.|鉴于有效性和可解释性的要求，确定网络平台上应用程序的欺诈风险是一个关键的挑战。在这些高风险的网络应用中，尤其是在金融场景中，由于对可解释人工智能(XAI)的需求日益增长，决策规则得到了广泛的应用。在这项工作中，我们开发了一个规则学习框架与规则挖掘和规则精炼模块，以解决学习效率和类不平衡的问题，同时使决策规则更广泛和简单地适用于风险管理场景。在四个基准数据集和两个大规模数据集上，证明了该框架的分类性能、可解释性和可扩展性，比最新的 SOTA 模型至少提高了26.2% 。蚂蚁集团是世界上最大的移动支付平台之一，目前有数亿用户正在使用该系统处理大量交易。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Practical+Rule+Learning+Framework+for+Risk+Management)|0|
|[WAM-studio, a Digital Audio Workstation (DAW) for the Web](https://doi.org/10.1145/3543873.3587987)|Michel Buffa, Antoine VidalMazuy|University Côte d'Azur / I3S Laboratory / INRIA, France; Université Côte d'Azur / I3S / INRIA, France|This paper presents WAM Studio, an open source, online Digital Audio Workstation (DAW) that takes advantages of several W3C Web APIs, such as Web Audio, Web Assembly, Web Components, Web Midi, Media Devices etc. It also uses the Web Audio Modules proposal that has been designed to facilitate the development of inter-operable audio plugins (effects, virtual instruments, virtual piano keyboards as controllers etc.) and host applications. DAWs are feature-rich software and therefore particularly complex to develop in terms of design, implementation, performances and ergonomics. Very few commercial online DAWs exist today and the only open-source examples lack features (no support for inter-operable plugins, for example) and do not take advantage of the recent possibilities offered by modern W3C APIs (e.g. AudioWorklets/Web Assembly). WAM Studio was developed as an open-source technology demonstrator with the aim of showcasing the potential of the web platform, made possible by these APIs. The paper highlights some of the difficulties we encountered (i.e limitations due to the sandboxed and constrained environments that are Web browsers, latency compensation etc.). An online demo, as well as a GitHub repository for the source code are available.|这篇文章介绍了 WAM 工作室，一个开源的在线数位音频工作站，它利用了一些 W3C 网络应用程序接口的优势，例如网络音频、网络组装、网络组件、网络迷笛、媒体设备等。它还使用了网络音频模块的建议，旨在促进互操作的音频插件(效果，虚拟乐器，虚拟钢琴键盘作为控制器等)和主机应用程序的开发。DAW 是功能丰富的软件，因此在设计、实现、性能和人机工程学方面开发特别复杂。现在很少有商业在线 DAW 存在，唯一的开源示例缺乏特性(例如，不支持可互操作的插件) ，也没有利用现代 W3C API (例如 AudioWorklets/Web Assembly)提供的最新可能性。WAM 工作室是作为一个开源样品开发的，目的是展示 web 平台的潜力，这些 API 使其成为可能。本文强调了我们遇到的一些困难(即由于沙箱和 Web 浏览器等受限环境的限制，延迟补偿等)。可以使用在线演示和 GitHub 源代码库。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=WAM-studio,+a+Digital+Audio+Workstation+(DAW)+for+the+Web)|0|
|[The Capable Web](https://doi.org/10.1145/3543873.3587988)|Thomas Steiner|Google Germany GmbH, Germany|In this paper, I discuss arguments in favor and in disfavor of building for the Web. I look at three extraordinary examples of apps built for the Web, and analyze reasons their creators provided for doing so. In continuation, I look at the decline of interest in cross-platform app frameworks with the exception of Flutter, which leads me to the two research questions RQ1 "Why do people not fully bet on PWA" and RQ2 "Why is Flutter so popular". My hypothesis for why developers don’t more frequently set on the Web is that in many cases they (or their non-technical reporting lines) don’t realize how powerful it has become. To counter that, I introduce a Web app and a browser extension that demonstrate the Web’s capabilities.|在本文中，我讨论了支持和反对为 Web 构建的论点。我看了三个为 Web 构建的非凡应用程序的例子，并分析了它们的创建者为此提供的原因。接下来，我看到除了 Flutter 之外，人们对跨平台应用框架的兴趣正在下降，这让我想到了两个研究问题: RQ1“为什么人们没有完全押注于 PWA”和 RQ2“为什么 Flutter 如此受欢迎”。我对开发人员为什么不更频繁地设置在 Web 上的假设是，在许多情况下，他们(或者他们的非技术报告线)没有意识到 Web 已经变得多么强大。为了解决这个问题，我引入了一个 Web 应用程序和一个浏览器扩展来展示 Web 的功能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Capable+Web)|0|
|[MetaCrimes: Criminal accountability for conducts in the Metaverse](https://doi.org/10.1145/3543873.3587535)|Gian Marco Bovenzi|Center for Higher Defence Studies, University of Turin, Italy|The research addresses a topic whose precise boundaries are yet to be defined: the criminal accountability for conducts committed in the Metaverse. Following a short introduction motivating the reason why this issue has to be considered as pivotal both for the Web and for the society, the main problem raised by the research will be identified, namely, whether an action taken against a person, that in real-life would be a criminal conduct, is considerable as a crime in the Metaverse as well. A short assessment of the (very little so far) current state of the art, as well as the proposed approach and methodology will be then overviewed; finally, the contribution shows its current results, and concludes stating that countries are highly encouraged to shape respective criminal frameworks when applied to the Metaverse, and that the international community should consider the topic as a priority in its agenda. Nevertheless, further experimental and research work has still to be made.|这项研究涉及一个尚未界定确切界限的主题: 在元宇宙中犯下的行为的刑事责任。在简短介绍了为什么这个问题必须被认为是网络和社会的关键的原因之后，研究提出的主要问题将被确定，即，对一个人采取的行动，在现实生活中将是一种犯罪行为，在元宇宙中是否也是一种犯罪。接下来，我们将对目前的最新技术状况(到目前为止很少)进行简短的评估，并对提出的方法和方法进行概述; 最后，这份报告展示了目前的结果，并得出结论说，当适用于元宇宙时，各国都被大力鼓励形成各自的犯罪框架，国际社会应该将这一主题作为其议程上的优先事项。然而，进一步的实验和研究工作还有待开展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MetaCrimes:+Criminal+accountability+for+conducts+in+the+Metaverse)|0|
|[SEKA: Seeking Knowledge Graph Anomalies](https://doi.org/10.1145/3543873.3587536)|Asara Senaratne|School of Computing, The Australian National University, Australia|Knowledge Graphs (KGs) form the backbone of many knowledge dependent applications such as search engines and digital personal assistants. KGs are generally constructed either manually or automatically using a variety of extraction techniques applied over multiple data sources. Due to the diverse quality of these data sources, there are likely anomalies introduced into any KG. Hence, it is unrealistic to expect a perfect archive of knowledge. Given how large KGs can be, manual validation is impractical, necessitating an automated approach for anomaly detection in KGs. To improve KG quality, and to identify interesting and abnormal triples (edges) and entities (nodes) that are worth investigating, we introduce SEKA, a novel unsupervised approach to detect anomalous triples and entities in a KG using both the structural characteristics and the content of edges and nodes of the graph. While an anomaly can be an interesting or unusual discovery, such as a fraudulent transaction requiring human intervention, anomaly detection can also identify potential errors. We propose a novel approach named Corroborative Path Algorithm to generate a matrix of semantic features, which we then use to train a one-class Support Vector Machine to identify abnormal triples and entities with no dependency on external sources. We evaluate our approach on four real-world KGs demonstrating the ability of SEKA to detect anomalies, and to outperform comparative baselines.|知识图(KGs)构成了许多依赖于知识的应用程序(如搜索引擎和数字个人助理)的主干。幼稚园一般以人手或自动方式建构，并采用多种抽取技术，应用于多个资料来源。由于这些数据源的质量各不相同，可能会在任何 KG 中引入异常。因此，期望一个完美的知识档案是不现实的。鉴于幼稚园的规模庞大，人手验证并不切实可行，因此必须采用自动化方法处理幼稚园的异常检测。为了提高 KG 质量，并识别有趣和异常的三元组(边)和实体(节点)值得研究，我们引入 SEKA，一种新的无监督方法来检测 KG 中的异常三元组和实体，使用图的结构特征和边和节点的内容。虽然异常可能是一个有趣或不寻常的发现，例如需要人为干预的欺诈性交易，但异常检测也可以识别潜在的错误。我们提出了一种新颖的方法，称为证实路径算法，以生成一个语义特征矩阵，然后我们使用训练一个一类的支持向量机识别不正常的三元组和实体没有依赖于外部来源。我们评估了我们的方法在四个真实世界的幼儿园展示 SEKA 的能力，检测异常，并超过比较基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SEKA:+Seeking+Knowledge+Graph+Anomalies)|0|
|[Detecting Cross-Lingual Information Gaps in Wikipedia](https://doi.org/10.1145/3543873.3587539)|Vahid Ashrafimoghari|Stevens Institute of Technology, USA|An information gap exists across Wikipedia’s language editions, with a considerable proportion of articles available in only a few languages. As an illustration, it has been observed that 10 languages possess half of the available Wikipedia articles, despite the existence of 330 Wikipedia language editions. To address this issue, this study presents an approach to identify the information gap between the different language editions of Wikipedia. The proposed approach employs Latent Dirichlet Allocation (LDA) to analyze linked entities in a cross-lingual knowledge graph in order to determine topic distributions for Wikipedia articles in 28 languages. The distance between paired articles across language editions is then calculated. The potential applications of the proposed algorithm to detecting sources of information disparity in Wikipedia are discussed, and directions for future research are put forward.|维基百科的语言版本之间存在着信息鸿沟，相当一部分文章只有几种语言版本。举例来说，有10种语言拥有维基百科文章的一半，尽管维基百科有330个语言版本。为了解决这一问题，本研究提出了一种方法，以确定信息差距的不同语言版本的维基百科。该方法使用隐含狄利克雷分布(LDA)来分析跨语言知识图表中的链接实体，以确定28种语言的维基百科文章的主题分布。然后计算跨语言版本的成对文章之间的距离。讨论了该算法在维基百科信息差源检测中的潜在应用，并对未来的研究方向进行了展望。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Cross-Lingual+Information+Gaps+in+Wikipedia)|0|
|[Caught in the Game: On the History and Evolution of Web Browser Gaming](https://doi.org/10.1145/3543873.3585572)|Naif Mehanna, Walter Rudametkin|Univ Lille, CNRS, Inria, UMR 9189 CRIStAL, France; Univ Rennes, Institut Universitaire de France (IUF), CNRS, Inria, UMR 6074 IRISA, France|Web browsers have come a long way since their inception, evolving from a simple means of displaying text documents over the network to complex software stacks with advanced graphics and network capabilities. As personal computers grew in popularity, developers jumped at the opportunity to deploy cross-platform games with centralized management and a low barrier to entry. Simply going to the right address is now enough to start a game. From text-based to GPU-powered 3D games, browser gaming has evolved to become a strong alternative to traditional console and mobile-based gaming, targeting both casual and advanced gamers. Browser technology has also evolved to accommodate more demanding applications, sometimes even supplanting functions typically left to the operating system. Today, websites display rich, computationally intensive, hardware-accelerated graphics, allowing developers to build ever-more impressive applications and games.In this paper, we present the evolution of browser gaming and the technologies that enabled it, from the release of the first text-based games in the early 1990s to current open-world and game-engine-powered browser games. We discuss the societal impact of browser gaming and how it has allowed a new target audience to accessdigital gaming. Finally, we review the potential future evolution ofthe browser gaming industry.|Web 浏览器自问世以来已经走过了漫长的道路，从通过网络显示文本文档的简单方法演变为具有先进图形和网络功能的复杂软件栈。随着个人电脑越来越受欢迎，开发人员抓住机会部署集中管理和低门槛的跨平台游戏。现在只要找到正确的地址就足以开始一场游戏了。从基于文本的游戏到基于 GPU 的3D 游戏，浏览器游戏已经发展成为传统游戏机和基于移动设备的游戏的强大替代品，目标客户既包括休闲玩家，也包括高级玩家。浏览器技术也在发展，以适应更加苛刻的应用程序，有时甚至取代通常留给操作系统的功能。今天，网站显示丰富的，计算密集的，硬件加速的图形，允许开发人员建立更令人印象深刻的应用程序和游戏。在这篇文章中，我们介绍了浏览器游戏的演变和技术，从20世纪90年代初发布的第一个基于文本的游戏到现在的开放世界和游戏引擎驱动的浏览器游戏。我们讨论了浏览器游戏的社会影响，以及它是如何让一个新的目标受众访问数字游戏的。最后，我们回顾了未来浏览器游戏产业的潜在发展趋势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Caught+in+the+Game:+On+the+History+and+Evolution+of+Web+Browser+Gaming)|0|
|[Identifying Stable States of Large Signed Graphs](https://doi.org/10.1145/3543873.3587544)|Muhieddine Shebaro, Jelena Tesic|Department of Computer Science, Texas State University, USA|Signed network graphs provide a way to model complex relationships and interdependencies between entities: negative edges allow for a deeper study of social dynamics. One approach to achieving balance in a network is to model the sources of conflict through structural balance. Current methods focus on computing the frustration index or finding the largest balanced clique, but these do not account for multiple ways to reach a consensus or scale well for large, sparse networks. In this paper, we propose an expansion of the frustration cloud computation and compare various tree-sampling algorithms that can discover a high number of diverse balanced states. Then, we compute and compare the frequencies of balanced states produced by each. Finally, we investigate these techniques’ impact on the consensus feature space.|签名网络图提供了一种模拟实体之间复杂关系和相互依赖的方法: 负边允许对社会动态进行更深入的研究。在网络中实现平衡的一种方法是通过结构平衡来模拟冲突的根源。目前的方法侧重于计算挫折指数或寻找最大的平衡集团，但这些并没有考虑到多种方式达成共识或规模良好的大型，稀疏的网络。本文提出了一种挫折云计算的扩展方法，并对各种能够发现大量不同平衡状态的树抽样算法进行了比较。然后，我们计算并比较了每种方法产生的平衡态的频率。最后，我们研究了这些技术对一致性特征空间的影响。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Identifying+Stable+States+of+Large+Signed+Graphs)|0|
|[Those who are left behind: A chronicle of internet access in Cuba](https://doi.org/10.1145/3543873.3585573)|Brenda Reyes Ayala|University of Alberta, Canada|This paper presents a personal chronicle of internet access in Cuba from the perspective of a visitor to the island. It is told across three time periods: 1997, 2010, and 2021. The story describes how the island first connected to the internet in the 90s, how internet access evolved throughout the 2000s, and ends in the role the internet played in the government protests on July 11, 2021. The article analyzes how internet access in Cuba has changed over the decades and its effects on civil society. It discusses issues such as Cuba’s technological infrastructure, internet censorship, and free expression.|本文从一位访问古巴的游客的角度，介绍了古巴互联网接入的个人编年史。它分为三个时间段: 1997年、2010年和2021年。这个故事描述了这个岛屿在90年代如何首次连接到互联网，互联网接入在整个2000年代如何演变，并以互联网在2021年7月11日政府抗议活动中所扮演的角色告终。文章分析了几十年来古巴互联网接入的变化及其对公民社会的影响。报告讨论了古巴的技术基础设施、互联网审查和言论自由等问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Those+who+are+left+behind:+A+chronicle+of+internet+access+in+Cuba)|0|
|[Reflex-in: Generate Music on the Web with Real-time Brain Wave](https://doi.org/10.1145/3543873.3587315)|Shihong Ren, Michel Buffa, Laurent Pottier, Yang Yu, Gerwin Schalk|Shanghai Key Laboratory for Music Acoustic, Shanghai Conservatory of Music, China; Frontier Lab for Applied Neurotechnology, Tianqiao and Chrissy Chen Institute, China; Laboratoire d'Informatique, Signaux et Systèmes de Sophia Antipolis, Université Côte d'Azur, France; Shanghai Key Laboratory for Music Acoustic, Shanghai Conservatory of Music, China and Laboratoire d'Études du Contemporain en Littératures, Langues, Arts, Université Jean Monnet, France; Laboratoire d'Études du Contemporain en Littératures, Langues, Arts, Université Jean Monnet, France|Reflex-in is a sound installation that uses brain-wave streams to create music composition within the Web environment in real time. The work incorporates various state-of-the-art Web technologies, including Web Audio, WebSocket, WebAssembly, and WebGL. The music generated from the algorithm - mapping brain wave signal to musical events - aims to produce a form of furniture music that is relaxing and meditative, possibly therapeutic. This effect can be further enhanced through binaural beats or other forms of auditory stimulation, also known as “digital drugs,” which can be enabled through the user interface. The system represents a potential avenue for the development of closed-loop brain-computer interfaces by using the listener’s own brain waves as the source of musical stimuli, which can be used for therapeutic or medical purposes.|Reflex-in 是一个声音装置，利用脑电波流在网络环境中实时创建音乐创作。这项工作结合了各种最先进的 Web 技术，包括 Web Audio、 WebSocket、 WebAssembly 和 WebGL。由算法产生的音乐——将脑电波信号映射到音乐事件——旨在产生一种家具音乐的形式，这种音乐是放松和冥想的，可能是治疗性的。这种效果可以通过双耳节拍或其他形式的听觉刺激进一步加强，也被称为“数字药物”，这可以通过用户界面启用。该系统利用听众自身的脑电波作为音乐刺激源，为开发闭环脑-计算机接口提供了一条潜在的途径，可用于治疗或医疗目的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reflex-in:+Generate+Music+on+the+Web+with+Real-time+Brain+Wave)|0|
|[Wikidata: The Making Of](https://doi.org/10.1145/3543873.3585579)|Denny Vrandecic, Lydia Pintscher, Markus Krötzsch|Wikimedia Foundation, USA; TU Dresden, Germany; Wikimedia Deutschland, Germany|Wikidata, now a decade old, is the largest public knowledge graph, with data on more than 100 million concepts contributed by over 560,000 editors. It is widely used in applications and research. At its launch in late 2012, however, it was little more than a hopeful new Wikimedia project, with no content, almost no community, and a severely restricted platform. Seven years earlier still, in 2005, it was merely a rough idea of a few PhD students, a conceptual nucleus that had yet to pick up many important influences from others to turn into what is now called Wikidata. In this paper, we try to recount this remarkable journey, and we review what has been accomplished, what has been given up on, and what is yet left to do for the future.|维基数据，现在已经有十年的历史，是最大的公共知识图表，由超过56万名编辑贡献了超过1亿个概念的数据。它在应用和研究中得到了广泛的应用。然而，在2012年末推出时，它只不过是一个充满希望的新维基媒体项目，没有内容，几乎没有社区，平台受到严格限制。七年前，也就是2005年，这只是几个博士生的一个粗略想法，一个概念核心，还没有从其他人那里获得许多重要的影响，转变成现在所谓的 Wikidata。在这篇文章中，我们试图回顾这个非凡的旅程，我们回顾什么已经完成，什么已经放弃，还有什么是未来要做的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Wikidata:+The+Making+Of)|0|
|[A History of Diversity in The Web (Conference)](https://doi.org/10.1145/3543873.3585576)|Siddharth D. Jaiswal, Animesh Mukherjee|Indian Institute of Technology, Kharagpur, India|The Web has grown considerably since its inception and opened up a multitude of opportunities for people all around the world for work, leisure, and learning. These opportunities were limited to western audiences earlier on, but globalization has now put almost the entire world online. While there is a growing social understanding and acknowledgment of various gender and ethnic groups in society, we still have a long way to go toward achieving equity in gender and ethnic representations, especially in the workplace. In this paper, we attempt to quantify the diversity and evenness in terms of gender and ethnicity of The WebConference participants over its 30 year history. The choice is motivated by the monumental contribution of this conference to the evolution of the web. In particular, we study the gender and ethnicity of program committee members, authors and other speakers at the conference between 1994-2022. We also generate the co-speaker network over the three decades to study how closely the speakers work with each other. Our findings show that we still have a long way to go before achieving fair representation at The WebConference, especially for female participants and individuals from non-White, non-Asian ethnicities.|自从网络诞生以来，它已经有了很大的发展，为世界各地的人们提供了大量的工作、休闲和学习的机会。早些时候，这些机会仅限于西方受众，但现在全球化已经使几乎整个世界上网。尽管社会上对各种性别和族裔群体的理解和认可日益增加，但要实现性别和族裔代表性方面的公平，特别是在工作场所，我们仍有很长的路要走。在本文中，我们试图量化的多样性和均匀性的性别和种族方面的网络会议与会者在其30年的历史。这一选择的动机是这次会议对网络发展的巨大贡献。特别是，我们研究了1994-2022年间项目委员会成员、作者和其他发言者的性别和种族。我们还在过去三十年中建立了共同发言者网络，以研究发言者之间的合作程度。我们的研究结果表明，要在网络会议上实现公平的代表性，我们还有很长的路要走，特别是对于女性与会者和来自非白人、非亚洲种族的个人来说。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+History+of+Diversity+in+The+Web+(Conference))|0|
|[Why are Hyperlinks Blue?: A deep dive into browser hyperlink color history](https://doi.org/10.1145/3543873.3587714)|Elise Blanchard|Mozilla, USA|The internet has ingrained itself into every aspect of our lives, but there's one aspect of the digital world that some take for granted. Did you ever notice that many links, specifically hyperlinks, are blue? When a coworker casually asked me why links are blue, I was stumped. As a user experience designer who has created websites since 2001, I've always made my links blue. I have advocated for the specific shade of blue, and for the consistent application of blue, yes, but I've never stopped and wondered, why are links blue? It was just a fact of life. Grass is green and hyperlinks are blue. Culturally, we associate links with the color blue so much that in 2016, when Google changed its links to black, it created quite a disruption [1]. But now, I find myself all consumed by the question, WHY are links blue? WHO decided to make them blue? WHEN was this decision made, and HOW has this decision made such a lasting impact? Mosaic, an early browser released by Marc Andreessen and Eric Bina on January 23, 1993 [2], had blue hyperlinks. To truly understand the origin and evolution of hyperlinks, I took a journey through technology history and interfaces to explore how links were handled before color monitors, and how interfaces and hyperlinks rapidly evolved once color monitors became an option.|互联网已经在我们生活的方方面面根深蒂固，但是在数字世界的某一方面，有些人认为是理所当然的。你有没有注意到许多链接，特别是超链接，是蓝色的？当一位同事不经意地问我为什么链接是蓝色的时候，我被难住了。作为一个从2001年开始创建网站的用户体验设计师，我总是把我的链接设成蓝色。我一直主张特定的蓝色阴影，并为蓝色的一致应用，是的，但我从来没有停止和疑惑，为什么链接是蓝色的？这就是生活。草是绿色的，超链接是蓝色的。从文化上讲，我们把链接和蓝色联系在一起，以至于在2016年，当谷歌把链接改成黑色时，它造成了相当大的破坏[1]。但是现在，我发现自己完全被这个问题所困扰，为什么链接是蓝色的？谁决定把它们变成蓝色的？这个决定是什么时候做出的? 这个决定是如何产生如此持久的影响的？马克 · 安德森(Marc Andreessen)和埃里克 · 比纳(Eric Bina)于1993年1月23日发布的早期浏览器 Mosaic 有蓝色的超链接。为了真正理解超链接的起源和演变，我通过技术历史和界面来探索在彩色显示器出现之前链接是如何处理的，以及一旦彩色显示器成为一种选择，界面和超链接是如何迅速演变的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Why+are+Hyperlinks+Blue?:+A+deep+dive+into+browser+hyperlink+color+history)|0|
|[The One Hundred Year Web](https://doi.org/10.1145/3543873.3585578)|Steven Pemberton|CWI, Netherlands|The year 2023 marks the thirty-second anniversary of the World Wide Web being announced. In the intervening years, the web has become an essential part of the fabric of society. Part of that is that huge amounts of information that used to be available (only) on paper is now available (only) electronically. One of the dangers of this is that owners of information often treat the data as ephemeral, and delete old information once it becomes out of date. As a result society is at risk of losing large parts of its history. So it is time to assess how we use the web, how it has been designed, and what we should do to ensure that in one hundred years time (and beyond) we will still be able to access, and read, what we are now producing. We can still read 100 year-old books; that should not be any different for the web. This paper takes a historical view of the web, and discusses the web from its early days: why it was successful compared with other similar systems emerging at the time, the things it did right, the mistakes that were made, and how it has developed to the web we know today, to what extent it meets the requirements needed for such an essential part of society's infrastructure, and what still needs to be done.|2023年是万维网发布32周年。在这些年里，网络已经成为社会结构的重要组成部分。部分原因是过去只能在纸上获得的大量信息现在只能通过电子方式获得。这样做的危险之一是，信息的所有者常常将数据视为昙花一现，一旦过时就会删除旧信息。因此，社会面临着失去大部分历史的危险。因此，现在是时候评估我们如何使用网络，它是如何被设计的，以及我们应该做些什么来确保在一百年以后我们仍然能够访问和阅读我们现在正在生产的东西。我们仍然可以阅读100年前的书籍，这对于网络来说应该没有什么不同。本文从历史的角度来看待万维网，并讨论了万维网的早期发展: 为什么它比当时出现的其他类似系统更成功，它做对了什么，犯了什么错误，以及它是如何发展到我们今天所知道的万维网的，它在多大程度上满足了社会基础设施这一重要组成部分所需要的要求，以及还需要做些什么。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+One+Hundred+Year+Web)|0|
|[Followers Tell Who an Influencer Is](https://doi.org/10.1145/3543873.3587576)|Dheeman Saha, Md Rashidul Hasan, Abdullah Mueen|Department of Mathematics and Statistics, University of New Mexico, USA; Department of Computer Science, University of New Mexico, USA|Influencers are followed by a relatively smaller group of people on social media platforms under a common theme. Unlike the global celebrities, it is challenging to categorize influencers into general categories of fame (e.g., Politics, Religion, Entertainment, etc.) because of their overlapping and narrow reach to people interested in these categories. In this paper, we focus on categorizing influencers based on their followers. We exploit the top-1K Twitter celebrities to identify the common interest among the followers of an influencer as his/her category. We annotate the top one thousand celebrities in multiple categories of popularity, language, and locations. Such categorization is essential for targeted marketing, recommending experts, etc. We define a novel FollowerSimilarity between the set of followers of an influencer and a celebrity. We propose an inverted index to calculate similarity values efficiently. We exploit the similarity score in a K-Nearest Neighbor classifier and visualize the top celebrities over a neighborhood-embedded space.|在社交媒体平台上，只有相对较少的一部分人关注影响者，他们的主题是一致的。与全球名人不同的是，将影响者分为一般的名声类别(例如，政治、宗教、娱乐等)是很有挑战性的，因为这些类别对于感兴趣的人来说是重叠和狭窄的。在这篇文章中，我们着重于根据他们的追随者对影响者进行分类。我们利用前1000名的 Twitter 名人来确定一个有影响力的人的追随者的共同兴趣作为他/她的类别。我们根据受欢迎程度、语言和地理位置来评选前一千位名人。这样的分类对于有针对性的营销、推荐专家等是必不可少的。我们定义了一个小说的追随者之间的影响者和名人的追随者集相似性。我们提出了一个反向指数来有效地计算相似度值。我们利用 K- 最近邻分类器中的相似性得分，并在邻里嵌入空间上可视化顶级名人。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Followers+Tell+Who+an+Influencer+Is)|0|
|[Smart Cities as Hubs: a use case from Biotechnology](https://doi.org/10.1145/3543873.3587582)|Tsapadikou Asteria, Leonidas G. Anthopoulos|Department of Biochemistry and Biotechnology, University of Thessaly, Greece; Department of Business Administration, University of Thessaly, Greece|Smart city platforms operate as central points of access for locally collected data. The smart city hub (SCHub) introduces a new concept that aims to homogenize data, service, human and material flows in cities. A proof of concept is based on non-typical data flows, like the ones that are collected by biotechnological activities, like Diet-related non-communicable diseases (NCDs). NCDs are responsible for 1 in 5 deaths globally. Most of the diet related NCDs are related to the gut microbiome, the microbial community that resides in our gastrointestinal tract. The imbalance or loss of microbiome diversity is one of the main factors leading to NCDs by affecting various functions, including energy metabolism, intestinal permeability, and brain function. Gut dysbiosis is reflected in altered concentrations of Short Chain Fatty Acids (SCFAs), produced by the gut microbiota. A microcapsule system can play the role of a sensor that collects data from the local community and transmits it to the SCHub in order for the doctors to receive the appropriate patients information and define the appropriate treatment method; for the city to process anonymized information and measure community's health in diet terms. A prototype with a biosensor that correlates the amount of gut SCFAs with gut microbiome functional capacities is presented in this paper, together with the use-case scenario that engages the SCHub.|智能城市平台作为本地收集数据的中心接入点运作。智能城市中心(SCHub)引入了一个新的概念，旨在统一城市中的数据、服务、人员和物质流。概念的证明是基于非典型的数据流，如通过生物技术活动收集的数据流，如与饮食相关的非传染性疾病(NCD)。全球五分之一的死亡是由非传染性疾病造成的。大部分与饮食有关的非传染性疾病都与肠道微生物有关，肠道微生物群落是我们肠粘膜中的微生物群落。微生物多样性的失衡或丧失是影响能量代谢、肠道通透性和脑功能等多种功能而导致非传染性疾病的主要因素之一。肠道生态失调反映在由肠道微生物群产生的短链脂肪酸(SCFAs)浓度的改变上。微胶囊系统可以发挥传感器的作用，从当地社区收集数据，并将其传送到 SCHub，以便医生接收适当的患者信息，并确定适当的治疗方法; 城市处理匿名信息，并以饮食方式衡量社区的健康。本文介绍了一个具有将肠道 SCFAs 量与肠道微生物组功能能力相关联的生物传感器的原型，以及参与 SCHub 的用例场景。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Smart+Cities+as+Hubs:+a+use+case+from+Biotechnology)|0|
|[Do bridges dream of water pollutants? Towards DreamsKG, a knowledge graph to make digital access for sustainable environmental assessment come true](https://doi.org/10.1145/3543873.3587590)|Darío Garigliotti, Johannes Bjerva, Finn Årup Nielsen, Annika Butzbach, Ivar Lyhne, Lone Kørnøv, Katja Hose|Technical University of Denmark, Denmark; Aalborg University, Denmark; Aalborg University, Denmark and TU Wien, Austria|An environmental assessment (EA) report describes and assesses the environmental impact of a series of activities involved in the development of a project. As such, EA is a key tool for sustainability. Improving information access to EA reporting is a billion-euro untapped business opportunity to build an engaging, efficient digital experience for EA. We aim to become a landmark initiative in making this experience come true, by transforming the traditional manual assessment of numerous heterogeneous reports by experts into a computer-assisted approach. Specifically, a knowledge graph that represents and stores facts about EA practice allows for what it is so far only accessible manually to become machine-readable, and by this, to enable downstream information access services. This paper describes the ongoing process of building DreamsKG, a knowledge graph that stores relevant data- and expert-driven EA reporting and practicing in Denmark. Representation of cause-effect relations in EA and integration of Sustainable Developmental Goals (SDGs) are among its prominent features.|环境评估(EA)报告描述并评估了项目开发过程中一系列活动对环境的影响。因此，EA 是实现可持续性的关键工具。改善对 EA 报告的信息访问是一个价值10亿欧元的未开发商业机会，可以为 EA 建立一个吸引人的、高效的数字体验。我们的目标是通过将专家对大量不同报告的传统手工评估转变为计算机辅助办法，成为实现这一经验的一项具有里程碑意义的举措。具体来说，一个表示和存储关于 EA 实践的事实的知识图允许目前为止只能手动访问的东西变得机器可读，并且通过这种方式使下游信息访问服务成为可能。本文描述了正在进行的构建 DreamsKG 的过程，这是一个存储相关数据和专家驱动的 EA 报告和实践的知识图表。因果关系在可持续发展目标中的体现和可持续发展目标的整合是可持续发展目标的显著特征。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Do+bridges+dream+of+water+pollutants?+Towards+DreamsKG,+a+knowledge+graph+to+make+digital+access+for+sustainable+environmental+assessment+come+true)|0|
|[Towards High Resolution Urban Heat Analysis: Incorporating Thermal Drones to Enhance Satellite Based Urban Heatmaps](https://doi.org/10.1145/3543873.3587682)|Bryan Rickens, Navid Hashemi Tonekaboni|Department of Computer Science, College of Charleston, USA|As remote-sensing becomes more actively utilized in the environmental sciences, our research continues the efforts in adapting smart cities by using civilian UAVs and drones for land surface temperature (LST) analysis. Given the increased spatial resolution that this technology provides as compared to standard satellite measurements, we sought to further study the urban heat island (UHI) effect – specifically when it comes to heterogeneous and dynamic landscapes such as the Charleston peninsula. Furthermore, we sought to develop a method to enhance the spatial resolution of publicly available LST temperature data (such as those measured from the Landsat satellites) by building a machine learning model utilizing remote-sensed data from drones. While we found a high correlation and an accurate degree of prediction for areas of open water and vegetation (respectively), our model struggled when it came to areas containing highly impervious surfaces. We believe, however, that these findings further illustrate the discrepancy between high and medium spatial resolutions, and demonstrate how urban environments specifically are prone to inaccurate LST measurements and are uniquely in need of an industry pursuit of higher spatial resolution for hyperlocal environmental sciences and urban analysis.|随着遥感技术在环境科学中的应用越来越活跃，我们的研究继续通过使用民用无人机和无人机进行地表温度(LST)分析来适应智能城市。与标准卫星测量相比，这种技术提供了更高的空间分辨率，因此我们试图进一步研究城市热岛效应——特别是当涉及到查尔斯顿半岛等异质和动态景观时。此外，我们试图开发一种方法，通过利用无人机的遥感数据建立一个机器学习模型，来提高公开可用的 LST 温度数据(例如从 Landsat 卫星测量的数据)的空间分辨率。虽然我们发现了开放水域和植被区域(分别)的高度相关性和准确度，但我们的模型在涉及到高度不透水表面的区域时遇到了困难。然而，我们认为，这些发现进一步说明了高和中等空间分辨率之间的差异，并表明城市环境特别容易出现不准确的 LST 测量，并且特别需要为超地方环境科学和城市分析寻求更高的空间分辨率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+High+Resolution+Urban+Heat+Analysis:+Incorporating+Thermal+Drones+to+Enhance+Satellite+Based+Urban+Heatmaps)|0|
|[Towards a Sustainability Index Calculator for Smart Cities](https://doi.org/10.1145/3543873.3587683)|Ramesh Gorantla, Srividya Bansal|Arizona State University, USA|In this era of rapid urbanization and our endeavors to create more smart cities, it's crucial to keep track of how our society and neighborhood are getting impacted. It is important to make conscious decisions to keep harmony in sustainability. There are multiple frameworks to evaluate how sustainability is measured and to understand how sustainable a place is, be it a city or a region, and one such framework is the Circles of Sustainability. Though these frameworks offer good solutions, it is a challenge to collect relevant data to make the framework widely usable. This paper focuses on this specific issue by utilizing the methodology introduced in the framework and applying it practically to better understand how sustainable our cities and society are. We present a unique web-based application which utilizes publicly accessible data to compute sustainability scores and rank for every city and presents the results in an intelligent and easy to comprehend visual interface. The paper also discusses the technical difficulties associated with creating such an application, including data collection, data processing, data integration, and scoring algorithm. The paper concludes by discussing the needs for such practical solutions for promoting sustainable urban development.|在这个快速城市化的时代，我们努力创造更多的智慧城市，跟踪我们的社会和邻里如何受到影响是至关重要的。做出有意识的决定以保持可持续性中的和谐是很重要的。有多个框架可以评估如何衡量可持续性，并了解一个地方(无论是一个城市还是一个地区)的可持续程度，其中一个框架就是可持续发展循环。尽管这些框架提供了很好的解决方案，但是收集相关数据以使框架广泛可用仍然是一个挑战。本文着眼于这一具体问题，利用框架中介绍的方法，并将其实际应用于更好地理解我们的城市和社会的可持续性。我们提出了一个独特的网络应用程序，利用公开数据来计算每个城市的可持续发展得分和排名，并在一个智能和易于理解的视觉界面上呈现结果。本文还讨论了与创建此类应用程序相关的技术难点，包括数据收集、数据处理、数据集成和评分算法。文章最后讨论了促进城市可持续发展的实际解决方案的需要。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+a+Sustainability+Index+Calculator+for+Smart+Cities)|0|
|[Sustainable Grain Transportation in Ukraine Amidst War Utilizing KNARM and KnowWhereGraph](https://doi.org/10.1145/3543873.3587618)|Yinglun Zhang, Antonina Broyaka, Jude Kastens, Allen M. Featherstone, Cogan Shimizu, Pascal Hitzler, Hande KüçükMcGinty|Department of Agricultural Economics, Kansas State Univerisity, USA; Kansas State University, USA; Wright State University, USA; University of Kansas, USA; Kansas State Univerisity, USA|In this work, we propose a sustainable path-finding application for grain transportation during the ongoing Russian military invasion in Ukraine. This application is to build a suite of algorithms to find possible optimal paths for transporting grain that remains in Ukraine. The application uses the KNowledge Acquisition and Representation Methodology(KNARM) and the KnowWhereGraph to achieve this goal. Currently, we are working towards creating an ontology that will allow for a more effective heuristic approach by incorporating the lessons learned from the KnowWhereGraph. The aim is to enhance the path-finding process and provide more accurate and efficient results. In the future, we will continue exploring and implementing new techniques that can further improve the sustainability of the path-finding applications with a knowledge graph backend for grain transportation through hazardous and adversarial environments. The code is available upon reviewer’s request. It can not be made public due to the sensitive nature of the data.|在这项工作中，我们提出了一个可持续的路径寻找应用粮食运输期间正在进行的俄罗斯军事入侵乌克兰。这个应用程序是建立一套算法，以找到可能的最佳路径运输粮食，仍然在乌克兰。应用程序采用知识获取与表示方法(KNARM)和知识图来实现这一目标。目前，我们正在努力创建一个本体论，通过整合从“知识图表”中吸取的经验教训，可以采用更有效的启发式方法。目的是加强路径寻找过程，并提供更准确和有效的结果。今后，我们将继续探索和实施新技术，以进一步提高路径寻找应用程序的可持续性，为通过危险和敌对环境的粮食运输提供知识图表后端。代码可根据审核人员的要求使用。由于数据的敏感性，它不能公开。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sustainable+Grain+Transportation+in+Ukraine+Amidst+War+Utilizing+KNARM+and+KnowWhereGraph)|0|
|[Entity and Event Topic Extraction from Podcast Episode Title and Description Using Entity Linking](https://doi.org/10.1145/3543873.3587648)|Christian Siagian, Amina Shabbeer|Amazon, USA|To improve Amazon Music podcast services and customer engagements, we introduce Entity-Linked Topic Extraction (ELTE) to identify well-known entity and event topics from podcast episodes. An entity can be a person, organization, work-of-art, etc., while an event, such as the Opioid epidemic, occurs at specific point(s) in time. ELTE first extracts key-phrases from episode title and description metadata. It then uses entity linking to canonicalize them against Wikipedia knowledge base (KB), ensuring that the topics exist in the real world. ELTE also models NIL-predictions for entity or event topics that are not in the KB, as well as topics that are not of entity or event type. To test the model, we construct a podcast topic database of 1166 episodes from various categories. Each episode comes with a Wiki-link annotated main topic or NIL-prediction. ELTE produces the best overall Exact Match EM score of .84, with by-far the best EM of .89 among the entity or event type episodes, as well as NIL-predictions for episodes without entity or event main topic (EM score of .86).|为了改善亚马逊音乐播客服务和客户参与，我们引入了实体链接主题提取(ELTE) ，以从播客节目中识别出知名的实体和事件主题。一个实体可以是一个人、一个组织、一件艺术品等，而一个事件，如阿片类药物流行病，发生在特定的时间点。ELTE 首先从剧集标题和描述元数据中提取关键词。然后使用实体链接对 Wikipedia 知识库(KB)进行规范化，确保主题存在于现实世界中。ELTE 还为不在 KB 中的实体或事件主题以及不属于实体或事件类型的主题建模 NIL 预测。为了测试这个模型，我们构建了一个来自不同类别的1166集播客主题数据库。每集都有一个带有 Wiki 链接注释的主题或 NIL 预测。ELTE 产生最好的整体精确匹配 EM 评分为.84，在实体或事件类型事件中，迄今为止最好的 EM 为.89，以及对没有实体或事件主题的事件的 NIL 预测(EM 评分为.86)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Entity+and+Event+Topic+Extraction+from+Podcast+Episode+Title+and+Description+Using+Entity+Linking)|0|
|[NASA Science Mission Directorate Knowledge Graph Discovery](https://doi.org/10.1145/3543873.3587585)|Roelien C. Timmer, Megan Mark, Fech Scen Khoo, Marcella Scoczynski Ribeiro Martins, Anamaria Berea, Gregory Renard, Kaylin M. Bugbee|; Federal University of Technology, Brazil; NASA Marshall Space Flight Center, USA; The University of New South Wales, Australia; The Applied AI Company (AAICO), USA; George Mason University, USA|The size of the National Aeronautics and Space Administration (NASA) Science Mission Directorate (SMD) is growing exponentially, allowing researchers to make discoveries. However, making discoveries is challenging and time-consuming due to the size of the data catalogs, and as many concepts and data are indirectly connected. This paper proposes a pipeline to generate knowledge graphs (KGs) representing different NASA SMD domains. These KGs can be used as the basis for dataset search engines, saving researchers time and supporting them in finding new connections. We collected textual data and used several modern natural language processing (NLP) methods to create the nodes and the edges of the KGs. We explore the cross-domain connections, discuss our challenges, and provide future directions to inspire researchers working on similar challenges.|美国国家航空航天局美国国家航空暨太空总署科学任务理事会的规模正以指数级增长，使得研究人员得以有所发现。然而，由于数据目录的大小以及许多概念和数据之间的间接联系，进行发现是具有挑战性和耗时的。提出了一种流水线生成代表不同 NASA SMD 领域的知识图(KGs)的方法。这些 KG 可以作为数据集搜索引擎的基础，节省研究人员的时间，并支持他们发现新的连接。我们收集文本数据，并使用几种现代自然语言处理(NLP)方法来创建幼儿园的节点和边缘。我们探索跨领域的联系，讨论我们的挑战，并提供未来的方向，以激励研究人员致力于类似的挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NASA+Science+Mission+Directorate+Knowledge+Graph+Discovery)|0|
|[Scientific Data Extraction from Oceanographic Papers](https://doi.org/10.1145/3543873.3587595)|Bartal Eyðfinsson Veyhe, Tomer Sagi, Katja Hose|Computer Science, Aalborg Universitet, Denmark; Aalborg University, Denmark and TU Wien, Austria; Computer Science, Aalborg University, Denmark|Scientific data collected in the oceanographic domain is invaluable to researchers when performing meta-analyses and examining changes over time in oceanic environments. However, many of the data samples and subsequent analyses published by researchers are not uploaded to a repository leaving the scientific paper as the only available source. Automated extraction of scientific data is, therefore, a valuable tool for such researchers. Specifically, much of the most valuable data in scientific papers are structured as tables, making these a prime target for information extraction research. Using the data relies on an additional step where the concepts mentioned in the tables, such as names of measures, units, and biological species, are identified within a domain ontology. Unfortunately, state-of-the-art table extraction leaves much to be desired and has not been attempted on a large scale on oceanographic papers. Furthermore, while entity linking in the context of a full paragraph of text has been heavily researched, it is still lacking in this harder task of linking single concepts. In this work, we present an annotated benchmark dataset of data tables from oceanographic papers. We further present the result of an evaluation on the extraction of these tables and the linking of the contained entities to the domain and general-purpose knowledge bases using the current state of the art. We highlight the challenges and quantify the performance of current tools for table extraction and table-concept linking.|在海洋学领域收集的科学数据对于研究人员进行综合分析和研究海洋环境随时间的变化是非常宝贵的。然而，研究人员发表的许多数据样本和随后的分析没有上传到存储库中，使科学论文成为唯一可用的来源。因此，科学数据的自动提取对这些研究人员来说是一个有价值的工具。具体来说，科学论文中许多最有价值的数据都是以表格的形式结构的，这使得这些数据成为信息抽取研究的首要目标。使用数据依赖于一个额外的步骤，在这个步骤中，表中提到的概念，例如度量值、单元和生物物种的名称，在一个领域本体中被识别出来。不幸的是，目前最先进的表格提取技术还有很多不足之处，在海洋学论文中也没有大规模地进行过尝试。此外，虽然对整段文字背景下的实体链接进行了大量研究，但在连接单个概念这一更艰巨的任务方面仍然缺乏研究。在这项工作中，我们提出了一个注释的基准数据集的数据表从海洋学论文。我们进一步介绍了使用当前技术状态对这些表的提取以及所包含的实体与领域和通用知识库的链接进行评估的结果。我们强调了挑战，并量化了当前表提取和表概念链接工具的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scientific+Data+Extraction+from+Oceanographic+Papers)|0|
|[Cross-Team Collaboration and Diversity in the Bridge2AI Project](https://doi.org/10.1145/3543873.3587579)|Huimin Xu, Chitrank Gupta, Zhandos Sembay, Swathi Thaker, Pamela PayneFoster, Jake Chen, Ying Ding|The University of Texas at Austin, USA; University of Alabama at Birmingham, USA; University of Alabama, USA|The Bridge2AI project, funded by the National Institutes of Health, involves researchers from different disciplines and backgrounds to develop well-curated AI health data and tools. Understanding cross-disciplinary and cross-organizational collaboration at the individual, team, and project levels is critical. In this paper, we matched Bridge2AI team members to the PubMed Knowledge dataset to get their health-related publications. We built the collaboration network for Bridge2AI members and all of their collaborators and sorted out researchers with the largest degree of centrality and betweenness centrality. Our finding suggests that Bridge2AI members need to strengthen internal collaborations and boost mutual understanding in this project. We also applied machine learning methods to cluster all the researchers and labeled publication topics in different clusters. Finally, by identifying the gender/racial diversity of researchers, we found that teams with higher racial diversity receive more citations, and individuals with diverse gender collaborators publish more papers.|由国立卫生研究院资助的 Bridge2AI 项目，让来自不同学科和背景的研究人员参与开发精心策划的人工智能健康数据和工具。理解个人、团队和项目级别的跨学科和跨组织协作是至关重要的。在本文中，我们将 Bridge2AI 团队成员与 PubMed 知识数据集进行匹配，以获得他们与健康相关的出版物。我们为 Bridge2AI 成员及其所有合作者建立了协作网络，并以最大程度的中心性和中间中心性对研究人员进行了分类。我们的研究结果表明，Bridge2AI 成员需要加强内部协作，促进相互了解，在这个项目。我们还应用机器学习方法对所有的研究者进行聚类，并将发表的主题标记在不同的聚类中。最后，通过识别研究人员的性别/种族多样性，我们发现种族多样性较高的团队获得了更多的引用，而性别合作者不同的个人发表了更多的论文。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-Team+Collaboration+and+Diversity+in+the+Bridge2AI+Project)|0|
|[A New Annotation Method and Dataset for Layout Analysis of Long Documents](https://doi.org/10.1145/3543873.3587609)|Aman Ahuja, Kevin Dinh, Brian Dinh, William A. Ingram, Edward A. Fox|Virginia Tech, USA; Department of Computer Science, Virginia Tech, USA|Parsing long documents, such as books, theses, and dissertations, is an important component of information extraction from scholarly documents. Layout analysis methods based on object detection have been developed in recent years to help with PDF document parsing. However, several challenges hinder the adoption of such methods for scholarly documents such as theses and dissertations. These include (a) the manual effort and resources required to annotate training datasets, (b) the scanned nature of many documents and the inherent noise present resulting from the capture process, and (c) the imbalanced distribution of various types of elements in the documents. In this paper, we address some of the challenges related to object detection based layout analysis for scholarly long documents. First, we propose an AI-aided annotation method to help develop training datasets for object detection based layout analysis. This leverages the knowledge of existing trained models to help human annotators, thus reducing the time required for annotation. It also addresses the class imbalance problem, guiding annotators to focus on labeling instances of rare classes. We also introduce ETD-ODv2, a novel dataset for object detection on electronic theses and dissertations (ETDs). In addition to the page images included in ETD-OD [1], our dataset consists of more than 16K manually annotated page images originating from 100 scanned ETDs, along with annotations for 20K page images primarily consisting of rare classes that were labeled using the proposed framework. The new dataset thus covers a diversity of document types, viz., scanned and born-digital, and is better balanced in terms of training samples from different object categories.|解析长篇文献，例如书籍、论文和论文，是学术文献信息抽取的一个重要组成部分。基于目标检测的布局分析方法近年来得到了发展，以帮助解析 PDF 文档。然而，一些挑战阻碍采用这种方法的学术文献，如论文和论文。这些因素包括: (a)对训练数据集进行注释所需的人工努力和资源; (b)许多文档的扫描性质以及捕获过程产生的固有噪音; (c)文档中各类元素的分布不平衡。在本文中，我们将讨论一些与基于目标检测的学术长文档布局分析相关的挑战。首先，我们提出了一个人工智能辅助注释方法，以帮助开发基于目标检测的布局分析的训练数据集。这将利用现有训练有素的模型的知识来帮助人工注释者，从而减少注释所需的时间。它还解决了类不平衡的问题，指导注释器关注稀有类的标记实例。我们还介绍了一个新的电子学位论文目标检测数据集(eTD-ODv2)。除了 ETD-OD [1]中包含的页面图像之外，我们的数据集由来自100个扫描的 ETD 的超过16K 的手动注释页面图像以及主要由使用所提出的框架标记的罕见类组成的20K 页面图像的注释组成。因此，新的数据集涵盖了文档类型的多样性，即扫描和数字化，并且在不同对象类别的训练样本方面更好地平衡。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+New+Annotation+Method+and+Dataset+for+Layout+Analysis+of+Long+Documents)|0|
|[Towards InnoGraph: A Knowledge Graph for AI Innovation](https://doi.org/10.1145/3543873.3587614)|M. Besher Massri, Blerina Spahiu, Marko Grobelnik, Vladimir Alexiev, Matteo Palmonari, Dumitru Roman|Department of Artificial Intelligence, Jozef Stefan Institute, Slovenia and Jozef Stefan International Postgraduate School, Slovenia; University of Milano-Bicocca, Italy; Department of Artificial Intelligence, Jozef Stefan Institute, Slovenia; SINTEF AS, Norway; Ontotext (Sirma AI), Bulgaria|Researchers seeking to comprehend the state-of-the-art innovations in a particular field of study must examine recent patents and scientific articles in that domain. Innovation ecosystems consist of interconnected information about entities such as researchers, institutions, projects, products, and technologies. However, representing such information in a machine-readable format is challenging because concepts like "knowledge" are not easily represented. Nonetheless, even a partial representation of innovation ecosystems provides valuable insights. Therefore, representing innovation ecosystems as knowledge graphs (KGs) would enable advanced data analysis and generate new insights. To this end, we propose InnoGraph, a framework that integrates multiple heterogeneous data sources to build a Knowledge Graph of the worldwide AI innovation ecosystem.|寻求理解某一特定研究领域最先进创新的研究人员必须审查该领域的最新专利和科学论文。创新生态系统由关于研究人员、机构、项目、产品和技术等实体的相互关联的信息组成。然而，用机器可读的格式表示这些信息是具有挑战性的，因为像“知识”这样的概念不容易表示。尽管如此，即使是对创新生态系统的部分描述也提供了有价值的见解。因此，将创新生态系统表示为知识图表(KGs)将能够进行高级数据分析并产生新的见解。为此，我们提出了 InnoGraph 框架，该框架集成了多个异构数据源，构建了全球人工智能创新生态系统的知识图。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+InnoGraph:+A+Knowledge+Graph+for+AI+Innovation)|0|
|[Assessing Scientific Contributions in Data Sharing Spaces](https://doi.org/10.1145/3543873.3587608)|Kacy Adams, Fernando Spadea, Conor Flynn, Oshani Seneviratne||In the present academic landscape, the process of collecting data is slow, and the lax infrastructures for data collaborations lead to significant delays in coming up with and disseminating conclusive findings. Therefore, there is an increasing need for a secure, scalable, and trustworthy data-sharing ecosystem that promotes and rewards collaborative data-sharing efforts among researchers, and a robust incentive mechanism is required to achieve this objective. Reputation-based incentives, such as the h-index, have historically played a pivotal role in the academic community. However, the h-index suffers from several limitations. This paper introduces the SCIENCE-index, a blockchain-based metric measuring a researcher's scientific contributions. Utilizing the Microsoft Academic Graph and machine learning techniques, the SCIENCE-index predicts the progress made by a researcher over their career and provides a soft incentive for sharing their datasets with peer researchers. To incentivize researchers to share their data, the SCIENCE-index is augmented to include a data-sharing parameter. DataCite, a database of openly available datasets, proxies this parameter, which is further enhanced by including a researcher's data-sharing activity. Our model is evaluated by comparing the distribution of its output for geographically diverse researchers to that of the h-index. We observe that it results in a much more even spread of evaluations. The SCIENCE-index is a crucial component in constructing a decentralized protocol that promotes trust-based data sharing, addressing the current inequity in dataset sharing. The work outlined in this paper provides the foundation for assessing scientific contributions in future data-sharing spaces powered by decentralized applications.|在目前的学术环境中，收集数据的进程缓慢，数据合作基础设施松懈，导致在提出和传播结论性结论方面出现重大延误。因此，越来越需要一个安全、可扩展和可信赖的数据共享生态系统，以促进和奖励研究人员之间的协作数据共享努力，并需要一个强有力的激励机制来实现这一目标。基于声誉的激励机制，如 h 指数，历来在学术界发挥着关键作用。然而，h 索引存在一些局限性。本文介绍了科学指数，一个基于区块链的度量衡量研究人员的科学贡献。利用微软学术图表和机器学习技术，科学指数预测研究人员在其职业生涯中取得的进展，并为与同行研究人员共享数据集提供软激励。为了激励研究人员共享他们的数据，科学索引被扩大到包括一个数据共享参数。DataCite 是一个公开可用数据集的数据库，它代理这个参数，通过包含研究人员的数据共享活动进一步增强了这个参数。我们的模型是通过比较不同地理区域的研究人员的产出分布和 h 指数的分布来评估的。我们注意到，它导致评价的分布更加均匀。科学索引是构建分散协议的重要组成部分，该协议促进了基于信任的数据共享，解决了当前数据集共享中的不公平问题。本文概述的工作为评估未来由分散应用驱动的数据共享空间的科学贡献提供了基础。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Assessing+Scientific+Contributions+in+Data+Sharing+Spaces)|0|
|[Promoting Inactive Members in Edge-Building Marketplace](https://doi.org/10.1145/3543873.3587647)|Ayan Acharya, Siyuan Gao, Borja Ocejo, Kinjal Basu, Ankan Saha, Sathiya Keerthi Selvaraj, Rahul Mazumder, Parag Agrawal, Aman Gupta|LinkedIn Corporation, USA and Massachusetts Institute of Technology, USA; LinkedIn, USA; LinkedIn Inc., USA; linkedin corporation, USA; LinkedIn Corporation, USA|Social networks are platforms where content creators and consumers share and consume content. The edge recommendation system, which determines who a member should connect with, significantly impacts the reach and engagement of the audience on such networks. This paper emphasizes improving the experience of inactive members (IMs) who do not have a large connection network by recommending better connections. To that end, we propose a multi-objective linear optimization framework and solve it using accelerated gradient descent. We report our findings regarding key business metrics related to user engagement on LinkedIn, a professional network with over 850 million members.|社交网络是内容创作者和消费者共享和消费内容的平台。边缘推荐系统决定了一个成员应该与谁联系，它显著地影响了受众在这些网络上的接触范围和参与程度。本文强调通过推荐更好的连接来改善那些没有大型连接网络的非活动成员(IM)的体验。为此，我们提出了一个多目标线性优化框架，并使用加速梯度下降法进行求解。我们报告关于 LinkedIn 用户参与度的关键商业指标的调查结果，LinkedIn 是一个拥有超过8.5亿会员的专业网络。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Promoting+Inactive+Members+in+Edge-Building+Marketplace)|0|
|[CLIME: Completeness-Constrained LIME](https://doi.org/10.1145/3543873.3587652)|Claudia V. Roberts, Ehtsham Elahi, Ashok Chandrashekar|Netflix, Inc., USA; WarnerMedia, USA; Princeton University, USA|We evaluate two popular local explainability techniques, LIME and SHAP, on a movie recommendation task. We discover that the two methods behave very differently depending on the sparsity of the data set, where sparsity is defined by the amount of historical viewing data available to explain a movie recommendation for a particular data instance. We find that LIME does better than SHAP in dense segments of the data set and SHAP does better in sparse segments. We trace this difference to the differing bias-variance characteristics of the underlying estimators of LIME and SHAP. We find that SHAP exhibits lower variance in sparse segments of the data compared to LIME. We attribute this lower variance to the completeness constraint property inherent in SHAP and missing in LIME. This constraint acts as a regularizer and therefore increases the bias of the SHAP estimator but decreases its variance, leading to a favorable bias-variance trade-off especially in high sparsity data settings. With this insight, we introduce the same constraint into LIME and formulate a novel local explainabilty framework called Completeness-Constrained LIME (CLIME) that is superior to LIME and much faster than SHAP.|在一个电影推荐任务中，我们评估了两种流行的局部可解释性技术，LIME 和 SHAP。我们发现，这两种方法的行为非常不同，这取决于数据集的稀疏性，其中稀疏性是由可用于解释特定数据实例的电影推荐的历史观看数据量来定义的。我们发现 LIME 在数据集的密集段中比 SHAP 做得更好，而 SHAP 在稀疏段中做得更好。我们将这种差异追溯到 LIME 和 SHAP 基本估计量的不同偏差-方差特征。我们发现，与 LIME 相比，SHAP 在数据的稀疏片段中表现出较低的方差。我们将这种较低的方差归因于 SHAP 中固有的完备性约束属性和 LIME 中的缺失。这种约束作为一个正则化器，因此增加了 SHAP 估计器的偏差，但减少其方差，导致一个有利的偏差-方差权衡，特别是在高稀疏数据设置。基于这种认识，我们将同样的约束引入到 LIME 中，并且提出了一种新的局部解释框架，称为完整性约束 LIME (完整性约束 LIME) ，它优于 LIME，并且比 SHAP 快得多。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CLIME:+Completeness-Constrained+LIME)|0|
|[DeepPvisit: A Deep Survival Model for Notification Management](https://doi.org/10.1145/3543873.3587666)|Guangyu Yang, Efrem Ghebreab, Jiaxi Xu, Xianen Qiu, Yiping Yuan, Wensheng Sun|LinkedIn Corporation, USA|Notification is a core feature of mobile applications. They inform users about a variety of events happening in the communities. Users may take immediate action to visit the app or ignore the notifications depending on the timing and the relevance of a notification to the user. In this paper, we present the design, implementation, and evaluation of DeepPvisit, a novel probabilistic deep learning survival method for modeling interactions between a user visit and a mobile notification decision, targeting notification volume and delivery time optimization, and driving long-term user engagements. Offline evaluations and online A/B test experiments show DeepPvisit outperforms the existing survival regression model and the other baseline models and delivers better business metrics online.|通知是移动应用程序的一个核心特性。它们向用户通报社区中发生的各种事件。用户可以立即采取行动访问应用程序或忽略通知，这取决于时间和相关性的通知给用户。在本文中，我们提出了一种新的概率深度学习生存方法 DeepPview 的设计、实现和评估，该方法用于建模用户访问和移动通知决策之间的交互，针对通知量和交付时间的优化，以及驱动长期用户参与。离线评估和在线 A/B 测试实验表明，DeepPview 的表现优于现有的生存回归模型和其他基线模型，并且在线提供了更好的业务指标。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DeepPvisit:+A+Deep+Survival+Model+for+Notification+Management)|0|
|[Digital Twins for Radiation Oncology](https://doi.org/10.1145/3543873.3587688)|James Jensen, Jun Deng|Department of Therapeutic Radiology, Yale University, USA|Digital twin technology has revolutionized the state-of-the-art practice in many industries, and digital twins have a natural application to modeling cancer patients. By simulating patients at a more fundamental level than conventional machine learning models, digital twins can provide unique insights by predicting each patient's outcome trajectory. This has numerous associated benefits, including patient-specific clinical decision-making support and the potential for large-scale virtual clinical trials. Historically, it has not been feasible to use digital twin technology to model cancer patients because of the large number of variables that impact each patient's outcome trajectory, including genotypic, phenotypic, social, and environmental factors. However, the path to digital twins in radiation oncology is becoming possible due to recent progress, such as multiscale modeling techniques that estimate patient-specific cellular, molecular, and histological distributions, and modern cryptographic techniques that enable secure and efficient centralization of patient data across multiple institutions. With these and other future scientific advances, digital twins for radiation oncology will likely become feasible. This work discusses the likely generalized architecture of patient-specific digital twins and digital twin networks, as well as the benefits, existing barriers, and potential gateways to the application of digital twin technology in radiation oncology.|数字双胞胎技术革新了许多行业最先进的实践，数字双胞胎有一个自然的应用模型癌症患者。通过在比传统机器学习模型更基础的水平上模拟患者，数字双胞胎可以通过预测每个患者的结局轨迹提供独特的见解。这有许多相关的好处，包括针对患者的临床决策支持和大规模虚拟临床试验的潜力。从历史上看，使用数字双胞胎技术来模拟癌症患者是不可行的，因为影响每个患者结果轨迹的变量很多，包括基因型，表型，社会和环境因素。然而，由于最近的进展，例如估计患者特异性细胞，分子和组织学分布的多尺度建模技术以及能够在多个机构中安全有效地集中患者数据的现代密码技术，放射肿瘤学中的数字双胞胎正在成为可能。随着这些以及其他未来的科学进步，用于放射肿瘤学的数字双胞胎很可能变得可行。这项工作讨论了患者特异性数字双胞胎和数字双胞胎网络的可能的广义结构，以及数字双胞胎技术在放射肿瘤学中应用的益处，现有的障碍和潜在的门户。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Digital+Twins+for+Radiation+Oncology)|0|
|[Graph-Based Hierarchical Attention Network for Suicide Risk Detection on Social Media](https://doi.org/10.1145/3543873.3587587)|Usman Naseem, Jinman Kim, Matloob Khushi, Adam G. Dunn|Department of Computer Science, Brunel University, United Kingdom; School of Computer Science, The University of Sydney, Australia; School of Medical Sciences, The University of Sydney, Australia|The widespread use of social media for expressing personal thoughts and emotions makes it a valuable resource for identifying individuals at risk of suicide. Existing sequential learning-based methods have shown promising results. However, these methods may fail to capture global features. Due to its inherent ability to learn interconnected data, graph-based methods can address this gap. In this paper, we present a new graph-based hierarchical attention network (GHAN) that uses a graph convolutional neural network with an ordinal loss to improve suicide risk identification on social media. Specifically, GHAN first captures global features by constructing three graphs to capture semantic, syntactic, and sequential contextual information. Then encoded textual features are fed to attentive transformers’ encoder and optimized to factor in the increasing suicide risk levels using an ordinal classification layer hierarchically for suicide risk detection. Experimental results show that the proposed GHAN outperformed state-of-the-art methods on a public Reddit dataset.|社交媒体在表达个人思想和情感方面的广泛使用，使其成为识别有自杀风险的个人的宝贵资源。现有的基于序列学习的方法已经取得了很好的效果。但是，这些方法可能无法捕获全局特征。由于其固有的学习互连数据的能力，基于图的方法可以解决这一差距。在这篇论文中，我们提出了一个新的基于图形的分层注意网络(GHAN) ，它使用一个带有序数损失的图形卷积神经网络来改善社交媒体上的自杀风险识别。具体来说，GHAN 首先通过构造三个图来捕获语义、语法和顺序上下文信息，从而捕获全局特征。然后将编码后的文本特征提供给注意变压器的编码器，并使用一个分层的有序分类层对自杀风险检测进行优化，以考虑自杀风险增加的因素。实验结果表明，所提出的 GHAN 在公共 Reddit 数据集上的性能优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph-Based+Hierarchical+Attention+Network+for+Suicide+Risk+Detection+on+Social+Media)|0|
|[I'm out of breath from laughing! I think? A dataset of COVID-19 Humor and its toxic variants](https://doi.org/10.1145/3543873.3587591)|Neha Reddy Bogireddy, Smriti Suresh, Sunny Rai|University of Pennsylvania, USA; George Mason University, USA; Boston University, USA|Humor is a cognitive construct that predominantly evokes the feeling of mirth. During the COVID-19 pandemic, the situations that arouse out of the pandemic were so incongruous to the world we knew that even factual statements often had a humorous reaction. In this paper, we present a dataset of 2510 samples hand-annotated with labels such as humor style, type, theme, target and stereotypes formed or exploited while creating the humor in addition to 909 memes. Our dataset comprises Reddit posts, comments, Onion news headlines, real news headlines, and tweets. We evaluate the task of humor detection and maladaptive humor detection on state-of-the-art models namely RoBERTa and GPT-3. The finetuned models trained on our dataset show significant gains over zero-shot models including GPT-3 when detecting humor. Even though GPT-3 is good at generating meaningful explanations, we observed that it fails to detect maladaptive humor due to the absence of overt targets and profanities. We believe that the presented dataset will be helpful in designing computational methods for topical humor processing as it provides a unique sample set to study the theory of incongruity in a post-pandemic world. The data is available to research community at https://github.com/smritae01/Covid19_Humor.|幽默是一种主要唤起快乐感觉的认知结构。在2019冠状病毒疾病大流行期间，由大流行引发的情况与世界格格不入，我们知道，即使是事实陈述往往也会引发幽默反应。本文收集了2510个幽默样本，除了909个模因外，还对幽默的风格、类型、主题、目标、刻板印象等进行了手工标注。我们的数据集包括 Reddit 帖子、评论、洋葱新闻标题、真实新闻标题和 tweet。我们在最先进的模型 RoBERTa 和 GPT-3上评估了幽默检测和非适应性幽默检测的任务。在我们的数据集上训练的微调模型显示在检测幽默时比包括 GPT-3在内的零拍模型有显著的提高。尽管 GPT-3在产生有意义的解释方面做得很好，但我们观察到，由于缺乏明显的目标和亵渎，它无法检测到适应不良的幽默。我们相信，本文提供的数据集将有助于设计局部幽默处理的计算方法，因为它提供了一个独特的样本集来研究大流行后的世界中的不一致性理论。这些数据可供研究团体 https://github.com/smritae01/covid19_humor 使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=I'm+out+of+breath+from+laughing!+I+think?+A+dataset+of+COVID-19+Humor+and+its+toxic+variants)|0|
|[LLMs to the Moon? Reddit Market Sentiment Analysis with Large Language Models](https://doi.org/10.1145/3543873.3587605)|Xiang Deng, Vasilisa Bashlovkina, Feng Han, Simon Baumgartner, Michael Bendersky|The Ohio State University, USA; Google, USA|Market sentiment analysis on social media content requires knowledge of both financial markets and social media jargon, which makes it a challenging task for human raters. The resulting lack of high-quality labeled data stands in the way of conventional supervised learning methods. In this work, we conduct a case study approaching this problem with semi-supervised learning using a large language model (LLM). We select Reddit as the target social media platform due to its broad coverage of topics and content types. Our pipeline first generates weak financial sentiment labels for Reddit posts with an LLM and then uses that data to train a small model that can be served in production. We find that prompting the LLM to produce Chain-of-Thought summaries and forcing it through several reasoning paths helps generate more stable and accurate labels, while training the student model using a regression loss further improves distillation quality. With only a handful of prompts, the final model performs on par with existing supervised models. Though production applications of our model are limited by ethical considerations, the model’s competitive performance points to the great potential of using LLMs for tasks that otherwise require skill-intensive annotation.|对社交媒体内容的市场情绪分析需要金融市场和社交媒体术语的双重知识，这对评估人员来说是一项具有挑战性的任务。由此导致的高质量标记数据的缺乏阻碍了传统的监督式学习分析方法的发展。在这项工作中，我们进行了一个案例研究来解决这个问题的半监督学习使用大型语言模型(LLM)。我们选择 Reddit 作为目标社交媒体平台，是因为它涵盖了广泛的主题和内容类型。我们的流水线首先为 Reddit 上有 LLM 的帖子生成弱金融情绪标签，然后使用这些数据来训练一个小型模型，这个模型可以在生产中使用。我们发现提示 LLM 生成思想链摘要并强制它通过几种推理路径有助于生成更稳定和准确的标签，同时使用回归损失训练学生模型进一步提高蒸馏质量。由于只有少量的提示，最终模型的表现与现有的监督模型不相上下。尽管我们模型的生产应用受到伦理考虑的限制，但是模型的竞争性表现指出了在需要技能密集型注释的任务中使用 LLM 的巨大潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLMs+to+the+Moon?+Reddit+Market+Sentiment+Analysis+with+Large+Language+Models)|0|
|[Forecasting COVID-19 Vaccination Rates using Social Media Data](https://doi.org/10.1145/3543873.3587639)|Xintian Li, Aron Culotta|Department of Computer Science, Tulane University, USA|The COVID-19 pandemic has had a profound impact on the global community, and vaccination has been recognized as a crucial intervention. To gain insight into public perceptions of COVID-19 vaccines, survey studies and the analysis of social media platforms have been conducted. However, existing methods lack consideration of individual vaccination intentions or status and the relationship between public perceptions and actual vaccine uptake. To address these limitations, this study proposes a text classification approach to identify tweets indicating a user’s intent or status on vaccination. A comparative analysis between the proportions of tweets from different categories and real-world vaccination data reveals notable alignment, suggesting that tweets may serve as a precursor to actual vaccination status. Further, regression analysis and time series forecasting were performed to explore the potential of tweet data, demonstrating the significance of incorporating tweet data in predicting future vaccination status. Finally, clustering was applied to the tweet sets with positive and negative labels to gain insights into underlying focuses of each stance.|2019冠状病毒疾病大流行对全球社会产生了深远影响，疫苗接种已被认为是一种重要的干预措施。为了深入了解公众对2019冠状病毒疾病疫苗的看法，进行了调查研究和对社交媒体平台的分析。然而，现有的方法缺乏对个人接种意图或状况的考虑，以及公众观念和实际接种疫苗之间的关系。为了解决这些局限性，本研究提出了一种文本分类方法来识别表明用户接种疫苗的意图或状态的 tweet。对来自不同类别的推文比例与真实疫苗接种数据的比较分析显示出显著的一致性，表明推文可能作为实际疫苗接种状态的前兆。此外，还进行了回归分析和时间序列预测，以探索 tweet 数据的潜力，表明将 tweet 数据纳入预测未来疫苗接种状况的重要性。最后，将聚类应用于带有正面和负面标签的 tweet 集合，以获得对每个立场的潜在焦点的洞察。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Forecasting+COVID-19+Vaccination+Rates+using+Social+Media+Data)|0|
|[A Cross-Modal Study of Pain Across Communities in the United States](https://doi.org/10.1145/3543873.3587642)|Arnav Aggarwal, Sunny Rai, Salvatore Giorgi, Shreya Havaldar, Garrick Sherman, Juhi Mittal, Sharath Chandra Guntuku|University of Pennsylvania, USA|Pain is one of the most prevalent reasons for seeking medical attention in the United States. Understanding how different communities report and express pain can aid in directing medical efforts and in advancing precision pain management. Using a large-scale self-report survey data set on pain from Gallup (2.5 million surveys) and social media posts from Twitter (1.8 million tweets), we investigate a) if Twitter posts could predict community-level pain and b) how expressions of pain differ across communities in the United States. Beyond observing an improvement of over 9% (in Pearson r) when using Twitter language over demographics to predict community-level pain, our study reveals that the discourse on pain varied significantly across communities in the United States. Evangelical Hubs frequently post about God, lessons from struggle, and prayers when expressing pain, whereas Working Class Country posts about regret and extreme endurance. Academic stresses, injuries, painkillers, and surgeries were the most commonly discussed pain themes in College Towns; Graying America discussed therapy, used emotional language around empathy and anger, and posted about chronic pain treatment; the African American South posted about struggles, patience, and faith when talking about pain. Our study demonstrates the efficacy of using Twitter to predict survey-based self-reports of pain across communities and has implications in aiding community-focused pain management interventions.|在美国，疼痛是寻求医疗救助的最普遍的原因之一。了解不同的社区如何报告和表达疼痛可以帮助指导医疗工作和推进精确的疼痛管理。利用盖洛普(Gallup)的大规模疼痛自我报告调查数据集(250万份调查)和 Twitter 的社交媒体帖子(180万条推文) ，我们调查 a) Twitter 帖子是否可以预测社区层面的疼痛，b)在美国社区中疼痛的表达是如何不同的。除了观察到使用 Twitter 语言比人口统计学预测社区级疼痛的改善超过9% (在 Pearson r)之外，我们的研究显示，在美国社区中关于疼痛的话语差异显着。福音中心频繁发布关于上帝，挣扎的教训，祈祷时表达痛苦，而工人阶级国家发布关于遗憾和极端忍耐力。学术压力、伤害、止痛药和手术是大学城最常讨论的疼痛主题; Graying America 讨论治疗，使用同理心和愤怒的情感语言，发布慢性疼痛治疗;。我们的研究证明了使用 Twitter 来预测基于调查的跨社区疼痛自我报告的有效性，并且在帮助以社区为中心的疼痛管理干预方面具有意义。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Cross-Modal+Study+of+Pain+Across+Communities+in+the+United+States)|0|
|[Claim Extraction and Dynamic Stance Detection in COVID-19 Tweets](https://doi.org/10.1145/3543873.3587643)|Noushin Salek Faramarzi, Fateme Hashemi Chaleshtori, Hossein Shirazi, Indrakshi Ray, Ritwik Banerjee|Stony Brook University, USA; Colorado State University, USA; San Diego State University, USA|The information ecosystem today is noisy, and rife with messages that contain a mix of objective claims and subjective remarks or reactions. Any automated system that intends to capture the social, cultural, or political zeitgeist, must be able to analyze the claims as well as the remarks. Due to the deluge of such messages on social media, and their tremendous power to shape our perceptions, there has never been a greater need to automate these analyses, which play a pivotal role in fact-checking, opinion mining, understanding opinion trends, and other such downstream tasks of social consequence. In this noisy ecosystem, not all claims are worth checking for veracity. Such a check-worthy claim, moreover, must be accurately distilled from subjective remarks surrounding it. Finally, and especially for understanding opinion trends, it is important to understand the stance of the remarks or reactions towards that specific claim. To this end, we introduce a COVID-19 Twitter dataset, and present a three-stage process to (i) determine whether a given Tweet is indeed check-worthy, and if so, (ii) which portion of the Tweet ought to be checked for veracity, and finally, (iii) determine the author’s stance towards the claim in that Tweet, thus introducing the novel task of topic-agnostic stance detection.|今天的信息生态系统是嘈杂的，充斥着包含客观主张和主观评论或反应的信息。任何旨在捕捉社会、文化或政治时代精神的自动化系统，必须能够分析索赔和评论。由于社交媒体上这类信息的泛滥，以及它们塑造我们认知的巨大力量，从来没有比现在更需要自动化这些分析，它们在事实核查、意见挖掘、理解意见趋势以及其他社会后果的下游任务中发挥着关键作用。在这个嘈杂的生态系统中，并非所有的声明都值得检查其准确性。此外，这种值得核实的说法必须准确地从围绕它的主观评论中提炼出来。最后，尤其是为了理解舆论趋势，理解评论的立场或对具体主张的反应是很重要的。为此，我们引入了一个2019冠状病毒疾病的 Twitter 数据集，并提出了一个三阶段的过程来(i)确定给定的 Twitter 是否确实值得检查，如果值得检查，(ii)应该检查 Twitter 的哪一部分的真实性，最后，(iii)确定作者对该 Twitter 中的声明的立场，从而引入了主题不可知立场检测的新任务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Claim+Extraction+and+Dynamic+Stance+Detection+in+COVID-19+Tweets)|0|
|[Self-supervised Pre-training and Semi-supervised Learning for Extractive Dialog Summarization](https://doi.org/10.1145/3543873.3587680)|Yingying Zhuang, Jiecheng Song, Narayanan Sadagopan, Anurag Beniwal|Amazon, USA|Language model pre-training has led to state-of-the-art performance in text summarization. While a variety of pre-trained transformer models are available nowadays, they are mostly trained on documents. In this study we introduce self-supervised pre-training to enhance the BERT model’s semantic and structural understanding of dialog texts from social media. We also propose a semi-supervised teacher-student learning framework to address the common issue of limited available labels in summarization datasets. We empirically evaluate our approach on extractive summarization task with the TWEETSUMM corpus, a recently introduced dialog summarization dataset from Twitter customer care conversations and demonstrate that our self-supervised pre-training and semi-supervised teacher-student learning are both beneficial in comparison to other pre-trained models. Additionally, we compare pre-training and teacher-student learning in various low data-resource settings, and find that pre-training outperforms teacher-student learning and the differences between the two are more significant when the available labels are scarce.|语言模型的预先训练使文本摘要的表现达到了最高水平。虽然各种预先训练的变压器模型现在可用，他们大多数是在文件的培训。在本研究中，我们引入自我监督预训练来提高 BERT 模型对社交媒体对话文本的语义和结构理解。我们还提出了一个半监督的师生学习框架，以解决有限的可用标签摘要数据集的共同问题。我们使用 TWEETSUMM 语料库(最近引入的来自 Twitter 客户关心对话的对话摘要数据集)对我们的提取摘要任务的方法进行了经验性评估，并证明我们的自我监督预训练和半监督师生学习与其他预训练模型相比都是有益的。此外，我们比较了在各种低数据资源环境下的预先培训和师生学习，发现预先培训优于师生学习，当可用标签稀缺时，两者之间的差异更为显著。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-supervised+Pre-training+and+Semi-supervised+Learning+for+Extractive+Dialog+Summarization)|0|
|[Ready, Aim, Snipe! Analysis of Sniper Bots and their Impact on the DeFi Ecosystem](https://doi.org/10.1145/3543873.3587612)|Federico Cernera, Massimo La Morgia, Alessandro Mei, Alberto Maria Mongardini, Francesco Sassi|Sapienza University of Rome, Italy|In the world of cryptocurrencies, public listing of a new token often generates significant hype, in many cases causing its price to skyrocket in a few seconds. In this scenario, timing is crucial to determine the success or failure of an investment opportunity. In this work, we present an in-depth analysis of sniper bots, automated tools designed to buy tokens as soon as they are listed on the market. We leverage GitHub open-source repositories of sniper bots to analyze their features and how they are implemented. Then, we build a dataset of Ethereum and BNB Smart Chain (BSC) liquidity pools to identify addresses that serially take advantage of sniper bots. Our findings reveal 14,029 sniping operations on Ethereum and 1,395,042 in BSC that bought tokens for a total of $10,144,808 dollars and $18,720,447, respectively. We find that Ethereum operations have a higher success rate but require a larger investment. Finally, we analyze token smart contracts to identify mechanisms that can hinder sniper bots.|在加密货币的世界里，一种新令牌的公开上市往往会产生巨大的炒作效应，在许多情况下会导致其价格在几秒钟内飙升。在这种情况下，时机对于决定投资机会的成败至关重要。在这项工作中，我们提出了一个狙击手机器人的深入分析，自动化工具设计购买代币，只要他们在市场上市。我们利用 GitHub 开源的狙击机器人仓库来分析它们的特性以及它们是如何实现的。然后，我们建立一个数据集的以太坊和 BNB 智能链(BSC)流动性池，以确定地址，连续利用狙击手机器人的优势。我们的调查结果显示，在以太坊有14,029次狙击行动，在 BSC 有1,395,042次狙击行动，分别以10,144,808美元和18,720,447美元的价格购买代币。我们发现以太坊的操作有较高的成功率，但需要较大的投资。最后，我们分析令牌智能合同，以确定机制，可以阻碍狙击手机器人。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ready,+Aim,+Snipe!+Analysis+of+Sniper+Bots+and+their+Impact+on+the+DeFi+Ecosystem)|0|
|[Regime-based Implied Stochastic Volatility Model for Crypto Option Pricing](https://doi.org/10.1145/3543873.3587621)|Danial Saef, Yuanrong Wang, Tomaso Aste|Humboldt University Berlin, Germany; University College London, United Kingdom|The increasing adoption of Digital Assets (DAs), such as Bitcoin (BTC), rises the need for accurate option pricing models. Yet, existing methodologies fail to cope with the volatile nature of the emerging DAs. Many models have been proposed to address the unorthodox market dynamics and frequent disruptions in the microstructure caused by the non-stationarity, and peculiar statistics, in DA markets. However, they are either prone to the curse of dimensionality, as additional complexity is required to employ traditional theories, or they overfit historical patterns that may never repeat. Instead, we leverage recent advances in market regime (MR) clustering with the Implied Stochastic Volatility Model (ISVM). Time-regime clustering is a temporal clustering method, that clusters the historic evolution of a market into different volatility periods accounting for non-stationarity. ISVM can incorporate investor expectations in each of the sentiment-driven periods by using implied volatility (IV) data. In this paper, we applied this integrated time-regime clustering and ISVM method (termed MR-ISVM) to high-frequency data on BTC options at the popular trading platform Deribit. We demonstrate that MR-ISVM contributes to overcome the burden of complex adaption to jumps in higher order characteristics of option pricing models. This allows us to price the market based on the expectations of its participants in an adaptive fashion.|随着比特币(BTC)等数字资产(DA)的日益普及，对精确期权定价模型的需求也随之增加。然而，现有的方法无法应对新出现的 DA 的不稳定性。为了解决 DA 市场中由于非平稳性和特殊统计引起的非正统市场动态和微观结构频繁中断的问题，人们提出了许多模型。然而，它们要么容易产生维数灾难，因为采用传统理论需要额外的复杂性，要么过于符合可能永远不会重复的历史模式。相反，我们利用最近在市场机制(MR)集群方面的进展，采用隐含 volatility Model 模型(ISVM)。时域聚类是一种时间聚类方法，它将市场的历史演化过程聚类为考虑非平稳性的不同波动周期。ISVM 可以通过使用隐含波动性(IV)数据，在每个情绪驱动的时期纳入投资者预期。本文将时域聚类与 ISVM 相结合的方法(称为 MR-ISVM)应用于广受欢迎的交易平台 Deribit 上的 BTC 期权的高频数据。我们证明 MR-ISVM 有助于克服期权定价模型高阶特征对跳跃的复杂适应负担。这使我们能够根据市场参与者的期望，以一种适应性的方式为市场定价。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Regime-based+Implied+Stochastic+Volatility+Model+for+Crypto+Option+Pricing)|0|
|[NLP4KGC: Natural Language Processing for Knowledge Graph Construction](https://doi.org/10.1145/3543873.3589746)|Edlira Vakaj, Sanju Tiwari, Nandana Mihindukulasooriya, Fernando OrtizRodríguez, Ryan McGranaghan|Computing and Data Science/Natural Language Processing Lab, Birmingham City University, United Kingdom; Universidad Autónoma de Tamaulipas, Mexico; IBM Research, Ireland; NASA Jet Propulsion Laboratory, USA|No abstract available.|没有摘要。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NLP4KGC:+Natural+Language+Processing+for+Knowledge+Graph+Construction)|0|
|[GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering](https://doi.org/10.1145/3543873.3587651)|Dhaval Taunk, Lakshya Khanna, Siri Venkata Pavan Kumar Kandru, Vasudeva Varma, Charu Sharma, Makarand Tapaswi|International Institute of Information Technology, India|Commonsense question-answering (QA) methods combine the power of pre-trained Language Models (LM) with the reasoning provided by Knowledge Graphs (KG). A typical approach collects nodes relevant to the QA pair from a KG to form a Working Graph (WG) followed by reasoning using Graph Neural Networks(GNNs). This faces two major challenges: (i) it is difficult to capture all the information from the QA in the WG, and (ii) the WG contains some irrelevant nodes from the KG. To address these, we propose GrapeQA with two simple improvements on the WG: (i) Prominent Entities for Graph Augmentation identifies relevant text chunks from the QA pair and augments the WG with corresponding latent representations from the LM, and (ii) Context-Aware Node Pruning removes nodes that are less relevant to the QA pair. We evaluate our results on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows consistent improvements over its LM + KG predecessor (QA-GNN in particular) and large improvements on OpenBookQA.|常识问答(QA)方法结合了预训练语言模型(LM)和知识图(KG)提供的推理能力。一种典型的方法是从一个 KG 中收集与 QA 对相关的节点，形成一个工作图(WG) ，然后使用图神经网络(GNN)进行推理。这面临着两个主要的挑战: (i)很难从工作组中的 QA 获取所有信息，以及(ii)工作组包含来自 KG 的一些不相关的节点。为了解决这些问题，我们提出了 GrapeQA，对 WG 进行了两个简单的改进: (i)用于图增强的突出实体识别 QA 对中的相关文本块，并用来自 LM 的相应潜在表示增强 WG; (ii)上下文感知节点修剪删除与 QA 对不太相关的节点。我们在 OpenBookQA，CommonsenseQA 和 MedQA-USMLE 上评估了我们的结果，发现 GrapeQA 比它的 LM + KG 前辈(尤其是 QA-GNN)有一致的改进，并且在 OpenBookQA 上有很大的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GrapeQA:+GRaph+Augmentation+and+Pruning+to+Enhance+Question-Answering)|0|
|[Federated Learning for Metaverse: A Survey](https://doi.org/10.1145/3543873.3587584)|Yao Chen, Shan Huang, Wensheng Gan, Gengsen Huang, Yongdong Wu|Jinan University, China|The metaverse, which is at the stage of innovation and exploration, faces the dilemma of data collection and the problem of private data leakage in the process of development. This can seriously hinder the widespread deployment of the metaverse. Fortunately, federated learning (FL) is a solution to the above problems. FL is a distributed machine learning paradigm with privacy-preserving features designed for a large number of edge devices. Federated learning for metaverse (FL4M) will be a powerful tool. Because FL allows edge devices to participate in training tasks locally using their own data, computational power, and model-building capabilities. Applying FL to the metaverse not only protects the data privacy of participants but also reduces the need for high computing power and high memory on servers. Until now, there have been many studies about FL and the metaverse, respectively. In this paper, we review some of the early advances of FL4M, which will be a research direction with unlimited development potential. We first introduce the concepts of metaverse and FL, respectively. Besides, we discuss the convergence of key metaverse technologies and FL in detail, such as big data, communication technology, the Internet of Things, edge computing, blockchain, and extended reality. Finally, we discuss some key challenges and promising directions of FL4M in detail. In summary, we hope that our up-to-date brief survey can help people better understand FL4M and build a fair, open, and secure metaverse.|元宇宙正处于创新和探索阶段，在发展过程中面临着数据采集的困境和私有数据泄漏的问题。这可能会严重阻碍元宇宙的广泛部署。幸运的是，联邦学习(FL)是上述问题的解决方案。FL 是一种为大量边缘设备设计的具有保密特性的分布式机器学习范式。元宇宙的联邦学习(FL4M)将是一个强大的工具。因为 FL 允许边缘设备使用它们自己的数据、计算能力和模型构建能力在本地参与培训任务。将 FL 应用于元宇宙不仅保护了参与者的数据隐私，而且减少了对服务器上高计算能力和高内存的需求。到目前为止，已有许多研究分别对 FL 和元宇宙进行了研究。本文综述了 FL4M 的一些早期进展，认为 FL4M 是一个具有无限发展潜力的研究方向。我们首先分别介绍了元宇宙和 FL 的概念。此外，本文还详细讨论了大数据、通信技术、物联网、边缘计算、区块链以及扩展现实等关键技术与 FL 的融合问题。最后，我们详细讨论了 FL4M 的一些关键挑战和发展方向。总之，我们希望我们最新的简短调查可以帮助人们更好地理解 FL4M，并建立一个公平、开放和安全的元宇宙。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Federated+Learning+for+Metaverse:+A+Survey)|0|
|[Understanding the Impact of Label Skewness and Optimization on Federated Learning for Text Classification](https://doi.org/10.1145/3543873.3587599)|Sumam Francis, Kanimozhi Uma, MarieFrancine Moens|LIIR lab, Department of Computer Science, KU Leuven, Belgium|Federated Learning (FL), also known as collaborative learning, is a distributed machine learning approach that collaboratively learns a shared prediction model without explicitly sharing private data. When dealing with sensitive data, privacy measures need to be carefully considered. Optimizers have a massive role in accelerating the learning process given the high dimensionality and non-convexity of the search space. The data partitioning in FL can be assumed to be either IID (independent and identically distributed) or non-IID. In this paper, we experiment with the impact of applying different adaptive optimization methods for FL frameworks in both IID and non-IID setups. We analyze the effects of label and quantity skewness, learning rate, and local client training on the learning process of optimizers as well as the overall performance of the global model. We evaluate the FL hyperparameter settings on biomedical text classification tasks on two datasets ADE V2 (Adverse Drug Effect: 2 classes) and Clinical-Trials (Reasons to stop trials: 17 classes).|联邦学习(FL) ，也称为合作学习学习，是一种分布式机器学习方法，它协作学习共享的预测模型，而不显式地共享私有数据。在处理敏感数据时，需要仔细考虑隐私措施。由于搜索空间的高维性和非凸性，优化器在加速学习过程中起着巨大的作用。FL 中的数据分区可以假定为 IID (独立且同分布)或非 IID。在本文中，我们实验了在 IID 和非 IID 设置中对 FL 框架应用不同的自适应优化方法的影响。我们分析了标签偏度和数量偏度、学习率和局部客户训练对优化器学习过程的影响以及全局模型的整体性能。我们评估两个数据集 ADE V2(不良药物效应: 2类)和临床试验(停止试验的原因: 17类)的生物医学文本分类任务的 FL 超参数设置。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+the+Impact+of+Label+Skewness+and+Optimization+on+Federated+Learning+for+Text+Classification)|0|
|[A Survey of Trustworthy Federated Learning with Perspectives on Security, Robustness and Privacy](https://doi.org/10.1145/3543873.3587681)|Yifei Zhang, Dun Zeng, Jinglong Luo, Zenglin Xu, Irwin King|Harbin Institute of Technology and Peng Cheng Lab, China; University of Electronic Science and Technology of China and Peng Cheng Lab, China; The Chinese University of Hong Kong, Hong Kong|Trustworthy artificial intelligence (AI) technology has revolutionized daily life and greatly benefited human society. Among various AI technologies, Federated Learning (FL) stands out as a promising solution for diverse real-world scenarios, ranging from risk evaluation systems in finance to cutting-edge technologies like drug discovery in life sciences. However, challenges around data isolation and privacy threaten the trustworthiness of FL systems. Adversarial attacks against data privacy, learning algorithm stability, and system confidentiality are particularly concerning in the context of distributed training in federated learning. Therefore, it is crucial to develop FL in a trustworthy manner, with a focus on security, robustness, and privacy. In this survey, we propose a comprehensive roadmap for developing trustworthy FL systems and summarize existing efforts from three key aspects: security, robustness, and privacy. We outline the threats that pose vulnerabilities to trustworthy federated learning across different stages of development, including data processing, model training, and deployment. To guide the selection of the most appropriate defense methods, we discuss specific technical solutions for realizing each aspect of Trustworthy FL (TFL). Our approach differs from previous work that primarily discusses TFL from a legal perspective or presents FL from a high-level, non-technical viewpoint.|值得信赖的人工智能(AI)技术彻底改变了人们的日常生活，极大地造福了人类社会。在各种人工智能技术中，联邦学习(FL)脱颖而出，作为一种有前途的解决方案，适用于从金融风险评估系统到生命科学药物发现等尖端技术的各种现实情景。然而，围绕数据隔离和隐私的挑战威胁着 FL 系统的可信度。在联邦学习的分布式训练环境中，对数据隐私、学习算法稳定性和系统机密性的对抗性攻击尤其受到关注。因此，以一种值得信赖的方式开发 FL 是至关重要的，重点是安全性、健壮性和隐私性。在本调查中，我们提出了一个开发可信 FL 系统的全面路线图，并从三个关键方面总结了现有的工作: 安全性、健壮性和隐私性。我们概述了在不同的开发阶段(包括数据处理、模型培训和部署)对可信联邦学习构成的威胁。为了指导选择最合适的防御方法，我们讨论了具体的技术解决方案，以实现值得信赖的外交(TFL)的各个方面。我们的方法不同于以前的工作，主要讨论任务型外语从法律的角度或提出从一个高层次的，非技术的观点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Survey+of+Trustworthy+Federated+Learning+with+Perspectives+on+Security,+Robustness+and+Privacy)|0|
|[A Federated Learning Benchmark for Drug-Target Interaction](https://doi.org/10.1145/3543873.3587687)|Gianluca Mittone, Filip Svoboda, Marco Aldinucci, Nicholas D. Lane, Pietro Lió|University of Cambridge, United Kingdom; Department of Computer Science, University of Turin, Italy|Aggregating pharmaceutical data in the drug-target interaction (DTI) domain has the potential to deliver life-saving breakthroughs. It is, however, notoriously difficult due to regulatory constraints and commercial interests. This work proposes the application of federated learning, which we argue to be reconcilable with the industry's constraints, as it does not require sharing of any information that would reveal the entities' data or any other high-level summary of it. When used on a representative GraphDTA model and the KIBA dataset it achieves up to 15% improved performance relative to the best available non-privacy preserving alternative. Our extensive battery of experiments shows that, unlike in other domains, the non-IID data distribution in the DTI datasets does not deteriorate FL performance. Additionally, we identify a material trade-off between the benefits of adding new data, and the cost of adding more clients.|在药物-靶标相互作用(DTI)领域聚合药物数据有可能带来拯救生命的突破。然而，由于监管限制和商业利益，这是出了名的困难。这项工作提出了联邦学习的应用，我们认为这与行业的约束是一致的，因为它不需要共享任何信息来揭示实体的数据或任何其他高层次的摘要。当在一个有代表性的 GraphDTA 模型和 KIBA 数据集上使用时，相对于最好的非隐私保护方案，它实现了高达15% 的性能提高。我们大量的实验表明，与其他领域不同，DTI 数据集中的非 IID 数据分布不会降低 FL 性能。此外，我们还确定了添加新数据的好处与添加更多客户端的成本之间的实质性权衡。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Federated+Learning+Benchmark+for+Drug-Target+Interaction)|0|
|[Towards Timeline Generation with Abstract Meaning Representation](https://doi.org/10.1145/3543873.3587670)|Behrooz Mansouri, Ricardo Campos, Adam Jatowt|University of Innsbruck, Austria; Ci2 - Polytechnic Institute of Tomar; INESC TEC, Portugal; Department of Computer Science, University of Southern Maine, USA|Timeline summarization (TLS) is a challenging research task that requires researchers to distill extensive and intricate temporal data into a concise and easily comprehensible representation. This paper proposes a novel approach to timeline summarization using Abstract Meaning Representations (AMRs), a graphical representation of the text where the nodes are semantic concepts and the edges denote relationships between concepts. With AMR, sentences with different wordings, but similar semantics, have similar representations. To make use of this feature for timeline summarization, a two-step sentence selection method that leverages features extracted from both AMRs and the text is proposed. First, AMRs are generated for each sentence. Sentences are then filtered out by removing those with no named-entities and keeping the ones with the highest number of named-entities. In the next step, sentences to appear in the timeline are selected based on two scores: Inverse Document Frequency (IDF) of AMR nodes combined with the score obtained by applying a keyword extraction method to the text. Our experimental results on the TLS-Covid19 test collection demonstrate the potential of the proposed approach.|时间轴摘要(TLS)是一项具有挑战性的研究任务，它要求研究人员将广泛而复杂的时间数据提取到一个简洁而易于理解的表示中。本文提出了一种基于抽象意义表示(AMR)的时间轴概括方法，AMR 是一种文本的图形表示方法，其中节点是语义概念，边表示概念之间的关系。使用 AMR，具有不同词汇但语义相似的句子具有相似的表示。为了利用这一特征进行时间轴摘要，提出了一种利用 AMR 和文本提取特征的两步句子选择方法。首先，为每个句子生成 AMR。然后，通过删除没有命名实体的句子并保留命名实体数量最多的句子来过滤掉句子。在下一步中，根据两个得分来选择出现在时间轴中的句子: AMR 节点的反向文档频率(IDF)与通过对文本应用关键字提取方法获得的得分相结合。我们在 TLS-Covid19测试集上的实验结果证明了该方法的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Timeline+Generation+with+Abstract+Meaning+Representation)|0|
|[Gone, Gone, but Not Really, and Gone, But Not forgotten: A Typology of Website Recoverability](https://doi.org/10.1145/3543873.3587671)|Brenda Reyes Ayala|University of Alberta, Canada|This paper presents a qualitative analysis of the recoverability of various webpages on the live web, using their archived counterparts as a baseline. We used a heterogeneous dataset consisting of four web archive collections, each with varying degrees of content drift. We were able to recover a small number of webpages previously thought to have been lost and analyzed their content and evolution. Our analysis yielded three types of lost webpages: 1) those that are not recoverable (with three subtypes), 2) those that are fully recoverable, and 3) those that are partially recoverable. The analysis presented here attempts to establish clear definitions and boundaries between the different degrees of webpage recoverabilty. By using a few simple methods, web archivists could discover the new locations of web content that was previously deemed lost, and include them in future crawling efforts, and lead to more complete web archives with less content drift.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Gone,+Gone,+but+Not+Really,+and+Gone,+But+Not+forgotten:+A+Typology+of+Website+Recoverability)|0|
|[Detecting the Hidden Dynamics of Networked Actors Using Temporal Correlations](https://doi.org/10.1145/3543873.3587672)|Keeley Erhardt, Dina Albassam|Massachusetts Institute of Technology, USA; King Abdulaziz City for Science and Technology, Saudi Arabia|Influence campaigns pose a threat to fact-based reasoning, erode trust in institutions, and tear at the fabric of our society. In the 21st century, influence campaigns have rapidly evolved, taking on new online identities. Many of these propaganda campaigns are persistent and well-resourced, making their identification and removal both hard and expensive. Social media companies have predominantly aimed to counter the threat of online propaganda by prioritizing the moderation of "coordinated inauthentic behavior". This strategy focuses on identifying orchestrated campaigns explicitly intended to deceive, rather than individual social media accounts or posts. In this paper, we study the Twitter footprint of a multi-year influence campaign linked to the Russian government. Drawing from the influence model, a generative model that describes the interactions between networked Markov chains, we demonstrate how temporal correlations in the sequential decision processes of individual social media accounts can reveal coordinated inauthentic activity.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+the+Hidden+Dynamics+of+Networked+Actors+Using+Temporal+Correlations)|0|
|[The Age of Snippet Programming: Toward Understanding Developer Communities in Stack Overflow and Reddit](https://doi.org/10.1145/3543873.3587673)|Alessia Antelmi, Gennaro Cordasco, Daniele De Vinco, Carmine Spagnuolo|Department of Computer Science, Università degli Studi di Salerno, Italy; Department of Psychology, Università della Campania, Italy|Today, coding skills are among the most required competencies worldwide, often also for non-computer scientists. Because of this trend, community contribution-based, question-and-answer (Q&A) platforms became prominent for finding the proper solution to all programming issues. Stack Overflow has been the most popular platform for technical-related questions for years. Still, recently, some programming-related subreddits of Reddit have become a standing stone for questions and discussions. This work investigates the developers’ behavior and community formation around the twenty most popular programming languages. We examined two consecutive years of programming-related questions from Stack Overflow and Reddit, performing a longitudinal study on users’ posting activity and their high-order interaction patterns abstracted via hypergraphs. Our analysis highlighted crucial differences in how these Q&A platforms are utilized by their users. In line with previous literature, it emphasized the constant decline of Stack Overflow in favor of more community-friendly platforms, such as Reddit, which has been growing rapidly lately.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Age+of+Snippet+Programming:+Toward+Understanding+Developer+Communities+in+Stack+Overflow+and+Reddit)|0|
|[Temporal Ordinance Mining for Event-Driven Social Media Reaction Analytics](https://doi.org/10.1145/3543873.3587674)|Aparna S. Varde, Gerard de Melo, Boxiang Dong|AI & Intelligent Systems, HPI, University of Potsdam, Germany; Dept. of Computer Science; Clean Energy & Sustainability Analytics Center, Montclair State University, USA; Dept. of Computer Science, Montclair State University, USA|As a growing number of policies are adopted to address the substantial rise in urbanization, there is a significant push for smart governance, endowing transparency in decision-making and enabling greater public involvement. The thriving concept of smart governance goes beyond just cities, ultimately aiming at a smart planet. Ordinances (local laws) affect our life with regard to health, business, etc. This is particularly notable during major events such as the recent pandemic, which may lead to rapid changes in ordinances, pertaining for instance to public safety, disaster management, and recovery phases. However, many citizens view ordinances as impervious and complex. This position paper proposes a research agenda enabling novel forms of ordinance content analysis over time and temporal web question answering (QA) for both legislators and the broader public. Along with this, we aim to analyze social media posts so as to track the public opinion before and after the introduction of ordinances. Challenges include addressing concepts changing over time and infusing subtle human reasoning in mining, which we aim to address by harnessing terminology evolution methods and commonsense knowledge sources, respectively. We aim to make the results of the historical ordinance mining and event-driven analysis seamlessly accessible, relying on a robust semantic understanding framework to flexibly support web QA.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Temporal+Ordinance+Mining+for+Event-Driven+Social+Media+Reaction+Analytics)|0|
|[A Chinese Fine-grained Financial Event Extraction Dataset](https://doi.org/10.1145/3543873.3587578)|Mengjie Wu, Maofu Liu, Luyao Wang, Huijun Hu|School of Computer Science and Technology, Hubei Province Key Laboratory of Intelligent Information Processing and Real-time Industrial System,Wuhan University of Science and Technology, China|The existing datasets are mostly composed of official documents, statements, news articles, and so forth. So far, only a little attention has been paid to the numerals in financial social comments. Therefore, this paper presents CFinNumAttr, a financial numeral attribute dataset in Chinese via annotating the stock reviews and comments collected from social networking platform. We also conduct several experiments on the CFinNumAttr dataset with state-of-the-art methods to discover the importance of the financial numeral attributes. The experimental results on the CFinNumAttr dataset show that the numeral attributes in social reviews or comments contain rich semantic information, and the numeral clue extraction and attribute classification tasks can make a great improvement in financial text understanding.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Chinese+Fine-grained+Financial+Event+Extraction+Dataset)|0|
|[Financial Technology on the Web](https://doi.org/10.1145/3543873.3589738)|ChungChi Chen, HenHsen Huang, Hiroya Takamura, HsinHsi Chen|Institute of Information Science, Academia Sinica, Taiwan; National Institute of Advanced Industrial Science and Technology, Japan; National Taiwan University, Taiwan|This paper shares our observations based on our three-year experience organizing the FinWeb workshop series. In addition to the widely-discussed topic, content analysis, we notice two tendencies for FinTech applications: customers’ behavior analysis and finance-oriented LegalTech. We also briefly share our idea on the research direction about reliable and trustworthy FinWeb from the investment perspective.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Financial+Technology+on+the+Web)|0|
|[Aspect-based Summarization of Legal Case Files using Sentence Classification](https://doi.org/10.1145/3543873.3587611)|Nikhil E, Anshul Padhi, Pulkit Parikh, Swati Kanwal, Kamalakar Karlapalem, Natraj Raman|IIIT Hyderabad, India; JP Morgan AI Research, United Kingdom; IIIT Hyderabad, Canada|Aspect-based summarization of a legal case file related to regulating bodies allows different stakeholders to consume information of interest therein efficiently. In this paper, we propose a multi-step process to achieve the same. First, we explore the semantic sentence segmentation of SEBI case files via classification. We also propose a dataset of Indian legal adjudicating orders which contain tags from carefully crafted domain-specific sentence categories with the help of legal experts. We experiment with various machine learning and deep learning methods for this multi-class classification. Then, we examine the performance of numerous summarization methods on the segmented document to generate persona-specific summaries. Finally, we develop a pipeline making use of the best methods in both sub-tasks to achieve high recall.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Aspect-based+Summarization+of+Legal+Case+Files+using+Sentence+Classification)|0|
|[Exploiting graph metrics to detect anomalies in cross-country money transfer temporal networks](https://doi.org/10.1145/3543873.3587602)|Salvatore Vilella, Arthur Thomas Edward Capozzi Lupi, Giancarlo Ruffo, Marco Fornasiero, Dario Moncalvo, Valeria Ricci, Silvia Ronchiadin|Università degli Studi del Piemonte Orientale, Italy; Anti Financial Crime Digital Hub, Italy; Department of Computer Science, University of Turin, Italy|During the last decades, Anti-Financial Crime (AFC) entities and Financial Institutions have put a constantly increasing effort to reduce financial crime and detect fraudulent activities, that are changing and developing in extremely complex ways. We propose an anomaly detection approach based on network analysis to help AFC officers navigating through the high load of information that is typical of AFC data-driven scenarios. By experimenting on a large financial dataset of more than 80M cross-country wire transfers, we leverage on the properties of complex networks to develop a tool for explainable anomaly detection, that can help in identifying outliers that could be engaged in potentially malicious activities according to financial regulations. We identify a set of network metrics that provide useful insights on individual nodes; by keeping track of the evolution over time of the metric-based node rankings, we are able to highlight sudden and unexpected changes in the roles of individual nodes that deserve further attention by AFC officers. Such changes can hardly be noticed by means of current AFC practices, that sometimes can lack a higher-level, global vision of the system. This approach represents a preliminary step in the automation of AFC and AML processes, serving the purpose of facilitating the work of AFC officers by providing them with a top-down view of the picture emerging from financial data.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploiting+graph+metrics+to+detect+anomalies+in+cross-country+money+transfer+temporal+networks)|0|
|[Multiple-Agent Deep Reinforcement Learning for Avatar Migration in Vehicular Metaverses](https://doi.org/10.1145/3543873.3587573)|Junlong Chen, Jiangtian Nie, Minrui Xu, Lingjuan Lyu, Zehui Xiong, Jiawen Kang, Yongju Tong, Wenchao Jiang|Guangdong University of Technology, China; Nanyang Technological University, Singapore; Singapore University of Technology and Design, Singapore; Sony(Japan), Japan|Vehicular Metaverses are widely considered as the next Internet revolution to build a 3D virtual world with immersive virtual-real interaction for passengers and drivers. In vehicular Metaverse applications, avatars are digital representations of on-board users to obtain and manage immersive vehicular services (i.e., avatar tasks) in Metaverses and the data they generate. However, traditional Internet of Vehicles (IoV) data management solutions have serious data security risks and privacy protection. Fortunately, blockchain-based Web 3.0 enables avatars to have an ownership identity to securely manage the data owned by users in a decentralized and transparent manner. To ensure users’ immersive experiences and securely manage their data, avatar tasks often require significant computing resources. Therefore, it is impractical for the vehicles to process avatar tasks locally, massive computation resources are needed to support the avatar tasks. To this end, offloading avatar tasks to nearby RoadSide Units (RSUs) is a promising solution to avoid computation overload. To ensure real-time and continuous Metaverse services, the avatar tasks should be migrated among the RSUs when the vehicle navigation. It is challenging for the vehicles to independently decide whether migrate or not according to current and future avatar states. Therefore, in this paper, we propose a new avatar task migration framework for vehicular Metaverses. We then formulate the avatar task migration problem as a Partially Observable Markov Decision Process (POMDP), and apply a Multi-Agent Deep Reinforcement Learning (MADRL) algorithm to dynamically make migration decisions for avatar tasks. Numerous results show that our proposed algorithm outperforms existing baselines for avatar task migration and enables immersive vehicular Metaverse services.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multiple-Agent+Deep+Reinforcement+Learning+for+Avatar+Migration+in+Vehicular+Metaverses)|0|
|[China's First Natural Language-based AI ChatBot Trader](https://doi.org/10.1145/3543873.3587633)|James Y. Zhang, Zhi Li, Hao Fang, Jun Wu, Zhongnan Shen, Jing Zheng, Wei Chu, Weiping Duan, Peng Xu|MyBank, China; Ant Group, USA|Repo (repurchase agreement) trading provides easy access to short-term financing secured by a pledge of collateral and plays an important role in the global financial system. However, repo traders face many tough challenges in their job, from managing complex financial transactions to keeping up with changing market trends and regulations in the complex financial transactions involved. Besides the difficult and tedious processes that take a lot of time and energy, repo traders need to keep up to date with various laws, regulations, and financial trends that may affect their job, worsened by the exposure to a variety of market risks. As the leader of the FinTech industry, Ant Group launched a new initiative to alleviate the affliction of the repo traders at MyBank1. By leveraging many existing platform technologies, such as AI ChatBot and forecasting platforms, and with the collective work of various engineering groups, we are able to create a ChatBot that communicates with other human traders in natural language and create electronic contracts based on the negotiated terms, equipped with proper trading strategies based on forecasting results. The fully automatic workflow not only frees our trader from tedious routines, but also reduces potential human errors. At the same time, it enables refined portfolio and risk management, while opening up the possibility to apply neural network-based trading strategies, and yielding greater returns comparing to traditional workflow reliant on human experiences. Our system has evolved beyond just providing services to our own traders, to now a fully commercialized product, covering other types of interbank trading.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=China's+First+Natural+Language-based+AI+ChatBot+Trader)|0|
|[Web 3.0: The Future of Internet](https://doi.org/10.1145/3543873.3587583)|Wensheng Gan, Zhenqiang Ye, Shicheng Wan, Philip S. Yu|Guangdong University of Technology, China; University of Illinois at Chicago, USA; Jinan University, China|With the rapid growth of the Internet, human daily life has become deeply bound to the Internet. To take advantage of massive amounts of data and information on the internet, the Web architecture is continuously being reinvented and upgraded. From the static informative characteristics of Web 1.0 to the dynamic interactive features of Web 2.0, scholars and engineers have worked hard to make the internet world more open, inclusive, and equal. Indeed, the next generation of Web evolution (i.e., Web 3.0) is already coming and shaping our lives. Web 3.0 is a decentralized Web architecture that is more intelligent and safer than before. The risks and ruin posed by monopolists or criminals will be greatly reduced by a complete reconstruction of the Internet and IT infrastructure. In a word, Web 3.0 is capable of addressing web data ownership according to distributed technology. It will optimize the internet world from the perspectives of economy, culture, and technology. Then it promotes novel content production methods, organizational structures, and economic forms. However, Web 3.0 is not mature and is now being disputed. Herein, this paper presents a comprehensive survey of Web 3.0, with a focus on current technologies, challenges, opportunities, and outlook. This article first introduces a brief overview of the history of World Wide Web as well as several differences among Web 1.0, Web 2.0, Web 3.0, and Web3. Then, some technical implementations of Web 3.0 are illustrated in detail. We discuss the revolution and benefits that Web 3.0 brings. Finally, we explore several challenges and issues in this promising area.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Web+3.0:+The+Future+of+Internet)|0|
|[DSNet: Efficient Lightweight Model for Video Salient Object Detection for IoT and WoT Applications](https://doi.org/10.1145/3543873.3587592)|Hemraj Singh, Mridula Verma, Ramalingaswamy Cheruku|Institute for Development and Research in Banking Technology, India; National Institute of Technology Warangal, India|The most challenging aspects of deploying deep models in IoT and embedded systems are extensive computational complexity and large training and inference time. Although various lightweight versions of state-of-the-art models are also being designed, maintaining the performance of such models is difficult. To overcome these problems, an efficient, lightweight, Deformable Separable Network (DSNet) is proposed for video salient object detection tasks, mainly for mobile and embedded vision applications. DSNet is equipped with a Deformable Convolution Network (DeCNet), Separable Convolution Network (SCNet), and Depth-wise Attention Response Propagation (DARP) module, which makes it maintain the trade-off between accuracy and latency. The proposed model generates saliency maps considering both the background and foreground simultaneously, making it perform better in unconstrained scenarios (such as partial occlusion, deformable background/objects, and illumination effect). The extensive experiments conducted on six benchmark datasets demonstrate that the proposed model outperforms state-of-art approaches in terms of computational complexity, number of parameters, and latency measures.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DSNet:+Efficient+Lightweight+Model+for+Video+Salient+Object+Detection+for+IoT+and+WoT+Applications)|0|
|[Weighted Statistically Significant Pattern Mining](https://doi.org/10.1145/3543873.3587586)|Tingfu Zhou, Zhenlian Qi, Wensheng Gan, Shicheng Wan, Guoting Chen|Guangdong University of Technology, China; Guangdong Eco-Engineering Polytechnic, China; Harbin Institute of Technology, China; Jinan University, China|Pattern discovery (aka pattern mining) is a fundamental task in the field of data science. Statistically significant pattern mining (SSPM) is the task of finding useful patterns that statistically occur more often from databases for one class than for another. The existing SSPM task does not consider the weight of each item. While in the real world, the significant level of different items/objects is various. Therefore, in this paper, we introduce the Weighted Statistically Significant Patterns Mining (WSSPM) problem and propose a novel WSSpm algorithm to successfully solve it. We present a new framework that effectively mines weighted statistically significant patterns by combining the weighted upper-bound model and the multiple hypotheses test. We also propose a new weighted support threshold that can satisfy the demand of WSSPM and prove its correctness and completeness. Besides, our weighted support threshold and modified weighted upper-bound can effectively shrink the mining range. Finally, experimental results on several real datasets show that the WSSpm algorithm performs well in terms of execution time and memory storage.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weighted+Statistically+Significant+Pattern+Mining)|0|
|[The Human-Centric Metaverse: A Survey](https://doi.org/10.1145/3543873.3587593)|Riyan Yang, Lin Li, Wensheng Gan, Zefeng Chen, Zhenlian Qi|Guangdong Eco-Engineering Polytechnic, China; Jinan University, China|In the era of the Web of Things, the Metaverse is expected to be the landing site for the next generation of the Internet, resulting in the increased popularity of related technologies and applications in recent years and gradually becoming the focus of Internet research. The Metaverse, as a link between the real and virtual worlds, can provide users with immersive experiences. As the concept of the Metaverse grows in popularity, many scholars and developers begin to focus on the Metaverse's ethics and core. This paper argues that the Metaverse should be centered on humans. That is, humans constitute the majority of the Metaverse. As a result, we begin this paper by introducing the Metaverse's origins, characteristics, related technologies, and the concept of the human-centric Metaverse (HCM). Second, we discuss the manifestation of human-centric in the Metaverse. Finally, we discuss some current issues in the construction of HCM. In this paper, we provide a detailed review of the applications of human-centric technologies in the Metaverse, as well as the relevant HCM application scenarios. We hope that this paper can provide researchers and developers with some directions and ideas for human-centric Metaverse construction.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Human-Centric+Metaverse:+A+Survey)|0|
|[Can Deepfakes be created on a whim?](https://doi.org/10.1145/3543873.3587581)|Pulak Mehta, Gauri Jagatap, Kevin Gallagher, Brian Timmerman, Progga Deb, Siddharth Garg, Rachel Greenstadt, Brendan DolanGavitt|New York University, USA; NOVA LINCS & Universidade NOVA de Lisboa, Portugal|Recent advancements in machine learning and computer vision have led to the proliferation of Deepfakes. As technology democratizes over time, there is an increasing fear that novice users can create Deepfakes, to discredit others and undermine public discourse. In this paper, we conduct user studies to understand whether participants with advanced computer skills and varying level of computer science expertise can create Deepfakes of a person saying a target statement using limited media files. We conduct two studies; in the first study (n = 39) participants try creating a target Deepfake in a constrained time frame using any tool they desire. In the second study (n = 29) participants use pre-specified deep learning based tools to create the same Deepfake. We find that for the first study, of the participants successfully created complete Deepfakes with audio and video, whereas for the second user study, of the participants were successful in stitching target speech to the target video. We further use Deepfake detection software tools as well as human examiner-based analysis, to classify the successfully generated Deepfake outputs as fake, suspicious, or real. The software detector classified of the Deepfakes as fake, whereas the human examiners classified of the videos as fake. We conclude that creating Deepfakes is a simple enough task for a novice user given adequate tools and time; however, the resulting Deepfakes are not sufficiently real-looking and are unable to completely fool detection software as well as human examiners.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+Deepfakes+be+created+on+a+whim?)|0|
|[On Cohesively Polarized Communities in Signed Networks](https://doi.org/10.1145/3543873.3587698)|Jason Niu, Ahmet Erdem Sariyüce|University at Buffalo, USA|Locating and characterizing polarization is one of the most important issues to enable a healthier web ecosystem. Finding groups of nodes that form strongly stable agreements and participate in collective conflicts with other groups is an important problem in this context. Previous works approach this problem by finding balanced subgraphs, in which the polarity measure is optimized, that result in large subgraphs without a clear notion of agreement or conflict. In real-world signed networks, balanced subgraphs are often not polarized as in the case of a subgraph with only positive edges. To remedy this issue, we leverage the notion of cohesion — we find pairs of cohesively polarized communities where each node in a community is positively connected to nodes in the same community and negatively connected to nodes in the other community. To capture the cohesion along with the polarization, we define a new measure, dichotomy. We leverage the balanced triangles, which model the cohesion and polarization at the same time, to design a heuristic that results in good seedbeds for polarized communities in real-world signed networks. Then, we introduce the electron decomposition which finds cohesively polarized communities with high dichotomy score. In an extensive experimental evaluation, we show that our method finds cohesively polarized communities and outperforms the state-of-the-art methods with respect to several measures. Moreover, our algorithm is more efficient than the existing methods and practical for large-scale networks.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Cohesively+Polarized+Communities+in+Signed+Networks)|0|
|[Towards Automated Detection of Risky Images Shared by Youth on Social Media](https://doi.org/10.1145/3543873.3587607)|Jinkyung Park, Joshua Gracie, Ashwaq Alsoubai, Gianluca Stringhini, Vivek K. Singh, Pamela J. Wisniewski|Vanderbilt University, USA; University of Central Florida, USA; Boston University, USA; Rutgers University, USA|With the growing ubiquity of the Internet and access to media-based social media platforms, the risks associated with media content sharing on social media and the need for safety measures against such risks have grown paramount. At the same time, risk is highly contextualized, especially when it comes to media content youth share privately on social media. In this work, we conducted qualitative content analyses on risky media content flagged by youth participants and research assistants of similar ages to explore contextual dimensions of youth online risks. The contextual risk dimensions were then used to inform semi- and self-supervised state-of-the-art vision transformers to automate the process of identifying risky images shared by youth. We found that vision transformers are capable of learning complex image features for use in automated risk detection and classification. The results of our study serve as a foundation for designing contextualized and youth-centered machine-learning methods for automated online risk detection.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Automated+Detection+of+Risky+Images+Shared+by+Youth+on+Social+Media)|0|
|[Detecting Social Media Manipulation in Low-Resource Languages](https://doi.org/10.1145/3543873.3587615)|Samar Haider, Luca Luceri, Ashok Deb, Adam Badawy, Nanyun Peng, Emilio Ferrara|University of Southern California, USA|Social media have been deliberately used for malicious purposes, including political manipulation and disinformation. Most research focuses on high-resource languages. However, malicious actors share content across countries and languages, including low-resource ones. Here, we investigate whether and to what extent malicious actors can be detected in low-resource language settings. We discovered that a high number of accounts posting in Tagalog were suspended as part of Twitter's crackdown on interference operations after the 2016 US Presidential election. By combining text embedding and transfer learning, our framework can detect, with promising accuracy, malicious users posting in Tagalog without any prior knowledge or training on malicious content in that language. We first learn an embedding model for each language, namely a high-resource language (English) and a low-resource one (Tagalog), independently. Then, we learn a mapping between the two latent spaces to transfer the detection model. We demonstrate that the proposed approach significantly outperforms state-of-the-art models, including BERT, and yields marked advantages in settings with very limited training data -- the norm when dealing with detecting malicious activity in online platforms.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Social+Media+Manipulation+in+Low-Resource+Languages)|0|
|[Text Mining-based Social-Psychological Vulnerability Analysis of Potential Victims To Cybergrooming: Insights and Lessons Learned](https://doi.org/10.1145/3543873.3587636)|Zhen Guo, Pei Wang, JinHee Cho, Lifu Huang|Computer Science, Virginia Tech, USA; Microsoft, USA|Cybergrooming is a serious cybercrime that primarily targets youths through online platforms. Although reactive predator detection methods have been studied, proactive victim protection and crime prevention can also be achieved through vulnerability analysis of potential youth victims. Despite its significance, vulnerability analysis has not been thoroughly studied in the data science literature, while several social science studies used survey-based methods. To address this gap, we investigate humans’ social-psychological traits and quantify key vulnerability factors to cybergrooming by analyzing text features in the Linguistic Inquiry and Word Count (LIWC). Through pairwise correlation studies, we demonstrate the degrees of key vulnerability dimensions to cybergrooming from youths’ conversational features. Our findings reveal that victims have negative correlations with family and community traits, contrasting with previous social survey studies that indicated family relationships or social support as key vulnerability factors. We discuss the current limitations of text mining analysis and suggest cross-validation methods to increase the validity of research findings. Overall, this study provides valuable insights into understanding the vulnerability factors to cybergrooming and highlights the importance of adopting multidisciplinary approaches.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Text+Mining-based+Social-Psychological+Vulnerability+Analysis+of+Potential+Victims+To+Cybergrooming:+Insights+and+Lessons+Learned)|0|
|[Evaluating the Emergence of Collective Identity using Socio-Computational Techniques](https://doi.org/10.1145/3543873.3587637)|Billy Spann, Nitin Agarwal, David Stafford, Obianuju Okeke|Collaboratorium for Social Media and Online Behavioral Studies (COSMOS) - University of Arkansas at Little Rock, USA|Social media platforms provide fertile ground for investigating the processes of identity creation and communication that shape individual and public opinion. The computational methods used in social network analysis have opened the way for new approaches to be used to understand the psychological and social processes that occur when users take part in online social movements or digital activism. The research in this paper takes an interdisciplinary approach bridging social identity and deindividuation theories to show how shared, individual social identities merge into a collective identity using computational techniques. We demonstrate a novel approach to evaluating the emergence of collective identity by measuring: 1) the statistical similarity of discussion topics within online communities and 2) the strength of these communities by examining network modularity and assortative properties of the network. To accomplish this, we examined the online connective action campaign of the #stopthesteal movement that emerged during the 2020 U.S. Presidential Election. Our dataset consisted of 838,395 tweets posted by 178,296 users collected from January 04, 2020, to January 31, 2021. The results show that the network becomes more cohesive and topic similarity increases within communities leading up to and just after the elections (event 1) and the U.S. Capitol riot (event 2). Taking this multi-method approach of measuring content and network structure over time helps researchers and social scientists understand the emergence of a collective community as it is being constructed. The use of computational methods to study collective identity formation can help researchers identify the behaviors and social dynamics emerging from this type of cyber-collective movement that often serve as catalysts for these types of events. Finally, this research offers a new way to assess the psycho-social drivers of participant behaviors in cyber collective action.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evaluating+the+Emergence+of+Collective+Identity+using+Socio-Computational+Techniques)|0|
|[Trusting Decentralised Knowledge Graphs and Web Data at the Web Conference](https://doi.org/10.1145/3543873.3589756)|John Domingue, Aisling Third, MariaEsther Vidal, Philipp D. Rohde, Juan Cano, Andrea Cimmino, Ruben Verborgh|Universidad Politécnica de Madrid, Spain; The Open University, United Kingdom; Ghent University, Belgium; Leibniz University Hannover and TIB Leibniz Information Centre for Science and Technology, Germany|Knowledge Graphs have become a foundation for sharing data on the web and building intelligent services across many sectors and also within some of the most successful corporations in the world. The over centralisation of data on the web, however, has been raised as a concern by a number of prominent researchers in the field. For example, at the beginning of 2022 a €2.7B civil lawsuit was launched against Meta on the basis that it has abused its market dominance to impose unfair terms and conditions on UK users in order to exploit their personal data. Data centralisation can lead to a number of problems including: lock-in/siloing effects, lack of user control over their personal data, limited incentives and opportunities for interoperability and openness, and the resulting detrimental effects on privacy and innovation. A number of diverse approaches and technologies exist for decentralising data, such as federated querying and distributed ledgers. The main question is, though, what does decentralisation really mean for web data and Knowledge Graphs? What are the main issues and tradeoffs involved? These questions and others are addressed in this workshop.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Trusting+Decentralised+Knowledge+Graphs+and+Web+Data+at+the+Web+Conference)|0|
|[A Decentralised Persistent Identification Layer for DCAT Datasets](https://doi.org/10.1145/3543873.3587589)|Fabian Kirstein, Anton Altenbernd, Sonja Schimmler, Manfred Hauswirth|Fraunhofer FOKUS, Germany; TU Berlin, Open Distributed Systems, Germany and Weizenbaum Institute, Germany; Fraunhofer FOKUS, Germany and Weizenbaum Institute, Germany|The Data Catalogue Vocabulary (DCAT) standard is a popular RDF vocabulary for publishing metadata about data catalogs and a valuable foundation for creating Knowledge Graphs. It has widespread application in the (Linked) Open Data and scientific communities. However, DCAT does not specify a robust mechanism to create and maintain persistent identifiers for the datasets. It relies on Internationalized Resource Identifiers (IRIs), that are not necessarily unique, resolvable and persistent. This impedes findability, citation abilities, and traceability of derived and aggregated data artifacts. As a remedy, we propose a decentralized identifier registry where persistent identifiers are managed by a set of collaborative distributed nodes. Every node gives full access to all identifiers, since an unambiguous state is shared across all nodes. This facilitates a common view on the identifiers without the need for a (virtually) centralized directory. To support this architecture, we propose a data model and network methodology based on a distributed ledger and the W3C recommendation for Decentralized Identifiers (DID). We implemented our approach as a working prototype on a five-peer test network based on Hyperledger Fabric.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Decentralised+Persistent+Identification+Layer+for+DCAT+Datasets)|0|
|[Practical challenges of ODRL and potential courses of action](https://doi.org/10.1145/3543873.3587628)|Andrea Cimmino, Juan CanoBenito, Raúl GarcíaCastro|Universidad Politécnica de Madrid, Spain; UPM, Spain|The Open Digital Rights Language (ODRL) is a standard widely adopted to express privacy policies. This article presents several challenges identified in the context of the European project AURORAL in which ODRL is used to express privacy policies for Smart Communities and Rural Areas. The article presents that some challenges should be addressed directly by the ODRL standardisation group to achieve the best course of action, although others exists. For others, the authors have presented a potential solution, in particular, for considering dynamic values coming from external data sources into privacy policies. Finally, the last challenge is an open research question, since it revolves around the interoperability of privacy policies that belong to different systems and that are expressed with different privacy languages.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Practical+challenges+of+ODRL+and+potential+courses+of+action)|0|
|[The Web and Linked Data as a Solid Foundation for Dataspaces](https://doi.org/10.1145/3543873.3587616)|Sascha Meckler, Rene Dorsch, Daniel Henselmann, Andreas Harth|Fraunhofer IIS, Fraunhofer Institute for Integrated Circuits IIS, Germany|The concepts for dataspaces range from database management systems to cross-company platforms for data and applications. In this short paper, we present the “Solid Data Space” (SDS), a concept for dataspaces that build on top of the (Semantic) Web and Social Linked Data (Solid). Existing Web technologies and Linked Data principles form the foundation for open, decentralized networks for sovereign data exchange between citizens, organizations and companies. Domain-specific dataspace implementations can extend the agreements for communication and collaboration to enable specific functionality. We compare the SDS with principles and components of the emerging International Data Spaces to identify similarities and point out technological differences.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Web+and+Linked+Data+as+a+Solid+Foundation+for+Dataspaces)|0|
|[Extending Actor Models in Data Spaces](https://doi.org/10.1145/3543873.3587645)|Hendrik Meyer zum Felde, Maarten Kollenstart, Thomas Bellebaum, Simon Dalmolen, Gerd Brost|Fraunhofer AISEC, Germany; TNO, Netherlands|In today’s internet almost any party can share sets of data with each other. However, creating frameworks and regulated realms for the sharing of data is very complex when multiple parties are involved and complicated regulation comes into play. As solution data spaces were introduced to enable participating parties to share data among themselves in an organized, regulated and standardized way. However, contract data processors, acting as data space participants, are currently unable to execute data requests on behalf of their contract partners. Here we show that an on-behalf-of actor model can be easily added to existing data spaces. We demonstrate how this extension can be realized using verifiable credentials. We provide a sample use case, a detailed sequence diagram and discuss necessary architectural adaptations and additions to established protocols. Using the extensions explained in this work numerous real life use cases which previously could technically not be realized can now be covered. This enables future data spaces to provide more dynamic and complex real world use cases.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Extending+Actor+Models+in+Data+Spaces)|0|
|[Towards Decentralised Learning Analytics (Positioning Paper)](https://doi.org/10.1145/3543873.3587644)|Audrey Ekuban, John Domingue|The Open University, United Kingdom|When students interact with an online course, the routes they take when navigating through the course can be captured. Learning Analytics is the process of measuring, collecting, recording, and analysing this Student Activity Data. Predictive Learning Analytics, a sub-field of Learning Analytics, can help to identify students who are at risk of dropping out or failing, as well as students who are close to a grade boundary. Course tutors can use the insights provided by the analyses to offer timely assistance to these students. Despite its usefulness, there are privacy and ethical issues with the typically centralised approach to Predictive Learning Analytics. In this positioning paper, it is proposed that the issues associated with Predictive Learning Analytics can be alleviated, in a framework called EMPRESS, by combining 1) self-sovereign data, where data owners control who legitimately has access to data pertaining to them, 2) Federated Learning, where the data remains on the data owner’s device and/or the data is processed by the data owners themselves, and 3) Graph Convolutional Networks for Heterogeneous graphs, which are examples of knowledge graphs.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Decentralised+Learning+Analytics+(Positioning+Paper))|0|
|[Analyzing Distributed Medical Data in FAIR Data Spaces](https://doi.org/10.1145/3543873.3587663)|Mehrshad Jaberansary, Macedo Maia, Yeliz Ucer Yediel, Oya Beyan, Toralf Kirsten|Institute for Biomedical Informatics, Faculty of Medicine and University Hospital Cologne, University of Cologne, Germany and Fraunhofer Institute for Applied Information Techniques (FIT), Germany; Fraunhofer Institute for Applied Information Technology (FIT), Germany and Information Systems and Database Technology Research Group, RWTH Aachen University, 52062 Aachen, Germany, Germany; Institute for Biomedical Informatics, Faculty of Medicine and University Hospital Cologne, University of Cologne, Germany; Institute for Medical Informatics, Statistics and Epidemiology (IMISE), Leipzig University, Germany|The exponential growth in data production has led to increasing demand for high-quality data-driven services. Additionally, the benefits of data-driven analysis are vast and have significantly propelled research in many fields. Data sharing benefits scientific advancement, as it promotes transparency, and collaboration, accelerates research and aids in making informed decisions. The European strategy for data aims to create a single data market that ensures Europe’s global competitiveness and data sovereignty. Common European Data Spaces ensure that data from different sources are available in the economy and society, while data providers (e.g., hospitals and scientists) control data access. The National Research Data Infrastructure for Personal Health Data (NFDI4Health) initiative is a prime example of an effort focused on data from clinical trials and public health studies. Collecting and analyzing this data is essential to developing novel therapies, comprehensive care approaches, and preventive measures in modern healthcare systems. This work describes distributed data analysis services and components that adhere to the FAIR data principles (Findable, Accessible, Interoperable, and Reusable) within the data space environment. We focus on distributed analytics functionality in Gaia-X-based data spaces. Gaia-X offers a trustworthy federation of data infrastructure and service providers for European countries.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analyzing+Distributed+Medical+Data+in+FAIR+Data+Spaces)|0|
|[Requirements and Building Blocks for Manufacturing Dataspaces](https://doi.org/10.1145/3543873.3587664)|Rohit A. Deshmukh, Sisay Adugna Chala, Christoph Lange|Fraunhofer Institute for Applied Information Technology FIT, 53754 Sankt Augustin, Germany; Fraunhofer Institute for Applied Information Technology FIT, 53754 Sankt Augustin, Germany and Chair of Databases and Information Systems (i5), RWTH Aachen University, 52074 Aachen, Germany|With the advent and pervasiveness of the Internet of Things (IoT), big data and cloud computing technologies, digitalization in enterprises and factories has rapidly increased in the last few years. Digital platforms have emerged as an effective mechanism for enabling the management and sharing of data from various companies. To enable sharing of data beyond the platform boundaries, definition of new platform federation approaches is on the rise. This makes enabling the federation of digital platforms a key requirement for large-scale dataspaces. Therefore, the identification of platform federation requirements and building blocks for such dataspaces needs to be systematically addressed. In this paper, we try to systematically explore the high-level requirements for enabling a federation of digital platforms in the manufacturing domain and identify a set of building blocks. We integrate the requirements and building blocks into the notion of dataspaces. The identified requirements and building blocks act as a blueprint for designing and instantiating new dataspaces, thereby speeding up the development process and reducing costs. We present a case study to illustrate how the use of common building blocks can act as common guiding principles and result in more complete and interoperable implementations.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Requirements+and+Building+Blocks+for+Manufacturing+Dataspaces)|0|
|[Towards Multimodal Knowledge Graphs for Data Spaces](https://doi.org/10.1145/3543873.3587665)|Atiya Usmani, Muhammad Jaleed Khan, John G. Breslin, Edward Curry|Insight SFI Research Centre for Data Analytics, Data Science Institute, University of Galway, Ireland; SFI Centre for Research Training in Artificial Intelligence, Data Science Institute, University of Galway, Ireland|Multimodal knowledge graphs have the potential to enhance data spaces by providing a unified and semantically grounded structured representation of multimodal data produced by multiple sources. With the ability to integrate and analyze data in real-time, multimodal knowledge graphs offer a wealth of insights for smart city applications, such as monitoring traffic flow, air quality, public safety, and identifying potential hazards. Knowledge enrichment can enable a more comprehensive representation of multimodal data and intuitive decision-making with improved expressiveness and generalizability. However, challenges remain in effectively modelling the complex relationships between and within different types of modalities in data spaces and infusing common sense knowledge from external sources. This paper reviews the related literature and identifies major challenges and key requirements for effectively developing multimodal knowledge graphs for data spaces, and proposes an ontology for their construction.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Multimodal+Knowledge+Graphs+for+Data+Spaces)|0|
|[SPACE_DS: Towards a Circular Economy Data Space](https://doi.org/10.1145/3543873.3587685)|André Pomp, Maike Jansen, Holger Berg, Tobias Meisen|Wuppertal Institute for Climate, Environment and Energy, Germany; Institute for Technologies and Management of Digital Transformation, University of Wuppertal, Germany|The circular economy (CE) is essential to achieving a sustainable future through resource conservation and climate protection. Efficient use of materials and products over time is a critical aspect of CE, helping to reduce CO2 emissions, waste and resource consumption. The Digital Product Passport (DPP) is a CE-specific approach that contains information about components and their origin, and can also provide environmental and social impact assessments. However, creating a DPP requires collecting and analyzing data from many different stakeholders along the supply chain and even throughout the product lifecycle. In this paper, we present a concept for the SPACE_DS, which is a data space for circular economy data. A key point here is that the SPACE_DS enables the creation of DPPs by especially considering privacy and security concerns of data providers.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SPACE_DS:+Towards+a+Circular+Economy+Data+Space)|0|
|[Towards a Data Space for Interoperability of Analytic Provenance](https://doi.org/10.1145/3543873.3587686)|Tristan Langer, André Pomp, Tobias Meisen|Institute for Technologies and Management of Digital Transformation, University of Wuppertal, Germany|Capturing, visualizing and analyzing provenance data to better understand and support analytic reasoning processes is a rapidly growing research field named analytic provenance. Provenance data includes the state of a visualization within a tool as well as the user’s interactions performed while interacting with the tool. Research in this field has produced in many new approaches that generate data for specific tools and use cases. However, since a variety of tools are used and analytic tasks are performed in real analysis use cases there is a problem in building an interoperable baseline data corpus for investigation of the transferability of different approaches. In this paper, we present a visionary data space architecture for integrating and processing analytic provenance data in a unified way using semantic modeling. We discuss emerging challenges and research opportunities to realize such a vision using semantic models in data spaces to enable analytic provenance data interoperability.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+a+Data+Space+for+Interoperability+of+Analytic+Provenance)|0|
|[Provenance for Lattice QCD workflows](https://doi.org/10.1145/3543873.3587559)|Tanja Auge, Gunnar Bali, Meike Klettke, Bertram Ludäscher, Wolfgang Söldner, Simon Weishäupl, Tilo Wettig|Department of Physics, University of Regensburg, Germany; School of Information Sciences, University of Illinois at Urbana-Champaign, USA; Faculty of Computer Science and Data Science, University of Regensburg, Germany|We present a provenance model for the generic workflow of numerical Lattice Quantum Chromodynamics (QCD) calculations, which constitute an important component of particle physics research. These calculations are carried out on the largest supercomputers worldwide with data in the multi-PetaByte range being generated and analyzed. In the Lattice QCD community, a custom metadata standard (QCDml) that includes certain provenance information already exists for one part of the workflow, the so-called generation of configurations. In this paper, we follow the W3C PROV standard and formulate a provenance model that includes both the generation part and the so-called measurement part of the Lattice QCD workflow. We demonstrate the applicability of this model and show how the model can be used to answer some provenance-related research questions. However, many important provenance questions in the Lattice QCD community require extensions of this provenance model. To this end, we propose a multi-layered provenance approach that combines prospective and retrospective elements.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Provenance+for+Lattice+QCD+workflows)|0|
|[Implementing an Environmental Management System Using Provenance-By-Design](https://doi.org/10.1145/3543873.3587560)|Luc Moreau, Nicola Hogan, Nick O'Donnell|Estates and Facilities, King's College London, United Kingdom; Department of Informatics, King's College London, United Kingdom|Organisations have to comply with environmental regulations to protect the environment and meet internationally agreed climate change targets. To assist organisations, processes and standards are being defined to manage these compliance obligations. They typically rely on a notion of Environmental Management System (EMS), defined as a reflective framework allowing organisations to set and manage their goals, and demonstrate they follow due processes in order to comply with prevailing regulations. The importance of these obligations can be highlighted by the fact that failing to comply may lead to significant liabilities for organisations. An EMS framework, typically structured as a set of documents and spreadsheets, contains a record of continuously evolving regulations, teams, stakeholders, actions and updates. However, the maintainance of an EMS is often human driven, and therefore is error prone despite the meticulousness of environmental officers, and further requires external human auditing to check their validity. To avoid green washing, but also to contain the burden and cost of compliance, it is desirable for these claims to be checked by trusted automated means. Provenance is ideally suited to track the changes occurring in an EMS, allowing queries to determine precisely which compliance objective is prevailing at any point in time, whether it is being met, and who is responsible for it. Thus, this paper has a dual aim: first, it investigates the benefits of provenance for EMS, second, it presents the application of an emerging approach “Provenance-By-Design”, which automatically converts a specification of an EMS data model and its provenance to a data backend, a service for processing and querying of EMS provenance data, a client-side library to interact with such a service, and a simple user interface allowing developers to navigate the provenance. The application of a Provenance-By-Design approach to EMS applications results in novel opportunities for a provenance-based EMS; we present our preliminary reflection on their potential.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Implementing+an+Environmental+Management+System+Using+Provenance-By-Design)|0|
|[Trust the Process: Analyzing Prospective Provenance for Data Cleaning](https://doi.org/10.1145/3543873.3587558)|Nikolaus Nova Parulian, Bertram Ludäscher|School of Information Sciences, University of Illinois at Urbana Champaign, USA|In the field of data-driven research and analysis, the quality of results largely depends on the quality of the data used. Data cleaning is a crucial step in improving the quality of data. Still, it is equally important to document the steps made during the data cleaning process to ensure transparency and enable others to assess the quality of the resulting data. While provenance models such as W3C PROV have been introduced to track changes and events related to any entity, their use in documenting the provenance of data-cleaning workflows can be challenging, particularly when mixing different types of documents or entities in the model. To address this, we propose a conceptual model and analysis that breaks down data-cleaning workflows into process abstraction and workflow recipes, refining operations to the column level. This approach provides users with detailed provenance information, enabling transparency, auditing, and support for data cleaning workflow improvements. Our model has several features that allow static analysis, e.g., to determine the minimal input schema and expected output schema for running a recipe, to identify which steps violate the column schema requirement constraint, and to assess the reusability of a recipe on a new dataset. We hope that our model and analysis will contribute to making data processing more transparent, accessible, and reusable.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Trust+the+Process:+Analyzing+Prospective+Provenance+for+Data+Cleaning)|0|
|[Deep Learning Provenance Data Integration: a Practical Approach](https://doi.org/10.1145/3543873.3587561)|Débora B. Pina, Adriane Chapman, Daniel de Oliveira, Marta Mattoso|Federal University of Rio de Janeiro (UFRJ), Brazil; University of Southampton, United Kingdom; Fluminense Federal University (UFF), Brazil|A Deep Learning (DL) life cycle involves several data transformations, such as performing data pre-processing, defining datasets to train and test a deep neural network (DNN), and training and evaluating the DL model. Choosing a final model requires DL model selection, which involves analyzing data from several training configurations (e.g. hyperparameters and DNN architectures). Tracing training data back to pre-processing operations can provide insights into the model selection step. Provenance is a natural solution to represent data derivation of the whole DL life cycle. However, there are challenges in providing an integration of the provenance of these different steps. There are a few approaches to capturing and integrating provenance data from the DL life cycle, but they require that the same provenance capture solution is used along all the steps, which can limit interoperability and flexibility when choosing the DL environment. Therefore, in this work, we present a prototype for provenance data integration using different capture solutions. We show use cases where the integrated provenance from pre-processing and training steps can show how data pre-processing decisions influenced the model selection. Experiments were performed using real-world datasets to train a DNN and provided evidence of the integration between the considered steps, answering queries such as how the data used to train a model that achieved a specific result was processed.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Learning+Provenance+Data+Integration:+a+Practical+Approach)|0|
|[Providing Data on Financial Results of Public Companies Enriched with Provenance for OBInvest](https://doi.org/10.1145/3543873.3587566)|Saulo Almeida, Gilberto Passos, Valquire Jesus, Sérgio Manuel Serra da Cruz, Jorge Zavaleta|UFRJ, Brazil|Financial Literacy (FL) initiatives, aimed at young people in formal or informal learning spaces, are defended and implemented in several countries, being encouraged since 2005 by the Organization for Economic Co-operation and Development (OECD). In Brazil, the teaching and learning process in several areas has been stimulated through Academic Competitions generally called Knowledge Olympics, which are essentially student contests that aim to encourage, find talent and awaken interest in the field knowledge presented in the competition. It was precisely for this purpose that the Brazilian Investment Olympics (OBInvest) was born, aiming to democratize access to education and promote reflections on economic and financial issues, through a FL perspective for high school students from all over the country. One of OBInvest’s objectives is to help boosting the development of computational tools, aiming to provide easier access to fundamental data for decision-making in the field of finance. However, from the tools developed by OBInvest, it was noted that the creation of new educational tools would be enhanced through the use of datasets enriched with provenance and aligned with FAIR principles. This work aims to offer a computational strategy based on data science techniques, which is easy to use and also provides curated data series through a reproducible pipeline, using open data on financial reports from publicly listed Brazilian companies, provided by the Brazilian Security and Exchange Commission, called Comissão de Valores Mobiliarios (CMV). During the exploration of related works, we found just a few academic works that use CVM data with little expressive results, which motivated the development of a tool called DRE-CVM, that was supported by computational tools, with a focus on the Python language, Pandas library, the KNIME workflow platform, and Jupyter integrated development environments, running on the Anaconda3 platform over a Docker container. It’s also possible run this experiment in the Google Colabotory cloud environment. This processing it’s capable of executing reproducible pipelines and using curated, fairified, and annotated data with the retrospective source metadata of the financial statements of publicly traded Brazilian companies. The artifact uses pipelines that can be reused by students and other interested parties in finance to study the behaviors of a company’s time series results and thus introduce research on predicting future results. The last executable version of the DRE-CVM experiment can be accessed through Zenodo website at https://doi.org/10.5281/zenodo.7110653 and can be reproduced using a Docker Container available on DockerHub repository. Some improvements can be incorporated into the presented work, the main suggestions for future work are: (i) Perform more substantial analyses on the created dataset, such as predicting results based on the history of demonstration results; (ii) Recover other types of information made available by CVM, to be used during the activities of the Brazilian Investment Olympics; (iii) Adapt the docker image so that it can be executed in the My Binder cloud environment, aiming to improve reproducibility issues.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Providing+Data+on+Financial+Results+of+Public+Companies+Enriched+with+Provenance+for+OBInvest)|0|
|[Using diversity as a source of scientific innovation for the Web](https://doi.org/10.1145/3543507.3593046)|Barbara Poblete|Department of Computer Science, University of Chile, Chile|The Web has become a resource that allows us to make sense of social phenomena around the world. This started the moment users became content creators, and has grown with the emergence of social platforms tailored to our need to connect and share with others. Throughout my work, I’ve come to appreciate how social media has democratized access to real-world news and social sentiment, while also witnessing the loss of trust created by fake information. As a computer scientist from Chile in Latin America, I have worked on a range of problems that were driven by local needs. Many times, I have tried to apply state of the art solutions to well-known problems, only to find that these don’t work outside of their initial evaluation dataset. In this talk, I’ll discuss how geographical, language, and social diversity have opened new avenues for innovation and better understanding the social Web. I’ll also show that to truly create useful technological solutions, we must develop inclusive research and resources.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Using+diversity+as+a+source+of+scientific+innovation+for+the+Web)|0|
|[Concept Regulation in the Social Sciences](https://doi.org/10.1145/3543507.3593050)|Zachary Elkins|University of Texas at Austin, USA|The sciences, notably biology and medicine, operate with highly regulated taxonomies and ontologies. The Social Sciences, on the other hand, muddle through in a proverbial tower of Babel. There may be some real benefits to an undisciplined set of ideas, but also some real costs. Over the last ten years, political scientists have attempted to get their semantic act by cooperating to formalize their vocabulary. The result has been a dramatic improvement in how scholars diagnose and treat problems of democracy, as well as a set of web applications that have changed the way countries write constitutions. Nevertheless, these methods of semantic cooperation have exposed some persistent challenges of “social engineering,” ones that may have tractable web solutions.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Concept+Regulation+in+the+Social+Sciences)|0|
|[GNNs and Graph Generative models for biomedical applications](https://doi.org/10.1145/3543507.3593049)|Michalis Vazirgiannis|Ecole Polytechnique de France, France|Graph generative models are recently gaining significant interest in current application domains. They are commonly used to model social networks, knowledge graphs, and protein-protein interaction networks. In this talk we will present the potential of graph generative models and our recent relevant efforts in the biomedical domain. More specifically we present a novel architecture that generates medical records as graphs with privacy guarantees. We capitalize and modify the graph Variational autoencoders (VAEs) architecture. We train the generative model with the well known MIMIC medical database and achieve generated data that are very similar to the real ones yet provide privacy guarantees. We also develop new GNNs for predicting antibiotic resistance and other protein related downstream tasks such as enzymes classifications and Gene Ontology classification. We achieve there as well promising results with potential for future application in broader biomedical related tasks. Finally we present future research directions for multi modal generative models involving graphs.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GNNs+and+Graph+Generative+models+for+biomedical+applications)|0|
|[Decolonizing Creative Labor in the age of AI](https://doi.org/10.1145/3543507.3593047)|Payal Arora|Erasmus University, Netherlands|Creative AI has got us asking existential questions of what makes us human. To crack the code, you need to crack the culture that makes us who we are. Who and what is creative remains largely disconnected from diverse and global cultural norms, rendering existing technology suboptimal and even unusable to the world’s majority. Creativity has long been dictated by the aesthetic taste, values, needs, concerns, and aspirations of the West. Today, India and China alone account for the majority of the world’s users. The Global South are fast shaping data systems in ways that remain underexamined and siloed as “Rest of World” among industry and government folks. With the rise of the creator economy across sectors, questions abound on creative rights, provenance, fairness, labor, and representation. This talk discusses concerns around digital labor, data materiality, media literacies, creative value, and online expression. In doing so, it sets a pathway towards designing inclusive and intersectional systems that transcend borders.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decolonizing+Creative+Labor+in+the+age+of+AI)|0|
|[Connectivity](https://doi.org/10.1145/3543507.3593048)|Robert Melancton Metcalfe|Univ Vienna, Dept Pharmaceut Chem, Pharmacoinformat Res Grp, A-1090 Vienna, Austria; Maastricht Univ, NUTRIM, Dept Bioinformat BiGCaT, NL-6229 ER Maastricht, Netherlands; Univ Fed Minas Gerais, Dept Bioquim & Imunol, Inst Ciencias Biol, BR-31270901 Belo Horizonte, MG, Brazil; Micelio, B-2180 Antwerp, Belgium; Gladstone Inst, Inst Data Sci & Biotechnol, San Francisco, CA 94158 USA|WikiPathways (https://www.wikipathways.org) is a biological pathway database known for its collaborative nature and open science approaches. With the core idea of the scientific community developing and curating biological knowledge in pathway models, WikiPathways lowers all barriers for accessing and using its content. Increasingly more content creators, initiatives, projects and tools have started using WikiPathways. Central in this growth and increased use of WikiPathways are the various communities that focus on particular subsets of molecular pathways such as for rare diseases and lipid metabolism. Knowledge from published pathway figures helps prioritize pathway development, using optical character and named entity recognition. We show the growth of WikiPathways over the last three years, highlight the new communities and collaborations of pathway authors and curators, and describe various technologies to connect to external resources and initiatives. The road toward a sustainable, community-driven pathway database goes through integration with other resources such as Wikidata and allowing more use, curation and redistribution of WikiPathways content.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Connectivity)|0|
|[Fair Graph Representation Learning via Diverse Mixture-of-Experts](https://doi.org/10.1145/3543507.3583207)|Zheyuan Liu, Chunhui Zhang, Yijun Tian, Erchi Zhang, Chao Huang, Yanfang Ye, Chuxu Zhang|University of Notre Dame, USA; University of Hong Kong, Hong Kong; Brandeis University, USA|Graph Neural Networks (GNNs) have demonstrated a great representation learning capability on graph data and have been utilized in various downstream applications. However, real-world data in web-based applications (e.g., recommendation and advertising) always contains bias, preventing GNNs from learning fair representations. Although many works were proposed to address the fairness issue, they suffer from the significant problem of insufficient learnable knowledge with limited attributes after debiasing. To address this problem, we develop Graph-Fairness Mixture of Experts (G-Fame), a novel plug-and-play method to assist any GNNs to learn distinguishable representations with unbiased attributes. Furthermore, based on G-Fame, we propose G-Fame++, which introduces three novel strategies to improve the representation fairness from node representations, model layer, and parameter redundancy perspectives. In particular, we first present the embedding diversified method to learn distinguishable node representations. Second, we design the layer diversified strategy to maximize the output difference of distinct model layers. Third, we introduce the expert diversified method to minimize expert parameter similarities to learn diverse and complementary representations. Extensive experiments demonstrate the superiority of G-Fame and G-Fame++ in both accuracy and fairness, compared to state-of-the-art methods across multiple graph datasets.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Graph+Representation+Learning+via+Diverse+Mixture-of-Experts)|0|
|[Multi-Aspect Heterogeneous Graph Augmentation](https://doi.org/10.1145/3543507.3583208)|Yuchen Zhou, Yanan Cao, Yongchao Liu, Yanmin Shang, Peng Zhang, Zheng Lin, Yun Yue, Baokun Wang, Xing Fu, Weiqiang Wang|Ant Group, China; Institute of Information Engineering, Chinese Academy of Sciences, China and School of Cyber Security, University of Chinese Academy of Sciences, China; School of Cyber Security, University of Chinese Academy of Sciences, China and Institute of Information Engineering, Chinese Academy of Sciences, China; Cyberspace Institute of Advanced Technology, Guangzhou University, China|Data augmentation has been widely studied as it can be used to improve the generalizability of graph representation learning models. However, existing works focus only on the data augmentation on homogeneous graphs. Data augmentation for heterogeneous graphs remains under-explored. Considering that heterogeneous graphs contain different types of nodes and links, ignoring the type information and directly applying the data augmentation methods of homogeneous graphs to heterogeneous graphs will lead to suboptimal results. In this paper, we propose a novel Multi-Aspect Heterogeneous Graph Augmentation framework named MAHGA. Specifically, MAHGA consists of two core augmentation strategies: structure-level augmentation and metapath-level augmentation. Structure-level augmentation pays attention to network schema aspect and designs a relation-aware conditional variational auto-encoder that can generate synthetic features of neighbors to augment the nodes and the node types with scarce links. Metapath-level augmentation concentrates on metapath aspect, which constructs metapath reachable graphs for different metapaths and estimates the graphons of them. By sampling and mixing up based on the graphons, MAHGA yields intra-metapath and inter-metapath augmentation. Finally, we conduct extensive experiments on multiple benchmarks to validate the effectiveness of MAHGA. Experimental results demonstrate that our method improves the performances across a set of heterogeneous graph learning models and datasets.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Aspect+Heterogeneous+Graph+Augmentation)|0|
|[Testing Cluster Properties of Signed Graphs](https://doi.org/10.1145/3543507.3583213)|Florian Adriaens, Simon Apers|University of Helsinki, Finland; Université de Paris, CNRS, IRIF, France|This work initiates the study of property testing in signed graphs, where every edge has either a positive or a negative sign. We show that there exist sublinear query and time algorithms for testing three key properties of signed graphs: balance (or 2-clusterability), clusterability and signed triangle freeness. We consider both the dense graph model, where one queries the adjacency matrix entries of a signed graph, and the bounded-degree model, where one queries for the neighbors of a node and the sign of the connecting edge. Our algorithms use a variety of tools from unsigned graph property testing, as well as reductions from one setting to the other. Our main technical contribution is a sublinear algorithm for testing clusterability in the bounded-degree model. This contrasts with the property of k-clusterability in unsigned graphs, which is not testable with a sublinear number of queries in the bounded-degree model. We experimentally evaluate the complexity and usefulness of several of our testers on real-life and synthetic datasets.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Testing+Cluster+Properties+of+Signed+Graphs)|0|
|[RSGNN: A Model-agnostic Approach for Enhancing the Robustness of Signed Graph Neural Networks](https://doi.org/10.1145/3543507.3583221)|Zeyu Zhang, Jiamou Liu, Xianda Zheng, Yifei Wang, Pengqian Han, Yupan Wang, Kaiqi Zhao, Zijian Zhang|; The University of Auckland, New Zealand; Beijing Institute of Technology, China|Signed graphs model complex relations using both positive and negative edges. Signed graph neural networks (SGNN) are powerful tools to analyze signed graphs. We address the vulnerability of SGNN to potential edge noise in the input graph. Our goal is to strengthen existing SGNN allowing them to withstand edge noises by extracting robust representations for signed graphs. First, we analyze the expressiveness of SGNN using an extended Weisfeiler-Lehman (WL) graph isomorphism test and identify the limitations to SGNN over triangles that are unbalanced. Then, we design some structure-based regularizers to be used in conjunction with an SGNN that highlight intrinsic properties of a signed graph. The tools and insights above allow us to propose a novel framework, Robust Signed Graph Neural Network (RSGNN), which adopts a dual architecture that simultaneously denoises the graph while learning node representations. We validate the performance of our model empirically on four real-world signed graph datasets, i.e., Bitcoin_OTC, Bitcoin_Alpha, Epinion and Slashdot, RSGNN can clearly improve the robustness of popular SGNN models. When the signed graphs are affected by random noise, our method outperforms baselines by up to 9.35% Binary-F1 for link sign prediction. Our implementation is available in PyTorch1.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RSGNN:+A+Model-agnostic+Approach+for+Enhancing+the+Robustness+of+Signed+Graph+Neural+Networks)|0|
|[Multi-aspect Diffusion Network Inference](https://doi.org/10.1145/3543507.3583228)|Hao Huang, Keqi Han, Beicheng Xu, Ting Gan|Wuhan University, China|To learn influence relationships between nodes in a diffusion network, most existing approaches resort to precise timestamps of historical node infections. The target network is customarily assumed as an one-aspect diffusion network, with homogeneous influence relationships. Nonetheless, tracing node infection timestamps is often infeasible due to high cost, and the type of influence relationships may be heterogeneous because of the diversity of propagation media. In this work, we study how to infer a multi-aspect diffusion network with heterogeneous influence relationships, using only node infection statuses that are more readily accessible in practice. Equipped with a probabilistic generative model, we iteratively conduct a posteriori, quantitative analysis on historical diffusion results of the network, and infer the structure and strengths of homogeneous influence relationships in each aspect. Extensive experiments on both synthetic and real-world networks are conducted, and the results verify the effectiveness and efficiency of our approach.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-aspect+Diffusion+Network+Inference)|0|
|[Encoding Node Diffusion Competence and Role Significance for Network Dismantling](https://doi.org/10.1145/3543507.3583233)|Jiazheng Zhang, Bang Wang|School of Electronic Information and Communications, Huazhong University of Science and Technology (HUST), Wuhan, China, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, China|Percolation theory shows that removing a small fraction of critical nodes can lead to the disintegration of a large network into many disconnected tiny subnetworks. The network dismantling task focuses on how to efficiently select the least such critical nodes. Most existing approaches focus on measuring nodes' importance from either functional or topological viewpoint. Different from theirs, we argue that nodes' importance can be measured from both of the two complementary aspects: The functional importance can be based on the nodes' competence in relaying network information; While the topological importance can be measured from nodes' regional structural patterns. In this paper, we propose an unsupervised learning framework for network dismantling, called DCRS, which encodes and fuses both node diffusion competence and role significance. Specifically, we propose a graph diffusion neural network which emulates information diffusion for competence encoding; We divide nodes with similar egonet structural patterns into a few roles, and construct a role graph on which to encode node role significance. The DCRS converts and fuses the two encodings to output a final ranking score for selecting critical nodes. Experiments on both real-world networks and synthetic networks demonstrate that our scheme significantly outperforms the state-of-the-art competitors for its mostly requiring much fewer nodes to dismantle a network.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Encoding+Node+Diffusion+Competence+and+Role+Significance+for+Network+Dismantling)|0|
|[Opinion Maximization in Social Networks via Leader Selection](https://doi.org/10.1145/3543507.3583243)|Xiaotian Zhou, Zhongzhi Zhang|Fudan University, China|We study a leader selection problem for the DeGroot model of opinion dynamics in a social network with n nodes and m edges, in the presence of s0 = O(1) leaders with opinion 0. Concretely, we consider the problem of maximizing the average opinion in equilibrium by selecting k = O(1) leaders with opinion 1 from the remaining n − s0 nodes, which was previously proved to be NP-hard. A deterministic greedy algorithm was also proposed to approximately solve the problem, which has an approximation factor (1 − 1/e) and time complexity O(n3), and thus does not apply to large networks. In this paper, we first give an interpretation for the opinion of each node in equilibrium and the disagreement of the model from the perspective of resistor networks. We then develop a fast randomized greedy algorithm to solve the problem. To this end, we express the average opinion in terms of the pseudoinverse and Schur complement of Laplacian matrix for . The key ingredients of our randomized algorithm are Laplacian solvers and node sparsifiers, where the latter can preserve pairwise effective resistance by viewing Schur complement as random walks with average length l. For any error parameter ϵ > 0, at each iteration, the randomized algorithm selects a node that deviates from the local optimum marginal gain at most ϵ. The time complexity of the fast algorithm is O(mkllog nϵ− 2). Extensive experiments on various real networks show that the effectiveness of our randomized algorithm is similar to that of the deterministic algorithm, both of which are better than several baseline algorithms, and that our randomized algorithm is more efficient and scalable to large graphs with more than one million nodes.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Opinion+Maximization+in+Social+Networks+via+Leader+Selection)|0|
|[Graph Self-supervised Learning with Augmentation-aware Contrastive Learning](https://doi.org/10.1145/3543507.3583246)|Dong Chen, Xiang Zhao, Wei Wang, Zhen Tan, Weidong Xiao|Data Science and Analytics Thrust, Information Hub, The Hong Kong University of Science and Technology (Guangzhou), China; College of Systems Engineering, National University of Defense Technology, China; Laboratory for Big Data and Decision, National University of Defense Technology, China|Graph self-supervised learning aims to mine useful information from unlabeled graph data, and has been successfully applied to pre-train graph representations. Many existing approaches use contrastive learning to learn powerful embeddings by learning contrastively from two augmented graph views. However, none of these graph contrastive methods fully exploits the diversity of different augmentations, and hence is prone to overfitting and limited generalization ability of learned representations. In this paper, we propose a novel Graph Self-supervised Learning method with Augmentation-aware Contrastive Learning. Our method is based on the finding that the pre-trained model after adding augmentation diversity can achieve better generalization ability. To make full use of the information from the diverse augmentation method, this paper constructs new augmentation-aware prediction task which complementary with the contrastive learning task. Similar to how pre-training requires fast adaptation to different downstream tasks, we simulate train-test adaptation on the constructed tasks for further enhancing the learning ability; this strategy can be deemed as a form of meta-learning. Experimental results show that our method outperforms previous methods and learns better representations for a variety of downstream tasks.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Self-supervised+Learning+with+Augmentation-aware+Contrastive+Learning)|0|
|[Unifying and Improving Graph Convolutional Neural Networks with Wavelet Denoising Filters](https://doi.org/10.1145/3543507.3583253)|Liangtian Wan, Xiaona Li, Huijin Han, Xiaoran Yan, Lu Sun, Zhaolong Ning, Feng Xia|Department of Communication Engineering, Institute of Information Science Technology, Dalian Maritime University, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing University of Posts and Telecommunications, China; Research Center of Big Data Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, China; School of Computing Technologies, RMIT University, Australia; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, China|Graph convolutional neural network (GCN) is a powerful deep learning framework for network data. However, variants of graph neural architectures can lead to drastically different performance on different tasks. Model comparison calls for a unifying framework with interpretability and principled experimental procedures. Based on the theories from graph signal processing (GSP), we show that GCN’s capability is fundamentally limited by the uncertainty principle, and wavelets provide a controllable trade-off between local and global information. We adapt wavelet denoising filters to the graph domain, unifying popular variants of GCN under a common interpretable mathematical framework. Furthermore, we propose WaveThresh and WaveShrink which are novel GCN models based on proven denoising filters from the signal processing literature. Empirically, we evaluate our models and other popular GCNs under a more principled procedure and analyze how trade-offs between local and global graph signals can lead to better performance in different datasets.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unifying+and+Improving+Graph+Convolutional+Neural+Networks+with+Wavelet+Denoising+Filters)|0|
|[Neighborhood Structure Configuration Models](https://doi.org/10.1145/3543507.3583266)|Felix I. Stamm, Michael Scholkemper, Michael T. Schaub, Markus Strohmaier|RWTH Aachen University, Germany; University of Mannheim & GESIS, Germany|We develop a new method to efficiently sample synthetic networks that preserve the d-hop neighborhood structure of a given network for any given d. The proposed algorithm trades off the diversity in network samples against the depth of the neighborhood structure that is preserved. Our key innovation is to employ a colored Configuration Model with colors derived from iterations of the so-called Color Refinement algorithm. We prove that with increasing iterations the preserved structural information increases: the generated synthetic networks and the original network become more and more similar, and are eventually indistinguishable in terms of centrality measures such as PageRank, HITS, Katz centrality and eigenvector centrality. Our work enables to efficiently generate samples with a precisely controlled similarity to the original network, especially for large networks.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neighborhood+Structure+Configuration+Models)|0|
|[CurvDrop: A Ricci Curvature Based Approach to Prevent Graph Neural Networks from Over-Smoothing and Over-Squashing](https://doi.org/10.1145/3543507.3583269)|Yang Liu, Chuan Zhou, Shirui Pan, Jia Wu, Zhao Li, Hongyang Chen, Peng Zhang|School of Information and Communication Technology, Griffith University, Australia; Hangzhou link2do Technology, China; Cyberspace Institute of Advanced Technology, Guangzhou University, China; AMSS, Chinese Academy of Science, China and School of Cyber Security, University of Chinese Academy of Science, China; Research Center for Graph Computing, Zhejiang Lab, China; School of Computing, Macquarie University, Australia; Academy of Mathematics and Systems Science, Chinese Academy of Sciences, China|Graph neural networks (GNNs) are powerful models to handle graph data and can achieve state-of-the-art in many critical tasks including node classification and link prediction. However, existing graph neural networks still face both challenges of over-smoothing and over-squashing based on previous literature. To this end, we propose a new Curvature-based topology-aware Dropout sampling technique named CurvDrop, in which we integrate the Discrete Ricci Curvature into graph neural networks to enable more expressive graph models. Also, this work can improve graph neural networks by quantifying connections in graphs and using structural information such as community structures in graphs. As a result, our method can tackle the both challenges of over-smoothing and over-squashing with theoretical justification. Also, numerous experiments on public datasets show the effectiveness and robustness of our proposed method. The code and data are released in https://github.com/liu-yang-maker/Curvature-based-Dropout.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CurvDrop:+A+Ricci+Curvature+Based+Approach+to+Prevent+Graph+Neural+Networks+from+Over-Smoothing+and+Over-Squashing)|0|
|[A Post-Training Framework for Improving Heterogeneous Graph Neural Networks](https://doi.org/10.1145/3543507.3583282)|Cheng Yang, Xumeng Gong, Chuan Shi, Philip S. Yu|Beijing University of Posts and Telecommunications, China; UNIVERSITY OF ILLINOIS AT CHICAGO, USA|Recent years have witnessed the success of heterogeneous graph neural networks (HGNNs) in modeling heterogeneous information networks (HINs). In this paper, we focus on the benchmark task of HGNNs, i.e., node classification, and empirically find that typical HGNNs are not good at predicting the label of a test node whose receptive field (1) has few training nodes from the same category or (2) has multiple training nodes from different categories. A possible explanation is that their message passing mechanisms may involve noises from different categories, and cannot fully explore task-specific knowledge such as the label dependency between distant nodes. Therefore, instead of introducing a new HGNN model, we propose a general post-training framework that can be applied on any pretrained HGNNs to further inject task-specific knowledge and enhance their prediction performance. Specifically, we first design an auxiliary system that estimates node labels based on (1) a global inference module of multi-channel label propagation and (2) a local inference module of network schema-aware prediction. The mechanism of our auxiliary system can complement the pretrained HGNNs by providing extra task-specific knowledge. During the post-training process, we will strengthen both system-level and module-level consistencies to encourage the cooperation between a pretrained HGNN and our auxiliary system. In this way, both systems can learn from each other for better performance. In experiments, we apply our framework to four typical HGNNs. Experimental results on three benchmark datasets show that compared with pretrained HGNNs, our post-training framework can enhance Micro-F1 by a relative improvement of 3.9% on average. Code, data and appendix are available at https://github.com/GXM1141/HGPF.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Post-Training+Framework+for+Improving+Heterogeneous+Graph+Neural+Networks)|0|
|[Link Prediction on Latent Heterogeneous Graphs](https://doi.org/10.1145/3543507.3583284)|TrungKien Nguyen, Zemin Liu, Yuan Fang|School of Computing & Information Systems, Singapore Management University, Singapore; School of Computing, National University of Singapore, Singapore|On graph data, the multitude of node or edge types gives rise to heterogeneous information networks (HINs). To preserve the heterogeneous semantics on HINs, the rich node/edge types become a cornerstone of HIN representation learning. However, in real-world scenarios, type information is often noisy, missing or inaccessible. Assuming no type information is given, we define a so-called latent heterogeneous graph (LHG), which carries latent heterogeneous semantics as the node/edge types cannot be observed. In this paper, we study the challenging and unexplored problem of link prediction on an LHG. As existing approaches depend heavily on type-based information, they are suboptimal or even inapplicable on LHGs. To address the absence of type information, we propose a model named LHGNN, based on the novel idea of semantic embedding at node and path levels, to capture latent semantics on and between nodes. We further design a personalization function to modulate the heterogeneous contexts conditioned on their latent semantics w.r.t. the target node, to enable finer-grained aggregation. Finally, we conduct extensive experiments on four benchmark datasets, and demonstrate the superior performance of LHGNN.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Link+Prediction+on+Latent+Heterogeneous+Graphs)|0|
|[Predicting the Silent Majority on Graphs: Knowledge Transferable Graph Neural Network](https://doi.org/10.1145/3543507.3583287)|Wendong Bi, Bingbing Xu, Xiaoqian Sun, Li Xu, Huawei Shen, Xueqi Cheng|Institute of Computing Technology, University of Chinese Academy of Sciences, China; Institute of Computing Technology, Chinese Academy of Sciences, China|Graphs consisting of vocal nodes ("the vocal minority") and silent nodes ("the silent majority"), namely VS-Graph, are ubiquitous in the real world. The vocal nodes tend to have abundant features and labels. In contrast, silent nodes only have incomplete features and rare labels, e.g., the description and political tendency of politicians (vocal) are abundant while not for ordinary people (silent) on the twitter's social network. Predicting the silent majority remains a crucial yet challenging problem. However, most existing message-passing based GNNs assume that all nodes belong to the same domain, without considering the missing features and distribution-shift between domains, leading to poor ability to deal with VS-Graph. To combat the above challenges, we propose Knowledge Transferable Graph Neural Network (KT-GNN), which models distribution shifts during message passing and representation learning by transferring knowledge from vocal nodes to silent nodes. Specifically, we design the domain-adapted "feature completion and message passing mechanism" for node representation learning while preserving domain difference. And a knowledge transferable classifier based on KL-divergence is followed. Comprehensive experiments on real-world scenarios (i.e., company financial risk assessment and political elections) demonstrate the superior performance of our method. Our source code has been open sourced.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Predicting+the+Silent+Majority+on+Graphs:+Knowledge+Transferable+Graph+Neural+Network)|0|
|[Automated Spatio-Temporal Graph Contrastive Learning](https://doi.org/10.1145/3543507.3583304)|Qianru Zhang, Chao Huang, Lianghao Xia, Zheng Wang, Zhonghang Li, SiuMing Yiu|South China University of Technology, China; Huawei Singapore Research Center, Singapore; The University of Hong Kong, Hong Kong|Among various region embedding methods, graph-based region relation learning models stand out, owing to their strong structure representation ability for encoding spatial correlations with graph neural networks. Despite their effectiveness, several key challenges have not been well addressed in existing methods: i) Data noise and missing are ubiquitous in many spatio-temporal scenarios due to a variety of factors. ii) Input spatio-temporal data (e.g., mobility traces) usually exhibits distribution heterogeneity across space and time. In such cases, current methods are vulnerable to the quality of the generated region graphs, which may lead to suboptimal performance. In this paper, we tackle the above challenges by exploring the Automated Spatio-Temporal graph contrastive learning paradigm (AutoST) over the heterogeneous region graph generated from multi-view data sources. Our \model\ framework is built upon a heterogeneous graph neural architecture to capture the multi-view region dependencies with respect to POI semantics, mobility flow patterns and geographical positions. To improve the robustness of our GNN encoder against data noise and distribution issues, we design an automated spatio-temporal augmentation scheme with a parameterized contrastive view generator. AutoST can adapt to the spatio-temporal heterogeneous graph with multi-view semantics well preserved. Extensive experiments for three downstream spatio-temporal mining tasks on several real-world datasets demonstrate the significant performance gain achieved by our \model\ over a variety of baselines. The code is publicly available at https://github.com/HKUDS/AutoST.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automated+Spatio-Temporal+Graph+Contrastive+Learning)|0|
|[Robust Mid-Pass Filtering Graph Convolutional Networks](https://doi.org/10.1145/3543507.3583335)|Jincheng Huang, Lun Du, Xu Chen, Qiang Fu, Shi Han, Dongmei Zhang|Microsoft Research Asia, China; School of Computer Science, Southwest Petroleum University, China; Microsoft Research Aisa, China|Graph convolutional networks (GCNs) are currently the most promising paradigm for dealing with graph-structure data, while recent studies have also shown that GCNs are vulnerable to adversarial attacks. Thus developing GCN models that are robust to such attacks become a hot research topic. However, the structural purification learning-based or robustness constraints-based defense GCN methods are usually designed for specific data or attacks, and introduce additional objective that is not for classification. Extra training overhead is also required in their design. To address these challenges, we conduct in-depth explorations on mid-frequency signals on graphs and propose a simple yet effective Mid-pass filter GCN (Mid-GCN). Theoretical analyses guarantee the robustness of signals through the mid-pass filter, and we also shed light on the properties of different frequency signals under adversarial attacks. Extensive experiments on six benchmark graph data further verify the effectiveness of our designed Mid-GCN in node classification accuracy compared to state-of-the-art GCNs under various adversarial attack strategies.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Mid-Pass+Filtering+Graph+Convolutional+Networks)|0|
|[PARROT: Position-Aware Regularized Optimal Transport for Network Alignment](https://doi.org/10.1145/3543507.3583357)|Zhichen Zeng, Si Zhang, Yinglong Xia, Hanghang Tong|Computer Science, University of Illinois at Urbana-Champaign, USA; Meta, USA; University of Illinois at Urbana-Champaign, USA|Network alignment is a critical steppingstone behind a variety of multi-network mining tasks. Most of the existing methods essentially optimize a Frobenius-like distance or ranking-based loss, ignoring the underlying geometry of graph data. Optimal transport (OT), together with Wasserstein distance, has emerged to be a powerful approach accounting for the underlying geometry explicitly. Promising as it might be, the state-of-the-art OT-based alignment methods suffer from two fundamental limitations, including (1) effectiveness due to the insufficient use of topology and consistency information and (2) scalability due to the non-convex formulation and repeated computationally costly loss calculation. In this paper, we propose a position-aware regularized optimal transport framework for network alignment named PARROT. To tackle the effectiveness issue, the proposed PARROT captures topology information by random walk with restart, with three carefully designed consistency regularization terms. To tackle the scalability issue, the regularized OT problem is decomposed into a series of convex subproblems and can be efficiently solved by the proposed constrained proximal point method with guaranteed convergence. Extensive experiments show that our algorithm achieves significant improvements in both effectiveness and scalability, outperforming the state-of-the-art network alignment methods and speeding up existing OT-based methods by up to 100 times.|网络对齐是多种网络挖掘任务背后的关键步骤。现有的大多数方法基本上都是优化一个 Frobenius 式的距离或基于排序的损失，而忽略了图形数据的基本几何形状。最优运输(OT) ，加上 Wasserstein 距离，已经成为一个强大的方法，明确地解释了基础的几何学。尽管它可能是有希望的，但是最先进的基于 OT 的比对方法受到两个基本限制，包括(1)由于拓扑和一致性信息使用不足而导致的有效性和(2)由于非凸公式和重复计算代价高昂的损失计算而导致的可伸缩性。本文提出了一种基于位置感知的网络对准正则化最优传输框架 PARROT。为了解决有效性问题，提出的 PARROT 通过重新启动的随机游动捕获拓扑信息，并使用三个精心设计的一致性正则化项。为了解决可扩展性问题，将正则 OT 问题分解为一系列凸子问题，并利用所提出的具有保证收敛性的约束近似点方法进行有效求解。大量的实验表明，我们的算法在有效性和可扩展性方面都取得了显著的改进，性能优于最先进的网络对齐方法，并将现有的基于 OT 的方法提高了100倍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PARROT:+Position-Aware+Regularized+Optimal+Transport+for+Network+Alignment)|0|
|[Label Information Enhanced Fraud Detection against Low Homophily in Graphs](https://doi.org/10.1145/3543507.3583373)|Yuchen Wang, Jinghui Zhang, Zhengjie Huang, Weibin Li, Shikun Feng, Ziheng Ma, Yu Sun, Dianhai Yu, Fang Dong, Jiahui Jin, Beilun Wang, Junzhou Luo|Southeast University, China; Baidu Inc., China|Node classification is a substantial problem in graph-based fraud detection. Many existing works adopt Graph Neural Networks (GNNs) to enhance fraud detectors. While promising, currently most GNN-based fraud detectors fail to generalize to the low homophily setting. Besides, label utilization has been proved to be significant factor for node classification problem. But we find they are less effective in fraud detection tasks due to the low homophily in graphs. In this work, we propose GAGA, a novel Group AGgregation enhanced TrAnsformer, to tackle the above challenges. Specifically, the group aggregation provides a portable method to cope with the low homophily issue. Such an aggregation explicitly integrates the label information to generate distinguishable neighborhood information. Along with group aggregation, an attempt towards end-to-end trainable group encoding is proposed which augments the original feature space with the class labels. Meanwhile, we devise two additional learnable encodings to recognize the structural and relational context. Then, we combine the group aggregation and the learnable encodings into a Transformer encoder to capture the semantic information. Experimental results clearly show that GAGA outperforms other competitive graph-based fraud detectors by up to 24.39% on two trending public datasets and a real-world industrial dataset from Anonymous. Even more, the group aggregation is demonstrated to outperform other label utilization methods (e.g., C&S, BoT/UniMP) in the low homophily setting.|节点分类是基于图的欺诈检测中的一个重要问题。现有的许多工作都采用图神经网络(GNN)来增强欺诈检测器。虽然有希望，目前大多数基于 GNN 的欺诈检测器不能泛化到低同调设置。此外，标签利用率也被证明是解决节点分类问题的重要因素。但是我们发现，由于图的同调性较低，它们在欺诈检测任务中的效率较低。在这项工作中，我们提出了 GAGA，一个新颖的群集增强的转换器，以解决上述挑战。具体来说，组聚合提供了一种可移植的方法来处理低同质性问题。这种聚合显式地集成标签信息以生成可区分的邻域信息。在分组聚合的基础上，提出了一种端到端的可训练分组编码方法，该方法通过类标签来增加原始特征空间。同时，我们设计了两个额外的可学习编码来识别结构和关系语境。然后，我们将分组聚合和可学习的编码结合到一个跑车编码器中来捕捉语义信息。实验结果清楚地表明，GAGA 在两个趋势公共数据集和一个来自 Anonymous 的真实世界工业数据集上的表现优于其他竞争性的基于图的欺诈检测器高达24.39% 。更重要的是，在低同质性设置中，组聚合被证明优于其他标签利用方法(例如 C & S、 BoT/UniMP)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Label+Information+Enhanced+Fraud+Detection+against+Low+Homophily+in+Graphs)|0|
|[An Attentional Multi-scale Co-evolving Model for Dynamic Link Prediction](https://doi.org/10.1145/3543507.3583396)|Guozhen Zhang, Tian Ye, Depeng Jin, Yong Li|Tsinghua University, China; Department of Electronic Engineering, Tsinghua University, China|Dynamic link prediction is essential for a wide range of domains, including social networks, bioinformatics, knowledge bases, and recommender systems. Existing works have demonstrated that structural information and temporal information are two of the most important information for this problem. However, existing works either focus on modeling them independently or modeling the temporal dynamics of a single structural scale, neglecting the complex correlations among them. This paper proposes to model the inherent correlations among the evolving dynamics of different structural scales for dynamic link prediction. Following this idea, we propose an Attentional Multi-scale Co-evolving Network (AMCNet). Specifically, We model multi-scale structural information by a motif-based graph neural network with multi-scale pooling. Then, we design a hierarchical attention-based sequence-to-sequence model for learning the complex correlations among the evolution dynamics of different structural scales. Extensive experiments on four real-world datasets with different characteristics demonstrate that AMCNet significantly outperforms the state-of-the-art in both single-step and multi-step dynamic link prediction tasks.|动态链接预测对于广泛的领域是必不可少的，包括社会网络、生物信息学、知识库和推荐系统。已有的研究表明，结构信息和时间信息是解决这一问题的两个重要信息。然而，现有的研究要么侧重于对它们进行独立的建模，要么侧重于对单一结构尺度的时间动力学进行建模，而忽视了它们之间复杂的相关性。本文提出建立不同结构尺度演化动力学之间的内在相关性模型，用于动态链接预测。根据这一思想，我们提出了一个注意力多尺度协同演化网络(AMCNet)。具体地说，我们利用基于模序的多尺度混合图神经网络对多尺度结构信息进行建模。然后，我们设计了一个分层的基于注意的序列-序列模型来学习不同结构尺度的进化动力学之间的复杂相关性。在四个不同特征的实际数据集上进行的大量实验表明，AMCNet 在单步和多步动态链接预测任务中的性能都明显优于最新技术。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Attentional+Multi-scale+Co-evolving+Model+for+Dynamic+Link+Prediction)|0|
|[Robust Graph Representation Learning for Local Corruption Recovery](https://doi.org/10.1145/3543507.3583399)|Bingxin Zhou, Yuanhong Jiang, Yuguang Wang, Jingwei Liang, Junbin Gao, Shirui Pan, Xiaoqun Zhang|The University of Sydney, Australia; Shanghai Jiao Tong University, China and The University of Sydney, Australia; Shanghai Jiao Tong University, China; Griffith University, Australia; Shanghai Jiao Tong University, China and Shanghai Artificial Intelligence Laboratory, China|The performance of graph representation learning is affected by the quality of graph input. While existing research usually pursues a globally smoothed graph embedding, we believe the rarely observed anomalies are as well harmful to an accurate prediction. This work establishes a graph learning scheme that automatically detects (locally) corrupted feature attributes and recovers robust embedding for prediction tasks. The detection operation leverages a graph autoencoder, which does not make any assumptions about the distribution of the local corruptions. It pinpoints the positions of the anomalous node attributes in an unbiased mask matrix, where robust estimations are recovered with sparsity promoting regularizer. The optimizer approaches a new embedding that is sparse in the framelet domain and conditionally close to input observations. Extensive experiments are provided to validate our proposed model can recover a robust graph representation from black-box poisoning and achieve excellent performance.|图形表示学习的性能受到图形输入质量的影响。虽然现有的研究通常追求全局光滑图嵌入，我们认为很少观察到的异常是有害的，以及准确的预测。本文建立了一个图形学习方案，该方案能够自动检测(局部)被破坏的特征属性并恢复预测任务的鲁棒嵌入。检测操作利用图形自动编码器，该编码器不对本地腐败的分布做任何假设。该算法在无偏掩码矩阵中精确定位异常节点属性，利用稀疏增强正则化器恢复鲁棒估计。该优化器能够实现一种新的嵌入算法，该算法在帧小波域中是稀疏的，并且有条件地接近输入观测值。通过大量的实验验证了该模型能够从黑盒中毒中恢复出一个鲁棒的图表示，并取得了良好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Graph+Representation+Learning+for+Local+Corruption+Recovery)|0|
|[Hyperbolic Geometric Graph Representation Learning for Hierarchy-imbalance Node Classification](https://doi.org/10.1145/3543507.3583403)|Xingcheng Fu, Yuecen Wei, Qingyun Sun, Haonan Yuan, Jia Wu, Hao Peng, Jianxin Li|Guangxi Key Lab of Multi-source Information Mining Security, Guangxi Normal University, China; School of Computer Science and Engineering, Beihang University, China; School of Cyber Science and Technology, Beihang University, China; School of Computing, Macquarie University, Australia; School of Computer Science and Engineering, Beihang University, China and Zhongguancun Lab, China|Learning unbiased node representations for imbalanced samples in the graph has become a more remarkable and important topic. For the graph, a significant challenge is that the topological properties of the nodes (e.g., locations, roles) are unbalanced (topology-imbalance), other than the number of training labeled nodes (quantity-imbalance). Existing studies on topology-imbalance focus on the location or the local neighborhood structure of nodes, ignoring the global underlying hierarchical properties of the graph, i.e., hierarchy. In the real-world scenario, the hierarchical structure of graph data reveals important topological properties of graphs and is relevant to a wide range of applications. We find that training labeled nodes with different hierarchical properties have a significant impact on the node classification tasks and confirm it in our experiments. It is well known that hyperbolic geometry has a unique advantage in representing the hierarchical structure of graphs. Therefore, we attempt to explore the hierarchy-imbalance issue for node classification of graph neural networks with a novelty perspective of hyperbolic geometry, including its characteristics and causes. Then, we propose a novel hyperbolic geometric hierarchy-imbalance learning framework, named HyperIMBA, to alleviate the hierarchy-imbalance issue caused by uneven hierarchy-levels and cross-hierarchy connectivity patterns of labeled nodes.Extensive experimental results demonstrate the superior effectiveness of HyperIMBA for hierarchy-imbalance node classification tasks.|学习图中不平衡样本的无偏节点表示已经成为一个非常引人注目和重要的课题。对于图，一个重大的挑战是，除了训练标记节点的数量(数量不平衡)之外，节点的拓扑属性(例如，位置，角色)是不平衡的(拓扑不平衡)。现有关于拓扑不平衡的研究主要集中在节点的位置或局部邻域结构上，忽略了图的全局潜在层次性质，即层次结构。在真实场景中，图数据的层次结构揭示了图的重要拓扑性质，并且与广泛的应用相关。我们发现训练具有不同层次属性的标记节点对节点分类任务有显著的影响，并在实验中得到了验证。众所周知，双曲几何在表示图形的层次结构方面具有独特的优势。因此，我们尝试从新颖的双曲几何角度探讨图形神经网络节点分类的层次不平衡问题，包括其特点和成因。在此基础上，提出了一种新的双曲型几何层次-不平衡学习框架 HyperIMBA，以解决标记节点层次不均匀和跨层次连通模式引起的层次-不平衡问题。大量的实验结果表明，HyperIMBA 对层次不平衡节点分类任务具有较好的效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hyperbolic+Geometric+Graph+Representation+Learning+for+Hierarchy-imbalance+Node+Classification)|0|
|[Graph Neural Networks without Propagation](https://doi.org/10.1145/3543507.3583419)|Liang Yang, Qiuliang Zhang, Runjie Shi, Wenmiao Zhou, Bingxin Niu, Chuan Wang, Xiaochun Cao, Dongxiao He, Zhen Wang, Yuanfang Guo|Hebei University of Technology, China; Sun Yat-sen University, China; Beihang University, China; Northwestern Polytechnical University, China; Chinese Academy of Sciences, China; Tianjin University, China|Due to the simplicity, intuition and explanation, most Graph Neural Networks (GNNs) are proposed by following the pipeline of message passing. Although they achieve superior performances in many tasks, propagation-based GNNs possess three essential drawbacks. Firstly, the propagation tends to produce smooth effect, which meets the inductive bias of homophily, and causes two serious issues: over-smoothing issue and performance drop on networks with heterophily. Secondly, the propagations to each node are irrelevant, which prevents GNNs from modeling high-order relation, and cause the GNNs fragile to the attributes noises. Thirdly, propagation-based GNNs may be fragile to topology noise, since they heavily relay on propagation over the topology. Therefore, the propagation, as the key component of most GNNs, may be the essence of some serious issues in GNNs. To get to the root of these issue, this paper attempts to replace the propagation with a novel local operation. Quantitative experimental analysis reveals: 1) the existence of low-rank characteristic in the node attributes from ego-networks and 2) the performance improvement by reducing its rank. Motivated by this finding, this paper propose the Low-Rank GNNs, whose key component is the low-rank attribute matrix approximation in ego-network. The graph topology is employed to construct the ego-networks instead of message propagation, which is sensitive to topology noises. The proposed Low-Rank GNNs posses some attractive characteristics, including robust to topology and attribute noises, parameter-free and parallelizable. Experimental evaluations demonstrate the superior performance, robustness to noises and universality of the proposed Low-Rank GNNs.|由于图神经网络的简单性、直观性和解释性，大多数图神经网络都是通过跟踪消息传递流水线来实现的。尽管基于传播的 GNN 在许多任务中取得了优越的性能，但是它有三个基本的缺点。首先，传播趋向于产生平滑效应，这满足了异质网络的归纳偏差，导致了两个严重的问题: 过平滑问题和异质网络的性能下降。其次，对每个节点的传播是不相关的，这阻碍了 GNN 对高阶关系的建模，使得 GNN 对属性噪声非常脆弱。第三，基于传播的 GNN 很容易受到拓扑噪声的影响，因为它们严重依赖于拓扑上的传播。因此，传播作为大多数 GNN 的关键组成部分，可能是 GNN 中一些严重问题的实质。为了找到这些问题的根源，本文试图用一种新的局部操作来代替传播。定量实验分析表明: 1)自我网络节点属性存在低级特征，2)通过降低自我网络节点属性的排名可以提高节点的性能。基于这一发现，本文提出了低秩 GNN，其核心部分是自我网络中的低秩属性矩阵逼近。利用图拓扑结构代替对拓扑噪声敏感的消息传播，构造自我网络。所提出的低阶 GNN 具有对拓扑和属性噪声的鲁棒性、无参数性和可并行性等特点。实验结果表明，所提出的低阶 GNN 具有良好的性能、抗噪声能力和通用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Neural+Networks+without+Propagation)|0|
|[Self-Supervised Teaching and Learning of Representations on Graphs](https://doi.org/10.1145/3543507.3583441)|Liangtian Wan, Zhenqiang Fu, Lu Sun, Xianpeng Wang, Gang Xu, Xiaoran Yan, Feng Xia|Department of Communication Engineering, Institute of Information Science Technology, Dalian Maritime University, China; State Key Laboratory of Millimeter Waves, School of Information Science and Engineering, Southeast University, China; Research Center of Big Data Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, China; School of Computing Technologies, RMIT University, Australia; State Key Laboratory of Marine Resource Utilization in South China Sea, School of Information and Communication Engineering, Hainan University, China|Recent years have witnessed significant advances in graph contrastive learning (GCL), while most GCL models use graph neural networks as encoders based on supervised learning. In this work, we propose a novel graph learning model called GraphTL, which explores self-supervised teaching and learning of representations on graphs. One critical objective of GCL is to retain original graph information. For this purpose, we design an encoder based on the idea of unsupervised dimensionality reduction of locally linear embedding (LLE). Specifically, we map one iteration of the LLE to one layer of the network. To guide the encoder to better retain the original graph information, we propose an unbalanced contrastive model consisting of two views, which are the learning view and the teaching view, respectively. Furthermore, we consider the nodes that are identical in muti-views as positive node pairs, and design the node similarity scorer so that the model can select positive samples of a target node. Extensive experiments have been conducted over multiple datasets to evaluate the performance of GraphTL in comparison with baseline models. Results demonstrate that GraphTL can reduce distances between similar nodes while preserving network topological and feature information, yielding better performance in node classification.|近年来，图形对比学习已经取得了显著的进展，而大多数图形对比学习模型使用图形神经网络作为基于监督式学习的编码器。在这项工作中，我们提出了一个新的图形学习模型称为 GraphTL，它探索自我监督的教学和图表示的学习。GCL 的一个关键目标是保留原始的图形信息。为此，我们设计了一个基于局部线性嵌入(lLE)无监督降维思想的编码器。具体来说，我们将 LLE 的一个迭代映射到网络的一个层。为了指导编码器更好地保留原始图形信息，本文提出了一种由学习视图和教学视图组成的非平衡对比模型。此外，我们将多视图中相同的节点视为正节点对，并设计了节点相似度计分器，使模型能够选择目标节点的正样本。在多个数据集上进行了广泛的实验，以评估 GraphTL 与基线模型相比的性能。结果表明，GraphTL 在保留网络拓扑和特征信息的同时，可以减少相似节点之间的距离，在节点分类方面取得了较好的效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Supervised+Teaching+and+Learning+of+Representations+on+Graphs)|0|
|[SE-GSL: A General and Effective Graph Structure Learning Framework through Structural Entropy Optimization](https://doi.org/10.1145/3543507.3583453)|Dongcheng Zou, Hao Peng, Xiang Huang, Renyu Yang, Jianxin Li, Jia Wu, Chunyang Liu, Philip S. Yu|Beihang University, China; University of Illinois Chicago, USA; Didi Chuxing, China; Macquarie University, Australia|Graph Neural Networks (GNNs) are de facto solutions to structural data learning. However, it is susceptible to low-quality and unreliable structure, which has been a norm rather than an exception in real-world graphs. Existing graph structure learning (GSL) frameworks still lack robustness and interpretability. This paper proposes a general GSL framework, SE-GSL, through structural entropy and the graph hierarchy abstracted in the encoding tree. Particularly, we exploit the one-dimensional structural entropy to maximize embedded information content when auxiliary neighbourhood attributes are fused to enhance the original graph. A new scheme of constructing optimal encoding trees is proposed to minimize the uncertainty and noises in the graph whilst assuring proper community partition in hierarchical abstraction. We present a novel sample-based mechanism for restoring the graph structure via node structural entropy distribution. It increases the connectivity among nodes with larger uncertainty in lower-level communities. SE-GSL is compatible with various GNN models and enhances the robustness towards noisy and heterophily structures. Extensive experiments show significant improvements in the effectiveness and robustness of structure learning and node representation learning.|图神经网络(GNN)是结构数据学习的实际解决方案。但是，它容易受到低质量和不可靠结构的影响，这在现实世界的图形中已经成为一种规范而不是例外。现有的图结构学习(GSL)框架仍然缺乏健壮性和可解释性。本文通过结构熵和编码树中抽象出的图层次结构，提出了一个通用的 GSL 框架 SE-GSL。特别地，当融合辅助邻域属性来增强原始图时，我们利用一维结构熵来最大化嵌入信息的内容。提出了一种新的构造最优编码树的方法，以最小化图中的不确定性和噪声，同时在分层抽象中保证适当的社区划分。提出了一种基于样本的节点结构熵分布恢复图结构的新机制。它增加了低层社区中具有较大不确定性的节点之间的连通性。SE-GSL 与各种 GNN 模型兼容，增强了对噪声和异质结构的鲁棒性。广泛的实验表明，结构学习和节点表示学习的有效性和鲁棒性有了显著的提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SE-GSL:+A+General+and+Effective+Graph+Structure+Learning+Framework+through+Structural+Entropy+Optimization)|0|
|[Homophily-oriented Heterogeneous Graph Rewiring](https://doi.org/10.1145/3543507.3583454)|Jiayan Guo, Lun Du, Wendong Bi, Qiang Fu, Xiaojun Ma, Xu Chen, Shi Han, Dongmei Zhang, Yan Zhang|Microsoft Research Asia, China; School of Intelligence Science and Technology, Peking University, China; Institute of Computing Technology, Chinese Academy of Sciences, China|With the rapid development of the World Wide Web (WWW), heterogeneous graphs (HG) have explosive growth. Recently, heterogeneous graph neural network (HGNN) has shown great potential in learning on HG. Current studies of HGNN mainly focus on some HGs with strong homophily properties (nodes connected by meta-path tend to have the same labels), while few discussions are made in those that are less homophilous. Recently, there have been many works on homogeneous graphs with heterophily. However, due to heterogeneity, it is non-trivial to extend their approach to deal with HGs with heterophily. In this work, based on empirical observations, we propose a meta-path-induced metric to measure the homophily degree of a HG. We also find that current HGNNs may have degenerated performance when handling HGs with less homophilous properties. Thus it is essential to increase the generalization ability of HGNNs on non-homophilous HGs. To this end, we propose HDHGR, a homophily-oriented deep heterogeneous graph rewiring approach that modifies the HG structure to increase the performance of HGNN. We theoretically verify HDHGR. In addition, experiments on real-world HGs demonstrate the effectiveness of HDHGR, which brings at most more than 10% relative gain.|随着万维网(WWW)的迅速发展，异构图形(HG)呈现出爆炸性的增长。近年来，异构图形神经网络(HGNN)在 HG 学习中显示出巨大的潜力。目前对 HGNN 的研究主要集中在一些具有强同质性的 HG (通过元路径连接的节点往往具有相同的标签) ，而对于那些不同质性的 HG 的研究很少。近年来，关于具有异拓的齐图的研究已经有了大量的工作。然而，由于异质性的原因，扩展它们的方法来处理具有异质性的 HGs 是非常重要的。在这项工作中，基于经验观察，我们提出了一个元路径诱导度量测量 HG 的同调度。我们还发现，当处理具有较少同质性的 HG 时，当前的 HGNN 可能具有退化的性能。因此，提高 HGNN 在非同源 HGs 上的泛化能力至关重要。为此，我们提出了一种面向同构的深度异构图重布线方法 HDHGR，该方法通过修改 HG 结构来提高 HGNN 的性能。我们理论上验证了 HDHGR。此外，在现实世界中进行的汞实验证明了 HDHGR 的有效性，它最多带来超过10% 的相对收益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Homophily-oriented+Heterogeneous+Graph+Rewiring)|0|
|[HGWaveNet: A Hyperbolic Graph Neural Network for Temporal Link Prediction](https://doi.org/10.1145/3543507.3583455)|Qijie Bai, Changli Nie, Haiwei Zhang, Dongming Zhao, Xiaojie Yuan|College of CS, TJ Key Lab of NDST, Nankai University, China; China Mobile Communication Group Tianjin Co., Ltd, China|Temporal link prediction, aiming to predict future edges between paired nodes in a dynamic graph, is of vital importance in diverse applications. However, existing methods are mainly built upon uniform Euclidean space, which has been found to be conflict with the power-law distributions of real-world graphs and unable to represent the hierarchical connections between nodes effectively. With respect to the special data characteristic, hyperbolic geometry offers an ideal alternative due to its exponential expansion property. In this paper, we propose HGWaveNet, a novel hyperbolic graph neural network that fully exploits the fitness between hyperbolic spaces and data distributions for temporal link prediction. Specifically, we design two key modules to learn the spatial topological structures and temporal evolutionary information separately. On the one hand, a hyperbolic diffusion graph convolution (HDGC) module effectively aggregates information from a wider range of neighbors. On the other hand, the internal order of causal correlation between historical states is captured by hyperbolic dilated causal convolution (HDCC) modules. The whole model is built upon the hyperbolic spaces to preserve the hierarchical structural information in the entire data flow. To prove the superiority of HGWaveNet, extensive experiments are conducted on six real-world graph datasets and the results show a relative improvement by up to 6.67% on AUC for temporal link prediction over SOTA methods.|时间链路预测，旨在预测动态图中成对节点之间的未来边缘，在不同的应用中具有重要意义。然而，现有的方法主要建立在统一的欧氏空间上，这与现实世界图的幂律分布相冲突，不能有效地表示节点之间的层次关系。关于特殊的数据特性，双曲几何提供了一个理想的选择，由于其指数膨胀性质。本文提出了一种新的双曲型图形神经网络 HGWave Net，它充分利用双曲型空间和数据分布之间的适应性来进行时间链路预测。具体来说，我们设计了两个关键模块来分别学习空间拓扑结构和时间进化信息。一方面，双曲扩散图卷积(HDGC)模块有效地聚集了来自更广泛的邻居的信息。另一方面，历史状态之间因果关系的内部顺序被双曲扩张因果卷积(HDCC)模块所捕获。整个模型建立在双曲空间的基础上，保留了整个数据流中的层次结构信息。为了证明 HGWaveNet 的优越性，在6个实际图形数据集上进行了广泛的实验，结果表明，与 SOTA 方法相比，时间链路预测的 AUC 相对提高了6.67% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HGWaveNet:+A+Hyperbolic+Graph+Neural+Network+for+Temporal+Link+Prediction)|0|
|[Rethinking Structural Encodings: Adaptive Graph Transformer for Node Classification Task](https://doi.org/10.1145/3543507.3583464)|Xiaojun Ma, Qin Chen, Yi Wu, Guojie Song, Liang Wang, Bo Zheng|Alibaba Group, China; Peking University, China; Microsoft, China|Graph Transformers have proved their advantages in graph data mining with elaborate Positional Encodings, especially in graph-level tasks. However, their application in the node classification task has not been fully exploited yet. In the node classification task, existing Graph Transformers with Positional Encodings are limited by the following issues: (i) PEs describing the node’s positional identities are insufficient for the node classification task on complex graphs, where a full portrayal of the local node property is needed. (ii) PEs for graphs are integrated with Transformers in a constant schema, resulting in the ignorance of local patterns that may vary among different nodes. In this paper, we propose Adaptive Graph Transformer (AGT) to tackle above issues. AGT consists of a Learnable Centrality Encoding and a Kernelized Local Structure Encoding. The two modules extract structural patterns from centrality and subgraph views in a learnable and scalable manner. Further, we design the Adaptive Transformer Block to adaptively integrate the attention scores and Structural Encodings in a node-specific manner. AGT achieves state-of-the-art performances on nine real-world web graphs (up to 1.6 million nodes). Furthermore, AGT shows outstanding results on two series of synthetic graphs with ranges of heterophily and noise ratios.|图形变换器通过精细的位置编码，尤其是在图级任务中，证明了它在图数据挖掘中的优势。但是，它们在节点分类任务中的应用还没有得到充分利用。在节点分类任务中，现有的带有位置编码的图形变换器受到以下问题的限制: (i)描述节点位置标识的 PE 不足以完成复杂图形上的节点分类任务，这需要对本地节点属性进行全面描述。(ii)图形的 PE 与变压器在一个固定的模式下集成，导致对不同节点之间可能变化的局部模式的忽视。本文提出了自适应图形变换器(AGT)来解决上述问题。AGT 由可学习中心性编码和核化局部结构编码两部分组成。这两个模块以一种可学习和可扩展的方式从中心性和子图视图中提取结构模式。进一步，我们设计了自适应变压器块，以节点特定的方式自适应地整合注意分数和结构编码。AGT 在9个真实世界的网络图表(多达160万个节点)上实现了最先进的性能。此外，AGT 在两组具有异质性和噪声比范围的合成图上显示了优异的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Structural+Encodings:+Adaptive+Graph+Transformer+for+Node+Classification+Task)|0|
|[Federated Node Classification over Graphs with Latent Link-type Heterogeneity](https://doi.org/10.1145/3543507.3583471)|Han Xie, Li Xiong, Carl Yang|Emory University, USA|Federated learning (FL) aims to train powerful and generalized global models without putting distributed data together, which has been shown effective in various domains of machine learning. The non-IIDness of data across local clients has been a major challenge for FL. In graphs, one specifically important perspective of non-IIDness is manifested in the link-type heterogeneity underlying homogeneous graphs– the seemingly uniform links captured in most real-world networks can carry different levels of homophily or semantics of relations, while the exact sets and distributions of such latent link-types can further differ across local clients. Through our preliminary data analysis, we are motivated to design a new graph FL framework that can simultaneously discover latent link-types and model message-passing w.r.t. the discovered link-types through the collaboration of distributed local clients. Specifically, we propose a framework FedLit that can dynamically detect the latent link-types during FL via an EM-based clustering algorithm and differentiate the message-passing through different types of links via multiple convolution channels. For experiments, we synthesize multiple realistic datasets of graphs with latent heterogeneous link-types from real-world data, and partition them with different levels of link-type heterogeneity. Comprehensive experimental results and in-depth analysis have demonstrated both superior performance and rational behaviors of our proposed techniques.|联邦学习(FL)旨在不将分布式数据放在一起而训练强大的广义全局模型，这已经在机器学习的各个领域中被证明是有效的。跨本地客户端的数据的非 IID 性一直是 FL 面临的主要挑战。在图中，非 IIDness 的一个特别重要的视角表现在同质图的链接类型异质性中——在大多数现实世界网络中捕获的看似统一的链接可以携带不同层次的同质性或关系语义，而这种潜在链接类型的确切集合和分布可以在本地客户端之间进一步不同。通过初步的数据分析，设计了一个新的图形 FL 框架，该框架可以通过分布式本地客户机的协作，同时发现潜在的链接类型和模型消息传递发现的链接类型。具体来说，我们提出了一个框架 FedLit，它可以通过一个基于 EM 的聚类算法动态检测 FL 中的潜在链接类型，并通过多个卷积通道区分不同类型的链接传递信息。对于实验，我们从现实数据中合成了多个具有潜在异构链接类型的图的现实数据集，并将它们按照不同的链接类型异构程度进行划分。综合实验结果和深入分析表明，我们提出的技术具有优越的性能和合理的行为。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Federated+Node+Classification+over+Graphs+with+Latent+Link-type+Heterogeneity)|0|
|[CMINet: a Graph Learning Framework for Content-aware Multi-channel Influence Diffusion](https://doi.org/10.1145/3543507.3583465)|HsiWen Chen, DeNian Yang, WangChien Lee, Philip S. Yu, MingSyan Chen|National Taiwan University, Taiwan; Academia Sinica, Taiwan; National Taiwan University, Taiwan and Academia Sinica, Taiwan; University of Illinois Chicago, USA; Pennsylvania State University, USA|The phenomena of influence diffusion on social networks have received tremendous research interests in the past decade. While most prior works mainly focus on predicting the total influence spread on a single network, a marketing campaign that exploits influence diffusion often involves multiple channels with various information disseminated on different media. In this paper, we introduce a new influence estimation problem, namely Content-aware Multi-channel Influence Diffusion (CMID), and accordingly propose CMINet to predict newly influenced users, given a set of seed users with different multimedia contents. In CMINet, we first introduce DiffGNN to encode the influencing power of users (nodes) and Influence-aware Optimal Transport (IOT) to align the embeddings to address the distribution shift across different diffusion channels. Then, we transform CMID into a node classification problem and propose Social-based Multimedia Feature Extractor (SMFE) and Content-aware Multi-channel Influence Propagation (CMIP) to jointly learn the user preferences on multimedia contents and predict the susceptibility of users. Furthermore, we prove that CMINet preserves monotonicity and submodularity, thus enabling (1 − 1/e)-approximate solutions for influence maximization. Experimental results manifest that CMINet outperforms eleven baselines on three public datasets.|近十年来，社会网络中的影响扩散现象受到了广泛的关注。虽然大多数以前的工作主要集中在预测总的影响传播在一个单一的网络，利用影响传播的营销活动往往涉及多个渠道，在不同的媒体传播各种各样的信息。本文提出了一个新的影响估计问题，即内容感知的多信道影响扩散(CMID) ，并相应地提出了 CMINet 来预测新的影响用户，给出了一组具有不同多媒体内容的种子用户。在 CMINet 中，我们首先引入区分 GNN 编码用户(节点)的影响力，以及感知影响的最优传输(IOT)来调整嵌入以解决不同扩散通道间的分布转移问题。然后，将 CMID 转化为一个节点分类问题，提出了基于社会化的多媒体特征提取器(SMFE)和内容感知的多信道影响传播(CMIP) ，以共同学习用户对多媒体内容的偏好并预测用户的易感性。此外，我们证明了 CMINet 保持了单调性和次模性，从而使(1-1/e)近似解的影响最大化。实验结果表明，CMINet 在三个公共数据集上的表现优于十一个基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CMINet:+a+Graph+Learning+Framework+for+Content-aware+Multi-channel+Influence+Diffusion)|0|
|[Auto-HeG: Automated Graph Neural Network on Heterophilic Graphs](https://doi.org/10.1145/3543507.3583498)|Xin Zheng, Miao Zhang, Chunyang Chen, Qin Zhang, Chuan Zhou, Shirui Pan|Harbin Institute of Technology (Shenzhen), China; Griffith University, Australia; Chinese Academy of Sciences, China; Monash University, Australia; Shenzhen University, China|Graph neural architecture search (NAS) has gained popularity in automatically designing powerful graph neural networks (GNNs) with relieving human efforts. However, existing graph NAS methods mainly work under the homophily assumption and overlook another important graph property, i.e., heterophily, which exists widely in various real-world applications. To date, automated heterophilic graph learning with NAS is still a research blank to be filled in. Due to the complexity and variety of heterophilic graphs, the critical challenge of heterophilic graph NAS mainly lies in developing the heterophily-specific search space and strategy. Therefore, in this paper, we propose a novel automated graph neural network on heterophilic graphs, namely Auto-HeG, to automatically build heterophilic GNN models with expressive learning abilities. Specifically, Auto-HeG incorporates heterophily into all stages of automatic heterophilic graph learning, including search space design, supernet training, and architecture selection. Through the diverse message-passing scheme with joint micro-level and macro-level designs, we first build a comprehensive heterophilic GNN search space, enabling Auto-HeG to integrate complex and various heterophily of graphs. With a progressive supernet training strategy, we dynamically shrink the initial search space according to layer-wise variation of heterophily, resulting in a compact and efficient supernet. Taking a heterophily-aware distance criterion as the guidance, we conduct heterophilic architecture selection in the leave-one-out pattern, so that specialized and expressive heterophilic GNN architectures can be derived. Extensive experiments illustrate the superiority of Auto-HeG in developing excellent heterophilic GNNs to human-designed models and graph NAS models.|图神经结构搜索(NAS)在自动设计功能强大的图神经网络(GNN)方面得到了广泛的应用。然而，现有的图 NAS 方法主要是在同伦假设下工作，忽略了在各种实际应用中广泛存在的另一个重要图性质，即异伦性质。迄今为止，利用 NAS 进行自动异质图学习仍然是一个有待填补的研究空白。由于异构图的复杂性和多样性，异构图 NAS 的关键挑战主要在于开发异构特定的搜索空间和策略。因此，本文提出了一种新的基于异质图的自动图神经网络 Auto-HeG，用于自动构建具有表达学习能力的异质 GNN 模型。具体来说，Auto-HeG 将异质性整合到自动异质性图学习的所有阶段，包括搜索空间设计、超网训练和体系结构选择。通过联合微观和宏观层次设计的多样化消息传递方案，首先构建一个全面的异构 GNN 搜索空间，使 Auto-HeG 能够集成复杂多样的异构图。采用渐进式超网训练策略，根据异质性的分层变化动态缩小初始搜索空间，形成紧凑高效的超网。以异质性感知距离标准为指导，在留一出模式下进行异质性体系结构选择，从而得到专门的、具有表现力的异质性 GNN 体系结构。大量的实验证明了 Auto-HeG 在开发优秀的异质性 GNN 方面优于人工设计的模型和图形 NAS 模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Auto-HeG:+Automated+Graph+Neural+Network+on+Heterophilic+Graphs)|0|
|[HINormer: Representation Learning On Heterogeneous Information Networks with Graph Transformer](https://doi.org/10.1145/3543507.3583493)|Qiheng Mao, Zemin Liu, Chenghao Liu, Jianling Sun|Salesforce Research Asia, Singapore; Zhejiang University, China and Alibaba-Zhejiang University Joint Institute of Frontier Technologies, China; National University of Singapore, Singapore|Recent studies have highlighted the limitations of message-passing based graph neural networks (GNNs), e.g., limited model expressiveness, over-smoothing, over-squashing, etc. To alleviate these issues, Graph Transformers (GTs) have been proposed which work in the paradigm that allows message passing to a larger coverage even across the whole graph. Hinging on the global range attention mechanism, GTs have shown a superpower for representation learning on homogeneous graphs. However, the investigation of GTs on heterogeneous information networks (HINs) is still under-exploited. In particular, on account of the existence of heterogeneity, HINs show distinct data characteristics and thus require different treatment. To bridge this gap, in this paper we investigate the representation learning on HINs with Graph Transformer, and propose a novel model named HINormer, which capitalizes on a larger-range aggregation mechanism for node representation learning. In particular, assisted by two major modules, i.e., a local structure encoder and a heterogeneous relation encoder, HINormer can capture both the structural and heterogeneous information of nodes on HINs for comprehensive node representations. We conduct extensive experiments on four HIN benchmark datasets, which demonstrate that our proposed model can outperform the state-of-the-art.|最近的研究强调了基于消息传递的图神经网络(GNN)的局限性，如模型表达能力有限、过度平滑、过度压缩等。为了缓解这些问题，已经提出了图形转换器(Graph Transformers，GTs) ，它的工作范式允许消息传递到更大的覆盖范围，甚至在整个图中。基于全局范围注意机制，GT 在齐次图表示学习中表现出了超强的表示学习能力。然而，在异构信息网络(HIN)上的 GT 研究仍然处于起步阶段。特别是，由于存在异质性，HIN 显示出不同的数据特征，因此需要不同的处理。为了弥补这一差距，本文研究了基于图形变换的 HIN 表示学习，提出了一种新的 HINormer 模型，该模型利用了一种更大范围的聚合机制来进行节点表示学习。HINormer 在局部结构编码器和异构关系编码器这两个主要模块的协助下，可以同时捕获 HIN 上节点的结构和异构信息，从而实现节点的全面表示。我们在四个 HIN 基准数据集上进行了广泛的实验，实验结果表明我们提出的模型可以胜过最先进的水平。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HINormer:+Representation+Learning+On+Heterogeneous+Information+Networks+with+Graph+Transformer)|0|
|[Minimum Topology Attacks for Graph Neural Networks](https://doi.org/10.1145/3543507.3583509)|Mengmei Zhang, Xiao Wang, Chuan Shi, Lingjuan Lyu, Tianchi Yang, Junping Du|Beijing University of Posts and Telecommunications, China; Sony AI, China|With the great popularity of Graph Neural Networks (GNNs), their robustness to adversarial topology attacks has received significant attention. Although many attack methods have been proposed, they mainly focus on fixed-budget attacks, aiming at finding the most adversarial perturbations within a fixed budget for target node. However, considering the varied robustness of each node, there is an inevitable dilemma caused by the fixed budget, i.e., no successful perturbation is found when the budget is relatively small, while if it is too large, the yielding redundant perturbations will hurt the invisibility. To break this dilemma, we propose a new type of topology attack, named minimum-budget topology attack, aiming to adaptively find the minimum perturbation sufficient for a successful attack on each node. To this end, we propose an attack model, named MiBTack, based on a dynamic projected gradient descent algorithm, which can effectively solve the involving non-convex constraint optimization on discrete topology. Extensive results on three GNNs and four real-world datasets show that MiBTack can successfully lead all target nodes misclassified with the minimum perturbation edges. Moreover, the obtained minimum budget can be used to measure node robustness, so we can explore the relationships of robustness, topology, and uncertainty for nodes, which is beyond what the current fixed-budget topology attacks can offer.|随着图形神经网络(GNN)的普及，其对抗性拓扑攻击的鲁棒性受到了广泛的关注。虽然已经提出了许多攻击方法，但它们主要针对固定预算的攻击，目的是在目标节点的固定预算内寻找最具对抗性的扰动。然而，考虑到每个节点的鲁棒性各不相同，固定预算不可避免地会产生一个两难问题，即当预算相对较小时，不会找到成功的扰动，而当预算过大时，屈服冗余扰动将损害不可见性。为了打破这一困境，我们提出了一种新的拓扑攻击方法——最小预算拓扑攻击，目的是自适应地寻找对每个节点成功进行攻击所需的最小扰动。为此，我们提出了一种基于动态投影梯度下降法算法的攻击模型—— MiBTack，该模型能够有效地解决离散拓扑中涉及非凸约束的优化问题。在三个 GNN 和四个实际数据集上的广泛应用结果表明，MiBTack 能够成功地引导所有目标节点以最小扰动边错误分类。此外，所得到的最小预算可以用来衡量节点的鲁棒性，因此我们可以探索节点的鲁棒性、拓扑结构和不确定性之间的关系，这是目前固定预算拓扑攻击所不能提供的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Minimum+Topology+Attacks+for+Graph+Neural+Networks)|0|
|[Learning Mixtures of Markov Chains with Quality Guarantees](https://doi.org/10.1145/3543507.3583524)|Fabian Spaeh, Charalampos E. Tsourakakis|Department of Computer Science, Boston University, USA|A large number of modern applications ranging from listening songs online and browsing the Web to using a navigation app on a smartphone generate a plethora of user trails. Clustering such trails into groups with a common sequence pattern can reveal significant structure in human behavior that can lead to improving user experience through better recommendations, and even prevent suicides [LMCR14]. One approach to modeling this problem mathematically is as a mixture of Markov chains. Recently, Gupta, Kumar and Vassilvitski [GKV16] introduced an algorithm (GKV-SVD) based on the singular value decomposition (SVD) that under certain conditions can perfectly recover a mixture of L chains on n states, given only the distribution of trails of length 3 (3-trail). In this work we contribute to the problem of unmixing Markov chains by highlighting and addressing two important constraints of the GKV-SVD algorithm [GKV16]: some chains in the mixture may not even be weakly connected, and secondly in practice one does not know beforehand the true number of chains. We resolve these issues in the Gupta et al. paper [GKV16]. Specifically, we propose an algebraic criterion that enables us to choose a value of L efficiently that avoids overfitting. Furthermore, we design a reconstruction algorithm that outputs the true mixture in the presence of disconnected chains and is robust to noise. We complement our theoretical results with experiments on both synthetic and real data, where we observe that our method outperforms the GKV-SVD algorithm. Finally, we empirically observe that combining an EM-algorithm with our method performs best in practice, both in terms of reconstruction error with respect to the distribution of 3-trails and the mixture of Markov Chains.|大量的现代应用程序，从在线听歌、浏览网页到在智能手机上使用导航应用程序，都会产生大量的用户踪迹。将这些痕迹聚类到具有共同序列模式的群体中，可以揭示人类行为的重要结构，通过更好的推荐来改善用户体验，甚至可以防止自杀[ LMCR14]。将这个问题数学化建模的一种方法是作为马尔可夫链的混合。最近，Gupta，Kumar 和 Vassilvitski [ GKV16]引入了一种基于奇异值分解(SVD)的算法(GKV-SVD) ，该算法在一定条件下可以完美地恢复 n 个状态上的 L 链的混合，只需给定长度为3(3-路径)的路径分布。在这项工作中，我们通过突出和解决 GKV-SVD 算法[ GKV16]的两个重要约束来解决马尔可夫链的分解问题: 混合物中的一些链甚至可能不是弱连通的，其次在实践中人们事先不知道链的真实数量。我们在 Gupta 等人的论文[ GKV16]中解决了这些问题。具体来说，我们提出了一个代数准则，使我们能够有效地选择 L 的值，避免过度拟合。此外，我们还设计了一种重建算法，该算法在存在不连通链的情况下输出真混合，并且对噪声具有鲁棒性。我们通过对合成和实际数据的实验来补充我们的理论结果，我们观察到我们的方法优于 GKV-SVD 算法。最后，我们实验性地观察到将 EM 算法与我们的方法结合在一起在实际中表现最好，无论是在重建误差方面的3-轨迹的分布和马尔可夫链的混合。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Mixtures+of+Markov+Chains+with+Quality+Guarantees)|0|
|[GIF: A General Graph Unlearning Strategy via Influence Function](https://doi.org/10.1145/3543507.3583521)|Jiancan Wu, Yi Yang, Yuchun Qian, Yongduo Sui, Xiang Wang, Xiangnan He|University of Science and Technology of China, China|With the greater emphasis on privacy and security in our society, the problem of graph unlearning -- revoking the influence of specific data on the trained GNN model, is drawing increasing attention. However, ranging from machine unlearning to recently emerged graph unlearning methods, existing efforts either resort to retraining paradigm, or perform approximate erasure that fails to consider the inter-dependency between connected neighbors or imposes constraints on GNN structure, therefore hard to achieve satisfying performance-complexity trade-offs. In this work, we explore the influence function tailored for graph unlearning, so as to improve the unlearning efficacy and efficiency for graph unlearning. We first present a unified problem formulation of diverse graph unlearning tasks \wrt node, edge, and feature. Then, we recognize the crux to the inability of traditional influence function for graph unlearning, and devise Graph Influence Function (GIF), a model-agnostic unlearning method that can efficiently and accurately estimate parameter changes in response to a $\epsilon$-mass perturbation in deleted data. The idea is to supplement the objective of the traditional influence function with an additional loss term of the influenced neighbors due to the structural dependency. Further deductions on the closed-form solution of parameter changes provide a better understanding of the unlearning mechanism. We conduct extensive experiments on four representative GNN models and three benchmark datasets to justify the superiority of GIF for diverse graph unlearning tasks in terms of unlearning efficacy, model utility, and unlearning efficiency. Our implementations are available at \url{https://github.com/wujcan/GIF-torch/}.|随着社会对隐私和安全问题的日益重视，图的去学习问题——去除特定数据对训练后的 GNN 模型的影响——日益引起人们的关注。然而，从机器学习到最近出现的图学习方法，现有的研究要么采用再训练范式，要么进行近似擦除，不考虑连通邻居之间的相互依赖性，要么对 GNN 结构施加约束，因此很难实现令人满意的性能-复杂度权衡。本文研究了适用于图形学习的影响函数，以提高图形学习的学习效率和效果。本文首先给出了不同的图去学习任务的节点、边和特征的统一问题表达式。然后，我们认识到了传统影响函数无法进行图形去学习的症结所在，并设计了图形影响函数(GIF) ，这是一种模型无关的去学习方法，可以有效而准确地估计在已删除数据中响应 $ε 质量扰动的参数变化。其思想是在传统影响函数的基础上，增加受影响邻域的结构依赖性损失项，以补充传统影响函数的目标。对参数变化的闭式解的进一步推导可以更好地理解遗忘机制。我们在四个具有代表性的 GNN 模型和三个基准数据集上进行了广泛的实验，以验证 GIF 在不同的图形忘却任务中在忘却效率、模型效用和忘却效率方面的优越性。我们的实现可以在 url { https://github.com/wujcan/gif-torch/}获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GIF:+A+General+Graph+Unlearning+Strategy+via+Influence+Function)|0|
|[INCREASE: Inductive Graph Representation Learning for Spatio-Temporal Kriging](https://doi.org/10.1145/3543507.3583525)|Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, Jianzhong Qi, Chaochao Chen, Longbiao Chen|University of Melbourne, Australia; Zhejiang University, China; School of Informatics, Xiamen University, China|Spatio-temporal kriging is an important problem in web and social applications, such as Web or Internet of Things, where things (e.g., sensors) connected into a web often come with spatial and temporal properties. It aims to infer knowledge for (the things at) unobserved locations using the data from (the things at) observed locations during a given time period of interest. This problem essentially requires \emph{inductive learning}. Once trained, the model should be able to perform kriging for different locations including newly given ones, without retraining. However, it is challenging to perform accurate kriging results because of the heterogeneous spatial relations and diverse temporal patterns. In this paper, we propose a novel inductive graph representation learning model for spatio-temporal kriging. We first encode heterogeneous spatial relations between the unobserved and observed locations by their spatial proximity, functional similarity, and transition probability. Based on each relation, we accurately aggregate the information of most correlated observed locations to produce inductive representations for the unobserved locations, by jointly modeling their similarities and differences. Then, we design relation-aware gated recurrent unit (GRU) networks to adaptively capture the temporal correlations in the generated sequence representations for each relation. Finally, we propose a multi-relation attention mechanism to dynamically fuse the complex spatio-temporal information at different time steps from multiple relations to compute the kriging output. Experimental results on three real-world datasets show that our proposed model outperforms state-of-the-art methods consistently, and the advantage is more significant when there are fewer observed locations. Our code is available at https://github.com/zhengchuanpan/INCREASE.|时空克里金是网络和社会应用中的一个重要问题，如网络或物联网，在这些应用中，连接到网络的东西(例如传感器)通常具有空间和时间属性。它的目的是推断知识(的东西)未被观察的位置使用数据从(的东西)观察位置在给定的时间段的兴趣。这个问题本质上需要方差{归纳学习}。一旦训练，模型应该能够执行不同的位置，包括新给定的克里金，没有再训练。然而，由于空间关系的异质性和时间模式的多样性，使得准确的克里格分析结果具有挑战性。本文提出了一种新的时空克里金归纳图表示学习模型。我们首先编码的异质空间关系之间的未观察和观察位置的空间接近，功能相似性和转移概率。基于每个关系，我们通过联合建模它们的相似性和差异性，准确地聚合大多数相关观测位置的信息，为未观测位置产生归纳表示。然后，我们设计关系感知门控回归单元(GRU)网络来自适应地捕获每个关系生成的序列表示中的时间相关性。最后，提出了一种多关系注意机制，将多关系在不同时间步长上的复杂时空信息动态融合，计算克里金输出。在三个实际数据集上的实验结果表明，我们提出的模型的性能优于最先进的方法一致，并且当观测位置较少时，优势更显著。我们的代码可以在 https://github.com/zhengchuanpan/increase 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=INCREASE:+Inductive+Graph+Representation+Learning+for+Spatio-Temporal+Kriging)|0|
|[Unlearning Graph Classifiers with Limited Data Resources](https://doi.org/10.1145/3543507.3583547)|Chao Pan, Eli Chien, Olgica Milenkovic|University of Illinois, Urbana-Champaign, USA|As the demand for user privacy grows, controlled data removal (machine unlearning) is becoming an important feature of machine learning models for data-sensitive Web applications such as social networks and recommender systems. Nevertheless, at this point it is still largely unknown how to perform efficient machine unlearning of graph neural networks (GNNs); this is especially the case when the number of training samples is small, in which case unlearning can seriously compromise the performance of the model. To address this issue, we initiate the study of unlearning the Graph Scattering Transform (GST), a mathematical framework that is efficient, provably stable under feature or graph topology perturbations, and offers graph classification performance comparable to that of GNNs. Our main contribution is the first known nonlinear approximate graph unlearning method based on GSTs. Our second contribution is a theoretical analysis of the computational complexity of the proposed unlearning mechanism, which is hard to replicate for deep neural networks. Our third contribution are extensive simulation results which show that, compared to complete retraining of GNNs after each removal request, the new GST-based approach offers, on average, a 10.38x speed-up and leads to a 2.6% increase in test accuracy during unlearning of 90 out of 100 training graphs from the IMDB dataset (10% training ratio). Our implementation is available online at https://doi.org/10.5281/zenodo.7613150.|随着对用户隐私需求的增长，受控数据删除(机器去除)正在成为数据敏感的 Web 应用程序(如社交网络和推荐系统)机器学习模型的一个重要特征。然而，在这一点上，如何对图神经网络(GNN)进行有效的机器学习仍然是一个很大的未知数; 特别是在训练样本数量很少的情况下，在这种情况下，学习会严重影响模型的性能。为了解决这个问题，我们开始研究去除图散射变换(GST) ，这是一个在特征或图拓扑扰动下高效、可证明稳定的数学框架，并提供了与 GNN 相当的图分类性能。我们的主要贡献是第一个已知的基于 GST 的非线性近似图去学习方法。我们的第二个贡献是对所提出的去学习机制的计算复杂性进行了理论分析，这种机制对于深层神经网络来说是难以复制的。我们的第三个贡献是广泛的模拟结果，这些结果表明，与每次删除请求后完成 GNN 的再训练相比，基于 GST 的新方法平均提供10.38倍的加速，并导致在从 IMDB 数据集中学习100个训练图表中的90个(10% 训练比率)时，测试准确率提高2.6% 。我们的实施方案可于网上 https://doi.org/10.5281/zenodo.7613150下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unlearning+Graph+Classifiers+with+Limited+Data+Resources)|0|
|[GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner](https://doi.org/10.1145/3543507.3583379)|Zhenyu Hou, Yufei He, Yukuo Cen, Xiao Liu, Yuxiao Dong, Evgeny Kharlamov, Jie Tang|Tsinghua University, China; Bosch Center for Artificial Intelligence, Germany; Beijing Institute of Technology, China|Graph self-supervised learning (SSL), including contrastive and generative approaches, offers great potential to address the fundamental challenge of label scarcity in real-world graph data. Among both sets of graph SSL techniques, the masked graph autoencoders (e.g., GraphMAE)--one type of generative method--have recently produced promising results. The idea behind this is to reconstruct the node features (or structures)--that are randomly masked from the input--with the autoencoder architecture. However, the performance of masked feature reconstruction naturally relies on the discriminability of the input features and is usually vulnerable to disturbance in the features. In this paper, we present a masked self-supervised learning framework GraphMAE2 with the goal of overcoming this issue. The idea is to impose regularization on feature reconstruction for graph SSL. Specifically, we design the strategies of multi-view random re-mask decoding and latent representation prediction to regularize the feature reconstruction. The multi-view random re-mask decoding is to introduce randomness into reconstruction in the feature space, while the latent representation prediction is to enforce the reconstruction in the embedding space. Extensive experiments show that GraphMAE2 can consistently generate top results on various public datasets, including at least 2.45% improvements over state-of-the-art baselines on ogbn-Papers100M with 111M nodes and 1.6B edges.|图的自监督学习(SSL)包括对比学习和生成学习，为解决现实世界图数据中标签稀缺的基本问题提供了巨大的潜力。在这两种图形 SSL 技术中，掩码图形自动编码器(如 GraphMAE)——一种生成方法——最近已经产生了有希望的结果。其背后的想法是使用自动编码器体系结构重建节点特性(或结构)——这些特性从输入中随机屏蔽。然而，掩蔽特征重建的性能自然依赖于输入特征的可识别性，并且通常容易受到特征的干扰。在本文中，我们提出了一个屏蔽自监督学习框架 GraphMAE2，目的是克服这个问题。其思想是在图 SSL 的特征重构中加入正则化。具体来说，我们设计了多视点随机重掩码解码和潜在表征预测的策略来规范特征重建。多视点随机重掩码译码是在特征空间重构中引入随机性，而潜在表示预测是在嵌入空间中加强重构。大量的实验表明，GraphMAE2可以始终如一地在各种公共数据集上产生最佳结果，包括在具有111M 节点和1.6 B 边的 ogbn-Papers100M 上比最先进的基线至少提高2.45% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphMAE2:+A+Decoding-Enhanced+Masked+Self-Supervised+Graph+Learner)|0|
|[KGTrust: Evaluating Trustworthiness of SIoT via Knowledge Enhanced Graph Neural Networks](https://doi.org/10.1145/3543507.3583549)|Zhizhi Yu, Di Jin, Cuiying Huo, Zhiqiang Wang, Xiulong Liu, Heng Qi, Jia Wu, Lingfei Wu|Content and Knowledge Graph, Pinterest, USA; School of Computer Science and Technology, Dalian University of Technology, China; School of Computing, Macquarie University, Australia; College of Intelligence and Computing, Tianjin University, China|Social Internet of Things (SIoT), a promising and emerging paradigm that injects the notion of social networking into smart objects (i.e., things), paving the way for the next generation of Internet of Things. However, due to the risks and uncertainty, a crucial and urgent problem to be settled is establishing reliable relationships within SIoT, that is, trust evaluation. Graph neural networks for trust evaluation typically adopt a straightforward way such as one-hot or node2vec to comprehend node characteristics, which ignores the valuable semantic knowledge attached to nodes. Moreover, the underlying structure of SIoT is usually complex, including both the heterogeneous graph structure and pairwise trust relationships, which renders hard to preserve the properties of SIoT trust during information propagation. To address these aforementioned problems, we propose a novel knowledge-enhanced graph neural network (KGTrust) for better trust evaluation in SIoT. Specifically, we first extract useful knowledge from users' comment behaviors and external structured triples related to object descriptions, in order to gain a deeper insight into the semantics of users and objects. Furthermore, we introduce a discriminative convolutional layer that utilizes heterogeneous graph structure, node semantics, and augmented trust relationships to learn node embeddings from the perspective of a user as a trustor or a trustee, effectively capturing multi-aspect properties of SIoT trust during information propagation. Finally, a trust prediction layer is developed to estimate the trust relationships between pairwise nodes. Extensive experiments on three public datasets illustrate the superior performance of KGTrust over state-of-the-art methods.|社交物联网(SIoT)是一种很有前途的新兴模式，它将社交网络的概念注入到智能对象(即物品)中，为下一代物联网铺平了道路。然而，由于存在风险和不确定性，在 SIoT 中建立可靠的关系，即信任评估，是一个亟待解决的关键问题。用于信任评估的图神经网络通常采用一个热点或节点2vec 等直观的方法来理解节点特征，忽略了节点所附带的有价值的语义知识。此外，SIoT 的底层结构通常是复杂的，包括异构图结构和成对信任关系，这使得在信息传播过程中很难保持 SIoT 信任的性质。为了解决上述问题，我们提出了一种新的知识增强图形神经网络(KGTrust) ，用于 SIoT 中更好的信任评估。具体来说，我们首先从用户的评论行为和与对象描述相关的外部结构化三元组中提取有用的知识，以便更深入地了解用户和对象的语义。此外，我们还引入了一个判别卷积层，该层利用异构图结构、节点语义和增强的信任关系，从用户作为信任者或受信者的角度学习节点嵌入，有效地捕获信息传播过程中 SIoT 信任的多方面特性。最后，开发了一个信任预测层来估计成对节点之间的信任关系。在三个公共数据集上的大量实验表明，KGTrust 的性能优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KGTrust:+Evaluating+Trustworthiness+of+SIoT+via+Knowledge+Enhanced+Graph+Neural+Networks)|0|
|[CogDL: A Comprehensive Library for Graph Deep Learning](https://doi.org/10.1145/3543507.3583472)|Yukuo Cen, Zhenyu Hou, Yan Wang, Qibin Chen, Yizhen Luo, Zhongming Yu, Hengrui Zhang, Xingcheng Yao, Aohan Zeng, Shiguang Guo, Yuxiao Dong, Yang Yang, Peng Zhang, Guohao Dai, Yu Wang, Chang Zhou, Hongxia Yang, Jie Tang|Tsinghua University, China; Alibaba Group, China; Zhejiang University, China; Zhipu AI, China|Graph neural networks (GNNs) have attracted tremendous attention from the graph learning community in recent years. It has been widely adopted in various real-world applications from diverse domains, such as social networks and biological graphs. The research and applications of graph deep learning present new challenges, including the sparse nature of graph data, complicated training of GNNs, and non-standard evaluation of graph tasks. To tackle the issues, we present CogDL, a comprehensive library for graph deep learning that allows researchers and practitioners to conduct experiments, compare methods, and build applications with ease and efficiency. In CogDL, we propose a unified design for the training and evaluation of GNN models for various graph tasks, making it unique among existing graph learning libraries. By utilizing this unified trainer, CogDL can optimize the GNN training loop with several training techniques, such as mixed precision training. Moreover, we develop efficient sparse operators for CogDL, enabling it to become the most competitive graph library for efficiency. Another important CogDL feature is its focus on ease of use with the aim of facilitating open and reproducible research of graph learning. We leverage CogDL to report and maintain benchmark results on fundamental graph tasks, which can be reproduced and directly used by the community.|近年来，图神经网络(GNN)引起了图学习界的广泛关注。它已被广泛应用于各种不同领域的现实世界应用，如社会网络和生物图表。图形深度学习的研究和应用面临着新的挑战，包括图形数据的稀疏性、图形神经网络的复杂训练以及图形任务的非标准评估。为了解决这些问题，我们提出了 CogDL，一个图形深度学习的综合库，允许研究人员和从业人员进行实验，比较方法，并轻松有效地构建应用程序。在 CogDL 中，我们提出了一种针对各种图形任务的 GNN 模型训练和评估的统一设计，使其在现有的图形学习库中独树一帜。利用这种统一的训练器，CogDL 可以通过混合精度训练等多种训练技术对 GNN 训练回路进行优化。此外，我们还为 CogDL 开发了高效的稀疏算子，使其成为最具竞争力的高效图形库。CogDL 的另一个重要特性是其对易用性的关注，旨在促进图形学习的开放性和可重复性研究。我们利用 CogDL 报告和维护基本图任务的基准测试结果，这些结果可以被社区复制和直接使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CogDL:+A+Comprehensive+Library+for+Graph+Deep+Learning)|0|
|[Tracing Knowledge Instead of Patterns: Stable Knowledge Tracing with Diagnostic Transformer](https://doi.org/10.1145/3543507.3583255)|Yu Yin, Le Dai, Zhenya Huang, Shuanghong Shen, Fei Wang, Qi Liu, Enhong Chen, Xin Li|University of Science and Technology of China, China and iFLYTEK Co., Ltd, China; Anhui Province Key Laboratory of Big Data Analysis and Application, School of Data Science, University of Science and Technology of China, China; Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China, China; Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology & School of Data Science, University of Science and Technology of China, China|Knowledge Tracing (KT) aims at tracing the evolution of the knowledge states along the learning process of a learner. It has become a crucial task for online learning systems to model the learning process of their users, and further provide their users a personalized learning guidance. However, recent developments in KT based on deep neural networks mostly focus on increasing the accuracy of predicting the next performance of students. We argue that current KT modeling, as well as training paradigm, can lead to models tracing patterns of learner’s learning activities, instead of their evolving knowledge states. In this paper, we propose a new architecture, Diagnostic Transformer (DTransformer), along with a new training paradigm, to tackle this challenge. With DTransformer, we build the architecture from question-level to knowledge-level, explicitly diagnosing learner’s knowledge proficiency from each question mastery states. We also propose a novel training algorithm based on contrastive learning that focuses on maintaining the stability of the knowledge state diagnosis. Through extensive experiments, we will show that with its understanding of knowledge state evolution, DTransformer achieves a better performance prediction accuracy and more stable knowledge state tracing results. We will also show that DTransformer is less sensitive to specific patterns with case study. We open-sourced our code and data at https://github.com/yxonic/DTransformer.|知识追踪(KT)旨在追踪学习者在学习过程中知识状态的演化过程。建立在线学习系统用户学习过程的模型，进一步为用户提供个性化的学习指导，已成为在线学习系统的一项重要任务。然而，基于深层神经网络的 KT 的最新发展主要集中在提高预测学生下一步成绩的准确性上。我们认为，目前的 KT 模型和训练范式可以导致模型跟踪学习者的学习活动模式，而不是其演化的知识状态。在本文中，我们提出了一个新的体系结构，诊断变压器(变压器) ，以及一个新的训练范例，以应对这一挑战。使用 Dformer，我们构建了从问题级到知识级的体系结构，从每个问题掌握状态明确诊断学习者的知识熟练程度。提出了一种新的基于对比学习的训练算法，着重于保持知识状态诊断的稳定性。通过大量的实验，我们将表明，由于对知识状态演化的理解，Dformer 获得了更好的性能预测精度和更稳定的知识状态跟踪结果。我们还将通过案例研究说明 Dformer 对特定模式的敏感性较低。我们开源代码和数据的 https://github.com/yxonic/dtransformer。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tracing+Knowledge+Instead+of+Patterns:+Stable+Knowledge+Tracing+with+Diagnostic+Transformer)|0|
|[Learning to Simulate Daily Activities via Modeling Dynamic Human Needs](https://doi.org/10.1145/3543507.3583276)|Yuan Yuan, Huandong Wang, Jingtao Ding, Depeng Jin, Yong Li|Department of Electronic Engineering, Tsinghua University, China|Daily activity data that records individuals' various types of activities in daily life are widely used in many applications such as activity scheduling, activity recommendation, and policymaking. Though with high value, its accessibility is limited due to high collection costs and potential privacy issues. Therefore, simulating human activities to produce massive high-quality data is of great importance to benefit practical applications. However, existing solutions, including rule-based methods with simplified assumptions of human behavior and data-driven methods directly fitting real-world data, both cannot fully qualify for matching reality. In this paper, motivated by the classic psychological theory, Maslow's need theory describing human motivation, we propose a knowledge-driven simulation framework based on generative adversarial imitation learning. To enhance the fidelity and utility of the generated activity data, our core idea is to model the evolution of human needs as the underlying mechanism that drives activity generation in the simulation model. Specifically, this is achieved by a hierarchical model structure that disentangles different need levels, and the use of neural stochastic differential equations that successfully captures piecewise-continuous characteristics of need dynamics. Extensive experiments demonstrate that our framework outperforms the state-of-the-art baselines in terms of data fidelity and utility. Besides, we present the insightful interpretability of the need modeling. The code is available at https://github.com/tsinghua-fib-lab/SAND.|记录个人日常生活中各种类型活动的日常活动数据被广泛应用于活动计划、活动推荐和决策等许多应用中。虽然具有很高的价值，但由于收集成本高和潜在的隐私问题，其可访问性受到限制。因此，模拟人类活动产生大量高质量的数据对于实际应用具有重要意义。然而，现有的解决方案，包括基于人类行为简化假设的规则方法和直接拟合真实世界数据的数据驱动方法，都不能完全符合现实。本文以经典心理学理论——马斯洛需要理论为基础，提出了一种基于生成对抗模仿学习的知识驱动模拟框架。为了提高生成的活动数据的保真度和实用性，我们的核心思想是将人类需求的演化建模为驱动仿真模型中活动生成的底层机制。具体来说，这是通过一个层次化的模型结构，解开不同的需求水平，并使用神经随机微分方程，成功地捕获分段连续的需求动态特征。大量的实验表明，我们的框架在数据保真度和实用性方面优于最先进的基线。此外，我们提出了深刻的需求建模的可解释性。密码可在 https://github.com/tsinghua-fib-lab/sand 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Simulate+Daily+Activities+via+Modeling+Dynamic+Human+Needs)|0|
|[Controllable Universal Fair Representation Learning](https://doi.org/10.1145/3543507.3583307)|Yue Cui, Ma Chen, Kai Zheng, Lei Chen, Xiaofang Zhou|University of Electronic Science and Technology of China, China; The Hong Kong University of Science and Technology, Hong Kong; City University of Hong Kong, Hong Kong|Learning fair and transferable representations of users that can be used for a wide spectrum of downstream tasks (specifically, machine learning models) has great potential in fairness-aware Web services. Existing studies focus on debiasing w.r.t. a small scale of (one or a handful of) fixed pre-defined sensitive attributes. However, in real practice, downstream data users can be interested in various protected groups and these are usually not known as prior. This requires the learned representations to be fair w.r.t. all possible sensitive attributes. We name this task universal fair representation learning, in which an exponential number of sensitive attributes need to be dealt with, bringing the challenges of unreasonable computational cost and un-guaranteed fairness constraints. To address these problems, we propose a controllable universal fair representation learning (CUFRL) method. An effective bound is first derived via the lens of mutual information to guarantee parity of the universal set of sensitive attributes while maintaining the accuracy of downstream tasks. We also theoretically establish that the number of sensitive attributes that need to be processed can be reduced from exponential to linear. Experiments on two public real-world datasets demonstrate CUFRL can achieve significantly better accuracy-fairness trade-off compared with baseline approaches.|学习可用于广泛的下游任务(特别是机器学习模型)的用户的公平和可转移的表示在公平感知的 Web 服务中具有巨大的潜力。现有的研究主要集中在消除一小部分(一个或少数)固定的预先定义的敏感属性的偏差。然而，在实际操作中，下游数据用户可能对各种受保护的组感兴趣，这些组通常不被称为优先级。这要求学习的表示是公平的，所有可能的敏感属性都是公平的。我们将这个任务命名为通用公平表示学习，其中需要处理的敏感属性数目呈指数增长，带来了计算代价不合理和公平性约束不能保证的挑战。为了解决这些问题，我们提出了一种可控的通用公平表示学习(CUFRL)方法。首先通过互信息透镜导出有效界，以保证通用敏感属性集的奇偶性，同时保持下游任务的准确性。我们还从理论上建立了需要处理的敏感属性的数量可以从指数减少到线性。在两个公共实际数据集上的实验表明，与基线方法相比，CUFRL 能够获得更好的精度-公平性权衡。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Controllable+Universal+Fair+Representation+Learning)|0|
|[Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling](https://doi.org/10.1145/3543507.3583380)|Aseem Srivastava, Ishan Pandey, Md. Shad Akhtar, Tanmoy Chakraborty|IIT Delhi, India; IIIT Delhi, India|Virtual Mental Health Assistants (VMHAs) have become a prevalent method for receiving mental health counseling in the digital healthcare space. An assistive counseling conversation commences with natural open-ended topics to familiarize the client with the environment and later converges into more fine-grained domain-specific topics. Unlike other conversational systems, which are categorized as open-domain or task-oriented systems, VMHAs possess a hybrid conversational flow. These counseling bots need to comprehend various aspects of the conversation, such as dialogue-acts, intents, etc., to engage the client in an effective conversation. Although the surge in digital health research highlights applications of many general-purpose response generation systems, they are barely suitable in the mental health domain -- the prime reason is the lack of understanding in mental health counseling. Moreover, in general, dialogue-act guided response generators are either limited to a template-based paradigm or lack appropriate semantics. To this end, we propose READER -- a REsponse-Act guided reinforced Dialogue genERation model for the mental health counseling conversations. READER is built on transformer to jointly predict a potential dialogue-act d(t+1) for the next utterance (aka response-act) and to generate an appropriate response u(t+1). Through the transformer-reinforcement-learning (TRL) with Proximal Policy Optimization (PPO), we guide the response generator to abide by d(t+1) and ensure the semantic richness of the responses via BERTScore in our reward computation. We evaluate READER on HOPE, a benchmark counseling conversation dataset and observe that it outperforms several baselines across several evaluation metrics -- METEOR, ROUGE, and BERTScore. We also furnish extensive qualitative and quantitative analyses on results, including error analysis, human evaluation, etc.|虚拟心理健康助理(VMHA)已成为数字化医疗空间中接受心理健康咨询的普遍方法。辅助性咨询对话以自然的开放式主题开始，以使客户熟悉环境，然后汇聚成更细粒度的特定领域的主题。与其他会话系统不同的是，VMHA 具有混合会话流。这些咨询机器人需要理解对话的各个方面，例如对话行为、意图等，以使客户参与到有效的对话中。尽管数字健康研究的高潮突出了许多通用反应生成系统的应用，但它们几乎不适合于心理健康领域——主要原因是缺乏对心理健康咨询的理解。此外，一般而言，对话行为指导的响应生成器要么局限于基于模板的范例，要么缺乏适当的语义。为此，我们提出了 READER ——一个由响应-行为引导的强化对话生成模型，用于心理健康咨询对话。读取器是建立在变压器联合预测潜在的对话行为 d (t + 1)为下一个话语(又名反应行为) ，并产生一个适当的反应 u (t + 1)。通过最近策略优化(PPO)的变压器强化学习(TRL) ，引导响应生成器遵守 d (t + 1) ，并在奖励计算中通过 BERTScore 保证响应的语义丰富性。我们在 HOPE 上评估 READER，HOPE 是一个基准咨询会话数据集，并观察到它在几个评估指标—— METEOR、 ROUGE 和 BERTScore 上的表现优于几个基准。我们还提供了广泛的定性和定量分析的结果，包括误差分析，人的评价等。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Response-act+Guided+Reinforced+Dialogue+Generation+for+Mental+Health+Counseling)|0|
|[Offline Policy Evaluation in Large Action Spaces via Outcome-Oriented Action Grouping](https://doi.org/10.1145/3543507.3583448)|Jie Peng, Hao Zou, Jiashuo Liu, Shaoming Li, Yibao Jiang, Jian Pei, Peng Cui|Tsinghua University, China; Duke University, USA; Meituan, China|Offline policy evaluation (OPE) aims to accurately estimate the performance of a hypothetical policy using only historical data, which has drawn increasing attention in a wide range of applications including recommender systems and personalized medicine. With the presence of rising granularity of consumer data, many industries started exploring larger action candidate spaces to support more precise personalized action. While inverse propensity score (IPS) is a standard OPE estimator, it suffers from more severe variance issues with increasing action spaces. To address this issue, we theoretically prove that the estimation variance can be reduced by merging actions into groups while the distinction among these action effects on the outcome can induce extra bias. Motivated by these, we propose a novel IPS estimator with outcome-oriented action Grouping (GroupIPS), which leverages a Lipschitz regularized network to measure the distance of action effects in the embedding space and merges nearest action neighbors. This strategy enables more robust estimation by achieving smaller variances while inducing minor additional bias. Empirically, extensive experiments on both synthetic and real world datasets demonstrate the effectiveness of our proposed method.|离线政策评估(OPE)旨在仅使用历史数据来准确地估计假设政策的性能，这已经引起了包括推荐系统和个体化医学在内的广泛应用中的越来越多的关注。随着消费者数据粒度的增加，许多行业开始探索更大的行动候选空间，以支持更精确的个性化行动。虽然逆倾向得分(IPS)是一个标准的 OPE 估计器，但随着行为空间的增加，它会遇到更严重的方差问题。为了解决这个问题，我们从理论上证明了估计方差可以通过合并行动成组而减少，而这些行动效应之间的区别对结果可能会导致额外的偏差。在此基础上，提出了一种新的基于结果导向的动作分组的 IPS 估计器(GroupIPS) ，该估计器利用 Lipschitz 正则化网络测量嵌入空间中动作效应的距离并合并最近的动作邻居。这种策略通过实现较小的方差同时诱导较小的附加偏差来实现更稳健的估计。经验证明，在合成和真实世界数据集上的广泛实验证明了我们提出的方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Offline+Policy+Evaluation+in+Large+Action+Spaces+via+Outcome-Oriented+Action+Grouping)|0|
|[Web Table Formatting Affects Readability on Mobile Devices](https://doi.org/10.1145/3543507.3583506)|Christopher Tensmeyer, Zoya Bylinskii, Tianyuan Cai, Dave Miller, Ani Nenkova, Aleena Gertrudes Niklaus, Shaun Wallace|Tufts, USA; Brown University, USA; Adobe, USA; Adobe Research, USA|Reading large tables on small mobile screens presents serious usability challenges that can be addressed, in part, by better table formatting. However, there are few evidenced-based guidelines for formatting mobile tables to improve readability. For this work, we first conducted a survey to investigate how people interact with tables on mobile devices and conducted a study with designers to identify which design considerations are most critical. Based on these findings, we designed and conducted three large scale studies with remote crowdworker participants. Across the studies, we analyze over 14,000 trials from 590 participants who each viewed and answered questions about 28 diverse tables rendered in different formats. We find that smaller cell padding and frozen headers lead to faster task completion, and that while zebra striping and row borders do not speed up tasks, they are still subjectively preferred by participants.|在小型移动屏幕上阅读大型表格提出了严峻的可用性挑战，这些挑战可以通过更好的表格格式来部分解决。然而，很少有基于证据的指导方针来格式化移动表以提高可读性。对于这项工作，我们首先进行了一项调查，以调查人们如何与移动设备上的表格互动，并与设计师进行了一项研究，以确定哪些设计考虑是最关键的。基于这些发现，我们设计并进行了三个大规模的研究与远程众包工作者的参与者。在这些研究中，我们分析了来自590名参与者的超过14,000个试验，每个参与者观看并回答了28个不同格式表格的问题。我们发现较小的单元格填充和冻结的标题可以更快地完成任务，虽然斑马条纹和行边框不能加速任务，但它们仍然是参与者的主观偏好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Web+Table+Formatting+Affects+Readability+on+Mobile+Devices)|0|
|[Web Structure Derived Clustering for Optimised Web Accessibility Evaluation](https://doi.org/10.1145/3543507.3583508)|Alexander Hambley, Yeliz Yesilada, Markel Vigo, Simon Harper|Middle East Technical University Northern Cyprus Campus, Turkey; University of Manchester, United Kingdom|Web accessibility evaluation is a costly and complex process due to limited time, resources and ambiguity. To optimise the accessibility evaluation process, we aim to reduce the number of pages auditors must review by employing statistically representative pages, reducing a site of thousands of pages to a manageable review of archetypal pages. Our paper focuses on representativeness, one of six proposed metrics that form our methodology, to address the limitations we have identified with the W3C Website Accessibility Conformance Evaluation Methodology (WCAG-EM). These include the evaluative scope, the non-probabilistic sampling approach, and the potential for bias within the selected sample. Representativeness, in particular, is a metric to assess the quality and coverage of sampling. To measure this, we systematically evaluate five web page representations with a website of 388 pages, including tags, structure, the DOM tree, content, and a mixture of structure and content. Our findings highlight the importance of including structural components in representations. We validate our conclusions using the same methodology for three additional random sites of 500 pages. As an exclusive attribute, we find that features derived from web content are suboptimal and can lead to lower quality and more disparate clustering for optimised accessibility evaluation.|由于时间、资源和含糊不清，网页亲和力评估是一个昂贵而复杂的过程。为了优化易访问性评估过程，我们的目标是通过使用具有统计代表性的页面，减少审计人员必须审查的页面数量，将数千页的网站减少到一个可管理的原型页面审查。我们的论文主要关注代表性，这是构成我们方法论的六个指标之一，以解决我们在 W3C 网站可访问性一致性评估方法(WCAG-EM)中发现的局限性。这些包括评估范围，非概率抽样方法，以及在选定的样本中存在偏差的可能性。具体而言，代表性是评估抽样质量和覆盖面的一个指标。为了衡量这一点，我们系统地评估了拥有388个页面的网站的五种网页表示形式，包括标签、结构、 DOM 树、内容以及结构和内容的混合。我们的发现强调了在表征中包含结构成分的重要性。我们使用相同的方法对另外三个500页的随机站点验证了我们的结论。作为一个独有的属性，我们发现从网页内容衍生出来的特征是次优的，可能导致质量下降和更多不同的聚类来优化易访问性评价。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Web+Structure+Derived+Clustering+for+Optimised+Web+Accessibility+Evaluation)|0|
|[Hashtag-Guided Low-Resource Tweet Classification](https://doi.org/10.1145/3543507.3583194)|Shizhe Diao, Sedrick Scott Keh, Liangming Pan, Zhiliang Tian, Yan Song, Tong Zhang|University of Science and Technology of China, China; The Hong Kong University of Science and Technology, Hong Kong; Carnegie Mellon University, USA; University of California, Santa Barbara, USA|Social media classification tasks (e.g., tweet sentiment analysis, tweet stance detection) are challenging because social media posts are typically short, informal, and ambiguous. Thus, training on tweets is challenging and demands large-scale human-annotated labels, which are time-consuming and costly to obtain. In this paper, we find that providing hashtags to social media tweets can help alleviate this issue because hashtags can enrich short and ambiguous tweets in terms of various information, such as topic, sentiment, and stance. This motivates us to propose a novel Hashtag-guided Tweet Classification model (HashTation), which automatically generates meaningful hashtags for the input tweet to provide useful auxiliary signals for tweet classification. To generate high-quality and insightful hashtags, our hashtag generation model retrieves and encodes the post-level and entity-level information across the whole corpus. Experiments show that HashTation achieves significant improvements on seven low-resource tweet classification tasks, in which only a limited amount of training data is provided, showing that automatically enriching tweets with model-generated hashtags could significantly reduce the demand for large-scale human-labeled data. Further analysis demonstrates that HashTation is able to generate high-quality hashtags that are consistent with the tweets and their labels. The code is available at https://github.com/shizhediao/HashTation.|社交媒体分类任务(例如，推文情绪分析，推文姿态检测)是具有挑战性的，因为社交媒体的帖子通常是短小、非正式和模棱两可的。因此，对 tweet 的培训是具有挑战性的，需要大规模的人工注释标签，这是耗时和昂贵的获得。在本文中，我们发现为社交媒体 tweet 提供 # 标签可以帮助缓解这个问题，因为 # 标签可以丰富各种信息，如主题、情感和立场方面的短小和模糊的 tweet。这促使我们提出一种新的 Hashtag 引导的 Tweet 分类模型(HashTation) ，它自动为输入 Tweet 生成有意义的 hashtag，为 Tweet 分类提供有用的辅助信号。为了生成高质量和有见地的 # 标签，我们的 # 标签生成模型检索并编码整个语料库的后级和实体级信息。实验表明，HashTation 在七个低资源的 tweet 分类任务上取得了显著的改进，其中只提供了有限数量的训练数据，这表明使用模型生成的 # 标签自动丰富 tweet 可以显著降低对大规模人类标记数据的需求。进一步的分析表明，HashTation 能够生成与 tweet 及其标签一致的高质量 # 标签。密码可在 https://github.com/shizhediao/hashtation 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hashtag-Guided+Low-Resource+Tweet+Classification)|0|
|[FormerTime: Hierarchical Multi-Scale Representations for Multivariate Time Series Classification](https://doi.org/10.1145/3543507.3583205)|Mingyue Cheng, Qi Liu, Zhiding Liu, Zhi Li, Yucong Luo, Enhong Chen|Tsinghua University, China; University of Science and Technology of China, China and State Key Laboratory of Cognitive Intelligence, China|Deep learning-based algorithms, e.g., convolutional networks, have significantly facilitated multivariate time series classification (MTSC) task. Nevertheless, they suffer from the limitation in modeling long-range dependence due to the nature of convolution operations. Recent advancements have shown the potential of transformers to capture long-range dependence. However, it would incur severe issues, such as fixed scale representations, temporal-invariant and quadratic time complexity, with transformers directly applicable to the MTSC task because of the distinct properties of time series data. To tackle these issues, we propose FormerTime, an hierarchical representation model for improving the classification capacity for the MTSC task. In the proposed FormerTime, we employ a hierarchical network architecture to perform multi-scale feature maps. Besides, a novel transformer encoder is further designed, in which an efficient temporal reduction attention layer and a well-informed contextual positional encoding generating strategy are developed. To sum up, FormerTime exhibits three aspects of merits: (1) learning hierarchical multi-scale representations from time series data, (2) inheriting the strength of both transformers and convolutional networks, and (3) tacking the efficiency challenges incurred by the self-attention mechanism. Extensive experiments performed on $10$ publicly available datasets from UEA archive verify the superiorities of the FormerTime compared to previous competitive baselines.|基于深度学习的算法，如卷积网络，极大地促进了多变量时间序列分类(MTSC)任务。然而，由于卷积运算的性质，它们在建立长程依赖模型方面存在局限性。最近的进展显示了变压器捕获远程依赖的潜力。然而，由于时间序列数据的独特性质，变压器直接适用于 MTSC 任务会产生严重的问题，如固定尺度表示、时间不变性和二次时间复杂性等。为了解决这些问题，我们提出了 FormerTime，一种用于提高 MTSC 任务分类能力的层次化表示模型。在所提出的 FormerTime 中，我们采用了层次化的网络结构来执行多尺度的特征映射。此外，本文还设计了一种新型的变压器编码器，其中提出了一种有效的时间约简注意层和充分知情的上下文位置编码生成策略。综上所述，FormerTime 具有三个方面的优点: (1)从时间序列数据中学习分层多尺度表示; (2)继承变压器和卷积网络的优点; (3)解决自我注意机制带来的效率挑战。对 UEA 档案中10美元可公开获得的数据集进行的广泛实验验证了 FormerTime 相对于以前的竞争基线的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FormerTime:+Hierarchical+Multi-Scale+Representations+for+Multivariate+Time+Series+Classification)|0|
|[HISum: Hyperbolic Interaction Model for Extractive Multi-Document Summarization](https://doi.org/10.1145/3543507.3583197)|Mingyang Song, Yi Feng, Liping Jing|Beijing Jiaotong University, China|Extractive summarization helps provide a short description or a digest of news or other web texts. It enhances the reading experience of users, especially when they are reading on small displays (e.g., mobile phones). Matching-based methods are recently proposed for the extractive summarization task, which extracts a summary from a global view via a document-summary matching framework. However, these methods only calculate similarities between candidate summaries and the entire document embeddings, insufficiently capturing interactions between different contextual information in the document to accurately estimate the importance of candidates. In this paper, we propose a new hyperbolic interaction model for extractive multi-document summarization (HISum). Specifically, HISum first learns document and candidate summary representations in the same hyperbolic space to capture latent hierarchical structures and then estimates the importance scores of candidates by jointly modeling interactions between each candidate and the document from global and local views. Finally, the importance scores are used to rank and extract the best candidate as the extracted summary. Experimental results on several benchmarks show that HISum outperforms the state-of-the-art extractive baselines1.|提取摘要有助于提供新闻或其他网络文本的简短描述或摘要。它增强了用户的阅读体验，特别是当他们在小型显示器(如手机)上阅读时。最近提出了一种基于匹配的提取摘要方法，该方法通过文档摘要匹配框架从全局视图中提取摘要。然而，这些方法只计算候选摘要和整个文档嵌入之间的相似性，不能充分捕获文档中不同上下文信息之间的交互作用，从而无法准确估计候选摘要的重要性。本文提出了一种新的双曲型交互模型，用于提取多文档摘要(HISum)。具体来说，hISum 首先在同一个双曲空间中学习文档和候选人摘要表示，以捕获潜在的层次结构，然后通过联合建模每个候选人和文档之间的互动，从全局和局部视图估计候选人的重要性得分。最后，利用重要性得分对最佳候选人进行排序和提取，作为提取的总结。对几个基准测试的实验结果表明，HISum 的性能优于最先进的提取基准1。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HISum:+Hyperbolic+Interaction+Model+for+Extractive+Multi-Document+Summarization)|0|
|[Descartes: Generating Short Descriptions of Wikipedia Articles](https://doi.org/10.1145/3543507.3583220)|Marija Sakota, Maxime Peyrard, Robert West|EPFL, Switzerland|Wikipedia is one of the richest knowledge sources on the Web today. In order to facilitate navigating, searching, and maintaining its content, Wikipedia's guidelines state that all articles should be annotated with a so-called short description indicating the article's topic (e.g., the short description of beer is "Alcoholic drink made from fermented cereal grains"). Nonetheless, a large fraction of articles (ranging from 10.2% in Dutch to 99.7% in Kazakh) have no short description yet, with detrimental effects for millions of Wikipedia users. Motivated by this problem, we introduce the novel task of automatically generating short descriptions for Wikipedia articles and propose Descartes, a multilingual model for tackling it. Descartes integrates three sources of information to generate an article description in a target language: the text of the article in all its language versions, the already-existing descriptions (if any) of the article in other languages, and semantic type information obtained from a knowledge graph. We evaluate a Descartes model trained for handling 25 languages simultaneously, showing that it beats baselines (including a strong translation-based baseline) and performs on par with monolingual models tailored for specific languages. A human evaluation on three languages further shows that the quality of Descartes's descriptions is largely indistinguishable from that of human-written descriptions; e.g., 91.3% of our English descriptions (vs. 92.1% of human-written descriptions) pass the bar for inclusion in Wikipedia, suggesting that Descartes is ready for production, with the potential to support human editors in filling a major gap in today's Wikipedia across languages.|维基百科是当今网络上最丰富的知识资源之一。为了便于浏览、搜索和维护其内容，维基百科的指导方针规定，所有文章都应该加注所谓的简短描述，指明文章的主题(例如，对啤酒的简短描述是“用发酵的谷物制成的含酒精饮料”)。尽管如此，大部分文章(荷兰语为10.2% ，哈萨克语为99.7%)还没有简短的描述，这对数百万维基百科用户造成了不利影响。基于这个问题，我们引入了自动生成维基百科文章简短描述的新任务，并提出了笛卡尔模型，一个多语言模型来解决这个问题。笛卡尔整合了三种信息来源，以目标语言生成文章描述: 文章文本的所有语言版本，已经存在的文章描述(如果有的话)的其他语言，和语义类型信息获得的知识图。我们评估了笛卡尔模型同时处理25种语言的训练，表明它打破了基线(包括一个强大的基于翻译的基线) ，并表现出与为特定语言量身定制的单语言模型相当的水平。人类对三种语言的评估进一步表明，笛卡尔描述的质量在很大程度上与人类书写的描述没有区别; 例如，91.3% 的英文描述(相对于92.1% 的人类书写的描述)通过了纳入维基百科的门槛，表明笛卡尔已经准备好生产，有可能支持人类编辑填补当今维基百科跨语言的主要空白。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Descartes:+Generating+Short+Descriptions+of+Wikipedia+Articles)|0|
|[A Dual Prompt Learning Framework for Few-Shot Dialogue State Tracking](https://doi.org/10.1145/3543507.3583238)|Yuting Yang, Wenqiang Lei, Pei Huang, Juan Cao, Jintao Li, TatSeng Chua|Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences, China; Institute of Computing Technology, Chinese Academy of Sciences, China; Stanford University, USA; Sichuan University, China; National University of Singapore, Singapore|Dialogue state tracking (DST) module is an important component for task-oriented dialog systems to understand users' goals and needs. Collecting dialogue state labels including slots and values can be costly, especially with the wide application of dialogue systems in more and more new-rising domains. In this paper, we focus on how to utilize the language understanding and generation ability of pre-trained language models for DST. We design a dual prompt learning framework for few-shot DST. Specifically, we consider the learning of slot generation and value generation as dual tasks, and two prompts are designed based on such a dual structure to incorporate task-related knowledge of these two tasks respectively. In this way, the DST task can be formulated as a language modeling task efficiently under few-shot settings. Experimental results on two task-oriented dialogue datasets show that the proposed method not only outperforms existing state-of-the-art few-shot methods, but also can generate unseen slots. It indicates that DST-related knowledge can be probed from PLM and utilized to address low-resource DST efficiently with the help of prompt learning.|对话状态跟踪(DST)模块是面向任务的对话系统理解用户目标和需求的重要组成部分。收集包括插槽和价值观在内的对话状态标签可能成本高昂，特别是随着对话系统在越来越多新兴领域的广泛应用。本文主要研究如何利用预训练语言模型的语言理解能力和生成能力来进行 DST。我们设计了一个双提示学习框架的少拍 DST。具体而言，我们将时隙生成和价值生成的学习视为双重任务，并基于这种双重结构设计了两个提示，分别结合这两个任务的任务相关知识。通过这种方式，DST 任务可以有效地表述为一种语言建模任务在少镜头设置下。在两个面向任务的对话数据集上的实验结果表明，该方法不仅优于现有的最先进的少镜头方法，而且能够产生看不见的时隙。结果表明，在快速学习的帮助下，可以从 PLM 中探索与 DST 相关的知识，并利用这些知识有效地解决低资源的 DST 问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Dual+Prompt+Learning+Framework+for+Few-Shot+Dialogue+State+Tracking)|0|
|[TTS: A Target-based Teacher-Student Framework for Zero-Shot Stance Detection](https://doi.org/10.1145/3543507.3583250)|Yingjie Li, Chenye Zhao, Cornelia Caragea|Computer Science, University of Illinois at Chicago, USA|The goal of zero-shot stance detection (ZSSD) is to identify the stance (in favor of, against, or neutral) of a text towards an unseen target in the inference stage. In this paper, we explore this problem from a novel angle by proposing a Target-based Teacher-Student learning (TTS) framework. Specifically, we first augment the training set by extracting diversified targets that are unseen during training with a keyphrase generation model. Then, we develop a teacher-student framework which effectively utilizes the augmented data. Extensive experiments show that our model significantly outperforms state-of-the-art ZSSD baselines on the available benchmark dataset for this task by 8.9% in macro-averaged F1. In addition, previous ZSSD requires human-annotated targets and labels during training, which may not be available in real-world applications. Therefore, we go one step further by proposing a more challenging open-world ZSSD task: identifying the stance of a text towards an unseen target without human-annotated targets and stance labels. We show that our TTS can be easily adapted to the new task. Remarkably, TTS without human-annotated targets and stance labels even significantly outperforms previous state-of-the-art ZSSD baselines trained with human-annotated data. We publicly release our code 1 to facilitate future research.|零拍姿态检测(ZSSD)的目标是在推理阶段识别文本对一个看不见的目标的姿态(支持、反对或中立)。本文从一个新的角度探讨了这个问题，提出了一个基于目标的师生学习(TTS)框架。具体来说，我们首先通过关键词生成模型提取训练过程中看不到的多样化目标来增加训练集。然后，我们开发了一个有效利用增强数据的师生框架。广泛的实验表明，我们的模型显着优于现有的 ZSSD 基准测试数据集的8.9% ，这项任务在宏观平均的 F1。此外，以前的 ZSSD 在培训期间需要人工注释的目标和标签，这在现实世界的应用程序中可能是不可用的。因此，我们更进一步，提出了一个更具挑战性的开放世界 ZSSD 任务: 确定文本对一个看不见的目标的立场，而没有人工注释的目标和立场标签。我们表明，我们的 TTS 可以很容易地适应新的任务。值得注意的是，没有人工注释目标和立场标签的 TTS 甚至明显优于以前用人工注释数据训练的最先进的 ZSSD 基线。我们公开发布代码1以促进未来的研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TTS:+A+Target-based+Teacher-Student+Framework+for+Zero-Shot+Stance+Detection)|0|
|[CL-WSTC: Continual Learning for Weakly Supervised Text Classification on the Internet](https://doi.org/10.1145/3543507.3583249)|Miaomiao Li, Jiaqi Zhu, Xin Yang, Yi Yang, Qiang Gao, Hongan Wang|Institute of Software, Chinese Academy of Sciences, China; Southwestern University of Finance and Economics, China; Institute of Software, Chinese Academy of Sciences, China and University of Chinese Academy of Sciences, China|Continual text classification is an important research direction in Web mining. Existing works are limited to supervised approaches relying on abundant labeled data, but in the open and dynamic environment of Internet, involving constant semantic change of known topics and the appearance of unknown topics, text annotations are hard to access in time for each period. That calls for the technique of weakly supervised text classification (WSTC), which requires just seed words for each category and has succeed in static text classification tasks. However, there are still no studies of applying WSTC methods in a continual learning paradigm to actually accommodate the open and evolving Internet. In this paper, we tackle this problem for the first time and propose a framework, named Continual Learning for Weakly Supervised Text Classification (CL-WSTC), which can take any WSTC method as base model. It consists of two modules, classification decision with delay and seed word updating. In the former, the probability threshold for each category in each period is adaptively learned to determine the acceptance/rejection of texts. In the latter, with candidate words output by the base model, seed words are added and deleted via reinforcement learning with immediate rewards, according to an empirically certified unsupervised measure. Extensive experiments show that our approach has strong universality and can achieve a better trade-off between classification accuracy and decision timeliness compared to non-continual counterparts, with intuitively interpretable updating of seed words.|连续文本分类是 Web 挖掘的一个重要研究方向。现有的著作仅限于依赖于大量标记数据的监督方法，但在互联网的开放动态环境中，涉及已知主题的语义不断变化和未知主题的出现，文本注释难以在每个时期及时获取。这就要求采用弱监督文本分类(WSTC)技术，对每个类别只需要种子词，并成功地完成了静态文本分类任务。然而，仍然没有研究应用 WSTC 方法在一个持续的学习范式，以实际适应开放和不断发展的互联网。本文首次提出了弱监督文本分类的持续学习框架(CL-WSTC) ，该框架可以采用任何 WSTC 方法作为基本模型。它包括延迟分类决策和种子词更新两个模块。在前者中，每个时间段内每个类别的概率阈值是自适应学习的，以确定文本的接受/拒绝。在后一种情况下，根据经验认证的无监督测量方法，在基础模型输出候选词的情况下，种子词通过带有即时奖励的强化学习被添加和删除。大量的实验表明，该方法具有很强的通用性，能够在分类精度和决策及时性之间取得比非连续对应方法更好的平衡，并能直观地解释种子词的更新。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CL-WSTC:+Continual+Learning+for+Weakly+Supervised+Text+Classification+on+the+Internet)|0|
|[Learning Robust Multi-Modal Representation for Multi-Label Emotion Recognition via Adversarial Masking and Perturbation](https://doi.org/10.1145/3543507.3583258)|Shiping Ge, Zhiwei Jiang, Zifeng Cheng, Cong Wang, Yafeng Yin, Qing Gu|State Key Laboratory for Novel Software Technology, Nanjing University, China|Recognizing emotions from multi-modal data is an emotion recognition task that requires strong multi-modal representation ability. The general approach to this task is to naturally train the representation model on training data without intervention. However, such natural training scheme is prone to modality bias of representation (i.e., tending to over-encode some informative modalities while neglecting other modalities) and data bias of training (i.e., tending to overfit training data). These biases may lead to instability (e.g., performing poorly when the neglected modality is dominant for recognition) and weak generalization (e.g., performing poorly when unseen data is inconsistent with overfitted data) of the model on unseen data. To address these problems, this paper presents two adversarial training strategies to learn more robust multi-modal representation for multi-label emotion recognition. Firstly, we propose an adversarial temporal masking strategy, which can enhance the encoding of other modalities by masking the most emotion-related temporal units (e.g., words for text or frames for video) of the informative modality. Secondly, we propose an adversarial parameter perturbation strategy, which can enhance the generalization of the model by adding the adversarial perturbation to the parameters of model. Both strategies boost model performance on the benchmark MMER datasets CMU-MOSEI and NEMu. Experimental results demonstrate the effectiveness of the proposed method compared with the previous state-of-the-art method. Code will be released at https://github.com/ShipingGe/MMER.|从多模态数据中识别情绪是一项需要很强的多模态表征能力的情绪识别任务。解决这一问题的一般方法是在不进行干预的情况下自然地对训练数据的表示模型进行训练。然而，这种自然的训练方案倾向于表征的模态偏差(即倾向于过度编码一些信息模式而忽略其他模式)和训练的数据偏差(即倾向于过度拟合训练数据)。这些偏差可能导致模型的不稳定性(例如，当被忽视的模式主要用于识别时表现不佳)和弱泛化(例如，当看不见的数据与过度拟合的数据不一致时表现不佳)。针对这些问题，本文提出了两种对抗性训练策略来学习多标签情绪识别的鲁棒性多模态表示。首先，本文提出了一种对抗性时间掩蔽策略，该策略通过掩蔽信息模式中与情绪最相关的时间单元(如文本中的词或视频中的帧)来增强其他模式的编码。其次，提出了一种对抗性参数摄动策略，通过在模型参数中加入对抗性摄动来提高模型的泛化能力。这两种策略都提高了基准 MMER 数据集 CMU-MOSEI 和 NEMu 的模型性能。实验结果表明，与已有的最新方法相比，本文提出的方法是有效的。密码将在 https://github.com/shipingge/mmer 公布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Robust+Multi-Modal+Representation+for+Multi-Label+Emotion+Recognition+via+Adversarial+Masking+and+Perturbation)|0|
|[Continual Few-shot Learning with Transformer Adaptation and Knowledge Regularization](https://doi.org/10.1145/3543507.3583262)|Xin Wang, Yue Liu, Jiapei Fan, Weigao Wen, Hui Xue, Wenwu Zhu|Alibaba Group, China; Department of Computer Science and Technology, Tsinghua University, China|Continual few-shot learning, as a paradigm that simultaneously solves continual learning and few-shot learning, has become a challenging problem in machine learning. An eligible continual few-shot learning model is expected to distinguish all seen classes upon new categories arriving, where each category only includes very few labeled data. However, existing continual few-shot learning methods only consider the visual modality, where the distributions of new categories often indistinguishably overlap with old categories, thus resulting in the severe catastrophic forgetting problem. To tackle this problem, in this paper we study continual few-shot learning with the assistance of semantic knowledge by simultaneously taking both visual modality and semantic concepts of categories into account. We propose a Continual few-shot learning algorithm with Semantic knowledge Regularization (CoSR) for adapting to the distribution changes of visual prototypes through a Transformer-based prototype adaptation mechanism. Specifically, the original visual prototypes from the backbone are fed into the well-designed Transformer with corresponding semantic concepts, where the semantic concepts are extracted from all categories. The semantic-level regularization forces the categories with similar semantics to be closely distributed, while the opposite ones are constrained to be far away from each other. The semantic regularization improves the model’s ability to distinguish between new and old categories, thus significantly mitigating the catastrophic forgetting problem in continual few-shot learning. Extensive experiments on CIFAR100, miniImageNet, CUB200 and an industrial dataset with long-tail distribution demonstrate the advantages of our CoSR model compared with state-of-the-art methods.|连续少镜头学习作为一种同时解决连续学习和少镜头学习的范式，已经成为机器学习中一个具有挑战性的问题。一个符合条件的连续少镜头学习模型被期望在新类别到达时区分所有看到的类，其中每个类别只包含很少的标记数据。然而，现有的连续少镜头学习方法只考虑视觉模态，新类别的分布往往与旧类别重叠不明显，从而导致严重的灾难性遗忘问题。为了解决这一问题，本文同时考虑了视觉情态和范畴的语义概念，研究了在语义知识辅助下的连续少镜头学习。提出了一种基于语义知识正则化(CoSR)的连续少镜头学习算法，该算法通过一种基于变换器的原型自适应机制来适应视觉原型的分布变化。具体来说，来自主干的原始可视化原型被输入到设计良好的 Transformer 中，其中包含相应的语义概念，语义概念从所有类别中提取出来。语义层次的正则化迫使具有相似语义的范畴紧密分布，而相反的范畴被约束得彼此远离。语义正则化提高了模型区分新旧类别的能力，从而显著减轻了连续少镜头学习中的灾难性遗忘问题。在 CIFAR100，miniImageNet，CUB200和具有长尾分布的工业数据集上的广泛实验证明了我们的 CoSR 模型与最先进的方法相比的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Continual+Few-shot+Learning+with+Transformer+Adaptation+and+Knowledge+Regularization)|0|
|[Open-World Social Event Classification](https://doi.org/10.1145/3543507.3583291)|Shengsheng Qian, Hong Chen, Dizhan Xue, Quan Fang, Changsheng Xu|Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences, China and Peng Cheng Laboratory, China; Henan Institute of Advanced Technology, Zhengzhou University, China; Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences, China|With the rapid development of Internet and the expanding scale of social media, social event classification has attracted increasing attention. The key to social event classification is effectively leveraging the visual and textual semantics for classification. However, most of the existing approaches may suffer from the following limitations: (1) Most of them just simply concatenate the image features and text features to get the multimodal features and ignore the fine-grained semantic relationship between modalities. (2) The majority of them hold the closed-world assumption that all classes in test are already seen in training, while this assumption can be easily broken in real-world applications. In practice, new events on Internet may not belong to any existing/seen class, and therefore cannot be correctly identified by closed-world learning algorithms. To tackle these challenges, we propose an Open-World Social Event Classifier (OWSEC) model in this paper. Firstly, we design a multimodal mask transformer network to capture cross-modal semantic relations and fuse fine-grained multimodal features of social events while masking redundant information. Secondly, we design an open-world classifier and propose a cross-modal event mixture mechanism with a novel open-world classification loss to capture the potential distribution space of the unseen class. Extensive experiments on two public datasets demonstrate the superiority of our proposed OWSEC model for open-world social event classification.|随着互联网的迅速发展和社会媒体规模的不断扩大，社会事件分类越来越受到人们的关注。社会事件分类的关键是有效地利用视觉语义和文本语义进行分类。然而，现有的方法大多存在以下局限性: (1)大多数方法只是简单地将图像特征和文本特征连接起来，得到多模态特征，而忽略了模态之间细粒度的语义关系。(2)他们大多数人持有的封闭世界的假设，所有的类在测试中已经看到在训练，而这一假设可以很容易地打破在现实世界的应用。在实践中，互联网上的新事件可能不属于任何现有的/可见的类，因此不能被封闭世界的学习算法正确识别。为了应对这些挑战，本文提出了一种开放世界社会事件分类器(OWSEC)模型。首先，我们设计了一个多模态掩模变压器网络来捕获跨模态的语义关系，融合社会事件的细粒度多模态特征，同时掩模冗余信息。其次，我们设计了一个开放世界分类器，并提出了一种具有新的开放世界分类损失的跨模态事件混合机制，以捕获看不见类的潜在分布空间。在两个公共数据集上的大量实验证明了我们提出的 OWSEC 模型在开放世界社会事件分类中的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Open-World+Social+Event+Classification)|0|
|[KHAN: Knowledge-Aware Hierarchical Attention Networks for Accurate Political Stance Prediction](https://doi.org/10.1145/3543507.3583300)|YunYong Ko, Seongeun Ryu, Soeun Han, Youngseung Jeon, Jaehoon Kim, Sohyun Park, Kyungsik Han, Hanghang Tong, SangWook Kim|Hanyang University, Republic of Korea; Ajou University, Republic of Korea; University of California, Los Angeles, USA; University of Illinois at Urbana-Champaign, USA|The political stance prediction for news articles has been widely studied to mitigate the echo chamber effect -- people fall into their thoughts and reinforce their pre-existing beliefs. The previous works for the political stance problem focus on (1) identifying political factors that could reflect the political stance of a news article and (2) capturing those factors effectively. Despite their empirical successes, they are not sufficiently justified in terms of how effective their identified factors are in the political stance prediction. Motivated by this, in this work, we conduct a user study to investigate important factors in political stance prediction, and observe that the context and tone of a news article (implicit) and external knowledge for real-world entities appearing in the article (explicit) are important in determining its political stance. Based on this observation, we propose a novel knowledge-aware approach to political stance prediction (KHAN), employing (1) hierarchical attention networks (HAN) to learn the relationships among words and sentences in three different levels and (2) knowledge encoding (KE) to incorporate external knowledge for real-world entities into the process of political stance prediction. Also, to take into account the subtle and important difference between opposite political stances, we build two independent political knowledge graphs (KG) (i.e., KG-lib and KG-con) by ourselves and learn to fuse the different political knowledge. Through extensive evaluations on three real-world datasets, we demonstrate the superiority of DASH in terms of (1) accuracy, (2) efficiency, and (3) effectiveness.|新闻文章的政治立场预测已被广泛研究，以减轻回声室效应——人们陷入他们的思想和加强他们先前存在的信念。以往关于政治立场问题的研究主要集中在: (1)识别能够反映新闻文章政治立场的政治因素; (2)有效地捕捉这些因素。尽管它们在实践中取得了成功，但就其所确定的因素在预测政治立场方面的有效性而言，它们没有得到充分的证明。在此基础上，本文进行了一项用户研究，考察了政治立场预测中的重要因素，发现新闻文章的语境和语气(隐含)以及文章中出现的现实世界实体的外部知识(显性)对于确定其政治立场非常重要。在此基础上，我们提出了一种新的知识感知的政治立场预测方法(KHAN) ，该方法采用(1)分层注意网络(HAN)来学习三个不同层次的词语和句子之间的关系，(2)知识编码(KE)将现实世界实体的外部知识融入政治立场预测过程。同时，为了考虑对立政治立场之间微妙而重要的差异，我们自己构建了两个独立的政治知识图(即 KG-lib 和 KG-con) ，并学会了融合不同的政治知识。通过对三个实际数据集的广泛评估，我们证明了 DASH 在(1)准确性、(2)效率和(3)有效性方面的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KHAN:+Knowledge-Aware+Hierarchical+Attention+Networks+for+Accurate+Political+Stance+Prediction)|0|
|[Improving (Dis)agreement Detection with Inductive Social Relation Information From Comment-Reply Interactions](https://doi.org/10.1145/3543507.3583314)|Yun Luo, Zihan Liu, Stan Z. Li, Yue Zhang|Westlake University, China; Westlake university, China|(Dis)agreement detection aims to identify the authors' attitudes or positions (\textit{{agree, disagree, neutral}}) towards a specific text. It is limited for existing methods merely using textual information for identifying (dis)agreements, especially for cross-domain settings. Social relation information can play an assistant role in the (dis)agreement task besides textual information. We propose a novel method to extract such relation information from (dis)agreement data into an inductive social relation graph, merely using the comment-reply pairs without any additional platform-specific information. The inductive social relation globally considers the historical discussion and the relation between authors. Textual information based on a pre-trained language model and social relation information encoded by pre-trained RGCN are jointly considered for (dis)agreement detection. Experimental results show that our model achieves state-of-the-art performance for both the in-domain and cross-domain tasks on the benchmark -- DEBAGREEMENT. We find social relations can boost the performance of the (dis)agreement detection model, especially for the long-token comment-reply pairs, demonstrating the effectiveness of the social relation graph. We also explore the effect of the knowledge graph embedding methods, the information fusing method, and the time interval in constructing the social relation graph, which shows the effectiveness of our model.|协议检测的目的是识别作者对特定文本的态度或立场(文本{{同意，不同意，中立}})。对于仅仅使用文本信息来识别(dis)协议的现有方法，尤其是对于跨域设置，它是有限的。除了文本信息外，社会关系信息还可以在异议任务中发挥辅助作用。我们提出了一种新的方法来提取这种关系信息从(的)协议数据到归纳的社会关系图，只使用评论-回复对没有任何额外的平台特定的信息。归纳社会关系从整体上考虑历史讨论和作者之间的关系。将基于预训练语言模型的文本信息和预训练 RGCN 编码的社会关系信息联合起来进行异议检测。实验结果表明，该模型在基准 DEBAGREEMENT 上实现了领域内和跨领域任务的最佳性能。我们发现社会关系可以提高异议检测模型的性能，特别是对于长令牌的评论-回复对，证明了社会关系图的有效性。探讨了知识图嵌入方法、信息融合方法和时间间隔对构建社会关系图的影响，表明了该模型的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+(Dis)agreement+Detection+with+Inductive+Social+Relation+Information+From+Comment-Reply+Interactions)|0|
|[Dynalogue: A Transformer-Based Dialogue System with Dynamic Attention](https://doi.org/10.1145/3543507.3583330)|Rongjunchen Zhang, Tingmin Wu, Xiao Chen, Sheng Wen, Surya Nepal, Cécile Paris, Yang Xiang|Swinburne University of Technology, Australia; CSIRO's Data61, Australia; Monash University, Australia|Businesses face a range of cyber risks, both external threats and internal vulnerabilities that continue to evolve over time. As cyber attacks continue to increase in complexity and sophistication, more organisations will experience them. For this reason, it is important that organisations seek timely consultancy from cyber professionals so that they can respond to and recover from cyber attacks as quickly as possible. However, huge surges in cyber attacks have long left cyber professionals short of what is required to cover the security needs. This problem is getting worse when an increasing number of people choose to work from home during the pandemic because this situation usually yields extra communication cost. In this paper, we propose to develop a cybersecurity-oriented dialogue system, called Dynalogue1, which can provide consultancy online as a cyber professional. For the first time, Dynalogue provides a promising solution to mitigate the need for cyber professionals via automatically generating problem-targeted conversions to victims of cyber attacks. In spite of many dialogue systems developed in the past, Dynalogue provides a distinct capability of handling long and complicated sentences that are common in cybersecurity-related conversations. It is challenging to have this capability because limited memory in dialogue systems can be hard to accommodate sufficient key information of long sentences. To overcome this challenge, Dynalogue utilises an attention mechanism that dynamically captures key semantics within a sentence instead of using fix window to cut off the sentence. To evaluate Dynalogue, we collect 67K real-world conversations (0.6M utterances) from Bleeping Computer2, which is one of the most popular cybersecurity consultancy websites in the world. The results suggest that Dynalogue outperforms all the existing dialogue systems with 1% ∼ 9% improvements on all different metrics. We further run Dynalogue on the public dataset WikiHow to validate its compatibility in other domains where conversations are also long and complicated. Dynalogue also outperforms all the other methods with at most 2.4% improvement.|企业面临一系列网络风险，包括外部威胁和内部漏洞，这些风险将随着时间的推移不断演变。随着网络攻击的复杂性和复杂程度不断提高，将有更多组织经历这些攻击。基于这个原因，机构必须及时向网络专业人士寻求咨询，以便他们能够尽快应对和恢复网络攻击。然而，长期以来，网络攻击的激增使得网络专业人士无法满足安全需求。当越来越多的人在大流行期间选择在家工作时，这个问题变得更加严重，因为这种情况通常会产生额外的通信费用。在本文中，我们建议开发一个面向网络安全的对话系统，称为 Dynalogue1，它可以作为一个网络专业人员提供在线咨询。Dynalog 首次提供了一个有前途的解决方案，通过自动生成针对问题的转换，减轻对网络专业人员的需求，转换为网络攻击的受害者。尽管过去开发了许多对话系统，Dynalog 提供了一种独特的能力，可以处理与网络安全相关的对话中常见的长而复杂的句子。具备这种能力是具有挑战性的，因为在对话系统中，有限的记忆可能难以容纳足够的长句关键信息。为了克服这个挑战，Dynalog 使用了一种注意机制，动态捕捉句子中的关键语义，而不是使用修复窗口来切断句子。为了评估 Dynalog，我们收集了来自 Bleping Computer2(世界上最受欢迎的网络安全咨询网站之一)的67K 真实世界的对话(0.6 M 的话语)。实验结果表明，Dynalog 的性能优于现有的所有对话系统，在所有不同指标上的性能提高了1% -9% 。我们进一步在公共数据集 WikiHow 上运行 Dynalog，以验证其在其他领域的兼容性，因为这些领域的会话也是冗长而复杂的。动态模拟也优于所有其他方法，最多有2.4% 的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynalogue:+A+Transformer-Based+Dialogue+System+with+Dynamic+Attention)|0|
|[Active Learning from the Web](https://doi.org/10.1145/3543507.3583346)|Ryoma Sato|Kyoto University, Japan and RIKEN AIP, Japan|Labeling data is one of the most costly processes in machine learning pipelines. Active learning is a standard approach to alleviating this problem. Pool-based active learning first builds a pool of unlabelled data and iteratively selects data to be labeled so that the total number of required labels is minimized, keeping the model performance high. Many effective criteria for choosing data from the pool have been proposed in the literature. However, how to build the pool is less explored. Specifically, most of the methods assume that a task-specific pool is given for free. In this paper, we advocate that such a task-specific pool is not always available and propose the use of a myriad of unlabelled data on the Web for the pool for which active learning is applied. As the pool is extremely large, it is likely that relevant data exist in the pool for many tasks, and we do not need to explicitly design and build the pool for each task. The challenge is that we cannot compute the acquisition scores of all data exhaustively due to the size of the pool. We propose an efficient method, Seafaring, to retrieve informative data in terms of active learning from the Web using a user-side information retrieval algorithm. In the experiments, we use the online Flickr environment as the pool for active learning. This pool contains more than ten billion images and is several orders of magnitude larger than the existing pools in the literature for active learning. We confirm that our method performs better than existing approaches of using a small unlabelled pool.|标记数据是机器学习管道中最昂贵的过程之一。主动学习是缓解这一问题的标准方法。基于池的主动学习首先构建一个未标记数据池，并迭代地选择要标记的数据，以使所需标记的总数最小化，从而保持模型的高性能。文献中已经提出了许多有效的数据选择标准。但是，如何构建池的探索较少。具体来说，大多数方法都假设特定于任务的池是免费提供的。在本文中，我们主张这样一个特定于任务的池并不总是可用的，并建议在 Web 上使用大量未标记的数据作为应用主动学习的池。由于池非常大，因此很可能存在许多任务的相关数据，我们不需要为每个任务显式设计和构建池。挑战在于，由于池的大小，我们无法详尽地计算所有数据的获取分数。我们提出了一种有效的方法，Seafaring，通过使用用户端的信息检索算法从网络中检索主动学习方面的信息数据。在实验中，我们使用在线 Flickr 环境作为主动学习的池。这个数量级包含超过一百亿张图片，比文献中现有的主动学习图片库还要大好几倍。我们确认我们的方法比使用一个小的未标记池的现有方法性能更好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Active+Learning+from+the+Web)|0|
|[The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study](https://doi.org/10.1145/3543507.3583354)|Yu Zhang, Bowen Jin, Qi Zhu, Yu Meng, Jiawei Han|University of Illinois at Urbana-Champaign, USA|Due to the exponential growth of scientific publications on the Web, there is a pressing need to tag each paper with fine-grained topics so that researchers can track their interested fields of study rather than drowning in the whole literature. Scientific literature tagging is beyond a pure multi-label text classification task because papers on the Web are prevalently accompanied by metadata information such as venues, authors, and references, which may serve as additional signals to infer relevant tags. Although there have been studies making use of metadata in academic paper classification, their focus is often restricted to one or two scientific fields (e.g., computer science and biomedicine) and to one specific model. In this work, we systematically study the effect of metadata on scientific literature tagging across 19 fields. We select three representative multi-label classifiers (i.e., a bag-of-words model, a sequence-based model, and a pre-trained language model) and explore their performance change in scientific literature tagging when metadata are fed to the classifiers as additional features. We observe some ubiquitous patterns of metadata's effects across all fields (e.g., venues are consistently beneficial to paper tagging in almost all cases), as well as some unique patterns in fields other than computer science and biomedicine, which are not explored in previous studies.|由于互联网上科学出版物的指数增长，迫切需要给每篇论文贴上细致的标签，这样研究人员就可以跟踪他们感兴趣的研究领域，而不是淹没在整篇文献中。科学文献标注不仅仅是一个纯粹的多标签文本分类任务，因为网络上的文献通常伴随着元数据信息，如地点、作者和参考文献，这些元数据信息可以作为推断相关标签的附加信号。尽管在学术论文分类中使用元数据的研究已经出现，但是它们的重点往往局限于一个或两个科学领域(例如，计算机科学和生物医学)和一个特定的模型。本文系统地研究了元数据对19个领域的科技文献标注的影响。我们选择了三个有代表性的多标签分类器(即，一个词袋模型，一个基于序列的模型和一个预先训练的语言模型) ，并探讨了当元数据作为额外特征提供给分类器时，它们在科学文献标签中的性能变化。我们观察到在所有领域中元数据影响的一些无处不在的模式(例如，场所在几乎所有情况下都始终有利于纸张标签) ，以及在计算机科学和生物医学以外的领域中的一些独特模式，这在以前的研究中没有探索。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Effect+of+Metadata+on+Scientific+Literature+Tagging:+A+Cross-Field+Cross-Model+Study)|0|
|["Why is this misleading?": Detecting News Headline Hallucinations with Explanations](https://doi.org/10.1145/3543507.3583375)|Jiaming Shen, Jialu Liu, Daniel Finnie, Negar Rahmati, Mike Bendersky, Marc Najork|Google, USA|Automatic headline generation enables users to comprehend ongoing news events promptly and has recently become an important task in web mining and natural language processing. With the growing need for news headline generation, we argue that the hallucination issue, namely the generated headlines being not supported by the original news stories, is a critical challenge for the deployment of this feature in web-scale systems Meanwhile, due to the infrequency of hallucination cases and the requirement of careful reading for raters to reach the correct consensus, it is difficult to acquire a large dataset for training a model to detect such hallucinations through human curation. In this work, we present a new framework named ExHalder to address this challenge for headline hallucination detection. ExHalder adapts the knowledge from public natural language inference datasets into the news domain and learns to generate natural language sentences to explain the hallucination detection results. To evaluate the model performance, we carefully collect a dataset with more than six thousand labeled <article, headline> pairs. Extensive experiments on this dataset and another six public ones demonstrate that ExHalder can identify hallucinated headlines accurately and justifies its predictions with human-readable natural language explanations.|自动生成标题使用户能够迅速理解正在进行的新闻事件，近年来已成为网络挖掘和自然语言处理中的一项重要任务。随着对新闻标题生成的需求日益增长，我们认为，幻觉问题，即生成的标题没有得到原始新闻故事的支持，是在网络规模系统中部署这一功能的一个关键挑战。同时，由于幻觉案例的频率很低，而且评估者需要仔细阅读才能达成正确的共识，因此很难获得一个大型数据集来训练模型，通过人类管理来检测这种幻觉。在这项工作中，我们提出了一个新的框架名为 ExHalder，以解决这一挑战的标题幻觉检测。ExHalder 将来自公共自然语言推理数据集的知识应用到新闻领域，并学习生成自然语言句子来解释幻觉检测结果。为了评估模型的性能，我们仔细地收集了超过六千个标有“文章，标题 > 对”的数据集。在这个数据集和另外六个公共数据集上的大量实验表明，ExHalder 可以准确地识别出幻觉标题，并用人类可读的自然语言解释来证明其预测的正确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q="Why+is+this+misleading?":+Detecting+News+Headline+Hallucinations+with+Explanations)|0|
|[DIWIFT: Discovering Instance-wise Influential Features for Tabular Data](https://doi.org/10.1145/3543507.3583382)|Dugang Liu, Pengxiang Cheng, Hong Zhu, Xing Tang, Yanyu Chen, Xiaoting Wang, Weike Pan, Zhong Ming, Xiuqiang He|Fit, Tencent, China; Huawei Technologies Co Ltd, China; College of Computer Science and Software Engineering, Shenzhen University, China; FIT, Tencent, China; Shenzhen University, China; Tsinghua-Berkeley Shenzhen Institute, China|Tabular data is one of the most common data storage formats behind many real-world web applications such as retail, banking, and e-commerce. The success of these web applications largely depends on the ability of the employed machine learning model to accurately distinguish influential features from all the predetermined features in tabular data. Intuitively, in practical business scenarios, different instances should correspond to different sets of influential features, and the set of influential features of the same instance may vary in different scenarios. However, most existing methods focus on global feature selection assuming that all instances have the same set of influential features, and few methods considering instance-wise feature selection ignore the variability of influential features in different scenarios. In this paper, we first introduce a new perspective based on the influence function for instance-wise feature selection, and give some corresponding theoretical insights, the core of which is to use the influence function as an indicator to measure the importance of an instance-wise feature. We then propose a new solution for discovering instance-wise influential features in tabular data (DIWIFT), where a self-attention network is used as a feature selection model and the value of the corresponding influence function is used as an optimization objective to guide the model. Benefiting from the advantage of the influence function, i.e., its computation does not depend on a specific architecture and can also take into account the data distribution in different scenarios, our DIWIFT has better flexibility and robustness. Finally, we conduct extensive experiments on both synthetic and real-world datasets to validate the effectiveness of our DIWIFT.|表格数据是许多真实网络应用程序(如零售、银行和电子商务)背后最常见的数据存储格式之一。这些 Web 应用程序的成功很大程度上取决于所使用的机器学习模型能够准确地区分表格数据中所有预先确定的特征和有影响的特征。直观地说，在实际的业务场景中，不同的实例应该对应于不同的影响特性集，同一实例的影响特性集在不同的场景中可能会有所不同。然而，现有的方法大多集中于全局特征选择，假设所有实例具有相同的影响特征集，很少有方法考虑实例特征选择忽略了不同场景中影响特征的可变性。本文首先介绍了基于影响函数的实例特征选择的新视角，并给出了相应的理论见解，其核心是利用影响函数作为指标来衡量实例特征的重要性。然后提出了一种新的表格数据实例影响特征发现方法(DIWIFT) ，该方法采用自注意网络作为特征选择模型，并以相应影响函数的值作为优化目标来指导模型。利用影响函数的优点，即它的计算不依赖于特定的体系结构，也可以考虑不同场景中的数据分布，我们的 DIWIFT 具有更好的灵活性和鲁棒性。最后，我们在合成数据集和真实数据集上进行了广泛的实验，以验证我们的 DIWIFT 算法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DIWIFT:+Discovering+Instance-wise+Influential+Features+for+Tabular+Data)|0|
|[XWikiGen: Cross-lingual Summarization for Encyclopedic Text Generation in Low Resource Languages](https://doi.org/10.1145/3543507.3583405)|Dhaval Taunk, Shivprasad Sagare, Anupam Patil, Shivansh Subramanian, Manish Gupta, Vasudeva Varma|International Institute of Information Technology, Hyderabad, India; International Institute of Information Technology, Hyderabad, India and Microsoft India, India; SCTR's Pune Institute of Computer Technology, India|Lack of encyclopedic text contributors, especially on Wikipedia, makes automated text generation for low resource (LR) languages a critical problem. Existing work on Wikipedia text generation has focused on English only where English reference articles are summarized to generate English Wikipedia pages. But, for low-resource languages, the scarcity of reference articles makes monolingual summarization ineffective in solving this problem. Hence, in this work, we propose XWikiGen, which is the task of cross-lingual multi-document summarization of text from multiple reference articles, written in various languages, to generate Wikipedia-style text. Accordingly, we contribute a benchmark dataset, XWikiRef, spanning ~69K Wikipedia articles covering five domains and eight languages. We harness this dataset to train a two-stage system where the input is a set of citations and a section title and the output is a section-specific LR summary. The proposed system is based on a novel idea of neural unsupervised extractive summarization to coarsely identify salient information followed by a neural abstractive model to generate the section-specific text. Extensive experiments show that multi-domain training is better than the multi-lingual setup on average.|缺乏百科全书式的文本贡献者，特别是在 Wikipedia 上，使得低资源(LR)语言的自动文本生成成为一个关键问题。现有的维基百科文本生成工作主要集中在英文方面，即将英文参考文章进行总结以生成英文维基百科页面。但是，对于资源匮乏的语言来说，参考文献的稀缺性使得单语文摘无法有效地解决这一问题。因此，在这项工作中，我们提出 XWikiGen，这是一个跨语言的多文档摘要文本的任务，从多个参考文章，写在不同的语言，生成维基百科风格的文本。因此，我们提供了一个基准数据集，XWikiRef，涵盖了5个域和8种语言的69K 维基百科文章。我们利用这个数据集来训练一个两阶段的系统，其中输入是一组引文和一个章节标题，输出是一个章节特定的 LR 摘要。该系统基于神经元无监督提取概括的新思想，对显著信息进行粗略识别，然后利用神经元抽象模型生成特定部分的文本。大量实验表明，多领域训练平均优于多语种训练。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=XWikiGen:+Cross-lingual+Summarization+for+Encyclopedic+Text+Generation+in+Low+Resource+Languages)|0|
|[Learning Structural Co-occurrences for Structured Web Data Extraction in Low-Resource Settings](https://doi.org/10.1145/3543507.3583387)|Zhenyu Zhang, Bowen Yu, Tingwen Liu, Tianyun Liu, Yubin Wang, Li Guo|Institute of Information Engineering, Chinese Academy of Sciences, China and School of Cyber Security, University of Chinese Academy of Sciences, China|Extracting structured information from all manner of webpages is an important problem with the potential to automate many real-world applications. Recent work has shown the effectiveness of leveraging DOM trees and pre-trained language models to describe and encode webpages. However, they typically optimize the model to learn the semantic co-occurrence of elements and labels in the same webpage, thus their effectiveness depends on sufficient labeled data, which is labor-intensive. In this paper, we further observe structural co-occurrences in different webpages of the same website: the same position in the DOM tree usually plays the same semantic role, and the DOM nodes in this position also share similar surface forms. Motivated by this, we propose a novel method, Structor, to effectively incorporate the structural co-occurrences over DOM tree and surface form into pre-trained language models. Such structural co-occurrences help the model learn the task better under low-resource settings, and we study two challenging experimental scenarios: website-level low-resource setting and webpage-level low-resource setting, to evaluate our approach. Extensive experiments on the public SWDE dataset show that Structor significantly outperforms the state-of-the-art models in both settings, and even achieves three times the performance of the strong baseline model in the case of extreme lack of training data.|从各种各样的网页中提取结构化信息是一个重要的问题，它有可能使许多现实世界的应用程序自动化。最近的工作已经显示了利用 DOM 树和预先训练的语言模型来描述和编码网页的有效性。然而，它们通常优化模型来学习同一网页中元素和标签的语义共现，因此它们的有效性依赖于足够的标签数据，这是劳动密集型的。在本文中，我们进一步观察了同一网站的不同网页在结构上的共现: 同一位置的 DOM 树通常扮演着相同的语义角色，而这个位置的 DOM 节点也具有相似的表面形式。受此启发，我们提出了一种新的方法，Structor，有效地将结构共现的 DOM 树和表面形式预先训练的语言模型。这种结构性共现有助于模型在低资源环境下更好地学习任务，我们研究了两个具有挑战性的实验场景: 网站级低资源环境和网页级低资源环境，以评估我们的方法。在公开的 SWDE 数据集上进行的大量实验表明，在这两种情况下，Structor 的性能都明显优于最先进的模型，甚至在极度缺乏训练数据的情况下，它的性能是强基线模型的三倍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Structural+Co-occurrences+for+Structured+Web+Data+Extraction+in+Low-Resource+Settings)|0|
|[TMMDA: A New Token Mixup Multimodal Data Augmentation for Multimodal Sentiment Analysis](https://doi.org/10.1145/3543507.3583406)|Xianbing Zhao, Yixin Chen, Sicen Liu, Xuan Zang, Yang Xiang, Buzhou Tang|Peng Cheng Laboratory, Shenzhen, China, China; Harbin Institute of Technology (Shenzhen), China|Existing methods for Multimodal Sentiment Analysis (MSA) mainly focus on integrating multimodal data effectively on limited multimodal data. Learning more informative multimodal representation often relies on large-scale labeled datasets, which are difficult and unrealistic to obtain. To learn informative multimodal representation on limited labeled datasets as more as possible, we proposed TMMDA for MSA, a new Token Mixup Multimodal Data Augmentation, which first generates new virtual modalities from the mixed token-level representation of raw modalities, and then enhances the representation of raw modalities by utilizing the representation of the generated virtual modalities. To preserve semantics during virtual modality generation, we propose a novel cross-modal token mixup strategy based on the generative adversarial network. Extensive experiments on two benchmark datasets, i.e., CMU-MOSI and CMU-MOSEI, verify the superiority of our model compared with several state-of-the-art baselines. The code is available at https://github.com/xiaobaicaihhh/TMMDA.|现有的多模态情绪分析(MSA)方法主要集中在有限的多模态数据上有效地集成多模态数据。学习更多信息的多模态表示通常依赖于大规模的标记数据集，这是困难的和不现实的获得。为了尽可能多地学习有限标记数据集上的信息化多模态表示，我们提出了一种新的令牌混合多模态数据增强算法 MSA-TMMDA，该算法首先利用原始模态的混合令牌级表示生成新的虚拟模态，然后利用生成的虚拟模态的表示来增强原始模态的表示。为了在虚拟形态生成过程中保持语义，提出了一种新的基于生成对抗网络的跨模态令牌混合策略。通过对 CMU-MOSI 和 CMU-MOSEI 两个基准数据集的大量实验，验证了该模型相对于几个最先进的基准线的优越性。密码可在 https://github.com/xiaobaicaihhh/tmmda 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TMMDA:+A+New+Token+Mixup+Multimodal+Data+Augmentation+for+Multimodal+Sentiment+Analysis)|0|
|[Node-wise Diffusion for Scalable Graph Learning](https://doi.org/10.1145/3543507.3583408)|Keke Huang, Jing Tang, Juncheng Liu, Renchi Yang, Xiaokui Xiao|Hong Kong Baptist University, Hong Kong; The Hong Kong University of Science and Technology (Guangzhou), China and The Hong Kong Uni. of Sci. and Tech., Hong Kong; [email protected]; National University of Singapore, Singapore|Graph Neural Networks (GNNs) have shown superior performance for semi-supervised learning of numerous web applications, such as classification on web services and pages, analysis of online social networks, and recommendation in e-commerce. The state of the art derives representations for all nodes in graphs following the same diffusion (message passing) model without discriminating their uniqueness. However, (i) labeled nodes involved in model training usually account for a small portion of graphs in the semi-supervised setting, and (ii) different nodes locate at different graph local contexts and it inevitably degrades the representation qualities if treating them undistinguishedly in diffusion. To address the above issues, we develop NDM, a universal node-wise diffusion model, to capture the unique characteristics of each node in diffusion, by which NDM is able to yield high-quality node representations. In what follows, we customize NDM for semi-supervised learning and design the NIGCN model. In particular, NIGCN advances the efficiency significantly since it (i) produces representations for labeled nodes only and (ii) adopts well-designed neighbor sampling techniques tailored for node representation generation. Extensive experimental results on various types of web datasets, including citation, social and co-purchasing graphs, not only verify the state-of-the-art effectiveness of NIGCN but also strongly support the remarkable scalability of NIGCN. In particular, NIGCN completes representation generation and training within 10 seconds on the dataset with hundreds of millions of nodes and billions of edges, up to orders of magnitude speedups over the baselines, while achieving the highest F1-scores on classification.|图形神经网络(GNN)在许多网络应用程序的半监督学习方面表现出卓越的性能，例如对网络服务和页面的分类、对在线社交网络的分析以及电子商务中的推荐。目前的技术状况是在不区分节点唯一性的前提下，推导出图中所有节点遵循相同扩散(消息传递)模型的表示形式。然而，(i)模型训练中的标记节点在半监督环境中通常只占图的一小部分，(ii)不同的节点位于不同的图局部上下文中，如果在扩散中不加区分地处理它们，则不可避免地会降低表示质量。为了解决上述问题，我们开发了 NDM，一个通用的节点扩散模型，以捕捉每个节点在扩散中的独特特征，通过 NDM 能够产生高质量的节点表示。接下来，我们为半监督学习定制了 NDM，并设计了 NIGCN 模型。特别地，NIGCN 显著提高了效率，因为它(i)仅产生标记节点的表示，(ii)采用了为节点表示生成量身定制的精心设计的邻居抽样技术。在各种类型的网络数据集(包括引用、社会和共同购买图表)上的广泛实验结果不仅验证了 NIGCN 的最新有效性，而且强烈支持 NIGCN 的显著可扩展性。特别是，NIGCN 在10秒内完成数据集上数亿个节点和数十亿条边的表示生成和训练，在基线上数量级加速，同时在分类上获得最高的 f 1分数。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Node-wise+Diffusion+for+Scalable+Graph+Learning)|0|
|[MetaTroll: Few-shot Detection of State-Sponsored Trolls with Transformer Adapters](https://doi.org/10.1145/3543507.3583417)|Lin Tian, Xiuzhen Zhang, Jey Han Lau|The University of Melbourne, Australia; RMIT University, Australia|State-sponsored trolls are the main actors of influence campaigns on social media and automatic troll detection is important to combat misinformation at scale. Existing troll detection models are developed based on training data for known campaigns (e.g.\ the influence campaign by Russia's Internet Research Agency on the 2016 US Election), and they fall short when dealing with {\em novel} campaigns with new targets. We propose MetaTroll, a text-based troll detection model based on the meta-learning framework that enables high portability and parameter-efficient adaptation to new campaigns using only a handful of labelled samples for few-shot transfer. We introduce \textit{campaign-specific} transformer adapters to MetaTroll to ``memorise'' campaign-specific knowledge so as to tackle catastrophic forgetting, where a model ``forgets'' how to detect trolls from older campaigns due to continual adaptation. Our experiments demonstrate that MetaTroll substantially outperforms baselines and state-of-the-art few-shot text classification models. Lastly, we explore simple approaches to extend MetaTroll to multilingual and multimodal detection. Source code for MetaTroll is available at: https://github.com/ltian678/metatroll-code.git.|国家赞助的喷子是影响社交媒体运动的主要行为者，自动检测喷子对于大规模打击错误信息非常重要。现有的网络喷子侦测模型是基于已知竞选活动的训练数据(例如俄罗斯互联网研究机构对2016年美国大选的影响力竞选)开发的，在处理有新目标的“新奇”竞选活动时，它们存在不足。我们提出了 MetaTroll，一个基于元学习框架的基于文本的喷子检测模型，该模型能够实现高可移植性和参数高效适应新的广告活动，只需使用少量标记样本进行短镜头传输。我们在 MetaTroll 中引入了文本{战役特定}变压器适配器，以“记忆”战役特定的知识，从而解决灾难性遗忘问题，即由于不断的适应，模型“忘记”如何检测来自旧战役的巨魔。我们的实验表明，MetaTroll 的性能大大优于基线和最先进的少镜头文本分类模型。最后，我们探索了将 MetaTroll 扩展到多语言和多模式检测的简单方法。MetaTroll 的源代码可以在以下 https://github.com/ltian678/MetaTroll-code.git 找到:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MetaTroll:+Few-shot+Detection+of+State-Sponsored+Trolls+with+Transformer+Adapters)|0|
|[EmpMFF: A Multi-factor Sequence Fusion Framework for Empathetic Response Generation](https://doi.org/10.1145/3543507.3583438)|Xiaobing Pang, Yequan Wang, Siqi Fan, Lisi Chen, Shuo Shang, Peng Han|University Of Electronic Science And Technology Of China, China; Beijing Academy of Artificial Intelligence, China|Empathy is one of the fundamental abilities of dialog systems. In order to build more intelligent dialogue systems, it’s important to learn how to demonstrate empathy toward others. Existing studies focus on identifying and leveraging the user’s coarse emotion to generate empathetic responses. However, human emotion and dialog act (e.g., intent) evolve as the talk goes along in an empathetic dialogue. This leads to the generated responses with very different intents from the human responses. As a result, empathy failure is ultimately caused. Therefore, using fine-grained emotion and intent sequential data on conversational emotions and dialog act is crucial for empathetic response generation. On the other hand, existing empathy models overvalue the empathy of responses while ignoring contextual relevance, which results in repetitive model-generated responses. To address these issues, we propose a Multi-Factor sequence Fusion framework (EmpMFF) based on conditional variational autoencoder. To generate empathetic responses, the proposed EmpMFF encodes a combination of contextual, emotion, and intent information into a continuous latent variable, which is then fed into the decoder. Experiments on the EmpatheticDialogues benchmark dataset demonstrate that EmpMFF exhibits exceptional performance in both automatic and human evaluations.|移情是对话系统的基本能力之一。为了建立更加智能的对话系统，学习如何对他人表现出同理心是很重要的。现有的研究集中在识别和利用用户的粗糙的情绪，以产生移情反应。然而，人类的情感和对话行为(例如，意图)是随着对话的进行而发展的。这导致所产生的反应与人类的反应有着非常不同的意图。因此，移情失败是最终导致的。因此，利用细粒度的情绪和意图序列数据对会话情绪和对话行为进行研究对于产生移情反应是至关重要的。另一方面，现有的移情模型高估了反应的移情作用，而忽视了语境相关性，从而导致重复的模型产生反应。为了解决这些问题，我们提出了一种基于条件变分自动编码器的多因子序列融合框架(EmpMFF)。为了产生移情反应，所提出的 EmpMFF 将上下文、情感和意图信息的组合编码成一个连续的潜变量，然后将其输入解码器。在 EmpatheticDialog 基准数据集上的实验表明，EmpMFF 在自动评估和人工评估方面都表现出非凡的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EmpMFF:+A+Multi-factor+Sequence+Fusion+Framework+for+Empathetic+Response+Generation)|0|
|[CEIL: A General Classification-Enhanced Iterative Learning Framework for Text Clustering](https://doi.org/10.1145/3543507.3583457)|Mingjun Zhao, Mengzhen Wang, Yinglong Ma, Di Niu, Haijiang Wu|North China Electric Power University, China; DiDi Global, China; Electrical & Computer Engineering, University of Alberta, Canada|Text clustering, as one of the most fundamental challenges in unsupervised learning, aims at grouping semantically similar text segments without relying on human annotations. With the rapid development of deep learning, deep clustering has achieved significant advantages over traditional clustering methods. Despite the effectiveness, most existing deep text clustering methods rely heavily on representations pre-trained in general domains, which may not be the most suitable solution for clustering in specific target domains. To address this issue, we propose CEIL, a novel Classification-Enhanced Iterative Learning framework for short text clustering, which aims at generally promoting the clustering performance by introducing a classification objective to iteratively improve feature representations. In each iteration, we first adopt a language model to retrieve the initial text representations, from which the clustering results are collected using our proposed Category Disentangled Contrastive Clustering (CDCC) algorithm. After strict data filtering and aggregation processes, samples with clean category labels are retrieved, which serve as supervision information to update the language model with the classification objective via a prompt learning approach. Finally, the updated language model with improved representation ability is used to enhance clustering in the next iteration. Extensive experiments demonstrate that the CEIL framework significantly improves the clustering performance over iterations, and is generally effective on various clustering algorithms. Moreover, by incorporating CEIL on CDCC, we achieve the state-of-the-art clustering performance on a wide range of short text clustering benchmarks outperforming other strong baseline methods.|文本聚类是非监督式学习领域最基本的挑战之一，其目的是在不依赖人工注释的情况下对语义相似的文本片段进行聚类。随着深度学习的快速发展，深度聚类方法已经取得了传统聚类方法所不能比拟的显著优势。目前的深度文本聚类方法虽然有效，但大多依赖于在一般领域中预先训练的表示，这可能不是最适合在特定目标领域中进行聚类的解决方案。为了解决这个问题，我们提出了 CEIL，一个新的分类增强的短文本聚类迭代学习框架，旨在通过引入一个分类目标来迭代地改进特征表示，从而提高聚类性能。在每次迭代中，我们首先采用一种语言模型来检索初始文本表示，然后使用我们提出的类别分离对比聚类(CDCC)算法从中收集聚类结果。经过严格的数据过滤和聚合处理，检索出具有清晰类别标签的样本，作为监督信息，通过快速学习方法更新语言模型，使其符合分类目标。最后，在下一次迭代中使用具有改进表示能力的更新语言模型来增强聚类。大量的实验表明，CEIL 框架在迭代过程中显著提高了聚类性能，并且对各种聚类算法都是有效的。此外，通过在 CDCC 上结合 CEIL，我们在一系列短文本聚类基准上取得了比其他强基准方法更好的聚类性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CEIL:+A+General+Classification-Enhanced+Iterative+Learning+Framework+for+Text+Clustering)|0|
|[Interval-censored Transformer Hawkes: Detecting Information Operations using the Reaction of Social Systems](https://doi.org/10.1145/3543507.3583481)|Quyu Kong, Pio Calderon, Rohit Ram, Olga Boichak, MarianAndrei Rizoiu|University of Technology, Sydney, Australia; The University of Sydney, Australia; Alibaba Group, China and University of Technology, Sydney, Australia|Social media is being increasingly weaponized by state-backed actors to elicit reactions, push narratives and sway public opinion. These are known as Information Operations (IO). The covert nature of IO makes their detection difficult. This is further amplified by missing data due to the user and content removal and privacy requirements. This work advances the hypothesis that the very reactions that Information Operations seek to elicit within the target social systems can be used to detect them. We propose an Interval-censored Transformer Hawkes (IC-TH) architecture and a novel data encoding scheme to account for both observed and missing data. We derive a novel log-likelihood function that we deploy together with a contrastive learning procedure. We showcase the performance of IC-TH on three real-world Twitter datasets and two learning tasks: future popularity prediction and item category prediction. The latter is particularly significant. Using the retweeting timing and patterns solely, we can predict the category of YouTube videos, guess whether news publishers are reputable or controversial and, most importantly, identify state-backed IO agent accounts. Additional qualitative investigations uncover that the automatically discovered clusters of Russian-backed agents appear to coordinate their behavior, activating simultaneously to push specific narratives.|社交媒体正在越来越多地被政府支持的行为者用作武器，以引发反应、推动叙事和左右公众舆论。这些被称为信息操作(IO)。IO 的隐蔽性使得它们的检测变得困难。由于用户和内容删除以及隐私要求而导致的数据丢失进一步加剧了这种情况。这项工作提出了一个假设，即信息作战部门试图在目标社会系统中引发的反应可以被用来检测它们。提出了一种区间截尾变压器霍克斯(IC-TH)结构和一种新的数据编码方案，以解决观测数据和缺失数据的问题。我们推导了一个新的对数似然函数，我们部署了一个对比学习过程。我们在三个真实的 Twitter 数据集和两个学习任务上展示了 IC-TH 的性能: 未来流行度预测和项目类别预测。后者尤其重要。仅仅使用转发时间和模式，我们就可以预测 YouTube 视频的类别，猜测新闻出版商是否有声誉或有争议，最重要的是，确定国家支持的 IO 代理账户。进一步的定性调查发现，自动发现的俄罗斯支持的代理人集群似乎协调他们的行为，同时激活推动具体的叙述。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interval-censored+Transformer+Hawkes:+Detecting+Information+Operations+using+the+Reaction+of+Social+Systems)|0|
|[Towards Model Robustness: Generating Contextual Counterfactuals for Entities in Relation Extraction](https://doi.org/10.1145/3543507.3583504)|Mi Zhang, Tieyun Qian, Ting Zhang, Xin Miao|School of Computer Science, Wuhan University, China|The goal of relation extraction (RE) is to extract the semantic relations between/among entities in the text. As a fundamental task in information systems, it is crucial to ensure the robustness of RE models. Despite the high accuracy current deep neural models have achieved in RE tasks, they are easily affected by spurious correlations. One solution to this problem is to train the model with counterfactually augmented data (CAD) such that it can learn the causation rather than the confounding. However, no attempt has been made on generating counterfactuals for RE tasks. In this paper, we formulate the problem of automatically generating CAD for RE tasks from an entity-centric viewpoint, and develop a novel approach to derive contextual counterfactuals for entities. Specifically, we exploit two elementary topological properties, i.e., the centrality and the shortest path, in syntactic and semantic dependency graphs, to first identify and then intervene on the contextual causal features for entities. We conduct a comprehensive evaluation on four RE datasets by combining our proposed approach with a variety of RE backbones. Results prove that our approach not only improves the performance of the backbones but also makes them more robust in the out-of-domain test 1.|关系抽取的目的是提取文本中实体之间的语义关系。作为信息系统中的一项基础性工作，确保可重构模型的鲁棒性至关重要。尽管目前的深层神经模型在逆向工程任务中已经达到了很高的精度，但是它们很容易受到伪相关的影响。解决这一问题的一种方法是用反事实扩充数据(CAD)训练模型，使其能够学习因果关系而不是混淆。然而，没有尝试为 RE 任务生成反事实。本文从实体中心的观点出发，提出了面向 RE 任务的 CAD 自动生成问题，并提出了一种新的实体上下文反事实推导方法。具体来说，我们利用句法和语义依赖图中的两个基本拓扑性质，即中心性和最短路径，首先识别并干预实体的上下文因果特征。我们对四个 RE 数据集进行了综合评估，结合我们提出的方法与各种 RE 骨干。实验结果表明，该方法不仅提高了骨干网的性能，而且使骨干网在域外测试1中具有更强的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Model+Robustness:+Generating+Contextual+Counterfactuals+for+Entities+in+Relation+Extraction)|0|
|[CitationSum: Citation-aware Graph Contrastive Learning for Scientific Paper Summarization](https://doi.org/10.1145/3543507.3583505)|Zheheng Luo, Qianqian Xie, Sophia Ananiadou|The University of Manchester, United Kingdom; University of Manchester, United Kingdom|Citation graphs can be helpful in generating high-quality summaries of scientific papers, where references of a scientific paper and their correlations can provide additional knowledge for contextualising its background and main contributions. Despite the promising contributions of citation graphs, it is still challenging to incorporate them into summarization tasks. This is due to the difficulty of accurately identifying and leveraging relevant content in references for a source paper, as well as capturing their correlations of different intensities. Existing methods either ignore references or utilize only abstracts indiscriminately from them, failing to tackle the challenge mentioned above. To fill that gap, we propose a novel citation-aware scientific paper summarization framework based on citation graphs, able to accurately locate and incorporate the salient contents from references, as well as capture varying relevance between source papers and their references. Specifically, we first build a domain-specific dataset PubMedCite with about 192K biomedical scientific papers and a large citation graph preserving 917K citation relationships between them. It is characterized by preserving the salient contents extracted from full texts of references, and the weighted correlation between the salient contents of references and the source paper. Based on it, we design a self-supervised citation-aware summarization framework (CitationSum) with graph contrastive learning, which boosts the summarization generation by efficiently fusing the salient information in references with source paper contents under the guidance of their correlations. Experimental results show that our model outperforms the state-of-the-art methods, due to efficiently leveraging the information of references and citation correlations.|引文图可以帮助生成高质量的科学论文摘要，其中科学论文的参考文献及其相关性可以提供额外的知识，以便将其背景和主要贡献联系起来。尽管引文图有很大的贡献，但是将它们整合到摘要任务中仍然具有挑战性。这是因为很难准确地识别和利用源文件参考文献中的相关内容，以及捕捉它们之间不同强度的相关性。现有的方法要么忽略引用，要么只是不加选择地使用它们的摘要，未能解决上面提到的问题。为了填补这一空白，我们提出了一种新的基于引文图的引文感知科技论文摘要框架，该框架能够准确地定位和整合引文中的显著内容，并能够捕获原始论文及其引文之间的不同相关性。具体来说，我们首先构建一个特定领域的数据集 PubMedCite，其中包含大约192K 的生物医学科学论文，以及一个大型引文图，保留它们之间的917K 引文关系。它的拥有属性是保留从参考文献全文中提取的显著内容，以及参考文献的显著内容与原文之间的加权相关性。在此基础上，设计了一个基于图形对比学习的自监督引文感知摘要框架(CitationSum)。实验结果表明，由于有效地利用了参考文献信息和引文相关性，我们的模型优于现有的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CitationSum:+Citation-aware+Graph+Contrastive+Learning+for+Scientific+Paper+Summarization)|0|
|[Set in Stone: Analysis of an Immutable Web3 Social Media Platform](https://doi.org/10.1145/3543507.3583510)|Wenrui Zuo, Aravindh Raman, Raul J. Mondragón, Gareth Tyson|Telefónica Research, Spain; Queen Mary University of London, United Kingdom; Hong Kong University of Science and Technology, Hong Kong|There has been growing interest in the so-called “Web3” movement. This loosely refers to a mix of decentralized technologies, often underpinned by blockchain technologies. Among these, Web3 social media platforms have begun to emerge. These store all social interaction data (e.g., posts) on a public ledger, removing the need for centralized data ownership and management. But this comes at a cost, which some argue is prohibitively expensive. As an exemplar within this growing ecosytem, we explore memo.cash, a microblogging service built on the Bitcoin Cash (BCH) blockchain. We gather data for 24K users, 317K posts, 2.57M user actions, which have facilitated $6.75M worth of transactions. A particularly unique feature is that users must pay BCH tokens for each interaction (e.g., posting, following). We study how this may impact the social makeup of the platform. We therefore study memo.cash as both a social network and a transaction platform.|人们对所谓的“ Web3”运动越来越感兴趣。这松散地指的是分散技术的混合，通常由区块链技术支持。其中，Web3社交媒体平台已经开始出现。它们将所有的社会交互数据(例如，文章)存储在一个公共分类账上，从而消除了集中数据所有权和管理的需要。但这是有代价的，有些人认为代价高得令人望而却步。作为这个不断增长的生态系统的一个范例，我们探索了 mem.Cash，一个基于比特币现金(BCH)区块链的微博客服务。我们收集了24K 用户、317K 帖子、257万用户行为的数据，这些数据促成了价值675万美元的交易。一个特别独特的特性是，用户必须为每次交互(例如，发布、跟踪)支付 BCH 令牌。我们研究这可能会如何影响平台的社会构成。因此，我们将 Memo.cash 作为一个社交网络和一个交易平台来研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Set+in+Stone:+Analysis+of+an+Immutable+Web3+Social+Media+Platform)|0|
|[Show me your NFT and I tell you how it will perform: Multimodal representation learning for NFT selling price prediction](https://doi.org/10.1145/3543507.3583520)|Davide Costa, Lucio La Cava, Andrea Tagarelli|DIMES - Dept. Computer Engineering, Modeling, Electronics, and Systems Engineering, University of Calabria, Italy|Non-Fungible Tokens (NFTs) represent deeds of ownership, based on blockchain technologies and smart contracts, of unique crypto assets on digital art forms (e.g., artworks or collectibles). In the spotlight after skyrocketing in 2021, NFTs have attracted the attention of crypto enthusiasts and investors intent on placing promising investments in this profitable market. However, the NFT financial performance prediction has not been widely explored to date. In this work, we address the above problem based on the hypothesis that NFT images and their textual descriptions are essential proxies to predict the NFT selling prices. To this purpose, we propose MERLIN, a novel multimodal deep learning framework designed to train Transformer-based language and visual models, along with graph neural network models, on collections of NFTs' images and texts. A key aspect in MERLIN is its independence on financial features, as it exploits only the primary data a user interested in NFT trading would like to deal with, i.e., NFT images and textual descriptions. By learning dense representations of such data, a price-category classification task is performed by MERLIN models, which can also be tuned according to user preferences in the inference phase to mimic different risk-return investment profiles. Experimental evaluation on a publicly available dataset has shown that MERLIN models achieve significant performances according to several financial assessment criteria, fostering profitable investments, and also beating baseline machine-learning classifiers based on financial features.|非可替换令牌(Non-Fungible Tokens，NFT)代表基于区块链技术和智能合同的所有权契约，在数字艺术形式(例如艺术品或收藏品)上拥有独特的加密资产。自2021年飙升以来，非加密货币基金已经吸引了加密货币爱好者和投资者的注意力，他们打算在这个有利可图的市场进行有前途的投资。然而，到目前为止，NFT 财务业绩预测还没有得到广泛的研究。本文基于 NFT 图像及其文本描述是预测 NFT 销售价格的重要指标这一假设，对上述问题进行了研究。为此，我们提出了 MERLIN，一个新的多模式深度学习框架，旨在训练基于变压器的语言和视觉模型，连同图形神经网络模型，对 NFT 的图像和文本的收集。MERLIN 的一个关键方面是其独立于金融功能，因为它只利用对 NFT 交易感兴趣的用户希望处理的主要数据，即 NFT 图像和文本描述。通过学习这些数据的密集表示，MERLIN 模型执行价格类别分类任务，该模型还可以根据推断阶段的用户偏好进行调整，以模拟不同的风险-收益投资情况。对公开数据集的实验评估表明，MERLIN 模型根据若干财务评估标准取得了显著的性能，促进了有利可图的投资，并且击败了基于财务特征的基线机器学习分类器。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Show+me+your+NFT+and+I+tell+you+how+it+will+perform:+Multimodal+representation+learning+for+NFT+selling+price+prediction)|0|
|[CoTel: Ontology-Neural Co-Enhanced Text Labeling](https://doi.org/10.1145/3543507.3583533)|MiaoHui Song, Lan Zhang, Mu Yuan, Zichong Li, Qi Song, Yijun Liu, Guidong Zheng|University of Science and Technology of China, China; China Merchants Bank, China; University of Science and Technology of China, China and Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, China|The success of many web services relies on the large-scale domain-specific high-quality labeled dataset. Insufficient public datasets motivate us to reduce the cost of data labeling while maintaining high accuracy in support of intelligent web applications. The rule-based method and the learning-based method are common techniques for labeling. In this work, we study how to utilize the rule-based and learning-based methods for resource-effective text labeling. We propose CoTel, the first ontology-neural co-enhanced framework for text labeling. We propose critical ontology extraction in the rule-based module and ontology-enhanced loss prediction in the learning-based module. CoTel can integrate explicit labeling rules and implicit labeling models and make them help each other to improve resource efficiency in text labeling tasks. We evaluate CoTel on both public datasets and real applications with three different tasks. Compared with the baseline, CoTel can reduce the time cost by 64.75% (a 2.84× speedup) and the number of labeling by 62.07%.|许多 Web 服务的成功依赖于特定领域的大规模高质量标记数据集。公共数据集的不足促使我们降低数据标签的成本，同时保持高精度以支持智能网络应用程序。基于规则的方法和基于学习的方法是常用的标记技术。在这项工作中，我们研究了如何利用基于规则和基于学习的方法资源有效的文本标注。我们提出了 CoTel，第一个本体-神经协同增强的文本标注框架。提出了基于规则模块的关键本体抽取和基于学习模块的本体增强损失预测。CoTel 可以集成显式标注规则和隐式标注模型，使两者相辅相成，提高文本标注任务的资源利用效率。我们评估 CoTel 的公共数据集和实际应用程序与三个不同的任务。与基准相比，CoTel 可以减少64.75% 的时间成本(2.84倍加速)和62.07% 的标签数量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CoTel:+Ontology-Neural+Co-Enhanced+Text+Labeling)|0|
|[Extracting Cultural Commonsense Knowledge at Scale](https://doi.org/10.1145/3543507.3583535)|TuanPhong Nguyen, Simon Razniewski, Aparna S. Varde, Gerhard Weikum|Montclair State University, USA; Max Planck Institute for Informatics, Germany|Structured knowledge is important for many AI applications. Commonsense knowledge, which is crucial for robust human-centric AI, is covered by a small number of structured knowledge projects. However, they lack knowledge about human traits and behaviors conditioned on socio-cultural contexts, which is crucial for situative AI. This paper presents CANDLE, an end-to-end methodology for extracting high-quality cultural commonsense knowledge (CCSK) at scale. CANDLE extracts CCSK assertions from a huge web corpus and organizes them into coherent clusters, for 3 domains of subjects (geography, religion, occupation) and several cultural facets (food, drinks, clothing, traditions, rituals, behaviors). CANDLE includes judicious techniques for classification-based filtering and scoring of interestingness. Experimental evaluations show the superiority of the CANDLE CCSK collection over prior works, and an extrinsic use case demonstrates the benefits of CCSK for the GPT-3 language model. Code and data can be accessed at https://candle.mpi-inf.mpg.de/.|结构化知识对于许多人工智能应用非常重要。常识知识对于健壮的以人为中心的人工智能是至关重要的，它被少量的结构化知识项目所覆盖。然而，他们缺乏关于社会文化背景下的人类特征和行为的知识，这对于情境人工智能是至关重要的。本文提出了一种端到端的方法 CANDLE，用于在规模上提取高质量的文化常识知识(CCSK)。CANDLE 从一个庞大的网络语料库中提取出 CCSK 断言，并将它们组织成连贯的集群，涉及3个主题领域(地理、宗教、职业)和几个文化方面(食物、饮料、服装、传统、仪式、行为)。CANDLE 包括基于分类的过滤和感兴趣度评分的明智技术。实验结果表明 CANDLE CCSK 集合优于以往的工作，并通过一个外部用例说明 CCSK 集合对 GPT-3语言模型的优越性。代码和数据可在 https://candle.mpi-inf.mpg.de/查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Extracting+Cultural+Commonsense+Knowledge+at+Scale)|0|
|[Unsupervised Event Chain Mining from Multiple Documents](https://doi.org/10.1145/3543507.3583295)|Yizhu Jiao, Ming Zhong, Jiaming Shen, Yunyi Zhang, Chao Zhang, Jiawei Han|Georgia Institute of Technology, USA; Google Research, USA; University of Illinois Urbana-Champaign, USA|Massive and fast-evolving news articles keep emerging on the web. To effectively summarize and provide concise insights into real-world events, we propose a new event knowledge extraction task Event Chain Mining in this paper. Given multiple documents about a super event, it aims to mine a series of salient events in temporal order. For example, the event chain of super event Mexico Earthquake in 2017 is {earthquake hit Mexico, destroy houses, kill people, block roads}. This task can help readers capture the gist of texts quickly, thereby improving reading efficiency and deepening text comprehension. To address this task, we regard an event as a cluster of different mentions of similar meanings. In this way, we can identify the different expressions of events, enrich their semantic knowledge and replenish relation information among them. Taking events as the basic unit, we present a novel unsupervised framework, EMiner. Specifically, we extract event mentions from texts and merge them with similar meanings into a cluster as a single event. By jointly incorporating both content and commonsense, essential events are then selected and arranged chronologically to form an event chain. Meanwhile, we annotate a multi-document benchmark to build a comprehensive testbed for the proposed task. Extensive experiments are conducted to verify the effectiveness of EMiner in terms of both automatic and human evaluations.|大量迅速发展的新闻文章不断出现在网络上。为了有效地总结和提供对现实世界事件的简明见解，本文提出了一种新的事件知识提取任务事件链挖掘。给定关于一个超级事件的多个文档，目的是按时间顺序挖掘一系列突出事件。例如，2017年墨西哥大地震的事件链是{地震袭击墨西哥，摧毁房屋，造成人员死亡，封锁道路}。这项任务可以帮助读者快速掌握文章的要点，从而提高阅读效率，深化文章理解。为了解决这个问题，我们把一个事件看作是一组提到相似含义的不同事件。通过这种方法，我们可以识别事件的不同表达形式，丰富它们的语义知识，补充它们之间的关系信息。以事件为基本单元，提出了一种新的无监督框架 EMiner。具体来说，我们从文本中提取事件提及，并将具有相似含义的事件合并到一个集群中作为单个事件。通过结合内容和常识，重要的事件然后选择和安排的时间顺序，形成一个事件链。同时，我们注释了一个多文档基准，以建立一个全面的测试平台，提出了任务。为了验证 EMiner 在自动评估和人工评估方面的有效性，进行了大量的实验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Event+Chain+Mining+from+Multiple+Documents)|0|
|[A Multi-view Meta-learning Approach for Multi-modal Response Generation](https://doi.org/10.1145/3543507.3583548)|Zhiliang Tian, Zheng Xie, Fuqiang Lin, Yiping Song|National University of Defense Technology, China|As massive conversation examples are easily accessible on the Internet, we are now able to organize large-scale conversation corpora to build chatbots in a data-driven manner. Multi-modal social chatbots produce conversational utterances according to both textual utterances and vision signals. Due to the difficulty of bridging different modalities, the dialogue generation model of chatbots falls into local minima that only capture the mapping between textual input and textual output, as a result, it almost ignores the non-textual signals. Further, similar to the dialogue model with plain text as input and output, the generated responses from multi-modal dialogue also lack diversity and informativeness. In this paper, to address the above issues, we propose a Multi-View Meta-Learning (MultiVML) algorithm that groups samples in multiple views and customizes generation models to different groups. We employ a multi-view clustering to group the training samples so as to attend more to the unique information in non-textual modality. Tailoring different sets of model parameters for each group boosts the genereation diversity via meta-learning. We evaluate MultiVML on two variants of the OpenViDial benchmark datasets. The experiments show that our model not only better explore the information from multiple modalities, but also excels baselines in both quality and diversity.|由于大量的会话实例在互联网上很容易访问，我们现在能够组织大规模的会话语料库来以数据驱动的方式构建聊天机器人。多模态社交聊天机器人根据文本话语和视觉信号产生会话话语。由于不同模式之间难以衔接，聊天机器人的对话生成模型陷入局部极小，只能捕捉文本输入和文本输出之间的映射，因此几乎忽略了非文本信号。此外，与以纯文本作为输入和输出的对话模式类似，多模式对话产生的回应也缺乏多样性和信息性。针对上述问题，本文提出了一种多视图元学习(MultiVML)算法，该算法将样本分组到多个视图中，并根据不同的视图组定制生成模型。采用多视角聚类方法对训练样本进行分组，以更多地关注非文本情态下的独特信息。通过元学习为每个群体裁剪不同的模型参数集合，提高了生成的多样性。我们在 OpenViDial 基准数据集的两个变体上评估 MultiVML。实验表明，该模型不仅能够更好地探索来自多种模式的信息，而且在质量和多样性方面都优于基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Multi-view+Meta-learning+Approach+for+Multi-modal+Response+Generation)|0|
|[Provenance of Training without Training Data: Towards Privacy-Preserving DNN Model Ownership Verification](https://doi.org/10.1145/3543507.3583198)|Yunpeng Liu, Kexin Li, Zhuotao Liu, Bihan Wen, Ke Xu, Weiqiang Wang, Wenbiao Zhao, Qi Li|Tsinghua University, China and Zhongguancun Laboratory, China; University of Toronto, Canada; Tsinghua University, China; Ant Group, China; Nanyang Technological University, Singapore|In the era of deep learning, it is critical to protect the intellectual property of high-performance deep neural network (DNN) models. Existing proposals, however, are subject to adversarial ownership forgery (e.g., methods based on watermarks or fingerprints) or require full access to the original training dataset for ownership verification (e.g., methods requiring the replay of the learning process). In this paper, we propose a novel Provenance of Training (PoT) scheme, the first empirical study towards verifying DNN model ownership without accessing any original dataset while being robust against existing attacks. At its core, PoT relies on a coherent model chain built from the intermediate checkpoints saved during model training to serve as the ownership certificate. Through an in-depth analysis of model training, we propose six key properties that a legitimate model chain shall naturally hold. In contrast, it is difficult for the adversary to forge a model chain that satisfies these properties simultaneously without performing actual training. We systematically analyze PoT’s robustness against various possible attacks, including the adaptive attacks that are designed given the full knowledge of PoT’s design, and further perform extensive empirical experiments to demonstrate our security analysis.|在深度学习时代，保护高性能深度神经网络(DNN)模型的知识产权至关重要。然而，现有的建议会受到对抗性所有权伪造(例如，基于水印或指纹的方法)或要求完全访问原始培训数据集以进行所有权核实(例如，需要重播学习过程的方法)的影响。在本文中，我们提出了一个新的训练起源(PoT)方案，第一个实证研究是在不访问任何原始数据集的情况下验证 DNN 模型的所有权，同时对现有的攻击具有鲁棒性。在其核心，PoT 依赖于一个连贯的模型链，这个模型链是从模型培训期间保存的中间检查点构建的，作为所有权证书。通过对模型训练的深入分析，我们提出了一个合法的模型链必须具备的六个关键性质。相比之下，如果不进行实际训练，对手很难建立一个同时满足这些特性的模型链。我们系统地分析了 PoT 对各种可能攻击的鲁棒性，包括在充分了解 PoT 设计知识的情况下设计的自适应攻击，并进一步进行了广泛的实证实验来验证我们的安全性分析。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Provenance+of+Training+without+Training+Data:+Towards+Privacy-Preserving+DNN+Model+Ownership+Verification)|0|
|[Efficient and Low Overhead Website Fingerprinting Attacks and Defenses based on TCP/IP Traffic](https://doi.org/10.1145/3543507.3583200)|Guodong Huang, Chuan Ma, Ming Ding, Yuwen Qian, Chunpeng Ge, Liming Fang, Zhe Liu|Nanjing University of Aeronautics and Astronautics, China; Shandong Univerisity, China; Zhejiang Lab, China; Data 61, CSIRO, Sydney, Australia; Nanjing University of Science and Technology, China|Website fingerprinting attack is an extensively studied technique used in a web browser to analyze traffic patterns and thus infer confidential information about users. Several website fingerprinting attacks based on machine learning and deep learning tend to use the most typical features to achieve a satisfactory performance of attacking rate. However, these attacks suffer from several practical implementation factors, such as a skillfully pre-processing step or a clean dataset. To defend against such attacks, random packet defense (RPD) with a high cost of excessive network overhead is usually applied. In this work, we first propose a practical filter-assisted attack against RPD, which can filter out the injected noises using the statistical characteristics of TCP/IP traffic. Then, we propose a list-assisted defensive mechanism to defend the proposed attack method. To achieve a configurable trade-off between the defense and the network overhead, we further improve the list-based defense by a traffic splitting mechanism, which can combat the mentioned attacks as well as save a considerable amount of network overhead. In the experiments, we collect real-life traffic patterns using three mainstream browsers, i.e., Microsoft Edge, Google Chrome, and Mozilla Firefox, and extensive results conducted on the closed and open-world datasets show the effectiveness of the proposed algorithms in terms of defense accuracy and network efficiency.|网站指纹攻击是一种被广泛研究的技术，用于网络浏览器分析流量模式，从而推断出用户的机密信息。几种基于机器学习和深度学习的网站指纹攻击都倾向于使用最典型的特征来获得满意的攻击率。然而，这些攻击受到几个实际实现因素的影响，例如一个熟练的预处理步骤或一个干净的数据集。为了防御这种攻击，通常采用具有高额网络开销的随机分组防御(RPD)技术。本文首先提出了一种实用的针对 RPD 的滤波辅助攻击方法，该方法可以利用 TCP/IP 流量的统计特性来滤除注入的噪声。然后，我们提出了一个列表辅助防御机制来防御所提出的攻击方法。为了在防御和网络开销之间实现可配置的平衡，我们进一步改进了基于列表的防御，采用了流量分割机制，可以抵抗上述攻击，并且节省了大量的网络开销。在实验中，我们使用三种主流浏览器，即 Microsoft Edge、 Google Chrome 和 Mozilla Firefox 收集了真实的流量模式，在封闭和开放世界的数据集上进行的大量实验结果显示了所提出的算法在防御精度和网络效率方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+and+Low+Overhead+Website+Fingerprinting+Attacks+and+Defenses+based+on+TCP/IP+Traffic)|0|
|[Curriculum Graph Poisoning](https://doi.org/10.1145/3543507.3583211)|Hanwen Liu, Peilin Zhao, Tingyang Xu, Yatao Bian, Junzhou Huang, Yuesheng Zhu, Yadong Mu|University of Texas at Arlington, USA; Peking University, China; Tencent AI Lab, China|Despite the success of graph neural networks (GNNs) over the Web in recent years, the typical transductive learning setting for node classification requires GNNs to be retrained frequently, making them vulnerable to poisoning attacks by corrupting the training graph. Poisoning attacks on graphs are, however, non-trivial as the attack space is potentially large, and the discrete graph structure makes the poisoning function non-differentiable. In this paper, we revisit the bi-level optimization problem in graph poisoning and propose a novel graph poisoning method, termed Curriculum Graph Poisoning (CuGPo), inspired by curriculum learning. In contrast to other poisoning attacks that use heuristics or directly optimize the graph, our method learns to generate poisoned graphs from basic adversarial knowledge first and advanced knowledge later. Specifically, for the outer optimization, we utilize the slightly perturbed graphs which represent the easy poisoning task at the beginning, and then enlarge the attack space until the final; for the inner optimization, we firstly exploit the knowledge from the clean graph and then adapt quickly to perturbed graphs to obtain the adversarial knowledge. Extensive experiments demonstrate that CuGPo achieves state-of-the-art performance in graph poisoning attacks.|尽管近年来图形神经网络(GNN)在网络上取得了成功，但是典型的传递式节点分类学习环境要求 GNN 经常被重新训练，这使得它们容易受到损坏训练图的中毒攻击。然而，由于图的攻击空间可能很大，并且离散的图结构使得中毒函数不可微，因此对图的中毒攻击是非平凡的。在这篇文章中，我们重新审视了图表中毒的双层最佳化问题，并提出了一种新的图表中毒方法，称为课程图表中毒(CuGPo) ，它是受课程学习的启发而提出的。与其他使用启发式或直接优化图表的中毒攻击不同，我们的方法首先学习从基本的对抗性知识生成中毒图表，然后再从高级知识生成中毒图表。具体而言，对于外部优化问题，我们首先利用代表容易中毒任务的微扰图，然后扩大攻击空间直到最后; 对于内部优化问题，我们首先利用干净图中的知识，然后对微扰图进行快速适应，以获得对抗性知识。大量的实验表明，CuGPo 在图形中毒攻击中取得了最先进的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Curriculum+Graph+Poisoning)|0|
|[Transferring Audio Deepfake Detection Capability across Languages](https://doi.org/10.1145/3543507.3583222)|Zhongjie Ba, Qing Wen, Peng Cheng, Yuwei Wang, Feng Lin, Li Lu, Zhenguang Liu|Zhejiang University, China and ZJU-Hangzhou Global Scientific and Technological Innovation Center, China|The proliferation of deepfake content has motivated a surge of detection studies. However, existing detection methods in the audio area exclusively work in English, and there is a lack of data resources in other languages. Cross-lingual deepfake detection, a critical but rarely explored area, urges more study. This paper conducts the first comprehensive study on the cross-lingual perspective of deepfake detection. We observe that English data enriched in deepfake algorithms can teach a detector the knowledge of various spoofing artifacts, contributing to performing detection across language domains. Based on the observation, we first construct a first-of-its-kind cross-lingual evaluation dataset including heterogeneous spoofed speech uttered in the two most widely spoken languages, then explored domain adaptation (DA) techniques to transfer the artifacts detection capability and propose effective and practical DA strategies fitting the cross-lingual scenario. Our adversarial-based DA paradigm teaches the model to learn real/fake knowledge while losing language dependency. Extensive experiments over 137-hour audio clips validate the adapted models can detect fake audio generated by unseen algorithms in the new domain.|深度伪造内容的泛滥激发了侦查研究的高潮。然而，现有的音频检测方法都是用英语进行的，缺乏其他语言的数据资源。跨语言深度伪造检测是一个关键但很少被探索的领域，促使更多的研究。本文首次对深度伪造检测的跨语言视角进行了全面的研究。我们观察到，在深度伪造算法中丰富的英语数据可以教给检测器各种欺骗伪造的知识，有助于跨语言领域执行检测。在此基础上，我们首先构建了一个包含两种最常用语言的异质欺骗语音的跨语言评价数据集，然后探索了领域适应(DA)技术来传递伪影检测能力，并提出了适合跨语言场景的有效而实用的 DA 策略。我们的基于对抗的 DA 范式教导模型学习真/假知识，同时失去语言依赖性。经过137小时的大量实验，验证了改进后的模型能够检测到新领域中未知算法产生的假音频。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Transferring+Audio+Deepfake+Detection+Capability+across+Languages)|0|
|[Web Photo Source Identification based on Neural Enhanced Camera Fingerprint](https://doi.org/10.1145/3543507.3583225)|Feng Qian, Sifeng He, Honghao Huang, Huanyu Ma, Xiaobo Zhang, Lei Yang|Ant Group, China|With the growing popularity of smartphone photography in recent years, web photos play an increasingly important role in all walks of life. Source camera identification of web photos aims to establish a reliable linkage from the captured images to their source cameras, and has a broad range of applications, such as image copyright protection, user authentication, investigated evidence verification, etc. This paper presents an innovative and practical source identification framework that employs neural-network enhanced sensor pattern noise to trace back web photos efficiently while ensuring security. Our proposed framework consists of three main stages: initial device fingerprint registration, fingerprint extraction and cryptographic connection establishment while taking photos, and connection verification between photos and source devices. By incorporating metric learning and frequency consistency into the deep network design, our proposed fingerprint extraction algorithm achieves state-of-the-art performance on modern smartphone photos for reliable source identification. Meanwhile, we also propose several optimization sub-modules to prevent fingerprint leakage and improve accuracy and efficiency. Finally for practical system design, two cryptographic schemes are introduced to reliably identify the correlation between registered fingerprint and verified photo fingerprint, i.e. fuzzy extractor and zero-knowledge proof (ZKP). The codes for fingerprint extraction network and benchmark dataset with modern smartphone cameras photos are all publicly available at https://github.com/PhotoNecf/PhotoNecf.|随着近年来智能手机摄影的日益普及，网络照片在各行各业中扮演着越来越重要的角色。网络照片的源摄像头识别旨在建立从捕获的图像到源摄像头的可靠联系，具有广泛的应用，如图像版权保护、用户认证、调查证据验证等。提出了一种新颖实用的源识别框架，该框架利用神经网络增强的传感器模式噪声，在保证安全的前提下有效地追踪网络照片。我们提出的框架包括三个主要阶段: 初始设备指纹注册、指纹提取和拍摄照片时的密码连接建立，以及照片与源设备之间的连接验证。通过在深度网络设计中引入度量学习和频率一致性，我们提出的指纹提取算法实现了对现代智能手机照片的可靠识别。同时，为了防止指纹泄漏，提高指纹识别的准确性和效率，提出了几个优化子模块。最后针对实际系统设计，提出了两种可靠识别已注册指纹与已验证照片指纹之间相关性的密码体制，即模糊提取器和零知识证明(ZKP)。指纹提取网络的代码和现代智能手机相机照片的基准数据集都已在 https://github.com/photonecf/photonecf 公开发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Web+Photo+Source+Identification+based+on+Neural+Enhanced+Camera+Fingerprint)|0|
|[TFE-GNN: A Temporal Fusion Encoder Using Graph Neural Networks for Fine-grained Encrypted Traffic Classification](https://doi.org/10.1145/3543507.3583227)|Haozhen Zhang, Le Yu, Xi Xiao, Qing Li, Francesco Mercaldo, Xiapu Luo, Qixu Liu|Institute of Information Engineering, Chinese Academy of Sciences, China; University of Molise, Italy and IIT-CNR, Italy; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Shenzhen International Graduate School, Tsinghua University, China; Peng Cheng Laboratory, China|Encrypted traffic classification is receiving widespread attention from researchers and industrial companies. However, the existing methods only extract flow-level features, failing to handle short flows because of unreliable statistical properties, or treat the header and payload equally, failing to mine the potential correlation between bytes. Therefore, in this paper, we propose a byte-level traffic graph construction approach based on point-wise mutual information (PMI), and a model named Temporal Fusion Encoder using Graph Neural Networks (TFE-GNN) for feature extraction. In particular, we design a dual embedding layer, a GNN-based traffic graph encoder as well as a cross-gated feature fusion mechanism, which can first embed the header and payload bytes separately and then fuses them together to obtain a stronger feature representation. The experimental results on two real datasets demonstrate that TFE-GNN outperforms multiple state-of-the-art methods in fine-grained encrypted traffic classification tasks.|加密流量分类受到了研究人员和工业企业的广泛关注。然而，现有的方法只能提取流级特征，由于统计特征不可靠而无法处理短流，或者对报头和有效负载一视同仁，无法挖掘字节之间潜在的相关性。因此，本文提出了一种基于点向互信息(PMI)的字节级流量图构造方法，并提出了一种基于图形神经网络(TFE-GNN)的时态融合编码器(TFE-GNN)模型用于流量图的特征提取。特别地，我们设计了一个双嵌入层、一个基于 GNN 的流量图编码器以及一个跨门限的特征融合机制，该机制首先分别嵌入报头和有效载荷字节，然后将它们融合在一起以获得更强的特征表示。在两个实际数据集上的实验结果表明，在细粒度加密流量分类任务中，TFE-GNN 方法的性能优于多种最新方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TFE-GNN:+A+Temporal+Fusion+Encoder+Using+Graph+Neural+Networks+for+Fine-grained+Encrypted+Traffic+Classification)|0|
|[Time-manipulation Attack: Breaking Fairness against Proof of Authority Aura](https://doi.org/10.1145/3543507.3583252)|Xinrui Zhang, Rujia Li, Qin Wang, Qi Wang, Sisi Duan|Research Institute of Trustworthy Autonomous Systems, Department of Computer Science and Engineering, Southern University of Science and Technology, China and School of Computer Science, The University of Sydney, Australia; Research Institute of Trustworthy Autonomous Systems, Department of Computer Science and Engineering, Southern University of Science and Technology, China and Institute for Advanced Study, Tsinghua University, China; Research Institute of Trustworthy Autonomous Systems, Department of Computer Science and Engineering, Southern University of Science and Technology, China; Institute for Advanced Study, Tsinghua University, China; CSIRO Data61, Australia|As blockchain-based commercial projects and startups flourish, efficiency becomes one of the critical metrics in designing blockchain systems. Due to its high efficiency, Proof of Authority (PoA) Aura has become one of the most widely adopted consensus solutions for blockchains. Our research finds over 4,000 projects have used Aura and its variants. In this paper, we provide a rigorous analysis of Aura. We propose three types of time-manipulation attacks, where a malicious leader simply needs to modify the timestamp in its proposed block or delay it to extract extra benefits. These attacks can easily break the legal leader election, thus directly harming the fairness of the block proposal. We apply our attacks to a mature Aura project called OpenEthereum. By repeatedly conducting our attacks1 over 15 days, we find that an adversary can gain on average 200% mining rewards of their fair shares. Furthermore, such attacks can even indirectly break the finality of blocks and the safety of the system. Based on the deployment of Aura as of September 2022, the potentially affected market cap is up to 2.13 billion USD. As a by-product, we further discuss solutions to mitigate such issues and report our observations to official teams.|随着基于区块链的商业项目和初创企业的蓬勃发展，效率成为设计区块链系统的关键指标之一。由于其高效率，证明权威(PoA)光环已成为一个最广泛采用的共识解决方案的区块链。我们的研究发现超过4000个项目已经使用了 Aura 及其变体。在本文中，我们提供了一个严格的光环分析。我们提出了三种类型的时间操纵攻击，其中恶意领导者只需修改其提议的块中的时间戳或延迟它，以获取额外的好处。这些攻击很容易破坏法定领导人的选举，从而直接损害阻止提案的公正性。我们将我们的攻击应用到一个称为 OpenEtherum 的成熟的 Aura 项目中。通过在15天内反复进行我们的攻击，我们发现敌人可以平均获得200% 的挖掘奖励。此外，这种攻击甚至可以间接地破坏块的终结性和系统的安全性。根据2022年9月 Aura 的部署，潜在受影响的市值高达21.3亿美元。作为副产品，我们进一步讨论解决方案，以减轻这些问题，并向官方团队报告我们的观察结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Time-manipulation+Attack:+Breaking+Fairness+against+Proof+of+Authority+Aura)|0|
|[Do NFTs' Owners Really Possess their Assets? A First Look at the NFT-to-Asset Connection Fragility](https://doi.org/10.1145/3543507.3583281)|Ziwei Wang, Jiashi Gao, Xuetao Wei|Southern University of Science and Technology, China|NFTs (Non-Fungible Tokens) have experienced an explosive growth and their record-breaking prices have been witnessed. Typically, the assets that NFTs represent are stored off-chain with a pointer, e.g., multi-hop URLs, due to the costly on-chain storage. Hence, this paper aims to answer the question: Is the NFT-to-Asset connection fragile? This paper makes a first step towards this end by characterizing NFT-to-Asset connections of 12,353 Ethereum NFT Contracts (6,234,141 NFTs in total) from three perspectives, storage, accessibility and duplication. In order to overcome challenges of affecting the measurement accuracy, e.g., IPFS instability and the changing availability of both IPFS and servers' data, we propose to leverage multiple gateways to enlarge the data coverage and extend a longer measurement period with non-trivial efforts. Results of our extensive study show that such connection is very fragile in practice. The loss, unavailability, or duplication of off-chain assets could render value of NFTs worthless. For instance, we find that assets of 25.24% of Ethereum NFT contracts are not accessible, and 21.48% of Ethereum NFT contracts include duplicated assets. Our work sheds light on the fragility along the NFT-to-Asset connection, which could help the NFT community to better enhance the trust of off-chain assets.|NFT (非可替换令牌)经历了爆炸性的增长，其破纪录的价格已经见证。通常，由于昂贵的上链存储，NFT 表示的资产通过一个指针(例如，多跳 URL)在链外存储。因此，本文旨在回答这样一个问题: NFT 与资产之间的联系是否脆弱？本文从存储、可访问性和复制三个角度对12,353个以太 NFT 合同(总共6,234,141个 NFT)的 NFT-to-Asset 连接进行了描述，从而为实现这一目标迈出了第一步。为了克服影响测量准确性的挑战，例如 IPFS 的不稳定性以及 IPFS 和服务器数据可用性的变化，我们建议利用多个网关来扩大数据覆盖范围，并通过非平凡的努力延长测量周期。我们广泛的研究结果表明，这种联系在实践中是非常脆弱的。脱链资产的损失、不可用性或重复可能使 NFT 的价值变得毫无价值。例如，我们发现25.24% 的以太基金合同中的资产是不可访问的，21.48% 的以太基金合同中包含重复资产。我们的工作揭示了 NFT-to-Asset 连接的脆弱性，这可以帮助 NFT 社区更好地增强脱链资产的信任。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Do+NFTs'+Owners+Really+Possess+their+Assets?+A+First+Look+at+the+NFT-to-Asset+Connection+Fragility)|0|
|[Preserving Missing Data Distribution in Synthetic Data](https://doi.org/10.1145/3543507.3583297)|Xinyue Wang, Hafiz Salman Asif, Jaideep Vaidya|Rutgers University, USA|Data from Web artifacts and from the Web is often sensitive and cannot be directly shared for data analysis. Therefore, synthetic data generated from the real data is increasingly used as a privacy-preserving substitute. In many cases, real data from the web has missing values where the missingness itself possesses important informational content, which domain experts leverage to improve their analysis. However, this information content is lost if either imputation or deletion is used before synthetic data generation. In this paper, we propose several methods to generate synthetic data that preserve both the observable and the missing data distributions. An extensive empirical evaluation over a range of carefully fabricated and real world datasets demonstrates the effectiveness of our approach.|来自 Web 工件和 Web 的数据通常是敏感的，不能直接共享用于数据分析。因此，由真实数据生成的合成数据越来越多地被用作保护隐私的替代品。在许多情况下，来自网络的真实数据具有缺失值，而缺失值本身具有重要的信息内容，领域专家利用这些信息来改进他们的分析。但是，如果在合成数据生成之前使用插入或删除，则此信息内容将丢失。在本文中，我们提出了几种生成综合数据的方法，既保留了可观测的数据分布，也保留了缺失的数据分布。对一系列精心制作的真实世界数据集进行了广泛的实证评估，证明了我们方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Preserving+Missing+Data+Distribution+in+Synthetic+Data)|0|
|[Not Seen, Not Heard in the Digital World! Measuring Privacy Practices in Children's Apps](https://doi.org/10.1145/3543507.3583327)|Ruoxi Sun, Minhui Xue, Gareth Tyson, Shuo Wang, Seyit Camtepe, Surya Nepal|CSIRO's Data61, Australia and Cybersecurity CRC, Australia; Hong Kong University of Science and Technology (GZ), China; University of Adelaide, Australia and CSIRO's Data61, Australia|The digital age has brought a world of opportunity to children. Connectivity can be a game-changer for some of the world's most marginalized children. However, while legislatures around the world have enacted regulations to protect children's online privacy, and app stores have instituted various protections, privacy in mobile apps remains a growing concern for parents and wider society. In this paper, we explore the potential privacy issues and threats that exist in these apps. We investigate 20,195 mobile apps from the Google Play store that are designed particularly for children (Family apps) or include children in their target user groups (Normal apps). Using both static and dynamic analysis, we find that 4.47% of Family apps request location permissions, even though collecting location information from children is forbidden by the Play store, and 81.25% of Family apps use trackers (which are not allowed in children's apps). Even major developers with 40+ kids apps on the Play store use ad trackers. Furthermore, we find that most permission request notifications are not well designed for children, and 19.25% apps have inconsistent content age ratings across the different protection authorities. Our findings suggest that, despite significant attention to children's privacy, a large gap between regulatory provisions, app store policies, and actual development practices exist. Our research sheds light for government policymakers, app stores, and developers.|数字时代给孩子们带来了一个充满机遇的世界。对于世界上一些最边缘化的儿童来说，网络连接可以改变游戏规则。然而，尽管世界各地的立法机构已经制定了保护儿童在线隐私的法规，应用商店也制定了各种各样的保护措施，但移动应用的隐私仍然是父母和更广泛的社会日益关注的问题。在本文中，我们探讨了这些应用程序中潜在的隐私问题和威胁。我们调查了来自 Google Play 商店的20,195个专门为儿童设计的移动应用程序(家庭应用程序) ，或者将儿童包括在他们的目标用户群中(普通应用程序)。通过静态和动态分析，我们发现4.47% 的家庭应用程序请求位置权限，尽管从儿童收集位置信息是被 Play 商店禁止的，81.25% 的家庭应用程序使用追踪器(这在儿童应用程序中是不允许的)。即使是在 Play 商店上拥有40多个儿童应用程序的大型开发商也使用广告跟踪器。此外，我们发现大多数的许可请求通知并没有很好地为儿童设计，19.25% 的应用程序在不同的保护机构中有不一致的内容年龄评级。我们的研究结果表明，尽管对儿童隐私的重视程度很高，但监管规定、应用程序商店政策和实际开发实践之间仍存在很大差距。我们的研究为政府决策者、应用商店和开发者提供了启示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Not+Seen,+Not+Heard+in+the+Digital+World!+Measuring+Privacy+Practices+in+Children's+Apps)|0|
|[Automatic Discovery of Emerging Browser Fingerprinting Techniques](https://doi.org/10.1145/3543507.3583333)|Junhua Su, Alexandros Kapravelos|Department of Computer Science, North Carolina State University, USA|With the progression of modern browsers, online tracking has become the most concerning issue for preserving privacy on the web. As major browser vendors plan to or already ban third-party cookies, trackers have to shift towards browser fingerprinting by incorporating novel browser APIs into their tracking arsenal. Understanding how new browser APIs are abused in browser fingerprinting techniques is a significant step toward ensuring protection from online tracking. In this paper, we propose a novel hybrid system, named BFAD, that automatically identifies previously unknown browser fingerprinting APIs in the wild. The system combines dynamic and static analysis to accurately reveal browser API usage and automatically infer browser fingerprinting behavior. Based on the observation that a browser fingerprint is constructed by pulling information from multiple APIs, we leverage dynamic analysis and a locality-based algorithm to discover all involved APIs and static analysis on the dataflow of fingerprinting information to accurately associate them together. Our system discovers 231 fingerprinting APIs in Alexa top 10K domains, starting with only 35 commonly known fingerprinting APIs and 17 data transmission APIs. Out of 231 APIs, 161 of them are not identified by state-of-the-art detection systems. Since our approach is fully automated, we repeat our experiments 11 months later and discover 18 new fingerprinting APIs that were not discovered in our previous experiment. We present with case studies the fingerprinting ability of a total of 249 detected APIs.|随着现代浏览器的发展，在线跟踪已经成为保护网络隐私最令人关注的问题。随着主要浏览器厂商计划或已经禁止第三方 cookie，跟踪器必须转向使用浏览器指纹识别技术，将新的浏览器 API 纳入他们的跟踪武器库。了解新的浏览器 API 是如何在浏览器指纹识别技术中被滥用的，是确保免受在线跟踪的重要一步。在本文中，我们提出了一个新的混合系统，称为 BFAD，自动识别以前未知的浏览器指纹 API 在野外。该系统结合了动态和静态分析，能够准确地揭示浏览器 API 的使用情况，并自动推断浏览器的指纹识别行为。基于从多个 API 中提取信息来构建一个浏览器指纹的观察，我们利用动态分析和基于位置的算法来发现所有涉及的 API 和指纹信息数据流的静态分析，以准确地将它们关联在一起。我们的系统在 Alexa top 10K 域中发现了231个指纹 API，最初只有35个常见的指纹 API 和17个数据传输 API。在231个 API 中，有161个没有被最先进的检测系统识别。由于我们的方法是完全自动化的，我们在11个月后重复我们的实验，发现了18个新的指纹 API，这些 API 在我们以前的实验中没有发现。我们提出的案例研究的指纹能力，共检测到249个原料药。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Discovery+of+Emerging+Browser+Fingerprinting+Techniques)|0|
|[BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection](https://doi.org/10.1145/3543507.3583345)|Sihao Hu, Zhen Zhang, Bingqiao Luo, Shengliang Lu, Bingsheng He, Ling Liu|National University of Singapore, Singapore and Georgia Institute of Technology, USA; Georgia Institute of Technology, USA; National University of Singapore, Singapore|As various forms of fraud proliferate on Ethereum, it is imperative to safeguard against these malicious activities to protect susceptible users from being victimized. While current studies solely rely on graph-based fraud detection approaches, it is argued that they may not be well-suited for dealing with highly repetitive, skew-distributed and heterogeneous Ethereum transactions. To address these challenges, we propose BERT4ETH, a universal pre-trained Transformer encoder that serves as an account representation extractor for detecting various fraud behaviors on Ethereum. BERT4ETH features the superior modeling capability of Transformer to capture the dynamic sequential patterns inherent in Ethereum transactions, and addresses the challenges of pre-training a BERT model for Ethereum with three practical and effective strategies, namely repetitiveness reduction, skew alleviation and heterogeneity modeling. Our empirical evaluation demonstrates that BERT4ETH outperforms state-of-the-art methods with significant enhancements in terms of the phishing account detection and de-anonymization tasks. The code for BERT4ETH is available at: https://github.com/git-disl/BERT4ETH.|由于各种形式的欺诈行为在 Ethereum 大行其道，我们必须防范这些恶意活动，以免易受影响的使用者成为受害者。虽然目前的研究仅仅依赖于基于图表的欺诈检测方法，但有人认为，这些方法可能不太适合处理高度重复、倾斜分布和异构的以太坊交易。为了应对这些挑战，我们提出 BERT4ETH，一个通用的预先训练的变压器编码器，作为一个帐户表示提取器检测各种欺诈行为的以太。BERT4eTH 的特点是它具有出色的建模能力，能够捕捉到 Ethereum 交易中固有的动态顺序模式，并通过三种实用而有效的策略，即重复性减少、偏移缓解和异质性建模，解决了预先培训以太坊 BERT 模型的挑战。我们的实证评估表明，BERT4ETH 优于最先进的方法，在钓鱼账户检测和去匿名任务方面有显著的增强。BERT4eTH 的代码可以在以下 https://github.com/git-disl/BERT4ETH 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BERT4ETH:+A+Pre-trained+Transformer+for+Ethereum+Fraud+Detection)|0|
|[Training-free Lexical Backdoor Attacks on Language Models](https://doi.org/10.1145/3543507.3583348)|Yujin Huang, Terry Yue Zhuo, Qiongkai Xu, Han Hu, Xingliang Yuan, Chunyang Chen|Monash University, Australia; Monash University, Australia and CSIRO's Data61, Australia; The University of Melbourne, Australia|Large-scale language models have achieved tremendous success across various natural language processing (NLP) applications. Nevertheless, language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to undesirable behaviors. Most existing backdoor attacks, such as data poisoning, require further (re)training or fine-tuning language models to learn the intended backdoor patterns. The additional training process however diminishes the stealthiness of the attacks, as training a language model usually requires long optimization time, a massive amount of data, and considerable modifications to the model parameters. In this work, we propose Training-Free Lexical Backdoor Attack (TFLexAttack) as the first training-free backdoor attack on language models. Our attack is achieved by injecting lexical triggers into the tokenizer of a language model via manipulating its embedding dictionary using carefully designed rules. These rules are explainable to human developers which inspires attacks from a wider range of hackers. The sparse manipulation of the dictionary also habilitates the stealthiness of our attack. We conduct extensive experiments on three dominant NLP tasks based on nine language models to demonstrate the effectiveness and universality of our attack. The code of this work is available at https://github.com/Jinxhy/TFLexAttack.|大规模的语言模型在各种自然语言处理(NLP)应用程序中取得了巨大的成功。然而，语言模型容易受到后门攻击，后门攻击会在模型中注入隐形触发器，引导语言模型产生不良行为。大多数现有的后门攻击，例如数据中毒，需要进一步(重新)训练或微调语言模型来学习预期的后门模式。然而，额外的训练过程降低了攻击的隐蔽性，因为训练一个语言模型通常需要很长的优化时间、大量的数据和对模型参数的相当大的修改。在这项工作中，我们提出的训练自由词汇后门攻击(TFLex颗粒攻击)作为第一个训练自由后门攻击的语言模型。我们的攻击是通过使用精心设计的规则来操纵语言模型的嵌入字典，从而将词法触发器注入到语言模型的标记器中来实现的。这些规则对人类开发人员来说是可以解释的，这些规则激发了更广泛的黑客攻击。字典的稀疏处理也证明了我们攻击的隐蔽性。我们基于九种语言模型对三种主要的自然语言处理任务进行了广泛的实验，以验证我们的攻击的有效性和通用性。这项工作的代码可在 https://github.com/jinxhy/tflexattack 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Training-free+Lexical+Backdoor+Attacks+on+Language+Models)|0|
|[The Benefits of Vulnerability Discovery and Bug Bounty Programs: Case Studies of Chromium and Firefox](https://doi.org/10.1145/3543507.3583352)|Soodeh Atefi, Amutheezan Sivagnanam, Afiya Ayman, Jens Grossklags, Aron Laszka|Pennsylvania State University, USA; Technical University of Munich, Germany; University of Houston, USA|Recently, bug-bounty programs have gained popularity and become a significant part of the security culture of many organizations. Bug-bounty programs enable organizations to enhance their security posture by harnessing the diverse expertise of crowds of external security experts (i.e., bug hunters). Nonetheless, quantifying the benefits of bug-bounty programs remains elusive, which presents a significant challenge for managing them. Previous studies focused on measuring their benefits in terms of the number of vulnerabilities reported or based on the properties of the reported vulnerabilities, such as severity or exploitability. However, beyond these inherent properties, the value of a report also depends on the probability that the vulnerability would be discovered by a threat actor before an internal expert could discover and patch it. In this paper, we present a data-driven study of the Chromium and Firefox vulnerability-reward programs. First, we estimate the difficulty of discovering a vulnerability using the probability of rediscovery as a novel metric. Our findings show that vulnerability discovery and patching provide clear benefits by making it difficult for threat actors to find vulnerabilities; however, we also identify opportunities for improvement, such as incentivizing bug hunters to focus more on development releases. Second, we compare the types of vulnerabilities that are discovered internally vs. externally and those that are exploited by threat actors. We observe significant differences between vulnerabilities found by external bug hunters, internal security teams, and external threat actors, which indicates that bug-bounty programs provide an important benefit by complementing the expertise of internal teams, but also that external hunters should be incentivized more to focus on the types of vulnerabilities that are likely to be exploited by threat actors.|最近，bug 奖励程序已经变得流行起来，并且成为许多组织安全文化的重要组成部分。漏洞奖励计划使组织能够通过利用外部安全专家(例如，漏洞搜寻者)的多样化专业知识来加强他们的安全态势。尽管如此，量化漏洞奖励计划的好处仍然是难以捉摸的，这对管理它们提出了重大挑战。以前的研究侧重于根据所报告的脆弱性的数量或根据所报告的脆弱性的特性(如严重性或可利用性)来衡量其益处。然而，除了这些固有属性之外，报告的价值还取决于在内部专家发现和修补漏洞之前，威胁行为者发现漏洞的可能性。在本文中，我们提出了一个数据驱动的研究 Chromium 和 Firefox 漏洞奖励程序。首先，我们使用重新发现的概率作为一个新的度量标准来估计发现漏洞的难度。我们的研究结果表明，漏洞发现和补丁通过使威胁行为者难以发现漏洞提供了明显的好处; 然而，我们也确定了改进的机会，例如鼓励 bug 搜索者更多地关注开发版本。其次，我们比较了内部和外部发现的漏洞类型，以及那些被威胁行为者利用的漏洞类型。我们观察到外部漏洞猎人、内部安全团队和外部威胁行为者发现的漏洞之间存在显著差异，这表明漏洞赏金计划通过补充内部团队的专业知识提供了一个重要的好处，但也表明应该更多地激励外部猎人关注可能被威胁行为者利用的漏洞类型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Benefits+of+Vulnerability+Discovery+and+Bug+Bounty+Programs:+Case+Studies+of+Chromium+and+Firefox)|0|
|[Net-track: Generic Web Tracking Detection Using Packet Metadata](https://doi.org/10.1145/3543507.3583372)|Dongkeun Lee, Minwoo Joo, Wonjun Lee|Korea University, Republic of Korea; Samsung Research, Republic of Korea|While third-party trackers breach users’ privacy by compiling large amounts of personal data through web tracking techniques, combating these trackers is still left at the hand of each user. Although network operators may attempt a network-wide detection of trackers through inspecting all web traffic inside the network, their methods are not only privacy-intrusive but of limited accuracy as these are susceptible to domain changes or ineffective against encrypted traffic. To this end, in this paper, we propose Net-track, a novel approach to managing a secure web environment through platform-independent, encryption-agnostic detection of trackers. Utilizing only side-channel data from network traffic that are still available when encrypted, Net-track accurately detects trackers network-wide, irrespective of user’s browsers or devices without looking into packet payloads or resources fetched from the web server. This prevents user data from leaking to tracking servers in a privacy-preserving manner. By measuring statistics from traffic traces and their similarities, we show distinctions between benign traffic and tracker traffic in their traffic patterns and build Net-track based on the features that fully capture trackers’ distinctive characteristics. Evaluation results show that Net-track is able to detect trackers with 94.02% accuracy and can even discover new trackers yet unrecognized by existing filter lists. Furthermore, Net-track shows its potential for real-time detection, maintaining its performance when using only a portion of each traffic trace.|虽然第三方追踪器通过网络追踪技术编译大量个人数据，侵犯了用户的隐私，但是对抗这些追踪器仍然留在每个用户的手中。尽管网络运营商可以通过检查网络内的所有网络流量来尝试在全网范围内检测跟踪器，但是他们的方法不仅侵犯隐私，而且精确度有限，因为这些方法容易受到域变化的影响，或者对加密流量无效。为此，在本文中，我们提出了网络跟踪，一种新颖的方法来管理一个安全的网络环境，通过平台无关，加密无关的检测跟踪器。网络跟踪仅利用加密后仍然可用的网络流量中的边通道数据，精确检测网络范围内的跟踪器，而不考虑用户的浏览器或设备，无需查看从网络服务器获取的数据包有效载荷或资源。这可以防止用户数据以保护隐私的方式泄露给跟踪服务器。通过测量交通路径的统计信息及其相似性，揭示了良性交通与跟踪交通在交通模式上的区别，并在充分反映跟踪者特征的基础上构建了网络交通路径。评估结果表明，网络跟踪能够以94.02% 的准确率检测出跟踪者，甚至能够发现现有过滤器列表无法识别的新跟踪者。此外，Net-track 显示了其实时检测的潜力，当只使用每个流量跟踪的一部分时，仍能保持其性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Net-track:+Generic+Web+Tracking+Detection+Using+Packet+Metadata)|0|
|[Cross-Modality Mutual Learning for Enhancing Smart Contract Vulnerability Detection on Bytecode](https://doi.org/10.1145/3543507.3583367)|Peng Qian, Zhenguang Liu, Yifang Yin, Qinming He|Institute for Infocomm Research, A*STAR, Singapore; Zhejiang University, China|Over the past couple of years, smart contracts have been plagued by multifarious vulnerabilities, which have led to catastrophic financial losses. Their security issues, therefore, have drawn intense attention. As countermeasures, a family of tools has been developed to identify vulnerabilities in smart contracts at the source-code level. Unfortunately, only a small fraction of smart contracts is currently open-sourced. Another spectrum of work is presented to deal with pure bytecode, but most such efforts still suffer from relatively low performance due to the inherent difficulty in restoring abundant semantics in the source code from the bytecode. This paper proposes a novel cross-modality mutual learning framework for enhancing smart contract vulnerability detection on bytecode. Specifically, we engage in two networks, a student network as the primary network and a teacher network as the auxiliary network. takes two modalities, i.e., source code and its corresponding bytecode as inputs, while is fed with only bytecode. By learning from , is trained to infer the missed source code embeddings and combine both modalities to approach precise vulnerability detection. To further facilitate mutual learning between and , we present a cross-modality mutual learning loss and two transfer losses. As a side contribution, we construct and release a labeled smart contract dataset that concerns four types of common vulnerabilities. Experimental results show that our method significantly surpasses state-of-the-art approaches.|在过去的几年里，聪明的合同一直受到各种漏洞的困扰，这些漏洞导致了灾难性的财务损失。因此，他们的安全问题引起了强烈的关注。作为对策，已经开发了一系列工具来在源代码级别识别智能契约中的漏洞。不幸的是，目前只有一小部分智能合同是开源的。提出了另一种处理纯字节码的工作范围，但是由于从字节码恢复源代码中丰富的语义存在固有的困难，大多数此类工作的性能仍然相对较低。提出了一种新的跨模态互学习框架，用于提高字节码智能契约漏洞检测能力。具体来说，我们从事两个网络，一个是以学生网络为主的网络，另一个是以教师网络为辅助的网络。采用两种模式，即源代码及其相应的字节码作为输入，而只提供字节码。通过学习，训练来推断错过的源代码嵌入，并结合两种方式来接近精确的漏洞检测。为了进一步促进相互学习和交叉学习，我们提出了一种交叉学习方式的相互学习损失和两种转移损失。作为附带贡献，我们构建并发布了一个标签化的智能契约数据集，它涉及到四种常见的漏洞。实验结果表明，我们的方法明显优于国家的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-Modality+Mutual+Learning+for+Enhancing+Smart+Contract+Vulnerability+Detection+on+Bytecode)|0|
|[The Chameleon on the Web: an Empirical Study of the Insidious Proactive Web Defacements](https://doi.org/10.1145/3543507.3583377)|Rui Zhao|University of Nebraska at Omaha, USA|Web defacement is one of the major promotional channels for online underground economies. It regularly compromises benign websites and injects fraudulent content to promote illicit goods and services. It inflicts significant harm to websites’ reputations and revenues and may lead to legal ramifications. In this paper, we uncover proactive web defacements, where the involved web pages (i.e., landing pages) proactively deface themselves within browsers using JavaScript (i.e., control scripts). Proactive web defacements have not yet received attention from research communities, anti-hacking organizations, or law-enforcement officials. To detect proactive web defacements, we designed a practical tool, PACTOR. It runs in the browser and intercepts JavaScript API calls that manipulate web page content. It takes snapshots of the rendered HTML source code immediately before and after the intercepted API calls and detects proactive web defacements by visually comparing every two consecutive snapshots. Our two-month empirical study, using PACTOR, on 2,454 incidents of proactive web defacements shows that they can evade existing URL safety-checking tools and effectively promote the ranking of their landing pages using legitimate content/keywords. We also investigated the vendor network of proactive web defacements and reported all the involved domains to law-enforcement officials and URL-safety checking tools.|网络涂鸦是网络地下经济的主要推广渠道之一。它经常损害良性网站和注入欺诈内容，以促进非法商品和服务。它对网站的声誉和收入造成重大损害，并可能导致法律后果。在本文中，我们揭示了前瞻性的网页涂改，其中涉及的网页(即登陆页)主动涂改自己在浏览器中使用 JavaScript (即控制脚本)。主动的网页涂鸦还没有得到研究团体、反黑客组织或执法官员的关注。为了检测主动的网页破坏，我们设计了一个实用的工具，PACTOR。它在浏览器中运行并拦截操作网页内容的 JavaScriptAPI 调用。它在截获的 API 调用之前和之后立即获取呈现的 HTML 源代码的快照，并通过可视化地比较每两个连续的快照来检测主动的 Web 破坏。我们通过使用 PACTOR 对2,454起主动网页破坏事件进行了为期两个月的实证研究，结果表明他们可以逃避现有的 URL 安全检查工具，并有效地利用合法的内容/关键词来提高其登陆页面的排名。我们还调查了供应商网络的主动网页污损，并报告了所有涉及的领域执法官员和网址安全检查工具。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Chameleon+on+the+Web:+an+Empirical+Study+of+the+Insidious+Proactive+Web+Defacements)|0|
|[Shield: Secure Allegation Escrow System with Stronger Guarantees](https://doi.org/10.1145/3543507.3583391)|Nishat Koti, Varsha Bhat Kukkala, Arpita Patra, Bhavish Raj Gopal|Indian Institute of Science, India|The rising issues of harassment, exploitation, corruption and other forms of abuse have led victims to seek comfort by acting in unison against common perpetrators. This is corroborated by the widespread #MeToo movement, which was explicitly against sexual harassment. Installation of escrow systems has allowed victims to report such incidents. The escrows are responsible for identifying the perpetrator and taking the necessary action to bring justice to all its victims. However, users hesitate to participate in these systems due to the fear of such sensitive reports being leaked to perpetrators, who may further misuse them. Thus, to increase trust in the system, cryptographic solutions are being designed to realize web-based secure allegation escrow (SAE) systems. While the work of Arun et al. (NDSS’20) presents the state-of-the-art solution, we identify attacks that can leak sensitive information and compromise victim privacy. We also report issues present in prior works that were left unidentified. Having identified the attacks and issues in all prior works, we put forth an SAE system that overcomes these while retaining all the existing salient features. The cryptographic technique of secure multiparty computation (MPC) serves as the primary underlying tool in designing our system. At the heart of our system lies a new duplicity check protocol and an improved matching protocol. We also provide essential features such as allegation modification and deletion, which were absent in the state of the art. To demonstrate feasibility, we benchmark the proposed system with state-of-the-art MPC protocols and report the cost of processing an allegation. Different settings that affect system performance are analyzed, and the reported values showcase the practicality of our solution.|骚扰、剥削、腐败和其他形式的虐待问题日益严重，导致受害者为了寻求安慰，联合起来对付共同的肇事者。这一点得到了广泛开展的“ # 我也是”(# meToo)运动的证实，该运动明确反对性骚扰。通过安装第三方托管系统，受害者可以报告此类事件。代管人负责查明肇事者，并采取必要行动为所有受害者伸张正义。但是，用户不愿意参与这些系统，因为他们担心这些敏感的报告会泄露给犯罪者，他们可能会进一步滥用这些报告。因此，为了增加对系统的信任，正在设计加密解决方案来实现基于 Web 的安全指控代管(SAE)系统。虽然 Arun 等人(NDSS’20)的工作提出了最先进的解决方案，但我们确定的攻击可以泄露敏感信息和损害受害者的隐私。我们也报告在以前的工程存在的问题，留下了不明。在确定了以前所有工作中的攻击和问题之后，我们提出了一个 SAE 系统，它克服了这些问题，同时保留了所有现有的显著特征。安全多方计算(MPC)加密技术是我们设计系统的主要基础工具。该系统的核心是一个新的双重性检测协议和一个改进的匹配协议。我们还提供了指控修改和删除等基本功能，这些功能在目前的技术水平上是不存在的。为了证明可行性，我们使用最先进的 MPC 协议对提议的系统进行基准测试，并报告处理指控的成本。分析了影响系统性能的不同设置，报告的值展示了我们的解决方案的实用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Shield:+Secure+Allegation+Escrow+System+with+Stronger+Guarantees)|0|
|[Unnoticeable Backdoor Attacks on Graph Neural Networks](https://doi.org/10.1145/3543507.3583392)|Enyan Dai, Minhua Lin, Xiang Zhang, Suhang Wang|Pennsylvania State University, USA|Graph Neural Networks (GNNs) have achieved promising results in various tasks such as node classification and graph classification. Recent studies find that GNNs are vulnerable to adversarial attacks. However, effective backdoor attacks on graphs are still an open problem. In particular, backdoor attack poisons the graph by attaching triggers and the target class label to a set of nodes in the training graph. The backdoored GNNs trained on the poisoned graph will then be misled to predict test nodes to target class once attached with triggers. Though there are some initial efforts in graph backdoor attacks, our empirical analysis shows that they may require a large attack budget for effective backdoor attacks and the injected triggers can be easily detected and pruned. Therefore, in this paper, we study a novel problem of unnoticeable graph backdoor attacks with limited attack budget. To fully utilize the attack budget, we propose to deliberately select the nodes to inject triggers and target class labels in the poisoning phase. An adaptive trigger generator is deployed to obtain effective triggers that are difficult to be noticed. Extensive experiments on real-world datasets against various defense strategies demonstrate the effectiveness of our proposed method in conducting effective unnoticeable backdoor attacks.|图神经网络在节点分类、图分类等方面都取得了很好的效果。最近的研究发现 GNN 很容易受到敌对攻击。然而，对图的有效后门攻击仍然是一个悬而未决的问题。特别是，后门攻击通过将触发器和目标类标签附加到训练图中的一组节点上来毒害图。在中毒图上训练的后门 GNN 将被误导，一旦与触发器连接，就可以预测测试节点到目标类。虽然对于图形后门攻击已经有了一些初步的研究，但是我们的实证分析表明，对于有效的后门攻击来说，它们可能需要大量的攻击预算，并且注入的触发器可以很容易地被检测到和删除。因此，本文研究了一个新的攻击预算有限的不易察觉的图形后门攻击问题。为了充分利用攻击预算，我们建议在中毒阶段刻意选择节点来注入触发器和目标类标签。自适应触发发生器的部署，以获得难以注意到的有效触发器。针对不同防御策略对真实世界数据集进行的大量实验证明了我们提出的方法在进行有效的不易察觉的后门攻击方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unnoticeable+Backdoor+Attacks+on+Graph+Neural+Networks)|0|
|[Bad Apples: Understanding the Centralized Security Risks in Decentralized Ecosystems](https://doi.org/10.1145/3543507.3583393)|Kailun Yan, Jilian Zhang, Xiangyu Liu, Wenrui Diao, Shanqing Guo|Shandong University, China; Alibaba Group, China; Jinan University, China|The blockchain-powered decentralized applications and systems have been widely deployed in recent years. The decentralization feature promises users anonymity, security, and non-censorship, which is especially welcomed in the areas of decentralized finance and digital assets. From the perspective of most common users, a decentralized ecosystem means every service follows the principle of decentralization. However, we find that the services in a decentralized ecosystem still may contain centralized components or scenarios, like third-party SDKs and privileged operations, which violate the promise of decentralization and may cause a series of centralized security risks. In this work, we systematically study the centralized security risks existing in decentralized ecosystems. Specifically, we identify seven centralized security risks in the deployment of two typical decentralized services – crypto wallets and DApps, such as anonymity loss and overpowered owner. Also, to measure these risks in the wild, we designed an automated detection tool called Naga and carried out large-scale experiments. Based on the measurement of 28 Ethereum crypto wallets (Android version) and 110,506 on-chain smart contracts, the result shows that the centralized security risks are widespread. Up to 96.4% of wallets and 83.5% of contracts exist at least one security risk, including 260 well-known tokens with a total market cap of over $98 billion.|区块链驱动的分散式应用和系统近年来得到了广泛的应用。这个地方分权功能允许用户匿名、安全和无审查，在分散金融和数字资产领域尤其受欢迎。从大多数普通用户的角度来看，一个分散的生态系统意味着每个服务都遵循地方分权原则。然而，我们发现，在一个分散的生态系统中，服务仍然可能包含集中的组件或场景，如第三方 SDK 和特权操作，这违反了地方分权的承诺，并可能导致一系列集中的安全风险。本文系统地研究了分散式生态系统中存在的集中式安全风险。具体来说，我们在部署两种典型的分散式服务(加密钱包和 DApps)时确定了七种集中式安全风险，比如匿名性损失和过度拥有者。此外，为了在野外测量这些风险，我们设计了一种叫做娜迦的自动检测工具，并进行了大规模的实验。通过对28个以太加密钱包(Android 版)和110,506个上链智能合同的测量，结果表明，集中式安全风险普遍存在。高达96.4% 的钱包和83.5% 的合同至少存在一种安全风险，其中包括260种知名令牌，总市值超过980亿美元。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bad+Apples:+Understanding+the+Centralized+Security+Risks+in+Decentralized+Ecosystems)|0|
|[Scan Me If You Can: Understanding and Detecting Unwanted Vulnerability Scanning](https://doi.org/10.1145/3543507.3583394)|Xigao Li, Babak Amin Azad, Amir Rahmati, Nick Nikiforakis|Computer Science, Stony Brook University, USA|Web vulnerability scanners (WVS) are an indispensable tool for penetration testers and developers of web applications, allowing them to identify and fix low-hanging vulnerabilities before they are discovered by attackers. Unfortunately, malicious actors leverage the very same tools to identify and exploit vulnerabilities in third-party websites. Existing research in the WVS space is largely concerned with how many vulnerabilities these tools can discover, as opposed to trying to identify the tools themselves when they are used illicitly. In this work, we design a testbed to characterize web vulnerability scanners using browser-based and network-based fingerprinting techniques. We conduct a measurement study over 12 web vulnerability scanners as well as 159 users who were recruited to interact with the same web applications that were targeted by the evaluated WVSs. By contrasting the traffic and behavior of these two groups, we discover tool-specific and type-specific behaviors in WVSs that are absent from regular users. Based on these observations, we design and build ScannerScope, a machine-learning-based, web vulnerability scanner detection system. ScannerScope consists of a transparent reverse proxy that injects fingerprinting modules on the fly without the assistance (or knowledge) of the protected web applications. Our evaluation results show that ScannerScope can effectively detect WVSs and protect web applications against unwanted vulnerability scanning, with a detection accuracy of over 99% combined with near-zero false positives on human-visitor traffic. Finally, we show that the asynchronous design of ScannerScope results in a negligible impact on server performance and demonstrate that its classifier can resist adversarial ML attacks launched by sophisticated adversaries.|Web 漏洞扫描器(WVS)是渗透测试人员和 Web 应用程序开发人员不可或缺的工具，它允许他们在发现低悬挂漏洞之前识别和修复这些漏洞。不幸的是，恶意行为者利用同样的工具来识别和利用第三方网站的漏洞。WVS 领域的现有研究主要关注这些工具可以发现多少漏洞，而不是在工具被非法使用时试图确定它们本身。在这项工作中，我们设计了一个基于浏览器和基于网络的指纹识别技术来描述网络漏洞扫描器的测试平台。我们对12个网络漏洞扫描器以及159个用户进行了测量研究，这些用户被招募来与被评估的 WVSs 所针对的相同的网络应用程序进行交互。通过对比这两个组的流量和行为，我们发现了常规用户所不具备的工具特定和类型特定的行为。基于这些观察，我们设计并构建了一个基于机器学习的网络漏洞扫描器检测系统 scannerScope。ScanerScope 由一个透明的反向代理组成，它可以在没有受保护的 Web 应用程序的帮助(或知识)的情况下动态地注入指纹识别模块。我们的评估结果表明，ScanerScope 能够有效地检测 WVSs，保护 Web 应用程序免受不必要的漏洞扫描，检测准确率超过99% ，并且对人类访问者流量的误报几乎为零。最后，我们证明了 ScanerScope 的异步设计对服务器性能的影响可以忽略不计，并且证明了它的分类器可以抵抗复杂对手发起的对抗性 ML 攻击。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scan+Me+If+You+Can:+Understanding+and+Detecting+Unwanted+Vulnerability+Scanning)|0|
|[The More Things Change, the More They Stay the Same: Integrity of Modern JavaScript](https://doi.org/10.1145/3543507.3583395)|Johnny So, Michael Ferdman, Nick Nikiforakis|Stony Brook University, USA|The modern web is a collection of remote resources that are identified by their location and composed of interleaving networks of trust. Supply chain attacks compromise the users of a target domain by leveraging its often large set of trusted third parties who provide resources such as JavaScript. The ubiquity of JavaScript, paired with its ability to execute arbitrary code on client machines, makes this particular web resource an ideal vector for supply chain attacks. Currently, there exists no robust method for users browsing the web to verify that the script content they receive from a third party is the expected content. In this paper, we present key insights to inform the design of robust integrity mechanisms, derived from our large-scale analyses of the 6M scripts we collected while crawling 44K domains every day for 77 days. We find that scripts that frequently change should be considered first-class citizens in the modern web ecosystem, and that the ways in which scripts change remain constant over time. Furthermore, we present analyses on the use of strict integrity verification (e.g., Subresource Integrity) at the granularity of the script providers themselves, offering a more complete perspective and demonstrating that the use of strict integrity alone cannot provide satisfactory security guarantees. We conclude that it is infeasible for a client to distinguish benign changes from malicious ones without additional, external knowledge, motivating the need for a new protocol to provide clients the necessary context to assess the potential ramifications of script changes.|现代网络是远程资源的集合，这些资源通过其位置来识别，并由交错的信任网络组成。供应链攻击通过利用目标域的大量可信第三方(提供诸如 JavaScript 之类的资源)来损害目标域的用户。JavaScript 的无处不在，加上它在客户端机器上执行任意代码的能力，使得这种特殊的 Web 资源成为供应链攻击的理想载体。目前，还没有一种健壮的方法可以让浏览网页的用户验证他们从第三方收到的脚本内容是否是预期的内容。在本文中，我们提出了关键的见解，通知设计健壮的完整性机制，从我们的大规模分析6M 脚本，我们收集了爬行44K 域每天77天。我们发现，在现代网络生态系统中，经常变化的脚本应该被视为一流的公民，而且脚本变化的方式随着时间的推移保持不变。此外，我们在脚本提供程序本身的粒度上分析了严格完整性验证(例如，子资源完整性)的使用，提供了一个更完整的视角，并证明仅仅使用严格完整性不能提供令人满意的安全保证。我们的结论是，在没有额外的外部知识的情况下，客户端将良性变化与恶意变化区分开来是不可行的，因此需要一个新的协议来为客户端提供必要的上下文，以评估脚本变化的潜在影响。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+More+Things+Change,+the+More+They+Stay+the+Same:+Integrity+of+Modern+JavaScript)|0|
|[AppSniffer: Towards Robust Mobile App Fingerprinting Against VPN](https://doi.org/10.1145/3543507.3583473)|Sanghak Oh, Minwook Lee, Hyunwoo Lee, Elisa Bertino, Hyoungshick Kim|Sungkyunkwan University, Republic of Korea; Korea Institute of Energy Technology, Republic of Korea; Purdue University, USA|Application fingerprinting is a useful data analysis technique for network administrators, marketing agencies, and security analysts. For example, an administrator can adopt application fingerprinting techniques to determine whether a user’s network access is allowed. Several mobile application fingerprinting techniques (e.g., FlowPrint, AppScanner, and ET-BERT) were recently introduced to identify applications using the characteristics of network traffic. However, we find that the performance of the existing mobile application fingerprinting systems significantly degrades when a virtual private network (VPN) is used. To address such a shortcoming, we propose a framework dubbed AppSniffer that uses a two-stage classification process for mobile app fingerprinting. In the first stage, we distinguish VPN traffic from normal traffic; in the second stage, we use the optimal model for each traffic type. Specifically, we propose a stacked ensemble model using Light Gradient Boosting Machine (LightGBM) and a FastAI library-based neural network model to identify applications’ traffic when a VPN is used. To show the feasibility of AppSniffer, we evaluate the detection accuracy of AppSniffer for 150 popularly used Android apps. Our experimental results show that AppSniffer effectively identifies mobile applications over VPNs with F1-scores between 84.66% and 95.49% across four different VPN protocols. In contrast, the best state-of-the-art method (i.e., AppScanner) demonstrates significantly lower F1-scores between 25.63% and 47.56% in the same settings. Overall, when normal traffic and VPN traffic are mixed, AppSniffer achieves an F1-score of 90.63%, which is significantly better than AppScanner that shows an F1-score of 70.36%.|应用程序指纹分析对于网络管理员、营销机构和安全分析师来说是一种有用的数据分析技术。例如，管理员可以采用应用程序指纹技术来确定是否允许用户的网络访问。最近引入了几种移动应用程序指纹识别技术(例如 FlowPrint、 AppScanner 和 ET-BERT)来利用网络流量的特征识别应用程序。然而，我们发现当使用虚拟专用网络(VPN)时，现有的移动应用指纹识别系统的性能会显著下降。为了解决这个问题，我们提出了一个名为 AppSniffer 的框架，该框架使用两阶段的分类过程进行移动应用指纹识别。在第一阶段，我们将 VPN 流量与正常流量区分开来，在第二阶段，我们使用每种流量类型的最优模型。具体来说，我们提出了一个使用梯度提升机(LightGBM)的堆叠集成模型和一个基于 FastAI 库的神经网络模型来识别使用 VPN 时应用程序的流量。为了证明 AppSniffer 的可行性，我们评估了 AppSniffer 对150个广泛使用的 Android 应用程序的检测准确性。我们的实验结果表明，AppSniffer 能够有效地识别 VPN 上的移动应用，在四种不同的 VPN 协议中，F1得分在84.66% 到95.49% 之间。相比之下，最先进的方法(即 AppScanner)显示，在相同的设置下，F1得分在25.63% 和47.56% 之间显著降低。总的来说，当正常流量和 VPN 流量混合时，AppSniffer 的 F1得分为90.63% ，明显优于 AppScanner 的70.36% 的 F1得分。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AppSniffer:+Towards+Robust+Mobile+App+Fingerprinting+Against+VPN)|0|
|[RICC: Robust Collective Classification of Sybil Accounts](https://doi.org/10.1145/3543507.3583475)|Dongwon Shin, Suyoung Lee, Sooel Son|School of Computing, KAIST, Republic of Korea|A Sybil attack is a critical threat that undermines the trust and integrity of web services by creating and exploiting a large number of fake (i.e., Sybil) accounts. To mitigate this threat, previous studies have proposed leveraging collective classification to detect Sybil accounts. Recently, researchers have demonstrated that state-of-the-art adversarial attacks are able to bypass existing collective classification methods, posing a new security threat. To this end, we propose RICC, the first robust collective classification framework, designed to identify adversarial Sybil accounts created by adversarial attacks. RICC leverages the novel observation that these adversarial attacks are highly tailored to a target collective classification model to optimize the attack budget. Owing to this adversarial strategy, the classification results for adversarial Sybil accounts often significantly change when deploying a new training set different from the original training set used for assigning prior reputation scores to user accounts. Leveraging this observation, RICC achieves robustness in collective classification by stabilizing classification results across different training sets randomly sampled in each round. RICC achieves false negative rates of 0.01, 0.11, 0.00, and 0.01 in detecting adversarial Sybil accounts for the Enron, Facebook, Twitter_S, and Twitter_L datasets, respectively. It also attains respective AUCs of 0.99, 1.00, 0.89, and 0.74 for these datasets, achieving high performance on the original task of detecting Sybil accounts. RICC significantly outperforms all existing Sybil detection methods, demonstrating superior robustness and efficacy in the collective classification of Sybil accounts.|Sybil 攻击是一个严重的威胁，它通过创建和利用大量虚假(比如 Sybil)帐户来破坏 Web 服务的信任和完整性。为了减轻这种威胁，以前的研究已经提出利用集体分类来检测 Sybil 帐户。最近，研究人员已经证明，最先进的对手攻击能够绕过现有的集体分类方法，构成新的安全威胁。为此，我们提出了 RICC，第一个健壮的集体分类框架，旨在识别由对手攻击创建的对手 Sybil 帐户。RICC 利用这种新颖的观察，即这些对手攻击高度适合于目标集体分类模型，以优化攻击预算。由于这种对抗性战略，当部署一套不同于原先用于为用户账户分配以前声誉评分的训练集的新训练集时，对抗性 Sybil 账户的分类结果往往会发生重大变化。利用这一观察结果，RICC 通过稳定每轮随机采样的不同训练集的分类结果来实现集体分类的鲁棒性。RICC 在检测 Enron，Facebook，Twitter _ S 和 Twitter _ L 数据集的对手 Sybil 帐户时分别达到0.01,0.11,0.00和0.01的假阴性率。它还为这些数据集分别获得0.99、1.00、0.89和0.74的 AUC，从而在检测 Sybil 帐户的原始任务上实现了高性能。RICC 显著优于所有现有的 Sybil 检测方法，在 Sybil 帐户的集体分类中显示出优越的稳健性和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RICC:+Robust+Collective+Classification+of+Sybil+Accounts)|0|
|[ZTLS: A DNS-based Approach to Zero Round Trip Delay in TLS handshake](https://doi.org/10.1145/3543507.3583516)|Sangwon Lim, Hyeonmin Lee, Hyunsoo Kim, Hyunwoo Lee, Ted Taekyoung Kwon|Korea Institute of Energy Technology, Republic of Korea; Seoul National University, Republic of Korea|Establishing secure connections fast to end-users is crucial to online services. However, when a client sets up a TLS session with a server, the TLS handshake needs one round trip time (RTT) to negotiate a session key. Additionally, establishing a TLS session also requires a DNS lookup (e.g., the A record lookup to fetch the IP address of the server) and a TCP handshake. In this paper, we propose ZTLS to eliminate the 1-RTT latency for the TLS handshake by leveraging the DNS. In ZTLS, a server distributes TLS handshake-related data (i.e., Diffie-Hellman elements), dubbed Z-data, as DNS records. A ZTLS client can fetch Z-data by DNS lookups and derive a session key. With the session key, the client can send encrypted data along with its ClientHello, achieving 0-RTT. ZTLS supports incremental deployability on the current TLS-based infrastructure. Our prototype-based experiments show that ZTLS is 1-RTT faster than TLS in terms of the first response time.|快速建立与终端用户的安全连接对在线服务至关重要。但是，当客户端设置与服务器的 TLS 会话时，TLS 握手需要一个往返时间(RTT)来协商会话密钥。此外，建立 TLS 会话还需要 DNS 查找(例如，A 记录查找以获取服务器的 IP 地址)和 TCP 握手。在本文中，我们提出利用 DNS 来消除 TLS 握手的1-RTT 延迟。在 ZTLS 中，服务器将与 TLS 握手相关的数据(例如，Diffie-Hellman 元素)(称为 Z-data)作为 DNS 记录分发。ZTLS 客户机可以通过 DNS 查找获取 Z 数据并派生会话密钥。使用会话密钥，客户端可以发送加密的数据及其 ClientHello，从而实现0-RTT。ZTLS 支持当前基于 TLS 的基础设施上的增量式部署。基于原型的实验表明，ZTLS 在第一响应时间方面比 TLS 快1-RTT。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ZTLS:+A+DNS-based+Approach+to+Zero+Round+Trip+Delay+in+TLS+handshake)|0|
|[AgrEvader: Poisoning Membership Inference against Byzantine-robust Federated Learning](https://doi.org/10.1145/3543507.3583542)|Yanjun Zhang, Guangdong Bai, Mahawaga Arachchige Pathum Chamikara, Mengyao Ma, Liyue Shen, Jingwei Wang, Surya Nepal, Minhui Xue, Long Wang, Joseph K. Liu|Deakin University, Australia; Intelligent Engine Department, Ant Group, MYBank, China; CSIRO's Data61, Australia; Monash University, Australia; The University of Queensland, Australia|The Poisoning Membership Inference Attack (PMIA) is a newly emerging privacy attack that poses a significant threat to federated learning (FL). An adversary conducts data poisoning (i.e., performing adversarial manipulations on training examples) to extract membership information by exploiting the changes in loss resulting from data poisoning. The PMIA significantly exacerbates the traditional poisoning attack that is primarily focused on model corruption. However, there has been a lack of a comprehensive systematic study that thoroughly investigates this topic. In this work, we conduct a benchmark evaluation to assess the performance of PMIA against the Byzantine-robust FL setting that is specifically designed to mitigate poisoning attacks. We find that all existing coordinate-wise averaging mechanisms fail to defend against the PMIA, while the detect-then-drop strategy was proven to be effective in most cases, implying that the poison injection is memorized and the poisonous effect rarely dissipates. Inspired by this observation, we propose AgrEvader, a PMIA that maximizes the adversarial impact on the victim samples while circumventing the detection by Byzantine-robust mechanisms. AgrEvader significantly outperforms existing PMIAs. For instance, AgrEvader achieved a high attack accuracy of between 72.78% (on CIFAR-10) to 97.80% (on Texas100), which is an average accuracy increase of 13.89% compared to the strongest PMIA reported in the literature. We evaluated AgrEvader on five datasets across different domains, against a comprehensive list of threat models, which included black-box, gray-box and white-box models for targeted and non-targeted scenarios. AgrEvader demonstrated consistent high accuracy across all settings tested. The code is available at: https://github.com/PrivSecML/AgrEvader.|中毒成员推理攻击(PMIA)是一种新兴的隐私攻击，对联邦学习(FL)构成了严重的威胁。一个对手进行数据中毒(即，对训练例子进行对抗性操作) ，通过利用数据中毒导致的损失变化来提取会员信息。PMIA 显著加剧了主要关注模型腐败的传统中毒攻击。然而，对这一课题的深入研究一直缺乏全面系统的研究。在这项工作中，我们进行了一个基准评估，以评估性能的 PMIA 对拜占庭稳健的 FL 设置，是专门设计用于减轻中毒攻击。我们发现所有现有的坐标平均机制都不能抵御 PMIA，而检测然后下降策略在大多数情况下被证明是有效的，这意味着毒物注射被记住，毒性效应很少消散。受这个观察的启发，我们提出 AgrEvader，一个 PMIA，它可以最大化对受害者样本的对抗性影响，同时规避拜占庭稳健机制的检测。AgrEvader 的性能明显优于现有的 PMIA。例如，AgrEvader 实现了72.78% (在 CIFAR-10上)至97.80% (在 Texas 100上)的高攻击准确率，与文献中报道的最强 PMIA 相比，平均准确率提高了13.89% 。我们在不同领域的五个数据集上对 AgrEvader 进行了评估，针对的是一个全面的威胁模型列表，其中包括针对目标和非目标场景的黑盒、灰盒和白盒模型。AgrEvader 在所有测试环境中都表现出一致的高精度。密码可于以下 https://github.com/privsecml/agrevader 索取:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AgrEvader:+Poisoning+Membership+Inference+against+Byzantine-robust+Federated+Learning)|0|
|[Event Prediction using Case-Based Reasoning over Knowledge Graphs](https://doi.org/10.1145/3543507.3583201)|Sola Shirai, Debarun Bhattacharjya, Oktie Hassanzadeh|IBM Research, USA; Rensselaer Polytechnic Institute, USA|Applying link prediction (LP) methods over knowledge graphs (KG) for tasks such as causal event prediction presents an exciting opportunity. However, typical LP models are ill-suited for this task as they are incapable of performing inductive link prediction for new, unseen event entities and they require retraining as knowledge is added or changed in the underlying KG. We introduce a case-based reasoning model, EvCBR, to predict properties about new consequent events based on similar cause-effect events present in the KG. EvCBR uses statistical measures to identify similar events and performs path-based predictions, requiring no training step. To generalize our methods beyond the domain of event prediction, we frame our task as a 2-hop LP task, where the first hop is a causal relation connecting a cause event to a new effect event and the second hop is a property about the new event which we wish to predict. The effectiveness of our method is demonstrated using a novel dataset of newsworthy events with causal relations curated from Wikidata, where EvCBR outperforms baselines including translational-distance-based, GNN-based, and rule-based LP models.|将知识图上的链接预测(LP)方法应用于因果事件预测等任务，提供了一个令人兴奋的机会。然而，典型的线性规划模型不适合这项任务，因为它们不能对新的、看不见的事件实体执行归纳链接预测，并且当知识在基础 KG 中被添加或改变时，它们需要再培训。我们引入一个案例推论模型，EvCBR，基于 KG 中相似的因果事件来预测新的结果事件的性质。EvCBR 使用统计方法来识别类似的事件，并执行基于路径的预测，不需要训练步骤。为了将我们的方法推广到事件预测领域之外，我们将我们的任务框架为一个2跳 LP 任务，其中第一跳是连接一个因果事件和一个新的结果事件的因果关系，第二跳是我们希望预测的新事件的性质。我们的方法的有效性是通过使用一个新的具有新闻价值的事件数据集和从 Wikidata 策划的因果关系来证明的，在那里，EvCBR 优于包括基于平移距离、基于 GNN 和基于规则的 LP 模型在内的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Event+Prediction+using+Case-Based+Reasoning+over+Knowledge+Graphs)|0|
|[Wikidata as a seed for Web Extraction](https://doi.org/10.1145/3543507.3583236)|Kunpeng Guo, Dennis Diefenbach, Antoine Gourru, Christophe Gravier|Laboratoire Hubert Curien UMR 5516, Université Jean Monnet, France; The QA Company SAS, France and Laboratoire Hubert Curien UMR 5516, Université Jean Monnet, France; The QA Company SAS, France and Laboratoire Hubert Curien, UMR CNRS 5516, Université Jean Monnet, France|Wikidata has grown to a knowledge graph with an impressive size. To date, it contains more than 17 billion triples collecting information about people, places, films, stars, publications, proteins, and many more. On the other side, most of the information on the Web is not published in highly structured data repositories like Wikidata, but rather as unstructured and semi-structured content, more concretely in HTML pages containing text and tables. Finding, monitoring, and organizing this data in a knowledge graph is requiring considerable work from human editors. The volume and complexity of the data make this task difficult and time-consuming. In this work, we present a framework that is able to identify and extract new facts that are published under multiple Web domains so that they can be proposed for validation by Wikidata editors. The framework is relying on question-answering technologies. We take inspiration from ideas that are used to extract facts from textual collections and adapt them to extract facts from Web pages. For achieving this, we demonstrate that language models can be adapted to extract facts not only from textual collections but also from Web pages. By exploiting the information already contained in Wikidata the proposed framework can be trained without the need for any additional learning signals and can extract new facts for a wide range of properties and domains. Following this path, Wikidata can be used as a seed to extract facts on the Web. Our experiments show that we can achieve a mean performance of 84.07 at F1-score. Moreover, our estimations show that we can potentially extract millions of facts that can be proposed for human validation. The goal is to help editors in their daily tasks and contribute to the completion of the Wikidata knowledge graph.|Wikidata 已经发展成为一个知识图表，其规模令人印象深刻。到目前为止，它包含超过170亿三倍收集信息的人，地方，电影，明星，出版物，蛋白质，和更多。另一方面，Web 上的大部分信息并不是像 Wikidata 那样在高度结构化的数据库中发布的，而是作为非结构化和半结构化的内容，更具体地说，是在包含文本和表格的 HTML 页面中发布的。在知识图中查找、监视和组织这些数据需要编辑人员进行大量的工作。数据的数量和复杂性使这项任务变得困难和耗时。在这项工作中，我们提出了一个框架，它能够识别和提取在多个 Web 域下发布的新事实，以便它们可以提交给 Wikidata 编辑器进行验证。该框架依赖于问答技术。我们从用于从文本集合中提取事实的想法中获得灵感，并对这些想法进行调整以从网页中提取事实。为了实现这一点，我们演示了语言模型不仅可以从文本集合中提取事实，还可以从 Web 页面中提取事实。通过利用 Wikidata 已有的信息，建议的框架可以在不需要任何额外学习信号的情况下进行培训，并且可以为广泛的属性和领域提取新的事实。沿着这条路径，Wikidata 可以作为种子在 Web 上提取事实。我们的实验表明，我们可以达到平均性能的84.07在 F1分数。此外，我们的估计表明，我们可以提取数以百万计的事实，可以提出人类验证。目标是帮助编辑完成他们的日常任务，并为完成 Wikidata 知识图谱做出贡献。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Wikidata+as+a+seed+for+Web+Extraction)|0|
|[Meta-Learning Based Knowledge Extrapolation for Temporal Knowledge Graph](https://doi.org/10.1145/3543507.3583279)|Zhongwu Chen, Chengjin Xu, Fenglong Su, Zhen Huang, Yong Dou|National University of Defense Technology, China; International Digital Economy Academy, China|In the last few years, the solution to Knowledge Graph (KG) completion via learning embeddings of entities and relations has attracted a surge of interest. Temporal KGs(TKGs) extend traditional Knowledge Graphs (KGs) by associating static triples with timestamps forming quadruples. Different from KGs and TKGs in the transductive setting, constantly emerging entities and relations in incomplete TKGs create demand to predict missing facts with unseen components, which is the extrapolation setting. Traditional temporal knowledge graph embedding (TKGE) methods are limited in the extrapolation setting since they are trained within a fixed set of components. In this paper, we propose a Meta-Learning based Temporal Knowledge Graph Extrapolation (MTKGE) model, which is trained on link prediction tasks sampled from the existing TKGs and tested in the emerging TKGs with unseen entities and relations. Specifically, we meta-train a GNN framework that captures relative position patterns and temporal sequence patterns between relations. The learned embeddings of patterns can be transferred to embed unseen components. Experimental results on two different TKG extrapolation datasets show that MTKGE consistently outperforms both the existing state-of-the-art models for knowledge graph extrapolation and specifically adapted KGE and TKGE baselines.|近年来，通过实体和关系的学习嵌入解决知识图(KG)完备性问题引起了人们极大的兴趣。时态 KGs (TKGs)通过将静态三元组与时间戳关联形成四元组，从而扩展了传统知识图(KGs)。与传导性背景下的幼稚园和传统幼稚园不同，不完整的传统幼稚园中不断出现的实体和关系创造了用看不见的成分预测缺失事实的需求，这就是外推背景。传统的时态知识图嵌入(TKGE)方法由于是在固定的组件集内进行训练，在外推设置上受到限制。本文提出了一种基于元学习的时态知识图外推(MTKGE)模型，该模型对从现有 TKG 中抽取的链接预测任务进行训练，并在新出现的具有不可见实体和关系的 TKG 中进行测试。具体地说，我们元训练了一个 GNN 框架，它捕获了关系之间的相对位置模式和时间序列模式。模式的学习嵌入可以转换为嵌入不可见的组件。在两个不同的 TKG 外推数据集上的实验结果表明，MTKGE 始终优于现有的最先进的知识图外推模型和特别适应的 KGE 和 TKGE 基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Meta-Learning+Based+Knowledge+Extrapolation+for+Temporal+Knowledge+Graph)|0|
|[Can Persistent Homology provide an efficient alternative for Evaluation of Knowledge Graph Completion Methods?](https://doi.org/10.1145/3543507.3583308)|Anson Bastos, Kuldeep Singh, Abhishek Nadgeri, Johannes Hoffart, Manish Singh, Toyotaro Suzumura|; Cerence GmbH and Zerotha Research, Germany; RWTH Aachen, Germany; SAP, Germany; IIT, Hyderabad, India|In this paper we present a novel method, $\textit{Knowledge Persistence}$ ($\mathcal{KP}$), for faster evaluation of Knowledge Graph (KG) completion approaches. Current ranking-based evaluation is quadratic in the size of the KG, leading to long evaluation times and consequently a high carbon footprint. $\mathcal{KP}$ addresses this by representing the topology of the KG completion methods through the lens of topological data analysis, concretely using persistent homology. The characteristics of persistent homology allow $\mathcal{KP}$ to evaluate the quality of the KG completion looking only at a fraction of the data. Experimental results on standard datasets show that the proposed metric is highly correlated with ranking metrics (Hits@N, MR, MRR). Performance evaluation shows that $\mathcal{KP}$ is computationally efficient: In some cases, the evaluation time (validation+test) of a KG completion method has been reduced from 18 hours (using Hits@10) to 27 seconds (using $\mathcal{KP}$), and on average (across methods & data) reduces the evaluation time (validation+test) by $\approx$ $\textbf{99.96}\%$.|本文提出了一种快速评估知识图(KG)完成方法的新方法，即知识持久化(KP)方法。目前以排名为基础的评估是以幼稚园的规模作为二次评估，评估时间较长，因此碳足印较高。$mathcal { KP } $通过拓扑数据分析的透镜来表示 KG 完成方法的拓扑，具体地使用持久同调来解决这个问题。持久同源的特征允许 $mathcal { KP } $评估 KG 完成的质量，只查看一小部分数据。在标准数据集上的实验结果表明，提出的度量与排序度量(Hits@N，MR，MRR)高度相关。性能评估表明 $mathcal { KP } $在计算上是有效的: 在某些情况下，KG 完成方法的评估时间(验证 + 测试)已经从18小时(使用 Hits@10)减少到27秒(使用 $mathcal { KP } $) ，并且平均(跨方法和数据)将评估时间(验证 + 测试)减少约 $$textbf {99.96}% $。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+Persistent+Homology+provide+an+efficient+alternative+for+Evaluation+of+Knowledge+Graph+Completion+Methods?)|0|
|[Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment](https://doi.org/10.1145/3543507.3583328)|Qian Li, Shu Guo, Yangyifei Luo, Cheng Ji, Lihong Wang, Jiawei Sheng, Jianxin Li|Beihang University, China; Institute of Information Engineering, Chinese Academy of Sciences, China; National Computer Network Emergency Response Technical Team/Coordination Center of China, China|The multi-modal entity alignment (MMEA) aims to find all equivalent entity pairs between multi-modal knowledge graphs (MMKGs). Rich attributes and neighboring entities are valuable for the alignment task, but existing works ignore contextual gap problems that the aligned entities have different numbers of attributes on specific modality when learning entity representations. In this paper, we propose a novel attribute-consistent knowledge graph representation learning framework for MMEA (ACK-MMEA) to compensate the contextual gaps through incorporating consistent alignment knowledge. Attribute-consistent KGs (ACKGs) are first constructed via multi-modal attribute uniformization with merge and generate operators so that each entity has one and only one uniform feature in each modality. The ACKGs are then fed into a relation-aware graph neural network with random dropouts, to obtain aggregated relation representations and robust entity representations. In order to evaluate the ACK-MMEA facilitated for entity alignment, we specially design a joint alignment loss for both entity and attribute evaluation. Extensive experiments conducted on two benchmark datasets show that our approach achieves excellent performance compared to its competitors.|多模态实体对齐(MMEA)的目的是寻找多模态知识图(MMKG)之间的所有等价实体对。丰富的属性和相邻实体对于对齐任务是有价值的，但现有的研究忽略了在学习实体表示时，对齐实体在特定情态下具有不同属性数目的上下文缺口问题。本文提出了一种新的基于属性一致性知识图表示的 MMEA (ACK-MMEA)学习框架，通过引入一致性对齐知识来弥补语境差异。属性一致的 KG (ACKG)首先通过多模态属性一致化构造，并使用合并和生成算子，使每个实体在每个模态中只有一个一致的特征。然后将这些 ACKG 输入到一个具有随机辍学的关系感知图神经网络中，以获得聚合关系表示和鲁棒实体表示。为了方便地评估 ACK-MMEA 算法对实体对齐的效果，我们特别设计了一种用于实体和属性评估的联合对齐损失算法。在两个基准数据集上进行的大量实验表明，与竞争对手相比，我们的方法取得了很好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Attribute-Consistent+Knowledge+Graph+Representation+Learning+for+Multi-Modal+Entity+Alignment)|0|
|[Hierarchy-Aware Multi-Hop Question Answering over Knowledge Graphs](https://doi.org/10.1145/3543507.3583376)|Junnan Dong, Qinggang Zhang, Xiao Huang, Keyu Duan, Qiaoyu Tan, Zhimeng Jiang|The Hong Kong Polytechnic University, Hong Kong; Texas A&M University, USA; National University of Singapore, Singapore|Knowledge graphs (KGs) have been widely used to enhance complex question answering (QA). To understand complex questions, existing studies employ language models (LMs) to encode contexts. Despite the simplicity, they neglect the latent relational information among question concepts and answers in KGs. While question concepts ubiquitously present hyponymy at the semantic level, e.g., mammals and animals, this feature is identically reflected in the hierarchical relations in KGs, e.g., a_type_of. Therefore, we are motivated to explore comprehensive reasoning by the hierarchical structures in KGs to help understand questions. However, it is non-trivial to reason over tree-like structures compared with chained paths. Moreover, identifying appropriate hierarchies relies on expertise. To this end, we propose HamQA, a novel Hierarchy-aware multi-hop Question Answering framework on knowledge graphs, to effectively align the mutual hierarchical information between question contexts and KGs. The entire learning is conducted in Hyperbolic space, inspired by its advantages of embedding hierarchical structures. Specifically, (i) we design a context-aware graph attentive network to capture context information. (ii) Hierarchical structures are continuously preserved in KGs by minimizing the Hyperbolic geodesic distances. The comprehensive reasoning is conducted to jointly train both components and provide a top-ranked candidate as an optimal answer. We achieve a higher ranking than the state-of-the-art multi-hop baselines on the official OpenBookQA leaderboard with an accuracy of 85%.|知识图已被广泛应用于提高复杂问题回答(QA)能力。为了理解复杂的问题，现有的研究采用语言模型(LM)对语境进行编码。幼儿教育中的问题概念和答案尽管简单，却忽视了潜在的关系信息。问题概念普遍存在于语义层面上的上下义关系，例如哺乳动物和动物，而这一特征在幼儿园的等级关系中得到了完全相同的体现，例如 a _ type _ of。因此，本研究旨在探讨幼稚园层级结构下的综合推理模式，以帮助学生理解问题。然而，与链式路径相比，对树状结构进行推理是非常重要的。此外，识别适当的层次结构依赖于专业知识。为此，我们提出了一种新的基于知识图的层次感知多跳问答框架 HamQA，以有效地调整问题上下文和 KG 之间的相互层次信息。整个学习过程是在双曲空间中进行的，灵感来自于其嵌入层级结构的优势。具体来说，(i)我们设计了一个上下文感知图注意网络来捕获上下文信息。(ii)幼稚园透过尽量减少双曲线测地距离，不断保留层次结构。通过综合推理对两个构件进行联合训练，并提供一个排名靠前的候选者作为最优答案。在 OpenBookQA 的官方排行榜上，我们获得了比最先进的多跳基线更高的排名，准确率达到85% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchy-Aware+Multi-Hop+Question+Answering+over+Knowledge+Graphs)|0|
|[Unsupervised Entity Alignment for Temporal Knowledge Graphs](https://doi.org/10.1145/3543507.3583381)|Xiaoze Liu, Junyang Wu, Tianyi Li, Lu Chen, Yunjun Gao|Zhejiang University, China; Aalborg University, Denmark|Entity alignment (EA) is a fundamental data integration task that identifies equivalent entities between different knowledge graphs (KGs). Temporal Knowledge graphs (TKGs) extend traditional knowledge graphs by introducing timestamps, which have received increasing attention. State-of-the-art time-aware EA studies have suggested that the temporal information of TKGs facilitates the performance of EA. However, existing studies have not thoroughly exploited the advantages of temporal information in TKGs. Also, they perform EA by pre-aligning entity pairs, which can be labor-intensive and thus inefficient. In this paper, we present DualMatch which effectively fuses the relational and temporal information for EA. DualMatch transfers EA on TKGs into a weighted graph matching problem. More specifically, DualMatch is equipped with an unsupervised method, which achieves EA without necessitating seed alignment. DualMatch has two steps: (i) encoding temporal and relational information into embeddings separately using a novel label-free encoder, Dual-Encoder; and (ii) fusing both information and transforming it into alignment using a novel graph-matching-based decoder, GM-Decoder. DualMatch is able to perform EA on TKGs with or without supervision, due to its capability of effectively capturing temporal information. Extensive experiments on three real-world TKG datasets offer the insight that DualMatch outperforms the state-of-the-art methods in terms of H@1 by 2.4% - 10.7% and MRR by 1.7% - 7.6%, respectively.|实体对齐(EA)是识别不同知识图之间等价实体的基本数据集成任务。时态知识图(TKGs)通过引入时间戳对传统知识图进行扩展，越来越受到人们的关注。最先进的时间意识电位研究表明，TKG 的时间信息有助于电位的执行。然而，现有的研究并没有充分利用时间信息在 TKG 中的优势。此外，它们通过预对齐实体对来执行 EA，这可能是劳动密集型的，因此效率低下。本文提出了一种有效融合关系信息和时间信息的双匹配算法。DualMatch 将 TKG 上的 EA 转化为一个加权图匹配问题。更具体地说，DualMatch 配备了一种无监督的方法，它不需要种子对齐就可以实现 EA。DualMatch 有两个步骤: (i)使用一种新的无标签编码器——双编码器，将时间和关系信息分别编码到嵌入中; (ii)使用一种新的基于图匹配的解码器—— GM-Decder，将两种信息融合并将其转换为对齐。DualMatch 能够在有监督或无监督的情况下对 TKG 进行 EA，因为它能够有效地捕获时间信息。在三个真实世界的 TKG 数据集上进行的广泛实验提供了这样的见解: DualMatch 在 H@1方面分别优于最先进的方法2.4% -10.7% 和 MRR 1.7% -7.6% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Entity+Alignment+for+Temporal+Knowledge+Graphs)|0|
|[IMF: Interactive Multimodal Fusion Model for Link Prediction](https://doi.org/10.1145/3543507.3583554)|Xinhang Li, Xiangyu Zhao, Jiaxing Xu, Yong Zhang, Chunxiao Xing|School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Computer Science and Technology, Tsinghua University, China; School of Data Science, City University of Hong Kong, Hong Kong|Link prediction aims to identify potential missing triples in knowledge graphs. To get better results, some recent studies have introduced multimodal information to link prediction. However, these methods utilize multimodal information separately and neglect the complicated interaction between different modalities. In this paper, we aim at better modeling the inter-modality information and thus introduce a novel Interactive Multimodal Fusion (IMF) model to integrate knowledge from different modalities. To this end, we propose a two-stage multimodal fusion framework to preserve modality-specific knowledge as well as take advantage of the complementarity between different modalities. Instead of directly projecting different modalities into a unified space, our multimodal fusion module limits the representations of different modalities independent while leverages bilinear pooling for fusion and incorporates contrastive learning as additional constraints. Furthermore, the decision fusion module delivers the learned weighted average over the predictions of all modalities to better incorporate the complementarity of different modalities. Our approach has been demonstrated to be effective through empirical evaluations on several real-world datasets. The implementation code is available online at https://github.com/HestiaSky/IMF-Pytorch.|链接预测旨在识别知识图中潜在的缺失三元组。为了得到更好的结果，最近的一些研究引入了多模态信息来链接预测。然而，这些方法分别利用多模态信息，而忽略了不同模态之间复杂的相互作用。本文旨在更好地建立模态间信息模型，从而引入一种新的交互式多模态融合(IMF)模型来整合来自不同模态的知识。为此，我们提出了一个两阶段的多模式融合框架，以保存模式特定的知识，并利用不同模式之间的互补性。我们的多模态融合模块不是直接将不同的模式投射到一个统一的空间中，而是限制不同模式独立的表示，同时利用双线性融合池进行融合，并将对比学习作为额外的约束。此外，决策融合模块提供所有模式预测的加权平均数，以更好地纳入不同模式的互补性。我们的方法已被证明是有效的，通过几个实际数据集的经验评估。实施守则可于网上 https://github.com/hestiasky/imf-pytorch 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IMF:+Interactive+Multimodal+Fusion+Model+for+Link+Prediction)|0|
|[Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer](https://doi.org/10.1145/3543507.3583301)|Wen Zhang, Yushan Zhu, Mingyang Chen, Yuxia Geng, Yufeng Huang, Yajing Xu, Wenting Song, Huajun Chen|Huawei Technologies Co., Ltd, China; Zhejiang University, China|Knowledge graphs (KG) are essential background knowledge providers in many tasks. When designing models for KG-related tasks, one of the key tasks is to devise the Knowledge Representation and Fusion (KRF) module that learns the representation of elements from KGs and fuses them with task representations. While due to the difference of KGs and perspectives to be considered during fusion across tasks, duplicate and ad hoc KRF modules design are conducted among tasks. In this paper, we propose a novel knowledge graph pretraining model KGTransformer that could serve as a uniform KRF module in diverse KG-related tasks. We pretrain KGTransformer with three self-supervised tasks with sampled sub-graphs as input. For utilization, we propose a general prompt-tuning mechanism regarding task data as a triple prompt to allow flexible interactions between task KGs and task data. We evaluate pretrained KGTransformer on three tasks, triple classification, zero-shot image classification, and question answering. KGTransformer consistently achieves better results than specifically designed task models. Through experiments, we justify that the pretrained KGTransformer could be used off the shelf as a general and effective KRF module across KG-related tasks. The code and datasets are available at https://github.com/zjukg/KGTransformer.|在许多任务中，知识图是必不可少的背景知识提供者。在设计幼儿园相关任务的模型时，其中一个关键任务是设计知识表示与融合(KRF)模块，该模块学习幼儿园元素的表示并将其与任务表示融合。由于不同任务之间的融合需要考虑幼儿园和视角的差异，在不同任务之间进行了重复和临时的 KRF 模块设计。在本文中，我们提出了一个新的知识图预训练模型 KG 变换器，可以作为一个统一的 KRF 模块在不同的 KG 相关的任务。我们以采样子图作为输入，预训练三个自我监督任务。为了利用起见，我们提出了一种通用的提示调优机制，将任务数据视为三重提示，以允许任务 KG 和任务数据之间进行灵活的交互。我们评估预先训练的 KG 变压器在三个任务，三重分类，零镜头图像分类和问题回答。与专门设计的任务模型相比，KG 变压器始终能够取得更好的结果。通过实验证明，预先训练好的 KG 变压器可以作为一个通用的、有效的 KRF 模块在 KG 相关的任务中使用。代码和数据集可在 https://github.com/zjukg/kgtransformer 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Structure+Pretraining+and+Prompt+Tuning+for+Knowledge+Graph+Transfer)|0|
|[TEA: Time-aware Entity Alignment in Knowledge Graphs](https://doi.org/10.1145/3543507.3583317)|Yu Liu, Wen Hua, Kexuan Xin, Saeid Hosseini, Xiaofang Zhou|Sohar University, Oman; School of Information Technology and Electrical Engineering, The University of Queensland, Australia; Department of Computing, The Hong Kong Polytechnic University, China; School of Information Science and Technology, University of International Relations, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, China|Entity alignment (EA) aims to identify equivalent entities between knowledge graphs (KGs), which is a key technique to improve the coverage of existing KGs. Current EA models largely ignore the importance of time information contained in KGs and treat relational facts or attribute values of entities as time-invariant. However, real-world entities could evolve over time, making the knowledge of the aligned entities very different in multiple KGs. This may cause incorrect matching between KGs if such entity dynamics is ignored. In this paper, we propose a time-aware entity alignment (TEA) model that discovers the entity evolving behaviour by exploring the time contexts in KGs and aggregates various contextual information to make the alignment decision. In particular, we address two main challenges in the TEA model: 1) How to identify highly-correlated temporal facts; 2) How to capture entity dynamics and incorporate it to learn a more informative entity representation for the alignment task. Experiments on real-world datasets1 verify the superiority of our TEA model over state-of-the-art entity aligners.|实体对齐(EA)的目的是识别知识图之间的等价实体，这是提高现有知识图覆盖率的关键技术。现有的 EA 模型大多忽略了幼儿园教师所包含的时间信息的重要性，而将实体的关系事实或属性值视为时不变量。然而，真实世界的实体可以随着时间的推移而发展，使得在多个 KG 中对齐的实体的知识非常不同。如果忽略这种实体动态，则可能导致 KG 之间的不正确匹配。本文提出了一种基于时间感知的实体对齐(TEA)模型，该模型通过对幼儿园时间环境的探索，发现实体的演化行为，并聚合各种环境信息进行对齐决策。特别是，我们解决了 TEA 模型中的两个主要挑战: 1)如何识别高度相关的时间事实; 2)如何捕获实体动态并将其结合起来，以便为对齐任务学习一个更具信息性的实体表示。在实际数据集上的实验验证了我们的 TEA 模型相对于最先进的实体校准器的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TEA:+Time-aware+Entity+Alignment+in+Knowledge+Graphs)|0|
|[Knowledge Graph Completion with Counterfactual Augmentation](https://doi.org/10.1145/3543507.3583401)|Heng Chang, Jie Cai, Jia Li|Tsinghua University, China; Hong Kong University of Science and Technology (Guangzhou), China|Graph Neural Networks (GNNs) have demonstrated great success in Knowledge Graph Completion (KGC) by modeling how entities and relations interact in recent years. However, most of them are designed to learn from the observed graph structure, which appears to have imbalanced relation distribution during the training stage. Motivated by the causal relationship among the entities on a knowledge graph, we explore this defect through a counterfactual question: "would the relation still exist if the neighborhood of entities became different from observation?". With a carefully designed instantiation of a causal model on the knowledge graph, we generate the counterfactual relations to answer the question by regarding the representations of entity pair given relation as context, structural information of relation-aware neighborhood as treatment, and validity of the composed triplet as the outcome. Furthermore, we incorporate the created counterfactual relations with the GNN-based framework on KGs to augment their learning of entity pair representations from both the observed and counterfactual relations. Experiments on benchmarks show that our proposed method outperforms existing methods on the task of KGC, achieving new state-of-the-art results. Moreover, we demonstrate that the proposed counterfactual relations-based augmentation also enhances the interpretability of the GNN-based framework through the path interpretations of predictions.|近年来，图神经网络(GNN)通过对实体和关系之间的相互作用进行建模，在知识图完成(KGC)方面取得了巨大的成功。然而，它们中的大多数都是为了学习观察到的图形结构，在训练阶段，这些图形结构似乎有不平衡的关系分布。受知识图上实体之间因果关系的启发，我们通过一个反事实的问题来探讨这一缺陷: “如果实体的邻域变得不同于观察，这种关系还会存在吗?”.通过在知识图上精心设计的因果模型实例，我们将给定关系的实体对的表示作为上下文，关系意识邻域的结构信息作为处理，组合三元组的有效性作为结果，生成反事实关系来回答这个问题。此外，我们将所建立的反事实关系与基于 GNN 的幼稚园框架结合起来，以增强幼稚园学生从观察到的关系和反事实关系中学习实体对表征的能力。基准测试实验表明，该方法在 KGC 任务上优于现有方法，取得了新的研究成果。此外，我们还证明了所提出的基于反事实关系的增强也通过对预测的路径解释增强了基于 GNN 的框架的可解释性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Graph+Completion+with+Counterfactual+Augmentation)|0|
|[Learning Social Meta-knowledge for Nowcasting Human Mobility in Disaster](https://doi.org/10.1145/3543507.3583991)|Renhe Jiang, Zhaonan Wang, Yudong Tao, Chuang Yang, Xuan Song, Ryosuke Shibasaki, ShuChing Chen, MeiLing Shyu|University of Missouri-Kansas City, USA; University of Miami, USA; The University of Tokyo, Japan|Human mobility nowcasting is a fundamental research problem for intelligent transportation planning, disaster responses and management, etc. In particular, human mobility under big disasters such as hurricanes and pandemics deviates from its daily routine to a large extent, which makes the task more challenging. Existing works mainly focus on traffic or crowd flow prediction in normal situations. To tackle this problem, in this study, disaster-related Twitter data is incorporated as a covariate to understand the public awareness and attention about the disaster events and thus perceive their impacts on the human mobility. Accordingly, we propose a Meta-knowledge-Memorizable Spatio-Temporal Network (MemeSTN), which leverages memory network and meta-learning to fuse social media and human mobility data. Extensive experiments over three real-world disasters including Japan 2019 typhoon season, Japan 2020 COVID-19 pandemic, and US 2019 hurricane season were conducted to illustrate the effectiveness of our proposed solution. Compared to the state-of-the-art spatio-temporal deep models and multivariate-time-series deep models, our model can achieve superior performance for nowcasting human mobility in disaster situations at both country level and state level.|人员流动临近预测是智能交通规划、灾害响应和管理等领域的基础性研究问题。特别是，在飓风和大流行病等重大灾害下，人员的流动性在很大程度上偏离了日常工作，这使得这项任务更具挑战性。现有的工作主要集中在正常情况下的交通或人流预测。为了解决这一问题，在这项研究中，将与灾害有关的推特数据作为一个协变量纳入，以了解公众对灾害事件的认识和关注，从而认识到它们对人员流动的影响。因此，我们提出了一个元知识记忆时空网络(MemeSTN) ，它利用记忆网络和元学习融合社会媒体和人类流动性数据。通过对日本2019年台风季节、日本2020年2019冠状病毒疾病大流行和美国2019年飓风季节等三种现实灾害的大量实验，说明了我们提出的解决方案的有效性。与现有的时空深度模型和多变量时间序列深度模型相比，该模型能够在国家和国家两级的灾害情况下实现人员流动的临近预测。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Social+Meta-knowledge+for+Nowcasting+Human+Mobility+in+Disaster)|0|
|[Cashing in on Contacts: Characterizing the OnlyFans Ecosystem](https://doi.org/10.1145/3543507.3583210)|Pelayo Vallina, Ignacio Castro, Gareth Tyson|Hong Kong University of Science and Technology, China; Queen Mary University of London, United Kingdom; IMDEA Networks, Spain and Universidad Carlos III de Madrid, Spain|Adult video-sharing has undergone dramatic shifts. New platforms that directly interconnect (often amateur) producers and consumers now allow content creators to promote material across the web and directly monetize the content they produce. OnlyFans is the most prominent example of this new trend. OnlyFans is a content subscription service where creators earn money from users who subscribe to their material. In contrast to prior adult platforms, OnlyFans emphasizes creator-consumer interaction for audience accumulation and maintenance. This results in a wide cross-platform ecosystem geared towards bringing consumers to creators’ accounts. In this paper, we inspect this emerging ecosystem, focusing on content creators and the third-party platforms they connect to.|成人视频分享经历了戏剧性的转变。新的平台直接连接(通常是业余的)生产者和消费者，现在允许内容创作者通过网络推广材料，并直接将他们制作的内容货币化。“只有粉丝”是这一新趋势最突出的例子。OnlyFans 是一个内容订阅服务，创作者从订阅他们材料的用户那里赚钱。与之前的成人平台不同，OnlyFans 强调创作者与消费者之间的互动，以促进观众的积累和维护。这导致了一个广泛的跨平台生态系统，旨在将消费者带到创造者的帐户中。在本文中，我们考察了这个新兴的生态系统，重点关注内容创建者和他们连接到的第三方平台。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cashing+in+on+Contacts:+Characterizing+the+OnlyFans+Ecosystem)|0|
|[Automated Content Moderation Increases Adherence to Community Guidelines](https://doi.org/10.1145/3543507.3583275)|Manoel Horta Ribeiro, Justin Cheng, Robert West|Facebook, USA; EPFL, Switzerland|Online social media platforms use automated moderation systems to remove or reduce the visibility of rule-breaking content. While previous work has documented the importance of manual content moderation, the effects of automated content moderation remain largely unknown. Here, in a large study of Facebook comments (n=412M), we used a fuzzy regression discontinuity design to measure the impact of automated content moderation on subsequent rule-breaking behavior (number of comments hidden/deleted) and engagement (number of additional comments posted). We found that comment deletion decreased subsequent rule-breaking behavior in shorter threads (20 or fewer comments), even among other participants, suggesting that the intervention prevented conversations from derailing. Further, the effect of deletion on the affected user's subsequent rule-breaking behavior was longer-lived than its effect on reducing commenting in general, suggesting that users were deterred from rule-breaking but not from commenting. In contrast, hiding (rather than deleting) content had small and statistically insignificant effects. Our results suggest that automated content moderation increases adherence to community guidelines.|在线社交媒体平台使用自动审核系统来删除或减少违规内容的可见性。虽然以前的工作已经记录了手动内容审核的重要性，但是自动内容审核的影响仍然很大程度上是未知的。在这里，在一项对 Facebook 评论(n = 412M)的大型研究中，我们使用模糊回归不连续性设计来衡量自动内容审核对后续违规行为(隐藏/删除评论数量)和参与度(发布额外评论数量)的影响。我们发现删除评论可以减少更短的线程(20条或更少的评论)中随后的违规行为，甚至在其他参与者中也是如此，这表明干预可以防止对话脱轨。此外，删除对受影响用户随后的违规行为的影响比它对减少一般评论的影响更为持久，这表明用户被阻止违规但不被阻止评论。相比之下，隐藏(而不是删除)内容的影响很小，而且在统计学上无关紧要。我们的研究结果表明，自动内容审核增加了对社区指导方针的遵守。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automated+Content+Moderation+Increases+Adherence+to+Community+Guidelines)|0|
|[Mental Health Coping Stories on Social Media: A Causal-Inference Study of Papageno Effect](https://doi.org/10.1145/3543507.3583350)|Yunhao Yuan, Koustuv Saha, Barbara Keller, Erkki Tapio Isometsä, Talayeh Aledavood|Microsoft Research, Canada; Department of Computer Science, Aalto University, Finland; University of Helsinki, Finland|The Papageno effect concerns how media can play a positive role in preventing and mitigating suicidal ideation and behaviors. With the increasing ubiquity and widespread use of social media, individuals often express and share lived experiences and struggles with mental health. However, there is a gap in our understanding about the existence and effectiveness of the Papageno effect in social media, which we study in this paper. In particular, we adopt a causal-inference framework to examine the impact of exposure to mental health coping stories on individuals on Twitter. We obtain a Twitter dataset with $\sim$2M posts by $\sim$10K individuals. We consider engaging with coping stories as the Treatment intervention, and adopt a stratified propensity score approach to find matched cohorts of Treatment and Control individuals. We measure the psychosocial shifts in affective, behavioral, and cognitive outcomes in longitudinal Twitter data before and after engaging with the coping stories. Our findings reveal that, engaging with coping stories leads to decreased stress and depression, and improved expressive writing, diversity, and interactivity. Our work discusses the practical and platform design implications in supporting mental wellbeing.|帕帕盖诺效应关注媒体如何在预防和减轻自杀念头和行为方面发挥积极作用。随着社交媒体的日益普及和广泛使用，个人往往表达和分享生活经历和心理健康的斗争。然而，对于社交媒体中巴巴哥诺效应的存在和有效性，我们的理解还存在差距，本文对此进行了研究。特别是，我们采用了一个因果推理框架来检验在 Twitter 上接触心理健康应对故事对个人的影响。我们获得了一个由 $sim $10K 个人发布的200万美元的 Twitter 数据集。我们考虑将应对故事作为治疗干预措施，并采用分层倾向评分方法来寻找匹配的治疗和控制个体队列。我们在参与应对故事之前和之后的纵向 Twitter 数据中测量情感、行为和认知结果的心理社会变化。我们的研究结果表明，参与应对故事可以减少压力和抑郁，提高表达性写作、多样性和互动性。我们的工作讨论了支持心理健康的实践和平台设计含义。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mental+Health+Coping+Stories+on+Social+Media:+A+Causal-Inference+Study+of+Papageno+Effect)|0|
|[A First Look at Public Service Websites from the Affordability Lens](https://doi.org/10.1145/3543507.3583415)|Rumaisa Habib, Aimen Inam, Ayesha Ali, Ihsan Ayyub Qazi, Zafar Ayyub Qazi|LUMS, Pakistan|Public service websites act as official gateways to services provided by governments. Many of these websites are essential for citizens to receive reliable information and online government services. However, the lack of affordability of mobile broadband services in many developing countries and the rising complexity of websites create barriers for citizens in accessing these government websites. This paper presents the first large-scale analysis of the affordability of public service websites in developing countries. We do this by collecting a corpus of 1900 public service websites, including public websites from nine developing countries and for comparison websites from nine developed countries. Our investigation is driven by website complexity analysis as well as evaluation through a recently proposed affordability index. Our analysis reveals that, in general, public service websites in developing countries do not meet the affordability target set by the UN’s Broadband Commission. However, we show that several countries can be brought within or closer to the affordability target by implementing webpage optimizations to reduce page sizes. We also discuss policy interventions that can help make access to public service website more affordable.|公共服务网站作为政府提供服务的官方门户。其中许多网站对于公民获得可靠信息和在线政府服务至关重要。然而，许多发展中国家缺乏负担得起的移动宽带服务，以及网站日益复杂，给公民访问这些政府网站造成了障碍。本文首次对发展中国家公共服务网站的可负担性进行了大规模分析。为此，我们收集了1900个公共服务网站，包括9个发展中国家的公共网站和9个发达国家的比较网站。我们的调查是由网站的复杂性分析以及评估通过最近提出的负担能力指数。我们的分析表明，总的来说，发展中国家的公共服务网站不能达到联合国宽带委员会设定的负担能力目标。然而，我们表明，若干国家可以通过实施网页优化来减少页面大小，从而达到或接近负担能力目标。我们还讨论了政策干预措施，这些措施可以帮助人们更负担得起访问公共服务网站的费用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+First+Look+at+Public+Service+Websites+from+the+Affordability+Lens)|0|
|[Who Funds Misinformation? A Systematic Analysis of the Ad-related Profit Routines of Fake News Sites](https://doi.org/10.1145/3543507.3583443)|Emmanouil Papadogiannakis, Panagiotis Papadopoulos, Evangelos P. Markatos, Nicolas Kourtellis||Fake news is an age-old phenomenon, widely assumed to be associated with political propaganda published to sway public opinion. Yet, with the growth of social media, it has become a lucrative business for Web publishers. Despite many studies performed and countermeasures proposed, unreliable news sites have increased in the last years their share of engagement among the top performing news sources. Stifling fake news impact depends on our efforts in limiting the (economic) incentives of fake news producers. In this paper, we aim at enhancing the transparency around these exact incentives, and explore: Who supports the existence of fake news websites via paid ads, either as an advertiser or an ad seller? Who owns these websites and what other Web business are they into? We are the first to systematize the auditing process of fake news revenue flows. We identify the companies that advertise in fake news websites and the intermediary companies responsible for facilitating those ad revenues. We study more than 2,400 popular news websites and show that well-known ad networks, such as Google and IndexExchange, have a direct advertising relation with more than 40% of fake news websites. Using a graph clustering approach on 114.5K sites, we show that entities who own fake news sites, also operate other types of websites pointing to the fact that owning a fake news website is part of a broader business operation.|假新闻是一种古老的现象，人们普遍认为它与政治宣传有关，通过发布假新闻来影响公众舆论。然而，随着社交媒体的发展，它已经成为网络出版商的一项有利可图的业务。尽管进行了许多研究并提出了对策，但不可靠的新闻网站在最佳新闻来源中所占的份额在过去几年中有所增加。遏制假新闻的影响取决于我们在限制假新闻制造者的(经济)动机方面的努力。在本文中，我们旨在提高透明度周围这些确切的激励，并探讨: 谁支持存在的虚假新闻网站通过付费广告，无论是作为广告商还是广告销售商？谁拥有这些网站，他们还涉足哪些其他网络业务？我们率先对虚假新闻收入流的审计过程进行了系统化。我们识别那些在假新闻网站上做广告的公司和那些为这些广告收入提供便利的中介公司。我们研究了2400多个热门新闻网站，发现谷歌和 IndexExchange 等知名广告网站与40% 以上的假新闻网站有直接的广告关系。通过对114.5 K 网站的图形聚类分析，我们发现拥有假新闻网站的实体，同时也经营其他类型的网站，这表明拥有假新闻网站是更广泛商业运作的一部分。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Who+Funds+Misinformation?+A+Systematic+Analysis+of+the+Ad-related+Profit+Routines+of+Fake+News+Sites)|0|
|[Evidence of Demographic rather than Ideological Segregation in News Discussion on Reddit](https://doi.org/10.1145/3543507.3583468)|Corrado Monti, Jacopo D'Ignazi, Michele Starnini, Gianmarco De Francisci Morales|CENTAI, Italy; ISI Foundation, Italy|We evaluate homophily and heterophily among ideological and demographic groups in a typical opinion formation context: online discussions of current news. We analyze user interactions across five years in the r/news community on Reddit, one of the most visited websites in the United States. Then, we estimate demographic and ideological attributes of these users. Thanks to a comparison with a carefully-crafted network null model, we establish which pairs of attributes foster interactions and which ones inhibit them. Individuals prefer to engage with the opposite ideological side, which contradicts the echo chamber narrative. Instead, demographic groups are homophilic, as individuals tend to interact within their own group - even in an online setting where such attributes are not directly observable. In particular, we observe age and income segregation consistently across years: users tend to avoid interactions when belonging to different groups. These results persist after controlling for the degree of interest by each demographic group in different news topics. Our findings align with the theory that affective polarization - the difficulty in socializing across political boundaries-is more connected with an increasingly divided society, rather than ideological echo chambers on social media. We publicly release our anonymized data set and all the code to reproduce our results: https://github.com/corradomonti/demographic-homophily|我们在一个典型的意见形成背景下评估意识形态和人口群体之间的同质性和异质性: 当前新闻的在线讨论。我们在美国访问量最大的网站之一 Reddit 的 r/news 社区分析了五年来的用户交互。然后，我们估计这些用户的人口统计学和意识形态属性。通过与精心构建的网络空模型进行比较，我们确定了哪些属性对促进交互，哪些属性对抑制交互。个人倾向于接触相反的意识形态，这与回音室叙事相矛盾。相反，人口统计学群体是同性恋，因为个人往往在他们自己的群体内互动-即使在网上设置，这些属性是不能直接观察到的。特别是，我们观察到年龄和收入隔离持续多年: 当用户属于不同的群体时，他们倾向于避免互动。在控制了每个人口群体对不同新闻主题的兴趣程度之后，这些结果仍然存在。我们的研究结果符合这样一个理论，即情感的两极分化——跨越政治界限进行社交的困难——更多地与日益分裂的社会有关，而不是与社交媒体上的意识形态回音室有关。我们公开发布我们的匿名数据集和所有重现结果的代码:  https://github.com/corradomonti/demographic-homophily|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evidence+of+Demographic+rather+than+Ideological+Segregation+in+News+Discussion+on+Reddit)|0|
|[Longitudinal Assessment of Reference Quality on Wikipedia](https://doi.org/10.1145/3543507.3583218)|Aitolkyn Baigutanova, Jaehyeon Myung, Diego SáezTrumper, AiJou Chou, Miriam Redi, Changwook Jung, Meeyoung Cha|Wikimedia Foundation, United Kingdom; IBS, Republic of Korea and KAIST, Republic of Korea; Wikimedia Foundation, Spain; School of Computing, KAIST, Republic of Korea; KAIST, Republic of Korea|Wikipedia plays a crucial role in the integrity of the Web. This work analyzes the reliability of this global encyclopedia through the lens of its references. We operationalize the notion of reference quality by defining reference need (RN), i.e., the percentage of sentences missing a citation, and reference risk (RR), i.e., the proportion of non-authoritative references. We release Citation Detective, a tool for automatically calculating the RN score, and discover that the RN score has dropped by 20 percent point in the last decade, with more than half of verifiable statements now accompanying references. The RR score has remained below 1% over the years as a result of the efforts of the community to eliminate unreliable references. We propose pairing novice and experienced editors on the same Wikipedia article as a strategy to enhance reference quality. Our quasi-experiment indicates that such a co-editing experience can result in a lasting advantage in identifying unreliable sources in future edits. As Wikipedia is frequently used as the ground truth for numerous Web applications, our findings and suggestions on its reliability can have a far-reaching impact. We discuss the possibility of other Web services adopting Wiki-style user collaboration to eliminate unreliable content.|维基百科在网络的完整性中扮演着至关重要的角色。本文从参考文献的角度分析了这部全球百科全书的可靠性。我们通过定义参考需求(RN) ，即缺少引用的句子的百分比和参考风险(RR) ，即非权威参考文献的比例，来操作参考质量的概念。我们发布了引文侦探，一个自动计算 RN 分数的工具，发现 RN 分数在过去的十年里下降了20% ，超过一半的可验证的陈述现在伴随着参考文献。多年来，由于社区努力消除不可靠的参考文献，RR 评分一直保持在1% 以下。我们建议将新手和经验丰富的编辑配对在同一维基百科文章上，作为提高参考文献质量的策略。我们的准实验表明，这种合作编辑的经验可以导致一个持久的优势，在识别不可靠的来源在未来的编辑。由于维基百科经常被用作众多 Web 应用程序的基本事实，我们对其可靠性的发现和建议可能产生深远的影响。我们讨论了其他 Web 服务采用 Wiki 风格的用户协作来消除不可靠内容的可能性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Longitudinal+Assessment+of+Reference+Quality+on+Wikipedia)|0|
|[Gateway Entities in Problematic Trajectories](https://doi.org/10.1145/3543507.3583283)|Xi Leslie Chen, Abhratanu Dutta, Sindhu Ernala, Stratis Ioannidis, Shankar Kalyanaraman, Israel Nir, Udi Weinsberg|Northwestern, USA; Meta, USA; Northeastern, USA; Columbia University, USA|Social media platforms like Facebook and YouTube connect people with communities that reflect their own values and experiences. People discover new communities either organically or through algorithmic recommendations based on their interests and preferences. We study online journeys users take through these communities, focusing particularly on ones that may lead to problematic outcomes. In particular, we propose and explore the concept of gateways, namely, entities associated with a higher likelihood of subsequent engagement with problematic content. We show, via a real-world application on Facebook groups, that a simple definition of gateway entities can be leveraged to reduce exposure to problematic content by 1% without any adverse impact on user engagement metrics. Motivated by this finding, we propose several formal definitions of gateways, via both frequentist and survival analysis methods, and evaluate their efficacy in predicting user behavior through offline experiments. Frequentist, duration-insensitive methods predict future harmful engagements with an 0.64–0.83 AUC, while survival analysis methods improve this to 0.72–0.90 AUC.|Facebook 和 YouTube 等社交媒体平台将人们与反映他们自身价值观和经历的社区联系起来。人们发现新的社区要么是有机的，要么是通过基于兴趣和偏好的算法推荐。我们研究用户通过这些社区进行的在线旅行，特别关注那些可能导致有问题的结果的在线旅行。特别是，我们提出并探索了网关的概念，即与后续参与有问题内容的可能性较高相关的实体。我们通过 Facebook 群组上的一个现实应用程序展示了，一个简单的网关实体定义可以用来减少1% 的问题内容暴露，而不会对用户参与度指标产生任何不利影响。基于这一发现，我们通过频率分析和生存分析的方法提出了几种网关的形式化定义，并通过离线实验评估了它们在预测用户行为方面的有效性。频繁，持续时间不敏感的方法预测未来的有害参与与0.64 -0.83 AUC，而生存分析方法将其改善为0.72 -0.90 AUC。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Gateway+Entities+in+Problematic+Trajectories)|0|
|[Unsupervised Anomaly Detection on Microservice Traces through Graph VAE](https://doi.org/10.1145/3543507.3583215)|Zhe Xie, Haowen Xu, Wenxiao Chen, Wanxue Li, Huai Jiang, Liangfei Su, Hanzhang Wang, Dan Pei|eBay, USA; eBay, China; Department of Computer Science and Technology, Tsinghua University, China|The microservice architecture is widely employed in large Internet systems. For each user request, a few of the microservices are called, and a trace is formed to record the tree-like call dependencies among microservices and the time consumption at each call node. Traces are useful in diagnosing system failures, but their complex structures make it difficult to model their patterns and detect their anomalies. In this paper, we propose a novel dual-variable graph variational autoencoder (VAE) for unsupervised anomaly detection on microservice traces. To reconstruct the time consumption of nodes, we propose a novel dispatching layer. We find that the inversion of negative log-likelihood (NLL) appears for some anomalous samples, which makes the anomaly score infeasible for anomaly detection. To address this, we point out that the NLL can be decomposed into KL-divergence and data entropy, whereas lower-dimensional anomalies can introduce an entropy gap with normal inputs. We propose three techniques to mitigate this entropy gap for trace anomaly detection: Bernoulli & Categorical Scaling, Node Count Normalization, and Gaussian Std-Limit. On five trace datasets from a top Internet company, our proposed TraceVAE achieves excellent F-scores.|微服务体系结构在大型 Internet 系统中得到了广泛的应用。对于每个用户请求，调用一些微服务，并形成一个跟踪来记录微服务之间的树状调用依赖关系以及每个调用节点的时间消耗。跟踪在诊断系统故障中很有用，但是它们的复杂结构使得建模它们的模式和检测它们的异常变得困难。在这篇文章中，我们提出了一个新的双变量图变分自动编码器(VAE) ，用于微服务跟踪的无监督异常检测。为了重构节点的时间消耗，我们提出了一种新的调度层。我们发现，一些异常样本会出现负对数似然(NLL)反演，这使得异常评分对于异常检测来说是不可行的。为了解决这个问题，我们指出 NLL 可以分解为 KL- 散度和数据熵，而低维异常可以引入正常输入的熵差。我们提出了三种技术来缓解跟踪异常检测的熵差: 伯努利和分类标度、节点计数归一化和高斯标准极限。在来自一家顶级互联网公司的五个跟踪数据集中，我们提出的 TraceVAE 获得了优秀的 F 分数。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Anomaly+Detection+on+Microservice+Traces+through+Graph+VAE)|0|
|[FedEdge: Accelerating Edge-Assisted Federated Learning](https://doi.org/10.1145/3543507.3583264)|Kaibin Wang, Qiang He, Feifei Chen, Hai Jin, Yun Yang|Deakin University, Australia; Swinburne University of Technology, Australia; Huazhong University of Science and Technology, China; Huazhong University of Science and Technology, China and Swinburne University of Technology, Australia|Federated learning (FL) has been widely acknowledged as a promising solution to training machine learning (ML) model training with privacy preservation. To reduce the traffic overheads incurred by FL systems, edge servers have been included between clients and the parameter server to aggregate clients’ local models. Recent studies on this edge-assisted hierarchical FL scheme have focused on ensuring or accelerating model convergence by coping with various factors, e.g., uncertain network conditions, unreliable clients, heterogeneous compute resources, etc. This paper presents our three new discoveries of the edge-assisted hierarchical FL scheme: 1) it wastes significant time during its two-phase training rounds; 2) it does not recognize or utilize model diversity when producing a global model; and 3) it is vulnerable to model poisoning attacks. To overcome these drawbacks, we propose FedEdge, a novel edge-assisted hierarchical FL scheme that accelerates model training with asynchronous local federated training and adaptive model aggregation. Extensive experiments are conducted on two widely-used public datasets. The results demonstrate that, compared with state-of-the-art FL schemes, FedEdge accelerates model convergence by 1.14 × −3.20 ×, and improves model accuracy by 2.14% - 6.63%.|联邦学习(FL)作为一种具有隐私保护的训练机器学习(ML)模型的解决方案，已经得到了广泛的认可。为了减少 FL 系统带来的流量开销，在客户端和参数服务器之间引入了边缘服务器，以聚合客户端的本地模型。近年来关于边缘辅助层次 FL 方案的研究主要集中在通过处理不确定的网络条件、不可靠的客户端、异构的计算资源等因素来保证或加速模型的收敛。本文介绍了边缘辅助层次 FL 方案的三个新发现: 1)它在两阶段训练中浪费了大量的时间; 2)它在生成全局模型时不能识别或利用模型的多样性; 3)它容易受到模型中毒攻击。为了克服这些缺点，我们提出了一种新的边缘辅助分层 FL 方案 FedEdge，该方案通过异步局部联邦训练和自适应模型聚合来加速模型训练。在两个广泛使用的公共数据集上进行了广泛的实验。结果表明，与现有的 FL 格式相比，FedEdge 格式加速模型收敛速度为1.14 × -3.20 × ，模型精度提高了2.14% -6.63% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedEdge:+Accelerating+Edge-Assisted+Federated+Learning)|0|
|[CausIL: Causal Graph for Instance Level Microservice Data](https://doi.org/10.1145/3543507.3583274)|Sarthak Chakraborty, Shaddy Garg, Shubham Agarwal, Ayush Chauhan, Shiv Kumar Saini|Adobe, India; The University of Texas at Austin, USA; Adobe Research, India|AI-based monitoring has become crucial for cloud-based services due to its scale. A common approach to AI-based monitoring is to detect causal relationships among service components and build a causal graph. Availability of domain information makes cloud systems even better suited for such causal detection approaches. In modern cloud systems, however, auto-scalers dynamically change the number of microservice instances, and a load-balancer manages the load on each instance. This poses a challenge for off-the-shelf causal structure detection techniques as they neither incorporate the system architectural domain information nor provide a way to model distributed compute across varying numbers of service instances. To address this, we develop CausIL, which detects a causal structure among service metrics by considering compute distributed across dynamic instances and incorporating domain knowledge derived from system architecture. Towards the application in cloud systems, CausIL estimates a causal graph using instance-specific variations in performance metrics, modeling multiple instances of a service as independent, conditional on system assumptions. Simulation study shows the efficacy of CausIL over baselines by improving graph estimation accuracy by ~25% as measured by Structural Hamming Distance whereas the real-world dataset demonstrates CausIL's applicability in deployment settings.|基于人工智能的监控由于其规模已经成为云服务的关键。基于人工智能监控的一种常见方法是检测服务组件之间的因果关系，并建立因果图。领域信息的可用性使得云系统更加适合这种因果检测方法。然而，在现代云系统中，自动伸缩器动态地更改微服务实例的数量，并且负载平衡器管理每个实例上的负载。这对现成的因果结构检测技术提出了挑战，因为它们既没有合并系统架构领域信息，也没有提供跨不同数量的服务实例建模分布式计算的方法。为了解决这个问题，我们开发了 CausIL，它通过考虑跨动态实例分布的计算和合并来自系统架构的领域知识来检测服务度量之间的因果结构。对于云系统中的应用程序，CausIL 使用特定于实例的性能指标变化来估计因果图，将服务的多个实例建模为独立的，并以系统假设为条件。仿真研究显示，通过结构化汉明距离测量，causIL 的图形估计精度比基线提高了约25% ，而现实世界的数据集证明了 causIL 在部署设置中的适用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CausIL:+Causal+Graph+for+Instance+Level+Microservice+Data)|0|
|[Learning Cooperative Oversubscription for Cloud by Chance-Constrained Multi-Agent Reinforcement Learning](https://doi.org/10.1145/3543507.3583298)|Junjie Sheng, Lu Wang, Fangkai Yang, Bo Qiao, Hang Dong, Xiangfeng Wang, Bo Jin, Jun Wang, Si Qin, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang|East China Normal University, China; Microsoft Research, China; Microsoft 365, USA|Oversubscription is a common practice for improving cloud resource utilization. It allows the cloud service provider to sell more resources than the physical limit, assuming not all users would fully utilize the resources simultaneously. However, how to design an oversubscription policy that improves utilization while satisfying the some safety constraints remains an open problem. Existing methods and industrial practices are over-conservative, ignoring the coordination of diverse resource usage patterns and probabilistic constraints. To address these two limitations, this paper formulates the oversubscription for cloud as a chance-constrained optimization problem and propose an effective Chance Constrained Multi-Agent Reinforcement Learning (C2MARL) method to solve this problem. Specifically, C2MARL reduces the number of constraints by considering their upper bounds and leverages a multi-agent reinforcement learning paradigm to learn a safe and optimal coordination policy. We evaluate our C2MARL on an internal cloud platform and public cloud datasets. Experiments show that our C2MARL outperforms existing methods in improving utilization ($20\%\sim 86\%$) under different levels of safety constraints.|超额订阅是提高云资源利用率的常见做法。它允许云服务提供商出售比物理限制更多的资源，假设并非所有用户都会同时充分利用这些资源。然而，如何设计一个超额订阅策略来提高利用率，同时满足一些安全约束仍然是一个悬而未决的问题。现有的方法和工业实践过于保守，忽视了不同资源使用模式和概率约束的协调。为了解决这两个限制，本文将云计算的超额认购作为一个机会约束的最佳化问题，并提出了一种有效的机会约束多强化学习代理(c2MARL)方法来解决这个问题。具体来说，C2MARL 通过考虑约束的上限来减少约束的数量，并利用多主体强化学习范式来学习安全和最优的协调策略。我们在内部云平台和公共云数据集上评估 C2MARL。实验表明，在不同的安全约束水平下，我们的 C2MARL 在提高利用率方面优于现有的方法(20% sim 86% $)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Cooperative+Oversubscription+for+Cloud+by+Chance-Constrained+Multi-Agent+Reinforcement+Learning)|0|
|[CMDiagnostor: An Ambiguity-Aware Root Cause Localization Approach Based on Call Metric Data](https://doi.org/10.1145/3543507.3583302)|Qingyang Yu, Changhua Pei, Bowen Hao, Mingjie Li, Zeyan Li, Shenglin Zhang, Xianglin Lu, Rui Wang, Jiaqi Li, Zhenyu Wu, Dan Pei|Tsinghua University, China; Nankai University, China; Computer Network Information Center, Chinese Academy of Sciences, China; Tencent, China|The availability of online services is vital as its strong relevance to revenue and user experience. To ensure online services’ availability, quickly localizing the root causes of system failures is crucial. Given the high resource consumption of traces, call metric data are widely used by existing approaches to construct call graphs in practice. However, ambiguous correspondences between upstream and downstream calls may exist and result in exploring unexpected edges in the constructed call graph. Conducting root cause localization on this graph may lead to misjudgments of real root causes. To the best of our knowledge, we are the first to investigate such ambiguity, which is overlooked in the existing literature. Inspired by the law of large numbers and the Markov properties of network traffic, we propose a regression-based method (named AmSitor) to address this problem effectively. Based on AmSitor, we propose an ambiguity-aware root cause localization approach based on Call Metric Data named CMDiagnostor, containing metric anomaly detection, ambiguity-free call graph construction, root cause exploration, and candidate root cause ranking modules. The comprehensive experimental evaluations conducted on real-world datasets show that our CMDiagnostor can outperform the state-of-the-art approaches by 14% on the top-5 hit rate. Moreover, AmSitor can also be applied to existing baseline approaches separately to improve their performances one step further. The source code is released at https://github.com/NetManAIOps/CMDiagnostor.|在线服务的提供至关重要，因为它与收入和用户体验密切相关。为了确保在线服务的可用性，快速本地化系统故障的根本原因是至关重要的。由于跟踪的高资源消耗，呼叫度量数据在现有的呼叫图构造方法中得到了广泛的应用。然而，上游或下游呼叫之间可能存在模糊的对应关系，从而导致在构建的呼叫图中探索意外的边缘。在这个图上进行根本原因定位可能会导致对真正根本原因的误判。据我们所知，我们是第一个研究这种模糊性，这是忽视了现有的文献。受大数定律和网络流量的马尔可夫特性的启发，我们提出了一种基于回归的方法(AmSitor)来有效地解决这一问题。基于 AmSitor，我们提出了一种基于调用度量数据的歧义感知根源定位方法 CMDiagnostor，该方法包含度量异常检测、无歧义调用图结构、根源探索和候选根源排序模块。对真实世界数据集进行的综合实验评估表明，我们的 CMDiagnostor 在前5位的命中率方面可以比最先进的方法高出14% 。此外，AmSitor 还可以分别应用于现有的基线方法，以进一步提高其性能。源代码在 https://github.com/netmanaiops/cmdiagnostor 发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CMDiagnostor:+An+Ambiguity-Aware+Root+Cause+Localization+Approach+Based+on+Call+Metric+Data)|0|
|[Visual-Aware Testing and Debugging for Web Performance Optimization](https://doi.org/10.1145/3543507.3583323)|Xinlei Yang, Wei Liu, Hao Lin, Zhenhua Li, Feng Qian, Xianlong Wang, Yunhao Liu, Tianyin Xu||Web performance optimization services, or web performance optimizers (WPOs), play a critical role in today’s web ecosystem by improving page load speed and saving network traffic. However, WPOs are known for introducing visual distortions that disrupt the users’ web experience. Unfortunately, visual distortions are hard to analyze, test, and debug, due to their subjective measure, dynamic content, and sophisticated WPO implementations. This paper presents Vetter, a novel and effective system that automatically tests and debugs visual distortions. Its key idea is to reason about the morphology of web pages, which describes the topological forms and scale-free geometrical structures of visual elements. Vetter efficiently calculates morphology and comparatively analyzes the morphologies of web pages before and after a WPO, which acts as a differential test oracle. Such morphology analysis enables Vetter to detect visual distortions accurately and reliably. Vetter further diagnoses the detected visual distortions to pinpoint the root causes in WPOs’ source code. This is achieved by morphological causal inference, which localizes the offending visual elements that trigger the distortion and maps them to the corresponding code. We applied Vetter to four representative WPOs. Vetter discovers 21 unknown defects responsible for 98% visual distortions; 12 of them have been confirmed and 5 have been fixed.|Web 性能优化服务或 Web 性能优化器(WPO)通过提高页面加载速度和节省网络流量在当今的 Web 生态系统中发挥着关键作用。然而，WPO 以引入视觉失真而闻名，这种失真会扰乱用户的网络体验。不幸的是，由于视觉失真的主观测量、动态内容和复杂的 WPO 实现，它们很难进行分析、测试和调试。本文介绍了 Vetter，一个新颖有效的系统，自动测试和调试视觉失真。其核心思想是对网页的形态进行推理，描述视觉元素的拓扑形态和无标度几何结构。Vetter 有效地计算了 WPO 前后网页的形态，并对比分析了 WPO 前后网页的形态，起到了差异化测试的作用。这种形态学分析使维特能够准确可靠地检测视觉失真。Vetter 进一步诊断检测到的视觉失真，以确定 WPO 源代码中的根本原因。这是通过形态学的因果推理来实现的，它定位引发失真的视觉元素，并将它们映射到相应的代码。我们将 Vetter 应用于四个有代表性的 WPO。Vetter 发现了21个未知缺陷，造成了98% 的视觉失真，其中12个已经被证实，5个已经被修复。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Visual-Aware+Testing+and+Debugging+for+Web+Performance+Optimization)|0|
|[Demystifying Mobile Extended Reality in Web Browsers: How Far Can We Go?](https://doi.org/10.1145/3543507.3583329)|Weichen Bi, Yun Ma, Deyu Tian, Qi Yang, Mingtao Zhang, Xiang Jing|Institute for Artificial Intelligence, Peking University, China; School of Computer Science, Peking University, China; School of Computer Science and Engineering, University of New South Wales, Australia; School of Software & Microelectronics, Peking University, China|Mobile extended reality (XR) has developed rapidly in recent years. Compared with the app-based XR, XR in web browsers has the advantages of being lightweight and cross-platform, providing users with a pervasive experience. Therefore, many frameworks are emerging to support the development of XR in web browsers. However, little has been known about how well these frameworks perform and how complex XR apps modern web browsers can support on mobile devices. To fill the knowledge gap, in this paper, we conduct an empirical study of mobile XR in web browsers. We select seven most popular web-based XR frameworks and investigate their runtime performance, including 3D rendering, camera capturing, and real-world understanding. We find that current frameworks have the potential to further enhance their performance by increasing GPU utilization or improving computing parallelism. Besides, for 3D scenes with good rendering performance, developers can feel free to add camera capturing with little influence on performance to support augmented reality (AR) and mixed reality (MR) applications. Based on our findings, we draw several practical implications to provide better XR support in web browsers.|移动扩展现实(XR)近年来发展迅速。与基于应用程序的 XR 相比，Web 浏览器中的 XR 具有轻量级和跨平台的优势，为用户提供了无处不在的体验。因此，许多框架正在出现，以支持在 Web 浏览器中开发 XR。然而，人们对这些框架的性能以及现代网络浏览器在移动设备上支持的复杂 XR 应用知之甚少。为了填补这一知识空白，本文对 Web 浏览器中的移动 XR 进行了实证研究。我们选择了七个最流行的基于 Web 的 XR 框架，并研究了它们的运行时性能，包括3D 渲染、摄像头捕捉和对真实世界的理解。我们发现当前的框架有潜力通过提高 GPU 利用率或改善计算并行性来进一步提高其性能。此外，对于具有良好渲染性能的3D 场景，开发人员可以随意添加对性能影响不大的摄像头捕捉，以支持扩增实境(AR)和混合现实(MR)应用程序。基于我们的发现，我们提出了一些实际的含义，以提供更好的 XR 支持在 Web 浏览器。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Demystifying+Mobile+Extended+Reality+in+Web+Browsers:+How+Far+Can+We+Go?)|0|
|[Look Deep into the Microservice System Anomaly through Very Sparse Logs](https://doi.org/10.1145/3543507.3583338)|Xinrui Jiang, Yicheng Pan, Meng Ma, Ping Wang|Peking University, China|Intensive monitoring and anomaly diagnosis have become a knotty problem for modern microservice architecture due to the dynamics of service dependency. While most previous studies rely heavily on ample monitoring metrics, we raise a fundamental but always neglected issue - the diagnostic metric integrity problem. This paper solves the problem by proposing MicroCU – a novel approach to diagnose microservice systems using very sparse API logs. We design a structure named dynamic causal curves to portray time-varying service dependencies and a temporal dynamics discovery algorithm based on Granger causal intervals. Our algorithm generates a smoother space of causal curves and designs the concept of causal unimodalization to calibrate the causality infidelities brought by missing metrics. Finally, a path search algorithm on dynamic causality graphs is proposed to pinpoint the root cause. Experiments on commercial system cases show that MicroCU outperforms many state-of-the-art approaches and reflects the superiorities of causal unimodalization to raw metric imputation.|由于服务依赖的动态性，密集监视和异常诊断已经成为现代微服务体系结构的一个棘手问题。虽然以前的大多数研究严重依赖于大量的监测指标，但我们提出了一个基本但总是被忽视的问题——诊断指标完整性问题。本文通过提出 MicroCU 来解决这个问题——一种利用非常稀疏的 API 日志诊断微服务系统的新方法。设计了一种描述时变服务依赖的动态因果曲线结构和一种基于 Granger 因果区间的时间动态发现算法。我们的算法生成一个更光滑的因果曲线空间，并设计了因果单模化的概念来校准因果失真带来的度量缺失。最后，提出了一种基于动态因果图的路径搜索算法来确定根本原因。商业系统案例的实验表明，MicroCU 优于许多最新的方法，反映了因果单模态化对原始度量插补的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Look+Deep+into+the+Microservice+System+Anomaly+through+Very+Sparse+Logs)|0|
|[Analyzing the Communication Clusters in Datacenters✱](https://doi.org/10.1145/3543507.3583410)|KlausTycho Foerster, Thibault Marette, Stefan Neumann, Claudia Plant, Ylli Sadikaj, Stefan Schmid, Yllka Velaj|KTH Royal Institute of Technology, France; TU Dortmund, Germany; Faculty of Computer Science, University of Vienna, Austria; Faculty of Computer Science, University of Vienna, Austria and UniVie Doctoral School Computer Science, University of Vienna, Austria; KTH Royal Institute of Technology, Sweden; Faculty of Computer Science, University of Vienna, Austria and ds:Univie, University of Vienna, Austria; TU Berlin & University of Vienna, Germany|Datacenter networks have become a critical infrastructure of our digital society and over the last years, great efforts have been made to better understand the communication patterns inside datacenters. In particular, existing empirical studies showed that datacenter traffic typically features much temporal and spatial structure, and that at any given time, some communication pairs interact much more frequently than others. This paper generalizes this study to communication groups and analyzes how clustered the datacenter traffic is, and how stable these clusters are over time. To this end, we propose a methodology which revolves around a biclustering approach, allowing us to identify groups of racks and servers which communicate frequently over the network. In particular, we consider communication patterns occurring in three different Facebook datacenters: a Web cluster consisting of web servers serving web traffic, a Database cluster which mainly consists of MySQL servers, and a Hadoop cluster. Interestingly, we find that in all three clusters, small groups of racks and servers can produce a large fraction of the network traffic, and we can determine these groups even when considering short snapshots of network traffic. We also show empirically that these clusters are fairly stable across time. Our insights on the size and stability of communication clusters hence uncover an interesting potential for resource optimizations in datacenter infrastructures.|数据中心网络已经成为我们数字社会的重要基础设施，在过去的几年里，人们付出了巨大的努力来更好地理解数据中心内部的通信模式。特别是，现有的实证研究表明，数据中心流量通常具有很多时间和空间结构，并且在任何给定的时间，一些通信对比其他通信对更频繁地进行交互。本文将这项研究推广到通信组，并分析数据中心流量是如何聚集的，以及随着时间的推移这些聚集是如何稳定的。为此，我们提出了一种围绕双群集方法的方法，该方法允许我们识别经常通过网络进行通信的机架和服务器组。特别是，我们考虑发生在三个不同 Facebook 数据中心的通信模式: 一个由服务于网络流量的 Web 服务器组成的 Web 集群，一个主要由 MySQL 服务器组成的数据库集群，以及一个 Hadoop 集群。有趣的是，我们发现在所有三个集群中，小组的机架和服务器可以产生大部分的网络流量，我们甚至可以在考虑网络流量的短快照时确定这些组。我们还通过经验表明，这些集群在不同的时间段是相当稳定的。因此，我们对通信集群的规模和稳定性的见解揭示了数据中心基础设施中资源优化的有趣潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analyzing+the+Communication+Clusters+in+Datacenters✱)|0|
|[DDPC: Automated Data-Driven Power-Performance Controller Design on-the-fly for Latency-sensitive Web Services](https://doi.org/10.1145/3543507.3583437)|Mehmet Savasci, Ahmed AliEldin, Johan Eker, Anders Robertsson, Prashant J. Shenoy|Chalmers University of Technology, Sweden; Ericsson Research, Sweden and Lund University, Sweden; Lund University, Sweden; University of Massachusetts Amherst, USA|Traditional power reduction techniques such as DVFS or RAPL are challenging to use with web services because they significantly affect the services’ latency and throughput. Previous work suggested the use of controllers based on control theory or machine learning to reduce performance degradation under constrained power. However, generating these controllers is challenging as every web service applications running in a data center requires a power-performance model and a fine-tuned controller. In this paper, we present DDPC, a system for autonomic data-driven controller generation for power-latency management. DDPC automates the process of designing and deploying controllers for dynamic power allocation to manage the power-performance trade-offs for latency-sensitive web applications such as a social network. For each application, DDPC uses system identification techniques to learn an adaptive power-performance model that captures the application’s power-latency trade-offs which is then used to generate and deploy a Proportional-Integral (PI) power controller with gain-scheduling to dynamically manage the power allocation to the server running application using RAPL. We evaluate DDPC with two realistic latency-sensitive web applications under varying load scenarios. Our results show that DDPC is capable of autonomically generating and deploying controllers within a few minutes reducing the active power allocation of a web-server by more than 50% compared to state-of-the-art techniques while maintaining the latency well below the target of the application.|传统的功耗降低技术，如 DVFS 或 RAPL，对 Web 服务的使用具有挑战性，因为它们严重影响服务的延迟和吞吐量。以往的工作建议使用基于控制理论或机器学习的控制器来减少功率受限情况下的性能下降。然而，生成这些控制器是具有挑战性的，因为在数据中心中运行的每个 Web 服务应用程序都需要电源性能模型和经过微调的控制器。在本文中，我们提出了 DDPC，一个自主数据驱动的控制器生成系统的功率延迟管理。DDPC 自动化设计和部署动态功率分配控制器的过程，以管理对延迟敏感的 Web 应用程序(如社交网络)的功率性能权衡。对于每个应用程序，DDPC 使用系统辨识技术来学习一个自适应功率性能模型，该模型捕捉应用程序的功率延迟权衡，然后用于生成和部署具有增益调度的比例积分(PI)功率控制器，以使用 RAPL 动态管理服务器运行应用程序的功率分配。在不同的负载情况下，我们使用两个真实的对延迟敏感的 Web 应用程序来评估 DDPC。我们的研究结果表明，DDPC 能够在几分钟内自动生成和部署控制器，与最先进的技术相比，网络服务器的有源功率分配减少了50% 以上，同时保持延迟远低于应用程序的目标。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DDPC:+Automated+Data-Driven+Power-Performance+Controller+Design+on-the-fly+for+Latency-sensitive+Web+Services)|0|
|[Will Admins Cope? Decentralized Moderation in the Fediverse](https://doi.org/10.1145/3543507.3583487)|Ishaku Hassan Anaobi, Aravindh Raman, Ignacio Castro, Haris Bin Zia, Damilola Ibosiola, Gareth Tyson|Hong Kong University of Science and Technology, China; School of Electronic Engineering and Computer Science, Queen Mary University, United Kingdom; Telefonica, Spain|As an alternative to Twitter and other centralized social networks, the Fediverse is growing in popularity. The recent, and polemical, takeover of Twitter by Elon Musk has exacerbated this trend. The Fediverse includes a growing number of decentralized social networks, such as Pleroma or Mastodon, that share the same subscription protocol (ActivityPub). Each of these decentralized social networks is composed of independent instances that are run by different administrators. Users, however, can interact with other users across the Fediverse regardless of the instance they are signed up to. The growing user base of the Fediverse creates key challenges for the administrators, who may experience a growing burden. In this paper, we explore how large that overhead is, and whether there are solutions to alleviate the burden. We study the overhead of moderation on the administrators. We observe a diversity of administrator strategies, with evidence that administrators on larger instances struggle to find sufficient resources. We then propose a tool, WatchGen, to semi-automate the process.|作为 Twitter 和其他集中式社交网络的替代品，Fedifferent 越来越受欢迎。埃隆•马斯克(Elon Musk)最近对 Twitter 的收购加剧了这一趋势。Fedifferent 包括越来越多分散的社交网络，如 Pleroma 或 Mastodon，它们共享相同的订阅协议(ActivityPub)。这些分散的社交网络中的每一个都由不同的管理员运行的独立实例组成。然而，不管用户注册的实例是什么，他们都可以通过 Fedifferent 与其他用户进行交互。不断增长的 Fedifferent 用户基础给管理员带来了关键的挑战，他们可能会面临越来越大的负担。在本文中，我们将探讨这个开销有多大，以及是否存在减轻负担的解决方案。我们研究管理员的节制开销。我们观察到管理员策略的多样性，有证据表明大型实例中的管理员很难找到足够的资源。然后，我们提出一个工具 WatchGen 来半自动化这个过程。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Will+Admins+Cope?+Decentralized+Moderation+in+the+Fediverse)|0|
|[Are Mobile Advertisements in Compliance with App's Age Group?](https://doi.org/10.1145/3543507.3583534)|Yanjie Zhao, Tianming Liu, Haoyu Wang, Yepang Liu, John C. Grundy, Li Li|School of Software, Beihang University, China and Monash University, Australia; Monash University, Australia; Huazhong University of Science and Technology, China; School of Software, Beihang University, China; Southern University of Science and Technology, China|As smartphones and mobile apps permeate every aspect of people’s lives, children are accessing mobile devices at an increasingly younger age. The inescapable exposure of advertisements in mobile apps to children has grown alarmingly. Mobile advertisements are placed by advertisers and subsequently distributed by ad SDKs, under the rare control of app developers and app markets’ content ratings. Indeed, content that is objectionable and harmful to children’s mental health has been reported to appear in advertising, such as pornography. However, few studies have yet concentrated on automatically and comprehensively identifying such kid-unsuitable mobile advertising. In this paper, we first characterize the regulations for mobile ads relating to children. We then propose our novel automated dynamic analysis framework, named AdRambler, that attempts to collect ad content throughout the lifespan of mobile ads and identify their inappropriateness for child app users. Using AdRambler, we conduct a large-scale (25,000 mobile apps) empirical investigation and reveal the non-incidental presence of inappropriate ads in apps with child-included target audiences. We collected 11,270 ad views and identified 1,289 ad violations (from 775 apps) of child user regulations, with roughly half of the app promotions not in compliance with host apps’ content ratings. Our finding indicates that even certified ad SDKs could still propagate inappropriate advertisements. We further delve into the question of accountability for the presence of inappropriate advertising and provide concrete suggestions for all stakeholders to take action for the benefit of children.|随着智能手机和移动应用程序渗透到人们生活的各个方面，儿童使用移动设备的年龄越来越小。移动应用程序中的广告不可避免地暴露在儿童面前，这种现象已经增长到令人担忧的地步。移动广告由广告商投放，然后由广告软件开发工具包(ad SDK)发布，在应用程序开发者和应用程序市场内容评级的罕见控制之下。事实上，有报道称，在色情等广告中出现了对儿童心理健康有害的令人反感的内容。然而，很少有研究集中在自动和全面识别这种儿童不适合的移动广告。在本文中，我们首先描述了与儿童有关的移动广告的规定。然后，我们提出了我们的新型自动化动态分析框架，命名为 AdRambler，该框架试图收集整个移动广告生命周期中的广告内容，并确定其不适合儿童应用程序用户。使用 AdRambler，我们进行了一个大规模(25,000个移动应用程序)的实证调查，并揭示了在包括儿童在内的目标受众的应用程序中非偶然存在不适当的广告。我们收集了11,270个广告浏览量，发现了1,289个违反儿童用户法规的广告(来自775个应用程序) ，其中大约一半的应用程序促销活动不符合主机应用程序的内容评级。我们的发现表明，即使是经过认证的广告 SDK 仍然可以传播不适当的广告。我们进一步探讨了对不适当广告的问责问题，并为所有利益攸关方采取有利于儿童的行动提出了具体建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Are+Mobile+Advertisements+in+Compliance+with+App's+Age+Group?)|0|
|[EdgeMove: Pipelining Device-Edge Model Training for Mobile Intelligence](https://doi.org/10.1145/3543507.3583540)|Zeqian Dong, Qiang He, Feifei Chen, Hai Jin, Tao Gu, Yun Yang|Deakin University, Australia; Huazhong University of Science and Technology, China and Swinburne University of Technology, Australia; Macquarie University, Australia; Swinburne University of Technology, Australia; Huazhong University of Science and Technology, China|Training machine learning (ML) models on mobile and Web-of-Things (WoT) has been widely acknowledged and employed as a promising solution to privacy-preserving ML. However, these end-devices often suffer from constrained resources and fail to accommodate increasingly large ML models that crave great computation power. Offloading ML models partially to the cloud for training strikes a trade-off between privacy preservation and resource requirements. However, device-cloud training creates communication overheads that delay model training tremendously. This paper presents EdgeMove, the first device-edge training scheme that enables fast pipelined model training across edge devices and edge servers. It employs probing-based mechanisms to tackle the new challenges raised by device-edge training. Before training begins, it probes nearby edge servers’ training performance and bootstraps model training by constructing a training pipeline with an approximate model partitioning. During the training process, EdgeMove accommodates user mobility and system dynamics by probing nearby edge servers’ training performance adaptively and adapting the training pipeline proactively. Extensive experiments are conducted with two popular DNN models trained on four datasets for three ML tasks. The results demonstrate that EdgeMove achieves a 1.3 × -2.1 × speedup over the state-of-the-art scheme.|基于移动设备和物联网的训练机器学习(ML)模型已经被广泛认可，并被用作保护机器学习隐私的一种有前途的解决方案。然而，这些终端设备往往受到资源的限制，无法适应日益增长的大型机器学习模型，渴望巨大的计算能力。将机器学习模型部分卸载到云中进行培训，在保护隐私和资源需求之间达成了一个平衡。然而，设备云训练产生的通信开销极大地延迟了模型训练。本文介绍了 EdgeMove，这是第一个支持跨边缘设备和边缘服务器的快速流水线模型训练的设备边缘训练方案。它采用基于探测的机制来应对由设备边缘训练带来的新挑战。在训练开始前，通过构造一个具有近似模型划分的训练流水线，探讨了附近边缘服务器的训练性能，并引导模型训练。在训练过程中，EdgeMove 通过自适应地探测附近边缘服务器的训练性能和主动地调整训练流水线来适应用户移动性和系统动态性。广泛的实验进行了两个流行的 DNN 模型训练的四个数据集的三个机器学习任务。实验结果表明，EdgeMove 比最先进的方案提高了1.3 × -2.1 × 的速度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EdgeMove:+Pipelining+Device-Edge+Model+Training+for+Mobile+Intelligence)|0|
|[HTTP Steady Connections for Robust Web Acceleration](https://doi.org/10.1145/3543507.3583550)|Sunjae Kim, Wonjun Lee|Korea University, Republic of Korea|HTTP’s intrinsic request-and-response traffic pattern makes most web servers often idle, leaving a potential to accelerate page loads. We present the notion of HTTP steady connections, which fully utilizes the server’s available network bandwidth during a page load using the promising HTTP/3 server push, transforming the intermittent workload of loading a page into a more steady one. To construct a proper server push policy to achieve this, we separate the structure of a page, which is a relatively static factor, from the page load environments including client and network characteristics, which are generally dynamic and unknown to servers. We formulate a deadline-based sequencing problem using a page load model with dependency graphs and design a feedback-based reprioritization mechanism within HTTP server push to reactively match client progress robustly. Experiments with a prototype and a wide range of real-world pages show that HTTP steady connections significantly improve web page loads compared with state-of-the-art accelerators, even under packet losses and without any prior knowledge of network environments.|HTTP 内在的请求-响应流量模式使得大多数 Web 服务器经常处于空闲状态，从而有可能加速页面加载。我们提出了 HTTP 稳定连接的概念，它在页面加载期间充分利用服务器的可用网络带宽，使用有前途的 HTTP/3服务器推送，将加载页面的间歇性工作负载转化为更稳定的工作负载。为了构建合适的服务器推策略来实现这一点，我们将页面结构(相对静态的因素)与页面加载环境(包括客户端和网络特性)分离开来，后者通常是动态的，服务器不知道。我们利用一个带有依赖图的页面负载模型，提出了一个基于截止日期的排序问题，并在 HTTP 服务器推送中设计了一个基于反馈的重新排序机制，使得客户端进度能够被动地匹配。对一个原型和大量真实世界页面的实验表明，与最先进的加速器相比，HTTP 稳定连接可以显著提高网页负载，即使在丢包的情况下，也不需要任何关于网络环境的先验知识。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HTTP+Steady+Connections+for+Robust+Web+Acceleration)|0|
|[Dynamic Interventions for Networked Contagions](https://doi.org/10.1145/3543507.3583470)|Marios Papachristou, Siddhartha Banerjee, Jon M. Kleinberg|Cornell University, USA|We study the problem of designing dynamic intervention policies for minimizing networked defaults in financial networks. Formally, we consider a dynamic version of the celebrated Eisenberg-Noe model of financial network liabilities and use this to study the design of external intervention policies. Our controller has a fixed resource budget in each round and can use this to minimize the effect of demand/supply shocks in the network. We formulate the optimal intervention problem as a Markov Decision Process and show how we can leverage the problem structure to efficiently compute optimal intervention policies with continuous interventions and provide approximation algorithms for discrete interventions. Going beyond financial networks, we argue that our model captures dynamic network intervention in a much broader class of dynamic demand/supply settings with networked inter-dependencies. To demonstrate this, we apply our intervention algorithms to various application domains, including ridesharing, online transaction platforms, and financial networks with agent mobility. In each case, we study the relationship between node centrality and intervention strength, as well as the fairness properties of the optimal interventions.|研究了金融网络中最小化网络违约的动态干预策略设计问题。在形式上，我们考虑著名的艾森伯格-诺伊金融网络负债模型的一个动态版本，并用它来研究外部干预政策的设计。我们的控制器在每一轮都有一个固定的资源预算，并且可以使用它来最小化网络中需求/供应冲击的影响。我们将最优干预问题表述为一个马可夫决策过程，并展示了我们如何利用问题结构来有效地计算连续干预的最优干预政策，并为离散干预提供近似算法。超越金融网络，我们认为我们的模型捕捉动态网络干预在一个更广泛的类动态需求/供应设置与网络相互依赖。为了证明这一点，我们将我们的干预算法应用到各种应用领域，包括共乘、在线交易平台和具有代理移动性的金融网络。在每种情况下，我们研究了节点集中度与干预强度之间的关系，以及最优干预的公平性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Interventions+for+Networked+Contagions)|0|
|[Randomized Pricing with Deferred Acceptance for Revenue Maximization with Submodular Objectives](https://doi.org/10.1145/3543507.3583477)|He Huang, Kai Han, Shuang Cui, Jing Tang|School of Computer Science and Technology, Soochow University, China; The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology, China; School of Computer Science and Technology, University of Science and Technology of China, China|A lot of applications in web economics need to maximize the revenue under a budget for payments and also guarantee the truthfulness of users, so Budget-Feasible Mechanism (BFM) Design has aroused great interests during last decade. Most of the existing BFMs concentrate on maximizing a monotone submodular function subject to a knapsack constraint, which is insufficient for many applications with complex objectives or constraints. Observing this, the recent studies (e.g., [4, 5, 11]) have considered non-monotone submodular objectives or more complex constraints such as a k-system constraint. In this study, we follow this line of research and propose truthful BFMs with improved performance bounds for non-monotone submodular objectives with or without a k-system constraint. Our BFMs leverage the idea of providing random prices to users while deferring the decision on the final winning set, and are also based on a novel randomized algorithm for the canonical constrained submodular maximization problem achieving better performance bounds compared to the state-of-the-art. Finally, the effectiveness and efficiency of our approach are demonstrated by extensive experiments on several applications about social network marketing, crowdsourcing and personalized recommendation.|网络经济中的许多应用需要在支付预算下实现收益最大化，同时又要保证用户的真实性，因此预算可行机制设计在近十年来引起了人们的极大兴趣。现有的 BFM 大多集中于最大化受背包约束的单调子模函数，这对于许多具有复杂目标或约束的应用来说是不够的。考虑到这一点，最近的研究(例如，[4,5,11])已经考虑了非单调子模目标或更复杂的约束，如 k- 系统约束。在这项研究中，我们遵循这条研究路线，提出了真实的 BFM 与改进的性能界限为非单调子模目标有或没有 k 系统约束。我们的 bfMs 利用向用户提供随机价格的想法，同时推迟对最终获胜集的决定，并且基于一个新的随机化算法，用于规范约束的子模块最大化问题，与最先进的技术相比，获得更好的性能界限。最后，通过在社交网络营销、众包和个性化推荐等多个应用领域的广泛实验，验证了该方法的有效性和高效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Randomized+Pricing+with+Deferred+Acceptance+for+Revenue+Maximization+with+Submodular+Objectives)|0|
|[Fairness-aware Guaranteed Display Advertising Allocation under Traffic Cost Constraint](https://doi.org/10.1145/3543507.3583501)|Liang Dai, Zhonglin Zu, Hao Wu, Liang Wang, Bo Zheng|Alibaba Group, China|Real-time Bidding (RTB) and Guaranteed Display (GD) advertising are two primary ways to sell impressions for publishers in online display advertising. Although GD contract serves less efficiently compared to RTB ads, it helps advertisers reach numerous target audiences at a lower cost and allows publishers to increase overall advertising revenue. However, with billion-scale requests online per day, it’s a challenging problem for publishers to decide whether and which GD ad to display for each impression. In this paper, we propose an optimal allocation model for GD contracts considering optimizing three objectives: maximizing guaranteed delivery and impressions’ quality and minimizing the extra traffic cost of GD contracts to increase overall revenue. The traffic cost of GD contracts is defined as the potential expected revenue if the impression is allocated to RTB ads. Our model dynamically adjusts the weights for each GD contract between impressions’ quality and traffic cost based on real-time performance, which produces fairness-aware allocation results. A parallel training framework based on Parameter-Server (PS) architecture is utilized to efficiently and periodically update the model. Deriving from the allocation model, we also propose a simple and adaptive online bidding strategy for GD contracts, which can be updated quickly by feedback-based algorithms to achieve optimal impression allocation even in complex and dynamic environments. We demonstrate the effectiveness of our proposed method by using both offline evaluation and online A/B testing.|实时竞价(RTB)和保证显示(GD)广告是出版商在线显示广告销售印象的两种主要方式。尽管 GD 合同的服务效率低于 RTB 广告，但它帮助广告商以较低的成本达到众多目标受众，并使发布商能够增加整体广告收入。然而，由于每天都有数十亿的在线请求，对于出版商来说，决定是否展示每个印象以及展示哪个 GD 广告是一个具有挑战性的问题。在本文中，我们提出了一个 GD 合同的最优分配模型，考虑优化三个目标: 最大化的保证交付和印象的质量和最小化额外的流量成本的 GD 合同，以增加总收入。广东合同的流量成本被定义为潜在的预期收入，如果印象是分配给 RTB 广告。我们的模型基于实时性能动态调整每个 GD 契约在印象质量和流量成本之间的权重，从而产生公平感知的分配结果。利用基于参数服务器(PS)体系结构的并行训练框架对模型进行高效、周期性的更新。在分配模型的基础上，提出了一种简单、自适应的 GD 合同在线投标策略，该策略可以通过基于反馈的算法快速更新，即使在复杂动态环境下也能实现最优印象分配。通过离线评估和在线 A/B 测试，验证了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness-aware+Guaranteed+Display+Advertising+Allocation+under+Traffic+Cost+Constraint)|0|
|[Is your digital neighbor a reliable investment advisor?](https://doi.org/10.1145/3543507.3583502)|Daisuke Kawai, Alejandro Cuevas, Bryan R. Routledge, Kyle Soska, Ariel ZetlinJones, Nicolas Christin|Ramiel Capital, USA; Carnegie Mellon University, USA|The web and social media platforms have drastically changed how investors produce and consume financial advice. Historically, individual investors were often relying on newsletters and related prospectus backed by the reputation and track record of their issuers. Nowadays, financial advice is frequently offered online, by anonymous or pseudonymous parties with little at stake. As such, a natural question is to investigate whether these modern financial “influencers” operate in good faith, or whether they might be misleading their followers intentionally. To start answering this question, we obtained data from a very large cryptocurrency derivatives exchange, from which we derived individual trading positions. Some of the investors on that platform elect to link to their Twitter profiles. We were thus able to compare the positions publicly espoused on Twitter with those actually taken in the market. We discovered that 1) staunchly “bullish” investors on Twitter often took much more moderate, if not outright opposite, positions in their own trades when the market was down, 2) their followers tended to align their positions with bullish Twitter outlooks, and 3) moderate voices on Twitter (and their own followers) were on the other hand far more consistent with their actual investment strategies. In other words, while social media advice may attempt to foster a sense of camaraderie among people of like-minded beliefs, the reality is that this is merely an illusion, which may result in financial losses for people blindly following advice.|网络和社交媒体平台已经彻底改变了投资者提供和消费金融建议的方式。从历史上看，个人投资者往往依赖时事通讯和相关的招股说明书，并以发行人的声誉和业绩记录作为依据。如今，财务咨询经常是在网上提供的，由匿名或匿名的团体提供，几乎没有什么利害关系。因此，一个自然而然的问题是调查这些现代金融“影响者”是否诚信经营，或者他们是否故意误导他们的追随者。为了开始回答这个问题，我们从一个非常大的加密货币衍生品交易所获得数据，从中我们得到了个人交易头寸。该平台上的一些投资者选择链接到他们的 Twitter 档案。因此，我们可以比较 Twitter 上公开支持的立场和市场上实际采取的立场。我们发现，1) Twitter 上坚定的“看涨”投资者在股市下跌时，往往在自己的交易中持有更为温和(如果不是完全相反的话)的头寸; 2)他们的追随者倾向于将自己的头寸与看涨的 Twitter 前景联系起来; 3) Twitter 上的温和声音(以及他们自己的追随者)则与他们的实际投资策略更为一致。换句话说，虽然社交媒体的建议可能试图培养志同道合者之间的友情，但事实上这只是一种幻觉，可能导致盲目听从建议的人蒙受经济损失。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+your+digital+neighbor+a+reliable+investment+advisor?)|0|
|[Impartial Selection with Prior Information](https://doi.org/10.1145/3543507.3583553)|Ioannis Caragiannis, George Christodoulou, Nicos Protopapas|University of Patras, Greece; Aarhus University, Denmark; Aristotle University of Thessaloniki, Greece and Archimedes/RC Athena, Greece|We study the problem of {\em impartial selection}, a topic that lies at the intersection of computational social choice and mechanism design. The goal is to select the most popular individual among a set of community members. The input can be modeled as a directed graph, where each node represents an individual, and a directed edge indicates nomination or approval of a community member to another. An {\em impartial mechanism} is robust to potential selfish behavior of the individuals and provides appropriate incentives to voters to report their true preferences by ensuring that the chance of a node to become a winner does not depend on its outgoing edges. The goal is to design impartial mechanisms that select a node with an in-degree that is as close as possible to the highest in-degree. We measure the efficiency of such a mechanism by the difference of these in-degrees, known as its {\em additive} approximation. In particular, we study the extent to which prior information on voters' preferences could be useful in the design of efficient deterministic impartial selection mechanisms with good additive approximation guarantees. We consider three models of prior information, which we call the {\em opinion poll}, the {\em a prior popularity}, and the {\em uniform} model. We analyze the performance of a natural selection mechanism that we call {\em approval voting with default} (AVD) and show that it achieves a $O(\sqrt{n\ln{n}})$ additive guarantee for opinion poll and a $O(\ln^2n)$ for a priori popularity inputs, where $n$ is the number of individuals. We consider this polylogarithmic bound as our main technical contribution. We complement this last result by showing that our analysis is close to tight, showing an $\Omega(\ln{n})$ lower bound. This holds in the uniform model, which is the simplest among the three models.|我们研究了处于计算社会选择和机制设计交叉点的{ em 公平选择}问题。目标是在一组社区成员中选出最受欢迎的个体。输入可以建模为有向图，其中每个节点代表一个个体，有向边表示社区成员对另一个成员的提名或批准。一个{ em 公正机制}对个体潜在的自私行为具有鲁棒性，并且通过确保节点成为赢家的机会不依赖于其外向边缘，为选民报告他们的真实偏好提供适当的激励。我们的目标是设计公正的机制，选择一个程度尽可能接近程度最高的节点。我们通过这些程度的差异来衡量这种机制的效率，称为它的{ em 加性}近似。特别地，我们研究了在设计具有良好的加性近似保证的有效的确定性公正选择机制时，关于选民偏好的先验信息可以在多大程度上发挥作用。我们考虑先验信息的三种模型，我们称之为{ em 民意调查}、{ em 先验流行}和{ em 统一}模型。我们分析了一个自然选择机制的性能，我们称之为{ em 认可投票与默认}(AVD) ，并表明它实现了一个 $O (sqrt { n ln { n }}}) $附加保证的意见调查和 $O (ln ^ 2n) $的先验受欢迎的输入，其中 $n $是个人的数量。我们认为这个多对数界限是我们的主要技术贡献。我们通过显示我们的分析接近紧凑来补充最后的结果，显示 $Omega (ln { n }) $下界。这在统一模型中是适用的，这是三种模型中最简单的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Impartial+Selection+with+Prior+Information)|0|
|[Do Language Models Plagiarize?](https://doi.org/10.1145/3543507.3583199)|Jooyoung Lee, Thai Le, Jinghui Chen, Dongwon Lee||Past literature has illustrated that language models (LMs) often memorize parts of training instances and reproduce them in natural language generation (NLG) processes. However, it is unclear to what extent LMs "reuse" a training corpus. For instance, models can generate paraphrased sentences that are contextually similar to training samples. In this work, therefore, we study three types of plagiarism (i.e., verbatim, paraphrase, and idea) among GPT-2 generated texts, in comparison to its training data, and further analyze the plagiarism patterns of fine-tuned LMs with domain-specific corpora which are extensively used in practice. Our results suggest that (1) three types of plagiarism widely exist in LMs beyond memorization, (2) both size and decoding methods of LMs are strongly associated with the degrees of plagiarism they exhibit, and (3) fine-tuned LMs' plagiarism patterns vary based on their corpus similarity and homogeneity. Given that a majority of LMs' training data is scraped from the Web without informing content owners, their reiteration of words, phrases, and even core ideas from training sets into generated texts has ethical implications. Their patterns are likely to exacerbate as both the size of LMs and their training data increase, raising concerns about indiscriminately pursuing larger models with larger training corpora. Plagiarized content can also contain individuals' personal and sensitive information. These findings overall cast doubt on the practicality of current LMs in mission-critical writing tasks and urge more discussions around the observed phenomena. Data and source code are available at https://github.com/Brit7777/LM-plagiarism.|过去的文献表明，语言模型(LM)经常记忆训练实例的一部分，并在自然语言生成(NLG)过程中重现它们。然而，还不清楚 LM 在多大程度上“重用”了一个训练语料库。例如，模型可以生成与训练样本上下文相似的解释句。因此，在这项工作中，我们研究了三种类型的剽窃(即，逐字，释义，和想法)在 GPT-2生成的文本，比较其训练数据，并进一步分析微调的生物多样性与领域特定语料库的剽窃模式在实践中广泛使用。我们的研究结果表明: (1)三种类型的剽窃广泛存在于记忆以外的 LM 中，(2) LM 的大小和解码方法与它们表现出的剽窃程度密切相关，(3)微调 LM 的剽窃模式基于它们的语料相似性和同质性而变化。考虑到 LM 的大部分训练数据都是从网上刮下来的，而没有通知内容所有者，他们重复单词、短语，甚至是将训练集中的核心思想转换成生成的文本，都有伦理上的含义。随着 LM 的规模和培训数据的增加，它们的模式可能会加剧，这引起了人们对不加区分地追求更大的模型和更大的培训语料库的担忧。剽窃的内容也可以包含个人的个人和敏感信息。这些研究结果总体上对当前任务批判性写作任务的语言学习模式的实用性提出了质疑，并敦促围绕所观察到的现象进行更多的讨论。数据和源代码可在 https://github.com/brit7777/lm-plagiarism 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Do+Language+Models+Plagiarize?)|0|
|[Path-specific Causal Fair Prediction via Auxiliary Graph Structure Learning](https://doi.org/10.1145/3543507.3583280)|Liuyi Yao, Yaliang Li, Bolin Ding, Jingren Zhou, Jinduo Liu, Mengdi Huai, Jing Gao|Alibaba Group, China; Beijing University of Technology, China; Alibaba, China; Iowa State University, USA; Alibaba Group, USA; Purdue University, USA|With ubiquitous adoption of machine learning algorithms in web technologies, such as recommendation system and social network, algorithm fairness has become a trending topic, and it has a great impact on social welfare. Among different fairness definitions, path-specific causal fairness is a widely adopted one with great potentials, as it distinguishes the fair and unfair effects that the sensitive attributes exert on algorithm predictions. Existing methods based on path-specific causal fairness either require graph structure as the prior knowledge or have high complexity in the calculation of path-specific effect. To tackle these challenges, we propose a novel casual graph based fair prediction framework which integrates graph structure learning into fair prediction to ensure that unfair pathways are excluded in the causal graph. Furthermore, we generalize the proposed framework to the scenarios where sensitive attributes can be non-root nodes and affected by other variables, which is commonly observed in real-world applications, such as recommendation system, but hardly addressed by existing works. We provide theoretical analysis on the generalization bound for the proposed fair prediction method, and conduct a series of experiments on real-world datasets to demonstrate that the proposed framework can provide better prediction performance and algorithm fairness trade-off.|随着机器学习算法在推荐系统、社交网络等网络技术中的广泛应用，算法公平性已成为一个热门话题，并对社会福利产生重大影响。在不同的公平性定义中，路径特定的因果公平性是一种被广泛采用的有很大潜力的公平性定义，因为它区分了敏感属性对算法预测的公平性和不公平性影响。现有的基于路径特定因果公平性的计算方法，要么以图结构作为先验知识，要么计算路径特定效应的复杂度较高。为了应对这些挑战，我们提出了一种新的基于因果图的公平预测框架，该框架将图结构学习与公平预测相结合，以确保不公平路径被排除在因果图之外。此外，我们还将该框架推广到了敏感属性可能是非根节点且受其他变量影响的情况，这种情况在推荐系统等实际应用中经常出现，但现有的工作很难解决。我们对提出的公平预测方法的泛化界限进行了理论分析，并在实际数据集上进行了一系列实验，结果表明该框架能够提供更好的预测性能和算法的公平性折衷。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Path-specific+Causal+Fair+Prediction+via+Auxiliary+Graph+Structure+Learning)|0|
|[HateProof: Are Hateful Meme Detection Systems really Robust?](https://doi.org/10.1145/3543507.3583356)|Piush Aggarwal, Pranit Chawla, Mithun Das, Punyajoy Saha, Binny Mathew, Torsten Zesch, Animesh Mukherjee||Exploiting social media to spread hate has tremendously increased over the years. Lately, multi-modal hateful content such as memes has drawn relatively more traction than uni-modal content. Moreover, the availability of implicit content payloads makes them fairly challenging to be detected by existing hateful meme detection systems. In this paper, we present a use case study to analyze such systems' vulnerabilities against external adversarial attacks. We find that even very simple perturbations in uni-modal and multi-modal settings performed by humans with little knowledge about the model can make the existing detection models highly vulnerable. Empirically, we find a noticeable performance drop of as high as 10% in the macro-F1 score for certain attacks. As a remedy, we attempt to boost the model's robustness using contrastive learning as well as an adversarial training-based method - VILLA. Using an ensemble of the above two approaches, in two of our high resolution datasets, we are able to (re)gain back the performance to a large extent for certain attacks. We believe that ours is a first step toward addressing this crucial problem in an adversarial setting and would inspire more such investigations in the future.|多年来，利用社交媒体传播仇恨的行为大幅增加。近来，模因等多模态仇恨内容比单模态内容吸引了更多的关注。此外，隐式内容有效载荷的可用性使得它们很难被现有的可恶的文化基因检测系统检测到。在本文中，我们提出了一个用例研究来分析这样的系统的外部对手攻击的脆弱性。我们发现，即使是非常简单的扰动，在单模态和多模态的设置执行人类对模型的知识很少，可以使现有的检测模型高度脆弱。根据经验，我们发现在某些攻击中，宏 F1得分的显著性能下降高达10% 。作为补救措施，我们尝试使用对比学习和基于对抗训练的方法 VILLA 来提高模型的鲁棒性。使用上述两种方法的集合，在我们的两个高分辨率数据集中，我们能够(重新)在很大程度上恢复某些攻击的性能。我们认为，我们的调查是在对抗性背景下解决这一关键问题的第一步，并将激发今后进行更多此类调查。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HateProof:+Are+Hateful+Meme+Detection+Systems+really+Robust?)|0|
|[DualFair: Fair Representation Learning at Both Group and Individual Levels via Contrastive Self-supervision](https://doi.org/10.1145/3543507.3583480)|Sungwon Han, SeungEon Lee, Fangzhao Wu, Sundong Kim, Chuhan Wu, Xiting Wang, Xing Xie, Meeyoung Cha||Algorithmic fairness has become an important machine learning problem, especially for mission-critical Web applications. This work presents a self-supervised model, called DualFair, that can debias sensitive attributes like gender and race from learned representations. Unlike existing models that target a single type of fairness, our model jointly optimizes for two fairness criteria - group fairness and counterfactual fairness - and hence makes fairer predictions at both the group and individual levels. Our model uses contrastive loss to generate embeddings that are indistinguishable for each protected group, while forcing the embeddings of counterfactual pairs to be similar. It then uses a self-knowledge distillation method to maintain the quality of representation for the downstream tasks. Extensive analysis over multiple datasets confirms the model's validity and further shows the synergy of jointly addressing two fairness criteria, suggesting the model's potential value in fair intelligent Web applications.|算法公平性已经成为一个重要的机器学习问题，尤其是对于关键任务的 Web 应用程序。这项工作提出了一个自我监督模型，称为 DualFair，可以从学习表征中去除敏感属性，如性别和种族。与现有的针对单一类型公平的模型不同，我们的模型共同优化了两个公平标准——群体公平和反事实公平——因此在群体和个人层面上做出了更公平的预测。我们的模型使用对比损失来生成对每个受保护群体无法区分的嵌入，同时迫使反事实对的嵌入相似。然后使用自知识精馏方法来维护下游任务的表示质量。通过对多个数据集的广泛分析，证实了该模型的有效性，并进一步显示了联合处理两个公平标准的协同效应，表明了该模型在公平智能 Web 应用中的潜在价值。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DualFair:+Fair+Representation+Learning+at+Both+Group+and+Individual+Levels+via+Contrastive+Self-supervision)|0|
|[PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous Link Prediction](https://doi.org/10.1145/3543507.3583511)|Shichang Zhang, Jiani Zhang, Xiang Song, Soji Adeshina, Da Zheng, Christos Faloutsos, Yizhou Sun||Transparency and accountability have become major concerns for black-box machine learning (ML) models. Proper explanations for the model behavior increase model transparency and help researchers develop more accountable models. Graph neural networks (GNN) have recently shown superior performance in many graph ML problems than traditional methods, and explaining them has attracted increased interest. However, GNN explanation for link prediction (LP) is lacking in the literature. LP is an essential GNN task and corresponds to web applications like recommendation and sponsored search on web. Given existing GNN explanation methods only address node/graph-level tasks, we propose Path-based GNN Explanation for heterogeneous Link prediction (PaGE-Link) that generates explanations with connection interpretability, enjoys model scalability, and handles graph heterogeneity. Qualitatively, PaGE-Link can generate explanations as paths connecting a node pair, which naturally captures connections between the two nodes and easily transfer to human-interpretable explanations. Quantitatively, explanations generated by PaGE-Link improve AUC for recommendation on citation and user-item graphs by 9 - 35% and are chosen as better by 78.79% of responses in human evaluation.|透明度和可信度已经成为黑盒机器学习(ML)模型的主要关注点。对模型行为的合理解释增加了模型的透明度，有助于研究人员建立更负责任的模型。近年来，图神经网络(GNN)在许多图 ML 问题中表现出比传统方法更好的性能，并且对它们的解释引起了人们越来越多的兴趣。然而，GNN 对链路预测(LP)的解释在文献中是缺乏的。LP 是一个基本的 GNN 任务，对应于 Web 应用程序，如推荐和网上赞助商搜索。鉴于现有的 GNN 解释方法只能解决节点/图级任务，本文提出了基于路径的 GNN 解释异构链路预测(PaGE-Link)方法，该方法能够产生具有连接可解释性的解释，具有模型可扩展性，并能处理图的异构性。从定性上讲，PaGE-Link 可以将解释作为连接节点对的路径生成，它自然地捕获两个节点之间的连接，并且很容易转换为人类可解释的解释。从数量上看，由 PaGE-Link 产生的解释使 AUC 对引文和用户项图的推荐提高了9-35% ，并且在人类评估中有78.79% 的回答选择了更好的解释。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PaGE-Link:+Path-based+Graph+Neural+Network+Explanation+for+Heterogeneous+Link+Prediction)|0|
|[Fairness in model-sharing games](https://doi.org/10.1145/3543507.3583483)|Kate Donahue, Jon M. Kleinberg|Cornell University, USA|In many real-world situations, data is distributed across multiple self-interested agents. These agents can collaborate to build a machine learning model based on data from multiple agents, potentially reducing the error each experiences. However, sharing models in this way raises questions of fairness: to what extent can the error experienced by one agent be significantly lower than the error experienced by another agent in the same coalition? In this work, we consider two notions of fairness that each may be appropriate in different circumstances: egalitarian fairness (which aims to bound how dissimilar error rates can be) and proportional fairness (which aims to reward players for contributing more data). We similarly consider two common methods of model aggregation, one where a single model is created for all agents (uniform), and one where an individualized model is created for each agent. For egalitarian fairness, we obtain a tight multiplicative bound on how widely error rates can diverge between agents collaborating (which holds for both aggregation methods). For proportional fairness, we show that the individualized aggregation method always gives a small player error that is upper bounded by proportionality. For uniform aggregation, we show that this upper bound is guaranteed for any individually rational coalition (where no player wishes to leave to do local learning).|在许多实际情况中，数据分布在多个自利代理之间。这些代理可以协作建立一个基于多个代理数据的机器学习模型，从而有可能减少每次经验中的错误。然而，以这种方式共享模型提出了公平性的问题: 在多大程度上，一个代理所经历的错误会明显低于同一联盟中另一个代理所经历的错误？在这项工作中，我们考虑了两个公平的概念，每个概念在不同的情况下可能是适当的: 平等主义公平(其目的是约束如何不同的错误率可以)和比例公平(其目的是奖励玩家贡献更多的数据)。我们同样考虑两种常见的模型聚合方法，一种是为所有代理创建单个模型(统一的) ，另一种是为每个代理创建个性化模型。对于平等主义的公平性，我们得到了一个关于协作的代理之间的错误率差异有多大的紧密的乘法界限(这适用于两种聚合方法)。对于比例公平性，我们证明了个性化聚合方法总是给出一个小的参与者误差，这个误差是比例上界的。对于一致聚合，我们证明了这个上界对于任何单独的理性联盟(没有球员希望离开去做局部学习)是有保证的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness+in+model-sharing+games)|0|
|[Combining Worker Factors for Heterogeneous Crowd Task Assignment](https://doi.org/10.1145/3543507.3583190)|Senuri Wijenayake, Danula Hettiachchi, Jorge Gonçalves|RMIT University, Australia; The University of Melbourne, Australia; The University of Sydney, Australia|Optimising the assignment of tasks to workers is an effective approach to ensure high quality in crowdsourced data - particularly in heterogeneous micro tasks. However, previous attempts at heterogeneous micro task assignment based on worker characteristics are limited to using cognitive skills, despite literature emphasising that worker performance varies based on other parameters. This study is an initial step towards understanding whether and how multiple parameters such as cognitive skills, mood, personality, alertness, comprehension skill, and social and physical context of workers can be leveraged in tandem to improve worker performance estimations in heterogeneous micro tasks. Our predictive models indicate that these parameters have varying effects on worker performance in the five task types considered – sentiment analysis, classification, transcription, named entity recognition and bounding box. Moreover, we note 0.003 - 0.018 reduction in mean absolute error of predicted worker accuracy across all tasks, when task assignment is based on models that consider all parameters vs. models that only consider workers’ cognitive skills. Our findings pave the way for the use of holistic approaches in micro task assignment that effectively quantify worker context.|优化工人的任务分配是一个有效的方法，以确保高质量的众包数据-特别是在异构微型任务。然而，以往基于工人特征的异质微任务分配的尝试仅限于使用认知技能，尽管文献强调工人的表现因其他参数而异。这项研究是了解是否以及如何利用工人的认知技能、情绪、性格、警觉性、理解技能以及社会和身体环境等多个参数来提高异质微任务中工人的绩效评估的第一步。我们的预测模型表明，这些参数在五种任务类型-情绪分析、分类、转录、命名实体识别和边界框中对工人绩效有不同的影响。此外，我们注意到，当任务分配基于考虑所有参数的模型与仅考虑工人认知技能的模型相比时，所有任务中工人预测准确度的平均绝对误差减少了0.003 -0.018。我们的研究结果为在微观任务分配中使用整体方法有效量化工人情境铺平了道路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Combining+Worker+Factors+for+Heterogeneous+Crowd+Task+Assignment)|0|
|[Hidden Indicators of Collective Intelligence in Crowdfunding](https://doi.org/10.1145/3543507.3583414)|EmokeÁgnes Horvát, Henry Kudzanai Dambanemuya, Jayaram Uparna, Brian Uzzi|Indian Institute of Management Udaipur, India; Northwestern University, USA|Extensive literature argues that crowds possess essential collective intelligence benefits that allow superior decision-making by untrained individuals working in low-information environments. Classic wisdom of crowds theory is based on evidence gathered from studying large groups of diverse and independent decision-makers. Yet, most human decisions are reached in online settings of interconnected like-minded people that challenge these criteria. This observation raises a key question: Are there surprising expressions of collective intelligence online? Here, we explore whether crowds furnish collective intelligence benefits in crowdfunding systems. Crowdfunding has grown and diversified quickly over the past decade, expanding from funding aspirant creative works and supplying pro-social donations to enabling large citizen-funded urban projects and providing commercial interest-based unsecured loans. Using nearly 10 million loan contributions from a market-dominant lending platform, we find evidence for collective intelligence indicators in crowdfunding. Our results, which are based on a two-stage Heckman selection model, indicate that opinion diversity and the speed at which funds are contributed predict who gets funded and who repays, even after accounting for traditional measures of creditworthiness. Moreover, crowds work consistently well in correctly assessing the outcome of high-risk projects. Finally, diversity and speed serve as early warning signals when inferring fundraising based solely on the initial part of the campaign. Our findings broaden the field of crowd-aware system design and inform discussions about the augmentation of traditional financing systems with tech innovations.|大量文献认为，群体拥有基本的集体智力优势，使未经训练的个人能够在低信息环境中作出优越的决策。群体理论的经典智慧是基于对大量多样化和独立决策者的研究收集到的证据。然而，大多数人类的决定都是在网络环境中做出的，这些网络环境中的人们互相联系，志趣相投，对这些标准提出了挑战。这一观察提出了一个关键问题: 网上是否存在令人惊讶的集体智慧表达？在这里，我们探讨群体是否提供集体智力的好处在众筹系统。过去10年，众筹迅速发展和多样化，从资助有抱负的创意作品和提供亲社会捐助扩大到支持大型公民资助的城市项目和提供基于商业利息的无担保贷款。利用市场主导的贷款平台提供的近1000万笔贷款，我们发现了众筹中集体智慧指标的证据。我们的研究结果基于两阶段 Heckman 选择模型，表明即使在考虑了传统的信用度量标准之后，意见多样性和基金投入的速度也可以预测谁获得了资金，谁偿还了贷款。此外，群体在正确评估高风险项目的结果方面始终表现良好。最后，多样性和速度作为预警信号时，推断筹款完全基于竞选的最初部分。我们的研究结果拓宽了群体感知系统设计的领域，并为关于利用技术创新增强传统融资系统的讨论提供了信息。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hidden+Indicators+of+Collective+Intelligence+in+Crowdfunding)|0|
|[Multiview Representation Learning from Crowdsourced Triplet Comparisons](https://doi.org/10.1145/3543507.3583431)|Xiaotian Lu, Jiyi Li, Koh Takeuchi, Hisashi Kashima|Kyoto University, Japan; University of Yamanashi, Japan|Crowdsourcing has been used to collect data at scale in numerous fields. Triplet similarity comparison is a type of crowdsourcing task, in which crowd workers are asked the question ``among three given objects, which two are more similar?'', which is relatively easy for humans to answer. However, the comparison can be sometimes based on multiple views, i.e., different independent attributes such as color and shape. Each view may lead to different results for the same three objects. Although an algorithm was proposed in prior work to produce multiview embeddings, it involves at least two problems: (1) the existing algorithm cannot independently predict multiview embeddings for a new sample, and (2) different people may prefer different views. In this study, we propose an end-to-end inductive deep learning framework to solve the multiview representation learning problem. The results show that our proposed method can obtain multiview embeddings of any object, in which each view corresponds to an independent attribute of the object. We collected two datasets from a crowdsourcing platform to experimentally investigate the performance of our proposed approach compared to conventional baseline methods.|众包已经被用来在许多领域大规模收集数据。三重相似性比较是一种众包任务，在这种任务中，众包工作者被问到“在给定的三个对象中，哪两个更相似?”这对人类来说相对容易回答。然而，比较有时可以基于多个视图，即不同的独立属性，如颜色和形状。对于相同的三个对象，每个视图可能导致不同的结果。虽然在以前的工作中提出了一种产生多视图嵌入的算法，但它至少涉及到两个问题: (1)现有的算法不能独立地预测新样本的多视图嵌入，(2)不同的人可能偏好不同的视图。在本研究中，我们提出一个端对端的归纳式深度学习框架来解决多视点表示学习问题。结果表明，该方法可以获得任意对象的多视图嵌入，其中每个视图对应于对象的一个独立属性。我们从一个众包平台收集了两个数据集，以实验研究我们提出的方法与传统的基线方法相比的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multiview+Representation+Learning+from+Crowdsourced+Triplet+Comparisons)|0|
|[Sedition Hunters: A Quantitative Study of the Crowdsourced Investigation into the 2021 U.S. Capitol Attack](https://doi.org/10.1145/3543507.3583514)|Tianjiao Yu, Sukrit Venkatagiri, Ismini Lourentzou, Kurt Luther|Virginia Tech, USA; University of Washington, USA|Social media platforms have enabled extremists to organize violent events, such as the 2021 U.S. Capitol Attack. Simultaneously, these platforms enable professional investigators and amateur sleuths to collaboratively collect and identify imagery of suspects with the goal of holding them accountable for their actions. Through a case study of Sedition Hunters, a Twitter community whose goal is to identify individuals who participated in the 2021 U.S. Capitol Attack, we explore what are the main topics or targets of the community, who participates in the community, and how. Using topic modeling, we find that information sharing is the main focus of the community. We also note an increase in awareness of privacy concerns. Furthermore, using social network analysis, we show how some participants played important roles in the community. Finally, we discuss implications for the content and structure of online crowdsourced investigations.|社交媒体平台使极端分子能够组织暴力事件，例如2021年的美国国会大厦袭击。同时，这些平台使专业调查人员和业余侦探能够合作收集和识别嫌疑人的图像，目的是让他们对自己的行为负责。通过一个关于煽动者的案例研究，这是一个 Twitter 社区，其目标是识别参与2021年美国国会大厦袭击的个人，我们探讨了社区的主要话题或目标，谁参与了社区，以及如何参与。通过主题建模，我们发现信息共享是社区的主要关注点。我们还注意到，人们对隐私问题的关注有所增加。此外，利用社会网络分析，我们显示了一些参与者如何在社区中发挥重要作用。最后，我们讨论在线众包调查的内容和结构的含义。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sedition+Hunters:+A+Quantitative+Study+of+the+Crowdsourced+Investigation+into+the+2021+U.S.+Capitol+Attack)|0|
|[Human-in-the-loop Regular Expression Extraction for Single Column Format Inconsistency](https://doi.org/10.1145/3543507.3583515)|Shaochen Yu, Lei Han, Marta Indulska, Shazia W. Sadiq, Gianluca Demartini|The University of Queensland, Australia|Format inconsistency is one of the most frequently appearing data quality issues encountered during data cleaning. Existing automated approaches commonly lack applicability and generalisability, while approaches with human inputs typically require specialized skills such as writing regular expressions. This paper proposes a novel hybrid human-machine system, namely “Data-Scanner-4C”, which leverages crowdsourcing to address syntactic format inconsistencies in a single column effectively. We first ask crowd workers to create examples from single-column data through “data selection” and “result validation” tasks. Then, we propose and use a novel rule-based learning algorithm to infer the regular expressions that propagate formats from created examples to the entire column. Our system integrates crowdsourcing and algorithmic format extraction techniques in a single workflow. Having human experts write regular expressions is no longer required, thereby reducing both the time as well as the opportunity for error. We conducted experiments through both synthetic and real-world datasets, and our results show how the proposed approach is applicable and effective across data types and formats.|格式不一致是数据清理过程中遇到的最常见的数据质量问题之一。现有的自动化方法通常缺乏适用性和普遍性，而具有人工输入的方法通常需要专门技能，如编写正则表达式。本文提出了一种新型的混合式人机系统“ Data-Scanner-4C”，该系统利用众包技术有效地解决单一列中的句法格式不一致问题。我们首先要求人群工作者通过“数据选择”和“结果验证”任务从单列数据创建示例。然后，我们提出并使用一个新的基于规则的学习算法来推断正则表达式，从创建的例子传播格式到整个列。我们的系统集成了众包和算法格式提取技术在一个单一的工作流程。不再需要人工专家编写正则表达式，因此既减少了时间，也减少了出错的机会。我们通过合成数据集和真实数据集进行了实验，结果显示了所提出的方法在跨数据类型和格式方面是如何适用和有效的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Human-in-the-loop+Regular+Expression+Extraction+for+Single+Column+Format+Inconsistency)|0|
|[Identifying Creative Harmful Memes via Prompt based Approach](https://doi.org/10.1145/3543507.3587427)|Junhui Ji, Wei Ren, Usman Naseem|School of Computer Science, University of Sydney, Australia|The creative nature of memes has made it possible for harmful content to spread quickly and widely on the internet. Harmful memes can range from spreading hate speech promoting violence, and causing emotional distress to individuals or communities. These memes are often designed to be misleading, manipulative, and controversial, making it challenging to detect and remove them from online platforms. Previous studies focused on how to fuse visual and language modalities to capture contextual information. However, meme analysis still severely suffers from data deficiency, resulting in insufficient learning of fusion modules. Further, using conventional pretrained encoders for text and images exhibits a greater semantic gap in feature spaces and leads to low performance. To address these gaps, this paper reformulates a harmful meme analysis as an auto-filling and presents a prompt-based approach to identify harmful memes. Specifically, we first transform multimodal data to a single (i.e., textual) modality by generating the captions and attributes of the visual data and then prepend the textual data in the prompt-based pre-trained language model. Experimental results on two benchmark harmful memes datasets demonstrate that our method outperformed state-of-the-art methods. We conclude with the transferability and robustness of our approach to identify creative harmful memes.|文化基因的创造性使得有害内容在互联网上迅速而广泛地传播成为可能。有害的模因包括散布煽动暴力的仇恨言论，以及对个人或社区造成情绪困扰。这些文化基因往往具有误导性、操纵性和争议性，使得从在线平台上检测和移除它们变得具有挑战性。以往的研究集中在如何融合视觉和语言模式，以捕捉上下文信息。然而，模因分析仍然存在严重的数据缺乏问题，导致融合模块学习不足。此外，使用传统的文本和图像预训练编码器表现出更大的特征空间语义差距，并导致低性能。为了解决这些差距，本文将有害模因分析重新表述为自动填充，并提出了一种基于提示的方法来识别有害模因。具体来说，我们首先通过生成可视化数据的标题和属性，将多模态数据转换为单一(即文本)模态，然后在基于提示的预训练语言模型中预置文本数据。在两个基准有害文化基因数据集上的实验结果表明，我们的方法优于最先进的方法。最后，我们总结了我们识别创造性有害模因的方法的可转移性和稳健性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Identifying+Creative+Harmful+Memes+via+Prompt+based+Approach)|0|
|[SA-Fusion: Multimodal Fusion Approach for Web-based Human-Computer Interaction in the Wild](https://doi.org/10.1145/3543507.3587429)|Xingyu Liu, Pengfei Ren, Yuchen Chen, Cong Liu, Jing Wang, Haifeng Sun, Qi Qi, Jingyu Wang|China Mobile Research Institute, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, China|Web-based AR technology has broadened human-computer interaction scenes from traditional mechanical devices and flat screens to the real world, resulting in unconstrained environmental challenges such as complex backgrounds, extreme illumination, depth range differences, and hand-object interaction. The previous hand detection and 3D hand pose estimation methods are usually based on single modality such as RGB or depth data, which are not available in some scenarios in unconstrained environments due to the differences between the two modalities. To address this problem, we propose a multimodal fusion approach, named Scene-Adapt Fusion (SA-Fusion), which can fully utilize the complementarity of RGB and depth modalities in web-based HCI tasks. SA-Fusion can be applied in existing hand detection and 3D hand pose estimation frameworks to boost their performance, and can be further integrated into the prototyping AR system to construct a web-based interactive AR application for unconstrained environments. To evaluate the proposed multimodal fusion method, we conduct two user studies on CUG Hand and DexYCB dataset, to demonstrate its effectiveness in terms of accurately detecting hand and estimating 3D hand pose in unconstrained environments and hand-object interaction.|基于网络的增强现实技术已经将人机交互场景从传统的机械设备和平板屏幕拓展到了现实世界，从而带来了无限制的环境挑战，例如复杂的背景、极端的照明、深度范围的差异以及手对象的交互。以往的人手检测和三维人手姿态估计方法通常是基于单一的模式，如 RGB 或深度数据，这是不可用的在一些无约束的环境中的情况下，由于两种模式之间的差异。为了解决这一问题，我们提出了一种多模态融合方法——场景适应融合(Scene-Adapt Fusion，SA-Fusion) ，该方法能够充分利用 RGB 和深度模式在基于 Web 的人机交互任务中的互补性。SA-Fusion 可以应用于现有的手部检测和三维手部姿态估计框架，以提高其性能，并可以进一步集成到原型 AR 系统中，构建基于 Web 的无约束交互式 AR 应用程序。为了评估所提出的多模态融合方法，我们在 CUG Hand 和 DexYCB 数据集上进行了两个用户研究，以验证该方法在无约束环境和手-物交互中准确检测手和估计三维手姿态方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SA-Fusion:+Multimodal+Fusion+Approach+for+Web-based+Human-Computer+Interaction+in+the+Wild)|0|
|[The Harmonic Memory: a Knowledge Graph of harmonic patterns as a trustworthy framework for computational creativity](https://doi.org/10.1145/3543507.3587428)|Jacopo de Berardinis, Albert MeroñoPeñuela, Andrea Poltronieri, Valentina Presutti|Department of Informatics, King's College London, United Kingdom; Department of Modern Languages, Literatures, and Cultures, University of Bologna, Italy; Deapartment of Computer Science and Engineering, University of Bologna, Italy|Computationally creative systems for music have recently achieved impressive results, fuelled by progress in generative machine learning. However, black-box approaches have raised fundamental concerns for ethics, accountability, explainability, and musical plausibility. To enable trustworthy machine creativity, we introduce the Harmonic Memory, a Knowledge Graph (KG) of harmonic patterns extracted from a large and heterogeneous musical corpus. By leveraging a cognitive model of tonal harmony, chord progressions are segmented into meaningful structures, and patterns emerge from their comparison via harmonic similarity. Akin to a music memory, the KG holds temporal connections between consecutive patterns, as well as salient similarity relationships. After demonstrating the validity of our choices, we provide examples of how this design enables novel pathways for combinational creativity. The memory provides a fully accountable and explainable framework to inspire and support creative professionals – allowing for the discovery of progressions consistent with given criteria, the recomposition of harmonic sections, but also the co-creation of new progressions.|计算机创造性的音乐系统最近取得了令人印象深刻的成果，推动了生成机器学习的进步。然而，黑盒方法已经引起了对道德、责任、可解释性和音乐合理性的基本关注。为了实现可信赖的机器创造性，我们引入了谐波记忆，它是一个知识图(KG) ，从一个大型的、异构的音乐语料库中提取出谐波模式。通过利用声调和谐的认知模型，和弦级数被分割成有意义的结构，并且通过和声相似性从它们的比较中产生模式。类似于音乐记忆，KG 持有连续模式之间的时间联系，以及显著的相似性关系。在证明了我们的选择的有效性之后，我们提供了这种设计如何为组合创造力创造新途径的例子。该记忆提供了一个完全负责和可解释的框架，以激励和支持创造性的专业人士-允许发现进展符合给定的标准，重组谐波部分，但也共同创造新的进展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Harmonic+Memory:+a+Knowledge+Graph+of+harmonic+patterns+as+a+trustworthy+framework+for+computational+creativity)|0|
|[A Prompt Log Analysis of Text-to-Image Generation Systems](https://doi.org/10.1145/3543507.3587430)|Yutong Xie, Zhaoying Pan, Jinge Ma, Luo Jie, Qiaozhu Mei|Niantic Inc., USA; School of Information, University of Michigan, USA; Electrical Engineering and Computer Science Department, University of Michigan, USA|Recent developments in large language models (LLM) and generative AI have unleashed the astonishing capabilities of text-to-image generation systems to synthesize high-quality images that are faithful to a given reference text, known as a "prompt". These systems have immediately received lots of attention from researchers, creators, and common users. Despite the plenty of efforts to improve the generative models, there is limited work on understanding the information needs of the users of these systems at scale. We conduct the first comprehensive analysis of large-scale prompt logs collected from multiple text-to-image generation systems. Our work is analogous to analyzing the query logs of Web search engines, a line of work that has made critical contributions to the glory of the Web search industry and research. Compared with Web search queries, text-to-image prompts are significantly longer, often organized into special structures that consist of the subject, form, and intent of the generation tasks and present unique categories of information needs. Users make more edits within creation sessions, which present remarkable exploratory patterns. There is also a considerable gap between the user-input prompts and the captions of the images included in the open training data of the generative models. Our findings provide concrete implications on how to improve text-to-image generation systems for creation purposes.|大型语言模型(LLM)和生成式人工智能的最新发展，已经释放了文本到图像生成系统的惊人能力，可以合成高质量的图像，这些图像忠实于给定的参考文本，即所谓的“提示”。这些系统立即受到了研究人员、创建者和普通用户的广泛关注。尽管在改进生成模型方面做了大量的努力，但在理解这些系统用户的大规模信息需求方面的工作有限。我们对从多个文本到图像生成系统收集的大规模提示日志进行了首次全面分析。我们的工作类似于分析 Web 搜索引擎的查询日志，这一系列工作为 Web 搜索行业和研究的辉煌做出了重要贡献。与 Web 搜索查询相比，文本到图像的提示要长得多，通常组织成特殊的结构，包括生成任务的主题、形式和意图，并呈现独特的信息需求类别。用户在创建会话中进行更多的编辑，这些编辑呈现出非凡的探索模式。生成模型的开放训练数据中包含的用户输入提示和图像标题之间也存在相当大的差距。我们的发现为如何改进文本到图像的生成系统以达到创建目的提供了具体的启示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Prompt+Log+Analysis+of+Text-to-Image+Generation+Systems)|0|
|[CAM: A Large Language Model-based Creative Analogy Mining Framework](https://doi.org/10.1145/3543507.3587431)|Bhavya, Jinjun Xiong, Chengxiang Zhai|University at Buffalo, USA; University of Illinois at Urbana-Champaign, USA|Analogies inspire creative solutions to problems, and facilitate the creative expression of ideas and the explanation of complex concepts. They have widespread applications in scientific innovation, creative writing, and education. The ability to discover creative analogies that are not explicitly mentioned but can be inferred from the web is highly desirable to power all such applications dynamically and augment human creativity. Recently, Large Pre-trained Language Models (PLMs), trained on massive Web data, have shown great promise in generating mostly known analogies that are explicitly mentioned on the Web. However, it is unclear how they could be leveraged for mining creative analogies not explicitly mentioned on the Web. We address this challenge and propose Creative Analogy Mining (CAM), a novel framework for mining creative analogies, which consists of the following three main steps: 1) Generate analogies using PLMs with effectively designed prompts, 2) Evaluate their quality using scoring functions, and 3) Refine the low-quality analogies by another round of prompt-based generation. We propose both unsupervised and supervised instantiations of the framework so that it can be used even without any annotated data. Based on human evaluation using Amazon Mechanical Turk, we find that our unsupervised framework can mine 13.7% highly-creative and 56.37% somewhat-creative analogies. Moreover, our supervised scores are generally better than the unsupervised ones and correlate moderately with human evaluators, indicating that they would be even more effective at mining creative analogies. These findings also shed light on the creativity of PLMs 1.|类比可以激发对问题的创造性解决方案，促进思想的创造性表达和复杂概念的解释。它们在科学创新、创造性写作和教育中有着广泛的应用。发现创造性类比的能力，没有明确提到，但可以从网络推断是非常理想的动力所有这些应用程序，并增强人类的创造力。最近，大型预训练语言模型(PLM) ，在海量的网络数据的训练，已经显示出巨大的前景，生成大多数已知的类比，明确提到了网络上。然而，目前还不清楚如何利用它们来挖掘网上没有明确提到的创造性类比。我们解决了这个问题，并提出了创造性类比挖掘(CAM) ，一个挖掘创造性类比的新框架，它由以下三个主要步骤组成: 1)使用 PLM 生成具有有效设计提示的类比，2)使用评分函数评估它们的质量，3)通过另一轮基于提示的生成精炼低质量的类比。我们提出了框架的无监督实例化和监督实例化，这样即使没有任何注释数据也可以使用框架。基于亚马逊土耳其机器人的人类评估，我们发现我们的无监督框架可以挖掘13.7% 的高创造性和56.37% 的有创造性的类比。此外，我们的监督分数一般优于无监督分数，并与人类评估者适度相关，表明他们将更有效地挖掘创造性类比。这些发现也揭示了 PLMs 1的创造性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CAM:+A+Large+Language+Model-based+Creative+Analogy+Mining+Framework)|0|
|[Tangible Web: An Interactive Immersion Virtual Reality Creativity System that Travels Across Reality](https://doi.org/10.1145/3543507.3587432)|Simin Yang, Ze Gao, Reza Hadi Mogavi, Pan Hui, Tristan Braud||We are all connected." These days, one hears constant reference to the notion that all human beings are in some fashion related and that what one person does inevitably affects other people around them. The idea has permeated pop culture in recent years, with novelists even making human ties tangible or visible in some way. In Stranger in a Strange Land, a Martian teaches humans to grok, which at first means only to understand one another, but which takes on richer dimensions as the book progresses. Carlos Castaneda made human ties visible to spiritual questers in his Journey to Ixtlan series. Orson Scott Card (Ender's Game, The Lost Boys) more recently gave human ties a basis in physics and used them for interstellar transport in one of his science fiction series. There is another sense in which "we are all connected" has come true in recent years. Since the invention of the telegraph in the 1840s, instantaneous communication has been possible and wires connecting people to one another have spread like kudzu across the face of the planet. The telephone network and, more recently, the Internet (the convergence of telecom and computing) are further tangible, visible expressions of the ties between people. It is an increasingly networked world. The observation is now commonplace that global computer networking has enabled the state-of-the-art in many fields to advance more quickly. Formerly, researchers were isolated and had to wait years for the work of others in their field to be available in published form. Now, with the Internet, researchers can communicate their results instantly to others whose minds are similarly engaged. Collaborative effort has to some extent displaced solitary pursuit. In this connection, the Internet has been called the World Brain. But humans are connected not only through their intellects but also through their emotional ties. Completely overlooked has been the amazing potential of the Internet to facilitate emotional ties among people no matter where they happen to be physically located and, thus, to draw the human family closer together. Internet technologies can be deployed to favorably condition empathic response in those who have offended against community norms out of a lack of appreciation for the effects that their criminal behavior has on other people. Technology is widely perceived as soulless and even as anti-human by some. But technology is only a tool and can be placed in the service of humane values. There is no reason the Internet cannot become the World Heart as well as the World Brain. Thinking in the field of criminology has cycled around broad themes of punishment, deterrence and rehabilitation for centuries. The recent trend toward victims' rights is really just the latest variation on a very old theme. The primacy of the victim hearkens back to Saxon, England, where victims and offenders handled restitution privately among themselves. Called "restorative justice" today, the concept has spawned, among other things, Victim-Offender Rehabilitation Programs which bring offenders face-to-face with their victims, the end product being written restitution agreements. Many offenders find emotional release in being understood ("I wanted to let them know I am not a bad person. I just made a mistake."). Forgiveness, understanding, redemption, empathy, atonement, the opportunity to "make things right" - the emotional intensity of these programs for offenders demonstrates that emotions are in play in offender psychology. This raises the possibility that emotions can be used to restructure thinking and reduce criminal propensities. Some years back, a TV newsmagazine featured a woman who went around to prisons talking about what a crime did to her and her family. She had the prisoners in tears as they came to realize the extent of the harm they themselves had caused their own victims. Empathy is now taught in some prisons and grade schools. One school found that students eventually internalize empathy and learn to rein in their own behavior. In California prisons, victim affinity groups explore the effects of a participant's crime on the victim, the victim's family and the offender's own family. In this way, empathy is learned and self-control is reinforced. One must make only modest claims for the benefits of teaching empathy to offenders. No single technique will reach every offender. Some will be deterred by punishment, and others will achieve self-control through moral instruction or the removal of cognitive distortions like exaggerated needs for immediate gratification. Nevertheless, some offenders will arrive at correct behavior through their emotional intelligence and, therefore, there is value in exploring new ways to encourage greater empathic response among them. Teaching offenders empathy might not change the world, but it will help. The Internet is a powerful medium. Its power lies in the ways it can allow people to interact. The Internet is an interactive medium, unlike today's TV or newspapers. It is the interactive nature of the Internet that opens new possibilities for teaching empathy to offenders. Reading a book is usually a solitary experience. Writing interactive fiction in real-time with other online users or participating in other online forums shifts the paradigm from solitary activity to collaboration. Successful collaboration requires consideration of others and their feelings, and thus may greater empathic response be born. Recreational materials like science fiction paperbacks and music cassettes are the most popular items in prison collections. However, many offenders are also interested in improving their lives. Many seek transformational experiences, creative writing outlets, ways to remain connected to the larger world around them and new skills like computer literacy to ease their transition back into community life. Many have trouble reading. Information professionals have a golden opportunity to address all these needs with current and future networking technologies. Prisoners receive visitors and get mail from the outside. Networking technologies can provide additional ties to the outside world and create a sense of community for those prisoners who would not dream of speaking up at a lecture or being brought face-to-face with their victims. Networking can also let prison librarians leverage their resources. Whatever is created in one location can be networked to other facilities via the Internet. The types of forums and multi-user domains available today and the virtual worlds of tomorrow can be disseminated instantly to other prisons around the world. The Present - Jurisdictions vary in the degree to which they currently use networking technologies in prison libraries. In Maryland, for example, the state is working to provide e-mail and the state information system ('Sailor') to its prison librarians. Because security is viewed as a manageable problem, some degree of Internet access for prisoners is envisioned for the future. In stark contrast, authorities in the federal system believe that sophisticated inmates will use the Internet to run criminal enterprises from prison and therefore have declared, once and for all, "no modems for prisoners." However, the availability of secure servers and firewall technology puts the federal authorities on the wrong side of history. The federal system may hold out for a time but will eventually follow Maryland's lead. The networking paradigm is unstoppable. A number of networking technologies and applications that could be used in prison libraries to teach empathy or serve other valid purposes already exist: • Interactive Forums - Moderated forums and online conferences can be used to bring victim presentations and victim affinity groups to broader prison audiences. Victims could tell their stories and offenders could discuss how becoming aware of the damage done to another changes their perceptions. • Interactive Fiction - Several multi-user domains (MUDs) have arisen in which hundreds of people participate in writing a story. Many prisoners have a strong need to write, and this would afford them the opportunity to do so in a collaborative instead of solitary fashion. There are classics of prison literature and someday there will be new classics of interactive prison literature. • Web Radio - Inmates could work together to produce shows on any number of topics - substance abuse, for example. Entire talk shows (complete with listeners calling in) could be offered. • Information Needs - Some argue that inmates have a "right to read" and should have access to the same materials available to other people. If so, then Internet access is the next logical step. A wealth of good, solid information is available on the Internet and can be brought to the desktop with the click of a mouse. Not only are books available in electronic form over the Internet, some are even available as audio files, perfect for a population with low education levels and literacy needs. Many people now research their own medical problems on the Internet. • Education - Many inmates take college-level courses and pursue degrees while in prison. Entire distance learning courses are delivered over the Internet and could be made available to inmates. • Recreation and Entertainment - There are any number of Web-based games now available from chess to action games like Quake! These have become enormously popular and provide interactivity with others who are online at the same time. The Future - Behold the future: networked virtual reality. Networked virtual reality is destined to play a prominent role in the future of mass computing because it is an extension of what may be called the "graphical revolution." Until recent times, computers were difficult to operate, required specialized knowledge of arcane keyboard commands and were the province of experts. It was not until the early 1980s that computers began to win a place in large numbers on the desktops of ordinary users. The creation of a mass market had to await the "graphical user interface," i.e., the point-&-click operating systems that made computers much easier to use. Ease of use is the cardinal principle that drives mass market adoption of new technologies. The graphical revolution continued in the mid-1990s when the Web, with its colorful graphics and point-&-click browser software, virtually eclipsed all of gopherspace and turned the Internet into a mass phenomenon. The next step in the graphical revolution is networked virtual reality precisely because the same ingredients are at work - visualization and ease of use. Networked virtual reality will have numerous applications - some useful and some entertaining - and will require no specialized knowledge of computing to operate. Virtual Reality is a term loosely applied to a set of developments ranging from enhanced computer graphics to the creation of entire imaginary worlds in which the computer user feels totally immersed. The ability to select any and all viewing positions places the user inside a computer-generated world that can be explored much like someone would move around a zoo or shopping mall in real life. Although the line is somewhat blurred between enhanced computer graphics and virtual reality, it is said that the sense of total immersion and the ability to interact with features within the simulated environment are the distinguishing characteristics of virtual reality. Everyone has seen the clumsy head gear commonly associated with virtual reality, but avatars (on-screen user representations having human form), voice commands and head trackers can effectively allow users to feel as if they are inhabiting the computer-generated space without bulky helmets, body suits or data gloves, simply by viewing an ordinary screen. The three-dimensional visual effects can be enhanced by using stereoscopic glasses of various types already available. Just a few years ago, the idea of a virtual reality Net was the stuff of science fiction. However, networked virtual environments will soon be upon us. Virtual space on the Internet is already in its infancy, characterized by static scenes and cartoon worlds. One can already visit sites on the Web featuring enhanced 3-D graphics generated with Virtual Reality Markup Language (VRML). The important point to grasp is that the Internet is not standing still; it is continuously undergoing development and improvement. Next Generation Internet initiatives will bring increased bandwidth, distributed supercomputing and virtual reality to the desktop within the next 5 to10 years. Because virtual reality is graphical, easy to use and requires no formal computer training, it is destined to hit a home run in the mass market. Accessing virtual worlds on the Net will be as routine tomorrow as online text searching is today. Virtual reality is already being used in training and education. VR flight simulators give new airline pilots a good sense of what it is like to fly a real plane. Virtual reality is also being used to recondition emotional responses. VR experiments have been conducted and have been proven effective in desensitizing people's fears of, among other things, bugs and airline travel. Decreased fears have been documented and the effects have been shown to be long-lasting. Moreover, subjects report that they recall their VR experiences when they encounter the real thing and that this calms them down. Thus, VR is shaping up to be a very powerful teaching tool. Using computer-generated environments to simulate real-world experiences allows people to learn at their own pace. Additionally, people are more highly motivated to learn when they are working through their own choices. Moreover, VR learning is multimodal and studies show that higher retention is achieved when learners "see, hear and do" than when the signal comes through just a single channel. VR lets people learn from their experiences, not just memorize a bunch of rules. For all of these reasons, training in sophisticated settings from business to the military is shifting to virtual reality. If VR has been shown effective in desensitizing emotional response, can the use of VR for consciousness-raising and heightening sensitivity be far behind? Psychology software is already in use in prisons addressing problem areas like addiction, stress, communication skills and relationships. Inmates are amazed at how accurately the programs describe their deficiencies. Virtual reality is the obvious next step. Virtual reality, because it can engender a sense of participating in an experience in which personal human choices determine the outcome, can be used to teach empathy to criminal offenders. Here are some ideas for using VR to teach empathy and to impart other humane values to prisoners: • Sage - An inmate takes a stroll in the garden with Aristotle, Martin Luther King or other wise personage who would be programmed to answer questions in words close to what the figure actually said in history. Physical activities could include sitting on benches and tossing coins from a bridge. • Ouch! - An inmate gives a virtual haircut to a person seated in a barber's chair. The challenge would be to find out what kind of haircut the person wants and to accomplish the task without inflicting pain. It would take time for the inmate to achieve the dexterity required to do a good job for someone without hurting them. Success in the game would be rewarded in real life with additional days of "good time" (time off for good behavior built into every sentence). • Commute - Here the inmate takes a ride on a crowded subway and tries to maneuver without bumping into anyone or knocking anyone with a briefcase. A pregnant woman with small child passes by a seated offender until the offender gets the point that the seat should be given up to a person who needs it more. Points would accrue if the offender apologizes for mistakes. This could be a multi-user environment offering interaction with other prisoners who are online at the same time. • Red Cross - A multi-user game where offenders win good time by successfully helping the Red Cross give relief to victims at an earthquake or other virtual disaster scene. Offenders must master carrying blankets, putting up tents, affixing bandages, resuscitating victims, etc. Some of these tasks would require cooperation so offenders would not win points unless they learned how to interact successfully with others. The casualty count and thus the reward of "good-time" would be determined by how effectively inmates cooperate in administering aid. • Everest - A single- or multi-user game in which the offender joins an expedition to climb Mt. Everest. Being tied by a rope to other climbers forces the inmate into an interdependent situation in which helping and being helped are critical to survival when missteps occur. • Jumbo Jet - An interactive modeling game where inmates cooperate in designing a jumbo jetliner and getting it to fly successfully. • Sighted Guide - Last, but not least, a game in which inmates score points by successfully leading a blind person through a building. This scenario is a natural for virtual reality because the guide has to navigate around corners, assist the blind person on a set of stairs, etc., all the while using proper techniques and avoiding potential hazards. For example, inmates must learn not to put items down where blind people can trip over them. Inmates must also learn not to carry on conversations in doorways with their backs to the traffic flow because blind people will run into them. Not only must the inmate faithfully discharge responsibility to another person in order to score points, the inmate acquires the perspective that comes with the realization that, no matter how bad the inmate's life has been, others are worse off. Networked virtual reality games in prison libraries? Maybe not this year or the next, but global computer networking is reaching critical mass. It is incumbent on information professionals to keep up with networking technologies and, just as importantly, to devise ways to deploy them in the service of humane values. All of the pieces for networked virtual reality applications will soon be ready for assembly. If information professionals don't do it, someone else will.|## 情感连接的技术革新：互联网如何重塑人类同理心"我们彼此相连。"近年来，这种认为所有人以某种方式相互关联、一个人的行为必然影响他人的观念不断被提及。这一理念已渗透到流行文化中——在《异乡异客》中，火星人教会人类"心灵融合"；卡洛斯·卡斯塔尼达通过《前往伊克斯特兰之旅》系列让精神求索者看见人际纽带；奥森·斯科特·卡德则在其科幻系列中为人际连接赋予物理学基础。自19世纪40年代电报发明以来，即时通信成为可能，连接人类的线缆如葛藤般在全球蔓延。电话网络和互联网（电信与计算的融合）进一步具象化了人际纽带。如今我们生活在一个日益网络化的世界——全球计算机网络使多领域前沿技术得以加速发展。研究者们摆脱了孤立状态，能即时与同行交流成果，协作研究在某种程度上已取代单打独斗。正因如此，互联网被称为"世界大脑"。但人类的连接不仅通过智力，更通过情感纽带。被完全忽视的是互联网在促进情感连接方面的惊人潜力——它能跨越物理距离，让人类大家庭更紧密。对于那些因漠视犯罪行为对他人影响而触犯社会规范者，互联网技术可以培养其共情能力。虽然技术常被视为冷漠甚至反人性的，但它终究只是工具，同样可以服务于人文价值。没有理由认为互联网不能同时成为"世界心脏"。犯罪学思想数百年来始终围绕惩罚、威慑与改造三大主题循环。近年兴起的"被害人权利运动"实质上是古老主题的最新变奏。这种被称为"恢复性司法"的理念催生了"加害者-被害人调解项目"，让双方面对面达成书面赔偿协议。许多加害者在被理解时获得情感宣泄（"我想让他们知道我不是坏人，只是犯了错"）。宽恕、理解、救赎、共情、补偿机会——这些项目中蕴含的情感强度证明：情感因素在犯罪心理中扮演重要角色，或可用来重构思维模式，降低犯罪倾向。美国一些监狱已开始尝试共情教育。加州监狱的"被害人共情小组"引导参与者认识其犯罪行为对被害人、被害人家属及自身家庭的影响，以此培养共情能力与自我控制。虽然共情教育不可能感化所有罪犯，但对那些能通过情感智能修正行为者确有价值。互联网作为强大媒介，其力量源于促进人际互动的能力。与电视报纸不同，互联网的交互特性为罪犯共情教育开创了新可能。实时协作创作互动小说或参与网络论坛，能将活动范式从孤立转向协作——成功的协作需要顾及他人感受，从而催生更强的共情反应。监狱信息工作者正面临黄金机遇：利用现有及未来的网络技术满足服刑人员多元需求。除传统读物外，许多服刑人员渴望改善生活、寻求转变、保持与外界联系，并掌握计算机等新技能以利回归社会。网络技术既能扩展囚犯与外界联系，也能让监狱图书馆资源效益最大化——任何地点的创作都能通过互联网共享至其他机构，各类论坛与多用户域能即时传播至全球监狱。当前，各司法管辖区对监狱使用网络技术的态度迥异。马里兰州正尝试为监狱图书馆提供电子邮件访问和州信息系统，而联邦监狱管理局则严禁囚犯接触互联网。但防火墙等安全技术的发展终将使联邦系统步马里兰后尘——网络化浪潮不可阻挡。以下技术已可用于监狱共情教育：- 互动论坛：组织被害人讲述经历，引导加害者讨论认知转变- 互动小说创作：满足囚犯写作需求，培养协作能力- 网络广播：合作制作戒毒等主题节目- 在线教育：提供远程学位课程- 娱乐互动：象棋到动作游戏的网络对战展望未来，联网虚拟现实（VR）将在大众计算领域大放异彩。作为"图形革命"的延伸，VR无需专业计算机知识即可操作。虽然当前互联网虚拟空间尚处萌芽阶段（以静态场景和卡通世界为特征），但"下一代互联网"计划将在5-10年内实现带宽提升、分布式超级计算和桌面级VR。VR已被证实能有效调节情绪反应。在矫正领域，VR的沉浸式特性使其成为培养共情能力的理想工具。我们可设计多种VR情景帮助囚犯建立同理心：1. 《智者同行》：与历史名人对话，领悟智慧2. 《理发师挑战》：学习在服务中避免伤害他人3. 《地铁通勤》：在拥挤环境中培养空间意识4. 《救灾行动》：通过团队协作获得减刑奖励5. 《珠峰探险》：在生死相依中理解互助价值6. 《盲导任务》：通过协助视障者获得角色转换体验监狱图书馆的联网VR应用或许不会立即实现，但全球计算机网络正达临界规模。信息专业人员有责任跟进网络技术发展，并设计其人文价值应用方案。联网VR的所有技术组件即将就绪——如果我们不行动，别人就会先行。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tangible+Web:+An+Interactive+Immersion+Virtual+Reality+Creativity+System+that+Travels+Across+Reality)|0|
|[Coherent Topic Modeling for Creative Multimodal Data on Social Media](https://doi.org/10.1145/3543507.3587433)|Junaid Rashid, Jungeun Kim, Usman Naseem|School of Computer Science, The University of Sydney, Sydney, Australia, Australia; Department of Data Science, Sejong University, Seoul, Republic of Korea, Republic of Korea; Department of Software, Kongju National University, Cheonan, Republic of Korea, Republic of Korea|The creative web is all about combining different types of media to create a unique and engaging online experience. Multimodal data, such as text and images, is a key component in the creative web. Social media posts that incorporate both text descriptions and images offer a wealth of information and context. Text in social media posts typically relates to one topic, while images often convey information about multiple topics due to the richness of visual content. Despite this potential, many existing multimodal topic models do not take these criteria into account, resulting in poor quality topics being generated. Therefore, we proposed a Coherent Topic modeling for Multimodal Data (CTM-MM), which takes into account that text in social media posts typically relates to one topic, while images can contain information about multiple topics. Our experimental results show that CTM-MM outperforms traditional multimodal topic models in terms of classification and topic coherence.|创意网站就是将不同类型的媒体结合起来，创造一种独特而迷人的在线体验。多模态数据，如文本和图像，是创造性网络的关键组成部分。结合文字描述和图像的社交媒体帖子提供了丰富的信息和背景。社交媒体帖子中的文本通常与一个主题相关，而图像由于视觉内容的丰富性，通常传达关于多个主题的信息。尽管有这种潜力，许多现有的多模式主题模型没有考虑到这些标准，导致生成的主题质量很差。因此，我们提出了一个多模态数据的连贯主题模型(CTM-MM) ，它考虑到了社会媒体帖子中的文本通常与一个主题相关，而图像可以包含关于多个主题的信息。实验结果表明，CTM-MM 模型在主题分类和主题连贯性方面优于传统的多模态主题模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Coherent+Topic+Modeling+for+Creative+Multimodal+Data+on+Social+Media)|0|
|[Improving Health Mention Classification Through Emphasising Literal Meanings: A Study Towards Diversity and Generalisation for Public Health Surveillance](https://doi.org/10.1145/3543507.3583877)|Olanrewaju Tahir Aduragba, Jialin Yu, Alexandra I. Cristea, Yang Long|Department of Computer Science, Durham University, United Kingdom and Kwara State University, Nigeria; Department of Computer Science, Durham University, United Kingdom; Department of Computer Science, Durham University, United Kingdom and University College London, United Kingdom|People often use disease or symptom terms on social media and online forums in ways other than to describe their health. Thus the NLP health mention classification (HMC) task aims to identify posts where users are discussing health conditions literally, not figuratively. Existing computational research typically only studies health mentions within well-represented groups in developed nations. Developing countries with limited health surveillance abilities fail to benefit from such data to manage public health crises. To advance the HMC research and benefit more diverse populations, we present the Nairaland health mention dataset (NHMD), a new dataset collected from a dedicated web forum for Nigerians. NHMD consists of 7,763 manually labelled posts extracted based on four prevalent diseases (HIV/AIDS, Malaria, Stroke and Tuberculosis) in Nigeria. With NHMD, we conduct extensive experiments using current state-of-the-art models for HMC and identify that, compared to existing public datasets, NHMD contains out-of-distribution examples. Hence, it is well suited for domain adaptation studies. The introduction of the NHMD dataset imposes better diversity coverage of vulnerable populations and generalisation for HMC tasks in a global public health surveillance setting. Additionally, we present a novel multi-task learning approach for HMC tasks by combining literal word meaning prediction as an auxiliary task. Experimental results demonstrate that the proposed approach outperforms state-of-the-art methods statistically significantly (p < 0.01, Wilcoxon test) in terms of F1 score over the state-of-the-art and shows that our new dataset poses a strong challenge to the existing HMC methods.|人们经常在社交媒体和在线论坛上使用疾病或症状的术语，而不是用来描述自己的健康状况。因此，NLP 健康提及分类(HMC)任务旨在确定用户讨论健康状况的帖子。现有的计算机研究通常只研究发达国家中有代表性的群体中的健康提及情况。卫生监测能力有限的发展中国家无法从这些数据中受益，无法管理公共卫生危机。为了推进健康数据中心的研究，让更多不同的人群受益，我们提出了 Nairaland 健康提及数据集(nhMD) ，这是一个新的数据集，收集自一个专门为尼日利亚人设立的网络论坛。NHMD 包括7,763个根据尼日利亚四种流行疾病(艾滋病毒/艾滋病、疟疾、中风和结核病)手工标记的帖子。使用 NHMD，我们使用当前最先进的 HMC 模型进行了广泛的实验，并发现，与现有的公共数据集相比，NHMD 包含超出分布范围的示例。因此，它非常适合于领域适应性研究。NHMD 数据集的引入提高了脆弱人群的多样性覆盖率，并在全球公共卫生监测背景下推广了 HMC 任务。此外，我们提出了一种新的 HMC 任务的多任务学习方法，结合字面意义预测作为辅助任务。实验结果表明，所提出的方法在 F1评分方面在统计学上显着优于最先进的方法(p < 0.01，Wilcoxon 检验) ，并且表明我们的新数据集对现有的 HMC 方法提出了强烈的挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Health+Mention+Classification+Through+Emphasising+Literal+Meanings:+A+Study+Towards+Diversity+and+Generalisation+for+Public+Health+Surveillance)|0|
|[Learning Faithful Attention for Interpretable Classification of Crisis-Related Microblogs under Constrained Human Budget](https://doi.org/10.1145/3543507.3583861)|Thi Huyen Nguyen, Koustav Rudra|Indian Institute of Technology (Indian School of Mines) Dhanbad, India; L3S Research Center, Germany|The recent widespread use of social media platforms has created convenient ways to obtain and spread up-to-date information during crisis events such as disasters. Time-critical analysis of crisis data can help human organizations gain actionable information and plan for aid responses. Many existing studies have proposed methods to identify informative messages and categorize them into different humanitarian classes. Advanced neural network architectures tend to achieve state-of-the-art performance, but the model decisions are opaque. While attention heatmaps show insights into the model’s prediction, some studies found that standard attention does not provide meaningful explanations. Alternatively, recent works proposed interpretable approaches for the classification of crisis events that rely on human rationales to train and extract short snippets as explanations. However, the rationale annotations are not always available, especially in real-time situations for new tasks and events. In this paper, we propose a two-stage approach to learn the rationales under minimal human supervision and derive faithful machine attention. Extensive experiments over four crisis events show that our model is able to obtain better or comparable classification performance (∼ 86% Macro-F1) to baselines and faithful attention heatmaps using only 40-50% human-level supervision. Further, we employ a zero-shot learning setup to detect actionable tweets along with actionable word snippets as rationales.|最近社交媒体平台的广泛使用为灾害等危机事件期间获取和传播最新信息创造了便利的途径。对危机数据进行时间要求严格的分析，可以帮助人类组织获得可操作的信息，并制定援助应对计划。许多现有研究提出了确定信息性信息并将其分为不同人道主义类别的方法。先进的神经网络架构往往能够实现最先进的性能，但是模型决策是不透明的。虽然注意力热图显示了对模型预测的洞察力，但一些研究发现，标准注意力并不能提供有意义的解释。或者，最近的研究提出了可解释的危机事件分类方法，这些方法依赖于人类的基本原理来训练和提取简短的片段作为解释。但是，基本原理注释并不总是可用的，特别是在新任务和事件的实时情况下。在本文中，我们提出了一个两阶段的方法来学习最小人类监督下的基本原理和获得忠实的机器注意。对四个危机事件的广泛实验表明，我们的模型能够使用40-50% 的人类水平监督获得更好或可比较的分类性能(something 86% 宏观 F1)到基线和忠实的注意热图。此外，我们使用了一个零拍摄学习设置来检测可操作的 tweet 以及可操作的单词片段作为基本原理。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Faithful+Attention+for+Interpretable+Classification+of+Crisis-Related+Microblogs+under+Constrained+Human+Budget)|0|
|[Attacking Fake News Detectors via Manipulating News Social Engagement](https://doi.org/10.1145/3543507.3583868)|Haoran Wang, Yingtong Dou, Canyu Chen, Lichao Sun, Philip S. Yu, Kai Shu|Department of Computer Science, University of Illinois Chicago, USA and Visa Research, USA; Department of Computer Science, University of Illinois at Chicago, USA; Department of Computer Science and Engineering, Lehigh University, USA; Department of Computer Science, Illinois Institute of Technology, USA|Social media is one of the main sources for news consumption, especially among the younger generation. With the increasing popularity of news consumption on various social media platforms, there has been a surge of misinformation which includes false information or unfounded claims. As various text- and social context-based fake news detectors are proposed to detect misinformation on social media, recent works start to focus on the vulnerabilities of fake news detectors. In this paper, we present the first adversarial attack framework against Graph Neural Network (GNN)-based fake news detectors to probe their robustness. Specifically, we leverage a multi-agent reinforcement learning (MARL) framework to simulate the adversarial behavior of fraudsters on social media. Research has shown that in real-world settings, fraudsters coordinate with each other to share different news in order to evade the detection of fake news detectors. Therefore, we modeled our MARL framework as a Markov Game with bot, cyborg, and crowd worker agents, which have their own distinctive cost, budget, and influence. We then use deep Q-learning to search for the optimal policy that maximizes the rewards. Extensive experimental results on two real-world fake news propagation datasets demonstrate that our proposed framework can effectively sabotage the GNN-based fake news detector performance. We hope this paper can provide insights for future research on fake news detection.|社交媒体是新闻消费的主要来源之一，尤其是在年轻一代中。随着新闻消费在各种社交媒体平台上的日益普及，虚假信息和毫无根据的言论层出不穷。随着各种基于文本和社会背景的假新闻检测器被提出来检测社交媒体上的错误信息，最近的工作开始关注假新闻检测器的脆弱性。针对基于图神经网络(GNN)的虚假新闻检测器提出了第一个对抗性攻击框架，以检验其鲁棒性。具体来说，我们利用一个多代理强化学习框架来模拟欺诈者在社交媒体上的对抗行为。研究表明，在现实世界中，欺诈者相互配合，分享不同的新闻，以逃避假新闻检测器的检测。因此，我们将 MARL 框架建模为一个包含机器人、半机器人和群体工作者代理的马尔可夫博弈，这些代理有自己独特的成本、预算和影响力。然后，我们使用深度 Q 学习来寻找最优的策略，使回报最大化。在两个真实假新闻传播数据集上的大量实验结果表明，我们提出的框架能够有效地破坏基于 GNN 的假新闻检测器的性能。希望本文的研究能为今后假新闻检测的研究提供一些启示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Attacking+Fake+News+Detectors+via+Manipulating+News+Social+Engagement)|0|
|[ContrastFaux: Sparse Semi-supervised Fauxtography Detection on the Web using Multi-view Contrastive Learning](https://doi.org/10.1145/3543507.3583869)|Ruohan Zong, Yang Zhang, Lanyu Shang, Dong Wang|School of Information Sciences, University of Illinois Urbana-Champaign, USA|The widespread misinformation on the Web has raised many concerns with serious societal consequences. In this paper, we study a critical type of online misinformation, namely fauxtography, where the image and associated text of a social media post jointly convey a questionable or false sense. In particular, we focus on a sparse semi-supervised fauxtography detection problem, which aims to accurately identify fauxtography by only using the sparsely annotated ground truth labels of social media posts. Our problem is motivated by the key limitation of current fauxtography detection approaches that often require a large amount of expensive and inefficient manual annotations to train an effective fauxtography detection model. We identify two key technical challenges in solving the problem: 1) it is non-trivial to train an accurate detection model given the sparse fauxtography annotations, and 2) it is difficult to extract the heterogeneous and complicated fauxtography features from the multi-modal social media posts for accurate fauxtography detection. To address the above challenges, we propose ContrastFaux, a multi-view contrastive learning framework that jointly explores the sparse fauxtography annotations and the cross-modal fauxtography feature similarity between the image and text in multi-modal posts to accurately detect fauxtography on social media. Evaluation results on two social media datasets demonstrate that ContrastFaux consistently outperforms state-of-the-art deep learning and semi-supervised learning fauxtography detection baselines by achieving the highest fauxtography detection accuracy.|网络上广泛存在的错误信息引起了许多严重的社会后果。在本文中，我们研究了一种关键类型的网络虚假信息，即人造图像，其中的图像和相关的文本的社会媒体帖子共同传达一个可疑的或错误的意义。特别地，我们集中在一个稀疏的半监督的人造图像检测问题，其目的是准确地识别人造图像只使用稀疏注释的地面真相标签的社会媒体帖子。我们的问题是由于当前人造地图检测方法的关键局限性而产生的，这些方法通常需要大量昂贵和低效的手工注释来训练一个有效的人造地图检测模型。我们确定了解决这个问题的两个关键技术挑战: 1)在稀疏的人造图注释的情况下，训练一个准确的检测模型是不容易的; 2)难以从多模态社交媒体帖子中提取异构和复杂的人造图特征来进行准确的人造图检测。为了应对上述挑战，我们提出了一个多视图对比学习框架，该框架共同探索稀疏的人造图像注释和多模态文章中图像和文本之间的跨模态人造图像特征的相似性，以准确检测社交媒体上的人造图像。对两个社交媒体数据集的评估结果表明，compastfaux 通过实现最高的人造图像检测准确度，始终优于最先进的深度学习和半监督学习人造图像检测基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ContrastFaux:+Sparse+Semi-supervised+Fauxtography+Detection+on+the+Web+using+Multi-view+Contrastive+Learning)|0|
|[Interpreting wealth distribution via poverty map inference using multimodal data](https://doi.org/10.1145/3543507.3583862)|Lisette EspínNoboa, János Kertész, Márton Karsai|Central European University, Austria and Rènyi Institute of Mathematics, Hungary; Central European University, Austria and Complexity Science Hub Vienna, Austria|Poverty maps are essential tools for governments and NGOs to track socioeconomic changes and adequately allocate infrastructure and services in places in need. Sensor and online crowd-sourced data combined with machine learning methods have provided a recent breakthrough in poverty map inference. However, these methods do not capture local wealth fluctuations, and are not optimized to produce accountable results that guarantee accurate predictions to all sub-populations. Here, we propose a pipeline of machine learning models to infer the mean and standard deviation of wealth across multiple geographically clustered populated places, and illustrate their performance in Sierra Leone and Uganda. These models leverage seven independent and freely available feature sources based on satellite images, and metadata collected via online crowd-sourcing and social media. Our models show that combined metadata features are the best predictors of wealth in rural areas, outperforming image-based models, which are the best for predicting the highest wealth quintiles. Our results recover the local mean and variation of wealth, and correctly capture the positive yet non-monotonous correlation between them. We further demonstrate the capabilities and limitations of model transfer across countries and the effects of data recency and other biases. Our methodology provides open tools to build towards more transparent and interpretable models to help governments and NGOs to make informed decisions based on data availability, urbanization level, and poverty thresholds.|贫困地图是政府和非政府组织跟踪社会经济变化和在需要的地方适当分配基础设施和服务的重要工具。传感器和在线众包数据结合机器学习方法为贫困地图推断提供了新的突破。然而，这些方法不能捕捉当地的财富波动，也不能优化以产生可问责的结果，从而保证对所有亚群的准确预测。在这里，我们提出了一系列机器学习模型，用于推断多个地理集群人口密集地区的财富平均值和标准差，并说明它们在塞拉利昂和乌干达的表现。这些模型利用了七个独立和免费提供的特征源，这些特征源基于卫星图像，以及通过在线众包和社交媒体收集的元数据。我们的模型显示，综合元数据特征是农村地区财富的最佳预测因子，表现优于基于图像的模型，后者是预测最高财富五分位数的最佳模型。我们的研究结果恢复了财富的局部均值和变化，并且正确地捕捉了它们之间的正相关和非单调相关。我们进一步展示了跨国模型转移的能力和局限性以及数据新近性和其他偏差的影响。我们的方法提供了开放的工具，以建立更加透明和可解释的模式，帮助政府和非政府组织根据数据可用性、城市化水平和贫困阈值作出知情决定。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interpreting+wealth+distribution+via+poverty+map+inference+using+multimodal+data)|0|
|[MSQ-BioBERT: Ambiguity Resolution to Enhance BioBERT Medical Question-Answering](https://doi.org/10.1145/3543507.3583878)|Muzhe Guo, Muhao Guo, Edward T. Dougherty, Fang Jin|Arizona State University, USA; Roger Williams University, USA; George Washington University, USA|Question answering (QA) is a task in the field of natural language processing (NLP) and information retrieval, which has pivotal applications in areas such as online reading comprehension and web search engines. Currently, Bidirectional Encoder Representations from Transformers (BERT) and its biomedical variation (BioBERT) achieve impressive results on the reading comprehension QA datasets and medical-related QA datasets, and so they are widely used for a variety of passage-based QA tasks. However, their performances rapidly deteriorate when encountering passage and context ambiguities. This issue is prevalent and unavoidable in many fields, notably the web-based medical field. In this paper, we introduced a novel approach called the Multiple Synonymous Questions BioBERT (MSQ-BioBERT), which integrates question augmentation, rather than the typical single question used by traditional BioBERT, to elevate BioBERT’s performance on medical QA tasks. In addition, we constructed an ambiguous medical dataset based on the information from Wikipedia web. Experiments with both this web-based constructed medical dataset and open biomedical datasets demonstrate the significant performance gains of the MSQ-BioBERT approach, showcasing a new method for addressing ambiguity in medical QA tasks.|问答是自然语言处理和信息检索领域的一项任务，在在线阅读理解和网络搜索引擎等领域有着关键的应用。目前，变压器双向编码器表示(BERT)及其生物医学变异(BioBERT)在阅读理解质量保证数据集和医学相关质量保证数据集上取得了令人印象深刻的结果，因此它们被广泛用于各种基于通道的质量保证任务。然而，当遇到短文和上下文歧义时，它们的表现会迅速恶化。这个问题在许多领域都是普遍的和不可避免的，特别是在基于网络的医学领域。为了提高 BioBERT 在医学 QA 任务中的表现，本文引入了一种新的方法——多同义问题 BioBERT (MSQ-BioBERT) ，该方法集成了问题增强，而不是传统 BioBERT 使用的典型的单个问题。此外，我们基于维基百科网站的信息构建了一个模糊的医学数据集。基于网络构建的医学数据集和开放式生物医学数据集的实验表明，MSQ-BioBERT 方法具有显著的性能增益，为解决医学 QA 任务中的模糊性提供了一种新的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MSQ-BioBERT:+Ambiguity+Resolution+to+Enhance+BioBERT+Medical+Question-Answering)|0|
|[Graph-based Village Level Poverty Identification](https://doi.org/10.1145/3543507.3583864)|Jing Ma, Liangwei Yang, Qiong Feng, Weizhi Zhang, Philip S. Yu|Southwest Minzu University, China; University of Electronic Science and Technology of China, China; University of Illinois Chicago, USA|Poverty status identification is the first obstacle to eradicating poverty. Village-level poverty identification is very challenging due to the arduous field investigation and insufficient information. The development of the Web infrastructure and its modeling tools provides fresh approaches to identifying poor villages. Upon those techniques, we build a village graph for village poverty status identification. By modeling the village connections as a graph through the geographic distance, we show the correlation between village poverty status and its graph topological position and identify two key factors (Centrality, Homophily Decaying effect) for identifying villages. We further propose the first graph-based method to identify poor villages. It includes a global Centrality2Vec module to embed village centrality into the dense vector and a local graph distance convolution module that captures the decaying effect. In this paper, we make the first attempt to interpret and identify village-level poverty from a graph perspective.|确定贫困状况是消除贫困的第一个障碍。由于实地调查工作的艰巨性和信息的不足，村级贫困的识别具有很大的挑战性。Web 基础设施及其建模工具的开发为识别贫困村庄提供了新的方法。基于这些技术，我们构建了一个村庄图，用于村庄贫困状况的识别。通过对村庄连接的地理距离建模，揭示了村庄贫困状况与其图形拓扑位置之间的相关关系，并确定了识别村庄的两个关键因素(中心性、同质性衰减效应)。进一步提出了第一种基于图论的贫困村识别方法。它包括一个全局 Centrality2Vec 模块，将村庄中心性嵌入到密集向量中，以及一个捕捉衰减效应的局部图距离卷积模块。本文首次尝试从图形的角度对村级贫困进行解释和识别。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph-based+Village+Level+Poverty+Identification)|0|
|[Mapping Flood Exposure, Damage, and Population Needs Using Remote and Social Sensing: A Case Study of 2022 Pakistan Floods](https://doi.org/10.1145/3543507.3583881)|Zainab Akhtar, Umair Qazi, Rizwan Sadiq, Aya ElSakka, Muhammad Sajjad, Ferda Ofli, Muhammad Imran|Department of Geography and Centre for Geocomputation Studies, Hong Kong Baptist University, Hong Kong; Qatar Computing Research Institute, Qatar|The devastating 2022 floods in Pakistan resulted in a catastrophe impacting millions of people and destroying thousands of homes. While disaster management efforts were taken, crisis responders struggled to understand the country-wide flood extent, population exposure, urgent needs of affected people, and various types of damage. To tackle this challenge, we leverage remote and social sensing with geospatial data using state-of-the-art machine learning techniques for text and image processing. Our satellite-based analysis over a one-month period (25 Aug–25 Sep) revealed that 11.48% of Pakistan was inundated. When combined with geospatial data, this meant 18.9 million people were at risk across 160 districts in Pakistan, with adults constituting 50% of the exposed population. Our social sensing data analysis surfaced 106.7k reports pertaining to deaths, injuries, and concerns of the affected people. To understand the urgent needs of the affected population, we analyzed tweet texts and found that South Karachi, Chitral and North Waziristan required the most basic necessities like food and shelter. Further analysis of tweet images revealed that Lasbela, Rajanpur, and Jhal Magsi had the highest damage reports normalized by their population. These damage reports were found to correlate strongly with affected people reports and need reports, achieving an R-Square of 0.96 and 0.94, respectively. Our extensive study shows that combining remote sensing, social sensing, and geospatial data can provide accurate and timely information during a disaster event, which is crucial in prioritizing areas for immediate and gradual response.|巴基斯坦2022年毁灭性的洪水造成了一场灾难，影响到数百万人，摧毁了数千所房屋。在灾害管理工作的同时，应对危机的人员努力了解全国范围的洪水范围、人口暴露、受灾人民的紧急需求以及各种类型的损失。为了应对这一挑战，我们利用地理空间数据进行遥感和社会感应，使用最先进的机器学习技术进行文本和图像处理。我们在一个月期间(8月25日至9月25日)的卫星分析显示，巴基斯坦11.48% 的土地被淹没。结合地理空间数据，这意味着巴基斯坦160个地区的1890万人处于危险之中，成年人占暴露人口的50% 。我们的社会传感数据分析显示，有106.7万份报告与受影响人群的死亡、受伤和担忧有关。为了了解受灾人口的迫切需求，我们分析了推特短信，发现南卡拉奇、 Chitral 和北瓦济里斯坦特区需要食物和住所等最基本的必需品。对推特图片的进一步分析显示，Lasbela、 Rajanpur 和贾尔•马格西(Jhal Magsi)的人口正常化损失报告最高。这些损害报告被发现与受影响的人的报告和需要报告密切相关，分别达到0.96和0.94的 R 平方。我们广泛的研究表明，结合遥感、社会感应和地理空间数据可以在灾害事件期间提供准确和及时的信息，这对于确定立即和逐步响应的优先领域至关重要。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mapping+Flood+Exposure,+Damage,+and+Population+Needs+Using+Remote+and+Social+Sensing:+A+Case+Study+of+2022+Pakistan+Floods)|0|
|[Web Information Extraction for Social Good: Food Pantry Answering As an Example](https://doi.org/10.1145/3543507.3583880)|HuanYuan Chen, Hong Yu|University of Massachusetts Lowell, USA; University of Massachusetts Amherst, USA|Social determinants of health (SDH) are the conditions in which we are born, live, work, and age. Food insecurity (FI) is an important domain of SDH. FI is associated with poor health outcomes. Food bank/pantry (food pantry) directly addresses FI. Improving the availability and quality of food from food pantries could reduce FI, leading to improved health outcomes. However, it is difficult for a client to access food pantry information. In this study, we built a food pantry answering framework by combining location-aware information retrieval, web information extraction and domain-specific answering. Our proposed framework first retrieves pantry candidates based on geolocation of the client, and utilizes structural information from markup language to extract semantic chunks related to six common client requests. We use BERT and RoBERTa as information extraction models and compare three different web page segmentation methods in the experiments.|健康的社会决定因素(SDH)是我们出生、生活、工作和年龄的条件。粮食不安全是 SDH 研究的一个重要领域。FI 与不良的健康结果相关。食品银行/食品储藏室(食品储藏室)直接地址为 FI。改善食品储藏室食品的供应和质量可以减少流动性感染，从而改善健康结果。然而，客户很难获得食品储藏室的信息。在这项研究中，我们通过结合位置感知信息检索、网络信息抽取和特定领域的应答，建立了一个食品储藏室应答框架。我们提出的框架首先基于客户端的地理位置检索食品储藏室候选者，然后利用标记语言中的结构信息提取与六个常见客户端请求相关的语义块。我们使用 BERT 和 roBERTa 作为信息抽取模型，并在实验中比较了三种不同的网页分割方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Web+Information+Extraction+for+Social+Good:+Food+Pantry+Answering+As+an+Example)|0|
|[Gender Pay Gap in Sports on a Fan-Request Celebrity Video Site](https://doi.org/10.1145/3543507.3583884)|Nazanin Sabri, Stephen Reysen, Ingmar Weber|University of California, San Diego, USA; Saarland University, Germany; Texas A&M University-Commerce, USA|The internet is often thought of as a democratizer, enabling equality in aspects such as pay, as well as a tool introducing novel communication and monetization opportunities. In this study we examine athletes on Cameo, a website that enables bi-directional fan-celebrity interactions, questioning whether the well-documented gender pay gaps in sports persist in this digital setting. Traditional studies into gender pay gaps in sports are mostly in a centralized setting where an organization decides the pay for the players, while Cameo facilitates grass-roots fan engagement where fans pay for video messages from their preferred athletes. The results showed that even on such a platform gender pay gaps persist, both in terms of cost-per-message, and in the number of requests, proxied by number of ratings. For instance, we find that female athletes have a median pay of 30$ per-video, while the same statistic is 40$ for men. The results also contribute to the study of parasocial relationships and personalized fan engagements over a distance. Something that has become more relevant during the ongoing COVID-19 pandemic, where in-person fan engagement has often been limited.|互联网往往被认为是一个民主化者，使平等的方面，如薪酬，以及工具引入新的沟通和货币化的机会。在这项研究中，我们对 Cameo 网站上的运动员进行了调查，这是一个允许粉丝和名人进行双向互动的网站，我们质疑在这种数字环境下，体育运动中的性别收入差距是否依然存在。关于体育运动中性别薪酬差距的传统研究大多集中在一个集中的环境中，由一个组织决定运动员的薪酬，而 Cameo 促进了基层球迷的参与，球迷为他们喜欢的运动员的视频信息付费。结果表明，即使在这样一个平台上，性别薪酬差距依然存在，无论是从每条信息的成本来看，还是从请求数量来看，都是以评级数量为代表的。例如，我们发现女运动员的平均收入是每段视频30美元，而男运动员的平均收入是40美元。这些结果也有助于研究准社会关系和个性化的球迷参与的距离。在当前的2019冠状病毒疾病流行期间，这一点变得更为重要，因为粉丝的亲自参与往往受到限制。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Gender+Pay+Gap+in+Sports+on+a+Fan-Request+Celebrity+Video+Site)|0|
|[Leveraging Existing Literature on the Web and Deep Neural Models to Build a Knowledge Graph Focused on Water Quality and Health Risks](https://doi.org/10.1145/3543507.3584185)|Nikita Gautam, David Shumway, Megan Kowalcyk, Sarthak Khanal, Doina Caragea, Cornelia Caragea, Hande Mcginty, Samuel Dorevitch|Public Health, University of Illinois Chicago, USA; Computer Science, University of Illinois Chicago, USA; Computer Science, Kansas State University, USA|A knowledge graph focusing on water quality in relation to health risks posed by water activities (such as diving or swimming) is not currently available. To address this limitation, we first use existing resources to construct a knowledge graph relevant to water quality and health risks using KNowledge Acquisition and Representation Methodology (KNARM). Subsequently, we explore knowledge graph completion approaches for maintaining and updating the graph. Specifically, we manually identify a set of domain-specific UMLS concepts and use them to extract a graph of approximately 75,000 semantic triples from the Semantic MEDLINE database (which contains head-relation-tail triples extracted from PubMed). Using the resulting knowledge graph, we experiment with the KG-BERT approach for graph completion by employing pre-trained BERT/RoBERTa models and also models fine-tuned on a collection of water quality and health risks abstracts retrieved from the Web of Science. Experimental results show that KG-BERT with BERT/RoBERTa models fine-tuned on a domain-specific corpus improves the performance of KG-BERT with pre-trained models. Furthermore, KG-BERT gives better results than several translational distance or semantic matching baseline models.|目前还没有一个关于水质与水活动(如潜水或游泳)造成的健康风险相关的知识图表。为了解决这一局限性，我们首先利用现有的资源，使用知识获取和表示方法(KNARM)构建了一个与水质和健康风险相关的知识图。随后，我们探讨了维护和更新知识图的知识图完成方法。具体而言，我们手动识别一组特定于领域的 UMLS 概念，并使用它们从 Semantic MEDLINE 数据库(其中包含从 PubMed 提取的头-关系-尾三元组)提取约75,000个语义三元组的图。使用得到的知识图表，我们通过使用预先训练的 BERT/RoBERTa 模型来实验用 KG-BERT 方法完成图表，并且还对从 Web of Science 检索到的一组水质和健康风险摘要进行微调。实验结果表明，带有 BERT/RoBERTa 模型的 KG-BERT 算法在特定领域语料库上进行了微调，提高了带有预训练模型的 KG-BERT 算法的性能。此外，KG-BERT 模型比多个平移距离或语义匹配基线模型得到了更好的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+Existing+Literature+on+the+Web+and+Deep+Neural+Models+to+Build+a+Knowledge+Graph+Focused+on+Water+Quality+and+Health+Risks)|0|
|[Believability and Harmfulness Shape the Virality of Misleading Social Media Posts](https://doi.org/10.1145/3543507.3583857)|Chiara Patricia Drolsbach, Nicolas Pröllochs||Misinformation on social media presents a major threat to modern societies. While previous research has analyzed the virality across true and false social media posts, not every misleading post is necessarily equally viral. Rather, misinformation has different characteristics and varies in terms of its believability and harmfulness - which might influence its spread. In this work, we study how the perceived believability and harmfulness of misleading posts are associated with their virality on social media. Specifically, we analyze (and validate) a large sample of crowd-annotated social media posts from Twitter's Birdwatch platform, on which users can rate the believability and harmfulness of misleading tweets. To address our research questions, we implement an explanatory regression model and link the crowd ratings for believability and harmfulness to the virality of misleading posts on Twitter. Our findings imply that misinformation that is (i) easily believable and (ii) not particularly harmful is associated with more viral resharing cascades. These results offer insights into how different kinds of crowd fact-checked misinformation spreads and suggest that the most viral misleading posts are often not the ones that are particularly concerning from the perspective of public safety. From a practical view, our findings may help platforms to develop more effective strategies to curb the proliferation of misleading posts on social media.|社交媒体上的错误信息对现代社会构成重大威胁。虽然之前的研究已经分析了真假社交媒体帖子的病毒性，但并不是每一个误导性帖子都一定是病毒性的。相反，错误信息具有不同的特征，在可信度和危害性方面各不相同，这可能会影响其传播。在这项工作中，我们研究了误导性帖子的可信度和危害性是如何与它们在社交媒体上的病毒性相关联的。具体来说，我们分析(并验证)来自 Twitter 的 Birdwatch 平台的大量带有群体注释的社交媒体帖子样本，用户可以在这些样本上评估误导性推文的可信度和危害性。为了解决我们的研究问题，我们实施了一个解释性回归模型，并将可信度和危害性的群体评分与 Twitter 上误导性帖子的病毒性联系起来。我们的研究结果暗示，(i)容易相信和(ii)不是特别有害的错误信息与更多的病毒重新共享级联有关。这些结果提供了不同类型的群众事实核查错误信息如何传播的见解，并表明，病毒性最大的误导性帖子往往不是那些从公共安全的角度特别关注。从实际角度来看，我们的研究结果可能有助于平台制定更有效的策略，以遏制社交媒体上误导性帖子的泛滥。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Believability+and+Harmfulness+Shape+the+Virality+of+Misleading+Social+Media+Posts)|0|
|[Enhancing Deep Knowledge Tracing with Auxiliary Tasks](https://doi.org/10.1145/3543507.3583866)|Zitao Liu, Qiongqiong Liu, Jiahao Chen, Shuyan Huang, Boyu Gao, Weiqi Luo, Jian Weng|College of Information Science and Technology, Jinan University, China; TAL Education Group, China; Guangdong Institute of Smart Education, Jinan University, China|Knowledge tracing (KT) is the problem of predicting students' future performance based on their historical interactions with intelligent tutoring systems. Recent studies have applied multiple types of deep neural networks to solve the KT problem. However, there are two important factors in real-world educational data that are not well represented. First, most existing works augment input representations with the co-occurrence matrix of questions and knowledge components\footnote{\label{ft:kc}A KC is a generalization of everyday terms like concept, principle, fact, or skill.} (KCs) but fail to explicitly integrate such intrinsic relations into the final response prediction task. Second, the individualized historical performance of students has not been well captured. In this paper, we proposed \emph{AT-DKT} to improve the prediction performance of the original deep knowledge tracing model with two auxiliary learning tasks, i.e., \emph{question tagging (QT) prediction task} and \emph{individualized prior knowledge (IK) prediction task}. Specifically, the QT task helps learn better question representations by predicting whether questions contain specific KCs. The IK task captures students' global historical performance by progressively predicting student-level prior knowledge that is hidden in students' historical learning interactions. We conduct comprehensive experiments on three real-world educational datasets and compare the proposed approach to both deep sequential KT models and non-sequential models. Experimental results show that \emph{AT-DKT} outperforms all sequential models with more than 0.9\% improvements of AUC for all datasets, and is almost the second best compared to non-sequential models. Furthermore, we conduct both ablation studies and quantitative analysis to show the effectiveness of auxiliary tasks and the superior prediction outcomes of \emph{AT-DKT}.|知识追踪(KT)是根据学生与智能辅导系统的历史交互关系来预测学生未来表现的问题。最近的研究已经应用了多种类型的深层神经网络来解决 KT 问题。然而，在现实世界的教育数据中有两个重要因素没有得到很好的表示。首先，现有的大多数作品都使用问题和知识组件的共现矩阵脚注{ label { ft: KC }来扩充输入表示。 KC 是概念、原则、事实或技能等日常术语的概括。}但未能将这种内在关系明确整合到最终反应预测任务中。第二，学生的个性化历史表现没有得到很好的捕捉。为了提高原有深度知识追踪模型的预测性能，本文提出了两个辅助学习任务: 重点{问题标注(QT)预测任务}和重点{个性化先验知识(IK)预测任务}。具体来说，QT 任务通过预测问题是否包含特定的关键字来帮助学习更好的问题表征。本课题通过逐步预测隐藏在学生历史学习互动中的学生层次的先验知识，捕捉学生的全球历史表现。我们在三个真实世界的教育数据集上进行了全面的实验，并将所提出的方法与深度序列 KT 模型和非序列模型进行了比较。实验结果表明，方差{ AT-DKT }优于所有序列模型，所有数据集的 AUC 提高了0.9% 以上，与非序列模型相比几乎是第二好的。此外，我们进行了消融研究和定量分析，以显示辅助任务的有效性和优越的预测结果的方法{ AT-DKT }。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Deep+Knowledge+Tracing+with+Auxiliary+Tasks)|0|
|[Learning to Simulate Crowd Trajectories with Graph Networks](https://doi.org/10.1145/3543507.3583858)|Hongzhi Shi, Quanming Yao, Yong Li|Tsinghua University, China|Crowd stampede disasters often occur, such as recent ones in Indonesia and South Korea, and crowd simulation is particularly important to prevent and avoid such disasters. Most traditional models for crowd simulation, such as the social force model, are hand-designed formulas, which use Newtonian forces to model the interactions between pedestrians. However, such formula-based methods may not be flexible enough to capture the complex interaction patterns in diverse crowd scenarios. Recently, due to the development of the Internet, a large amount of pedestrian movement data has been collected, allowing us to study crowd simulation in a data-driven way. Inspired by the recent success of graph network-based simulation (GNS), we propose a novel method under the framework of GNS, which simulates the crowd in a data-driven way. Specifically, we propose to model the interactions among people and the environment using a heterogeneous graph. Then, we design a heterogeneous gated message-passing network to learn the interaction pattern that depends on the visual field. Finally, the randomness is introduced by modeling the context’s different influences on pedestrians with a probabilistic emission function. Extensive experiments on synthetic data, controlled-environment data and real-world data are performed. Extensive results show that our model can generally capture the three main factors which contribute to crowd trajectories while adapting to the data characteristics beyond the strong assumption of formulas-based methods. As a result, the proposed method outperforms existing methods by a large margin.|人群踩踏灾害经常发生，如最近在印度尼西亚和韩国发生的踩踏事故，人群模拟对于预防和避免这类灾害尤为重要。传统的人群模拟模型，如社会力量模型，大多是手工设计的公式，使用牛顿力来模拟行人之间的相互作用。然而，这种基于公式的方法可能不够灵活，无法在不同的人群场景中捕获复杂的交互模式。近年来，由于互联网的发展，大量的行人运动数据被收集，使我们能够以数据驱动的方式研究人群模拟。受近年来基于图形网络仿真(GNS)的成功启发，本文提出了一种在 GNS 框架下以数据驱动的方式模拟人群的新方法。具体来说，我们建议使用一个异构图来模拟人与环境之间的相互作用。然后，我们设计了一个异构的门限消息传递网络来学习依赖于视场的交互模式。最后，利用概率发射函数模拟环境对行人的不同影响，引入随机性。对合成数据、受控环境数据和真实世界数据进行了广泛的实验。大量的实验结果表明，除了基于公式的方法的强假设外，我们的模型能够在适应数据特征的同时，普遍地捕捉到影响人群轨迹的三个主要因素。因此，提出的方法比现有的方法有很大的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Simulate+Crowd+Trajectories+with+Graph+Networks)|0|
