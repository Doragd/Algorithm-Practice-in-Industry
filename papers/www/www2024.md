# WWW2024 Paper List

|论文|作者|组织|摘要|翻译|代码|引用数|
|---|---|---|---|---|---|---|
|[Unmasking the Web of Deceit: Uncovering Coordinated Activity to Expose Information Operations on Twitter](https://doi.org/10.1145/3589334.3645529)|Luca Luceri, Valeria Pantè, Keith Burghardt, Emilio Ferrara||Social media platforms, particularly Twitter, have become pivotal arenas for influence campaigns, often orchestrated by state-sponsored information operations (IOs). This paper delves into the detection of key players driving IOs by employing similarity graphs constructed from behavioral pattern data. We unveil that well-known, yet underutilized network properties can help accurately identify coordinated IO drivers. Drawing from a comprehensive dataset of 49 million tweets from six countries, which includes multiple verified IOs, our study reveals that traditional network filtering techniques do not consistently pinpoint IO drivers across campaigns. We first propose a framework based on node pruning that emerges superior, particularly when combining multiple behavioral indicators across different networks. Then, we introduce a supervised machine learning model that harnesses a vector representation of the fused similarity network. This model, which boasts a precision exceeding 0.95, adeptly classifies IO drivers on a global scale and reliably forecasts their temporal engagements. Our findings are crucial in the fight against deceptive influence campaigns on social media, helping us better understand and detect them.|社交媒体平台，尤其是 Twitter，已经成为影响力运动的关键舞台，这些运动通常由国家资助的信息运作(IOs)策划。本文利用由行为模式数据构造的相似度图，对操纵 IOs 的关键人物进行了检测。我们发现，众所周知的，但未充分利用的网络属性可以帮助准确识别协调的 IO 驱动程序。我们的研究从来自六个国家的4900万条推文的综合数据集中，其中包括多个经过验证的 IO，我们的研究显示传统的网络过滤技术并不能一致地确定不同活动的 IO 驱动因素。我们首先提出了一个基于节点剪枝的框架，它显示出优越性，特别是当跨不同网络结合多个行为指标时。然后，我们引入一个监督式学习模型，利用融合相似网络的向量表示。该模型具有超过0.95的精度，能够在全球范围内对 IO 驱动程序进行分类，并可靠地预测它们的时间参与。我们的发现对于打击社交媒体上的欺骗性影响运动至关重要，有助于我们更好地理解和发现它们。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unmasking+the+Web+of+Deceit:+Uncovering+Coordinated+Activity+to+Expose+Information+Operations+on+Twitter)|2|
|[Blockchain Censorship](https://doi.org/10.1145/3589334.3645431)|Anton Wahrstätter, Jens Ernstberger, Aviv Yaish, Liyi Zhou, Kaihua Qin, Taro Tsuchiya, Sebastian Steinhorst, Davor Svetinovic, Nicolas Christin, Mikolaj Barczentewicz, Arthur Gervais||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Blockchain+Censorship)|2|
|[Fast Graph Condensation with Structure-based Neural Tangent Kernel](https://doi.org/10.1145/3589334.3645694)|Lin Wang, Wenqi Fan, Jiatong Li, Yao Ma, Qing Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Graph+Condensation+with+Structure-based+Neural+Tangent+Kernel)|2|
|[Susceptibility to Unreliable Information Sources: Swift Adoption with Minimal Exposure](https://doi.org/10.1145/3589334.3648154)|Jinyi Ye, Luca Luceri, Julie Jiang, Emilio Ferrara||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Susceptibility+to+Unreliable+Information+Sources:+Swift+Adoption+with+Minimal+Exposure)|2|
|[Personalized Audiobook Recommendations at Spotify Through Graph Neural Networks](https://doi.org/10.1145/3589335.3648339)|Marco De Nadai, Francesco Fabbri, Paul Gigioli, Alice Wang, Ang Li, Fabrizio Silvestri, Laura Kim, Shawn Lin, Vladan Radosavljevic, Sandeep Ghael, David Nyhan, Hugues Bouchard, Mounia Lalmas, Andreas Damianou||In the ever-evolving digital audio landscape, Spotify, well-known for its music and talk content, has recently introduced audiobooks to its vast user base. While promising, this move presents significant challenges for personalized recommendations. Unlike music and podcasts, audiobooks, initially available for a fee, cannot be easily skimmed before purchase, posing higher stakes for the relevance of recommendations. Furthermore, introducing a new content type into an existing platform confronts extreme data sparsity, as most users are unfamiliar with this new content type. Lastly, recommending content to millions of users requires the model to react fast and be scalable. To address these challenges, we leverage podcast and music user preferences and introduce 2T-HGNN, a scalable recommendation system comprising Heterogeneous Graph Neural Networks (HGNNs) and a Two Tower (2T) model. This novel approach uncovers nuanced item relationships while ensuring low latency and complexity. We decouple users from the HGNN graph and propose an innovative multi-link neighbor sampler. These choices, together with the 2T component, significantly reduce the complexity of the HGNN model. Empirical evaluations involving millions of users show significant improvement in the quality of personalized recommendations, resulting in a +46 a +23 beyond audiobooks, benefiting established products like podcasts.|在不断发展的数字音频领域，以音乐和谈话内容闻名的 Spotify 最近向其庞大的用户群推出了有声读物。尽管前景看好，但这一举措对个性化推荐提出了重大挑战。与音乐和播客不同，有声书籍最初是收费的，不能在购买前轻易浏览，这对推荐的相关性构成了更大的威胁。此外，在现有平台中引入新的内容类型会面临数据极度稀缺的问题，因为大多数用户不熟悉这种新的内容类型。最后，向数百万用户推荐内容需要模型快速反应和可伸缩性。为了应对这些挑战，我们利用播客和音乐用户的偏好，引入了2T-HGNN，一个由异构图神经网络(HGNN)和双塔(2T)模型组成的可扩展推荐系统。这种新颖的方法揭示了细微的项目关系，同时确保低延迟和复杂性。我们从 HGNN 图解耦用户，并提出了一个创新的多链路邻居采样器。这些选择以及2T 组件显著降低了 HGNN 模型的复杂性。涉及数百万用户的经验性评价显示，个性化推荐的质量有了显著提高，在有声读物之外产生了 + 46 a + 23，使播客等既有产品受益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Audiobook+Recommendations+at+Spotify+Through+Graph+Neural+Networks)|1|
|[Intelligent Model Update Strategy for Sequential Recommendation](https://doi.org/10.1145/3589334.3645316)|Zheqi Lv, Wenqiao Zhang, Zhengyu Chen, Shengyu Zhang, Kun Kuang||Modern online platforms are increasingly employing recommendation systems to address information overload and improve user engagement. There is an evolving paradigm in this research field that recommendation network learning occurs both on the cloud and on edges with knowledge transfer in between (i.e., edge-cloud collaboration). Recent works push this filed further by enabling edge-specific context-aware adaptivity, where model parameters are updated in real-time based on incoming on-edge data. However, we argue that frequent data exchanges between the cloud and edges often lead to inefficiency and waste of communication/computation resources, as considerable parameter updates might be redundant. To investigate this problem, we introduce Intelligent Edge-Cloud Parameter Request Model (IntellectReq). IntellectReq is designed to operate on edge, evaluating the cost-benefit landscape of parameter requests with minimal computation and communication overhead. We formulate this as a novel learning task, aimed at the detection of out-of-distribution data, thereby fine-tuning adaptive communication strategies. Further, we employ statistical mapping techniques to convert real-time user behavior into a normal distribution, thereby employing multi-sample outputs to quantify the model's uncertainty and thus its generalization capabilities. Rigorous empirical validation on four widely-adopted benchmarks evaluates our approach, evidencing a marked improvement in the efficiency and generalizability of edge-cloud collaborative and dynamic recommendation systems.|现代在线平台越来越多地使用推荐系统来解决信息超载问题，提高用户参与度。在这个研究领域有一个不断发展的范例，即推荐网络学习既发生在云端，也发生在知识转移之间(即，边缘云协作)的边缘。最近的工作通过启用特定于边缘的上下文感知自适应性进一步推动了这一领域的发展，其中模型参数根据传入的边缘数据实时更新。然而，我们认为云和边缘之间频繁的数据交换通常会导致效率低下和通信/计算资源的浪费，因为大量的参数更新可能是多余的。为了研究这个问题，我们引入了智能边缘云参数请求模型(IntelligectReq)。IntelectReq 设计用于边缘操作，以最小的计算和通信开销评估参数请求的成本效益。我们提出了一个新的学习任务，旨在检测分布外的数据，从而微调自适应通信策略。进一步，我们使用统计映射技术将实时用户行为转换为正态分布，从而使用多样本输出量化模型的不确定性，从而其泛化能力。对四个被广泛采用的基准测试的严格的经验验证评估了我们的方法，证明了边缘云协作和动态推荐系统的效率和通用性的显著提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Intelligent+Model+Update+Strategy+for+Sequential+Recommendation)|1|
|[Top-Personalized-K Recommendation](https://doi.org/10.1145/3589334.3645417)|Wonbin Kweon, SeongKu Kang, Sanghwan Jang, Hwanjo Yu||The conventional top-K recommendation, which presents the top-K items with the highest ranking scores, is a common practice for generating personalized ranking lists. However, is this fixed-size top-K recommendation the optimal approach for every user's satisfaction? Not necessarily. We point out that providing fixed-size recommendations without taking into account user utility can be suboptimal, as it may unavoidably include irrelevant items or limit the exposure to relevant ones. To address this issue, we introduce Top-Personalized-K Recommendation, a new recommendation task aimed at generating a personalized-sized ranking list to maximize individual user satisfaction. As a solution to the proposed task, we develop a model-agnostic framework named PerK. PerK estimates the expected user utility by leveraging calibrated interaction probabilities, subsequently selecting the recommendation size that maximizes this expected utility. Through extensive experiments on real-world datasets, we demonstrate the superiority of PerK in Top-Personalized-K recommendation task. We expect that Top-Personalized-K recommendation has the potential to offer enhanced solutions for various real-world recommendation scenarios, based on its great compatibility with existing models.|传统的 top-K 推荐是生成个性化排名列表的一种常见实践，它给出排名最高的 top-K 项目。然而，这个固定大小的 top-K 推荐是否是每个用户满意度的最佳方法？不一定。我们指出，在不考虑用户效用的情况下提供固定大小的推荐可能是次优的，因为它可能不可避免地包括不相关的项目或限制相关项目的曝光。为了解决这个问题，我们引入了 Top-Personalization-K 推荐，这是一个新的推荐任务，旨在产生一个个性化大小的排名列表，以最大限度地提高个人用户的满意度。作为提出的任务的解决方案，我们开发了一个名为 PerK 的模型无关框架。PerK 通过利用校准的交互概率来估计预期的用户效用，随后选择最大化这个预期效用的推荐大小。通过对实际数据集的大量实验，验证了 PerK 在 Top-Personalization-K 推荐任务中的优越性。我们期望 Top-Personalization-K 推荐基于其与现有模型的巨大兼容性，有潜力为各种真实世界的推荐场景提供增强的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Top-Personalized-K+Recommendation)|1|
|[AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems](https://doi.org/10.1145/3589334.3645537)|Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian J. McAuley, Wayne Xin Zhao, Leyu Lin, JiRong Wen||Recently, there has been an emergence of employing LLM-powered agents as believable human proxies, based on their remarkable decision-making capability. However, existing studies mainly focus on simulating human dialogue. Human non-verbal behaviors, such as item clicking in recommender systems, although implicitly exhibiting user preferences and could enhance the modeling of users, have not been deeply explored. The main reasons lie in the gap between language modeling and behavior modeling, as well as the incomprehension of LLMs about user-item relations. To address this issue, we propose AgentCF for simulating user-item interactions in recommender systems through agent-based collaborative filtering. We creatively consider not only users but also items as agents, and develop a collaborative learning approach that optimizes both kinds of agents together. Specifically, at each time step, we first prompt the user and item agents to interact autonomously. Then, based on the disparities between the agents' decisions and real-world interaction records, user and item agents are prompted to reflect on and adjust the misleading simulations collaboratively, thereby modeling their two-sided relations. The optimized agents can also propagate their preferences to other agents in subsequent interactions, implicitly capturing the collaborative filtering idea. Overall, the optimized agents exhibit diverse interaction behaviors within our framework, including user-item, user-user, item-item, and collective interactions. The results show that these agents can demonstrate personalized behaviors akin to those of real-world individuals, sparking the development of next-generation user behavior simulation.|近年来，基于 LLM 驱动的智能体显著的决策能力，出现了一种使用 LLM 驱动的智能体作为可信的人类代理的方法。然而，现有的研究主要集中在模拟人类对话方面。人类的非语言行为，如推荐系统中的项目点击，虽然隐含地表现出用户的偏好，可以增强用户的建模，但还没有被深入探索。其主要原因在于语言建模与行为建模之间的差距，以及对用户-项目关系的 LLM 模型的不理解。为了解决这个问题，我们建议 AgentCF 通过基于代理的协同过滤来模拟推荐系统中的用户-项目交互。我们不仅创造性地考虑用户，而且还将物品作为代理，并开发一种合作学习的方法，将这两种代理结合起来优化。具体来说，在每个步骤中，我们首先提示用户和项代理自主交互。然后，基于智能体的决策与实际交互记录之间的差异，提示用户和项目智能体协同反思和调整误导性模拟，从而建立它们之间的双边关系模型。经过优化的代理还可以在随后的交互中将自己的偏好传播给其他代理，从而隐含地捕捉到协同过滤的想法。总的来说，优化的代理在我们的框架中展示了不同的交互行为，包括用户-项目、用户-用户、项目-项目和集体交互。结果表明，这些代理能够表现出类似于真实世界个体的个性化行为，从而推动了下一代用户行为仿真的发展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AgentCF:+Collaborative+Learning+with+Autonomous+Language+Agents+for+Recommender+Systems)|1|
|[Aligning Language Models for Versatile Text-based Item Retrieval](https://doi.org/10.1145/3589335.3651468)|Yuxuan Lei, Jianxun Lian, Jing Yao, Mingqi Wu, Defu Lian, Xing Xie||This paper addresses the gap between general-purpose text embeddings and the specific demands of item retrieval tasks. We demonstrate the shortcomings of existing models in capturing the nuances necessary for zero-shot performance on item retrieval tasks. To overcome these limitations, we propose generate in-domain dataset from ten tasks tailored to unlocking models' representation ability for item retrieval. Our empirical studies demonstrate that fine-tuning embedding models on the dataset leads to remarkable improvements in a variety of retrieval tasks. We also illustrate the practical application of our refined model in a conversational setting, where it enhances the capabilities of LLM-based Recommender Agents like Chat-Rec. Our code is available at https://github.com/microsoft/RecAI.|本文探讨了通用文本嵌入与项目检索任务的具体要求之间的差距。我们证明了现有模型在捕捉项目检索任务中零拍性能所必需的细微差别方面的不足。为了克服这些限制，我们提出从十个任务中生成域内数据集，以解锁项目检索模型的表示能力。我们的实证研究表明，在数据集上微调嵌入模型可以显著改善各种检索任务。我们还举例说明了我们的改进模型在会话环境中的实际应用，它增强了基于 LLM 的推荐代理(如 Chat-Rec)的功能。我们的代码可以在 https://github.com/microsoft/recai 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Aligning+Language+Models+for+Versatile+Text-based+Item+Retrieval)|1|
|[Macro Graph Neural Networks for Online Billion-Scale Recommender Systems](https://doi.org/10.1145/3589334.3645517)|Hao Chen, Yuanchen Bei, Qijie Shen, Yue Xu, Sheng Zhou, Wenbing Huang, Feiran Huang, Senzhang Wang, Xiao Huang||Predicting Click-Through Rate (CTR) in billion-scale recommender systems poses a long-standing challenge for Graph Neural Networks (GNNs) due to the overwhelming computational complexity involved in aggregating billions of neighbors. To tackle this, GNN-based CTR models usually sample hundreds of neighbors out of the billions to facilitate efficient online recommendations. However, sampling only a small portion of neighbors results in a severe sampling bias and the failure to encompass the full spectrum of user or item behavioral patterns. To address this challenge, we name the conventional user-item recommendation graph as "micro recommendation graph" and introduce a more suitable MAcro Recommendation Graph (MAG) for billion-scale recommendations. MAG resolves the computational complexity problems in the infrastructure by reducing the node count from billions to hundreds. Specifically, MAG groups micro nodes (users and items) with similar behavior patterns to form macro nodes. Subsequently, we introduce tailored Macro Graph Neural Networks (MacGNN) to aggregate information on a macro level and revise the embeddings of macro nodes. MacGNN has already served Taobao's homepage feed for two months, providing recommendations for over one billion users. Extensive offline experiments on three public benchmark datasets and an industrial dataset present that MacGNN significantly outperforms twelve CTR baselines while remaining computationally efficient. Besides, online A/B tests confirm MacGNN's superiority in billion-scale recommender systems.|在十亿规模的推荐系统中预测点进率(CTR)对于图形神经网络(GNN)来说是一个长期存在的挑战，因为聚合数十亿个邻居涉及到极其复杂的计算。为了解决这个问题，基于 GNN 的点击率模型通常从数十亿个邻居中抽样数百个，以促进有效的在线推荐。然而，只有一小部分邻居的抽样导致严重的抽样偏差和未能涵盖用户或项目的行为模式的全部范围。为了解决这个问题，我们将传统的用户项目推荐图命名为“微推荐图”，并为十亿级推荐引入了更合适的宏推荐图(MAG)。MAG 通过将节点数从几十亿减少到几百个，解决了基础设施中的计算复杂度问题。具体来说，MAG 将具有相似行为模式的微节点(用户和项)分组以形成宏节点。随后，我们引入量身定制的宏图神经网络(MacGNN) ，在宏观层面上聚集信息，并修正宏观节点的嵌入。MacGNN 已经为淘宝网服务了两个月，为超过十亿用户提供推荐。针对三个公共基准数据集和一个工业数据集进行的大量离线实验表明，MacGNN 在保持计算效率的同时，显著优于十二个点击率基准。此外，在线 A/B 测试证实了 MacGNN 在数十亿规模的推荐系统中的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Macro+Graph+Neural+Networks+for+Online+Billion-Scale+Recommender+Systems)|1|
|[Doubly Calibrated Estimator for Recommendation on Data Missing Not at Random](https://doi.org/10.1145/3589334.3645617)|Wonbin Kweon, Hwanjo Yu||Recommender systems often suffer from selection bias as users tend to rate their preferred items. The datasets collected under such conditions exhibit entries missing not at random and thus are not randomized-controlled trials representing the target population. To address this challenge, a doubly robust estimator and its enhanced variants have been proposed as they ensure unbiasedness when accurate imputed errors or predicted propensities are provided. However, we argue that existing estimators rely on miscalibrated imputed errors and propensity scores as they depend on rudimentary models for estimation. We provide theoretical insights into how miscalibrated imputation and propensity models may limit the effectiveness of doubly robust estimators and validate our theorems using real-world datasets. On this basis, we propose a Doubly Calibrated Estimator that involves the calibration of both the imputation and propensity models. To achieve this, we introduce calibration experts that consider different logit distributions across users. Moreover, we devise a tri-level joint learning framework, allowing the simultaneous optimization of calibration experts alongside prediction and imputation models. Through extensive experiments on real-world datasets, we demonstrate the superiority of the Doubly Calibrated Estimator in the context of debiased recommendation tasks.|推荐系统经常受到选择偏差的影响，因为用户倾向于对他们喜欢的项目进行评分。在这种条件下收集的数据集显示条目不是随机丢失的，因此不是代表目标人群的随机对照试验。为了应对这一挑战，提出了一种双稳健估计器及其增强变量，以确保在提供准确的估计误差或预测倾向时的无偏性。然而，我们认为，现有的估计依赖于错误校准估算误差和倾向得分，因为他们依赖于估计的基本模型。我们提供的理论见解，如何错误校准插补和倾向模型可以限制双鲁棒估计器的有效性，并验证我们的定理使用真实世界的数据集。在此基础上，我们提出了一个双校正估计，涉及校准的插补和倾向模型。为了实现这一点，我们介绍了校准专家，他们考虑了不同用户之间的 logit 分布。此外，我们设计了一个三级联合学习框架，使校准专家同时优化预测和插补模型。通过对实际数据集的大量实验，我们证明了双校正估计器在去偏推荐任务中的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Doubly+Calibrated+Estimator+for+Recommendation+on+Data+Missing+Not+at+Random)|1|
|[Item-side Fairness of Large Language Model-based Recommendation System](https://doi.org/10.1145/3589334.3648158)|Meng Jiang, Keqin Bao, Jizhi Zhang, Wenjie Wang, Zhengyi Yang, Fuli Feng, Xiangnan He||Recommendation systems for Web content distribution intricately connect to the information access and exposure opportunities for vulnerable populations. The emergence of Large Language Models-based Recommendation System (LRS) may introduce additional societal challenges to recommendation systems due to the inherent biases in Large Language Models (LLMs). From the perspective of item-side fairness, there remains a lack of comprehensive investigation into the item-side fairness of LRS given the unique characteristics of LRS compared to conventional recommendation systems. To bridge this gap, this study examines the property of LRS with respect to item-side fairness and reveals the influencing factors of both historical users' interactions and inherent semantic biases of LLMs, shedding light on the need to extend conventional item-side fairness methods for LRS. Towards this goal, we develop a concise and effective framework called IFairLRS to enhance the item-side fairness of an LRS. IFairLRS covers the main stages of building an LRS with specifically adapted strategies to calibrate the recommendations of LRS. We utilize IFairLRS to fine-tune LLaMA, a representative LLM, on MovieLens and Steam datasets, and observe significant item-side fairness improvements. The code can be found in https://github.com/JiangM-C/IFairLRS.git.|网络内容分发推荐系统与弱势群体的信息获取和接触机会错综复杂地联系在一起。基于大语言模型的推荐系统(LRS)的出现可能会给推荐系统带来额外的社会挑战，因为大语言模型(LLM)存在固有的偏差。由于推荐系统与传统推荐系统相比具有独特的特点，从项目侧公平的角度来看，对于推荐系统的项目侧公平性缺乏全面的研究。为了弥补这一差距，本研究考察了 LRS 在项目侧公平性方面的性质，揭示了历史用户交互和 LLM 固有语义偏差的影响因素，阐明了传统项目侧公平性方法在 LRS 中扩展的必要性。为了实现这一目标，我们开发了一个简洁有效的框架，称为 IFairLRS，以增强 LRS 项目的公平性。IFairLRS 涵盖了建立 LRS 的主要阶段，其中特别采用了校准 LRS 建议的策略。我们利用 IFairLRS 在 MovieLens 和 STEAM 数据集上对 LLaMA (一种代表性的 LLM)进行微调，并观察到显著的项目端公平性改进。密码可以在 https://github.com/jiangm-c/ifairlrs.git 中找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Item-side+Fairness+of+Large+Language+Model-based+Recommendation+System)|1|
|[Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems](https://doi.org/10.1145/3589335.3651905)|Anuja Tayal, Aman Tyagi||When interacting with Retrieval-Augmented Generation (RAG)-based conversational agents, the users must carefully craft their queries to be understood correctly. Yet, understanding the system's capabilities can be challenging for the users, leading to ambiguous questions that necessitate further clarification. This work aims to bridge the gap by developing a suggestion question generator. To generate suggestion questions, our approach involves utilizing dynamic context, which includes both dynamic few-shot examples and dynamically retrieved contexts. Through experiments, we show that the dynamic contexts approach can generate better suggestion questions as compared to other prompting approaches.|当与基于检索增强生成(RAG)的会话代理交互时，用户必须精心设计他们的查询才能被正确理解。然而，理解系统的功能对用户来说可能是一个挑战，导致需要进一步澄清的模棱两可的问题。本研究旨在通过开发一个建议问题产生器来弥补这一差距。为了产生建议问题，我们的方法涉及到利用动态上下文，其中包括动态少镜头示例和动态检索的上下文。通过实验，我们发现动态语境法比其他激励方法能够产生更好的建议问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Contexts+for+Generating+Suggestion+Questions+in+RAG+Based+Conversational+Systems)|1|
|[Linear-Time Graph Neural Networks for Scalable Recommendations](https://doi.org/10.1145/3589334.3645486)|Jiahao Zhang, Rui Xue, Wenqi Fan, Xin Xu, Qing Li, Jian Pei, Xiaorui Liu||In an era of information explosion, recommender systems are vital tools to deliver personalized recommendations for users. The key of recommender systems is to forecast users' future behaviors based on previous user-item interactions. Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging Graph Neural Networks (GNNs) to boost the prediction performance of recommender systems. Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale recommender systems due to their scalability advantages. Despite the existence of GNN-acceleration solutions, it remains an open question whether GNN-based recommender systems can scale as efficiently as classic MF and DNN methods. In this paper, we propose a Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender systems to achieve comparable scalability as classic MF approaches while maintaining GNNs' powerful expressiveness for superior prediction accuracy. Extensive experiments and ablation studies are presented to validate the effectiveness and scalability of the proposed algorithm. Our implementation based on PyTorch is available.|在信息爆炸的时代，推荐系统是为用户提供个性化推荐的重要工具。推荐系统的关键是根据用户之前的交互行为预测用户的未来行为。由于它们在用户交互数据中捕获高阶连接性的强大表达能力，近年来人们对利用图形神经网络(GNN)来提高推荐系统的预测性能越来越感兴趣。尽管如此，经典的矩阵分解(MF)和深度神经网络(DNN)方法仍然在现实世界的大规模推荐系统中发挥着重要作用，因为它们具有可扩展性的优势。尽管存在 GNN 加速解决方案，但是基于 GNN 的推荐系统能否像经典的 MF 和 DNN 方法那样有效地扩展仍然是一个悬而未决的问题。本文提出了一种线性时间图神经网络(LTGNN)来扩展基于 GNN 的推荐系统，以实现与经典 MF 方法相当的可扩展性，同时保持 GNN 的强大表达能力，获得更高的预测精度。通过大量的实验和烧蚀研究，验证了该算法的有效性和可扩展性。我们基于 PyTorch 的实现是可用的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Linear-Time+Graph+Neural+Networks+for+Scalable+Recommendations)|1|
|[Collaborate to Adapt: Source-Free Graph Domain Adaptation via Bi-directional Adaptation](https://doi.org/10.1145/3589334.3645507)|Zhen Zhang, Meihan Liu, Anhui Wang, Hongyang Chen, Zhao Li, Jiajun Bu, Bingsheng He||Unsupervised Graph Domain Adaptation (UGDA) has emerged as a practical solution to transfer knowledge from a label-rich source graph to a completely unlabelled target graph. However, most methods require a labelled source graph to provide supervision signals, which might not be accessible in the real-world settings due to regulations and privacy concerns. In this paper, we explore the scenario of source-free unsupervised graph domain adaptation, which tries to address the domain adaptation problem without accessing the labelled source graph. Specifically, we present a novel paradigm called GraphCTA, which performs model adaptation and graph adaptation collaboratively through a series of procedures: (1) conduct model adaptation based on node's neighborhood predictions in target graph considering both local and global information; (2) perform graph adaptation by updating graph structure and node attributes via neighborhood contrastive learning; and (3) the updated graph serves as an input to facilitate the subsequent iteration of model adaptation, thereby establishing a collaborative loop between model adaptation and graph adaptation. Comprehensive experiments are conducted on various public datasets. The experimental results demonstrate that our proposed model outperforms recent source-free baselines by large margins.|无监督图域适应(UGDA)已经成为一种实用的解决方案，用于将知识从一个标签丰富的源图转移到一个完全无标签的目标图。然而，大多数方法都需要一个带标签的源图来提供监控信号，由于法规和隐私问题，这些信号在现实环境中可能无法访问。本文探讨了无源无监督图域自适应的场景，该场景试图在不访问标记源图的情况下解决域自适应问题。具体来说，我们提出了一种新的模型适应和图形适应协同执行的范式称为 GraphCTA，通过一系列的程序: (1)进行模型适应的基础上，节点的邻域预测的目标图，考虑到局部和全局信息; (2)执行图适应更新的图结构和节点属性，通过邻域对比学习; (3)更新的图作为输入，以促进模型适应的后续迭代，从而建立一个协作环之间的模型适应和图适应。在各种公共数据集上进行了综合实验。实验结果表明，我们提出的模型优于最近的无源基线大幅度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaborate+to+Adapt:+Source-Free+Graph+Domain+Adaptation+via+Bi-directional+Adaptation)|1|
|[Assessing Web Fingerprinting Risk](https://doi.org/10.1145/3589335.3648322)|Enrico Bacis, Igor Bilogrevic, Róbert BusaFekete, Asanka Herath, Antonio Sartori, Umar Syed||Modern Web APIs allow developers to provide extensively customized experiences for website visitors, but the richness of the device information they provide also make them vulnerable to being abused to construct browser fingerprints, device-specific identifiers that enable covert tracking of users even when cookies are disabled. Previous research has established entropy, a measure of information, as the key metric for quantifying fingerprinting risk. However, earlier studies had two major limitations. First, their entropy estimates were based on either a single website or a very small sample of devices. Second, they did not adequately consider correlations among different Web APIs, potentially grossly overestimating their fingerprinting risk. We provide the first study of browser fingerprinting which addresses the limitations of prior work. Our study is based on actual visited pages and Web APIs reported by tens of millions of real Chrome browsers in-the-wild. We accounted for the dependencies and correlations among Web APIs, which is crucial for obtaining more realistic entropy estimates. We also developed a novel experimental design that accurately and efficiently estimates entropy while never observing too much information from any single user. Our results provide an understanding of the distribution of entropy for different website categories, confirm the utility of entropy as a fingerprinting proxy, and offer a method for evaluating browser enhancements which are intended to mitigate fingerprinting.|现代 Web API 允许开发人员为网站访问者提供广泛的定制体验，但是他们提供的丰富的设备信息也使他们很容易被滥用来构建浏览器指纹，即使在 Cookie 被禁用的情况下，设备特定的标识符也能够隐蔽地跟踪用户。先前的研究已经建立了熵，一种信息的度量，作为量化指纹风险的关键指标。然而，早期的研究有两个主要的局限性。首先，他们的熵估计是基于一个单一的网站或一个非常小的设备样本。其次，他们没有充分考虑不同 Web API 之间的相关性，可能严重高估了他们的指纹风险。我们提供了第一个研究的浏览器指纹，解决了以前的工作的局限性。我们的研究是基于实际访问的页面和 Web API 报告的数千万真正的 Chrome 浏览器在野外。我们考虑了 Web API 之间的依赖关系和相关性，这对于获得更真实的熵估计是至关重要的。我们还开发了一种新颖的实验设计，它能够准确有效地估计熵，同时从不观察来自任何单个用户的太多信息。我们的结果提供了一个不同网站类别的熵分布的理解，证实了熵作为指纹代理的效用，并提供了一种方法来评估浏览器增强，旨在减轻指纹。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Assessing+Web+Fingerprinting+Risk)|1|
|[Large Language Model Powered Agents in the Web](https://doi.org/10.1145/3589335.3641240)|Yang Deng, An Zhang, Yankai Lin, Xu Chen, JiRong Wen, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Model+Powered+Agents+in+the+Web)|1|
|[Automatic Design Summary Generation with Generative AI](https://doi.org/10.1145/3589335.3651901)|Daisuke Ikoma, Eisuke Aoki, Tomoki Taniguchi, Shinya Suzuki, Tomoko Ohkuma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Design+Summary+Generation+with+Generative+AI)|1|
|[Data Augmentation for Smishing Detection: A Theory-based Prompt Engineering Approach](https://doi.org/10.1145/3589335.3651903)|Ho Sung Shim, Hyoungjun Park, Kyuhan Lee, JangSun Park, Seonhye Kang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data+Augmentation+for+Smishing+Detection:+A+Theory-based+Prompt+Engineering+Approach)|1|
|[Prompt-Eng: Healthcare Prompt Engineering: Revolutionizing Healthcare Applications with Precision Prompts](https://doi.org/10.1145/3589335.3651904)|Awais Ahmed, Mengshu Hou, Rui Xi, Xiaoyang Zeng, Syed Attique Shah||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prompt-Eng:+Healthcare+Prompt+Engineering:+Revolutionizing+Healthcare+Applications+with+Precision+Prompts)|1|
|[Implementing Sustainable Urban Mobility Transitions in Positive Energy Districts](https://doi.org/10.1145/3589335.3651899)|Dirk Ahlers, Bjørn Ove Berthelsen, Tor Rune Skoglund, Kelly Riedesel||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Implementing+Sustainable+Urban+Mobility+Transitions+in+Positive+Energy+Districts)|1|
|[Contextualizing Internet Memes Across Social Media Platforms](https://doi.org/10.1145/3589335.3651970)|Saurav Joshi, Filip Ilievski, Luca Luceri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contextualizing+Internet+Memes+Across+Social+Media+Platforms)|1|
|[Faithful Temporal Question Answering over Heterogeneous Sources](https://doi.org/10.1145/3589334.3645547)|Zhen Jia, Philipp Christmann, Gerhard Weikum||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Faithful+Temporal+Question+Answering+over+Heterogeneous+Sources)|1|
|[From Shapes to Shapes: Inferring SHACL Shapes for Results of SPARQL CONSTRUCT Queries](https://doi.org/10.1145/3589334.3645550)|Philipp Seifer, Daniel Hernández, Ralf Lämmel, Steffen Staab||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Shapes+to+Shapes:+Inferring+SHACL+Shapes+for+Results+of+SPARQL+CONSTRUCT+Queries)|1|
|[A Multifaceted Look at Starlink Performance](https://doi.org/10.1145/3589334.3645328)|Nitinder Mohan, Andrew E. Ferguson, Hendrik Cech, Rohan Bose, Prakita Rayyan Renatin, Mahesh K. Marina, Jörg Ott||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Multifaceted+Look+at+Starlink+Performance)|1|
|[Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice](https://doi.org/10.1145/3589334.3645504)|Tanner Fiez, Houssam Nassif, YuCheng Chen, Sergio Gamez, Lalit Jain||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Best+of+Three+Worlds:+Adaptive+Experimentation+for+Digital+Marketing+in+Practice)|1|
|[PRINT: Personalized Relevance Incentive Network for CTR Prediction in Sponsored Search](https://doi.org/10.1145/3589335.3648316)|Zhaolin Hong, Haitao Wang, Chengjie Qian, Wei Chen, Tianqi He, Yajie Zou, Qiang Liu, Xingxing Wang||Click-Through Rate (CTR) prediction plays a critical role in sponsored search. Modeling the semantic relevance between queries and ads is one of the most crucial factors affecting the performance of CTR prediction. However, different users have different sensitivities to semantic relevance due to their personalized relevance preferences. Therefore, semantic relevance may have different incentives on the user's click probability (i.e., stimulative incentive, inhibitive incentive, or irrelevant incentive). Unfortunately, few works have studied the phenomenon, which ignores the complicated incentive effects of semantic relevance and limits the performance of CTR prediction. To this end, we propose a novel Personalized Relevance Incentive N eTwork (PRINT for short) to explicitly model the personalized incentives of query-ad semantic relevance on user's click probability. Specifically, we introduce a User Relevance Preference Module (usertask) to extract the user's personalized relevance preference from historical query-ad interacted sequence. Then, a RElevance Incentive Module (REIM) is designed to discern three incentive types and model the personalized incentive effects on CTR prediction. Experiments on public datasets and industrial datasets demonstrate the significant improvement of our PRINT. Furthermore, PRINT is also deployed in the sponsored search advertising system in Meituan, obtaining an improvement of 1.94% and 2.29% in CTR and Cost Per Mile (CPM) respectively. We publish the source code at https://anonymous.4open.science/r/PRINT-D365/.|点进率预测在赞助商搜索中扮演着重要的角色。查询与广告之间的语义相关性建模是影响 CTR 预测性能的关键因素之一。然而，不同的用户由于个性化的相关偏好，对语义相关的敏感性也不同。因此，语义相关性可能对用户的点击概率有不同的激励(即，刺激性激励，抑制性激励，或不相关的激励)。遗憾的是，很少有文献对这一现象进行研究，忽视了语义相关性的复杂激励效应，限制了 CTR 预测的效果。为此，我们提出了一种新的个性化关联激励网络(PRINT) ，以显式模型的查询广告语义相关性的个性化激励用户的点击概率。具体来说，我们引入了用户相关偏好模块(usertask) ，从历史查询-广告交互序列中提取用户的个性化相关偏好。然后，设计了相关激励模块(REIM) ，识别出三种激励类型，并建立了个性化激励效应模型。在公共数据集和工业数据集上的实验表明，我们的 PRINT 算法有了显著的改进。此外，PRINT 在赞助商搜索广告系统中的美团也有所提高，点击率和每英里成本分别提高了1.94% 和2.29% 。我们在 https://anonymous.4open.science/r/print-d365/公布源代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PRINT:+Personalized+Relevance+Incentive+Network+for+CTR+Prediction+in+Sponsored+Search)|0|
|[Dynamic Search Results Re-ranking Method by Advertisement Relevance Feedback based on Users' Unconscious Expectations for Listing Advertisement](https://doi.org/10.1145/3589335.3651495)|Da Li, Shigenaga Hamaguchi, Ruman Suyama, Shinsuke Nakajima, Yukiko Kawai||Search results hold paramount importance for both users and advertisers. However, re-ranking results based on the timing of user clicks on listing advertisements poses a considerable challenge. In this study, we introduce a dynamic re-ranking method to re-rank search results triggered by the viewing of listing advertisements containing search terms after the initial presentation of search results. The proposed method leverages relevance feedback to enhance search results, providing valuable support to users in their searches. We conducted a comparative verification of accuracy rates between methods with and without the added weight. The results suggest that considering the timings of clicking on listing ads for re-ranking results can more effectively reflect user interest compared to approaches that don't take this timing into account. This study contributes to the advancement of search result presentation strategies by incorporating dynamic user behavior considerations.|搜索结果对于用户和广告商来说都至关重要。然而，根据用户点击列表广告的时间进行重新排名的结果构成了相当大的挑战。在本研究中，我们引入一个动态重新排序的方法来重新排序搜索结果所触发的包含搜索词的列表广告在搜索结果的初始呈现之后。建议的方法利用关联反馈提升搜索结果，为用户的搜索提供有价值的支持。我们进行了比较验证的准确率之间的方法有和没有增加的权重。研究结果表明，与不考虑时间安排的方法相比，考虑点击列表广告的时间安排可以更有效地反映用户的兴趣。这项研究有助于推进搜索结果表示策略，纳入动态用户行为的考虑。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Search+Results+Re-ranking+Method+by+Advertisement+Relevance+Feedback+based+on+Users'+Unconscious+Expectations+for+Listing+Advertisement)|0|
|[Adversarial-Enhanced Causal Multi-Task Framework for Debiasing Post-Click Conversion Rate Estimation](https://doi.org/10.1145/3589334.3645379)|Xinyue Zhang, Cong Huang, Kun Zheng, Hongzu Su, Tianxu Ji, Wei Wang, Hongkai Qi, Jingjing Li||In real-world industrial scenarios, post-click conversion rate (CVR) prediction models are trained offline based on click events and subsequently applied online to both clicked and unclicked events. Unfortunately, unclicked events are inevitably difficult to estimate due to user self-selection, which leads to a degradation of CVR prediction accuracy. In order to estimate the prediction of unclicked events, the current mainstream Doubly Robust (DR) estimators introduce the concept of imputed errors. However, inaccuracies in imputed errors can increase the uncertainty in the generalization bound of CVR predictions, consequently resulting in a decline in the CVR prediction accuracy. To challenge this issue, we first present a theoretical analysis of the bias and variance inherent in DR estimators and then introduce a novel causal estimator that seeks to strike a balance between bias and variance within the DR framework, thus optimizing the learning of the imputation model in a more robust manner. Additionally, drawing inspiration from adversarial learning techniques, we propose a novel dual adversarial component, which learns from both the space level and the task level to eliminate the causal influence of input features on the CTR task (i.e., the click propensity), with the goal of achieving unbiased estimations. Our extensive experimental evaluations, conducted on both the widely used benchmark and the real-world large-scale Internet giant platform, convincingly demonstrate the effectiveness of our proposed scheme. Besides, we have released a high-quality industrial dataset named Tenc-UnionAds used for selection bias research in the advertising field.|在真实的工业场景中，点击后转换率(CVR)预测模型是基于点击事件离线训练的，随后在线应用于点击事件和未点击事件。不幸的是，由于用户的自我选择，未点击事件不可避免地难以估计，从而导致 CVR 预测精度的下降。为了估计未点击事件的预测，目前主流的双鲁棒(DR)估计器引入了估计误差的概念。然而，估算误差的不准确性会增加 CVR 预测推广范围的不确定性，从而导致 CVR 预测精度的下降。为了挑战这个问题，我们首先提出了 DR 估计量固有的偏差和方差的理论分析，然后介绍了一种新的因果估计量，试图在 DR 框架内平衡偏差和方差，从而以更健壮的方式优化插补模型的学习。此外，从对抗性学习技术中获得灵感，我们提出了一种新的双对抗性组件，它从空间水平和任务水平学习，以消除输入特征对 CTR 任务(即点击倾向)的因果影响，目标是实现无偏估计。我们在广泛使用的基准和现实世界的大规模互联网巨头平台上进行了广泛的实验评估，令人信服地证明了我们提出的方案的有效性。此外，我们还发布了一个名为 Tenc-UnionAds 的高质量工业数据集，用于广告领域的选择偏差研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adversarial-Enhanced+Causal+Multi-Task+Framework+for+Debiasing+Post-Click+Conversion+Rate+Estimation)|0|
|[Enhancing Sequential Recommendation via LLM-based Semantic Embedding Learning](https://doi.org/10.1145/3589335.3648307)|Jun Hu, Wenwen Xia, Xiaolu Zhang, Chilin Fu, Weichang Wu, Zhaoxin Huan, Ang Li, Zuoli Tang, Jun Zhou||Sequential recommendation systems (SRS) are crucial in various applications as they enable users to discover relevant items based on their past interactions. Recent advancements involving large language models (LLMs) have shown significant promise in addressing intricate recommendation challenges. However, these efforts exhibit certain limitations. Specifically, directly extracting representations from an LLM based on items' textual features and feeding them into a sequential model hold no guarantee that the semantic information of texts could be preserved in these representations. Additionally, concatenating textual descriptions of all items in an item sequence into a long text and feeding it into an LLM for recommendation results in lengthy token sequences, which largely diminishes the practical efficiency. In this paper, we introduce SAID, a framework that utilizes LLMs to explicitly learn Semantically Aligned item ID embeddings based on texts. For each item, SAID employs a projector module to transform an item ID into an embedding vector, which will be fed into an LLM to elicit the exact descriptive text tokens accompanied by the item. The item embeddings are forced to preserve fine-grained semantic information of textual descriptions. Further, the learned embeddings can be integrated with lightweight downstream sequential models for practical recommendations. In this way, SAID circumvents lengthy token sequences in previous works, reducing resources required in industrial scenarios and also achieving superior recommendation performance. Experiments on six public datasets demonstrate that SAID outperforms baselines by about 5% to 15% in terms of NDCG@10. Moreover, SAID has been deployed in Alipay's online advertising platform, achieving a 3.07% relative improvement of cost per mille (CPM) over baselines, with an online response time of under 20 milliseconds.|序列推荐系统(SRS)在各种应用程序中都是至关重要的，因为它们使用户能够根据过去的交互发现相关项目。涉及大型语言模型(LLM)的最新进展显示了在解决复杂的推荐挑战方面的巨大前景。然而，这些努力表现出一定的局限性。具体来说，根据项目的文本特征直接从 LLM 中提取表示并将其输入到一个顺序模型中，并不能保证文本的语义信息能够在这些表示中得到保留。此外，将一个项目序列中所有项目的文本描述连接到一个长文本中，并将其提供给 LLM 进行推荐，会导致冗长的标记序列，这大大降低了实际效率。在本文中，我们介绍了 SAID，一个利用 LLM 显式学习基于文本的语义对齐项 ID 嵌入的框架。对于每个条目，SAID 使用一个投影仪模块将条目 ID 转换为嵌入向量，嵌入向量将被输入到 LLM 中，以引出条目附带的精确描述性文本标记。项目嵌入被迫保留文本描述的细粒度语义信息。此外，学习嵌入可以与轻量级的下游顺序模型集成，以获得实用的推荐。通过这种方式，SAID 规避了以前工作中冗长的令牌序列，减少了工业场景中所需的资源，并且实现了更好的推荐性能。在六个公共数据集上的实验表明，按照 NDCG@10计算，SAID 的性能比基线高出约5% 至15% 。此外，SAID 已经部署在支付宝的在线广告平台上，实现了每公里成本(CPM)相对基线提高3.07% ，在线响应时间低于20毫秒。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Sequential+Recommendation+via+LLM-based+Semantic+Embedding+Learning)|0|
|[OmniSearchSage: Multi-Task Multi-Entity Embeddings for Pinterest Search](https://doi.org/10.1145/3589335.3648309)|Prabhat Agarwal, Minhazul Islam SK, Nikil Pancha, Kurchi Subhra Hazra, Jiajing Xu, Chuck Rosenberg||In this paper, we present OmniSearchSage, a versatile and scalable system for understanding search queries, pins, and products for Pinterest search. We jointly learn a unified query embedding coupled with pin and product embeddings, leading to an improvement of >8% relevance, >7% engagement, and >5% ads CTR in Pinterest's production search system. The main contributors to these gains are improved content understanding, better multi-task learning, and real-time serving. We enrich our entity representations using diverse text derived from image captions from a generative LLM, historical engagement, and user-curated boards. Our multitask learning setup produces a single search query embedding in the same space as pin and product embeddings and compatible with pre-existing pin and product embeddings. We show the value of each feature through ablation studies, and show the effectiveness of a unified model compared to standalone counterparts. Finally, we share how these embeddings have been deployed across the Pinterest search stack, from retrieval to ranking, scaling to serve 300k requests per second at low latency. Our implementation of this work is available at https://github.com/pinterest/atg-research/tree/main/omnisearchsage.|在本文中，我们介绍 OmniSearchSage，这是一个通用的、可扩展的系统，用于理解 Pinterest 搜索的查询、引脚和产品。我们共同学习了一个统一的查询嵌入结合引脚和产品嵌入，导致在 Pinterest 的产品搜索系统中相关性提高 > 8% ，参与度提高 > 7% ，广告点击率提高 > 5% 。这些收获的主要贡献者是改进的内容理解、更好的多任务学习和实时服务。我们丰富了我们的实体表示使用不同的文字衍生自一个生成 LLM 的图像标题，历史参与和用户策划的董事会。我们的多任务学习设置产生一个单一的搜索查询嵌入在同样的空间作为引脚和产品嵌入和兼容预先存在的引脚和产品嵌入。我们通过烧蚀研究显示了每个特征的价值，并且与独立的对应物相比，显示了统一模型的有效性。最后，我们将分享如何在 Pinterest 搜索堆栈中部署这些嵌入，从检索到排名，扩展到以低延迟每秒30万个请求。我们的这项工作可以在 https://github.com/pinterest/atg-research/tree/main/omnisearchsage 得到落实。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OmniSearchSage:+Multi-Task+Multi-Entity+Embeddings+for+Pinterest+Search)|0|
|[ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation](https://doi.org/10.1145/3589334.3645467)|Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, Weinan Zhang||With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data quality of testing samples, which greatly reduces the difficulty for LLMs to extract the essential knowledge from user behavior sequences. As for few-shot recommendation, we further design retrieval-enhanced instruction tuning (ReiT) by adopting SUBR as a data augmentation technique for training samples. Specifically, we develop a mixed training dataset consisting of both the original data samples and their retrieval-enhanced counterparts. We conduct extensive experiments on three real-world public datasets to demonstrate the superiority of ReLLa compared with existing baseline models, as well as its capability for lifelong sequential behavior comprehension. To be highlighted, with only less than 10 traditional CTR models that are trained on the entire training set (e.g., DCNv2, DIN, SIM). The code is available <https://github.com/LaVieEnRose365/ReLLa>.|随着大型语言模型(LLM)在自然语言处理(NLP)领域取得显著突破，LLM 增强型推荐系统受到了广泛关注，目前正在积极探索之中。在本文中，我们着重于适应和授权一个纯大型语言模型来完成零镜头和少镜头的推荐任务。首先，我们确定并制定了推荐域中 LLM 的终身序列行为不理解问题，即 LLM 不能从长用户行为序列的文本上下文中提取有用的信息，即使上下文的长度远未达到 LLM 的上下文限制。为了解决这一问题，提高 LLM 的推荐性能，我们提出了一种新的框架，即检索增强的大语言模型(ReLLa) ，用于在零镜头和少镜头情况下的推荐任务。对于零镜头推荐，我们通过语义用户行为检索(SUBR)来提高测试样本的数据质量，大大降低了 LLM 从用户行为序列中提取基本知识的难度。对于少镜头推荐，我们采用 SUBR 作为训练样本的数据增强技术，进一步设计了检索增强型指令调优(ReiT)。具体来说，我们开发了一个混合训练数据集，包括原始数据样本和它们的检索增强对应物。我们在三个真实世界的公共数据集上进行了广泛的实验，以证明与现有的基线模型相比，ReLLa 的优越性，以及它终身连续行为理解的能力。要突出显示的是，只有少于10个传统的 CTR 模型是在整个训练集(例如，DCNv2，DIN，SIM)上训练的。代码可在 <  https://github.com/lavieenrose365/rella。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReLLa:+Retrieval-enhanced+Large+Language+Models+for+Lifelong+Sequential+Behavior+Comprehension+in+Recommendation)|0|
|[Large Language Model based Long-tail Query Rewriting in Taobao Search](https://doi.org/10.1145/3589335.3648298)|Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Derong Xu, Tong Xu, Enhong Chen||In the realm of e-commerce search, the significance of semantic matching cannot be overstated, as it directly impacts both user experience and company revenue. Along this line, query rewriting, serving as an important technique to bridge the semantic gaps inherent in the semantic matching process, has attached wide attention from the industry and academia. However, existing query rewriting methods often struggle to effectively optimize long-tail queries and alleviate the phenomenon of "few-recall" caused by semantic gap. In this paper, we present BEQUE, a comprehensive framework that Bridges the sEmantic gap for long-tail QUEries. In detail, BEQUE comprises three stages: multi-instruction supervised fine tuning (SFT), offline feedback, and objective alignment. We first construct a rewriting dataset based on rejection sampling and auxiliary tasks mixing to fine-tune our large language model (LLM) in a supervised fashion. Subsequently, with the well-trained LLM, we employ beam search to generate multiple candidate rewrites, and feed them into Taobao offline system to obtain the partial order. Leveraging the partial order of rewrites, we introduce a contrastive learning method to highlight the distinctions between rewrites, and align the model with the Taobao online objectives. Offline experiments prove the effectiveness of our method in bridging semantic gap. Online A/B tests reveal that our method can significantly boost gross merchandise volume (GMV), number of transaction (#Trans) and unique visitor (UV) for long-tail queries. BEQUE has been deployed on Taobao, one of most popular online shopping platforms in China, since October 2023.|在电子商务搜索领域，语义匹配的重要性怎么强调都不为过，因为它直接影响用户体验和公司收入。沿着这条路线，查询重写作为弥合语义匹配过程中固有的语义差异的一项重要技术，受到了业界和学术界的广泛关注。然而，现有的查询重写方法往往难以有效地优化长尾查询，缓解由于语义差异造成的“少召回”现象。在本文中，我们提出了 BEQUE，一个综合框架，桥梁的语义差距为长尾查询。具体而言，BEQUE 包括三个阶段: 多指令监督微调(SFT)、离线反馈和目标校准。我们首先构建一个基于拒绝采样和辅助任务混合的重写数据集，以监督的方式对大型语言模型(LLM)进行微调。然后，利用训练有素的 LLM，通过波束搜索生成多个候选重写，并将其输入到淘宝离线系统中，获得部分序列。利用重写的部分顺序，我们引入了一种对比学习方法来突出重写之间的区别，并使模型与淘宝在线目标相一致。离线实验证明了该方法在消除语义鸿沟方面的有效性。在线 A/B 测试表明，我们的方法可以显着提高总商品数量(GMV) ，交易数量(# Trans)和独立访问者(UV)的长尾查询。自2023年10月以来，BEQUE 已经部署在淘宝上，淘宝是中国最受欢迎的在线购物平台之一。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Model+based+Long-tail+Query+Rewriting+in+Taobao+Search)|0|
|[An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce](https://doi.org/10.1145/3589335.3648318)|Nurendra Choudhary, Edward W. Huang, Karthik Subbian, Chandan K. Reddy||The problem of search relevance in the E-commerce domain is a challenging one since it involves understanding the intent of a user's short nuanced query and matching it with the appropriate products in the catalog. This problem has traditionally been addressed using language models (LMs) and graph neural networks (GNNs) to capture semantic and inter-product behavior signals, respectively. However, the rapid development of new architectures has created a gap between research and the practical adoption of these techniques. Evaluating the generalizability of these models for deployment requires extensive experimentation on complex, real-world datasets, which can be non-trivial and expensive. Furthermore, such models often operate on latent space representations that are incomprehensible to humans, making it difficult to evaluate and compare the effectiveness of different models. This lack of interpretability hinders the development and adoption of new techniques in the field. To bridge this gap, we propose Plug and Play Graph LAnguage Model (PP-GLAM), an explainable ensemble of plug and play models. Our approach uses a modular framework with uniform data processing pipelines. It employs additive explanation metrics to independently decide whether to include (i) language model candidates, (ii) GNN model candidates, and (iii) inter-product behavioral signals. For the task of search relevance, we show that PP-GLAM outperforms several state-of-the-art baselines as well as a proprietary model on real-world multilingual, multi-regional e-commerce datasets. To promote better model comprehensibility and adoption, we also provide an analysis of the explainability and computational complexity of our model. We also provide the public codebase and provide a deployment strategy for practical implementation.|电子商务领域中的搜索相关性问题是一个具有挑战性的问题，因为它涉及到理解用户的短细微差别查询的意图，并将其与目录中适当的产品进行匹配。传统上，这个问题被分别用语言模型(LMs)和图神经网络(GNN)来捕获语义信号和产品间行为信号。然而，新体系结构的快速发展在这些技术的研究和实际应用之间产生了差距。评估这些部署模型的通用性需要在复杂的、真实世界的数据集上进行大量的实验，这些实验可能是非常重要和昂贵的。此外，这些模型经常运行在人类无法理解的潜在空间表征上，使得难以评估和比较不同模型的有效性。这种缺乏可解释性的情况妨碍了这一领域新技术的开发和采用。为了弥补这一差距，我们提出了即插即用图语言模型(PP-GLAM) ，这是一个可解释的即插即用模型集合。我们的方法使用具有统一数据处理管道的模块化框架。它使用加性解释度量来独立决定是否包括(i)语言模型候选者，(ii) GNN 模型候选者，和(iii)产品间行为信号。对于搜索相关性的任务，我们表明，PP-GLAM 优于几个最先进的基线，以及在现实世界中的多语言，多地区电子商务数据集的专有模型。为了促进更好的模型可理解性和采用性，我们还对模型的可解释性和计算复杂性进行了分析。我们还提供了公共代码库，并为实际实现提供了部署策略。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Interpretable+Ensemble+of+Graph+and+Language+Models+for+Improving+Search+Relevance+in+E-Commerce)|0|
|[Hierarchical Query Classification in E-commerce Search](https://doi.org/10.1145/3589335.3648332)|Bing He, Sreyashi Nag, Limeng Cui, Suhang Wang, Zheng Li, Rahul Goutam, Zhen Li, Haiyang Zhang||E-commerce platforms typically store and structure product information and search data in a hierarchy. Efficiently categorizing user search queries into a similar hierarchical structure is paramount in enhancing user experience on e-commerce platforms as well as news curation and academic research. The significance of this task is amplified when dealing with sensitive query categorization or critical information dissemination, where inaccuracies can lead to considerable negative impacts. The inherent complexity of hierarchical query classification is compounded by two primary challenges: (1) the pronounced class imbalance that skews towards dominant categories, and (2) the inherent brevity and ambiguity of search queries that hinder accurate classification. To address these challenges, we introduce a novel framework that leverages hierarchical information through (i) enhanced representation learning that utilizes the contrastive loss to discern fine-grained instance relationships within the hierarchy, called ”instance hierarchy”, and (ii) a nuanced hierarchical classification loss that attends to the intrinsic label taxonomy, named ”label hierarchy”. Additionally, based on our observation that certain unlabeled queries share typographical similarities with labeled queries, we propose a neighborhood-aware sampling technique to intelligently select these unlabeled queries to boost the classification performance. Extensive experiments demonstrate that our proposed method is better than state-of-the-art (SOTA) on the proprietary Amazon dataset, and comparable to SOTA on the public datasets of Web of Science and RCV1-V2. These results underscore the efficacy of our proposed solution, and pave the path toward the next generation of hierarchy-aware query classification systems.|电子商务平台通常在一个层次结构中存储和构造产品信息和搜索数据。有效地将用户搜索查询分类到一个类似的层次结构中，对于提高电子商务平台上的用户体验以及新闻策划和学术研究都是至关重要的。在处理敏感的查询分类或关键信息传播时，这项任务的重要性被放大，因为不准确可能导致相当大的负面影响。层次化查询分类的内在复杂性由两个主要挑战所复杂化: (1)明显的类不平衡倾向于主导类别，(2)搜索查询的内在简洁性和模糊性阻碍了准确分类。为了解决这些挑战，我们引入了一个新的框架，通过(i)增强的表示学习利用层次结构中的细粒度实例关系，称为“实例层次结构”，以及(ii)一个细微的层次分类损失，涉及内在的标签分类法，称为“标签层次结构”。此外，基于我们观察到的某些未标记查询与标记查询在字体上有相似之处，我们提出了一种邻域感知抽样技术来智能地选择这些未标记查询以提高分类性能。大量的实验表明，我们提出的方法在专有 Amazon 数据集上优于最先进的 SOTA (State-of-art) ，在公共数据集上与 SOTA (Web of Science)和 RCV1-V2相当。这些结果强调了我们提出的解决方案的有效性，并为下一代感知层次的查询分类系统铺平了道路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Query+Classification+in+E-commerce+Search)|0|
|[Detecting Generated Native Ads in Conversational Search](https://doi.org/10.1145/3589335.3651489)|Sebastian Schmidt, Ines Zelch, Janek Bevendorff, Benno Stein, Matthias Hagen, Martin Potthast||Conversational search engines such as YouChat and Microsoft Copilot use large language models (LLMs) to generate answers to queries. It is only a small step to also use this technology to generate and integrate advertising within these answers - instead of placing ads separately from the organic search results. This type of advertising is reminiscent of native advertising and product placement, both of which are very effective forms of subtle and manipulative advertising. It is likely that information seekers will be confronted with such use of LLM technology in the near future, especially when considering the high computational costs associated with LLMs, for which providers need to develop sustainable business models. This paper investigates whether LLMs can also be used as a countermeasure against generated native ads, i.e., to block them. For this purpose we compile a large dataset of ad-prone queries and of generated answers with automatically integrated ads to experiment with fine-tuned sentence transformers and state-of-the-art LLMs on the task of recognizing the ads. In our experiments sentence transformers achieve detection precision and recall values above 0.9, while the investigated LLMs struggle with the task.|像 YouChat 和 Microsoft Copilot 这样的会话搜索引擎使用大型语言模型(LLM)来生成查询的答案。使用这种技术在这些答案中生成和整合广告，而不是将广告与有机搜索结果分开放置，这只是一小步。这种类型的广告让人想起原生广告和植入性营销，它们都是非常有效的微妙和操纵性的广告形式。在不久的将来，信息寻求者可能会遇到这种使用 LLM 技术的情况，特别是考虑到与 LLM 相关的高计算成本，供应商需要为此开发可持续的商业模式。本文研究 LLM 是否也可以作为一种对抗生成本地广告的对策，即屏蔽本地广告。为此，我们编译了一个大型的广告倾向查询数据集和自动整合广告生成的答案数据集，以便在识别广告的任务中使用微调的句子转换器和最先进的 LLM 进行实验。在我们的实验中，句子转换器的检测准确率召回率大于0.9，而被调查的 LLM 则在努力完成这项任务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Generated+Native+Ads+in+Conversational+Search)|0|
|[Recall-Augmented Ranking: Enhancing Click-Through Rate Prediction Accuracy with Cross-Stage Data](https://doi.org/10.1145/3589335.3651551)|Junjie Huang, Guohao Cai, Jieming Zhu, Zhenhua Dong, Ruiming Tang, Weinan Zhang, Yong Yu||Click-through rate (CTR) prediction plays an indispensable role in online platforms. Numerous models have been proposed to capture users' shifting preferences by leveraging user behavior sequences. However, these historical sequences often suffer from severe homogeneity and scarcity compared to the extensive item pool. Relying solely on such sequences for user representations is inherently restrictive, as user interests extend beyond the scope of items they have previously engaged with. To address this challenge, we propose a data-driven approach to enrich user representations. We recognize user profiling and recall items as two ideal data sources within the cross-stage framework, encompassing the u2u (user-to-user) and i2i (item-to-item) aspects respectively. In this paper, we propose a novel architecture named Recall-Augmented Ranking (RAR). RAR consists of two key sub-modules, which synergistically gather information from a vast pool of look-alike users and recall items, resulting in enriched user representations. Notably, RAR is orthogonal to many existing CTR models, allowing for consistent performance improvements in a plug-and-play manner. Extensive experiments are conducted, which verify the efficacy and compatibility of RAR against the SOTA methods.|点进率预测在网络平台上扮演着不可或缺的角色。已经有很多模型被提出来通过利用用户行为序列来捕捉用户不断变化的偏好。然而，这些历史序列往往遭受严重的同质性和稀缺性相比，广泛的项目池。仅仅依靠这些序列来表示用户本身就是限制性的，因为用户的兴趣已经超出了他们以前参与的项目的范围。为了解决这个问题，我们提出了一种数据驱动的方法来丰富用户表示。我们将用户分析和召回项目视为跨阶段框架内的两个理想数据源，分别包括 u2u (用户对用户)和 i2i (项目对项目)方面。在本文中，我们提出了一种新的体系结构，称为召回增强排序(RAR)。RAR 由两个关键的子模块组成，它们协同地从大量外观相似的用户和召回项目中收集信息，从而丰富用户表示。值得注意的是，RAR 与许多现有的 CTR 模型是正交的，允许以即插即用的方式提高性能。进行了大量的实验，验证了 RAR 方法对 SOTA 方法的有效性和相容性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recall-Augmented+Ranking:+Enhancing+Click-Through+Rate+Prediction+Accuracy+with+Cross-Stage+Data)|0|
|[RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction](https://doi.org/10.1145/3589335.3651550)|Yushen Li, Jinpeng Wang, Tao Dai, Jieming Zhu, Jun Yuan, Rui Zhang, ShuTao Xia||Predicting click-through rates (CTR) is a fundamental task for Web applications, where a key issue is to devise effective models for feature interactions. Current methodologies predominantly concentrate on modeling feature interactions within an individual sample, while overlooking the potential cross-sample relationships that can serve as a reference context to enhance the prediction. To make up for such deficiency, this paper develops a Retrieval-Augmented Transformer (RAT), aiming to acquire fine-grained feature interactions within and across samples. By retrieving similar samples, we construct augmented input for each target sample. We then build Transformer layers with cascaded attention to capture both intra- and cross-sample feature interactions, facilitating comprehensive reasoning for improved CTR prediction while retaining efficiency. Extensive experiments on real-world datasets substantiate the effectiveness of RAT and suggest its advantage in long-tail scenarios. The code has been open-sourced at <https://github.com/YushenLi807/WWW24-RAT>.|预测点击率(CTR)是 Web 应用程序的基本任务，其中一个关键问题是为特性交互设计有效的模型。目前的方法主要集中在建模个体样本内的特征相互作用，而忽视了可以作为参考背景的潜在跨样本关系，以增强预测。为了弥补这一不足，本文开发了一种检索增强变压器(RAT) ，旨在获取样本内部和样本间的细粒度特征交互。通过检索相似的样本，我们为每个目标样本构造增强的输入。然后，我们建立级联注意力的变压器层，以捕获内部和交叉样本的特征相互作用，促进综合推理改善 CTR 预测，同时保留效率。对真实世界数据集的大量实验证实了 RAT 的有效性，并提出了它在长尾场景中的优势。该代码已在 <  https://github.com/yushenli807/www24-rat 开放源码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RAT:+Retrieval-Augmented+Transformer+for+Click-Through+Rate+Prediction)|0|
|[Personalized Ordering of Recommendation-Modules on an E-Commerce Homepage](https://doi.org/10.1145/3589335.3651545)|Haggai Roitman, Alexander Nus, Yotam Eshel||The homepage of an E-Commerce website may accommodate multiple and diverse recommendation modules; with each module is designed to cover some facet of the user's needs. Commonly, the recommendation modules are ordered in the same way for all homepage users, which leads to a sub-optimal user experience. In this work, we present a novel personalized module ordering solution that provides a more educated way to determine an ordering of the homepage modules based on historical user-interactions. Overall, we evaluate our solution and demonstrate its merits.|电子商务网站的主页可以容纳多个不同的推荐模块; 每个模块的设计是为了满足用户的某些方面的需求。通常，推荐模块对所有主页用户的排序方式都是相同的，这导致了次优的用户体验。在这项工作中，我们提出了一个新颖的个性化模块订购解决方案，提供了一个更好的方法来确定订购的主页模块的历史用户交互的基础上。总的来说，我们评估了我们的解决方案并展示了它的优点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Ordering+of+Recommendation-Modules+on+an+E-Commerce+Homepage)|0|
|[Retrieval-augmented Query Reformulation for Heterogeneous Research Asset Retrieval in Virtual Research Environment](https://doi.org/10.1145/3589335.3651553)|Peide Zhu, Na Li, Zhiming Zhao||Discovering and reusing research assets such as datasets and computational notebooks is crucial for building research workflows in data-centric studies. The rapid growth of research assets in scientific communities provides scientists with great opportunities to enhance research efficacy but also poses significant challenges in finding suitable materials for specific tasks. Scientists, especially those focusing on cross-disciplinary research, often find it difficult to formulate effective queries to retrieve desired resources. Previous work has proposed query reformulation methods to increase the efficiency of research asset search. However, it relies on existent knowledge graphs and is constrained to computational notebooks only. As research assets utilized by data analytic workflows are in essence heterogeneous, i.e., of distinct kinds and from diversified sources, query reformulation methods in this regard should consider the relationship between different types of research assets. To address the above challenges, we propose a retrieval-augmented query reformulation method for heterogeneous research asset retrieval. It is developed in the context of a Notebook-based virtual research environment (VRE) and offers query reformulation services to other VRE components. We demonstrate the effectiveness of the proposed query reformulation service with experiments on dataset and notebook retrieval. Up till now, we have indexed 8,954 datasets and 18,158 notebooks. The experimental results show that the proposed service can create useful query suggestions.|在以数据为中心的研究中，发现和重用数据集和计算笔记本等研究资产对于建立研究工作流程至关重要。科学界研究资产的迅速增长为科学家提供了提高研究效率的巨大机会，但也对为具体任务寻找合适材料提出了重大挑战。科学家，尤其是那些专注于跨学科研究的科学家，往往发现很难制定有效的查询来检索所需的资源。前人的工作已经提出了查询重构方法，以提高研究资产搜索的效率。但是，它依赖于现有的知识图，并且仅限于计算笔记本。由于数据分析工作流所使用的研究资产本质上是异构的，即来自不同种类和不同来源的研究资产，因此在这方面的查询重构方法应考虑不同类型研究资产之间的关系。针对上述挑战，我们提出了一种异构研究资产检索的检索增强查询重构方法。它是在基于笔记本的虚拟研究环境(VRE)的背景下开发的，并为其他 VRE 组件提供查询重构服务。通过对数据集和笔记本检索的实验，验证了该查询重构服务的有效性。到目前为止，我们已经索引了8,954个数据集和18,158个笔记本电脑。实验结果表明，该服务能够创建有用的查询建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retrieval-augmented+Query+Reformulation+for+Heterogeneous+Research+Asset+Retrieval+in+Virtual+Research+Environment)|0|
|[List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation](https://doi.org/10.1145/3589334.3645336)|Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, Xueqi Cheng||The results of information retrieval (IR) are usually presented in the form of a ranked list of candidate documents, such as web search for humans and retrieval-augmented generation for large language models (LLMs). List-aware retrieval aims to capture the list-level contextual features to return a better list, mainly including reranking and truncation. Reranking finely re-scores the documents in the list. Truncation dynamically determines the cut-off point of the ranked list to achieve the trade-off between overall relevance and avoiding misinformation from irrelevant documents. Previous studies treat them as two separate tasks and model them separately. However, the separation is not optimal. First, it is hard to share the contextual information of the ranking list between the two tasks. Second, the separate pipeline usually meets the error accumulation problem, where the small error from the reranking stage can largely affect the truncation stage. To solve these problems, we propose a Reranking-Truncation joint model (GenRT) that can perform the two tasks concurrently. GenRT integrates reranking and truncation via generative paradigm based on encoder-decoder architecture. We also design the novel loss functions for joint optimization to make the model learn both tasks. Sharing parameters by the joint model is conducive to making full use of the common modeling information of the two tasks. Besides, the two tasks are performed concurrently and co-optimized to solve the error accumulation problem between separate stages. Experiments on public learning-to-rank benchmarks and open-domain Q&A tasks show that our method achieves SOTA performance on both reranking and truncation tasks for web search and retrieval-augmented LLMs.|信息检索检索(IR)的结果通常以候选文档的排序列表的形式呈现，例如人类的网络搜索和大型语言模型的检索增强生成(LLM)。列表感知检索旨在捕获列表级上下文特征以返回更好的列表，主要包括重新排序和截断。对列表中的文档进行精细的重新排序。截断动态确定排序列表的截止点，以实现总体相关性和避免来自不相关文档的错误信息之间的权衡。以往的研究将它们视为两个独立的任务，并分别建立模型。然而，这种分离并不理想。首先，很难在两个任务之间共享排名列表的上下文信息。其次，分离的流水线通常会遇到误差累积问题，重新排序阶段的小误差会对截断阶段产生很大的影响。为了解决这些问题，我们提出了一个可以同时执行这两个任务的重新排序-截断联合模型(GenRT)。GenRT 通过基于编解码体系结构的生成范式集成了重新排序和截断。设计了新的联合优化损失函数，使模型能够同时学习两个任务。通过联合模型共享参数有利于充分利用两个任务的共同建模信息。此外，为了解决不同阶段之间的误差累积问题，两个任务同时进行，并进行了协同优化。通过对公共学习排序基准测试和开放域问答任务的实验表明，该方法在网络搜索和检索增强 LLM 的重排序任务和截断任务上都达到了 SOTA 性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=List-aware+Reranking-Truncation+Joint+Model+for+Search+and+Retrieval-augmented+Generation)|0|
|[Query in Your Tongue: Reinforce Large Language Models with Retrievers for Cross-lingual Search Generative Experience](https://doi.org/10.1145/3589334.3645701)|Ping Guo, Yue Hu, Yanan Cao, Yubing Ren, Yunpeng Li, Heyan Huang||In the contemporary digital landscape, search engines play an invaluable role in information access, yet they often face challenges in Cross-Lingual Information Retrieval (CLIR). Though attempts are made to improve CLIR, current methods still leave users grappling with issues such as misplaced named entities and lost cultural context when querying in non-native languages. While some advances have been made using Neural Machine Translation models and cross-lingual representation, these are not without limitations. Enter the paradigm shift brought about by Large Language Models (LLMs), which have transformed search engines from simple retrievers to generators of contextually relevant information. This paper introduces the Multilingual Information Model for Intelligent Retrieval (MIMIR). Built on the power of LLMs, MIMIR directly responds in the language of the user's query, reducing the need for post-search translations. Our model's architecture encompasses a dual-module system: a retriever for searching multilingual documents and a responder for crafting answers in the user's desired language. Through a unique unified training framework, with the retriever serving as a reward model supervising the responder, and in turn, the responder producing synthetic data to refine the retriever's proficiency, MIMIR's retriever and responder iteratively enhance each other. Performance evaluations via CLEF and MKQA benchmarks reveal MIMIR's superiority over existing models, effectively addressing traditional CLIR challenges.|在当今的数字世界中，搜索引擎在信息获取方面扮演着非常重要的角色，但它们在 Cross-Lingual 信息检索(CLIR)中经常面临挑战。尽管已经尝试改进 CLIR，但是目前的方法仍然让用户在使用非母语进行查询时遇到一些问题，例如命名实体放错地方和文化上下文丢失。虽然神经机器翻译模型和跨语言表示已经取得了一些进展，但这些并非没有局限性。进入大语言模型(LLM)带来的范式转变，它已经将搜索引擎从简单的检索器转变为上下文相关信息的生成器。本文介绍了智能检索的多语种信息模型(MIMIR)。基于 LLM 的强大功能，MIMIR 直接用用户的查询语言进行回复，减少了对搜索后翻译的需求。我们的模型的体系结构包含一个双模块系统: 一个用于搜索多语言文档的检索器和一个用于以用户所需语言编写答案的响应器。通过一个独特的统一训练框架，猎犬作为奖励模型监督应答者，反过来，应答者产生合成数据来提高猎犬的熟练程度，MIMIR 的猎犬和应答者反复地相互增强。通过 CLEF 和 MKQA 基准进行的绩效评估揭示了 MIMIR 相对于现有模式的优越性，有效地应对了传统的 CLIR 挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query+in+Your+Tongue:+Reinforce+Large+Language+Models+with+Retrievers+for+Cross-lingual+Search+Generative+Experience)|0|
|[UnifiedSSR: A Unified Framework of Sequential Search and Recommendation](https://doi.org/10.1145/3589334.3645427)|Jiayi Xie, Shang Liu, Gao Cong, Zhenzhong Chen||In this work, we propose a Unified framework of Sequential Search and Recommendation (UnifiedSSR) for joint learning of user behavior history in both search and recommendation scenarios. Specifically, we consider user-interacted products in the recommendation scenario, user-interacted products and user-issued queries in the search scenario as three distinct types of user behaviors. We propose a dual-branch network to encode the pair of interacted product history and issued query history in the search scenario in parallel. This allows for cross-scenario modeling by deactivating the query branch for the recommendation scenario. Through the parameter sharing between dual branches, as well as between product branches in two scenarios, we incorporate cross-view and cross-scenario associations of user behaviors, providing a comprehensive understanding of user behavior patterns. To further enhance user behavior modeling by capturing the underlying dynamic intent, an Intent-oriented Session Modeling module is designed for inferring intent-oriented semantic sessions from the contextual information in behavior sequences. In particular, we consider self-supervised learning signals from two perspectives for intent-oriented semantic session locating, which encourage session discrimination within each behavior sequence and session alignment between dual behavior sequences. Extensive experiments on three public datasets demonstrate that UnifiedSSR consistently outperforms state-of-the-art methods for both search and recommendation.|在这项工作中，我们提出了一个统一的线性搜索和推荐框架(UnifiedSSR) ，用于在搜索和推荐场景中联合学习用户行为历史。具体来说，我们将推荐场景中的用户交互产品、搜索场景中的用户交互产品和用户发布的查询视为三种不同类型的用户行为。我们提出了一个双分支网络，在搜索场景中并行编码交互的产品历史记录和发布的查询历史记录。这允许通过停用推荐场景的查询分支来进行跨场景建模。通过双分支之间的参数共享，以及两个场景中产品分支之间的参数共享，我们整合了用户行为的跨视图和跨场景关联，提供了对用户行为模式的全面理解。为了通过捕获潜在的动态意图进一步增强用户行为建模，设计了一个面向意图的会话建模模块，用于从行为序列中的上下文信息推断面向意图的语义会话。特别地，我们从两个角度考虑了自我监督学习信号的意图导向语义会话定位，它鼓励在每个行为序列中的会话区分和双行为序列之间的会话对齐。在三个公共数据集上的大量实验表明，UnifiedSSR 在搜索和推荐方面始终优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UnifiedSSR:+A+Unified+Framework+of+Sequential+Search+and+Recommendation)|0|
|[Scaling User Modeling: Large-scale Online User Representations for Ads Personalization in Meta](https://doi.org/10.1145/3589335.3648301)|Wei Zhang, Dai Li, Chen Liang, Fang Zhou, Zhongke Zhang, Xuewei Wang, Ru Li, Yi Zhou, Yaning Huang, Dong Liang, Kai Wang, Zhangyuan Wang, Zhengxing Chen, Fenggang Wu, Minghai Chen, Huayu Li, Yunnan Wu, Zhan Shu, Mindi Yuan, Sri Reddy||Effective user representations are pivotal in personalized advertising. However, stringent constraints on training throughput, serving latency, and memory, often limit the complexity and input feature set of online ads ranking models. This challenge is magnified in extensive systems like Meta's, which encompass hundreds of models with diverse specifications, rendering the tailoring of user representation learning for each model impractical. To address these challenges, we present Scaling User Modeling (SUM), a framework widely deployed in Meta's ads ranking system, designed to facilitate efficient and scalable sharing of online user representation across hundreds of ads models. SUM leverages a few designated upstream user models to synthesize user embeddings from massive amounts of user features with advanced modeling techniques. These embeddings then serve as inputs to downstream online ads ranking models, promoting efficient representation sharing. To adapt to the dynamic nature of user features and ensure embedding freshness, we designed SUM Online Asynchronous Platform (SOAP), a latency free online serving system complemented with model freshness and embedding stabilization, which enables frequent user model updates and online inference of user embeddings upon each user request. We share our hands-on deployment experiences for the SUM framework and validate its superiority through comprehensive experiments. To date, SUM has been launched to hundreds of ads ranking models in Meta, processing hundreds of billions of user requests daily, yielding significant online metric gains and infrastructure cost savings.|有效的用户表示是个性化广告的关键。然而，严格限制训练吞吐量、服务延迟和内存，往往限制了在线广告排名模型的复杂性和输入特征集。这个挑战在像 Meta 这样的广泛系统中被放大，它包含了数百个具有不同规格的模型，使得为每个模型裁剪用户表示学习变得不切实际。为了应对这些挑战，我们提出了缩放用户建模(SUM) ，这是一个广泛应用于 Meta 广告排名系统的框架，旨在促进高效和可扩展的在线用户表示在数百个广告模型之间的共享。SUM 利用一些指定的上游用户模型，通过先进的建模技术从大量的用户特性中综合用户嵌入。然后这些嵌入作为下游在线广告排名模型的输入，促进有效的表示共享。为了适应用户特征的动态性和保证嵌入的新鲜性，我们设计了 SUM 在线异步平台(SOAP) ，这是一个无延迟的在线服务系统，它补充了模型的新鲜性和嵌入的稳定性，能够根据用户的每个请求频繁地更新用户模型和在线推断用户的嵌入。我们分享了 SUM 框架的实际部署经验，并通过全面的实验验证了其优越性。迄今为止，SUM 已经在 Meta 中推出了数百个广告排名模型，每天处理数千亿用户请求，产生了显著的在线度量收益和基础设施成本节约。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scaling+User+Modeling:+Large-scale+Online+User+Representations+for+Ads+Personalization+in+Meta)|0|
|[Finding What Users Look for by Attribute-Aware Personalized Item Comparison in Relevant Recommendation](https://doi.org/10.1145/3589335.3651508)|Rui Ma, Dike Sun, Jincheng Xu, Jingsong Yuan, Jiandong Zhang||Relevant recommendation is a distinctive recommendation scenario in e-commerce platforms, which provides an extended set of items that are relevant to the trigger item (the item that triggers the relevant recommendation). Different from the general recommendations whose item feeds are diversified, relevant recommendation regards the trigger item as a key component. From one perspective, the trigger item reveals users' current interests and determines the range of the recommendation results. From the other perspective, users may have the mindset to look for items that have directional attribute differences from the trigger item. In this paper, we present an attribute-aware personalized item comparison framework. Under this framework, an item subtraction module is first applied over the trigger item and the candidate item, which calculates their directional difference with consideration of their intrinsic similarity. Then two modules are used to estimate users' preference for this current item pair: one learns the collective preference of all users, and the other learns the current user's personal evolutional preference. Experiments on a CTR prediction task over both a public dataset and an industrial dataset from our shopping app show that the proposed method outperforms the state-of-the-art algorithms and also achieves better generalization ability.|相关推荐是电子商务平台中一个独特的推荐场景，它提供了一组与触发条目(触发相关推荐的条目)相关的扩展条目。不同于一般推荐的项目种类多样化，相关推荐将触发项目作为一个关键组成部分。从一个角度来看，触发条目揭示了用户当前的兴趣，并确定了推荐结果的范围。从另一个角度来看，用户可能倾向于寻找与触发器项具有方向属性差异的项。在本文中，我们提出了一个属性感知的个性化项目比较框架。在此框架下，首先对触发项和候选项应用项减法模块，计算它们的方向差，并考虑它们的内在相似性。然后利用两个模块来估计用户对当前项目对的偏好: 一个模块学习所有用户的集体偏好，另一个模块学习当前用户的个人进化偏好。通过对一个公共数据集和一个工业数据集的点击率预测任务的实验表明，该方法的性能优于目前最先进的算法，并且具有更好的泛化能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Finding+What+Users+Look+for+by+Attribute-Aware+Personalized+Item+Comparison+in+Relevant+Recommendation)|0|
|[De-Anchor: Mitigating Attention Polarization for Lifelong User Behavior Modeling in Click-Through Rate Prediction](https://doi.org/10.1145/3589335.3651486)|Hongzun Liu, Kang Yin, Tianyu Sun, Rui Huang, Yunsong Li, Xiao Fang, Zhaojie Liu, Weidong Liu, Guorui Zhou||User lifelong behavior sequences are essential for click-through rate (CTR) prediction tasks in industrial recommender systems. Attention-based module, especially multi-head target attention (MHTA), has been proven to be effective in aggregating behavior features given a certain target item. However, we found a common phenomenon that attention weights in MHTA tend to over-concentrate on merely a small subset of a user's historical behaviors, producing a sparse one-hot distributed attention weights in training gradually, which we callAttention Polarization (AP). These polarized weights on certain behaviors (which we call "\textitattention anchor ") could make the model fail to capture a user's diversified interests, and harm the learning of behavior embedding as the gradients on these features are nearly zero. We introduce two indicators:anchor rate andattention entropy to measure the magnitude of AP, and proposeDe-Anchor, a novel method to alleviate it, which can serve as a stand-alone and parameter-efficient plug-in to existing CTR backbones. De-Anchor contains two modules:Anchor-aware gradient dropout (AGD) forcing the model to capture diversified interest information from behavior sequences by discarding gradients of non-behavior features, andTarget-aware attention anchor (TAA) providing a pseudo behavior to offload excessive weights of MHTA. Extensive offline experiments and industrial online A/B tests demonstrate the efficacy of our method.|在工业推荐系统中，用户终身行为序列对于点进率(CTR)预测任务至关重要。基于注意的模型，尤其是多目标注意模型(MHTA) ，已经被证明能够有效地聚合给定目标项的行为特征。然而，我们发现了一个普遍的现象，即 MHTA 中的注意权重往往过度集中在用户历史行为的一小部分，在训练中逐渐产生稀疏的一热分布的注意权重，我们称之为注意极化(AP)。这些对特定行为(我们称之为“文本注意锚”)的极化权重可能使模型无法捕捉用户的多样化兴趣，并且由于这些特征上的梯度几乎为零而损害行为嵌入的学习。我们引入锚定率和注意熵两个指标来度量 AP 的大小，并提出了一种新的缓解 AP 的方法 De-Anchor，它可以作为现有 CTR 骨干网的一个独立的、参数高效的插件。去锚模型包含两个模块: 锚感知梯度丢失(AGD)强制模型通过丢弃非行为特征的梯度来从行为序列中捕获多样化的兴趣信息; 目标感知注意锚(TAA)提供一种伪行为来卸载 MHTA 的过度权重。大量的离线实验和工业在线 A/B 测试证明了我们方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=De-Anchor:+Mitigating+Attention+Polarization+for+Lifelong+User+Behavior+Modeling+in+Click-Through+Rate+Prediction)|0|
|[User Distribution Mapping Modelling with Collaborative Filtering for Cross Domain Recommendation](https://doi.org/10.1145/3589334.3645331)|Weiming Liu, Chaochao Chen, Xinting Liao, Mengling Hu, Jiajie Su, Yanchao Tan, Fan Wang||User cold-start recommendation aims to provide accurate items for the newly joint users and is a hot and challenging problem. Nowadays as people participant in different domains, how to recommend items in the new domain for users in an old domain has become more urgent. In this paper, we focus on the Dual Cold-Start Cross Domain Recommendation (Dual-CSCDR) problem. That is, providing the most relevant items for new users on the source and target domains. The prime task in Dual-CSCDR is to properly model user-item rating interactions and map user expressive embeddings across domains. However, previous approaches cannot solve Dual-CSCDR well, since they separate the collaborative filtering and distribution mapping process, leading to the error superimposition issue. Moreover, most of these methods fail to fully exploit the cross-domain relationship among large number of non-overlapped users, which strongly limits their performance. To fill this gap, we propose User Distribution Mapping model with Collaborative Filtering (UDMCF), a novel end-to-end cold-start cross-domain recommendation framework for the Dual-CSCDR problem. UDMCF includes two main modules, i.e., rating prediction module and distribution alignment module. The former module adopts one-hot ID vectors and multi-hot historical ratings for collaborative filtering via a contrastive loss. The latter module contains overlapped user embedding alignment and general user subgroup distribution alignment. Specifically, we innovatively propose unbalance distribution optimal transport with typical subgroup discovering algorithm to map the whole user distributions. Our empirical study on several datasets demonstrates that UDMCF significantly outperforms the state-of-the-art models under the Dual-CSCDR setting.|用户冷启动推荐旨在为新联合用户提供准确的项目，是一个热点和难点问题。随着人们在不同领域的参与，如何为旧领域的用户推荐新领域的项目已经变得越来越迫切。本文主要研究双冷启动跨域推荐问题。也就是说，为源域和目标域上的新用户提供最相关的项。双重 CSCDR 的主要任务是正确建模用户-项目评分交互并映射跨域的用户表达嵌入。然而，以前的方法并不能很好地解决双协同散射计算问题，因为它们分离了协同过滤和分布映射过程，导致了误差叠加问题。此外，这些方法大多未能充分利用大量非重叠用户之间的跨域关系，严重限制了它们的性能。为了填补这个空白，我们提出了带协同过滤的用户分布映射模型(UDMCF) ，这是一个新颖的端到端冷启动跨域推荐框架，用于解决双协同监控系统问题。UDMCF 包括两个主要模块，即定额预测模块和配电网调度模块。前一个模块采用一个热门的身份证矢量和多个热门的历史评级，通过对比损失的协同过滤。后一个模块包含重叠用户嵌入对齐和一般用户子组分布对齐。具体来说，我们创新性地提出了不平衡分布最优运输的典型子群发现算法来映射整个用户分布。我们对几个数据集的实证研究表明，在双 CSCDR 设置下，UDMCF 明显优于最先进的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User+Distribution+Mapping+Modelling+with+Collaborative+Filtering+for+Cross+Domain+Recommendation)|0|
|[Leave No One Behind: Online Self-Supervised Self-Distillation for Sequential Recommendation](https://doi.org/10.1145/3589334.3645590)|Shaowei Wei, Zhengwei Wu, Xin Li, Qintong Wu, Zhiqiang Zhang, Jun Zhou, Lihong Gu, Jinjie Gu||Sequential recommendation methods play a pivotal role in modern recommendation systems. A key challenge lies in accurately modeling user preferences in the face of data sparsity. To tackle this challenge, recent methods leverage contrastive learning (CL) to derive self-supervision signals by maximizing the mutual information of two augmented views of the original user behavior sequence. Despite their effectiveness, CL-based methods encounter a limitation in fully exploiting self-supervision signals for users with limited behavior data, as users with extensive behaviors naturally offer more information. To address this problem, we introduce a novel learning paradigm, named Online Self-Supervised Self-distillation for Sequential Recommendation ($S^4$Rec), effectively bridging the gap between self-supervised learning and self-distillation methods. Specifically, we employ online clustering to proficiently group users by their distinct latent intents. Additionally, an adversarial learning strategy is utilized to ensure that the clustering procedure is not affected by the behavior length factor. Subsequently, we employ self-distillation to facilitate the transfer of knowledge from users with extensive behaviors (teachers) to users with limited behaviors (students). Experiments conducted on four real-world datasets validate the effectiveness of the proposed method\footnote{Code is available at https://github.com/xjaw/S4Rec|序贯推荐方法在现代推荐系统中占有举足轻重的地位。一个关键的挑战在于在面对数据稀少的情况下准确地建模用户偏好。为了应对这一挑战，最近的方法利用对比学习(CL) ，通过最大化原始用户行为序列的两个增强视图的互信息来获得自我监督信号。尽管有效，但基于 CL 的方法在充分利用行为数据有限的用户的自我监督信号方面遇到了局限，因为行为广泛的用户自然会提供更多的信息。为了解决这一问题，我们引入了一种新的学习范式，称为序贯推荐在线自我监督自我精馏($S ^ 4 $Rec) ，有效地弥补了自我监督学习和自我精馏方法之间的差距。具体来说，我们使用在线集群来熟练地根据用户不同的潜在意图对他们进行分组。此外，采用对抗学习策略，确保聚类过程不受行为长度因子的影响。随后，我们采用自我提取的方法，促进知识从行为广泛的用户(教师)转移到行为有限的用户(学生)。在四个真实世界的数据集上进行的实验验证了提议方法的有效性脚注{代码可在 https://github.com/xjaw/s4rec 获得|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leave+No+One+Behind:+Online+Self-Supervised+Self-Distillation+for+Sequential+Recommendation)|0|
|[Improving Search for New Product Categories via Synthetic Query Generation Strategies](https://doi.org/10.1145/3589335.3648299)|Akshay Jagatap, Srujana Merugu, Prakash Mandayam Comar||Efficient retrieval and ranking of relevant products in e-commerce product search relies on accurate mapping of queries to product categories. This query classification typically utilizes a combination of textual and customer behavioral signals. However, new product categories often lack customer interaction data leading to poor performance. In this paper, we present a novel approach to mitigate this cold start problem in product ranking via synthetic generation of queries as well as simulation of customer interactions. Specifically we study two strategies for synthetic data generation: (i) fine-tuning a generative language model (LLM) on historical product-query interactions and using it to generate synthetic queries from the product catalog, (ii) Bayesian prompt optimization with an instruction-tuned LLM to directly generate queries from catalog. Empirical evaluation of the proposed approaches on public datasets and real-world customer queries demonstrates significant benefits (+2.96% and +2.34% in PR-AUC on e-commerce queries)1 relative to the baseline approach without synthetic data augmentation. Furthermore, evaluation of the augmented model on live search page results in a substantial increase in highly relevant product results (+3.35%) and reduction (-3.07%) in irrelevant results.|电子商务产品搜索中相关产品的有效检索和排序依赖于查询到产品类别的精确映射。这种查询分类通常使用文本和客户行为信号的组合。然而，新产品类别往往缺乏客户交互数据，导致性能较差。在本文中，我们提出了一种新的方法来缓解这种冷启动问题的产品排名通过合成生成的查询和模拟客户交互。具体而言，我们研究了合成数据生成的两种策略: (i)在历史产品-查询交互中微调生成语言模型(LLM) ，并使用它从产品目录生成合成查询; (ii)贝叶斯提示优化，使用指令调优的 LLM 直接从目录生成查询。对公共数据集和现实世界客户查询的提议方法的实证评估表明，相对于没有合成数据增强的基线方法，显着的益处(电子商务查询的 PR-AUC 分别为 + 2.96% 和 + 2.34%)1。此外，在实时搜索页面上对增强模型的评估导致高度相关的产品结果大幅增加(+ 3.35%) ，不相关的结果减少(-3.07%)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Search+for+New+Product+Categories+via+Synthetic+Query+Generation+Strategies)|0|
|[MS MARCO Web Search: A Large-scale Information-rich Web Dataset with Millions of Real Click Labels](https://doi.org/10.1145/3589335.3648327)|Qi Chen, Xiubo Geng, Corby Rosset, Carolyn Buractaon, Jingwen Lu, Tao Shen, Kun Zhou, Chenyan Xiong, Yeyun Gong, Paul N. Bennett, Nick Craswell, Xing Xie, Fan Yang, Bryan Tower, Nikhil Rao, Anlei Dong, Wenqi Jiang, Zheng Liu, Mingqin Li, Chuanjie Liu, Zengzhong Li, Rangan Majumder, Jennifer Neville, Andy Oakley, Knut Magne Risvik, Harsha Vardhan Simhadri, Manik Varma, Yujing Wang, Linjun Yang, Mao Yang, Ce Zhang||Recent breakthroughs in large models have highlighted the critical significance of data scale, labels and modals. In this paper, we introduce MS MARCO Web Search, the first large-scale information-rich web dataset, featuring millions of real clicked query-document labels. This dataset closely mimics real-world web document and query distribution, provides rich information for various kinds of downstream tasks and encourages research in various areas, such as generic end-to-end neural indexer models, generic embedding models, and next generation information access system with large language models. MS MARCO Web Search offers a retrieval benchmark with three web retrieval challenge tasks that demand innovations in both machine learning and information retrieval system research domains. As the first dataset that meets large, real and rich data requirements, MS MARCO Web Search paves the way for future advancements in AI and system research. MS MARCO Web Search dataset is available at: https://github.com/microsoft/MS-MARCO-Web-Search.|最近在大型模型方面的突破突出了数据规模、标签和模态的关键意义。本文介绍了第一个大规模信息丰富的网络数据集 MS MARCO Web Search，它包含了数百万个实际点击的查询文档标签。这个数据集非常接近真实世界的网络文档和查询分布，为各种下游任务提供丰富的信息，并鼓励在各个领域的研究，如通用的端到端神经索引器模型，通用的嵌入模型，下一代信息访问系统与大型语言模型。微软 MARCO 网络搜索提供了一个检索基准，包含三个网络检索挑战任务，需要在机器学习和信息检索系统研究领域进行创新。作为第一个满足大型、真实、丰富数据需求的数据集，微软 MARCO 网络搜索为人工智能和系统研究的未来发展铺平了道路。微软马可网上搜寻资料集可于以下 https://github.com/microsoft/MS-MARCO-Web-Search 索取:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MS+MARCO+Web+Search:+A+Large-scale+Information-rich+Web+Dataset+with+Millions+of+Real+Click+Labels)|0|
|[Enhancing Cross-Domain Click-Through Rate Prediction via Explicit Feature Augmentation](https://doi.org/10.1145/3589335.3648341)|Xu Chen, Zida Cheng, Jiangchao Yao, Chen Ju, Weilin Huang, Jinsong Lan, Xiaoyi Zeng, Shuai Xiao||Cross-domain CTR (CDCTR) prediction is an important research topic that studies how to leverage meaningful data from a related domain to help CTR prediction in target domain. Most existing CDCTR works design implicit ways to transfer knowledge across domains such as parameter-sharing that regularizes the model training in target domain. More effectively, recent researchers propose explicit techniques to extract user interest knowledge and transfer this knowledge to target domain. However, the proposed method mainly faces two issues: 1) it usually requires a super domain, i.e. an extremely large source domain, to cover most users or items of target domain, and 2) the extracted user interest knowledge is static no matter what the context is in target domain. These limitations motivate us to develop a more flexible and efficient technique to explicitly transfer knowledge. In this work, we propose a cross-domain augmentation network (CDAnet) being able to perform explicit knowledge transfer between two domains. Specifically, CDAnet contains a designed translation network and an augmentation network which are trained sequentially. The translation network computes latent features from two domains and learns meaningful cross-domain knowledge of each input in target domain by using a designed cross-supervised feature translator. Later the augmentation network employs the explicit cross-domain knowledge as augmented information to boost the target domain CTR prediction. Through extensive experiments on two public benchmarks and one industrial production dataset, we show CDAnet can learn meaningful translated features and largely improve the performance of CTR prediction. CDAnet has been conducted online A/B test in image2product retrieval at Taobao app, bringing an absolute 0.11 point CTR improvement, a relative 0.64|跨域 CTR (Cross-domain CTR)预测是研究如何利用相关领域有意义的数据来帮助目标领域 CTR 预测的一个重要研究课题。大多数现有的 CDCTR 工作设计了隐式的跨领域知识转移方法，如参数共享，规范了目标领域的模型训练。更有效的是，最近的研究人员提出了显性技术来提取用户兴趣知识并将这些知识转移到目标领域。然而，该方法主要面临两个问题: 1)它通常需要一个超级域，即一个非常大的源域，以覆盖目标域的大多数用户或项目; 2)提取的用户兴趣知识是静态的，不管目标域的上下文是什么。这些限制促使我们开发一种更灵活、更有效的技术来明确地传递知识。在这项工作中，我们提出了一个跨域增强网络(CDAnet) ，它可以在两个域之间进行外显知识传输。具体来说，CDAnet 包含一个设计好的翻译网络和一个按顺序训练的增强网络。该翻译网络利用设计的交叉监督特征转换器计算两个领域的潜在特征，并学习目标领域中每个输入的有意义的跨领域知识。随后增强网络利用显式的跨领域知识作为增强信息来提高目标领域的 CTR 预测能力。通过对两个公共基准和一个工业生产数据集的大量实验，我们发现 CDAnet 能够学习到有意义的翻译特征，从而大大提高了 CTR 预测的性能。CDAnet 在淘宝应用图像2产品检索中进行了在线 A/B 测试，绝对点击率提高了0.11个百分点，相对提高了0.64个百分点|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Cross-Domain+Click-Through+Rate+Prediction+via+Explicit+Feature+Augmentation)|0|
|[Item-Ranking Promotion in Recommender Systems](https://doi.org/10.1145/3589335.3651529)|HongKyun Bae, HaeRi Jang, YangSae Moon, SangWook Kim||In this paper, we first define the problem of item-ranking promotion (IRP) in recommender systems as (Goal 1) maintaining a high level of overall recommendation accuracy while (Goal 2) recommending the items with extra values (i.e., RP-items) to as many users as possible. Our novel framework, proposed to address the IRP problem, is based on our own loss function that simultaneously aims to achieve the two goals above and employs a learning-to-rank scheme for training a recommender model. Via extensive experiments, we validate the effectiveness of our framework in terms of the exposure rate of RP-items and the accuracy of recommendation.|本文首先将推荐系统中的项目排名推广问题定义为(目标1)保持较高的总体推荐准确率，同时(目标2)向尽可能多的用户推荐具有额外价值的项目(即 RP 项目)。针对 IRP 问题，我们提出了一种基于自身损失函数的新框架，该框架同时实现了上述两个目标，并采用了一种学习排序方案来训练一个推荐模型。通过大量的实验，我们验证了我们的框架在快速反应项目的暴露率和推荐的准确性方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Item-Ranking+Promotion+in+Recommender+Systems)|0|
|[Understanding and Counteracting Feature-Level Bias in Click-Through Rate Prediction](https://doi.org/10.1145/3589335.3651576)|Jinqiu Jin, Sihao Ding, Wenjie Wang, Fuli Feng||Common click-through rate (CTR) prediction recommender models tend to exhibit feature-level bias, which leads to unfair recommendations among item groups and inaccurate recommendations for users. While existing methods address this issue by adjusting the learning of CTR models, such as through additional optimization objectives, they fail to consider how the bias is caused within these models. To address this research gap, our study performs a top-down analysis on representative CTR models. Through blocking different components of a trained CTR model one by one, we identify the key contribution of the linear component to feature-level bias. We conduct a theoretical analysis of the learning process for the weights in the linear component, revealing how group-wise properties of training data influence them. Our experimental and statistical analyses demonstrate a strong correlation between imbalanced positive sample ratios across item groups and feature-level bias. Based on this understanding, we propose a minimally invasive yet effective strategy to counteract feature-level bias in CTR models by removing the biased linear weights from trained models. Additionally, we present a linear weight adjusting strategy that requires fewer random exposure records than relevant debiasing methods. The superiority of our proposed strategies are validated through extensive experiments on three real-world datasets.|常见的点进率预测推荐模型往往表现出特征层面的偏差，导致项目组之间的不公平推荐和对用户的不准确推荐。虽然现有的方法通过调整 CTR 模型的学习来解决这个问题，例如通过额外的优化目标，但是他们没有考虑这些模型中偏差是如何产生的。为了解决这一研究差距，我们的研究对代表性的 CTR 模型进行了自上而下的分析。通过对训练好的 CTR 模型的不同分量逐个进行分块，我们确定了线性分量对特征级偏差的主要贡献。我们对线性分量中权重的学习过程进行了理论分析，揭示了训练数据的群体性质如何影响它们。我们的实验和统计分析表明，项目组间不平衡的正样本比率和特征水平偏差之间有很强的相关性。基于这一认识，我们提出了一种微创但有效的策略，通过去除训练后的模型中有偏的线性权重来抵消 CTR 模型中的特征水平偏差。此外，我们提出了一个线性权重调整策略，需要较少的随机曝光记录比相关的消偏方法。通过在三个实际数据集上的大量实验，验证了所提策略的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+and+Counteracting+Feature-Level+Bias+in+Click-Through+Rate+Prediction)|0|
|[A Demonstration of Decentralized Search Over Solid Personal Online Datastores](https://doi.org/10.1145/3589335.3651248)|Mohamed Ragab, Yury Savateev, Helen Oliver, Reza Moosaei, Thanassis Tiropanis, Alexandra Poulovassilis, Adriane Chapman, George Roussos||In the modern Web landscape, data privacy and control are increasingly unattainable for users. Solid 1, a decentralized Web ecosystem, restores individual privacy and control by separating data from applications, allowing integration across applications while enabling users to control access. The growth in Solid's decentralized pods, and the escalating amounts of data stored in them, necessitate a decentralized search mechanism to query data within personal datastores, while respecting varying access constraints. This demo paper presents our decentralized search system (ESPRESSO) for querying RDF and non-RDF data in Solid datastores, tackling challenges like query propagation, data indexing, privacy, and results aggregation.|在现代 Web 环境中，用户越来越难以实现数据隐私和控制。Solid 1，一个分散的 Web 生态系统，通过将数据从应用程序中分离出来，恢复个人隐私和控制，允许跨应用程序集成，同时允许用户控制访问。Solid 的分散式豆荚的增长，以及其中存储的数据量的不断增加，需要一种分散式搜索机制来查询个人数据存储中的数据，同时尊重不同的访问限制。本演示文章介绍了我们的分散式搜索系统(ESPRESSO) ，用于在 Solid 数据存储中查询 RDF 和非 RDF 数据，解决查询传播、数据索引、隐私和结果聚合等挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Demonstration+of+Decentralized+Search+Over+Solid+Personal+Online+Datastores)|0|
|[Neural Contextual Bandits for Personalized Recommendation](https://doi.org/10.1145/3589335.3641241)|Yikun Ban, Yunzhe Qi, Jingrui He||In the dynamic landscape of online businesses, recommender systems are pivotal in enhancing user experiences. While traditional approaches have relied on static supervised learning, the quest for adaptive, user-centric recommendations has led to the emergence of the formulation of contextual bandits. This tutorial investigates the contextual bandits as a powerful framework for personalized recommendations. We delve into the challenges, advanced algorithms and theories, collaborative strategies, and open challenges and future prospects within this field. Different from existing related tutorials, (1) we focus on the exploration perspective of contextual bandits to alleviate the ``Matthew Effect'' in the recommender systems, i.e., the rich get richer and the poor get poorer, concerning the popularity of items; (2) in addition to the conventional linear contextual bandits, we will also dedicated to neural contextual bandits which have emerged as an important branch in recent years, to investigate how neural networks benefit contextual bandits for personalized recommendation both empirically and theoretically; (3) we will cover the latest topic, collaborative neural contextual bandits, to incorporate both user heterogeneity and user correlations customized for recommender system; (4) we will provide and discuss the new emerging challenges and open questions for neural contextual bandits with applications in the personalized recommendation, especially for large neural models.|在动态的在线商务环境中，推荐系统是增强用户体验的关键。虽然传统的方法依赖于静态的监督式学习，但是对适应性的、以用户为中心的建议的追求已经导致了关联强盗的出现。本教程研究了上下文强盗作为个性化推荐的强大框架。我们深入研究这个领域的挑战、先进的算法和理论、协作策略、开放的挑战和未来的前景。与现有的相关教程不同的是，(1)我们着重从语境土匪的角度来探索，以缓解推荐系统中的“马太效应”。(2)除了传统的线性情境强盗，我们还将致力于近年来兴起的一个重要分支——神经情境强盗，研究神经网络如何在实证和理论上有利于情境强盗的个性化推荐;(3)我们将涵盖最新的主题，协作神经上下文盗贼，以纳入用户异质性和用户相关性定制的推荐系统; (4)我们将提供和讨论新出现的挑战和神经上下文盗贼的开放性问题的应用，在个性化推荐，特别是在大型神经模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Contextual+Bandits+for+Personalized+Recommendation)|0|
|[Causality-driven User Modeling for Sequential Recommendations over Time](https://doi.org/10.1145/3589335.3651896)|Xingming Chen, Qing Li||Contemporary sequential recommendation systems predominantly leverage statistical correlations derived from user interaction histories to predict future preferences. However, these correlations often mask implicit challenges. On the one hand, user data is frequently plagued by implicit, noisy feedback, misdirecting users towards items that fail to align with their actual interests, which is magnified in sequential recommendation contexts. On the other hand, prevalent methods tend to over-rely on similarity-based attention mechanisms across item pairs, which are prone to utilizing heuristic shortcuts, thereby leading to suboptimal recommendation. To tackle these issues, we put forward a causality-driven user modeling approach for sequential recommendation, which pivots towards a causal perspective. Specifically, we involves the application of a causal graph to identify confounding factors that give rise to spurious correlations and to isolate conceptual variables that causally encapsulate user preferences. By learning the representation of these disentangled causal variables at the conceptual level, we can distinguish between causal and non-causal associations while preserving the inherent sequential nature of user behaviors. This enables us to ascertain which elements are critical and which may induce unintended biases. The framework of our method can be compatible with various mainstream sequential models, which offers a robust foundation for reconstructing more accurate and meaningful user and item representations driven by causality.|当代的顺序推荐系统主要利用来自用户交互历史的统计相关性来预测未来的偏好。然而，这些相关性往往掩盖了隐含的挑战。一方面，用户数据经常受到隐含的、嘈杂的反馈的困扰，将用户误导到与他们的实际兴趣不一致的项目上，这种情况在顺序推荐上下文中被放大。另一方面，流行的方法倾向于过度依赖跨项目对的基于相似性的注意机制，这种机制倾向于利用启发式快捷方式，从而导致次优推荐。为了解决这些问题，我们提出了一种基于因果关系的连续推荐用户建模方法。具体来说，我们涉及到一个因果图的应用，以确定产生虚假相关性的混杂因素，并隔离概念变量，因果封装用户偏好。通过在概念层面上学习这些非纠缠因果变量的表示，我们可以区分因果关联和非因果关联，同时保留用户行为固有的序列性质。这使我们能够确定哪些因素是关键的，哪些因素可能引起意想不到的偏见。该方法的框架可以与各种主流的序列模型兼容，为因果关系驱动下的用户和项目表示重构提供了坚实的基础。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causality-driven+User+Modeling+for+Sequential+Recommendations+over+Time)|0|
|[ConvSDG: Session Data Generation for Conversational Search](https://doi.org/10.1145/3589335.3651940)|Fengran Mo, Bole Yi, Kelong Mao, Chen Qu, Kaiyu Huang, JianYun Nie||Conversational search provides a more convenient interface for users to search by allowing multi-turn interaction with the search engine. However, the effectiveness of the conversational dense retrieval methods is limited by the scarcity of training data required for their fine-tuning. Thus, generating more training conversational sessions with relevant labels could potentially improve search performance. Based on the promising capabilities of large language models (LLMs) on text generation, we propose ConvSDG, a simple yet effective framework to explore the feasibility of boosting conversational search by using LLM for session data generation. Within this framework, we design dialogue/session-level and query-level data generation with unsupervised and semi-supervised learning, according to the availability of relevance judgments. The generated data are used to fine-tune the conversational dense retriever. Extensive experiments on four widely used datasets demonstrate the effectiveness and broad applicability of our ConvSDG framework compared with several strong baselines.|通过允许与搜索引擎进行多次交互，对话式搜索为用户提供了更方便的搜索界面。然而，会话密集检索方法的有效性受限于其微调所需的训练数据的稀缺性。因此，使用相关标签生成更多的训练对话会话可能会提高搜索性能。基于大语言模型(LLM)在文本生成方面的潜在能力，本文提出了一个简单而有效的框架，用于探索利用 LLM 进行会话数据生成来提高会话搜索的可行性。在这个框架内，我们根据相关性判断的可用性，设计无监督和半监督学习的对话/会话级和查询级数据生成。生成的数据用于对会话密集型检索器进行微调。在四个广泛使用的数据集上进行的大量实验表明，与几个强大的基线相比，我们的 ConvSDG 框架是有效的，并且具有广泛的适用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ConvSDG:+Session+Data+Generation+for+Conversational+Search)|0|
|[How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation](https://doi.org/10.1145/3589335.3651955)|Lixi Zhu, Xiaowen Huang, Jitao Sang||Conversational Recommender System (CRS) interacts with users through natural language to understand their preferences and provide personalized recommendations in real-time. CRS has demonstrated significant potential, prompting researchers to address the development of more realistic and reliable user simulators as a key focus. Recently, the capabilities of Large Language Models (LLMs) have attracted a lot of attention in various fields. Simultaneously, efforts are underway to construct user simulators based on LLMs. While these works showcase innovation, they also come with certain limitations that require attention. In this work, we aim to analyze the limitations of using LLMs in constructing user simulators for CRS, to guide future research. To achieve this goal, we conduct analytical validation on the notable work, iEvaLM. Through multiple experiments on two widely-used datasets in the field of conversational recommendation, we highlight several issues with the current evaluation methods for user simulators based on LLMs: (1) Data leakage, which occurs in conversational history and the user simulator's replies, results in inflated evaluation results. (2) The success of CRS recommendations depends more on the availability and quality of conversational history than on the responses from user simulators. (3) Controlling the output of the user simulator through a single prompt template proves challenging. To overcome these limitations, we propose SimpleUserSim, employing a straightforward strategy to guide the topic toward the target items. Our study validates the ability of CRS models to utilize the interaction information, significantly improving the recommendation results.|会话推荐系统(CRS)通过自然语言与用户互动，了解他们的偏好，并实时提供个性化的建议。CRS 显示了巨大的潜力，促使研究人员将开发更加现实和可靠的用户模拟器作为一个关键重点。近年来，大语言模型(LLM)的性能在各个领域引起了广泛的关注。同时，正在努力构建基于 LLM 的用户模拟器。虽然这些作品展示了创新，但它们也有一定的局限性，需要注意。本文旨在分析 LLM 在构建 CRS 用户模拟器方面的局限性，以指导未来的研究工作。为了实现这个目标，我们对值得注意的工作 iEvaLM 进行了分析验证。通过对会话推荐领域两个广泛使用的数据集进行多次实验，突出了目前基于 LLM 的用户模拟器评估方法存在的几个问题: (1)会话历史和用户模拟器回复中出现的数据泄漏导致评估结果膨胀。(2) CRS 推荐的成功与否更多地取决于会话历史的可用性和质量，而非来自用户模拟器的响应。(3)通过一个单一的提示模板控制用户模拟器的输出具有挑战性。为了克服这些限制，我们提出了 SimpleUserSim，它采用了一种简单的策略来将主题引导到目标项。我们的研究验证了 CRS 模型利用交互信息的能力，显著提高了推荐结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Reliable+is+Your+Simulator?+Analysis+on+the+Limitations+of+Current+LLM-based+User+Simulators+for+Conversational+Recommendation)|0|
|[Mining Exploratory Queries for Conversational Search](https://doi.org/10.1145/3589334.3645424)|Wenhan Liu, Ziliang Zhao, Yutao Zhu, Zhicheng Dou||Users' queries are usually vague, and their search intents tend to be ambiguous, thereby needing search clarification to clarify users' current intent by asking a clarifying question and providing several clickable sub-intent items as clarification options. However, in addition to drilling down the current query, users may also have exploratory needs that diverge from their current intent. For example, a user searching for the query "Cartier women watches'' may also potentially want to explore some parallel information by issuing queries such as "Rolex women watches'' or "Cartier women bracelets'', named exploratory queries in this paper. These exploratory needs are common during the search process yet cannot be satisfied by current search clarification approaches which typically stick to the sub-intents of the query. This paper focuses on mining exploratory queries as additional options to meet users' exploratory needs in conversational search systems. Specifically, we first design a rule-based model that generates exploratory queries based on the current query's top retrieved documents. Then, we propose using the data generated by the rule-based model to train a neural generation model through multi-task learning for further generalization. Finally, we borrow the in-context learning ability of the large language model to generate exploratory queries based on prompt engineering. We constructed an evaluation dataset based on human annotations and conduct an extensive set of experiments. The results show that our proposed methods generate higher-quality exploratory queries compared with several baselines.|用户的查询通常是模糊的，他们的搜索意图往往是模糊的，因此需要通过提出一个澄清问题和提供几个可点击的子意图项作为澄清选项来澄清用户的当前意图。然而，除了深入当前查询之外，用户还可能有与当前意图不同的探索性需求。例如，搜索“卡地亚女性手表”的用户可能也想通过发出“劳力士女性手表”或“卡地亚女性手镯”等查询来探索一些并行信息，这些查询在本文中被命名为探索性查询。这些探索性需求在搜索过程中很常见，但当前的搜索澄清方法不能满足这些需求，因为这些方法通常坚持查询的子意图。在会话搜索系统中，为了满足用户的探索性需求，本文将探索性查询作为附加选项进行挖掘。具体来说，我们首先设计一个基于规则的模型，该模型基于当前查询的最高检索文档生成探索性查询。然后，我们提出利用基于规则的模型生成的数据，通过多任务学习训练神经生成模型，以便进一步推广。最后，借鉴大型语言模型的上下文学习能力，基于快速工程生成探索性查询。我们构建了一个基于人工注释的评价数据集，并进行了大量的实验。结果表明，与几个基线相比，我们提出的方法产生了更高质量的探索性查询。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mining+Exploratory+Queries+for+Conversational+Search)|0|
|[An In-depth Investigation of User Response Simulation for Conversational Search](https://doi.org/10.1145/3589334.3645447)|Zhenduo Wang, Zhichao Xu, Vivek Srikumar, Qingyao Ai||Conversational search has seen increased recent attention in both the IR and NLP communities. It seeks to clarify and solve users' search needs through multi-turn natural language interactions. However, most existing systems are trained and demonstrated with recorded or artificial conversation logs. Eventually, conversational search systems should be trained, evaluated, and deployed in an open-ended setting with unseen conversation trajectories. A key challenge is that training and evaluating such systems both require a human-in-the-loop, which is expensive and does not scale. One strategy is to simulate users, thereby reducing the scaling costs. However, current user simulators are either limited to only responding to yes-no questions from the conversational search system or unable to produce high-quality responses in general. In this paper, we show that existing user simulation systems could be significantly improved by a smaller finetuned natural language generation model. However, rather than merely reporting it as the new state-of-the-art, we consider it a strong baseline and present an in-depth investigation of simulating user response for conversational search. Our goal is to supplement existing work with an insightful hand-analysis of unsolved challenges by the baseline and propose our solutions. The challenges we identified include (1) a blind spot that is difficult to learn, and (2) a specific type of misevaluation in the standard setup. We propose a new generation system to effectively cover the training blind spot and suggest a new evaluation setup to avoid misevaluation. Our proposed system leads to significant improvements over existing systems and large language models such as GPT-4. Additionally, our analysis provides insights into the nature of user simulation to facilitate future work.|会话搜索最近在 IR 和 NLP 社区都受到了越来越多的关注。它试图通过多回合的自然语言交互来阐明和解决用户的搜索需求。然而，大多数现有的系统都是通过记录或人工会话日志进行训练和演示的。最终，会话搜索系统应该训练，评估，并部署在一个开放的设置与看不见的会话轨迹。一个关键的挑战是，培训和评估这样的系统都需要一个人在回路，这是昂贵的，而且没有规模。一种策略是模拟用户，从而降低缩放成本。然而，目前的用户模拟器要么仅限于回答会话搜索系统中的“是”或“否”问题，要么一般无法产生高质量的回答。在本文中，我们表明，现有的用户模拟系统可以显着改善，一个较小的微调自然语言生成模型。然而，我们并不仅仅把它作为最新的技术来报告，我们认为它是一个强有力的基准，并且提出了一个模拟用户对会话搜索的反应的深入研究。我们的目标是通过对未解决的挑战进行深刻的手工分析来补充现有的工作，并提出我们的解决方案。我们确定的挑战包括(1)一个难以学习的盲点，和(2)在标准设置中一个特定类型的错误评估。我们提出了一个新的系统，以有效地覆盖训练盲点，并提出了一个新的评估设置，以避免错误的评估。我们提出的系统导致了对现有系统和大型语言模型(如 GPT-4)的重大改进。此外，我们的分析提供了对用户模拟的本质的洞察，以促进未来的工作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+In-depth+Investigation+of+User+Response+Simulation+for+Conversational+Search)|0|
|[Rethinking Cross-Domain Sequential Recommendation under Open-World Assumptions](https://doi.org/10.1145/3589334.3645351)|Wujiang Xu, Qitian Wu, Runzhong Wang, Mingming Ha, Qiongxu Ma, Linxun Chen, Bing Han, Junchi Yan||Cross-Domain Sequential Recommendation (CDSR) methods aim to tackle the data sparsity and cold-start problems present in Single-Domain Sequential Recommendation (SDSR). Existing CDSR works design their elaborate structures relying on overlapping users to propagate the cross-domain information. However, current CDSR methods make closed-world assumptions, assuming fully overlapping users across multiple domains and that the data distribution remains unchanged from the training environment to the test environment. As a result, these methods typically result in lower performance on online real-world platforms due to the data distribution shifts. To address these challenges under open-world assumptions, we design an Adaptive Multi-Interest Debiasing framework for cross-domain sequential recommendation (AMID), which consists of a multi-interest information module (MIM) and a doubly robust estimator (DRE). Our framework is adaptive for open-world environments and can improve the model of most off-the-shelf single-domain sequential backbone models for CDSR. Our MIM establishes interest groups that consider both overlapping and non-overlapping users, allowing us to effectively explore user intent and explicit interest. To alleviate biases across multiple domains, we developed the DRE for the CDSR methods. We also provide a theoretical analysis that demonstrates the superiority of our proposed estimator in terms of bias and tail bound, compared to the IPS estimator used in previous work.|跨域序列推荐(CDSR)方法旨在解决单域序列推荐(SDSR)中存在的数据稀疏和冷启动问题。现有的 CDSR 作品设计了复杂的结构，依靠重叠用户传播跨域信息。然而，目前的 CDSR 方法是封闭世界的假设，假设用户在多个域之间完全重叠，并且从训练环境到测试环境的数据分布保持不变。因此，由于数据分布的变化，这些方法通常会导致在线真实世界平台上的性能降低。为了解决开放世界条件下的这些挑战，我们设计了一个跨域序列推荐(AMID)的自适应多兴趣去偏框架，该框架由一个多兴趣信息模块(MIM)和一个双鲁棒估计器(DRE)组成。我们的框架是适应开放世界的环境，可以改进大多数现成的单域顺序骨干模型的 CDSR。我们的 MIM 建立了考虑重叠和非重叠用户的兴趣组，允许我们有效地探索用户意图和明确的兴趣。为了减轻跨多个域的偏差，我们为 CDSR 方法开发了 DRE。我们还提供了一个理论分析，证明了我们提出的估计器在偏差和尾界方面的优越性，相比之下，在以前的工作中使用的 IPS 估计器。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Cross-Domain+Sequential+Recommendation+under+Open-World+Assumptions)|0|
|[Not All Embeddings are Created Equal: Towards Robust Cross-domain Recommendation via Contrastive Learning](https://doi.org/10.1145/3589334.3645357)|Wenhao Yang, Yingchun Jian, Yibo Wang, Shiyin Lu, Lei Shen, Bing Wang, Haihong Tang, Lijun Zhang||Cross-domain recommendation (CDR) aims to leverage the rich information from the source domain to enhance recommendation performance in the target domain. However, the data imbalance problem inherent across different domains compromises the effectiveness of CDR approaches, posing a significant challenge to CDR. Most current CDR methodologies focus on creating better user embeddings for the target domain, yet usually neglect the inconsistency in user activities due to data imbalance. As a result, the process of creating user embeddings tends to prioritize users with more frequent interactions and leave less active users underserved, leading these CDR methods to struggle in making accurate recommendations for those with fewer interactions. Such bias in creating embeddings reveals the fact that ''not all embeddings are created equal'' in CDR, which serves as the primary motivation of this study. Inspired by the recent development of contrastive learning, this paper proposes User-aware Contrastive Learning for Robust cross-domain recommendation (UCLR), enhancing the robustness of cross-domain recommendation. Specifically, our proposed method consists of two sub-modules: (i) pretrained global embedding, where the global user embeddings are pretrained across all the domains; (ii) contrastive dual-stream collaborative autoencoder, where more equal user embeddings are generated by optimizing contrastive loss with individualized temperatures. To further improve the performance of our method in each domain, we finetune the whole framework of UCLR based on Low-Rank Adaptation (LoRA). Theoretically, our method is equipped with a provable convergence guarantee during the contrastive learning stage. Furthermore, we also conduct comprehensive experiments on real-world datasets to validate the effectiveness of our proposed method.|跨域推荐(CDR)旨在利用源域中的丰富信息来提高目标域中的推荐性能。然而，不同领域固有的数据不平衡问题影响了 CDR 方法的有效性，对 CDR 提出了严峻的挑战。目前大多数 CDR 方法侧重于为目标域创建更好的用户嵌入，但往往忽视了由于数据不平衡而导致的用户活动的不一致性。因此，创建用户嵌入的过程往往会优先考虑交互更频繁的用户，使得活跃度较低的用户得不到充分的服务，导致这些 CDR 方法难以为交互较少的用户提供准确的建议。创建嵌入的这种偏见揭示了 CDR 中“不是所有的嵌入都是平等的”这一事实，这是本研究的主要动机。受对比学习的启发，本文提出了基于用户感知的鲁棒跨域推荐对比学习(UCLR)方法，增强了跨域推荐的鲁棒性。具体而言，我们提出的方法由两个子模块组成: (i)预训练的全局嵌入，其中全局用户嵌入在所有领域预训练; (ii)对比双流协同自动编码器，其中更多的相等的用户嵌入是通过优化对比损失与个性化温度。为了进一步提高我们的方法在每个领域的性能，我们微调了整个框架的 UCLR 的基础上低秩自适应(LoRA)。理论上，我们的方法在对比学习阶段具有可证明的收敛性保证。此外，我们还对真实世界的数据集进行了全面的实验，以验证我们提出的方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Not+All+Embeddings+are+Created+Equal:+Towards+Robust+Cross-domain+Recommendation+via+Contrastive+Learning)|0|
|[Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation](https://doi.org/10.1145/3589334.3645380)|Yongqiang Han, Hao Wang, Kefan Wang, Likang Wu, Zhi Li, Wei Guo, Yong Liu, Defu Lian, Enhong Chen||In recommendation systems, users frequently engage in multiple types of behaviors, such as clicking, adding to a cart, and purchasing. However, with diversified behavior data, user behavior sequences will become very long in the short term, which brings challenges to the efficiency of the sequence recommendation model. Meanwhile, some behavior data will also bring inevitable noise to the modeling of user interests. To address the aforementioned issues, firstly, we develop the Efficient Behavior Sequence Miner (EBM) that efficiently captures intricate patterns in user behavior while maintaining low time complexity and parameter count. Secondly, we design hard and soft denoising modules for different noise types and fully explore the relationship between behaviors and noise. Finally, we introduce a contrastive loss function along with a guided training strategy to compare the valid information in the data with the noisy signal, and seamlessly integrate the two denoising processes to achieve a high degree of decoupling of the noisy signal. Sufficient experiments on real-world datasets demonstrate the effectiveness and efficiency of our approach in dealing with multi-behavior sequential recommendation.|在推荐系统中，用户经常参与多种类型的行为，比如点击、添加到购物车和购买。然而，随着用户行为数据的多样化，短期内用户行为序列将变得非常长，这对序列推荐模型的有效性提出了挑战。同时，一些行为数据也会给用户兴趣建模带来不可避免的干扰。为了解决上述问题，首先，我们开发了高效行为序列挖掘器(EBM) ，它能够有效地捕获用户行为中的复杂模式，同时保持较低的时间复杂度和参数计数。其次，针对不同的噪声类型设计了软硬件去噪模块，并充分探讨了行为与噪声之间的关系。最后，引入对比损失函数和引导训练策略，将数据中的有效信息与噪声信号进行比较，并将两个去噪过程无缝结合，实现了噪声信号的高度解耦。通过对实际数据集的大量实验，验证了该方法处理多行为顺序推荐的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Noise-Decoupling+for+Multi-Behavior+Sequential+Recommendation)|0|
|[Debiasing Recommendation with Personal Popularity](https://doi.org/10.1145/3589334.3645421)|Wentao Ning, Reynold Cheng, Xiao Yan, Ben Kao, Nan Huo, Nur Al Hasan Haldar, Bo Tang||Global popularity (GP) bias is the phenomenon that popular items are recommended much more frequently than they should be, which goes against the goal of providing personalized recommendations and harms user experience and recommendation accuracy. Many methods have been proposed to reduce GP bias but they fail to notice the fundamental problem of GP, i.e., it considers popularity from a global perspective of all users and uses a single set of popular items, and thus cannot capture the interests of individual users. As such, we propose a user-aware version of item popularity named personal popularity (PP), which identifies different popular items for each user by considering the users that share similar interests. As PP models the preferences of individual users, it naturally helps to produce personalized recommendations and mitigate GP bias. To integrate PP into recommendation, we design a general personal popularity aware counterfactual (PPAC) framework, which adapts easily to existing recommendation models. In particular, PPAC recognizes that PP and GP have both direct and indirect effects on recommendations and controls direct effects with counterfactual inference techniques for unbiased recommendations. All codes and datasets are available at <https://github.com/Stevenn9981/PPAC>.|全球流行度偏差(GP 偏差)是指流行项目被推荐的频率大大高于其应有频率的现象，与提供个性化推荐的目标背道而驰，损害了用户体验和推荐准确性。人们提出了许多减少 GP 偏差的方法，但都没有注意到 GP 的根本问题，即它从全局的角度考虑所有用户的受欢迎程度，使用单一的流行项目集，因此不能捕捉到个体用户的兴趣。因此，我们提出了一个名为个人知名度(PP)的项目知名度的用户感知版本，它通过考虑具有相似兴趣的用户来为每个用户识别不同的流行项目。作为 PP 模型的个人用户的偏好，它自然有助于产生个性化的推荐和减轻 GP 偏见。为了将 PP 整合到推荐中，我们设计了一个通用的个人知名度反事实(PPAC)框架，该框架很容易适应现有的推荐模型。特别是，PPAC 认识到 PP 和 GP 对建议有直接和间接的影响，并通过反事实推理技术控制对无偏见建议的直接影响。所有代码和数据集都可以在 <  https://github.com/stevenn9981/ppac 获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Debiasing+Recommendation+with+Personal+Popularity)|0|
|[Negative Sampling in Next-POI Recommendations: Observation, Approach, and Evaluation](https://doi.org/10.1145/3589334.3645681)|HongKyun Bae, Yebeen Kim, Hyunjoon Kim, SangWook Kim||To recommend the points of interest (POIs) that a user would check-in next, most deep-learning (DL)-based existing studies have employed random negative (RN) sampling during model training. In this paper, we claim and validate that, as the training proceeds, such an RN sampling in reality performs as sampling easy negative (EN) POIs (i.e., EN sampling) that a user was highly unlikely to check-in at her check-in time point. Furthermore, we verify that EN sampling is more disadvantageous in improving the accuracy than sampling hard negative (HN) POIs (i.e., HN sampling) that a user was highly likely to check-in. To address this limitation, we present the novel concept of the Degree of Positiveness (DoP), which can be formulated by two factors: (i) the degree to which a POI has the characteristics preferred by a user; (ii) the geographical distance between a user and a POI. Then, we propose a new model-training scheme based on HN sampling by using DoP. Using real-world datasets (i.e., NYC, TKY, and Brightkite), we demonstrate that all the state-of-the-art models trained by our scheme showed dramatic improvements in accuracy by up to about 82.8%.|为了推荐用户接下来要检入的兴趣点(POI) ，大多数基于深度学习(DL)的现有研究在模型训练中采用了随机负取样(RN)。在本文中，我们声称并验证，随着培训的进行，这样的 RN 采样实际上表现为采样容易阴性(EN) POI (即 EN 采样) ，用户不太可能在她的签入时间点签入。此外，我们验证了 EN 抽样在提高准确性方面比抽样硬负(HN) POI (即 HN 抽样)更不利，因为用户很可能签到。为了解决这个限制，我们提出了积极程度(DoP)的新概念，可以由两个因素来表述: (i) POI 具有用户首选特征的程度; (ii)用户与 POI 之间的地理距离。然后，利用 DoP 提出了一种新的基于 HN 抽样的模型训练方案。使用真实世界的数据集(例如，NYC、 TKY 和 Brightkite) ，我们证明了所有经过我们的方案训练的最先进的模型在准确性方面显示出了高达82.8% 的显著提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Negative+Sampling+in+Next-POI+Recommendations:+Observation,+Approach,+and+Evaluation)|0|
|[Towards Personalized Privacy: User-Governed Data Contribution for Federated Recommendation](https://doi.org/10.1145/3589334.3645690)|Liang Qu, Wei Yuan, Ruiqi Zheng, Lizhen Cui, Yuhui Shi, Hongzhi Yin||Federated recommender systems (FedRecs) have gained significant attention for their potential to protect user's privacy by keeping user privacy data locally and only communicating model parameters/gradients to the server. Nevertheless, the currently existing architecture of FedRecs assumes that all users have the same 0-privacy budget, i.e., they do not upload any data to the server, thus overlooking those users who are less concerned about privacy and are willing to upload data to get a better recommendation service. To bridge this gap, this paper explores a user-governed data contribution federated recommendation architecture where users are free to take control of whether they share data and the proportion of data they share to the server. To this end, this paper presents a cloud-device collaborative graph neural network federated recommendation model, named CDCGNNFed. It trains user-centric ego graphs locally, and high-order graphs based on user-shared data in the server in a collaborative manner via contrastive learning. Furthermore, a graph mending strategy is utilized to predict missing links in the graph on the server, thus leveraging the capabilities of graph neural networks over high-order graphs. Extensive experiments were conducted on two public datasets, and the results demonstrate the effectiveness of the proposed method.|联邦推荐系统(FedRecs)通过将用户隐私数据保存在本地，并且只与服务器通信模型参数/梯度，从而保护用户隐私，因此受到了广泛关注。然而，目前现有的 FedRecs 架构假设所有用户都有相同的零隐私预算，也就是说，他们不向服务器上传任何数据，从而忽略了那些不太关心隐私并愿意上传数据以获得更好的推荐服务的用户。为了弥合这一差距，本文探索了一种用户治理的数据贡献联邦推荐体系结构，在该体系结构中，用户可以自由控制是否共享数据以及共享数据到服务器的比例。为此，本文提出了一种云设备协同图形神经网络联邦推荐模型 CDCGNNFed。它通过对比学习，以协作的方式在本地培训以用户为中心的自我图形和基于服务器中用户共享数据的高阶图形。此外，利用图修正策略来预测服务器上图中缺失的链接，从而利用图神经网络对高阶图的能力。在两个公共数据集上进行了广泛的实验，实验结果表明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Personalized+Privacy:+User-Governed+Data+Contribution+for+Federated+Recommendation)|0|
|[A Semi-supervised Multi-channel Graph Convolutional Network for Query Classification in E-commerce](https://doi.org/10.1145/3589335.3648302)|Chunyuan Yuan, Ming Pang, Zheng Fang, Xue Jiang, Changping Peng, Zhangang Lin||Query intent classification is an essential module for customers to quickly find desired products on the e-commerce application. Most existing query intent classification methods rely on the users' click behavior as a supervised signal to construct training samples. However, these methods based entirely on posterior labels may lead to serious category imbalance problems because of the Matthew effect in click samples. Compared with popular categories, it is difficult for products under long-tail categories to obtain traffic and user clicks, which makes the models unable to detect users' intent for products under long-tail categories. This in turn aggravates the problem that long-tail categories cannot obtain traffic, forming a vicious circle. In addition, due to the randomness of the user's click, the posterior label is unstable for the query with similar semantics, which makes the model very sensitive to the input, leading to an unstable and incomplete recall of categories. In this paper, we propose a novel Semi-supervised Multi-channel Graph Convolutional Network (SMGCN) to address the above problems from the perspective of label association and semi-supervised learning. SMGCN extends category information and enhances the posterior label by utilizing the similarity score between the query and categories. Furthermore, it leverages the co-occurrence and semantic similarity graph of categories to strengthen the relations among labels and weaken the influence of posterior label instability. We conduct extensive offline and online A/B experiments, and the experimental results show that SMGCN significantly outperforms the strong baselines, which shows its effectiveness and practicality.|查询意图分类是客户在电子商务应用程序中快速查找所需产品的重要模块。现有的查询意图分类方法大多依赖于用户的点击行为作为监督信号来构造训练样本。然而，由于点击样本中的马太效应，这些完全基于后验标签的方法可能会导致严重的类别不平衡问题。与流行类别相比，长尾类别下的产品很难获得流量和用户点击量，这使得模型无法检测出用户对长尾类别下产品的意图。这反过来又加剧了长尾类无法获得流量的问题，形成了一个恶性循环。此外，由于用户点击的随机性，对于语义相似的查询，后验标签是不稳定的，这使得模型对输入非常敏感，导致类别的不稳定和不完全召回。在本文中，我们提出了一个新的半监督多通道图卷积网络(SMGCN) ，从标签关联和半监督学习的角度来解决上述问题。SMGCN 利用查询与类别之间的相似度评分扩展类别信息，增强后验标签。此外，利用类别的共现性和语义相似性图增强了标签之间的关系，减弱了后验标签不稳定性的影响。我们进行了大量的离线和在线 A/B 实验，实验结果表明 SMGCN 的性能明显优于强基线，表明了它的有效性和实用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Semi-supervised+Multi-channel+Graph+Convolutional+Network+for+Query+Classification+in+E-commerce)|0|
|[Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale](https://doi.org/10.1145/3589335.3648304)|Wei Wen, KuangHung Liu, Igor Fedorov, Xin Zhang, Hang Yin, Weiwei Chu, Kaveh Hassani, Mengying Sun, Jiang Liu, Xu Wang, Lin Jiang, Yuxin Chen, Buyun Zhang, Xi Liu, Dehua Cheng, Zhengxing Chen, Guang Zhao, Fangqiu Han, Jiyan Yang, Yuchen Hao, Liang Xiong, WenYen Chen||Neural Architecture Search (NAS) has demonstrated its efficacy in computer vision and potential for ranking systems. However, prior work focused on academic problems, which are evaluated at small scale under well-controlled fixed baselines. In industry system, such as ranking system in Meta, it is unclear whether NAS algorithms from the literature can outperform production baselines because of: (1) scale - Meta ranking systems serve billions of users, (2) strong baselines - the baselines are production models optimized by hundreds to thousands of world-class engineers for years since the rise of deep learning, (3) dynamic baselines - engineers may have established new and stronger baselines during NAS search, and (4) efficiency - the search pipeline must yield results quickly in alignment with the productionization life cycle. In this paper, we present Rankitect, a NAS software framework for ranking systems at Meta. Rankitect seeks to build brand new architectures by composing low level building blocks from scratch. Rankitect implements and improves state-of-the-art (SOTA) NAS methods for comprehensive and fair comparison under the same search space, including sampling-based NAS, one-shot NAS, and Differentiable NAS (DNAS). We evaluate Rankitect by comparing to multiple production ranking models at Meta. We find that Rankitect can discover new models from scratch achieving competitive tradeoff between Normalized Entropy loss and FLOPs. When utilizing search space designed by engineers, Rankitect can generate better models than engineers, achieving positive offline evaluation and online A/B test at Meta scale.|神经结构搜索(NAS)在计算机视觉方面的有效性和排序系统的潜力已得到证实。然而，以前的工作集中在学术问题，这是在小规模评估良好控制的固定基线。在工业系统中，例如 Meta 中的排名系统，目前还不清楚文献中的 NAS 算法是否能够胜过生产基线，因为: (1)规模-Meta 排名系统服务于数十亿用户，(2)强大的基线-基线是自深度学习兴起以来数十到数千名世界级工程师多年来优化的生产模型，(3)动态基线-工程师可能在 NAS 搜索期间建立了新的和更强大的基线，(4)效率-搜索管道必须迅速产生与生产生命周期一致的结果。在本文中，我们介绍了 Rankitect，一个用于 Meta 排序系统的 NAS 软件框架。Rankitect 通过从零开始构建低层次的构建块来构建全新的体系结构。Rankitect 实现并改进了最先进的(SOTA) NAS 方法，以便在同一搜索空间下进行全面和公平的比较，包括基于抽样的 NAS、一次性 NAS 和可微 NAS (DNAS)。我们评估 Rankitect 通过比较在 Meta 的多个生产排名模型。我们发现 Rankitect 可以从头开始发现新的模型，实现归一化熵损失和 FLOP 之间的竞争权衡。当利用工程师设计的搜索空间，Rankitect 可以产生比工程师更好的模型，实现积极的离线评价和在线 A/B 测试在 Meta 规模。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rankitect:+Ranking+Architecture+Search+Battling+World-class+Engineers+at+Meta+Scale)|0|
|[Aligned Side Information Fusion Method for Sequential Recommendation](https://doi.org/10.1145/3589335.3648308)|Shuhan Wang, Bin Shen, Xu Min, Yong He, Xiaolu Zhang, Liang Zhang, Jun Zhou, Linjian Mo||Combining contextual information (i.e., side information) of items beyond IDs has become an important way to improve the performance in recommender systems. Existing self-attention-based side information fusion methods can be categorized into early, late, and hybrid fusion. In practice, naive early fusion may interfere with the representation of IDs, resulting in negative effects, while late fusion misses effective interactions between IDs and side information. Some hybrid methods have been proposed to address these issues, but they only utilize side information in calculating attention scores, which may lead to information loss. To harness the full potential of side information without noisy interference, we propose an <u>A</u>ligned <u>S</u>ide <u>I</u>nformation <u>F</u>usion (ASIF) method for sequential recommendation, consisting of two parts: Fused Attention with Untied Positions and Representation Alignment. Specifically, we first decouple the positions to exclude the noisy interference in the attention scores. Secondly, we adopt the contrastive objective to maintain the semantic consistency between IDs and side information and then employ orthogonal decomposition to extract the homogeneous parts. By aligning the representations and fusing them together, ASIF makes full use of the side information without interfering with IDs. Offline experimental results on four datasets demonstrate the superiority of ASIF. Additionally, we successfully deployed the model in Alipay's advertising system and achieved 1.09% and 1.86% improvements on clicks and Cost Per Mille (CPM).|合并 ID 之外的条目的上下文信息(即边信息)已经成为提高推荐系统性能的重要方法。现有的基于自注意的边缘信息融合方法可分为早期融合、晚期融合和混合融合。在实践中，幼稚的早期融合可能干扰 ID 的表示，导致负面影响，而晚期融合错过了 ID 和侧信息之间的有效交互作用。针对这些问题，人们提出了一些混合方法，但这些方法只利用侧信息来计算注意分数，从而可能导致信息丢失。为了在没有噪音干扰的情况下充分利用侧面信息的潜力，我们提出了一种顺序推荐的信息融合(ASIF)方法，该方法由两部分组成: 融合注意力与不绑定位置和表示对齐。具体来说，我们首先解耦位置，以排除噪声干扰的注意分数。其次，采用对比目标来保持 ID 与边信息之间的语义一致性，然后采用正交分解来提取同质部分;。通过对齐表示并将它们融合在一起，ASIF 充分利用了侧信息，而不会干扰 ID。在四个数据集上的离线实验结果表明了 ASIF 算法的优越性。此外，我们成功地在支付宝的广告系统中部署了该模型，在点击量和每公里成本(CPM)上分别实现了1.09% 和1.86% 的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Aligned+Side+Information+Fusion+Method+for+Sequential+Recommendation)|0|
|[Discrete Conditional Diffusion for Reranking in Recommendation](https://doi.org/10.1145/3589335.3648313)|Xiao Lin, Xiaokai Chen, Chenyang Wang, Hantao Shu, Linfeng Song, Biao Li, Peng Jiang||Reranking plays a crucial role in modern multi-stage recommender systems by rearranging the initial ranking list to model interplay between items. Considering the inherent challenges of reranking such as combinatorial searching space, some previous studies have adopted the evaluator-generator paradigm, with a generator producing feasible sequences and a evaluator selecting the best one based on estimated listwise utility. Inspired by the remarkable success of diffusion generative models, this paper explores the potential of diffusion models for generating high-quality sequences in reranking. However, we argue that it is nontrivial to take diffusion models as the generator in the context of recommendation. Firstly, diffusion models primarily operate in continuous data space, differing from the discrete data space of item permutations. Secondly, the recommendation task is different from conventional generation tasks as the purpose of recommender systems is to fulfill user interests. Lastly, real-life recommender systems require efficiency, posing challenges for the inference of diffusion models. To overcome these challenges, we propose a novel Discrete Conditional Diffusion Reranking (DCDR) framework for recommendation. DCDR extends traditional diffusion models by introducing a discrete forward process with tractable posteriors, which adds noise to item sequences through step-wise discrete operations (e.g., swapping). Additionally, DCDR incorporates a conditional reverse process that generates item sequences conditioned on expected user responses. Extensive offline experiments conducted on public datasets demonstrate that DCDR outperforms state-of-the-art reranking methods. Furthermore, DCDR has been deployed in a real-world video app with over 300 million daily active users, significantly enhancing online recommendation quality.|重新排序在现代多阶段推荐系统中起着至关重要的作用，它通过重新排列初始排序列表来模拟项目之间的相互作用。考虑到组合搜索空间等重新排序的内在挑战，以前的一些研究采用了评价者-生成器范式，生成器生成可行序列，评价者根据估计的列表效用选择最佳序列。受扩散生成模型的巨大成功的启发，本文探讨了扩散模型在重新排序中生成高质量序列的潜力。然而，我们认为，在推荐的上下文中，将扩散模型作为生成器是不平凡的。首先，扩散模型主要在连续数据空间中运行，不同于项目排列的离散数据空间。其次，推荐任务不同于传统的生成任务，推荐系统的目的是满足用户的兴趣。最后，现实生活中的推荐系统需要效率，对扩散模型的推理提出了挑战。为了克服这些挑战，我们提出了一个新的离散条件扩散重排序(DCDR)框架的推荐。DCDR 扩展了传统的扩散模型，引入了一个离散的前向过程和易处理的后向过程，通过分步离散操作(例如交换)给项目序列增加了噪声。此外，DCDR 还包含一个条件反向过程，该过程根据预期的用户响应生成项目序列。在公共数据集上进行的大量离线实验表明，DCDR 优于最先进的重新排序方法。此外，DCDR 已经部署在一个现实世界的视频应用程序中，每天有超过3亿的活跃用户，大大提高了在线推荐的质量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Discrete+Conditional+Diffusion+for+Reranking+in+Recommendation)|0|
|[A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model](https://doi.org/10.1145/3589335.3648315)|Hao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, Yifan Zeng||In online advertising scenario, sellers often create multiple creatives to provide comprehensive demonstrations, making it essential to present the most appealing design to maximize the Click-Through Rate (CTR). However, sellers generally struggle to consider users preferences for creative design, leading to the relatively lower aesthetics and quantities compared to Artificial Intelligence (AI)-based approaches. Traditional AI-based approaches still face the same problem of not considering user information while having limited aesthetic knowledge from designers. In fact that fusing the user information, the generated creatives can be more attractive because different users may have different preferences. To optimize the results, the generated creatives in traditional methods are then ranked by another module named creative ranking model. The ranking model can predict the CTR score for each creative considering user features. However, the two above stages are regarded as two different tasks and are optimized separately. In this paper, we proposed a new automated Creative Generation pipeline for Click-Through Rate (CG4CTR) with the goal of improving CTR during the creative generation stage. Our contributions have 4 parts: 1) The inpainting mode in stable diffusion is firstly applied to creative generation task in online advertising scene. A self-cyclic generation pipeline is proposed to ensure the convergence of training. 2) Prompt model is designed to generate individualized creatives for different user groups, which can further improve the diversity and quality. 3) Reward model comprehensively considers the multimodal features of image and text to improve the effectiveness of creative ranking task, and it is also critical in self-cyclic pipeline. 4) The significant benefits obtained in online and offline experiments verify the significance of our proposed method.|在网络广告中，卖家经常创造出多种创意来提供全面的展示，因此展示最具吸引力的设计来最大限度地提高点进率是必不可少的。然而，与基于人工智能(AI)的方法相比，卖家通常难以考虑用户对创意设计的偏好，导致相对较低的审美和数量。传统的基于人工智能的方法仍然面临着同样的问题，没有考虑用户信息，同时从设计师有限的美学知识。事实上，融合用户信息，生成的创意可以更具吸引力，因为不同的用户可能有不同的偏好。为了优化结果，在传统方法中生成的创意，然后排名的另一个模块称为创意排名模型。该排名模型可以预测每个创意的 CTR 得分考虑用户的特点。然而，上述两个阶段被视为两个不同的任务，并分别进行了优化。在这篇文章中，我们提出了一个新的自动化创意生成流水线(cg4CTR)点进率，目标是在创意生成阶段提高点击率。本文的工作主要包括四个部分: 1)首次将稳定扩散的修补模式应用于网络广告场景的创意生成任务。为了保证训练的收敛性，提出了一种自循环发电流水线。2)提示模型针对不同的用户群体生成个性化的创意，从而进一步提高用户群体的多样性和质量。3)奖励模型综合考虑了图像和文本的多模态特征，提高了创造性排序任务的有效性，在自循环流水线中起着关键作用。4)在在线和离线实验中获得的显著好处验证了我们提出的方法的重要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+New+Creative+Generation+Pipeline+for+Click-Through+Rate+with+Stable+Diffusion+Model)|0|
|[NoteLLM: A Retrievable Large Language Model for Note Recommendation](https://doi.org/10.1145/3589335.3648314)|Chao Zhang, Shiwei Wu, Haoxin Zhang, Tong Xu, Yan Gao, Yao Hu, Enhong Chen||People enjoy sharing "notes" including their experiences within online communities. Therefore, recommending notes aligned with user interests has become a crucial task. Existing online methods only input notes into BERT-based models to generate note embeddings for assessing similarity. However, they may underutilize some important cues, e.g., hashtags or categories, which represent the key concepts of notes. Indeed, learning to generate hashtags/categories can potentially enhance note embeddings, both of which compress key note information into limited content. Besides, Large Language Models (LLMs) have significantly outperformed BERT in understanding natural languages. It is promising to introduce LLMs into note recommendation. In this paper, we propose a novel unified framework called NoteLLM, which leverages LLMs to address the item-to-item (I2I) note recommendation. Specifically, we utilize Note Compression Prompt to compress a note into a single special token, and further learn the potentially related notes' embeddings via a contrastive learning approach. Moreover, we use NoteLLM to summarize the note and generate the hashtag/category automatically through instruction tuning. Extensive validations on real scenarios demonstrate the effectiveness of our proposed method compared with the online baseline and show major improvements in the recommendation system of Xiaohongshu.|人们喜欢分享“笔记”，包括他们在网上社区的经历。因此，推荐符合用户兴趣的注意事项已成为一项关键任务。现有的在线方法仅将注释输入到基于 BERT 的模型中，以生成注释嵌入以评估相似性。然而，他们可能没有充分利用一些重要的线索，例如，标签或类别，这代表了笔记的关键概念。事实上，学习生成 # 标签/类别可以潜在地增强笔记的嵌入，这两者都可以将关键笔记信息压缩到有限的内容中。此外，大型语言模型(LLM)在理解自然语言方面明显优于 BERT。有希望将 LLM 引入票据推荐中。在本文中，我们提出了一个新的统一框架 NoteLLM，它利用 LLM 来处理项到项(I2I)注释推荐。具体来说，我们利用 Note CompressionPrompt 将一个音符压缩成一个单独的特殊标记，并通过对比学习的方法进一步学习潜在相关音符的嵌入。此外，我们使用 NoteLLM 来汇总注释，并通过指令调优自动生成 hashtag/type。对实际情景的广泛验证表明，与在线基准相比，我们建议的方法是有效的，并显示 Xiaohongshu 的推荐系统有了重大改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NoteLLM:+A+Retrievable+Large+Language+Model+for+Note+Recommendation)|0|
|[Lightweight GCN Encoder and Sequential Decoder for Multi-Candidate Carpooling Route Planning in Road Network](https://doi.org/10.1145/3589335.3648328)|Yucen Gao, Li Ma, Zhemeng Yu, Songjian Zhang, Jun Fang, Xiaofeng Gao, Guihai Chen||Carpooling Route Planning (CRP) has become an important issue with the growth of low-carbon traffic systems. We investigate a meaningful and challenging scenario for CRP in industry, where each passenger may have several potential positions to get on and off the car. Traditional graph search algorithms or indexing methods usually consume a lot of time and space or perform poorly. In this paper, we propose an end-to-end encoder-decoder model to plan a route for each many-to-one carpooling order with various data-driven mechanisms such as graph partitioning and feature crossover. The encoder is a filter-integrated Graph Convolution Network with external information fusion combining a supervised pre-training classification task, while the latter mimics a pointer network with a rule-based mask mechanism and a domain feature crossover module. We validate the effectiveness and efficiency of our model based on both synthetic and real-world datasets.|随着低碳交通系统的发展，拼车路径规划(CRP)已成为一个重要课题。我们调查了一个有意义的和具有挑战性的情况下的 CRP 在工业，其中每个乘客可能有几个潜在的位置上车和下车。传统的图形搜索算法或索引方法通常会消耗大量的时间和空间，或者性能较差。在本文中，我们提出一个端到端的编码器-解码器模型，以规划一个路由为每个多对一的拼车订单与各种数据驱动的机制，如图划分和特征交叉。该编码器采用外部信息融合的滤波集成图卷积网络，结合有监督的预训练分类任务，而后者采用基于规则的掩码机制和领域特征交叉模块来模拟指针网络。我们验证了我们的模型的有效性和效率的基础上，合成和真实世界的数据集。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lightweight+GCN+Encoder+and+Sequential+Decoder+for+Multi-Candidate+Carpooling+Route+Planning+in+Road+Network)|0|
|[PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction](https://doi.org/10.1145/3589335.3648329)|Yuanbo Gao, Peng Lin, Dongyue Wang, Feng Mei, Xiwei Zhao, Sulong Xu, Jinghe Hu||Click-through rate (CTR) prediction is a core task in recommender systems. Existing methods (IDRec for short) rely on unique identities to represent distinct users and items that have prevailed for decades. On one hand, IDRec often faces significant performance degradation on cold-start problem; on the other hand, IDRec cannot use longer training data due to constraints imposed by iteration efficiency. Most prior studies alleviate the above problems by introducing pre-trained knowledge(e.g. pre-trained user model or multi-modal embeddings). However, the explosive growth of online latency can be attributed to the huge parameters in the pre-trained model. Therefore, most of them cannot employ the unified model of end-to-end training with IDRec in industrial recommender systems, thus limiting the potential of the pre-trained model. To this end, we propose a Pre-trained Plug-in CTR Model, namely PPM. PPM employs multi-modal features as input and utilizes large-scale data for pre-training. Then, PPM is plugged in IDRec model to enhance unified model's performance and iteration efficiency. Upon incorporating IDRec model, certain intermediate results within the network are cached, with only a subset of the parameters participating in training and serving. Hence, our approach can successfully deploy an end-to-end model without causing huge latency increases. Comprehensive offline experiments and online A/B testing at JD E-commerce demonstrate the efficiency and effectiveness of PPM.|点进率预测是推荐系统的核心任务。现有的方法(简称 IDRec)依赖于独特的标识来表示流行了几十年的不同用户和项目。一方面，IDRec 在冷启动问题上经常面临严重的性能下降; 另一方面，由于迭代效率的限制，IDRec 不能使用更长的训练数据。大多数先前的研究通过引入预先训练的知识(例如预先训练的用户模型或多模态嵌入)来缓解上述问题。然而，在线延迟的爆炸性增长可归因于预训练模型中的大量参数。因此，它们中的大多数无法在工业推荐系统中使用 IDRec 的端到端培训统一模型，从而限制了预先培训模型的潜力。为此，我们提出了一个预训练的插件点击率模型，即 PPM。PPM 采用多模态特征作为输入，利用大规模数据进行预训练。然后，在 IDRec 模型中插入 PPM，以提高统一模型的性能和迭代效率。在加入 IDRec 模型后，网络中的某些中间结果被缓存，只有一部分参数参与训练和服务。因此，我们的方法可以成功地部署端到端模型，而不会造成巨大的延迟增加。JD 电子商务的全面离线实验和在线 A/B 测试证明了 PPM 的效率和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PPM+:+A+Pre-trained+Plug-in+Model+for+Click-through+Rate+Prediction)|0|
|[Towards Robustness Analysis of E-Commerce Ranking System](https://doi.org/10.1145/3589335.3648335)|Ningfei Wang, Yupin Huang, Han Cheng, Jiri Gesi, Xiaojie Wang, Vivek Mittal||Information retrieval (IR) is a pivotal component in various applications. Recent advances in machine learning (ML) have enabled the integration of ML algorithms into IR, particularly in ranking systems. While there is a plethora of research on the robustness of ML-based ranking systems, these studies largely neglect commercial e-commerce systems and fail to establish a connection between real-world and manipulated query relevance. In this paper, we present the first systematic measurement study on the robustness of e-commerce ranking systems. We define robustness as the consistency of ranking outcomes for semantically identical queries. To quantitatively analyze robustness, we propose a novel metric that considers both ranking position and item-specific information that are absent in existing metrics. Our large-scale measurement study with real-world data from e-commerce retailers reveals an open opportunity to measure and improve robustness since semantically identical queries often yield inconsistent ranking results. Based on our observations, we propose several solution directions to enhance robustness, such as the use of Large Language Models. Note that the issue of robustness discussed herein does not constitute an error or oversight. Rather, in scenarios where there exists a vast array of choices, it is feasible to present a multitude of products in various permutations, all of which could be equally appealing. However, this extensive selection may lead to customer confusion. As e-commerce retailers use various techniques to improve the quality of search results, we hope that this research offers valuable guidance for measuring the robustness of the ranking systems.|信息检索(IR)是各种应用中的关键组成部分。机器学习(ML)的最新进展使得 ML 算法能够集成到 IR 中，特别是在排序系统中。尽管对基于机器学习的排序系统的稳健性有大量的研究，但这些研究很大程度上忽视了商业电子商务系统，并且未能在现实世界和被操纵的查询相关性之间建立联系。本文首次对电子商务排名系统的稳健性进行了系统的度量研究。我们将鲁棒性定义为语义相同查询的排序结果的一致性。为了定量分析稳健性，我们提出了一种新的度量方法，该方法同时考虑了排名位置和现有度量方法中不存在的项目特定信息。我们对来自电子商务零售商的真实世界数据进行的大规模测量研究揭示了测量和提高健壮性的公开机会，因为语义相同的查询经常产生不一致的排名结果。基于我们的观察，我们提出了几个增强健壮性的解决方案方向，例如使用大型语言模型。请注意，本文讨论的健壮性问题并不构成错误或疏忽。相反，在存在大量选择的情况下，以不同的排列方式呈现多种产品是可行的，所有这些产品都同样具有吸引力。然而，这种广泛的选择可能会导致客户的困惑。随着电子商务零售商使用各种技术来提高搜索结果的质量，我们希望这项研究能够为衡量排名系统的稳健性提供有价值的指导。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Robustness+Analysis+of+E-Commerce+Ranking+System)|0|
|[End-to-End Graph-Sequential Representation Learning for Accurate Recommendations](https://doi.org/10.1145/3589335.3651499)|Vladimir Baikalov, Evgeny Frolov||Recent recommender system advancements have focused on developing sequence-based and graph-based approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item recommendation tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter aims to capture indirect dependencies across the interactions graph. This paper presents a novel multi-representational learning framework exploiting these two paradigms' synergies. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and graph components with the proposed framework significantly improves recommendations performance.|最近的推荐系统进展集中在开发基于序列和基于图表的方法。事实证明，这两种方法在建模行为数据中错综复杂的关系时都很有用，在保持良好可伸缩性的同时，在个性化排名和下一项推荐任务中产生了有希望的结果。然而，它们从数据中捕获非常不同的信号。前一种方法通过与最近项目的有序交互直接表示用户，而后一种方法旨在捕获交互图中的间接依赖关系。本文提出了一个新的多表征学习框架，利用这两个范式的协同作用。我们对几个数据集的实证评估表明，使用所提出的框架对顺序和图形组件进行相互训练可以显著提高推荐性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=End-to-End+Graph-Sequential+Representation+Learning+for+Accurate+Recommendations)|0|
|[DiffuRetrieval: A Chain-of-Thought Enhanced Diffusion Retrieval in Sponsored Search](https://doi.org/10.1145/3589335.3651491)|Yadong Zhang, Siyu Lu, Qiang Liu, Xingxing Wang||Embedding-based Retrieval (EBR) system is a fundamental component that supplies candidates for downstream ranking mechanisms in the sponsored search system. To enhance search experience and ensure effective retrieval, EBR usually accounts for various objectives including the semantic relevance and personalization of search results. However, traditional multi-task EBR models ignore the intrinsic progressive relationship between relevant and personalized candidates during a search. Recognizing this gap, we make the very first attempt to utilize the representation generation capabilities of Diffusion Models in EBR. In this paper, we present a novel model DiffuRetrieval to address the progressive objectives for high-quality item retrieval. In forward process, DiffuRetrieval incrementally corrupts item representations through controlled noise injection. Conversely, in reverse process, we refine the representations based on query information in a chain-of-thought manner, initially establishing coarse-grained relevance and progressively moving towards fine-grained personalization. Online A/B tests on Meituan sponsored search platform demonstrate that our approach markedly surpasses the baselines, delivering substantial improvements in revenue, relevance and personalization.|基于嵌入式的检索(EBR)系统是为赞助商搜索系统中的下游排序机制提供候选者的基本组件。为了增强搜索体验并确保有效的检索，EBR 通常考虑各种目标，包括搜索结果的语义相关性和个性化。然而，传统的多任务 EBR 模型忽视了搜索过程中相关候选人和个性化候选人之间内在的进步关系。认识到这一差距，我们首次尝试在 EBR 中利用扩散模型的表示生成功能。本文提出了一种新的区分检索模型来解决高质量项目检索的渐进目标问题。在前向过程中，扬声检索通过受控噪声注入增量损坏项表示。相反，在反向过程中，我们以思想链的方式精化基于查询信息的表示，最初建立粗粒度的相关性，并逐步向细粒度的个性化发展。在美团赞助的搜索平台上进行的在线 A/B 测试表明，我们的方法明显超越了基线，在收入、相关性和个性化方面带来了实质性的改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DiffuRetrieval:+A+Chain-of-Thought+Enhanced+Diffusion+Retrieval+in+Sponsored+Search)|0|
|[The Impact of Cluster Centroid and Text Review Embeddings on Recommendation Methods](https://doi.org/10.1145/3589335.3651570)|Peter Dolog, Ylli Sadikaj, Yllka Velaj, Andreas Stephan, Benjamin Roth, Claudia Plant||Recommendation systems often neglect global patterns that can be provided by clusters of similar items or even additional information such as text. Therefore, we study the impact of integrating clustering embeddings, review embeddings, and their combinations with embeddings obtained by a recommender system. Our work assesses the performance of this approach across various state-of-the-art recommender system algorithms. Our study highlights the improvement of recommendation performance through clustering, particularly evident when combined with review embeddings, and the enhanced performance of neural methods when incorporating review embeddings.|推荐系统往往忽略了类似项目集群甚至文本等附加信息可以提供的全局模式。因此，我们研究了集群嵌入、评论嵌入以及它们与推荐系统获得的嵌入的结合的影响。我们的工作评估了这种方法在各种最先进的推荐系统算法中的性能。我们的研究强调了通过聚类来提高推荐性能，特别是在与评论嵌入相结合时，以及在与评论嵌入相结合时提高神经方法的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Impact+of+Cluster+Centroid+and+Text+Review+Embeddings+on+Recommendation+Methods)|0|
|[Rethinking Sequential Relationships: Improving Sequential Recommenders with Inter-Sequence Data Augmentation](https://doi.org/10.1145/3589335.3651552)|Yang Jiao, Fan Yang, Yetian Chen, Yan Gao, Jia Liu, Yi Sun||Predicting customer preferences for each item is a prerequisite module for most recommender systems in e-commerce. However, the sparsity of behavioral data is often a challenge to learn accurate prediction models. Given millions of items, each customer may only be able to interact with a small subset of them over time. This sparse behavioral data is insufficient to represent item-customer and item-item relations for a machine learning model to digest, resulting in limited prediction accuracy that hinders recommendation performance. To mitigate this issue, this study introduces an inter-sequence data augmentation method, SDAinter, that enhances data density by leveraging cross-customer behavioral patterns to enrich item relations. Tested on three public and one proprietary e-commerce dataset, SDAinter significantly increases data density, leading to notable improvements in both evaluation and business metrics. Our findings demonstrate SDAinter's effectiveness and its potential to complement existing data augmentation strategies in recommender systems. See https://github.com/ML-apollo/SDA_inter.|预测顾客对每个商品的偏好是电子商务中大多数推荐系统的一个先决条件。然而，稀疏的行为数据往往是一个挑战，学习准确的预测模型。给定数以百万计的项目，随着时间的推移，每个客户可能只能与其中的一小部分进行交互。这种稀疏的行为数据不足以表示机器学习模型要消化的项目-客户和项目-项目关系，从而导致有限的预测准确性，阻碍了推荐性能。为了缓解这一问题，本研究引入了一种序列间数据增强方法 SDAinter，该方法通过利用跨客户行为模式来丰富项目关系来增强数据密度。在三个公共的和一个专有的电子商务数据集上进行了测试，SDAinter 显著地增加了数据密度，导致了评估和业务指标的显著改进。我们的研究结果表明 SDAinter 的有效性和它的潜力，以补充现有的数据增强策略在推荐系统。Https://github.com/ml-apollo/sda_inter.|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Sequential+Relationships:+Improving+Sequential+Recommenders+with+Inter-Sequence+Data+Augmentation)|0|
|[Unsupervised Search Algorithm Configuration using Query Performance Prediction](https://doi.org/10.1145/3589335.3651579)|Haggai Roitman||Search engine configuration can be quite difficult for inexpert developers. Instead, an auto-configuration approach can be used to speed up development time. Yet, such an automatic process usually requires relevance labels to train a supervised model. In this work, we suggest a simple solution based on query performance prediction that requires no relevance labels but only a sample of queries in a given domain. Using two example usecases we demonstrate the merits of our solution.|搜索 engine configuration 对于不专业的开发者来说是相当困难的。相反，可以使用自动配置方法来加快开发时间。然而，这样一个自动化的过程通常需要相关标签来训练一个受监督的模型。在这项工作中，我们提出了一个基于查询性能预测的简单解决方案，它不需要相关标签，只需要给定域中的查询样本。通过两个例子，我们证明了我们的解决方案的优点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Search+Algorithm+Configuration+using+Query+Performance+Prediction)|0|
|[I-CoSim: Efficient Dynamic CoSimRank Retrieval on Evolving Networks](https://doi.org/10.1145/3589335.3651523)|Xiaoyu Xu, Weiren Yu||CoSimRank, a favorable measure for assessing node similarity based on graphs, faces computational challenges on real evolving graphs. The best-of-breed algorithm, D-CoSim, for incremental CoSimRank search evaluates similarity changes by summing dot products between two vectors. These vectors are iteratively generated from scratch in the original high-dimensional space, leading to significant costs. In this paper, we propose I-CoSim, a novel efficient dynamic CoSimRank algorithm for evolving graphs. I-CoSim resorts to two low-dimensional Krylov subspaces and maximally reuses previously computed similarities in the original graph, which substantially expedites CoSimRank search on evolving graphs. We also theoretically provide an error bound on the I-CoSim estimation with guaranteed accuracy. Experimental results on real datasets show that I-CoSim is up to 28 times faster than the best-known competitor, with only a slight compromise in accuracy.|CoSimRank 是一种基于图的节点相似性度量方法，它面临着实际演化图的计算挑战。用于增量 CoSimRank 搜索的最佳种类算法 D-CoSim 通过求和两个向量之间的点积来评估相似性变化。这些向量是在原始的高维空间中从零开始迭代生成的，导致了巨大的成本。在本文中，我们提出了一种新的高效的进化图动态 CoSimRank 算法 I-CoSim。I-CoSim 利用两个低维 Krylov 子空间，最大限度地重用原始图中先前计算出的相似性，从而大大加快了对演化图的 CoSimRank 搜索。理论上还给出了 I-CoSim 估计的误差界，保证了估计的精度。在真实数据集上的实验结果表明，I-CoSim 比最知名的竞争对手快28倍，在精度上只有轻微的折衷。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=I-CoSim:+Efficient+Dynamic+CoSimRank+Retrieval+on+Evolving+Networks)|0|
|[Unlocking the Potential of Health Data with Decentralised Search in Personal Health Datastores](https://doi.org/10.1145/3589335.3651454)|Mohamed Ragab, Yury Savateev, Helen Oliver, Thanassis Tiropanis, Alexandra Poulovassilis, Adriane Chapman, George Roussos||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unlocking+the+Potential+of+Health+Data+with+Decentralised+Search+in+Personal+Health+Datastores)|0|
|[TATKC: A Temporal Graph Neural Network for Fast Approximate Temporal Katz Centrality Ranking](https://doi.org/10.1145/3589334.3645432)|Tianming Zhang, Junkai Fang, Zhengyi Yang, Bin Cao, Jing Fan||Numerous real-world networks are represented as temporal graphs, which capture the dynamics of connections over time. Identifying important nodes on temporal graphs has a plethora of real-life applications, such as information propagation and influential user identification, etc. Temporal Katz centrality, a popular temporal metric, gauges the importance of nodes by taking into account both the number of temporal walks and the timespan between the interactions. The computation of traditional temporal Katz centrality is computationally expensive, especially when applied to massive temporal graphs. Therefore, in this paper, we design a temporal graph neural network to approximate temporal Katz centrality computation. To the best of our knowledge, we are the first to address temporal Katz centrality computation purely from a learning-based perspective. We propose a time-injected self-attention model that consists of two phases. In the first phase, we utilize a time-injected self-attention mechanism to acquire node representations that encompass both structural information and temporal relevance. The second phase is structured as a multi-layer perceptron (MLP) which uses the learned node representation to predict node rankings. Furthermore, normalization and neighbor sampling strategies are integrated into the model to enhance its overall performance. Extensive experiments on real-world networks demonstrate the efficiency and accuracy of TATKC.|许多现实世界的网络被表示为时间图，这些图捕获了随时间变化的连接动态。识别时间图上的重要节点有着广泛的实际应用，如信息传播和有影响力的用户识别等。时间 Katz 中心度是一种流行的时间度量，它通过考虑时间步行的次数和相互作用之间的时间跨度来衡量节点的重要性。传统的时间 Katz 中心性的计算是昂贵的，特别是当应用到大量的时间图。因此，本文设计了一个时态图神经网络来近似计算时态 Katz 中心性。据我们所知，我们是第一个纯粹从基于学习的角度来处理时态 Katz 中心计算的。我们提出了一个由两个阶段组成的时间注入自我注意模型。在第一阶段，我们利用时间注入的自我注意机制来获取包含结构信息和时间相关性的节点表征。第二阶段采用多层感知器(MLP)结构，使用学习节点表示来预测节点排名。此外，归一化和邻居抽样策略的集成模型，以提高其整体性能。在实际网络上的大量实验证明了 TATKC 算法的有效性和准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TATKC:+A+Temporal+Graph+Neural+Network+for+Fast+Approximate+Temporal+Katz+Centrality+Ranking)|0|
|[FairSync: Ensuring Amortized Group Exposure in Distributed Recommendation Retrieval](https://doi.org/10.1145/3589334.3645413)|Chen Xu, Jun Xu, Yiming Ding, Xiao Zhang, Qi Qi||In pursuit of fairness and balanced development, recommender systems (RS) often prioritize group fairness, ensuring that specific groups maintain a minimum level of exposure over a given period. For example, RS platforms aim to ensure adequate exposure for new providers or specific categories of items according to their needs. Modern industry RS usually adopts a two-stage pipeline: stage-1 (retrieval stage) retrieves hundreds of candidates from millions of items distributed across various servers, and stage-2 (ranking stage) focuses on presenting a small-size but accurate selection from items chosen in stage-1. Existing efforts for ensuring amortized group exposures focus on stage-2, however, stage-1 is also critical for the task. Without a high-quality set of candidates, the stage-2 ranker cannot ensure the required exposure of groups. Previous fairness-aware works designed for stage-2 typically require accessing and traversing all items. In stage-1, however, millions of items are distributively stored in servers, making it infeasible to traverse all of them. How to ensure group exposures in the distributed retrieval process is a challenging question. To address this issue, we introduce a model named FairSync, which transforms the problem into a constrained distributed optimization problem. Specifically, FairSync resolves the issue by moving it to the dual space, where a central node aggregates historical fairness data into a vector and distributes it to all servers. To trade off the efficiency and accuracy, the gradient descent technique is used to periodically update the parameter of the dual vector. The experiment results on two public recommender retrieval datasets showcased that FairSync outperformed all the baselines, achieving the desired minimum level of exposures while maintaining a high level of retrieval accuracy.|为了追求公平和均衡发展，推荐系统(RS)经常优先考虑群体公平性，确保特定群体在给定的时间内保持最低的曝光水平。例如，RS 平台旨在确保新供应商或根据其需要的具体项目类别获得足够的曝光。现代工业 RS 通常采用两阶段流水线: 阶段1(检索阶段)从分布在不同服务器上的数百万个项目中检索数百个候选项，阶段2(排名阶段)侧重于从阶段1中选择的项目中提供一个小型但准确的选择。现有的努力，以确保摊销群体暴露集中在第二阶段，然而，第一阶段也是至关重要的任务。如果没有一组高质量的候选人，阶段2的排名不能确保所需的群体曝光。以前为阶段2设计的公平感知工作通常需要访问和遍历所有项。然而，在阶段1中，数以百万计的条目分布式地存储在服务器中，这使得遍历所有条目变得不可行。如何保证分布式检索过程中的群体曝光是一个具有挑战性的问题。为了解决这个问题，我们引入了一个名为 FairSync 的模型，它将问题转化为一个受限的分布式最佳化问题。具体来说，FairSync 通过将其移动到双空间来解决这个问题，在双空间中，一个中央节点将历史公平数据聚合到一个向量中，并将其分发到所有服务器。为了权衡效率和准确性，梯度下降法技术被用来定期更新双向量的参数。在两个公共推荐检索数据集上的实验结果表明，FairSync 的性能优于所有基线，在保持较高的检索准确率的同时，达到了预期的最小曝光水平。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FairSync:+Ensuring+Amortized+Group+Exposure+in+Distributed+Recommendation+Retrieval)|0|
|[Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism](https://doi.org/10.1145/3589334.3645482)|Yujia Zhou, Qiannan Zhu, Jiajie Jin, Zhicheng Dou||Traditional search engines usually provide identical search results for all users, overlooking individual preferences. To counter this limitation, personalized search has been developed to re-rank results based on user preferences derived from query logs. Deep learning-based personalized search methods have shown promise, but they rely heavily on abundant training data, making them susceptible to data sparsity challenges. This paper proposes a Cognitive Personalized Search (CoPS) model, which integrates Large Language Models (LLMs) with a cognitive memory mechanism inspired by human cognition. CoPS employs LLMs to enhance user modeling and user search experience. The cognitive memory mechanism comprises sensory memory for quick sensory responses, working memory for sophisticated cognitive responses, and long-term memory for storing historical interactions. CoPS handles new queries using a three-step approach: identifying re-finding behaviors, constructing user profiles with relevant historical information, and ranking documents based on personalized query intent. Experiments show that CoPS outperforms baseline models in zero-shot scenarios.|传统的搜索引擎通常为所有用户提供相同的搜索结果，忽略了个人偏好。为了克服这个限制，个性化检索已经开发出来，可以根据从查询日志中得到的用户偏好对结果进行重新排序。基于深度学习的个性化检索训练方法已经显示出了前景，但它们严重依赖于大量的训练数据，因此容易受到数据稀少的挑战。本文提出了一个认知个性化检索模型(CoPS) ，它将大语言模型(LLMs)与受人类认知启发的认知记忆机制结合在一起。CoPS 使用 LLM 来增强用户建模和用户搜索体验。认知记忆机制包括快速感觉反应的感觉记忆、复杂认知反应的工作记忆和存储历史互动的长期记忆。CoPS 使用三个步骤来处理新的查询: 识别重新查找行为，构建具有相关历史信息的用户配置文件，以及基于个性化查询意图的文档排序。实验结果表明，在零射击情况下，CoPS 的性能优于基线模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cognitive+Personalized+Search+Integrating+Large+Language+Models+with+an+Efficient+Memory+Mechanism)|0|
|[Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search](https://doi.org/10.1145/3589334.3645483)|Yifei Yuan, Clemencia Siro, Mohammad Aliannejadi, Maarten de Rijke, Wai Lam||In mixed-initiative conversational search systems, clarifying questions are used to help users who struggle to express their intentions in a single query. These questions aim to uncover user's information needs and resolve query ambiguities. We hypothesize that in scenarios where multimodal information is pertinent, the clarification process can be improved by using non-textual information. Therefore, we propose to add images to clarifying questions and formulate the novel task of asking multimodal clarifying questions in open-domain, mixed-initiative conversational search systems. To facilitate research into this task, we collect a dataset named Melon that contains over 4k multimodal clarifying questions, enriched with over 14k images. We also propose a multimodal query clarification model named Marto and adopt a prompt-based, generative fine-tuning strategy to perform the training of different stages with different prompts. Several analyses are conducted to understand the importance of multimodal contents during the query clarification phase. Experimental results indicate that the addition of images leads to significant improvements of up to 90 images. Extensive analyses are also performed to show the superiority of Marto compared with discriminative baselines in terms of effectiveness and efficiency.|在混合主动会话搜索系统中，澄清问题用于帮助那些在单个查询中难以表达意图的用户。这些问题旨在揭示用户的信息需求和解决查询模糊性。我们假设在多模态信息相关的场景中，可以通过使用非文本信息来改进澄清过程。因此，我们建议在会话搜索系统中增加图像来解决问题，并提出在开放领域、混合主动的会话搜索系统中提出多模态问题的新任务。为了促进这项任务的研究，我们收集了一个名为 Melon 的数据集，其中包含了超过4k 个多模式澄清问题，丰富了超过14k 个图像。我们还提出了一个名为 Marto 的多模式查询澄清模型，并采用基于提示的生成式微调策略对不同提示的不同阶段进行训练。为了理解在查询澄清阶段多通道内容的重要性，进行了一些分析。实验结果表明，增加图像导致显着改善高达90图像。还进行了广泛的分析，以显示 Marto 相对于有效性和效率的判别基线的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Asking+Multimodal+Clarifying+Questions+in+Mixed-Initiative+Conversational+Search)|0|
|[Benchmark and Neural Architecture for Conversational Entity Retrieval from a Knowledge Graph](https://doi.org/10.1145/3589334.3645676)|Mona Zamiri, Yao Qiang, Fedor Nikolaev, Dongxiao Zhu, Alexander Kotov||This paper introduces a novel information retrieval (IR) task of Conversational Entity Retrieval from a Knowledge Graph (CER-KG), which extends non-conversational entity retrieval from a knowledge graph (KG) to the conversational scenario. The user queries in CER-KG dialog turns may rely on the results of the preceding turns, which are KG entities. Similar to the conversational document IR, CER-KG can be viewed as a sequence of interrelated ranking tasks. To enable future research on CER-KG, we created QBLink-KG, a publicly available benchmark that was adapted from QBLink, a benchmark for text-based conversational reading comprehension of Wikipedia. As an initial approach to CER-KG, we experimented with Transformer- and LSTM-based query encoders in combination with the Neural Architecture for Conversational Entity Retrieval (NACER), our proposed feature-based neural architecture for entity ranking in CER-KG. NACER computes the ranking score of a candidate KG entity by taking into account diverse lexical and semantic matching signals between various KG components in its neighborhood, such as entities, categories, and literals, as well as entities in the results of the preceding turns in dialog history. The reported experimental results reveal the key challenges of CER-KG along with the possible directions for new approaches to this task.|本文介绍了一个新颖的知识图中的会话实体检索任务(CER-KG) ，该任务将非会话实体检索从知识图中扩展到会话场景中，从而提高了会话实体检索的信息检索。用户在 CER-KG 对话轮中的查询可能依赖于前面几轮的结果，即 KG 实体。与会话文档 IR 类似，CER-KG 可以看作是一系列相互关联的排序任务。为了将来能够对 CER-KG 进行研究，我们创建了 QBLink-KG，这是一个公开可用的基准，改编自 QBLink，一个基于文本的维基百科会话阅读理解的基准。作为 CER-KG 的一个初始方法，我们使用变压器和 LSTM 为基础的查询编码器结合会话实体检索(NACER)的神经结构进行实验，NACER 是我们提出的用于 CER-KG 中实体排序的基于特征的神经结构。NACER 通过考虑其邻近的各种 KG 组件之间的不同词汇和语义匹配信号(如实体、类别和文字)以及对话历史中前一轮结果中的实体来计算候选 KG 实体的排名得分。报告的实验结果揭示了 CER-KG 的关键挑战以及这项任务的新方法的可能方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Benchmark+and+Neural+Architecture+for+Conversational+Entity+Retrieval+from+a+Knowledge+Graph)|0|
|[A Data-Centric Multi-Objective Learning Framework for Responsible Recommendation Systems](https://doi.org/10.1145/3589334.3645324)|Xu Huang, Jianxun Lian, Hao Wang, Hao Liao, Defu Lian, Xing Xie||Recommendation systems effectively guide users in locating their desired information within extensive content repositories. Generally, a recommendation model is optimized to enhance accuracy metrics from a user utility standpoint, such as click-through rate or matching relevance. However, a responsible industrial recommendation system must address not only user utility (responsibility to users) but also other objectives, including increasing platform revenue (responsibility to platforms), ensuring fairness (responsibility to content creators), and maintaining unbiasedness (responsibility to long-term healthy development). Multi-objective learning is a potent approach for achieving responsible recommendation systems. Nevertheless, current methods encounter two challenges: difficulty in scaling to heterogeneous objectives within a unified framework, and inadequate controllability over objective priority during optimization, leading to uncontrollable solutions. In this paper, we present a data-centric optimization framework, MoRec, which unifies the learning of diverse objectives. MoRec is a tri-level framework: the outer level manages the balance between different objectives, utilizing a proportional-integral-derivative (PID)-based controller to ensure a preset regularization on the primary objective. The middle level transforms objective-aware optimization into data sampling weights using sign gradients. The inner level employs a standard optimizer to update model parameters with the sampled data. Consequently, MoRec can flexibly support various objectives while maintaining the original model intact. Comprehensive experiments on two public datasets and one industrial dataset showcase the effectiveness, controllability, flexibility, and Pareto efficiency of MoRec, making it highly suitable for real-world implementation.|推荐系统有效地指导用户在广泛的内容存储库中查找所需的信息。一般来说，推荐模型会优化，以从用户效用的角度提高准确性指标，比如点进率或匹配相关性。然而，一个负责任的行业推荐系统不仅要解决用户效用(对用户的责任) ，还要解决其他目标，包括增加平台收入(对平台的责任) ，确保公平(对内容创建者的责任) ，保持无偏见(对长期健康发展的责任)。多目标学习是实现负责任推荐系统的有效方法。然而，目前的方法遇到两个挑战: 难以在统一的框架内扩展到异构的目标，以及在优化过程中对目标优先级的可控性不足，导致不可控的解决方案。在本文中，我们提出了一个以数据为中心的优化框架 MoRec，它统一了不同目标的学习。MoRec 是一个三层框架: 外层管理不同目标之间的平衡，利用比例积分微分(PID)为基础的控制器，以确保预设正则化的主要目标。中间层利用符号梯度将目标感知优化转换为数据采样权重。内部级别使用标准优化器用采样数据更新模型参数。因此，MoRec 可以灵活地支持各种目标，同时保持原始模型的完整性。在两个公共数据集和一个工业数据集上进行的全面实验展示了 MoRec 的有效性、可控性、灵活性和帕累托最优，使其非常适合于现实世界的实现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Data-Centric+Multi-Objective+Learning+Framework+for+Responsible+Recommendation+Systems)|0|
|[Collaborative Large Language Model for Recommender Systems](https://doi.org/10.1145/3589334.3645347)|Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li||Recently, there is a growing interest in developing next-generation recommender systems (RSs) based on pretrained large language models (LLMs), fully utilizing their encoded knowledge and reasoning ability. However, the semantic gap between natural language and recommendation tasks is still not well addressed, leading to multiple issues such as spuriously-correlated user/item descriptors, ineffective language modeling on user/item contents, and inefficient recommendations via auto-regression, etc. In this paper, we propose CLLM4Rec, the first generative RS that tightly integrates the LLM paradigm and ID paradigm of RS, aiming to address the above challenges simultaneously. We first extend the vocabulary of pretrained LLMs with user/item ID tokens to faithfully model the user/item collaborative and content semantics. Accordingly, in the pretraining stage, a novel soft+hard prompting strategy is proposed to effectively learn user/item collaborative/content token embeddings via language modeling on RS-specific corpora established from user-item interactions and user/item features, where each document is split into a prompt consisting of heterogeneous soft (user/item) tokens and hard (vocab) tokens and a main text consisting of homogeneous item tokens or vocab tokens that facilitates stable and effective language modeling. In addition, a novel mutual regularization strategy is introduced to encourage the CLLM4Rec to capture recommendation-oriented information from user/item contents. Finally, we propose a novel recommendation-oriented finetuning strategy for CLLM4Rec, where an item prediction head with multinomial likelihood is added to the pretrained CLLM4Rec backbone to predict hold-out items based on the soft+hard prompts established from masked user-item interaction history, where recommendations of multiple items can be generated efficiently.|近年来，基于预先训练的大语言模型(LLM) ，充分利用其编码知识和推理能力，开发下一代推荐系统(RS)越来越受到人们的关注。然而，自然语言和推荐任务之间的语义差距仍然没有得到很好的解决，导致多种问题，如虚假相关的用户/项目描述符，对用户/项目内容无效的语言建模，以及通过自动回归的低效推荐等。在本文中，我们提出了 CLLM4Rec，这是第一个紧密集成了 RS 的 LLM 范式和 ID 范式的生成 RS，旨在同时解决上述挑战。我们首先扩展带有用户/项目 ID 令牌的预训练 LLM 的词汇表，以忠实地建立用户/项目协作和内容语义的模型。因此，在预训练阶段，提出了一种新的软硬件提示策略，通过对基于用户-项目交互和用户/项目特征建立的 RS 特定语料库进行语言建模，有效地学习用户/项目协作/内容令牌嵌入，将每个文档分解为由异构软(用户/项目)令牌和硬(词汇)令牌组成的提示和由同质项目令牌或词汇令牌组成的主文本，以促进稳定有效的语言建模。此外，还引入了一种新的相互正则化策略，以鼓励 CLLM4Rec 从用户/项目内容中捕获面向推荐的信息。最后，我们为 CLLM4Rec 提出了一种新的面向推荐的微调策略，其中将具有多项式可能性的项目预测头添加到预先训练的 CLLM4Rec 骨干中，以基于从掩盖的用户项目交互历史建立的软 + 硬提示来预测坚持项目，其中可以有效地生成多个项目的推荐。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaborative+Large+Language+Model+for+Recommender+Systems)|0|
|[Harnessing Large Language Models for Text-Rich Sequential Recommendation](https://doi.org/10.1145/3589334.3645358)|Zhi Zheng, Wenshuo Chao, Zhaopeng Qiu, Hengshu Zhu, Hui Xiong||Recent advances in Large Language Models (LLMs) have been changing the paradigm of Recommender Systems (RS). However, when items in the recommendation scenarios contain rich textual information, such as product descriptions in online shopping or news headlines on social media, LLMs require longer texts to comprehensively depict the historical user behavior sequence. This poses significant challenges to LLM-based recommenders, such as over-length limitations, extensive time and space overheads, and suboptimal model performance. To this end, in this paper, we design a novel framework for harnessing Large Language Models for Text-Rich Sequential Recommendation (LLM-TRSR). Specifically, we first propose to segment the user historical behaviors and subsequently employ an LLM-based summarizer for summarizing these user behavior blocks. Particularly, drawing inspiration from the successful application of Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) models in user modeling, we introduce two unique summarization techniques in this paper, respectively hierarchical summarization and recurrent summarization. Then, we construct a prompt text encompassing the user preference summary, recent user interactions, and candidate item information into an LLM-based recommender, which is subsequently fine-tuned using Supervised Fine-Tuning (SFT) techniques to yield our final recommendation model. We also use Low-Rank Adaptation (LoRA) for Parameter-Efficient Fine-Tuning (PEFT). We conduct experiments on two public datasets, and the results clearly demonstrate the effectiveness of our approach.|大语言模型(LLM)的最新进展已经改变了推荐系统(RS)的范式。但是，当推荐场景中的条目包含丰富的文本信息时，如在线购物中的产品描述或社交媒体上的新闻标题，LLM 需要更长的文本来全面描述历史用户行为序列。这对基于 LLM 的推荐程序提出了重大挑战，例如超长限制、大量的时间和空间开销以及次优模型性能。为此，在本文中，我们设计了一个新的框架，用于利用大型语言模型进行富文本顺序推荐(LLM-TRSR)。具体来说，我们首先建议分割用户的历史行为，然后使用一个基于 LLM 的总结器来总结这些用户行为块。特别地，借鉴了卷积神经网络(CNN)和递归神经网络(RNN)模型在用户建模中的成功应用，本文介绍了两种独特的文摘技术，分别是层次文摘和递归文摘。然后，我们构建一个包含用户偏好摘要，最近的用户交互和候选项信息的提示文本到基于 LLM 的推荐器中，随后使用监督微调(SFT)技术进行微调以产生我们的最终推荐模型。我们还使用低秩自适应(LoRA)的参数有效微调(PEFT)。我们在两个公共数据集上进行了实验，结果清楚地证明了我们方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Harnessing+Large+Language+Models+for+Text-Rich+Sequential+Recommendation)|0|
|[ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction](https://doi.org/10.1145/3589334.3645396)|Jianghao Lin, Bo Chen, Hangyu Wang, Yunjia Xi, Yanru Qu, Xinyi Dai, Kangning Zhang, Ruiming Tang, Yong Yu, Weinan Zhang||Click-through rate (CTR) prediction has become increasingly indispensable for various Internet applications. Traditional CTR models convert the multi-field categorical data into ID features via one-hot encoding, and extract the collaborative signals among features. Such a paradigm suffers from the problem of semantic information loss. Another line of research explores the potential of pretrained language models (PLMs) for CTR prediction by converting input data into textual sentences through hard prompt templates. Although semantic signals are preserved, they generally fail to capture the collaborative information (e.g., feature interactions, pure ID features), not to mention the unacceptable inference overhead brought by the huge model size. In this paper, we aim to model both the semantic knowledge and collaborative knowledge for accurate CTR estimation, and meanwhile address the inference inefficiency issue. To benefit from both worlds and close their gaps, we propose a novel model-agnostic framework (i.e., ClickPrompt), where we incorporate CTR models to generate interaction-aware soft prompts for PLMs. We design a prompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM has to recover the masked tokens based on the language context, as well as the soft prompts generated by CTR model. The collaborative and semantic knowledge from ID and textual features would be explicitly aligned and interacted via the prompt interface. Then, we can either tune the CTR model with PLM for superior performance, or solely tune the CTR model without PLM for inference efficiency. Experiments on four real-world datasets validate the effectiveness of ClickPrompt compared with existing baselines.|对于各种互联网应用而言，点进率预测(ctrl)已变得越来越不可或缺。传统的 CTR 模型通过一次热编码将多领域分类数据转换为 ID 特征，并提取特征间的协同信号。这种模式存在语义信息损失的问题。另一项研究通过硬提示模板将输入数据转换成文本句子，探索了预训练语言模型(PLM)在 CTR 预测中的潜力。尽管保留了语义信号，但它们通常无法捕获协作信息(例如，特征交互、纯 ID 特征) ，更不用说巨大的模型规模带来的不可接受的推理开销。本文旨在建立语义知识和协同知识的模型，以便准确地估计 CTR，同时解决推理效率低下的问题。为了从这两个世界中获益并弥合它们之间的差距，我们提出了一个新的模型无关框架(即 ClickPrompt) ，其中我们结合了 CTR 模型来为 PLM 生成感知交互的软提示。我们设计了一个提示增强的掩码语言建模(PA-MLM)预训练任务，其中 PLM 必须基于语言上下文以及由 CTR 模型生成的软提示来恢复掩码标记。来自 ID 和文本特性的协作和语义知识将通过提示界面显式地对齐和交互。然后，我们可以使用 PLM 调整 CTR 模型以获得更好的性能，或者单独调整不使用 PLM 的 CTR 模型以获得更高的推理效率。在四个实际数据集上的实验验证了 ClickPrompt 与现有基线相比的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ClickPrompt:+CTR+Models+are+Strong+Prompt+Generators+for+Adapting+Language+Models+to+CTR+Prediction)|0|
|[Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion](https://doi.org/10.1145/3589334.3645404)|Jinheon Baek, Nirupama Chandrasekaran, Silviu Cucerzan, Allen Herring, Sujay Kumar Jauhar||Large Language Models (LLMs) excel at tackling various natural language tasks. However, due to the significant costs involved in re-training or fine-tuning them, they remain largely static and difficult to personalize. Nevertheless, a variety of applications could benefit from generations that are tailored to users' preferences, goals, and knowledge. Among them is web search, where knowing what a user is trying to accomplish, what they care about, and what they know can lead to improved search experiences. In this work, we propose a novel and general approach that augments an LLM with relevant context from users' interaction histories with a search engine in order to personalize its outputs. Specifically, we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web, which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is light-weight, since it only produces user-specific aggregate projections of interests and knowledge onto public knowledge graphs, and leverages existing search log infrastructure, thereby mitigating the privacy, compliance, and scalability concerns associated with building deep user profiles for personalization. We validate our approach on the task of contextual query suggestion, which requires understanding not only the user's current search context but also what they historically know and care about. Through a number of experiments based on human evaluation, we show that our approach is significantly better than several other LLM-powered baselines, generating query suggestions that are contextually more relevant, personalized, and useful.|大型语言模型(LLM)擅长处理各种自然语言任务。然而，由于重新培训或微调它们所涉及的巨大成本，它们在很大程度上仍然是静态的，难以个性化。尽管如此，各种各样的应用程序可以受益于根据用户的偏好、目标和知识量身定制的几代人。其中之一是网络搜索，知道用户想要完成什么，他们关心什么，以及他们知道什么可以改善搜索体验。在这项工作中，我们提出了一个新颖的和通用的方法，增强了 LLM 与相关的上下文从用户的互动历史与搜索引擎，以个性化其输出。具体来说，我们根据每个用户在 Web 上的搜索和浏览活动，为他们构建一个以实体为中心的知识库，然后利用这个知识库提供与上下文相关的 LLM 提示增强。这个知识存储是轻量级的，因为它只生成用户特定的兴趣和知识的集合投影到公共知识图表上，并利用现有的搜索日志基础设施，从而减轻隐私、合规性和可扩展性方面的关切，这些关切与建立深度用户个性化配置文件有关。我们验证了我们的方法在上下文查询建议的任务，这不仅需要理解用户的当前搜索上下文，而且他们历史上知道和关心。通过大量基于人工评估的实验，我们发现我们的方法明显优于其他几个基于 LLM 的基线，生成的查询建议在上下文中更相关、更个性化和更有用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge-Augmented+Large+Language+Models+for+Personalized+Contextual+Query+Suggestion)|0|
|[Enhancing Recommendation Accuracy and Diversity with Box Embedding: A Universal Framework](https://doi.org/10.1145/3589334.3645577)|Cheng Wu, Shaoyun Shi, Chaokun Wang, Ziyang Liu, Wang Peng, Wenjin Wu, Dongying Kong, Han Li, Kun Gai||Recommender systems have emerged as an indispensable mean to meet personalized interests of users and alleviate information overload. Despite the great success, accuracy-oriented recommendation models are creating information cocoons, i.e., it is becoming increasingly difficult for users to see other items they might be interested in. Although recent studies start paying attention to enhancing recommendation diversity, models based on point embedding fail to describe the range of user preferences and item features well, which is essential for diversified matching. To this end, we propose LCD-UC, a novel List-Check-Decide framework with UnCertainty masking based on box embedding to improve recommendation diversity with recommendation accuracy maintained. Specifically, LCD-UC creates hypercubes to represent users and items using box embedding for high model flexibility and expressiveness. Then, a hypercube similarity scoring function is designed to measure the similarity between hypercubes representing users and items. To make a balance between the accuracy and diversity of recommendations and achieve personalized diversity needs, we further develop a user-item pairwise attention mechanism as well as a user uncertainty masking mechanism in LCD-UC. Besides, we present two new metrics for better evaluation on recommendation diversity, which address the issue that existing metrics only consider the coverage of categories while ignore the frequency of categories. The extensive experiments on three real-world datasets show that LCD-UC can improve both recommendation accuracy and diversity over three base models, and is superior to six state-of-the-art recommendation models. An online 10-day AB test also demonstrates that LCD-UC can improve the performance of a real-world advertising system.|推荐系统已经成为满足用户个性化兴趣和减轻信息超载的不可或缺的手段。尽管取得了巨大的成功，但以准确性为导向的推荐模型正在创造信息茧，也就是说，用户越来越难以看到他们可能感兴趣的其他项目。虽然最近的研究开始关注提高推荐多样性，但基于点嵌入的模型并不能很好地描述用户偏好和项目特征的范围，这对于多样化匹配是必不可少的。为此，我们提出了一种新的基于框嵌入的带不确定性掩蔽的列表检查决策框架 LCD-UC，以提高推荐的多样性，同时保持推荐的准确性。具体来说，LCD-UC 使用框嵌入创建超立方体来表示用户和项目，以获得高模型灵活性和表现力。然后，设计了一个超立方体相似度评分函数来度量代表用户的超立方体与项目之间的相似度。为了在推荐的准确性和多样性之间取得平衡，实现个性化的多样性需求，我们进一步开发了 LCD-UC 中的用户项成对注意机制和用户不确定性掩蔽机制。此外，我们还提出了两个新的指标来更好地评估推荐多样性，这两个指标解决了现有指标只考虑类别的覆盖率而忽略类别的频率的问题。在三个实际数据集上的大量实验表明，LCD-UC 在三个基本模型上都能提高推荐的准确性和多样性，并且优于六个最先进的推荐模型。在线10天 AB 测试也表明，LCD-UC 可以提高现实世界中广告系统的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Recommendation+Accuracy+and+Diversity+with+Box+Embedding:+A+Universal+Framework)|0|
|[Is Contrastive Learning Necessary? A Study of Data Augmentation vs Contrastive Learning in Sequential Recommendation](https://doi.org/10.1145/3589334.3645661)|Peilin Zhou, YouLiang Huang, Yueqi Xie, Jingqi Gao, Shoujin Wang, Jae Boum Kim, Sunghun Kim||Sequential recommender systems (SRS) are designed to predict users' future behaviors based on their historical interaction data. Recent research has increasingly utilized contrastive learning (CL) to leverage unsupervised signals to alleviate the data sparsity issue in SRS. In general, CL-based SRS first augments the raw sequential interaction data by using data augmentation strategies and employs a contrastive training scheme to enforce the representations of those sequences from the same raw interaction data to be similar. Despite the growing popularity of CL, data augmentation, as a basic component of CL, has not received sufficient attention. This raises the question: Is it possible to achieve superior recommendation results solely through data augmentation? To answer this question, we benchmark eight widely used data augmentation strategies, as well as state-of-the-art CL-based SRS methods, on four real-world datasets under both warm- and cold-start settings. Intriguingly, the conclusion drawn from our study is that, certain data augmentation strategies can achieve similar or even superior performance compared with some CL-based methods, demonstrating the potential to significantly alleviate the data sparsity issue with fewer computational overhead. We hope that our study can further inspire more fundamental studies on the key functional components of complex CL techniques. Our processed datasets and codes are available at https://github.com/AIM-SE/DA4Rec.|序贯推荐系统(SRS)是基于用户的历史交互数据来预测用户未来行为的系统。最近的研究越来越多地利用对比学习(CL)来利用无监督信号来缓解 SRS 中的数据稀疏问题。一般情况下，基于 CL 的 SRS 首先通过数据增强策略对原始序列交互数据进行增强，然后采用对比训练方案强制相同原始交互数据的序列表示相似。数据增强作为协同学习的一个基本组成部分，虽然受到了越来越多的关注，但并没有得到足够的重视。这就提出了一个问题: 是否有可能仅仅通过数据增强来获得更好的推荐结果？为了回答这个问题，我们基准八个广泛使用的数据增强策略，以及最先进的基于 CL 的 SRS 方法，在四个真实世界的数据集上，在暖启动和冷启动设置下。有趣的是，我们的研究得出的结论是，与一些基于 CL 的方法相比，某些数据增强策略可以实现相似甚至更好的性能，表明可以显着缓解数据稀疏问题，计算开销更少。我们希望我们的研究能够进一步启发更多关于复杂化学发光技术关键功能成分的基础研究。我们处理过的数据集和代码可以在 https://github.com/aim-se/da4rec 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+Contrastive+Learning+Necessary?+A+Study+of+Data+Augmentation+vs+Contrastive+Learning+in+Sequential+Recommendation)|0|
|[Can Small Language Models be Good Reasoners for Sequential Recommendation?](https://doi.org/10.1145/3589334.3645671)|Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Liang Pang, Xiao Wang||Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larger teacher model. The rationales generated by the teacher model are then utilized as labels to distill the downstream smaller student model (e.g., LLaMA2-7B). In this way, the student model acquires the step-by-step reasoning capabilities in recommendation tasks. We encode the generated rationales from the student model into a dense vector, which empowers recommendation in both ID-based and ID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of SLIM over state-of-the-art baselines, and further analysis showcasing its ability to generate meaningful recommendation reasoning at affordable costs.|大型语言模型(LLM)由于其卓越的语言理解和生成能力，为顺序推荐开辟了新的视野。然而，要成功实施 LLM 授权的顺序建议，仍然需要应对许多挑战。首先，用户行为模式通常很复杂，仅仅依靠 LLM 的一步推理可能会导致不正确的或与任务无关的响应。其次，对于真正的顺序推荐系统来说，LLM (如 ChatGPT-175B)过高的资源需求是不切实际的。在本文中，我们提出了一种新的逐步推荐知识提取框架(SLIM) ，为顺序推荐者以“苗条”(即资源效率)的方式享受 LLM 的异常推理能力铺平了一条有希望的道路。我们在较大的教师模型中引入了基于用户行为序列的 CoT 提示。由教师模型产生的基本原理然后被用作标签来提取下游较小的学生模型(例如，LLaMA2-7B)。通过这种方式，学生模型在推荐任务中获得了分步推理的能力。我们将从学生模型生成的基本原理编码成一个密集向量，这使得推荐在基于身份和身份不可知的情况下都有效。大量的实验证明了 SLIM 在最先进的基线上的有效性，并且进一步的分析显示了它以可承受的成本产生有意义的推荐推理的能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+Small+Language+Models+be+Good+Reasoners+for+Sequential+Recommendation?)|0|
|[Predictive Relevance Uncertainty for Recommendation Systems](https://doi.org/10.1145/3589334.3645689)|Charul Paliwal, Anirban Majumder, Sivaramakrishnan Kaveri||Click-through Rate (CTR) module is the foundation block of recommendation system and used for search, content selection, advertising, video streaming etc. CTR is modelled as a classification problem and extensive research is done to improve the CTR models. However, uncertainty method for these models are still an unexplored area. In this work we analyse popular uncertainty methods in the context of recommendation system. We found that popular uncertainty models fails to capture the predictive uncertainty of the CTR model that exist unique to the recommendation models and is not prevalent in the traditional classification models. We empirical show why a different uncertainty measure is required for the recommendation system CTR prediction models. We propose PRU (Predictive Relevance Uncertainty), a single forward pass uncertainty approach for a sample as a distance from the predictive relevance samples of the training data. We show the efficacy of the proposed predictive relevance uncertainty (PRU) on selective prediction. Further, we demonstrate the utility of the proposed framework on the downstream task of OOD detection and active learning while maintaining the latency of a single pass deterministic model.|点击点进率(ctrl)模块是推荐系统的基础，用于搜索、内容选择、广告、视频流等。CTR 被建模为一个分类问题，为了改进 CTR 模型，人们进行了广泛的研究。然而，这些模型的测量不确定度方法仍然是一个未开发的领域。本文分析了推荐系统中常用的不确定性方法。我们发现，流行的不确定性模型未能捕捉到推荐模型所特有的 CTR 模型的预测不确定性，这种不确定性在传统的分类模型中并不普遍。我们实证研究了为什么推荐系统的 CTR 预测模型需要不同的不确定性度量。我们提出了 PRU (预测相关不确定性) ，一个单一的前向通过不确定性方法的样本作为一个距离的预测相关样本的训练数据。我们证明了所提出的预测相关不确定度(PRU)在选择性预测中的有效性。此外，我们展示了建议的框架在面向对象的检测和主动学习的下游任务中的实用性，同时保持单次通过确定性模型的延迟。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Predictive+Relevance+Uncertainty+for+Recommendation+Systems)|0|
|[Decentralized Collaborative Learning with Adaptive Reference Data for On-Device POI Recommendation](https://doi.org/10.1145/3589334.3645696)|Ruiqi Zheng, Liang Qu, Tong Chen, Lizhen Cui, Yuhui Shi, Hongzhi Yin||In Location-based Social Networks, Point-of-Interest (POI) recommendation helps users discover interesting places. There is a trend to move from the cloud-based model to on-device recommendations for privacy protection and reduced server reliance. Due to the scarcity of local user-item interactions on individual devices, solely relying on local instances is not adequate. Collaborative Learning (CL) emerges to promote model sharing among users, where reference data is an intermediary that allows users to exchange their soft decisions without directly sharing their private data or parameters, ensuring privacy and benefiting from collaboration. However, existing CL-based recommendations typically use a single reference for all users. Reference data valuable for one user might be harmful to another, given diverse user preferences. Users may not offer meaningful soft decisions on items outside their interest scope. Consequently, using the same reference data for all collaborations can impede knowledge exchange and lead to sub-optimal performance. To address this gap, we introduce the Decentralized Collaborative Learning with Adaptive Reference Data (DARD) framework, which crafts adaptive reference data for effective user collaboration. It first generates a desensitized public reference data pool with transformation and probability data generation methods. For each user, the selection of adaptive reference data is executed in parallel by training loss tracking and influence function. Local models are trained with individual private data and collaboratively with the geographical and semantic neighbors. During the collaboration between two users, they exchange soft decisions based on a combined set of their adaptive reference data. Our evaluations across two real-world datasets highlight DARD's superiority in recommendation performance and addressing the scarcity of available reference data.|在基于位置的社交网络中，兴趣点(POI)推荐帮助用户发现有趣的地方。有一种趋势是从基于云的模型转向基于设备的建议，以保护隐私并减少对服务器的依赖。由于在单个设备上缺乏本地用户项交互，仅仅依赖本地实例是不够的。合作学习(CL)的出现是为了促进用户之间的模式共享，其中参考数据是一种中介，允许用户在不直接共享其私人数据或参数的情况下交换软决策，确保隐私并从合作中受益。但是，现有的基于 CL 的建议通常对所有用户使用单个引用。鉴于不同的用户偏好，对一个用户有价值的参考数据可能对另一个用户有害。用户可能不会对他们感兴趣的范围之外的项目提供有意义的软决策。因此，对所有协作使用相同的参考数据可能会阻碍知识交流并导致次优性能。为了解决这一差距，我们引入了自适应参考数据(dARD)框架的分散式合作学习，该框架为有效的用户协作提供自适应参考数据。它首先使用转换和概率数据生成方法生成一个不敏感的公共参考数据池。对于每个用户，通过训练损失跟踪和影响函数并行执行自适应参考数据的选择。局部模型用单个私有数据进行训练，并与地理和语义邻居协作进行训练。在两个用户之间的协作过程中，他们基于一组自适应参考数据交换软决策。我们对两个实际数据集的评估突出了 DARD 在推荐性能和解决可用参考数据稀缺性方面的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decentralized+Collaborative+Learning+with+Adaptive+Reference+Data+for+On-Device+POI+Recommendation)|0|
|[Towards Efficient Communication and Secure Federated Recommendation System via Low-rank Training](https://doi.org/10.1145/3589334.3645702)|NgocHieu Nguyen, TuanAnh Nguyen, Tuan Nguyen, Vu Tien Hoang, Dung D. Le, KokSeng Wong||Federated Recommendation (FedRec) systems have emerged as a solution to safeguard users' data in response to growing regulatory concerns. However, one of the major challenges in these systems lies in the communication costs that arise from the need to transmit neural network models between user devices and a central server. Prior approaches to these challenges often lead to issues such as computational overheads, model specificity constraints, and compatibility issues with secure aggregation protocols. In response, we propose a novel framework, called Correlated Low-rank Structure (CoLR), which leverages the concept of adjusting lightweight trainable parameters while keeping most parameters frozen. Our approach substantially reduces communication overheads without introducing additional computational burdens. Critically, our framework remains fully compatible with secure aggregation protocols, including the robust use of Homomorphic Encryption. The approach resulted in a reduction of up to 93.75 recommendation performance across datasets. Code for reproducing our experiments can be found at https://github.com/NNHieu/CoLR-FedRec.|联邦推荐(FedRec)系统已经成为保护用户数据的一种解决方案，以应对日益增长的监管问题。然而，这些系统的主要挑战之一在于需要在用户设备和中央服务器之间传输神经网络模型而产生的通信成本。先前解决这些挑战的方法通常会导致诸如计算开销、模型特异性约束和与安全聚合协议的兼容性问题等问题。作为回应，我们提出了一种新的框架，称为相关低级结构(CoLR) ，它利用了调整轻量级可训练参数的概念，同时保持大多数参数冻结。我们的方法大大减少了通信开销，而不会引入额外的计算负担。重要的是，我们的框架仍然完全兼容安全的聚合协议，包括同态加密的健壮使用。这种方法导致跨数据集的推荐性能最多降低了93.75。重现我们实验的代码可以在 https://github.com/nnhieu/colr-fedrec 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Efficient+Communication+and+Secure+Federated+Recommendation+System+via+Low-rank+Training)|0|
|[A User-State Based Interest Transfer Network for Cross-Domain Recommendation](https://doi.org/10.1145/3589335.3651465)|Pingjun Pan, Jialu Wang, Tingting Zhou, Wenyu Yang, Hongxiang Chen||Cross-domain recommendation (CDR) has emerged as a promising approach to improve click-through rate (CTR) in the target domain by effectively transferring user interests from the source domain. However, existing methods either use a uniform interest transfer function or focus on user-level personalized transfer functions, neglecting the fact that the transition of user states in the target domain also influence the interests in the source domain. To address this issue, we present User-State based Interest Transfer network (USIT), a novel method that takes into account the user state evolution. USIT contains two main components: a User-State Transition module (UST) and a State-Level Interests Transfer module (SLIT). UST models the evolution of user states by predicting the next state in the target domain. As the user's state evolves, SLIT adaptively weights the interests by interest-level mask attention in the source domain. Extensive offline experiments and online A/B tests demonstrate that our proposed USIT method significantly outperforms current state-of-the-art models in CDR scenarios. Currently, we have deployed it on NetEase Cloud Music, affecting millions of users.|跨域推荐已成为一种有前途的方法，可以有效地将用户的兴趣从源域转移到目标域，从而改善目标域的点进率。然而，现有的方法要么使用统一的兴趣传递函数，要么只关注用户级的个性化传递函数，忽略了目标域中用户状态的转换也会影响源域中的兴趣。为了解决这个问题，我们提出了一种基于用户状态的兴趣传输网络(USIT) ，这是一种考虑用户状态演化的新方法。USIT 包含两个主要组件: 用户状态转换模块(UST)和状态级兴趣转移模块(SLIT)。UST 通过预测目标域中的下一个状态来建模用户状态的演化。随着用户状态的演化，SLIT 通过源域中的兴趣级掩码注意自适应地对兴趣进行权重分配。大量的离线实验和在线 A/B 测试表明，我们提出的 USIT 方法在 CDR 场景中显著优于目前最先进的模型。目前，我们已经在网易云音乐上部署了它，影响了数百万用户。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+User-State+Based+Interest+Transfer+Network+for+Cross-Domain+Recommendation)|0|
|[Position Bias Estimation with Item Embedding for Sparse Dataset](https://doi.org/10.1145/3589335.3651546)|Shion Ishikawa, Yun Ching Liu, Youngjoo Chung, Yu Hirate||Estimating position bias is a well-known challenge in Learning to Rank (L2R). Click data in e-commerce applications, such as targeted advertisements and search engines, provides implicit but abundant feedback to improve personalized rankings. However, click data inherently includes various biases like position bias. Based on the position-based click model, Result Randomization and Regression Expectation-Maximization algorithm (REM) have been proposed to estimate position bias, but they require various paired observations of (item, position). In real-world scenarios of advertising, marketers frequently display advertisements in a fixed pre-determined order, which creates difficulties in estimation due to the limited availability of various pairs in the training data, resulting in a sparse dataset. We propose a variant of the REM that utilizes item embeddings to alleviate the sparsity of (item, position). Using a public dataset and internal carousel advertisement click dataset, we empirically show that item embedding with Latent Semantic Indexing (LSI) and Variational Auto-Encoder (VAE) improves the accuracy of position bias estimation and the estimated position bias enhances Learning to Rank performance. We also show that LSI is more effective as an embedding creation method for position bias estimation.|估计位置偏差是学习排名(L2R)中一个众所周知的挑战。电子商务应用程序中的点击数据，例如有针对性的广告和搜索引擎，提供了隐含但丰富的反馈，以改善个性化排名。然而，点击数据本质上包含各种偏差，如位置偏差。基于位置点击模型，结果随机化和回归期望最大化算法(REM)已被提出来估计位置偏差，但它们需要不同的配对观察(项目，位置)。在现实世界的广告情景中，营销人员经常以固定的预先确定的顺序显示广告，由于训练数据中各种对的可用性有限，造成估计困难，从而导致数据集稀疏。我们提出了 REM 的一种变体，它利用项目嵌入来减轻(项目，位置)的稀疏性。使用公共数据集和内部旋转木马广告点击数据集，我们经验表明，项目嵌入潜在语义索引(LSI)和变分自动编码器(VAE)提高了位置偏差估计的准确性，估计的位置偏差提高了学习排名的性能。我们还证明了大规模集成电路作为位置偏差估计的一种嵌入式创建方法是更有效的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Position+Bias+Estimation+with+Item+Embedding+for+Sparse+Dataset)|0|
|[When Federated Recommendation Meets Cold-Start Problem: Separating Item Attributes and User Interactions](https://doi.org/10.1145/3589334.3645525)|Chunxu Zhang, Guodong Long, Tianyi Zhou, Zijian Zhang, Peng Yan, Bo Yang||Federated recommendation system usually trains a global model on the server without direct access to users' private data on their own devices. However, this separation of the recommendation model and users' private data poses a challenge in providing quality service, particularly when it comes to new items, namely cold-start recommendations in federated settings. This paper introduces a novel method called Item-aligned Federated Aggregation (IFedRec) to address this challenge. It is the first research work in federated recommendation to specifically study the cold-start scenario. The proposed method learns two sets of item representations by leveraging item attributes and interaction records simultaneously. Additionally, an item representation alignment mechanism is designed to align two item representations and learn the meta attribute network at the server within a federated learning framework. Experiments on four benchmark datasets demonstrate IFedRec's superior performance for cold-start scenarios. Furthermore, we also verify IFedRec owns good robustness when the system faces limited client participation and noise injection, which brings promising practical application potential in privacy-protection enhanced federated recommendation systems. The implementation code is available|联邦推荐系统通常在服务器上培训一个全局模型，而不直接访问用户自己设备上的私有数据。然而，推荐模型和用户私有数据的这种分离对提供高质量的服务提出了挑战，特别是当涉及到新项目时，即联邦设置中的冷启动推荐。本文介绍了一种新的方法，称为项目对齐联邦聚合(IFedRec) ，以解决这一挑战。这是联合推荐系统中第一个专门研究冷启动方案的研究工作。该方法通过同时利用项目属性和交互记录来学习两组项目表示。此外，设计了一种项表示对齐机制，用于对齐两个项表示，并在联邦学习框架内学习服务器上的元属性网络。在四个基准数据集上的实验证明了 IFedRec 在冷启动场景下的优越性能。此外，我们还验证了 IFedRec 在有限的客户参与和噪声注入情况下具有良好的鲁棒性，这为隐私保护增强型联邦推荐系统带来了很好的实际应用前景。实现代码是可用的|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+Federated+Recommendation+Meets+Cold-Start+Problem:+Separating+Item+Attributes+and+User+Interactions)|0|
|[Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai](https://doi.org/10.1145/3589335.3648334)|Zhichao Feng, Junjie Xie, Kaiyuan Li, Yu Qin, Pengfei Wang, Qianzhong Li, Bin Yin, Xiang Li, Wei Lin, Shangguang Wang||In the recommender system of Meituan Waimai, we are dealing with ever-lengthening user behavior sequences, which pose an increasing challenge to modeling user preference effectively. Existing sequential recommendation models often fail to capture long-term dependencies or are too complex, complicating the fulfillment of Meituan Waimai's unique business needs. To better model user interests, we consider selecting relevant sub-sequences from users' extensive historical behaviors based on their preferences. In this specific scenario, we've noticed that the contexts in which users interact have a significant impact on their preferences. For this purpose, we introduce a novel method called Context-based Fast Recommendation Strategy to tackle the issue of long sequences. We first identify contexts that share similar user preferences with the target context and then locate the corresponding PoIs based on these identified contexts. This approach eliminates the necessity to select a sub-sequence for every candidate PoI, thereby avoiding high time complexity. Specifically, we implement a prototype-based approach to pinpoint contexts that mirror similar user preferences. To amplify accuracy and interpretability, we employ JS divergence of PoI attributes such as categories and prices as a measure of similarity between contexts. A temporal graph integrating both prototype and context nodes helps incorporate temporal information. We then identify appropriate prototypes considering both target contexts and short-term user preferences. Following this, we utilize contexts aligned with these prototypes to generate a sub-sequence, aimed at predicting CTR and CTCVR scores with target attention. Since its inception in 2023, this strategy has been adopted in Meituan Waimai's display recommender system, leading to a 4.6 in CTR and a 4.2|在 Waimai 的推荐系统美团中，我们正在处理不断延长的用户行为序列，这对有效地建立用户偏好模型提出了越来越大的挑战。现有的顺序推荐模型往往无法捕捉到长期的依赖关系，或者过于复杂，使得美团外卖独特业务需求的实现变得复杂。为了更好地建立用户兴趣模型，我们考虑根据用户的偏好，从用户广泛的历史行为中选择相关的子序列。在这个特定的场景中，我们已经注意到用户交互的上下文对他们的偏好有显著的影响。为此，我们提出了一种基于上下文的快速推荐策略来解决长序列的问题。我们首先识别与目标上下文共享相似用户偏好的上下文，然后基于这些识别的上下文定位相应的 PoI。这种方法消除了为每个候选 PoI 选择子序列的必要性，从而避免了高时间复杂度。具体来说，我们实现了一种基于原型的方法来精确定位反映类似用户偏好的上下文。为了增强准确性和可解释性，我们使用 PoI 属性(如类别和价格)的 JS 差异作为上下文之间相似性的度量。集成原型节点和上下文节点的时态图有助于合并时态信息。然后，我们根据目标环境和短期用户偏好确定合适的原型。接下来，我们利用与这些原型对齐的上下文来生成一个子序列，目的是用目标注意力预测 CTR 和 CTCVR 评分。自2023年推出以来，Waimai 美团的显示器推荐系统一直采用这一策略，点击率分别为4.6和4.2|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Context-based+Fast+Recommendation+Strategy+for+Long+User+Behavior+Sequence+in+Meituan+Waimai)|0|
|[Large Language Models as Data Augmenters for Cold-Start Item Recommendation](https://doi.org/10.1145/3589335.3651532)|Jianling Wang, Haokai Lu, James Caverlee, Ed H. Chi, Minmin Chen||The reasoning and generalization capabilities of LLMs can help us better understand user preferences and item characteristics, offering exciting prospects to enhance recommendation systems. Though effective while user-item interactions are abundant, conventional recommendation systems struggle to recommend cold-start items without historical interactions. To address this, we propose utilizing LLMs as data augmenters to bridge the knowledge gap on cold-start items during training. We employ LLMs to infer user preferences for cold-start items based on textual description of user historical behaviors and new item descriptions. The augmented training signals are then incorporated into learning the downstream recommendation models through an auxiliary pairwise loss. Through experiments on public Amazon datasets, we demonstrate that LLMs can effectively augment the training signals for cold-start items, leading to significant improvements in cold-start item recommendation for various recommendation models.|LLM 的推理和推广能力可以帮助我们更好地理解用户的偏好和项目特征，提供令人兴奋的前景，以加强推荐系统。尽管用户项目交互非常丰富，但传统的推荐系统很难在没有历史交互的情况下推荐冷启动项目。为了解决这个问题，我们提出利用 LLM 作为数据增强器来弥补训练过程中冷启动项目的知识缺口。我们使用 LLM 根据用户历史行为的文本描述和新项描述来推断用户对冷启动项的偏好。然后通过辅助成对损失将增强的训练信号合并到下游推荐模型中。通过对公共 Amazon 数据集的实验，我们发现 LLM 可以有效地增强冷启动项目的训练信号，从而显著改善了各种推荐模型的冷启动项目推荐。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+as+Data+Augmenters+for+Cold-Start+Item+Recommendation)|0|
|[Filter Bubble or Homogenization? Disentangling the Long-Term Effects of Recommendations on User Consumption Patterns](https://doi.org/10.1145/3589334.3645497)|Md Sanzeed Anwar, Grant Schoenebeck, Paramveer S. Dhillon||Recommendation algorithms play a pivotal role in shaping our media choices, which makes it crucial to comprehend their long-term impact on user behavior. These algorithms are often linked to two critical outcomes: homogenization, wherein users consume similar content despite disparate underlying preferences, and the filter bubble effect, wherein individuals with differing preferences only consume content aligned with their preferences (without much overlap with other users). Prior research assumes a trade-off between homogenization and filter bubble effects and then shows that personalized recommendations mitigate filter bubbles by fostering homogenization. However, because of this assumption of a tradeoff between these two effects, prior work cannot develop a more nuanced view of how recommendation systems may independently impact homogenization and filter bubble effects. We develop a more refined definition of homogenization and the filter bubble effect by decomposing them into two key metrics: how different the average consumption is between users (inter-user diversity) and how varied an individual's consumption is (intra-user diversity). We then use a novel agent-based simulation framework that enables a holistic view of the impact of recommendation systems on homogenization and filter bubble effects. Our simulations show that traditional recommendation algorithms (based on past behavior) mainly reduce filter bubbles by affecting inter-user diversity without significantly impacting intra-user diversity. Building on these findings, we introduce two new recommendation algorithms that take a more nuanced approach by accounting for both types of diversity.|推荐算法在塑造我们的媒体选择中起着关键作用，这使得理解它们对用户行为的长期影响至关重要。这些算法通常与两个关键的结果相关联: 同质化，其中用户尽管不同的潜在偏好消费相似的内容，以及过滤泡沫效应，其中具有不同偏好的个体仅消费与其偏好一致的内容(与其他用户没有太多重叠)。先前的研究假设在均匀化和过滤泡效应之间进行权衡，然后表明个性化推荐通过促进均匀化来减轻过滤泡。然而，由于在这两种效应之间进行权衡的假设，以前的工作无法对推荐系统如何独立地影响同质化和过滤泡沫效应形成更加细致入微的观点。我们通过将同质化和过滤泡沫效应分解为两个关键指标，发展了一个更精确的定义: 用户之间的平均消费有多大差异(用户间多样性)和个人的消费有多大差异(用户内部多样性)。然后，我们使用一个新的基于代理的仿真框架，使推荐系统的同质化和过滤气泡效应的影响的整体观点。我们的仿真结果表明，传统的推荐算法(基于过去的行为)主要通过影响用户间的分集来减少滤波器泡沫，而不会显著影响用户内的分集。在这些发现的基础上，我们引入了两种新的推荐算法，它们通过考虑两种类型的多样性，采取了更加细致入微的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Filter+Bubble+or+Homogenization?+Disentangling+the+Long-Term+Effects+of+Recommendations+on+User+Consumption+Patterns)|0|
|[Can One Embedding Fit All? A Multi-Interest Learning Paradigm Towards Improving User Interest Diversity Fairness](https://doi.org/10.1145/3589334.3645662)|Yuying Zhao, Minghua Xu, Huiyuan Chen, Yuzhong Chen, Yiwei Cai, Rashidul Islam, Yu Wang, Tyler Derr||Recommender systems (RSs) have gained widespread applications across various domains owing to the superior ability to capture users' interests. However, the complexity and nuanced nature of users' interests, which span a wide range of diversity, pose a significant challenge in delivering fair recommendations. In practice, user preferences vary significantly; some users show a clear preference toward certain item categories, while others have a broad interest in diverse ones. Even though it is expected that all users should receive high-quality recommendations, the effectiveness of RSs in catering to this disparate interest diversity remains under-explored. In this work, we investigate whether users with varied levels of interest diversity are treated fairly. Our empirical experiments reveal an inherent disparity: users with broader interests often receive lower-quality recommendations. To mitigate this, we propose a multi-interest framework that uses multiple (virtual) interest embeddings rather than single ones to represent users. Specifically, the framework consists of stacked multi-interest representation layers, which include an interest embedding generator that derives virtual interests from shared parameters, and a center embedding aggregator that facilitates multi-hop aggregation. Experiments demonstrate the effectiveness of the framework in achieving better trade-off between fairness and utility across various datasets and backbones.|推荐系统(RS)由于具有捕获用户兴趣的优越能力，在各个领域得到了广泛的应用。然而，用户兴趣的复杂性和细微差别，跨越广泛的多样性，对提供公平的建议构成重大挑战。实际上，用户的偏好差异很大; 一些用户对某些项目类别有明显的偏好，而另一些用户对不同的项目类别有广泛的兴趣。尽管预计所有用户都会收到高质量的建议，但是对于 RSS 在满足这种不同兴趣多样性方面的有效性仍然探索不足。在这项工作中，我们调查是否不同水平的兴趣多样性的用户被公平对待。我们的经验实验揭示了一个内在的差异: 兴趣广泛的用户经常收到质量较低的推荐。为了解决这个问题，我们提出了一个多兴趣框架，它使用多个(虚拟)兴趣嵌入而不是单个兴趣嵌入来表示用户。具体来说，该框架由多个层叠的多兴趣表示层组成，其中包括一个从共享参数获取虚拟兴趣的兴趣嵌入生成器和一个促进多跳聚合的中心嵌入聚合器。实验证明了该框架在实现跨各种数据集和主干网的公平性和实用性之间更好的平衡方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+One+Embedding+Fit+All?+A+Multi-Interest+Learning+Paradigm+Towards+Improving+User+Interest+Diversity+Fairness)|0|
|[Uplift Modeling for Target User Attacks on Recommender Systems](https://doi.org/10.1145/3589334.3645403)|Wenjie Wang, Changsheng Wang, Fuli Feng, Wentao Shi, Daizong Ding, TatSeng Chua||Recommender systems are vulnerable to injective attacks, which inject limited fake users into the platforms to manipulate the exposure of target items to all users. In this work, we identify that conventional injective attackers overlook the fact that each item has its unique potential audience, and meanwhile, the attack difficulty across different users varies. Blindly attacking all users will result in a waste of fake user budgets and inferior attack performance. To address these issues, we focus on an under-explored attack task called target user attacks, aiming at promoting target items to a particular user group. In addition, we formulate the varying attack difficulty as heterogeneous treatment effects through a causal lens and propose an Uplift-guided Budget Allocation (UBA) framework. UBA estimates the treatment effect on each target user and optimizes the allocation of fake user budgets to maximize the attack performance. Theoretical and empirical analysis demonstrates the rationality of treatment effect estimation methods of UBA. By instantiating UBA on multiple attackers, we conduct extensive experiments on three datasets under various settings with different target items, target users, fake user budgets, victim models, and defense models, validating the effectiveness and robustness of UBA.|推荐系统容易受到注入式攻击，即向平台注入有限的虚假用户，以操纵目标项目对所有用户的暴露。在本研究中，我们发现传统的注入式攻击者忽略了每个项目都有其独特的潜在受众这一事实，同时，不同用户之间的攻击难度是不同的。盲目攻击所有用户将导致虚假用户预算的浪费和较差的攻击性能。为了解决这些问题，我们将重点放在一个未被充分研究的攻击任务，称为目标用户攻击，旨在将目标项提升到特定的用户组。此外，我们通过因果透镜将不同的攻击难度描述为异质治疗效果，并提出了一个提升引导的预算分配(UBA)框架。UBA 估计每个目标用户的治疗效果，优化虚假用户预算的分配，以最大限度地提高攻击性能。理论和实证分析证明了 UBA 治疗效果评价方法的合理性。通过实例化多个攻击者的 UBA，我们在不同目标条目、目标用户、虚假用户预算、受害者模型和防御模型的不同设置下对三个数据集进行了广泛的实验，验证了 UBA 的有效性和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uplift+Modeling+for+Target+User+Attacks+on+Recommender+Systems)|0|
|[TikTok and the Art of Personalization: Investigating Exploration and Exploitation on Social Media Feeds](https://doi.org/10.1145/3589334.3645600)|Karan Vombatkere, Sepehr Mousavi, Savvas Zannettou, Franziska Roesner, Krishna P. Gummadi||Recommendation algorithms for social media feeds often function as black boxes from the perspective of users. We aim to detect whether social media feed recommendations are personalized to users, and to characterize the factors contributing to personalization in these feeds. We introduce a general framework to examine a set of social media feed recommendations for a user as a timeline. We label items in the timeline as the result of exploration vs. exploitation of the user's interests on the part of the recommendation algorithm and introduce a set of metrics to capture the extent of personalization across user timelines. We apply our framework to a real TikTok dataset and validate our results using a baseline generated from automated TikTok bots, as well as a randomized baseline. We also investigate the extent to which factors such as video viewing duration, liking, and following drive the personalization of content on TikTok. Our results demonstrate that our framework produces intuitive and explainable results, and can be used to audit and understand personalization in social media feeds.|从用户的角度来看，社交媒体 feed 的推荐算法通常起到黑盒的作用。我们的目标是检测社交媒体的 feed 推荐是否对用户个性化，并描述这些 feed 中促成个性化的因素。我们引入了一个通用框架来检查一组社交媒体提要推荐给用户作为时间轴。我们根据推荐算法对用户兴趣的探索和利用来标记时间线中的项目，并引入一组度量标准来捕获跨用户时间线的个性化程度。我们将框架应用于一个真实的 TikTok 数据集，并使用自动化 TikTok 机器人生成的基线以及随机基线验证结果。我们还调查了视频观看时间、喜欢程度和跟随程度等因素在多大程度上驱动了 TikTok 上内容的个性化。我们的结果表明，我们的框架产生直观和可解释的结果，并可用于审计和理解个性化的社会媒体饲料。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TikTok+and+the+Art+of+Personalization:+Investigating+Exploration+and+Exploitation+on+Social+Media+Feeds)|0|
|[Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits](https://doi.org/10.1145/3589334.3645420)|Yu Xia, Fang Kong, Tong Yu, Liya Guo, Ryan A. Rossi, Sungchul Kim, Shuai Li||Web-based applications such as chatbots, search engines and news recommendations continue to grow in scale and complexity with the recent surge in the adoption of LLMs. Online model selection has thus garnered increasing attention due to the need to choose the best model among a diverse set while balancing task reward and exploration cost. Organizations faces decisions like whether to employ a costly API-based LLM or a locally finetuned small LLM, weighing cost against performance. Traditional selection methods often evaluate every candidate model before choosing one, which are becoming impractical given the rising costs of training and finetuning LLMs. Moreover, it is undesirable to allocate excessive resources towards exploring poor-performing models. While some recent works leverage online bandit algorithm to manage such exploration-exploitation trade-off in model selection, they tend to overlook the increasing-then-converging trend in model performances as the model is iteratively finetuned, leading to less accurate predictions and suboptimal model selections. In this paper, we propose a time-increasing bandit algorithm TI-UCB, which effectively predicts the increase of model performances due to finetuning and efficiently balances exploration and exploitation in model selection. To further capture the converging points of models, we develop a change detection mechanism by comparing consecutive increase predictions. We theoretically prove that our algorithm achieves a logarithmic regret upper bound in a typical increasing bandit setting, which implies a fast convergence rate. The advantage of our method is also empirically validated through extensive experiments on classification model selection and online selection of LLMs. Our results highlight the importance of utilizing increasing-then-converging pattern for more efficient and economic model selection in the deployment of LLMs.|基于 Web 的应用程序，如聊天机器人、搜索引擎和新闻推荐，随着最近采用 LLM 的激增，其规模和复杂性继续增长。因此，在线模型选择越来越受到人们的关注，因为在平衡任务报酬和探索成本的同时，需要在多样化的模型集合中选择最佳模型。组织面临是否使用昂贵的基于 API 的 LLM 或局部微调的小型 LLM 等决策，权衡成本和性能。传统的选择方法通常在选择一个候选模型之前对每个候选模型进行评估，但由于训练和微调 LLM 的成本不断上升，这种方法已经变得不切实际。此外，将过多的资源用于探索表现不佳的模型也是不可取的。虽然最近的一些工作利用在线盗贼算法来管理模型选择中的这种勘探-开发权衡，但是他们往往忽略了模型性能的增加然后收敛的趋势，因为模型是迭代微调的，导致不太准确的预测和次优模型选择。本文提出了一种时间增长的土匪算法 TI-UCB，该算法能够有效地预测由于微调而带来的模型性能的提高，并能够有效地平衡模型选择中的探索和开发。为了进一步捕捉模型的收敛点，我们通过比较连续的增长预测，发展了一种变化检测机制。从理论上证明了该算法在典型的增长型强盗环境下达到了对数遗憾上界，具有较快的收敛速度。通过广泛的分类模型选择和 LLM 在线选择实验，验证了该方法的优越性。我们的研究结果突出了利用增加-然后收敛模式的重要性，更有效和经济的模型选择在 LLM 的部署。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Which+LLM+to+Play?+Convergence-Aware+Online+Model+Selection+with+Time-Increasing+Bandits)|0|
|[Full-stage Diversified Recommendation: Large-scale Online Experiments in Short-video Platform](https://doi.org/10.1145/3589334.3648144)|Nian Li, Yunzhu Pan, Chen Gao, Depeng Jin, Qingmin Liao||The recommender systems on online platforms assist users in finding personalized information, yet this also leads to the issue of limited diversity, potentially giving rise to societal issues such as filter bubbles. Despite significant progress in diversified recommendation algorithms, they have not been extensively experimented with and evaluated for effectiveness in large-scale, full-stage industrial recommender systems. Specifically, industrial recommenders usually consist of three stages of matching, ranking, and re-ranking, in which specific characteristics lead to critical challenges for promoting both recommendation diversity and user engagement. First, user interests are partially observed due to only relevance maximization. Second, item-side feature-aware bias causes imbalanced recommendations. Last, the impact of diversity perception on user engagement stresses the necessity of explicit diversity modeling. To address these challenges in industrial systems, in this work, we deploy several existing diversified algorithms in a real-world short-video platform, including exploration-exploitation, feature-aware debiasing, and diversity optimization. We conduct large-scale online A/B testing for evaluation via online metrics of user engagement and recommendation diversity. Performance improvement across full stages demonstrates the effectiveness of these simple solutions. From comparing performance across different stages and algorithms, we identify that the ranking stage is the most suitable for real-world deployment, and the combination of debiasing and diversity optimization is a promising direction in terms of diversified recommendations. This work provides experiential guidance for the large-scale deployment of diversified algorithms and the construction of a more inclusive platform on the Web.|在线平台上的推荐系统有助于用户查找个性化信息，但这也导致多样性有限的问题，可能引起诸如过滤器泡沫等社会问题。尽管在多样化推荐算法方面取得了重大进展，但在大规模、全阶段的工业推荐系统中，这些算法还没有得到广泛的实验和评估。具体来说，行业推荐通常包括匹配、排名和重新排名三个阶段，在这三个阶段中，特定的特征导致了提高推荐多样性和用户参与度的关键挑战。首先，由于只有相关性最大化，用户的兴趣被部分地观察到。第二，项目侧特征意识偏差导致推荐不平衡。最后，多样性感知对用户参与度的影响强调了显式多样性建模的必要性。为了应对工业系统中的这些挑战，本文在一个真实的短视频平台上部署了几种现有的多样化算法，包括探索-开发、特征感知去偏和多样性优化。我们通过用户参与度和推荐多样性的在线指标进行大规模的在线 A/B 测试以进行评估。整个阶段的性能改进证明了这些简单解决方案的有效性。通过比较不同阶段和算法的性能，我们发现排名阶段最适合于现实世界的部署，消偏和多样性优化相结合是多样化推荐的一个有前途的方向。这项工作为大规模部署各种算法和建立一个更具包容性的网络平台提供了经验指导。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Full-stage+Diversified+Recommendation:+Large-scale+Online+Experiments+in+Short-video+Platform)|0|
|[Improving Item-side Fairness of Multimodal Recommendation via Modality Debiasing](https://doi.org/10.1145/3589334.3648156)|Yu Shang, Chen Gao, Jiansheng Chen, Depeng Jin, Yong Li||Multimodal recommender systems have acquired applications in broad web scenarios such as e-commerce businesses and short-video platforms. Existing multimodal recommendation methods generally boost performance by introducing item-side multimodal content as supplement information. However, the common training paradigm, i.e., encoding unimodal content respectively and fusing them to fit user preference scores, makes the model biased towards items with prevailing modality content under non-uniform training data. This results in a serious item-side unfairness issue, i.e., some items with prevailing modality content are over-recommended while a large number of items don't receive adequate recommendation opportunities, leaving corresponding content providers at great disadvantage. Aiming to eliminate such modality bias and promote item-side fairness, we propose a fairness-aware modality debiasing framework based on counterfactual inference. In the training stage, we additionally introduce unimodal prediction branches to capture the modality bias. In the inference stage, we conduct a fairness-aware counterfactual inference to adaptively eliminate the modality bias. The proposed framework is model-agnostic and flexible to be implemented in various multimodal recommendation models. Extensive experiments on two datasets demonstrate that the proposed method can significantly enhance item-side fairness while providing competitive recommendation accuracy. Our proposed framework is expected to help mitigate the unfair treatment experienced by vulnerable content providers on multimedia web platforms. Codes are available in https://github.com/tsinghua-fib-lab-WWW2024-Modality-Debiasing.|多模式推荐系统已经在电子商务和短视频平台等广泛的网络场景中获得了应用。现有的多通道推荐方法通常通过引入项目端多通道内容作为补充信息来提高性能。然而，常见的训练范式，即分别编码单峰内容并将其融合以适应用户偏好得分，使得模型偏向于非统一训练数据下具有主流情态内容的项目。这导致了一个严重的项目不公平问题，也就是说，一些具有主流模式内容的项目被过度推荐，而大量的项目没有得到足够的推荐机会，使相应的内容提供商处于极大的不利地位。为了消除这种情态偏差，促进项目公平，我们提出了一个基于反事实推理的公平感知情态消偏框架。在训练阶段，我们引入单峰预测分支来捕捉模态偏差。在推理阶段，我们进行公平意识的反事实推理，以自适应地消除情态偏差。所提出的框架与模型无关，可以灵活地在各种多模式推荐模型中实现。在两个数据集上的大量实验表明，该方法在提供竞争性推荐准确性的同时，可以显著提高项目侧公平性。我们提出的框架预计将有助于减轻脆弱的内容提供商在多媒体网络平台上遭受的不公平待遇。密码有 https://github.com/tsinghua-fib-lab-www2024-modality-debiasing。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Item-side+Fairness+of+Multimodal+Recommendation+via+Modality+Debiasing)|0|
|[Knowledge Enhanced Multi-intent Transformer Network for Recommendation](https://doi.org/10.1145/3589335.3648296)|Ding Zou, Wei Wei, Feida Zhu, Chuanyu Xu, Tao Zhang, Chengfu Huo||Incorporating Knowledge Graphs into Recommendation has attracted growing attention in industry, due to the great potential of KG in providing abundant supplementary information and interpretability for the underlying models. However, simply integrating KG into recommendation usually brings in negative feedback in industry, due to the ignorance of the following two factors: i) users' multiple intents, which involve diverse nodes in KG. For example, in e-commerce scenarios, users may exhibit preferences for specific styles, brands, or colors. ii) knowledge noise, which is a prevalent issue in Knowledge Enhanced Recommendation (KGR) and even more severe in industry scenarios. The irrelevant knowledge properties of items may result in inferior model performance compared to approaches that do not incorporate knowledge. To tackle these challenges, we propose a novel approach named Knowledge Enhanced Multi-intent Transformer Network for Recommendation (KGTN), comprising two primary modules: Global Intents Modeling with Graph Transformer, and Knowledge Contrastive Denoising under Intents. Specifically, Global Intents with Graph Transformer focuses on capturing learnable user intents, by incorporating global signals from user-item-relation-entity interactions with a graph transformer, meanwhile learning intent-aware user/item representations. Knowledge Contrastive Denoising under Intents is dedicated to learning precise and robust representations. It leverages intent-aware representations to sample relevant knowledge, and proposes a local-global contrastive mechanism to enhance noise-irrelevant representation learning. Extensive experiments conducted on benchmark datasets show the superior performance of our proposed method over the state-of-the-arts. And online A/B testing results on Alibaba large-scale industrial recommendation platform also indicate the real-scenario effectiveness of KGTN.|由于 KG 在为基础模型提供丰富的补充信息和可解释性方面的巨大潜力，将知识图引入推荐系统已经引起了业界越来越多的关注。然而，由于忽视了以下两个因素，简单地将 KG 整合到推荐中往往会带来负面的反馈: 一是用户的多重意图，涉及到 KG 中的多个节点。例如，在电子商务场景中，用户可能会展示对特定风格、品牌或颜色的偏好。(ii)知识噪音，这是一个普遍的问题，在知识增强推荐(KGR) ，甚至更严重的行业情景。与不包含知识的方法相比，项目的不相关知识属性可能导致较差的模型性能。为了应对这些挑战，我们提出了一种新的方法，称为知识增强的多意图推荐转换网络(KGTN) ，包括两个主要模块: 全局意图建模与图形转换和知识对比去噪的意图。具体来说，使用图形转换器的全局意图侧重于捕获可学习的用户意图，通过将来自用户-项目-关系-实体交互的全局信号与图形转换器相结合，同时学习意图感知的用户/项目表示。目的下的知识对比去噪致力于学习精确和鲁棒的表示。该方法利用意图感知表示对相关知识进行抽样，并提出了一种局部-全局对比机制来增强噪声无关表示学习。在基准数据集上进行的大量实验表明，我们提出的方法的性能优于目前的技术水平。而在阿里巴巴大型工业推荐平台上的在线 A/B 测试结果也显示了 KGTN 的实际应用效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Enhanced+Multi-intent+Transformer+Network+for+Recommendation)|0|
|[Modeling User Viewing Flow using Large Language Models for Article Recommendation](https://doi.org/10.1145/3589335.3648305)|Zhenghao Liu, Zulong Chen, Moufeng Zhang, Shaoyang Duan, Hong Wen, Liangyue Li, Nan Li, Yu Gu, Ge Yu||This paper proposes the User Viewing Flow Modeling (SINGLE) method for the article recommendation task, which models the user constant preference and instant interest from user-clicked articles. Specifically, we first employ a user constant viewing flow modeling method to summarize the user's general interest to recommend articles. In this case, we utilize Large Language Models (LLMs) to capture constant user preferences from previously clicked articles, such as skills and positions. Then we design the user instant viewing flow modeling method to build interactions between user-clicked article history and candidate articles. It attentively reads the representations of user-clicked articles and aims to learn the user's different interest views to match the candidate article. Our experimental results on the Alibaba Technology Association (ATA) website show the advantage of SINGLE, achieving a 2.4 improvement over previous baseline models in the online A/B test. Our further analyses illustrate that SINGLE has the ability to build a more tailored recommendation system by mimicking different article viewing behaviors of users and recommending more appropriate and diverse articles to match user interests.|针对文章推荐任务，提出了用户查看流建模(SINGLE)方法，该方法从用户点击的文章中建立用户常量偏好和即时兴趣模型。具体来说，我们首先使用一个用户常量查看流建模方法来总结用户的一般兴趣来推荐文章。在这种情况下，我们利用大型语言模型(LLM)从以前单击的文章(如技能和职位)中获取常量用户首选项。然后设计用户即时查看流建模方法，建立用户点击文章历史和候选文章之间的交互。它专注地阅读用户点击的文章的表示，并旨在了解用户的不同兴趣视图，以匹配候选文章。我们在阿里巴巴科技协会(ATA)网站上的实验结果显示了 SINGLE 的优势，在线 A/B 测试中比以前的基准模型提高了2.4个百分点。我们进一步的分析表明，SINGLE 能够通过模仿用户不同的文章浏览行为，推荐更合适、更多样的文章来匹配用户的兴趣，从而建立一个更具针对性的推荐系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+User+Viewing+Flow+using+Large+Language+Models+for+Article+Recommendation)|0|
|[Counterfactual Data Augmentation for Debiased Coupon Recommendations Based on Potential Knowledge](https://doi.org/10.1145/3589335.3648306)|Junpeng Fang, Gongduo Zhang, Qing Cui, Lihong Gu, Longfei Li, Jinjie Gu, Jun Zhou||In real-world coupon recommendations, the coupon allocation process is influenced by both the recommendation model trained with historical interaction data and marketing tactics aimed at specific commercial goals. These tactics can cause an imbalance in user-coupon interactions, leading to a deviation from users' natural preferences. We refer to this deviation as the matching bias. Theoretically, unbiased data which is assumed to be collected via a randomized allocating policy (i.e., without model or tactics intervention) is ideal training data because it reflects the user's natural preferences. However, obtaining unbiased data in real-world scenarios is costly and sometimes unfeasible. To address this problem, we propose a novel model-agnostic training paradigm named <u>C</u>ounterfactual <u>D</u>ata <u>A</u>ugmentation for debiased coupon recommendations based on <u>P</u>otential <u>K</u>nowledge (CDAPK) for the marketing scenario that allocates coupons with discounts. We leverage the counterfactual data augmentation technique to answer the following key question: If a user is offered a coupon that he has never seen before in his history, will he use this coupon? By creating the counterfactual interaction data and assigning labels based on the potential knowledge of the given scenario, CDAPK shifts the original data distribution into an unbiased distribution, facilitating model optimization and debiasing. The advantage of CDAPK lies in its ability to approximate the ideal states of the training data without depleting the real-world traffic flow. We implement CDAPK on five representative models: FM, DNN, NCF, MASKNET, and DEEPFM, and conduct extensive offline and online experiments against SOTA debiasing methods to validate the superiority of CDAPK.|在现实世界的优惠券推荐中，优惠券分配过程既受到历史交互数据训练的推荐模型的影响，也受到针对特定商业目标的营销策略的影响。这些策略可能导致用户-优惠券交互的不平衡，从而导致偏离用户的自然偏好。我们把这种偏差称为匹配偏差。从理论上讲，假设通过随机分配策略收集的无偏数据(即没有模型或战术干预)是理想的训练数据，因为它反映了用户的自然偏好。然而，在现实世界中获得无偏的数据是昂贵的，有时是不可行的。为了解决这个问题，我们提出了一个新的模型不可知训练范式，命名为 < u > C </u > 反事实 < u > D </u > ata < u > A </u > 增强去偏差优惠券推荐的基础上 < u > P </u > 潜在 < u > K </u > 知识(CDAPK)的营销场景，分配折扣优惠券。我们利用反事实数据增强技术来回答以下关键问题: 如果一个用户收到了一张他以前从未见过的优惠券，他会使用这张优惠券吗？通过创建反事实交互数据并根据给定场景的潜在知识分配标签，CDAPK 将原始数据分布转化为无偏分布，有利于模型优化和消除偏差。CDAPK 的优点在于它能够在不消耗实际流量的情况下逼近训练数据的理想状态。我们在 FM、 DNN、 nCF、 MASKNET 和 DEEPFM 这五种有代表性的模型上实现了 CDAPK，并针对 SOTA 去偏方法进行了广泛的离线和在线实验，以验证 CDAPK 的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Data+Augmentation+for+Debiased+Coupon+Recommendations+Based+on+Potential+Knowledge)|0|
|[User Response Modeling in Reinforcement Learning for Ads Allocation](https://doi.org/10.1145/3589335.3648310)|Zhiyuan Zhang, Qichao Zhang, Xiaoxu Wu, Xiaowen Shi, Guogang Liao, Yongkang Wang, Xingxing Wang, Dongbin Zhao||User response modeling can enhance the learning of user representations and further improve the reinforcement learning (RL) recommender agent. However, as users' behaviors are influenced by their long-term preferences and short-term stochastic factors (e.g., weather, mood, or fashion trends), it remains challenging for previous works focusing on recurrent neural network-based user response modeling. Meanwhile, due to the dynamic interests of users, it is often unrealistic to assume the dynamics of users are stationary. Drawing inspiration from opponent modeling, we propose a novel network structure, Deep User Q-Network (DUQN), incorporating a user response probabilistic model into the Q-learning ads allocation strategy to capture the effect of the non-stationary user policy on Q-values. Moreover, we utilize the Recurrent State-Space Model (RSSM) to develop the user response model, which includes deterministic and stochastic components, enabling us to fully consider user long-term preferences and short-term stochastic factors. In particular, we design a RetNet version of RSSM (R-RSSM) to support parallel computation. The R-RSSM model can be further used for multi-step predictions to enable bootstrapping over multiple steps simultaneously. Finally, we conduct extensive experiments on a large-scale offline dataset from the Meituan food delivery platform and a public benchmark. Experimental results show that our method yields superior performance to state-of-the-art (SOTA) baselines. Moreover, our model demonstrates a significant improvement in the online A/B test and has been fully deployed on the industrial Meituan platform, serving more than 500 million customers.|用户响应建模可以增强用户表示的学习，进一步改进强化学习推荐代理。然而，由于用户的行为受到他们的长期偏好和短期随机因素(如天气、情绪或时尚趋势)的影响，以往的研究主要集中在基于反复神经网络的用户响应建模方面，这仍然具有挑战性。同时，由于用户的动态兴趣，假设用户的动态是平稳的往往是不现实的。基于对手模型的启发，本文提出了一种新的网络结构——深度用户 Q 网络(DUQN) ，将用户响应概率模型引入 Q 学习广告分配策略，以捕捉非平稳用户策略对 Q 值的影响。此外，利用递归状态空间模型(RSSM)建立了包含确定性和随机性成分的用户响应模型，使得我们能够充分考虑用户的长期偏好和短期随机性因素。特别地，我们设计了 RSSM 的 RetNet 版本(R-RSSM)来支持并行计算。R-RSSM 模型可以进一步用于多步预测，以使自举能够同时跨越多个步骤。最后，我们对来自美团食品配送平台和公共基准的大规模离线数据集进行了广泛的实验。实验结果表明，我们的方法产生的性能优于国家最先进的(SOTA)基线。此外，我们的模型显示了在线 A/B 测试的显著改进，并已完全部署在工业美团平台上，为超过5亿客户提供服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User+Response+Modeling+in+Reinforcement+Learning+for+Ads+Allocation)|0|
|[Cluster Anchor Regularization to Alleviate Popularity Bias in Recommender Systems](https://doi.org/10.1145/3589335.3648312)|Bo Chang, Changping Meng, He Ma, Shuo Chang, Yang Gu, Yajun Peng, Jingchen Feng, Yaping Zhang, Shuchao Bi, Ed H. Chi, Minmin Chen||Recommender systems are essential for finding personalized content for users on online platforms. These systems are often trained on historical user interaction data, which collects user feedback on system recommendations. This creates a feedback loop leading to popularity bias; popular content is over-represented in the data, better learned, and thus recommended even more. Less popular content struggles to reach its potential audiences. Popularity bias limits the diversity of content that users are exposed to, and makes it harder for new creators to gain traction. Existing methods to alleviate popularity bias tend to trade off the performance of popular items. In this work, we propose a new method for alleviating popularity bias in recommender systems, called the cluster anchor regularization, which partitions the large item corpus into hierarchical clusters, and then leverages the cluster information of each item to facilitate transfer learning from head items to tail items. Our results demonstrate the effectiveness of the proposed method with offline analyses and live experiments on a large-scale industrial recommendation platform, where it significantly increases tail recommendation without hurting the overall user experience.|推荐系统对于在线平台上为用户寻找个性化内容至关重要。这些系统通常接受历史用户交互数据的培训，这些数据收集用户对系统建议的反馈。这就形成了一个导致流行偏见的反馈循环; 流行内容在数据中被过度表示，学习得更好，因此被推荐更多。不太受欢迎的内容难以触及潜在受众。流行偏见限制了用户接触到的内容的多样性，使得新的创作者更难获得吸引力。现有的方法，以减少流行偏见往往权衡的表现，受欢迎的项目。本文针对推荐系统中的普遍性偏差问题，提出了一种新的聚类锚正则化方法，该方法将大项目集划分为层次聚类，然后利用每个项目的聚类信息，实现从头项目到尾项目的转移学习。我们的研究结果证明了该方法在大规模工业推荐平台上进行离线分析和现场实验的有效性，该方法在不损害整体用户体验的情况下显著提高了尾部推荐。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cluster+Anchor+Regularization+to+Alleviate+Popularity+Bias+in+Recommender+Systems)|0|
|[MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation](https://doi.org/10.1145/3589335.3648319)|Wenhao Wu, Jialiang Zhou, Ailong He, Shuguang Han, Jufeng Chen, Bo Zheng||Compared to business-to-consumer (B2C) e-commerce systems, consumer-to-consumer (C2C) e-commerce platforms usually encounter the limited-stock problem, that is, a product can only be sold one time in a C2C system. This poses several unique challenges for click-through rate (CTR) prediction. Due to limited user interactions for each product (i.e. item), the corresponding item embedding in the CTR model may not easily converge. This makes the conventional sequence modeling based approaches cannot effectively utilize user history information since historical user behaviors contain a mixture of items with different volume of stocks. Particularly, the attention mechanism in a sequence model tends to assign higher score to products with more accumulated user interactions, making limited-stock products being ignored and contribute less to the final output. To this end, we propose the Meta-Split Network (MSN) to split user history sequence regarding to the volume of stock for each product, and adopt differentiated modeling approaches for different sequences. As for the limited-stock products, a meta-learning approach is applied to address the problem of inconvergence, which is achieved by designing meta scaling and shifting networks with ID and side information. In addition, traditional approach can hardly update item embedding once the product is consumed. Thereby, we propose an auxiliary loss that makes the parameters updatable even when the product is no longer in distribution. To the best of our knowledge, this is the first solution addressing the recommendation of limited-stock product. Experimental results on the production dataset and online A/B testing demonstrate the effectiveness of our proposed method.|与 B2C 电子商务系统相比，C2C (C2C)电子商务平台通常会遇到库存有限的问题，即产品只能在 C2C 系统中销售一次。这给点进率预测带来了几个独特的挑战。由于每个产品(即项目)的用户交互有限，嵌入在 CTR 模型中的相应项目可能不容易收敛。这使得传统的基于序列建模的方法不能有效地利用用户历史信息，因为历史用户行为包含不同股票数量的混合项。特别地，序列模型中的注意机制倾向于给具有更多累积用户交互的产品分配更高的分数，使得有限库存产品被忽略，对最终产出的贡献更小。为此，我们提出了元分割网络(MSN)来分割用户历史序列对于每个产品的库存量，并采用不同的建模方法为不同的序列。对于库存有限的产品，采用元学习方法解决不收敛问题，通过设计具有 ID 和边信息的元尺度和移位网络来实现。此外，传统的方法很难更新嵌入项一旦产品消耗。因此，我们提出了一个辅助损失，使参数可更新，即使当产品不再在分布。据我们所知，这是第一个解决推荐有限库存产品的解决方案。生产数据集和在线 A/B 测试的实验结果证明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MetaSplit:+Meta-Split+Network+for+Limited-Stock+Product+Recommendation)|0|
|[Knowledge Graph-based Session Recommendation with Session-Adaptive Propagation](https://doi.org/10.1145/3589335.3648324)|Yu Wang, Amin Javari, Janani Balaji, Walid Shalaby, Tyler Derr, Xiquan Cui||Session-based recommender systems (SBRSs) predict users' next interacted items based on their historical activities. While most SBRSs capture purchasing intentions locally within each session, capturing items' global information across different sessions is crucial in characterizing their general properties. Previous works capture this cross-session information by constructing graphs and incorporating neighbor information. However, this incorporation cannot vary adaptively according to the unique intention of each session, and the constructed graphs consist of only one type of user-item interaction. To address these limitations, we propose knowledge graph-based session recommendation with session-adaptive propagation. Specifically, we build a knowledge graph by connecting items with multi-typed edges to characterize various user-item interactions. Then, we adaptively aggregate items' neighbor information considering user intention within the learned session. Experimental results demonstrate that equipping our constructed knowledge graph and session-adaptive propagation enhances session recommendation backbones by 10 study showing our proposed framework achieves 2 existing well-deployed model at The Home Depot e-platform.|基于会话的推荐系统(SBRS)根据用户的历史活动预测用户的下一个交互项。虽然大多数 SBRS 在每个会话中捕获本地的购买意图，但是在不同会话中捕获物品的全局信息对于描述它们的一般属性是至关重要的。以前的作品通过构造图表和合并邻居信息来捕获这种跨会话信息。但是，此合并不能根据每个会话的独特意图自适应地变化，并且构造的图仅包含一种类型的用户项交互。为了解决这些局限性，我们提出了基于知识图的会话推荐和会话自适应传播。具体来说，我们通过连接具有多种类型边的项目来描述各种用户-项目交互，从而构建一个知识图。然后，在学习会话中考虑用户意图，自适应地聚合项目的邻居信息。实验结果表明，我们构建的知识图和会话自适应传播增强了会话推荐骨干的10个研究表明，我们提出的框架实现了2个现有的良好部署模型在家得宝电子平台。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Graph-based+Session+Recommendation+with+Session-Adaptive+Propagation)|0|
|[Cache-Aware Reinforcement Learning in Large-Scale Recommender Systems](https://doi.org/10.1145/3589335.3648326)|Xiaoshuang Chen, Gengrui Zhang, Yao Wang, Yulin Wu, Shuo Su, Kaiqiao Zhan, Ben Wang||Modern large-scale recommender systems are built upon computation-intensive infrastructure and usually suffer from a huge difference in traffic between peak and off-peak periods. In peak periods, it is challenging to perform real-time computation for each request due to the limited budget of computational resources. The recommendation with a cache is a solution to this problem, where a user-wise result cache is used to provide recommendations when the recommender system cannot afford a real-time computation. However, the cached recommendations are usually suboptimal compared to real-time computation, and it is challenging to determine the items in the cache for each user. In this paper, we provide a cache-aware reinforcement learning (CARL) method to jointly optimize the recommendation by real-time computation and by the cache. We formulate the problem as a Markov decision process with user states and a cache state, where the cache state represents whether the recommender system performs recommendations by real-time computation or by the cache. The computational load of the recommender system determines the cache state. We perform reinforcement learning based on such a model to improve user engagement over multiple requests. Moreover, we show that the cache will introduce a challenge called critic dependency, which deteriorates the performance of reinforcement learning. To tackle this challenge, we propose an eigenfunction learning (EL) method to learn independent critics for CARL. Experiments show that CARL can significantly improve the users' engagement when considering the result cache. CARL has been fully launched in Kwai app, serving over 100 million users.|现代大规模推荐系统是建立在计算密集型基础设施之上的，通常在高峰期和非高峰期之间存在巨大的流量差异。在高峰期，由于计算资源的预算有限，对每个请求执行实时计算是一个挑战。使用缓存的推荐就是这个问题的解决方案，当推荐系统无法支付实时计算时，使用用户明智的结果缓存来提供推荐。然而，与实时计算相比，缓存的建议通常是次优的，并且为每个用户确定缓存中的项目是具有挑战性的。在本文中，我们提供了一个缓存感知强化学习(CARL)方法，通过实时计算和缓存联合优化推荐。我们用用户状态和缓存状态来描述问题，缓存状态表示马可夫决策过程是通过实时计算还是通过缓存执行推荐推荐系统。推荐系统的计算负载决定了缓存的状态。我们基于这种模型执行强化学习，以提高用户对多个请求的参与度。此外，我们表明缓存将引入一个叫做评论依赖的挑战，这会恶化强化学习的性能。为了应对这一挑战，我们提出了一种特征函数学习(EL)方法来学习独立的批评家为卡尔。实验表明，在考虑结果缓存的情况下，CARL 可以显著提高用户参与度。CARL 已经在葵应用程序中全面推出，为超过1亿用户提供服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cache-Aware+Reinforcement+Learning+in+Large-Scale+Recommender+Systems)|0|
|[Enhancing Interpretability and Effectiveness in Recommendation with Numerical Features via Learning to Contrast the Counterfactual samples](https://doi.org/10.1145/3589335.3648345)|Xiaoxiao Xu, Hao Wu, Wenhui Yu, Lantao Hu, Peng Jiang, Kun Gai||We propose a general model-agnostic Contrastive learning framework with Counterfactual Samples Synthesizing (CCSS) for modeling the monotonicity between the neural network output and numerical features which is critical for interpretability and effectiveness of recommender systems. CCSS models the monotonicity via a two-stage process: synthesizing counterfactual samples and contrasting the counterfactual samples. The two techniques are naturally integrated into a model-agnostic framework, forming an end-to-end training process. Abundant empirical tests are conducted on a publicly available dataset and a real industrial dataset, and the results well demonstrate the effectiveness of our proposed CCSS. Besides, CCSS has been deployed in our real large-scale industrial recommender, successfully serving over hundreds of millions users.|针对神经网络输出与数值特征之间的单调性对推荐系统的可解释性和有效性至关重要的问题，提出了一种基于反事实样本合成(CCSS)的通用模型无关对比学习框架。CCSS 通过两个阶段的过程对单调性进行建模: 合成反事实样本和对比反事实样本。这两种技术自然地集成到一个模型无关的框架中，形成一个端到端的培训过程。在一个公开的数据集和一个真实的工业数据集上进行了大量的实证检验，结果很好地证明了我们提出的 CCSS 的有效性。此外，CCSS 已经部署在我们真正的大规模工业推荐，成功地服务于数亿用户。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Interpretability+and+Effectiveness+in+Recommendation+with+Numerical+Features+via+Learning+to+Contrast+the+Counterfactual+samples)|0|
|[Term Importance for Transformer-Based QA Retrieval: A Case Study of StackExchange](https://doi.org/10.1145/3589335.3651568)|Bryan Zhi Yang Tan, Hady W. Lauw||Question-answering (QA) retrieval is the task of retrieving the most relevant answer to a given question from a collection of answers. Various approaches to QA retrieval have been developed recently. One successful and popular model is Contextualized Late Interaction over BERT (ColBERT), a transformer-based approach that adopts a query-document scoring mechanism that retains the granularity of transformer matching, whilst improving on efficiency. However, one key limitation is that it requires further fine-tuning for new query or collection types. In this work, we explore and propose several non-parametric retrieval augmentation methods based on explicit signals of term importance that improve over ColBERT's baseline performance. In particular, we consider the QA retrieval task in the context of StackExchange question-answering forum, verifying the effectiveness of our methods in this setting.|问题回答(QA)检索是从一组答案中检索与给定问题最相关的答案的任务。质量保证(QA)检索的各种方法最近得到了发展。一个成功且流行的模型是基于上下文的 BERT (ColBERT) ，这是一种基于变压器的方法，它采用了一种查询文档评分机制，保留了变压器匹配的粒度，同时提高了效率。但是，一个关键的限制是需要对新的查询或集合类型进行进一步的微调。在这项工作中，我们探索和提出了几种非参数检索增强方法的基础上的显式信号的项重要性，提高了 ColBERT 的基线性能。特别是，我们在 StackExchange 问答论坛的上下文中考虑 QA 检索任务，验证我们的方法在这种情况下的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Term+Importance+for+Transformer-Based+QA+Retrieval:+A+Case+Study+of+StackExchange)|0|
|[GreenRec: A Large-Scale Dataset for Green Food Recommendation](https://doi.org/10.1145/3589335.3651516)|Lingzi Zhang, Yinan Zhang, Xin Zhou, Zhiqi Shen||In response to growing interest in sustainable living from both governmental and public spheres, there is an increased effort to understand environmental implications. Recommendation systems, which are widely applied in various aspects of daily life, are crucial tools in encouraging and guiding users toward sustainable choices. However, existing public recommendation datasets primarily focus on user-item interactions and lack sufficient emphasis on sustainability, posing significant challenges to developing recommendations for sustainable items. In this work, we enrich a public food recommendation dataset by assigning environmental impact, nutritional impact, and health scores to each recipe, following well-recognized sustainability measurements. Through this work, we aim to lay a groundwork for recommending foods that are both healthy and environmentally conscious, all while maintaining recommendation accuracy.|由于政府和公共领域对可持续生活的兴趣日益增加，人们加大了了解环境影响的努力。推荐系统广泛应用于日常生活的各个方面，是鼓励和指导用户做出可持续选择的重要工具。然而，现有的公共建议数据集主要侧重于用户与项目之间的互动，对可持续性缺乏足够的重视，对制定关于可持续项目的建议构成重大挑战。在这项工作中，我们丰富了公共食品推荐数据集，通过分配环境影响，营养影响和健康评分到每个配方，遵循公认的可持续性测量。通过这项工作，我们的目标是奠定一个基础，推荐食品既健康和环保意识，同时保持推荐的准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GreenRec:+A+Large-Scale+Dataset+for+Green+Food+Recommendation)|0|
|[RimiRec: Modeling Refined Multi-interest in Hierarchical Structure for Recommendation](https://doi.org/10.1145/3589335.3651554)|Haolei Pei, Yuanyuan Xu, Yangping Zhu, Yuan Nie||Industrial recommender systems usually consist of the retrieval stage and the ranking stage, to handle the billion-scale of users and items. The retrieval stage retrieves candidate items relevant to user interests for recommendations and has attracted much attention. Frequently, a user shows refined multi-interests in a hierarchical structure. For example, a user likes Conan and Kuroba Kaito, which are the roles in hierarchical structure "Animation, Japanese Animation, Detective Conan". However, most existing methods ignore this hierarchical nature, and simply average the fine-grained interest information. Therefore, we propose a novel two-stage approach to explicitly modeling refined multi-interest in a hierarchical structure for recommendation. In the first hierarchical multi-interest mining stage, the hierarchical clustering and transformer-based model adaptively generate circles or sub-circles that users are interested in. In the second stage, the partition of retrieval space allows the EBR models to deal only with items within each circle and accurately capture users' refined interests. Experimental results show that the proposed approach achieves state-of-the-art performance. Our framework has also been deployed at Lofter.|工业推荐系统通常由检索阶段和排名阶段组成，用于处理数十亿规模的用户和项目。检索阶段检索与用户兴趣相关的候选项，以便进行推荐，引起了人们的广泛关注。通常，用户在层次结构中显示精确的多重兴趣。例如，一个用户喜欢柯南和黑叶海东，这是角色的层次结构“动画，日本动画，侦探柯南”。然而，大多数现有的方法忽略了这种层次性质，只是对细粒度的兴趣信息进行平均。因此，我们提出了一种新的两阶段的方法来显式建模精细的多兴趣在一个层次结构的推荐。在第一层次多兴趣挖掘阶段，层次聚类和基于变换器的模型自适应地生成用户感兴趣的圈或子圈。在第二阶段，检索空间的划分允许 EBR 模型只处理每个圈内的项目，并准确地捕获用户的细化兴趣。实验结果表明，该方法具有较好的性能。我们的架构也已部署在洛夫特。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RimiRec:+Modeling+Refined+Multi-interest+in+Hierarchical+Structure+for+Recommendation)|0|
|[Is Cosine-Similarity of Embeddings Really About Similarity?](https://doi.org/10.1145/3589335.3651526)|Harald Steck, Chaitanya Ekanadham, Nathan Kallus||Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless `similarities.' For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models; these have implicit and unintended effects when taking cosine-similarities of the resulting embeddings, rendering results opaque and possibly arbitrary. Based on these insights, we caution against blindly using cosine-similarity and outline alternatives.|余弦相似度是两个向量之间夹角的余弦，或者相当于两个向量标准化之间的点积。一个流行的应用是将余弦相似度应用于学习的低维特征嵌入，量化高维对象之间的语义相似度。在实际应用中，这种方法可以比嵌入向量之间的非规范化点积更好，但有时也更差。为了深入了解这一经验观察，我们研究了从正则线性模型衍生的嵌入，其中闭式解决方案促进了分析见解。我们解析地推导出余弦相似度如何产生任意的，因此也就是无意义的相似度。对于一些线性模型，相似性甚至不是唯一的，而对于另一些线性模型，它们是由正则化隐式控制的。我们讨论线性模型以外的含义: 当学习深度模型时，使用不同的正则化组合; 当采用结果嵌入的余弦相似性时，这些组合具有隐含的和意想不到的效果，使得结果不透明，可能是任意的。基于这些见解，我们警告不要盲目地使用余弦相似性和轮廓替代。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+Cosine-Similarity+of+Embeddings+Really+About+Similarity?)|0|
|[Discrete Semantic Tokenization for Deep CTR Prediction](https://doi.org/10.1145/3589335.3651558)|Qijiong Liu, Hengchang Hu, Jiahao Wu, Jieming Zhu, MinYen Kan, XiaoMing Wu||Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings and then caches them, prioritizes space over time. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user–item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.|将项目内容信息纳入点进率预测模型仍然是一个挑战，特别是在工业情景的时间和空间限制下。内容编码范例将用户和条目编码器直接集成到 CTR 模型中，随着时间的推移优先考虑空间。相比之下，基于嵌入的范例将项目和用户语义转换为潜在嵌入，然后缓存它们，随着时间的推移优先考虑空间。本文介绍了一种新的语义标记方法，提出了一种用于用户和项目表示的离散语义标记方法 UIST。UIST 促进快速训练和推理，同时保持一个保守的记忆足迹。具体来说，UIST 将密集嵌入向量量化为较短长度的离散令牌，并采用分层混合推理模块来衡量每个用户项令牌对的贡献。我们在新闻推荐上的实验结果展示了 UIST 用于 CTR 预测的有效性和效率(约200倍的空间压缩)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Discrete+Semantic+Tokenization+for+Deep+CTR+Prediction)|0|
|[Cornac-AB: An Open-Source Recommendation Framework with Native A/B Testing Integration](https://doi.org/10.1145/3589335.3651241)|Darryl Ong, QuocTuan Truong, Hady W. Lauw|Amazon, Seattle, WA, USA; Singapore Management University, Singapore, Singapore|Recommender systems significantly impact user experience across diverse domains, yet existing frameworks often prioritize offline evaluation metrics, neglecting the crucial integration of A/B testing for forward-looking assessments. In response, this paper introduces a new framework seamlessly incorporating A/B testing into the Cornac recommendation library. Leveraging a diverse collection of model implementations in Cornac, our framework enables effortless A/B testing experiment setup from offline trained models. We introduce a carefully designed dashboard and a robust backend for efficient logging and analysis of user feedback. This not only streamlines the A/B testing process but also enhances the evaluation of recommendation models in an online environment. Demonstrating the simplicity of on-demand online model evaluations, our work contributes to advancing recommender system evaluation methodologies, underscoring the significance of A/B testing and providing a practical framework for implementation. The framework is open-sourced at https://github.com/PreferredAI/cornac-ab.|推荐系统对不同领域的用户体验有重要影响，但现有框架往往优先考虑离线评估指标，忽视了前瞻性评估中 A/B 测试的关键集成。作为回应，本文介绍了一个将 A/B 测试无缝地结合到 Cornac 推荐库中的新框架。我们的框架利用 Cornac 多种多样的模型实现，从离线培训的模型轻松建立了 A/B 测试实验。我们引入了一个精心设计的仪表板和一个健壮的后端，用于有效的日志记录和用户反馈分析。这不仅简化了 A/B 测试过程，而且加强了在线环境中对推荐模式的评估。我们的工作展示了在线模型评估的简单性，有助于推进推荐系统评估方法，强调 A/B 测试的重要性，并为实施提供一个切实可行的框架。该框架在 https://github.com/preferredai/cornac-ab 上是开源的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cornac-AB:+An+Open-Source+Recommendation+Framework+with+Native+A/B+Testing+Integration)|0|
|[Towards Reliable and Efficient Long-Term Recommendation with Large Foundation Models](https://doi.org/10.1145/3589335.3651258)|Wentao Shi||Prioritizing long-term engagement rather than immediate benefits has garnered increasing attention in recent years. However, current research on long-term recommendation faces substantial challenges in terms of model evaluation and design: 1) Traditional evaluation approaches suffer from limitations due to the sparsity and bias in the offline data and fail to capture user psychological influences. 2) Existing recommenders based on Reinforcement Learning (RL) are entirely data-driven and constrained by sparse and long-tail distributed offline data. Fortunately, recent advancements in Large Foundation Models (LFMs), characterized by remarkable simulation and planning capacity, offer significant opportunities for long-term recommendation. Despite potential, due to the substantial scenario divergence between LFM pre-training and recommendation, employing LFMs in long-term recommendation still faces certain challenges. To this end, this research focuses on adapting the remarkable capabilities of LFMs to long-term recommendations to devise reliable evaluation schemes and efficient recommenders.|近年来，优先考虑长期参与而不是眼前利益的做法越来越受到重视。然而，目前长期推荐的研究在模型评价和设计方面面临着巨大的挑战: 1)传统的评价方法由于离线数据的稀疏性和偏倚性而受到限制，无法捕捉到用户的心理影响。2)现有的基于强化学习的推荐系统完全由数据驱动，并受到稀疏和长尾分布式离线数据的限制。幸运的是，最近在大型基础模型(lfMs)方面的进展，拥有属性显著的模拟和规划能力，为长期推荐提供了重要的机会。尽管潜力巨大，但由于 LFM 预训练和推荐之间存在巨大的情景差异，在长期推荐中使用 LFM 仍然面临一定的挑战。为此，本研究侧重于使 LFM 的显著能力适应长期建议，以制定可靠的评估方案和有效的建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Reliable+and+Efficient+Long-Term+Recommendation+with+Large+Foundation+Models)|0|
|[Clickbait vs. Quality: How Engagement-Based Optimization Shapes the Content Landscape in Online Platforms](https://doi.org/10.1145/3589334.3645353)|Nicole Immorlica, Meena Jagadeesan, Brendan Lucier||Online content platforms commonly use engagement-based optimization when making recommendations. This encourages content creators to invest in quality, but also rewards gaming tricks such as clickbait. To understand the total impact on the content landscape, we study a game between content creators competing on the basis of engagement metrics and analyze the equilibrium decisions about investment in quality and gaming. First, we show the content created at equilibrium exhibits a positive correlation between quality and gaming, and we empirically validate this finding on a Twitter dataset. Using the equilibrium structure of the content landscape, we then examine the downstream performance of engagement-based optimization along several axes. Perhaps counterintuitively, the average quality of content consumed by users can decrease at equilibrium as gaming tricks become more costly for content creators to employ. Moreover, engagement-based optimization can perform worse in terms of user utility than a baseline with random recommendations, and engagement-based optimization is also suboptimal in terms of realized engagement relative to quality-based optimization. Altogether, our results highlight the need to consider content creator incentives when evaluating a platform's choice of optimization metric.|在线内容平台通常在提出建议时使用基于参与的优化。这鼓励内容创作者投资于质量，但也奖励了点击诱饵等游戏技巧。为了理解对内容景观的总体影响，我们研究了基于参与度指标的内容创作者之间的竞争博弈，并分析了质量投资和博弈的均衡决策。首先，我们展示了在均衡状态下创建的内容在质量和游戏之间呈现出正相关性，并且我们在 Twitter 数据集上验证了这一发现。然后利用内容景观的均衡结构，考察了基于约定的优化在多个轴上的下游性能。也许与直觉相反的是，随着内容创作者使用游戏技巧的成本越来越高，用户消费的内容的平均质量可能在均衡状态下下降。此外，就用户效用而言，基于约定的优化可能比基于随机推荐的基线执行得更差，而就已实现的约定而言，相对于基于质量的优化而言，基于约定的优化也是次优的。总之，我们的研究结果强调了在评估平台对优化指标的选择时需要考虑内容创建者的激励因素。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Clickbait+vs.+Quality:+How+Engagement-Based+Optimization+Shapes+the+Content+Landscape+in+Online+Platforms)|0|
|[Bidder Selection Problem in Position Auctions: A Fast and Simple Algorithm via Poisson Approximation](https://doi.org/10.1145/3589334.3645418)|Nikolai Gravin, Yixuan Even Xu, Renfei Zhou||In the Bidder Selection Problem (BSP) there is a large pool of n potential advertisers competing for ad slots on the user's web page. Due to strict computational restrictions, the advertising platform can run a proper auction only for a fraction k<n of advertisers. We consider the basic optimization problem underlying BSP: given n independent prior distributions, how to efficiently find a subset of k with the objective of either maximizing expected social welfare or revenue of the platform. We study BSP in the classic multi-winner model of position auctions for welfare and revenue objectives using the optimal (respectively, VCG mechanism, or Myerson's auction) format for the selected set of bidders. Previous PTAS results for BSP optimization were only known for single-item auctions and in case of [Segev and Singla 2021] for l-unit auctions. More importantly, all of these PTASes were computational complexity results with impractically large running times, which defeats the purpose of using these algorithms under severe computational constraints. We propose a novel Poisson relaxation of BSP for position auctions that immediately implies that 1) BSP is polynomial-time solvable up to a vanishingly small error as the problem size k grows; 2) there is a PTAS for position auctions after combining our relaxation with the trivial brute force algorithm. Unlike all previous PTASes, we implemented our algorithm and did extensive numerical experiments on practically relevant input sizes. First, our experiments corroborate the previous experimental findings of Mehta et al. that a few simple heuristics used in practice perform surprisingly well in terms of approximation factor. Furthermore, our algorithm outperforms Greedy both in running time and approximation on medium and large-sized instances.|在投标人选择问题(BSP)中，有大量潜在的广告商在争夺用户网页上的广告位置。由于严格的计算限制，广告平台可以运行一个适当的拍卖只有一小部分 k < n 的广告商。我们考虑了基本的最佳化问题: 给定 n 个独立的先验分布，如何有效地找到 k 的一个子集，目标是最大化期望的社会福利或平台的收入。我们研究了福利和收入目标的经典多赢家位置拍卖模型中的 BSP 问题。以前的 PTAS 结果的 BSP 优化只知道单项拍卖和情况下的[ Segev 和 Singla 2021]的 l- 单位拍卖。更重要的是，所有这些 PTASes 都是计算复杂度不切实际的大运行时间的结果，这违背了在严格的计算约束下使用这些算法的目的。我们提出了一个新的位置拍卖的 BSP 的泊松松弛立即意味着: 1) BSP 是多项式时间可解到一个消失的小误差随着问题大小 k 的增长; 2)有一个 PTAS 的位置拍卖结合我们的松弛与平凡的蛮力算法。与以前的 PTASes 不同，我们实现了我们的算法，并对实际相关的输入大小进行了广泛的数值实验。首先，我们的实验证实了 Mehta 等人先前的实验结果，即在实践中使用的一些简单的启发式算法在近似因子方面表现出令人惊讶的好。此外，我们的算法在运行时间和对中型和大型实例的逼近方面都优于贪婪算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bidder+Selection+Problem+in+Position+Auctions:+A+Fast+and+Simple+Algorithm+via+Poisson+Approximation)|0|
|[A Fast Hop-Biased Approximation Algorithm for the Quadratic Group Steiner Tree Problem](https://doi.org/10.1145/3589334.3645325)|Xiaoqing Wang, Gong Cheng||Knowledge Graph (KG) exploration helps Web users understand the contents of a large and unfamiliar KG and extract relevant insights. The task has recently been formulated as a Quadratic Group Steiner Tree Problem (QGSTP) to search for a semantically cohesive subgraph connecting entities that match query keywords. However, on large graphs, existing algorithms for this NP-hard problem cannot meet the performance need. In this paper, we propose a novel approximation algorithm for QGSTP called HB. It finds and merges an optimal set of paths according to a Hop-Biased objective function, which not only leads to a guaranteed approximation ratio but is also decomposable by paths to enable efficient dynamic programming based search. Accompanied by a set of pruning heuristics, HB outperformed the state of the art by 1-2 orders of magnitude, empirically reducing the average time for answering a query on a million-scale graph from about one minute to one second.|知识图(KG)探索可以帮助 Web 用户理解一个大型的、不熟悉的 KG 的内容，并提取相关的见解。这个任务最近被设计成一个二次群斯坦纳树问题(qadratic Group) ，用于搜索一个语义内聚的子图，该子图连接与查询关键字匹配的实体。然而，在大图上，现有的求解这个 NP 难问题的算法不能满足性能要求。在这篇文章中，我们提出了一个新的近似演算法 QGSTP 称为 HB。它根据一个有跳偏的目标函数寻找并合并一组最优路径，不仅保证了逼近比，而且可以按路径进行分解，从而实现高效的基于动态规划的搜索。伴随着一系列的修剪启发法，HB 的表现超过了最先进的数量级1-2倍，从经验上来说，在一个百万尺度的图表上回答一个问题的平均时间从大约1分钟减少到1秒。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Fast+Hop-Biased+Approximation+Algorithm+for+the+Quadratic+Group+Steiner+Tree+Problem)|0|
|[Link Prediction on Multilayer Networks through Learning of Within-Layer and Across-Layer Node-Pair Structural Features and Node Embedding Similarity](https://doi.org/10.1145/3589334.3645646)|Lorenzo Zangari, Domenico Mandaglio, Andrea Tagarelli||Link prediction has traditionally been studied in the context of simple graphs, although real-world networks are inherently complex as they are often comprised of multiple interconnected components, or layers. Predicting links in such network systems, or multilayer networks, require to consider both the internal structure of a target layer as well as the structure of the other layers in a network, in addition to layer-specific node-attributes when available. This problem poses several challenges, even for graph neural network based approaches despite their successful and wide application to a variety of graph learning problems. In this work, we aim to fill a lack of multilayer graph representation learning methods designed for link prediction. Our proposal is a novel neural-network-based learning framework for link prediction on (attributed) multilayer networks, whose key idea is to combine (i) pairwise similarities of multilayer node embeddings learned by a graph neural network model, and (ii) structural features learned from both within-layer and across-layer link information based on overlapping multilayer neighborhoods. Extensive experimental results have shown that our framework consistently outperforms both single-layer and multilayer methods for link prediction on popular real-world multilayer networks, with an average percentage increase in AUC up to 38%. We make source code and evaluation data available at https://mlnteam-unical.github.io/resources/.|尽管现实世界的网络本质上是复杂的，因为它们通常由多个相互连接的组件或层组成，但是链接预测一直以来都是在简单图形的背景下进行研究的。在这样的网络系统或多层网络中，预测链路需要同时考虑目标层的内部结构和网络中其他层的结构，以及可用的特定层节点属性。这个问题提出了几个挑战，即使基于图神经网络的方法，尽管他们成功的和广泛的应用于各种图学习问题。在这项工作中，我们的目标是填补缺乏多层图表示学习方法设计的链接预测。本文提出了一种新的基于神经网络的多层网络链接预测学习框架，其核心思想是结合(i)图形神经网络模型学习的多层节点嵌入的成对相似性，以及(ii)基于重叠多层邻域的层内和跨层链接信息学习的结构特征。广泛的实验结果表明，我们的框架一贯优于单层和多层方法的链路预测流行的现实世界多层网络，平均百分比增加 AUC 高达38% 。我们在 https://mlnteam-unical.github.io/resources/提供源代码和评估数据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Link+Prediction+on+Multilayer+Networks+through+Learning+of+Within-Layer+and+Across-Layer+Node-Pair+Structural+Features+and+Node+Embedding+Similarity)|0|
|[Diffusion-based Negative Sampling on Graphs for Link Prediction](https://doi.org/10.1145/3589334.3645650)|TrungKien Nguyen, Yuan Fang||Link prediction is a fundamental task for graph analysis with important applications on the Web, such as social network analysis and recommendation systems, etc. Modern graph link prediction methods often employ a contrastive approach to learn robust node representations, where negative sampling is pivotal. Typical negative sampling methods aim to retrieve hard examples based on either predefined heuristics or automatic adversarial approaches, which might be inflexible or difficult to control. Furthermore, in the context of link prediction, most previous methods sample negative nodes from existing substructures of the graph, missing out on potentially more optimal samples in the latent space. To address these issues, we investigate a novel strategy of multi-level negative sampling that enables negative node generation with flexible and controllable “hardness” levels from the latent space. Our method, called Conditional Diffusion-based Multi-level Negative Sampling (DMNS), leverages the Markov chain property of diffusion models to generate negative nodes in multiple levels of variable hardness and reconcile them for effective graph link prediction. We further demonstrate that DMNS follows the sub-linear positivity principle for robust negative sampling. Extensive experiments on several benchmark datasets demonstrate the effectiveness of DMNS.|链接预测是图形分析的基础性工作，在网络上有着重要的应用，如社会网络分析和推荐系统等。现代图形链接预测方法往往采用对比的方法来学习鲁棒的节点表示，其中负采样是关键。典型的负抽样方法旨在检索基于预定义启发式或自动对抗式方法的硬实例，这些方法可能缺乏灵活性或难以控制。此外，在链路预测的背景下，大多数以前的方法从图的现有子结构中抽取负节点，在潜在空间中遗漏了潜在的更优样本。为了解决这些问题，我们研究了一种新的多级负采样策略，该策略使负节点的生成具有灵活和可控的潜在空间“硬度”水平。该方法利用扩散模型的马尔可夫链特性，在多个可变硬度水平上生成负节点，并协调它们进行有效的图形链接预测。进一步证明了对于鲁棒负采样，DMNS 遵循亚线性正性原理。在几个基准数据集上的大量实验证明了 DMNS 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diffusion-based+Negative+Sampling+on+Graphs+for+Link+Prediction)|0|
|[InfoRank: Unbiased Learning-to-Rank via Conditional Mutual Information Minimization](https://doi.org/10.1145/3589334.3645356)|Jiarui Jin, Zexue He, Mengyue Yang, Weinan Zhang, Yong Yu, Jun Wang, Julian J. McAuley||Ranking items regarding individual user interests is a core technique of multiple downstream tasks such as recommender systems. Learning such a personalized ranker typically relies on the implicit feedback from users' past click-through behaviors. However, collected feedback is biased toward previously highly-ranked items and directly learning from it would result in a "rich-get-richer" phenomenon. In this paper, we propose a simple yet sufficient unbiased learning-to-rank paradigm named InfoRank that aims to simultaneously address both position and popularity biases. We begin by consolidating the impacts of those biases into a single observation factor, thereby providing a unified approach to addressing bias-related issues. Subsequently, we minimize the mutual information between the observation estimation and the relevance estimation conditioned on the input features. By doing so, our relevance estimation can be proved to be free of bias. To implement InfoRank, we first incorporate an attention mechanism to capture latent correlations within user-item features, thereby generating estimations of observation and relevance. We then introduce a regularization term, grounded in conditional mutual information, to promote conditional independence between relevance estimation and observation estimation. Experimental evaluations conducted across three extensive recommendation and search datasets reveal that InfoRank learns more precise and unbiased ranking strategies.|关于个人用户兴趣的项目排序是多下游任务(如推荐系统)的核心技术。学习这样一个个性化的排名通常依赖于来自用户过去点击行为的隐式反馈。然而，收集的反馈偏向于先前排名较高的项目，直接从中学习将导致“富者越富”的现象。在本文中，我们提出了一个简单而充分的无偏学习排名范式称为 InfoRank，旨在同时解决位置和流行偏见。我们首先将这些偏见的影响合并为一个单一的观察因素，从而为解决偏见相关问题提供一个统一的方法。然后，在输入特征的条件下，最小化观测估计和相关估计之间的互信息。通过这样做，我们的相关性估计可以证明是没有偏见的。为了实现 InfoRank，我们首先引入一个注意机制来捕捉用户项目特征中的潜在相关性，从而产生观察和相关性的估计。然后，我们引入一个基于条件互信息的正则项，以促进相关估计和观测估计之间的条件独立。对三个广泛的推荐和搜索数据集进行的实验评估表明，InfoRank 学会了更精确和无偏见的排名策略。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=InfoRank:+Unbiased+Learning-to-Rank+via+Conditional+Mutual+Information+Minimization)|0|
|[Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback](https://doi.org/10.1145/3589334.3645365)|Zheng Wang, Bingzheng Gan, Wei Shi||In the rapidly evolving landscape of information retrieval, search engines strive to provide more personalized and relevant results to users. Query suggestion systems play a crucial role in achieving this goal by assisting users in formulating effective queries. However, existing query suggestion systems mainly rely on textual inputs, potentially limiting user search experiences for querying images. In this paper, we introduce a novel Multimodal Query Suggestion (MMQS) task, which aims to generate query suggestions based on user query images to improve the intentionality and diversity of search results. We present the RL4Sugg framework, leveraging the power of Large Language Models (LLMs) with Multi-Agent Reinforcement Learning from Human Feedback to optimize the generation process. Through comprehensive experiments, we validate the effectiveness of RL4Sugg, demonstrating a 18 compared to the best existing approach. Moreover, the MMQS has been transferred into real-world search engine products, which yield enhanced user engagement. Our research advances query suggestion systems and provides a new perspective on multimodal information retrieval.|在快速发展的信息检索中，搜索引擎努力为用户提供更个性化和相关的结果。查询建议系统通过协助用户构建有效的查询，在实现这一目标方面发挥着至关重要的作用。然而，现有的查询建议系统主要依赖于文本输入，这可能会限制用户查询图像的搜索体验。本文介绍了一种新的多模式查询建议(MMQS)任务，该任务旨在基于用户查询图像生成查询建议，以提高搜索结果的意向性和多样性。我们介绍了 RL4Sugg 框架，利用大型语言模型(LLM)和来自人类反馈的多代理强化学习的能力来优化生成过程。通过综合实验，验证了 RL4Sugg 的有效性，与现有的最佳方法相比，验证了18种方法的有效性。此外，MMQS 已经转移到现实世界的搜索引擎产品，从而提高了用户参与度。我们的研究发展了查询建议系统，为多模式信息检索提供了一个新的视角。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Query+Suggestion+with+Multi-Agent+Reinforcement+Learning+from+Human+Feedback)|0|
|[A Fast Similarity Matrix Calibration Method with Incomplete Query](https://doi.org/10.1145/3589334.3645456)|Changyi Ma, Runsheng Yu, Youzhi Zhang||The similarity matrix is at the core of similarity search problems. However, incomplete observations are ubiquitous in real scenarios leading to a less accurate similarity matrix. To alleviate this problem, in this paper, based on the key insight that the similarity matrix enjoys both the symmetric and positive semi-definiteness (PSD) properties, we propose a novel similarity matrix calibration method, which is scalable, effective, and sound. Specifically, we establish the PSD property as a constraint for the similarity matrix calibration problem and propose a novel similarity matrix calibration method to estimate the similarity matrix, which approximates the unknown complete ground-truth similarity matrix. To enable a fast optimization process, we further develop a general approximated algorithm that bypasses the computation of singular values. Theoretical analysis ensures stable calibration performance and convergence speed. Extensive experiments of similarity matrix calibration on real-world datasets demonstrate that our proposed method outperforms baseline methods in terms of both accuracy and speed.|相似矩阵是最近邻搜索问题的核心。然而，不完全观测在实际场景中普遍存在，导致相似矩阵不够精确。为了解决这一问题，本文基于相似矩阵同时具有对称性和正半确定性的特点，提出了一种可扩展、有效、合理的相似矩阵标定方法。具体来说，我们将 PSD 特性作为相似矩阵标定问题的约束条件，提出了一种新的相似矩阵标定方法来估计相似矩阵，该方法逼近未知的完全地面真相相似矩阵。为了实现快速优化过程，我们进一步开发了一个通用的近似算法，绕过奇异值的计算。理论分析保证了稳定的校准性能和收敛速度。在实际数据集上进行的大量相似矩阵校正实验表明，该方法在精度和速度上均优于基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Fast+Similarity+Matrix+Calibration+Method+with+Incomplete+Query)|0|
|[Whole Page Unbiased Learning to Rank](https://doi.org/10.1145/3589334.3645474)|Haitao Mao, Lixin Zou, Yujia Zheng, Jiliang Tang, Xiaokai Chu, Jiashu Zhao, Qian Wang, Dawei Yin||The page presentation biases in the information retrieval system, especially on the click behavior, is a well-known challenge that hinders improving ranking models' performance with implicit user feedback. Unbiased Learning to Rank (ULTR) algorithms are then proposed to learn an unbiased ranking model with biased click data. However, most existing algorithms are specifically designed to mitigate position-related bias, e.g., trust bias, without considering biases induced by other features in search result page presentation(SERP), e.g. attractive bias induced by the multimedia. Unfortunately, those biases widely exist in industrial systems and may lead to an unsatisfactory search experience. Therefore, we introduce a new problem, i.e., whole-page Unbiased Learning to Rank(WP-ULTR), aiming to handle biases induced by whole-page SERP features simultaneously. It presents tremendous challenges: (1) a suitable user behavior model (user behavior hypothesis) can be hard to find; and (2) complex biases cannot be handled by existing algorithms. To address the above challenges, we propose a Bias Agnostic whole-page unbiased Learning to rank algorithm, named BAL, to automatically find the user behavior model with causal discovery and mitigate the biases induced by multiple SERP features with no specific design. Experimental results on a real-world dataset verify the effectiveness of the BAL.|信息检索系统中的页面呈现偏差，尤其是点击行为，是一个众所周知的挑战，它阻碍了隐含用户反馈来提高排名模型的性能。然后提出无偏学习排序(ULTR)算法来学习一个有偏点击数据的无偏排序模型。然而，大多数现有的算法都是专门设计来减轻位置相关的偏差，例如信任偏差，而不考虑搜索结果页面呈现(SERP)中其他特征引起的偏差，例如多媒体引起的吸引力偏差。不幸的是，这些偏见广泛存在于工业系统中，并可能导致不令人满意的搜索体验。因此，我们引入了一个新的问题，即整页无偏学习排序(WP-ULTR) ，旨在同时处理整页 SERP 特征引起的偏差。它提出了巨大的挑战: (1)一个合适的用户行为模型(用户行为假设)可能很难找到; (2)复杂的偏差不能处理现有的算法。为了解决上述挑战，我们提出了一种名为 BAL 的偏倚不可知全页无偏学习排序算法，以自动找到具有因果发现的用户行为模型，并减轻由没有特定设计的多个 SERP 特征引起的偏差。在实际数据集上的实验结果验证了 BAL 算法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Whole+Page+Unbiased+Learning+to+Rank)|0|
|[Mitigating Exploitation Bias in Learning to Rank with an Uncertainty-aware Empirical Bayes Approach](https://doi.org/10.1145/3589334.3645487)|Tao Yang, Cuize Han, Chen Luo, Parth Gupta, Jeff M. Phillips, Qingyao Ai||Ranking is at the core of many artificial intelligence (AI) applications, including search engines, recommender systems, etc. Modern ranking systems are often constructed with learning-to-rank (LTR) models built from user behavior signals. While previous studies have demonstrated the effectiveness of using user behavior signals (e.g., clicks) as both features and labels of LTR algorithms, we argue that existing LTR algorithms that indiscriminately treat behavior and non-behavior signals in input features could lead to suboptimal performance in practice. Particularly because user behavior signals often have strong correlations with the ranking objective and can only be collected on items that have already been shown to users, directly using behavior signals in LTR could create an exploitation bias that hurts the system performance in the long run. To address the exploitation bias, we propose EBRank, an empirical Bayes-based uncertainty-aware ranking algorithm. Specifically, to overcome exploitation bias brought by behavior features in ranking models, EBRank uses a sole non-behavior feature based prior model to get a prior estimation of relevance. In the dynamic training and serving of ranking systems, EBRank uses the observed user behaviors to update posterior relevance estimation instead of concatenating behaviors as features in ranking models. Besides, EBRank additionally applies an uncertainty-aware exploration strategy to explore actively, collect user behaviors for empirical Bayesian modeling and improve ranking performance. Experiments on three public datasets show that EBRank is effective, practical and significantly outperforms state-of-the-art ranking algorithms.|排名是许多人工智能(AI)应用程序的核心，包括搜索引擎、推荐系统等。现代排序系统通常是由用户行为信号构建的学习排序(LTR)模型构建的。虽然以前的研究已经证明了使用用户行为信号(例如点击)作为 LTR 算法的特征和标签的有效性，但是我们认为现有的 LTR 算法在输入特征中不加区分地处理行为和非行为信号可能导致实践中的次优性能。特别是由于用户行为信号往往与排名目标有很强的相关性，只能在已经显示给用户的条目上收集，直接在 LTR 中使用行为信号可能会产生利用偏差，从长远来看会损害系统性能。针对开发偏差问题，提出了一种基于经验贝叶斯的不确定性排序算法 EBRank。具体来说，为了克服排序模型中行为特征带来的利用偏差，EBRank 使用基于非行为特征的先验模型来获得相关性的先验估计。在排序系统的动态训练和服务中，EBRank 利用观察到的用户行为更新后验相关估计，而不是将用户行为作为排序模型的特征进行连接。此外，EBRank 还采用了不确定性探索策略，积极探索，收集用户行为进行经验贝叶斯建模，提高排序性能。在三个公共数据集上的实验表明，EBRank 算法是有效的、实用的，其性能明显优于最先进的排序算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mitigating+Exploitation+Bias+in+Learning+to+Rank+with+an+Uncertainty-aware+Empirical+Bayes+Approach)|0|
|[Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://doi.org/10.1145/3589334.3645574)|Songhao Wu, Quan Tu, Hong Liu, Jia Xu, Zhongyi Liu, Guannan Zhang, Ran Wang, Xiuying Chen, Rui Yan||Session search involves a series of interactive queries and actions to fulfill user's complex information need. Current strategies typically prioritize sequential modeling for deep semantic understanding, overlooking the graph structure in interactions. While some approaches focus on capturing structural information, they use a generalized representation for documents, neglecting the word-level semantic modeling. In this paper, we propose Symbolic Graph Ranker (SGR), which aims to take advantage of both text-based and graph-based approaches by leveraging the power of recent Large Language Models (LLMs). Concretely, we first introduce a set of symbolic grammar rules to convert session graph into text. This allows integrating session history, interaction process, and task instruction seamlessly as inputs for the LLM. Moreover, given the natural discrepancy between LLMs pre-trained on textual corpora, and the symbolic language we produce using our graph-to-text grammar, our objective is to enhance LLMs' ability to capture graph structures within a textual format. To achieve this, we introduce a set of self-supervised symbolic learning tasks including link prediction, node content generation, and generative contrastive learning, to enable LLMs to capture the topological information from coarse-grained to fine-grained. Experiment results and comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm the superiority of our approach. Our paradigm also offers a novel and effective methodology that bridges the gap between traditional search strategies and modern LLMs.|会话搜索涉及到一系列的交互式查询和操作，以满足用户复杂的信息需求。当前的策略通常优先考虑深层语义理解的顺序建模，忽略了交互中的图形结构。虽然一些方法侧重于获取结构信息，但它们对文档使用广义表示，忽略了词级语义建模。在本文中，我们提出了符号图排序器(SGR) ，它旨在利用基于文本和基于图的方法，利用最近的大型语言模型(LLM)的能力。具体来说，我们首先引入了一套符号语法规则来将会话图转换成文本。这允许将会话历史、交互过程和任务指令无缝地集成为 LLM 的输入。此外，考虑到预先在文本语料库中训练的 LLM 与我们使用图到文本语法生成的符号语言之间的自然差异，我们的目标是提高 LLM 在文本格式中捕获图结构的能力。为了实现这一目标，我们引入了一组自监督的符号学习任务，包括链路预测、节点内容生成和生成对比学习，使 LLM 能够捕获从粗粒度到细粒度的拓扑信息。通过对 AOL 和天宫 ST 两个基准数据集的实验和综合分析，证实了该方法的优越性。我们的范式还提供了一个新颖和有效的方法，桥梁之间的差距传统搜索策略和现代 LLM。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unify+Graph+Learning+with+Text:+Unleashing+LLM+Potentials+for+Session+Search)|0|
|[Matching Feature Separation Network for Domain Adaptation in Entity Matching](https://doi.org/10.1145/3589334.3645397)|Chenchen Sun, Yang Xu, Derong Shen, Tiezheng Nie||Entity matching (EM) determines whether two records from different data sources refer to the same real-world entity. It is a fundamental task in knowledge graph construction and data integration. Currently, deep learning (DL) based EM methods have achieved state-of-the-art (SOTA) results. However, apply-ing DL-based EM methods often costs a lot of human efforts to label the data. To address this challenge, we propose a new do-main adaptation (DA) framework for EM called Matching Fea-ture Separation Network (MFSN). We implement DA by sepa-rating private and common matching features. Briefly, MFSN first uses three encoders to explicitly model the private and common matching features in both the source and target do-mains. Then, it transfers the knowledge learned from the source common matching features to the target domain. We also pro-pose an enhanced variant called Feature Representation and Separation Enhanced MFSN (MFSN-FRSE). Compared with MFSN, it has superior feature representation and separation capabilities. We evaluate the effectiveness of MFSN and MFSN-FRSE on twelve DA in EM tasks. The results show that our framework is approximately 7% higher in F1 score on average than the previous SOTA methods. Then, we verify the effec-tiveness of each module in MFSN and MFSN-FRSE by ablation study. Finally, we explore the optimal strategy of each module in MFSN and MFSN-FRSE through detailed tests.|实体匹配(EM)确定来自不同数据源的两个记录是否引用相同的现实世界实体。它是知识图形构建和数据集成的基础性工作。目前，基于深度学习(DL)的 EM 方法已经取得了最先进的效果。然而，应用基于 DL 的 EM 方法通常需要花费大量人力来标记数据。为了应对这一挑战，我们提出了一种新的电磁主干适应(DA)框架，称为匹配特征分离网络(MFSN)。我们通过分级私有和公共匹配特性来实现 DA。简而言之，MFSN 首先使用三个编码器来显式地为源和目标操作系统中的私有和公共匹配特性建模。然后，将从源共同匹配特征中学到的知识转移到目标领域。我们还提出了一种增强型 MFSN (MFSN-FRSE) ，称为特征表示和分离增强 MFSN。与 MFSN 相比，它具有优越的特征表示和分离能力。我们评估了多功能神经网络(MFSN)和多功能神经网络-自由神经网络(MFSN-frse)在电磁任务中对十二种 DA 的有效性。结果表明，我们的框架在 F1得分平均比以前的 SOTA 方法高约7% 。然后，通过烧蚀研究验证了 MFSN 和 MFSN-FRSE 中各模块的有效性。最后，通过详细的测试，探讨了 MFSN 和 MFSN-FRSE 中各模块的优化策略。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Matching+Feature+Separation+Network+for+Domain+Adaptation+in+Entity+Matching)|0|
|[FedUP: Querying Large-Scale Federations of SPARQL Endpoints](https://doi.org/10.1145/3589334.3645704)|Julien AimonierDavat, Brice Nédelec, Minh Hoang Dang, Pascal Molli, Hala SkafMolli||Processing SPARQL queries over large federations of SPARQL endpoints is crucial for keeping the Semantic Web decentralized. Despite the existence of hundreds of SPARQL endpoints, current federation engines only scale to dozens. One major issue comes from the current definition of the source selection problem, i.e., finding the minimal set of SPARQL endpoints to contact per triple pattern. Even if such a source selection is minimal, only a few combinations of sources may return results. Consequently, most of the query processing time is wasted evaluating combinations that return no results. In this paper, we introduce the concept of Result-Aware query plans. This concept ensures that every subquery of the query plan effectively contributes to the result of the query. To compute a Result-Aware query plan, we propose FedUP, a new federation engine able to produce Result-Aware query plans by tracking the provenance of query results. However, getting query results requires computing source selection, and computing source selection requires query results. To break this vicious cycle, FedUP computes results and provenances on tiny quotient summaries of federations at the cost of source selection accuracy. Experimental results on federated benchmarks demonstrate that FedUP outperforms state-of-the-art federation engines by orders of magnitude in the context of large-scale federations.|在大型 SPARQL 端点联合上处理 SPARQL 查询对于保持语义 Web 的分散性至关重要。尽管存在数百个 SPARQL 端点，但当前的联合引擎只能扩展到几十个。一个主要问题来自源选择问题的当前定义，也就是说，寻找每个三重模式联系的最小 SPARQL 端点集。即使这样的源选择很少，也只有少数几个源组合可以返回结果。因此，大多数查询处理时间都浪费在评估不返回任何结果的组合上。本文介绍了结果感知查询计划的概念。这个概念确保查询计划的每个子查询都有效地提供查询结果。为了计算结果感知查询计划，我们提出了 FedUP，这是一个新的联合引擎，能够通过跟踪查询结果的来源来生成结果感知查询计划。但是，获取查询结果需要计算源选择，而计算源选择需要查询结果。为了打破这种恶性循环，FedUP 以牺牲源选择的准确性为代价，在联合的微小商汇总上计算结果和出处。联邦基准测试的实验结果表明，在大规模联邦环境中，联邦统一数量级的性能优于最先进的联邦引擎。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedUP:+Querying+Large-Scale+Federations+of+SPARQL+Endpoints)|0|
|[Prompt-enhanced Federated Content Representation Learning for Cross-domain Recommendation](https://doi.org/10.1145/3589334.3645337)|Lei Guo, Ziang Lu, Junliang Yu, Quoc Viet Hung Nguyen, Hongzhi Yin||Cross-domain Recommendation (CDR) as one of the effective techniques in alleviating the data sparsity issues has been widely studied in recent years. However, previous works may cause domain privacy leakage since they necessitate the aggregation of diverse domain data into a centralized server during the training process. Though several studies have conducted privacy preserving CDR via Federated Learning (FL), they still have the following limitations: 1) They need to upload users' personal information to the central server, posing the risk of leaking user privacy. 2) Existing federated methods mainly rely on atomic item IDs to represent items, which prevents them from modeling items in a unified feature space, increasing the challenge of knowledge transfer among domains. 3) They are all based on the premise of knowing overlapped users between domains, which proves impractical in real-world applications. To address the above limitations, we focus on Privacy-preserving Cross-domain Recommendation (PCDR) and propose PFCR as our solution. For Limitation 1, we develop a FL schema by exclusively utilizing users' interactions with local clients and devising an encryption method for gradient encryption. For Limitation 2, we model items in a universal feature space by their description texts. For Limitation 3, we initially learn federated content representations, harnessing the generality of natural language to establish bridges between domains. Subsequently, we craft two prompt fine-tuning strategies to tailor the pre-trained model to the target domain. Extensive experiments on two real-world datasets demonstrate the superiority of our PFCR method compared to the SOTA approaches.|跨域推荐(CDR)作为缓解数据稀疏性问题的有效技术之一，近年来得到了广泛的研究。然而，以前的工作可能会导致域隐私泄漏，因为它们需要在培训过程中将不同的域数据集中到一个集中的服务器。虽然已有多项研究通过联邦学习(FL)进行隐私保护 CDR，但仍存在以下局限性: 1)需要将用户的个人信息上传到中央服务器，存在泄露用户隐私的风险。2)现有的联邦方法主要依靠原子项目 ID 来表示项目，这使得联邦方法无法在统一的特征空间中对项目进行建模，增加了领域间知识转移的难度。3)它们都是建立在知道域间重叠用户的前提上的，这在现实应用中是不切实际的。针对上述局限性，本文重点研究了保护隐私的跨域推荐(PCDR)技术，并提出了 PCR 技术作为解决方案。对于限制1，我们通过专门利用用户与本地客户端的交互和设计一种梯度加密的加密方法来开发一个 FL 模式。对于局限性2，我们通过描述文本在通用特征空间中对项目进行建模。对于限制3，我们最初学习联邦内容表示，利用自然语言的通用性在域之间建立桥梁。随后，我们制定了两个快速微调策略，以裁剪预训练模型的目标领域。在两个实际数据集上的大量实验证明了我们的 PFCR 方法相对于 SOTA 方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prompt-enhanced+Federated+Content+Representation+Learning+for+Cross-domain+Recommendation)|0|
|[PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning](https://doi.org/10.1145/3589334.3645359)|Wei Wei, Jiabin Tang, Lianghao Xia, Yangqin Jiang, Chao Huang||Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems. These modalities provide intuitive semantics that facilitate modality-aware user preference modeling. However, two key challenges in multi-modal recommenders remain unresolved: i) The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT). ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference. To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation. Specifically, PromptMM conducts model compression through distilling u-i edge relationship and multi-modal node content from cumbersome teachers to relieve students from the additional feature reduction parameters. To bridge the semantic gap between multi-modal context and collaborative signals for empowering the overfitting teacher, soft prompt-tuning is introduced to perform student task-adaptive. Additionally, to adjust the impact of inaccuracies in multimedia data, a disentangled multi-modal list-wise distillation is developed with modality-aware re-weighting mechanism. Experiments on real-world data demonstrate PromptMM's superiority over existing techniques. Ablation tests confirm the effectiveness of key components. Additional tests show the efficiency and effectiveness.|多媒体在线平台(如亚马逊、 TikTok)从将多媒体(如视觉、文本和声学)内容纳入其个人推荐系统中获益匪浅。这些模式提供了直观的语义，促进了模式感知用户偏好建模。然而，多模态推荐器中的两个关键挑战仍然没有解决: i)引入具有大量额外参数的多模态编码器导致过拟合，给定提取器(例如，ViT，BERT)提供的高维多模态特征。(2)侧面信息不可避免地引入不准确性和冗余性，从而使情态交互依赖偏离了真实的用户偏好。为了解决这些问题，我们建议通过多模态知识提取(PromptMM)简化和授权推荐程序，并进行及时调优，以实现自适应质量提取。具体来说，PromptMM 通过从繁琐的教师中提取 u-i 边缘关系和多模态节点内容来进行模型压缩，以减轻学生对附加特征约简参数的依赖。为了消除多模态语境和协作信号之间的语义鸿沟，提出了软提示调整来实现学生的任务适应。此外，为了调整多媒体数据中不准确性的影响，利用模态感知的重新加权机制，开发了一种分离多模态列表式精馏算法。对真实世界数据的实验证明了 PromptMM 相对于现有技术的优越性。烧蚀试验证实了关键部位的有效性。额外的测试显示了效率和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PromptMM:+Multi-Modal+Knowledge+Distillation+for+Recommendation+with+Prompt-Tuning)|0|
|[Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum Heating](https://doi.org/10.1145/3589334.3645377)|Hyunsik Jeon, Jongeun Lee, Jeongin Yun, U Kang||How can we recommend cold-start bundles to users? The cold-start problem in bundle recommendation is crucial because new bundles are continuously created on the Web for various marketing purposes. Despite its importance, existing methods for cold-start item recommendation are not readily applicable to bundles. They depend overly on historical information, even for less popular bundles, failing to address the primary challenge of the highly skewed distribution of bundle interactions. In this work, we propose CoHeat (Popularity-based Coalescence and Curriculum Heating), an accurate approach for cold-start bundle recommendation. CoHeat first represents users and bundles through graph-based views, capturing collaborative information effectively. To estimate the user-bundle relationship more accurately, CoHeat addresses the highly skewed distribution of bundle interactions through a popularity-based coalescence approach, which incorporates historical and affiliation information based on the bundle's popularity. Furthermore, it effectively learns latent representations by exploiting curriculum learning and contrastive learning. CoHeat demonstrates superior performance in cold-start bundle recommendation, achieving up to 193|我们如何向用户推荐冷启动捆绑包？捆绑包推荐中的冷启动问题是至关重要的，因为出于各种营销目的，新的捆绑包不断地在 Web 上创建。尽管它的重要性，现有的冷启动项目推荐方法不容易适用于捆绑包。它们过度依赖于历史信息，即使对于不太流行的捆绑包也是如此，未能解决捆绑包交互的高度倾斜分布这一主要挑战。在这项工作中，我们提出了 CoHeat (基于流行度的聚合和课程加热) ，一种准确的冷启动捆绑推荐方法。CoHeat 首先通过基于图形的视图表示用户和捆绑包，有效地捕获协作信息。为了更准确地估计用户与捆绑包的关系，CoHeat 通过基于流行度的合并方法解决了捆绑包交互的高度倾斜分布，该方法结合了基于捆绑包流行度的历史和从属关系信息。此外，它还通过课程学习和对比学习有效地学习潜在表征。CoHeat 在冷启动捆绑推荐系统中表现出卓越的性能，最高达到193|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cold-start+Bundle+Recommendation+via+Popularity-based+Coalescence+and+Curriculum+Heating)|0|
|[Scalable and Provably Fair Exposure Control for Large-Scale Recommender Systems](https://doi.org/10.1145/3589334.3645390)|Riku Togashi, Kenshi Abe, Yuta Saito||Typical recommendation and ranking methods aim to optimize the satisfaction of users, but they are often oblivious to their impact on the items (e.g., products, jobs, news, video) and their providers. However, there has been a growing understanding that the latter is crucial to consider for a wide range of applications, since it determines the utility of those being recommended. Prior approaches to fairness-aware recommendation optimize a regularized objective to balance user satisfaction and item fairness based on some notion such as exposure fairness. These existing methods have been shown to be effective in controlling fairness, however, most of them are computationally inefficient, limiting their applications to only unrealistically small-scale situations. This indeed implies that the literature does not yet provide a solution to enable a flexible control of exposure in the industry-scale recommender systems where millions of users and items exist. To enable a computationally efficient exposure control even for such large-scale systems, this work develops a scalable, fast, and fair method called exposure-aware ADMM (exADMM). exADMM is based on implicit alternating least squares (iALS), a conventional scalable algorithm for collaborative filtering, but optimizes a regularized objective to achieve a flexible control of accuracy-fairness tradeoff. A particular technical challenge in developing exADMM is the fact that the fairness regularizer destroys the separability of optimization subproblems for users and items, which is an essential property to ensure the scalability of iALS. Therefore, we develop a set of optimization tools to enable yet scalable fairness control with provable convergence guarantees as a basis of our algorithm.|典型的推荐和排名方法旨在优化用户的满意度，但他们往往忽视了它们对项目(例如，产品、工作、新闻、视频)及其提供商的影响。然而，人们日益认识到，后者对于考虑范围广泛的应用至关重要，因为它决定了所推荐的应用的效用。先前的公平感知推荐方法优化了一个规范化的目标，以平衡用户满意度和项目公平性，基于一些概念，如曝光公平性。这些现有的方法已被证明是有效的控制公平，但是，其中大多数是计算效率低下，限制其应用于不切实际的小规模情况。这实际上意味着，文献尚未提供一个解决方案，以便在存在数百万用户和项目的行业规模的推荐系统中实现对曝光的灵活控制。为了实现计算效率高的曝光控制，即使是在这样的大规模系统中，这项工作开发了一种可伸缩、快速和公平的方法，称为曝光感知 ADMM (exADMM)。ExADMM 基于隐式交替最小二乘(iALS)算法，这是一种传统的可扩展的协同过滤算法，但优化了正则化目标，以实现灵活的精度-公平权衡控制。开发 exADMM 的一个特殊的技术挑战是公平正则化器破坏了用户和项目优化子问题的可分性，而这种可分性是保证 iALS 可扩展性的一个重要特性。因此，我们开发了一套优化工具，使得可扩展的公平性控制具有可证明的收敛保证作为我们的算法的基础。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+and+Provably+Fair+Exposure+Control+for+Large-Scale+Recommender+Systems)|0|
|[Causally Debiased Time-aware Recommendation](https://doi.org/10.1145/3589334.3645400)|Lei Wang, Chen Ma, Xian Wu, Zhaopeng Qiu, Yefeng Zheng, Xu Chen||Time-aware recommendation has been widely studied for modeling the user dynamic preference and a lot of models have been proposed. However, these models often overlook the fact that users may not behave evenly on the timeline, and observed datasets can be biased by user intrinsic preferences or previous recommender systems, leading to degraded model performance. We propose a causally debiased time-aware recommender framework to accurately learn user preference. We formulate the task of time-aware recommendation by a causal graph, identifying two types of biases on the item and time levels. To optimize the ideal unbiased learning objective, we propose a debiased framework based on the inverse propensity score (IPS) and extend it to the doubly robust method. Considering that the user preference can be diverse and complex, which may result in unmeasured confounders, we develop a sensitivity analysis method to obtain more accurate IPS. We theoretically draw a connection between the proposed method and the ideal learning objective, which to the best of our knowledge, is the first time in the research community. We conduct extensive experiments on three real-world datasets to demonstrate the effectiveness of our model. To promote this research direction, we have released our project at https://paitesanshi.github.io/CDTR/.|时间感知推荐在用户动态偏好建模方面得到了广泛的研究，并提出了许多模型。然而，这些模型往往忽略了这样一个事实，即用户可能在时间轴上的行为不均匀，并且观察到的数据集可能受到用户内在偏好或以前的推荐系统的影响，导致模型性能下降。我们提出了一个因果消除时间感知的推荐框架，以准确地了解用户偏好。我们通过一个因果图表来描述有时间意识的推荐任务，识别项目和时间水平上的两种偏差。为了优化理想的无偏学习目标，提出了一种基于逆倾向得分(IPS)的去偏框架，并将其推广到双稳健方法。考虑到用户偏好可能是多样和复杂的，这可能导致不可测量的混杂因素，我们开发了一个敏感度分析方法来获得更准确的 IPS。我们从理论上把这种方法与理想的学习目标联系起来，据我们所知，这在研究界还是第一次。为了验证模型的有效性，我们在三个实际数据集上进行了大量的实验。为了推动这个研究方向，我们已经在 https://paitesanshi.github.io/cdtr/上发布了我们的项目。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causally+Debiased+Time-aware+Recommendation)|0|
|[Learning Category Trees for ID-Based Recommendation: Exploring the Power of Differentiable Vector Quantization](https://doi.org/10.1145/3589334.3645484)|Qijiong Liu, Jiaren Xiao, Lu Fan, Jieming Zhu, XiaoMing Wu||Category information plays a crucial role in enhancing the quality and personalization of recommender systems. Nevertheless, the availability of item category information is not consistently present, particularly in the context of ID-based recommendations. In this work, we propose a novel approach to automatically learn and generate entity (i.e., user or item) category trees for ID-based recommendation. Specifically, we devise a differentiable vector quantization framework for automatic category tree generation, namely CAGE, which enables the simultaneous learning and refinement of categorical code representations and entity embeddings in an end-to-end manner, starting from the randomly initialized states. With its high adaptability, CAGE can be easily integrated into both sequential and non-sequential recommender systems. We validate the effectiveness of CAGE on various recommendation tasks including list completion, collaborative filtering, and click-through rate prediction, across different recommendation models. We release the code and data for others to reproduce the reported results.|分类信息在提高推荐系统的质量和个性化方面起着至关重要的作用。尽管如此，项目类别信息的可用性并不一致，特别是在基于 ID 的推荐上下文中。在这项工作中，我们提出了一个新颖的方法来自动学习和生成实体(即，用户或项目)的类别树为基于身份的推荐。具体来说，我们设计了一个自动分类树生成的可微向量量化框架，即 CAGE，它能够从随机初始化状态开始，以端到端的方式同时学习和细化分类代码表示和实体嵌入。CAGE 具有很强的适应性，可以很容易地集成到顺序推荐系统和非顺序推荐系统中。在不同的推荐模型中，我们验证了 CAGE 在各种推荐任务中的有效性，包括列表完成、协同过滤和点进率预测。我们将代码和数据发布给其他人，以重现报告的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Category+Trees+for+ID-Based+Recommendation:+Exploring+the+Power+of+Differentiable+Vector+Quantization)|0|
|[Poisoning Federated Recommender Systems with Fake Users](https://doi.org/10.1145/3589334.3645492)|Ming Yin, Yichang Xu, Minghong Fang, Neil Zhenqiang Gong||Federated recommendation is a prominent use case within federated learning, yet it remains susceptible to various attacks, from user to server-side vulnerabilities. Poisoning attacks are particularly notable among user-side attacks, as participants upload malicious model updates to deceive the global model, often intending to promote or demote specific targeted items. This study investigates strategies for executing promotion attacks in federated recommender systems. Current poisoning attacks on federated recommender systems often rely on additional information, such as the local training data of genuine users or item popularity. However, such information is challenging for the potential attacker to obtain. Thus, there is a need to develop an attack that requires no extra information apart from item embeddings obtained from the server. In this paper, we introduce a novel fake user based poisoning attack named PoisonFRS to promote the attacker-chosen targeted item in federated recommender systems without requiring knowledge about user-item rating data, user attributes, or the aggregation rule used by the server. Extensive experiments on multiple real-world datasets demonstrate that PoisonFRS can effectively promote the attacker-chosen targeted item to a large portion of genuine users and outperform current benchmarks that rely on additional information about the system. We further observe that the model updates from both genuine and fake users are indistinguishable within the latent space.|联合推荐是联合学习中一个突出的用例，但它仍然容易受到从用户到服务器端漏洞的各种攻击。中毒攻击在用户端攻击中尤其显著，因为参与者上传恶意模型更新以欺骗全局模型，通常意图促进或降级特定目标项目。本研究探讨在联邦推荐系统中执行推广攻击的策略。当前对联邦推荐系统的中毒攻击通常依赖于附加信息，比如真正用户的本地培训数据或项目流行度。然而，这样的信息对于潜在的攻击者来说是具有挑战性的。因此，需要开发一种除了从服务器获得的项嵌入之外不需要额外信息的攻击。本文提出了一种新的基于虚假用户的中毒攻击 PoisonFRS，它在不需要用户评分数据、用户属性和服务器聚合规则的情况下，在联邦推荐系统中推广攻击者选择的目标项。在多个真实世界数据集上的大量实验表明，PoisonFRS 能够有效地将攻击者选择的目标项目推广到大部分真实用户，并且比依赖于系统附加信息的当前基准测试表现更好。我们进一步观察到，来自真实用户和虚假用户的模型更新在潜在空间内是不可区分的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Poisoning+Federated+Recommender+Systems+with+Fake+Users)|0|
|[Could Small Language Models Serve as Recommenders? Towards Data-centric Cold-start Recommendation](https://doi.org/10.1145/3589334.3645494)|Xuansheng Wu, Huachi Zhou, Yucheng Shi, Wenlin Yao, Xiao Huang, Ninghao Liu||Recommendation systems help users find information that matches their interests based on their historical behaviors. However, generating personalized recommendations becomes challenging in the absence of historical user-item interactions, a practical problem for startups known as the system cold-start recommendation. Current research tackles user or item cold-start scenarios but lacks solutions for system cold-start. To tackle the problem, we initially propose PromptRec, a simple but effective approach based on in-context learning of language models, where we transform the recommendation task into the sentiment analysis task on natural language containing user and item profiles. However, this naive strategy heavily relied on the strong in-context learning ability emerged from large language models, which could suffer from significant latency for online recommendations. To fill this gap, we present a theoretical framework to formalize the connection between in-context recommendation and language modeling. Based on it, we propose to enhance small language models with a data-centric pipeline, which consists of: (1) constructing a refined corpus for model pre-training; (2) constructing a decomposed prompt template via prompt pre-training. They correspond to the development of training data and inference data, respectively. To evaluate our proposed method, we introduce a cold-start recommendation benchmark, and the results demonstrate that the enhanced small language models can achieve comparable cold-start recommendation performance to that of large models with only around 17 time. To the best of our knowledge, this is the first study to tackle the system cold-start recommendation problem. We believe our findings will provide valuable insights for future works. The benchmark and implementations are available at https://github.com/JacksonWuxs/PromptRec.|推荐系统帮助用户根据他们的历史行为找到符合他们兴趣的信息。然而，在缺乏历史用户项目交互的情况下，生成个性化推荐变得具有挑战性，这是创业公司面临的一个实际问题，称为系统冷启动推荐。目前的研究涉及用户或项目冷启动场景，但缺乏系统冷启动的解决方案。为了解决这一问题，我们首先提出了 PromptRec，这是一种基于语言模型上下文学习的简单而有效的方法，我们将推荐任务转化为包含用户和项目概要的自然语言情感分析任务。然而，这种幼稚的策略在很大程度上依赖于大型语言模型中出现的强大的上下文学习能力，这种能力可能会受到在线推荐显著延迟的影响。为了填补这一空白，我们提出了一个理论框架，以形式化之间的联系上下文推荐和语言建模。在此基础上，我们提出了一种以数据为中心的流水线来增强小语言模型，包括: (1)构造一个精化的模型预训练语料库; (2)通过提示预训练构造一个分解的提示模板。它们分别对应于训练数据和推理数据的发展。为了对我们提出的方法进行评估，我们引入了一个冷启动推荐基准，结果表明，增强的小语言模型只需要17次左右的时间就可以实现与大型模型相当的冷启动推荐性能。据我们所知，这是第一个解决系统冷启动推荐问题的研究。我们相信我们的发现将为今后的工作提供有价值的见解。基准及实施 https://github.com/jacksonwuxs/promptrec 已备妥。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Could+Small+Language+Models+Serve+as+Recommenders?+Towards+Data-centric+Cold-start+Recommendation)|0|
|[Recommender Transformers with Behavior Pathways](https://doi.org/10.1145/3589334.3645528)|Zhiyu Yao, Xinyang Chen, Sinan Wang, Qinyan Dai, Yumeng Li, Tanchao Zhu, Mingsheng Long||Sequential recommendation requires the recommender to capture the evolving behavior characteristics from logged user behavior data for accurate recommendations. However, user behavior sequences are viewed as a script with multiple ongoing threads intertwined. We find that only a small set of pivotal behaviors can be evolved into the user's future action. As a result, the future behavior of the user is hard to predict. We conclude this characteristic for sequential behaviors of each user as the Behavior Pathway. Different users have their unique behavior pathways. Among existing sequential models, transformers have shown great capacity in capturing global-dependent characteristics. However, these models mainly provide a dense distribution over all previous behaviors using the self-attention mechanism, making the final predictions overwhelmed by the trivial behaviors not adjusted to each user. In this paper, we build the Recommender Transformer (RETR) with a novel Pathway Attention mechanism. RETR can dynamically plan the behavior pathway specified for each user, and sparingly activate the network through this behavior pathway to effectively capture evolving patterns useful for recommendation. The key design is a learned binary route to prevent the behavior pathway from being overwhelmed by trivial behaviors. We empirically verify the effectiveness of RETR on seven real-world datasets and RETR yields state-of-the-art performance.|连续推荐需要推荐者从日志用户行为数据中捕获不断变化的行为特征，以获得准确的推荐。但是，用户行为序列被看作是一个多个正在进行的线程交织在一起的脚本。我们发现只有一小部分关键行为可以演化为用户未来的行为。因此，用户未来的行为很难预测。我们将每个用户的连续行为归结为行为路径。不同的用户有他们独特的行为路径。在现有的顺序模型中，变压器在捕捉全局相关特性方面表现出了很大的能力。然而，这些模型主要利用自我注意机制，在所有以前的行为上提供了一个密集的分布，使得最终的预测被不适应每个用户的琐碎行为所淹没。在本文中，我们建立了推荐变压器(RETR)与一个新的路径注意机制。RETR 可以动态规划为每个用户指定的行为路径，并通过该行为路径激活网络，以有效地捕获推荐所需的演化模式。关键的设计是一个学习的二进制路径，以防止行为路径被琐碎的行为所淹没。我们通过实验验证了 RETR 在七个实际数据集上的有效性，并且 RETR 产生了最先进的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recommender+Transformers+with+Behavior+Pathways)|0|
|[Ensuring User-side Fairness in Dynamic Recommender Systems](https://doi.org/10.1145/3589334.3645536)|Hyunsik Yoo, Zhichen Zeng, Jian Kang, Ruizhong Qiu, David Zhou, Zhining Liu, Fei Wang, Charlie Xu, Eunice Chan, Hanghang Tong||User-side group fairness is crucial for modern recommender systems, aiming to alleviate performance disparities among user groups defined by sensitive attributes like gender, race, or age. In the ever-evolving landscape of user-item interactions, continual adaptation to newly collected data is crucial for recommender systems to stay aligned with the latest user preferences. However, we observe that such continual adaptation often exacerbates performance disparities. This necessitates a thorough investigation into user-side fairness in dynamic recommender systems, an area that has been unexplored in the literature. This problem is challenging due to distribution shifts, frequent model updates, and non-differentiability of ranking metrics. To our knowledge, this paper presents the first principled study on ensuring user-side fairness in dynamic recommender systems. We start with theoretical analyses on fine-tuning v.s. retraining, showing that the best practice is incremental fine-tuning with restart. Guided by our theoretical analyses, we propose FAir Dynamic rEcommender (FADE), an end-to-end fine-tuning framework to dynamically ensure user-side fairness over time. To overcome the non-differentiability of recommendation metrics in the fairness loss, we further introduce Differentiable Hit (DH) as an improvement over the recent NeuralNDCG method, not only alleviating its gradient vanishing issue but also achieving higher efficiency. Besides that, we also address the instability issue of the fairness loss by leveraging the competing nature between the recommendation loss and the fairness loss. Through extensive experiments on real-world datasets, we demonstrate that FADE effectively and efficiently reduces performance disparities with little sacrifice in the overall recommendation performance.|用户端群组公平性对于现代推荐系统至关重要，旨在缓解由性别、种族或年龄等敏感属性定义的用户群之间的性能差异。在不断变化的用户项交互环境中，持续适应新收集的数据对于推荐系统保持与最新用户偏好一致至关重要。然而，我们观察到这种持续的适应常常加剧性能差异。这就需要对动态推荐系统中的用户端公平性进行彻底的研究，这是文献中未曾探索过的领域。这个问题是具有挑战性的，因为分布转移，频繁的模型更新，和不可区分的排名度量。据我们所知，本文提出了第一个原则性的研究，以确保用户端公平性的动态推荐系统。我们从微调和再训练的理论分析开始，表明最佳实践是重新启动后的渐进微调。在理论分析的指导下，我们提出了 FAir Dynamic reComdender (FADE) ，这是一个端到端的微调框架，可以随着时间的推移动态地保证用户端的公平性。为了克服推荐度量在公平性损失中的不可微性，我们进一步引入了可微命中(DH)算法，作为对最近的神经网络 NDCG 方法的改进，不仅减轻了它的梯度消失问题，而且实现了更高的效率。此外，我们还利用推荐损失和公平损失之间的竞争性质来解决公平损失的不稳定性问题。通过对现实世界数据集的大量实验，我们证明了 FADE 能够有效地降低性能差异，并且在总体推荐性能方面做出了很小的牺牲。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ensuring+User-side+Fairness+in+Dynamic+Recommender+Systems)|0|
|[Mirror Gradient: Towards Robust Multimodal Recommender Systems via Exploring Flat Local Minima](https://doi.org/10.1145/3589334.3645553)|Shanshan Zhong, Zhongzhan Huang, Daifeng Li, Wushao Wen, Jinghui Qin, Liang Lin||Multimodal recommender systems utilize various types of information to model user preferences and item features, helping users discover items aligned with their interests. The integration of multimodal information mitigates the inherent challenges in recommender systems, e.g., the data sparsity problem and cold-start issues. However, it simultaneously magnifies certain risks from multimodal information inputs, such as information adjustment risk and inherent noise risk. These risks pose crucial challenges to the robustness of recommendation models. In this paper, we analyze multimodal recommender systems from the novel perspective of flat local minima and propose a concise yet effective gradient strategy called Mirror Gradient (MG). This strategy can implicitly enhance the model's robustness during the optimization process, mitigating instability risks arising from multimodal information inputs. We also provide strong theoretical evidence and conduct extensive empirical experiments to show the superiority of MG across various multimodal recommendation models and benchmarks. Furthermore, we find that the proposed MG can complement existing robust training methods and be easily extended to diverse advanced recommendation models, making it a promising new and fundamental paradigm for training multimodal recommender systems. The code is released at https://github.com/Qrange-group/Mirror-Gradient.|多模式推荐系统利用各种类型的信息来建模用户偏好和项目特征，帮助用户发现符合他们兴趣的项目。多模态信息的集成缓解了推荐系统固有的挑战，如数据稀疏问题和冷启动问题。然而，它同时放大了来自多模态信息输入的某些风险，如信息调整风险和固有噪声风险。这些风险对推荐模型的稳健性提出了严峻的挑战。本文从平坦局部极小的角度分析了多模态推荐系统，提出了一种简洁有效的梯度策略——镜像梯度(MG)。该策略可以在优化过程中增强模型的鲁棒性，减少多模态信息输入引起的不稳定风险。我们还提供了强有力的理论证据，并进行了广泛的实证实验，以显示 MG 在各种多模式推荐模型和基准测试中的优越性。此外，我们发现所提出的 MG 可以补充现有的鲁棒训练方法，并且可以很容易地扩展到不同的高级推荐模型，使它成为训练多模式推荐系统的一个有前途的新的和基本的范例。密码在 https://github.com/qrange-group/mirror-gradient 发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mirror+Gradient:+Towards+Robust+Multimodal+Recommender+Systems+via+Exploring+Flat+Local+Minima)|0|
|[Adaptive Neural Ranking Framework: Toward Maximized Business Goal for Cascade Ranking Systems](https://doi.org/10.1145/3589334.3645605)|Yunli Wang, Zhiqiang Wang, Jian Yang, Shiyang Wen, Dongying Kong, Han Li, Kun Gai||Cascade ranking is widely used for large-scale top-k selection problems in online advertising and recommendation systems, and learning-to-rank is an important way to optimize the models in cascade ranking. Previous works on learning-to-rank usually focus on letting the model learn the complete order or top-k order, and adopt the corresponding rank metrics (e.g. OPA and NDCG@k) as optimization targets. However, these targets can not adapt to various cascade ranking scenarios with varying data complexities and model capabilities; and the existing metric-driven methods such as the Lambda framework can only optimize a rough upper bound of limited metrics, potentially resulting in sub-optimal and performance misalignment. To address these issues, we propose a novel perspective on optimizing cascade ranking systems by highlighting the adaptability of optimization targets to data complexities and model capabilities. Concretely, we employ multi-task learning to adaptively combine the optimization of relaxed and full targets, which refers to metrics Recall@m@k and OPA respectively. We also introduce permutation matrix to represent the rank metrics and employ differentiable sorting techniques to relax hard permutation matrix with controllable approximate error bound. This enables us to optimize both the relaxed and full targets directly and more appropriately. We named this method as Adaptive Neural Ranking Framework (abbreviated as ARF). Furthermore, we give a specific practice under ARF. We use the NeuralSort to obtain the relaxed permutation matrix and draw on the variant of the uncertainty weight method in multi-task learning to optimize the proposed losses jointly. Experiments on a total of 4 public and industrial benchmarks show the effectiveness and generalization of our method, and online experiment shows that our method has significant application value.|在在线广告和推荐系统中，级联排名被广泛应用于大规模的 Top-k 选择问题，而学习排名是级联排名模型优化的重要途径。先前关于学习排序的工作通常集中在让模型学习完整的顺序或 top-k 顺序，并采用相应的排序指标(如 OPA 和 NDCG@k)作为优化目标。然而，这些目标不能适应各种级联排序场景与不同的数据复杂性和模型能力; 和现有的度量驱动的方法，如 Lambda 框架只能优化一个粗略的上限有限的度量，潜在地导致次优化和性能不一致。为了解决这些问题，我们提出了一个优化级联排序系统的新视角，强调优化目标对数据复杂性和模型能力的适应性。具体而言，我们采用多任务学习的方法，自适应地将松弛目标和完全目标的优化结合起来，分别使用 Recall@m@k 和 OPA 度量。我们还引入置换矩阵来表示排名度量，并使用可微分排序技术来放松具有可控近似误差界限的硬置换矩阵。这使我们能够直接和更适当地优化宽松和完整的目标。我们将这种方法命名为自适应神经排序框架(简称 ARF)。此外，我们还给出了 ARF 下的具体实践。我们使用神经排序来获得松弛置换矩阵，并利用多任务学习中不确定性权重方法的变体来共同优化提出的损失。在4个公共基准和行业基准上的实验表明了该方法的有效性和推广性，在线实验表明该方法具有较高的应用价值。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adaptive+Neural+Ranking+Framework:+Toward+Maximized+Business+Goal+for+Cascade+Ranking+Systems)|0|
|[General Debiasing for Graph-based Collaborative Filtering via Adversarial Graph Dropout](https://doi.org/10.1145/3589334.3645667)|An Zhang, Wenchang Ma, Pengbo Wei, Leheng Sheng, Xiang Wang||Graph neural networks (GNNs) have shown impressive performance in recommender systems, particularly in collaborative filtering (CF). The key lies in aggregating neighborhood information on a user-item interaction graph to enhance user/item representations. However, we have discovered that this aggregation mechanism comes with a drawback, which amplifies biases present in the interaction graph. For instance, a user's interactions with items can be driven by both unbiased true interest and various biased factors like item popularity or exposure. However, the current aggregation approach combines all information, both biased and unbiased, leading to biased representation learning. Consequently, graph-based recommenders can learn distorted views of users/items, hindering the modeling of their true preferences and generalizations. To address this issue, we introduce a novel framework called Adversarial Graph Dropout (AdvDrop). It differentiates between unbiased and biased interactions, enabling unbiased representation learning. For each user/item, AdvDrop employs adversarial learning to split the neighborhood into two views: one with bias-mitigated interactions and the other with bias-aware interactions. After view-specific aggregation, AdvDrop ensures that the bias-mitigated and bias-aware representations remain invariant, shielding them from the influence of bias. We validate AdvDrop's effectiveness on five public datasets that cover both general and specific biases, demonstrating significant improvements. Furthermore, our method exhibits meaningful separation of subgraphs and achieves unbiased representations for graph-based CF models, as revealed by in-depth analysis. Our code is publicly available at https://github.com/Arthurma71/AdvDrop.|图形神经网络(GNN)在推荐系统中表现出了令人印象深刻的性能，特别是在协同过滤(CF)中。关键在于聚合用户-项目交互图上的邻域信息，以增强用户/项目的表示。然而，我们发现这种聚合机制有一个缺点，那就是放大了交互图中存在的偏差。例如，用户与项目的交互可以由无偏见的真实兴趣和各种偏见的因素驱动，如项目受欢迎程度或曝光率。然而，目前的聚合方法结合了所有的信息，包括有偏的和无偏的，导致有偏的表征学习。因此，基于图表的推荐程序可以了解用户/项目的扭曲视图，从而阻碍对其真实偏好和概括的建模。为了解决这个问题，我们引入了一个新的框架，称为对抗图丢失(AdvDrop)。它区分无偏见和有偏见的互动，使无偏见的表征学习。对于每个用户/项目，AdvDrop 使用对抗性学习将邻居分成两个视图: 一个视图具有减轻偏见的交互作用，另一个视图具有意识到偏见的交互作用。在视图特定聚合之后，AdvDrop 确保偏差缓解和偏差感知表示保持不变，从而保护它们免受偏差的影响。我们验证了 AdvDrop 在五个公共数据集上的有效性，这些数据集涵盖了一般和特定的偏差，显示了显著的改进。此外，我们的方法表现出有意义的子图分离，并实现了基于图的 CF 模型的无偏表示，如深入分析所揭示的。我们的代码可以在 https://github.com/arthurma71/advdrop 上公开获取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=General+Debiasing+for+Graph-based+Collaborative+Filtering+via+Adversarial+Graph+Dropout)|0|
|[Online Sequential Decision-Making with Unknown Delays](https://doi.org/10.1145/3589334.3645388)|Ping Wu, Heyan Huang, Zhengyang Liu||In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay. Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback. Our proposed algorithms are versatile and applicable to universal norms. Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points. For each type of algorithm, we provide corresponding regret bounds under cases of general convexity and relative strong convexity, respectively. We also demonstrate the efficiency of each algorithm under different norms through concrete examples. Furthermore, our theoretical results are consistent with the current best bounds when degenerated to standard settings.|在在线顺序决策领域，我们利用在线凸优化(OCO)的框架来解决延迟问题，其中决策的反馈可以以未知的延迟到达。与以往仅限于欧氏范数和梯度信息的研究不同，我们提出了三类基于近似解的延迟算法来处理不同类型的接收反馈。我们提出的算法是通用的，并适用于普遍规范。具体来说，我们介绍了一系列的延迟正则化领先算法用于损失函数的全信息反馈，一系列的延迟镜像下降算法用于损失函数的梯度信息反馈，以及一系列的简化延迟镜像下降算法用于相应决策点的损失函数梯度的值信息反馈。对于每种算法，我们分别在一般凸性和相对强凸性情况下给出了相应的后悔界。并通过具体实例说明了每种算法在不同规范下的有效性。此外，我们的理论结果与当前退化到标准设置的最佳界限一致。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Sequential+Decision-Making+with+Unknown+Delays)|0|
|[Triage of Messages and Conversations in a Large-Scale Child Victimization Corpus](https://doi.org/10.1145/3589334.3648142)|Prasanna Lakkur Subramanyam, Mohit Iyyer, Brian Neil Levine||Children are among the most vulnerable online populations. Reports of child sexual exploitation on social media and apps have grown annually at an alarming rate and are overwhelming investigators. Even a single case can require examining millions of messages involving hundreds of victims. Triage and prioritization based on victims' experiences is an unfortunate necessity. Using a chat dataset of more than 3 million messages between victims and perpetrators, we evaluate and contribute tools for analyzing the experiences of victims of sexual exploitation. We develop both supervised and unsupervised methods to classify messages into categories of interest to law enforcement, such as age requests, persuasion, and sexual messages. We also introduce a conversation clustering technique to illuminate differences among victims' experiences based on their chat history. Through a qualitative analysis, we demonstrate that the learned clusters are coherent and represent distinct conversation patterns. For example, we can distinguish groups of users who never comply with sexual requests, comply after a few conversations, or comply immediately after being targeted. We expect this approach and associated visualizations will aid law enforcement, industry moderators, and sociologists who need to analyze massive corpora in this domain. Finally, we validate prior models derived from conversations involving adults pretending to be minors and provide statistics that could help undercover adults more accurately portray minor victims.|儿童是最容易受到网络攻击的群体之一。社交媒体和应用程序上关于儿童性剥削的报告每年都在以惊人的速度增长，调查人员已经无所适从。即使是一个单独的案件也可能需要检查涉及数百名受害者的数百万条信息。根据受害者的经历进行分类和优先排序是一种不幸的必要性。我们利用受害者和犯罪者之间300多万条信息的聊天数据集，评估和贡献工具，分析性剥削受害者的经历。我们开发了有监督和无监督的方法，将信息分类为执法部门感兴趣的类别，如年龄要求、说服和性信息。我们还引入了一种会话聚类技术，根据受害者的聊天历史来说明他们的不同经历。通过定性分析，我们证明了学习集群是连贯的，代表了不同的会话模式。例如，我们可以区分从不服从性要求、在几次谈话后服从或在成为目标后立即服从的用户群。我们期望这种方法和相关的可视化将有助于执法，行业协调员和社会学家谁需要分析这个领域的大量语料库。最后，我们验证了先前从成年人假装未成年人的谈话中得出的模型，并提供统计数据，可以帮助卧底成年人更准确地描绘未成年受害者。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Triage+of+Messages+and+Conversations+in+a+Large-Scale+Child+Victimization+Corpus)|0|
|[Collaborative-Enhanced Prediction of Spending on Newly Downloaded Mobile Games under Consumption Uncertainty](https://doi.org/10.1145/3589335.3648297)|Peijie Sun, Yifan Wang, Min Zhang, Chuhan Wu, Yan Fang, Hong Zhu, Yuan Fang, Meng Wang||With the surge in mobile gaming, accurately predicting user spending on newly downloaded games has become paramount for maximizing revenue. However, the inherently unpredictable nature of user behavior poses significant challenges in this endeavor. To address this, we propose a robust model training and evaluation framework aimed at standardizing spending data to mitigate label variance and extremes, ensuring stability in the modeling process. Within this framework, we introduce a collaborative-enhanced model designed to predict user game spending without relying on user IDs, thus ensuring user privacy and enabling seamless online training. Our model adopts a unique approach by separately representing user preferences and game features before merging them as input to the spending prediction module. Through rigorous experimentation, our approach demonstrates notable improvements over production models, achieving a remarkable 17.11% enhancement on offline data and an impressive 50.65% boost in an online A/B test. In summary, our contributions underscore the importance of stable model training frameworks and the efficacy of collaborative-enhanced models in predicting user spending behavior in mobile gaming.|随着手机游戏的迅猛发展，准确预测用户在新下载游戏上的支出对于实现收入最大化至关重要。然而，用户行为固有的不可预测性在这方面提出了重大的挑战。为了解决这个问题，我们提出了一个健壮的模型训练和评估框架，旨在标准化支出数据，以减少标签差异和极端情况，确保建模过程的稳定性。在这个框架中，我们引入了一个协作增强的模型，该模型旨在预测用户的游戏支出，而不依赖于用户 ID，从而确保用户的隐私并实现无缝在线培训。我们的模型采用了一种独特的方法，分别表示用户偏好和游戏特征，然后将它们合并为消费预测模块的输入。通过严格的实验，我们的方法显示了生产模型的显著改进，在离线数据上实现了17.11% 的显著增强，在线 A/B 测试中实现了50.65% 的显著增强。总之，我们的贡献强调了稳定模型训练框架的重要性，以及协作增强模型在预测手机游戏用户消费行为方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaborative-Enhanced+Prediction+of+Spending+on+Newly+Downloaded+Mobile+Games+under+Consumption+Uncertainty)|0|
|[HiFI: Hierarchical Fairness-aware Integrated Ranking with Constrained Reinforcement Learning](https://doi.org/10.1145/3589335.3648317)|Yifan Liu, Wei Xia, Weiwen Liu, Menghui Zhu, Weinan Zhang, Ruiming Tang, Yong Yu||Integrated ranking is a critical component in industrial recommendation platforms. It combines candidate lists from different upstream channels or sources and ranks them into an integrated list, which will be exposed to users. During this process, to take responsibility for channel providers, the integrated ranking system needs to consider the exposure fairness among channels, which directly affects the opportunities of different channels being displayed to users. Besides, personalization also requires the integrated ranking system to consider the user's diverse preference on different channels besides items. Existing methods are hard to address both problems effectively. In this paper, we propose a <u>Hi</u>erarchical <u>F</u>airness-aware <u>I</u>ntegrated ranking (HiFI) framework. It contains a channel recommender and an item recommender, and the fairness constraint is on channels with constrained RL. We also design a gated attention layer (GAL) to effectively capture users' multi-faceted preferences. We compare HiFI with various baselines on public and industrial datasets, and HiFI achieves the state-of-the-art performance on both utility and fairness metrics. We also conduct an online A/B test to further validate the effectiveness of HiFI.|综合排名是行业推荐平台的重要组成部分。它综合了来自不同上游渠道或来源的候选人名单，并将其排列成一个综合名单，向用户公布。在此过程中，为了对渠道提供商负责，综合排名系统需要考虑渠道之间的曝光公平性，这直接影响到不同渠道向用户显示的机会。此外，个性化还要求综合排名系统考虑用户在不同渠道上的不同偏好。现有的方法很难有效地解决这两个问题。本文提出了一个层次化的综合排名(HiFI)框架，该框架包括: “ u”、“ Hi”、“ F”、“空气感知”、“ I”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“。它包含一个通道推荐器和一个项目推荐器，公平性约束在 RL 受限的通道上。我们还设计了一个门限注意层(GAL)来有效地捕获用户的多方面的偏好。我们将 HiFI 与公共和工业数据集上的各种基线进行了比较，HiFI 在效用和公平性指标上都取得了最先进的性能。我们还进行了在线 A/B 测试，以进一步验证 HiFI 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HiFI:+Hierarchical+Fairness-aware+Integrated+Ranking+with+Constrained+Reinforcement+Learning)|0|
|[On Practical Diversified Recommendation with Controllable Category Diversity Framework](https://doi.org/10.1145/3589335.3648323)|Tao Zhang, Luwei Yang, Zhibo Xiao, Wen Jiang, Wei Ning||Recommender systems have made significant strides in various industries, primarily driven by extensive efforts to enhance recommendation accuracy. However, this pursuit of accuracy has inadvertently given rise to echo chamber/filter bubble effects. Especially in industry, it could impair user's experiences and prevent user from accessing a wider range of items. One of the solutions is to take diversity into account. However, most of existing works focus on user's explicit preferences, while rarely exploring user's non-interaction preferences. These neglected non-interaction preferences are especially important for broadening user's interests in alleviating echo chamber/filter bubble effects.Therefore, in this paper, we first define diversity as two distinct definitions, i.e., user-explicit diversity (U-diversity) and user-item non-interaction diversity (N-diversity) based on user historical behaviors. Then, we propose a succinct and effective method, named as Controllable Category Diversity Framework (CCDF) to achieve both high U-diversity and N-diversity simultaneously.Specifically, CCDF consists of two stages, User-Category Matching and Constrained Item Matching. The User-Category Matching utilizes the DeepU2C model and a combined loss to capture user's preferences in categories, and then selects the top-K categories with a controllable parameter K.These top-K categories will be used as trigger information in Constrained Item Matching. Offline experimental results show that our proposed DeepU2C outperforms state-of-the-art diversity-oriented methods, especially on N-diversity task. The whole framework is validated in a real-world production environment by conducting online A/B testing.|推荐系统在各个行业都取得了长足的进步，主要是受到提高推荐准确性的广泛努力的推动。然而，这种对准确性的追求无意中导致了回声室/过滤器气泡效应。特别是在工业领域，它会损害用户的体验，阻止用户访问更广泛的项目。解决办法之一是考虑到多样性。然而，现有的大多数作品只关注用户的显性偏好，很少探讨用户的非交互偏好。这些被忽视的非交互偏好对于扩大用户的兴趣以减轻回声室/过滤器气泡效应特别重要。因此，本文首先将多样性定义为基于用户历史行为的用户显性多样性(U 多样性)和用户项非交互多样性(N 多样性)。然后，我们提出了一种简洁有效的方法，称为可控分类分集框架(CCDF) ，同时实现高 U 分集和 N 分集。具体来说，CCDF 包括两个阶段，用户类别匹配和约束条目匹配。用户类别匹配利用 DeepU2C 模型和综合损失来获取用户在类别中的偏好，然后选择带有可控参数 K 的 top-K 类别。这些 top-K 类别将作为受限项目匹配的触发信息。离线实验结果表明，本文提出的 DeepU2C 方法优于目前最先进的面向多样性的方法，尤其是在 N 分集任务上。通过在线 A/B 测试，在现实生产环境中验证了整个框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Practical+Diversified+Recommendation+with+Controllable+Category+Diversity+Framework)|0|
|[Optimization-Based Budget Pacing in eBay Sponsored Search](https://doi.org/10.1145/3589335.3648331)|Qinyi Chen, Phuong Ha Nguyen, Djordje Gligorijevic||In online platforms like eBay, sponsored search advertising has become instrumental for businesses aiming for enhanced visibility. However, in automated ad auctions, the sellers (ad campaigns) run the risk of exhausting their budgets prematurely in the absence of proper pacing strategies. In response to this, online platforms have been prompted to employ budget pacing strategies to maintain consistent spending patterns for their sellers. While numerous budget pacing strategies have been introduced, they predominantly stem from either empirical or theoretical perspectives, often functioning in isolation. This paper aims to bridge this gap by investigating the performance of a theoretically inspired optimization-based bid shading method, AdaptivePacing, within eBay's sponsored search environment and proposing variants of the algorithm tailored to real-world environments. Our findings highlight the benefits of applying theoretical pacing approaches in practical contexts. Specifically, the optimization-based AdaptivePacing method offers the platform flexible control over campaign spending patterns, accounts for business constraints, and suggests tailored strategies for distinct advertisers. Furthermore, when evaluating AdaptivePacing alongside established empirical methods, we demonstrate its practical effectiveness and pinpoint areas for further refinement.|在 eBay 这样的在线平台上，赞助商搜索广告已经成为企业提高知名度的重要手段。然而，在自动化广告拍卖中，如果没有适当的节奏策略，卖家(广告活动)就有过早耗尽预算的风险。为了应对这种情况，在线平台采用了预算调整策略，以保持卖家的一致消费模式。虽然已经引入了许多预算调整策略，但它们主要来自经验或理论视角，往往是孤立运作的。本文旨在通过调查 eBay 赞助商搜索环境中基于理论优化的投标着色方法 AdaptivePacing 的性能，并提出适合于现实环境的算法变体，来弥补这一差距。我们的研究结果强调了在实际环境中应用理论起搏方法的好处。具体来说，基于优化的 AdaptivePacing 方法为平台提供了对广告支出模式的灵活控制，解释了业务约束，并为不同的广告商提供了量身定制的策略。此外，当评估适应性起搏与已建立的经验方法，我们证明了其实际有效性和精确的领域进一步完善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimization-Based+Budget+Pacing+in+eBay+Sponsored+Search)|0|
|[AutoML for Large Capacity Modeling of Meta's Ranking Systems](https://doi.org/10.1145/3589335.3648336)|Hang Yin, KuangHung Liu, Mengying Sun, Yuxin Chen, Buyun Zhang, Jiang Liu, Vivek Sehgal, Rudresh Rajnikant Panchal, Eugen Hotaj, Xi Liu, Daifeng Guo, Jamey Zhang, Zhou Wang, Shali Jiang, Huayu Li, Zhengxing Chen, WenYen Chen, Jiyan Yang, Wei Wen||Web-scale ranking systems at Meta serving billions of users is complex. Improving ranking models is essential but engineering heavy. Automated Machine Learning (AutoML) can release engineers from labor intensive work of tuning ranking models; however, it is unknown if AutoML is efficient enough to meet tight production timeline in real-world and, at the same time, bring additional improvements to the strong baselines. Moreover, to achieve higher ranking performance, there is an ever-increasing demand to scale up ranking models to even larger capacity, which imposes more challenges on the efficiency. The large scale of models and tight production schedule requires AutoML to outperform human baselines by only using a small number of model evaluation trials (around 100). We presents a sampling-based AutoML method, focusing on neural architecture search and hyperparameter optimization, addressing these challenges in Meta-scale production when building large capacity models. Our approach efficiently handles large-scale data demands. It leverages a lightweight predictor-based searcher and reinforcement learning to explore vast search spaces, significantly reducing the number of model evaluations. Through experiments in large capacity modeling for CTR and CVR applications, we show that our method achieves outstanding Return on Investment (ROI) versus human tuned baselines, with up to 0.09% Normalized Entropy (NE) loss reduction or $25\%$ Query per Second (QPS) increase by only sampling one hundred models on average from a curated search space. The proposed AutoML method has already made real-world impact where a discovered Instagram CTR model with up to -0.36% NE gain (over existing production baseline) was selected for large-scale online A/B test and show statistically significant gain. These production results proved AutoML efficacy and accelerated its adoption in ranking systems at Meta.|Meta 为数十亿用户服务的网络级别排名系统非常复杂。改进排名模型至关重要，但工程量很大。自动机器学习(Automated Machine Learning，AutoML)可以让工程师们摆脱劳动密集型的排序模型调整工作; 然而，目前还不清楚 AutoML 是否足够有效，能够满足现实世界中紧张的生产时间表，同时还能给强大的基线带来额外的改进。此外，为了获得更高的排序性能，将排序模型扩展到更大容量的需求也在不断增加，这对效率提出了更多的挑战。大规模的模型和紧凑的生产进度要求 AutoML 仅仅使用少量的模型评估试验(大约100次)就能超越人类基线。提出了一种基于抽样的 AutoML 方法，重点研究了神经网络结构搜索和超参数优化，解决了元规模生产在建立大容量模型时遇到的问题。我们的方法有效地处理大规模的数据需求。它利用一个轻量级的基于预测器的搜索器和强化学习来探索广阔的搜索空间，大大减少了模型评估的数量。通过在 CTR 和 CVR 应用中的大容量建模实验，我们表明我们的方法实现了出色的投资回报率(ROI)相对于人工调整的基线，高达0.09% 的归一化熵(NE)损失减少或 $25% $Query per Second (QPS)增加通过平均抽样一百个模型从一个精心策划的搜索空间。提出的 AutoML 方法已经产生了现实世界的影响，其中发现的 Instagram CTR 模型具有高达 -0.36% 的 NE 增益(超过现有的生产基线) ，被选择用于大规模在线 A/B 测试，并显示出统计学显着的增益。这些生产结果证明了 AutoML 的有效性，并加速了其在 Meta 排名系统中的应用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoML+for+Large+Capacity+Modeling+of+Meta's+Ranking+Systems)|0|
|[OneSparse: A Unified System for Multi-index Vector Search](https://doi.org/10.1145/3589335.3648338)|Yaoqi Chen, Ruicheng Zheng, Qi Chen, Shuotao Xu, Qianxi Zhang, Xue Wu, Weihao Han, Hua Yuan, Mingqin Li, Yujing Wang, Jason Li, Fan Yang, Hao Sun, Weiwei Deng, Feng Sun, Qi Zhang, Mao Yang||Multi-index vector search has become the cornerstone for many applications, such as recommendation systems. Efficient search in such a multi-modal hybrid vector space is challenging since no single index design performs well for all kinds of vector data. Existing approaches to processing multi-index hybrid queries either suffer from algorithmic limitations or processing inefficiency. In this paper, we propose OneSparse, a unified multi-vector index query system that incorporates multiple posting-based vector indices, which enables highly efficient retrieval of multi-modal data-sets. OneSparse introduces a novel multi-index query engine design of inter-index intersection push-down. It also optimizes the vector posting format to expedite multi-index queries. Our experiments show OneSparse achieves more than 6x search performance improvement while maintaining comparable accuracy. OneSparse has already been integrated into Microsoft online web search and advertising systems with 5x+ latency gain for Bing web search and 2.0% Revenue Per Mille (RPM) gain for Bing sponsored search.|多索引向量搜索已经成为许多应用程序(如推荐系统)的基石。在这样一个多模态混合向量空间中的有效搜索是具有挑战性的，因为没有单一索引设计能够很好地处理所有类型的向量数据。现有的处理多索引混合查询的方法要么受到算法限制，要么处理效率低下。本文提出了一个统一的多向量索引查询系统 OneSparse，该系统集成了多个基于发布的向量索引，能够高效地检索多模态数据集。OneSparse 提出了一种新颖的索引交集下推的多索引查询引擎设计。它还优化了向量发布格式，以加快多索引查询。我们的实验表明，OneSparse 在保持可比准确性的同时，实现了超过6倍的搜索性能改进。OneSparse 已经被整合到微软在线网络搜索和广告系统中，Bing 的网络搜索延迟增加了5倍以上，Bing 赞助的搜索每公里收入(RPM)增加了2.0% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OneSparse:+A+Unified+System+for+Multi-index+Vector+Search)|0|
|[LLaCE: Locally Linear Contrastive Embedding](https://doi.org/10.1145/3589335.3651534)|Ruichen Liu, Yang Liu, Jiming Liu||Node embedding is one of the most widely adopted techniques in numerous graph analysis tasks, such as node classification. Methods for node embedding can be broadly classified into three categories: proximity matrix factorization approaches, sampling methods, and deep learning strategies. Among the deep learning strategies, graph contrastive learning has attracted significant interest. Yet, it has been observed that existing graph contrastive learning approaches do not adequately preserve the local topological structure of the original graphs, particularly when neighboring nodes belong to disparate categories. To address this challenge, this paper introduces a novel node embedding approach named Locally Linear Contrastive Embedding (LLaCE). LLaCE is designed to maintain the intrinsic geometric structure of graph data by utilizing locally linear formulation, thereby ensuring that the local topological characteristics are accurately reflected in the embedding space. Experimental results on one synthetic dataset and five real-world datasets validate the effectiveness of our proposed method.|节点嵌入是许多图分析任务(如节点分类)中广泛采用的技术之一。节点嵌入的方法大致可分为三类: 接近矩阵分解方法、抽样方法和深度学习策略。在深度学习策略中，图形对比学习引起了人们的极大兴趣。然而，已有的图对比学习方法并不能充分保留原始图的局部拓扑结构，尤其是当相邻节点属于不同类别时。为了解决这一问题，本文提出了一种新的节点嵌入方法——局部线性对比嵌入(LLaCE)。LLaCE 通过利用局部线性公式来保持图形数据的内在几何结构，从而保证局部拓扑特征在嵌入空间中得到准确的反映。在一个合成数据集和五个实际数据集上的实验结果验证了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLaCE:+Locally+Linear+Contrastive+Embedding)|0|
|[GraphSAGE-based POI Recommendation via Continuous-Time Modeling](https://doi.org/10.1145/3589335.3651515)|Yuwen Liu, Lianyong Qi, Weiming Liu, Xiaolong Xu, Xuyun Zhang, Wanchun Dou||With the proliferation of Location-based Social Networks (LBSNs), user check-in data at Points-of-Interest (POIs) has surged, reshaping user-environment interaction. However, POI recommendation remains a challenging task for two primary reasons. First, external incentives often drive users' check-ins, potentially misrepresenting their genuine preferences. Second, while many current research model the temporal dynamics of user preferences in a discrete space, they ignore capturing the continuous evolution of these preferences. To address these challenges, we propose the GraphSAGE-based POI Recommendation via Continuous-Time Modeling (GSA-CTM). We first utilize GraphSAGE to identify real user preferences and filter out noise beyond the user's real preferences. After GraphSAGE captures complex interaction, we use Gated Recurrent Unit (GRU) combined with neural Ordinary Differential Equations (ODEs) to capture the temporal information embedded in the interaction, and then use neural ODEs to model the user's continuous dynamic preferences into continuous space. Experiments on two widely-used public datasets validate the superiority of our method.|随着基于位置的社交网络(LBSNs)的普及，用户在兴趣点(POI)的签到数据激增，重塑了用户与环境的交互。然而，POI 推荐仍然是一个具有挑战性的任务，主要有两个原因。首先，外部激励往往驱动用户的签到，可能会歪曲他们真正的偏好。其次，当前的许多研究在离散空间中模拟用户偏好的时间动态，他们忽略了捕捉这些偏好的持续演化。为了应对这些挑战，我们提出了基于 GraphSAGE 的连续时间建模 POI 建议(GSA-CTM)。我们首先利用 GraphSAGE 识别真实的用户偏好，并过滤掉超出用户真实偏好的噪音。在 GraphSAGE 捕获复杂交互后，利用门限回归单元(GRU)结合神经常微分方程(ODEs)捕获交互中的时间信息，然后利用神经常微分方程将用户的连续动态偏好建模为连续空间。在两个广泛使用的公共数据集上的实验验证了该方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphSAGE-based+POI+Recommendation+via+Continuous-Time+Modeling)|0|
|[Boost Social Recommendation via Adaptive Denoising Network](https://doi.org/10.1145/3589335.3651473)|Xinran Chen, Chaobo He, Quanlong Guan||Social recommendation aims to integrate social relationships to improve the performance of recommendation, and has attracted increasing attention in the field of recommendation system. Recently, Graph Neural Networks (GNNs) based methods for social recommendation are very competitive, but most of them overlook the fact that social relationships may have potential noises. Through the message passing mechanism of GNNs, these noises could be propagated and amplified, ultimately reducing the performance of recommendation. In view of this, we propose a novel GNN-based Adaptive Denoising Social Recommendation (ADSRec) method. It devises a denoising network, which can alleviate the impact of social relationships noises via the adaptive weight adjustment strategy. By further introducing the contrastive learning, the representations of users and items can be enhanced, leading to better recommendation results. Extensive experiments on three widely used datasets demonstrate the superiority of ADSRec over baselines.|社会推荐旨在整合社会关系，提高推荐绩效，在推荐系统领域受到越来越多的关注。近年来，基于图神经网络(GNN)的社会推荐方法竞争激烈，但大多忽视了社会关系可能存在潜在噪声的事实。通过 GNN 的消息传递机制，这些噪声可以被传播和放大，最终降低推荐的性能。鉴于此，我们提出了一种新的基于 GNN 的自适应去噪社会推荐(ADSRec)方法。通过自适应权重调整策略，设计了一个去噪网络，可以减轻社会关系噪声的影响。通过进一步引入对比学习，可以增强用户和项目的表示，从而获得更好的推荐结果。在三个广泛使用的数据集上的大量实验证明了 ADSRec 相对于基线的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Boost+Social+Recommendation+via+Adaptive+Denoising+Network)|0|
|[3D Face Reconstruction Using A Spectral-Based Graph Convolution Encoder](https://doi.org/10.1145/3589335.3651460)|Haoxin Xu, Zezheng Zhao, Yuxin Cao, Chunyu Chen, Hao Ge, Ziyao Liu||Monocular 3D face reconstruction plays a crucial role in avatar generation, with significant demand in web-related applications such as generating virtual financial advisors in FinTech. Current reconstruction methods predominantly rely on deep learning techniques and employ 2D self-supervision as a means to guide model learning. However, these methods encounter challenges in capturing the comprehensive 3D structural information of the face due to the utilization of 2D images for model training purposes. To overcome this limitation and enhance the reconstruction of 3D structural features, we propose an innovative approach that integrates existing 2D features with 3D features to guide the model learning process. Specifically, we introduce the 3D-ID Loss, which leverages the high-dimensional structure features extracted from a Spectral-Based Graph Convolution Encoder applied to the facial mesh. This approach surpasses the sole reliance on the 3D information provided by the facial mesh vertices coordinates. Our model is trained using 2D-3D data pairs from a combination of datasets and achieves state-of-the-art performance on the NoW benchmark.|单目3D 人脸重建在虚拟化身生成中起着至关重要的作用，在金融科技虚拟财务顾问生成等网络相关应用方面有着巨大的需求。目前的重建方法主要依赖于深度学习技术，采用二维自我监督作为指导模型学习的手段。然而，由于二维图像用于模型训练目的，这些方法在获取人脸的全面三维结构信息方面遇到了挑战。为了克服这一局限性，并加强三维结构特征的重建，我们提出了一种创新的方法，集成现有的二维特征与三维特征，以指导模型学习过程。具体来说，我们介绍了3D-ID 丢失，它利用了从基于光谱的图卷积编码器提取的高维结构特征应用到面部网格。这种方法超越了对面部网格顶点坐标提供的三维信息的唯一依赖。我们的模型是训练使用的2D-3D 数据对从一个组合的数据集，并实现了国家的最先进的性能的 NoW 基准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=3D+Face+Reconstruction+Using+A+Spectral-Based+Graph+Convolution+Encoder)|0|
|[General2Specialized LLMs Translation for E-commerce](https://doi.org/10.1145/3589335.3651510)|Kaidi Chen, Ben Chen, Dehong Gao, Huangyu Dai, Wen Jiang, Wei Ning, Shanqing Yu, Libin Yang, Xiaoyan Cai||Existing Neural Machine Translation (NMT) models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents. Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current NMT methods. To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain. Furthermore, we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce. The paradigm can be used for the NMT models based on Large language models (LLMs). Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and robustness of our G2ST approach, as compared with state-of-the-art NMT models such as LLaMA, Qwen, GPT-3.5, and even GPT-4.|现有的神经机器翻译(NMT)模型主要处理一般领域的翻译，而忽略了电子商务、法律文件等具有特殊写作公式的领域。以电子商务为例，文本通常包含大量领域相关词汇，存在较多的语法问题，导致现有的 NMT 方法性能较差。为了解决这些问题，我们收集了两个与领域相关的资源，包括一组术语对(对齐的汉英双语术语)和一个为电子商务领域注释的平行语料库。在此基础上，本文提出了一个具有自对比语义增强的两步微调范式(G2ST) ，将一个通用的 NMT 模型转化为电子商务的专用 NMT 模型。该范例可用于基于大型语言模型(LLM)的 NMT 模型。对实际电子商务标题的广泛评估表明，与 LLaMA，Qwen，GPT-3.5甚至 GPT-4等最先进的 NMT 模型相比，我们的 G2ST 方法具有优越的翻译质量和稳健性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=General2Specialized+LLMs+Translation+for+E-commerce)|0|
|[Counterfactual Explanations for Visual Recommender Systems](https://doi.org/10.1145/3589335.3651484)|Neham Jain, Vibhhu Sharma, Gaurav Sinha||Users rely on clever recommendations for items they might like to buy, and service providers rely on clever recommender systems to ensure that their product is recommended to their target audience. Providing explanations for recommendations helps to increase transparency and the users' overall trust in the system, besides helping practitioners debug their recommendation model. Modern recommendation systems utilize multi-modal data such as reviews and images to provide recommendation. In this work, we propose CAVIAR (Counterfactual explanations for VIsual Recommender systems), a novel method to explain recommender systems that utilize visual features of items. Our explanation is counterfactual and is optimized to be simultaneously simple and effective. Given an item in the user's top-K recommended list, CAVIAR makes a minimal, yet meaningful, perturbation to the item's image-embedding such that it is no longer a part of the list. In this way, CAVIAR aims to find the visual features of the item that were the most relevant for the recommendation. In order to lend meaning to the perturbations, we leverage CLIP model to connect the perturbed image features to textual features. We frame the explanation as a natural language counterfactual by contrasting the observed visual features in the item before and after the perturbation.|用户依靠聪明的推荐系统来选择他们想要购买的商品，而服务提供商则依靠聪明的推荐系统来确保他们的产品被推荐给他们的目标受众。除了帮助从业人员调试他们的推荐模型之外，为推荐提供解释有助于提高透明度和用户对系统的总体信任。现代推荐系统利用评论和图像等多模态数据来提供推荐。在这项工作中，我们提出了 CAVIAR (反事实解释的可视化推荐系统) ，一种新的方法来解释推荐系统，利用项目的视觉特征。我们的解释是反事实的，并优化为同时简单和有效。给定用户的 top-K 推荐列表中的一个项目，CAVIAR 对该项目的图像嵌入进行了最小但有意义的干扰，使其不再是列表的一部分。通过这种方式，CAVIAR 旨在找到与推荐最相关的项目的视觉特征。为了给扰动赋予意义，我们利用 CLIP 模型将受扰动的图像特征与文本特征连接起来。我们把解释作为一种反事实的自然语言，通过对比项目中观察到的视觉特征在扰动之前和之后。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Explanations+for+Visual+Recommender+Systems)|0|
|[Ad Laundering: How Websites Deceive Advertisers into Rendering Ads Next to Illicit Content](https://doi.org/10.1145/3589335.3651466)|Emmanouil Papadogiannakis, Panagiotis Papadopoulos, Evangelos P. Markatos, Nicolas Kourtellis||Providing online content monetized via ads to users is a lucrative business. But what if the content is pirated or illicit, thus harming the brand safety of the advertiser? In this paper, we are the first to investigate Ad Laundering: a technique with which bad actors deceive advertisers by hiding illicit content within evidently lawful websites to monetize the generated traffic. We develop a client-side detection methodology to detect and analyze websites performing ad laundering. We describe in detail the techniques these websites use to cloak content, and provide estimations for the ad revenues they are able to collect on a monthly basis. Finally, we attribute the generated revenue to different traffic channels and establish that even popular brands have their ads rendered next to undesirable content.|通过广告向用户提供在线内容是一项利润丰厚的业务。但如果内容是盗版或非法的，从而损害了广告商的品牌安全呢？在本文中，我们是第一个调查广告洗钱: 一种技术与不良行为者欺骗广告客户隐藏非法内容在明显合法的网站，以货币化所产生的流量。我们开发一个客户端检测方法来检测和分析网站进行广告洗钱。我们详细描述了这些网站用于隐藏内容的技术，并提供了他们每月能够收集的广告收入的估计。最后，我们将产生的收入归因于不同的流量渠道，并确定即使是流行品牌的广告也会出现在不受欢迎的内容旁边。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ad+Laundering:+How+Websites+Deceive+Advertisers+into+Rendering+Ads+Next+to+Illicit+Content)|0|
|[Is the 'Impression Log' Beneficial to Evaluating News Recommender Systems? No, it is Not!](https://doi.org/10.1145/3589335.3651527)|Jeewon Ahn, HongKyun Bae, SangWook Kim|Hanyang University, Seoul, Republic of Korea|This paper aims to answer the question of whether to use the impression log in evaluating news recommendation models. We start with a claim that the testing with the impression log composed of only hard-negative news (i.e., impression (IMP)-based test) is not beneficial to evaluating the models precisely. Based on the claim, we discuss a way of evaluating models by employing all kinds of negative news articles (i.e., Total test). Also, we propose a more-efficient way of evaluating models by sampling only a small number of negative articles (i.e., random-sampling (RS)-based test). We verify our claim by extensively comparing the evaluation results on six models from the IMP-based, Total, and RS-based tests: the RS-based test shows more accurate results than the IMP-based test in determining the superiority among the models while providing higher efficiency than the Total test. Therefore, our answer to the question above would be "do not employ the impression log in testing models even if it is available." This result is quite meaningful since it enables news recommendation researchers and practitioners, who have been using the impression log thus going to the wrong way, to turn to the right one.|本文旨在回答是否使用印象日志评价新闻推荐模型的问题。我们首先声称，印象日志的测试只有硬负面新闻(即，印象(IMP)为基础的测试)是不利于精确评价模型。在此基础上，我们讨论了一种利用各种负面新闻文章(即全面检验)来评价模型的方法。此外，我们提出了一个更有效的方法来评价模型，只抽样少量的负面文章(即，随机抽样(RS)为基础的测试)。我们通过广泛比较 IMP 测试、 Total 测试和 RS 测试的6个模型的评估结果来验证我们的说法: RS 测试在确定模型优劣方面比 IMP 测试显示出更准确的结果，同时提供了比 Total 测试更高的效率。因此，我们对上述问题的回答是“即使印象日志可用，也不要在测试模型中使用它。”这个结果是非常有意义的，因为它使新闻推荐的研究人员和从业人员，谁一直使用的印象日志，从而走向错误的方式，转向正确的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+the+'Impression+Log'+Beneficial+to+Evaluating+News+Recommender+Systems?+No,+it+is+Not!)|0|
|[How Good are LLMs in Generating Personalized Advertisements?](https://doi.org/10.1145/3589335.3651520)|Elyas Meguellati, Lei Han, Abraham Bernstein, Shazia W. Sadiq, Gianluca Demartini||In this paper, we explore the potential of large language models (LLMs) in generating personalized online advertisements (ads) tailored to specific personality traits, focusing on openness and neuroticism. We conducted a user study involving two tasks to understand the performance of LLM-generated ads compared to human-written ads in different online environments. Task 1 simulates a social media environment where users encounter ads while scrolling through their feed. Task 2 mimics a shopping website environment where users are presented with multiple sponsored products side-by-side. Our results indicate that LLM-generated ads targeting the openness trait positively impact user engagement and preferences, with performance comparable to human-written ads. Furthermore, in both scenarios, the overall effectiveness of LLM-generated ads was found to be similar to that of human-written ads, highlighting the potential of LLM-generated personalised content to rival traditional advertising methods with the added advantage of scalability. This study underscores the need for cautious consideration in the deployment of LLM-generated content at scale. While our findings confirm the scalability and potential effectiveness of LLM-generated content, there is an equally pressing concern about the ease with which it can be misused.|本文以开放性和神经质为核心，探讨了大语言模型(LLM)在生成针对特定人格特征的个性化网络广告中的潜力。我们进行了一项涉及两个任务的用户研究，以了解在不同的在线环境下，LLM 生成的广告相对于人写广告的表现。Task 1模拟了一个社交媒体环境，在这个环境中，用户在浏览 feed 时会看到广告。Task 2模拟了一个购物网站环境，在这个环境中，用户可以并排看到多种赞助商产品。我们的研究结果表明，LLM 生成的针对开放性特征的广告正向影响用户参与度和偏好，其性能与人写广告相当。此外，在这两种情况下，LLM 生成的广告的整体效果被发现类似于人写广告，突出了 LLM 生成的个性化内容与传统广告方法竞争的潜力，并具有额外的可扩展性优势。这项研究强调了在大规模部署 LLM 生成的内容时需要谨慎考虑的必要性。虽然我们的研究结果证实了 LLM 生成的内容的可伸缩性和潜在的有效性，但是同样紧迫的问题是它容易被滥用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Good+are+LLMs+in+Generating+Personalized+Advertisements?)|0|
|[Compact Interpretable Tensor Graph Multi-Modal News Embeddings](https://doi.org/10.1145/3589335.3651480)|Dawon Ahn, William Shiao, Arindam Khaled, Andrew Bauer, Stefanos Poulis, Evangelos E. Papalexakis||Online news articles encompass a variety of modalities such as text and images. How can we learn a representation that incorporates information from all those modalities in a compact and interpretable manner? In this paper, we propose CITEM (Compact Interpretable Tensor graph multi-modal news EMbedding), a tensor-based framework for compact and interpretable multi-modal news representations. CITEM generates a tensor graph consisting of a news similarity graph for each modality and employs a tensor decomposition to produce compact and interpretable embeddings, each dimension of which is a heterogeneous co-cluster of news articles and corresponding modalities. We extensively validate CITEM compared to baselines on two news classification tasks: misinformation news detection and news categorization. The experimental results show that CITEM performs within the same range of AUC as state-of-the-art baselines while producing 7x to 10.5x more compact embeddings. In addition, each embedding dimension of CITEM is interpretable, representing a latent co-cluster of articles.|在线新闻文章包括各种形式，如文字和图像。我们如何才能学会一种以简洁和可解释的方式将所有这些模式的信息纳入其中的表述方式？本文提出了紧凑可解释张量图多模态新闻嵌入(CITEM) ，这是一种基于张量的紧凑可解释多模态新闻表示框架。CITEM 为每种情态生成一个由新闻相似度图组成的张量图，并使用张量分解产生紧凑的、可解释的嵌入，其中每个维度是一个异质的新闻文章和相应模态的共簇。在错误信息检测和新闻分类这两个新闻分类任务中，我们将 CITEM 与基线进行了广泛的验证。实验结果表明，CITEM 在与最先进的基线相同的 AUC 范围内执行，同时产生7倍至10.5倍更紧凑的嵌入。此外，CITEM 的每个嵌入维度都是可解释的，表示一个潜在的文章共簇。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Compact+Interpretable+Tensor+Graph+Multi-Modal+News+Embeddings)|0|
|[Proactive Recommendation with Iterative Preference Guidance](https://doi.org/10.1145/3589335.3651548)|Shuxian Bi, Wenjie Wang, Hang Pan, Fuli Feng, Xiangnan He||Recommender systems mainly tailor personalized recommendations according to user interests learned from user feedback. However, such recommender systems passively cater to user interests and even reinforce existing interests in the feedback loop, leading to problems like filter bubbles and opinion polarization. To counteract this, proactive recommendation actively steers users towards developing new interests in a target item or topic by strategically modulating recommendation sequences. Existing work for proactive recommendation faces significant hurdles: 1) overlooking the user feedback in the guidance process; 2) lacking explicit modeling of the guiding objective; and 3) insufficient flexibility for integration into existing industrial recommender systems. To address these issues, we introduce an Iterative Preference Guidance (IPG) framework. IPG performs proactive recommendation in a flexible post-processing manner by ranking items according to their IPG scores that consider both interaction probability and guiding value. These scores are explicitly estimated with iteratively updated user representation that considers the most recent user interactions. Extensive experiments validate that IPG can effectively guide user interests toward target interests with a reasonable trade-off in recommender accuracy. The code is available at https://github.com/GabyUSTC/IPG-Rec.|推荐系统主要根据用户反馈中学到的用户兴趣量身定制个性化推荐。然而，这样的推荐系统被动地迎合了用户的兴趣，甚至加强了反馈回路中已有的兴趣，导致了过滤泡沫和观点两极分化等问题。为了解决这个问题，主动推荐通过策略性地调整推荐顺序，积极地引导用户在目标项目或主题中发展新的兴趣。主动推荐的现有工作面临着重大障碍: 1)忽视指导过程中的用户反馈; 2)缺乏指导目标的明确建模; 3)集成到现有行业推荐系统的灵活性不足。为了解决这些问题，我们引入了一个迭代偏好指导(IPG)框架。IPG 采用灵活的后处理方式，根据 IPG 分数对项目进行排序，同时考虑交互概率和指导价值，从而实现主动推荐。这些分数是通过考虑最近的用户交互的迭代更新的用户表示来显式估计的。大量的实验验证了 IPG 能够有效地引导用户兴趣向目标兴趣转移，并在推荐准确性方面做出了合理的权衡。密码可在 https://github.com/gabyustc/ipg-rec 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Proactive+Recommendation+with+Iterative+Preference+Guidance)|0|
|[FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs](https://doi.org/10.1145/3589335.3651504)|EunCheol Choi, Emilio Ferrara||Our society is facing rampant misinformation harming public health and trust. To address the societal challenge, we introduce FACT-GPT, a system leveraging Large Language Models (LLMs) to automate the claim matching stage of fact-checking. FACT-GPT, trained on a synthetic dataset, identifies social media content that aligns with, contradicts, or is irrelevant to previously debunked claims. Our evaluation shows that our specialized LLMs can match the accuracy of larger models in identifying related claims, closely mirroring human judgment. This research provides an automated solution for efficient claim matching, demonstrates the potential of LLMs in supporting fact-checkers, and offers valuable resources for further research in the field.|我们的社会正面临猖獗的错误信息危害公众健康和信任。为了应对社会挑战，我们引入了 FACT-GPT，这是一个利用大型语言模型(LLM)来自动化索赔匹配事实检查阶段的系统。FACT-GPT 在一个合成数据集上进行训练，识别与之前被揭穿的主张相一致、相互矛盾或无关的社交媒体内容。我们的评估表明，我们的专业 LLM 可以匹配的准确性较大的模型在识别相关的索赔，密切反映人类的判断。该研究为索赔匹配提供了一个自动化的解决方案，展示了 LLM 在支持事实核查方面的潜力，并为该领域的进一步研究提供了有价值的资源。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FACT-GPT:+Fact-Checking+Augmentation+via+Claim+Matching+with+LLMs)|0|
|[Benchmarking News Recommendation in the Era of Green AI](https://doi.org/10.1145/3589335.3651472)|Qijiong Liu, Jieming Zhu, Quanyu Dai, XiaoMing Wu||Over recent years, news recommender systems have gained significant attention in both academia and industry, emphasizing the need for a standardized benchmark to evaluate and compare the performance of these systems. Concurrently, Green AI advocates for reducing the energy consumption and environmental impact of machine learning. To address these concerns, we introduce the first Green AI benchmarking framework for news recommendation, known as GreenRec, and propose a metric for assessing the tradeoff between recommendation accuracy and efficiency. Our benchmark encompasses 30 base models and their variants, covering traditional end-to-end training paradigms as well as our proposed efficient only-encode-once (OLEO) paradigm. Through experiments consuming 2000 GPU hours, we observe that the OLEO paradigm achieves competitive accuracy compared to state-of-the-art end-to-end paradigms and delivers up to a 2992% improvement in sustainability metrics.|近年来，新闻推荐系统引起了学术界和工业界的重视，强调需要一个标准化的基准来评估和比较这些系统的性能。同时，绿色人工智能倡导降低机器学习的能源消耗和环境影响。为了解决这些问题，我们引入了第一个新闻推荐的绿色 AI 基准框架，称为 GreenRec，并提出了一个评估推荐准确性和效率之间权衡的度量标准。我们的基准包括30个基本模型及其变体，涵盖了传统的端到端训练范例以及我们提出的有效的只编码一次(OLEO)范例。通过消耗2000 GPU 小时的实验，我们观察到 OLEO 范例与最先进的端到端范例相比达到了具有竞争力的准确性，并在可持续性指标方面提供了高达2992% 的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Benchmarking+News+Recommendation+in+the+Era+of+Green+AI)|0|
|[Event GDR: Event-Centric Generative Document Retrieval](https://doi.org/10.1145/3589335.3651500)|Yong Guan, Dingxiao Liu, Jinchen Ma, Hao Peng, Xiaozhi Wang, Lei Hou, Ru Li||Generative document retrieval, an emerging paradigm in information retrieval, learns to build connections between documents and identifiers within a single model, garnering significant attention. However, there are still two challenges: (1) neglecting inner-content correlation during document representation; (2) lacking explicit semantic structure during identifier construction. Nonetheless, events have enriched relations and well-defined taxonomy, which could facilitate addressing the above two challenges. Inspired by this, we propose Event GDR, an event-centric generative document retrieval model, integrating event knowledge into this task. Specifically, we utilize an exchange-then-reflection method based on multi-agents for event knowledge extraction. For document representation, we employ events and relations to model the document to guarantee the comprehensiveness and inner-content correlation. For identifier construction, we map the events to well-defined event taxonomy to construct the identifiers with explicit semantic structure. Our method achieves significant improvement over the baselines on two datasets, and also hopes to provide insights for future research.|生成文献检索是一种新兴的信息检索范式，它学会在单一模型中建立文档和标识符之间的联系，引起了人们的极大关注。但是，目前仍然存在两个问题: (1)忽视文档表示中的内容相关性; (2)标识符构造中缺乏明确的语义结构。尽管如此，事件丰富了关系和明确的分类，这可能有助于解决上述两个挑战。受此启发，我们提出以事件为中心的生成文献检索模型 Event gDR，将事件知识整合到这项任务中。具体地说，我们利用了一种基于多智能体的交换-反射方法来提取事件知识。对于文档表示，我们使用事件和关系对文档进行建模，以保证文档的全面性和内容的相关性。对于标识符构造，我们将事件映射到定义良好的事件分类法，以构造具有显式语义结构的标识符。我们的方法在两个数据集的基线上取得了显著的改进，同时也希望能够为以后的研究提供一些启示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Event+GDR:+Event-Centric+Generative+Document+Retrieval)|0|
|[DIAERESIS: Knowledge Graph Partitioning for Efficient Query Answering](https://doi.org/10.1145/3589335.3651231)|Georgia Troullinou, Kostas Stefanidis, Dimitris Plexousakis, Haridimos Kondylakis||The rapid explosion of linked data demands effective and efficient storage, management, and querying methods. Apache Spark is one of the most widely used engines for big data processing, with more and more systems adopting it for efficient query answering. Existing approaches, exploiting Spark for querying RDF data, adopt partitioning techniques for reducing the data that need to be accessed in order to improve efficiency. However, simplistic methods for data partitioning fail to minimize data access at query answering and effectively improve query efficiency. In this demonstration, we present DIAERESIS, a novel platform that exploits a summary-based partitioning strategy achieving a significant improvement in minimizing data access and as such improving query-answering efficiency. DIAERESIS first identifies the top-k most important schema nodes and distributes the other schema nodes to the centroid they mostly depend on. Then, it allocates the corresponding instance nodes to the schema nodes they are instantiated under, creating vertical sub-partitions and indexes. We allow conference participants to actively identify the impact of our partitioning methodology on data distribution and replication, data accessed for query answering, and query answering efficiency. Further, we contrast our approach with existing partitioning approaches adopted by state-of-the-art systems in the domain, providing a deep understanding of the challenges in the area.|链接数据的快速增长需要高效的存储、管理和查询方法。Apache Spark 是应用最广泛的大数据处理引擎之一，越来越多的系统采用它来进行高效的查询应答。利用 Spark 查询 RDF 数据的现有方法采用了分区技术，以减少需要访问的数据，从而提高效率。然而，过于简单化的数据分区方法并不能使查询应答时的数据访问最小化，从而有效地提高查询效率。在这个演示中，我们介绍了 DIAERESIS，这是一个利用基于摘要的分区策略的新平台，它在最小化数据访问方面实现了显著的改进，从而提高了查询应答的效率。DIAERESIS 首先标识 top-k 最重要的模式节点，并将其他模式节点分配到它们主要依赖的中心。然后，它将相应的实例节点分配给它们被实例化的模式节点，从而创建垂直的子分区和索引。我们允许与会者主动确定我们的分区方法对数据分布和复制、查询应答所访问的数据以及查询应答效率的影响。此外，我们将我们的方法与该领域最先进的系统采用的现有分区方法进行对比，从而深入了解该领域的挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DIAERESIS:+Knowledge+Graph+Partitioning+for+Efficient+Query+Answering)|0|
|[Box2Go: Collaborative Interactive Infobox Filling](https://doi.org/10.1145/3589335.3651235)|Benjamin Hättasch, Carsten Binnig||Infoboxes can be useful to quickly learn about the contents of text collections, but manually creating them is error-prone and time-consuming, and existing automatic approaches require training data or resources like ontologies that are not available for every domain. Moreover, they lack techniques for adaptation to the user. We therefore propose a system to automatically fill user-defined attributes of infoboxes with the human-in-the-loop which provides this adaptation, and works without training data and domain-specific resources. Our approach generalizes simple user feedback to explore a joint embedding space and find the correct values for the attributes. These structured representations of the texts can be used for collaborative exploration of text collections on the web. We provide a prototypic implementation for such a collaborative web application and demonstrate its usage.|Infobox 对于快速了解文本集合的内容非常有用，但是手动创建它们是容易出错和耗时的，而且现有的自动化方法需要培训数据或资源，比如本体，这些并不适用于每个领域。此外，他们缺乏适应用户的技术。因此，我们提出了一个系统来自动填充用户定义的属性信息箱与人在循环提供这种适应，并没有训练数据和领域特定的资源。我们的方法通过简单的用户反馈来探索一个联合嵌入空间，并找到属性的正确值。这些文本的结构化表示可用于协作探索网络上的文本集合。我们为这样一个协作 Web 应用程序提供了一个原型实现，并演示了它的用法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Box2Go:+Collaborative+Interactive+Infobox+Filling)|0|
|[HINCare: An Intelligent Helper Recommender System for Elderly Care](https://doi.org/10.1145/3589335.3651236)|Carrie Wang, Wentao Ning, Xiaoman Wu, Reynold Cheng||In Hong Kong, the number of elderly citizens will reach one-third of the population within the next decade. To mitigate this problem, timebanking has received attention in recent years. In timebanking, an NGO helper earns time credits through providing voluntary services (e.g., household duties) to elders. These time credits can be used to acquire other services. Although timebanking has shown the promise of promoting mutual care in many countries, its potential has not been fully utilized, due to the lack of IT and data support. We thus develop HINCare, a software platform that supports timebanking for multiple NGOs. Besides providing convenience to NGO supervisors, helpers, and elders, HINCare makes use of a heterogeneous information network (HIN) for recommending suitable helpers to elders. This is the first time a graph-based recommender system is used for such purposes. Currently, HINCare is used by 12 NGOs to serve more than 5000 users in Hong Kong. In this demonstration, participants can play the role of helpers and elders in the HINCare environment.|在香港，长者人数将在未来十年内达到人口的三分之一。为了缓解这个问题，时间银行近年来受到了关注。在时间银行方面，非政府机构的助手透过为长者提供义务服务(例如家务) ，赚取时间信贷。这些时间积分可以用来获得其他服务。虽然时间银行在许多国家显示了促进相互照顾的希望，但由于缺乏信息技术和数据支持，其潜力尚未得到充分利用。因此，我们开发了 HINCare，一个支持多个非政府组织时间银行的软件平台。除了为非政府机构的主管、协助者和长者提供方便外，HINCare 亦利用多元化的资讯网络，向长者推荐合适的协助者。这是基于图表的推荐系统首次用于这种目的。目前，有12个非政府机构使用 HINCare 服务，为香港超过5000名使用者提供服务。在这个示范中，参与者可以在 HINCare 环境中扮演帮助者和长者的角色。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HINCare:+An+Intelligent+Helper+Recommender+System+for+Elderly+Care)|0|
|[Linked Open Literature Review using the Neuro-symbolic Open Research Knowledge Graph](https://doi.org/10.1145/3589335.3651238)|Azanzi Jiomekong, Sören Auer, Allard Oelen||The way scholarly knowledge and in particular literature reviews are communicated today rather resembles static, unstructured, pseudo-digitized articles, which are hardly processable by machines and AI. This demo showcases a novel way to create and publish scholarly literature reviews, also called semantic reviews. The neuro-symbolic approach consists of extracting key insights from scientific papers leveraging neural models and organizing them using a symbolic scholarly knowledge graph. The food information engineering review case study will allow participants to see how this approach is implemented using the Open Research Knowledge Graph (ORKG). The real-time demo will allow participants to play with the ORKG and create their own living, semantic review.|今天，学术知识，特别是文献综述的传播方式非常类似于静态的、非结构化的、伪数字化的文章，这些文章很难被机器和人工智能处理。这个演示展示了一种创建和发布学术文献评论的新方法，也称为语义评论。神经-符号方法包括利用神经模型从科学论文中提取关键见解，并使用符号学术知识图表对其进行组织。食品信息工程回顾案例研究将使参与者了解如何使用开放研究知识图(ORKG)实施这种方法。实时演示将允许参与者与 ORKG 一起玩，并创建他们自己的生活，语义审查。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Linked+Open+Literature+Review+using+the+Neuro-symbolic+Open+Research+Knowledge+Graph)|0|
|[RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems](https://doi.org/10.1145/3589335.3651242)|Jianxun Lian, Yuxuan Lei, Xu Huang, Jing Yao, Wei Xu, Xing Xie||This paper introduces RecAI, a practical toolkit designed to augment or even revolutionize recommender systems with the advanced capabilities of Large Language Models (LLMs). RecAI provides a suite of tools, including Recommender AI Agent, Recommendation-oriented Language Models, Knowledge Plugin, RecExplainer, and Evaluator, to facilitate the integration of LLMs into recommender systems from multifaceted perspectives. The new generation of recommender systems, empowered by LLMs, are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric recommendation experiences. We hope the open-source of RecAI can help accelerate evolution of new advanced recommender systems. The source code of RecAI is available at <https://github.com/microsoft/RecAI>.|本文介绍了 RecAI，这是一个实用的工具包，旨在用大型语言模型(LLM)的先进功能来增强甚至革新推荐系统。RecAI 提供了一套工具，包括推荐人工智能代理、面向推荐的语言模型、知识插件、重新解释器和评估器，以促进从多方面的角度将 LLM 集成到推荐系统中。由 LLM 授权的新一代推荐系统预计将更加通用、可解释、可对话和可控，为更加智能和以用户为中心的推荐体验铺平道路。我们希望 RecAI 的开源能够帮助加速新的高级推荐系统的发展。RecAI 的源代码可于 <  https://github.com/microsoft/RecAI 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RecAI:+Leveraging+Large+Language+Models+for+Next-Generation+Recommender+Systems)|0|
|[Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform](https://doi.org/10.1145/3589335.3651243)|Mingyue Cheng, Hao Zhang, Jiqian Yang, Qi Liu, Li Li, Xin Huang, Liwei Song, Zhi Li, Zhenya Huang, Enhong Chen||Large language model evaluation plays a pivotal role in the enhancement of its capacity. Previously, numerous methods for evaluating large language models have been proposed in this area. Despite their effectiveness, these existing works mainly focus on assessing objective questions, overlooking the capability to evaluate subjective questions which is extremely common for large language models. Additionally, these methods predominantly utilize centralized datasets for evaluation, with question banks concentrated within the evaluation platforms themselves. Moreover, the evaluation processes employed by these platforms often overlook personalized factors, neglecting to consider the individual characteristics of both the evaluators and the models being evaluated. To address these limitations, we propose a novel anonymous crowd-sourcing evaluation platform, BingJian, for large language models that employs a competitive scoring mechanism where users participate in ranking models based on their performance. This platform stands out not only for its support of centralized evaluations to assess the general capabilities of models but also for offering an open evaluation gateway. Through this gateway, users have the opportunity to submit their questions, testing the models on a personalized and potentially broader range of capabilities. Furthermore, our platform introduces personalized evaluation scenarios, leveraging various forms of human-computer interaction to assess large language models in a manner that accounts for individual user preferences and contexts. The demonstration of BingJian can be accessed at https://github.com/Mingyue-Cheng/Bingjian.|大型语言模型评价在提高语言能力方面起着举足轻重的作用。在此之前，许多评估大型语言模型的方法已经被提出在这个领域。尽管这些工作很有效，但现有的工作主要集中在评价客观问题上，忽视了评价主观问题的能力，而这种能力在大型语言模型中极为常见。此外，这些方法主要利用集中的数据集进行评估，问题库集中在评估平台本身。此外，这些平台所采用的评价过程往往忽视个性化因素，忽视考虑评价者和被评价模型的个性特征。为了解决这些局限性，我们提出了一个新的匿名众包评估平台，兵鉴，为大型语言模型，采用竞争性评分机制，其中用户参与排名模型的基础上，他们的表现。该平台不仅支持集中评价，以评估模型的一般能力，而且还提供了一个开放的评价网关。通过这个网关，用户有机会提交他们的问题，测试模型的个性化和潜在的更广泛的能力范围。此外，我们的平台引入了个性化的评估场景，利用各种形式的人机交互来评估大型语言模型，以考虑到个人用户偏好和上下文的方式。市民可透过 https://github.com/mingyue-cheng/BingJian 浏览「冰箭」的演示资料。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Personalized+Evaluation+of+Large+Language+Models+with+An+Anonymous+Crowd-Sourcing+Platform)|0|
|[PKG API: A Tool for Personal Knowledge Graph Management](https://doi.org/10.1145/3589335.3651247)|Nolwenn Bernard, Ivica Kostric, Weronika Lajewska, Krisztian Balog, Petra Galuscáková, Vinay Setty, Martin G. Skjæveland||Personal knowledge graphs (PKGs) offer individuals a way to store and consolidate their fragmented personal data in a central place, improving service personalization while maintaining full user control. Despite their potential, practical PKG implementations with user-friendly interfaces remain scarce. This work addresses this gap by proposing a complete solution to represent, manage, and interface with PKGs. Our approach includes (1) a user-facing PKG Client, enabling end-users to administer their personal data easily via natural language statements, and (2) a service-oriented PKG API. To tackle the complexity of representing these statements within a PKG, we present an RDF-based PKG vocabulary that supports this, along with properties for access rights and provenance.|个人知识图(PKG)为个人提供了一种在中心位置存储和整合支离破碎的个人数据的方法，改善了服务的个性化，同时保持了完全的用户控制。尽管 PKG 具有潜力，但实际的具有用户友好界面的 PKG 实现仍然很少。这项工作通过提出一个完整的解决方案来表示、管理和与 PKG 的接口来弥补这一差距。我们的方法包括(1)一个面向用户的 PKG 客户端，使最终用户能够通过自然语言语句轻松地管理他们的个人数据，和(2)一个面向服务的 PKG API。为了解决在 PKG 中表示这些语句的复杂性，我们提供了一个基于 RDF 的 PKG 词汇表来支持这一点，以及访问权限和出处的属性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PKG+API:+A+Tool+for+Personal+Knowledge+Graph+Management)|0|
|[Brinjal: A Web-Plugin for Collaborative Hate Speech Detection](https://doi.org/10.1145/3589335.3651250)|Ming Shan Hee, Karandeep Singh, Charlotte Ng Si Min, Kenny Tsu Wei Choo, Roy KaWei Lee||The proliferation of hate speech (HS) has compromised the safety and trustworthiness of the internet, exacerbating social divides by promoting hatred and discrimination. Although recent studies have produced guidelines and developed advanced technologies for the automated detection of HS, their efficacy and adaptability in real-world applications remain unclear. Furthermore, existing guidelines on what constitutes HS might not reflect the perspectives and beliefs of individuals and communities. This paper introduces Brinjal, a multifaceted web plugin designed for the collaborative detection of HS. Brinjal enables individuals to identify instances of HS and engage in discussions to verify such content, thereby enhancing the collective understanding of HS. Additionally, Brinjal serves as a practical platform for deploying and evaluating advanced HS detection models, facilitating user interaction and performance assessment. Lastly, Brinjal includes an analytical tool for analyzing HS, offering insights based on the crowdsourced instances and discussions about HS across various websites. The video demonstration of Brinjal can be viewed here: https://youtu.be/\_JxziIVWBO4. Disclaimer: This paper contains violent and discriminatory content that may be disturbing to some readers.|仇恨言论(HS)的泛滥损害了互联网的安全性和可信度，通过宣扬仇恨和歧视加剧了社会分歧。尽管最近的研究为 HS 的自动检测制定了指导方针并开发了先进技术，但其在现实应用中的功效和适应性仍不清楚。此外，关于何为协调制度的现行准则可能不反映个人和社区的观点和信仰。本文介绍了一个用于协同检测 HS 的多方面 Web 插件 Brinjal。Brinjal 使个人能够确定协调制度的实例，并参与讨论以核实这些内容，从而加强对协调制度的集体理解。此外，Brinjal 还是部署和评估先进 HS 检测模型的实用平台，方便用户交互和性能评估。最后，Brinjal 包括一个分析 HS 的分析工具，提供基于众包实例的见解，以及跨不同网站的关于 HS 的讨论。Brinjal 的视频展示可以在这里看到:  https://youtu.be/_jxziivwbo4。免责声明: 本文含有暴力和歧视性内容，可能会令一些读者感到不安。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Brinjal:+A+Web-Plugin+for+Collaborative+Hate+Speech+Detection)|0|
|[SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores](https://doi.org/10.1145/3589335.3651251)|Vidminas Vizgirda, Rui Zhao, Naman Goel||We present SocialGenPod, a decentralised and privacy-friendly way of deploying generative AI Web applications. Unlike centralised Web and data architectures that keep user data tied to application and service providers, we show how one can use Solid – a decentralised Web specification – to decouple user data from generative AI applications. We demonstrate SocialGenPod using a prototype that allows users to converse with different Large Language Models, optionally leveraging Retrieval Augmented Generation to generate answers grounded in private documents stored in any Solid Pod that the user is allowed to access, directly or indirectly. SocialGenPod makes use of Solid access control mechanisms to give users full control of determining who has access to data stored in their Pods. SocialGenPod keeps all user data (chat history, app configuration, personal documents, etc) securely in the user's personal Pod; separate from specific model or application providers. Besides better privacy controls, this approach also enables portability across different services and applications. Finally, we discuss challenges, posed by the large compute requirements of state-of-the-art models, that future research in this area should address. Our prototype is open-source and available at: https://github.com/Vidminas/socialgenpod/.|我们展示了 SocialGenPod，一种分散的和隐私友好的方式来部署生成性 AI Web 应用程序。与将用户数据与应用程序和服务提供商绑定在一起的集中式 Web 和数据架构不同，我们展示了如何使用 Solid ——一种分散式 Web 规范——将用户数据与生成式 AI 应用程序分离开来。我们使用一个允许用户与不同的大型语言模型进行对话的原型来演示 SocialGenPod，可以选择性地利用获取增强生成技术来生成基于私有文档的答案，这些私有文档存储在 Solid Pod 中，用户可以直接或间接地访问这些文档。SocialGenPod 利用 Solid 访问控制机制，让用户完全控制谁可以访问存储在 Pods 中的数据。SocialGenPod 将所有用户数据(聊天记录、应用程序配置、个人文档等)安全地保存在用户的个人 Pod 中; 与特定的模型或应用程序提供商分开。除了更好的隐私控制，这种方法还支持跨不同服务和应用程序的可移植性。最后，我们讨论了这一领域未来研究应该解决的挑战，这些挑战是由最先进模型的巨大计算需求造成的。我们的原型是开源的，可以在以下 https://github.com/vidminas/socialgenpod/获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SocialGenPod:+Privacy-Friendly+Generative+AI+Social+Web+Applications+with+Decentralised+Personal+Data+Stores)|0|
|[Ducho 2.0: Towards a More Up-to-Date Unified Framework for the Extraction of Multimodal Features in Recommendation](https://doi.org/10.1145/3589335.3651440)|Matteo Attimonelli, Danilo Danese, Daniele Malitesta, Claudio Pomo, Giuseppe Gassi, Tommaso Di Noia||In this work, we introduce Ducho 2.0, the latest stable version of our framework. Differently from Ducho, Ducho 2.0 offers a more personalized user experience with the definition and import of custom extraction models fine-tuned on specific tasks and datasets. Moreover, the new version is capable of extracting and processing features through multimodal-by-design large models. Notably, all these new features are supported by optimized data loading and storing to the local memory. To showcase the capabilities of Ducho 2.0, we demonstrate a complete multimodal recommendation pipeline, from the extraction/processing to the final recommendation. The idea is to provide practitioners and experienced scholars with a ready-to-use tool that, put on top of any multimodal recommendation framework, may permit them to run extensive benchmarking analyses. All materials are accessible at: <https://github.com/sisinflab/Ducho>.|在本文中，我们将介绍 Ducho 2.0，它是我们框架的最新稳定版本。与 Ducho 不同，Ducho 2.0提供了更个性化的用户体验，定义和导入了针对特定任务和数据集进行微调的自定义提取模型。此外，新版本还能够通过设计的多模态大模型提取和处理特征。值得注意的是，所有这些新特性都得到了优化的数据加载和存储到本地内存的支持。为了展示 Ducho 2.0的功能，我们展示了一个完整的多模式推荐管道，从提取/处理到最终推荐。这个想法是为从业人员和有经验的学者提供一个现成的工具，放在任何多模式建议框架之上，可以允许他们进行广泛的基准分析。所有资料可浏览以下 https://github.com/sisinflab/ducho  :。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ducho+2.0:+Towards+a+More+Up-to-Date+Unified+Framework+for+the+Extraction+of+Multimodal+Features+in+Recommendation)|0|
|[SE-PQA: Personalized Community Question Answering](https://doi.org/10.1145/3589335.3651445)|Pranav Kasela, Marco Braga, Gabriella Pasi, Raffaele Perego||Personalization in Information Retrieval is a topic studied for a long time. Nevertheless, there is still a lack of high-quality, real-world datasets to conduct large-scale experiments and evaluate models for personalized search. This paper contributes to filling this gap by introducing SE-PQA (StackExchange - Personalized Question Answering), a new curated resource to design and evaluate personalized models related to the task of community Question Answering (cQA). The contributed dataset includes more than 1 million queries and 2 million answers, annotated with a rich set of features modeling the social interactions among the users of a popular cQA platform. We describe the characteristics of SE-PQA and detail the features associated with questions and answers. We also provide reproducible baseline methods for the cQA task based on the resource, including deep learning models and personalization approaches. The results of the preliminary experiments conducted show the appropriateness of SE-PQA to train effective cQA models; they also show that personalization remarkably improves the effectiveness of all the methods tested. Furthermore, we show the benefits in terms of robustness and generalization of combining data from multiple communities for personalization purposes.|信息检索的个性化是一个长期研究的课题。尽管如此，仍然缺乏高质量的、真实世界的数据集来进行大规模的实验和评估个性化检索模型。本文通过引入 SE-PQA 来填补这一空白。 SE-PQA 是一种新的策划资源，用于设计和评估与社区问答任务(cQA)相关的个性化模型。贡献的数据集包括超过100万个查询和200万个答案，并用一组丰富的特性对流行的 cQA 平台的用户之间的社交互动进行了建模。我们描述了 SE-PQA 的特征，并详细描述了与问答相关的特征。我们还为基于资源的 cQA 任务提供了可重复的基线方法，包括深度学习模型和个性化方法。初步实验结果表明，SE-PQA 方法训练有效的 cQA 模型是合适的，并且个性化显著提高了所有测试方法的有效性。此外，我们还展示了将来自多个社区的数据用于个性化目的的健壮性和通用性方面的好处。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SE-PQA:+Personalized+Community+Question+Answering)|0|
|[Digital Democracy at Crossroads: A Meta-Analysis of Web and AI Influence on Global Elections](https://doi.org/10.1145/3589335.3652003)|Zheng Wei, Xian Xu, Pan Hui||2024 will be the largest election year in history involving over 50 countries and approximately 4.2 billion people. Since 1996, the Web has been instrumental in political campaigns, enhancing public engagement and creating new communication avenues for elections. Nevertheless, the proliferation of generative AI technologies has made false information dissemination simpler and quicker, posing a substantial threat to election integrity and democratic processes. The 2024 global elections underscore the need to comprehend and tackle the impact of such technologies on democracy. In this paper, we undertake a detailed meta-analysis, scrutinizing 44 papers published in The Web Conference, detailing the influence of the Web on elections. Our research reveals key historical trends on how the Web has impacted elections: first, social media has revolutionized election strategies through direct voter-candidate interactions. Second, big data and algorithm-driven campaigns are commonplace. Third, AI advancements have exacerbated the spread of fake news, risking election fairness. Predominantly from studies published since 2018 among 44 papers, we underscore the necessity for advanced detection tools, policy formulation, and responsible AI use to maintain electoral integrity. This analysis offers an insight into the Web and AI's impact on elections, presenting pointers for addressing challenges and leveraging opportunities in the 2024 and future elections.|2024年将是历史上规模最大的选举年，有50多个国家参加，人口约42亿。自1996年以来，网络在政治运动、加强公众参与和为选举创造新的沟通渠道方面发挥了重要作用。然而，生成性人工智能技术的扩散使虚假信息的传播变得更加简单和迅速，对选举公正性和民主进程构成重大威胁。2024年的全球大选突显出，有必要理解和应对此类技术对民主政体的影响。在本文中，我们进行了详细的元分析，审查了44篇论文发表在网络会议，详细说明了网络对选举的影响。我们的研究揭示了网络如何影响选举的关键历史趋势: 首先，社交媒体通过选民与候选人之间的直接互动，彻底改变了选举策略。其次，大数据和算法驱动的活动很常见。第三，人工智能的进步加剧了假新闻的传播，使选举公平面临风险。从2018年以来发表的44篇论文中，我们主要强调了使用先进的检测工具、政策制定和负责任的人工智能来维护选举公正性的必要性。本文分析了网络和人工智能对选举的影响，提出了在2024年和未来选举中应对挑战和利用机会的建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Digital+Democracy+at+Crossroads:+A+Meta-Analysis+of+Web+and+AI+Influence+on+Global+Elections)|0|
|[BoxCare: A Box Embedding Model for Disease Representation and Diagnosis Prediction in Healthcare Data](https://doi.org/10.1145/3589335.3651448)|Hang Lv, Zehai Chen, Yacong Yang, Guofang Ma, Yanchao Tan, Carl Yang||Diagnosis prediction is becoming crucial to develop healthcare plans for patients based on Electronic Health Records (EHRs). Existing works usually enhance diagnosis prediction via learning accurate disease representation, where many of them try to capture inclusive relations based on the hierarchical structures of existing disease ontologies such as those provided by ICD-9 codes. However, they overlook exclusive relations that can reflect different and complementary perspectives of the ICD-9 structures, and thus fail to accurately represent relations among diseases and ICD-9 codes. To this end, we propose to project disease embeddings and ICD-9 code embeddings into boxes, where a box is an axis-aligned hyperrectangle with a geometric region and two boxes can clearly "include" or "exclude" each other. Upon box embeddings, we further obtain patient embeddings via aggregating the disease representations for diagnosis prediction. Extensive experiments on two real-world EHR datasets show significant performance gains brought by our proposed framework, yielding average improvements of 6.04% for diagnosis prediction over state-of-the-art competitors.|基于电子健康记录(EHRs)为患者制定医疗保健计划时，诊断预测变得至关重要。现有的工作通常通过学习准确的疾病表示来增强诊断预测，其中许多工作试图捕获基于现有疾病本体(如 ICD-9代码提供的那些)的层次结构的包容性关系。然而，他们忽视了可以反映 ICD-9结构的不同和互补视角的排他性关系，因此不能准确地表示疾病和 ICD-9编码之间的关系。为此，我们建议将疾病嵌入和 ICD-9代码嵌入投影到盒子中，其中盒子是具有几何区域的轴对齐的超矩形，并且两个盒子可以清楚地“包含”或“排除”彼此。在盒子嵌入后，我们通过聚合疾病表示来进一步获得病人嵌入以进行诊断预测。在两个真实世界的 EHR 数据集上进行的大量实验表明，我们提出的框架带来了显著的性能提升，与最先进的竞争对手相比，诊断预测平均提高了6.04% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BoxCare:+A+Box+Embedding+Model+for+Disease+Representation+and+Diagnosis+Prediction+in+Healthcare+Data)|0|
|[Enabling Pre-Shock State Detection using Electrogram Signals from Implantable Cardioverter-Defibrillators](https://doi.org/10.1145/3589335.3651450)|Runze Yan, Neal K. Bhatia, Faisal M. Merchant, Alex Fedorov, Ran Xiao, Cheng Ding, Xiao Hu||Identifying electrical signatures preceding a ventricular arrhythmia from the implantable cardioverter-defibrillators (ICDs) can help predict an upcoming ICD shock. To achieve this, we first deployed a large-scale study (N=326) to continuously monitor the electrogram (EGM) data from the ICDs and select the EGM segments prior to a shock event and under the normal condition. Next, we design a novel cohesive framework that integrates metric learning, prototype learning, and few-shot learning, enabling learning from an imbalanced dataset. We implement metric learning by leveraging a Siamese neural network architecture, which incorporates LSTM units. We innovatively utilize triplet and pair losses in a sequential manner throughout the training process on EGM samples. This approach generates embeddings that significantly enhance the distinction of EGM signals under different conditions. In the inference stage, k-means clustering identifies prototypes representing pre-shock and normal states from these embeddings. In summary, this framework leverages the predictive potential of signals before ICD shocks, addressing the gap in early cardiac arrhythmia detection. Our experimental results show a notable F1 score of 0.87, sensitivity of 0.97, and precision of 0.79. Our framework offers a significant advancement in cardiac care predictive analytics, promising enhanced ICD decision-making for improved patient outcomes.|从植入式心脏复律除颤器(ICD)中识别室性心律失常前的电特征可以帮助预测即将到来的 ICD 休克。为了实现这一点，我们首先部署了一项大规模的研究(N = 326) ，以连续监测来自 ICD 的电图(EGM)数据，并在休克事件之前和正常情况下选择 EGM 片段。接下来，我们设计了一个新的内聚框架，集成度量学习，原型学习和少镜头学习，使学习从一个不平衡的数据集。我们利用结合了 LSTM 单元的暹罗神经网络结构来实现度量学习。在 EGM 样本的训练过程中，我们创新性地以连续的方式利用了三元组和配对损失。这种方法产生的嵌入显着增强了区分 EGM 信号在不同的条件下。在推理阶段，K平均算法从这些嵌入中识别代表预休克和正常状态的原型。总之，这个框架利用了 ICD 冲击前信号的预测潜力，弥补了早期心律不整检测的差距。实验结果表明，F1得分为0.87，灵敏度为0.97，精密度为0.79。我们的框架在心脏护理预测分析方面取得了重大进展，有望增强 ICD 决策，改善患者预后。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enabling+Pre-Shock+State+Detection+using+Electrogram+Signals+from+Implantable+Cardioverter-Defibrillators)|0|
|[In The Beginning, Let There Be The Word: Challenges and Insights in Applying Sentiment Analysis to Social Research](https://doi.org/10.1145/3589335.3651264)|Andrzej Meler||Sentiment analysis based on lexical corpora is widely employed despite its inherent limitations in capturing nuances such as sarcasm and irony. This research delves into the application of sentiment analysis to political communication. To address the limitations of the Bag of Words methodology, a comparative study of sentiment analysis tools and emotion detection from speech is conducted, using automated speech recognition as a benchmark. Emotion recognition from speech has shown promising results, indicating its potential superiority over other methods [1], [2]. This study uses media material from Polish radio and television broadcasts, focusing on political interviews during a significant period marked by a high-profile assassination attempt. Results indicate challenges at the micro-level, but aggregated data reveals a significant correlation between valence measured from voice and text. While sentiment analysis may lack sensitivity in capturing mourning-related discourse, it proves effective in political communication devoid of such nuances. This suggests that valence in sentiment analysis reflects emotional content derived from intonation fairly accurately.|尽管基于词汇语料库的情感分析在捕捉讽刺和反讽等细微差别方面存在固有的局限性，但它仍然得到了广泛的应用。本研究探讨情绪分析在政治传播中的应用。针对“词袋”方法的局限性，以自动语音识别为基准，对情感分析工具和语音情感检测工具进行了比较研究。来自语音的情感识别已经显示出有希望的结果，表明其潜在的优势比其他方法[1] ，[2]。这项研究使用了来自波兰广播和电视广播的媒体材料，重点是在一个引人注目的企图谋杀的重要时期的政治采访。结果表明在微观层面上存在挑战，但汇总数据显示从语音和文本测量的价值之间存在显著的相关性。虽然情绪分析在捕捉与哀悼有关的话语时可能缺乏敏感性，但它在没有这种细微差别的政治交流中被证明是有效的。这表明情感分析中的配价能够相当准确地反映由语调产生的情感内容。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=In+The+Beginning,+Let+There+Be+The+Word:+Challenges+and+Insights+in+Applying+Sentiment+Analysis+to+Social+Research)|0|
|[Modeling Multidimensional Cognitive Search in Creativity with Generalized Additive Model](https://doi.org/10.1145/3589335.3651267)|Jia Lin Cheoh||Creativity is the ability to develop innovative functional ideas through unconventional associations. The consensus view on creativity in the literature involves divergence from stereotypical and habitual thought patterns [29, 39]. Creativity relies on search to explore diverse solutions. Search requires charting the mental terrain, leveraging past experiences and knowledge to manipulate and reconfigure components for new solutions [28]. The generally-accepted and overly-narrow view on creativity, however, neglects the fact that creativity is multidimensional [14]. This one-dimensional view of creativity triggers questions such as "Does one consider an unethical but novel creation to be creative?" and "Does one consider a new iPhone with mainstream functionalities but advanced camera features to be creative?" This research challenges the one-dimensional view of creativity, offering a more all-encompassing conceptualization of creativity [14]. The research examines the multidimensional nature of creativity by building a computational model of a designer's mutual search process across multiple mutually dependent search spaces. The research examines the trajectory of mutual search across multiple cognitive search spaces using a Generalized Additive Model (GAM). The field experiment employs 108 designers who develop their web designs through five iterations, utilizing computer graphics methods to extract the images. Through measuring the distance of search by considering changes in visual and source code in each iteration, the study argues that the search patterns differ in the degree of exploration in these search spaces over time. The research concludes that designers' search processes are non-linear and argues that there are more than one or two search spaces. The research also provides perceptual explanations of the multiple search processes in designs and argues for a more encompassing view of creativity.|创造力是通过非传统的联想来发展创新的功能性想法的能力。文献中关于创造力的一致观点涉及到从陈规定型和习惯性思维模式的分歧[29,39]。创造力依赖于探索不同的解决方案。搜索需要绘制心理地形图，利用过去的经验和知识来操纵和重新配置新的解决方案的组件[28]。然而，普遍接受和过于狭隘的创造力观点忽视了这样一个事实，即创造力是多维的[14]。这种关于创造力的一维观点引发了诸如“一个人是否认为一个不道德但新颖的创造是创造性的?”以及“是否有人认为一款拥有主流功能但高级摄像功能的新 iPhone 具有创造性?”这项研究挑战了创造力的一维观点，提供了一个更全面的创造力概念化[14]。该研究通过建立一个设计师在多个相互依赖的搜索空间中相互搜索过程的计算模型，来检验创造力的多维性。本研究使用广义加法模型(GAM)检验了跨多个认知搜索空间的相互搜索轨迹。现场实验雇佣了108名设计师，他们通过五次迭代开发他们的网页设计，利用计算机图形学方法提取图像。通过考虑每次迭代中视觉和源代码的变化来测量搜索距离，研究认为，随着时间的推移，这些搜索空间中的搜索模式在程度上是不同的。研究得出的结论是，设计师的搜索过程是非线性的，并认为有一个或两个以上的搜索空间。该研究还提供了对设计中的多重搜索过程的感性解释，并提出了一个更全面的创造力观点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+Multidimensional+Cognitive+Search+in+Creativity+with+Generalized+Additive+Model)|0|
|[Data Augmentation for Conversational AI](https://doi.org/10.1145/3589335.3641238)|Heydar Soudani, Roxana Petcu, Evangelos Kanoulas, Faegheh Hasibi|Radboud Univ Nijmegen, Nijmegen, Netherlands; Univ Amsterdam, Amsterdam, Netherlands|Advancements in conversational systems have revolutionized information access, surpassing the limitations of single queries. However, developing dialogue systems requires a large amount of training data, which is a challenge in low-resource domains and languages. Traditional data collection methods like crowd-sourcing are labor-intensive and time-consuming, making them ineffective in this context. Data augmentation (DA) is an affective approach to alleviate the data scarcity problem in conversational systems. This tutorial provides a comprehensive and up-to-date overview of DA approaches in the context of conversational systems. It highlights recent advances in conversation augmentation, open domain and task-oriented conversation generation, and different paradigms of evaluating these models. We also discuss current challenges and future directions in order to help researchers and practitioners to further advance the field in this area.|会话系统的进步彻底改变了信息访问，超越了单个查询的局限性。然而，开发对话系统需要大量的训练数据，这在资源不足的领域和语言中是一个挑战。传统的数据收集方法，如众包，是劳动密集型和耗时的，使他们在这种情况下无效。数据增强是解决会话系统中数据稀缺问题的有效途径。本教程提供了在会话系统上下文中 DA 方法的全面和最新概述。它强调了会话增强，开放领域和面向任务的会话生成的最新进展，以及评估这些模型的不同范式。我们还讨论了当前的挑战和未来的方向，以帮助研究人员和从业人员进一步推进该领域。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data+Augmentation+for+Conversational+AI)|0|
|[Recent Advances in Generative Information Retrieval](https://doi.org/10.1145/3589335.3641239)|Yubao Tang, Ruqing Zhang, Weiwei Sun, Jiafeng Guo, Maarten de Rijke||Generative retrieval (GR) has witnessed significant growth recently in the area of information retrieval. Compared to the traditional "index-retrieve-then-rank'' pipeline, the GR paradigm aims to consolidate all information within a corpus into a single model. Typically, a sequence-to-sequence model is trained to directly map a query to its relevant document identifiers (i.e., docids). This tutorial offers an introduction to the core concepts of the GR paradigm and a comprehensive overview of recent advances in its foundations and applications. We start by providing preliminary information covering foundational aspects and problem formulations of GR. Then, our focus shifts towards recent progress in docid design, training approaches, inference strategies, and applications of GR. We end by outlining challenges and issuing a call for future GR research.This tutorial is intended to be beneficial to both researchers and industry practitioners interested in developing novel GR solutions or applying them in real-world scenarios.|生成性检索(GR)近来在信息检索领域取得了显著的发展。与传统的“索引-检索-然后排名”流水线相比，GR 范式旨在将语料库中的所有信息合并到一个单一模型中。通常，序列到序列模型被训练为直接将查询映射到其相关文档标识符(即 docids)。本教程介绍了 GR 范式的核心概念，并全面概述了其基础和应用方面的最新进展。我们首先提供涵盖 GR 的基础方面和问题表述的初步信息。然后，我们的重点转移到最近的进展，医学设计，训练方法，推理策略，和应用的 GR。最后，我们概述了挑战，并呼吁未来的 GR 研究。本教程的目的是为有兴趣开发新颖 GR 解决方案或将其应用于实际场景的研究人员和行业从业人员提供有益的帮助。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recent+Advances+in+Generative+Information+Retrieval)|0|
|[Large Language Models for Recommendation: Progresses and Future Directions](https://doi.org/10.1145/3589335.3641247)|Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He|Univ Sci & Technol China, Hefei, Peoples R China; Natl Univ Singapore, Singapore, Singapore|The powerful large language models (LLMs) have played a pivotal role in advancing recommender systems. Recently, in both academia and industry, there has been a surge of interest in developing LLMs for recommendation, referred to as LLM4Rec. This includes endeavors like leveraging LLMs for generative item retrieval and ranking, as well as the exciting possibility of building universal LLMs for diverse open-ended recommendation tasks. These developments hold the potential to reshape the traditional recommender paradigm, paving the way for the next-generation recommender systems. In this tutorial, we aim to retrospect the evolution of LLM4Rec and conduct a comprehensive review of existing research. In particular, we will clarify how recommender systems benefit from LLMs through a variety of perspectives, including the model architecture, learning paradigm, and the strong abilities of LLMs such as chatting, generalization, planning, and generation. Furthermore, we will discuss the critical challenges and open problems in this emerging field, for instance, the trustworthiness, efficiency, and model retraining issues. Lastly, we will summarize the implications of previous work and outline future research directions. We believe that this tutorial will assist the audience in better understanding the progress and prospects of LLM4Rec, inspiring them for future exploration. This, in turn, will drive the prosperity of LLM4Rec, possibly fostering a paradigm shift in recommendation systems.|强大的大型语言模型(LLM)在推进推荐系统方面发挥了关键作用。最近，在学术界和工业界，对开发推荐 LLM 的兴趣激增，称为 LLM4Rec。这包括利用 LLM 进行生成性项目检索和排名，以及为各种开放式推荐任务构建通用 LLM 的令人兴奋的可能性。这些发展有可能重塑传统的推荐模式，为下一代推荐系统铺平道路。在本教程中，我们的目标是回顾 LLM4Rec 的演变，并对现有的研究进行一个全面的回顾。特别是，我们将通过多种视角阐明推荐系统如何从 LLM 中受益，包括模型体系结构、学习范式以及 LLM 的强大能力，如聊天、泛化、规划和生成。此外，我们将讨论在这个新兴领域的关键挑战和公开问题，例如，可信赖性，效率和模型再培训问题。最后，我们将总结以往工作的启示，并概述未来的研究方向。我们相信，本教程将帮助观众更好地了解 LLM4Rec 的进展和前景，鼓舞他们进行未来的探索。这反过来将推动 LLM4Rec 的繁荣，可能促进推荐系统的范式转变。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+for+Recommendation:+Progresses+and+Future+Directions)|0|
|[Multimodal Pretraining and Generation for Recommendation: A Tutorial](https://doi.org/10.1145/3589335.3641248)|Jieming Zhu, Xin Zhou, Chuhan Wu, Rui Zhang, Zhenhua Dong||Personalized recommendation stands as a ubiquitous channel for users to explore information or items aligned with their interests. Nevertheless, prevailing recommendation models predominantly rely on unique IDs and categorical features for user-item matching. While this ID-centric approach has witnessed considerable success, it falls short in comprehensively grasping the essence of raw item contents across diverse modalities, such as text, image, audio, and video. This underutilization of multimodal data poses a limitation to recommender systems, particularly in the realm of multimedia services like news, music, and short-video platforms. The recent surge in pretraining and generation techniques presents both opportunities and challenges in the development of multimodal recommender systems. This tutorial seeks to provide a thorough exploration of the latest advancements and future trajectories in multimodal pretraining and generation techniques within the realm of recommender systems. The tutorial comprises three parts: multimodal pretraining, multimodal generation, and industrial applications and open challenges in the field of recommendation. Our target audience encompasses scholars, practitioners, and other parties interested in this domain. By providing a succinct overview of the field, we aspire to facilitate a swift understanding of multimodal recommendation and foster meaningful discussions on the future development of this evolving landscape.|个性化推荐是用户探索符合自己兴趣的信息或项目的一个无处不在的渠道。然而，流行的推荐模型主要依赖于用户项匹配的唯一 ID 和分类特征。虽然这种以 ID 为中心的方法已经取得了相当大的成功，但是它不能全面地掌握原始项目内容在不同模式(如文本、图像、音频和视频)中的本质。这种对多模式数据的利用不足对推荐系统造成了限制，特别是在新闻、音乐和短视频平台等多媒体服务领域。最近在培训前和生成技术方面的激增在开发多式联运推荐系统方面既带来了机遇，也带来了挑战。本教程旨在全面探讨推荐系统领域内多模式预培训和生成技术的最新进展和未来发展轨迹。本教程包括三个部分: 多模式预训练、多模式生成、工业应用和推荐领域的公开挑战。我们的目标受众包括学者、实践者和对该领域感兴趣的其他方面。我们希望通过简要介绍这一领域的情况，促进对多式联运建议的迅速理解，并推动就这一不断变化的格局的未来发展进行有意义的讨论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Pretraining+and+Generation+for+Recommendation:+A+Tutorial)|0|
|[On-Device Recommender Systems: A Tutorial on The New-Generation Recommendation Paradigm](https://doi.org/10.1145/3589335.3641250)|Hongzhi Yin, Tong Chen, Liang Qu, Bin Cui||Given the sheer volume of contemporary e-commerce applications, recommender systems (RSs) have gained significant attention in both academia and industry. However, traditional cloud-based RSs face inevitable challenges, such as resource-intensive computation, reliance on network access, and privacy breaches. In response, a new paradigm called on-device recommender systems (ODRSs) has emerged recently in various industries like Taobao, Google, and Kuaishou. ODRSs unleash the computational capacity of user devices with lightweight recommendation models tailored for resource-constrained environments, enabling real-time inference with users' local data. This tutorial aims to systematically introduce methodologies of ODRSs, including (1) an overview of existing research on ODRSs; (2) a comprehensive taxonomy of ODRSs, where the core technical content to be covered span across three major ODRS research directions, including on-device deployment and inference, on-device training, and privacy/security of ODRSs; (3) limitations and future directions of ODRSs. This tutorial expects to lay the foundation and spark new insights for follow-up research and applications concerning this new recommendation paradigm.|鉴于当代电子商务应用的庞大数量，推荐系统(RS)已经引起了学术界和工业界的重视。然而，传统的基于云的 RSS 面临着不可避免的挑战，如资源密集型计算、对网络访问的依赖和隐私泄露。作为回应，最近在淘宝、谷歌和 Kuaishou 等不同行业出现了一种名为在设备上推荐系统(on-device )的新模式。ODRS 使用轻量级推荐模型释放用户设备的计算能力，这些模型专门针对资源受限的环境，支持对用户的本地数据进行实时推断。本教程旨在系统地介绍 ODRS 的方法，包括(1) ODRS 现有研究的概述; (2) ODRS 的综合分类，其中涵盖的核心技术内容跨越三个主要的 ODRS 研究方向，包括在设备上的部署和推理，在设备上的培训，以及 ODRS 的隐私/安全; (3) ODRS 的局限性和未来的方向。本教程期望为后续研究和有关这种新的推荐范例的应用程序奠定基础并激发新的见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On-Device+Recommender+Systems:+A+Tutorial+on+The+New-Generation+Recommendation+Paradigm)|0|
|[TransDrift: Modeling Word-Embedding Drift using Transformer](https://doi.org/10.1145/3589335.3651894)|Nishtha Madaan, Prateek Chaudhury, Nishant Kumar, Srikanta Bedathur||In modern NLP applications, word embeddings are a crucial backbone that can be readily shared across a number of tasks. However as the text distributions change and word semantics evolve over time, the downstream applications using the embeddings can suffer if the word representations do not conform to the data drift. Thus, maintaining word embeddings to be consistent with the underlying data distribution is a key problem. In this work, we tackle this problem and propose TransDrift, a transformer-based prediction model for word embeddings. Leveraging the flexibility of transformer, our model accurately learns the dynamics of the embedding drift and predicts the future embedding. In experiments, we compare with existing methods and show that our model makes significantly more accurate predictions of the word embedding than the baselines. Crucially, by applying the predicted embeddings as a backbone for downstream classification tasks, we show that our embeddings lead to superior performance compared to the previous methods.|在现代自然语言处理应用中，单词嵌入是一个关键的骨干，可以很容易地跨多个任务共享。然而，随着文本分布的变化和词语义的演变，如果词表示不符合数据漂移，使用嵌入的下游应用程序可能会受到影响。因此，维护单词嵌入与底层数据分布的一致性是一个关键问题。在这项工作中，我们解决了这个问题，并提出了 TransDrift，一个基于变压器的字嵌入预测模型。利用变压器的灵活性，我们的模型准确地学习嵌入漂移的动态，并预测未来的嵌入。在实验中，我们与现有的方法进行了比较，结果表明我们的模型对单词嵌入的预测明显比基线更准确。重要的是，通过将预测嵌入作为下游分类任务的骨干，我们表明我们的嵌入方法比以前的方法具有更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TransDrift:+Modeling+Word-Embedding+Drift+using+Transformer)|0|
|[Automated Claim Matching with Large Language Models: Empowering Fact-Checkers in the Fight Against Misinformation](https://doi.org/10.1145/3589335.3651910)|EunCheol Choi, Emilio Ferrara||In today's digital era, the rapid spread of misinformation poses threats to public well-being and societal trust. As online misinformation proliferates, manual verification by fact checkers becomes increasingly challenging. We introduce FACT-GPT (Fact-checking Augmentation with Claim matching Task-oriented Generative Pre-trained Transformer), a framework designed to automate the claim matching phase of fact-checking using Large Language Models (LLMs). This framework identifies new social media content that either supports or contradicts claims previously debunked by fact-checkers. Our approach employs GPT-4 to generate a labeled dataset consisting of simulated social media posts. This data set serves as a training ground for fine-tuning more specialized LLMs. We evaluated FACT-GPT on an extensive dataset of social media content related to public health. The results indicate that our fine-tuned LLMs rival the performance of larger pre-trained LLMs in claim matching tasks, aligning closely with human annotations. This study achieves three key milestones: it provides an automated framework for enhanced fact-checking; demonstrates the potential of LLMs to complement human expertise; offers public resources, including datasets and models, to further research and applications in the fact-checking domain.|在当今的数字时代，错误信息的迅速传播对公众福祉和社会信任构成了威胁。随着在线错误信息的激增，事实核查人员的手工验证变得越来越具有挑战性。本文介绍了 FACT-GPT (FACT-GPT) ，这是一个使用大型语言模型(LLM)实现索赔匹配阶段事实检查自动化的框架。该框架确定了新的社交媒体内容，这些内容要么支持要么与事实核查人员之前揭穿的说法相矛盾。我们的方法使用 GPT-4来生成一个由模拟的社交媒体帖子组成的标记数据集。这个数据集可以作为微调更专业化 LLM 的训练基地。我们评估了 FACT-GPT 的广泛数据集的社会媒体内容相关的公共卫生。结果表明，我们的微调 LLM 在索赔匹配任务中的性能与较大的预训练 LLM 相当，与人工注释非常接近。这项研究达到了三个关键的里程碑: 它提供了一个增强事实核查的自动化框架; 展示了 LLM 补充人类专业知识的潜力; 提供公共资源，包括数据集和模型，以进一步研究和应用在事实核查领域。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automated+Claim+Matching+with+Large+Language+Models:+Empowering+Fact-Checkers+in+the+Fight+Against+Misinformation)|0|
|[Decoding YouTube's Recommendation System: A Comparative Study of Metadata and GPT-4 Extracted Narratives](https://doi.org/10.1145/3589335.3651913)|Mayor Inna Gurung, Md Monoarul Islam Bhuiyan, Ahmed AlTaweel, Nitin Agarwal||YouTube's recommendation system is integral to shaping user experiences by suggesting content based on past interactions using collaborative filtering techniques. Nonetheless, concerns about potential biases and homogeneity in these recommendations are prevalent, with the danger of leading users into filter bubbles and echo chambers that reinforce their pre-existing beliefs. Researchers have sought to understand and address these biases in recommendation systems. However, traditionally, such research has relied primarily on metadata, such as video titles, which does not always encapsulate the full content or context of the videos. This reliance on metadata can overlook the nuances and substantive content of videos, potentially perpetuating the very biases and echo chambers that the research aims to unravel. This study advances the examination of sentiment, toxicity, and emotion within YouTube content by conducting a comparative analysis across various depths of titles and narratives extracted by leveraging GPT-4. Our analysis reveals a clear trend in sentiment, emotion, and toxicity levels as the depth of content analysis increases. Notably, there is a general shift from neutral to positive sentiments in both YouTube video titles and narratives. Emotion analysis indicates an increase in positive emotions, particularly joy, with a corresponding decrease in negative emotions such as anger and disgust in narratives, while video titles show a steady decrease in anger. Additionally, toxicity analysis presents a contrasting pattern, with video titles displaying an upward trend in toxicity, peaking at the greatest depth analyzed, whereas narratives exhibit a high initial toxicity level that sharply decreases and stabilizes at lower depths. These findings suggest that the depth of engagement with video content significantly influences emotional and sentiment expressions.|YouTube 的推荐系统是塑造用户体验不可或缺的一部分，它通过使用协同过滤技术，根据过去的互动提供内容建议。尽管如此，对这些建议中潜在的偏见和同质性的担忧普遍存在，有可能导致用户进入过滤器泡沫和回声室，从而加强他们先前存在的信念。研究人员试图理解和解决推荐系统中的这些偏见。然而，传统上，这类研究主要依赖于元数据，如视频标题，它并不总是封装视频的完整内容或上下文。这种对元数据的依赖可能会忽视视频的细微差别和实质性内容，有可能使研究旨在揭示的偏见和回声室永久化。这项研究通过利用 GPT-4对不同深度的标题和叙述进行比较分析，提高了对 YouTube 内容中的情绪、毒性和情感的检查。我们的分析显示，随着内容分析深度的增加，情绪、情绪和毒性水平有明显的趋势。值得注意的是，无论是 YouTube 视频标题还是叙事，都普遍从中性情绪转向积极情绪。情绪分析表明积极情绪的增加，特别是快乐，与相应的消极情绪，如愤怒和厌恶的减少叙述，而视频标题显示了一个稳定的减少愤怒。此外，毒性分析呈现出一种对比模式，视频标题显示毒性呈上升趋势，在分析的最深处达到峰值，而叙述性展示出高的初始毒性水平，在较低深度急剧下降并稳定下来。这些发现表明，视频内容的参与程度显著影响情绪和情绪的表达。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decoding+YouTube's+Recommendation+System:+A+Comparative+Study+of+Metadata+and+GPT-4+Extracted+Narratives)|0|
|[Graph Coarsening via Convolution Matching for Scalable Graph Neural Network Training](https://doi.org/10.1145/3589335.3651920)|Charles Dickens, Edward W. Huang, Aishwarya Reganti, Jiong Zhu, Karthik Subbian, Danai Koutra||Graph summarization as a preprocessing step is an effective and complementary technique for scalable graph neural network (GNN) training. In this work, we propose the Coarsening Via Convolution Matching (CONVMATCH) algorithm and a highly scalable variant, A-CONVMATCH, for creating summarized graphs that preserve the output of graph convolution. We evaluate CONVMATCH on six real-world link prediction and node classification graph datasets, and show it is efficient and preserves prediction performance while significantly reducing the graph size. Notably, CONVMATCH achieves up to 95 performance of GNNs on node classification while trained on graphs summarized down to 1 tasks, CONVMATCH consistently outperforms all baselines, achieving up to a 2x improvement.|图形摘要作为一种预处理步骤，是可扩展图神经网络(GNN)训练的有效补充技术。在这项工作中，我们提出了粗化通过卷积匹配(CONVMATCH)算法和一个高度可伸缩的变体，A-CONVMATCH，用于创建总结图保持图卷积的输出。对实际的六个链路预测和节点分类图数据集进行了 CONVMATCH 评估，结果表明该算法在保持预测性能的同时，显著减小了图的大小。值得注意的是，CONVMATCH 在节点分类上实现了高达95个 GNN 的性能，同时对归纳为1个任务的图进行训练，CONVMATCH 始终优于所有基线，实现了高达2倍的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Coarsening+via+Convolution+Matching+for+Scalable+Graph+Neural+Network+Training)|0|
|[FedHLT: Efficient Federated Low-Rank Adaption with Hierarchical Language Tree for Multilingual Modeling](https://doi.org/10.1145/3589335.3651933)|Zhihan Guo, Yifei Zhang, Zhuo Zhang, Zenglin Xu, Irwin King||Federated Multilingual Modeling (FMM) has become an essential approach in natural language processing (NLP) due to increasing linguistic diversity and the heightened emphasis on data privacy. However, FMM faces two primary challenges: 1) the high communication costs inherent in network operations, and 2) the complexities arising from parameter interference, as languages exhibit both unique characteristics and shared features. To tackle these issues, we introduce a communication-efficient framework for Multilingual Modeling (MM) that combines low-rank adaptation with a hierarchical language tree structure. Our method maintains the base model's weights while focusing on updating only the Low-rank adaptation (LoRA) parameters, significantly reducing communication costs. Additionally, we mitigate parameter conflicts by organizing languages based on their familial ties rather than merging all LoRA parameters together. Our experimental findings reveal that this novel model surpasses established baseline models in performance and markedly decreases communication overhead.|由于语言多样性的增加和对数据隐私的重视，联邦多语言建模(FMM)已经成为自然语言处理(NLP)中的一种重要方法。然而，FMM 面临着两个主要的挑战: 1)网络操作固有的高通信成本，2)参数干扰产生的复杂性，因为语言既表现出独特的特征又表现出共享的特征。为了解决这些问题，我们引入了一个多语言建模(MM)的通信高效框架，它结合了低等级适应性和层次化的语言树结构。该方法在保持基本模型权重的同时，只更新低秩自适应(LoRA)参数，大大降低了通信成本。此外，我们根据语言的亲缘关系来组织语言，而不是将所有 LoRA 参数合并在一起，从而减少了参数冲突。我们的实验结果表明，这种新模型在性能上超过了已建立的基线模型，并显著降低了通信开销。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedHLT:+Efficient+Federated+Low-Rank+Adaption+with+Hierarchical+Language+Tree+for+Multilingual+Modeling)|0|
|[Information Retrieval Meets Large Language Models](https://doi.org/10.1145/3589335.3641299)|Zheng Liu, Yujia Zhou, Yutao Zhu, Jianxun Lian, Chaozhuo Li, Zhicheng Dou, Defu Lian, JianYun Nie|Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, China; Renmin University of China, China; Wuhan University, China; Zhejiang University, China; South China University of Technology, China; Jilin University, China; University of Science and Technology of China, China; Huawei Technologies Ltd. Co, China; Shandong Artificial Intelligence Institute, China; Beijing University of Posts and Telecommunications, China; Institute of Computing Technology, Chinese Academy of Sciences, China; Tsinghua University, China; Shandong University, China|The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a central role of demanders and evaluators to the reliability of information services. Nevertheless, significant challenges exist, including computational costs, credibility concerns, domain-specific limitations, and ethical considerations. To thoroughly discuss the transformative impact of LLMs on IR research, the Chinese IR community conducted a strategic workshop in April 2023, yielding valuable insights. This paper provides a summary of the workshop’s outcomes, including the rethinking of IR’s core values, the mutual enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and open challenges.|信息检索搜索(IR)的研究领域已经发生了显著的变化，超越了传统的搜索，以满足不同的用户信息需求。最近，大语言模型(LLM)在文本理解、生成和知识推理方面展示了非凡的能力，为 IR 研究开辟了令人兴奋的途径。LLM 不仅促进了生成检索，而且为用户理解、模型评估和用户系统交互提供了改进的解决方案。更重要的是，IR 模型、 LLM 和人类之间的协同关系形成了一种新的技术范式，这种范式在信息搜索方面更加强大。IR 模型提供实时和相关的信息，LLM 提供内部知识，而人在信息服务的可靠性中扮演着需求者和评估者的中心角色。尽管如此，仍然存在重大的挑战，包括计算成本、可信度问题、特定领域的局限性和伦理考虑。为了深入讨论 LLM 对国际关系研究的变革性影响，中国国际关系界于2023年4月举办了一次战略研讨会，提出了宝贵的见解。本文提供了研讨会成果的总结，包括重新思考 IR 的核心价值，LLM 和 IR 的相互增强，一个新的 IR 技术范式的提议，以及开放的挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Information+Retrieval+Meets+Large+Language+Models)|0|
|[Heterogeneous Knowledge Grounding for Medical Question Answering with Retrieval Augmented Large Language Model](https://doi.org/10.1145/3589335.3651941)|Wenting Zhao, Zhongfen Deng, Shweta Yadav, Philip S. Yu||The Large Language Model (LLM) is renowned for its ability to encode a vast amount of general domain knowledge, enabling it to excel in question-answering, dialogue systems, and summarization tasks. However, the medical domain presents a unique challenge to LLM due to the distribution of medical knowledge, which follows a long-tail pattern. Existing approaches address this challenge by injecting medical knowledge into LLM through single sources such as medical textbooks or medical knowledge bases. However, medical knowledge is distributed across multiple heterogeneous information sources. A medical question-answering system can enhance answer coverage and confidence by considering these diverse knowledge sources together. To bridge this gap, we propose a novel approach called Heterogeneous Knowledge Retrieval-Augmented LLM for medical domain question answering. Our experiments, conducted on the MedQA-USMLE dataset, demonstrate promising performance improvements. These results underscore the importance of harnessing heterogeneous knowledge sources in the medical domain.|大型语言模型(LLM)以其编码大量通用领域知识的能力而闻名，使其能够在问答、对话系统和摘要任务方面表现出色。然而，由于医学知识的分布遵循长尾模式，医学领域对 LLM 提出了独特的挑战。现有方法通过单一来源，如医学教科书或医学知识库，将医学知识注入法医学。然而，医学知识是分布在多个异构信息源之间的。一个医学问答系统可以通过综合考虑这些不同的知识来源来提高答案的覆盖范围和信心。为了弥补这一差距，我们提出了一种新的医学领域问题回答方法——异构知识检索增强 LLM。我们在 MedQA-USMLE 数据集上进行的实验证明了有希望的性能改进。这些结果强调了在医学领域利用异质知识来源的重要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Heterogeneous+Knowledge+Grounding+for+Medical+Question+Answering+with+Retrieval+Augmented+Large+Language+Model)|0|
|[Weakly Supervised Video Moment Retrieval via Location-irrelevant Proposal Learning](https://doi.org/10.1145/3589335.3651942)|Wei Ji, Ruiqi Shi, Yinwei Wei, Shanshan Zhao, Roger Zimmermann||This paper deals with Video Moment Retrieval (VMR) in a weakly-supervised fashion, which aims to retrieve local video clips with only global video-level descriptions. Scrutinizing the recent advances in VMR, we find that the fully-supervised models achieve strong performance, but they are heavily relied on the precise temporal annotations. Weakly-supervised methods do not rely on temporal annotations, however, their performance is much weaker than the fully-supervised ones. To fill such gap, we propose to take advantage of a pretrained video-text model as hitchhiker to generate pseudo temporal labels. The pseudo temporal labels, together with the descriptive labels, are then utilized to guide the training of the proposed VMR model. The proposed Location-irrelevant Proposal Learning (LPL) model is based on a pretrained video-text model with cross-modal prompt learning, together with different strategies to generate reasonable proposals with various lengths. Despite the simplicity, we find that our method performs much better than the previous state-of-the-art methods on standard benchmarks, eg., +4.4% and +1.4% in mIoU on the Charades and ActivityNet-Caption datasets respectively, which benefits from training with fine-grained video-text pairs. Further experiments on two synthetic datasets with shuffled temporal location and longer video length demonstrate our model's robustness towards temporal localization bias as well as its strength in handling long video sequences.|本文采用弱监督方式处理视频矩检索(VMR) ，目的是检索只有全局视频级描述的局部视频片段。仔细研究 VMR 的最新进展，我们发现全监督模型取得了很好的性能，但是它们严重依赖于精确的时间注释。弱监督方法不依赖于时间注释，但是它们的性能要比完全监督方法弱得多。为了填补这一空白，我们提出利用预先训练的视频文本模型作为搭便车者来生成伪时态标签。然后利用伪时态标签和描述性标签对 VMR 模型进行训练。提出的位置不相关建议学习(LPL)模型是基于预先训练的视频文本模型和跨模式的提示学习，以及不同的策略来产生不同长度的合理建议。尽管简单，我们发现我们的方法在标准基准上比以前的最先进的方法表现得更好，例如，在 Charades 和 ActivityNet-Caption 数据集上分别增加了4.4% 和1.4% 的 mIoU，这得益于细粒度视频文本对的训练。通过对两个具有混合时间定位和较长视频长度的合成数据集的进一步实验，证明了该模型对时间定位偏差的鲁棒性以及对长视频序列的处理能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weakly+Supervised+Video+Moment+Retrieval+via+Location-irrelevant+Proposal+Learning)|0|
|[One-step Reach: LLM-based Keyword Generation for Sponsored Search Advertising](https://doi.org/10.1145/3589335.3651943)|Yang Wang, Zheyi Sha, Kunhai Lin, Chaobing Feng, Kunhong Zhu, Lipeng Wang, Xuewu Jiao, Fei Huang, Chao Ye, Dengwu He, Zhi Guo, Shuanglong Li, Lin Liu||Query keyword matching plays a crucial role in sponsored search advertising by retrieving semantically related keywords of the user query to target relevant advertisements. Conventional technical solutions adopt the retrieve-judge-then-rank retrieval framework structured in cascade funnels. However, it has limitations in accurately depicting the semantic relevance between the query and keyword, and the cumulative funnel losses result in unsatisfactory precision and recall. To address the above issues, this paper proposes a Large Language Model (LLM)-based keyword generation method (LKG) to reach related keywords from the search query in one step. LKG models the query keyword matching as an end-to-end keyword generation task based on the LLM through multi-match prompt tuning. Moreover, it employs the feedback tuning and the prefix tree-based constrained beam search to improve the generation quality and efficiency. Extensive offline experiments and online A/B testing demonstrate the effectiveness and superiority of LKG which is fully deployed in the Baidu sponsored search system bringing significant improvements.|查询关键词匹配通过检索用户查询的语义相关关键词来定位相关广告，在赞助商搜索广告中起着至关重要的作用。传统的技术解决方案采用级联漏斗结构的检索-判决-秩检索框架。然而，它在准确描述查询和关键字之间的语义相关性方面存在局限性，并且累积漏斗损失导致不满意的准确率召回率。针对上述问题，本文提出了一种基于大语言模型(LLM)的关键词生成方法(LKG) ，从搜索查询一步到位地生成相关关键词。LKG 通过多匹配提示调优，将查询关键字匹配建模为基于 LLM 的端到端关键字生成任务。同时，采用反馈调整和基于前缀树的约束波束搜索来提高生成质量和效率。大量的离线实验和在线 A/B 测试证明了 lkG 的有效性和优越性。 LKG 已完全部署在百度赞助的搜索系统中，带来了显著的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=One-step+Reach:+LLM-based+Keyword+Generation+for+Sponsored+Search+Advertising)|0|
|[A Case Study of Enhancing Sparse Retrieval using LLMs](https://doi.org/10.1145/3589335.3651945)|Michael Antonios Kruse Ayoub, Zhan Su, Qiuchi Li||While dense retrieval methods have made significant advancements, sparse retrieval techniques continue to offer advantages in terms of interpretability and generalizability. However, query-document term mismatch in sparse retrieval persists, rendering it infeasible for many practical applications. Recent research has shown that Large Language Models (LLMs) hold relevant information that can enhance sparse retrieval through the application of prompt engineering. In this paper, we build upon this concept to explore various strategies employing LLMs for information retrieval purposes. Specifically, we utilize LLMs to enhance sparse retrieval by query rewriting and query expansion. In query rewriting, the original query is refined by creating several new queries. For query expansion, LLMs are employed to generate extra terms, thereby enriching the original query. We conduct experiments on a range of well-known information retrieval datasets, including MSMARCO-passage, TREC2019, TREC2020, Natural Questions, SCIFACT. The experiments show that LLMs can be beneficial for sparse methods since the added information provided by the LLMs can help diminish the discrepancy between the term frequencies of the important terms in a query and the relevant document. In certain domains, we demonstrate that the effectiveness of LLMs is constrained, indicating that they may not consistently perform optimally, which will be explored in future research.|虽然密集检索方法取得了重大进展，但稀疏检索技术在可解释性和普遍性方面仍然具有优势。然而，在稀疏检索中，查询-文档关键词不匹配问题依然存在，这使得它在许多实际应用中不可行。最近的研究表明，大语言模型(LLM)包含相关信息，可以通过应用快速工程提高稀疏检索。在本文中，我们将以这个概念为基础，探讨利用 LLM 实现信息检索目的的各种策略。具体来说，我们利用 LLM 通过查询重写和查询扩展来增强稀疏检索。在查询重写中，原始查询通过创建几个新查询进行细化。对于查询扩展，使用 LLM 生成额外的术语，从而丰富了原始查询。我们在一系列著名的信息检索数据集上进行实验，包括 MSMARCO 通道，TREC2019，TREC2020，自然问题，SCIFACT。实验表明，LLM 对稀疏方法是有益的，因为 LLM 提供的附加信息有助于减少查询中重要术语的词频与相关文档之间的差异。在某些领域，我们证明了 LLM 的有效性是受限制的，表明它们可能不能始终保持最佳性能，这将在未来的研究中进行探索。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Case+Study+of+Enhancing+Sparse+Retrieval+using+LLMs)|0|
|[Bi-CAT: Improving Robustness of LLM-based Text Rankers to Conditional Distribution Shifts](https://doi.org/10.1145/3589335.3651947)|Sriram Srinivasan, Stephen Sheng, Rishabh Deshmukh, Chen Luo, Yesh Dattatreya, Subhajit Sanyal, S. V. N. Vishwanathan||Retrieval and ranking lie at the heart of several applications like search, question-answering, and recommendations. The use of Large language models (LLMs) such as BERT in these applications have shown promising results in recent times. Recent works on text-based retrievers and rankers show promising results by using bi-encoders (BE) architecture with BERT like LLMs for retrieval and a cross-attention transformer (CAT) architecture BERT or other LLMs for ranking the results retrieved. Although the use of CAT architecture for re-ranking improves ranking metrics, their robustness to data shifts is not guaranteed. In this work we analyze the robustness of CAT-based rankers. Specifically, we show that CAT rankers are sensitive to item distribution shifts conditioned on a query, we refer to this as conditional item distribution shift (CIDS). CIDS naturally occurs in large online search systems as the retrievers keep evolving, making it challenging to consistently train and evaluate rankers with the same item distribution. In this paper, we formally define CIDS and show that while CAT rankers are sensitive to this, BE models are far more robust to CIDS. We propose a simple yet effective approach referred to as BI-CAT which augments BE model outputs with CAT rankers, to significantly improve the robustness of CAT rankers without any drop in in-distribution performance. We conducted a series of experiments on two publicly available ranking datasets and one dataset from a large e-commerce store. Our results on dataset with CIDS demonstrate that the BI-CAT model significantly improves the robustness of CAT rankers by roughly 100-1000bps in F1 without any reduction in in-distribution model performance.|检索和排名是搜索、问答和推荐等应用程序的核心。近年来，诸如 BERT 之类的大语言模型在这些应用中的应用取得了令人鼓舞的成果。基于文本的检索器和排序器的最新研究表明，使用带有 BERT 的双编码器(BE)结构进行检索，使用交叉注意力转换器(CAT)结构 BERT 或其他 LLM 对检索结果进行排序，可以取得令人满意的结果。虽然使用 CAT 体系结构进行重新排序可以改善排序度量，但是它们对数据移位的鲁棒性并不能得到保证。本文分析了基于 CAT 的排序算法的鲁棒性。具体来说，我们表明 CAT 排名对条件查询下的项目分布移位是敏感的，我们称之为条件项目分布移位(CIDS)。CIDS 自然地出现在大型在线搜索系统中，因为检索器不断进化，使得对具有相同项目分布的排名者进行一致的训练和评估变得具有挑战性。在本文中，我们正式定义了 CIDS，并表明 CAT 排名对此敏感，而 BE 模型对 CIDS 的鲁棒性要强得多。我们提出了一种简单而有效的方法，称为 BI-CAT，它将 BE 模型的输出与 CAT 分类器相结合，以显著提高 CAT 分类器的鲁棒性，同时不降低分布式性能。我们对两个公开可用的排名数据集和一个来自大型电子商务商店的数据集进行了一系列的实验。我们在 CIDS 数据集上的结果表明，BI-CAT 模型在 F1中显著提高了 CAT 分类器的鲁棒性，大约提高了100-1000bps，而分布式模型的性能没有任何下降。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bi-CAT:+Improving+Robustness+of+LLM-based+Text+Rankers+to+Conditional+Distribution+Shifts)|0|
|[Deep Learning for Hate Speech Detection: A Personality-based Approach](https://doi.org/10.1145/3589335.3652502)|Kyuhan Lee, Sudha Ram||A crucial element in the combat against hate speech is the development of efficient algorithms for automatically detecting hate speech. Previous research, however, has primarily neglected important insights from the field of psychology literature, particularly the relationship between personality and hate, resulting in suboptimal performance in hate speech detection. To this end, we propose a novel framework for detecting hate speech focusing on people's personality factors reflected in their writing. Our framework has two components: (i) a knowledge distillation model for fully automating the process of personality inference from text and (ii) a personality-based deep learning model for hate speech detection. Our approach is unique in that it incorporates low-level personality factors, which have been largely neglected in prior literature, into automated hate speech detection and proposes novel deep learning components for fully exploiting the intricate relationship between personality and hate (i.e., intermediate personality factors). The evaluation shows that our model significantly outperforms state-of-the-art baselines. Our study paves the way for future research by incorporating personality aspects into the design of automated hate speech detection. In addition, it offers substantial assistance to online social platforms and governmental authorities facing challenges in effectively moderating hate speech.|打击仇恨言论的一个关键因素是开发自动检测仇恨言论的高效算法。然而，以往的研究主要忽视了心理学文献领域的重要见解，特别是人格和仇恨之间的关系，导致了仇恨言论检测的次优表现。为此，我们提出了一个新的框架来检测仇恨言论集中在人们的人格因素反映在他们的写作。我们的框架由两部分组成: (1)一个完全自动化的人格推理过程的知识提取模型和(2)一个基于人格的深度学习模型的仇恨言语检测。我们的方法是独特的，因为它将以前的文献中基本上被忽视的低层次人格因素纳入自动仇恨言论检测中，并提出了新颖的深度学习组件，以充分利用人格和仇恨(即中间人格因素)之间的复杂关系。评估表明，我们的模型明显优于最先进的基线。我们的研究通过将个性方面融入到自动仇恨语音检测的设计中，为未来的研究铺平了道路。此外，它还为在线社交平台和政府当局在有效缓和仇恨言论方面面临的挑战提供大量援助。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Learning+for+Hate+Speech+Detection:+A+Personality-based+Approach)|0|
|[Requirements and Challenges for Query Execution across Decentralized Environments](https://doi.org/10.1145/3589335.3652523)|Ruben Taelman||Due to the economic and societal problems being caused by the Web's growing centralization, there is an increasing interest in de-centralizing data on the Web. This decentralization does however cause a number of technical challenges. If we want to give users in decentralized environments the same level of user experience as they are used to with centralized applications, we need solutions to these challenges. We discuss how query engines can act as layer between applications on the one hand, and decentralized environments on the other hand, Query engines therefore act as an abstraction layer that hides the complexities of decentralized data management for application developers. In this article, we outline the requirements for query engines over decentralized environments. Furthermore, we show how existing approaches meet these requirements, and which challenges remain. As such, this article offers a high-level overview of a roadmap in the query and decentralization research domains.|由于经济和社会问题是由网络的日益集中所引起的，人们对网络上的数据分散化越来越感兴趣。然而，这种地方分权确实带来了一些技术挑战。如果我们希望在分散的环境中为用户提供与集中式应用程序相同水平的用户体验，我们需要解决这些挑战。我们讨论了查询引擎如何在应用程序和分散式环境之间扮演层次的角色，因此，查询引擎就像一个抽象层，为应用程序开发人员隐藏了分散式数据管理的复杂性。在本文中，我们概述了分散环境中查询引擎的需求。此外，我们还展示了现有的方法如何满足这些需求，以及仍然存在哪些挑战。因此，本文对查询和地方分权研究领域的路线图提供了一个高层次的概述。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Requirements+and+Challenges+for+Query+Execution+across+Decentralized+Environments)|0|
|[Diffusion Recommendation with Implicit Sequence Influence](https://doi.org/10.1145/3589335.3651951)|Yong Niu, Xing Xing, Zhichun Jia, Ruidi Liu, Mindong Xin, Jianfu Cui||Sequence recommendation tasks often have performance bottlenecks, mainly reflected in the following two aspects: previous research relied on a single item embedding distribution, resulting in a decrease in overall modeling ability. In addition, the implicit dynamic preferences reflected in user interaction sequences are not distinguished, and the feature representation ability is insufficient. To address these issues, we propose a novel model called Diffusion Recommendation with Implicit Sequence Influence (DiffRIS). Specifically, we establish an implicit feature extraction module, which includes multi-scale CNN and residual LSTM networks that learn local and global features of sequence information, respectively, to explore the length dependence of data features. Subsequently, we use the output of the module as a conditional input for the diffusion model, guiding the denoising process based on historical interactions. Through experiments on two open-source datasets, we find that implicit features of sequences have a positive impact on the diffusion process. The proposed DiffRIS framework performs well compared to multiple baseline models, effectively improving the accuracy of sequential recommendation models. We believe that the proposed DiffRIS can provide some research ideas for diffusion sequence recommendation.|序列推荐任务往往存在性能瓶颈，主要体现在以下两个方面: 以往的研究依赖于单项嵌入分布，导致整体建模能力下降。此外，用户交互序列中反映的隐式动态偏好没有得到区分，特征表示能力不足。为了解决这些问题，我们提出了一种新的模型，称为隐式序列影响的扩散推荐模型。具体来说，我们建立了一个隐式特征提取模块，该模块包括多尺度 CNN 网络和残差 LSTM 网络，分别学习序列信息的局部和全局特征，以探索数据特征的长度依赖性。然后，我们使用模块的输出作为扩散模型的条件输入，指导基于历史交互的去噪过程。通过对两个开源数据集的实验，我们发现序列的隐含特征对扩散过程有积极的影响。与多个基线模型相比，区分 RIS 框架具有良好的性能，有效地提高了序贯推荐模型的准确性。我们相信本文提出的区分红外推荐系统可以为扩散序列推荐提供一些研究思路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diffusion+Recommendation+with+Implicit+Sequence+Influence)|0|
|[Multimodal Conditioned Diffusion Model for Recommendation](https://doi.org/10.1145/3589335.3651956)|Haokai Ma, Yimeng Yang, Lei Meng, Ruobing Xie, Xiangxu Meng||Multimodal recommendation aims at to modeling the feature distributions of items by using their multi-modal information. Prior efforts typically focus on the denoising of the user-item graph with a degree-sensitive strategy, which may not well-handle the users' consistent preference across modalities. More importantly, it has been observed that existing methods may learn ill-posed item embeddings due to their focus on a specific auxiliary optimization task for multimodal representations rather than explicitly modeling them. This paper therefore presents a solution that takes the advantages of the explicit uncertainty injection ability of Diffusion Model (DM) for the modeling and fusion of multi-modal information. Specifically, we propose a novel Multimodal Conditioned Diffusion Model for Recommendation (MCDRec), which tailors DM with two technical modules to model the high-order multimodal knowledge. The first module is multimodal-conditioned representation diffusion (MRD), which integrates pre-extracted multimodal knowledge into the item representation modeling via a tailored DM. This smoothly bridges the insurmountable gap between the multi-modal content features and the collaborative signals. Secondly, with the diffusion-guided graph denoising (DGD) module, MCDRec may effectively denoise the user-item graph by filtering the occasional interactions in user historical behaviors. This is achieved with the power of DM in aligning the users' collaborative preferences with their shared items' content information. Extensive experiments compared to several SOTA baselines on two real-word datasets demonstrate the effectiveness of MCDRec. The specific visualization also reveals the potential of MRD to precisely handling the high-order representation correlations among the user embeddings and the multi-modal heterogeneous representations of items.|多模态推荐的目的是利用项目的多模态信息对项目的特征分布进行建模。先前的工作通常集中在用户项目图的去噪与度敏感的策略，这可能不能很好地处理用户的一致性偏好跨模式。更重要的是，已经观察到，现有的方法可能会学习病态项嵌入，因为他们的重点是一个具体的辅助优化任务的多模态表示，而不是显式建模它们。为此，本文提出了一种利用扩散模型(DM)的显式不确定性注入能力对多模态信息进行建模和融合的解决方案。具体地说，我们提出了一种新的多模态条件扩散推荐模型(MCDRec) ，该模型将 DM 与两个技术模块相结合，对高阶多模态知识进行建模。第一个模块是多模态条件表示扩散(MRD) ，它通过一个定制的 DM 将预提取的多模态知识集成到项目表示模型中。这顺利地弥合了多模态内容特性和协作信号之间不可逾越的鸿沟。其次，利用扩散引导图去噪(DGD)模块，MCDRec 可以通过过滤用户历史行为中的偶然交互来有效地去除用户项图的噪声。这是通过 DM 的功能来实现的，它可以使用户的协作偏好与共享项目的内容信息保持一致。在两个实际数据集上对几个 SOTA 基线进行了大量的实验，证明了 MCDRec 算法的有效性。具体的可视化还揭示了 MRD 在精确处理用户嵌入和多模态异构项表示之间的高阶表示相关性方面的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Conditioned+Diffusion+Model+for+Recommendation)|0|
|[Universal Knowledge Graph Embeddings](https://doi.org/10.1145/3589335.3651978)|N'Dah Jean Kouagou, Caglar Demir, Hamada M. Zahera, Adrian Wilke, Stefan Heindorf, Jiayi Li, AxelCyrille Ngonga Ngomo||A variety of knowledge graph embedding approaches have been developed. Most of them obtain embeddings by learning the structure of the knowledge graph within a link prediction setting. As a result, the embeddings reflect only the semantics of a single knowledge graph, and embeddings for different knowledge graphs are not aligned, e.g., they cannot be used to find similar entities across knowledge graphs via nearest neighbor search. However, knowledge graph embedding applications such as entity disambiguation require a more global representation, i.e., a representation that is valid across multiple sources. We propose to learn universal knowledge graph embeddings from large-scale interlinked knowledge sources. To this end, we fuse large knowledge graphs based on the owl:sameAs relation such that every entity is represented by a unique identity. We instantiate our idea by computing universal embeddings based on DBpedia and Wikidata yielding embeddings for about 180 million entities, 15 thousand relations, and 1.2 billion triples. Moreover, we develop a convenient API to provide embeddings as a service. Experiments on link prediction show that universal knowledge graph embeddings encode better semantics compared to embeddings computed on a single knowledge graph. For reproducibility purposes, we provide our source code and datasets open access at https://github.com/dice-group/Universal_Embeddings|发展了各种知识图嵌入方法。它们中的大多数通过学习链接预测设置中的知识图的结构来获得嵌入。因此，嵌入只能反映单个知识图的语义，不同知识图的嵌入是不一致的，例如，它们不能通过最近邻搜索在知识图中找到相似的实体。然而，知识图嵌入应用程序(如实体消歧)需要一个更全局的表示，即一个跨多个源有效的表示。我们提出了从大规模相互关联的知识源中学习通用知识图嵌入。为此，我们融合了基于猫头鹰的大型知识图: 相同的关系，使每个实体都有一个独特的身份。我们通过基于 DBpedia 和 Wikidata 的通用嵌入计算实例化了我们的想法，产生了大约1.8亿个实体、1.5万个关系和12亿个三元组的嵌入。此外，我们还开发了一个方便的 API 来提供嵌入式服务。链接预测实验表明，通用知识图的嵌入编码比单一知识图的嵌入编码具有更好的语义性能。出于可重复性的目的，我们提供源代码和数据集的开放访问 https://github.com/dice-group/universal_embeddings|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Universal+Knowledge+Graph+Embeddings)|0|
|[Towards Graph Foundation Models for Personalization](https://doi.org/10.1145/3589335.3651980)|Andreas Damianou, Francesco Fabbri, Paul Gigioli, Marco De Nadai, Alice Wang, Enrico Palumbo, Mounia Lalmas||In the realm of personalization, integrating diverse information sources such as consumption signals and content-based representations is becoming increasingly critical to build state-of-the-art solutions. In this regard, two of the biggest trends in research around this subject are Graph Neural Networks (GNNs) and Foundation Models (FMs). While GNNs emerged as a popular solution in industry for powering personalization at scale, FMs have only recently caught attention for their promising performance in personalization tasks like ranking and retrieval. In this paper, we present a graph-based foundation modeling approach tailored to personalization. Central to this approach is a Heterogeneous GNN (HGNN) designed to capture multi-hop content and consumption relationships across a range of recommendable item types. To ensure the generality required from a Foundation Model, we employ a Large Language Model (LLM) text-based featurization of nodes that accommodates all item types, and construct the graph using co-interaction signals, which inherently transcend content specificity. To facilitate practical generalization, we further couple the HGNN with an adaptation mechanism based on a two-tower (2T) architecture, which also operates agnostically to content type. This multi-stage approach ensures high scalability; while the HGNN produces general purpose embeddings, the 2T component models in a continuous space the sheer size of user-item interaction data. Our comprehensive approach has been rigorously tested and proven effective in delivering recommendations across a diverse array of products within a real-world, industrial audio streaming platform.|在个性化领域，集成消费信号和基于内容的表示等多种信息源对于构建最先进的解决方案正变得越来越重要。在这方面，围绕这个主题的两个最大的研究趋势是图形神经网络(GNN)和基础模型(FM)。虽然 GNN 作为一种大规模个性化的解决方案出现在行业中，但是 FM 只是在最近才因其在个性化任务(如排名和检索)中的良好表现而引起人们的注意。在本文中，我们提出了一种基于图的基础建模方法，以适应个性化。这种方法的核心是异构 GNN (HGNN) ，它设计用于捕获跨一系列可推荐的项目类型的多跳内容和消费关系。为了确保基础模型所需的通用性，我们采用了基于大语言模型(LLM)的节点文本特征化，以适应所有项目类型，并使用共同交互信号构建图，这本质上超越了内容特异性。为了便于实际推广，我们进一步将 HGNN 与一个基于双塔(2T)架构的适应机制耦合，该架构也对内容类型不可知地运行。这种多阶段的方法确保了高度的可伸缩性; 当 HGNN 产生通用的嵌入时，2T 组件模型在一个连续的空间中，大量的用户项交互数据。我们的综合方法已经经过严格的测试，并被证明是有效的，可以在一个真实世界的工业音频流媒体平台上提供多种产品的建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Graph+Foundation+Models+for+Personalization)|0|
|[AI for Materials Innovation: Self-Improving Photosensitizer Discovery System via Bayesian Search with First-Principles Simulation](https://doi.org/10.1145/3589334.3649115)|Bin Liu||Artificial intelligence (AI) based self-learning or self-improving material discovery systems will enable next-generation material discovery. Herein, we demonstrate how to combine accurate prediction of material performance via first-principles calculation and Bayesian optimization-based active learning to realize a self-improving discovery system for high-performance photosensitizers (PSs). Through self-improving cycles, such a system can improve the model prediction accuracy (best mean absolute error of 0.090 eV for singlet--triplet spitting) and high-performance PS search ability, realizing efficient discovery of PSs. From a molecular space with more than 7 million molecules, 5357 potential high-performance PSs were discovered. Four PSs were further synthesized to show performance comparable with or superior to commercial ones. This work highlights the potential of active learning in first principle-based materials design, and the discovered structures could boost the development of photosensitization-related applications, which is one of the typical examples of how AI can be used to accelerate materials innovation and facilitate science development in general.|基于人工智能(AI)的自学习或自我改进的材料发现系统将使下一代材料发现成为可能。本文阐述了如何将基于第一性原理计算和基于贝叶斯优化的主动学习相结合，实现高性能光敏剂的自我改进发现系统。该系统通过自我改进循环，提高了模型预测精度(单重态-三重态分裂的最佳平均绝对误差为0.090 eV)和高性能 PS 搜索能力，实现了 PSS 的有效发现。从一个有超过700万个分子的分子空间中，发现了5357个潜在的高性能 PSS。进一步合成了四个 PPS，以显示与商业性能相当或优于商业性能。这项工作强调了主动学习在基于第一原理的材料设计中的潜力，并且发现的结构可以促进光敏化相关应用的发展，这是人工智能如何加速材料创新和促进科学发展的典型例子之一。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI+for+Materials+Innovation:+Self-Improving+Photosensitizer+Discovery+System+via+Bayesian+Search+with+First-Principles+Simulation)|0|
|[Tight Competitive and Variance Analyses of Matching Policies in Gig Platforms](https://doi.org/10.1145/3589334.3645335)|Pan Xu||In this paper, we propose an online-matching-based model to tackle the two fundamental issues, matching and pricing, existing in a wide range of real-world gig platforms, including ride-hailing (matching riders and drivers), crowdsourcing markets (pairing workers and tasks), and online recommendations (offering items to customers). Our model assumes the arriving distributions of dynamic agents (e.g., riders, workers, and buyers) are accessible in advance, and they can change over time, which is referred to as Known Heterogeneous Distributions (KHD). In this paper, we initiate variance analysis for online matching algorithms under KHD. Unlike the popular competitive-ratio (CR) metric, the variance of online algorithms' performance is rarely studied due to inherent technical challenges, though it is well linked to robustness. We focus on two natural parameterized sampling policies, denoted by 𝖠𝖳𝖳(γ) and 𝖲𝖠𝖬𝖯(γ), which appear as foundational bedrock in online algorithm design. We offer rigorous competitive ratio (CR) and variance analyses for both policies. Specifically, we show that 𝖠𝖳𝖳(γ) with γ∈ [0,1/2] achieves a CR of γ and a variance of γ· (1-γ) · B on the total number of matches with B being the total matching capacity. In contrast, 𝖲𝖠𝖬𝖯(γ) with γ∈ [0,1] accomplishes a CR of γ (1-γ) and a variance of γ̅ (1-γ̅)· B with γ̅=min(γ,1/2). All CR and variance analyses are tight and unconditional of any benchmark. As a byproduct, we prove that 𝖠𝖳𝖳(γ=1/2) achieves an optimal CR of 1/2.|在本文中，我们提出了一个基于在线匹配的模型来解决两个基本问题，匹配和定价，存在于广泛的现实世界的零工平台，包括叫车(匹配乘客和司机) ，众包市场(配对工人和任务)和在线推荐(提供项目给客户)。我们的模型假设动态代理人(如乘客、工人和购买者)的到达分布可以提前获得，并且它们可以随时间变化，这被称为已知的异质分布(KHD)。本文对 KHD 下的在线匹配算法进行了方差分析。与流行的竞争比率(CR)度量不同，在线算法的性能方差很少被研究，由于固有的技术挑战，虽然它很好地与健壮性联系在一起。本文重点研究了在线算法设计中的两个自然参数化抽样策略，即 ATT (γ)和 SAMP (γ) ，它们是在线算法设计的基础。我们提供了严格的竞争比率(CR)和方差分析的两个政策。具体地说，我们证明了当总匹配次数为 B 时，具有 γ ∈[0,1/2]的 ATT (γ)达到 γ 的 CR 和 γ · (1-γ) · B 的方差。相比之下，具有 γ ∈[0,1]的 SAMP (γ)实现了 γ (1-γ)的 CR 和 γ (1-γ) · B 与 γ = min (γ，1/2)的方差。所有 CR 和方差分析对任何基准都是严格和无条件的。作为副产品，我们证明了 ATT (γ = 1/2)达到了1/2的最优 CR。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tight+Competitive+and+Variance+Analyses+of+Matching+Policies+in+Gig+Platforms)|0|
|[Ad vs Organic: Revisiting Incentive Compatible Mechanism Design in E-commerce Platforms](https://doi.org/10.1145/3589334.3645638)|Ningyuan Li, Yunxuan Ma, Yang Zhao, Qian Wang, Zhilin Zhang, Chuan Yu, Jian Xu, Bo Zheng, Xiaotie Deng||On typical e-commerce platforms, a product can be displayed to users in two possible forms, as an ad item or an organic item. Usually, ad and organic items are separately selected by the advertising system and recommendation system, and then combined by a content merging mechanism. Although the design of the content merging mechanism has been extensively studied, little attention has been given to a crucial situation where there is an overlap between candidate ad and organic items. Despite its common occurrence, this situation is not correctly handled by almost all existing works, potentially leading to incentive problems for advertisers and the violation of economic constraints. To address these issues, we revisit the design of the content merging mechanism. We introduce a necessary property called form stability, and provide simplification results of the mechanism design problem. Furthermore, we design two simple mechanisms strictly ensuring desired economic properties including incentive compatibility, and demonstrate their guaranteed performance through competitive ratio analysis under certain conditions.|在典型的电子商务平台上，产品可以以两种可能的形式向用户显示，即广告项目或有机项目。通常，广告和有机项目分别由广告系统和推荐系统选择，然后通过内容合并机制进行组合。尽管对内容合并机制的设计进行了广泛的研究，但很少注意到候选广告和有机项目之间存在重叠的关键情况。尽管这种情况经常发生，但几乎所有现有作品都没有正确处理这种情况，这可能导致广告商的激励问题和违反经济制约。为了解决这些问题，我们重新讨论了内容合并机制的设计。我们引入了形式稳定性这一必要性质，并给出了机构设计问题的简化结果。此外，我们设计了两个简单的机制，严格保证理想的经济属性，包括激励相容，并通过在一定条件下的竞争比率分析来证明它们的保证性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ad+vs+Organic:+Revisiting+Incentive+Compatible+Mechanism+Design+in+E-commerce+Platforms)|0|
|[Towards Expansive and Adaptive Hard Negative Mining: Graph Contrastive Learning via Subspace Preserving](https://doi.org/10.1145/3589334.3645327)|Zhezheng Hao, Haonan Xin, Long Wei, Liaoyuan Tang, Rong Wang, Feiping Nie||Graph Neural Networks (GNNs) have emerged as the predominant approach for analyzing graph data on the web and beyond. Contrastive learning (CL), a self-supervised paradigm, not only mitigates reliance on annotations but also has potential in performance. The hard negative sampling strategy that benefits CL in other domains proves ineffective in the context of Graph Contrastive Learning (GCL) due to the message passing mechanism. Embracing the subspace hypothesis in clustering, we propose a method towards expansive and adaptive hard negative mining, referred to as G raph contR astive leA rning via subsP ace prE serving (GRAPE ). Beyond homophily, we argue that false negatives are prevalent over an expansive range and exploring them confers benefits upon GCL. Diverging from existing neighbor-based methods, our method seeks to mine long-range hard negatives throughout subspace, where message passing is conceived as interactions between subspaces. %Empirical investigations back up this strategy. Additionally, our method adaptively scales the hard negatives set through subspace preservation during training. In practice, we develop two schemes to enhance GCL that are pluggable into existing GCL frameworks. The underlying mechanisms are analyzed and the connections to related methods are investigated. Comprehensive experiments demonstrate that our method outperforms across diverse graph datasets and remains competitive across varied application scenarios\footnoteOur code is available at https://github.com/zz-haooo/WWW24-GRAPE. .|图形神经网络(GNN)已经成为分析网络和其他领域图形数据的主要方法。对比学习(CL)是一种自我监督的范式，它不仅减轻了对注释的依赖，而且在性能方面也有潜力。在图形对比学习(GCL)环境中，由于消息传递机制的存在，硬负抽样策略在其他领域对 CL 有利。基于聚类中的子空间假设，本文提出了一种扩展的自适应硬负挖掘方法，即通过子空间 prE 服务(GRAPE)进行 G 图控制的被动挖掘。除了同调，我们认为，假否定是普遍存在的一个广泛的范围和探索他们赋予 GCL 的好处。与现有的基于邻居的方法不同，我们的方法寻求挖掘整个子空间的长程硬负面，其中信息传递被认为是子空间之间的相互作用。经验调查支持这一策略。此外，我们的方法在训练过程中通过子空间保持自适应地扩展硬负片集。在实践中，我们开发了两个可插入现有 GCL 框架的方案来增强 GCL。分析了其机理，并探讨了与相关方法的联系。综合实验表明，我们的方法在不同的图形数据集中表现优异，并且在不同的应用场景中仍然具有竞争力。脚注我们的代码可以在 https://github.com/zz-haooo/www24-grape 获得。.|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Expansive+and+Adaptive+Hard+Negative+Mining:+Graph+Contrastive+Learning+via+Subspace+Preserving)|0|
|[Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction](https://doi.org/10.1145/3589334.3645372)|Minsang Kim, Seung Baek||Learning positional information of nodes in a graph is important for link prediction tasks. We propose a representation of positional information using representative nodes called landmarks. A small number of nodes with high degree centrality are selected as landmarks, which serve as reference points for the nodes' positions. We justify this selection strategy for well-known random graph models and derive closed-form bounds on the average path lengths involving landmarks. In a model for power-law graphs, we prove that landmarks provide asymptotically exact information on inter-node distances. We apply theoretical insights to practical networks and propose Hierarchical Position embedding with Landmarks and Clustering (HPLC). HPLC combines landmark selection and graph clustering, where the graph is partitioned into densely connected clusters in which nodes with the highest degree are selected as landmarks. HPLC leverages the positional information of nodes based on landmarks at various levels of hierarchy such as nodes' distances to landmarks, inter-landmark distances and hierarchical grouping of clusters. Experiments show that HPLC achieves state-of-the-art performances of link prediction on various datasets in terms of HIT@K, MRR, and AUC. The code is available at <https://github.com/kmswin1/HPLC>.|学习图中节点的位置信息是链路预测任务的重要内容。我们提出了一种位置信息的表示使用代表性的节点称为地标。选取少量高度集中的节点作为标志点，作为节点位置的参考点。对于已知的随机图模型，我们证明了这种选择策略的合理性，并推导出了包含地标的平均路径长度的闭合界。在幂律图模型中，我们证明了路标提供节点间距离的渐近精确信息。将理论知识应用于实际网络，提出了基于地标和聚类(HPLC)的层次位置嵌入算法。高效液相色谱将地标选择和图聚类相结合，将图划分为密集连通的聚类，其中选择程度最高的节点作为地标。高效液相色谱利用节点的位置信息的基础上，在不同的层次结构，如节点的距离地标，地标之间的距离和等级分组的集群。实验结果表明，高效液相色谱在 HIT@K、 MRR 和 AUC 方面实现了对不同数据集的链接预测。代码可在 <  https://github.com/kmswin1/hplc 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Position+Embedding+of+Graphs+with+Landmarks+and+Clustering+for+Link+Prediction)|0|
|[Collaborative Metapath Enhanced Corporate Default Risk Assessment on Heterogeneous Graph](https://doi.org/10.1145/3589334.3645402)|Zheng Zhang, Yingsheng Ji, Jiachen Shen, Yushu Chen, Xi Zhang, Guangwen Yang||Default risk assessment for small companies is a tough problem in financial services. Recent efforts utilize advanced Heterogeneous Graph Neural Networks (HGNNs) with metapaths to exploit interactive features in corporate activities for risk analysis. However, few works are proposed for commercial banks. Given a real financial graph, how to detect corporate default risks? We identify two challenges for the task. (1) Massive noisy connections hinder HGNNs to achieve strong results. (2) Multiple semantic connections greatly increase transitive default risk, while existing aggregation schemes do not leverage such connection patterns. In this work, we propose a novel Heterogeneous Graph Co-Attention Network for corporate default risk assessment. Our model takes advantage of collaborative metapaths to distill risky features by a co-attentive aggregation mechanism. First, the local attention score models the importance of neighbors under each metapath by holistic metapath context. Second, the global attention score fuse local attention scores to filter valuable/noisy signals. Then, pairwise importance learning aims to enhance attention scores of multi-metapath neighbors for risky feature distillation. Extensive experiments on large-scale banking datasets demonstrate the effectiveness of our method.|小企业违约风险评估是金融服务领域的一个难题。最近的努力利用先进的异构图神经网络(HGNNs)与元路径开发交互特征的企业活动的风险分析。然而，对于商业银行来说，提出的工作却很少。给定一个真实的财务图表，如何检测企业违约风险？我们确定了这项任务的两个挑战。(1)大量的噪声连接阻碍 HGNN 获得强大的结果。(2)多个语义连接极大地增加了传递缺省风险，而现有的聚合方案没有利用这种连接模式。在本研究中，我们提出一个新颖的异质图形共注意网路来评估公司违约风险。我们的模型利用协作元路径，通过协同关注聚合机制提取风险特征。首先，局部注意得分通过整体元路径上下文模拟每个元路径下邻居的重要性。其次，全局注意得分融合局部注意得分，以过滤有价值/有噪声的信号。其次，成对重要性学习旨在提高多元路径邻居对危险特征提取的注意分数。在大规模银行数据集上的大量实验证明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaborative+Metapath+Enhanced+Corporate+Default+Risk+Assessment+on+Heterogeneous+Graph)|0|
|[Graph Contrastive Learning with Kernel Dependence Maximization for Social Recommendation](https://doi.org/10.1145/3589334.3645412)|Xuelian Ni, Fei Xiong, Yu Zheng, Liang Wang||Contrastive learning (CL) has recently catalyzed a productive avenue of research for recommendation. The efficacy of most CL methods for recommendation may hinge on their capacity to learn representation uniformity by mapping the data onto a hypersphere. Nonetheless, applying contrastive learning to downstream recommendation tasks remains challenging, as existing CL methods encounter difficulties in capturing the nonlinear dependence of representations in high-dimensional space and struggle to learn hierarchical social dependency among users-essential points for modeling user preferences. Moreover, the subtle distinctions between the augmented representations render CL methods sensitive to noise perturbations. Inspired by the Hilbert-Schmidt independence criterion (HSIC), we propose a graph Contrastive Learning model with Kernel Dependence Maximization CL-KDM for social recommendation to address these challenges. Specifically, to explicitly learn the kernel dependence of representations and improve the robustness and generalization of recommendation, we maximize the kernel dependence of augmented representations in kernel Hilbert space by introducing HSIC into the graph contrastive learning. Additionally, to simultaneously extract the hierarchical social dependency across users while preserving underlying structures, we design a hierarchical mutual information maximization module for generating augmented user representations, which are injected into the message passing of a graph neural network to enhance recommendation. Extensive experiments are conducted on three social recommendation datasets, and the results indicate that CL-KDM outperforms various baseline recommendation methods.|对比学习(CL)最近催化了一个富有成效的推荐研究途径。大多数 CL 推荐方法的有效性可能取决于它们通过将数据映射到超球面来学习表示一致性的能力。尽管如此，将对比学习应用于下游推荐任务仍然具有挑战性，因为现有的 CL 方法在捕获高维空间中表示的非线性依赖性方面遇到困难，并且努力学习用户之间的等级社会依赖性-建模用户偏好的关键点。此外，增广表示之间的细微差别使 CL 方法对噪声扰动敏感。受到 Hilbert-Schmidt 独立性标准(HSIC)的启发，我们提出了一个基于核依赖最大化 CL-kDM 的图形对比学习模型，用于社会推荐，以应对这些挑战。特别地，为了明确地学习表示的核依赖性，提高推荐的鲁棒性和泛化性，我们在核 Hilbert 空间中引入 HSIC，最大化增广表示的核依赖性。此外，为了在保留底层结构的同时提取用户之间的层次化社会依赖，我们设计了一个层次化互信息最大化模块来生成增强的用户表示，并将其注入到图神经网络的消息传递中以增强推荐。在三个社会推荐数据集上进行了广泛的实验，结果表明 CL-KDM 的性能优于各种基线推荐方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Contrastive+Learning+with+Kernel+Dependence+Maximization+for+Social+Recommendation)|0|
|[MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs](https://doi.org/10.1145/3589334.3645423)|Xingtong Yu, Chang Zhou, Yuan Fang, Xinming Zhang||Graphs can inherently model interconnected objects on the Web, thereby facilitating a series of Web applications, such as web analyzing and content recommendation. Recently, Graph Neural Networks (GNNs) have emerged as a mainstream technique for graph representation learning. However, their efficacy within an end-to-end supervised framework is significantly tied to the availabilityof task-specific labels. To mitigate labeling costs and enhance robustness in few-shot settings, pre-training on self-supervised tasks has emerged as a promising method, while prompting has been proposed to further narrow the objective gap between pretext and downstream tasks. Although there has been some initial exploration of prompt-based learning on graphs, they primarily leverage a single pretext task, resulting in a limited subset of general knowledge that could be learned from the pre-training data. Hence, in this paper, we propose MultiGPrompt, a novel multi-task pre-training and prompting framework to exploit multiple pretext tasks for more comprehensive pre-trained knowledge. First, in pre-training, we design a set of pretext tokens to synergize multiple pretext tasks. Second, we propose a dual-prompt mechanism consisting of composed and open prompts to leverage task-specific and global pre-training knowledge, to guide downstream tasks in few-shot settings. Finally, we conduct extensive experiments on six public datasets to evaluate and analyze MultiGPrompt.|图形可以内在地对 Web 上相互连接的对象建模，从而方便了一系列 Web 应用程序，例如 Web 分析和内容推荐。近年来，图神经网络已经成为图表示学习的主流技术。然而，它们在端到端监督框架内的有效性与特定任务标签的可用性有着显著的联系。为了降低标记成本和增强在少镜头环境下的鲁棒性，自我监督任务的预训练已经成为一种有前途的方法，同时提出了提示来进一步缩小借口和下游任务之间的目标差距。虽然已经有一些基于图表的及时学习的初步探索，他们主要利用一个单一的借口任务，导致有限的一般知识子集，可以从培训前的数据学习。因此，本文提出了一种新颖的多任务预训练和激励框架 MultiGPrompt，该框架可以利用多任务来获得更全面的预训练知识。首先，在预训中，我们设计一组借口标记来协同多个借口任务。其次，我们提出了一个双提示机制，包括组成和开放的提示，以利用任务特定和全球培训前的知识，以指导下游任务在少镜头设置。最后，我们在六个公共数据集上进行了广泛的实验来评估和分析 MultiGPrompt。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MultiGPrompt+for+Multi-Task+Pre-Training+and+Prompting+on+Graphs)|0|
|[SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding](https://doi.org/10.1145/3589334.3645441)|Ruiyi Yang, Flora D. Salim, Hao Xue||Knowledge graphs (KGs) have been increasingly employed for link prediction and recommendation using real-world datasets. However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios. This often results in suboptimal predictions and recommendations. Although there are effective spatio-temporal inference methods, they face challenges such as scalability with large datasets and inadequate semantic understanding, which impede their performance. To address these limitations, this paper introduces a novel framework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing and exploring spatio-temporal KGs. To integrate spatial and temporal data into KGs, our framework exploited through a new 3-step embedding method. Output embeddings can be used for future temporal sequence prediction and spatial information recommendation, providing valuable insights for various applications such as retail sales forecasting and traffic volume prediction. Our framework offers a simple but comprehensive way to understand the underlying patterns and trends in dynamic KG, thereby enhancing the accuracy of predictions and the relevance of recommendations. This work paves the way for more effective utilization of spatio-temporal data in KGs, with potential impacts across a wide range of sectors.|知识图(KGs)越来越多地被用于利用实际数据集进行链接预测和推荐。然而，现有的方法大多依赖于静态数据，忽视了真实场景的动态性和隐藏的时空属性。这常常导致不理想的预测和建议。虽然有效的时空推理方法已经存在，但是它们面临着大数据集的可扩展性和语义理解不足等问题，这些问题阻碍了它们的性能。针对这些局限性，本文提出了一种构建和探索时空 KG 的新框架——简单时空知识图(SSTKG)。为了将空间数据和时间数据整合到 KG 中，我们的框架采用了一种新的三步嵌入方法。输出嵌入可用于未来的时间序列预测和空间信息推荐，为零售业销售预测和交通量预测等各种应用提供有价值的见解。我们的框架提供了一个简单而全面的方法来了解动态幼稚园的基本模式和趋势，从而提高预测的准确性和建议的相关性。这项工作为幼儿园更有效地利用时空数据铺平了道路，并可能对多个部门产生影响。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SSTKG:+Simple+Spatio-Temporal+Knowledge+Graph+for+Intepretable+and+Versatile+Dynamic+Information+Embedding)|0|
|[Spectral Heterogeneous Graph Convolutions via Positive Noncommutative Polynomials](https://doi.org/10.1145/3589334.3645515)|Mingguo He, Zhewei Wei, Shikun Feng, Zhengjie Huang, Weibin Li, Yu Sun, Dianhai Yu||Heterogeneous Graph Neural Networks (HGNNs) have gained significant popularity in various heterogeneous graph learning tasks. However, most existing HGNNs rely on spatial domain-based methods to aggregate information, i.e., manually selected meta-paths or some heuristic modules, lacking theoretical guarantees. Furthermore, these methods cannot learn arbitrary valid heterogeneous graph filters within the spectral domain, which have limited expressiveness. To tackle these issues, we present a positive spectral heterogeneous graph convolution via positive noncommutative polynomials. Then, using this convolution, we propose PSHGCN, a novel Positive Spectral Heterogeneous Graph Convolutional Network. PSHGCN offers a simple yet effective method for learning valid heterogeneous graph filters. Moreover, we demonstrate the rationale of PSHGCN in the graph optimization framework. We conducted an extensive experimental study to show that PSHGCN can learn diverse heterogeneous graph filters and outperform all baselines on open benchmarks. Notably, PSHGCN exhibits remarkable scalability, efficiently handling large real-world graphs comprising millions of nodes and edges. Our codes are available at https://github.com/ivam-he/PSHGCN.|异构图神经网络在各种异构图学习任务中得到了广泛的应用。然而，现有的 HGNN 大多依赖于基于空间域的方法来聚合信息，即手工选择元路径或一些启发式模块，缺乏理论保证。此外，这些方法不能在谱域内学习任意有效的异构图滤波器，表达能力有限。为了解决这些问题，我们提出了一个正谱异质图卷积通过正的非交换多项式。然后，利用这种卷积，我们提出了一种新的正谱异质图卷积网络 PSHGCN。PSHGCN 为学习有效的异构图过滤器提供了一种简单而有效的方法。此外，本文还在图优化框架中论证了 PSHGCN 算法的基本原理。我们进行了广泛的实验研究表明，PSHGCN 可以学习不同的异构图过滤器和优于所有基线的开放基准。值得注意的是，PSHGCN 具有显著的可伸缩性，能够有效地处理包含数百万个节点和边的大型真实世界图。我们的代码可以在 https://github.com/ivam-he/pshgcn 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spectral+Heterogeneous+Graph+Convolutions+via+Positive+Noncommutative+Polynomials)|0|
|[Densest Subhypergraph: Negative Supermodular Functions and Strongly Localized Methods](https://doi.org/10.1145/3589334.3645624)|Yufan Huang, David F. Gleich, Nate Veldt||Dense subgraph discovery is a fundamental primitive in graph and hypergraph analysis which among other applications has been used for real-time story detection on social media and improving access to data stores of social networking systems. We present several contributions for localized densest subgraph discovery, which seeks dense subgraphs located nearby given seed sets of nodes. We first introduce a generalization of a recent anchored densest subgraph problem, extending this previous objective to hypergraphs and also adding a tunable locality parameter that controls the extent to which the output set overlaps with seed nodes. Our primary technical contribution is to prove when it is possible to obtain a strongly-local algorithm for solving this problem, meaning that the runtime depends only on the size of the input set. We provide a strongly-local algorithm that applies whenever the locality parameter is not too small, and show via counterexample why strongly-local algorithms are impossible below a certain threshold. Along the way to proving our results for localized densest subgraph discovery, we also provide several advances in solving global dense subgraph discovery objectives. This includes the first strongly polynomial time algorithm for the densest supermodular set problem and a flow-based exact algorithm for a heavy and dense subgraph discovery problem in graphs with arbitrary node weights. We demonstrate our algorithms on several web-based data analysis tasks.|密集子图发现是图和超图分析中的一个基本原理，除其他应用外，它还被用于社交媒体上的实时故事检测和改善对社交网络系统数据存储的访问。我们提出了局部密集子图发现的几个贡献，它寻找位于给定种子集附近的密集子图。我们首先引入了一个最近抛锚的最密集子图问题的推广，将这个先前的目标扩展到超图，并且添加了一个可调的局部性参数来控制输出集与种子节点重叠的程度。我们的主要技术贡献是证明何时可以获得解决这个问题的强局部算法，这意味着运行时仅取决于输入集的大小。我们提供了一个强局部算法，适用于局部参数不太小的情况，并通过反例说明为什么强局部算法不可能低于一定的阈值。在证明局部密集子图发现结果的过程中，我们还提供了解决全局密集子图发现目标的一些进展。这包括求解最稠密上模集问题的第一个强多项式时间算法和求解任意节点权图中的稠密子图发现问题的基于流的精确算法。我们在几个基于 Web 的数据分析任务中演示了我们的算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Densest+Subhypergraph:+Negative+Supermodular+Functions+and+Strongly+Localized+Methods)|0|
|[Towards Deeper Understanding of PPR-based Embedding Approaches: A Topological Perspective](https://doi.org/10.1145/3589334.3645663)|Xingyi Zhang, Zixuan Weng, Sibo Wang||Node embedding learns low-dimensional vectors for nodes in the graph. Recent state-of-the-art embedding approaches take Personalized PageRank (PPR) as the proximity measure and factorize the PPR matrix or its adaptation to generate embeddings. However, little previous work analyzes what information is encoded by these approaches, and how the information correlates with their superb performance in downstream tasks. In this work, we first show that state-of-the-art embedding approaches that factorize a PPR-related matrix can be unified into a closed-form framework. Then, we study whether the embeddings generated by this strategy can be inverted to better recover the graph topology information than random-walk based embeddings. To achieve this, we propose two methods for recovering graph topology via PPR-based embeddings, including the analytical method and the optimization method. Extensive experimental results demonstrate that the embeddings generated by factorizing a PPR-related matrix maintain more topological information, such as common edges and community structures, than that generated by random walks, paving a new way to systematically comprehend why PPR-based node embedding approaches outperform random walk-based alternatives in various downstream tasks. To the best of our knowledge, this is the first work that focuses on the interpretability of PPR-based node embedding approaches.|节点嵌入学习图中节点的低维向量。最新的嵌入方法采用个性化 PageRank (PPR)作为邻近度量，对 PPR 矩阵或其适应性进行因子分解以生成嵌入。然而，以前的工作很少分析这些方法编码的信息，以及这些信息如何与它们在下游任务中的卓越性能相关联。在这项工作中，我们首先表明，国家的最先进的嵌入方法，分解一个 PPR 相关的矩阵可以统一成一个封闭的形式框架。然后，研究该策略生成的嵌入能否比基于随机游走的嵌入更好地恢复图的拓扑信息。为此，我们提出了两种基于 PPR 嵌入的图拓扑恢复方法，包括解析法和优化法。大量的实验结果表明，PPR 相关矩阵分解产生的嵌入比随机游走产生的嵌入保持了更多的拓扑信息，如共同边和社区结构，为系统地理解为什么 PPR 节点嵌入方法在各种下游任务中优于随机游走方法提供了新的途径。据我们所知，这是第一个重点研究基于 PPR 的节点嵌入方法的可解释性的工作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Deeper+Understanding+of+PPR-based+Embedding+Approaches:+A+Topological+Perspective)|0|
|[Globally Interpretable Graph Learning via Distribution Matching](https://doi.org/10.1145/3589334.3645674)|Yi Nian, Yurui Chang, Wei Jin, Lu Lin||Graph neural networks (GNNs) have emerged as a powerful model to capture critical graph patterns. Instead of treating them as black boxes in an end-to-end fashion, attempts are arising to explain the model behavior. Existing works mainly focus on local interpretation to reveal the discriminative pattern for each individual instance, which however cannot directly reflect the high-level model behavior across instances. To gain global insights, we aim to answer an important question that is not yet well studied: how to provide a global interpretation for the graph learning procedure? We formulate this problem as globally interpretable graph learning, which targets on distilling high-level and human-intelligible patterns that dominate the learning procedure, such that training on this pattern can recover a similar model. As a start, we propose a novel model fidelity metric, tailored for evaluating the fidelity of the resulting model trained on interpretations. Our preliminary analysis shows that interpretative patterns generated by existing global methods fail to recover the model training procedure. Thus, we further propose our solution, Graph Distribution Matching (GDM), which synthesizes interpretive graphs by matching the distribution of the original and interpretive graphs in the GNN's feature space as its training proceeds, thus capturing the most informative patterns the model learns during training. Extensive experiments on graph classification datasets demonstrate multiple advantages of the proposed method, including high model fidelity, predictive accuracy and time efficiency, as well as the ability to reveal class-relevant structure.|图形神经网络(GNN)已经成为一种捕获关键图形模式的强大模型。不是以端到端的方式将它们视为黑盒，而是尝试解释模型行为。现有的工作主要集中在局部解释上，以揭示每个实例的区分模式，但不能直接反映跨实例的高层模型行为。为了获得全局性的见解，我们的目标是回答一个尚未得到很好研究的重要问题: 如何为图形学习过程提供一个全局性的解释？我们把这个问题表述为全局可解释的图学习，其目标是提取主导学习过程的高层次和人类可理解的模式，这样对这种模式的训练可以恢复类似的模型。作为一个开始，我们提出了一个新的模型保真度量，专为评估保真度的结果模型训练的解释。我们的初步分析表明，现有的全局方法产生的解释模式不能恢复模型训练过程。因此，我们进一步提出了我们的解决方案，图分布匹配(GDM) ，它通过在 GNN 的特征空间中匹配原始图和解释图的分布来合成解释图，从而捕获模型在训练过程中学习到的最具信息量的模式。在图形分类数据集上的大量实验表明，该方法具有模型保真度高、预测精度高、时间效率高、能够揭示类相关结构等优点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Globally+Interpretable+Graph+Learning+via+Distribution+Matching)|0|
|[Retention Depolarization in Recommender System](https://doi.org/10.1145/3589334.3645485)|Xiaoying Zhang, Hongning Wang, Yang Liu||Repeated risk minimization is a popular choice in real-world recommender systems driving their recommendation algorithms to adapt to user preferences and trends. However, numerous studies have shown that it exacerbates retention disparities among user groups, resulting in polarization within the user population. Given the primary objective of improving long-term user engagement in most industrial recommender systems and the significant commercial benefits from a diverse user population, enforcing retention fairness across user population is therefore crucial. Nonetheless, this goal is highly challenging due to the unknown dynamics of user retention (e.g., when a user would abandon the system) and the simultaneous aim to maximize the experience of every user. In this paper, we propose ReFair, the first computational framework that continuously improves recommendation algorithms while ensuring long-term retention fairness in the entire user population. ReFair alternates between environment learning (i.e., estimate the user retention dynamics) and fairness constrained policy improvement with respect to the estimated environment, while effectively handling uncertainties in the estimation. Our solution provides strong theoretical guarantees for long-term recommendation performance and retention fairness violation. Empirical experiments on two real-world recommendation datasets also demonstrate its effectiveness in realizing these two goals.|在现实推荐系统中，重复风险最小化是一种流行的选择，它驱动推荐算法来适应用户的偏好和趋势。然而，许多研究表明，它加剧了用户群体之间的保留差异，导致用户群体内的两极分化。鉴于改善大多数工业推荐系统的长期用户参与的主要目标以及来自不同用户群体的重大商业利益，因此，在所有用户群体中实行保留公平是至关重要的。尽管如此，由于用户保持的未知动态(例如，当用户将放弃系统)和同时目标最大化每个用户的体验，这个目标是非常具有挑战性的。在本文中，我们提出了第一个计算框架 ReFair，它不断改进推荐算法，同时确保整个用户群的长期保留公平性。ReFair 在环境学习(即，估计用户保留动态)和公平约束的策略改进之间轮流对估计的环境进行改进，同时有效地处理估计中的不确定性。我们的解决方案为长期推荐性能和违反保留公平性提供了强有力的理论保证。在两个真实世界的推荐数据集上的实验也证明了该方法实现这两个目标的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retention+Depolarization+in+Recommender+System)|0|
|[Uncovering the Hidden Data Costs of Mobile YouTube Video Ads](https://doi.org/10.1145/3589334.3645496)|Emaan Atique, Saad Sher Alam, Harris Ahmad, Ihsan Ayyub Qazi, Zafar Ayyub Qazi||Popular video streaming platforms attract a large number of global marketers who use the platform to advertise their services. While benefiting platforms and advertisers, users are burdened with the costs of advertisements. Users not only pay for these ads with their invested time and personal information, but also through a substantial amount of data translating into direct financial cost. The financial cost becomes even more pronounced in developing countries, where the cost of mobile broadband can be disproportionately high relative to average income levels. In this paper, we perform the first independent and empirical analysis of the data costs of mobile video ads on YouTube, the most popular video platform, from the users' perspective. To do so, we collect and analyze a data set of over 46,000 YouTube video ads. We find that streaming video ads have multiplelatent andavoidable sources of data wastage, which can lead to excessive data consumption by users. We also conduct an affordability analysis to quantify the overall impact of data wastage and reveal the specific data costs per country associated with these losses. Our findings highlight the need for video platform providers, such as YouTube, to minimize data wastage linked to ads, to make their services more affordable and inclusive.|流行的视频流媒体平台吸引了大量的全球营销人员，他们利用这个平台为自己的服务做广告。在惠及平台和广告商的同时，用户还要承担广告费用。用户不仅用他们投入的时间和个人信息来支付这些广告，而且还通过大量的数据转化为直接的财务成本。在发展中国家，移动宽带的费用相对于平均收入水平可能高得不成比例，因此财务成本变得更加突出。本文首先从用户的角度对最流行的视频平台 YouTube 上的移动视频广告的数据成本进行了独立的实证分析。为此，我们收集并分析了超过46000个 YouTube 视频广告的数据集。我们发现流媒体视频广告具有多重且可避免的数据浪费来源，这会导致用户过度消耗数据。我们还进行了负担能力分析，以量化数据浪费的总体影响，并揭示与这些损失相关的每个国家的具体数据成本。我们的研究结果强调视频平台提供商，如 YouTube，需要尽量减少与广告相关的数据浪费，使他们的服务更便宜，更具包容性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncovering+the+Hidden+Data+Costs+of+Mobile+YouTube+Video+Ads)|0|
|[Perceptions in Pixels: Analyzing Perceived Gender and Skin Tone in Real-world Image Search Results](https://doi.org/10.1145/3589334.3645666)|Jeffrey L. Gleason, Avijit Ghosh, Ronald E. Robertson, Christo Wilson||The results returned by image search engines have the power to shape peoples' perceptions about social groups. Existing work on image search engines leverages hand-selected queries for occupations like "doctor" and "engineer" to quantify racial and gender bias in search results. We complement this work by analyzing peoples' real-world image search queries and measuring the distributions of perceived gender, skin tone, and age in their results. We collect 54,070 unique image search queries and analyze 1,481 open-ended people queries (i.e. not queries for named entities) from a representative sample of 643 US residents. For each query, we analyze the top 15 results returned on both Google and Bing Images. Analysis of real-world image search queries produces multiple insights. First, less than 5% of unique queries are open-ended people queries. Second, fashion queries are, by far, the most common category of open-ended people queries, accounting for over 30% of the total. Third, the modal skin tone on the Monk Skin Tone scale is two out of ten (the second lightest) for images from both search engines. Finally, we observe a bias against older people: eleven of our top fifteen query categories have a median age that is lower than the median age in the US.|图片搜索引擎返回的结果有能力影响人们对社会群体的看法。现有的图片搜索引擎利用手工选择的关于“医生”和“工程师”等职业的查询来量化搜索结果中的种族和性别偏见。我们通过分析人们在现实世界中的图像搜索查询以及测量结果中感知的性别、肤色和年龄的分布来补充这项工作。我们从643名美国居民的代表性样本中收集了54,070个独特的图像搜索查询，并分析了1,481个开放式人员查询(即不包括命名实体的查询)。对于每个查询，我们分析 Google 和 Bing 图片上返回的前15个结果。对真实世界图像搜索查询的分析产生了多种见解。首先，不到5% 的唯一查询是开放式人员查询。其次，到目前为止，时尚查询是最常见的开放式人员查询类别，占总数的30% 以上。第三，Monk Skin Tone 刻度上的模态肤色是两个搜索引擎图像的十分之二(第二轻)。最后，我们观察到一种对老年人的偏见: 在我们的前十一个查询类别中，有十五个的中位年龄低于美国的中位年龄。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Perceptions+in+Pixels:+Analyzing+Perceived+Gender+and+Skin+Tone+in+Real-world+Image+Search+Results)|0|
|[Reconciling the Accuracy-Diversity Trade-off in Recommendations](https://doi.org/10.1145/3589334.3645625)|Kenny Peng, Manish Raghavan, Emma Pierson, Jon M. Kleinberg, Nikhil Garg||In recommendation settings, there is an apparent trade-off between the goals of accuracy (to recommend items a user is most likely to want) and diversity (to recommend items representing a range of categories). As such, real-world recommender systems often explicitly incorporate diversity separately from accuracy. This approach, however, leaves a basic question unanswered: Why is there a trade-off in the first place? We show how the trade-off can be explained via a user's consumption constraints -- users typically only consume a few of the items they are recommended. In a stylized model we introduce, objectives that account for this constraint induce diverse recommendations, while objectives that do not account for this constraint induce homogeneous recommendations. This suggests that accuracy and diversity appear misaligned because standard accuracy metrics do not consider consumption constraints. Our model yields precise and interpretable characterizations of diversity in different settings, giving practical insights into the design of diverse recommendations.|在推荐设置中，在准确性目标(推荐用户最可能想要的项目)和多样性目标(推荐代表一系列类别的项目)之间存在明显的权衡。因此，现实世界中的推荐系统通常明确地将多样性与准确性分开。然而，这种方法留下了一个基本问题没有得到解答: 为什么首先要进行权衡？我们展示了如何通过用户的消费约束来解释这种权衡——用户通常只消费推荐的几个项目。在我们引入的程式化模型中，解释这种约束的目标会产生不同的建议，而不解释这种约束的目标会产生同质的建议。这表明准确性和多样性似乎不一致，因为标准的准确性指标没有考虑消费约束。我们的模型产生了不同环境下多样性的精确和可解释的特征，为不同建议的设计提供了实用的见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reconciling+the+Accuracy-Diversity+Trade-off+in+Recommendations)|0|
|[MileCut: A Multi-view Truncation Framework for Legal Case Retrieval](https://doi.org/10.1145/3589334.3645349)|Fuda Ye, Shuangyin Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MileCut:+A+Multi-view+Truncation+Framework+for+Legal+Case+Retrieval)|0|
|[Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks](https://doi.org/10.1145/3589334.3645363)|Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, TatSeng Chua||Making the content generated by Large Language Model (LLM), accurate, credible and traceable is crucial, especially in complex knowledge-intensive tasks that require multi-step reasoning and each step needs knowledge to solve. Retrieval-augmented generation is good potential to solve this problem. However, where and how to introduce Information Retrieval (IR) to LLM is a big challenge. Previous work has the problems that wrong knowledge retrieved by IR misleads the LLM and interaction between IR and LLM breaks the reasoning chain of LLM. This paper proposes a novel framework named Search-in-the-Chain (SearChain) for the interaction between LLM and IR to solve the challenges. First, LLM generates the reasoning chain named Chain-of-Query (CoQ) where each node consists of an IR-oriented query-answer pair. Second, IR verifies the answer of each node of CoQ. It corrects the answer that is not consistent with the retrieved information when IR gives high confidence, which improves the credibility. Third, LLM can indicate its missing knowledge in CoQ and rely on IR to provide this knowledge to LLM. These operations improve the accuracy in terms of reasoning and knowledge. Finally, SearChain generates the reasoning process and marks references to supporting documents for each reasoning step, which improves traceability. Interaction with IR in SearChain forms a novel reasoning path based on a tree, which enables LLM to dynamically modify the direction of reasoning. Experiments show that SearChain outperforms state-of-the-art baselines on complex knowledge-intensive tasks including multi-hop Q&A, slot filling, fact checking, and long-form Q&A.|使大语言模型(LLM)生成的内容准确、可信、可追溯是关键，特别是在需要多步推理、每一步都需要知识求解的复杂知识密集型任务中。提取增强生成技术是解决这一问题的有力工具。然而，在何处以及如何向 LLM 引入信息检索是一个巨大的挑战。以往的研究存在的问题是，由 IR 检索出的错误知识误导了 LLM，而且 IR 与 LLM 之间的相互作用破坏了 LLM 的推理链。本文提出了一个新的框架，称为搜索在链(搜索链)之间的交互 LLM 和信息检索，以解决这一挑战。首先，LLM 生成名为查询链(Chain-of-Query，CoQ)的推理链，其中每个节点由一个面向 IR 的查询-应答对组成。其次，IR 验证 CoQ 的每个节点的答案。当信息检索的可信度较高时，它可以纠正与检索到的信息不一致的答案，从而提高了信息的可信度。第三，LLM 可以在 CoQ 中表示其缺失的知识，并依靠 IR 向 LLM 提供这些知识。这些操作提高了推理和知识的准确性。最后，SearChain 生成推理过程，并在每个推理步骤中标记对支持文档的引用，从而提高了可追溯性。在 SearChain 中，与 IR 的交互形成了一种新的基于树的推理路径，使 LLM 能够动态地修改推理的方向。实验表明，SearChain 在复杂的知识密集型任务(包括多跳问答、插槽填充、事实检查和长形式问答)中的表现优于最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Search-in-the-Chain:+Interactively+Enhancing+Large+Language+Models+with+Search+for+Knowledge-intensive+Tasks)|0|
|[Scalable and Effective Generative Information Retrieval](https://doi.org/10.1145/3589334.3645477)|Hansi Zeng, Chen Luo, Bowen Jin, Sheikh Muhammad Sarwar, Tianxin Wei, Hamed Zamani||Recent research has shown that transformer networks can be used as differentiable search indexes by representing each document as a sequence of document ID tokens. These generative retrieval models cast the retrieval problem to a document ID generation problem for each query. Despite their elegant design, existing generative retrieval models only perform well on artificially-constructed and small-scale collections. This paper represents an important milestone in generative retrieval research by showing that generative retrieval models can be trained to perform effectively on large-scale standard retrieval benchmarks. In more detail, we propose RIPOR- an optimization framework for generative retrieval that is designed based on two often-overlooked fundamental design considerations. First, RIPOR introduces a novel prefix-oriented ranking optimization algorithm for accurate estimation of relevance score during sequential document ID generation. Second, RIPOR constructs document IDs based on the relevance associations between queries and documents. Evaluation on MSMARCO and TREC Deep Learning Track reveals that RIPOR surpasses state-of-the-art generative retrieval models by a large margin (e.g., 30.5% MRR improvements on MS MARCO Dev Set).|最近的研究表明，通过将每个文档表示为一系列文档 ID 令牌，变压器网络可以作为可微搜索索引。这些生成检索模型将检索问题转换为每个查询的文档 ID 生成问题。尽管其设计优雅，现有的生成检索模型只能在人工构建的小规模集合上表现良好。本文通过对生成检索模型的训练，使其能够在大规模的标准检索基准上有效地执行，从而标志着生成检索研究的一个重要里程碑。更详细地说，我们提出了 RIPOR-一个生成检索的优化框架，它是基于两个经常被忽视的基本设计考虑而设计的。首先，RIPOR 引入了一种新的面向前缀的排序优化算法，用于准确估计序列文档 ID 生成过程中的相关度得分。其次，RIPOR 基于查询和文档之间的相关性关联构造文档 ID。对 MSMARCO 和 TREC 深度学习跟踪的评估显示，RIPOR 大大超过了最先进的生成检索模型(例如，MS MARCO Dev Set 的30.5% MRR 改进)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+and+Effective+Generative+Information+Retrieval)|0|
|[Metacognitive Retrieval-Augmented Large Language Models](https://doi.org/10.1145/3589334.3645481)|Yujia Zhou, Zheng Liu, Jiajie Jin, JianYun Nie, Zhicheng Dou||Retrieval-augmented generation have become central in natural language processing due to their efficacy in generating factual content. While traditional methods employ single-time retrieval, more recent approaches have shifted towards multi-time retrieval for multi-hop reasoning tasks. However, these strategies are bound by predefined reasoning steps, potentially leading to inaccuracies in response generation. This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition. Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes. By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective reasoning abilities. Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes them. Empirical evaluations show that MetaRAG significantly outperforms existing methods.|提取增强生成由于其生成事实内容的功效，已成为自然语言处理的核心。传统的检索方法采用单次检索，而最近的检索方法已经转向多跳推理任务的多次检索。然而，这些策略受到预先定义的推理步骤的约束，可能导致响应生成的不准确性。本文介绍了一种将检索增强生成过程与元认知相结合的方法 MetaRAG。元认知从认知心理学的角度出发，允许一个实体对其认知过程进行自我反思和批判性评价。通过集成这一点，MetaRAG 使模型能够监视、评估和规划其响应策略，增强其内省推理能力。通过三步元认知调节管道，该模型可以识别初始认知反应中的不足并加以修正。经验性评估表明，MetaRAG 的性能明显优于现有方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Metacognitive+Retrieval-Augmented+Large+Language+Models)|0|
|[Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy](https://doi.org/10.1145/3589334.3645512)|SeongKu Kang, Shivam Agarwal, Bowen Jin, Dongha Lee, Hwanjo Yu, Jiawei Han||Document retrieval has greatly benefited from the advancements of large-scale pre-trained language models (PLMs). However, their effectiveness is often limited in theme-specific applications for specialized areas or industries, due to unique terminologies, incomplete contexts of user queries, and specialized search intents. To capture the theme-specific information and improve retrieval, we propose to use a corpus topical taxonomy, which outlines the latent topic structure of the corpus while reflecting user-interested aspects. We introduce ToTER (Topical Taxonomy Enhanced Retrieval) framework, which identifies the central topics of queries and documents with the guidance of the taxonomy, and exploits their topical relatedness to supplement missing contexts. As a plug-and-play framework, ToTER can be flexibly employed to enhance various PLM-based retrievers. Through extensive quantitative, ablative, and exploratory experiments on two real-world datasets, we ascertain the benefits of using topical taxonomy for retrieval in theme-specific applications and demonstrate the effectiveness of ToTER.|大规模预先培训语言模型(plm)的发展极大地促进了文献检索的发展。然而，由于独特的术语、用户查询的不完整上下文以及专门的搜索意图，它们在专门领域或行业的特定主题应用程序中的有效性往往受到限制。为了获取特定主题的信息，提高检索效率，我们提出了一种语料库主题分类法，它在反映用户感兴趣的方面的同时，勾勒出语料库的潜在主题结构。本文介绍了 ToTER (Topical Taxonomy Advanced Retrieval，主题分类增强检索)框架，该框架在分类学的指导下识别查询和文档的中心主题，并利用它们的主题相关性来补充缺失的上下文。作为一个即插即用的框架，ToTER 可以灵活地用于增强各种基于 PLM 的检索器。通过对两个实际数据集进行广泛的定量、消融和探索性实验，我们确定了在特定主题应用中使用主题分类法进行检索的好处，并证明了 ToTER 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Retrieval+in+Theme-specific+Applications+using+a+Corpus+Topical+Taxonomy)|0|
|[(In)Security of File Uploads in Node.js](https://doi.org/10.1145/3589334.3645342)|Harun Oz, Abbas Acar, Ahmet Aris, Güliz Seray Tuncay, Amin Kharraz, A. Selcuk Uluagac||File upload is a critical feature incorporated by a myriad of web applications in an effort to enable users to share and manage their files conveniently. It has been used in many useful services such as file-sharing and social media. While file upload is an essential component of web applications, the lack of rigorous checks on the file name, type, and content of the uploaded files can result in security issues, often referred to as Unrestricted File Upload (UFU). In this study, we analyze the (in)security of popular file upload libraries and real-world applications in the Node.js ecosystem. To automate our analysis, we propose and implement NodeSEC- a tool designed to analyze file upload insecurities in Node.js applications and libraries. NodeSEC generates unique payloads and thoroughly evaluates the application's file upload security against 13 distinct UFU-type attacks. Utilizing NodeSEC, we analyze the most popular file upload libraries and real-world applications in the Node.js ecosystem. Our analysis results reveal that some real-world web applications are vulnerable to UFU attacks and disclose serious security bugs in file upload libraries. As of this writing, we received 19 CVEs and two US-CERT cases for the security issues that we reported. Our findings provide strong evidence that dynamic features of Node.js applications introduce security shortcomings and that web developers should be cautious when implementing file upload features in their applications. Finally, combining our responsible disclosure experience and root cause analysis, we identified the main causes of significant security weaknesses in file uploads in Node.js.|文件上传是许多网络应用程序的一个重要特性，它使用户能够方便地共享和管理他们的文件。它已经被用于许多有用的服务，如文件共享和社会媒体。虽然文件上传是 Web 应用程序的重要组成部分，但是缺乏对上传文件的文件名、类型和内容的严格检查可能会导致安全问题，通常被称为无限制文件上传(UFU)。在本研究中，我们分析了 Node.js 生态系统中流行的文件上传库和实际应用程序的(In)安全性。为了使我们的分析自动化，我们提出并实现了 NodeSEC ——一个用于分析 Node.js 应用程序和库中文件上传不安全性的工具。NodeSEC 生成唯一的有效负载，并对应用程序的文件上传安全性进行全面评估，以抵御13种不同的 UFU 类型的攻击。利用 NodeSEC，我们分析了 Node.js 生态系统中最流行的文件上传库和实际应用程序。我们的分析结果表明，一些现实世界的网络应用程序是脆弱的 UFU 攻击，并揭示了严重的安全漏洞的文件上传库。在撰写本文时，我们收到了19个 CVE 和两个 US-CERT 案例，用于我们报告的安全问题。我们的研究结果有力地证明了 Node.js 应用程序的动态特性引入了安全缺陷，并且 web 开发人员在应用程序中实现文件上传特性时应该谨慎。最后，结合我们负责任的披露经验和根本原因分析，我们找出了 Node.js 文件上传中存在重大安全缺陷的主要原因。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=(In)Security+of+File+Uploads+in+Node.js)|0|
|[IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion](https://doi.org/10.1145/3589334.3645361)|Jiapu Wang, Zheng Cui, Boyue Wang, Shirui Pan, Junbin Gao, Baocai Yin, Wen Gao||Temporal Knowledge Graphs (TKGs) incorporate a temporal dimension, allowing for a precise capture of the evolution of knowledge and reflecting the dynamic nature of the real world. Typically, TKGs contain complex geometric structures, with various geometric structures interwoven. However, existing Temporal Knowledge Graph Completion (TKGC) methods either model TKGs in a single space or neglect the heterogeneity of different curvature spaces, thus constraining their capacity to capture these intricate geometric structures. In this paper, we propose a novel Integrating Multi-curvature shared and specific Embedding (IME) model for TKGC tasks. Concretely, IME models TKGs into multi-curvature spaces, including hyperspherical, hyperbolic, and Euclidean spaces. Subsequently, IME incorporates two key properties, namely space-shared property and space-specific property. The space-shared property facilitates the learning of commonalities across different curvature spaces and alleviates the spatial gap caused by the heterogeneous nature of multi-curvature spaces, while the space-specific property captures characteristic features. Meanwhile, IME proposes an Adjustable Multi-curvature Pooling (AMP) approach to effectively retain important information. Furthermore, IME innovatively designs similarity, difference, and structure loss functions to attain the stated objective. Experimental results clearly demonstrate the superior performance of IME over existing state-of-the-art TKGC models.|时间知识图(TKGs)包含了一个时间维度，允许精确地捕捉知识的演变，并反映真实世界的动态性质。通常，TKG 包含复杂的几何结构，各种几何结构交织在一起。然而，现有的时态知识图完成(TKGC)方法要么在单个空间中对 TKG 进行建模，要么忽略了不同曲率空间的异质性，从而限制了它们捕获这些复杂几何结构的能力。针对 TKGC 任务，提出了一种新的集成多曲率共享和特定嵌入(IME)模型。具体而言，IME 将 TKG 模型分解为多曲率空间，包括超球面空间、双曲空间和欧氏空间。随后，IME 合并了两个关键属性，即空间共享属性和空间特定属性。空间共享属性有利于不同曲率空间之间的共性学习，缓解了多曲率空间异质性所造成的空间差距，而空间特定属性则捕捉特征。同时，IME 提出了一种可调多曲率池(AMP)方法来有效地保留重要信息。此外，IME 创新地设计了相似性、差异性和结构损失函数来达到既定的目标。实验结果清楚地表明，IME 的性能优于现有的最先进的 TKGC 模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IME:+Integrating+Multi-curvature+Shared+and+Specific+Embedding+for+Temporal+Knowledge+Graph+Completion)|0|
|[Poisoning Attack on Federated Knowledge Graph Embedding](https://doi.org/10.1145/3589334.3645422)|Enyuan Zhou, Song Guo, Zhixiu Ma, Zicong Hong, Tao Guo, Peiran Dong||Federated Knowledge Graph Embedding (FKGE) is an emerging collaborative learning technique for deriving expressive representations (i.e., embeddings) from client-maintained distributed knowledge graphs (KGs). However, poisoning attacks in FKGE, which lead to biased decisions by downstream applications, remain unexplored. This paper is the first work to systematize the risks of FKGE poisoning attacks, from which we develop a novel framework for poisoning attacks that force the victim client to predict specific false facts. Unlike centralized KGEs, FKGE maintains KGs locally, making direct injection of poisoned data challenging. Instead, attackers must create poisoned data without access to the victim's KG and inject it indirectly through FKGE aggregation. Specifically, to create poisoned data, the attacker first infers the targeted relations in the victim's local KG via a new KG component inference attack. Then, to accurately mislead the victim's embeddings via aggregation, the attacker locally trains a shadow model using the poisoned data and uses an optimized dynamic poisoning scheme to adjust the model and generate progressive poisoned updates. Our experimental results demonstrate the attack's effectiveness, achieving a remarkable success rate on various KGE models (e.g., 100% on TransE with WN18RR) while keeping the original task's performance nearly unchanged.|联邦知识图嵌入(FKGE)是一种新兴的合作学习技术，用于从客户维护的分布式知识图(KGs)中获得表达式表示(即嵌入)。然而，在 FKGE 的中毒攻击，导致下游应用程序的偏见决策，仍然没有探索。本文首次对 FKGE 中毒攻击的风险进行了系统化分析，并在此基础上提出了一个新的中毒攻击框架，迫使受害者客户预测特定的虚假事实。与集中的 KGE 不同，FKGE 在本地维护 KGs，这使得直接注入有毒数据变得很困难。相反，攻击者必须在无法访问受害者的 KG 的情况下创建有毒数据，并通过 FKGE 聚合间接注入这些数据。具体来说，为了创建有毒数据，攻击者首先通过一个新的 KG 组件推断攻击来推断受害者的本地 KG 中的目标关系。然后，为了通过聚合准确地误导受害者的嵌入，攻击者使用中毒数据在本地训练一个阴影模型，并使用一个优化的动态中毒方案来调整模型和生成渐进的中毒更新。我们的实验结果证明了该攻击的有效性，在不同的 KGE 模型(例如，100% 的 TransE 和 WN18RR)上获得了显著的成功率，同时保持了原始任务的性能几乎不变。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Poisoning+Attack+on+Federated+Knowledge+Graph+Embedding)|0|
|[ReliK: A Reliability Measure for Knowledge Graph Embeddings](https://doi.org/10.1145/3589334.3645430)|Maximilian K. Egger, Wenyue Ma, Davide Mottin, Panagiotis Karras, Ilaria Bordino, Francesco Gullo, Aris Anagnostopoulos||Can we assess a priori how well a knowledge graph embedding will perform on a specific downstream task and in a specific part of the knowledge graph? Knowledge graph embeddings (KGEs) represent entities (e.g., "da Vinci," "Mona Lisa") and relationships (e.g., "painted") of a knowledge graph (KG) as vectors. KGEs are generated by optimizing an embedding score, which assesses whether a triple (e.g., "da Vinci," "painted," "Mona Lisa") exists in the graph. KGEs have been proven effective in a variety of web-related downstream tasks, including, for instance, predicting relationships among entities. However, the problem of anticipating the performance of a given KGE in a certain downstream task and locally to a specific individual triple, has not been tackled so far. In this paper, we fill this gap with ReliK, a Reliability measure for KGEs. ReliK relies solely on KGE embedding scores, is task- and KGE-agnostic, and requires no further KGE training. As such, it is particularly appealing for semantic web applications which call for testing multiple KGE methods on various parts of the KG and on each individual downstream task. Through extensive experiments, we attest that ReliK correlates well with both common downstream tasks, such as tail or relation prediction and triple classification, as well as advanced downstream tasks, such as rule mining and question answering, while preserving locality.|我们能否先验地评估知识图表嵌入在特定的下游任务和知识图表的特定部分中的表现如何？知识图嵌入(KGEs)表示知识图的实体(如“达芬奇”、“蒙娜丽莎”)和关系(如“画”)作为向量。KGE 是通过优化嵌入分数生成的，嵌入分数评估图中是否存在三元组(例如，“达芬奇”、“绘画”、“蒙娜丽莎”)。KGEs 已被证明在各种与网络相关的下游任务中是有效的，包括，例如，预测实体之间的关系。然而，预测某一知识专长在某一下游任务中的表现以及局部地区对某一特定个体三重性的表现这一问题迄今尚未得到解决。在本文中，我们填补这一空白与 ReliK，一个可靠性测度的 KGE。ReliK 完全依赖于 KGE 嵌入分数，是任务和 KGE 不可知的，不需要进一步的 KGE 培训。因此，它对语义 Web 应用程序特别有吸引力，这些应用程序需要在 KG 的各个部分和每个单独的下游任务上测试多个 KGE 方法。通过大量的实验，我们证明了 ReliK 与常见的下游任务(如尾部或关系预测和三重分类)以及高级的下游任务(如规则挖掘和问题回答)都有很好的相关性，同时保留了局部性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReliK:+A+Reliability+Measure+for+Knowledge+Graph+Embeddings)|0|
|[A Method for Assessing Inference Patterns Captured by Embedding Models in Knowledge Graphs](https://doi.org/10.1145/3589334.3645505)|Narayanan Asuri Krishnan, Carlos R. Rivero||Various methods embed knowledge graphs with the goal of predicting missing edges. Inference patterns are the logical relationships that occur in a graph. To make proper predictions, models trained by embedding methods must capture inference patterns. There are several theoretical analyses studying pattern-capturing capabilities. Unfortunately, these analyses are challenging and many embedding methods remain unstudied. Also, they do not quantify how accurately a pattern is captured in real-world datasets. Existing empirical studies have studied a small subset of simple inference patterns, and the analysis methods used have varied depending on the models evaluated. In this paper, we present a model-agnostic method to empirically quantify how patterns are captured by trained embedding models. We collect the most plausible predictions to form a new graph, and use it to globally assess pattern-capturing capabilities. For a given pattern, we study positive and negative evidence, i.e., edges that the pattern deems correct and incorrect based on the partial completeness assumption. As far as we know, it is the first time negative evidence is analyzed. Our experiments show that several models effectively capture the positive evidence of inference patterns. However, the performance is poor for negative evidence, which entails that models fail to learn the partial completeness assumption. We also identify new inference patterns not studied before. Surprisingly, models generally achieve better performance in these new patterns that we introduce.|各种方法嵌入知识图，目的是预测缺失边缘。推理模式是图中出现的逻辑关系。为了做出正确的预测，通过嵌入方法训练的模型必须捕获推理模式。关于模式捕获能力的理论分析主要有以下几种。不幸的是，这些分析是具有挑战性的，许多嵌入方法仍然没有研究。此外，它们不能量化在真实世界的数据集中捕获模式的准确程度。现有的实证研究已经研究了一小部分简单的推理模式，所使用的分析方法根据所评估的模型而有所不同。在本文中，我们提出了一个模型无关的方法，以经验量化如何捕获模式训练嵌入模型。我们收集最合理的预测，形成一个新的图表，并使用它来全球评估模式捕获能力。对于一个给定的模式，我们研究积极和消极的证据，即，边缘模式认为正确和不正确的基础上的部分完备性假设。据我们所知，这是第一次对负面证据进行分析。我们的实验表明，几个模型有效地捕获了推理模式的积极证据。然而，否定证据的性能较差，这意味着模型不能学习部分完备性假设。我们还发现了以前没有研究过的新的推理模式。令人惊讶的是，模型通常在我们引入的这些新模式中获得更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Method+for+Assessing+Inference+Patterns+Captured+by+Embedding+Models+in+Knowledge+Graphs)|0|
|[Fact Embedding through Diffusion Model for Knowledge Graph Completion](https://doi.org/10.1145/3589334.3645451)|Xiao Long, Liansheng Zhuang, Aodi Li, Houqiang Li, Shafei Wang||Knowledge graph embedding (KGE) is an efficient and scalable method for knowledge graph completion tasks. Existing KGE models typically map entities and relations into a unified continuous vector space and define a score function to capture the connectivity patterns among the elements (entities and relations) of facts. The score on a fact measures its plausibility in a knowledge graph (KG). However, since the connectivity patterns are very complex in a real knowledge graph, it is difficult to define an explicit and efficient score function to capture them, which also limits their performance. This paper argues that plausible facts in a knowledge graph come from a distribution in the low-dimensional fact space. Inspired by this insight, this paper proposes a novel framework called Fact Embedding through Diffusion Model (FDM) to address the knowledge graph completion task. Instead of defining a score function to measure the plausibility of facts in a knowledge graph, this framework directly learns the distribution of plausible facts from the known knowledge graph and casts the entity prediction task into the conditional fact generation task. Specifically, we concatenate the elements embedding in a fact as a whole and take it as input. Then, we introduce a Conditional Fact Denoiser to learn the reverse denoising diffusion process and generate the target fact embedding from noised data. Extensive experiments demonstrate that FDM significantly outperforms existing state-of-the-art methods in three benchmark datasets.|知识图嵌入(KGE)是一种高效、可扩展的知识图完成任务方法。现有的 KGE 模型通常将实体和关系映射到一个统一的连续向量空间中，并定义一个评分函数来捕获事实要素(实体和关系)之间的连通性模式。事实的得分在知识图(KG)中衡量事实的合理性。然而，由于连通性模式在真实的知识图中是非常复杂的，因此很难定义一个明确有效的评分函数来捕获它们，这也限制了它们的性能。本文认为，知识图中的似然事实来自于低维事实空间中的分布。受此启发，本文提出了一种基于扩散模型的事实嵌入(FDM)框架来解决知识图的完成任务。该框架不需要定义一个评分函数来度量知识图中事实的合理性，而是直接从已知的知识图中学习合理事实的分布，并将实体预测任务转化为条件事实生成任务。具体来说，我们将嵌入在事实中的元素作为一个整体连接起来，并将其作为输入。然后引入条件事实去噪器学习反向去噪扩散过程，从含噪数据中生成目标事实。大量的实验表明，FDM 在三个基准数据集中的性能明显优于现有的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fact+Embedding+through+Diffusion+Model+for+Knowledge+Graph+Completion)|0|
|[HaSa: Hardness and Structure-Aware Contrastive Knowledge Graph Embedding](https://doi.org/10.1145/3589334.3645564)|Honggen Zhang, June Zhang, Igor Molybog||We consider a contrastive learning approach to knowledge graph embedding (KGE) via InfoNCE. For KGE, efficient learning relies on augmenting the training data with negative triples. However, most KGE works overlook the bias from generating the negative triples-false negative triples (factual triples missing from the knowledge graph). We argue that the generation of high-quality (i.e., hard) negative triples might lead to an increase in false negative triples. To mitigate the impact of false negative triples during the generation of hard negative triples, we propose the Hardness and Structure-aware (\textbf{HaSa}) contrastive KGE method, which alleviates the effect of false negative triples while generating the hard negative triples. Experiments show that HaSa improves the performance of InfoNCE-based KGE approaches and achieves state-of-the-art results in several metrics for WN18RR datasets and competitive results for FB15k-237 datasets compared to both classic and pre-trained LM-based KGE methods.|提出了一种基于 InfoNCE 的知识图嵌入对比学习方法。对于 KGE，有效的学习依赖于用负三元组增加训练数据。然而，大多数 KGE 作品忽略了产生负三元组的偏差-假负三元组(知识图中缺失的事实三元组)。我们认为产生高质量(例如，硬)负三元组可能导致增加假负三元组。为了减轻硬负三元组生成过程中假负三元组的影响，提出了硬度和结构感知(textbf { HaSa })对比 KGE 方法，该方法在生成硬负三元组的同时，减轻了假负三元组的影响。实验表明，与经典的和预先训练的基于 LM 的 KGE 方法相比，HaSa 提高了基于 InfoNCE 的 KGE 方法的性能，在 WN18RR 数据集的几个度量指标和 FB15k-237数据集的竞争结果方面取得了最先进的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HaSa:+Hardness+and+Structure-Aware+Contrastive+Knowledge+Graph+Embedding)|0|
|[Bridging the Space Gap: Unifying Geometry Knowledge Graph Embedding with Optimal Transport](https://doi.org/10.1145/3589334.3645565)|Yuhan Liu, Zelin Cao, Xing Gao, Ji Zhang, Rui Yan||Knowledge Graph Embedding (KGE) is a critical field aiming to transform the elements of knowledge graphs (KGs) into continuous spaces, offering great potential for structured data representation. In contemporary KGE research, the utilization of either hyperbolic or Euclidean space for knowledge graph Embedding is a common practice. However, knowledge graphs encompass diverse geometric data structures, including chains and hierarchies, whose hybrid nature exceeds the capacity of a single embedding space to capture effectively. This paper introduces a novel and highly effective approach called Unified Geometry Knowledge Graph Embedding (UniGE) to address the challenge of representing diverse geometric data in KGs. UniGE stands out as a novel KGE method that seamlessly integrates KGE in both Euclidean and hyperbolic geometric spaces. We introduce an embedding alignment method and fusion strategy, which harnesses optimal transport techniques and the Wasserstein barycenter method. Furthermore, we offer a comprehensive theoretical analysis to substantiate the superiority of our approach, as evident from a more robust error bound. To substantiate the strength of UniGE, we conducted comprehensive experiments on three benchmark datasets. The results consistently demonstrate that UniGE outperforms state-of-the-art methods, aligning with the conclusions drawn from our theoretical analysis.|知识图嵌入(KGE)是将知识图元素转化为连续空间的关键领域，为结构化数据表示提供了巨大的潜力。在当代 KGE 研究中，利用双曲空间或欧氏空间嵌入知识图是一种常见的实践。然而，知识图包含不同的几何数据结构，包括链和层次结构，其混合性质超过了单一嵌入空间有效捕获的能力。本文介绍了一种新颖而高效的方法——统一几何知识图嵌入(UniGE) ，以解决幼儿园中表示不同几何数据的难题。UniGE 是一种新颖的 KGE 方法，它将 KGE 无缝地集成到欧几里得空间和双曲几何空间中。本文介绍了一种融合最优传输技术和 Wasserstein 重心法的嵌入对准方法和融合策略。此外，我们提供了一个全面的理论分析，以证明我们的方法的优越性，从一个更强大的误差界。为了验证 UniGE 的强度，我们在三个基准数据集上进行了全面的实验。结果一致表明，UniGE 优于国家的最先进的方法，从我们的理论分析得出的结论一致。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bridging+the+Space+Gap:+Unifying+Geometry+Knowledge+Graph+Embedding+with+Optimal+Transport)|0|
|[Query Optimization for Ontology-Mediated Query Answering](https://doi.org/10.1145/3589334.3645567)|Wafaa El Husseini, Cheikh Brahim El Vaigh, François Goasdoué, Hélène Jaudoin||Ontology-mediated query answering (OMQA) consists in asking database queries on knowledge bases (KBs); a KB is a set of facts called the KB's database, which is described by domain knowledge called the KB's ontology. A widely-investigated OMQA technique is FO-rewriting: every query asked on a KB is reformulated w.r.t. the KB's ontology, so that its answers are computed by the relational evaluation of the query reformulation on the KB's database. Crucially, because FO-rewriting compiles the domain knowledge relevant to queries into their reformulations, query reformulations may be complex and their optimization is the crux of efficiency. We devise a novel optimization framework for a large set of OMQA settings that enjoy FO-rewriting: conjunctive queries, i.e., the core select-project-join queries, asked on KBs expressed using datalog+/-, description logics, existential rules, OWL, or RDFS. We optimize the query reformulations produced by state-of-the-art FO-rewriting algorithms by computing rapidly, with the help of a KB's database summary, simpler (contained) queries with the same answers that can be evaluated faster by RDBMSs. We show on a well-established OMQA benchmark that time performance is significantly improved by our optimization framework in general, up to three orders of magnitude.|本体介导的查询回答(OMQA)包括在知识库(KB)上询问数据库查询; 知识库是一组称为知识库数据库的事实，由称为知识库本体的领域知识描述。一种被广泛研究的 OMQA 技术是 FO 重写(FO-rewrite) : 对知识库询问的每个查询都被重新表述为 W.r.t. 知识库的本体，因此它的答案是通过对知识库数据库上的查询重新表述的关系计算得到的。关键是，由于 FO 重写将与查询相关的领域知识编译成查询重写，因此查询重写可能比较复杂，其优化是提高查询效率的关键。我们为享受 FO 重写的大量 OMQA 设置设计了一个新颖的优化框架: 合取查询，即核心 select-project-join 查询，使用 datalog +/-、描述逻辑、存在规则、 OWL 或 RDFS 表示。我们通过快速计算优化了由最先进的 FO 重写算法产生的查询重写，在知识库数据库摘要的帮助下，更简单(包含)的查询具有相同的答案，可以由 RDBMS 更快地进行评估。我们展示了一个完善的 OMQA 基准，通过我们的优化框架，时间性能得到了显著改善，数量级最高可达3。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query+Optimization+for+Ontology-Mediated+Query+Answering)|0|
|[Query2GMM: Learning Representation with Gaussian Mixture Model for Reasoning over Knowledge Graphs](https://doi.org/10.1145/3589334.3645569)|Yuhan Wu, Yuanyuan Xu, Wenjie Zhang, Xiwei Xu, Ying Zhang||Logical query answering over Knowledge Graphs (KGs) is a fundamental yet complex task. A promising approach to achieve this is to embed queries and entities jointly into the same embedding space. Research along this line suggests that using multi-modal distribution to represent answer entities is more suitable than uni-modal distribution, as a single query may contain multiple disjoint answer subsets due to the compositional nature of multi-hop queries and the varying latent semantics of relations. However, existing methods based on multi-modal distribution roughly represent each subset without capturing its accurate cardinality, or even degenerate into uni-modal distribution learning during the reasoning process due to the lack of an effective similarity measure. To better model queries with diversified answers, we propose Query2GMM for answering logical queries over knowledge graphs. In Query2GMM, we present the GMM embedding to represent each query using a univariate Gaussian Mixture Model (GMM). Each subset of a query is encoded by its cardinality, semantic center and dispersion degree, allowing for precise representation of multiple subsets. Then we design specific neural networks for each operator to handle the inherent complexity that comes with multi-modal distribution while alleviating the cascading errors. Last, we design a new similarity measure to assess the relationships between an entity and a query's multi-answer subsets, enabling effective multi-modal distribution learning for reasoning. Comprehensive experimental results show that Query2GMM outperforms the best competitor by an absolute average of 6.35%.|基于知识图的逻辑查询回答是一项基本而又复杂的任务。实现这一点的一个有希望的方法是将查询和实体联合嵌入到相同的嵌入空间中。沿着这条线的研究表明，使用多模态分布来表示答案实体比单模态分布更合适，因为由于多跳查询的组合性质和关系的不同潜在语义，单个查询可能包含多个不相交的答案子集。然而，现有的基于多模态分布的方法在推理过程中，由于缺乏有效的相似性度量，往往不能准确地表示每个子集，甚至退化为单模态分布学习。为了更好地建立具有多样化答案的查询模型，我们提出了基于知识图的查询模型 Query2GMM。在 Query2GMM 中，我们使用单变量高斯混合模型(GMM)来表示每个查询。查询的每个子集由其基数、语义中心和分散度编码，允许对多个子集进行精确表示。然后针对每个算子设计特定的神经网络，以处理多模态分布所带来的固有复杂性，同时减小级联误差。最后，我们设计了一个新的相似性度量来评估一个实体和查询的多答案子集之间的关系，使有效的多模态分布学习推理。综合实验结果表明，Query2GMM 的性能绝对平均优于最佳竞争对手6.35% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query2GMM:+Learning+Representation+with+Gaussian+Mixture+Model+for+Reasoning+over+Knowledge+Graphs)|0|
|[Enhancing Complex Question Answering over Knowledge Graphs through Evidence Pattern Retrieval](https://doi.org/10.1145/3589334.3645563)|Wentao Ding, Jinmao Li, Liangchuan Luo, Yuzhong Qu||Information retrieval (IR) methods for KGQA consist of two stages: subgraph extraction and answer reasoning. We argue current subgraph extraction methods underestimate the importance of structural dependencies among evidence facts. We propose Evidence Pattern Retrieval (EPR) to explicitly model the structural dependencies during subgraph extraction. We implement EPR by indexing the atomic adjacency pattern of resource pairs. Given a question, we perform dense retrieval to obtain atomic patterns formed by resource pairs. We then enumerate their combinations to construct candidate evidence patterns. These evidence patterns are scored using a neural model, and the best one is selected to extract a subgraph for downstream answer reasoning. Experimental results demonstrate that the EPR-based approach has significantly improved the F1 scores of IR-KGQA methods by over 10 points on ComplexWebQuestions and achieves competitive performance on WebQuestionsSP.|信息检索分析方法包括子图提取和答案推理两个阶段。我们认为现有的子图提取方法低估了证据事实之间结构相关性的重要性。我们提出证据模式检索(EPR)来显式建模子图提取过程中的结构依赖。我们通过索引资源对的原子邻接模式来实现 EPR。给定一个问题，我们执行密集检索来获得由资源对形成的原子模式。然后我们列举它们的组合来构造候选证据模式。这些证据模式评分使用神经模型，并选择最好的一个提取子图的下游答案推理。实验结果表明，基于 EPR 的方法显著提高了 IR-KGQA 方法在 ComplexWebquestions 上的 F1得分，提高了10分以上，并且在 Webquestions-SP 上取得了较好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Complex+Question+Answering+over+Knowledge+Graphs+through+Evidence+Pattern+Retrieval)|0|
|[Author Name Disambiguation via Paper Association Refinement and Compositional Contrastive Embedding](https://doi.org/10.1145/3589334.3645596)|Dezhi Liu, Richong Zhang, Junfan Chen, Xinyue Chen||Author name disambiguation (AND) is an essential task for online academic retrieval systems. Recent models adopt representation learning in the author's name disambiguation. Despite achieving remarkable success, these methods may be limited in two aspects. First, the heuristically constructed paper association graphs used for representation learning contain uncertainties that may cause negative supervision. Second, existing algorithms, such as binary cross-entropy loss, used to train representation learning models may not produce sufficiently high-quality representations for AND. To tackle the above problems, we propose an association refining and compositional contrasting (ARCC) framework for AND tasks. ARCC first adopts an iterative graph structure refinement process to dynamically reduce the uncertainties in paper graphs. Then, a compositional contrastive learning method is proposed to encourage learning more discriminative representations for AND. Empirical studies on two benchmark datasets suggest that ARCC is effective for AND and outperforms the state-of-the-art models.|作者姓名歧义消除(AND)是在线学术检索系统的一项重要任务。最近的模型采用表征学习来消除作者姓名的歧义。尽管这些方法取得了显著的成功，但它们可能在两个方面受到限制。首先，用于表示学习的启发式构造的纸张关联图含有可能导致负监督的不确定性。其次，现有的用于训练表示学习模型的算法，如二进制交叉熵损失算法，可能不能为 AND 提供足够高质量的表示。针对上述问题，提出了一种基于关联细化和组合对比(ARCC)的 AND 任务框架。ARCC 首先采用一种迭代的图结构细化过程来动态地降低纸质图中的不确定性。然后，提出了一种组合对比学习方法，以鼓励学习更多的与的区分表示。对两个基准数据集的实证研究表明，ARCC 对 AND 模型是有效的，并且优于最先进的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Author+Name+Disambiguation+via+Paper+Association+Refinement+and+Compositional+Contrastive+Embedding)|0|
|[Dual Box Embeddings for the Description Logic EL++](https://doi.org/10.1145/3589334.3645648)|Mathias Jackermeier, Jiaoyan Chen, Ian Horrocks||OWL ontologies, whose formal semantics are rooted in Description Logic (DL), have been widely used for knowledge representation. Similar to Knowledge Graphs (KGs), ontologies are often incomplete, and maintaining and constructing them has proved challenging. While classical deductive reasoning algorithms use the precise formal semantics of an ontology to predict missing facts, recent years have witnessed growing interest in inductive reasoning techniques that can derive probable facts from an ontology. Similar to KGs, a promising approach is to learn ontology embeddings in a latent vector space, while additionally ensuring they adhere to the semantics of the underlying DL. While a variety of approaches have been proposed, current ontology embedding methods suffer from several shortcomings, especially that they all fail to faithfully model one-to-many, many-to-one, and many-to-many relations and role inclusion axioms. To address this problem and improve ontology completion performance, we propose a novel ontology embedding method named Box^2EL for the DL EL++, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles), and models inter-concept relationships using a bumping mechanism. We theoretically prove the soundness of Box^2EL and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets on the tasks of subsumption prediction, role assertion prediction, and approximating deductive reasoning.|OWL 本体形式语义学植根于描述逻辑(Description Logic，DL) ，已被广泛用于知识表示。与知识图(KGs)类似，本体通常是不完整的，维护和构建它们被证明是具有挑战性的。虽然经典的演绎推理算法使用本体的精确形式语义学来预测缺失的事实，但近年来人们对从本体中获取可能事实的归纳推理技术的兴趣日益增长。与 KG 类似，一种有前途的方法是学习潜在向量空间中的本体嵌入，同时确保它们遵循底层 DL 的语义。虽然提出了各种方法，但是现有的本体嵌入方法都存在一些缺陷，特别是它们都不能忠实地建模一对多、多对一和多对多的关系和角色包含公理。为了解决这个问题，提高本体的完成性能，我们提出了一种新的本体嵌入方法，称为盒子 ^ 2EL 的 DL EL + + ，表示概念和角色的盒子(即，轴对齐的超矩形) ，并使用碰撞机制模型的概念间关系。我们从理论上证明了 Box ^ 2EL 的可靠性，并进行了广泛的实验评估，在包容预测、角色断言预测和近似演绎推理任务的各种数据集上取得了最先进的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual+Box+Embeddings+for+the+Description+Logic+EL++)|0|
|[Jointly Canonicalizing and Linking Open Knowledge Base via Unified Embedding Learning](https://doi.org/10.1145/3589334.3645700)|Wei Shen, Binhan Yang, Yinan Liu||Recent years have witnessed increasing attention on the semantic knowledge integration between curated knowledge bases (CKBs) and open knowledge bases (OKBs), which is non-trivial due to the intrinsically heterogeneous features involved in CKBs and OKBs. OKB canonicalization and OKB linking are regarded as two vital tasks to achieve the knowledge integration. Although these two tasks are inherently complementary with each other, previous studies just solve them separately or via superficial interaction. To address this issue, we propose CLUE, a novel framework that jointly encodes the OKB and CKB into a unified embedding space, to tackle OKB canonicalization and OKB linking simultaneously and make them benefit each other reciprocally. We design an expectation-maximization (EM) based approach to iteratively refine the unified embedding space via performing seed generation and embedding refinement alternately, by leveraging the deep interaction between OKB canonicalization and OKB linking. Curriculum learning is employed to yield high-quality canonicalization seeds and linking seeds adaptively, according to two elaborately designed metrics (i.e., a margin-based linking metric and an entropy-based cluster metric). A thorough experimental study over two public benchmark data sets demonstrates that our proposed CLUE consistently outperforms state-of-the-art baselines for the task of OKB canonicalization (resp. OKB linking) in terms of average F1 (resp. accuracy).|近年来，策划知识库(CKB)和开放知识库(OKB)之间的语义知识集成受到越来越多的关注。OKB 规范化和 OKB 链接是实现知识集成的两个重要任务。虽然这两个任务在本质上是相辅相成的，但以往的研究只是单独或通过表面的互动来解决这两个问题。为了解决这个问题，我们提出了一种新的框架 CLUE，将 OKB 和 CKB 联合编码成一个统一的嵌入空间，同时处理 OKB 的规范化和 OKB 的链接，使它们相互受益。设计了一种基于期望最大化(EM)的方法，利用 OKB 规范化和 OKB 链接之间的深层交互，通过交替进行种子生成和嵌入求精，迭代求精统一嵌入空间。课程学习是根据两个精心设计的度量(即基于边际的连接度量和基于熵的聚类度量)来产生高质量的规范化种子和自适应地连接种子。通过对两个公共基准数据集的深入实验研究表明，我们提出的 CLUE 在 OKB 规范化任务中始终优于最先进的基准。OKB 链接)的平均 F1(相当于。准确性)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Jointly+Canonicalizing+and+Linking+Open+Knowledge+Base+via+Unified+Embedding+Learning)|0|
|[Efficient Exact and Approximate Betweenness Centrality Computation for Temporal Graphs](https://doi.org/10.1145/3589334.3645438)|Tianming Zhang, Yunjun Gao, Jie Zhao, Lu Chen, Lu Jin, Zhengyi Yang, Bin Cao, Jing Fan||Betweenness centrality of a vertex in a graph evaluates how often the vertex occurs in the shortest paths. It is a widely used metric of vertex importance in graph analytics. While betweenness centrality on static graphs has been extensively investigated, many real-world graphs are time-varying and modeled as temporal graphs. Examples include social networks and telecommunication networks, where a relationship between two vertices occurs at a specific time. Hence, in this paper, we target efficient methods for temporal betweenness centrality computation. We firstly propose an exact algorithm with the new notion of time instance graph, based on which, we derive a temporal dependency accumulation theory for iterative computation. To reduce the size of the time instance graph and improve the efficiency, we propose an additional optimization, which compresses the time instance graph with equivalent vertices and edges, and extends the dependency theory to the compressed graph. Since it is theoretically complex to compute temporal betweenness centrality, we further devise a probabilistically guaranteed approximate method to handle massive temporal graphs. Extensive experimental results on real-world temporal networks demonstrate the superior performance of the proposed methods. In particular, our exact and approximate methods outperform the state-of-the-art methods by up to two and five orders of magnitude, respectively.|图中顶点的间距中心度计算顶点在最短路径中出现的频率。它是图分析中广泛使用的顶点重要性度量。静态图的中心性已经得到了广泛的研究，但是现实世界中的许多图都是时变的，并且被建模为时态图。例子包括社会网络和电信网络，其中两个顶点之间的关系发生在特定的时间。因此，本文针对时间间隔中心性计算的有效方法进行了研究。本文首先提出了一种新的时间实例图概念的精确算法，在此基础上推导了迭代计算的时间依赖积累理论。为了减少时间实例图的大小和提高效率，我们提出了一种额外的优化方法，即用等效的顶点和边压缩时间实例图，并将依赖理论扩展到压缩后的图。由于计算时间间隔中心性在理论上比较复杂，我们进一步提出了一种处理海量时间图的概率保证近似方法。在实际时间网络上的大量实验结果表明，该方法具有良好的性能。特别是，我们的精确方法和近似方法比最先进的方法分别高出2数量级和5倍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Exact+and+Approximate+Betweenness+Centrality+Computation+for+Temporal+Graphs)|0|
|[PaCEr: Network Embedding From Positional to Structural](https://doi.org/10.1145/3589334.3645516)|Yuchen Yan, Yongyi Hu, Qinghai Zhou, Lihui Liu, Zhichen Zeng, Yuzhong Chen, Menghai Pan, Huiyuan Chen, Mahashweta Das, Hanghang Tong||Network embedding plays an important role in a variety of social network applications. Existing network embedding methods, explicitly or implicitly, can be categorized into positional embedding (PE) methods or structural embedding (SE) methods. Specifically, PE methods encode the positional information and obtain similar embeddings for adjacent/close nodes, while SE methods aim to learn identical representations for nodes with the same local structural patterns, even if the two nodes are far away from each other. The disparate designs of the two types of methods lead to an apparent dilemma in that no embedding could perfectly capture both positional and structural information. In this paper, we seek to demystify the underlying relationship between positional embedding and structural embedding. We first point out that the positional embedding can produce the structural embedding with simple transformations, while the opposite direction cannot hold. Based on this finding, a novel network embedding model PACER is proposed, which optimizes the positional embedding with the help of random walk with restart (RWR) proximity distribution, and such positional embedding is then used to seamlessly obtain the structural embedding with simple transformations. Furthermore, two variants of PACER are proposed to handle node classification task on homophilic and heterophilic graphs. Extensive experiments on 17 datasets show that PACER achieves comparable or better performance than the state-of-the-arts.|网络嵌入在各种社交网络应用中发挥着重要作用。现有的网络嵌入方法，无论是显式的还是隐式的，都可以分为位置嵌入(PE)方法和结构嵌入(SE)方法。具体来说，PE 方法对相邻/相近节点的位置信息进行编码并获得相似的嵌入，而 SE 方法的目标是学习具有相同局部结构模式的节点的相同表示，即使这两个节点相距很远。这两种方法的不同设计导致了一个明显的困境，即没有任何嵌入能够完美地捕获位置和结构信息。本文试图揭示位置嵌入和结构嵌入之间的内在联系。首先指出位置嵌入可以通过简单的变换产生结构嵌入，而相反的方向不能保持。在此基础上，提出了一种新的网络嵌入模型 PACER，该模型利用重启随机游走(RWR)邻近分布对位置嵌入进行优化，然后利用位置嵌入通过简单的变换实现结构嵌入的无缝化。在此基础上，提出了两种 PACER 变体来处理同亲图和异亲图上的节点分类任务。对17个数据集进行的大量实验表明，PACER 的性能达到了与现有技术相当或更好的水平。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PaCEr:+Network+Embedding+From+Positional+to+Structural)|0|
|[Link Recommendation to Augment Influence Diffusion with Provable Guarantees](https://doi.org/10.1145/3589334.3645521)|Xiaolong Chen, Yifan Song, Jing Tang||Link recommendation systems in online social networks (OSNs), such as Facebook's “People You May Know”, Twitter's “Who to Follow”, and Instagram's “Suggested Accounts”, facilitate the formation of new connections among users. This paper addresses the challenge of link recommendation for the purpose of social influence maximization. In particular, given a graph G and the seed set S, our objective is to select k edges that connect seed nodes and ordinary nodes to optimize the influence dissemination of the seed set. This problem, referred to as influence maximization with augmentation (IMA), has been proven to be NP-hard. In this paper, we propose an algorithm, namely , consisting of an efficient estimator for augmented influence estimation and an accelerated sampling approach. provides a (1-1/e-ε)-approximate solution with a high probability of 1-δ, and runs in O(k^2 (m+n) log (n / δ) / ε^2 + k |E_𝒞|) time assuming that the influence of any singleton node is smaller than that of the seed set. To the best of our knowledge, this is the first algorithm that can be implemented on large graphs containing millions of nodes while preserving strong theoretical guarantees. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed algorithm.|在线社交网络(OSNs)中的链接推荐系统，如 Facebook 的“你可能认识的人”、 Twitter 的“关注谁”和 Instagram 的“推荐账户”，促进了用户之间新的联系的形成。本文以社会影响力最大化为目标，解决了链接推荐的挑战。特别地，给定一个图 G 和种子集 S，我们的目标是选择连接种子节点和普通节点的 k 条边，以优化种子集的影响传播。这个问题被称为增广影响最大化问题(IMA) ，已被证明是 NP 难的。在本文中，我们提出了一个算法，即由一个有效的估计增强影响估计和加速抽样方法。提供了一个(1-1/e-ε)-近似解，其概率为1-δ，假设任一单点节点的影响小于种子集的影响，在 O (k ^ 2(m + n) log (n/δ)/ε ^ 2 + k | E _ C |)时间内运行。据我们所知，这是第一个可以在包含数百万个节点的大图上实现的算法，同时保留了很强的理论保证。我们进行了广泛的实验，以证明我们提出的算法的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Link+Recommendation+to+Augment+Influence+Diffusion+with+Provable+Guarantees)|0|
|[Fairness Rising from the Ranks: HITS and PageRank on Homophilic Networks](https://doi.org/10.1145/3589334.3645609)|AnaAndreea Stoica, Nelly Litvak, Augustin Chaintreau||In this paper, we investigate the conditions under which link analysis algorithms prevent minority groups from reaching high ranking slots. We find that the most common link-based algorithms using centrality metrics, such as PageRank and HITS, can reproduce and even amplify bias against minority groups in networks. Yet, their behavior differs: one one hand, we empirically show that PageRank mirrors the degree distribution for most of the ranking positions and it can equalize representation of minorities among the top ranked nodes; on the other hand, we find that HITS amplifies pre-existing bias in homophilic networks through a novel theoretical analysis, supported by empirical results. We find the root cause of bias amplification in HITS to be the level of homophily present in the network, modeled through an evolving network model with two communities. We illustrate our theoretical analysis on both synthetic and real datasets and we present directions for future work.|在本文中，我们研究了链接分析算法阻止少数群体达到高排名位置的条件。我们发现，最常见的基于链路的算法使用中心度量，如 PageRank 和 HITS，可以重现，甚至放大对少数群体的偏见网络。然而，他们的行为是不同的: 一方面，我们经验证明 PageRank 反映了大多数排名位置的程度分布，它可以平衡顶级节点中少数群体的代表性; 另一方面，我们发现 HITS 通过一个新的理论分析放大了同质网络中预先存在的偏见，并得到了经验结果的支持。我们发现 HITS 中偏差放大的根本原因是网络中存在的同质性水平，通过一个具有两个群体的演化网络模型来模拟。我们举例说明我们的理论分析合成和真实的数据集，我们提出了未来的工作方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness+Rising+from+the+Ranks:+HITS+and+PageRank+on+Homophilic+Networks)|0|
|[Modeling the Impact of Timeline Algorithms on Opinion Dynamics Using Low-rank Updates](https://doi.org/10.1145/3589334.3645714)|Tianyi Zhou, Stefan Neumann, Kiran Garimella, Aristides Gionis||Timeline algorithms are key parts of online social networks, but during recent years they have been blamed for increasing polarization and disagreement in our society. Opinion-dynamics models have been used to study a variety of phenomena in online social networks, but an open question remains on how these models can be augmented to take into account the fine-grained impact of user-level timeline algorithms. We make progress on this question by providing a way to model the impact of timeline algorithms on opinion dynamics. Specifically, we show how the popular Friedkin–Johnsen opinion-formation model can be augmented based on aggregate information, extracted from timeline data. We use our model to study the problem of minimizing the polarization and disagreement; we assume that we are allowed to make small changes to the users' timeline compositions by strengthening some topics of discussion and penalizing some others. We present a gradient descent-based algorithm for this problem, and show that under realistic parameter settings, our algorithm computes a (1+ε)-approximate solution in time Õ(m√(n)(1/ε)), where m is the number of edges in the graph and n is the number of vertices. We also present an algorithm that provably computes an ε-approximation of our model in near-linear time. We evaluate our method on real-world data and show that it effectively reduces the polarization and disagreement in the network. Finally, we release an anonymized graph dataset with ground-truth opinions and more than 27 000 nodes (the previously largest publicly available dataset contains less than 550 nodes).|时间轴算法是在线社交网络的关键组成部分，但近年来，它们被指责为导致社会两极分化和分歧加剧的罪魁祸首。舆论动力学模型已经被用来研究在线社交网络中的各种现象，但是一个悬而未决的问题是如何扩展这些模型以考虑用户级时间轴算法的细粒度影响。我们在这个问题上取得了进展，提供了一种方法来模拟时间轴算法对意见动态的影响。具体地说，我们展示了如何基于从时间轴数据中提取的聚合信息扩展流行的 Friedkin-Johnsen 意见形成模型。我们使用我们的模型来研究最小化两极分化和分歧的问题; 我们假设我们可以通过加强讨论的一些话题和惩罚其他一些话题来对用户的时间表组成做一些小的改变。本文提出了一种基于梯度下降的算法，并证明了在实际参数设置条件下，该算法计算的时间域(m √(n)(1/ε))为(1 + ε)-近似解，其中 m 是图的边数，n 是顶点数。我们还提出了一个算法，可证明计算我们的模型在近线性时间的 ε- 近似。我们评估了我们的方法对真实世界的数据，并表明它有效地减少了极化和网络中的分歧。最后，我们发布了一个匿名的图形数据集，其中包含地面真相观点和超过27000个节点(以前最大的公开可用数据集包含不到550个节点)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+the+Impact+of+Timeline+Algorithms+on+Opinion+Dynamics+Using+Low-rank+Updates)|0|
|[PAGE: Equilibrate Personalization and Generalization in Federated Learning](https://doi.org/10.1145/3589334.3645513)|Qian Chen, Zilong Wang, Jiaqi Hu, Haonan Yan, Jianying Zhou, Xiaodong Lin||Federated learning (FL) is becoming a major driving force behind machine learning as a service, where customers (clients) collaboratively benefit from shared local updates under the orchestration of the service provider (server). Representing clients' current demands and the server's future demand, local model personalization and global model generalization are separately investigated, as the ill-effects of data heterogeneity enforce the community to focus on one over the other. However, these two seemingly competing goals are of equal importance rather than black and white issues, and should be achieved simultaneously. In this paper, we propose the first algorithm to balance personalization and generalization on top of game theory, dubbed PAGE, which reshapes FL as a co-opetition game between clients and the server. To explore the equilibrium, PAGE further formulates the game as Markov decision processes, and leverages the reinforcement learning algorithm, which simplifies the solving complexity. Extensive experiments on four widespread datasets show that PAGE outperforms state-of-the-art FL baselines in terms of global and local prediction accuracy simultaneously, and the accuracy can be improved by up to 35.20% and 39.91%, respectively. In addition, biased variants of PAGE imply promising adaptiveness to demand shifts in practice.|联邦学习(FL)正在成为机器学习作为一种服务背后的主要驱动力，在服务提供者(服务器)的协调下，客户(客户)协同受益于共享的本地更新。由于数据异构性的负面影响迫使社区将注意力放在一个服务器上，因此本地模型个性化和全局模型泛化分别代表了客户当前的需求和服务器未来的需求。然而，这两个看似相互竞争的目标是同等重要的，而不是黑白分明的问题，应该同时实现。本文在博弈论的基础上，提出了平衡个性化和泛化的第一种算法 PAGE，它将 FL 重塑为客户端和服务器之间的一种合作竞争博弈。为了探索均衡，PAGE 进一步将博弈表述为马尔可夫决策过程，并利用强化学习算法，从而简化了求解的复杂性。在四个广泛使用的数据集上进行的大量实验表明，PAGE 在全局和局部预测准确度方面同时优于最先进的 FL 基线，其准确度分别可提高35.20% 和39.91% 。此外，有偏见的 PAGE 变体意味着有希望在实践中适应需求变化。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PAGE:+Equilibrate+Personalization+and+Generalization+in+Federated+Learning)|0|
|[MatchNAS: Optimizing Edge AI in Sparse-Label Data Contexts via Automating Deep Neural Network Porting for Mobile Deployment](https://doi.org/10.1145/3589334.3645538)|Hongtao Huang, Xiaojun Chang, Wen Hu, Lina Yao||Recent years have seen the explosion of edge intelligence with powerful Deep Neural Networks (DNNs). One popular scheme is training DNNs on powerful cloud servers and subsequently porting them to mobile devices after being lightweight. Conventional approaches manually specialized DNNs for various edge platforms and retrain them with real-world data. However, as the number of platforms increases, these approaches become labour-intensive and computationally prohibitive. Additionally, real-world data tends to be sparse-label, further increasing the difficulty of lightweight models. In this paper, we propose MatchNAS, a novel scheme for porting DNNs to mobile devices. Specifically, we simultaneously optimise a large network family using both labelled and unlabelled data and then automatically search for tailored networks for different hardware platforms. MatchNAS acts as an intermediary that bridges the gap between cloud-based DNNs and edge-based DNNs.|近年来，随着强大的深度神经网络(DNN)的出现，边缘智能得到了迅猛发展。一个流行的方案是在强大的云服务器上培训 DNN，然后在轻量级之后将它们移植到移动设备上。传统的方法手动为各种边缘平台专门化 DNN，并用真实世界的数据重新训练它们。然而，随着平台数量的增加，这些方法变得劳动密集型和计算禁止。此外，真实世界的数据往往是稀疏标签，进一步增加了轻量级模型的难度。在本文中，我们提出了 MatchNAS，一种移植 DNN 到移动设备的新方案。具体来说，我们同时使用有标签和无标签的数据优化一个大型网络系列，然后自动为不同的硬件平台搜索量身定制的网络。MatchNAS 作为一个中介，在基于云的 DNN 和基于边缘的 DNN 之间架起了桥梁。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MatchNAS:+Optimizing+Edge+AI+in+Sparse-Label+Data+Contexts+via+Automating+Deep+Neural+Network+Porting+for+Mobile+Deployment)|0|
|[Temporal Conformity-aware Hawkes Graph Network for Recommendations](https://doi.org/10.1145/3589334.3645354)|Chenglong Ma, Yongli Ren, Pablo Castells, Mark Sanderson||Many existing recommender systems (RSs) assume user behavior is governed solely by their interests. However, the peer effect often influences individual decision-making, which leads to conformity behavior. Conventional solutions that eliminate indiscriminately such bias may cause RSs to neglect valuable information and depersonalize the recommendation results. Also, conformity can transform into user interest, e.g., discovering new tastes after a glance at popular music. By better representing different forms of conformity influence, we can do a better job at interest mining and debiasing. In certain extreme circumstances, the herd effect may be exacerbated by user anxiety with uncertainty (e.g., panic buying during the COVID-19 pandemic). RSs may thus fail to respond in time due to sudden and dramatic changes. Moreover, many existing studies potentially conflate conformity bias with popularity bias and lump together various factors responsible for differences in popularity. In this paper, we identify two distinct types of conformity behavior: informational conformity and normative conformity. To address this, we introduce the TCHN model, which utilizes attentional Hawkes processes to disentangle user self-interest and conformity in a personalized manner. Our approach incorporates temporal graph attention networks to capture users' stable and volatile dynamics. We conduct experiments on three real-world datasets, which uncover diverse levels of conformity among users. The results show that TCHN excels in recommendation accuracy, diversity, and fairness across various user groups.|许多现有的推荐系统(RS)假设用户行为完全由他们的兴趣所支配。然而，同伴效应往往影响个体的决策，从而导致从众行为。传统的解决方案，消除不分青红皂白的这种偏见可能会导致 RSS 忽视有价值的信息和去个性化的推荐结果。此外，一致性可以转化为用户的兴趣，例如，发现新的口味一瞥流行音乐。通过更好地表征不同形式的整合影响，可以更好地挖掘和消除利益偏差。在某些极端情况下，用户对不确定性的焦虑可能会加剧羊群效应(例如，2019冠状病毒疾病大流行期间的恐慌性购买)。因此，由于突然和剧烈的变化，RSS 可能无法及时响应。此外，许多现有的研究可能将从众偏见与流行偏见混为一谈，并将造成流行程度差异的各种因素混为一谈。在本文中，我们确定了两种不同类型的从众行为: 信息从众和规范从众。为了解决这一问题，我们引入了 TCHN 模型，该模型利用注意霍克斯过程以个性化的方式将用户的自身利益和一致性分离开来。我们的方法结合了时间图注意网络来捕捉用户的稳定和不稳定的动态。我们在三个真实世界的数据集上进行实验，这些数据集揭示了用户之间不同程度的一致性。结果表明，TCHN 在推荐的准确性、多样性和公平性方面在不同的用户群中表现出色。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Temporal+Conformity-aware+Hawkes+Graph+Network+for+Recommendations)|0|
|[Hierarchical Graph Signal Processing for Collaborative Filtering](https://doi.org/10.1145/3589334.3645368)|Jiafeng Xia, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, Li Shang, Ning Gu||Graph Signal Processing (GSP) has proven to be a highly effective and efficient tool for predicting user future interactions in recommender systems. However, current GSP methods recognize user interaction patterns based on the interactions of all users, so that the recognized interaction patterns are not fully user-matched and easily impacted by other users with different interaction behaviors, resulting in sub-optimal recommendation performance. To this end, we propose a hierarchical graph signal processing method (HiGSP) for collaborative filtering, which consists of two key modules: 1) the cluster-wise filter module that recognizes user unique interaction patterns merely from interactions of users with similar preferences, making the recognized patterns able to reflect user preference without being influenced by other users with different interaction behaviors, and 2) the globally-aware filter module that serves as a complementary to the cluster-wise filter module to recognize user general interaction patterns more effectively from all user interactions. By linearly combining these two modules, HiGSP can recognize user-matched interaction patterns, so as to model user preference and predict user future interactions more accurately. Extensive experiments on six real-world datasets demonstrate the superiority of HiGSP compared to other GCN-based and GSP-based recommendation methods in terms of efficacy and efficiency.|在推荐系统中，图形信号处理(GSP)已被证明是预测用户未来交互的高效工具。然而，目前的 GSP 方法是基于所有用户的交互来识别用户交互模式的，因此识别出的交互模式并不完全匹配，容易受到其他具有不同交互行为的用户的影响，导致推荐性能不理想。为此，我们提出了一种适用于协同过滤的分层图形信号处理方法(higSP) ，该方法由两个关键模块组成: 1)集群式过滤模块，仅仅从具有相似偏好的用户的交互中识别用户独特的交互模式，使得识别出的模式能够反映用户偏好，而不受具有不同交互行为的其他用户的影响; 2)全局感知过滤模块，作为集群式过滤模块的补充，从所有用户交互中更有效地识别用户一般交互模式。通过线性组合这两个模块，HiGSP 可以识别用户匹配的交互模式，从而更准确地建立用户偏好模型，预测用户未来的交互行为。在六个实际数据集上的大量实验表明，与其他基于 GCN 和基于 GSP 的推荐方法相比，HiGSP 具有更高的效率和效力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Graph+Signal+Processing+for+Collaborative+Filtering)|0|
|[Lower-Left Partial AUC: An Effective and Efficient Optimization Metric for Recommendation](https://doi.org/10.1145/3589334.3645371)|Wentao Shi, Chenxu Wang, Fuli Feng, Yang Zhang, Wenjie Wang, Junkang Wu, Xiangnan He||Optimization metrics are crucial for building recommendation systems at scale. However, an effective and efficient metric for practical use remains elusive. While Top-K ranking metrics are the gold standard for optimization, they suffer from significant computational overhead. Alternatively, the more efficient accuracy and AUC metrics often fall short of capturing the true targets of recommendation tasks, leading to suboptimal performance. To overcome this dilemma, we propose a new optimization metric, Lower-Left Partial AUC (LLPAUC), which is computationally efficient like AUC but strongly correlates with Top-K ranking metrics. Compared to AUC, LLPAUC considers only the partial area under the ROC curve in the Lower-Left corner to push the optimization focus on Top-K. We provide theoretical validation of the correlation between LLPAUC and Top-K ranking metrics and demonstrate its robustness to noisy user feedback. We further design an efficient point-wise recommendation loss to maximize LLPAUC and evaluate it on three datasets, validating its effectiveness and robustness.|优化度量对于大规模构建推荐系统至关重要。然而，对于实际应用来说，一个有效和高效的度量标准仍然是难以捉摸的。尽管 Top-K 排名指标是优化的黄金标准，但它们承受着巨大的计算开销。或者，更有效的准确性和 AUC 指标往往不能捕获推荐任务的真正目标，导致性能不理想。为了克服这一困境，我们提出了一种新的优化度量，下左偏 AUC (LLPAUC) ，它与 AUC 一样具有计算效率，但与 Top-K 排名度量强相关。与 AUC 相比，LLPAUC 只考虑了 Lower-Left 角 ROC 曲线下的部分面积，从而将优化重点放在 Top-K 上。我们提供了 LLPAUC 和 Top-K 排名指标之间相关性的理论验证，并证明了其对噪声用户反馈的鲁棒性。我们进一步设计了一个有效的逐点推荐损失来最大化 LLPAUC，并在三个数据集上对其进行评估，验证了其有效性和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lower-Left+Partial+AUC:+An+Effective+and+Efficient+Optimization+Metric+for+Recommendation)|0|
|[Learning to Rewrite Prompts for Personalized Text Generation](https://doi.org/10.1145/3589334.3645408)|Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, Michael Bendersky||Facilitated by large language models (LLMs), personalized text generation has become a rapidly growing research direction. Most existing studies focus on designing specialized models for a particular domain, or they require fine-tuning the LLMs to generate personalized text. We consider a typical scenario in which the large language model, which generates personalized output, is frozen and can only be accessed through APIs. Under this constraint, all one can do is to improve the input text (i.e., text prompts) sent to the LLM, a procedure that is usually done manually. In this paper, we propose a novel method to automatically revise prompts for personalized text generation. The proposed method takes the initial prompts generated by a state-of-the-art, multistage framework for personalized generation and rewrites a few critical components that summarize and synthesize the personal context. The prompt rewriter employs a training paradigm that chains together supervised learning (SL) and reinforcement learning (RL), where SL reduces the search space of RL and RL facilitates end-to-end training of the rewriter. Using datasets from three representative domains, we demonstrate that the rewritten prompts outperform both the original prompts and the prompts optimized via supervised learning or reinforcement learning alone. In-depth analysis of the rewritten prompts shows that they are not only human readable, but also able to guide manual revision of prompts when there is limited resource to employ reinforcement learning to train the prompt rewriter, or when it is costly to deploy an automatic prompt rewriter for inference.|在大型语言模型(LLM)的推动下，个性化文本生成已经成为一个迅速发展的研究方向。大多数现有的研究侧重于为特定领域设计专门的模型，或者需要对 LLM 进行微调以生成个性化的文本。我们考虑一个典型的场景，其中生成个性化输出的大型语言模型被冻结，只能通过 API 访问。在这个约束下，我们所能做的就是改进发送到 LLM 的输入文本(即文本提示) ，这个过程通常是手动完成的。针对个性化文本生成，提出了一种新的自动修改提示的方法。提出的方法采用最先进的多阶段个性化生成框架产生的初始提示，并重写了一些总结和综合个性化上下文的关键组件。提示重写器采用了一种将监督式学习(SL)和强化学习(RL)连接在一起的培训范式，其中 SL 减少了 RL 的搜索空间，而 RL 促进了重写器的端到端培训。通过使用来自三个代表性领域的数据集，我们证明了重写提示符的表现优于原始提示符和单独通过监督式学习或强化学习优化的提示符。对重写提示的深入分析表明，它们不仅具有人类可读性，而且在资源有限、无法使用强化学习培训提示重写程序、或者部署自动提示重写程序进行推理成本高昂的情况下，还能指导人工修改提示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Rewrite+Prompts+for+Personalized+Text+Generation)|0|
|[Physical Trajectory Inference Attack and Defense in Decentralized POI Recommendation](https://doi.org/10.1145/3589334.3645410)|Jing Long, Tong Chen, Guanhua Ye, Kai Zheng, Quoc Viet Hung Nguyen, Hongzhi Yin||As an indispensable personalized service within Location-Based Social Networks (LBSNs), the Point-of-Interest (POI) recommendation aims to assist individuals in discovering attractive and engaging places. However, the accurate recommendation capability relies on the powerful server collecting a vast amount of users' historical check-in data, posing significant risks of privacy breaches. Although several collaborative learning (CL) frameworks for POI recommendation enhance recommendation resilience and allow users to keep personal data on-device, they still share personal knowledge to improve recommendation performance, thus leaving vulnerabilities for potential attackers. Given this, we design a new Physical Trajectory Inference Attack (PTIA) to expose users' historical trajectories. Specifically, for each user, we identify the set of interacted POIs by analyzing the aggregated information from the target POIs and their correlated POIs. We evaluate the effectiveness of PTIA on two real-world datasets across two types of decentralized CL frameworks for POI recommendation. Empirical results demonstrate that PTIA poses a significant threat to users' historical trajectories. Furthermore, Local Differential Privacy (LDP), the traditional privacy-preserving method for CL frameworks, has also been proven ineffective against PTIA. In light of this, we propose a novel defense mechanism (AGD) against PTIA based on an adversarial game to eliminate sensitive POIs and their information in correlated POIs. After conducting intensive experiments, AGD has been proven precise and practical, with minimal impact on recommendation performance.|作为基于位置的社交网络(LBSNs)中不可或缺的个性化服务，兴趣点(POI)推荐旨在帮助个人发现有吸引力和有吸引力的地方。然而，准确的推荐功能依赖于强大的服务器收集大量用户的历史签入数据，从而带来严重的隐私泄露风险。虽然一些用于 POI 推荐的合作学习(CL)框架增强了推荐的弹性，并允许用户将个人数据保存在设备上，但它们仍然共享个人知识以提高推荐性能，从而为潜在的攻击者留下了漏洞。鉴于此，我们设计了一个新的物理轨迹推断攻击(PTIA)来暴露用户的历史轨迹。具体来说，对于每个用户，我们通过分析来自目标 POI 及其相关 POI 的聚合信息来识别交互的 POI 集合。我们评估 PTIA 在两种分散式 CL 框架的两个实际数据集上对 POI 推荐的有效性。实证结果表明，PTIA 对用户的历史轨迹构成了显著的威胁。此外，CL 框架传统的保护隐私的方法——本地差分隐私(LDP)也被证明对 PTIA 无效。在此基础上，提出了一种新的基于对抗博弈的防御机制(AGD)来消除敏感 POI 及其相关 POI 中的信息。经过深入的实验，AGD 已经被证明是精确和实用的，对推荐性能的影响最小。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Physical+Trajectory+Inference+Attack+and+Defense+in+Decentralized+POI+Recommendation)|0|
|[Towards the Identifiability and Explainability for Personalized Learner Modeling: An Inductive Paradigm](https://doi.org/10.1145/3589334.3645437)|Jiatong Li, Qi Liu, Fei Wang, Jiayu Liu, Zhenya Huang, Fangzhou Yao, Linbo Zhu, Yu Su||Personalized learner modeling using cognitive diagnosis (CD), which aims to model learners' cognitive states by diagnosing learner traits from behavioral data, is a fundamental yet significant task in many web learning services. Existing cognitive diagnosis models (CDMs) follow the proficiency-response paradigm that views learner traits and question parameters as trainable embeddings and learns them through learner performance prediction. However, we notice that this paradigm leads to the inevitable non-identifiability and explainability overfitting problem, which is harmful to the quantification of learners' cognitive states and the quality of web learning services. To address these problems, we propose an identifiable cognitive diagnosis framework (ID-CDF) based on a novel response-proficiency-response paradigm inspired by encoder-decoder models. Specifically, we first devise the diagnostic module of ID-CDF, which leverages inductive learning to eliminate randomness in optimization to guarantee identifiability and captures the monotonicity between overall response data distribution and cognitive states to prevent explainability overfitting. Next, we propose a flexible predictive module for ID-CDF to ensure diagnosis preciseness. We further present an implementation of ID-CDF, i.e., ID-CDM, to illustrate its usability. Extensive experiments on four real-world datasets with different characteristics demonstrate that ID-CDF can effectively address the problems without loss of diagnosis preciseness.|基于认知诊断的个性化学习者建模是许多网络学习服务中的基础性工作，其目的是通过对学习者行为特征的诊断来建立学习者的认知状态模型。现有的认知诊断模型遵循熟练度-反应范式，将学习者特征和问题参数视为可训练的嵌入，并通过学习者表现预测来学习。然而，我们注意到这种范式导致了不可避免的不可识别性和可解释性过度拟合问题，这不利于学习者认知状态的量化和网络学习服务的质量。为了解决这些问题，我们提出了一个可识别的认知诊断框架(ID-CDF)基于一个新的反应-熟练程度-反应范式的启发编码器-解码器模型。具体而言，我们首先设计 ID-CDF 的诊断模块，其利用归纳学习消除优化中的随机性以保证可识别性，并捕获总体响应数据分布和认知状态之间的单调性以防止可解释性过度拟合。接下来，我们提出了一个灵活的 ID-CDF 预测模块，以确保诊断的准确性。我们进一步展示了 ID-CDF 的实现，即 ID-CDM，以说明其可用性。对四个具有不同特征的实际数据集进行的大量实验表明，ID-CDF 算法能够在不损失诊断精度的情况下有效地解决问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+the+Identifiability+and+Explainability+for+Personalized+Learner+Modeling:+An+Inductive+Paradigm)|0|
|[Generative News Recommendation](https://doi.org/10.1145/3589334.3645448)|Shen Gao, Jiabao Fang, Quan Tu, Zhitao Yao, Zhumin Chen, Pengjie Ren, Zhaochun Ren||Most existing news recommendation methods tackle this task by conducting semantic matching between candidate news and user representation produced by historical clicked news. However, they overlook the high-level connections among different news articles and also ignore the profound relationship between these news articles and users. And the definition of these methods dictates that they can only deliver news articles as-is. On the contrary, integrating several relevant news articles into a coherent narrative would assist users in gaining a quicker and more comprehensive understanding of events. In this paper, we propose a novel generative news recommendation paradigm that includes two steps: (1) Leveraging the internal knowledge and reasoning capabilities of the Large Language Model (LLM) to perform high-level matching between candidate news and user representation; (2) Generating a coherent and logically structured narrative based on the associations between related news and user interests, thus engaging users in further reading of the news. Specifically, we propose GNR to implement the generative news recommendation paradigm. First, we compose the dual-level representation of news and users by leveraging LLM to generate theme-level representations and combine them with semantic-level representations. Next, in order to generate a coherent narrative, we explore the news relation and filter the related news according to the user preference. Finally, we propose a novel training method named UIFT to train the LLM to fuse multiple news articles in a coherent narrative. Extensive experiments show that GNR can improve recommendation accuracy and eventually generate more personalized and factually consistent narratives.|现有的大多数新闻推荐方法都是通过对历史点击新闻产生的候选新闻和用户表示进行语义匹配来解决这一问题。然而，他们忽视了不同新闻文章之间的高层次联系，也忽视了这些新闻文章与用户之间的深层次关系。这些方法的定义表明，它们只能按原样发布新闻文章。相反，将若干相关新闻文章整合成一个连贯的叙述将有助于用户更快、更全面地了解事件。本文提出了一种新的生成性新闻推荐范式，包括两个步骤: (1)利用大语言模型(LLM)的内部知识和推理能力，在候选新闻和用户表征之间进行高层次的匹配; (2)基于相关新闻和用户兴趣之间的关联，生成一个连贯的、逻辑结构化的叙事，从而使用户进一步阅读新闻。具体来说，我们提出 GNR 来实现生成性新闻推荐范式。首先，我们利用 LLM 生成主题级表示，并将它们与语义级表示结合起来，从而构成新闻和用户的双层表示。其次，为了生成连贯的叙述，我们探索新闻关系，并根据用户偏好对相关新闻进行过滤。最后，我们提出了一种新的训练方法，命名为 UIFT，训练 LLM 在一个连贯的叙述中融合多篇新闻文章。广泛的实验表明，GNR 可以提高推荐的准确性，并最终产生更个性化和事实一致的叙述。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+News+Recommendation)|0|
|[MMPOI: A Multi-Modal Content-Aware Framework for POI Recommendations](https://doi.org/10.1145/3589334.3645449)|Yang Xu, Gao Cong, Lei Zhu, Lizhen Cui||The Point-of-Interest (POI) recommendation system, designed to recommend potential future visits of users based on their check-in sequences, faces the challenge of data scarcity. This challenge primarily stems from the data sparsity issue, namely users interact with only a small number of POIs. Most existing studies attempt to solve this problem by focusing on POI check-in sequences, without considering the substantial multi-modal content information (e.g. textual and image data) commonly associated with POIs. In this paper, we propose a novel multi-modal content-aware framework for POI recommendation (MMPOI). Our approach addresses the issue of data sparsity by incorporating multi-modal content information about POIs from a new perspective. Specifically, MMPOI leverages pre-trained models for inter-modal conversion and employs a unified pre-trained model to extract modal-specific features from each modality, effectively bridging the semantic gap between different modalities. We propose to build a Multi-Modal Trajectory Flow Graph (MTFG) which combines the multi-modal semantic structure with check-in sequences. Moreover, we design an adaptive multi-task Transformer that models users' multi-modal movement patterns and integrates them for the next POI recommendation tasks. Extensive experiments on four real-world datasets demonstrate that MMPOI outperforms state-of-the-art POI recommendation methods. To facilitate reproducibility, we have released both the code and the multi-modal POI recommendation datasets we collect https://github.com/zzmylq/MMPOI|兴趣点(POI)推荐系统，旨在推荐潜在的用户未来访问的基础上，他们的签入序列，面临的数据稀缺性的挑战。这一挑战主要源于数据稀疏问题，即用户只与少量 POI 交互。大多数现有的研究试图通过关注 POI 签入序列来解决这个问题，而没有考虑通常与 POI 相关的大量多模态内容信息(例如文本和图像数据)。本文提出了一种新的多模态内容感知 POI 推荐框架(MMPOI)。我们的方法通过从一个新的角度整合关于 POI 的多模式内容信息来解决数据稀少的问题。具体来说，MMPOI 利用预训练模型进行多模态转换，并采用统一的预训练模型从每种模态中提取模态特征，有效地弥合了不同模态之间的语义鸿沟。提出了一种将多模态语义结构与检入序列相结合的多模态轨迹流图(MTFG)。此外，我们还设计了一个自适应多任务转换器，模拟用户的多模态运动模式，并将其集成到下一个 POI 推荐任务中。在四个实际数据集上的大量实验表明，MMPOI 优于最先进的 POI 推荐方法。为了便于重现，我们发布了代码和我们收集的多模式 POI 推荐数据集 https://github.com/zzmylq/mmpoi|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MMPOI:+A+Multi-Modal+Content-Aware+Framework+for+POI+Recommendations)|0|
|[Representation Learning with Large Language Models for Recommendation](https://doi.org/10.1145/3589334.3645458)|Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang||Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks, particularly in capturing complex user-item relationships. However, these graph-based recommenders heavily depend on ID-based data, potentially disregarding valuable textual information associated with users and items, resulting in less informative learned representations. Moreover, the utilization of implicit feedback data introduces potential noise and bias, posing challenges for the effectiveness of user preference learning. While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention, challenges such as scalability issues, limitations in text-only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems. To address these challenges, we propose a model-agnostic framework RLMRec that aims to enhance existing recommenders with LLM-empowered representation learning. It proposes a recommendation paradigm that integrates representation learning with LLMs to capture intricate semantic aspects of user behaviors and preferences. RLMRec incorporates auxiliary textual signals, develops a user/item profiling paradigm empowered by LLMs, and aligns the semantic space of LLMs with the representation space of collaborative relational signals through a cross-view alignment framework. This work further establish a theoretical foundation demonstrating that incorporating textual signals through mutual information maximization enhances the quality of representations. In our evaluation, we integrate RLMRec with state-of-the-art recommender models, while also analyzing its efficiency and robustness to noise data. Our implementation codes are available at https://github.com/HKUDS/RLMRec.|在深度学习和图形神经网络的影响下，推荐系统已经取得了显著的进步，特别是在捕获复杂的用户-项目关系方面。然而，这些基于图表的推荐严重依赖于基于 ID 的数据，可能会忽略与用户和项目相关的有价值的文本信息，从而导致信息量较小的学习表示。此外，隐式反馈数据的利用还会引入潜在的噪声和偏差，对用户偏好学习的有效性提出了挑战。在传统的基于 ID 的推荐系统中集成大型语言模型(LLM)已经引起了人们的关注，但是为了在实际的推荐系统中有效地实现，需要解决诸如可伸缩性问题、仅依赖文本的局限性和及时的输入限制等挑战。为了应对这些挑战，我们提出了一个模型无关的框架 RLMRec，旨在通过 LLM 授权的表示学习来增强现有的推荐系统。它提出了一种推荐范式，将表示学习与 LLM 相结合，以捕获用户行为和偏好的复杂语义方面。RLMRec 结合了辅助文本信号，开发了一个由 LLM 授权的用户/项目剖析范式，并通过跨视图对齐框架将 LLM 的语义空间与协作关系信号的表示空间对齐。本文的工作进一步奠定了理论基础，表明通过互信息最大化结合文本信号可以提高表征的质量。在我们的评估中，我们整合了 RLMRec 和最先进的推荐模型，同时也分析了它的效率和对噪声数据的鲁棒性。我们的执行守则可在 https://github.com/hkuds/rlmrec 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Representation+Learning+with+Large+Language+Models+for+Recommendation)|0|
|[Challenging Low Homophily in Social Recommendation](https://doi.org/10.1145/3589334.3645460)|Wei Jiang, Xinyi Gao, Guandong Xu, Tong Chen, Hongzhi Yin||Social relations are leveraged to tackle the sparsity issue of user-item interaction data in recommendation under the assumption of social homophily. However, social recommendation paradigms predominantly focus on homophily based on user preferences. While social information can enhance recommendations, its alignment with user preferences is not guaranteed, thereby posing the risk of introducing informational redundancy. We empirically discover that social graphs in real recommendation data exhibit low preference-aware homophily, which limits the effect of social recommendation models. To comprehensively extract preference-aware homophily information latent in the social graph, we propose Social Heterophily-alleviating Rewiring (SHaRe), a data-centric framework for enhancing existing graph-based social recommendation models. We adopt Graph Rewiring technique to capture and add highly homophilic social relations, and cut low homophilic (or heterophilic) relations. To better refine the user representations from reliable social relations, we integrate a contrastive learning method into the training of SHaRe, aiming to calibrate the user representations for enhancing the result of Graph Rewiring. Experiments on real-world datasets show that the proposed framework not only exhibits enhanced performances across varying homophily ratios but also improves the performance of existing state-of-the-art (SOTA) social recommendation models.|在社会同质性假设下，利用社会关系解决推荐中用户交互数据的稀疏性问题。然而，社交推荐模式主要关注基于用户偏好的同质性。虽然社交信息可以增强推荐，但它与用户偏好的一致性并不能得到保证，从而带来了引入信息冗余的风险。实证研究发现，真实推荐数据中的社会图表现出较低的偏好感知同质性，从而限制了社会推荐模型的效果。为了全面提取隐藏在社会图中的偏好感知同质性信息，本文提出了基于数据的社会推荐模型——社会异质性缓解重构框架(SHaRe)。我们采用图重构技术来捕获和添加高度同质的社会关系，并切断低度同质(或异质)关系。为了更好地从可靠的社会关系中提炼出用户表征，我们将对比学习方法融入到 ShareRe 的训练中，旨在校准用户表征以提高图重构的效果。在实际数据集上的实验表明，该框架不仅在不同的同质性比率下表现出更好的性能，而且改善了现有的最先进的社会推荐模型(SOTA)的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Challenging+Low+Homophily+in+Social+Recommendation)|0|
|[Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization](https://doi.org/10.1145/3589334.3645463)|Zirui Zhu, Yong Liu, Zangwei Zheng, Huifeng Guo, Yang You||Click-Through Rate (CTR) prediction holds paramount significance in online advertising and recommendation scenarios. Despite the proliferation of recent CTR prediction models, the improvements in performance have remained limited, as evidenced by open-source benchmark assessments. Current researchers tend to focus on developing new models for various datasets and settings, often neglecting a crucial question: What is the key challenge that truly makes CTR prediction so demanding? In this paper, we approach the problem of CTR prediction from an optimization perspective. We explore the typical data characteristics and optimization statistics of CTR prediction, revealing a strong positive correlation between the top hessian eigenvalue and feature frequency. This correlation implies that frequently occurring features tend to converge towards sharp local minima, ultimately leading to suboptimal performance. Motivated by the recent advancements in sharpness-aware minimization (SAM), which considers the geometric aspects of the loss landscape during optimization, we present a dedicated optimizer crafted for CTR prediction, named Helen. Helen incorporates frequency-wise Hessian eigenvalue regularization, achieved through adaptive perturbations based on normalized feature frequencies. Empirical results under the open-source benchmark framework underscore Helen's effectiveness. It successfully constrains the top eigenvalue of the Hessian matrix and demonstrates a clear advantage over widely used optimization algorithms when applied to seven popular models across three public benchmark datasets on BARS. Our code locates at github.com/NUS-HPC-AI-Lab/Helen.|在在线广告和推荐场景中，点进率(ctrl)预测具有至关重要的意义。尽管最近 CTR 预测模型激增，但性能的改进仍然有限，开源基准评估就是证明。目前的研究人员往往专注于为各种数据集和设置开发新的模型，往往忽略了一个关键问题: 什么是真正使 CTR 预测如此苛刻的关键挑战？本文从最优化的角度探讨了 CTR 预测问题。我们探讨了 CTR 预测的典型数据特征和优化统计，发现最高黑森特征值与特征频率之间存在很强的正相关关系。这种相关性意味着频繁出现的特征往往趋向于尖锐的局部极小值，最终导致次优性能。由于最近在锐度感知最小化(SAM)方面的进展，其中考虑了优化过程中损失景观的几何方面，我们提出了一个专用的 CTR 预测优化器，命名为 Helen。海伦采用了频率明确的黑森特征值正则化，实现了通过自适应扰动的基础上归一化的特征频率。开源基准框架下的实证结果强调了 Helen 的有效性。该算法成功地约束了 Hessian 矩阵的最高特征值，并且在 BARS 的三个公共基准数据集上应用于七个流行的模型时，显示出比广泛使用的优化算法有明显的优势。我们的代码位于 github.com/nus-hpc-ai-lab/helen。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Helen:+Optimizing+CTR+Prediction+Models+with+Frequency-wise+Hessian+Eigenvalue+Regularization)|0|
|[Intersectional Two-sided Fairness in Recommendation](https://doi.org/10.1145/3589334.3645518)|Yifan Wang, Peijie Sun, Weizhi Ma, Min Zhang, Yuan Zhang, Peng Jiang, Shaoping Ma||Fairness of recommender systems (RS) has attracted increasing attention recently. Based on the involved stakeholders, the fairness of RS can be divided into user fairness, item fairness, and two-sided fairness which considers both user and item fairness simultaneously. However, we argue that the intersectional two-sided unfairness may still exist even if the RS is two-sided fair, which is observed and shown by empirical studies on real-world data in this paper, and has not been well-studied previously. To mitigate this problem, we propose a novel approach called Intersectional Two-sided Fairness Recommendation (ITFR). Our method utilizes a sharpness-aware loss to perceive disadvantaged groups, and then uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups. Additionally, predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups. Extensive experiments and analyses on three public datasets show that our proposed approach effectively alleviates the intersectional two-sided unfairness and consistently outperforms previous state-of-the-art methods.|推荐系统的公平性近年来受到越来越多的关注。基于所涉及的利益相关者，RS 的公平性可以分为用户公平性、项目公平性和同时考虑用户公平性和项目公平性的双边公平性。然而，本文通过对实际数据的实证研究发现，即使 RS 是双边公平的，交叉口的双边不公平性仍然存在，而且这种不公平性还没有得到很好的研究。为了缓解这一问题，我们提出了一种新的方法称为交叉双边公平推荐(ITFR)。我们的方法利用敏锐感知损失来感知弱势群体，然后利用协作损失平衡来发展不同交叉群体的一致识别能力。此外，预测分数标准化是利用调整积极的预测分数，以公平对待积极的不同交叉组。对三个公共数据集的大量实验和分析表明，我们提出的方法有效地缓解了交叉口的双向不公平性，并始终优于以前的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Intersectional+Two-sided+Fairness+in+Recommendation)|0|
|[Full Stage Learning to Rank: A Unified Framework for Multi-Stage Systems](https://doi.org/10.1145/3589334.3645523)|Kai Zheng, Haijun Zhao, Rui Huang, Beichuan Zhang, Na Mou, Yanan Niu, Yang Song, Hongning Wang, Kun Gai||The Probability Ranking Principle (PRP) has been considered as the foundational standard in the design of information retrieval (IR) systems. The principle requires an IR module's returned list of results to be ranked with respect to the underlying user interests, so as to maximize the results' utility. Nevertheless, we point out that it is inappropriate to indiscriminately apply PRP through every stage of a contemporary IR system. Such systems contain multiple stages (e.g., retrieval, pre-ranking, ranking, and re-ranking stages, as examined in this paper). The selection bias inherent in the model of each stage significantly influences the results that are ultimately presented to users. To address this issue, we propose an improved ranking principle for multi-stage systems, namely the Generalized Probability Ranking Principle (GPRP), to emphasize both the selection bias in each stage of the system pipeline as well as the underlying interest of users. We realize GPRP via a unified algorithmic framework named Full Stage Learning to Rank. Our core idea is to first estimate the selection bias in the subsequent stages and then learn a ranking model that best complies with the downstream modules' selection bias so as to deliver its top ranked results to the final ranked list in the system's output. We performed extensive experiment evaluations of our developed Full Stage Learning to Rank solution, using both simulations and online A/B tests in one of the leading short-video recommendation platforms. The algorithm is proved to be effective in both retrieval and ranking stages. Since deployed, the algorithm has brought consistent and significant performance gain to the platform.|概率等级原则(PRP)一直被认为是信息检索系统设计的基本标准。该原则要求 IR 模块返回的结果列表根据潜在用户的兴趣进行排序，以最大限度地发挥结果的效用。然而，我们指出，在当代国际关系体系的每一个阶段，不加区分地适用 PRP 是不适当的。这样的系统包含多个阶段(例如，检索、预排序、排序和重排序阶段，正如本文所研究的)。每个阶段模型固有的选择偏差显著影响最终呈现给用户的结果。为了解决这一问题，我们提出了一种改进的多阶段系统排序原则，即广义概率排序原则(GPRP) ，以强调系统流水线各阶段的选择偏差以及用户的潜在兴趣。我们通过一个统一的算法框架——全阶段排序学习来实现 GPRP。我们的核心思想是首先估计后续阶段的选择偏差，然后学习一个最符合下游模块选择偏差的排名模型，以便将其排名最高的结果交付给系统输出的最终排名列表。我们在一个领先的短视频推荐平台上使用模拟和在线 A/B 测试，对我们开发的全阶段学习排名解决方案进行了广泛的实验评估。实验结果表明，该算法在检索和排序阶段都是有效的。自部署以来，该算法为平台带来了一致的、显著的性能增益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Full+Stage+Learning+to+Rank:+A+Unified+Framework+for+Multi-Stage+Systems)|0|
|[RecDCL: Dual Contrastive Learning for Recommendation](https://doi.org/10.1145/3589334.3645533)|Dan Zhang, Yangliao Geng, Wenwen Gong, Zhongang Qi, Zhiyu Chen, Xing Tang, Ying Shan, Yuxiao Dong, Jie Tang||Self-supervised recommendation (SSR) has achieved great success in mining the potential interacted behaviors for collaborative filtering in recent years. As a major branch, Contrastive Learning (CL) based SSR conquers data sparsity in Web platforms by contrasting the embedding between raw data and augmented data. However, existing CL-based SSR methods mostly focus on contrasting in a batch-wise way, failing to exploit potential regularity in the feature-wise dimension, leading to redundant solutions during the representation learning process of users (items) from Websites. Furthermore, the joint benefits of utilizing both Batch-wise CL (BCL) and Feature-wise CL (FCL) for recommendations remain underexplored. To address these issues, we investigate the relationship of objectives between BCL and FCL. Our study suggests a cooperative benefit of employing both methods, as evidenced from theoretical and experimental perspectives. Based on these insights, we propose a dual CL method for recommendation, referred to as RecDCL. RecDCL first eliminates redundant solutions on user-item positive pairs in a feature-wise manner. It then optimizes the uniform distributions within users and items using a polynomial kernel from an FCL perspective. Finally, it generates contrastive embedding on output vectors in a batch-wise objective. We conduct experiments on four widely-used benchmarks and an industrial dataset. The results consistently demonstrate that the proposed RecDCL outperforms the state-of-the-art GNNs-based and SSL-based models (with up to a 5.65\% improvement in terms of Recall@20), thereby confirming the effectiveness of the joint-wise objective. All source codes used in this paper are publicly available at \url{https://github.com/THUDM/RecDCL}}.|近年来，自我监督推荐在挖掘协同过滤潜在的交互行为方面取得了巨大成功。作为一个主要的分支，基于对比学习(CL)的 SSR 通过对比原始数据和增强数据的嵌入来克服 Web 平台中的数据稀疏性。然而，现有的基于 CL 的 SSR 方法大多侧重于批量对比，未能充分利用特征维中潜在的规律性，导致在网站用户(项目)的表示学习过程中出现冗余解决方案。此外，使用批处理式 CL (BCL)和特征式 CL (FCL)的联合优势仍然没有得到充分的探索。为了解决这些问题，我们调查了 BCL 和 FCL 之间的目标关系。我们的研究表明，从理论和实验的角度证明，采用这两种方法是一种合作的益处。基于这些见解，我们提出了一种双 CL 推荐方法，称为 RecDCL。RecDCL 首先以特性明智的方式消除用户项正对上的冗余解决方案。然后从 FCL 的角度使用多项式核优化用户和项目内的均匀分布。最后，在分批目标下对输出向量进行对比嵌入。我们在四个广泛使用的基准和一个工业数据集上进行实验。结果一致表明，拟议的 RecDCL 优于最先进的基于 GNN 和基于 SSL 的模型(在 Recall@20方面提高了5.65%) ，从而证实了联合目标的有效性。本文中使用的所有源代码都可以在 url { https://github.com/thudm/recdcl }上公开获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RecDCL:+Dual+Contrastive+Learning+for+Recommendation)|0|
|[GraphPro: Graph Pre-training and Prompt Learning for Recommendation](https://doi.org/10.1145/3589334.3645546)|Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang||GNN-based recommendation systems have been successful in capturing complex user-item interactions using multi-hop message passing. However, these methods often struggle to handle the dynamic nature of user-item interactions, making it challenging to adapt to changes in user preferences and new data distributions. This limits their scalability and performance in real-world dynamic scenarios. In our study, we propose a framework called GraphPro that combines dynamic graph pre-training with prompt learning in an efficient way. This unique approach allows GNNs to effectively capture both long-term user preferences and short-term behavior changes, resulting in accurate and up-to-date recommendations. To address the issue of changing user preferences, we integrate a temporal prompt mechanism and a graph-structural prompt learning mechanism into the pre-trained GNN architecture. The temporal prompt mechanism incorporates time-related information into user-item interactions, enabling the model to naturally incorporate temporal dynamics. The graph-structural prompt learning mechanism allows the model to apply pre-trained insights to new behavior dynamics without the need for continuous retraining. We also introduce a dynamic evaluation framework for recommendations that better reflects real-world scenarios and reduces the offline-online discrepancy. Through comprehensive experiments, including deployment in a large-scale industrial scenario, we demonstrate the seamless scalability of GraphPro with various leading recommenders. Our results highlight the superiority of GraphPro in terms of effectiveness, robustness, and efficiency. We release the model implementation at the link: https://github.com/HKUDS/GraphPro.|基于 GNN 的推荐系统已经成功地利用多跳消息传递来捕获复杂的用户-项目交互。然而，这些方法往往难以处理用户项交互的动态特性，使其难以适应用户偏好和新数据分布的变化。这限制了它们在真实动态场景中的可伸缩性和性能。在我们的研究中，我们提出了一个叫做 GraphPro 的框架，它将动态图预训练和快速学习有效地结合起来。这种独特的方法允许 GNN 有效地捕获长期用户偏好和短期行为变化，从而产生准确和最新的建议。为了解决用户偏好变化的问题，我们将时间提示机制和图结构提示学习机制集成到预先训练好的 GNN 体系结构中。时间提示机制将与时间相关的信息合并到用户项交互中，使模型能够自然地合并时间动态。图形结构的提示学习机制允许模型应用预先训练的洞察力到新的行为动力学而不需要连续的再训练。我们还为建议引入了一个动态评估框架，它能够更好地反映真实世界的场景，并减少离线-在线差异。通过全面的实验，包括在大规模工业场景中的部署，我们展示了 GraphPro 的无缝伸缩性和各种领先的推荐。我们的结果突出了 GraphPro 在有效性、鲁棒性和效率方面的优势。我们在 link:  https://github.com/hkuds/graphpro 发布模型实现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphPro:+Graph+Pre-training+and+Prompt+Learning+for+Recommendation)|0|
|[Modeling Balanced Explicit and Implicit Relations with Contrastive Learning for Knowledge Concept Recommendation in MOOCs](https://doi.org/10.1145/3589334.3645559)|Hengnian Gu, Zhiyi Duan, Pan Xie, Dongdai Zhou||The knowledge concept recommendation in Massive Open Online Courses (MOOCs) is a significant issue that has garnered widespread attention. Existing methods primarily rely on the explicit relations between users and knowledge concepts on the MOOC platforms for recommendation. However, there are numerous implicit relations (e.g., shared interests or same knowledge levels between users) generated within the users' learning activities on the MOOC platforms. Existing methods fail to consider these implicit relations, and these relations themselves are difficult to learn and represent, causing poor performance in knowledge concept recommendation and an inability to meet users' personalized needs. To address this issue, we propose a novel framework based on contrastive learning, which can represent and balance the explicit and implicit relations for knowledge concept recommendation in MOOCs (CL-KCRec). Specifically, we first construct a MOOCs heterogeneous information network (HIN) by modeling the data from the MOOC platforms. Then, we utilize a relation-updated graph convolutional network and stacked multi-channel graph neural network to represent the explicit and implicit relations in the HIN, respectively. Considering that the quantity of explicit relations is relatively fewer compared to implicit relations in MOOCs, we propose a contrastive learning with prototypical graph to enhance the representations of both relations to capture their fruitful inherent relational knowledge, which can guide the propagation of students' preferences within the HIN. Based on these enhanced representations, to ensure the balanced contribution of both towards the final recommendation, we propose a dual-head attention mechanism for balanced fusion. Experimental results demonstrate that CL-KCRec outperforms several state-of-the-art baselines on real-world datasets in terms of HR, NDCG and MRR.|大型网络开放课程(MOOC)中的知识概念推荐是一个引起广泛关注的重要问题。现有的推荐方法主要依赖于 MOOC 平台上用户和知识概念之间的明确关系。然而，在 MOOC 平台上，用户的学习活动产生了大量的隐性关系(例如，用户之间的共同兴趣或相同的知识水平)。现有的方法没有考虑到这些隐含关系，而且这些关系本身难以学习和表示，导致知识概念推荐效果不佳，无法满足用户的个性化需求。针对这一问题，本文提出了一种基于对比学习的知识概念推荐框架(CL-KCRec) ，该框架能够表示和平衡 MOOC 中知识概念推荐的显性和隐性关系。具体来说，我们首先通过对 MOOC 平台上的数据进行建模来构建一个 MOOC 异构信息网络(HIN)。然后利用关系更新的图卷积网络和叠加的多通道图神经网络分别表示 HIN 中的显式关系和隐式关系。考虑到 MOOCs 中显性关系的数量相对于隐性关系的数量较少，我们提出了一种原型图的对比学习方法，以增强两种关系的表示，从而捕捉它们富有成效的内在关系知识，从而指导学生偏好在 HIN 中的传播。基于这些增强的表示，为了确保两者对最终建议的平衡贡献，我们提出了一种平衡融合的双头注意机制。实验结果表明，CL-KCRec 在 HR、 NDCG 和 MRR 方面优于现实世界数据集上的几个最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+Balanced+Explicit+and+Implicit+Relations+with+Contrastive+Learning+for+Knowledge+Concept+Recommendation+in+MOOCs)|0|
|[A Counterfactual Framework for Learning and Evaluating Explanations for Recommender Systems](https://doi.org/10.1145/3589334.3645560)|Oren Barkan, Veronika Bogina, Liya Gurevitch, Yuval Asher, Noam Koenigstein||In the field of recommender systems, explainability remains a pivotal yet challenging aspect. To address this, we introduce the Learning to eXplain Recommendations (LXR) framework, a post-hoc, model-agnostic approach designed for providing counterfactual explanations. LXR is compatible with any differentiable recommender algorithm and scores the relevance of user data in relation to recommended items. A distinctive feature of LXR is its use of novel self-supervised counterfactual loss terms, which effectively highlight the most influential user data responsible for a specific recommended item. Additionally, we propose several innovative counterfactual evaluation metrics specifically tailored for assessing the quality of explanations in recommender systems. Our code is available on our GitHub repository: https://github.com/DeltaLabTLV/LXR.|在推荐系统领域，可解释性仍然是一个关键但具有挑战性的方面。为了解决这个问题，我们介绍了学习解释建议(LXR)框架，这是一种事后的、与模型无关的方法，旨在提供反事实的解释。LXR 与任何可微推荐算法兼容，并对用户数据与推荐项目的相关性进行评分。LXR 的一个区辨特征是它使用了新颖的自我监督的反事实损失术语，它有效地突出了对特定推荐项目负责的最有影响力的用户数据。此外，我们还提出了几个创新的反事实评估指标，专门用于评估推荐系统中解释的质量。我们的代码可以在我们的 gitHub 存储库中找到:  https://GitHub.com/deltalabtlv/lxr。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Counterfactual+Framework+for+Learning+and+Evaluating+Explanations+for+Recommender+Systems)|0|
|[Category-based and Popularity-guided Video Game Recommendation: A Balance-oriented Framework](https://doi.org/10.1145/3589334.3645573)|Xiping Li, Jianghong Ma, Kangzhe Liu, Shanshan Feng, Haijun Zhang, Yutong Wang||In recent years, the video game industry has experienced substantial growth, presenting players with a vast array of game choices. This surge in options has spurred the need for a specialized recommender system tailored for video games. However, current video game recommendation approaches tend to prioritize accuracy over diversity, potentially leading to unvaried game suggestions. In addition, the existing game recommendation methods commonly lack the ability to establish strict connections between games to enhance accuracy. Furthermore, many existing diversity-focused methods fail to leverage crucial item information, such as item category and popularity during neighbor modeling and message propagation. To address these challenges, we introduce a novel framework, called CPGRec, comprising three modules, namely accuracy-driven, diversity-driven, and comprehensive modules. The first module extends the state-of-the-art accuracy-focused game recommendation method by connecting games in a more stringent manner to enhance recommendation accuracy. The second module connects neighbors with diverse categories within the proposed game graph and harnesses the advantages of popular game nodes to amplify the influence of long-tail games within the player-game bipartite graph, thereby enriching recommendation diversity. The third module combines the above two modules and employs a new negative-sample rating score reweighting method to balance accuracy and diversity. Experimental results on the Steam dataset demonstrate the effectiveness of our proposed method in improving game recommendations. The dataset and source codes are anonymously released at: https://github.com/CPGRec2024/CPGRec.git.|近年来，电子游戏产业经历了长足的发展，为玩家提供了大量的游戏选择。选项的激增促使人们需要一种专门为视频游戏量身定制的推荐系统。然而，目前的视频游戏推荐方法倾向于将准确性置于多样性之上，潜在地导致了不变的游戏推荐。此外，现有的游戏推荐方法普遍缺乏在游戏之间建立严格连接以提高准确性的能力。此外，在邻居建模和消息传播过程中，许多现有的基于多样性的方法未能充分利用关键项目信息，如项目类别和流行程度。为了应对这些挑战，我们引入了一个新的框架，称为 CPGRec，它由三个模块组成，即精度驱动模块、多样性驱动模块和综合模块。第一个模块扩展了最先进的以精确度为中心的游戏推荐方法，以更严格的方式连接游戏以提高推荐精确度。第二个模块连接所提出的博弈图中具有不同类别的邻居，利用流行博弈节点的优势放大长尾博弈在博弈者-博弈二部图中的影响，从而丰富推荐多样性。第三个模块将上述两个模块结合起来，采用一种新的负样本评分重新加权方法来平衡准确性和多样性。蒸汽数据集的实验结果证明了我们提出的方法在改进游戏推荐方面的有效性。数据集和源代码是匿名发布在:  https://github.com/cpgrec2024/cpgrec.git。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Category-based+and+Popularity-guided+Video+Game+Recommendation:+A+Balance-oriented+Framework)|0|
|[Unleashing the Power of Knowledge Graph for Recommendation via Invariant Learning](https://doi.org/10.1145/3589334.3645576)|Shuyao Wang, Yongduo Sui, Chao Wang, Hui Xiong||Knowledge graph (KG) demonstrates substantial potential for enhancing the performance of recommender systems. Due to its rich semantic content and associations among interactive entities, it can effectively alleviate inherent limitations in collaborative filtering (CF), such as data sparsity or cold-start issues. However, most existing knowledge-aware recommendation models indiscriminately aggregate all information in KG, without considering information specifically relevant to the recommendation task. Such indiscriminate aggregation could introduce additional noisy knowledge into representation learning, which can distort the understanding of users' genuine preferences, thereby sacrificing the recommendation quality. In this paper, we introduce the principle of invariance to the knowledge-aware recommendation, culminating in our Knowledge Graph Invariant Learning (KGIL) framework. It aims to discern and harness the task-relevant knowledge connections within KG to enhance the recommendation models. Specifically, we employ multiple environment generators to simulate diverse noisy KG-environments. Then we devise a novel attention learning mechanism for KG and user-item interaction graph, aiming to learn environment-invariant subgraphs. Leveraging an adversarial optimization strategy, we enhance the diversity of the environments, meanwhile, promote invariant representation learning across environments. We conduct extensive experiments on three datasets and compare KGIL with state-of-the-art methods. The experimental results further demonstrate the superiority of our approach.|知识图(KG)显示了提高推荐系统性能的巨大潜力。由于其丰富的语义内容和互动实体之间的关联，它可以有效地缓解协同过滤(CF)固有的局限性，例如数据稀疏或冷启动问题。然而，大多数现有的知识感知推荐模型不加区分地聚合幼儿园中的所有信息，而没有考虑与推荐任务具体相关的信息。这种不加区分的聚合可能会在表示学习中引入额外的噪声知识，从而扭曲对用户真实偏好的理解，从而牺牲推荐质量。在本文中，我们将不变性原理引入知识感知推荐中，最终形成我们的知识图不变性学习(KGIL)框架。它旨在识别和利用幼儿园内与任务相关的知识联系，以加强推荐模型。具体来说，我们使用多个环境生成器来模拟不同的噪声 KG 环境。然后针对 KG 和用户项目交互图提出了一种新的注意学习机制，旨在学习环境不变的子图。利用对抗优化策略，我们增强了环境的多样性，同时促进了环境间的不变表示学习。我们在三个数据集上进行了广泛的实验，并将 KGIL 与最先进的方法进行了比较。实验结果进一步证明了该方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unleashing+the+Power+of+Knowledge+Graph+for+Recommendation+via+Invariant+Learning)|0|
|[Distributionally Robust Graph-based Recommendation System](https://doi.org/10.1145/3589334.3645598)|Bohao Wang, Jiawei Chen, Changdong Li, Sheng Zhou, Qihao Shi, Yang Gao, Yan Feng, Chun Chen, Can Wang||With the capacity to capture high-order collaborative signals, Graph Neural Networks (GNNs) have emerged as powerful methods in Recommender Systems (RS). However, their efficacy often hinges on the assumption that training and testing data share the same distribution (a.k.a. IID assumption), and exhibits significant declines under distribution shifts. Distribution shifts commonly arises in RS, often attributed to the dynamic nature of user preferences or ubiquitous biases during data collection in RS. Despite its significance, researches on GNN-based recommendation against distribution shift are still sparse. To bridge this gap, we propose Distributionally Robust GNN (DR-GNN) that incorporates Distributional Robust Optimization (DRO) into the GNN-based recommendation. DR-GNN addresses two core challenges: 1) To enable DRO to cater to graph data intertwined with GNN, we reinterpret GNN as a graph smoothing regularizer, thereby facilitating the nuanced application of DRO; 2) Given the typically sparse nature of recommendation data, which might impede robust optimization, we introduce slight perturbations in the training distribution to expand its support. Notably, while DR-GNN involves complex optimization, it can be implemented easily and efficiently. Our extensive experiments validate the effectiveness of DR-GNN against three typical distribution shifts. The code is available at https://github.com/WANGBohaO-jpg/DR-GNN .|图形神经网络(GNN)具有捕获高阶协同信号的能力，已经成为推荐系统(RS)中的一种强有力的方法。然而，它们的有效性往往取决于训练和测试数据分布相同的假设(又称为 IID 假设) ，并且在分布变化下表现出显著的下降。分布偏移通常出现在 RS 中，通常归因于用户偏好的动态特性或 RS 中数据收集过程中普遍存在的偏差。尽管有其重要意义，但基于 GNN 的配电网分布移位推荐研究还很少。为了弥补这一差距，我们提出了分布式鲁棒 GNN (DR-GNN) ，将分布式鲁棒优化(DRO)结合到基于 GNN 的推荐中。DR-GNN 解决了两个核心挑战: 1)为了使 DRO 能够迎合与 GNN 交织在一起的图形数据，我们将 GNN 重新解释为一个图形平滑正则化器，从而促进 DRO 的微妙应用; 2)鉴于推荐数据的典型稀疏性质，这可能会阻碍稳健优化，我们在训练分布中引入轻微的扰动，以扩大其支持。值得注意的是，虽然 DR-GNN 涉及到复杂的优化，但它可以轻松有效地实现。我们的大量实验验证了 DR-GNN 对三种典型分布移位的有效性。密码可在 https://github.com/wangbohao-jpg/dr-gnn 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributionally+Robust+Graph-based+Recommendation+System)|0|
|[Co-clustering for Federated Recommender System](https://doi.org/10.1145/3589334.3645626)|Xinrui He, Shuo Liu, Jacky Keung, Jingrui He||As data privacy and security attract increasing attention, Federated Recommender System (FRS) offers a solution that strikes a balance between providing high-quality recommendations and preserving user privacy. However, the presence of statistical heterogeneity in FRS, commonly observed due to personalized decision-making patterns, can pose challenges. To address this issue and maximize the benefit of collaborative filtering (CF) in FRS, it is intuitive to consider clustering clients (users) as well as items into different groups and learning group-specific models. Existing methods either resort to client clustering via user representations-risking privacy leakage, or employ classical clustering strategies on item embeddings or gradients, which we found are plagued by the curse of dimensionality. In this paper, we delve into the inefficiencies of the K-Means method in client grouping, attributing failures due to the high dimensionality as well as data sparsity occurring in FRS, and propose CoFedRec, a novel Co-clustering Federated Recommendation mechanism, to address clients heterogeneity and enhance the collaborative filtering within the federated framework. Specifically, the server initially formulates an item membership from the client-provided item networks. Subsequently, clients are grouped regarding a specific item category picked from the item membership during each communication round, resulting in an intelligently aggregated group model. Meanwhile, to comprehensively capture the global inter-relationships among items, we incorporate an additional supervised contrastive learning term based on the server-side generated item membership into the local training phase for each client. Extensive experiments on four datasets are provided, which verify the effectiveness of the proposed CoFedRec.|随着数据隐私和安全越来越受到关注，联邦推荐系统(FRS)提供了一种在提供高质量建议和保护用户隐私之间取得平衡的解决方案。然而，FRS 中统计异质性的存在，通常由于个性化的决策模式而被观察到，可能带来挑战。为了解决这个问题，并最大限度地发挥协同过滤(CF)在财务报告系统中的作用，直观地考虑将客户(用户)以及项目分为不同的组和学习组特定的模型。现有的方法要么通过用户表示来进行客户聚类——这有泄露隐私的风险，要么在条目嵌入或渐变上采用传统的聚类策略，我们发现这些方法受到了维数灾难的困扰。在本文中，我们深入研究了 K-Means 方法在客户分组中的低效性，归因于 FRS 中出现的高维度和数据稀疏性导致的失败，并提出了一种新的协同聚类联邦推荐机制 CofedRec，以解决客户异构性问题，增强联邦框架中的协同过滤。具体来说，服务器最初从客户端提供的项目网络中规定项目成员关系。随后，在每个通信回合中，根据从项目成员中挑选的特定项目类别对客户进行分组，从而产生一个智能聚合的组模型。同时，为了全面捕捉项目之间的全局相互关系，我们在每个客户端的局部训练阶段加入了一个基于服务器端生成的项目成员关系的监督对比学习术语。在四个数据集上进行了大量的实验，验证了所提出的 CoFedRec 算法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Co-clustering+for+Federated+Recommender+System)|0|
|[PMG : Personalized Multimodal Generation with Large Language Models](https://doi.org/10.1145/3589334.3645633)|Xiaoteng Shen, Rui Zhang, Xiaoyan Zhao, Jieming Zhu, Xi Xiao||The emergence of large language models (LLMs) has revolutionized the capabilities of text comprehension and generation. Multi-modal generation attracts great attention from both the industry and academia, but there is little work on personalized generation, which has important applications such as recommender systems. This paper proposes the first method for personalized multimodal generation using LLMs, showcases its applications and validates its performance via an extensive experimental study on two datasets. The proposed method, Personalized Multimodal Generation (PMG for short) first converts user behaviors (e.g., clicks in recommender systems or conversations with a virtual assistant) into natural language to facilitate LLM understanding and extract user preference descriptions. Such user preferences are then fed into a generator, such as a multimodal LLM or diffusion model, to produce personalized content. To capture user preferences comprehensively and accurately, we propose to let the LLM output a combination of explicit keywords and implicit embeddings to represent user preferences. Then the combination of keywords and embeddings are used as prompts to condition the generator. We optimize a weighted sum of the accuracy and preference scores so that the generated content has a good balance between them. Compared to a baseline method without personalization, PMG has a significant improvement on personalization for up to 8|大型语言模型(LLM)的出现彻底改变了文本理解和生成的能力。多模态生成技术引起了业界和学术界的广泛关注，但在个性化生成技术方面的研究却很少，它在推荐系统等方面有着重要的应用。本文提出了第一种基于 LLM 的个性化多模态生成方法，通过对两个数据集的大量实验研究，展示了该方法的应用，并验证了该方法的性能。该方法首先将用户行为(如推荐系统中的点击或与虚拟助手的对话)转换为自然语言，以便于 LLM 理解和提取用户偏好描述。然后将这些用户首选项输入生成器，例如多模态 LLM 或扩散模型，以生成个性化内容。为了全面和准确地捕获用户偏好，我们建议让 LLM 输出显式关键字和隐式嵌入的组合来表示用户偏好。然后使用关键字和嵌入的组合作为提示来调整生成器。我们优化了准确性和偏好得分的加权和，以便生成的内容之间有一个良好的平衡。与没有个性化的基线方法相比，PMG 在个性化方面有显著的提高，最高可达8|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PMG+:+Personalized+Multimodal+Generation+with+Large+Language+Models)|0|
|[M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation](https://doi.org/10.1145/3589334.3645635)|Jiachen Zhu, Yichao Wang, Jianghao Lin, Jiarui Qin, Ruiming Tang, Weinan Zhang, Yong Yu||We primarily focus on the field of multi-scenario recommendation, which poses a significant challenge in effectively leveraging data from different scenarios to enhance predictions in scenarios with limited data. Current mainstream efforts mainly center around innovative model network architectures, with the aim of enabling the network to implicitly acquire knowledge from diverse scenarios. However, the uncertainty of implicit learning in networks arises from the absence of explicit modeling, leading to not only difficulty in training but also incomplete user representation and suboptimal performance. Furthermore, through causal graph analysis, we have discovered that the scenario itself directly influences click behavior, yet existing approaches directly incorporate data from other scenarios during the training of the current scenario, leading to prediction biases when they directly utilize click behaviors from other scenarios to train models. To address these problems, we propose the Multi-Scenario Causal-driven Adaptive Network M-scan). This model incorporates a Scenario-Aware Co-Attention mechanism that explicitly extracts user interests from other scenarios that align with the current scenario. Additionally, it employs a Scenario Bias Eliminator module utilizing causal counterfactual inference to mitigate biases introduced by data from other scenarios. Extensive experiments on two public datasets demonstrate the efficacy of our M-scan compared to the existing baseline models.|我们主要关注多方案建议领域，这对于有效利用不同方案的数据来增强数据有限的方案中的预测提出了重大挑战。目前的主流工作主要围绕创新型网络架构展开，目的是使网络能够从不同的情景中隐含地获取知识。然而，网络中隐式学习的不确定性源于显式建模的缺失，这不仅导致了训练的困难，而且还导致了用户表示的不完全性和性能的次优化。此外，通过因果图分析，我们发现场景本身直接影响点击行为，但现有的方法在当前场景的训练过程中直接合并来自其他场景的数据，导致预测偏差，当他们直接利用其他场景的点击行为来训练模型。为了解决这些问题，我们提出了多场景因果驱动的自适应网络 M 扫描算法。该模型结合了场景感知协同关注机制，该机制显式地从与当前场景一致的其他场景中提取用户兴趣。此外，它还使用了一个场景偏差消除器模块，该模块利用因果反事实推断来减轻其他场景数据引入的偏差。在两个公共数据集上的大量实验证明了我们的 M 扫描与现有的基线模型相比的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=M-scan:+A+Multi-Scenario+Causal-driven+Adaptive+Network+for+Recommendation)|0|
|[Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation](https://doi.org/10.1145/3589334.3645693)|Bo Yan, Yang Cao, Haoyu Wang, Wenchuan Yang, Junping Du, Chuan Shi||The heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has emerged as a potent tool for mitigating data sparsity in recommender systems. Existing HIN-based recommender systems operate under the assumption of centralized storage and model training. However, real-world data is often distributed due to privacy concerns, leading to the semantic broken issue within HINs and consequent failures in centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored on the client side and shared HINs on the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which facilitates collaborative training of a recommendation model using distributed HINs while protecting user privacy. Specifically, we first formalize the privacy definition for HIN-based federated recommendation (FedRec) in the light of differential privacy, with the goal of protecting user-item interactions within private HIN as well as users' high-order patterns from shared HINs. To recover the broken meta-path based semantics and ensure proposed privacy measures, we elaborately design a semantic-preserving user interactions publishing method, which locally perturbs user's high-order patterns and related user-item interactions for publishing. Subsequently, we introduce an HGNN model for recommendation, which conducts node- and semantic-level aggregations to capture recovered semantics. Extensive experiments on four datasets demonstrate that our model outperforms existing methods by a substantial margin (up to 34 reasonable privacy budget.|异构信息网络(HIN)包含丰富的元路径描述语义，已成为缓解推荐系统中数据稀疏性的有力工具。现有的基于 HIN 的推荐系统是在集中存储和模型训练的假设下运行的。然而，由于隐私问题，现实世界中的数据经常被分发，导致 HIN 内部的语义破碎问题以及随之而来的基于 HIN 的集中式推荐的失败。在本文中，我们建议将 HIN 划分为存储在客户端的私有 HIN 和在服务器端共享的 HIN。在此基础上，提出了一种基于联邦异构图神经网络(FedHGNN)的框架，该框架在保护用户隐私的同时，有利于使用分布式 HIN 对推荐模型进行协同训练。具体来说，我们首先根据差分隐私正式确定了基于 HIN 的联邦推荐(fed rec)的隐私定义，目的是保护私有 HIN 内的用户项交互以及共享 HIN 中用户的高阶模式。为了恢复破碎的基于元路径的语义，保证提出的隐私措施，我们精心设计了一种语义保持的用户交互发布方法，该方法局部扰动用户的高阶模式和相关的用户项交互发布。随后，我们引入了 HGNN 推荐模型，该模型通过节点级和语义级的聚合来捕获恢复的语义。对四个数据集的大量实验表明，我们的模型优于现有的方法大幅度(多达34合理的隐私预算。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Federated+Heterogeneous+Graph+Neural+Network+for+Privacy-preserving+Recommendation)|0|
|[Understanding Human Preferences: Towards More Personalized Video to Text Generation](https://doi.org/10.1145/3589334.3645711)|Yihan Wu, Ruihua Song, Xu Chen, Hao Jiang, Zhao Cao, Jin Yu||While previous video to text models have achieved remarkable successes, they mostly focus on how to understand the video contents in a general sense, but fail to capture the human personalized preferences, which is highly demanded for an engaging multimodal chatbots. Different from user modeling in collaborative filtering, there is no other user behaviors in inference as a real-time video stream is coming. In this paper, we formally define the task of personalized video commenting task and design an end-to-end personalized framework for solving this task. In specific, we argue that the personalization for video comment generation can be reflected in two aspects, that is, (1) for the same video, different users may comment on different clips, and (2) for the same clip, different people may also express various opinions with diverse commentary styles. Motivated by these considerations, we design our framework based on two components. The first one is a clip selector, which is responsible for predicting the clips that the user may comment in the video. The second one is a text generator, which aims to produce the comment based on the above predicted clips and the user's preference. In our framework, these two components are optimized in an end-to-end manner to mutually enhance each other, where we design confidence-aware scheduled sampling and iterative inference strategies to solve the problem that the ground truth clips are absent in the inference phase. As the absence of personalized video to text dataset, we collect and release a new dataset for studying this problem. We conduct extensive experiments to demonstrate the effectiveness of our model.|虽然以往的视频文本模型取得了显著的成功，但它们主要集中在如何理解一般意义上的视频内容，但未能捕捉到人类的个性化偏好，这是一个具有吸引力的多通道聊天机器人高度需求。与协同过滤中的用户建模不同，随着实时视频流的到来，没有其他用户行为的推断。本文正式定义了个性化视频评论任务的任务，并设计了一个端到端的个性化框架来解决这个任务。具体而言，我们认为视频评论生成的个性化可以体现在两个方面，即(1)对于同一个视频，不同的用户可以对不同的视频片段进行评论，(2)对于同一个视频片段，不同的人也可以用不同的评论风格表达不同的意见。基于这些考虑，我们设计了基于两个组件的框架。第一个是剪辑选择器，它负责预测用户可能在视频中发表评论的剪辑。第二个是文本生成器，其目的是根据上述预测剪辑和用户的偏好生成评论。在我们的框架中，这两个组件以端到端的方式进行优化，以相互增强，其中我们设计了可信度感知的计划采样和迭代推理策略，以解决推理阶段缺乏地面真相剪辑的问题。由于缺乏个性化的视频文本数据集，我们收集并发布了一个新的数据集来研究这个问题。我们进行了广泛的实验，以证明我们的模型的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+Human+Preferences:+Towards+More+Personalized+Video+to+Text+Generation)|0|
|[Entity Disambiguation with Extreme Multi-label Ranking](https://doi.org/10.1145/3589334.3645498)|JyunYu Jiang, WeiCheng Chang, Jiong Zhang, ChoJui Hsieh, HsiangFu Yu||Entity disambiguation is one of the most important natural language tasks to identify entities behind ambiguous surface mentions within a knowledge base. Although many recent studies apply deep learning to achieve decent results, they need exhausting pre-training and mediocre recall in the retrieval stage. In this paper, we propose a novel framework, eXtreme Multi-label Ranking for Entity Disambiguation (XMRED), to address this challenge. An efficient zero-shot entity retriever with auxiliary data is first pre-trained to recall relevant entities based on linear models. Specifically, the retrieval process can be considered as an extreme multi-label ranking (XMR) task. Entities are first clustered at different scales to form a label tree, thereby learning multi-scale entity retrievers over the label tree with high recall. Moreover, XMRED applies deep cross-encoder as a re-ranker to achieve high precision based on high-quality candidates. Extensive experimental results based on the AIDA-CoNLL benchmark and five zero-shot testing datasets demonstrate that XMRED obtains 98% and over 95% recall scores for in-domain and zero-shot datasets with top-10 retrieved entities. With a deep cross-encoder as the re-ranker, XMRED further outperforms the previous state-of-the-art by 1.74% in In-KB micro-F1 scores on average with a significant improvement on the training efficiency from days to 3.48 hours. In addition, XMRED also beats the state-of-the-art for page-level document retrieval by 2.38% in accuracy and 1.90% in recall@5.|实体消歧是识别知识库中模糊表面提及背后的实体的重要自然语言任务之一。虽然最近的许多研究应用深度学习来获得不错的结果，但是在检索阶段，他们需要耗费精力的预训练和平庸的回忆。在本文中，我们提出了一个新的框架，eXtreme 多标签实体消歧排序(XMRED) ，以解决这一挑战。首先对带有辅助数据的高效零拍实体检索器进行预训练，使其能够基于线性模型进行相关实体的检索。具体来说，检索过程可以看作是一个极端的多标签排序(XMR)任务。实体首先在不同的尺度上聚类，形成一个标签树，从而在标签树上学习具有高召回率的多尺度实体检索器。此外，XMRED 应用深交叉编码器作为一个重新排序，以实现高精度的基础上高质量的候选人。基于 AIDA-CoNLL 基准和五个零拍测试数据集的广泛实验结果表明，XMRED 获得了98% 和95% 以上的领域内和零拍数据集的回忆分数与前10名检索实体。随着深交叉编码器作为重新排名，XMRED 进一步优于以前的国家的最先进的1.74% 在 In-KB 微 F1分数平均有显着改善的训练效率从天到3.48小时。此外，XMRED 在准确率方面也比最先进的页面级文献检索高出2.38% ，在召回率方面高出1.90% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Entity+Disambiguation+with+Extreme+Multi-label+Ranking)|0|
|[BOND: Bootstrapping From-Scratch Name Disambiguation with Multi-task Promoting](https://doi.org/10.1145/3589334.3645580)|Yuqing Cheng, Bo Chen, Fanjin Zhang, Jie Tang||From-scratch name disambiguation is an essential task for establishing a reliable foundation for academic platforms. It involves partitioning documents authored by identically named individuals into groups representing distinct real-life experts. Canonically, the process is divided into two decoupled tasks: locally estimating the pairwise similarities between documents followed by globally grouping these documents into appropriate clusters. However, such a decoupled approach often inhibits optimal information exchange between these intertwined tasks. Therefore, we present BOND, which bootstraps the local and global informative signals to promote each other in an end-to-end regime. Specifically, BOND harnesses local pairwise similarities to drive global clustering, subsequently generating pseudo-clustering labels. These global signals further refine local pairwise characterizations. The experimental results establish BOND's superiority, outperforming other advanced baselines by a substantial margin. Moreover, an enhanced version, BOND+, incorporating ensemble and post-match techniques, rivals the top methods in the WhoIsWho competition.|从头开始消除名称歧义是为学术平台建立可靠基础的一项重要任务。它涉及到将由具有相同名称的个人编写的文档划分为代表不同的现实生活专家的组。规范地说，这个过程分为两个解耦的任务: 在本地估计文档之间的成对相似性，然后将这些文档全局分组到适当的集群中。然而，这种解耦的方法往往会抑制这些交织在一起的任务之间的最佳信息交换。因此，我们提出的 BOND，它引导本地和全球信息信号，以促进在一个端到端的制度。具体来说，BOND 利用局部成对的相似性来驱动全局聚类，随后生成伪聚类标签。这些全局信号进一步完善了局部成对特征。实验结果证实了 BOND 的优越性，大大优于其他高级基线。此外，一个加强版本，BOND + ，结合合奏和赛后技术，竞争对手的顶尖方法在 whoIsWho 比赛。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BOND:+Bootstrapping+From-Scratch+Name+Disambiguation+with+Multi-task+Promoting)|0|
|[Inductive Graph Alignment Prompt: Bridging the Gap between Graph Pre-training and Inductive Fine-tuning From Spectral Perspective](https://doi.org/10.1145/3589334.3645620)|Yuchen Yan, Peiyan Zhang, Zheng Fang, Qingqing Long||The "Graph pre-training and fine-tuning" paradigm has significantly improved Graph Neural Networks(GNNs) by capturing general knowledge without manual annotations for downstream tasks. However, due to the immense gap of data and tasks between the pre-training and fine-tuning stages, the model performance is still limited. Inspired by prompt fine-tuning in Natural Language Processing(NLP), many endeavors have been made to bridge the gap in graph domain. But existing methods simply reformulate the form of fine-tuning tasks to the pre-training ones. With the premise that the pre-training graphs are compatible with the fine-tuning ones, these methods typically operate in transductive setting. In order to generalize graph pre-training to inductive scenario where the fine-tuning graphs might significantly differ from pre-training ones, we propose a novel graph prompt based method called Inductive Graph Alignment Prompt(IGAP). Firstly, we unify the mainstream graph pre-training frameworks and analyze the essence of graph pre-training from graph spectral theory. Then we identify the two sources of the data gap in inductive setting: (i) graph signal gap and (ii) graph structure gap. Based on the insight of graph pre-training, we propose to bridge the graph signal gap and the graph structure gap with learnable prompts in the spectral space. A theoretical analysis ensures the effectiveness of our method. At last, we conduct extensive experiments among nodes classification and graph classification tasks under the transductive, semi-inductive and inductive settings. The results demonstrate that our proposed method can successfully bridge the data gap under different settings.|“图形预训练和微调”范例显著改善了图形神经网络(GNN) ，通过捕获一般知识而无需手动注释的下游任务。然而，由于预训练阶段和微调阶段之间数据和任务的巨大差距，模型的性能仍然是有限的。受到自然语言处理(NLP)中快速微调的启发，人们付出了许多努力来弥补图域中的差距。但现有的方法只是简单地将微调任务的形式重新表述为训练前的任务。在预训练图与微调图兼容的前提下，这些方法通常采用传导设置。为了将图的预训练推广到图的微调可能明显不同于预训练的情况，提出了一种基于图提示的归纳图对齐提示(IGAP)方法。首先，统一了主流的图预训练框架，从图谱理论的角度分析了图预训练的本质。然后，我们确定了归纳设置中数据间隙的两个来源: (i)图信号间隙和(ii)图结构间隙。基于图预训练的思想，我们提出在谱空间中用可学习的提示来桥接图信号间隙和图结构间隙。理论分析确保了我们方法的有效性。最后，在传导、半归纳和归纳环境下，对节点分类和图分类任务进行了广泛的实验。实验结果表明，该方法能够在不同设置下成功地消除数据间隙。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Inductive+Graph+Alignment+Prompt:+Bridging+the+Gap+between+Graph+Pre-training+and+Inductive+Fine-tuning+From+Spectral+Perspective)|0|
|[Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction](https://doi.org/10.1145/3589334.3645678)|Qi Sun, Kun Huang, Xiaocui Yang, Rong Tong, Kun Zhang, Soujanya Poria||Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in information systems that aims to simultaneously extract entities with semantic relations from a document. Existing methods heavily rely on a substantial amount of fully labeled data. However, collecting and annotating data for newly emerging relations is time-consuming and labor-intensive. Recent advanced Large Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text generation capabilities, inspiring us to explore an alternative approach for obtaining auto-labeled documents with new relations. In this paper, we propose a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework, which generates labeled data by retrieval and denoising knowledge from LLMs, called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step. To improve the quality of synthetic data, we propose a denoising strategy based on the consistency of cross-document knowledge. Leveraging our denoised synthetic data, we proceed to fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets. We perform experiments for both zero-shot document-level relation and triplet extraction on two public datasets. The experimental results illustrate that our GenRDK framework outperforms strong baselines.|文档级关系三联抽取(DocRTE)是信息系统中的一项基本任务，旨在从文档中同时抽取具有语义关系的实体。现有的方法严重依赖于大量完全标记的数据。然而，为新出现的关系收集和注释数据是耗费时间和劳动密集型的。最近的高级大语言模型(LLM) ，例如 ChatGPT 和 LLaMA，展示了令人印象深刻的长文本生成能力，启发我们探索一种替代方法来获得具有新关系的自动标记文档。本文提出了一种零拍文档级关系三联体提取(Zero-shot Document-level Relations Triplet 萃取，ZeroDocRTE)框架 GenRDK，该框架通过检索和去噪 LLM 中的知识来生成标记数据。具体来说，我们提出了一个检索链提示来指导 ChatGPT 逐步生成带标签的长文本数据。为了提高综合数据的质量，提出了一种基于跨文档知识一致性的去噪策略。利用我们去噪的合成数据，我们继续微调 LLaMA2-13B-Chat 以提取文档级关系三联体。我们在两个公共数据集上进行了零拍文档级关系和三元组提取的实验。实验结果表明，我们的 GenRDK 框架优于强基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Consistency+Guided+Knowledge+Retrieval+and+Denoising+in+LLMs+for+Zero-shot+Document-level+Relation+Triplet+Extraction)|0|
|[Detecting Illicit Food Factories from Chemical Declaration Data via Graph-aware Self-supervised Contrastive Anomaly Ranking](https://doi.org/10.1145/3589334.3648138)|ShengFang Yang, ChengTe Li||In the global food industry, where the line between legitimate and illicit manufacturing is increasingly blurred by the scale and complexity of the supply chain, safeguarding consumer health and trust necessitates innovative detection methods. Addressing this, this paper presents Graph-aware Self-supervised Contrastive Anomaly Ranking (GraphCAR), a novel unsupervised learning model, devised to identify illicit food factories through the scrutiny of chemical declaration data. GraphCAR tackles the scarcity of labeled data and the intricacies inherent in the vast array of declared chemicals, leveraging a Graph Autoencoder fused with a self-supervised contrastive learning mechanism. This fusion not only simplifies the feature space by embedding chemical declarations within a bipartite graph but also adeptly flags subtle, potentially illicit patterns through contrastively inspecting the learned factory representations. Through rigorous evaluations conducted on real-world factory's chemical declaration data, GraphCAR has demonstrated superior performance over conventional methods on unsupervised outlier detection and one-class classification tasks, showcasing its accuracy, robustness and reliability in flagging potential malpractice. With its successful application in food safety, GraphCAR stands as a testament to the potential of AI-driven solutions to address multifaceted challenges for the greater good.|在全球食品工业中，由于供应链的规模和复杂性，合法和非法制造之间的界限日益模糊，因此，为了保障消费者的健康和信任，必须采用创新的检测方法。针对这一问题，本文提出了一种新的非监督式学习模型——图形感知自监督对比异常排名(GraphCAR) ，该模型旨在通过审查化学品申报数据来识别非法食品工厂。GraphCAR 解决了标记数据的稀缺性和大量申报化学品中固有的复杂性，利用了融合了自我监督对比学习机制的 Graph Autoencoder。这种融合不仅通过在二分图中嵌入化学声明来简化特征空间，而且还通过对比检查学习到的工厂表示来熟练地标记微妙的、潜在的非法模式。通过对真实世界工厂的化学品申报数据进行严格的评估，GraphCAR 在无监督的异常检测和一类分类任务方面显示出优于传统方法的性能，在标记潜在违规行为方面显示出其准确性、稳健性和可靠性。随着它在食品安全方面的成功应用，GraphCAR 证明了人工智能驱动的解决方案在解决多方面挑战以实现更大利益方面的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Illicit+Food+Factories+from+Chemical+Declaration+Data+via+Graph-aware+Self-supervised+Contrastive+Anomaly+Ranking)|0|
|[Bayesian Iterative Prediction and Lexical-based Interpretation for Disturbed Chinese Sentence Pair Matching](https://doi.org/10.1145/3589334.3648149)|Muzhe Guo, Muhao Guo, Juntao Su, Junyu Chen, Jiaqian Yu, Jiaqi Wang, Hongfei Du, Parmanand Sahu, Ashwin Assysh Sharma, Fang Jin||In an era dominated by web-based intelligent customer services, the applications of Sentence Pair Matching are profoundly broad. Web agents, for example, automatically respond to customer queries by finding similar past questions, significantly reducing customer service expenses. While current large language models (LLMs) offer powerful text generation capabilities, they often struggle with opacity, potential text toxicity, and difficulty managing domain-specific and confidential business inquiries. Consequently, the widespread adoption of web-based intelligent customer services in real-world business still greatly relies on query-based interactions. In this paper, we introduce a series of model-agnostic techniques aimed at enhancing both the accuracy and interpretability of Chinese pairwise sentence-matching models. Our contributions include (1) An Edit-distance-weighted fine-tuning method, (2) A Bayesian Iterative Prediction algorithm, (3) A Lexical-based Dual Ranking Interpreter, and (4) A Bi-criteria Denoising strategy. Experimental results on the Large-scale Chinese Question Matching Corpus (LCQMC) with a disturbed test demonstrate that our fine-tuning and prediction methods can steadily improve matching accuracy, building on the current state-of-the-art models. Besides, our interpreter with denoising strategy markedly enhances token-level interpretation in rationality and loyalty. In both matching accuracy and interpretation, our approaches outperform classic methods and even LLMs.|在一个以基于网络的智能客户服务为主导的时代，句子配对的应用是非常广泛的。例如，Web 代理会自动响应客户查询，寻找过去相似的问题，从而大大减少客户服务费用。虽然目前的大型语言模型(LLM)提供了强大的文本生成功能，但是它们经常会遇到不透明性、潜在的文本毒性以及难以管理特定领域和保密业务查询等问题。因此，基于 Web 的智能客户服务在现实商业中的广泛应用仍然在很大程度上依赖于基于查询的交互。本文介绍了一系列模型不可知技术，旨在提高汉语成对句子匹配模型的准确性和可解释性。我们的贡献包括(1)编辑距离加权微调方法，(2)贝叶斯迭代预测算法，(3)基于词汇的双排序解释器，和(4)双准则去噪策略。大规模中文问题匹配语料库的实验结果显示，我们的微调和预测方法可以在现有先进模型的基础上稳步提高匹配的准确性。此外，采用去噪策略的口译员显著提高了理性和忠诚度的象征层次解释。在匹配精度和解释方面，我们的方法优于经典方法甚至 LLM。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bayesian+Iterative+Prediction+and+Lexical-based+Interpretation+for+Disturbed+Chinese+Sentence+Pair+Matching)|0|
|[Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video Recommendation](https://doi.org/10.1145/3589334.3648159)|Nicholas Sukiennik, Chen Gao, Nian Li||Filter bubbles have been studied extensively within the context of online content platforms due to their potential to cause undesirable outcomes such as user dissatisfaction or polarization. With the rise of short-video platforms, the filter bubble has been given extra attention because these platforms rely on an unprecedented use of the recommender system to provide relevant content. In our work, we investigate the deep filter bubble, which refers to the user being exposed to narrow content within their broad interests. We accomplish this using one-year interaction data from a top short-video platform in China, which includes hierarchical data with three levels of categories for each video. We formalize our definition of a "deep" filter bubble within this context, and then explore various correlations within the data: first understanding the evolution of the deep filter bubble over time, and later revealing some of the factors that give rise to this phenomenon, such as specific categories, user demographics, and feedback type. We observe that while the overall proportion of users in a filter bubble remains largely constant over time, the depth composition of their filter bubble changes. In addition, we find that some demographic groups that have a higher likelihood of seeing narrower content and implicit feedback signals can lead to less bubble formation. Finally, we propose some ways in which recommender systems can be designed to reduce the risk of a user getting caught in a bubble.|由于过滤泡有可能导致用户不满或两极分化等不良后果，因此在在线内容平台的背景下，人们对过滤泡进行了广泛的研究。随着短视频平台的兴起，过滤器泡沫受到了额外的关注，因为这些平台前所未有地依赖推荐系统来提供相关内容。在我们的工作中，我们研究了深层过滤气泡，这是指用户暴露在狭窄的内容在他们的广泛兴趣。我们使用来自中国顶级短视频平台的为期一年的交互数据来实现这一点，该平台包括每个视频的三个层次的分类数据。在这个背景下，我们正式定义了“深度”过滤泡沫的定义，然后探索数据中的各种相关性: 首先理解深度过滤泡沫随时间的演变，然后揭示引起这种现象的一些因素，如特定类别、用户人口统计学和反馈类型。我们观察到，虽然过滤泡中用户的总体比例在很大程度上随着时间的推移保持不变，但他们的过滤泡的深度组成发生了变化。此外，我们发现，一些人口统计学群体，有较高的可能性看到较窄的内容和隐含的反馈信号，可以导致较少的泡沫形成。最后，我们提出了一些设计推荐系统的方法，以减少用户陷入泡沫的风险。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncovering+the+Deep+Filter+Bubble:+Narrow+Exposure+in+Short-Video+Recommendation)|0|
|[Mining Interest Diffusion in Online Activity Data Streams](https://doi.org/10.1145/3589335.3651259)|Shingo Higashiguchi||Modeling and forecasting such data is difficult because online activity data is high-dimensional and composed of multiple time-varying dynamics such as trends, seasonality, and diffusion of interest. In this paper, we propose D-Tracker, designed to capture latent dynamics in online activity data streams and forecast future values. Our proposed method has the following properties: (a) Interpretable: it uses interpretable differential equations to model the latent dynamics in online activity data, which enables us to capture trends and interest diffusion among locations; (b) Automatic: it determines the number of latent dynamics and the number of seasonal patterns fully automatically; (c) Scalable: it incrementally and adaptively detects shifting points of patterns for a semi-infinite collection of tensor streams. (c)Scalable : the computation time of D-Tracker is independent of the time series length. Experiments using web search volume data obtained from GoogleTrends show that the proposed method can achieve higher forecasting accuracy in less computation time than existing methods while extracting the patterns of interest diffusion among locations.|建模和预测这样的数据是困难的，因为在线活动数据是高维的，由多种时变动态组成，如趋势、季节性和兴趣的扩散。在本文中，我们提出 D-Tracker，旨在捕捉在线活动数据流中的潜在动态，并预测未来的价值。我们提出的方法具有以下特性: (a)可解释性: 它使用可解释的微分方程模拟在线活动数据中的潜在动态，这使我们能够捕获趋势和兴趣在位置之间的扩散; (b)自动化: 它完全自动地确定潜在动态的数量和季节性模式的数量; (c)可扩展性: 它增量和自适应地检测模式的移动点的张量流的半无限集合。(c)可伸缩性: D-Tracker 的计算时间与时间序列长度无关。利用从 GoogleTrends 获得的网络搜索量数据进行的实验表明，与现有方法相比，提出的方法能够以更少的计算时间获得更高的预测精度，同时提取地点之间的兴趣扩散模式。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mining+Interest+Diffusion+in+Online+Activity+Data+Streams)|0|
|[Temporal Interest Network for User Response Prediction](https://doi.org/10.1145/3589335.3648340)|Haolin Zhou, Junwei Pan, Xinyi Zhou, Xihua Chen, Jie Jiang, Xiaofeng Gao, Guihai Chen||User response prediction is essential in industrial recommendation systems, such as online display advertising. Among all the features in recommendation models, user behaviors are among the most critical. Many works have revealed that a user's behavior reflects her interest in the candidate item, owing to the semantic or temporal correlation between behaviors and the candidate. While the literature has individually examined each of these correlations, researchers have yet to analyze them in combination, that is, the semantic-temporal correlation. We empirically measure this correlation and observe intuitive yet robust patterns. We then examine several popular user interest models and find that, surprisingly, none of them learn such correlation well. To fill this gap, we propose a Temporal Interest Network (TIN) to capture the semantic-temporal correlation simultaneously between behaviors and the target. We achieve this by incorporating target-aware temporal encoding, in addition to semantic encoding, to represent behaviors and the target. Furthermore, we conduct explicit 4-way interaction by deploying target-aware attention and target-aware representation to capture both semantic and temporal correlation. We conduct comprehensive evaluations on two popular public datasets, and our proposed TIN outperforms the best-performing baselines by 0.43 GAUC, respectively. During online A/B testing in Tencent's advertising platform, TIN achieves 1.65 It has been successfully deployed in production since October 2023, serving the WeChat Moments traffic. We have released our code at https://github.com/zhouxy1003/TIN.|用户反应预测在工业推荐系统中是必不可少的，例如在线显示广告。在推荐模型的所有特性中，用户行为是最关键的。许多研究表明，由于行为和候选项之间的语义或时间相关性，用户的行为反映了她对候选项的兴趣。虽然文献已经分别检查了这些相关性，但研究人员还没有对它们进行组合分析，即语义-时间相关性。我们通过经验测量这种相关性，并观察直观但稳健的模式。然后，我们检查了几个流行的用户兴趣模型，发现令人惊讶的是，它们都没有很好地学习这种相关性。为了填补这一空白，我们提出了一个时态兴趣网络(TIN)来同时捕获行为和目标之间的语义-时态关联。我们通过结合目标感知的时间编码，以及语义编码，来表示行为和目标。此外，我们通过部署目标感知注意和目标感知表征来捕捉语义和时间的相关性，进行显性的四向交互。我们对两个流行的公共数据集进行全面的评估，我们提出的 TIN 表现优于最佳基线0.43 GAUC，分别。在腾讯的广告平台进行的在线 A/B 测试中，TIN 达到1.65。自2023年10月起，TIN 已成功投入生产，服务于微信朋友圈的流量。我们已经在 https://github.com/zhouxy1003/tin 发布了代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Temporal+Interest+Network+for+User+Response+Prediction)|0|
|[Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation](https://doi.org/10.1145/3589335.3648347)|Zezhong Zhang, Ted Tao Yuan||To speed up online testing, adaptive traffic experimentation through multi-armed bandit algorithms is rising as an essential complementary alternative to the fixed horizon A/B testing. Based on recent research on best arm identification and statistical inference with adaptively collected data, this paper derives and evaluates four Bayesian batch bandit algorithms (NB-TS, WB-TS, NB-TTTS, WB-TTTS), which are combinations of two ways of weighting batches (Naive Batch and Weighted Batch) and two Bayesian sampling strategies (Thompson Sampling and Top-Two Thompson Sampling) to adaptively determine traffic allocation. These derived Bayesian sampling algorithms are practically based on summary batch statistics of a reward metric for pilot experiments, where one of the combination WB-TTTS in this paper seems to be newly discussed. The comprehensive evaluation on the four Bayesian sampling algorithms covers trustworthiness, sensitivity and regret of a testing methodology. Moreover, the evaluation includes 4 real-world eBay experiments and 40 reproducible synthetic experiments to reveal the learnings, which covers both stationary and non-stationary situations. Our evaluation reveals that, (a) There exist false positives inflation with equivalent best arms, while seldom discussed in literatures; (b) To control false positives, connections between convergence of posterior optimal probabilities and neutral posterior reshaping are discovered; (c) WB-TTTS shows competitive recall, higher precision, and robustness against non-stationary trend; (d) NB-TS outperforms on minimizing regret trials except on precision and robustness; (e) WB-TTTS is a promising alternative if regret of A/B Testing is affordable, otherwise NB-TS is still a powerful choice with regret consideration for pilot experiments.|为了加速在线测试，通过多臂老虎机算法进行的自适应流量实验正在兴起，成为固定视野 A/B 测试的一个重要补充。基于最佳手臂识别和自适应采集数据推论统计学的最新研究，本文推导并评估了4种贝叶斯批处理盗贼算法(NB-TS，WB-TS，NB-ttTS，WB-ttTS) ，它们是两种加权批处理(朴素批处理和加权批处理)和两种贝叶斯抽样策略(汤普森抽样和前两汤普森抽样)的组合，以自适应确定流量分配。这些推导出的贝叶斯抽样算法实际上是基于飞行试验奖励度量的总结批量统计，其中 WB-TTTS 组合似乎是本文新的讨论之一。对四种贝叶斯抽样算法的综合评价包括测试方法的可信度、敏感度和遗憾度。此外，评估包括4个真实世界的 eBay 实验和40个可重复的综合实验，以揭示学习，其中包括静态和非静态情况。我们的评估表明，(a)存在假阳性通货膨胀与等效的最佳武器，而很少在文献中讨论; (b)为了控制假阳性，收敛的后验最优概率和中性后验重塑之间的联系被发现; (c) WB-TTTS 显示竞争性召回，更高的精度，和对非平稳趋势的鲁棒性; (d) NB-TS 优于最小化遗憾试验，除了在精度和鲁棒性; (e) WB-TTTS 是一个有前途的替代选择，如果遗憾的 A/B 测试是可负担的，否则 NB-TS 仍然是一个强大的选择，遗憾的考虑试点实验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Practical+Batch+Bayesian+Sampling+Algorithms+for+Online+Adaptive+Traffic+Experimentation)|0|
|[Exploring Representational Similarity Analysis to Protect Federated Learning from Data Poisoning](https://doi.org/10.1145/3589335.3651503)|Gengxiang Chen, Kai Li, Ahmed M. Abdelmoniem, Linlin You||As a paradigm that preserves privacy, Federated Learning (FL) enables distributed clients to cooperatively train global models using local datasets. However, this approach also provides opportunities for adversaries to compromise system stability by contaminating local data, such as through Label-Flipping Attacks (LFAs). In addressing these security challenges, most existing defense strategies presume the presence of an independent and identically distributed (IID) environment, resulting in suboptimal performance under Non-IID conditions. This paper introduces RSim-FL, a novel and pragmatic defense mechanism that incorporates Representational Similarity Analysis (RSA) into the detection of malevolent updates. This is achieved by calculating the similarity between uploaded local models and the global model. The evaluation, conducted against five state-of-the-art baselines, demonstrates that RSim-FL can accurately identify malicious local models and effectively mitigate divergent Label-Flipping Attacks (LFAs) in a Non-IID setting.|作为一种保护隐私的范例，联邦学习(Federated Learning，FL)使分布式客户机能够使用本地数据集合合作地训练全局模型。然而，这种方法也为对手提供了通过污染本地数据(如通过标签翻转攻击(Label-Flip Attacks，LFA))来损害系统稳定性的机会。为了应对这些安全挑战，大多数现有的防御策略都假定存在一个独立的同分布(IID)环境，从而导致在非 IID 条件下性能不理想。本文介绍了一种新的语用防御机制 RSim-FL，它将表征相似性分析(RSA)引入到恶意更新的检测中。这是通过计算上传的局部模型和全局模型之间的相似度来实现的。针对五个最先进的基线进行的评估表明，RSim-FL 能够准确识别恶意的本地模型，并在非 IID 设置中有效地缓解不同的标签翻转攻击(Label-Flipping Attacks，LFA)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Representational+Similarity+Analysis+to+Protect+Federated+Learning+from+Data+Poisoning)|0|
|[Online Sampling of Summaries from Public SPARQL Endpoints](https://doi.org/10.1145/3589335.3651543)|Thi Hoang Thi Pham, Hala SkafMolli, Pascal Molli, Brice Nédelec||Collecting statistics from online public SPARQL endpoints is hampered by their fair usage policies. These restrictions hinder several critical operations, such as aggregate query processing, portal development, and data summarization. Online sampling enables the collection of statistics while respecting fair usage policies. However, sampling has not yet been integrated into the SPARQL standard. Although integrating sampling into the SPARQL standard appears beneficial, its effectiveness must be demonstrated in a practical semantic web context. This paper investigates whether online sampling can generate summaries useful in cutting-edge SPARQL federation engines. Our experimental studies indicate that sampling allows the creation and maintenance of summaries by exploring less than 20% of datasets.|从在线公共 SPARQL 端点收集统计数据受到公平使用政策的阻碍。这些限制阻碍了一些关键操作，例如聚合查询处理、门户开发和数据汇总。在线抽样能够在尊重公平使用政策的同时收集统计数据。但是，抽样尚未集成到 SPARQL 标准中。尽管将抽样集成到 SPARQL 标准中似乎是有益的，但是它的有效性必须在实际的语义 Web 上下文中得到证明。本文研究在线抽样是否能够生成有用的摘要在尖端 SPARQL 联邦引擎。我们的实验研究表明，抽样允许通过探索少于20% 的数据集来创建和维护摘要。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Sampling+of+Summaries+from+Public+SPARQL+Endpoints)|0|
|[Coordinated Activity Modulates the Behavior and Emotions of Organic Users: A Case Study on Tweets about the Gaza Conflict](https://doi.org/10.1145/3589335.3651483)|Priyanka Dey, Luca Luceri, Emilio Ferrara||Social media has become a crucial conduit for the swift dissemination of information during global crises. However, this also paves the way for the manipulation of narratives by malicious actors. This research delves into the interaction dynamics between coordinated (malicious) entities and organic (regular) users on Twitter amidst the Gaza conflict. Through the analysis of approximately 3.5 million tweets from over 1.3 million users, our study uncovers that coordinated users significantly impact the information landscape, successfully disseminating their content across the network: a substantial fraction of their messages is adopted and shared by organic users. Furthermore, the study documents a progressive increase in organic users' engagement with coordinated content, which is paralleled by a discernible shift towards more emotionally polarized expressions in their subsequent communications. These results highlight the critical need for vigilance and a nuanced understanding of information manipulation on social media platforms.|社交媒体已成为在全球危机期间迅速传播信息的重要渠道。然而，这也为恶意行为者操纵叙述铺平了道路。这项研究深入探讨了在加沙冲突中协调(恶意)实体和有机(常规) Twitter 用户之间的互动动态。通过对超过130万用户的大约350万条推文的分析，我们的研究发现，协调的用户显著地影响了信息景观，成功地在网络上传播他们的内容: 他们的信息的很大一部分被有机用户采用和分享。此外，该研究还记录了有机用户参与协调内容的逐渐增加，与此同时，他们在随后的交流中明显地转向更加情绪化的表达方式。这些结果突出表明，对于社交媒体平台上的信息操纵活动，亟需保持警惕，并有细致入微的理解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Coordinated+Activity+Modulates+the+Behavior+and+Emotions+of+Organic+Users:+A+Case+Study+on+Tweets+about+the+Gaza+Conflict)|0|
|[From Files to Streams: Revisiting Web History and Exploring Potentials for Future Prospects](https://doi.org/10.1145/3589335.3652001)|Lucas Vogel, Thomas Springer, Matthias Wählisch||Over the last 30 years, the World Wide Web has changed significantly. In this paper, we argue that common practices to prepare web pages for delivery conflict with many efforts to present content with minimal latency, one fundamental goal that pushed changes in the WWW. To bolster our arguments, we revisit reasons that led to changes of HTTP and compare them systematically with techniques to prepare web pages. We found that the structure of many web pages leverages features of HTTP/1.1 but hinders the use of recent HTTP features to present content quickly. To improve the situation in the future, we propose fine-grained content segmentation. This would allow to exploit streaming capabilities of recent HTTP versions and to render content as quickly as possible without changing underlying protocols or web browsers.|在过去的30年里，万维网发生了巨大的变化。在本文中，我们认为，准备交付网页的常见做法与以最小延迟呈现内容的许多努力相冲突，延迟是推动 WWW 变化的一个基本目标。为了支持我们的论点，我们重新审视导致 HTTP 变化的原因，并将其与准备网页的技术进行系统的比较。我们发现许多网页的结构利用了 HTTP/1.1的特性，但是阻碍了使用最新的 HTTP 特性来快速显示内容。为了改善未来的情况，我们提出了细粒度内容分割。这将允许利用最新 HTTP 版本的流功能，并在不改变底层协议或 Web 浏览器的情况下尽可能快地呈现内容。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Files+to+Streams:+Revisiting+Web+History+and+Exploring+Potentials+for+Future+Prospects)|0|
|[Revisiting the Behavioral Foundations of User Modeling Algorithms](https://doi.org/10.1145/3589334.3649114)|Jon M. Kleinberg||One of the fundamental problems that platform algorithms face is the process of inferring user preferences from observed behavior; the vast amounts of data a platform collects become much less useful if they cannot effectively inform this type of inference. Traditional approaches to this problem rely on an often unstated revealed-preference assumption: that choice reveals preference. Yet a long line of work in psychology and behavioral economics reveals the gaps that can open up between choice and preference, and experience with platform dynamics makes clear how it can arise in some of the most basic online settings; for example, we might choose content to consume in the present and then later regret the time we spent on it. More generally, behavioral biases and inconsistent preferences make it highly challenging to appropriately interpret the user data that we observe. We discuss a set of models and algorithms that address this challenge through a process of "inversion", in which an algorithm must try inferring mental states that are not directly measured in the data. The talk is based on joint work with Jens Ludwig, Sendhil Mullainathan, and Manish Raghavan.|平台算法面临的一个基本问题是根据观察到的行为推断用户偏好的过程; 如果平台收集的大量数据不能有效地提供这种推断，那么它们就变得不那么有用了。解决这个问题的传统方法依赖于一个通常未说明的显示偏好假设: 这种选择显示偏好。然而，心理学和行为经济学的一长串研究揭示了选择和偏好之间的差距，平台动态的经验清楚地表明，在一些最基本的网络环境中，这种差距是如何产生的; 例如，我们可能会选择目前消费的内容，然后后悔花时间在这上面。更一般地说，行为偏差和不一致的偏好使得适当地解释我们观察到的用户数据变得非常具有挑战性。我们讨论了一组模型和算法，通过一个“反转”的过程来解决这个问题，在这个过程中，算法必须尝试推断心理状态，这些心理状态不是直接测量的数据。这次演讲是基于与延斯 · 路德维希、森德希尔穆拉伊特丹和马尼什 · 拉加万的合作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+the+Behavioral+Foundations+of+User+Modeling+Algorithms)|0|
|[User Response in Ad Auctions: An MDP Formulation of Long-term Revenue Optimization](https://doi.org/10.1145/3589334.3645495)|Yang Cai, Zhe Feng, Christopher Liaw, Aranyak Mehta, Grigoris Velegkas||We propose a new Markov Decision Process (MDP) model for ad auctions to capture the user response to the quality of ads, with the objective of maximizing the long-term discounted revenue. By incorporating user response, our model takes into consideration all three parties involved in the auction (advertiser, auctioneer, and user). The state of the user is modeled as a user-specific click-through rate (CTR) with the CTR changing in the next round according to the set of ads shown to the user in the current round. We characterize the optimal mechanism for this MDP as a Myerson's auction with a notion of modified virtual value, which relies on the value distribution of the advertiser, the current user state, and the future impact of showing the ad to the user. Leveraging this characterization, we design a sample-efficient and computationally-efficient algorithm which outputs an approximately optimal policy that requires only sample access to the true MDP and the value distributions of the bidders. Finally, we propose a simple mechanism built upon second price auctions with personalized reserve prices and show it can achieve a constant-factor approximation to the optimal long term discounted revenue.|我们提出了一种新的马可夫决策过程拍卖模型来捕捉用户对广告质量的反应，目标是最大化长期折扣收入。通过合并用户反应，我们的模型考虑了所有参与拍卖的三方(广告商、拍卖商和用户)。用户的状态被建模为一个用户特定的点进率(ctrR) ，ctrR 在下一轮中根据当前轮中显示给用户的广告集发生变化。我们将这个 MDP 的最优机制描述为一个 Myerson 拍卖，它具有修改后的虚拟价值的概念，依赖于广告商的价值分布、当前用户状态以及向用户展示广告的未来影响。利用这个角色塑造，我们设计了一个样本效率和计算效率高的算法，它输出一个近似最优的策略，只需要对真正的 MDP 和投标人的价值分布进行样本访问。最后，我们提出了一个基于个性化保留价格的第二价格拍卖的简单机制，并证明了该机制可以实现对最优长期折扣收益的常数逼近。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User+Response+in+Ad+Auctions:+An+MDP+Formulation+of+Long-term+Revenue+Optimization)|0|
|[Exploring Neural Scaling Law and Data Pruning Methods For Node Classification on Large-scale Graphs](https://doi.org/10.1145/3589334.3645571)|Zhen Wang, Yaliang Li, Bolin Ding, Yule Li, Zhewei Wei||Recently, how the model performance scales with the training sample size has been extensively studied for large models on vision and language related domains. Nevertheless, the ubiquitous node classification tasks on web-scale graphs were ignored, where the traits of these tasks, such as non-IIDness and transductive setting, are likely to cause different scaling laws and motivate novel techniques to beat the law. Therefore, we first explore the neural scaling law for node classification tasks on three large-scale graphs. Then, we benchmark several state-of-the-art data pruning methods on these tasks, not only validating the possibility of improving the original unsatisfactory power law but also gaining insights into a hard-and-representative principle on picking an effective subset of training nodes. Moreover, we leverage the transductive setting to propose a novel data pruning method, which instantiates our principle in a test set-targeted manner. Our method consistently outperforms related methods on all three datasets. Meanwhile, we utilize a PAC-Bayesian framework to analyze our method, extending prior results to account for both hardness and representativeness. In addition to a promising way to ease GNN training on web-scale graphs, our study offers knowledge of the relationship between training nodes and GNN generalization.|近年来，在视觉和语言相关领域的大型模型中，模型性能如何随训练样本量的变化而变化的问题得到了广泛的研究。然而，网络尺度图上普遍存在的节点分类任务被忽视了，这些任务的特点，如非 IID 性和传导性设置，可能会导致不同的尺度规律，并激励新技术打破这一规律。因此，我们首先探讨了三个大规模图上节点分类任务的神经尺度规律。然后，我们对这些任务的几种最先进的数据剪枝方法进行了基准测试，不仅验证了改进原始不满意幂律的可能性，而且深入了解了挑选有效训练节点子集的硬性和代表性原则。此外，我们利用传导设置提出了一种新的数据剪枝方法，它以测试集目标的方式实例化我们的原则。我们的方法在所有三个数据集上的性能始终优于相关方法。同时，我们利用 PAC-Bayes 框架来分析我们的方法，扩展了先前的结果来考虑硬度和代表性。除了一个有希望的方法来简化网络尺度图的 GNN 训练，我们的研究提供了训练节点和 GNN 推广之间的关系的知识。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Neural+Scaling+Law+and+Data+Pruning+Methods+For+Node+Classification+on+Large-scale+Graphs)|0|
|[Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised Semantic Hashing](https://doi.org/10.1145/3589334.3645440)|Liyang He, Zhenya Huang, Jiayu Liu, Enhong Chen, Fei Wang, Jing Sha, Shijin Wang||Unsupervised semantic hashing has emerged as an indispensable technique for fast image search, which aims to convert images into binary hash codes without relying on labels. Recent advancements in the field demonstrate that employing large-scale backbones (e.g., ViT) in unsupervised semantic hashing models can yield substantial improvements. However, the inference delay has become increasingly difficult to overlook. Knowledge distillation provides a means for practical model compression to alleviate this delay. Nevertheless, the prevailing knowledge distillation approaches are not explicitly designed for semantic hashing. They ignore the unique search paradigm of semantic hashing, the inherent necessities of the distillation process, and the property of hash codes. In this paper, we propose an innovative Bit-mask Robust Contrastive knowledge Distillation (BRCD) method, specifically devised for the distillation of semantic hashing models. To ensure the effectiveness of two kinds of search paradigms in the context of semantic hashing, BRCD first aligns the semantic spaces between the teacher and student models through a contrastive knowledge distillation objective. Additionally, to eliminate noisy augmentations and ensure robust optimization, a cluster-based method within the knowledge distillation process is introduced. Furthermore, through a bit-level analysis, we uncover the presence of redundancy bits resulting from the bit independence property. To mitigate these effects, we introduce a bit mask mechanism in our knowledge distillation objective. Finally, extensive experiments not only showcase the noteworthy performance of our BRCD method in comparison to other knowledge distillation methods but also substantiate the generality of our methods across diverse semantic hashing models and backbones. The code for BRCD is available at https://github.com/hly1998/BRCD.|无监督语义哈希已经成为快速图像搜索不可或缺的技术，其目的是在不依赖标签的情况下将图像转换为二进制哈希码。该领域的最新进展表明，在无监督语义哈希模型中使用大规模骨干(例如，ViT)可以产生实质性的改进。然而，推理延迟已变得越来越难以忽视。知识提取为实际的模型压缩提供了一种方法来减少这种延迟。然而，流行的知识提取方法并没有明确地为语义哈希设计。它们忽略了语义哈希的独特搜索范式、蒸馏过程的内在必要性以及哈希码的性质。本文提出了一种新颖的位掩码鲁棒对比知识提取(BRCD)方法，专门用于提取语义哈希模型。为了保证两种搜索范式在语义哈希环境下的有效性，BRCD 首先通过对比知识提取目标对教师和学生模型之间的语义空间进行对齐。此外，为了消除噪声增量，确保鲁棒优化，在知识精馏过程中引入了一种基于聚类的方法。此外，通过位级分析，我们发现由于位无关性质而导致的冗余位的存在。为了减轻这些影响，我们在知识提取目标中引入了位掩码机制。最后，广泛的实验不仅展示了我们的 BRCD 方法相对于其他知识提取方法的显著性能，而且还证实了我们的方法在不同的语义哈希模型和主干上的通用性。BRCD 的代码可在 https://github.com/hly1998/BRCD 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bit-mask+Robust+Contrastive+Knowledge+Distillation+for+Unsupervised+Semantic+Hashing)|0|
|[Perennial Semantic Data Terms of Use for Decentralized Web](https://doi.org/10.1145/3589334.3645631)|Rui Zhao, Jun Zhao||In today's digital landscape, the Web has become increasingly centralized, raising concerns about user privacy violations. Decentralized Web architectures, such as Solid, offer a promising solution by empowering users with better control over their data in their personal `Pods'. However, a significant challenge remains: users must navigate numerous applications to decide which application can be trusted with access to their data Pods. This often involves reading lengthy and complex Terms of Use agreements, a process that users often find daunting or simply ignore. This compromises user autonomy and impedes detection of data misuse. We propose a novel formal description of Data Terms of Use (DToU), along with a DToU reasoner. Users and applications specify their own parts of the DToU policy with local knowledge, covering permissions, requirements, prohibitions and obligations. Automated reasoning verifies compliance, and also derives policies for output data. This constitutes a “perennial” DToU language, where the policy authoring only occurs once, and we can conduct ongoing automated checks across users, applications and activity cycles. Our solution is built on Turtle, Notation 3 and RDF Surfaces, for the language and the reasoning engine. It ensures seamless integration with other semantic tools for enhanced interoperability. We have successfully integrated this language into the Solid framework, and conducted performance benchmark. We believe this work demonstrates a practicality of a perennial DToU language and the potential of a paradigm shift to how users interact with data and applications in a decentralized Web, offering both improved privacy and usability.|在今天的数字世界里，网络已经变得越来越集中，引起了人们对侵犯用户隐私的担忧。像 Solid 这样的分散式 Web 架构提供了一种有前途的解决方案，它赋予用户更好地控制他们个人“ Pods”中的数据的权力。然而，一个重大的挑战仍然存在: 用户必须浏览大量的应用程序，以决定哪些应用程序可以信任访问他们的数据 Pods。这通常涉及阅读冗长而复杂的使用条款协议，用户经常发现这个过程令人生畏，或者干脆忽略它。这损害了用户的自主性，阻碍了对数据误用的检测。我们提出了一个新的数据使用条款(DToU)的形式描述，以及一个 DToU 推理器。用户和应用程序使用本地知识指定 DToU 策略中自己的部分，包括许可、要求、禁止和义务。自动推理验证遵从性，并派生输出数据的策略。这构成了一种“常年”DToU 语言，其中策略创作只发生一次，我们可以跨用户、应用程序和活动周期进行持续的自动检查。我们的解决方案是基于 Turtle、 Notation3和 RDF Surface 构建的，用于语言和推理引擎。它确保与其他语义工具的无缝集成，以增强互操作性。我们已经成功地将该语言集成到 Solid 框架中，并进行了性能基准测试。我们相信这项工作展示了长期使用的 DToU 语言的实用性，以及向用户如何在分散的 Web 中与数据和应用程序交互的范式转变的潜力，提供了更好的隐私性和可用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Perennial+Semantic+Data+Terms+of+Use+for+Decentralized+Web)|0|
|[Enhancing Fairness in Meta-learned User Modeling via Adaptive Sampling](https://doi.org/10.1145/3589334.3645369)|Zheng Zhang, Qi Liu, Zirui Hu, Yi Zhan, Zhenya Huang, Weibo Gao, Qingyang Mao||Meta-learning has been widely employed to tackle the cold-start problem in user modeling. Similar to a guidebook for a new traveler, meta-learning significantly affects decision-making for new users in crucial scenarios, such as career recommendations. Consequently, the issue of fairness in meta-learning has gained paramount importance. Several methods have been proposed to mitigate unfairness in meta-learning and have shown promising results. However, a fundamental question remains unexplored: What is the critical factor leading to unfairness in meta-learned user modeling? Through the theoretical analysis that integrates the meta-learning paradigm with group fairness metrics, we identify group proportion imbalance as a critical factor. Subsequently, in order to mitigate the impact of this factor, we introduce a novel Fairness-aware Adaptive Sampling framework for meTa-learning, abbreviated as FAST. Its core concept involves adaptively adjusting the sampling distribution for different user groups during the interleaved training process of meta-learning. Furthermore, we provide theoretical guarantees demonstrating the convergence of FAST. Finally, empirical experiments conducted on three datasets reveal that FAST effectively enhances fairness while maintaining high accuracy. The code for FAST is available at https://github.com/zhengz99/FAST.|元学习已被广泛应用于解决用户建模中的冷启动问题。类似于一本旅行指南，元学习显著影响新用户在关键情景下的决策，如职业推荐。因此，元学习中的公平性问题变得至关重要。一些方法已经被提出来减少元学习中的不公平现象，并且已经显示出有希望的结果。然而，一个基本的问题仍然没有被探索: 什么是导致元学习用户建模不公平的关键因素？通过将元学习范式与群体公平指标相结合的理论分析，我们发现群体比例失衡是一个关键因素。随后，为了减轻这个因素的影响，我们引入了一个新的公平感知的自适应抽样框架，简称为 FAST。其核心概念是在元学习的交叉训练过程中，针对不同的用户群体自适应地调整抽样分布。此外，我们还提供了证明 FAST 收敛性的理论保证。最后，通过对三个数据集的实验表明，FAST 在保持高精度的同时，有效地提高了公平性。FAST 的代码可在 https://github.com/zhengz99/FAST 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Fairness+in+Meta-learned+User+Modeling+via+Adaptive+Sampling)|0|
|[MMLSCU: A Dataset for Multi-modal Multi-domain Live Streaming Comment Understanding](https://doi.org/10.1145/3589334.3645677)|Zixiang Meng, Qiang Gao, Di Guo, Yunlong Li, Bobo Li, Hao Fei, Shengqiong Wu, Fei Li, Chong Teng, Donghong Ji||With the increasing popularity of live streaming, the interactions from viewers during a live streaming can provide more specific and constructive feedback for both the streamer and platform. In such scenario, the primary and most direct feedback method from the audience is through comments. Thus, mining these live streaming comments to unearth the intentions behind them and, in turn, aiding streamers to enhance their live streaming quality is significant for the well development of live streaming ecosystem. To this end, we introduce the MMLSCU dataset, containing 50,129 intention-annotated comments across multiple modalities (text, images, vi-deos, audio) from eight streaming domains. Using multimodal pretrained large model and drawing inspiration from the Chain of Thoughts (CoT) concept, we implement an end-to-end model to sequentially perform the following tasks: viewer comment intent detection ➛ intent cause mining ➛ viewer comment explanation ➛ streamer policy suggestion. We employ distinct branches for video and audio to process their respective modalities. After obtaining the video and audio representations, we conduct a multimodal fusion with the comment. This integrated data is then fed into the large language model to perform inference across the four tasks following the CoT framework. Experimental results indicate that our model outperforms three multimodal classification baselines on comment intent detection and streamer policy suggestion, and one multimodal generation baselines on intent cause mining and viewer comment explanation. Compared to the models using only text, our multimodal setting yields superior outcomes. Moreover, incorporating CoT allows our model to enhance comment interpretation and more precise suggestions for the streamers. Our proposed dataset and model will bring new research attention on multimodal live streaming comment understanding.|随着流媒体直播的日益普及，观众在直播过程中的互动可以为流媒体和平台提供更具体、更有建设性的反馈。在这种情况下，来自听众的主要和最直接的反馈方法是通过评论。因此，挖掘这些直播评论，挖掘其背后的意图，进而帮助流媒体提高直播质量，对于直播生态系统的良好发展具有重要意义。为此，我们引入了 MMLSCU 数据集，其中包含来自8个流域的50,129个跨多种模式(文本、图像、视频、音频)的意向注释评论。利用多模态预训练大模型，借鉴思维链概念，实现端到端模型，依次完成以下任务: 浏览者评论意图检测、意图挖掘、浏览者评论解释、流媒体政策建议。我们使用不同的视频和音频分支来处理它们各自的模式。在获得视频和音频表示之后，我们对评论进行多模态融合。然后将这些集成数据输入到大型语言模型中，以便在遵循 CoT 框架的四个任务之间执行推理。实验结果表明，该模型在评论意图检测和流策略建议方面优于三个多模态分类基线，在意图挖掘和评论解释方面优于一个多模态生成基线。与只使用文本的模型相比，我们的多模式设置产生更好的结果。此外，合并 CoT 允许我们的模型增强评论解释和更精确的建议为主播。我们提出的数据集和模型将为多模式流媒体评论理解带来新的研究重点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MMLSCU:+A+Dataset+for+Multi-modal+Multi-domain+Live+Streaming+Comment+Understanding)|0|
|[Entire Chain Uplift Modeling with Context-Enhanced Learning for Intelligent Marketing](https://doi.org/10.1145/3589335.3648320)|Yinqiu Huang, Shuli Wang, Min Gao, Xue Wei, Changhao Li, Chuan Luo, Yinhua Zhu, Xiong Xiao, Yi Luo||Uplift modeling, vital in online marketing, seeks to accurately measure the impact of various strategies, such as coupons or discounts, on different users by predicting the Individual Treatment Effect (ITE). In an e-commerce setting, user behavior follows a defined sequential chain, including impression, click, and conversion. Marketing strategies exert varied uplift effects at each stage within this chain, impacting metrics like click-through and conversion rate. Despite its utility, existing research has neglected to consider the inter-task across all stages impacts within a specific treatment and has insufficiently utilized the treatment information, potentially introducing substantial bias into subsequent marketing decisions. We identify these two issues as the chain-bias problem and the treatment-unadaptive problem. This paper introduces the Entire Chain UPlift method with context-enhanced learning (ECUP), devised to tackle these issues. ECUP consists of two primary components: 1) the Entire Chain-Enhanced Network, which utilizes user behavior patterns to estimate ITE throughout the entire chain space, models the various impacts of treatments on each task, and integrates task prior information to enhance context awareness across all stages, capturing the impact of treatment on different tasks, and 2) the Treatment-Enhanced Network, which facilitates fine-grained treatment modeling through bit-level feature interactions, thereby enabling adaptive feature adjustment. Extensive experiments on public and industrial datasets validate ECUPs effectiveness. Moreover, ECUP has been deployed on the Meituan food delivery platform, serving millions of daily active users, with the related dataset released for future research.|提升模型在网络营销中至关重要，它试图通过预测个人待遇效应(ITE)来准确测量各种策略(如优惠券或折扣)对不同用户的影响。在电子商务环境中，用户行为遵循定义的顺序链，包括印象、单击和转换。营销策略在这个链条的每个阶段都发挥着不同的提升作用，影响着点击率和转化率等指标。尽管已有的研究很有用，但是它忽略了考虑特定治疗中所有阶段的任务间影响，并且没有充分利用治疗信息，这可能会给随后的营销决策带来很大的偏差。我们认为这两个问题是链式偏差问题和治疗不适应问题。本文介绍了基于上下文增强学习(ECUP)的整链提升方法，该方法是为解决这些问题而设计的。ECUP 由两个主要组成部分组成: 1)整个链增强网络，利用用户行为模式来估计整个链空间中的 ITE，模拟治疗对每个任务的各种影响，并整合任务先验信息以增强所有阶段的上下文意识，捕获治疗对不同任务的影响，2)治疗增强网络，通过位级特征交互促进细粒度治疗建模，从而实现自适应特征调整。在公共和工业数据集上的大量实验验证了 ECUPs 的有效性。此外，ECUP 已经部署在美团食品配送平台上，为数以百万计的日常活跃用户提供服务，并发布了相关的数据集，供日后研究使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Entire+Chain+Uplift+Modeling+with+Context-Enhanced+Learning+for+Intelligent+Marketing)|0|
|[Large Multimodal Model Compression via Iterative Efficient Pruning and Distillation](https://doi.org/10.1145/3589335.3648321)|Maolin Wang, Yao Zhao, Jiajia Liu, Jingdong Chen, Chenyi Zhuang, Jinjie Gu, Ruocheng Guo, Xiangyu Zhao||The deployment of Large Multimodal Models (LMMs) within Ant Group has significantly advanced multimodal tasks in payment, security, and advertising, notably enhancing advertisement audition tasks in Alipay. However, the deployment of such sizable models introduces challenges, particularly in increased latency and carbon emissions, which are antithetical to the ideals of Green AI. This paper introduces a novel multi-stage compression strategy for our proprietary LLM, AntGMM. Our methodology pivots on three main aspects: employing small training sample sizes, addressing multi-level redundancy through multi-stage pruning, and introducing an advanced distillation loss design. In our research, we constructed a dataset, the Multimodal Advertisement Audition Dataset (MAAD), from real-world scenarios within Alipay, and conducted experiments to validate the reliability of our proposed strategy. Furthermore, the effectiveness of our strategy is evident in its operational success in Alipay's real-world multimodal advertisement audition for three months from September 2023. Notably, our approach achieved a substantial reduction in latency, decreasing it from 700ms to 90ms, while maintaining online performance with only a slight performance decrease. Moreover, our compressed model is estimated to reduce electricity consumption by approximately 75 million kWh annually compared to the direct deployment of AntGMM, demonstrating our commitment to green AI initiatives.|蚂蚁集团大型多通道模型(LMM)的部署显著提高了支付、安全和广告的多通道任务，显著提高了支付宝的广告听力任务。然而，部署如此大规模的模型带来了挑战，尤其是在延迟和碳排放增加方面，这与绿色人工智能的理想背道而驰。本文介绍了一种新的多级压缩策略，用于我们专有的 LLM-ANTGMM。我们的方法主要集中在三个方面: 采用小样本量的训练样本，通过多级剪枝来解决多级冗余问题，以及引入先进的蒸馏损失设计。在本研究中，我们从支付宝的真实场景中构建了一个数据集，叫做多模式广告审核数据集(Multimode Advertisement Audition Dataset，MAAD) ，并通过实验验证了我们提出的策略的可靠性。此外，我们的战略的有效性是显而易见的，在支付宝的现实世界的多模式广告试听从2023年9月三个月的运营成功。值得注意的是，我们的方法实现了延迟的大幅减少，从700毫秒减少到90毫秒，同时保持在线性能，只有轻微的性能下降。此外，我们的压缩模型估计每年比直接部署的 AntGMM 减少约7500万千瓦小时的电力消耗，这表明我们对绿色人工智能倡议的承诺。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Multimodal+Model+Compression+via+Iterative+Efficient+Pruning+and+Distillation)|0|
|[Mystique: A Budget Pacing System for Performance Optimization in Online Advertising](https://doi.org/10.1145/3589335.3648342)|Rotem Stram, Rani Abboud, Alex Shtoff, Oren Somekh, Ariel Raviv, Yair Koren||Online advertising plays a pivotal role in sustaining the accessibility of free content on the Internet, serving as a primary revenue source for websites and online services. This dynamic marketplace sees advertisers allocating budgets and competing for the opportunity to present ads to users engaging with web pages, online services, and mobile apps. Modern online advertising often employs first-price auctions to determine ad placements. Yet, conducting auctions as isolated events in a greedy manner, may lead to sub-optimal results, necessitating some form of budget pacing. Traditionally, budget pacing has been achieved through hard throttling, where ads or campaigns are selectively made eligible for each auction using a biased coin-toss with a specified probability (or pacing-signal). More recently, the pacing signal has been leveraged to soft throttle ads, and is used as a multiplicative factor on their bids, thus enabling participation in all auctions but with potentially modified bids. In this study, we introduce Mystique, a "soft" throttling-based budget pacing system. Mystique operates on two levels: it utilizes spending data to establish a daily target spending curve for each campaign, and continuously updates a pacing signal to align the actual spending with this curve. Our offline evaluation in a complex simulated marketplace, demonstrates Mystique's ability to outperform several baseline algorithms, enabling budget depletion while securing more opportunities. Mystique has been in production for several years now, serving a major native advertising marketplace, and successfully pacing over one billion USD annually.|在线广告在维持互联网上免费内容的可获得性方面发挥着关键作用，是网站和在线服务的主要收入来源。这个动态的市场看到广告商分配预算和竞争的机会，提出广告给用户从事网页，在线服务和移动应用程序。现代在线广告通常使用第一价格拍卖来确定广告位置。然而，以贪婪的方式将拍卖作为孤立事件进行，可能会导致次优结果，从而需要某种形式的预算节奏。传统上，预算节奏已经通过硬节流，其中广告或活动是有选择地使每个拍卖有资格使用一个具有特定概率(或节奏信号)的有偏见的掷硬币。最近，节奏信号已经被用于软油门广告，并被用作其出价的乘数因素，从而使参与所有拍卖，但可能修改出价。在这项研究中，我们介绍了魔形，一个“软”节流为基础的预算起搏系统。魔形女在两个层面上运作: 它利用消费数据建立每个战役的每日目标消费曲线，并不断更新步伐信号，使实际消费与这个曲线保持一致。我们在一个复杂的模拟市场中的离线评估，展示了魔形女超越几个基线算法的能力，使预算耗尽，同时获得更多的机会。魔形女已经生产了几年，服务于一个主要的原生广告市场，并成功地每年超过十亿美元。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mystique:+A+Budget+Pacing+System+for+Performance+Optimization+in+Online+Advertising)|0|
|[HBIAS FedAvg: Smooth Federated Learning Transition for In-use Edge Models](https://doi.org/10.1145/3589335.3651518)|Anupam Gupta, Pabitra Mitra, Sudip Misra||Federated learning is an approach for privacy preserving machine learning. It is increasingly being used in a number of classification as well as ranking tasks. Protocols for federated learning involve model update at the edge devices and aggregation at the central servers over multiple rounds. In practice, most deep learning models deployed on the edge are already trained and in-use. Federated learning protocols lead to an oscillation in the performance of these local models over the epochs. The drop in accuracy is more prominent in the early phases. In this article, we study such effects for the popular FedAvg federated learning algorithm and suggest the modified HBIAS FedAvg algorithm. The algorithm proposes a heuristic based initialization adoption strategy for this purpose. We find that this protocol leads to smoother performance variation for experiments on benchmark datasets.|联邦学习是一种保护隐私的机器学习方法。它越来越多地被用于许多分类和排序任务。联邦学习协议包括边缘设备上的模型更新和多轮中央服务器上的聚合。在实践中，部署在边缘的大多数深度学习模型已经被训练和使用。联邦学习协议导致这些局部模型在各个时代的性能发生振荡。精度的下降在早期阶段更为突出。本文研究了流行的 FedAvg 联邦学习算法的这种效应，并提出了一种改进的 HBIAS FedAvg 联邦学习算法。为此，该算法提出了一种基于启发式的初始化采用策略。我们发现该协议可以使基准测试数据集的性能变化更加平滑。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HBIAS+FedAvg:+Smooth+Federated+Learning+Transition+for+In-use+Edge+Models)|0|
|[The Effect of Alter Ego Accounts on A/B Tests in Social Networks](https://doi.org/10.1145/3589335.3651569)|Katherine Avery, Amir Houmansadr, David D. Jensen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Effect+of+Alter+Ego+Accounts+on+A/B+Tests+in+Social+Networks)|0|
|[CardiO: Predicting Cardinality from Online Sources](https://doi.org/10.1145/3589335.3651477)|Shrestha Ghosh, Simon Razniewski, Damien Graux, Gerhard Weikum||Count questions are an important type of information need, though often present in noisy, contradictory, or semantically not fully aligned form on the Web. In this work, we propose CardiO, a lightweight and modular framework for searching entity counts on the Web. CardiO extracts all counts from a set of relevant Web snippets, and infers the most central count based on semantic and numeric distances from other candidates. In the absence of supporting evidence, the system relies on peer sets of similar size, to provide an estimate. Experiments show that CardiO can produce accurate and traceable counts better than small LLM-only methods. Although larger models have higher precision, when used to enhance CardiO components, they do not contribute to the final precision or recall.|计数问题是一种重要的信息需求类型，尽管在 Web 上经常以嘈杂、矛盾或语义不完全一致的形式出现。在这项工作中，我们提出了 CardiO，一个轻量级和模块化的框架，用于在 Web 上搜索实体计数。CardiO 从一组相关的 Web 片段中提取所有计数，并根据与其他候选者的语义和数字距离推断出最集中的计数。在缺乏支持性证据的情况下，该系统依赖于大小相似的对等集来提供估计。实验表明，CardiO 能够比单纯的 LLM 方法更好地产生准确和可追踪的计数。虽然较大的型号有更高的精确度，当用于增强心脏组件，他们并不有助于最终的精确度或召回。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CardiO:+Predicting+Cardinality+from+Online+Sources)|0|
|[An Identity Alignment Method based on Online Tracking](https://doi.org/10.1145/3589335.3651469)|Ruisheng Shi, Zhiyuan Peng, Tong Fu, Lina Lan, Jiaqi Zeng, Yuyang Shi, Jinqiao Shi, Shenwen Lin, Lin Li||Companies track user data and sell it to advertisers. They claim to protect user privacy by anonymization, but our research shows that significant risks are still involved. Even with anonymous data, attackers can identify users on other websites from tracking records. We propose an identity alignment method of deanonymization attack, which analyzes tracker data to align identities. We explore the key factors affecting the effectiveness of identity alignment and analyze its impact on user privacy. We use crawling data to create tracker data close to ground-truth scenarios and propose an evaluation framework for online tracking based identity alignment.|公司跟踪用户数据并将其出售给广告商。他们声称通过匿名保护用户隐私，但我们的研究表明，仍然存在重大风险。即使使用匿名数据，攻击者也可以通过跟踪记录来识别其他网站上的用户。提出了一种去匿名化攻击的身份对齐方法，该方法通过分析跟踪数据来对齐身份。探讨了影响身份匹配有效性的关键因素，分析了身份匹配对用户隐私的影响。我们使用爬行数据来创建接近地面真实场景的跟踪器数据，并提出了一个基于身份对齐的在线跟踪评估框架。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Identity+Alignment+Method+based+on+Online+Tracking)|0|
|[Semantic interlinking of Immigration Data using LLMs for Knowledge Graph Construction](https://doi.org/10.1145/3589335.3651557)|Radhakrishnan Venkatakrishnan, Emrah Tanyildizi, M. Abdullah Canbaz||The challenge of managing immigration data is exacerbated by its reliance on paper-based, evidence-driven records maintained by legal professionals, creating obstacles for efficient processing and analysis due to inherent trust issues with AI-based systems. This paper introduces a cutting-edge framework to surmount these hurdles by synergizing Large Language Models (LLMs) with Knowledge Graphs (KGs), revolutionizing traditional data handling methods. Our method transforms archaic, paper-based immigration records into a structured, interconnected knowledge network that intricately mirrors the legal and procedural nuances of immigration, ensuring a dynamic and trustworthy platform for data analysis. Utilizing LLMs, we extract vital entities and relationships from diverse legal documents to forge a comprehensive knowledge graph, encapsulating the complex legalities and procedural disparities in immigration processes and mapping the multifaceted interactions among stakeholders like applicants, sponsors, and legal experts. This graph not only facilitates a deep dive into the legal stipulations but also incorporates them, significantly boosting the system's reliability and precision. With the integration of Retrieval Augmented Generation (RAG) for exact, context-aware data retrieval and Augmented Knowledge Creation for developing a conversational interface via LLMs, our framework offers a scalable, adaptable solution to immigration data management. This innovative amalgamation of LLMs, KGs, and RAG techniques marks a paradigm shift towards more informed, efficient, and trustworthy decision-making in the sphere of global migration, setting a new benchmark for legal technology and data source management.|管理移民数据的挑战由于依赖法律专业人员维护的基于纸张的、证据驱动的记录而加剧，由于与基于人工智能的系统存在固有的信任问题，给有效处理和分析造成了障碍。本文介绍了一个前沿的框架，通过协同大型语言模型(LLM)和知识图(KGs)来克服这些障碍，彻底改革了传统的数据处理方法。我们的方法将古老的纸质移民记录转化为一个结构化的、相互关联的知识网络，这个网络错综复杂地反映了移民的法律和程序细微差别，确保了一个动态和可靠的数据分析平台。利用 LLM，我们从不同的法律文件中提取重要的实体和关系，以形成一个全面的知识图表，概括移民过程中复杂的法律和程序差异，并绘制利益攸关方(如申请人、担保人和法律专家)之间的多方面互动。这个图表不仅有助于深入了解法律规定，而且还包含了它们，极大地提高了系统的可靠性和精确性。该框架集成了检索增强生成(RAG)和增强知识创建(AR)两种技术，分别用于精确的、上下文感知的数据检索和通过 LLM 开发会话界面，为移民数据管理提供了一种可扩展的、适应性强的解决方案。LLM、 KGs 和 RAG 技术的这种创新性融合标志着全球移民领域向更知情、高效和可信赖的决策转变，为法律技术和数据源管理设定了新的基准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semantic+interlinking+of+Immigration+Data+using+LLMs+for+Knowledge+Graph+Construction)|0|
|[Why Deeper Layers May Introduce More Bias](https://doi.org/10.1145/3589335.3651583)|Rui Xia, Lisong Wang, Pingping Shi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Why+Deeper+Layers+May+Introduce+More+Bias)|0|
|[Shock! Quantifying the Impact of Core Developers' Dropout on the Productivity of OSS Projects](https://doi.org/10.1145/3589335.3651559)|Giuseppe Russo Latona, Christoph Gote, Christian Zingg, Giona Casiraghi, Luca Verginer, Frank Schweitzer||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Shock!+Quantifying+the+Impact+of+Core+Developers'+Dropout+on+the+Productivity+of+OSS+Projects)|0|
|[Over-Sampling Strategy in Feature Space for Graphs based Class-imbalanced Bot Detection](https://doi.org/10.1145/3589335.3651544)|Shuhao Shi, Kai Qiao, Chen Chen, Jie Yang, Jian Chen, Bin Yan||The presence of a large number of bots in Online Social Networks (OSN) leads to undesirable social effects. Graph neural networks (GNNs) have achieved state-of-the-art performance in bot detection since they can effectively utilize user interaction. In most scenarios, the distribution of bots and humans is imbalanced, resulting in under-represent minority class samples and sub-optimal performance. However, previous GNN-based methods for bot detection seldom consider the impact of class-imbalanced issues. In this paper, we propose an over-sampling strategy for GNN (OS-GNN) that can mitigate the effect of class imbalance in bot detection. Compared with previous over-sampling methods for GNNs, OS-GNN does not call for edge synthesis, eliminating the noise inevitably introduced during the edge construction. Specifically, node features are first mapped to a feature space through neighborhood aggregation and then generated samples for the minority class in the feature space. Finally, the augmented features are fed into GNNs to train the classifiers. This framework is general and can be easily extended into different GNN architectures. The proposed framework is evaluated using three real-world bot detection benchmark datasets, and it consistently exhibits superiority over the baselines.|在线社交网络(OSN)中大量机器人的出现导致了不良的社交效应。图神经网络由于能够有效地利用用户交互，在机器人检测方面取得了一流的性能。在大多数情况下，机器人和人类的分布是不平衡的，导致少数类样本表示不足，性能次优。然而，以往基于 GNN 的机器人检测方法很少考虑类不平衡问题的影响。本文针对 GNN (OS-GNN)提出了一种过采样策略，以减轻类不平衡对机器人检测的影响。与以往的过采样方法相比，OS-GNN 不需要边缘合成，消除了边缘构造过程中不可避免的噪声。首先通过邻域聚合将节点特征映射到特征空间，然后为特征空间中的少数类生成样本。最后，将增强特征输入到 GNN 中，对分类器进行训练。这个框架是通用的，可以很容易地扩展到不同的 GNN 体系结构中。利用三个实际的机器人检测基准数据集对该框架进行了评估，结果表明该框架始终显示出优于基准的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Over-Sampling+Strategy+in+Feature+Space+for+Graphs+based+Class-imbalanced+Bot+Detection)|0|
|[Dual Graph Networks with Synthetic Oversampling for Imbalanced Rumor Detection on Social Media](https://doi.org/10.1145/3589335.3651494)|YenWen Lu, ChihYao Chen, ChengTe Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual+Graph+Networks+with+Synthetic+Oversampling+for+Imbalanced+Rumor+Detection+on+Social+Media)|0|
|[Travel Demand Prediction with Application to Commuter Demand Estimation on Urban Railways](https://doi.org/10.1145/3589335.3651574)|Yohei Kodama, Yuki Akeyama, Yusuke Miyazaki, Koh Takeuchi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Travel+Demand+Prediction+with+Application+to+Commuter+Demand+Estimation+on+Urban+Railways)|0|
|[Burstiness-aware Bipartite Graph Neural Networks for Fraudulent User Detection on Rating Platforms](https://doi.org/10.1145/3589335.3651475)|YenWen Lu, YuChe Tsai, ChengTe Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Burstiness-aware+Bipartite+Graph+Neural+Networks+for+Fraudulent+User+Detection+on+Rating+Platforms)|0|
|[A Tale of Two Communities: Exploring Academic References on Stack Overflow](https://doi.org/10.1145/3589335.3651464)|Run Huang, Souti Chattopadhyay||Stack Overflow is widely recognized by software practitioners as the go-to resource for addressing technical issues and sharing practical solutions. While it is not typically seen as a forum for scholarly discourse, users on Stack Overflow often refer to academic sources in their discussions. Yet, little is known about these referenced works from the academic community and how they intersect the needs and interests of the Stack Overflow community. To bridge this gap, we conducted a large-scale study on academic references in Stack Overflow. Our findings reveal that Stack Overflow communities with different domains of interest engage with academic literature at varying frequencies and speeds. The contradicting patterns suggest that some disciplines may have diverged in their interests and development trajectories from the corresponding practitioner community. Finally, we discuss the potential of Stack Overflow in gauging the real-world relevance of academic research.|Stack Overflow 被软件从业者广泛认为是解决技术问题和分享实际解决方案的可用资源。虽然 Stack Overflow 通常不被视为学术讨论的论坛，但它的用户在讨论中经常引用学术资源。然而，对于这些来自学术界的参考著作，以及它们如何与 Stack Overflow 社区的需求和兴趣相交，我们知之甚少。为了弥补这一差距，我们对 Stack Overflow 中的学术参考文献进行了大规模的研究。我们的研究结果表明，Stack Overflow 社区具有不同的兴趣领域，以不同的频率和速度参与学术文献。相互矛盾的模式表明，一些学科在其兴趣和发展轨迹方面可能与相应的从业者群体不同。最后，我们讨论了堆栈溢出在衡量学术研究的现实相关性方面的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Tale+of+Two+Communities:+Exploring+Academic+References+on+Stack+Overflow)|0|
|[Efficient Location Sampling Algorithms for Road Networks](https://doi.org/10.1145/3589335.3651497)|Sara Ahmadian, Sreenivas Gollapudi, Kostas Kollias, Vivek Kumar, Ameya Velingker, Santhoshini Velusamy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Location+Sampling+Algorithms+for+Road+Networks)|0|
|[A Category-agnostic Graph Attention-based Approach for Determining Notability of Articles for Wikipedia](https://doi.org/10.1145/3589335.3651461)|Gokul Thota, Vasudeva Varma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Category-agnostic+Graph+Attention-based+Approach+for+Determining+Notability+of+Articles+for+Wikipedia)|0|
|[Concentration of Power and Participation in Online Governance: the Ecosystem of Decentralized Autonomous Organizations](https://doi.org/10.1145/3589335.3651481)|Andrea PeñaCalvin, Javier Arroyo, Andrew Schwartz, Samer Hassan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Concentration+of+Power+and+Participation+in+Online+Governance:+the+Ecosystem+of+Decentralized+Autonomous+Organizations)|0|
|["All of Me": Mining Users' Attributes from their Public Spotify Playlists](https://doi.org/10.1145/3589335.3651459)|Pier Paolo Tricomi, Luca Pajola, Luca Pasa, Mauro Conti||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q="All+of+Me":+Mining+Users'+Attributes+from+their+Public+Spotify+Playlists)|0|
|[PyGDebias: A Python Library for Debiasing in Graph Learning](https://doi.org/10.1145/3589335.3651239)|Yushun Dong, Zhenyu Lei, Zaiyi Zheng, Song Wang, Jing Ma, Alex Jing Huang, Chen Chen, Jundong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PyGDebias:+A+Python+Library+for+Debiasing+in+Graph+Learning)|0|
|[Synslator: An Interactive Machine Translation Tool with Online Learning](https://doi.org/10.1145/3589335.3651240)|Jiayi Wang, Ke Wang, Fengming Zhou, Chengyu Wang, Zhiyong Fu, Zeyu Feng, Yu Zhao, Yuqi Zhang||Interactive machine translation (IMT) has emerged as a progression of the computer-aided translation paradigm, where the machine translation system and the human translator collaborate to produce high-quality translations. This paper introduces Synslator, a user-friendly computer-aided translation (CAT) tool that not only supports IMT, but is adept at online learning with real-time translation memories. To accommodate various deployment environments for CAT services, Synslator integrates two different neural translation models to handle translation memories for online learning. Additionally, the system employs a language model to enhance the fluency of translations in an interactive mode. In evaluation, we have confirmed the effectiveness of online learning through the translation models, and have observed a 13% increase in post-editing efficiency with the interactive functionalities of Synslator. A tutorial video is available at:https://youtu.be/K0vRsb2lTt8.|交互式机器翻译(IMT)已经成为计算机辅助翻译范式的一个发展阶段，机器翻译系统和人工翻译协作生成高质量的翻译。本文介绍了 Synslator，一个用户友好的计算机辅助翻译(CAT)工具，它不仅支持 IMT，而且善于利用实时翻译记忆进行在线学习。为了适应 CAT 服务的不同部署环境，Synslator 集成了两种不同的神经翻译模型来处理在线学习的翻译记忆。此外，该系统采用了一种语言模式，以提高交互式翻译的流畅性。在评估中，我们通过翻译模型证实了在线学习的有效性，并观察到 Synslator 的交互功能使后期编辑效率提高了13% 。教程视频可以在以下 https://youtu.be/k0vrsb2ltt8找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Synslator:+An+Interactive+Machine+Translation+Tool+with+Online+Learning)|0|
|[ACCORD: Constraint-driven Mediation of Multi-user Conflicts in Cloud Services](https://doi.org/10.1145/3589335.3651244)|Abhiroop Tippavajjula, Primal Pappachan, Anna Cinzia Squicciarini, Jose Such||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ACCORD:+Constraint-driven+Mediation+of+Multi-user+Conflicts+in+Cloud+Services)|0|
|[Fediscount: Shopping Online at a Federated Store Using FedUP as SPARQL Federation Engine](https://doi.org/10.1145/3589335.3651249)|Julien AimonierDavat, Minh Hoang Dang, Pascal Molli, Brice Nédelec, Hala SkafMolli||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fediscount:+Shopping+Online+at+a+Federated+Store+Using+FedUP+as+SPARQL+Federation+Engine)|0|
|[Online Disinformation and Generative Language Models: Motivations, Challenges, and Mitigations](https://doi.org/10.1145/3589335.3651254)|Ziyi Guo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Disinformation+and+Generative+Language+Models:+Motivations,+Challenges,+and+Mitigations)|0|
|[Quantifying Governance of Online Communities at Web Scale](https://doi.org/10.1145/3589335.3651266)|Galen Cassebeer Weld||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quantifying+Governance+of+Online+Communities+at+Web+Scale)|0|
|[Tutorial on User Simulation for Evaluating Information Access Systems on the Web](https://doi.org/10.1145/3589335.3641243)|Krisztian Balog, ChengXiang Zhai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tutorial+on+User+Simulation+for+Evaluating+Information+Access+Systems+on+the+Web)|0|
|[Archiving and Temporal Analysis of Behavioral Web Data - Tales from the Inside](https://doi.org/10.1145/3589335.3641260)|Stefan Dietze||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Archiving+and+Temporal+Analysis+of+Behavioral+Web+Data+-+Tales+from+the+Inside)|0|
|[A Case Study Comparing Twitter Communities Detected by the Louvain and Leiden Algorithms During the 2022 War in Ukraine](https://doi.org/10.1145/3589335.3651892)|Karolina Sliwa, Ema Kusen, Mark Strembeck||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Case+Study+Comparing+Twitter+Communities+Detected+by+the+Louvain+and+Leiden+Algorithms+During+the+2022+War+in+Ukraine)|0|
|[AI Driven Online Advertising: Market Design, Generative AI, and Ethics](https://doi.org/10.1145/3589335.3641295)|Fengxiang He, Mengnan Du, Aris FilosRatsikas, Lu Cheng, Qingquan Song, Min Lin, John Vines||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI+Driven+Online+Advertising:+Market+Design,+Generative+AI,+and+Ethics)|0|
|[Comparative Analysis of Discussion Intensity and Semantic Diversity in Early vs. Late Engagers: A Study of Japanese Tweets about ChatGPT](https://doi.org/10.1145/3589335.3651908)|Tomoki Fukuma, Koki Noda, Yuta Yamamoto, Takaya Hoshi, Yoshiharu Ichikawa, Kyosuke Kambe, Yu Masubuchi, Fujio Toriumi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Comparative+Analysis+of+Discussion+Intensity+and+Semantic+Diversity+in+Early+vs.+Late+Engagers:+A+Study+of+Japanese+Tweets+about+ChatGPT)|0|
|[Analysis of the Effect between the Information Type on SNSs and User Attributes during Disaster](https://doi.org/10.1145/3589335.3652500)|Kosuke Wakasugi, Yu Suzuki, Akiyo Nadamoto||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analysis+of+the+Effect+between+the+Information+Type+on+SNSs+and+User+Attributes+during+Disaster)|0|
|[Understanding the Impact of COVID-19 on Online Eating Disorder Communities on Reddit](https://doi.org/10.1145/3589335.3652506)|Md Al Amin, Lu Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+the+Impact+of+COVID-19+on+Online+Eating+Disorder+Communities+on+Reddit)|0|
|[Statistical Confidence in Mining Power Estimates for PoW Blockchains](https://doi.org/10.1145/3589335.3651960)|Mary Milad, Christina Ovezik, Dimitris Karakostas, Daniel W. Woods||The security of blockchain systems depends on the distribution of mining power across participants. If sufficient mining power is controlled by one entity, they can force their own version of events. This may allow them to double spend coins, for example. For Proof of Work (PoW) blockchains, however, the distribution of mining power cannot be read directly from the blockchain and must instead be inferred from the number of blocks mined in a specific sample window. We introduce a framework to quantify this statistical uncertainty for the Nakamoto coefficient, which is a commonly-used measure of blockchain decentralization. We show that aggregating blocks over a day can lead to considerable uncertainty, with Bitcoin failing more than half the hypothesis tests (α = 0.05) when using a daily granularity. For these reasons, we recommend that blocks are aggregated over a sample window of at least 7 days. Instead of reporting a single value, our approach produces a range of possible Nakamoto coefficient values that have statistical support at a particular significance level α.|区块链系统的安全性取决于采矿权在参与者之间的分配。如果一个实体控制了足够的挖掘能力，那么它们可以强制执行自己版本的事件。例如，这可能使他们的支出增加一倍。然而，对于工作证明(PoW)区块链，挖掘功率的分布不能直接从区块链中读取，而必须从特定样本窗口中挖掘的区块数量中推断出来。我们引入一个框架来量化中本系数的统计不确定性，中本系数是区块链地方分权的常用度量。我们发现，当比特币使用每日粒度时，一天内的块聚合会导致相当大的不确定性，超过一半的假设检验(α = 0.05)都失败了。由于这些原因，我们建议在至少7天的示例窗口中聚合块。我们的方法不是报告一个单一的值，而是产生一系列可能的 Nakamoto 系数值，这些值在统计学上支持特定的显著性水平 α。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Statistical+Confidence+in+Mining+Power+Estimates+for+PoW+Blockchains)|0|
|[From Bollywood Son Preference to Moral Policing on Women in Iran - A 360° View of Gender Bias](https://doi.org/10.1145/3589335.3653010)|Ashiqur R. KhudaBukhsh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Bollywood+Son+Preference+to+Moral+Policing+on+Women+in+Iran+-+A+360°+View+of+Gender+Bias)|0|
|[On Truthful Item-Acquiring Mechanisms for Reward Maximization](https://doi.org/10.1145/3589334.3645345)|Liang Shan, Shuo Zhang, Jie Zhang, Zihe Wang||In this research, we study the problem that a collector acquires items from the owner based on the item qualities the owner declares and an independent appraiser's assessments. The owner is interested in maximizing the probability that the collector acquires the items and is the only one who knows the items' factual quality. The appraiser performs her duties with impartiality, but her assessment may be subject to random noises, so it may not accurately reflect the factual quality of the items. The main challenge lies in devising mechanisms that prompt the owner to reveal accurate information, thereby optimizing the collector's expected reward. We consider the menu size of mechanisms as a measure of their practicability and study its impact on the attainable expected reward. For the single-item setting, we design optimal mechanisms with a monotone increasing menu size. Although the reward gap between the simplest and optimal mechanisms is bounded, we show that simple mechanisms with a small menu size cannot ensure any positive fraction of the optimal reward of mechanisms with a larger menu size. For the multi-item setting, we show that an ordinal mechanism that only takes the owner's ordering of the items as input is not incentive-compatible. We then propose a set of Union mechanisms that combine single-item mechanisms. Moreover, we run experiments to examine these mechanisms' robustness against the independent appraiser's assessment accuracy and the items' acquiring rate.|在本研究中，我们研究收集者根据物品所有者声明的物品品质和独立评估者的评估从所有者那里获取物品的问题。所有者感兴趣的是最大化收集器获得项目的概率，并且是唯一知道项目实际质量的人。鉴定人公正地履行职责，但鉴定可能会受到随机噪音的影响，因而不能准确地反映物品的实际质量。主要的挑战在于设计机制，促使所有者披露准确的信息，从而优化收集者的预期回报。我们将机制的菜单大小作为衡量其实用性的一个指标，并研究其对可达到的期望报酬的影响。对于单项设置，我们设计单调增加菜单大小的最佳机制。虽然最简单机构和最优机构之间的奖励差距是有界的，但是我们证明了菜单大小较小的简单机构不能保证菜单大小较大的机构的最优奖励的任何正分数。对于多项目设置，我们表明，一个序机制，只采取所有者的项目作为输入的顺序是不相容的激励。然后，我们提出一组结合单项机制的 Union 机制。此外，我们还进行了实验，以检验这些机制对独立评估者的评估准确度和项目获取率的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Truthful+Item-Acquiring+Mechanisms+for+Reward+Maximization)|0|
|[Barter Exchange with Shared Item Valuations](https://doi.org/10.1145/3589334.3645632)|Juan Luque, Sharmila Duppala, John P. Dickerson, Aravind Srinivasan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Barter+Exchange+with+Shared+Item+Valuations)|0|
|[Efficiency of Non-Truthful Auctions in Auto-bidding with Budget Constraints](https://doi.org/10.1145/3589334.3645636)|Christopher Liaw, Aranyak Mehta, Wennan Zhu||We study the efficiency of non-truthful auctions for auto-bidders with both return on spend (ROS) and budget constraints. The efficiency of a mechanism is measured by the price of anarchy (PoA), which is the worst case ratio between the liquid welfare of any equilibrium and the optimal (possibly randomized) allocation. Our first main result is that the first-price auction (FPA) is optimal, among deterministic mechanisms, in this setting. Without any assumptions, the PoA of FPA is n which we prove is tight for any deterministic mechanism. However, under a mild assumption that a bidder's value for any query does not exceed their total budget, we show that the PoA is at most 2. This bound is also tight as it matches the optimal PoA without a budget constraint. We next analyze two randomized mechanisms: randomized FPA (rFPA) and "quasi-proportional" FPA. We prove two results that highlight the efficacy of randomization in this setting. First, we show that the PoA of rFPA for two bidders is at most 1.8 without requiring any assumptions. This extends prior work which focused only on an ROS constraint. Second, we show that quasi-proportional FPA has a PoA of 2 for any number of bidders, without any assumptions. Both of these bypass lower bounds in the deterministic setting. Finally, we study the setting where bidders are assumed to bid uniformly. We show that uniform bidding can be detrimental for efficiency in deterministic mechanisms while being beneficial for randomized mechanisms, which is in stark contrast with the settings without budget constraints.|研究了同时考虑投资回报率(ROS)和预算约束的汽车投标人非真实拍卖的有效性问题。一个机制的效率是通过无政府状态的代价(PoA)来衡量的，这是任何平衡的液体福利和最优(可能随机)分配之间的最坏情况比。我们的第一个主要结果是，在这种情况下，在确定性机制中，第一价格拍卖(FPA)是最优的。在没有任何假设的情况下，我们证明了 FPA 的 PoA 对于任何确定性机制都是紧的。然而，在一个温和的假设下，任何查询的投标者的价值不会超过他们的总预算，我们显示 PoA 最多是2。这个界限也很紧密，因为它匹配没有预算线的最优 PoA。接下来我们分析了两种随机机制: 随机 FPA (rFPA)和“准比例”FPA。我们证明了两个结果，突出了在这种情况下随机化的功效。首先，我们在不需要任何假设的情况下，证明了两个投标者的 rFPA 的 PoA 最多为1.8。这扩展了先前只关注 ROS 约束的工作。其次，在没有任何假设的情况下，我们证明了对于任意数量的投标者，准比例平均积分有一个2的 PoA。这两个都在确定性设置中绕过了下限。最后，我们研究了假设投标人均匀投标的情况。我们表明，在确定性机制中，统一投标可能不利于效率，而有利于随机机制，这与没有预算约束的设置形成鲜明对比。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficiency+of+Non-Truthful+Auctions+in+Auto-bidding+with+Budget+Constraints)|0|
|[Individual Welfare Guarantees in the Autobidding World with Machine-learned Advice](https://doi.org/10.1145/3589334.3645660)|Yuan Deng, Negin Golrezaei, Patrick Jaillet, Jason Cheuk Nam Liang, Vahab Mirrokni||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Individual+Welfare+Guarantees+in+the+Autobidding+World+with+Machine-learned+Advice)|0|
|[Non-uniform Bid-scaling and Equilibria for Different Auctions: An Empirical Study](https://doi.org/10.1145/3589334.3645659)|Yuan Deng, Jieming Mao, Vahab Mirrokni, Yifeng Teng, Song Zuo||In recent years, the growing adoption of autobidding has motivated the study of auction design with value-maximizing auto-bidders. It is known that under mild assumptions, uniform bid-scaling is an optimal bidding strategy in truthful auctions, e.g., Vickrey-Clarke-Groves auction (VCG), and the price of anarchy for VCG is $2$. However, for other auction formats like First-Price Auction (FPA) and Generalized Second-Price auction (GSP), uniform bid-scaling may not be an optimal bidding strategy, and bidders have incentives to deviate to adopt strategies with non-uniform bid-scaling. Moreover, FPA can achieve optimal welfare if restricted to uniform bid-scaling, while its price of anarchy becomes $2$ when non-uniform bid-scaling strategies are allowed. All these price of anarchy results have been focused on welfare approximation in the worst-case scenarios. To complement theoretical understandings, we empirically study how different auction formats (FPA, GSP, VCG) with different levels of non-uniform bid-scaling perform in an autobidding world with a synthetic dataset for auctions. Our empirical findings include: * For both uniform bid-scaling and non-uniform bid-scaling, FPA is better than GSP and GSP is better than VCG in terms of both welfare and profit; * A higher level of non-uniform bid-scaling leads to lower welfare performance in both FPA and GSP, while different levels of non-uniform bid-scaling have no effect in VCG. Our methodology of synthetic data generation may be of independent interest.|近年来，越来越多的自动竞价的采用推动了价值最大化自动竞价拍卖设计的研究。众所周知，在温和的假设下，统一的出价比例是一个最优的出价策略在真实的拍卖，例如，维克里-克拉克-格罗夫斯拍卖(VCG)和无政府状态的价格为2美元。然而，对于第一价格拍卖(FPA)和广义第二价格拍卖(GSP)等其他拍卖形式，统一标尺可能不是最优的竞价策略，竞价者有偏离采用非统一标尺策略的动机。此外，如果限制在均匀标尺下，FPA 可以获得最佳的福利，而当允许非均匀标尺策略时，其无政府状态的价格变为2美元。所有这些无政府结果的代价都集中在最坏情况下的福利近似上。为了补充理论知识，我们实证研究了不同的拍卖格式(FPA，GSP，VCG)和不同水平的非均匀标尺在自动竞价世界中的表现。我们的实证研究结果包括: * 在统一标尺和非统一标尺两种情况下，平均分配比例均优于普惠制，而普惠制在福利和利润方面均优于 VCG; * 较高水平的非统一标尺导致平均分配比例和普惠制的福利表现较差，而不同水平的非统一标尺对 VCG 没有影响。我们的综合数据生成方法可能具有独立的兴趣。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Non-uniform+Bid-scaling+and+Equilibria+for+Different+Auctions:+An+Empirical+Study)|0|
|[A Similarity-based Approach for Efficient Large Quasi-clique Detection](https://doi.org/10.1145/3589334.3645374)|Jiayang Pang, Chenhao Ma, Yixiang Fang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Similarity-based+Approach+for+Efficient+Large+Quasi-clique+Detection)|0|
|[GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning](https://doi.org/10.1145/3589334.3645439)|Yun Zhu, Yaoke Wang, Haizhou Shi, Zhenshuo Zhang, Dian Jiao, Siliang Tang||Graph-structured data is ubiquitous in the world which models complex relationships between objects, enabling various Web applications. Daily influxes of unlabeled graph data on the Web offer immense potential for these applications. Graph self-supervised algorithms have achieved significant success in acquiring generic knowledge from abundant unlabeled graph data. These pre-trained models can be applied to various downstream Web applications, saving training time and improving downstream (target) performance. However, different graphs, even across seemingly similar domains, can differ significantly in terms of attribute semantics, posing difficulties, if not infeasibility, for transferring the pre-trained models to downstream tasks. Concretely speaking, for example, the additional task-specific node information in downstream tasks (specificity) is usually deliberately omitted so that the pre-trained representation (transferability) can be leveraged. The trade-off as such is termed as "transferability-specificity dilemma" in this work. To address this challenge, we introduce an innovative deployment module coined as GraphControl, motivated by ControlNet, to realize better graph domain transfer learning. Specifically, by leveraging universal structural pre-trained models and GraphControl, we align the input space across various graphs and incorporate unique characteristics of target data as conditional inputs. These conditions will be progressively integrated into the model during fine-tuning or prompt tuning through ControlNet, facilitating personalized deployment. Extensive experiments show that our method significantly enhances the adaptability of pre-trained models on target attributed datasets, achieving 1.4-3x performance gain. Furthermore, it outperforms training-from-scratch methods on target data with a comparable margin and exhibits faster convergence.|图形结构化数据在世界上无处不在，它建模对象之间的复杂关系，支持各种 Web 应用程序。网络上每天涌入的未标记图形数据为这些应用程序提供了巨大的潜力。图自监督算法在从大量未标记图数据中获取通用知识方面取得了显著的成功。这些预先训练的模型可以应用于各种下游 Web 应用程序，节省训练时间，提高下游(目标)性能。然而，不同的图，即使是在看似相似的领域，在属性语义方面也可能有很大的不同，对于将预先训练好的模型转移到下游任务造成了困难，如果不是不可行的话。具体地说，例如，在下游任务中额外的特定于任务的节点信息(特异性)通常被故意省略，以便可以利用预先训练的表示(可转移性)。在这项工作中，这种权衡被称为“可转移性-特异性困境”。为了应对这一挑战，我们引入了一个创新的部署模块，称为 GraphControl，由 ControlNet 推动，以实现更好的图域传输学习。具体来说，通过利用通用的结构化预训练模型和 GraphControl，我们跨不同的图对齐输入空间，并将目标数据的独特特征作为条件输入。在通过 ControlNet 进行微调或快速调整期间，这些条件将逐步集成到模型中，从而促进个性化部署。大量实验表明，该方法显著提高了预训练模型对目标属性数据集的适应性，获得了1.4 -3倍的性能增益。此外，该方法对目标数据的训练效果优于从头开始的方法，具有可比较的边界，收敛速度更快。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphControl:+Adding+Conditional+Control+to+Universal+Graph+Pre-trained+Models+for+Graph+Domain+Transfer+Learning)|0|
|[Rethinking Node-wise Propagation for Large-scale Graph Learning](https://doi.org/10.1145/3589334.3645450)|Xunkai Li, Jingyuan Ma, Zhengyu Wu, Daohan Su, Wentao Zhang, RongHua Li, Guoren Wang||Scalable graph neural networks (GNNs) have emerged as a promising technique, which exhibits superior predictive performance and high running efficiency across numerous large-scale graph-based web applications. However, (i) Most scalable GNNs tend to treat all nodes in graphs with the same propagation rules, neglecting their topological uniqueness; (ii) Existing node-wise propagation optimization strategies are insufficient on web-scale graphs with intricate topology, where a full portrayal of nodes' local properties is required. Intuitively, different nodes in web-scale graphs possess distinct topological roles, and therefore propagating them indiscriminately or neglect local contexts may compromise the quality of node representations. This intricate topology in web-scale graphs cannot be matched by small-scale scenarios. To address the above issues, we propose Adaptive Topology-aware Propagation (ATP), which reduces potential high-bias propagation and extracts structural patterns of each node in a scalable manner to improve running efficiency and predictive performance. Remarkably, ATP is crafted to be a plug-and-play node-wise propagation optimization strategy, allowing for offline execution independent of the graph learning process in a new perspective. Therefore, this approach can be seamlessly integrated into most scalable GNNs while remain orthogonal to existing node-wise propagation optimization strategies. Extensive experiments on 12 datasets, including the most representative large-scale ogbn-papers100M, have demonstrated the effectiveness of ATP. Specifically, ATP has proven to be efficient in improving the performance of prevalent scalable GNNs for semi-supervised node classification while addressing redundant computational costs.|可伸缩图神经网络(GNN)已经成为一种很有前途的技术，它在众多大规模的基于图的 Web 应用中表现出优越的预测性能和高效的运行效率。然而，(i)大多数可伸缩的 GNN 倾向于用相同的传播规则处理图中的所有节点，而忽略了它们的拓扑唯一性; (ii)现有的节点传播优化策略对于具有复杂拓扑结构的网络尺度图是不够的，因为在这种情况下需要对节点的本地属性进行全面描述。直观地看，网络尺度图中的不同节点具有不同的拓扑角色，因此，不加区分地传播它们或忽略局部上下文可能会影响节点表示的质量。网络尺度图中这种错综复杂的拓扑结构无法与小尺度场景匹配。为了解决上述问题，我们提出了自适应拓扑感知传播(ATP) ，它减少了潜在的高偏差传播，并以可扩展的方式提取每个节点的结构模式，以提高运行效率和预测性能。值得注意的是，ATP 是一个即插即用的节点传播优化策略，从一个新的角度考虑，允许离线执行独立于图形学习过程。因此，这种方法可以无缝集成到大多数可扩展的 GNN 中，同时与现有的节点传播优化策略保持正交。在12个数据集上的广泛实验，包括最具代表性的大规模 ogbn 论文100M，已经证明了 ATP 的有效性。具体而言，ATP 已被证明是有效的改善性能的普遍可伸缩的 GNN 的半监督节点分类，同时解决冗余计算代价。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Node-wise+Propagation+for+Large-scale+Graph+Learning)|0|
|[Graph Contrastive Learning Reimagined: Exploring Universality](https://doi.org/10.1145/3589334.3645480)|Jiaming Zhuo, Can Cui, Kun Fu, Bingxin Niu, Dongxiao He, Chuan Wang, Yuanfang Guo, Zhen Wang, Xiaochun Cao, Liang Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Contrastive+Learning+Reimagined:+Exploring+Universality)|0|
|[MuGSI: Distilling GNNs with Multi-Granularity Structural Information for Graph Classification](https://doi.org/10.1145/3589334.3645542)|Tianjun Yao, Jiaqi Sun, Defu Cao, Kun Zhang, Guangyi Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MuGSI:+Distilling+GNNs+with+Multi-Granularity+Structural+Information+for+Graph+Classification)|0|
|[Efficient Computation for Diagonal of Forest Matrix via Variance-Reduced Forest Sampling](https://doi.org/10.1145/3589334.3645578)|Haoxin Sun, Zhongzhi Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Computation+for+Diagonal+of+Forest+Matrix+via+Variance-Reduced+Forest+Sampling)|0|
|[Decoupled Variational Graph Autoencoder for Link Prediction](https://doi.org/10.1145/3589334.3645601)|YoonSik Cho||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decoupled+Variational+Graph+Autoencoder+for+Link+Prediction)|0|
|[ModelGo: A Practical Tool for Machine Learning License Analysis](https://doi.org/10.1145/3589334.3645520)|Moming Duan, Qinbin Li, Bingsheng He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ModelGo:+A+Practical+Tool+for+Machine+Learning+License+Analysis)|0|
|[Content Moderation and the Formation of Online Communities: A Theoretical Framework](https://doi.org/10.1145/3589334.3645490)|Cynthia Dwork, Chris Hays, Jon M. Kleinberg, Manish Raghavan||We study the impact of content moderation policies in online communities. In our theoretical model, a platform chooses a content moderation policy and individuals choose whether or not to participate in the community according to the fraction of user content that aligns with their preferences. The effects of content moderation, at first blush, might seem obvious: it restricts speech on a platform. However, when user participation decisions are taken into account, its effects can be more subtle $\unicode{x2013}$ and counter-intuitive. For example, our model can straightforwardly demonstrate how moderation policies may increase participation and diversify content available on the platform. In our analysis, we explore a rich set of interconnected phenomena related to content moderation in online communities. We first characterize the effectiveness of a natural class of moderation policies for creating and sustaining stable communities. Building on this, we explore how resource-limited or ideological platforms might set policies, how communities are affected by differing levels of personalization, and competition between platforms. Our model provides a vocabulary and mathematically tractable framework for analyzing platform decisions about content moderation.|我们研究了在线社区内容管制政策的影响。在我们的理论模型中，一个平台选择一个内容管理政策，个人根据与他们偏好相一致的用户内容的比例来选择是否参与社区。乍一看，内容管制的效果似乎很明显: 它限制了平台上的言论。但是，当考虑到用户参与决策时，其效果可能更为微妙，而且与直觉相反。例如，我们的模型可以直接演示调节策略如何增加参与，并使平台上可用的内容多样化。在我们的分析中，我们探索了与在线社区中的内容审核相关的一组丰富的相互关联的现象。我们首先描述了一类自然的节制政策在创建和维持稳定社区方面的有效性。在此基础上，我们探讨资源有限或意识形态平台如何制定政策，社区如何受到不同层次的个性化影响，以及平台之间的竞争。我们的模型提供了一个词汇表和数学上易处理的框架，用于分析关于内容管理的平台决策。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Content+Moderation+and+the+Formation+of+Online+Communities:+A+Theoretical+Framework)|0|
|[Getting Bored of Cyberwar: Exploring the Role of Low-level Cybercrime Actors in the Russia-Ukraine Conflict](https://doi.org/10.1145/3589334.3645401)|Anh V. Vu, Daniel R. Thomas, Ben Collier, Alice Hutchings, Richard Clayton, Ross J. Anderson||There has been substantial commentary on the role of cyberattacks carried by low-level cybercrime actors in the Russia-Ukraine conflict. We analyse 358k web defacement attacks, 1.7M reflected DDoS attacks, 1764 Hack Forums posts mentioning the two countries, and 441 announcements (with 58k replies) of a volunteer hacking group for two months before and four months after the invasion. We find the conflict briefly but notably caught the attention of low-level cybercrime actors, with significant increases in online discussion and both types of attack targeting Russia and Ukraine. However, there was little evidence of high-profile actions; the role of these players in the ongoing hybrid warfare is minor, and they should be separated from persistent and motivated 'hacktivists' in state-sponsored operations. Their involvement in the conflict appears to have been short-lived and fleeting, with a clear loss of interest in discussing the situation and carrying out both defacement and DDoS attacks against either Russia or Ukraine after a few weeks.|关于低级别网络犯罪行为者在俄罗斯-乌克兰冲突中发动的网络攻击的作用，已有大量评论。我们分析了35.8万次网络破坏攻击，170万次 DDoS 攻击，1764个黑客论坛帖子提到了这两个国家，以及441个志愿者黑客组织的声明(回复了58000条) ，分别发生在入侵前两个月和入侵后四个月。我们发现，这场冲突短暂但引人注目地引起了低级别网络犯罪行为者的注意，网上讨论大幅增加，针对俄罗斯和乌克兰的两类攻击也有所增加。然而，几乎没有高调行动的证据; 这些参与者在正在进行的混合战争中的作用是微不足道的，他们应该与在国家支持的行动中坚持和积极的“黑客活动分子”分开。他们参与冲突的时间似乎很短暂，而且转瞬即逝，几周后，他们显然对讨论局势、对俄罗斯或乌克兰进行涂鸦和 DDoS 攻击失去了兴趣。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Getting+Bored+of+Cyberwar:+Exploring+the+Role+of+Low-level+Cybercrime+Actors+in+the+Russia-Ukraine+Conflict)|0|
|[The Double Edged Sword: Identifying Authentication Pages and their Fingerprinting Behavior](https://doi.org/10.1145/3589334.3645493)|Asuman Senol, Alisha Ukani, Dylan Cutler, Igor Bilogrevic||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Double+Edged+Sword:+Identifying+Authentication+Pages+and+their+Fingerprinting+Behavior)|0|
|["Are Adversarial Phishing Webpages a Threat in Reality?" Understanding the Users' Perception of Adversarial Webpages](https://doi.org/10.1145/3589334.3645502)|Ying Yuan, Qingying Hao, Giovanni Apruzzese, Mauro Conti, Gang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q="Are+Adversarial+Phishing+Webpages+a+Threat+in+Reality?"+Understanding+the+Users'+Perception+of+Adversarial+Webpages)|0|
|[Hyperlink Hijacking: Exploiting Erroneous URL Links to Phantom Domains](https://doi.org/10.1145/3589334.3645510)|Kevin Saric, Felix Savins, Gowri Sankar Ramachandran, Raja Jurdak, Surya Nepal||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hyperlink+Hijacking:+Exploiting+Erroneous+URL+Links+to+Phantom+Domains)|0|
|[Fake Resume Attacks: Data Poisoning on Online Job Platforms](https://doi.org/10.1145/3589334.3645524)|Michiharu Yamashita, Thanh Tran, Dongwon Lee||While recent studies have exposed various vulnerabilities incurred from data poisoning attacks in many web services, little is known about the vulnerability on online professional job platforms (e.g., LinkedIn and Indeed). In this work, first time, we demonstrate the critical vulnerabilities found in the common Human Resources (HR) task of matching job seekers and companies on online job platforms. Capitalizing on the unrestricted format and contents of job seekers' resumes and easy creation of accounts on job platforms, we demonstrate three attack scenarios: (1) company promotion attack to increase the likelihood of target companies being recommended, (2) company demotion attack to decrease the likelihood of target companies being recommended, and (3) user promotion attack to increase the likelihood of certain users being matched to certain companies. To this end, we develop an end-to-end "fake resume" generation framework, titled FRANCIS, that induces systematic prediction errors via data poisoning. Our empirical evaluation on real-world datasets reveals that data poisoning attacks can markedly skew the results of matchmaking between job seekers and companies, regardless of underlying models, with vulnerability amplified in proportion to poisoning intensity. These findings suggest that the outputs of various services from job platforms can be potentially hacked by malicious users.|虽然最近的研究已经暴露了许多网络服务中的数据中毒攻击所带来的各种漏洞，但是对于在线专业工作平台(例如 LinkedIn 和》)上的漏洞知之甚少。在这项工作中，第一次，我们证明了在一般的人力资源(HR)任务匹配求职者和在线求职平台上的公司中发现的关键弱点。利用求职者简历中不受限制的格式和内容以及在求职平台上容易创建的账户，我们展示了三种攻击场景: (1)公司促销攻击，以增加目标公司被推荐的可能性; (2)公司降级攻击，以降低目标公司被推荐的可能性; (3)用户促销攻击，以增加某些用户与某些公司匹配的可能性。为此，我们开发了一个端到端的“虚假简历”生成框架，名为 FRANCIS，该框架通过数据中毒诱导系统预测错误。我们对现实世界数据集的实证评估表明，数据中毒攻击可以明显扭曲求职者和公司之间匹配的结果，无论潜在模型如何，脆弱性与中毒强度成比例放大。这些发现表明，来自工作平台的各种服务的输出可能被恶意用户黑客攻击。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fake+Resume+Attacks:+Data+Poisoning+on+Online+Job+Platforms)|0|
|[Identifying VPN Servers through Graph-Represented Behaviors](https://doi.org/10.1145/3589334.3645552)|Chenxu Wang, Jiangyi Yin, Zhao Li, Hongbo Xu, Zhongyi Zhang, Qingyun Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Identifying+VPN+Servers+through+Graph-Represented+Behaviors)|0|
|[Discovering and Measuring CDNs Prone to Domain Fronting](https://doi.org/10.1145/3589334.3645656)|Karthika Subramani, Roberto Perdisci, PierrosChristos Skafidas, Manos Antonakakis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Discovering+and+Measuring+CDNs+Prone+to+Domain+Fronting)|0|
|[Exploring Unconfirmed Transactions for Effective Bitcoin Address Clustering](https://doi.org/10.1145/3589334.3645684)|Kai Wang, Yakun Cheng, Michael Wen Tong, Zhenghao Niu, Jun Pang, Weili Han||The development of clustering heuristics has demonstrated that Bitcoin is not completely anonymous. Currently, existing clustering heuristics only consider confirmed transactions recorded in the Bitcoin blockchain. However, unconfirmed transactions in the mempool have yet to be utilized to improve the performance of the clustering heuristics. In this paper, we bridge this gap by combining unconfirmed and confirmed transactions for clustering Bitcoin addresses effectively. First, we present a data collection system for capturing unconfirmed transactions. Two case studies are performed to show the presence of user behaviors in unconfirmed transactions not present in confirmed transactions. Next, we apply the state-of-the-art clustering heuristics to unconfirmed transactions, and the clustering results can reduce the number of entities after applying, for example, the co-spend heuristics in confirmed transactions by 2.3%. Finally, we propose three novel clustering heuristics to capture specific behavior patterns in unconfirmed transactions, which further reduce the number of entities after the application of the co-spend heuristics by 9.8%. Our results demonstrate the utility of unconfirmed transactions in address clustering and further shed light on the limitations of anonymity in cryptocurrencies. To the best of our knowledge, this paper is the first to apply the unconfirmed transactions in Bitcoin to cluster addresses.|聚类启发法的发展表明，比特币并非完全匿名。目前，现有的集群试探法只考虑比特币区块链中记录的已确认交易。然而，内存池中未经证实的事务还没有被用来改进集群启发式算法的性能。在本文中，我们通过结合未确认和已确认的交易有效地聚类比特币地址来弥补这一差距。首先，我们提出了一个数据收集系统捕捉未确认的交易。两个案例研究表明，用户行为的存在在未经证实的交易中不存在的确认交易。接下来，我们将最先进的聚类启发法应用于未确认交易，聚类结果可以减少实体的数量，例如，在确认交易中共同支出启发法可以减少2.3% 。最后，我们提出了三种新的聚类启发式算法来捕获未确认事务中的特定行为模式，使得共花费启发式算法应用后的实体数量进一步减少了9.8% 。我们的研究结果证明了未确认事务在地址聚类中的有效性，并进一步揭示了加密货币中匿名性的局限性。据我们所知，本文是第一篇将未经证实的比特币交易应用于集群地址的论文。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Unconfirmed+Transactions+for+Effective+Bitcoin+Address+Clustering)|0|
|[AdFlush: A Real-World Deployable Machine Learning Solution for Effective Advertisement and Web Tracker Prevention](https://doi.org/10.1145/3589334.3645698)|Kiho Lee, Chaejin Lim, Beomjin Jin, Taeyoung Kim, Hyoungshick Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdFlush:+A+Real-World+Deployable+Machine+Learning+Solution+for+Effective+Advertisement+and+Web+Tracker+Prevention)|0|
|[Fingerprinting the Shadows: Unmasking Malicious Servers with Machine Learning-Powered TLS Analysis](https://doi.org/10.1145/3589334.3645719)|Andreas Theofanous, Eva Papadogiannaki, Alexander Shevtsov, Sotiris Ioannidis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fingerprinting+the+Shadows:+Unmasking+Malicious+Servers+with+Machine+Learning-Powered+TLS+Analysis)|0|
|[Efficient Computation of Signature-Restricted Views for Semantic Web Ontologies](https://doi.org/10.1145/3589334.3645317)|Yizheng Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Computation+of+Signature-Restricted+Views+for+Semantic+Web+Ontologies)|0|
|[Follow the Path: Hierarchy-Aware Extreme Multi-Label Completion for Semantic Text Tagging](https://doi.org/10.1145/3589334.3645558)|Natalia Ostapuk, Julien Audiffren, Ljiljana Dolamic, Alain Mermoud, Philippe CudréMauroux||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Follow+the+Path:+Hierarchy-Aware+Extreme+Multi-Label+Completion+for+Semantic+Text+Tagging)|0|
|[Multi-Label Zero-Shot Product Attribute-Value Extraction](https://doi.org/10.1145/3589334.3645649)|Jiaying Gong, Hoda Eldardiry||E-commerce platforms should provide detailed product descriptions (attribute values) for effective product search and recommendation. However, attribute value information is typically not available for new products. To predict unseen attribute values, large quantities of labeled training data are needed to train a traditional supervised learning model. Typically, it is difficult, time-consuming, and costly to manually label large quantities of new product profiles. In this paper, we propose a novel method to efficiently and effectively extract unseen attribute values from new products in the absence of labeled data (zero-shot setting). We propose HyperPAVE, a multi-label zero-shot attribute value extraction model that leverages inductive inference in heterogeneous hypergraphs. In particular, our proposed technique constructs heterogeneous hypergraphs to capture complex higher-order relations (i.e. user behavior information) to learn more accurate feature representations for graph nodes. Furthermore, our proposed HyperPAVE model uses an inductive link prediction mechanism to infer future connections between unseen nodes. This enables HyperPAVE to identify new attribute values without the need for labeled training data. We conduct extensive experiments with ablation studies on different categories of the MAVE dataset. The results demonstrate that our proposed HyperPAVE model significantly outperforms existing classification-based, generation-based large language models for attribute value extraction in the zero-shot setting.|电子商务平台应提供详细的产品描述(属性值) ，以便进行有效的产品搜索和推荐。但是，属性值信息通常不适用于新产品。为了预测未知的属性值，需要大量的标记训练数据来训练传统的监督式学习模型。通常，手动标记大量的新产品配置文件是困难的、耗时的和昂贵的。在本文中，我们提出了一种新的方法，有效地提取未见属性值的新产品在没有标记的数据(零拍设置)。我们提出了 HyperPAVE，一个利用异构超图中归纳推理的多标签零冲值属性值提取模型。特别是，我们提出的技术构造异构超图来捕获复杂的高阶关系(即用户行为信息) ，以学习更准确的图节点特征表示。此外，我们提出的 HyperPAVE 模型使用感应链路预测机制来推断未来未知节点之间的连接。这使得 HyperPAVE 能够识别新的属性值，而不需要标记的训练数据。我们进行了广泛的实验与烧蚀研究的不同类别的 MAVE 数据集。结果表明，我们提出的 HyperPAVE 模型明显优于现有的基于分类的、基于生成的大语言模型，在零拍摄环境下用于属性值提取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Label+Zero-Shot+Product+Attribute-Value+Extraction)|0|
|[Aligning Out-of-Distribution Web Images and Caption Semantics via Evidential Learning](https://doi.org/10.1145/3589334.3645653)|Guohao Sun, Yue Bai, Xueying Yang, Yi Fang, Yun Fu, Zhiqiang Tao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Aligning+Out-of-Distribution+Web+Images+and+Caption+Semantics+via+Evidential+Learning)|0|
|[Deliberate Exposure to Opposing Views and Its Association with Behavior and Rewards on Political Communities](https://doi.org/10.1145/3589334.3645375)|Alexandros Efstratiou||Engaging with diverse political views is important for reaching better collective decisions, however, users online tend to remain confined within ideologically homogeneous spaces. In this work, we study users who are members of these spaces but who also show a willingness to engage with diverse views, as they have the potential to introduce more informational diversity into their communities. Across four Reddit communities (r/Conservative, r/The_Donald, r/ChapoTrapHouse, r/SandersForPresident), we find that these users tend to use less hostile and more advanced and personable language, but receive fewer social rewards from their peers compared to others. We also find that social sanctions on the discussion community r/changemyview are insufficient to drive them out in the short term, though they may play a role over the longer term.|接触不同的政治观点对于达成更好的集体决策很重要，然而，在线用户往往被限制在意识形态相同的空间内。在这项工作中，我们研究了属于这些空间成员但也表示愿意接触不同观点的用户，因为他们有可能为其社区引入更多的信息多样性。在 Reddit 的四个社区(r/Insurance，r/The _ Donald，r/ChapoTrapHouse，r/SandersForPresident)中，我们发现这些用户倾向于使用更少的敌意，更先进和有个性的语言，但是从他们的同龄人那里得到的社会奖励比其他人少。我们还发现，对讨论社区 r/changemyview 的社会制裁不足以在短期内驱逐它们，尽管它们可能在较长期内发挥作用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deliberate+Exposure+to+Opposing+Views+and+Its+Association+with+Behavior+and+Rewards+on+Political+Communities)|0|
|[Invariant Graph Learning for Causal Effect Estimation](https://doi.org/10.1145/3589334.3645549)|Yongduo Sui, Caizhi Tang, Zhixuan Chu, Junfeng Fang, Yuan Gao, Qing Cui, Longfei Li, Jun Zhou, Xiang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Invariant+Graph+Learning+for+Causal+Effect+Estimation)|0|
|[Sublinear-Time Opinion Estimation in the Friedkin-Johnsen Model](https://doi.org/10.1145/3589334.3645572)|Stefan Neumann, Yinhao Dong, Pan Peng||Online social networks are ubiquitous parts of modern societies and the discussions that take place in these networks impact people's opinions on diverse topics, such as politics or vaccination. One of the most popular models to formally describe this opinion formation process is the Friedkin–Johnsen (FJ) model, which allows to define measures, such as the polarization and the disagreement of a network. Recently, Xu, Bao and Zhang (WebConf'21) showed that all opinions and relevant measures in the FJ model can be approximated in near-linear time. However, their algorithm requires the entire network and the opinions of all nodes as input. Given the sheer size of online social networks and increasing data-access limitations, obtaining the entirety of this data might, however, be unrealistic in practice. In this paper, we show that node opinions and all relevant measures, like polarization and disagreement, can be efficiently approximated in time that is sublinear in the size of the network. Particularly, our algorithms only require query-access to the network and do not have to preprocess the graph. Furthermore, we use a connection between FJ opinion dynamics and personalized PageRank, and show that in d-regular graphs, we can deterministically approximate each node's opinion by only looking at a constant-size neighborhood, independently of the network size. We also experimentally validate that our estimation algorithms perform well in practice.|在线社交网络是现代社会中无处不在的一部分，在这些网络中发生的讨论会影响人们对不同话题的看法，比如政治或疫苗接种。形式上描述这种观点形成过程的最流行的模型之一是 Friedkin-Johnsen (FJ)模型，它允许定义衡量标准，例如网络的极化和分歧。最近，徐、鲍、张(WebConf’21)表明，FJ 模型中的所有观点和相关措施都可以在近似线性时间内进行近似。然而，他们的算法需要整个网络和所有节点的意见作为输入。然而，考虑到在线社交网络的庞大规模和日益增加的数据访问限制，获取这些数据的全部在实践中可能是不现实的。在本文中，我们表明，节点意见和所有相关措施，如极化和分歧，可以有效地近似的时间，是次线性的网络规模。特别地，我们的算法只需要对网络进行查询访问，而不需要对图进行预处理。此外，我们使用 FJ 意见动态和个性化 PageRank 之间的联系，并表明在 d- 正则图中，我们可以确定性地近似每个节点的意见，只看一个恒定大小的邻域，独立于网络大小。我们还通过实验验证了我们的估计算法在实际应用中的良好性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sublinear-Time+Opinion+Estimation+in+the+Friedkin-Johnsen+Model)|0|
|[Bots, Elections, and Controversies: Twitter Insights from Brazil's Polarised Elections](https://doi.org/10.1145/3589334.3645651)|Diogo Pacheco||From 2018 to 2023, Brazil experienced its most fiercely contested elections in history, resulting in the election of far-right candidate Jair Bolsonaro followed by the left-wing, Lula da Silva. This period was marked by a murder attempt, a coup attempt, the pandemic, and a plethora of conspiracy theories and controversies. This paper analyses 437 million tweets originating from 13 million accounts associated with Brazilian politics during these two presidential election cycles. We focus on accounts' behavioural patterns. We noted a quasi-monotonic escalation in bot engagement, marked by notable surges both during COVID-19 and in the aftermath of the 2022 election. The data revealed a strong correlation between bot engagement and the number of replies during a single day ($r=0.66$, $p<0.01$). Furthermore, we identified a range of suspicious activities, including an unusually high number of accounts being created on the same day, with some days witnessing over 20,000 new accounts and super-prolific accounts generating close to 100,000 tweets. Lastly, we uncovered a sprawling network of accounts sharing Twitter handles, with a select few managing to utilise more than 100 distinct handles. This work can be instrumental in dismantling coordinated campaigns and offer valuable insights for the enhancement of bot detection algorithms.|从2018年到2023年，巴西经历了历史上竞争最激烈的选举，导致极右翼候选人雅伊尔 · 博索纳罗(Jair Bolsonaro)当选，随后是左翼人士卢拉 · 达席尔瓦(Lula da Silva)。这个时期的特点是谋杀未遂，政变未遂，流行病，以及过多的阴谋论和争议。本文分析了在这两个总统选举周期中，来自1300万个与巴西政治有关的账户的4.37亿条推文。我们关注客户的行为模式。我们注意到，机器人程序的参与程度出现了近乎单调的升级，无论是在2019冠状病毒疾病期间还是在2022年大选之后，都出现了明显的激增。数据显示，聊天机器人的参与度与一天内回复的数量之间存在很强的相关性($r = 0.66 $，$p < 0.01 $)。此外，我们还发现了一系列可疑活动，包括在同一天创建的账户数量异常之多，有些日子见证了超过20,000个新账户和产生近100,000条推文的超多产账户。最后，我们发现了一个庞大的帐户共享网络的 Twitter 句柄，其中一些设法使用超过100个不同的句柄。这项工作可以有助于拆除协调运动，并提供宝贵的见解增强机器人检测算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bots,+Elections,+and+Controversies:+Twitter+Insights+from+Brazil's+Polarised+Elections)|0|
|[Scalable Continuous-time Diffusion Framework for Network Inference and Influence Estimation](https://doi.org/10.1145/3589334.3645652)|Keke Huang, Ruize Gao, Bogdan Cautis, Xiaokui Xiao||The study of continuous-time information diffusion has been an important area of research for many applications in recent years. When only the diffusion traces (cascades) are accessible, cascade-based network inference and influence estimation are two essential problems to explore. Alas, existing methods exhibit limited capability to infer and process networks with more than a few thousand nodes, suffering from scalability issues. In this paper, we view the diffusion process as a continuous-time dynamical system, based on which we establish a continuous-time diffusion model. Subsequently, we instantiate the model to a scalable and effective framework (FIM) to approximate the diffusion propagation from available cascades, thereby inferring the underlying network structure. Furthermore, we undertake an analysis of the approximation error of FIM for network inference. To achieve the desired scalability for influence estimation, we devise an advanced sampling technique and significantly boost the efficiency. We also quantify the effect of the approximation error on influence estimation theoretically. Experimental results showcase the effectiveness and superior scalability of FIM on network inference and influence estimation.|连续时间信息扩散的研究是近年来许多应用领域的一个重要研究方向。当只有扩散轨迹(级联)可访问时，基于级联的网络推理和影响估计是两个需要研究的基本问题。遗憾的是，现有的方法在推断和处理超过几千个节点的网络方面能力有限，存在可伸缩性问题。在本文中，我们把扩散过程看作是一个连续时间的动力系统，在此基础上我们建立了一个连续时间扩散模型。随后，我们将该模型实例化为一个可扩展的有效框架(FIM) ，以近似可用级联的扩散传播，从而推断出潜在的网络结构。此外，我们亦会分析 FIM 在网络推理方面的逼近误差。为了实现影响估计所需的可扩展性，我们设计了一种先进的采样技术，大大提高了效率。我们还从理论上量化了逼近误差对影响估计的影响。实验结果表明 FIM 在网络推理和影响估计方面的有效性和优越的可扩展性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Continuous-time+Diffusion+Framework+for+Network+Inference+and+Influence+Estimation)|0|
|[Friend or Foe? Mining Suspicious Behavior via Graph Capsule Infomax Detector against Fraudsters](https://doi.org/10.1145/3589334.3645706)|Xiangping Zheng, Bo Wu, Xun Liang, Wei Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Friend+or+Foe?+Mining+Suspicious+Behavior+via+Graph+Capsule+Infomax+Detector+against+Fraudsters)|0|
|[PASS: Predictive Auto-Scaling System for Large-scale Enterprise Web Applications](https://doi.org/10.1145/3589334.3645330)|Yunda Guo, Jiake Ge, Panfeng Guo, Yunpeng Chai, Tao Li, Mengnan Shi, Yang Tu, Jian Ouyang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PASS:+Predictive+Auto-Scaling+System+for+Large-scale+Enterprise+Web+Applications)|0|
|[Unlocking the Non-deterministic Computing Power with Memory-Elastic Multi-Exit Neural Networks](https://doi.org/10.1145/3589334.3645340)|Jiaming Huang, Yi Gao, Wei Dong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unlocking+the+Non-deterministic+Computing+Power+with+Memory-Elastic+Multi-Exit+Neural+Networks)|0|
|[ZipZap: Efficient Training of Language Models for Large-Scale Fraud Detection on Blockchain](https://doi.org/10.1145/3589334.3645352)|Sihao Hu, Tiansheng Huang, KaHo Chow, Wenqi Wei, Yanzhao Wu, Ling Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ZipZap:+Efficient+Training+of+Language+Models+for+Large-Scale+Fraud+Detection+on+Blockchain)|0|
|[Cold Start or Hot Start? Robust Slow Start in Congestion Control with A Priori Knowledge for Mobile Web Services](https://doi.org/10.1145/3589334.3645393)|Jia Zhang, Haixuan Tong, Enhuan Dong, Xin Qian, Mingwei Xu, Xiaotian Li, Zili Meng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cold+Start+or+Hot+Start?+Robust+Slow+Start+in+Congestion+Control+with+A+Priori+Knowledge+for+Mobile+Web+Services)|0|
|[Investigations of Top-Level Domain Name Collisions in Blockchain Naming Services](https://doi.org/10.1145/3589334.3645459)|Daiki Ito, Yuta Takata, Hiroshi Kumagai, Masaki Kamizono||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Investigations+of+Top-Level+Domain+Name+Collisions+in+Blockchain+Naming+Services)|0|
|[Don't Bite Off More than You Can Chew: Investigating Excessive Permission Requests in Trigger-Action Integrations](https://doi.org/10.1145/3589334.3645721)|Liuhuo Wan, Kailong Wang, Kulani Mahadewa, Haoyu Wang, Guangdong Bai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Don't+Bite+Off+More+than+You+Can+Chew:+Investigating+Excessive+Permission+Requests+in+Trigger-Action+Integrations)|0|
|[Off-Policy Evaluation of Slate Bandit Policies via Optimizing Abstraction](https://doi.org/10.1145/3589334.3645343)|Haruka Kiyohara, Masahiro Nomura, Yuta Saito||We study off-policy evaluation (OPE) in the problem of slate contextual bandits where a policy selects multi-dimensional actions known as slates. This problem is widespread in recommender systems, search engines, marketing, to medical applications, however, the typical Inverse Propensity Scoring (IPS) estimator suffers from substantial variance due to large action spaces, making effective OPE a significant challenge. The PseudoInverse (PI) estimator has been introduced to mitigate the variance issue by assuming linearity in the reward function, but this can result in significant bias as this assumption is hard-to-verify from observed data and is often substantially violated. To address the limitations of previous estimators, we develop a novel estimator for OPE of slate bandits, called Latent IPS (LIPS), which defines importance weights in a low-dimensional slate abstraction space where we optimize slate abstractions to minimize the bias and variance of LIPS in a data-driven way. By doing so, LIPS can substantially reduce the variance of IPS without imposing restrictive assumptions on the reward function structure like linearity. Through empirical evaluation, we demonstrate that LIPS substantially outperforms existing estimators, particularly in scenarios with non-linear rewards and large slate spaces.|我们研究了非政策评估(OPE)在板岩情境土匪问题中的应用，其中一个政策选择了多维行为即板岩。这个问题在推荐系统、搜索引擎、市场营销以及医疗应用中广泛存在，然而，典型的反倾向评分(IPS)估计因为大的行为空间而存在很大的差异，使得有效的 OPE 成为一个重大的挑战。已经引入了伪逆(PI)估计器来通过假设奖励函数的线性来缓解方差问题，但是这可能导致显着的偏差，因为这个假设很难从观测数据中验证，并且经常被严重违反。为了解决先前估计量的局限性，我们开发了一种新的板岩土匪 OPE 估计量，称为潜在 IPS (LIPS) ，它定义了低维板岩抽象空间中的重要权重，其中我们优化板岩抽象，以数据驱动的方式最小化 LIPS 的偏差和方差。通过这样做，LIPS 可以大大减少 IPS 的方差，而不必对奖励函数结构(如线性)施加限制性假设。通过实证评估，我们证明了 LIPS 在很大程度上优于现有的估计量，特别是在非线性回报和大板岩空间的情况下。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Off-Policy+Evaluation+of+Slate+Bandit+Policies+via+Optimizing+Abstraction)|0|
|[Long-term Off-Policy Evaluation and Learning](https://doi.org/10.1145/3589334.3645446)|Yuta Saito, Himan Abdollahpouri, Jesse Anderton, Ben Carterette, Mounia Lalmas||Short- and long-term outcomes of an algorithm often differ, with damaging downstream effects. A known example is a click-bait algorithm, which may increase short-term clicks but damage long-term user engagement. A possible solution to estimate the long-term outcome is to run an online experiment or A/B test for the potential algorithms, but it takes months or even longer to observe the long-term outcomes of interest, making the algorithm selection process unacceptably slow. This work thus studies the problem of feasibly yet accurately estimating the long-term outcome of an algorithm using only historical and short-term experiment data. Existing approaches to this problem either need a restrictive assumption about the short-term outcomes called surrogacy or cannot effectively use short-term outcomes, which is inefficient. Therefore, we propose a new framework called Long-term Off-Policy Evaluation (LOPE), which is based on reward function decomposition. LOPE works under a more relaxed assumption than surrogacy and effectively leverages short-term rewards to substantially reduce the variance. Synthetic experiments show that LOPE outperforms existing approaches particularly when surrogacy is severely violated and the long-term reward is noisy. In addition, real-world experiments on large-scale A/B test data collected on a music streaming platform show that LOPE can estimate the long-term outcome of actual algorithms more accurately than existing feasible methods.|算法的短期和长期结果往往不同，具有破坏性的下游影响。一个已知的例子是点击诱饵算法，它可能增加短期点击，但损害长期用户参与。估计长期结果的一个可能的解决方案是对潜在的算法进行在线实验或 A/B 测试，但是观察感兴趣的长期结果需要几个月甚至更长的时间，使得算法选择过程慢得令人无法接受。因此，这项工作研究的问题是可行的，但准确地估计长期结果的算法只使用历史和短期的实验数据。解决这个问题的现有方法要么需要对称为代孕的短期结果进行限制性假设，要么不能有效地利用短期结果，这是低效的。因此，我们提出了一个基于报酬函数分解的长期非政策评估(LOPE)框架。LOPE 工作在一个比代孕更宽松的假设下，并有效地利用短期回报来大大减少差异。综合实验表明，LOPE 优于现有的方法，特别是当代孕是严重违反和长期奖励是噪音。此外，在音乐流媒体平台上对大规模 A/B 测试数据进行的实际测试表明，LOPE 能够比现有的可行方法更准确地估计实际算法的长期结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Long-term+Off-Policy+Evaluation+and+Learning)|0|
|[Unified Uncertainty Estimation for Cognitive Diagnosis Models](https://doi.org/10.1145/3589334.3645488)|Fei Wang, Qi Liu, Enhong Chen, Chuanren Liu, Zhenya Huang, Jinze Wu, Shijin Wang||Cognitive diagnosis models have been widely used in different areas, especially intelligent education, to measure users' proficiency levels on knowledge concepts, based on which users can get personalized instructions. As the measurement is not always reliable due to the weak links of the models and data, the uncertainty of measurement also offers important information for decisions. However, the research on the uncertainty estimation lags behind that on advanced model structures for cognitive diagnosis. Existing approaches have limited efficiency and leave an academic blank for sophisticated models which have interaction function parameters (e.g., deep learning-based models). To address these problems, we propose a unified uncertainty estimation approach for a wide range of cognitive diagnosis models. Specifically, based on the idea of estimating the posterior distributions of cognitive diagnosis model parameters, we first provide a unified objective function for mini-batch based optimization that can be more efficiently applied to a wide range of models and large datasets. Then, we modify the reparameterization approach in order to adapt to parameters defined on different domains. Furthermore, we decompose the uncertainty of diagnostic parameters into data aspect and model aspect, which better explains the source of uncertainty. Extensive experiments demonstrate that our method is effective and can provide useful insights into the uncertainty of cognitive diagnosis.|认知诊断模型已广泛应用于各个领域，尤其是智能教育领域，用于测量用户对知识概念的熟练程度，以此为基础，用户可以获得个性化的指令。由于模型和数据的薄弱环节，测量结果并不总是可靠的，因此测量的不确定性也为决策提供了重要信息。然而，对于认知诊断的不确定性估计的研究远远落后于先进的认知诊断模型结构。现有的方法效率有限，对于具有交互函数参数的复杂模型(例如，基于深度学习的模型)留下了学术空白。为了解决这些问题，我们提出了一个统一的不确定性估计方法的认知诊断模型的广泛范围。具体地说，基于认知诊断模型参数后验分布估计的思想，我们首先提供了一个统一的基于小批量优化的目标函数，可以更有效地应用于范围广泛的模型和大数据集。然后，我们修改了重参数化方法，以适应不同领域定义的参数。将诊断参数的不确定性分解为数据方面和模型方面，较好地解释了不确定性的来源。大量的实验表明，我们的方法是有效的，可以提供有益的洞察认知诊断的不确定性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unified+Uncertainty+Estimation+for+Cognitive+Diagnosis+Models)|0|
|[A Cross Domain Method for Customer Lifetime Value Prediction in Supply Chain Platform](https://doi.org/10.1145/3589334.3645391)|Zhiyuan Zhou, Li Lin, Hai Wang, Xiaolei Zhou, Gong Wei, Shuai Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Cross+Domain+Method+for+Customer+Lifetime+Value+Prediction+in+Supply+Chain+Platform)|0|
|[UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting](https://doi.org/10.1145/3589334.3645434)|Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang, Bryan Hooi, Roger Zimmermann||Multivariate time series forecasting plays a pivotal role in contemporary web technologies. In contrast to conventional methods that involve creating dedicated models for specific time series application domains, this research advocates for a unified model paradigm that transcends domain boundaries. However, learning an effective cross-domain model presents the following challenges. First, various domains exhibit disparities in data characteristics, e.g., the number of variables, posing hurdles for existing models that impose inflexible constraints on these factors. Second, the model may encounter difficulties in distinguishing data from various domains, leading to suboptimal performance in our assessments. Third, the diverse convergence rates of time series domains can also result in compromised empirical performance. To address these issues, we propose UniTime for effective cross-domain time series learning. Concretely, UniTime can flexibly adapt to data with varying characteristics. It also uses domain instructions and a Language-TS Transformer to offer identification information and align two modalities. In addition, UniTime employs masking to alleviate domain convergence speed imbalance issues. Our extensive experiments demonstrate the effectiveness of UniTime in advancing state-of-the-art forecasting performance and zero-shot transferability.|多变量时间序列预测在当代网络技术中起着举足轻重的作用。与传统的方法，涉及创建专门的模型为特定的时间序列应用领域，这项研究提倡一个统一的模型范例，超越领域边界。然而，学习一个有效的跨领域模型提出了以下挑战。首先，不同的领域表现出数据特征的差异，例如，变量的数量，为对这些因素施加不灵活约束的现有模型设置了障碍。其次，该模型在区分不同领域的数据时可能会遇到困难，从而导致我们的评估性能不理想。第三，不同的收敛速度的时间序列领域也可能导致折衷的经验性能。为了解决这些问题，我们提出了有效的跨域时间序列学习 UniTime。具体来说，UniTime 可以灵活地适应不同特征的数据。它还使用领域指令和 Language-TS Transformer 来提供标识信息和对齐两种模式。另外，UniTime 采用屏蔽技术来缓解领域收敛速度不平衡的问题。我们的大量实验证明了 UniTime 在提高最先进的预测性能和零点可转移性方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UniTime:+A+Language-Empowered+Unified+Model+for+Cross-Domain+Time+Series+Forecasting)|0|
|[Semantic Evolvement Enhanced Graph Autoencoder for Rumor Detection](https://doi.org/10.1145/3589334.3645478)|Xiang Tao, Liang Wang, Qiang Liu, Shu Wu, Liang Wang||Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Recently, numerous rumor detection models which utilize textual information and the propagation structure of events have been proposed. However, these methods overlook the importance of semantic evolvement information of event in propagation process, which is often challenging to be truly learned in supervised training paradigms and traditional rumor detection methods. To address this issue, we propose a novel semantic evolvement enhanced Graph Autoencoder for Rumor Detection (GARD) model in this paper. The model learns semantic evolvement information of events by capturing local semantic changes and global semantic evolvement information through specific graph autoencoder and reconstruction strategies. By combining semantic evolvement information and propagation structure information, the model achieves a comprehensive understanding of event propagation and perform accurate and robust detection, while also detecting rumors earlier by capturing semantic evolvement information in the early stages. Moreover, in order to enhance the model's ability to learn the distinct patterns of rumors and non-rumors, we introduce a uniformity regularizer to further improve the model's performance. Experimental results on three public benchmark datasets confirm the superiority of our GARD method over the state-of-the-art approaches in both overall performance and early rumor detection.|由于谣言在社交媒体上的迅速传播，谣言检测已经成为一个极其重要的挑战。最近，许多利用文本信息和事件传播结构的谣言检测模型被提出。然而，这些方法忽视了事件语义演化信息在传播过程中的重要性，在有监督的训练范式和传统的谣言检测方法中往往难以真正学习到这些信息。针对这一问题，本文提出了一种新的语义演化增强型图形自动编码器用于谣言检测(GARD)模型。该模型通过特定的图形自动编码和重构策略获取事件的局部语义变化和全局语义演化信息，从而获取事件的语义演化信息。该模型将语义演化信息与传播结构信息相结合，实现了对事件传播的全面理解和准确鲁棒的检测，同时在早期阶段通过捕获语义演化信息提前检测谣言。此外，为了提高模型学习谣言和非谣言的不同模式的能力，我们引入了一个一致性正则化器来进一步提高模型的性能。在三个公共基准数据集上的实验结果证实了我们的 GARD 方法在总体性能和早期谣言检测方面都优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semantic+Evolvement+Enhanced+Graph+Autoencoder+for+Rumor+Detection)|0|
|[Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding](https://doi.org/10.1145/3589334.3645534)|Haoming Li, Yusen Huo, Shuai Dou, Zhenzhe Zheng, Zhilin Zhang, Chuan Yu, Jian Xu, Fan Wu||In online advertising, advertisers participate in ad auctions to acquire ad opportunities, often by utilizing auto-bidding tools provided by demand-side platforms (DSPs). The current auto-bidding algorithms typically employ reinforcement learning (RL). However, due to safety concerns, most RL-based auto-bidding policies are trained in simulation, leading to a performance degradation when deployed in online environments. To narrow this gap, we can deploy multiple auto-bidding agents in parallel to collect a large interaction dataset. Offline RL algorithms can then be utilized to train a new policy. The trained policy can subsequently be deployed for further data collection, resulting in an iterative training framework, which we refer to as iterative offline RL. In this work, we identify the performance bottleneck of this iterative offline RL framework, which originates from the ineffective exploration and exploitation caused by the inherent conservatism of offline RL algorithms. To overcome this bottleneck, we propose Trajectory-wise Exploration and Exploitation (TEE), which introduces a novel data collecting and data utilization method for iterative offline RL from a trajectory perspective. Furthermore, to ensure the safety of online exploration while preserving the dataset quality for TEE, we propose Safe Exploration by Adaptive Action Selection (SEAS). Both offline experiments and real-world experiments on Alibaba display advertising platform demonstrate the effectiveness of our proposed method.|在网络广告中，广告主通过参与广告拍卖来获取广告机会，通常是利用需求侧平台(DSP)提供的自动投标工具。目前的自动竞价算法通常采用强化学习(RL)。然而，出于安全考虑，大多数基于 RL 的自动投标策略都是在仿真中进行训练的，当部署在在线环境中时，会导致性能下降。为了缩小这一差距，我们可以并行部署多个自动招标代理来收集大型交互数据集。然后可以利用离线 RL 算法来训练新的策略。经过训练的策略随后可以部署用于进一步的数据收集，从而产生迭代训练框架，我们称之为迭代离线 RL。在这项工作中，我们确定了这个迭代离线 RL 框架的性能瓶颈，这源于无效的探索和开发造成的固有的保守性离线 RL 算法。为了克服这一瓶颈，本文提出了基于轨迹的探索与开发(TEE)方法，从轨迹的角度出发，提出了一种新的迭代离线 RL 的数据采集与利用方法。此外，为了保证在线勘探的安全性，同时保证 TEE 数据集的质量，我们提出了自适应行动选择(SEAS)的安全勘探方法。在阿里巴巴展示广告平台上的离线实验和现实世界的实验都证明了我们提出的方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Trajectory-wise+Iterative+Reinforcement+Learning+Framework+for+Auto-bidding)|0|
|[Stable-Sketch: A Versatile Sketch for Accurate, Fast, Web-Scale Data Stream Processing](https://doi.org/10.1145/3589334.3645581)|Weihe Li, Paul Patras||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Stable-Sketch:+A+Versatile+Sketch+for+Accurate,+Fast,+Web-Scale+Data+Stream+Processing)|0|
|[Self-Paced Pairwise Representation Learning for Semi-Supervised Text Classification](https://doi.org/10.1145/3589334.3645664)|Junfan Chen, Richong Zhang, Jiarui Wang, Chunming Hu, Yongyi Mao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Paced+Pairwise+Representation+Learning+for+Semi-Supervised+Text+Classification)|0|
|[Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering](https://doi.org/10.1145/3589334.3645670)|Hongda Sun, Yuxuan Liu, Chengwei Wu, Haiyu Yan, Cheng Tai, Xin Gao, Shuo Shang, Rui Yan||Open-domain question answering (ODQA) has emerged as a pivotal research spotlight in information systems. Existing methods follow two main paradigms to collect evidence: (1) The retrieve-then-read paradigm retrieves pertinent documents from an external corpus; and (2) the generate-then-read paradigm employs large language models (LLMs) to generate relevant documents. However, neither can fully address multifaceted requirements for evidence. To this end, we propose LLMQA, a generalized framework that formulates the ODQA process into three basic steps: query expansion, document selection, and answer generation, combining the superiority of both retrieval-based and generation-based evidence. Since LLMs exhibit their excellent capabilities to accomplish various tasks, we instruct LLMs to play multiple roles as generators, rerankers, and evaluators within our framework, integrating them to collaborate in the ODQA process. Furthermore, we introduce a novel prompt optimization algorithm to refine role-playing prompts and steer LLMs to produce higher-quality evidence and answers. Extensive experimental results on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate that LLMQA achieves the best performance in terms of both answer accuracy and evidence quality, showcasing its potential for advancing ODQA research and applications.|开放域问答技术(ODQA)已经成为信息系统领域的一个重要研究热点。现有方法遵循两种主要的证据收集范式: (1)检索-然后阅读范式从外部语料库检索相关文档; (2)生成-然后阅读范式使用大语言模型(LLM)生成相关文档。然而，两者都不能完全满足证据的多方面需求。为此，结合基于检索和基于生成的证据的优势，我们提出了一个通用框架 LLMQA，它将 ODQA 过程描述为三个基本步骤: 查询扩展、文档选择和答案生成。由于 LLM 展示了它们完成各种任务的出色能力，我们指示 LLM 在我们的框架中扮演生成器、重新排序器和评估器的多重角色，将它们集成到 ODQA 过程中进行协作。此外，我们还引入了一种新的提示优化算法来细化角色扮演提示，并引导 LLM 产生更高质量的证据和答案。在广泛使用的基准测试(NQ，WebQ 和 TriviaQA)上的广泛实验结果表明，LLMQA 在答案准确性和证据质量方面取得了最佳性能，展示了其推进 ODQA 研究和应用的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Harnessing+Multi-Role+Capabilities+of+Large+Language+Models+for+Open-Domain+Question+Answering)|0|
|[POLISH: Adaptive Online Cross-Modal Hashing for Class Incremental Data](https://doi.org/10.1145/3589334.3645716)|YuWei Zhan, Xin Luo, ZhenDuo Chen, Yongxin Wang, Yinwei Wei, XinShun Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=POLISH:+Adaptive+Online+Cross-Modal+Hashing+for+Class+Incremental+Data)|0|
|[How Contentious Terms About People and Cultures are Used in Linked Open Data](https://doi.org/10.1145/3589334.3648140)|Andrei Nesterov, Laura Hollink, Jacco van Ossenbruggen||Web resources in linked open data (LOD) are comprehensible to humans through literal textual values attached to them, such as labels, notes, or comments. Word choices in literals may not always be neutral. When outdated and culturally stereotyping terminology is used in literals, they may appear as offensive to users in interfaces and propagate stereotypes to algorithms trained on them. We study how frequently and in which literals contentious terms about people and cultures occur in LOD and whether there are attempts to mark the usage of such terms. For our analysis, we reuse English and Dutch terms from a knowledge graph that provides opinions of experts from the cultural heritage domain about terms' contentiousness. We inspect occurrences of these terms in four widely used datasets: Wikidata, The Getty Art & Architecture Thesaurus, Princeton WordNet, and Open Dutch WordNet. Some terms are ambiguous and contentious only in particular senses. Applying word sense disambiguation, we generate a set of literals relevant to our analysis. We found that outdated, derogatory, stereotyping terms frequently appear in descriptive and labelling literals, such as preferred labels that are usually displayed in interfaces and used for indexing. In some cases, LOD contributors mark contentious terms with words and phrases in literals (implicit markers) or properties linked to resources (explicit markers). However, such marking is rare and non-consistent in all datasets. Our quantitative and qualitative insights could be helpful in developing more systematic approaches to address the propagation of stereotypes via LOD.|链接开放数据(LOD)中的 Web 资源可以通过附加到它们的文本值(如标签、注释或注释)让人们理解。字面意义上的单词选择可能并不总是中性的。当过时的和文化上的陈规定型术语用于文字时，它们可能会在界面中显得冒犯用户，并将陈规定型传播给接受过相关培训的算法。我们研究 LOD 中关于人和文化的有争议的字面术语的出现频率和出现频率，以及是否有人试图标记这些术语的用法。对于我们的分析，我们从一个知识图中重用英语和荷兰语术语，该知识图提供了来自文化遗产领域的专家关于术语争议性的意见。我们在四个广泛使用的数据集中检查这些术语的出现情况: Wikidata、 The Getty Art & Architecture Thesaurus、 Princeton WordNet 和 Open Dutch WordNet。有些术语只在特定的意义上是模棱两可和有争议的。应用词义消歧，我们生成了一组与我们的分析相关的文字。我们发现，过时的、贬义的、刻板的术语经常出现在描述和标签文字中，例如首选标签通常显示在接口中并用于索引。在某些情况下，LOD 参与者使用文字(隐式标记)或与资源(显式标记)相关的属性中的单词和短语来标记有争议的术语。然而，这样的标记在所有数据集中是罕见和不一致的。我们在定量和定性方面的见解可能有助于发展更系统的方法来解决通过检测限传播刻板印象的问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Contentious+Terms+About+People+and+Cultures+are+Used+in+Linked+Open+Data)|0|
|[MMAdapt: A Knowledge-guided Multi-source Multi-class Domain Adaptive Framework for Early Health Misinformation Detection](https://doi.org/10.1145/3589334.3648152)|Lanyu Shang, Yang Zhang, Bozhang Chen, Ruohan Zong, Zhenrui Yue, Huimin Zeng, Na Wei, Dong Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MMAdapt:+A+Knowledge-guided+Multi-source+Multi-class+Domain+Adaptive+Framework+for+Early+Health+Misinformation+Detection)|0|
|[LightCS: Selecting Quadratic Feature Crosses in Linear Complexity](https://doi.org/10.1145/3589335.3648300)|Zhaocheng Du, Junhao Chen, Qinglin Jia, Chuhan Wu, Jieming Zhu, Zhenhua Dong, Ruiming Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LightCS:+Selecting+Quadratic+Feature+Crosses+in+Linear+Complexity)|0|
|[SOIL: Score Conditioned Diffusion Model for Imbalanced Cloud Failure Prediction](https://doi.org/10.1145/3589335.3648303)|Chiming Duan, Fangkai Yang, Pu Zhao, Lingling Zheng, Yash Dagli, Yudong Liu, Qingwei Lin, Dongmei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SOIL:+Score+Conditioned+Diffusion+Model+for+Imbalanced+Cloud+Failure+Prediction)|0|
|[Dependency Aware Incident Linking in Large Cloud Systems](https://doi.org/10.1145/3589335.3648311)|Supriyo Ghosh, Karish Grover, Jimmy Wong, Chetan Bansal, Rakesh Namineni, Mohit Verma, Saravan Rajmohan||Despite significant reliability efforts, large-scale cloud services inevitably experience production incidents that can significantly impact service availability and customer's satisfaction. Worse, in many cases one incident can lead to multiple downstream failures due to cascading effects that creates several related incidents across different dependent services. Often time On-call Engineers (OCEs) examine these incidents in silos that lead to significant amount of manual toil and increase the overall time-to-mitigate incidents. Therefore, developing efficient incident linking models is of paramount importance for grouping related incidents into clusters so as to quickly resolve major outages and reduce on-call fatigue. Existing incident linking methods mostly leverages textual and contextual information of incidents (e.g., title, description, severity, impacted components), thus failing to leverage the inter-dependencies between services. In this paper, we propose the dependency-aware incident linking (DiLink) framework which leverages both textual and service dependency graph information to improve the accuracy and coverage of incident links not only coming from same service, but also from different services and workloads. Furthermore, we propose a novel method to align the embeddings of multi-modal (i.e., textual and graphical) data using Orthogonal Procrustes. Extensive experimental results on real-world incidents from 5 workloads of Microsoft demonstrate that our alignment method has an F1-score of 0.96 (14 are also in the process of deploying this solution across 610 services from these 5 workloads for continuously supporting OCEs improving incident management and reducing manual toil.|尽管在可靠性方面做出了重大努力，但大规模云服务不可避免地会遇到生产事件，这些事件会显著影响服务的可用性和客户的满意度。更糟糕的是，在许多情况下，由于级联效应，一个事件可能导致多个下游故障，从而在不同的依赖服务之间产生多个相关事件。通常情况下，待命工程师(OCEs)在筒仓中检查这些事故，这些事故导致大量的人工劳动，并增加了缓解事故的整体时间。因此，开发有效的事故连接模型对于将相关事故分组以快速解决重大故障并减少待命疲劳至关重要。现有的事件链接方法主要利用事件的文本和上下文信息(例如，标题、描述、严重性、受影响的组件) ，因此无法利用服务之间的相互依赖性。在本文中，我们提出了依赖感知事件链接(DiLink)框架，它利用文本和服务依赖图信息来提高事件链接的准确性和覆盖率，这些事件链接不仅来自相同的服务，而且来自不同的服务和工作负载。此外，我们提出了一种新的方法来对齐多模态(即，文本和图形)数据的嵌入使用正交 Procrustes。来自微软5个工作负载的关于真实世界事件的广泛实验结果表明，我们的对齐方法的 F1得分为0.96(14个也正在从这5个工作负载部署这个解决方案到610个服务，以持续支持 OCEs 改进事件管理和减少手工劳动。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dependency+Aware+Incident+Linking+in+Large+Cloud+Systems)|0|
|[A Graph-based Framework for Reducing False Positives in Authentication Alerts in Security Systems](https://doi.org/10.1145/3589335.3648325)|Yanbang Wang, Karl Hallgren, Jonathan Larson||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Graph-based+Framework+for+Reducing+False+Positives+in+Authentication+Alerts+in+Security+Systems)|0|
|[FinReport: Explainable Stock Earnings Forecasting via News Factor Analyzing Model](https://doi.org/10.1145/3589335.3648330)|Xiangyu Li, Xinjie Shen, Yawen Zeng, Xiaofen Xing, Jin Xu||The task of stock earnings forecasting has received considerable attention due to the demand investors in real-world scenarios. However, compared with financial institutions, it is not easy for ordinary investors to mine factors and analyze news. On the other hand, although large language models in the financial field can serve users in the form of dialogue robots, it still requires users to have financial knowledge to ask reasonable questions. To serve the user experience, we aim to build an automatic system, FinReport, for ordinary investors to collect information, analyze it, and generate reports after summarizing. Specifically, our FinReport is based on financial news announcements and a multi-factor model to ensure the professionalism of the report. The FinReport consists of three modules: news factorization module, return forecasting module, risk assessment module. The news factorization module involves understanding news information and combining it with stock factors, the return forecasting module aim to analysis the impact of news on market sentiment, and the risk assessment module is adopted to control investment risk. Extensive experiments on real-world datasets have well verified the effectiveness and explainability of our proposed FinReport. Our codes and datasets are available at https://github.com/frinkleko/FinReport.|由于在现实世界中需求投资者的存在，股票收益预测的任务受到了相当大的关注。然而，与金融机构相比，普通投资者要挖掘因素、分析新闻并不容易。另一方面，尽管金融领域的大型语言模型可以以对话机器人的形式为用户提供服务，但它仍然需要用户具备金融知识才能提出合理的问题。为了服务于用户体验，我们的目标是建立一个自动化的系统，FinReport，为普通投资者收集信息，分析它，并在总结后生成报告。具体来说，我们的 FinReport 以财经新闻公告和多因素模型为基础，以确保报告的专业性。FinReport 由三个模块组成: 新闻分解模块、收益预测模块、风险评估模块。新闻因子分解模块包括了解新闻信息并将其与股票因子相结合，收益预测模块旨在分析新闻对市场情绪的影响，风险评估模块用于控制投资风险。在真实世界数据集上的大量实验已经很好地验证了我们提出的 FinReport 的有效性和可解释性。我们的代码和数据集 https://github.com/frinkleko/finreport 可查。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FinReport:+Explainable+Stock+Earnings+Forecasting+via+News+Factor+Analyzing+Model)|0|
|[DISKCO : Disentangling Knowledge from Cross-Encoder to Bi-Encoder](https://doi.org/10.1145/3589335.3648333)|Ankith M. S, Arindam Bhattacharya, Ankit Gandhi, Vijay Huddar, Atul Saroop, Rahul Bhagat||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DISKCO+:+Disentangling+Knowledge+from+Cross-Encoder+to+Bi-Encoder)|0|
|[Information Diffusion Meets Invitation Mechanism](https://doi.org/10.1145/3589335.3648337)|Shiqi Zhang, Jiachen Sun, Wenqing Lin, Xiaokui Xiao, Yiqian Huang, Bo Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Information+Diffusion+Meets+Invitation+Mechanism)|0|
|[The MMO Economist: AI Empowers Robust, Healthy, and Sustainable P2W MMO Economies](https://doi.org/10.1145/3589335.3648344)|Shiwei Zhao, Xi Yuan, Runze Wu, Zhipeng Hu, Haoyu Liu, Kai Wang, Yujing Hu, Tangjie Lv, Changjie Fan, Xin Tong, Jiangze Han, Yan Zheng, Jianye Hao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+MMO+Economist:+AI+Empowers+Robust,+Healthy,+and+Sustainable+P2W+MMO+Economies)|0|
|[Skewness-aware Boosting Regression Trees for Customer Contribution Prediction in Financial Precision Marketing](https://doi.org/10.1145/3589335.3648346)|HsinYu Chen, ChengTe Li, TingYu Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Skewness-aware+Boosting+Regression+Trees+for+Customer+Contribution+Prediction+in+Financial+Precision+Marketing)|0|
|[Can we Soft Prompt LLMs for Graph Learning Tasks?](https://doi.org/10.1145/3589335.3651476)|Zheyuan Liu, Xiaoxin He, Yijun Tian, Nitesh V. Chawla||Graph plays an important role in representing complex relationships in real-world applications such as social networks, biological data and citation networks. In recent years, Large Language Models (LLMs) have achieved tremendous success in various domains, which makes applying LLMs to graphs particularly appealing. However, directly applying LLMs to graph modalities presents unique challenges due to the discrepancy and mismatch between the graph and text modalities. Hence, to further investigate LLMs' potential for comprehending graph information, we introduce GraphPrompter, a novel framework designed to align graph information with LLMs via soft prompts. Specifically, GraphPrompter consists of two main components: a graph neural network to encode complex graph information and an LLM that effectively processes textual information. Comprehensive experiments on various benchmark datasets under node classification and link prediction tasks demonstrate the effectiveness of our proposed method. The GraphPrompter framework unveils the substantial capabilities of LLMs as predictors in graph-related tasks, enabling researchers to utilize LLMs across a spectrum of real-world graph scenarios more effectively.|在社会网络、生物数据和引文网络等现实应用中，图表在表示复杂关系方面发挥着重要作用。近年来，大型语言模型(LLM)在各个领域取得了巨大的成功，这使得将 LLM 应用于图形尤其具有吸引力。然而，由于图形和文本模式之间的差异和不匹配，直接将 LLM 应用于图形模式提出了独特的挑战。因此，为了进一步研究 LLM 理解图形信息的潜力，我们引入了 GraphPrompter，这是一个通过软提示将图形信息与 LLM 对齐的新框架。具体来说，GraphPrompter 由两个主要部分组成: 一个用于编码复杂图形信息的图形神经网络和一个有效处理文本信息的 LLM。在节点分类和链路预测任务下对各种基准数据集进行了综合实验，验证了该方法的有效性。GraphPrompter 框架揭示了 LLM 在图形相关任务中作为预测器的巨大能力，使研究人员能够更有效地利用 LLM 跨越一系列真实世界的图形场景。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+we+Soft+Prompt+LLMs+for+Graph+Learning+Tasks?)|0|
|[Everything Perturbed All at Once: Enabling Differentiable Graph Attacks](https://doi.org/10.1145/3589335.3651501)|Haoran Liu, Bokun Wang, Jianling Wang, Xiangjue Dong, Tianbao Yang, James Caverlee||As powerful tools for representation learning on graphs, graph neural networks (GNNs) have played an important role in applications including social networks, recommendation systems, and online web services. However, GNNs have been shown to be vulnerable to adversarial attacks, which can significantly degrade their effectiveness. Recent state-of-the-art approaches in adversarial attacks rely on gradient-based meta-learning to selectively perturb a single edge with the highest attack score until they reach the budget constraint. While effective in identifying vulnerable links, these methods are plagued by high computational costs. By leveraging continuous relaxation and parameterization of the graph structure, we propose a novel attack method called Differentiable Graph Attack (DGA) to efficiently generate effective attacks and meanwhile eliminate the need for costly retraining. Compared to the state-of-the-art, DGA achieves nearly equivalent attack performance with 6 times less training time and 11 times smaller GPU memory footprint on different benchmark datasets. Additionally, we provide extensive experimental analyses of the transferability of the DGA among different graph models, as well as its robustness against widely-used defense mechanisms.|图神经网络(GNN)作为图表示学习的强大工具，在社交网络、推荐系统和在线 Web 服务等应用中发挥着重要作用。然而，GNN 已被证明易受敌对攻击，这可以显着降低他们的有效性。最近最先进的对抗性攻击方法依赖于基于梯度的元学习来有选择性地扰乱攻击分数最高的单个边缘，直到它们达到预算线。虽然这些方法能有效地识别易受攻击的链接，但计算成本较高。通过利用图结构的连续松弛和参量化，我们提出了一种新的攻击方法，称为可微图攻击(DGA) ，以有效地生成有效的攻击，同时消除了昂贵的再训练的需要。与现有技术相比，DGA 在不同的基准数据集上实现了几乎相同的攻击性能，训练时间减少了6倍，GPU 内存占用减少了11倍。此外，我们提供了广泛的实验分析 DGA 在不同的图模型之间的可转移性，以及其对广泛使用的防御机制的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Everything+Perturbed+All+at+Once:+Enabling+Differentiable+Graph+Attacks)|0|
|[Unlink to Unlearn: Simplifying Edge Unlearning in GNNs](https://doi.org/10.1145/3589335.3651578)|Jiajun Tan, Fei Sun, Ruichen Qiu, Du Su, Huawei Shen||As concerns over data privacy intensify, unlearning in Graph Neural Networks (GNNs) has emerged as a prominent research frontier in academia. This concept is pivotal in enforcing the right to be forgotten, which entails the selective removal of specific data from trained GNNs upon user request. Our research focuses on edge unlearning, a process of particular relevance to real-world applications, owing to its widespread applicability. Current state-of-the-art approaches like GNNDelete can eliminate the influence of specific edges, yet our research has revealed a critical limitation in these approaches, termed over-forgetting. It occurs when the unlearning process inadvertently removes excessive information beyond specific data, leading to a significant decline in prediction accuracy for the remaining edges. To address this issue, we have identified the loss functions of GNNDelete as the primary source of the over-forgetting phenomenon. Furthermore, our analysis also suggests that loss functions may not be essential for effective edge unlearning. Building on these insights, we have simplified GNNDelete to develop Unlink-to-Unlearn (UtU), a novel method that facilitates unlearning exclusively through unlinking the forget edges from graph structure. Our extensive experiments demonstrate that UtU delivers privacy protection on par with that of a retrained model while preserving high accuracy in downstream tasks. Specifically, UtU upholds over 97.3 link prediction accuracy. Meanwhile, UtU requires only constant computational demands, underscoring its advantage as a highly lightweight and practical edge unlearning solution.|随着人们对数据隐私的担忧加剧，图形神经网络(GNN)中的去学习已经成为学术界一个突出的研究前沿。这一概念对于落实被遗忘的权利至关重要，因为这需要根据用户的要求，有选择地从受过训练的 GNN 中删除特定数据。我们的研究侧重于边缘去除，由于其广泛的适用性，这是一个与现实世界应用特别相关的过程。当前最先进的方法如 GNNDelete 可以消除特定边缘的影响，但我们的研究揭示了这些方法的一个关键限制，称为过度遗忘。当忘记过程无意中删除了特定数据之外的过多信息时，就会发生这种情况，导致剩余边缘的预测准确性显著下降。为了解决这个问题，我们确定了 GNNDelete 的损失函数是过度遗忘现象的主要来源。此外，我们的分析还表明，损失函数可能不是必不可少的有效边缘去除。在这些见解的基础上，我们简化了 GNNDelete 来开发 Unlink-to-Unlearn (UtU) ，这是一种新的方法，通过从图形结构中解除遗忘边的连接来促进完全忘记。我们的大量实验表明，UtU 提供了与再训练模型相当的隐私保护，同时在下游任务中保持了高精度。具体来说，UtU 支持超过97.3的链路预测精度。同时，UtU 只需要不断的计算需求，突出了其作为一个高度轻量级和实用的边缘去学习解决方案的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unlink+to+Unlearn:+Simplifying+Edge+Unlearning+in+GNNs)|0|
|[Near-duplicate Question Detection](https://doi.org/10.1145/3589335.3651538)|Preetam Prabhu Srikar Dammu, Omar Alonso||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Near-duplicate+Question+Detection)|0|
|[Tackling Long-Tail Entities for Temporal Knowledge Graph Completion](https://doi.org/10.1145/3589335.3651565)|Mehrnoosh Mirtaheri, Ryan A. Rossi, Sungchul Kim, Kanak Mahadik, Tong Yu, Xiang Chen, Mohammad Rostami||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tackling+Long-Tail+Entities+for+Temporal+Knowledge+Graph+Completion)|0|
|[From Creation to Clarification: ChatGPT's Journey Through the Fake News Quagmire](https://doi.org/10.1145/3589335.3651509)|Yue Huang, Kai Shu, Philip S. Yu, Lichao Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Creation+to+Clarification:+ChatGPT's+Journey+Through+the+Fake+News+Quagmire)|0|
|[The Invisible Game on the Internet: A Case Study of Decoding Deceptive Patterns](https://doi.org/10.1145/3589335.3651571)|Zewei Shi, Ruoxi Sun, Jieshan Chen, Jiamou Sun, Minhui Xue||Deceptive patterns are design practices embedded in digital platforms to manipulate users, representing a widespread and long-standing issue in the web and mobile software development industry. Legislative actions highlight the urgency of globally regulating deceptive patterns. However, despite advancements in detection tools, a significant gap exists in assessing deceptive pattern risks. In this study, we introduce a comprehensive approach involving the interactions between the Adversary, Watchdog (e.g., detection tools), and Challengers (e.g., users) to formalize and decode deceptive pattern threats. Based on this, we propose a quantitative risk assessment system. Representative cases are analyzed to showcase the practicability of the proposed risk scoring system, emphasizing the importance of involving human factors in deceptive pattern risk assessment.|欺骗性模式是嵌入数字平台以操纵用户的设计实践，代表了网络和移动软件开发行业中一个广泛和长期存在的问题。立法行动突出了在全球监管欺骗模式的紧迫性。然而，尽管检测工具有所进步，但在评估欺骗模式风险方面仍存在显著差距。在这项研究中，我们介绍了一个全面的方法，涉及之间的互动的对手，看门狗(例如，检测工具)和挑战者(例如，用户)形式化和解码欺骗模式的威胁。在此基础上，我们提出了一个定量的风险评估体系。通过对典型案例的分析，说明了该风险评分系统的实用性，强调了在欺骗模式风险评估中引入人为因素的重要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Invisible+Game+on+the+Internet:+A+Case+Study+of+Decoding+Deceptive+Patterns)|0|
|[Improving Model Robustness against Adversarial Examples with Redundant Fully Connected Layer](https://doi.org/10.1145/3589335.3651524)|Ziming Zhao, Zhaoxuan Li, Tingting Li, Jiongchi Yu, Fan Zhang, Rui Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Model+Robustness+against+Adversarial+Examples+with+Redundant+Fully+Connected+Layer)|0|
|[Thought Graph: Generating Thought Process for Biological Reasoning](https://doi.org/10.1145/3589335.3651572)|ChiYang Hsu, Kyle Cox, Jiawei Xu, Zhen Tan, Tianhua Zhai, Mengzhou Hu, Dexter Pratt, Tianlong Chen, Ziniu Hu, Ying Ding||We present the Thought Graph as a novel framework to support complex reasoning and use gene set analysis as an example to uncover semantic relationships between biological processes. Our framework stands out for its ability to provide a deeper understanding of gene sets, significantly surpassing GSEA by 40.28 to human annotations. Our analysis further provides insights into future directions of biological processes naming, and implications for bioinformatics and precision medicine.|我们提出思想图作为一个新的框架，以支持复杂的推理和使用基因集分析作为一个例子，揭示语义关系之间的生物过程。我们的框架能够提供对基因组更深入的理解，显著超过 GSEA 的40.28个人类注释。我们的分析进一步提供了对未来生物过程命名方向的洞察，以及对生物信息学和精准医学的影响。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Thought+Graph:+Generating+Thought+Process+for+Biological+Reasoning)|0|
|[On the Scale-Free Property of Citation Networks: An Empirical Study](https://doi.org/10.1145/3589335.3651541)|Xiaoshi Zhong, Huizhi Liang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Scale-Free+Property+of+Citation+Networks:+An+Empirical+Study)|0|
|[Automatic Construction of Expiration Time Expression Dataset from Retweets](https://doi.org/10.1145/3589335.3651471)|Hirotaka Nagashima, Keishi Tajima||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Construction+of+Expiration+Time+Expression+Dataset+from+Retweets)|0|
|[Finding Dense and Persistently Expansive Subgraphs](https://doi.org/10.1145/3589335.3651507)|Petros Petsinis, Charalampos E. Tsourakakis, Panagiotis Karras||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Finding+Dense+and+Persistently+Expansive+Subgraphs)|0|
|[Hyperbolic Heterogeneous Graph Attention Networks](https://doi.org/10.1145/3589335.3651522)|Jongmin Park, Seunghoon Han, Soohwan Jeong, Sungsu Lim||Most previous heterogeneous graph embedding models represent elements in a heterogeneous graph as vector representations in a low-dimensional Euclidean space. However, because heterogeneous graphs inherently possess complex structures, such as hierarchical or power-law structures, distortions can occur when representing them in Euclidean space. To overcome this limitation, we propose Hyperbolic Heterogeneous Graph Attention Networks (HHGAT) that learn vector representations in hyperbolic spaces with meta-path instances. We conducted experiments on three real-world heterogeneous graph datasets, demonstrating that HHGAT outperforms state-of-the-art heterogeneous graph embedding models in node classification and clustering tasks.|以往的异构图嵌入模型大多将异构图中的元素表示为低维欧氏空间中的向量表示。然而，由于异质图本身具有复杂的结构，例如层次结构或幂律结构，当它们在欧氏空间中表示时会发生畸变。为了克服这一局限性，我们提出了双曲异构图注意网络(HHGAT) ，它通过元路径实例学习双曲空间中的向量表示。我们在三个真实世界的异构图数据集上进行了实验，结果表明 HHGAT 在节点分类和聚类任务方面优于现有的异构图嵌入模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hyperbolic+Heterogeneous+Graph+Attention+Networks)|0|
|[Task-Driven Quantum Device Fingerprint Identification via Modeling QNN Outcome Shift Induced by Quantum Noise](https://doi.org/10.1145/3589335.3651567)|Tingting Li, Ziming Zhao, Jianwei Yin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Task-Driven+Quantum+Device+Fingerprint+Identification+via+Modeling+QNN+Outcome+Shift+Induced+by+Quantum+Noise)|0|
|[News-Driven Price Movement Forecasting with Label-Prior Graph Attention](https://doi.org/10.1145/3589335.3651539)|YiTing Liu, ChungChi Chen, HenHsen Huang, HsinHsi Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=News-Driven+Price+Movement+Forecasting+with+Label-Prior+Graph+Attention)|0|
|[Enabling Patient-side Disease Prediction via the Integration of Patient Narratives](https://doi.org/10.1145/3589335.3651498)|Zhixiang Su, Yinan Zhang, Jiazheng Jing, Jie Xiao, Zhiqi Shen||Disease prediction holds considerable significance in modern healthcare, because of its crucial role in facilitating early intervention and implementing effective prevention measures. However, most recent disease prediction approaches heavily rely on laboratory test outcomes (e.g., blood tests and medical imaging from X-rays). Gaining access to such data for precise disease prediction is often a complex task from the standpoint of a patient and is always only available post-patient consultation. To make disease prediction available from patient-side, we propose Personalized Medical Disease Prediction (PoMP), which predicts diseases using patient health narratives including textual descriptions and demographic information. By applying PoMP, patients can gain a clearer comprehension of their conditions, empowering them to directly seek appropriate medical specialists and thereby reducing the time spent navigating healthcare communication to locate suitable doctors. We conducted extensive experiments using real-world data from Haodf to showcase the effectiveness of PoMP.|疾病预测在现代医疗保健中具有重要意义，因为它在促进早期干预和实施有效预防措施方面具有关键作用。然而，大多数最新的疾病预测方法严重依赖于实验室检测结果(例如，血液检测和 X 射线医学成像)。从患者的角度来看，获取这些数据以进行精确的疾病预测往往是一项复杂的任务，而且往往只有患者术后才能进行咨询。为了使疾病预测从患者方面可用，我们提出了个性化医疗疾病预测(PoMP) ，它使用患者健康叙述，包括文本描述和人口统计信息来预测疾病。通过应用 PoMP，患者可以更清楚地了解自己的病情，使他们能够直接寻求适当的医疗专家，从而减少花在导航卫生保健通信以找到合适的医生上的时间。我们利用来自 Haodf 的实际数据进行了广泛的实验，以展示 PoMP 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enabling+Patient-side+Disease+Prediction+via+the+Integration+of+Patient+Narratives)|0|
|[LinkGuard: Link Locally Privacy-Preserving Graph Neural Networks with Integrated Denoising and Private Learning](https://doi.org/10.1145/3589335.3651533)|Yuxin Qi, Xi Lin, Ziyao Liu, Gaolei Li, Jingyu Wang, Jianhua Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LinkGuard:+Link+Locally+Privacy-Preserving+Graph+Neural+Networks+with+Integrated+Denoising+and+Private+Learning)|0|
|[Generator-Guided Crowd Reaction Assessment](https://doi.org/10.1145/3589335.3651512)|Sohom Ghosh, ChungChi Chen, Sudip Kumar Naskar||In the realm of social media, understanding and predicting post reach is a significant challenge. This paper presents a Crowd Reaction AssessMent (CReAM) task designed to estimate if a given social media post will receive more reaction than another, a particularly essential task for digital marketers and content writers. We introduce the Crowd Reaction Estimation Dataset (CRED), consisting of pairs of tweets from The White House with comparative measures of retweet count. The proposed Generator-Guided Estimation Approach (GGEA) leverages generative Large Language Models (LLMs), such as ChatGPT, FLAN-UL2, and Claude, to guide classification models for making better predictions. Our results reveal that a fine-tuned FLANG-RoBERTa model, utilizing a cross-encoder architecture with tweet content and responses generated by Claude, performs optimally. We further use a T5-based paraphraser to generate paraphrases of a given post and demonstrate GGEA's ability to predict which post will elicit the most reactions. We believe this novel application of LLMs provides a significant advancement in predicting social media post reach.|在社会化媒体领域，理解和预测发帖范围是一个重大的挑战。本文提出了一个群体反应评估(CReAM)任务，旨在估计是否一个给定的社会媒体帖子将获得更多的反应比另一个，一个特别重要的任务数字营销人员和内容作家。我们介绍了群体反应估计数据集(CRED) ，包括来自白宫的一对推文，以及对推文数量的比较测量。提出的生成器引导的评估方法(GGEA)利用生成的大语言模型(LLM) ，如 ChatGPT、 FLAN-UL2和 Claude，来指导分类模型，以便做出更好的预测。我们的研究结果表明，一个微调的 FLANG-RoBERTa 模型，利用一个交叉编码器架构与 tweet 内容和响应生成的克劳德，执行最佳。我们进一步使用一个基于 T5的解释器来生成给定文章的解释，并证明 GGEA 预测哪篇文章将引起最多反应的能力。我们相信 LLM 的这个新颖的应用为预测社会媒体的发布范围提供了一个重要的进步。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generator-Guided+Crowd+Reaction+Assessment)|0|
|[Hierarchical Tensor Clustering for Multiple Graphs Representation](https://doi.org/10.1145/3589335.3651519)|Karima Boutalbi, Rafika Boutalbi, Hervé Verjus, Kavé Salamatian||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Tensor+Clustering+for+Multiple+Graphs+Representation)|0|
|[Smooth Anonymity for Sparse Graphs](https://doi.org/10.1145/3589335.3651561)|Alessandro Epasto, Hossein Esfandiari, Vahab Mirrokni, Andrés Muñoz Medina||When working with user data providing well-defined privacy guarantees is paramount. In this work, we aim to manipulate and share an entire sparse dataset with a third party privately. In fact, differential privacy has emerged as the gold standard of privacy, however, when it comes to sharing sparse datasets, e.g. sparse networks, as one of our main results, we prove that any differentially private mechanism that maintains a reasonable similarity with the initial dataset is doomed to have a very weak privacy guarantee. In such situations, we need to look into other privacy notions such as k-anonymity. In this work, we consider a variation of k-anonymity, which we call smooth-k-anonymity, and design simple large-scale algorithms that efficiently provide smooth-k-anonymity. We further perform an empirical evaluation to back our theoretical guarantees and show that our algorithm improves the performance in downstream machine learning tasks on anonymized data.|在处理提供良好定义的隐私保障的用户数据时，最重要的是。在这项工作中，我们的目标是操作和共享一个完整的稀疏数据集与第三方私人。事实上，差分隐私已经成为隐私的黄金标准，然而，当涉及到共享稀疏数据集，例如稀疏网络，作为我们的主要结果之一，我们证明，任何差异私有机制，保持与初始数据集的合理相似性，注定有一个非常薄弱的隐私保障。在这种情况下，我们需要研究其他隐私概念，如 k 匿名。在本文中，我们考虑了一种称为光滑 k 匿名的变形，并且设计了简单的大规模算法来有效地提供光滑 k 匿名。我们进一步进行了实证评估，以支持我们的理论保证，并表明我们的算法提高性能在下游机器学习任务的匿名数据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Smooth+Anonymity+for+Sparse+Graphs)|0|
|[Predicting Node Influence in Complex Networks by the K-Shell Entropy and Degree Centrality](https://doi.org/10.1145/3589335.3651547)|Shima Esfandiari, Mostafa Fakhrahmad||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Predicting+Node+Influence+in+Complex+Networks+by+the+K-Shell+Entropy+and+Degree+Centrality)|0|
|[Knowledge Induced Transformer Network for Causality Prediction](https://doi.org/10.1145/3589335.3651531)|Tirthankar Dasgupta, Manjira Sinha, Abir Naskar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Induced+Transformer+Network+for+Causality+Prediction)|0|
|[MetroGNN: Metro Network Expansion with Reinforcement Learning](https://doi.org/10.1145/3589335.3651536)|Hongyuan Su, Yu Zheng, Jingtao Ding, Depeng Jin, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MetroGNN:+Metro+Network+Expansion+with+Reinforcement+Learning)|0|
|[Breaking the Bot Barrier: Evaluating Adversarial AI Techniques Against Multi-Modal Defense Models](https://doi.org/10.1145/3589335.3651474)|Behzad Ousat, Dongsheng Luo, Amin Kharraz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Breaking+the+Bot+Barrier:+Evaluating+Adversarial+AI+Techniques+Against+Multi-Modal+Defense+Models)|0|
|[RealGraphGPU++: A High-Performance GPU-Based Graph Engine with Direct Storage-to-DM IO](https://doi.org/10.1145/3589335.3651549)|JeongMin Park, MyungHwan Jang, DuckHo Bae, SangWook Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RealGraphGPU++:+A+High-Performance+GPU-Based+Graph+Engine+with+Direct+Storage-to-DM+IO)|0|
|[A Study of Vulnerability Repair in JavaScript Programs with Large Language Models](https://doi.org/10.1145/3589335.3651463)|Tan Khang Le, Saba Alimadadi, Steven Y. Ko||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Study+of+Vulnerability+Repair+in+JavaScript+Programs+with+Large+Language+Models)|0|
|[Deanonymizing Transactions Originating from Monero Tor Hidden Service Nodes](https://doi.org/10.1145/3589335.3651487)|Ruisheng Shi, Yulian Ge, Lina Lan, Zhiyuan Peng, Shenwen Lin, Lin Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deanonymizing+Transactions+Originating+from+Monero+Tor+Hidden+Service+Nodes)|0|
|[WebGraph: The Next Generation (Is in Rust)](https://doi.org/10.1145/3589335.3651581)|Tommaso Fontana, Sebastiano Vigna, Stefano Zacchiroli||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=WebGraph:+The+Next+Generation+(Is+in+Rust))|0|
|[GradFilt: Class-wise Targeted Data Reconstruction from Gradients in Federated Learning](https://doi.org/10.1145/3589335.3651514)|Rui Zhang, Song Guo, Ping Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GradFilt:+Class-wise+Targeted+Data+Reconstruction+from+Gradients+in+Federated+Learning)|0|
|[Advancing Stance Detection of Political Fan Pages: A Multimodal Approach](https://doi.org/10.1145/3589335.3651467)|KuanHung Kuo, MingHung Wang, HungYu Kao, YuChen Dai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Advancing+Stance+Detection+of+Political+Fan+Pages:+A+Multimodal+Approach)|0|
|[Structural Podcast Content Modeling with Generalizability](https://doi.org/10.1145/3589335.3651563)|Yijun Tian, Maryam Aziz, Alice Wang, Enrico Palumbo, Hugues Bouchard||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Structural+Podcast+Content+Modeling+with+Generalizability)|0|
|[Detecting Poisoning Attacks on Federated Learning Using Gradient-Weighted Class Activation Mapping](https://doi.org/10.1145/3589335.3651490)|Jingjing Zheng, Kai Li, Xin Yuan, Wei Ni, Eduardo Tovar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Poisoning+Attacks+on+Federated+Learning+Using+Gradient-Weighted+Class+Activation+Mapping)|0|
|[Fighting against Fake News on Newly-Emerging Crisis: A Case Study of COVID-19](https://doi.org/10.1145/3589335.3651506)|Migyeong Yang, Chaewon Park, Jiwon Kang, Daeun Lee, Daejin Choi, Jinyoung Han||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fighting+against+Fake+News+on+Newly-Emerging+Crisis:+A+Case+Study+of+COVID-19)|0|
|[Targeted Filter Bubbles Mitigating via Edges Insertion](https://doi.org/10.1145/3589335.3651566)|Yanping Wu, Jinghao Wang, Renjie Sun, Chen Chen, Xiaoyang Wang, Ying Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Targeted+Filter+Bubbles+Mitigating+via+Edges+Insertion)|0|
|[Unveiling Wash Trading in Popular NFT Markets](https://doi.org/10.1145/3589335.3651580)|Yuanzheng Niu, Xiaoqi Li, Hongli Peng, Wenkai Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+Wash+Trading+in+Popular+NFT+Markets)|0|
|[Understanding Deployment Experience of 5G](https://doi.org/10.1145/3589335.3651577)|Ziyi Liu, Huandong Wang, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+Deployment+Experience+of+5G)|0|
|[FaST: Accelerating Web Front-end Data Binding with Compiler and Visible Anchor](https://doi.org/10.1145/3589335.3651505)|Tatsuru Tomizawa, Seiki Makino, Taiga Kume, Satoki Hamanaka, Tadashi Okoshi, Jin Nakazawa||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FaST:+Accelerating+Web+Front-end+Data+Binding+with+Compiler+and+Visible+Anchor)|0|
|[How We Refute Claims: Automatic Fact-Checking through Flaw Identification and Explanation](https://doi.org/10.1145/3589335.3651521)|WeiYu Kao, AnZi Yen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+We+Refute+Claims:+Automatic+Fact-Checking+through+Flaw+Identification+and+Explanation)|0|
|[Characterizing the Solana NFT Ecosystem](https://doi.org/10.1145/3589335.3651478)|Dechao Kong, Xiaoqi Li, Wenkai Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Characterizing+the+Solana+NFT+Ecosystem)|0|
|[Multi-round Counterfactual Generation: Interpreting and Improving Models of Text Classification](https://doi.org/10.1145/3589335.3651537)|Huajie Zhang, Yuxin Ying, Fuzhen Zhuang, Haiqin Weng, Sun Ying, Zhao Zhang, Yiqi Tong, Yan Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-round+Counterfactual+Generation:+Interpreting+and+Improving+Models+of+Text+Classification)|0|
|[iSpLib: A Library for Accelerating Graph Neural Networks using Auto-tuned Sparse Operations](https://doi.org/10.1145/3589335.3651528)|Md Saidul Hoque Anik, Pranav Badhe, Rohit Gampa, Ariful Azad||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=iSpLib:+A+Library+for+Accelerating+Graph+Neural+Networks+using+Auto-tuned+Sparse+Operations)|0|
|[DeFiTail: DeFi Protocol Inspection through Cross-Contract Execution Analysis](https://doi.org/10.1145/3589335.3651488)|Wenkai Li, Xiaoqi Li, Yuqing Zhang, Zongwei Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DeFiTail:+DeFi+Protocol+Inspection+through+Cross-Contract+Execution+Analysis)|0|
|[Are we Making Much Progress? Revisiting Chemical Reaction Yield Prediction from an Imbalanced Regression Perspective](https://doi.org/10.1145/3589335.3651470)|Yihong Ma, Xiaobao Huang, Bozhao Nan, Nuno Moniz, Xiangliang Zhang, Olaf Wiest, Nitesh V. Chawla||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Are+we+Making+Much+Progress?+Revisiting+Chemical+Reaction+Yield+Prediction+from+an+Imbalanced+Regression+Perspective)|0|
|[Simple Multigraph Convolution Networks](https://doi.org/10.1145/3589335.3651560)|Danyang Wu, Xinjie Shen, Jitao Lu, Jin Xu, Feiping Nie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Simple+Multigraph+Convolution+Networks)|0|
|[Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks](https://doi.org/10.1145/3589335.3651555)|Yichang Xu, Ming Yin, Minghong Fang, Neil Zhenqiang Gong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Federated+Learning+Mitigates+Client-side+Training+Data+Distribution+Inference+Attacks)|0|
|[Group-wise K-anonymity meets (ε, δ) Differentially Privacy Scheme](https://doi.org/10.1145/3589335.3651517)|Kenneth Odoh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Group-wise+K-anonymity+meets+(ε,+δ)+Differentially+Privacy+Scheme)|0|
|[Generating Privacy-preserving Educational Data Records with Diffusion Model](https://doi.org/10.1145/3589335.3651511)|Quanlong Guan, Yanchong Yu, Xiujie Huang, Liangda Fang, Chaobo He, Lusheng Wu, Weiqi Luo, Guanliang Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generating+Privacy-preserving+Educational+Data+Records+with+Diffusion+Model)|0|
|[StateGuard: Detecting State Derailment Defects in Decentralized Exchange Smart Contract](https://doi.org/10.1145/3589335.3651562)|Zongwei Li, Wenkai Li, Xiaoqi Li, Yuqing Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=StateGuard:+Detecting+State+Derailment+Defects+in+Decentralized+Exchange+Smart+Contract)|0|
|[Rumor Mitigation in Social Media Platforms with Deep Reinforcement Learning](https://doi.org/10.1145/3589335.3651556)|Hongyuan Su, Yu Zheng, Jingtao Ding, Depeng Jin, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rumor+Mitigation+in+Social+Media+Platforms+with+Deep+Reinforcement+Learning)|0|
|[SWATTING Spambots: Real-time Detection of Malicious Bots on X](https://doi.org/10.1145/3589335.3651564)|Cristian Brokate, Manon Richard, Lisa Giordani, Jean Liénard||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SWATTING+Spambots:+Real-time+Detection+of+Malicious+Bots+on+X)|0|
|[One-shot Pairing and Authentication Using Moms Secret](https://doi.org/10.1145/3589335.3651542)|Ubaid Ur Rehman, Sungyoung Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=One-shot+Pairing+and+Authentication+Using+Moms+Secret)|0|
|[GPT-generated Text Detection: Benchmark Dataset and Tensor-based Detection Method](https://doi.org/10.1145/3589335.3651513)|Zubair Qazi, William Shiao, Evangelos E. Papalexakis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GPT-generated+Text+Detection:+Benchmark+Dataset+and+Tensor-based+Detection+Method)|0|
|[Knowledge Guided Conditional Diffusion Model for Controllable Mobile Traffic Generation](https://doi.org/10.1145/3589335.3651530)|Haoye Chai, Tong Li, Fenyu Jiang, Shiyuan Zhang, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Guided+Conditional+Diffusion+Model+for+Controllable+Mobile+Traffic+Generation)|0|
|[Critical Nodes Detection: Node Merging Approach](https://doi.org/10.1145/3589335.3651485)|Hongbo Qiu, Renjie Sun, Chen Chen, Xiaoyang Wang, Ying Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Critical+Nodes+Detection:+Node+Merging+Approach)|0|
|[Dual-level Hypergraph Contrastive Learning with Adaptive Temperature Enhancement](https://doi.org/10.1145/3589335.3651493)|Yiyue Qian, Tianyi Ma, Chuxu Zhang, Yanfang Ye||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual-level+Hypergraph+Contrastive+Learning+with+Adaptive+Temperature+Enhancement)|0|
|[Towards Understanding Crypto-Asset Risks on Ethereum Caused by Key Leakage on the Internet](https://doi.org/10.1145/3589335.3651573)|Yuxuan Zhou, Jiaqi Chen, Yibo Wang, Yuzhe Tang, Guofei Gu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Understanding+Crypto-Asset+Risks+on+Ethereum+Caused+by+Key+Leakage+on+the+Internet)|0|
|[Turning A Curse into A Blessing: Data-Aware Memory-Efficient Training of Graph Neural Networks by Dynamic Exiting](https://doi.org/10.1145/3589335.3651575)|Yan Han, Kaiqi Chen, Shan Li, Ji Yan, Baoxu Shi, Lei Zhang, Fei Chen, Jaewon Yang, Yunpeng Xu, Xiaoqiang Luo, Qi He, Ying Ding, Zhangyang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Turning+A+Curse+into+A+Blessing:+Data-Aware+Memory-Efficient+Training+of+Graph+Neural+Networks+by+Dynamic+Exiting)|0|
|[A Heterogeneous Network fused with Context-aware Contrastive Learning for Sarcasm Topic-Target Pair Identification](https://doi.org/10.1145/3589335.3651462)|Minjie Yuan, Mengyu Xiang, Yuxuan Song, Qiudan Li, Jinye Fu, Daniel Dajun Zeng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Heterogeneous+Network+fused+with+Context-aware+Contrastive+Learning+for+Sarcasm+Topic-Target+Pair+Identification)|0|
|[Disentangled Anomaly Detection For Multivariate Time Series](https://doi.org/10.1145/3589335.3651492)|Xin Jie, Xixi Zhou, Chanfei Su, Zijun Zhou, Yuqing Yuan, Jiajun Bu, Haishuai Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disentangled+Anomaly+Detection+For+Multivariate+Time+Series)|0|
|[Content Moderation on Social Media in the EU: Insights From the DSA Transparency Database](https://doi.org/10.1145/3589335.3651482)|Chiara Patricia Drolsbach, Nicolas Pröllochs||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Content+Moderation+on+Social+Media+in+the+EU:+Insights+From+the+DSA+Transparency+Database)|0|
|[Object-level Copy-Move Forgery Image Detection based on Inconsistency Mining](https://doi.org/10.1145/3589335.3651540)|Jingyu Wang, Niantai Jing, Ziyao Liu, Jie Nie, Yuxin Qi, ChiHung Chi, KwokYan Lam||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Object-level+Copy-Move+Forgery+Image+Detection+based+on+Inconsistency+Mining)|0|
|[Efficacy of Large Language Models in Predicting Hindi Movies' Attributes: A Comprehensive Survey and Content-Based Analysis](https://doi.org/10.1145/3589335.3651496)|Prabir Mondal, Siddharth Singh, Kushum, Sriparna Saha, Jyoti Prakash Singh, Brijraj Singh, Niranjan Pedanekar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficacy+of+Large+Language+Models+in+Predicting+Hindi+Movies'+Attributes:+A+Comprehensive+Survey+and+Content-Based+Analysis)|0|
|[Interpretation-Empowered Neural Cleanse for Backdoor Attacks](https://doi.org/10.1145/3589335.3651525)|Liangbo Ning, Zeyu Dai, Jingran Su, Chao Pan, Luning Wang, Wenqi Fan, Qing Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interpretation-Empowered+Neural+Cleanse+for+Backdoor+Attacks)|0|
|[Who is Creating Malware Repositories on GitHub and Why?](https://doi.org/10.1145/3589335.3651582)|Nishat Ara Tania, Md Rayhanul Masud, Md Omar Faruk Rokon, Qian Zhang, Michalis Faloutsos||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Who+is+Creating+Malware+Repositories+on+GitHub+and+Why?)|0|
|[Zero-shot Explainable Mental Health Analysis on Social Media by Incorporating Mental Scales](https://doi.org/10.1145/3589335.3651584)|Wenyu Li, Yinuo Zhu, Xin Lin, Ming Li, Ziyue Jiang, Ziqian Zeng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Zero-shot+Explainable+Mental+Health+Analysis+on+Social+Media+by+Incorporating+Mental+Scales)|0|
|[MART: Learning Hierarchical Music Audio Representations with Part-Whole Transformer](https://doi.org/10.1145/3589335.3651535)|Dong Yao, Jieming Zhu, Jiahao Xun, Shengyu Zhang, Zhou Zhao, Liqun Deng, Wenqiao Zhang, Zhenhua Dong, Xin Jiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MART:+Learning+Hierarchical+Music+Audio+Representations+with+Part-Whole+Transformer)|0|
|[Exploiting Associations among Multi-Aspect Node Properties in Heterogeneous Graphs for Link Prediction](https://doi.org/10.1145/3589335.3651502)|Chenguang Du, Hao Geng, Deqing Wang, Fuzhen Zhuang, Zhiqiang Zhang, Lanshan Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploiting+Associations+among+Multi-Aspect+Node+Properties+in+Heterogeneous+Graphs+for+Link+Prediction)|0|
|[Automating the Information Extraction from Semi-Structured Interview Transcripts](https://doi.org/10.1145/3589335.3651230)|Angelina Parfenova Lucerne||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automating+the+Information+Extraction+from+Semi-Structured+Interview+Transcripts)|0|
|[FashionReGen: LLM-Empowered Fashion Report Generation](https://doi.org/10.1145/3589335.3651232)|Yujuan Ding, Yunshan Ma, Wenqi Fan, Yige Yao, TatSeng Chua, Qing Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FashionReGen:+LLM-Empowered+Fashion+Report+Generation)|0|
|[Tender Document Analyzer with the Combination of Supervised Learning and LLM-based Improver](https://doi.org/10.1145/3589335.3651233)|Tomoki Ito, Shun Nakagawa||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tender+Document+Analyzer+with+the+Combination+of+Supervised+Learning+and+LLM-based+Improver)|0|
|[GRACE: Generating Cause and Effect of Disaster Sub-Events from Social Media Text](https://doi.org/10.1145/3589335.3651234)|Xinxi Jiang, Xiang Li, Qifeng Zhou, Qing Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GRACE:+Generating+Cause+and+Effect+of+Disaster+Sub-Events+from+Social+Media+Text)|0|
|[RealGraphGPUWeb: A Convenient and Efficient GPU-Based Graph Analysis Platform on the Web](https://doi.org/10.1145/3589335.3651237)|JeongMin Park, MyungHwan Jang, SangWook Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RealGraphGPUWeb:+A+Convenient+and+Efficient+GPU-Based+Graph+Analysis+Platform+on+the+Web)|0|
|[BoostER: Leveraging Large Language Models for Enhancing Entity Resolution](https://doi.org/10.1145/3589335.3651245)|Huahang Li, Shuangyin Li, Fei Hao, Chen Jason Zhang, Yuanfeng Song, Lei Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BoostER:+Leveraging+Large+Language+Models+for+Enhancing+Entity+Resolution)|0|
|[Scenario-Driven Cyber-Physical-Social System: Intelligent Workflow Generation Based on Capability](https://doi.org/10.1145/3589335.3651246)|Yi Li, Xinkui Zhao, Chen Chen, Shengye Pang, Zhengyang Zhou, Jianwei Yin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scenario-Driven+Cyber-Physical-Social+System:+Intelligent+Workflow+Generation+Based+on+Capability)|0|
|[REAL-UP: Urban Perceptions From LBSNs Helping Moving Real-Estate Market to the Next Level](https://doi.org/10.1145/3589335.3651252)|Frances Albert Santos, Thiago H. Silva, Leandro A. Villas||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=REAL-UP:+Urban+Perceptions+From+LBSNs+Helping+Moving+Real-Estate+Market+to+the+Next+Level)|0|
|[The Web Data Commons Schema.org Table Corpora](https://doi.org/10.1145/3589335.3651441)|Ralph Peeters, Alexander Brinkmann, Christian Bizer||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Web+Data+Commons+Schema.org+Table+Corpora)|0|
|[Tel2Veh: Fusion of Telecom Data and Vehicle Flow to Predict Camera-Free Traffic via a Spatio-Temporal Framework](https://doi.org/10.1145/3589335.3651442)|ChungYi Lin, ShenLung Tung, HungTing Su, Winston H. Hsu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tel2Veh:+Fusion+of+Telecom+Data+and+Vehicle+Flow+to+Predict+Camera-Free+Traffic+via+a+Spatio-Temporal+Framework)|0|
|[An Open Platform for Quality Measures in a Linked Data Index](https://doi.org/10.1145/3589335.3651443)|Pierre Maillot, Jennie Andersen, Sylvie Cazalens, Catherine Faron, Fabien Gandon, Philippe Lamarre, Franck Michel||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Open+Platform+for+Quality+Measures+in+a+Linked+Data+Index)|0|
|[CompMix: A Benchmark for Heterogeneous Question Answering](https://doi.org/10.1145/3589335.3651444)|Philipp Christmann, Rishiraj Saha Roy, Gerhard Weikum||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CompMix:+A+Benchmark+for+Heterogeneous+Question+Answering)|0|
|[Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery](https://doi.org/10.1145/3589335.3651446)|Yuxuan Yao, Sichun Luo, Haohan Zhao, Guanzhi Deng, Linqi Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+LLM+Substitute+Human+Labeling?+A+Case+Study+of+Fine-grained+Chinese+Address+Entity+Recognition+Dataset+for+UAV+Delivery)|0|
|[Graphameleon: Relational Learning and Anomaly Detection on Web Navigation Traces Captured as Knowledge Graphs](https://doi.org/10.1145/3589335.3651447)|Lionel Tailhardat, Benjamin Stach, Yoan Chabot, Raphaël Troncy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graphameleon:+Relational+Learning+and+Anomaly+Detection+on+Web+Navigation+Traces+Captured+as+Knowledge+Graphs)|0|
|[Revisiting 30 years of the Network Time Protocol](https://doi.org/10.1145/3589335.3651998)|Simon S. Woo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+30+years+of+the+Network+Time+Protocol)|0|
|[Toward Making Opaque Web Content More Accessible: Accessibility From Adobe Flash to Canvas-Rendered Apps](https://doi.org/10.1145/3589335.3651999)|Thomas Steiner||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Toward+Making+Opaque+Web+Content+More+Accessible:+Accessibility+From+Adobe+Flash+to+Canvas-Rendered+Apps)|0|
|[History in Making: Political Campaigns in the Era of Artificial Intelligence-Generated Content](https://doi.org/10.1145/3589335.3652000)|Ehsan ul Haq, Yiming Zhu, Pan Hui, Gareth Tyson||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=History+in+Making:+Political+Campaigns+in+the+Era+of+Artificial+Intelligence-Generated+Content)|0|
|[Me, the Web and Digital Accessibility](https://doi.org/10.1145/3589335.3652002)|Reinaldo Ferraz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Me,+the+Web+and+Digital+Accessibility)|0|
|[A Non-Intrusive Approach to Assessing Dysarthria Severity: Advancing Clinical Diagnosis](https://doi.org/10.1145/3589335.3651449)|Ganjun Liu, Xiaohui Hou, Meng Ge, Tao Zhang, Haizhou Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Non-Intrusive+Approach+to+Assessing+Dysarthria+Severity:+Advancing+Clinical+Diagnosis)|0|
|[AI-based Prediction of Catheter-related Thrombosis Risk for Cancer Patients](https://doi.org/10.1145/3589335.3651452)|Yaoqi Guo, Yun Ma, Zijian Shao, Weichen Bi, Yanfeng Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI-based+Prediction+of+Catheter-related+Thrombosis+Risk+for+Cancer+Patients)|0|
|[Health CLIP: Depression Rate Prediction Using Health Related Features in Satellite and Street View Images](https://doi.org/10.1145/3589335.3651451)|Tianjian Ouyang, Xin Zhang, Zhenyu Han, Yu Shang, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Health+CLIP:+Depression+Rate+Prediction+Using+Health+Related+Features+in+Satellite+and+Street+View+Images)|0|
|[AI in Health and Social Care: A Methodology for Privacy Risk Modeling and Simulation](https://doi.org/10.1145/3589335.3651453)|Laura Carmichael, Steve Taylor, Adriane Chapman, Michael J. Boniface||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI+in+Health+and+Social+Care:+A+Methodology+for+Privacy+Risk+Modeling+and+Simulation)|0|
|[Sociotechnical Considerations for Accessibility and Equity in AI for Healthcare](https://doi.org/10.1145/3589335.3651455)|Adriane Chapman, Chloe L. Harrison, Caroline Jones, James Thornton, Rose Worley, Jeremy C. Wyatt||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sociotechnical+Considerations+for+Accessibility+and+Equity+in+AI+for+Healthcare)|0|
|[Uncertainty-Aware Pre-Trained Foundation Models for Patient Risk Prediction via Gaussian Process](https://doi.org/10.1145/3589335.3651456)|Jiaying Lu, Shifan Zhao, Wenjing Ma, Hui Shao, Xiao Hu, Yuanzhe Xi, Carl Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncertainty-Aware+Pre-Trained+Foundation+Models+for+Patient+Risk+Prediction+via+Gaussian+Process)|0|
|[Enhancing Progressive Diagnosis Prediction in Healthcare with Continuous Normalizing Flows](https://doi.org/10.1145/3589335.3651457)|Yanchao Tan, Hengyu Zhang, Zihao Zhou, Guofang Ma, Fan Wang, Weiming Liu, Xinting Liao, Vicki Stover Hertzberg, Carl Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Progressive+Diagnosis+Prediction+in+Healthcare+with+Continuous+Normalizing+Flows)|0|
|[Improving Prostate Cancer Risk Prediction through Partial AUC Optimization](https://doi.org/10.1145/3589335.3651458)|Xinyuan Zhu, Xiaohan Ren, Wentao Shi, Changming Wang, Xuehan Liu, Yuqing Liu, Tao Tao, Fuli Feng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Prostate+Cancer+Risk+Prediction+through+Partial+AUC+Optimization)|0|
|[Distributed Transparent Data Layer for Next Generation Blockchains](https://doi.org/10.1145/3589335.3651255)|Li Quan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributed+Transparent+Data+Layer+for+Next+Generation+Blockchains)|0|
|[Temporal Knowledge Graph Extraction and Modeling across Multiple Documents for Health Risk Prediction](https://doi.org/10.1145/3589335.3651256)|Rochana Chaturvedi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Temporal+Knowledge+Graph+Extraction+and+Modeling+across+Multiple+Documents+for+Health+Risk+Prediction)|0|
|[When Crypto Economics Meet Graph Analytics and Learning](https://doi.org/10.1145/3589335.3651257)|Bingqiao Luo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+Crypto+Economics+Meet+Graph+Analytics+and+Learning)|0|
|[Comprehensively Auditing the TikTok Mobile App](https://doi.org/10.1145/3589335.3651260)|Levi Kaplan, Piotr Sapiezynski||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Comprehensively+Auditing+the+TikTok+Mobile+App)|0|
|[Outgroup Dehumanisation in Telegram - the Role of Ingroup Identity and Perception](https://doi.org/10.1145/3589335.3651261)|Elizaveta Chernenko||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Outgroup+Dehumanisation+in+Telegram+-+the+Role+of+Ingroup+Identity+and+Perception)|0|
|[Leveraging Knowledge-aware Methodologies for Multi-document Summarization](https://doi.org/10.1145/3589335.3651262)|Yutong Qu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+Knowledge-aware+Methodologies+for+Multi-document+Summarization)|0|
|[Knowledge Enabled Relation Extraction](https://doi.org/10.1145/3589335.3651263)|Monika Jain||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Enabled+Relation+Extraction)|0|
|[Graph Unlearning with Efficient Partial Retraining](https://doi.org/10.1145/3589335.3651265)|Jiahao Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Unlearning+with+Efficient+Partial+Retraining)|0|
|[Incentives in the Ether: Practical Cryptocurrency Economics & Security](https://doi.org/10.1145/3589335.3651268)|Aviv Yaish||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Incentives+in+the+Ether:+Practical+Cryptocurrency+Economics+&+Security)|0|
|[Social Psychology Meets Social Computing: State of the Art and Future Directions](https://doi.org/10.1145/3589335.3641242)|Sourav S. Bhowmick, Hui Li, S. H. Annabel Chen, Yining Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Social+Psychology+Meets+Social+Computing:+State+of+the+Art+and+Future+Directions)|0|
|[Discrete Choice and Applications](https://doi.org/10.1145/3589335.3641244)|Flavio Chierichetti, Ravi Kumar, Andrew Tomkins||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Discrete+Choice+and+Applications)|0|
|[Mining Temporal Networks](https://doi.org/10.1145/3589335.3641245)|Aristides Gionis, Lutz Oettershagen, Ilie Sarpe||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mining+Temporal+Networks)|0|
|[Lecture-style Tutorial: Towards Graph Foundation Models](https://doi.org/10.1145/3589335.3641246)|Chuan Shi, Cheng Yang, Yuan Fang, Lichao Sun, Philip S. Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lecture-style+Tutorial:+Towards+Graph+Foundation+Models)|0|
|[Understanding (Dark) Humour with Internet Meme Analysis](https://doi.org/10.1145/3589335.3641249)|Ming Shan Hee, Rui Cao, Tanmoy Chakraborty, Roy KaWei Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+(Dark)+Humour+with+Internet+Meme+Analysis)|0|
|[Privacy in Web Advertising: Analytics and Modeling](https://doi.org/10.1145/3589335.3641252)|Badih Ghazi, Ravi Kumar, Pasin Manurangsi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privacy+in+Web+Advertising:+Analytics+and+Modeling)|0|
|[Large Language Models for Graphs: Progresses and Directions](https://doi.org/10.1145/3589335.3641251)|Chao Huang, Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh V. Chawla||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+for+Graphs:+Progresses+and+Directions)|0|
|[New Frontiers of Knowledge Graph Reasoning: Recent Advances and Future Trends](https://doi.org/10.1145/3589335.3641254)|Lihui Liu, Zihao Wang, Jiaxin Bai, Yangqiu Song, Hanghang Tong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=New+Frontiers+of+Knowledge+Graph+Reasoning:+Recent+Advances+and+Future+Trends)|0|
|[Simulating Human Society with Large Language Model Agents: City, Social Media, and Economic System](https://doi.org/10.1145/3589335.3641253)|Chen Gao, Fengli Xu, Xu Chen, Xiang Wang, Xiangnan He, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Simulating+Human+Society+with+Large+Language+Model+Agents:+City,+Social+Media,+and+Economic+System)|0|
|[Text-Attributed Graph Representation Learning: Methods, Applications, and Challenges](https://doi.org/10.1145/3589335.3641255)|Delvin Ce Zhang, Menglin Yang, Rex Ying, Hady W. Lauw||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Text-Attributed+Graph+Representation+Learning:+Methods,+Applications,+and+Challenges)|0|
|[Toward Mitigating Misinformation and Social Media Manipulation in LLM Era](https://doi.org/10.1145/3589335.3641256)|Yizhou Zhang, Karishma Sharma, Lun Du, Yan Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Toward+Mitigating+Misinformation+and+Social+Media+Manipulation+in+LLM+Era)|0|
|[Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models](https://doi.org/10.1145/3589335.3641257)|Xin Wang, Yuwei Zhou, Hong Chen, Wenwu Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Curriculum+Learning:+Theories,+Approaches,+Applications,+Tools,+and+Future+Directions+in+the+Era+of+Large+Language+Models)|0|
|[English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts](https://doi.org/10.1145/3589335.3651902)|Patrick Bareiß, Roman Klinger, Jeremy Barnes||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=English+Prompts+are+Better+for+NLI-based+Zero-Shot+Emotion+Classification+than+Target-Language+Prompts)|0|
|[Towards Invariant Time Series Forecasting in Smart Cities](https://doi.org/10.1145/3589335.3651897)|Ziyi Zhang, Shaogang Ren, Xiaoning Qian, Nick Duffield||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Invariant+Time+Series+Forecasting+in+Smart+Cities)|0|
|[Open Metaverse: Issues, Evolution, and Future](https://doi.org/10.1145/3589335.3651898)|Zefeng Chen, Wensheng Gan, Jiayi Sun, Jiayang Wu, Philip S. Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Open+Metaverse:+Issues,+Evolution,+and+Future)|0|
|[Homogenizing Data Flows in Smart Cities: Value-driven use Cases in the Era of Citiverse](https://doi.org/10.1145/3589335.3651900)|Leonidas G. Anthopoulos, Ioannis Nikolaou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Homogenizing+Data+Flows+in+Smart+Cities:+Value-driven+use+Cases+in+the+Era+of+Citiverse)|0|
|[Spatio-Temporal Challenges in Understanding your (Smart) City](https://doi.org/10.1145/3589335.3652580)|Dirk Ahlers||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spatio-Temporal+Challenges+in+Understanding+your+(Smart)+City)|0|
|[A Longitudinal Study of Content Control Mechanisms](https://doi.org/10.1145/3589335.3651893)|Michael Dinzinger, Michael Granitzer||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Longitudinal+Study+of+Content+Control+Mechanisms)|0|
|[TIQ: A Benchmark for Temporal Question Answering with Implicit Time Constraints](https://doi.org/10.1145/3589335.3651895)|Zhen Jia, Philipp Christmann, Gerhard Weikum||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TIQ:+A+Benchmark+for+Temporal+Question+Answering+with+Implicit+Time+Constraints)|0|
|[Attentive Partial Convolution for RGBD Image Inpainting](https://doi.org/10.1145/3589335.3651906)|Ankan Dash, Guiling Wang, Tao Han||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Attentive+Partial+Convolution+for+RGBD+Image+Inpainting)|0|
|[GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text](https://doi.org/10.1145/3589335.3651909)|Kyle Hamilton, Luca Longo, Bojan Bozic||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GPT+Assisted+Annotation+of+Rhetorical+and+Linguistic+Features+for+Interpretable+Propaganda+Technique+Detection+in+News+Text)|0|
|[A Bayesian Framework for Measuring Association and Its Application to Emotional Dynamics in Web Discourse](https://doi.org/10.1145/3589335.3651911)|Henrique S. Xavier, Diogo Cortiz, Mateus Silvestrin, Ana Luísa Freitas, Letícia Yumi Nakao Morello, Fernanda Naomi Pantaleão, Gabriel Gaudencio do Rêgo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Bayesian+Framework+for+Measuring+Association+and+Its+Application+to+Emotional+Dynamics+in+Web+Discourse)|0|
|[Leveraging Large Language Models to Detect Influence Campaigns on Social Media](https://doi.org/10.1145/3589335.3651912)|Luca Luceri, Eric Boniardi, Emilio Ferrara||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+Large+Language+Models+to+Detect+Influence+Campaigns+on+Social+Media)|0|
|[Towards Fact-check Summarization Leveraging on Argumentation Elements Tied to Entity Graphs](https://doi.org/10.1145/3589335.3651914)|Katerina Haniková, David Chudán, Vojtech Svátek, Peter Vajdecka, Raphaël Troncy, Filip Vencovský, Jana Syrovátková||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Fact-check+Summarization+Leveraging+on+Argumentation+Elements+Tied+to+Entity+Graphs)|0|
|[DCAI: Data-centric Artificial Intelligence](https://doi.org/10.1145/3589335.3641297)|Wei Jin, Haohan Wang, Daochen Zha, Qiaoyu Tan, Yao Ma, Sharon Li, SuIn Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DCAI:+Data-centric+Artificial+Intelligence)|0|
|[Robust Data-centric Graph Structure Learning for Text Classification](https://doi.org/10.1145/3589335.3651915)|Jun Zhuang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Data-centric+Graph+Structure+Learning+for+Text+Classification)|0|
|[Data Quality-based Gradient Optimization for Recurrent Neural Networks](https://doi.org/10.1145/3589335.3651918)|Feihu Huang, Peiyu Yi, Shan Li, Haiwen Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data+Quality-based+Gradient+Optimization+for+Recurrent+Neural+Networks)|0|
|[CFinDEE: A Chinese Fine-Grained Financial Dataset for Document-Level Event Extraction](https://doi.org/10.1145/3589335.3651921)|Tian Zhang, Maofu Liu, Bingying Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CFinDEE:+A+Chinese+Fine-Grained+Financial+Dataset+for+Document-Level+Event+Extraction)|0|
|[FASETS: Discovering Faceted Sets of Entities](https://doi.org/10.1145/3589335.3651924)|Koninika Pal, Hiba Arnaout, Simon Razniewski, Gerhard Weikum||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FASETS:+Discovering+Faceted+Sets+of+Entities)|0|
|[Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept Understanding](https://doi.org/10.1145/3589335.3651927)|Zezhong Fan, Xiaohan Li, Kaushiki Nag, Chenhao Fang, Topojoy Biswas, Jianpeng Xu, Kannan Achan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prompt+Optimizer+of+Text-to-Image+Diffusion+Models+for+Abstract+Concept+Understanding)|0|
|[LLM-Guided Counterfactual Data Generation for Fairer AI](https://doi.org/10.1145/3589335.3651929)|Ashish Mishra, Gyanaranjan Nayak, Suparna Bhattacharya, Tarun Kumar, Arpit Shah, Martin Foltin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLM-Guided+Counterfactual+Data+Generation+for+Fairer+AI)|0|
|[Only Send What You Need: Learning to Communicate Efficiently in Federated Multilingual Machine Translation](https://doi.org/10.1145/3589335.3651931)|YunWei Chu, DongJun Han, Christopher G. Brinton||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Only+Send+What+You+Need:+Learning+to+Communicate+Efficiently+in+Federated+Multilingual+Machine+Translation)|0|
|[Federated Learning in Large Model Era: Vision-Language Model for Smart City Safety Operation Management](https://doi.org/10.1145/3589335.3651939)|Zengxiang Li, Zhaoxiang Hou, Hui Liu, Tongzhi Li, Chengyi Yang, Ying Wang, Chao Shi, Longfei Xie, Weishan Zhang, Liang Xu, Zelei Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Federated+Learning+in+Large+Model+Era:+Vision-Language+Model+for+Smart+City+Safety+Operation+Management)|0|
|[Phoenix: A Federated Generative Diffusion Model](https://doi.org/10.1145/3589335.3651935)|Fiona Victoria Stanley Jothiraj, Afra Mashhadi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Phoenix:+A+Federated+Generative+Diffusion+Model)|0|
|[LLM Driven Web Profile Extraction for Identical Names](https://doi.org/10.1145/3589335.3651946)|Prateek Sancheti, Kamalakar Karlapalem, Kavita Vemuri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLM+Driven+Web+Profile+Extraction+for+Identical+Names)|0|
|[Contrastive Disentanglement for Authorship Attribution](https://doi.org/10.1145/3589335.3652501)|Zhiqiang Hu, Thao Thanh Nguyen, Yujia Hu, ChiaYu Hung, Ming Shan Hee, ChunWei Seah, Roy KaWei Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Disentanglement+for+Authorship+Attribution)|0|
|[Large Language Models for Graph Learning](https://doi.org/10.1145/3589335.3641300)|Yujuan Ding, Wenqi Fan, Xiao Huang, Qing Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+for+Graph+Learning)|0|
|[Multi-Granularity Tibetan Textual Adversarial Attack Method Based on Masked Language Model](https://doi.org/10.1145/3589335.3652503)|Xi Cao, Nuo Qun, Quzong Gesang, Yulei Zhu, Trashi Nyima||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Granularity+Tibetan+Textual+Adversarial+Attack+Method+Based+on+Masked+Language+Model)|0|
|[Decoding Memes: A Comprehensive Analysis of Late and Early Fusion Models for Explainable Meme Analysis](https://doi.org/10.1145/3589335.3652504)|Faseela Abdullakutty, Usman Naseem||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decoding+Memes:+A+Comprehensive+Analysis+of+Late+and+Early+Fusion+Models+for+Explainable+Meme+Analysis)|0|
|[SigBart: Enhanced Pre-training via Salient Content Representation Learning for Social Media Summarization](https://doi.org/10.1145/3589335.3652505)|Sajad Sotudeh, Nazli Goharian||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SigBart:+Enhanced+Pre-training+via+Salient+Content+Representation+Learning+for+Social+Media+Summarization)|0|
|[An Investigation into the Feasibility of Performing Federated Learning on Social Linked Data Servers](https://doi.org/10.1145/3589335.3651950)|Nayil Arana, Mohamed Ragab, Thanassis Tiropanis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Investigation+into+the+Feasibility+of+Performing+Federated+Learning+on+Social+Linked+Data+Servers)|0|
|[Detecting Financial Bots on the Ethereum Blockchain](https://doi.org/10.1145/3589335.3651959)|Thomas Niedermayer, Pietro Saggese, Bernhard Haslhofer||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Financial+Bots+on+the+Ethereum+Blockchain)|0|
|[Measuring Arbitrage Losses and Profitability of AMM Liquidity](https://doi.org/10.1145/3589335.3651961)|Robin Fritsch, Andrea Canidio||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Measuring+Arbitrage+Losses+and+Profitability+of+AMM+Liquidity)|0|
|[Anonymity Analysis of the Umbra Stealth Address Scheme on Ethereum](https://doi.org/10.1145/3589335.3651963)|Alex Márk Kovács, István András Seres||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Anonymity+Analysis+of+the+Umbra+Stealth+Address+Scheme+on+Ethereum)|0|
|[Seamlessly Transferring Assets through Layer-0 Bridges: An Empirical Analysis of Stargate Bridge's Architecture and Dynamics](https://doi.org/10.1145/3589335.3651964)|Chuanshan Huang, Tao Yan, Claudio J. Tessone||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Seamlessly+Transferring+Assets+through+Layer-0+Bridges:+An+Empirical+Analysis+of+Stargate+Bridge's+Architecture+and+Dynamics)|0|
|[Understanding, Leveraging, and Improving Large Language Models](https://doi.org/10.1145/3589335.3653009)|Soujanya Poria||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding,+Leveraging,+and+Improving+Large+Language+Models)|0|
|[Love-Hate Dataset: A Multi-Modal Multi-Platform Dataset Depicting Emotions in the 2023 Israel-Hamas War](https://doi.org/10.1145/3589335.3651966)|Lynnette Hui Xian Ng, Adrian Xuan Wei Lim, Roy KaWei Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Love-Hate+Dataset:+A+Multi-Modal+Multi-Platform+Dataset+Depicting+Emotions+in+the+2023+Israel-Hamas+War)|0|
|[Textual Context guided Vision Transformer with Rotated Multi-Head Attention for Sentiment Analysis](https://doi.org/10.1145/3589335.3651968)|Chhavi Dhiman, Gaurav Kumar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Textual+Context+guided+Vision+Transformer+with+Rotated+Multi-Head+Attention+for+Sentiment+Analysis)|0|
|[A Novel Dual-Pipeline based Attention Mechanism for Multimodal Social Sentiment Analysis](https://doi.org/10.1145/3589335.3651967)|Ali Braytee, Andy ShuehChih Yang, Ali Anaissi, Kunal Chaturvedi, Mukesh Prasad||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Novel+Dual-Pipeline+based+Attention+Mechanism+for+Multimodal+Social+Sentiment+Analysis)|0|
|[Unraveling the Tangle of Disinformation: A Multimodal Approach for Fake News Identification on Social Media](https://doi.org/10.1145/3589335.3651972)|Junaid Rashid, Jungeun Kim, Anum Masood||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unraveling+the+Tangle+of+Disinformation:+A+Multimodal+Approach+for+Fake+News+Identification+on+Social+Media)|0|
|[RUHate-MM: Identification of Hate Speech and Targets using Multimodal Data from Russia-Ukraine Crisis](https://doi.org/10.1145/3589335.3651973)|Surendrabikram Thapa, Farhan Ahmad Jafri, Kritesh Rauniyar, Mehwish Nasim, Usman Naseem||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RUHate-MM:+Identification+of+Hate+Speech+and+Targets+using+Multimodal+Data+from+Russia-Ukraine+Crisis)|0|
|[Unveiling Misogyny Memes: A Multimodal Analysis of Modality Effects on Identification](https://doi.org/10.1145/3589335.3651974)|Shijing Chen, Usman Naseem, Imran Razzak, Flora D. Salim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+Misogyny+Memes:+A+Multimodal+Analysis+of+Modality+Effects+on+Identification)|0|
|[Ensemble Pretrained Models for Multimodal Sentiment Analysis using Textual and Video Data Fusion](https://doi.org/10.1145/3589335.3651971)|Zhicheng Liu, Ali Braytee, Ali Anaissi, Guifu Zhang, Lingyun Qin, Junaid Akram||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ensemble+Pretrained+Models+for+Multimodal+Sentiment+Analysis+using+Textual+and+Video+Data+Fusion)|0|
|[AI Deepfakes on the Web: The 'Wicked' Challenges for AI Ethics, Law and Technology](https://doi.org/10.1145/3589334.3649116)|Jeannie Marie Paterson||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI+Deepfakes+on+the+Web:+The+'Wicked'+Challenges+for+AI+Ethics,+Law+and+Technology)|0|
|[Challenges Toward AGI and Its Impact to the Web](https://doi.org/10.1145/3589334.3649113)|Bo Zhang, Jie Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Challenges+Toward+AGI+and+Its+Impact+to+the+Web)|0|
|[Budget-Constrained Auctions with Unassured Priors: Strategic Equivalence and Structural Properties](https://doi.org/10.1145/3589334.3645344)|Zhaohua Chen, Mingwei Yang, Chang Wang, Jicheng Li, Zheng Cai, Yukun Ren, Zhihua Zhu, Xiaotie Deng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Budget-Constrained+Auctions+with+Unassured+Priors:+Strategic+Equivalence+and+Structural+Properties)|0|
|[Efficiency of the Generalized Second-Price Auction for Value Maximizers](https://doi.org/10.1145/3589334.3645360)|Yuan Deng, Mohammad Mahdian, Jieming Mao, Vahab Mirrokni, Hanrui Zhang, Song Zuo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficiency+of+the+Generalized+Second-Price+Auction+for+Value+Maximizers)|0|
|[Data Exchange Markets via Utility Balancing](https://doi.org/10.1145/3589334.3645364)|Aditya Bhaskara, Sreenivas Gollapudi, Sungjin Im, Kostas Kollias, Kamesh Munagala, Govind S. Sankar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data+Exchange+Markets+via+Utility+Balancing)|0|
|[Fine-Tuning Games: Bargaining and Adaptation for General-Purpose Models](https://doi.org/10.1145/3589334.3645366)|Benjamin Laufer, Jon M. Kleinberg, Hoda Heidari||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fine-Tuning+Games:+Bargaining+and+Adaptation+for+General-Purpose+Models)|0|
|[Robust Decision Aggregation with Second-order Information](https://doi.org/10.1145/3589334.3645384)|Yuqi Pan, Zhaohua Chen, Yuqing Kong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Decision+Aggregation+with+Second-order+Information)|0|
|[Identifying Risky Vendors in Cryptocurrency P2P Marketplaces](https://doi.org/10.1145/3589334.3645475)|Taro Tsuchiya, Alejandro Cuevas, Nicolas Christin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Identifying+Risky+Vendors+in+Cryptocurrency+P2P+Marketplaces)|0|
|[Prior-Free Mechanism with Welfare Guarantees](https://doi.org/10.1145/3589334.3645500)|Guru Guruganesh, Jon Schneider, Joshua R. Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prior-Free+Mechanism+with+Welfare+Guarantees)|0|
|[Mechanism Design for Large Language Models](https://doi.org/10.1145/3589334.3645511)|Paul Dütting, Vahab Mirrokni, Renato Paes Leme, Haifeng Xu, Song Zuo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mechanism+Design+for+Large+Language+Models)|0|
|[Core-Competitiveness in Partially Observable Networked Market](https://doi.org/10.1145/3589334.3645555)|Bin Li, Dong Hao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Core-Competitiveness+in+Partially+Observable+Networked+Market)|0|
|[Unveiling the Paradox of NFT Prosperity](https://doi.org/10.1145/3589334.3645566)|Jintao Huang, Pengcheng Xia, Jiefeng Li, Kai Ma, Gareth Tyson, Xiapu Luo, Lei Wu, Yajin Zhou, Wei Cai, Haoyu Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+the+Paradox+of+NFT+Prosperity)|0|
|[Fair Surveillance Assignment Problem](https://doi.org/10.1145/3589334.3645613)|Fangxiao Wang, Bo Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Surveillance+Assignment+Problem)|0|
|[Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools](https://doi.org/10.1145/3589334.3645615)|Wentao Zhang, Yilei Zhao, Shuo Sun, Jie Ying, Yonggang Xie, Zitao Song, Xinrun Wang, Bo An||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reinforcement+Learning+with+Maskable+Stock+Representation+for+Portfolio+Management+in+Customizable+Stock+Pools)|0|
|[Exit Ripple Effects: Understanding the Disruption of Socialization Networks Following Employee Departures](https://doi.org/10.1145/3589334.3645634)|David Gamba, Yulin Yu, Yuan Yuan, Grant Schoenebeck, Daniel M. Romero||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exit+Ripple+Effects:+Understanding+the+Disruption+of+Socialization+Networks+Following+Employee+Departures)|0|
|[APT-Pipe: A Prompt-Tuning Tool for Social Data Annotation using ChatGPT](https://doi.org/10.1145/3589334.3645642)|Yiming Zhu, Zhizhuo Yin, Gareth Tyson, Ehsan ul Haq, LikHang Lee, Pan Hui||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=APT-Pipe:+A+Prompt-Tuning+Tool+for+Social+Data+Annotation+using+ChatGPT)|0|
|[Spot Check Equivalence: An Interpretable Metric for Information Elicitation Mechanisms](https://doi.org/10.1145/3589334.3645679)|Shengwei Xu, Yichi Zhang, Paul Resnick, Grant Schoenebeck||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spot+Check+Equivalence:+An+Interpretable+Metric+for+Information+Elicitation+Mechanisms)|0|
|[Optimal Engagement-Diversity Tradeoffs in Social Media](https://doi.org/10.1145/3589334.3645713)|Fabian Baumann, Daniel Halpern, Ariel D. Procaccia, Iyad Rahwan, Itai Shapira, Manuel Wüthrich||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimal+Engagement-Diversity+Tradeoffs+in+Social+Media)|0|
|[MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning](https://doi.org/10.1145/3589334.3645322)|Yun Zhu, Haizhou Shi, Zhenshuo Zhang, Siliang Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MARIO:+Model+Agnostic+Recipe+for+Improving+OOD+Generalization+of+Graph+Contrastive+Learning)|0|
|[Cooperative Classification and Rationalization for Graph Generalization](https://doi.org/10.1145/3589334.3645332)|Linan Yue, Qi Liu, Ye Liu, Weibo Gao, Fangzhou Yao, Wenfeng Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cooperative+Classification+and+Rationalization+for+Graph+Generalization)|0|
|[Cost-effective Data Labelling for Graph Neural Networks](https://doi.org/10.1145/3589334.3645339)|Shixun Huang, Ge Lee, Zhifeng Bao, Shirui Pan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cost-effective+Data+Labelling+for+Graph+Neural+Networks)|0|
|[Masked Graph Autoencoder with Non-discrete Bandwidths](https://doi.org/10.1145/3589334.3645370)|Ziwen Zhao, Yuhua Li, Yixiong Zou, Jiliang Tang, Ruixuan Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Masked+Graph+Autoencoder+with+Non-discrete+Bandwidths)|0|
|[Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks](https://doi.org/10.1145/3589334.3645367)|Hao Liu, Jiarui Feng, Lecheng Kong, Dacheng Tao, Yixin Chen, Muhan Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Contrastive+Learning+Meets+Graph+Meta+Learning:+A+Unified+Method+for+Few-shot+Node+Tasks)|0|
|[Local Centrality Minimization with Quality Guarantees](https://doi.org/10.1145/3589334.3645382)|Atsushi Miyauchi, Lorenzo Severini, Francesco Bonchi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Local+Centrality+Minimization+with+Quality+Guarantees)|0|
|[Fast Inference of Removal-Based Node Influence](https://doi.org/10.1145/3589334.3645389)|Weikai Li, Zhiping Xiao, Xiao Luo, Yizhou Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Inference+of+Removal-Based+Node+Influence)|0|
|[Memory Disagreement: A Pseudo-Labeling Measure from Training Dynamics for Semi-supervised Graph Learning](https://doi.org/10.1145/3589334.3645398)|Hongbin Pei, Yuheng Xiong, Pinghui Wang, Jing Tao, Jialun Liu, Huiqi Deng, Jie Ma, Xiaohong Guan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Memory+Disagreement:+A+Pseudo-Labeling+Measure+from+Training+Dynamics+for+Semi-supervised+Graph+Learning)|0|
|[Descriptive Kernel Convolution Network with Improved Random Walk Kernel](https://doi.org/10.1145/3589334.3645405)|MengChieh Lee, Lingxiao Zhao, Leman Akoglu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Descriptive+Kernel+Convolution+Network+with+Improved+Random+Walk+Kernel)|0|
|[Dynamic Graph Information Bottleneck](https://doi.org/10.1145/3589334.3645411)|Haonan Yuan, Qingyun Sun, Xingcheng Fu, Cheng Ji, Jianxin Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Graph+Information+Bottleneck)|0|
|[Extracting Small Subgraphs in Road Networks](https://doi.org/10.1145/3589334.3645415)|Sara Ahmadian, Sreenivas Gollapudi, Gregory Hutchins, Kostas Kollias, Xizhi Tan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Extracting+Small+Subgraphs+in+Road+Networks)|0|
|[Game-theoretic Counterfactual Explanation for Graph Neural Networks](https://doi.org/10.1145/3589334.3645419)|Chirag Chhablani, Sarthak Jain, Akshay Channesh, Ian A. Kash, Sourav Medya||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Game-theoretic+Counterfactual+Explanation+for+Graph+Neural+Networks)|0|
|[Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph](https://doi.org/10.1145/3589334.3645452)|Linfeng Cao, Haoran Deng, Yang Yang, Chunping Wang, Lei Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph-Skeleton:+~1%+Nodes+are+Sufficient+to+Represent+Billion-Scale+Graph)|0|
|[GAUSS: GrAph-customized Universal Self-Supervised Learning](https://doi.org/10.1145/3589334.3645453)|Liang Yang, Weixiao Hu, Jizhong Xu, Runjie Shi, Dongxiao He, Chuan Wang, Xiaochun Cao, Zhen Wang, Bingxin Niu, Yuanfang Guo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GAUSS:+GrAph-customized+Universal+Self-Supervised+Learning)|0|
|[Optimizing Network Resilience via Vertex Anchoring](https://doi.org/10.1145/3589334.3645465)|Siyi Teng, Jiadong Xie, Fan Zhang, Can Lu, Juntao Fang, Kai Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Network+Resilience+via+Vertex+Anchoring)|0|
|[VilLain: Self-Supervised Learning on Homogeneous Hypergraphs without Features via Virtual Label Propagation](https://doi.org/10.1145/3589334.3645454)|Geon Lee, Soo Yong Lee, Kijung Shin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=VilLain:+Self-Supervised+Learning+on+Homogeneous+Hypergraphs+without+Features+via+Virtual+Label+Propagation)|0|
|[SMUG: Sand Mixing for Unobserved Class Detection in Graph Few-Shot Learning](https://doi.org/10.1145/3589334.3645466)|Chenxu Wang, Xichan Nie, Jinfeng Chen, Pinghui Wang, Junzhou Zhao, Xiaohong Guan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SMUG:+Sand+Mixing+for+Unobserved+Class+Detection+in+Graph+Few-Shot+Learning)|0|
|[Graph Contrastive Learning with Cohesive Subgraph Awareness](https://doi.org/10.1145/3589334.3645470)|Yucheng Wu, Leye Wang, Xiao Han, HanJia Ye||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Contrastive+Learning+with+Cohesive+Subgraph+Awareness)|0|
|[GNNFingers: A Fingerprinting Framework for Verifying Ownerships of Graph Neural Networks](https://doi.org/10.1145/3589334.3645489)|Xiaoyu You, Youhe Jiang, Jianwei Xu, Mi Zhang, Min Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GNNFingers:+A+Fingerprinting+Framework+for+Verifying+Ownerships+of+Graph+Neural+Networks)|0|
|[Graph Fairness Learning under Distribution Shifts](https://doi.org/10.1145/3589334.3645508)|Yibo Li, Xiao Wang, Yujie Xing, Shaohua Fan, Ruijia Wang, Yaoqi Liu, Chuan Shi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Fairness+Learning+under+Distribution+Shifts)|0|
|[Self-Guided Robust Graph Structure Refinement](https://doi.org/10.1145/3589334.3645522)|Yeonjun In, Kanghoon Yoon, Kibum Kim, Kijung Shin, Chanyoung Park||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Guided+Robust+Graph+Structure+Refinement)|0|
|[DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning](https://doi.org/10.1145/3589334.3645561)|Seungyoon Choi, Wonjoong Kim, Sungwon Kim, Yeonjun In, Sein Kim, Chanyoung Park||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DSLR:+Diversity+Enhancement+and+Structure+Learning+for+Rehearsal-based+Graph+Continual+Learning)|0|
|[Calibrating Graph Neural Networks from a Data-centric Perspective](https://doi.org/10.1145/3589334.3645562)|Cheng Yang, Chengdong Yang, Chuan Shi, Yawen Li, Zhiqiang Zhang, Jun Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Calibrating+Graph+Neural+Networks+from+a+Data-centric+Perspective)|0|
|[EXGC: Bridging Efficiency and Explainability in Graph Condensation](https://doi.org/10.1145/3589334.3645551)|Junfeng Fang, Xinglin Li, Yongduo Sui, Yuan Gao, Guibin Zhang, Kun Wang, Xiang Wang, Xiangnan He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EXGC:+Bridging+Efficiency+and+Explainability+in+Graph+Condensation)|0|
|[Fast and Accurate Fair k-Center Clustering in Doubling Metrics](https://doi.org/10.1145/3589334.3645568)|Matteo Ceccarello, Andrea Pietracaprina, Geppino Pucci||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+and+Accurate+Fair+k-Center+Clustering+in+Doubling+Metrics)|0|
|[Graph Principal Flow Network for Conditional Graph Generation](https://doi.org/10.1145/3589334.3645570)|Zhanfeng Mo, Tianze Luo, Sinno Jialin Pan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Principal+Flow+Network+for+Conditional+Graph+Generation)|0|
|[Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem](https://doi.org/10.1145/3589334.3645583)|Chen Huang, Haoyang Li, Yifan Zhang, Wenqiang Lei, Jiancheng Lv||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-Space+Adaptive+Filter:+Integrating+Graph+Topology+and+Node+Attributes+for+Alleviating+the+Over-smoothing+Problem)|0|
|[A Quasi-Wasserstein Loss for Learning Graph Neural Networks](https://doi.org/10.1145/3589334.3645586)|Minjie Cheng, Hongteng Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Quasi-Wasserstein+Loss+for+Learning+Graph+Neural+Networks)|0|
|[GNNShap: Scalable and Accurate GNN Explanation using Shapley Values](https://doi.org/10.1145/3589334.3645599)|Selahattin Akkas, Ariful Azad||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GNNShap:+Scalable+and+Accurate+GNN+Explanation+using+Shapley+Values)|0|
|[Graph Out-of-Distribution Generalization via Causal Intervention](https://doi.org/10.1145/3589334.3645604)|Qitian Wu, Fan Nie, Chenxiao Yang, Tianyi Bao, Junchi Yan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Out-of-Distribution+Generalization+via+Causal+Intervention)|0|
|[Adversarial Mask Explainer for Graph Neural Networks](https://doi.org/10.1145/3589334.3645608)|Wei Zhang, Xiaofan Li, Wolfgang Nejdl||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adversarial+Mask+Explainer+for+Graph+Neural+Networks)|0|
|[On the Feasibility of Simple Transformer for Dynamic Graph Modeling](https://doi.org/10.1145/3589334.3645622)|Yuxia Wu, Yuan Fang, Lizi Liao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Feasibility+of+Simple+Transformer+for+Dynamic+Graph+Modeling)|0|
|[Can GNN be Good Adapter for LLMs?](https://doi.org/10.1145/3589334.3645627)|Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei Chai, Qi Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+GNN+be+Good+Adapter+for+LLMs?)|0|
|[When Imbalance Meets Imbalance: Structure-driven Learning for Imbalanced Graph Classification](https://doi.org/10.1145/3589334.3645629)|Wei Xu, Pengkun Wang, Zhe Zhao, Binwu Wang, Xu Wang, Yang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+Imbalance+Meets+Imbalance:+Structure-driven+Learning+for+Imbalanced+Graph+Classification)|0|
|[Disambiguated Node Classification with Graph Neural Networks](https://doi.org/10.1145/3589334.3645637)|Tianxiang Zhao, Xiang Zhang, Suhang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disambiguated+Node+Classification+with+Graph+Neural+Networks)|0|
|[Finding Densest Subgraphs with Edge-Color Constraints](https://doi.org/10.1145/3589334.3645647)|Lutz Oettershagen, Honglian Wang, Aristides Gionis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Finding+Densest+Subgraphs+with+Edge-Color+Constraints)|0|
|[Low Mileage, High Fidelity: Evaluating Hypergraph Expansion Methods by Quantifying the Information Loss](https://doi.org/10.1145/3589334.3645657)|David Y. Kang, Qiaozhu Mei, SangWook Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Low+Mileage,+High+Fidelity:+Evaluating+Hypergraph+Expansion+Methods+by+Quantifying+the+Information+Loss)|0|
|[Learning Scalable Structural Representations for Link Prediction with Bloom Signatures](https://doi.org/10.1145/3589334.3645672)|Tianyi Zhang, Haoteng Yin, Rongzhe Wei, Pan Li, Anshumali Shrivastava||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Scalable+Structural+Representations+for+Link+Prediction+with+Bloom+Signatures)|0|
|[GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks](https://doi.org/10.1145/3589334.3645682)|Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, Chuan Shi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphTranslator:+Aligning+Graph+Model+to+Large+Language+Model+for+Open-ended+Tasks)|0|
|[HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks](https://doi.org/10.1145/3589334.3645685)|Yihong Ma, Ning Yan, Jiayu Li, Masood S. Mortazavi, Nitesh V. Chawla||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HetGPT:+Harnessing+the+Power+of+Prompt+Tuning+in+Pre-Trained+Heterogeneous+Graph+Neural+Networks)|0|
|[Graph Contrastive Learning via Interventional View Generation](https://doi.org/10.1145/3589334.3645687)|Zengyi Wo, Minglai Shao, Wenjun Wang, Xuan Guo, Lu Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Contrastive+Learning+via+Interventional+View+Generation)|0|
|[Endowing Pre-trained Graph Models with Provable Fairness](https://doi.org/10.1145/3589334.3645703)|Zhongjian Zhang, Mengmei Zhang, Yue Yu, Cheng Yang, Jiawei Liu, Chuan Shi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Endowing+Pre-trained+Graph+Models+with+Provable+Fairness)|0|
|[Unveiling Delay Effects in Traffic Forecasting: A Perspective from Spatial-Temporal Delay Differential Equations](https://doi.org/10.1145/3589334.3645688)|Qingqing Long, Zheng Fang, Chen Fang, Chong Chen, Pengfei Wang, Yuanchun Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+Delay+Effects+in+Traffic+Forecasting:+A+Perspective+from+Spatial-Temporal+Delay+Differential+Equations)|0|
|[Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace Approach](https://doi.org/10.1145/3589334.3645705)|Keke Huang, Wencai Cao, Hoang Ta, Xiaokui Xiao, Pietro Liò||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Polynomial+Graph+Filters:+A+Novel+Adaptive+Krylov+Subspace+Approach)|0|
|[Full-Attention Driven Graph Contrastive Learning: with Effective Mutual Information Insight](https://doi.org/10.1145/3589334.3645717)|Long Li, Zemin Liu, Chenghao Liu, Jianling Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Full-Attention+Driven+Graph+Contrastive+Learning:+with+Effective+Mutual+Information+Insight)|0|
|[Understanding GDPR Non-Compliance in Privacy Policies of Alexa Skills in European Marketplaces](https://doi.org/10.1145/3589334.3645409)|Song Liao, Mohammed Aldeen, Jingwen Yan, Long Cheng, Xiapu Luo, Haipeng Cai, Hongxin Hu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+GDPR+Non-Compliance+in+Privacy+Policies+of+Alexa+Skills+in+European+Marketplaces)|0|
|[Differentially Private Selection from Secure Distributed Computing](https://doi.org/10.1145/3589334.3645435)|Ivan Damgård, Hannah Keller, Boel Nelson, Claudio Orlandi, Rasmus Pagh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Differentially+Private+Selection+from+Secure+Distributed+Computing)|0|
|[The Dynamics of (Not) Unfollowing Misinformation Spreaders](https://doi.org/10.1145/3589334.3645445)|Joshua Ashkinaze, Eric Gilbert, Ceren Budak||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Dynamics+of+(Not)+Unfollowing+Misinformation+Spreaders)|0|
|[Federated Learning Vulnerabilities: Privacy Attacks with Denoising Diffusion Probabilistic Models](https://doi.org/10.1145/3589334.3645514)|Hongyan Gu, Xinyi Zhang, Jiang Li, Hui Wei, Baiqi Li, Xinli Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Federated+Learning+Vulnerabilities:+Privacy+Attacks+with+Denoising+Diffusion+Probabilistic+Models)|0|
|[Fair Graph Representation Learning via Sensitive Attribute Disentanglement](https://doi.org/10.1145/3589334.3645532)|Yuchang Zhu, Jintang Li, Zibin Zheng, Liang Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Graph+Representation+Learning+via+Sensitive+Attribute+Disentanglement)|0|
|[DPAR: Decoupled Graph Neural Networks with Node-Level Differential Privacy](https://doi.org/10.1145/3589334.3645531)|Qiuchen Zhang, HongKyu Lee, Jing Ma, Jian Lou, Carl Yang, Li Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DPAR:+Decoupled+Graph+Neural+Networks+with+Node-Level+Differential+Privacy)|0|
|[A Worldwide View on the Reachability of Encrypted DNS Services](https://doi.org/10.1145/3589334.3645539)|Ruixuan Li, Baojun Liu, Chaoyi Lu, Haixin Duan, Jun Shao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Worldwide+View+on+the+Reachability+of+Encrypted+DNS+Services)|0|
|[Contrastive Fingerprinting: A Novel Website Fingerprinting Attack over Few-shot Traces](https://doi.org/10.1145/3589334.3645575)|Yi Xie, Jiahao Feng, Wenju Huang, Yixi Zhang, Xueliang Sun, Xiaochou Chen, Xiapu Luo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Fingerprinting:+A+Novel+Website+Fingerprinting+Attack+over+Few-shot+Traces)|0|
|[Analyzing Ad Exposure and Content in Child-Oriented Videos on YouTube](https://doi.org/10.1145/3589334.3645585)|Emaan Bilal Khan, Nida Tanveer, Aima Shahid, Mohammad Jaffer Iqbal, Haashim Ali Mirza, Armish Javed, Ihsan Ayyub Qazi, Zafar Ayyub Qazi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analyzing+Ad+Exposure+and+Content+in+Child-Oriented+Videos+on+YouTube)|0|
|[A Study of GDPR Compliance under the Transparency and Consent Framework](https://doi.org/10.1145/3589334.3645618)|Michael Smith, Antonio TorresAgüero, Riley Grossman, Pritam Sen, Yi Chen, Cristian Borcea||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Study+of+GDPR+Compliance+under+the+Transparency+and+Consent+Framework)|0|
|[Breaking the Trilemma of Privacy, Utility, and Efficiency via Controllable Machine Unlearning](https://doi.org/10.1145/3589334.3645669)|Zheyuan Liu, Guangyao Dou, Eli Chien, Chunhui Zhang, Yijun Tian, Ziwei Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Breaking+the+Trilemma+of+Privacy,+Utility,+and+Efficiency+via+Controllable+Machine+Unlearning)|0|
|[Heterogeneous Subgraph Transformer for Fake News Detection](https://doi.org/10.1145/3589334.3645680)|Yuchen Zhang, Xiaoxiao Ma, Jia Wu, Jian Yang, Hao Fan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Heterogeneous+Subgraph+Transformer+for+Fake+News+Detection)|0|
|[Experimental Security Analysis of Sensitive Data Access by Browser Extensions](https://doi.org/10.1145/3589334.3645683)|Asmit Nayak, Rishabh Khandelwal, Earlence Fernandes, Kassem Fawaz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Experimental+Security+Analysis+of+Sensitive+Data+Access+by+Browser+Extensions)|0|
|[Automating Website Registration for Studying GDPR Compliance](https://doi.org/10.1145/3589334.3645709)|Karel Kubicek, Jakob Merane, Ahmed Bouhoula, David A. Basin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automating+Website+Registration+for+Studying+GDPR+Compliance)|0|
|[Generating Multi-turn Clarification for Web Information Seeking](https://doi.org/10.1145/3589334.3645712)|Ziliang Zhao, Zhicheng Dou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generating+Multi-turn+Clarification+for+Web+Information+Seeking)|0|
|[Advancing Web 3.0: Making Smart Contracts Smarter on Blockchain](https://doi.org/10.1145/3589334.3645319)|Junqin Huang, Linghe Kong, Guanjie Cheng, Qiao Xiang, Guihai Chen, Gang Huang, Xue Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Advancing+Web+3.0:+Making+Smart+Contracts+Smarter+on+Blockchain)|0|
|[From Promises to Practice: Evaluating the Private Browsing Modes of Android Browser Apps](https://doi.org/10.1145/3589334.3645320)|Xiaoyin Liu, Wenzhi Li, Qinsheng Hou, Shishuai Yang, Lingyun Ying, Wenrui Diao, Yanan Li, Shanqing Guo, Haixin Duan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Promises+to+Practice:+Evaluating+the+Private+Browsing+Modes+of+Android+Browser+Apps)|0|
|[Interface Illusions: Uncovering the Rise of Visual Scams in Cryptocurrency Wallets](https://doi.org/10.1145/3589334.3645348)|Guoyi Ye, Geng Hong, Yuan Zhang, Min Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interface+Illusions:+Uncovering+the+Rise+of+Visual+Scams+in+Cryptocurrency+Wallets)|0|
|[Trident: A Universal Framework for Fine-Grained and Class-Incremental Unknown Traffic Detection](https://doi.org/10.1145/3589334.3645407)|Ziming Zhao, Zhaoxuan Li, Zhuoxue Song, Wenhao Li, Fan Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Trident:+A+Universal+Framework+for+Fine-Grained+and+Class-Incremental+Unknown+Traffic+Detection)|0|
|[SSI, from Specifications to Protocol? Formally Verify Security!](https://doi.org/10.1145/3589334.3645426)|Christoph H.J. Braun, Ross Horne, Tobias Käfer, Sjouke Mauw||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SSI,+from+Specifications+to+Protocol?+Formally+Verify+Security!)|0|
|[GRASP: Hardening Serverless Applications through Graph Reachability Analysis of Security Policies](https://doi.org/10.1145/3589334.3645436)|Isaac Polinsky, Pubali Datta, Adam Bates, William Enck||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GRASP:+Hardening+Serverless+Applications+through+Graph+Reachability+Analysis+of+Security+Policies)|0|
|[Divide, Conquer, and Coalesce: Meta Parallel Graph Neural Network for IoT Intrusion Detection at Scale](https://doi.org/10.1145/3589334.3645457)|Hua Ding, Lixing Chen, Shenghong Li, Yang Bai, Pan Zhou, Zhe Qu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Divide,+Conquer,+and+Coalesce:+Meta+Parallel+Graph+Neural+Network+for+IoT+Intrusion+Detection+at+Scale)|0|
|[Medusa: Unveil Memory Exhaustion DoS Vulnerabilities in Protocol Implementations](https://doi.org/10.1145/3589334.3645476)|Zhengjie Du, Yuekang Li, Yaowen Zheng, Xiaohan Zhang, Cen Zhang, Yi Liu, Sheikh Mahbub Habib, Xinghua Li, Linzhang Wang, Yang Liu, Bing Mao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Medusa:+Unveil+Memory+Exhaustion+DoS+Vulnerabilities+in+Protocol+Implementations)|0|
|[ContraMTD: An Unsupervised Malicious Network Traffic Detection Method based on Contrastive Learning](https://doi.org/10.1145/3589334.3645479)|Xueying Han, Susu Cui, Jian Qin, Song Liu, Bo Jiang, Cong Dong, Zhigang Lu, Baoxu Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ContraMTD:+An+Unsupervised+Malicious+Network+Traffic+Detection+Method+based+on+Contrastive+Learning)|0|
|[Unfiltered: Measuring Cloud-based Email Filtering Bypasses](https://doi.org/10.1145/3589334.3645499)|Sumanth Rao, Enze Liu, Grant Ho, Geoffrey M. Voelker, Stefan Savage||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unfiltered:+Measuring+Cloud-based+Email+Filtering+Bypasses)|0|
|[RecurScan: Detecting Recurring Vulnerabilities in PHP Web Applications](https://doi.org/10.1145/3589334.3645530)|Youkun Shi, Yuan Zhang, Tianhao Bai, Lei Zhang, Xin Tan, Min Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RecurScan:+Detecting+Recurring+Vulnerabilities+in+PHP+Web+Applications)|0|
|[Phishing Vs. Legit: Comparative Analysis of Client-Side Resources of Phishing and Target Brand Websites](https://doi.org/10.1145/3589334.3645535)|Kyungchan Lim, Jaehwan Park, Doowon Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Phishing+Vs.+Legit:+Comparative+Analysis+of+Client-Side+Resources+of+Phishing+and+Target+Brand+Websites)|0|
|[Detecting and Understanding Self-Deleting JavaScript Code](https://doi.org/10.1145/3589334.3645540)|Xinzhe Wang, Zeyang Zhuang, Wei Meng, James Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+and+Understanding+Self-Deleting+JavaScript+Code)|0|
|[Malicious Package Detection using Metadata Information](https://doi.org/10.1145/3589334.3645543)|Sajal Halder, Michael Bewong, Arash Mahboubi, Yinhao Jiang, Md Rafiqul Islam, Md Zahidul Islam, Ryan HL Ip, Muhammad Ejaz Ahmed, Gowri Sankar Ramachandran, Muhammad Ali Babar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Malicious+Package+Detection+using+Metadata+Information)|0|
|[Unveiling the Invisible: Detection and Evaluation of Prototype Pollution Gadgets with Dynamic Taint Analysis](https://doi.org/10.1145/3589334.3645579)|Mikhail Shcherbakov, Paul Moosbrugger, Musard Balliu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+the+Invisible:+Detection+and+Evaluation+of+Prototype+Pollution+Gadgets+with+Dynamic+Taint+Analysis)|0|
|[ARTEMIS: Detecting Airdrop Hunters in NFT Markets with a Graph Learning System](https://doi.org/10.1145/3589334.3645597)|Chenyu Zhou, Hongzhou Chen, Hao Wu, Junyu Zhang, Wei Cai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ARTEMIS:+Detecting+Airdrop+Hunters+in+NFT+Markets+with+a+Graph+Learning+System)|0|
|[HSDirSniper: A New Attack Exploiting Vulnerabilities in Tor's Hidden Service Directories](https://doi.org/10.1145/3589334.3645591)|Qingfeng Zhang, Zhiyang Teng, Xuebin Wang, Yue Gao, Qingyun Liu, Jinqiao Shi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HSDirSniper:+A+New+Attack+Exploiting+Vulnerabilities+in+Tor's+Hidden+Service+Directories)|0|
|[The Matter of Captchas: An Analysis of a Brittle Security Feature on the Modern Web](https://doi.org/10.1145/3589334.3645619)|Behzad Ousat, Esteban Schafir, Duc C. Hoang, Mohammad Ali Tofighi, Cuong V. Nguyen, Sajjad Arshad, A. Selcuk Uluagac, Amin Kharraz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Matter+of+Captchas:+An+Analysis+of+a+Brittle+Security+Feature+on+the+Modern+Web)|0|
|[Characterizing Ethereum Upgradable Smart Contracts and Their Security Implications](https://doi.org/10.1145/3589334.3645640)|Xiaofan Li, Jin Yang, Jiaqi Chen, Yuzhe Tang, Xing Gao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Characterizing+Ethereum+Upgradable+Smart+Contracts+and+Their+Security+Implications)|0|
|[IDEA-DAC: Integrity-Driven Editing for Accountable Decentralized Anonymous Credentials via ZK-JSON](https://doi.org/10.1145/3589334.3645658)|Shuhao Zheng, Zonglun Li, Junliang Luo, Ziyue Xin, Xue Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IDEA-DAC:+Integrity-Driven+Editing+for+Accountable+Decentralized+Anonymous+Credentials+via+ZK-JSON)|0|
|[Is It Safe to Share Your Files? An Empirical Security Analysis of Google Workspace](https://doi.org/10.1145/3589334.3645697)|Liuhuo Wan, Kailong Wang, Haoyu Wang, Guangdong Bai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+It+Safe+to+Share+Your+Files?+An+Empirical+Security+Analysis+of+Google+Workspace)|0|
|[PanoptiChrome: A Modern In-browser Taint Analysis Framework](https://doi.org/10.1145/3589334.3645699)|Rahul Kanyal, Smruti R. Sarangi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PanoptiChrome:+A+Modern+In-browser+Taint+Analysis+Framework)|0|
|[PhishinWebView: Analysis of Anti-Phishing Entities in Mobile Apps with WebView Targeted Phishing](https://doi.org/10.1145/3589334.3645708)|Yoonjung Choi, Woonghee Lee, Junbeom Hur||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PhishinWebView:+Analysis+of+Anti-Phishing+Entities+in+Mobile+Apps+with+WebView+Targeted+Phishing)|0|
|[Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models](https://doi.org/10.1145/3589334.3645376)|Chenhan Yuan, Qianqian Xie, Jimin Huang, Sophia Ananiadou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Back+to+the+Future:+Towards+Explainable+Temporal+Reasoning+with+Large+Language+Models)|0|
|[A Knowledge-Injected Curriculum Pretraining Framework for Question Answering](https://doi.org/10.1145/3589334.3645406)|Xin Lin, Tianhuang Su, Zhenya Huang, Shangzi Xue, Haifeng Liu, Enhong Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Knowledge-Injected+Curriculum+Pretraining+Framework+for+Question+Answering)|0|
|[Using Model Calibration to Evaluate Link Prediction in Knowledge Graphs](https://doi.org/10.1145/3589334.3645506)|Aishwarya Rao, Narayanan Asuri Krishnan, Carlos R. Rivero||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Using+Model+Calibration+to+Evaluate+Link+Prediction+in+Knowledge+Graphs)|0|
|[Zero-shot Image Classification with Logic Adapter and Rule Prompt](https://doi.org/10.1145/3589334.3645554)|Dongran Yu, Xueyan Liu, Bo Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Zero-shot+Image+Classification+with+Logic+Adapter+and+Rule+Prompt)|0|
|[NPCS: Native Provenance Computation for SPARQL](https://doi.org/10.1145/3589334.3645557)|Zubaria Asma, Daniel Hernández, Luis Galárraga, Giorgos Flouris, Irini Fundulaki, Katja Hose||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NPCS:+Native+Provenance+Computation+for+SPARQL)|0|
|[Taxonomy Completion via Implicit Concept Insertion](https://doi.org/10.1145/3589334.3645584)|Jingchuan Shi, Hang Dong, Jiaoyan Chen, Zhe Wu, Ian Horrocks||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Taxonomy+Completion+via+Implicit+Concept+Insertion)|0|
|[UniLP: Unified Topology-aware Generative Framework for Link Prediction in Knowledge Graph](https://doi.org/10.1145/3589334.3645592)|Ben Liu, Miao Peng, Wenjie Xu, Xu Jia, Min Peng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UniLP:+Unified+Topology-aware+Generative+Framework+for+Link+Prediction+in+Knowledge+Graph)|0|
|[A Symbolic Rule Integration Framework with Logic Transformer for Inductive Relation Prediction](https://doi.org/10.1145/3589334.3645594)|Yudai Pan, Jun Liu, Tianzhe Zhao, Lingling Zhang, Yun Lin, Jin Song Dong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Symbolic+Rule+Integration+Framework+with+Logic+Transformer+for+Inductive+Relation+Prediction)|0|
|[Causal Question Answering with Reinforcement Learning](https://doi.org/10.1145/3589334.3645610)|Lukas Blübaum, Stefan Heindorf||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Question+Answering+with+Reinforcement+Learning)|0|
|[DRAM-like Architecture with Asynchronous Refreshing for Continual Relation Extraction](https://doi.org/10.1145/3589334.3645621)|Tianci Bu, Kang Yang, Wenchuan Yang, Jiawei Feng, Xiaoyu Zhang, Xin Lu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DRAM-like+Architecture+with+Asynchronous+Refreshing+for+Continual+Relation+Extraction)|0|
|[KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large Language Models](https://doi.org/10.1145/3589334.3645623)|Yuyang Bai, Shangbin Feng, Vidhisha Balachandran, Zhaoxuan Tan, Shiqi Lou, Tianxing He, Yulia Tsvetkov||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KGQuiz:+Evaluating+the+Generalization+of+Encoded+Knowledge+in+Large+Language+Models)|0|
|[Robust Link Prediction over Noisy Hyper-Relational Knowledge Graphs via Active Learning](https://doi.org/10.1145/3589334.3645686)|Weijian Yu, Jie Yang, Dingqi Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Link+Prediction+over+Noisy+Hyper-Relational+Knowledge+Graphs+via+Active+Learning)|0|
|[OODREB: Benchmarking State-of-the-Art Methods for Out-Of-Distribution Generalization on Relation Extraction](https://doi.org/10.1145/3589334.3645695)|Haotian Chen, Houjing Guo, Bingsheng Chen, Xiangdong Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OODREB:+Benchmarking+State-of-the-Art+Methods+for+Out-Of-Distribution+Generalization+on+Relation+Extraction)|0|
|[Toward Practical Entity Alignment Method Design: Insights from New Highly Heterogeneous Knowledge Graph Datasets](https://doi.org/10.1145/3589334.3645720)|Xuhui Jiang, Chengjin Xu, Yinghan Shen, Yuanzhuo Wang, Fenglong Su, Zhichao Shi, Fei Sun, Zixuan Li, Jian Guo, Huawei Shen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Toward+Practical+Entity+Alignment+Method+Design:+Insights+from+New+Highly+Heterogeneous+Knowledge+Graph+Datasets)|0|
|[Social Media Discourses on Interracial Intimacy: Tracking Racism and Sexism through Chinese Geo-located Social Media Data](https://doi.org/10.1145/3589334.3645334)|Zheng Wei, Yixuan Xie, Danyun Xiao, Simin Zhang, Pan Hui, Muzhi Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Social+Media+Discourses+on+Interracial+Intimacy:+Tracking+Racism+and+Sexism+through+Chinese+Geo-located+Social+Media+Data)|0|
|[Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models](https://doi.org/10.1145/3589334.3645381)|Hongzhan Lin, Ziyang Luo, Wei Gao, Jing Ma, Bo Wang, Ruichao Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Explainable+Harmful+Meme+Detection+through+Multimodal+Debate+between+Large+Language+Models)|0|
|[What News Do People Get on Social Media? Analyzing Exposure and Consumption of News through Data Donations](https://doi.org/10.1145/3589334.3645399)|Salim Chouaki, Abhijnan Chakraborty, Oana Goga, Savvas Zannettou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What+News+Do+People+Get+on+Social+Media?+Analyzing+Exposure+and+Consumption+of+News+through+Data+Donations)|0|
|[Euphemism Identification via Feature Fusion and Individualization](https://doi.org/10.1145/3589334.3645433)|Yuxue Hu, Mingmin Wu, Zhongqiang Huang, Junsong Li, Xing Ge, Ying Sha||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Euphemism+Identification+via+Feature+Fusion+and+Individualization)|0|
|[Team Formation amidst Conflicts](https://doi.org/10.1145/3589334.3645444)|Iasonas Nikolaou, Evimaria Terzi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Team+Formation+amidst+Conflicts)|0|
|[T3RD: Test-Time Training for Rumor Detection on Social Media](https://doi.org/10.1145/3589334.3645443)|Huaiwen Zhang, Xinxin Liu, Qing Yang, Yang Yang, Fan Qi, Shengsheng Qian, Changsheng Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=T3RD:+Test-Time+Training+for+Rumor+Detection+on+Social+Media)|0|
|[ESCNet: Entity-enhanced and Stance Checking Network for Multi-modal Fact-Checking](https://doi.org/10.1145/3589334.3645455)|Fanrui Zhang, Jiawei Liu, Jingyi Xie, Qiang Zhang, Yongchao Xu, ZhengJun Zha||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ESCNet:+Entity-enhanced+and+Stance+Checking+Network+for+Multi-modal+Fact-Checking)|0|
|[Labor Space: A Unifying Representation of the Labor Market via Large Language Models](https://doi.org/10.1145/3589334.3645464)|Seongwoon Kim, YongYeol Ahn, Jaehyuk Park||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Labor+Space:+A+Unifying+Representation+of+the+Labor+Market+via+Large+Language+Models)|0|
|[Explainable Fake News Detection with Large Language Model via Defense Among Competing Wisdom](https://doi.org/10.1145/3589334.3645471)|Bo Wang, Jing Ma, Hongzhan Lin, Zhiwei Yang, Ruichao Yang, Yuan Tian, Yi Chang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+Fake+News+Detection+with+Large+Language+Model+via+Defense+Among+Competing+Wisdom)|0|
|[Navigating the Post-API Dilemma](https://doi.org/10.1145/3589334.3645503)|Amrit Poudel, Tim Weninger||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Navigating+the+Post-API+Dilemma)|0|
|[Unraveling the Dynamics of Stable and Curious Audiences in Web Systems](https://doi.org/10.1145/3589334.3645473)|Rodrigo Alves, Antoine Ledent, Renato Assunção, Pedro O. S. Vaz de Melo, Marius Kloft||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unraveling+the+Dynamics+of+Stable+and+Curious+Audiences+in+Web+Systems)|0|
|[SymLearn: A Symbiotic Crowd-AI Collective Learning Framework to Web-based Healthcare Policy Adherence Assessment](https://doi.org/10.1145/3589334.3645519)|Yang Zhang, Ruohan Zong, Lanyu Shang, Huimin Zeng, Zhenrui Yue, Dong Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SymLearn:+A+Symbiotic+Crowd-AI+Collective+Learning+Framework+to+Web-based+Healthcare+Policy+Adherence+Assessment)|0|
|[An Efficient Automatic Meta-Path Selection for Social Event Detection via Hyperbolic Space](https://doi.org/10.1145/3589334.3645526)|Zitai Qiu, Congbo Ma, Jia Wu, Jian Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Efficient+Automatic+Meta-Path+Selection+for+Social+Event+Detection+via+Hyperbolic+Space)|0|
|[NETEVOLVE: Social Network Forecasting using Multi-Agent Reinforcement Learning with Interpretable Features](https://doi.org/10.1145/3589334.3647982)|Kentaro Miyake, Hiroyoshi Ito, Christos Faloutsos, Hirotomo Matsumoto, Atsuyuki Morishima||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NETEVOLVE:+Social+Network+Forecasting+using+Multi-Agent+Reinforcement+Learning+with+Interpretable+Features)|0|
|[Analysis and Detection of "Pink Slime" Websites in Social Media Posts](https://doi.org/10.1145/3589334.3645588)|Abdullah Aljebreen, Weiyi Meng, Eduard C. Dragut||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analysis+and+Detection+of+"Pink+Slime"+Websites+in+Social+Media+Posts)|0|
|[Navigating Multidimensional Ideologies with Reddit's Political Compass: Economic Conflict and Social Affinity](https://doi.org/10.1145/3589334.3645606)|Ernesto Colacrai, Federico Cinus, Gianmarco De Francisci Morales, Michele Starnini||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Navigating+Multidimensional+Ideologies+with+Reddit's+Political+Compass:+Economic+Conflict+and+Social+Affinity)|0|
|[Unifying Local and Global Knowledge: Empowering Large Language Models as Political Experts with Knowledge Graphs](https://doi.org/10.1145/3589334.3645616)|Xinyi Mou, Zejun Li, Hanjia Lyu, Jiebo Luo, Zhongyu Wei||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unifying+Local+and+Global+Knowledge:+Empowering+Large+Language+Models+as+Political+Experts+with+Knowledge+Graphs)|0|
|[Not All Asians are the Same: A Disaggregated Approach to Identifying Anti-Asian Racism in Social Media](https://doi.org/10.1145/3589334.3645630)|Fan Wu, Sanyam Lakhanpal, Qian Li, Kookjin Lee, Doowon Kim, Heewon Chae, Kyounghee Hazel Kwon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Not+All+Asians+are+the+Same:+A+Disaggregated+Approach+to+Identifying+Anti-Asian+Racism+in+Social+Media)|0|
|[Global News Synchrony and Diversity During the Start of the COVID-19 Pandemic](https://doi.org/10.1145/3589334.3645645)|Xi Chen, Scott A. Hale, David Jurgens, Mattia Samory, Ethan Zuckerman, Przemyslaw A. Grabowicz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Global+News+Synchrony+and+Diversity+During+the+Start+of+the+COVID-19+Pandemic)|0|
|[Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries](https://doi.org/10.1145/3589334.3645643)|Yiqiao Jin, Mohit Chandra, Gaurav Verma, Yibo Hu, Munmun De Choudhury, Srijan Kumar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Better+to+Ask+in+English:+Cross-Lingual+Evaluation+of+Large+Language+Models+for+Healthcare+Queries)|0|
|[Bridging or Breaking: Impact of Intergroup Interactions on Religious Polarization](https://doi.org/10.1145/3589334.3645675)|Rochana Chaturvedi, Sugat Chaturvedi, Elena Zheleva||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bridging+or+Breaking:+Impact+of+Intergroup+Interactions+on+Religious+Polarization)|0|
|[ARES: Predictable Traffic Engineering under Controller Failures in SD-WANs](https://doi.org/10.1145/3589334.3645321)|Songshi Dou, Li Qi, Zehua Guo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ARES:+Predictable+Traffic+Engineering+under+Controller+Failures+in+SD-WANs)|0|
|[QUIC is not Quick Enough over Fast Internet](https://doi.org/10.1145/3589334.3645323)|Xumiao Zhang, Shuowei Jin, Yi He, Ahmad Hassan, Z. Morley Mao, Feng Qian, ZhiLi Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QUIC+is+not+Quick+Enough+over+Fast+Internet)|0|
|[GEES: Enabling Location Privacy-Preserving Energy Saving in Multi-Access Edge Computing](https://doi.org/10.1145/3589334.3645329)|Ziqi Wang, Xiaoyu Xia, Minhui Xue, Ibrahim Khalil, Minghui Liwang, Xun Yi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GEES:+Enabling+Location+Privacy-Preserving+Energy+Saving+in+Multi-Access+Edge+Computing)|0|
|[DirectFaaS: A Clean-Slate Network Architecture for Efficient Serverless Chain Communications](https://doi.org/10.1145/3589334.3645333)|Qingyang Zeng, Kaiyu Hou, Xue Leng, Yan Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DirectFaaS:+A+Clean-Slate+Network+Architecture+for+Efficient+Serverless+Chain+Communications)|0|
|[Meet Challenges of RTT Jitter, A Hybrid Internet Congestion Control Algorithm](https://doi.org/10.1145/3589334.3645338)|Lianchen Jia, Chao Zhou, Tianchi Huang, Chaoyang Li, Lifeng Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Meet+Challenges+of+RTT+Jitter,+A+Hybrid+Internet+Congestion+Control+Algorithm)|0|
|[Towards Energy-efficient Federated Learning via INT8-based Training on Mobile DSPs](https://doi.org/10.1145/3589334.3645341)|Jinliang Yuan, Shangguang Wang, Hongyu Li, Daliang Xu, Yuanchun Li, Mengwei Xu, Xuanzhe Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Energy-efficient+Federated+Learning+via+INT8-based+Training+on+Mobile+DSPs)|0|
|[FreqMAE: Frequency-Aware Masked Autoencoder for Multi-Modal IoT Sensing](https://doi.org/10.1145/3589334.3645346)|Denizhan Kara, Tomoyoshi Kimura, Shengzhong Liu, Jinyang Li, Dongxin Liu, Tianshi Wang, Ruijie Wang, Yizhuo Chen, Yigong Hu, Tarek F. Abdelzaher||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FreqMAE:+Frequency-Aware+Masked+Autoencoder+for+Multi-Modal+IoT+Sensing)|0|
|[Air-CAD: Edge-Assisted Multi-Drone Network for Real-time Crowd Anomaly Detection](https://doi.org/10.1145/3589334.3645362)|Yuanzheng Tan, Qing Li, Junkun Peng, Zhenhui Yuan, Yong Jiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Air-CAD:+Edge-Assisted+Multi-Drone+Network+for+Real-time+Crowd+Anomaly+Detection)|0|
|[λGrapher: A Resource-Efficient Serverless System for GNN Serving through Graph Sharing](https://doi.org/10.1145/3589334.3645383)|Haichuan Hu, Fangming Liu, Qiangyu Pei, Yongjie Yuan, Zichen Xu, Lin Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=λGrapher:+A+Resource-Efficient+Serverless+System+for+GNN+Serving+through+Graph+Sharing)|0|
|[SPRING: Improving the Throughput of Sharding Blockchain via Deep Reinforcement Learning Based State Placement](https://doi.org/10.1145/3589334.3645386)|Pengze Li, Mingxuan Song, Mingzhe Xing, Zhen Xiao, Qiuyu Ding, Shengjie Guan, Jieyi Long||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SPRING:+Improving+the+Throughput+of+Sharding+Blockchain+via+Deep+Reinforcement+Learning+Based+State+Placement)|0|
|[Supervised Fine-Tuning for Unsupervised KPI Anomaly Detection for Mobile Web Systems](https://doi.org/10.1145/3589334.3645392)|Zhaoyang Yu, Shenglin Zhang, Mingze Sun, Yingke Li, Yankai Zhao, Xiaolei Hua, Lin Zhu, Xidao Wen, Dan Pei||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Supervised+Fine-Tuning+for+Unsupervised+KPI+Anomaly+Detection+for+Mobile+Web+Systems)|0|
|[NCTM: A Novel Coded Transmission Mechanism for Short Video Deliveries](https://doi.org/10.1145/3589334.3645387)|Zhenge Xu, Qing Li, Wanxin Shi, Yong Jiang, Zhenhui Yuan, Peng Zhang, GabrielMiro Muntean||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NCTM:+A+Novel+Coded+Transmission+Mechanism+for+Short+Video+Deliveries)|0|
|[InArt: In-Network Aggregation with Route Selection for Accelerating Distributed Training](https://doi.org/10.1145/3589334.3645394)|Jiawei Liu, Yutong Zhai, Gongming Zhao, Hongli Xu, Jin Fang, Zhen Zeng, Ying Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=InArt:+In-Network+Aggregation+with+Route+Selection+for+Accelerating+Distributed+Training)|0|
|[FusionRender: Harnessing WebGPU's Power for Enhanced Graphics Performance on Web Browsers](https://doi.org/10.1145/3589334.3645395)|Weichen Bi, Yun Ma, Yudong Han, Yifan Chen, Deyu Tian, Jiaqi Du||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FusionRender:+Harnessing+WebGPU's+Power+for+Enhanced+Graphics+Performance+on+Web+Browsers)|0|
|[FedDSE: Distribution-aware Sub-model Extraction for Federated Learning over Resource-constrained Devices](https://doi.org/10.1145/3589334.3645416)|Haozhao Wang, Yabo Jia, Meng Zhang, Qinghao Hu, Hao Ren, Peng Sun, Yonggang Wen, Tianwei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedDSE:+Distribution-aware+Sub-model+Extraction+for+Federated+Learning+over+Resource-constrained+Devices)|0|
|[BlockDFL: A Blockchain-based Fully Decentralized Peer-to-Peer Federated Learning Framework](https://doi.org/10.1145/3589334.3645425)|Zhen Qin, Xueqiang Yan, Mengchu Zhou, Shuiguang Deng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BlockDFL:+A+Blockchain-based+Fully+Decentralized+Peer-to-Peer+Federated+Learning+Framework)|0|
|[Incentive and Dynamic Client Selection for Federated Unlearning](https://doi.org/10.1145/3589334.3645462)|Yijing Lin, Zhipeng Gao, Hongyang Du, Dusit Niyato, Jiawen Kang, Xiaoyuan Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Incentive+and+Dynamic+Client+Selection+for+Federated+Unlearning)|0|
|[Accelerating the Decentralized Federated Learning via Manipulating Edges](https://doi.org/10.1145/3589334.3645509)|Mingyang Zhou, Gang Liu, Kezhong Lu, Rui Mao, Hao Liao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Accelerating+the+Decentralized+Federated+Learning+via+Manipulating+Edges)|0|
|[How Few Davids Improve One Goliath: Federated Learning in Resource-Skewed Edge Computing Environments](https://doi.org/10.1145/3589334.3645544)|Jiayun Zhang, Shuheng Li, Haiyu Huang, Zihan Wang, Xiaohan Fu, Dezhi Hong, Rajesh K. Gupta, Jingbo Shang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Few+Davids+Improve+One+Goliath:+Federated+Learning+in+Resource-Skewed+Edge+Computing+Environments)|0|
|[Privacy-Preserving and Fairness-Aware Federated Learning for Critical Infrastructure Protection and Resilience](https://doi.org/10.1145/3589334.3645545)|Yanjun Zhang, Ruoxi Sun, Liyue Shen, Guangdong Bai, Minhui Xue, Mark Huasong Meng, Xue Li, Ryan K. L. Ko, Surya Nepal||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privacy-Preserving+and+Fairness-Aware+Federated+Learning+for+Critical+Infrastructure+Protection+and+Resilience)|0|
|[Making Cloud Spot Instance Interruption Events Visible](https://doi.org/10.1145/3589334.3645548)|Kyunghwan Kim, Kyungyong Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Making+Cloud+Spot+Instance+Interruption+Events+Visible)|0|
|[E2Usd: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series](https://doi.org/10.1145/3589334.3645593)|Zhichen Lai, Huan Li, Dalin Zhang, Yan Zhao, Weizhu Qian, Christian S. Jensen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=E2Usd:+Efficient-yet-effective+Unsupervised+State+Detection+for+Multivariate+Time+Series)|0|
|[Robust Route Planning under Uncertain Pickup Requests for Last-mile Delivery](https://doi.org/10.1145/3589334.3645595)|Hua Yan, Heng Tan, Haotian Wang, Desheng Zhang, Yu Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Route+Planning+under+Uncertain+Pickup+Requests+for+Last-mile+Delivery)|0|
|[Unity is Strength? Benchmarking the Robustness of Fusion-based 3D Object Detection against Physical Sensor Attack](https://doi.org/10.1145/3589334.3645612)|Zizhi Jin, Xuancun Lu, Bo Yang, Yushi Cheng, Chen Yan, Xiaoyu Ji, Wenyuan Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unity+is+Strength?+Benchmarking+the+Robustness+of+Fusion-based+3D+Object+Detection+against+Physical+Sensor+Attack)|0|
|[WEFix: Intelligent Automatic Generation of Explicit Waits for Efficient Web End-to-End Flaky Tests](https://doi.org/10.1145/3589334.3645628)|Xinyue Liu, Zihe Song, Weike Fang, Wei Yang, Weihang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=WEFix:+Intelligent+Automatic+Generation+of+Explicit+Waits+for+Efficient+Web+End-to-End+Flaky+Tests)|0|
|[SatGuard: Concealing Endless and Bursty Packet Losses in LEO Satellite Networks for Delay-Sensitive Web Applications](https://doi.org/10.1145/3589334.3645639)|Jihao Li, Hewu Li, Zeqi Lai, Qian Wu, Yijie Liu, Qi Zhang, Yuanjie Li, Jun Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SatGuard:+Concealing+Endless+and+Bursty+Packet+Losses+in+LEO+Satellite+Networks+for+Delay-Sensitive+Web+Applications)|0|
|[More Than Routing: Joint GPS and Route Modeling for Refine Trajectory Representation Learning](https://doi.org/10.1145/3589334.3645644)|Zhipeng Ma, Zheyan Tu, Xinhai Chen, Yan Zhang, Deguo Xia, Guyue Zhou, Yilun Chen, Yu Zheng, Jiangtao Gong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=More+Than+Routing:+Joint+GPS+and+Route+Modeling+for+Refine+Trajectory+Representation+Learning)|0|
|[Cardinality Counting in "Alcatraz": A Privacy-aware Federated Learning Approach](https://doi.org/10.1145/3589334.3645655)|Nan Wu, Xin Yuan, Shuo Wang, Hongsheng Hu, Minhui Xue||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cardinality+Counting+in+"Alcatraz":+A+Privacy-aware+Federated+Learning+Approach)|0|
|[GAMMA: Graph Neural Network-Based Multi-Bottleneck Localization for Microservices Applications](https://doi.org/10.1145/3589334.3645665)|Gagan Somashekar, Anurag Dutt, Mainak Adak, Tania LoridoBotran, Anshul Gandhi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GAMMA:+Graph+Neural+Network-Based+Multi-Bottleneck+Localization+for+Microservices+Applications)|0|
|[Revisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective](https://doi.org/10.1145/3589334.3645710)|Zexin Wang, Changhua Pei, Minghua Ma, Xin Wang, Zhihan Li, Dan Pei, Saravan Rajmohan, Dongmei Zhang, Qingwei Lin, Haiming Zhang, Jianhui Li, Gaogang Xie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+VAE+for+Unsupervised+Time+Series+Anomaly+Detection:+A+Frequency+Perspective)|0|
|[Interpretable Knowledge Tracing with Multiscale State Representation](https://doi.org/10.1145/3589334.3645373)|Jianwen Sun, Fenghua Yu, Qian Wan, Qing Li, Sannyuya Liu, Xiaoxuan Shen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interpretable+Knowledge+Tracing+with+Multiscale+State+Representation)|0|
|[COLA: Cross-city Mobility Transformer for Human Trajectory Simulation](https://doi.org/10.1145/3589334.3645469)|Yu Wang, Tongya Zheng, Yuxuan Liang, Shunyu Liu, Mingli Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=COLA:+Cross-city+Mobility+Transformer+for+Human+Trajectory+Simulation)|0|
|[Off-Policy Evaluation for Large Action Spaces via Policy Convolution](https://doi.org/10.1145/3589334.3645501)|Noveen Sachdeva, Lequn Wang, Dawen Liang, Nathan Kallus, Julian J. McAuley||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Off-Policy+Evaluation+for+Large+Action+Spaces+via+Policy+Convolution)|0|
|[LFDe: A Lighter, Faster and More Data-Efficient Pre-training Framework for Event Extraction](https://doi.org/10.1145/3589334.3645318)|Zhigang Kan, Liwen Peng, Yifu Gao, Ning Liu, Linbo Qiao, Dongsheng Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LFDe:+A+Lighter,+Faster+and+More+Data-Efficient+Pre-training+Framework+for+Event+Extraction)|0|
|[Multi-Scenario Pricing for Hotel Revenue Management](https://doi.org/10.1145/3589334.3645350)|Wendong Xiao, Shuqi Zhang, Zhiyi Huang, Yao Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Scenario+Pricing+for+Hotel+Revenue+Management)|0|
|[Collaboration-Aware Hybrid Learning for Knowledge Development Prediction](https://doi.org/10.1145/3589334.3645326)|Liyi Chen, Chuan Qin, Ying Sun, Xin Song, Tong Xu, Hengshu Zhu, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaboration-Aware+Hybrid+Learning+for+Knowledge+Development+Prediction)|0|
|[Span-Pair Interaction and Tagging for Dialogue-Level Aspect-Based Sentiment Quadruple Analysis](https://doi.org/10.1145/3589334.3645355)|Changzhi Zhou, Zhijing Wu, Dandan Song, Linmei Hu, Yuhang Tian, Jing Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Span-Pair+Interaction+and+Tagging+for+Dialogue-Level+Aspect-Based+Sentiment+Quadruple+Analysis)|0|
|[UrbanCLIP: Learning Text-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining from the Web](https://doi.org/10.1145/3589334.3645378)|Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen, Roger Zimmermann, Yuxuan Liang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UrbanCLIP:+Learning+Text-enhanced+Urban+Region+Profiling+with+Contrastive+Language-Image+Pretraining+from+the+Web)|0|
|[MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection](https://doi.org/10.1145/3589334.3645385)|Yupeng Li, Haorui He, Jin Bai, Dacheng Wen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MCFEND:+A+Multi-source+Benchmark+Dataset+for+Chinese+Fake+News+Detection)|0|
|[LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty](https://doi.org/10.1145/3589334.3645414)|Zhen Zhang, Yuhua Zhao, Hang Gao, Mengting Hu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LinkNER:+Linking+Local+Named+Entity+Recognition+Models+to+Large+Language+Models+using+Uncertainty)|0|
|[RicciNet: Deep Clustering via A Riemannian Generative Model](https://doi.org/10.1145/3589334.3645428)|Li Sun, Jingbin Hu, Suyang Zhou, Zhenhao Huang, Junda Ye, Hao Peng, Zhengtao Yu, Philip S. Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RicciNet:+Deep+Clustering+via+A+Riemannian+Generative+Model)|0|
|[Weakly Supervised Anomaly Detection via Knowledge-Data Alignment](https://doi.org/10.1145/3589334.3645429)|Haihong Zhao, Chenyi Zi, Yang Liu, Chen Zhang, Yan Zhou, Jia Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weakly+Supervised+Anomaly+Detection+via+Knowledge-Data+Alignment)|0|
|[MULAN: Multi-modal Causal Structure Learning and Root Cause Analysis for Microservice Systems](https://doi.org/10.1145/3589334.3645442)|Lecheng Zheng, Zhengzhang Chen, Jingrui He, Haifeng Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MULAN:+Multi-modal+Causal+Structure+Learning+and+Root+Cause+Analysis+for+Microservice+Systems)|0|
|[Dynamic Multi-Network Mining of Tensor Time Series](https://doi.org/10.1145/3589334.3645461)|Kohei Obata, Koki Kawabata, Yasuko Matsubara, Yasushi Sakurai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Multi-Network+Mining+of+Tensor+Time+Series)|0|
|[MSynFD: Multi-hop Syntax Aware Fake News Detection](https://doi.org/10.1145/3589334.3645468)|Liang Xiao, Qi Zhang, Chongyang Shi, Shoujin Wang, Usman Naseem, Liang Hu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MSynFD:+Multi-hop+Syntax+Aware+Fake+News+Detection)|0|
|[LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Time Series Anomaly Detection](https://doi.org/10.1145/3589334.3645472)|Feiyi Chen, Zhen Qin, Mengchu Zhou, Yingying Zhang, Shuiguang Deng, Lunting Fan, Guansong Pang, Qingsong Wen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LARA:+A+Light+and+Anti-overfitting+Retraining+Approach+for+Unsupervised+Time+Series+Anomaly+Detection)|0|
|[Markovletics: Methods and A Novel Application for Learning Continuous-Time Markov Chain Mixtures](https://doi.org/10.1145/3589334.3645491)|Fabian Spaeh, Charalampos E. Tsourakakis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Markovletics:+Methods+and+A+Novel+Application+for+Learning+Continuous-Time+Markov+Chain+Mixtures)|0|
|[NAT4AT: Using Non-Autoregressive Translation Makes Autoregressive Translation Faster and Better](https://doi.org/10.1145/3589334.3645527)|Huanran Zheng, Wei Zhu, Xiaoling Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NAT4AT:+Using+Non-Autoregressive+Translation+Makes+Autoregressive+Translation+Faster+and+Better)|0|
|[Breaking the Time-Frequency Granularity Discrepancy in Time-Series Anomaly Detection](https://doi.org/10.1145/3589334.3645556)|Youngeun Nam, Susik Yoon, Yooju Shin, Minyoung Bae, Hwanjun Song, JaeGil Lee, Byung Suk Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Breaking+the+Time-Frequency+Granularity+Discrepancy+in+Time-Series+Anomaly+Detection)|0|
|[Question Difficulty Consistent Knowledge Tracing](https://doi.org/10.1145/3589334.3645582)|Guimei Liu, Huijing Zhan, Jungjae Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Question+Difficulty+Consistent+Knowledge+Tracing)|0|
|[A Simple but Effective Approach for Unsupervised Few-Shot Graph Classification](https://doi.org/10.1145/3589334.3645587)|Yonghao Liu, Lan Huang, Bowen Cao, Ximing Li, Fausto Giunchiglia, Xiaoyue Feng, Renchu Guan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Simple+but+Effective+Approach+for+Unsupervised+Few-Shot+Graph+Classification)|0|
|[Inductive Cognitive Diagnosis for Fast Student Learning in Web-Based Intelligent Education Systems](https://doi.org/10.1145/3589334.3645589)|Shuo Liu, Junhao Shen, Hong Qian, Aimin Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Inductive+Cognitive+Diagnosis+for+Fast+Student+Learning+in+Web-Based+Intelligent+Education+Systems)|0|
|[RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules](https://doi.org/10.1145/3589334.3645602)|Miaomiao Li, Jiaqi Zhu, Yang Wang, Yi Yang, Yilin Li, Hongan Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RulePrompt:+Weakly+Supervised+Text+Classification+with+Prompting+PLMs+and+Self-Iterative+Logical+Rules)|0|
|[Multimodal Relation Extraction via a Mixture of Hierarchical Visual Context Learners](https://doi.org/10.1145/3589334.3645603)|Xiyang Liu, Chunming Hu, Richong Zhang, Kai Sun, Samuel Mensah, Yongyi Mao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Relation+Extraction+via+a+Mixture+of+Hierarchical+Visual+Context+Learners)|0|
|[Diagrammatic Reasoning for ALC Visualization with Logic Graphs](https://doi.org/10.1145/3589334.3645607)|Ildar Baimuratov||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diagrammatic+Reasoning+for+ALC+Visualization+with+Logic+Graphs)|0|
|[Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models](https://doi.org/10.1145/3589334.3645611)|Kelvin J. L. Koa, Yunshan Ma, Ritchie Ng, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Generate+Explainable+Stock+Predictions+using+Self-Reflective+Large+Language+Models)|0|
|[High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text Attributed Graphs](https://doi.org/10.1145/3589334.3645614)|Peiyan Zhang, Chaozhuo Li, Liying Kang, Feiran Huang, Senzhang Wang, Xing Xie, Sunghun Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=High-Frequency-aware+Hierarchical+Contrastive+Selective+Coding+for+Representation+Learning+on+Text+Attributed+Graphs)|0|
|[Distributed Data Placement and Content Delivery in Web Caches with Non-Metric Access Costs](https://doi.org/10.1145/3589334.3645654)|||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributed+Data+Placement+and+Content+Delivery+in+Web+Caches+with+Non-Metric+Access+Costs)|0|
|[DualCL: Principled Supervised Contrastive Learning as Mutual Information Maximization for Text Classification](https://doi.org/10.1145/3589334.3645668)|Junfan Chen, Richong Zhang, Yaowei Zheng, Qianben Chen, Chunming Hu, Yongyi Mao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DualCL:+Principled+Supervised+Contrastive+Learning+as+Mutual+Information+Maximization+for+Text+Classification)|0|
|[Graph Anomaly Detection with Bi-level Optimization](https://doi.org/10.1145/3589334.3645673)|Yuan Gao, Junfeng Fang, Yongduo Sui, Yangyang Li, Xiang Wang, Huamin Feng, Yongdong Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Anomaly+Detection+with+Bi-level+Optimization)|0|
|[AN-Net: an Anti-Noise Network for Anonymous Traffic Classification](https://doi.org/10.1145/3589334.3645691)|Xianwen Deng, Yijun Wang, Zhi Xue||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AN-Net:+an+Anti-Noise+Network+for+Anonymous+Traffic+Classification)|0|
|[DenseFlow: Spotting Cryptocurrency Money Laundering in Ethereum Transaction Graphs](https://doi.org/10.1145/3589334.3645692)|Dan Lin, Jiajing Wu, Yunmei Yu, Qishuang Fu, Zibin Zheng, Changlin Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DenseFlow:+Spotting+Cryptocurrency+Money+Laundering+in+Ethereum+Transaction+Graphs)|0|
|[Towards Cross-Table Masked Pretraining for Web Data Mining](https://doi.org/10.1145/3589334.3645707)|Chao Ye, Guoshan Lu, Haobo Wang, Liyao Li, Sai Wu, Gang Chen, Junbo Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Cross-Table+Masked+Pretraining+for+Web+Data+Mining)|0|
|[Beyond Labels and Topics: Discovering Causal Relationships in Neural Topic Modeling](https://doi.org/10.1145/3589334.3645715)|YiKun Tang, Heyan Huang, Xuewen Shi, XianLing Mao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beyond+Labels+and+Topics:+Discovering+Causal+Relationships+in+Neural+Topic+Modeling)|0|
|[HD-KT: Advancing Robust Knowledge Tracing via Anomalous Learning Interaction Detection](https://doi.org/10.1145/3589334.3645718)|Haiping Ma, Yong Yang, Chuan Qin, Xiaoshan Yu, Shangshang Yang, Xingyi Zhang, Hengshu Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HD-KT:+Advancing+Robust+Knowledge+Tracing+via+Anomalous+Learning+Interaction+Detection)|0|
|[MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models](https://doi.org/10.1145/3589334.3648137)|Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, Sophia Ananiadou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MentaLLaMA:+Interpretable+Mental+Health+Analysis+on+Social+Media+with+Large+Language+Models)|0|
|[Message Injection Attack on Rumor Detection under the Black-Box Evasion Setting Using Large Language Model](https://doi.org/10.1145/3589334.3648139)|Yifeng Luo, Yupeng Li, Dacheng Wen, Liang Lan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Message+Injection+Attack+on+Rumor+Detection+under+the+Black-Box+Evasion+Setting+Using+Large+Language+Model)|0|
|[Human vs ChatGPT: Effect of Data Annotation in Interpretable Crisis-Related Microblog Classification](https://doi.org/10.1145/3589334.3648141)|Thi Huyen Nguyen, Koustav Rudra||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Human+vs+ChatGPT:+Effect+of+Data+Annotation+in+Interpretable+Crisis-Related+Microblog+Classification)|0|
|[Contrastive Learning for Multimodal Classification of Crisis related Tweets](https://doi.org/10.1145/3589334.3648143)|Bishwas Mandal, Sarthak Khanal, Doina Caragea||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Learning+for+Multimodal+Classification+of+Crisis+related+Tweets)|0|
|[Modularized Networks for Few-shot Hateful Meme Detection](https://doi.org/10.1145/3589334.3648145)|Rui Cao, Roy KaWei Lee, Jing Jiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modularized+Networks+for+Few-shot+Hateful+Meme+Detection)|0|
|[CapAlign: Improving Cross Modal Alignment via Informative Captioning for Harmful Meme Detection](https://doi.org/10.1145/3589334.3648146)|Junhui Ji, Xuanrui Lin, Usman Naseem||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CapAlign:+Improving+Cross+Modal+Alignment+via+Informative+Captioning+for+Harmful+Meme+Detection)|0|
|[Unveiling Climate Drivers via Feature Importance Shift Analysis in New Zealand](https://doi.org/10.1145/3589334.3648147)|Bowen Chen, Gillian Dobbie, Neelesh Rampal, Yun Sing Koh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+Climate+Drivers+via+Feature+Importance+Shift+Analysis+in+New+Zealand)|0|
|[Causal Graph ODE: Continuous Treatment Effect Modeling in Multi-agent Dynamical Systems](https://doi.org/10.1145/3589334.3648148)|Zijie Huang, Jeehyun Hwang, Junkai Zhang, Jinwoo Baik, Weitong Zhang, Dominik Wodarz, Yizhou Sun, Quanquan Gu, Wei Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Graph+ODE:+Continuous+Treatment+Effect+Modeling+in+Multi-agent+Dynamical+Systems)|0|
|[SceneDAPR: A Scene-Level Free-Hand Drawing Dataset for Web-based Psychological Drawing Assessment](https://doi.org/10.1145/3589334.3648150)|Jiwon Kang, Jiwon Kim, Migyeong Yang, Chaehee Park, Taeeun Kim, Hayeon Song, Jinyoung Han||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SceneDAPR:+A+Scene-Level+Free-Hand+Drawing+Dataset+for+Web-based+Psychological+Drawing+Assessment)|0|
|[MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation](https://doi.org/10.1145/3589334.3648151)|Han Wang, Roy KaWei Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MemeCraft:+Contextual+and+Stance-Driven+Multimodal+Meme+Generation)|0|
|[Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response](https://doi.org/10.1145/3589334.3648153)|Md Towhidul Absar Chowdhury, Soumyajit Datta, Naveen Sharma, Ashiqur R. KhudaBukhsh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Infrastructure+Ombudsman:+Mining+Future+Failure+Concerns+from+Structural+Disaster+Response)|0|
|[Predicting and Presenting Task Difficulty for Crowdsourcing Food Rescue Platforms](https://doi.org/10.1145/3589334.3648155)|Zheyuan Ryan Shi, Jiayin Zhi, Siqi Zeng, Zhicheng Zhang, Ameesh Kapoor, Sean Hudson, Hong Shen, Fei Fang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Predicting+and+Presenting+Task+Difficulty+for+Crowdsourcing+Food+Rescue+Platforms)|0|
|[GraphLeak: Patient Record Leakage through Gradients with Knowledge Graph](https://doi.org/10.1145/3589334.3648157)|Xi Sheryl Zhang, Weifan Guan, Jiahao Lu, Zhaopeng Qiu, Jian Cheng, Xian Wu, Yefeng Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphLeak:+Patient+Record+Leakage+through+Gradients+with+Knowledge+Graph)|0|
