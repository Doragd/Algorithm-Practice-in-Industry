# WWW2024 Paper List

|论文|作者|组织|摘要|翻译|代码|引用数|
|---|---|---|---|---|---|---|
|[Unmasking the Web of Deceit: Uncovering Coordinated Activity to Expose Information Operations on Twitter](https://doi.org/10.1145/3589334.3645529)|Luca Luceri, Valeria Pantè, Keith Burghardt, Emilio Ferrara||Social media platforms, particularly Twitter, have become pivotal arenas for influence campaigns, often orchestrated by state-sponsored information operations (IOs). This paper delves into the detection of key players driving IOs by employing similarity graphs constructed from behavioral pattern data. We unveil that well-known, yet underutilized network properties can help accurately identify coordinated IO drivers. Drawing from a comprehensive dataset of 49 million tweets from six countries, which includes multiple verified IOs, our study reveals that traditional network filtering techniques do not consistently pinpoint IO drivers across campaigns. We first propose a framework based on node pruning that emerges superior, particularly when combining multiple behavioral indicators across different networks. Then, we introduce a supervised machine learning model that harnesses a vector representation of the fused similarity network. This model, which boasts a precision exceeding 0.95, adeptly classifies IO drivers on a global scale and reliably forecasts their temporal engagements. Our findings are crucial in the fight against deceptive influence campaigns on social media, helping us better understand and detect them.|社交媒体平台，尤其是 Twitter，已经成为影响力运动的关键舞台，这些运动通常由国家资助的信息运作(IOs)策划。本文利用由行为模式数据构造的相似度图，对操纵 IOs 的关键人物进行了检测。我们发现，众所周知的，但未充分利用的网络属性可以帮助准确识别协调的 IO 驱动程序。我们的研究从来自六个国家的4900万条推文的综合数据集中，其中包括多个经过验证的 IO，我们的研究显示传统的网络过滤技术并不能一致地确定不同活动的 IO 驱动因素。我们首先提出了一个基于节点剪枝的框架，它显示出优越性，特别是当跨不同网络结合多个行为指标时。然后，我们引入一个监督式学习模型，利用融合相似网络的向量表示。该模型具有超过0.95的精度，能够在全球范围内对 IO 驱动程序进行分类，并可靠地预测它们的时间参与。我们的发现对于打击社交媒体上的欺骗性影响运动至关重要，有助于我们更好地理解和发现它们。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unmasking+the+Web+of+Deceit:+Uncovering+Coordinated+Activity+to+Expose+Information+Operations+on+Twitter)|2|
|[Blockchain Censorship](https://doi.org/10.1145/3589334.3645431)|Anton Wahrstätter, Jens Ernstberger, Aviv Yaish, Liyi Zhou, Kaihua Qin, Taro Tsuchiya, Sebastian Steinhorst, Davor Svetinovic, Nicolas Christin, Mikolaj Barczentewicz, Arthur Gervais||Permissionless blockchains promise to be resilient against censorship by a single entity. This suggests that deterministic rules, and not third-party actors, are responsible for deciding if a transaction is appended to the blockchain or not. In 2022, the U.S. Office of Foreign Assets Control (OFAC) sanctioned a Bitcoin mixer and an Ethereum application, putting the neutrality of permissionless blockchains to the test. In this paper, we formalize quantify and analyze the security impact of blockchain censorship. We start by defining censorship, followed by a quantitative assessment of current censorship practices. We find that 46% of Ethereum blocks were made by censoring actors that intend to comply with OFAC sanctions, indicating the significant impact of OFAC sanctions on the neutrality of public blockchains. We further uncover that censorship not only impacts neutrality, but also security. We show how after Ethereum's move to Proof-of-Stake (PoS) and adoption of Proposer-Builder Separation (PBS) the inclusion of censored transactions was delayed by an average of 85%. Inclusion delays compromise a transaction's security by, e.g., strengthening a sandwich adversary. Finally we prove a fundamental limitation of PoS and Proof-of-Work (PoW) protocols against censorship resilience.|无许可的区块链承诺对单一实体的审查具有弹性。这表明确定性规则，而不是第三方参与者，负责决定事务是否被附加到区块链。2022年，美国外国资产管制办公室(OFAC)批准了一个比特币混合器和一个以太网应用程序，对无许可区块链的中立性进行了测试。本文对区块链审查的安全影响进行了形式化的量化和分析。我们首先定义审查制度，然后对当前的审查做法进行定量评估。我们发现，46% 的以太坊区块是通过审查打算遵守外国资产管制处制裁措施的行为者制造的，这表明外国资产管制处的制裁对公共区块链的中立性产生了重大影响。我们进一步发现，审查制度不仅影响中立，而且影响安全。我们展示了在 Etherum 转移到 Proof-of-Stake (PoS)和采用 Proposer-Builder 隔离(PBS)之后，包含审查交易的时间平均延迟了85% 。包含延迟通过加强三明治对手等方式损害事务的安全性。最后，我们证明了 PoS 和 Proof-of-Work (PoW)协议对抗审查弹性的根本局限性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Blockchain+Censorship)|2|
|[Fast Graph Condensation with Structure-based Neural Tangent Kernel](https://doi.org/10.1145/3589334.3645694)|Lin Wang, Wenqi Fan, Jiatong Li, Yao Ma, Qing Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Graph+Condensation+with+Structure-based+Neural+Tangent+Kernel)|2|
|[Susceptibility to Unreliable Information Sources: Swift Adoption with Minimal Exposure](https://doi.org/10.1145/3589334.3648154)|Jinyi Ye, Luca Luceri, Julie Jiang, Emilio Ferrara||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Susceptibility+to+Unreliable+Information+Sources:+Swift+Adoption+with+Minimal+Exposure)|2|
|[Personalized Audiobook Recommendations at Spotify Through Graph Neural Networks](https://doi.org/10.1145/3589335.3648339)|Marco De Nadai, Francesco Fabbri, Paul Gigioli, Alice Wang, Ang Li, Fabrizio Silvestri, Laura Kim, Shawn Lin, Vladan Radosavljevic, Sandeep Ghael, David Nyhan, Hugues Bouchard, Mounia Lalmas, Andreas Damianou||In the ever-evolving digital audio landscape, Spotify, well-known for its music and talk content, has recently introduced audiobooks to its vast user base. While promising, this move presents significant challenges for personalized recommendations. Unlike music and podcasts, audiobooks, initially available for a fee, cannot be easily skimmed before purchase, posing higher stakes for the relevance of recommendations. Furthermore, introducing a new content type into an existing platform confronts extreme data sparsity, as most users are unfamiliar with this new content type. Lastly, recommending content to millions of users requires the model to react fast and be scalable. To address these challenges, we leverage podcast and music user preferences and introduce 2T-HGNN, a scalable recommendation system comprising Heterogeneous Graph Neural Networks (HGNNs) and a Two Tower (2T) model. This novel approach uncovers nuanced item relationships while ensuring low latency and complexity. We decouple users from the HGNN graph and propose an innovative multi-link neighbor sampler. These choices, together with the 2T component, significantly reduce the complexity of the HGNN model. Empirical evaluations involving millions of users show significant improvement in the quality of personalized recommendations, resulting in a +46 a +23 beyond audiobooks, benefiting established products like podcasts.|在不断发展的数字音频领域，以音乐和谈话内容闻名的 Spotify 最近向其庞大的用户群推出了有声读物。尽管前景看好，但这一举措对个性化推荐提出了重大挑战。与音乐和播客不同，有声书籍最初是收费的，不能在购买前轻易浏览，这对推荐的相关性构成了更大的威胁。此外，在现有平台中引入新的内容类型会面临数据极度稀缺的问题，因为大多数用户不熟悉这种新的内容类型。最后，向数百万用户推荐内容需要模型快速反应和可伸缩性。为了应对这些挑战，我们利用播客和音乐用户的偏好，引入了2T-HGNN，一个由异构图神经网络(HGNN)和双塔(2T)模型组成的可扩展推荐系统。这种新颖的方法揭示了细微的项目关系，同时确保低延迟和复杂性。我们从 HGNN 图解耦用户，并提出了一个创新的多链路邻居采样器。这些选择以及2T 组件显著降低了 HGNN 模型的复杂性。涉及数百万用户的经验性评价显示，个性化推荐的质量有了显著提高，在有声读物之外产生了 + 46 a + 23，使播客等既有产品受益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Audiobook+Recommendations+at+Spotify+Through+Graph+Neural+Networks)|1|
|[Intelligent Model Update Strategy for Sequential Recommendation](https://doi.org/10.1145/3589334.3645316)|Zheqi Lv, Wenqiao Zhang, Zhengyu Chen, Shengyu Zhang, Kun Kuang||Modern online platforms are increasingly employing recommendation systems to address information overload and improve user engagement. There is an evolving paradigm in this research field that recommendation network learning occurs both on the cloud and on edges with knowledge transfer in between (i.e., edge-cloud collaboration). Recent works push this filed further by enabling edge-specific context-aware adaptivity, where model parameters are updated in real-time based on incoming on-edge data. However, we argue that frequent data exchanges between the cloud and edges often lead to inefficiency and waste of communication/computation resources, as considerable parameter updates might be redundant. To investigate this problem, we introduce Intelligent Edge-Cloud Parameter Request Model (IntellectReq). IntellectReq is designed to operate on edge, evaluating the cost-benefit landscape of parameter requests with minimal computation and communication overhead. We formulate this as a novel learning task, aimed at the detection of out-of-distribution data, thereby fine-tuning adaptive communication strategies. Further, we employ statistical mapping techniques to convert real-time user behavior into a normal distribution, thereby employing multi-sample outputs to quantify the model's uncertainty and thus its generalization capabilities. Rigorous empirical validation on four widely-adopted benchmarks evaluates our approach, evidencing a marked improvement in the efficiency and generalizability of edge-cloud collaborative and dynamic recommendation systems.|现代在线平台越来越多地使用推荐系统来解决信息超载问题，提高用户参与度。在这个研究领域有一个不断发展的范例，即推荐网络学习既发生在云端，也发生在知识转移之间(即，边缘云协作)的边缘。最近的工作通过启用特定于边缘的上下文感知自适应性进一步推动了这一领域的发展，其中模型参数根据传入的边缘数据实时更新。然而，我们认为云和边缘之间频繁的数据交换通常会导致效率低下和通信/计算资源的浪费，因为大量的参数更新可能是多余的。为了研究这个问题，我们引入了智能边缘云参数请求模型(IntelligectReq)。IntelectReq 设计用于边缘操作，以最小的计算和通信开销评估参数请求的成本效益。我们提出了一个新的学习任务，旨在检测分布外的数据，从而微调自适应通信策略。进一步，我们使用统计映射技术将实时用户行为转换为正态分布，从而使用多样本输出量化模型的不确定性，从而其泛化能力。对四个被广泛采用的基准测试的严格的经验验证评估了我们的方法，证明了边缘云协作和动态推荐系统的效率和通用性的显著提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Intelligent+Model+Update+Strategy+for+Sequential+Recommendation)|1|
|[Top-Personalized-K Recommendation](https://doi.org/10.1145/3589334.3645417)|Wonbin Kweon, SeongKu Kang, Sanghwan Jang, Hwanjo Yu||The conventional top-K recommendation, which presents the top-K items with the highest ranking scores, is a common practice for generating personalized ranking lists. However, is this fixed-size top-K recommendation the optimal approach for every user's satisfaction? Not necessarily. We point out that providing fixed-size recommendations without taking into account user utility can be suboptimal, as it may unavoidably include irrelevant items or limit the exposure to relevant ones. To address this issue, we introduce Top-Personalized-K Recommendation, a new recommendation task aimed at generating a personalized-sized ranking list to maximize individual user satisfaction. As a solution to the proposed task, we develop a model-agnostic framework named PerK. PerK estimates the expected user utility by leveraging calibrated interaction probabilities, subsequently selecting the recommendation size that maximizes this expected utility. Through extensive experiments on real-world datasets, we demonstrate the superiority of PerK in Top-Personalized-K recommendation task. We expect that Top-Personalized-K recommendation has the potential to offer enhanced solutions for various real-world recommendation scenarios, based on its great compatibility with existing models.|传统的 top-K 推荐是生成个性化排名列表的一种常见实践，它给出排名最高的 top-K 项目。然而，这个固定大小的 top-K 推荐是否是每个用户满意度的最佳方法？不一定。我们指出，在不考虑用户效用的情况下提供固定大小的推荐可能是次优的，因为它可能不可避免地包括不相关的项目或限制相关项目的曝光。为了解决这个问题，我们引入了 Top-Personalization-K 推荐，这是一个新的推荐任务，旨在产生一个个性化大小的排名列表，以最大限度地提高个人用户的满意度。作为提出的任务的解决方案，我们开发了一个名为 PerK 的模型无关框架。PerK 通过利用校准的交互概率来估计预期的用户效用，随后选择最大化这个预期效用的推荐大小。通过对实际数据集的大量实验，验证了 PerK 在 Top-Personalization-K 推荐任务中的优越性。我们期望 Top-Personalization-K 推荐基于其与现有模型的巨大兼容性，有潜力为各种真实世界的推荐场景提供增强的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Top-Personalized-K+Recommendation)|1|
|[AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems](https://doi.org/10.1145/3589334.3645537)|Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian J. McAuley, Wayne Xin Zhao, Leyu Lin, JiRong Wen||Recently, there has been an emergence of employing LLM-powered agents as believable human proxies, based on their remarkable decision-making capability. However, existing studies mainly focus on simulating human dialogue. Human non-verbal behaviors, such as item clicking in recommender systems, although implicitly exhibiting user preferences and could enhance the modeling of users, have not been deeply explored. The main reasons lie in the gap between language modeling and behavior modeling, as well as the incomprehension of LLMs about user-item relations. To address this issue, we propose AgentCF for simulating user-item interactions in recommender systems through agent-based collaborative filtering. We creatively consider not only users but also items as agents, and develop a collaborative learning approach that optimizes both kinds of agents together. Specifically, at each time step, we first prompt the user and item agents to interact autonomously. Then, based on the disparities between the agents' decisions and real-world interaction records, user and item agents are prompted to reflect on and adjust the misleading simulations collaboratively, thereby modeling their two-sided relations. The optimized agents can also propagate their preferences to other agents in subsequent interactions, implicitly capturing the collaborative filtering idea. Overall, the optimized agents exhibit diverse interaction behaviors within our framework, including user-item, user-user, item-item, and collective interactions. The results show that these agents can demonstrate personalized behaviors akin to those of real-world individuals, sparking the development of next-generation user behavior simulation.|近年来，基于 LLM 驱动的智能体显著的决策能力，出现了一种使用 LLM 驱动的智能体作为可信的人类代理的方法。然而，现有的研究主要集中在模拟人类对话方面。人类的非语言行为，如推荐系统中的项目点击，虽然隐含地表现出用户的偏好，可以增强用户的建模，但还没有被深入探索。其主要原因在于语言建模与行为建模之间的差距，以及对用户-项目关系的 LLM 模型的不理解。为了解决这个问题，我们建议 AgentCF 通过基于代理的协同过滤来模拟推荐系统中的用户-项目交互。我们不仅创造性地考虑用户，而且还将物品作为代理，并开发一种合作学习的方法，将这两种代理结合起来优化。具体来说，在每个步骤中，我们首先提示用户和项代理自主交互。然后，基于智能体的决策与实际交互记录之间的差异，提示用户和项目智能体协同反思和调整误导性模拟，从而建立它们之间的双边关系模型。经过优化的代理还可以在随后的交互中将自己的偏好传播给其他代理，从而隐含地捕捉到协同过滤的想法。总的来说，优化的代理在我们的框架中展示了不同的交互行为，包括用户-项目、用户-用户、项目-项目和集体交互。结果表明，这些代理能够表现出类似于真实世界个体的个性化行为，从而推动了下一代用户行为仿真的发展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AgentCF:+Collaborative+Learning+with+Autonomous+Language+Agents+for+Recommender+Systems)|1|
|[Aligning Language Models for Versatile Text-based Item Retrieval](https://doi.org/10.1145/3589335.3651468)|Yuxuan Lei, Jianxun Lian, Jing Yao, Mingqi Wu, Defu Lian, Xing Xie||This paper addresses the gap between general-purpose text embeddings and the specific demands of item retrieval tasks. We demonstrate the shortcomings of existing models in capturing the nuances necessary for zero-shot performance on item retrieval tasks. To overcome these limitations, we propose generate in-domain dataset from ten tasks tailored to unlocking models' representation ability for item retrieval. Our empirical studies demonstrate that fine-tuning embedding models on the dataset leads to remarkable improvements in a variety of retrieval tasks. We also illustrate the practical application of our refined model in a conversational setting, where it enhances the capabilities of LLM-based Recommender Agents like Chat-Rec. Our code is available at https://github.com/microsoft/RecAI.|本文探讨了通用文本嵌入与项目检索任务的具体要求之间的差距。我们证明了现有模型在捕捉项目检索任务中零拍性能所必需的细微差别方面的不足。为了克服这些限制，我们提出从十个任务中生成域内数据集，以解锁项目检索模型的表示能力。我们的实证研究表明，在数据集上微调嵌入模型可以显著改善各种检索任务。我们还举例说明了我们的改进模型在会话环境中的实际应用，它增强了基于 LLM 的推荐代理(如 Chat-Rec)的功能。我们的代码可以在 https://github.com/microsoft/recai 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Aligning+Language+Models+for+Versatile+Text-based+Item+Retrieval)|1|
|[Macro Graph Neural Networks for Online Billion-Scale Recommender Systems](https://doi.org/10.1145/3589334.3645517)|Hao Chen, Yuanchen Bei, Qijie Shen, Yue Xu, Sheng Zhou, Wenbing Huang, Feiran Huang, Senzhang Wang, Xiao Huang||Predicting Click-Through Rate (CTR) in billion-scale recommender systems poses a long-standing challenge for Graph Neural Networks (GNNs) due to the overwhelming computational complexity involved in aggregating billions of neighbors. To tackle this, GNN-based CTR models usually sample hundreds of neighbors out of the billions to facilitate efficient online recommendations. However, sampling only a small portion of neighbors results in a severe sampling bias and the failure to encompass the full spectrum of user or item behavioral patterns. To address this challenge, we name the conventional user-item recommendation graph as "micro recommendation graph" and introduce a more suitable MAcro Recommendation Graph (MAG) for billion-scale recommendations. MAG resolves the computational complexity problems in the infrastructure by reducing the node count from billions to hundreds. Specifically, MAG groups micro nodes (users and items) with similar behavior patterns to form macro nodes. Subsequently, we introduce tailored Macro Graph Neural Networks (MacGNN) to aggregate information on a macro level and revise the embeddings of macro nodes. MacGNN has already served Taobao's homepage feed for two months, providing recommendations for over one billion users. Extensive offline experiments on three public benchmark datasets and an industrial dataset present that MacGNN significantly outperforms twelve CTR baselines while remaining computationally efficient. Besides, online A/B tests confirm MacGNN's superiority in billion-scale recommender systems.|在十亿规模的推荐系统中预测点进率(CTR)对于图形神经网络(GNN)来说是一个长期存在的挑战，因为聚合数十亿个邻居涉及到极其复杂的计算。为了解决这个问题，基于 GNN 的点击率模型通常从数十亿个邻居中抽样数百个，以促进有效的在线推荐。然而，只有一小部分邻居的抽样导致严重的抽样偏差和未能涵盖用户或项目的行为模式的全部范围。为了解决这个问题，我们将传统的用户项目推荐图命名为“微推荐图”，并为十亿级推荐引入了更合适的宏推荐图(MAG)。MAG 通过将节点数从几十亿减少到几百个，解决了基础设施中的计算复杂度问题。具体来说，MAG 将具有相似行为模式的微节点(用户和项)分组以形成宏节点。随后，我们引入量身定制的宏图神经网络(MacGNN) ，在宏观层面上聚集信息，并修正宏观节点的嵌入。MacGNN 已经为淘宝网服务了两个月，为超过十亿用户提供推荐。针对三个公共基准数据集和一个工业数据集进行的大量离线实验表明，MacGNN 在保持计算效率的同时，显著优于十二个点击率基准。此外，在线 A/B 测试证实了 MacGNN 在数十亿规模的推荐系统中的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Macro+Graph+Neural+Networks+for+Online+Billion-Scale+Recommender+Systems)|1|
|[Doubly Calibrated Estimator for Recommendation on Data Missing Not at Random](https://doi.org/10.1145/3589334.3645617)|Wonbin Kweon, Hwanjo Yu||Recommender systems often suffer from selection bias as users tend to rate their preferred items. The datasets collected under such conditions exhibit entries missing not at random and thus are not randomized-controlled trials representing the target population. To address this challenge, a doubly robust estimator and its enhanced variants have been proposed as they ensure unbiasedness when accurate imputed errors or predicted propensities are provided. However, we argue that existing estimators rely on miscalibrated imputed errors and propensity scores as they depend on rudimentary models for estimation. We provide theoretical insights into how miscalibrated imputation and propensity models may limit the effectiveness of doubly robust estimators and validate our theorems using real-world datasets. On this basis, we propose a Doubly Calibrated Estimator that involves the calibration of both the imputation and propensity models. To achieve this, we introduce calibration experts that consider different logit distributions across users. Moreover, we devise a tri-level joint learning framework, allowing the simultaneous optimization of calibration experts alongside prediction and imputation models. Through extensive experiments on real-world datasets, we demonstrate the superiority of the Doubly Calibrated Estimator in the context of debiased recommendation tasks.|推荐系统经常受到选择偏差的影响，因为用户倾向于对他们喜欢的项目进行评分。在这种条件下收集的数据集显示条目不是随机丢失的，因此不是代表目标人群的随机对照试验。为了应对这一挑战，提出了一种双稳健估计器及其增强变量，以确保在提供准确的估计误差或预测倾向时的无偏性。然而，我们认为，现有的估计依赖于错误校准估算误差和倾向得分，因为他们依赖于估计的基本模型。我们提供的理论见解，如何错误校准插补和倾向模型可以限制双鲁棒估计器的有效性，并验证我们的定理使用真实世界的数据集。在此基础上，我们提出了一个双校正估计，涉及校准的插补和倾向模型。为了实现这一点，我们介绍了校准专家，他们考虑了不同用户之间的 logit 分布。此外，我们设计了一个三级联合学习框架，使校准专家同时优化预测和插补模型。通过对实际数据集的大量实验，我们证明了双校正估计器在去偏推荐任务中的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Doubly+Calibrated+Estimator+for+Recommendation+on+Data+Missing+Not+at+Random)|1|
|[Item-side Fairness of Large Language Model-based Recommendation System](https://doi.org/10.1145/3589334.3648158)|Meng Jiang, Keqin Bao, Jizhi Zhang, Wenjie Wang, Zhengyi Yang, Fuli Feng, Xiangnan He||Recommendation systems for Web content distribution intricately connect to the information access and exposure opportunities for vulnerable populations. The emergence of Large Language Models-based Recommendation System (LRS) may introduce additional societal challenges to recommendation systems due to the inherent biases in Large Language Models (LLMs). From the perspective of item-side fairness, there remains a lack of comprehensive investigation into the item-side fairness of LRS given the unique characteristics of LRS compared to conventional recommendation systems. To bridge this gap, this study examines the property of LRS with respect to item-side fairness and reveals the influencing factors of both historical users' interactions and inherent semantic biases of LLMs, shedding light on the need to extend conventional item-side fairness methods for LRS. Towards this goal, we develop a concise and effective framework called IFairLRS to enhance the item-side fairness of an LRS. IFairLRS covers the main stages of building an LRS with specifically adapted strategies to calibrate the recommendations of LRS. We utilize IFairLRS to fine-tune LLaMA, a representative LLM, on MovieLens and Steam datasets, and observe significant item-side fairness improvements. The code can be found in https://github.com/JiangM-C/IFairLRS.git.|网络内容分发推荐系统与弱势群体的信息获取和接触机会错综复杂地联系在一起。基于大语言模型的推荐系统(LRS)的出现可能会给推荐系统带来额外的社会挑战，因为大语言模型(LLM)存在固有的偏差。由于推荐系统与传统推荐系统相比具有独特的特点，从项目侧公平的角度来看，对于推荐系统的项目侧公平性缺乏全面的研究。为了弥补这一差距，本研究考察了 LRS 在项目侧公平性方面的性质，揭示了历史用户交互和 LLM 固有语义偏差的影响因素，阐明了传统项目侧公平性方法在 LRS 中扩展的必要性。为了实现这一目标，我们开发了一个简洁有效的框架，称为 IFairLRS，以增强 LRS 项目的公平性。IFairLRS 涵盖了建立 LRS 的主要阶段，其中特别采用了校准 LRS 建议的策略。我们利用 IFairLRS 在 MovieLens 和 STEAM 数据集上对 LLaMA (一种代表性的 LLM)进行微调，并观察到显著的项目端公平性改进。密码可以在 https://github.com/jiangm-c/ifairlrs.git 中找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Item-side+Fairness+of+Large+Language+Model-based+Recommendation+System)|1|
|[Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems](https://doi.org/10.1145/3589335.3651905)|Anuja Tayal, Aman Tyagi||When interacting with Retrieval-Augmented Generation (RAG)-based conversational agents, the users must carefully craft their queries to be understood correctly. Yet, understanding the system's capabilities can be challenging for the users, leading to ambiguous questions that necessitate further clarification. This work aims to bridge the gap by developing a suggestion question generator. To generate suggestion questions, our approach involves utilizing dynamic context, which includes both dynamic few-shot examples and dynamically retrieved contexts. Through experiments, we show that the dynamic contexts approach can generate better suggestion questions as compared to other prompting approaches.|当与基于检索增强生成(RAG)的会话代理交互时，用户必须精心设计他们的查询才能被正确理解。然而，理解系统的功能对用户来说可能是一个挑战，导致需要进一步澄清的模棱两可的问题。本研究旨在通过开发一个建议问题产生器来弥补这一差距。为了产生建议问题，我们的方法涉及到利用动态上下文，其中包括动态少镜头示例和动态检索的上下文。通过实验，我们发现动态语境法比其他激励方法能够产生更好的建议问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Contexts+for+Generating+Suggestion+Questions+in+RAG+Based+Conversational+Systems)|1|
|[Linear-Time Graph Neural Networks for Scalable Recommendations](https://doi.org/10.1145/3589334.3645486)|Jiahao Zhang, Rui Xue, Wenqi Fan, Xin Xu, Qing Li, Jian Pei, Xiaorui Liu||In an era of information explosion, recommender systems are vital tools to deliver personalized recommendations for users. The key of recommender systems is to forecast users' future behaviors based on previous user-item interactions. Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging Graph Neural Networks (GNNs) to boost the prediction performance of recommender systems. Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale recommender systems due to their scalability advantages. Despite the existence of GNN-acceleration solutions, it remains an open question whether GNN-based recommender systems can scale as efficiently as classic MF and DNN methods. In this paper, we propose a Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender systems to achieve comparable scalability as classic MF approaches while maintaining GNNs' powerful expressiveness for superior prediction accuracy. Extensive experiments and ablation studies are presented to validate the effectiveness and scalability of the proposed algorithm. Our implementation based on PyTorch is available.|在信息爆炸的时代，推荐系统是为用户提供个性化推荐的重要工具。推荐系统的关键是根据用户之前的交互行为预测用户的未来行为。由于它们在用户交互数据中捕获高阶连接性的强大表达能力，近年来人们对利用图形神经网络(GNN)来提高推荐系统的预测性能越来越感兴趣。尽管如此，经典的矩阵分解(MF)和深度神经网络(DNN)方法仍然在现实世界的大规模推荐系统中发挥着重要作用，因为它们具有可扩展性的优势。尽管存在 GNN 加速解决方案，但是基于 GNN 的推荐系统能否像经典的 MF 和 DNN 方法那样有效地扩展仍然是一个悬而未决的问题。本文提出了一种线性时间图神经网络(LTGNN)来扩展基于 GNN 的推荐系统，以实现与经典 MF 方法相当的可扩展性，同时保持 GNN 的强大表达能力，获得更高的预测精度。通过大量的实验和烧蚀研究，验证了该算法的有效性和可扩展性。我们基于 PyTorch 的实现是可用的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Linear-Time+Graph+Neural+Networks+for+Scalable+Recommendations)|1|
|[Collaborate to Adapt: Source-Free Graph Domain Adaptation via Bi-directional Adaptation](https://doi.org/10.1145/3589334.3645507)|Zhen Zhang, Meihan Liu, Anhui Wang, Hongyang Chen, Zhao Li, Jiajun Bu, Bingsheng He||Unsupervised Graph Domain Adaptation (UGDA) has emerged as a practical solution to transfer knowledge from a label-rich source graph to a completely unlabelled target graph. However, most methods require a labelled source graph to provide supervision signals, which might not be accessible in the real-world settings due to regulations and privacy concerns. In this paper, we explore the scenario of source-free unsupervised graph domain adaptation, which tries to address the domain adaptation problem without accessing the labelled source graph. Specifically, we present a novel paradigm called GraphCTA, which performs model adaptation and graph adaptation collaboratively through a series of procedures: (1) conduct model adaptation based on node's neighborhood predictions in target graph considering both local and global information; (2) perform graph adaptation by updating graph structure and node attributes via neighborhood contrastive learning; and (3) the updated graph serves as an input to facilitate the subsequent iteration of model adaptation, thereby establishing a collaborative loop between model adaptation and graph adaptation. Comprehensive experiments are conducted on various public datasets. The experimental results demonstrate that our proposed model outperforms recent source-free baselines by large margins.|无监督图域适应(UGDA)已经成为一种实用的解决方案，用于将知识从一个标签丰富的源图转移到一个完全无标签的目标图。然而，大多数方法都需要一个带标签的源图来提供监控信号，由于法规和隐私问题，这些信号在现实环境中可能无法访问。本文探讨了无源无监督图域自适应的场景，该场景试图在不访问标记源图的情况下解决域自适应问题。具体来说，我们提出了一种新的模型适应和图形适应协同执行的范式称为 GraphCTA，通过一系列的程序: (1)进行模型适应的基础上，节点的邻域预测的目标图，考虑到局部和全局信息; (2)执行图适应更新的图结构和节点属性，通过邻域对比学习; (3)更新的图作为输入，以促进模型适应的后续迭代，从而建立一个协作环之间的模型适应和图适应。在各种公共数据集上进行了综合实验。实验结果表明，我们提出的模型优于最近的无源基线大幅度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaborate+to+Adapt:+Source-Free+Graph+Domain+Adaptation+via+Bi-directional+Adaptation)|1|
|[Assessing Web Fingerprinting Risk](https://doi.org/10.1145/3589335.3648322)|Enrico Bacis, Igor Bilogrevic, Róbert BusaFekete, Asanka Herath, Antonio Sartori, Umar Syed||Modern Web APIs allow developers to provide extensively customized experiences for website visitors, but the richness of the device information they provide also make them vulnerable to being abused to construct browser fingerprints, device-specific identifiers that enable covert tracking of users even when cookies are disabled. Previous research has established entropy, a measure of information, as the key metric for quantifying fingerprinting risk. However, earlier studies had two major limitations. First, their entropy estimates were based on either a single website or a very small sample of devices. Second, they did not adequately consider correlations among different Web APIs, potentially grossly overestimating their fingerprinting risk. We provide the first study of browser fingerprinting which addresses the limitations of prior work. Our study is based on actual visited pages and Web APIs reported by tens of millions of real Chrome browsers in-the-wild. We accounted for the dependencies and correlations among Web APIs, which is crucial for obtaining more realistic entropy estimates. We also developed a novel experimental design that accurately and efficiently estimates entropy while never observing too much information from any single user. Our results provide an understanding of the distribution of entropy for different website categories, confirm the utility of entropy as a fingerprinting proxy, and offer a method for evaluating browser enhancements which are intended to mitigate fingerprinting.|现代 Web API 允许开发人员为网站访问者提供广泛的定制体验，但是他们提供的丰富的设备信息也使他们很容易被滥用来构建浏览器指纹，即使在 Cookie 被禁用的情况下，设备特定的标识符也能够隐蔽地跟踪用户。先前的研究已经建立了熵，一种信息的度量，作为量化指纹风险的关键指标。然而，早期的研究有两个主要的局限性。首先，他们的熵估计是基于一个单一的网站或一个非常小的设备样本。其次，他们没有充分考虑不同 Web API 之间的相关性，可能严重高估了他们的指纹风险。我们提供了第一个研究的浏览器指纹，解决了以前的工作的局限性。我们的研究是基于实际访问的页面和 Web API 报告的数千万真正的 Chrome 浏览器在野外。我们考虑了 Web API 之间的依赖关系和相关性，这对于获得更真实的熵估计是至关重要的。我们还开发了一种新颖的实验设计，它能够准确有效地估计熵，同时从不观察来自任何单个用户的太多信息。我们的结果提供了一个不同网站类别的熵分布的理解，证实了熵作为指纹代理的效用，并提供了一种方法来评估浏览器增强，旨在减轻指纹。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Assessing+Web+Fingerprinting+Risk)|1|
|[Large Language Model Powered Agents in the Web](https://doi.org/10.1145/3589335.3641240)|Yang Deng, An Zhang, Yankai Lin, Xu Chen, JiRong Wen, TatSeng Chua||Web applications serve as vital interfaces for users to access information, perform various tasks, and engage with content. Traditional web designs have predominantly focused on user interfaces and static experiences. With the advent of large language models (LLMs), there's a paradigm shift as we integrate LLM-powered agents into these platforms. These agents bring forth crucial human capabilities like memory and planning to make them behave like humans in completing various tasks, effectively enhancing user engagement and offering tailored interactions in web applications. In this tutorial, we delve into the cutting-edge techniques of LLM-powered agents across various web applications, such as web mining, social networks, recommender systems, and conversational systems. We will also explore the prevailing challenges in seamlessly incorporating these agents and hint at prospective research avenues that can revolutionize the way we interact with web platforms.|Web 应用程序充当用户访问信息、执行各种任务和参与内容的重要接口。传统的网页设计主要侧重于用户界面和静态体验。随着大型语言模型(LLM)的出现，我们将 LLM 驱动的代理集成到这些平台中，这是一个范式转变。这些代理带来了至关重要的人类能力，如记忆和计划，使他们在完成各种任务时表现得像人类一样，有效地提高用户参与度，并在网络应用程序中提供量身定制的交互。在本教程中，我们将深入研究基于 LLM 的代理跨各种 Web 应用程序的尖端技术，例如 Web 挖掘、社交网络、推荐系统和会话系统。我们还将探讨在无缝合并这些代理方面的主要挑战，并暗示可以彻底改变我们与网络平台互动方式的前瞻性研究途径。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Model+Powered+Agents+in+the+Web)|1|
|[Automatic Design Summary Generation with Generative AI](https://doi.org/10.1145/3589335.3651901)|Daisuke Ikoma, Eisuke Aoki, Tomoki Taniguchi, Shinya Suzuki, Tomoko Ohkuma||In the private residential sales market, obtaining orders for exterior design requires a proposal considering the constraints of the location and customer. The design summary is a proposal document that describes concepts at the earliest stage of exterior design. It not only appeals to the customer, but also is referenced in the design drawing. However, the quality varies depending on the skill of the creator because the construction of a design summary requires the knowledge of human experts. This paper aims to generate the design summary using generative AI. Firstly, we analyze the characteristics of the design summary to identify the essential elements. Then, we propose a sequence of prompts to generate the design summary. Finally, we conducted a comparative evaluation between design summaries created by experts and those generated by generative AI.|在私人住宅销售市场中，获得外观设计的订单需要一个考虑到位置和客户约束的建议。设计摘要是在外观设计的最初阶段描述概念的提案文档。它不仅吸引客户，而且在设计图纸中引用。然而，由于设计概要的构建需要人类专家的知识，因此质量取决于创作者的技能。本文旨在利用生成式人工智能生成设计概要。首先，分析了设计总结的特点，确定了设计总结的基本要素。然后，我们提出了一系列的提示来生成设计总结。最后，对专家创建的设计概要和生成式人工智能创建的设计概要进行了比较评价。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Design+Summary+Generation+with+Generative+AI)|1|
|[Data Augmentation for Smishing Detection: A Theory-based Prompt Engineering Approach](https://doi.org/10.1145/3589335.3651903)|Ho Sung Shim, Hyoungjun Park, Kyuhan Lee, JangSun Park, Seonhye Kang||Smishing, which refers to social engineering attacks delivered through mobile devices such as smartphones, poses significant threats, yet limited data hinder the development of effective countermeasures. To tackle this, we propose a novel prompt engineering method for data augmentation in smishing detection. Distinguished by its utilization of insights from social science on smishing mechanisms, our approach offers a promising avenue for improving machine learning models in combating smishing attacks.|Smishing 指的是通过智能手机等移动设备发动的社会工程攻击，它构成了重大威胁，但有限的数据阻碍了有效对策的发展。针对这一问题，提出了一种新的快速工程方法用于抛光检测中的数据增强。我们的方法利用了社会科学关于破碎机制的见解，因此为改进机器学习模型以对抗破碎攻击提供了一个有希望的途径。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data+Augmentation+for+Smishing+Detection:+A+Theory-based+Prompt+Engineering+Approach)|1|
|[Prompt-Eng: Healthcare Prompt Engineering: Revolutionizing Healthcare Applications with Precision Prompts](https://doi.org/10.1145/3589335.3651904)|Awais Ahmed, Mengshu Hou, Rui Xi, Xiaoyang Zeng, Syed Attique Shah||Prompt Engineering has emerged as a pivotal technique in Natural Language Processing, providing a flexible approach for leveraging pre-trained language models. Particularly, a prompt is used to instruct the model to adopt the nature of given prompts, which became a well-adoptable approach in wide areas of domains. Yet, existing prompt-guided frameworks are experiencing various challenges, such as crafting prompts for specific tasks to achieve clarity and conciseness and avoid ambiguity, which requires time and computational resources. Further existing methods heavily rely on the extensive labelled datasets, yet many domain-specific challenges exist, particularly in healthcare. This study presents Prompt-Eng, a novel framework emphasizing its wide-ranging applications in healthcare, where we design precise prompts with positive and negative aspects; we hypothesize that designing prompts in pairs helps models to generalize effectively. We delve into the significance of quick design and optimization, highlighting its influence in shaping model responses. In addition, we explore the increasing demand for prompts that are aware of the context in multimodal data analysis and the incorporation of prompt engineering in new machine-learning approaches. The essence of our approach is in creating tailored prompts, which serve as instructive guidelines for the models during the prediction procedure. The proposed methodology emphasizes utilizing context-aware prompt pairs to facilitate interpreting and extracting healthcare information from a health corpus by models. The study uses the medical MIMIC-III \footnotehttps://physionet.org/content/mimiciii/1.4/ corpus to predict medicine prescriptions. The paper also explores visual and textual prompts for X-ray image analysis for pneumonia prediction on the MIMIC-CXR \footnote\urlhttps://physionet.org/content/mimic-cxr/2.0.0/ dataset. This approach stands out from existing methods by addressing challenges such as clarity, conciseness, and context awareness, thereby enabling improved interpretation and extraction of healthcare information from diverse data sources.|Prompt Engineering 已经成为自然语言处理中的一项关键技术，为利用预先训练好的语言模型提供了一种灵活的方法。特别是，提示符用于指示模型采用给定提示符的性质，这在广泛的领域中成为一种很好采用的方法。然而，现有的快速指导框架正在经历各种挑战，例如为具体任务设计提示，以实现清晰和简洁，避免含糊不清，这需要时间和计算资源。进一步的现有方法严重依赖于广泛的标记数据集，但是存在许多特定于领域的挑战，特别是在医疗领域。这项研究提出了 Prompt-Eng，一个新的框架，强调其在医疗保健领域的广泛应用，在这里我们设计具有积极和消极方面的精确提示; 我们假设成对设计提示有助于模型的有效推广。我们深入研究了快速设计和优化的意义，强调了它在塑造模型响应的影响。此外，我们还探讨了日益增长的对能够意识到多模式数据分析背景的提示的需求，以及在新的机器学习方法中纳入快速工程的问题。我们的方法的实质是创建量身定制的提示，这些提示在预测过程中为模型提供了指导性的指南。提出的方法强调利用上下文感知的提示对，以促进解释和提取卫生保健信息从健康语料库的模型。这项研究使用医学 MIMIC-III 脚注/https:// physionet.org/content/mimiciii/1.4/语料库来预测药物处方。本文还探讨了在 MIMIC-CXR 脚注 urlhttps:// physionet.org/content/MIMIC-CXR/2.0.0/数据集上用于肺炎预测的 X 射线图像分析的视觉和文本提示。这种方法从现有的方法中脱颖而出，解决了诸如清晰度、简洁性和上下文感知等挑战，从而能够改进对来自不同数据源的医疗保健信息的解释和提取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prompt-Eng:+Healthcare+Prompt+Engineering:+Revolutionizing+Healthcare+Applications+with+Precision+Prompts)|1|
|[Implementing Sustainable Urban Mobility Transitions in Positive Energy Districts](https://doi.org/10.1145/3589335.3651899)|Dirk Ahlers, Bjørn Ove Berthelsen, Tor Rune Skoglund, Kelly Riedesel||This paper examines Smart Cities transition with the focus on urban mobility. We demonstrate how a multi-stakeholder, multi-disciplinary approach can support integration of systems, data, people, and organisations with a case study on new integrated mobility solutions and urban decarbonisation.|本文以城市人口流动为研究重点，探讨智慧城市转型问题。我们通过一个关于新的综合流动性解决方案和城市脱碳的案例研究，展示了一个多利益相关者、多学科的方法如何支持系统、数据、人员和组织的整合。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Implementing+Sustainable+Urban+Mobility+Transitions+in+Positive+Energy+Districts)|1|
|[Contextualizing Internet Memes Across Social Media Platforms](https://doi.org/10.1145/3589335.3651970)|Saurav Joshi, Filip Ilievski, Luca Luceri||Internet memes have emerged as a novel format for communication and expressing ideas on the web. Their fluidity and creative nature are reflected in their widespread use, often across platforms and occasionally for unethical or harmful purposes. While computational work has already analyzed their high-level virality over time and developed specialized classifiers for hate speech detection, there have been no efforts to date that aim to holistically track, identify, and map internet memes posted on social media. To bridge this gap, we investigate whether internet memes across social media platforms can be contextualized by using a semantic repository of knowledge, namely, a knowledge graph. We collect thousands of potential internet meme posts from two social media platforms, namely Reddit and Discord, and perform an extract-transform-load procedure to create a data lake with candidate meme posts. By using vision transformer-based similarity, we match these candidates against the memes cataloged in a recently released knowledge graph of internet memes, IMKG. We provide evidence that memes published online can be identified by mapping them to IMKG. We leverage this grounding to study the prevalence of memes on different platforms, discover popular memes, and select common meme channels and subreddits. Finally, we illustrate how the grounding can enable users to get context about memes on social media thanks to their link to the knowledge graph.|网络文化基因已经作为一种新的形式出现在网络上，用于交流和表达思想。它们的流动性和创造性反映在它们的广泛使用上，通常是跨平台的，有时是出于不道德或有害的目的。虽然计算工作已经分析了它们随着时间的推移的高水平病毒性，并为仇恨言论检测开发了专门的分类器，但迄今为止还没有努力全面跟踪、识别和绘制发布在社交媒体上的互联网文化基因。为了弥补这一差距，我们研究了跨社交媒体平台的网络文化基因是否可以通过使用知识的语义库，即知识图表来进行语境化。我们从 Reddit 和 Discord 这两个社交媒体平台上收集了成千上万潜在的网络文化基因帖子，然后执行一个提取-转换-加载过程来创建一个包含候选文化基因帖子的数据湖。通过使用基于视觉变换的相似性，我们将这些候选模因与最近发布的网络模因知识图 IMKG 中的模因进行匹配。我们提供的证据表明，在线发表的文化基因可以通过映射到 IMKG 来识别。我们利用这个基础来研究不同平台上流行的文化基因，发现流行的文化基因，并选择常见的文化基因频道和转载。最后，我们举例说明了基础知识是如何通过用户与知识图表的链接，使他们能够在社交媒体上获得关于模因的上下文。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contextualizing+Internet+Memes+Across+Social+Media+Platforms)|1|
|[Faithful Temporal Question Answering over Heterogeneous Sources](https://doi.org/10.1145/3589334.3645547)|Zhen Jia, Philipp Christmann, Gerhard Weikum||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Faithful+Temporal+Question+Answering+over+Heterogeneous+Sources)|1|
|[From Shapes to Shapes: Inferring SHACL Shapes for Results of SPARQL CONSTRUCT Queries](https://doi.org/10.1145/3589334.3645550)|Philipp Seifer, Daniel Hernández, Ralf Lämmel, Steffen Staab||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Shapes+to+Shapes:+Inferring+SHACL+Shapes+for+Results+of+SPARQL+CONSTRUCT+Queries)|1|
|[A Multifaceted Look at Starlink Performance](https://doi.org/10.1145/3589334.3645328)|Nitinder Mohan, Andrew E. Ferguson, Hendrik Cech, Rohan Bose, Prakita Rayyan Renatin, Mahesh K. Marina, Jörg Ott||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Multifaceted+Look+at+Starlink+Performance)|1|
|[Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice](https://doi.org/10.1145/3589334.3645504)|Tanner Fiez, Houssam Nassif, YuCheng Chen, Sergio Gamez, Lalit Jain||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Best+of+Three+Worlds:+Adaptive+Experimentation+for+Digital+Marketing+in+Practice)|1|
|[PRINT: Personalized Relevance Incentive Network for CTR Prediction in Sponsored Search](https://doi.org/10.1145/3589335.3648316)|Zhaolin Hong, Haitao Wang, Chengjie Qian, Wei Chen, Tianqi He, Yajie Zou, Qiang Liu, Xingxing Wang||Click-Through Rate (CTR) prediction plays a critical role in sponsored search. Modeling the semantic relevance between queries and ads is one of the most crucial factors affecting the performance of CTR prediction. However, different users have different sensitivities to semantic relevance due to their personalized relevance preferences. Therefore, semantic relevance may have different incentives on the user's click probability (i.e., stimulative incentive, inhibitive incentive, or irrelevant incentive). Unfortunately, few works have studied the phenomenon, which ignores the complicated incentive effects of semantic relevance and limits the performance of CTR prediction. To this end, we propose a novel Personalized Relevance Incentive N eTwork (PRINT for short) to explicitly model the personalized incentives of query-ad semantic relevance on user's click probability. Specifically, we introduce a User Relevance Preference Module (usertask) to extract the user's personalized relevance preference from historical query-ad interacted sequence. Then, a RElevance Incentive Module (REIM) is designed to discern three incentive types and model the personalized incentive effects on CTR prediction. Experiments on public datasets and industrial datasets demonstrate the significant improvement of our PRINT. Furthermore, PRINT is also deployed in the sponsored search advertising system in Meituan, obtaining an improvement of 1.94% and 2.29% in CTR and Cost Per Mile (CPM) respectively. We publish the source code at https://anonymous.4open.science/r/PRINT-D365/.|点进率预测在赞助商搜索中扮演着重要的角色。查询与广告之间的语义相关性建模是影响 CTR 预测性能的关键因素之一。然而，不同的用户由于个性化的相关偏好，对语义相关的敏感性也不同。因此，语义相关性可能对用户的点击概率有不同的激励(即，刺激性激励，抑制性激励，或不相关的激励)。遗憾的是，很少有文献对这一现象进行研究，忽视了语义相关性的复杂激励效应，限制了 CTR 预测的效果。为此，我们提出了一种新的个性化关联激励网络(PRINT) ，以显式模型的查询广告语义相关性的个性化激励用户的点击概率。具体来说，我们引入了用户相关偏好模块(usertask) ，从历史查询-广告交互序列中提取用户的个性化相关偏好。然后，设计了相关激励模块(REIM) ，识别出三种激励类型，并建立了个性化激励效应模型。在公共数据集和工业数据集上的实验表明，我们的 PRINT 算法有了显著的改进。此外，PRINT 在赞助商搜索广告系统中的美团也有所提高，点击率和每英里成本分别提高了1.94% 和2.29% 。我们在 https://anonymous.4open.science/r/print-d365/公布源代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PRINT:+Personalized+Relevance+Incentive+Network+for+CTR+Prediction+in+Sponsored+Search)|0|
|[Dynamic Search Results Re-ranking Method by Advertisement Relevance Feedback based on Users' Unconscious Expectations for Listing Advertisement](https://doi.org/10.1145/3589335.3651495)|Da Li, Shigenaga Hamaguchi, Ruman Suyama, Shinsuke Nakajima, Yukiko Kawai||Search results hold paramount importance for both users and advertisers. However, re-ranking results based on the timing of user clicks on listing advertisements poses a considerable challenge. In this study, we introduce a dynamic re-ranking method to re-rank search results triggered by the viewing of listing advertisements containing search terms after the initial presentation of search results. The proposed method leverages relevance feedback to enhance search results, providing valuable support to users in their searches. We conducted a comparative verification of accuracy rates between methods with and without the added weight. The results suggest that considering the timings of clicking on listing ads for re-ranking results can more effectively reflect user interest compared to approaches that don't take this timing into account. This study contributes to the advancement of search result presentation strategies by incorporating dynamic user behavior considerations.|搜索结果对于用户和广告商来说都至关重要。然而，根据用户点击列表广告的时间进行重新排名的结果构成了相当大的挑战。在本研究中，我们引入一个动态重新排序的方法来重新排序搜索结果所触发的包含搜索词的列表广告在搜索结果的初始呈现之后。建议的方法利用关联反馈提升搜索结果，为用户的搜索提供有价值的支持。我们进行了比较验证的准确率之间的方法有和没有增加的权重。研究结果表明，与不考虑时间安排的方法相比，考虑点击列表广告的时间安排可以更有效地反映用户的兴趣。这项研究有助于推进搜索结果表示策略，纳入动态用户行为的考虑。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Search+Results+Re-ranking+Method+by+Advertisement+Relevance+Feedback+based+on+Users'+Unconscious+Expectations+for+Listing+Advertisement)|0|
|[Adversarial-Enhanced Causal Multi-Task Framework for Debiasing Post-Click Conversion Rate Estimation](https://doi.org/10.1145/3589334.3645379)|Xinyue Zhang, Cong Huang, Kun Zheng, Hongzu Su, Tianxu Ji, Wei Wang, Hongkai Qi, Jingjing Li||In real-world industrial scenarios, post-click conversion rate (CVR) prediction models are trained offline based on click events and subsequently applied online to both clicked and unclicked events. Unfortunately, unclicked events are inevitably difficult to estimate due to user self-selection, which leads to a degradation of CVR prediction accuracy. In order to estimate the prediction of unclicked events, the current mainstream Doubly Robust (DR) estimators introduce the concept of imputed errors. However, inaccuracies in imputed errors can increase the uncertainty in the generalization bound of CVR predictions, consequently resulting in a decline in the CVR prediction accuracy. To challenge this issue, we first present a theoretical analysis of the bias and variance inherent in DR estimators and then introduce a novel causal estimator that seeks to strike a balance between bias and variance within the DR framework, thus optimizing the learning of the imputation model in a more robust manner. Additionally, drawing inspiration from adversarial learning techniques, we propose a novel dual adversarial component, which learns from both the space level and the task level to eliminate the causal influence of input features on the CTR task (i.e., the click propensity), with the goal of achieving unbiased estimations. Our extensive experimental evaluations, conducted on both the widely used benchmark and the real-world large-scale Internet giant platform, convincingly demonstrate the effectiveness of our proposed scheme. Besides, we have released a high-quality industrial dataset named Tenc-UnionAds used for selection bias research in the advertising field.|在真实的工业场景中，点击后转换率(CVR)预测模型是基于点击事件离线训练的，随后在线应用于点击事件和未点击事件。不幸的是，由于用户的自我选择，未点击事件不可避免地难以估计，从而导致 CVR 预测精度的下降。为了估计未点击事件的预测，目前主流的双鲁棒(DR)估计器引入了估计误差的概念。然而，估算误差的不准确性会增加 CVR 预测推广范围的不确定性，从而导致 CVR 预测精度的下降。为了挑战这个问题，我们首先提出了 DR 估计量固有的偏差和方差的理论分析，然后介绍了一种新的因果估计量，试图在 DR 框架内平衡偏差和方差，从而以更健壮的方式优化插补模型的学习。此外，从对抗性学习技术中获得灵感，我们提出了一种新的双对抗性组件，它从空间水平和任务水平学习，以消除输入特征对 CTR 任务(即点击倾向)的因果影响，目标是实现无偏估计。我们在广泛使用的基准和现实世界的大规模互联网巨头平台上进行了广泛的实验评估，令人信服地证明了我们提出的方案的有效性。此外，我们还发布了一个名为 Tenc-UnionAds 的高质量工业数据集，用于广告领域的选择偏差研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adversarial-Enhanced+Causal+Multi-Task+Framework+for+Debiasing+Post-Click+Conversion+Rate+Estimation)|0|
|[Enhancing Sequential Recommendation via LLM-based Semantic Embedding Learning](https://doi.org/10.1145/3589335.3648307)|Jun Hu, Wenwen Xia, Xiaolu Zhang, Chilin Fu, Weichang Wu, Zhaoxin Huan, Ang Li, Zuoli Tang, Jun Zhou||Sequential recommendation systems (SRS) are crucial in various applications as they enable users to discover relevant items based on their past interactions. Recent advancements involving large language models (LLMs) have shown significant promise in addressing intricate recommendation challenges. However, these efforts exhibit certain limitations. Specifically, directly extracting representations from an LLM based on items' textual features and feeding them into a sequential model hold no guarantee that the semantic information of texts could be preserved in these representations. Additionally, concatenating textual descriptions of all items in an item sequence into a long text and feeding it into an LLM for recommendation results in lengthy token sequences, which largely diminishes the practical efficiency. In this paper, we introduce SAID, a framework that utilizes LLMs to explicitly learn Semantically Aligned item ID embeddings based on texts. For each item, SAID employs a projector module to transform an item ID into an embedding vector, which will be fed into an LLM to elicit the exact descriptive text tokens accompanied by the item. The item embeddings are forced to preserve fine-grained semantic information of textual descriptions. Further, the learned embeddings can be integrated with lightweight downstream sequential models for practical recommendations. In this way, SAID circumvents lengthy token sequences in previous works, reducing resources required in industrial scenarios and also achieving superior recommendation performance. Experiments on six public datasets demonstrate that SAID outperforms baselines by about 5% to 15% in terms of NDCG@10. Moreover, SAID has been deployed in Alipay's online advertising platform, achieving a 3.07% relative improvement of cost per mille (CPM) over baselines, with an online response time of under 20 milliseconds.|序列推荐系统(SRS)在各种应用程序中都是至关重要的，因为它们使用户能够根据过去的交互发现相关项目。涉及大型语言模型(LLM)的最新进展显示了在解决复杂的推荐挑战方面的巨大前景。然而，这些努力表现出一定的局限性。具体来说，根据项目的文本特征直接从 LLM 中提取表示并将其输入到一个顺序模型中，并不能保证文本的语义信息能够在这些表示中得到保留。此外，将一个项目序列中所有项目的文本描述连接到一个长文本中，并将其提供给 LLM 进行推荐，会导致冗长的标记序列，这大大降低了实际效率。在本文中，我们介绍了 SAID，一个利用 LLM 显式学习基于文本的语义对齐项 ID 嵌入的框架。对于每个条目，SAID 使用一个投影仪模块将条目 ID 转换为嵌入向量，嵌入向量将被输入到 LLM 中，以引出条目附带的精确描述性文本标记。项目嵌入被迫保留文本描述的细粒度语义信息。此外，学习嵌入可以与轻量级的下游顺序模型集成，以获得实用的推荐。通过这种方式，SAID 规避了以前工作中冗长的令牌序列，减少了工业场景中所需的资源，并且实现了更好的推荐性能。在六个公共数据集上的实验表明，按照 NDCG@10计算，SAID 的性能比基线高出约5% 至15% 。此外，SAID 已经部署在支付宝的在线广告平台上，实现了每公里成本(CPM)相对基线提高3.07% ，在线响应时间低于20毫秒。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Sequential+Recommendation+via+LLM-based+Semantic+Embedding+Learning)|0|
|[OmniSearchSage: Multi-Task Multi-Entity Embeddings for Pinterest Search](https://doi.org/10.1145/3589335.3648309)|Prabhat Agarwal, Minhazul Islam SK, Nikil Pancha, Kurchi Subhra Hazra, Jiajing Xu, Chuck Rosenberg||In this paper, we present OmniSearchSage, a versatile and scalable system for understanding search queries, pins, and products for Pinterest search. We jointly learn a unified query embedding coupled with pin and product embeddings, leading to an improvement of >8% relevance, >7% engagement, and >5% ads CTR in Pinterest's production search system. The main contributors to these gains are improved content understanding, better multi-task learning, and real-time serving. We enrich our entity representations using diverse text derived from image captions from a generative LLM, historical engagement, and user-curated boards. Our multitask learning setup produces a single search query embedding in the same space as pin and product embeddings and compatible with pre-existing pin and product embeddings. We show the value of each feature through ablation studies, and show the effectiveness of a unified model compared to standalone counterparts. Finally, we share how these embeddings have been deployed across the Pinterest search stack, from retrieval to ranking, scaling to serve 300k requests per second at low latency. Our implementation of this work is available at https://github.com/pinterest/atg-research/tree/main/omnisearchsage.|在本文中，我们介绍 OmniSearchSage，这是一个通用的、可扩展的系统，用于理解 Pinterest 搜索的查询、引脚和产品。我们共同学习了一个统一的查询嵌入结合引脚和产品嵌入，导致在 Pinterest 的产品搜索系统中相关性提高 > 8% ，参与度提高 > 7% ，广告点击率提高 > 5% 。这些收获的主要贡献者是改进的内容理解、更好的多任务学习和实时服务。我们丰富了我们的实体表示使用不同的文字衍生自一个生成 LLM 的图像标题，历史参与和用户策划的董事会。我们的多任务学习设置产生一个单一的搜索查询嵌入在同样的空间作为引脚和产品嵌入和兼容预先存在的引脚和产品嵌入。我们通过烧蚀研究显示了每个特征的价值，并且与独立的对应物相比，显示了统一模型的有效性。最后，我们将分享如何在 Pinterest 搜索堆栈中部署这些嵌入，从检索到排名，扩展到以低延迟每秒30万个请求。我们的这项工作可以在 https://github.com/pinterest/atg-research/tree/main/omnisearchsage 得到落实。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OmniSearchSage:+Multi-Task+Multi-Entity+Embeddings+for+Pinterest+Search)|0|
|[ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation](https://doi.org/10.1145/3589334.3645467)|Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, Weinan Zhang||With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data quality of testing samples, which greatly reduces the difficulty for LLMs to extract the essential knowledge from user behavior sequences. As for few-shot recommendation, we further design retrieval-enhanced instruction tuning (ReiT) by adopting SUBR as a data augmentation technique for training samples. Specifically, we develop a mixed training dataset consisting of both the original data samples and their retrieval-enhanced counterparts. We conduct extensive experiments on three real-world public datasets to demonstrate the superiority of ReLLa compared with existing baseline models, as well as its capability for lifelong sequential behavior comprehension. To be highlighted, with only less than 10 traditional CTR models that are trained on the entire training set (e.g., DCNv2, DIN, SIM). The code is available <https://github.com/LaVieEnRose365/ReLLa>.|随着大型语言模型(LLM)在自然语言处理(NLP)领域取得显著突破，LLM 增强型推荐系统受到了广泛关注，目前正在积极探索之中。在本文中，我们着重于适应和授权一个纯大型语言模型来完成零镜头和少镜头的推荐任务。首先，我们确定并制定了推荐域中 LLM 的终身序列行为不理解问题，即 LLM 不能从长用户行为序列的文本上下文中提取有用的信息，即使上下文的长度远未达到 LLM 的上下文限制。为了解决这一问题，提高 LLM 的推荐性能，我们提出了一种新的框架，即检索增强的大语言模型(ReLLa) ，用于在零镜头和少镜头情况下的推荐任务。对于零镜头推荐，我们通过语义用户行为检索(SUBR)来提高测试样本的数据质量，大大降低了 LLM 从用户行为序列中提取基本知识的难度。对于少镜头推荐，我们采用 SUBR 作为训练样本的数据增强技术，进一步设计了检索增强型指令调优(ReiT)。具体来说，我们开发了一个混合训练数据集，包括原始数据样本和它们的检索增强对应物。我们在三个真实世界的公共数据集上进行了广泛的实验，以证明与现有的基线模型相比，ReLLa 的优越性，以及它终身连续行为理解的能力。要突出显示的是，只有少于10个传统的 CTR 模型是在整个训练集(例如，DCNv2，DIN，SIM)上训练的。代码可在 <  https://github.com/lavieenrose365/rella。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReLLa:+Retrieval-enhanced+Large+Language+Models+for+Lifelong+Sequential+Behavior+Comprehension+in+Recommendation)|0|
|[Large Language Model based Long-tail Query Rewriting in Taobao Search](https://doi.org/10.1145/3589335.3648298)|Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Derong Xu, Tong Xu, Enhong Chen||In the realm of e-commerce search, the significance of semantic matching cannot be overstated, as it directly impacts both user experience and company revenue. Along this line, query rewriting, serving as an important technique to bridge the semantic gaps inherent in the semantic matching process, has attached wide attention from the industry and academia. However, existing query rewriting methods often struggle to effectively optimize long-tail queries and alleviate the phenomenon of "few-recall" caused by semantic gap. In this paper, we present BEQUE, a comprehensive framework that Bridges the sEmantic gap for long-tail QUEries. In detail, BEQUE comprises three stages: multi-instruction supervised fine tuning (SFT), offline feedback, and objective alignment. We first construct a rewriting dataset based on rejection sampling and auxiliary tasks mixing to fine-tune our large language model (LLM) in a supervised fashion. Subsequently, with the well-trained LLM, we employ beam search to generate multiple candidate rewrites, and feed them into Taobao offline system to obtain the partial order. Leveraging the partial order of rewrites, we introduce a contrastive learning method to highlight the distinctions between rewrites, and align the model with the Taobao online objectives. Offline experiments prove the effectiveness of our method in bridging semantic gap. Online A/B tests reveal that our method can significantly boost gross merchandise volume (GMV), number of transaction (#Trans) and unique visitor (UV) for long-tail queries. BEQUE has been deployed on Taobao, one of most popular online shopping platforms in China, since October 2023.|在电子商务搜索领域，语义匹配的重要性怎么强调都不为过，因为它直接影响用户体验和公司收入。沿着这条路线，查询重写作为弥合语义匹配过程中固有的语义差异的一项重要技术，受到了业界和学术界的广泛关注。然而，现有的查询重写方法往往难以有效地优化长尾查询，缓解由于语义差异造成的“少召回”现象。在本文中，我们提出了 BEQUE，一个综合框架，桥梁的语义差距为长尾查询。具体而言，BEQUE 包括三个阶段: 多指令监督微调(SFT)、离线反馈和目标校准。我们首先构建一个基于拒绝采样和辅助任务混合的重写数据集，以监督的方式对大型语言模型(LLM)进行微调。然后，利用训练有素的 LLM，通过波束搜索生成多个候选重写，并将其输入到淘宝离线系统中，获得部分序列。利用重写的部分顺序，我们引入了一种对比学习方法来突出重写之间的区别，并使模型与淘宝在线目标相一致。离线实验证明了该方法在消除语义鸿沟方面的有效性。在线 A/B 测试表明，我们的方法可以显着提高总商品数量(GMV) ，交易数量(# Trans)和独立访问者(UV)的长尾查询。自2023年10月以来，BEQUE 已经部署在淘宝上，淘宝是中国最受欢迎的在线购物平台之一。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Model+based+Long-tail+Query+Rewriting+in+Taobao+Search)|0|
|[An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce](https://doi.org/10.1145/3589335.3648318)|Nurendra Choudhary, Edward W. Huang, Karthik Subbian, Chandan K. Reddy||The problem of search relevance in the E-commerce domain is a challenging one since it involves understanding the intent of a user's short nuanced query and matching it with the appropriate products in the catalog. This problem has traditionally been addressed using language models (LMs) and graph neural networks (GNNs) to capture semantic and inter-product behavior signals, respectively. However, the rapid development of new architectures has created a gap between research and the practical adoption of these techniques. Evaluating the generalizability of these models for deployment requires extensive experimentation on complex, real-world datasets, which can be non-trivial and expensive. Furthermore, such models often operate on latent space representations that are incomprehensible to humans, making it difficult to evaluate and compare the effectiveness of different models. This lack of interpretability hinders the development and adoption of new techniques in the field. To bridge this gap, we propose Plug and Play Graph LAnguage Model (PP-GLAM), an explainable ensemble of plug and play models. Our approach uses a modular framework with uniform data processing pipelines. It employs additive explanation metrics to independently decide whether to include (i) language model candidates, (ii) GNN model candidates, and (iii) inter-product behavioral signals. For the task of search relevance, we show that PP-GLAM outperforms several state-of-the-art baselines as well as a proprietary model on real-world multilingual, multi-regional e-commerce datasets. To promote better model comprehensibility and adoption, we also provide an analysis of the explainability and computational complexity of our model. We also provide the public codebase and provide a deployment strategy for practical implementation.|电子商务领域中的搜索相关性问题是一个具有挑战性的问题，因为它涉及到理解用户的短细微差别查询的意图，并将其与目录中适当的产品进行匹配。传统上，这个问题被分别用语言模型(LMs)和图神经网络(GNN)来捕获语义信号和产品间行为信号。然而，新体系结构的快速发展在这些技术的研究和实际应用之间产生了差距。评估这些部署模型的通用性需要在复杂的、真实世界的数据集上进行大量的实验，这些实验可能是非常重要和昂贵的。此外，这些模型经常运行在人类无法理解的潜在空间表征上，使得难以评估和比较不同模型的有效性。这种缺乏可解释性的情况妨碍了这一领域新技术的开发和采用。为了弥补这一差距，我们提出了即插即用图语言模型(PP-GLAM) ，这是一个可解释的即插即用模型集合。我们的方法使用具有统一数据处理管道的模块化框架。它使用加性解释度量来独立决定是否包括(i)语言模型候选者，(ii) GNN 模型候选者，和(iii)产品间行为信号。对于搜索相关性的任务，我们表明，PP-GLAM 优于几个最先进的基线，以及在现实世界中的多语言，多地区电子商务数据集的专有模型。为了促进更好的模型可理解性和采用性，我们还对模型的可解释性和计算复杂性进行了分析。我们还提供了公共代码库，并为实际实现提供了部署策略。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Interpretable+Ensemble+of+Graph+and+Language+Models+for+Improving+Search+Relevance+in+E-Commerce)|0|
|[Hierarchical Query Classification in E-commerce Search](https://doi.org/10.1145/3589335.3648332)|Bing He, Sreyashi Nag, Limeng Cui, Suhang Wang, Zheng Li, Rahul Goutam, Zhen Li, Haiyang Zhang||E-commerce platforms typically store and structure product information and search data in a hierarchy. Efficiently categorizing user search queries into a similar hierarchical structure is paramount in enhancing user experience on e-commerce platforms as well as news curation and academic research. The significance of this task is amplified when dealing with sensitive query categorization or critical information dissemination, where inaccuracies can lead to considerable negative impacts. The inherent complexity of hierarchical query classification is compounded by two primary challenges: (1) the pronounced class imbalance that skews towards dominant categories, and (2) the inherent brevity and ambiguity of search queries that hinder accurate classification. To address these challenges, we introduce a novel framework that leverages hierarchical information through (i) enhanced representation learning that utilizes the contrastive loss to discern fine-grained instance relationships within the hierarchy, called ”instance hierarchy”, and (ii) a nuanced hierarchical classification loss that attends to the intrinsic label taxonomy, named ”label hierarchy”. Additionally, based on our observation that certain unlabeled queries share typographical similarities with labeled queries, we propose a neighborhood-aware sampling technique to intelligently select these unlabeled queries to boost the classification performance. Extensive experiments demonstrate that our proposed method is better than state-of-the-art (SOTA) on the proprietary Amazon dataset, and comparable to SOTA on the public datasets of Web of Science and RCV1-V2. These results underscore the efficacy of our proposed solution, and pave the path toward the next generation of hierarchy-aware query classification systems.|电子商务平台通常在一个层次结构中存储和构造产品信息和搜索数据。有效地将用户搜索查询分类到一个类似的层次结构中，对于提高电子商务平台上的用户体验以及新闻策划和学术研究都是至关重要的。在处理敏感的查询分类或关键信息传播时，这项任务的重要性被放大，因为不准确可能导致相当大的负面影响。层次化查询分类的内在复杂性由两个主要挑战所复杂化: (1)明显的类不平衡倾向于主导类别，(2)搜索查询的内在简洁性和模糊性阻碍了准确分类。为了解决这些挑战，我们引入了一个新的框架，通过(i)增强的表示学习利用层次结构中的细粒度实例关系，称为“实例层次结构”，以及(ii)一个细微的层次分类损失，涉及内在的标签分类法，称为“标签层次结构”。此外，基于我们观察到的某些未标记查询与标记查询在字体上有相似之处，我们提出了一种邻域感知抽样技术来智能地选择这些未标记查询以提高分类性能。大量的实验表明，我们提出的方法在专有 Amazon 数据集上优于最先进的 SOTA (State-of-art) ，在公共数据集上与 SOTA (Web of Science)和 RCV1-V2相当。这些结果强调了我们提出的解决方案的有效性，并为下一代感知层次的查询分类系统铺平了道路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Query+Classification+in+E-commerce+Search)|0|
|[Detecting Generated Native Ads in Conversational Search](https://doi.org/10.1145/3589335.3651489)|Sebastian Schmidt, Ines Zelch, Janek Bevendorff, Benno Stein, Matthias Hagen, Martin Potthast||Conversational search engines such as YouChat and Microsoft Copilot use large language models (LLMs) to generate answers to queries. It is only a small step to also use this technology to generate and integrate advertising within these answers - instead of placing ads separately from the organic search results. This type of advertising is reminiscent of native advertising and product placement, both of which are very effective forms of subtle and manipulative advertising. It is likely that information seekers will be confronted with such use of LLM technology in the near future, especially when considering the high computational costs associated with LLMs, for which providers need to develop sustainable business models. This paper investigates whether LLMs can also be used as a countermeasure against generated native ads, i.e., to block them. For this purpose we compile a large dataset of ad-prone queries and of generated answers with automatically integrated ads to experiment with fine-tuned sentence transformers and state-of-the-art LLMs on the task of recognizing the ads. In our experiments sentence transformers achieve detection precision and recall values above 0.9, while the investigated LLMs struggle with the task.|像 YouChat 和 Microsoft Copilot 这样的会话搜索引擎使用大型语言模型(LLM)来生成查询的答案。使用这种技术在这些答案中生成和整合广告，而不是将广告与有机搜索结果分开放置，这只是一小步。这种类型的广告让人想起原生广告和植入性营销，它们都是非常有效的微妙和操纵性的广告形式。在不久的将来，信息寻求者可能会遇到这种使用 LLM 技术的情况，特别是考虑到与 LLM 相关的高计算成本，供应商需要为此开发可持续的商业模式。本文研究 LLM 是否也可以作为一种对抗生成本地广告的对策，即屏蔽本地广告。为此，我们编译了一个大型的广告倾向查询数据集和自动整合广告生成的答案数据集，以便在识别广告的任务中使用微调的句子转换器和最先进的 LLM 进行实验。在我们的实验中，句子转换器的检测准确率召回率大于0.9，而被调查的 LLM 则在努力完成这项任务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Generated+Native+Ads+in+Conversational+Search)|0|
|[Recall-Augmented Ranking: Enhancing Click-Through Rate Prediction Accuracy with Cross-Stage Data](https://doi.org/10.1145/3589335.3651551)|Junjie Huang, Guohao Cai, Jieming Zhu, Zhenhua Dong, Ruiming Tang, Weinan Zhang, Yong Yu||Click-through rate (CTR) prediction plays an indispensable role in online platforms. Numerous models have been proposed to capture users' shifting preferences by leveraging user behavior sequences. However, these historical sequences often suffer from severe homogeneity and scarcity compared to the extensive item pool. Relying solely on such sequences for user representations is inherently restrictive, as user interests extend beyond the scope of items they have previously engaged with. To address this challenge, we propose a data-driven approach to enrich user representations. We recognize user profiling and recall items as two ideal data sources within the cross-stage framework, encompassing the u2u (user-to-user) and i2i (item-to-item) aspects respectively. In this paper, we propose a novel architecture named Recall-Augmented Ranking (RAR). RAR consists of two key sub-modules, which synergistically gather information from a vast pool of look-alike users and recall items, resulting in enriched user representations. Notably, RAR is orthogonal to many existing CTR models, allowing for consistent performance improvements in a plug-and-play manner. Extensive experiments are conducted, which verify the efficacy and compatibility of RAR against the SOTA methods.|点进率预测在网络平台上扮演着不可或缺的角色。已经有很多模型被提出来通过利用用户行为序列来捕捉用户不断变化的偏好。然而，这些历史序列往往遭受严重的同质性和稀缺性相比，广泛的项目池。仅仅依靠这些序列来表示用户本身就是限制性的，因为用户的兴趣已经超出了他们以前参与的项目的范围。为了解决这个问题，我们提出了一种数据驱动的方法来丰富用户表示。我们将用户分析和召回项目视为跨阶段框架内的两个理想数据源，分别包括 u2u (用户对用户)和 i2i (项目对项目)方面。在本文中，我们提出了一种新的体系结构，称为召回增强排序(RAR)。RAR 由两个关键的子模块组成，它们协同地从大量外观相似的用户和召回项目中收集信息，从而丰富用户表示。值得注意的是，RAR 与许多现有的 CTR 模型是正交的，允许以即插即用的方式提高性能。进行了大量的实验，验证了 RAR 方法对 SOTA 方法的有效性和相容性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recall-Augmented+Ranking:+Enhancing+Click-Through+Rate+Prediction+Accuracy+with+Cross-Stage+Data)|0|
|[RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction](https://doi.org/10.1145/3589335.3651550)|Yushen Li, Jinpeng Wang, Tao Dai, Jieming Zhu, Jun Yuan, Rui Zhang, ShuTao Xia||Predicting click-through rates (CTR) is a fundamental task for Web applications, where a key issue is to devise effective models for feature interactions. Current methodologies predominantly concentrate on modeling feature interactions within an individual sample, while overlooking the potential cross-sample relationships that can serve as a reference context to enhance the prediction. To make up for such deficiency, this paper develops a Retrieval-Augmented Transformer (RAT), aiming to acquire fine-grained feature interactions within and across samples. By retrieving similar samples, we construct augmented input for each target sample. We then build Transformer layers with cascaded attention to capture both intra- and cross-sample feature interactions, facilitating comprehensive reasoning for improved CTR prediction while retaining efficiency. Extensive experiments on real-world datasets substantiate the effectiveness of RAT and suggest its advantage in long-tail scenarios. The code has been open-sourced at <https://github.com/YushenLi807/WWW24-RAT>.|预测点击率(CTR)是 Web 应用程序的基本任务，其中一个关键问题是为特性交互设计有效的模型。目前的方法主要集中在建模个体样本内的特征相互作用，而忽视了可以作为参考背景的潜在跨样本关系，以增强预测。为了弥补这一不足，本文开发了一种检索增强变压器(RAT) ，旨在获取样本内部和样本间的细粒度特征交互。通过检索相似的样本，我们为每个目标样本构造增强的输入。然后，我们建立级联注意力的变压器层，以捕获内部和交叉样本的特征相互作用，促进综合推理改善 CTR 预测，同时保留效率。对真实世界数据集的大量实验证实了 RAT 的有效性，并提出了它在长尾场景中的优势。该代码已在 <  https://github.com/yushenli807/www24-rat 开放源码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RAT:+Retrieval-Augmented+Transformer+for+Click-Through+Rate+Prediction)|0|
|[Personalized Ordering of Recommendation-Modules on an E-Commerce Homepage](https://doi.org/10.1145/3589335.3651545)|Haggai Roitman, Alexander Nus, Yotam Eshel||The homepage of an E-Commerce website may accommodate multiple and diverse recommendation modules; with each module is designed to cover some facet of the user's needs. Commonly, the recommendation modules are ordered in the same way for all homepage users, which leads to a sub-optimal user experience. In this work, we present a novel personalized module ordering solution that provides a more educated way to determine an ordering of the homepage modules based on historical user-interactions. Overall, we evaluate our solution and demonstrate its merits.|电子商务网站的主页可以容纳多个不同的推荐模块; 每个模块的设计是为了满足用户的某些方面的需求。通常，推荐模块对所有主页用户的排序方式都是相同的，这导致了次优的用户体验。在这项工作中，我们提出了一个新颖的个性化模块订购解决方案，提供了一个更好的方法来确定订购的主页模块的历史用户交互的基础上。总的来说，我们评估了我们的解决方案并展示了它的优点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Ordering+of+Recommendation-Modules+on+an+E-Commerce+Homepage)|0|
|[Retrieval-augmented Query Reformulation for Heterogeneous Research Asset Retrieval in Virtual Research Environment](https://doi.org/10.1145/3589335.3651553)|Peide Zhu, Na Li, Zhiming Zhao||Discovering and reusing research assets such as datasets and computational notebooks is crucial for building research workflows in data-centric studies. The rapid growth of research assets in scientific communities provides scientists with great opportunities to enhance research efficacy but also poses significant challenges in finding suitable materials for specific tasks. Scientists, especially those focusing on cross-disciplinary research, often find it difficult to formulate effective queries to retrieve desired resources. Previous work has proposed query reformulation methods to increase the efficiency of research asset search. However, it relies on existent knowledge graphs and is constrained to computational notebooks only. As research assets utilized by data analytic workflows are in essence heterogeneous, i.e., of distinct kinds and from diversified sources, query reformulation methods in this regard should consider the relationship between different types of research assets. To address the above challenges, we propose a retrieval-augmented query reformulation method for heterogeneous research asset retrieval. It is developed in the context of a Notebook-based virtual research environment (VRE) and offers query reformulation services to other VRE components. We demonstrate the effectiveness of the proposed query reformulation service with experiments on dataset and notebook retrieval. Up till now, we have indexed 8,954 datasets and 18,158 notebooks. The experimental results show that the proposed service can create useful query suggestions.|在以数据为中心的研究中，发现和重用数据集和计算笔记本等研究资产对于建立研究工作流程至关重要。科学界研究资产的迅速增长为科学家提供了提高研究效率的巨大机会，但也对为具体任务寻找合适材料提出了重大挑战。科学家，尤其是那些专注于跨学科研究的科学家，往往发现很难制定有效的查询来检索所需的资源。前人的工作已经提出了查询重构方法，以提高研究资产搜索的效率。但是，它依赖于现有的知识图，并且仅限于计算笔记本。由于数据分析工作流所使用的研究资产本质上是异构的，即来自不同种类和不同来源的研究资产，因此在这方面的查询重构方法应考虑不同类型研究资产之间的关系。针对上述挑战，我们提出了一种异构研究资产检索的检索增强查询重构方法。它是在基于笔记本的虚拟研究环境(VRE)的背景下开发的，并为其他 VRE 组件提供查询重构服务。通过对数据集和笔记本检索的实验，验证了该查询重构服务的有效性。到目前为止，我们已经索引了8,954个数据集和18,158个笔记本电脑。实验结果表明，该服务能够创建有用的查询建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retrieval-augmented+Query+Reformulation+for+Heterogeneous+Research+Asset+Retrieval+in+Virtual+Research+Environment)|0|
|[List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation](https://doi.org/10.1145/3589334.3645336)|Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, Xueqi Cheng||The results of information retrieval (IR) are usually presented in the form of a ranked list of candidate documents, such as web search for humans and retrieval-augmented generation for large language models (LLMs). List-aware retrieval aims to capture the list-level contextual features to return a better list, mainly including reranking and truncation. Reranking finely re-scores the documents in the list. Truncation dynamically determines the cut-off point of the ranked list to achieve the trade-off between overall relevance and avoiding misinformation from irrelevant documents. Previous studies treat them as two separate tasks and model them separately. However, the separation is not optimal. First, it is hard to share the contextual information of the ranking list between the two tasks. Second, the separate pipeline usually meets the error accumulation problem, where the small error from the reranking stage can largely affect the truncation stage. To solve these problems, we propose a Reranking-Truncation joint model (GenRT) that can perform the two tasks concurrently. GenRT integrates reranking and truncation via generative paradigm based on encoder-decoder architecture. We also design the novel loss functions for joint optimization to make the model learn both tasks. Sharing parameters by the joint model is conducive to making full use of the common modeling information of the two tasks. Besides, the two tasks are performed concurrently and co-optimized to solve the error accumulation problem between separate stages. Experiments on public learning-to-rank benchmarks and open-domain Q&A tasks show that our method achieves SOTA performance on both reranking and truncation tasks for web search and retrieval-augmented LLMs.|信息检索检索(IR)的结果通常以候选文档的排序列表的形式呈现，例如人类的网络搜索和大型语言模型的检索增强生成(LLM)。列表感知检索旨在捕获列表级上下文特征以返回更好的列表，主要包括重新排序和截断。对列表中的文档进行精细的重新排序。截断动态确定排序列表的截止点，以实现总体相关性和避免来自不相关文档的错误信息之间的权衡。以往的研究将它们视为两个独立的任务，并分别建立模型。然而，这种分离并不理想。首先，很难在两个任务之间共享排名列表的上下文信息。其次，分离的流水线通常会遇到误差累积问题，重新排序阶段的小误差会对截断阶段产生很大的影响。为了解决这些问题，我们提出了一个可以同时执行这两个任务的重新排序-截断联合模型(GenRT)。GenRT 通过基于编解码体系结构的生成范式集成了重新排序和截断。设计了新的联合优化损失函数，使模型能够同时学习两个任务。通过联合模型共享参数有利于充分利用两个任务的共同建模信息。此外，为了解决不同阶段之间的误差累积问题，两个任务同时进行，并进行了协同优化。通过对公共学习排序基准测试和开放域问答任务的实验表明，该方法在网络搜索和检索增强 LLM 的重排序任务和截断任务上都达到了 SOTA 性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=List-aware+Reranking-Truncation+Joint+Model+for+Search+and+Retrieval-augmented+Generation)|0|
|[Query in Your Tongue: Reinforce Large Language Models with Retrievers for Cross-lingual Search Generative Experience](https://doi.org/10.1145/3589334.3645701)|Ping Guo, Yue Hu, Yanan Cao, Yubing Ren, Yunpeng Li, Heyan Huang||In the contemporary digital landscape, search engines play an invaluable role in information access, yet they often face challenges in Cross-Lingual Information Retrieval (CLIR). Though attempts are made to improve CLIR, current methods still leave users grappling with issues such as misplaced named entities and lost cultural context when querying in non-native languages. While some advances have been made using Neural Machine Translation models and cross-lingual representation, these are not without limitations. Enter the paradigm shift brought about by Large Language Models (LLMs), which have transformed search engines from simple retrievers to generators of contextually relevant information. This paper introduces the Multilingual Information Model for Intelligent Retrieval (MIMIR). Built on the power of LLMs, MIMIR directly responds in the language of the user's query, reducing the need for post-search translations. Our model's architecture encompasses a dual-module system: a retriever for searching multilingual documents and a responder for crafting answers in the user's desired language. Through a unique unified training framework, with the retriever serving as a reward model supervising the responder, and in turn, the responder producing synthetic data to refine the retriever's proficiency, MIMIR's retriever and responder iteratively enhance each other. Performance evaluations via CLEF and MKQA benchmarks reveal MIMIR's superiority over existing models, effectively addressing traditional CLIR challenges.|在当今的数字世界中，搜索引擎在信息获取方面扮演着非常重要的角色，但它们在 Cross-Lingual 信息检索(CLIR)中经常面临挑战。尽管已经尝试改进 CLIR，但是目前的方法仍然让用户在使用非母语进行查询时遇到一些问题，例如命名实体放错地方和文化上下文丢失。虽然神经机器翻译模型和跨语言表示已经取得了一些进展，但这些并非没有局限性。进入大语言模型(LLM)带来的范式转变，它已经将搜索引擎从简单的检索器转变为上下文相关信息的生成器。本文介绍了智能检索的多语种信息模型(MIMIR)。基于 LLM 的强大功能，MIMIR 直接用用户的查询语言进行回复，减少了对搜索后翻译的需求。我们的模型的体系结构包含一个双模块系统: 一个用于搜索多语言文档的检索器和一个用于以用户所需语言编写答案的响应器。通过一个独特的统一训练框架，猎犬作为奖励模型监督应答者，反过来，应答者产生合成数据来提高猎犬的熟练程度，MIMIR 的猎犬和应答者反复地相互增强。通过 CLEF 和 MKQA 基准进行的绩效评估揭示了 MIMIR 相对于现有模式的优越性，有效地应对了传统的 CLIR 挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query+in+Your+Tongue:+Reinforce+Large+Language+Models+with+Retrievers+for+Cross-lingual+Search+Generative+Experience)|0|
|[UnifiedSSR: A Unified Framework of Sequential Search and Recommendation](https://doi.org/10.1145/3589334.3645427)|Jiayi Xie, Shang Liu, Gao Cong, Zhenzhong Chen||In this work, we propose a Unified framework of Sequential Search and Recommendation (UnifiedSSR) for joint learning of user behavior history in both search and recommendation scenarios. Specifically, we consider user-interacted products in the recommendation scenario, user-interacted products and user-issued queries in the search scenario as three distinct types of user behaviors. We propose a dual-branch network to encode the pair of interacted product history and issued query history in the search scenario in parallel. This allows for cross-scenario modeling by deactivating the query branch for the recommendation scenario. Through the parameter sharing between dual branches, as well as between product branches in two scenarios, we incorporate cross-view and cross-scenario associations of user behaviors, providing a comprehensive understanding of user behavior patterns. To further enhance user behavior modeling by capturing the underlying dynamic intent, an Intent-oriented Session Modeling module is designed for inferring intent-oriented semantic sessions from the contextual information in behavior sequences. In particular, we consider self-supervised learning signals from two perspectives for intent-oriented semantic session locating, which encourage session discrimination within each behavior sequence and session alignment between dual behavior sequences. Extensive experiments on three public datasets demonstrate that UnifiedSSR consistently outperforms state-of-the-art methods for both search and recommendation.|在这项工作中，我们提出了一个统一的线性搜索和推荐框架(UnifiedSSR) ，用于在搜索和推荐场景中联合学习用户行为历史。具体来说，我们将推荐场景中的用户交互产品、搜索场景中的用户交互产品和用户发布的查询视为三种不同类型的用户行为。我们提出了一个双分支网络，在搜索场景中并行编码交互的产品历史记录和发布的查询历史记录。这允许通过停用推荐场景的查询分支来进行跨场景建模。通过双分支之间的参数共享，以及两个场景中产品分支之间的参数共享，我们整合了用户行为的跨视图和跨场景关联，提供了对用户行为模式的全面理解。为了通过捕获潜在的动态意图进一步增强用户行为建模，设计了一个面向意图的会话建模模块，用于从行为序列中的上下文信息推断面向意图的语义会话。特别地，我们从两个角度考虑了自我监督学习信号的意图导向语义会话定位，它鼓励在每个行为序列中的会话区分和双行为序列之间的会话对齐。在三个公共数据集上的大量实验表明，UnifiedSSR 在搜索和推荐方面始终优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UnifiedSSR:+A+Unified+Framework+of+Sequential+Search+and+Recommendation)|0|
|[Scaling User Modeling: Large-scale Online User Representations for Ads Personalization in Meta](https://doi.org/10.1145/3589335.3648301)|Wei Zhang, Dai Li, Chen Liang, Fang Zhou, Zhongke Zhang, Xuewei Wang, Ru Li, Yi Zhou, Yaning Huang, Dong Liang, Kai Wang, Zhangyuan Wang, Zhengxing Chen, Fenggang Wu, Minghai Chen, Huayu Li, Yunnan Wu, Zhan Shu, Mindi Yuan, Sri Reddy||Effective user representations are pivotal in personalized advertising. However, stringent constraints on training throughput, serving latency, and memory, often limit the complexity and input feature set of online ads ranking models. This challenge is magnified in extensive systems like Meta's, which encompass hundreds of models with diverse specifications, rendering the tailoring of user representation learning for each model impractical. To address these challenges, we present Scaling User Modeling (SUM), a framework widely deployed in Meta's ads ranking system, designed to facilitate efficient and scalable sharing of online user representation across hundreds of ads models. SUM leverages a few designated upstream user models to synthesize user embeddings from massive amounts of user features with advanced modeling techniques. These embeddings then serve as inputs to downstream online ads ranking models, promoting efficient representation sharing. To adapt to the dynamic nature of user features and ensure embedding freshness, we designed SUM Online Asynchronous Platform (SOAP), a latency free online serving system complemented with model freshness and embedding stabilization, which enables frequent user model updates and online inference of user embeddings upon each user request. We share our hands-on deployment experiences for the SUM framework and validate its superiority through comprehensive experiments. To date, SUM has been launched to hundreds of ads ranking models in Meta, processing hundreds of billions of user requests daily, yielding significant online metric gains and infrastructure cost savings.|有效的用户表示是个性化广告的关键。然而，严格限制训练吞吐量、服务延迟和内存，往往限制了在线广告排名模型的复杂性和输入特征集。这个挑战在像 Meta 这样的广泛系统中被放大，它包含了数百个具有不同规格的模型，使得为每个模型裁剪用户表示学习变得不切实际。为了应对这些挑战，我们提出了缩放用户建模(SUM) ，这是一个广泛应用于 Meta 广告排名系统的框架，旨在促进高效和可扩展的在线用户表示在数百个广告模型之间的共享。SUM 利用一些指定的上游用户模型，通过先进的建模技术从大量的用户特性中综合用户嵌入。然后这些嵌入作为下游在线广告排名模型的输入，促进有效的表示共享。为了适应用户特征的动态性和保证嵌入的新鲜性，我们设计了 SUM 在线异步平台(SOAP) ，这是一个无延迟的在线服务系统，它补充了模型的新鲜性和嵌入的稳定性，能够根据用户的每个请求频繁地更新用户模型和在线推断用户的嵌入。我们分享了 SUM 框架的实际部署经验，并通过全面的实验验证了其优越性。迄今为止，SUM 已经在 Meta 中推出了数百个广告排名模型，每天处理数千亿用户请求，产生了显著的在线度量收益和基础设施成本节约。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scaling+User+Modeling:+Large-scale+Online+User+Representations+for+Ads+Personalization+in+Meta)|0|
|[Finding What Users Look for by Attribute-Aware Personalized Item Comparison in Relevant Recommendation](https://doi.org/10.1145/3589335.3651508)|Rui Ma, Dike Sun, Jincheng Xu, Jingsong Yuan, Jiandong Zhang||Relevant recommendation is a distinctive recommendation scenario in e-commerce platforms, which provides an extended set of items that are relevant to the trigger item (the item that triggers the relevant recommendation). Different from the general recommendations whose item feeds are diversified, relevant recommendation regards the trigger item as a key component. From one perspective, the trigger item reveals users' current interests and determines the range of the recommendation results. From the other perspective, users may have the mindset to look for items that have directional attribute differences from the trigger item. In this paper, we present an attribute-aware personalized item comparison framework. Under this framework, an item subtraction module is first applied over the trigger item and the candidate item, which calculates their directional difference with consideration of their intrinsic similarity. Then two modules are used to estimate users' preference for this current item pair: one learns the collective preference of all users, and the other learns the current user's personal evolutional preference. Experiments on a CTR prediction task over both a public dataset and an industrial dataset from our shopping app show that the proposed method outperforms the state-of-the-art algorithms and also achieves better generalization ability.|相关推荐是电子商务平台中一个独特的推荐场景，它提供了一组与触发条目(触发相关推荐的条目)相关的扩展条目。不同于一般推荐的项目种类多样化，相关推荐将触发项目作为一个关键组成部分。从一个角度来看，触发条目揭示了用户当前的兴趣，并确定了推荐结果的范围。从另一个角度来看，用户可能倾向于寻找与触发器项具有方向属性差异的项。在本文中，我们提出了一个属性感知的个性化项目比较框架。在此框架下，首先对触发项和候选项应用项减法模块，计算它们的方向差，并考虑它们的内在相似性。然后利用两个模块来估计用户对当前项目对的偏好: 一个模块学习所有用户的集体偏好，另一个模块学习当前用户的个人进化偏好。通过对一个公共数据集和一个工业数据集的点击率预测任务的实验表明，该方法的性能优于目前最先进的算法，并且具有更好的泛化能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Finding+What+Users+Look+for+by+Attribute-Aware+Personalized+Item+Comparison+in+Relevant+Recommendation)|0|
|[De-Anchor: Mitigating Attention Polarization for Lifelong User Behavior Modeling in Click-Through Rate Prediction](https://doi.org/10.1145/3589335.3651486)|Hongzun Liu, Kang Yin, Tianyu Sun, Rui Huang, Yunsong Li, Xiao Fang, Zhaojie Liu, Weidong Liu, Guorui Zhou||User lifelong behavior sequences are essential for click-through rate (CTR) prediction tasks in industrial recommender systems. Attention-based module, especially multi-head target attention (MHTA), has been proven to be effective in aggregating behavior features given a certain target item. However, we found a common phenomenon that attention weights in MHTA tend to over-concentrate on merely a small subset of a user's historical behaviors, producing a sparse one-hot distributed attention weights in training gradually, which we callAttention Polarization (AP). These polarized weights on certain behaviors (which we call "\textitattention anchor ") could make the model fail to capture a user's diversified interests, and harm the learning of behavior embedding as the gradients on these features are nearly zero. We introduce two indicators:anchor rate andattention entropy to measure the magnitude of AP, and proposeDe-Anchor, a novel method to alleviate it, which can serve as a stand-alone and parameter-efficient plug-in to existing CTR backbones. De-Anchor contains two modules:Anchor-aware gradient dropout (AGD) forcing the model to capture diversified interest information from behavior sequences by discarding gradients of non-behavior features, andTarget-aware attention anchor (TAA) providing a pseudo behavior to offload excessive weights of MHTA. Extensive offline experiments and industrial online A/B tests demonstrate the efficacy of our method.|在工业推荐系统中，用户终身行为序列对于点进率(CTR)预测任务至关重要。基于注意的模型，尤其是多目标注意模型(MHTA) ，已经被证明能够有效地聚合给定目标项的行为特征。然而，我们发现了一个普遍的现象，即 MHTA 中的注意权重往往过度集中在用户历史行为的一小部分，在训练中逐渐产生稀疏的一热分布的注意权重，我们称之为注意极化(AP)。这些对特定行为(我们称之为“文本注意锚”)的极化权重可能使模型无法捕捉用户的多样化兴趣，并且由于这些特征上的梯度几乎为零而损害行为嵌入的学习。我们引入锚定率和注意熵两个指标来度量 AP 的大小，并提出了一种新的缓解 AP 的方法 De-Anchor，它可以作为现有 CTR 骨干网的一个独立的、参数高效的插件。去锚模型包含两个模块: 锚感知梯度丢失(AGD)强制模型通过丢弃非行为特征的梯度来从行为序列中捕获多样化的兴趣信息; 目标感知注意锚(TAA)提供一种伪行为来卸载 MHTA 的过度权重。大量的离线实验和工业在线 A/B 测试证明了我们方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=De-Anchor:+Mitigating+Attention+Polarization+for+Lifelong+User+Behavior+Modeling+in+Click-Through+Rate+Prediction)|0|
|[User Distribution Mapping Modelling with Collaborative Filtering for Cross Domain Recommendation](https://doi.org/10.1145/3589334.3645331)|Weiming Liu, Chaochao Chen, Xinting Liao, Mengling Hu, Jiajie Su, Yanchao Tan, Fan Wang||User cold-start recommendation aims to provide accurate items for the newly joint users and is a hot and challenging problem. Nowadays as people participant in different domains, how to recommend items in the new domain for users in an old domain has become more urgent. In this paper, we focus on the Dual Cold-Start Cross Domain Recommendation (Dual-CSCDR) problem. That is, providing the most relevant items for new users on the source and target domains. The prime task in Dual-CSCDR is to properly model user-item rating interactions and map user expressive embeddings across domains. However, previous approaches cannot solve Dual-CSCDR well, since they separate the collaborative filtering and distribution mapping process, leading to the error superimposition issue. Moreover, most of these methods fail to fully exploit the cross-domain relationship among large number of non-overlapped users, which strongly limits their performance. To fill this gap, we propose User Distribution Mapping model with Collaborative Filtering (UDMCF), a novel end-to-end cold-start cross-domain recommendation framework for the Dual-CSCDR problem. UDMCF includes two main modules, i.e., rating prediction module and distribution alignment module. The former module adopts one-hot ID vectors and multi-hot historical ratings for collaborative filtering via a contrastive loss. The latter module contains overlapped user embedding alignment and general user subgroup distribution alignment. Specifically, we innovatively propose unbalance distribution optimal transport with typical subgroup discovering algorithm to map the whole user distributions. Our empirical study on several datasets demonstrates that UDMCF significantly outperforms the state-of-the-art models under the Dual-CSCDR setting.|用户冷启动推荐旨在为新联合用户提供准确的项目，是一个热点和难点问题。随着人们在不同领域的参与，如何为旧领域的用户推荐新领域的项目已经变得越来越迫切。本文主要研究双冷启动跨域推荐问题。也就是说，为源域和目标域上的新用户提供最相关的项。双重 CSCDR 的主要任务是正确建模用户-项目评分交互并映射跨域的用户表达嵌入。然而，以前的方法并不能很好地解决双协同散射计算问题，因为它们分离了协同过滤和分布映射过程，导致了误差叠加问题。此外，这些方法大多未能充分利用大量非重叠用户之间的跨域关系，严重限制了它们的性能。为了填补这个空白，我们提出了带协同过滤的用户分布映射模型(UDMCF) ，这是一个新颖的端到端冷启动跨域推荐框架，用于解决双协同监控系统问题。UDMCF 包括两个主要模块，即定额预测模块和配电网调度模块。前一个模块采用一个热门的身份证矢量和多个热门的历史评级，通过对比损失的协同过滤。后一个模块包含重叠用户嵌入对齐和一般用户子组分布对齐。具体来说，我们创新性地提出了不平衡分布最优运输的典型子群发现算法来映射整个用户分布。我们对几个数据集的实证研究表明，在双 CSCDR 设置下，UDMCF 明显优于最先进的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User+Distribution+Mapping+Modelling+with+Collaborative+Filtering+for+Cross+Domain+Recommendation)|0|
|[Leave No One Behind: Online Self-Supervised Self-Distillation for Sequential Recommendation](https://doi.org/10.1145/3589334.3645590)|Shaowei Wei, Zhengwei Wu, Xin Li, Qintong Wu, Zhiqiang Zhang, Jun Zhou, Lihong Gu, Jinjie Gu||Sequential recommendation methods play a pivotal role in modern recommendation systems. A key challenge lies in accurately modeling user preferences in the face of data sparsity. To tackle this challenge, recent methods leverage contrastive learning (CL) to derive self-supervision signals by maximizing the mutual information of two augmented views of the original user behavior sequence. Despite their effectiveness, CL-based methods encounter a limitation in fully exploiting self-supervision signals for users with limited behavior data, as users with extensive behaviors naturally offer more information. To address this problem, we introduce a novel learning paradigm, named Online Self-Supervised Self-distillation for Sequential Recommendation ($S^4$Rec), effectively bridging the gap between self-supervised learning and self-distillation methods. Specifically, we employ online clustering to proficiently group users by their distinct latent intents. Additionally, an adversarial learning strategy is utilized to ensure that the clustering procedure is not affected by the behavior length factor. Subsequently, we employ self-distillation to facilitate the transfer of knowledge from users with extensive behaviors (teachers) to users with limited behaviors (students). Experiments conducted on four real-world datasets validate the effectiveness of the proposed method\footnote{Code is available at https://github.com/xjaw/S4Rec|序贯推荐方法在现代推荐系统中占有举足轻重的地位。一个关键的挑战在于在面对数据稀少的情况下准确地建模用户偏好。为了应对这一挑战，最近的方法利用对比学习(CL) ，通过最大化原始用户行为序列的两个增强视图的互信息来获得自我监督信号。尽管有效，但基于 CL 的方法在充分利用行为数据有限的用户的自我监督信号方面遇到了局限，因为行为广泛的用户自然会提供更多的信息。为了解决这一问题，我们引入了一种新的学习范式，称为序贯推荐在线自我监督自我精馏($S ^ 4 $Rec) ，有效地弥补了自我监督学习和自我精馏方法之间的差距。具体来说，我们使用在线集群来熟练地根据用户不同的潜在意图对他们进行分组。此外，采用对抗学习策略，确保聚类过程不受行为长度因子的影响。随后，我们采用自我提取的方法，促进知识从行为广泛的用户(教师)转移到行为有限的用户(学生)。在四个真实世界的数据集上进行的实验验证了提议方法的有效性脚注{代码可在 https://github.com/xjaw/s4rec 获得|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leave+No+One+Behind:+Online+Self-Supervised+Self-Distillation+for+Sequential+Recommendation)|0|
|[Improving Search for New Product Categories via Synthetic Query Generation Strategies](https://doi.org/10.1145/3589335.3648299)|Akshay Jagatap, Srujana Merugu, Prakash Mandayam Comar||Efficient retrieval and ranking of relevant products in e-commerce product search relies on accurate mapping of queries to product categories. This query classification typically utilizes a combination of textual and customer behavioral signals. However, new product categories often lack customer interaction data leading to poor performance. In this paper, we present a novel approach to mitigate this cold start problem in product ranking via synthetic generation of queries as well as simulation of customer interactions. Specifically we study two strategies for synthetic data generation: (i) fine-tuning a generative language model (LLM) on historical product-query interactions and using it to generate synthetic queries from the product catalog, (ii) Bayesian prompt optimization with an instruction-tuned LLM to directly generate queries from catalog. Empirical evaluation of the proposed approaches on public datasets and real-world customer queries demonstrates significant benefits (+2.96% and +2.34% in PR-AUC on e-commerce queries)1 relative to the baseline approach without synthetic data augmentation. Furthermore, evaluation of the augmented model on live search page results in a substantial increase in highly relevant product results (+3.35%) and reduction (-3.07%) in irrelevant results.|电子商务产品搜索中相关产品的有效检索和排序依赖于查询到产品类别的精确映射。这种查询分类通常使用文本和客户行为信号的组合。然而，新产品类别往往缺乏客户交互数据，导致性能较差。在本文中，我们提出了一种新的方法来缓解这种冷启动问题的产品排名通过合成生成的查询和模拟客户交互。具体而言，我们研究了合成数据生成的两种策略: (i)在历史产品-查询交互中微调生成语言模型(LLM) ，并使用它从产品目录生成合成查询; (ii)贝叶斯提示优化，使用指令调优的 LLM 直接从目录生成查询。对公共数据集和现实世界客户查询的提议方法的实证评估表明，相对于没有合成数据增强的基线方法，显着的益处(电子商务查询的 PR-AUC 分别为 + 2.96% 和 + 2.34%)1。此外，在实时搜索页面上对增强模型的评估导致高度相关的产品结果大幅增加(+ 3.35%) ，不相关的结果减少(-3.07%)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Search+for+New+Product+Categories+via+Synthetic+Query+Generation+Strategies)|0|
|[MS MARCO Web Search: A Large-scale Information-rich Web Dataset with Millions of Real Click Labels](https://doi.org/10.1145/3589335.3648327)|Qi Chen, Xiubo Geng, Corby Rosset, Carolyn Buractaon, Jingwen Lu, Tao Shen, Kun Zhou, Chenyan Xiong, Yeyun Gong, Paul N. Bennett, Nick Craswell, Xing Xie, Fan Yang, Bryan Tower, Nikhil Rao, Anlei Dong, Wenqi Jiang, Zheng Liu, Mingqin Li, Chuanjie Liu, Zengzhong Li, Rangan Majumder, Jennifer Neville, Andy Oakley, Knut Magne Risvik, Harsha Vardhan Simhadri, Manik Varma, Yujing Wang, Linjun Yang, Mao Yang, Ce Zhang||Recent breakthroughs in large models have highlighted the critical significance of data scale, labels and modals. In this paper, we introduce MS MARCO Web Search, the first large-scale information-rich web dataset, featuring millions of real clicked query-document labels. This dataset closely mimics real-world web document and query distribution, provides rich information for various kinds of downstream tasks and encourages research in various areas, such as generic end-to-end neural indexer models, generic embedding models, and next generation information access system with large language models. MS MARCO Web Search offers a retrieval benchmark with three web retrieval challenge tasks that demand innovations in both machine learning and information retrieval system research domains. As the first dataset that meets large, real and rich data requirements, MS MARCO Web Search paves the way for future advancements in AI and system research. MS MARCO Web Search dataset is available at: https://github.com/microsoft/MS-MARCO-Web-Search.|最近在大型模型方面的突破突出了数据规模、标签和模态的关键意义。本文介绍了第一个大规模信息丰富的网络数据集 MS MARCO Web Search，它包含了数百万个实际点击的查询文档标签。这个数据集非常接近真实世界的网络文档和查询分布，为各种下游任务提供丰富的信息，并鼓励在各个领域的研究，如通用的端到端神经索引器模型，通用的嵌入模型，下一代信息访问系统与大型语言模型。微软 MARCO 网络搜索提供了一个检索基准，包含三个网络检索挑战任务，需要在机器学习和信息检索系统研究领域进行创新。作为第一个满足大型、真实、丰富数据需求的数据集，微软 MARCO 网络搜索为人工智能和系统研究的未来发展铺平了道路。微软马可网上搜寻资料集可于以下 https://github.com/microsoft/MS-MARCO-Web-Search 索取:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MS+MARCO+Web+Search:+A+Large-scale+Information-rich+Web+Dataset+with+Millions+of+Real+Click+Labels)|0|
|[Enhancing Cross-Domain Click-Through Rate Prediction via Explicit Feature Augmentation](https://doi.org/10.1145/3589335.3648341)|Xu Chen, Zida Cheng, Jiangchao Yao, Chen Ju, Weilin Huang, Jinsong Lan, Xiaoyi Zeng, Shuai Xiao||Cross-domain CTR (CDCTR) prediction is an important research topic that studies how to leverage meaningful data from a related domain to help CTR prediction in target domain. Most existing CDCTR works design implicit ways to transfer knowledge across domains such as parameter-sharing that regularizes the model training in target domain. More effectively, recent researchers propose explicit techniques to extract user interest knowledge and transfer this knowledge to target domain. However, the proposed method mainly faces two issues: 1) it usually requires a super domain, i.e. an extremely large source domain, to cover most users or items of target domain, and 2) the extracted user interest knowledge is static no matter what the context is in target domain. These limitations motivate us to develop a more flexible and efficient technique to explicitly transfer knowledge. In this work, we propose a cross-domain augmentation network (CDAnet) being able to perform explicit knowledge transfer between two domains. Specifically, CDAnet contains a designed translation network and an augmentation network which are trained sequentially. The translation network computes latent features from two domains and learns meaningful cross-domain knowledge of each input in target domain by using a designed cross-supervised feature translator. Later the augmentation network employs the explicit cross-domain knowledge as augmented information to boost the target domain CTR prediction. Through extensive experiments on two public benchmarks and one industrial production dataset, we show CDAnet can learn meaningful translated features and largely improve the performance of CTR prediction. CDAnet has been conducted online A/B test in image2product retrieval at Taobao app, bringing an absolute 0.11 point CTR improvement, a relative 0.64|跨域 CTR (Cross-domain CTR)预测是研究如何利用相关领域有意义的数据来帮助目标领域 CTR 预测的一个重要研究课题。大多数现有的 CDCTR 工作设计了隐式的跨领域知识转移方法，如参数共享，规范了目标领域的模型训练。更有效的是，最近的研究人员提出了显性技术来提取用户兴趣知识并将这些知识转移到目标领域。然而，该方法主要面临两个问题: 1)它通常需要一个超级域，即一个非常大的源域，以覆盖目标域的大多数用户或项目; 2)提取的用户兴趣知识是静态的，不管目标域的上下文是什么。这些限制促使我们开发一种更灵活、更有效的技术来明确地传递知识。在这项工作中，我们提出了一个跨域增强网络(CDAnet) ，它可以在两个域之间进行外显知识传输。具体来说，CDAnet 包含一个设计好的翻译网络和一个按顺序训练的增强网络。该翻译网络利用设计的交叉监督特征转换器计算两个领域的潜在特征，并学习目标领域中每个输入的有意义的跨领域知识。随后增强网络利用显式的跨领域知识作为增强信息来提高目标领域的 CTR 预测能力。通过对两个公共基准和一个工业生产数据集的大量实验，我们发现 CDAnet 能够学习到有意义的翻译特征，从而大大提高了 CTR 预测的性能。CDAnet 在淘宝应用图像2产品检索中进行了在线 A/B 测试，绝对点击率提高了0.11个百分点，相对提高了0.64个百分点|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Cross-Domain+Click-Through+Rate+Prediction+via+Explicit+Feature+Augmentation)|0|
|[Item-Ranking Promotion in Recommender Systems](https://doi.org/10.1145/3589335.3651529)|HongKyun Bae, HaeRi Jang, YangSae Moon, SangWook Kim||In this paper, we first define the problem of item-ranking promotion (IRP) in recommender systems as (Goal 1) maintaining a high level of overall recommendation accuracy while (Goal 2) recommending the items with extra values (i.e., RP-items) to as many users as possible. Our novel framework, proposed to address the IRP problem, is based on our own loss function that simultaneously aims to achieve the two goals above and employs a learning-to-rank scheme for training a recommender model. Via extensive experiments, we validate the effectiveness of our framework in terms of the exposure rate of RP-items and the accuracy of recommendation.|本文首先将推荐系统中的项目排名推广问题定义为(目标1)保持较高的总体推荐准确率，同时(目标2)向尽可能多的用户推荐具有额外价值的项目(即 RP 项目)。针对 IRP 问题，我们提出了一种基于自身损失函数的新框架，该框架同时实现了上述两个目标，并采用了一种学习排序方案来训练一个推荐模型。通过大量的实验，我们验证了我们的框架在快速反应项目的暴露率和推荐的准确性方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Item-Ranking+Promotion+in+Recommender+Systems)|0|
|[Understanding and Counteracting Feature-Level Bias in Click-Through Rate Prediction](https://doi.org/10.1145/3589335.3651576)|Jinqiu Jin, Sihao Ding, Wenjie Wang, Fuli Feng||Common click-through rate (CTR) prediction recommender models tend to exhibit feature-level bias, which leads to unfair recommendations among item groups and inaccurate recommendations for users. While existing methods address this issue by adjusting the learning of CTR models, such as through additional optimization objectives, they fail to consider how the bias is caused within these models. To address this research gap, our study performs a top-down analysis on representative CTR models. Through blocking different components of a trained CTR model one by one, we identify the key contribution of the linear component to feature-level bias. We conduct a theoretical analysis of the learning process for the weights in the linear component, revealing how group-wise properties of training data influence them. Our experimental and statistical analyses demonstrate a strong correlation between imbalanced positive sample ratios across item groups and feature-level bias. Based on this understanding, we propose a minimally invasive yet effective strategy to counteract feature-level bias in CTR models by removing the biased linear weights from trained models. Additionally, we present a linear weight adjusting strategy that requires fewer random exposure records than relevant debiasing methods. The superiority of our proposed strategies are validated through extensive experiments on three real-world datasets.|常见的点进率预测推荐模型往往表现出特征层面的偏差，导致项目组之间的不公平推荐和对用户的不准确推荐。虽然现有的方法通过调整 CTR 模型的学习来解决这个问题，例如通过额外的优化目标，但是他们没有考虑这些模型中偏差是如何产生的。为了解决这一研究差距，我们的研究对代表性的 CTR 模型进行了自上而下的分析。通过对训练好的 CTR 模型的不同分量逐个进行分块，我们确定了线性分量对特征级偏差的主要贡献。我们对线性分量中权重的学习过程进行了理论分析，揭示了训练数据的群体性质如何影响它们。我们的实验和统计分析表明，项目组间不平衡的正样本比率和特征水平偏差之间有很强的相关性。基于这一认识，我们提出了一种微创但有效的策略，通过去除训练后的模型中有偏的线性权重来抵消 CTR 模型中的特征水平偏差。此外，我们提出了一个线性权重调整策略，需要较少的随机曝光记录比相关的消偏方法。通过在三个实际数据集上的大量实验，验证了所提策略的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+and+Counteracting+Feature-Level+Bias+in+Click-Through+Rate+Prediction)|0|
|[A Demonstration of Decentralized Search Over Solid Personal Online Datastores](https://doi.org/10.1145/3589335.3651248)|Mohamed Ragab, Yury Savateev, Helen Oliver, Reza Moosaei, Thanassis Tiropanis, Alexandra Poulovassilis, Adriane Chapman, George Roussos||In the modern Web landscape, data privacy and control are increasingly unattainable for users. Solid 1, a decentralized Web ecosystem, restores individual privacy and control by separating data from applications, allowing integration across applications while enabling users to control access. The growth in Solid's decentralized pods, and the escalating amounts of data stored in them, necessitate a decentralized search mechanism to query data within personal datastores, while respecting varying access constraints. This demo paper presents our decentralized search system (ESPRESSO) for querying RDF and non-RDF data in Solid datastores, tackling challenges like query propagation, data indexing, privacy, and results aggregation.|在现代 Web 环境中，用户越来越难以实现数据隐私和控制。Solid 1，一个分散的 Web 生态系统，通过将数据从应用程序中分离出来，恢复个人隐私和控制，允许跨应用程序集成，同时允许用户控制访问。Solid 的分散式豆荚的增长，以及其中存储的数据量的不断增加，需要一种分散式搜索机制来查询个人数据存储中的数据，同时尊重不同的访问限制。本演示文章介绍了我们的分散式搜索系统(ESPRESSO) ，用于在 Solid 数据存储中查询 RDF 和非 RDF 数据，解决查询传播、数据索引、隐私和结果聚合等挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Demonstration+of+Decentralized+Search+Over+Solid+Personal+Online+Datastores)|0|
|[Neural Contextual Bandits for Personalized Recommendation](https://doi.org/10.1145/3589335.3641241)|Yikun Ban, Yunzhe Qi, Jingrui He||In the dynamic landscape of online businesses, recommender systems are pivotal in enhancing user experiences. While traditional approaches have relied on static supervised learning, the quest for adaptive, user-centric recommendations has led to the emergence of the formulation of contextual bandits. This tutorial investigates the contextual bandits as a powerful framework for personalized recommendations. We delve into the challenges, advanced algorithms and theories, collaborative strategies, and open challenges and future prospects within this field. Different from existing related tutorials, (1) we focus on the exploration perspective of contextual bandits to alleviate the ``Matthew Effect'' in the recommender systems, i.e., the rich get richer and the poor get poorer, concerning the popularity of items; (2) in addition to the conventional linear contextual bandits, we will also dedicated to neural contextual bandits which have emerged as an important branch in recent years, to investigate how neural networks benefit contextual bandits for personalized recommendation both empirically and theoretically; (3) we will cover the latest topic, collaborative neural contextual bandits, to incorporate both user heterogeneity and user correlations customized for recommender system; (4) we will provide and discuss the new emerging challenges and open questions for neural contextual bandits with applications in the personalized recommendation, especially for large neural models.|在动态的在线商务环境中，推荐系统是增强用户体验的关键。虽然传统的方法依赖于静态的监督式学习，但是对适应性的、以用户为中心的建议的追求已经导致了关联强盗的出现。本教程研究了上下文强盗作为个性化推荐的强大框架。我们深入研究这个领域的挑战、先进的算法和理论、协作策略、开放的挑战和未来的前景。与现有的相关教程不同的是，(1)我们着重从语境土匪的角度来探索，以缓解推荐系统中的“马太效应”。(2)除了传统的线性情境强盗，我们还将致力于近年来兴起的一个重要分支——神经情境强盗，研究神经网络如何在实证和理论上有利于情境强盗的个性化推荐;(3)我们将涵盖最新的主题，协作神经上下文盗贼，以纳入用户异质性和用户相关性定制的推荐系统; (4)我们将提供和讨论新出现的挑战和神经上下文盗贼的开放性问题的应用，在个性化推荐，特别是在大型神经模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Contextual+Bandits+for+Personalized+Recommendation)|0|
|[Causality-driven User Modeling for Sequential Recommendations over Time](https://doi.org/10.1145/3589335.3651896)|Xingming Chen, Qing Li||Contemporary sequential recommendation systems predominantly leverage statistical correlations derived from user interaction histories to predict future preferences. However, these correlations often mask implicit challenges. On the one hand, user data is frequently plagued by implicit, noisy feedback, misdirecting users towards items that fail to align with their actual interests, which is magnified in sequential recommendation contexts. On the other hand, prevalent methods tend to over-rely on similarity-based attention mechanisms across item pairs, which are prone to utilizing heuristic shortcuts, thereby leading to suboptimal recommendation. To tackle these issues, we put forward a causality-driven user modeling approach for sequential recommendation, which pivots towards a causal perspective. Specifically, we involves the application of a causal graph to identify confounding factors that give rise to spurious correlations and to isolate conceptual variables that causally encapsulate user preferences. By learning the representation of these disentangled causal variables at the conceptual level, we can distinguish between causal and non-causal associations while preserving the inherent sequential nature of user behaviors. This enables us to ascertain which elements are critical and which may induce unintended biases. The framework of our method can be compatible with various mainstream sequential models, which offers a robust foundation for reconstructing more accurate and meaningful user and item representations driven by causality.|当代的顺序推荐系统主要利用来自用户交互历史的统计相关性来预测未来的偏好。然而，这些相关性往往掩盖了隐含的挑战。一方面，用户数据经常受到隐含的、嘈杂的反馈的困扰，将用户误导到与他们的实际兴趣不一致的项目上，这种情况在顺序推荐上下文中被放大。另一方面，流行的方法倾向于过度依赖跨项目对的基于相似性的注意机制，这种机制倾向于利用启发式快捷方式，从而导致次优推荐。为了解决这些问题，我们提出了一种基于因果关系的连续推荐用户建模方法。具体来说，我们涉及到一个因果图的应用，以确定产生虚假相关性的混杂因素，并隔离概念变量，因果封装用户偏好。通过在概念层面上学习这些非纠缠因果变量的表示，我们可以区分因果关联和非因果关联，同时保留用户行为固有的序列性质。这使我们能够确定哪些因素是关键的，哪些因素可能引起意想不到的偏见。该方法的框架可以与各种主流的序列模型兼容，为因果关系驱动下的用户和项目表示重构提供了坚实的基础。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causality-driven+User+Modeling+for+Sequential+Recommendations+over+Time)|0|
|[ConvSDG: Session Data Generation for Conversational Search](https://doi.org/10.1145/3589335.3651940)|Fengran Mo, Bole Yi, Kelong Mao, Chen Qu, Kaiyu Huang, JianYun Nie||Conversational search provides a more convenient interface for users to search by allowing multi-turn interaction with the search engine. However, the effectiveness of the conversational dense retrieval methods is limited by the scarcity of training data required for their fine-tuning. Thus, generating more training conversational sessions with relevant labels could potentially improve search performance. Based on the promising capabilities of large language models (LLMs) on text generation, we propose ConvSDG, a simple yet effective framework to explore the feasibility of boosting conversational search by using LLM for session data generation. Within this framework, we design dialogue/session-level and query-level data generation with unsupervised and semi-supervised learning, according to the availability of relevance judgments. The generated data are used to fine-tune the conversational dense retriever. Extensive experiments on four widely used datasets demonstrate the effectiveness and broad applicability of our ConvSDG framework compared with several strong baselines.|通过允许与搜索引擎进行多次交互，对话式搜索为用户提供了更方便的搜索界面。然而，会话密集检索方法的有效性受限于其微调所需的训练数据的稀缺性。因此，使用相关标签生成更多的训练对话会话可能会提高搜索性能。基于大语言模型(LLM)在文本生成方面的潜在能力，本文提出了一个简单而有效的框架，用于探索利用 LLM 进行会话数据生成来提高会话搜索的可行性。在这个框架内，我们根据相关性判断的可用性，设计无监督和半监督学习的对话/会话级和查询级数据生成。生成的数据用于对会话密集型检索器进行微调。在四个广泛使用的数据集上进行的大量实验表明，与几个强大的基线相比，我们的 ConvSDG 框架是有效的，并且具有广泛的适用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ConvSDG:+Session+Data+Generation+for+Conversational+Search)|0|
|[How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation](https://doi.org/10.1145/3589335.3651955)|Lixi Zhu, Xiaowen Huang, Jitao Sang||Conversational Recommender System (CRS) interacts with users through natural language to understand their preferences and provide personalized recommendations in real-time. CRS has demonstrated significant potential, prompting researchers to address the development of more realistic and reliable user simulators as a key focus. Recently, the capabilities of Large Language Models (LLMs) have attracted a lot of attention in various fields. Simultaneously, efforts are underway to construct user simulators based on LLMs. While these works showcase innovation, they also come with certain limitations that require attention. In this work, we aim to analyze the limitations of using LLMs in constructing user simulators for CRS, to guide future research. To achieve this goal, we conduct analytical validation on the notable work, iEvaLM. Through multiple experiments on two widely-used datasets in the field of conversational recommendation, we highlight several issues with the current evaluation methods for user simulators based on LLMs: (1) Data leakage, which occurs in conversational history and the user simulator's replies, results in inflated evaluation results. (2) The success of CRS recommendations depends more on the availability and quality of conversational history than on the responses from user simulators. (3) Controlling the output of the user simulator through a single prompt template proves challenging. To overcome these limitations, we propose SimpleUserSim, employing a straightforward strategy to guide the topic toward the target items. Our study validates the ability of CRS models to utilize the interaction information, significantly improving the recommendation results.|会话推荐系统(CRS)通过自然语言与用户互动，了解他们的偏好，并实时提供个性化的建议。CRS 显示了巨大的潜力，促使研究人员将开发更加现实和可靠的用户模拟器作为一个关键重点。近年来，大语言模型(LLM)的性能在各个领域引起了广泛的关注。同时，正在努力构建基于 LLM 的用户模拟器。虽然这些作品展示了创新，但它们也有一定的局限性，需要注意。本文旨在分析 LLM 在构建 CRS 用户模拟器方面的局限性，以指导未来的研究工作。为了实现这个目标，我们对值得注意的工作 iEvaLM 进行了分析验证。通过对会话推荐领域两个广泛使用的数据集进行多次实验，突出了目前基于 LLM 的用户模拟器评估方法存在的几个问题: (1)会话历史和用户模拟器回复中出现的数据泄漏导致评估结果膨胀。(2) CRS 推荐的成功与否更多地取决于会话历史的可用性和质量，而非来自用户模拟器的响应。(3)通过一个单一的提示模板控制用户模拟器的输出具有挑战性。为了克服这些限制，我们提出了 SimpleUserSim，它采用了一种简单的策略来将主题引导到目标项。我们的研究验证了 CRS 模型利用交互信息的能力，显著提高了推荐结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Reliable+is+Your+Simulator?+Analysis+on+the+Limitations+of+Current+LLM-based+User+Simulators+for+Conversational+Recommendation)|0|
|[Mining Exploratory Queries for Conversational Search](https://doi.org/10.1145/3589334.3645424)|Wenhan Liu, Ziliang Zhao, Yutao Zhu, Zhicheng Dou||Users' queries are usually vague, and their search intents tend to be ambiguous, thereby needing search clarification to clarify users' current intent by asking a clarifying question and providing several clickable sub-intent items as clarification options. However, in addition to drilling down the current query, users may also have exploratory needs that diverge from their current intent. For example, a user searching for the query "Cartier women watches'' may also potentially want to explore some parallel information by issuing queries such as "Rolex women watches'' or "Cartier women bracelets'', named exploratory queries in this paper. These exploratory needs are common during the search process yet cannot be satisfied by current search clarification approaches which typically stick to the sub-intents of the query. This paper focuses on mining exploratory queries as additional options to meet users' exploratory needs in conversational search systems. Specifically, we first design a rule-based model that generates exploratory queries based on the current query's top retrieved documents. Then, we propose using the data generated by the rule-based model to train a neural generation model through multi-task learning for further generalization. Finally, we borrow the in-context learning ability of the large language model to generate exploratory queries based on prompt engineering. We constructed an evaluation dataset based on human annotations and conduct an extensive set of experiments. The results show that our proposed methods generate higher-quality exploratory queries compared with several baselines.|用户的查询通常是模糊的，他们的搜索意图往往是模糊的，因此需要通过提出一个澄清问题和提供几个可点击的子意图项作为澄清选项来澄清用户的当前意图。然而，除了深入当前查询之外，用户还可能有与当前意图不同的探索性需求。例如，搜索“卡地亚女性手表”的用户可能也想通过发出“劳力士女性手表”或“卡地亚女性手镯”等查询来探索一些并行信息，这些查询在本文中被命名为探索性查询。这些探索性需求在搜索过程中很常见，但当前的搜索澄清方法不能满足这些需求，因为这些方法通常坚持查询的子意图。在会话搜索系统中，为了满足用户的探索性需求，本文将探索性查询作为附加选项进行挖掘。具体来说，我们首先设计一个基于规则的模型，该模型基于当前查询的最高检索文档生成探索性查询。然后，我们提出利用基于规则的模型生成的数据，通过多任务学习训练神经生成模型，以便进一步推广。最后，借鉴大型语言模型的上下文学习能力，基于快速工程生成探索性查询。我们构建了一个基于人工注释的评价数据集，并进行了大量的实验。结果表明，与几个基线相比，我们提出的方法产生了更高质量的探索性查询。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mining+Exploratory+Queries+for+Conversational+Search)|0|
|[An In-depth Investigation of User Response Simulation for Conversational Search](https://doi.org/10.1145/3589334.3645447)|Zhenduo Wang, Zhichao Xu, Vivek Srikumar, Qingyao Ai||Conversational search has seen increased recent attention in both the IR and NLP communities. It seeks to clarify and solve users' search needs through multi-turn natural language interactions. However, most existing systems are trained and demonstrated with recorded or artificial conversation logs. Eventually, conversational search systems should be trained, evaluated, and deployed in an open-ended setting with unseen conversation trajectories. A key challenge is that training and evaluating such systems both require a human-in-the-loop, which is expensive and does not scale. One strategy is to simulate users, thereby reducing the scaling costs. However, current user simulators are either limited to only responding to yes-no questions from the conversational search system or unable to produce high-quality responses in general. In this paper, we show that existing user simulation systems could be significantly improved by a smaller finetuned natural language generation model. However, rather than merely reporting it as the new state-of-the-art, we consider it a strong baseline and present an in-depth investigation of simulating user response for conversational search. Our goal is to supplement existing work with an insightful hand-analysis of unsolved challenges by the baseline and propose our solutions. The challenges we identified include (1) a blind spot that is difficult to learn, and (2) a specific type of misevaluation in the standard setup. We propose a new generation system to effectively cover the training blind spot and suggest a new evaluation setup to avoid misevaluation. Our proposed system leads to significant improvements over existing systems and large language models such as GPT-4. Additionally, our analysis provides insights into the nature of user simulation to facilitate future work.|会话搜索最近在 IR 和 NLP 社区都受到了越来越多的关注。它试图通过多回合的自然语言交互来阐明和解决用户的搜索需求。然而，大多数现有的系统都是通过记录或人工会话日志进行训练和演示的。最终，会话搜索系统应该训练，评估，并部署在一个开放的设置与看不见的会话轨迹。一个关键的挑战是，培训和评估这样的系统都需要一个人在回路，这是昂贵的，而且没有规模。一种策略是模拟用户，从而降低缩放成本。然而，目前的用户模拟器要么仅限于回答会话搜索系统中的“是”或“否”问题，要么一般无法产生高质量的回答。在本文中，我们表明，现有的用户模拟系统可以显着改善，一个较小的微调自然语言生成模型。然而，我们并不仅仅把它作为最新的技术来报告，我们认为它是一个强有力的基准，并且提出了一个模拟用户对会话搜索的反应的深入研究。我们的目标是通过对未解决的挑战进行深刻的手工分析来补充现有的工作，并提出我们的解决方案。我们确定的挑战包括(1)一个难以学习的盲点，和(2)在标准设置中一个特定类型的错误评估。我们提出了一个新的系统，以有效地覆盖训练盲点，并提出了一个新的评估设置，以避免错误的评估。我们提出的系统导致了对现有系统和大型语言模型(如 GPT-4)的重大改进。此外，我们的分析提供了对用户模拟的本质的洞察，以促进未来的工作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+In-depth+Investigation+of+User+Response+Simulation+for+Conversational+Search)|0|
|[Rethinking Cross-Domain Sequential Recommendation under Open-World Assumptions](https://doi.org/10.1145/3589334.3645351)|Wujiang Xu, Qitian Wu, Runzhong Wang, Mingming Ha, Qiongxu Ma, Linxun Chen, Bing Han, Junchi Yan||Cross-Domain Sequential Recommendation (CDSR) methods aim to tackle the data sparsity and cold-start problems present in Single-Domain Sequential Recommendation (SDSR). Existing CDSR works design their elaborate structures relying on overlapping users to propagate the cross-domain information. However, current CDSR methods make closed-world assumptions, assuming fully overlapping users across multiple domains and that the data distribution remains unchanged from the training environment to the test environment. As a result, these methods typically result in lower performance on online real-world platforms due to the data distribution shifts. To address these challenges under open-world assumptions, we design an Adaptive Multi-Interest Debiasing framework for cross-domain sequential recommendation (AMID), which consists of a multi-interest information module (MIM) and a doubly robust estimator (DRE). Our framework is adaptive for open-world environments and can improve the model of most off-the-shelf single-domain sequential backbone models for CDSR. Our MIM establishes interest groups that consider both overlapping and non-overlapping users, allowing us to effectively explore user intent and explicit interest. To alleviate biases across multiple domains, we developed the DRE for the CDSR methods. We also provide a theoretical analysis that demonstrates the superiority of our proposed estimator in terms of bias and tail bound, compared to the IPS estimator used in previous work.|跨域序列推荐(CDSR)方法旨在解决单域序列推荐(SDSR)中存在的数据稀疏和冷启动问题。现有的 CDSR 作品设计了复杂的结构，依靠重叠用户传播跨域信息。然而，目前的 CDSR 方法是封闭世界的假设，假设用户在多个域之间完全重叠，并且从训练环境到测试环境的数据分布保持不变。因此，由于数据分布的变化，这些方法通常会导致在线真实世界平台上的性能降低。为了解决开放世界条件下的这些挑战，我们设计了一个跨域序列推荐(AMID)的自适应多兴趣去偏框架，该框架由一个多兴趣信息模块(MIM)和一个双鲁棒估计器(DRE)组成。我们的框架是适应开放世界的环境，可以改进大多数现成的单域顺序骨干模型的 CDSR。我们的 MIM 建立了考虑重叠和非重叠用户的兴趣组，允许我们有效地探索用户意图和明确的兴趣。为了减轻跨多个域的偏差，我们为 CDSR 方法开发了 DRE。我们还提供了一个理论分析，证明了我们提出的估计器在偏差和尾界方面的优越性，相比之下，在以前的工作中使用的 IPS 估计器。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Cross-Domain+Sequential+Recommendation+under+Open-World+Assumptions)|0|
|[Not All Embeddings are Created Equal: Towards Robust Cross-domain Recommendation via Contrastive Learning](https://doi.org/10.1145/3589334.3645357)|Wenhao Yang, Yingchun Jian, Yibo Wang, Shiyin Lu, Lei Shen, Bing Wang, Haihong Tang, Lijun Zhang||Cross-domain recommendation (CDR) aims to leverage the rich information from the source domain to enhance recommendation performance in the target domain. However, the data imbalance problem inherent across different domains compromises the effectiveness of CDR approaches, posing a significant challenge to CDR. Most current CDR methodologies focus on creating better user embeddings for the target domain, yet usually neglect the inconsistency in user activities due to data imbalance. As a result, the process of creating user embeddings tends to prioritize users with more frequent interactions and leave less active users underserved, leading these CDR methods to struggle in making accurate recommendations for those with fewer interactions. Such bias in creating embeddings reveals the fact that ''not all embeddings are created equal'' in CDR, which serves as the primary motivation of this study. Inspired by the recent development of contrastive learning, this paper proposes User-aware Contrastive Learning for Robust cross-domain recommendation (UCLR), enhancing the robustness of cross-domain recommendation. Specifically, our proposed method consists of two sub-modules: (i) pretrained global embedding, where the global user embeddings are pretrained across all the domains; (ii) contrastive dual-stream collaborative autoencoder, where more equal user embeddings are generated by optimizing contrastive loss with individualized temperatures. To further improve the performance of our method in each domain, we finetune the whole framework of UCLR based on Low-Rank Adaptation (LoRA). Theoretically, our method is equipped with a provable convergence guarantee during the contrastive learning stage. Furthermore, we also conduct comprehensive experiments on real-world datasets to validate the effectiveness of our proposed method.|跨域推荐(CDR)旨在利用源域中的丰富信息来提高目标域中的推荐性能。然而，不同领域固有的数据不平衡问题影响了 CDR 方法的有效性，对 CDR 提出了严峻的挑战。目前大多数 CDR 方法侧重于为目标域创建更好的用户嵌入，但往往忽视了由于数据不平衡而导致的用户活动的不一致性。因此，创建用户嵌入的过程往往会优先考虑交互更频繁的用户，使得活跃度较低的用户得不到充分的服务，导致这些 CDR 方法难以为交互较少的用户提供准确的建议。创建嵌入的这种偏见揭示了 CDR 中“不是所有的嵌入都是平等的”这一事实，这是本研究的主要动机。受对比学习的启发，本文提出了基于用户感知的鲁棒跨域推荐对比学习(UCLR)方法，增强了跨域推荐的鲁棒性。具体而言，我们提出的方法由两个子模块组成: (i)预训练的全局嵌入，其中全局用户嵌入在所有领域预训练; (ii)对比双流协同自动编码器，其中更多的相等的用户嵌入是通过优化对比损失与个性化温度。为了进一步提高我们的方法在每个领域的性能，我们微调了整个框架的 UCLR 的基础上低秩自适应(LoRA)。理论上，我们的方法在对比学习阶段具有可证明的收敛性保证。此外，我们还对真实世界的数据集进行了全面的实验，以验证我们提出的方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Not+All+Embeddings+are+Created+Equal:+Towards+Robust+Cross-domain+Recommendation+via+Contrastive+Learning)|0|
|[Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation](https://doi.org/10.1145/3589334.3645380)|Yongqiang Han, Hao Wang, Kefan Wang, Likang Wu, Zhi Li, Wei Guo, Yong Liu, Defu Lian, Enhong Chen||In recommendation systems, users frequently engage in multiple types of behaviors, such as clicking, adding to a cart, and purchasing. However, with diversified behavior data, user behavior sequences will become very long in the short term, which brings challenges to the efficiency of the sequence recommendation model. Meanwhile, some behavior data will also bring inevitable noise to the modeling of user interests. To address the aforementioned issues, firstly, we develop the Efficient Behavior Sequence Miner (EBM) that efficiently captures intricate patterns in user behavior while maintaining low time complexity and parameter count. Secondly, we design hard and soft denoising modules for different noise types and fully explore the relationship between behaviors and noise. Finally, we introduce a contrastive loss function along with a guided training strategy to compare the valid information in the data with the noisy signal, and seamlessly integrate the two denoising processes to achieve a high degree of decoupling of the noisy signal. Sufficient experiments on real-world datasets demonstrate the effectiveness and efficiency of our approach in dealing with multi-behavior sequential recommendation.|在推荐系统中，用户经常参与多种类型的行为，比如点击、添加到购物车和购买。然而，随着用户行为数据的多样化，短期内用户行为序列将变得非常长，这对序列推荐模型的有效性提出了挑战。同时，一些行为数据也会给用户兴趣建模带来不可避免的干扰。为了解决上述问题，首先，我们开发了高效行为序列挖掘器(EBM) ，它能够有效地捕获用户行为中的复杂模式，同时保持较低的时间复杂度和参数计数。其次，针对不同的噪声类型设计了软硬件去噪模块，并充分探讨了行为与噪声之间的关系。最后，引入对比损失函数和引导训练策略，将数据中的有效信息与噪声信号进行比较，并将两个去噪过程无缝结合，实现了噪声信号的高度解耦。通过对实际数据集的大量实验，验证了该方法处理多行为顺序推荐的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Noise-Decoupling+for+Multi-Behavior+Sequential+Recommendation)|0|
|[Debiasing Recommendation with Personal Popularity](https://doi.org/10.1145/3589334.3645421)|Wentao Ning, Reynold Cheng, Xiao Yan, Ben Kao, Nan Huo, Nur Al Hasan Haldar, Bo Tang||Global popularity (GP) bias is the phenomenon that popular items are recommended much more frequently than they should be, which goes against the goal of providing personalized recommendations and harms user experience and recommendation accuracy. Many methods have been proposed to reduce GP bias but they fail to notice the fundamental problem of GP, i.e., it considers popularity from a global perspective of all users and uses a single set of popular items, and thus cannot capture the interests of individual users. As such, we propose a user-aware version of item popularity named personal popularity (PP), which identifies different popular items for each user by considering the users that share similar interests. As PP models the preferences of individual users, it naturally helps to produce personalized recommendations and mitigate GP bias. To integrate PP into recommendation, we design a general personal popularity aware counterfactual (PPAC) framework, which adapts easily to existing recommendation models. In particular, PPAC recognizes that PP and GP have both direct and indirect effects on recommendations and controls direct effects with counterfactual inference techniques for unbiased recommendations. All codes and datasets are available at <https://github.com/Stevenn9981/PPAC>.|全球流行度偏差(GP 偏差)是指流行项目被推荐的频率大大高于其应有频率的现象，与提供个性化推荐的目标背道而驰，损害了用户体验和推荐准确性。人们提出了许多减少 GP 偏差的方法，但都没有注意到 GP 的根本问题，即它从全局的角度考虑所有用户的受欢迎程度，使用单一的流行项目集，因此不能捕捉到个体用户的兴趣。因此，我们提出了一个名为个人知名度(PP)的项目知名度的用户感知版本，它通过考虑具有相似兴趣的用户来为每个用户识别不同的流行项目。作为 PP 模型的个人用户的偏好，它自然有助于产生个性化的推荐和减轻 GP 偏见。为了将 PP 整合到推荐中，我们设计了一个通用的个人知名度反事实(PPAC)框架，该框架很容易适应现有的推荐模型。特别是，PPAC 认识到 PP 和 GP 对建议有直接和间接的影响，并通过反事实推理技术控制对无偏见建议的直接影响。所有代码和数据集都可以在 <  https://github.com/stevenn9981/ppac 获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Debiasing+Recommendation+with+Personal+Popularity)|0|
|[Negative Sampling in Next-POI Recommendations: Observation, Approach, and Evaluation](https://doi.org/10.1145/3589334.3645681)|HongKyun Bae, Yebeen Kim, Hyunjoon Kim, SangWook Kim||To recommend the points of interest (POIs) that a user would check-in next, most deep-learning (DL)-based existing studies have employed random negative (RN) sampling during model training. In this paper, we claim and validate that, as the training proceeds, such an RN sampling in reality performs as sampling easy negative (EN) POIs (i.e., EN sampling) that a user was highly unlikely to check-in at her check-in time point. Furthermore, we verify that EN sampling is more disadvantageous in improving the accuracy than sampling hard negative (HN) POIs (i.e., HN sampling) that a user was highly likely to check-in. To address this limitation, we present the novel concept of the Degree of Positiveness (DoP), which can be formulated by two factors: (i) the degree to which a POI has the characteristics preferred by a user; (ii) the geographical distance between a user and a POI. Then, we propose a new model-training scheme based on HN sampling by using DoP. Using real-world datasets (i.e., NYC, TKY, and Brightkite), we demonstrate that all the state-of-the-art models trained by our scheme showed dramatic improvements in accuracy by up to about 82.8%.|为了推荐用户接下来要检入的兴趣点(POI) ，大多数基于深度学习(DL)的现有研究在模型训练中采用了随机负取样(RN)。在本文中，我们声称并验证，随着培训的进行，这样的 RN 采样实际上表现为采样容易阴性(EN) POI (即 EN 采样) ，用户不太可能在她的签入时间点签入。此外，我们验证了 EN 抽样在提高准确性方面比抽样硬负(HN) POI (即 HN 抽样)更不利，因为用户很可能签到。为了解决这个限制，我们提出了积极程度(DoP)的新概念，可以由两个因素来表述: (i) POI 具有用户首选特征的程度; (ii)用户与 POI 之间的地理距离。然后，利用 DoP 提出了一种新的基于 HN 抽样的模型训练方案。使用真实世界的数据集(例如，NYC、 TKY 和 Brightkite) ，我们证明了所有经过我们的方案训练的最先进的模型在准确性方面显示出了高达82.8% 的显著提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Negative+Sampling+in+Next-POI+Recommendations:+Observation,+Approach,+and+Evaluation)|0|
|[Towards Personalized Privacy: User-Governed Data Contribution for Federated Recommendation](https://doi.org/10.1145/3589334.3645690)|Liang Qu, Wei Yuan, Ruiqi Zheng, Lizhen Cui, Yuhui Shi, Hongzhi Yin||Federated recommender systems (FedRecs) have gained significant attention for their potential to protect user's privacy by keeping user privacy data locally and only communicating model parameters/gradients to the server. Nevertheless, the currently existing architecture of FedRecs assumes that all users have the same 0-privacy budget, i.e., they do not upload any data to the server, thus overlooking those users who are less concerned about privacy and are willing to upload data to get a better recommendation service. To bridge this gap, this paper explores a user-governed data contribution federated recommendation architecture where users are free to take control of whether they share data and the proportion of data they share to the server. To this end, this paper presents a cloud-device collaborative graph neural network federated recommendation model, named CDCGNNFed. It trains user-centric ego graphs locally, and high-order graphs based on user-shared data in the server in a collaborative manner via contrastive learning. Furthermore, a graph mending strategy is utilized to predict missing links in the graph on the server, thus leveraging the capabilities of graph neural networks over high-order graphs. Extensive experiments were conducted on two public datasets, and the results demonstrate the effectiveness of the proposed method.|联邦推荐系统(FedRecs)通过将用户隐私数据保存在本地，并且只与服务器通信模型参数/梯度，从而保护用户隐私，因此受到了广泛关注。然而，目前现有的 FedRecs 架构假设所有用户都有相同的零隐私预算，也就是说，他们不向服务器上传任何数据，从而忽略了那些不太关心隐私并愿意上传数据以获得更好的推荐服务的用户。为了弥合这一差距，本文探索了一种用户治理的数据贡献联邦推荐体系结构，在该体系结构中，用户可以自由控制是否共享数据以及共享数据到服务器的比例。为此，本文提出了一种云设备协同图形神经网络联邦推荐模型 CDCGNNFed。它通过对比学习，以协作的方式在本地培训以用户为中心的自我图形和基于服务器中用户共享数据的高阶图形。此外，利用图修正策略来预测服务器上图中缺失的链接，从而利用图神经网络对高阶图的能力。在两个公共数据集上进行了广泛的实验，实验结果表明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Personalized+Privacy:+User-Governed+Data+Contribution+for+Federated+Recommendation)|0|
|[A Semi-supervised Multi-channel Graph Convolutional Network for Query Classification in E-commerce](https://doi.org/10.1145/3589335.3648302)|Chunyuan Yuan, Ming Pang, Zheng Fang, Xue Jiang, Changping Peng, Zhangang Lin||Query intent classification is an essential module for customers to quickly find desired products on the e-commerce application. Most existing query intent classification methods rely on the users' click behavior as a supervised signal to construct training samples. However, these methods based entirely on posterior labels may lead to serious category imbalance problems because of the Matthew effect in click samples. Compared with popular categories, it is difficult for products under long-tail categories to obtain traffic and user clicks, which makes the models unable to detect users' intent for products under long-tail categories. This in turn aggravates the problem that long-tail categories cannot obtain traffic, forming a vicious circle. In addition, due to the randomness of the user's click, the posterior label is unstable for the query with similar semantics, which makes the model very sensitive to the input, leading to an unstable and incomplete recall of categories. In this paper, we propose a novel Semi-supervised Multi-channel Graph Convolutional Network (SMGCN) to address the above problems from the perspective of label association and semi-supervised learning. SMGCN extends category information and enhances the posterior label by utilizing the similarity score between the query and categories. Furthermore, it leverages the co-occurrence and semantic similarity graph of categories to strengthen the relations among labels and weaken the influence of posterior label instability. We conduct extensive offline and online A/B experiments, and the experimental results show that SMGCN significantly outperforms the strong baselines, which shows its effectiveness and practicality.|查询意图分类是客户在电子商务应用程序中快速查找所需产品的重要模块。现有的查询意图分类方法大多依赖于用户的点击行为作为监督信号来构造训练样本。然而，由于点击样本中的马太效应，这些完全基于后验标签的方法可能会导致严重的类别不平衡问题。与流行类别相比，长尾类别下的产品很难获得流量和用户点击量，这使得模型无法检测出用户对长尾类别下产品的意图。这反过来又加剧了长尾类无法获得流量的问题，形成了一个恶性循环。此外，由于用户点击的随机性，对于语义相似的查询，后验标签是不稳定的，这使得模型对输入非常敏感，导致类别的不稳定和不完全召回。在本文中，我们提出了一个新的半监督多通道图卷积网络(SMGCN) ，从标签关联和半监督学习的角度来解决上述问题。SMGCN 利用查询与类别之间的相似度评分扩展类别信息，增强后验标签。此外，利用类别的共现性和语义相似性图增强了标签之间的关系，减弱了后验标签不稳定性的影响。我们进行了大量的离线和在线 A/B 实验，实验结果表明 SMGCN 的性能明显优于强基线，表明了它的有效性和实用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Semi-supervised+Multi-channel+Graph+Convolutional+Network+for+Query+Classification+in+E-commerce)|0|
|[Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale](https://doi.org/10.1145/3589335.3648304)|Wei Wen, KuangHung Liu, Igor Fedorov, Xin Zhang, Hang Yin, Weiwei Chu, Kaveh Hassani, Mengying Sun, Jiang Liu, Xu Wang, Lin Jiang, Yuxin Chen, Buyun Zhang, Xi Liu, Dehua Cheng, Zhengxing Chen, Guang Zhao, Fangqiu Han, Jiyan Yang, Yuchen Hao, Liang Xiong, WenYen Chen||Neural Architecture Search (NAS) has demonstrated its efficacy in computer vision and potential for ranking systems. However, prior work focused on academic problems, which are evaluated at small scale under well-controlled fixed baselines. In industry system, such as ranking system in Meta, it is unclear whether NAS algorithms from the literature can outperform production baselines because of: (1) scale - Meta ranking systems serve billions of users, (2) strong baselines - the baselines are production models optimized by hundreds to thousands of world-class engineers for years since the rise of deep learning, (3) dynamic baselines - engineers may have established new and stronger baselines during NAS search, and (4) efficiency - the search pipeline must yield results quickly in alignment with the productionization life cycle. In this paper, we present Rankitect, a NAS software framework for ranking systems at Meta. Rankitect seeks to build brand new architectures by composing low level building blocks from scratch. Rankitect implements and improves state-of-the-art (SOTA) NAS methods for comprehensive and fair comparison under the same search space, including sampling-based NAS, one-shot NAS, and Differentiable NAS (DNAS). We evaluate Rankitect by comparing to multiple production ranking models at Meta. We find that Rankitect can discover new models from scratch achieving competitive tradeoff between Normalized Entropy loss and FLOPs. When utilizing search space designed by engineers, Rankitect can generate better models than engineers, achieving positive offline evaluation and online A/B test at Meta scale.|神经结构搜索(NAS)在计算机视觉方面的有效性和排序系统的潜力已得到证实。然而，以前的工作集中在学术问题，这是在小规模评估良好控制的固定基线。在工业系统中，例如 Meta 中的排名系统，目前还不清楚文献中的 NAS 算法是否能够胜过生产基线，因为: (1)规模-Meta 排名系统服务于数十亿用户，(2)强大的基线-基线是自深度学习兴起以来数十到数千名世界级工程师多年来优化的生产模型，(3)动态基线-工程师可能在 NAS 搜索期间建立了新的和更强大的基线，(4)效率-搜索管道必须迅速产生与生产生命周期一致的结果。在本文中，我们介绍了 Rankitect，一个用于 Meta 排序系统的 NAS 软件框架。Rankitect 通过从零开始构建低层次的构建块来构建全新的体系结构。Rankitect 实现并改进了最先进的(SOTA) NAS 方法，以便在同一搜索空间下进行全面和公平的比较，包括基于抽样的 NAS、一次性 NAS 和可微 NAS (DNAS)。我们评估 Rankitect 通过比较在 Meta 的多个生产排名模型。我们发现 Rankitect 可以从头开始发现新的模型，实现归一化熵损失和 FLOP 之间的竞争权衡。当利用工程师设计的搜索空间，Rankitect 可以产生比工程师更好的模型，实现积极的离线评价和在线 A/B 测试在 Meta 规模。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rankitect:+Ranking+Architecture+Search+Battling+World-class+Engineers+at+Meta+Scale)|0|
|[Aligned Side Information Fusion Method for Sequential Recommendation](https://doi.org/10.1145/3589335.3648308)|Shuhan Wang, Bin Shen, Xu Min, Yong He, Xiaolu Zhang, Liang Zhang, Jun Zhou, Linjian Mo||Combining contextual information (i.e., side information) of items beyond IDs has become an important way to improve the performance in recommender systems. Existing self-attention-based side information fusion methods can be categorized into early, late, and hybrid fusion. In practice, naive early fusion may interfere with the representation of IDs, resulting in negative effects, while late fusion misses effective interactions between IDs and side information. Some hybrid methods have been proposed to address these issues, but they only utilize side information in calculating attention scores, which may lead to information loss. To harness the full potential of side information without noisy interference, we propose an <u>A</u>ligned <u>S</u>ide <u>I</u>nformation <u>F</u>usion (ASIF) method for sequential recommendation, consisting of two parts: Fused Attention with Untied Positions and Representation Alignment. Specifically, we first decouple the positions to exclude the noisy interference in the attention scores. Secondly, we adopt the contrastive objective to maintain the semantic consistency between IDs and side information and then employ orthogonal decomposition to extract the homogeneous parts. By aligning the representations and fusing them together, ASIF makes full use of the side information without interfering with IDs. Offline experimental results on four datasets demonstrate the superiority of ASIF. Additionally, we successfully deployed the model in Alipay's advertising system and achieved 1.09% and 1.86% improvements on clicks and Cost Per Mille (CPM).|合并 ID 之外的条目的上下文信息(即边信息)已经成为提高推荐系统性能的重要方法。现有的基于自注意的边缘信息融合方法可分为早期融合、晚期融合和混合融合。在实践中，幼稚的早期融合可能干扰 ID 的表示，导致负面影响，而晚期融合错过了 ID 和侧信息之间的有效交互作用。针对这些问题，人们提出了一些混合方法，但这些方法只利用侧信息来计算注意分数，从而可能导致信息丢失。为了在没有噪音干扰的情况下充分利用侧面信息的潜力，我们提出了一种顺序推荐的信息融合(ASIF)方法，该方法由两部分组成: 融合注意力与不绑定位置和表示对齐。具体来说，我们首先解耦位置，以排除噪声干扰的注意分数。其次，采用对比目标来保持 ID 与边信息之间的语义一致性，然后采用正交分解来提取同质部分;。通过对齐表示并将它们融合在一起，ASIF 充分利用了侧信息，而不会干扰 ID。在四个数据集上的离线实验结果表明了 ASIF 算法的优越性。此外，我们成功地在支付宝的广告系统中部署了该模型，在点击量和每公里成本(CPM)上分别实现了1.09% 和1.86% 的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Aligned+Side+Information+Fusion+Method+for+Sequential+Recommendation)|0|
|[Discrete Conditional Diffusion for Reranking in Recommendation](https://doi.org/10.1145/3589335.3648313)|Xiao Lin, Xiaokai Chen, Chenyang Wang, Hantao Shu, Linfeng Song, Biao Li, Peng Jiang||Reranking plays a crucial role in modern multi-stage recommender systems by rearranging the initial ranking list to model interplay between items. Considering the inherent challenges of reranking such as combinatorial searching space, some previous studies have adopted the evaluator-generator paradigm, with a generator producing feasible sequences and a evaluator selecting the best one based on estimated listwise utility. Inspired by the remarkable success of diffusion generative models, this paper explores the potential of diffusion models for generating high-quality sequences in reranking. However, we argue that it is nontrivial to take diffusion models as the generator in the context of recommendation. Firstly, diffusion models primarily operate in continuous data space, differing from the discrete data space of item permutations. Secondly, the recommendation task is different from conventional generation tasks as the purpose of recommender systems is to fulfill user interests. Lastly, real-life recommender systems require efficiency, posing challenges for the inference of diffusion models. To overcome these challenges, we propose a novel Discrete Conditional Diffusion Reranking (DCDR) framework for recommendation. DCDR extends traditional diffusion models by introducing a discrete forward process with tractable posteriors, which adds noise to item sequences through step-wise discrete operations (e.g., swapping). Additionally, DCDR incorporates a conditional reverse process that generates item sequences conditioned on expected user responses. Extensive offline experiments conducted on public datasets demonstrate that DCDR outperforms state-of-the-art reranking methods. Furthermore, DCDR has been deployed in a real-world video app with over 300 million daily active users, significantly enhancing online recommendation quality.|重新排序在现代多阶段推荐系统中起着至关重要的作用，它通过重新排列初始排序列表来模拟项目之间的相互作用。考虑到组合搜索空间等重新排序的内在挑战，以前的一些研究采用了评价者-生成器范式，生成器生成可行序列，评价者根据估计的列表效用选择最佳序列。受扩散生成模型的巨大成功的启发，本文探讨了扩散模型在重新排序中生成高质量序列的潜力。然而，我们认为，在推荐的上下文中，将扩散模型作为生成器是不平凡的。首先，扩散模型主要在连续数据空间中运行，不同于项目排列的离散数据空间。其次，推荐任务不同于传统的生成任务，推荐系统的目的是满足用户的兴趣。最后，现实生活中的推荐系统需要效率，对扩散模型的推理提出了挑战。为了克服这些挑战，我们提出了一个新的离散条件扩散重排序(DCDR)框架的推荐。DCDR 扩展了传统的扩散模型，引入了一个离散的前向过程和易处理的后向过程，通过分步离散操作(例如交换)给项目序列增加了噪声。此外，DCDR 还包含一个条件反向过程，该过程根据预期的用户响应生成项目序列。在公共数据集上进行的大量离线实验表明，DCDR 优于最先进的重新排序方法。此外，DCDR 已经部署在一个现实世界的视频应用程序中，每天有超过3亿的活跃用户，大大提高了在线推荐的质量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Discrete+Conditional+Diffusion+for+Reranking+in+Recommendation)|0|
|[A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model](https://doi.org/10.1145/3589335.3648315)|Hao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, Yifan Zeng||In online advertising scenario, sellers often create multiple creatives to provide comprehensive demonstrations, making it essential to present the most appealing design to maximize the Click-Through Rate (CTR). However, sellers generally struggle to consider users preferences for creative design, leading to the relatively lower aesthetics and quantities compared to Artificial Intelligence (AI)-based approaches. Traditional AI-based approaches still face the same problem of not considering user information while having limited aesthetic knowledge from designers. In fact that fusing the user information, the generated creatives can be more attractive because different users may have different preferences. To optimize the results, the generated creatives in traditional methods are then ranked by another module named creative ranking model. The ranking model can predict the CTR score for each creative considering user features. However, the two above stages are regarded as two different tasks and are optimized separately. In this paper, we proposed a new automated Creative Generation pipeline for Click-Through Rate (CG4CTR) with the goal of improving CTR during the creative generation stage. Our contributions have 4 parts: 1) The inpainting mode in stable diffusion is firstly applied to creative generation task in online advertising scene. A self-cyclic generation pipeline is proposed to ensure the convergence of training. 2) Prompt model is designed to generate individualized creatives for different user groups, which can further improve the diversity and quality. 3) Reward model comprehensively considers the multimodal features of image and text to improve the effectiveness of creative ranking task, and it is also critical in self-cyclic pipeline. 4) The significant benefits obtained in online and offline experiments verify the significance of our proposed method.|在网络广告中，卖家经常创造出多种创意来提供全面的展示，因此展示最具吸引力的设计来最大限度地提高点进率是必不可少的。然而，与基于人工智能(AI)的方法相比，卖家通常难以考虑用户对创意设计的偏好，导致相对较低的审美和数量。传统的基于人工智能的方法仍然面临着同样的问题，没有考虑用户信息，同时从设计师有限的美学知识。事实上，融合用户信息，生成的创意可以更具吸引力，因为不同的用户可能有不同的偏好。为了优化结果，在传统方法中生成的创意，然后排名的另一个模块称为创意排名模型。该排名模型可以预测每个创意的 CTR 得分考虑用户的特点。然而，上述两个阶段被视为两个不同的任务，并分别进行了优化。在这篇文章中，我们提出了一个新的自动化创意生成流水线(cg4CTR)点进率，目标是在创意生成阶段提高点击率。本文的工作主要包括四个部分: 1)首次将稳定扩散的修补模式应用于网络广告场景的创意生成任务。为了保证训练的收敛性，提出了一种自循环发电流水线。2)提示模型针对不同的用户群体生成个性化的创意，从而进一步提高用户群体的多样性和质量。3)奖励模型综合考虑了图像和文本的多模态特征，提高了创造性排序任务的有效性，在自循环流水线中起着关键作用。4)在在线和离线实验中获得的显著好处验证了我们提出的方法的重要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+New+Creative+Generation+Pipeline+for+Click-Through+Rate+with+Stable+Diffusion+Model)|0|
|[NoteLLM: A Retrievable Large Language Model for Note Recommendation](https://doi.org/10.1145/3589335.3648314)|Chao Zhang, Shiwei Wu, Haoxin Zhang, Tong Xu, Yan Gao, Yao Hu, Enhong Chen||People enjoy sharing "notes" including their experiences within online communities. Therefore, recommending notes aligned with user interests has become a crucial task. Existing online methods only input notes into BERT-based models to generate note embeddings for assessing similarity. However, they may underutilize some important cues, e.g., hashtags or categories, which represent the key concepts of notes. Indeed, learning to generate hashtags/categories can potentially enhance note embeddings, both of which compress key note information into limited content. Besides, Large Language Models (LLMs) have significantly outperformed BERT in understanding natural languages. It is promising to introduce LLMs into note recommendation. In this paper, we propose a novel unified framework called NoteLLM, which leverages LLMs to address the item-to-item (I2I) note recommendation. Specifically, we utilize Note Compression Prompt to compress a note into a single special token, and further learn the potentially related notes' embeddings via a contrastive learning approach. Moreover, we use NoteLLM to summarize the note and generate the hashtag/category automatically through instruction tuning. Extensive validations on real scenarios demonstrate the effectiveness of our proposed method compared with the online baseline and show major improvements in the recommendation system of Xiaohongshu.|人们喜欢分享“笔记”，包括他们在网上社区的经历。因此，推荐符合用户兴趣的注意事项已成为一项关键任务。现有的在线方法仅将注释输入到基于 BERT 的模型中，以生成注释嵌入以评估相似性。然而，他们可能没有充分利用一些重要的线索，例如，标签或类别，这代表了笔记的关键概念。事实上，学习生成 # 标签/类别可以潜在地增强笔记的嵌入，这两者都可以将关键笔记信息压缩到有限的内容中。此外，大型语言模型(LLM)在理解自然语言方面明显优于 BERT。有希望将 LLM 引入票据推荐中。在本文中，我们提出了一个新的统一框架 NoteLLM，它利用 LLM 来处理项到项(I2I)注释推荐。具体来说，我们利用 Note CompressionPrompt 将一个音符压缩成一个单独的特殊标记，并通过对比学习的方法进一步学习潜在相关音符的嵌入。此外，我们使用 NoteLLM 来汇总注释，并通过指令调优自动生成 hashtag/type。对实际情景的广泛验证表明，与在线基准相比，我们建议的方法是有效的，并显示 Xiaohongshu 的推荐系统有了重大改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NoteLLM:+A+Retrievable+Large+Language+Model+for+Note+Recommendation)|0|
|[Lightweight GCN Encoder and Sequential Decoder for Multi-Candidate Carpooling Route Planning in Road Network](https://doi.org/10.1145/3589335.3648328)|Yucen Gao, Li Ma, Zhemeng Yu, Songjian Zhang, Jun Fang, Xiaofeng Gao, Guihai Chen||Carpooling Route Planning (CRP) has become an important issue with the growth of low-carbon traffic systems. We investigate a meaningful and challenging scenario for CRP in industry, where each passenger may have several potential positions to get on and off the car. Traditional graph search algorithms or indexing methods usually consume a lot of time and space or perform poorly. In this paper, we propose an end-to-end encoder-decoder model to plan a route for each many-to-one carpooling order with various data-driven mechanisms such as graph partitioning and feature crossover. The encoder is a filter-integrated Graph Convolution Network with external information fusion combining a supervised pre-training classification task, while the latter mimics a pointer network with a rule-based mask mechanism and a domain feature crossover module. We validate the effectiveness and efficiency of our model based on both synthetic and real-world datasets.|随着低碳交通系统的发展，拼车路径规划(CRP)已成为一个重要课题。我们调查了一个有意义的和具有挑战性的情况下的 CRP 在工业，其中每个乘客可能有几个潜在的位置上车和下车。传统的图形搜索算法或索引方法通常会消耗大量的时间和空间，或者性能较差。在本文中，我们提出一个端到端的编码器-解码器模型，以规划一个路由为每个多对一的拼车订单与各种数据驱动的机制，如图划分和特征交叉。该编码器采用外部信息融合的滤波集成图卷积网络，结合有监督的预训练分类任务，而后者采用基于规则的掩码机制和领域特征交叉模块来模拟指针网络。我们验证了我们的模型的有效性和效率的基础上，合成和真实世界的数据集。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lightweight+GCN+Encoder+and+Sequential+Decoder+for+Multi-Candidate+Carpooling+Route+Planning+in+Road+Network)|0|
|[PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction](https://doi.org/10.1145/3589335.3648329)|Yuanbo Gao, Peng Lin, Dongyue Wang, Feng Mei, Xiwei Zhao, Sulong Xu, Jinghe Hu||Click-through rate (CTR) prediction is a core task in recommender systems. Existing methods (IDRec for short) rely on unique identities to represent distinct users and items that have prevailed for decades. On one hand, IDRec often faces significant performance degradation on cold-start problem; on the other hand, IDRec cannot use longer training data due to constraints imposed by iteration efficiency. Most prior studies alleviate the above problems by introducing pre-trained knowledge(e.g. pre-trained user model or multi-modal embeddings). However, the explosive growth of online latency can be attributed to the huge parameters in the pre-trained model. Therefore, most of them cannot employ the unified model of end-to-end training with IDRec in industrial recommender systems, thus limiting the potential of the pre-trained model. To this end, we propose a Pre-trained Plug-in CTR Model, namely PPM. PPM employs multi-modal features as input and utilizes large-scale data for pre-training. Then, PPM is plugged in IDRec model to enhance unified model's performance and iteration efficiency. Upon incorporating IDRec model, certain intermediate results within the network are cached, with only a subset of the parameters participating in training and serving. Hence, our approach can successfully deploy an end-to-end model without causing huge latency increases. Comprehensive offline experiments and online A/B testing at JD E-commerce demonstrate the efficiency and effectiveness of PPM.|点进率预测是推荐系统的核心任务。现有的方法(简称 IDRec)依赖于独特的标识来表示流行了几十年的不同用户和项目。一方面，IDRec 在冷启动问题上经常面临严重的性能下降; 另一方面，由于迭代效率的限制，IDRec 不能使用更长的训练数据。大多数先前的研究通过引入预先训练的知识(例如预先训练的用户模型或多模态嵌入)来缓解上述问题。然而，在线延迟的爆炸性增长可归因于预训练模型中的大量参数。因此，它们中的大多数无法在工业推荐系统中使用 IDRec 的端到端培训统一模型，从而限制了预先培训模型的潜力。为此，我们提出了一个预训练的插件点击率模型，即 PPM。PPM 采用多模态特征作为输入，利用大规模数据进行预训练。然后，在 IDRec 模型中插入 PPM，以提高统一模型的性能和迭代效率。在加入 IDRec 模型后，网络中的某些中间结果被缓存，只有一部分参数参与训练和服务。因此，我们的方法可以成功地部署端到端模型，而不会造成巨大的延迟增加。JD 电子商务的全面离线实验和在线 A/B 测试证明了 PPM 的效率和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PPM+:+A+Pre-trained+Plug-in+Model+for+Click-through+Rate+Prediction)|0|
|[Towards Robustness Analysis of E-Commerce Ranking System](https://doi.org/10.1145/3589335.3648335)|Ningfei Wang, Yupin Huang, Han Cheng, Jiri Gesi, Xiaojie Wang, Vivek Mittal||Information retrieval (IR) is a pivotal component in various applications. Recent advances in machine learning (ML) have enabled the integration of ML algorithms into IR, particularly in ranking systems. While there is a plethora of research on the robustness of ML-based ranking systems, these studies largely neglect commercial e-commerce systems and fail to establish a connection between real-world and manipulated query relevance. In this paper, we present the first systematic measurement study on the robustness of e-commerce ranking systems. We define robustness as the consistency of ranking outcomes for semantically identical queries. To quantitatively analyze robustness, we propose a novel metric that considers both ranking position and item-specific information that are absent in existing metrics. Our large-scale measurement study with real-world data from e-commerce retailers reveals an open opportunity to measure and improve robustness since semantically identical queries often yield inconsistent ranking results. Based on our observations, we propose several solution directions to enhance robustness, such as the use of Large Language Models. Note that the issue of robustness discussed herein does not constitute an error or oversight. Rather, in scenarios where there exists a vast array of choices, it is feasible to present a multitude of products in various permutations, all of which could be equally appealing. However, this extensive selection may lead to customer confusion. As e-commerce retailers use various techniques to improve the quality of search results, we hope that this research offers valuable guidance for measuring the robustness of the ranking systems.|信息检索(IR)是各种应用中的关键组成部分。机器学习(ML)的最新进展使得 ML 算法能够集成到 IR 中，特别是在排序系统中。尽管对基于机器学习的排序系统的稳健性有大量的研究，但这些研究很大程度上忽视了商业电子商务系统，并且未能在现实世界和被操纵的查询相关性之间建立联系。本文首次对电子商务排名系统的稳健性进行了系统的度量研究。我们将鲁棒性定义为语义相同查询的排序结果的一致性。为了定量分析稳健性，我们提出了一种新的度量方法，该方法同时考虑了排名位置和现有度量方法中不存在的项目特定信息。我们对来自电子商务零售商的真实世界数据进行的大规模测量研究揭示了测量和提高健壮性的公开机会，因为语义相同的查询经常产生不一致的排名结果。基于我们的观察，我们提出了几个增强健壮性的解决方案方向，例如使用大型语言模型。请注意，本文讨论的健壮性问题并不构成错误或疏忽。相反，在存在大量选择的情况下，以不同的排列方式呈现多种产品是可行的，所有这些产品都同样具有吸引力。然而，这种广泛的选择可能会导致客户的困惑。随着电子商务零售商使用各种技术来提高搜索结果的质量，我们希望这项研究能够为衡量排名系统的稳健性提供有价值的指导。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Robustness+Analysis+of+E-Commerce+Ranking+System)|0|
|[End-to-End Graph-Sequential Representation Learning for Accurate Recommendations](https://doi.org/10.1145/3589335.3651499)|Vladimir Baikalov, Evgeny Frolov||Recent recommender system advancements have focused on developing sequence-based and graph-based approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item recommendation tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter aims to capture indirect dependencies across the interactions graph. This paper presents a novel multi-representational learning framework exploiting these two paradigms' synergies. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and graph components with the proposed framework significantly improves recommendations performance.|最近的推荐系统进展集中在开发基于序列和基于图表的方法。事实证明，这两种方法在建模行为数据中错综复杂的关系时都很有用，在保持良好可伸缩性的同时，在个性化排名和下一项推荐任务中产生了有希望的结果。然而，它们从数据中捕获非常不同的信号。前一种方法通过与最近项目的有序交互直接表示用户，而后一种方法旨在捕获交互图中的间接依赖关系。本文提出了一个新的多表征学习框架，利用这两个范式的协同作用。我们对几个数据集的实证评估表明，使用所提出的框架对顺序和图形组件进行相互训练可以显著提高推荐性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=End-to-End+Graph-Sequential+Representation+Learning+for+Accurate+Recommendations)|0|
|[DiffuRetrieval: A Chain-of-Thought Enhanced Diffusion Retrieval in Sponsored Search](https://doi.org/10.1145/3589335.3651491)|Yadong Zhang, Siyu Lu, Qiang Liu, Xingxing Wang||Embedding-based Retrieval (EBR) system is a fundamental component that supplies candidates for downstream ranking mechanisms in the sponsored search system. To enhance search experience and ensure effective retrieval, EBR usually accounts for various objectives including the semantic relevance and personalization of search results. However, traditional multi-task EBR models ignore the intrinsic progressive relationship between relevant and personalized candidates during a search. Recognizing this gap, we make the very first attempt to utilize the representation generation capabilities of Diffusion Models in EBR. In this paper, we present a novel model DiffuRetrieval to address the progressive objectives for high-quality item retrieval. In forward process, DiffuRetrieval incrementally corrupts item representations through controlled noise injection. Conversely, in reverse process, we refine the representations based on query information in a chain-of-thought manner, initially establishing coarse-grained relevance and progressively moving towards fine-grained personalization. Online A/B tests on Meituan sponsored search platform demonstrate that our approach markedly surpasses the baselines, delivering substantial improvements in revenue, relevance and personalization.|基于嵌入式的检索(EBR)系统是为赞助商搜索系统中的下游排序机制提供候选者的基本组件。为了增强搜索体验并确保有效的检索，EBR 通常考虑各种目标，包括搜索结果的语义相关性和个性化。然而，传统的多任务 EBR 模型忽视了搜索过程中相关候选人和个性化候选人之间内在的进步关系。认识到这一差距，我们首次尝试在 EBR 中利用扩散模型的表示生成功能。本文提出了一种新的区分检索模型来解决高质量项目检索的渐进目标问题。在前向过程中，扬声检索通过受控噪声注入增量损坏项表示。相反，在反向过程中，我们以思想链的方式精化基于查询信息的表示，最初建立粗粒度的相关性，并逐步向细粒度的个性化发展。在美团赞助的搜索平台上进行的在线 A/B 测试表明，我们的方法明显超越了基线，在收入、相关性和个性化方面带来了实质性的改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DiffuRetrieval:+A+Chain-of-Thought+Enhanced+Diffusion+Retrieval+in+Sponsored+Search)|0|
|[The Impact of Cluster Centroid and Text Review Embeddings on Recommendation Methods](https://doi.org/10.1145/3589335.3651570)|Peter Dolog, Ylli Sadikaj, Yllka Velaj, Andreas Stephan, Benjamin Roth, Claudia Plant||Recommendation systems often neglect global patterns that can be provided by clusters of similar items or even additional information such as text. Therefore, we study the impact of integrating clustering embeddings, review embeddings, and their combinations with embeddings obtained by a recommender system. Our work assesses the performance of this approach across various state-of-the-art recommender system algorithms. Our study highlights the improvement of recommendation performance through clustering, particularly evident when combined with review embeddings, and the enhanced performance of neural methods when incorporating review embeddings.|推荐系统往往忽略了类似项目集群甚至文本等附加信息可以提供的全局模式。因此，我们研究了集群嵌入、评论嵌入以及它们与推荐系统获得的嵌入的结合的影响。我们的工作评估了这种方法在各种最先进的推荐系统算法中的性能。我们的研究强调了通过聚类来提高推荐性能，特别是在与评论嵌入相结合时，以及在与评论嵌入相结合时提高神经方法的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Impact+of+Cluster+Centroid+and+Text+Review+Embeddings+on+Recommendation+Methods)|0|
|[Rethinking Sequential Relationships: Improving Sequential Recommenders with Inter-Sequence Data Augmentation](https://doi.org/10.1145/3589335.3651552)|Yang Jiao, Fan Yang, Yetian Chen, Yan Gao, Jia Liu, Yi Sun||Predicting customer preferences for each item is a prerequisite module for most recommender systems in e-commerce. However, the sparsity of behavioral data is often a challenge to learn accurate prediction models. Given millions of items, each customer may only be able to interact with a small subset of them over time. This sparse behavioral data is insufficient to represent item-customer and item-item relations for a machine learning model to digest, resulting in limited prediction accuracy that hinders recommendation performance. To mitigate this issue, this study introduces an inter-sequence data augmentation method, SDAinter, that enhances data density by leveraging cross-customer behavioral patterns to enrich item relations. Tested on three public and one proprietary e-commerce dataset, SDAinter significantly increases data density, leading to notable improvements in both evaluation and business metrics. Our findings demonstrate SDAinter's effectiveness and its potential to complement existing data augmentation strategies in recommender systems. See https://github.com/ML-apollo/SDA_inter.|预测顾客对每个商品的偏好是电子商务中大多数推荐系统的一个先决条件。然而，稀疏的行为数据往往是一个挑战，学习准确的预测模型。给定数以百万计的项目，随着时间的推移，每个客户可能只能与其中的一小部分进行交互。这种稀疏的行为数据不足以表示机器学习模型要消化的项目-客户和项目-项目关系，从而导致有限的预测准确性，阻碍了推荐性能。为了缓解这一问题，本研究引入了一种序列间数据增强方法 SDAinter，该方法通过利用跨客户行为模式来丰富项目关系来增强数据密度。在三个公共的和一个专有的电子商务数据集上进行了测试，SDAinter 显著地增加了数据密度，导致了评估和业务指标的显著改进。我们的研究结果表明 SDAinter 的有效性和它的潜力，以补充现有的数据增强策略在推荐系统。Https://github.com/ml-apollo/sda_inter.|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Sequential+Relationships:+Improving+Sequential+Recommenders+with+Inter-Sequence+Data+Augmentation)|0|
|[Unsupervised Search Algorithm Configuration using Query Performance Prediction](https://doi.org/10.1145/3589335.3651579)|Haggai Roitman||Search engine configuration can be quite difficult for inexpert developers. Instead, an auto-configuration approach can be used to speed up development time. Yet, such an automatic process usually requires relevance labels to train a supervised model. In this work, we suggest a simple solution based on query performance prediction that requires no relevance labels but only a sample of queries in a given domain. Using two example usecases we demonstrate the merits of our solution.|搜索 engine configuration 对于不专业的开发者来说是相当困难的。相反，可以使用自动配置方法来加快开发时间。然而，这样一个自动化的过程通常需要相关标签来训练一个受监督的模型。在这项工作中，我们提出了一个基于查询性能预测的简单解决方案，它不需要相关标签，只需要给定域中的查询样本。通过两个例子，我们证明了我们的解决方案的优点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Search+Algorithm+Configuration+using+Query+Performance+Prediction)|0|
|[I-CoSim: Efficient Dynamic CoSimRank Retrieval on Evolving Networks](https://doi.org/10.1145/3589335.3651523)|Xiaoyu Xu, Weiren Yu||CoSimRank, a favorable measure for assessing node similarity based on graphs, faces computational challenges on real evolving graphs. The best-of-breed algorithm, D-CoSim, for incremental CoSimRank search evaluates similarity changes by summing dot products between two vectors. These vectors are iteratively generated from scratch in the original high-dimensional space, leading to significant costs. In this paper, we propose I-CoSim, a novel efficient dynamic CoSimRank algorithm for evolving graphs. I-CoSim resorts to two low-dimensional Krylov subspaces and maximally reuses previously computed similarities in the original graph, which substantially expedites CoSimRank search on evolving graphs. We also theoretically provide an error bound on the I-CoSim estimation with guaranteed accuracy. Experimental results on real datasets show that I-CoSim is up to 28 times faster than the best-known competitor, with only a slight compromise in accuracy.|CoSimRank 是一种基于图的节点相似性度量方法，它面临着实际演化图的计算挑战。用于增量 CoSimRank 搜索的最佳种类算法 D-CoSim 通过求和两个向量之间的点积来评估相似性变化。这些向量是在原始的高维空间中从零开始迭代生成的，导致了巨大的成本。在本文中，我们提出了一种新的高效的进化图动态 CoSimRank 算法 I-CoSim。I-CoSim 利用两个低维 Krylov 子空间，最大限度地重用原始图中先前计算出的相似性，从而大大加快了对演化图的 CoSimRank 搜索。理论上还给出了 I-CoSim 估计的误差界，保证了估计的精度。在真实数据集上的实验结果表明，I-CoSim 比最知名的竞争对手快28倍，在精度上只有轻微的折衷。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=I-CoSim:+Efficient+Dynamic+CoSimRank+Retrieval+on+Evolving+Networks)|0|
|[Unlocking the Potential of Health Data with Decentralised Search in Personal Health Datastores](https://doi.org/10.1145/3589335.3651454)|Mohamed Ragab, Yury Savateev, Helen Oliver, Thanassis Tiropanis, Alexandra Poulovassilis, Adriane Chapman, George Roussos||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unlocking+the+Potential+of+Health+Data+with+Decentralised+Search+in+Personal+Health+Datastores)|0|
|[TATKC: A Temporal Graph Neural Network for Fast Approximate Temporal Katz Centrality Ranking](https://doi.org/10.1145/3589334.3645432)|Tianming Zhang, Junkai Fang, Zhengyi Yang, Bin Cao, Jing Fan||Numerous real-world networks are represented as temporal graphs, which capture the dynamics of connections over time. Identifying important nodes on temporal graphs has a plethora of real-life applications, such as information propagation and influential user identification, etc. Temporal Katz centrality, a popular temporal metric, gauges the importance of nodes by taking into account both the number of temporal walks and the timespan between the interactions. The computation of traditional temporal Katz centrality is computationally expensive, especially when applied to massive temporal graphs. Therefore, in this paper, we design a temporal graph neural network to approximate temporal Katz centrality computation. To the best of our knowledge, we are the first to address temporal Katz centrality computation purely from a learning-based perspective. We propose a time-injected self-attention model that consists of two phases. In the first phase, we utilize a time-injected self-attention mechanism to acquire node representations that encompass both structural information and temporal relevance. The second phase is structured as a multi-layer perceptron (MLP) which uses the learned node representation to predict node rankings. Furthermore, normalization and neighbor sampling strategies are integrated into the model to enhance its overall performance. Extensive experiments on real-world networks demonstrate the efficiency and accuracy of TATKC.|许多现实世界的网络被表示为时间图，这些图捕获了随时间变化的连接动态。识别时间图上的重要节点有着广泛的实际应用，如信息传播和有影响力的用户识别等。时间 Katz 中心度是一种流行的时间度量，它通过考虑时间步行的次数和相互作用之间的时间跨度来衡量节点的重要性。传统的时间 Katz 中心性的计算是昂贵的，特别是当应用到大量的时间图。因此，本文设计了一个时态图神经网络来近似计算时态 Katz 中心性。据我们所知，我们是第一个纯粹从基于学习的角度来处理时态 Katz 中心计算的。我们提出了一个由两个阶段组成的时间注入自我注意模型。在第一阶段，我们利用时间注入的自我注意机制来获取包含结构信息和时间相关性的节点表征。第二阶段采用多层感知器(MLP)结构，使用学习节点表示来预测节点排名。此外，归一化和邻居抽样策略的集成模型，以提高其整体性能。在实际网络上的大量实验证明了 TATKC 算法的有效性和准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TATKC:+A+Temporal+Graph+Neural+Network+for+Fast+Approximate+Temporal+Katz+Centrality+Ranking)|0|
|[FairSync: Ensuring Amortized Group Exposure in Distributed Recommendation Retrieval](https://doi.org/10.1145/3589334.3645413)|Chen Xu, Jun Xu, Yiming Ding, Xiao Zhang, Qi Qi||In pursuit of fairness and balanced development, recommender systems (RS) often prioritize group fairness, ensuring that specific groups maintain a minimum level of exposure over a given period. For example, RS platforms aim to ensure adequate exposure for new providers or specific categories of items according to their needs. Modern industry RS usually adopts a two-stage pipeline: stage-1 (retrieval stage) retrieves hundreds of candidates from millions of items distributed across various servers, and stage-2 (ranking stage) focuses on presenting a small-size but accurate selection from items chosen in stage-1. Existing efforts for ensuring amortized group exposures focus on stage-2, however, stage-1 is also critical for the task. Without a high-quality set of candidates, the stage-2 ranker cannot ensure the required exposure of groups. Previous fairness-aware works designed for stage-2 typically require accessing and traversing all items. In stage-1, however, millions of items are distributively stored in servers, making it infeasible to traverse all of them. How to ensure group exposures in the distributed retrieval process is a challenging question. To address this issue, we introduce a model named FairSync, which transforms the problem into a constrained distributed optimization problem. Specifically, FairSync resolves the issue by moving it to the dual space, where a central node aggregates historical fairness data into a vector and distributes it to all servers. To trade off the efficiency and accuracy, the gradient descent technique is used to periodically update the parameter of the dual vector. The experiment results on two public recommender retrieval datasets showcased that FairSync outperformed all the baselines, achieving the desired minimum level of exposures while maintaining a high level of retrieval accuracy.|为了追求公平和均衡发展，推荐系统(RS)经常优先考虑群体公平性，确保特定群体在给定的时间内保持最低的曝光水平。例如，RS 平台旨在确保新供应商或根据其需要的具体项目类别获得足够的曝光。现代工业 RS 通常采用两阶段流水线: 阶段1(检索阶段)从分布在不同服务器上的数百万个项目中检索数百个候选项，阶段2(排名阶段)侧重于从阶段1中选择的项目中提供一个小型但准确的选择。现有的努力，以确保摊销群体暴露集中在第二阶段，然而，第一阶段也是至关重要的任务。如果没有一组高质量的候选人，阶段2的排名不能确保所需的群体曝光。以前为阶段2设计的公平感知工作通常需要访问和遍历所有项。然而，在阶段1中，数以百万计的条目分布式地存储在服务器中，这使得遍历所有条目变得不可行。如何保证分布式检索过程中的群体曝光是一个具有挑战性的问题。为了解决这个问题，我们引入了一个名为 FairSync 的模型，它将问题转化为一个受限的分布式最佳化问题。具体来说，FairSync 通过将其移动到双空间来解决这个问题，在双空间中，一个中央节点将历史公平数据聚合到一个向量中，并将其分发到所有服务器。为了权衡效率和准确性，梯度下降法技术被用来定期更新双向量的参数。在两个公共推荐检索数据集上的实验结果表明，FairSync 的性能优于所有基线，在保持较高的检索准确率的同时，达到了预期的最小曝光水平。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FairSync:+Ensuring+Amortized+Group+Exposure+in+Distributed+Recommendation+Retrieval)|0|
|[Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism](https://doi.org/10.1145/3589334.3645482)|Yujia Zhou, Qiannan Zhu, Jiajie Jin, Zhicheng Dou||Traditional search engines usually provide identical search results for all users, overlooking individual preferences. To counter this limitation, personalized search has been developed to re-rank results based on user preferences derived from query logs. Deep learning-based personalized search methods have shown promise, but they rely heavily on abundant training data, making them susceptible to data sparsity challenges. This paper proposes a Cognitive Personalized Search (CoPS) model, which integrates Large Language Models (LLMs) with a cognitive memory mechanism inspired by human cognition. CoPS employs LLMs to enhance user modeling and user search experience. The cognitive memory mechanism comprises sensory memory for quick sensory responses, working memory for sophisticated cognitive responses, and long-term memory for storing historical interactions. CoPS handles new queries using a three-step approach: identifying re-finding behaviors, constructing user profiles with relevant historical information, and ranking documents based on personalized query intent. Experiments show that CoPS outperforms baseline models in zero-shot scenarios.|传统的搜索引擎通常为所有用户提供相同的搜索结果，忽略了个人偏好。为了克服这个限制，个性化检索已经开发出来，可以根据从查询日志中得到的用户偏好对结果进行重新排序。基于深度学习的个性化检索训练方法已经显示出了前景，但它们严重依赖于大量的训练数据，因此容易受到数据稀少的挑战。本文提出了一个认知个性化检索模型(CoPS) ，它将大语言模型(LLMs)与受人类认知启发的认知记忆机制结合在一起。CoPS 使用 LLM 来增强用户建模和用户搜索体验。认知记忆机制包括快速感觉反应的感觉记忆、复杂认知反应的工作记忆和存储历史互动的长期记忆。CoPS 使用三个步骤来处理新的查询: 识别重新查找行为，构建具有相关历史信息的用户配置文件，以及基于个性化查询意图的文档排序。实验结果表明，在零射击情况下，CoPS 的性能优于基线模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cognitive+Personalized+Search+Integrating+Large+Language+Models+with+an+Efficient+Memory+Mechanism)|0|
|[Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search](https://doi.org/10.1145/3589334.3645483)|Yifei Yuan, Clemencia Siro, Mohammad Aliannejadi, Maarten de Rijke, Wai Lam||In mixed-initiative conversational search systems, clarifying questions are used to help users who struggle to express their intentions in a single query. These questions aim to uncover user's information needs and resolve query ambiguities. We hypothesize that in scenarios where multimodal information is pertinent, the clarification process can be improved by using non-textual information. Therefore, we propose to add images to clarifying questions and formulate the novel task of asking multimodal clarifying questions in open-domain, mixed-initiative conversational search systems. To facilitate research into this task, we collect a dataset named Melon that contains over 4k multimodal clarifying questions, enriched with over 14k images. We also propose a multimodal query clarification model named Marto and adopt a prompt-based, generative fine-tuning strategy to perform the training of different stages with different prompts. Several analyses are conducted to understand the importance of multimodal contents during the query clarification phase. Experimental results indicate that the addition of images leads to significant improvements of up to 90 images. Extensive analyses are also performed to show the superiority of Marto compared with discriminative baselines in terms of effectiveness and efficiency.|在混合主动会话搜索系统中，澄清问题用于帮助那些在单个查询中难以表达意图的用户。这些问题旨在揭示用户的信息需求和解决查询模糊性。我们假设在多模态信息相关的场景中，可以通过使用非文本信息来改进澄清过程。因此，我们建议在会话搜索系统中增加图像来解决问题，并提出在开放领域、混合主动的会话搜索系统中提出多模态问题的新任务。为了促进这项任务的研究，我们收集了一个名为 Melon 的数据集，其中包含了超过4k 个多模式澄清问题，丰富了超过14k 个图像。我们还提出了一个名为 Marto 的多模式查询澄清模型，并采用基于提示的生成式微调策略对不同提示的不同阶段进行训练。为了理解在查询澄清阶段多通道内容的重要性，进行了一些分析。实验结果表明，增加图像导致显着改善高达90图像。还进行了广泛的分析，以显示 Marto 相对于有效性和效率的判别基线的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Asking+Multimodal+Clarifying+Questions+in+Mixed-Initiative+Conversational+Search)|0|
|[Benchmark and Neural Architecture for Conversational Entity Retrieval from a Knowledge Graph](https://doi.org/10.1145/3589334.3645676)|Mona Zamiri, Yao Qiang, Fedor Nikolaev, Dongxiao Zhu, Alexander Kotov||This paper introduces a novel information retrieval (IR) task of Conversational Entity Retrieval from a Knowledge Graph (CER-KG), which extends non-conversational entity retrieval from a knowledge graph (KG) to the conversational scenario. The user queries in CER-KG dialog turns may rely on the results of the preceding turns, which are KG entities. Similar to the conversational document IR, CER-KG can be viewed as a sequence of interrelated ranking tasks. To enable future research on CER-KG, we created QBLink-KG, a publicly available benchmark that was adapted from QBLink, a benchmark for text-based conversational reading comprehension of Wikipedia. As an initial approach to CER-KG, we experimented with Transformer- and LSTM-based query encoders in combination with the Neural Architecture for Conversational Entity Retrieval (NACER), our proposed feature-based neural architecture for entity ranking in CER-KG. NACER computes the ranking score of a candidate KG entity by taking into account diverse lexical and semantic matching signals between various KG components in its neighborhood, such as entities, categories, and literals, as well as entities in the results of the preceding turns in dialog history. The reported experimental results reveal the key challenges of CER-KG along with the possible directions for new approaches to this task.|本文介绍了一个新颖的知识图中的会话实体检索任务(CER-KG) ，该任务将非会话实体检索从知识图中扩展到会话场景中，从而提高了会话实体检索的信息检索。用户在 CER-KG 对话轮中的查询可能依赖于前面几轮的结果，即 KG 实体。与会话文档 IR 类似，CER-KG 可以看作是一系列相互关联的排序任务。为了将来能够对 CER-KG 进行研究，我们创建了 QBLink-KG，这是一个公开可用的基准，改编自 QBLink，一个基于文本的维基百科会话阅读理解的基准。作为 CER-KG 的一个初始方法，我们使用变压器和 LSTM 为基础的查询编码器结合会话实体检索(NACER)的神经结构进行实验，NACER 是我们提出的用于 CER-KG 中实体排序的基于特征的神经结构。NACER 通过考虑其邻近的各种 KG 组件之间的不同词汇和语义匹配信号(如实体、类别和文字)以及对话历史中前一轮结果中的实体来计算候选 KG 实体的排名得分。报告的实验结果揭示了 CER-KG 的关键挑战以及这项任务的新方法的可能方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Benchmark+and+Neural+Architecture+for+Conversational+Entity+Retrieval+from+a+Knowledge+Graph)|0|
|[A Data-Centric Multi-Objective Learning Framework for Responsible Recommendation Systems](https://doi.org/10.1145/3589334.3645324)|Xu Huang, Jianxun Lian, Hao Wang, Hao Liao, Defu Lian, Xing Xie||Recommendation systems effectively guide users in locating their desired information within extensive content repositories. Generally, a recommendation model is optimized to enhance accuracy metrics from a user utility standpoint, such as click-through rate or matching relevance. However, a responsible industrial recommendation system must address not only user utility (responsibility to users) but also other objectives, including increasing platform revenue (responsibility to platforms), ensuring fairness (responsibility to content creators), and maintaining unbiasedness (responsibility to long-term healthy development). Multi-objective learning is a potent approach for achieving responsible recommendation systems. Nevertheless, current methods encounter two challenges: difficulty in scaling to heterogeneous objectives within a unified framework, and inadequate controllability over objective priority during optimization, leading to uncontrollable solutions. In this paper, we present a data-centric optimization framework, MoRec, which unifies the learning of diverse objectives. MoRec is a tri-level framework: the outer level manages the balance between different objectives, utilizing a proportional-integral-derivative (PID)-based controller to ensure a preset regularization on the primary objective. The middle level transforms objective-aware optimization into data sampling weights using sign gradients. The inner level employs a standard optimizer to update model parameters with the sampled data. Consequently, MoRec can flexibly support various objectives while maintaining the original model intact. Comprehensive experiments on two public datasets and one industrial dataset showcase the effectiveness, controllability, flexibility, and Pareto efficiency of MoRec, making it highly suitable for real-world implementation.|推荐系统有效地指导用户在广泛的内容存储库中查找所需的信息。一般来说，推荐模型会优化，以从用户效用的角度提高准确性指标，比如点进率或匹配相关性。然而，一个负责任的行业推荐系统不仅要解决用户效用(对用户的责任) ，还要解决其他目标，包括增加平台收入(对平台的责任) ，确保公平(对内容创建者的责任) ，保持无偏见(对长期健康发展的责任)。多目标学习是实现负责任推荐系统的有效方法。然而，目前的方法遇到两个挑战: 难以在统一的框架内扩展到异构的目标，以及在优化过程中对目标优先级的可控性不足，导致不可控的解决方案。在本文中，我们提出了一个以数据为中心的优化框架 MoRec，它统一了不同目标的学习。MoRec 是一个三层框架: 外层管理不同目标之间的平衡，利用比例积分微分(PID)为基础的控制器，以确保预设正则化的主要目标。中间层利用符号梯度将目标感知优化转换为数据采样权重。内部级别使用标准优化器用采样数据更新模型参数。因此，MoRec 可以灵活地支持各种目标，同时保持原始模型的完整性。在两个公共数据集和一个工业数据集上进行的全面实验展示了 MoRec 的有效性、可控性、灵活性和帕累托最优，使其非常适合于现实世界的实现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Data-Centric+Multi-Objective+Learning+Framework+for+Responsible+Recommendation+Systems)|0|
|[Collaborative Large Language Model for Recommender Systems](https://doi.org/10.1145/3589334.3645347)|Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li||Recently, there is a growing interest in developing next-generation recommender systems (RSs) based on pretrained large language models (LLMs), fully utilizing their encoded knowledge and reasoning ability. However, the semantic gap between natural language and recommendation tasks is still not well addressed, leading to multiple issues such as spuriously-correlated user/item descriptors, ineffective language modeling on user/item contents, and inefficient recommendations via auto-regression, etc. In this paper, we propose CLLM4Rec, the first generative RS that tightly integrates the LLM paradigm and ID paradigm of RS, aiming to address the above challenges simultaneously. We first extend the vocabulary of pretrained LLMs with user/item ID tokens to faithfully model the user/item collaborative and content semantics. Accordingly, in the pretraining stage, a novel soft+hard prompting strategy is proposed to effectively learn user/item collaborative/content token embeddings via language modeling on RS-specific corpora established from user-item interactions and user/item features, where each document is split into a prompt consisting of heterogeneous soft (user/item) tokens and hard (vocab) tokens and a main text consisting of homogeneous item tokens or vocab tokens that facilitates stable and effective language modeling. In addition, a novel mutual regularization strategy is introduced to encourage the CLLM4Rec to capture recommendation-oriented information from user/item contents. Finally, we propose a novel recommendation-oriented finetuning strategy for CLLM4Rec, where an item prediction head with multinomial likelihood is added to the pretrained CLLM4Rec backbone to predict hold-out items based on the soft+hard prompts established from masked user-item interaction history, where recommendations of multiple items can be generated efficiently.|近年来，基于预先训练的大语言模型(LLM) ，充分利用其编码知识和推理能力，开发下一代推荐系统(RS)越来越受到人们的关注。然而，自然语言和推荐任务之间的语义差距仍然没有得到很好的解决，导致多种问题，如虚假相关的用户/项目描述符，对用户/项目内容无效的语言建模，以及通过自动回归的低效推荐等。在本文中，我们提出了 CLLM4Rec，这是第一个紧密集成了 RS 的 LLM 范式和 ID 范式的生成 RS，旨在同时解决上述挑战。我们首先扩展带有用户/项目 ID 令牌的预训练 LLM 的词汇表，以忠实地建立用户/项目协作和内容语义的模型。因此，在预训练阶段，提出了一种新的软硬件提示策略，通过对基于用户-项目交互和用户/项目特征建立的 RS 特定语料库进行语言建模，有效地学习用户/项目协作/内容令牌嵌入，将每个文档分解为由异构软(用户/项目)令牌和硬(词汇)令牌组成的提示和由同质项目令牌或词汇令牌组成的主文本，以促进稳定有效的语言建模。此外，还引入了一种新的相互正则化策略，以鼓励 CLLM4Rec 从用户/项目内容中捕获面向推荐的信息。最后，我们为 CLLM4Rec 提出了一种新的面向推荐的微调策略，其中将具有多项式可能性的项目预测头添加到预先训练的 CLLM4Rec 骨干中，以基于从掩盖的用户项目交互历史建立的软 + 硬提示来预测坚持项目，其中可以有效地生成多个项目的推荐。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaborative+Large+Language+Model+for+Recommender+Systems)|0|
|[Harnessing Large Language Models for Text-Rich Sequential Recommendation](https://doi.org/10.1145/3589334.3645358)|Zhi Zheng, Wenshuo Chao, Zhaopeng Qiu, Hengshu Zhu, Hui Xiong||Recent advances in Large Language Models (LLMs) have been changing the paradigm of Recommender Systems (RS). However, when items in the recommendation scenarios contain rich textual information, such as product descriptions in online shopping or news headlines on social media, LLMs require longer texts to comprehensively depict the historical user behavior sequence. This poses significant challenges to LLM-based recommenders, such as over-length limitations, extensive time and space overheads, and suboptimal model performance. To this end, in this paper, we design a novel framework for harnessing Large Language Models for Text-Rich Sequential Recommendation (LLM-TRSR). Specifically, we first propose to segment the user historical behaviors and subsequently employ an LLM-based summarizer for summarizing these user behavior blocks. Particularly, drawing inspiration from the successful application of Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) models in user modeling, we introduce two unique summarization techniques in this paper, respectively hierarchical summarization and recurrent summarization. Then, we construct a prompt text encompassing the user preference summary, recent user interactions, and candidate item information into an LLM-based recommender, which is subsequently fine-tuned using Supervised Fine-Tuning (SFT) techniques to yield our final recommendation model. We also use Low-Rank Adaptation (LoRA) for Parameter-Efficient Fine-Tuning (PEFT). We conduct experiments on two public datasets, and the results clearly demonstrate the effectiveness of our approach.|大语言模型(LLM)的最新进展已经改变了推荐系统(RS)的范式。但是，当推荐场景中的条目包含丰富的文本信息时，如在线购物中的产品描述或社交媒体上的新闻标题，LLM 需要更长的文本来全面描述历史用户行为序列。这对基于 LLM 的推荐程序提出了重大挑战，例如超长限制、大量的时间和空间开销以及次优模型性能。为此，在本文中，我们设计了一个新的框架，用于利用大型语言模型进行富文本顺序推荐(LLM-TRSR)。具体来说，我们首先建议分割用户的历史行为，然后使用一个基于 LLM 的总结器来总结这些用户行为块。特别地，借鉴了卷积神经网络(CNN)和递归神经网络(RNN)模型在用户建模中的成功应用，本文介绍了两种独特的文摘技术，分别是层次文摘和递归文摘。然后，我们构建一个包含用户偏好摘要，最近的用户交互和候选项信息的提示文本到基于 LLM 的推荐器中，随后使用监督微调(SFT)技术进行微调以产生我们的最终推荐模型。我们还使用低秩自适应(LoRA)的参数有效微调(PEFT)。我们在两个公共数据集上进行了实验，结果清楚地证明了我们方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Harnessing+Large+Language+Models+for+Text-Rich+Sequential+Recommendation)|0|
|[ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction](https://doi.org/10.1145/3589334.3645396)|Jianghao Lin, Bo Chen, Hangyu Wang, Yunjia Xi, Yanru Qu, Xinyi Dai, Kangning Zhang, Ruiming Tang, Yong Yu, Weinan Zhang||Click-through rate (CTR) prediction has become increasingly indispensable for various Internet applications. Traditional CTR models convert the multi-field categorical data into ID features via one-hot encoding, and extract the collaborative signals among features. Such a paradigm suffers from the problem of semantic information loss. Another line of research explores the potential of pretrained language models (PLMs) for CTR prediction by converting input data into textual sentences through hard prompt templates. Although semantic signals are preserved, they generally fail to capture the collaborative information (e.g., feature interactions, pure ID features), not to mention the unacceptable inference overhead brought by the huge model size. In this paper, we aim to model both the semantic knowledge and collaborative knowledge for accurate CTR estimation, and meanwhile address the inference inefficiency issue. To benefit from both worlds and close their gaps, we propose a novel model-agnostic framework (i.e., ClickPrompt), where we incorporate CTR models to generate interaction-aware soft prompts for PLMs. We design a prompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM has to recover the masked tokens based on the language context, as well as the soft prompts generated by CTR model. The collaborative and semantic knowledge from ID and textual features would be explicitly aligned and interacted via the prompt interface. Then, we can either tune the CTR model with PLM for superior performance, or solely tune the CTR model without PLM for inference efficiency. Experiments on four real-world datasets validate the effectiveness of ClickPrompt compared with existing baselines.|对于各种互联网应用而言，点进率预测(ctrl)已变得越来越不可或缺。传统的 CTR 模型通过一次热编码将多领域分类数据转换为 ID 特征，并提取特征间的协同信号。这种模式存在语义信息损失的问题。另一项研究通过硬提示模板将输入数据转换成文本句子，探索了预训练语言模型(PLM)在 CTR 预测中的潜力。尽管保留了语义信号，但它们通常无法捕获协作信息(例如，特征交互、纯 ID 特征) ，更不用说巨大的模型规模带来的不可接受的推理开销。本文旨在建立语义知识和协同知识的模型，以便准确地估计 CTR，同时解决推理效率低下的问题。为了从这两个世界中获益并弥合它们之间的差距，我们提出了一个新的模型无关框架(即 ClickPrompt) ，其中我们结合了 CTR 模型来为 PLM 生成感知交互的软提示。我们设计了一个提示增强的掩码语言建模(PA-MLM)预训练任务，其中 PLM 必须基于语言上下文以及由 CTR 模型生成的软提示来恢复掩码标记。来自 ID 和文本特性的协作和语义知识将通过提示界面显式地对齐和交互。然后，我们可以使用 PLM 调整 CTR 模型以获得更好的性能，或者单独调整不使用 PLM 的 CTR 模型以获得更高的推理效率。在四个实际数据集上的实验验证了 ClickPrompt 与现有基线相比的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ClickPrompt:+CTR+Models+are+Strong+Prompt+Generators+for+Adapting+Language+Models+to+CTR+Prediction)|0|
|[Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion](https://doi.org/10.1145/3589334.3645404)|Jinheon Baek, Nirupama Chandrasekaran, Silviu Cucerzan, Allen Herring, Sujay Kumar Jauhar||Large Language Models (LLMs) excel at tackling various natural language tasks. However, due to the significant costs involved in re-training or fine-tuning them, they remain largely static and difficult to personalize. Nevertheless, a variety of applications could benefit from generations that are tailored to users' preferences, goals, and knowledge. Among them is web search, where knowing what a user is trying to accomplish, what they care about, and what they know can lead to improved search experiences. In this work, we propose a novel and general approach that augments an LLM with relevant context from users' interaction histories with a search engine in order to personalize its outputs. Specifically, we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web, which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is light-weight, since it only produces user-specific aggregate projections of interests and knowledge onto public knowledge graphs, and leverages existing search log infrastructure, thereby mitigating the privacy, compliance, and scalability concerns associated with building deep user profiles for personalization. We validate our approach on the task of contextual query suggestion, which requires understanding not only the user's current search context but also what they historically know and care about. Through a number of experiments based on human evaluation, we show that our approach is significantly better than several other LLM-powered baselines, generating query suggestions that are contextually more relevant, personalized, and useful.|大型语言模型(LLM)擅长处理各种自然语言任务。然而，由于重新培训或微调它们所涉及的巨大成本，它们在很大程度上仍然是静态的，难以个性化。尽管如此，各种各样的应用程序可以受益于根据用户的偏好、目标和知识量身定制的几代人。其中之一是网络搜索，知道用户想要完成什么，他们关心什么，以及他们知道什么可以改善搜索体验。在这项工作中，我们提出了一个新颖的和通用的方法，增强了 LLM 与相关的上下文从用户的互动历史与搜索引擎，以个性化其输出。具体来说，我们根据每个用户在 Web 上的搜索和浏览活动，为他们构建一个以实体为中心的知识库，然后利用这个知识库提供与上下文相关的 LLM 提示增强。这个知识存储是轻量级的，因为它只生成用户特定的兴趣和知识的集合投影到公共知识图表上，并利用现有的搜索日志基础设施，从而减轻隐私、合规性和可扩展性方面的关切，这些关切与建立深度用户个性化配置文件有关。我们验证了我们的方法在上下文查询建议的任务，这不仅需要理解用户的当前搜索上下文，而且他们历史上知道和关心。通过大量基于人工评估的实验，我们发现我们的方法明显优于其他几个基于 LLM 的基线，生成的查询建议在上下文中更相关、更个性化和更有用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge-Augmented+Large+Language+Models+for+Personalized+Contextual+Query+Suggestion)|0|
|[Enhancing Recommendation Accuracy and Diversity with Box Embedding: A Universal Framework](https://doi.org/10.1145/3589334.3645577)|Cheng Wu, Shaoyun Shi, Chaokun Wang, Ziyang Liu, Wang Peng, Wenjin Wu, Dongying Kong, Han Li, Kun Gai||Recommender systems have emerged as an indispensable mean to meet personalized interests of users and alleviate information overload. Despite the great success, accuracy-oriented recommendation models are creating information cocoons, i.e., it is becoming increasingly difficult for users to see other items they might be interested in. Although recent studies start paying attention to enhancing recommendation diversity, models based on point embedding fail to describe the range of user preferences and item features well, which is essential for diversified matching. To this end, we propose LCD-UC, a novel List-Check-Decide framework with UnCertainty masking based on box embedding to improve recommendation diversity with recommendation accuracy maintained. Specifically, LCD-UC creates hypercubes to represent users and items using box embedding for high model flexibility and expressiveness. Then, a hypercube similarity scoring function is designed to measure the similarity between hypercubes representing users and items. To make a balance between the accuracy and diversity of recommendations and achieve personalized diversity needs, we further develop a user-item pairwise attention mechanism as well as a user uncertainty masking mechanism in LCD-UC. Besides, we present two new metrics for better evaluation on recommendation diversity, which address the issue that existing metrics only consider the coverage of categories while ignore the frequency of categories. The extensive experiments on three real-world datasets show that LCD-UC can improve both recommendation accuracy and diversity over three base models, and is superior to six state-of-the-art recommendation models. An online 10-day AB test also demonstrates that LCD-UC can improve the performance of a real-world advertising system.|推荐系统已经成为满足用户个性化兴趣和减轻信息超载的不可或缺的手段。尽管取得了巨大的成功，但以准确性为导向的推荐模型正在创造信息茧，也就是说，用户越来越难以看到他们可能感兴趣的其他项目。虽然最近的研究开始关注提高推荐多样性，但基于点嵌入的模型并不能很好地描述用户偏好和项目特征的范围，这对于多样化匹配是必不可少的。为此，我们提出了一种新的基于框嵌入的带不确定性掩蔽的列表检查决策框架 LCD-UC，以提高推荐的多样性，同时保持推荐的准确性。具体来说，LCD-UC 使用框嵌入创建超立方体来表示用户和项目，以获得高模型灵活性和表现力。然后，设计了一个超立方体相似度评分函数来度量代表用户的超立方体与项目之间的相似度。为了在推荐的准确性和多样性之间取得平衡，实现个性化的多样性需求，我们进一步开发了 LCD-UC 中的用户项成对注意机制和用户不确定性掩蔽机制。此外，我们还提出了两个新的指标来更好地评估推荐多样性，这两个指标解决了现有指标只考虑类别的覆盖率而忽略类别的频率的问题。在三个实际数据集上的大量实验表明，LCD-UC 在三个基本模型上都能提高推荐的准确性和多样性，并且优于六个最先进的推荐模型。在线10天 AB 测试也表明，LCD-UC 可以提高现实世界中广告系统的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Recommendation+Accuracy+and+Diversity+with+Box+Embedding:+A+Universal+Framework)|0|
|[Is Contrastive Learning Necessary? A Study of Data Augmentation vs Contrastive Learning in Sequential Recommendation](https://doi.org/10.1145/3589334.3645661)|Peilin Zhou, YouLiang Huang, Yueqi Xie, Jingqi Gao, Shoujin Wang, Jae Boum Kim, Sunghun Kim||Sequential recommender systems (SRS) are designed to predict users' future behaviors based on their historical interaction data. Recent research has increasingly utilized contrastive learning (CL) to leverage unsupervised signals to alleviate the data sparsity issue in SRS. In general, CL-based SRS first augments the raw sequential interaction data by using data augmentation strategies and employs a contrastive training scheme to enforce the representations of those sequences from the same raw interaction data to be similar. Despite the growing popularity of CL, data augmentation, as a basic component of CL, has not received sufficient attention. This raises the question: Is it possible to achieve superior recommendation results solely through data augmentation? To answer this question, we benchmark eight widely used data augmentation strategies, as well as state-of-the-art CL-based SRS methods, on four real-world datasets under both warm- and cold-start settings. Intriguingly, the conclusion drawn from our study is that, certain data augmentation strategies can achieve similar or even superior performance compared with some CL-based methods, demonstrating the potential to significantly alleviate the data sparsity issue with fewer computational overhead. We hope that our study can further inspire more fundamental studies on the key functional components of complex CL techniques. Our processed datasets and codes are available at https://github.com/AIM-SE/DA4Rec.|序贯推荐系统(SRS)是基于用户的历史交互数据来预测用户未来行为的系统。最近的研究越来越多地利用对比学习(CL)来利用无监督信号来缓解 SRS 中的数据稀疏问题。一般情况下，基于 CL 的 SRS 首先通过数据增强策略对原始序列交互数据进行增强，然后采用对比训练方案强制相同原始交互数据的序列表示相似。数据增强作为协同学习的一个基本组成部分，虽然受到了越来越多的关注，但并没有得到足够的重视。这就提出了一个问题: 是否有可能仅仅通过数据增强来获得更好的推荐结果？为了回答这个问题，我们基准八个广泛使用的数据增强策略，以及最先进的基于 CL 的 SRS 方法，在四个真实世界的数据集上，在暖启动和冷启动设置下。有趣的是，我们的研究得出的结论是，与一些基于 CL 的方法相比，某些数据增强策略可以实现相似甚至更好的性能，表明可以显着缓解数据稀疏问题，计算开销更少。我们希望我们的研究能够进一步启发更多关于复杂化学发光技术关键功能成分的基础研究。我们处理过的数据集和代码可以在 https://github.com/aim-se/da4rec 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+Contrastive+Learning+Necessary?+A+Study+of+Data+Augmentation+vs+Contrastive+Learning+in+Sequential+Recommendation)|0|
|[Can Small Language Models be Good Reasoners for Sequential Recommendation?](https://doi.org/10.1145/3589334.3645671)|Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Liang Pang, Xiao Wang||Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larger teacher model. The rationales generated by the teacher model are then utilized as labels to distill the downstream smaller student model (e.g., LLaMA2-7B). In this way, the student model acquires the step-by-step reasoning capabilities in recommendation tasks. We encode the generated rationales from the student model into a dense vector, which empowers recommendation in both ID-based and ID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of SLIM over state-of-the-art baselines, and further analysis showcasing its ability to generate meaningful recommendation reasoning at affordable costs.|大型语言模型(LLM)由于其卓越的语言理解和生成能力，为顺序推荐开辟了新的视野。然而，要成功实施 LLM 授权的顺序建议，仍然需要应对许多挑战。首先，用户行为模式通常很复杂，仅仅依靠 LLM 的一步推理可能会导致不正确的或与任务无关的响应。其次，对于真正的顺序推荐系统来说，LLM (如 ChatGPT-175B)过高的资源需求是不切实际的。在本文中，我们提出了一种新的逐步推荐知识提取框架(SLIM) ，为顺序推荐者以“苗条”(即资源效率)的方式享受 LLM 的异常推理能力铺平了一条有希望的道路。我们在较大的教师模型中引入了基于用户行为序列的 CoT 提示。由教师模型产生的基本原理然后被用作标签来提取下游较小的学生模型(例如，LLaMA2-7B)。通过这种方式，学生模型在推荐任务中获得了分步推理的能力。我们将从学生模型生成的基本原理编码成一个密集向量，这使得推荐在基于身份和身份不可知的情况下都有效。大量的实验证明了 SLIM 在最先进的基线上的有效性，并且进一步的分析显示了它以可承受的成本产生有意义的推荐推理的能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+Small+Language+Models+be+Good+Reasoners+for+Sequential+Recommendation?)|0|
|[Predictive Relevance Uncertainty for Recommendation Systems](https://doi.org/10.1145/3589334.3645689)|Charul Paliwal, Anirban Majumder, Sivaramakrishnan Kaveri||Click-through Rate (CTR) module is the foundation block of recommendation system and used for search, content selection, advertising, video streaming etc. CTR is modelled as a classification problem and extensive research is done to improve the CTR models. However, uncertainty method for these models are still an unexplored area. In this work we analyse popular uncertainty methods in the context of recommendation system. We found that popular uncertainty models fails to capture the predictive uncertainty of the CTR model that exist unique to the recommendation models and is not prevalent in the traditional classification models. We empirical show why a different uncertainty measure is required for the recommendation system CTR prediction models. We propose PRU (Predictive Relevance Uncertainty), a single forward pass uncertainty approach for a sample as a distance from the predictive relevance samples of the training data. We show the efficacy of the proposed predictive relevance uncertainty (PRU) on selective prediction. Further, we demonstrate the utility of the proposed framework on the downstream task of OOD detection and active learning while maintaining the latency of a single pass deterministic model.|点击点进率(ctrl)模块是推荐系统的基础，用于搜索、内容选择、广告、视频流等。CTR 被建模为一个分类问题，为了改进 CTR 模型，人们进行了广泛的研究。然而，这些模型的测量不确定度方法仍然是一个未开发的领域。本文分析了推荐系统中常用的不确定性方法。我们发现，流行的不确定性模型未能捕捉到推荐模型所特有的 CTR 模型的预测不确定性，这种不确定性在传统的分类模型中并不普遍。我们实证研究了为什么推荐系统的 CTR 预测模型需要不同的不确定性度量。我们提出了 PRU (预测相关不确定性) ，一个单一的前向通过不确定性方法的样本作为一个距离的预测相关样本的训练数据。我们证明了所提出的预测相关不确定度(PRU)在选择性预测中的有效性。此外，我们展示了建议的框架在面向对象的检测和主动学习的下游任务中的实用性，同时保持单次通过确定性模型的延迟。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Predictive+Relevance+Uncertainty+for+Recommendation+Systems)|0|
|[Decentralized Collaborative Learning with Adaptive Reference Data for On-Device POI Recommendation](https://doi.org/10.1145/3589334.3645696)|Ruiqi Zheng, Liang Qu, Tong Chen, Lizhen Cui, Yuhui Shi, Hongzhi Yin||In Location-based Social Networks, Point-of-Interest (POI) recommendation helps users discover interesting places. There is a trend to move from the cloud-based model to on-device recommendations for privacy protection and reduced server reliance. Due to the scarcity of local user-item interactions on individual devices, solely relying on local instances is not adequate. Collaborative Learning (CL) emerges to promote model sharing among users, where reference data is an intermediary that allows users to exchange their soft decisions without directly sharing their private data or parameters, ensuring privacy and benefiting from collaboration. However, existing CL-based recommendations typically use a single reference for all users. Reference data valuable for one user might be harmful to another, given diverse user preferences. Users may not offer meaningful soft decisions on items outside their interest scope. Consequently, using the same reference data for all collaborations can impede knowledge exchange and lead to sub-optimal performance. To address this gap, we introduce the Decentralized Collaborative Learning with Adaptive Reference Data (DARD) framework, which crafts adaptive reference data for effective user collaboration. It first generates a desensitized public reference data pool with transformation and probability data generation methods. For each user, the selection of adaptive reference data is executed in parallel by training loss tracking and influence function. Local models are trained with individual private data and collaboratively with the geographical and semantic neighbors. During the collaboration between two users, they exchange soft decisions based on a combined set of their adaptive reference data. Our evaluations across two real-world datasets highlight DARD's superiority in recommendation performance and addressing the scarcity of available reference data.|在基于位置的社交网络中，兴趣点(POI)推荐帮助用户发现有趣的地方。有一种趋势是从基于云的模型转向基于设备的建议，以保护隐私并减少对服务器的依赖。由于在单个设备上缺乏本地用户项交互，仅仅依赖本地实例是不够的。合作学习(CL)的出现是为了促进用户之间的模式共享，其中参考数据是一种中介，允许用户在不直接共享其私人数据或参数的情况下交换软决策，确保隐私并从合作中受益。但是，现有的基于 CL 的建议通常对所有用户使用单个引用。鉴于不同的用户偏好，对一个用户有价值的参考数据可能对另一个用户有害。用户可能不会对他们感兴趣的范围之外的项目提供有意义的软决策。因此，对所有协作使用相同的参考数据可能会阻碍知识交流并导致次优性能。为了解决这一差距，我们引入了自适应参考数据(dARD)框架的分散式合作学习，该框架为有效的用户协作提供自适应参考数据。它首先使用转换和概率数据生成方法生成一个不敏感的公共参考数据池。对于每个用户，通过训练损失跟踪和影响函数并行执行自适应参考数据的选择。局部模型用单个私有数据进行训练，并与地理和语义邻居协作进行训练。在两个用户之间的协作过程中，他们基于一组自适应参考数据交换软决策。我们对两个实际数据集的评估突出了 DARD 在推荐性能和解决可用参考数据稀缺性方面的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decentralized+Collaborative+Learning+with+Adaptive+Reference+Data+for+On-Device+POI+Recommendation)|0|
|[Towards Efficient Communication and Secure Federated Recommendation System via Low-rank Training](https://doi.org/10.1145/3589334.3645702)|NgocHieu Nguyen, TuanAnh Nguyen, Tuan Nguyen, Vu Tien Hoang, Dung D. Le, KokSeng Wong||Federated Recommendation (FedRec) systems have emerged as a solution to safeguard users' data in response to growing regulatory concerns. However, one of the major challenges in these systems lies in the communication costs that arise from the need to transmit neural network models between user devices and a central server. Prior approaches to these challenges often lead to issues such as computational overheads, model specificity constraints, and compatibility issues with secure aggregation protocols. In response, we propose a novel framework, called Correlated Low-rank Structure (CoLR), which leverages the concept of adjusting lightweight trainable parameters while keeping most parameters frozen. Our approach substantially reduces communication overheads without introducing additional computational burdens. Critically, our framework remains fully compatible with secure aggregation protocols, including the robust use of Homomorphic Encryption. The approach resulted in a reduction of up to 93.75 recommendation performance across datasets. Code for reproducing our experiments can be found at https://github.com/NNHieu/CoLR-FedRec.|联邦推荐(FedRec)系统已经成为保护用户数据的一种解决方案，以应对日益增长的监管问题。然而，这些系统的主要挑战之一在于需要在用户设备和中央服务器之间传输神经网络模型而产生的通信成本。先前解决这些挑战的方法通常会导致诸如计算开销、模型特异性约束和与安全聚合协议的兼容性问题等问题。作为回应，我们提出了一种新的框架，称为相关低级结构(CoLR) ，它利用了调整轻量级可训练参数的概念，同时保持大多数参数冻结。我们的方法大大减少了通信开销，而不会引入额外的计算负担。重要的是，我们的框架仍然完全兼容安全的聚合协议，包括同态加密的健壮使用。这种方法导致跨数据集的推荐性能最多降低了93.75。重现我们实验的代码可以在 https://github.com/nnhieu/colr-fedrec 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Efficient+Communication+and+Secure+Federated+Recommendation+System+via+Low-rank+Training)|0|
|[A User-State Based Interest Transfer Network for Cross-Domain Recommendation](https://doi.org/10.1145/3589335.3651465)|Pingjun Pan, Jialu Wang, Tingting Zhou, Wenyu Yang, Hongxiang Chen||Cross-domain recommendation (CDR) has emerged as a promising approach to improve click-through rate (CTR) in the target domain by effectively transferring user interests from the source domain. However, existing methods either use a uniform interest transfer function or focus on user-level personalized transfer functions, neglecting the fact that the transition of user states in the target domain also influence the interests in the source domain. To address this issue, we present User-State based Interest Transfer network (USIT), a novel method that takes into account the user state evolution. USIT contains two main components: a User-State Transition module (UST) and a State-Level Interests Transfer module (SLIT). UST models the evolution of user states by predicting the next state in the target domain. As the user's state evolves, SLIT adaptively weights the interests by interest-level mask attention in the source domain. Extensive offline experiments and online A/B tests demonstrate that our proposed USIT method significantly outperforms current state-of-the-art models in CDR scenarios. Currently, we have deployed it on NetEase Cloud Music, affecting millions of users.|跨域推荐已成为一种有前途的方法，可以有效地将用户的兴趣从源域转移到目标域，从而改善目标域的点进率。然而，现有的方法要么使用统一的兴趣传递函数，要么只关注用户级的个性化传递函数，忽略了目标域中用户状态的转换也会影响源域中的兴趣。为了解决这个问题，我们提出了一种基于用户状态的兴趣传输网络(USIT) ，这是一种考虑用户状态演化的新方法。USIT 包含两个主要组件: 用户状态转换模块(UST)和状态级兴趣转移模块(SLIT)。UST 通过预测目标域中的下一个状态来建模用户状态的演化。随着用户状态的演化，SLIT 通过源域中的兴趣级掩码注意自适应地对兴趣进行权重分配。大量的离线实验和在线 A/B 测试表明，我们提出的 USIT 方法在 CDR 场景中显著优于目前最先进的模型。目前，我们已经在网易云音乐上部署了它，影响了数百万用户。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+User-State+Based+Interest+Transfer+Network+for+Cross-Domain+Recommendation)|0|
|[Position Bias Estimation with Item Embedding for Sparse Dataset](https://doi.org/10.1145/3589335.3651546)|Shion Ishikawa, Yun Ching Liu, Youngjoo Chung, Yu Hirate||Estimating position bias is a well-known challenge in Learning to Rank (L2R). Click data in e-commerce applications, such as targeted advertisements and search engines, provides implicit but abundant feedback to improve personalized rankings. However, click data inherently includes various biases like position bias. Based on the position-based click model, Result Randomization and Regression Expectation-Maximization algorithm (REM) have been proposed to estimate position bias, but they require various paired observations of (item, position). In real-world scenarios of advertising, marketers frequently display advertisements in a fixed pre-determined order, which creates difficulties in estimation due to the limited availability of various pairs in the training data, resulting in a sparse dataset. We propose a variant of the REM that utilizes item embeddings to alleviate the sparsity of (item, position). Using a public dataset and internal carousel advertisement click dataset, we empirically show that item embedding with Latent Semantic Indexing (LSI) and Variational Auto-Encoder (VAE) improves the accuracy of position bias estimation and the estimated position bias enhances Learning to Rank performance. We also show that LSI is more effective as an embedding creation method for position bias estimation.|估计位置偏差是学习排名(L2R)中一个众所周知的挑战。电子商务应用程序中的点击数据，例如有针对性的广告和搜索引擎，提供了隐含但丰富的反馈，以改善个性化排名。然而，点击数据本质上包含各种偏差，如位置偏差。基于位置点击模型，结果随机化和回归期望最大化算法(REM)已被提出来估计位置偏差，但它们需要不同的配对观察(项目，位置)。在现实世界的广告情景中，营销人员经常以固定的预先确定的顺序显示广告，由于训练数据中各种对的可用性有限，造成估计困难，从而导致数据集稀疏。我们提出了 REM 的一种变体，它利用项目嵌入来减轻(项目，位置)的稀疏性。使用公共数据集和内部旋转木马广告点击数据集，我们经验表明，项目嵌入潜在语义索引(LSI)和变分自动编码器(VAE)提高了位置偏差估计的准确性，估计的位置偏差提高了学习排名的性能。我们还证明了大规模集成电路作为位置偏差估计的一种嵌入式创建方法是更有效的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Position+Bias+Estimation+with+Item+Embedding+for+Sparse+Dataset)|0|
|[When Federated Recommendation Meets Cold-Start Problem: Separating Item Attributes and User Interactions](https://doi.org/10.1145/3589334.3645525)|Chunxu Zhang, Guodong Long, Tianyi Zhou, Zijian Zhang, Peng Yan, Bo Yang||Federated recommendation system usually trains a global model on the server without direct access to users' private data on their own devices. However, this separation of the recommendation model and users' private data poses a challenge in providing quality service, particularly when it comes to new items, namely cold-start recommendations in federated settings. This paper introduces a novel method called Item-aligned Federated Aggregation (IFedRec) to address this challenge. It is the first research work in federated recommendation to specifically study the cold-start scenario. The proposed method learns two sets of item representations by leveraging item attributes and interaction records simultaneously. Additionally, an item representation alignment mechanism is designed to align two item representations and learn the meta attribute network at the server within a federated learning framework. Experiments on four benchmark datasets demonstrate IFedRec's superior performance for cold-start scenarios. Furthermore, we also verify IFedRec owns good robustness when the system faces limited client participation and noise injection, which brings promising practical application potential in privacy-protection enhanced federated recommendation systems. The implementation code is available|联邦推荐系统通常在服务器上培训一个全局模型，而不直接访问用户自己设备上的私有数据。然而，推荐模型和用户私有数据的这种分离对提供高质量的服务提出了挑战，特别是当涉及到新项目时，即联邦设置中的冷启动推荐。本文介绍了一种新的方法，称为项目对齐联邦聚合(IFedRec) ，以解决这一挑战。这是联合推荐系统中第一个专门研究冷启动方案的研究工作。该方法通过同时利用项目属性和交互记录来学习两组项目表示。此外，设计了一种项表示对齐机制，用于对齐两个项表示，并在联邦学习框架内学习服务器上的元属性网络。在四个基准数据集上的实验证明了 IFedRec 在冷启动场景下的优越性能。此外，我们还验证了 IFedRec 在有限的客户参与和噪声注入情况下具有良好的鲁棒性，这为隐私保护增强型联邦推荐系统带来了很好的实际应用前景。实现代码是可用的|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+Federated+Recommendation+Meets+Cold-Start+Problem:+Separating+Item+Attributes+and+User+Interactions)|0|
|[Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai](https://doi.org/10.1145/3589335.3648334)|Zhichao Feng, Junjie Xie, Kaiyuan Li, Yu Qin, Pengfei Wang, Qianzhong Li, Bin Yin, Xiang Li, Wei Lin, Shangguang Wang||In the recommender system of Meituan Waimai, we are dealing with ever-lengthening user behavior sequences, which pose an increasing challenge to modeling user preference effectively. Existing sequential recommendation models often fail to capture long-term dependencies or are too complex, complicating the fulfillment of Meituan Waimai's unique business needs. To better model user interests, we consider selecting relevant sub-sequences from users' extensive historical behaviors based on their preferences. In this specific scenario, we've noticed that the contexts in which users interact have a significant impact on their preferences. For this purpose, we introduce a novel method called Context-based Fast Recommendation Strategy to tackle the issue of long sequences. We first identify contexts that share similar user preferences with the target context and then locate the corresponding PoIs based on these identified contexts. This approach eliminates the necessity to select a sub-sequence for every candidate PoI, thereby avoiding high time complexity. Specifically, we implement a prototype-based approach to pinpoint contexts that mirror similar user preferences. To amplify accuracy and interpretability, we employ JS divergence of PoI attributes such as categories and prices as a measure of similarity between contexts. A temporal graph integrating both prototype and context nodes helps incorporate temporal information. We then identify appropriate prototypes considering both target contexts and short-term user preferences. Following this, we utilize contexts aligned with these prototypes to generate a sub-sequence, aimed at predicting CTR and CTCVR scores with target attention. Since its inception in 2023, this strategy has been adopted in Meituan Waimai's display recommender system, leading to a 4.6 in CTR and a 4.2|在 Waimai 的推荐系统美团中，我们正在处理不断延长的用户行为序列，这对有效地建立用户偏好模型提出了越来越大的挑战。现有的顺序推荐模型往往无法捕捉到长期的依赖关系，或者过于复杂，使得美团外卖独特业务需求的实现变得复杂。为了更好地建立用户兴趣模型，我们考虑根据用户的偏好，从用户广泛的历史行为中选择相关的子序列。在这个特定的场景中，我们已经注意到用户交互的上下文对他们的偏好有显著的影响。为此，我们提出了一种基于上下文的快速推荐策略来解决长序列的问题。我们首先识别与目标上下文共享相似用户偏好的上下文，然后基于这些识别的上下文定位相应的 PoI。这种方法消除了为每个候选 PoI 选择子序列的必要性，从而避免了高时间复杂度。具体来说，我们实现了一种基于原型的方法来精确定位反映类似用户偏好的上下文。为了增强准确性和可解释性，我们使用 PoI 属性(如类别和价格)的 JS 差异作为上下文之间相似性的度量。集成原型节点和上下文节点的时态图有助于合并时态信息。然后，我们根据目标环境和短期用户偏好确定合适的原型。接下来，我们利用与这些原型对齐的上下文来生成一个子序列，目的是用目标注意力预测 CTR 和 CTCVR 评分。自2023年推出以来，Waimai 美团的显示器推荐系统一直采用这一策略，点击率分别为4.6和4.2|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Context-based+Fast+Recommendation+Strategy+for+Long+User+Behavior+Sequence+in+Meituan+Waimai)|0|
|[Large Language Models as Data Augmenters for Cold-Start Item Recommendation](https://doi.org/10.1145/3589335.3651532)|Jianling Wang, Haokai Lu, James Caverlee, Ed H. Chi, Minmin Chen||The reasoning and generalization capabilities of LLMs can help us better understand user preferences and item characteristics, offering exciting prospects to enhance recommendation systems. Though effective while user-item interactions are abundant, conventional recommendation systems struggle to recommend cold-start items without historical interactions. To address this, we propose utilizing LLMs as data augmenters to bridge the knowledge gap on cold-start items during training. We employ LLMs to infer user preferences for cold-start items based on textual description of user historical behaviors and new item descriptions. The augmented training signals are then incorporated into learning the downstream recommendation models through an auxiliary pairwise loss. Through experiments on public Amazon datasets, we demonstrate that LLMs can effectively augment the training signals for cold-start items, leading to significant improvements in cold-start item recommendation for various recommendation models.|LLM 的推理和推广能力可以帮助我们更好地理解用户的偏好和项目特征，提供令人兴奋的前景，以加强推荐系统。尽管用户项目交互非常丰富，但传统的推荐系统很难在没有历史交互的情况下推荐冷启动项目。为了解决这个问题，我们提出利用 LLM 作为数据增强器来弥补训练过程中冷启动项目的知识缺口。我们使用 LLM 根据用户历史行为的文本描述和新项描述来推断用户对冷启动项的偏好。然后通过辅助成对损失将增强的训练信号合并到下游推荐模型中。通过对公共 Amazon 数据集的实验，我们发现 LLM 可以有效地增强冷启动项目的训练信号，从而显著改善了各种推荐模型的冷启动项目推荐。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+as+Data+Augmenters+for+Cold-Start+Item+Recommendation)|0|
|[Filter Bubble or Homogenization? Disentangling the Long-Term Effects of Recommendations on User Consumption Patterns](https://doi.org/10.1145/3589334.3645497)|Md Sanzeed Anwar, Grant Schoenebeck, Paramveer S. Dhillon||Recommendation algorithms play a pivotal role in shaping our media choices, which makes it crucial to comprehend their long-term impact on user behavior. These algorithms are often linked to two critical outcomes: homogenization, wherein users consume similar content despite disparate underlying preferences, and the filter bubble effect, wherein individuals with differing preferences only consume content aligned with their preferences (without much overlap with other users). Prior research assumes a trade-off between homogenization and filter bubble effects and then shows that personalized recommendations mitigate filter bubbles by fostering homogenization. However, because of this assumption of a tradeoff between these two effects, prior work cannot develop a more nuanced view of how recommendation systems may independently impact homogenization and filter bubble effects. We develop a more refined definition of homogenization and the filter bubble effect by decomposing them into two key metrics: how different the average consumption is between users (inter-user diversity) and how varied an individual's consumption is (intra-user diversity). We then use a novel agent-based simulation framework that enables a holistic view of the impact of recommendation systems on homogenization and filter bubble effects. Our simulations show that traditional recommendation algorithms (based on past behavior) mainly reduce filter bubbles by affecting inter-user diversity without significantly impacting intra-user diversity. Building on these findings, we introduce two new recommendation algorithms that take a more nuanced approach by accounting for both types of diversity.|推荐算法在塑造我们的媒体选择中起着关键作用，这使得理解它们对用户行为的长期影响至关重要。这些算法通常与两个关键的结果相关联: 同质化，其中用户尽管不同的潜在偏好消费相似的内容，以及过滤泡沫效应，其中具有不同偏好的个体仅消费与其偏好一致的内容(与其他用户没有太多重叠)。先前的研究假设在均匀化和过滤泡效应之间进行权衡，然后表明个性化推荐通过促进均匀化来减轻过滤泡。然而，由于在这两种效应之间进行权衡的假设，以前的工作无法对推荐系统如何独立地影响同质化和过滤泡沫效应形成更加细致入微的观点。我们通过将同质化和过滤泡沫效应分解为两个关键指标，发展了一个更精确的定义: 用户之间的平均消费有多大差异(用户间多样性)和个人的消费有多大差异(用户内部多样性)。然后，我们使用一个新的基于代理的仿真框架，使推荐系统的同质化和过滤气泡效应的影响的整体观点。我们的仿真结果表明，传统的推荐算法(基于过去的行为)主要通过影响用户间的分集来减少滤波器泡沫，而不会显著影响用户内的分集。在这些发现的基础上，我们引入了两种新的推荐算法，它们通过考虑两种类型的多样性，采取了更加细致入微的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Filter+Bubble+or+Homogenization?+Disentangling+the+Long-Term+Effects+of+Recommendations+on+User+Consumption+Patterns)|0|
|[Can One Embedding Fit All? A Multi-Interest Learning Paradigm Towards Improving User Interest Diversity Fairness](https://doi.org/10.1145/3589334.3645662)|Yuying Zhao, Minghua Xu, Huiyuan Chen, Yuzhong Chen, Yiwei Cai, Rashidul Islam, Yu Wang, Tyler Derr||Recommender systems (RSs) have gained widespread applications across various domains owing to the superior ability to capture users' interests. However, the complexity and nuanced nature of users' interests, which span a wide range of diversity, pose a significant challenge in delivering fair recommendations. In practice, user preferences vary significantly; some users show a clear preference toward certain item categories, while others have a broad interest in diverse ones. Even though it is expected that all users should receive high-quality recommendations, the effectiveness of RSs in catering to this disparate interest diversity remains under-explored. In this work, we investigate whether users with varied levels of interest diversity are treated fairly. Our empirical experiments reveal an inherent disparity: users with broader interests often receive lower-quality recommendations. To mitigate this, we propose a multi-interest framework that uses multiple (virtual) interest embeddings rather than single ones to represent users. Specifically, the framework consists of stacked multi-interest representation layers, which include an interest embedding generator that derives virtual interests from shared parameters, and a center embedding aggregator that facilitates multi-hop aggregation. Experiments demonstrate the effectiveness of the framework in achieving better trade-off between fairness and utility across various datasets and backbones.|推荐系统(RS)由于具有捕获用户兴趣的优越能力，在各个领域得到了广泛的应用。然而，用户兴趣的复杂性和细微差别，跨越广泛的多样性，对提供公平的建议构成重大挑战。实际上，用户的偏好差异很大; 一些用户对某些项目类别有明显的偏好，而另一些用户对不同的项目类别有广泛的兴趣。尽管预计所有用户都会收到高质量的建议，但是对于 RSS 在满足这种不同兴趣多样性方面的有效性仍然探索不足。在这项工作中，我们调查是否不同水平的兴趣多样性的用户被公平对待。我们的经验实验揭示了一个内在的差异: 兴趣广泛的用户经常收到质量较低的推荐。为了解决这个问题，我们提出了一个多兴趣框架，它使用多个(虚拟)兴趣嵌入而不是单个兴趣嵌入来表示用户。具体来说，该框架由多个层叠的多兴趣表示层组成，其中包括一个从共享参数获取虚拟兴趣的兴趣嵌入生成器和一个促进多跳聚合的中心嵌入聚合器。实验证明了该框架在实现跨各种数据集和主干网的公平性和实用性之间更好的平衡方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+One+Embedding+Fit+All?+A+Multi-Interest+Learning+Paradigm+Towards+Improving+User+Interest+Diversity+Fairness)|0|
|[Uplift Modeling for Target User Attacks on Recommender Systems](https://doi.org/10.1145/3589334.3645403)|Wenjie Wang, Changsheng Wang, Fuli Feng, Wentao Shi, Daizong Ding, TatSeng Chua||Recommender systems are vulnerable to injective attacks, which inject limited fake users into the platforms to manipulate the exposure of target items to all users. In this work, we identify that conventional injective attackers overlook the fact that each item has its unique potential audience, and meanwhile, the attack difficulty across different users varies. Blindly attacking all users will result in a waste of fake user budgets and inferior attack performance. To address these issues, we focus on an under-explored attack task called target user attacks, aiming at promoting target items to a particular user group. In addition, we formulate the varying attack difficulty as heterogeneous treatment effects through a causal lens and propose an Uplift-guided Budget Allocation (UBA) framework. UBA estimates the treatment effect on each target user and optimizes the allocation of fake user budgets to maximize the attack performance. Theoretical and empirical analysis demonstrates the rationality of treatment effect estimation methods of UBA. By instantiating UBA on multiple attackers, we conduct extensive experiments on three datasets under various settings with different target items, target users, fake user budgets, victim models, and defense models, validating the effectiveness and robustness of UBA.|推荐系统容易受到注入式攻击，即向平台注入有限的虚假用户，以操纵目标项目对所有用户的暴露。在本研究中，我们发现传统的注入式攻击者忽略了每个项目都有其独特的潜在受众这一事实，同时，不同用户之间的攻击难度是不同的。盲目攻击所有用户将导致虚假用户预算的浪费和较差的攻击性能。为了解决这些问题，我们将重点放在一个未被充分研究的攻击任务，称为目标用户攻击，旨在将目标项提升到特定的用户组。此外，我们通过因果透镜将不同的攻击难度描述为异质治疗效果，并提出了一个提升引导的预算分配(UBA)框架。UBA 估计每个目标用户的治疗效果，优化虚假用户预算的分配，以最大限度地提高攻击性能。理论和实证分析证明了 UBA 治疗效果评价方法的合理性。通过实例化多个攻击者的 UBA，我们在不同目标条目、目标用户、虚假用户预算、受害者模型和防御模型的不同设置下对三个数据集进行了广泛的实验，验证了 UBA 的有效性和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uplift+Modeling+for+Target+User+Attacks+on+Recommender+Systems)|0|
|[TikTok and the Art of Personalization: Investigating Exploration and Exploitation on Social Media Feeds](https://doi.org/10.1145/3589334.3645600)|Karan Vombatkere, Sepehr Mousavi, Savvas Zannettou, Franziska Roesner, Krishna P. Gummadi||Recommendation algorithms for social media feeds often function as black boxes from the perspective of users. We aim to detect whether social media feed recommendations are personalized to users, and to characterize the factors contributing to personalization in these feeds. We introduce a general framework to examine a set of social media feed recommendations for a user as a timeline. We label items in the timeline as the result of exploration vs. exploitation of the user's interests on the part of the recommendation algorithm and introduce a set of metrics to capture the extent of personalization across user timelines. We apply our framework to a real TikTok dataset and validate our results using a baseline generated from automated TikTok bots, as well as a randomized baseline. We also investigate the extent to which factors such as video viewing duration, liking, and following drive the personalization of content on TikTok. Our results demonstrate that our framework produces intuitive and explainable results, and can be used to audit and understand personalization in social media feeds.|从用户的角度来看，社交媒体 feed 的推荐算法通常起到黑盒的作用。我们的目标是检测社交媒体的 feed 推荐是否对用户个性化，并描述这些 feed 中促成个性化的因素。我们引入了一个通用框架来检查一组社交媒体提要推荐给用户作为时间轴。我们根据推荐算法对用户兴趣的探索和利用来标记时间线中的项目，并引入一组度量标准来捕获跨用户时间线的个性化程度。我们将框架应用于一个真实的 TikTok 数据集，并使用自动化 TikTok 机器人生成的基线以及随机基线验证结果。我们还调查了视频观看时间、喜欢程度和跟随程度等因素在多大程度上驱动了 TikTok 上内容的个性化。我们的结果表明，我们的框架产生直观和可解释的结果，并可用于审计和理解个性化的社会媒体饲料。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TikTok+and+the+Art+of+Personalization:+Investigating+Exploration+and+Exploitation+on+Social+Media+Feeds)|0|
|[Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits](https://doi.org/10.1145/3589334.3645420)|Yu Xia, Fang Kong, Tong Yu, Liya Guo, Ryan A. Rossi, Sungchul Kim, Shuai Li||Web-based applications such as chatbots, search engines and news recommendations continue to grow in scale and complexity with the recent surge in the adoption of LLMs. Online model selection has thus garnered increasing attention due to the need to choose the best model among a diverse set while balancing task reward and exploration cost. Organizations faces decisions like whether to employ a costly API-based LLM or a locally finetuned small LLM, weighing cost against performance. Traditional selection methods often evaluate every candidate model before choosing one, which are becoming impractical given the rising costs of training and finetuning LLMs. Moreover, it is undesirable to allocate excessive resources towards exploring poor-performing models. While some recent works leverage online bandit algorithm to manage such exploration-exploitation trade-off in model selection, they tend to overlook the increasing-then-converging trend in model performances as the model is iteratively finetuned, leading to less accurate predictions and suboptimal model selections. In this paper, we propose a time-increasing bandit algorithm TI-UCB, which effectively predicts the increase of model performances due to finetuning and efficiently balances exploration and exploitation in model selection. To further capture the converging points of models, we develop a change detection mechanism by comparing consecutive increase predictions. We theoretically prove that our algorithm achieves a logarithmic regret upper bound in a typical increasing bandit setting, which implies a fast convergence rate. The advantage of our method is also empirically validated through extensive experiments on classification model selection and online selection of LLMs. Our results highlight the importance of utilizing increasing-then-converging pattern for more efficient and economic model selection in the deployment of LLMs.|基于 Web 的应用程序，如聊天机器人、搜索引擎和新闻推荐，随着最近采用 LLM 的激增，其规模和复杂性继续增长。因此，在线模型选择越来越受到人们的关注，因为在平衡任务报酬和探索成本的同时，需要在多样化的模型集合中选择最佳模型。组织面临是否使用昂贵的基于 API 的 LLM 或局部微调的小型 LLM 等决策，权衡成本和性能。传统的选择方法通常在选择一个候选模型之前对每个候选模型进行评估，但由于训练和微调 LLM 的成本不断上升，这种方法已经变得不切实际。此外，将过多的资源用于探索表现不佳的模型也是不可取的。虽然最近的一些工作利用在线盗贼算法来管理模型选择中的这种勘探-开发权衡，但是他们往往忽略了模型性能的增加然后收敛的趋势，因为模型是迭代微调的，导致不太准确的预测和次优模型选择。本文提出了一种时间增长的土匪算法 TI-UCB，该算法能够有效地预测由于微调而带来的模型性能的提高，并能够有效地平衡模型选择中的探索和开发。为了进一步捕捉模型的收敛点，我们通过比较连续的增长预测，发展了一种变化检测机制。从理论上证明了该算法在典型的增长型强盗环境下达到了对数遗憾上界，具有较快的收敛速度。通过广泛的分类模型选择和 LLM 在线选择实验，验证了该方法的优越性。我们的研究结果突出了利用增加-然后收敛模式的重要性，更有效和经济的模型选择在 LLM 的部署。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Which+LLM+to+Play?+Convergence-Aware+Online+Model+Selection+with+Time-Increasing+Bandits)|0|
|[Full-stage Diversified Recommendation: Large-scale Online Experiments in Short-video Platform](https://doi.org/10.1145/3589334.3648144)|Nian Li, Yunzhu Pan, Chen Gao, Depeng Jin, Qingmin Liao||The recommender systems on online platforms assist users in finding personalized information, yet this also leads to the issue of limited diversity, potentially giving rise to societal issues such as filter bubbles. Despite significant progress in diversified recommendation algorithms, they have not been extensively experimented with and evaluated for effectiveness in large-scale, full-stage industrial recommender systems. Specifically, industrial recommenders usually consist of three stages of matching, ranking, and re-ranking, in which specific characteristics lead to critical challenges for promoting both recommendation diversity and user engagement. First, user interests are partially observed due to only relevance maximization. Second, item-side feature-aware bias causes imbalanced recommendations. Last, the impact of diversity perception on user engagement stresses the necessity of explicit diversity modeling. To address these challenges in industrial systems, in this work, we deploy several existing diversified algorithms in a real-world short-video platform, including exploration-exploitation, feature-aware debiasing, and diversity optimization. We conduct large-scale online A/B testing for evaluation via online metrics of user engagement and recommendation diversity. Performance improvement across full stages demonstrates the effectiveness of these simple solutions. From comparing performance across different stages and algorithms, we identify that the ranking stage is the most suitable for real-world deployment, and the combination of debiasing and diversity optimization is a promising direction in terms of diversified recommendations. This work provides experiential guidance for the large-scale deployment of diversified algorithms and the construction of a more inclusive platform on the Web.|在线平台上的推荐系统有助于用户查找个性化信息，但这也导致多样性有限的问题，可能引起诸如过滤器泡沫等社会问题。尽管在多样化推荐算法方面取得了重大进展，但在大规模、全阶段的工业推荐系统中，这些算法还没有得到广泛的实验和评估。具体来说，行业推荐通常包括匹配、排名和重新排名三个阶段，在这三个阶段中，特定的特征导致了提高推荐多样性和用户参与度的关键挑战。首先，由于只有相关性最大化，用户的兴趣被部分地观察到。第二，项目侧特征意识偏差导致推荐不平衡。最后，多样性感知对用户参与度的影响强调了显式多样性建模的必要性。为了应对工业系统中的这些挑战，本文在一个真实的短视频平台上部署了几种现有的多样化算法，包括探索-开发、特征感知去偏和多样性优化。我们通过用户参与度和推荐多样性的在线指标进行大规模的在线 A/B 测试以进行评估。整个阶段的性能改进证明了这些简单解决方案的有效性。通过比较不同阶段和算法的性能，我们发现排名阶段最适合于现实世界的部署，消偏和多样性优化相结合是多样化推荐的一个有前途的方向。这项工作为大规模部署各种算法和建立一个更具包容性的网络平台提供了经验指导。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Full-stage+Diversified+Recommendation:+Large-scale+Online+Experiments+in+Short-video+Platform)|0|
|[Improving Item-side Fairness of Multimodal Recommendation via Modality Debiasing](https://doi.org/10.1145/3589334.3648156)|Yu Shang, Chen Gao, Jiansheng Chen, Depeng Jin, Yong Li||Multimodal recommender systems have acquired applications in broad web scenarios such as e-commerce businesses and short-video platforms. Existing multimodal recommendation methods generally boost performance by introducing item-side multimodal content as supplement information. However, the common training paradigm, i.e., encoding unimodal content respectively and fusing them to fit user preference scores, makes the model biased towards items with prevailing modality content under non-uniform training data. This results in a serious item-side unfairness issue, i.e., some items with prevailing modality content are over-recommended while a large number of items don't receive adequate recommendation opportunities, leaving corresponding content providers at great disadvantage. Aiming to eliminate such modality bias and promote item-side fairness, we propose a fairness-aware modality debiasing framework based on counterfactual inference. In the training stage, we additionally introduce unimodal prediction branches to capture the modality bias. In the inference stage, we conduct a fairness-aware counterfactual inference to adaptively eliminate the modality bias. The proposed framework is model-agnostic and flexible to be implemented in various multimodal recommendation models. Extensive experiments on two datasets demonstrate that the proposed method can significantly enhance item-side fairness while providing competitive recommendation accuracy. Our proposed framework is expected to help mitigate the unfair treatment experienced by vulnerable content providers on multimedia web platforms. Codes are available in https://github.com/tsinghua-fib-lab-WWW2024-Modality-Debiasing.|多模式推荐系统已经在电子商务和短视频平台等广泛的网络场景中获得了应用。现有的多通道推荐方法通常通过引入项目端多通道内容作为补充信息来提高性能。然而，常见的训练范式，即分别编码单峰内容并将其融合以适应用户偏好得分，使得模型偏向于非统一训练数据下具有主流情态内容的项目。这导致了一个严重的项目不公平问题，也就是说，一些具有主流模式内容的项目被过度推荐，而大量的项目没有得到足够的推荐机会，使相应的内容提供商处于极大的不利地位。为了消除这种情态偏差，促进项目公平，我们提出了一个基于反事实推理的公平感知情态消偏框架。在训练阶段，我们引入单峰预测分支来捕捉模态偏差。在推理阶段，我们进行公平意识的反事实推理，以自适应地消除情态偏差。所提出的框架与模型无关，可以灵活地在各种多模式推荐模型中实现。在两个数据集上的大量实验表明，该方法在提供竞争性推荐准确性的同时，可以显著提高项目侧公平性。我们提出的框架预计将有助于减轻脆弱的内容提供商在多媒体网络平台上遭受的不公平待遇。密码有 https://github.com/tsinghua-fib-lab-www2024-modality-debiasing。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Item-side+Fairness+of+Multimodal+Recommendation+via+Modality+Debiasing)|0|
|[Knowledge Enhanced Multi-intent Transformer Network for Recommendation](https://doi.org/10.1145/3589335.3648296)|Ding Zou, Wei Wei, Feida Zhu, Chuanyu Xu, Tao Zhang, Chengfu Huo||Incorporating Knowledge Graphs into Recommendation has attracted growing attention in industry, due to the great potential of KG in providing abundant supplementary information and interpretability for the underlying models. However, simply integrating KG into recommendation usually brings in negative feedback in industry, due to the ignorance of the following two factors: i) users' multiple intents, which involve diverse nodes in KG. For example, in e-commerce scenarios, users may exhibit preferences for specific styles, brands, or colors. ii) knowledge noise, which is a prevalent issue in Knowledge Enhanced Recommendation (KGR) and even more severe in industry scenarios. The irrelevant knowledge properties of items may result in inferior model performance compared to approaches that do not incorporate knowledge. To tackle these challenges, we propose a novel approach named Knowledge Enhanced Multi-intent Transformer Network for Recommendation (KGTN), comprising two primary modules: Global Intents Modeling with Graph Transformer, and Knowledge Contrastive Denoising under Intents. Specifically, Global Intents with Graph Transformer focuses on capturing learnable user intents, by incorporating global signals from user-item-relation-entity interactions with a graph transformer, meanwhile learning intent-aware user/item representations. Knowledge Contrastive Denoising under Intents is dedicated to learning precise and robust representations. It leverages intent-aware representations to sample relevant knowledge, and proposes a local-global contrastive mechanism to enhance noise-irrelevant representation learning. Extensive experiments conducted on benchmark datasets show the superior performance of our proposed method over the state-of-the-arts. And online A/B testing results on Alibaba large-scale industrial recommendation platform also indicate the real-scenario effectiveness of KGTN.|由于 KG 在为基础模型提供丰富的补充信息和可解释性方面的巨大潜力，将知识图引入推荐系统已经引起了业界越来越多的关注。然而，由于忽视了以下两个因素，简单地将 KG 整合到推荐中往往会带来负面的反馈: 一是用户的多重意图，涉及到 KG 中的多个节点。例如，在电子商务场景中，用户可能会展示对特定风格、品牌或颜色的偏好。(ii)知识噪音，这是一个普遍的问题，在知识增强推荐(KGR) ，甚至更严重的行业情景。与不包含知识的方法相比，项目的不相关知识属性可能导致较差的模型性能。为了应对这些挑战，我们提出了一种新的方法，称为知识增强的多意图推荐转换网络(KGTN) ，包括两个主要模块: 全局意图建模与图形转换和知识对比去噪的意图。具体来说，使用图形转换器的全局意图侧重于捕获可学习的用户意图，通过将来自用户-项目-关系-实体交互的全局信号与图形转换器相结合，同时学习意图感知的用户/项目表示。目的下的知识对比去噪致力于学习精确和鲁棒的表示。该方法利用意图感知表示对相关知识进行抽样，并提出了一种局部-全局对比机制来增强噪声无关表示学习。在基准数据集上进行的大量实验表明，我们提出的方法的性能优于目前的技术水平。而在阿里巴巴大型工业推荐平台上的在线 A/B 测试结果也显示了 KGTN 的实际应用效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Enhanced+Multi-intent+Transformer+Network+for+Recommendation)|0|
|[Modeling User Viewing Flow using Large Language Models for Article Recommendation](https://doi.org/10.1145/3589335.3648305)|Zhenghao Liu, Zulong Chen, Moufeng Zhang, Shaoyang Duan, Hong Wen, Liangyue Li, Nan Li, Yu Gu, Ge Yu||This paper proposes the User Viewing Flow Modeling (SINGLE) method for the article recommendation task, which models the user constant preference and instant interest from user-clicked articles. Specifically, we first employ a user constant viewing flow modeling method to summarize the user's general interest to recommend articles. In this case, we utilize Large Language Models (LLMs) to capture constant user preferences from previously clicked articles, such as skills and positions. Then we design the user instant viewing flow modeling method to build interactions between user-clicked article history and candidate articles. It attentively reads the representations of user-clicked articles and aims to learn the user's different interest views to match the candidate article. Our experimental results on the Alibaba Technology Association (ATA) website show the advantage of SINGLE, achieving a 2.4 improvement over previous baseline models in the online A/B test. Our further analyses illustrate that SINGLE has the ability to build a more tailored recommendation system by mimicking different article viewing behaviors of users and recommending more appropriate and diverse articles to match user interests.|针对文章推荐任务，提出了用户查看流建模(SINGLE)方法，该方法从用户点击的文章中建立用户常量偏好和即时兴趣模型。具体来说，我们首先使用一个用户常量查看流建模方法来总结用户的一般兴趣来推荐文章。在这种情况下，我们利用大型语言模型(LLM)从以前单击的文章(如技能和职位)中获取常量用户首选项。然后设计用户即时查看流建模方法，建立用户点击文章历史和候选文章之间的交互。它专注地阅读用户点击的文章的表示，并旨在了解用户的不同兴趣视图，以匹配候选文章。我们在阿里巴巴科技协会(ATA)网站上的实验结果显示了 SINGLE 的优势，在线 A/B 测试中比以前的基准模型提高了2.4个百分点。我们进一步的分析表明，SINGLE 能够通过模仿用户不同的文章浏览行为，推荐更合适、更多样的文章来匹配用户的兴趣，从而建立一个更具针对性的推荐系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+User+Viewing+Flow+using+Large+Language+Models+for+Article+Recommendation)|0|
|[Counterfactual Data Augmentation for Debiased Coupon Recommendations Based on Potential Knowledge](https://doi.org/10.1145/3589335.3648306)|Junpeng Fang, Gongduo Zhang, Qing Cui, Lihong Gu, Longfei Li, Jinjie Gu, Jun Zhou||In real-world coupon recommendations, the coupon allocation process is influenced by both the recommendation model trained with historical interaction data and marketing tactics aimed at specific commercial goals. These tactics can cause an imbalance in user-coupon interactions, leading to a deviation from users' natural preferences. We refer to this deviation as the matching bias. Theoretically, unbiased data which is assumed to be collected via a randomized allocating policy (i.e., without model or tactics intervention) is ideal training data because it reflects the user's natural preferences. However, obtaining unbiased data in real-world scenarios is costly and sometimes unfeasible. To address this problem, we propose a novel model-agnostic training paradigm named <u>C</u>ounterfactual <u>D</u>ata <u>A</u>ugmentation for debiased coupon recommendations based on <u>P</u>otential <u>K</u>nowledge (CDAPK) for the marketing scenario that allocates coupons with discounts. We leverage the counterfactual data augmentation technique to answer the following key question: If a user is offered a coupon that he has never seen before in his history, will he use this coupon? By creating the counterfactual interaction data and assigning labels based on the potential knowledge of the given scenario, CDAPK shifts the original data distribution into an unbiased distribution, facilitating model optimization and debiasing. The advantage of CDAPK lies in its ability to approximate the ideal states of the training data without depleting the real-world traffic flow. We implement CDAPK on five representative models: FM, DNN, NCF, MASKNET, and DEEPFM, and conduct extensive offline and online experiments against SOTA debiasing methods to validate the superiority of CDAPK.|在现实世界的优惠券推荐中，优惠券分配过程既受到历史交互数据训练的推荐模型的影响，也受到针对特定商业目标的营销策略的影响。这些策略可能导致用户-优惠券交互的不平衡，从而导致偏离用户的自然偏好。我们把这种偏差称为匹配偏差。从理论上讲，假设通过随机分配策略收集的无偏数据(即没有模型或战术干预)是理想的训练数据，因为它反映了用户的自然偏好。然而，在现实世界中获得无偏的数据是昂贵的，有时是不可行的。为了解决这个问题，我们提出了一个新的模型不可知训练范式，命名为 < u > C </u > 反事实 < u > D </u > ata < u > A </u > 增强去偏差优惠券推荐的基础上 < u > P </u > 潜在 < u > K </u > 知识(CDAPK)的营销场景，分配折扣优惠券。我们利用反事实数据增强技术来回答以下关键问题: 如果一个用户收到了一张他以前从未见过的优惠券，他会使用这张优惠券吗？通过创建反事实交互数据并根据给定场景的潜在知识分配标签，CDAPK 将原始数据分布转化为无偏分布，有利于模型优化和消除偏差。CDAPK 的优点在于它能够在不消耗实际流量的情况下逼近训练数据的理想状态。我们在 FM、 DNN、 nCF、 MASKNET 和 DEEPFM 这五种有代表性的模型上实现了 CDAPK，并针对 SOTA 去偏方法进行了广泛的离线和在线实验，以验证 CDAPK 的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Data+Augmentation+for+Debiased+Coupon+Recommendations+Based+on+Potential+Knowledge)|0|
|[User Response Modeling in Reinforcement Learning for Ads Allocation](https://doi.org/10.1145/3589335.3648310)|Zhiyuan Zhang, Qichao Zhang, Xiaoxu Wu, Xiaowen Shi, Guogang Liao, Yongkang Wang, Xingxing Wang, Dongbin Zhao||User response modeling can enhance the learning of user representations and further improve the reinforcement learning (RL) recommender agent. However, as users' behaviors are influenced by their long-term preferences and short-term stochastic factors (e.g., weather, mood, or fashion trends), it remains challenging for previous works focusing on recurrent neural network-based user response modeling. Meanwhile, due to the dynamic interests of users, it is often unrealistic to assume the dynamics of users are stationary. Drawing inspiration from opponent modeling, we propose a novel network structure, Deep User Q-Network (DUQN), incorporating a user response probabilistic model into the Q-learning ads allocation strategy to capture the effect of the non-stationary user policy on Q-values. Moreover, we utilize the Recurrent State-Space Model (RSSM) to develop the user response model, which includes deterministic and stochastic components, enabling us to fully consider user long-term preferences and short-term stochastic factors. In particular, we design a RetNet version of RSSM (R-RSSM) to support parallel computation. The R-RSSM model can be further used for multi-step predictions to enable bootstrapping over multiple steps simultaneously. Finally, we conduct extensive experiments on a large-scale offline dataset from the Meituan food delivery platform and a public benchmark. Experimental results show that our method yields superior performance to state-of-the-art (SOTA) baselines. Moreover, our model demonstrates a significant improvement in the online A/B test and has been fully deployed on the industrial Meituan platform, serving more than 500 million customers.|用户响应建模可以增强用户表示的学习，进一步改进强化学习推荐代理。然而，由于用户的行为受到他们的长期偏好和短期随机因素(如天气、情绪或时尚趋势)的影响，以往的研究主要集中在基于反复神经网络的用户响应建模方面，这仍然具有挑战性。同时，由于用户的动态兴趣，假设用户的动态是平稳的往往是不现实的。基于对手模型的启发，本文提出了一种新的网络结构——深度用户 Q 网络(DUQN) ，将用户响应概率模型引入 Q 学习广告分配策略，以捕捉非平稳用户策略对 Q 值的影响。此外，利用递归状态空间模型(RSSM)建立了包含确定性和随机性成分的用户响应模型，使得我们能够充分考虑用户的长期偏好和短期随机性因素。特别地，我们设计了 RSSM 的 RetNet 版本(R-RSSM)来支持并行计算。R-RSSM 模型可以进一步用于多步预测，以使自举能够同时跨越多个步骤。最后，我们对来自美团食品配送平台和公共基准的大规模离线数据集进行了广泛的实验。实验结果表明，我们的方法产生的性能优于国家最先进的(SOTA)基线。此外，我们的模型显示了在线 A/B 测试的显著改进，并已完全部署在工业美团平台上，为超过5亿客户提供服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User+Response+Modeling+in+Reinforcement+Learning+for+Ads+Allocation)|0|
|[Cluster Anchor Regularization to Alleviate Popularity Bias in Recommender Systems](https://doi.org/10.1145/3589335.3648312)|Bo Chang, Changping Meng, He Ma, Shuo Chang, Yang Gu, Yajun Peng, Jingchen Feng, Yaping Zhang, Shuchao Bi, Ed H. Chi, Minmin Chen||Recommender systems are essential for finding personalized content for users on online platforms. These systems are often trained on historical user interaction data, which collects user feedback on system recommendations. This creates a feedback loop leading to popularity bias; popular content is over-represented in the data, better learned, and thus recommended even more. Less popular content struggles to reach its potential audiences. Popularity bias limits the diversity of content that users are exposed to, and makes it harder for new creators to gain traction. Existing methods to alleviate popularity bias tend to trade off the performance of popular items. In this work, we propose a new method for alleviating popularity bias in recommender systems, called the cluster anchor regularization, which partitions the large item corpus into hierarchical clusters, and then leverages the cluster information of each item to facilitate transfer learning from head items to tail items. Our results demonstrate the effectiveness of the proposed method with offline analyses and live experiments on a large-scale industrial recommendation platform, where it significantly increases tail recommendation without hurting the overall user experience.|推荐系统对于在线平台上为用户寻找个性化内容至关重要。这些系统通常接受历史用户交互数据的培训，这些数据收集用户对系统建议的反馈。这就形成了一个导致流行偏见的反馈循环; 流行内容在数据中被过度表示，学习得更好，因此被推荐更多。不太受欢迎的内容难以触及潜在受众。流行偏见限制了用户接触到的内容的多样性，使得新的创作者更难获得吸引力。现有的方法，以减少流行偏见往往权衡的表现，受欢迎的项目。本文针对推荐系统中的普遍性偏差问题，提出了一种新的聚类锚正则化方法，该方法将大项目集划分为层次聚类，然后利用每个项目的聚类信息，实现从头项目到尾项目的转移学习。我们的研究结果证明了该方法在大规模工业推荐平台上进行离线分析和现场实验的有效性，该方法在不损害整体用户体验的情况下显著提高了尾部推荐。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cluster+Anchor+Regularization+to+Alleviate+Popularity+Bias+in+Recommender+Systems)|0|
|[MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation](https://doi.org/10.1145/3589335.3648319)|Wenhao Wu, Jialiang Zhou, Ailong He, Shuguang Han, Jufeng Chen, Bo Zheng||Compared to business-to-consumer (B2C) e-commerce systems, consumer-to-consumer (C2C) e-commerce platforms usually encounter the limited-stock problem, that is, a product can only be sold one time in a C2C system. This poses several unique challenges for click-through rate (CTR) prediction. Due to limited user interactions for each product (i.e. item), the corresponding item embedding in the CTR model may not easily converge. This makes the conventional sequence modeling based approaches cannot effectively utilize user history information since historical user behaviors contain a mixture of items with different volume of stocks. Particularly, the attention mechanism in a sequence model tends to assign higher score to products with more accumulated user interactions, making limited-stock products being ignored and contribute less to the final output. To this end, we propose the Meta-Split Network (MSN) to split user history sequence regarding to the volume of stock for each product, and adopt differentiated modeling approaches for different sequences. As for the limited-stock products, a meta-learning approach is applied to address the problem of inconvergence, which is achieved by designing meta scaling and shifting networks with ID and side information. In addition, traditional approach can hardly update item embedding once the product is consumed. Thereby, we propose an auxiliary loss that makes the parameters updatable even when the product is no longer in distribution. To the best of our knowledge, this is the first solution addressing the recommendation of limited-stock product. Experimental results on the production dataset and online A/B testing demonstrate the effectiveness of our proposed method.|与 B2C 电子商务系统相比，C2C (C2C)电子商务平台通常会遇到库存有限的问题，即产品只能在 C2C 系统中销售一次。这给点进率预测带来了几个独特的挑战。由于每个产品(即项目)的用户交互有限，嵌入在 CTR 模型中的相应项目可能不容易收敛。这使得传统的基于序列建模的方法不能有效地利用用户历史信息，因为历史用户行为包含不同股票数量的混合项。特别地，序列模型中的注意机制倾向于给具有更多累积用户交互的产品分配更高的分数，使得有限库存产品被忽略，对最终产出的贡献更小。为此，我们提出了元分割网络(MSN)来分割用户历史序列对于每个产品的库存量，并采用不同的建模方法为不同的序列。对于库存有限的产品，采用元学习方法解决不收敛问题，通过设计具有 ID 和边信息的元尺度和移位网络来实现。此外，传统的方法很难更新嵌入项一旦产品消耗。因此，我们提出了一个辅助损失，使参数可更新，即使当产品不再在分布。据我们所知，这是第一个解决推荐有限库存产品的解决方案。生产数据集和在线 A/B 测试的实验结果证明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MetaSplit:+Meta-Split+Network+for+Limited-Stock+Product+Recommendation)|0|
|[Knowledge Graph-based Session Recommendation with Session-Adaptive Propagation](https://doi.org/10.1145/3589335.3648324)|Yu Wang, Amin Javari, Janani Balaji, Walid Shalaby, Tyler Derr, Xiquan Cui||Session-based recommender systems (SBRSs) predict users' next interacted items based on their historical activities. While most SBRSs capture purchasing intentions locally within each session, capturing items' global information across different sessions is crucial in characterizing their general properties. Previous works capture this cross-session information by constructing graphs and incorporating neighbor information. However, this incorporation cannot vary adaptively according to the unique intention of each session, and the constructed graphs consist of only one type of user-item interaction. To address these limitations, we propose knowledge graph-based session recommendation with session-adaptive propagation. Specifically, we build a knowledge graph by connecting items with multi-typed edges to characterize various user-item interactions. Then, we adaptively aggregate items' neighbor information considering user intention within the learned session. Experimental results demonstrate that equipping our constructed knowledge graph and session-adaptive propagation enhances session recommendation backbones by 10 study showing our proposed framework achieves 2 existing well-deployed model at The Home Depot e-platform.|基于会话的推荐系统(SBRS)根据用户的历史活动预测用户的下一个交互项。虽然大多数 SBRS 在每个会话中捕获本地的购买意图，但是在不同会话中捕获物品的全局信息对于描述它们的一般属性是至关重要的。以前的作品通过构造图表和合并邻居信息来捕获这种跨会话信息。但是，此合并不能根据每个会话的独特意图自适应地变化，并且构造的图仅包含一种类型的用户项交互。为了解决这些局限性，我们提出了基于知识图的会话推荐和会话自适应传播。具体来说，我们通过连接具有多种类型边的项目来描述各种用户-项目交互，从而构建一个知识图。然后，在学习会话中考虑用户意图，自适应地聚合项目的邻居信息。实验结果表明，我们构建的知识图和会话自适应传播增强了会话推荐骨干的10个研究表明，我们提出的框架实现了2个现有的良好部署模型在家得宝电子平台。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Graph-based+Session+Recommendation+with+Session-Adaptive+Propagation)|0|
|[Cache-Aware Reinforcement Learning in Large-Scale Recommender Systems](https://doi.org/10.1145/3589335.3648326)|Xiaoshuang Chen, Gengrui Zhang, Yao Wang, Yulin Wu, Shuo Su, Kaiqiao Zhan, Ben Wang||Modern large-scale recommender systems are built upon computation-intensive infrastructure and usually suffer from a huge difference in traffic between peak and off-peak periods. In peak periods, it is challenging to perform real-time computation for each request due to the limited budget of computational resources. The recommendation with a cache is a solution to this problem, where a user-wise result cache is used to provide recommendations when the recommender system cannot afford a real-time computation. However, the cached recommendations are usually suboptimal compared to real-time computation, and it is challenging to determine the items in the cache for each user. In this paper, we provide a cache-aware reinforcement learning (CARL) method to jointly optimize the recommendation by real-time computation and by the cache. We formulate the problem as a Markov decision process with user states and a cache state, where the cache state represents whether the recommender system performs recommendations by real-time computation or by the cache. The computational load of the recommender system determines the cache state. We perform reinforcement learning based on such a model to improve user engagement over multiple requests. Moreover, we show that the cache will introduce a challenge called critic dependency, which deteriorates the performance of reinforcement learning. To tackle this challenge, we propose an eigenfunction learning (EL) method to learn independent critics for CARL. Experiments show that CARL can significantly improve the users' engagement when considering the result cache. CARL has been fully launched in Kwai app, serving over 100 million users.|现代大规模推荐系统是建立在计算密集型基础设施之上的，通常在高峰期和非高峰期之间存在巨大的流量差异。在高峰期，由于计算资源的预算有限，对每个请求执行实时计算是一个挑战。使用缓存的推荐就是这个问题的解决方案，当推荐系统无法支付实时计算时，使用用户明智的结果缓存来提供推荐。然而，与实时计算相比，缓存的建议通常是次优的，并且为每个用户确定缓存中的项目是具有挑战性的。在本文中，我们提供了一个缓存感知强化学习(CARL)方法，通过实时计算和缓存联合优化推荐。我们用用户状态和缓存状态来描述问题，缓存状态表示马可夫决策过程是通过实时计算还是通过缓存执行推荐推荐系统。推荐系统的计算负载决定了缓存的状态。我们基于这种模型执行强化学习，以提高用户对多个请求的参与度。此外，我们表明缓存将引入一个叫做评论依赖的挑战，这会恶化强化学习的性能。为了应对这一挑战，我们提出了一种特征函数学习(EL)方法来学习独立的批评家为卡尔。实验表明，在考虑结果缓存的情况下，CARL 可以显著提高用户参与度。CARL 已经在葵应用程序中全面推出，为超过1亿用户提供服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cache-Aware+Reinforcement+Learning+in+Large-Scale+Recommender+Systems)|0|
|[Enhancing Interpretability and Effectiveness in Recommendation with Numerical Features via Learning to Contrast the Counterfactual samples](https://doi.org/10.1145/3589335.3648345)|Xiaoxiao Xu, Hao Wu, Wenhui Yu, Lantao Hu, Peng Jiang, Kun Gai||We propose a general model-agnostic Contrastive learning framework with Counterfactual Samples Synthesizing (CCSS) for modeling the monotonicity between the neural network output and numerical features which is critical for interpretability and effectiveness of recommender systems. CCSS models the monotonicity via a two-stage process: synthesizing counterfactual samples and contrasting the counterfactual samples. The two techniques are naturally integrated into a model-agnostic framework, forming an end-to-end training process. Abundant empirical tests are conducted on a publicly available dataset and a real industrial dataset, and the results well demonstrate the effectiveness of our proposed CCSS. Besides, CCSS has been deployed in our real large-scale industrial recommender, successfully serving over hundreds of millions users.|针对神经网络输出与数值特征之间的单调性对推荐系统的可解释性和有效性至关重要的问题，提出了一种基于反事实样本合成(CCSS)的通用模型无关对比学习框架。CCSS 通过两个阶段的过程对单调性进行建模: 合成反事实样本和对比反事实样本。这两种技术自然地集成到一个模型无关的框架中，形成一个端到端的培训过程。在一个公开的数据集和一个真实的工业数据集上进行了大量的实证检验，结果很好地证明了我们提出的 CCSS 的有效性。此外，CCSS 已经部署在我们真正的大规模工业推荐，成功地服务于数亿用户。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Interpretability+and+Effectiveness+in+Recommendation+with+Numerical+Features+via+Learning+to+Contrast+the+Counterfactual+samples)|0|
|[Term Importance for Transformer-Based QA Retrieval: A Case Study of StackExchange](https://doi.org/10.1145/3589335.3651568)|Bryan Zhi Yang Tan, Hady W. Lauw||Question-answering (QA) retrieval is the task of retrieving the most relevant answer to a given question from a collection of answers. Various approaches to QA retrieval have been developed recently. One successful and popular model is Contextualized Late Interaction over BERT (ColBERT), a transformer-based approach that adopts a query-document scoring mechanism that retains the granularity of transformer matching, whilst improving on efficiency. However, one key limitation is that it requires further fine-tuning for new query or collection types. In this work, we explore and propose several non-parametric retrieval augmentation methods based on explicit signals of term importance that improve over ColBERT's baseline performance. In particular, we consider the QA retrieval task in the context of StackExchange question-answering forum, verifying the effectiveness of our methods in this setting.|问题回答(QA)检索是从一组答案中检索与给定问题最相关的答案的任务。质量保证(QA)检索的各种方法最近得到了发展。一个成功且流行的模型是基于上下文的 BERT (ColBERT) ，这是一种基于变压器的方法，它采用了一种查询文档评分机制，保留了变压器匹配的粒度，同时提高了效率。但是，一个关键的限制是需要对新的查询或集合类型进行进一步的微调。在这项工作中，我们探索和提出了几种非参数检索增强方法的基础上的显式信号的项重要性，提高了 ColBERT 的基线性能。特别是，我们在 StackExchange 问答论坛的上下文中考虑 QA 检索任务，验证我们的方法在这种情况下的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Term+Importance+for+Transformer-Based+QA+Retrieval:+A+Case+Study+of+StackExchange)|0|
|[GreenRec: A Large-Scale Dataset for Green Food Recommendation](https://doi.org/10.1145/3589335.3651516)|Lingzi Zhang, Yinan Zhang, Xin Zhou, Zhiqi Shen||In response to growing interest in sustainable living from both governmental and public spheres, there is an increased effort to understand environmental implications. Recommendation systems, which are widely applied in various aspects of daily life, are crucial tools in encouraging and guiding users toward sustainable choices. However, existing public recommendation datasets primarily focus on user-item interactions and lack sufficient emphasis on sustainability, posing significant challenges to developing recommendations for sustainable items. In this work, we enrich a public food recommendation dataset by assigning environmental impact, nutritional impact, and health scores to each recipe, following well-recognized sustainability measurements. Through this work, we aim to lay a groundwork for recommending foods that are both healthy and environmentally conscious, all while maintaining recommendation accuracy.|由于政府和公共领域对可持续生活的兴趣日益增加，人们加大了了解环境影响的努力。推荐系统广泛应用于日常生活的各个方面，是鼓励和指导用户做出可持续选择的重要工具。然而，现有的公共建议数据集主要侧重于用户与项目之间的互动，对可持续性缺乏足够的重视，对制定关于可持续项目的建议构成重大挑战。在这项工作中，我们丰富了公共食品推荐数据集，通过分配环境影响，营养影响和健康评分到每个配方，遵循公认的可持续性测量。通过这项工作，我们的目标是奠定一个基础，推荐食品既健康和环保意识，同时保持推荐的准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GreenRec:+A+Large-Scale+Dataset+for+Green+Food+Recommendation)|0|
|[RimiRec: Modeling Refined Multi-interest in Hierarchical Structure for Recommendation](https://doi.org/10.1145/3589335.3651554)|Haolei Pei, Yuanyuan Xu, Yangping Zhu, Yuan Nie||Industrial recommender systems usually consist of the retrieval stage and the ranking stage, to handle the billion-scale of users and items. The retrieval stage retrieves candidate items relevant to user interests for recommendations and has attracted much attention. Frequently, a user shows refined multi-interests in a hierarchical structure. For example, a user likes Conan and Kuroba Kaito, which are the roles in hierarchical structure "Animation, Japanese Animation, Detective Conan". However, most existing methods ignore this hierarchical nature, and simply average the fine-grained interest information. Therefore, we propose a novel two-stage approach to explicitly modeling refined multi-interest in a hierarchical structure for recommendation. In the first hierarchical multi-interest mining stage, the hierarchical clustering and transformer-based model adaptively generate circles or sub-circles that users are interested in. In the second stage, the partition of retrieval space allows the EBR models to deal only with items within each circle and accurately capture users' refined interests. Experimental results show that the proposed approach achieves state-of-the-art performance. Our framework has also been deployed at Lofter.|工业推荐系统通常由检索阶段和排名阶段组成，用于处理数十亿规模的用户和项目。检索阶段检索与用户兴趣相关的候选项，以便进行推荐，引起了人们的广泛关注。通常，用户在层次结构中显示精确的多重兴趣。例如，一个用户喜欢柯南和黑叶海东，这是角色的层次结构“动画，日本动画，侦探柯南”。然而，大多数现有的方法忽略了这种层次性质，只是对细粒度的兴趣信息进行平均。因此，我们提出了一种新的两阶段的方法来显式建模精细的多兴趣在一个层次结构的推荐。在第一层次多兴趣挖掘阶段，层次聚类和基于变换器的模型自适应地生成用户感兴趣的圈或子圈。在第二阶段，检索空间的划分允许 EBR 模型只处理每个圈内的项目，并准确地捕获用户的细化兴趣。实验结果表明，该方法具有较好的性能。我们的架构也已部署在洛夫特。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RimiRec:+Modeling+Refined+Multi-interest+in+Hierarchical+Structure+for+Recommendation)|0|
|[Is Cosine-Similarity of Embeddings Really About Similarity?](https://doi.org/10.1145/3589335.3651526)|Harald Steck, Chaitanya Ekanadham, Nathan Kallus||Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless `similarities.' For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models; these have implicit and unintended effects when taking cosine-similarities of the resulting embeddings, rendering results opaque and possibly arbitrary. Based on these insights, we caution against blindly using cosine-similarity and outline alternatives.|余弦相似度是两个向量之间夹角的余弦，或者相当于两个向量标准化之间的点积。一个流行的应用是将余弦相似度应用于学习的低维特征嵌入，量化高维对象之间的语义相似度。在实际应用中，这种方法可以比嵌入向量之间的非规范化点积更好，但有时也更差。为了深入了解这一经验观察，我们研究了从正则线性模型衍生的嵌入，其中闭式解决方案促进了分析见解。我们解析地推导出余弦相似度如何产生任意的，因此也就是无意义的相似度。对于一些线性模型，相似性甚至不是唯一的，而对于另一些线性模型，它们是由正则化隐式控制的。我们讨论线性模型以外的含义: 当学习深度模型时，使用不同的正则化组合; 当采用结果嵌入的余弦相似性时，这些组合具有隐含的和意想不到的效果，使得结果不透明，可能是任意的。基于这些见解，我们警告不要盲目地使用余弦相似性和轮廓替代。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+Cosine-Similarity+of+Embeddings+Really+About+Similarity?)|0|
|[Discrete Semantic Tokenization for Deep CTR Prediction](https://doi.org/10.1145/3589335.3651558)|Qijiong Liu, Hengchang Hu, Jiahao Wu, Jieming Zhu, MinYen Kan, XiaoMing Wu||Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings and then caches them, prioritizes space over time. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user–item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.|将项目内容信息纳入点进率预测模型仍然是一个挑战，特别是在工业情景的时间和空间限制下。内容编码范例将用户和条目编码器直接集成到 CTR 模型中，随着时间的推移优先考虑空间。相比之下，基于嵌入的范例将项目和用户语义转换为潜在嵌入，然后缓存它们，随着时间的推移优先考虑空间。本文介绍了一种新的语义标记方法，提出了一种用于用户和项目表示的离散语义标记方法 UIST。UIST 促进快速训练和推理，同时保持一个保守的记忆足迹。具体来说，UIST 将密集嵌入向量量化为较短长度的离散令牌，并采用分层混合推理模块来衡量每个用户项令牌对的贡献。我们在新闻推荐上的实验结果展示了 UIST 用于 CTR 预测的有效性和效率(约200倍的空间压缩)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Discrete+Semantic+Tokenization+for+Deep+CTR+Prediction)|0|
|[Cornac-AB: An Open-Source Recommendation Framework with Native A/B Testing Integration](https://doi.org/10.1145/3589335.3651241)|Darryl Ong, QuocTuan Truong, Hady W. Lauw|Singapore Management University, Singapore, Singapore; Amazon, Seattle, WA, USA|Recommender systems significantly impact user experience across diverse domains, yet existing frameworks often prioritize offline evaluation metrics, neglecting the crucial integration of A/B testing for forward-looking assessments. In response, this paper introduces a new framework seamlessly incorporating A/B testing into the Cornac recommendation library. Leveraging a diverse collection of model implementations in Cornac, our framework enables effortless A/B testing experiment setup from offline trained models. We introduce a carefully designed dashboard and a robust backend for efficient logging and analysis of user feedback. This not only streamlines the A/B testing process but also enhances the evaluation of recommendation models in an online environment. Demonstrating the simplicity of on-demand online model evaluations, our work contributes to advancing recommender system evaluation methodologies, underscoring the significance of A/B testing and providing a practical framework for implementation. The framework is open-sourced at https://github.com/PreferredAI/cornac-ab.|推荐系统对不同领域的用户体验有重要影响，但现有框架往往优先考虑离线评估指标，忽视了前瞻性评估中 A/B 测试的关键集成。作为回应，本文介绍了一个将 A/B 测试无缝地结合到 Cornac 推荐库中的新框架。我们的框架利用 Cornac 多种多样的模型实现，从离线培训的模型轻松建立了 A/B 测试实验。我们引入了一个精心设计的仪表板和一个健壮的后端，用于有效的日志记录和用户反馈分析。这不仅简化了 A/B 测试过程，而且加强了在线环境中对推荐模式的评估。我们的工作展示了在线模型评估的简单性，有助于推进推荐系统评估方法，强调 A/B 测试的重要性，并为实施提供一个切实可行的框架。该框架在 https://github.com/preferredai/cornac-ab 上是开源的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cornac-AB:+An+Open-Source+Recommendation+Framework+with+Native+A/B+Testing+Integration)|0|
|[Towards Reliable and Efficient Long-Term Recommendation with Large Foundation Models](https://doi.org/10.1145/3589335.3651258)|Wentao Shi||Prioritizing long-term engagement rather than immediate benefits has garnered increasing attention in recent years. However, current research on long-term recommendation faces substantial challenges in terms of model evaluation and design: 1) Traditional evaluation approaches suffer from limitations due to the sparsity and bias in the offline data and fail to capture user psychological influences. 2) Existing recommenders based on Reinforcement Learning (RL) are entirely data-driven and constrained by sparse and long-tail distributed offline data. Fortunately, recent advancements in Large Foundation Models (LFMs), characterized by remarkable simulation and planning capacity, offer significant opportunities for long-term recommendation. Despite potential, due to the substantial scenario divergence between LFM pre-training and recommendation, employing LFMs in long-term recommendation still faces certain challenges. To this end, this research focuses on adapting the remarkable capabilities of LFMs to long-term recommendations to devise reliable evaluation schemes and efficient recommenders.|近年来，优先考虑长期参与而不是眼前利益的做法越来越受到重视。然而，目前长期推荐的研究在模型评价和设计方面面临着巨大的挑战: 1)传统的评价方法由于离线数据的稀疏性和偏倚性而受到限制，无法捕捉到用户的心理影响。2)现有的基于强化学习的推荐系统完全由数据驱动，并受到稀疏和长尾分布式离线数据的限制。幸运的是，最近在大型基础模型(lfMs)方面的进展，拥有属性显著的模拟和规划能力，为长期推荐提供了重要的机会。尽管潜力巨大，但由于 LFM 预训练和推荐之间存在巨大的情景差异，在长期推荐中使用 LFM 仍然面临一定的挑战。为此，本研究侧重于使 LFM 的显著能力适应长期建议，以制定可靠的评估方案和有效的建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Reliable+and+Efficient+Long-Term+Recommendation+with+Large+Foundation+Models)|0|
|[Clickbait vs. Quality: How Engagement-Based Optimization Shapes the Content Landscape in Online Platforms](https://doi.org/10.1145/3589334.3645353)|Nicole Immorlica, Meena Jagadeesan, Brendan Lucier||Online content platforms commonly use engagement-based optimization when making recommendations. This encourages content creators to invest in quality, but also rewards gaming tricks such as clickbait. To understand the total impact on the content landscape, we study a game between content creators competing on the basis of engagement metrics and analyze the equilibrium decisions about investment in quality and gaming. First, we show the content created at equilibrium exhibits a positive correlation between quality and gaming, and we empirically validate this finding on a Twitter dataset. Using the equilibrium structure of the content landscape, we then examine the downstream performance of engagement-based optimization along several axes. Perhaps counterintuitively, the average quality of content consumed by users can decrease at equilibrium as gaming tricks become more costly for content creators to employ. Moreover, engagement-based optimization can perform worse in terms of user utility than a baseline with random recommendations, and engagement-based optimization is also suboptimal in terms of realized engagement relative to quality-based optimization. Altogether, our results highlight the need to consider content creator incentives when evaluating a platform's choice of optimization metric.|在线内容平台通常在提出建议时使用基于参与的优化。这鼓励内容创作者投资于质量，但也奖励了点击诱饵等游戏技巧。为了理解对内容景观的总体影响，我们研究了基于参与度指标的内容创作者之间的竞争博弈，并分析了质量投资和博弈的均衡决策。首先，我们展示了在均衡状态下创建的内容在质量和游戏之间呈现出正相关性，并且我们在 Twitter 数据集上验证了这一发现。然后利用内容景观的均衡结构，考察了基于约定的优化在多个轴上的下游性能。也许与直觉相反的是，随着内容创作者使用游戏技巧的成本越来越高，用户消费的内容的平均质量可能在均衡状态下下降。此外，就用户效用而言，基于约定的优化可能比基于随机推荐的基线执行得更差，而就已实现的约定而言，相对于基于质量的优化而言，基于约定的优化也是次优的。总之，我们的研究结果强调了在评估平台对优化指标的选择时需要考虑内容创建者的激励因素。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Clickbait+vs.+Quality:+How+Engagement-Based+Optimization+Shapes+the+Content+Landscape+in+Online+Platforms)|0|
|[Bidder Selection Problem in Position Auctions: A Fast and Simple Algorithm via Poisson Approximation](https://doi.org/10.1145/3589334.3645418)|Nikolai Gravin, Yixuan Even Xu, Renfei Zhou||In the Bidder Selection Problem (BSP) there is a large pool of n potential advertisers competing for ad slots on the user's web page. Due to strict computational restrictions, the advertising platform can run a proper auction only for a fraction k<n of advertisers. We consider the basic optimization problem underlying BSP: given n independent prior distributions, how to efficiently find a subset of k with the objective of either maximizing expected social welfare or revenue of the platform. We study BSP in the classic multi-winner model of position auctions for welfare and revenue objectives using the optimal (respectively, VCG mechanism, or Myerson's auction) format for the selected set of bidders. Previous PTAS results for BSP optimization were only known for single-item auctions and in case of [Segev and Singla 2021] for l-unit auctions. More importantly, all of these PTASes were computational complexity results with impractically large running times, which defeats the purpose of using these algorithms under severe computational constraints. We propose a novel Poisson relaxation of BSP for position auctions that immediately implies that 1) BSP is polynomial-time solvable up to a vanishingly small error as the problem size k grows; 2) there is a PTAS for position auctions after combining our relaxation with the trivial brute force algorithm. Unlike all previous PTASes, we implemented our algorithm and did extensive numerical experiments on practically relevant input sizes. First, our experiments corroborate the previous experimental findings of Mehta et al. that a few simple heuristics used in practice perform surprisingly well in terms of approximation factor. Furthermore, our algorithm outperforms Greedy both in running time and approximation on medium and large-sized instances.|在投标人选择问题(BSP)中，有大量潜在的广告商在争夺用户网页上的广告位置。由于严格的计算限制，广告平台可以运行一个适当的拍卖只有一小部分 k < n 的广告商。我们考虑了基本的最佳化问题: 给定 n 个独立的先验分布，如何有效地找到 k 的一个子集，目标是最大化期望的社会福利或平台的收入。我们研究了福利和收入目标的经典多赢家位置拍卖模型中的 BSP 问题。以前的 PTAS 结果的 BSP 优化只知道单项拍卖和情况下的[ Segev 和 Singla 2021]的 l- 单位拍卖。更重要的是，所有这些 PTASes 都是计算复杂度不切实际的大运行时间的结果，这违背了在严格的计算约束下使用这些算法的目的。我们提出了一个新的位置拍卖的 BSP 的泊松松弛立即意味着: 1) BSP 是多项式时间可解到一个消失的小误差随着问题大小 k 的增长; 2)有一个 PTAS 的位置拍卖结合我们的松弛与平凡的蛮力算法。与以前的 PTASes 不同，我们实现了我们的算法，并对实际相关的输入大小进行了广泛的数值实验。首先，我们的实验证实了 Mehta 等人先前的实验结果，即在实践中使用的一些简单的启发式算法在近似因子方面表现出令人惊讶的好。此外，我们的算法在运行时间和对中型和大型实例的逼近方面都优于贪婪算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bidder+Selection+Problem+in+Position+Auctions:+A+Fast+and+Simple+Algorithm+via+Poisson+Approximation)|0|
|[A Fast Hop-Biased Approximation Algorithm for the Quadratic Group Steiner Tree Problem](https://doi.org/10.1145/3589334.3645325)|Xiaoqing Wang, Gong Cheng||Knowledge Graph (KG) exploration helps Web users understand the contents of a large and unfamiliar KG and extract relevant insights. The task has recently been formulated as a Quadratic Group Steiner Tree Problem (QGSTP) to search for a semantically cohesive subgraph connecting entities that match query keywords. However, on large graphs, existing algorithms for this NP-hard problem cannot meet the performance need. In this paper, we propose a novel approximation algorithm for QGSTP called HB. It finds and merges an optimal set of paths according to a Hop-Biased objective function, which not only leads to a guaranteed approximation ratio but is also decomposable by paths to enable efficient dynamic programming based search. Accompanied by a set of pruning heuristics, HB outperformed the state of the art by 1-2 orders of magnitude, empirically reducing the average time for answering a query on a million-scale graph from about one minute to one second.|知识图(KG)探索可以帮助 Web 用户理解一个大型的、不熟悉的 KG 的内容，并提取相关的见解。这个任务最近被设计成一个二次群斯坦纳树问题(qadratic Group) ，用于搜索一个语义内聚的子图，该子图连接与查询关键字匹配的实体。然而，在大图上，现有的求解这个 NP 难问题的算法不能满足性能要求。在这篇文章中，我们提出了一个新的近似演算法 QGSTP 称为 HB。它根据一个有跳偏的目标函数寻找并合并一组最优路径，不仅保证了逼近比，而且可以按路径进行分解，从而实现高效的基于动态规划的搜索。伴随着一系列的修剪启发法，HB 的表现超过了最先进的数量级1-2倍，从经验上来说，在一个百万尺度的图表上回答一个问题的平均时间从大约1分钟减少到1秒。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Fast+Hop-Biased+Approximation+Algorithm+for+the+Quadratic+Group+Steiner+Tree+Problem)|0|
|[Link Prediction on Multilayer Networks through Learning of Within-Layer and Across-Layer Node-Pair Structural Features and Node Embedding Similarity](https://doi.org/10.1145/3589334.3645646)|Lorenzo Zangari, Domenico Mandaglio, Andrea Tagarelli||Link prediction has traditionally been studied in the context of simple graphs, although real-world networks are inherently complex as they are often comprised of multiple interconnected components, or layers. Predicting links in such network systems, or multilayer networks, require to consider both the internal structure of a target layer as well as the structure of the other layers in a network, in addition to layer-specific node-attributes when available. This problem poses several challenges, even for graph neural network based approaches despite their successful and wide application to a variety of graph learning problems. In this work, we aim to fill a lack of multilayer graph representation learning methods designed for link prediction. Our proposal is a novel neural-network-based learning framework for link prediction on (attributed) multilayer networks, whose key idea is to combine (i) pairwise similarities of multilayer node embeddings learned by a graph neural network model, and (ii) structural features learned from both within-layer and across-layer link information based on overlapping multilayer neighborhoods. Extensive experimental results have shown that our framework consistently outperforms both single-layer and multilayer methods for link prediction on popular real-world multilayer networks, with an average percentage increase in AUC up to 38%. We make source code and evaluation data available at https://mlnteam-unical.github.io/resources/.|尽管现实世界的网络本质上是复杂的，因为它们通常由多个相互连接的组件或层组成，但是链接预测一直以来都是在简单图形的背景下进行研究的。在这样的网络系统或多层网络中，预测链路需要同时考虑目标层的内部结构和网络中其他层的结构，以及可用的特定层节点属性。这个问题提出了几个挑战，即使基于图神经网络的方法，尽管他们成功的和广泛的应用于各种图学习问题。在这项工作中，我们的目标是填补缺乏多层图表示学习方法设计的链接预测。本文提出了一种新的基于神经网络的多层网络链接预测学习框架，其核心思想是结合(i)图形神经网络模型学习的多层节点嵌入的成对相似性，以及(ii)基于重叠多层邻域的层内和跨层链接信息学习的结构特征。广泛的实验结果表明，我们的框架一贯优于单层和多层方法的链路预测流行的现实世界多层网络，平均百分比增加 AUC 高达38% 。我们在 https://mlnteam-unical.github.io/resources/提供源代码和评估数据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Link+Prediction+on+Multilayer+Networks+through+Learning+of+Within-Layer+and+Across-Layer+Node-Pair+Structural+Features+and+Node+Embedding+Similarity)|0|
|[Diffusion-based Negative Sampling on Graphs for Link Prediction](https://doi.org/10.1145/3589334.3645650)|TrungKien Nguyen, Yuan Fang||Link prediction is a fundamental task for graph analysis with important applications on the Web, such as social network analysis and recommendation systems, etc. Modern graph link prediction methods often employ a contrastive approach to learn robust node representations, where negative sampling is pivotal. Typical negative sampling methods aim to retrieve hard examples based on either predefined heuristics or automatic adversarial approaches, which might be inflexible or difficult to control. Furthermore, in the context of link prediction, most previous methods sample negative nodes from existing substructures of the graph, missing out on potentially more optimal samples in the latent space. To address these issues, we investigate a novel strategy of multi-level negative sampling that enables negative node generation with flexible and controllable “hardness” levels from the latent space. Our method, called Conditional Diffusion-based Multi-level Negative Sampling (DMNS), leverages the Markov chain property of diffusion models to generate negative nodes in multiple levels of variable hardness and reconcile them for effective graph link prediction. We further demonstrate that DMNS follows the sub-linear positivity principle for robust negative sampling. Extensive experiments on several benchmark datasets demonstrate the effectiveness of DMNS.|链接预测是图形分析的基础性工作，在网络上有着重要的应用，如社会网络分析和推荐系统等。现代图形链接预测方法往往采用对比的方法来学习鲁棒的节点表示，其中负采样是关键。典型的负抽样方法旨在检索基于预定义启发式或自动对抗式方法的硬实例，这些方法可能缺乏灵活性或难以控制。此外，在链路预测的背景下，大多数以前的方法从图的现有子结构中抽取负节点，在潜在空间中遗漏了潜在的更优样本。为了解决这些问题，我们研究了一种新的多级负采样策略，该策略使负节点的生成具有灵活和可控的潜在空间“硬度”水平。该方法利用扩散模型的马尔可夫链特性，在多个可变硬度水平上生成负节点，并协调它们进行有效的图形链接预测。进一步证明了对于鲁棒负采样，DMNS 遵循亚线性正性原理。在几个基准数据集上的大量实验证明了 DMNS 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diffusion-based+Negative+Sampling+on+Graphs+for+Link+Prediction)|0|
|[InfoRank: Unbiased Learning-to-Rank via Conditional Mutual Information Minimization](https://doi.org/10.1145/3589334.3645356)|Jiarui Jin, Zexue He, Mengyue Yang, Weinan Zhang, Yong Yu, Jun Wang, Julian J. McAuley||Ranking items regarding individual user interests is a core technique of multiple downstream tasks such as recommender systems. Learning such a personalized ranker typically relies on the implicit feedback from users' past click-through behaviors. However, collected feedback is biased toward previously highly-ranked items and directly learning from it would result in a "rich-get-richer" phenomenon. In this paper, we propose a simple yet sufficient unbiased learning-to-rank paradigm named InfoRank that aims to simultaneously address both position and popularity biases. We begin by consolidating the impacts of those biases into a single observation factor, thereby providing a unified approach to addressing bias-related issues. Subsequently, we minimize the mutual information between the observation estimation and the relevance estimation conditioned on the input features. By doing so, our relevance estimation can be proved to be free of bias. To implement InfoRank, we first incorporate an attention mechanism to capture latent correlations within user-item features, thereby generating estimations of observation and relevance. We then introduce a regularization term, grounded in conditional mutual information, to promote conditional independence between relevance estimation and observation estimation. Experimental evaluations conducted across three extensive recommendation and search datasets reveal that InfoRank learns more precise and unbiased ranking strategies.|关于个人用户兴趣的项目排序是多下游任务(如推荐系统)的核心技术。学习这样一个个性化的排名通常依赖于来自用户过去点击行为的隐式反馈。然而，收集的反馈偏向于先前排名较高的项目，直接从中学习将导致“富者越富”的现象。在本文中，我们提出了一个简单而充分的无偏学习排名范式称为 InfoRank，旨在同时解决位置和流行偏见。我们首先将这些偏见的影响合并为一个单一的观察因素，从而为解决偏见相关问题提供一个统一的方法。然后，在输入特征的条件下，最小化观测估计和相关估计之间的互信息。通过这样做，我们的相关性估计可以证明是没有偏见的。为了实现 InfoRank，我们首先引入一个注意机制来捕捉用户项目特征中的潜在相关性，从而产生观察和相关性的估计。然后，我们引入一个基于条件互信息的正则项，以促进相关估计和观测估计之间的条件独立。对三个广泛的推荐和搜索数据集进行的实验评估表明，InfoRank 学会了更精确和无偏见的排名策略。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=InfoRank:+Unbiased+Learning-to-Rank+via+Conditional+Mutual+Information+Minimization)|0|
|[Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback](https://doi.org/10.1145/3589334.3645365)|Zheng Wang, Bingzheng Gan, Wei Shi||In the rapidly evolving landscape of information retrieval, search engines strive to provide more personalized and relevant results to users. Query suggestion systems play a crucial role in achieving this goal by assisting users in formulating effective queries. However, existing query suggestion systems mainly rely on textual inputs, potentially limiting user search experiences for querying images. In this paper, we introduce a novel Multimodal Query Suggestion (MMQS) task, which aims to generate query suggestions based on user query images to improve the intentionality and diversity of search results. We present the RL4Sugg framework, leveraging the power of Large Language Models (LLMs) with Multi-Agent Reinforcement Learning from Human Feedback to optimize the generation process. Through comprehensive experiments, we validate the effectiveness of RL4Sugg, demonstrating a 18 compared to the best existing approach. Moreover, the MMQS has been transferred into real-world search engine products, which yield enhanced user engagement. Our research advances query suggestion systems and provides a new perspective on multimodal information retrieval.|在快速发展的信息检索中，搜索引擎努力为用户提供更个性化和相关的结果。查询建议系统通过协助用户构建有效的查询，在实现这一目标方面发挥着至关重要的作用。然而，现有的查询建议系统主要依赖于文本输入，这可能会限制用户查询图像的搜索体验。本文介绍了一种新的多模式查询建议(MMQS)任务，该任务旨在基于用户查询图像生成查询建议，以提高搜索结果的意向性和多样性。我们介绍了 RL4Sugg 框架，利用大型语言模型(LLM)和来自人类反馈的多代理强化学习的能力来优化生成过程。通过综合实验，验证了 RL4Sugg 的有效性，与现有的最佳方法相比，验证了18种方法的有效性。此外，MMQS 已经转移到现实世界的搜索引擎产品，从而提高了用户参与度。我们的研究发展了查询建议系统，为多模式信息检索提供了一个新的视角。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Query+Suggestion+with+Multi-Agent+Reinforcement+Learning+from+Human+Feedback)|0|
|[A Fast Similarity Matrix Calibration Method with Incomplete Query](https://doi.org/10.1145/3589334.3645456)|Changyi Ma, Runsheng Yu, Youzhi Zhang||The similarity matrix is at the core of similarity search problems. However, incomplete observations are ubiquitous in real scenarios leading to a less accurate similarity matrix. To alleviate this problem, in this paper, based on the key insight that the similarity matrix enjoys both the symmetric and positive semi-definiteness (PSD) properties, we propose a novel similarity matrix calibration method, which is scalable, effective, and sound. Specifically, we establish the PSD property as a constraint for the similarity matrix calibration problem and propose a novel similarity matrix calibration method to estimate the similarity matrix, which approximates the unknown complete ground-truth similarity matrix. To enable a fast optimization process, we further develop a general approximated algorithm that bypasses the computation of singular values. Theoretical analysis ensures stable calibration performance and convergence speed. Extensive experiments of similarity matrix calibration on real-world datasets demonstrate that our proposed method outperforms baseline methods in terms of both accuracy and speed.|相似矩阵是最近邻搜索问题的核心。然而，不完全观测在实际场景中普遍存在，导致相似矩阵不够精确。为了解决这一问题，本文基于相似矩阵同时具有对称性和正半确定性的特点，提出了一种可扩展、有效、合理的相似矩阵标定方法。具体来说，我们将 PSD 特性作为相似矩阵标定问题的约束条件，提出了一种新的相似矩阵标定方法来估计相似矩阵，该方法逼近未知的完全地面真相相似矩阵。为了实现快速优化过程，我们进一步开发了一个通用的近似算法，绕过奇异值的计算。理论分析保证了稳定的校准性能和收敛速度。在实际数据集上进行的大量相似矩阵校正实验表明，该方法在精度和速度上均优于基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Fast+Similarity+Matrix+Calibration+Method+with+Incomplete+Query)|0|
|[Whole Page Unbiased Learning to Rank](https://doi.org/10.1145/3589334.3645474)|Haitao Mao, Lixin Zou, Yujia Zheng, Jiliang Tang, Xiaokai Chu, Jiashu Zhao, Qian Wang, Dawei Yin||The page presentation biases in the information retrieval system, especially on the click behavior, is a well-known challenge that hinders improving ranking models' performance with implicit user feedback. Unbiased Learning to Rank (ULTR) algorithms are then proposed to learn an unbiased ranking model with biased click data. However, most existing algorithms are specifically designed to mitigate position-related bias, e.g., trust bias, without considering biases induced by other features in search result page presentation(SERP), e.g. attractive bias induced by the multimedia. Unfortunately, those biases widely exist in industrial systems and may lead to an unsatisfactory search experience. Therefore, we introduce a new problem, i.e., whole-page Unbiased Learning to Rank(WP-ULTR), aiming to handle biases induced by whole-page SERP features simultaneously. It presents tremendous challenges: (1) a suitable user behavior model (user behavior hypothesis) can be hard to find; and (2) complex biases cannot be handled by existing algorithms. To address the above challenges, we propose a Bias Agnostic whole-page unbiased Learning to rank algorithm, named BAL, to automatically find the user behavior model with causal discovery and mitigate the biases induced by multiple SERP features with no specific design. Experimental results on a real-world dataset verify the effectiveness of the BAL.|信息检索系统中的页面呈现偏差，尤其是点击行为，是一个众所周知的挑战，它阻碍了隐含用户反馈来提高排名模型的性能。然后提出无偏学习排序(ULTR)算法来学习一个有偏点击数据的无偏排序模型。然而，大多数现有的算法都是专门设计来减轻位置相关的偏差，例如信任偏差，而不考虑搜索结果页面呈现(SERP)中其他特征引起的偏差，例如多媒体引起的吸引力偏差。不幸的是，这些偏见广泛存在于工业系统中，并可能导致不令人满意的搜索体验。因此，我们引入了一个新的问题，即整页无偏学习排序(WP-ULTR) ，旨在同时处理整页 SERP 特征引起的偏差。它提出了巨大的挑战: (1)一个合适的用户行为模型(用户行为假设)可能很难找到; (2)复杂的偏差不能处理现有的算法。为了解决上述挑战，我们提出了一种名为 BAL 的偏倚不可知全页无偏学习排序算法，以自动找到具有因果发现的用户行为模型，并减轻由没有特定设计的多个 SERP 特征引起的偏差。在实际数据集上的实验结果验证了 BAL 算法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Whole+Page+Unbiased+Learning+to+Rank)|0|
|[Mitigating Exploitation Bias in Learning to Rank with an Uncertainty-aware Empirical Bayes Approach](https://doi.org/10.1145/3589334.3645487)|Tao Yang, Cuize Han, Chen Luo, Parth Gupta, Jeff M. Phillips, Qingyao Ai||Ranking is at the core of many artificial intelligence (AI) applications, including search engines, recommender systems, etc. Modern ranking systems are often constructed with learning-to-rank (LTR) models built from user behavior signals. While previous studies have demonstrated the effectiveness of using user behavior signals (e.g., clicks) as both features and labels of LTR algorithms, we argue that existing LTR algorithms that indiscriminately treat behavior and non-behavior signals in input features could lead to suboptimal performance in practice. Particularly because user behavior signals often have strong correlations with the ranking objective and can only be collected on items that have already been shown to users, directly using behavior signals in LTR could create an exploitation bias that hurts the system performance in the long run. To address the exploitation bias, we propose EBRank, an empirical Bayes-based uncertainty-aware ranking algorithm. Specifically, to overcome exploitation bias brought by behavior features in ranking models, EBRank uses a sole non-behavior feature based prior model to get a prior estimation of relevance. In the dynamic training and serving of ranking systems, EBRank uses the observed user behaviors to update posterior relevance estimation instead of concatenating behaviors as features in ranking models. Besides, EBRank additionally applies an uncertainty-aware exploration strategy to explore actively, collect user behaviors for empirical Bayesian modeling and improve ranking performance. Experiments on three public datasets show that EBRank is effective, practical and significantly outperforms state-of-the-art ranking algorithms.|排名是许多人工智能(AI)应用程序的核心，包括搜索引擎、推荐系统等。现代排序系统通常是由用户行为信号构建的学习排序(LTR)模型构建的。虽然以前的研究已经证明了使用用户行为信号(例如点击)作为 LTR 算法的特征和标签的有效性，但是我们认为现有的 LTR 算法在输入特征中不加区分地处理行为和非行为信号可能导致实践中的次优性能。特别是由于用户行为信号往往与排名目标有很强的相关性，只能在已经显示给用户的条目上收集，直接在 LTR 中使用行为信号可能会产生利用偏差，从长远来看会损害系统性能。针对开发偏差问题，提出了一种基于经验贝叶斯的不确定性排序算法 EBRank。具体来说，为了克服排序模型中行为特征带来的利用偏差，EBRank 使用基于非行为特征的先验模型来获得相关性的先验估计。在排序系统的动态训练和服务中，EBRank 利用观察到的用户行为更新后验相关估计，而不是将用户行为作为排序模型的特征进行连接。此外，EBRank 还采用了不确定性探索策略，积极探索，收集用户行为进行经验贝叶斯建模，提高排序性能。在三个公共数据集上的实验表明，EBRank 算法是有效的、实用的，其性能明显优于最先进的排序算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mitigating+Exploitation+Bias+in+Learning+to+Rank+with+an+Uncertainty-aware+Empirical+Bayes+Approach)|0|
|[Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://doi.org/10.1145/3589334.3645574)|Songhao Wu, Quan Tu, Hong Liu, Jia Xu, Zhongyi Liu, Guannan Zhang, Ran Wang, Xiuying Chen, Rui Yan||Session search involves a series of interactive queries and actions to fulfill user's complex information need. Current strategies typically prioritize sequential modeling for deep semantic understanding, overlooking the graph structure in interactions. While some approaches focus on capturing structural information, they use a generalized representation for documents, neglecting the word-level semantic modeling. In this paper, we propose Symbolic Graph Ranker (SGR), which aims to take advantage of both text-based and graph-based approaches by leveraging the power of recent Large Language Models (LLMs). Concretely, we first introduce a set of symbolic grammar rules to convert session graph into text. This allows integrating session history, interaction process, and task instruction seamlessly as inputs for the LLM. Moreover, given the natural discrepancy between LLMs pre-trained on textual corpora, and the symbolic language we produce using our graph-to-text grammar, our objective is to enhance LLMs' ability to capture graph structures within a textual format. To achieve this, we introduce a set of self-supervised symbolic learning tasks including link prediction, node content generation, and generative contrastive learning, to enable LLMs to capture the topological information from coarse-grained to fine-grained. Experiment results and comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm the superiority of our approach. Our paradigm also offers a novel and effective methodology that bridges the gap between traditional search strategies and modern LLMs.|会话搜索涉及到一系列的交互式查询和操作，以满足用户复杂的信息需求。当前的策略通常优先考虑深层语义理解的顺序建模，忽略了交互中的图形结构。虽然一些方法侧重于获取结构信息，但它们对文档使用广义表示，忽略了词级语义建模。在本文中，我们提出了符号图排序器(SGR) ，它旨在利用基于文本和基于图的方法，利用最近的大型语言模型(LLM)的能力。具体来说，我们首先引入了一套符号语法规则来将会话图转换成文本。这允许将会话历史、交互过程和任务指令无缝地集成为 LLM 的输入。此外，考虑到预先在文本语料库中训练的 LLM 与我们使用图到文本语法生成的符号语言之间的自然差异，我们的目标是提高 LLM 在文本格式中捕获图结构的能力。为了实现这一目标，我们引入了一组自监督的符号学习任务，包括链路预测、节点内容生成和生成对比学习，使 LLM 能够捕获从粗粒度到细粒度的拓扑信息。通过对 AOL 和天宫 ST 两个基准数据集的实验和综合分析，证实了该方法的优越性。我们的范式还提供了一个新颖和有效的方法，桥梁之间的差距传统搜索策略和现代 LLM。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unify+Graph+Learning+with+Text:+Unleashing+LLM+Potentials+for+Session+Search)|0|
|[Matching Feature Separation Network for Domain Adaptation in Entity Matching](https://doi.org/10.1145/3589334.3645397)|Chenchen Sun, Yang Xu, Derong Shen, Tiezheng Nie||Entity matching (EM) determines whether two records from different data sources refer to the same real-world entity. It is a fundamental task in knowledge graph construction and data integration. Currently, deep learning (DL) based EM methods have achieved state-of-the-art (SOTA) results. However, apply-ing DL-based EM methods often costs a lot of human efforts to label the data. To address this challenge, we propose a new do-main adaptation (DA) framework for EM called Matching Fea-ture Separation Network (MFSN). We implement DA by sepa-rating private and common matching features. Briefly, MFSN first uses three encoders to explicitly model the private and common matching features in both the source and target do-mains. Then, it transfers the knowledge learned from the source common matching features to the target domain. We also pro-pose an enhanced variant called Feature Representation and Separation Enhanced MFSN (MFSN-FRSE). Compared with MFSN, it has superior feature representation and separation capabilities. We evaluate the effectiveness of MFSN and MFSN-FRSE on twelve DA in EM tasks. The results show that our framework is approximately 7% higher in F1 score on average than the previous SOTA methods. Then, we verify the effec-tiveness of each module in MFSN and MFSN-FRSE by ablation study. Finally, we explore the optimal strategy of each module in MFSN and MFSN-FRSE through detailed tests.|实体匹配(EM)确定来自不同数据源的两个记录是否引用相同的现实世界实体。它是知识图形构建和数据集成的基础性工作。目前，基于深度学习(DL)的 EM 方法已经取得了最先进的效果。然而，应用基于 DL 的 EM 方法通常需要花费大量人力来标记数据。为了应对这一挑战，我们提出了一种新的电磁主干适应(DA)框架，称为匹配特征分离网络(MFSN)。我们通过分级私有和公共匹配特性来实现 DA。简而言之，MFSN 首先使用三个编码器来显式地为源和目标操作系统中的私有和公共匹配特性建模。然后，将从源共同匹配特征中学到的知识转移到目标领域。我们还提出了一种增强型 MFSN (MFSN-FRSE) ，称为特征表示和分离增强 MFSN。与 MFSN 相比，它具有优越的特征表示和分离能力。我们评估了多功能神经网络(MFSN)和多功能神经网络-自由神经网络(MFSN-frse)在电磁任务中对十二种 DA 的有效性。结果表明，我们的框架在 F1得分平均比以前的 SOTA 方法高约7% 。然后，通过烧蚀研究验证了 MFSN 和 MFSN-FRSE 中各模块的有效性。最后，通过详细的测试，探讨了 MFSN 和 MFSN-FRSE 中各模块的优化策略。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Matching+Feature+Separation+Network+for+Domain+Adaptation+in+Entity+Matching)|0|
|[FedUP: Querying Large-Scale Federations of SPARQL Endpoints](https://doi.org/10.1145/3589334.3645704)|Julien AimonierDavat, Brice Nédelec, Minh Hoang Dang, Pascal Molli, Hala SkafMolli||Processing SPARQL queries over large federations of SPARQL endpoints is crucial for keeping the Semantic Web decentralized. Despite the existence of hundreds of SPARQL endpoints, current federation engines only scale to dozens. One major issue comes from the current definition of the source selection problem, i.e., finding the minimal set of SPARQL endpoints to contact per triple pattern. Even if such a source selection is minimal, only a few combinations of sources may return results. Consequently, most of the query processing time is wasted evaluating combinations that return no results. In this paper, we introduce the concept of Result-Aware query plans. This concept ensures that every subquery of the query plan effectively contributes to the result of the query. To compute a Result-Aware query plan, we propose FedUP, a new federation engine able to produce Result-Aware query plans by tracking the provenance of query results. However, getting query results requires computing source selection, and computing source selection requires query results. To break this vicious cycle, FedUP computes results and provenances on tiny quotient summaries of federations at the cost of source selection accuracy. Experimental results on federated benchmarks demonstrate that FedUP outperforms state-of-the-art federation engines by orders of magnitude in the context of large-scale federations.|在大型 SPARQL 端点联合上处理 SPARQL 查询对于保持语义 Web 的分散性至关重要。尽管存在数百个 SPARQL 端点，但当前的联合引擎只能扩展到几十个。一个主要问题来自源选择问题的当前定义，也就是说，寻找每个三重模式联系的最小 SPARQL 端点集。即使这样的源选择很少，也只有少数几个源组合可以返回结果。因此，大多数查询处理时间都浪费在评估不返回任何结果的组合上。本文介绍了结果感知查询计划的概念。这个概念确保查询计划的每个子查询都有效地提供查询结果。为了计算结果感知查询计划，我们提出了 FedUP，这是一个新的联合引擎，能够通过跟踪查询结果的来源来生成结果感知查询计划。但是，获取查询结果需要计算源选择，而计算源选择需要查询结果。为了打破这种恶性循环，FedUP 以牺牲源选择的准确性为代价，在联合的微小商汇总上计算结果和出处。联邦基准测试的实验结果表明，在大规模联邦环境中，联邦统一数量级的性能优于最先进的联邦引擎。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedUP:+Querying+Large-Scale+Federations+of+SPARQL+Endpoints)|0|
|[Prompt-enhanced Federated Content Representation Learning for Cross-domain Recommendation](https://doi.org/10.1145/3589334.3645337)|Lei Guo, Ziang Lu, Junliang Yu, Quoc Viet Hung Nguyen, Hongzhi Yin||Cross-domain Recommendation (CDR) as one of the effective techniques in alleviating the data sparsity issues has been widely studied in recent years. However, previous works may cause domain privacy leakage since they necessitate the aggregation of diverse domain data into a centralized server during the training process. Though several studies have conducted privacy preserving CDR via Federated Learning (FL), they still have the following limitations: 1) They need to upload users' personal information to the central server, posing the risk of leaking user privacy. 2) Existing federated methods mainly rely on atomic item IDs to represent items, which prevents them from modeling items in a unified feature space, increasing the challenge of knowledge transfer among domains. 3) They are all based on the premise of knowing overlapped users between domains, which proves impractical in real-world applications. To address the above limitations, we focus on Privacy-preserving Cross-domain Recommendation (PCDR) and propose PFCR as our solution. For Limitation 1, we develop a FL schema by exclusively utilizing users' interactions with local clients and devising an encryption method for gradient encryption. For Limitation 2, we model items in a universal feature space by their description texts. For Limitation 3, we initially learn federated content representations, harnessing the generality of natural language to establish bridges between domains. Subsequently, we craft two prompt fine-tuning strategies to tailor the pre-trained model to the target domain. Extensive experiments on two real-world datasets demonstrate the superiority of our PFCR method compared to the SOTA approaches.|跨域推荐(CDR)作为缓解数据稀疏性问题的有效技术之一，近年来得到了广泛的研究。然而，以前的工作可能会导致域隐私泄漏，因为它们需要在培训过程中将不同的域数据集中到一个集中的服务器。虽然已有多项研究通过联邦学习(FL)进行隐私保护 CDR，但仍存在以下局限性: 1)需要将用户的个人信息上传到中央服务器，存在泄露用户隐私的风险。2)现有的联邦方法主要依靠原子项目 ID 来表示项目，这使得联邦方法无法在统一的特征空间中对项目进行建模，增加了领域间知识转移的难度。3)它们都是建立在知道域间重叠用户的前提上的，这在现实应用中是不切实际的。针对上述局限性，本文重点研究了保护隐私的跨域推荐(PCDR)技术，并提出了 PCR 技术作为解决方案。对于限制1，我们通过专门利用用户与本地客户端的交互和设计一种梯度加密的加密方法来开发一个 FL 模式。对于局限性2，我们通过描述文本在通用特征空间中对项目进行建模。对于限制3，我们最初学习联邦内容表示，利用自然语言的通用性在域之间建立桥梁。随后，我们制定了两个快速微调策略，以裁剪预训练模型的目标领域。在两个实际数据集上的大量实验证明了我们的 PFCR 方法相对于 SOTA 方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prompt-enhanced+Federated+Content+Representation+Learning+for+Cross-domain+Recommendation)|0|
|[PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning](https://doi.org/10.1145/3589334.3645359)|Wei Wei, Jiabin Tang, Lianghao Xia, Yangqin Jiang, Chao Huang||Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems. These modalities provide intuitive semantics that facilitate modality-aware user preference modeling. However, two key challenges in multi-modal recommenders remain unresolved: i) The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT). ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference. To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation. Specifically, PromptMM conducts model compression through distilling u-i edge relationship and multi-modal node content from cumbersome teachers to relieve students from the additional feature reduction parameters. To bridge the semantic gap between multi-modal context and collaborative signals for empowering the overfitting teacher, soft prompt-tuning is introduced to perform student task-adaptive. Additionally, to adjust the impact of inaccuracies in multimedia data, a disentangled multi-modal list-wise distillation is developed with modality-aware re-weighting mechanism. Experiments on real-world data demonstrate PromptMM's superiority over existing techniques. Ablation tests confirm the effectiveness of key components. Additional tests show the efficiency and effectiveness.|多媒体在线平台(如亚马逊、 TikTok)从将多媒体(如视觉、文本和声学)内容纳入其个人推荐系统中获益匪浅。这些模式提供了直观的语义，促进了模式感知用户偏好建模。然而，多模态推荐器中的两个关键挑战仍然没有解决: i)引入具有大量额外参数的多模态编码器导致过拟合，给定提取器(例如，ViT，BERT)提供的高维多模态特征。(2)侧面信息不可避免地引入不准确性和冗余性，从而使情态交互依赖偏离了真实的用户偏好。为了解决这些问题，我们建议通过多模态知识提取(PromptMM)简化和授权推荐程序，并进行及时调优，以实现自适应质量提取。具体来说，PromptMM 通过从繁琐的教师中提取 u-i 边缘关系和多模态节点内容来进行模型压缩，以减轻学生对附加特征约简参数的依赖。为了消除多模态语境和协作信号之间的语义鸿沟，提出了软提示调整来实现学生的任务适应。此外，为了调整多媒体数据中不准确性的影响，利用模态感知的重新加权机制，开发了一种分离多模态列表式精馏算法。对真实世界数据的实验证明了 PromptMM 相对于现有技术的优越性。烧蚀试验证实了关键部位的有效性。额外的测试显示了效率和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PromptMM:+Multi-Modal+Knowledge+Distillation+for+Recommendation+with+Prompt-Tuning)|0|
|[Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum Heating](https://doi.org/10.1145/3589334.3645377)|Hyunsik Jeon, Jongeun Lee, Jeongin Yun, U Kang||How can we recommend cold-start bundles to users? The cold-start problem in bundle recommendation is crucial because new bundles are continuously created on the Web for various marketing purposes. Despite its importance, existing methods for cold-start item recommendation are not readily applicable to bundles. They depend overly on historical information, even for less popular bundles, failing to address the primary challenge of the highly skewed distribution of bundle interactions. In this work, we propose CoHeat (Popularity-based Coalescence and Curriculum Heating), an accurate approach for cold-start bundle recommendation. CoHeat first represents users and bundles through graph-based views, capturing collaborative information effectively. To estimate the user-bundle relationship more accurately, CoHeat addresses the highly skewed distribution of bundle interactions through a popularity-based coalescence approach, which incorporates historical and affiliation information based on the bundle's popularity. Furthermore, it effectively learns latent representations by exploiting curriculum learning and contrastive learning. CoHeat demonstrates superior performance in cold-start bundle recommendation, achieving up to 193|我们如何向用户推荐冷启动捆绑包？捆绑包推荐中的冷启动问题是至关重要的，因为出于各种营销目的，新的捆绑包不断地在 Web 上创建。尽管它的重要性，现有的冷启动项目推荐方法不容易适用于捆绑包。它们过度依赖于历史信息，即使对于不太流行的捆绑包也是如此，未能解决捆绑包交互的高度倾斜分布这一主要挑战。在这项工作中，我们提出了 CoHeat (基于流行度的聚合和课程加热) ，一种准确的冷启动捆绑推荐方法。CoHeat 首先通过基于图形的视图表示用户和捆绑包，有效地捕获协作信息。为了更准确地估计用户与捆绑包的关系，CoHeat 通过基于流行度的合并方法解决了捆绑包交互的高度倾斜分布，该方法结合了基于捆绑包流行度的历史和从属关系信息。此外，它还通过课程学习和对比学习有效地学习潜在表征。CoHeat 在冷启动捆绑推荐系统中表现出卓越的性能，最高达到193|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cold-start+Bundle+Recommendation+via+Popularity-based+Coalescence+and+Curriculum+Heating)|0|
|[Scalable and Provably Fair Exposure Control for Large-Scale Recommender Systems](https://doi.org/10.1145/3589334.3645390)|Riku Togashi, Kenshi Abe, Yuta Saito||Typical recommendation and ranking methods aim to optimize the satisfaction of users, but they are often oblivious to their impact on the items (e.g., products, jobs, news, video) and their providers. However, there has been a growing understanding that the latter is crucial to consider for a wide range of applications, since it determines the utility of those being recommended. Prior approaches to fairness-aware recommendation optimize a regularized objective to balance user satisfaction and item fairness based on some notion such as exposure fairness. These existing methods have been shown to be effective in controlling fairness, however, most of them are computationally inefficient, limiting their applications to only unrealistically small-scale situations. This indeed implies that the literature does not yet provide a solution to enable a flexible control of exposure in the industry-scale recommender systems where millions of users and items exist. To enable a computationally efficient exposure control even for such large-scale systems, this work develops a scalable, fast, and fair method called exposure-aware ADMM (exADMM). exADMM is based on implicit alternating least squares (iALS), a conventional scalable algorithm for collaborative filtering, but optimizes a regularized objective to achieve a flexible control of accuracy-fairness tradeoff. A particular technical challenge in developing exADMM is the fact that the fairness regularizer destroys the separability of optimization subproblems for users and items, which is an essential property to ensure the scalability of iALS. Therefore, we develop a set of optimization tools to enable yet scalable fairness control with provable convergence guarantees as a basis of our algorithm.|典型的推荐和排名方法旨在优化用户的满意度，但他们往往忽视了它们对项目(例如，产品、工作、新闻、视频)及其提供商的影响。然而，人们日益认识到，后者对于考虑范围广泛的应用至关重要，因为它决定了所推荐的应用的效用。先前的公平感知推荐方法优化了一个规范化的目标，以平衡用户满意度和项目公平性，基于一些概念，如曝光公平性。这些现有的方法已被证明是有效的控制公平，但是，其中大多数是计算效率低下，限制其应用于不切实际的小规模情况。这实际上意味着，文献尚未提供一个解决方案，以便在存在数百万用户和项目的行业规模的推荐系统中实现对曝光的灵活控制。为了实现计算效率高的曝光控制，即使是在这样的大规模系统中，这项工作开发了一种可伸缩、快速和公平的方法，称为曝光感知 ADMM (exADMM)。ExADMM 基于隐式交替最小二乘(iALS)算法，这是一种传统的可扩展的协同过滤算法，但优化了正则化目标，以实现灵活的精度-公平权衡控制。开发 exADMM 的一个特殊的技术挑战是公平正则化器破坏了用户和项目优化子问题的可分性，而这种可分性是保证 iALS 可扩展性的一个重要特性。因此，我们开发了一套优化工具，使得可扩展的公平性控制具有可证明的收敛保证作为我们的算法的基础。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+and+Provably+Fair+Exposure+Control+for+Large-Scale+Recommender+Systems)|0|
|[Causally Debiased Time-aware Recommendation](https://doi.org/10.1145/3589334.3645400)|Lei Wang, Chen Ma, Xian Wu, Zhaopeng Qiu, Yefeng Zheng, Xu Chen||Time-aware recommendation has been widely studied for modeling the user dynamic preference and a lot of models have been proposed. However, these models often overlook the fact that users may not behave evenly on the timeline, and observed datasets can be biased by user intrinsic preferences or previous recommender systems, leading to degraded model performance. We propose a causally debiased time-aware recommender framework to accurately learn user preference. We formulate the task of time-aware recommendation by a causal graph, identifying two types of biases on the item and time levels. To optimize the ideal unbiased learning objective, we propose a debiased framework based on the inverse propensity score (IPS) and extend it to the doubly robust method. Considering that the user preference can be diverse and complex, which may result in unmeasured confounders, we develop a sensitivity analysis method to obtain more accurate IPS. We theoretically draw a connection between the proposed method and the ideal learning objective, which to the best of our knowledge, is the first time in the research community. We conduct extensive experiments on three real-world datasets to demonstrate the effectiveness of our model. To promote this research direction, we have released our project at https://paitesanshi.github.io/CDTR/.|时间感知推荐在用户动态偏好建模方面得到了广泛的研究，并提出了许多模型。然而，这些模型往往忽略了这样一个事实，即用户可能在时间轴上的行为不均匀，并且观察到的数据集可能受到用户内在偏好或以前的推荐系统的影响，导致模型性能下降。我们提出了一个因果消除时间感知的推荐框架，以准确地了解用户偏好。我们通过一个因果图表来描述有时间意识的推荐任务，识别项目和时间水平上的两种偏差。为了优化理想的无偏学习目标，提出了一种基于逆倾向得分(IPS)的去偏框架，并将其推广到双稳健方法。考虑到用户偏好可能是多样和复杂的，这可能导致不可测量的混杂因素，我们开发了一个敏感度分析方法来获得更准确的 IPS。我们从理论上把这种方法与理想的学习目标联系起来，据我们所知，这在研究界还是第一次。为了验证模型的有效性，我们在三个实际数据集上进行了大量的实验。为了推动这个研究方向，我们已经在 https://paitesanshi.github.io/cdtr/上发布了我们的项目。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causally+Debiased+Time-aware+Recommendation)|0|
|[Learning Category Trees for ID-Based Recommendation: Exploring the Power of Differentiable Vector Quantization](https://doi.org/10.1145/3589334.3645484)|Qijiong Liu, Jiaren Xiao, Lu Fan, Jieming Zhu, XiaoMing Wu||Category information plays a crucial role in enhancing the quality and personalization of recommender systems. Nevertheless, the availability of item category information is not consistently present, particularly in the context of ID-based recommendations. In this work, we propose a novel approach to automatically learn and generate entity (i.e., user or item) category trees for ID-based recommendation. Specifically, we devise a differentiable vector quantization framework for automatic category tree generation, namely CAGE, which enables the simultaneous learning and refinement of categorical code representations and entity embeddings in an end-to-end manner, starting from the randomly initialized states. With its high adaptability, CAGE can be easily integrated into both sequential and non-sequential recommender systems. We validate the effectiveness of CAGE on various recommendation tasks including list completion, collaborative filtering, and click-through rate prediction, across different recommendation models. We release the code and data for others to reproduce the reported results.|分类信息在提高推荐系统的质量和个性化方面起着至关重要的作用。尽管如此，项目类别信息的可用性并不一致，特别是在基于 ID 的推荐上下文中。在这项工作中，我们提出了一个新颖的方法来自动学习和生成实体(即，用户或项目)的类别树为基于身份的推荐。具体来说，我们设计了一个自动分类树生成的可微向量量化框架，即 CAGE，它能够从随机初始化状态开始，以端到端的方式同时学习和细化分类代码表示和实体嵌入。CAGE 具有很强的适应性，可以很容易地集成到顺序推荐系统和非顺序推荐系统中。在不同的推荐模型中，我们验证了 CAGE 在各种推荐任务中的有效性，包括列表完成、协同过滤和点进率预测。我们将代码和数据发布给其他人，以重现报告的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Category+Trees+for+ID-Based+Recommendation:+Exploring+the+Power+of+Differentiable+Vector+Quantization)|0|
|[Poisoning Federated Recommender Systems with Fake Users](https://doi.org/10.1145/3589334.3645492)|Ming Yin, Yichang Xu, Minghong Fang, Neil Zhenqiang Gong||Federated recommendation is a prominent use case within federated learning, yet it remains susceptible to various attacks, from user to server-side vulnerabilities. Poisoning attacks are particularly notable among user-side attacks, as participants upload malicious model updates to deceive the global model, often intending to promote or demote specific targeted items. This study investigates strategies for executing promotion attacks in federated recommender systems. Current poisoning attacks on federated recommender systems often rely on additional information, such as the local training data of genuine users or item popularity. However, such information is challenging for the potential attacker to obtain. Thus, there is a need to develop an attack that requires no extra information apart from item embeddings obtained from the server. In this paper, we introduce a novel fake user based poisoning attack named PoisonFRS to promote the attacker-chosen targeted item in federated recommender systems without requiring knowledge about user-item rating data, user attributes, or the aggregation rule used by the server. Extensive experiments on multiple real-world datasets demonstrate that PoisonFRS can effectively promote the attacker-chosen targeted item to a large portion of genuine users and outperform current benchmarks that rely on additional information about the system. We further observe that the model updates from both genuine and fake users are indistinguishable within the latent space.|联合推荐是联合学习中一个突出的用例，但它仍然容易受到从用户到服务器端漏洞的各种攻击。中毒攻击在用户端攻击中尤其显著，因为参与者上传恶意模型更新以欺骗全局模型，通常意图促进或降级特定目标项目。本研究探讨在联邦推荐系统中执行推广攻击的策略。当前对联邦推荐系统的中毒攻击通常依赖于附加信息，比如真正用户的本地培训数据或项目流行度。然而，这样的信息对于潜在的攻击者来说是具有挑战性的。因此，需要开发一种除了从服务器获得的项嵌入之外不需要额外信息的攻击。本文提出了一种新的基于虚假用户的中毒攻击 PoisonFRS，它在不需要用户评分数据、用户属性和服务器聚合规则的情况下，在联邦推荐系统中推广攻击者选择的目标项。在多个真实世界数据集上的大量实验表明，PoisonFRS 能够有效地将攻击者选择的目标项目推广到大部分真实用户，并且比依赖于系统附加信息的当前基准测试表现更好。我们进一步观察到，来自真实用户和虚假用户的模型更新在潜在空间内是不可区分的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Poisoning+Federated+Recommender+Systems+with+Fake+Users)|0|
|[Could Small Language Models Serve as Recommenders? Towards Data-centric Cold-start Recommendation](https://doi.org/10.1145/3589334.3645494)|Xuansheng Wu, Huachi Zhou, Yucheng Shi, Wenlin Yao, Xiao Huang, Ninghao Liu||Recommendation systems help users find information that matches their interests based on their historical behaviors. However, generating personalized recommendations becomes challenging in the absence of historical user-item interactions, a practical problem for startups known as the system cold-start recommendation. Current research tackles user or item cold-start scenarios but lacks solutions for system cold-start. To tackle the problem, we initially propose PromptRec, a simple but effective approach based on in-context learning of language models, where we transform the recommendation task into the sentiment analysis task on natural language containing user and item profiles. However, this naive strategy heavily relied on the strong in-context learning ability emerged from large language models, which could suffer from significant latency for online recommendations. To fill this gap, we present a theoretical framework to formalize the connection between in-context recommendation and language modeling. Based on it, we propose to enhance small language models with a data-centric pipeline, which consists of: (1) constructing a refined corpus for model pre-training; (2) constructing a decomposed prompt template via prompt pre-training. They correspond to the development of training data and inference data, respectively. To evaluate our proposed method, we introduce a cold-start recommendation benchmark, and the results demonstrate that the enhanced small language models can achieve comparable cold-start recommendation performance to that of large models with only around 17 time. To the best of our knowledge, this is the first study to tackle the system cold-start recommendation problem. We believe our findings will provide valuable insights for future works. The benchmark and implementations are available at https://github.com/JacksonWuxs/PromptRec.|推荐系统帮助用户根据他们的历史行为找到符合他们兴趣的信息。然而，在缺乏历史用户项目交互的情况下，生成个性化推荐变得具有挑战性，这是创业公司面临的一个实际问题，称为系统冷启动推荐。目前的研究涉及用户或项目冷启动场景，但缺乏系统冷启动的解决方案。为了解决这一问题，我们首先提出了 PromptRec，这是一种基于语言模型上下文学习的简单而有效的方法，我们将推荐任务转化为包含用户和项目概要的自然语言情感分析任务。然而，这种幼稚的策略在很大程度上依赖于大型语言模型中出现的强大的上下文学习能力，这种能力可能会受到在线推荐显著延迟的影响。为了填补这一空白，我们提出了一个理论框架，以形式化之间的联系上下文推荐和语言建模。在此基础上，我们提出了一种以数据为中心的流水线来增强小语言模型，包括: (1)构造一个精化的模型预训练语料库; (2)通过提示预训练构造一个分解的提示模板。它们分别对应于训练数据和推理数据的发展。为了对我们提出的方法进行评估，我们引入了一个冷启动推荐基准，结果表明，增强的小语言模型只需要17次左右的时间就可以实现与大型模型相当的冷启动推荐性能。据我们所知，这是第一个解决系统冷启动推荐问题的研究。我们相信我们的发现将为今后的工作提供有价值的见解。基准及实施 https://github.com/jacksonwuxs/promptrec 已备妥。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Could+Small+Language+Models+Serve+as+Recommenders?+Towards+Data-centric+Cold-start+Recommendation)|0|
|[Recommender Transformers with Behavior Pathways](https://doi.org/10.1145/3589334.3645528)|Zhiyu Yao, Xinyang Chen, Sinan Wang, Qinyan Dai, Yumeng Li, Tanchao Zhu, Mingsheng Long||Sequential recommendation requires the recommender to capture the evolving behavior characteristics from logged user behavior data for accurate recommendations. However, user behavior sequences are viewed as a script with multiple ongoing threads intertwined. We find that only a small set of pivotal behaviors can be evolved into the user's future action. As a result, the future behavior of the user is hard to predict. We conclude this characteristic for sequential behaviors of each user as the Behavior Pathway. Different users have their unique behavior pathways. Among existing sequential models, transformers have shown great capacity in capturing global-dependent characteristics. However, these models mainly provide a dense distribution over all previous behaviors using the self-attention mechanism, making the final predictions overwhelmed by the trivial behaviors not adjusted to each user. In this paper, we build the Recommender Transformer (RETR) with a novel Pathway Attention mechanism. RETR can dynamically plan the behavior pathway specified for each user, and sparingly activate the network through this behavior pathway to effectively capture evolving patterns useful for recommendation. The key design is a learned binary route to prevent the behavior pathway from being overwhelmed by trivial behaviors. We empirically verify the effectiveness of RETR on seven real-world datasets and RETR yields state-of-the-art performance.|连续推荐需要推荐者从日志用户行为数据中捕获不断变化的行为特征，以获得准确的推荐。但是，用户行为序列被看作是一个多个正在进行的线程交织在一起的脚本。我们发现只有一小部分关键行为可以演化为用户未来的行为。因此，用户未来的行为很难预测。我们将每个用户的连续行为归结为行为路径。不同的用户有他们独特的行为路径。在现有的顺序模型中，变压器在捕捉全局相关特性方面表现出了很大的能力。然而，这些模型主要利用自我注意机制，在所有以前的行为上提供了一个密集的分布，使得最终的预测被不适应每个用户的琐碎行为所淹没。在本文中，我们建立了推荐变压器(RETR)与一个新的路径注意机制。RETR 可以动态规划为每个用户指定的行为路径，并通过该行为路径激活网络，以有效地捕获推荐所需的演化模式。关键的设计是一个学习的二进制路径，以防止行为路径被琐碎的行为所淹没。我们通过实验验证了 RETR 在七个实际数据集上的有效性，并且 RETR 产生了最先进的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recommender+Transformers+with+Behavior+Pathways)|0|
|[Ensuring User-side Fairness in Dynamic Recommender Systems](https://doi.org/10.1145/3589334.3645536)|Hyunsik Yoo, Zhichen Zeng, Jian Kang, Ruizhong Qiu, David Zhou, Zhining Liu, Fei Wang, Charlie Xu, Eunice Chan, Hanghang Tong||User-side group fairness is crucial for modern recommender systems, aiming to alleviate performance disparities among user groups defined by sensitive attributes like gender, race, or age. In the ever-evolving landscape of user-item interactions, continual adaptation to newly collected data is crucial for recommender systems to stay aligned with the latest user preferences. However, we observe that such continual adaptation often exacerbates performance disparities. This necessitates a thorough investigation into user-side fairness in dynamic recommender systems, an area that has been unexplored in the literature. This problem is challenging due to distribution shifts, frequent model updates, and non-differentiability of ranking metrics. To our knowledge, this paper presents the first principled study on ensuring user-side fairness in dynamic recommender systems. We start with theoretical analyses on fine-tuning v.s. retraining, showing that the best practice is incremental fine-tuning with restart. Guided by our theoretical analyses, we propose FAir Dynamic rEcommender (FADE), an end-to-end fine-tuning framework to dynamically ensure user-side fairness over time. To overcome the non-differentiability of recommendation metrics in the fairness loss, we further introduce Differentiable Hit (DH) as an improvement over the recent NeuralNDCG method, not only alleviating its gradient vanishing issue but also achieving higher efficiency. Besides that, we also address the instability issue of the fairness loss by leveraging the competing nature between the recommendation loss and the fairness loss. Through extensive experiments on real-world datasets, we demonstrate that FADE effectively and efficiently reduces performance disparities with little sacrifice in the overall recommendation performance.|用户端群组公平性对于现代推荐系统至关重要，旨在缓解由性别、种族或年龄等敏感属性定义的用户群之间的性能差异。在不断变化的用户项交互环境中，持续适应新收集的数据对于推荐系统保持与最新用户偏好一致至关重要。然而，我们观察到这种持续的适应常常加剧性能差异。这就需要对动态推荐系统中的用户端公平性进行彻底的研究，这是文献中未曾探索过的领域。这个问题是具有挑战性的，因为分布转移，频繁的模型更新，和不可区分的排名度量。据我们所知，本文提出了第一个原则性的研究，以确保用户端公平性的动态推荐系统。我们从微调和再训练的理论分析开始，表明最佳实践是重新启动后的渐进微调。在理论分析的指导下，我们提出了 FAir Dynamic reComdender (FADE) ，这是一个端到端的微调框架，可以随着时间的推移动态地保证用户端的公平性。为了克服推荐度量在公平性损失中的不可微性，我们进一步引入了可微命中(DH)算法，作为对最近的神经网络 NDCG 方法的改进，不仅减轻了它的梯度消失问题，而且实现了更高的效率。此外，我们还利用推荐损失和公平损失之间的竞争性质来解决公平损失的不稳定性问题。通过对现实世界数据集的大量实验，我们证明了 FADE 能够有效地降低性能差异，并且在总体推荐性能方面做出了很小的牺牲。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ensuring+User-side+Fairness+in+Dynamic+Recommender+Systems)|0|
|[Mirror Gradient: Towards Robust Multimodal Recommender Systems via Exploring Flat Local Minima](https://doi.org/10.1145/3589334.3645553)|Shanshan Zhong, Zhongzhan Huang, Daifeng Li, Wushao Wen, Jinghui Qin, Liang Lin||Multimodal recommender systems utilize various types of information to model user preferences and item features, helping users discover items aligned with their interests. The integration of multimodal information mitigates the inherent challenges in recommender systems, e.g., the data sparsity problem and cold-start issues. However, it simultaneously magnifies certain risks from multimodal information inputs, such as information adjustment risk and inherent noise risk. These risks pose crucial challenges to the robustness of recommendation models. In this paper, we analyze multimodal recommender systems from the novel perspective of flat local minima and propose a concise yet effective gradient strategy called Mirror Gradient (MG). This strategy can implicitly enhance the model's robustness during the optimization process, mitigating instability risks arising from multimodal information inputs. We also provide strong theoretical evidence and conduct extensive empirical experiments to show the superiority of MG across various multimodal recommendation models and benchmarks. Furthermore, we find that the proposed MG can complement existing robust training methods and be easily extended to diverse advanced recommendation models, making it a promising new and fundamental paradigm for training multimodal recommender systems. The code is released at https://github.com/Qrange-group/Mirror-Gradient.|多模式推荐系统利用各种类型的信息来建模用户偏好和项目特征，帮助用户发现符合他们兴趣的项目。多模态信息的集成缓解了推荐系统固有的挑战，如数据稀疏问题和冷启动问题。然而，它同时放大了来自多模态信息输入的某些风险，如信息调整风险和固有噪声风险。这些风险对推荐模型的稳健性提出了严峻的挑战。本文从平坦局部极小的角度分析了多模态推荐系统，提出了一种简洁有效的梯度策略——镜像梯度(MG)。该策略可以在优化过程中增强模型的鲁棒性，减少多模态信息输入引起的不稳定风险。我们还提供了强有力的理论证据，并进行了广泛的实证实验，以显示 MG 在各种多模式推荐模型和基准测试中的优越性。此外，我们发现所提出的 MG 可以补充现有的鲁棒训练方法，并且可以很容易地扩展到不同的高级推荐模型，使它成为训练多模式推荐系统的一个有前途的新的和基本的范例。密码在 https://github.com/qrange-group/mirror-gradient 发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mirror+Gradient:+Towards+Robust+Multimodal+Recommender+Systems+via+Exploring+Flat+Local+Minima)|0|
|[Adaptive Neural Ranking Framework: Toward Maximized Business Goal for Cascade Ranking Systems](https://doi.org/10.1145/3589334.3645605)|Yunli Wang, Zhiqiang Wang, Jian Yang, Shiyang Wen, Dongying Kong, Han Li, Kun Gai||Cascade ranking is widely used for large-scale top-k selection problems in online advertising and recommendation systems, and learning-to-rank is an important way to optimize the models in cascade ranking. Previous works on learning-to-rank usually focus on letting the model learn the complete order or top-k order, and adopt the corresponding rank metrics (e.g. OPA and NDCG@k) as optimization targets. However, these targets can not adapt to various cascade ranking scenarios with varying data complexities and model capabilities; and the existing metric-driven methods such as the Lambda framework can only optimize a rough upper bound of limited metrics, potentially resulting in sub-optimal and performance misalignment. To address these issues, we propose a novel perspective on optimizing cascade ranking systems by highlighting the adaptability of optimization targets to data complexities and model capabilities. Concretely, we employ multi-task learning to adaptively combine the optimization of relaxed and full targets, which refers to metrics Recall@m@k and OPA respectively. We also introduce permutation matrix to represent the rank metrics and employ differentiable sorting techniques to relax hard permutation matrix with controllable approximate error bound. This enables us to optimize both the relaxed and full targets directly and more appropriately. We named this method as Adaptive Neural Ranking Framework (abbreviated as ARF). Furthermore, we give a specific practice under ARF. We use the NeuralSort to obtain the relaxed permutation matrix and draw on the variant of the uncertainty weight method in multi-task learning to optimize the proposed losses jointly. Experiments on a total of 4 public and industrial benchmarks show the effectiveness and generalization of our method, and online experiment shows that our method has significant application value.|在在线广告和推荐系统中，级联排名被广泛应用于大规模的 Top-k 选择问题，而学习排名是级联排名模型优化的重要途径。先前关于学习排序的工作通常集中在让模型学习完整的顺序或 top-k 顺序，并采用相应的排序指标(如 OPA 和 NDCG@k)作为优化目标。然而，这些目标不能适应各种级联排序场景与不同的数据复杂性和模型能力; 和现有的度量驱动的方法，如 Lambda 框架只能优化一个粗略的上限有限的度量，潜在地导致次优化和性能不一致。为了解决这些问题，我们提出了一个优化级联排序系统的新视角，强调优化目标对数据复杂性和模型能力的适应性。具体而言，我们采用多任务学习的方法，自适应地将松弛目标和完全目标的优化结合起来，分别使用 Recall@m@k 和 OPA 度量。我们还引入置换矩阵来表示排名度量，并使用可微分排序技术来放松具有可控近似误差界限的硬置换矩阵。这使我们能够直接和更适当地优化宽松和完整的目标。我们将这种方法命名为自适应神经排序框架(简称 ARF)。此外，我们还给出了 ARF 下的具体实践。我们使用神经排序来获得松弛置换矩阵，并利用多任务学习中不确定性权重方法的变体来共同优化提出的损失。在4个公共基准和行业基准上的实验表明了该方法的有效性和推广性，在线实验表明该方法具有较高的应用价值。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adaptive+Neural+Ranking+Framework:+Toward+Maximized+Business+Goal+for+Cascade+Ranking+Systems)|0|
|[General Debiasing for Graph-based Collaborative Filtering via Adversarial Graph Dropout](https://doi.org/10.1145/3589334.3645667)|An Zhang, Wenchang Ma, Pengbo Wei, Leheng Sheng, Xiang Wang||Graph neural networks (GNNs) have shown impressive performance in recommender systems, particularly in collaborative filtering (CF). The key lies in aggregating neighborhood information on a user-item interaction graph to enhance user/item representations. However, we have discovered that this aggregation mechanism comes with a drawback, which amplifies biases present in the interaction graph. For instance, a user's interactions with items can be driven by both unbiased true interest and various biased factors like item popularity or exposure. However, the current aggregation approach combines all information, both biased and unbiased, leading to biased representation learning. Consequently, graph-based recommenders can learn distorted views of users/items, hindering the modeling of their true preferences and generalizations. To address this issue, we introduce a novel framework called Adversarial Graph Dropout (AdvDrop). It differentiates between unbiased and biased interactions, enabling unbiased representation learning. For each user/item, AdvDrop employs adversarial learning to split the neighborhood into two views: one with bias-mitigated interactions and the other with bias-aware interactions. After view-specific aggregation, AdvDrop ensures that the bias-mitigated and bias-aware representations remain invariant, shielding them from the influence of bias. We validate AdvDrop's effectiveness on five public datasets that cover both general and specific biases, demonstrating significant improvements. Furthermore, our method exhibits meaningful separation of subgraphs and achieves unbiased representations for graph-based CF models, as revealed by in-depth analysis. Our code is publicly available at https://github.com/Arthurma71/AdvDrop.|图形神经网络(GNN)在推荐系统中表现出了令人印象深刻的性能，特别是在协同过滤(CF)中。关键在于聚合用户-项目交互图上的邻域信息，以增强用户/项目的表示。然而，我们发现这种聚合机制有一个缺点，那就是放大了交互图中存在的偏差。例如，用户与项目的交互可以由无偏见的真实兴趣和各种偏见的因素驱动，如项目受欢迎程度或曝光率。然而，目前的聚合方法结合了所有的信息，包括有偏的和无偏的，导致有偏的表征学习。因此，基于图表的推荐程序可以了解用户/项目的扭曲视图，从而阻碍对其真实偏好和概括的建模。为了解决这个问题，我们引入了一个新的框架，称为对抗图丢失(AdvDrop)。它区分无偏见和有偏见的互动，使无偏见的表征学习。对于每个用户/项目，AdvDrop 使用对抗性学习将邻居分成两个视图: 一个视图具有减轻偏见的交互作用，另一个视图具有意识到偏见的交互作用。在视图特定聚合之后，AdvDrop 确保偏差缓解和偏差感知表示保持不变，从而保护它们免受偏差的影响。我们验证了 AdvDrop 在五个公共数据集上的有效性，这些数据集涵盖了一般和特定的偏差，显示了显著的改进。此外，我们的方法表现出有意义的子图分离，并实现了基于图的 CF 模型的无偏表示，如深入分析所揭示的。我们的代码可以在 https://github.com/arthurma71/advdrop 上公开获取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=General+Debiasing+for+Graph-based+Collaborative+Filtering+via+Adversarial+Graph+Dropout)|0|
|[Online Sequential Decision-Making with Unknown Delays](https://doi.org/10.1145/3589334.3645388)|Ping Wu, Heyan Huang, Zhengyang Liu||In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay. Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback. Our proposed algorithms are versatile and applicable to universal norms. Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points. For each type of algorithm, we provide corresponding regret bounds under cases of general convexity and relative strong convexity, respectively. We also demonstrate the efficiency of each algorithm under different norms through concrete examples. Furthermore, our theoretical results are consistent with the current best bounds when degenerated to standard settings.|在在线顺序决策领域，我们利用在线凸优化(OCO)的框架来解决延迟问题，其中决策的反馈可以以未知的延迟到达。与以往仅限于欧氏范数和梯度信息的研究不同，我们提出了三类基于近似解的延迟算法来处理不同类型的接收反馈。我们提出的算法是通用的，并适用于普遍规范。具体来说，我们介绍了一系列的延迟正则化领先算法用于损失函数的全信息反馈，一系列的延迟镜像下降算法用于损失函数的梯度信息反馈，以及一系列的简化延迟镜像下降算法用于相应决策点的损失函数梯度的值信息反馈。对于每种算法，我们分别在一般凸性和相对强凸性情况下给出了相应的后悔界。并通过具体实例说明了每种算法在不同规范下的有效性。此外，我们的理论结果与当前退化到标准设置的最佳界限一致。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Sequential+Decision-Making+with+Unknown+Delays)|0|
|[Triage of Messages and Conversations in a Large-Scale Child Victimization Corpus](https://doi.org/10.1145/3589334.3648142)|Prasanna Lakkur Subramanyam, Mohit Iyyer, Brian Neil Levine||Children are among the most vulnerable online populations. Reports of child sexual exploitation on social media and apps have grown annually at an alarming rate and are overwhelming investigators. Even a single case can require examining millions of messages involving hundreds of victims. Triage and prioritization based on victims' experiences is an unfortunate necessity. Using a chat dataset of more than 3 million messages between victims and perpetrators, we evaluate and contribute tools for analyzing the experiences of victims of sexual exploitation. We develop both supervised and unsupervised methods to classify messages into categories of interest to law enforcement, such as age requests, persuasion, and sexual messages. We also introduce a conversation clustering technique to illuminate differences among victims' experiences based on their chat history. Through a qualitative analysis, we demonstrate that the learned clusters are coherent and represent distinct conversation patterns. For example, we can distinguish groups of users who never comply with sexual requests, comply after a few conversations, or comply immediately after being targeted. We expect this approach and associated visualizations will aid law enforcement, industry moderators, and sociologists who need to analyze massive corpora in this domain. Finally, we validate prior models derived from conversations involving adults pretending to be minors and provide statistics that could help undercover adults more accurately portray minor victims.|儿童是最容易受到网络攻击的群体之一。社交媒体和应用程序上关于儿童性剥削的报告每年都在以惊人的速度增长，调查人员已经无所适从。即使是一个单独的案件也可能需要检查涉及数百名受害者的数百万条信息。根据受害者的经历进行分类和优先排序是一种不幸的必要性。我们利用受害者和犯罪者之间300多万条信息的聊天数据集，评估和贡献工具，分析性剥削受害者的经历。我们开发了有监督和无监督的方法，将信息分类为执法部门感兴趣的类别，如年龄要求、说服和性信息。我们还引入了一种会话聚类技术，根据受害者的聊天历史来说明他们的不同经历。通过定性分析，我们证明了学习集群是连贯的，代表了不同的会话模式。例如，我们可以区分从不服从性要求、在几次谈话后服从或在成为目标后立即服从的用户群。我们期望这种方法和相关的可视化将有助于执法，行业协调员和社会学家谁需要分析这个领域的大量语料库。最后，我们验证了先前从成年人假装未成年人的谈话中得出的模型，并提供统计数据，可以帮助卧底成年人更准确地描绘未成年受害者。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Triage+of+Messages+and+Conversations+in+a+Large-Scale+Child+Victimization+Corpus)|0|
|[Collaborative-Enhanced Prediction of Spending on Newly Downloaded Mobile Games under Consumption Uncertainty](https://doi.org/10.1145/3589335.3648297)|Peijie Sun, Yifan Wang, Min Zhang, Chuhan Wu, Yan Fang, Hong Zhu, Yuan Fang, Meng Wang||With the surge in mobile gaming, accurately predicting user spending on newly downloaded games has become paramount for maximizing revenue. However, the inherently unpredictable nature of user behavior poses significant challenges in this endeavor. To address this, we propose a robust model training and evaluation framework aimed at standardizing spending data to mitigate label variance and extremes, ensuring stability in the modeling process. Within this framework, we introduce a collaborative-enhanced model designed to predict user game spending without relying on user IDs, thus ensuring user privacy and enabling seamless online training. Our model adopts a unique approach by separately representing user preferences and game features before merging them as input to the spending prediction module. Through rigorous experimentation, our approach demonstrates notable improvements over production models, achieving a remarkable 17.11% enhancement on offline data and an impressive 50.65% boost in an online A/B test. In summary, our contributions underscore the importance of stable model training frameworks and the efficacy of collaborative-enhanced models in predicting user spending behavior in mobile gaming.|随着手机游戏的迅猛发展，准确预测用户在新下载游戏上的支出对于实现收入最大化至关重要。然而，用户行为固有的不可预测性在这方面提出了重大的挑战。为了解决这个问题，我们提出了一个健壮的模型训练和评估框架，旨在标准化支出数据，以减少标签差异和极端情况，确保建模过程的稳定性。在这个框架中，我们引入了一个协作增强的模型，该模型旨在预测用户的游戏支出，而不依赖于用户 ID，从而确保用户的隐私并实现无缝在线培训。我们的模型采用了一种独特的方法，分别表示用户偏好和游戏特征，然后将它们合并为消费预测模块的输入。通过严格的实验，我们的方法显示了生产模型的显著改进，在离线数据上实现了17.11% 的显著增强，在线 A/B 测试中实现了50.65% 的显著增强。总之，我们的贡献强调了稳定模型训练框架的重要性，以及协作增强模型在预测手机游戏用户消费行为方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaborative-Enhanced+Prediction+of+Spending+on+Newly+Downloaded+Mobile+Games+under+Consumption+Uncertainty)|0|
|[HiFI: Hierarchical Fairness-aware Integrated Ranking with Constrained Reinforcement Learning](https://doi.org/10.1145/3589335.3648317)|Yifan Liu, Wei Xia, Weiwen Liu, Menghui Zhu, Weinan Zhang, Ruiming Tang, Yong Yu||Integrated ranking is a critical component in industrial recommendation platforms. It combines candidate lists from different upstream channels or sources and ranks them into an integrated list, which will be exposed to users. During this process, to take responsibility for channel providers, the integrated ranking system needs to consider the exposure fairness among channels, which directly affects the opportunities of different channels being displayed to users. Besides, personalization also requires the integrated ranking system to consider the user's diverse preference on different channels besides items. Existing methods are hard to address both problems effectively. In this paper, we propose a <u>Hi</u>erarchical <u>F</u>airness-aware <u>I</u>ntegrated ranking (HiFI) framework. It contains a channel recommender and an item recommender, and the fairness constraint is on channels with constrained RL. We also design a gated attention layer (GAL) to effectively capture users' multi-faceted preferences. We compare HiFI with various baselines on public and industrial datasets, and HiFI achieves the state-of-the-art performance on both utility and fairness metrics. We also conduct an online A/B test to further validate the effectiveness of HiFI.|综合排名是行业推荐平台的重要组成部分。它综合了来自不同上游渠道或来源的候选人名单，并将其排列成一个综合名单，向用户公布。在此过程中，为了对渠道提供商负责，综合排名系统需要考虑渠道之间的曝光公平性，这直接影响到不同渠道向用户显示的机会。此外，个性化还要求综合排名系统考虑用户在不同渠道上的不同偏好。现有的方法很难有效地解决这两个问题。本文提出了一个层次化的综合排名(HiFI)框架，该框架包括: “ u”、“ Hi”、“ F”、“空气感知”、“ I”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“ HiFI”、“。它包含一个通道推荐器和一个项目推荐器，公平性约束在 RL 受限的通道上。我们还设计了一个门限注意层(GAL)来有效地捕获用户的多方面的偏好。我们将 HiFI 与公共和工业数据集上的各种基线进行了比较，HiFI 在效用和公平性指标上都取得了最先进的性能。我们还进行了在线 A/B 测试，以进一步验证 HiFI 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HiFI:+Hierarchical+Fairness-aware+Integrated+Ranking+with+Constrained+Reinforcement+Learning)|0|
|[On Practical Diversified Recommendation with Controllable Category Diversity Framework](https://doi.org/10.1145/3589335.3648323)|Tao Zhang, Luwei Yang, Zhibo Xiao, Wen Jiang, Wei Ning||Recommender systems have made significant strides in various industries, primarily driven by extensive efforts to enhance recommendation accuracy. However, this pursuit of accuracy has inadvertently given rise to echo chamber/filter bubble effects. Especially in industry, it could impair user's experiences and prevent user from accessing a wider range of items. One of the solutions is to take diversity into account. However, most of existing works focus on user's explicit preferences, while rarely exploring user's non-interaction preferences. These neglected non-interaction preferences are especially important for broadening user's interests in alleviating echo chamber/filter bubble effects.Therefore, in this paper, we first define diversity as two distinct definitions, i.e., user-explicit diversity (U-diversity) and user-item non-interaction diversity (N-diversity) based on user historical behaviors. Then, we propose a succinct and effective method, named as Controllable Category Diversity Framework (CCDF) to achieve both high U-diversity and N-diversity simultaneously.Specifically, CCDF consists of two stages, User-Category Matching and Constrained Item Matching. The User-Category Matching utilizes the DeepU2C model and a combined loss to capture user's preferences in categories, and then selects the top-K categories with a controllable parameter K.These top-K categories will be used as trigger information in Constrained Item Matching. Offline experimental results show that our proposed DeepU2C outperforms state-of-the-art diversity-oriented methods, especially on N-diversity task. The whole framework is validated in a real-world production environment by conducting online A/B testing.|推荐系统在各个行业都取得了长足的进步，主要是受到提高推荐准确性的广泛努力的推动。然而，这种对准确性的追求无意中导致了回声室/过滤器气泡效应。特别是在工业领域，它会损害用户的体验，阻止用户访问更广泛的项目。解决办法之一是考虑到多样性。然而，现有的大多数作品只关注用户的显性偏好，很少探讨用户的非交互偏好。这些被忽视的非交互偏好对于扩大用户的兴趣以减轻回声室/过滤器气泡效应特别重要。因此，本文首先将多样性定义为基于用户历史行为的用户显性多样性(U 多样性)和用户项非交互多样性(N 多样性)。然后，我们提出了一种简洁有效的方法，称为可控分类分集框架(CCDF) ，同时实现高 U 分集和 N 分集。具体来说，CCDF 包括两个阶段，用户类别匹配和约束条目匹配。用户类别匹配利用 DeepU2C 模型和综合损失来获取用户在类别中的偏好，然后选择带有可控参数 K 的 top-K 类别。这些 top-K 类别将作为受限项目匹配的触发信息。离线实验结果表明，本文提出的 DeepU2C 方法优于目前最先进的面向多样性的方法，尤其是在 N 分集任务上。通过在线 A/B 测试，在现实生产环境中验证了整个框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Practical+Diversified+Recommendation+with+Controllable+Category+Diversity+Framework)|0|
|[Optimization-Based Budget Pacing in eBay Sponsored Search](https://doi.org/10.1145/3589335.3648331)|Qinyi Chen, Phuong Ha Nguyen, Djordje Gligorijevic||In online platforms like eBay, sponsored search advertising has become instrumental for businesses aiming for enhanced visibility. However, in automated ad auctions, the sellers (ad campaigns) run the risk of exhausting their budgets prematurely in the absence of proper pacing strategies. In response to this, online platforms have been prompted to employ budget pacing strategies to maintain consistent spending patterns for their sellers. While numerous budget pacing strategies have been introduced, they predominantly stem from either empirical or theoretical perspectives, often functioning in isolation. This paper aims to bridge this gap by investigating the performance of a theoretically inspired optimization-based bid shading method, AdaptivePacing, within eBay's sponsored search environment and proposing variants of the algorithm tailored to real-world environments. Our findings highlight the benefits of applying theoretical pacing approaches in practical contexts. Specifically, the optimization-based AdaptivePacing method offers the platform flexible control over campaign spending patterns, accounts for business constraints, and suggests tailored strategies for distinct advertisers. Furthermore, when evaluating AdaptivePacing alongside established empirical methods, we demonstrate its practical effectiveness and pinpoint areas for further refinement.|在 eBay 这样的在线平台上，赞助商搜索广告已经成为企业提高知名度的重要手段。然而，在自动化广告拍卖中，如果没有适当的节奏策略，卖家(广告活动)就有过早耗尽预算的风险。为了应对这种情况，在线平台采用了预算调整策略，以保持卖家的一致消费模式。虽然已经引入了许多预算调整策略，但它们主要来自经验或理论视角，往往是孤立运作的。本文旨在通过调查 eBay 赞助商搜索环境中基于理论优化的投标着色方法 AdaptivePacing 的性能，并提出适合于现实环境的算法变体，来弥补这一差距。我们的研究结果强调了在实际环境中应用理论起搏方法的好处。具体来说，基于优化的 AdaptivePacing 方法为平台提供了对广告支出模式的灵活控制，解释了业务约束，并为不同的广告商提供了量身定制的策略。此外，当评估适应性起搏与已建立的经验方法，我们证明了其实际有效性和精确的领域进一步完善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimization-Based+Budget+Pacing+in+eBay+Sponsored+Search)|0|
|[AutoML for Large Capacity Modeling of Meta's Ranking Systems](https://doi.org/10.1145/3589335.3648336)|Hang Yin, KuangHung Liu, Mengying Sun, Yuxin Chen, Buyun Zhang, Jiang Liu, Vivek Sehgal, Rudresh Rajnikant Panchal, Eugen Hotaj, Xi Liu, Daifeng Guo, Jamey Zhang, Zhou Wang, Shali Jiang, Huayu Li, Zhengxing Chen, WenYen Chen, Jiyan Yang, Wei Wen||Web-scale ranking systems at Meta serving billions of users is complex. Improving ranking models is essential but engineering heavy. Automated Machine Learning (AutoML) can release engineers from labor intensive work of tuning ranking models; however, it is unknown if AutoML is efficient enough to meet tight production timeline in real-world and, at the same time, bring additional improvements to the strong baselines. Moreover, to achieve higher ranking performance, there is an ever-increasing demand to scale up ranking models to even larger capacity, which imposes more challenges on the efficiency. The large scale of models and tight production schedule requires AutoML to outperform human baselines by only using a small number of model evaluation trials (around 100). We presents a sampling-based AutoML method, focusing on neural architecture search and hyperparameter optimization, addressing these challenges in Meta-scale production when building large capacity models. Our approach efficiently handles large-scale data demands. It leverages a lightweight predictor-based searcher and reinforcement learning to explore vast search spaces, significantly reducing the number of model evaluations. Through experiments in large capacity modeling for CTR and CVR applications, we show that our method achieves outstanding Return on Investment (ROI) versus human tuned baselines, with up to 0.09% Normalized Entropy (NE) loss reduction or $25\%$ Query per Second (QPS) increase by only sampling one hundred models on average from a curated search space. The proposed AutoML method has already made real-world impact where a discovered Instagram CTR model with up to -0.36% NE gain (over existing production baseline) was selected for large-scale online A/B test and show statistically significant gain. These production results proved AutoML efficacy and accelerated its adoption in ranking systems at Meta.|Meta 为数十亿用户服务的网络级别排名系统非常复杂。改进排名模型至关重要，但工程量很大。自动机器学习(Automated Machine Learning，AutoML)可以让工程师们摆脱劳动密集型的排序模型调整工作; 然而，目前还不清楚 AutoML 是否足够有效，能够满足现实世界中紧张的生产时间表，同时还能给强大的基线带来额外的改进。此外，为了获得更高的排序性能，将排序模型扩展到更大容量的需求也在不断增加，这对效率提出了更多的挑战。大规模的模型和紧凑的生产进度要求 AutoML 仅仅使用少量的模型评估试验(大约100次)就能超越人类基线。提出了一种基于抽样的 AutoML 方法，重点研究了神经网络结构搜索和超参数优化，解决了元规模生产在建立大容量模型时遇到的问题。我们的方法有效地处理大规模的数据需求。它利用一个轻量级的基于预测器的搜索器和强化学习来探索广阔的搜索空间，大大减少了模型评估的数量。通过在 CTR 和 CVR 应用中的大容量建模实验，我们表明我们的方法实现了出色的投资回报率(ROI)相对于人工调整的基线，高达0.09% 的归一化熵(NE)损失减少或 $25% $Query per Second (QPS)增加通过平均抽样一百个模型从一个精心策划的搜索空间。提出的 AutoML 方法已经产生了现实世界的影响，其中发现的 Instagram CTR 模型具有高达 -0.36% 的 NE 增益(超过现有的生产基线) ，被选择用于大规模在线 A/B 测试，并显示出统计学显着的增益。这些生产结果证明了 AutoML 的有效性，并加速了其在 Meta 排名系统中的应用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoML+for+Large+Capacity+Modeling+of+Meta's+Ranking+Systems)|0|
|[OneSparse: A Unified System for Multi-index Vector Search](https://doi.org/10.1145/3589335.3648338)|Yaoqi Chen, Ruicheng Zheng, Qi Chen, Shuotao Xu, Qianxi Zhang, Xue Wu, Weihao Han, Hua Yuan, Mingqin Li, Yujing Wang, Jason Li, Fan Yang, Hao Sun, Weiwei Deng, Feng Sun, Qi Zhang, Mao Yang||Multi-index vector search has become the cornerstone for many applications, such as recommendation systems. Efficient search in such a multi-modal hybrid vector space is challenging since no single index design performs well for all kinds of vector data. Existing approaches to processing multi-index hybrid queries either suffer from algorithmic limitations or processing inefficiency. In this paper, we propose OneSparse, a unified multi-vector index query system that incorporates multiple posting-based vector indices, which enables highly efficient retrieval of multi-modal data-sets. OneSparse introduces a novel multi-index query engine design of inter-index intersection push-down. It also optimizes the vector posting format to expedite multi-index queries. Our experiments show OneSparse achieves more than 6x search performance improvement while maintaining comparable accuracy. OneSparse has already been integrated into Microsoft online web search and advertising systems with 5x+ latency gain for Bing web search and 2.0% Revenue Per Mille (RPM) gain for Bing sponsored search.|多索引向量搜索已经成为许多应用程序(如推荐系统)的基石。在这样一个多模态混合向量空间中的有效搜索是具有挑战性的，因为没有单一索引设计能够很好地处理所有类型的向量数据。现有的处理多索引混合查询的方法要么受到算法限制，要么处理效率低下。本文提出了一个统一的多向量索引查询系统 OneSparse，该系统集成了多个基于发布的向量索引，能够高效地检索多模态数据集。OneSparse 提出了一种新颖的索引交集下推的多索引查询引擎设计。它还优化了向量发布格式，以加快多索引查询。我们的实验表明，OneSparse 在保持可比准确性的同时，实现了超过6倍的搜索性能改进。OneSparse 已经被整合到微软在线网络搜索和广告系统中，Bing 的网络搜索延迟增加了5倍以上，Bing 赞助的搜索每公里收入(RPM)增加了2.0% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OneSparse:+A+Unified+System+for+Multi-index+Vector+Search)|0|
|[LLaCE: Locally Linear Contrastive Embedding](https://doi.org/10.1145/3589335.3651534)|Ruichen Liu, Yang Liu, Jiming Liu||Node embedding is one of the most widely adopted techniques in numerous graph analysis tasks, such as node classification. Methods for node embedding can be broadly classified into three categories: proximity matrix factorization approaches, sampling methods, and deep learning strategies. Among the deep learning strategies, graph contrastive learning has attracted significant interest. Yet, it has been observed that existing graph contrastive learning approaches do not adequately preserve the local topological structure of the original graphs, particularly when neighboring nodes belong to disparate categories. To address this challenge, this paper introduces a novel node embedding approach named Locally Linear Contrastive Embedding (LLaCE). LLaCE is designed to maintain the intrinsic geometric structure of graph data by utilizing locally linear formulation, thereby ensuring that the local topological characteristics are accurately reflected in the embedding space. Experimental results on one synthetic dataset and five real-world datasets validate the effectiveness of our proposed method.|节点嵌入是许多图分析任务(如节点分类)中广泛采用的技术之一。节点嵌入的方法大致可分为三类: 接近矩阵分解方法、抽样方法和深度学习策略。在深度学习策略中，图形对比学习引起了人们的极大兴趣。然而，已有的图对比学习方法并不能充分保留原始图的局部拓扑结构，尤其是当相邻节点属于不同类别时。为了解决这一问题，本文提出了一种新的节点嵌入方法——局部线性对比嵌入(LLaCE)。LLaCE 通过利用局部线性公式来保持图形数据的内在几何结构，从而保证局部拓扑特征在嵌入空间中得到准确的反映。在一个合成数据集和五个实际数据集上的实验结果验证了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLaCE:+Locally+Linear+Contrastive+Embedding)|0|
|[GraphSAGE-based POI Recommendation via Continuous-Time Modeling](https://doi.org/10.1145/3589335.3651515)|Yuwen Liu, Lianyong Qi, Weiming Liu, Xiaolong Xu, Xuyun Zhang, Wanchun Dou||With the proliferation of Location-based Social Networks (LBSNs), user check-in data at Points-of-Interest (POIs) has surged, reshaping user-environment interaction. However, POI recommendation remains a challenging task for two primary reasons. First, external incentives often drive users' check-ins, potentially misrepresenting their genuine preferences. Second, while many current research model the temporal dynamics of user preferences in a discrete space, they ignore capturing the continuous evolution of these preferences. To address these challenges, we propose the GraphSAGE-based POI Recommendation via Continuous-Time Modeling (GSA-CTM). We first utilize GraphSAGE to identify real user preferences and filter out noise beyond the user's real preferences. After GraphSAGE captures complex interaction, we use Gated Recurrent Unit (GRU) combined with neural Ordinary Differential Equations (ODEs) to capture the temporal information embedded in the interaction, and then use neural ODEs to model the user's continuous dynamic preferences into continuous space. Experiments on two widely-used public datasets validate the superiority of our method.|随着基于位置的社交网络(LBSNs)的普及，用户在兴趣点(POI)的签到数据激增，重塑了用户与环境的交互。然而，POI 推荐仍然是一个具有挑战性的任务，主要有两个原因。首先，外部激励往往驱动用户的签到，可能会歪曲他们真正的偏好。其次，当前的许多研究在离散空间中模拟用户偏好的时间动态，他们忽略了捕捉这些偏好的持续演化。为了应对这些挑战，我们提出了基于 GraphSAGE 的连续时间建模 POI 建议(GSA-CTM)。我们首先利用 GraphSAGE 识别真实的用户偏好，并过滤掉超出用户真实偏好的噪音。在 GraphSAGE 捕获复杂交互后，利用门限回归单元(GRU)结合神经常微分方程(ODEs)捕获交互中的时间信息，然后利用神经常微分方程将用户的连续动态偏好建模为连续空间。在两个广泛使用的公共数据集上的实验验证了该方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphSAGE-based+POI+Recommendation+via+Continuous-Time+Modeling)|0|
|[Boost Social Recommendation via Adaptive Denoising Network](https://doi.org/10.1145/3589335.3651473)|Xinran Chen, Chaobo He, Quanlong Guan||Social recommendation aims to integrate social relationships to improve the performance of recommendation, and has attracted increasing attention in the field of recommendation system. Recently, Graph Neural Networks (GNNs) based methods for social recommendation are very competitive, but most of them overlook the fact that social relationships may have potential noises. Through the message passing mechanism of GNNs, these noises could be propagated and amplified, ultimately reducing the performance of recommendation. In view of this, we propose a novel GNN-based Adaptive Denoising Social Recommendation (ADSRec) method. It devises a denoising network, which can alleviate the impact of social relationships noises via the adaptive weight adjustment strategy. By further introducing the contrastive learning, the representations of users and items can be enhanced, leading to better recommendation results. Extensive experiments on three widely used datasets demonstrate the superiority of ADSRec over baselines.|社会推荐旨在整合社会关系，提高推荐绩效，在推荐系统领域受到越来越多的关注。近年来，基于图神经网络(GNN)的社会推荐方法竞争激烈，但大多忽视了社会关系可能存在潜在噪声的事实。通过 GNN 的消息传递机制，这些噪声可以被传播和放大，最终降低推荐的性能。鉴于此，我们提出了一种新的基于 GNN 的自适应去噪社会推荐(ADSRec)方法。通过自适应权重调整策略，设计了一个去噪网络，可以减轻社会关系噪声的影响。通过进一步引入对比学习，可以增强用户和项目的表示，从而获得更好的推荐结果。在三个广泛使用的数据集上的大量实验证明了 ADSRec 相对于基线的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Boost+Social+Recommendation+via+Adaptive+Denoising+Network)|0|
|[3D Face Reconstruction Using A Spectral-Based Graph Convolution Encoder](https://doi.org/10.1145/3589335.3651460)|Haoxin Xu, Zezheng Zhao, Yuxin Cao, Chunyu Chen, Hao Ge, Ziyao Liu||Monocular 3D face reconstruction plays a crucial role in avatar generation, with significant demand in web-related applications such as generating virtual financial advisors in FinTech. Current reconstruction methods predominantly rely on deep learning techniques and employ 2D self-supervision as a means to guide model learning. However, these methods encounter challenges in capturing the comprehensive 3D structural information of the face due to the utilization of 2D images for model training purposes. To overcome this limitation and enhance the reconstruction of 3D structural features, we propose an innovative approach that integrates existing 2D features with 3D features to guide the model learning process. Specifically, we introduce the 3D-ID Loss, which leverages the high-dimensional structure features extracted from a Spectral-Based Graph Convolution Encoder applied to the facial mesh. This approach surpasses the sole reliance on the 3D information provided by the facial mesh vertices coordinates. Our model is trained using 2D-3D data pairs from a combination of datasets and achieves state-of-the-art performance on the NoW benchmark.|单目3D 人脸重建在虚拟化身生成中起着至关重要的作用，在金融科技虚拟财务顾问生成等网络相关应用方面有着巨大的需求。目前的重建方法主要依赖于深度学习技术，采用二维自我监督作为指导模型学习的手段。然而，由于二维图像用于模型训练目的，这些方法在获取人脸的全面三维结构信息方面遇到了挑战。为了克服这一局限性，并加强三维结构特征的重建，我们提出了一种创新的方法，集成现有的二维特征与三维特征，以指导模型学习过程。具体来说，我们介绍了3D-ID 丢失，它利用了从基于光谱的图卷积编码器提取的高维结构特征应用到面部网格。这种方法超越了对面部网格顶点坐标提供的三维信息的唯一依赖。我们的模型是训练使用的2D-3D 数据对从一个组合的数据集，并实现了国家的最先进的性能的 NoW 基准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=3D+Face+Reconstruction+Using+A+Spectral-Based+Graph+Convolution+Encoder)|0|
|[General2Specialized LLMs Translation for E-commerce](https://doi.org/10.1145/3589335.3651510)|Kaidi Chen, Ben Chen, Dehong Gao, Huangyu Dai, Wen Jiang, Wei Ning, Shanqing Yu, Libin Yang, Xiaoyan Cai||Existing Neural Machine Translation (NMT) models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents. Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current NMT methods. To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain. Furthermore, we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce. The paradigm can be used for the NMT models based on Large language models (LLMs). Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and robustness of our G2ST approach, as compared with state-of-the-art NMT models such as LLaMA, Qwen, GPT-3.5, and even GPT-4.|现有的神经机器翻译(NMT)模型主要处理一般领域的翻译，而忽略了电子商务、法律文件等具有特殊写作公式的领域。以电子商务为例，文本通常包含大量领域相关词汇，存在较多的语法问题，导致现有的 NMT 方法性能较差。为了解决这些问题，我们收集了两个与领域相关的资源，包括一组术语对(对齐的汉英双语术语)和一个为电子商务领域注释的平行语料库。在此基础上，本文提出了一个具有自对比语义增强的两步微调范式(G2ST) ，将一个通用的 NMT 模型转化为电子商务的专用 NMT 模型。该范例可用于基于大型语言模型(LLM)的 NMT 模型。对实际电子商务标题的广泛评估表明，与 LLaMA，Qwen，GPT-3.5甚至 GPT-4等最先进的 NMT 模型相比，我们的 G2ST 方法具有优越的翻译质量和稳健性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=General2Specialized+LLMs+Translation+for+E-commerce)|0|
|[Counterfactual Explanations for Visual Recommender Systems](https://doi.org/10.1145/3589335.3651484)|Neham Jain, Vibhhu Sharma, Gaurav Sinha||Users rely on clever recommendations for items they might like to buy, and service providers rely on clever recommender systems to ensure that their product is recommended to their target audience. Providing explanations for recommendations helps to increase transparency and the users' overall trust in the system, besides helping practitioners debug their recommendation model. Modern recommendation systems utilize multi-modal data such as reviews and images to provide recommendation. In this work, we propose CAVIAR (Counterfactual explanations for VIsual Recommender systems), a novel method to explain recommender systems that utilize visual features of items. Our explanation is counterfactual and is optimized to be simultaneously simple and effective. Given an item in the user's top-K recommended list, CAVIAR makes a minimal, yet meaningful, perturbation to the item's image-embedding such that it is no longer a part of the list. In this way, CAVIAR aims to find the visual features of the item that were the most relevant for the recommendation. In order to lend meaning to the perturbations, we leverage CLIP model to connect the perturbed image features to textual features. We frame the explanation as a natural language counterfactual by contrasting the observed visual features in the item before and after the perturbation.|用户依靠聪明的推荐系统来选择他们想要购买的商品，而服务提供商则依靠聪明的推荐系统来确保他们的产品被推荐给他们的目标受众。除了帮助从业人员调试他们的推荐模型之外，为推荐提供解释有助于提高透明度和用户对系统的总体信任。现代推荐系统利用评论和图像等多模态数据来提供推荐。在这项工作中，我们提出了 CAVIAR (反事实解释的可视化推荐系统) ，一种新的方法来解释推荐系统，利用项目的视觉特征。我们的解释是反事实的，并优化为同时简单和有效。给定用户的 top-K 推荐列表中的一个项目，CAVIAR 对该项目的图像嵌入进行了最小但有意义的干扰，使其不再是列表的一部分。通过这种方式，CAVIAR 旨在找到与推荐最相关的项目的视觉特征。为了给扰动赋予意义，我们利用 CLIP 模型将受扰动的图像特征与文本特征连接起来。我们把解释作为一种反事实的自然语言，通过对比项目中观察到的视觉特征在扰动之前和之后。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Explanations+for+Visual+Recommender+Systems)|0|
|[Ad Laundering: How Websites Deceive Advertisers into Rendering Ads Next to Illicit Content](https://doi.org/10.1145/3589335.3651466)|Emmanouil Papadogiannakis, Panagiotis Papadopoulos, Evangelos P. Markatos, Nicolas Kourtellis||Providing online content monetized via ads to users is a lucrative business. But what if the content is pirated or illicit, thus harming the brand safety of the advertiser? In this paper, we are the first to investigate Ad Laundering: a technique with which bad actors deceive advertisers by hiding illicit content within evidently lawful websites to monetize the generated traffic. We develop a client-side detection methodology to detect and analyze websites performing ad laundering. We describe in detail the techniques these websites use to cloak content, and provide estimations for the ad revenues they are able to collect on a monthly basis. Finally, we attribute the generated revenue to different traffic channels and establish that even popular brands have their ads rendered next to undesirable content.|通过广告向用户提供在线内容是一项利润丰厚的业务。但如果内容是盗版或非法的，从而损害了广告商的品牌安全呢？在本文中，我们是第一个调查广告洗钱: 一种技术与不良行为者欺骗广告客户隐藏非法内容在明显合法的网站，以货币化所产生的流量。我们开发一个客户端检测方法来检测和分析网站进行广告洗钱。我们详细描述了这些网站用于隐藏内容的技术，并提供了他们每月能够收集的广告收入的估计。最后，我们将产生的收入归因于不同的流量渠道，并确定即使是流行品牌的广告也会出现在不受欢迎的内容旁边。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ad+Laundering:+How+Websites+Deceive+Advertisers+into+Rendering+Ads+Next+to+Illicit+Content)|0|
|[Is the 'Impression Log' Beneficial to Evaluating News Recommender Systems? No, it is Not!](https://doi.org/10.1145/3589335.3651527)|Jeewon Ahn, HongKyun Bae, SangWook Kim|Hanyang University, Seoul, Republic of Korea|This paper aims to answer the question of whether to use the impression log in evaluating news recommendation models. We start with a claim that the testing with the impression log composed of only hard-negative news (i.e., impression (IMP)-based test) is not beneficial to evaluating the models precisely. Based on the claim, we discuss a way of evaluating models by employing all kinds of negative news articles (i.e., Total test). Also, we propose a more-efficient way of evaluating models by sampling only a small number of negative articles (i.e., random-sampling (RS)-based test). We verify our claim by extensively comparing the evaluation results on six models from the IMP-based, Total, and RS-based tests: the RS-based test shows more accurate results than the IMP-based test in determining the superiority among the models while providing higher efficiency than the Total test. Therefore, our answer to the question above would be "do not employ the impression log in testing models even if it is available." This result is quite meaningful since it enables news recommendation researchers and practitioners, who have been using the impression log thus going to the wrong way, to turn to the right one.|本文旨在回答是否使用印象日志评价新闻推荐模型的问题。我们首先声称，印象日志的测试只有硬负面新闻(即，印象(IMP)为基础的测试)是不利于精确评价模型。在此基础上，我们讨论了一种利用各种负面新闻文章(即全面检验)来评价模型的方法。此外，我们提出了一个更有效的方法来评价模型，只抽样少量的负面文章(即，随机抽样(RS)为基础的测试)。我们通过广泛比较 IMP 测试、 Total 测试和 RS 测试的6个模型的评估结果来验证我们的说法: RS 测试在确定模型优劣方面比 IMP 测试显示出更准确的结果，同时提供了比 Total 测试更高的效率。因此，我们对上述问题的回答是“即使印象日志可用，也不要在测试模型中使用它。”这个结果是非常有意义的，因为它使新闻推荐的研究人员和从业人员，谁一直使用的印象日志，从而走向错误的方式，转向正确的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+the+'Impression+Log'+Beneficial+to+Evaluating+News+Recommender+Systems?+No,+it+is+Not!)|0|
|[How Good are LLMs in Generating Personalized Advertisements?](https://doi.org/10.1145/3589335.3651520)|Elyas Meguellati, Lei Han, Abraham Bernstein, Shazia W. Sadiq, Gianluca Demartini||In this paper, we explore the potential of large language models (LLMs) in generating personalized online advertisements (ads) tailored to specific personality traits, focusing on openness and neuroticism. We conducted a user study involving two tasks to understand the performance of LLM-generated ads compared to human-written ads in different online environments. Task 1 simulates a social media environment where users encounter ads while scrolling through their feed. Task 2 mimics a shopping website environment where users are presented with multiple sponsored products side-by-side. Our results indicate that LLM-generated ads targeting the openness trait positively impact user engagement and preferences, with performance comparable to human-written ads. Furthermore, in both scenarios, the overall effectiveness of LLM-generated ads was found to be similar to that of human-written ads, highlighting the potential of LLM-generated personalised content to rival traditional advertising methods with the added advantage of scalability. This study underscores the need for cautious consideration in the deployment of LLM-generated content at scale. While our findings confirm the scalability and potential effectiveness of LLM-generated content, there is an equally pressing concern about the ease with which it can be misused.|本文以开放性和神经质为核心，探讨了大语言模型(LLM)在生成针对特定人格特征的个性化网络广告中的潜力。我们进行了一项涉及两个任务的用户研究，以了解在不同的在线环境下，LLM 生成的广告相对于人写广告的表现。Task 1模拟了一个社交媒体环境，在这个环境中，用户在浏览 feed 时会看到广告。Task 2模拟了一个购物网站环境，在这个环境中，用户可以并排看到多种赞助商产品。我们的研究结果表明，LLM 生成的针对开放性特征的广告正向影响用户参与度和偏好，其性能与人写广告相当。此外，在这两种情况下，LLM 生成的广告的整体效果被发现类似于人写广告，突出了 LLM 生成的个性化内容与传统广告方法竞争的潜力，并具有额外的可扩展性优势。这项研究强调了在大规模部署 LLM 生成的内容时需要谨慎考虑的必要性。虽然我们的研究结果证实了 LLM 生成的内容的可伸缩性和潜在的有效性，但是同样紧迫的问题是它容易被滥用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Good+are+LLMs+in+Generating+Personalized+Advertisements?)|0|
|[Compact Interpretable Tensor Graph Multi-Modal News Embeddings](https://doi.org/10.1145/3589335.3651480)|Dawon Ahn, William Shiao, Arindam Khaled, Andrew Bauer, Stefanos Poulis, Evangelos E. Papalexakis||Online news articles encompass a variety of modalities such as text and images. How can we learn a representation that incorporates information from all those modalities in a compact and interpretable manner? In this paper, we propose CITEM (Compact Interpretable Tensor graph multi-modal news EMbedding), a tensor-based framework for compact and interpretable multi-modal news representations. CITEM generates a tensor graph consisting of a news similarity graph for each modality and employs a tensor decomposition to produce compact and interpretable embeddings, each dimension of which is a heterogeneous co-cluster of news articles and corresponding modalities. We extensively validate CITEM compared to baselines on two news classification tasks: misinformation news detection and news categorization. The experimental results show that CITEM performs within the same range of AUC as state-of-the-art baselines while producing 7x to 10.5x more compact embeddings. In addition, each embedding dimension of CITEM is interpretable, representing a latent co-cluster of articles.|在线新闻文章包括各种形式，如文字和图像。我们如何才能学会一种以简洁和可解释的方式将所有这些模式的信息纳入其中的表述方式？本文提出了紧凑可解释张量图多模态新闻嵌入(CITEM) ，这是一种基于张量的紧凑可解释多模态新闻表示框架。CITEM 为每种情态生成一个由新闻相似度图组成的张量图，并使用张量分解产生紧凑的、可解释的嵌入，其中每个维度是一个异质的新闻文章和相应模态的共簇。在错误信息检测和新闻分类这两个新闻分类任务中，我们将 CITEM 与基线进行了广泛的验证。实验结果表明，CITEM 在与最先进的基线相同的 AUC 范围内执行，同时产生7倍至10.5倍更紧凑的嵌入。此外，CITEM 的每个嵌入维度都是可解释的，表示一个潜在的文章共簇。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Compact+Interpretable+Tensor+Graph+Multi-Modal+News+Embeddings)|0|
|[Proactive Recommendation with Iterative Preference Guidance](https://doi.org/10.1145/3589335.3651548)|Shuxian Bi, Wenjie Wang, Hang Pan, Fuli Feng, Xiangnan He||Recommender systems mainly tailor personalized recommendations according to user interests learned from user feedback. However, such recommender systems passively cater to user interests and even reinforce existing interests in the feedback loop, leading to problems like filter bubbles and opinion polarization. To counteract this, proactive recommendation actively steers users towards developing new interests in a target item or topic by strategically modulating recommendation sequences. Existing work for proactive recommendation faces significant hurdles: 1) overlooking the user feedback in the guidance process; 2) lacking explicit modeling of the guiding objective; and 3) insufficient flexibility for integration into existing industrial recommender systems. To address these issues, we introduce an Iterative Preference Guidance (IPG) framework. IPG performs proactive recommendation in a flexible post-processing manner by ranking items according to their IPG scores that consider both interaction probability and guiding value. These scores are explicitly estimated with iteratively updated user representation that considers the most recent user interactions. Extensive experiments validate that IPG can effectively guide user interests toward target interests with a reasonable trade-off in recommender accuracy. The code is available at https://github.com/GabyUSTC/IPG-Rec.|推荐系统主要根据用户反馈中学到的用户兴趣量身定制个性化推荐。然而，这样的推荐系统被动地迎合了用户的兴趣，甚至加强了反馈回路中已有的兴趣，导致了过滤泡沫和观点两极分化等问题。为了解决这个问题，主动推荐通过策略性地调整推荐顺序，积极地引导用户在目标项目或主题中发展新的兴趣。主动推荐的现有工作面临着重大障碍: 1)忽视指导过程中的用户反馈; 2)缺乏指导目标的明确建模; 3)集成到现有行业推荐系统的灵活性不足。为了解决这些问题，我们引入了一个迭代偏好指导(IPG)框架。IPG 采用灵活的后处理方式，根据 IPG 分数对项目进行排序，同时考虑交互概率和指导价值，从而实现主动推荐。这些分数是通过考虑最近的用户交互的迭代更新的用户表示来显式估计的。大量的实验验证了 IPG 能够有效地引导用户兴趣向目标兴趣转移，并在推荐准确性方面做出了合理的权衡。密码可在 https://github.com/gabyustc/ipg-rec 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Proactive+Recommendation+with+Iterative+Preference+Guidance)|0|
|[FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs](https://doi.org/10.1145/3589335.3651504)|EunCheol Choi, Emilio Ferrara||Our society is facing rampant misinformation harming public health and trust. To address the societal challenge, we introduce FACT-GPT, a system leveraging Large Language Models (LLMs) to automate the claim matching stage of fact-checking. FACT-GPT, trained on a synthetic dataset, identifies social media content that aligns with, contradicts, or is irrelevant to previously debunked claims. Our evaluation shows that our specialized LLMs can match the accuracy of larger models in identifying related claims, closely mirroring human judgment. This research provides an automated solution for efficient claim matching, demonstrates the potential of LLMs in supporting fact-checkers, and offers valuable resources for further research in the field.|我们的社会正面临猖獗的错误信息危害公众健康和信任。为了应对社会挑战，我们引入了 FACT-GPT，这是一个利用大型语言模型(LLM)来自动化索赔匹配事实检查阶段的系统。FACT-GPT 在一个合成数据集上进行训练，识别与之前被揭穿的主张相一致、相互矛盾或无关的社交媒体内容。我们的评估表明，我们的专业 LLM 可以匹配的准确性较大的模型在识别相关的索赔，密切反映人类的判断。该研究为索赔匹配提供了一个自动化的解决方案，展示了 LLM 在支持事实核查方面的潜力，并为该领域的进一步研究提供了有价值的资源。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FACT-GPT:+Fact-Checking+Augmentation+via+Claim+Matching+with+LLMs)|0|
|[Benchmarking News Recommendation in the Era of Green AI](https://doi.org/10.1145/3589335.3651472)|Qijiong Liu, Jieming Zhu, Quanyu Dai, XiaoMing Wu||Over recent years, news recommender systems have gained significant attention in both academia and industry, emphasizing the need for a standardized benchmark to evaluate and compare the performance of these systems. Concurrently, Green AI advocates for reducing the energy consumption and environmental impact of machine learning. To address these concerns, we introduce the first Green AI benchmarking framework for news recommendation, known as GreenRec, and propose a metric for assessing the tradeoff between recommendation accuracy and efficiency. Our benchmark encompasses 30 base models and their variants, covering traditional end-to-end training paradigms as well as our proposed efficient only-encode-once (OLEO) paradigm. Through experiments consuming 2000 GPU hours, we observe that the OLEO paradigm achieves competitive accuracy compared to state-of-the-art end-to-end paradigms and delivers up to a 2992% improvement in sustainability metrics.|近年来，新闻推荐系统引起了学术界和工业界的重视，强调需要一个标准化的基准来评估和比较这些系统的性能。同时，绿色人工智能倡导降低机器学习的能源消耗和环境影响。为了解决这些问题，我们引入了第一个新闻推荐的绿色 AI 基准框架，称为 GreenRec，并提出了一个评估推荐准确性和效率之间权衡的度量标准。我们的基准包括30个基本模型及其变体，涵盖了传统的端到端训练范例以及我们提出的有效的只编码一次(OLEO)范例。通过消耗2000 GPU 小时的实验，我们观察到 OLEO 范例与最先进的端到端范例相比达到了具有竞争力的准确性，并在可持续性指标方面提供了高达2992% 的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Benchmarking+News+Recommendation+in+the+Era+of+Green+AI)|0|
|[Event GDR: Event-Centric Generative Document Retrieval](https://doi.org/10.1145/3589335.3651500)|Yong Guan, Dingxiao Liu, Jinchen Ma, Hao Peng, Xiaozhi Wang, Lei Hou, Ru Li||Generative document retrieval, an emerging paradigm in information retrieval, learns to build connections between documents and identifiers within a single model, garnering significant attention. However, there are still two challenges: (1) neglecting inner-content correlation during document representation; (2) lacking explicit semantic structure during identifier construction. Nonetheless, events have enriched relations and well-defined taxonomy, which could facilitate addressing the above two challenges. Inspired by this, we propose Event GDR, an event-centric generative document retrieval model, integrating event knowledge into this task. Specifically, we utilize an exchange-then-reflection method based on multi-agents for event knowledge extraction. For document representation, we employ events and relations to model the document to guarantee the comprehensiveness and inner-content correlation. For identifier construction, we map the events to well-defined event taxonomy to construct the identifiers with explicit semantic structure. Our method achieves significant improvement over the baselines on two datasets, and also hopes to provide insights for future research.|生成文献检索是一种新兴的信息检索范式，它学会在单一模型中建立文档和标识符之间的联系，引起了人们的极大关注。但是，目前仍然存在两个问题: (1)忽视文档表示中的内容相关性; (2)标识符构造中缺乏明确的语义结构。尽管如此，事件丰富了关系和明确的分类，这可能有助于解决上述两个挑战。受此启发，我们提出以事件为中心的生成文献检索模型 Event gDR，将事件知识整合到这项任务中。具体地说，我们利用了一种基于多智能体的交换-反射方法来提取事件知识。对于文档表示，我们使用事件和关系对文档进行建模，以保证文档的全面性和内容的相关性。对于标识符构造，我们将事件映射到定义良好的事件分类法，以构造具有显式语义结构的标识符。我们的方法在两个数据集的基线上取得了显著的改进，同时也希望能够为以后的研究提供一些启示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Event+GDR:+Event-Centric+Generative+Document+Retrieval)|0|
|[DIAERESIS: Knowledge Graph Partitioning for Efficient Query Answering](https://doi.org/10.1145/3589335.3651231)|Georgia Troullinou, Kostas Stefanidis, Dimitris Plexousakis, Haridimos Kondylakis||The rapid explosion of linked data demands effective and efficient storage, management, and querying methods. Apache Spark is one of the most widely used engines for big data processing, with more and more systems adopting it for efficient query answering. Existing approaches, exploiting Spark for querying RDF data, adopt partitioning techniques for reducing the data that need to be accessed in order to improve efficiency. However, simplistic methods for data partitioning fail to minimize data access at query answering and effectively improve query efficiency. In this demonstration, we present DIAERESIS, a novel platform that exploits a summary-based partitioning strategy achieving a significant improvement in minimizing data access and as such improving query-answering efficiency. DIAERESIS first identifies the top-k most important schema nodes and distributes the other schema nodes to the centroid they mostly depend on. Then, it allocates the corresponding instance nodes to the schema nodes they are instantiated under, creating vertical sub-partitions and indexes. We allow conference participants to actively identify the impact of our partitioning methodology on data distribution and replication, data accessed for query answering, and query answering efficiency. Further, we contrast our approach with existing partitioning approaches adopted by state-of-the-art systems in the domain, providing a deep understanding of the challenges in the area.|链接数据的快速增长需要高效的存储、管理和查询方法。Apache Spark 是应用最广泛的大数据处理引擎之一，越来越多的系统采用它来进行高效的查询应答。利用 Spark 查询 RDF 数据的现有方法采用了分区技术，以减少需要访问的数据，从而提高效率。然而，过于简单化的数据分区方法并不能使查询应答时的数据访问最小化，从而有效地提高查询效率。在这个演示中，我们介绍了 DIAERESIS，这是一个利用基于摘要的分区策略的新平台，它在最小化数据访问方面实现了显著的改进，从而提高了查询应答的效率。DIAERESIS 首先标识 top-k 最重要的模式节点，并将其他模式节点分配到它们主要依赖的中心。然后，它将相应的实例节点分配给它们被实例化的模式节点，从而创建垂直的子分区和索引。我们允许与会者主动确定我们的分区方法对数据分布和复制、查询应答所访问的数据以及查询应答效率的影响。此外，我们将我们的方法与该领域最先进的系统采用的现有分区方法进行对比，从而深入了解该领域的挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DIAERESIS:+Knowledge+Graph+Partitioning+for+Efficient+Query+Answering)|0|
|[Box2Go: Collaborative Interactive Infobox Filling](https://doi.org/10.1145/3589335.3651235)|Benjamin Hättasch, Carsten Binnig||Infoboxes can be useful to quickly learn about the contents of text collections, but manually creating them is error-prone and time-consuming, and existing automatic approaches require training data or resources like ontologies that are not available for every domain. Moreover, they lack techniques for adaptation to the user. We therefore propose a system to automatically fill user-defined attributes of infoboxes with the human-in-the-loop which provides this adaptation, and works without training data and domain-specific resources. Our approach generalizes simple user feedback to explore a joint embedding space and find the correct values for the attributes. These structured representations of the texts can be used for collaborative exploration of text collections on the web. We provide a prototypic implementation for such a collaborative web application and demonstrate its usage.|Infobox 对于快速了解文本集合的内容非常有用，但是手动创建它们是容易出错和耗时的，而且现有的自动化方法需要培训数据或资源，比如本体，这些并不适用于每个领域。此外，他们缺乏适应用户的技术。因此，我们提出了一个系统来自动填充用户定义的属性信息箱与人在循环提供这种适应，并没有训练数据和领域特定的资源。我们的方法通过简单的用户反馈来探索一个联合嵌入空间，并找到属性的正确值。这些文本的结构化表示可用于协作探索网络上的文本集合。我们为这样一个协作 Web 应用程序提供了一个原型实现，并演示了它的用法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Box2Go:+Collaborative+Interactive+Infobox+Filling)|0|
|[HINCare: An Intelligent Helper Recommender System for Elderly Care](https://doi.org/10.1145/3589335.3651236)|Carrie Wang, Wentao Ning, Xiaoman Wu, Reynold Cheng||In Hong Kong, the number of elderly citizens will reach one-third of the population within the next decade. To mitigate this problem, timebanking has received attention in recent years. In timebanking, an NGO helper earns time credits through providing voluntary services (e.g., household duties) to elders. These time credits can be used to acquire other services. Although timebanking has shown the promise of promoting mutual care in many countries, its potential has not been fully utilized, due to the lack of IT and data support. We thus develop HINCare, a software platform that supports timebanking for multiple NGOs. Besides providing convenience to NGO supervisors, helpers, and elders, HINCare makes use of a heterogeneous information network (HIN) for recommending suitable helpers to elders. This is the first time a graph-based recommender system is used for such purposes. Currently, HINCare is used by 12 NGOs to serve more than 5000 users in Hong Kong. In this demonstration, participants can play the role of helpers and elders in the HINCare environment.|在香港，长者人数将在未来十年内达到人口的三分之一。为了缓解这个问题，时间银行近年来受到了关注。在时间银行方面，非政府机构的助手透过为长者提供义务服务(例如家务) ，赚取时间信贷。这些时间积分可以用来获得其他服务。虽然时间银行在许多国家显示了促进相互照顾的希望，但由于缺乏信息技术和数据支持，其潜力尚未得到充分利用。因此，我们开发了 HINCare，一个支持多个非政府组织时间银行的软件平台。除了为非政府机构的主管、协助者和长者提供方便外，HINCare 亦利用多元化的资讯网络，向长者推荐合适的协助者。这是基于图表的推荐系统首次用于这种目的。目前，有12个非政府机构使用 HINCare 服务，为香港超过5000名使用者提供服务。在这个示范中，参与者可以在 HINCare 环境中扮演帮助者和长者的角色。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HINCare:+An+Intelligent+Helper+Recommender+System+for+Elderly+Care)|0|
|[Linked Open Literature Review using the Neuro-symbolic Open Research Knowledge Graph](https://doi.org/10.1145/3589335.3651238)|Azanzi Jiomekong, Sören Auer, Allard Oelen||The way scholarly knowledge and in particular literature reviews are communicated today rather resembles static, unstructured, pseudo-digitized articles, which are hardly processable by machines and AI. This demo showcases a novel way to create and publish scholarly literature reviews, also called semantic reviews. The neuro-symbolic approach consists of extracting key insights from scientific papers leveraging neural models and organizing them using a symbolic scholarly knowledge graph. The food information engineering review case study will allow participants to see how this approach is implemented using the Open Research Knowledge Graph (ORKG). The real-time demo will allow participants to play with the ORKG and create their own living, semantic review.|今天，学术知识，特别是文献综述的传播方式非常类似于静态的、非结构化的、伪数字化的文章，这些文章很难被机器和人工智能处理。这个演示展示了一种创建和发布学术文献评论的新方法，也称为语义评论。神经-符号方法包括利用神经模型从科学论文中提取关键见解，并使用符号学术知识图表对其进行组织。食品信息工程回顾案例研究将使参与者了解如何使用开放研究知识图(ORKG)实施这种方法。实时演示将允许参与者与 ORKG 一起玩，并创建他们自己的生活，语义审查。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Linked+Open+Literature+Review+using+the+Neuro-symbolic+Open+Research+Knowledge+Graph)|0|
|[RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems](https://doi.org/10.1145/3589335.3651242)|Jianxun Lian, Yuxuan Lei, Xu Huang, Jing Yao, Wei Xu, Xing Xie||This paper introduces RecAI, a practical toolkit designed to augment or even revolutionize recommender systems with the advanced capabilities of Large Language Models (LLMs). RecAI provides a suite of tools, including Recommender AI Agent, Recommendation-oriented Language Models, Knowledge Plugin, RecExplainer, and Evaluator, to facilitate the integration of LLMs into recommender systems from multifaceted perspectives. The new generation of recommender systems, empowered by LLMs, are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric recommendation experiences. We hope the open-source of RecAI can help accelerate evolution of new advanced recommender systems. The source code of RecAI is available at <https://github.com/microsoft/RecAI>.|本文介绍了 RecAI，这是一个实用的工具包，旨在用大型语言模型(LLM)的先进功能来增强甚至革新推荐系统。RecAI 提供了一套工具，包括推荐人工智能代理、面向推荐的语言模型、知识插件、重新解释器和评估器，以促进从多方面的角度将 LLM 集成到推荐系统中。由 LLM 授权的新一代推荐系统预计将更加通用、可解释、可对话和可控，为更加智能和以用户为中心的推荐体验铺平道路。我们希望 RecAI 的开源能够帮助加速新的高级推荐系统的发展。RecAI 的源代码可于 <  https://github.com/microsoft/RecAI 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RecAI:+Leveraging+Large+Language+Models+for+Next-Generation+Recommender+Systems)|0|
|[Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform](https://doi.org/10.1145/3589335.3651243)|Mingyue Cheng, Hao Zhang, Jiqian Yang, Qi Liu, Li Li, Xin Huang, Liwei Song, Zhi Li, Zhenya Huang, Enhong Chen||Large language model evaluation plays a pivotal role in the enhancement of its capacity. Previously, numerous methods for evaluating large language models have been proposed in this area. Despite their effectiveness, these existing works mainly focus on assessing objective questions, overlooking the capability to evaluate subjective questions which is extremely common for large language models. Additionally, these methods predominantly utilize centralized datasets for evaluation, with question banks concentrated within the evaluation platforms themselves. Moreover, the evaluation processes employed by these platforms often overlook personalized factors, neglecting to consider the individual characteristics of both the evaluators and the models being evaluated. To address these limitations, we propose a novel anonymous crowd-sourcing evaluation platform, BingJian, for large language models that employs a competitive scoring mechanism where users participate in ranking models based on their performance. This platform stands out not only for its support of centralized evaluations to assess the general capabilities of models but also for offering an open evaluation gateway. Through this gateway, users have the opportunity to submit their questions, testing the models on a personalized and potentially broader range of capabilities. Furthermore, our platform introduces personalized evaluation scenarios, leveraging various forms of human-computer interaction to assess large language models in a manner that accounts for individual user preferences and contexts. The demonstration of BingJian can be accessed at https://github.com/Mingyue-Cheng/Bingjian.|大型语言模型评价在提高语言能力方面起着举足轻重的作用。在此之前，许多评估大型语言模型的方法已经被提出在这个领域。尽管这些工作很有效，但现有的工作主要集中在评价客观问题上，忽视了评价主观问题的能力，而这种能力在大型语言模型中极为常见。此外，这些方法主要利用集中的数据集进行评估，问题库集中在评估平台本身。此外，这些平台所采用的评价过程往往忽视个性化因素，忽视考虑评价者和被评价模型的个性特征。为了解决这些局限性，我们提出了一个新的匿名众包评估平台，兵鉴，为大型语言模型，采用竞争性评分机制，其中用户参与排名模型的基础上，他们的表现。该平台不仅支持集中评价，以评估模型的一般能力，而且还提供了一个开放的评价网关。通过这个网关，用户有机会提交他们的问题，测试模型的个性化和潜在的更广泛的能力范围。此外，我们的平台引入了个性化的评估场景，利用各种形式的人机交互来评估大型语言模型，以考虑到个人用户偏好和上下文的方式。市民可透过 https://github.com/mingyue-cheng/BingJian 浏览「冰箭」的演示资料。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Personalized+Evaluation+of+Large+Language+Models+with+An+Anonymous+Crowd-Sourcing+Platform)|0|
|[PKG API: A Tool for Personal Knowledge Graph Management](https://doi.org/10.1145/3589335.3651247)|Nolwenn Bernard, Ivica Kostric, Weronika Lajewska, Krisztian Balog, Petra Galuscáková, Vinay Setty, Martin G. Skjæveland||Personal knowledge graphs (PKGs) offer individuals a way to store and consolidate their fragmented personal data in a central place, improving service personalization while maintaining full user control. Despite their potential, practical PKG implementations with user-friendly interfaces remain scarce. This work addresses this gap by proposing a complete solution to represent, manage, and interface with PKGs. Our approach includes (1) a user-facing PKG Client, enabling end-users to administer their personal data easily via natural language statements, and (2) a service-oriented PKG API. To tackle the complexity of representing these statements within a PKG, we present an RDF-based PKG vocabulary that supports this, along with properties for access rights and provenance.|个人知识图(PKG)为个人提供了一种在中心位置存储和整合支离破碎的个人数据的方法，改善了服务的个性化，同时保持了完全的用户控制。尽管 PKG 具有潜力，但实际的具有用户友好界面的 PKG 实现仍然很少。这项工作通过提出一个完整的解决方案来表示、管理和与 PKG 的接口来弥补这一差距。我们的方法包括(1)一个面向用户的 PKG 客户端，使最终用户能够通过自然语言语句轻松地管理他们的个人数据，和(2)一个面向服务的 PKG API。为了解决在 PKG 中表示这些语句的复杂性，我们提供了一个基于 RDF 的 PKG 词汇表来支持这一点，以及访问权限和出处的属性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PKG+API:+A+Tool+for+Personal+Knowledge+Graph+Management)|0|
|[Brinjal: A Web-Plugin for Collaborative Hate Speech Detection](https://doi.org/10.1145/3589335.3651250)|Ming Shan Hee, Karandeep Singh, Charlotte Ng Si Min, Kenny Tsu Wei Choo, Roy KaWei Lee||The proliferation of hate speech (HS) has compromised the safety and trustworthiness of the internet, exacerbating social divides by promoting hatred and discrimination. Although recent studies have produced guidelines and developed advanced technologies for the automated detection of HS, their efficacy and adaptability in real-world applications remain unclear. Furthermore, existing guidelines on what constitutes HS might not reflect the perspectives and beliefs of individuals and communities. This paper introduces Brinjal, a multifaceted web plugin designed for the collaborative detection of HS. Brinjal enables individuals to identify instances of HS and engage in discussions to verify such content, thereby enhancing the collective understanding of HS. Additionally, Brinjal serves as a practical platform for deploying and evaluating advanced HS detection models, facilitating user interaction and performance assessment. Lastly, Brinjal includes an analytical tool for analyzing HS, offering insights based on the crowdsourced instances and discussions about HS across various websites. The video demonstration of Brinjal can be viewed here: https://youtu.be/\_JxziIVWBO4. Disclaimer: This paper contains violent and discriminatory content that may be disturbing to some readers.|仇恨言论(HS)的泛滥损害了互联网的安全性和可信度，通过宣扬仇恨和歧视加剧了社会分歧。尽管最近的研究为 HS 的自动检测制定了指导方针并开发了先进技术，但其在现实应用中的功效和适应性仍不清楚。此外，关于何为协调制度的现行准则可能不反映个人和社区的观点和信仰。本文介绍了一个用于协同检测 HS 的多方面 Web 插件 Brinjal。Brinjal 使个人能够确定协调制度的实例，并参与讨论以核实这些内容，从而加强对协调制度的集体理解。此外，Brinjal 还是部署和评估先进 HS 检测模型的实用平台，方便用户交互和性能评估。最后，Brinjal 包括一个分析 HS 的分析工具，提供基于众包实例的见解，以及跨不同网站的关于 HS 的讨论。Brinjal 的视频展示可以在这里看到:  https://youtu.be/_jxziivwbo4。免责声明: 本文含有暴力和歧视性内容，可能会令一些读者感到不安。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Brinjal:+A+Web-Plugin+for+Collaborative+Hate+Speech+Detection)|0|
|[SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores](https://doi.org/10.1145/3589335.3651251)|Vidminas Vizgirda, Rui Zhao, Naman Goel||We present SocialGenPod, a decentralised and privacy-friendly way of deploying generative AI Web applications. Unlike centralised Web and data architectures that keep user data tied to application and service providers, we show how one can use Solid – a decentralised Web specification – to decouple user data from generative AI applications. We demonstrate SocialGenPod using a prototype that allows users to converse with different Large Language Models, optionally leveraging Retrieval Augmented Generation to generate answers grounded in private documents stored in any Solid Pod that the user is allowed to access, directly or indirectly. SocialGenPod makes use of Solid access control mechanisms to give users full control of determining who has access to data stored in their Pods. SocialGenPod keeps all user data (chat history, app configuration, personal documents, etc) securely in the user's personal Pod; separate from specific model or application providers. Besides better privacy controls, this approach also enables portability across different services and applications. Finally, we discuss challenges, posed by the large compute requirements of state-of-the-art models, that future research in this area should address. Our prototype is open-source and available at: https://github.com/Vidminas/socialgenpod/.|我们展示了 SocialGenPod，一种分散的和隐私友好的方式来部署生成性 AI Web 应用程序。与将用户数据与应用程序和服务提供商绑定在一起的集中式 Web 和数据架构不同，我们展示了如何使用 Solid ——一种分散式 Web 规范——将用户数据与生成式 AI 应用程序分离开来。我们使用一个允许用户与不同的大型语言模型进行对话的原型来演示 SocialGenPod，可以选择性地利用获取增强生成技术来生成基于私有文档的答案，这些私有文档存储在 Solid Pod 中，用户可以直接或间接地访问这些文档。SocialGenPod 利用 Solid 访问控制机制，让用户完全控制谁可以访问存储在 Pods 中的数据。SocialGenPod 将所有用户数据(聊天记录、应用程序配置、个人文档等)安全地保存在用户的个人 Pod 中; 与特定的模型或应用程序提供商分开。除了更好的隐私控制，这种方法还支持跨不同服务和应用程序的可移植性。最后，我们讨论了这一领域未来研究应该解决的挑战，这些挑战是由最先进模型的巨大计算需求造成的。我们的原型是开源的，可以在以下 https://github.com/vidminas/socialgenpod/获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SocialGenPod:+Privacy-Friendly+Generative+AI+Social+Web+Applications+with+Decentralised+Personal+Data+Stores)|0|
|[Ducho 2.0: Towards a More Up-to-Date Unified Framework for the Extraction of Multimodal Features in Recommendation](https://doi.org/10.1145/3589335.3651440)|Matteo Attimonelli, Danilo Danese, Daniele Malitesta, Claudio Pomo, Giuseppe Gassi, Tommaso Di Noia||In this work, we introduce Ducho 2.0, the latest stable version of our framework. Differently from Ducho, Ducho 2.0 offers a more personalized user experience with the definition and import of custom extraction models fine-tuned on specific tasks and datasets. Moreover, the new version is capable of extracting and processing features through multimodal-by-design large models. Notably, all these new features are supported by optimized data loading and storing to the local memory. To showcase the capabilities of Ducho 2.0, we demonstrate a complete multimodal recommendation pipeline, from the extraction/processing to the final recommendation. The idea is to provide practitioners and experienced scholars with a ready-to-use tool that, put on top of any multimodal recommendation framework, may permit them to run extensive benchmarking analyses. All materials are accessible at: <https://github.com/sisinflab/Ducho>.|在本文中，我们将介绍 Ducho 2.0，它是我们框架的最新稳定版本。与 Ducho 不同，Ducho 2.0提供了更个性化的用户体验，定义和导入了针对特定任务和数据集进行微调的自定义提取模型。此外，新版本还能够通过设计的多模态大模型提取和处理特征。值得注意的是，所有这些新特性都得到了优化的数据加载和存储到本地内存的支持。为了展示 Ducho 2.0的功能，我们展示了一个完整的多模式推荐管道，从提取/处理到最终推荐。这个想法是为从业人员和有经验的学者提供一个现成的工具，放在任何多模式建议框架之上，可以允许他们进行广泛的基准分析。所有资料可浏览以下 https://github.com/sisinflab/ducho  :。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ducho+2.0:+Towards+a+More+Up-to-Date+Unified+Framework+for+the+Extraction+of+Multimodal+Features+in+Recommendation)|0|
|[SE-PQA: Personalized Community Question Answering](https://doi.org/10.1145/3589335.3651445)|Pranav Kasela, Marco Braga, Gabriella Pasi, Raffaele Perego||Personalization in Information Retrieval is a topic studied for a long time. Nevertheless, there is still a lack of high-quality, real-world datasets to conduct large-scale experiments and evaluate models for personalized search. This paper contributes to filling this gap by introducing SE-PQA (StackExchange - Personalized Question Answering), a new curated resource to design and evaluate personalized models related to the task of community Question Answering (cQA). The contributed dataset includes more than 1 million queries and 2 million answers, annotated with a rich set of features modeling the social interactions among the users of a popular cQA platform. We describe the characteristics of SE-PQA and detail the features associated with questions and answers. We also provide reproducible baseline methods for the cQA task based on the resource, including deep learning models and personalization approaches. The results of the preliminary experiments conducted show the appropriateness of SE-PQA to train effective cQA models; they also show that personalization remarkably improves the effectiveness of all the methods tested. Furthermore, we show the benefits in terms of robustness and generalization of combining data from multiple communities for personalization purposes.|信息检索的个性化是一个长期研究的课题。尽管如此，仍然缺乏高质量的、真实世界的数据集来进行大规模的实验和评估个性化检索模型。本文通过引入 SE-PQA 来填补这一空白。 SE-PQA 是一种新的策划资源，用于设计和评估与社区问答任务(cQA)相关的个性化模型。贡献的数据集包括超过100万个查询和200万个答案，并用一组丰富的特性对流行的 cQA 平台的用户之间的社交互动进行了建模。我们描述了 SE-PQA 的特征，并详细描述了与问答相关的特征。我们还为基于资源的 cQA 任务提供了可重复的基线方法，包括深度学习模型和个性化方法。初步实验结果表明，SE-PQA 方法训练有效的 cQA 模型是合适的，并且个性化显著提高了所有测试方法的有效性。此外，我们还展示了将来自多个社区的数据用于个性化目的的健壮性和通用性方面的好处。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SE-PQA:+Personalized+Community+Question+Answering)|0|
|[Digital Democracy at Crossroads: A Meta-Analysis of Web and AI Influence on Global Elections](https://doi.org/10.1145/3589335.3652003)|Zheng Wei, Xian Xu, Pan Hui||2024 will be the largest election year in history involving over 50 countries and approximately 4.2 billion people. Since 1996, the Web has been instrumental in political campaigns, enhancing public engagement and creating new communication avenues for elections. Nevertheless, the proliferation of generative AI technologies has made false information dissemination simpler and quicker, posing a substantial threat to election integrity and democratic processes. The 2024 global elections underscore the need to comprehend and tackle the impact of such technologies on democracy. In this paper, we undertake a detailed meta-analysis, scrutinizing 44 papers published in The Web Conference, detailing the influence of the Web on elections. Our research reveals key historical trends on how the Web has impacted elections: first, social media has revolutionized election strategies through direct voter-candidate interactions. Second, big data and algorithm-driven campaigns are commonplace. Third, AI advancements have exacerbated the spread of fake news, risking election fairness. Predominantly from studies published since 2018 among 44 papers, we underscore the necessity for advanced detection tools, policy formulation, and responsible AI use to maintain electoral integrity. This analysis offers an insight into the Web and AI's impact on elections, presenting pointers for addressing challenges and leveraging opportunities in the 2024 and future elections.|2024年将是历史上规模最大的选举年，有50多个国家参加，人口约42亿。自1996年以来，网络在政治运动、加强公众参与和为选举创造新的沟通渠道方面发挥了重要作用。然而，生成性人工智能技术的扩散使虚假信息的传播变得更加简单和迅速，对选举公正性和民主进程构成重大威胁。2024年的全球大选突显出，有必要理解和应对此类技术对民主政体的影响。在本文中，我们进行了详细的元分析，审查了44篇论文发表在网络会议，详细说明了网络对选举的影响。我们的研究揭示了网络如何影响选举的关键历史趋势: 首先，社交媒体通过选民与候选人之间的直接互动，彻底改变了选举策略。其次，大数据和算法驱动的活动很常见。第三，人工智能的进步加剧了假新闻的传播，使选举公平面临风险。从2018年以来发表的44篇论文中，我们主要强调了使用先进的检测工具、政策制定和负责任的人工智能来维护选举公正性的必要性。本文分析了网络和人工智能对选举的影响，提出了在2024年和未来选举中应对挑战和利用机会的建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Digital+Democracy+at+Crossroads:+A+Meta-Analysis+of+Web+and+AI+Influence+on+Global+Elections)|0|
|[BoxCare: A Box Embedding Model for Disease Representation and Diagnosis Prediction in Healthcare Data](https://doi.org/10.1145/3589335.3651448)|Hang Lv, Zehai Chen, Yacong Yang, Guofang Ma, Yanchao Tan, Carl Yang||Diagnosis prediction is becoming crucial to develop healthcare plans for patients based on Electronic Health Records (EHRs). Existing works usually enhance diagnosis prediction via learning accurate disease representation, where many of them try to capture inclusive relations based on the hierarchical structures of existing disease ontologies such as those provided by ICD-9 codes. However, they overlook exclusive relations that can reflect different and complementary perspectives of the ICD-9 structures, and thus fail to accurately represent relations among diseases and ICD-9 codes. To this end, we propose to project disease embeddings and ICD-9 code embeddings into boxes, where a box is an axis-aligned hyperrectangle with a geometric region and two boxes can clearly "include" or "exclude" each other. Upon box embeddings, we further obtain patient embeddings via aggregating the disease representations for diagnosis prediction. Extensive experiments on two real-world EHR datasets show significant performance gains brought by our proposed framework, yielding average improvements of 6.04% for diagnosis prediction over state-of-the-art competitors.|基于电子健康记录(EHRs)为患者制定医疗保健计划时，诊断预测变得至关重要。现有的工作通常通过学习准确的疾病表示来增强诊断预测，其中许多工作试图捕获基于现有疾病本体(如 ICD-9代码提供的那些)的层次结构的包容性关系。然而，他们忽视了可以反映 ICD-9结构的不同和互补视角的排他性关系，因此不能准确地表示疾病和 ICD-9编码之间的关系。为此，我们建议将疾病嵌入和 ICD-9代码嵌入投影到盒子中，其中盒子是具有几何区域的轴对齐的超矩形，并且两个盒子可以清楚地“包含”或“排除”彼此。在盒子嵌入后，我们通过聚合疾病表示来进一步获得病人嵌入以进行诊断预测。在两个真实世界的 EHR 数据集上进行的大量实验表明，我们提出的框架带来了显著的性能提升，与最先进的竞争对手相比，诊断预测平均提高了6.04% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BoxCare:+A+Box+Embedding+Model+for+Disease+Representation+and+Diagnosis+Prediction+in+Healthcare+Data)|0|
|[Enabling Pre-Shock State Detection using Electrogram Signals from Implantable Cardioverter-Defibrillators](https://doi.org/10.1145/3589335.3651450)|Runze Yan, Neal K. Bhatia, Faisal M. Merchant, Alex Fedorov, Ran Xiao, Cheng Ding, Xiao Hu||Identifying electrical signatures preceding a ventricular arrhythmia from the implantable cardioverter-defibrillators (ICDs) can help predict an upcoming ICD shock. To achieve this, we first deployed a large-scale study (N=326) to continuously monitor the electrogram (EGM) data from the ICDs and select the EGM segments prior to a shock event and under the normal condition. Next, we design a novel cohesive framework that integrates metric learning, prototype learning, and few-shot learning, enabling learning from an imbalanced dataset. We implement metric learning by leveraging a Siamese neural network architecture, which incorporates LSTM units. We innovatively utilize triplet and pair losses in a sequential manner throughout the training process on EGM samples. This approach generates embeddings that significantly enhance the distinction of EGM signals under different conditions. In the inference stage, k-means clustering identifies prototypes representing pre-shock and normal states from these embeddings. In summary, this framework leverages the predictive potential of signals before ICD shocks, addressing the gap in early cardiac arrhythmia detection. Our experimental results show a notable F1 score of 0.87, sensitivity of 0.97, and precision of 0.79. Our framework offers a significant advancement in cardiac care predictive analytics, promising enhanced ICD decision-making for improved patient outcomes.|从植入式心脏复律除颤器(ICD)中识别室性心律失常前的电特征可以帮助预测即将到来的 ICD 休克。为了实现这一点，我们首先部署了一项大规模的研究(N = 326) ，以连续监测来自 ICD 的电图(EGM)数据，并在休克事件之前和正常情况下选择 EGM 片段。接下来，我们设计了一个新的内聚框架，集成度量学习，原型学习和少镜头学习，使学习从一个不平衡的数据集。我们利用结合了 LSTM 单元的暹罗神经网络结构来实现度量学习。在 EGM 样本的训练过程中，我们创新性地以连续的方式利用了三元组和配对损失。这种方法产生的嵌入显着增强了区分 EGM 信号在不同的条件下。在推理阶段，K平均算法从这些嵌入中识别代表预休克和正常状态的原型。总之，这个框架利用了 ICD 冲击前信号的预测潜力，弥补了早期心律不整检测的差距。实验结果表明，F1得分为0.87，灵敏度为0.97，精密度为0.79。我们的框架在心脏护理预测分析方面取得了重大进展，有望增强 ICD 决策，改善患者预后。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enabling+Pre-Shock+State+Detection+using+Electrogram+Signals+from+Implantable+Cardioverter-Defibrillators)|0|
|[In The Beginning, Let There Be The Word: Challenges and Insights in Applying Sentiment Analysis to Social Research](https://doi.org/10.1145/3589335.3651264)|Andrzej Meler||Sentiment analysis based on lexical corpora is widely employed despite its inherent limitations in capturing nuances such as sarcasm and irony. This research delves into the application of sentiment analysis to political communication. To address the limitations of the Bag of Words methodology, a comparative study of sentiment analysis tools and emotion detection from speech is conducted, using automated speech recognition as a benchmark. Emotion recognition from speech has shown promising results, indicating its potential superiority over other methods [1], [2]. This study uses media material from Polish radio and television broadcasts, focusing on political interviews during a significant period marked by a high-profile assassination attempt. Results indicate challenges at the micro-level, but aggregated data reveals a significant correlation between valence measured from voice and text. While sentiment analysis may lack sensitivity in capturing mourning-related discourse, it proves effective in political communication devoid of such nuances. This suggests that valence in sentiment analysis reflects emotional content derived from intonation fairly accurately.|尽管基于词汇语料库的情感分析在捕捉讽刺和反讽等细微差别方面存在固有的局限性，但它仍然得到了广泛的应用。本研究探讨情绪分析在政治传播中的应用。针对“词袋”方法的局限性，以自动语音识别为基准，对情感分析工具和语音情感检测工具进行了比较研究。来自语音的情感识别已经显示出有希望的结果，表明其潜在的优势比其他方法[1] ，[2]。这项研究使用了来自波兰广播和电视广播的媒体材料，重点是在一个引人注目的企图谋杀的重要时期的政治采访。结果表明在微观层面上存在挑战，但汇总数据显示从语音和文本测量的价值之间存在显著的相关性。虽然情绪分析在捕捉与哀悼有关的话语时可能缺乏敏感性，但它在没有这种细微差别的政治交流中被证明是有效的。这表明情感分析中的配价能够相当准确地反映由语调产生的情感内容。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=In+The+Beginning,+Let+There+Be+The+Word:+Challenges+and+Insights+in+Applying+Sentiment+Analysis+to+Social+Research)|0|
|[Modeling Multidimensional Cognitive Search in Creativity with Generalized Additive Model](https://doi.org/10.1145/3589335.3651267)|Jia Lin Cheoh||Creativity is the ability to develop innovative functional ideas through unconventional associations. The consensus view on creativity in the literature involves divergence from stereotypical and habitual thought patterns [29, 39]. Creativity relies on search to explore diverse solutions. Search requires charting the mental terrain, leveraging past experiences and knowledge to manipulate and reconfigure components for new solutions [28]. The generally-accepted and overly-narrow view on creativity, however, neglects the fact that creativity is multidimensional [14]. This one-dimensional view of creativity triggers questions such as "Does one consider an unethical but novel creation to be creative?" and "Does one consider a new iPhone with mainstream functionalities but advanced camera features to be creative?" This research challenges the one-dimensional view of creativity, offering a more all-encompassing conceptualization of creativity [14]. The research examines the multidimensional nature of creativity by building a computational model of a designer's mutual search process across multiple mutually dependent search spaces. The research examines the trajectory of mutual search across multiple cognitive search spaces using a Generalized Additive Model (GAM). The field experiment employs 108 designers who develop their web designs through five iterations, utilizing computer graphics methods to extract the images. Through measuring the distance of search by considering changes in visual and source code in each iteration, the study argues that the search patterns differ in the degree of exploration in these search spaces over time. The research concludes that designers' search processes are non-linear and argues that there are more than one or two search spaces. The research also provides perceptual explanations of the multiple search processes in designs and argues for a more encompassing view of creativity.|创造力是通过非传统的联想来发展创新的功能性想法的能力。文献中关于创造力的一致观点涉及到从陈规定型和习惯性思维模式的分歧[29,39]。创造力依赖于探索不同的解决方案。搜索需要绘制心理地形图，利用过去的经验和知识来操纵和重新配置新的解决方案的组件[28]。然而，普遍接受和过于狭隘的创造力观点忽视了这样一个事实，即创造力是多维的[14]。这种关于创造力的一维观点引发了诸如“一个人是否认为一个不道德但新颖的创造是创造性的?”以及“是否有人认为一款拥有主流功能但高级摄像功能的新 iPhone 具有创造性?”这项研究挑战了创造力的一维观点，提供了一个更全面的创造力概念化[14]。该研究通过建立一个设计师在多个相互依赖的搜索空间中相互搜索过程的计算模型，来检验创造力的多维性。本研究使用广义加法模型(GAM)检验了跨多个认知搜索空间的相互搜索轨迹。现场实验雇佣了108名设计师，他们通过五次迭代开发他们的网页设计，利用计算机图形学方法提取图像。通过考虑每次迭代中视觉和源代码的变化来测量搜索距离，研究认为，随着时间的推移，这些搜索空间中的搜索模式在程度上是不同的。研究得出的结论是，设计师的搜索过程是非线性的，并认为有一个或两个以上的搜索空间。该研究还提供了对设计中的多重搜索过程的感性解释，并提出了一个更全面的创造力观点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+Multidimensional+Cognitive+Search+in+Creativity+with+Generalized+Additive+Model)|0|
|[Data Augmentation for Conversational AI](https://doi.org/10.1145/3589335.3641238)|Heydar Soudani, Roxana Petcu, Evangelos Kanoulas, Faegheh Hasibi|Radboud Univ Nijmegen, Nijmegen, Netherlands; Univ Amsterdam, Amsterdam, Netherlands|Advancements in conversational systems have revolutionized information access, surpassing the limitations of single queries. However, developing dialogue systems requires a large amount of training data, which is a challenge in low-resource domains and languages. Traditional data collection methods like crowd-sourcing are labor-intensive and time-consuming, making them ineffective in this context. Data augmentation (DA) is an affective approach to alleviate the data scarcity problem in conversational systems. This tutorial provides a comprehensive and up-to-date overview of DA approaches in the context of conversational systems. It highlights recent advances in conversation augmentation, open domain and task-oriented conversation generation, and different paradigms of evaluating these models. We also discuss current challenges and future directions in order to help researchers and practitioners to further advance the field in this area.|会话系统的进步彻底改变了信息访问，超越了单个查询的局限性。然而，开发对话系统需要大量的训练数据，这在资源不足的领域和语言中是一个挑战。传统的数据收集方法，如众包，是劳动密集型和耗时的，使他们在这种情况下无效。数据增强是解决会话系统中数据稀缺问题的有效途径。本教程提供了在会话系统上下文中 DA 方法的全面和最新概述。它强调了会话增强，开放领域和面向任务的会话生成的最新进展，以及评估这些模型的不同范式。我们还讨论了当前的挑战和未来的方向，以帮助研究人员和从业人员进一步推进该领域。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data+Augmentation+for+Conversational+AI)|0|
|[Recent Advances in Generative Information Retrieval](https://doi.org/10.1145/3589335.3641239)|Yubao Tang, Ruqing Zhang, Weiwei Sun, Jiafeng Guo, Maarten de Rijke||Generative retrieval (GR) has witnessed significant growth recently in the area of information retrieval. Compared to the traditional "index-retrieve-then-rank'' pipeline, the GR paradigm aims to consolidate all information within a corpus into a single model. Typically, a sequence-to-sequence model is trained to directly map a query to its relevant document identifiers (i.e., docids). This tutorial offers an introduction to the core concepts of the GR paradigm and a comprehensive overview of recent advances in its foundations and applications. We start by providing preliminary information covering foundational aspects and problem formulations of GR. Then, our focus shifts towards recent progress in docid design, training approaches, inference strategies, and applications of GR. We end by outlining challenges and issuing a call for future GR research.This tutorial is intended to be beneficial to both researchers and industry practitioners interested in developing novel GR solutions or applying them in real-world scenarios.|生成性检索(GR)近来在信息检索领域取得了显著的发展。与传统的“索引-检索-然后排名”流水线相比，GR 范式旨在将语料库中的所有信息合并到一个单一模型中。通常，序列到序列模型被训练为直接将查询映射到其相关文档标识符(即 docids)。本教程介绍了 GR 范式的核心概念，并全面概述了其基础和应用方面的最新进展。我们首先提供涵盖 GR 的基础方面和问题表述的初步信息。然后，我们的重点转移到最近的进展，医学设计，训练方法，推理策略，和应用的 GR。最后，我们概述了挑战，并呼吁未来的 GR 研究。本教程的目的是为有兴趣开发新颖 GR 解决方案或将其应用于实际场景的研究人员和行业从业人员提供有益的帮助。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recent+Advances+in+Generative+Information+Retrieval)|0|
|[Large Language Models for Recommendation: Progresses and Future Directions](https://doi.org/10.1145/3589335.3641247)|Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He|Univ Sci & Technol China, Hefei, Peoples R China; Natl Univ Singapore, Singapore, Singapore|The powerful large language models (LLMs) have played a pivotal role in advancing recommender systems. Recently, in both academia and industry, there has been a surge of interest in developing LLMs for recommendation, referred to as LLM4Rec. This includes endeavors like leveraging LLMs for generative item retrieval and ranking, as well as the exciting possibility of building universal LLMs for diverse open-ended recommendation tasks. These developments hold the potential to reshape the traditional recommender paradigm, paving the way for the next-generation recommender systems. In this tutorial, we aim to retrospect the evolution of LLM4Rec and conduct a comprehensive review of existing research. In particular, we will clarify how recommender systems benefit from LLMs through a variety of perspectives, including the model architecture, learning paradigm, and the strong abilities of LLMs such as chatting, generalization, planning, and generation. Furthermore, we will discuss the critical challenges and open problems in this emerging field, for instance, the trustworthiness, efficiency, and model retraining issues. Lastly, we will summarize the implications of previous work and outline future research directions. We believe that this tutorial will assist the audience in better understanding the progress and prospects of LLM4Rec, inspiring them for future exploration. This, in turn, will drive the prosperity of LLM4Rec, possibly fostering a paradigm shift in recommendation systems.|强大的大型语言模型(LLM)在推进推荐系统方面发挥了关键作用。最近，在学术界和工业界，对开发推荐 LLM 的兴趣激增，称为 LLM4Rec。这包括利用 LLM 进行生成性项目检索和排名，以及为各种开放式推荐任务构建通用 LLM 的令人兴奋的可能性。这些发展有可能重塑传统的推荐模式，为下一代推荐系统铺平道路。在本教程中，我们的目标是回顾 LLM4Rec 的演变，并对现有的研究进行一个全面的回顾。特别是，我们将通过多种视角阐明推荐系统如何从 LLM 中受益，包括模型体系结构、学习范式以及 LLM 的强大能力，如聊天、泛化、规划和生成。此外，我们将讨论在这个新兴领域的关键挑战和公开问题，例如，可信赖性，效率和模型再培训问题。最后，我们将总结以往工作的启示，并概述未来的研究方向。我们相信，本教程将帮助观众更好地了解 LLM4Rec 的进展和前景，鼓舞他们进行未来的探索。这反过来将推动 LLM4Rec 的繁荣，可能促进推荐系统的范式转变。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+for+Recommendation:+Progresses+and+Future+Directions)|0|
|[Multimodal Pretraining and Generation for Recommendation: A Tutorial](https://doi.org/10.1145/3589335.3641248)|Jieming Zhu, Xin Zhou, Chuhan Wu, Rui Zhang, Zhenhua Dong||Personalized recommendation stands as a ubiquitous channel for users to explore information or items aligned with their interests. Nevertheless, prevailing recommendation models predominantly rely on unique IDs and categorical features for user-item matching. While this ID-centric approach has witnessed considerable success, it falls short in comprehensively grasping the essence of raw item contents across diverse modalities, such as text, image, audio, and video. This underutilization of multimodal data poses a limitation to recommender systems, particularly in the realm of multimedia services like news, music, and short-video platforms. The recent surge in pretraining and generation techniques presents both opportunities and challenges in the development of multimodal recommender systems. This tutorial seeks to provide a thorough exploration of the latest advancements and future trajectories in multimodal pretraining and generation techniques within the realm of recommender systems. The tutorial comprises three parts: multimodal pretraining, multimodal generation, and industrial applications and open challenges in the field of recommendation. Our target audience encompasses scholars, practitioners, and other parties interested in this domain. By providing a succinct overview of the field, we aspire to facilitate a swift understanding of multimodal recommendation and foster meaningful discussions on the future development of this evolving landscape.|个性化推荐是用户探索符合自己兴趣的信息或项目的一个无处不在的渠道。然而，流行的推荐模型主要依赖于用户项匹配的唯一 ID 和分类特征。虽然这种以 ID 为中心的方法已经取得了相当大的成功，但是它不能全面地掌握原始项目内容在不同模式(如文本、图像、音频和视频)中的本质。这种对多模式数据的利用不足对推荐系统造成了限制，特别是在新闻、音乐和短视频平台等多媒体服务领域。最近在培训前和生成技术方面的激增在开发多式联运推荐系统方面既带来了机遇，也带来了挑战。本教程旨在全面探讨推荐系统领域内多模式预培训和生成技术的最新进展和未来发展轨迹。本教程包括三个部分: 多模式预训练、多模式生成、工业应用和推荐领域的公开挑战。我们的目标受众包括学者、实践者和对该领域感兴趣的其他方面。我们希望通过简要介绍这一领域的情况，促进对多式联运建议的迅速理解，并推动就这一不断变化的格局的未来发展进行有意义的讨论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Pretraining+and+Generation+for+Recommendation:+A+Tutorial)|0|
|[On-Device Recommender Systems: A Tutorial on The New-Generation Recommendation Paradigm](https://doi.org/10.1145/3589335.3641250)|Hongzhi Yin, Tong Chen, Liang Qu, Bin Cui||Given the sheer volume of contemporary e-commerce applications, recommender systems (RSs) have gained significant attention in both academia and industry. However, traditional cloud-based RSs face inevitable challenges, such as resource-intensive computation, reliance on network access, and privacy breaches. In response, a new paradigm called on-device recommender systems (ODRSs) has emerged recently in various industries like Taobao, Google, and Kuaishou. ODRSs unleash the computational capacity of user devices with lightweight recommendation models tailored for resource-constrained environments, enabling real-time inference with users' local data. This tutorial aims to systematically introduce methodologies of ODRSs, including (1) an overview of existing research on ODRSs; (2) a comprehensive taxonomy of ODRSs, where the core technical content to be covered span across three major ODRS research directions, including on-device deployment and inference, on-device training, and privacy/security of ODRSs; (3) limitations and future directions of ODRSs. This tutorial expects to lay the foundation and spark new insights for follow-up research and applications concerning this new recommendation paradigm.|鉴于当代电子商务应用的庞大数量，推荐系统(RS)已经引起了学术界和工业界的重视。然而，传统的基于云的 RSS 面临着不可避免的挑战，如资源密集型计算、对网络访问的依赖和隐私泄露。作为回应，最近在淘宝、谷歌和 Kuaishou 等不同行业出现了一种名为在设备上推荐系统(on-device )的新模式。ODRS 使用轻量级推荐模型释放用户设备的计算能力，这些模型专门针对资源受限的环境，支持对用户的本地数据进行实时推断。本教程旨在系统地介绍 ODRS 的方法，包括(1) ODRS 现有研究的概述; (2) ODRS 的综合分类，其中涵盖的核心技术内容跨越三个主要的 ODRS 研究方向，包括在设备上的部署和推理，在设备上的培训，以及 ODRS 的隐私/安全; (3) ODRS 的局限性和未来的方向。本教程期望为后续研究和有关这种新的推荐范例的应用程序奠定基础并激发新的见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On-Device+Recommender+Systems:+A+Tutorial+on+The+New-Generation+Recommendation+Paradigm)|0|
|[TransDrift: Modeling Word-Embedding Drift using Transformer](https://doi.org/10.1145/3589335.3651894)|Nishtha Madaan, Prateek Chaudhury, Nishant Kumar, Srikanta Bedathur||In modern NLP applications, word embeddings are a crucial backbone that can be readily shared across a number of tasks. However as the text distributions change and word semantics evolve over time, the downstream applications using the embeddings can suffer if the word representations do not conform to the data drift. Thus, maintaining word embeddings to be consistent with the underlying data distribution is a key problem. In this work, we tackle this problem and propose TransDrift, a transformer-based prediction model for word embeddings. Leveraging the flexibility of transformer, our model accurately learns the dynamics of the embedding drift and predicts the future embedding. In experiments, we compare with existing methods and show that our model makes significantly more accurate predictions of the word embedding than the baselines. Crucially, by applying the predicted embeddings as a backbone for downstream classification tasks, we show that our embeddings lead to superior performance compared to the previous methods.|在现代自然语言处理应用中，单词嵌入是一个关键的骨干，可以很容易地跨多个任务共享。然而，随着文本分布的变化和词语义的演变，如果词表示不符合数据漂移，使用嵌入的下游应用程序可能会受到影响。因此，维护单词嵌入与底层数据分布的一致性是一个关键问题。在这项工作中，我们解决了这个问题，并提出了 TransDrift，一个基于变压器的字嵌入预测模型。利用变压器的灵活性，我们的模型准确地学习嵌入漂移的动态，并预测未来的嵌入。在实验中，我们与现有的方法进行了比较，结果表明我们的模型对单词嵌入的预测明显比基线更准确。重要的是，通过将预测嵌入作为下游分类任务的骨干，我们表明我们的嵌入方法比以前的方法具有更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TransDrift:+Modeling+Word-Embedding+Drift+using+Transformer)|0|
|[Automated Claim Matching with Large Language Models: Empowering Fact-Checkers in the Fight Against Misinformation](https://doi.org/10.1145/3589335.3651910)|EunCheol Choi, Emilio Ferrara||In today's digital era, the rapid spread of misinformation poses threats to public well-being and societal trust. As online misinformation proliferates, manual verification by fact checkers becomes increasingly challenging. We introduce FACT-GPT (Fact-checking Augmentation with Claim matching Task-oriented Generative Pre-trained Transformer), a framework designed to automate the claim matching phase of fact-checking using Large Language Models (LLMs). This framework identifies new social media content that either supports or contradicts claims previously debunked by fact-checkers. Our approach employs GPT-4 to generate a labeled dataset consisting of simulated social media posts. This data set serves as a training ground for fine-tuning more specialized LLMs. We evaluated FACT-GPT on an extensive dataset of social media content related to public health. The results indicate that our fine-tuned LLMs rival the performance of larger pre-trained LLMs in claim matching tasks, aligning closely with human annotations. This study achieves three key milestones: it provides an automated framework for enhanced fact-checking; demonstrates the potential of LLMs to complement human expertise; offers public resources, including datasets and models, to further research and applications in the fact-checking domain.|在当今的数字时代，错误信息的迅速传播对公众福祉和社会信任构成了威胁。随着在线错误信息的激增，事实核查人员的手工验证变得越来越具有挑战性。本文介绍了 FACT-GPT (FACT-GPT) ，这是一个使用大型语言模型(LLM)实现索赔匹配阶段事实检查自动化的框架。该框架确定了新的社交媒体内容，这些内容要么支持要么与事实核查人员之前揭穿的说法相矛盾。我们的方法使用 GPT-4来生成一个由模拟的社交媒体帖子组成的标记数据集。这个数据集可以作为微调更专业化 LLM 的训练基地。我们评估了 FACT-GPT 的广泛数据集的社会媒体内容相关的公共卫生。结果表明，我们的微调 LLM 在索赔匹配任务中的性能与较大的预训练 LLM 相当，与人工注释非常接近。这项研究达到了三个关键的里程碑: 它提供了一个增强事实核查的自动化框架; 展示了 LLM 补充人类专业知识的潜力; 提供公共资源，包括数据集和模型，以进一步研究和应用在事实核查领域。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automated+Claim+Matching+with+Large+Language+Models:+Empowering+Fact-Checkers+in+the+Fight+Against+Misinformation)|0|
|[Decoding YouTube's Recommendation System: A Comparative Study of Metadata and GPT-4 Extracted Narratives](https://doi.org/10.1145/3589335.3651913)|Mayor Inna Gurung, Md Monoarul Islam Bhuiyan, Ahmed AlTaweel, Nitin Agarwal||YouTube's recommendation system is integral to shaping user experiences by suggesting content based on past interactions using collaborative filtering techniques. Nonetheless, concerns about potential biases and homogeneity in these recommendations are prevalent, with the danger of leading users into filter bubbles and echo chambers that reinforce their pre-existing beliefs. Researchers have sought to understand and address these biases in recommendation systems. However, traditionally, such research has relied primarily on metadata, such as video titles, which does not always encapsulate the full content or context of the videos. This reliance on metadata can overlook the nuances and substantive content of videos, potentially perpetuating the very biases and echo chambers that the research aims to unravel. This study advances the examination of sentiment, toxicity, and emotion within YouTube content by conducting a comparative analysis across various depths of titles and narratives extracted by leveraging GPT-4. Our analysis reveals a clear trend in sentiment, emotion, and toxicity levels as the depth of content analysis increases. Notably, there is a general shift from neutral to positive sentiments in both YouTube video titles and narratives. Emotion analysis indicates an increase in positive emotions, particularly joy, with a corresponding decrease in negative emotions such as anger and disgust in narratives, while video titles show a steady decrease in anger. Additionally, toxicity analysis presents a contrasting pattern, with video titles displaying an upward trend in toxicity, peaking at the greatest depth analyzed, whereas narratives exhibit a high initial toxicity level that sharply decreases and stabilizes at lower depths. These findings suggest that the depth of engagement with video content significantly influences emotional and sentiment expressions.|YouTube 的推荐系统是塑造用户体验不可或缺的一部分，它通过使用协同过滤技术，根据过去的互动提供内容建议。尽管如此，对这些建议中潜在的偏见和同质性的担忧普遍存在，有可能导致用户进入过滤器泡沫和回声室，从而加强他们先前存在的信念。研究人员试图理解和解决推荐系统中的这些偏见。然而，传统上，这类研究主要依赖于元数据，如视频标题，它并不总是封装视频的完整内容或上下文。这种对元数据的依赖可能会忽视视频的细微差别和实质性内容，有可能使研究旨在揭示的偏见和回声室永久化。这项研究通过利用 GPT-4对不同深度的标题和叙述进行比较分析，提高了对 YouTube 内容中的情绪、毒性和情感的检查。我们的分析显示，随着内容分析深度的增加，情绪、情绪和毒性水平有明显的趋势。值得注意的是，无论是 YouTube 视频标题还是叙事，都普遍从中性情绪转向积极情绪。情绪分析表明积极情绪的增加，特别是快乐，与相应的消极情绪，如愤怒和厌恶的减少叙述，而视频标题显示了一个稳定的减少愤怒。此外，毒性分析呈现出一种对比模式，视频标题显示毒性呈上升趋势，在分析的最深处达到峰值，而叙述性展示出高的初始毒性水平，在较低深度急剧下降并稳定下来。这些发现表明，视频内容的参与程度显著影响情绪和情绪的表达。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decoding+YouTube's+Recommendation+System:+A+Comparative+Study+of+Metadata+and+GPT-4+Extracted+Narratives)|0|
|[Graph Coarsening via Convolution Matching for Scalable Graph Neural Network Training](https://doi.org/10.1145/3589335.3651920)|Charles Dickens, Edward W. Huang, Aishwarya Reganti, Jiong Zhu, Karthik Subbian, Danai Koutra||Graph summarization as a preprocessing step is an effective and complementary technique for scalable graph neural network (GNN) training. In this work, we propose the Coarsening Via Convolution Matching (CONVMATCH) algorithm and a highly scalable variant, A-CONVMATCH, for creating summarized graphs that preserve the output of graph convolution. We evaluate CONVMATCH on six real-world link prediction and node classification graph datasets, and show it is efficient and preserves prediction performance while significantly reducing the graph size. Notably, CONVMATCH achieves up to 95 performance of GNNs on node classification while trained on graphs summarized down to 1 tasks, CONVMATCH consistently outperforms all baselines, achieving up to a 2x improvement.|图形摘要作为一种预处理步骤，是可扩展图神经网络(GNN)训练的有效补充技术。在这项工作中，我们提出了粗化通过卷积匹配(CONVMATCH)算法和一个高度可伸缩的变体，A-CONVMATCH，用于创建总结图保持图卷积的输出。对实际的六个链路预测和节点分类图数据集进行了 CONVMATCH 评估，结果表明该算法在保持预测性能的同时，显著减小了图的大小。值得注意的是，CONVMATCH 在节点分类上实现了高达95个 GNN 的性能，同时对归纳为1个任务的图进行训练，CONVMATCH 始终优于所有基线，实现了高达2倍的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Coarsening+via+Convolution+Matching+for+Scalable+Graph+Neural+Network+Training)|0|
|[FedHLT: Efficient Federated Low-Rank Adaption with Hierarchical Language Tree for Multilingual Modeling](https://doi.org/10.1145/3589335.3651933)|Zhihan Guo, Yifei Zhang, Zhuo Zhang, Zenglin Xu, Irwin King||Federated Multilingual Modeling (FMM) has become an essential approach in natural language processing (NLP) due to increasing linguistic diversity and the heightened emphasis on data privacy. However, FMM faces two primary challenges: 1) the high communication costs inherent in network operations, and 2) the complexities arising from parameter interference, as languages exhibit both unique characteristics and shared features. To tackle these issues, we introduce a communication-efficient framework for Multilingual Modeling (MM) that combines low-rank adaptation with a hierarchical language tree structure. Our method maintains the base model's weights while focusing on updating only the Low-rank adaptation (LoRA) parameters, significantly reducing communication costs. Additionally, we mitigate parameter conflicts by organizing languages based on their familial ties rather than merging all LoRA parameters together. Our experimental findings reveal that this novel model surpasses established baseline models in performance and markedly decreases communication overhead.|由于语言多样性的增加和对数据隐私的重视，联邦多语言建模(FMM)已经成为自然语言处理(NLP)中的一种重要方法。然而，FMM 面临着两个主要的挑战: 1)网络操作固有的高通信成本，2)参数干扰产生的复杂性，因为语言既表现出独特的特征又表现出共享的特征。为了解决这些问题，我们引入了一个多语言建模(MM)的通信高效框架，它结合了低等级适应性和层次化的语言树结构。该方法在保持基本模型权重的同时，只更新低秩自适应(LoRA)参数，大大降低了通信成本。此外，我们根据语言的亲缘关系来组织语言，而不是将所有 LoRA 参数合并在一起，从而减少了参数冲突。我们的实验结果表明，这种新模型在性能上超过了已建立的基线模型，并显著降低了通信开销。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedHLT:+Efficient+Federated+Low-Rank+Adaption+with+Hierarchical+Language+Tree+for+Multilingual+Modeling)|0|
|[Information Retrieval Meets Large Language Models](https://doi.org/10.1145/3589335.3641299)|Zheng Liu, Yujia Zhou, Yutao Zhu, Jianxun Lian, Chaozhuo Li, Zhicheng Dou, Defu Lian, JianYun Nie|Shandong Artificial Intelligence Institute, China; Huawei Technologies Ltd. Co, China; University of Science and Technology of China, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, China; Shandong University, China; South China University of Technology, China; Zhejiang University, China; Renmin University of China, China; Institute of Computing Technology, Chinese Academy of Sciences, China; Tsinghua University, China; Wuhan University, China; Jilin University, China; Beijing University of Posts and Telecommunications, China|The research field of Information Retrieval (IR) has evolved significantly, expanding beyond traditional search to meet diverse user information needs. Recently, Large Language Models (LLMs) have demonstrated exceptional capabilities in text understanding, generation, and knowledge inference, opening up exciting avenues for IR research. LLMs not only facilitate generative retrieval but also offer improved solutions for user understanding, model evaluation, and user-system interactions. More importantly, the synergistic relationship among IR models, LLMs, and humans forms a new technical paradigm that is more powerful for information seeking. IR models provide real-time and relevant information, LLMs contribute internal knowledge, and humans play a central role of demanders and evaluators to the reliability of information services. Nevertheless, significant challenges exist, including computational costs, credibility concerns, domain-specific limitations, and ethical considerations. To thoroughly discuss the transformative impact of LLMs on IR research, the Chinese IR community conducted a strategic workshop in April 2023, yielding valuable insights. This paper provides a summary of the workshop’s outcomes, including the rethinking of IR’s core values, the mutual enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and open challenges.|信息检索搜索(IR)的研究领域已经发生了显著的变化，超越了传统的搜索，以满足不同的用户信息需求。最近，大语言模型(LLM)在文本理解、生成和知识推理方面展示了非凡的能力，为 IR 研究开辟了令人兴奋的途径。LLM 不仅促进了生成检索，而且为用户理解、模型评估和用户系统交互提供了改进的解决方案。更重要的是，IR 模型、 LLM 和人类之间的协同关系形成了一种新的技术范式，这种范式在信息搜索方面更加强大。IR 模型提供实时和相关的信息，LLM 提供内部知识，而人在信息服务的可靠性中扮演着需求者和评估者的中心角色。尽管如此，仍然存在重大的挑战，包括计算成本、可信度问题、特定领域的局限性和伦理考虑。为了深入讨论 LLM 对国际关系研究的变革性影响，中国国际关系界于2023年4月举办了一次战略研讨会，提出了宝贵的见解。本文提供了研讨会成果的总结，包括重新思考 IR 的核心价值，LLM 和 IR 的相互增强，一个新的 IR 技术范式的提议，以及开放的挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Information+Retrieval+Meets+Large+Language+Models)|0|
|[Heterogeneous Knowledge Grounding for Medical Question Answering with Retrieval Augmented Large Language Model](https://doi.org/10.1145/3589335.3651941)|Wenting Zhao, Zhongfen Deng, Shweta Yadav, Philip S. Yu||The Large Language Model (LLM) is renowned for its ability to encode a vast amount of general domain knowledge, enabling it to excel in question-answering, dialogue systems, and summarization tasks. However, the medical domain presents a unique challenge to LLM due to the distribution of medical knowledge, which follows a long-tail pattern. Existing approaches address this challenge by injecting medical knowledge into LLM through single sources such as medical textbooks or medical knowledge bases. However, medical knowledge is distributed across multiple heterogeneous information sources. A medical question-answering system can enhance answer coverage and confidence by considering these diverse knowledge sources together. To bridge this gap, we propose a novel approach called Heterogeneous Knowledge Retrieval-Augmented LLM for medical domain question answering. Our experiments, conducted on the MedQA-USMLE dataset, demonstrate promising performance improvements. These results underscore the importance of harnessing heterogeneous knowledge sources in the medical domain.|大型语言模型(LLM)以其编码大量通用领域知识的能力而闻名，使其能够在问答、对话系统和摘要任务方面表现出色。然而，由于医学知识的分布遵循长尾模式，医学领域对 LLM 提出了独特的挑战。现有方法通过单一来源，如医学教科书或医学知识库，将医学知识注入法医学。然而，医学知识是分布在多个异构信息源之间的。一个医学问答系统可以通过综合考虑这些不同的知识来源来提高答案的覆盖范围和信心。为了弥补这一差距，我们提出了一种新的医学领域问题回答方法——异构知识检索增强 LLM。我们在 MedQA-USMLE 数据集上进行的实验证明了有希望的性能改进。这些结果强调了在医学领域利用异质知识来源的重要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Heterogeneous+Knowledge+Grounding+for+Medical+Question+Answering+with+Retrieval+Augmented+Large+Language+Model)|0|
|[Weakly Supervised Video Moment Retrieval via Location-irrelevant Proposal Learning](https://doi.org/10.1145/3589335.3651942)|Wei Ji, Ruiqi Shi, Yinwei Wei, Shanshan Zhao, Roger Zimmermann||This paper deals with Video Moment Retrieval (VMR) in a weakly-supervised fashion, which aims to retrieve local video clips with only global video-level descriptions. Scrutinizing the recent advances in VMR, we find that the fully-supervised models achieve strong performance, but they are heavily relied on the precise temporal annotations. Weakly-supervised methods do not rely on temporal annotations, however, their performance is much weaker than the fully-supervised ones. To fill such gap, we propose to take advantage of a pretrained video-text model as hitchhiker to generate pseudo temporal labels. The pseudo temporal labels, together with the descriptive labels, are then utilized to guide the training of the proposed VMR model. The proposed Location-irrelevant Proposal Learning (LPL) model is based on a pretrained video-text model with cross-modal prompt learning, together with different strategies to generate reasonable proposals with various lengths. Despite the simplicity, we find that our method performs much better than the previous state-of-the-art methods on standard benchmarks, eg., +4.4% and +1.4% in mIoU on the Charades and ActivityNet-Caption datasets respectively, which benefits from training with fine-grained video-text pairs. Further experiments on two synthetic datasets with shuffled temporal location and longer video length demonstrate our model's robustness towards temporal localization bias as well as its strength in handling long video sequences.|本文采用弱监督方式处理视频矩检索(VMR) ，目的是检索只有全局视频级描述的局部视频片段。仔细研究 VMR 的最新进展，我们发现全监督模型取得了很好的性能，但是它们严重依赖于精确的时间注释。弱监督方法不依赖于时间注释，但是它们的性能要比完全监督方法弱得多。为了填补这一空白，我们提出利用预先训练的视频文本模型作为搭便车者来生成伪时态标签。然后利用伪时态标签和描述性标签对 VMR 模型进行训练。提出的位置不相关建议学习(LPL)模型是基于预先训练的视频文本模型和跨模式的提示学习，以及不同的策略来产生不同长度的合理建议。尽管简单，我们发现我们的方法在标准基准上比以前的最先进的方法表现得更好，例如，在 Charades 和 ActivityNet-Caption 数据集上分别增加了4.4% 和1.4% 的 mIoU，这得益于细粒度视频文本对的训练。通过对两个具有混合时间定位和较长视频长度的合成数据集的进一步实验，证明了该模型对时间定位偏差的鲁棒性以及对长视频序列的处理能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weakly+Supervised+Video+Moment+Retrieval+via+Location-irrelevant+Proposal+Learning)|0|
|[One-step Reach: LLM-based Keyword Generation for Sponsored Search Advertising](https://doi.org/10.1145/3589335.3651943)|Yang Wang, Zheyi Sha, Kunhai Lin, Chaobing Feng, Kunhong Zhu, Lipeng Wang, Xuewu Jiao, Fei Huang, Chao Ye, Dengwu He, Zhi Guo, Shuanglong Li, Lin Liu||Query keyword matching plays a crucial role in sponsored search advertising by retrieving semantically related keywords of the user query to target relevant advertisements. Conventional technical solutions adopt the retrieve-judge-then-rank retrieval framework structured in cascade funnels. However, it has limitations in accurately depicting the semantic relevance between the query and keyword, and the cumulative funnel losses result in unsatisfactory precision and recall. To address the above issues, this paper proposes a Large Language Model (LLM)-based keyword generation method (LKG) to reach related keywords from the search query in one step. LKG models the query keyword matching as an end-to-end keyword generation task based on the LLM through multi-match prompt tuning. Moreover, it employs the feedback tuning and the prefix tree-based constrained beam search to improve the generation quality and efficiency. Extensive offline experiments and online A/B testing demonstrate the effectiveness and superiority of LKG which is fully deployed in the Baidu sponsored search system bringing significant improvements.|查询关键词匹配通过检索用户查询的语义相关关键词来定位相关广告，在赞助商搜索广告中起着至关重要的作用。传统的技术解决方案采用级联漏斗结构的检索-判决-秩检索框架。然而，它在准确描述查询和关键字之间的语义相关性方面存在局限性，并且累积漏斗损失导致不满意的准确率召回率。针对上述问题，本文提出了一种基于大语言模型(LLM)的关键词生成方法(LKG) ，从搜索查询一步到位地生成相关关键词。LKG 通过多匹配提示调优，将查询关键字匹配建模为基于 LLM 的端到端关键字生成任务。同时，采用反馈调整和基于前缀树的约束波束搜索来提高生成质量和效率。大量的离线实验和在线 A/B 测试证明了 lkG 的有效性和优越性。 LKG 已完全部署在百度赞助的搜索系统中，带来了显著的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=One-step+Reach:+LLM-based+Keyword+Generation+for+Sponsored+Search+Advertising)|0|
|[A Case Study of Enhancing Sparse Retrieval using LLMs](https://doi.org/10.1145/3589335.3651945)|Michael Antonios Kruse Ayoub, Zhan Su, Qiuchi Li||While dense retrieval methods have made significant advancements, sparse retrieval techniques continue to offer advantages in terms of interpretability and generalizability. However, query-document term mismatch in sparse retrieval persists, rendering it infeasible for many practical applications. Recent research has shown that Large Language Models (LLMs) hold relevant information that can enhance sparse retrieval through the application of prompt engineering. In this paper, we build upon this concept to explore various strategies employing LLMs for information retrieval purposes. Specifically, we utilize LLMs to enhance sparse retrieval by query rewriting and query expansion. In query rewriting, the original query is refined by creating several new queries. For query expansion, LLMs are employed to generate extra terms, thereby enriching the original query. We conduct experiments on a range of well-known information retrieval datasets, including MSMARCO-passage, TREC2019, TREC2020, Natural Questions, SCIFACT. The experiments show that LLMs can be beneficial for sparse methods since the added information provided by the LLMs can help diminish the discrepancy between the term frequencies of the important terms in a query and the relevant document. In certain domains, we demonstrate that the effectiveness of LLMs is constrained, indicating that they may not consistently perform optimally, which will be explored in future research.|虽然密集检索方法取得了重大进展，但稀疏检索技术在可解释性和普遍性方面仍然具有优势。然而，在稀疏检索中，查询-文档关键词不匹配问题依然存在，这使得它在许多实际应用中不可行。最近的研究表明，大语言模型(LLM)包含相关信息，可以通过应用快速工程提高稀疏检索。在本文中，我们将以这个概念为基础，探讨利用 LLM 实现信息检索目的的各种策略。具体来说，我们利用 LLM 通过查询重写和查询扩展来增强稀疏检索。在查询重写中，原始查询通过创建几个新查询进行细化。对于查询扩展，使用 LLM 生成额外的术语，从而丰富了原始查询。我们在一系列著名的信息检索数据集上进行实验，包括 MSMARCO 通道，TREC2019，TREC2020，自然问题，SCIFACT。实验表明，LLM 对稀疏方法是有益的，因为 LLM 提供的附加信息有助于减少查询中重要术语的词频与相关文档之间的差异。在某些领域，我们证明了 LLM 的有效性是受限制的，表明它们可能不能始终保持最佳性能，这将在未来的研究中进行探索。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Case+Study+of+Enhancing+Sparse+Retrieval+using+LLMs)|0|
|[Bi-CAT: Improving Robustness of LLM-based Text Rankers to Conditional Distribution Shifts](https://doi.org/10.1145/3589335.3651947)|Sriram Srinivasan, Stephen Sheng, Rishabh Deshmukh, Chen Luo, Yesh Dattatreya, Subhajit Sanyal, S. V. N. Vishwanathan||Retrieval and ranking lie at the heart of several applications like search, question-answering, and recommendations. The use of Large language models (LLMs) such as BERT in these applications have shown promising results in recent times. Recent works on text-based retrievers and rankers show promising results by using bi-encoders (BE) architecture with BERT like LLMs for retrieval and a cross-attention transformer (CAT) architecture BERT or other LLMs for ranking the results retrieved. Although the use of CAT architecture for re-ranking improves ranking metrics, their robustness to data shifts is not guaranteed. In this work we analyze the robustness of CAT-based rankers. Specifically, we show that CAT rankers are sensitive to item distribution shifts conditioned on a query, we refer to this as conditional item distribution shift (CIDS). CIDS naturally occurs in large online search systems as the retrievers keep evolving, making it challenging to consistently train and evaluate rankers with the same item distribution. In this paper, we formally define CIDS and show that while CAT rankers are sensitive to this, BE models are far more robust to CIDS. We propose a simple yet effective approach referred to as BI-CAT which augments BE model outputs with CAT rankers, to significantly improve the robustness of CAT rankers without any drop in in-distribution performance. We conducted a series of experiments on two publicly available ranking datasets and one dataset from a large e-commerce store. Our results on dataset with CIDS demonstrate that the BI-CAT model significantly improves the robustness of CAT rankers by roughly 100-1000bps in F1 without any reduction in in-distribution model performance.|检索和排名是搜索、问答和推荐等应用程序的核心。近年来，诸如 BERT 之类的大语言模型在这些应用中的应用取得了令人鼓舞的成果。基于文本的检索器和排序器的最新研究表明，使用带有 BERT 的双编码器(BE)结构进行检索，使用交叉注意力转换器(CAT)结构 BERT 或其他 LLM 对检索结果进行排序，可以取得令人满意的结果。虽然使用 CAT 体系结构进行重新排序可以改善排序度量，但是它们对数据移位的鲁棒性并不能得到保证。本文分析了基于 CAT 的排序算法的鲁棒性。具体来说，我们表明 CAT 排名对条件查询下的项目分布移位是敏感的，我们称之为条件项目分布移位(CIDS)。CIDS 自然地出现在大型在线搜索系统中，因为检索器不断进化，使得对具有相同项目分布的排名者进行一致的训练和评估变得具有挑战性。在本文中，我们正式定义了 CIDS，并表明 CAT 排名对此敏感，而 BE 模型对 CIDS 的鲁棒性要强得多。我们提出了一种简单而有效的方法，称为 BI-CAT，它将 BE 模型的输出与 CAT 分类器相结合，以显著提高 CAT 分类器的鲁棒性，同时不降低分布式性能。我们对两个公开可用的排名数据集和一个来自大型电子商务商店的数据集进行了一系列的实验。我们在 CIDS 数据集上的结果表明，BI-CAT 模型在 F1中显著提高了 CAT 分类器的鲁棒性，大约提高了100-1000bps，而分布式模型的性能没有任何下降。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bi-CAT:+Improving+Robustness+of+LLM-based+Text+Rankers+to+Conditional+Distribution+Shifts)|0|
|[Deep Learning for Hate Speech Detection: A Personality-based Approach](https://doi.org/10.1145/3589335.3652502)|Kyuhan Lee, Sudha Ram||A crucial element in the combat against hate speech is the development of efficient algorithms for automatically detecting hate speech. Previous research, however, has primarily neglected important insights from the field of psychology literature, particularly the relationship between personality and hate, resulting in suboptimal performance in hate speech detection. To this end, we propose a novel framework for detecting hate speech focusing on people's personality factors reflected in their writing. Our framework has two components: (i) a knowledge distillation model for fully automating the process of personality inference from text and (ii) a personality-based deep learning model for hate speech detection. Our approach is unique in that it incorporates low-level personality factors, which have been largely neglected in prior literature, into automated hate speech detection and proposes novel deep learning components for fully exploiting the intricate relationship between personality and hate (i.e., intermediate personality factors). The evaluation shows that our model significantly outperforms state-of-the-art baselines. Our study paves the way for future research by incorporating personality aspects into the design of automated hate speech detection. In addition, it offers substantial assistance to online social platforms and governmental authorities facing challenges in effectively moderating hate speech.|打击仇恨言论的一个关键因素是开发自动检测仇恨言论的高效算法。然而，以往的研究主要忽视了心理学文献领域的重要见解，特别是人格和仇恨之间的关系，导致了仇恨言论检测的次优表现。为此，我们提出了一个新的框架来检测仇恨言论集中在人们的人格因素反映在他们的写作。我们的框架由两部分组成: (1)一个完全自动化的人格推理过程的知识提取模型和(2)一个基于人格的深度学习模型的仇恨言语检测。我们的方法是独特的，因为它将以前的文献中基本上被忽视的低层次人格因素纳入自动仇恨言论检测中，并提出了新颖的深度学习组件，以充分利用人格和仇恨(即中间人格因素)之间的复杂关系。评估表明，我们的模型明显优于最先进的基线。我们的研究通过将个性方面融入到自动仇恨语音检测的设计中，为未来的研究铺平了道路。此外，它还为在线社交平台和政府当局在有效缓和仇恨言论方面面临的挑战提供大量援助。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Learning+for+Hate+Speech+Detection:+A+Personality-based+Approach)|0|
|[Requirements and Challenges for Query Execution across Decentralized Environments](https://doi.org/10.1145/3589335.3652523)|Ruben Taelman||Due to the economic and societal problems being caused by the Web's growing centralization, there is an increasing interest in de-centralizing data on the Web. This decentralization does however cause a number of technical challenges. If we want to give users in decentralized environments the same level of user experience as they are used to with centralized applications, we need solutions to these challenges. We discuss how query engines can act as layer between applications on the one hand, and decentralized environments on the other hand, Query engines therefore act as an abstraction layer that hides the complexities of decentralized data management for application developers. In this article, we outline the requirements for query engines over decentralized environments. Furthermore, we show how existing approaches meet these requirements, and which challenges remain. As such, this article offers a high-level overview of a roadmap in the query and decentralization research domains.|由于经济和社会问题是由网络的日益集中所引起的，人们对网络上的数据分散化越来越感兴趣。然而，这种地方分权确实带来了一些技术挑战。如果我们希望在分散的环境中为用户提供与集中式应用程序相同水平的用户体验，我们需要解决这些挑战。我们讨论了查询引擎如何在应用程序和分散式环境之间扮演层次的角色，因此，查询引擎就像一个抽象层，为应用程序开发人员隐藏了分散式数据管理的复杂性。在本文中，我们概述了分散环境中查询引擎的需求。此外，我们还展示了现有的方法如何满足这些需求，以及仍然存在哪些挑战。因此，本文对查询和地方分权研究领域的路线图提供了一个高层次的概述。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Requirements+and+Challenges+for+Query+Execution+across+Decentralized+Environments)|0|
|[Diffusion Recommendation with Implicit Sequence Influence](https://doi.org/10.1145/3589335.3651951)|Yong Niu, Xing Xing, Zhichun Jia, Ruidi Liu, Mindong Xin, Jianfu Cui||Sequence recommendation tasks often have performance bottlenecks, mainly reflected in the following two aspects: previous research relied on a single item embedding distribution, resulting in a decrease in overall modeling ability. In addition, the implicit dynamic preferences reflected in user interaction sequences are not distinguished, and the feature representation ability is insufficient. To address these issues, we propose a novel model called Diffusion Recommendation with Implicit Sequence Influence (DiffRIS). Specifically, we establish an implicit feature extraction module, which includes multi-scale CNN and residual LSTM networks that learn local and global features of sequence information, respectively, to explore the length dependence of data features. Subsequently, we use the output of the module as a conditional input for the diffusion model, guiding the denoising process based on historical interactions. Through experiments on two open-source datasets, we find that implicit features of sequences have a positive impact on the diffusion process. The proposed DiffRIS framework performs well compared to multiple baseline models, effectively improving the accuracy of sequential recommendation models. We believe that the proposed DiffRIS can provide some research ideas for diffusion sequence recommendation.|序列推荐任务往往存在性能瓶颈，主要体现在以下两个方面: 以往的研究依赖于单项嵌入分布，导致整体建模能力下降。此外，用户交互序列中反映的隐式动态偏好没有得到区分，特征表示能力不足。为了解决这些问题，我们提出了一种新的模型，称为隐式序列影响的扩散推荐模型。具体来说，我们建立了一个隐式特征提取模块，该模块包括多尺度 CNN 网络和残差 LSTM 网络，分别学习序列信息的局部和全局特征，以探索数据特征的长度依赖性。然后，我们使用模块的输出作为扩散模型的条件输入，指导基于历史交互的去噪过程。通过对两个开源数据集的实验，我们发现序列的隐含特征对扩散过程有积极的影响。与多个基线模型相比，区分 RIS 框架具有良好的性能，有效地提高了序贯推荐模型的准确性。我们相信本文提出的区分红外推荐系统可以为扩散序列推荐提供一些研究思路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diffusion+Recommendation+with+Implicit+Sequence+Influence)|0|
|[Multimodal Conditioned Diffusion Model for Recommendation](https://doi.org/10.1145/3589335.3651956)|Haokai Ma, Yimeng Yang, Lei Meng, Ruobing Xie, Xiangxu Meng||Multimodal recommendation aims at to modeling the feature distributions of items by using their multi-modal information. Prior efforts typically focus on the denoising of the user-item graph with a degree-sensitive strategy, which may not well-handle the users' consistent preference across modalities. More importantly, it has been observed that existing methods may learn ill-posed item embeddings due to their focus on a specific auxiliary optimization task for multimodal representations rather than explicitly modeling them. This paper therefore presents a solution that takes the advantages of the explicit uncertainty injection ability of Diffusion Model (DM) for the modeling and fusion of multi-modal information. Specifically, we propose a novel Multimodal Conditioned Diffusion Model for Recommendation (MCDRec), which tailors DM with two technical modules to model the high-order multimodal knowledge. The first module is multimodal-conditioned representation diffusion (MRD), which integrates pre-extracted multimodal knowledge into the item representation modeling via a tailored DM. This smoothly bridges the insurmountable gap between the multi-modal content features and the collaborative signals. Secondly, with the diffusion-guided graph denoising (DGD) module, MCDRec may effectively denoise the user-item graph by filtering the occasional interactions in user historical behaviors. This is achieved with the power of DM in aligning the users' collaborative preferences with their shared items' content information. Extensive experiments compared to several SOTA baselines on two real-word datasets demonstrate the effectiveness of MCDRec. The specific visualization also reveals the potential of MRD to precisely handling the high-order representation correlations among the user embeddings and the multi-modal heterogeneous representations of items.|多模态推荐的目的是利用项目的多模态信息对项目的特征分布进行建模。先前的工作通常集中在用户项目图的去噪与度敏感的策略，这可能不能很好地处理用户的一致性偏好跨模式。更重要的是，已经观察到，现有的方法可能会学习病态项嵌入，因为他们的重点是一个具体的辅助优化任务的多模态表示，而不是显式建模它们。为此，本文提出了一种利用扩散模型(DM)的显式不确定性注入能力对多模态信息进行建模和融合的解决方案。具体地说，我们提出了一种新的多模态条件扩散推荐模型(MCDRec) ，该模型将 DM 与两个技术模块相结合，对高阶多模态知识进行建模。第一个模块是多模态条件表示扩散(MRD) ，它通过一个定制的 DM 将预提取的多模态知识集成到项目表示模型中。这顺利地弥合了多模态内容特性和协作信号之间不可逾越的鸿沟。其次，利用扩散引导图去噪(DGD)模块，MCDRec 可以通过过滤用户历史行为中的偶然交互来有效地去除用户项图的噪声。这是通过 DM 的功能来实现的，它可以使用户的协作偏好与共享项目的内容信息保持一致。在两个实际数据集上对几个 SOTA 基线进行了大量的实验，证明了 MCDRec 算法的有效性。具体的可视化还揭示了 MRD 在精确处理用户嵌入和多模态异构项表示之间的高阶表示相关性方面的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Conditioned+Diffusion+Model+for+Recommendation)|0|
|[Universal Knowledge Graph Embeddings](https://doi.org/10.1145/3589335.3651978)|N'Dah Jean Kouagou, Caglar Demir, Hamada M. Zahera, Adrian Wilke, Stefan Heindorf, Jiayi Li, AxelCyrille Ngonga Ngomo||A variety of knowledge graph embedding approaches have been developed. Most of them obtain embeddings by learning the structure of the knowledge graph within a link prediction setting. As a result, the embeddings reflect only the semantics of a single knowledge graph, and embeddings for different knowledge graphs are not aligned, e.g., they cannot be used to find similar entities across knowledge graphs via nearest neighbor search. However, knowledge graph embedding applications such as entity disambiguation require a more global representation, i.e., a representation that is valid across multiple sources. We propose to learn universal knowledge graph embeddings from large-scale interlinked knowledge sources. To this end, we fuse large knowledge graphs based on the owl:sameAs relation such that every entity is represented by a unique identity. We instantiate our idea by computing universal embeddings based on DBpedia and Wikidata yielding embeddings for about 180 million entities, 15 thousand relations, and 1.2 billion triples. Moreover, we develop a convenient API to provide embeddings as a service. Experiments on link prediction show that universal knowledge graph embeddings encode better semantics compared to embeddings computed on a single knowledge graph. For reproducibility purposes, we provide our source code and datasets open access at https://github.com/dice-group/Universal_Embeddings|发展了各种知识图嵌入方法。它们中的大多数通过学习链接预测设置中的知识图的结构来获得嵌入。因此，嵌入只能反映单个知识图的语义，不同知识图的嵌入是不一致的，例如，它们不能通过最近邻搜索在知识图中找到相似的实体。然而，知识图嵌入应用程序(如实体消歧)需要一个更全局的表示，即一个跨多个源有效的表示。我们提出了从大规模相互关联的知识源中学习通用知识图嵌入。为此，我们融合了基于猫头鹰的大型知识图: 相同的关系，使每个实体都有一个独特的身份。我们通过基于 DBpedia 和 Wikidata 的通用嵌入计算实例化了我们的想法，产生了大约1.8亿个实体、1.5万个关系和12亿个三元组的嵌入。此外，我们还开发了一个方便的 API 来提供嵌入式服务。链接预测实验表明，通用知识图的嵌入编码比单一知识图的嵌入编码具有更好的语义性能。出于可重复性的目的，我们提供源代码和数据集的开放访问 https://github.com/dice-group/universal_embeddings|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Universal+Knowledge+Graph+Embeddings)|0|
|[Towards Graph Foundation Models for Personalization](https://doi.org/10.1145/3589335.3651980)|Andreas Damianou, Francesco Fabbri, Paul Gigioli, Marco De Nadai, Alice Wang, Enrico Palumbo, Mounia Lalmas||In the realm of personalization, integrating diverse information sources such as consumption signals and content-based representations is becoming increasingly critical to build state-of-the-art solutions. In this regard, two of the biggest trends in research around this subject are Graph Neural Networks (GNNs) and Foundation Models (FMs). While GNNs emerged as a popular solution in industry for powering personalization at scale, FMs have only recently caught attention for their promising performance in personalization tasks like ranking and retrieval. In this paper, we present a graph-based foundation modeling approach tailored to personalization. Central to this approach is a Heterogeneous GNN (HGNN) designed to capture multi-hop content and consumption relationships across a range of recommendable item types. To ensure the generality required from a Foundation Model, we employ a Large Language Model (LLM) text-based featurization of nodes that accommodates all item types, and construct the graph using co-interaction signals, which inherently transcend content specificity. To facilitate practical generalization, we further couple the HGNN with an adaptation mechanism based on a two-tower (2T) architecture, which also operates agnostically to content type. This multi-stage approach ensures high scalability; while the HGNN produces general purpose embeddings, the 2T component models in a continuous space the sheer size of user-item interaction data. Our comprehensive approach has been rigorously tested and proven effective in delivering recommendations across a diverse array of products within a real-world, industrial audio streaming platform.|在个性化领域，集成消费信号和基于内容的表示等多种信息源对于构建最先进的解决方案正变得越来越重要。在这方面，围绕这个主题的两个最大的研究趋势是图形神经网络(GNN)和基础模型(FM)。虽然 GNN 作为一种大规模个性化的解决方案出现在行业中，但是 FM 只是在最近才因其在个性化任务(如排名和检索)中的良好表现而引起人们的注意。在本文中，我们提出了一种基于图的基础建模方法，以适应个性化。这种方法的核心是异构 GNN (HGNN) ，它设计用于捕获跨一系列可推荐的项目类型的多跳内容和消费关系。为了确保基础模型所需的通用性，我们采用了基于大语言模型(LLM)的节点文本特征化，以适应所有项目类型，并使用共同交互信号构建图，这本质上超越了内容特异性。为了便于实际推广，我们进一步将 HGNN 与一个基于双塔(2T)架构的适应机制耦合，该架构也对内容类型不可知地运行。这种多阶段的方法确保了高度的可伸缩性; 当 HGNN 产生通用的嵌入时，2T 组件模型在一个连续的空间中，大量的用户项交互数据。我们的综合方法已经经过严格的测试，并被证明是有效的，可以在一个真实世界的工业音频流媒体平台上提供多种产品的建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Graph+Foundation+Models+for+Personalization)|0|
|[AI for Materials Innovation: Self-Improving Photosensitizer Discovery System via Bayesian Search with First-Principles Simulation](https://doi.org/10.1145/3589334.3649115)|Bin Liu||Artificial intelligence (AI) based self-learning or self-improving material discovery systems will enable next-generation material discovery. Herein, we demonstrate how to combine accurate prediction of material performance via first-principles calculation and Bayesian optimization-based active learning to realize a self-improving discovery system for high-performance photosensitizers (PSs). Through self-improving cycles, such a system can improve the model prediction accuracy (best mean absolute error of 0.090 eV for singlet--triplet spitting) and high-performance PS search ability, realizing efficient discovery of PSs. From a molecular space with more than 7 million molecules, 5357 potential high-performance PSs were discovered. Four PSs were further synthesized to show performance comparable with or superior to commercial ones. This work highlights the potential of active learning in first principle-based materials design, and the discovered structures could boost the development of photosensitization-related applications, which is one of the typical examples of how AI can be used to accelerate materials innovation and facilitate science development in general.|基于人工智能(AI)的自学习或自我改进的材料发现系统将使下一代材料发现成为可能。本文阐述了如何将基于第一性原理计算和基于贝叶斯优化的主动学习相结合，实现高性能光敏剂的自我改进发现系统。该系统通过自我改进循环，提高了模型预测精度(单重态-三重态分裂的最佳平均绝对误差为0.090 eV)和高性能 PS 搜索能力，实现了 PSS 的有效发现。从一个有超过700万个分子的分子空间中，发现了5357个潜在的高性能 PSS。进一步合成了四个 PPS，以显示与商业性能相当或优于商业性能。这项工作强调了主动学习在基于第一原理的材料设计中的潜力，并且发现的结构可以促进光敏化相关应用的发展，这是人工智能如何加速材料创新和促进科学发展的典型例子之一。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI+for+Materials+Innovation:+Self-Improving+Photosensitizer+Discovery+System+via+Bayesian+Search+with+First-Principles+Simulation)|0|
|[Tight Competitive and Variance Analyses of Matching Policies in Gig Platforms](https://doi.org/10.1145/3589334.3645335)|Pan Xu||In this paper, we propose an online-matching-based model to tackle the two fundamental issues, matching and pricing, existing in a wide range of real-world gig platforms, including ride-hailing (matching riders and drivers), crowdsourcing markets (pairing workers and tasks), and online recommendations (offering items to customers). Our model assumes the arriving distributions of dynamic agents (e.g., riders, workers, and buyers) are accessible in advance, and they can change over time, which is referred to as Known Heterogeneous Distributions (KHD). In this paper, we initiate variance analysis for online matching algorithms under KHD. Unlike the popular competitive-ratio (CR) metric, the variance of online algorithms' performance is rarely studied due to inherent technical challenges, though it is well linked to robustness. We focus on two natural parameterized sampling policies, denoted by 𝖠𝖳𝖳(γ) and 𝖲𝖠𝖬𝖯(γ), which appear as foundational bedrock in online algorithm design. We offer rigorous competitive ratio (CR) and variance analyses for both policies. Specifically, we show that 𝖠𝖳𝖳(γ) with γ∈ [0,1/2] achieves a CR of γ and a variance of γ· (1-γ) · B on the total number of matches with B being the total matching capacity. In contrast, 𝖲𝖠𝖬𝖯(γ) with γ∈ [0,1] accomplishes a CR of γ (1-γ) and a variance of γ̅ (1-γ̅)· B with γ̅=min(γ,1/2). All CR and variance analyses are tight and unconditional of any benchmark. As a byproduct, we prove that 𝖠𝖳𝖳(γ=1/2) achieves an optimal CR of 1/2.|在本文中，我们提出了一个基于在线匹配的模型来解决两个基本问题，匹配和定价，存在于广泛的现实世界的零工平台，包括叫车(匹配乘客和司机) ，众包市场(配对工人和任务)和在线推荐(提供项目给客户)。我们的模型假设动态代理人(如乘客、工人和购买者)的到达分布可以提前获得，并且它们可以随时间变化，这被称为已知的异质分布(KHD)。本文对 KHD 下的在线匹配算法进行了方差分析。与流行的竞争比率(CR)度量不同，在线算法的性能方差很少被研究，由于固有的技术挑战，虽然它很好地与健壮性联系在一起。本文重点研究了在线算法设计中的两个自然参数化抽样策略，即 ATT (γ)和 SAMP (γ) ，它们是在线算法设计的基础。我们提供了严格的竞争比率(CR)和方差分析的两个政策。具体地说，我们证明了当总匹配次数为 B 时，具有 γ ∈[0,1/2]的 ATT (γ)达到 γ 的 CR 和 γ · (1-γ) · B 的方差。相比之下，具有 γ ∈[0,1]的 SAMP (γ)实现了 γ (1-γ)的 CR 和 γ (1-γ) · B 与 γ = min (γ，1/2)的方差。所有 CR 和方差分析对任何基准都是严格和无条件的。作为副产品，我们证明了 ATT (γ = 1/2)达到了1/2的最优 CR。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tight+Competitive+and+Variance+Analyses+of+Matching+Policies+in+Gig+Platforms)|0|
|[Ad vs Organic: Revisiting Incentive Compatible Mechanism Design in E-commerce Platforms](https://doi.org/10.1145/3589334.3645638)|Ningyuan Li, Yunxuan Ma, Yang Zhao, Qian Wang, Zhilin Zhang, Chuan Yu, Jian Xu, Bo Zheng, Xiaotie Deng||On typical e-commerce platforms, a product can be displayed to users in two possible forms, as an ad item or an organic item. Usually, ad and organic items are separately selected by the advertising system and recommendation system, and then combined by a content merging mechanism. Although the design of the content merging mechanism has been extensively studied, little attention has been given to a crucial situation where there is an overlap between candidate ad and organic items. Despite its common occurrence, this situation is not correctly handled by almost all existing works, potentially leading to incentive problems for advertisers and the violation of economic constraints. To address these issues, we revisit the design of the content merging mechanism. We introduce a necessary property called form stability, and provide simplification results of the mechanism design problem. Furthermore, we design two simple mechanisms strictly ensuring desired economic properties including incentive compatibility, and demonstrate their guaranteed performance through competitive ratio analysis under certain conditions.|在典型的电子商务平台上，产品可以以两种可能的形式向用户显示，即广告项目或有机项目。通常，广告和有机项目分别由广告系统和推荐系统选择，然后通过内容合并机制进行组合。尽管对内容合并机制的设计进行了广泛的研究，但很少注意到候选广告和有机项目之间存在重叠的关键情况。尽管这种情况经常发生，但几乎所有现有作品都没有正确处理这种情况，这可能导致广告商的激励问题和违反经济制约。为了解决这些问题，我们重新讨论了内容合并机制的设计。我们引入了形式稳定性这一必要性质，并给出了机构设计问题的简化结果。此外，我们设计了两个简单的机制，严格保证理想的经济属性，包括激励相容，并通过在一定条件下的竞争比率分析来证明它们的保证性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ad+vs+Organic:+Revisiting+Incentive+Compatible+Mechanism+Design+in+E-commerce+Platforms)|0|
|[Towards Expansive and Adaptive Hard Negative Mining: Graph Contrastive Learning via Subspace Preserving](https://doi.org/10.1145/3589334.3645327)|Zhezheng Hao, Haonan Xin, Long Wei, Liaoyuan Tang, Rong Wang, Feiping Nie||Graph Neural Networks (GNNs) have emerged as the predominant approach for analyzing graph data on the web and beyond. Contrastive learning (CL), a self-supervised paradigm, not only mitigates reliance on annotations but also has potential in performance. The hard negative sampling strategy that benefits CL in other domains proves ineffective in the context of Graph Contrastive Learning (GCL) due to the message passing mechanism. Embracing the subspace hypothesis in clustering, we propose a method towards expansive and adaptive hard negative mining, referred to as G raph contR astive leA rning via subsP ace prE serving (GRAPE ). Beyond homophily, we argue that false negatives are prevalent over an expansive range and exploring them confers benefits upon GCL. Diverging from existing neighbor-based methods, our method seeks to mine long-range hard negatives throughout subspace, where message passing is conceived as interactions between subspaces. %Empirical investigations back up this strategy. Additionally, our method adaptively scales the hard negatives set through subspace preservation during training. In practice, we develop two schemes to enhance GCL that are pluggable into existing GCL frameworks. The underlying mechanisms are analyzed and the connections to related methods are investigated. Comprehensive experiments demonstrate that our method outperforms across diverse graph datasets and remains competitive across varied application scenarios\footnoteOur code is available at https://github.com/zz-haooo/WWW24-GRAPE. .|图形神经网络(GNN)已经成为分析网络和其他领域图形数据的主要方法。对比学习(CL)是一种自我监督的范式，它不仅减轻了对注释的依赖，而且在性能方面也有潜力。在图形对比学习(GCL)环境中，由于消息传递机制的存在，硬负抽样策略在其他领域对 CL 有利。基于聚类中的子空间假设，本文提出了一种扩展的自适应硬负挖掘方法，即通过子空间 prE 服务(GRAPE)进行 G 图控制的被动挖掘。除了同调，我们认为，假否定是普遍存在的一个广泛的范围和探索他们赋予 GCL 的好处。与现有的基于邻居的方法不同，我们的方法寻求挖掘整个子空间的长程硬负面，其中信息传递被认为是子空间之间的相互作用。经验调查支持这一策略。此外，我们的方法在训练过程中通过子空间保持自适应地扩展硬负片集。在实践中，我们开发了两个可插入现有 GCL 框架的方案来增强 GCL。分析了其机理，并探讨了与相关方法的联系。综合实验表明，我们的方法在不同的图形数据集中表现优异，并且在不同的应用场景中仍然具有竞争力。脚注我们的代码可以在 https://github.com/zz-haooo/www24-grape 获得。.|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Expansive+and+Adaptive+Hard+Negative+Mining:+Graph+Contrastive+Learning+via+Subspace+Preserving)|0|
|[Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction](https://doi.org/10.1145/3589334.3645372)|Minsang Kim, Seung Baek||Learning positional information of nodes in a graph is important for link prediction tasks. We propose a representation of positional information using representative nodes called landmarks. A small number of nodes with high degree centrality are selected as landmarks, which serve as reference points for the nodes' positions. We justify this selection strategy for well-known random graph models and derive closed-form bounds on the average path lengths involving landmarks. In a model for power-law graphs, we prove that landmarks provide asymptotically exact information on inter-node distances. We apply theoretical insights to practical networks and propose Hierarchical Position embedding with Landmarks and Clustering (HPLC). HPLC combines landmark selection and graph clustering, where the graph is partitioned into densely connected clusters in which nodes with the highest degree are selected as landmarks. HPLC leverages the positional information of nodes based on landmarks at various levels of hierarchy such as nodes' distances to landmarks, inter-landmark distances and hierarchical grouping of clusters. Experiments show that HPLC achieves state-of-the-art performances of link prediction on various datasets in terms of HIT@K, MRR, and AUC. The code is available at <https://github.com/kmswin1/HPLC>.|学习图中节点的位置信息是链路预测任务的重要内容。我们提出了一种位置信息的表示使用代表性的节点称为地标。选取少量高度集中的节点作为标志点，作为节点位置的参考点。对于已知的随机图模型，我们证明了这种选择策略的合理性，并推导出了包含地标的平均路径长度的闭合界。在幂律图模型中，我们证明了路标提供节点间距离的渐近精确信息。将理论知识应用于实际网络，提出了基于地标和聚类(HPLC)的层次位置嵌入算法。高效液相色谱将地标选择和图聚类相结合，将图划分为密集连通的聚类，其中选择程度最高的节点作为地标。高效液相色谱利用节点的位置信息的基础上，在不同的层次结构，如节点的距离地标，地标之间的距离和等级分组的集群。实验结果表明，高效液相色谱在 HIT@K、 MRR 和 AUC 方面实现了对不同数据集的链接预测。代码可在 <  https://github.com/kmswin1/hplc 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Position+Embedding+of+Graphs+with+Landmarks+and+Clustering+for+Link+Prediction)|0|
|[Collaborative Metapath Enhanced Corporate Default Risk Assessment on Heterogeneous Graph](https://doi.org/10.1145/3589334.3645402)|Zheng Zhang, Yingsheng Ji, Jiachen Shen, Yushu Chen, Xi Zhang, Guangwen Yang||Default risk assessment for small companies is a tough problem in financial services. Recent efforts utilize advanced Heterogeneous Graph Neural Networks (HGNNs) with metapaths to exploit interactive features in corporate activities for risk analysis. However, few works are proposed for commercial banks. Given a real financial graph, how to detect corporate default risks? We identify two challenges for the task. (1) Massive noisy connections hinder HGNNs to achieve strong results. (2) Multiple semantic connections greatly increase transitive default risk, while existing aggregation schemes do not leverage such connection patterns. In this work, we propose a novel Heterogeneous Graph Co-Attention Network for corporate default risk assessment. Our model takes advantage of collaborative metapaths to distill risky features by a co-attentive aggregation mechanism. First, the local attention score models the importance of neighbors under each metapath by holistic metapath context. Second, the global attention score fuse local attention scores to filter valuable/noisy signals. Then, pairwise importance learning aims to enhance attention scores of multi-metapath neighbors for risky feature distillation. Extensive experiments on large-scale banking datasets demonstrate the effectiveness of our method.|小企业违约风险评估是金融服务领域的一个难题。最近的努力利用先进的异构图神经网络(HGNNs)与元路径开发交互特征的企业活动的风险分析。然而，对于商业银行来说，提出的工作却很少。给定一个真实的财务图表，如何检测企业违约风险？我们确定了这项任务的两个挑战。(1)大量的噪声连接阻碍 HGNN 获得强大的结果。(2)多个语义连接极大地增加了传递缺省风险，而现有的聚合方案没有利用这种连接模式。在本研究中，我们提出一个新颖的异质图形共注意网路来评估公司违约风险。我们的模型利用协作元路径，通过协同关注聚合机制提取风险特征。首先，局部注意得分通过整体元路径上下文模拟每个元路径下邻居的重要性。其次，全局注意得分融合局部注意得分，以过滤有价值/有噪声的信号。其次，成对重要性学习旨在提高多元路径邻居对危险特征提取的注意分数。在大规模银行数据集上的大量实验证明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaborative+Metapath+Enhanced+Corporate+Default+Risk+Assessment+on+Heterogeneous+Graph)|0|
|[Graph Contrastive Learning with Kernel Dependence Maximization for Social Recommendation](https://doi.org/10.1145/3589334.3645412)|Xuelian Ni, Fei Xiong, Yu Zheng, Liang Wang||Contrastive learning (CL) has recently catalyzed a productive avenue of research for recommendation. The efficacy of most CL methods for recommendation may hinge on their capacity to learn representation uniformity by mapping the data onto a hypersphere. Nonetheless, applying contrastive learning to downstream recommendation tasks remains challenging, as existing CL methods encounter difficulties in capturing the nonlinear dependence of representations in high-dimensional space and struggle to learn hierarchical social dependency among users-essential points for modeling user preferences. Moreover, the subtle distinctions between the augmented representations render CL methods sensitive to noise perturbations. Inspired by the Hilbert-Schmidt independence criterion (HSIC), we propose a graph Contrastive Learning model with Kernel Dependence Maximization CL-KDM for social recommendation to address these challenges. Specifically, to explicitly learn the kernel dependence of representations and improve the robustness and generalization of recommendation, we maximize the kernel dependence of augmented representations in kernel Hilbert space by introducing HSIC into the graph contrastive learning. Additionally, to simultaneously extract the hierarchical social dependency across users while preserving underlying structures, we design a hierarchical mutual information maximization module for generating augmented user representations, which are injected into the message passing of a graph neural network to enhance recommendation. Extensive experiments are conducted on three social recommendation datasets, and the results indicate that CL-KDM outperforms various baseline recommendation methods.|对比学习(CL)最近催化了一个富有成效的推荐研究途径。大多数 CL 推荐方法的有效性可能取决于它们通过将数据映射到超球面来学习表示一致性的能力。尽管如此，将对比学习应用于下游推荐任务仍然具有挑战性，因为现有的 CL 方法在捕获高维空间中表示的非线性依赖性方面遇到困难，并且努力学习用户之间的等级社会依赖性-建模用户偏好的关键点。此外，增广表示之间的细微差别使 CL 方法对噪声扰动敏感。受到 Hilbert-Schmidt 独立性标准(HSIC)的启发，我们提出了一个基于核依赖最大化 CL-kDM 的图形对比学习模型，用于社会推荐，以应对这些挑战。特别地，为了明确地学习表示的核依赖性，提高推荐的鲁棒性和泛化性，我们在核 Hilbert 空间中引入 HSIC，最大化增广表示的核依赖性。此外，为了在保留底层结构的同时提取用户之间的层次化社会依赖，我们设计了一个层次化互信息最大化模块来生成增强的用户表示，并将其注入到图神经网络的消息传递中以增强推荐。在三个社会推荐数据集上进行了广泛的实验，结果表明 CL-KDM 的性能优于各种基线推荐方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Contrastive+Learning+with+Kernel+Dependence+Maximization+for+Social+Recommendation)|0|
|[MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs](https://doi.org/10.1145/3589334.3645423)|Xingtong Yu, Chang Zhou, Yuan Fang, Xinming Zhang||Graphs can inherently model interconnected objects on the Web, thereby facilitating a series of Web applications, such as web analyzing and content recommendation. Recently, Graph Neural Networks (GNNs) have emerged as a mainstream technique for graph representation learning. However, their efficacy within an end-to-end supervised framework is significantly tied to the availabilityof task-specific labels. To mitigate labeling costs and enhance robustness in few-shot settings, pre-training on self-supervised tasks has emerged as a promising method, while prompting has been proposed to further narrow the objective gap between pretext and downstream tasks. Although there has been some initial exploration of prompt-based learning on graphs, they primarily leverage a single pretext task, resulting in a limited subset of general knowledge that could be learned from the pre-training data. Hence, in this paper, we propose MultiGPrompt, a novel multi-task pre-training and prompting framework to exploit multiple pretext tasks for more comprehensive pre-trained knowledge. First, in pre-training, we design a set of pretext tokens to synergize multiple pretext tasks. Second, we propose a dual-prompt mechanism consisting of composed and open prompts to leverage task-specific and global pre-training knowledge, to guide downstream tasks in few-shot settings. Finally, we conduct extensive experiments on six public datasets to evaluate and analyze MultiGPrompt.|图形可以内在地对 Web 上相互连接的对象建模，从而方便了一系列 Web 应用程序，例如 Web 分析和内容推荐。近年来，图神经网络已经成为图表示学习的主流技术。然而，它们在端到端监督框架内的有效性与特定任务标签的可用性有着显著的联系。为了降低标记成本和增强在少镜头环境下的鲁棒性，自我监督任务的预训练已经成为一种有前途的方法，同时提出了提示来进一步缩小借口和下游任务之间的目标差距。虽然已经有一些基于图表的及时学习的初步探索，他们主要利用一个单一的借口任务，导致有限的一般知识子集，可以从培训前的数据学习。因此，本文提出了一种新颖的多任务预训练和激励框架 MultiGPrompt，该框架可以利用多任务来获得更全面的预训练知识。首先，在预训中，我们设计一组借口标记来协同多个借口任务。其次，我们提出了一个双提示机制，包括组成和开放的提示，以利用任务特定和全球培训前的知识，以指导下游任务在少镜头设置。最后，我们在六个公共数据集上进行了广泛的实验来评估和分析 MultiGPrompt。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MultiGPrompt+for+Multi-Task+Pre-Training+and+Prompting+on+Graphs)|0|
|[SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding](https://doi.org/10.1145/3589334.3645441)|Ruiyi Yang, Flora D. Salim, Hao Xue||Knowledge graphs (KGs) have been increasingly employed for link prediction and recommendation using real-world datasets. However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios. This often results in suboptimal predictions and recommendations. Although there are effective spatio-temporal inference methods, they face challenges such as scalability with large datasets and inadequate semantic understanding, which impede their performance. To address these limitations, this paper introduces a novel framework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing and exploring spatio-temporal KGs. To integrate spatial and temporal data into KGs, our framework exploited through a new 3-step embedding method. Output embeddings can be used for future temporal sequence prediction and spatial information recommendation, providing valuable insights for various applications such as retail sales forecasting and traffic volume prediction. Our framework offers a simple but comprehensive way to understand the underlying patterns and trends in dynamic KG, thereby enhancing the accuracy of predictions and the relevance of recommendations. This work paves the way for more effective utilization of spatio-temporal data in KGs, with potential impacts across a wide range of sectors.|知识图(KGs)越来越多地被用于利用实际数据集进行链接预测和推荐。然而，现有的方法大多依赖于静态数据，忽视了真实场景的动态性和隐藏的时空属性。这常常导致不理想的预测和建议。虽然有效的时空推理方法已经存在，但是它们面临着大数据集的可扩展性和语义理解不足等问题，这些问题阻碍了它们的性能。针对这些局限性，本文提出了一种构建和探索时空 KG 的新框架——简单时空知识图(SSTKG)。为了将空间数据和时间数据整合到 KG 中，我们的框架采用了一种新的三步嵌入方法。输出嵌入可用于未来的时间序列预测和空间信息推荐，为零售业销售预测和交通量预测等各种应用提供有价值的见解。我们的框架提供了一个简单而全面的方法来了解动态幼稚园的基本模式和趋势，从而提高预测的准确性和建议的相关性。这项工作为幼儿园更有效地利用时空数据铺平了道路，并可能对多个部门产生影响。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SSTKG:+Simple+Spatio-Temporal+Knowledge+Graph+for+Intepretable+and+Versatile+Dynamic+Information+Embedding)|0|
|[Spectral Heterogeneous Graph Convolutions via Positive Noncommutative Polynomials](https://doi.org/10.1145/3589334.3645515)|Mingguo He, Zhewei Wei, Shikun Feng, Zhengjie Huang, Weibin Li, Yu Sun, Dianhai Yu||Heterogeneous Graph Neural Networks (HGNNs) have gained significant popularity in various heterogeneous graph learning tasks. However, most existing HGNNs rely on spatial domain-based methods to aggregate information, i.e., manually selected meta-paths or some heuristic modules, lacking theoretical guarantees. Furthermore, these methods cannot learn arbitrary valid heterogeneous graph filters within the spectral domain, which have limited expressiveness. To tackle these issues, we present a positive spectral heterogeneous graph convolution via positive noncommutative polynomials. Then, using this convolution, we propose PSHGCN, a novel Positive Spectral Heterogeneous Graph Convolutional Network. PSHGCN offers a simple yet effective method for learning valid heterogeneous graph filters. Moreover, we demonstrate the rationale of PSHGCN in the graph optimization framework. We conducted an extensive experimental study to show that PSHGCN can learn diverse heterogeneous graph filters and outperform all baselines on open benchmarks. Notably, PSHGCN exhibits remarkable scalability, efficiently handling large real-world graphs comprising millions of nodes and edges. Our codes are available at https://github.com/ivam-he/PSHGCN.|异构图神经网络在各种异构图学习任务中得到了广泛的应用。然而，现有的 HGNN 大多依赖于基于空间域的方法来聚合信息，即手工选择元路径或一些启发式模块，缺乏理论保证。此外，这些方法不能在谱域内学习任意有效的异构图滤波器，表达能力有限。为了解决这些问题，我们提出了一个正谱异质图卷积通过正的非交换多项式。然后，利用这种卷积，我们提出了一种新的正谱异质图卷积网络 PSHGCN。PSHGCN 为学习有效的异构图过滤器提供了一种简单而有效的方法。此外，本文还在图优化框架中论证了 PSHGCN 算法的基本原理。我们进行了广泛的实验研究表明，PSHGCN 可以学习不同的异构图过滤器和优于所有基线的开放基准。值得注意的是，PSHGCN 具有显著的可伸缩性，能够有效地处理包含数百万个节点和边的大型真实世界图。我们的代码可以在 https://github.com/ivam-he/pshgcn 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spectral+Heterogeneous+Graph+Convolutions+via+Positive+Noncommutative+Polynomials)|0|
|[Densest Subhypergraph: Negative Supermodular Functions and Strongly Localized Methods](https://doi.org/10.1145/3589334.3645624)|Yufan Huang, David F. Gleich, Nate Veldt||Dense subgraph discovery is a fundamental primitive in graph and hypergraph analysis which among other applications has been used for real-time story detection on social media and improving access to data stores of social networking systems. We present several contributions for localized densest subgraph discovery, which seeks dense subgraphs located nearby given seed sets of nodes. We first introduce a generalization of a recent anchored densest subgraph problem, extending this previous objective to hypergraphs and also adding a tunable locality parameter that controls the extent to which the output set overlaps with seed nodes. Our primary technical contribution is to prove when it is possible to obtain a strongly-local algorithm for solving this problem, meaning that the runtime depends only on the size of the input set. We provide a strongly-local algorithm that applies whenever the locality parameter is not too small, and show via counterexample why strongly-local algorithms are impossible below a certain threshold. Along the way to proving our results for localized densest subgraph discovery, we also provide several advances in solving global dense subgraph discovery objectives. This includes the first strongly polynomial time algorithm for the densest supermodular set problem and a flow-based exact algorithm for a heavy and dense subgraph discovery problem in graphs with arbitrary node weights. We demonstrate our algorithms on several web-based data analysis tasks.|密集子图发现是图和超图分析中的一个基本原理，除其他应用外，它还被用于社交媒体上的实时故事检测和改善对社交网络系统数据存储的访问。我们提出了局部密集子图发现的几个贡献，它寻找位于给定种子集附近的密集子图。我们首先引入了一个最近抛锚的最密集子图问题的推广，将这个先前的目标扩展到超图，并且添加了一个可调的局部性参数来控制输出集与种子节点重叠的程度。我们的主要技术贡献是证明何时可以获得解决这个问题的强局部算法，这意味着运行时仅取决于输入集的大小。我们提供了一个强局部算法，适用于局部参数不太小的情况，并通过反例说明为什么强局部算法不可能低于一定的阈值。在证明局部密集子图发现结果的过程中，我们还提供了解决全局密集子图发现目标的一些进展。这包括求解最稠密上模集问题的第一个强多项式时间算法和求解任意节点权图中的稠密子图发现问题的基于流的精确算法。我们在几个基于 Web 的数据分析任务中演示了我们的算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Densest+Subhypergraph:+Negative+Supermodular+Functions+and+Strongly+Localized+Methods)|0|
|[Towards Deeper Understanding of PPR-based Embedding Approaches: A Topological Perspective](https://doi.org/10.1145/3589334.3645663)|Xingyi Zhang, Zixuan Weng, Sibo Wang||Node embedding learns low-dimensional vectors for nodes in the graph. Recent state-of-the-art embedding approaches take Personalized PageRank (PPR) as the proximity measure and factorize the PPR matrix or its adaptation to generate embeddings. However, little previous work analyzes what information is encoded by these approaches, and how the information correlates with their superb performance in downstream tasks. In this work, we first show that state-of-the-art embedding approaches that factorize a PPR-related matrix can be unified into a closed-form framework. Then, we study whether the embeddings generated by this strategy can be inverted to better recover the graph topology information than random-walk based embeddings. To achieve this, we propose two methods for recovering graph topology via PPR-based embeddings, including the analytical method and the optimization method. Extensive experimental results demonstrate that the embeddings generated by factorizing a PPR-related matrix maintain more topological information, such as common edges and community structures, than that generated by random walks, paving a new way to systematically comprehend why PPR-based node embedding approaches outperform random walk-based alternatives in various downstream tasks. To the best of our knowledge, this is the first work that focuses on the interpretability of PPR-based node embedding approaches.|节点嵌入学习图中节点的低维向量。最新的嵌入方法采用个性化 PageRank (PPR)作为邻近度量，对 PPR 矩阵或其适应性进行因子分解以生成嵌入。然而，以前的工作很少分析这些方法编码的信息，以及这些信息如何与它们在下游任务中的卓越性能相关联。在这项工作中，我们首先表明，国家的最先进的嵌入方法，分解一个 PPR 相关的矩阵可以统一成一个封闭的形式框架。然后，研究该策略生成的嵌入能否比基于随机游走的嵌入更好地恢复图的拓扑信息。为此，我们提出了两种基于 PPR 嵌入的图拓扑恢复方法，包括解析法和优化法。大量的实验结果表明，PPR 相关矩阵分解产生的嵌入比随机游走产生的嵌入保持了更多的拓扑信息，如共同边和社区结构，为系统地理解为什么 PPR 节点嵌入方法在各种下游任务中优于随机游走方法提供了新的途径。据我们所知，这是第一个重点研究基于 PPR 的节点嵌入方法的可解释性的工作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Deeper+Understanding+of+PPR-based+Embedding+Approaches:+A+Topological+Perspective)|0|
|[Globally Interpretable Graph Learning via Distribution Matching](https://doi.org/10.1145/3589334.3645674)|Yi Nian, Yurui Chang, Wei Jin, Lu Lin||Graph neural networks (GNNs) have emerged as a powerful model to capture critical graph patterns. Instead of treating them as black boxes in an end-to-end fashion, attempts are arising to explain the model behavior. Existing works mainly focus on local interpretation to reveal the discriminative pattern for each individual instance, which however cannot directly reflect the high-level model behavior across instances. To gain global insights, we aim to answer an important question that is not yet well studied: how to provide a global interpretation for the graph learning procedure? We formulate this problem as globally interpretable graph learning, which targets on distilling high-level and human-intelligible patterns that dominate the learning procedure, such that training on this pattern can recover a similar model. As a start, we propose a novel model fidelity metric, tailored for evaluating the fidelity of the resulting model trained on interpretations. Our preliminary analysis shows that interpretative patterns generated by existing global methods fail to recover the model training procedure. Thus, we further propose our solution, Graph Distribution Matching (GDM), which synthesizes interpretive graphs by matching the distribution of the original and interpretive graphs in the GNN's feature space as its training proceeds, thus capturing the most informative patterns the model learns during training. Extensive experiments on graph classification datasets demonstrate multiple advantages of the proposed method, including high model fidelity, predictive accuracy and time efficiency, as well as the ability to reveal class-relevant structure.|图形神经网络(GNN)已经成为一种捕获关键图形模式的强大模型。不是以端到端的方式将它们视为黑盒，而是尝试解释模型行为。现有的工作主要集中在局部解释上，以揭示每个实例的区分模式，但不能直接反映跨实例的高层模型行为。为了获得全局性的见解，我们的目标是回答一个尚未得到很好研究的重要问题: 如何为图形学习过程提供一个全局性的解释？我们把这个问题表述为全局可解释的图学习，其目标是提取主导学习过程的高层次和人类可理解的模式，这样对这种模式的训练可以恢复类似的模型。作为一个开始，我们提出了一个新的模型保真度量，专为评估保真度的结果模型训练的解释。我们的初步分析表明，现有的全局方法产生的解释模式不能恢复模型训练过程。因此，我们进一步提出了我们的解决方案，图分布匹配(GDM) ，它通过在 GNN 的特征空间中匹配原始图和解释图的分布来合成解释图，从而捕获模型在训练过程中学习到的最具信息量的模式。在图形分类数据集上的大量实验表明，该方法具有模型保真度高、预测精度高、时间效率高、能够揭示类相关结构等优点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Globally+Interpretable+Graph+Learning+via+Distribution+Matching)|0|
|[Retention Depolarization in Recommender System](https://doi.org/10.1145/3589334.3645485)|Xiaoying Zhang, Hongning Wang, Yang Liu||Repeated risk minimization is a popular choice in real-world recommender systems driving their recommendation algorithms to adapt to user preferences and trends. However, numerous studies have shown that it exacerbates retention disparities among user groups, resulting in polarization within the user population. Given the primary objective of improving long-term user engagement in most industrial recommender systems and the significant commercial benefits from a diverse user population, enforcing retention fairness across user population is therefore crucial. Nonetheless, this goal is highly challenging due to the unknown dynamics of user retention (e.g., when a user would abandon the system) and the simultaneous aim to maximize the experience of every user. In this paper, we propose ReFair, the first computational framework that continuously improves recommendation algorithms while ensuring long-term retention fairness in the entire user population. ReFair alternates between environment learning (i.e., estimate the user retention dynamics) and fairness constrained policy improvement with respect to the estimated environment, while effectively handling uncertainties in the estimation. Our solution provides strong theoretical guarantees for long-term recommendation performance and retention fairness violation. Empirical experiments on two real-world recommendation datasets also demonstrate its effectiveness in realizing these two goals.|在现实推荐系统中，重复风险最小化是一种流行的选择，它驱动推荐算法来适应用户的偏好和趋势。然而，许多研究表明，它加剧了用户群体之间的保留差异，导致用户群体内的两极分化。鉴于改善大多数工业推荐系统的长期用户参与的主要目标以及来自不同用户群体的重大商业利益，因此，在所有用户群体中实行保留公平是至关重要的。尽管如此，由于用户保持的未知动态(例如，当用户将放弃系统)和同时目标最大化每个用户的体验，这个目标是非常具有挑战性的。在本文中，我们提出了第一个计算框架 ReFair，它不断改进推荐算法，同时确保整个用户群的长期保留公平性。ReFair 在环境学习(即，估计用户保留动态)和公平约束的策略改进之间轮流对估计的环境进行改进，同时有效地处理估计中的不确定性。我们的解决方案为长期推荐性能和违反保留公平性提供了强有力的理论保证。在两个真实世界的推荐数据集上的实验也证明了该方法实现这两个目标的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retention+Depolarization+in+Recommender+System)|0|
|[Uncovering the Hidden Data Costs of Mobile YouTube Video Ads](https://doi.org/10.1145/3589334.3645496)|Emaan Atique, Saad Sher Alam, Harris Ahmad, Ihsan Ayyub Qazi, Zafar Ayyub Qazi||Popular video streaming platforms attract a large number of global marketers who use the platform to advertise their services. While benefiting platforms and advertisers, users are burdened with the costs of advertisements. Users not only pay for these ads with their invested time and personal information, but also through a substantial amount of data translating into direct financial cost. The financial cost becomes even more pronounced in developing countries, where the cost of mobile broadband can be disproportionately high relative to average income levels. In this paper, we perform the first independent and empirical analysis of the data costs of mobile video ads on YouTube, the most popular video platform, from the users' perspective. To do so, we collect and analyze a data set of over 46,000 YouTube video ads. We find that streaming video ads have multiplelatent andavoidable sources of data wastage, which can lead to excessive data consumption by users. We also conduct an affordability analysis to quantify the overall impact of data wastage and reveal the specific data costs per country associated with these losses. Our findings highlight the need for video platform providers, such as YouTube, to minimize data wastage linked to ads, to make their services more affordable and inclusive.|流行的视频流媒体平台吸引了大量的全球营销人员，他们利用这个平台为自己的服务做广告。在惠及平台和广告商的同时，用户还要承担广告费用。用户不仅用他们投入的时间和个人信息来支付这些广告，而且还通过大量的数据转化为直接的财务成本。在发展中国家，移动宽带的费用相对于平均收入水平可能高得不成比例，因此财务成本变得更加突出。本文首先从用户的角度对最流行的视频平台 YouTube 上的移动视频广告的数据成本进行了独立的实证分析。为此，我们收集并分析了超过46000个 YouTube 视频广告的数据集。我们发现流媒体视频广告具有多重且可避免的数据浪费来源，这会导致用户过度消耗数据。我们还进行了负担能力分析，以量化数据浪费的总体影响，并揭示与这些损失相关的每个国家的具体数据成本。我们的研究结果强调视频平台提供商，如 YouTube，需要尽量减少与广告相关的数据浪费，使他们的服务更便宜，更具包容性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncovering+the+Hidden+Data+Costs+of+Mobile+YouTube+Video+Ads)|0|
|[Perceptions in Pixels: Analyzing Perceived Gender and Skin Tone in Real-world Image Search Results](https://doi.org/10.1145/3589334.3645666)|Jeffrey L. Gleason, Avijit Ghosh, Ronald E. Robertson, Christo Wilson||The results returned by image search engines have the power to shape peoples' perceptions about social groups. Existing work on image search engines leverages hand-selected queries for occupations like "doctor" and "engineer" to quantify racial and gender bias in search results. We complement this work by analyzing peoples' real-world image search queries and measuring the distributions of perceived gender, skin tone, and age in their results. We collect 54,070 unique image search queries and analyze 1,481 open-ended people queries (i.e. not queries for named entities) from a representative sample of 643 US residents. For each query, we analyze the top 15 results returned on both Google and Bing Images. Analysis of real-world image search queries produces multiple insights. First, less than 5% of unique queries are open-ended people queries. Second, fashion queries are, by far, the most common category of open-ended people queries, accounting for over 30% of the total. Third, the modal skin tone on the Monk Skin Tone scale is two out of ten (the second lightest) for images from both search engines. Finally, we observe a bias against older people: eleven of our top fifteen query categories have a median age that is lower than the median age in the US.|图片搜索引擎返回的结果有能力影响人们对社会群体的看法。现有的图片搜索引擎利用手工选择的关于“医生”和“工程师”等职业的查询来量化搜索结果中的种族和性别偏见。我们通过分析人们在现实世界中的图像搜索查询以及测量结果中感知的性别、肤色和年龄的分布来补充这项工作。我们从643名美国居民的代表性样本中收集了54,070个独特的图像搜索查询，并分析了1,481个开放式人员查询(即不包括命名实体的查询)。对于每个查询，我们分析 Google 和 Bing 图片上返回的前15个结果。对真实世界图像搜索查询的分析产生了多种见解。首先，不到5% 的唯一查询是开放式人员查询。其次，到目前为止，时尚查询是最常见的开放式人员查询类别，占总数的30% 以上。第三，Monk Skin Tone 刻度上的模态肤色是两个搜索引擎图像的十分之二(第二轻)。最后，我们观察到一种对老年人的偏见: 在我们的前十一个查询类别中，有十五个的中位年龄低于美国的中位年龄。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Perceptions+in+Pixels:+Analyzing+Perceived+Gender+and+Skin+Tone+in+Real-world+Image+Search+Results)|0|
|[Reconciling the Accuracy-Diversity Trade-off in Recommendations](https://doi.org/10.1145/3589334.3645625)|Kenny Peng, Manish Raghavan, Emma Pierson, Jon M. Kleinberg, Nikhil Garg||In recommendation settings, there is an apparent trade-off between the goals of accuracy (to recommend items a user is most likely to want) and diversity (to recommend items representing a range of categories). As such, real-world recommender systems often explicitly incorporate diversity separately from accuracy. This approach, however, leaves a basic question unanswered: Why is there a trade-off in the first place? We show how the trade-off can be explained via a user's consumption constraints -- users typically only consume a few of the items they are recommended. In a stylized model we introduce, objectives that account for this constraint induce diverse recommendations, while objectives that do not account for this constraint induce homogeneous recommendations. This suggests that accuracy and diversity appear misaligned because standard accuracy metrics do not consider consumption constraints. Our model yields precise and interpretable characterizations of diversity in different settings, giving practical insights into the design of diverse recommendations.|在推荐设置中，在准确性目标(推荐用户最可能想要的项目)和多样性目标(推荐代表一系列类别的项目)之间存在明显的权衡。因此，现实世界中的推荐系统通常明确地将多样性与准确性分开。然而，这种方法留下了一个基本问题没有得到解答: 为什么首先要进行权衡？我们展示了如何通过用户的消费约束来解释这种权衡——用户通常只消费推荐的几个项目。在我们引入的程式化模型中，解释这种约束的目标会产生不同的建议，而不解释这种约束的目标会产生同质的建议。这表明准确性和多样性似乎不一致，因为标准的准确性指标没有考虑消费约束。我们的模型产生了不同环境下多样性的精确和可解释的特征，为不同建议的设计提供了实用的见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reconciling+the+Accuracy-Diversity+Trade-off+in+Recommendations)|0|
|[MileCut: A Multi-view Truncation Framework for Legal Case Retrieval](https://doi.org/10.1145/3589334.3645349)|Fuda Ye, Shuangyin Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MileCut:+A+Multi-view+Truncation+Framework+for+Legal+Case+Retrieval)|0|
|[Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks](https://doi.org/10.1145/3589334.3645363)|Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, TatSeng Chua||Making the content generated by Large Language Model (LLM), accurate, credible and traceable is crucial, especially in complex knowledge-intensive tasks that require multi-step reasoning and each step needs knowledge to solve. Retrieval-augmented generation is good potential to solve this problem. However, where and how to introduce Information Retrieval (IR) to LLM is a big challenge. Previous work has the problems that wrong knowledge retrieved by IR misleads the LLM and interaction between IR and LLM breaks the reasoning chain of LLM. This paper proposes a novel framework named Search-in-the-Chain (SearChain) for the interaction between LLM and IR to solve the challenges. First, LLM generates the reasoning chain named Chain-of-Query (CoQ) where each node consists of an IR-oriented query-answer pair. Second, IR verifies the answer of each node of CoQ. It corrects the answer that is not consistent with the retrieved information when IR gives high confidence, which improves the credibility. Third, LLM can indicate its missing knowledge in CoQ and rely on IR to provide this knowledge to LLM. These operations improve the accuracy in terms of reasoning and knowledge. Finally, SearChain generates the reasoning process and marks references to supporting documents for each reasoning step, which improves traceability. Interaction with IR in SearChain forms a novel reasoning path based on a tree, which enables LLM to dynamically modify the direction of reasoning. Experiments show that SearChain outperforms state-of-the-art baselines on complex knowledge-intensive tasks including multi-hop Q&A, slot filling, fact checking, and long-form Q&A.|使大语言模型(LLM)生成的内容准确、可信、可追溯是关键，特别是在需要多步推理、每一步都需要知识求解的复杂知识密集型任务中。提取增强生成技术是解决这一问题的有力工具。然而，在何处以及如何向 LLM 引入信息检索是一个巨大的挑战。以往的研究存在的问题是，由 IR 检索出的错误知识误导了 LLM，而且 IR 与 LLM 之间的相互作用破坏了 LLM 的推理链。本文提出了一个新的框架，称为搜索在链(搜索链)之间的交互 LLM 和信息检索，以解决这一挑战。首先，LLM 生成名为查询链(Chain-of-Query，CoQ)的推理链，其中每个节点由一个面向 IR 的查询-应答对组成。其次，IR 验证 CoQ 的每个节点的答案。当信息检索的可信度较高时，它可以纠正与检索到的信息不一致的答案，从而提高了信息的可信度。第三，LLM 可以在 CoQ 中表示其缺失的知识，并依靠 IR 向 LLM 提供这些知识。这些操作提高了推理和知识的准确性。最后，SearChain 生成推理过程，并在每个推理步骤中标记对支持文档的引用，从而提高了可追溯性。在 SearChain 中，与 IR 的交互形成了一种新的基于树的推理路径，使 LLM 能够动态地修改推理的方向。实验表明，SearChain 在复杂的知识密集型任务(包括多跳问答、插槽填充、事实检查和长形式问答)中的表现优于最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Search-in-the-Chain:+Interactively+Enhancing+Large+Language+Models+with+Search+for+Knowledge-intensive+Tasks)|0|
|[Scalable and Effective Generative Information Retrieval](https://doi.org/10.1145/3589334.3645477)|Hansi Zeng, Chen Luo, Bowen Jin, Sheikh Muhammad Sarwar, Tianxin Wei, Hamed Zamani||Recent research has shown that transformer networks can be used as differentiable search indexes by representing each document as a sequence of document ID tokens. These generative retrieval models cast the retrieval problem to a document ID generation problem for each query. Despite their elegant design, existing generative retrieval models only perform well on artificially-constructed and small-scale collections. This paper represents an important milestone in generative retrieval research by showing that generative retrieval models can be trained to perform effectively on large-scale standard retrieval benchmarks. In more detail, we propose RIPOR- an optimization framework for generative retrieval that is designed based on two often-overlooked fundamental design considerations. First, RIPOR introduces a novel prefix-oriented ranking optimization algorithm for accurate estimation of relevance score during sequential document ID generation. Second, RIPOR constructs document IDs based on the relevance associations between queries and documents. Evaluation on MSMARCO and TREC Deep Learning Track reveals that RIPOR surpasses state-of-the-art generative retrieval models by a large margin (e.g., 30.5% MRR improvements on MS MARCO Dev Set).|最近的研究表明，通过将每个文档表示为一系列文档 ID 令牌，变压器网络可以作为可微搜索索引。这些生成检索模型将检索问题转换为每个查询的文档 ID 生成问题。尽管其设计优雅，现有的生成检索模型只能在人工构建的小规模集合上表现良好。本文通过对生成检索模型的训练，使其能够在大规模的标准检索基准上有效地执行，从而标志着生成检索研究的一个重要里程碑。更详细地说，我们提出了 RIPOR-一个生成检索的优化框架，它是基于两个经常被忽视的基本设计考虑而设计的。首先，RIPOR 引入了一种新的面向前缀的排序优化算法，用于准确估计序列文档 ID 生成过程中的相关度得分。其次，RIPOR 基于查询和文档之间的相关性关联构造文档 ID。对 MSMARCO 和 TREC 深度学习跟踪的评估显示，RIPOR 大大超过了最先进的生成检索模型(例如，MS MARCO Dev Set 的30.5% MRR 改进)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+and+Effective+Generative+Information+Retrieval)|0|
|[Metacognitive Retrieval-Augmented Large Language Models](https://doi.org/10.1145/3589334.3645481)|Yujia Zhou, Zheng Liu, Jiajie Jin, JianYun Nie, Zhicheng Dou||Retrieval-augmented generation have become central in natural language processing due to their efficacy in generating factual content. While traditional methods employ single-time retrieval, more recent approaches have shifted towards multi-time retrieval for multi-hop reasoning tasks. However, these strategies are bound by predefined reasoning steps, potentially leading to inaccuracies in response generation. This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition. Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes. By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective reasoning abilities. Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes them. Empirical evaluations show that MetaRAG significantly outperforms existing methods.|提取增强生成由于其生成事实内容的功效，已成为自然语言处理的核心。传统的检索方法采用单次检索，而最近的检索方法已经转向多跳推理任务的多次检索。然而，这些策略受到预先定义的推理步骤的约束，可能导致响应生成的不准确性。本文介绍了一种将检索增强生成过程与元认知相结合的方法 MetaRAG。元认知从认知心理学的角度出发，允许一个实体对其认知过程进行自我反思和批判性评价。通过集成这一点，MetaRAG 使模型能够监视、评估和规划其响应策略，增强其内省推理能力。通过三步元认知调节管道，该模型可以识别初始认知反应中的不足并加以修正。经验性评估表明，MetaRAG 的性能明显优于现有方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Metacognitive+Retrieval-Augmented+Large+Language+Models)|0|
|[Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy](https://doi.org/10.1145/3589334.3645512)|SeongKu Kang, Shivam Agarwal, Bowen Jin, Dongha Lee, Hwanjo Yu, Jiawei Han||Document retrieval has greatly benefited from the advancements of large-scale pre-trained language models (PLMs). However, their effectiveness is often limited in theme-specific applications for specialized areas or industries, due to unique terminologies, incomplete contexts of user queries, and specialized search intents. To capture the theme-specific information and improve retrieval, we propose to use a corpus topical taxonomy, which outlines the latent topic structure of the corpus while reflecting user-interested aspects. We introduce ToTER (Topical Taxonomy Enhanced Retrieval) framework, which identifies the central topics of queries and documents with the guidance of the taxonomy, and exploits their topical relatedness to supplement missing contexts. As a plug-and-play framework, ToTER can be flexibly employed to enhance various PLM-based retrievers. Through extensive quantitative, ablative, and exploratory experiments on two real-world datasets, we ascertain the benefits of using topical taxonomy for retrieval in theme-specific applications and demonstrate the effectiveness of ToTER.|大规模预先培训语言模型(plm)的发展极大地促进了文献检索的发展。然而，由于独特的术语、用户查询的不完整上下文以及专门的搜索意图，它们在专门领域或行业的特定主题应用程序中的有效性往往受到限制。为了获取特定主题的信息，提高检索效率，我们提出了一种语料库主题分类法，它在反映用户感兴趣的方面的同时，勾勒出语料库的潜在主题结构。本文介绍了 ToTER (Topical Taxonomy Advanced Retrieval，主题分类增强检索)框架，该框架在分类学的指导下识别查询和文档的中心主题，并利用它们的主题相关性来补充缺失的上下文。作为一个即插即用的框架，ToTER 可以灵活地用于增强各种基于 PLM 的检索器。通过对两个实际数据集进行广泛的定量、消融和探索性实验，我们确定了在特定主题应用中使用主题分类法进行检索的好处，并证明了 ToTER 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Retrieval+in+Theme-specific+Applications+using+a+Corpus+Topical+Taxonomy)|0|
|[(In)Security of File Uploads in Node.js](https://doi.org/10.1145/3589334.3645342)|Harun Oz, Abbas Acar, Ahmet Aris, Güliz Seray Tuncay, Amin Kharraz, A. Selcuk Uluagac||File upload is a critical feature incorporated by a myriad of web applications in an effort to enable users to share and manage their files conveniently. It has been used in many useful services such as file-sharing and social media. While file upload is an essential component of web applications, the lack of rigorous checks on the file name, type, and content of the uploaded files can result in security issues, often referred to as Unrestricted File Upload (UFU). In this study, we analyze the (in)security of popular file upload libraries and real-world applications in the Node.js ecosystem. To automate our analysis, we propose and implement NodeSEC- a tool designed to analyze file upload insecurities in Node.js applications and libraries. NodeSEC generates unique payloads and thoroughly evaluates the application's file upload security against 13 distinct UFU-type attacks. Utilizing NodeSEC, we analyze the most popular file upload libraries and real-world applications in the Node.js ecosystem. Our analysis results reveal that some real-world web applications are vulnerable to UFU attacks and disclose serious security bugs in file upload libraries. As of this writing, we received 19 CVEs and two US-CERT cases for the security issues that we reported. Our findings provide strong evidence that dynamic features of Node.js applications introduce security shortcomings and that web developers should be cautious when implementing file upload features in their applications. Finally, combining our responsible disclosure experience and root cause analysis, we identified the main causes of significant security weaknesses in file uploads in Node.js.|文件上传是许多网络应用程序的一个重要特性，它使用户能够方便地共享和管理他们的文件。它已经被用于许多有用的服务，如文件共享和社会媒体。虽然文件上传是 Web 应用程序的重要组成部分，但是缺乏对上传文件的文件名、类型和内容的严格检查可能会导致安全问题，通常被称为无限制文件上传(UFU)。在本研究中，我们分析了 Node.js 生态系统中流行的文件上传库和实际应用程序的(In)安全性。为了使我们的分析自动化，我们提出并实现了 NodeSEC ——一个用于分析 Node.js 应用程序和库中文件上传不安全性的工具。NodeSEC 生成唯一的有效负载，并对应用程序的文件上传安全性进行全面评估，以抵御13种不同的 UFU 类型的攻击。利用 NodeSEC，我们分析了 Node.js 生态系统中最流行的文件上传库和实际应用程序。我们的分析结果表明，一些现实世界的网络应用程序是脆弱的 UFU 攻击，并揭示了严重的安全漏洞的文件上传库。在撰写本文时，我们收到了19个 CVE 和两个 US-CERT 案例，用于我们报告的安全问题。我们的研究结果有力地证明了 Node.js 应用程序的动态特性引入了安全缺陷，并且 web 开发人员在应用程序中实现文件上传特性时应该谨慎。最后，结合我们负责任的披露经验和根本原因分析，我们找出了 Node.js 文件上传中存在重大安全缺陷的主要原因。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=(In)Security+of+File+Uploads+in+Node.js)|0|
|[IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion](https://doi.org/10.1145/3589334.3645361)|Jiapu Wang, Zheng Cui, Boyue Wang, Shirui Pan, Junbin Gao, Baocai Yin, Wen Gao||Temporal Knowledge Graphs (TKGs) incorporate a temporal dimension, allowing for a precise capture of the evolution of knowledge and reflecting the dynamic nature of the real world. Typically, TKGs contain complex geometric structures, with various geometric structures interwoven. However, existing Temporal Knowledge Graph Completion (TKGC) methods either model TKGs in a single space or neglect the heterogeneity of different curvature spaces, thus constraining their capacity to capture these intricate geometric structures. In this paper, we propose a novel Integrating Multi-curvature shared and specific Embedding (IME) model for TKGC tasks. Concretely, IME models TKGs into multi-curvature spaces, including hyperspherical, hyperbolic, and Euclidean spaces. Subsequently, IME incorporates two key properties, namely space-shared property and space-specific property. The space-shared property facilitates the learning of commonalities across different curvature spaces and alleviates the spatial gap caused by the heterogeneous nature of multi-curvature spaces, while the space-specific property captures characteristic features. Meanwhile, IME proposes an Adjustable Multi-curvature Pooling (AMP) approach to effectively retain important information. Furthermore, IME innovatively designs similarity, difference, and structure loss functions to attain the stated objective. Experimental results clearly demonstrate the superior performance of IME over existing state-of-the-art TKGC models.|时间知识图(TKGs)包含了一个时间维度，允许精确地捕捉知识的演变，并反映真实世界的动态性质。通常，TKG 包含复杂的几何结构，各种几何结构交织在一起。然而，现有的时态知识图完成(TKGC)方法要么在单个空间中对 TKG 进行建模，要么忽略了不同曲率空间的异质性，从而限制了它们捕获这些复杂几何结构的能力。针对 TKGC 任务，提出了一种新的集成多曲率共享和特定嵌入(IME)模型。具体而言，IME 将 TKG 模型分解为多曲率空间，包括超球面空间、双曲空间和欧氏空间。随后，IME 合并了两个关键属性，即空间共享属性和空间特定属性。空间共享属性有利于不同曲率空间之间的共性学习，缓解了多曲率空间异质性所造成的空间差距，而空间特定属性则捕捉特征。同时，IME 提出了一种可调多曲率池(AMP)方法来有效地保留重要信息。此外，IME 创新地设计了相似性、差异性和结构损失函数来达到既定的目标。实验结果清楚地表明，IME 的性能优于现有的最先进的 TKGC 模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IME:+Integrating+Multi-curvature+Shared+and+Specific+Embedding+for+Temporal+Knowledge+Graph+Completion)|0|
|[Poisoning Attack on Federated Knowledge Graph Embedding](https://doi.org/10.1145/3589334.3645422)|Enyuan Zhou, Song Guo, Zhixiu Ma, Zicong Hong, Tao Guo, Peiran Dong||Federated Knowledge Graph Embedding (FKGE) is an emerging collaborative learning technique for deriving expressive representations (i.e., embeddings) from client-maintained distributed knowledge graphs (KGs). However, poisoning attacks in FKGE, which lead to biased decisions by downstream applications, remain unexplored. This paper is the first work to systematize the risks of FKGE poisoning attacks, from which we develop a novel framework for poisoning attacks that force the victim client to predict specific false facts. Unlike centralized KGEs, FKGE maintains KGs locally, making direct injection of poisoned data challenging. Instead, attackers must create poisoned data without access to the victim's KG and inject it indirectly through FKGE aggregation. Specifically, to create poisoned data, the attacker first infers the targeted relations in the victim's local KG via a new KG component inference attack. Then, to accurately mislead the victim's embeddings via aggregation, the attacker locally trains a shadow model using the poisoned data and uses an optimized dynamic poisoning scheme to adjust the model and generate progressive poisoned updates. Our experimental results demonstrate the attack's effectiveness, achieving a remarkable success rate on various KGE models (e.g., 100% on TransE with WN18RR) while keeping the original task's performance nearly unchanged.|联邦知识图嵌入(FKGE)是一种新兴的合作学习技术，用于从客户维护的分布式知识图(KGs)中获得表达式表示(即嵌入)。然而，在 FKGE 的中毒攻击，导致下游应用程序的偏见决策，仍然没有探索。本文首次对 FKGE 中毒攻击的风险进行了系统化分析，并在此基础上提出了一个新的中毒攻击框架，迫使受害者客户预测特定的虚假事实。与集中的 KGE 不同，FKGE 在本地维护 KGs，这使得直接注入有毒数据变得很困难。相反，攻击者必须在无法访问受害者的 KG 的情况下创建有毒数据，并通过 FKGE 聚合间接注入这些数据。具体来说，为了创建有毒数据，攻击者首先通过一个新的 KG 组件推断攻击来推断受害者的本地 KG 中的目标关系。然后，为了通过聚合准确地误导受害者的嵌入，攻击者使用中毒数据在本地训练一个阴影模型，并使用一个优化的动态中毒方案来调整模型和生成渐进的中毒更新。我们的实验结果证明了该攻击的有效性，在不同的 KGE 模型(例如，100% 的 TransE 和 WN18RR)上获得了显著的成功率，同时保持了原始任务的性能几乎不变。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Poisoning+Attack+on+Federated+Knowledge+Graph+Embedding)|0|
|[ReliK: A Reliability Measure for Knowledge Graph Embeddings](https://doi.org/10.1145/3589334.3645430)|Maximilian K. Egger, Wenyue Ma, Davide Mottin, Panagiotis Karras, Ilaria Bordino, Francesco Gullo, Aris Anagnostopoulos||Can we assess a priori how well a knowledge graph embedding will perform on a specific downstream task and in a specific part of the knowledge graph? Knowledge graph embeddings (KGEs) represent entities (e.g., "da Vinci," "Mona Lisa") and relationships (e.g., "painted") of a knowledge graph (KG) as vectors. KGEs are generated by optimizing an embedding score, which assesses whether a triple (e.g., "da Vinci," "painted," "Mona Lisa") exists in the graph. KGEs have been proven effective in a variety of web-related downstream tasks, including, for instance, predicting relationships among entities. However, the problem of anticipating the performance of a given KGE in a certain downstream task and locally to a specific individual triple, has not been tackled so far. In this paper, we fill this gap with ReliK, a Reliability measure for KGEs. ReliK relies solely on KGE embedding scores, is task- and KGE-agnostic, and requires no further KGE training. As such, it is particularly appealing for semantic web applications which call for testing multiple KGE methods on various parts of the KG and on each individual downstream task. Through extensive experiments, we attest that ReliK correlates well with both common downstream tasks, such as tail or relation prediction and triple classification, as well as advanced downstream tasks, such as rule mining and question answering, while preserving locality.|我们能否先验地评估知识图表嵌入在特定的下游任务和知识图表的特定部分中的表现如何？知识图嵌入(KGEs)表示知识图的实体(如“达芬奇”、“蒙娜丽莎”)和关系(如“画”)作为向量。KGE 是通过优化嵌入分数生成的，嵌入分数评估图中是否存在三元组(例如，“达芬奇”、“绘画”、“蒙娜丽莎”)。KGEs 已被证明在各种与网络相关的下游任务中是有效的，包括，例如，预测实体之间的关系。然而，预测某一知识专长在某一下游任务中的表现以及局部地区对某一特定个体三重性的表现这一问题迄今尚未得到解决。在本文中，我们填补这一空白与 ReliK，一个可靠性测度的 KGE。ReliK 完全依赖于 KGE 嵌入分数，是任务和 KGE 不可知的，不需要进一步的 KGE 培训。因此，它对语义 Web 应用程序特别有吸引力，这些应用程序需要在 KG 的各个部分和每个单独的下游任务上测试多个 KGE 方法。通过大量的实验，我们证明了 ReliK 与常见的下游任务(如尾部或关系预测和三重分类)以及高级的下游任务(如规则挖掘和问题回答)都有很好的相关性，同时保留了局部性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReliK:+A+Reliability+Measure+for+Knowledge+Graph+Embeddings)|0|
|[A Method for Assessing Inference Patterns Captured by Embedding Models in Knowledge Graphs](https://doi.org/10.1145/3589334.3645505)|Narayanan Asuri Krishnan, Carlos R. Rivero||Various methods embed knowledge graphs with the goal of predicting missing edges. Inference patterns are the logical relationships that occur in a graph. To make proper predictions, models trained by embedding methods must capture inference patterns. There are several theoretical analyses studying pattern-capturing capabilities. Unfortunately, these analyses are challenging and many embedding methods remain unstudied. Also, they do not quantify how accurately a pattern is captured in real-world datasets. Existing empirical studies have studied a small subset of simple inference patterns, and the analysis methods used have varied depending on the models evaluated. In this paper, we present a model-agnostic method to empirically quantify how patterns are captured by trained embedding models. We collect the most plausible predictions to form a new graph, and use it to globally assess pattern-capturing capabilities. For a given pattern, we study positive and negative evidence, i.e., edges that the pattern deems correct and incorrect based on the partial completeness assumption. As far as we know, it is the first time negative evidence is analyzed. Our experiments show that several models effectively capture the positive evidence of inference patterns. However, the performance is poor for negative evidence, which entails that models fail to learn the partial completeness assumption. We also identify new inference patterns not studied before. Surprisingly, models generally achieve better performance in these new patterns that we introduce.|各种方法嵌入知识图，目的是预测缺失边缘。推理模式是图中出现的逻辑关系。为了做出正确的预测，通过嵌入方法训练的模型必须捕获推理模式。关于模式捕获能力的理论分析主要有以下几种。不幸的是，这些分析是具有挑战性的，许多嵌入方法仍然没有研究。此外，它们不能量化在真实世界的数据集中捕获模式的准确程度。现有的实证研究已经研究了一小部分简单的推理模式，所使用的分析方法根据所评估的模型而有所不同。在本文中，我们提出了一个模型无关的方法，以经验量化如何捕获模式训练嵌入模型。我们收集最合理的预测，形成一个新的图表，并使用它来全球评估模式捕获能力。对于一个给定的模式，我们研究积极和消极的证据，即，边缘模式认为正确和不正确的基础上的部分完备性假设。据我们所知，这是第一次对负面证据进行分析。我们的实验表明，几个模型有效地捕获了推理模式的积极证据。然而，否定证据的性能较差，这意味着模型不能学习部分完备性假设。我们还发现了以前没有研究过的新的推理模式。令人惊讶的是，模型通常在我们引入的这些新模式中获得更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Method+for+Assessing+Inference+Patterns+Captured+by+Embedding+Models+in+Knowledge+Graphs)|0|
|[Fact Embedding through Diffusion Model for Knowledge Graph Completion](https://doi.org/10.1145/3589334.3645451)|Xiao Long, Liansheng Zhuang, Aodi Li, Houqiang Li, Shafei Wang||Knowledge graph embedding (KGE) is an efficient and scalable method for knowledge graph completion tasks. Existing KGE models typically map entities and relations into a unified continuous vector space and define a score function to capture the connectivity patterns among the elements (entities and relations) of facts. The score on a fact measures its plausibility in a knowledge graph (KG). However, since the connectivity patterns are very complex in a real knowledge graph, it is difficult to define an explicit and efficient score function to capture them, which also limits their performance. This paper argues that plausible facts in a knowledge graph come from a distribution in the low-dimensional fact space. Inspired by this insight, this paper proposes a novel framework called Fact Embedding through Diffusion Model (FDM) to address the knowledge graph completion task. Instead of defining a score function to measure the plausibility of facts in a knowledge graph, this framework directly learns the distribution of plausible facts from the known knowledge graph and casts the entity prediction task into the conditional fact generation task. Specifically, we concatenate the elements embedding in a fact as a whole and take it as input. Then, we introduce a Conditional Fact Denoiser to learn the reverse denoising diffusion process and generate the target fact embedding from noised data. Extensive experiments demonstrate that FDM significantly outperforms existing state-of-the-art methods in three benchmark datasets.|知识图嵌入(KGE)是一种高效、可扩展的知识图完成任务方法。现有的 KGE 模型通常将实体和关系映射到一个统一的连续向量空间中，并定义一个评分函数来捕获事实要素(实体和关系)之间的连通性模式。事实的得分在知识图(KG)中衡量事实的合理性。然而，由于连通性模式在真实的知识图中是非常复杂的，因此很难定义一个明确有效的评分函数来捕获它们，这也限制了它们的性能。本文认为，知识图中的似然事实来自于低维事实空间中的分布。受此启发，本文提出了一种基于扩散模型的事实嵌入(FDM)框架来解决知识图的完成任务。该框架不需要定义一个评分函数来度量知识图中事实的合理性，而是直接从已知的知识图中学习合理事实的分布，并将实体预测任务转化为条件事实生成任务。具体来说，我们将嵌入在事实中的元素作为一个整体连接起来，并将其作为输入。然后引入条件事实去噪器学习反向去噪扩散过程，从含噪数据中生成目标事实。大量的实验表明，FDM 在三个基准数据集中的性能明显优于现有的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fact+Embedding+through+Diffusion+Model+for+Knowledge+Graph+Completion)|0|
|[HaSa: Hardness and Structure-Aware Contrastive Knowledge Graph Embedding](https://doi.org/10.1145/3589334.3645564)|Honggen Zhang, June Zhang, Igor Molybog||We consider a contrastive learning approach to knowledge graph embedding (KGE) via InfoNCE. For KGE, efficient learning relies on augmenting the training data with negative triples. However, most KGE works overlook the bias from generating the negative triples-false negative triples (factual triples missing from the knowledge graph). We argue that the generation of high-quality (i.e., hard) negative triples might lead to an increase in false negative triples. To mitigate the impact of false negative triples during the generation of hard negative triples, we propose the Hardness and Structure-aware (\textbf{HaSa}) contrastive KGE method, which alleviates the effect of false negative triples while generating the hard negative triples. Experiments show that HaSa improves the performance of InfoNCE-based KGE approaches and achieves state-of-the-art results in several metrics for WN18RR datasets and competitive results for FB15k-237 datasets compared to both classic and pre-trained LM-based KGE methods.|提出了一种基于 InfoNCE 的知识图嵌入对比学习方法。对于 KGE，有效的学习依赖于用负三元组增加训练数据。然而，大多数 KGE 作品忽略了产生负三元组的偏差-假负三元组(知识图中缺失的事实三元组)。我们认为产生高质量(例如，硬)负三元组可能导致增加假负三元组。为了减轻硬负三元组生成过程中假负三元组的影响，提出了硬度和结构感知(textbf { HaSa })对比 KGE 方法，该方法在生成硬负三元组的同时，减轻了假负三元组的影响。实验表明，与经典的和预先训练的基于 LM 的 KGE 方法相比，HaSa 提高了基于 InfoNCE 的 KGE 方法的性能，在 WN18RR 数据集的几个度量指标和 FB15k-237数据集的竞争结果方面取得了最先进的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HaSa:+Hardness+and+Structure-Aware+Contrastive+Knowledge+Graph+Embedding)|0|
|[Bridging the Space Gap: Unifying Geometry Knowledge Graph Embedding with Optimal Transport](https://doi.org/10.1145/3589334.3645565)|Yuhan Liu, Zelin Cao, Xing Gao, Ji Zhang, Rui Yan||Knowledge Graph Embedding (KGE) is a critical field aiming to transform the elements of knowledge graphs (KGs) into continuous spaces, offering great potential for structured data representation. In contemporary KGE research, the utilization of either hyperbolic or Euclidean space for knowledge graph Embedding is a common practice. However, knowledge graphs encompass diverse geometric data structures, including chains and hierarchies, whose hybrid nature exceeds the capacity of a single embedding space to capture effectively. This paper introduces a novel and highly effective approach called Unified Geometry Knowledge Graph Embedding (UniGE) to address the challenge of representing diverse geometric data in KGs. UniGE stands out as a novel KGE method that seamlessly integrates KGE in both Euclidean and hyperbolic geometric spaces. We introduce an embedding alignment method and fusion strategy, which harnesses optimal transport techniques and the Wasserstein barycenter method. Furthermore, we offer a comprehensive theoretical analysis to substantiate the superiority of our approach, as evident from a more robust error bound. To substantiate the strength of UniGE, we conducted comprehensive experiments on three benchmark datasets. The results consistently demonstrate that UniGE outperforms state-of-the-art methods, aligning with the conclusions drawn from our theoretical analysis.|知识图嵌入(KGE)是将知识图元素转化为连续空间的关键领域，为结构化数据表示提供了巨大的潜力。在当代 KGE 研究中，利用双曲空间或欧氏空间嵌入知识图是一种常见的实践。然而，知识图包含不同的几何数据结构，包括链和层次结构，其混合性质超过了单一嵌入空间有效捕获的能力。本文介绍了一种新颖而高效的方法——统一几何知识图嵌入(UniGE) ，以解决幼儿园中表示不同几何数据的难题。UniGE 是一种新颖的 KGE 方法，它将 KGE 无缝地集成到欧几里得空间和双曲几何空间中。本文介绍了一种融合最优传输技术和 Wasserstein 重心法的嵌入对准方法和融合策略。此外，我们提供了一个全面的理论分析，以证明我们的方法的优越性，从一个更强大的误差界。为了验证 UniGE 的强度，我们在三个基准数据集上进行了全面的实验。结果一致表明，UniGE 优于国家的最先进的方法，从我们的理论分析得出的结论一致。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bridging+the+Space+Gap:+Unifying+Geometry+Knowledge+Graph+Embedding+with+Optimal+Transport)|0|
|[Query Optimization for Ontology-Mediated Query Answering](https://doi.org/10.1145/3589334.3645567)|Wafaa El Husseini, Cheikh Brahim El Vaigh, François Goasdoué, Hélène Jaudoin||Ontology-mediated query answering (OMQA) consists in asking database queries on knowledge bases (KBs); a KB is a set of facts called the KB's database, which is described by domain knowledge called the KB's ontology. A widely-investigated OMQA technique is FO-rewriting: every query asked on a KB is reformulated w.r.t. the KB's ontology, so that its answers are computed by the relational evaluation of the query reformulation on the KB's database. Crucially, because FO-rewriting compiles the domain knowledge relevant to queries into their reformulations, query reformulations may be complex and their optimization is the crux of efficiency. We devise a novel optimization framework for a large set of OMQA settings that enjoy FO-rewriting: conjunctive queries, i.e., the core select-project-join queries, asked on KBs expressed using datalog+/-, description logics, existential rules, OWL, or RDFS. We optimize the query reformulations produced by state-of-the-art FO-rewriting algorithms by computing rapidly, with the help of a KB's database summary, simpler (contained) queries with the same answers that can be evaluated faster by RDBMSs. We show on a well-established OMQA benchmark that time performance is significantly improved by our optimization framework in general, up to three orders of magnitude.|本体介导的查询回答(OMQA)包括在知识库(KB)上询问数据库查询; 知识库是一组称为知识库数据库的事实，由称为知识库本体的领域知识描述。一种被广泛研究的 OMQA 技术是 FO 重写(FO-rewrite) : 对知识库询问的每个查询都被重新表述为 W.r.t. 知识库的本体，因此它的答案是通过对知识库数据库上的查询重新表述的关系计算得到的。关键是，由于 FO 重写将与查询相关的领域知识编译成查询重写，因此查询重写可能比较复杂，其优化是提高查询效率的关键。我们为享受 FO 重写的大量 OMQA 设置设计了一个新颖的优化框架: 合取查询，即核心 select-project-join 查询，使用 datalog +/-、描述逻辑、存在规则、 OWL 或 RDFS 表示。我们通过快速计算优化了由最先进的 FO 重写算法产生的查询重写，在知识库数据库摘要的帮助下，更简单(包含)的查询具有相同的答案，可以由 RDBMS 更快地进行评估。我们展示了一个完善的 OMQA 基准，通过我们的优化框架，时间性能得到了显著改善，数量级最高可达3。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query+Optimization+for+Ontology-Mediated+Query+Answering)|0|
|[Query2GMM: Learning Representation with Gaussian Mixture Model for Reasoning over Knowledge Graphs](https://doi.org/10.1145/3589334.3645569)|Yuhan Wu, Yuanyuan Xu, Wenjie Zhang, Xiwei Xu, Ying Zhang||Logical query answering over Knowledge Graphs (KGs) is a fundamental yet complex task. A promising approach to achieve this is to embed queries and entities jointly into the same embedding space. Research along this line suggests that using multi-modal distribution to represent answer entities is more suitable than uni-modal distribution, as a single query may contain multiple disjoint answer subsets due to the compositional nature of multi-hop queries and the varying latent semantics of relations. However, existing methods based on multi-modal distribution roughly represent each subset without capturing its accurate cardinality, or even degenerate into uni-modal distribution learning during the reasoning process due to the lack of an effective similarity measure. To better model queries with diversified answers, we propose Query2GMM for answering logical queries over knowledge graphs. In Query2GMM, we present the GMM embedding to represent each query using a univariate Gaussian Mixture Model (GMM). Each subset of a query is encoded by its cardinality, semantic center and dispersion degree, allowing for precise representation of multiple subsets. Then we design specific neural networks for each operator to handle the inherent complexity that comes with multi-modal distribution while alleviating the cascading errors. Last, we design a new similarity measure to assess the relationships between an entity and a query's multi-answer subsets, enabling effective multi-modal distribution learning for reasoning. Comprehensive experimental results show that Query2GMM outperforms the best competitor by an absolute average of 6.35%.|基于知识图的逻辑查询回答是一项基本而又复杂的任务。实现这一点的一个有希望的方法是将查询和实体联合嵌入到相同的嵌入空间中。沿着这条线的研究表明，使用多模态分布来表示答案实体比单模态分布更合适，因为由于多跳查询的组合性质和关系的不同潜在语义，单个查询可能包含多个不相交的答案子集。然而，现有的基于多模态分布的方法在推理过程中，由于缺乏有效的相似性度量，往往不能准确地表示每个子集，甚至退化为单模态分布学习。为了更好地建立具有多样化答案的查询模型，我们提出了基于知识图的查询模型 Query2GMM。在 Query2GMM 中，我们使用单变量高斯混合模型(GMM)来表示每个查询。查询的每个子集由其基数、语义中心和分散度编码，允许对多个子集进行精确表示。然后针对每个算子设计特定的神经网络，以处理多模态分布所带来的固有复杂性，同时减小级联误差。最后，我们设计了一个新的相似性度量来评估一个实体和查询的多答案子集之间的关系，使有效的多模态分布学习推理。综合实验结果表明，Query2GMM 的性能绝对平均优于最佳竞争对手6.35% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query2GMM:+Learning+Representation+with+Gaussian+Mixture+Model+for+Reasoning+over+Knowledge+Graphs)|0|
|[Enhancing Complex Question Answering over Knowledge Graphs through Evidence Pattern Retrieval](https://doi.org/10.1145/3589334.3645563)|Wentao Ding, Jinmao Li, Liangchuan Luo, Yuzhong Qu||Information retrieval (IR) methods for KGQA consist of two stages: subgraph extraction and answer reasoning. We argue current subgraph extraction methods underestimate the importance of structural dependencies among evidence facts. We propose Evidence Pattern Retrieval (EPR) to explicitly model the structural dependencies during subgraph extraction. We implement EPR by indexing the atomic adjacency pattern of resource pairs. Given a question, we perform dense retrieval to obtain atomic patterns formed by resource pairs. We then enumerate their combinations to construct candidate evidence patterns. These evidence patterns are scored using a neural model, and the best one is selected to extract a subgraph for downstream answer reasoning. Experimental results demonstrate that the EPR-based approach has significantly improved the F1 scores of IR-KGQA methods by over 10 points on ComplexWebQuestions and achieves competitive performance on WebQuestionsSP.|信息检索分析方法包括子图提取和答案推理两个阶段。我们认为现有的子图提取方法低估了证据事实之间结构相关性的重要性。我们提出证据模式检索(EPR)来显式建模子图提取过程中的结构依赖。我们通过索引资源对的原子邻接模式来实现 EPR。给定一个问题，我们执行密集检索来获得由资源对形成的原子模式。然后我们列举它们的组合来构造候选证据模式。这些证据模式评分使用神经模型，并选择最好的一个提取子图的下游答案推理。实验结果表明，基于 EPR 的方法显著提高了 IR-KGQA 方法在 ComplexWebquestions 上的 F1得分，提高了10分以上，并且在 Webquestions-SP 上取得了较好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Complex+Question+Answering+over+Knowledge+Graphs+through+Evidence+Pattern+Retrieval)|0|
|[Author Name Disambiguation via Paper Association Refinement and Compositional Contrastive Embedding](https://doi.org/10.1145/3589334.3645596)|Dezhi Liu, Richong Zhang, Junfan Chen, Xinyue Chen||Author name disambiguation (AND) is an essential task for online academic retrieval systems. Recent models adopt representation learning in the author's name disambiguation. Despite achieving remarkable success, these methods may be limited in two aspects. First, the heuristically constructed paper association graphs used for representation learning contain uncertainties that may cause negative supervision. Second, existing algorithms, such as binary cross-entropy loss, used to train representation learning models may not produce sufficiently high-quality representations for AND. To tackle the above problems, we propose an association refining and compositional contrasting (ARCC) framework for AND tasks. ARCC first adopts an iterative graph structure refinement process to dynamically reduce the uncertainties in paper graphs. Then, a compositional contrastive learning method is proposed to encourage learning more discriminative representations for AND. Empirical studies on two benchmark datasets suggest that ARCC is effective for AND and outperforms the state-of-the-art models.|作者姓名歧义消除(AND)是在线学术检索系统的一项重要任务。最近的模型采用表征学习来消除作者姓名的歧义。尽管这些方法取得了显著的成功，但它们可能在两个方面受到限制。首先，用于表示学习的启发式构造的纸张关联图含有可能导致负监督的不确定性。其次，现有的用于训练表示学习模型的算法，如二进制交叉熵损失算法，可能不能为 AND 提供足够高质量的表示。针对上述问题，提出了一种基于关联细化和组合对比(ARCC)的 AND 任务框架。ARCC 首先采用一种迭代的图结构细化过程来动态地降低纸质图中的不确定性。然后，提出了一种组合对比学习方法，以鼓励学习更多的与的区分表示。对两个基准数据集的实证研究表明，ARCC 对 AND 模型是有效的，并且优于最先进的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Author+Name+Disambiguation+via+Paper+Association+Refinement+and+Compositional+Contrastive+Embedding)|0|
|[Dual Box Embeddings for the Description Logic EL++](https://doi.org/10.1145/3589334.3645648)|Mathias Jackermeier, Jiaoyan Chen, Ian Horrocks||OWL ontologies, whose formal semantics are rooted in Description Logic (DL), have been widely used for knowledge representation. Similar to Knowledge Graphs (KGs), ontologies are often incomplete, and maintaining and constructing them has proved challenging. While classical deductive reasoning algorithms use the precise formal semantics of an ontology to predict missing facts, recent years have witnessed growing interest in inductive reasoning techniques that can derive probable facts from an ontology. Similar to KGs, a promising approach is to learn ontology embeddings in a latent vector space, while additionally ensuring they adhere to the semantics of the underlying DL. While a variety of approaches have been proposed, current ontology embedding methods suffer from several shortcomings, especially that they all fail to faithfully model one-to-many, many-to-one, and many-to-many relations and role inclusion axioms. To address this problem and improve ontology completion performance, we propose a novel ontology embedding method named Box^2EL for the DL EL++, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles), and models inter-concept relationships using a bumping mechanism. We theoretically prove the soundness of Box^2EL and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets on the tasks of subsumption prediction, role assertion prediction, and approximating deductive reasoning.|OWL 本体形式语义学植根于描述逻辑(Description Logic，DL) ，已被广泛用于知识表示。与知识图(KGs)类似，本体通常是不完整的，维护和构建它们被证明是具有挑战性的。虽然经典的演绎推理算法使用本体的精确形式语义学来预测缺失的事实，但近年来人们对从本体中获取可能事实的归纳推理技术的兴趣日益增长。与 KG 类似，一种有前途的方法是学习潜在向量空间中的本体嵌入，同时确保它们遵循底层 DL 的语义。虽然提出了各种方法，但是现有的本体嵌入方法都存在一些缺陷，特别是它们都不能忠实地建模一对多、多对一和多对多的关系和角色包含公理。为了解决这个问题，提高本体的完成性能，我们提出了一种新的本体嵌入方法，称为盒子 ^ 2EL 的 DL EL + + ，表示概念和角色的盒子(即，轴对齐的超矩形) ，并使用碰撞机制模型的概念间关系。我们从理论上证明了 Box ^ 2EL 的可靠性，并进行了广泛的实验评估，在包容预测、角色断言预测和近似演绎推理任务的各种数据集上取得了最先进的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual+Box+Embeddings+for+the+Description+Logic+EL++)|0|
|[Jointly Canonicalizing and Linking Open Knowledge Base via Unified Embedding Learning](https://doi.org/10.1145/3589334.3645700)|Wei Shen, Binhan Yang, Yinan Liu||Recent years have witnessed increasing attention on the semantic knowledge integration between curated knowledge bases (CKBs) and open knowledge bases (OKBs), which is non-trivial due to the intrinsically heterogeneous features involved in CKBs and OKBs. OKB canonicalization and OKB linking are regarded as two vital tasks to achieve the knowledge integration. Although these two tasks are inherently complementary with each other, previous studies just solve them separately or via superficial interaction. To address this issue, we propose CLUE, a novel framework that jointly encodes the OKB and CKB into a unified embedding space, to tackle OKB canonicalization and OKB linking simultaneously and make them benefit each other reciprocally. We design an expectation-maximization (EM) based approach to iteratively refine the unified embedding space via performing seed generation and embedding refinement alternately, by leveraging the deep interaction between OKB canonicalization and OKB linking. Curriculum learning is employed to yield high-quality canonicalization seeds and linking seeds adaptively, according to two elaborately designed metrics (i.e., a margin-based linking metric and an entropy-based cluster metric). A thorough experimental study over two public benchmark data sets demonstrates that our proposed CLUE consistently outperforms state-of-the-art baselines for the task of OKB canonicalization (resp. OKB linking) in terms of average F1 (resp. accuracy).|近年来，策划知识库(CKB)和开放知识库(OKB)之间的语义知识集成受到越来越多的关注。OKB 规范化和 OKB 链接是实现知识集成的两个重要任务。虽然这两个任务在本质上是相辅相成的，但以往的研究只是单独或通过表面的互动来解决这两个问题。为了解决这个问题，我们提出了一种新的框架 CLUE，将 OKB 和 CKB 联合编码成一个统一的嵌入空间，同时处理 OKB 的规范化和 OKB 的链接，使它们相互受益。设计了一种基于期望最大化(EM)的方法，利用 OKB 规范化和 OKB 链接之间的深层交互，通过交替进行种子生成和嵌入求精，迭代求精统一嵌入空间。课程学习是根据两个精心设计的度量(即基于边际的连接度量和基于熵的聚类度量)来产生高质量的规范化种子和自适应地连接种子。通过对两个公共基准数据集的深入实验研究表明，我们提出的 CLUE 在 OKB 规范化任务中始终优于最先进的基准。OKB 链接)的平均 F1(相当于。准确性)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Jointly+Canonicalizing+and+Linking+Open+Knowledge+Base+via+Unified+Embedding+Learning)|0|
|[Efficient Exact and Approximate Betweenness Centrality Computation for Temporal Graphs](https://doi.org/10.1145/3589334.3645438)|Tianming Zhang, Yunjun Gao, Jie Zhao, Lu Chen, Lu Jin, Zhengyi Yang, Bin Cao, Jing Fan||Betweenness centrality of a vertex in a graph evaluates how often the vertex occurs in the shortest paths. It is a widely used metric of vertex importance in graph analytics. While betweenness centrality on static graphs has been extensively investigated, many real-world graphs are time-varying and modeled as temporal graphs. Examples include social networks and telecommunication networks, where a relationship between two vertices occurs at a specific time. Hence, in this paper, we target efficient methods for temporal betweenness centrality computation. We firstly propose an exact algorithm with the new notion of time instance graph, based on which, we derive a temporal dependency accumulation theory for iterative computation. To reduce the size of the time instance graph and improve the efficiency, we propose an additional optimization, which compresses the time instance graph with equivalent vertices and edges, and extends the dependency theory to the compressed graph. Since it is theoretically complex to compute temporal betweenness centrality, we further devise a probabilistically guaranteed approximate method to handle massive temporal graphs. Extensive experimental results on real-world temporal networks demonstrate the superior performance of the proposed methods. In particular, our exact and approximate methods outperform the state-of-the-art methods by up to two and five orders of magnitude, respectively.|图中顶点的间距中心度计算顶点在最短路径中出现的频率。它是图分析中广泛使用的顶点重要性度量。静态图的中心性已经得到了广泛的研究，但是现实世界中的许多图都是时变的，并且被建模为时态图。例子包括社会网络和电信网络，其中两个顶点之间的关系发生在特定的时间。因此，本文针对时间间隔中心性计算的有效方法进行了研究。本文首先提出了一种新的时间实例图概念的精确算法，在此基础上推导了迭代计算的时间依赖积累理论。为了减少时间实例图的大小和提高效率，我们提出了一种额外的优化方法，即用等效的顶点和边压缩时间实例图，并将依赖理论扩展到压缩后的图。由于计算时间间隔中心性在理论上比较复杂，我们进一步提出了一种处理海量时间图的概率保证近似方法。在实际时间网络上的大量实验结果表明，该方法具有良好的性能。特别是，我们的精确方法和近似方法比最先进的方法分别高出2数量级和5倍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Exact+and+Approximate+Betweenness+Centrality+Computation+for+Temporal+Graphs)|0|
|[PaCEr: Network Embedding From Positional to Structural](https://doi.org/10.1145/3589334.3645516)|Yuchen Yan, Yongyi Hu, Qinghai Zhou, Lihui Liu, Zhichen Zeng, Yuzhong Chen, Menghai Pan, Huiyuan Chen, Mahashweta Das, Hanghang Tong||Network embedding plays an important role in a variety of social network applications. Existing network embedding methods, explicitly or implicitly, can be categorized into positional embedding (PE) methods or structural embedding (SE) methods. Specifically, PE methods encode the positional information and obtain similar embeddings for adjacent/close nodes, while SE methods aim to learn identical representations for nodes with the same local structural patterns, even if the two nodes are far away from each other. The disparate designs of the two types of methods lead to an apparent dilemma in that no embedding could perfectly capture both positional and structural information. In this paper, we seek to demystify the underlying relationship between positional embedding and structural embedding. We first point out that the positional embedding can produce the structural embedding with simple transformations, while the opposite direction cannot hold. Based on this finding, a novel network embedding model PACER is proposed, which optimizes the positional embedding with the help of random walk with restart (RWR) proximity distribution, and such positional embedding is then used to seamlessly obtain the structural embedding with simple transformations. Furthermore, two variants of PACER are proposed to handle node classification task on homophilic and heterophilic graphs. Extensive experiments on 17 datasets show that PACER achieves comparable or better performance than the state-of-the-arts.|网络嵌入在各种社交网络应用中发挥着重要作用。现有的网络嵌入方法，无论是显式的还是隐式的，都可以分为位置嵌入(PE)方法和结构嵌入(SE)方法。具体来说，PE 方法对相邻/相近节点的位置信息进行编码并获得相似的嵌入，而 SE 方法的目标是学习具有相同局部结构模式的节点的相同表示，即使这两个节点相距很远。这两种方法的不同设计导致了一个明显的困境，即没有任何嵌入能够完美地捕获位置和结构信息。本文试图揭示位置嵌入和结构嵌入之间的内在联系。首先指出位置嵌入可以通过简单的变换产生结构嵌入，而相反的方向不能保持。在此基础上，提出了一种新的网络嵌入模型 PACER，该模型利用重启随机游走(RWR)邻近分布对位置嵌入进行优化，然后利用位置嵌入通过简单的变换实现结构嵌入的无缝化。在此基础上，提出了两种 PACER 变体来处理同亲图和异亲图上的节点分类任务。对17个数据集进行的大量实验表明，PACER 的性能达到了与现有技术相当或更好的水平。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PaCEr:+Network+Embedding+From+Positional+to+Structural)|0|
|[Link Recommendation to Augment Influence Diffusion with Provable Guarantees](https://doi.org/10.1145/3589334.3645521)|Xiaolong Chen, Yifan Song, Jing Tang||Link recommendation systems in online social networks (OSNs), such as Facebook's “People You May Know”, Twitter's “Who to Follow”, and Instagram's “Suggested Accounts”, facilitate the formation of new connections among users. This paper addresses the challenge of link recommendation for the purpose of social influence maximization. In particular, given a graph G and the seed set S, our objective is to select k edges that connect seed nodes and ordinary nodes to optimize the influence dissemination of the seed set. This problem, referred to as influence maximization with augmentation (IMA), has been proven to be NP-hard. In this paper, we propose an algorithm, namely , consisting of an efficient estimator for augmented influence estimation and an accelerated sampling approach. provides a (1-1/e-ε)-approximate solution with a high probability of 1-δ, and runs in O(k^2 (m+n) log (n / δ) / ε^2 + k |E_𝒞|) time assuming that the influence of any singleton node is smaller than that of the seed set. To the best of our knowledge, this is the first algorithm that can be implemented on large graphs containing millions of nodes while preserving strong theoretical guarantees. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed algorithm.|在线社交网络(OSNs)中的链接推荐系统，如 Facebook 的“你可能认识的人”、 Twitter 的“关注谁”和 Instagram 的“推荐账户”，促进了用户之间新的联系的形成。本文以社会影响力最大化为目标，解决了链接推荐的挑战。特别地，给定一个图 G 和种子集 S，我们的目标是选择连接种子节点和普通节点的 k 条边，以优化种子集的影响传播。这个问题被称为增广影响最大化问题(IMA) ，已被证明是 NP 难的。在本文中，我们提出了一个算法，即由一个有效的估计增强影响估计和加速抽样方法。提供了一个(1-1/e-ε)-近似解，其概率为1-δ，假设任一单点节点的影响小于种子集的影响，在 O (k ^ 2(m + n) log (n/δ)/ε ^ 2 + k | E _ C |)时间内运行。据我们所知，这是第一个可以在包含数百万个节点的大图上实现的算法，同时保留了很强的理论保证。我们进行了广泛的实验，以证明我们提出的算法的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Link+Recommendation+to+Augment+Influence+Diffusion+with+Provable+Guarantees)|0|
|[Fairness Rising from the Ranks: HITS and PageRank on Homophilic Networks](https://doi.org/10.1145/3589334.3645609)|AnaAndreea Stoica, Nelly Litvak, Augustin Chaintreau||In this paper, we investigate the conditions under which link analysis algorithms prevent minority groups from reaching high ranking slots. We find that the most common link-based algorithms using centrality metrics, such as PageRank and HITS, can reproduce and even amplify bias against minority groups in networks. Yet, their behavior differs: one one hand, we empirically show that PageRank mirrors the degree distribution for most of the ranking positions and it can equalize representation of minorities among the top ranked nodes; on the other hand, we find that HITS amplifies pre-existing bias in homophilic networks through a novel theoretical analysis, supported by empirical results. We find the root cause of bias amplification in HITS to be the level of homophily present in the network, modeled through an evolving network model with two communities. We illustrate our theoretical analysis on both synthetic and real datasets and we present directions for future work.|在本文中，我们研究了链接分析算法阻止少数群体达到高排名位置的条件。我们发现，最常见的基于链路的算法使用中心度量，如 PageRank 和 HITS，可以重现，甚至放大对少数群体的偏见网络。然而，他们的行为是不同的: 一方面，我们经验证明 PageRank 反映了大多数排名位置的程度分布，它可以平衡顶级节点中少数群体的代表性; 另一方面，我们发现 HITS 通过一个新的理论分析放大了同质网络中预先存在的偏见，并得到了经验结果的支持。我们发现 HITS 中偏差放大的根本原因是网络中存在的同质性水平，通过一个具有两个群体的演化网络模型来模拟。我们举例说明我们的理论分析合成和真实的数据集，我们提出了未来的工作方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness+Rising+from+the+Ranks:+HITS+and+PageRank+on+Homophilic+Networks)|0|
|[Modeling the Impact of Timeline Algorithms on Opinion Dynamics Using Low-rank Updates](https://doi.org/10.1145/3589334.3645714)|Tianyi Zhou, Stefan Neumann, Kiran Garimella, Aristides Gionis||Timeline algorithms are key parts of online social networks, but during recent years they have been blamed for increasing polarization and disagreement in our society. Opinion-dynamics models have been used to study a variety of phenomena in online social networks, but an open question remains on how these models can be augmented to take into account the fine-grained impact of user-level timeline algorithms. We make progress on this question by providing a way to model the impact of timeline algorithms on opinion dynamics. Specifically, we show how the popular Friedkin–Johnsen opinion-formation model can be augmented based on aggregate information, extracted from timeline data. We use our model to study the problem of minimizing the polarization and disagreement; we assume that we are allowed to make small changes to the users' timeline compositions by strengthening some topics of discussion and penalizing some others. We present a gradient descent-based algorithm for this problem, and show that under realistic parameter settings, our algorithm computes a (1+ε)-approximate solution in time Õ(m√(n)(1/ε)), where m is the number of edges in the graph and n is the number of vertices. We also present an algorithm that provably computes an ε-approximation of our model in near-linear time. We evaluate our method on real-world data and show that it effectively reduces the polarization and disagreement in the network. Finally, we release an anonymized graph dataset with ground-truth opinions and more than 27 000 nodes (the previously largest publicly available dataset contains less than 550 nodes).|时间轴算法是在线社交网络的关键组成部分，但近年来，它们被指责为导致社会两极分化和分歧加剧的罪魁祸首。舆论动力学模型已经被用来研究在线社交网络中的各种现象，但是一个悬而未决的问题是如何扩展这些模型以考虑用户级时间轴算法的细粒度影响。我们在这个问题上取得了进展，提供了一种方法来模拟时间轴算法对意见动态的影响。具体地说，我们展示了如何基于从时间轴数据中提取的聚合信息扩展流行的 Friedkin-Johnsen 意见形成模型。我们使用我们的模型来研究最小化两极分化和分歧的问题; 我们假设我们可以通过加强讨论的一些话题和惩罚其他一些话题来对用户的时间表组成做一些小的改变。本文提出了一种基于梯度下降的算法，并证明了在实际参数设置条件下，该算法计算的时间域(m √(n)(1/ε))为(1 + ε)-近似解，其中 m 是图的边数，n 是顶点数。我们还提出了一个算法，可证明计算我们的模型在近线性时间的 ε- 近似。我们评估了我们的方法对真实世界的数据，并表明它有效地减少了极化和网络中的分歧。最后，我们发布了一个匿名的图形数据集，其中包含地面真相观点和超过27000个节点(以前最大的公开可用数据集包含不到550个节点)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+the+Impact+of+Timeline+Algorithms+on+Opinion+Dynamics+Using+Low-rank+Updates)|0|
|[PAGE: Equilibrate Personalization and Generalization in Federated Learning](https://doi.org/10.1145/3589334.3645513)|Qian Chen, Zilong Wang, Jiaqi Hu, Haonan Yan, Jianying Zhou, Xiaodong Lin||Federated learning (FL) is becoming a major driving force behind machine learning as a service, where customers (clients) collaboratively benefit from shared local updates under the orchestration of the service provider (server). Representing clients' current demands and the server's future demand, local model personalization and global model generalization are separately investigated, as the ill-effects of data heterogeneity enforce the community to focus on one over the other. However, these two seemingly competing goals are of equal importance rather than black and white issues, and should be achieved simultaneously. In this paper, we propose the first algorithm to balance personalization and generalization on top of game theory, dubbed PAGE, which reshapes FL as a co-opetition game between clients and the server. To explore the equilibrium, PAGE further formulates the game as Markov decision processes, and leverages the reinforcement learning algorithm, which simplifies the solving complexity. Extensive experiments on four widespread datasets show that PAGE outperforms state-of-the-art FL baselines in terms of global and local prediction accuracy simultaneously, and the accuracy can be improved by up to 35.20% and 39.91%, respectively. In addition, biased variants of PAGE imply promising adaptiveness to demand shifts in practice.|联邦学习(FL)正在成为机器学习作为一种服务背后的主要驱动力，在服务提供者(服务器)的协调下，客户(客户)协同受益于共享的本地更新。由于数据异构性的负面影响迫使社区将注意力放在一个服务器上，因此本地模型个性化和全局模型泛化分别代表了客户当前的需求和服务器未来的需求。然而，这两个看似相互竞争的目标是同等重要的，而不是黑白分明的问题，应该同时实现。本文在博弈论的基础上，提出了平衡个性化和泛化的第一种算法 PAGE，它将 FL 重塑为客户端和服务器之间的一种合作竞争博弈。为了探索均衡，PAGE 进一步将博弈表述为马尔可夫决策过程，并利用强化学习算法，从而简化了求解的复杂性。在四个广泛使用的数据集上进行的大量实验表明，PAGE 在全局和局部预测准确度方面同时优于最先进的 FL 基线，其准确度分别可提高35.20% 和39.91% 。此外，有偏见的 PAGE 变体意味着有希望在实践中适应需求变化。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PAGE:+Equilibrate+Personalization+and+Generalization+in+Federated+Learning)|0|
|[MatchNAS: Optimizing Edge AI in Sparse-Label Data Contexts via Automating Deep Neural Network Porting for Mobile Deployment](https://doi.org/10.1145/3589334.3645538)|Hongtao Huang, Xiaojun Chang, Wen Hu, Lina Yao||Recent years have seen the explosion of edge intelligence with powerful Deep Neural Networks (DNNs). One popular scheme is training DNNs on powerful cloud servers and subsequently porting them to mobile devices after being lightweight. Conventional approaches manually specialized DNNs for various edge platforms and retrain them with real-world data. However, as the number of platforms increases, these approaches become labour-intensive and computationally prohibitive. Additionally, real-world data tends to be sparse-label, further increasing the difficulty of lightweight models. In this paper, we propose MatchNAS, a novel scheme for porting DNNs to mobile devices. Specifically, we simultaneously optimise a large network family using both labelled and unlabelled data and then automatically search for tailored networks for different hardware platforms. MatchNAS acts as an intermediary that bridges the gap between cloud-based DNNs and edge-based DNNs.|近年来，随着强大的深度神经网络(DNN)的出现，边缘智能得到了迅猛发展。一个流行的方案是在强大的云服务器上培训 DNN，然后在轻量级之后将它们移植到移动设备上。传统的方法手动为各种边缘平台专门化 DNN，并用真实世界的数据重新训练它们。然而，随着平台数量的增加，这些方法变得劳动密集型和计算禁止。此外，真实世界的数据往往是稀疏标签，进一步增加了轻量级模型的难度。在本文中，我们提出了 MatchNAS，一种移植 DNN 到移动设备的新方案。具体来说，我们同时使用有标签和无标签的数据优化一个大型网络系列，然后自动为不同的硬件平台搜索量身定制的网络。MatchNAS 作为一个中介，在基于云的 DNN 和基于边缘的 DNN 之间架起了桥梁。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MatchNAS:+Optimizing+Edge+AI+in+Sparse-Label+Data+Contexts+via+Automating+Deep+Neural+Network+Porting+for+Mobile+Deployment)|0|
|[Temporal Conformity-aware Hawkes Graph Network for Recommendations](https://doi.org/10.1145/3589334.3645354)|Chenglong Ma, Yongli Ren, Pablo Castells, Mark Sanderson||Many existing recommender systems (RSs) assume user behavior is governed solely by their interests. However, the peer effect often influences individual decision-making, which leads to conformity behavior. Conventional solutions that eliminate indiscriminately such bias may cause RSs to neglect valuable information and depersonalize the recommendation results. Also, conformity can transform into user interest, e.g., discovering new tastes after a glance at popular music. By better representing different forms of conformity influence, we can do a better job at interest mining and debiasing. In certain extreme circumstances, the herd effect may be exacerbated by user anxiety with uncertainty (e.g., panic buying during the COVID-19 pandemic). RSs may thus fail to respond in time due to sudden and dramatic changes. Moreover, many existing studies potentially conflate conformity bias with popularity bias and lump together various factors responsible for differences in popularity. In this paper, we identify two distinct types of conformity behavior: informational conformity and normative conformity. To address this, we introduce the TCHN model, which utilizes attentional Hawkes processes to disentangle user self-interest and conformity in a personalized manner. Our approach incorporates temporal graph attention networks to capture users' stable and volatile dynamics. We conduct experiments on three real-world datasets, which uncover diverse levels of conformity among users. The results show that TCHN excels in recommendation accuracy, diversity, and fairness across various user groups.|许多现有的推荐系统(RS)假设用户行为完全由他们的兴趣所支配。然而，同伴效应往往影响个体的决策，从而导致从众行为。传统的解决方案，消除不分青红皂白的这种偏见可能会导致 RSS 忽视有价值的信息和去个性化的推荐结果。此外，一致性可以转化为用户的兴趣，例如，发现新的口味一瞥流行音乐。通过更好地表征不同形式的整合影响，可以更好地挖掘和消除利益偏差。在某些极端情况下，用户对不确定性的焦虑可能会加剧羊群效应(例如，2019冠状病毒疾病大流行期间的恐慌性购买)。因此，由于突然和剧烈的变化，RSS 可能无法及时响应。此外，许多现有的研究可能将从众偏见与流行偏见混为一谈，并将造成流行程度差异的各种因素混为一谈。在本文中，我们确定了两种不同类型的从众行为: 信息从众和规范从众。为了解决这一问题，我们引入了 TCHN 模型，该模型利用注意霍克斯过程以个性化的方式将用户的自身利益和一致性分离开来。我们的方法结合了时间图注意网络来捕捉用户的稳定和不稳定的动态。我们在三个真实世界的数据集上进行实验，这些数据集揭示了用户之间不同程度的一致性。结果表明，TCHN 在推荐的准确性、多样性和公平性方面在不同的用户群中表现出色。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Temporal+Conformity-aware+Hawkes+Graph+Network+for+Recommendations)|0|
|[Hierarchical Graph Signal Processing for Collaborative Filtering](https://doi.org/10.1145/3589334.3645368)|Jiafeng Xia, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, Li Shang, Ning Gu||Graph Signal Processing (GSP) has proven to be a highly effective and efficient tool for predicting user future interactions in recommender systems. However, current GSP methods recognize user interaction patterns based on the interactions of all users, so that the recognized interaction patterns are not fully user-matched and easily impacted by other users with different interaction behaviors, resulting in sub-optimal recommendation performance. To this end, we propose a hierarchical graph signal processing method (HiGSP) for collaborative filtering, which consists of two key modules: 1) the cluster-wise filter module that recognizes user unique interaction patterns merely from interactions of users with similar preferences, making the recognized patterns able to reflect user preference without being influenced by other users with different interaction behaviors, and 2) the globally-aware filter module that serves as a complementary to the cluster-wise filter module to recognize user general interaction patterns more effectively from all user interactions. By linearly combining these two modules, HiGSP can recognize user-matched interaction patterns, so as to model user preference and predict user future interactions more accurately. Extensive experiments on six real-world datasets demonstrate the superiority of HiGSP compared to other GCN-based and GSP-based recommendation methods in terms of efficacy and efficiency.|在推荐系统中，图形信号处理(GSP)已被证明是预测用户未来交互的高效工具。然而，目前的 GSP 方法是基于所有用户的交互来识别用户交互模式的，因此识别出的交互模式并不完全匹配，容易受到其他具有不同交互行为的用户的影响，导致推荐性能不理想。为此，我们提出了一种适用于协同过滤的分层图形信号处理方法(higSP) ，该方法由两个关键模块组成: 1)集群式过滤模块，仅仅从具有相似偏好的用户的交互中识别用户独特的交互模式，使得识别出的模式能够反映用户偏好，而不受具有不同交互行为的其他用户的影响; 2)全局感知过滤模块，作为集群式过滤模块的补充，从所有用户交互中更有效地识别用户一般交互模式。通过线性组合这两个模块，HiGSP 可以识别用户匹配的交互模式，从而更准确地建立用户偏好模型，预测用户未来的交互行为。在六个实际数据集上的大量实验表明，与其他基于 GCN 和基于 GSP 的推荐方法相比，HiGSP 具有更高的效率和效力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Graph+Signal+Processing+for+Collaborative+Filtering)|0|
|[Lower-Left Partial AUC: An Effective and Efficient Optimization Metric for Recommendation](https://doi.org/10.1145/3589334.3645371)|Wentao Shi, Chenxu Wang, Fuli Feng, Yang Zhang, Wenjie Wang, Junkang Wu, Xiangnan He||Optimization metrics are crucial for building recommendation systems at scale. However, an effective and efficient metric for practical use remains elusive. While Top-K ranking metrics are the gold standard for optimization, they suffer from significant computational overhead. Alternatively, the more efficient accuracy and AUC metrics often fall short of capturing the true targets of recommendation tasks, leading to suboptimal performance. To overcome this dilemma, we propose a new optimization metric, Lower-Left Partial AUC (LLPAUC), which is computationally efficient like AUC but strongly correlates with Top-K ranking metrics. Compared to AUC, LLPAUC considers only the partial area under the ROC curve in the Lower-Left corner to push the optimization focus on Top-K. We provide theoretical validation of the correlation between LLPAUC and Top-K ranking metrics and demonstrate its robustness to noisy user feedback. We further design an efficient point-wise recommendation loss to maximize LLPAUC and evaluate it on three datasets, validating its effectiveness and robustness.|优化度量对于大规模构建推荐系统至关重要。然而，对于实际应用来说，一个有效和高效的度量标准仍然是难以捉摸的。尽管 Top-K 排名指标是优化的黄金标准，但它们承受着巨大的计算开销。或者，更有效的准确性和 AUC 指标往往不能捕获推荐任务的真正目标，导致性能不理想。为了克服这一困境，我们提出了一种新的优化度量，下左偏 AUC (LLPAUC) ，它与 AUC 一样具有计算效率，但与 Top-K 排名度量强相关。与 AUC 相比，LLPAUC 只考虑了 Lower-Left 角 ROC 曲线下的部分面积，从而将优化重点放在 Top-K 上。我们提供了 LLPAUC 和 Top-K 排名指标之间相关性的理论验证，并证明了其对噪声用户反馈的鲁棒性。我们进一步设计了一个有效的逐点推荐损失来最大化 LLPAUC，并在三个数据集上对其进行评估，验证了其有效性和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lower-Left+Partial+AUC:+An+Effective+and+Efficient+Optimization+Metric+for+Recommendation)|0|
|[Learning to Rewrite Prompts for Personalized Text Generation](https://doi.org/10.1145/3589334.3645408)|Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, Michael Bendersky||Facilitated by large language models (LLMs), personalized text generation has become a rapidly growing research direction. Most existing studies focus on designing specialized models for a particular domain, or they require fine-tuning the LLMs to generate personalized text. We consider a typical scenario in which the large language model, which generates personalized output, is frozen and can only be accessed through APIs. Under this constraint, all one can do is to improve the input text (i.e., text prompts) sent to the LLM, a procedure that is usually done manually. In this paper, we propose a novel method to automatically revise prompts for personalized text generation. The proposed method takes the initial prompts generated by a state-of-the-art, multistage framework for personalized generation and rewrites a few critical components that summarize and synthesize the personal context. The prompt rewriter employs a training paradigm that chains together supervised learning (SL) and reinforcement learning (RL), where SL reduces the search space of RL and RL facilitates end-to-end training of the rewriter. Using datasets from three representative domains, we demonstrate that the rewritten prompts outperform both the original prompts and the prompts optimized via supervised learning or reinforcement learning alone. In-depth analysis of the rewritten prompts shows that they are not only human readable, but also able to guide manual revision of prompts when there is limited resource to employ reinforcement learning to train the prompt rewriter, or when it is costly to deploy an automatic prompt rewriter for inference.|在大型语言模型(LLM)的推动下，个性化文本生成已经成为一个迅速发展的研究方向。大多数现有的研究侧重于为特定领域设计专门的模型，或者需要对 LLM 进行微调以生成个性化的文本。我们考虑一个典型的场景，其中生成个性化输出的大型语言模型被冻结，只能通过 API 访问。在这个约束下，我们所能做的就是改进发送到 LLM 的输入文本(即文本提示) ，这个过程通常是手动完成的。针对个性化文本生成，提出了一种新的自动修改提示的方法。提出的方法采用最先进的多阶段个性化生成框架产生的初始提示，并重写了一些总结和综合个性化上下文的关键组件。提示重写器采用了一种将监督式学习(SL)和强化学习(RL)连接在一起的培训范式，其中 SL 减少了 RL 的搜索空间，而 RL 促进了重写器的端到端培训。通过使用来自三个代表性领域的数据集，我们证明了重写提示符的表现优于原始提示符和单独通过监督式学习或强化学习优化的提示符。对重写提示的深入分析表明，它们不仅具有人类可读性，而且在资源有限、无法使用强化学习培训提示重写程序、或者部署自动提示重写程序进行推理成本高昂的情况下，还能指导人工修改提示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Rewrite+Prompts+for+Personalized+Text+Generation)|0|
|[Physical Trajectory Inference Attack and Defense in Decentralized POI Recommendation](https://doi.org/10.1145/3589334.3645410)|Jing Long, Tong Chen, Guanhua Ye, Kai Zheng, Quoc Viet Hung Nguyen, Hongzhi Yin||As an indispensable personalized service within Location-Based Social Networks (LBSNs), the Point-of-Interest (POI) recommendation aims to assist individuals in discovering attractive and engaging places. However, the accurate recommendation capability relies on the powerful server collecting a vast amount of users' historical check-in data, posing significant risks of privacy breaches. Although several collaborative learning (CL) frameworks for POI recommendation enhance recommendation resilience and allow users to keep personal data on-device, they still share personal knowledge to improve recommendation performance, thus leaving vulnerabilities for potential attackers. Given this, we design a new Physical Trajectory Inference Attack (PTIA) to expose users' historical trajectories. Specifically, for each user, we identify the set of interacted POIs by analyzing the aggregated information from the target POIs and their correlated POIs. We evaluate the effectiveness of PTIA on two real-world datasets across two types of decentralized CL frameworks for POI recommendation. Empirical results demonstrate that PTIA poses a significant threat to users' historical trajectories. Furthermore, Local Differential Privacy (LDP), the traditional privacy-preserving method for CL frameworks, has also been proven ineffective against PTIA. In light of this, we propose a novel defense mechanism (AGD) against PTIA based on an adversarial game to eliminate sensitive POIs and their information in correlated POIs. After conducting intensive experiments, AGD has been proven precise and practical, with minimal impact on recommendation performance.|作为基于位置的社交网络(LBSNs)中不可或缺的个性化服务，兴趣点(POI)推荐旨在帮助个人发现有吸引力和有吸引力的地方。然而，准确的推荐功能依赖于强大的服务器收集大量用户的历史签入数据，从而带来严重的隐私泄露风险。虽然一些用于 POI 推荐的合作学习(CL)框架增强了推荐的弹性，并允许用户将个人数据保存在设备上，但它们仍然共享个人知识以提高推荐性能，从而为潜在的攻击者留下了漏洞。鉴于此，我们设计了一个新的物理轨迹推断攻击(PTIA)来暴露用户的历史轨迹。具体来说，对于每个用户，我们通过分析来自目标 POI 及其相关 POI 的聚合信息来识别交互的 POI 集合。我们评估 PTIA 在两种分散式 CL 框架的两个实际数据集上对 POI 推荐的有效性。实证结果表明，PTIA 对用户的历史轨迹构成了显著的威胁。此外，CL 框架传统的保护隐私的方法——本地差分隐私(LDP)也被证明对 PTIA 无效。在此基础上，提出了一种新的基于对抗博弈的防御机制(AGD)来消除敏感 POI 及其相关 POI 中的信息。经过深入的实验，AGD 已经被证明是精确和实用的，对推荐性能的影响最小。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Physical+Trajectory+Inference+Attack+and+Defense+in+Decentralized+POI+Recommendation)|0|
|[Towards the Identifiability and Explainability for Personalized Learner Modeling: An Inductive Paradigm](https://doi.org/10.1145/3589334.3645437)|Jiatong Li, Qi Liu, Fei Wang, Jiayu Liu, Zhenya Huang, Fangzhou Yao, Linbo Zhu, Yu Su||Personalized learner modeling using cognitive diagnosis (CD), which aims to model learners' cognitive states by diagnosing learner traits from behavioral data, is a fundamental yet significant task in many web learning services. Existing cognitive diagnosis models (CDMs) follow the proficiency-response paradigm that views learner traits and question parameters as trainable embeddings and learns them through learner performance prediction. However, we notice that this paradigm leads to the inevitable non-identifiability and explainability overfitting problem, which is harmful to the quantification of learners' cognitive states and the quality of web learning services. To address these problems, we propose an identifiable cognitive diagnosis framework (ID-CDF) based on a novel response-proficiency-response paradigm inspired by encoder-decoder models. Specifically, we first devise the diagnostic module of ID-CDF, which leverages inductive learning to eliminate randomness in optimization to guarantee identifiability and captures the monotonicity between overall response data distribution and cognitive states to prevent explainability overfitting. Next, we propose a flexible predictive module for ID-CDF to ensure diagnosis preciseness. We further present an implementation of ID-CDF, i.e., ID-CDM, to illustrate its usability. Extensive experiments on four real-world datasets with different characteristics demonstrate that ID-CDF can effectively address the problems without loss of diagnosis preciseness.|基于认知诊断的个性化学习者建模是许多网络学习服务中的基础性工作，其目的是通过对学习者行为特征的诊断来建立学习者的认知状态模型。现有的认知诊断模型遵循熟练度-反应范式，将学习者特征和问题参数视为可训练的嵌入，并通过学习者表现预测来学习。然而，我们注意到这种范式导致了不可避免的不可识别性和可解释性过度拟合问题，这不利于学习者认知状态的量化和网络学习服务的质量。为了解决这些问题，我们提出了一个可识别的认知诊断框架(ID-CDF)基于一个新的反应-熟练程度-反应范式的启发编码器-解码器模型。具体而言，我们首先设计 ID-CDF 的诊断模块，其利用归纳学习消除优化中的随机性以保证可识别性，并捕获总体响应数据分布和认知状态之间的单调性以防止可解释性过度拟合。接下来，我们提出了一个灵活的 ID-CDF 预测模块，以确保诊断的准确性。我们进一步展示了 ID-CDF 的实现，即 ID-CDM，以说明其可用性。对四个具有不同特征的实际数据集进行的大量实验表明，ID-CDF 算法能够在不损失诊断精度的情况下有效地解决问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+the+Identifiability+and+Explainability+for+Personalized+Learner+Modeling:+An+Inductive+Paradigm)|0|
|[Generative News Recommendation](https://doi.org/10.1145/3589334.3645448)|Shen Gao, Jiabao Fang, Quan Tu, Zhitao Yao, Zhumin Chen, Pengjie Ren, Zhaochun Ren||Most existing news recommendation methods tackle this task by conducting semantic matching between candidate news and user representation produced by historical clicked news. However, they overlook the high-level connections among different news articles and also ignore the profound relationship between these news articles and users. And the definition of these methods dictates that they can only deliver news articles as-is. On the contrary, integrating several relevant news articles into a coherent narrative would assist users in gaining a quicker and more comprehensive understanding of events. In this paper, we propose a novel generative news recommendation paradigm that includes two steps: (1) Leveraging the internal knowledge and reasoning capabilities of the Large Language Model (LLM) to perform high-level matching between candidate news and user representation; (2) Generating a coherent and logically structured narrative based on the associations between related news and user interests, thus engaging users in further reading of the news. Specifically, we propose GNR to implement the generative news recommendation paradigm. First, we compose the dual-level representation of news and users by leveraging LLM to generate theme-level representations and combine them with semantic-level representations. Next, in order to generate a coherent narrative, we explore the news relation and filter the related news according to the user preference. Finally, we propose a novel training method named UIFT to train the LLM to fuse multiple news articles in a coherent narrative. Extensive experiments show that GNR can improve recommendation accuracy and eventually generate more personalized and factually consistent narratives.|现有的大多数新闻推荐方法都是通过对历史点击新闻产生的候选新闻和用户表示进行语义匹配来解决这一问题。然而，他们忽视了不同新闻文章之间的高层次联系，也忽视了这些新闻文章与用户之间的深层次关系。这些方法的定义表明，它们只能按原样发布新闻文章。相反，将若干相关新闻文章整合成一个连贯的叙述将有助于用户更快、更全面地了解事件。本文提出了一种新的生成性新闻推荐范式，包括两个步骤: (1)利用大语言模型(LLM)的内部知识和推理能力，在候选新闻和用户表征之间进行高层次的匹配; (2)基于相关新闻和用户兴趣之间的关联，生成一个连贯的、逻辑结构化的叙事，从而使用户进一步阅读新闻。具体来说，我们提出 GNR 来实现生成性新闻推荐范式。首先，我们利用 LLM 生成主题级表示，并将它们与语义级表示结合起来，从而构成新闻和用户的双层表示。其次，为了生成连贯的叙述，我们探索新闻关系，并根据用户偏好对相关新闻进行过滤。最后，我们提出了一种新的训练方法，命名为 UIFT，训练 LLM 在一个连贯的叙述中融合多篇新闻文章。广泛的实验表明，GNR 可以提高推荐的准确性，并最终产生更个性化和事实一致的叙述。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+News+Recommendation)|0|
|[MMPOI: A Multi-Modal Content-Aware Framework for POI Recommendations](https://doi.org/10.1145/3589334.3645449)|Yang Xu, Gao Cong, Lei Zhu, Lizhen Cui||The Point-of-Interest (POI) recommendation system, designed to recommend potential future visits of users based on their check-in sequences, faces the challenge of data scarcity. This challenge primarily stems from the data sparsity issue, namely users interact with only a small number of POIs. Most existing studies attempt to solve this problem by focusing on POI check-in sequences, without considering the substantial multi-modal content information (e.g. textual and image data) commonly associated with POIs. In this paper, we propose a novel multi-modal content-aware framework for POI recommendation (MMPOI). Our approach addresses the issue of data sparsity by incorporating multi-modal content information about POIs from a new perspective. Specifically, MMPOI leverages pre-trained models for inter-modal conversion and employs a unified pre-trained model to extract modal-specific features from each modality, effectively bridging the semantic gap between different modalities. We propose to build a Multi-Modal Trajectory Flow Graph (MTFG) which combines the multi-modal semantic structure with check-in sequences. Moreover, we design an adaptive multi-task Transformer that models users' multi-modal movement patterns and integrates them for the next POI recommendation tasks. Extensive experiments on four real-world datasets demonstrate that MMPOI outperforms state-of-the-art POI recommendation methods. To facilitate reproducibility, we have released both the code and the multi-modal POI recommendation datasets we collect https://github.com/zzmylq/MMPOI|兴趣点(POI)推荐系统，旨在推荐潜在的用户未来访问的基础上，他们的签入序列，面临的数据稀缺性的挑战。这一挑战主要源于数据稀疏问题，即用户只与少量 POI 交互。大多数现有的研究试图通过关注 POI 签入序列来解决这个问题，而没有考虑通常与 POI 相关的大量多模态内容信息(例如文本和图像数据)。本文提出了一种新的多模态内容感知 POI 推荐框架(MMPOI)。我们的方法通过从一个新的角度整合关于 POI 的多模式内容信息来解决数据稀少的问题。具体来说，MMPOI 利用预训练模型进行多模态转换，并采用统一的预训练模型从每种模态中提取模态特征，有效地弥合了不同模态之间的语义鸿沟。提出了一种将多模态语义结构与检入序列相结合的多模态轨迹流图(MTFG)。此外，我们还设计了一个自适应多任务转换器，模拟用户的多模态运动模式，并将其集成到下一个 POI 推荐任务中。在四个实际数据集上的大量实验表明，MMPOI 优于最先进的 POI 推荐方法。为了便于重现，我们发布了代码和我们收集的多模式 POI 推荐数据集 https://github.com/zzmylq/mmpoi|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MMPOI:+A+Multi-Modal+Content-Aware+Framework+for+POI+Recommendations)|0|
|[Representation Learning with Large Language Models for Recommendation](https://doi.org/10.1145/3589334.3645458)|Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang||Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks, particularly in capturing complex user-item relationships. However, these graph-based recommenders heavily depend on ID-based data, potentially disregarding valuable textual information associated with users and items, resulting in less informative learned representations. Moreover, the utilization of implicit feedback data introduces potential noise and bias, posing challenges for the effectiveness of user preference learning. While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention, challenges such as scalability issues, limitations in text-only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems. To address these challenges, we propose a model-agnostic framework RLMRec that aims to enhance existing recommenders with LLM-empowered representation learning. It proposes a recommendation paradigm that integrates representation learning with LLMs to capture intricate semantic aspects of user behaviors and preferences. RLMRec incorporates auxiliary textual signals, develops a user/item profiling paradigm empowered by LLMs, and aligns the semantic space of LLMs with the representation space of collaborative relational signals through a cross-view alignment framework. This work further establish a theoretical foundation demonstrating that incorporating textual signals through mutual information maximization enhances the quality of representations. In our evaluation, we integrate RLMRec with state-of-the-art recommender models, while also analyzing its efficiency and robustness to noise data. Our implementation codes are available at https://github.com/HKUDS/RLMRec.|在深度学习和图形神经网络的影响下，推荐系统已经取得了显著的进步，特别是在捕获复杂的用户-项目关系方面。然而，这些基于图表的推荐严重依赖于基于 ID 的数据，可能会忽略与用户和项目相关的有价值的文本信息，从而导致信息量较小的学习表示。此外，隐式反馈数据的利用还会引入潜在的噪声和偏差，对用户偏好学习的有效性提出了挑战。在传统的基于 ID 的推荐系统中集成大型语言模型(LLM)已经引起了人们的关注，但是为了在实际的推荐系统中有效地实现，需要解决诸如可伸缩性问题、仅依赖文本的局限性和及时的输入限制等挑战。为了应对这些挑战，我们提出了一个模型无关的框架 RLMRec，旨在通过 LLM 授权的表示学习来增强现有的推荐系统。它提出了一种推荐范式，将表示学习与 LLM 相结合，以捕获用户行为和偏好的复杂语义方面。RLMRec 结合了辅助文本信号，开发了一个由 LLM 授权的用户/项目剖析范式，并通过跨视图对齐框架将 LLM 的语义空间与协作关系信号的表示空间对齐。本文的工作进一步奠定了理论基础，表明通过互信息最大化结合文本信号可以提高表征的质量。在我们的评估中，我们整合了 RLMRec 和最先进的推荐模型，同时也分析了它的效率和对噪声数据的鲁棒性。我们的执行守则可在 https://github.com/hkuds/rlmrec 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Representation+Learning+with+Large+Language+Models+for+Recommendation)|0|
|[Challenging Low Homophily in Social Recommendation](https://doi.org/10.1145/3589334.3645460)|Wei Jiang, Xinyi Gao, Guandong Xu, Tong Chen, Hongzhi Yin||Social relations are leveraged to tackle the sparsity issue of user-item interaction data in recommendation under the assumption of social homophily. However, social recommendation paradigms predominantly focus on homophily based on user preferences. While social information can enhance recommendations, its alignment with user preferences is not guaranteed, thereby posing the risk of introducing informational redundancy. We empirically discover that social graphs in real recommendation data exhibit low preference-aware homophily, which limits the effect of social recommendation models. To comprehensively extract preference-aware homophily information latent in the social graph, we propose Social Heterophily-alleviating Rewiring (SHaRe), a data-centric framework for enhancing existing graph-based social recommendation models. We adopt Graph Rewiring technique to capture and add highly homophilic social relations, and cut low homophilic (or heterophilic) relations. To better refine the user representations from reliable social relations, we integrate a contrastive learning method into the training of SHaRe, aiming to calibrate the user representations for enhancing the result of Graph Rewiring. Experiments on real-world datasets show that the proposed framework not only exhibits enhanced performances across varying homophily ratios but also improves the performance of existing state-of-the-art (SOTA) social recommendation models.|在社会同质性假设下，利用社会关系解决推荐中用户交互数据的稀疏性问题。然而，社交推荐模式主要关注基于用户偏好的同质性。虽然社交信息可以增强推荐，但它与用户偏好的一致性并不能得到保证，从而带来了引入信息冗余的风险。实证研究发现，真实推荐数据中的社会图表现出较低的偏好感知同质性，从而限制了社会推荐模型的效果。为了全面提取隐藏在社会图中的偏好感知同质性信息，本文提出了基于数据的社会推荐模型——社会异质性缓解重构框架(SHaRe)。我们采用图重构技术来捕获和添加高度同质的社会关系，并切断低度同质(或异质)关系。为了更好地从可靠的社会关系中提炼出用户表征，我们将对比学习方法融入到 ShareRe 的训练中，旨在校准用户表征以提高图重构的效果。在实际数据集上的实验表明，该框架不仅在不同的同质性比率下表现出更好的性能，而且改善了现有的最先进的社会推荐模型(SOTA)的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Challenging+Low+Homophily+in+Social+Recommendation)|0|
|[Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization](https://doi.org/10.1145/3589334.3645463)|Zirui Zhu, Yong Liu, Zangwei Zheng, Huifeng Guo, Yang You||Click-Through Rate (CTR) prediction holds paramount significance in online advertising and recommendation scenarios. Despite the proliferation of recent CTR prediction models, the improvements in performance have remained limited, as evidenced by open-source benchmark assessments. Current researchers tend to focus on developing new models for various datasets and settings, often neglecting a crucial question: What is the key challenge that truly makes CTR prediction so demanding? In this paper, we approach the problem of CTR prediction from an optimization perspective. We explore the typical data characteristics and optimization statistics of CTR prediction, revealing a strong positive correlation between the top hessian eigenvalue and feature frequency. This correlation implies that frequently occurring features tend to converge towards sharp local minima, ultimately leading to suboptimal performance. Motivated by the recent advancements in sharpness-aware minimization (SAM), which considers the geometric aspects of the loss landscape during optimization, we present a dedicated optimizer crafted for CTR prediction, named Helen. Helen incorporates frequency-wise Hessian eigenvalue regularization, achieved through adaptive perturbations based on normalized feature frequencies. Empirical results under the open-source benchmark framework underscore Helen's effectiveness. It successfully constrains the top eigenvalue of the Hessian matrix and demonstrates a clear advantage over widely used optimization algorithms when applied to seven popular models across three public benchmark datasets on BARS. Our code locates at github.com/NUS-HPC-AI-Lab/Helen.|在在线广告和推荐场景中，点进率(ctrl)预测具有至关重要的意义。尽管最近 CTR 预测模型激增，但性能的改进仍然有限，开源基准评估就是证明。目前的研究人员往往专注于为各种数据集和设置开发新的模型，往往忽略了一个关键问题: 什么是真正使 CTR 预测如此苛刻的关键挑战？本文从最优化的角度探讨了 CTR 预测问题。我们探讨了 CTR 预测的典型数据特征和优化统计，发现最高黑森特征值与特征频率之间存在很强的正相关关系。这种相关性意味着频繁出现的特征往往趋向于尖锐的局部极小值，最终导致次优性能。由于最近在锐度感知最小化(SAM)方面的进展，其中考虑了优化过程中损失景观的几何方面，我们提出了一个专用的 CTR 预测优化器，命名为 Helen。海伦采用了频率明确的黑森特征值正则化，实现了通过自适应扰动的基础上归一化的特征频率。开源基准框架下的实证结果强调了 Helen 的有效性。该算法成功地约束了 Hessian 矩阵的最高特征值，并且在 BARS 的三个公共基准数据集上应用于七个流行的模型时，显示出比广泛使用的优化算法有明显的优势。我们的代码位于 github.com/nus-hpc-ai-lab/helen。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Helen:+Optimizing+CTR+Prediction+Models+with+Frequency-wise+Hessian+Eigenvalue+Regularization)|0|
|[Intersectional Two-sided Fairness in Recommendation](https://doi.org/10.1145/3589334.3645518)|Yifan Wang, Peijie Sun, Weizhi Ma, Min Zhang, Yuan Zhang, Peng Jiang, Shaoping Ma||Fairness of recommender systems (RS) has attracted increasing attention recently. Based on the involved stakeholders, the fairness of RS can be divided into user fairness, item fairness, and two-sided fairness which considers both user and item fairness simultaneously. However, we argue that the intersectional two-sided unfairness may still exist even if the RS is two-sided fair, which is observed and shown by empirical studies on real-world data in this paper, and has not been well-studied previously. To mitigate this problem, we propose a novel approach called Intersectional Two-sided Fairness Recommendation (ITFR). Our method utilizes a sharpness-aware loss to perceive disadvantaged groups, and then uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups. Additionally, predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups. Extensive experiments and analyses on three public datasets show that our proposed approach effectively alleviates the intersectional two-sided unfairness and consistently outperforms previous state-of-the-art methods.|推荐系统的公平性近年来受到越来越多的关注。基于所涉及的利益相关者，RS 的公平性可以分为用户公平性、项目公平性和同时考虑用户公平性和项目公平性的双边公平性。然而，本文通过对实际数据的实证研究发现，即使 RS 是双边公平的，交叉口的双边不公平性仍然存在，而且这种不公平性还没有得到很好的研究。为了缓解这一问题，我们提出了一种新的方法称为交叉双边公平推荐(ITFR)。我们的方法利用敏锐感知损失来感知弱势群体，然后利用协作损失平衡来发展不同交叉群体的一致识别能力。此外，预测分数标准化是利用调整积极的预测分数，以公平对待积极的不同交叉组。对三个公共数据集的大量实验和分析表明，我们提出的方法有效地缓解了交叉口的双向不公平性，并始终优于以前的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Intersectional+Two-sided+Fairness+in+Recommendation)|0|
|[Full Stage Learning to Rank: A Unified Framework for Multi-Stage Systems](https://doi.org/10.1145/3589334.3645523)|Kai Zheng, Haijun Zhao, Rui Huang, Beichuan Zhang, Na Mou, Yanan Niu, Yang Song, Hongning Wang, Kun Gai||The Probability Ranking Principle (PRP) has been considered as the foundational standard in the design of information retrieval (IR) systems. The principle requires an IR module's returned list of results to be ranked with respect to the underlying user interests, so as to maximize the results' utility. Nevertheless, we point out that it is inappropriate to indiscriminately apply PRP through every stage of a contemporary IR system. Such systems contain multiple stages (e.g., retrieval, pre-ranking, ranking, and re-ranking stages, as examined in this paper). The selection bias inherent in the model of each stage significantly influences the results that are ultimately presented to users. To address this issue, we propose an improved ranking principle for multi-stage systems, namely the Generalized Probability Ranking Principle (GPRP), to emphasize both the selection bias in each stage of the system pipeline as well as the underlying interest of users. We realize GPRP via a unified algorithmic framework named Full Stage Learning to Rank. Our core idea is to first estimate the selection bias in the subsequent stages and then learn a ranking model that best complies with the downstream modules' selection bias so as to deliver its top ranked results to the final ranked list in the system's output. We performed extensive experiment evaluations of our developed Full Stage Learning to Rank solution, using both simulations and online A/B tests in one of the leading short-video recommendation platforms. The algorithm is proved to be effective in both retrieval and ranking stages. Since deployed, the algorithm has brought consistent and significant performance gain to the platform.|概率等级原则(PRP)一直被认为是信息检索系统设计的基本标准。该原则要求 IR 模块返回的结果列表根据潜在用户的兴趣进行排序，以最大限度地发挥结果的效用。然而，我们指出，在当代国际关系体系的每一个阶段，不加区分地适用 PRP 是不适当的。这样的系统包含多个阶段(例如，检索、预排序、排序和重排序阶段，正如本文所研究的)。每个阶段模型固有的选择偏差显著影响最终呈现给用户的结果。为了解决这一问题，我们提出了一种改进的多阶段系统排序原则，即广义概率排序原则(GPRP) ，以强调系统流水线各阶段的选择偏差以及用户的潜在兴趣。我们通过一个统一的算法框架——全阶段排序学习来实现 GPRP。我们的核心思想是首先估计后续阶段的选择偏差，然后学习一个最符合下游模块选择偏差的排名模型，以便将其排名最高的结果交付给系统输出的最终排名列表。我们在一个领先的短视频推荐平台上使用模拟和在线 A/B 测试，对我们开发的全阶段学习排名解决方案进行了广泛的实验评估。实验结果表明，该算法在检索和排序阶段都是有效的。自部署以来，该算法为平台带来了一致的、显著的性能增益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Full+Stage+Learning+to+Rank:+A+Unified+Framework+for+Multi-Stage+Systems)|0|
|[RecDCL: Dual Contrastive Learning for Recommendation](https://doi.org/10.1145/3589334.3645533)|Dan Zhang, Yangliao Geng, Wenwen Gong, Zhongang Qi, Zhiyu Chen, Xing Tang, Ying Shan, Yuxiao Dong, Jie Tang||Self-supervised recommendation (SSR) has achieved great success in mining the potential interacted behaviors for collaborative filtering in recent years. As a major branch, Contrastive Learning (CL) based SSR conquers data sparsity in Web platforms by contrasting the embedding between raw data and augmented data. However, existing CL-based SSR methods mostly focus on contrasting in a batch-wise way, failing to exploit potential regularity in the feature-wise dimension, leading to redundant solutions during the representation learning process of users (items) from Websites. Furthermore, the joint benefits of utilizing both Batch-wise CL (BCL) and Feature-wise CL (FCL) for recommendations remain underexplored. To address these issues, we investigate the relationship of objectives between BCL and FCL. Our study suggests a cooperative benefit of employing both methods, as evidenced from theoretical and experimental perspectives. Based on these insights, we propose a dual CL method for recommendation, referred to as RecDCL. RecDCL first eliminates redundant solutions on user-item positive pairs in a feature-wise manner. It then optimizes the uniform distributions within users and items using a polynomial kernel from an FCL perspective. Finally, it generates contrastive embedding on output vectors in a batch-wise objective. We conduct experiments on four widely-used benchmarks and an industrial dataset. The results consistently demonstrate that the proposed RecDCL outperforms the state-of-the-art GNNs-based and SSL-based models (with up to a 5.65\% improvement in terms of Recall@20), thereby confirming the effectiveness of the joint-wise objective. All source codes used in this paper are publicly available at \url{https://github.com/THUDM/RecDCL}}.|近年来，自我监督推荐在挖掘协同过滤潜在的交互行为方面取得了巨大成功。作为一个主要的分支，基于对比学习(CL)的 SSR 通过对比原始数据和增强数据的嵌入来克服 Web 平台中的数据稀疏性。然而，现有的基于 CL 的 SSR 方法大多侧重于批量对比，未能充分利用特征维中潜在的规律性，导致在网站用户(项目)的表示学习过程中出现冗余解决方案。此外，使用批处理式 CL (BCL)和特征式 CL (FCL)的联合优势仍然没有得到充分的探索。为了解决这些问题，我们调查了 BCL 和 FCL 之间的目标关系。我们的研究表明，从理论和实验的角度证明，采用这两种方法是一种合作的益处。基于这些见解，我们提出了一种双 CL 推荐方法，称为 RecDCL。RecDCL 首先以特性明智的方式消除用户项正对上的冗余解决方案。然后从 FCL 的角度使用多项式核优化用户和项目内的均匀分布。最后，在分批目标下对输出向量进行对比嵌入。我们在四个广泛使用的基准和一个工业数据集上进行实验。结果一致表明，拟议的 RecDCL 优于最先进的基于 GNN 和基于 SSL 的模型(在 Recall@20方面提高了5.65%) ，从而证实了联合目标的有效性。本文中使用的所有源代码都可以在 url { https://github.com/thudm/recdcl }上公开获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RecDCL:+Dual+Contrastive+Learning+for+Recommendation)|0|
|[GraphPro: Graph Pre-training and Prompt Learning for Recommendation](https://doi.org/10.1145/3589334.3645546)|Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang||GNN-based recommendation systems have been successful in capturing complex user-item interactions using multi-hop message passing. However, these methods often struggle to handle the dynamic nature of user-item interactions, making it challenging to adapt to changes in user preferences and new data distributions. This limits their scalability and performance in real-world dynamic scenarios. In our study, we propose a framework called GraphPro that combines dynamic graph pre-training with prompt learning in an efficient way. This unique approach allows GNNs to effectively capture both long-term user preferences and short-term behavior changes, resulting in accurate and up-to-date recommendations. To address the issue of changing user preferences, we integrate a temporal prompt mechanism and a graph-structural prompt learning mechanism into the pre-trained GNN architecture. The temporal prompt mechanism incorporates time-related information into user-item interactions, enabling the model to naturally incorporate temporal dynamics. The graph-structural prompt learning mechanism allows the model to apply pre-trained insights to new behavior dynamics without the need for continuous retraining. We also introduce a dynamic evaluation framework for recommendations that better reflects real-world scenarios and reduces the offline-online discrepancy. Through comprehensive experiments, including deployment in a large-scale industrial scenario, we demonstrate the seamless scalability of GraphPro with various leading recommenders. Our results highlight the superiority of GraphPro in terms of effectiveness, robustness, and efficiency. We release the model implementation at the link: https://github.com/HKUDS/GraphPro.|基于 GNN 的推荐系统已经成功地利用多跳消息传递来捕获复杂的用户-项目交互。然而，这些方法往往难以处理用户项交互的动态特性，使其难以适应用户偏好和新数据分布的变化。这限制了它们在真实动态场景中的可伸缩性和性能。在我们的研究中，我们提出了一个叫做 GraphPro 的框架，它将动态图预训练和快速学习有效地结合起来。这种独特的方法允许 GNN 有效地捕获长期用户偏好和短期行为变化，从而产生准确和最新的建议。为了解决用户偏好变化的问题，我们将时间提示机制和图结构提示学习机制集成到预先训练好的 GNN 体系结构中。时间提示机制将与时间相关的信息合并到用户项交互中，使模型能够自然地合并时间动态。图形结构的提示学习机制允许模型应用预先训练的洞察力到新的行为动力学而不需要连续的再训练。我们还为建议引入了一个动态评估框架，它能够更好地反映真实世界的场景，并减少离线-在线差异。通过全面的实验，包括在大规模工业场景中的部署，我们展示了 GraphPro 的无缝伸缩性和各种领先的推荐。我们的结果突出了 GraphPro 在有效性、鲁棒性和效率方面的优势。我们在 link:  https://github.com/hkuds/graphpro 发布模型实现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphPro:+Graph+Pre-training+and+Prompt+Learning+for+Recommendation)|0|
|[Modeling Balanced Explicit and Implicit Relations with Contrastive Learning for Knowledge Concept Recommendation in MOOCs](https://doi.org/10.1145/3589334.3645559)|Hengnian Gu, Zhiyi Duan, Pan Xie, Dongdai Zhou||The knowledge concept recommendation in Massive Open Online Courses (MOOCs) is a significant issue that has garnered widespread attention. Existing methods primarily rely on the explicit relations between users and knowledge concepts on the MOOC platforms for recommendation. However, there are numerous implicit relations (e.g., shared interests or same knowledge levels between users) generated within the users' learning activities on the MOOC platforms. Existing methods fail to consider these implicit relations, and these relations themselves are difficult to learn and represent, causing poor performance in knowledge concept recommendation and an inability to meet users' personalized needs. To address this issue, we propose a novel framework based on contrastive learning, which can represent and balance the explicit and implicit relations for knowledge concept recommendation in MOOCs (CL-KCRec). Specifically, we first construct a MOOCs heterogeneous information network (HIN) by modeling the data from the MOOC platforms. Then, we utilize a relation-updated graph convolutional network and stacked multi-channel graph neural network to represent the explicit and implicit relations in the HIN, respectively. Considering that the quantity of explicit relations is relatively fewer compared to implicit relations in MOOCs, we propose a contrastive learning with prototypical graph to enhance the representations of both relations to capture their fruitful inherent relational knowledge, which can guide the propagation of students' preferences within the HIN. Based on these enhanced representations, to ensure the balanced contribution of both towards the final recommendation, we propose a dual-head attention mechanism for balanced fusion. Experimental results demonstrate that CL-KCRec outperforms several state-of-the-art baselines on real-world datasets in terms of HR, NDCG and MRR.|大型网络开放课程(MOOC)中的知识概念推荐是一个引起广泛关注的重要问题。现有的推荐方法主要依赖于 MOOC 平台上用户和知识概念之间的明确关系。然而，在 MOOC 平台上，用户的学习活动产生了大量的隐性关系(例如，用户之间的共同兴趣或相同的知识水平)。现有的方法没有考虑到这些隐含关系，而且这些关系本身难以学习和表示，导致知识概念推荐效果不佳，无法满足用户的个性化需求。针对这一问题，本文提出了一种基于对比学习的知识概念推荐框架(CL-KCRec) ，该框架能够表示和平衡 MOOC 中知识概念推荐的显性和隐性关系。具体来说，我们首先通过对 MOOC 平台上的数据进行建模来构建一个 MOOC 异构信息网络(HIN)。然后利用关系更新的图卷积网络和叠加的多通道图神经网络分别表示 HIN 中的显式关系和隐式关系。考虑到 MOOCs 中显性关系的数量相对于隐性关系的数量较少，我们提出了一种原型图的对比学习方法，以增强两种关系的表示，从而捕捉它们富有成效的内在关系知识，从而指导学生偏好在 HIN 中的传播。基于这些增强的表示，为了确保两者对最终建议的平衡贡献，我们提出了一种平衡融合的双头注意机制。实验结果表明，CL-KCRec 在 HR、 NDCG 和 MRR 方面优于现实世界数据集上的几个最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+Balanced+Explicit+and+Implicit+Relations+with+Contrastive+Learning+for+Knowledge+Concept+Recommendation+in+MOOCs)|0|
|[A Counterfactual Framework for Learning and Evaluating Explanations for Recommender Systems](https://doi.org/10.1145/3589334.3645560)|Oren Barkan, Veronika Bogina, Liya Gurevitch, Yuval Asher, Noam Koenigstein||In the field of recommender systems, explainability remains a pivotal yet challenging aspect. To address this, we introduce the Learning to eXplain Recommendations (LXR) framework, a post-hoc, model-agnostic approach designed for providing counterfactual explanations. LXR is compatible with any differentiable recommender algorithm and scores the relevance of user data in relation to recommended items. A distinctive feature of LXR is its use of novel self-supervised counterfactual loss terms, which effectively highlight the most influential user data responsible for a specific recommended item. Additionally, we propose several innovative counterfactual evaluation metrics specifically tailored for assessing the quality of explanations in recommender systems. Our code is available on our GitHub repository: https://github.com/DeltaLabTLV/LXR.|在推荐系统领域，可解释性仍然是一个关键但具有挑战性的方面。为了解决这个问题，我们介绍了学习解释建议(LXR)框架，这是一种事后的、与模型无关的方法，旨在提供反事实的解释。LXR 与任何可微推荐算法兼容，并对用户数据与推荐项目的相关性进行评分。LXR 的一个区辨特征是它使用了新颖的自我监督的反事实损失术语，它有效地突出了对特定推荐项目负责的最有影响力的用户数据。此外，我们还提出了几个创新的反事实评估指标，专门用于评估推荐系统中解释的质量。我们的代码可以在我们的 gitHub 存储库中找到:  https://GitHub.com/deltalabtlv/lxr。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Counterfactual+Framework+for+Learning+and+Evaluating+Explanations+for+Recommender+Systems)|0|
|[Category-based and Popularity-guided Video Game Recommendation: A Balance-oriented Framework](https://doi.org/10.1145/3589334.3645573)|Xiping Li, Jianghong Ma, Kangzhe Liu, Shanshan Feng, Haijun Zhang, Yutong Wang||In recent years, the video game industry has experienced substantial growth, presenting players with a vast array of game choices. This surge in options has spurred the need for a specialized recommender system tailored for video games. However, current video game recommendation approaches tend to prioritize accuracy over diversity, potentially leading to unvaried game suggestions. In addition, the existing game recommendation methods commonly lack the ability to establish strict connections between games to enhance accuracy. Furthermore, many existing diversity-focused methods fail to leverage crucial item information, such as item category and popularity during neighbor modeling and message propagation. To address these challenges, we introduce a novel framework, called CPGRec, comprising three modules, namely accuracy-driven, diversity-driven, and comprehensive modules. The first module extends the state-of-the-art accuracy-focused game recommendation method by connecting games in a more stringent manner to enhance recommendation accuracy. The second module connects neighbors with diverse categories within the proposed game graph and harnesses the advantages of popular game nodes to amplify the influence of long-tail games within the player-game bipartite graph, thereby enriching recommendation diversity. The third module combines the above two modules and employs a new negative-sample rating score reweighting method to balance accuracy and diversity. Experimental results on the Steam dataset demonstrate the effectiveness of our proposed method in improving game recommendations. The dataset and source codes are anonymously released at: https://github.com/CPGRec2024/CPGRec.git.|近年来，电子游戏产业经历了长足的发展，为玩家提供了大量的游戏选择。选项的激增促使人们需要一种专门为视频游戏量身定制的推荐系统。然而，目前的视频游戏推荐方法倾向于将准确性置于多样性之上，潜在地导致了不变的游戏推荐。此外，现有的游戏推荐方法普遍缺乏在游戏之间建立严格连接以提高准确性的能力。此外，在邻居建模和消息传播过程中，许多现有的基于多样性的方法未能充分利用关键项目信息，如项目类别和流行程度。为了应对这些挑战，我们引入了一个新的框架，称为 CPGRec，它由三个模块组成，即精度驱动模块、多样性驱动模块和综合模块。第一个模块扩展了最先进的以精确度为中心的游戏推荐方法，以更严格的方式连接游戏以提高推荐精确度。第二个模块连接所提出的博弈图中具有不同类别的邻居，利用流行博弈节点的优势放大长尾博弈在博弈者-博弈二部图中的影响，从而丰富推荐多样性。第三个模块将上述两个模块结合起来，采用一种新的负样本评分重新加权方法来平衡准确性和多样性。蒸汽数据集的实验结果证明了我们提出的方法在改进游戏推荐方面的有效性。数据集和源代码是匿名发布在:  https://github.com/cpgrec2024/cpgrec.git。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Category-based+and+Popularity-guided+Video+Game+Recommendation:+A+Balance-oriented+Framework)|0|
|[Unleashing the Power of Knowledge Graph for Recommendation via Invariant Learning](https://doi.org/10.1145/3589334.3645576)|Shuyao Wang, Yongduo Sui, Chao Wang, Hui Xiong||Knowledge graph (KG) demonstrates substantial potential for enhancing the performance of recommender systems. Due to its rich semantic content and associations among interactive entities, it can effectively alleviate inherent limitations in collaborative filtering (CF), such as data sparsity or cold-start issues. However, most existing knowledge-aware recommendation models indiscriminately aggregate all information in KG, without considering information specifically relevant to the recommendation task. Such indiscriminate aggregation could introduce additional noisy knowledge into representation learning, which can distort the understanding of users' genuine preferences, thereby sacrificing the recommendation quality. In this paper, we introduce the principle of invariance to the knowledge-aware recommendation, culminating in our Knowledge Graph Invariant Learning (KGIL) framework. It aims to discern and harness the task-relevant knowledge connections within KG to enhance the recommendation models. Specifically, we employ multiple environment generators to simulate diverse noisy KG-environments. Then we devise a novel attention learning mechanism for KG and user-item interaction graph, aiming to learn environment-invariant subgraphs. Leveraging an adversarial optimization strategy, we enhance the diversity of the environments, meanwhile, promote invariant representation learning across environments. We conduct extensive experiments on three datasets and compare KGIL with state-of-the-art methods. The experimental results further demonstrate the superiority of our approach.|知识图(KG)显示了提高推荐系统性能的巨大潜力。由于其丰富的语义内容和互动实体之间的关联，它可以有效地缓解协同过滤(CF)固有的局限性，例如数据稀疏或冷启动问题。然而，大多数现有的知识感知推荐模型不加区分地聚合幼儿园中的所有信息，而没有考虑与推荐任务具体相关的信息。这种不加区分的聚合可能会在表示学习中引入额外的噪声知识，从而扭曲对用户真实偏好的理解，从而牺牲推荐质量。在本文中，我们将不变性原理引入知识感知推荐中，最终形成我们的知识图不变性学习(KGIL)框架。它旨在识别和利用幼儿园内与任务相关的知识联系，以加强推荐模型。具体来说，我们使用多个环境生成器来模拟不同的噪声 KG 环境。然后针对 KG 和用户项目交互图提出了一种新的注意学习机制，旨在学习环境不变的子图。利用对抗优化策略，我们增强了环境的多样性，同时促进了环境间的不变表示学习。我们在三个数据集上进行了广泛的实验，并将 KGIL 与最先进的方法进行了比较。实验结果进一步证明了该方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unleashing+the+Power+of+Knowledge+Graph+for+Recommendation+via+Invariant+Learning)|0|
|[Distributionally Robust Graph-based Recommendation System](https://doi.org/10.1145/3589334.3645598)|Bohao Wang, Jiawei Chen, Changdong Li, Sheng Zhou, Qihao Shi, Yang Gao, Yan Feng, Chun Chen, Can Wang||With the capacity to capture high-order collaborative signals, Graph Neural Networks (GNNs) have emerged as powerful methods in Recommender Systems (RS). However, their efficacy often hinges on the assumption that training and testing data share the same distribution (a.k.a. IID assumption), and exhibits significant declines under distribution shifts. Distribution shifts commonly arises in RS, often attributed to the dynamic nature of user preferences or ubiquitous biases during data collection in RS. Despite its significance, researches on GNN-based recommendation against distribution shift are still sparse. To bridge this gap, we propose Distributionally Robust GNN (DR-GNN) that incorporates Distributional Robust Optimization (DRO) into the GNN-based recommendation. DR-GNN addresses two core challenges: 1) To enable DRO to cater to graph data intertwined with GNN, we reinterpret GNN as a graph smoothing regularizer, thereby facilitating the nuanced application of DRO; 2) Given the typically sparse nature of recommendation data, which might impede robust optimization, we introduce slight perturbations in the training distribution to expand its support. Notably, while DR-GNN involves complex optimization, it can be implemented easily and efficiently. Our extensive experiments validate the effectiveness of DR-GNN against three typical distribution shifts. The code is available at https://github.com/WANGBohaO-jpg/DR-GNN .|图形神经网络(GNN)具有捕获高阶协同信号的能力，已经成为推荐系统(RS)中的一种强有力的方法。然而，它们的有效性往往取决于训练和测试数据分布相同的假设(又称为 IID 假设) ，并且在分布变化下表现出显著的下降。分布偏移通常出现在 RS 中，通常归因于用户偏好的动态特性或 RS 中数据收集过程中普遍存在的偏差。尽管有其重要意义，但基于 GNN 的配电网分布移位推荐研究还很少。为了弥补这一差距，我们提出了分布式鲁棒 GNN (DR-GNN) ，将分布式鲁棒优化(DRO)结合到基于 GNN 的推荐中。DR-GNN 解决了两个核心挑战: 1)为了使 DRO 能够迎合与 GNN 交织在一起的图形数据，我们将 GNN 重新解释为一个图形平滑正则化器，从而促进 DRO 的微妙应用; 2)鉴于推荐数据的典型稀疏性质，这可能会阻碍稳健优化，我们在训练分布中引入轻微的扰动，以扩大其支持。值得注意的是，虽然 DR-GNN 涉及到复杂的优化，但它可以轻松有效地实现。我们的大量实验验证了 DR-GNN 对三种典型分布移位的有效性。密码可在 https://github.com/wangbohao-jpg/dr-gnn 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributionally+Robust+Graph-based+Recommendation+System)|0|
|[Co-clustering for Federated Recommender System](https://doi.org/10.1145/3589334.3645626)|Xinrui He, Shuo Liu, Jacky Keung, Jingrui He||As data privacy and security attract increasing attention, Federated Recommender System (FRS) offers a solution that strikes a balance between providing high-quality recommendations and preserving user privacy. However, the presence of statistical heterogeneity in FRS, commonly observed due to personalized decision-making patterns, can pose challenges. To address this issue and maximize the benefit of collaborative filtering (CF) in FRS, it is intuitive to consider clustering clients (users) as well as items into different groups and learning group-specific models. Existing methods either resort to client clustering via user representations-risking privacy leakage, or employ classical clustering strategies on item embeddings or gradients, which we found are plagued by the curse of dimensionality. In this paper, we delve into the inefficiencies of the K-Means method in client grouping, attributing failures due to the high dimensionality as well as data sparsity occurring in FRS, and propose CoFedRec, a novel Co-clustering Federated Recommendation mechanism, to address clients heterogeneity and enhance the collaborative filtering within the federated framework. Specifically, the server initially formulates an item membership from the client-provided item networks. Subsequently, clients are grouped regarding a specific item category picked from the item membership during each communication round, resulting in an intelligently aggregated group model. Meanwhile, to comprehensively capture the global inter-relationships among items, we incorporate an additional supervised contrastive learning term based on the server-side generated item membership into the local training phase for each client. Extensive experiments on four datasets are provided, which verify the effectiveness of the proposed CoFedRec.|随着数据隐私和安全越来越受到关注，联邦推荐系统(FRS)提供了一种在提供高质量建议和保护用户隐私之间取得平衡的解决方案。然而，FRS 中统计异质性的存在，通常由于个性化的决策模式而被观察到，可能带来挑战。为了解决这个问题，并最大限度地发挥协同过滤(CF)在财务报告系统中的作用，直观地考虑将客户(用户)以及项目分为不同的组和学习组特定的模型。现有的方法要么通过用户表示来进行客户聚类——这有泄露隐私的风险，要么在条目嵌入或渐变上采用传统的聚类策略，我们发现这些方法受到了维数灾难的困扰。在本文中，我们深入研究了 K-Means 方法在客户分组中的低效性，归因于 FRS 中出现的高维度和数据稀疏性导致的失败，并提出了一种新的协同聚类联邦推荐机制 CofedRec，以解决客户异构性问题，增强联邦框架中的协同过滤。具体来说，服务器最初从客户端提供的项目网络中规定项目成员关系。随后，在每个通信回合中，根据从项目成员中挑选的特定项目类别对客户进行分组，从而产生一个智能聚合的组模型。同时，为了全面捕捉项目之间的全局相互关系，我们在每个客户端的局部训练阶段加入了一个基于服务器端生成的项目成员关系的监督对比学习术语。在四个数据集上进行了大量的实验，验证了所提出的 CoFedRec 算法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Co-clustering+for+Federated+Recommender+System)|0|
|[PMG : Personalized Multimodal Generation with Large Language Models](https://doi.org/10.1145/3589334.3645633)|Xiaoteng Shen, Rui Zhang, Xiaoyan Zhao, Jieming Zhu, Xi Xiao||The emergence of large language models (LLMs) has revolutionized the capabilities of text comprehension and generation. Multi-modal generation attracts great attention from both the industry and academia, but there is little work on personalized generation, which has important applications such as recommender systems. This paper proposes the first method for personalized multimodal generation using LLMs, showcases its applications and validates its performance via an extensive experimental study on two datasets. The proposed method, Personalized Multimodal Generation (PMG for short) first converts user behaviors (e.g., clicks in recommender systems or conversations with a virtual assistant) into natural language to facilitate LLM understanding and extract user preference descriptions. Such user preferences are then fed into a generator, such as a multimodal LLM or diffusion model, to produce personalized content. To capture user preferences comprehensively and accurately, we propose to let the LLM output a combination of explicit keywords and implicit embeddings to represent user preferences. Then the combination of keywords and embeddings are used as prompts to condition the generator. We optimize a weighted sum of the accuracy and preference scores so that the generated content has a good balance between them. Compared to a baseline method without personalization, PMG has a significant improvement on personalization for up to 8|大型语言模型(LLM)的出现彻底改变了文本理解和生成的能力。多模态生成技术引起了业界和学术界的广泛关注，但在个性化生成技术方面的研究却很少，它在推荐系统等方面有着重要的应用。本文提出了第一种基于 LLM 的个性化多模态生成方法，通过对两个数据集的大量实验研究，展示了该方法的应用，并验证了该方法的性能。该方法首先将用户行为(如推荐系统中的点击或与虚拟助手的对话)转换为自然语言，以便于 LLM 理解和提取用户偏好描述。然后将这些用户首选项输入生成器，例如多模态 LLM 或扩散模型，以生成个性化内容。为了全面和准确地捕获用户偏好，我们建议让 LLM 输出显式关键字和隐式嵌入的组合来表示用户偏好。然后使用关键字和嵌入的组合作为提示来调整生成器。我们优化了准确性和偏好得分的加权和，以便生成的内容之间有一个良好的平衡。与没有个性化的基线方法相比，PMG 在个性化方面有显著的提高，最高可达8|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PMG+:+Personalized+Multimodal+Generation+with+Large+Language+Models)|0|
|[M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation](https://doi.org/10.1145/3589334.3645635)|Jiachen Zhu, Yichao Wang, Jianghao Lin, Jiarui Qin, Ruiming Tang, Weinan Zhang, Yong Yu||We primarily focus on the field of multi-scenario recommendation, which poses a significant challenge in effectively leveraging data from different scenarios to enhance predictions in scenarios with limited data. Current mainstream efforts mainly center around innovative model network architectures, with the aim of enabling the network to implicitly acquire knowledge from diverse scenarios. However, the uncertainty of implicit learning in networks arises from the absence of explicit modeling, leading to not only difficulty in training but also incomplete user representation and suboptimal performance. Furthermore, through causal graph analysis, we have discovered that the scenario itself directly influences click behavior, yet existing approaches directly incorporate data from other scenarios during the training of the current scenario, leading to prediction biases when they directly utilize click behaviors from other scenarios to train models. To address these problems, we propose the Multi-Scenario Causal-driven Adaptive Network M-scan). This model incorporates a Scenario-Aware Co-Attention mechanism that explicitly extracts user interests from other scenarios that align with the current scenario. Additionally, it employs a Scenario Bias Eliminator module utilizing causal counterfactual inference to mitigate biases introduced by data from other scenarios. Extensive experiments on two public datasets demonstrate the efficacy of our M-scan compared to the existing baseline models.|我们主要关注多方案建议领域，这对于有效利用不同方案的数据来增强数据有限的方案中的预测提出了重大挑战。目前的主流工作主要围绕创新型网络架构展开，目的是使网络能够从不同的情景中隐含地获取知识。然而，网络中隐式学习的不确定性源于显式建模的缺失，这不仅导致了训练的困难，而且还导致了用户表示的不完全性和性能的次优化。此外，通过因果图分析，我们发现场景本身直接影响点击行为，但现有的方法在当前场景的训练过程中直接合并来自其他场景的数据，导致预测偏差，当他们直接利用其他场景的点击行为来训练模型。为了解决这些问题，我们提出了多场景因果驱动的自适应网络 M 扫描算法。该模型结合了场景感知协同关注机制，该机制显式地从与当前场景一致的其他场景中提取用户兴趣。此外，它还使用了一个场景偏差消除器模块，该模块利用因果反事实推断来减轻其他场景数据引入的偏差。在两个公共数据集上的大量实验证明了我们的 M 扫描与现有的基线模型相比的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=M-scan:+A+Multi-Scenario+Causal-driven+Adaptive+Network+for+Recommendation)|0|
|[Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation](https://doi.org/10.1145/3589334.3645693)|Bo Yan, Yang Cao, Haoyu Wang, Wenchuan Yang, Junping Du, Chuan Shi||The heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has emerged as a potent tool for mitigating data sparsity in recommender systems. Existing HIN-based recommender systems operate under the assumption of centralized storage and model training. However, real-world data is often distributed due to privacy concerns, leading to the semantic broken issue within HINs and consequent failures in centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored on the client side and shared HINs on the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which facilitates collaborative training of a recommendation model using distributed HINs while protecting user privacy. Specifically, we first formalize the privacy definition for HIN-based federated recommendation (FedRec) in the light of differential privacy, with the goal of protecting user-item interactions within private HIN as well as users' high-order patterns from shared HINs. To recover the broken meta-path based semantics and ensure proposed privacy measures, we elaborately design a semantic-preserving user interactions publishing method, which locally perturbs user's high-order patterns and related user-item interactions for publishing. Subsequently, we introduce an HGNN model for recommendation, which conducts node- and semantic-level aggregations to capture recovered semantics. Extensive experiments on four datasets demonstrate that our model outperforms existing methods by a substantial margin (up to 34 reasonable privacy budget.|异构信息网络(HIN)包含丰富的元路径描述语义，已成为缓解推荐系统中数据稀疏性的有力工具。现有的基于 HIN 的推荐系统是在集中存储和模型训练的假设下运行的。然而，由于隐私问题，现实世界中的数据经常被分发，导致 HIN 内部的语义破碎问题以及随之而来的基于 HIN 的集中式推荐的失败。在本文中，我们建议将 HIN 划分为存储在客户端的私有 HIN 和在服务器端共享的 HIN。在此基础上，提出了一种基于联邦异构图神经网络(FedHGNN)的框架，该框架在保护用户隐私的同时，有利于使用分布式 HIN 对推荐模型进行协同训练。具体来说，我们首先根据差分隐私正式确定了基于 HIN 的联邦推荐(fed rec)的隐私定义，目的是保护私有 HIN 内的用户项交互以及共享 HIN 中用户的高阶模式。为了恢复破碎的基于元路径的语义，保证提出的隐私措施，我们精心设计了一种语义保持的用户交互发布方法，该方法局部扰动用户的高阶模式和相关的用户项交互发布。随后，我们引入了 HGNN 推荐模型，该模型通过节点级和语义级的聚合来捕获恢复的语义。对四个数据集的大量实验表明，我们的模型优于现有的方法大幅度(多达34合理的隐私预算。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Federated+Heterogeneous+Graph+Neural+Network+for+Privacy-preserving+Recommendation)|0|
|[Understanding Human Preferences: Towards More Personalized Video to Text Generation](https://doi.org/10.1145/3589334.3645711)|Yihan Wu, Ruihua Song, Xu Chen, Hao Jiang, Zhao Cao, Jin Yu||While previous video to text models have achieved remarkable successes, they mostly focus on how to understand the video contents in a general sense, but fail to capture the human personalized preferences, which is highly demanded for an engaging multimodal chatbots. Different from user modeling in collaborative filtering, there is no other user behaviors in inference as a real-time video stream is coming. In this paper, we formally define the task of personalized video commenting task and design an end-to-end personalized framework for solving this task. In specific, we argue that the personalization for video comment generation can be reflected in two aspects, that is, (1) for the same video, different users may comment on different clips, and (2) for the same clip, different people may also express various opinions with diverse commentary styles. Motivated by these considerations, we design our framework based on two components. The first one is a clip selector, which is responsible for predicting the clips that the user may comment in the video. The second one is a text generator, which aims to produce the comment based on the above predicted clips and the user's preference. In our framework, these two components are optimized in an end-to-end manner to mutually enhance each other, where we design confidence-aware scheduled sampling and iterative inference strategies to solve the problem that the ground truth clips are absent in the inference phase. As the absence of personalized video to text dataset, we collect and release a new dataset for studying this problem. We conduct extensive experiments to demonstrate the effectiveness of our model.|虽然以往的视频文本模型取得了显著的成功，但它们主要集中在如何理解一般意义上的视频内容，但未能捕捉到人类的个性化偏好，这是一个具有吸引力的多通道聊天机器人高度需求。与协同过滤中的用户建模不同，随着实时视频流的到来，没有其他用户行为的推断。本文正式定义了个性化视频评论任务的任务，并设计了一个端到端的个性化框架来解决这个任务。具体而言，我们认为视频评论生成的个性化可以体现在两个方面，即(1)对于同一个视频，不同的用户可以对不同的视频片段进行评论，(2)对于同一个视频片段，不同的人也可以用不同的评论风格表达不同的意见。基于这些考虑，我们设计了基于两个组件的框架。第一个是剪辑选择器，它负责预测用户可能在视频中发表评论的剪辑。第二个是文本生成器，其目的是根据上述预测剪辑和用户的偏好生成评论。在我们的框架中，这两个组件以端到端的方式进行优化，以相互增强，其中我们设计了可信度感知的计划采样和迭代推理策略，以解决推理阶段缺乏地面真相剪辑的问题。由于缺乏个性化的视频文本数据集，我们收集并发布了一个新的数据集来研究这个问题。我们进行了广泛的实验，以证明我们的模型的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+Human+Preferences:+Towards+More+Personalized+Video+to+Text+Generation)|0|
|[Entity Disambiguation with Extreme Multi-label Ranking](https://doi.org/10.1145/3589334.3645498)|JyunYu Jiang, WeiCheng Chang, Jiong Zhang, ChoJui Hsieh, HsiangFu Yu||Entity disambiguation is one of the most important natural language tasks to identify entities behind ambiguous surface mentions within a knowledge base. Although many recent studies apply deep learning to achieve decent results, they need exhausting pre-training and mediocre recall in the retrieval stage. In this paper, we propose a novel framework, eXtreme Multi-label Ranking for Entity Disambiguation (XMRED), to address this challenge. An efficient zero-shot entity retriever with auxiliary data is first pre-trained to recall relevant entities based on linear models. Specifically, the retrieval process can be considered as an extreme multi-label ranking (XMR) task. Entities are first clustered at different scales to form a label tree, thereby learning multi-scale entity retrievers over the label tree with high recall. Moreover, XMRED applies deep cross-encoder as a re-ranker to achieve high precision based on high-quality candidates. Extensive experimental results based on the AIDA-CoNLL benchmark and five zero-shot testing datasets demonstrate that XMRED obtains 98% and over 95% recall scores for in-domain and zero-shot datasets with top-10 retrieved entities. With a deep cross-encoder as the re-ranker, XMRED further outperforms the previous state-of-the-art by 1.74% in In-KB micro-F1 scores on average with a significant improvement on the training efficiency from days to 3.48 hours. In addition, XMRED also beats the state-of-the-art for page-level document retrieval by 2.38% in accuracy and 1.90% in recall@5.|实体消歧是识别知识库中模糊表面提及背后的实体的重要自然语言任务之一。虽然最近的许多研究应用深度学习来获得不错的结果，但是在检索阶段，他们需要耗费精力的预训练和平庸的回忆。在本文中，我们提出了一个新的框架，eXtreme 多标签实体消歧排序(XMRED) ，以解决这一挑战。首先对带有辅助数据的高效零拍实体检索器进行预训练，使其能够基于线性模型进行相关实体的检索。具体来说，检索过程可以看作是一个极端的多标签排序(XMR)任务。实体首先在不同的尺度上聚类，形成一个标签树，从而在标签树上学习具有高召回率的多尺度实体检索器。此外，XMRED 应用深交叉编码器作为一个重新排序，以实现高精度的基础上高质量的候选人。基于 AIDA-CoNLL 基准和五个零拍测试数据集的广泛实验结果表明，XMRED 获得了98% 和95% 以上的领域内和零拍数据集的回忆分数与前10名检索实体。随着深交叉编码器作为重新排名，XMRED 进一步优于以前的国家的最先进的1.74% 在 In-KB 微 F1分数平均有显着改善的训练效率从天到3.48小时。此外，XMRED 在准确率方面也比最先进的页面级文献检索高出2.38% ，在召回率方面高出1.90% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Entity+Disambiguation+with+Extreme+Multi-label+Ranking)|0|
|[BOND: Bootstrapping From-Scratch Name Disambiguation with Multi-task Promoting](https://doi.org/10.1145/3589334.3645580)|Yuqing Cheng, Bo Chen, Fanjin Zhang, Jie Tang||From-scratch name disambiguation is an essential task for establishing a reliable foundation for academic platforms. It involves partitioning documents authored by identically named individuals into groups representing distinct real-life experts. Canonically, the process is divided into two decoupled tasks: locally estimating the pairwise similarities between documents followed by globally grouping these documents into appropriate clusters. However, such a decoupled approach often inhibits optimal information exchange between these intertwined tasks. Therefore, we present BOND, which bootstraps the local and global informative signals to promote each other in an end-to-end regime. Specifically, BOND harnesses local pairwise similarities to drive global clustering, subsequently generating pseudo-clustering labels. These global signals further refine local pairwise characterizations. The experimental results establish BOND's superiority, outperforming other advanced baselines by a substantial margin. Moreover, an enhanced version, BOND+, incorporating ensemble and post-match techniques, rivals the top methods in the WhoIsWho competition.|从头开始消除名称歧义是为学术平台建立可靠基础的一项重要任务。它涉及到将由具有相同名称的个人编写的文档划分为代表不同的现实生活专家的组。规范地说，这个过程分为两个解耦的任务: 在本地估计文档之间的成对相似性，然后将这些文档全局分组到适当的集群中。然而，这种解耦的方法往往会抑制这些交织在一起的任务之间的最佳信息交换。因此，我们提出的 BOND，它引导本地和全球信息信号，以促进在一个端到端的制度。具体来说，BOND 利用局部成对的相似性来驱动全局聚类，随后生成伪聚类标签。这些全局信号进一步完善了局部成对特征。实验结果证实了 BOND 的优越性，大大优于其他高级基线。此外，一个加强版本，BOND + ，结合合奏和赛后技术，竞争对手的顶尖方法在 whoIsWho 比赛。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BOND:+Bootstrapping+From-Scratch+Name+Disambiguation+with+Multi-task+Promoting)|0|
|[Inductive Graph Alignment Prompt: Bridging the Gap between Graph Pre-training and Inductive Fine-tuning From Spectral Perspective](https://doi.org/10.1145/3589334.3645620)|Yuchen Yan, Peiyan Zhang, Zheng Fang, Qingqing Long||The "Graph pre-training and fine-tuning" paradigm has significantly improved Graph Neural Networks(GNNs) by capturing general knowledge without manual annotations for downstream tasks. However, due to the immense gap of data and tasks between the pre-training and fine-tuning stages, the model performance is still limited. Inspired by prompt fine-tuning in Natural Language Processing(NLP), many endeavors have been made to bridge the gap in graph domain. But existing methods simply reformulate the form of fine-tuning tasks to the pre-training ones. With the premise that the pre-training graphs are compatible with the fine-tuning ones, these methods typically operate in transductive setting. In order to generalize graph pre-training to inductive scenario where the fine-tuning graphs might significantly differ from pre-training ones, we propose a novel graph prompt based method called Inductive Graph Alignment Prompt(IGAP). Firstly, we unify the mainstream graph pre-training frameworks and analyze the essence of graph pre-training from graph spectral theory. Then we identify the two sources of the data gap in inductive setting: (i) graph signal gap and (ii) graph structure gap. Based on the insight of graph pre-training, we propose to bridge the graph signal gap and the graph structure gap with learnable prompts in the spectral space. A theoretical analysis ensures the effectiveness of our method. At last, we conduct extensive experiments among nodes classification and graph classification tasks under the transductive, semi-inductive and inductive settings. The results demonstrate that our proposed method can successfully bridge the data gap under different settings.|“图形预训练和微调”范例显著改善了图形神经网络(GNN) ，通过捕获一般知识而无需手动注释的下游任务。然而，由于预训练阶段和微调阶段之间数据和任务的巨大差距，模型的性能仍然是有限的。受到自然语言处理(NLP)中快速微调的启发，人们付出了许多努力来弥补图域中的差距。但现有的方法只是简单地将微调任务的形式重新表述为训练前的任务。在预训练图与微调图兼容的前提下，这些方法通常采用传导设置。为了将图的预训练推广到图的微调可能明显不同于预训练的情况，提出了一种基于图提示的归纳图对齐提示(IGAP)方法。首先，统一了主流的图预训练框架，从图谱理论的角度分析了图预训练的本质。然后，我们确定了归纳设置中数据间隙的两个来源: (i)图信号间隙和(ii)图结构间隙。基于图预训练的思想，我们提出在谱空间中用可学习的提示来桥接图信号间隙和图结构间隙。理论分析确保了我们方法的有效性。最后，在传导、半归纳和归纳环境下，对节点分类和图分类任务进行了广泛的实验。实验结果表明，该方法能够在不同设置下成功地消除数据间隙。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Inductive+Graph+Alignment+Prompt:+Bridging+the+Gap+between+Graph+Pre-training+and+Inductive+Fine-tuning+From+Spectral+Perspective)|0|
|[Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction](https://doi.org/10.1145/3589334.3645678)|Qi Sun, Kun Huang, Xiaocui Yang, Rong Tong, Kun Zhang, Soujanya Poria||Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in information systems that aims to simultaneously extract entities with semantic relations from a document. Existing methods heavily rely on a substantial amount of fully labeled data. However, collecting and annotating data for newly emerging relations is time-consuming and labor-intensive. Recent advanced Large Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text generation capabilities, inspiring us to explore an alternative approach for obtaining auto-labeled documents with new relations. In this paper, we propose a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework, which generates labeled data by retrieval and denoising knowledge from LLMs, called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step. To improve the quality of synthetic data, we propose a denoising strategy based on the consistency of cross-document knowledge. Leveraging our denoised synthetic data, we proceed to fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets. We perform experiments for both zero-shot document-level relation and triplet extraction on two public datasets. The experimental results illustrate that our GenRDK framework outperforms strong baselines.|文档级关系三联抽取(DocRTE)是信息系统中的一项基本任务，旨在从文档中同时抽取具有语义关系的实体。现有的方法严重依赖于大量完全标记的数据。然而，为新出现的关系收集和注释数据是耗费时间和劳动密集型的。最近的高级大语言模型(LLM) ，例如 ChatGPT 和 LLaMA，展示了令人印象深刻的长文本生成能力，启发我们探索一种替代方法来获得具有新关系的自动标记文档。本文提出了一种零拍文档级关系三联体提取(Zero-shot Document-level Relations Triplet 萃取，ZeroDocRTE)框架 GenRDK，该框架通过检索和去噪 LLM 中的知识来生成标记数据。具体来说，我们提出了一个检索链提示来指导 ChatGPT 逐步生成带标签的长文本数据。为了提高综合数据的质量，提出了一种基于跨文档知识一致性的去噪策略。利用我们去噪的合成数据，我们继续微调 LLaMA2-13B-Chat 以提取文档级关系三联体。我们在两个公共数据集上进行了零拍文档级关系和三元组提取的实验。实验结果表明，我们的 GenRDK 框架优于强基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Consistency+Guided+Knowledge+Retrieval+and+Denoising+in+LLMs+for+Zero-shot+Document-level+Relation+Triplet+Extraction)|0|
|[Detecting Illicit Food Factories from Chemical Declaration Data via Graph-aware Self-supervised Contrastive Anomaly Ranking](https://doi.org/10.1145/3589334.3648138)|ShengFang Yang, ChengTe Li||In the global food industry, where the line between legitimate and illicit manufacturing is increasingly blurred by the scale and complexity of the supply chain, safeguarding consumer health and trust necessitates innovative detection methods. Addressing this, this paper presents Graph-aware Self-supervised Contrastive Anomaly Ranking (GraphCAR), a novel unsupervised learning model, devised to identify illicit food factories through the scrutiny of chemical declaration data. GraphCAR tackles the scarcity of labeled data and the intricacies inherent in the vast array of declared chemicals, leveraging a Graph Autoencoder fused with a self-supervised contrastive learning mechanism. This fusion not only simplifies the feature space by embedding chemical declarations within a bipartite graph but also adeptly flags subtle, potentially illicit patterns through contrastively inspecting the learned factory representations. Through rigorous evaluations conducted on real-world factory's chemical declaration data, GraphCAR has demonstrated superior performance over conventional methods on unsupervised outlier detection and one-class classification tasks, showcasing its accuracy, robustness and reliability in flagging potential malpractice. With its successful application in food safety, GraphCAR stands as a testament to the potential of AI-driven solutions to address multifaceted challenges for the greater good.|在全球食品工业中，由于供应链的规模和复杂性，合法和非法制造之间的界限日益模糊，因此，为了保障消费者的健康和信任，必须采用创新的检测方法。针对这一问题，本文提出了一种新的非监督式学习模型——图形感知自监督对比异常排名(GraphCAR) ，该模型旨在通过审查化学品申报数据来识别非法食品工厂。GraphCAR 解决了标记数据的稀缺性和大量申报化学品中固有的复杂性，利用了融合了自我监督对比学习机制的 Graph Autoencoder。这种融合不仅通过在二分图中嵌入化学声明来简化特征空间，而且还通过对比检查学习到的工厂表示来熟练地标记微妙的、潜在的非法模式。通过对真实世界工厂的化学品申报数据进行严格的评估，GraphCAR 在无监督的异常检测和一类分类任务方面显示出优于传统方法的性能，在标记潜在违规行为方面显示出其准确性、稳健性和可靠性。随着它在食品安全方面的成功应用，GraphCAR 证明了人工智能驱动的解决方案在解决多方面挑战以实现更大利益方面的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Illicit+Food+Factories+from+Chemical+Declaration+Data+via+Graph-aware+Self-supervised+Contrastive+Anomaly+Ranking)|0|
|[Bayesian Iterative Prediction and Lexical-based Interpretation for Disturbed Chinese Sentence Pair Matching](https://doi.org/10.1145/3589334.3648149)|Muzhe Guo, Muhao Guo, Juntao Su, Junyu Chen, Jiaqian Yu, Jiaqi Wang, Hongfei Du, Parmanand Sahu, Ashwin Assysh Sharma, Fang Jin||In an era dominated by web-based intelligent customer services, the applications of Sentence Pair Matching are profoundly broad. Web agents, for example, automatically respond to customer queries by finding similar past questions, significantly reducing customer service expenses. While current large language models (LLMs) offer powerful text generation capabilities, they often struggle with opacity, potential text toxicity, and difficulty managing domain-specific and confidential business inquiries. Consequently, the widespread adoption of web-based intelligent customer services in real-world business still greatly relies on query-based interactions. In this paper, we introduce a series of model-agnostic techniques aimed at enhancing both the accuracy and interpretability of Chinese pairwise sentence-matching models. Our contributions include (1) An Edit-distance-weighted fine-tuning method, (2) A Bayesian Iterative Prediction algorithm, (3) A Lexical-based Dual Ranking Interpreter, and (4) A Bi-criteria Denoising strategy. Experimental results on the Large-scale Chinese Question Matching Corpus (LCQMC) with a disturbed test demonstrate that our fine-tuning and prediction methods can steadily improve matching accuracy, building on the current state-of-the-art models. Besides, our interpreter with denoising strategy markedly enhances token-level interpretation in rationality and loyalty. In both matching accuracy and interpretation, our approaches outperform classic methods and even LLMs.|在一个以基于网络的智能客户服务为主导的时代，句子配对的应用是非常广泛的。例如，Web 代理会自动响应客户查询，寻找过去相似的问题，从而大大减少客户服务费用。虽然目前的大型语言模型(LLM)提供了强大的文本生成功能，但是它们经常会遇到不透明性、潜在的文本毒性以及难以管理特定领域和保密业务查询等问题。因此，基于 Web 的智能客户服务在现实商业中的广泛应用仍然在很大程度上依赖于基于查询的交互。本文介绍了一系列模型不可知技术，旨在提高汉语成对句子匹配模型的准确性和可解释性。我们的贡献包括(1)编辑距离加权微调方法，(2)贝叶斯迭代预测算法，(3)基于词汇的双排序解释器，和(4)双准则去噪策略。大规模中文问题匹配语料库的实验结果显示，我们的微调和预测方法可以在现有先进模型的基础上稳步提高匹配的准确性。此外，采用去噪策略的口译员显著提高了理性和忠诚度的象征层次解释。在匹配精度和解释方面，我们的方法优于经典方法甚至 LLM。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bayesian+Iterative+Prediction+and+Lexical-based+Interpretation+for+Disturbed+Chinese+Sentence+Pair+Matching)|0|
|[Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video Recommendation](https://doi.org/10.1145/3589334.3648159)|Nicholas Sukiennik, Chen Gao, Nian Li||Filter bubbles have been studied extensively within the context of online content platforms due to their potential to cause undesirable outcomes such as user dissatisfaction or polarization. With the rise of short-video platforms, the filter bubble has been given extra attention because these platforms rely on an unprecedented use of the recommender system to provide relevant content. In our work, we investigate the deep filter bubble, which refers to the user being exposed to narrow content within their broad interests. We accomplish this using one-year interaction data from a top short-video platform in China, which includes hierarchical data with three levels of categories for each video. We formalize our definition of a "deep" filter bubble within this context, and then explore various correlations within the data: first understanding the evolution of the deep filter bubble over time, and later revealing some of the factors that give rise to this phenomenon, such as specific categories, user demographics, and feedback type. We observe that while the overall proportion of users in a filter bubble remains largely constant over time, the depth composition of their filter bubble changes. In addition, we find that some demographic groups that have a higher likelihood of seeing narrower content and implicit feedback signals can lead to less bubble formation. Finally, we propose some ways in which recommender systems can be designed to reduce the risk of a user getting caught in a bubble.|由于过滤泡有可能导致用户不满或两极分化等不良后果，因此在在线内容平台的背景下，人们对过滤泡进行了广泛的研究。随着短视频平台的兴起，过滤器泡沫受到了额外的关注，因为这些平台前所未有地依赖推荐系统来提供相关内容。在我们的工作中，我们研究了深层过滤气泡，这是指用户暴露在狭窄的内容在他们的广泛兴趣。我们使用来自中国顶级短视频平台的为期一年的交互数据来实现这一点，该平台包括每个视频的三个层次的分类数据。在这个背景下，我们正式定义了“深度”过滤泡沫的定义，然后探索数据中的各种相关性: 首先理解深度过滤泡沫随时间的演变，然后揭示引起这种现象的一些因素，如特定类别、用户人口统计学和反馈类型。我们观察到，虽然过滤泡中用户的总体比例在很大程度上随着时间的推移保持不变，但他们的过滤泡的深度组成发生了变化。此外，我们发现，一些人口统计学群体，有较高的可能性看到较窄的内容和隐含的反馈信号，可以导致较少的泡沫形成。最后，我们提出了一些设计推荐系统的方法，以减少用户陷入泡沫的风险。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncovering+the+Deep+Filter+Bubble:+Narrow+Exposure+in+Short-Video+Recommendation)|0|
|[Mining Interest Diffusion in Online Activity Data Streams](https://doi.org/10.1145/3589335.3651259)|Shingo Higashiguchi||Modeling and forecasting such data is difficult because online activity data is high-dimensional and composed of multiple time-varying dynamics such as trends, seasonality, and diffusion of interest. In this paper, we propose D-Tracker, designed to capture latent dynamics in online activity data streams and forecast future values. Our proposed method has the following properties: (a) Interpretable: it uses interpretable differential equations to model the latent dynamics in online activity data, which enables us to capture trends and interest diffusion among locations; (b) Automatic: it determines the number of latent dynamics and the number of seasonal patterns fully automatically; (c) Scalable: it incrementally and adaptively detects shifting points of patterns for a semi-infinite collection of tensor streams. (c)Scalable : the computation time of D-Tracker is independent of the time series length. Experiments using web search volume data obtained from GoogleTrends show that the proposed method can achieve higher forecasting accuracy in less computation time than existing methods while extracting the patterns of interest diffusion among locations.|建模和预测这样的数据是困难的，因为在线活动数据是高维的，由多种时变动态组成，如趋势、季节性和兴趣的扩散。在本文中，我们提出 D-Tracker，旨在捕捉在线活动数据流中的潜在动态，并预测未来的价值。我们提出的方法具有以下特性: (a)可解释性: 它使用可解释的微分方程模拟在线活动数据中的潜在动态，这使我们能够捕获趋势和兴趣在位置之间的扩散; (b)自动化: 它完全自动地确定潜在动态的数量和季节性模式的数量; (c)可扩展性: 它增量和自适应地检测模式的移动点的张量流的半无限集合。(c)可伸缩性: D-Tracker 的计算时间与时间序列长度无关。利用从 GoogleTrends 获得的网络搜索量数据进行的实验表明，与现有方法相比，提出的方法能够以更少的计算时间获得更高的预测精度，同时提取地点之间的兴趣扩散模式。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mining+Interest+Diffusion+in+Online+Activity+Data+Streams)|0|
|[Temporal Interest Network for User Response Prediction](https://doi.org/10.1145/3589335.3648340)|Haolin Zhou, Junwei Pan, Xinyi Zhou, Xihua Chen, Jie Jiang, Xiaofeng Gao, Guihai Chen||User response prediction is essential in industrial recommendation systems, such as online display advertising. Among all the features in recommendation models, user behaviors are among the most critical. Many works have revealed that a user's behavior reflects her interest in the candidate item, owing to the semantic or temporal correlation between behaviors and the candidate. While the literature has individually examined each of these correlations, researchers have yet to analyze them in combination, that is, the semantic-temporal correlation. We empirically measure this correlation and observe intuitive yet robust patterns. We then examine several popular user interest models and find that, surprisingly, none of them learn such correlation well. To fill this gap, we propose a Temporal Interest Network (TIN) to capture the semantic-temporal correlation simultaneously between behaviors and the target. We achieve this by incorporating target-aware temporal encoding, in addition to semantic encoding, to represent behaviors and the target. Furthermore, we conduct explicit 4-way interaction by deploying target-aware attention and target-aware representation to capture both semantic and temporal correlation. We conduct comprehensive evaluations on two popular public datasets, and our proposed TIN outperforms the best-performing baselines by 0.43 GAUC, respectively. During online A/B testing in Tencent's advertising platform, TIN achieves 1.65 It has been successfully deployed in production since October 2023, serving the WeChat Moments traffic. We have released our code at https://github.com/zhouxy1003/TIN.|用户反应预测在工业推荐系统中是必不可少的，例如在线显示广告。在推荐模型的所有特性中，用户行为是最关键的。许多研究表明，由于行为和候选项之间的语义或时间相关性，用户的行为反映了她对候选项的兴趣。虽然文献已经分别检查了这些相关性，但研究人员还没有对它们进行组合分析，即语义-时间相关性。我们通过经验测量这种相关性，并观察直观但稳健的模式。然后，我们检查了几个流行的用户兴趣模型，发现令人惊讶的是，它们都没有很好地学习这种相关性。为了填补这一空白，我们提出了一个时态兴趣网络(TIN)来同时捕获行为和目标之间的语义-时态关联。我们通过结合目标感知的时间编码，以及语义编码，来表示行为和目标。此外，我们通过部署目标感知注意和目标感知表征来捕捉语义和时间的相关性，进行显性的四向交互。我们对两个流行的公共数据集进行全面的评估，我们提出的 TIN 表现优于最佳基线0.43 GAUC，分别。在腾讯的广告平台进行的在线 A/B 测试中，TIN 达到1.65。自2023年10月起，TIN 已成功投入生产，服务于微信朋友圈的流量。我们已经在 https://github.com/zhouxy1003/tin 发布了代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Temporal+Interest+Network+for+User+Response+Prediction)|0|
|[Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation](https://doi.org/10.1145/3589335.3648347)|Zezhong Zhang, Ted Tao Yuan||To speed up online testing, adaptive traffic experimentation through multi-armed bandit algorithms is rising as an essential complementary alternative to the fixed horizon A/B testing. Based on recent research on best arm identification and statistical inference with adaptively collected data, this paper derives and evaluates four Bayesian batch bandit algorithms (NB-TS, WB-TS, NB-TTTS, WB-TTTS), which are combinations of two ways of weighting batches (Naive Batch and Weighted Batch) and two Bayesian sampling strategies (Thompson Sampling and Top-Two Thompson Sampling) to adaptively determine traffic allocation. These derived Bayesian sampling algorithms are practically based on summary batch statistics of a reward metric for pilot experiments, where one of the combination WB-TTTS in this paper seems to be newly discussed. The comprehensive evaluation on the four Bayesian sampling algorithms covers trustworthiness, sensitivity and regret of a testing methodology. Moreover, the evaluation includes 4 real-world eBay experiments and 40 reproducible synthetic experiments to reveal the learnings, which covers both stationary and non-stationary situations. Our evaluation reveals that, (a) There exist false positives inflation with equivalent best arms, while seldom discussed in literatures; (b) To control false positives, connections between convergence of posterior optimal probabilities and neutral posterior reshaping are discovered; (c) WB-TTTS shows competitive recall, higher precision, and robustness against non-stationary trend; (d) NB-TS outperforms on minimizing regret trials except on precision and robustness; (e) WB-TTTS is a promising alternative if regret of A/B Testing is affordable, otherwise NB-TS is still a powerful choice with regret consideration for pilot experiments.|为了加速在线测试，通过多臂老虎机算法进行的自适应流量实验正在兴起，成为固定视野 A/B 测试的一个重要补充。基于最佳手臂识别和自适应采集数据推论统计学的最新研究，本文推导并评估了4种贝叶斯批处理盗贼算法(NB-TS，WB-TS，NB-ttTS，WB-ttTS) ，它们是两种加权批处理(朴素批处理和加权批处理)和两种贝叶斯抽样策略(汤普森抽样和前两汤普森抽样)的组合，以自适应确定流量分配。这些推导出的贝叶斯抽样算法实际上是基于飞行试验奖励度量的总结批量统计，其中 WB-TTTS 组合似乎是本文新的讨论之一。对四种贝叶斯抽样算法的综合评价包括测试方法的可信度、敏感度和遗憾度。此外，评估包括4个真实世界的 eBay 实验和40个可重复的综合实验，以揭示学习，其中包括静态和非静态情况。我们的评估表明，(a)存在假阳性通货膨胀与等效的最佳武器，而很少在文献中讨论; (b)为了控制假阳性，收敛的后验最优概率和中性后验重塑之间的联系被发现; (c) WB-TTTS 显示竞争性召回，更高的精度，和对非平稳趋势的鲁棒性; (d) NB-TS 优于最小化遗憾试验，除了在精度和鲁棒性; (e) WB-TTTS 是一个有前途的替代选择，如果遗憾的 A/B 测试是可负担的，否则 NB-TS 仍然是一个强大的选择，遗憾的考虑试点实验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Practical+Batch+Bayesian+Sampling+Algorithms+for+Online+Adaptive+Traffic+Experimentation)|0|
|[Exploring Representational Similarity Analysis to Protect Federated Learning from Data Poisoning](https://doi.org/10.1145/3589335.3651503)|Gengxiang Chen, Kai Li, Ahmed M. Abdelmoniem, Linlin You||As a paradigm that preserves privacy, Federated Learning (FL) enables distributed clients to cooperatively train global models using local datasets. However, this approach also provides opportunities for adversaries to compromise system stability by contaminating local data, such as through Label-Flipping Attacks (LFAs). In addressing these security challenges, most existing defense strategies presume the presence of an independent and identically distributed (IID) environment, resulting in suboptimal performance under Non-IID conditions. This paper introduces RSim-FL, a novel and pragmatic defense mechanism that incorporates Representational Similarity Analysis (RSA) into the detection of malevolent updates. This is achieved by calculating the similarity between uploaded local models and the global model. The evaluation, conducted against five state-of-the-art baselines, demonstrates that RSim-FL can accurately identify malicious local models and effectively mitigate divergent Label-Flipping Attacks (LFAs) in a Non-IID setting.|作为一种保护隐私的范例，联邦学习(Federated Learning，FL)使分布式客户机能够使用本地数据集合合作地训练全局模型。然而，这种方法也为对手提供了通过污染本地数据(如通过标签翻转攻击(Label-Flip Attacks，LFA))来损害系统稳定性的机会。为了应对这些安全挑战，大多数现有的防御策略都假定存在一个独立的同分布(IID)环境，从而导致在非 IID 条件下性能不理想。本文介绍了一种新的语用防御机制 RSim-FL，它将表征相似性分析(RSA)引入到恶意更新的检测中。这是通过计算上传的局部模型和全局模型之间的相似度来实现的。针对五个最先进的基线进行的评估表明，RSim-FL 能够准确识别恶意的本地模型，并在非 IID 设置中有效地缓解不同的标签翻转攻击(Label-Flipping Attacks，LFA)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Representational+Similarity+Analysis+to+Protect+Federated+Learning+from+Data+Poisoning)|0|
|[Online Sampling of Summaries from Public SPARQL Endpoints](https://doi.org/10.1145/3589335.3651543)|Thi Hoang Thi Pham, Hala SkafMolli, Pascal Molli, Brice Nédelec||Collecting statistics from online public SPARQL endpoints is hampered by their fair usage policies. These restrictions hinder several critical operations, such as aggregate query processing, portal development, and data summarization. Online sampling enables the collection of statistics while respecting fair usage policies. However, sampling has not yet been integrated into the SPARQL standard. Although integrating sampling into the SPARQL standard appears beneficial, its effectiveness must be demonstrated in a practical semantic web context. This paper investigates whether online sampling can generate summaries useful in cutting-edge SPARQL federation engines. Our experimental studies indicate that sampling allows the creation and maintenance of summaries by exploring less than 20% of datasets.|从在线公共 SPARQL 端点收集统计数据受到公平使用政策的阻碍。这些限制阻碍了一些关键操作，例如聚合查询处理、门户开发和数据汇总。在线抽样能够在尊重公平使用政策的同时收集统计数据。但是，抽样尚未集成到 SPARQL 标准中。尽管将抽样集成到 SPARQL 标准中似乎是有益的，但是它的有效性必须在实际的语义 Web 上下文中得到证明。本文研究在线抽样是否能够生成有用的摘要在尖端 SPARQL 联邦引擎。我们的实验研究表明，抽样允许通过探索少于20% 的数据集来创建和维护摘要。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Sampling+of+Summaries+from+Public+SPARQL+Endpoints)|0|
|[Coordinated Activity Modulates the Behavior and Emotions of Organic Users: A Case Study on Tweets about the Gaza Conflict](https://doi.org/10.1145/3589335.3651483)|Priyanka Dey, Luca Luceri, Emilio Ferrara||Social media has become a crucial conduit for the swift dissemination of information during global crises. However, this also paves the way for the manipulation of narratives by malicious actors. This research delves into the interaction dynamics between coordinated (malicious) entities and organic (regular) users on Twitter amidst the Gaza conflict. Through the analysis of approximately 3.5 million tweets from over 1.3 million users, our study uncovers that coordinated users significantly impact the information landscape, successfully disseminating their content across the network: a substantial fraction of their messages is adopted and shared by organic users. Furthermore, the study documents a progressive increase in organic users' engagement with coordinated content, which is paralleled by a discernible shift towards more emotionally polarized expressions in their subsequent communications. These results highlight the critical need for vigilance and a nuanced understanding of information manipulation on social media platforms.|社交媒体已成为在全球危机期间迅速传播信息的重要渠道。然而，这也为恶意行为者操纵叙述铺平了道路。这项研究深入探讨了在加沙冲突中协调(恶意)实体和有机(常规) Twitter 用户之间的互动动态。通过对超过130万用户的大约350万条推文的分析，我们的研究发现，协调的用户显著地影响了信息景观，成功地在网络上传播他们的内容: 他们的信息的很大一部分被有机用户采用和分享。此外，该研究还记录了有机用户参与协调内容的逐渐增加，与此同时，他们在随后的交流中明显地转向更加情绪化的表达方式。这些结果突出表明，对于社交媒体平台上的信息操纵活动，亟需保持警惕，并有细致入微的理解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Coordinated+Activity+Modulates+the+Behavior+and+Emotions+of+Organic+Users:+A+Case+Study+on+Tweets+about+the+Gaza+Conflict)|0|
|[From Files to Streams: Revisiting Web History and Exploring Potentials for Future Prospects](https://doi.org/10.1145/3589335.3652001)|Lucas Vogel, Thomas Springer, Matthias Wählisch||Over the last 30 years, the World Wide Web has changed significantly. In this paper, we argue that common practices to prepare web pages for delivery conflict with many efforts to present content with minimal latency, one fundamental goal that pushed changes in the WWW. To bolster our arguments, we revisit reasons that led to changes of HTTP and compare them systematically with techniques to prepare web pages. We found that the structure of many web pages leverages features of HTTP/1.1 but hinders the use of recent HTTP features to present content quickly. To improve the situation in the future, we propose fine-grained content segmentation. This would allow to exploit streaming capabilities of recent HTTP versions and to render content as quickly as possible without changing underlying protocols or web browsers.|在过去的30年里，万维网发生了巨大的变化。在本文中，我们认为，准备交付网页的常见做法与以最小延迟呈现内容的许多努力相冲突，延迟是推动 WWW 变化的一个基本目标。为了支持我们的论点，我们重新审视导致 HTTP 变化的原因，并将其与准备网页的技术进行系统的比较。我们发现许多网页的结构利用了 HTTP/1.1的特性，但是阻碍了使用最新的 HTTP 特性来快速显示内容。为了改善未来的情况，我们提出了细粒度内容分割。这将允许利用最新 HTTP 版本的流功能，并在不改变底层协议或 Web 浏览器的情况下尽可能快地呈现内容。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Files+to+Streams:+Revisiting+Web+History+and+Exploring+Potentials+for+Future+Prospects)|0|
|[Revisiting the Behavioral Foundations of User Modeling Algorithms](https://doi.org/10.1145/3589334.3649114)|Jon M. Kleinberg||One of the fundamental problems that platform algorithms face is the process of inferring user preferences from observed behavior; the vast amounts of data a platform collects become much less useful if they cannot effectively inform this type of inference. Traditional approaches to this problem rely on an often unstated revealed-preference assumption: that choice reveals preference. Yet a long line of work in psychology and behavioral economics reveals the gaps that can open up between choice and preference, and experience with platform dynamics makes clear how it can arise in some of the most basic online settings; for example, we might choose content to consume in the present and then later regret the time we spent on it. More generally, behavioral biases and inconsistent preferences make it highly challenging to appropriately interpret the user data that we observe. We discuss a set of models and algorithms that address this challenge through a process of "inversion", in which an algorithm must try inferring mental states that are not directly measured in the data. The talk is based on joint work with Jens Ludwig, Sendhil Mullainathan, and Manish Raghavan.|平台算法面临的一个基本问题是根据观察到的行为推断用户偏好的过程; 如果平台收集的大量数据不能有效地提供这种推断，那么它们就变得不那么有用了。解决这个问题的传统方法依赖于一个通常未说明的显示偏好假设: 这种选择显示偏好。然而，心理学和行为经济学的一长串研究揭示了选择和偏好之间的差距，平台动态的经验清楚地表明，在一些最基本的网络环境中，这种差距是如何产生的; 例如，我们可能会选择目前消费的内容，然后后悔花时间在这上面。更一般地说，行为偏差和不一致的偏好使得适当地解释我们观察到的用户数据变得非常具有挑战性。我们讨论了一组模型和算法，通过一个“反转”的过程来解决这个问题，在这个过程中，算法必须尝试推断心理状态，这些心理状态不是直接测量的数据。这次演讲是基于与延斯 · 路德维希、森德希尔穆拉伊特丹和马尼什 · 拉加万的合作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+the+Behavioral+Foundations+of+User+Modeling+Algorithms)|0|
|[User Response in Ad Auctions: An MDP Formulation of Long-term Revenue Optimization](https://doi.org/10.1145/3589334.3645495)|Yang Cai, Zhe Feng, Christopher Liaw, Aranyak Mehta, Grigoris Velegkas||We propose a new Markov Decision Process (MDP) model for ad auctions to capture the user response to the quality of ads, with the objective of maximizing the long-term discounted revenue. By incorporating user response, our model takes into consideration all three parties involved in the auction (advertiser, auctioneer, and user). The state of the user is modeled as a user-specific click-through rate (CTR) with the CTR changing in the next round according to the set of ads shown to the user in the current round. We characterize the optimal mechanism for this MDP as a Myerson's auction with a notion of modified virtual value, which relies on the value distribution of the advertiser, the current user state, and the future impact of showing the ad to the user. Leveraging this characterization, we design a sample-efficient and computationally-efficient algorithm which outputs an approximately optimal policy that requires only sample access to the true MDP and the value distributions of the bidders. Finally, we propose a simple mechanism built upon second price auctions with personalized reserve prices and show it can achieve a constant-factor approximation to the optimal long term discounted revenue.|我们提出了一种新的马可夫决策过程拍卖模型来捕捉用户对广告质量的反应，目标是最大化长期折扣收入。通过合并用户反应，我们的模型考虑了所有参与拍卖的三方(广告商、拍卖商和用户)。用户的状态被建模为一个用户特定的点进率(ctrR) ，ctrR 在下一轮中根据当前轮中显示给用户的广告集发生变化。我们将这个 MDP 的最优机制描述为一个 Myerson 拍卖，它具有修改后的虚拟价值的概念，依赖于广告商的价值分布、当前用户状态以及向用户展示广告的未来影响。利用这个角色塑造，我们设计了一个样本效率和计算效率高的算法，它输出一个近似最优的策略，只需要对真正的 MDP 和投标人的价值分布进行样本访问。最后，我们提出了一个基于个性化保留价格的第二价格拍卖的简单机制，并证明了该机制可以实现对最优长期折扣收益的常数逼近。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User+Response+in+Ad+Auctions:+An+MDP+Formulation+of+Long-term+Revenue+Optimization)|0|
|[Exploring Neural Scaling Law and Data Pruning Methods For Node Classification on Large-scale Graphs](https://doi.org/10.1145/3589334.3645571)|Zhen Wang, Yaliang Li, Bolin Ding, Yule Li, Zhewei Wei||Recently, how the model performance scales with the training sample size has been extensively studied for large models on vision and language related domains. Nevertheless, the ubiquitous node classification tasks on web-scale graphs were ignored, where the traits of these tasks, such as non-IIDness and transductive setting, are likely to cause different scaling laws and motivate novel techniques to beat the law. Therefore, we first explore the neural scaling law for node classification tasks on three large-scale graphs. Then, we benchmark several state-of-the-art data pruning methods on these tasks, not only validating the possibility of improving the original unsatisfactory power law but also gaining insights into a hard-and-representative principle on picking an effective subset of training nodes. Moreover, we leverage the transductive setting to propose a novel data pruning method, which instantiates our principle in a test set-targeted manner. Our method consistently outperforms related methods on all three datasets. Meanwhile, we utilize a PAC-Bayesian framework to analyze our method, extending prior results to account for both hardness and representativeness. In addition to a promising way to ease GNN training on web-scale graphs, our study offers knowledge of the relationship between training nodes and GNN generalization.|近年来，在视觉和语言相关领域的大型模型中，模型性能如何随训练样本量的变化而变化的问题得到了广泛的研究。然而，网络尺度图上普遍存在的节点分类任务被忽视了，这些任务的特点，如非 IID 性和传导性设置，可能会导致不同的尺度规律，并激励新技术打破这一规律。因此，我们首先探讨了三个大规模图上节点分类任务的神经尺度规律。然后，我们对这些任务的几种最先进的数据剪枝方法进行了基准测试，不仅验证了改进原始不满意幂律的可能性，而且深入了解了挑选有效训练节点子集的硬性和代表性原则。此外，我们利用传导设置提出了一种新的数据剪枝方法，它以测试集目标的方式实例化我们的原则。我们的方法在所有三个数据集上的性能始终优于相关方法。同时，我们利用 PAC-Bayes 框架来分析我们的方法，扩展了先前的结果来考虑硬度和代表性。除了一个有希望的方法来简化网络尺度图的 GNN 训练，我们的研究提供了训练节点和 GNN 推广之间的关系的知识。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Neural+Scaling+Law+and+Data+Pruning+Methods+For+Node+Classification+on+Large-scale+Graphs)|0|
|[Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised Semantic Hashing](https://doi.org/10.1145/3589334.3645440)|Liyang He, Zhenya Huang, Jiayu Liu, Enhong Chen, Fei Wang, Jing Sha, Shijin Wang||Unsupervised semantic hashing has emerged as an indispensable technique for fast image search, which aims to convert images into binary hash codes without relying on labels. Recent advancements in the field demonstrate that employing large-scale backbones (e.g., ViT) in unsupervised semantic hashing models can yield substantial improvements. However, the inference delay has become increasingly difficult to overlook. Knowledge distillation provides a means for practical model compression to alleviate this delay. Nevertheless, the prevailing knowledge distillation approaches are not explicitly designed for semantic hashing. They ignore the unique search paradigm of semantic hashing, the inherent necessities of the distillation process, and the property of hash codes. In this paper, we propose an innovative Bit-mask Robust Contrastive knowledge Distillation (BRCD) method, specifically devised for the distillation of semantic hashing models. To ensure the effectiveness of two kinds of search paradigms in the context of semantic hashing, BRCD first aligns the semantic spaces between the teacher and student models through a contrastive knowledge distillation objective. Additionally, to eliminate noisy augmentations and ensure robust optimization, a cluster-based method within the knowledge distillation process is introduced. Furthermore, through a bit-level analysis, we uncover the presence of redundancy bits resulting from the bit independence property. To mitigate these effects, we introduce a bit mask mechanism in our knowledge distillation objective. Finally, extensive experiments not only showcase the noteworthy performance of our BRCD method in comparison to other knowledge distillation methods but also substantiate the generality of our methods across diverse semantic hashing models and backbones. The code for BRCD is available at https://github.com/hly1998/BRCD.|无监督语义哈希已经成为快速图像搜索不可或缺的技术，其目的是在不依赖标签的情况下将图像转换为二进制哈希码。该领域的最新进展表明，在无监督语义哈希模型中使用大规模骨干(例如，ViT)可以产生实质性的改进。然而，推理延迟已变得越来越难以忽视。知识提取为实际的模型压缩提供了一种方法来减少这种延迟。然而，流行的知识提取方法并没有明确地为语义哈希设计。它们忽略了语义哈希的独特搜索范式、蒸馏过程的内在必要性以及哈希码的性质。本文提出了一种新颖的位掩码鲁棒对比知识提取(BRCD)方法，专门用于提取语义哈希模型。为了保证两种搜索范式在语义哈希环境下的有效性，BRCD 首先通过对比知识提取目标对教师和学生模型之间的语义空间进行对齐。此外，为了消除噪声增量，确保鲁棒优化，在知识精馏过程中引入了一种基于聚类的方法。此外，通过位级分析，我们发现由于位无关性质而导致的冗余位的存在。为了减轻这些影响，我们在知识提取目标中引入了位掩码机制。最后，广泛的实验不仅展示了我们的 BRCD 方法相对于其他知识提取方法的显著性能，而且还证实了我们的方法在不同的语义哈希模型和主干上的通用性。BRCD 的代码可在 https://github.com/hly1998/BRCD 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bit-mask+Robust+Contrastive+Knowledge+Distillation+for+Unsupervised+Semantic+Hashing)|0|
|[Perennial Semantic Data Terms of Use for Decentralized Web](https://doi.org/10.1145/3589334.3645631)|Rui Zhao, Jun Zhao||In today's digital landscape, the Web has become increasingly centralized, raising concerns about user privacy violations. Decentralized Web architectures, such as Solid, offer a promising solution by empowering users with better control over their data in their personal `Pods'. However, a significant challenge remains: users must navigate numerous applications to decide which application can be trusted with access to their data Pods. This often involves reading lengthy and complex Terms of Use agreements, a process that users often find daunting or simply ignore. This compromises user autonomy and impedes detection of data misuse. We propose a novel formal description of Data Terms of Use (DToU), along with a DToU reasoner. Users and applications specify their own parts of the DToU policy with local knowledge, covering permissions, requirements, prohibitions and obligations. Automated reasoning verifies compliance, and also derives policies for output data. This constitutes a “perennial” DToU language, where the policy authoring only occurs once, and we can conduct ongoing automated checks across users, applications and activity cycles. Our solution is built on Turtle, Notation 3 and RDF Surfaces, for the language and the reasoning engine. It ensures seamless integration with other semantic tools for enhanced interoperability. We have successfully integrated this language into the Solid framework, and conducted performance benchmark. We believe this work demonstrates a practicality of a perennial DToU language and the potential of a paradigm shift to how users interact with data and applications in a decentralized Web, offering both improved privacy and usability.|在今天的数字世界里，网络已经变得越来越集中，引起了人们对侵犯用户隐私的担忧。像 Solid 这样的分散式 Web 架构提供了一种有前途的解决方案，它赋予用户更好地控制他们个人“ Pods”中的数据的权力。然而，一个重大的挑战仍然存在: 用户必须浏览大量的应用程序，以决定哪些应用程序可以信任访问他们的数据 Pods。这通常涉及阅读冗长而复杂的使用条款协议，用户经常发现这个过程令人生畏，或者干脆忽略它。这损害了用户的自主性，阻碍了对数据误用的检测。我们提出了一个新的数据使用条款(DToU)的形式描述，以及一个 DToU 推理器。用户和应用程序使用本地知识指定 DToU 策略中自己的部分，包括许可、要求、禁止和义务。自动推理验证遵从性，并派生输出数据的策略。这构成了一种“常年”DToU 语言，其中策略创作只发生一次，我们可以跨用户、应用程序和活动周期进行持续的自动检查。我们的解决方案是基于 Turtle、 Notation3和 RDF Surface 构建的，用于语言和推理引擎。它确保与其他语义工具的无缝集成，以增强互操作性。我们已经成功地将该语言集成到 Solid 框架中，并进行了性能基准测试。我们相信这项工作展示了长期使用的 DToU 语言的实用性，以及向用户如何在分散的 Web 中与数据和应用程序交互的范式转变的潜力，提供了更好的隐私性和可用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Perennial+Semantic+Data+Terms+of+Use+for+Decentralized+Web)|0|
|[Enhancing Fairness in Meta-learned User Modeling via Adaptive Sampling](https://doi.org/10.1145/3589334.3645369)|Zheng Zhang, Qi Liu, Zirui Hu, Yi Zhan, Zhenya Huang, Weibo Gao, Qingyang Mao||Meta-learning has been widely employed to tackle the cold-start problem in user modeling. Similar to a guidebook for a new traveler, meta-learning significantly affects decision-making for new users in crucial scenarios, such as career recommendations. Consequently, the issue of fairness in meta-learning has gained paramount importance. Several methods have been proposed to mitigate unfairness in meta-learning and have shown promising results. However, a fundamental question remains unexplored: What is the critical factor leading to unfairness in meta-learned user modeling? Through the theoretical analysis that integrates the meta-learning paradigm with group fairness metrics, we identify group proportion imbalance as a critical factor. Subsequently, in order to mitigate the impact of this factor, we introduce a novel Fairness-aware Adaptive Sampling framework for meTa-learning, abbreviated as FAST. Its core concept involves adaptively adjusting the sampling distribution for different user groups during the interleaved training process of meta-learning. Furthermore, we provide theoretical guarantees demonstrating the convergence of FAST. Finally, empirical experiments conducted on three datasets reveal that FAST effectively enhances fairness while maintaining high accuracy. The code for FAST is available at https://github.com/zhengz99/FAST.|元学习已被广泛应用于解决用户建模中的冷启动问题。类似于一本旅行指南，元学习显著影响新用户在关键情景下的决策，如职业推荐。因此，元学习中的公平性问题变得至关重要。一些方法已经被提出来减少元学习中的不公平现象，并且已经显示出有希望的结果。然而，一个基本的问题仍然没有被探索: 什么是导致元学习用户建模不公平的关键因素？通过将元学习范式与群体公平指标相结合的理论分析，我们发现群体比例失衡是一个关键因素。随后，为了减轻这个因素的影响，我们引入了一个新的公平感知的自适应抽样框架，简称为 FAST。其核心概念是在元学习的交叉训练过程中，针对不同的用户群体自适应地调整抽样分布。此外，我们还提供了证明 FAST 收敛性的理论保证。最后，通过对三个数据集的实验表明，FAST 在保持高精度的同时，有效地提高了公平性。FAST 的代码可在 https://github.com/zhengz99/FAST 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Fairness+in+Meta-learned+User+Modeling+via+Adaptive+Sampling)|0|
|[MMLSCU: A Dataset for Multi-modal Multi-domain Live Streaming Comment Understanding](https://doi.org/10.1145/3589334.3645677)|Zixiang Meng, Qiang Gao, Di Guo, Yunlong Li, Bobo Li, Hao Fei, Shengqiong Wu, Fei Li, Chong Teng, Donghong Ji||With the increasing popularity of live streaming, the interactions from viewers during a live streaming can provide more specific and constructive feedback for both the streamer and platform. In such scenario, the primary and most direct feedback method from the audience is through comments. Thus, mining these live streaming comments to unearth the intentions behind them and, in turn, aiding streamers to enhance their live streaming quality is significant for the well development of live streaming ecosystem. To this end, we introduce the MMLSCU dataset, containing 50,129 intention-annotated comments across multiple modalities (text, images, vi-deos, audio) from eight streaming domains. Using multimodal pretrained large model and drawing inspiration from the Chain of Thoughts (CoT) concept, we implement an end-to-end model to sequentially perform the following tasks: viewer comment intent detection ➛ intent cause mining ➛ viewer comment explanation ➛ streamer policy suggestion. We employ distinct branches for video and audio to process their respective modalities. After obtaining the video and audio representations, we conduct a multimodal fusion with the comment. This integrated data is then fed into the large language model to perform inference across the four tasks following the CoT framework. Experimental results indicate that our model outperforms three multimodal classification baselines on comment intent detection and streamer policy suggestion, and one multimodal generation baselines on intent cause mining and viewer comment explanation. Compared to the models using only text, our multimodal setting yields superior outcomes. Moreover, incorporating CoT allows our model to enhance comment interpretation and more precise suggestions for the streamers. Our proposed dataset and model will bring new research attention on multimodal live streaming comment understanding.|随着流媒体直播的日益普及，观众在直播过程中的互动可以为流媒体和平台提供更具体、更有建设性的反馈。在这种情况下，来自听众的主要和最直接的反馈方法是通过评论。因此，挖掘这些直播评论，挖掘其背后的意图，进而帮助流媒体提高直播质量，对于直播生态系统的良好发展具有重要意义。为此，我们引入了 MMLSCU 数据集，其中包含来自8个流域的50,129个跨多种模式(文本、图像、视频、音频)的意向注释评论。利用多模态预训练大模型，借鉴思维链概念，实现端到端模型，依次完成以下任务: 浏览者评论意图检测、意图挖掘、浏览者评论解释、流媒体政策建议。我们使用不同的视频和音频分支来处理它们各自的模式。在获得视频和音频表示之后，我们对评论进行多模态融合。然后将这些集成数据输入到大型语言模型中，以便在遵循 CoT 框架的四个任务之间执行推理。实验结果表明，该模型在评论意图检测和流策略建议方面优于三个多模态分类基线，在意图挖掘和评论解释方面优于一个多模态生成基线。与只使用文本的模型相比，我们的多模式设置产生更好的结果。此外，合并 CoT 允许我们的模型增强评论解释和更精确的建议为主播。我们提出的数据集和模型将为多模式流媒体评论理解带来新的研究重点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MMLSCU:+A+Dataset+for+Multi-modal+Multi-domain+Live+Streaming+Comment+Understanding)|0|
|[Entire Chain Uplift Modeling with Context-Enhanced Learning for Intelligent Marketing](https://doi.org/10.1145/3589335.3648320)|Yinqiu Huang, Shuli Wang, Min Gao, Xue Wei, Changhao Li, Chuan Luo, Yinhua Zhu, Xiong Xiao, Yi Luo||Uplift modeling, vital in online marketing, seeks to accurately measure the impact of various strategies, such as coupons or discounts, on different users by predicting the Individual Treatment Effect (ITE). In an e-commerce setting, user behavior follows a defined sequential chain, including impression, click, and conversion. Marketing strategies exert varied uplift effects at each stage within this chain, impacting metrics like click-through and conversion rate. Despite its utility, existing research has neglected to consider the inter-task across all stages impacts within a specific treatment and has insufficiently utilized the treatment information, potentially introducing substantial bias into subsequent marketing decisions. We identify these two issues as the chain-bias problem and the treatment-unadaptive problem. This paper introduces the Entire Chain UPlift method with context-enhanced learning (ECUP), devised to tackle these issues. ECUP consists of two primary components: 1) the Entire Chain-Enhanced Network, which utilizes user behavior patterns to estimate ITE throughout the entire chain space, models the various impacts of treatments on each task, and integrates task prior information to enhance context awareness across all stages, capturing the impact of treatment on different tasks, and 2) the Treatment-Enhanced Network, which facilitates fine-grained treatment modeling through bit-level feature interactions, thereby enabling adaptive feature adjustment. Extensive experiments on public and industrial datasets validate ECUPs effectiveness. Moreover, ECUP has been deployed on the Meituan food delivery platform, serving millions of daily active users, with the related dataset released for future research.|提升模型在网络营销中至关重要，它试图通过预测个人待遇效应(ITE)来准确测量各种策略(如优惠券或折扣)对不同用户的影响。在电子商务环境中，用户行为遵循定义的顺序链，包括印象、单击和转换。营销策略在这个链条的每个阶段都发挥着不同的提升作用，影响着点击率和转化率等指标。尽管已有的研究很有用，但是它忽略了考虑特定治疗中所有阶段的任务间影响，并且没有充分利用治疗信息，这可能会给随后的营销决策带来很大的偏差。我们认为这两个问题是链式偏差问题和治疗不适应问题。本文介绍了基于上下文增强学习(ECUP)的整链提升方法，该方法是为解决这些问题而设计的。ECUP 由两个主要组成部分组成: 1)整个链增强网络，利用用户行为模式来估计整个链空间中的 ITE，模拟治疗对每个任务的各种影响，并整合任务先验信息以增强所有阶段的上下文意识，捕获治疗对不同任务的影响，2)治疗增强网络，通过位级特征交互促进细粒度治疗建模，从而实现自适应特征调整。在公共和工业数据集上的大量实验验证了 ECUPs 的有效性。此外，ECUP 已经部署在美团食品配送平台上，为数以百万计的日常活跃用户提供服务，并发布了相关的数据集，供日后研究使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Entire+Chain+Uplift+Modeling+with+Context-Enhanced+Learning+for+Intelligent+Marketing)|0|
|[Large Multimodal Model Compression via Iterative Efficient Pruning and Distillation](https://doi.org/10.1145/3589335.3648321)|Maolin Wang, Yao Zhao, Jiajia Liu, Jingdong Chen, Chenyi Zhuang, Jinjie Gu, Ruocheng Guo, Xiangyu Zhao||The deployment of Large Multimodal Models (LMMs) within Ant Group has significantly advanced multimodal tasks in payment, security, and advertising, notably enhancing advertisement audition tasks in Alipay. However, the deployment of such sizable models introduces challenges, particularly in increased latency and carbon emissions, which are antithetical to the ideals of Green AI. This paper introduces a novel multi-stage compression strategy for our proprietary LLM, AntGMM. Our methodology pivots on three main aspects: employing small training sample sizes, addressing multi-level redundancy through multi-stage pruning, and introducing an advanced distillation loss design. In our research, we constructed a dataset, the Multimodal Advertisement Audition Dataset (MAAD), from real-world scenarios within Alipay, and conducted experiments to validate the reliability of our proposed strategy. Furthermore, the effectiveness of our strategy is evident in its operational success in Alipay's real-world multimodal advertisement audition for three months from September 2023. Notably, our approach achieved a substantial reduction in latency, decreasing it from 700ms to 90ms, while maintaining online performance with only a slight performance decrease. Moreover, our compressed model is estimated to reduce electricity consumption by approximately 75 million kWh annually compared to the direct deployment of AntGMM, demonstrating our commitment to green AI initiatives.|蚂蚁集团大型多通道模型(LMM)的部署显著提高了支付、安全和广告的多通道任务，显著提高了支付宝的广告听力任务。然而，部署如此大规模的模型带来了挑战，尤其是在延迟和碳排放增加方面，这与绿色人工智能的理想背道而驰。本文介绍了一种新的多级压缩策略，用于我们专有的 LLM-ANTGMM。我们的方法主要集中在三个方面: 采用小样本量的训练样本，通过多级剪枝来解决多级冗余问题，以及引入先进的蒸馏损失设计。在本研究中，我们从支付宝的真实场景中构建了一个数据集，叫做多模式广告审核数据集(Multimode Advertisement Audition Dataset，MAAD) ，并通过实验验证了我们提出的策略的可靠性。此外，我们的战略的有效性是显而易见的，在支付宝的现实世界的多模式广告试听从2023年9月三个月的运营成功。值得注意的是，我们的方法实现了延迟的大幅减少，从700毫秒减少到90毫秒，同时保持在线性能，只有轻微的性能下降。此外，我们的压缩模型估计每年比直接部署的 AntGMM 减少约7500万千瓦小时的电力消耗，这表明我们对绿色人工智能倡议的承诺。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Multimodal+Model+Compression+via+Iterative+Efficient+Pruning+and+Distillation)|0|
|[Mystique: A Budget Pacing System for Performance Optimization in Online Advertising](https://doi.org/10.1145/3589335.3648342)|Rotem Stram, Rani Abboud, Alex Shtoff, Oren Somekh, Ariel Raviv, Yair Koren||Online advertising plays a pivotal role in sustaining the accessibility of free content on the Internet, serving as a primary revenue source for websites and online services. This dynamic marketplace sees advertisers allocating budgets and competing for the opportunity to present ads to users engaging with web pages, online services, and mobile apps. Modern online advertising often employs first-price auctions to determine ad placements. Yet, conducting auctions as isolated events in a greedy manner, may lead to sub-optimal results, necessitating some form of budget pacing. Traditionally, budget pacing has been achieved through hard throttling, where ads or campaigns are selectively made eligible for each auction using a biased coin-toss with a specified probability (or pacing-signal). More recently, the pacing signal has been leveraged to soft throttle ads, and is used as a multiplicative factor on their bids, thus enabling participation in all auctions but with potentially modified bids. In this study, we introduce Mystique, a "soft" throttling-based budget pacing system. Mystique operates on two levels: it utilizes spending data to establish a daily target spending curve for each campaign, and continuously updates a pacing signal to align the actual spending with this curve. Our offline evaluation in a complex simulated marketplace, demonstrates Mystique's ability to outperform several baseline algorithms, enabling budget depletion while securing more opportunities. Mystique has been in production for several years now, serving a major native advertising marketplace, and successfully pacing over one billion USD annually.|在线广告在维持互联网上免费内容的可获得性方面发挥着关键作用，是网站和在线服务的主要收入来源。这个动态的市场看到广告商分配预算和竞争的机会，提出广告给用户从事网页，在线服务和移动应用程序。现代在线广告通常使用第一价格拍卖来确定广告位置。然而，以贪婪的方式将拍卖作为孤立事件进行，可能会导致次优结果，从而需要某种形式的预算节奏。传统上，预算节奏已经通过硬节流，其中广告或活动是有选择地使每个拍卖有资格使用一个具有特定概率(或节奏信号)的有偏见的掷硬币。最近，节奏信号已经被用于软油门广告，并被用作其出价的乘数因素，从而使参与所有拍卖，但可能修改出价。在这项研究中，我们介绍了魔形，一个“软”节流为基础的预算起搏系统。魔形女在两个层面上运作: 它利用消费数据建立每个战役的每日目标消费曲线，并不断更新步伐信号，使实际消费与这个曲线保持一致。我们在一个复杂的模拟市场中的离线评估，展示了魔形女超越几个基线算法的能力，使预算耗尽，同时获得更多的机会。魔形女已经生产了几年，服务于一个主要的原生广告市场，并成功地每年超过十亿美元。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mystique:+A+Budget+Pacing+System+for+Performance+Optimization+in+Online+Advertising)|0|
|[HBIAS FedAvg: Smooth Federated Learning Transition for In-use Edge Models](https://doi.org/10.1145/3589335.3651518)|Anupam Gupta, Pabitra Mitra, Sudip Misra||Federated learning is an approach for privacy preserving machine learning. It is increasingly being used in a number of classification as well as ranking tasks. Protocols for federated learning involve model update at the edge devices and aggregation at the central servers over multiple rounds. In practice, most deep learning models deployed on the edge are already trained and in-use. Federated learning protocols lead to an oscillation in the performance of these local models over the epochs. The drop in accuracy is more prominent in the early phases. In this article, we study such effects for the popular FedAvg federated learning algorithm and suggest the modified HBIAS FedAvg algorithm. The algorithm proposes a heuristic based initialization adoption strategy for this purpose. We find that this protocol leads to smoother performance variation for experiments on benchmark datasets.|联邦学习是一种保护隐私的机器学习方法。它越来越多地被用于许多分类和排序任务。联邦学习协议包括边缘设备上的模型更新和多轮中央服务器上的聚合。在实践中，部署在边缘的大多数深度学习模型已经被训练和使用。联邦学习协议导致这些局部模型在各个时代的性能发生振荡。精度的下降在早期阶段更为突出。本文研究了流行的 FedAvg 联邦学习算法的这种效应，并提出了一种改进的 HBIAS FedAvg 联邦学习算法。为此，该算法提出了一种基于启发式的初始化采用策略。我们发现该协议可以使基准测试数据集的性能变化更加平滑。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HBIAS+FedAvg:+Smooth+Federated+Learning+Transition+for+In-use+Edge+Models)|0|
|[The Effect of Alter Ego Accounts on A/B Tests in Social Networks](https://doi.org/10.1145/3589335.3651569)|Katherine Avery, Amir Houmansadr, David D. Jensen|College of Information and Computer Sciences, University of Massachusetts Amherst, Amherst, MA, USA|Social network users often maintain multiple active accounts, sometimes referred to as alter egos. Examples of alter egos include personal and professional accounts or named and anonymous accounts. If alter egos are common on a platform, they can affect the results of A/B testing because a user's alter egos can influence each other. For a single user, one account may be assigned treatment, while another is assigned control. Alter-ego bias is relevant when the treatment affects the individual user rather than the account. Through experimentation and theoretical analysis, we examine the worst and expected case bias for different numbers of alter egos and for a variety of network structures and peer effect strengths. We show that alter egos moderately bias the results of simulated A/B tests on several network structures, including a real-world Facebook subgraph and several types of synthetic networks: small world networks, forest fire networks, stochastic block models, and a worst-case structure. We also show that bias increases with the number of alter egos and that different network structures have different upper bounds on bias.|社交网络用户经常拥有多个活跃账户，有时也被称为“另一个自我”。另一个自我的例子包括个人的和专业的帐户或指定的和匿名的帐户。如果改变的自我在一个平台上很常见，他们会影响 A/B 测试的结果，因为用户的改变的自我可以相互影响。对于单个用户，一个帐户可能被分配处理，而另一个帐户被分配控制。当治疗影响的是个人用户而不是账户时，改变自我的偏见是相关的。通过实验和理论分析，我们考察了不同数量的改变自我和不同的网络结构和同伴效应强度的最坏情况和预期情况偏差。我们发现，在几种网络结构上，包括一个真实世界的 Facebook 子图和几种类型的合成网络: 小世界网络，森林火灾网络，随机块模型和最坏情况结构，改变自我适度偏差模拟 A/B 测试的结果。我们还发现，偏见随着改变自我的人数的增加而增加，不同的网络结构对偏见的上界也不同。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Effect+of+Alter+Ego+Accounts+on+A/B+Tests+in+Social+Networks)|0|
|[CardiO: Predicting Cardinality from Online Sources](https://doi.org/10.1145/3589335.3651477)|Shrestha Ghosh, Simon Razniewski, Damien Graux, Gerhard Weikum||Count questions are an important type of information need, though often present in noisy, contradictory, or semantically not fully aligned form on the Web. In this work, we propose CardiO, a lightweight and modular framework for searching entity counts on the Web. CardiO extracts all counts from a set of relevant Web snippets, and infers the most central count based on semantic and numeric distances from other candidates. In the absence of supporting evidence, the system relies on peer sets of similar size, to provide an estimate. Experiments show that CardiO can produce accurate and traceable counts better than small LLM-only methods. Although larger models have higher precision, when used to enhance CardiO components, they do not contribute to the final precision or recall.|计数问题是一种重要的信息需求类型，尽管在 Web 上经常以嘈杂、矛盾或语义不完全一致的形式出现。在这项工作中，我们提出了 CardiO，一个轻量级和模块化的框架，用于在 Web 上搜索实体计数。CardiO 从一组相关的 Web 片段中提取所有计数，并根据与其他候选者的语义和数字距离推断出最集中的计数。在缺乏支持性证据的情况下，该系统依赖于大小相似的对等集来提供估计。实验表明，CardiO 能够比单纯的 LLM 方法更好地产生准确和可追踪的计数。虽然较大的型号有更高的精确度，当用于增强心脏组件，他们并不有助于最终的精确度或召回。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CardiO:+Predicting+Cardinality+from+Online+Sources)|0|
|[An Identity Alignment Method based on Online Tracking](https://doi.org/10.1145/3589335.3651469)|Ruisheng Shi, Zhiyuan Peng, Tong Fu, Lina Lan, Jiaqi Zeng, Yuyang Shi, Jinqiao Shi, Shenwen Lin, Lin Li||Companies track user data and sell it to advertisers. They claim to protect user privacy by anonymization, but our research shows that significant risks are still involved. Even with anonymous data, attackers can identify users on other websites from tracking records. We propose an identity alignment method of deanonymization attack, which analyzes tracker data to align identities. We explore the key factors affecting the effectiveness of identity alignment and analyze its impact on user privacy. We use crawling data to create tracker data close to ground-truth scenarios and propose an evaluation framework for online tracking based identity alignment.|公司跟踪用户数据并将其出售给广告商。他们声称通过匿名保护用户隐私，但我们的研究表明，仍然存在重大风险。即使使用匿名数据，攻击者也可以通过跟踪记录来识别其他网站上的用户。提出了一种去匿名化攻击的身份对齐方法，该方法通过分析跟踪数据来对齐身份。探讨了影响身份匹配有效性的关键因素，分析了身份匹配对用户隐私的影响。我们使用爬行数据来创建接近地面真实场景的跟踪器数据，并提出了一个基于身份对齐的在线跟踪评估框架。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Identity+Alignment+Method+based+on+Online+Tracking)|0|
|[Semantic interlinking of Immigration Data using LLMs for Knowledge Graph Construction](https://doi.org/10.1145/3589335.3651557)|Radhakrishnan Venkatakrishnan, Emrah Tanyildizi, M. Abdullah Canbaz||The challenge of managing immigration data is exacerbated by its reliance on paper-based, evidence-driven records maintained by legal professionals, creating obstacles for efficient processing and analysis due to inherent trust issues with AI-based systems. This paper introduces a cutting-edge framework to surmount these hurdles by synergizing Large Language Models (LLMs) with Knowledge Graphs (KGs), revolutionizing traditional data handling methods. Our method transforms archaic, paper-based immigration records into a structured, interconnected knowledge network that intricately mirrors the legal and procedural nuances of immigration, ensuring a dynamic and trustworthy platform for data analysis. Utilizing LLMs, we extract vital entities and relationships from diverse legal documents to forge a comprehensive knowledge graph, encapsulating the complex legalities and procedural disparities in immigration processes and mapping the multifaceted interactions among stakeholders like applicants, sponsors, and legal experts. This graph not only facilitates a deep dive into the legal stipulations but also incorporates them, significantly boosting the system's reliability and precision. With the integration of Retrieval Augmented Generation (RAG) for exact, context-aware data retrieval and Augmented Knowledge Creation for developing a conversational interface via LLMs, our framework offers a scalable, adaptable solution to immigration data management. This innovative amalgamation of LLMs, KGs, and RAG techniques marks a paradigm shift towards more informed, efficient, and trustworthy decision-making in the sphere of global migration, setting a new benchmark for legal technology and data source management.|管理移民数据的挑战由于依赖法律专业人员维护的基于纸张的、证据驱动的记录而加剧，由于与基于人工智能的系统存在固有的信任问题，给有效处理和分析造成了障碍。本文介绍了一个前沿的框架，通过协同大型语言模型(LLM)和知识图(KGs)来克服这些障碍，彻底改革了传统的数据处理方法。我们的方法将古老的纸质移民记录转化为一个结构化的、相互关联的知识网络，这个网络错综复杂地反映了移民的法律和程序细微差别，确保了一个动态和可靠的数据分析平台。利用 LLM，我们从不同的法律文件中提取重要的实体和关系，以形成一个全面的知识图表，概括移民过程中复杂的法律和程序差异，并绘制利益攸关方(如申请人、担保人和法律专家)之间的多方面互动。这个图表不仅有助于深入了解法律规定，而且还包含了它们，极大地提高了系统的可靠性和精确性。该框架集成了检索增强生成(RAG)和增强知识创建(AR)两种技术，分别用于精确的、上下文感知的数据检索和通过 LLM 开发会话界面，为移民数据管理提供了一种可扩展的、适应性强的解决方案。LLM、 KGs 和 RAG 技术的这种创新性融合标志着全球移民领域向更知情、高效和可信赖的决策转变，为法律技术和数据源管理设定了新的基准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semantic+interlinking+of+Immigration+Data+using+LLMs+for+Knowledge+Graph+Construction)|0|
|[Why Deeper Layers May Introduce More Bias](https://doi.org/10.1145/3589335.3651583)|Rui Xia, Lisong Wang, Pingping Shi||One of the byproducts of message passing neural networks (MPNNs) is their potential bias towards weakly connected nodes, which can result in degraded performance. This paper confirms that as the number of layers increases, this bias becomes more closely associated with an imbalance in the distribution of eigenvector centrality, known as localization, which further amplifies the discrepancy in label influence on nodes, resulting in a performance gap. Therefore, we explore the effectiveness of non-backtracking centrality and PageRank centrality in mitigating this bias in MPNNs.|消息传递神经网络(MPNN)的副产品之一是它们对弱连接节点的潜在偏差，这可能导致性能下降。本文证实，随着层数的增加，这种偏差与特征向量中心性分布的不平衡(称为局部化)变得更加密切相关，这种不平衡进一步放大了标签对节点影响的差异，导致性能差距。因此，我们探讨了非回溯中心性和 PageRank 中心性在减轻 MPNN 中这种偏差方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Why+Deeper+Layers+May+Introduce+More+Bias)|0|
|[Shock! Quantifying the Impact of Core Developers' Dropout on the Productivity of OSS Projects](https://doi.org/10.1145/3589335.3651559)|Giuseppe Russo Latona, Christoph Gote, Christian Zingg, Giona Casiraghi, Luca Verginer, Frank Schweitzer||Open Source Software (OSS) projects play a critical role in the digital infrastructure of companies and services provided to millions of people. Given their importance, understanding the resilience of OSS projects is paramount. A primary reason for OSS project failure is the shock caused by the dropout of a core developer, which can jeopardize productivity and project survival. Using a difference-in-differences (DiD) analysis, this study investigates the repercussions of this shock on the productivity of 8,234 developers identified among 9,573 OSS GitHub projects. Our findings reveal the indirect impact of the core developer's dropout. The remaining developers experienced a 20% productivity drop. This observation is troubling because it suggests that the shock might push other developers to drop out, putting the collaboration structure of the project at risk. Also, projects with higher productivity before the shock experienced a larger drop-down after the shock. This points to a tradeoff between productivity and resilience, i.e., the ability of OSS projects to recover from the dropout of a core developer. Our findings underscore the importance of a balanced approach in OSS project management, harmonizing productivity goals with resilience considerations.|开源软件(OSS)项目在为数百万人提供的公司和服务的数字基础设施中发挥着关键作用。鉴于它们的重要性，理解开放源码软件项目的弹性是至关重要的。OSS 项目失败的一个主要原因是核心开发人员的退出所造成的冲击，这可能会危及生产力和项目生存。使用差异分析(DiD) ，本研究调查了这种冲击对9,573个 OSS GitHub 项目中确定的8,234个开发人员的生产力的影响。我们的研究结果揭示了核心开发人员辍学的间接影响。其余的开发人员的生产率下降了20% 。这种观察结果令人不安，因为它表明，这种冲击可能会促使其他开发人员退出，从而使项目的协作结构面临风险。另外，在冲击之前生产率较高的项目在冲击之后经历了较大的下降。这指向了生产力和弹性之间的权衡，也就是说，OSS 项目从核心开发人员的退出中恢复的能力。我们的研究结果强调了开放源码软件项目管理中平衡方法的重要性，即将生产力目标与弹性考虑协调起来。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Shock!+Quantifying+the+Impact+of+Core+Developers'+Dropout+on+the+Productivity+of+OSS+Projects)|0|
|[Over-Sampling Strategy in Feature Space for Graphs based Class-imbalanced Bot Detection](https://doi.org/10.1145/3589335.3651544)|Shuhao Shi, Kai Qiao, Chen Chen, Jie Yang, Jian Chen, Bin Yan||The presence of a large number of bots in Online Social Networks (OSN) leads to undesirable social effects. Graph neural networks (GNNs) have achieved state-of-the-art performance in bot detection since they can effectively utilize user interaction. In most scenarios, the distribution of bots and humans is imbalanced, resulting in under-represent minority class samples and sub-optimal performance. However, previous GNN-based methods for bot detection seldom consider the impact of class-imbalanced issues. In this paper, we propose an over-sampling strategy for GNN (OS-GNN) that can mitigate the effect of class imbalance in bot detection. Compared with previous over-sampling methods for GNNs, OS-GNN does not call for edge synthesis, eliminating the noise inevitably introduced during the edge construction. Specifically, node features are first mapped to a feature space through neighborhood aggregation and then generated samples for the minority class in the feature space. Finally, the augmented features are fed into GNNs to train the classifiers. This framework is general and can be easily extended into different GNN architectures. The proposed framework is evaluated using three real-world bot detection benchmark datasets, and it consistently exhibits superiority over the baselines.|在线社交网络(OSN)中大量机器人的出现导致了不良的社交效应。图神经网络由于能够有效地利用用户交互，在机器人检测方面取得了一流的性能。在大多数情况下，机器人和人类的分布是不平衡的，导致少数类样本表示不足，性能次优。然而，以往基于 GNN 的机器人检测方法很少考虑类不平衡问题的影响。本文针对 GNN (OS-GNN)提出了一种过采样策略，以减轻类不平衡对机器人检测的影响。与以往的过采样方法相比，OS-GNN 不需要边缘合成，消除了边缘构造过程中不可避免的噪声。首先通过邻域聚合将节点特征映射到特征空间，然后为特征空间中的少数类生成样本。最后，将增强特征输入到 GNN 中，对分类器进行训练。这个框架是通用的，可以很容易地扩展到不同的 GNN 体系结构中。利用三个实际的机器人检测基准数据集对该框架进行了评估，结果表明该框架始终显示出优于基准的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Over-Sampling+Strategy+in+Feature+Space+for+Graphs+based+Class-imbalanced+Bot+Detection)|0|
|[Dual Graph Networks with Synthetic Oversampling for Imbalanced Rumor Detection on Social Media](https://doi.org/10.1145/3589335.3651494)|YenWen Lu, ChihYao Chen, ChengTe Li||Rumor detection is to identify and mitigate potentially damaging falsehoods, thereby shielding the public from misleading information. However, existing methods fall short of tackling class imbalance, meaning rumor is less common than true messages, as they lack specific adaptation for the context of rumor dissemination. In this work, we propose Dual Graph Networks with Synthetic Oversampling (SynDGN), a novel method that can determine whether a claim made on social media is rumor or not in the presence of class imbalance. SynDGN properly utilizes dual graphs to integrate social media contexts and user characteristics to make accurate predictions. Experiments conducted on two well-known datasets verify that SynDGN consistently outperforms state-of-the-art models, regardless of whether the data is balanced or not.|谣言侦测是为了识别和减轻潜在的破坏性谎言，从而保护公众免受误导性信息的影响。然而，现有的方法并不能解决阶级不平衡问题，这意味着谣言不如真实信息普遍，因为它们缺乏对谣言传播环境的特定适应性。在这项工作中，我们提出了合成过采样的双图网络(SynDGN) ，一种新的方法，可以确定是否是谣言的社会媒体上的声明是否存在阶级不平衡。SynDGN 正确地利用双重图表来整合社交媒体上下文和用户特征，以做出准确的预测。在两个众所周知的数据集上进行的实验证实，无论数据是否平衡，SynDGN 始终优于最先进的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual+Graph+Networks+with+Synthetic+Oversampling+for+Imbalanced+Rumor+Detection+on+Social+Media)|0|
|[Travel Demand Prediction with Application to Commuter Demand Estimation on Urban Railways](https://doi.org/10.1145/3589335.3651574)|Yohei Kodama, Yuki Akeyama, Yusuke Miyazaki, Koh Takeuchi||Travel demand forecasting is a vital problem in the development of smart cities, infrastructure planning, and transportation management. The advent of contactless smart card systems has enabled the collection of data regarding daily transit and purchasing activities, providing a rich source of insights into citizen behavior. In this paper, we introduce a new problem of predicting changes in travel demand resulting from the installation of a new facility while preserving privacy. To address this problem, we propose a simple but effective supervised learning method that can capture the relationships between residential areas and existing facility locations, and exploit spatial features to forecast future demand in response to a new facility location. As a workable example, we employ real-world data to predict the future travel demand triggered by the installation of a new station in a railway system. Through extensive experiments, we demonstrate that our method improves the prediction accuracy.|出行需求预测是智能城市发展、基础设施规划和交通管理中的一个重要问题。免触碰智能卡系统的出现使我们能够收集有关日常交通和购物活动的数据，提供了一个深入了解公民行为的丰富来源。在本文中，我们提出了一个新的问题，预测变化的旅游需求由于安装新的设施，同时保护隐私。为了解决这个问题，我们提出了一个简单而有效的监督式学习方法，可以捕捉住宅区和现有设施位置之间的关系，并利用空间特征来预测未来的需求，以响应一个新的设施位置。作为一个可行的例子，我们使用现实世界的数据来预测未来的旅游需求，由安装一个新的车站在铁路系统。通过大量实验表明，该方法提高了预测精度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Travel+Demand+Prediction+with+Application+to+Commuter+Demand+Estimation+on+Urban+Railways)|0|
|[Burstiness-aware Bipartite Graph Neural Networks for Fraudulent User Detection on Rating Platforms](https://doi.org/10.1145/3589335.3651475)|YenWen Lu, YuChe Tsai, ChengTe Li||As the digital commerce landscape continues to expand with rating platforms, the consumer base has similarly grown, marking a pivotal reliance on user ratings and reviews. However, the rise of fraudulent users leveraging deceitful conduct, such as manipulation of product rankings, challenges the credibility of these platforms and compels the necessity for an effective detection model. Amid the challenges of evolving fraudulent patterns and label scarcity, this paper presents a novel model, Burstiness-aware Bipartite Graph Neural Networks (BurstBGN), which combats fraud by exploiting user-product bipartite graphs and timestamped rating activities. BurstBGN encapsulates two key ideas: the modeling of user-product interaction via historical rating data through an Edge-time GNN module and the exhaustive mapping of bursty fraudulent user activities. The performance of BurstBGN is demonstrated through rigorous benchmarking against established methods across three datasets. Our results show that BurstBGN consistently outperforms these methods under both transductive and inductive settings, confirming its effectiveness in detecting fraudulent users from limited annotated data, and thereby providing a safeguard for maintaining user trust in e-commerce platforms.|随着数字商务领域随着评级平台不断扩大，消费者基础也同样增长，标志着对用户评级和评论的关键依赖。然而，欺诈性用户利用欺骗行为(如操纵产品排名)的增加，对这些平台的可信度提出了挑战，并迫使建立一个有效的检测模型。针对欺诈模式不断演化和标签稀缺的挑战，本文提出了一种新的模型——突发感知二分图神经网络(BurstBGN) ，该模型利用用户产品二分图和时间戳评级活动来打击欺诈行为。BurstBGN 封装了两个关键思想: 通过边缘时间 GNN 模块通过历史评级数据建立用户-产品交互模型，以及突发欺诈用户活动的详尽映射。BurstBGN 的性能是通过对三个数据集的既定方法进行严格的基准测试来证明的。我们的研究结果表明，BurstBGN 在传导性和归纳性设置下始终优于这些方法，证实了它在从有限的注释数据中检测欺诈用户方面的有效性，从而为维持用户对电子商务平台的信任提供了保障。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Burstiness-aware+Bipartite+Graph+Neural+Networks+for+Fraudulent+User+Detection+on+Rating+Platforms)|0|
|[A Tale of Two Communities: Exploring Academic References on Stack Overflow](https://doi.org/10.1145/3589335.3651464)|Run Huang, Souti Chattopadhyay||Stack Overflow is widely recognized by software practitioners as the go-to resource for addressing technical issues and sharing practical solutions. While it is not typically seen as a forum for scholarly discourse, users on Stack Overflow often refer to academic sources in their discussions. Yet, little is known about these referenced works from the academic community and how they intersect the needs and interests of the Stack Overflow community. To bridge this gap, we conducted a large-scale study on academic references in Stack Overflow. Our findings reveal that Stack Overflow communities with different domains of interest engage with academic literature at varying frequencies and speeds. The contradicting patterns suggest that some disciplines may have diverged in their interests and development trajectories from the corresponding practitioner community. Finally, we discuss the potential of Stack Overflow in gauging the real-world relevance of academic research.|Stack Overflow 被软件从业者广泛认为是解决技术问题和分享实际解决方案的可用资源。虽然 Stack Overflow 通常不被视为学术讨论的论坛，但它的用户在讨论中经常引用学术资源。然而，对于这些来自学术界的参考著作，以及它们如何与 Stack Overflow 社区的需求和兴趣相交，我们知之甚少。为了弥补这一差距，我们对 Stack Overflow 中的学术参考文献进行了大规模的研究。我们的研究结果表明，Stack Overflow 社区具有不同的兴趣领域，以不同的频率和速度参与学术文献。相互矛盾的模式表明，一些学科在其兴趣和发展轨迹方面可能与相应的从业者群体不同。最后，我们讨论了堆栈溢出在衡量学术研究的现实相关性方面的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Tale+of+Two+Communities:+Exploring+Academic+References+on+Stack+Overflow)|0|
|[Efficient Location Sampling Algorithms for Road Networks](https://doi.org/10.1145/3589335.3651497)|Sara Ahmadian, Sreenivas Gollapudi, Kostas Kollias, Vivek Kumar, Ameya Velingker, Santhoshini Velusamy||Many geographic information systems applications rely on data provided by user devices in the road network, including traffic monitoring, driving navigation, and road closure detection. The underlying signal is generally collected by sampling locations from user trajectories. The sampling process, though critical for various applications, has not been studied sufficiently in the literature. While the most natural way to sample a trajectory may be to use a frequency based algorithm, e.g., sampling locations every x seconds, such a sampling strategy can be quite wasteful in resources (e.g., server-side processing, user battery) as well as stored user data. In this work, we conduct a horizontal study of various location sampling algorithms (based on frequency, road geography, reservoir sampling, etc.) and assess their trade-offs in terms of the size of the stored data and the induced quality of training for prediction tasks (specifically predicting speeds on road segments).|许多地理信息系统应用程序依赖于道路网络中用户设备提供的数据，包括交通监控、驾驶导航和道路封闭检测。底层信号通常通过从用户轨迹中采样位置来收集。抽样过程，虽然对各种应用至关重要，但在文献中还没有得到充分的研究。虽然最自然的取样轨迹的方法可能是使用一个基于频率的算法，例如，每 x 秒取样位置，这样的取样策略可能相当浪费资源(例如，服务器端处理，用户电池)以及存储的用户数据。在这项工作中，我们对各种位置采样算法(基于频率、道路地理、水塘抽样等)进行了横向研究，并根据存储数据的大小和预测任务(特别是预测路段速度)的训练诱导质量来评估它们之间的权衡。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Location+Sampling+Algorithms+for+Road+Networks)|0|
|[A Category-agnostic Graph Attention-based Approach for Determining Notability of Articles for Wikipedia](https://doi.org/10.1145/3589335.3651461)|Gokul Thota, Vasudeva Varma||Wikipedia is a highly essential platform because of its informative, dynamic, and easily accessible nature. To identify topics/titles warranting their own Wikipedia article, editors of Wikipedia defined "Notability" guidelines. So far notability is enforced by humans, which makes scalability an issue. There has been no significant work on Notability determination for titles with complex category dependencies. We design a mechanism to identify such titles. We construct a dataset with 9k such titles and propose a category-agnostic approach utilizing Graph neural networks, for their notability determination. Our system outperforms machine learning-based, transformer-based classifiers and entity salience methods. It provides a scalable alternative for notability detection.|维基百科是一个非常重要的平台，因为它信息量大、动态、易于访问。维基百科的编辑们定义了“知名度”指导原则，以确定哪些主题/标题需要他们自己的维基百科文章。到目前为止，知名度是由人类强制执行的，这使得可伸缩性成为一个问题。在确定具有复杂类别依赖关系的头衔的知名度方面没有重要的工作。我们设计了一个机制来识别这样的标题。我们建立了一个9k 这样的标题的数据集，并提出了一个类别不可知的方法，利用图神经网络，为他们的显着性决定。我们的系统优于基于机器学习、基于变压器的分类器和实体显著性方法。它为知名度检测提供了一个可扩展的替代方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Category-agnostic+Graph+Attention-based+Approach+for+Determining+Notability+of+Articles+for+Wikipedia)|0|
|[Concentration of Power and Participation in Online Governance: the Ecosystem of Decentralized Autonomous Organizations](https://doi.org/10.1145/3589335.3651481)|Andrea PeñaCalvin, Javier Arroyo, Andrew Schwartz, Samer Hassan||Blockchain technology enables a new form of online community: Decentralized Autonomous Organizations (DAOs), where members typically vote on proposals using tokens. Enthusiasts claim DAOs provide new opportunities for openness, horizontality, and democratization. However, this phenomenon is still under research, especially given the lack of quantitative studies. This paper presents the first census-like quantitative analysis of the whole ecosystem of DAOs, including 30K DAO communities on the main DAO platforms. This enables us to provide insights into the allegedly "democratic'' nature of DAOs, building metrics concerning their lifespan, participation, and power concentration. Most DAOs have a short lifespan and low participation. There is also a positive correlation between community size and voting power concentration. Like other online communities, DAOs seem to follow the iron law: becoming increasingly oligarchic as they grow. Still, a significant amount of DAOs of varying sizes defy this idea by being egalitarian by design.|Blockchain 的技术使一种新的在线社区形式成为可能: 去中心化的自治组织(dalos) ，其成员通常使用令牌对提案进行投票。爱好者声称 DAO 为开放性、横向性和民主化提供了新的机会。然而，这一现象仍在研究中，特别是缺乏定量研究。本文首次对 DAO 的整个生态系统，包括主要 DAO 平台上的30K DAO 群落进行了类似普查的定量分析。这使我们能够深入了解所谓的 DAO 的“民主”本质，构建关于它们的寿命、参与和权力集中的度量。大多数 DAO 的寿命较短，参与度较低。社区规模与选举权集中度呈正相关。与其他在线社区一样，DAO 似乎遵循着铁律: 随着成长，它们变得越来越寡头化。尽管如此，仍然有大量不同大小的 DAO 在设计上是平等的，从而违背了这一理念。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Concentration+of+Power+and+Participation+in+Online+Governance:+the+Ecosystem+of+Decentralized+Autonomous+Organizations)|0|
|["All of Me": Mining Users' Attributes from their Public Spotify Playlists](https://doi.org/10.1145/3589335.3651459)|Pier Paolo Tricomi, Luca Pajola, Luca Pasa, Mauro Conti|Spritz Matter Srl & University of Padova, Padova, Italy; Department of Mathematics, University of Padova, Padova, Italy; Department of Mathematics, University of Padova & Spritz Matter Srl, Padova, Italy|In the age of digital music streaming, playlists on platforms like Spotify have become an integral part of individuals' musical experiences. People create and publicly share their own playlists to express their musical tastes, promote the discovery of their favorite artists, and foster social connections. In this work, we aim to address the question: can we infer users' private attributes from their public Spotify playlists? To this end, we conducted an online survey involving 739 Spotify users, resulting in a dataset of 10,286 publicly shared playlists comprising over 200,000 unique songs and 55,000 artists. Then, we utilize statistical analyses and machine learning algorithms to build accurate predictive models for users' attributes.|在数字音乐流媒体时代，像 Spotify 这样的平台上的播放列表已经成为个人音乐体验不可或缺的一部分。人们创建并公开分享自己的播放列表，以表达自己的音乐品味，促进发现自己喜欢的艺术家，并培养社会关系。在这项工作中，我们的目标是解决这个问题: 我们能从用户的公共 Spotify 播放列表中推断出用户的私人属性吗？为此，我们对739名 Spotify 用户进行了在线调查，得到了10286个公开分享的播放列表数据集，其中包括超过20万首独特歌曲和55000名艺术家。然后，利用统计分析和机器学习算法建立精确的用户属性预测模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q="All+of+Me":+Mining+Users'+Attributes+from+their+Public+Spotify+Playlists)|0|
|[PyGDebias: A Python Library for Debiasing in Graph Learning](https://doi.org/10.1145/3589335.3651239)|Yushun Dong, Zhenyu Lei, Zaiyi Zheng, Song Wang, Jing Ma, Alex Jing Huang, Chen Chen, Jundong Li||Graph-structured data is ubiquitous among a plethora of real-world applications. However, as graph learning algorithms have been increasingly deployed to help decision-making, there has been rising societal concern in the bias these algorithms may exhibit. In certain high-stake decision-making scenarios, the decisions made may be life-changing for the involved individuals. Accordingly, abundant explorations have been made to mitigate the bias for graph learning algorithms in recent years. However, there still lacks a library to collectively consolidate existing debiasing techniques and help practitioners to easily perform bias mitigation for graph learning algorithms. In this paper, we present PyGDebias, an open-source Python library for bias mitigation in graph learning algorithms. As the first comprehensive library of its kind, PyGDebias covers 13 popular debiasing methods under common fairness notions together with 26 commonly used graph datasets. In addition, PyGDebias also comes with comprehensive performance benchmarks and well-documented API designs for both researchers and practitioners. To foster convenient accessibility, PyGDebias is released under a permissive BSD-license together with performance benchmarks, API documentation, and use examples at https://github.com/yushundong/PyGDebias.|图形结构化数据在大量真实世界的应用程序中无处不在。然而，随着图形学习算法越来越多地被用于帮助决策，这些算法可能表现出的偏差引起了社会越来越多的关注。在某些高风险的决策情景中，所做的决定可能会改变相关个人的一生。因此，近年来人们在减小图学习算法的偏差方面进行了大量的探索。然而，仍然缺乏一个库来整合现有的去偏技术，并帮助从业人员轻松地执行图形学习算法的偏差缓解。在本文中，我们介绍了 PyGDebian，一个用于图学习算法中减少偏差的开源 Python 库。作为第一个综合性图书馆，PyGDebian 涵盖了13种常用的公平性概念下的去偏方法和26种常用的图形数据集。此外，PyGDebian 还为研究人员和从业者提供了全面的性能基准和详细记录的 API 设计。为了促进方便的可访问性，PyGdeias 是在一个宽松的 BSD 许可下发布的，同时还有性能基准测试、 API 文档和 https://github.com/yushundong/PyGDebias 使用示例。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PyGDebias:+A+Python+Library+for+Debiasing+in+Graph+Learning)|0|
|[Synslator: An Interactive Machine Translation Tool with Online Learning](https://doi.org/10.1145/3589335.3651240)|Jiayi Wang, Ke Wang, Fengming Zhou, Chengyu Wang, Zhiyong Fu, Zeyu Feng, Yu Zhao, Yuqi Zhang||Interactive machine translation (IMT) has emerged as a progression of the computer-aided translation paradigm, where the machine translation system and the human translator collaborate to produce high-quality translations. This paper introduces Synslator, a user-friendly computer-aided translation (CAT) tool that not only supports IMT, but is adept at online learning with real-time translation memories. To accommodate various deployment environments for CAT services, Synslator integrates two different neural translation models to handle translation memories for online learning. Additionally, the system employs a language model to enhance the fluency of translations in an interactive mode. In evaluation, we have confirmed the effectiveness of online learning through the translation models, and have observed a 13% increase in post-editing efficiency with the interactive functionalities of Synslator. A tutorial video is available at:https://youtu.be/K0vRsb2lTt8.|交互式机器翻译(IMT)已经成为计算机辅助翻译范式的一个发展阶段，机器翻译系统和人工翻译协作生成高质量的翻译。本文介绍了 Synslator，一个用户友好的计算机辅助翻译(CAT)工具，它不仅支持 IMT，而且善于利用实时翻译记忆进行在线学习。为了适应 CAT 服务的不同部署环境，Synslator 集成了两种不同的神经翻译模型来处理在线学习的翻译记忆。此外，该系统采用了一种语言模式，以提高交互式翻译的流畅性。在评估中，我们通过翻译模型证实了在线学习的有效性，并观察到 Synslator 的交互功能使后期编辑效率提高了13% 。教程视频可以在以下 https://youtu.be/k0vrsb2ltt8找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Synslator:+An+Interactive+Machine+Translation+Tool+with+Online+Learning)|0|
|[ACCORD: Constraint-driven Mediation of Multi-user Conflicts in Cloud Services](https://doi.org/10.1145/3589335.3651244)|Abhiroop Tippavajjula, Primal Pappachan, Anna Cinzia Squicciarini, Jose Such||When multiple users adopt collaborative cloud services like Google Drive to work on a shared resource, incorrect or missing permis- sions may cause conflicting or inconsistent access or use privileges. These issues (or conflicts) compromise resources confidentiality, integrity, or availability leading to a lack of trust in cloud services. An example conflict is when a user with editor permissions changes the permissions on a shared resource without consent from the orig- inal resource owner. In this demonstration, we introduce ACCORD, a web application built on top of Google Drive able to detect and resolve multi-user conflicts. ACCORD employs a simulator to help users preemptively identify potential conflicts and assists them in defining action constraints. Using these constraints, ACCORD can automatically detect and resolve any future conflicts.|当多个用户采用像 Google Drive 这样的协作云服务来处理共享资源时，不正确或缺失的权限可能会导致相互冲突或不一致的访问或使用权限。这些问题(或冲突)损害了资源的机密性、完整性或可用性，导致对云服务缺乏信任。示例冲突是当具有编辑器权限的用户未经原始资源所有者同意而更改对共享资源的权限时。在本演示中，我们将介绍 ACCORD，这是一个构建在 Google Drive 之上的 Web 应用程序，能够检测和解决多用户冲突。ACCORD 使用模拟器帮助用户预先识别潜在的冲突，并协助他们定义行动约束。使用这些约束，ACCORD 可以自动检测和解决任何未来的冲突。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ACCORD:+Constraint-driven+Mediation+of+Multi-user+Conflicts+in+Cloud+Services)|0|
|[Fediscount: Shopping Online at a Federated Store Using FedUP as SPARQL Federation Engine](https://doi.org/10.1145/3589335.3651249)|Julien AimonierDavat, Minh Hoang Dang, Pascal Molli, Brice Nédelec, Hala SkafMolli||Processing SPARQL queries over large federations of SPARQL endpoints is essential for maintaining the Semantic Web decentralized. However, existing federation engines struggle to query more than a dozen of endpoints. We recently proposed FedUP, a new type of federation engine based on unions-over-joins query plans that outperforms state-of-the-art federation engines by orders of magnitude on large federations. This demonstration paper introduces Fediscount, a federated online shopping application based on the FedShop benchmark, illustrating the capabilities of FedUP. The application is based on standard Semantic Web technologies, enabling end-users to shop online in a virtual federated store comprising 20, 100, or even 200 SPARQL endpoints. This breakthrough opens up promising new avenues for developing and deploying federated applications.|在大型 SPARQL 端点联合上处理 SPARQL 查询对于维护语义 Web 分散化是必不可少的。然而，现有的联合引擎很难查询十多个端点。我们最近提出了 FedUP，这是一种基于联合超连接查询计划的新型联邦引擎，其性能优于最先进的联邦引擎，数量级是大型联邦。这篇演示文章介绍了基于 FedShop 基准的联邦在线购物应用程序 Fediscn，并说明了 FedUP 的功能。该应用程序基于标准的语义 Web 技术，允许终端用户在包含20、100甚至200个 SPARQL 端点的虚拟联合商店中在线购物。这一突破为开发和部署联邦应用程序开辟了有希望的新途径。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fediscount:+Shopping+Online+at+a+Federated+Store+Using+FedUP+as+SPARQL+Federation+Engine)|0|
|[Online Disinformation and Generative Language Models: Motivations, Challenges, and Mitigations](https://doi.org/10.1145/3589335.3651254)|Ziyi Guo||Disinformation refers to the deliberate dissemination of fake or misleading information, which significantly threatens the modern social stability by undermining trust, intensifying polarization and manipulating public opinion. With the advances of generative AI, the landscape of modern disinformation is changing following the rise of Large Language Models. Recent studies has revealed the capability of generative language models to create convincing and misleading content against the truth and warned the availability of such models to be maliciously abused for deceptive generation. However, AI-driven disinformation is a human-centered societal issue in nature, the realization of which requires not only the in- depth discussion on the latest trends from both sides of generative AI and disinformation, but a critical analysis on the uncertainty of their potential interaction in practice as well. The paper introduces the new vision of AI-driven disinformation campaigns from the perspectives of human-centered AI, proposes a framework of core research questions based on the existing research gap, discusses the preliminary discovery in literature and initial experiments, and elaborates the main lines of research in the future work.|虚假信息是指蓄意传播虚假或误导性信息，通过破坏信任、加剧两极分化和操纵舆论，严重威胁现代社会稳定。随着生成性人工智能的发展，随着大语言模型的兴起，现代虚假信息的格局正在发生变化。最近的研究揭示了生成语言模型的能力，创造令人信服的和误导性的内容对真相，并警告这种模型的可用性被恶意滥用的欺骗性生成。然而，人工智能驱动的虚假信息在本质上是一个以人为中心的社会问题，其实现不仅需要对生成性人工智能和虚假信息双方的最新趋势进行深入讨论，而且需要对它们在实践中潜在互动的不确定性进行批判性分析。本文从以人为中心的人工智能视角，介绍了人工智能驱动的虚假信息运动的新视野，在现有研究差距的基础上，提出了核心研究问题的框架，讨论了文献和初步实验中的初步发现，并阐述了今后工作的主要研究方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Disinformation+and+Generative+Language+Models:+Motivations,+Challenges,+and+Mitigations)|0|
|[Quantifying Governance of Online Communities at Web Scale](https://doi.org/10.1145/3589335.3651266)|Galen Cassebeer Weld||Online communities are powerful tools to connect people and are used worldwide by billions of people. Nearly all online communities rely upon moderators or admins to govern the community in order to mitigate potential harms such as harassment, polarization, and deleterious effects on mental health. However, online communities are complex systems, and studying the impact of community governance empirically at scale is challenging because of the many aspects of community governance and outcomes that must be quantified. In this work, we develop methods to quantify the governance of online communities at web scale. We survey community members to build a comprehensive understanding of what it means to make communities 'better,' then assess existing governance practices and associate them with important outcomes to inform community moderators. We collaborate with communities to deploy our governance interventions to maximize the positive impact of our work, and, at every step of the way, we make our datasets and methods public to support further research on this important topic.|在线社区是连接人们的强大工具，全世界有数十亿人在使用它。几乎所有的在线社区都依靠版主或管理员来管理社区，以减轻潜在的危害，如骚扰、两极分化和对心理健康的有害影响。然而，在线社区是一个复杂的系统，由于社区治理的许多方面以及必须量化的成果，以实证方式大规模研究社区治理的影响具有挑战性。在这项工作中，我们开发的方法来量化网络规模的在线社区的治理。我们调查社区成员，以建立一个全面的了解什么是意味着让社区“更好”，然后评估现有的治理实践，并联系他们与重要的成果，以通知社区调解员。我们与社区合作，部署我们的治理干预措施，以最大限度地发挥我们工作的积极影响，并在每一个步骤中公开我们的数据集和方法，以支持对这一重要主题的进一步研究。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quantifying+Governance+of+Online+Communities+at+Web+Scale)|0|
|[Tutorial on User Simulation for Evaluating Information Access Systems on the Web](https://doi.org/10.1145/3589335.3641243)|Krisztian Balog, ChengXiang Zhai||Users routinely interact with the Web via information access systems such as search engines and recommender systems. How to accurately evaluate such interactive systems with reproducible experiments is an important, yet difficult challenge. To address this challenge, user simulation has emerged as a promising solution. This half-day tutorial focuses on providing a thorough introduction to user simulation techniques designed specifically for evaluating information access systems on the Web. We systematically review major research progress, covering both general frameworks for designing user simulators, and specific models and algorithms for simulating user interactions with search engines, recommender systems, and conversational assistants. We also highlight some important future research directions.|用户通常通过搜索引擎和推荐系统等信息访问系统与网络进行交互。如何通过可重复性实验准确地评估这种交互式系统是一个重要而又困难的挑战。为了应对这一挑战，用户模拟已经成为一种有前途的解决方案。本教程为期半天，重点介绍专门为评估 Web 上的信息访问系统而设计的用户模拟技术。我们系统地回顾了主要的研究进展，包括用于设计用户模拟器的一般框架，以及用于模拟用户与搜索引擎、推荐系统和会话助手的交互的具体模型和算法。我们还强调了一些重要的未来研究方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tutorial+on+User+Simulation+for+Evaluating+Information+Access+Systems+on+the+Web)|0|
|[Archiving and Temporal Analysis of Behavioral Web Data - Tales from the Inside](https://doi.org/10.1145/3589335.3641260)|Stefan Dietze||Behavioral web data such as social web activity streams, query logs or behavioral traces from web search and navigation are crucial to understand the temporal evolution of the web and the human interactions that produce web data and models trained on such data. Thus, behavioral web data empowers research in various fields, such as (temporal) information retrieval, computational social science or cognitive and behavioral modeling of users over time. On the other hand, archiving and using such kind of data is associated with a number of technical and non-technical challenges, e.g. legal and ethical concerns, fast decay of data over time, as well as dependency on 3rd party gatekeepers, such as Twitter/X or Google. We present case studies from many years of research into archiving and temporal analysis of behavioral web data, including large-scale social web archives such as TweetsKB, based on an archive of 14 bn tweets harvested continuously since 2013, and web search and navigation behavior tracked through user studies, longitudinal panels or crowdsourcing-based quasi experiments.|行为网络数据，例如社交网络活动流、查询日志或网络搜索和导航中的行为追踪，对于理解网络的时间演变和人类交互产生网络数据以及基于这些数据训练的模型至关重要。因此，行为网络数据支持各个领域的研究，比如(时间)信息检索、计算社会科学或者用户的认知和行为建模。另一方面，归档和使用这类数据与许多技术和非技术挑战有关，例如法律和道德问题，数据随着时间的推移快速衰退，以及对第三方网关的依赖，如 Twitter/X 或谷歌。我们展示了多年来对行为网络数据的归档和时间分析的案例研究，包括大规模的社交网络档案，如 TweetsKB，基于自2013年以来持续收集的140亿条推文的档案，以及通过用户研究、纵向面板或基于众包的准实验跟踪的网络搜索和导航行为。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Archiving+and+Temporal+Analysis+of+Behavioral+Web+Data+-+Tales+from+the+Inside)|0|
|[A Case Study Comparing Twitter Communities Detected by the Louvain and Leiden Algorithms During the 2022 War in Ukraine](https://doi.org/10.1145/3589335.3651892)|Karolina Sliwa, Ema Kusen, Mark Strembeck||This paper presents a case study regarding a comparative examination of the Louvain and Leiden community detection algorithms. The case study was conducted on a real-world communication network consisting of 3,222,623 nodes and 27,423,553 edges. In particular, the network in our case study models the communication between Twitter users during the initial four weeks of the 2022 war in Ukraine. In addition, we also applied dynamic topic modeling in order to examine differences in the detected communities.|本文介绍了一个案例研究，对 Louvain 和莱顿社区检测算法进行了比较研究。该案例研究是在一个由3,222,623个节点和27,423,553个边组成的现实通信网络上进行的。特别是，我们案例研究中的网络模拟了2022年乌克兰战争最初四周 Twitter 用户之间的交流。此外，我们还应用动态主题模型来检验检测到的社区的差异。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Case+Study+Comparing+Twitter+Communities+Detected+by+the+Louvain+and+Leiden+Algorithms+During+the+2022+War+in+Ukraine)|0|
|[AI Driven Online Advertising: Market Design, Generative AI, and Ethics](https://doi.org/10.1145/3589335.3641295)|Fengxiang He, Mengnan Du, Aris FilosRatsikas, Lu Cheng, Qingquan Song, Min Lin, John Vines||Online advertising contributes a considerable part of the tech sector's revenue, and has been remarkably influencing the public agenda. With evolving developments, AI is playing an increasingly significant role in online advertising. We propose to create a forum for researchers, developers, users, ventures, policymakers, and other stakeholders to exchange ideas, research, innovations, etc. with emphasis on (1) AI driven mechanism design for distributing advertisements, (2) generative AI for creating content in advertisements, such as the promotion images/videos, and (3) ethics issues, especially in political advertisements, such as user privacy, fairness, hating speech, misinformation, etc. Relevant but not mentioned areas are also much encouraged. We plan to organize a half-day workshop.|在线广告贡献了科技行业相当大的一部分收入，而且一直在显著影响公共议程。随着发展的不断深入，人工智能在网络广告中扮演着越来越重要的角色。我们建议为研究人员、开发人员、用户、企业、决策者和其他利益相关者创建一个论坛，以交流思想、研究、创新等，重点是: (1)人工智能驱动的广告分发机制设计; (2)生成性人工智能用于在广告中创建内容，如推广图像/视频; (3)道德问题，特别是在政治广告中，如用户隐私、公平、仇恨言论、错误信息等。相关但未提及的领域也受到大力鼓励。我们计划组织一次为期半天的研讨会。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI+Driven+Online+Advertising:+Market+Design,+Generative+AI,+and+Ethics)|0|
|[Comparative Analysis of Discussion Intensity and Semantic Diversity in Early vs. Late Engagers: A Study of Japanese Tweets about ChatGPT](https://doi.org/10.1145/3589335.3651908)|Tomoki Fukuma, Koki Noda, Yuta Yamamoto, Takaya Hoshi, Yoshiharu Ichikawa, Kyosuke Kambe, Yu Masubuchi, Fujio Toriumi||This study investigates engagement patterns related to OpenAI's ChatGPT on Japanese Twitter, focusing on two distinct user groups - early and late engagers, inspired by the Innovation Theory. Early engagers are defined as individuals who initiated conversations about ChatGPT during its early stages, whereas late engagers are those who began participating at a later date. To examine the nature of the conversations, we conduct a dual methodology, encompassing both quantitative and qualitative analyses. The quantitative analysis reveals that early engagers often engage with more forward-looking and speculative topics, emphasizing the technological advancements and potential transformative impact of ChatGPT. Conversely, the late engagers intereact more with contemporary topics, focusing on the optimization of existing AI capabilities and considering their inherent limitations. Through our qualitative analysis, we propose a method to measure the proportion of shared or unique viewpoints within topics across both groups. We found that early engagers generally concentrate on a more limited range of perspectives, whereas late engagers exhibit a wider range of viewpoints. Interestingly, a weak correlation was found between the volume of tweets and the diversity of discussed topics in both groups. These findings underscore the importance of identifying semantic diversity, rather than relying solely on the volume of tweets, for understanding differences in communication styles between groups within a given topic. Moreover, our versatile dual methodology holds potential for broader applications, such as studying online discourse patterns within different user groups, or in contexts beyond ChatGPT.|这项研究调查了与 OpenAI 在日本 Twitter 上的 ChatGPT 相关的参与模式，重点关注两个截然不同的用户群——受创新理论启发的早期参与者和晚期参与者。早期参与者被定义为在 ChatGPT 的早期阶段就开始谈论它的个人，而晚期参与者是那些在后期才开始参与的个人。为了检验会话的本质，我们采用了双重方法，包括定量分析和定性分析。定量分析表明，早期参与者往往参与更具前瞻性和投机性的主题，强调 ChatGPT 的技术进步和潜在的变革性影响。相反，后来的参与者更多地与当代主题互动，侧重于优化现有的人工智能能力，并考虑其固有的局限性。通过我们的定性分析，我们提出了一种方法来衡量共享或独特的观点比例的主题跨两组。我们发现，早期的参与者通常关注更有限的视角范围，而晚期的参与者则表现出更广泛的视角范围。有趣的是，在两组人群中，推特的数量和讨论话题的多样性之间的相关性很弱。这些发现强调了识别语义多样性的重要性，而不是仅仅依赖于推文的数量，以理解在一个给定的主题内的群体之间的沟通风格的差异。此外，我们多才多艺的双重方法具有更广泛的应用潜力，如研究在不同用户群体的在线话语模式，或在超越 ChatGPT 的上下文。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Comparative+Analysis+of+Discussion+Intensity+and+Semantic+Diversity+in+Early+vs.+Late+Engagers:+A+Study+of+Japanese+Tweets+about+ChatGPT)|0|
|[Analysis of the Effect between the Information Type on SNSs and User Attributes during Disaster](https://doi.org/10.1145/3589335.3652500)|Kosuke Wakasugi, Yu Suzuki, Akiyo Nadamoto||In the event of a disaster, many people post a lot of information on SNS that promotes or inhibits action we designate this information "behavioral facilitation information". Such information is likely to have various effects on user behavior. A wide variety of people browse SNSs, and different readers perceive the same information in different ways. Therefore, in this study, we focus on users' attributes which are personality traits, age, and gender, and analyze how different users perceive information that promotes behavior. Specifically, we extract behavioral facilitation information from SNSs at the time of a disaster using deep learning, and classify the information into four user's personality traits: "suggestion," "inhibition," "encouragement," and "wish." Then, we conduct an experiment in which subjects classified by user's attributes which are personality traits, age, and gender read and judge how they feel about behavioral facilitation information. We then analyze the results, to determine the relationship between the behavioral facilitation information and the reader's attributes.|在灾难发生时，许多人会在 SNS 上发布大量促进或抑制行为的信息，我们将这些信息称为“行为促进信息”。这些信息可能会对用户行为产生各种各样的影响。不同的人浏览 SNS，不同的读者以不同的方式理解相同的信息。因此，本研究主要针对使用者的人格特质、年龄、性别等属性，分析不同使用者对促进行为的信息的感知。具体来说，我们利用深度学习从 SNS 中提取灾难发生时的行为促进信息，并将这些信息分为四类用户的性格特征: “暗示”、“抑制”、“鼓励”和“愿望”然后，我们进行了一个实验，在实验中，被试通过人格特质、年龄和性别等属性来阅读和判断他们对行为促进信息的感受。然后我们分析结果，以确定行为促进信息和读者的属性之间的关系。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analysis+of+the+Effect+between+the+Information+Type+on+SNSs+and+User+Attributes+during+Disaster)|0|
|[Understanding the Impact of COVID-19 on Online Eating Disorder Communities on Reddit](https://doi.org/10.1145/3589335.3652506)|Md Al Amin, Lu Liu||The social restrictions, disruptions in daily activities, and psychological stressors arising from the COVID-19 pandemic constitute a psychological burden for people worldwide, which can be especially detrimental for individuals with mental disorders like Eating Disorders (ED). In this research, we aim to comprehend how COVID-19 has affected individuals with eating disorders through a comparative analysis of data obtained from online communities. We collected data spanning two years before and after the declaration of the pandemic from the subreddits r/AnorexiaNervosa, r/BingeEatingDisorder, and r/EatingDisorders. The research presents multi-faceted tasks where we analyze the content of each of the subreddits by applying a strategy that combines topic modeling, social network analysis, and time series modeling for a better understanding of these communities on both content and network levels. Through a comparative analysis, we address the discussion topic changes based on users' content and determine how COVID-19 leads to changes in communication patterns within the communities. Finally, we implement time series models like ARIMA, Prophet, LSTM, and Transformer on daily posts and comments count to forecast users' activities within the subreddit and establish a performance comparison of these time series models. The findings indicate that both the content of users' discussions and the level of communication and online support-seeking related to eating disorders on Reddit underwent significant changes during the pandemic. The data of this study is available at this GitHub https://github.com/alamincse32/Reddit-Data-for-Eating-Disoder-Community-During-Covid-Pandemic|由2019冠状病毒疾病引起的社会限制、日常活动中断和心理压力，对全世界人民构成了心理负担，对饮食失调(ED)等精神疾病患者尤其有害。在这项研究中，我们的目标是通过对在线社区获得的数据进行比较分析，了解2019冠状病毒疾病是如何影响饮食失调个体的。我们从 r/AnorexiaNervosa，r/BingeEatingDisorder 和 r/EatingDisorders 子版收集了大流行宣布前后两年的数据。这项研究提出了多方面的任务，其中我们分析每个子版块的内容，采用一种策略，结合主题建模，社会网络分析和时间序列建模，以更好地理解这些社区的内容和网络水平。通过比较分析，我们根据用户的内容讨论话题的变化，并确定2019冠状病毒疾病如何导致社区内沟通模式的变化。最后，我们在每天的帖子和评论数量上实现了 ARIMA、 Prophet、 LSTM 和 former 等时间序列模型，以预测子版用户的活动，并建立这些时间序列模型的性能比较。研究结果表明，在大流行期间，Reddit 上与饮食失调相关的用户讨论内容、沟通水平和在线寻求支持的程度都发生了显著变化。这项研究的数据可以在这个 gitHub  https://GitHub.com/alamincse32/reddit-data-for-eating-disoder-community-during-covid-pandemic 找到|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+the+Impact+of+COVID-19+on+Online+Eating+Disorder+Communities+on+Reddit)|0|
|[Statistical Confidence in Mining Power Estimates for PoW Blockchains](https://doi.org/10.1145/3589335.3651960)|Mary Milad, Christina Ovezik, Dimitris Karakostas, Daniel W. Woods||The security of blockchain systems depends on the distribution of mining power across participants. If sufficient mining power is controlled by one entity, they can force their own version of events. This may allow them to double spend coins, for example. For Proof of Work (PoW) blockchains, however, the distribution of mining power cannot be read directly from the blockchain and must instead be inferred from the number of blocks mined in a specific sample window. We introduce a framework to quantify this statistical uncertainty for the Nakamoto coefficient, which is a commonly-used measure of blockchain decentralization. We show that aggregating blocks over a day can lead to considerable uncertainty, with Bitcoin failing more than half the hypothesis tests (α = 0.05) when using a daily granularity. For these reasons, we recommend that blocks are aggregated over a sample window of at least 7 days. Instead of reporting a single value, our approach produces a range of possible Nakamoto coefficient values that have statistical support at a particular significance level α.|区块链系统的安全性取决于采矿权在参与者之间的分配。如果一个实体控制了足够的挖掘能力，那么它们可以强制执行自己版本的事件。例如，这可能使他们的支出增加一倍。然而，对于工作证明(PoW)区块链，挖掘功率的分布不能直接从区块链中读取，而必须从特定样本窗口中挖掘的区块数量中推断出来。我们引入一个框架来量化中本系数的统计不确定性，中本系数是区块链地方分权的常用度量。我们发现，当比特币使用每日粒度时，一天内的块聚合会导致相当大的不确定性，超过一半的假设检验(α = 0.05)都失败了。由于这些原因，我们建议在至少7天的示例窗口中聚合块。我们的方法不是报告一个单一的值，而是产生一系列可能的 Nakamoto 系数值，这些值在统计学上支持特定的显著性水平 α。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Statistical+Confidence+in+Mining+Power+Estimates+for+PoW+Blockchains)|0|
|[From Bollywood Son Preference to Moral Policing on Women in Iran - A 360° View of Gender Bias](https://doi.org/10.1145/3589335.3653010)|Ashiqur R. KhudaBukhsh||Do longitudinal studies reveal a skewed gender distribution among newborn babies depicted in Bollywood movies? Who dominates the speaking time in political conversations on 24x7 news networks in the United States-men or women? How does Twitter discourse on gender equality evolve when a woman dies in police custody in Iran after being arrested (reportedly) due to improper headscarf-wearing? What is the representation of women in divorce court proceedings in India? This broad talk, where cutting-edge AI intersects with social science research questions, encompasses a diverse array of studies that unveil gender bias in various forms. In this presentation, I will describe the substantive findings, social impact, methodological challenges, scope for multimodal investigations, and the novelties entailed in this research. I will conclude the talk with our findings on worrisome gender bias in several large language models.|纵向研究是否揭示了宝莱坞电影中新生儿性别分布的偏差？在美国24x7新闻网络的政治对话中，谁主导着发言时间——男人还是女人？据报道，一名伊朗妇女因戴头巾不当而被捕，在警察拘留期间死亡，Twitter 上关于性别平等的讨论是如何演变的？印度妇女在离婚诉讼中的代表性如何？这个广泛的演讲，其中尖端的人工智能与社会科学研究问题交叉，包括一系列不同的研究，揭示性别偏见的各种形式。在本报告中，我将描述实质性的发现，社会影响，方法学的挑战，多模式调查的范围，以及在这项研究中包含的新颖性。我将以我们在几个大型语言模型中令人担忧的性别偏见的研究结果来结束这次演讲。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Bollywood+Son+Preference+to+Moral+Policing+on+Women+in+Iran+-+A+360°+View+of+Gender+Bias)|0|
|[On Truthful Item-Acquiring Mechanisms for Reward Maximization](https://doi.org/10.1145/3589334.3645345)|Liang Shan, Shuo Zhang, Jie Zhang, Zihe Wang||In this research, we study the problem that a collector acquires items from the owner based on the item qualities the owner declares and an independent appraiser's assessments. The owner is interested in maximizing the probability that the collector acquires the items and is the only one who knows the items' factual quality. The appraiser performs her duties with impartiality, but her assessment may be subject to random noises, so it may not accurately reflect the factual quality of the items. The main challenge lies in devising mechanisms that prompt the owner to reveal accurate information, thereby optimizing the collector's expected reward. We consider the menu size of mechanisms as a measure of their practicability and study its impact on the attainable expected reward. For the single-item setting, we design optimal mechanisms with a monotone increasing menu size. Although the reward gap between the simplest and optimal mechanisms is bounded, we show that simple mechanisms with a small menu size cannot ensure any positive fraction of the optimal reward of mechanisms with a larger menu size. For the multi-item setting, we show that an ordinal mechanism that only takes the owner's ordering of the items as input is not incentive-compatible. We then propose a set of Union mechanisms that combine single-item mechanisms. Moreover, we run experiments to examine these mechanisms' robustness against the independent appraiser's assessment accuracy and the items' acquiring rate.|在本研究中，我们研究收集者根据物品所有者声明的物品品质和独立评估者的评估从所有者那里获取物品的问题。所有者感兴趣的是最大化收集器获得项目的概率，并且是唯一知道项目实际质量的人。鉴定人公正地履行职责，但鉴定可能会受到随机噪音的影响，因而不能准确地反映物品的实际质量。主要的挑战在于设计机制，促使所有者披露准确的信息，从而优化收集者的预期回报。我们将机制的菜单大小作为衡量其实用性的一个指标，并研究其对可达到的期望报酬的影响。对于单项设置，我们设计单调增加菜单大小的最佳机制。虽然最简单机构和最优机构之间的奖励差距是有界的，但是我们证明了菜单大小较小的简单机构不能保证菜单大小较大的机构的最优奖励的任何正分数。对于多项目设置，我们表明，一个序机制，只采取所有者的项目作为输入的顺序是不相容的激励。然后，我们提出一组结合单项机制的 Union 机制。此外，我们还进行了实验，以检验这些机制对独立评估者的评估准确度和项目获取率的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Truthful+Item-Acquiring+Mechanisms+for+Reward+Maximization)|0|
|[Barter Exchange with Shared Item Valuations](https://doi.org/10.1145/3589334.3645632)|Juan Luque, Sharmila Duppala, John P. Dickerson, Aravind Srinivasan||In barter exchanges agents enter seeking to swap their items for other items on their wishlist. We consider a centralized barter exchange with a set of agents and items where each item has a positive value. The goal is to compute a (re)allocation of items maximizing the agents' collective utility subject to each agent's total received value being comparable to their total given value. Many such centralized barter exchanges exist and serve crucial roles; e.g., kidney exchange programs, which are often formulated as variants of directed cycle packing. We show finding a reallocation where each agent's total given and total received values are equal is NP-hard. On the other hand, we develop a randomized algorithm that achieves optimal utility in expectation and where, i) for any agent, with probability 1 their received value is at least their given value minus v^* where v^* is said agent's most valuable owned and wished-for item, and ii) each agent's given and received values are equal in expectation.|在易货交易中，代理商试图用他们的物品交换他们愿望清单上的其他物品。我们认为一个集中的易货交换与一组代理商和项目，其中每个项目有积极的价值。目标是计算一个(重新)分配的项目最大化的代理人的集体效用，每个代理人的总收到的价值是可比较的总给定价值。许多这样的集中式易货交易存在并发挥关键作用; 例如，肾脏交换计划，这往往是制定定向循环包装的变体。我们证明了在每个代理的总给定值和总接收值相等的情况下重新分配是 NP 难的。另一方面，我们开发了一个在期望中达到最佳效用的随机化算法，其中，i)对于任何代理，概率1他们的接收值至少是给定值减去 v ^ * ，其中 v ^ * 被称为代理最有价值的拥有和希望得到的物品，ii)每个代理的给定值和接收值在期望中是相等的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Barter+Exchange+with+Shared+Item+Valuations)|0|
|[Efficiency of Non-Truthful Auctions in Auto-bidding with Budget Constraints](https://doi.org/10.1145/3589334.3645636)|Christopher Liaw, Aranyak Mehta, Wennan Zhu||We study the efficiency of non-truthful auctions for auto-bidders with both return on spend (ROS) and budget constraints. The efficiency of a mechanism is measured by the price of anarchy (PoA), which is the worst case ratio between the liquid welfare of any equilibrium and the optimal (possibly randomized) allocation. Our first main result is that the first-price auction (FPA) is optimal, among deterministic mechanisms, in this setting. Without any assumptions, the PoA of FPA is n which we prove is tight for any deterministic mechanism. However, under a mild assumption that a bidder's value for any query does not exceed their total budget, we show that the PoA is at most 2. This bound is also tight as it matches the optimal PoA without a budget constraint. We next analyze two randomized mechanisms: randomized FPA (rFPA) and "quasi-proportional" FPA. We prove two results that highlight the efficacy of randomization in this setting. First, we show that the PoA of rFPA for two bidders is at most 1.8 without requiring any assumptions. This extends prior work which focused only on an ROS constraint. Second, we show that quasi-proportional FPA has a PoA of 2 for any number of bidders, without any assumptions. Both of these bypass lower bounds in the deterministic setting. Finally, we study the setting where bidders are assumed to bid uniformly. We show that uniform bidding can be detrimental for efficiency in deterministic mechanisms while being beneficial for randomized mechanisms, which is in stark contrast with the settings without budget constraints.|研究了同时考虑投资回报率(ROS)和预算约束的汽车投标人非真实拍卖的有效性问题。一个机制的效率是通过无政府状态的代价(PoA)来衡量的，这是任何平衡的液体福利和最优(可能随机)分配之间的最坏情况比。我们的第一个主要结果是，在这种情况下，在确定性机制中，第一价格拍卖(FPA)是最优的。在没有任何假设的情况下，我们证明了 FPA 的 PoA 对于任何确定性机制都是紧的。然而，在一个温和的假设下，任何查询的投标者的价值不会超过他们的总预算，我们显示 PoA 最多是2。这个界限也很紧密，因为它匹配没有预算线的最优 PoA。接下来我们分析了两种随机机制: 随机 FPA (rFPA)和“准比例”FPA。我们证明了两个结果，突出了在这种情况下随机化的功效。首先，我们在不需要任何假设的情况下，证明了两个投标者的 rFPA 的 PoA 最多为1.8。这扩展了先前只关注 ROS 约束的工作。其次，在没有任何假设的情况下，我们证明了对于任意数量的投标者，准比例平均积分有一个2的 PoA。这两个都在确定性设置中绕过了下限。最后，我们研究了假设投标人均匀投标的情况。我们表明，在确定性机制中，统一投标可能不利于效率，而有利于随机机制，这与没有预算约束的设置形成鲜明对比。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficiency+of+Non-Truthful+Auctions+in+Auto-bidding+with+Budget+Constraints)|0|
|[Individual Welfare Guarantees in the Autobidding World with Machine-learned Advice](https://doi.org/10.1145/3589334.3645660)|Yuan Deng, Negin Golrezaei, Patrick Jaillet, Jason Cheuk Nam Liang, Vahab Mirrokni||Online advertising channels commonly focus on maximizing total advertiser welfare to enhance channel health, and previous literature has studied augmenting ad auctions with machine learning predictions on advertiser values (also known asmachine-learned advice ) to improve total welfare. Yet, such improvements could come at the cost of individual bidders' welfare and do not shed light on how particular advertiser bidding strategies impact welfare. Motivated by this, we present an analysis on an individual bidder's welfare loss in the autobidding world for auctions with and without machine-learned advice, and also uncover how advertiser strategies relate to such losses. In particular, we demonstrate how ad platforms can utilize ML advice to improve welfare guarantee on the aggregate and individual bidder level by setting ML advice as personalized reserve prices when the platform consists ofautobidders who maximize value while respecting a return on ad spend (ROAS) constraint. Under parallel VCG auctions with such ML advice-based reserves, we present a worst-case welfare lower-bound guarantee for an individual autobidder, and show that the lower-bound guarantee is positively correlated with ML advice quality as well as the scale of bids induced by the autobidder's bidding strategies. Further, we show that no truthful, and possibly randomized mechanism with anonymous allocations can achieve universally better individual welfare guarantees than VCG, in the presence of personalized reserves based on ML-advice of equal quality. Moreover, we extend our individual welfare guarantee results to generalized first price (GFP) and generalized second price (GSP) auctions. Finally, we present numerical studies using semi-synthetic data derived from ad auction logs of a search ad platform to showcase improvements in individual welfare when setting personalized reserve prices with ML-advice.|在线广告渠道通常关注于最大化广告客户的总体福利，以提高渠道的健康程度，以前的文献研究了利用机器学习预测广告客户价值(也称为机器学习建议)来增加广告拍卖，以提高总体福利。然而，这种改善可能会以个体投标者的福利为代价，而且无法揭示特定广告商的投标策略如何影响福利。受此启发，我们提出了一个个人投标人的福利损失在自动竞价世界的拍卖和没有机器学习的建议，并揭示了如何广告客户策略相关的损失。特别是，我们展示了广告平台如何利用机器学习建议，通过将机器学习建议设置为个性化的保留价格来提高总体和个人投标者水平上的福利保障，当平台由自动投标者组成，他们在尊重广告支出回报(ROAS)约束的同时实现价值最大化。在基于机器学习建议准备金的并行 VCG 拍卖下，我们给出了一个最坏情况下个体自动投标者的福利下限保证，并且证明了下限保证与机器学习建议质量以及自动投标者的投标策略诱导的出价规模正相关。进一步，我们表明，没有真实的，可能随机的匿名分配机制可以实现普遍更好的个人福利保障比 VCG，在个性化储备的基础上，机器学习建议的同等质量。此外，我们将我们的个人福利保障结果推广到广义第一价格(GFP)和广义第二价格(GSP)拍卖。最后，我们利用搜索广告平台的广告拍卖日志得到的半合成数据进行数值研究，以展示在使用 ML 建议设置个性化保留价格时个人福利的改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Individual+Welfare+Guarantees+in+the+Autobidding+World+with+Machine-learned+Advice)|0|
|[Non-uniform Bid-scaling and Equilibria for Different Auctions: An Empirical Study](https://doi.org/10.1145/3589334.3645659)|Yuan Deng, Jieming Mao, Vahab Mirrokni, Yifeng Teng, Song Zuo||In recent years, the growing adoption of autobidding has motivated the study of auction design with value-maximizing auto-bidders. It is known that under mild assumptions, uniform bid-scaling is an optimal bidding strategy in truthful auctions, e.g., Vickrey-Clarke-Groves auction (VCG), and the price of anarchy for VCG is $2$. However, for other auction formats like First-Price Auction (FPA) and Generalized Second-Price auction (GSP), uniform bid-scaling may not be an optimal bidding strategy, and bidders have incentives to deviate to adopt strategies with non-uniform bid-scaling. Moreover, FPA can achieve optimal welfare if restricted to uniform bid-scaling, while its price of anarchy becomes $2$ when non-uniform bid-scaling strategies are allowed. All these price of anarchy results have been focused on welfare approximation in the worst-case scenarios. To complement theoretical understandings, we empirically study how different auction formats (FPA, GSP, VCG) with different levels of non-uniform bid-scaling perform in an autobidding world with a synthetic dataset for auctions. Our empirical findings include: * For both uniform bid-scaling and non-uniform bid-scaling, FPA is better than GSP and GSP is better than VCG in terms of both welfare and profit; * A higher level of non-uniform bid-scaling leads to lower welfare performance in both FPA and GSP, while different levels of non-uniform bid-scaling have no effect in VCG. Our methodology of synthetic data generation may be of independent interest.|近年来，越来越多的自动竞价的采用推动了价值最大化自动竞价拍卖设计的研究。众所周知，在温和的假设下，统一的出价比例是一个最优的出价策略在真实的拍卖，例如，维克里-克拉克-格罗夫斯拍卖(VCG)和无政府状态的价格为2美元。然而，对于第一价格拍卖(FPA)和广义第二价格拍卖(GSP)等其他拍卖形式，统一标尺可能不是最优的竞价策略，竞价者有偏离采用非统一标尺策略的动机。此外，如果限制在均匀标尺下，FPA 可以获得最佳的福利，而当允许非均匀标尺策略时，其无政府状态的价格变为2美元。所有这些无政府结果的代价都集中在最坏情况下的福利近似上。为了补充理论知识，我们实证研究了不同的拍卖格式(FPA，GSP，VCG)和不同水平的非均匀标尺在自动竞价世界中的表现。我们的实证研究结果包括: * 在统一标尺和非统一标尺两种情况下，平均分配比例均优于普惠制，而普惠制在福利和利润方面均优于 VCG; * 较高水平的非统一标尺导致平均分配比例和普惠制的福利表现较差，而不同水平的非统一标尺对 VCG 没有影响。我们的综合数据生成方法可能具有独立的兴趣。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Non-uniform+Bid-scaling+and+Equilibria+for+Different+Auctions:+An+Empirical+Study)|0|
|[A Similarity-based Approach for Efficient Large Quasi-clique Detection](https://doi.org/10.1145/3589334.3645374)|Jiayang Pang, Chenhao Ma, Yixiang Fang||Identifying dense subgraphs called quasi-cliques is pivotal in various graph mining tasks across domains like biology, social networks, and e-commerce. However, recent algorithms still suffer from efficiency issues when mining large quasi-cliques in massive and complex graphs. Our key insight is that vertices within a quasi-clique exhibit similar neighborhoods to some extent. Based on this, we introduce NBSim and FastNBSim, efficient algorithms that find near-maximum quasi-cliques by exploiting vertex neighborhood similarity. FastNBSim further uses MinHash approximations to reduce the time complexity for similarity computation. Empirical evaluation on 10 real-world graphs shows that our algorithms deliver up to three orders of magnitude speedup versus the state-of-the-art algorithms, while ensuring high-quality quasi-clique extraction.|在生物学、社交网络和电子商务等领域的各种图挖掘任务中，识别称为准集团的密集子图是至关重要的。然而，最近的算法仍然受到效率问题，挖掘大规模和复杂图中的大准集团。我们的关键见解是，在一个准集团内的顶点表现出相似的邻域在一定程度上。在此基础上，介绍了 NBSim 和 FastNBSim 两种利用顶点邻域相似性寻找近似最大拟团的高效算法。FastNBSim 进一步使用 MinHash 近似来降低相似度计算的时间复杂度。对10个真实世界图表的实验评估表明，我们的算法相对于最先进的算法提供了高达3数量级的加速，同时确保了高质量的准团体提取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Similarity-based+Approach+for+Efficient+Large+Quasi-clique+Detection)|0|
|[GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning](https://doi.org/10.1145/3589334.3645439)|Yun Zhu, Yaoke Wang, Haizhou Shi, Zhenshuo Zhang, Dian Jiao, Siliang Tang||Graph-structured data is ubiquitous in the world which models complex relationships between objects, enabling various Web applications. Daily influxes of unlabeled graph data on the Web offer immense potential for these applications. Graph self-supervised algorithms have achieved significant success in acquiring generic knowledge from abundant unlabeled graph data. These pre-trained models can be applied to various downstream Web applications, saving training time and improving downstream (target) performance. However, different graphs, even across seemingly similar domains, can differ significantly in terms of attribute semantics, posing difficulties, if not infeasibility, for transferring the pre-trained models to downstream tasks. Concretely speaking, for example, the additional task-specific node information in downstream tasks (specificity) is usually deliberately omitted so that the pre-trained representation (transferability) can be leveraged. The trade-off as such is termed as "transferability-specificity dilemma" in this work. To address this challenge, we introduce an innovative deployment module coined as GraphControl, motivated by ControlNet, to realize better graph domain transfer learning. Specifically, by leveraging universal structural pre-trained models and GraphControl, we align the input space across various graphs and incorporate unique characteristics of target data as conditional inputs. These conditions will be progressively integrated into the model during fine-tuning or prompt tuning through ControlNet, facilitating personalized deployment. Extensive experiments show that our method significantly enhances the adaptability of pre-trained models on target attributed datasets, achieving 1.4-3x performance gain. Furthermore, it outperforms training-from-scratch methods on target data with a comparable margin and exhibits faster convergence.|图形结构化数据在世界上无处不在，它建模对象之间的复杂关系，支持各种 Web 应用程序。网络上每天涌入的未标记图形数据为这些应用程序提供了巨大的潜力。图自监督算法在从大量未标记图数据中获取通用知识方面取得了显著的成功。这些预先训练的模型可以应用于各种下游 Web 应用程序，节省训练时间，提高下游(目标)性能。然而，不同的图，即使是在看似相似的领域，在属性语义方面也可能有很大的不同，对于将预先训练好的模型转移到下游任务造成了困难，如果不是不可行的话。具体地说，例如，在下游任务中额外的特定于任务的节点信息(特异性)通常被故意省略，以便可以利用预先训练的表示(可转移性)。在这项工作中，这种权衡被称为“可转移性-特异性困境”。为了应对这一挑战，我们引入了一个创新的部署模块，称为 GraphControl，由 ControlNet 推动，以实现更好的图域传输学习。具体来说，通过利用通用的结构化预训练模型和 GraphControl，我们跨不同的图对齐输入空间，并将目标数据的独特特征作为条件输入。在通过 ControlNet 进行微调或快速调整期间，这些条件将逐步集成到模型中，从而促进个性化部署。大量实验表明，该方法显著提高了预训练模型对目标属性数据集的适应性，获得了1.4 -3倍的性能增益。此外，该方法对目标数据的训练效果优于从头开始的方法，具有可比较的边界，收敛速度更快。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphControl:+Adding+Conditional+Control+to+Universal+Graph+Pre-trained+Models+for+Graph+Domain+Transfer+Learning)|0|
|[Rethinking Node-wise Propagation for Large-scale Graph Learning](https://doi.org/10.1145/3589334.3645450)|Xunkai Li, Jingyuan Ma, Zhengyu Wu, Daohan Su, Wentao Zhang, RongHua Li, Guoren Wang||Scalable graph neural networks (GNNs) have emerged as a promising technique, which exhibits superior predictive performance and high running efficiency across numerous large-scale graph-based web applications. However, (i) Most scalable GNNs tend to treat all nodes in graphs with the same propagation rules, neglecting their topological uniqueness; (ii) Existing node-wise propagation optimization strategies are insufficient on web-scale graphs with intricate topology, where a full portrayal of nodes' local properties is required. Intuitively, different nodes in web-scale graphs possess distinct topological roles, and therefore propagating them indiscriminately or neglect local contexts may compromise the quality of node representations. This intricate topology in web-scale graphs cannot be matched by small-scale scenarios. To address the above issues, we propose Adaptive Topology-aware Propagation (ATP), which reduces potential high-bias propagation and extracts structural patterns of each node in a scalable manner to improve running efficiency and predictive performance. Remarkably, ATP is crafted to be a plug-and-play node-wise propagation optimization strategy, allowing for offline execution independent of the graph learning process in a new perspective. Therefore, this approach can be seamlessly integrated into most scalable GNNs while remain orthogonal to existing node-wise propagation optimization strategies. Extensive experiments on 12 datasets, including the most representative large-scale ogbn-papers100M, have demonstrated the effectiveness of ATP. Specifically, ATP has proven to be efficient in improving the performance of prevalent scalable GNNs for semi-supervised node classification while addressing redundant computational costs.|可伸缩图神经网络(GNN)已经成为一种很有前途的技术，它在众多大规模的基于图的 Web 应用中表现出优越的预测性能和高效的运行效率。然而，(i)大多数可伸缩的 GNN 倾向于用相同的传播规则处理图中的所有节点，而忽略了它们的拓扑唯一性; (ii)现有的节点传播优化策略对于具有复杂拓扑结构的网络尺度图是不够的，因为在这种情况下需要对节点的本地属性进行全面描述。直观地看，网络尺度图中的不同节点具有不同的拓扑角色，因此，不加区分地传播它们或忽略局部上下文可能会影响节点表示的质量。网络尺度图中这种错综复杂的拓扑结构无法与小尺度场景匹配。为了解决上述问题，我们提出了自适应拓扑感知传播(ATP) ，它减少了潜在的高偏差传播，并以可扩展的方式提取每个节点的结构模式，以提高运行效率和预测性能。值得注意的是，ATP 是一个即插即用的节点传播优化策略，从一个新的角度考虑，允许离线执行独立于图形学习过程。因此，这种方法可以无缝集成到大多数可扩展的 GNN 中，同时与现有的节点传播优化策略保持正交。在12个数据集上的广泛实验，包括最具代表性的大规模 ogbn 论文100M，已经证明了 ATP 的有效性。具体而言，ATP 已被证明是有效的改善性能的普遍可伸缩的 GNN 的半监督节点分类，同时解决冗余计算代价。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Node-wise+Propagation+for+Large-scale+Graph+Learning)|0|
|[Graph Contrastive Learning Reimagined: Exploring Universality](https://doi.org/10.1145/3589334.3645480)|Jiaming Zhuo, Can Cui, Kun Fu, Bingxin Niu, Dongxiao He, Chuan Wang, Yuanfang Guo, Zhen Wang, Xiaochun Cao, Liang Yang||Real-world graphs exhibit diverse structures, including homophilic and heterophilic patterns, necessitating the development of a universal Graph Contrastive Learning (GCL) framework. Nonetheless, the existing GCLs, especially those with a local focus, lack universality due to the mismatch between the input graph structure and the homophily assumption for two primary components of GCLs. Firstly, the encoder, commonly Graph Convolution Network (GCN), operates as a low-pass filter, which assumes the input graph to be homophilic. This makes it challenging to aggregate features from neighbor nodes of the same class on heterophilic graphs. Secondly, the local positive sampling regards neighbor nodes as positive samples, which is inspired by the homophily assumption. This results in feature similarity amplification for the samples from the different classes (i.e., FALSE positive samples). Therefore, it is crucial to feed the encoder and positive sampling of GCLs with homophilic graph structures. This paper presents a novel GCL framework, named gRaph cOntraStive Exploring uNiversality (ROSEN), designed to achieve this objective. Specifically, ROSEN equips a local graph structure inference module, utilizing the Block Diagonal Property (BDP) of the affinity matrix extracted from node ego networks. This module can generate the homophilic graph structure by selectively removing disassortative edges. Extensive evaluations validate the effectiveness and universality of ROSEN across node classification and node clustering tasks.|现实世界中的图表呈现出多样化的结构，包括同族和异族模式，这就需要开发一个通用的图形对比学习(GCL)框架。尽管如此，现有的 GCLs，特别是那些具有局部焦点的 GCLs，由于输入图结构和 GCLs 的两个主要组成部分的同调假设之间的不匹配而缺乏普遍性。首先，编码器(通常是图卷积网络(GCN))作为低通滤波器运行，假设输入图是同态的。这使得在异质图上聚合同类相邻节点的特征具有挑战性。其次，局部正抽样将邻居节点视为正抽样，这是受同态假设的启发。这导致了不同类别样本(即假阳性样本)的特征相似性放大。因此，对具有同态图结构的 GCLs 进行编码器和正采样是至关重要的。本文提出了一个新的 GCL 框架，命名为 gRaph cOntraStive 探索通用性(ROSEN) ，旨在实现这一目标。具体来说，ROSEN 利用从节点自我网络中提取的亲和矩阵的块对角线性质(BDP) ，设计了一个局部图结构推理模块。该模块通过选择性地去除不协调的边来生成同态图结构。广泛的评估验证了 ROSEN 在节点分类和节点聚类任务中的有效性和通用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Contrastive+Learning+Reimagined:+Exploring+Universality)|0|
|[MuGSI: Distilling GNNs with Multi-Granularity Structural Information for Graph Classification](https://doi.org/10.1145/3589334.3645542)|Tianjun Yao, Jiaqi Sun, Defu Cao, Kun Zhang, Guangyi Chen||Recent works have introduced GNN-to-MLP knowledge distillation (KD) frameworks to combine both GNN's superior performance and MLP's fast inference speed. However, existing KD frameworks are primarily designed for node classification within single graphs, leaving their applicability to graph classification largely unexplored. Two main challenges arise when extending KD for node classification to graph classification: (1) The inherent sparsity of learning signals due to soft labels being generated at the graph level; (2) The limited expressiveness of student MLPs, especially in datasets with limited input feature spaces. To overcome these challenges, we introduce MuGSI, a novel KD framework that employs Multi-granularity Structural Information for graph classification. Specifically, we propose multi-granularity distillation loss in MuGSI to tackle the first challenge. This loss function is composed of three distinct components: graph-level distillation, subgraph-level distillation, and node-level distillation. Each component targets a specific granularity of the graph structure, ensuring a comprehensive transfer of structural knowledge from the teacher model to the student model. To tackle the second challenge, MuGSI proposes to incorporate a node feature augmentation component, thereby enhancing the expressiveness of the student MLPs and making them more capable learners. We perform extensive experiments across a variety of datasets and different teacher/student model architectures. The experiment results demonstrate the effectiveness, efficiency, and robustness of MuGSI. Codes are publicly available at: <https://github.com/tianyao-aka/MuGSI>.|最近的研究引入了 GNN-to-MLP 知识精馏(KD)框架，将 GNN 的优越性能和 MLP 的快速推理速度结合起来。然而，现有的 KD 框架主要是为单个图中的节点分类而设计的，它们对图分类的适用性在很大程度上还没有得到探索。当将 KD 用于节点分类扩展到图分类时，出现了两个主要的挑战: (1)由于软标签在图层级生成而固有的学习信号稀疏性; (2)学生 MLP 的有限表达性，特别是在输入特征空间有限的数据集中。为了克服这些挑战，我们引入 MuGSI，一个新的 KD 框架，使用多粒度结构信息进行图分类。具体来说，我们提出了 MuGSI 中的多粒度蒸馏损失来解决第一个挑战。这个损失函数由三个不同的部分组成: 图级精馏、子图级精馏和节点级精馏。每个组件都针对图结构的特定粒度，确保将结构知识从教师模型全面转移到学生模型。为了解决第二个挑战，MuGSI 建议加入一个节点特征增强组件，从而提高学生 MLP 的表达能力，使他们成为更有能力的学习者。我们在各种数据集和不同的教师/学生模型架构上进行广泛的实验。实验结果表明了 MuGSI 的有效性、高效性和鲁棒性。守则可于以下 https://github.com/tianyao-aka/mugsi 公开索取:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MuGSI:+Distilling+GNNs+with+Multi-Granularity+Structural+Information+for+Graph+Classification)|0|
|[Efficient Computation for Diagonal of Forest Matrix via Variance-Reduced Forest Sampling](https://doi.org/10.1145/3589334.3645578)|Haoxin Sun, Zhongzhi Zhang||The forest matrix of a graph, particularly its diagonal elements, has far-reaching implications in network science and machine learning. The state-of-the-art algorithms for the diagonal of forest matrix computation are based on the fast Laplacian solver. However, these algorithms encounter limitations when applied to digraphs due to the incapacity of the Laplacian solver. To overcome the issue, in this paper, we propose three novel sampling-based algorithms:SCF,SCFV,and SCFV+. Our first algorithm SCF leverages a probability interpretation of the diagonal of the forest matrix and utilizes an extension of Wilson's algorithm to sample spanning converging forests. To reduce the variance in the forest sampling, we develop two novel variance-reduced techniques. The first technique, leading to the proposal of the SCFV algorithm, is inspired by opinion dynamics in graphs and applies matrix-vector iteration to the spanning forest sampling. While SCFV achieves reduced variance compared to SCF, the cross-product term in its variance expression can be complex and potentially large in certain graphs. Therefore, we develop another technique, leading to a new iteration equation and the SCFV+ algorithm. SCFV+ achieves further reduced variance without the cross-product term in the variance of SCFV. We prove that SCFV+ can achieve a relative error guarantee with high probability and maintain a linear time complexity relative to the number of nodes in the graph, presenting a superior theoretical result compared to state-of-the-art algorithms. Finally, we conduct extensive experiments on various real-world networks, showing that our algorithms achieve better estimation accuracy and are more time-efficient than the state-of-the-art algorithms. Particularly, our algorithms are scalable to massive graphs with more than twenty million nodes in both undirected and directed graphs.|图的森林矩阵，特别是其对角元素，在网络科学和机器学习中有着深远的意义。森林矩阵对角线计算的最新算法是基于快速拉普拉斯算法的。然而，这些算法遇到的限制时，适用于有向图由于拉普拉斯求解器的能力。为了克服这一问题，本文提出了三种新的基于抽样的算法: SCF、 SCFV 和 SCFV + 。我们的第一个算法 SCF 利用了森林矩阵对角线的概率解释，并利用 Wilson 算法的一个扩展来对跨越会聚森林的样本进行抽样。为了减少森林采样中的方差，我们开发了两种新的方差减少技术。第一种方法是根据图中的意见动力学原理提出的 SCFV 算法，并将矩阵向量迭代应用于生成森林样本。虽然 SCFV 比 SCF 方差更小，但其方差表达式中的交叉乘积项在某些图中可能很复杂并且可能很大。因此，我们发展了另一种技术，导致了一个新的迭代方程和 SCFV + 算法。SCFV + 进一步减少了 SCFV 方差中没有交叉积项的方差。我们证明了 SCFV + 可以实现高概率的相对误差保证，并且相对于图中的节点数保持线性时间复杂度，与现有的算法相比，提出了一个更优越的理论结果。最后，我们在各种实际网络上进行了广泛的实验，结果表明我们的算法比最先进的算法具有更好的估计精度和更高的时间效率。特别是，我们的算法可以扩展到无向图和有向图中有超过二千万个节点的海量图。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Computation+for+Diagonal+of+Forest+Matrix+via+Variance-Reduced+Forest+Sampling)|0|
|[Decoupled Variational Graph Autoencoder for Link Prediction](https://doi.org/10.1145/3589334.3645601)|YoonSik Cho||Link prediction is an important learning task for graph-structured data, and has become increasingly popular due to its wide application areas. Graph Neural Network (GNN)-based approaches including Variational Graph Autoencoder (VGAE) have achieved promising performance on link prediction outperforming conventional models which use hand-crafted features. VGAE learns latent node representations and predicts links based on the similarities between nodes. While the inner product based decoder effectively utilizes the node representations for link prediction, it exhibits sub-optimal performance due to the intrinsic limitation of the inner product. We found that the the cosine similarity and norm simultaneously try to explain the link probability, which hinders the gradient flow during training. We also point out the message passing scheme is unexpectedly dominated by the nodes with large norm values. In this paper, we propose a stochastic VGAE-based method that can effectively decouple the norm and angle in the embeddings. Specifically, we relate the cosine similarity and norm to two fundamental principles in graph: homophily and node popularity respectively. Our learning scheme is based on a hard expectation maximization learning method; we infer which of the two has been exerted for link formation, and subsequently optimize based on this guess. Through extensive experiments on real-world datasets, we demonstrate our model outperforms the existing state-of-the-art methods on link prediction and achieves comparable performances on other downstream tasks such as node classification and clustering. Our code is at https://github.com/yoonsikcho/d-vgae.|链路预测是图结构数据的一项重要学习任务，由于其广泛的应用领域而日益受到人们的重视。基于图神经网络(GNN)的方法，包括变分图自动编码器(VGAE)已经取得了良好的性能，链接预测优于传统的使用手工特征的模型。VGAE 学习潜在节点表示，并根据节点之间的相似性预测链路。基于内积的译码器在有效利用节点表示进行链路预测的同时，由于内积的固有局限性，性能表现为次优。我们发现，余弦距离和范数同时试图解释连接概率，这阻碍了训练过程中的梯度流。我们还指出，消息传递方案意外地被具有大范数值的节点所主导。本文提出了一种基于随机 VGAE 的嵌入算法，该算法能够有效地解耦嵌入过程中的范数和角度。具体来说，我们将余弦距离和规范与图中的两个基本原则联系起来: 同调性原则和节点普及性原则。我们的学习方案是基于一个硬期望最大化学习方法，我们推断两者中的哪一个被用于链接形成，然后基于这个猜测进行优化。通过在实际数据集上的大量实验，我们证明了该模型在链路预测方面优于现有的最先进的方法，并且在节点分类和聚类等其他下游任务上取得了可比较的性能。我们的代码是 https://github.com/yoonsikcho/d-vgae。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decoupled+Variational+Graph+Autoencoder+for+Link+Prediction)|0|
|[ModelGo: A Practical Tool for Machine Learning License Analysis](https://doi.org/10.1145/3589334.3645520)|Moming Duan, Qinbin Li, Bingsheng He||Productionizing machine learning projects is inherently complex, involving a multitude of interconnected components that are assembled like LEGO blocks and evolve throughout development lifecycle. These components encompass software, databases, and models, each subject to various licenses governing their reuse and redistribution. However, existing license analysis approaches for Open Source Software (OSS) are not well-suited for this context. For instance, some projects are licensed without explicitly granting sublicensing rights, or the granted rights can be revoked, potentially exposing their derivatives to legal risks. Indeed, the analysis of licenses in machine learning projects grows significantly more intricate as it involves interactions among diverse types of licenses and licensed materials. To the best of our knowledge, no prior research has delved into the exploration of license conflicts within this domain. In this paper, we introduce ModelGo, a practical tool for auditing potential legal risks in machine learning projects to enhance compliance and fairness. With ModelGo, we present license assessment reports based on five use cases with diverse model-reusing scenarios, rendered by real-world machine learning components. Finally, we summarize the reasons behind license conflicts and provide guidelines for minimizing them. Our code is publicly available at https://github.com/Xtra-Computing/ModelGo.|生产化的机器学习项目本身就很复杂，涉及到大量相互关联的组件，这些组件像乐高积木一样被组装起来，并在整个开发生命周期中不断演进。这些组件包括软件、数据库和模型，每个组件都受制于管理其重用和再分发的各种许可证。然而，现有的开放源码软件(OSS)许可证分析方法并不适合这种情况。例如，一些项目没有明确授予次级许可权就获得了许可，或者授予的权利可能被撤销，这可能使它们的衍生品面临法律风险。事实上，机器学习项目中的许可证分析变得更加复杂，因为它涉及到不同类型的许可证和许可材料之间的交互。据我们所知，还没有任何先前的研究深入探索这个领域内的许可冲突。在本文中，我们介绍了 ModelGo，一个实用的工具，用于审计机器学习项目中的潜在法律风险，以提高遵从性和公平性。使用 ModelGo，我们基于五个具有不同模型重用场景的用例，通过真实世界的机器学习组件呈现许可评估报告。最后，我们总结了许可冲突背后的原因，并提供了减少这些冲突的指导方针。我们的代码可以在 https://github.com/xtra-computing/modelgo 上公开获取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ModelGo:+A+Practical+Tool+for+Machine+Learning+License+Analysis)|0|
|[Content Moderation and the Formation of Online Communities: A Theoretical Framework](https://doi.org/10.1145/3589334.3645490)|Cynthia Dwork, Chris Hays, Jon M. Kleinberg, Manish Raghavan||We study the impact of content moderation policies in online communities. In our theoretical model, a platform chooses a content moderation policy and individuals choose whether or not to participate in the community according to the fraction of user content that aligns with their preferences. The effects of content moderation, at first blush, might seem obvious: it restricts speech on a platform. However, when user participation decisions are taken into account, its effects can be more subtle $\unicode{x2013}$ and counter-intuitive. For example, our model can straightforwardly demonstrate how moderation policies may increase participation and diversify content available on the platform. In our analysis, we explore a rich set of interconnected phenomena related to content moderation in online communities. We first characterize the effectiveness of a natural class of moderation policies for creating and sustaining stable communities. Building on this, we explore how resource-limited or ideological platforms might set policies, how communities are affected by differing levels of personalization, and competition between platforms. Our model provides a vocabulary and mathematically tractable framework for analyzing platform decisions about content moderation.|我们研究了在线社区内容管制政策的影响。在我们的理论模型中，一个平台选择一个内容管理政策，个人根据与他们偏好相一致的用户内容的比例来选择是否参与社区。乍一看，内容管制的效果似乎很明显: 它限制了平台上的言论。但是，当考虑到用户参与决策时，其效果可能更为微妙，而且与直觉相反。例如，我们的模型可以直接演示调节策略如何增加参与，并使平台上可用的内容多样化。在我们的分析中，我们探索了与在线社区中的内容审核相关的一组丰富的相互关联的现象。我们首先描述了一类自然的节制政策在创建和维持稳定社区方面的有效性。在此基础上，我们探讨资源有限或意识形态平台如何制定政策，社区如何受到不同层次的个性化影响，以及平台之间的竞争。我们的模型提供了一个词汇表和数学上易处理的框架，用于分析关于内容管理的平台决策。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Content+Moderation+and+the+Formation+of+Online+Communities:+A+Theoretical+Framework)|0|
|[Getting Bored of Cyberwar: Exploring the Role of Low-level Cybercrime Actors in the Russia-Ukraine Conflict](https://doi.org/10.1145/3589334.3645401)|Anh V. Vu, Daniel R. Thomas, Ben Collier, Alice Hutchings, Richard Clayton, Ross J. Anderson||There has been substantial commentary on the role of cyberattacks carried by low-level cybercrime actors in the Russia-Ukraine conflict. We analyse 358k web defacement attacks, 1.7M reflected DDoS attacks, 1764 Hack Forums posts mentioning the two countries, and 441 announcements (with 58k replies) of a volunteer hacking group for two months before and four months after the invasion. We find the conflict briefly but notably caught the attention of low-level cybercrime actors, with significant increases in online discussion and both types of attack targeting Russia and Ukraine. However, there was little evidence of high-profile actions; the role of these players in the ongoing hybrid warfare is minor, and they should be separated from persistent and motivated 'hacktivists' in state-sponsored operations. Their involvement in the conflict appears to have been short-lived and fleeting, with a clear loss of interest in discussing the situation and carrying out both defacement and DDoS attacks against either Russia or Ukraine after a few weeks.|关于低级别网络犯罪行为者在俄罗斯-乌克兰冲突中发动的网络攻击的作用，已有大量评论。我们分析了35.8万次网络破坏攻击，170万次 DDoS 攻击，1764个黑客论坛帖子提到了这两个国家，以及441个志愿者黑客组织的声明(回复了58000条) ，分别发生在入侵前两个月和入侵后四个月。我们发现，这场冲突短暂但引人注目地引起了低级别网络犯罪行为者的注意，网上讨论大幅增加，针对俄罗斯和乌克兰的两类攻击也有所增加。然而，几乎没有高调行动的证据; 这些参与者在正在进行的混合战争中的作用是微不足道的，他们应该与在国家支持的行动中坚持和积极的“黑客活动分子”分开。他们参与冲突的时间似乎很短暂，而且转瞬即逝，几周后，他们显然对讨论局势、对俄罗斯或乌克兰进行涂鸦和 DDoS 攻击失去了兴趣。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Getting+Bored+of+Cyberwar:+Exploring+the+Role+of+Low-level+Cybercrime+Actors+in+the+Russia-Ukraine+Conflict)|0|
|[The Double Edged Sword: Identifying Authentication Pages and their Fingerprinting Behavior](https://doi.org/10.1145/3589334.3645493)|Asuman Senol, Alisha Ukani, Dylan Cutler, Igor Bilogrevic||Browser fingerprinting is often associated with cross-site user tracking, a practice that many browsers (e.g., Safari, Brave, Edge, Firefox, and Chrome) want to block. However, less is publicly known about its uses to enhance online safety, where it can provide an additional security layer against service abuses (e.g., in combination with CAPTCHAs) or during user authentication. To the best of our knowledge, no fingerprinting defenses deployed thus far consider this important distinction when blocking fingerprinting attempts, so they might negatively affect website functionality and security. To address this issue we make three main contributions. First, we introduce a novel machine learning-based method to automatically identify authentication pages (i.e. login and sign-up pages). Our supervised algorithm achieves 96-98% precision and recall on a manually-labelled dataset of almost 1,000 popular sites. Second, we compare our algorithm with methods from prior works on the same dataset, showing that it significantly outperforms all of them. Third, we quantify the prevalence of fingerprinting scripts across login and sign-up pages (10.2%) versus those executed on other pages (9.2%); while the rates of fingerprinting are similar, home pages and authentication pages differ in the third-party scripts they include and how often these scripts are labeled as tracking. We also highlight the substantial differences in fingerprinting on login and sign-up pages. Our work sheds light on the complicated reality that fingerprinting is used to both protect user security and invade user privacy; this dual nature must be considered by fingerprinting mitigations.|浏览器指纹识别通常与跨站点用户跟踪有关，这是许多浏览器(例如 Safari、 Brave、 Edge、 Firefox 和 Chrome)都想阻止的一种做法。然而，对于其用于加强网络安全的用途，公众知之甚少，因为它可以提供一个额外的安全层，防止滥用服务(例如，与 CAPTCHA 结合使用)或在用户认证期间。据我们所知，到目前为止还没有部署指纹防御系统在阻止指纹识别尝试时考虑到这一重要区别，因此它们可能会对网站的功能和安全性产生负面影响。为了解决这个问题，我们做出了三个主要贡献。首先，我们介绍一种新的基于机器学习的方法来自动识别认证页面(即登录和注册页面)。我们的监督算法在手工标记的近1000个热门网站的数据集上达到了96-98% 的准确率召回率。其次，我们将我们的算法与之前在同一数据集上使用的方法进行了比较，结果表明该算法的性能明显优于所有方法。第三，我们量化了登录和注册页面上指纹脚本的流行率(10.2%)与其他页面上执行的指纹脚本的流行率(9.2%) ; 虽然指纹识别的比率相似，但是主页和认证页面在它们包含的第三方脚本中有所不同，以及这些脚本被标记为跟踪的频率有多高。我们还强调了在登录和注册页面上指纹识别的实质性差异。我们的工作揭示了复杂的现实，即指纹识别被用来保护用户安全和侵犯用户隐私; 这种双重性质必须通过指纹识别缓解来考虑。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Double+Edged+Sword:+Identifying+Authentication+Pages+and+their+Fingerprinting+Behavior)|0|
|["Are Adversarial Phishing Webpages a Threat in Reality?" Understanding the Users' Perception of Adversarial Webpages](https://doi.org/10.1145/3589334.3645502)|Ying Yuan, Qingying Hao, Giovanni Apruzzese, Mauro Conti, Gang Wang||Machine learning based phishing website detectors (ML-PWD) are a critical part of today's anti-phishing solutions in operation. Unfortunately, ML-PWD are prone to adversarial evasions, evidenced by both academic studies and analyses of real-world adversarial phishing webpages. However, existing works mostly focused on assessing adversarial phishing webpages against ML-PWD, while neglecting a crucial aspect: investigating whether they can deceive the actual target of phishing – the end users. In this paper, we fill this gap by conducting two user studies (n=470) to examine how human users perceive adversarial phishing webpages, spanning both synthetically crafted ones (which we create by evading a state-of-the-art ML-PWD) as well as real adversarial webpages (taken from the wild Web) that bypassed a production-grade ML-PWD. Our findings confirm that adversarial phishing is a threat to both users and ML-PWD, since most adversarial phishing webpages have comparable effectiveness on users w.r.t. unperturbed ones. However, not all adversarial perturbations are equally effective. For example, those with added typos are significantly more noticeable to users, who tend to overlook perturbations of higher visual magnitude (such as replacing the background). We also show that users' self-reported frequency of visiting a brand's website has a statistically negative correlation with their phishing detection accuracy, which is likely caused by overconfidence. We release our resources.|基于机器学习的网络钓鱼网站检测器(ML-PWD)是当今运行中的反钓鱼解决方案的重要组成部分。不幸的是，ML-PWD 倾向于对抗性的回避，学术研究和对现实世界的对抗性钓鱼网页的分析都证明了这一点。然而，现有的工作主要集中在评估针对 ML-PWD 的对抗性钓鱼网页，而忽略了一个关键方面: 调查他们是否能欺骗钓鱼的实际目标——最终用户。在这篇论文中，我们通过进行两个用户研究(n = 470)来填补这个空白，以检查人类用户如何感知对抗性钓鱼网页，跨越综合制作的网页(我们通过避开最先进的 ML-PWD 来创建)和真正的对抗性网页(从野生网页) ，绕过生产级别的 ML-PWD。我们的研究结果证实，对抗性网络钓鱼对用户和 ML-PWD 都是一种威胁，因为大多数对抗性网络钓鱼网页对于用户来说都具有相当的有效性。然而，并非所有的对抗性扰动都同样有效。例如，那些添加了拼写错误的用户更容易注意到，他们往往会忽略更高视觉级别的扰动(比如替换背景)。我们还发现，用户自我报告的访问某品牌网站的频率与其网络钓鱼检测的准确性在统计学上呈负相关，这可能是由于过度自信造成的。我们释放我们的资源。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q="Are+Adversarial+Phishing+Webpages+a+Threat+in+Reality?"+Understanding+the+Users'+Perception+of+Adversarial+Webpages)|0|
|[Hyperlink Hijacking: Exploiting Erroneous URL Links to Phantom Domains](https://doi.org/10.1145/3589334.3645510)|Kevin Saric, Felix Savins, Gowri Sankar Ramachandran, Raja Jurdak, Surya Nepal||Web users often follow hyperlinks hastily, expecting them to be correctly programmed. However, it is possible those links contain typos or other mistakes. By discovering active but erroneous hyperlinks, a malicious actor can spoof a website or service, impersonating the expected content and phishing private information. In 'typosquatting,' misspellings of common domains are registered to exploit errors when users mistype a web address. Yet, no prior research has been dedicated to situations where the linking errors of web publishers (i.e. developers and content contributors) propagate to users. We hypothesize that these 'hijackable hyperlinks' exist in large quantities with the potential to generate substantial traffic. Analyzing large-scale crawls of the web using high-performance computing, we show the web currently contains active links to more than 572,000 dot-com domains that have never been registered, what we term 'phantom domains.' Registering 51 of these, we see 88% of phantom domains exceeding the traffic of a control domain, with up to 10 times more visits. Our analysis shows that these links exist due to 17 common publisher error modes, with the phantom domains they point to free for anyone to purchase and exploit for under 20, representing a low barrier to entry for potential attackers.|网络用户经常匆忙地跟随超链接，希望它们被正确编程。但是，这些链接可能包含错误或其他错误。通过发现活跃但错误的超链接，恶意行为者可以欺骗网站或服务，模仿预期的内容和钓鱼隐私信息。在“排版”(typosquatting)中，当用户键入错误的网址时，常见域名的拼写错误会被注册以利用错误。然而，先前没有研究专门针对网络出版商(即开发者和内容贡献者)的链接错误传播给用户的情况。我们假设这些“可劫持的超链接”大量存在，有可能产生大量流量。通过分析使用高性能计算的大规模网络爬虫，我们发现网络当前包含超过572,000个从未注册过的.com 域名的活跃链接，我们称之为“幽灵域名”。注册其中的51个，我们看到88% 的幻象域超过控制域的流量，访问量高达10倍以上。我们的分析表明，这些链接的存在是由于17个常见的发布者错误模式，与幻象域名，他们指向免费为任何人购买和利用20以下，代表一个低门槛进入潜在的攻击者。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hyperlink+Hijacking:+Exploiting+Erroneous+URL+Links+to+Phantom+Domains)|0|
|[Fake Resume Attacks: Data Poisoning on Online Job Platforms](https://doi.org/10.1145/3589334.3645524)|Michiharu Yamashita, Thanh Tran, Dongwon Lee||While recent studies have exposed various vulnerabilities incurred from data poisoning attacks in many web services, little is known about the vulnerability on online professional job platforms (e.g., LinkedIn and Indeed). In this work, first time, we demonstrate the critical vulnerabilities found in the common Human Resources (HR) task of matching job seekers and companies on online job platforms. Capitalizing on the unrestricted format and contents of job seekers' resumes and easy creation of accounts on job platforms, we demonstrate three attack scenarios: (1) company promotion attack to increase the likelihood of target companies being recommended, (2) company demotion attack to decrease the likelihood of target companies being recommended, and (3) user promotion attack to increase the likelihood of certain users being matched to certain companies. To this end, we develop an end-to-end "fake resume" generation framework, titled FRANCIS, that induces systematic prediction errors via data poisoning. Our empirical evaluation on real-world datasets reveals that data poisoning attacks can markedly skew the results of matchmaking between job seekers and companies, regardless of underlying models, with vulnerability amplified in proportion to poisoning intensity. These findings suggest that the outputs of various services from job platforms can be potentially hacked by malicious users.|虽然最近的研究已经暴露了许多网络服务中的数据中毒攻击所带来的各种漏洞，但是对于在线专业工作平台(例如 LinkedIn 和》)上的漏洞知之甚少。在这项工作中，第一次，我们证明了在一般的人力资源(HR)任务匹配求职者和在线求职平台上的公司中发现的关键弱点。利用求职者简历中不受限制的格式和内容以及在求职平台上容易创建的账户，我们展示了三种攻击场景: (1)公司促销攻击，以增加目标公司被推荐的可能性; (2)公司降级攻击，以降低目标公司被推荐的可能性; (3)用户促销攻击，以增加某些用户与某些公司匹配的可能性。为此，我们开发了一个端到端的“虚假简历”生成框架，名为 FRANCIS，该框架通过数据中毒诱导系统预测错误。我们对现实世界数据集的实证评估表明，数据中毒攻击可以明显扭曲求职者和公司之间匹配的结果，无论潜在模型如何，脆弱性与中毒强度成比例放大。这些发现表明，来自工作平台的各种服务的输出可能被恶意用户黑客攻击。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fake+Resume+Attacks:+Data+Poisoning+on+Online+Job+Platforms)|0|
|[Identifying VPN Servers through Graph-Represented Behaviors](https://doi.org/10.1145/3589334.3645552)|Chenxu Wang, Jiangyi Yin, Zhao Li, Hongbo Xu, Zhongyi Zhang, Qingyun Liu||Identifying VPN servers is a crucial task in various situations, such as geo-fraud detection, bot traffic analysis and network attack identification. Although numerous studies that focus on network traffic detection have achieved excellent performance in closed-world scenarios, particularly those methods based on deep learning, they may exhibit significant performance degradation due to changes in network environment. To mitigate this issue, a few studies have attempted to use methods based on active probing to detect VPN servers. However, these methods still have two limitations. They cannot handle situations without probing responses and are limited in applicability due to their focus on specific VPNs. In this work, we propose VPNChecker, which utilizes the graph-represented behaviors to detect VPN servers in real-world scenarios. VPNChecker outperforms existing methods in four offline datasets. The results from our datasets, containing multiple different VPNs, indicate that VPNChecker has better applicability. Furthermore, we deploy VPNChecker in an Internet Service Provider's (ISP) environment to evaluate its effectiveness. The results show that VPNChecker can improve the coverage of sophisticated detection engines and serve as a complement to existing methods.|在地理欺诈检测、机器人流量分析和网络攻击识别等多种情况下，识别 VPN 服务器是一项非常重要的任务。尽管许多关于网络流量检测的研究已经在封闭世界的场景中取得了很好的性能，特别是那些基于深度学习的方法，但是由于网络环境的变化，它们可能表现出显著的性能下降。为了缓解这个问题，一些研究试图使用基于主动探测的方法来检测 VPN 服务器。然而，这些方法仍然有两个局限性。它们不能在不探测反应的情况下处理各种情况，并且由于侧重于特定的虚拟专用网，其适用性受到限制。在这项工作中，我们提出了 VPNChecker，它利用图表示的行为来检测真实场景中的 VPN 服务器。VPNChecker 在四个脱机数据集中的性能优于现有方法。从我们的数据集，包含多个不同的 VPN 的结果表明，VPNChecker 有更好的适用性。此外，我们在 Internet 服务提供商(ISP)环境中部署 VPNChecker 来评估其有效性。结果表明，VPNChecker 可以提高复杂检测引擎的覆盖率，并可以作为现有检测方法的补充。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Identifying+VPN+Servers+through+Graph-Represented+Behaviors)|0|
|[Discovering and Measuring CDNs Prone to Domain Fronting](https://doi.org/10.1145/3589334.3645656)|Karthika Subramani, Roberto Perdisci, PierrosChristos Skafidas, Manos Antonakakis||Domain fronting is a network communication technique that involves leveraging (or abusing) content delivery networks (CDNs) to disguise the final destination of network packets by presenting them as if they were intended for a different domain than their actual endpoint. This technique can be used for both benign and malicious purposes, such as circumventing censorship or hiding malware-related communications from network security systems. Since domain fronting has been known for a few years, some popular CDN providers have implemented traffic filtering approaches to curb its use at their CDN infrastructure. However, it remains unclear to what extent domain fronting has been mitigated. To better understand whether domain fronting can still be effectively used, we propose a systematic approach to discover CDNs that are still prone to domain fronting. To this end, we leverage passive and active DNS traffic analysis to pinpoint domain names served by CDNs and build an automated tool that can be used to discover CDNs that allow domain fronting in their infrastructure. Our results reveal that domain fronting is feasible in 22 out of 30 CDNs that we tested, including some major CDN providers like Akamai and Fastly. This indicates that domain fronting remains widely available and can be easily abused for malicious purposes.|域前端是一种网络通信技术，涉及到利用(或滥用)内容传递网络(CDN)来掩盖网络数据包的最终目的地，将它们表示为与其实际端点不同的域。这种技术既可以用于良性目的，也可以用于恶意目的，例如规避审查或在网络安全系统中隐藏与恶意软件相关的通信。由于域前端已经知道了几年，一些流行的 CDN 提供商已经实施了流量过滤方法，以抑制其在 CDN 基础设施中的使用。然而，目前尚不清楚域名前置问题在多大程度上得到了缓解。为了更好地理解域前端是否仍然可以有效地使用，我们提出了一个系统的方法来发现 CDN 仍然是易于领域前端。为此，我们利用被动和主动 DNS 流量分析来精确定位由 CDN 服务的域名，并建立一个自动化工具，可用于发现允许在其基础设施中进行域名前置处理的 CDN。我们的研究结果显示，在我们测试的30个 CDN 中，有22个是可行的，包括一些主要的 CDN 提供商，如 Akamai 和 Fastly。这表明域名前置仍然是广泛可用的，并且很容易被滥用于恶意目的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Discovering+and+Measuring+CDNs+Prone+to+Domain+Fronting)|0|
|[Exploring Unconfirmed Transactions for Effective Bitcoin Address Clustering](https://doi.org/10.1145/3589334.3645684)|Kai Wang, Yakun Cheng, Michael Wen Tong, Zhenghao Niu, Jun Pang, Weili Han||The development of clustering heuristics has demonstrated that Bitcoin is not completely anonymous. Currently, existing clustering heuristics only consider confirmed transactions recorded in the Bitcoin blockchain. However, unconfirmed transactions in the mempool have yet to be utilized to improve the performance of the clustering heuristics. In this paper, we bridge this gap by combining unconfirmed and confirmed transactions for clustering Bitcoin addresses effectively. First, we present a data collection system for capturing unconfirmed transactions. Two case studies are performed to show the presence of user behaviors in unconfirmed transactions not present in confirmed transactions. Next, we apply the state-of-the-art clustering heuristics to unconfirmed transactions, and the clustering results can reduce the number of entities after applying, for example, the co-spend heuristics in confirmed transactions by 2.3%. Finally, we propose three novel clustering heuristics to capture specific behavior patterns in unconfirmed transactions, which further reduce the number of entities after the application of the co-spend heuristics by 9.8%. Our results demonstrate the utility of unconfirmed transactions in address clustering and further shed light on the limitations of anonymity in cryptocurrencies. To the best of our knowledge, this paper is the first to apply the unconfirmed transactions in Bitcoin to cluster addresses.|聚类启发法的发展表明，比特币并非完全匿名。目前，现有的集群试探法只考虑比特币区块链中记录的已确认交易。然而，内存池中未经证实的事务还没有被用来改进集群启发式算法的性能。在本文中，我们通过结合未确认和已确认的交易有效地聚类比特币地址来弥补这一差距。首先，我们提出了一个数据收集系统捕捉未确认的交易。两个案例研究表明，用户行为的存在在未经证实的交易中不存在的确认交易。接下来，我们将最先进的聚类启发法应用于未确认交易，聚类结果可以减少实体的数量，例如，在确认交易中共同支出启发法可以减少2.3% 。最后，我们提出了三种新的聚类启发式算法来捕获未确认事务中的特定行为模式，使得共花费启发式算法应用后的实体数量进一步减少了9.8% 。我们的研究结果证明了未确认事务在地址聚类中的有效性，并进一步揭示了加密货币中匿名性的局限性。据我们所知，本文是第一篇将未经证实的比特币交易应用于集群地址的论文。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Unconfirmed+Transactions+for+Effective+Bitcoin+Address+Clustering)|0|
|[AdFlush: A Real-World Deployable Machine Learning Solution for Effective Advertisement and Web Tracker Prevention](https://doi.org/10.1145/3589334.3645698)|Kiho Lee, Chaejin Lim, Beomjin Jin, Taeyoung Kim, Hyoungshick Kim||Conventional ad blocking and tracking prevention tools often fall short in addressing web content manipulation. Machine learning approaches have been proposed to enhance detection accuracy, yet aspects of practical deployment have frequently been overlooked. This paper introduces AdFlush, a novel machine learning model for real-world browsers. To develop AdFlush, we evaluated the effectiveness of 883 features, ultimately selecting 27 key features for optimal performance. We tested AdFlush on a dataset of 10,000 real-world websites, achieving an F1 score of 0.98, thereby outperforming AdGraph (F1 score: 0.93), WebGraph (F1 score: 0.90), and WTAgraph (F1 score: 0.84). Additionally, AdFlush significantly reduces computational overhead, requiring 56% less CPU and 80% less memory than AdGraph. We also assessed AdFlush's robustness against adversarial manipulations, demonstrating superior resilience with F1 scores ranging from 0.89 to 0.98, surpassing the performance of AdGraph and WebGraph, which recorded F1 scores between 0.81 and 0.87. A six-month longitudinal study confirmed that AdFlush maintains a high F1 score above 0.97 without the need for retraining, underscoring its effectiveness.|传统的广告拦截和跟踪预防工具往往不能解决网页内容操纵问题。为了提高检测的准确性，机器学习方法已经被提出，然而实际部署的方面却经常被忽视。本文介绍了 AdFlush，一种面向真实世界浏览器的新型机器学习模型。为了开发 AdFlush，我们评估了883个特性的有效性，最终选择了27个关键特性以获得最佳性能。我们在10,000个真实世界网站的数据集上测试了 AdFlush，获得了0.98的 F1分数，从而优于 AdGraph (F1分数: 0.93) ，WebGraph (F1分数: 0.90)和 WTAgraph (F1分数: 0.84)。此外，AdFlush 显著降低了计算开销，比 AdGraph 减少了56% 的 CPU 和80% 的内存。我们还评估了 AdFlush 对抗对手操纵的稳健性，证明了 F1评分范围从0.89到0.98的优越弹性，超过了 AdGraph 和 WebGraph 的性能，后者记录的 F1评分在0.81和0.87之间。一项为期六个月的追踪研究证实，AdFlush 在不需要再培训的情况下，仍然保持着高于0.97的 f 1分数，这强调了它的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdFlush:+A+Real-World+Deployable+Machine+Learning+Solution+for+Effective+Advertisement+and+Web+Tracker+Prevention)|0|
|[Fingerprinting the Shadows: Unmasking Malicious Servers with Machine Learning-Powered TLS Analysis](https://doi.org/10.1145/3589334.3645719)|Andreas Theofanous, Eva Papadogiannaki, Alexander Shevtsov, Sotiris Ioannidis||Over the last few years, the adoption of encryption in network traffic has been constantly increasing. The percentage of encrypted communications worldwide is estimated to exceed 90%. Although network encryption protocols mainly aim to secure and protect users' online activities and communications, they have been exploited by malicious entities that hide their presence in the network. It was estimated that in 2022, more than 85% of the malware used encrypted communication channels. In this work, we examine state-of-the-art fingerprinting techniques and extend a machine learning pipeline for effective and practical server classification. Specifically, we actively contact servers to initiate communication over the TLS protocol and through exhaustive requests, we extract communication metadata. We investigate which features favor an effective classification, following state-of-the-art approaches. Our extended pipeline can indicate whether a server is malicious or not with 91% precision and 95% recall, while it can specify the botnet family with 99% precision and 99% recall.|在过去的几年中，加密技术在网络流量中的应用不断增加。全世界加密通信的比例估计超过90% 。尽管网络加密协议的主要目的是保护和保护用户的在线活动和通信，但它们已被恶意实体利用，这些恶意实体隐藏了用户在网络中的存在。据估计，在2022年，超过85% 的恶意软件使用加密通信渠道。在这项工作中，我们检查国家的最先进的指纹技术和扩展机器学习管道有效和实际的服务器分类。具体来说，我们主动与服务器联系，通过 TLS 协议启动通信，并通过详尽的请求提取通信元数据。我们调查哪些特征有利于有效的分类，遵循最先进的方法。我们的扩展流水线可以以91% 的精度和95% 的召回率指示服务器是否恶意，同时可以以99% 的精度和99% 的召回率指定僵尸网络系列。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fingerprinting+the+Shadows:+Unmasking+Malicious+Servers+with+Machine+Learning-Powered+TLS+Analysis)|0|
|[Efficient Computation of Signature-Restricted Views for Semantic Web Ontologies](https://doi.org/10.1145/3589334.3645317)|Yizheng Zhao||Uniform Interpolation (UI) is an advanced reasoning service used to narrow down an ontology to a restricted view. This new ontology, known as a uniform interpolant, will only consist of the ''relevant names'', yet it will retain their original meanings. UI is immensely promising due to its applicability across various domains where custom views of ontologies are essential. Nonetheless, to unlock its full potential, we need optimized techniques to generate these tailored views. Previous studies suggest that creating uniform interpolants for EL-ontologies is notably challenging. In some instances, it is not even feasible to compute a uniform interpolant; when feasible, the size of the uniform interpolant can be up to triple exponentially larger than the source ontology. Despite these challenges, our paper introduces an improved ''forgetting'' technique specifically designed for computing uniform interpolants of ELI-ontologies. We demonstrate that, with good normalization and inference strategies, such uniform interpolants can be efficiently computed, just as quickly as computing ''modules''. A comprehensive evaluation with a prototypical implementation of the method shows superb success rates over two popular benchmark datasets, demonstrating a clear computational advantage over state-of-the-art approaches.|统一插值(UI)是一种先进的推理服务，用于将本体缩小到受限视图。这种新的本体论，称为统一插值，将只包括“相关的名称”，但它将保留其原来的意义。UI 是非常有前途的，因为它适用于各种领域，其中定制的本体视图是必不可少的。尽管如此，为了释放其全部潜力，我们需要优化技术来生成这些量身定制的视图。先前的研究表明，为 EL 本体创建统一的插值是非常具有挑战性的。在某些情况下，计算一致插值甚至是不可行的; 在可行的情况下，一致插值的大小可以达到源本体的三倍指数大。尽管存在这些挑战，我们的文章还是介绍了一种改进的“遗忘”技术，专门用于计算 ELI 本体的一致插值。我们证明，有了良好的归一化和推理策略，这样的一致插值可以有效地计算，一样快的计算“模块”。对该方法的原型实现进行了全面的评估，结果显示，与两个流行的基准数据集相比，该方法具有极高的成功率，显示出与最先进的方法相比，该方法具有明显的计算优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Computation+of+Signature-Restricted+Views+for+Semantic+Web+Ontologies)|0|
|[Follow the Path: Hierarchy-Aware Extreme Multi-Label Completion for Semantic Text Tagging](https://doi.org/10.1145/3589334.3645558)|Natalia Ostapuk, Julien Audiffren, Ljiljana Dolamic, Alain Mermoud, Philippe CudréMauroux||Extreme Multi Label (XML) problems, and in particular XML completion -- the task of prediction the missing labels of an entity -- have attracted significant attention in the past few years. Most XML completion problems can organically leverage a label hierarchy, which can be represented as a tree that encodes the relations between the different labels. In this paper, we propose a new algorithm, HECTOR - Hierarchical Extreme Completion for Text based on TransfORmer, to solve XML Completion problems more effectively. HECTOR operates by directly predicting paths in the label tree rather than individual labels, thus taking advantage of information encoded in the hierarchy. Due to the sequential aspect of these paths, HECTOR can leverage the effectiveness and performance of the Transformer architecture to outperform state-of-the-art of XML completion methods. Extensive evaluations on three real-world datasets demonstrate the effectiveness of our approach for XML completion. We compare HECTOR with several state-of-the-art XML completion methods for various completion problems, and in particular for label refinement, i.e., the scenario where only the coarse labels (i.e. the first few top levels in a taxonomy) are observed. Empirical results on three different datasets show that our method significantly outperforms the state of the art, with HECTOR frequently outperforming previous techniques by more than 10% according to multiple metrics.|在过去的几年中，极端多标签(XML)问题，尤其是 XML 完成(预测实体丢失标签的任务)已经引起了广泛的关注。大多数 XML 完成问题都可以有机地利用标签层次结构，这种层次结构可以表示为对不同标签之间的关系进行编码的树。为了更有效地解决 XML 补全问题，本文提出了一种新的算法——基于 TransfORmer 的 HECTOR 文本分层极限补全算法。HECTOR 的操作方式是直接预测标签树中的路径，而不是单个标签，从而利用层次结构中编码的信息。由于这些路径的顺序方面，HECTOR 可以利用 Transformer 体系结构的有效性和性能来超越最先进的 XML 完成方法。对三个实际数据集的广泛评估证明了我们的 XML 完成方法的有效性。我们将 HECTOR 与几种最先进的 XML 完成方法进行比较，以解决各种完成问题，尤其是标签细化，即只观察到粗标签(即分类法中的前几个顶级级别)的情况。在三个不同数据集上的实验结果表明，我们的方法明显优于最先进的技术，根据多个指标，HECTOR 的性能经常比以前的技术高出10% 以上。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Follow+the+Path:+Hierarchy-Aware+Extreme+Multi-Label+Completion+for+Semantic+Text+Tagging)|0|
|[Multi-Label Zero-Shot Product Attribute-Value Extraction](https://doi.org/10.1145/3589334.3645649)|Jiaying Gong, Hoda Eldardiry||E-commerce platforms should provide detailed product descriptions (attribute values) for effective product search and recommendation. However, attribute value information is typically not available for new products. To predict unseen attribute values, large quantities of labeled training data are needed to train a traditional supervised learning model. Typically, it is difficult, time-consuming, and costly to manually label large quantities of new product profiles. In this paper, we propose a novel method to efficiently and effectively extract unseen attribute values from new products in the absence of labeled data (zero-shot setting). We propose HyperPAVE, a multi-label zero-shot attribute value extraction model that leverages inductive inference in heterogeneous hypergraphs. In particular, our proposed technique constructs heterogeneous hypergraphs to capture complex higher-order relations (i.e. user behavior information) to learn more accurate feature representations for graph nodes. Furthermore, our proposed HyperPAVE model uses an inductive link prediction mechanism to infer future connections between unseen nodes. This enables HyperPAVE to identify new attribute values without the need for labeled training data. We conduct extensive experiments with ablation studies on different categories of the MAVE dataset. The results demonstrate that our proposed HyperPAVE model significantly outperforms existing classification-based, generation-based large language models for attribute value extraction in the zero-shot setting.|电子商务平台应提供详细的产品描述(属性值) ，以便进行有效的产品搜索和推荐。但是，属性值信息通常不适用于新产品。为了预测未知的属性值，需要大量的标记训练数据来训练传统的监督式学习模型。通常，手动标记大量的新产品配置文件是困难的、耗时的和昂贵的。在本文中，我们提出了一种新的方法，有效地提取未见属性值的新产品在没有标记的数据(零拍设置)。我们提出了 HyperPAVE，一个利用异构超图中归纳推理的多标签零冲值属性值提取模型。特别是，我们提出的技术构造异构超图来捕获复杂的高阶关系(即用户行为信息) ，以学习更准确的图节点特征表示。此外，我们提出的 HyperPAVE 模型使用感应链路预测机制来推断未来未知节点之间的连接。这使得 HyperPAVE 能够识别新的属性值，而不需要标记的训练数据。我们进行了广泛的实验与烧蚀研究的不同类别的 MAVE 数据集。结果表明，我们提出的 HyperPAVE 模型明显优于现有的基于分类的、基于生成的大语言模型，在零拍摄环境下用于属性值提取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Label+Zero-Shot+Product+Attribute-Value+Extraction)|0|
|[Aligning Out-of-Distribution Web Images and Caption Semantics via Evidential Learning](https://doi.org/10.1145/3589334.3645653)|Guohao Sun, Yue Bai, Xueying Yang, Yi Fang, Yun Fu, Zhiqiang Tao||Vision-language models, pre-trained on web-scale datasets, have the potential to greatly enhance the intelligence of web applications (e.g., search engines, chatbots, and art tools). Precisely, these models align disparate domains into a co-embedding space, achieving impressive zero-shot performance on multi-modal tasks (e.g., image-text retrieval, VQA). However, existing methods often rely on well-prepared data that less frequently contain noise and variability encountered in real-world scenarios, leading to severe performance drops in handling out-of-distribution (OOD) samples. This work first comprehensively analyzes the performance drop between in-distribution (ID) and OOD retrieval. Based on empirical observations, we introduce a novel approach, Evidential Language-Image Posterior (ELIP), to achieve robust alignment between web images and semantic knowledge across various OOD cases by leveraging evidential uncertainties. The proposed ELIP can be seamlessly integrated into general image-text contrastive learning frameworks, providing an efficient fine-tuning approach without exacerbating the need for additional data. To validate the effectiveness of ELIP, we systematically design a series of OOD cases (e.g., image distortion, spelling errors, and a combination of both) on two benchmark datasets to mimic noisy data in real-world web applications. Our experimental results demonstrate that ELIP improves the performance and robustness of mainstream pre-trained vision-language models facing OOD samples in image-text retrieval tasks.|视觉语言模型，在网络规模的数据集上预先训练，有潜力大大提高网络应用程序的智能(例如，搜索引擎，聊天机器人和艺术工具)。准确地说，这些模型将不同的域对齐到一个协同嵌入空间中，在多模态任务(例如，图像文本检索，VQA)上实现了令人印象深刻的零射击性能。然而，现有的方法往往依赖于准备充分的数据，这些数据不太经常包含在真实场景中遇到的噪声和可变性，导致处理分布不均(OOD)样本的性能严重下降。本文首先全面分析了内部分发(ID)和面向对象检索之间的性能差异。在实证研究的基础上，我们引入了一种新的方法——证据语言-图像后验(ELIP) ，通过利用证据不确定性实现不同 OOD 情况下网络图像和语义知识之间的鲁棒对齐。拟议的 ELIP 可以无缝地整合到一般的图文对比学习框架中，提供一种有效的微调方法，而不会加剧对额外数据的需求。为了验证 ELIP 的有效性，我们在两个基准数据集上系统地设计了一系列面向对象分析的案例(例如，畸变、拼写错误，以及两者的组合) ，以模拟现实世界网络应用程序中的噪音数据。实验结果表明，在图像文本检索任务中，ELIP 提高了面向 OOD 样本的主流预训练视觉语言模型的性能和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Aligning+Out-of-Distribution+Web+Images+and+Caption+Semantics+via+Evidential+Learning)|0|
|[Deliberate Exposure to Opposing Views and Its Association with Behavior and Rewards on Political Communities](https://doi.org/10.1145/3589334.3645375)|Alexandros Efstratiou||Engaging with diverse political views is important for reaching better collective decisions, however, users online tend to remain confined within ideologically homogeneous spaces. In this work, we study users who are members of these spaces but who also show a willingness to engage with diverse views, as they have the potential to introduce more informational diversity into their communities. Across four Reddit communities (r/Conservative, r/The_Donald, r/ChapoTrapHouse, r/SandersForPresident), we find that these users tend to use less hostile and more advanced and personable language, but receive fewer social rewards from their peers compared to others. We also find that social sanctions on the discussion community r/changemyview are insufficient to drive them out in the short term, though they may play a role over the longer term.|接触不同的政治观点对于达成更好的集体决策很重要，然而，在线用户往往被限制在意识形态相同的空间内。在这项工作中，我们研究了属于这些空间成员但也表示愿意接触不同观点的用户，因为他们有可能为其社区引入更多的信息多样性。在 Reddit 的四个社区(r/Insurance，r/The _ Donald，r/ChapoTrapHouse，r/SandersForPresident)中，我们发现这些用户倾向于使用更少的敌意，更先进和有个性的语言，但是从他们的同龄人那里得到的社会奖励比其他人少。我们还发现，对讨论社区 r/changemyview 的社会制裁不足以在短期内驱逐它们，尽管它们可能在较长期内发挥作用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deliberate+Exposure+to+Opposing+Views+and+Its+Association+with+Behavior+and+Rewards+on+Political+Communities)|0|
|[Invariant Graph Learning for Causal Effect Estimation](https://doi.org/10.1145/3589334.3645549)|Yongduo Sui, Caizhi Tang, Zhixuan Chu, Junfeng Fang, Yuan Gao, Qing Cui, Longfei Li, Jun Zhou, Xiang Wang||Causal effect estimation from networked observational data encounters notable challenges, primarily hidden confounders arising from network structure, or spillover effects that influence unit's outcomes based on neighboring treatment assignments. Existing graph neural network (GNN)-based methods have endeavored to address these challenges, utilizing the GNN's message-passing mechanism to capture hidden confounders or model spillover effects. However, they mainly focus on transductive causal effect learning on a single networked data, limiting their efficacy in inductive settings for real-world applications where networked data often originates from multiple environments influenced by potentially varying time or geographical regions. In light of this, we introduce the principle of invariance to the task of causal effect estimation on networked data, culminating in our Invariant Graph Learning (IGL) framework. Specifically, it first generates multiple networked data to simulate diverse environments from a given observational data. Then it further encourages the model to learn environment-invariant representations for confounders and spillover effects. Such a design enables the model to extrapolate beyond a single observed environment, thereby improving the performance of causal effect estimation in potential new environments. Extensive experiments on two real-world datasets demonstrates the superiority of our approach.|网络化观测数据的因果效应估计遇到了显著的挑战，主要是由网络结构引起的隐藏混杂因素，或者基于相邻治疗分配影响单元结果的溢出效应。现有的基于图形神经网络(GNN)的方法致力于解决这些挑战，利用 GNN 的消息传递机制来捕获隐藏的混杂因素或模型溢出效应。然而，它们主要侧重于对单个网络数据的传导性因果效应学习，限制了它们在现实世界应用的归纳环境中的功效，因为网络数据往往来自受潜在变化的时间或地理区域影响的多个环境。鉴于此，我们将不变性原理引入到网络数据的因果效应估计任务中，最终形成我们的不变图学习(IGL)框架。具体来说，它首先从给定的观测数据生成多个网络数据来模拟不同的环境。然后进一步鼓励模型学习混杂因素和溢出效应的环境不变表示。这样的设计使模型能够超越单个观测环境进行外推，从而提高了在潜在新环境中因果效应估计的性能。在两个实际数据集上的大量实验证明了该方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Invariant+Graph+Learning+for+Causal+Effect+Estimation)|0|
|[Sublinear-Time Opinion Estimation in the Friedkin-Johnsen Model](https://doi.org/10.1145/3589334.3645572)|Stefan Neumann, Yinhao Dong, Pan Peng||Online social networks are ubiquitous parts of modern societies and the discussions that take place in these networks impact people's opinions on diverse topics, such as politics or vaccination. One of the most popular models to formally describe this opinion formation process is the Friedkin–Johnsen (FJ) model, which allows to define measures, such as the polarization and the disagreement of a network. Recently, Xu, Bao and Zhang (WebConf'21) showed that all opinions and relevant measures in the FJ model can be approximated in near-linear time. However, their algorithm requires the entire network and the opinions of all nodes as input. Given the sheer size of online social networks and increasing data-access limitations, obtaining the entirety of this data might, however, be unrealistic in practice. In this paper, we show that node opinions and all relevant measures, like polarization and disagreement, can be efficiently approximated in time that is sublinear in the size of the network. Particularly, our algorithms only require query-access to the network and do not have to preprocess the graph. Furthermore, we use a connection between FJ opinion dynamics and personalized PageRank, and show that in d-regular graphs, we can deterministically approximate each node's opinion by only looking at a constant-size neighborhood, independently of the network size. We also experimentally validate that our estimation algorithms perform well in practice.|在线社交网络是现代社会中无处不在的一部分，在这些网络中发生的讨论会影响人们对不同话题的看法，比如政治或疫苗接种。形式上描述这种观点形成过程的最流行的模型之一是 Friedkin-Johnsen (FJ)模型，它允许定义衡量标准，例如网络的极化和分歧。最近，徐、鲍、张(WebConf’21)表明，FJ 模型中的所有观点和相关措施都可以在近似线性时间内进行近似。然而，他们的算法需要整个网络和所有节点的意见作为输入。然而，考虑到在线社交网络的庞大规模和日益增加的数据访问限制，获取这些数据的全部在实践中可能是不现实的。在本文中，我们表明，节点意见和所有相关措施，如极化和分歧，可以有效地近似的时间，是次线性的网络规模。特别地，我们的算法只需要对网络进行查询访问，而不需要对图进行预处理。此外，我们使用 FJ 意见动态和个性化 PageRank 之间的联系，并表明在 d- 正则图中，我们可以确定性地近似每个节点的意见，只看一个恒定大小的邻域，独立于网络大小。我们还通过实验验证了我们的估计算法在实际应用中的良好性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sublinear-Time+Opinion+Estimation+in+the+Friedkin-Johnsen+Model)|0|
|[Bots, Elections, and Controversies: Twitter Insights from Brazil's Polarised Elections](https://doi.org/10.1145/3589334.3645651)|Diogo Pacheco||From 2018 to 2023, Brazil experienced its most fiercely contested elections in history, resulting in the election of far-right candidate Jair Bolsonaro followed by the left-wing, Lula da Silva. This period was marked by a murder attempt, a coup attempt, the pandemic, and a plethora of conspiracy theories and controversies. This paper analyses 437 million tweets originating from 13 million accounts associated with Brazilian politics during these two presidential election cycles. We focus on accounts' behavioural patterns. We noted a quasi-monotonic escalation in bot engagement, marked by notable surges both during COVID-19 and in the aftermath of the 2022 election. The data revealed a strong correlation between bot engagement and the number of replies during a single day ($r=0.66$, $p<0.01$). Furthermore, we identified a range of suspicious activities, including an unusually high number of accounts being created on the same day, with some days witnessing over 20,000 new accounts and super-prolific accounts generating close to 100,000 tweets. Lastly, we uncovered a sprawling network of accounts sharing Twitter handles, with a select few managing to utilise more than 100 distinct handles. This work can be instrumental in dismantling coordinated campaigns and offer valuable insights for the enhancement of bot detection algorithms.|从2018年到2023年，巴西经历了历史上竞争最激烈的选举，导致极右翼候选人雅伊尔 · 博索纳罗(Jair Bolsonaro)当选，随后是左翼人士卢拉 · 达席尔瓦(Lula da Silva)。这个时期的特点是谋杀未遂，政变未遂，流行病，以及过多的阴谋论和争议。本文分析了在这两个总统选举周期中，来自1300万个与巴西政治有关的账户的4.37亿条推文。我们关注客户的行为模式。我们注意到，机器人程序的参与程度出现了近乎单调的升级，无论是在2019冠状病毒疾病期间还是在2022年大选之后，都出现了明显的激增。数据显示，聊天机器人的参与度与一天内回复的数量之间存在很强的相关性($r = 0.66 $，$p < 0.01 $)。此外，我们还发现了一系列可疑活动，包括在同一天创建的账户数量异常之多，有些日子见证了超过20,000个新账户和产生近100,000条推文的超多产账户。最后，我们发现了一个庞大的帐户共享网络的 Twitter 句柄，其中一些设法使用超过100个不同的句柄。这项工作可以有助于拆除协调运动，并提供宝贵的见解增强机器人检测算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bots,+Elections,+and+Controversies:+Twitter+Insights+from+Brazil's+Polarised+Elections)|0|
|[Scalable Continuous-time Diffusion Framework for Network Inference and Influence Estimation](https://doi.org/10.1145/3589334.3645652)|Keke Huang, Ruize Gao, Bogdan Cautis, Xiaokui Xiao||The study of continuous-time information diffusion has been an important area of research for many applications in recent years. When only the diffusion traces (cascades) are accessible, cascade-based network inference and influence estimation are two essential problems to explore. Alas, existing methods exhibit limited capability to infer and process networks with more than a few thousand nodes, suffering from scalability issues. In this paper, we view the diffusion process as a continuous-time dynamical system, based on which we establish a continuous-time diffusion model. Subsequently, we instantiate the model to a scalable and effective framework (FIM) to approximate the diffusion propagation from available cascades, thereby inferring the underlying network structure. Furthermore, we undertake an analysis of the approximation error of FIM for network inference. To achieve the desired scalability for influence estimation, we devise an advanced sampling technique and significantly boost the efficiency. We also quantify the effect of the approximation error on influence estimation theoretically. Experimental results showcase the effectiveness and superior scalability of FIM on network inference and influence estimation.|连续时间信息扩散的研究是近年来许多应用领域的一个重要研究方向。当只有扩散轨迹(级联)可访问时，基于级联的网络推理和影响估计是两个需要研究的基本问题。遗憾的是，现有的方法在推断和处理超过几千个节点的网络方面能力有限，存在可伸缩性问题。在本文中，我们把扩散过程看作是一个连续时间的动力系统，在此基础上我们建立了一个连续时间扩散模型。随后，我们将该模型实例化为一个可扩展的有效框架(FIM) ，以近似可用级联的扩散传播，从而推断出潜在的网络结构。此外，我们亦会分析 FIM 在网络推理方面的逼近误差。为了实现影响估计所需的可扩展性，我们设计了一种先进的采样技术，大大提高了效率。我们还从理论上量化了逼近误差对影响估计的影响。实验结果表明 FIM 在网络推理和影响估计方面的有效性和优越的可扩展性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Continuous-time+Diffusion+Framework+for+Network+Inference+and+Influence+Estimation)|0|
|[Friend or Foe? Mining Suspicious Behavior via Graph Capsule Infomax Detector against Fraudsters](https://doi.org/10.1145/3589334.3645706)|Xiangping Zheng, Bo Wu, Xun Liang, Wei Li||Anomaly detection on graphs has recently attracted considerable attention due to its broad range of high-impact applications, including cybersecurity, financial transactions, and recommendation systems. Although many efforts have thus far been made, how to effectively handle the high inconsistency between users' behavior and labels, a fundamental issue in anomaly detection, has not yet received sufficient concern. Moreover, the inconsistency problem is hard to investigate and even deteriorates the performance of anomaly detectors. To this end, we propose a novel graph self-supervised learning framework, Capsule Graph Infomax (termed CapsGI), to overcome the inconsistency of anomaly detection. Inspired by the recent advances of capsules on images, we explore another possibility of reforming the node embedding by capsule ideas to represent the unique node's properties. Concretely, by disentangling heterogeneous factors underlying each node representation, we can establish node capsules such that their representation can reflect intrinsic node properties. To strengthen the connection among normal nodes, CapsGI further represents the part-whole contrastive learning between lower-level capsules (part) and higher-level capsules (whole) by explicitly considering the context graph relations. Extensive experiments on multiple real-world datasets demonstrate that our model significantly outperforms state-of-the-art models.|图表异常检测最近引起了广泛的关注，因为它的应用范围非常广泛，包括网络安全、金融交易和推荐系统。尽管到目前为止已经做了很多努力，但是如何有效地处理用户行为和标签之间的高度不一致性，这是异常检测的一个基本问题，还没有得到足够的关注。此外，不一致性问题很难研究，甚至会降低异常检测器的性能。为此，我们提出了一个新的图形自监督学习框架，胶囊图形信息量(CapsGI) ，以克服异常检测的不一致性。受到近年来胶囊图像技术的启发，我们探索了另一种可能性，即通过胶囊思想改造节点嵌入，以表现独特的节点性质。具体地说，通过分离每个节点表示下的异质因素，我们可以建立节点胶囊，使它们的表示能够反映节点的内在属性。为了加强正常节点之间的联系，CapsGI 通过明确考虑上下文图关系进一步表示低层胶囊(部分)和高层胶囊(整体)之间的部分-整体对比学习。在多个真实世界数据集上的大量实验表明，我们的模型明显优于最先进的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Friend+or+Foe?+Mining+Suspicious+Behavior+via+Graph+Capsule+Infomax+Detector+against+Fraudsters)|0|
|[PASS: Predictive Auto-Scaling System for Large-scale Enterprise Web Applications](https://doi.org/10.1145/3589334.3645330)|Yunda Guo, Jiake Ge, Panfeng Guo, Yunpeng Chai, Tao Li, Mengnan Shi, Yang Tu, Jian Ouyang||We confront two challenges in the management of a vast and diverse array of online web applications deployed on enterprise-grade auto-scaling infrastructure, primarily focused on ensuring Quality of Service (QoS) for large-scale applications and optimizing resource costs. Firstly, reacting to increased load with a response-based approach can temporarily degrade QoS because many web applications need a few minutes to warm up. Therefore, precise workload prediction is critical for predictive scaling. However, our analysis of real-world applications underscores the substantial challenges arising from the limited precision and robustness of existing single prediction algorithms in the context of predictive auto-scaling. Secondly, guaranteeing the QoS of online applications within a cost-effective structure is crucial, as it is inherently linked to corporate profitability. Nevertheless, our study shows that mainstream auto-scaling methods exhibit various limitations, either being unsuitable for online environments or inadequately ensuring QoS. To address these issues, we introduce PASS, a Predictive Auto-Scaling System tailored for large-scale online web applications in enterprise settings. Our highly robust and accurate prediction framework dynamically integrates and calibrates appropriate prediction algorithms based on the unique characteristics of each application to effectively manage workload diversity. We further establish a performance model derived from online historical logs, enhancing auto-scaling to ensure diverse QoS without adverse impacts on online applications. Additionally, we implement a reactive strategy grounded in queuing theory to promptly address QoS violations resulting from inaccurate predictions or unexpected events. Across a wide spectrum of applications and real-world workloads, PASS outperforms state-of-the-art methods, achieving higher workload prediction accuracy and a superior QoS guarantee rate with less resource cost.|我们在管理大量和多样化的在线 Web 应用程序方面面临两个挑战，这些应用程序部署在企业级的自动伸缩基础设施上，主要关注于确保大规模应用程序的服务质量(QoS)和优化资源成本。首先，使用基于响应的方法来应对增加的负载可能会暂时降低 QoS，因为许多 Web 应用程序需要几分钟的预热时间。因此，精确的工作负载预测对预测缩放至关重要。然而，我们对实际应用的分析强调了现有的单一预测算法在预测性自动缩放方面的有限精度和鲁棒性所带来的实质性挑战。其次，在一个具有成本效益的结构中保证在线应用程序的服务质量是至关重要的，因为它与公司的盈利能力有着内在的联系。然而，我们的研究表明，主流的自动缩放方法表现出各种各样的局限性，要么不适合在线环境，要么不能充分保证 QoS。为了解决这些问题，我们介绍了 PASS，这是一个预测性自动缩放系统，专为企业环境中的大规模在线 Web 应用程序而设计。我们高度健壮和准确的预测框架根据每个应用程序的独特特征动态地集成和校准适当的预测算法，以有效地管理工作负载多样性。我们进一步建立了源自在线历史日志的性能模型，增强了自动伸缩性，以确保不同的 QoS 而不会对在线应用程序产生不利影响。此外，我们实现了一个基于排队论的被动策略，以及时处理由不准确的预测或意外事件引起的服务质量违规。在广泛的应用程序和现实世界的工作负载中，PASS 优于最先进的方法，实现了更高的工作负载预测精度和更低的资源成本的更高的 QoS 保证率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PASS:+Predictive+Auto-Scaling+System+for+Large-scale+Enterprise+Web+Applications)|0|
|[Unlocking the Non-deterministic Computing Power with Memory-Elastic Multi-Exit Neural Networks](https://doi.org/10.1145/3589334.3645340)|Jiaming Huang, Yi Gao, Wei Dong||With the increasing demand for Web of Things (WoT) and edge computing, the efficient utilization of limited computing power on edge devices is becoming a crucial challenge. Traditional neural networks (NNs) as web services rely on deterministic computational resources. However, they may fail to output the results on non-deterministic computing power which could be preempted at any time, degrading the task performance significantly. Multi-exit NNs with multiple branches have been proposed as a solution, but the accuracy of intermediate results may be unsatisfactory. In this paper, we propose MEEdge, a system that automatically transforms classic single-exit models into heterogeneous and dynamic multi-exit models which enables Memory-Elastic inference at the Edge with non-deterministic computing power. To build heterogeneous multi-exit models, MEEdge uses efficient convolutions to form a branch zoo and High Priority First (HPF)-based branch placement method for branch growth. To adapt models to dynamically varying computational resources, we employ a novel on-device scheduler for collaboration. Further, to reduce the memory overhead caused by dynamic branches, we propose neuron-level weight sharing and few-shot knowledge distillation(KD) retraining. Our experimental results show that models generated by MEEdge can achieve up to 27.31% better performance than existing multi-exit NNs.|随着对物联网和边缘计算需求的不断增长，有限的计算能力在边缘设备上的有效利用已经成为一个重要的挑战。传统的神经网络(NN)作为 Web 服务依赖于确定性的计算资源。然而，它们可能无法在任何时候都可能被抢占的非确定性计算能力上输出结果，从而显著降低任务性能。多出口多分支神经网络已被提出作为一种解决方案，但中间结果的准确性可能不能令人满意。本文提出了 MEEdge 系统，该系统将经典的单出口模型自动转换为异构的动态多出口模型，使边缘处的记忆弹性推理具有非确定性的计算能力。为了建立异构的多出口模型，MEEdge 利用有效的卷积构造了一个分支动物园，并采用基于高优先级优先(HPF)的分支布局方法进行分支生长。为了使模型适应动态变化的计算资源，我们采用了一种新的设备上协作调度器。进一步，为了减少由动态分支引起的记忆开销，我们提出了神经元级权重共享和少镜头知识提取(KD)再训练方法。实验结果表明，MEEdge 生成的模型比现有的多出口神经网络性能提高了27.31% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unlocking+the+Non-deterministic+Computing+Power+with+Memory-Elastic+Multi-Exit+Neural+Networks)|0|
|[ZipZap: Efficient Training of Language Models for Large-Scale Fraud Detection on Blockchain](https://doi.org/10.1145/3589334.3645352)|Sihao Hu, Tiansheng Huang, KaHo Chow, Wenqi Wei, Yanzhao Wu, Ling Liu||Language models (LMs) have demonstrated superior performance in detecting fraudulent activities on Blockchains. Nonetheless, the sheer volume of Blockchain data results in excessive memory and computational costs when training LMs from scratch, limiting their capabilities to large-scale applications. In this paper, we present ZipZap, a framework tailored to achieve both parameter and computational efficiency when training LMs on large-scale transaction data. First, with the frequency-aware compression, an LM can be compressed down to a mere 7.5% of its initial size with an imperceptible performance dip. This technique correlates the embedding dimension of an address with its occurrence frequency in the dataset, motivated by the observation that embeddings of low-frequency addresses are insufficiently trained and thus negating the need for a uniformly large dimension for knowledge representation. Second, ZipZap accelerates the speed through the asymmetric training paradigm: It performs transaction dropping and cross-layer parameter-sharing to expedite the pre-training process, while revert to the standard training paradigm for fine-tuning to strike a balance between efficiency and efficacy, motivated by the observation that the optimization goals of pre-training and fine-tuning are inconsistent. Evaluations on real-world, large-scale datasets demonstrate that ZipZap delivers notable parameter and computational efficiency improvements for training LMs. Our implementation is available at: https://github.com/git-disl/ZipZap.|语言模型(LM)在检测区块链上的欺诈活动方面表现出优越的性能。尽管如此，在从零开始训练 LMs 时，Blockchain 数据的庞大数量会导致内存和计算成本过高，从而限制了它们在大规模应用中的能力。在本文中，我们提出了 ZipZap，一个框架，以实现参数和计算效率时，训练大规模事务数据的 LM。首先，通过频率感知压缩，LM 可以被压缩到其初始尺寸的7.5% ，同时具有不易察觉的性能下降。这种技术将地址的嵌入维度与其在数据集中的出现频率相关联，其动机是观察到低频地址的嵌入没有得到充分的训练，从而消除了对知识表示的一致大维度的需要。其次，ZipZap 通过非对称训练范式加快了速度: 它执行事务丢弃和跨层参数共享来加快预训练过程，同时回到标准的训练范式进行微调以在效率和效力之间达到平衡，这是由于观察到预训练和微调的优化目标不一致。对现实世界中大规模数据集的评估表明，ZipZap 为训练 LM 提供了显著的参数和计算效率改进。我们的实施方案可参阅以下 https://github.com/git-disl/zipzap  :。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ZipZap:+Efficient+Training+of+Language+Models+for+Large-Scale+Fraud+Detection+on+Blockchain)|0|
|[Cold Start or Hot Start? Robust Slow Start in Congestion Control with A Priori Knowledge for Mobile Web Services](https://doi.org/10.1145/3589334.3645393)|Jia Zhang, Haixuan Tong, Enhuan Dong, Xin Qian, Mingwei Xu, Xiaotian Li, Zili Meng||Mobile web services value a quick loading of contents in the first page, which is quantified by the above-the-fold time of the first page (first AFT) and is likely to fall into the slow start phase in congestion control. However, the widely deployed slow start mechanism is "cold start", which manually hardcodes the parameters and is not suitable for the first AFT of heterogeneous mobile web services. We revisit the slow start mechanism and find that it could be optimized with a priori knowledge. However, blindly relying on a priori knowledge is not robust enough to handle the fluctuating mobile networks and unpredictable application traffic. In this paper, we propose WiseStart, a "hot-start-based" slow start mechanism. WiseStart utilizes the priori knowledge to set the initial parameters, continuously probes the new connection to handle the fluctuating network conditions, and carefully adapts to the application-limit scenarios. We implement WiseStart in a popular mobile web service online in production. Comprehensive experiments demonstrate that WiseStart reduces the First AFT by 25.43% and the average RCT at connection establishment by 16.15% compared to the default slow start mechanism and other state-of-the-art baselines.|移动网络服务的价值在于快速加载第一页的内容，这是由第一页(第一个 AFT)的上方折叠时间来量化的，并且很可能在拥塞控制中进入缓慢启动阶段。然而，目前广泛应用的慢启动机制是“冷启动”，它是手工硬编码的参数，不适合异构移动 Web 服务的首次 AFT。我们重新审视缓慢启动机制，发现它可以用先验知识进行优化。然而，盲目依赖先验知识不足以处理波动的移动网络和不可预测的应用流量。在本文中，我们提出了 WiseStart，一种“基于热启动”的慢启动机制。WiseStart 利用先验知识设置初始参数，不断探测新连接以处理波动的网络条件，并仔细适应应用程序限制场景。我们在一个流行的在线移动 Web 服务中实现了 WiseStart。综合实验表明，与默认的慢启动机制和其他最先进的基线相比，WiseStart 将第一个 AFT 减少了25.43% ，连接建立时的平均 RCT 减少了16.15% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cold+Start+or+Hot+Start?+Robust+Slow+Start+in+Congestion+Control+with+A+Priori+Knowledge+for+Mobile+Web+Services)|0|
|[Investigations of Top-Level Domain Name Collisions in Blockchain Naming Services](https://doi.org/10.1145/3589334.3645459)|Daiki Ito, Yuta Takata, Hiroshi Kumagai, Masaki Kamizono||Traditionally, top-level domains (TLDs) are managed by the Internet corporation for assigned names and numbers (ICANN), and the domain names under them are managed by registrars. Against such a centralized management, a blockchain naming service (BNS) has been proposed to manage TLDs on blockchains without authority intervention. BNS users can register TLD strings as non-fungible tokens and manage the TLD root zone. However, such a decentralized management results in the introduction of a new security issue, BNS TLD name collision, wherein the same TLD is registered in several different BNSs. In this study, we investigated BNS TLD name collisions by analyzing TLDs registered on two BNSs: Handshake and Decentraweb. Specifically, we collected TLDs registered in Handshake and Decentraweb and the associated data, and analyzed the data registration status of BNS TLDs and BNS TLD name collisions. The analysis of 11,595,406 Handshake and 11,889 Decentraweb TLDs revealed 6,973 BNS TLD name collisions. In particular, lastname TLDs, which are intended for use as person names, yielded a large number of registered domain names. In addition, the analysis identified 10 name collisions between the BNS and operational ICANN TLDs. Further, the ICANN TLD candidates under review also had name collisions against the BNS TLDs. Consequently, based on the characteristics of these name collisions and discussions in BNS communities, we considered countermeasures against BNS TLD name collisions. For the further development of BNSs, we believe that it is essential to discuss with the existing Internet communities and coexist with the existing Internet.|传统上，顶级域名(TLD)是由互联网公司为指定的名称和数字(ICANN)管理的，其下的域名由注册机构管理。针对这种集中式管理，提出了一种区块链命名服务(BNS)来管理区块链上的 TLD，而无需权限干预。BNS 用户可以将 TLD 字符串注册为不可替换的令牌并管理 TLD 根区域。然而，这种分散管理的结果，引入了一个新的安全问题，BNS TLD 名称冲突，其中相同的 TLD 注册在几个不同的 BNS。在这项研究中，我们调查了 BNS TLD 名称冲突通过分析 TLD 登记在两个 BNS: 握手和 Decentraweb。具体来说，我们收集了在握手和 Decentraweb 中注册的 TLD 以及相关数据，并分析了 BNS TLD 和 BNS TLD 名称冲突的数据注册状态。对11,595,406个握手和11,889个 Decentraweb TLD 的分析揭示了6,973个 BNS TLD 名称冲突。特别是，用作人名的姓氏顶级域名产生了大量的注册域名。此外，分析确定了10个名称之间的冲突 BNS 和运营 ICANN 顶级域名。此外，被审查的 ICANN TLD 候选者也与 BNS TLD 有名称冲突。因此，根据这些名称冲突的特点和在 BNS 社区的讨论，我们考虑了针对 BNS TLD 名称冲突的对策。我们认为，为了进一步发展网络营销系统，有必要与现有的互联网社区进行讨论，并与现有的互联网共存。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Investigations+of+Top-Level+Domain+Name+Collisions+in+Blockchain+Naming+Services)|0|
|[Don't Bite Off More than You Can Chew: Investigating Excessive Permission Requests in Trigger-Action Integrations](https://doi.org/10.1145/3589334.3645721)|Liuhuo Wan, Kailong Wang, Kulani Mahadewa, Haoyu Wang, Guangdong Bai||Web-based trigger-action platforms (TAP) allow users to integrate Internet of Things (IoT) systems and online services into trigger-action integrations (TAIs), facilitating rich automation tasks known as applets. Despite their benefits, these integrations~(typically involving the TAP, trigger, and action service providers) pose significant security and privacy challenges, such as mis-triggering and data leakage. This work investigates cross-entity permission management within TAIs to address the underlying causes of these security and privacy issues, emphasizing permission-functionality consistency to ensure fairness in permission requests. We introduce PFCon, a system that leverages GPT-based language models for analyzing required and requested permissions, revealing excessive permission requests in a large-scale study of IFTTT TAP. Our findings highlight the need for service providers to enforce permission-functionality consistency, raising awareness of the importance of security and privacy in TAI.|基于网络的触发动作平台(TAP)允许用户将物联网(IoT)系统和在线服务集成到触发动作集成(TAI)中，促进称为 applet 的丰富的自动化任务。尽管这些集成具有很多优点，但是它们(通常涉及 TAP、触发器和操作服务提供商)仍然带来了严重的安全和隐私挑战，例如错误触发和数据泄漏。本文研究 TAI 中的跨实体权限管理，以解决这些安全和隐私问题的根本原因，强调权限-功能的一致性，以确保权限请求的公平性。我们介绍 PFCon，这是一个利用基于 GPT 的语言模型来分析所需权限和请求权限的系统，在 IFTTT TAP 的大规模研究中揭示了过多的权限请求。我们的研究结果强调了服务提供商强制执行权限-功能一致性的必要性，提高了对 TAI 中安全和隐私重要性的认识。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Don't+Bite+Off+More+than+You+Can+Chew:+Investigating+Excessive+Permission+Requests+in+Trigger-Action+Integrations)|0|
|[Off-Policy Evaluation of Slate Bandit Policies via Optimizing Abstraction](https://doi.org/10.1145/3589334.3645343)|Haruka Kiyohara, Masahiro Nomura, Yuta Saito||We study off-policy evaluation (OPE) in the problem of slate contextual bandits where a policy selects multi-dimensional actions known as slates. This problem is widespread in recommender systems, search engines, marketing, to medical applications, however, the typical Inverse Propensity Scoring (IPS) estimator suffers from substantial variance due to large action spaces, making effective OPE a significant challenge. The PseudoInverse (PI) estimator has been introduced to mitigate the variance issue by assuming linearity in the reward function, but this can result in significant bias as this assumption is hard-to-verify from observed data and is often substantially violated. To address the limitations of previous estimators, we develop a novel estimator for OPE of slate bandits, called Latent IPS (LIPS), which defines importance weights in a low-dimensional slate abstraction space where we optimize slate abstractions to minimize the bias and variance of LIPS in a data-driven way. By doing so, LIPS can substantially reduce the variance of IPS without imposing restrictive assumptions on the reward function structure like linearity. Through empirical evaluation, we demonstrate that LIPS substantially outperforms existing estimators, particularly in scenarios with non-linear rewards and large slate spaces.|我们研究了非政策评估(OPE)在板岩情境土匪问题中的应用，其中一个政策选择了多维行为即板岩。这个问题在推荐系统、搜索引擎、市场营销以及医疗应用中广泛存在，然而，典型的反倾向评分(IPS)估计因为大的行为空间而存在很大的差异，使得有效的 OPE 成为一个重大的挑战。已经引入了伪逆(PI)估计器来通过假设奖励函数的线性来缓解方差问题，但是这可能导致显着的偏差，因为这个假设很难从观测数据中验证，并且经常被严重违反。为了解决先前估计量的局限性，我们开发了一种新的板岩土匪 OPE 估计量，称为潜在 IPS (LIPS) ，它定义了低维板岩抽象空间中的重要权重，其中我们优化板岩抽象，以数据驱动的方式最小化 LIPS 的偏差和方差。通过这样做，LIPS 可以大大减少 IPS 的方差，而不必对奖励函数结构(如线性)施加限制性假设。通过实证评估，我们证明了 LIPS 在很大程度上优于现有的估计量，特别是在非线性回报和大板岩空间的情况下。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Off-Policy+Evaluation+of+Slate+Bandit+Policies+via+Optimizing+Abstraction)|0|
|[Long-term Off-Policy Evaluation and Learning](https://doi.org/10.1145/3589334.3645446)|Yuta Saito, Himan Abdollahpouri, Jesse Anderton, Ben Carterette, Mounia Lalmas||Short- and long-term outcomes of an algorithm often differ, with damaging downstream effects. A known example is a click-bait algorithm, which may increase short-term clicks but damage long-term user engagement. A possible solution to estimate the long-term outcome is to run an online experiment or A/B test for the potential algorithms, but it takes months or even longer to observe the long-term outcomes of interest, making the algorithm selection process unacceptably slow. This work thus studies the problem of feasibly yet accurately estimating the long-term outcome of an algorithm using only historical and short-term experiment data. Existing approaches to this problem either need a restrictive assumption about the short-term outcomes called surrogacy or cannot effectively use short-term outcomes, which is inefficient. Therefore, we propose a new framework called Long-term Off-Policy Evaluation (LOPE), which is based on reward function decomposition. LOPE works under a more relaxed assumption than surrogacy and effectively leverages short-term rewards to substantially reduce the variance. Synthetic experiments show that LOPE outperforms existing approaches particularly when surrogacy is severely violated and the long-term reward is noisy. In addition, real-world experiments on large-scale A/B test data collected on a music streaming platform show that LOPE can estimate the long-term outcome of actual algorithms more accurately than existing feasible methods.|算法的短期和长期结果往往不同，具有破坏性的下游影响。一个已知的例子是点击诱饵算法，它可能增加短期点击，但损害长期用户参与。估计长期结果的一个可能的解决方案是对潜在的算法进行在线实验或 A/B 测试，但是观察感兴趣的长期结果需要几个月甚至更长的时间，使得算法选择过程慢得令人无法接受。因此，这项工作研究的问题是可行的，但准确地估计长期结果的算法只使用历史和短期的实验数据。解决这个问题的现有方法要么需要对称为代孕的短期结果进行限制性假设，要么不能有效地利用短期结果，这是低效的。因此，我们提出了一个基于报酬函数分解的长期非政策评估(LOPE)框架。LOPE 工作在一个比代孕更宽松的假设下，并有效地利用短期回报来大大减少差异。综合实验表明，LOPE 优于现有的方法，特别是当代孕是严重违反和长期奖励是噪音。此外，在音乐流媒体平台上对大规模 A/B 测试数据进行的实际测试表明，LOPE 能够比现有的可行方法更准确地估计实际算法的长期结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Long-term+Off-Policy+Evaluation+and+Learning)|0|
|[Unified Uncertainty Estimation for Cognitive Diagnosis Models](https://doi.org/10.1145/3589334.3645488)|Fei Wang, Qi Liu, Enhong Chen, Chuanren Liu, Zhenya Huang, Jinze Wu, Shijin Wang||Cognitive diagnosis models have been widely used in different areas, especially intelligent education, to measure users' proficiency levels on knowledge concepts, based on which users can get personalized instructions. As the measurement is not always reliable due to the weak links of the models and data, the uncertainty of measurement also offers important information for decisions. However, the research on the uncertainty estimation lags behind that on advanced model structures for cognitive diagnosis. Existing approaches have limited efficiency and leave an academic blank for sophisticated models which have interaction function parameters (e.g., deep learning-based models). To address these problems, we propose a unified uncertainty estimation approach for a wide range of cognitive diagnosis models. Specifically, based on the idea of estimating the posterior distributions of cognitive diagnosis model parameters, we first provide a unified objective function for mini-batch based optimization that can be more efficiently applied to a wide range of models and large datasets. Then, we modify the reparameterization approach in order to adapt to parameters defined on different domains. Furthermore, we decompose the uncertainty of diagnostic parameters into data aspect and model aspect, which better explains the source of uncertainty. Extensive experiments demonstrate that our method is effective and can provide useful insights into the uncertainty of cognitive diagnosis.|认知诊断模型已广泛应用于各个领域，尤其是智能教育领域，用于测量用户对知识概念的熟练程度，以此为基础，用户可以获得个性化的指令。由于模型和数据的薄弱环节，测量结果并不总是可靠的，因此测量的不确定性也为决策提供了重要信息。然而，对于认知诊断的不确定性估计的研究远远落后于先进的认知诊断模型结构。现有的方法效率有限，对于具有交互函数参数的复杂模型(例如，基于深度学习的模型)留下了学术空白。为了解决这些问题，我们提出了一个统一的不确定性估计方法的认知诊断模型的广泛范围。具体地说，基于认知诊断模型参数后验分布估计的思想，我们首先提供了一个统一的基于小批量优化的目标函数，可以更有效地应用于范围广泛的模型和大数据集。然后，我们修改了重参数化方法，以适应不同领域定义的参数。将诊断参数的不确定性分解为数据方面和模型方面，较好地解释了不确定性的来源。大量的实验表明，我们的方法是有效的，可以提供有益的洞察认知诊断的不确定性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unified+Uncertainty+Estimation+for+Cognitive+Diagnosis+Models)|0|
|[A Cross Domain Method for Customer Lifetime Value Prediction in Supply Chain Platform](https://doi.org/10.1145/3589334.3645391)|Zhiyuan Zhou, Li Lin, Hai Wang, Xiaolei Zhou, Gong Wei, Shuai Wang||Accurate customer LifeTime Value (LTV) predictions are crucial for customer relationship management, especially in Supply Chain Platforms (SCP), which involve effectively managing the service resources in business decision-making. Previous LTV prediction methods usually rely on ample historical customer data, which is not available in the early stages of a customer's lifecycle. It makes the modeling of the historical customer data a difficult task due to the data sparsity. Besides, the long-tail distribution of customer LTV also brings new challenges to the prediction of LTV. To tackle the above issues, we propose CDLtvS, a novel Cross Domain method for customer Lifetime value prediction in SCP. It leverages rich cross-domain information from upstream platforms to enhance LTV predictions in downstream platforms. Firstly, CDLtvS pre-trains the customer representations by an LTV modeling framework named LtvS in source and target domains separately. Specifically, LtvS incorporates the Expert Mask Network (ExMN), which not only effectively models the long-tail distribution of LTV in single-domain but also resolves cross-domain learning model bias resulting from this distribution. Then, the various-level alignment mechanism is introduced to keep the consistency of knowledge transferring from source to target domains on both sparse and non-sparse data. Comprehensive experiments on real-world data from JD, one of the world's largest supply chain platforms, demonstrate that CDLtvS achieves a normalized mean average error of 0.3378 in LTV prediction, outperforming 16.3% to the baseline. Additionally, the improvements of ≥2.3% across various data sparsity levels (0% -- 80%) provide valuable insights into cross-domain LTV modeling.|准确的客户生命周期价值(LTV)预测对于客户关系管理至关重要，尤其是在供应链平台(SCP)中，它涉及到在商业决策中对服务资源的有效管理。以前的 LTV 预测方法通常依赖于大量的历史客户数据，而这些数据在客户生命周期的早期阶段是不可用的。由于数据的稀疏性，使得历史客户数据的建模成为一项艰巨的任务。此外，客户 LTV 的长尾分布也给 LTV 的预测带来了新的挑战。针对上述问题，本文提出了一种新的跨域 SCP 客户生命周期价值预测方法—— CDLtd. vS。它利用来自上游平台的丰富的跨域信息来增强下游平台的 LTV 预测。首先，在源域和目标域分别采用 LTV 建模框架 LTV 对客户表示进行预训练。具体来说，Ltd. vS 采用了专家面具网络(ExMN) ，不仅有效地模拟了 LTV 在单个领域的长尾分布，而且解决了由于这种分布而产生的跨领域学习模型偏差。然后，引入了不同层次的对齐机制，以保持稀疏和非稀疏数据上从源到目标领域的知识传递的一致性。对世界上最大的供应链平台之一 JD 的实际数据进行了全面的实验，结果表明 CDLtd 在 LTV 预测中达到了0.3378的标准化平均误差，比基线高出16.3% 。此外，在不同的数据稀疏级别(0% -80%)中，≥2.3% 的改进为跨域 LTV 建模提供了有价值的见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Cross+Domain+Method+for+Customer+Lifetime+Value+Prediction+in+Supply+Chain+Platform)|0|
|[UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting](https://doi.org/10.1145/3589334.3645434)|Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang, Bryan Hooi, Roger Zimmermann||Multivariate time series forecasting plays a pivotal role in contemporary web technologies. In contrast to conventional methods that involve creating dedicated models for specific time series application domains, this research advocates for a unified model paradigm that transcends domain boundaries. However, learning an effective cross-domain model presents the following challenges. First, various domains exhibit disparities in data characteristics, e.g., the number of variables, posing hurdles for existing models that impose inflexible constraints on these factors. Second, the model may encounter difficulties in distinguishing data from various domains, leading to suboptimal performance in our assessments. Third, the diverse convergence rates of time series domains can also result in compromised empirical performance. To address these issues, we propose UniTime for effective cross-domain time series learning. Concretely, UniTime can flexibly adapt to data with varying characteristics. It also uses domain instructions and a Language-TS Transformer to offer identification information and align two modalities. In addition, UniTime employs masking to alleviate domain convergence speed imbalance issues. Our extensive experiments demonstrate the effectiveness of UniTime in advancing state-of-the-art forecasting performance and zero-shot transferability.|多变量时间序列预测在当代网络技术中起着举足轻重的作用。与传统的方法，涉及创建专门的模型为特定的时间序列应用领域，这项研究提倡一个统一的模型范例，超越领域边界。然而，学习一个有效的跨领域模型提出了以下挑战。首先，不同的领域表现出数据特征的差异，例如，变量的数量，为对这些因素施加不灵活约束的现有模型设置了障碍。其次，该模型在区分不同领域的数据时可能会遇到困难，从而导致我们的评估性能不理想。第三，不同的收敛速度的时间序列领域也可能导致折衷的经验性能。为了解决这些问题，我们提出了有效的跨域时间序列学习 UniTime。具体来说，UniTime 可以灵活地适应不同特征的数据。它还使用领域指令和 Language-TS Transformer 来提供标识信息和对齐两种模式。另外，UniTime 采用屏蔽技术来缓解领域收敛速度不平衡的问题。我们的大量实验证明了 UniTime 在提高最先进的预测性能和零点可转移性方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UniTime:+A+Language-Empowered+Unified+Model+for+Cross-Domain+Time+Series+Forecasting)|0|
|[Semantic Evolvement Enhanced Graph Autoencoder for Rumor Detection](https://doi.org/10.1145/3589334.3645478)|Xiang Tao, Liang Wang, Qiang Liu, Shu Wu, Liang Wang||Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Recently, numerous rumor detection models which utilize textual information and the propagation structure of events have been proposed. However, these methods overlook the importance of semantic evolvement information of event in propagation process, which is often challenging to be truly learned in supervised training paradigms and traditional rumor detection methods. To address this issue, we propose a novel semantic evolvement enhanced Graph Autoencoder for Rumor Detection (GARD) model in this paper. The model learns semantic evolvement information of events by capturing local semantic changes and global semantic evolvement information through specific graph autoencoder and reconstruction strategies. By combining semantic evolvement information and propagation structure information, the model achieves a comprehensive understanding of event propagation and perform accurate and robust detection, while also detecting rumors earlier by capturing semantic evolvement information in the early stages. Moreover, in order to enhance the model's ability to learn the distinct patterns of rumors and non-rumors, we introduce a uniformity regularizer to further improve the model's performance. Experimental results on three public benchmark datasets confirm the superiority of our GARD method over the state-of-the-art approaches in both overall performance and early rumor detection.|由于谣言在社交媒体上的迅速传播，谣言检测已经成为一个极其重要的挑战。最近，许多利用文本信息和事件传播结构的谣言检测模型被提出。然而，这些方法忽视了事件语义演化信息在传播过程中的重要性，在有监督的训练范式和传统的谣言检测方法中往往难以真正学习到这些信息。针对这一问题，本文提出了一种新的语义演化增强型图形自动编码器用于谣言检测(GARD)模型。该模型通过特定的图形自动编码和重构策略获取事件的局部语义变化和全局语义演化信息，从而获取事件的语义演化信息。该模型将语义演化信息与传播结构信息相结合，实现了对事件传播的全面理解和准确鲁棒的检测，同时在早期阶段通过捕获语义演化信息提前检测谣言。此外，为了提高模型学习谣言和非谣言的不同模式的能力，我们引入了一个一致性正则化器来进一步提高模型的性能。在三个公共基准数据集上的实验结果证实了我们的 GARD 方法在总体性能和早期谣言检测方面都优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semantic+Evolvement+Enhanced+Graph+Autoencoder+for+Rumor+Detection)|0|
|[Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding](https://doi.org/10.1145/3589334.3645534)|Haoming Li, Yusen Huo, Shuai Dou, Zhenzhe Zheng, Zhilin Zhang, Chuan Yu, Jian Xu, Fan Wu||In online advertising, advertisers participate in ad auctions to acquire ad opportunities, often by utilizing auto-bidding tools provided by demand-side platforms (DSPs). The current auto-bidding algorithms typically employ reinforcement learning (RL). However, due to safety concerns, most RL-based auto-bidding policies are trained in simulation, leading to a performance degradation when deployed in online environments. To narrow this gap, we can deploy multiple auto-bidding agents in parallel to collect a large interaction dataset. Offline RL algorithms can then be utilized to train a new policy. The trained policy can subsequently be deployed for further data collection, resulting in an iterative training framework, which we refer to as iterative offline RL. In this work, we identify the performance bottleneck of this iterative offline RL framework, which originates from the ineffective exploration and exploitation caused by the inherent conservatism of offline RL algorithms. To overcome this bottleneck, we propose Trajectory-wise Exploration and Exploitation (TEE), which introduces a novel data collecting and data utilization method for iterative offline RL from a trajectory perspective. Furthermore, to ensure the safety of online exploration while preserving the dataset quality for TEE, we propose Safe Exploration by Adaptive Action Selection (SEAS). Both offline experiments and real-world experiments on Alibaba display advertising platform demonstrate the effectiveness of our proposed method.|在网络广告中，广告主通过参与广告拍卖来获取广告机会，通常是利用需求侧平台(DSP)提供的自动投标工具。目前的自动竞价算法通常采用强化学习(RL)。然而，出于安全考虑，大多数基于 RL 的自动投标策略都是在仿真中进行训练的，当部署在在线环境中时，会导致性能下降。为了缩小这一差距，我们可以并行部署多个自动招标代理来收集大型交互数据集。然后可以利用离线 RL 算法来训练新的策略。经过训练的策略随后可以部署用于进一步的数据收集，从而产生迭代训练框架，我们称之为迭代离线 RL。在这项工作中，我们确定了这个迭代离线 RL 框架的性能瓶颈，这源于无效的探索和开发造成的固有的保守性离线 RL 算法。为了克服这一瓶颈，本文提出了基于轨迹的探索与开发(TEE)方法，从轨迹的角度出发，提出了一种新的迭代离线 RL 的数据采集与利用方法。此外，为了保证在线勘探的安全性，同时保证 TEE 数据集的质量，我们提出了自适应行动选择(SEAS)的安全勘探方法。在阿里巴巴展示广告平台上的离线实验和现实世界的实验都证明了我们提出的方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Trajectory-wise+Iterative+Reinforcement+Learning+Framework+for+Auto-bidding)|0|
|[Stable-Sketch: A Versatile Sketch for Accurate, Fast, Web-Scale Data Stream Processing](https://doi.org/10.1145/3589334.3645581)|Weihe Li, Paul Patras||Data stream processing plays a pivotal role in various web-related applications, including click fraud detection, anomaly identification, and recommendation systems. Accurate and fast detection of items relevant to such tasks within data streams, e.g., heavy hitters, heavy changers, and persistent items, is however non-trivial. This is due to growing streaming speeds, limited fast memory (L1 cache) available in current systems, and highly skewed item distributions encountered in practice. In effect, items of interest that are tracked only based on their features (e.g., item frequency or persistence value) are susceptible to replacement by non-relevant ones, leading to modest detection accuracy, as we reveal. In this work, we introduce the notion of bucket stability, which quantifies the degree of recorded item variation, and show that this is a powerful metric for identifying distinct item types. We propose Stable-Sketch, an elegant and versatile sketch that exploits multidimensional information, including item statistics and bucket stability, and adopts a stochastic approach to drive replacement decisions. We present a theoretical analysis of the error bounds of Stable-Sketch, and conduct extensive experiments to demonstrate that our solution achieves substantially higher accuracy and faster processing speeds than state-of-the-art sketches in a range of item detection tasks, even with tight memories. We further enhance Stable-Sketch's update throughput with Single Instruction Multiple Data (SIMD) instructions and implement our solution with P4, demonstrating real world deployment viability.|数据流处理在各种 Web 相关应用程序中起着关键作用，包括点击欺诈检测、异常识别和推荐系统。然而，在数据流中准确、快速地检测与这些任务相关的项目(例如，重击者、重变更者和持久性项目)并非易事。这是由于流速不断提高，当前系统中可用的快速内存(L1缓存)有限，以及在实践中遇到的高度倾斜的项目分布。实际上，只根据其特征(例如，项目频率或持久性值)跟踪的感兴趣项目容易被不相关的项目替换，从而导致适度的检测准确性，正如我们所揭示的。在这项工作中，我们引入了桶稳定性的概念，它量化记录项目的变化程度，并表明这是一个识别不同项目类型的强大度量。我们提出了 Stable-Sketch，这是一个优雅而通用的草图，它利用多维信息，包括项目统计和桶的稳定性，并采用随机方法来驱动更换决策。我们提出了一个稳定草图误差界限的理论分析，并进行了广泛的实验，以证明我们的解决方案实现了更高的准确性和更快的处理速度比国家的最先进的草图在一系列的项目检测任务，即使紧张的记忆。我们进一步提高了 Stable-Sketch 的单指令流多数据流指令更新吞吐量，并用 p4实现了我们的解决方案，展示了真实世界部署的可行性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Stable-Sketch:+A+Versatile+Sketch+for+Accurate,+Fast,+Web-Scale+Data+Stream+Processing)|0|
|[Self-Paced Pairwise Representation Learning for Semi-Supervised Text Classification](https://doi.org/10.1145/3589334.3645664)|Junfan Chen, Richong Zhang, Jiarui Wang, Chunming Hu, Yongyi Mao||Text classification is one vital tool assisting web content mining. Semi-supervised text classification (SSTC) offers an approach to alleviate the burden of annotation costs by training on a few labeled texts alongside many unlabeled texts. Unsolved challenges in SSTC are the overfitting problem caused by the limited labeled data and the mislabeling problem of unlabeled texts. To address these issues, this paper proposes a Self-Paced PairWise representation learning (SPPW) model. Concretely, SPPW alleviates the overfitting problem by replacing the overfitting-prone learning of a parameterized classifier with representation learning in a pair-wise manner. Besides, we propose a novel self-paced text filtering method that effectively integrates both label confidence and text hardness to reduce mislabeled texts synergistically. Extensive experiments on 3 benchmark SSTC datasets show that SPPW outperforms baselines and is effective in mitigating overfitting and mislabeling problems.|文本分类是辅助 Web 内容挖掘的重要工具。半监督文本分类(SSTC)提供了一种通过对少量标记文本和大量未标记文本进行训练来减轻注释成本负担的方法。SSTC 中尚未解决的问题是有限的标记数据引起的过拟合问题和未标记文本的标记错误问题。为了解决这些问题，本文提出了一种自适应成对表示学习(SPPW)模型。具体来说，SPPW 通过将参数化分类器的过拟合倾向学习替换为表示学习，从而缓解了过拟合问题。此外，我们提出了一种新的自定速文本过滤方法，有效地结合标签置信度和文本硬度，以减少错误标签文本协同。通过对3个基准 SSTC 数据集的大量实验表明，SPPW 算法性能优于基准算法，能够有效地解决过拟合和错标问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Paced+Pairwise+Representation+Learning+for+Semi-Supervised+Text+Classification)|0|
|[Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering](https://doi.org/10.1145/3589334.3645670)|Hongda Sun, Yuxuan Liu, Chengwei Wu, Haiyu Yan, Cheng Tai, Xin Gao, Shuo Shang, Rui Yan||Open-domain question answering (ODQA) has emerged as a pivotal research spotlight in information systems. Existing methods follow two main paradigms to collect evidence: (1) The retrieve-then-read paradigm retrieves pertinent documents from an external corpus; and (2) the generate-then-read paradigm employs large language models (LLMs) to generate relevant documents. However, neither can fully address multifaceted requirements for evidence. To this end, we propose LLMQA, a generalized framework that formulates the ODQA process into three basic steps: query expansion, document selection, and answer generation, combining the superiority of both retrieval-based and generation-based evidence. Since LLMs exhibit their excellent capabilities to accomplish various tasks, we instruct LLMs to play multiple roles as generators, rerankers, and evaluators within our framework, integrating them to collaborate in the ODQA process. Furthermore, we introduce a novel prompt optimization algorithm to refine role-playing prompts and steer LLMs to produce higher-quality evidence and answers. Extensive experimental results on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate that LLMQA achieves the best performance in terms of both answer accuracy and evidence quality, showcasing its potential for advancing ODQA research and applications.|开放域问答技术(ODQA)已经成为信息系统领域的一个重要研究热点。现有方法遵循两种主要的证据收集范式: (1)检索-然后阅读范式从外部语料库检索相关文档; (2)生成-然后阅读范式使用大语言模型(LLM)生成相关文档。然而，两者都不能完全满足证据的多方面需求。为此，结合基于检索和基于生成的证据的优势，我们提出了一个通用框架 LLMQA，它将 ODQA 过程描述为三个基本步骤: 查询扩展、文档选择和答案生成。由于 LLM 展示了它们完成各种任务的出色能力，我们指示 LLM 在我们的框架中扮演生成器、重新排序器和评估器的多重角色，将它们集成到 ODQA 过程中进行协作。此外，我们还引入了一种新的提示优化算法来细化角色扮演提示，并引导 LLM 产生更高质量的证据和答案。在广泛使用的基准测试(NQ，WebQ 和 TriviaQA)上的广泛实验结果表明，LLMQA 在答案准确性和证据质量方面取得了最佳性能，展示了其推进 ODQA 研究和应用的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Harnessing+Multi-Role+Capabilities+of+Large+Language+Models+for+Open-Domain+Question+Answering)|0|
|[POLISH: Adaptive Online Cross-Modal Hashing for Class Incremental Data](https://doi.org/10.1145/3589334.3645716)|YuWei Zhan, Xin Luo, ZhenDuo Chen, Yongxin Wang, Yinwei Wei, XinShun Xu||In recent years, hashing-based online cross-modal retrieval has garnered growing attention. This trend is motivated by the fact that web data is increasingly delivered in a streaming manner as opposed to batch processing. Simultaneously, the sheer scale of web data sometimes makes it impractical to fully load for the training of hashing models. Despite the evolution of online cross-modal hashing techniques, several challenges remain: 1) Most existing methods learn hash codes by considering the relevance among newly arriving data or between new data and the existing data, often disregarding valuable global semantic information. 2) A common but limiting assumption in many methods is that the label space remains constant, implying that all class labels should be provided within the first data chunk. This assumption does not hold in real-world scenarios, and the presence of new labels in incoming data chunks can severely degrade or even break these methods. To tackle these issues, we introduce a novel supervised online cross-modal hashing method named adaPtive Online cLass-Incremental haSHing (POLISH). Leveraging insights from language models, POLISH generates representations for new class label from multiple angles. Meanwhile, POLISH treats label embeddings, which remain unchanged once learned, as stable global information to produce high-quality hash codes. POLISH also puts forward an efficient optimization algorithm for hash code learning. Extensive experiments on two real-world benchmark datasets show the effectiveness of the proposed POLISH for class incremental data in the cross-modal hashing domain.|近年来，基于散列的在线跨模态检索越来越受到人们的关注。这一趋势的动因是网络数据越来越多地以流的方式交付，而不是批处理。同时，Web 数据的庞大规模有时使得完全加载哈希模型的训练变得不切实际。尽管在线跨模态哈希技术不断发展，但仍然存在一些挑战: 1)大多数现有方法通过考虑新到数据之间或新数据与现有数据之间的相关性来学习哈希代码，往往忽视有价值的全球语义信息。2)许多方法中一个常见但有限的假设是标签空间保持不变，这意味着所有类标签都应该在第一个数据块中提供。这个假设在真实的场景中是不成立的，在传入的数据块中出现新的标签会严重降低甚至破坏这些方法。为了解决这些问题，我们提出了一种新的有监督的在线跨模态散列方法——自适应在线类增量散列(POLISH)。利用语言模型的洞察力，POLISH 从多个角度生成新类标签的表示。同时，POLISH 将标签嵌入视为稳定的全局信息，从而生成高质量的哈希码。POLISH 还提出了一种有效的哈希码学习优化算法。在两个真实世界的基准数据集上的大量实验表明了该方法对于跨模态散列域中类增量数据的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=POLISH:+Adaptive+Online+Cross-Modal+Hashing+for+Class+Incremental+Data)|0|
|[How Contentious Terms About People and Cultures are Used in Linked Open Data](https://doi.org/10.1145/3589334.3648140)|Andrei Nesterov, Laura Hollink, Jacco van Ossenbruggen||Web resources in linked open data (LOD) are comprehensible to humans through literal textual values attached to them, such as labels, notes, or comments. Word choices in literals may not always be neutral. When outdated and culturally stereotyping terminology is used in literals, they may appear as offensive to users in interfaces and propagate stereotypes to algorithms trained on them. We study how frequently and in which literals contentious terms about people and cultures occur in LOD and whether there are attempts to mark the usage of such terms. For our analysis, we reuse English and Dutch terms from a knowledge graph that provides opinions of experts from the cultural heritage domain about terms' contentiousness. We inspect occurrences of these terms in four widely used datasets: Wikidata, The Getty Art & Architecture Thesaurus, Princeton WordNet, and Open Dutch WordNet. Some terms are ambiguous and contentious only in particular senses. Applying word sense disambiguation, we generate a set of literals relevant to our analysis. We found that outdated, derogatory, stereotyping terms frequently appear in descriptive and labelling literals, such as preferred labels that are usually displayed in interfaces and used for indexing. In some cases, LOD contributors mark contentious terms with words and phrases in literals (implicit markers) or properties linked to resources (explicit markers). However, such marking is rare and non-consistent in all datasets. Our quantitative and qualitative insights could be helpful in developing more systematic approaches to address the propagation of stereotypes via LOD.|链接开放数据(LOD)中的 Web 资源可以通过附加到它们的文本值(如标签、注释或注释)让人们理解。字面意义上的单词选择可能并不总是中性的。当过时的和文化上的陈规定型术语用于文字时，它们可能会在界面中显得冒犯用户，并将陈规定型传播给接受过相关培训的算法。我们研究 LOD 中关于人和文化的有争议的字面术语的出现频率和出现频率，以及是否有人试图标记这些术语的用法。对于我们的分析，我们从一个知识图中重用英语和荷兰语术语，该知识图提供了来自文化遗产领域的专家关于术语争议性的意见。我们在四个广泛使用的数据集中检查这些术语的出现情况: Wikidata、 The Getty Art & Architecture Thesaurus、 Princeton WordNet 和 Open Dutch WordNet。有些术语只在特定的意义上是模棱两可和有争议的。应用词义消歧，我们生成了一组与我们的分析相关的文字。我们发现，过时的、贬义的、刻板的术语经常出现在描述和标签文字中，例如首选标签通常显示在接口中并用于索引。在某些情况下，LOD 参与者使用文字(隐式标记)或与资源(显式标记)相关的属性中的单词和短语来标记有争议的术语。然而，这样的标记在所有数据集中是罕见和不一致的。我们在定量和定性方面的见解可能有助于发展更系统的方法来解决通过检测限传播刻板印象的问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Contentious+Terms+About+People+and+Cultures+are+Used+in+Linked+Open+Data)|0|
|[MMAdapt: A Knowledge-guided Multi-source Multi-class Domain Adaptive Framework for Early Health Misinformation Detection](https://doi.org/10.1145/3589334.3648152)|Lanyu Shang, Yang Zhang, Bozhang Chen, Ruohan Zong, Zhenrui Yue, Huimin Zeng, Na Wei, Dong Wang||This paper studies a critical problem of emergent health misinformation detection, aiming to mitigate the spread of misinformation in emergent health domains to support well-informed healthcare decisions towards a Web for good health. Our work is motivated by the lack of timely resources (e.g., medical knowledge, annotated data) during the initial phases of an emergent health event or topic. In this paper, we develop a multi-source domain adaptive framework that jointly exploits medical knowledge and annotated data from different high-resource source domains (e.g., cancer, COVID-19) to detect misleading posts in an emergent target domain (e.g., mpox, polio). Two important challenges exist in developing our solution: 1) how to accurately detect the partially misleading and unverifiable content in an emergent target domain? 2) How to identify the conflicting knowledge facts from different source domains to accurately detect emergent misinformation in the target domain? To address these challenges, we develop MMAdapt, a multi-source multi-class domain adaptive misinformation detection framework that effectively explores diverse knowledge facts from different source domains to accurately detect not only the outright misleading but also the partially misleading or unverifiable posts on the Web. Extensive experimental results on four real-world misinformation datasets demonstrate that MMAdapt substantially outperforms state-of-the-art baselines in accurately detecting misinformation in an emergent health domain.|本文研究了突发性健康错误信息检测的一个关键问题，旨在减少突发性健康领域中错误信息的传播，以支持健康网络中知情的医疗决策。我们的工作是由于在紧急健康事件或主题的初始阶段缺乏及时的资源(例如，医学知识，注释数据)而产生的。在本文中，我们开发了一个多源域自适应框架，该框架共同利用医学知识和来自不同高资源源域(如癌症、2019冠状病毒疾病)的注释数据，以检测紧急目标域(如天花、脊髓灰质炎)中的误导性信息。在开发我们的解决方案时存在两个重要的挑战: 1)如何准确地检测紧急目标领域中部分误导和无法验证的内容？2)如何识别来自不同源域的相互冲突的知识事实，准确检测目标域中出现的错误信息？为了应对这些挑战，我们开发了 MMAdapt，这是一个多源多类域自适应错误信息检测框架，它有效地探索来自不同源域的多种知识事实，以准确地检测出不仅是彻底的误导，而且是部分误导或无法验证的网络帖子。对四个真实世界错误信息数据集的大量实验结果表明，MMAdapt 在准确检测紧急健康领域中的错误信息方面大大优于最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MMAdapt:+A+Knowledge-guided+Multi-source+Multi-class+Domain+Adaptive+Framework+for+Early+Health+Misinformation+Detection)|0|
|[LightCS: Selecting Quadratic Feature Crosses in Linear Complexity](https://doi.org/10.1145/3589335.3648300)|Zhaocheng Du, Junhao Chen, Qinglin Jia, Chuhan Wu, Jieming Zhu, Zhenhua Dong, Ruiming Tang||Feature crosses, which represent joint features synthesized by two single features, are critical for deep recommender systems to model sophisticated feature relations. In practice, only a tiny fraction of feature crosses among massive possible ones are informative, while introducing irrelevant or noisy ones may increase online service latency and boost the risk of overfitting. Therefore, picking high-quality feature crosses is essential in practical recommender systems. However, even for selecting quadratic feature crosses, existing algorithms still incur either o(n2) time complexity or o(n2) space complexity, which is inefficient and unscalable in industrial scenarios. In this paper, we present an efficient and accurate quadratic feature cross selection method with both linear time and space complexity. Motivated by the idea of Quasi-Newton methods, we propose to use 2nd-order derivative matrix to evaluate all theoretically possible feature crosses concurrently without the need of constructing them explicitly, where an approximation of 2nd-order gradient is applied to guarantee both low time and space complexity. Furthermore, we decouple the feature crosses' novelty from single features' joint importance. Experiments on two public recommendation datasets and a private dataset validate the efficiency and effectiveness of our method, and it has also become a fundamental feature cross selection tool used by Huawei Ads Platform.|特征交叉表示由两个单一特征合成的联合特征，是深度推荐系统建立复杂特征关系的关键。在实践中，只有很小一部分功能交叉在大量可能的功能中是信息性的，而引入不相关或噪音的功能可能会增加在线服务的延迟并增加过度适应的风险。因此，在实际的推荐系统中，选择高质量的特征杂交是必不可少的。然而，即使在选择二次特征交叉时，现有的算法仍然会产生 o (n2)时间复杂度或 o (n2)空间复杂度，这在工业场景中是低效的和不可扩展的。本文提出了一种高效、准确的二次特征交叉选择方法，该方法具有线性时间和空间复杂度。受 Quasi-Newton 方法思想的启发，我们建议使用二阶导数矩阵来同时评估所有理论上可能的特征交叉，而不需要明确地构造它们，其中采用二阶梯度近似来保证低时间和空间复杂度。此外，我们还将特征交叉的新颖性与单个特征的联合重要性分离开来。两个公共推荐数据集和一个私人数据集的实验验证了我们的方法的效率和有效性，它也成为华为广告平台使用的一个基本功能交叉选择工具。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LightCS:+Selecting+Quadratic+Feature+Crosses+in+Linear+Complexity)|0|
|[SOIL: Score Conditioned Diffusion Model for Imbalanced Cloud Failure Prediction](https://doi.org/10.1145/3589335.3648303)|Chiming Duan, Fangkai Yang, Pu Zhao, Lingling Zheng, Yash Dagli, Yudong Liu, Qingwei Lin, Dongmei Zhang||Cloud failure prediction (e.g., disk failure prediction, memory failure prediction, node failure prediction, etc.) is a crucial task for ensuring the reliability and performance of cloud systems.However, the problem of class imbalance poses a huge challenge for accurate prediction as the number of healthy components (majority class) in a cloud system is much larger than the number of failed components (minority class). The consequences of this class imbalance include biased model performance and insufficient learning, as the model may lack adequate information to learn the characteristics associated with cloud failure effectively. Moreover, current methods for addressing the class imbalance problem, such as SMOTE and its variants, exhibit certain drawbacks, such as generating noisy samples and struggling to maintain sample diversity, which limit their effectiveness in addressing the challenges presented by the class imbalance in cloud failure prediction. In this paper, we propose a novel oversampling method for imbalanced classification, named SOIL (Score cOnditioned dIffusion modeL), which employs a score-conditioned diffusion model to generate high-quality synthetic samples for the minority class, more accurately representing real-world cloud failure patterns. By incorporating classification probabilities as conditional scores, SOIL offers supervision to the generation process, effectively limiting noise production while maintaining sample diversity. Through extensive experiments on various public and industrial datasets, upon adopting our method, the cloud failure prediction model's F1-score is improved by an average of 5.39% and consistently outperforms state-of-the-art competitors in addressing the class imbalance problem, which confirm the effectiveness and robustness of SOIL. In addition, SOIL has been successfully applied to a global large-scale cloud platform serving billions of customers, demonstrating its practicability.|云故障预测(如磁盘故障预测、内存故障预测、节点故障预测等)是确保云系统可靠性和性能的关键任务。然而，类不平衡的问题给准确预测带来了巨大的挑战，因为云系统中健康组件(多数类)的数量远远大于失败组件(少数类)的数量。这种类不平衡的后果包括模型性能偏差和学习不足，因为模型可能缺乏足够的信息来有效地学习与云故障相关的特征。此外，目前解决类不平衡问题的方法，如 SMOTE 及其变体，表现出一定的缺陷，如产生噪声样本和难以维持样本多样性，这限制了它们在解决云失效预测中类不平衡所带来的挑战的有效性。本文提出了一种新的不平衡分类过采样方法 SOIL (Score condiated dIffusion modeL) ，该方法采用分数条件扩散模型为少数族群生成高质量的合成样本，更准确地反映真实世界的云失效模式。通过将分类概率作为条件得分，SOIL 提供了对生成过程的监督，有效地限制了噪声的产生，同时保持了样本的多样性。通过对各种公共和工业数据集的大量实验，采用本文的方法，云失效预测模型的 F1得分平均提高了5.39% ，在解决类别不平衡问题方面始终优于最先进的竞争对手，证实了 SOIL 的有效性和鲁棒性。此外，SOIL 已成功应用于为数十亿客户服务的全球大规模云平台，证明了 SOIL 的实用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SOIL:+Score+Conditioned+Diffusion+Model+for+Imbalanced+Cloud+Failure+Prediction)|0|
|[Dependency Aware Incident Linking in Large Cloud Systems](https://doi.org/10.1145/3589335.3648311)|Supriyo Ghosh, Karish Grover, Jimmy Wong, Chetan Bansal, Rakesh Namineni, Mohit Verma, Saravan Rajmohan||Despite significant reliability efforts, large-scale cloud services inevitably experience production incidents that can significantly impact service availability and customer's satisfaction. Worse, in many cases one incident can lead to multiple downstream failures due to cascading effects that creates several related incidents across different dependent services. Often time On-call Engineers (OCEs) examine these incidents in silos that lead to significant amount of manual toil and increase the overall time-to-mitigate incidents. Therefore, developing efficient incident linking models is of paramount importance for grouping related incidents into clusters so as to quickly resolve major outages and reduce on-call fatigue. Existing incident linking methods mostly leverages textual and contextual information of incidents (e.g., title, description, severity, impacted components), thus failing to leverage the inter-dependencies between services. In this paper, we propose the dependency-aware incident linking (DiLink) framework which leverages both textual and service dependency graph information to improve the accuracy and coverage of incident links not only coming from same service, but also from different services and workloads. Furthermore, we propose a novel method to align the embeddings of multi-modal (i.e., textual and graphical) data using Orthogonal Procrustes. Extensive experimental results on real-world incidents from 5 workloads of Microsoft demonstrate that our alignment method has an F1-score of 0.96 (14 are also in the process of deploying this solution across 610 services from these 5 workloads for continuously supporting OCEs improving incident management and reducing manual toil.|尽管在可靠性方面做出了重大努力，但大规模云服务不可避免地会遇到生产事件，这些事件会显著影响服务的可用性和客户的满意度。更糟糕的是，在许多情况下，由于级联效应，一个事件可能导致多个下游故障，从而在不同的依赖服务之间产生多个相关事件。通常情况下，待命工程师(OCEs)在筒仓中检查这些事故，这些事故导致大量的人工劳动，并增加了缓解事故的整体时间。因此，开发有效的事故连接模型对于将相关事故分组以快速解决重大故障并减少待命疲劳至关重要。现有的事件链接方法主要利用事件的文本和上下文信息(例如，标题、描述、严重性、受影响的组件) ，因此无法利用服务之间的相互依赖性。在本文中，我们提出了依赖感知事件链接(DiLink)框架，它利用文本和服务依赖图信息来提高事件链接的准确性和覆盖率，这些事件链接不仅来自相同的服务，而且来自不同的服务和工作负载。此外，我们提出了一种新的方法来对齐多模态(即，文本和图形)数据的嵌入使用正交 Procrustes。来自微软5个工作负载的关于真实世界事件的广泛实验结果表明，我们的对齐方法的 F1得分为0.96(14个也正在从这5个工作负载部署这个解决方案到610个服务，以持续支持 OCEs 改进事件管理和减少手工劳动。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dependency+Aware+Incident+Linking+in+Large+Cloud+Systems)|0|
|[A Graph-based Framework for Reducing False Positives in Authentication Alerts in Security Systems](https://doi.org/10.1145/3589335.3648325)|Yanbang Wang, Karl Hallgren, Jonathan Larson||The high false positive (FP) rate of authentication alerts remains to be a prominent challenge in cybersecurity nowadays. We identify two problems that cause this issue, which are unaddressed in existing learning-based anomaly detection methods. First, in industrial applications, ground-truth labels for malicious authentication events are extremely scarce. Therefore, learning-based methods must optimize their procedures for auto-generating high-quality training instances, an aspect that existing works have overlooked. Second, every existing model is based on a single form of data representation, either stream or graph snapshot, which may not be expressive enough to identify heterogeneity in behaviors of networked entities. This results in misclassifying a legitimate but differently-behaved authentication event into an anomalous one. We address these problems by proposing a new framework based on self-supervised link prediction on dynamic authentication networks, with two highlighted features: (1) our framework is based on the unification of two most popular views of dynamic interconnected systems: graph snapshots and link stream, ensuring the best coverage of behavioral heterogeneity; (2) to generate high-quality training samples, we propose a carefully designed negative sampling procedure called filtered rewiring, to ensure that the negative samples used for training are both truly negative and instructive. We validate our framework on 4 months of authentication data of 125 randomly selected, real organizations that subscribe to Microsoft's defense services.|高误报率的认证警报仍然是当今网络安全面临的一个突出挑战。我们发现了导致这个问题的两个问题，这两个问题在现有的基于学习的异常检测方法中没有得到解决。首先，在工业应用程序中，恶意身份验证事件的地面真相标签极其缺乏。因此，基于学习的方法必须优化自动生成高质量培训实例的过程，这是现有工作忽视的一个方面。其次，每个现有的模型都基于一种单一的数据表示形式，无论是流还是图快照，这种表示形式可能不足以识别网络实体行为的异构性。这会导致将合法但行为不同的身份验证事件错误分类为异常事件。针对这些问题，本文提出了一种基于动态认证网络自监督链路预测的新框架，该框架具有两个突出的特点: (1)该框架基于动态互联系统两种最流行的观点的统一: 图形快照和链路流，确保行为异质性的最佳覆盖; (2)为了生成高质量的训练样本，我们提出了一种精心设计的负采样程序，称为过滤重连接，以确保用于训练的负采样都是真正的负的和有益的。我们验证了我们的框架在4个月的认证数据的125个随机选择，真正的组织，订阅微软的防御服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Graph-based+Framework+for+Reducing+False+Positives+in+Authentication+Alerts+in+Security+Systems)|0|
|[FinReport: Explainable Stock Earnings Forecasting via News Factor Analyzing Model](https://doi.org/10.1145/3589335.3648330)|Xiangyu Li, Xinjie Shen, Yawen Zeng, Xiaofen Xing, Jin Xu||The task of stock earnings forecasting has received considerable attention due to the demand investors in real-world scenarios. However, compared with financial institutions, it is not easy for ordinary investors to mine factors and analyze news. On the other hand, although large language models in the financial field can serve users in the form of dialogue robots, it still requires users to have financial knowledge to ask reasonable questions. To serve the user experience, we aim to build an automatic system, FinReport, for ordinary investors to collect information, analyze it, and generate reports after summarizing. Specifically, our FinReport is based on financial news announcements and a multi-factor model to ensure the professionalism of the report. The FinReport consists of three modules: news factorization module, return forecasting module, risk assessment module. The news factorization module involves understanding news information and combining it with stock factors, the return forecasting module aim to analysis the impact of news on market sentiment, and the risk assessment module is adopted to control investment risk. Extensive experiments on real-world datasets have well verified the effectiveness and explainability of our proposed FinReport. Our codes and datasets are available at https://github.com/frinkleko/FinReport.|由于在现实世界中需求投资者的存在，股票收益预测的任务受到了相当大的关注。然而，与金融机构相比，普通投资者要挖掘因素、分析新闻并不容易。另一方面，尽管金融领域的大型语言模型可以以对话机器人的形式为用户提供服务，但它仍然需要用户具备金融知识才能提出合理的问题。为了服务于用户体验，我们的目标是建立一个自动化的系统，FinReport，为普通投资者收集信息，分析它，并在总结后生成报告。具体来说，我们的 FinReport 以财经新闻公告和多因素模型为基础，以确保报告的专业性。FinReport 由三个模块组成: 新闻分解模块、收益预测模块、风险评估模块。新闻因子分解模块包括了解新闻信息并将其与股票因子相结合，收益预测模块旨在分析新闻对市场情绪的影响，风险评估模块用于控制投资风险。在真实世界数据集上的大量实验已经很好地验证了我们提出的 FinReport 的有效性和可解释性。我们的代码和数据集 https://github.com/frinkleko/finreport 可查。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FinReport:+Explainable+Stock+Earnings+Forecasting+via+News+Factor+Analyzing+Model)|0|
|[DISKCO : Disentangling Knowledge from Cross-Encoder to Bi-Encoder](https://doi.org/10.1145/3589335.3648333)|Ankith M. S, Arindam Bhattacharya, Ankit Gandhi, Vijay Huddar, Atul Saroop, Rahul Bhagat||In the field of Natural Language Processing (NLP), sentence pair classification is important in various real-world applications. Bi-encoders are commonly used to address these problems due to their low-latency requirements, and their ability to act as effective retrievers. However, bi-encoders often under-perform compared to cross-encoders by a significant margin. To address this gap, many Knowledge Distillation (KD) techniques have been proposed. Most existing KD methods focus solely on utilizing the prediction scores of cross-encoder models and overlook the fact that cross-encoders and bi-encoders have fundamentally different input structures. In this work, we introduce a novel knowledge distillation approach called DISKCO, which DISentangles the Knowledge learned in Cross-encoder models especially from multi-head cross-attention models and transfers it to bi-encoder models. DISKCO leverages the information encoded in the cross-attention weights of the trained cross-encoder model, and provide it as contextual cues for the student bi-encoder model during training and inference. DISKCO combines the benefits of independent encoding for low-latency applications with the knowledge acquired from cross-encoders, resulting in improved performance. Empirically, we demonstrate the effectiveness of DISKCO on proprietary and on various publicly available datasets. Our experiments show that DISKCO outperforms traditional knowledge distillation methods by upto 2%.|在自然语言处理(NLP)领域，句子对分类在各种实际应用中非常重要。双编码器通常用于解决这些问题，因为它们的低延迟需求，以及它们作为有效检索器的能力。然而，与交叉编码器相比，双编码器往往表现不佳，差距很大。为了解决这一问题，人们提出了许多知识提取(KD)技术。大多数现有的 KD 方法只关注利用交叉编码器模型的预测分数，而忽视了交叉编码器和双编码器具有根本不同的输入结构这一事实。本文介绍了一种新的知识提取方法 DISKCO，该方法将交叉编码器模型特别是多头交叉注意模型中学到的知识进行分离，并将其转化为双编码器模型。DISKCO 利用训练后的交叉编码器模型的交叉注意权重中编码的信息，并在训练和推理过程中为学生的双编码器模型提供上下文线索。DISKCO 将低延迟应用程序独立编码的好处与从交叉编码器获得的知识结合起来，从而提高了性能。经验上，我们证明了 DISKCO 在专有数据集和各种公开数据集上的有效性。我们的实验表明 DISKCO 比传统的知识提取方法提高了2% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DISKCO+:+Disentangling+Knowledge+from+Cross-Encoder+to+Bi-Encoder)|0|
|[Information Diffusion Meets Invitation Mechanism](https://doi.org/10.1145/3589335.3648337)|Shiqi Zhang, Jiachen Sun, Wenqing Lin, Xiaokui Xiao, Yiqian Huang, Bo Tang||The dissemination of information is a complex process that plays a crucial role in real-world applications, especially when intertwined with friend invitations and their ensuing responses. Traditional diffusion models, however, often do not adequately capture this invitation-aware diffusion (IAD), rendering inferior results. These models typically focus on describing the social influence process, i.e., how a user is informed by friends, but tend to overlook the subsequent behavioral changes that invitations might precipitate. To this end, we present the Independent Cascade with Invitation (ICI) model, which incorporates both the social influence process and multi-stage behavior conversions in IAD. We validate our design through an empirical study on in-game IAD. Furthermore, we conduct extensive experiments to evaluate the effectiveness of our proposal against 6 state-of-the-art models on 6 real-world datasets. In particular, we demonstrate that our solution can outperform the best competitor by up to 5× in cascade estimation and 17.2% in diffusion prediction. We deploy our proposal in the seed selection and friend ranking scenarios of Tencent's online games, where it achieves improvements of up to 170% and 20.3%, respectively.|信息的传播是一个复杂的过程，在现实世界的应用程序中起着至关重要的作用，特别是当与好友邀请及其随后的反应交织在一起时。然而，传统的扩散模型往往没有充分捕捉到这种邀请感知扩散(IAD) ，造成了较差的结果。这些模型通常侧重于描述社会影响过程，也就是说，用户是如何被朋友告知的，但往往忽略了邀请可能引发的后续行为变化。为此，我们提出了独立级联邀请(ICI)模型，该模型同时考虑了网络成瘾的社会影响过程和多阶段行为转化。通过对游戏中网络成瘾的实证研究，验证了我们的设计。此外，我们进行了广泛的实验，以评估我们的建议对6个现实世界数据集上的6个最先进的模型的有效性。特别地，我们证明了我们的解决方案在级联估计和扩散预测中可以优于最好的竞争对手达到5倍和17.2% 。我们将我们的建议应用于腾讯网络游戏的种子选择和好友排名场景中，分别实现了170% 和20.3% 的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Information+Diffusion+Meets+Invitation+Mechanism)|0|
|[The MMO Economist: AI Empowers Robust, Healthy, and Sustainable P2W MMO Economies](https://doi.org/10.1145/3589335.3648344)|Shiwei Zhao, Xi Yuan, Runze Wu, Zhipeng Hu, Haoyu Liu, Kai Wang, Yujing Hu, Tangjie Lv, Changjie Fan, Xin Tong, Jiangze Han, Yan Zheng, Jianye Hao||Massively Multiplayer Online Games (MMOs) feature intricate virtual economies that permeate various in-game activities. However, the balancing act between profitability and equality in MMO economic design proves to be a persistent conundrum, especially in nascent business models like Pay-to-Win (P2W). Conventional efforts are curtailed by two primary constraints: the inability to verify and the provision of suboptimal solutions. In light of these predicaments, this paper delves into MMO economies and explores the promising potential of integrating emerging AI methodologies into economic design. Specifically, we introduce a novel hierarchical Reinforcement Learning (RL) solution for achieving Pareto optimality between profitability and equality in P2W economies. Leveraging our substantial industrial acumen and expertise, we establish an economic simulation environment that facilitates authentic and realistic assessments of MMO economic evolution. Building upon this foundation, we reconceptualize the P2W economic design process within the paradigm of a Markov Decision Process (MDP) and tackle it as a standard RL problem. Comprehensive evaluations corroborate that our solution demonstrates consistent personality specialization in economic simulations akin to real-world MMOs and significantly outperforms other baselines in economic design. Further discussions highlight its superiority in both frontier research and practical applications within the game industry.|大型多人在线游戏(MMO)的特点是错综复杂的虚拟经济，渗透到游戏中的各种活动。然而，在 MMO 经济设计中，盈利性和公平性之间的平衡行为被证明是一个持久的难题，尤其是在新生的商业模式，如支付到赢(P2W)。传统的努力受到两个主要限制: 无法核实和提供次优解决方案。鉴于这些困境，本文深入研究了 MMO 经济，并探讨了将新兴 AI 方法融入经济设计的潜力。具体来说，我们引入了一种新的等级强化学习(RL)解决方案，以实现在 P2W 经济体中盈利能力和公平性之间的帕累托最优。利用我们丰富的产业智慧和专业知识，我们建立一个经济模拟环境，促进真实和现实的评估 MMO 经济演变。在此基础上，我们将 P2W 经济设计过程重新概念化为一个马可夫决策过程(mDP)范式，并将其作为一个标准的 RL 问题来处理。综合评估证实，我们的解决方案在类似于现实世界 MMO 的经济模拟中显示出一致的个性专业化，并且在经济设计中显著优于其他基线。进一步的讨论突出了它在前沿研究和游戏产业的实际应用方面的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+MMO+Economist:+AI+Empowers+Robust,+Healthy,+and+Sustainable+P2W+MMO+Economies)|0|
|[Skewness-aware Boosting Regression Trees for Customer Contribution Prediction in Financial Precision Marketing](https://doi.org/10.1145/3589335.3648346)|HsinYu Chen, ChengTe Li, TingYu Chen||In an era of digital evolution, banking sectors face the dual challenge of nurturing a digitally savvy demographic and managing potential dormant account holders. This study delves deep into the prediction of customer contributions, particularly considering the skewed nature of such data. The inherent skewness in customer contribution data, highlighted by the substantial low-value contribution group and the vast variability among high-value contributors, necessitates an advanced prediction model. Addressing this, we present the Skewness-aware Boosting Regression Trees (SBRT) framework to predict customer contributions whose distribution exhibit high skewness. SBRT seamlessly combines the strength of Gradient Boosted Decision Trees with a novel mechanism of random tree deactivation, adeptly tackling distribution skewness. The model's effectiveness is rooted in four principles: cross-feature extraction, a percentile-based calibration and rebalancing method, tree deactivation during the boosting phase, and the utilization of Huber loss. Extensive real-world bank data testing underscores SBRT's promising capability in managing skewed distributions, setting it a cut above in predicting customer contributions. The culmination of this work lies in its practical validation, where online A/B tests highlight SBRT's tangible industrial applicability.|在一个数字化发展的时代，银行业面临着双重挑战: 一方面要培养精通数字技术的人才，另一方面要管理潜在的休眠账户持有人。这项研究深入探讨了客户贡献的预测，特别是考虑到这种数据的扭曲性质。客户贡献数据的固有偏差，由大量低价值贡献群体和高价值贡献者之间的巨大差异所突出，需要一个先进的预测模型。为了解决这个问题，我们提出了偏度感知的 Booting 回归树(SBRT)框架来预测分布呈现高偏度的客户贡献。SBRT 将梯度增强决策树的优势与随机树失活的新机制无缝结合起来，巧妙地处理了分布偏态问题。该模型的有效性基于四个原则: 交叉特征提取、基于百分点的标定和再平衡方法、增强阶段的树失活和 Huber 损失的利用。广泛的现实世界银行数据测试强调了 SBRT 在管理偏态分布方面的有前途的能力，使其在预测客户贡献方面略胜一筹。这项工作的高潮在于它的实际验证，其中在线 A/B 测试突出了 SBRT 的有形工业适用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Skewness-aware+Boosting+Regression+Trees+for+Customer+Contribution+Prediction+in+Financial+Precision+Marketing)|0|
|[Can we Soft Prompt LLMs for Graph Learning Tasks?](https://doi.org/10.1145/3589335.3651476)|Zheyuan Liu, Xiaoxin He, Yijun Tian, Nitesh V. Chawla||Graph plays an important role in representing complex relationships in real-world applications such as social networks, biological data and citation networks. In recent years, Large Language Models (LLMs) have achieved tremendous success in various domains, which makes applying LLMs to graphs particularly appealing. However, directly applying LLMs to graph modalities presents unique challenges due to the discrepancy and mismatch between the graph and text modalities. Hence, to further investigate LLMs' potential for comprehending graph information, we introduce GraphPrompter, a novel framework designed to align graph information with LLMs via soft prompts. Specifically, GraphPrompter consists of two main components: a graph neural network to encode complex graph information and an LLM that effectively processes textual information. Comprehensive experiments on various benchmark datasets under node classification and link prediction tasks demonstrate the effectiveness of our proposed method. The GraphPrompter framework unveils the substantial capabilities of LLMs as predictors in graph-related tasks, enabling researchers to utilize LLMs across a spectrum of real-world graph scenarios more effectively.|在社会网络、生物数据和引文网络等现实应用中，图表在表示复杂关系方面发挥着重要作用。近年来，大型语言模型(LLM)在各个领域取得了巨大的成功，这使得将 LLM 应用于图形尤其具有吸引力。然而，由于图形和文本模式之间的差异和不匹配，直接将 LLM 应用于图形模式提出了独特的挑战。因此，为了进一步研究 LLM 理解图形信息的潜力，我们引入了 GraphPrompter，这是一个通过软提示将图形信息与 LLM 对齐的新框架。具体来说，GraphPrompter 由两个主要部分组成: 一个用于编码复杂图形信息的图形神经网络和一个有效处理文本信息的 LLM。在节点分类和链路预测任务下对各种基准数据集进行了综合实验，验证了该方法的有效性。GraphPrompter 框架揭示了 LLM 在图形相关任务中作为预测器的巨大能力，使研究人员能够更有效地利用 LLM 跨越一系列真实世界的图形场景。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+we+Soft+Prompt+LLMs+for+Graph+Learning+Tasks?)|0|
|[Everything Perturbed All at Once: Enabling Differentiable Graph Attacks](https://doi.org/10.1145/3589335.3651501)|Haoran Liu, Bokun Wang, Jianling Wang, Xiangjue Dong, Tianbao Yang, James Caverlee||As powerful tools for representation learning on graphs, graph neural networks (GNNs) have played an important role in applications including social networks, recommendation systems, and online web services. However, GNNs have been shown to be vulnerable to adversarial attacks, which can significantly degrade their effectiveness. Recent state-of-the-art approaches in adversarial attacks rely on gradient-based meta-learning to selectively perturb a single edge with the highest attack score until they reach the budget constraint. While effective in identifying vulnerable links, these methods are plagued by high computational costs. By leveraging continuous relaxation and parameterization of the graph structure, we propose a novel attack method called Differentiable Graph Attack (DGA) to efficiently generate effective attacks and meanwhile eliminate the need for costly retraining. Compared to the state-of-the-art, DGA achieves nearly equivalent attack performance with 6 times less training time and 11 times smaller GPU memory footprint on different benchmark datasets. Additionally, we provide extensive experimental analyses of the transferability of the DGA among different graph models, as well as its robustness against widely-used defense mechanisms.|图神经网络(GNN)作为图表示学习的强大工具，在社交网络、推荐系统和在线 Web 服务等应用中发挥着重要作用。然而，GNN 已被证明易受敌对攻击，这可以显着降低他们的有效性。最近最先进的对抗性攻击方法依赖于基于梯度的元学习来有选择性地扰乱攻击分数最高的单个边缘，直到它们达到预算线。虽然这些方法能有效地识别易受攻击的链接，但计算成本较高。通过利用图结构的连续松弛和参量化，我们提出了一种新的攻击方法，称为可微图攻击(DGA) ，以有效地生成有效的攻击，同时消除了昂贵的再训练的需要。与现有技术相比，DGA 在不同的基准数据集上实现了几乎相同的攻击性能，训练时间减少了6倍，GPU 内存占用减少了11倍。此外，我们提供了广泛的实验分析 DGA 在不同的图模型之间的可转移性，以及其对广泛使用的防御机制的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Everything+Perturbed+All+at+Once:+Enabling+Differentiable+Graph+Attacks)|0|
|[Unlink to Unlearn: Simplifying Edge Unlearning in GNNs](https://doi.org/10.1145/3589335.3651578)|Jiajun Tan, Fei Sun, Ruichen Qiu, Du Su, Huawei Shen||As concerns over data privacy intensify, unlearning in Graph Neural Networks (GNNs) has emerged as a prominent research frontier in academia. This concept is pivotal in enforcing the right to be forgotten, which entails the selective removal of specific data from trained GNNs upon user request. Our research focuses on edge unlearning, a process of particular relevance to real-world applications, owing to its widespread applicability. Current state-of-the-art approaches like GNNDelete can eliminate the influence of specific edges, yet our research has revealed a critical limitation in these approaches, termed over-forgetting. It occurs when the unlearning process inadvertently removes excessive information beyond specific data, leading to a significant decline in prediction accuracy for the remaining edges. To address this issue, we have identified the loss functions of GNNDelete as the primary source of the over-forgetting phenomenon. Furthermore, our analysis also suggests that loss functions may not be essential for effective edge unlearning. Building on these insights, we have simplified GNNDelete to develop Unlink-to-Unlearn (UtU), a novel method that facilitates unlearning exclusively through unlinking the forget edges from graph structure. Our extensive experiments demonstrate that UtU delivers privacy protection on par with that of a retrained model while preserving high accuracy in downstream tasks. Specifically, UtU upholds over 97.3 link prediction accuracy. Meanwhile, UtU requires only constant computational demands, underscoring its advantage as a highly lightweight and practical edge unlearning solution.|随着人们对数据隐私的担忧加剧，图形神经网络(GNN)中的去学习已经成为学术界一个突出的研究前沿。这一概念对于落实被遗忘的权利至关重要，因为这需要根据用户的要求，有选择地从受过训练的 GNN 中删除特定数据。我们的研究侧重于边缘去除，由于其广泛的适用性，这是一个与现实世界应用特别相关的过程。当前最先进的方法如 GNNDelete 可以消除特定边缘的影响，但我们的研究揭示了这些方法的一个关键限制，称为过度遗忘。当忘记过程无意中删除了特定数据之外的过多信息时，就会发生这种情况，导致剩余边缘的预测准确性显著下降。为了解决这个问题，我们确定了 GNNDelete 的损失函数是过度遗忘现象的主要来源。此外，我们的分析还表明，损失函数可能不是必不可少的有效边缘去除。在这些见解的基础上，我们简化了 GNNDelete 来开发 Unlink-to-Unlearn (UtU) ，这是一种新的方法，通过从图形结构中解除遗忘边的连接来促进完全忘记。我们的大量实验表明，UtU 提供了与再训练模型相当的隐私保护，同时在下游任务中保持了高精度。具体来说，UtU 支持超过97.3的链路预测精度。同时，UtU 只需要不断的计算需求，突出了其作为一个高度轻量级和实用的边缘去学习解决方案的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unlink+to+Unlearn:+Simplifying+Edge+Unlearning+in+GNNs)|0|
|[Near-duplicate Question Detection](https://doi.org/10.1145/3589335.3651538)|Preetam Prabhu Srikar Dammu, Omar Alonso||Suggesting relevant questions to users is an important task in various applications, such as community Q&A or e-commerce websites. To ensure that there is no redundancy in the selected set of candidate questions, it is essential to filter out any near-duplicate questions. Identifying near-duplicate questions has another use case in light of the adoption of Large Language Models (LLMs) - fetching pre-computed answers for similar questions. However, identifying the similarity of questions is a bit more complex in comparison to generic text, as questions entail open-ended information that is not explicitly contained within the wording of the question itself. We introduce a taxonomy that accounts for the subtle intricacies characteristic of near-duplicate questions and propose a method for detecting them utilizing the capabilities of LLMs.|在各种应用程序(如社区问答或电子商务网站)中，向用户提出相关问题是一项重要任务。为了确保所选择的候选问题集没有冗余，有必要过滤掉任何近乎重复的问题。鉴于大型语言模型(LLM)的采用，识别接近重复的问题还有另一个用例——获取类似问题的预计算答案。然而，与通用案文相比，确定问题的相似性要复杂一些，因为问题涉及开放式信息，而这些信息并没有明确包含在问题本身的措辞中。我们介绍了一种分类法，它解释了近重复问题的微妙复杂性特征，并提出了一种利用 LLM 的能力来检测它们的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Near-duplicate+Question+Detection)|0|
|[Tackling Long-Tail Entities for Temporal Knowledge Graph Completion](https://doi.org/10.1145/3589335.3651565)|Mehrnoosh Mirtaheri, Ryan A. Rossi, Sungchul Kim, Kanak Mahadik, Tong Yu, Xiang Chen, Mohammad Rostami||Most Temporal Knowledge Graphs (TKGs) exhibit a long-tail entity distribution, where the majority of entities have sparse connections. Existing TKG completion methods struggle with managing new or unseen entities that often lack sufficient connections. In this paper, we introduce a model-agnostic enhancement layer that can be integrated with any existing TKG completion method to improve its performance. This enhancement layer employs a broader, global definition of entity similarity, transcending the limitations of local neighborhood proximity found in Graph Neural Network (GNN) based methods. Additionally, we conduct our evaluations in a novel, realistic setup that treats the TKG as a stream of evolving data. Evaluations on two benchmark datasets demonstrate that our framework surpasses existing methods in overall link prediction, inductive link prediction, and in addressing long-tail entities. Notably, our approach achieves a 10% improvement in MRR on one dataset and a 15% increase on another.|大多数时态知识图(TKG)呈现长尾实体分布，其中大多数实体具有稀疏连接。现有的 TKG 完成方法难以管理新的或看不见的实体，往往缺乏足够的连接。在本文中，我们引入了一个模型无关的增强层，可以集成任何现有的 TKG 完成方法，以改善其性能。该增强层采用了更广泛的、全局的实体相似性定义，超越了基于图形神经网络(GNN)方法的局部邻域接近性的局限性。此外，我们进行我们的评估在一个新颖的，现实的设置，对待 TKG 作为一个不断发展的数据流。对两个基准数据集的评估表明，我们的框架在整体链接预测、归纳链接预测和寻址长尾实体方面优于现有的方法。值得注意的是，我们的方法在一个数据集上实现了10% 的 MRR 改进，在另一个数据集上实现了15% 的增长。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tackling+Long-Tail+Entities+for+Temporal+Knowledge+Graph+Completion)|0|
|[From Creation to Clarification: ChatGPT's Journey Through the Fake News Quagmire](https://doi.org/10.1145/3589335.3651509)|Yue Huang, Kai Shu, Philip S. Yu, Lichao Sun||The rampant spread of fake news has adversely affected society, resulting in extensive research on curbing its spread. As a notable milestone in large language models (LLMs), ChatGPT has gained significant attention due to its exceptional capabilities. In this study, we present an exploration of ChatGPT's proficiency in generating, explaining, and detecting fake news as follows.Generation -- We employ different prompt methods to generate fake news and prove the high quality of these instances through both self-assessment and human evaluation.Explanation -- We obtain nine features to characterize fake news based on ChatGPT's explanations and analyze the distribution of these factors across multiple public datasets.Detection -- We examine ChatGPT's capacity to identify fake news. We propose a reason-aware prompt method to improve its performance. We further probe into the potential extra information that could bolster its effectiveness in detecting fake news.|假新闻的猖獗传播对社会产生了负面影响，导致了对遏制假新闻传播的广泛研究。作为大型语言模型(LLM)中的一个里程碑，ChatGPT 由于其卓越的功能而受到了广泛的关注。在这项研究中，我们提出了一个探讨聊天 GPT 的熟练程度，生成，解释和检测假新闻如下。生成——我们采用不同的快速生成方法，通过自我评价和人工评价来生成假新闻，证明这些实例的高质量。解释——基于 ChatGPT 的解释，我们得到了九个描述假新闻的特征，并分析了这些因素在多个公共数据集中的分布情况。检测——我们检测聊天 GPT 识别假新闻的能力。我们提出了一种理性感知的提示方法来改善其性能。我们进一步探讨了潜在的额外信息，可以提高其在检测假新闻的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Creation+to+Clarification:+ChatGPT's+Journey+Through+the+Fake+News+Quagmire)|0|
|[The Invisible Game on the Internet: A Case Study of Decoding Deceptive Patterns](https://doi.org/10.1145/3589335.3651571)|Zewei Shi, Ruoxi Sun, Jieshan Chen, Jiamou Sun, Minhui Xue||Deceptive patterns are design practices embedded in digital platforms to manipulate users, representing a widespread and long-standing issue in the web and mobile software development industry. Legislative actions highlight the urgency of globally regulating deceptive patterns. However, despite advancements in detection tools, a significant gap exists in assessing deceptive pattern risks. In this study, we introduce a comprehensive approach involving the interactions between the Adversary, Watchdog (e.g., detection tools), and Challengers (e.g., users) to formalize and decode deceptive pattern threats. Based on this, we propose a quantitative risk assessment system. Representative cases are analyzed to showcase the practicability of the proposed risk scoring system, emphasizing the importance of involving human factors in deceptive pattern risk assessment.|欺骗性模式是嵌入数字平台以操纵用户的设计实践，代表了网络和移动软件开发行业中一个广泛和长期存在的问题。立法行动突出了在全球监管欺骗模式的紧迫性。然而，尽管检测工具有所进步，但在评估欺骗模式风险方面仍存在显著差距。在这项研究中，我们介绍了一个全面的方法，涉及之间的互动的对手，看门狗(例如，检测工具)和挑战者(例如，用户)形式化和解码欺骗模式的威胁。在此基础上，我们提出了一个定量的风险评估体系。通过对典型案例的分析，说明了该风险评分系统的实用性，强调了在欺骗模式风险评估中引入人为因素的重要性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Invisible+Game+on+the+Internet:+A+Case+Study+of+Decoding+Deceptive+Patterns)|0|
|[Improving Model Robustness against Adversarial Examples with Redundant Fully Connected Layer](https://doi.org/10.1145/3589335.3651524)|Ziming Zhao, Zhaoxuan Li, Tingting Li, Jiongchi Yu, Fan Zhang, Rui Zhang||Recent studies show that deep neural networks are extremely vulnerable, especially for adversarial examples of image classification models. However, the current defense technologies exhibit a series of limitations in terms of the adaptability of different attacks, the trade-off between clean-instance accuracy and robust one, as well as efficiency for train time overhead. To tackle these problems, we present a novel component, named redundant fully connected layer, which can be combined with existing model backbones in a pluggable manner. Specifically, we design a tailor-made loss function for it that leverages cosine similarity to maximize the difference and diversity of multiple fully connected parts. We conduct extensive experiments against 12 representative attacks (white-box and black-box), based on the popular dataset. The empirical evaluations show that our scheme realizes significant outcomes against various attacks with negligible additional training overhead, while hardly bringing collateral damage for clean-instance accuracy.|最近的研究表明，深层神经网络是极其脆弱的，特别是对于图像分类模型的敌对例子。然而，目前的防御技术在不同攻击的适应性、清晰实例精度与鲁棒性之间的权衡以及训练时间开销的效率等方面存在一系列的局限性。为了解决这些问题，我们提出了一种新的组件，称为冗余完全连接层，它可以与现有的模型骨干结合在一起，以可插拔的方式。具体来说，我们为它设计了一个量身定制的损失功能，利用余弦距离最大化多个完全连接的部件的差异和多样性。我们基于流行的数据集对12种有代表性的攻击(白盒和黑盒)进行了广泛的实验。经验性的评估表明，我们的方案实现了对各种攻击的显著成果与可忽略的额外训练开销，同时几乎不会带来附带损害的清洁实例的准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Model+Robustness+against+Adversarial+Examples+with+Redundant+Fully+Connected+Layer)|0|
|[Thought Graph: Generating Thought Process for Biological Reasoning](https://doi.org/10.1145/3589335.3651572)|ChiYang Hsu, Kyle Cox, Jiawei Xu, Zhen Tan, Tianhua Zhai, Mengzhou Hu, Dexter Pratt, Tianlong Chen, Ziniu Hu, Ying Ding||We present the Thought Graph as a novel framework to support complex reasoning and use gene set analysis as an example to uncover semantic relationships between biological processes. Our framework stands out for its ability to provide a deeper understanding of gene sets, significantly surpassing GSEA by 40.28 to human annotations. Our analysis further provides insights into future directions of biological processes naming, and implications for bioinformatics and precision medicine.|我们提出思想图作为一个新的框架，以支持复杂的推理和使用基因集分析作为一个例子，揭示语义关系之间的生物过程。我们的框架能够提供对基因组更深入的理解，显著超过 GSEA 的40.28个人类注释。我们的分析进一步提供了对未来生物过程命名方向的洞察，以及对生物信息学和精准医学的影响。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Thought+Graph:+Generating+Thought+Process+for+Biological+Reasoning)|0|
|[On the Scale-Free Property of Citation Networks: An Empirical Study](https://doi.org/10.1145/3589335.3651541)|Xiaoshi Zhong, Huizhi Liang||Citation networks have been thought to exhibit scale-free property for many years; however, this assertion has been doubted recently. In this paper, we conduct extensive experiments to resolve this controversial issue. We firstly demonstrate the scale-free property in scale-free networks sampled from the popular Barabasi-Albert (BA) model. To this end, we employ a merged rank distribution, which is divided into outliers, power-law segment, and non-power-law data, to characterize network degrees, and propose a random sample consensus (RANSAC)-based method to identify power-law segments from merged rank distributions, and use the Kolmogorov-Smirnov (KS) test to examine the scale-free property in power-law segments. Subsequently, we apply the same methods to examine the scale-free property in real-world citation networks. Experimental results confirm the scale-free property in citation networks and attribute previous skepticism to the presence of outliers.|多年来，人们一直认为引文网络具有无标度特性，然而，这一观点最近受到了质疑。在本文中，我们进行了广泛的实验来解决这个有争议的问题。我们首先证明了从流行的 Barabasi-Albert (BA)模型采样的无标度网络的无标度性质。为此，我们采用合并秩分布(分为离群值、幂律分段和非幂律分段)来表征网络度，提出了一种基于随机样本一致性(RANSAC)的方法从合并秩分布中识别幂律分段，并利用 Kolmogorov-Smirnov (KS)检验幂律分段的无标度性质。随后，我们应用同样的方法来检验真实世界引文网络中的无标度特性。实验结果证实了引文网络的无标度特性，并将之前的怀疑归因于异常值的存在。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Scale-Free+Property+of+Citation+Networks:+An+Empirical+Study)|0|
|[Automatic Construction of Expiration Time Expression Dataset from Retweets](https://doi.org/10.1145/3589335.3651471)|Hirotaka Nagashima, Keishi Tajima||Documents describing information with expiration time often include time expressions specifying the expiration time. To train a classifier determining if a time expression represents an expiration time, we need a labeled dataset. We propose a method of automatically constructing such a dataset. Our method collects tweets including time expressions, and automatically determines whether the time expressions represent expiration times based on temporal changes in the frequency of retweets. Our experimental result shows that our method produces an effective dataset.|描述具有过期时间的信息的文档通常包括指定过期时间的时间表达式。为了训练分类器来确定时间表达式是否表示过期时间，我们需要一个标记的数据集。我们提出了一种自动构造这样一个数据集的方法。我们的方法收集包括时间表达式在内的 tweet，并根据转发频率的时间变化自动确定时间表达式是否表示过期时间。实验结果表明，该方法产生了一个有效的数据集。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Construction+of+Expiration+Time+Expression+Dataset+from+Retweets)|0|
|[Finding Dense and Persistently Expansive Subgraphs](https://doi.org/10.1145/3589335.3651507)|Petros Petsinis, Charalampos E. Tsourakakis, Panagiotis Karras||How can we detect a group of individuals whose connectivity persists and even strengthens over time? Despite extensive research on temporal networks, this practically pertinent question has been scantily investigated. In this paper, we formulate the problem of selecting a subset of nodes whose induced subgraph maximizes the overall edge count while abiding by time-aware spectral connectivity constraints. We solve the problem via a semidefinite programming (SDP) relaxation. Our experiments on a broad array of synthetic and real-world data establish the effectiveness of our method and deliver key insights on real-world temporal graphs.|我们如何才能发现一组个体的连接性持续存在，甚至随着时间的推移而加强？尽管对时间网络进行了广泛的研究，但这个实际上相关的问题却很少被研究。本文提出了在遵守时间感知的谱连通性约束的情况下，选择其诱导子图最大化整体边计数的节点子集的问题。我们通过半定规划(SDP)松弛来解决这个问题。我们对大量合成和真实数据的实验证实了我们方法的有效性，并提供了对真实世界时间图的关键见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Finding+Dense+and+Persistently+Expansive+Subgraphs)|0|
|[Hyperbolic Heterogeneous Graph Attention Networks](https://doi.org/10.1145/3589335.3651522)|Jongmin Park, Seunghoon Han, Soohwan Jeong, Sungsu Lim||Most previous heterogeneous graph embedding models represent elements in a heterogeneous graph as vector representations in a low-dimensional Euclidean space. However, because heterogeneous graphs inherently possess complex structures, such as hierarchical or power-law structures, distortions can occur when representing them in Euclidean space. To overcome this limitation, we propose Hyperbolic Heterogeneous Graph Attention Networks (HHGAT) that learn vector representations in hyperbolic spaces with meta-path instances. We conducted experiments on three real-world heterogeneous graph datasets, demonstrating that HHGAT outperforms state-of-the-art heterogeneous graph embedding models in node classification and clustering tasks.|以往的异构图嵌入模型大多将异构图中的元素表示为低维欧氏空间中的向量表示。然而，由于异质图本身具有复杂的结构，例如层次结构或幂律结构，当它们在欧氏空间中表示时会发生畸变。为了克服这一局限性，我们提出了双曲异构图注意网络(HHGAT) ，它通过元路径实例学习双曲空间中的向量表示。我们在三个真实世界的异构图数据集上进行了实验，结果表明 HHGAT 在节点分类和聚类任务方面优于现有的异构图嵌入模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hyperbolic+Heterogeneous+Graph+Attention+Networks)|0|
|[Task-Driven Quantum Device Fingerprint Identification via Modeling QNN Outcome Shift Induced by Quantum Noise](https://doi.org/10.1145/3589335.3651567)|Tingting Li, Ziming Zhao, Jianwei Yin||Quantum computing (QC) has recently achieved significant technological advancements, attracting widespread attention. Current users mainly access QC resources through cloud services. However, cloud-based quantum services provide convenience while also introducing security risks. For example, attackers could steal private information or inject malicious programs into quantum devices, while quantum device fingerprinting may be the first step for these malicious intents. In this paper, we propose a novel Task-Driven Quantum Device Fingerprinting (TD-QDF) identification method based on quantum neural network (QNN) task outcomes. Unlike previous research, our method does not require any hardware details, resulting in high availability in practice. Extensive experiments involving 3 QNN circuits on 10 real IBM quantum computers show that our method can effectively identify quantum devices. This research contributes to advancing quantum fingerprinting technologies and holds promising implications for enhancing the security and accountability of quantum computing systems.|量子计算(QC)近年来取得了显著的技术进步，引起了人们的广泛关注。目前用户主要通过云服务访问 QC 资源。然而，基于云的量子服务提供了便利，同时也带来了安全风险。例如，攻击者可能窃取私人信息或将恶意程序注入量子设备，而量子设备指纹识别可能是这些恶意意图的第一步。本文提出了一种基于量子神经网络(QNN)任务结果的任务驱动量子器件指纹识别方法。与以前的研究不同，我们的方法不需要任何硬件细节，因此在实践中会产生高可用性。在10台实际的 IBM 量子计算机上对3个 QNN 电路进行了大量的实验，结果表明该方法能够有效地识别量子器件。该研究有助于推动量子指纹技术的发展，并对提高量子计算系统的安全性和可靠性具有积极的意义。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Task-Driven+Quantum+Device+Fingerprint+Identification+via+Modeling+QNN+Outcome+Shift+Induced+by+Quantum+Noise)|0|
|[News-Driven Price Movement Forecasting with Label-Prior Graph Attention](https://doi.org/10.1145/3589335.3651539)|YiTing Liu, ChungChi Chen, HenHsen Huang, HsinHsi Chen||This paper introduces a novel approach to stock movement prediction using multi-label classification, leveraging the interconnections between news articles and related company stocks. We present the Label-Prior Graph Attention (LPGA) model, which significantly enhances the performance of news-driven stock price movement forecasting. This model is comprised of a unique graph attention architecture, incorporating a label encoder and a text encoder, designed to effectively capture and utilize the relationships between labels in a graph-based context. Our model demonstrates superior performance over several benchmark models. The LPGA model's efficacy is further validated through experiments on two multi-label datasets. The model outperforms established baseline models across various evaluation metrics. The success of the LPGA model in both stock movement prediction and general multi-label classification tasks indicates its potential as a versatile tool in the realm of machine learning and financial analysis.|本文利用新闻文章与相关公司股票之间的相互联系，提出了一种基于多标签分类的股票走势预测新方法。本文提出了标签-优先图注意(LPGA)模型，显著提高了新闻驱动的股价波动预测的性能。该模型由独特的图形注意力结构组成，包括标签编码器和文本编码器，旨在有效地捕获和利用基于图形的上下文中的标签之间的关系。我们的模型显示出优于几个基准模型的性能。通过对两个多标记数据集的实验，进一步验证了 LPGA 模型的有效性。该模型在各种评估指标中的性能优于已建立的基线模型。LPGA 模型在股票走势预测和一般多标签分类任务中的成功表明了它在机器学习和财务分析领域中作为一种通用工具的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=News-Driven+Price+Movement+Forecasting+with+Label-Prior+Graph+Attention)|0|
|[Enabling Patient-side Disease Prediction via the Integration of Patient Narratives](https://doi.org/10.1145/3589335.3651498)|Zhixiang Su, Yinan Zhang, Jiazheng Jing, Jie Xiao, Zhiqi Shen||Disease prediction holds considerable significance in modern healthcare, because of its crucial role in facilitating early intervention and implementing effective prevention measures. However, most recent disease prediction approaches heavily rely on laboratory test outcomes (e.g., blood tests and medical imaging from X-rays). Gaining access to such data for precise disease prediction is often a complex task from the standpoint of a patient and is always only available post-patient consultation. To make disease prediction available from patient-side, we propose Personalized Medical Disease Prediction (PoMP), which predicts diseases using patient health narratives including textual descriptions and demographic information. By applying PoMP, patients can gain a clearer comprehension of their conditions, empowering them to directly seek appropriate medical specialists and thereby reducing the time spent navigating healthcare communication to locate suitable doctors. We conducted extensive experiments using real-world data from Haodf to showcase the effectiveness of PoMP.|疾病预测在现代医疗保健中具有重要意义，因为它在促进早期干预和实施有效预防措施方面具有关键作用。然而，大多数最新的疾病预测方法严重依赖于实验室检测结果(例如，血液检测和 X 射线医学成像)。从患者的角度来看，获取这些数据以进行精确的疾病预测往往是一项复杂的任务，而且往往只有患者术后才能进行咨询。为了使疾病预测从患者方面可用，我们提出了个性化医疗疾病预测(PoMP) ，它使用患者健康叙述，包括文本描述和人口统计信息来预测疾病。通过应用 PoMP，患者可以更清楚地了解自己的病情，使他们能够直接寻求适当的医疗专家，从而减少花在导航卫生保健通信以找到合适的医生上的时间。我们利用来自 Haodf 的实际数据进行了广泛的实验，以展示 PoMP 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enabling+Patient-side+Disease+Prediction+via+the+Integration+of+Patient+Narratives)|0|
|[LinkGuard: Link Locally Privacy-Preserving Graph Neural Networks with Integrated Denoising and Private Learning](https://doi.org/10.1145/3589335.3651533)|Yuxin Qi, Xi Lin, Ziyao Liu, Gaolei Li, Jingyu Wang, Jianhua Li||Recent studies have introduced privacy-preserving graph neural networks to safeguard the privacy of sensitive link information in graphs. However, existing link protection mechanisms in GNNs, particularly over decentralized nodes, struggle to strike an optimal balance between privacy and utility. We argue that a pivotal issue is the separation of noisy topology denoising and GNN private learning into distinct phases at the server side, leading to an under-denoising problem in the noisy topology. To address this, we propose a dynamic, adaptive Link LDP framework that performs noisy topology denoising on the server side in a dynamic manner. This approach aims to mitigate the impact of local noise on the GNN training process, reducing the uncertainty introduced by local noise. Furthermore, we integrate the noise generation and private training processes across all existing Link LDP GNNs into a unified framework. Experimental results demonstrate that our method surpasses existing approaches, obtaining around a 7% performance improvement under strong privacy strength and achieving a better trade-off between utility and privacy.|最近的研究引入了保护隐私的图神经网络来保护图中敏感链接信息的隐私。然而，GNN 中现有的链路保护机制，特别是在分散的节点上，很难在隐私和效用之间达到最佳的平衡。我们认为，一个关键的问题是噪声拓扑去噪和 GNN 私有学习在服务器端分离成不同的阶段，导致了噪声拓扑中的一个欠去噪问题。为了解决这个问题，我们提出了一个动态的，自适应的 Link LDP 框架，它以动态的方式在服务器端执行噪声拓扑去噪。该方法旨在减轻局部噪声对 GNN 训练过程的影响，降低局部噪声带来的不确定性。此外，我们将所有现有的 Link LDP GNN 的噪声产生和私有训练过程集成到一个统一的框架中。实验结果表明，我们的方法超越了现有的方法，在强隐私强度下获得了7% 左右的性能提高，并在实用性和隐私性之间取得了更好的平衡。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LinkGuard:+Link+Locally+Privacy-Preserving+Graph+Neural+Networks+with+Integrated+Denoising+and+Private+Learning)|0|
|[Generator-Guided Crowd Reaction Assessment](https://doi.org/10.1145/3589335.3651512)|Sohom Ghosh, ChungChi Chen, Sudip Kumar Naskar||In the realm of social media, understanding and predicting post reach is a significant challenge. This paper presents a Crowd Reaction AssessMent (CReAM) task designed to estimate if a given social media post will receive more reaction than another, a particularly essential task for digital marketers and content writers. We introduce the Crowd Reaction Estimation Dataset (CRED), consisting of pairs of tweets from The White House with comparative measures of retweet count. The proposed Generator-Guided Estimation Approach (GGEA) leverages generative Large Language Models (LLMs), such as ChatGPT, FLAN-UL2, and Claude, to guide classification models for making better predictions. Our results reveal that a fine-tuned FLANG-RoBERTa model, utilizing a cross-encoder architecture with tweet content and responses generated by Claude, performs optimally. We further use a T5-based paraphraser to generate paraphrases of a given post and demonstrate GGEA's ability to predict which post will elicit the most reactions. We believe this novel application of LLMs provides a significant advancement in predicting social media post reach.|在社会化媒体领域，理解和预测发帖范围是一个重大的挑战。本文提出了一个群体反应评估(CReAM)任务，旨在估计是否一个给定的社会媒体帖子将获得更多的反应比另一个，一个特别重要的任务数字营销人员和内容作家。我们介绍了群体反应估计数据集(CRED) ，包括来自白宫的一对推文，以及对推文数量的比较测量。提出的生成器引导的评估方法(GGEA)利用生成的大语言模型(LLM) ，如 ChatGPT、 FLAN-UL2和 Claude，来指导分类模型，以便做出更好的预测。我们的研究结果表明，一个微调的 FLANG-RoBERTa 模型，利用一个交叉编码器架构与 tweet 内容和响应生成的克劳德，执行最佳。我们进一步使用一个基于 T5的解释器来生成给定文章的解释，并证明 GGEA 预测哪篇文章将引起最多反应的能力。我们相信 LLM 的这个新颖的应用为预测社会媒体的发布范围提供了一个重要的进步。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generator-Guided+Crowd+Reaction+Assessment)|0|
|[Hierarchical Tensor Clustering for Multiple Graphs Representation](https://doi.org/10.1145/3589335.3651519)|Karima Boutalbi, Rafika Boutalbi, Hervé Verjus, Kavé Salamatian||Graph clustering is a challenging task, especially when there is a hierarchical structure. The availability of multiple graphs (or relational graphs), in the multi-graph setting, provides additional information that can be leveraged to improve clustering results. This paper aims to develop a new hierarchical clustering algorithm for multi-graphs, the HTGM algorithm. This algorithm represents the set of graphs in the multi-graph as a 3-way tensor, and maximizes a modularity measure, extending the modularity-based graph clustering algorithm to multi-graphs and tensor structures. We evaluate the proposed algorithm over synthetic and real-world datasets and show the effectiveness of the proposed algorithm by benchmarking it to alternative clustering algorithms.|图的聚类是一项具有挑战性的任务，尤其是在存在层次结构的情况下。在多图设置中，多图(或关系图)的可用性提供了额外的信息，可以利用这些信息来改进聚类结果。本文旨在开发一种新的多图层次聚类算法—— HTGM 算法。该算法将多图中的一组图表示为一个三向张量，最大化了模性度量，将基于模性的图聚类算法推广到多图和张量结构。通过对合成数据集和实际数据集的评估，证明了算法的有效性，并将其与其他聚类算法进行了比较。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Tensor+Clustering+for+Multiple+Graphs+Representation)|0|
|[Smooth Anonymity for Sparse Graphs](https://doi.org/10.1145/3589335.3651561)|Alessandro Epasto, Hossein Esfandiari, Vahab Mirrokni, Andrés Muñoz Medina||When working with user data providing well-defined privacy guarantees is paramount. In this work, we aim to manipulate and share an entire sparse dataset with a third party privately. In fact, differential privacy has emerged as the gold standard of privacy, however, when it comes to sharing sparse datasets, e.g. sparse networks, as one of our main results, we prove that any differentially private mechanism that maintains a reasonable similarity with the initial dataset is doomed to have a very weak privacy guarantee. In such situations, we need to look into other privacy notions such as k-anonymity. In this work, we consider a variation of k-anonymity, which we call smooth-k-anonymity, and design simple large-scale algorithms that efficiently provide smooth-k-anonymity. We further perform an empirical evaluation to back our theoretical guarantees and show that our algorithm improves the performance in downstream machine learning tasks on anonymized data.|在处理提供良好定义的隐私保障的用户数据时，最重要的是。在这项工作中，我们的目标是操作和共享一个完整的稀疏数据集与第三方私人。事实上，差分隐私已经成为隐私的黄金标准，然而，当涉及到共享稀疏数据集，例如稀疏网络，作为我们的主要结果之一，我们证明，任何差异私有机制，保持与初始数据集的合理相似性，注定有一个非常薄弱的隐私保障。在这种情况下，我们需要研究其他隐私概念，如 k 匿名。在本文中，我们考虑了一种称为光滑 k 匿名的变形，并且设计了简单的大规模算法来有效地提供光滑 k 匿名。我们进一步进行了实证评估，以支持我们的理论保证，并表明我们的算法提高性能在下游机器学习任务的匿名数据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Smooth+Anonymity+for+Sparse+Graphs)|0|
|[Predicting Node Influence in Complex Networks by the K-Shell Entropy and Degree Centrality](https://doi.org/10.1145/3589335.3651547)|Shima Esfandiari, Mostafa Fakhrahmad||Currently, with the expansion of the use of social networks, the topic of information dissemination has achieved significant importance. The spread of rumors and efforts to stop them have led researchers to pay more attention than ever to predicting the impact of each individual on the network. Various methods have been proposed for this purpose, such as the Hybrid global structure model (HGSM), Generalized Gravity centrality (GGC), and Degree and neighborhood centrality (DNC). However, alongside their advantages, they have drawbacks, such as high time complexity, low accuracy, or inefficiency in distinguishing between the dissemination abilities of different individuals. Therefore, this paper focuses on a method based on degree, K-shell, and K-shell diversity in the neighborhood of each individual. Simulations were conducted using the Susceptible-Infected-Recovered (SIR) model and compared with 9 recent methods. Evaluations of 7 different networks in terms of resolution, accuracy, time complexity, and correlation exhibit the superiority of the proposed method.|目前，随着社交网络使用的扩大，信息传播的话题已经取得了显著的重要性。谣言的传播和阻止谣言的努力使得研究人员比以往任何时候都更加关注预测每个人对网络的影响。为此，人们提出了各种方法，如混合全局结构模型(HGSM)、广义重心度(GGC)、度和邻域中心度(DNC)。然而，除了它们的优点之外，它们也有缺点，如时间复杂度高、准确度低或者在区分不同个体的传播能力方面效率低下。因此，本文重点研究了一种基于度、 K- 壳和 K- 壳多样性的方法。采用易感染-感染-恢复(SIR)模型进行模拟，并与9种最新方法进行比较。从分辨率、精度、时间复杂度和相关性等方面对7种不同网络进行了评价，显示了该方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Predicting+Node+Influence+in+Complex+Networks+by+the+K-Shell+Entropy+and+Degree+Centrality)|0|
|[Knowledge Induced Transformer Network for Causality Prediction](https://doi.org/10.1145/3589335.3651531)|Tirthankar Dasgupta, Manjira Sinha, Abir Naskar||Causal extraction from text plays a crucial role in various downstream analytical and predictive tasks, such as constructing repositories of causal insights for reasoning. However, existing models often overlook the rich contextual commonsense knowledge that could enhance the reasoning process and evaluate underlying causal mechanisms. In this study, we introduce a knowledge-induced transformer architecture for predicting causality. Our model accepts an antecedent and a set of contextual knowledge as input, then ranks plausible consequences from a given set of hypotheses. To enhance semantic understanding, we augment the transformer with a relational graph network, which computes fine-grained semantic information between the antecedent, knowledge, and hypotheses using a similarity matrix that quantifies word-to-word similarity. We evaluate the proposed architecture against state-of-the-art models using openly available datasets and demonstrate its superior performance.|从文本中提取因果关系在各种下游分析和预测任务中起着至关重要的作用，例如构建用于推理的因果洞察力库。然而，现有的模型往往忽视了丰富的上下文常识知识，可以增强推理过程和评估潜在的因果机制。在这项研究中，我们介绍了一个知识感应变压器架构，用于预测因果关系。我们的模型接受一个先行事件和一组上下文知识作为输入，然后从给定的一组假设中排列合理的结果。为了增强语义理解，我们用一个关系图网络来增强转换器，该网络使用一个量化词与词之间相似度的相似矩阵来计算先行词、知识和假设之间的细粒度语义信息。我们使用公开可用的数据集评估所提出的架构和最先进的模型，并证明其优越的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Induced+Transformer+Network+for+Causality+Prediction)|0|
|[MetroGNN: Metro Network Expansion with Reinforcement Learning](https://doi.org/10.1145/3589335.3651536)|Hongyuan Su, Yu Zheng, Jingtao Ding, Depeng Jin, Yong Li||Selecting urban regions for metro network expansion to meet maximal transportation demands is crucial for urban development, while computationally challenging to solve. The expansion process relies not only on complicated features like urban demographics and origin-destination (OD) flow but is also constrained by the existing metro network and urban geography. In this paper, we introduce a reinforcement learning framework to address a Markov decision process within an urban heterogeneous multi-graph. Our approach employs an attentive policy network that intelligently selects nodes based on information captured by a graph neural network. Experiments on real-world urban data demonstrate that our proposed methodology substantially improve the satisfied transportation demands by over 30% when compared with state-of-the-art methods. Codes are published at https://github.com/tsinghua-fib-lab/MetroGNN.|选择城市区域进行地铁网络扩展，以满足最大的交通需求，是城市发展的关键，同时也是计算上的挑战。城市扩展过程不仅依赖于复杂的城市人口统计特征和发源地-目的地(OD)流量，而且还受到现有地铁网络和城市地理的制约。在这篇文章中，我们引入了一个强化学习框架来处理城市异质多图中的一个马可夫决策过程。该方法采用注意策略网络，根据图神经网络捕获的信息智能选择节点。对真实城市数据的实验表明，与最先进的方法相比，我们提出的方法大大提高了30% 以上的满足交通需求。密码是在 https://github.com/tsinghua-fib-lab/metrognn 公布的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MetroGNN:+Metro+Network+Expansion+with+Reinforcement+Learning)|0|
|[Breaking the Bot Barrier: Evaluating Adversarial AI Techniques Against Multi-Modal Defense Models](https://doi.org/10.1145/3589335.3651474)|Behzad Ousat, Dongsheng Luo, Amin Kharraz||Websites utilize several approaches to detect automated agents. The agents are deployed either for beneficial purposes such as search engine crawlers, or to perform tasks on behalf of the adversary such as scanning for vulnerabilities. Recent methods in detecting such agents include the analysis of the behavior that the agents show when visiting the website. In this paper, I) we describe a deep learning framework that analyzes the triggered browser events to classify the visitor. II) We develop two adversarial attacks in order to bypass the defense by generating adversarial vectors that are misclassified by the model. III) We discuss how applicable the attacks are by reviewing the limitations of the popular tools (i.e., Selenium and Puppeteer) used for the development of automated agents based on full-fledged browsers.|网站利用多种方法来检测自动代理。代理的部署要么是为了搜索引擎爬虫之类的有益目的，要么是为了代表对手执行扫描漏洞之类的任务。最近检测这类代理的方法包括分析代理在访问网站时所表现出的行为。在本文中，我们描述了一个深度学习框架，它分析触发的浏览器事件来对访问者进行分类。为了绕过防御，我们开发了两种对抗性攻击，通过生成被模型错误分类的对抗性向量。III)我们通过回顾用于开发基于成熟浏览器的自动代理的流行工具(例如 Selenium 和 Puppeteer)的局限性来讨论这些攻击是如何适用的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Breaking+the+Bot+Barrier:+Evaluating+Adversarial+AI+Techniques+Against+Multi-Modal+Defense+Models)|0|
|[RealGraphGPU++: A High-Performance GPU-Based Graph Engine with Direct Storage-to-DM IO](https://doi.org/10.1145/3589335.3651549)|JeongMin Park, MyungHwan Jang, DuckHo Bae, SangWook Kim||Recently, with the increasing size of real-world networks, graph engines have been studied extensively for efficient graph analysis. As one of the state-of-the-art single-machine-based graph engines, \textRealGraph ^\textGPU processes large-scale graphs very efficiently thanks to its well-designed architecture and the strong parallel-computing power of GPU. Via a preliminary analysis, we first observe \textRealGraph ^\textGPU has a good chance for more performance improvement in IOs between storage and GPU's device memory. This motivates us to present \textRealGraph ^\textGPU++, a solution that substantially reduces IO time by establishing adirect data path between storage and device memory. Additionally, it employsasynchronous processing of CPU and GPU tasks to issue IO requests more frequently, thereby improving overall performance by achieving higher IO bandwidth. Experimental results on real-world datasets show that \textRealGraph ^\textGPU++ outperforms dramatically existing 11 state-of-the-art graph engines including \textRealGraph ^\textGPU .|近年来，随着现实世界网络规模的不断扩大，图引擎已经被广泛地研究用于有效的图分析。TextRealGraph ^ textGPU 是最先进的基于单机的图形引擎之一，由于其设计良好的体系结构和 GPU 强大的并行计算能力，textRealGraph ^ textGPU 能够非常有效地处理大规模图形。通过初步分析，我们首先观察到 textRealGraph ^ textGPU 在存储和 GPU 的设备内存之间的 IOs 方面有很大的性能提升机会。这促使我们提出 textRealGraph ^ textGPU + + ，这个解决方案通过在存储和设备内存之间建立直接数据路径来大大减少 IO 时间。此外，它使用 CPU 和 GPU 任务的异步处理来更频繁地发出 IO 请求，从而通过实现更高的 IO 带宽来提高整体性能。在真实数据集上的实验结果表明，textRealGraph ^ textGPU + + 的性能明显优于现有的11个最先进的图形引擎，包括 textRealGraph ^ textGPU。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RealGraphGPU++:+A+High-Performance+GPU-Based+Graph+Engine+with+Direct+Storage-to-DM+IO)|0|
|[A Study of Vulnerability Repair in JavaScript Programs with Large Language Models](https://doi.org/10.1145/3589335.3651463)|Tan Khang Le, Saba Alimadadi, Steven Y. Ko||In recent years, JavaScript has become the most widely used programming language, especially in web development. However, writing secure JavaScript code is not trivial, and programmers often make mistakes that lead to security vulnerabilities in web applications. Large Language Models (LLMs) have demonstrated substantial advancements across multiple domains, and their evolving capabilities indicate their potential for automatic code generation based on a required specification, including automatic bug fixing. In this study, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding and fixing security vulnerabilities in JavaScript programs. We also investigate the impact of context in a prompt on directing LLMs to produce a correct patch of vulnerable JavaScript code. Our experiments on real-world software vulnerabilities show that while LLMs are promising in automatic program repair of JavaScript code, achieving a correct bug fix often requires an appropriate amount of context in the prompt.|近年来，JavaScript 已经成为最广泛使用的编程语言，尤其是在 Web 开发中。然而，编写安全的 JavaScript 代码并非易事，程序员经常会犯错误，从而导致 Web 应用程序中的安全漏洞。大型语言模型(Large Language Model，LLM)已经在多个领域展示了巨大的进步，它们不断发展的能力表明了它们在基于所需规范的自动代码生成方面的潜力，包括自动修复 bug。在这项研究中，我们探讨 LLM，即 ChatGPT 和 Bard，在发现和修复 JavaScript 程序中的安全漏洞的准确性。我们还研究了提示符中的上下文对指导 LLM 生成易受攻击的 JavaScript 代码的正确补丁的影响。我们对真实世界软件漏洞的实验表明，虽然 LLM 在 JavaScript 代码的自动程序修复方面很有前途，但实现正确的 bug 修复通常需要在提示符中添加适量的上下文。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Study+of+Vulnerability+Repair+in+JavaScript+Programs+with+Large+Language+Models)|0|
|[Deanonymizing Transactions Originating from Monero Tor Hidden Service Nodes](https://doi.org/10.1145/3589335.3651487)|Ruisheng Shi, Yulian Ge, Lina Lan, Zhiyuan Peng, Shenwen Lin, Lin Li||Monero is a privacy-focused cryptocurrency that incorporates anonymity networks (such as Tor and I2P) and deploys the Dandelion++ protocol to prevent malicious attackers from linking transactions with their source IPs. However, this paper highlights a vulnerability in Monero's integration of the Tor network, which allows an attacker to successfully deanonymize transactions originating from Monero Tor hidden service nodes at the network-layer level. Our approach involves injecting malicious Monero Tor hidden service nodes into the Monero P2P network to correlate the onion addresses of incoming Monero Tor hidden service peers with their originating transactions. And by sending a signal watermark embedded with the onion address to the Tor circuit, we establish a correlation between the onion address and IP address of a Monero Tor hidden service node. Ultimately, we correlate transactions and IPs of Monero Tor hidden service nodes. Through experimentation on the Monero testnet, we provide empirical evidence of the effectiveness of our approach in successfully deanonymizing transactions originating from Monero Tor hidden service nodes.|Monero 是一种注重隐私的加密货币，它结合了匿名网络(如 Tor 和 I2P) ，并部署了 Dandelion + + 协议，以防止恶意攻击者将交易与源 IP 链接起来。然而，本文强调了莫内罗 Tor 网络整合的一个漏洞，即允许攻击者成功地在网络层次上去匿名化来自 Monero Tor 隐藏服务节点的交易。我们的方法包括将恶意的 Monero Tor 隐藏服务节点注入到 Monero P2P 网络中，将传入的 Monero Tor 隐藏服务节点的洋葱地址与它们的原始事务相关联。通过向 Tor 电路发送嵌入洋葱地址的信号水印，建立了 Monero Tor 隐藏服务节点的洋葱地址和 IP 地址之间的相关性。最终，我们将 MoneroTor 隐藏服务节点的事务和 IP 关联起来。通过在 Monero testnet 上的实验，我们提供了我们的方法在成功去匿名化来自 Monero Tor 隐藏服务节点的交易方面的有效性的经验证明。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deanonymizing+Transactions+Originating+from+Monero+Tor+Hidden+Service+Nodes)|0|
|[WebGraph: The Next Generation (Is in Rust)](https://doi.org/10.1145/3589335.3651581)|Tommaso Fontana, Sebastiano Vigna, Stefano Zacchiroli||We report the results of a yearlong effort at the Laboratory for Web Algorithmics and Inria to port the WebGraph framework [4] from Java to Rust. For two decades WebGraph has been instrumental in the analysis and distribution of large graphs for the research community of TheWebConf, but the intrinsic limitations of the Java Virtual Machine had become a bottleneck for very large use cases, such as the Software Heritage Merkle graph [2] with its half a trillion arcs. As part of this clean-slate implementation of WebGraph in Rust, we developed a few ancillary projects bringing to the Rust ecosystem some missing features of independent interest, such as easy, consistent and zero-cost memory mapping of data structures. WebGraph in Rust offers impressive performance improvements over the previous implementation, enabling open-source graph analytics on very large datasets like Common Crawl, on top of a modern systems programming language.|我们报告了网络算法实验室和 Inria 一年来将网络图框架从 Java 移植到 Rust 的努力结果。二十年来，WebGraph 一直在为 TheWebConf 的研究社区分析和发布大型图形，但 Java 虚拟机的内在限制已经成为非常大的用例的瓶颈，例如软件遗产 Merkle 图形[2] ，其半万亿弧。作为 Rust 中 WebGraph 全新实现的一部分，我们开发了一些辅助项目，为 Rust 生态系统带来了一些独立感兴趣的缺失特性，比如数据结构的简单、一致和零成本内存映射。与之前的实现相比，WebGraph in Rust 提供了令人印象深刻的性能改进，支持在现代系统编程语言之上对非常大的数据集(如 Common Crawl)进行开放源码的图形分析。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=WebGraph:+The+Next+Generation+(Is+in+Rust))|0|
|[GradFilt: Class-wise Targeted Data Reconstruction from Gradients in Federated Learning](https://doi.org/10.1145/3589335.3651514)|Rui Zhang, Song Guo, Ping Li||Gradient Inversion Attacks (GIAs) have shown that private training data can be recovered from gradient updates in Federated Learning (FL). However, these GIAs can only recover the entire batch of data with limited performance or stochastically restore some random instances. In this paper, we propose a class-wise targeted attack, named GradFilt, which can reconstruct the training data of some specified class(es) from the batch-averaged gradients. By modifying the parameters of the classification layer, we create a filter within the FL model that eliminates the gradients of non-target data while preserving the gradients of target data. We evaluate GradFilt with image datasets on popular FL model architectures. The results show that GradFilt can effectively reconstruct the desired samples with higher accuracies than the existing GIAs. Moreover, we can also achieve 100% success rate in restoring the batch labels. We hope this work can raise awareness of the privacy risks in FL and inspire effective defense mechanisms.|梯度反转攻击(GIA)表明，在联邦学习(FL)中，可以从梯度更新中恢复私有训练数据。但是，这些 GIA 只能以有限的性能恢复整批数据或随机恢复某些随机实例。本文提出了一种基于类的目标攻击方法，叫做 GradFilt，它可以从批量平均的梯度重建特定类的训练数据。通过修改分类层的参数，我们在 FL 模型中创建了一个过滤器，在保留目标数据梯度的同时消除了非目标数据的梯度。我们在流行的 FL 模型结构上使用图像数据集来评估 graFilt。结果表明，与现有的 GIA 相比，GradFilt 能够有效地重建所需样本，并具有更高的精度。此外，我们还可以达到100% 的成功率恢复批标签。我们希望这项工作可以提高对隐私风险的认识，并启发有效的防御机制。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GradFilt:+Class-wise+Targeted+Data+Reconstruction+from+Gradients+in+Federated+Learning)|0|
|[Advancing Stance Detection of Political Fan Pages: A Multimodal Approach](https://doi.org/10.1145/3589335.3651467)|KuanHung Kuo, MingHung Wang, HungYu Kao, YuChen Dai||The evolution of political campaigns is evident with the ascent of social media. Ideological beliefs are increasingly disseminated through political-affiliated fan pages. The interaction between politicians and the general public on these platforms plays a pivotal role in election outcomes. In this study, we utilize a multimodal approach to explore and quantify similarities of ideologies among political fan pages. we employed visualization techniques to demonstrate the political stance of each fan page. To validate our proposal, we concentrated on an analysis of the 2021 national referendums in Taiwan, encompassing a collection of fan pages and their corresponding posts that were related to these referendums. Through a qualitative analysis of the content of these fan pages, the efficacy of our multimodal framework in clustering fan pages according to their respective political ideologies was evaluated. The findings of this study underscore the significant enhancement in the accuracy of stance detection when integrating multiple modalities of data, namely textual content, visual imagery, and user interactions.|随着社交媒体的崛起，政治运动的演变显而易见。意识形态信仰越来越多地通过附属于政治的粉丝网页传播。政治家和公众在这些平台上的互动对选举结果起着关键作用。在这项研究中，我们使用多模态的方法来探索和量化意识形态的相似性的政治粉丝页面。我们使用可视化技术来演示每个粉丝页面的政治立场。为了验证我们的建议，我们集中分析了台湾2021年全民投票的情况，其中包括与这些全民投票有关的粉丝网页及其相应的帖子。通过对这些扇页内容的定性分析，评估了我们的多模式框架根据扇页各自的政治意识形态对其进行聚类的有效性。这项研究的结果强调了在整合多种模式的数据，即文本内容，视觉图像和用户交互时，姿态检测的准确性显著提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Advancing+Stance+Detection+of+Political+Fan+Pages:+A+Multimodal+Approach)|0|
|[Structural Podcast Content Modeling with Generalizability](https://doi.org/10.1145/3589335.3651563)|Yijun Tian, Maryam Aziz, Alice Wang, Enrico Palumbo, Hugues Bouchard||Podcast content modeling is crucial for a variety of practical web uses, such as the recommendation and classification of podcasts. However, previous studies on podcast content modeling rely on task-specific datasets to train dedicated models for each downstream application, which are labels heavily dependent and the learned representations are non-generalizable across different tasks. In addition, the rich and intricate structural information among users, podcasts, and topics are neglected. In this paper, we propose to model podcast content without labels and learn general podcast representations without prior knowledge of downstream tasks. Moreover, the learned podcast representations encode crucial structural information, complementary to the independent content information of each podcast. In particular, we first collect a new and large-scale podcast graph from Spotify. Then, we propose Podcast2Vec, a novel self-supervised podcast content modeling method to learn podcast representations. Podcast2Vec captures general transferable knowledge across different tasks and complex structures via a metapath-based neighbor sampling strategy and a multi-view relational modeling framework. Thorough experiments demonstrate the superiority of our method on four real-world podcast content modeling tasks.|播客内容建模对于各种实际的网络应用来说是至关重要的，比如播客的推荐和分类。然而，以前关于播客内容建模的研究依赖于特定于任务的数据集来为每个下游应用训练专门的模型，这些模型严重依赖于标签，并且所学到的表示在不同的任务中是不可推广的。此外，用户、播客和主题之间丰富而复杂的结构信息被忽略。在这篇文章中，我们建议在没有标签的情况下对播客内容进行建模，并且在不知道下游任务的情况下学习一般的播客表示。此外，学习的播客表示编码关键的结构信息，补充每个播客的独立内容信息。特别是，我们首先从 Spotify 收集一个新的大型播客图表。然后，我们提出了 Podcast2Vec，一种新的自我监督的播客内容建模方法来学习播客表示。Podcast2Vec 通过基于元路径的邻居抽样策略和多视图关系建模框架获取跨不同任务和复杂结构的通用可转移知识。通过实验验证了该方法在四个实际播客内容建模任务中的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Structural+Podcast+Content+Modeling+with+Generalizability)|0|
|[Detecting Poisoning Attacks on Federated Learning Using Gradient-Weighted Class Activation Mapping](https://doi.org/10.1145/3589335.3651490)|Jingjing Zheng, Kai Li, Xin Yuan, Wei Ni, Eduardo Tovar||This paper proposes a new defense mechanism, namely, GCAMA, against model poisoning attacks on Federated learning (FL), which integrates <u>G</u>radient-weighted <u>C</u>lass <u>A</u>ctivation <u>M</u>apping (GradCAM) and <u>A</u>utoencoder to offer a scientifically more powerful detection capability compared to existing Euclidean distance-based approaches. Particularly, GCAMA generates a heat map for each uploaded local model update, transforming each local model update into a lower-dimensional, visual representation, thereby accentuating the hidden features of the heat maps and increasing the success rate of identifying anomalous heat maps and malicious local models. We test ResNet-18 and MobileNetV3-Large deep learning models with CIFAR-10 and GTSRB datasets under Non-Independent and Identically Distributed (Non-IID) setting, respectively. The results demonstrate that GCAMA offers superior test accuracy of FL global model compared to the state-of-the-art methods. Our code is available at: https://github.com/jjzgeeks/GradCAM-AE|本文提出了一种新的防御机制，即 GCAMA 防御模型中毒攻击的联邦学习(FL) ，该机制集成了 < u > G </u > 辐射加权 < u > C </u > lass < u > A </u > 激活 < u > M </u > app (gradCAM)和 < u > A </u > utoencoder，提供了比现有欧几里得距离方法更强大的科学检测能力。特别是，GCAMA 为每个上传的本地模型更新生成热图，将每个本地模型更新转换为低维可视化表示，从而强调热图的隐藏特征，并提高识别异常热图和恶意本地模型的成功率。我们分别在非独立和同分布(Non-IID)设置下用 CIFAR-10和 GTSRB 数据集测试 ResNet-18和 MobileNetV3-大型深度学习模型。结果表明，GCAMA 提供了更好的测试精度的 FL 全局模型相比，国家的最新方法。我们的代码可以在以下 https://github.com/jjzgeeks/gradcam-ae 找到|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Poisoning+Attacks+on+Federated+Learning+Using+Gradient-Weighted+Class+Activation+Mapping)|0|
|[Fighting against Fake News on Newly-Emerging Crisis: A Case Study of COVID-19](https://doi.org/10.1145/3589335.3651506)|Migyeong Yang, Chaewon Park, Jiwon Kang, Daeun Lee, Daejin Choi, Jinyoung Han||As social media users can easily access, generate, and spread information regardless of its authenticity, the proliferation of fake news related to public health has become a serious problem. Since these rumors have caused severe social issues, detecting them in the early stage is imminent. Therefore, in this paper, we propose a deep learning model that can debunk fake news on COVID-19, as a case study, at the initial stage of emergence. The evaluation with a newly-collected dataset consisting of both the COVID-19 and Non-COVID-19 fake news claims demonstrates that the proposed model achieves high performance, indicating that the model can identify fake news on COVID-19 in the early stage with a small amount of data. We believe that our methodology and findings can be applied to detect fake news on newly-emerging and critical topics, which should be performed with insufficient resources.|由于社交媒体用户可以很容易地获取、生成和传播信息，不管其真实性如何，与公共卫生有关的假新闻的泛滥已经成为一个严重的问题。由于这些谣言已经引起了严重的社会问题，对它们的早期识别迫在眉睫。因此，在本文中，我们提出了一个深度学习模型，可以揭穿2019冠状病毒疾病假新闻，作为一个案例研究，在出现的初始阶段。该评估使用了一个新收集的包含2019冠状病毒疾病和 non-COVID-19假新闻声明的数据集，表明该模型获得了很高的性能，表明该模型可以在早期阶段用少量数据识别2019冠状病毒疾病上的假新闻。我们认为，我们的方法和调查结果可以用于发现新出现的和重要话题的假新闻，这应该在资源不足的情况下进行。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fighting+against+Fake+News+on+Newly-Emerging+Crisis:+A+Case+Study+of+COVID-19)|0|
|[Targeted Filter Bubbles Mitigating via Edges Insertion](https://doi.org/10.1145/3589335.3651566)|Yanping Wu, Jinghao Wang, Renjie Sun, Chen Chen, Xiaoyang Wang, Ying Zhang||The emergence of filter bubbles leads to various harms. To mitigate filter bubbles, some recent works select the seeds for different viewpoints to minimize the formation of bubbles under the influence propagation model. Different from these works where the diffusion networks remain unchanged, in this paper, we conduct the first attempt to mitigate filter bubbles via edge insertion. Besides, to be more generalized, we focus on mitigating filter bubbles for the given target node set since the audiences can be different for different scenarios. Specifically, we propose the concept of openness score for each target node, which serves as a metric to assess the likelihood of this node being influenced by multiple viewpoints simultaneously. Given a directed graph G, two seed sets, a positive integer k and a target node set, we aim to find k edges incident to the given seeds such that the total openness score is maximized. We prove the NP-hardness of problem studied. A baseline method is first presented by extending the greedy framework. To handle large graphs efficiently, we develop a sampling-based strategy. A data-dependent approximation method is developed with theoretical guarantees. Experiments over real social networks are conducted to demonstrate the advantages of proposed techniques.|过滤气泡的出现导致了各种危害。为了减小滤波器中的气泡，最近的一些工作选择了不同观点的种子，以减少影响传播模型下气泡的形成。与传统的扩散网络不变的情况不同，本文首次尝试通过边缘插入的方法来抑制过滤气泡。此外，由于不同场景的受众可能不同，因此为了更一般化，我们将重点放在减少给定目标节点集的过滤泡上。具体来说，我们提出了每个目标节点的开放度评分的概念，它作为一个度量来评估这个节点同时受到多个视点影响的可能性。给定一个有向图 G、两个种子集、一个正整数 k 和一个目标节点集，我们的目标是找到与给定种子相关的 k 个边，使总开放度得到最大化。证明了所研究问题的 NP- 硬度。通过扩展贪婪框架，提出了一种基线方法。为了有效地处理大型图形，我们提出了一种基于抽样的策略。提出了一种具有理论保证的数据相关近似方法。在真实的社会网络上进行了实验，以证明所提出的技术的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Targeted+Filter+Bubbles+Mitigating+via+Edges+Insertion)|0|
|[Unveiling Wash Trading in Popular NFT Markets](https://doi.org/10.1145/3589335.3651580)|Yuanzheng Niu, Xiaoqi Li, Hongli Peng, Wenkai Li||As emerging digital assets, NFTs are susceptible to anomalous trading behaviors due to the lack of stringent regulatory mechanisms, potentially causing economic losses. In this paper, we conduct the first systematic analysis of four non-fungible tokens (NFT) markets. Specifically, we analyze more than 25 million transactions within these markets, to explore the evolution of wash trade activities. Furthermore, we propose a heuristic algorithm that integrates the network characteristics of transactions with behavioral analysis, to detect wash trading activities in NFT markets. Our findings indicate that NFT markets with incentivized structures exhibit higher proportions of wash trading volume compared to those without incentives. Notably, the LooksRare and X2Y2 markets are detected with wash trading volume proportions as high as 94.5|作为新兴的数字资产，由于缺乏严格的监管机制，非交易所交易容易出现不正常的交易行为，有可能造成经济损失。本文首次对四个非可替代代币市场进行了系统分析。具体来说，我们分析了这些市场中超过2500万笔交易，以探索洗涤贸易活动的演变。此外，我们提出了一个启发式的算法，结合交易的网络特性和行为分析，以检测洗手交易活动在 NFT 市场。我们的研究结果表明，与没有激励措施的市场相比，具有激励结构的 NFT 市场表现出更高的洗涤交易量比例。值得注意的是，LooksREE 和 X2Y2市场的洗涤交易量比例高达94.5|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+Wash+Trading+in+Popular+NFT+Markets)|0|
|[Understanding Deployment Experience of 5G](https://doi.org/10.1145/3589335.3651577)|Ziyi Liu, Huandong Wang, Yong Li||The global rollout of 5G mobile networks has prompted discussions on deployment strategies. Given the knowledge gap in the current deployment strategies of 5G base stations, understanding the deployment experience from regions with widespread 5G base stations is valuable for guiding future deployments elsewhere. In this study, based on a large data set collected from a metropolitan city in China, we discover the misalignment between 5G traffic demand and the number of base stations. Then we introduce a factor to quantify the misalignment. Our analysis indicates the following important observations. Firstly, unique traffic patterns of functional areas contribute to different misalignment factors, i.e., transport areas exhibit a positive factor, in contrast to the negative factor observed in urban comprehensive and residential areas. Secondly, regions with a high density of base stations still suffer from low energy and resource utilization efficiency due to their high energy consumption. Thirdly, our analysis reveals that 5G base stations are frequently located in areas with large 4G traffic, yet the incomplete migration of traffic to 5G results in misalignment. This understanding of the 5G deployment experience can help further studies on optimizing energy efficiency and network utilization rate of the mobile networks.|5G 移动网络的全球铺设促使人们讨论部署战略。鉴于目前5G 基站部署战略中的知识差距，了解拥有广泛的5G 基站的区域的部署经验对于指导今后在其他地方的部署很有价值。在本研究中，我们基于中国某大都市的大量资料，发现5G 流量需求与基站数目之间存在偏差。然后，我们引入一个因素来量化失调。我们的分析表明了以下重要的观察结果。首先，功能区独特的交通模式导致了不同的错位因素，即交通区呈现出积极的因素，而城市综合区和居住区则呈现出消极的因素。其次，基站密度高的地区由于能耗高，能源和资源利用效率低。第三，我们的分析表明，5G 基站经常位于4G 流量大的地区，然而流量向5G 的不完全迁移导致了错位。对5G 部署经验的了解，有助于进一步研究移动网络能量效率和网络利用率的优化问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+Deployment+Experience+of+5G)|0|
|[FaST: Accelerating Web Front-end Data Binding with Compiler and Visible Anchor](https://doi.org/10.1145/3589335.3651505)|Tatsuru Tomizawa, Seiki Makino, Taiga Kume, Satoki Hamanaka, Tadashi Okoshi, Jin Nakazawa||Data binding in web front-end development has made a significant contribution to removing complexity from development and simplifying programming. However, data binding has caused a degradation of website performance at the cost of reducing the burden on programmers. In this paper, we propose Visible Anchor to solve the performance degradation caused by data binding. We develop a compiler called FaST that implements the method. Then, We compared the rendering time among websites built by existing methods and FaST compiler. The evaluation result revealed that the websites built by FaST compiler are at minimum 2.9 times faster to be rendered than the ones built by the existing methods. FaST made a significant contribution to improving the performance of web front-end data binding. Consequently, data binding with FaST can be a better choice for web front-end development.|Web 前端开发中的数据绑定为消除开发的复杂性和简化编程做出了重大贡献。然而，数据绑定导致了网站性能的下降，代价是减轻了程序员的负担。为了解决数据绑定引起的性能下降问题，本文提出了可视化锚。我们开发了一个名为 FaST 的编译器来实现这个方法。然后，比较了现有方法和 FaST 编译器构建的网站的渲染时间。评价结果表明，使用 FaST 编译器构建的网站比使用现有方法构建的网站渲染速度至少快2.9倍。FAST 为提高 Web 前端数据绑定的性能做出了重大贡献。因此，使用 FAST 的数据绑定可以成为 Web 前端开发的更好选择。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FaST:+Accelerating+Web+Front-end+Data+Binding+with+Compiler+and+Visible+Anchor)|0|
|[How We Refute Claims: Automatic Fact-Checking through Flaw Identification and Explanation](https://doi.org/10.1145/3589335.3651521)|WeiYu Kao, AnZi Yen||Automated fact-checking is a crucial task in the governance of internet content. Although various studies utilize advanced models to tackle this issue, a significant gap persists in addressing complex real-world rumors and deceptive claims. To address this challenge, this paper explores the novel task of flaw-oriented fact-checking, including aspect generation and flaw identification. We also introduce RefuteClaim, a new framework designed specifically for this task. Given the absence of an existing dataset, we present FlawCheck, a dataset created by extracting and transforming insights from expert reviews into relevant aspects and identified flaws. The experimental results underscore the efficacy of RefuteClaim, particularly in classifying and elucidating false claims.|自动化事实核查是互联网内容治理中的一项重要任务。虽然各种研究利用先进的模型来解决这一问题，但在处理复杂的现实世界的谣言和欺骗性声明方面仍然存在重大差距。为了应对这一挑战，本文探讨了面向缺陷的事实检查的新任务，包括方面生成和缺陷识别。我们还将介绍 RefuteClaim，这是一个专门为此任务设计的新框架。鉴于缺乏现有的数据集，我们提出 FlawCheck，一个数据集创建的提取和转换的见解从专家评论到相关方面和确定的缺陷。实验结果强调了 RefuteClaim 的有效性，特别是在分类和阐明虚假声明方面。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+We+Refute+Claims:+Automatic+Fact-Checking+through+Flaw+Identification+and+Explanation)|0|
|[Characterizing the Solana NFT Ecosystem](https://doi.org/10.1145/3589335.3651478)|Dechao Kong, Xiaoqi Li, Wenkai Li||Non-Fungible Tokens (NFTs) are digital assets recorded on the blockchain, providing cryptographic proof of ownership over digital or physical items. Although Solana has only begun to gain popularity in recent years, its NFT market has seen substantial transaction volumes. In this paper, we conduct the first systematic research on the characteristics of Solana NFTs from two perspectives: longitudinal measurement and wash trading security audit. We gathered 132,736 Solana NFT from Solscan and analyzed the sales data within these collections. Investigating users' economic activity and NFT owner information reveals that the top users in Solana NFT are skewed toward a higher distribution of purchases. Subsequently, we employ the Local Outlier Factor algorithm to conduct a wash trading audit on 2,175 popular Solana NFTs. We discovered that 138 NFT pools are involved in wash trading, with 8 of these NFTs having a wash trading rate exceeding 50 have been entirely washed out.|非可替换令牌(NFT)是记录在区块链上的数字资产，提供对数字或物理项的所有权的加密证明。尽管索拉纳近年来才开始受到欢迎，但其 NFT 市场的交易量相当可观。本文首次从纵向计量和洗牌交易安全审计两个角度对索拉纳非交易安全审计的特点进行了系统的研究。我们从 Solscan 收集了132,736份索拉纳 NFT，并分析了其中的销售数据。调查用户的经济活动和 NFT 所有者信息显示，索拉纳 NFT 的顶级用户倾向于更高的购买分布。随后，我们使用局部异常因子算法对2,175个流行的索拉纳非交易系统进行了清洗交易审计。我们发现有138个 NFT 池参与了洗牌交易，其中8个洗牌交易率超过50的 NFT 池已经完全被洗掉了。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Characterizing+the+Solana+NFT+Ecosystem)|0|
|[Multi-round Counterfactual Generation: Interpreting and Improving Models of Text Classification](https://doi.org/10.1145/3589335.3651537)|Huajie Zhang, Yuxin Ying, Fuzhen Zhuang, Haiqin Weng, Sun Ying, Zhao Zhang, Yiqi Tong, Yan Liu||In recent years, natural language processing (NLP) models have demonstrated remarkable performance in text classification tasks. However, trust in the decision-making process requires a deeper understanding of the operational principles of these networks. Therefore, there is an urgent need to enhance transparency and the interpretability of these "black boxes". Aligned with this, we propose a model-agnostic interpretability method named MCG. This method generates counterfactual interpretations that are more faithful to the original models' performance through a multi-round dialogue, in which a new template is generated based on the evaluation of the previous counterfactual interpretation. In addition, MCG proposes a solution to improve model performance through counterfactual data augmentation for cases where the model to be interpreted is misclassified, which is rarely covered by existing counterfactual methods. Extensive experiments on three datasets demonstrate that our MCG outperforms current state-of-the-art methods in counterfactual generation for interpretability.|近年来，自然语言处理(NLP)模型在文本分类任务中表现出了显著的性能。然而，对决策过程的信任需要对这些网络的操作原则有更深入的理解。因此，迫切需要提高这些“黑匣子”的透明度和可解释性。与此相对应，我们提出了一种模型无关的可解释性方法 MCG。这种方法通过多轮对话产生更加忠实于原有模型表现的反事实解释，在此过程中，基于对前人反事实解释的评价产生新的模板。此外，MCG 提出了一种解决方案，通过反事实数据增强来改善模型的性能，以解释模型被错误分类的情况，现有的反事实方法很少涵盖这种情况。在三个数据集上的大量实验表明，我们的 MCG 在可解释性的反事实生成方面优于目前最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-round+Counterfactual+Generation:+Interpreting+and+Improving+Models+of+Text+Classification)|0|
|[iSpLib: A Library for Accelerating Graph Neural Networks using Auto-tuned Sparse Operations](https://doi.org/10.1145/3589335.3651528)|Md Saidul Hoque Anik, Pranav Badhe, Rohit Gampa, Ariful Azad||Core computations in Graph Neural Network (GNN) training and inference are often mapped to sparse matrix operations such as sparse-dense matrix multiplication (SpMM). These sparse operations are harder to optimize by manual tuning because their performance depends significantly on the sparsity of input graphs, GNN models, and computing platforms. To address this challenge, we present iSpLib, a PyTorch-based C++ library equipped with auto-tuned sparse operations. iSpLib expedites GNN training with a cache-enabled backpropagation that stores intermediate matrices in local caches. The library offers a user-friendly Python plug-in that allows users to take advantage of our optimized PyTorch operations out-of-the-box for any existing linear algebra-based PyTorch implementation of popular GNNs (Graph Convolution Network, GraphSAGE, Graph Inference Network, etc.) with only two lines of additional code. We demonstrate that iSpLib obtains up to 27x overall training speedup compared to the equivalent PyTorch 2.1.0 and PyTorch Geometric 2.4.0 implementations on the CPU. Our library is publicly available at https://github.com/HipGraph/iSpLib (https://doi.org/10.5281/zenodo.10806511).|图形神经网络(GNN)训练和推理中的核心计算通常映射到稀疏矩阵操作，例如稀疏密集矩阵乘法(spMM)。这些稀疏操作很难通过手动调优进行优化，因为它们的性能在很大程度上取决于输入图、 GNN 模型和计算平台的稀疏性。为了解决这个问题，我们展示了 iSpLib，这是一个基于 PyTorch 的 C + + 库，配备了自动调优的稀疏操作。ISpLib 使用支持缓存的反向传播加快 GNN 培训，该反向传播将中间矩阵存储在本地缓存中。这个库提供了一个用户友好的 Python 插件，允许用户利用我们优化的 PyTorch 操作，对于流行的 GNN (图形卷积网络，GraphSAGE，图形推理网络等)的任何现有的基于线性代数的 PyTorch 实现，只需要两行额外的代码。我们演示了 iSpLib 与 CPU 上等效的 PyTorch 2.1.0和 PyTorch Geometer 2.4.0实现相比，获得了高达27倍的整体训练加速。我们的图书馆可在 https://github.com/hipgraph/isplib ( https://doi.org/10.5281/zenodo.10806511)使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=iSpLib:+A+Library+for+Accelerating+Graph+Neural+Networks+using+Auto-tuned+Sparse+Operations)|0|
|[DeFiTail: DeFi Protocol Inspection through Cross-Contract Execution Analysis](https://doi.org/10.1145/3589335.3651488)|Wenkai Li, Xiaoqi Li, Yuqing Zhang, Zongwei Li||Decentralized finance (DeFi) protocols are crypto projects developed on the blockchain to manage digital assets. Attacks on DeFi have been frequent and have resulted in losses exceeding $77 billion. However, detection methods for malicious DeFi events are still lacking. In this paper, we propose DeFiTail, the first framework that utilizes deep learning to detect access control and flash loan exploits that may occur on DeFi. Since the DeFi protocol events involve invocations with multi-account transactions, which requires execution path unification with different contracts. Moreover, to mitigate the impact of mistakes in Control Flow Graph (CFG) connections, we validate the data path by employing the symbolic execution stack. Furthermore, we feed the data paths through our model to achieve the inspection of DeFi protocols. Experimental results indicate that DeFiTail achieves the highest accuracy, with 98.39 access control and 97.43 enhanced capability to detect malicious contracts, identifying 86.67 from the CVE dataset.|分散财务(DeFi)协议是在区块链上开发的用于管理数字资产的加密项目。对 DeFi 的攻击频繁发生，造成的损失超过770亿美元。然而，仍然缺乏针对恶意 DeFi 事件的检测方法。在本文中，我们提出了 DeFiTail，这是第一个利用深度学习来检测 DeFi 上可能发生的访问控制和 flash 贷款漏洞的框架。由于 DeFi 协议事件涉及对多帐户事务的调用，这就要求使用不同的契约统一执行路径。此外，为了减轻控制流程图连接错误的影响，我们使用符号执行堆栈来验证数据路径。此外，我们通过我们的模型提供数据路径来实现对 DeFi 协议的检测。实验结果表明，DeFiTail 获得了最高的准确率，具有98.39的访问控制和97.43的增强能力来检测恶意合同，从 CVE 数据集识别86.67。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DeFiTail:+DeFi+Protocol+Inspection+through+Cross-Contract+Execution+Analysis)|0|
|[Are we Making Much Progress? Revisiting Chemical Reaction Yield Prediction from an Imbalanced Regression Perspective](https://doi.org/10.1145/3589335.3651470)|Yihong Ma, Xiaobao Huang, Bozhao Nan, Nuno Moniz, Xiangliang Zhang, Olaf Wiest, Nitesh V. Chawla||The yield of a chemical reaction quantifies the percentage of the target product formed in relation to the reactants consumed during the chemical reaction. Accurate yield prediction can guide chemists toward selecting high-yield reactions during synthesis planning, offering valuable insights before dedicating time and resources to wet lab experiments. While recent advancements in yield prediction have led to overall performance improvement across the entire yield range, an open challenge remains in enhancing predictions for high-yield reactions, which are of greater concern to chemists. In this paper, we argue that the performance gap in high-yield predictions results from the imbalanced distribution of real-world data skewed towards low-yield reactions, often due to unreacted starting materials and inherent ambiguities in the reaction processes. Despite this data imbalance, existing yield prediction methods continue to treat different yield ranges equally, assuming a balanced training distribution. Through extensive experiments on three real-world yield prediction datasets, we emphasize the urgent need to reframe reaction yield prediction as an imbalanced regression problem. Finally, we demonstrate that incorporating simple cost-sensitive re-weighting methods can significantly enhance the performance of yield prediction models on underrepresented high-yield regions.|化学反应的产率量化了与化学反应期间消耗的反应物相关的目标产物的百分比。准确的产率预测可以指导化学家在合成计划中选择高产率的反应，在投入时间和资源进行湿实验之前提供有价值的见解。虽然最近在产率预测方面的进展已导致在整个产率范围内的总体性能改善，但在提高高产率反应的预测方面仍然存在一个公开的挑战，这是化学家们更关心的问题。在本文中，我们认为，高产率预测中的性能差距是由于现实世界数据的不平衡分布倾向于低产率反应，往往是由于未反应的起始材料和反应过程中固有的模糊性。尽管数据不平衡，现有的产量预测方法继续平等对待不同的产量范围，假设均衡的训练分布。通过对三个实际产量预测数据集的大量实验，我们强调了将反应产量预测重构为一个不平衡回归问题的迫切性。最后，我们证明，加入简单的成本敏感的重新加权方法可以显著提高产量预测模型在低代表性的高产地区的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Are+we+Making+Much+Progress?+Revisiting+Chemical+Reaction+Yield+Prediction+from+an+Imbalanced+Regression+Perspective)|0|
|[Simple Multigraph Convolution Networks](https://doi.org/10.1145/3589335.3651560)|Danyang Wu, Xinjie Shen, Jitao Lu, Jin Xu, Feiping Nie||Existing multigraph convolution methods either ignore the cross-view interaction among multiple graphs, or induce extremely high computational cost due to standard cross-view polynomial operators. To alleviate this problem, this paper proposes a Simple MultiGraph Convolution Networks (SMGCN) which first extracts consistent cross-view topology from multigraphs including edge-level and subgraph-level topology, then performs polynomial expansion based on raw multigraphs and consistent topologies. In theory, SMGCN utilizes the consistent topologies in polynomial expansion rather than standard cross-view polynomial expansion, which performs credible cross-view spatial message-passing, follows the spectral convolution paradigm, and effectively reduces the complexity of standard polynomial expansion. In the simulations, experimental results demonstrate that SMGCN achieves state-of-the-art performance on ACM and DBLP multigraph benchmark datasets. Our codes are available at https://github.com/frinkleko/SMGCN.|现有的多图卷积方法要么忽略多图之间的交叉视图交互作用，要么由于标准的交叉视图多项式算子而导致极高的计算代价。针对这一问题，提出了一种简单的多图卷积网络(SMGCN)。理论上，SMGCN 利用多项式展开中的一致性拓扑，而非标准的横视多项式展开，实现了可靠的横视空间信息传递，遵循谱卷积范式，有效地降低了标准多项式展开的复杂性。仿真实验结果表明，SMGCN 在 ACM 和 DBLP 多图基准数据集上取得了较好的性能。我们的密码可以在 https://github.com/frinkleko/smgcn 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Simple+Multigraph+Convolution+Networks)|0|
|[Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks](https://doi.org/10.1145/3589335.3651555)|Yichang Xu, Ming Yin, Minghong Fang, Neil Zhenqiang Gong||Recent studies have revealed that federated learning (FL), once considered secure due to clients not sharing their private data with the server, is vulnerable to attacks such as client-side training data distribution inference, where a malicious client can recreate the victim's data. While various countermeasures exist, they are not practical, often assuming server access to some training data or knowledge of label distribution before the attack. In this work, we bridge the gap by proposing InferGuard, a novel Byzantine-robust aggregation rule aimed at defending against client-side training data distribution inference attacks. In our proposed InferGuard, the server first calculates the coordinate-wise median of all the model updates it receives. A client's model update is considered malicious if it significantly deviates from the computed median update. We conduct a thorough evaluation of our proposed InferGuard on five benchmark datasets and perform a comparison with ten baseline methods. The results of our experiments indicate that our defense mechanism is highly effective in protecting against client-side training data distribution inference attacks, even against strong adaptive attacks. Furthermore, our method substantially outperforms the baseline methods in various practical FL scenarios.|最近的研究表明，联邦学习(FL) ，一度被认为是安全的，因为客户端不与服务器共享他们的私人数据，容易受到攻击，如客户端训练数据分布推断，其中一个恶意的客户端可以重建受害者的数据。虽然存在各种对策，但它们并不实用，通常假设服务器在攻击之前访问一些训练数据或标签分发的知识。在这项工作中，我们通过提出一种新的拜占庭-鲁棒聚合规则，旨在防御客户端训练数据分布推理攻击，从而弥补了差距。在我们提出的“狱警”中，服务器首先计算它接收到的所有模型更新的坐标中值。如果客户端的模型更新严重偏离计算出的中位数更新，则该更新被认为是恶意的。我们进行了一个彻底的评估，我们提出了五个基准数据集的推论，并与十个基准方法进行了比较。实验结果表明，该防御机制能够有效地抵御客户端的训练数据分布推理攻击，甚至能够抵御强自适应攻击。此外，我们的方法在各种实际 FL 场景中大大优于基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Federated+Learning+Mitigates+Client-side+Training+Data+Distribution+Inference+Attacks)|0|
|[Group-wise K-anonymity meets (ε, δ) Differentially Privacy Scheme](https://doi.org/10.1145/3589335.3651517)|Kenneth Odoh||We studied the link between K-anonymity and differential privacy as the basis for deriving a novel method for noise estimation. Hence, we provide threefold contributions: First, we use the birthday-bound paradox for uniqueness to estimate the noise level, ε in (ε, δ) differentially privacy scheme. Second, our group-aware formulation provides resilience to a series of inference attacks by using the group privacy property in our unique group-centric formulation. Third, draw a connection between the attacker advantage, δ, and ε for univariate and multivariate cases. Finally, we demonstrate applicability in Laplacian, Gaussian, and Exponential mechanisms.|我们研究了 k 匿名性和差分隐私之间的联系，作为推导噪声估计新方法的基础。因此，我们提供了三方面的贡献: 首先，我们使用唯一性的生日定界悖论来估计噪声水平，ε 在(ε，δ)差分隐私格式。其次，我们的群体意识公式通过在我们独特的以群体为中心的公式中使用群体隐私属性来提供对一系列推理攻击的弹性。第三，在单变量和多变量情况下建立攻击者优势、 δ 和 ε 之间的联系。最后，我们展示了 Laplacian、高斯和指数机制的适用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Group-wise+K-anonymity+meets+(ε,+δ)+Differentially+Privacy+Scheme)|0|
|[Generating Privacy-preserving Educational Data Records with Diffusion Model](https://doi.org/10.1145/3589335.3651511)|Quanlong Guan, Yanchong Yu, Xiujie Huang, Liangda Fang, Chaobo He, Lusheng Wu, Weiqi Luo, Guanliang Chen||Educational Data Records (EDR) are crucial for capturing teaching behavior and student information, forming the basis for achieving educational intelligence. However, ensuring educational privacy has become a pressing concern, posing practical challenges to the use and sharing of educational data. To address the issue of EDR privacy preserving, we present EduSyn, a privacy data release scheme that utilizes generative diffusion models and differential privacy methods. Specifically, we adopt a diffusion modeling scheme that can be applied to both discrete and continuous types of data to accommodate the data characteristics of EDR, while an invariant Post Randomization (PRAM) perturbation method that satisfies local differential privacy is applied for data attributes that need to be specially protected before model training. We conduct comprehensive validation of this scheme within the domain of education applications, showcasing that EduSyn generates a superior private EDR dataset compared to similar generative methods and strikes a better privacy-utility trade-off.|教育数据记录(EDR)是获取教学行为和学生信息的关键，是实现教育智能的基础。然而，确保教育隐私已成为一个迫切的关注，对教育数据的使用和共享提出了实际的挑战。为了解决 EDR 隐私保护问题，我们提出了 EduSyn，一个利用生成扩散模型和差分隐私方法的隐私数据发布方案。具体来说，我们采用了一种扩散建模方案，可以应用于离散和连续类型的数据，以适应 EDR 的数据特征，而不变的后随机化(pRAM)扰动方法，满足局部差分隐私的数据属性，需要特别保护之前的模型训练。我们在教育应用领域对该方案进行了全面验证，表明与类似的生成方法相比，EduSyn 生成了一个更好的私有 EDR 数据集，并取得了更好的隐私-效用权衡。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generating+Privacy-preserving+Educational+Data+Records+with+Diffusion+Model)|0|
|[StateGuard: Detecting State Derailment Defects in Decentralized Exchange Smart Contract](https://doi.org/10.1145/3589335.3651562)|Zongwei Li, Wenkai Li, Xiaoqi Li, Yuqing Zhang||Decentralized Exchanges (DEXs), leveraging blockchain technology and smart contracts, have emerged in decentralized finance. However, the DEX project with multi-contract interaction is accompanied by complex state logic, which makes it challenging to solve state defects. In this paper, we conduct the first systematic study on state derailment defects of DEXs. These defects could lead to incorrect, incomplete, or unauthorized changes to the system state during contract execution, potentially causing security threats. We propose StateGuard, a deep learning-based framework to detect state derailment defects in DEX smart contracts. StateGuard constructs an Abstract Syntax Tree (AST) of the smart contract, extracting key features to generate a graph representation. Then, it leverages a Graph Convolutional Network (GCN) to discover defects. Evaluating StateGuard on 46 DEX projects with 5,671 smart contracts reveals its effectiveness, with a precision of 92.24 we used StateGuard to audit real-world smart contracts and successfully authenticated multiple novel CVEs.|分散交易所(DEX) ，利用区块链技术和智能合同，已经出现在分散金融。然而，多契约交互的 DEX 项目伴随着复杂的状态逻辑，这使得解决状态缺陷变得非常困难。本文首次对 DEX 的状态脱轨缺陷进行了系统的研究。这些缺陷可能导致在合同执行期间对系统状态进行不正确、不完整或未经授权的更改，从而潜在地造成安全威胁。我们提出 StateGuard，一个基于深度学习的框架，用于检测 DEX 智能合同中的状态脱轨缺陷。StateGuard 构建智能契约的抽象语法树(AST) ，提取关键特性来生成图表。然后，它利用图卷积网络(GCN)来发现缺陷。对46个 DEX 项目中5,671个智能合同的 StateGuard 进行评估显示了其有效性，精度为92.24，我们使用 StateGuard 来审计现实世界中的智能合同，并成功地验证了多个新的 CVE。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=StateGuard:+Detecting+State+Derailment+Defects+in+Decentralized+Exchange+Smart+Contract)|0|
|[Rumor Mitigation in Social Media Platforms with Deep Reinforcement Learning](https://doi.org/10.1145/3589335.3651556)|Hongyuan Su, Yu Zheng, Jingtao Ding, Depeng Jin, Yong Li||Social media platforms have become one of the main channels where people disseminate and acquire information, of which the reliability is severely threatened by rumors widespread in the network. Existing approaches such as suspending users or broadcasting real information to combat rumors are either with high cost or disturbing users. In this paper, we introduce a novel rumor mitigation paradigm, where only a minimal set of links in the social network are intervened to decelerate the propagation of rumors, countering misinformation with low business cost and user awareness. A knowledge-informed agent embodying rumor propagation mechanisms is developed, which intervenes the social network with a graph neural network for capturing information flow in the social media platforms and a policy network for selecting links. Experiments on real social media platforms demonstrate that the proposed approach can effectively alleviate the influence of rumors, substantially reducing the affected populations by over 25 released at https://github.com/tsinghua-fib-lab/DRL-Rumor-Mitigation.|社交媒体平台已成为人们传播和获取信息的主要渠道之一，其可靠性受到网络谣言的严重威胁。现有的方法，如暂停用户或广播真实信息，以打击谣言，要么成本高，要么干扰用户。本文提出了一种新的谣言消除方法，该方法通过干预社交网络中的最小链接集来减缓谣言的传播速度，以较低的商业成本和用户意识来对抗虚假信息。本文提出了一种包含谣言传播机制的知识型智能体，该智能体使用图形神经网络对社交网络进行干预，以获取社交媒体平台中的信息流，并使用策略网络进行链接选择。在真实的社交媒体平台上进行的实验表明，提出的方法可以有效地减轻谣言的影响，大大减少了受影响的人口超过25 https://github.com/tsinghua-fib-lab/drl-rumor-mitigation。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rumor+Mitigation+in+Social+Media+Platforms+with+Deep+Reinforcement+Learning)|0|
|[SWATTING Spambots: Real-time Detection of Malicious Bots on X](https://doi.org/10.1145/3589335.3651564)|Cristian Brokate, Manon Richard, Lisa Giordani, Jean Liénard||Spambot activity has become increasingly pervasive on social media platforms, such as X (formerly known as Twitter), leading to concerns over information quality and user experience. This study presents an innovative approach for real-time detection and reporting of spambots on Twitter platform. Using data analytics technique, we adapted a comprehensive framework capable of accurately identifying and categorizing spambot accounts based on their behavioral patterns and characteristics. By providing an efficient solution to this growing issue, our research aims to enhance user trust in social media communication channels and promote a more transparent and authentic online environment for users to engage with each other and share information.|垃圾邮件机器人(Spambot)活动已经越来越普遍地出现在社交媒体平台上，比如 X (以前称为 Twitter) ，这导致了对信息质量和用户体验的担忧。该研究提出了一种在 Twitter 平台上实时检测和报告垃圾邮件机器人的创新方法。利用数据分析技术，我们采用了一个全面的框架，能够准确地识别和分类垃圾邮件机器人帐户根据其行为模式和特点。通过为这一日益严重的问题提供一个有效的解决方案，我们的研究旨在增强用户对社交媒体沟通渠道的信任，促进一个更加透明和真实的网络环境，让用户相互交流和分享信息。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SWATTING+Spambots:+Real-time+Detection+of+Malicious+Bots+on+X)|0|
|[One-shot Pairing and Authentication Using Moms Secret](https://doi.org/10.1145/3589335.3651542)|Ubaid Ur Rehman, Sungyoung Lee||The existing pairing and authentication mechanisms adopt either fuzzy commitment or fuzzy password-authenticated key exchange for device fingerprint generation, detecting and correcting multiple symbol errors, leading to guessing attacks and increased pairing time. In this study, we propose a one-shot pairing and authentication approach that generates a device fingerprint from the selected contextual data using Median-of-medians (Moms), ensuring randomness and preventing guessing attacks. Moreover, we integrate the Moms secret into Password Authenticated Key Exchange (PAKE) to reduce the pairing time and improve security. The evaluation demonstrates that our proposed one-shot pairing and authentication approach ensures strong resistance against information gain, reduces the probability of guessing attacks, and significantly decreases the pairing time compared to state-of-the-art approaches.|现有的配对和认证机制采用模糊承诺或模糊口令认证密钥交换来生成设备指纹，检测和纠正多个符号错误，从而导致猜测攻击和增加配对时间。在这项研究中，我们提出了一个一次性配对和认证方法，从选定的上下文数据生成设备指纹使用中位数(Moms) ，确保随机性和防止猜测攻击。此外，我们还将 Moms 密钥集成到密码认证密钥交换(PAKE)中，以减少配对时间，提高安全性。评估表明，我们提出的一次性配对和认证方法确保了对信息增益的强大抵抗力，降低了猜测攻击的概率，并显著减少了配对时间相比，最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=One-shot+Pairing+and+Authentication+Using+Moms+Secret)|0|
|[GPT-generated Text Detection: Benchmark Dataset and Tensor-based Detection Method](https://doi.org/10.1145/3589335.3651513)|Zubair Qazi, William Shiao, Evangelos E. Papalexakis||As natural language models like ChatGPT become increasingly prevalent in applications and services, the need for robust and accurate methods to detect their output is of paramount importance. In this paper, we present GPT Reddit Dataset (GRiD), a novel Generative Pretrained Transformer (GPT)-generated text detection dataset designed to assess the performance of detection models in identifying generated responses from ChatGPT. The dataset consists of a diverse collection of context-prompt pairs based on Reddit, with human-generated and ChatGPT-generated responses. We provide an analysis of the dataset's characteristics, including linguistic diversity, context complexity, and response quality. To showcase the dataset's utility, we benchmark several detection methods on it, demonstrating their efficacy in distinguishing between human and ChatGPT-generated responses. This dataset serves as a resource for evaluating and advancing detection techniques in the context of ChatGPT and contributes to the ongoing efforts to ensure responsible and trustworthy AI-driven communication on the internet. Finally, we propose GpTen, a novel tensor-based GPT text detection method that is semi-supervised in nature since it only has access to human-generated text and performs on par with fully-supervised baselines.|随着像 ChatGPT 这样的自然语言模型在应用程序和服务中越来越普遍，需要健壮和准确的方法来检测它们的输出是至关重要的。在本文中，我们提出了 GPT Reddit 数据集(GRiD) ，一个新的生成预训练变压器(GPT)生成的文本检测数据集，旨在评估检测模型在识别 ChatGPT 生成的响应方面的性能。该数据集由基于 Reddit 的上下文提示对的不同集合组成，其中包含人工生成的响应和 ChatGPT 生成的响应。我们提供了一个数据集的特征分析，包括语言多样性，上下文复杂性和响应质量。为了展示数据集的实用性，我们对其进行了几种检测方法的基准测试，证明了它们在区分人类和 ChatGPT 产生的反应方面的有效性。该数据集作为评估和推进 ChatGPT 背景下的检测技术的资源，并有助于正在进行的努力，以确保负责任和可信赖的人工智能驱动的互联网通信。最后，我们提出了 GpTen，一种新的基于张量的 GPT 文本检测方法，它是半监督性质的，因为它只能访问人类生成的文本，性能与全监督基线相当。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GPT-generated+Text+Detection:+Benchmark+Dataset+and+Tensor-based+Detection+Method)|0|
|[Knowledge Guided Conditional Diffusion Model for Controllable Mobile Traffic Generation](https://doi.org/10.1145/3589335.3651530)|Haoye Chai, Tong Li, Fenyu Jiang, Shiyuan Zhang, Yong Li||Generating mobile traffic in urban contexts is important for network optimization. However, existing solutions show weakness in capturing complex temporal features of mobile traffic. In this paper, we propose a Knowledge-Guided Conditional Diffusion model (KGDiff) for controllable mobile traffic generation, where a customized denoising network of diffusion model is designed to explore the temporal features of mobile traffic. Specifically, we design a frequency attention mechanism that incorporates an Urban Knowledge Graph (UKG) to adaptively capture implicit correlations between mobile traffic and urban environments in the frequency domain. This approach enables the model to generate network traffic corresponding to different environments in a controlled manner, enhancing the model's controllability. Experiments on one real-world dataset show that the proposed framework has good controllability and can improve generation fidelity with gains surpassing 19%.|在城市环境下生成移动交通对于网络优化具有重要意义。然而，现有的解决方案在捕获移动业务复杂的时间特征方面存在不足。本文提出了一种基于知识引导的条件扩散模型(KGDiff)用于可控的移动流量生成，设计了一个定制的扩散模型去噪网络来探索移动流量的时间特性。具体来说，我们设计了一个频率注意机制，它结合了城市知识图(UKG)来自适应地捕捉移动交通与城市环境之间在频率域中的隐含相关性。该方法使模型能够以受控的方式生成与不同环境相对应的网络流量，增强了模型的可控性。在一个实际数据集上的实验表明，该框架具有良好的可控性，能够提高产生保真度，增益超过19% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Guided+Conditional+Diffusion+Model+for+Controllable+Mobile+Traffic+Generation)|0|
|[Critical Nodes Detection: Node Merging Approach](https://doi.org/10.1145/3589335.3651485)|Hongbo Qiu, Renjie Sun, Chen Chen, Xiaoyang Wang, Ying Zhang||Various cohesive models are widely employed for the analysis of social networks to identify critical users or key relationships, with the k-core being a particularly popular approach. Existing works, such as the anchor k-core problem, aim to maximize k-core by anchoring nodes (the degree of anchor nodes are set as infinity). However, we find that node merging can also enlarge the k-core size. Different from anchoring nodes, nodes merging can cause both degree increase and decrease which brings more challenges. In this paper, we study the <u>c</u>ore <u>m</u>aximization by <u>n</u>ode <u>m</u>erging problem (CMNM) and prove its hardness. A greedy framework is first presented due to its hardness. To scale for large networks, we categorize potentially influential nodes and provide a detailed analysis of all node merging pairs. Then, based on these analyses, a fast and effective algorithm is developed. Finally, we conduct comprehensive experiments on real-world networks to evaluate the effectiveness and efficiency of the proposed method.|各种内聚模型被广泛应用于社会网络分析，以确定关键用户或关键关系，其中 k 核是一种特别流行的方法。现有的工作，如锚 k 核问题，旨在最大化 k 核的锚节点(锚节点的度设置为无穷大)。然而，我们发现节点合并也可以扩大 k- 核的大小。与锚定节点不同，节点融合会引起节点度的增减，从而带来更多的挑战。本文研究了用 < u > n </u > ode < u > m </u > 冒出问题(CMNM)进行的 < u > c </u > 矿石 < u > m </u > 氧化，并证明了其硬度。贪婪的框架首先由于其硬度而被提出。为了适应大型网络，我们对具有潜在影响力的节点进行了分类，并对所有节点合并对进行了详细的分析。在此基础上，提出了一种快速有效的算法。最后，我们在实际网络上进行了综合实验，以评估该方法的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Critical+Nodes+Detection:+Node+Merging+Approach)|0|
|[Dual-level Hypergraph Contrastive Learning with Adaptive Temperature Enhancement](https://doi.org/10.1145/3589335.3651493)|Yiyue Qian, Tianyi Ma, Chuxu Zhang, Yanfang Ye||Inspired by the success of graph contrastive learning, researchers have begun exploring the benefits of contrastive learning over hypergraphs. However, these works have the following limitations in modeling the high-order relationships over unlabeled data: (i) They primarily focus on maximizing the agreements among individual node embeddings while neglecting the capture of group-wise collective behaviors within hypergraphs; (ii) Most of them disregard the importance of the temperature index in discriminating contrastive pairs during contrast optimization. To address these limitations, we propose a novel dual-level Hy perG raph C ontrastive L earning framework with Ad aptive T emperature (HyGCL-AdT ) to boost contrastive learning over hypergraphs. Specifically, unlike most works that merely maximize the agreement of node embeddings in hypergraphs, we propose a dual-level contrast mechanism that not only captures the individual node behaviors in a local context but also models the group-wise collective behaviors of nodes within hyperedges from a community perspective. Besides, we design an adaptive temperature-enhanced contrastive optimization to improve the discrimination ability between contrastive pairs. Empirical experiments conducted on seven benchmark hypergraphs demonstrate that HyGCL-AdT exhibits excellent effectiveness compared to state-of-the-art baseline models. The source code is available at \hrefhttps://github.com/graphprojects/HyGCL-AdT https://github.com/graphprojects/HyGCL-AdT.|受到图形对比学习成功的启发，研究人员开始探索对比学习优于超图的好处。然而，这些工作在未标记数据的高阶关系建模方面有以下局限性: (i)他们主要关注于最大化个体节点嵌入之间的一致性，而忽略了超图中群体行为的捕获; (ii)大多数忽视了温度指数在对比度优化过程中区分对比对的重要性。为了解决这些局限性，我们提出了一种新的具有自适应 T 温度(HyGCL-AdT)的双层 HyperG 图 C 对比 L 学习框架，以促进超图的对比学习。具体而言，与仅仅最大化超图中节点嵌入的一致性的大多数作品不同，我们提出了一种双层对比机制，它不仅捕获局部上下文中的单个节点行为，而且从社区角度模拟超边缘内节点的群体集体行为。此外，我们设计了一个自适应温度增强的对比度优化，以提高对比度对之间的鉴别能力。在七个基准超图上进行的实证实验表明，与最先进的基准模型相比，HyGCL-AdT 具有优异的有效性。源代码可以在 hrefhttps:// github.com/graphprojects/hygcl-adt  https://github.com/graphprojects/hygcl-adt 获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual-level+Hypergraph+Contrastive+Learning+with+Adaptive+Temperature+Enhancement)|0|
|[Towards Understanding Crypto-Asset Risks on Ethereum Caused by Key Leakage on the Internet](https://doi.org/10.1145/3589335.3651573)|Yuxuan Zhou, Jiaqi Chen, Yibo Wang, Yuzhe Tang, Guofei Gu||In public blockchains, leaking secret keys can cause the permanent loss of crypto assets. It is imperative to understand the illicit activities on blockchains related to leaked keys. This paper presents the first measurement study that uncovers, quantifies, and characterizes the actual misuses of the leaked keys from top websites on the Internet to withdraw assets on Ethereum. By finding key-leaking web pages and joining them with transactions, the study reveals 7.29*10^6/0.59*10^6 USD worth of assets on Ethereum mainnet/Binance Smart Chain (BSC) are withdrawn from 1421/1514 leaked secret keys. Mitigations are proposed to avoid the financial loss caused by leaked keys.|在公共区块链中，泄露秘密密钥可能导致加密资产的永久性损失。必须了解与泄漏密钥有关的区块链上的非法活动。本文提出了第一个衡量研究，揭示，量化和特征的泄漏密钥实际滥用互联网上的顶级网站提取资产在以太网上。通过发现泄密网页并将其与交易联系起来，研究显示以太网/比宁智能链(BSC)上价值7.29 * 10 ^ 6/0.59 * 10 ^ 6美元的资产从1421/1514泄密密钥中撤出。建议采取缓解措施，以避免密钥泄漏造成的财务损失。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Understanding+Crypto-Asset+Risks+on+Ethereum+Caused+by+Key+Leakage+on+the+Internet)|0|
|[Turning A Curse into A Blessing: Data-Aware Memory-Efficient Training of Graph Neural Networks by Dynamic Exiting](https://doi.org/10.1145/3589335.3651575)|Yan Han, Kaiqi Chen, Shan Li, Ji Yan, Baoxu Shi, Lei Zhang, Fei Chen, Jaewon Yang, Yunpeng Xu, Xiaoqiang Luo, Qi He, Ying Ding, Zhangyang Wang||Training Graph Neural Networks (GNNs) efficiently remains a challenge due to the high memory demands, especially during recursive neighborhood aggregation. Traditional sampling-based GNN training methods often overlook the data's inherent structure, such as the power-law distribution observed in most real-world graphs, which results in inefficient memory usage and processing. We introduce a novel framework, M emory-A ware D ynamic E xiting GNN (MADE-GNN )), which capitalizes on the power-law nature of graph data to enhance training efficiency. MADE-GNN is designed to be data-aware, dynamically adjusting the depth of feature aggregation based on the connectivity of each node. Specifically, it routes well-connected "head'' nodes through extensive aggregation while allowing sparsely connected "tail'' nodes to exit early, thus reducing memory consumption without sacrificing model performance. This approach not only addresses the challenge of memory-intensive GNN training but also turns the power-law distribution from a traditional "curse'' into a strategic "blessing''. By enabling partial weight sharing between the early-exit mechanism and the full model, MADE-GNN effectively improves the representation of cold-start nodes, leveraging the structural information from head nodes to enhance generalization across the network. Our extensive evaluations across multiple public benchmarks, including industrial-level graphs, show that MADE-GNN outperforms existing GNN training methods in both memory efficiency and performance, offering significant improvements particularly for tail nodes. This demonstrates MADE-GNN's potential as a versatile solution for GNN applications facing similar scalability and distribution challenges.|图形神经网络(GNN)的高效训练仍然是一个挑战，由于高内存需求，特别是在递归邻域聚合。传统的基于抽样的 GNN 训练方法往往忽视了数据的固有结构，如在大多数真实世界图中观察到的幂律分布，导致内存使用和处理效率低下。本文介绍了一种新的训练框架 M memory-A ware D Dynamic E Xiting GNN (MADE-GNN) ，它利用图数据的幂律性质来提高训练效率。GNN 被设计成数据感知的，根据每个节点的连通性动态调整特征聚合的深度。具体来说，它通过广泛的聚合来路由连接良好的“头”节点，同时允许稀疏连接的“尾”节点提前退出，从而在不牺牲模型性能的情况下降低内存消耗。这种方法不仅解决了内存密集型 GNN 训练的挑战，而且将幂律分布从传统的“诅咒”转化为战略上的“祝福”。通过在早退机制和完整模型之间实现部分权重共享，MADE-GNN 有效地改善了冷启动节点的表示，利用头部节点的结构信息增强了整个网络的泛化能力。我们对包括工业级图表在内的多个公共基准进行了广泛的评估，结果表明 MADE-GNN 在内存效率和性能方面都优于现有的 GNN 训练方法，特别是对尾部节点提供了显著的改进。这表明 MADE-GNN 作为面临类似可伸缩性和分发挑战的 GNN 应用程序的通用解决方案的潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Turning+A+Curse+into+A+Blessing:+Data-Aware+Memory-Efficient+Training+of+Graph+Neural+Networks+by+Dynamic+Exiting)|0|
|[A Heterogeneous Network fused with Context-aware Contrastive Learning for Sarcasm Topic-Target Pair Identification](https://doi.org/10.1145/3589335.3651462)|Minjie Yuan, Mengyu Xiang, Yuxuan Song, Qiudan Li, Jinye Fu, Daniel Dajun Zeng||Sarcastic comments are often used to express dissatisfaction with products or events. Mining the topics and targets can provide clues for analyzing the underlying reasons behind the sarcasm, which helps understand user demands and improve products service. Existing research mainly focuses on mining single facet of sarcasm, such as topic or target, ignoring the complex interrelations between them. To overcome the above challenges, this paper proposes a Heterogeneous Information Network fused with Context-Aware Contrastive Learning (HINCCL) method. This approach aims to model multi-view features including syntactic style, domain knowledge, and textual semantics through a hierarchical attention aggregation mechanism. Furthermore, a context-aware negative contrastive training strategy is designed to learn the differentiated representations between different topic-target pairs. The effectiveness of the proposed method is validated on a dataset constructed in the digital domain.|讽刺性的评论经常被用来表达对产品或事件的不满。挖掘主题和目标可以为分析挖苦背后的深层原因提供线索，有助于理解用户需求和改善产品服务。现有的研究主要集中在挖掘讽刺的单一方面，如话题或目标，忽略了它们之间复杂的相互关系。为了克服上述挑战，本文提出了一种融合上下文感知对比学习(HINCCL)方法的异构信息网络。该方法通过层次化的注意聚合机制对包括句法风格、领域知识和文本语义在内的多视图特征进行建模。此外，设计了一种基于上下文的负向对比训练策略来学习不同话题-目标对之间的差异表征。通过在数字域内构造的数据集验证了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Heterogeneous+Network+fused+with+Context-aware+Contrastive+Learning+for+Sarcasm+Topic-Target+Pair+Identification)|0|
|[Disentangled Anomaly Detection For Multivariate Time Series](https://doi.org/10.1145/3589335.3651492)|Xin Jie, Xixi Zhou, Chanfei Su, Zijun Zhou, Yuqing Yuan, Jiajun Bu, Haishuai Wang||Anomaly detection in time series that aims to identify unusual patterns has attracted a lot of attention recently. However, the representation of abnormal and normal data is difffcult to be distinguished because they are usually entangled. Recently, disentanglement theory based on variational auto-encoder (VAE) has shown great potential in machine learning and achieved great success in computer vision and natural language processing. In this paper, we propose a novel disentangled anomaly detection approach that adopts VAE-based disentanglement networks for anomaly detection in multivariate time series. The proposed method learns highquality disentangled latent factors in a continuous representation space to facilitate the identiffcation of anomalies from normal data. Extensive experiments demonstrate that our proposed lightweight model DA-VAE achieves state-of-the-art performance.|时间序列异常检测的目的是识别不寻常的模式，近来引起了广泛的关注。然而，异常数据和正常数据的表示很难区分，因为它们通常是纠缠在一起的。近年来，基于变分自动编码器(VAE)的解缠理论在机器学习方面显示出巨大的潜力，并在计算机视觉和自然语言处理方面取得了巨大的成功。在这篇论文中，我们提出了一种新的解缠异常检测方法，它采用基于 VAE 的解缠网络来处理多变量时间序列中的异常检测。该方法在连续表示空间中学习高质量的解纠潜因子，便于从正常数据中识别异常。大量的实验表明，我们提出的轻量级模型 DA-VAE 实现了最先进的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disentangled+Anomaly+Detection+For+Multivariate+Time+Series)|0|
|[Content Moderation on Social Media in the EU: Insights From the DSA Transparency Database](https://doi.org/10.1145/3589335.3651482)|Chiara Patricia Drolsbach, Nicolas Pröllochs||The Digital Services Act (DSA) requires large social media platforms in the EU to provide clear and specific information whenever they remove or restrict access to certain content. These "Statements of Reasons" (SoRs) are collected in the DSA Transparency Database to ensure transparency and scrutiny of content moderation decisions of the providers of online platforms. In this work, we empirically analyze 156 million SoRs within an observation period of two months to provide an early look at content moderation decisions of social media platforms in the EU. Our empirical analysis yields the following main findings: (i) There are vast differences in the frequency of content moderation across platforms. For instance, TikTok performs more than 350 times more content moderation decisions per user than X/Twitter. (ii) Content moderation is most commonly applied for text and videos, whereas images and other content formats undergo moderation less frequently. (ii) The primary reasons for moderation include content falling outside the platform's scope of service, illegal/harmful speech, and pornography/sexualized content, with moderation of misinformation being relatively uncommon. (iii) The majority of rule-breaking content is detected and decided upon via automated means rather than manual intervention. However, X/Twitter reports that it relies solely on non-automated methods. (iv) There is significant variation in the content moderation actions taken across platforms. Altogether, our study implies inconsistencies in how social media platforms implement their obligations under the DSA -- resulting in a fragmented outcome that the DSA is meant to avoid. Our findings have important implications for regulators to clarify existing guidelines or lay out more specific rules that ensure common standards on how social media providers handle rule-breaking content on their platforms.|《数字服务法》(DSA)要求欧盟的大型社交媒体平台在删除或限制访问某些内容时提供明确和具体的信息。这些“理由陈述”(SoRs)收集在 DSA 透明度数据库，以确保透明度和审查的内容审查决定的供应商的在线平台。在这项工作中，我们在两个月的观察期内实证分析了1.56亿个 SOR，以提供对欧盟社交媒体平台内容管制决策的早期观察。我们的实证分析得出以下主要结论: (i)跨平台的内容审核频率存在巨大差异。例如，TikTok 在每个用户上执行的内容审核决策是 X/Twitter 的350多倍。(ii)内容审查最常用于文本和视频，而图像和其他内容格式的审查次数较少。(ii)节制的主要原因包括不属于平台服务范围的内容、非法/有害言论、色情/色情内容，而节制错误信息的情况相对较少。(iii)大多数违反规则的内容是通过自动手段而不是人工干预检测和决定的。然而，X/Twitter 报告说它仅仅依赖于非自动化的方法。(iv)跨平台采取的内容审核行动有很大差异。总之，我们的研究暗示了社交媒体平台在履行 DSA 义务方面的不一致性——导致了 DSA 意图避免的支离破碎的结果。我们的研究结果对监管机构澄清现有指导方针或制定更具体的规则具有重要意义，以确保社交媒体提供商如何处理其平台上违规内容的共同标准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Content+Moderation+on+Social+Media+in+the+EU:+Insights+From+the+DSA+Transparency+Database)|0|
|[Object-level Copy-Move Forgery Image Detection based on Inconsistency Mining](https://doi.org/10.1145/3589335.3651540)|Jingyu Wang, Niantai Jing, Ziyao Liu, Jie Nie, Yuxin Qi, ChiHung Chi, KwokYan Lam||In copy-move tampering operations, perpetrators often employ techniques, such as blurring, to conceal tampering traces, posing significant challenges to the detection of object-level targets with intact structures. Focus on these challenges, this paper proposes an Object-level Copy-Move Forgery Image Detection based on Inconsistency Mining (IMNet). To obtain complete object-level targets, we customize prototypes for both the source and tampered regions and dynamically update them. Additionally, we extract inconsistent regions between coarse similar regions obtained through self-correlation calculations and regions composed of prototypes. The detected inconsistent regions are used as supplements to coarse similar regions to refine pixel-level detection. We operate experiments on three public datasets which validate the effectiveness and the robustness of the proposed IMNet.|在复制-移动篡改操作中，作案者经常使用模糊等技术来隐藏篡改痕迹，对检测具有完整结构的对象级目标构成重大挑战。针对这些挑战，本文提出了一种基于不一致性挖掘(IMNet)的对象级复制-移动伪造图像检测方法。为了获得完整的对象级目标，我们为源区域和被篡改的区域定制原型并动态更新它们。此外，我们还提取了通过自相关计算得到的粗相似区域和由原型组成的区域之间的不一致区域。将检测到的不一致区域作为粗相似区域的补充，以提高像素级检测精度。我们在三个公共数据集上进行了实验，验证了所提出的 IMNet 的有效性和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Object-level+Copy-Move+Forgery+Image+Detection+based+on+Inconsistency+Mining)|0|
|[Efficacy of Large Language Models in Predicting Hindi Movies' Attributes: A Comprehensive Survey and Content-Based Analysis](https://doi.org/10.1145/3589335.3651496)|Prabir Mondal, Siddharth Singh, Kushum, Sriparna Saha, Jyoti Prakash Singh, Brijraj Singh, Niranjan Pedanekar||This research explores the efficacy of four state-of-the-art Large Language Models (LLMs): GPT-3.5-turbo-0301, Vicuna, PaLM 2, and Dolly in predicting (i) movie genres using audio transcripts of movie trailers and (ii) meta-information such as director and cast details using movie name and its year-of-release (YoR) for Hindi movies. In the contemporary landscape, training models for movie meta-information prediction often demand extensive data and parameters, posing significant challenges. We aim to discern whether LLMs mitigate these challenges. Focusing on Hindi movies within the Flickscore dataset, our study concentrates on trailer data. Preliminary findings reveal that GPT-3.5 stands out as the most effective LLM in predicting movie meta-information. Despite the inherent complexities of predicting diverse aspects such as genres and user preferences, GPT-3.5 exhibits promising capabilities. This research not only contributes to advancing our understanding of LLMs in the context of movie-related tasks but also sheds light on their potential application in Recommendation Systems (RS), indicating a notable leap forward in user preference comprehension and personalized content recommendations.|这项研究探讨了四种最先进的大语言模型(LLM)的功效: GPT-3.5-turbo-0301，Vicuna，PalM 2和 Dolly 在预测(i)使用电影预告片的音频文本的电影类型和(ii)元信息，如导演和演员细节使用电影名称及其印地语电影的发行年份(YoR)。在当代景观中，电影元信息预测的训练模型往往需要大量的数据和参数，提出了严峻的挑战。我们的目标是弄清 LLM 是否能减轻这些挑战。关注 Flickscore 数据集中的印地语电影，我们的研究集中在预告片数据上。初步研究结果表明，GPT-3.5是预测电影元信息最有效的 LLM。尽管预测不同的方面如流派和用户偏好固有的复杂性，GPT-3.5展示了有希望的能力。这项研究不仅有助于提高我们对电影相关任务背景下的 LLM 的理解，而且还揭示了它们在推荐系统(RS)中的潜在应用，表明在用户偏好理解和个性化内容推荐方面有了显著的飞跃。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficacy+of+Large+Language+Models+in+Predicting+Hindi+Movies'+Attributes:+A+Comprehensive+Survey+and+Content-Based+Analysis)|0|
|[Interpretation-Empowered Neural Cleanse for Backdoor Attacks](https://doi.org/10.1145/3589335.3651525)|Liangbo Ning, Zeyu Dai, Jingran Su, Chao Pan, Luning Wang, Wenqi Fan, Qing Li||Backdoor attacks have posed a significant threat to deep neural networks, highlighting the need for robust defense strategies. Previous research has demonstrated that attribution maps change substantially when exposed to attacks, suggesting the potential of interpreters in detecting adversarial examples. However, most existing defense methods against backdoor attacks overlook the untapped capabilities of interpreters, failing to fully leverage their potential. In this paper, we propose a novel approach called interpretation-empowered neural cleanse (IENC ) for defending backdoor attacks. Specifically, integrated gradient (IG) is adopted to bridge the interpreters and classifiers to reverse and reconstruct the high-quality backdoor trigger. Then, an interpretation-empowered adaptative pruning strategy (IEAPS) is proposed to cleanse the backdoor-related neurons without the pre-defined threshold. Additionally, a hybrid model patching approach is employed to integrate the IEAPS and preprocessing techniques to enhance the defense performance. Comprehensive experiments are constructed on various datasets, demonstrating the potential of interpretations in defending backdoor attacks and the superiority of the proposed method.|后门攻击已经对深层神经网络构成了重大威胁，突出了强有力的防御策略的必要性。以往的研究表明，归因地图在受到攻击时会发生很大的变化，这表明口译员在发现敌对事例方面具有潜力。然而，大多数现有的防御后门攻击的方法忽视了口译员尚未开发的能力，未能充分发挥其潜力。在本文中，我们提出了一种新的方法称为解释-授权神经清洗(IENC)防御后门攻击。具体而言，采用集成梯度(IG)作为解释器和分类器之间的桥梁，对高质量的后门触发器进行反向重构。然后，提出了一种解释授权的自适应修剪策略(IEAPS) ，以清除后门相关的神经元没有预定义的阈值。此外，采用混合模型修补方法，将 IEAPS 和预处理技术相结合，提高了防御性能。在各种数据集上进行了综合实验，证明了解释在防御后门攻击中的潜力和所提方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interpretation-Empowered+Neural+Cleanse+for+Backdoor+Attacks)|0|
|[Who is Creating Malware Repositories on GitHub and Why?](https://doi.org/10.1145/3589335.3651582)|Nishat Ara Tania, Md Rayhanul Masud, Md Omar Faruk Rokon, Qian Zhang, Michalis Faloutsos||Recent studies have found thousands of malware source code repositories on GitHub. For the first time, we propose to understand the origins and motivations behind the creation of such malware repositories. For that, we collect and profile the authors of malware repositories using a three-fold systematic approach. First, we identify 14K users in GitHub who have authored at least one malware repository. Second, we leverage a pretrained large language model (LLM) to estimate the likelihood of malicious intent of these authors. This innovative approach led us to categorize 3339 as Malicious, 3354 as Likely Malicious, and 7574 as Benign authors. Further, to validate the accuracy and reliability of our classification, we conduct a manual review of 200 randomly selected authors. Third, our analysis provides insights into the authors' profiles and motivations. We find that Malicious authors often have sparse profiles and focus on creating and spreading malware, while Benign authors typically have complete profiles with a focus on cybersecurity research and education. Likely Malicious authors show varying levels of engagement and ambiguous intentions. We see our study as a key step towards understanding the ecosystem of malware authorship on GitHub.|最近的研究发现 GitHub 上有成千上万的恶意软件源代码库。第一次，我们建议理解这种恶意软件仓库创建背后的起源和动机。为此，我们使用三重系统方法收集和分析恶意软件库的作者。首先，我们在 GitHub 中确定至少创建了一个恶意软件库的14K 用户。其次，我们利用预先训练的大型语言模型(LLM)来估计这些作者的恶意意图的可能性。这种创新的方法使我们将3339归类为恶意，3354归类为可能的恶意，7574归类为良性作者。此外，为了验证我们的分类的准确性和可靠性，我们对随机选择的200位作者进行了手工审查。第三，我们的分析提供了对作者概况和动机的洞察力。我们发现，恶意作者往往有稀疏的配置文件和重点创建和传播恶意软件，而良性作者通常有完整的配置文件，重点是网络安全研究和教育。可能的恶意作者表现出不同程度的参与和模棱两可的意图。我们认为我们的研究是理解 GitHub 上恶意软件作者生态系统的关键一步。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Who+is+Creating+Malware+Repositories+on+GitHub+and+Why?)|0|
|[Zero-shot Explainable Mental Health Analysis on Social Media by Incorporating Mental Scales](https://doi.org/10.1145/3589335.3651584)|Wenyu Li, Yinuo Zhu, Xin Lin, Ming Li, Ziyue Jiang, Ziqian Zeng||Traditional discriminative approaches in mental health analysis are known for their strong capacity but lack interpretability and demand large-scale annotated data. The generative approaches, such as those based on large language models (LLMs), have the potential to get rid of heavy annotations and provide explanations but their capabilities still fall short compared to discriminative approaches, and their explanations may be unreliable due to the fact that the generation of explanation is a black-box process. Inspired by the psychological assessment practice of using scales to evaluate mental states, our method which is called Mental Analysis by Incorporating Mental Scales (MAIMS), incorporates two procedures via LLMs. First, the patient completes mental scales, and second, the psychologist interprets the collected information from the mental scales and makes informed decisions. Experimental results show that MAIMS outperforms other zero-shot methods. MAIMS can generate more rigorous explanation based on the outputs of mental scales|心理健康分析中的传统判别方法以其强大的能力而著称，但缺乏可解释性，需要大规模的注释数据。基于大型语言模型(LLM)的生成方法有可能去除繁重的注释并提供解释，但与区分方法相比，它们的能力仍然不足，而且由于生成解释是一个黑箱过程，它们的解释可能是不可靠的。受到心理评估实践中使用量表来评估心理状态的启发，我们的方法被称为结合心理量表的心理分析(MAIMS) ，通过 LLM 结合了两个程序。首先，患者完成心理量表，其次，心理学家解释从心理量表收集到的信息并做出明智的决定。实验结果表明，MAIMS 方法的性能优于其他零拍方法。MAIMS 可以根据心理量表的输出产生更严格的解释|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Zero-shot+Explainable+Mental+Health+Analysis+on+Social+Media+by+Incorporating+Mental+Scales)|0|
|[MART: Learning Hierarchical Music Audio Representations with Part-Whole Transformer](https://doi.org/10.1145/3589335.3651535)|Dong Yao, Jieming Zhu, Jiahao Xun, Shengyu Zhang, Zhou Zhao, Liqun Deng, Wenqiao Zhang, Zhenhua Dong, Xin Jiang||Recent research in self-supervised contrastive learning of music representations has demonstrated remarkable results across diverse downstream tasks. However, a prevailing trend in existing methods involves representing equally-sized music clips in either waveform or spectrogram formats, often overlooking the intrinsic part-whole hierarchies within music. In our quest to comprehend the bottom-up structure of music, we introduce MART, a hierarchical music representation learning approach that facilitates feature interactions among cropped music clips while considering their part-whole hierarchies. Specifically, we propose a hierarchical part-whole transformer to capture the structural relationships between music clips in a part-whole hierarchy. Furthermore, a hierarchical contrastive learning objective is crafted to align part-whole music representations at adjacent levels, progressively establishing a multi-hierarchy representation space. The effectiveness of our music representation learning from part-whole hierarchies has been empirically validated across multiple downstream tasks, including music classification and cover song identification.|最近对音乐表征的自我监督对比学习的研究已经证明在不同的下游任务中取得了显著的成果。然而，现有方法中的一个流行趋势涉及以波形或光谱图格式表示同样大小的音乐片段，往往忽略了音乐内在的部分-整体层次结构。在我们寻求理解自下而上的音乐结构的过程中，我们引入了 MART，一种分层的音乐表示学习方法，它促进了裁剪音乐片段之间的特征交互，同时考虑了它们的部分-整体层次结构。具体来说，我们提出了一个分层的部分-整体转换器来捕获部分-整体层次中音乐片段之间的结构关系。此外，一个层次对比学习目标被精心设计，以在相邻的层次上调整部分-整体的音乐表现，逐步建立一个多层次的表现空间。我们从部分-整体层次学习音乐表现的有效性已经在多个下游任务中得到了验证，包括音乐分类和翻唱歌曲识别。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MART:+Learning+Hierarchical+Music+Audio+Representations+with+Part-Whole+Transformer)|0|
|[Exploiting Associations among Multi-Aspect Node Properties in Heterogeneous Graphs for Link Prediction](https://doi.org/10.1145/3589335.3651502)|Chenguang Du, Hao Geng, Deqing Wang, Fuzhen Zhuang, Zhiqiang Zhang, Lanshan Zhang||Recent years have witnessed the abundant emergence of heterogeneous graph neural networks (HGNNs) for link prediction. In heterogeneous graphs, different meta-paths connected to nodes reflect different aspects of the nodes' properties. Existing work fuses the multi-aspect properties of each node into a single vector representation, which makes them fail to capture fine-grained associations between multiple node properties. To this end, we propose a heterogeneous graph neural network with Multi-Aspect Node Association awareness, namely MANA. MANA leverages key associations among multi-aspect node properties to achieve link prediction. Specifically, to avoid the loss of effective association information for link prediction, we design a transformer-based Multi-Aspect Association Mining module to capture multi-aspect associations between nodes. Then, we introduce the Multi-Aspect Link Prediction module, empowering MANA to focus on the key associations among all, thus avoiding the negative impact of ineffective associations on the model's performance. We conduct extensive experiments on three widely used datasets from Heterogeneous Graph Benchmark (HGB). Experimental results show that our proposed method outperforms state-of-the-art baselines.|近年来出现了大量用于链路预测的异构图神经网络(HGNNs)。在异构图中，连接到节点的不同元路径反映了节点属性的不同方面。现有的工作将每个节点的多方面属性融合为一个向量表示，这使得它们无法捕获多个节点属性之间的细粒度关联。为此，我们提出了一种具有多方面节点关联感知的异构图形神经网络，即 MANA。MANA 利用多方面节点属性之间的关键关联来实现链路预测。为了避免在链路预测中丢失有效的关联信息，我们设计了一个基于变压器的多方面关联挖掘模块来捕获节点之间的多方面关联。然后，引入多方面链路预测模块，使得 MANA 能够关注所有关键关联，从而避免无效关联对模型性能的负面影响。我们在三个广泛使用的异构图基准(HGB)数据集上进行了广泛的实验。实验结果表明，我们提出的方法性能优于最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploiting+Associations+among+Multi-Aspect+Node+Properties+in+Heterogeneous+Graphs+for+Link+Prediction)|0|
|[Automating the Information Extraction from Semi-Structured Interview Transcripts](https://doi.org/10.1145/3589335.3651230)|Angelina Parfenova Lucerne||This paper explores the development and application of an automated system designed to extract information from semi-structured interview transcripts. Given the labor-intensive nature of traditional qualitative analysis methods, such as coding, there exists a significant demand for tools that can facilitate the analysis process. Our research investigates various topic modeling techniques and concludes that the best model for analyzing interview texts is a combination of BERT embeddings and HDBSCAN clustering. We present a user-friendly software prototype that enables researchers, including those without programming skills, to efficiently process and visualize the thematic structure of interview data. This tool not only facilitates the initial stages of qualitative analysis but also offers insights into the interconnectedness of topics revealed, thereby enhancing the depth of qualitative analysis.|本文探讨了一个自动化系统的开发和应用，该系统旨在从半结构化访谈记录中提取信息。鉴于传统定性分析方法(如编码)的劳动密集性，存在对能够促进分析过程的工具的显著需求。我们的研究调查了各种主题建模技术，并得出结论，最好的模型分析面试文本是一个结合 BERT 嵌入和 HDBSCAN 聚类。我们提出了一个用户友好的软件原型，使研究人员，包括那些没有编程技能，有效地处理和可视化的主题结构的访谈数据。这一工具不仅便利了定性分析的初始阶段，而且提供了对所揭示的主题的相互关联性的深入了解，从而提高了定性分析的深度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automating+the+Information+Extraction+from+Semi-Structured+Interview+Transcripts)|0|
|[FashionReGen: LLM-Empowered Fashion Report Generation](https://doi.org/10.1145/3589335.3651232)|Yujuan Ding, Yunshan Ma, Wenqi Fan, Yige Yao, TatSeng Chua, Qing Li||Fashion analysis refers to the process of examining and evaluating trends, styles, and elements within the fashion industry to understand and interpret its current state, generating fashion reports. It is traditionally performed by fashion professionals based on their expertise and experience, which requires high labour cost and may also produce biased results for relying heavily on a small group of people. In this paper, to tackle the Fashion Report Generation (FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting system based the advanced Large Language Models (LLMs), debbed as GPT-FAR. Specifically, it tries to deliver FashionReGen based on effective catwalk analysis, which is equipped with several key procedures, namely, catwalk understanding, collective organization and analysis, and report generation. By posing and exploring such an open-ended, complex and domain-specific task of FashionReGen, it is able to test the general capability of LLMs in fashion domain. It also inspires the explorations of more high-level tasks with industrial significance in other domains. Video illustration and more materials of GPT-FAR can be found in https://github.com/CompFashion/FashionReGen.|时尚分析是指审查和评估时尚行业的趋势、风格和要素，以了解和解释其现状，生成时尚报告的过程。它传统上是由时尚专业人士根据他们的专业知识和经验进行的，这需要很高的劳动力成本，也可能产生严重依赖少数人的有偏见的结果。为了解决时尚报告生成(FashionReGen)的任务，提出了一种基于高级大语言模型(LLM)的智能时尚分析与报告系统，称为 GPT-FAR。具体来说，它试图提供时尚 ReGen 的基础上有效的时装表演分析，其中配备了几个关键的程序，即时装表演的理解，集体组织和分析，以及报告生成。通过对 FashionReGen 这样一个开放、复杂、领域特定的任务的提出和探索，可以测试 LLM 在时尚领域的一般性能。它也激发了在其他领域具有工业意义的更高层次任务的探索。Https://github.com/compfashion/fashionregen 提供更多关于「 GPT-FAR 」的录像插图及资料。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FashionReGen:+LLM-Empowered+Fashion+Report+Generation)|0|
|[Tender Document Analyzer with the Combination of Supervised Learning and LLM-based Improver](https://doi.org/10.1145/3589335.3651233)|Tomoki Ito, Shun Nakagawa||Bidders often take a long time to read and understand tender documents because they require specialized knowledge, and tender documents are generally long. Bidders first overview the specific items, such as payment and warranty, in a tender document and then check the overall document. Therefore, the function that can extract specific items (i.e., item extractor) and the function that can highlight words or phrases related to specific items (i.e., word-phrase highlighter) are in great demand. To develop the above two types of functions, we need to solve two problems. The first problem is the problem related to the annotated data set. The second problem concerns the BERT NER-based prediction approach in a small training dataset setting. To solve the first problem, we created two types of sequence labeling datasets related to Item Extractor and Word-Phrase Highlighter. To solve the second problem, we propose the Information Extraction (IE) method, which combines (1) a supervised learning approach using Bidirectional Encoder Representations from Transformers (BERT) and (2) a large language model (LLM)-based improver. We then developed the web application system called Tender Document Analyzer (TDDA), which includes "Item Extractor" and "Word-Phrase Highlighter". Experimental evaluation shows that our approach is practical. Firstly, the evaluation for extraction ability shows that the performance of our proposed method is much higher than the baseline approach that uses GPT 3.5, as well as demonstrates that the proposed LLM-based improver can improve the IE ability. In addition, the usability evaluation shows that bidders can solve the task in less time using our system.|投标人往往需要很长时间阅读和理解投标文件，因为他们需要专门知识，投标文件通常很长。投标人首先概述投标文件中的具体项目，如付款和保修，然后检查整个文件。因此，能够提取特定项目的功能(即项目提取器)和能够突出显示与特定项目相关的单词或短语的功能(即单词短语突出显示器)需求量很大。为了开发上述两类函数，我们需要解决两个问题。第一个问题是与带注释的数据集相关的问题。第二个问题涉及在一个小的训练数据集中基于 BERT NER 的预测方法。为了解决第一个问题，我们创建了两种类型的序列标签数据集相关的项目提取器和词语短语高亮。为了解决第二个问题，我们提出了信息抽取(IE)方法，它结合了(1)使用来自变形金刚的双向编码器表示(BERT)的监督式学习方法和(2)基于大语言模型(LLM)的改进器。然后我们开发了网络应用系统，称为投标文件分析仪(TDDA) ，其中包括“项目提取器”和“词组高亮显示器”。实验结果表明，该方法是可行的。首先，对提取能力的评估表明，该方法的性能远远高于基线方法使用的 GPT 3.5，并表明，提出的基于 LLM 的改进器可以提高 IE 能力。此外，可用性评估表明，投标人可以在较短的时间内解决该任务使用我们的系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tender+Document+Analyzer+with+the+Combination+of+Supervised+Learning+and+LLM-based+Improver)|0|
|[GRACE: Generating Cause and Effect of Disaster Sub-Events from Social Media Text](https://doi.org/10.1145/3589335.3651234)|Xinxi Jiang, Xiang Li, Qifeng Zhou, Qing Wang||In recent years, social media has emerged as a pivotal source of emergency response for natural disasters. Causal analysis of disaster sub-events is one of crucial concerns. However, the design and implementation of its application scenario present significant challenges, due to the intricate nature of events and information overload. In this work, we introduce GRACE, a system designed for generating the cause and effect of disaster sub-events from social media text. GRACE aims to provide a rapid, comprehensive, and real-time analysis of disaster intelligence. Different from conventional information digestion systems, GRACE employs event evolution reasoning by constructing a causal knowledge graph for disaster sub-events (referred to as DSECG) and fine-tuning GPT-2 on DSECG. This system offers users a comprehensive understanding of disaster events and supports human organizations in enhancing response efforts during disaster situations. Moreover, an online demo is accessible, allowing user interaction with GRACE and providing a visual representation of the cause and effect of disaster sub-events.|近年来，社交媒体已经成为自然灾害应急反应的一个重要来源。灾害子事件的因果分析是人们关注的重要问题之一。然而，由于事件和信息超载的复杂性，其应用场景的设计和实现存在重大挑战。在这项工作中，我们介绍 GRACE，一个从社会媒体文本中生成灾难子事件的因果关系的系统。GRACE 的目标是提供快速、全面和实时的灾害情报分析。GRACE 不同于传统的信息消化系统，它采用事件进化推理方法，构建灾害子事件的因果知识图(称为 DSECG) ，并在 DSECG 上微调 GPT-2。该系统为用户提供对灾害事件的全面了解，并支持人类组织在灾害情况下加强应对工作。此外，可以访问在线演示，允许用户与 GRACE 交互，并提供灾难子事件的原因和影响的可视化表示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GRACE:+Generating+Cause+and+Effect+of+Disaster+Sub-Events+from+Social+Media+Text)|0|
|[RealGraphGPUWeb: A Convenient and Efficient GPU-Based Graph Analysis Platform on the Web](https://doi.org/10.1145/3589335.3651237)|JeongMin Park, MyungHwan Jang, SangWook Kim||In this demo paper, we present RealGraphGPUWeb a web-based graph analysis platform with the following features: (1) easy to use user-friendly GUI, (2) high processing performance, (3) various graph algorithms and data formats supported, (4) high accessibility anywhere on the web, and (5) no coding requirements. In our demo, we show how a naive user (e.g., non-CS researcher) gets graph analysis results conveniently and efficiently with a few clicks through the web-based GUI inRealGraphGPUWeb. We also make the user feel the effect of performance improvement obtained by our optimization strategies employed in RealGraphGPUWeb. We believe that RealGraphGPUWeb could be a good platform not only for CS users but also for non-CS users who want to analyze big graphs for their applications easily and efficiently.|在本演示文章中，我们提出了一个基于 Web 的图形分析平台 RealGraphGPUWeb，它具有以下特点: (1)易于使用的用户友好的 GUI，(2)高处理性能，(3)支持各种图形算法和数据格式，(4)高可访问性在网络上的任何地方，(5)没有编码要求。在我们的演示中，我们展示了一个天真的用户(例如，非 CS 研究人员)如何通过基于 Web 的 GUI inRealGraphGPUWeb 通过几次点击就可以方便有效地获得图形分析结果。我们还让用户感受到我们在 RealGraphGPUWeb 中使用的优化策略所获得的性能改进的效果。我们相信 RealGraphGPUWeb 不仅可以为 CS 用户提供一个很好的平台，而且还可以为那些希望为自己的应用程序轻松高效地分析大图的非 CS 用户提供一个很好的平台。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RealGraphGPUWeb:+A+Convenient+and+Efficient+GPU-Based+Graph+Analysis+Platform+on+the+Web)|0|
|[BoostER: Leveraging Large Language Models for Enhancing Entity Resolution](https://doi.org/10.1145/3589335.3651245)|Huahang Li, Shuangyin Li, Fei Hao, Chen Jason Zhang, Yuanfeng Song, Lei Chen||Entity resolution, which involves identifying and merging records that refer to the same real-world entity, is a crucial task in areas like Web data integration. This importance is underscored by the presence of numerous duplicated and multi-version data resources on the Web. However, achieving high-quality entity resolution typically demands significant effort. The advent of Large Language Models (LLMs) like GPT-4 has demonstrated advanced linguistic capabilities, which can be a new paradigm for this task. In this paper, we propose a demonstration system named BoostER that examines the possibility of leveraging LLMs in the entity resolution process, revealing advantages in both easy deployment and low cost. Our approach optimally selects a set of matching questions and poses them to LLMs for verification, then refines the distribution of entity resolution results with the response of LLMs. This offers promising prospects to achieve a high-quality entity resolution result for real-world applications, especially to individuals or small companies without the need for extensive model training or significant financial investment.|实体解析涉及识别和合并引用相同实际实体的记录，是 Web 数据集成等领域的关键任务。万维网上存在大量重复和多版本的数据资源，突出了这一重要性。然而，实现高质量的实体解析通常需要大量的工作。像 GPT-4这样的大型语言模型(LLM)的出现已经证明了先进的语言能力，这可以成为这项任务的一个新范例。在本文中，我们提出了一个名为 BoostER 的演示系统，该系统检查了在实体解析过程中利用 LLM 的可能性，揭示了易于部署和低成本的优势。我们的方法优化选择一组匹配问题并将它们提交给 LLM 进行验证，然后利用 LLM 的响应精化实体解析结果的分布。这为实现现实世界应用程序的高质量实体解析结果提供了有希望的前景，特别是对个人或小公司而言，无需广泛的示范培训或大量的财务投资。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BoostER:+Leveraging+Large+Language+Models+for+Enhancing+Entity+Resolution)|0|
|[Scenario-Driven Cyber-Physical-Social System: Intelligent Workflow Generation Based on Capability](https://doi.org/10.1145/3589335.3651246)|Yi Li, Xinkui Zhao, Chen Chen, Shengye Pang, Zhengyang Zhou, Jianwei Yin||Cyber-Physical-Social System (CPSS) is a pioneering solution in Crowd Computing, which integrates heterogeneous resources from cyber, physical, and social spaces, possessing collaborative capabilities in perception, computation, and control. However, existing CPSSs usually confine their functionality to rigid scenarios or tasks, and often oversimplify human resource modeling that fails to dynamically recognize human capabilities. In this work, we propose a Scenario-Driven CPSS to enable an adaptive resource choreography across scenarios. More concretely, we leverage temporal environments to identify events and disassemble these events into workflows, triggering the execution of corresponding capability units, where the capability units abstract the shared functionality of heterogeneous resource groups. Meanwhile, we improve the pre-assuming human capabilities to construct the relationship between human and their capabilities during the execution of workflows, sufficiently promoting human intelligence. Our real-world demo on fire rescue demonstrates the effectiveness of the solution.|网络-物理-社会系统(Cyber-Physical-Social System，CPSS)是集群计算领域的一个开创性的解决方案，它集成了来自网络、物理和社会空间的异构资源，具有感知、计算和控制方面的协作能力。然而，现有的 CPSS 通常将其功能局限于严格的场景或任务，并且经常过度简化人力资源建模，这种建模不能动态地识别人力资源能力。在这项工作中，我们提出了一个场景驱动的 CPSS 来支持跨场景的自适应资源编排。更具体地说，我们利用时间环境来识别事件并将这些事件分解为工作流，从而触发相应功能单元的执行，其中功能单元抽象异构资源组的共享功能。同时，在工作流的执行过程中，提高人的预设能力，建立人与能力之间的关系，充分提升人的智力。我们在现实世界中的消防救援演示证明了该解决方案的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scenario-Driven+Cyber-Physical-Social+System:+Intelligent+Workflow+Generation+Based+on+Capability)|0|
|[REAL-UP: Urban Perceptions From LBSNs Helping Moving Real-Estate Market to the Next Level](https://doi.org/10.1145/3589335.3651252)|Frances Albert Santos, Thiago H. Silva, Leandro A. Villas||Finding the best short- or long-term accommodation is troublesome in unknown areas. Current tools provided by the real-estate market offer valuable information regarding the property, such as price, photos, and descriptions of the space; however, this market has little explored other relevant information regarding the surrounding area, such as what is nearby and users' subjective perception of the property's area. To address this gap, we propose REAL-UP, an interactive tool designed to enrich real-estate marketplaces. In addition to information commonly provided by such applications, e.g., rent price, REAL-UP also provides subjective neighborhood information based on Location-Based Social Networks (LBSNs) messages. This novel tool helps to represent complex users' subjective perceptions of urban areas, which could ease the process of finding the best accommodation.|在未知地区寻找最好的短期或长期住所是件麻烦事。目前房地产市场提供的工具提供了关于房产的有价值的信息，如价格、照片和空间描述; 然而，这个市场很少探索其他关于周围地区的相关信息，如附近的情况和用户对房产面积的主观感知。为了解决这一差距，我们提出 REAL-UP，一个旨在丰富房地产市场的交互式工具。除了这些应用程序通常提供的信息，如租金价格，REAL-UP 还提供基于位置社交网络(LBSNs)消息的主观邻里信息。这个新颖的工具有助于表现复杂用户对城市地区的主观感知，这可以简化寻找最佳住宿的过程。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=REAL-UP:+Urban+Perceptions+From+LBSNs+Helping+Moving+Real-Estate+Market+to+the+Next+Level)|0|
|[The Web Data Commons Schema.org Table Corpora](https://doi.org/10.1145/3589335.3651441)|Ralph Peeters, Alexander Brinkmann, Christian Bizer||The research on table representation learning, data retrieval, and data integration in the context of data lakes requires large table corpora for the training and evaluation of the developed methods. Over the years, several large table corpora such as WikiTables, GitTables, or the Dresden Web Table Corpus have been published and are used by the research community. This paper complements the set of public table corpora with the Web Data Commons Schema.org table corpora, two table corpora consisting of 4.2 (Release 2020) and 5 million (Release 2023) relational tables describing products, events, local businesses, job postings, recipes, movies, books, as well as 37 further types of entities. The feature that distinguishes the corpora from all other publicly available large table corpora is that all tables that describe entities of a specific type use the same attributes to describe these entities, i.e. all tables use a shared schema, the schema.org vocabulary. The shared schema eases the integration of data from different sources and allows training processes to focus on specific types of entities or specific attributes. Altogether the tables contain ~653 million rows of data which have been extracted from the Common Crawl web corpus and have been grouped into separate tables for each class/host combination, i.e. all records of a specific class that originate from a specific website are put into a single table. This paper describes the creation of the WDC Schema.org Table Corpora, gives an overview of the content of the corpora, and discusses their use cases.|数据湖背景下的表格学习、数据检索和数据集成的研究需要大量的表格语料库来训练和评价已有的方法。多年来，一些大型的表语料库，如 WikiTables、 GitTables 或 Dresden Web Table 语料库已经发布，并被研究社区使用。本文通过 Web Data Commons Schema.org 表语料库、由4.2(2020年发布)和500万(2023年发布)关系表组成的两个表语料库来补充公共表语料库的集合，这两个表语料库描述产品、事件、本地企业、招聘广告、食谱、电影、书籍，以及其他37种类型的实体。语料库区别于所有其他公开可用的大型表语料库的特点是，所有描述特定类型实体的表都使用相同的属性来描述这些实体，即所有表都使用一个共享模式 Schema.org 词汇表。共享模式简化了来自不同来源的数据的集成，并允许培训过程关注特定类型的实体或特定属性。这些表格总共包含约6.53亿行数据，这些数据是从 Common Crawl 网络语料库中提取出来的，并按每个类/主机组合分成单独的表格，也就是说，源自特定网站的特定类的所有记录都放在一个表格中。本文描述了 WDC Schema.org 表语料库的创建，概述了语料库的内容，并讨论了它们的用例。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Web+Data+Commons+Schema.org+Table+Corpora)|0|
|[Tel2Veh: Fusion of Telecom Data and Vehicle Flow to Predict Camera-Free Traffic via a Spatio-Temporal Framework](https://doi.org/10.1145/3589335.3651442)|ChungYi Lin, ShenLung Tung, HungTing Su, Winston H. Hsu||Vehicle flow, a crucial indicator for transportation, is often limited by detector coverage. With the advent of extensive mobile network coverage, we can leverage mobile user activities, or cellular traffic, on roadways as a proxy for vehicle flow. However, as counts of cellular traffic may not directly align with vehicle flow due to data from various user types, we present a new task: predicting vehicle flow in camera-free areas using cellular traffic. To uncover correlations within multi-source data, we deployed cameras on selected roadways to establish the Tel2Veh dataset, consisting of extensive cellular traffic and sparse vehicle flows. Addressing this challenge, we propose a framework that independently extracts features and integrates them with a graph neural network (GNN)-based fusion to discern disparities, thereby enabling the prediction of unseen vehicle flows using cellular traffic. This work advances the use of telecom data in transportation and pioneers the fusion of telecom and vision-based data, offering solutions for traffic management.|车辆流量是交通运输的一个重要指标，通常受到检测器覆盖面的限制。随着广泛的移动网络覆盖的出现，我们可以利用移动用户活动，或蜂窝通信量，在道路上作为车辆流量的代理。然而，由于来自不同用户类型的数据，蜂窝网络流量的计数可能与车辆流量不直接一致，我们提出了一个新的任务: 使用蜂窝网络流量预测无摄像头区域的车辆流量。为了揭示多源数据之间的相关性，我们在选定的道路上部署了摄像机来建立 Tel2Veh 数据集，其中包括广泛的蜂窝式交通和稀疏的车流。针对这一挑战，我们提出了一个独立提取特征的框架，并将其与基于图形神经网络(GNN)的融合相结合，以识别差异，从而使得能够使用细胞流量预测看不见的车辆流量。本文的工作推进了电信数据在交通运输中的应用，开创了电信数据与视觉数据融合的先河，为交通管理提供了解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tel2Veh:+Fusion+of+Telecom+Data+and+Vehicle+Flow+to+Predict+Camera-Free+Traffic+via+a+Spatio-Temporal+Framework)|0|
|[An Open Platform for Quality Measures in a Linked Data Index](https://doi.org/10.1145/3589335.3651443)|Pierre Maillot, Jennie Andersen, Sylvie Cazalens, Catherine Faron, Fabien Gandon, Philippe Lamarre, Franck Michel||There is a great diversity of RDF datasets publicly available on the web. Choosing among them requires assessing their "fitness for use'' for a particular use case, and thus, finding the right quality measures and evaluating data sources according to them. However, this is not an easy task due to the large number of possible quality measures, and the multiplicity of implementation and assessment platforms. Therefore, there is a need for a common way to define measures and evaluate RDF datasets, using open standards and tools. IndeGx is a SPARQL-based framework to design indexes of Knowledge Graphs declaratively. We extend it to support more advanced data quality measures. We demonstrate our approach by reproducing two existing measures, showing how one can formalize and add measures using such an open declarative framework.|网络上公开的 RDF 数据集种类繁多。从中进行选择需要评估它们对于特定用例的“适用性”，从而找到正确的质量度量并根据它们评估数据源。然而，这并不是一项容易的任务，因为有大量可能的质量措施，以及实施和评估平台的多样性。因此，需要使用开放标准和工具来定义度量和评估 RDF 数据集的通用方法。Index 是一个基于 SPARQL 的框架，用于声明性地设计知识图的索引。我们扩展它以支持更高级的数据质量度量。我们通过重现两个现有措施来展示我们的方法，说明如何使用这样一个开放的声明性框架来正式确定和添加措施。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Open+Platform+for+Quality+Measures+in+a+Linked+Data+Index)|0|
|[CompMix: A Benchmark for Heterogeneous Question Answering](https://doi.org/10.1145/3589335.3651444)|Philipp Christmann, Rishiraj Saha Roy, Gerhard Weikum||Fact-centric question answering (QA) often requires access to multiple, heterogeneous, information sources. By jointly considering several sources like a knowledge base (KB), a text collection, and tables from the web, QA systems can enhance their answer coverage and confidence. However, existing QA benchmarks are mostly constructed with a single source of knowledge in mind. This limits capabilities of these benchmarks to fairly evaluate QA systems that can tap into more than one information repository. To bridge this gap, we release CompMix, a crowdsourced QA benchmark which naturally demands the integration of a mixture of input sources. CompMix has a total of 9,410 questions, and features several complex intents like joins and temporal conditions. Evaluation of a range of QA systems on CompMix highlights the need for further research on leveraging information from heterogeneous sources.|以事实为中心的问答(QA)通常需要访问多种不同的信息源。通过联合考虑几个来源，如知识库(KB)、文本集合和来自 Web 的表格，QA 系统可以增强它们的答案覆盖范围和信心。然而，现有的质量保证基准大部分都是基于单一的知识来源构建的。这限制了这些基准测试公平评估能够利用多个信息存储库的 QA 系统的能力。为了弥合这一差距，我们发布了 CompMix，一个众包 QA 基准，它自然而然地要求集成多种输入源。CompMix 总共有9,410个问题，并且具有一些复杂的意图，比如连接和时态条件。对 CompMix 上一系列 QA 系统的评估突出表明，需要进一步研究如何利用来自不同来源的信息。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CompMix:+A+Benchmark+for+Heterogeneous+Question+Answering)|0|
|[Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery](https://doi.org/10.1145/3589335.3651446)|Yuxuan Yao, Sichun Luo, Haohan Zhao, Guanzhi Deng, Linqi Song||We present CNER-UAV, a fine-grained Chinese Name Entity Recognition dataset specifically designed for the task of address resolution in Unmanned Aerial Vehicle delivery systems. The dataset encompasses a diverse range of five categories, enabling comprehensive training and evaluation of NER models. To construct this dataset, we sourced the data from a real-world UAV delivery system and conducted a rigorous data cleaning and desensitization process to ensure privacy and data integrity. The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and Large Language Model annotation. We evaluated classical NER models on our dataset and provided in-depth analysis. The dataset and models are publicly available at <https://github.com/zhhvvv/CNER-UAV>.|我们介绍了 CNER-UAV，这是一个细粒度的中文名称实体识别数据集，专门为无人航空载具传输系统中的地址解析任务而设计。该数据集包括五个类别的不同范围，使 NER 模型的综合培训和评估成为可能。为了构建这个数据集，我们从一个真实世界的无人机传输系统中获取数据，并进行了严格的数据清洗和脱敏过程，以确保隐私和数据完整性。最终得到的数据集由大约12,000个注释样本组成，经过了人类专家和大型语言模型注释。我们在我们的数据集上评估了经典的 NER 模型，并提供了深入的分析。数据集和模型可以在 <  https://github.com/zhhvvv/cner-uav 上公开获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+LLM+Substitute+Human+Labeling?+A+Case+Study+of+Fine-grained+Chinese+Address+Entity+Recognition+Dataset+for+UAV+Delivery)|0|
|[Graphameleon: Relational Learning and Anomaly Detection on Web Navigation Traces Captured as Knowledge Graphs](https://doi.org/10.1145/3589335.3651447)|Lionel Tailhardat, Benjamin Stach, Yoan Chabot, Raphaël Troncy||User and Entity Behavior Analytics (UEBA) is key for managing security risks on information systems and comprehending user activities' impact on the network infrastructure. However, accessing network traffic and Web logs is challenging due to encryption or decentralized systems. Qualifying activities also requires contextualizing them according to the network's topology, as it determines potential exchanges and carries information about which services are used. This complexity hinders learning behavioral patterns when precise user action sequences are needed. We propose to tackle these challenges with Graphameleon, an open-source Web extension for capturing Web navigation traces. We model user activities in an RDF Knowledge Graph (KG), drawing from the UCO and NORIA-O ontologies. With this approach, we are able to distinguish analytics strategies implemented across different websites.|用户和实体行为分析(UEBA)是管理信息系统安全风险和理解用户活动对网络基础设施的影响的关键。然而，由于加密或分散系统，访问网络流量和 Web 日志具有挑战性。合格的活动还需要根据网络的拓扑结构将它们上下文化，因为它确定潜在的交换，并携带关于使用哪些服务的信息。当需要精确的用户操作序列时，这种复杂性阻碍了学习行为模式。我们建议使用 Graphameleon 来解决这些挑战，Graphameleon 是一个用于捕获 Web 导航跟踪的开源 Web 扩展。我们在一个 RDF 知识图(KG)中建模用户活动，借鉴 UCO 和 NORIA-O 本体。通过这种方法，我们能够区分不同网站实施的分析策略。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graphameleon:+Relational+Learning+and+Anomaly+Detection+on+Web+Navigation+Traces+Captured+as+Knowledge+Graphs)|0|
|[Revisiting 30 years of the Network Time Protocol](https://doi.org/10.1145/3589335.3651998)|Simon S. Woo||Since the inception of the Internet and WWW, providing the time among multiple nodes on the Internet has been one of the most critical challenges. David Mills is the pioneer to provide time on the Internet, inventing the Network Time Protocol (NTP), and synchronizing the clocks in computer systems. Now, the NTP is predominantly used on the Internet and WWW. In this paper, we revisit the NTP, and present the overview of the NTP. In particular, we highlight the advanced research effort, the SpaceNTP, to synchronize the clocks among assets and entities in space. The SpaceNTP designed for space environments will be the fundamental medium and enabling block to provide the future web services in space.|自从互联网和万维网诞生以来，在互联网上的多个节点之间提供时间一直是最关键的挑战之一。大卫 · 米尔斯是在互联网上提供时间的先驱，他发明了网络时间协议(NTP) ，并在计算机系统中同步时钟。现在，NTP 主要用于 Internet 和 WWW。在本文中，我们重新审视了 NTP，并提出了 NTP 的概述。特别是，我们强调了先进的研究成果 SpaceNTP，它使空间中的资产和实体之间的时钟同步。为空间环境设计的 SpaceNTP 将成为提供未来空间网络服务的基本媒介和支持块。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+30+years+of+the+Network+Time+Protocol)|0|
|[Toward Making Opaque Web Content More Accessible: Accessibility From Adobe Flash to Canvas-Rendered Apps](https://doi.org/10.1145/3589335.3651999)|Thomas Steiner||Adobe Flash, once a ubiquitous multimedia platform, played a pivotal role in shaping the digital landscape for nearly two decades. Its capabilities ranged from animated banners and immersive websites to complex online applications and games. Flash content was embedded on websites with the embed or the object element. To the browser, the embedded content is opaque by default, which means Flash content can't be used for the accessibility tree that the browser creates based on the DOM tree, which is used by platform-specific accessibility APIs to provide a representation that can be understood by assistive technologies, such as screen readers. With Flash losing out on popularity, HTML 5 introduced the canvas element, which for the first time allowed developers to draw graphics and animations with either the canvas scripting API or the WebGL API directly natively in the browser. Similar to Flash, such canvas-rendered content is opaque by default and unusable for the accessibility tree. Lastly, the implementation of the WebAssembly Garbage Collection (WasmGC) standard in browsers allowed developers to port applications, written in non-Web programming languages like Kotlin for non-Web platforms like Android, to the Web by compiling them to WasmGC and rendering the entire app into a canvas. In the most extreme of cases, this means that the entire HTML code of an application can consist of a sole canvas tag, which evidently is opaque to the browser and impossible to leverage for the accessibility tree. Without judging their quality, this paper focuses on documenting approaches then and now for making such opaque Web content more accessible to users of assistive technologies.|Adobe Flash 曾经是一个无处不在的多媒体平台，在近20年的时间里，它在塑造数字世界的过程中发挥了关键作用。它的功能范围从动画横幅和身临其境的网站到复杂的在线应用程序和游戏。Flash 内容是嵌入在网站的嵌入或对象元素。对于浏览器来说，默认情况下嵌入的内容是不透明的，这意味着 Flash 内容不能用于浏览器基于 DOM 树创建的可访问性树，而 DOM 树被特定平台的可访问性 API 用来提供一种可以被辅助技术(如屏幕阅读器)理解的表示。由于 Flash 失去了流行性，HTML 5引入了画布元素，这是第一次允许开发人员直接在浏览器中使用画布脚本 API 或 WebGL API 绘制图形和动画。与 Flash 类似，这种画布呈现的内容在默认情况下是不透明的，无法用于可访问性树。最后，在浏览器中实现的 WebAssembly 垃圾收集(wasmGC)标准允许开发人员将应用程序移植到网络上，这些应用程序是用非网络编程语言编写的，比如 Kotlin，用于非网络平台，比如安卓系统，将它们编译到 wasmGC 中，并将整个应用程序渲染成一个画布。在最极端的情况下，这意味着应用程序的整个 HTML 代码可以由一个单独的画布标记组成，这对浏览器来说显然是不透明的，不可能利用可访问性树。在不评判其质量的情况下，本文侧重于记录当时和现在使这种不透明的 Web 内容更容易被辅助技术的用户访问的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Toward+Making+Opaque+Web+Content+More+Accessible:+Accessibility+From+Adobe+Flash+to+Canvas-Rendered+Apps)|0|
|[History in Making: Political Campaigns in the Era of Artificial Intelligence-Generated Content](https://doi.org/10.1145/3589335.3652000)|Ehsan ul Haq, Yiming Zhu, Pan Hui, Gareth Tyson||Web 2.0 provided impactful tools, based on user-generated content, for political campaigns and opinion engineering. However, in recent months, AI advances and the ease of access to AI-generated content (AIGC) have led to a paradigm shift in political participation by politicians and electorates alike. This paper aims to explore a historical analysis of this shift. We provide anecdotal evidence of new trends, potential impact, and challenges. We discuss the usage of AIGC in political campaigns, and how AIGC is used as a substitute for incarcerated politicians. Such a usage presents novel ways for leaders to reach the public and keep them politically active. However, AIGC also has risks when used for disinformation, such as DeepFake media and caller bots, to undermine and malign the opponents. On the other hand, the evidence shows that governments can nudge AIGC content by censoring Internet services. We also report challenges facing AIGC usage, such as model bias and hallucinations, along with a governance perspective on ethics and regulations.|Web 2.0为政治活动和舆论工程提供了基于用户生成内容的有影响力的工具。然而，近几个月来，人工智能的进步和获取人工智能内容(AIGC)的便利导致了政治家和选民参与政治的范式转变。本文旨在探讨这种转变的历史分析。我们提供新趋势、潜在影响和挑战的轶事证据。我们讨论 AIGC 在政治运动中的使用，以及 AIGC 如何被用来替代被监禁的政治家。这种用法为领导人提供了接触公众并让他们保持政治活跃的新方法。然而，当 AIGC 被用于虚假信息(如 DeepFake 媒体和呼叫机器人)以破坏和诽谤对手时，也存在风险。另一方面，有证据表明，政府可以通过审查互联网服务来推动 AIGC 的内容。我们还报告了 AIGC 使用所面临的挑战，例如模型偏差和幻觉，以及对道德和法规的治理观点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=History+in+Making:+Political+Campaigns+in+the+Era+of+Artificial+Intelligence-Generated+Content)|0|
|[Me, the Web and Digital Accessibility](https://doi.org/10.1145/3589335.3652002)|Reinaldo Ferraz||This essay will briefly narrate the relationship between my professional growth and the evolution of web technology and the internet, especially in Brazil, where I live. I start by briefly discussing my background and how I came to be involved with technology, from my earliest computer experiences to getting online and continuing my schooling. At this time, several significant Web and Internet events occurred, influencing my career path choice. I also write about how my work has focused on digital accessibility and how my engagement has impacted the Web in my country. I close the article by examining my present work environment and how the Web has impacted my life, motivating me to use free and open technology.|本文将简要叙述我的专业成长与网络技术和互联网的发展之间的关系，尤其是在我居住的巴西。我首先简要地讨论了我的背景，以及我是如何与科技打交道的，从我最早的计算机经历到上网和继续我的学业。在这个时候，一些重要的网络和互联网事件发生了，影响了我的职业道路的选择。我还写了我的工作如何集中在数字可访问性上，以及我的参与如何影响了网络。在文章的最后，我审视了我目前的工作环境以及网络是如何影响我的生活，激励我使用免费和开放的技术。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Me,+the+Web+and+Digital+Accessibility)|0|
|[A Non-Intrusive Approach to Assessing Dysarthria Severity: Advancing Clinical Diagnosis](https://doi.org/10.1145/3589335.3651449)|Ganjun Liu, Xiaohui Hou, Meng Ge, Tao Zhang, Haizhou Li||AI-driven severity assessment techniques for dysarthric disorders show promise in aiding speech-language pathologists with diagnostics and therapeutic follow-ups for patients. Existing solutions generally focus on the average intelligibility and hoarseness of the individual speaker's speech (i.e., speaker-level classification). This potentially ignores the slight variations in pronunciation attributed to the speaker's dysarthric disorders, e.g., /t/ and /d/. To address this issue, we rethink the inherent differences in the dysarthria speech, and propose a non-intrusive severity assessment approach called DysarNet. Specifically, we first design a prosodic emphasis module based on frame-level speech features to highlight the fine-grained temporal changes including pronunciation content, rhythm, and timing. Second, we design a multi-scale aggregation strategy to collect statistical cues on articulatory information at different scales, i.e., frame-level and utterance-level. By doing so, multi-scale prosody and articulatory cues are directly assist the prediction network for assessing dysarthria severity from multiple views, and naturally achieve speaker-independent generalization ability. Experimental results on VCC 2018 and TORGO datasets show that our DysarNet excels in assessing dysarthria severity.|人工智能驱动的严重程度评估技术在协助语言病理学家对患者进行诊断和治疗随访方面显示出前景。现有的解决方案通常侧重于个别说话者的语音的平均清晰度和声音嘶哑度(即，说话者级别分类)。这可能忽略了说话者的关节紊乱导致的发音上的细微变化，例如/t/和/d/。为了解决这个问题，我们重新思考构音障碍言语的固有差异，并提出一种非侵入性的严重程度评估方法称为 DysarNet。具体来说，我们首先设计了一个基于帧级语音特征的韵律强调模块，突出了语音内容、节奏和时序等细粒度的时间变化。其次，我们设计了一个多尺度的聚合策略来收集不同尺度的发音信息的统计信息，即框架水平和话语水平。这样，多尺度的韵律和发音线索可以直接帮助预测网络从多个角度评估构音障碍的严重程度，并自然而然地获得与说话人无关的泛化能力。对 VCC 2018和 TORGO 数据集的实验结果表明，我们的 DysarNet 在评估构音障碍严重程度方面表现出色。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Non-Intrusive+Approach+to+Assessing+Dysarthria+Severity:+Advancing+Clinical+Diagnosis)|0|
|[AI-based Prediction of Catheter-related Thrombosis Risk for Cancer Patients](https://doi.org/10.1145/3589335.3651452)|Yaoqi Guo, Yun Ma, Zijian Shao, Weichen Bi, Yanfeng Wang||Cancer patients face a heightened risk of venous thromboembolism (VTE), emerging as the second most prevalent cause of death within this population. Central venous catheterization (CVC), a routine procedure in cancer care, amplifies the VTE risk, leading to catheter-related thrombosis (CRT). Although traditional risk-assessment models and certain AI methods exist for VTE prediction, their capability and application in CRT risk prediciton for cancer patients remains limited. This paper addresses the shortcomings of current models (RAMs) by crafting a dedicated AI model to predict CRT risks for cancer patients. Leveraging a dataset encompassing 10,512 cancer patients undergoing catheterization over a decade, we meticulously select nine specific features for model construction, resulting in an impressive 0.794 AUROC in prediction, 54.9% higher than baseline. Furthermore, we estimate CRT-free probability using the Kaplan-Meier method. We also develop a WeChat Mini Program designed for efficient data collection and risk prediction, enhancing the efficiency of CRT risk detection for both doctors and patients.|癌症患者面临静脉血栓栓塞(VTE)的高风险，成为这一人群中第二大最常见的死亡原因。中心静脉导管插入术(CVC)是癌症护理的常规手术，它增加了 VTE 的风险，导致导管相关血栓形成(CRT)。尽管传统的风险评估模型和一定的 AI 方法可用于静脉血栓栓塞(VTE)的预测，但它们在癌症患者 CRT 风险预测中的能力和应用仍然有限。本文针对目前模型(RAM)的缺点，制定了一个专门的 AI 模型来预测癌症患者的 CRT 风险。利用一个包括10,512名接受导管插入术的癌症患者的数据集，我们仔细选择了9个模型构建的特征，导致令人印象深刻的0.794 AUROC 预测，比基线高54.9% 。此外，我们使用 Kaplan-Meier 方法估计无 CRT 的概率。我们还开发了一个微信迷你程序，用于有效的数据收集和风险预测，提高医生和患者 CRT 风险检测的效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI-based+Prediction+of+Catheter-related+Thrombosis+Risk+for+Cancer+Patients)|0|
|[Health CLIP: Depression Rate Prediction Using Health Related Features in Satellite and Street View Images](https://doi.org/10.1145/3589335.3651451)|Tianjian Ouyang, Xin Zhang, Zhenyu Han, Yu Shang, Yong Li||Mental health is a state of mental well-being that enables people to cope with the stresses of life, realize their abilities, learn well and work well, and contribute to their community. It has intrinsic and instrumental value and is integral to our well-being, and its correlation with environmental factors has been a subject of growing interest. As the pressure of society keeps growing, depression has become a severe problem in modern cities, and finding a way to estimate depression rate is of significance to relieve the problem. In this study, we introduce a Contrastive Language-Image Pretraining (CLIP) based novel approach to predict mental health indicators, especially depression rate, through satellite and street view images. Our methodology uses state-of-the-art Multimodal Large Language Model (MLLM), GPT4-vision, to generate health related captions for satellite and street view images, then we use the generated image-text pairs to fine-tune the CLIP model, making its image encoder extract health related features such as green spaces, sports fields, and infrastructral characteristics. The fine-tuning process is employed to bridge the semantic gap between textual descriptions and visual representations, enabling a comprehensive analysis of geo-tagged images. Consequently, our methodology achieves a notable R2 value of 0.565 on prediction of depression rate in New York City with the combination of satellite and street view images. The successful deployment of Health CLIP in a real-world scenario underscores the practical applicability of our approach.|心理健康是一种心理健康状态，它使人们能够应对生活压力，认识自己的能力，学习好，工作好，并为社会做出贡献。它具有内在的和工具性的价值，是我们的福祉不可或缺的一部分，它与环境因素的相关性已成为一个日益感兴趣的主题。随着社会压力的不断增大，抑郁症已成为现代城市的一个严重问题，寻找一种估算抑郁症发病率的方法对缓解这一问题具有重要意义。在这项研究中，我们介绍了一种基于对比语言-图像预训练(CLIP)的新方法来预测心理健康指标，特别是抑郁率，通过卫星和街景图像。我们的方法使用最先进的多模态大语言模型(MLLM) ，GPT4-vision，为卫星和街景图像生成与健康相关的标题，然后使用生成的图像-文本对对 CLIP 模型进行微调，使其图像编码器提取与健康相关的特征，如绿色空间，体育场和基础设施特征。微调过程被用来弥合文本描述和视觉表征之间的语义鸿沟，从而能够对带有地理标记的图像进行全面的分析。因此，我们的方法实现了一个显着的 R2值0.565在预测抑郁率在纽约市结合卫星和街景图像。HealthCLIP 在真实场景中的成功部署强调了我们方法的实际适用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Health+CLIP:+Depression+Rate+Prediction+Using+Health+Related+Features+in+Satellite+and+Street+View+Images)|0|
|[AI in Health and Social Care: A Methodology for Privacy Risk Modeling and Simulation](https://doi.org/10.1145/3589335.3651453)|Laura Carmichael, Steve Taylor, Adriane Chapman, Michael J. Boniface||As health and social care data networks evolve and adapt to greater digitalization and datafication of health, data and analytics systems are developing and bringing forward new ways to share, access and analyze data. Organizations and individuals making data sharing decisions for AI-enabled health and social care services need to be able to balance the benefits of such uses with the possible risks that may ensue - including those related to issues of privacy and security. In this paper, we provide an overview of our approach to privacy risk assessment for cross-domain access and re-use of sensitive data for research purposes using Spyderisk - an automated risk assessment tool. We apply Spyderisk to a real AI research scenario and consider the ways in which such techniques could support multiple stakeholders to assess privacy and security risks.|随着卫生和社会保健数据网络的发展和适应卫生数字化和数据化程度的提高，数据和分析系统正在发展和提出共享、获取和分析数据的新方法。为基于人工智能的保健和社会护理服务做出数据共享决策的组织和个人需要能够在这种使用的好处与可能随之而来的风险(包括与隐私和安全问题有关的风险)之间取得平衡。在本文中，我们提供了一个概述我们的方法隐私风险评估跨域访问和重用敏感数据的研究目的使用 Spyderisk-一个自动化的风险评估工具。我们将 Spyderisk 应用于真实的人工智能研究场景，并考虑这些技术可以支持多个利益相关者评估隐私和安全风险的方式。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI+in+Health+and+Social+Care:+A+Methodology+for+Privacy+Risk+Modeling+and+Simulation)|0|
|[Sociotechnical Considerations for Accessibility and Equity in AI for Healthcare](https://doi.org/10.1145/3589335.3651455)|Adriane Chapman, Chloe L. Harrison, Caroline Jones, James Thornton, Rose Worley, Jeremy C. Wyatt||As AI systems are built and deployed to support mental health services, it is imperative to fully understand the stakeholder acceptability of such systems so that these concerns can be taken into account in system design. As such, we undertook a consultation with staff (therapists) and service-users at Adferiad Recovery (a large mental health charity). The aim was to capture insights about their understanding of trust, and different trust factors for AI in mental health care. Surveys, interviews and focus groups were conducted with service users and therapists. Key takeaways for computer scientists and the developers of AI systems are presented.|随着人工智能系统的建立和部署以支持精神卫生服务，必须充分了解利益攸关方对这类系统的接受程度，以便在系统设计中考虑到这些关切。因此，我们在 Adferiad Recovery (一家大型精神健康慈善机构)与员工(治疗师)和服务使用者进行了磋商。研究的目的是了解他们对信任的理解，以及人工智能在心理健康护理中的不同信任因素。对服务使用者和治疗师进行了调查、访谈和焦点小组。介绍了计算机科学家和人工智能系统开发人员的关键要点。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sociotechnical+Considerations+for+Accessibility+and+Equity+in+AI+for+Healthcare)|0|
|[Uncertainty-Aware Pre-Trained Foundation Models for Patient Risk Prediction via Gaussian Process](https://doi.org/10.1145/3589335.3651456)|Jiaying Lu, Shifan Zhao, Wenjing Ma, Hui Shao, Xiao Hu, Yuanzhe Xi, Carl Yang||Patient risk prediction models are crucial as they enable healthcare providers to proactively identify and address potential health risks. Large pre-trained foundation models offer remarkable performance in risk prediction tasks by analyzing multimodal patient data. However, a notable limitation of pre-trained foundation models lies in their deterministic predictions (i.e., lacking the ability to acknowledge uncertainty). We propose Gaussian Process-based foundation models to enable the generation of accurate predictions with instance-level uncertainty quantification, thus allowing healthcare professionals to make more informed and cautious decisions. Our proposed approach is principled and architecture-agnostic. Experimental results show that our proposed approach achieves competitive performance on classical classification metrics. Moreover, we observe that the accuracy of certain predictions is much higher than that of the uncertain ones, which validates the uncertainty awareness of our proposed method. Therefore, healthcare providers can trust low-uncertainty predictions and conduct more comprehensive investigations on high-uncertainty predictions, ultimately enhancing patient outcomes with less expert intervention.|患者风险预测模型是至关重要的，因为它们使医疗保健提供者能够主动识别和处理潜在的健康风险。大型预先训练的基础模型通过分析多模态患者数据，在风险预测任务中提供了显著的性能。然而，预先训练的基础模型的一个显著的局限性在于它们的确定性预测(即，缺乏承认不确定性的能力)。我们提出了基于高斯过程的基础模型，以便能够生成具有实例级不确定性量化的准确预测，从而使医疗保健专业人员能够做出更明智和谨慎的决定。我们提出的方法是原则性的和体系结构无关的。实验结果表明，该方法在经典分类指标上取得了良好的性能。此外，我们观察到某些预测的准确度远远高于不确定的，这验证了我们提出的方法的不确定性意识。因此，医疗保健提供者可以信任低不确定性预测，并对高不确定性预测进行更全面的调查，最终通过较少的专家干预来提高患者预后。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncertainty-Aware+Pre-Trained+Foundation+Models+for+Patient+Risk+Prediction+via+Gaussian+Process)|0|
|[Enhancing Progressive Diagnosis Prediction in Healthcare with Continuous Normalizing Flows](https://doi.org/10.1145/3589335.3651457)|Yanchao Tan, Hengyu Zhang, Zihao Zhou, Guofang Ma, Fan Wang, Weiming Liu, Xinting Liao, Vicki Stover Hertzberg, Carl Yang||Progressive diagnosis prediction in healthcare is a promising yet challenging task. Existing studies usually assume a pre-defined prior for generating patient distributions (e.g., Gaussian). However, the inferred approximate posterior can deviate from the real-world distribution, which further affects the modeling of continuous disease progression over time. To alleviate such inference bias, we propose an enhanced progressive diagnostic prediction model (i.e., ProCNF), which integrates continuous normalizing flows (CNF) and neural ordinary differential equations (ODEs) to achieve more accurate approximations of patient health trajectories while capturing the continuity underlying disease progression. We first learn patient embeddings with CNF to construct a complex posterior approximation of patient distributions. Then, we devise a CNF-enhanced neural ODE module for progressive diagnostic prediction, which aims to improve the modeling of disease progression for individual patients. Extensive experiments on two real-world longitudinal EHR datasets show significant performance gains brought by our method over state-of-the-art competitors.|渐进式诊断预测在医疗保健领域是一项前景广阔但具有挑战性的任务。现有的研究通常假设一个预先定义的病人分布(例如，高斯)。然而，推断的近似后验可能偏离现实世界的分布，这进一步影响了随着时间的推移连续疾病进展的模型。为了减轻这种推断偏倚，我们提出了一种增强的渐进诊断预测模型(即 ProCNF) ，其集成了连续正常化流(CNF)和神经常微分方程(ODEs) ，以实现更准确的患者健康轨迹的近似，同时捕获疾病进展的连续性。我们首先学习患者嵌入 CNF 来构建患者分布的复杂后验近似。然后，我们设计了一个 CNF 增强的神经 ODE 模块用于渐进诊断预测，旨在改进个体患者疾病进展的建模。在两个真实世界纵向 EHR 数据集上的大量实验表明，我们的方法比最先进的竞争对手带来了显著的性能提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Progressive+Diagnosis+Prediction+in+Healthcare+with+Continuous+Normalizing+Flows)|0|
|[Improving Prostate Cancer Risk Prediction through Partial AUC Optimization](https://doi.org/10.1145/3589335.3651458)|Xinyuan Zhu, Xiaohan Ren, Wentao Shi, Changming Wang, Xuehan Liu, Yuqing Liu, Tao Tao, Fuli Feng||Prostate cancer risk prediction (PCRP) is crucial in guiding clinical decision-making and ensuring accurate diagnoses. The area under the receiver operating characteristic curve (AUC) is typically used for the evaluation of PCRP models. However, AUC considers regions with high false positive rates (FPRs), which are not applicable in clinical practice. To address this concern, we propose to use partial AUC (pAUC) as a more clinically meaningful metric which evaluates PCRP models with restricted FPR. Moreover, we propose a new PCRP framework named pAUCP, which optimizes pAUC to train PCRP models and adopts model ensemble to further enhance its usability. We construct clinical datasets obtained from two medical centers over an extended period to evaluate the proposed pAUCP framework. Extensive experiments demonstrate the rationality and superiority of the pAUCP framework, especially the cross-time and cross-center transferability of the obtained PCRP model.|前列腺癌风险预测(PCRP)是指导临床决策和确保准确诊断的关键。ROC曲线曲线下面积(AUC)通常用于评估 PCRP 模型。然而，AUC 认为高假阳性率(FPR)的区域在临床实践中是不适用的。为了解决这个问题，我们建议使用部分 AUC (pAUC)作为一个更有临床意义的指标，用限制性 FPR 评估 PCRP 模型。此外，我们提出了一个新的 PCRP 框架 pAUCP，它优化了 pAUC 来训练 PCRP 模型，并采用模型集成来进一步提高其可用性。我们构建从两个医疗中心长期获得的临床数据集来评估所提出的 pAUCP 框架。大量的实验证明了 pAUCP 框架的合理性和优越性，特别是所得到的 PCRP 模型的跨时间和跨中心可转移性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Prostate+Cancer+Risk+Prediction+through+Partial+AUC+Optimization)|0|
|[Distributed Transparent Data Layer for Next Generation Blockchains](https://doi.org/10.1145/3589335.3651255)|Li Quan||Distributed Transparent Data Layer (DTDL) aims to overcome the significant storage inefficiencies in blockchain technology. The proposed scheme enhances scalability and enables broader adoption by allowing nodes to store only portions of the blockchain history, diverging from traditional methods like sharding. It consists of three main components: a transparent authentication scheme, a verifiable search tree, and a data availability sampling scheme, supporting diverse applications including zero-knowledge machine learning. This approach not only maintains transparency to the blockchain's upper layer but also offers seamless integration with existing systems without requiring forks. Additionally, the paper introduces an innovative transparent authentication method for Luby Transform (LT) codes using KZG commitments, enabling efficient and secure verification of encoded symbols without decoding. Addressing the challenges of data outsourcing in blockchain, our proposed model ensures data integrity and robust security in a potentially malicious publisher environment, marking a significant advancement in blockchain storage and data integrity solutions.|分布式透明数据层(DTDL)旨在克服区块链技术中存储效率低下的问题。提出的方案增强了可伸缩性，允许节点只存储区块链历史的一部分，从而实现更广泛的采用，不同于传统的方法，如分片。它由三个主要部分组成: 透明认证方案、可验证搜索树和数据可用性抽样方案，支持包括零知识机器学习在内的多种应用。这种方法不仅保持了区块链上层的透明度，而且还提供了与现有系统的无缝集成，而不需要分叉。此外，本文还介绍了一种新颖的基于 KZG 承诺的 LT 码透明认证方法，使得无需解码即可对编码符号进行高效、安全的验证。针对区块链数据外包的挑战，我们提出的模型确保数据完整性和强大的安全性，在一个潜在的恶意出版商环境，标志着区块链存储和数据完整性解决方案的重大进步。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributed+Transparent+Data+Layer+for+Next+Generation+Blockchains)|0|
|[Temporal Knowledge Graph Extraction and Modeling across Multiple Documents for Health Risk Prediction](https://doi.org/10.1145/3589335.3651256)|Rochana Chaturvedi||Clinical text in electronic health records (EHR) holds vital cues into a patient's journey, often absent in structured EHR data. Evidence-based healthcare decisions demand accurate extraction and modeling of these cues. The goal of our study is to predict Type-II Diabetes by utilizing concept-based models of visit sequences from longitudinal EHR data. We undertake the challenging task of fine-grained temporal information extraction from clinical text using a recent span-based approach with pre-trained transformers. We achieve a new state-of-the-art in end-to-end relation extraction from 2012 clinical temporal relations corpus. We propose to apply our model to a new dataset and extract patient-centric temporal knowledge graphs from their visits-fusing temporal orderings within documents and across visits. Beyond the current focus of our work on Type-II Diabetes risk prediction from EHR, our versatile framework can be extended to other domains including web-based healthcare systems for personalized medicine. It can not only model health outcomes having long progression timelines but also various socio-economic outcomes such as conflict, natural disasters, and financial markets by leveraging news, reports, and social-media text for extracting and modeling irregular time-series and help inform a variety of web-based applications and policies.|电子健康记录(EHR)中的临床文本为病人的旅程提供了重要的线索，而结构化的 EHR 数据往往缺乏这些线索。基于证据的医疗决策需要准确地提取和建模这些线索。我们研究的目的是利用纵向 EHR 数据中访问序列的基于概念的模型来预测 II 型糖尿病。我们采用最近的基于跨度的方法，使用经过训练的变压器，从临床文本中获取细粒度的时间信息抽取，这是一项具有挑战性的任务。我们从2012年的临床时间关系语料库中获得了一个新的端到端关系提取技术。我们建议将我们的模型应用于一个新的数据集，并从他们的访问中提取以患者为中心的时间知识图——融合文档内和跨访问的时间顺序。除了我们目前的工作重点是从电子健康记录中预测二型糖尿病的风险，我们的多功能框架可以扩展到其他领域，包括基于网络的个体化医学保健系统。它不仅可以通过利用新闻、报告和社交媒体文本来提取和建模不规则的时间序列，不仅可以模拟具有较长进展时间表的健康结果，而且还可以模拟冲突、自然灾害和金融市场等各种社会经济结果，并帮助为各种基于网络的应用程序和政策提供信息。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Temporal+Knowledge+Graph+Extraction+and+Modeling+across+Multiple+Documents+for+Health+Risk+Prediction)|0|
|[When Crypto Economics Meet Graph Analytics and Learning](https://doi.org/10.1145/3589335.3651257)|Bingqiao Luo||Utilizing graph analytics and learning has proven to be an effective method for exploring aspects of crypto economics such as network effects, decentralization, tokenomics, and fraud detection. However, the majority of existing research predominantly focuses on leading cryptocurrencies, namely Bitcoin (BTC) and Ethereum (ETH), overlooking the vast diversity among the more than 10,000 cryptocurrency projects. This oversight may result in skewed insights. In our paper, we aim to broaden the scope of investigation to encompass the entire spectrum of cryptocurrencies, examining various coins across their entire life cycles. Furthermore, we intend to pioneer advanced methodologies, including graph transfer learning and the innovative concept of "graph of graphs". By extending our research beyond the confines of BTC and ETH, our goal is to enhance the depth of our understanding of crypto economics and to advance the development of more intricate graph-based techniques.|利用图形分析和学习已被证明是探索密码经济学方面的有效方法，如网络效应、地方分权、标记组学和欺诈检测。然而，现有的大多数研究主要集中在领先的加密货币，即比特币(BTC)和以太坊(ETH) ，忽视了1万多个加密货币项目的巨大多样性。这种疏忽可能导致洞察力的扭曲。在本文中，我们的目标是扩大调查范围，以涵盖整个加密货币范畴，研究不同的硬币在其整个生命周期内的情况。此外，我们打算开创先进的方法论，包括图的转移学习和“图的图”的创新概念。通过将我们的研究扩展到 BTC 和 ETH 的范围之外，我们的目标是加深我们对密码经济学的理解，并推动更复杂的基于图的技术的发展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+Crypto+Economics+Meet+Graph+Analytics+and+Learning)|0|
|[Comprehensively Auditing the TikTok Mobile App](https://doi.org/10.1145/3589335.3651260)|Levi Kaplan, Piotr Sapiezynski||TikTok has become a dominant force in the social media landscape of the United States, and has spawned other social media sites emulating their algorithmically-driven short form content recommendation platform (e.g. Youtube Shorts and Instagram Reels). The short-form vertical content is designed to be consumed on mobile phones, but existing audits have predominantly, and to a limited degree, investigated TikTok using the web application. Additionally, there are no advertisements on the web version of TikTok, and as such the advertising ecosystem of the platform has thusfar largely gone unstudied. In this work we propose a technique for auditing TikTok's recommendation algorithm through interfacing with emulators and intercepting network traffic. In this way we are able to measure the personalization that comes from user-specified demographics such as gender and age and better understand how ads are delivered to these groups. Future work will investigate personalization from user interaction such as liking posts and following creators based on their interest, and will study the role that algorithmic personalization plays in ad targeting.|TikTok 已经成为美国社交媒体领域的主导力量，并催生了其他社交媒体网站模仿他们的算法驱动的短格式内容推荐平台(如 Youtube Shorts 和 Instagram Reels)。短格式的垂直内容被设计用于移动电话上，但是现有的审计已经主要并且在有限的程度上调查了使用 Web 应用程序的 TikTok。此外，TikTok 的网络版本上没有广告，因此该平台的广告生态系统到目前为止基本上还没有被研究过。在这项工作中，我们提出了一种审核 TikTok 的推荐算法的技术，通过与仿真器接口和拦截网络流量。通过这种方式，我们能够衡量来自用户指定的人口统计数据(如性别和年龄)的个性化程度，并更好地了解广告是如何传递给这些群体的。未来的工作将调查个性化的用户互动，如喜欢的帖子和关注创作者的基础上，他们的兴趣，并将研究的作用，算法个性化在广告定位。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Comprehensively+Auditing+the+TikTok+Mobile+App)|0|
|[Outgroup Dehumanisation in Telegram - the Role of Ingroup Identity and Perception](https://doi.org/10.1145/3589335.3651261)|Elizaveta Chernenko||Intergroup dehumanisation represents a pressing concern for today's society. It hinders empathy, prosocial behaviour, and contributes to between-group aggression. Its consequences are particularly dangerous in the context of international military conflicts as dehumanisation contributes to support for war, war-related violence, and usually accompanies genocidal conflicts. This motivated the focus of this study on the blatant forms of dehumanisation towards an outgroup defined in political or national terms, with a specific focus on the relations between Ukrainians, Russians, and Belarusians around the time of the Russian invasion of Ukraine in 2022. The study draws attention to previously under-researched aspect in outgroup dehumanisation, specifically the role of ingroup perception in it. Outgroup dehumanisation involves excluding the outgroup from the community one identifies with, thus reinforcing the boundary between ingroup and outgroup. This highlights the comparative nature of dehumanisation, suggesting its basis might lie more in comparative ingroup superiority bias rather than in outgroup inferiority bias. Existing research however generally concentrates solely on negative aspects of outgroup perception in dehumanising attitudes. While some studies have gauged dehumanisation through ingroup-outgroup perception differences, they lacked a ground truth measure for dehumanisation, leaving its comparative nature largely unexamined. Employing generative Large Language Model, we develop a dataset of Telegram channels posts, classified as dehumanising or neutral. Utilising NLP tools we analyse the role of ingroup-outgroup perception disparities in dehumanisation, specifically addressing its relation to affective polarisation.|群体间的非人化是当今社会迫切关注的问题。它阻碍同情心，亲社会行为，并促进群体间的攻击性。在国际军事冲突的背景下，其后果尤其危险，因为非人化助长了对战争、与战争有关的暴力的支持，而且通常伴随着种族灭绝冲突。这促使这项研究的重点放在公然形式的非人化，对政治或国家定义的外群体，特别是2022年俄罗斯入侵乌克兰时期乌克兰人，俄罗斯人和白俄罗斯人之间的关系。该研究提请注意以前未被充分研究的外群体非人性化的方面，特别是内群体知觉在其中的作用。外群体非人性化包括将外群体排除在一个人认同的社区之外，从而加强内群体和外群体之间的界限。这突出了非人性化的比较性质，表明其基础可能更多地存在于群体内部的比较自我优越心理，而不是群体外部的自卑偏见。然而，现有的研究一般只集中在非人性化态度中外群体感知的消极方面。虽然一些研究通过内群体-外群体的感知差异来衡量非人性化，但是他们缺乏一个基本的真理来衡量非人性化，使其比较性质基本上没有得到检验。采用生成式大语言模型，我们开发了一个电报渠道帖子的数据集，分类为非人性化或中性。利用自然语言处理工具，我们分析了内群体-外群体感知差异在非人性化中的作用，特别是它与情感两极化的关系。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Outgroup+Dehumanisation+in+Telegram+-+the+Role+of+Ingroup+Identity+and+Perception)|0|
|[Leveraging Knowledge-aware Methodologies for Multi-document Summarization](https://doi.org/10.1145/3589335.3651262)|Yutong Qu||With the development of information technology, a large amount of information and corpora has been incrementally sparked from the Web, stimulating an increasingly high demand for summarizing. Document Summarization is one of Natural Language Processing tasks, which aims to generate abridged versions of a given single or multiple documents as concise and coherent as possible while preserving salient information from the source texts. Recent research in the area has started to use knowledge graphs as they can capture more factual and applicable information from more facets along with source information, benefiting fact consistency and informativeness of generated summaries, rather than just from a linguistic perspective. However, there is no explicit investigation of the effects of different kinds of knowledge graphs on document summarization. The proposed method is to use structured informative and knowledgeable auxiliary information, especially knowledge graphs, into pre-trained summarization models, advancing summary qualities. Expected outcomes are exploring knowledge and knowledge graph incorporation for multi-document summarization, and achieving more informative, coherent, and factually consistent summaries.|随着信息技术的发展，大量的信息和语料越来越多地从网络中涌现出来，对文摘的需求也越来越高。文档摘要是自然语言处理中的一项任务，它的目标是生成给定的单个或多个文档的简化版本，使其尽可能简洁和连贯，同时保留源文本中的显著信息。该领域最近的研究已开始使用知识图表，因为它们可以从更多的方面获取更多的事实和适用信息以及来源信息，有利于所生成摘要的事实一致性和信息性，而不仅仅是从语言的角度。然而，目前还没有明确的研究表明不同类型的知识图对文档摘要的影响。该方法将结构化的信息和知识化的辅助信息，特别是知识图，应用于预训练的文摘模型中，提高了文摘质量。预期的结果是探索知识和知识图表合并为多文件摘要，并实现更多的信息，连贯性和事实一致的摘要。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+Knowledge-aware+Methodologies+for+Multi-document+Summarization)|0|
|[Knowledge Enabled Relation Extraction](https://doi.org/10.1145/3589335.3651263)|Monika Jain||Relation extraction is the task of extracting relationships from input text, where input can be a sentence, document, or multiple documents. This task has been popular for decades and is still of keen interest. Various techniques have been proposed to solve the relation extraction problem, among which the most popular are using distant supervision, deep learning-based models, reasoning-based models, and transformer-based models. We propose three approaches (named ReOnto, DocRE-CLip, and KDocRE) for relation extraction from text at three levels of granularity (sentence, document and across documents). These approaches embed knowledge in a deep learning based model to improve performance. ReOnto and DocRE-CLip have been evaluated and the source code is publicly available. We are currently implementing and evaluating KDocRE.|关系提取是从输入文本中提取关系的任务，其中输入可以是一个句子、文档或多个文档。几十年来，这项任务一直很受欢迎，并且仍然引起人们的浓厚兴趣。人们提出了各种各样的方法来解决关系抽取问题，其中最流行的方法是使用远程监控、基于深度学习的模型、基于推理的模型和基于变压器的模型。我们提出了三种方法(命名为 ReOnto、 DocRE-Clip 和 KDocRE) ，用于在三个粒度级别(句子、文档和跨文档)从文本中提取关系。这些方法将知识嵌入到基于深度学习的模型中，以提高性能。已经对 ReOnto 和 DocRE-Clip 进行了评估，并且源代码公开可用。我们目前正在实现和评估 KDocRE。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Enabled+Relation+Extraction)|0|
|[Graph Unlearning with Efficient Partial Retraining](https://doi.org/10.1145/3589335.3651265)|Jiahao Zhang||Graph Neural Networks (GNNs) have achieved remarkable success in various real-world applications. However, GNNs may be trained on undesirable graph data, which can degrade their performance and reliability. To enable trained GNNs to efficiently unlearn unwanted data, a desirable solution is retraining-based graph unlearning, which partitions the training graph into subgraphs and trains sub-models on them, allowing fast unlearning through partial retraining. However, the graph partition process causes information loss in the training graph, resulting in the low model utility of sub-GNN models. In this paper, we propose GraphRevoker, a novel graph unlearning framework that better maintains the model utility of unlearnable GNNs. Specifically, we preserve the graph property with graph property-aware sharding and effectively aggregate the sub-GNN models for prediction with graph contrastive sub-model aggregation. We conduct extensive experiments to demonstrate the superiority of our proposed approach.|图形神经网络(GNN)在各种实际应用中取得了显著的成功。然而，GNN 可能被训练在不需要的图形数据上，这会降低它们的性能和可靠性。为了使经过训练的 GNN 能够有效地去除不需要的数据，一个理想的解决方案是基于再训练的图去除，它将训练图划分为子图，并在子图上训练子模型，允许通过部分再训练来快速去除。然而，图划分过程导致训练图中的信息丢失，从而导致子 GNN 模型的效用较低。在本文中，我们提出了一个新的图形去学习框架 GraphRevoker，它可以更好地维护不可学习 GNN 的模型效用。具体来说，我们采用图属性感知的分片方法保留了图的属性，并有效地聚集了用于预测的子 GNN 模型和图的对比子模型聚集。我们进行了广泛的实验，以证明我们提出的方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Unlearning+with+Efficient+Partial+Retraining)|0|
|[Incentives in the Ether: Practical Cryptocurrency Economics & Security](https://doi.org/10.1145/3589335.3651268)|Aviv Yaish||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Incentives+in+the+Ether:+Practical+Cryptocurrency+Economics+&+Security)|0|
|[Social Psychology Meets Social Computing: State of the Art and Future Directions](https://doi.org/10.1145/3589335.3641242)|Sourav S. Bhowmick, Hui Li, S. H. Annabel Chen, Yining Zhao||Social computing platforms typically deal with data that are either related to humans or generated by humans. Consequently, effective design of these platforms needs to be cognizant ofsocial psychology theories. In this tutorial, we review and summarize the research thus far into the paradigm ofpsychology theory-informed design of social computing platforms where the design is guided by theories from social psychology in addition to theories from computer science. Specifically, we review techniques and frameworks that embrace this paradigm in the arena of social influence. In addition, we suggest open problems and new research directions.|社会计算平台通常处理与人类相关或由人类生成的数据。因此，这些平台的有效设计需要社会心理学理论的认识。在本教程中，我们回顾和总结迄今为止的研究范式的心理学理论知情设计的社会计算平台的设计是由社会心理学的理论指导，以及从计算机科学的理论。具体来说，我们回顾了在社会影响领域采用这种范式的技术和框架。此外，还提出了一些开放性问题和新的研究方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Social+Psychology+Meets+Social+Computing:+State+of+the+Art+and+Future+Directions)|0|
|[Discrete Choice and Applications](https://doi.org/10.1145/3589335.3641244)|Flavio Chierichetti, Ravi Kumar, Andrew Tomkins|Massachusetts Institute of Technology, Department of Civil and Environmental Engineering, 77 Massachusetts Avenue, Room 1-181, Cambridge, MA 02139, United States of America; Delft University of Technology, Department of Maritime and Transport Technology, Netherlands; Massachusetts Institute of Technology, Department of Civil and Environmental Engineering, Room 181, United States of America; Massachusetts Institute of Technology, Department of Civil and Environmental Engineering, United States of America|This paper presents a framework for estimating and updating user preferences in the context of app-based recommender systems. We specifically consider recommender systems which provide personalized menus of options to users. A Hierarchical Bayes procedure is applied in order to account for inter- and intra-consumer heterogeneity, representing random taste variations among individuals and among choice situations (menus) for a given individual, respectively. Three levels of preference parameters are estimated: population-level, individual-level and menu-specific. In the context of a recommender system, the estimation of these parameters is repeated periodically in an offline process in order to account for trends, such as changing market conditions. Furthermore, the individual-level parameters are updated in real-time as users make choices in order to incorporate the latest information from the users. This online update is computationally efficient which makes it feasible to embed it in a real-time recommender system. The estimated individual-level preferences are stored for each user and retrieved as inputs to a menu optimization model in order to provide recommendations. The proposed methodology is applied to both Monte-Carlo and real data. It is observed that the online update of the parameters is successful in improving the parameter estimates in real-time. This framework is relevant to various recommender systems that generate personalized recommendations ranging from transportation to e-commerce and online marketing, but is particularly useful when the attributes of the alternatives vary over time.|本文提出了一个在基于应用程序的推荐系统中估计和更新用户偏好的框架。我们特别考虑推荐系统，它为用户提供个性化的选项菜单。为了解释消费者之间和消费者内部的异质性，应用层次贝叶斯过程，分别代表个体之间和给定个体的选择情况(菜单)之间的随机口味变化。估计了三个水平的偏好参数: 人口水平、个人水平和菜单特定。在推荐系统的情况下，这些参数的估计会在离线过程中周期性地重复，以便解释趋势，例如不断变化的市场状况。此外，当用户做出选择时，个人级别的参数会实时更新，以便纳入用户提供的最新信息。这种在线更新计算效率高，可以嵌入到实时推荐系统中。为每个用户存储估计的个人级首选项，并将其作为菜单优化模型的输入进行检索，以便提供建议。该方法同时适用于蒙特卡罗和实际数据。实验结果表明，在线更新参数可以有效地提高参数估计的实时性。这一框架与各种推荐系统相关，这些系统产生从运输到电子商务和在线营销的个性化推荐，但是当替代品的属性随着时间的推移而变化时，这一框架特别有用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Discrete+Choice+and+Applications)|0|
|[Mining Temporal Networks](https://doi.org/10.1145/3589335.3641245)|Aristides Gionis, Lutz Oettershagen, Ilie Sarpe|Nordea Data Science Lab, Helsinki, Finland; Aalto University, Helsinki, Finland|Networks (or graphs) are used to represent and analyze large datasets of objects and their relations. Naturally, real-world networks have a temporal component: for instance, interactions between objects have a timestamp and a duration. In this tutorial we present models and algorithms for mining temporal networks, i.e., network data with temporal information. We overview different models used to represent temporal networks. We highlight the main differences between static and temporal networks, and discuss the challenges arising from introducing the temporal dimension in the network representation. We present recent papers addressing the most well-studied problems in the setting of temporal networks, including computation of centrality measures, motif detection and counting, community detection and monitoring, event and anomaly detection, analysis of epidemic processes and influence spreading, network summarization, and structure prediction.|网络(或图形)是用来表示和分析大型数据集的对象及其关系。自然地，现实世界的网络有一个时间组件: 例如，对象之间的交互有一个时间戳和一个持续时间。在本教程中，我们介绍了挖掘时态网络的模型和算法，即，带有时态信息的网络数据。我们概述了用于表示时间网络的不同模型。我们强调了静态网络和时态网络的主要区别，并讨论了在网络表示中引入时态维所带来的挑战。我们提交了最近的论文，这些论文涉及时间网络设置中研究最多的问题，包括中心度量的计算、基序检测和计数、社区检测和监测、事件和异常检测、流行病过程和影响传播的分析、网络总结和结构预测。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mining+Temporal+Networks)|0|
|[Lecture-style Tutorial: Towards Graph Foundation Models](https://doi.org/10.1145/3589335.3641246)|Chuan Shi, Cheng Yang, Yuan Fang, Lichao Sun, Philip S. Yu||Emerging as fundamental building blocks for diverse artificial intelligence applications, foundation models have achieved notable success across natural language processing and many other domains. Concurrently, graph machine learning has gradually evolved from shallow methods to deep models to leverage the abundant graph-structured data that constitute an important pillar in the data ecosystem for artificial intelligence. Naturally, the emergence and homogenization capabilities of foundation models have piqued the interest of graph machine learning researchers. This has sparked discussions about developing a next-generation graph learning paradigm, one that is pre-trained on broad graph data and can be adapted to a wide range of downstream graph-based tasks. However, there is currently no clear definition or systematic analysis for this type of work. In this tutorial, we will introduce the concept of graph foundation models (GFMs), and provide a comprehensive exposition on their key characteristics and underpinning technologies. Subsequently, we will thoroughly review existing works that lay the groundwork towards GFMs, which are summarized into three primary categories based on their roots in graph neural networks, large language models, or a hybrid of both. Beyond providing a comprehensive overview and in-depth analysis of the current landscape and progress towards graph foundation models, this tutorial will also explore potential avenues for future research in this important and dynamic field. Finally, to help the audience gain a systematic understanding of the topics covered in this tutorial, we present further details in our recent preprint paper, "Towards Graph Foundation Models: A Survey and Beyond"[4], available at https://arxiv.org/pdf/2310.11829.pdf.|基础模型作为各种人工智能应用的基本构件，已经在自然语言处理和许多其他领域取得了显著的成功。同时，图形机器学习也逐渐从浅层方法演变为深层模型，以利用构成人工智能数据生态系统重要支柱的大量图形结构化数据。自然，基础模型的出现和同质化能力引起了图机学习研究者的兴趣。这引发了关于开发下一代图形学习范式的讨论，这种范式预先对广泛的图形数据进行了培训，可以适应广泛的下游基于图形的任务。然而，目前还没有明确的定义或系统的分析这种类型的工作。在本教程中，我们将介绍图形基础模型(GFMs)的概念，并对其关键特征和支撑技术进行全面的阐述。随后，我们将彻底回顾现有的工作，奠定了基础的通用语言模型，这是总结为三个主要类别的基础上，根据其在图神经网络，大型语言模型，或两者的混合。除了提供当前景观的全面概述和深入分析以及图形基础模型的进展之外，本教程还将探索这一重要和动态领域未来研究的潜在途径。最后，为了帮助读者对本教程所涵盖的主题有一个系统的理解，我们在最近的预印本论文《走向图形基础模型: 调查与超越》[4]中提供了进一步的详细 https://arxiv.org/pdf/2310.11829.pdf。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lecture-style+Tutorial:+Towards+Graph+Foundation+Models)|0|
|[Understanding (Dark) Humour with Internet Meme Analysis](https://doi.org/10.1145/3589335.3641249)|Ming Shan Hee, Rui Cao, Tanmoy Chakraborty, Roy KaWei Lee||Internet memes, in their ubiquitous spread across the digital landscape, have transformed into a potent communicative force. Their significance beckons keen interest from researchers and practitioners alike, necessitating a deep comprehension of their nuanced forms and functions. Recent studies have honed in on diverse facets of memes, particularly in detecting offensive material and discerning sarcasm, yet comprehensive instructional resources remain sparse. Addressing this void, our tutorial delivers an integrated framework for dissecting the complex humor of memes. It weaves together disciplines such as natural language processing, computer vision, and multimodal modeling, empowering participants to decode meanings, analyze sentiments, and identify offensive content within memes. Attendees will engage in hands-on exercises and observe demonstrations, tapping into established datasets and cutting-edge algorithms. This equips them with the expertise to navigate the intricacies of meme analysis and to contribute substantively to this dynamic domain. For more information, please check out our tutorial teaser video.|互联网文化基因在数字世界中无处不在，已经转变成一种强有力的交流力量。它们的重要性引起了研究者和实践者的浓厚兴趣，需要对其微妙的形式和功能有深刻的理解。最近的研究已经打磨在模因的不同方面，特别是在检测攻击性材料和识别讽刺，但是综合的教学资源仍然稀缺。为了填补这个空白，我们的教程提供了一个解剖模因复杂幽默的集成框架。它将自然语言处理、计算机视觉和多模态建模等学科编织在一起，赋予参与者解码意义、分析情绪和识别模因中令人不快的内容的能力。与会者将参与实践练习和观察演示，利用已建立的数据集和最先进的算法。这使他们具备专业知识，能够驾驭模因分析的复杂性，并为这一动态领域做出实质性贡献。欲了解更多信息，请查看我们的教程预告视频。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+(Dark)+Humour+with+Internet+Meme+Analysis)|0|
|[Privacy in Web Advertising: Analytics and Modeling](https://doi.org/10.1145/3589335.3641252)|Badih Ghazi, Ravi Kumar, Pasin Manurangsi||Privacy in general, and differential privacy (DP) in particular, have become important topics in data mining and machine learning. Digital advertising is a critical component of the internet and is powered by large-scale data analytics and machine learning models; privacy concerns around these are on the rise. Despite the central importance of private ad analytics and training privacy-preserving ad prediction models, there has been relatively little exposure of this subject to the broader Web community. In the past three years, the interest in privacy and the interest in online advertising have been steadily growing. The aim of this tutorial is to provide researchers with an introduction to the problems that arise in private analytics and modeling in advertising, survey recent results, and describe the main research challenges in the space.|一般而言，私隐，特别是差分隐私(DP) ，已成为数据挖掘和机器学习的重要课题。数字广告是互联网的重要组成部分，由大规模的数据分析和机器学习模型提供动力; 围绕这些的隐私问题正在上升。尽管私有广告分析和培训保护隐私的广告预测模型具有核心重要性，但在更广泛的网络社区中，这一主题的曝光相对较少。在过去的三年里，人们对隐私的兴趣和对在线广告的兴趣一直在稳步增长。本教程的目的是为研究人员提供一个介绍的问题出现在私人分析和建模在广告，调查最近的结果，并描述了主要的研究挑战空间。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privacy+in+Web+Advertising:+Analytics+and+Modeling)|0|
|[Large Language Models for Graphs: Progresses and Directions](https://doi.org/10.1145/3589335.3641251)|Chao Huang, Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh V. Chawla||Graph neural networks (GNNs) have emerged as fundamental methods for handling structured graph data in various domains, including citation networks, molecule prediction, and recommender systems. They enable the learning of informative node or graph representations, which are crucial for tasks such as link prediction and node classification in the context of graphs. To achieve high-quality graph representation learning, certain essential factors come into play: clean labels, accurate graph structures, and sufficient initial node features. However, real-world graph data often suffer from noise and sparse labels, while different datasets have unique feature constructions. These factors significantly impact the generalization capabilities of graph neural networks, particularly when faced with unseen tasks. Recently, due to the efficent text processing and task generalization capability of large language models (LLMs), there has been a promising approach to address the challenges mentioned above by combining large language models with graph data. This tutorial offers an overview of incorporating large language models into the graph domain, accompanied by practical examples. The methods are categorized into three dimensions: utilizing LLMs as augmenters, predictors, and agents for graph learning tasks. We will delve into the current progress and future directions within this field. By introducing this emerging topic, our aim is to enhance the audience's understanding of LLM-based graph learning techniques, foster idea exchange, and encourage discussions that drive continuous advancements in this domain.|图形神经网络(GNN)已经成为处理各种领域的结构化图形数据的基本方法，包括引文网络、分子预测和推荐系统。它们使得信息节点或图表示的学习成为可能，这对于链路预测和图上下文中的节点分类等任务是至关重要的。为了实现高质量的图形表示学习，必须考虑清晰的标签、准确的图形结构和充分的初始节点特征。然而，真实世界的图形数据往往受到噪声和稀疏标签的影响，而不同的数据集具有独特的特征结构。这些因素显著地影响了图形神经网络的泛化能力，特别是在面对未知任务时。近年来，由于大型语言模型(LLM)具有高效的文本处理和任务泛化能力，通过将大型语言模型与图形数据相结合来解决上述问题已经成为一种很有前途的方法。本教程提供了将大型语言模型合并到图形领域的概述，并附有实例。该方法分为三个维度: 利用 LLM 作为图形学习任务的增强子、预测子和代理。我们将深入研究这一领域当前的进展和未来的方向。通过介绍这个新兴的主题，我们的目标是提高受众对基于 LLM 的图形学习技术的理解，促进思想交流，并鼓励讨论，推动该领域的持续发展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+for+Graphs:+Progresses+and+Directions)|0|
|[New Frontiers of Knowledge Graph Reasoning: Recent Advances and Future Trends](https://doi.org/10.1145/3589335.3641254)|Lihui Liu, Zihao Wang, Jiaxin Bai, Yangqiu Song, Hanghang Tong||Knowledge graph reasoning plays an important role in data mining, AI, Web, and social science. These knowledge graphs serve as intuitive repositories of human knowledge, allowing for the inference of new information. However, traditional symbolic reasoning, while powerful in its own right, faces challenges posed by incomplete and noisy data in the knowledge graphs. In contrast, recent years have witnessed the emergence of Neural Symbolic AI, an exciting development that fuses the capabilities of deep learning and symbolic reasoning. It aims to create AI systems that are not only highly interpretable and explainable but also incredibly versatile, effectively bridging the gap between symbolic and neural approaches. Furthermore, with the advent of large language models, the integration of LLMs with knowledge graph reasoning has emerged as a prominent frontier, offering the potential to unlock unprecedented capabilities. This tutorial aims to comprehensively review different aspects of knowledge graph reasoning applications and also introduce the recent advances about Neural Symbolic reasoning and combining knowledge graph reasoning with large language models. It is intended to benefit researchers and practitioners in the fields of data mining, AI, Web, and social science.|知识图论证在数据挖掘、人工智能、网络和社会科学等领域具有重要作用。这些知识图表作为人类知识的直观存储库，允许推断新的信息。然而，传统的符号推理虽然本身很强大，却面临着知识图中不完整和噪声数据的挑战。相比之下，近年来出现了神经符号人工智能，一个令人兴奋的发展，融合了深度学习和符号推理的能力。它的目标是创建人工智能系统，不仅是高度可解释性和可解释性，而且令人难以置信的多功能，有效地弥合之间的差距符号和神经方法。此外，随着大型语言模型的出现，LLM 与知识图推理的集成已经成为一个突出的前沿领域，提供了释放前所未有能力的潜力。本教程旨在全面回顾知识图形推理应用的各个方面，并介绍神经符号推理以及将知识图形推理与大型语言模型相结合的最新进展。它旨在使数据挖掘、人工智能、网络和社会科学领域的研究人员和从业人员受益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=New+Frontiers+of+Knowledge+Graph+Reasoning:+Recent+Advances+and+Future+Trends)|0|
|[Simulating Human Society with Large Language Model Agents: City, Social Media, and Economic System](https://doi.org/10.1145/3589335.3641253)|Chen Gao, Fengli Xu, Xu Chen, Xiang Wang, Xiangnan He, Yong Li||This tutorial will delve into the fascinating realm of simulating human society using Large Language Model (LLM)-driven agents, exploring their applications in cities, social media, and economic systems. Through this tutorial, participants will gain insights into the integration of LLMs into human society simulation, providing a comprehensive understanding of how these models can accurately represent human interactions, decision-making processes, and societal dynamics from cities to social media and to economic systems. The tutorial will introduce the essential background, discuss the motivation and challenges, and elaborate on the recent advances.|本教程将深入研究使用大语言模型(LLM)驱动的代理模拟人类社会的迷人领域，探索它们在城市、社交媒体和经济系统中的应用。通过本教程，参与者将深入了解 LLM 与人类社会模拟的整合，全面了解这些模型如何准确地表示从城市到社交媒体和经济系统的人类互动，决策过程和社会动态。本教程将介绍基本背景，讨论动机和挑战，并阐述最近的进展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Simulating+Human+Society+with+Large+Language+Model+Agents:+City,+Social+Media,+and+Economic+System)|0|
|[Text-Attributed Graph Representation Learning: Methods, Applications, and Challenges](https://doi.org/10.1145/3589335.3641255)|Delvin Ce Zhang, Menglin Yang, Rex Ying, Hady W. Lauw||Text documents are usually connected in a graph structure, resulting in an important class of data named text-attributed graph, e.g., paper citation graph and Web page hyperlink graph. On the one hand, Graph Neural Networks (GNNs) consider text in each document as general vertex attribute and do not specifically deal with text data. On the other hand, Pre-trained Language Models (PLMs) and Topic Models (TMs) learn effective document embeddings. However, most models focus on text content in each single document only, ignoring link adjacency across documents. The above two challenges motivate the development of text-attributed graph representation learning, combining GNNs with PLMs and TMs into a unified model and learning document embeddings preserving both modalities, which fulfill applications, e.g., text classification, citation recommendation, question answering, etc. In this lecture-style tutorial, we will provide a systematic review of text-attributed graph, including its formal definition, recent methods, diverse applications, and challenges. Specifically, i) we will formally define text-attributed graph and briefly review GNNs, PLMs, and TMs, which are the fundamentals of some existing methods. ii) We will then revisit the technical details of text-attributed graph models, which are generally split into two categories, PLM-based and TM-based. iii) Besides, we will show diverse applications built on text-attributed graph. iv) Finally, we will discuss some challenges of existing models and propose solutions for future research.|文本文档通常以图形结构连接，形成一类重要的数据，即文本属性图，如论文引文图和网页超链接图。一方面，图神经网络(GNN)将每个文档中的文本视为一般的顶点属性，并不专门处理文本数据。另一方面，预训练语言模型(PLM)和主题模型(TM)学习有效的文档嵌入。但是，大多数模型只关注每个文档中的文本内容，而忽略了文档之间的链接邻接。上述两个挑战推动了文本属性图表示学习的发展，将 GNN 与 PLM 和 TM 结合成一个统一的模型，学习文档嵌入保留了这两种模式，实现了文本分类、引文推荐、问题回答等应用。在这个讲座风格的教程中，我们将提供一个文本属性图表的系统综述，包括它的正式定义，最近的方法，不同的应用和挑战。具体来说，我们将正式定义文本属性图，并简要回顾 GNN、 PLM 和 TM，它们是一些现有方法的基础。Ii)然后，我们将重新审视文本属性图模型的技术细节，这些模型通常分为基于 PLM 和基于 TM 两大类。此外，我们还将展示基于文本属性图的多种应用。最后，我们将讨论现有模型的一些挑战，并提出未来研究的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Text-Attributed+Graph+Representation+Learning:+Methods,+Applications,+and+Challenges)|0|
|[Toward Mitigating Misinformation and Social Media Manipulation in LLM Era](https://doi.org/10.1145/3589335.3641256)|Yizhou Zhang, Karishma Sharma, Lun Du, Yan Liu||The pervasive abuse of misinformation to influence public opinion on social media has become increasingly evident in various domains, encompassing politics, as seen in presidential elections, and healthcare, most notably during the recent COVID-19 pandemic. This threat has grown in severity as the development of Large Language Models (LLMs) empowers manipulators to generate highly convincing deceptive content with greater efficiency. Furthermore, the recent strides in chatbots integrated with LLMs, such as ChatGPT, have enabled the creation of human-like interactive social bots, posing a significant challenge to both human users and the social-bot-detection systems of social media platforms.These challenges motivate researchers to develop algorithms to mitigate misinformation and social media manipulations. This tutorial introduces the advanced machine learning researches that are helpful for this goal, including (1) detection of social manipulators, (2) learning causal models of misinformation and social manipulation, and (3) LLM-generated misinformation detection. In addition, we also present possible future directions.|滥用错误信息影响社交媒体舆论的普遍现象在各个领域日益明显，包括政治(如总统选举)和医疗保健，最明显的是最近的2019冠状病毒疾病疫情。随着大型语言模型(LLM)的发展，这种威胁已经变得越来越严重，因为 LLM 使操作者能够以更高的效率生成高度令人信服的欺骗性内容。此外，与 LLM 相结合的聊天机器人(如 ChatGPT)最近取得了长足进步，使得类人交互式社交机器人得以创建，对人类用户和社交媒体平台的社交机器人检测系统都构成了重大挑战。这些挑战促使研究人员开发算法来减少错误信息和社交媒体操纵。本教程介绍了有助于实现这一目标的高级机器学习研究，包括(1)社会操纵者的检测，(2)错误信息和社会操纵的学习因果模型，以及(3) LLM 生成的错误信息检测。此外，我们还提出了未来可能的发展方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Toward+Mitigating+Misinformation+and+Social+Media+Manipulation+in+LLM+Era)|0|
|[Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models](https://doi.org/10.1145/3589335.3641257)|Xin Wang, Yuwei Zhou, Hong Chen, Wenwu Zhu||This tutorial focuses on curriculum learning (CL), an important topic in machine learning, which gains an increasing amount of attention in the research community. CL is a learning paradigm that enables machines to learn from easy data to hard data, imitating the meaningful procedure of human learning with curricula. As an easy-to-use plug-in, CL has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision, natural language processing, data mining, reinforcement learning, etc. Therefore, it is essential introducing CL to more scholars and researchers in the machine learning community. However, there have been no tutorials on CL so far, motivating the organization of our tutorial on CL at WWW 2024. To give a comprehensive tutorial on CL, we plan to organize it from the following aspects: (1) theories, (2) approaches, (3) applications, (4) tools and (5) future directions. First, we introduce the motivations, theories and insights behind CL. Second, we advocate novel, high-quality approaches, as well as innovative solutions to the challenging problems in CL. Then we present the applications of CL in various scenarios, followed by some relevant tools. In the end, we discuss open questions and the future direction in the era of large language models. We believe this topic is at the core of the scope of WWW and is attractive to the audience interested in machine learning from both academia and industry.|本教程的重点是课程学习(CL) ，一个重要的主题在机器学习，它获得了越来越多的关注在研究社区。CL 是一种学习范式，它使机器能够从简单的数据学习到硬数据，模仿人类学习的有意义的过程与课程。作为一个易于使用的插件，CL 展示了它在提高各种模型的推广能力和收敛速度方面的强大功能，这些模型包括计算机视觉、自然语言处理、数据挖掘、强化学习等。因此，向机器学习领域的更多学者和研究人员介绍 CL 是非常必要的。然而，到目前为止还没有关于 CL 的教程，这促使我们在 WWW2024上组织关于 CL 的教程。为了给出一个全面的指导，我们计划从以下几个方面组织它: (1)理论，(2)方法，(3)应用，(4)工具和(5)未来的方向。首先，我们介绍了合作学习背后的动机、理论和见解。其次，我们倡导新颖的，高质量的方法，以及创新的解决方案的挑战性问题的合作学习。然后介绍了 CL 在各种场景中的应用，并给出了一些相关的工具。最后，我们讨论了开放性问题和大语言模型时代的未来发展方向。我们相信这个主题是万维网范围的核心，并且对学术界和工业界对机器学习感兴趣的观众都很有吸引力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Curriculum+Learning:+Theories,+Approaches,+Applications,+Tools,+and+Future+Directions+in+the+Era+of+Large+Language+Models)|0|
|[English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts](https://doi.org/10.1145/3589335.3651902)|Patrick Bareiß, Roman Klinger, Jeremy Barnes||Emotion classification in text is a challenging and subjective task, due to the involved cognitive inference processes that are required to interpret a textual stimulus. In addition, the set of emotion categories is highly domain-specific. For instance, literature analysis might require the use of aesthetic emotions (e.g., finding something beautiful), and social media analysis could benefit from fine-grained sets (e.g., separating anger from annoyance) in contrast to basic emotion categories. This renders the task an interesting field for zero-shot classifications, in which the label set is not known at model development time. Unfortunately, most resources for emotion analysis are English, and therefore, most studies on emotion analysis have been performed in English, including those that involve prompting language models for text labels. This leaves us with a research gap that we address in this paper: In which language should we prompt for emotion labels on non-English texts? This is particularly of interest when we have access to a multilingual large language model, because we could request labels with English prompts even for non-English data. Our experiments with natural language inference-based language models show that it is consistently better to use English prompts even if the data is in a different language.|语篇中的情绪分类是一项具有挑战性和主观性的任务，因为解释语篇刺激需要涉及到认知推理过程。此外，情绪类别的集合是高度特定的领域。例如，文学分析可能需要使用审美情感(例如，找到一些美丽的东西) ，社会媒体分析可以受益于细粒度的集合(例如，分离愤怒和烦恼)相对于基本的情感类别。这使任务成为一个有趣的零拍分类字段，其中标签集在模型开发时是未知的。不幸的是，大多数情绪分析的资源是英语，因此，大多数情绪分析的研究都是用英语进行的，包括那些涉及文本标签提示语言模型的研究。这给我们留下了一个研究空白，我们在这篇论文中提出: 我们应该用哪种语言提示非英语文本上的情感标签？当我们访问一个多语言的大型语言模型时，这是特别有意义的，因为我们可以使用英语提示来请求标签，即使是对于非英语数据也是如此。我们对基于自然语言推理的语言模型进行的实验表明，即使数据来自不同的语言，使用英语提示也总是更好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=English+Prompts+are+Better+for+NLI-based+Zero-Shot+Emotion+Classification+than+Target-Language+Prompts)|0|
|[Towards Invariant Time Series Forecasting in Smart Cities](https://doi.org/10.1145/3589335.3651897)|Ziyi Zhang, Shaogang Ren, Xiaoning Qian, Nick Duffield||In the transformative landscape of smart cities, the integration of the cutting-edge web technologies into time series forecasting presents a pivotal opportunity to enhance urban planning, sustainability, and economic growth. The advancement of deep neural networks has significantly improved forecasting performance. However, a notable challenge lies in the ability of these models to generalize well to out-of-distribution (OOD) time series data. The inherent spatial heterogeneity and domain shifts across urban environments create hurdles that prevent models from adapting and performing effectively in new urban environments. To tackle this problem, we propose a solution to derive invariant representations for more robust predictions under different urban environments instead of relying on spurious correlation across urban environments for better generalizability. Through extensive experiments on both synthetic and real-world data, we demonstrate that our proposed method outperforms traditional time series forecasting models when tackling domain shifts in changing urban environments. The effectiveness and robustness of our method can be extended to diverse fields including climate modeling, urban planning, and smart city resource management.|在智慧城市的转型景观中，将尖端网络技术融入时间序列预测，为加强城市规划、可持续性和经济增长提供了一个关键机遇。深层神经网络的发展极大地提高了预测性能。然而，一个显著的挑战在于这些模型能够很好地推广到分布外(OOD)时间序列数据。城市环境中固有的空间异质性和领域变化造成了障碍，使模型无法在新的城市环境中有效地适应和执行。为了解决这个问题，我们提出了一个解决方案，在不同的城市环境下推导出更稳健的预测的不变表示，而不是依赖于跨城市环境的伪相关来获得更好的普遍性。通过对合成数据和真实数据的大量实验，我们证明了我们提出的方法在处理变化的城市环境中的领域变化时优于传统的时间序列预测模型。该方法的有效性和鲁棒性可以推广到气候建模、城市规划和智能城市资源管理等多个领域。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Invariant+Time+Series+Forecasting+in+Smart+Cities)|0|
|[Open Metaverse: Issues, Evolution, and Future](https://doi.org/10.1145/3589335.3651898)|Zefeng Chen, Wensheng Gan, Jiayi Sun, Jiayang Wu, Philip S. Yu||With the evolution of content on the web and the Internet, there is a need for cyberspace that can be used to work, live, and play in digital worlds regardless of geography. The Metaverse provides the possibility of future Internet and represents a future trend. In the future, the Metaverse will be a space where the real and the virtual are combined. In this article, we have a comprehensive survey of the compelling Metaverse. We introduce computer technology, the history of the Internet, and the promise of the Metaverse as the next generation of the Internet. In addition, we briefly introduce the related concepts of the Metaverse, including novel terms like trusted Metaverse, human-intelligence Metaverse, personalized Metaverse, AI-enabled Metaverse, Metaverse-as-a-service, etc. Moreover, we present the challenges of the Metaverse such as limited resources and ethical issues. We also present Metaverse's promising directions, including lightweight Metaverse and autonomous Metaverse. We hope this survey will provide some helpful prospects and insightful directions about the Metaverse to related developments.|随着网络和互联网内容的发展，人们需要能够在数字世界中工作、生活和玩耍的网络空间，而不管地理位置如何。元宇宙提供了未来互联网的可能性，代表了未来的趋势。在未来，元宇宙将是一个真实和虚拟结合的空间。在本文中，我们将对引人注目的 Metaverse 进行全面的调查。我们介绍计算机技术，互联网的历史，以及元宇宙作为下一代互联网的前景。此外，简要介绍了元宇宙的相关概念，包括可信元宇宙、人类智能元宇宙、个性化元宇宙、人工智能元宇宙、服务型元宇宙等新术语。此外，我们提出了元宇宙的挑战，如有限的资源和伦理问题。我们还介绍了 Metaverse 的前景方向，包括轻量级 Metaverse 和自主 Metaverse。我们希望这个调查将提供一些有用的前景和有关元宇宙的相关发展的深刻指导。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Open+Metaverse:+Issues,+Evolution,+and+Future)|0|
|[Homogenizing Data Flows in Smart Cities: Value-driven use Cases in the Era of Citiverse](https://doi.org/10.1145/3589335.3651900)|Leonidas G. Anthopoulos, Ioannis Nikolaou||Context based data receive an increasing attention for Smart City (SC) flow homogenization and standardization. SC hubness can be a solution that can simplify these flows and transform them to standardized message exchanges, which can be easily retrieved. Several use cases can justify the SC hubness' potential and will be summarized in this paper. However, the aim of this article is to analyze and explain their potential in the era of metaverse in cities, the so-called "citiverse". The role of data in citiverse gains an additional importance since it is a data-oriented ecosystem, but, it has to be seen around the new capabilities and values that citiverse may bring.|基于上下文数据的智能城市(SC)流同质化和标准化日益受到重视。SC 集线器可以是一种解决方案，可以简化这些流，并将其转换为标准化的消息交换，从而很容易检索到这些消息交换。几个用例可以证明供应链集线器的潜力，并将在本文中进行总结。然而，本文的目的是分析和解释他们在城市元宇宙时代的潜力，即所谓的“城市宇宙”。由于数据是一个面向数据的生态系统，因此数据在城市宇宙中的作用变得更加重要，但是，必须围绕城市宇宙可能带来的新能力和价值来看待数据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Homogenizing+Data+Flows+in+Smart+Cities:+Value-driven+use+Cases+in+the+Era+of+Citiverse)|0|
|[Spatio-Temporal Challenges in Understanding your (Smart) City](https://doi.org/10.1145/3589335.3652580)|Dirk Ahlers||In this talk, we will explore challenges and opportunities of spatio-temporal information access as connecting temporal and spatial dimensions of mining and analysis. We focus on use cases and examples in the development of systems and services in smart sustainable cities, and in urban energy and climate transitions.|在这个演讲中，我们将探讨时空信息获取的挑战和机遇，作为连接采矿和分析的时空维度。我们侧重于在智能可持续城市以及城市能源和气候转型中开发系统和服务的用例和实例。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spatio-Temporal+Challenges+in+Understanding+your+(Smart)+City)|0|
|[A Longitudinal Study of Content Control Mechanisms](https://doi.org/10.1145/3589335.3651893)|Michael Dinzinger, Michael Granitzer||As generative AI continues to evolve, it becomes increasingly important for site owners to effectively communicate their conditions and preferences to web agents to maintain data sovereignty. This necessity underscores the importance of an ecosystem where the technical means to prevent unauthorized data mining and to set conditions on the usage of web resources are readily available. Our research focuses on the temporal development of such technical content control methods, examining two primary mechanisms: the regulation of web robots via the Robots Exclusion Protocol and the semantic annotation of web documents with licensing information. Through a longitudinal study, we analyze the implementation and recent modifications of robots.txt files, robot directives (such as noindex, nofollow, etc.), and license-related HTML annotations. This study is driven by the growing awareness among site owners regarding the control over their content in the face of the progression of AI, highlighting the critical need for effective web content control strategies to protect and appropriately manage the wealth of texts, images, videos, and other content populating the internet.|随着生成性 AI 的不断发展，网站所有者有效地向网络代理传达他们的条件和偏好以维护数据主权变得越来越重要。这一必要性强调了一个生态系统的重要性，在这个生态系统中，随时可以获得防止未经授权的数据挖掘和为使用网络资源设定条件的技术手段。我们的研究集中在这些技术内容控制方法的时间发展上，研究了两个主要机制: 通过 Robots.txt 对网络机器人进行管理，以及通过许可信息对网络文档进行语义注释。通过一个追踪研究，我们分析了 robots.txt 文件、机器人指令(如 noindex、 nofollow 等)和与许可相关的 HTML 注释的实现和最近的修改。这项研究是由网站所有者面对人工智能的发展对其内容的控制日益增长的意识所驱动的，突出了有效的网络内容控制策略的关键需要，以保护和适当管理文本，图像，视频和其他内容的财富填充互联网。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Longitudinal+Study+of+Content+Control+Mechanisms)|0|
|[TIQ: A Benchmark for Temporal Question Answering with Implicit Time Constraints](https://doi.org/10.1145/3589335.3651895)|Zhen Jia, Philipp Christmann, Gerhard Weikum||Temporal question answering (QA) involves explicit (e.g., "...before 2024") or implicit (e.g., "...during the Cold War period") time constraints. Implicit constraints are more challenging; yet benchmarks for temporal QA largely disregard such questions. This shortcoming spans three aspects. First, implicit questions are scarce in existing benchmarks. Second, questions are created based on hand-crafted rules, thus lacking diversity in formulations. Third, the source for answering is either a KB or a text corpus, disregarding cues from multiple sources. We propose a benchmark, called TIQ (Temporal Implicit Questions), based on novel techniques for constructing questions with implicit time constraints. First, questions are created automatically, with systematic control of topical diversity, timeframe, head vs. tail entities, etc. Second, questions are formulated using diverse snippets and further paraphrasing by a large language model. Third, snippets for answering come from a variety of sources including KB, text, and infoboxes. The TIQ benchmark contains 10,000 questions with ground-truth answers and underlying snippets as supporting evidence.|时间问题回答(QA)涉及明确的(例如，“ ... 在2024年之前”)或隐含的(例如，“ ... 在冷战时期”)时间限制。隐式约束更具挑战性; 然而，时间 QA 的基准很大程度上忽略了这些问题。这种不足主要表现在三个方面。首先，现有基准中很少出现隐性问题。其次，问题是基于手工制定的规则而产生的，因此缺乏公式的多样性。第三，回答的来源要么是知识库，要么是文本语料库，不考虑来自多个来源的提示。我们提出了一个基准，称为 TIQ (时间隐式问题) ，基于新的技术构造问题的隐式时间约束。首先，问题是自动生成的，系统地控制主题的多样性，时间框架，头尾实体等。其次，问题是使用不同的片段和进一步解释的大型语言模型。第三，用于回答问题的代码片段来自各种来源，包括知识库、文本和信息框。TIQ 基准测试包含10,000个问题，其中包含基本事实的答案和作为支持证据的基本片段。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TIQ:+A+Benchmark+for+Temporal+Question+Answering+with+Implicit+Time+Constraints)|0|
|[Attentive Partial Convolution for RGBD Image Inpainting](https://doi.org/10.1145/3589335.3651906)|Ankan Dash, Guiling Wang, Tao Han||In this work, we demonstrated the use of Partial Convolutions for RGBD image inpainting. We proposed the two models L-PConv and Attn-PConv. The baseline partial convolution model is outperformed by both of our proposed models, with the Attn-PConv model performing the best. The proposed Attn-PConv model is able to infill missing pixels with relatively less training time when compared to some other GAN-based models. As far as we know this is the first time a partial convolution model has been used successfully for RGBD image inpainting. The results( Image SSIM: 0.9787, Image PSNR: 30.9665, Depth SSIM: 0.9818, Depth PSNR: 35.7311) indicate that our model is successful in RGBD image inpainting. The addition of the additional loss terms and the Attentive Normalization techniques help improve the performance of the model significantly. We believe our model can be successfully used in AR-related applications where infilling missing pixels is performed frequently especially for both RGB and Depth images together. Beyond the scope of this study, we envision practical applications for our model in augmented reality, particularly in scenarios where frequent pixel infilling is required for both RGB and Depth images. In the future our research trajectory aims to incorporate higher resolution, aligning with the capabilities of modern cameras capable of capturing images at 4k resolution.|在这项工作中，我们展示了部分卷积在 RGBD 图像修复中的应用。提出了 L-PConv 模型和 Attn-PConv 模型。基线部分卷积模型的性能优于我们提出的两个模型，其中 Attn-PConv 模型的性能最好。与其他基于 GAN 的模型相比，本文提出的 Attn-PConv 模型能够以相对较少的训练时间填充缺失的像素。据我们所知，这是第一次将部分卷积模型成功地用于 RGBD 图像修复。结果(图像 SSIM: 0.9787，图像 PSNR: 30.9665，深度 SSIM: 0.9818，深度 PSNR: 35.7311)表明我们的模型是成功的 RGBD 图像修复。增加附加损失项和注意标准化技术有助于显著提高模型的性能。我们相信我们的模型可以成功地应用在 AR 相关的应用中，其中填充缺失的像素是经常执行的，特别是对于 RGB 和深度图像一起。除了这项研究的范围之外，我们设想了我们的模型在扩增实境上的实际应用，特别是在需要经常填充像素的情况下，对于 RGB 和深度图像。在未来，我们的研究轨迹旨在纳入更高的分辨率，与现代摄像机的能力相一致，能够捕捉4k 分辨率的图像。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Attentive+Partial+Convolution+for+RGBD+Image+Inpainting)|0|
|[GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text](https://doi.org/10.1145/3589335.3651909)|Kyle Hamilton, Luca Longo, Bojan Bozic||While the use of machine learning for the detection of propaganda techniques in text has garnered considerable attention, most approaches focus on "black-box" solutions with opaque inner workings. Interpretable approaches provide a solution, however, they depend on careful feature engineering and costly expert annotated data. Additionally, language features specific to propagandistic text are generally the focus of rhetoricians or linguists, and there is no data set labeled with such features suitable for machine learning. This study codifies 22 rhetorical and linguistic features identified in literature related to the language of persuasion for the purpose of annotating an existing data set labeled with propaganda techniques. To help human experts annotate natural language sentences with these features, RhetAnn, a web application, was specifically designed to minimize an otherwise considerable mental effort. Finally, a small set of annotated data was used to fine-tune GPT-3.5, a generative large language model (LLM), to annotate the remaining data while optimizing for financial cost and classification accuracy. This study demonstrates how combining a small number of human annotated examples with GPT can be an effective strategy for scaling the annotation process at a fraction of the cost of traditional annotation relying solely on human experts. The results are on par with the best performing model at the time of writing, namely GPT-4, at 10x less the cost. Our contribution is a set of features, their properties, definitions, and examples in a machine-readable format, along with the code for RhetAnn and the GPT prompts and fine-tuning procedures for advancing state-of-the-art interpretable propaganda technique detection.|利用机器学习检测文本中的宣传技术已经引起了相当大的关注，但是大多数方法都集中在具有不透明内部工作机制的“黑箱”解决方案上。可解释的方法提供了一种解决方案，然而，它们依赖于仔细的特征工程和昂贵的专家注释数据。此外，针对宣传性文本的语言特征通常是修辞学家或语言学家关注的焦点，目前还没有适合机器学习的标记这些特征的数据集。本研究编纂了与说服语言相关的文献中的22个修辞和语言特征，目的是对现有的标有宣传技巧的数据集进行注释。为了帮助人类专家用这些特性来注释自然语言句子，RhetAnn，一个网络应用程序，被特别设计用来最大限度地减少相当大的脑力劳动。最后，使用一小组注释数据来微调 GPT-3.5(一种生成式大语言模型(LLM)) ，以便在优化财务成本和分类准确性的同时对剩余数据进行注释。本研究展示了如何将少量的人工注释示例与 GPT 相结合，可以成为一种有效的策略，以较少的成本扩展注释过程，而仅仅依赖于人工专家。其结果与编写本文时性能最好的模型(即 GPT-4)相当，只是成本降低了10倍。我们的贡献是一组特性、它们的属性、定义和机器可读格式的示例，以及 RhetAnn 的代码和 GPT 提示和微调程序，用于推进最先进的可解释的宣传技术检测。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GPT+Assisted+Annotation+of+Rhetorical+and+Linguistic+Features+for+Interpretable+Propaganda+Technique+Detection+in+News+Text)|0|
|[A Bayesian Framework for Measuring Association and Its Application to Emotional Dynamics in Web Discourse](https://doi.org/10.1145/3589335.3651911)|Henrique S. Xavier, Diogo Cortiz, Mateus Silvestrin, Ana Luísa Freitas, Letícia Yumi Nakao Morello, Fernanda Naomi Pantaleão, Gabriel Gaudencio do Rêgo||This paper introduces a Bayesian framework designed to measure the degree of association between categorical random variables. The method is grounded in the formal definition of variable independence and is implemented using Markov Chain Monte Carlo (MCMC) techniques. Unlike commonly employed techniques in Association Rule Learning, this approach enables a clear and precise estimation of confidence intervals and the statistical significance of the measured degree of association. We applied the method to non-exclusive emotions identified by annotators in 4,613 tweets written in Portuguese. This analysis revealed pairs of emotions that exhibit associations and mutually opposed pairs. Moreover, the method identifies hierarchical relations between categories, a feature observed in our data, and is utilized to cluster emotions into basic-level groups.|本文介绍了一个用于测量分类随机变量之间关联度的贝叶斯框架。该方法基于变量独立性的形式化定义，并使用马尔科夫蒙特卡洛(MCMC)技术实现。与关联规则学习中常用的技术不同，这种方法能够清晰而精确地估计置信区间和测量关联度的统计显著性。我们将这个方法应用于由注释者在4613条葡萄牙语推文中识别出的非排他性情绪。这项分析揭示了一对表现出联想和相互对立的情感对。此外，该方法识别类别之间的等级关系，在我们的数据中观察到的特征，并被用来聚类情绪到基本水平的群体。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Bayesian+Framework+for+Measuring+Association+and+Its+Application+to+Emotional+Dynamics+in+Web+Discourse)|0|
|[Leveraging Large Language Models to Detect Influence Campaigns on Social Media](https://doi.org/10.1145/3589335.3651912)|Luca Luceri, Eric Boniardi, Emilio Ferrara||Social media influence campaigns pose significant challenges to public discourse and democracy. Traditional detection methods fall short due to the complexity and dynamic nature of social media. Addressing this, we propose a novel detection method using Large Language Models (LLMs) that incorporates both user metadata and network structures. By converting these elements into a text format, our approach effectively processes multilingual content and adapts to the shifting tactics of malicious campaign actors. We validate our model through rigorous testing on multiple datasets, showcasing its superior performance in identifying influence efforts. This research not only offers a powerful tool for detecting campaigns, but also sets the stage for future enhancements to keep up with the fast-paced evolution of social media-based influence tactics.|社交媒体的影响力运动对公共话语和民主政体构成了重大挑战。由于社会媒体的复杂性和动态性，传统的检测方法存在不足。针对这一问题，我们提出了一种新的检测方法，使用大语言模型(LLM) ，结合用户元数据和网络结构。通过将这些要素转换成文本格式，我们的方法有效地处理了多语种内容，并适应了恶意宣传行为者不断变化的策略。我们通过对多个数据集的严格测试来验证我们的模型，展示了它在识别影响努力方面的卓越性能。这项研究不仅提供了一个强大的工具来检测活动，但也为未来的增强，以跟上社会媒体为基础的影响力策略的快速发展奠定了基础。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+Large+Language+Models+to+Detect+Influence+Campaigns+on+Social+Media)|0|
|[Towards Fact-check Summarization Leveraging on Argumentation Elements Tied to Entity Graphs](https://doi.org/10.1145/3589335.3651914)|Katerina Haniková, David Chudán, Vojtech Svátek, Peter Vajdecka, Raphaël Troncy, Filip Vencovský, Jana Syrovátková||Fact-check consumers can have different preferences regarding the amount of text being used for explaining the claim veracity verdict. Dynamically adapting the size of a fact-check report is thus an important functionality for systems designed to convey claim verification explainability. Recent works have experimented with applying transformers-based or LLM-based text summarization methods in a zero-shot or few-shot manner, making use of some existing texts available in the summary parts of fact-check reports (e.g., called "justification'' in PolitiFact). However, for complex fact-checks, the purely sub-symbolic summarizers tend to either omit some elements of the fact-checker's argumentation chains or include contextual statements that may not be essential at the given level of granularity. In this paper, we propose a new method for enhancing fact-check summarization with the aim of injecting elements of structured fact-checker argumentation. This argumentation is, in turn, not only captured at the discourse level but tied to an entity graph representing the fact-check, for which we employ the PURO diagrammatic language. We have empirically performed a manual analysis of fact-check reports from two fact-checker websites, yielding (1) textual snippets containing the argumentation essence of the fact-check report and (2) categorized argumentation elements tied to entity graphs. These snippets are then fed to a state-of-the-art hybrid summarizer which has previously produced accurate fact-check summaries, as an additional input. We observe mild improvements on various ROUGE metrics, even if the validity of the results is limited given the small size of the dataset. We also compare the human-provided argumentation element categories with those returned, for the given fact-check ground truth summary, using a pre-trained language model upon both basic and augmented prompting. This yields a moderate accuracy as the model often fails to comply with the explicit given instructions.|事实核查消费者对于用于解释索赔准确性裁决的文本数量可能有不同的偏好。因此，动态调整事实核查报告的大小对于旨在传达索赔验证可解释性的系统来说是一项重要功能。最近的工作已经尝试应用基于变压器或基于 LLM 的文本摘要方法的零拍摄或少拍摄的方式，利用一些现有的文本摘要部分的事实核查报告(例如，所谓的“理由”在 PolitiFact)。然而，对于复杂的事实检查，纯粹的子符号汇总器倾向于要么省略事实检查者的论证链中的一些元素，要么包括在给定粒度级别上可能不必要的上下文语句。本文提出了一种新的增强事实检验总结的方法，目的是为结构化事实检验论证注入要素。反过来，这种论证不仅在话语层次上被捕获，而且被绑定到一个表示事实检查的实体图，为此我们使用 PURO 图表语言。我们对两个事实核查网站的事实核查报告进行了实证分析，得到了(1)包含事实核查报告论证本质的文本片段和(2)与实体图相关的分类论证元素。然后，将这些片段作为额外的输入，提供给一个最先进的混合摘要器，该摘要器先前已经生成了准确的事实检查摘要。我们观察到各种 ROUGE 指标的轻微改进，即使由于数据集的小规模，结果的有效性受到限制。我们还将人工提供的论证元素类别与返回的论证元素类别进行比较，对于给定的事实检查基本真相摘要，在基本提示和增强提示上使用预先训练的语言模型。由于模型常常不能遵守给定的明确指令，因此产生了一定的准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Fact-check+Summarization+Leveraging+on+Argumentation+Elements+Tied+to+Entity+Graphs)|0|
|[DCAI: Data-centric Artificial Intelligence](https://doi.org/10.1145/3589335.3641297)|Wei Jin, Haohan Wang, Daochen Zha, Qiaoyu Tan, Yao Ma, Sharon Li, SuIn Lee||The emergence of Data-centric AI (DCAI) represents a pivotal shift in AI development, redirecting focus from model refinement to prioritizing data quality. This paradigmatic transition emphasizes the critical role of data in AI. While past approaches centered on refining models, they often overlooked potential data imperfections, raising questions about the true potential of enhanced model performance. DCAI advocates the systematic engineering of data, complementing existing efforts and playing a vital role in driving AI success. This transition has spurred innovation in various machine learning and data mining algorithms and their applications on the Web. Therefore, we propose the DCAI Workshop at WWW'24, which offers a platform for academic researchers and industry practitioners to showcase the latest advancements in DCAI research and their practical applications in the real world.|以数据为中心的人工智能(DCAI)的出现代表了人工智能发展的一个关键转变，它将重点从模型细化转向优先考虑数据质量。这种范式转换强调了数据在人工智能中的关键作用。虽然过去的方法主要集中在改进模型上，但它们往往忽视了潜在的数据缺陷，从而引发了关于增强模型性能的真正潜力的问题。DCAI 倡导数据的系统工程，补充现有的努力，并在推动人工智能的成功中发挥重要作用。这种转变促进了各种机器学习和数据挖掘算法及其在 Web 上的应用程序的创新。因此，我们提出在 WWW’24举办 DCAI 研讨会，为学术研究人员和业界从业人员提供一个平台，展示 DCAI 研究的最新进展及其在现实世界中的实际应用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DCAI:+Data-centric+Artificial+Intelligence)|0|
|[Robust Data-centric Graph Structure Learning for Text Classification](https://doi.org/10.1145/3589335.3651915)|Jun Zhuang||Over the past decades, text classification underwent remarkable evolution across diverse domains. Despite these advancements, most existing model-centric methods in text classification cannot generalize well on class-imbalanced datasets that contain high-similarity textual information. Instead of developing new model architectures, data-centric approaches enhance the performance by manipulating the data structure. In this study, we aim to investigate robust data-centric approaches that can help text classification in our collected dataset, the metadata of survey papers about Large Language Models (LLMs). In the experiments, we explore four paradigms and observe that leveraging arXiv's co-category information on graphs can help robustly classify the text data over the other three paradigms, conventional machine-learning algorithms, pre-trained language models' fine-tuning, and zero-shot / few-shot classifications using LLMs.|在过去的几十年里，文本分类在不同的领域经历了显著的演变。尽管取得了这些进展，但现有的大多数以模型为中心的文本分类方法在包含高相似性文本信息的类不平衡数据集上不能很好地推广。与开发新的模型体系结构不同，以数据为中心的方法通过操纵数据结构来提高性能。在这项研究中，我们的目标是调查可靠的数据为中心的方法，可以帮助文本分类在我们收集的数据集，元数据的调查论文大语言模型(LLM)。在实验中，我们探索了四种范例，并观察到利用 arXiv 在图上的共分类信息可以帮助将文本数据与其他三种范例，传统的机器学习算法，预先训练的语言模型的微调和使用 LLM 的零拍/少拍分类进行强有力的分类。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Data-centric+Graph+Structure+Learning+for+Text+Classification)|0|
|[Data Quality-based Gradient Optimization for Recurrent Neural Networks](https://doi.org/10.1145/3589335.3651918)|Feihu Huang, Peiyu Yi, Shan Li, Haiwen Xu||Time series forecasting holds significant value in various application scenarios. However, existing forecasting methods primarily focus on optimizing model architecture while neglecting the substantial impact of data quality on model learning. In this study, we aim to enhance model performance by optimizing data utilization based on data quality and propose a Data Quality-based Gradient Optimization (DQGO) method to facilitate training of recurrent neural networks. Firstly, we define sample quality as the matching degree between samples and model, and suggest using the attention entropy to calculate the sample quality through an attention mechanism. Secondly, we optimize the model's gradient vector by giving different weights to samples with different quality. Through experiments conducted on six datasets, the results demonstrate that DQGO significantly improves LSTM's performance. In certain cases, it even surpasses the state-of-the-art models.|时间序列预测在各种应用场景中具有重要价值。然而，现有的预测方法主要侧重于优化模型结构，而忽视了数据质量对模型学习的实质性影响。本研究旨在藉由基于资料品质的资料利用率优化来提升模型的效能，并提出一种基于资料品质的梯度优化(DQGO)方法，以方便递归神经网路的训练。首先，我们将样本质量定义为样本与模型的匹配程度，并建议使用注意熵通过一种注意机制来计算样本质量。其次，通过对不同质量的样本赋予不同的权重，对模型的梯度向量进行优化。通过对六个数据集的实验表明，DQGO 算法显著提高了 LSTM 的性能。在某些情况下，它甚至超过了最先进的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data+Quality-based+Gradient+Optimization+for+Recurrent+Neural+Networks)|0|
|[CFinDEE: A Chinese Fine-Grained Financial Dataset for Document-Level Event Extraction](https://doi.org/10.1145/3589335.3651921)|Tian Zhang, Maofu Liu, Bingying Zhou||Document-level event extraction faces numerous challenges in accurately modeling real-world financial scenarios, particularly due to the inadequacies in existing datasets regarding data scale and fine-grained annotations. The development of datasets is a crucial factor in driving research progress; therefore, we present a high-quality Chinese document-level event extraction dataset, CFinDEE. This dataset, grounded in real-world financial news, defines 22 event types and 116 argument roles, annotating 26,483 events and 107,096 event arguments. CFinDEE aims to address these shortcomings by providing more comprehensive annotations and data augmentation, offering richer resources for document-level event extraction in the financial domain. CFinDEE extends data both horizontally and vertically, where horizontal expansion enriches the types of financial events, enhancing the diversity of the dataset; vertical expansion, by increasing the scale of the data, effectively boosts the practical value of the dataset. Experiments conducted on multiple advanced models have validated the high applicability and effectiveness of the CFinDEE dataset for document-level event extraction tasks in the financial field.|文档级事件提取在精确建模真实世界的金融场景方面面临着许多挑战，特别是由于现有数据集中关于数据规模和细粒度注释的不足。数据集的开发是推动研究进展的关键因素，因此，我们提出了一个高质量的中文文档级事件提取数据集 CFinDEE。该数据集基于真实世界的金融新闻，定义了22个事件类型和116个参数角色，注释了26,483个事件和107,096个事件参数。CFinDEE 旨在通过提供更全面的注释和数据增强，为金融领域的文档级事件提取提供更丰富的资源，从而解决这些缺陷。CFinDEE 横向和纵向扩展数据，横向扩展丰富了金融事件的类型，增强了数据集的多样性; 纵向扩展通过增加数据的规模，有效地提高了数据集的实用价值。在多个先进模型上进行的实验验证了 CFinDEE 数据集对于金融领域文档级事件抽取任务的高度适用性和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CFinDEE:+A+Chinese+Fine-Grained+Financial+Dataset+for+Document-Level+Event+Extraction)|0|
|[FASETS: Discovering Faceted Sets of Entities](https://doi.org/10.1145/3589335.3651924)|Koninika Pal, Hiba Arnaout, Simon Razniewski, Gerhard Weikum||Computing related entities for a given seed entity is an important task in exploratory search and comparative data analysis.Prior works, using the seed-based set expansion paradigm, have focused on the single aspect of identifying homogeneous sets with high pairwise relatedness. A few recent works discuss cluster-based approaches to tackle multi-faceted set expansion, however, they fail in harnessing the specificity of the clusters and generating an explanation for them. This paper poses the multi-faceted set expansion as an optimization problem, where the goal is to compute multiple groups of entities that convey different aspects in an explainable manner, with high similarity within each group and diversity across groups. To extend a seed entity, we collect a large pool of candidate entities and facets (e.g., categories)from Wikipedia and knowledge bases, and construct a candidate graph. We propose FASETS, an efficient algorithm for computing faceted groups of bounded size, based on random walks over the candidate graph. Our extensive evaluation shows the superiority of FASETS against prior baselines, with regard to ground-truth collected from crowdsourcing.|计算给定种子实体的相关实体是探索性搜索和比较数据分析中的一项重要任务。先前的工作，使用基于种子的集合扩展范式，集中在单方面的识别具有高成对相关性的齐次集。最近的一些著作讨论了基于集群的方法来处理多方面的集合扩展，然而，它们未能利用集群的特殊性并对其作出解释。本文将多方面集合扩展作为一个最佳化问题，其目标是计算多组实体，这些实体以可解释的方式传达不同的方面，在每个组内具有高度的相似性，在不同组之间具有多样性。为了扩展种子实体，我们从 Wikipedia 和知识库中收集了大量的候选实体和方面(例如，类别) ，并构建了一个候选图。提出了一种基于候选图上随机游动的有界刻面群的高效算法 FASETS。我们的广泛评估表明，与先前的基线相比，FASETS 在众包收集的地面真相方面具有优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FASETS:+Discovering+Faceted+Sets+of+Entities)|0|
|[Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept Understanding](https://doi.org/10.1145/3589335.3651927)|Zezhong Fan, Xiaohan Li, Kaushiki Nag, Chenhao Fang, Topojoy Biswas, Jianpeng Xu, Kannan Achan||The rapid evolution of text-to-image diffusion models has opened the door of generative AI, enabling the translation of textual descriptions into visually compelling images with remarkable quality. However, a persistent challenge within this domain is the optimization of prompts to effectively convey abstract concepts into concrete objects. For example, text encoders can hardly express "peace", while can easily illustrate olive branches and white doves. This paper introduces a novel approach named Prompt Optimizer for Abstract Concepts (POAC) specifically designed to enhance the performance of text-to-image diffusion models in interpreting and generating images from abstract concepts. We propose a Prompt Language Model (PLM), which is initialized from a pre-trained language model, and then fine-tuned with a curated dataset of abstract concept prompts. The dataset is created with GPT-4 to extend the abstract concept to a scene and concrete objects. Our framework employs a Reinforcement Learning (RL)-based optimization strategy, focusing on the alignment between the generated images by a stable diffusion model and optimized prompts. Through extensive experiments, we demonstrate that our proposed POAC significantly improves the accuracy and aesthetic quality of generated images, particularly in the description of abstract concepts and alignment with optimized prompts. We also present a comprehensive analysis of our model's performance across diffusion models under different settings, showcasing its versatility and effectiveness in enhancing abstract concept representation.|文本-图像扩散模型的快速发展打开了生成式人工智能的大门，使得文本描述能够以非凡的质量翻译成视觉上引人注目的图像。然而，这个领域中的一个持续的挑战是优化提示，以有效地将抽象概念转换为具体对象。例如，文本编码器很难表达“和平”，而可以很容易地说明橄榄枝和白鸽。本文介绍了一种新的方法，即抽象概念的提示优化器(POAC) ，专门设计来提高性能的文本到图像扩散模型的解释和生成图像的抽象概念。我们提出了一个提示语言模型(PLM) ，它是从一个预先训练的语言模型初始化，然后用一个精选的抽象概念提示数据集进行微调。数据集是用 GPT-4创建的，用于将抽象概念扩展到场景和具体对象。我们的框架采用了一种基于强化学习的优化策略，通过一个稳定的扩散模型和优化的提示在生成的图像之间进行对齐。通过大量的实验，我们证明了我们提出的 POAC 显著提高了生成图像的准确性和美学质量，特别是在抽象概念的描述和优化提示对齐方面。我们还提出了我们的模型在不同设置下的扩散模型的性能的综合分析，展示了它的通用性和增强抽象概念表示的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prompt+Optimizer+of+Text-to-Image+Diffusion+Models+for+Abstract+Concept+Understanding)|0|
|[LLM-Guided Counterfactual Data Generation for Fairer AI](https://doi.org/10.1145/3589335.3651929)|Ashish Mishra, Gyanaranjan Nayak, Suparna Bhattacharya, Tarun Kumar, Arpit Shah, Martin Foltin||With the widespread adoption of deep learning-based models in practical applications, concerns about their fairness have become increasingly prominent. Existing research indicates that both the model itself and the datasets on which they are trained can contribute to unfair decisions. In this paper, we address the data-related aspect of the problem, aiming to enhance the data to guide the model towards greater trustworthiness. Due to their uncontrolled curation and limited understanding of fairness drivers, real-world datasets pose challenges in eliminating unfairness. Recent findings highlight the potential of Foundation Models in generating substantial datasets. We leverage these foundation models in conjunction with state-of-the-art explainability and fairness platforms to generate counterfactual examples. These examples are used to augment the existing dataset, resulting in a more fair learning model. Our experiments were conducted on the CelebA and UTKface datasets, where we assessed the quality of generated counterfactual data using various bias-related metrics. We observed improvements in bias mitigation across several protected attributes in the fine-tuned model when utilizing counterfactual data.|随着深度学习模型在实际应用中的广泛应用，对其公平性的关注日益突出。现有的研究表明，模型本身和数据集都可能导致不公平的决策。在本文中，我们处理与数据相关的问题，旨在增强数据，以指导模型朝着更大的可信度。由于不受控制的管理和对公平驱动因素的理解有限，现实世界的数据集在消除不公平方面提出了挑战。最近的发现强调了基础模型在生成大量数据集方面的潜力。我们利用这些基础模型结合最先进的可解释性和公平性平台来生成反事实的例子。这些例子被用来增加现有的数据集，从而产生一个更公平的学习模型。我们的实验是在 CelebA 和 UTKface 数据集上进行的，在那里我们使用各种与偏见相关的指标来评估所生成的反事实数据的质量。当使用反事实数据时，我们观察到在微调模型中几个受保护属性的偏差缓解方面的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLM-Guided+Counterfactual+Data+Generation+for+Fairer+AI)|0|
|[Only Send What You Need: Learning to Communicate Efficiently in Federated Multilingual Machine Translation](https://doi.org/10.1145/3589335.3651931)|YunWei Chu, DongJun Han, Christopher G. Brinton||Federated learning (FL) is a promising approach for solving multilingual tasks, potentially enabling clients with their own language-specific data to collaboratively construct a high-quality neural machine translation (NMT) model. However, communication constraints in practical network systems present challenges for exchanging large-scale NMT engines between FL parties. In this paper, we propose a meta-learning-based adaptive parameter selection methodology, MetaSend, that improves the communication efficiency of model transmissions from clients during FL-based multilingual NMT training. Our approach learns a dynamic threshold for filtering parameters prior to transmission without compromising the NMT model quality, based on the tensor deviations of clients between different FL rounds. Through experiments on two NMT datasets with different language distributions, we demonstrate that MetaSend obtains substantial improvements over baselines in translation quality in the presence of a limited communication budget.|联邦学习(FL)是解决多语言任务的一种有前途的方法，可以使客户利用自己的语言特定数据协同构建高质量的神经机器翻译(NMT)模型。然而，实际网络系统中的通信约束对 FL 各方之间交换大规模 NMT 引擎提出了挑战。本文提出了一种基于元学习的自适应参数选择方法 MetaSend，该方法提高了基于 FL 的多语种 NMT 训练过程中客户端模型传输的通信效率。基于客户端在不同 FL 轮之间的张量偏差，我们的方法在不影响 NMT 模型质量的情况下学习了在传输之前滤波参数的动态阈值。通过在两个不同语言分布的 NMT 数据集上的实验，我们证明了在通信预算有限的情况下，MetaSend 在翻译质量方面比基线有了很大的提高。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Only+Send+What+You+Need:+Learning+to+Communicate+Efficiently+in+Federated+Multilingual+Machine+Translation)|0|
|[Federated Learning in Large Model Era: Vision-Language Model for Smart City Safety Operation Management](https://doi.org/10.1145/3589335.3651939)|Zengxiang Li, Zhaoxiang Hou, Hui Liu, Tongzhi Li, Chengyi Yang, Ying Wang, Chao Shi, Longfei Xie, Weishan Zhang, Liang Xu, Zelei Liu||With the tremendous success of large language models such as ChatGPT, artificial intelligence has entered a new era of large models. Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. In this paper, we tackle the problem of building large vision-language intelligent models for specific industrial domains by leveraging the general large models and federated learning. We compare the challenges faced by federated learning in the era of small models and large models from different dimensions, and propose a technical framework for federated learning in the era of large models.Specifically, our framework mainly considers three aspects: heterogeneous model fusion, flexible aggregation methods, and data quality improvement. Based on this framework, we conduct a case study of leading enterprises contributing vision-language data and expert knowledge to city safety operation management. The preliminary experiments show that enterprises can enhance and accumulate their intelligence capabilities through federated learning, and jointly create an intelligent city model that provides high-quality intelligent services covering energy infrastructure security, residential community security and urban operation management.|随着 ChatGPT 等大型语言模型的巨大成功，人工智能已经进入了一个大型模型的新时代。多模态数据能够全面地感知和识别物理世界，已经成为通向一般人工智能的必由之路。然而，在公共数据集上训练的多模式大型模型在特定的工业领域往往表现不佳。在本文中，我们利用一般的大型模型和联邦学习来解决特定工业领域的大型视觉语言智能模型的构建问题。从不同维度比较了小模型时代和大模型时代联邦学习面临的挑战，提出了大模型时代联邦学习的技术框架。具体来说，我们的框架主要考虑三个方面: 异构模型融合、灵活的聚合方法和数据质量改进。基于此框架，本文对龙头企业为城市安全运营管理贡献视觉语言数据和专家知识进行了案例研究。初步实验表明，企业可以通过联邦学习提高和积累智能能力，共同创建智能城市模式，提供能源基础设施安全、住宅小区安全和城市运营管理等高质量的智能服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Federated+Learning+in+Large+Model+Era:+Vision-Language+Model+for+Smart+City+Safety+Operation+Management)|0|
|[Phoenix: A Federated Generative Diffusion Model](https://doi.org/10.1145/3589335.3651935)|Fiona Victoria Stanley Jothiraj, Afra Mashhadi||Generative AI has made impressive strides in enabling users to create diverse and realistic visual content such as images, videos, and audio. However, training generative models on large centralized datasets can pose challenges in terms of data privacy, security, and accessibility. Federated learning (FL) is an approach that uses decentralized techniques to collaboratively train a shared deep learning model while retaining the training data on individual edge devices to preserve data privacy. This paper proposes a novel method for training a Denoising Diffusion Probabilistic Model (DDPM) across multiple data sources using FL techniques. Diffusion models, a newly emerging generative model, show promising results in achieving superior quality images than Generative Adversarial Networks (GANs). Our proposed method Phoenix is an unconditional diffusion model that leverages strategies to improve the data diversity of generated samples even when trained on data with statistical heterogeneity or Non-IID (Non-Independent and Identically Distributed) data. We demonstrate how our approach outperforms the default diffusion model in an FL setting. These results indicate that high-quality samples can be generated by maintaining data diversity, preserving privacy, and reducing communication between data sources, offering exciting new possibilities in the field of generative AI.|生成式人工智能已经取得了令人印象深刻的进展，使用户能够创建多样化和真实的视觉内容，如图像，视频和音频。然而，在大型集中式数据集上训练生成模型可能会在数据隐私、安全性和可访问性方面带来挑战。联邦学习(FL)是一种利用分散技术协同训练共享深度学习模型，同时保留单个边缘设备上的训练数据以保护数据隐私的方法。提出了一种基于 FL 技术的多数据源去噪扩散概率模型(DDPM)训练方法。扩散模型是一种新兴的生成模型模型，它在获得比生成对抗网络(gANs)更高质量的图像方面显示出令人满意的结果。我们提出的方法菲尼克斯是一个无条件扩散模型，利用策略来改善生成的样本的数据多样性，即使训练的数据具有统计异质性或非 IID (非独立和同分布)数据。我们演示了我们的方法如何在 FL 设置中优于默认的扩散模型。这些结果表明，通过保持数据的多样性，保护隐私，减少数据源之间的通信，可以生成高质量的样本，为生成性人工智能领域提供了令人兴奋的新的可能性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Phoenix:+A+Federated+Generative+Diffusion+Model)|0|
|[LLM Driven Web Profile Extraction for Identical Names](https://doi.org/10.1145/3589335.3651946)|Prateek Sancheti, Kamalakar Karlapalem, Kavita Vemuri||The number of individuals having identical names on the internet is increasing. Thus making the task of searching for a specific individual tedious. The user must vet through many profiles with identical names to get to the actual individual of interest. The online presence of an individual forms the profile of the individual. We need a solution that helps users by consolidating the profiles of such individuals by retrieving factual information available on the web and providing the same as a single result. We present a novel solution that retrieves web profiles belonging to those bearing identical Full Names through an end-to-end pipeline. Our solution involves information retrieval from the web (extraction), LLM-driven Named Entity Extraction (retrieval), and standardization of facts using Wikipedia, which returns profiles with fourteen multi-valued attributes. After that, profiles that correspond to the same real-world individuals are determined. We accomplish this by identifying similarities among profiles based on the extracted facts using a Prefix Tree inspired data structure (validation) and utilizing ChatGPT's contextual comprehension (revalidation). The system offers varied levels of strictness while consolidating these profiles, namely strict, relaxed, and loose matching. The novelty of our solution lies in the innovative use of GPT -- a highly powerful yet an unpredictable tool, for such a nuanced task. A study involving twenty participants, along with other results, found that one could effectively retrieve information for a specific individual.|在互联网上拥有相同姓名的个人数量正在增加。因此，寻找特定个体的任务变得乏味。用户必须审查许多具有相同名称的配置文件，才能找到真正感兴趣的个人。个人的在线表现形成了个人的形象。我们需要一个解决方案，通过检索网络上可用的事实信息并提供同样的单一结果来整合这些个人的档案，从而帮助用户。我们提出了一种新的解决方案，通过端到端的管道检索属于那些具有相同全名的 Web 配置文件。我们的解决方案包括网络信息检索(提取)、 LLM 驱动的命名实体提取(检索) ，以及使用 Wikipedia 对事实进行标准化，后者返回具有十四个多值属性的配置文件。然后，确定与现实世界中的相同个体相对应的配置文件。我们通过使用前缀树启发的数据结构(验证)和利用 ChatGPT 的上下文理解(重新验证)来识别基于提取事实的概要文件之间的相似性。该系统在整合这些配置文件时提供了不同程度的严格性，即严格、松散和松散匹配。我们解决方案的新颖之处在于创新性地使用 GPT ——一种功能强大但不可预测的工具，可以完成如此微妙的任务。一项涉及二十名参与者的研究以及其他研究结果发现，人们可以有效地检索特定个体的信息。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLM+Driven+Web+Profile+Extraction+for+Identical+Names)|0|
|[Contrastive Disentanglement for Authorship Attribution](https://doi.org/10.1145/3589335.3652501)|Zhiqiang Hu, Thao Thanh Nguyen, Yujia Hu, ChiaYu Hung, Ming Shan Hee, ChunWei Seah, Roy KaWei Lee||Authorship Attribution (AA) seeks to determine the authorship of texts by examining distinctive writing styles. Although current AA methods have shown promising results, they often underperform in scenarios with significant topic shifts. This limitation arises from their inability to effectively separate topical content from the author's stylistic elements. Furthermore, most studies have focused on individual-level AA, overlooking the potential of regional-level AA to uncover linguistic patterns influenced by cultural and geographical factors. To bridge these gaps, this paper introduces ContrastDistAA, a novel framework that leverages contrastive learning and mutual information maximization to disentangle content and stylistic features in latent representations for AA. Our extensive experiments demonstrate that ContrastDistAA surpasses existing state-of-the-art models in both individual and regional-level AA tasks. This breakthrough not only improves the accuracy of authorship attribution but also broadens its applicability to include regional linguistic analysis, making a substantial contribution to the field of computational linguistics.|作者署名(AA)试图通过检查不同的写作风格来确定文本的作者身份。尽管目前的 AA 方法已经显示出有希望的结果，但是它们在具有显著主题转移的场景中往往表现不佳。这种局限性源于他们无法有效地将主题内容从作者的文体元素中分离出来。此外，大多数研究侧重于个人层面的 AA，忽视了区域层面的 AA 在揭示受文化和地理因素影响的语言模式方面的潜力。为了弥补这些差距，本文介绍了一个新的框架，利用对比学习和互信息最大化，以分离内容和风格特征的潜在表征的 AA。我们广泛的实验表明，在个人和区域层面的 AA 任务方面，对比度远远超过了现有的最先进的模型。这一突破不仅提高了作者归属的准确性，而且扩大了其适用范围，包括区域语言分析，对计算语言学领域做出了重大贡献。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Disentanglement+for+Authorship+Attribution)|0|
|[Large Language Models for Graph Learning](https://doi.org/10.1145/3589335.3641300)|Yujuan Ding, Wenqi Fan, Xiao Huang, Qing Li||Graphs are widely applied to encode entities with various relations in web applications such as social media and recommender systems. Meanwhile, graph learning-based technologies, such as graph neural networks, are demanding to support the analysis, understanding, and usage of the data in graph structures. Recently, the boom of language foundation models, especially Large Language Models (LLMs), has advanced several main research areas in artificial intelligence, such as natural language processing, graph mining, and recommender systems. The synergy between LLMs and graph learning holds great potential to prompt the research in both areas. For example, LLMs can facilitate existing graph learning models by providing high-quality textual features for entities and edges, or enhancing the graph data with encoded knowledge and information. It may also innovate with novel problem formulations on graph-related tasks. Due to the research significance as well as the potential, the convergent area of LLMs and graph learning has attracted considerable research attention. Therefore, we propose to hold the workshop Large Language Models for Graph Learning at WWW'24, in order to provide a venue to gather researchers in academia and practitioners in the industry to present the recent progress on relevant topics and exchange their critical insights.|图形被广泛应用于在诸如社交媒体和推荐系统等 Web 应用程序中对具有各种关系的实体进行编码。同时，基于图学习的技术，如图神经网络，对图结构中数据的分析、理解和使用提出了更高的要求。最近，语言基础模型，尤其是大语言模型(LLM)的兴起，推动了人工智能领域的几个主要研究领域，如自然语言处理、图形挖掘和推荐系统。LLM 和图形学习的协同作用对于推动这两个领域的研究具有巨大的潜力。例如，LLM 可以通过为实体和边提供高质量的文本特征，或者使用编码的知识和信息来增强图形数据，从而促进现有的图形学习模型。它也可以在图形相关的任务上创新新的问题公式。由于其研究意义和潜力，LLM 和图学习的收敛领域引起了人们的广泛关注。因此，我们建议在 WWW’24举办“图形学习的大语言模型”研讨会，以便为学术界的研究人员和业内从业人员提供一个聚集的场所，介绍相关主题的最新进展，并交流他们的批判性见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+for+Graph+Learning)|0|
|[Multi-Granularity Tibetan Textual Adversarial Attack Method Based on Masked Language Model](https://doi.org/10.1145/3589335.3652503)|Xi Cao, Nuo Qun, Quzong Gesang, Yulei Zhu, Trashi Nyima||In social media, neural network models have been applied to hate speech detection, sentiment analysis, etc., but neural network models are susceptible to adversarial attacks. For instance, in a text classification task, the attacker elaborately introduces perturbations to the original texts that hardly alter the original semantics in order to trick the model into making different predictions. By studying textual adversarial attack methods, the robustness of language models can be evaluated and then improved. Currently, most of the research in this field focuses on English, and there is also a certain amount of research on Chinese. However, there is little research targeting Chinese minority languages. With the rapid development of artificial intelligence technology and the emergence of Chinese minority language models, textual adversarial attacks become a new challenge for the information processing of Chinese minority languages. In response to this situation, we propose a multi-granularity Tibetan textual adversarial attack method based on masked language models called TSTricker. We utilize the masked language models to generate candidate substitution syllables or words, adopt the scoring mechanism to determine the substitution order, and then conduct the attack method on several fine-tuned victim models. The experimental results show that TSTricker reduces the accuracy of the classification models by more than 28.70% and makes the classification models change the predictions of more than 90.60% of the samples, which has an evidently higher attack effect than the baseline method.|在社交媒体中，神经网络模型被用于仇恨语音检测、情绪分析等，但是神经网络模型容易受到敌对攻击。例如，在一个文本分类任务中，攻击者精心地向原始文本引入扰动，这些扰动几乎不改变原始语义，以欺骗模型做出不同的预测。通过研究文本对抗性攻击方法，可以评估语言模型的鲁棒性，进而提高其鲁棒性。目前，该领域的研究大多集中在英语方面，也有一定数量的汉语方面的研究。然而，很少有针对中国少数民族语言的研究。随着人工智能技术的飞速发展和少数民族语言模型的出现，文本对抗性攻击成为少数民族语言信息处理面临的新挑战。针对这种情况，我们提出了一种基于掩蔽语言模型的多粒度藏文文本对抗攻击方法，称为 TSTricker。我们利用掩蔽语言模型生成候选替换音节或词，采用评分机制确定替换顺序，然后对几个微调的受害者模型进行攻击。实验结果表明，TSTricker 方法使分类模型的准确率降低了28.70% 以上，并使90.60% 以上的样本的预测值发生了变化，其攻击效果明显高于基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Granularity+Tibetan+Textual+Adversarial+Attack+Method+Based+on+Masked+Language+Model)|0|
|[Decoding Memes: A Comprehensive Analysis of Late and Early Fusion Models for Explainable Meme Analysis](https://doi.org/10.1145/3589335.3652504)|Faseela Abdullakutty, Usman Naseem||Memes are important because they serve as conduits for expressing emotions, opinions, and social commentary online, providing valuable insight into public sentiment, trends, and social interactions. By combining textual and visual elements, multi-modal fusion techniques enhance meme analysis, enabling the classification of offensive and sentimental memes effectively. Early and late fusion methods effectively integrate multi-modal data but face limitations. Early fusion integrates features from different modalities before classification. Late fusion combines classification outcomes from each modality after individual classification and reclassifies the combined results. This paper compares early and late fusion models in meme analysis. It showcases their efficacy in extracting meme concepts and classifying meme reasoning. Pre-trained vision encoders, including ViT and VGG-16, and language encoders such as BERT, AlBERT, and DistilBERT, were employed to extract image and text features. These features were subsequently utilized for performing both early and late fusion techniques. This paper further compares the explainability of fusion models through SHAP analysis. In comprehensive experiments, various classifiers such as XGBoost and Random Forest, along with combinations of different vision and text features across multiple sentiment scenarios, showcased the superior effectiveness of late fusion over early fusion.|模因之所以重要，是因为它们可以作为在线表达情感、观点和社会评论的管道，为公众情绪、趋势和社会互动提供有价值的洞察力。多模态融合技术将文本元素和视觉元素相结合，增强了模因分析能力，有效地对攻击性和感伤性模因进行了分类。早期和后期融合方法有效地整合了多模态数据，但面临局限性。早期融合在分类之前综合了来自不同模式的特征。后期融合结合了个体分类后每种模式的分类结果，并对组合结果进行了重新分类。本文对模因分析中的早期和晚期融合模型进行了比较。它在提取模因概念和分类模因推理方面显示了其有效性。采用事先训练好的视觉编码器(包括 ViT 和 VGG-16)和语言编码器(如 BERT、 AlBERT 和 DistilBERT)提取图像和文本特征。这些特征随后被用于执行早期和晚期融合技术。本文通过 SHAP 分析进一步比较了融合模型的可解释性。在综合实验中，各种分类器，如 XGBoost 和随机森林，以及不同视觉和文本特征在多种情感场景中的组合，显示了后期融合优于早期融合的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decoding+Memes:+A+Comprehensive+Analysis+of+Late+and+Early+Fusion+Models+for+Explainable+Meme+Analysis)|0|
|[SigBart: Enhanced Pre-training via Salient Content Representation Learning for Social Media Summarization](https://doi.org/10.1145/3589335.3652505)|Sajad Sotudeh, Nazli Goharian||Our approach to automatically summarizing online mental health posts could help counselors by reducing their reading time, enabling quicker and more effective support for individuals seeking mental health assistance. Neural text summarization methods demonstrate promising performance owing to their strong pre-training procedure. Random token/span masking technique is often relied upon by existing pre-trained language models; an approach that overlooks the importance of content when learning word representations. In an attempt to rectify this, we propose using source and summary alignments as a saliency signal to enhance the pre-training strategy of language model for better representation learning of important content, paving the way for a positive impact on the model fine-tuning phase. Our experiments on a mental health-related dataset for user post summarization MentSum reveal improved performance, as evidenced by human evaluation metrics, surpassing the current state-of-the-art system.|我们自动总结在线心理健康帖子的方法可以帮助咨询师减少他们的阅读时间，为寻求心理健康援助的个人提供更快、更有效的支持。神经网络文本摘要方法由于其强大的预训练过程而表现出良好的性能。随机标记/跨度掩蔽技术通常依赖于现有的预先训练的语言模型; 这种方法在学习单词表示时忽略了内容的重要性。为了解决这一问题，我们提出以源和摘要对齐作为显著性信号，加强语言模型的预训练策略，以更好地表示重要内容，为模型微调阶段的积极影响铺平道路。我们在一个与心理健康相关的数据集上进行的实验显示，MentSum 的性能得到了改善，人类评估指标证明了这一点，超过了目前最先进的系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SigBart:+Enhanced+Pre-training+via+Salient+Content+Representation+Learning+for+Social+Media+Summarization)|0|
|[An Investigation into the Feasibility of Performing Federated Learning on Social Linked Data Servers](https://doi.org/10.1145/3589335.3651950)|Nayil Arana, Mohamed Ragab, Thanassis Tiropanis||Federated Learning (FL) and the Social Linked Data (\textttSolid ~\footnotehttps://solidproject.org/ ) framework represent decentralized approaches to machine learning and web development, respectively, with a focus on preserving privacy. Federated learning enables the distributed training of machine learning models across datasets partitioned across multiple clients, whereas applications developed with the Solid approach store data inPersonal Online Data Stores (pods) under the control of individual users. This paper discusses the merits and challenges of executing Federated Learning on Solid pods and the readiness of the Solid server architecture to support this. We aim to detail these challenges, in addition to identifying avenues for further work to fully harness the benefits of Federated Learning in Solid environments, where users retain sovereignty over their data.|联邦学习(FL)和社会关联数据(textttSolid ~ foonote https:// solidproject.org/)框架分别代表了机器学习和网络开发的分散方法，侧重于保护隐私。联合学习使机器学习模型的分布式训练跨越多个客户端分区的数据集，而应用程序开发的固体方法存储数据在个人在线数据存储(吊舱)在个人用户的控制下。本文讨论了在 Solid pods 上执行 Federated Learning 的优点和挑战，以及 Solid 服务器体系结构支持这一点的准备情况。我们的目标是详细说明这些挑战，除了确定进一步工作的途径，以充分利用联邦学习在实体环境中的好处，其中用户保留对其数据的主权。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Investigation+into+the+Feasibility+of+Performing+Federated+Learning+on+Social+Linked+Data+Servers)|0|
|[Detecting Financial Bots on the Ethereum Blockchain](https://doi.org/10.1145/3589335.3651959)|Thomas Niedermayer, Pietro Saggese, Bernhard Haslhofer||The integration of bots in Distributed Ledger Technologies (DLTs) fosters efficiency and automation. However, their use is also associated with predatory trading and market manipulation, and can pose threats to system integrity. It is therefore essential to understand the extent of bot deployment in DLTs; despite this, current detection systems are predominantly rule-based and lack flexibility. In this study, we present a novel approach that utilizes machine learning for the detection of financial bots on the Ethereum platform. First, we systematize existing scientific literature and collect anecdotal evidence to establish a taxonomy for financial bots, comprising 7 categories and 24 subcategories. Next, we create a ground-truth dataset consisting of 133 human and 137 bot addresses. Third, we employ both unsupervised and supervised machine learning algorithms to detect bots deployed on Ethereum. The highest-performing clustering algorithm is a Gaussian Mixture Model with an average cluster purity of 82.6 classification is a Random Forest with an accuracy of 83 learning-based detection mechanism contributes to understanding the Ethereum ecosystem dynamics by providing additional insights into the current bot landscape.|分布式分类账技术(DLT)中机器人的集成促进了效率和自动化。然而，它们的使用也与掠夺性交易和市场操纵有关，并可能对系统的完整性构成威胁。因此，了解 DLT 中 bot 部署的程度是至关重要的; 尽管如此，目前的检测系统主要是基于规则的，缺乏灵活性。在这项研究中，我们提出了一种新的方法，利用机器学习检测金融机器人在以太网平台。首先，我们对现有的科学文献进行系统化整理，收集轶事证据，为金融机器人建立一个分类，包括7个类别和24个子类别。接下来，我们创建一个由133个人类和137个机器人地址组成的地面真相数据集。第三，我们同时使用无监督算法和监督式学习算法来检测部署在以太空的机器人。最高性能的聚类算法是高斯混合模型，平均聚类纯度为82.6分类是一个随机森林，具有83个基于学习的检测机制的准确性，通过提供对当前机器人景观的额外见解，有助于理解以太坊生态系统动态。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Financial+Bots+on+the+Ethereum+Blockchain)|0|
|[Measuring Arbitrage Losses and Profitability of AMM Liquidity](https://doi.org/10.1145/3589335.3651961)|Robin Fritsch, Andrea Canidio||This paper presents the results of a comprehensive empirical study of losses to arbitrageurs (following the formalization of loss-versus-rebalancing by [Milionis et al., 2022]) incurred by liquidity on automated market makers (AMMs). Through a systematic comparison between historical earnings from trading fees and losses to arbitrageurs, our findings indicate an insufficient compensation from fees for arbitrage losses across many of the largest AMM liquidity pools (on Uniswap). Remarkably, we identify a higher profitability among less capital-efficient Uniswap v2 pools compared to their Uniswap v3 counterparts. Moreover, we investigate a possible LVR mitigation by quantifying how arbitrage losses reduce with shorter block times. We observe notable variations in the manner of decline of arbitrage losses across different trading pairs. For instance, when comparing 100ms block times to Ethereum's current 12-second block times, the decrease in losses to arbitrageurs ranges between 20|本文提出了一个全面的实证研究结果的损失，套利者(后损失与再平衡的正式化[米利奥尼斯等人，2022])所招致的流动性在自动市场做市商(AMM)。通过对交易费用和套利者损失的历史收益进行系统比较，我们的研究结果表明，许多最大的 AMM 流动性池(Uniswap)的套利损失费用补偿不足。值得注意的是，与 Uniswap v3相比，我们发现资本效率较低的 Uniswap v2池有更高的盈利能力。此外，我们研究了一种可能的 LVR 缓解通过量化如何减少套利损失与较短的阻塞时间。我们观察到不同交易对之间套利损失下降方式的显著差异。例如，将100毫秒的阻塞时间与 Ethereum 目前的12秒阻塞时间相比，套利者损失的减少幅度在20之间|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Measuring+Arbitrage+Losses+and+Profitability+of+AMM+Liquidity)|0|
|[Anonymity Analysis of the Umbra Stealth Address Scheme on Ethereum](https://doi.org/10.1145/3589335.3651963)|Alex Márk Kovács, István András Seres||Stealth addresses are a privacy-enhancing technology that provides recipient anonymity on blockchains. In this work, we investigate the recipient anonymity and unlinkability guarantees of Umbra, the most widely used implementation of the stealth address scheme on Ethereum, and its three off-chain scalability solutions, e.g., Arbitrum, Optimism, and Polygon. We define and evaluate four heuristics to uncover the real recipients of stealth payments. We find that for the majority of Umbra payments, it is straightforward to establish the recipient, hence nullifying the benefits of using Umbra. Specifically, we find the real recipient of $48.5\%$, $25.8\%$, $65.7\%$, and $52.6\%$ of all Umbra transactions on the Ethereum main net, Polygon, Arbitrum, and Optimism networks, respectively. Finally, we suggest easily implementable countermeasures to evade our deanonymization and linking attacks.|隐藏地址是一种隐私增强技术，它在区块链上为接收者提供匿名性。在这项工作中，我们调查了 Umbra 的接收者匿名性和不可链接性保证，这是以太网上使用最广泛的隐形地址方案的实现，以及它的三个脱链可伸缩性解决方案，如仲裁、乐观和 Polygon。我们定义并评估四种启发式方法，以发现隐形支付的真正接受者。我们发现，对于大多数的 Umbra 付款，它是直接建立接受者，因此取消了使用 Umbra 的好处。具体来说，我们发现所有 Umbra 交易的真正接受者分别是以太网(Etherum main net)、 Polygon、仲裁(Arbitrum)和乐观主义网络(Optimm network)的48.5% 、25.8% 、65.7% 和52.6% 。最后，我们提出了易于实施的对策来规避我们的去匿名化和链接攻击。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Anonymity+Analysis+of+the+Umbra+Stealth+Address+Scheme+on+Ethereum)|0|
|[Seamlessly Transferring Assets through Layer-0 Bridges: An Empirical Analysis of Stargate Bridge's Architecture and Dynamics](https://doi.org/10.1145/3589335.3651964)|Chuanshan Huang, Tao Yan, Claudio J. Tessone||The increasing number of distinct blockchains has led to a growing need for data exchange and asset transfer across various isolated blockchains. To address this, cross-chain bridges have emerged as a critical mechanism for enabling interoperability and facilitating data and asset exchange across diverse blockchains. Among these bridges, the Layer-0 bridge stands out as a scalability solution that enhances blockchain performance at the foundational layer of data transition, without altering the blockchain's structure. Stargate is a notable Layer-0 Lock-and-Unlock cross-chain bridge that supports transactions across various EVM-based blockchains, with the highest Total Value Locked (TVL) among cross-chain bridges of the same kind. While previous cross-chain research has primarily focused on Layer-2 bridges, this study specifically examines Stargate and analyzes its dynamics as well as potential vulnerabilities. We collect transaction data of Stargate on six blockchains including Ethereum, Polygon, Binance Smart Chain, Avalanche, Arbitrum and Optimism. Our findings reveal the transaction patterns and evidence of exploitations of Stargate by investigating its transaction dynamics over time.|不同区块链的数量不断增加，导致对数据交换和跨各种孤立区块链的资产转移的需求日益增长。为了解决这个问题，跨链桥已经成为实现互操作性和促进跨不同区块链的数据和资产交换的关键机制。在这些桥中，第0层桥脱颖而出，作为一个可伸缩性解决方案，在数据转换的基础层提高了区块链性能，而不改变区块链的结构。Stargate 是一个著名的 Layer-0 Lock-and-Unlock 跨链桥梁，支持跨各种基于 EVM 的区块链的事务，其总价值锁定(TVL)是同类跨链桥梁中最高的。虽然以前的跨链研究主要集中在第二层桥梁，这项研究具体检查星际之门，并分析其动力学以及潜在的脆弱性。我们收集星际之门在六个区块链上的交易数据，包括以太、多边形、比宁斯智能链、雪崩、仲裁和乐观。我们的发现揭示了交易模式和利用星际之门的证据，通过调查其交易动态随着时间的推移。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Seamlessly+Transferring+Assets+through+Layer-0+Bridges:+An+Empirical+Analysis+of+Stargate+Bridge's+Architecture+and+Dynamics)|0|
|[Understanding, Leveraging, and Improving Large Language Models](https://doi.org/10.1145/3589335.3653009)|Soujanya Poria||The emergence of Large Language Models (LLMs) has marked a substantial advancement in Natural Language Processing (NLP), contributing significantly to enhanced task performance both within and outside specific domains. However, amidst these achievements, three key questions remain unanswered: 1) The mechanism through which LLMs accomplish their tasks and their limitations, 2) Effectively harnessing the power of LLMs across diverse domains, and 3) Strategies for enhancing the performance of LLMs. This talk aims to delve into our research group's endeavors to address these pivotal questions. Firstly, I will outline our approach, which involves utilizing ontology-guided prompt perturbations to unravel the primary limitations of LLMs in solving mathematical problems. Moving on to the second question, we will explore the utilization of synthetic data generated by LLMs to bolster challenging downstream tasks, particularly focusing on structured prediction where LLMs face persistent challenges. I will elaborate on our initiatives aimed at improving LLMs by incorporating highly effective retrieval strategies, specifically addressing the prevalent challenge of hallucinations that often plagues contemporary LLMs. Finally, I will present a technique on LLM realignment to restore safety lost during fine-tuning.|大语言模型(LLM)的出现标志着自然语言处理(NLP)的重大进步，对提高特定领域内外的任务性能作出了显著贡献。然而，在这些成就中，有三个关键问题仍然没有得到解答: 1) LLM 完成任务的机制及其局限性，2)在不同领域有效地利用 LLM 的力量，3)提高 LLM 性能的策略。本演讲旨在深入探讨我们研究小组为解决这些关键问题所做的努力。首先，我将概述我们的方法，其中涉及到利用本体引导的快速扰动来揭示 LLM 在解决数学问题时的主要局限性。继续第二个问题，我们将探讨如何利用 LLM 生成的合成数据来支持具有挑战性的下游任务，特别是关注 LLM 面临持续挑战的结构化预测。我将详细阐述我们旨在通过纳入高效的检索策略来改善 LLM 的举措，特别是解决经常困扰当代 LLM 的幻觉的普遍挑战。最后，我将介绍一种 LLM 重新校准技术，以恢复微调过程中的安全损失。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding,+Leveraging,+and+Improving+Large+Language+Models)|0|
|[Love-Hate Dataset: A Multi-Modal Multi-Platform Dataset Depicting Emotions in the 2023 Israel-Hamas War](https://doi.org/10.1145/3589335.3651966)|Lynnette Hui Xian Ng, Adrian Xuan Wei Lim, Roy KaWei Lee||War brings about strong feelings of hate, and showcases the love of humanity. During war, social media is utilized for citizen journalism, supply organization and activism, but also for people to express their emotions. In this paper, we present a multi-modal multi-platform dataset that depicts the expression of love and hate towards both sides of the Israel-Hamas War. This dataset presents posts in English from Facebook and Instagram that contain the terms "love" or "hate" in the context of the war during 7 October 2023 (onset of war) to 31 December 2023. We find that over time, the number of posts on the war decreased, suggesting interest in the war has waned; posts about Love reference religion while posts about Hate references hostility; and emojis in Love posts represent hearts, peace and listening while emojis in Hate posts represent being watched, sadness and warning. Finally, we generated Instagram posts with GPT4-V using our dataset as a reference, and the model returned posts of generic love messages with art-form images. We hope our dataset is useful to researchers studying multi-modal and multi-platform information and emotions on social media during a war.|战争带来强烈的仇恨感，展示了人性的爱。在战争期间，社会媒体被用于公民新闻、供应组织和行动主义，但也用于人们表达他们的情感。在本文中，我们提出了一个多模态多平台的数据集，描述了对以色列和哈马斯战争双方的爱和恨的表达。这个数据集提供了来自 Facebook 和 Instagram 的英文帖子，其中包含了2023年10月7日(战争开始)至2023年12月31日期间战争背景下的“爱”或“恨”。我们发现，随着时间的推移，关于战争的帖子数量逐渐减少，表明人们对战争的兴趣已经减弱; 关于爱的帖子提到宗教，而关于恨的帖子提到敌意; 爱的帖子中的表情符号代表心灵、和平和倾听，而关于恨的帖子中的表情符号代表被关注、悲伤和警告。最后，我们使用我们的数据集作为参考，用 GPT4-V 生成 Instagram 帖子，模型用艺术形式的图像返回通用爱情信息的帖子。我们希望我们的数据集对于研究战争期间社会媒体上多模态和多平台的信息和情感有所帮助。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Love-Hate+Dataset:+A+Multi-Modal+Multi-Platform+Dataset+Depicting+Emotions+in+the+2023+Israel-Hamas+War)|0|
|[Textual Context guided Vision Transformer with Rotated Multi-Head Attention for Sentiment Analysis](https://doi.org/10.1145/3589335.3651968)|Chhavi Dhiman, Gaurav Kumar||Social media multimodal sentiment analysis has proliferated the research attention of the research community, as it opens up various paradigms for social issues such as cyberbullying, hate speech, healthcare, politics, business analysis and many more. At the same time, it is an open problem to learn the intrinsic representation of multiple modalities in order to identify correlated patterns. In the proposed work a sentiment analysis framework is presented that defines Textual Context guided Vision Transformer with Rotated Multi-Head Attention, in order to exploit correlation between image-text pair and mine rich discriminatory features for multimodal sentiment analysis. A novel Rotated-Multi-head attention mechanism is defined that translates the visual or text embeddings in distinct feature space resulting in Adaptively Rotated Refined Embedding (ARREmb). To exhibit the performance of the proposed work, extensive experiments are carried out on three publicly available datasets-BG, Twitter and MVSA-single dataset, in terms of Precision, Recall, F1-score and accuracy. The experiments support superior performance of the proposed approach by laying out comparison with SOTA followed by ablation study.|社交媒体多模态情绪分析已经激发了研究界的研究热情，因为它为网络欺凌、仇恨言论、医疗保健、政治、商业分析等社会问题开辟了各种范式。同时，为了识别相关模式，学习多模式的内在表征是一个开放的问题。本文提出了一种情绪分析框架，该框架定义了文本上下文引导的旋转多头注意视觉变换器，以利用图像-文本对和矿井丰富的识别特征之间的相关性进行多模态情绪分析。定义了一种新的旋转多头注意机制，将视觉或文本嵌入转换为不同的特征空间，产生自适应旋转精细嵌入(ARREmb)。为了展示所提出的工作的性能，在三个公开可用的数据集-BG，Twitter 和 MVSA-单个数据集上进行了广泛的实验，在精度，召回，F1得分和准确性方面。通过与 SOTA 进行比较并进行烧蚀研究，实验结果支持了该方法的优越性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Textual+Context+guided+Vision+Transformer+with+Rotated+Multi-Head+Attention+for+Sentiment+Analysis)|0|
|[A Novel Dual-Pipeline based Attention Mechanism for Multimodal Social Sentiment Analysis](https://doi.org/10.1145/3589335.3651967)|Ali Braytee, Andy ShuehChih Yang, Ali Anaissi, Kunal Chaturvedi, Mukesh Prasad||Traditionally, sentiment analysis methods rely solely on text or image data. However, most user-generated social media content includes both textual and image content. In this study, we propose a novel Dual-Pipeline based Attentional method that uses different modalities of data, including text and images, to analyse and interpret emotions and sentiments expressed in tweets. Our proposed method simultaneously extracts meaningful local and global contextual features from multiple modalities. Local fusion layers within each pipeline combine modality-specific features using an attention mechanism to enrich the joint multimodal representation. A global fusion layer consolidates the collective sentiment representation by seamlessly intermixing the outputs of both pipelines. We evaluate our proposed method using performance metrics such as accuracy and F1-score. Through extensive experimentation on the MVSA dataset, our method demonstrates superior performance compared to state-of-the-art techniques in identifying the sentiment conveyed in social media data.|传统上，情感分析方法只依赖于文本或图像数据。然而，大多数用户生成的社交媒体内容包括文本内容和图像内容。在这项研究中，我们提出了一个新颖的双管道注意方法，使用不同形式的数据，包括文本和图像，分析和解释情绪和情绪表达的推文。我们提出的方法同时提取有意义的局部和全局上下文特征的多模式。每条管道内的局部融合层使用注意机制结合特定模态特征来丰富联合多模态表示。全局融合层通过无缝地混合两个管道的输出来巩固集体情感表达。我们评估我们提出的方法使用性能指标，如准确性和 F1得分。通过在 MVSA 数据集上的大量实验，我们的方法在识别社交媒体数据中传达的情绪方面比最先进的技术表现出更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Novel+Dual-Pipeline+based+Attention+Mechanism+for+Multimodal+Social+Sentiment+Analysis)|0|
|[Unraveling the Tangle of Disinformation: A Multimodal Approach for Fake News Identification on Social Media](https://doi.org/10.1145/3589335.3651972)|Junaid Rashid, Jungeun Kim, Anum Masood||The growth of interactive and multimedia content on the Internet has made it an essential news source for people worldwide. Social media is a platform for sharing information and facilitates the spread of fake news. The dissemination of disinformation on social media has a significant impact on society. Conventional methods used in the identification of fake news often struggle to analyze textual, visual, and combined aspects of news shared on social media. Therefore, we propose the Multimodal Approach for Fake News Identification (MuAFaNI), which uses a combined representation of text and images to assess news authenticity as fake or real. MuAFaNI uses the RoBERTa language model for text analysis and ResNet-50 for image analysis. Experiments on two prominent social media datasets, Twitter and Weibo, showed that MuAFaNI performed better than state-of-the-art fake news techniques in terms of accuracy, precision, recall and F1 score.|互联网上互动和多媒体内容的增长使其成为全世界人民的重要新闻来源。社交媒体是一个分享信息的平台，有利于假新闻的传播。在社交媒体上传播虚假信息对社会有重大影响。用于识别假新闻的传统方法往往难以分析社交媒体上分享的新闻的文本、视觉和综合方面。因此，我们提出了虚假新闻识别的多模态方法(MuAFaNI) ，该方法利用文本和图像的组合表示来评估新闻的真实性是假的还是真的。MuAFNI 使用 RoBERTa 语言模型进行文本分析，使用 ResNet-50进行图像分析。在两个著名的社交媒体数据集 Twitter 和 Weibo 上进行的实验表明，MuAFANI 在准确性、精确度、召回率和 F1得分方面都优于最先进的假新闻技术。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unraveling+the+Tangle+of+Disinformation:+A+Multimodal+Approach+for+Fake+News+Identification+on+Social+Media)|0|
|[RUHate-MM: Identification of Hate Speech and Targets using Multimodal Data from Russia-Ukraine Crisis](https://doi.org/10.1145/3589335.3651973)|Surendrabikram Thapa, Farhan Ahmad Jafri, Kritesh Rauniyar, Mehwish Nasim, Usman Naseem||During the conflict between Ukraine and Russia, hate speech targeted toward specific groups was widespread on different social media platforms. With most social platforms allowing multimodal content, the use of multimodal content to express hate speech is widespread on the Internet. Although there has been considerable research in detecting hate speech within unimodal content, the investigation into multimodal content remains insufficient. The limited availability of annotated multimodal datasets further restricts our ability to explore new methods to interpret and identify hate speech and its targets. The availability of annotated datasets for hate speech detection during political events, such as invasions, are even limited. To fill this gap, we introduce a comprehensive multimodal dataset consisting of 20,675 posts related to the Russia-Ukraine crisis, which were manually annotated as either 'Hate Speech' or 'No Hate Speech'. Additionally, we categorize the hate speech data into three targets: 'Individual', 'Organization', and 'Community'. Our benchmarked evaluations show that there is still room for improvement in accurately identifying hate speech and its targets. We hope that the availability of this dataset and the evaluations performed on it will encourage the development of new methods for identifying hate speech and its targets during political events like invasions and wars. The dataset and resources are made available at https://github.com/Farhan-jafri/Russia-Ukraine.|在乌克兰和俄罗斯冲突期间，针对特定群体的仇恨言论在不同的社交媒体平台上广泛传播。由于大多数社交平台允许多式联运内容，使用多式联运内容来表达仇恨言论在互联网上十分普遍。尽管在检测单通道内容中的仇恨言论方面已有相当多的研究，但对多通道内容的调查仍然不够。注释多模式数据集的有限可用性进一步限制了我们探索新方法来解释和识别仇恨言论及其目标的能力。在诸如入侵等政治事件期间，用于检测仇恨言论的注释数据集的可用性甚至是有限的。为了填补这一空白，我们引入了一个全面的多模式数据集，其中包括20,675个与俄罗斯-乌克兰危机有关的帖子，这些帖子被手动注释为“仇恨言论”或“无仇恨言论”。此外，我们将仇恨言论数据分为三个目标: “个人”、“组织”和“社区”。我们的基准评价表明，在准确识别仇恨言论及其目标方面仍有改进余地。我们希望，这一数据集的提供和对其进行的评估将鼓励开发新的方法，以便在入侵和战争等政治事件期间查明仇恨言论及其目标。数据集和资源可在 https://github.com/farhan-jafri/russia-ukraine 获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RUHate-MM:+Identification+of+Hate+Speech+and+Targets+using+Multimodal+Data+from+Russia-Ukraine+Crisis)|0|
|[Unveiling Misogyny Memes: A Multimodal Analysis of Modality Effects on Identification](https://doi.org/10.1145/3589335.3651974)|Shijing Chen, Usman Naseem, Imran Razzak, Flora D. Salim||In today's digital era, memes have become a popular means of communication that often reflect societal attitudes as well as prejudices. Misogyny memes are a form of memes that explicitly discriminate against women in various aspects, such as shaming or stereotyping. This research aims to identify misogynous memes through deep learning multimodal analysis and determine which modality, text or image, plays a more significant role in fairness considerations. To achieve this, we utilized the dataset GOAT-benchmarks, which comprises over 6,000 diverse memes covering topics like implicit hate speech, sexism, and cyberbullying. Furthermore, we evaluated the fairness of these models by assessing their performance across different demographic groups. Our findings revealed that while both text and image modalities contribute to identifying misogynous memes, text plays a significant role in misogyny identification, while image contributes further in terms of fairness. This study emphasizes the importance of multimodal analysis in recognizing and mitigating biases in online content. Disclaimer: This paper contains content that may be disturbing to some readers.|在今天的数字时代，模因已经成为一种流行的交流方式，经常反映社会态度和偏见。厌女症模因是一种明显歧视妇女的模因形式，在各个方面，如羞辱或成见。本研究旨在通过深度学习多模态分析来识别歧视女性的模因，并确定哪种情态、文本或图像在公平考虑中发挥更重要的作用。为了实现这一点，我们使用了 GOAT 基准数据集，它包含了超过6000种不同的文化基因，涵盖了诸如隐性仇恨言论、性别歧视和网络欺凌等主题。此外，我们通过评估这些模型在不同人群中的表现来评估它们的公平性。我们的研究结果表明，文本和图像模式都有助于识别歧视女性的模因，文本在歧视女性的识别中起着重要的作用，而图像在公平性方面起着进一步的作用。本研究强调了多模态分析在识别和减轻网络内容偏差方面的重要性。免责声明: 本文包含的内容可能会令一些读者感到不安。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+Misogyny+Memes:+A+Multimodal+Analysis+of+Modality+Effects+on+Identification)|0|
|[Ensemble Pretrained Models for Multimodal Sentiment Analysis using Textual and Video Data Fusion](https://doi.org/10.1145/3589335.3651971)|Zhicheng Liu, Ali Braytee, Ali Anaissi, Guifu Zhang, Lingyun Qin, Junaid Akram||We introduce an ensemble model approach for multimodal sentiment analysis, focusing on the fusion of textual and video data to enhance the accuracy and depth of emotion interpretation. By integrating three foundational models-IFFSA, BFSA, and TBJE-using advanced ensemble techniques, we achieve a significant improvement in sentiment analysis performance across diverse datasets, including MOSI and MOSEI. Specifically, we propose two novel models-IFFSA and BFSA, which utilise the large language models BERT and GPT-2 to extract the features from text modality and ResNet and VGG for video modality. Our work uniquely contributes to the field by demonstrating the synergistic potential of combining different modal analytical strengths, thereby addressing the intricate challenge of nuanced emotion detection in multimodal contexts. Through comprehensive experiments and an extensive ablation study, we not only validate the superior performance of our ensemble model against current state-of-the-art benchmarks but also reveal critical insights into the model's capability to discern complex emotional states. Our findings underscore the strategic advantage of ensemble methods in multimodal sentiment analysis and set a new precedent for future research in effectively integrating multimodal data sources.|本文提出了一种多模态情绪分析的集成模型方法，着重于文本和视频数据的融合，以提高情绪解释的准确性和深度。通过集成三个基础模型-IFFSA，BFSA 和 TBJE-使用先进的集合技术，我们实现了情绪分析性能在不同数据集，包括 MOSI 和 MOSEI 的显着改善。具体来说，我们提出了两种新的模型-IFFSA 和 BFSA，它们利用大语言模型 BERT 和 GPT-2提取文本模式和视频模式的 ResNet 和 VGG 特征。我们的工作独一无二地为该领域做出了贡献，展示了结合不同模态分析优势的协同潜力，从而解决了在多模态情境下微妙情绪检测的复杂挑战。通过全面的实验和广泛的消融研究，我们不仅验证了我们的集合模型对当前最先进的基准的优越性能，而且揭示了模型辨别复杂情绪状态的能力的关键见解。我们的研究结果强调了集合方法在多模态情绪分析中的战略优势，并为今后有效整合多模态数据源的研究开创了一个新的先例。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ensemble+Pretrained+Models+for+Multimodal+Sentiment+Analysis+using+Textual+and+Video+Data+Fusion)|0|
|[AI Deepfakes on the Web: The 'Wicked' Challenges for AI Ethics, Law and Technology](https://doi.org/10.1145/3589334.3649116)|Jeannie Marie Paterson||Advances in generative AI and the increasingly easy availability of tools for creating text, code, audio, and images have impacted almost all industry sectors, promising new efficiencies and changing work patterns. The darker side of this same technology is the problematic case of deepfakes created by AI and spread online to humiliate, manipulate, trick, or defraud ordinary individuals and public figures. Transparency, fairness, and beneficence are vital values of responsible and ethical AI. All of these values would preclude harmful uses of AI deep fakes. However, harmful deepfakes are usually the work of fraudsters with little regard for ethics and beyond the reach of the law. So, who should be responsible? Arguably, principles of responsible AI require tech companies and digital platforms to take responsibility for reducing harmful uses of deepfakes. These entities are gatekeepers to the creation and distribution of deepfakes. Therefore, they are ethically obligated to respond to the foreseeable consequential harms arising from generative AI. Increasingly, this is the response of lawmakers. Gatekeeper responsibility envisages that tech producers and platforms will proactively invest in technical solutions to harmful deepfakes, such as watermarking, finetuning, red teaming or automated content moderation, and proactive take-down responses. This response is compelling and might seem straightforward. As always, the details are more complex. The efficiency of the proposed technical responses is still emerging. They raise as yet unaddressed implications for smaller providers and the relations between tech companies and digital platforms. Moreover, even beginning to respond to online deepfakes requires social policy decisions that assess and weigh incommensurable considerations, including retaining trust on the Web, keeping vulnerable groups safe, preserving free speech and creativity, and not stifling the development of potentially beneficial technology. This presentation addresses these problematic choices in responding to the 'wicked' challenge of AI deepfakes on the Web. It proposes a networked response to the problem, embracing multiple relevant actors and influences.|生成性人工智能的进步以及创建文本、代码、音频和图像的工具越来越容易获得，已经影响了几乎所有的行业部门，有望带来新的效率和不断变化的工作模式。这种技术的阴暗面是由人工智能创造的深度伪造的问题案例，它在网上传播，以羞辱、操纵、欺骗或欺骗普通个人和公众人物。透明度、公平和仁慈是负责任和有道德的人工智能的重要价值观。所有这些价值观将排除人工智能深度伪造的有害使用。然而，有害的深度伪造通常是欺诈者的工作，他们很少考虑道德问题，也超出了法律的范围。那么，谁该为此负责呢？可以说，负责任的人工智能原则要求科技公司和数字平台承担责任，减少“深度假”的有害使用。这些实体是创造和发行深度赝品的看门人。因此，他们在伦理上有义务应对可预见的由生殖性人工智能引起的后果性损害。这越来越成为立法者的回应。“看门人”的责任设想，技术生产商和平台将主动投资于有害的“深度赝品”的技术解决方案，如水印、微调、红色团队或自动内容审核，以及主动撤下反应。这种反应是令人信服的，看起来可能是直截了当的。一如既往，细节更为复杂。拟议技术应对措施的效率仍在提高。它们对规模较小的供应商以及科技公司与数字平台之间的关系产生了尚未解决的影响。此外，即使是开始应对网络“深度造假”，也需要社会政策决策来评估和权衡不可比拟的因素，包括保持网络上的信任、保护弱势群体的安全、保护言论自由和创造力，以及不抑制潜在有益技术的发展。这个演示解决了这些问题的选择，以回应“邪恶”的挑战，人工智能在网络上的深度伪造。它提出了一个网络对问题的反应，包括多个相关的行为者和影响。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI+Deepfakes+on+the+Web:+The+'Wicked'+Challenges+for+AI+Ethics,+Law+and+Technology)|0|
|[Challenges Toward AGI and Its Impact to the Web](https://doi.org/10.1145/3589334.3649113)|Bo Zhang, Jie Tang||Large language models have substantially advanced the state of the art in various AI tasks, such as natural language understanding and text generation, and image processing, and multimodal modeling. In this talk, we will first introduce the development of AI in the past decades, in particular from the angle of China. We will also talk about the opportunities, challenges, and risks of AGI in the future, and its impact on the Web. In the second part of the talk, we will use ChatGLM, an alternative but open sourced model to ChatGPT, as an example to explain our understandings and insights derived during the implementation of the model.|大型语言模型大大提高了各种人工智能任务的技术水平，如自然语言理解和文本生成、图像处理和多模态建模。在这次演讲中，我们将首先介绍人工智能在过去几十年的发展，特别是从中国的角度。我们还将讨论 AGI 在未来的机遇、挑战和风险，以及它对 Web 的影响。在演讲的第二部分，我们将使用 ChatGPT 的另一个开源模型 ChatGLM 作为一个例子来解释我们在模型实现过程中得到的理解和见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Challenges+Toward+AGI+and+Its+Impact+to+the+Web)|0|
|[Budget-Constrained Auctions with Unassured Priors: Strategic Equivalence and Structural Properties](https://doi.org/10.1145/3589334.3645344)|Zhaohua Chen, Mingwei Yang, Chang Wang, Jicheng Li, Zheng Cai, Yukun Ren, Zhihua Zhu, Xiaotie Deng||In today's online advertising markets, it is common for advertisers to set long-term budgets. Correspondingly, advertising platforms adopt budget control methods to ensure that advertisers' payments lie within their budgets. Most budget control methods rely on the value distributions of advertisers. However, due to the complex advertising landscape and potential privacy concerns, the platform hardly learns advertisers' true priors. Thus, it is crucial to understand how budget control auction mechanisms perform under unassured priors. This work answers this problem from multiple aspects. We consider the unassured prior game among the seller and all buyers induced by different mechanisms in the stochastic model. We restrict the parameterized mechanisms to satisfy the budget-extracting condition, which maximizes the seller's revenue by extracting buyers' budgets as effectively as possible. Our main result shows that the Bayesian revenue-optimal mechanism and the budget-extracting bid-discount first-price mechanism yield the same set of Nash equilibrium outcomes in the unassured prior game. This implies that simple mechanisms can be as robust as the optimal mechanism under unassured priors in the budget-constrained setting. In the symmetric case, we further show that all these five (budget-extracting) mechanisms share the same set of possible outcomes. We further dig into the structural properties of these mechanisms. We characterize sufficient and necessary conditions on the budget-extracting parameter tuple for bid-discount/pacing first-price auctions. Meanwhile, when buyers do not take strategic behaviors, we exploit the dominance relationships of these mechanisms by revealing their intrinsic structures.|在今天的在线广告市场，广告商设定长期预算是很常见的。相应地，广告平台采用预算控制的方法来确保广告商的支付在他们的预算之内。大多数预算控制方法依赖于广告商的价值分配。然而，由于复杂的广告环境和潜在的隐私问题，该平台很难了解广告商的真实优先级。因此，了解预算控制拍卖机制在不确定的先验条件下如何运作至关重要。本文从多个方面回答了这个问题。在随机模型中，我们考虑了由不同机制诱导的卖方和所有买方之间的不确定先验博弈。我们将参数化机制限制在满足预算抽取条件下，通过尽可能有效地抽取买方预算使卖方收益最大化。我们的主要结果表明，贝叶斯收入最优机制和预算抽取投标折扣第一价格机制在不确定的先前博弈中产生了相同的纳什均衡点结果。这意味着，在预算有限的情况下，简单机制可以与不确定先验条件下的最优机制一样稳健。在对称情况下，我们进一步展示了所有这五个(预算抽取)机制共享同一组可能的结果。我们进一步挖掘这些机制的结构特性。我们刻画了竞价折扣/起拍第一价格拍卖中预算提取参数元组的充分必要条件。同时，当购买者不采取战略行为时，我们通过揭示其内在结构来利用这些机制的优势关系。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Budget-Constrained+Auctions+with+Unassured+Priors:+Strategic+Equivalence+and+Structural+Properties)|0|
|[Efficiency of the Generalized Second-Price Auction for Value Maximizers](https://doi.org/10.1145/3589334.3645360)|Yuan Deng, Mohammad Mahdian, Jieming Mao, Vahab Mirrokni, Hanrui Zhang, Song Zuo||We study the price of anarchy of the generalized second-price auction where bidders are value maximizers (i.e., autobidders). We show that in general the price of anarchy can be as bad as $0$. For comparison, the price of anarchy of running VCG is $1/2$ in the autobidding world. We further show a fined-grained price of anarchy with respect to the discount factors (i.e., the ratios of click probabilities between lower slots and the highest slot in each auction) in the generalized second-price auction, which highlights the qualitative relation between the smoothness of the discount factors and the efficiency of the generalized second-price auction.|我们研究了广义二级价格拍卖的无序价格，其中投标人是价值最大化者(即自动投标人)。我们表明，一般来说，无政府状态的代价可能和0美元一样糟糕。相比之下，无政府状态下运行 VCG 的价格是1/2 $在自动竞价的世界。进一步给出了广义二级价格拍卖中折扣因子(即每次拍卖中低位和高位之间点击概率的比值)的无序细粒度价格，突出了折扣因子的平滑性与广义二级价格拍卖的有效性之间的定性关系。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficiency+of+the+Generalized+Second-Price+Auction+for+Value+Maximizers)|0|
|[Data Exchange Markets via Utility Balancing](https://doi.org/10.1145/3589334.3645364)|Aditya Bhaskara, Sreenivas Gollapudi, Sungjin Im, Kostas Kollias, Kamesh Munagala, Govind S. Sankar||This paper explores the design of a balanced data-sharing marketplace for entities with heterogeneous datasets and machine learning models that they seek to refine using data from other agents. The goal of the marketplace is to encourage participation for data sharing in the presence of such heterogeneity. Our market design approach for data sharing focuses on interim utility balance, where participants contribute and receive equitable utility from refinement of their models. We present such a market model for which we study computational complexity, solution existence, and approximation algorithms for welfare maximization and core stability. We finally support our theoretical insights with simulations on a mean estimation task inspired by road traffic delay estimation.|本文探讨了一个平衡的数据共享市场的设计与异构数据集和机器学习模型的实体，他们寻求改进使用其他代理的数据。市场的目标是鼓励在存在这种异质性的情况下参与数据共享。我们的数据共享市场设计方法侧重于临时效用平衡，即参与者通过改进其模型贡献并获得公平效用。我们提出这样一个市场模型，我们研究计算复杂性，解的存在性，近似算法的福利最大化和核心稳定性。最后，我们以道路交通延误估计为启发，对一个平均估计任务进行了仿真，以支持我们的理论见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data+Exchange+Markets+via+Utility+Balancing)|0|
|[Fine-Tuning Games: Bargaining and Adaptation for General-Purpose Models](https://doi.org/10.1145/3589334.3645366)|Benjamin Laufer, Jon M. Kleinberg, Hoda Heidari||Major advances in Machine Learning (ML) and Artificial Intelligence (AI) increasingly take the form of developing and releasing general-purpose models. These models are designed to be adapted by other businesses and agencies to perform a particular, domain-specific function. This process has become known as adaptation or fine-tuning. This paper offers a model of the fine-tuning process where a Generalist brings the technological product (here an ML model) to a certain level of performance, and one or more Domain-specialist(s) adapts it for use in a particular domain. Both entities are profit-seeking and incur costs when they invest in the technology, and they must reach a bargaining agreement on how to share the revenue for the technology to reach the market. For a relatively general class of cost and revenue functions, we characterize the conditions under which the fine-tuning game yields a profit-sharing solution. We observe that any potential domain-specialization will either contribute, free-ride, or abstain in their uptake of the technology, and we provide conditions yielding these different strategies. We show how methods based on bargaining solutions and sub-game perfect equilibria provide insights into the strategic behavior of firms in these types of interactions, and we find that profit-sharing can still arise even when one firm has significantly higher costs than another. We also provide methods for identifying Pareto-optimal bargaining arrangements for a general set of utility functions.|机器学习(ML)和人工智能(AI)的主要进展日益采取开发和发布通用模型的形式。这些模型被设计用于其他企业和机构，以执行特定领域的特定功能。这个过程被称为适应或微调。本文提供了一个微调过程的模型，在这个过程中，一个多面手将技术产品(这里是一个机器学习模型)带到一定的性能水平，并且一个或多个领域专家将其适用于特定的领域。这两个实体都是追求利润的，在投资技术时都会产生成本，它们必须就如何分享技术进入市场的收入达成谈判协议。对于一类相对一般的成本和收益函数，我们刻画了微调博弈产生利润分享解的条件。我们观察到，任何潜在的领域专业化要么有助于、免费搭便车，要么放弃对技术的吸收，我们提供了产生这些不同策略的条件。我们展示了基于讨价还价解决方案和子博弈完美均衡的方法是如何提供对企业在这些类型的交互中的战略行为的洞察力的，我们发现即使一个企业的成本明显高于另一个企业，利润分享仍然可以出现。我们还提供了确定一般效用函数集的帕累托最优讨价还价安排的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fine-Tuning+Games:+Bargaining+and+Adaptation+for+General-Purpose+Models)|0|
|[Robust Decision Aggregation with Second-order Information](https://doi.org/10.1145/3589334.3645384)|Yuqi Pan, Zhaohua Chen, Yuqing Kong||We consider a decision aggregation problem with two experts who each make a binary recommendation after observing a private signal about an unknown binary world state. An agent, who does not know the joint information structure between signals and states, sees the experts' recommendations and aims to match the action with the true state. Under the scenario, we study whether supplemented additionally with second-order information (each expert's forecast on the other's recommendation) could enable a better aggregation. We adopt a minimax regret framework to evaluate the aggregator's performance, by comparing it to an omniscient benchmark that knows the joint information structure. With general information structures, we show that second-order information provides no benefit. No aggregator can improve over a trivial aggregator, which always follows the first expert's recommendation. However, positive results emerge when we assume experts' signals are conditionally independent given the world state. When the aggregator is deterministic, we present a robust aggregator that leverages second-order information, which can significantly outperform counterparts without it. Second, when two experts are homogeneous, by adding a non-degenerate assumption on the signals, we demonstrate that random aggregators using second-order information can surpass optimal ones without it. In the remaining settings, the second-order information is not beneficial. We also extend the above results to the setting when the aggregator's utility function is more general.|我们考虑一个由两个专家组成的决策聚合问题，每个专家在观察到一个未知二进制世界状态的私有信号之后提出一个二进制推荐。一个不知道信号和状态之间的联合信息结构的代理，看到了专家的建议，并且旨在使行动与真实状态相匹配。在这种情况下，我们研究是否额外补充二阶信息(每个专家根据对方的建议进行预测)可以实现更好的聚合。我们采用极小极大遗憾框架来评估聚合器的性能，通过比较它与一个知道联合信息结构的无所不知的基准。对于一般的信息结构，我们表明二阶信息没有提供任何好处。没有一个聚合器可以比一个微不足道的聚合器更好，因为后者总是遵循第一位专家的建议。然而，当我们假设专家的信号在世界状态下是条件独立的时候，积极的结果出现了。当聚合器是确定性的时候，我们提出了一个利用二阶信息的健壮的聚合器，它可以显著地优于没有二阶信息的对应方。其次，当两个专家具有齐次性时，通过在信号上加入一个非退化假设，我们证明了使用二阶信息的随机聚集器可以在没有二阶信息的情况下超过最优聚集器。在其余的设置中，二阶信息不是有益的。我们还将上述结果扩展到聚合器的效用函数更一般的情况。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Decision+Aggregation+with+Second-order+Information)|0|
|[Identifying Risky Vendors in Cryptocurrency P2P Marketplaces](https://doi.org/10.1145/3589334.3645475)|Taro Tsuchiya, Alejandro Cuevas, Nicolas Christin||Peer-to-Peer (P2P) cryptocurrency exchanges are two-sided marketplaces, similar to eBay, where individuals can offer to sell cryptocurrencies in exchange for payment. Due to disintermediation, these marketplaces trade off increased privacy for higher risk (e.g., scams/fraud). Although these marketplaces use feedback systems to encourage healthier transactions, anecdotal evidence suggests that feedback often fails to capture vendor-associated risks. This work documents the online safety of cryptocurrency P2P marketplaces, identifies underlying issues in feedback-based reputation systems, and proposes improved mechanisms for predicting/monitoring risky accounts. We collect data from two cryptocurrency marketplaces, Paxful and LocalCoinSwap (LCS) for 12 months (06/2022--06/2023). The data includes over 396,000 listings, 67,000 vendors, and 4.7 million feedback for Paxful; and about 52,000 listings, 14,000 users, and 146,000 feedback for LCS.First, we show that the current feedback system does not sufficiently convey enough information about risky vendors, and is susceptible to reputation manipulation through user collusion and automation. Second, combining various publicly available information, we build machine learning models to predict account suspension, and achieve a 0.86 F1-score and 0.93 AUC for Paxful. Third, while our models appear to have limited transferability across markets, we identify which features most help account suspension across platforms. Finally, we perform a month-long online evaluation to show that our models are significantly more successful than mere feedback-based reputation schemes at predicting which users will be suspended in the future.|P2P (Peer-to-Peer)加密货币交易所是一个双边市场，类似于 eBay，个人可以通过出售加密货币来换取支付。由于非中介化的存在，这些市场在增加隐私的同时，也承担了更高的风险(例如，诈骗/欺诈)。尽管这些市场使用反馈系统来鼓励更健康的交易，但轶事证据表明，反馈往往未能捕捉到与供应商相关的风险。这项工作记录了加密货币 P2P 市场的在线安全性，确定了基于反馈的声誉系统中的潜在问题，并提出了改进的预测/监测风险账户的机制。我们从两个加密货币市场(Paxful 和 LocalCoinSwap (LCS))收集12个月的数据(06/2022——06/2023)。数据包括超过396,000个列表，67,000个供应商和470万个 Paxful 的反馈; 以及大约52,000个列表，14,000个用户和146,000个 LCS 的反馈。首先，我们表明目前的反馈系统没有充分传达关于风险供应商的足够信息，并且容易通过用户串通和自动化操纵声誉。其次，结合各种公开信息，建立机器学习模型来预测账户暂停，得到了 Paxful 的0.86 F1得分和0.93 AUC。第三，尽管我们的模型在市场间的可转移性似乎有限，但我们确定了哪些功能最有助于在平台间暂停账户。最后，我们进行了为期一个月的在线评估，结果表明，我们的模型在预测哪些用户将在未来被停用方面，明显比单纯的基于反馈的声誉计划更为成功。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Identifying+Risky+Vendors+in+Cryptocurrency+P2P+Marketplaces)|0|
|[Prior-Free Mechanism with Welfare Guarantees](https://doi.org/10.1145/3589334.3645500)|Guru Guruganesh, Jon Schneider, Joshua R. Wang||We consider the problem of designing prior-free revenue-maximizing mechanisms for allocating items to n buyers when the mechanism is additionally provided with an estimate for the optimal welfare (which is guaranteed to be correct to within a multiplicative factor of 1/α). In the digital goods setting (where we can allocate items to an arbitrary subset of the buyers), we demonstrate a mechanism that achieves revenue that is O(log n/α)-competitive with the optimal welfare. In the public goods setting (where we either must allocate the item to all buyers or to no buyers), we demonstrate a mechanism which is O(n log 1/α) competitive. In both settings, we show the dependence on α and n is tight. Finally, we discuss generalizations to broader classes of allocation constraints.|我们考虑的问题，设计先验自由收入最大化机制分配项目给 n 个买家时，该机制另外提供了一个估计的最佳福利(这是保证正确的乘法因子内的1/α)。在数字商品环境下(我们可以将商品分配给任意一个买家子集) ，我们证明了一种机制，它实现了具有最优福利的 O (log n/α)竞争收入。在公共产品环境下(我们必须将产品分配给所有买家或不分配给任何买家) ，我们证明了一个 O (n log 1/α)竞争机制。在这两种情况下，我们证明了对 α 和 n 的依赖是紧密的。最后，我们讨论分配约束类型的泛化。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prior-Free+Mechanism+with+Welfare+Guarantees)|0|
|[Mechanism Design for Large Language Models](https://doi.org/10.1145/3589334.3645511)|Paul Dütting, Vahab Mirrokni, Renato Paes Leme, Haifeng Xu, Song Zuo||We investigate auction mechanisms to support the emerging format of AI-generated content. We in particular study how to aggregate several LLMs in an incentive compatible manner. In this problem, the preferences of each agent over stochastically generated contents are described/encoded as an LLM. A key motivation is to design an auction format for AI-generated ad creatives to combine inputs from different advertisers. We argue that this problem, while generally falling under the umbrella of mechanism design, has several unique features. We propose a general formalism -- the token auction model -- for studying this problem. A key feature of this model is that it acts on a token-by-token basis and lets LLM agents influence generated contents through single dimensional bids. We first explore a robust auction design approach, in which all we assume is that agent preferences entail partial orders over outcome distributions. We formulate two natural incentive properties, and show that these are equivalent to a monotonicity condition on distribution aggregation. We also show that for such aggregation functions, it is possible to design a second-price auction, despite the absence of bidder valuation functions. We then move to designing concrete aggregation functions by focusing on specific valuation forms based on KL-divergence, a commonly used loss function in LLM. The welfare-maximizing aggregation rules turn out to be the weighted (log-space) convex combination of the target distributions from all participants. We conclude with experimental results in support of the token auction formulation.|我们研究拍卖机制，以支持新兴格式的人工智能生成的内容。我们特别研究了如何以激励相容的方式聚合多个 LLM。在这个问题中，每个代理相对于随机生成的内容的首选项被描述/编码为 LLM。一个关键的动机是为人工智能生成的广告创意设计一种拍卖格式，将来自不同广告商的投入结合起来。我们认为，这个问题，虽然一般属于机构设计的伞，有几个独特的特点。我们提出了一个通用的形式主义——令牌拍卖模型——来研究这个问题。该模型的一个关键特征是它基于令牌逐个令牌的方式运行，并允许 LLM 代理通过单维出价影响生成的内容。我们首先探索一个稳健的拍卖设计方法，在这个方法中，我们假设代理人的偏好需要比结果分布更多的部分订单。我们建立了两个自然激励性质，并证明了这两个性质等价于分布集合的单调性条件。我们还表明，对于这样的聚合函数，尽管没有投标人估价函数，设计一个二级价格拍卖是可能的。然后着重讨论基于 KL- 散度的具体估值形式，设计具体的聚合函数，KL- 散度是 LLM 中常用的损失函数。福利最大化聚合规则是来自所有参与者的目标分布的加权(对数空间)凸组合。最后给出了实验结果以支持令牌拍卖的公式。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mechanism+Design+for+Large+Language+Models)|0|
|[Core-Competitiveness in Partially Observable Networked Market](https://doi.org/10.1145/3589334.3645555)|Bin Li, Dong Hao||In auction theory, a core is a stable outcome where no subgroup of participants can achieve better results for themselves. Core-competitive auctions aim to generate revenue that is achievable in a core. They are particularly important because they not only generate optimized revenue for the seller, but also provide an efficient and stable environment for participants. We generalize the design of core-competitive auctions to encompass partially observable networked markets (PONM). Unlike traditional auctions, which often deal with scenarios of limited trading activity, our approach to core-competitive auctions for PONM captures the nature of real-world transaction markets, which is a large linking world for the economic entities and commodities circulate among the entities in the market. Our generalizing the auction market to PONM can much improve the liquidity of the auction, and is especially meaningful for the web economics. Specifically, we quantify the upper and lower bounds of the minimum core revenue in PONM, and further prove that there does not exist any truthful auction for PONM which is efficient and core-competitive. Governed by this impossible result, we identify the criteria that the allocation rule for PONM should meet. Based on these criteria, we propose a new class of auction mechanisms for PONM that is individually rational, incentive-compatible, and core-competitive.|在拍卖理论中，一个核心是一个稳定的结果，在这个结果中没有一个子集的参与者能够为自己取得更好的结果。核心竞争性拍卖旨在产生核心能够实现的收入。它们之所以特别重要，是因为它们不仅为卖方创造了最优化的收入，而且还为参与者提供了一个有效和稳定的环境。我们将核心竞争拍卖的设计推广到包含部分可观测的网络市场(PONM)。与传统拍卖不同，传统拍卖通常处理有限交易活动的情况，我们对 PONM 核心竞争性拍卖的做法抓住了现实世界交易市场的性质，这是一个经济实体和商品在市场实体之间流通的大型联系世界。我们将拍卖市场推广到 PONM，可以大大提高拍卖的流动性，对网络经济尤其有意义。具体地，我们量化了 PONM 中最小核心收入的上下界，并进一步证明了 PONM 中不存在任何有效且具有核心竞争力的真实拍卖。根据这一不可能的结果，我们确定了 PONM 的分配规则应该满足的标准。基于这些准则，我们提出了一类新的 PONM 拍卖机制，它是个体理性的、激励相容的、核心竞争的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Core-Competitiveness+in+Partially+Observable+Networked+Market)|0|
|[Unveiling the Paradox of NFT Prosperity](https://doi.org/10.1145/3589334.3645566)|Jintao Huang, Pengcheng Xia, Jiefeng Li, Kai Ma, Gareth Tyson, Xiapu Luo, Lei Wu, Yajin Zhou, Wei Cai, Haoyu Wang||Unlike fungible tokens (e.g., cryptocurrency), a Non-Fungible Token (NFT) is unique and indivisible. As such, they can be used to authenticate ownership of digital assets (e.g., a photo) in a decentralized fashion. Given that NFTs have generated significant media attention since 2021, we perform a large-scale measurement study of the NFT ecosystem. We collect over 242M transfer logs and over 97M marketplace transactions until Aug 1st, 2023, by far the largest NFT dataset, to the best of our knowledge. We characterize the on-chain behavior of NFTs and their trading across five major marketplaces. We find that, although the NFT ecosystem is growing rapidly, it is driven by a relatively small set of dominant centralized players, with suspicious trade activities, e.g., over 23% of the monetary volume is generated by malicious wash trading and the ecosystem has experienced over 157K cases of NFT arbitrage, with a total sum of over \25M profit. Our observations motivate the need for more research efforts in the NFT security analysis.|与可替换令牌(例如加密货币)不同，非可替换令牌(NFT)是唯一和不可分的。因此，它们可以用来以分散的方式验证数字资产(如照片)的所有权。鉴于自2021年以来 NFT 引起了媒体的广泛关注，我们对 NFT 生态系统进行了大规模的测量研究。我们收集了超过242M 的传输日志和超过97M 的市场交易，直到2023年8月1日，迄今为止最大的 NFT 数据集，就我们所知。我们描述了非交易所交易的链上行为及其在五个主要市场的交易。我们发现，尽管 NFT 生态系统正在迅速发展，但它是由一个相对较小的主导集中的参与者驱动的，可疑的交易活动，例如，超过23% 的货币量是由恶意洗钱交易产生的，生态系统已经经历了超过157K 的 NFT 套利案例，总计超过2500万利润。我们的观察激发了在 NFT 安全分析中更多研究工作的需要。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+the+Paradox+of+NFT+Prosperity)|0|
|[Fair Surveillance Assignment Problem](https://doi.org/10.1145/3589334.3645613)|Fangxiao Wang, Bo Li||Monitoring a specific set of locations serves multiple purposes, such as infrastructure inspection and safety surveillance. We study a generalization of the surveillance problem, where the monitoring area, represented by a graph, is divided and assigned to a set of agents with personalized cost functions. In this paper, each agent's patrolling cost towards receiving a subgraph is measured by the weight of the minimum vertex cover therein, and our objective is to design algorithms to compute fair assignments of the surveillance tasks. The fairness is assessed using maximin share (MMS) fairness proposed by Budish [J. Political Econ., 2011]. Our main result is an algorithm which ensures a 4.562-approximate MMS allocation for any number of agents with arbitrary vertex weights. We then prove that no algorithm can be better than 2-approximate MMS. For scenarios involving no more than four agents, we improve the approximation ratio to 2, which is thus the optimal achievable ratio.|监控一组特定的位置有多种用途，例如基础设施检查和安全监视。研究了一类具有个性化成本函数的监控问题的推广，其中监控区域由一个图表示，并分配给一组具有个性化成本函数的代理。本文通过最小顶点覆盖的权重度量每个代理对接收子图的巡逻成本，设计计算监视任务公平分配的算法。使用 Budish 提出的最大份额(MMS)公平性来评估公平性[ J。政治经济学，2011]。我们的主要结果是一个算法，确保4.562-近似 MMS 分配的任意数量的代理任意顶点权重。然后证明了任何算法都不能比2-近似 MMS 算法更好。对于不超过四个代理的情况，我们将近似比提高到2，这是最佳可达比。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Surveillance+Assignment+Problem)|0|
|[Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools](https://doi.org/10.1145/3589334.3645615)|Wentao Zhang, Yilei Zhao, Shuo Sun, Jie Ying, Yonggang Xie, Zitao Song, Xinrun Wang, Bo An||Portfolio management (PM) is a fundamental financial trading task, which explores the optimal periodical reallocation of capitals into different stocks to pursue long-term profits. Reinforcement learning (RL) has recently shown its potential to train profitable agents for PM through interacting with financial markets. However, existing work mostly focuses on fixed stock pools, which is inconsistent with investors' practical demand. Specifically, the target stock pool of different investors varies dramatically due to their discrepancy on market states and individual investors may temporally adjust stocks they desire to trade (e.g., adding one popular stocks), which lead to customizable stock pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny change of the stock pool, which leads to high computational cost and unstable performance. To tackle this challenge, we propose EarnMore, a rEinforcement leARNing framework with Maskable stOck REpresentation to handle PM with CSPs through one-shot training in a global stock pool (GSP). Specifically, we first introduce a mechanism to mask out the representation of the stocks outside the target pool. Second, we learn meaningful stock representations through a self-supervised masking and reconstruction process. Third, a re-weighting mechanism is designed to make the portfolio concentrate on favorable stocks and neglect the stocks outside the target pool. Through extensive experiments on 8 subset stock pools of the US stock market, we demonstrate that EarnMore significantly outperforms 14 state-of-the-art baselines in terms of 6 popular financial metrics with over 40|投资组合管理(PM)是一项基础性的金融交易任务，它探索资本在不同股票上的最优周期性重新配置，以追求长期利润。强化学习(RL)最近展示了其通过与金融市场互动为 PM 培训盈利代理的潜力。然而，现有的研究主要集中在固定股票池上，这与投资者的实际需求不一致。具体来说，不同投资者的目标股票池由于其市场状态的差异而变化很大，个人投资者可能会暂时调整他们想要交易的股票(例如，增加一只流行股) ，从而导致可定制的股票池(CSP)。现有的 RL 方法需要对 RL 代理进行重新训练，即使存储池发生很小的变化，也会导致较高的计算代价和不稳定的性能。为了应对这一挑战，我们提出 EarnMore，一个具有可隐藏股票表示的强化学习框架，通过在全球股票池(gSP)中的一次性培训，与 CSP 一起处理 PM。具体来说，我们首先引入一种机制来掩盖目标池外股票的表示。其次，我们通过自我监督的掩蔽和重构过程来学习有意义的股票表示。第三，设计了一种重新加权机制，使投资组合集中于有利的股票，而忽略目标池外的股票。通过对美国股票市场8个子集的广泛实验，我们证明 EarnMore 在6个流行的财务指标方面显著优于14个最先进的基准，超过40个|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reinforcement+Learning+with+Maskable+Stock+Representation+for+Portfolio+Management+in+Customizable+Stock+Pools)|0|
|[Exit Ripple Effects: Understanding the Disruption of Socialization Networks Following Employee Departures](https://doi.org/10.1145/3589334.3645634)|David Gamba, Yulin Yu, Yuan Yuan, Grant Schoenebeck, Daniel M. Romero||Amidst growing uncertainty and frequent restructurings, the impacts of employee exits are becoming one of the central concerns for organizations. Using rich communication data from a large holding company, we examine the effects of employee departures on socialization networks among the remaining coworkers. Specifically, we investigate how network metrics change among people who historically interacted with departing employees. We find evidence of “breakdown" in communication among the remaining coworkers, who tend to become less connected with fewer interactions after their coworkers' departure. This effect appears to be moderated by both external factors, such as periods of high organizational stress, and internal factors, such as the characteristics of the departing employee. At the external level, periods of high stress correspond to greater communication breakdown; at the internal level, however, we find patterns suggesting individuals may end up better positioned in their networks after a network neighbor's departure. Overall, our study provides critical insights into managing workforce changes and preserving communication dynamics in the face of employee exits.|在日益增长的不确定性和频繁的重组中，员工离职的影响正在成为组织关注的中心问题之一。使用来自一家大型控股公司的丰富的沟通数据，我们考察了员工离职对剩余同事之间的社交网络的影响。具体来说，我们调查网络指标是如何变化的人谁与离职的员工历史互动。我们发现其他同事之间的沟通“崩溃”的证据，在同事离开后，他们之间的交流越来越少。这种效应似乎受到外部因素(如高组织压力时期)和内部因素(如离职员工的特点)的调节。在外部层面上，高压力时期对应着更大的沟通破裂; 然而在内部层面上，我们发现模式表明，在网络邻居离开后，个人可能最终在他们的网络中处于更好的位置。总的来说，我们的研究提供了管理员工变化和面对员工退出时保持沟通动态的关键见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exit+Ripple+Effects:+Understanding+the+Disruption+of+Socialization+Networks+Following+Employee+Departures)|0|
|[APT-Pipe: A Prompt-Tuning Tool for Social Data Annotation using ChatGPT](https://doi.org/10.1145/3589334.3645642)|Yiming Zhu, Zhizhuo Yin, Gareth Tyson, Ehsan ul Haq, LikHang Lee, Pan Hui||Recent research has highlighted the potential of LLM applications, like ChatGPT, for performing label annotation on social computing text. However, it is already well known that performance hinges on the quality of the input prompts. To address this, there has been a flurry of research into prompt tuning – techniques and guidelines that attempt to improve the quality of prompts. Yet these largely rely on manual effort and prior knowledge of the dataset being annotated. To address this limitation, we propose APT-Pipe, an automated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts to enhance ChatGPT's text classification performance on any given dataset. We implement APT-Pipe and test it across twelve distinct text classification datasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher weighted F1-score on nine out of twelve experimented datasets, with an improvement of 7.01 a framework by showing how it can be extended to support additional tuning mechanisms.|最近的研究强调了 LLM 应用程序(如 ChatGPT)在社交计算文本上执行标签注释的潜力。然而，众所周知，性能取决于输入提示的质量。为了解决这个问题，已经有一系列关于快速调优技术和指导方针的研究，这些技术和指导方针试图提高快速提示的质量。然而，这在很大程度上依赖于手工操作和注释数据集的先验知识。为了解决这个限制，我们提出 APT-Tube，一种自动化的提示调优管道。管道旨在自动调优提示，以增强 ChatGPT 在任何给定数据集上的文本分类性能。我们实现了 APT-Tube，并在十二个不同的文本分类数据集上进行了测试。我们发现，通过 APT-Tube 调优的提示帮助 ChatgPT 在十二个实验数据集中的9个上获得更高的加权 f1分数，通过展示如何扩展它以支持额外的调优机制，提高了7.01 a 框架。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=APT-Pipe:+A+Prompt-Tuning+Tool+for+Social+Data+Annotation+using+ChatGPT)|0|
|[Spot Check Equivalence: An Interpretable Metric for Information Elicitation Mechanisms](https://doi.org/10.1145/3589334.3645679)|Shengwei Xu, Yichi Zhang, Paul Resnick, Grant Schoenebeck||Because high-quality data is like oxygen for AI systems, effectively eliciting information from crowdsourcing workers has become a first-order problem for developing high-performance machine learning algorithms. Two prevalent paradigms, spot-checking and peer prediction, enable the design of mechanisms to evaluate and incentivize high-quality data from human labelers. So far, at least three metrics have been proposed to compare the performances of these techniques [33, 8, 3]. However, different metrics lead to divergent and even contradictory results in various contexts. In this paper, we harmonize these divergent stories, showing that two of these metrics are actually the same within certain contexts and explain the divergence of the third. Moreover, we unify these different contexts by introducing Spot Check Equivalence, which offers an interpretable metric for the effectiveness of a peer prediction mechanism. Finally, we present two approaches to compute spot check equivalence in various contexts, where simulation results verify the effectiveness of our proposed metric.|由于高质量的数据就像人工智能系统的氧气一样，有效地从众包工作者那里获取信息已经成为开发高性能机器学习算法的首要问题。两个流行的范例，即抽样检查和同行预测，使机制的设计，以评估和激励高质量的数据从人类标签。到目前为止，已经提出了至少三个指标来比较这些技术的性能[33,8,3]。然而，在不同的上下文中，不同的度量会导致不同甚至相互矛盾的结果。在本文中，我们协调这些不同的故事，表明其中两个度量实际上是相同的，在某些情况下，并解释了第三个分歧。此外，我们通过引入抽样检查等价来统一这些不同的上下文，它为同行预测机制的有效性提供了一个可解释的度量。最后，我们提出了两种在不同情况下计算抽查等价性的方法，仿真结果验证了我们提出的度量方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spot+Check+Equivalence:+An+Interpretable+Metric+for+Information+Elicitation+Mechanisms)|0|
|[Optimal Engagement-Diversity Tradeoffs in Social Media](https://doi.org/10.1145/3589334.3645713)|Fabian Baumann, Daniel Halpern, Ariel D. Procaccia, Iyad Rahwan, Itai Shapira, Manuel Wüthrich||Social media platforms are known to optimize user engagement with the help of algorithms. It is widely understood that this practice gives rise to echo chambers\emdash users are mainly exposed to opinions that are similar to their own. In this paper, we ask whether echo chambers are an inevitable result of high engagement; we address this question in a novel model. Our main theoretical results establish bounds on the maximum engagement achievable under a diversity constraint, for suitable measures of engagement and diversity; we can therefore quantify the worst-case tradeoff between these two objectives. Our empirical results, based on real data from Twitter, chart the Pareto frontier of the engagement-diversity tradeoff.|众所周知，社交媒体平台利用算法优化用户参与度。人们普遍认为，这种做法产生的回声室镶嵌用户主要是接触到的意见，类似于自己的。在本文中，我们询问回声室是否是高度参与的必然结果; 我们在一个新的模型中解决这个问题。我们的主要理论成果为在多样性约束下可实现的最大参与设定了界限，以便采取适当的参与和多样性措施; 因此，我们可以量化这两个目标之间的最坏情况权衡。我们的实证结果基于 Twitter 上的真实数据，绘制了参与与多样性权衡的帕累托前沿。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimal+Engagement-Diversity+Tradeoffs+in+Social+Media)|0|
|[MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning](https://doi.org/10.1145/3589334.3645322)|Yun Zhu, Haizhou Shi, Zhenshuo Zhang, Siliang Tang||In this work, we investigate the problem of out-of-distribution (OOD) generalization for unsupervised learning methods on graph data. This scenario is particularly challenging because graph neural networks (GNNs) have been shown to be sensitive to distributional shifts, even when labels are available. To address this challenge, we propose a \underline{M}odel-\underline{A}gnostic \underline{R}ecipe for \underline{I}mproving \underline{O}OD generalizability of unsupervised graph contrastive learning methods, which we refer to as MARIO. MARIO introduces two principles aimed at developing distributional-shift-robust graph contrastive methods to overcome the limitations of existing frameworks: (i) Information Bottleneck (IB) principle for achieving generalizable representations and (ii) Invariant principle that incorporates adversarial data augmentation to obtain invariant representations. To the best of our knowledge, this is the first work that investigates the OOD generalization problem of graph contrastive learning, with a specific focus on node-level tasks. Through extensive experiments, we demonstrate that our method achieves state-of-the-art performance on the OOD test set, while maintaining comparable performance on the in-distribution test set when compared to existing approaches. The source code for our method can be found at: https://github.com/ZhuYun97/MARIO|在这项工作中，我们研究了图形数据的非监督式学习方法的分布外(oOD)推广问题。这种情况特别具有挑战性，因为图形神经网络(GNN)已被证明对分布变化很敏感，即使有标签可用。为了解决这个问题，我们提出了一个下划线{ M }模型-下划线{ A }灵知下划线{ R } ，用于改进无监督图对比学习方法的下划线{ I }的下划线{ O } OD 泛化能力，我们称之为 MARIO。MARIO 介绍了两个原则，旨在发展分布移位鲁棒图对比方法，以克服现有框架的局限性: (i)信息瓶颈(IB)原则，以实现一般化表示和(ii)不变原则，结合对抗性数据增强，以获得不变表示。据我们所知，这是第一个研究图形对比学习的面向对象的泛化问题的工作，重点是节点级任务。通过大量的实验，我们证明了我们的方法在 OOD 测试集上达到了最先进的性能，同时与现有的方法相比，在分布式测试集上保持了可比的性能。我们方法的源代码可以在以下 https://github.com/zhuyun97/mario 找到:|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MARIO:+Model+Agnostic+Recipe+for+Improving+OOD+Generalization+of+Graph+Contrastive+Learning)|0|
|[Cooperative Classification and Rationalization for Graph Generalization](https://doi.org/10.1145/3589334.3645332)|Linan Yue, Qi Liu, Ye Liu, Weibo Gao, Fangzhou Yao, Wenfeng Li||Graph Neural Networks (GNNs) have achieved impressive results in graph classification tasks, but they struggle to generalize effectively when faced with out-of-distribution (OOD) data. Several approaches have been proposed to address this problem. Among them, one solution is to diversify training distributions in vanilla classification by modifying the data environment, yet accessing the environment information is complex. Besides, another promising approach involves rationalization, extracting invariant rationales for predictions. However, extracting rationales is difficult due to limited learning signals, resulting in less accurate rationales and diminished predictions. To address these challenges, in this paper, we propose a Cooperative Classification and Rationalization (C2R) method, consisting of the classification and the rationalization module. Specifically, we first assume that multiple environments are available in the classification module. Then, we introduce diverse training distributions using an environment-conditional generative network, enabling robust graph representations. Meanwhile, the rationalization module employs a separator to identify relevant rationale subgraphs while the remaining non-rationale subgraphs are de-correlated with labels. Next, we align graph representations from the classification module with rationale subgraph representations using the knowledge distillation methods, enhancing the learning signal for rationales. Finally, we infer multiple environments by gathering non-rationale representations and incorporate them into the classification module for cooperative learning. Extensive experimental results on both benchmarks and synthetic datasets demonstrate the effectiveness of C2R. Code is available at https://github.com/yuelinan/Codes-of-C2R.|图神经网络(GNN)在图分类任务中取得了令人印象深刻的成果，但在面对分布不均(OOD)数据时，它们难以有效地进行泛化。已经提出了几种解决这个问题的方法。其中，一种解决方案是通过修改数据环境使普通分类中的训练分布多样化，但是访问环境信息非常复杂。此外，另一个有前途的方法涉及合理化，提取预测不变的基本原理。然而，由于有限的学习信号，提取基本原理是困难的，导致不太准确的基本原理和减少预测。为了应对这些挑战，本文提出了一种合作分类与合理化(C2R)方法，该方法由分类与合理化模块组成。具体来说，我们首先假设分类模块中有多个可用的环境。然后，我们使用一个环境条件生成网络引入不同的训练分布，从而实现鲁棒的图表示。同时，合理化模块使用一个分隔符来识别相关的理论基础子图，而剩余的非理论基础子图与标签不相关。其次，利用知识提取方法将分类模块中的图表示与理论子图表示进行对齐，增强了理论的学习信号。最后，我们通过收集非理论表示来推断多个环境，并将它们合并到合作学习的分类模块中。在基准测试和合成数据集上的大量实验结果证明了 C2R 的有效性。密码可于 https://github.com/yuelinan/codes-of-c2r 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cooperative+Classification+and+Rationalization+for+Graph+Generalization)|0|
|[Cost-effective Data Labelling for Graph Neural Networks](https://doi.org/10.1145/3589334.3645339)|Shixun Huang, Ge Lee, Zhifeng Bao, Shirui Pan||Active learning (AL), that aims to label limited data samples to effectively train the model, stands as a very cost-effective data labelling strategy in machine learning. Given the state-of-the-art performance GNNs have achieved in graph-based tasks, it is critical to design proper AL methods for graph neural networks (GNNs). However, existing GNN-based AL methods require considerable supervised information to guide the AL process, such as the GNN model to use, and initially labelled nodes and labels of newly selected nodes. Such dependency on supervised information limits both flexibility and scalabilty. In this paper, we propose an unsupervised, scalable and flexible AL method - it incurs low memory footprints and time cost, is flexible to the choice of underlying GNNs, and operates without requiring GNN-model-specific knowledge or labels of selected nodes. Specifically, we leverage the commonality of existing GNNs to reformulate the unsupervised AL problem as the Aggregation Involvement Maximization (AIM) problem. The objective of AIM is to maximize the involvement or participation of all nodes during the feature aggregation process of GNNs for nodes to be labelled. In this way, the aggregated features of labelled nodes can be diversified to a large extent, thereby benefiting the training of feature transformation matrices which are major trainable components in GNNs. We prove that the AIM problem is NP-hard and propose an efficient solution with theoretical guarantees. Extensive experiments on public datasets demonstrate the effectiveness, scalability and flexibility of our method. Our study is highly relevant to the track "Graph Algorithms and Modeling for the Web" since we focus one of the major listed topics "Graph Embedding and GNNs for the Web" and AL for GNNs, as an important research problem, is faced by aforementioned challenges to be tackled in this paper.|主动学习(AL)是机器学习中一种非常经济有效的数据标记策略，其目的是标记有限的数据样本以有效地训练模型。鉴于图神经网络在基于图的任务中取得的最新性能，设计合适的图神经网络 AL 方法至关重要。然而，现有的基于 GNN 的 AL 方法需要大量的监督信息来指导 AL 过程，如要使用的 GNN 模型，以及最初标记的节点和新选择节点的标签。这种对监督信息的依赖性限制了灵活性和可扩展性。在本文中，我们提出了一种无监督、可扩展和灵活的 AL 方法-它具有低内存占用和时间成本，对底层 GNN 的选择具有灵活性，并且不需要特定于 GNN 模型的知识或选定节点的标签。具体来说，我们利用现有 GNN 的共性将无监督 AL 问题重新表述为聚合参与最大化(AIM)问题。AIM 的目标是在标记节点的 GNN 特征聚合过程中使所有节点最大限度地参与或参与。这样，标记节点的聚合特征就可以在很大程度上实现多样化，从而有利于训练特征转换矩阵，而特征转换矩阵是全球导航网中主要的可训练部分。我们证明了 AIM 问题是 NP 难的，并提出了一个有效的解决方案，给出了理论保证。在公共数据集上的大量实验证明了该方法的有效性、可扩展性和灵活性。我们的研究与“网络图形算法与建模”的课题密切相关，因为我们关注的主要课题之一是“网络图形嵌入与 GNNs”和 GNNs 的 AL 作为一个重要的研究问题，正面临着上述的挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cost-effective+Data+Labelling+for+Graph+Neural+Networks)|0|
|[Masked Graph Autoencoder with Non-discrete Bandwidths](https://doi.org/10.1145/3589334.3645370)|Ziwen Zhao, Yuhua Li, Yixiong Zou, Jiliang Tang, Ruixuan Li||Masked graph autoencoders have emerged as a powerful graph self-supervised learning method that has yet to be fully explored. In this paper, we unveil that the existing discrete edge masking and binary link reconstruction strategies are insufficient to learn topologically informative representations, from the perspective of message propagation on graph neural networks. These limitations include blocking message flows, vulnerability to over-smoothness, and suboptimal neighborhood discriminability. Inspired by these understandings, we explore non-discrete edge masks, which are sampled from a continuous and dispersive probability distribution instead of the discrete Bernoulli distribution. These masks restrict the amount of output messages for each edge, referred to as "bandwidths". We propose a novel, informative, and effective topological masked graph autoencoder using bandwidth masking and a layer-wise bandwidth prediction objective. We demonstrate its powerful graph topological learning ability both theoretically and empirically. Our proposed framework outperforms representative baselines in both self-supervised link prediction (improving the discrete edge reconstructors by at most 20%) and node classification on numerous datasets, solely with a structure-learning pretext. Our implementation is available at https://github.com/Newiz430/Bandana.|掩模图形自动编码器作为一种功能强大的图形自监督学习方法已经出现，但尚未得到充分的发展。本文从图神经网络的信息传播角度，揭示了现有的离散边掩蔽和二进制链路重构策略不足以学习拓扑信息表示。这些限制包括阻塞消息流、过于平滑的脆弱性和次优邻域分辨率。受到这些理解的启发，我们探索了非离散的边缘掩模，这些掩模是从连续和分散的概率分布取样，而不是离散的伯努利分布。这些掩码限制每个边的输出消息量，称为“带宽”。我们提出了一种新颖的，信息丰富，有效的拓扑屏蔽图自动编码器使用带宽屏蔽和分层带宽预测目标。本文从理论和实证两方面论证了它强大的图拓扑学习能力。我们提出的框架在自监督链接预测(最多提高20% 的离散边缘重构器)和节点分类两个方面都优于代表性的基线，在大量的数据集上，只有一个结构学习的借口。我们的实施 https://github.com/newiz430/bandana 可供参考。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Masked+Graph+Autoencoder+with+Non-discrete+Bandwidths)|0|
|[Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks](https://doi.org/10.1145/3589334.3645367)|Hao Liu, Jiarui Feng, Lecheng Kong, Dacheng Tao, Yixin Chen, Muhan Zhang||Graph Neural Networks (GNNs) have become popular in Graph Representation Learning (GRL). One fundamental application is few-shot node classification. Most existing methods follow the meta learning paradigm, showing the ability of fast generalization to few-shot tasks. However, recent works indicate that graph contrastive learning combined with fine-tuning can significantly outperform meta learning methods. Despite the empirical success, there is limited understanding of the reasons behind it. In our study, we first identify two crucial advantages of contrastive learning compared to meta learning, including (1) the comprehensive utilization of graph nodes and (2) the power of graph augmentations. To integrate the strength of both contrastive learning and meta learning on the few-shot node classification tasks, we introduce a new paradigm: Contrastive Few-Shot Node Classification (COLA). Specifically, COLA employs graph augmentations to identify semantically similar nodes, which enables the construction of meta-tasks without the need for label information. Therefore, COLA can utilize all nodes to construct meta-tasks, further reducing the risk of overfitting. Through extensive experiments, we validate the essentiality of each component in our design and demonstrate that COLA achieves new state-of-the-art on all tasks.|图形神经网络(GNN)在图形表示学习(GRL)中得到了广泛的应用。一个基本的应用是少镜头节点分类。现有的大多数方法都遵循元学习范式，表现出对短镜头任务的快速泛化能力。然而，最近的研究表明，图形对比学习结合微调可以显着优于元学习方法。尽管取得了经验上的成功，但对其背后的原因了解有限。在我们的研究中，我们首先确定了对比学习相对于元学习的两个关键优势，包括(1)图节点的综合利用和(2)图增强的力量。为了综合对比学习和元学习在少镜头节点分类任务中的优势，我们引入了一种新的范式: 对比少镜头节点分类(COLA)。具体来说，COLA 使用图增强来识别语义相似的节点，这使得元任务的构造不需要标签信息。因此，COLA 可以利用所有节点构建元任务，进一步降低过度适应的风险。通过大量的实验，我们验证了设计中每个组件的重要性，并证明 COLA 在所有任务上都达到了新的最先进水平。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Contrastive+Learning+Meets+Graph+Meta+Learning:+A+Unified+Method+for+Few-shot+Node+Tasks)|0|
|[Local Centrality Minimization with Quality Guarantees](https://doi.org/10.1145/3589334.3645382)|Atsushi Miyauchi, Lorenzo Severini, Francesco Bonchi||Centrality measures, quantifying the importance of vertices or edges, play a fundamental role in network analysis. To date, triggered by some positive approximability results, a large body of work has been devoted to studying centrality maximization, where the goal is to maximize the centrality score of a target vertex by manipulating the structure of a given network. On the other hand, due to the lack of such results, only very little attention has been paid to centrality minimization, despite its practical usefulness. In this study, we introduce a novel optimization model for local centrality minimization, where the manipulation is allowed only around the target vertex. We prove the NP-hardness of our model and that the most intuitive greedy algorithm has a quite limited performance in terms of approximation ratio. Then we design two effective approximation algorithms: The first algorithm is a highly-scalable algorithm that has an approximation ratio unachievable by the greedy algorithm, while the second algorithm is a bicriteria approximation algorithm that solves a continuous relaxation based on the Lovász extension, using a projected subgradient method. To the best of our knowledge, ours are the first polynomial-time algorithms with provable approximation guarantees for centrality minimization. Experiments using a variety of real-world networks demonstrate the effectiveness of our proposed algorithms: Our first algorithm is applicable to million-scale graphs and obtains much better solutions than those of scalable baselines, while our second algorithm is rather strong against adversarial instances.|中心性度量，量化顶点或边的重要性，在网络分析中起着基础性的作用。迄今为止，由一些正近似结果引发的大量工作都致力于研究中心性最大化，其目标是通过操纵给定网络的结构来使目标顶点的中心性得分最大化。另一方面，由于缺乏这样的结果，尽管集中度最小化具有实际的应用价值，但人们对它的关注却很少。在本研究中，我们引入一个新的局部中心性最小化的优化模型，其中只允许操作周围的目标顶点。我们证明了模型的 NP- 硬度，并且证明了最直观的贪婪算法在逼近比方面具有相当有限的性能。然后我们设计了两个有效的近似算法: 第一个算法是一个高度可扩展的算法，其近似比率是贪婪算法无法达到的，而第二个算法是一个双标准近似演算法，它使用一个投影次梯度法来解决基于 Lovász 扩展的连续松弛问题。据我们所知，我们是第一个多项式时间算法与可证明的近似保证中心性最小化。使用各种真实世界网络的实验证明了我们提出的算法的有效性: 我们的第一个算法适用于百万尺度的图形，并获得比那些可伸缩基线更好的解决方案，而我们的第二个算法是相当强大的对抗实例。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Local+Centrality+Minimization+with+Quality+Guarantees)|0|
|[Fast Inference of Removal-Based Node Influence](https://doi.org/10.1145/3589334.3645389)|Weikai Li, Zhiping Xiao, Xiao Luo, Yizhou Sun||Graph neural networks (GNNs) are widely utilized to capture the information spreading patterns in graphs. While remarkable performance has been achieved, there is a new trending topic of evaluating node influence. We propose a new method of evaluating node influence, which measures the prediction change of a trained GNN model caused by removing a node. A real-world application is, "In the task of predicting Twitter accounts' polarity, had a particular account been removed, how would others' polarity change?". We use the GNN as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal. To obtain the influence for every node, a straightforward way is to alternately remove every node and apply the trained GNN on the modified graph. It is reliable but time-consuming, so we need an efficient method. The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot directly satisfy our needs, since they do not focus on the global influence score for every node. We propose an efficient and intuitive method, NOde-Removal-based fAst GNN inference (NORA), which uses the gradient to approximate the node-removal influence. It only costs one forward propagation and one backpropagation to approximate the influence score for all nodes. Extensive experiments on six datasets and six GNN models verify the effectiveness of NORA. Our code is available at https://github.com/weikai-li/NORA.git.|图神经网络(GNN)被广泛用于捕获图中的信息传播模式。在取得显著成绩的同时，节点影响力评价也成为一个新的研究热点。本文提出了一种新的评估节点影响的方法，该方法通过删除一个节点来测量训练后的 GNN 模型的预测变化。现实世界中的应用程序是，“在预测 Twitter 帐户的极性的任务中，如果一个特定帐户被删除，其他帐户的极性将如何改变?”.我们使用 GNN 作为替代模型，其预测可以模拟节点移除引起的节点或边的变化。为了获得对每个节点的影响，一个简单的方法是交替删除每个节点并将训练后的 GNN 应用于修改后的图。这是可靠的，但费时，所以我们需要一个有效的方法。图形对抗性攻击和反事实解释等相关工作不能直接满足我们的需求，因为它们不关注每个节点的全局影响得分。提出了一种高效、直观的基于节点去除的快速 GNN 推理方法(NORA) ，该方法利用梯度逼近节点去除的影响。它只需要一个正向传播和一个反向传播来近似所有节点的影响评分。通过对6个数据集和6个 GNN 模型的大量实验，验证了 NORA 算法的有效性。我们的代码可以在 https://github.com/weikai-li/nora.git 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Inference+of+Removal-Based+Node+Influence)|0|
|[Memory Disagreement: A Pseudo-Labeling Measure from Training Dynamics for Semi-supervised Graph Learning](https://doi.org/10.1145/3589334.3645398)|Hongbin Pei, Yuheng Xiong, Pinghui Wang, Jing Tao, Jialun Liu, Huiqi Deng, Jie Ma, Xiaohong Guan||In the realm of semi-supervised graph learning, pseudo-labeling is a pivotal strategy to utilize both labeled and unlabeled nodes for model training. Currently, confidence score is the most frequently used pseudo-labeling measure, however, it suffers from poor calibration and issues in out-of-distribution data. In this paper, we propose memory disagreement (MoDis for short), a novel uncertainty measure for pseudo-labeling. We uncover that training dynamics offer significant insights into prediction uncertainty --- if a graph model makes consistent predictions for an unlabeled node throughout training, the corresponding predicted label is likely to be correct. Thus, the node should be suitable for pseudo-labeling. The basic idea is supported by recent studies on training dynamics. We implement MoDis as the entropy of an accumulated distribution that summarizes the disagreement of the model's predictions throughout training. We further enhance and analyze MoDis in case studies, which show nodes with low MoDis are suitable for pseudo-labeling as these nodes tend to be distant from boundaries in both graph and representation space. We design MoDis based pseudo-label selection algorithm and corresponding pseudo-labeling algorithm, which are applicable to various graph neural networks. We empirically validate MoDis on eight benchmark graph datasets. The experimental results show that pseudo labels given by MoDis have better quality in correctness and information gain, and the algorithm benefits various graph neural networks, achieving an average relative improvement of 3.11% and reaching up to 30.24% when compared to the wildly-used uncertainty measure, confidence score. Moreover, we demonstrate the efficacy of MoDis on out-of-distribution nodes.|在半监督图学习领域，伪标记是利用标记节点和未标记节点进行模型训练的关键策略。目前，置信度评分是最常用的伪标记度量方法，但它存在校正效果差和数据分布不均等等问题。在本文中，我们提出了记忆不一致(简称 MoDis) ，一种新的伪标记不确定度度量。我们发现训练动力学为预测不确定性提供了重要的见解——如果一个图模型在整个训练过程中对一个未标记的节点做出一致的预测，相应的预测标记可能是正确的。因此，节点应该适合伪标记。这一基本思想得到了近年来训练动力学研究的支持。我们将 MoDis 实现为一个累积分布的熵，它总结了整个训练过程中模型预测的不一致性。在实例研究中，我们进一步增强和分析了 MoD，发现低 MoD 的节点适合于伪标记，因为这些节点在图和表示空间中往往距离边界较远。设计了适用于各种图形神经网络的基于 MoDIS 的伪标记选择算法和相应的伪标记算法。我们在八个基准图数据集上实验验证了 MoDis。实验结果表明，MoD 给出的伪标签具有较好的正确性和信息增益，该算法有利于各种图形神经网络，与目前广泛使用的不确定性度量——置信度相比，平均相对提高了3.11% ，达到了30.24% 。此外，我们证明了 MoDIS 在分布外节点上的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Memory+Disagreement:+A+Pseudo-Labeling+Measure+from+Training+Dynamics+for+Semi-supervised+Graph+Learning)|0|
|[Descriptive Kernel Convolution Network with Improved Random Walk Kernel](https://doi.org/10.1145/3589334.3645405)|MengChieh Lee, Lingxiao Zhao, Leman Akoglu||Graph kernels used to be the dominant approach to feature engineering for structured data, which are superseded by modern GNNs as the former lacks learnability. Recently, a suite of Kernel Convolution Networks (KCNs) successfully revitalized graph kernels by introducing learnability, which convolves input with learnable hidden graphs using a certain graph kernel. The random walk kernel (RWK) has been used as the default kernel in many KCNs, gaining increasing attention. In this paper, we first revisit the RWK and its current usage in KCNs, revealing several shortcomings of the existing designs, and propose an improved graph kernel RWK^+, by introducing color-matching random walks and deriving its efficient computation. We then propose RWK^+ CN, a KCN that uses RWK^+ as the core kernel to learn descriptive graph features with an unsupervised objective, which can not be achieved by GNNs. Further, by unrolling RWK^+, we discover its connection with a regular GCN layer, and propose a novel GNN layer RWK^+ Conv. In the first part of experiments, we demonstrate the descriptive learning ability of RWK^+ CN with the improved random walk kernel RWK^+ on unsupervised pattern mining tasks; in the second part, we show the effectiveness of RWK^+ for a variety of KCN architectures and supervised graph learning tasks, and demonstrate the expressiveness of RWK^+ Conv layer, especially on the graph-level tasks. RWK^+ and RWK^+ Conv adapt to various real-world applications, including web applications such as bot detection in a web-scale Twitter social network, and community classification in Reddit social interaction networks.|图核过去是结构化数据特征工程的主要方法，但由于图核缺乏可学性而被现代 GNN 所取代。最近，一套核卷积网络(KCNs)通过引入可学习性成功地重新激活了图核，该网络使用特定的图核将输入与可学习的隐藏图卷积在一起。随机游走内核(RWK)在许多 KCNs 中被用作默认内核，越来越受到关注。本文首先回顾了 RWK 及其在 KCNs 中的应用，揭示了现有设计的一些不足，并通过引入颜色匹配随机游动和推导其有效计算，提出了一种改进的图核 RWK ^ + 。然后，我们提出了 RWK ^ + CN，这是一种使用 RWK ^ + 作为核心核心的 KCN，它以一个无监督的目标来学习描述图特征，这是 GNN 无法实现的。此外，通过展开 RWK ^ + ，我们发现它与一个规则的 GCN 层的连接，并提出了一个新的 GNN 层 RWK ^ + Conv。在实验的第一部分，我们证明了 RWK ^ + CN 在无监督模式挖掘任务中使用改进的随机游走核 RWK ^ + 的描述性学习能力; 在第二部分，我们证明了 RWK ^ + 在各种 KCN 架构和监督图学习任务中的有效性，并证明了 RWK ^ + Conv 层的表达能力，特别是在图级任务中。RWK ^ + 和 RWK ^ + Conv 适用于各种现实世界的应用程序，包括 Web 应用程序，例如在网络规模的 Twitter 社交网络中的 bot 检测，以及在 Reddit 社交网络中的社区分类。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Descriptive+Kernel+Convolution+Network+with+Improved+Random+Walk+Kernel)|0|
|[Dynamic Graph Information Bottleneck](https://doi.org/10.1145/3589334.3645411)|Haonan Yuan, Qingyun Sun, Xingcheng Fu, Cheng Ji, Jianxin Li||Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB_MS and DGIB_C, in which the DGIB_MS channel aims to learn the minimal and sufficient representations, with the DGIB_MS channel guarantees the predictive consensus. Extensive experiments on real-world and synthetic dynamic graph datasets demonstrate the superior robustness of DGIB against adversarial attacks compared with state-of-the-art baselines in the link prediction task. To the best of our knowledge, DGIB is the first work to learn robust representations of dynamic graphs grounded in the information-theoretic IB principle.|动态图在现实世界中广泛存在，它承载着复杂的空间和时间特征模式，对其表示学习提出了挑战。动态图神经网络(DGNNs)利用内在动力学显示出令人印象深刻的预测能力。然而，DGNN 表现出有限的健壮性，容易受到敌对攻击。本文提出了一种新的动态图形信息瓶颈(DGIB)框架来学习鲁棒性和区分性表示。利用信息瓶颈(IB)原理，我们首先提出期望最优表示应满足最小-足够-共识(MSC)条件。为了压缩冗余信息并将有价值的信息保留到潜在表示中，DGIB 迭代地指导和细化通过图形快照传递的结构和特征信息流。为了满足 MSC 条件，我们将整个 IB 目标分解为 DGIB _ MS 和 DGIB _ C，其中 DGIB _ MS 信道旨在学习最小和充分的表示，DGIB _ MS 信道保证预测一致性。在现实世界和合成动态图数据集上的大量实验表明，在链路预测任务中，DGIB 对抗对手攻击的鲁棒性优于最先进的基线。据我们所知，DGIB 是第一个基于信息论 IB 原理学习动态图的鲁棒表示的工作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Graph+Information+Bottleneck)|0|
|[Extracting Small Subgraphs in Road Networks](https://doi.org/10.1145/3589334.3645415)|Sara Ahmadian, Sreenivas Gollapudi, Gregory Hutchins, Kostas Kollias, Xizhi Tan||Online navigation platforms are well optimized to solve the standard objective of minimizing travel time and typically require precomputation-based architectures (such as Contraction Hierarchies and Customizable Route Planning) to do so in a fast manner. The reason for this dependence is the size of the graph that represents the road network, which is large. The need to go beyond minimizing the travel time and introduce various types of customizations has led to approaches that rely on alternative route computation or, more generally, small subgraph extraction. On a small subgraph, one can run computationally expensive algorithms at query time and compute optimal solutions for multiple routing problems. In this framework, it is critical for the subgraph to (a) be small and (b) include (near) optimal routes for a collection of customizations. This is precisely the setting that we study in this work. We design algorithms that extract a subgraph connecting designated terminals with the objective of minimizing the subgraph's size and the constraint of including near-optimal routes for a set of predefined cost functions. We provide theoretical guarantees for our algorithms and evaluate them empirically using real-world road networks.|在线导航平台经过了很好的优化，以解决最小化旅行时间的标准目标，通常需要基于预计算的体系结构(如收缩层次结构和可定制路径规划)来快速完成这一任务。这种依赖性的原因是代表道路网络的图形的大小，它很大。超越最小化旅行时间和引入各种定制的需要已经导致了依赖于可选路线计算的方法，或者更一般地说，小子图提取。在一个小子图上，可以在查询时运行计算量很大的算法，并计算出多个路由问题的最优解。在这个框架中，关键是子图(a)要小，(b)要为定制集合包含(接近)最佳路由。这正是我们在这项工作中研究的背景。设计了以子图的大小最小化为目标，以包含一组预定义代价函数的近最优路径为约束条件，提取连接指定终端的子图的算法。我们为我们的算法提供了理论上的保证，并利用现实世界的道路网络对它们进行了实证评估。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Extracting+Small+Subgraphs+in+Road+Networks)|0|
|[Game-theoretic Counterfactual Explanation for Graph Neural Networks](https://doi.org/10.1145/3589334.3645419)|Chirag Chhablani, Sarthak Jain, Akshay Channesh, Ian A. Kash, Sourav Medya||Graph Neural Networks (GNNs) have been a powerful tool for node classification tasks in complex networks. However, their decision-making processes remain a black-box to users, making it challenging to understand the reasoning behind their predictions. Counterfactual explanations (CFE) have shown promise in enhancing the interpretability of machine learning models. Prior approaches to compute CFE for GNNS often are learning-based approaches that require training additional graphs. In this paper, we propose a semivalue-based, non-learning approach to generate CFE for node classification tasks, eliminating the need for any additional training. Our results reveals that computing Banzhaf values requires lower sample complexity in identifying the counterfactual explanations compared to other popular methods such as computing Shapley values. Our empirical evidence indicates computing Banzhaf values can achieve up to a fourfold speed up compared to Shapley values. We also design a thresholding method for computing Banzhaf values and show theoretical and empirical results on its robustness in noisy environments, making it superior to Shapley values. Furthermore, the thresholded Banzhaf values are shown to enhance efficiency without compromising the quality (i.e., fidelity) in the explanations in three popular graph datasets.|图神经网络(GNN)已经成为复杂网络中节点分类任务的有力工具。然而，他们的决策过程对于用户来说仍然是一个黑盒子，这使得理解他们预测背后的推理变得具有挑战性。反事实解释(CFE)在提高机器学习模型的可解释性方面显示了希望。先前计算 GNNS CFE 的方法通常是基于学习的方法，需要额外的图形训练。在本文中，我们提出了一个基于半值的，非学习的方法来生成 CFE 的节点分类任务，消除了任何额外的训练的需要。我们的研究结果表明，与其他常用的方法(如计算 Shapley 值)相比，计算 Banzhaf 值在识别反事实解释方面需要较低的样本复杂度。我们的经验证明表明，计算 Banzhaf 值的速度可以达到沙普利值的四倍。我们还设计了一种计算 Banzhaf 值的阈值方法，并对其在噪声环境下的鲁棒性进行了理论和实验验证，使其优于 Shapley 值。此外，阈值 Banzhaf 值显示提高效率而不损害质量(即，保真度)的解释在三个流行的图形数据集。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Game-theoretic+Counterfactual+Explanation+for+Graph+Neural+Networks)|0|
|[Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph](https://doi.org/10.1145/3589334.3645452)|Linfeng Cao, Haoran Deng, Yang Yang, Chunping Wang, Lei Chen|The Ohio State University, Columbus, USA; Zhejiang University, Hangzhou, China; FinVolution Group, Shanghai, China|Due to the ubiquity of graph data on the web, web graph mining has become a hot research spot. Nonetheless, the prevalence of largescale web graphs in real applications poses significant challenges to storage, computational capacity and graph model design. Despite numerous studies to enhance the scalability of graph models, a noticeable gap remains between academic research and practical web graph mining applications. One major cause is that in most industrial scenarios, only a small part of nodes in a web graph are actually required to be analyzed, where we term these nodes as target nodes, while others as background nodes. In this paper, we argue that properly fetching and condensing the background nodes from massive web graph data might be a more economical shortcut to tackle the obstacles fundamentally. To this end, we make the first attempt to study the problem of massive background nodes compression for target nodes classification. Through extensive experiments, we reveal two critical roles played by the background nodes in target node classification: enhancing structural connectivity between target nodes, and feature correlation with target nodes. Following this, we propose a novel Graph-Skeleton model, which properly fetches the background nodes, and further condenses the semantic and topological information of background nodes within similar target-background local structures. Extensive experiments on various web graph datasets demonstrate the effectiveness and efficiency of the proposed method. In particular, for MAG240M dataset with 0.24 billion nodes, our generated skeleton graph achieves highly comparable performance while only containing 1.8% nodes of the original graph.|由于网络上图形数据的普遍存在，网络图形挖掘已经成为研究的热点。尽管如此，大规模网络图在实际应用中的流行对存储、计算能力和图模型设计提出了严峻的挑战。尽管许多研究提高了图模型的可伸缩性，但是在学术研究和实际的网络图挖掘应用之间仍然存在着明显的差距。一个主要原因是，在大多数工业场景中，实际上只需要对 Web 图中的一小部分节点进行分析，我们将这些节点称为目标节点，而将其他节点称为背景节点。在本文中，我们认为从海量的网络图形数据中正确地提取和压缩背景节点可能是从根本上解决障碍的一个更经济的捷径。为此，我们首次尝试研究用于目标节点分类的海量背景节点压缩问题。通过大量实验，揭示了背景节点在目标节点分类中的两个关键作用: 增强目标节点之间的结构连通性，以及与目标节点的特征相关性。在此基础上，提出了一种新的图骨架模型，该模型能够很好地提取背景节点，并在相似的目标-背景局部结构中进一步压缩背景节点的语义和拓扑信息。在各种网络图形数据集上的大量实验表明了该方法的有效性和高效性。特别是对于具有2.4亿个节点的 MAG240M 数据集，我们生成的骨架图在只包含原始图的1.8% 节点的情况下实现了高度可比的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph-Skeleton:+~1%+Nodes+are+Sufficient+to+Represent+Billion-Scale+Graph)|0|
|[GAUSS: GrAph-customized Universal Self-Supervised Learning](https://doi.org/10.1145/3589334.3645453)|Liang Yang, Weixiao Hu, Jizhong Xu, Runjie Shi, Dongxiao He, Chuan Wang, Xiaochun Cao, Zhen Wang, Bingxin Niu, Yuanfang Guo||To make Graph Neural Networks (GNNs) meet the requirements of the Web, the universality and the generalization become two important research directions. On one hand, many universal GNNs are presented for semi-supervised tasks on both homophilic and non-homophilic graphs by distinguishing homophilic and heterophilic edges with the help of labels. On the other hand, self-supervised learning (SSL) algorithms on graphs are presented by leveraging the self-supervised learning schemes from computer vision and natural language processing. Unfortunately, graph universal self-supervised learning remains resolved. Most existing SSL methods on graphs, which often employ two-layer GCN as the encoder and train the mapping functions, can't alter the low-passing filtering characteristic of GCN. Therefore, to be universal, SSL must becustomized for the graph, i.e., learning the graph. However, learning the graph via universal GNNs is disabled in SSL, since their distinguishability on homophilic and heterophilic edges disappears without the labels. To overcome this difficulty, this paper proposes novel GrAph-customized Universal Self-Supervised Learning (GAUSS) by exploiting local attribute distribution. The main idea is to replace the global parameters with locally learnable propagation. To make the propagation matrix demonstrate the affinity between the nodes, the self-representative learning framework is employed with k-block diagonal regularization. Extensive experiments on synthetic and real-world datasets demonstrate its effectiveness, universality and robustness to noises.|为了使图形神经网络(GNN)能够满足 Web 的需求，通用性和泛化性成为两个重要的研究方向。一方面，许多通用 GNN 通过标签的帮助来区分同亲和边和异亲和边，被提出用于同亲和图和非同亲图的半监督任务。另一方面，利用计算机视觉和自然语言处理中的自监督学习方案，提出了图的自监督学习(SSL)算法。遗憾的是，图的通用自监督学习仍然得到解决。现有的图上 SSL 方法大多采用两层 GCN 作为编码器，训练映射函数，不能改变 GCN 的低通滤波特性。因此，要实现通用性，必须为图定制 SSL，即学习图。然而，通过通用 GNN 学习图在 SSL 中是禁用的，因为它们在同亲和异亲边上的可区分性在没有标签的情况下消失了。为了克服这一困难，本文提出了一种基于局部属性分布的图定制通用自监督学习(GAUSS)方法。其主要思想是用局部可学习传播代替全局参数。为了使传播矩阵表示节点之间的亲和力，采用了自代表学习框架和 k 块对角正则化。在合成数据集和真实数据集上的大量实验表明了该算法的有效性、通用性和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GAUSS:+GrAph-customized+Universal+Self-Supervised+Learning)|0|
|[Optimizing Network Resilience via Vertex Anchoring](https://doi.org/10.1145/3589334.3645465)|Siyi Teng, Jiadong Xie, Fan Zhang, Can Lu, Juntao Fang, Kai Wang||Network resilience is a critical ability of a network to maintain its functionality against disturbances. A network is resilient/robust when a large portion of the nodes are to be better engaged in the network, i.e., they are less likely to leave given the changes on the network. Existing studies validate that the engagement of a node can be well captured by its coreness on network topology. Therefore, it is promising to maximize the number of nodes with increasing coreness values. In this paper, we propose and study thefollower maximization problem: maximizing the resilience gain (the number of coreness-increased vertices) via anchoring a set of vertices within a given budget. We prove that the problem is NP-hard and W[2]-hard, and it is NP-hard to approximate within an O(n^1-ε ) factor. We first propose an advanced greedy approach, followed by a time-dependent framework designed to quickly find high-quality results. The framework is initialized by the advanced greedy algorithm and incorporates novel techniques for optimizing the search space. The effectiveness and efficiency of our solution are verified with extensive experiments on 8 real-life datasets. Our source codes are available at https://github.com/Tsyxxxka/Follower-Maximization.|网络弹性是网络保持其抗干扰功能的关键能力。当大部分节点更好地参与网络时，网络具有弹性/健壮性，也就是说，考虑到网络上的变化，它们不太可能离开。现有的研究证实，节点的参与可以通过其核心网络拓扑很好地捕捉到。因此，随着核心值的增加，最大化节点的数量是有希望的。在本文中，我们提出并研究了跟随者最大化问题: 通过在给定的预算内锚定一组顶点，最大化弹性增益(增核顶点的数目)。证明了问题是 NP- 困难的，W [2]-困难的，在 O (n ^ 1-ε)因子范围内是 NP- 困难的。我们首先提出了一种先进的贪婪方法，然后是一个依赖于时间的框架，旨在快速找到高质量的结果。该框架采用先进的贪婪算法进行初始化，并结合了优化搜索空间的新技术。通过在8个实际数据集上的大量实验，验证了该方案的有效性和高效性。我们的源代码可以在 https://github.com/tsyxxxka/follower-maximization 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Network+Resilience+via+Vertex+Anchoring)|0|
|[VilLain: Self-Supervised Learning on Homogeneous Hypergraphs without Features via Virtual Label Propagation](https://doi.org/10.1145/3589334.3645454)|Geon Lee, Soo Yong Lee, Kijung Shin||Group interactions arise in various scenarios in real-world systems: collaborations of researchers, co-purchases of products, and discussions in online Q&A sites, to name a few. Such higher-order relations are naturally modeled as hypergraphs, which consist of hyperedges (i.e., any-sized subsets of nodes). For hypergraphs, the challenge to learn node representation when features or labels are not available is imminent, given that (a) most real-world hypergraphs are not equipped with external features while (b) most existing approaches for hypergraph learning resort to additional information. Thus, in this work, we propose VilLain, a novel self-supervised hypergraph representation learning method based on the propagation of virtual labels (v-labels). Specifically, we learn for each node a sparse probability distribution over v-labels as its feature vector, and we propagate the vectors to construct the final node embeddings. Inspired by higher-order label homogeneity, which we discover in real-world hypergraphs, we design novel self-supervised loss functions for the v-labels to reproduce the higher-order structure-label pattern. We demonstrate that VilLain is: (a) Requirement-free: learning node embeddings without relying on node labels and features, (b) Versatile: giving embeddings that are not specialized to specific tasks but generalizable to diverse downstream tasks, and (c) Accurate: more accurate than its competitors for node classification, hyperedge prediction, node clustering, and node retrieval tasks. Our code and dataset are available at https://github.com/geon0325/VilLain.|在现实世界系统中，团队互动出现在各种场景中: 研究人员的合作、产品的共同购买以及在线问答网站上的讨论等等。这种高阶关系自然地被建模为超图，它由超边组成(例如，任意大小的节点子集)。对于超图，当特征或标签不可用时学习节点表示的挑战迫在眉睫，因为(a)大多数现实世界的超图没有配备外部特征，而(b)大多数现有的超图学习方法诉诸于额外的信息。因此，本文提出了一种新的基于虚拟标签(v 标签)传播的自监督超图表示学习方法 VilLain。具体来说，我们学习每个节点在 v 标签上的稀疏概率分布作为其特征向量，然后传播这些向量来构造最终的节点嵌入。受高阶标号同质性的启发，我们在现实世界的超图中发现，我们为 v 标号设计了新的自监督损失函数来再现高阶结构标号模式。我们证明 VilLain 是: (a)无需要求: 不依赖节点标签和特性的学习节点嵌入，(b)多功能: 提供不专门针对特定任务的嵌入，但可推广到不同的下游任务，和(c)精确: 在节点分类，超边缘预测，节点聚类和节点检索任务方面比竞争对手更准确。我们的代码和数据集可在 https://github.com/geon0325/villain 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=VilLain:+Self-Supervised+Learning+on+Homogeneous+Hypergraphs+without+Features+via+Virtual+Label+Propagation)|0|
|[SMUG: Sand Mixing for Unobserved Class Detection in Graph Few-Shot Learning](https://doi.org/10.1145/3589334.3645466)|Chenxu Wang, Xichan Nie, Jinfeng Chen, Pinghui Wang, Junzhou Zhao, Xiaohong Guan||Graph few-shot learning (GFSL) has achieved great success in node classification tasks with rare labels. However, graph few-shot classification (GFSC) models often encounter the problem of classifying test samples with unobserved (or unknown) classes due to the rareness of labels. We formulate this problem as out-of-distribution (OOD) sample detection in inductive graph few-shot learning. This paper presents SMUG, a novel GFSL framework that can detect unobserved classes. Since we have no ground-truth OOD samples in a practical training dataset, it is challenging for the GFSC model to retrieve knowledge about unknown classes from labeled samples. To address this difficulty, we propose a sand mixing scheme to introduce observed classes as artificial OOD samples into meta-tasks. We also develop two unsupervised OOD discriminators to identify OOD samples. Thus, we can assess the performance of OOD discriminators since we know the true classes of these artificial OOD samples. Subsequently, we design a novel training procedure to optimize the encoder based on the performance of the OOD discriminators and the GFSC model. It not only enables the GFSL model to distinguish OOD samples but also promotes the classification accuracy of normal samples. We conduct extensive experiments to evaluate the effectiveness of SMUG based on four benchmark datasets. Experimental results demonstrate that SMUG achieves superior performance over state-of-the-art approaches in OOD detection and node classification. The source code of this paper is available at https://github.com/Memepp/SMUG.|图少镜头学习(GFSL)在具有稀有标签的节点分类任务中取得了巨大的成功。然而，由于标签的稀有性，图少镜头分类(GFSC)模型经常遇到对未观测(或未知)类别的测试样本进行分类的问题。我们将这个问题表述为归纳图少镜头学习中的超分布(OOD)样本检测问题。本文提出了一种新的 GFSL 框架 SMUG，它可以检测未观察到的类。由于在实际训练数据集中没有地面真实 OOD 样本，因此从标记样本中检索未知类别的知识对 GFSC 模型来说是一个挑战。为了解决这个问题，我们提出了一个混砂方案，将观测类作为人工面向对象的样本引入到元任务中。我们还开发了两个无监督的 OOD 鉴别器来识别 OOD 样本。因此，我们可以评估性能的面向对象的鉴别，因为我们知道这些人工面向对象样本的真实类别。随后，我们设计了一种新的训练程序，以优化编码器的基础上的 OOD 鉴别器的性能和 GFSC 模型。它不仅使 GFSL 模型能够区分 OOD 样本，而且提高了正常样本的分类准确性。我们进行了广泛的实验来评估 SMUG 的有效性基于四个基准数据集。实验结果表明，SMUG 在面向对象的检测和节点分类方面比现有的方法具有更好的性能。本文的源代码可在 https://github.com/memepp/smug 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SMUG:+Sand+Mixing+for+Unobserved+Class+Detection+in+Graph+Few-Shot+Learning)|0|
|[Graph Contrastive Learning with Cohesive Subgraph Awareness](https://doi.org/10.1145/3589334.3645470)|Yucheng Wu, Leye Wang, Xiao Han, HanJia Ye||Graph contrastive learning (GCL) has emerged as a state-of-the-art strategy for learning representations of diverse graphs including social and biomedical networks. GCL widely uses stochastic graph topology augmentation, such as uniform node dropping, to generate augmented graphs. However, such stochastic augmentations may severely damage the intrinsic properties of a graph and deteriorate the following representation learning process. We argue that incorporating an awareness of cohesive subgraphs during the graph augmentation and learning processes has the potential to enhance GCL performance. To this end, we propose a novel unified framework called CTAug, to seamlessly integrate cohesion awareness into various existing GCL mechanisms. In particular, CTAug comprises two specialized modules: topology augmentation enhancement and graph learning enhancement. The former module generates augmented graphs that carefully preserve cohesion properties, while the latter module bolsters the graph encoder's ability to discern subgraph patterns. Theoretical analysis shows that CTAug can strictly improve existing GCL mechanisms. Empirical experiments verify that CTAug can achieve state-of-the-art performance for graph representation learning, especially for graphs with high degrees. The code is available at https://doi.org/10.5281/zenodo.10594093, or https://github.com/wuyucheng2002/CTAug.|图形对比学习(GCL)已经成为学习包括社会和生物医学网络在内的多种图形表示的最先进的策略。GCL 广泛使用随机图拓扑增广，如统一节点删除，生成增广图。然而，这种随机增广会严重地破坏图的内在性质，并且恶化后续的表示学习过程。我们认为在图增强和学习过程中引入内聚子图的意识有可能提高 GCL 的性能。为此，我们提出了一个新的统一框架 CTAug，将内聚意识无缝集成到各种现有的 GCL 机制中。具体而言，CTAug 包括两个专门的模块: 拓扑增强增强和图形学习增强。前一个模块生成增强图，仔细保留内聚属性，而后一个模块支持图编码器识别子图模式的能力。理论分析表明，CTAug 可以对现有的 GCL 机构进行严格的改进。实验结果表明，CTAug 在图表示学习方面，特别是在高度图表示学习方面，能够取得较好的效果。代码可在 https://doi.org/10.5281/zenodo.10594093或 https://github.com/wuyucheng2002/ctaug 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Contrastive+Learning+with+Cohesive+Subgraph+Awareness)|0|
|[GNNFingers: A Fingerprinting Framework for Verifying Ownerships of Graph Neural Networks](https://doi.org/10.1145/3589334.3645489)|Xiaoyu You, Youhe Jiang, Jianwei Xu, Mi Zhang, Min Yang||Graph neural networks (GNNs) have emerged as the state of the art for a variety of graph-related tasks and have been widely commercialized in real-world scenarios. Behind its revolutionary representation capability, the huge training costs also expose GNNs to the risks of potential model piracy attacks which threaten the intellectual property (IP) of GNNs. In this work, we design a novel and effective ownership verification framework for GNNs called GNNFingers to safeguard the IP of GNNs. The key design of the proposed framework is two-fold: graph fingerprint construction and robust verification module. With GNNFingers, a GNN model owner can verify if a deployed model is stolen from the source GNN simply by querying with graph inputs. Besides, GNNFingers could be applied to various GNN models and graph-related tasks. We extensively evaluate the proposed framework on various GNNs designed for multiple graph-related tasks including graph classification, graph matching, node classification, and link prediction. Our results show that GNNFingers can robustly distinguish post-processed surrogate GNNs from irrelevant GNNs, e.g., GNNFingers achieves 100% true positives and 100% true negatives on the test of 200 suspect GNNs of both graph classification and node classification tasks.|图形神经网络(GNN)已经成为各种图形相关任务的最新技术，并在现实世界中得到了广泛的商业化应用。在其革命性的代表能力背后，巨大的培训成本也使 GNN 暴露于潜在的模型盗版攻击的风险，这些攻击威胁到 GNN 的知识产权(IP)。在这项工作中，我们设计了一个新颖而有效的 GNNNs 所有权验证框架，称为 GNNFinger，以保护 GNNs 的 IP 地址。该框架的关键设计包括两个方面: 图形指纹构造和鲁棒性验证模块。使用 GNNFinger，GNN 模型所有者可以通过使用图形输入进行查询来验证已部署的模型是否被从源 GNN 窃取。此外，GNNFinger 还可以应用于各种 GNN 模型和与图相关的任务。我们广泛地评估了所提出的框架在各种 GNN 设计的多图相关的任务，包括图分类，图匹配，节点分类和链路预测。我们的研究结果表明，GNNFinger 可以强有力地区分后处理的代理 GNN 和不相关的 GNN，例如，GNNFinger 在对200个图分类和节点分类任务的可疑 GNN 进行测试时，获得了100% 的真正正面和100% 的真正负面。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GNNFingers:+A+Fingerprinting+Framework+for+Verifying+Ownerships+of+Graph+Neural+Networks)|0|
|[Graph Fairness Learning under Distribution Shifts](https://doi.org/10.1145/3589334.3645508)|Yibo Li, Xiao Wang, Yujie Xing, Shaohua Fan, Ruijia Wang, Yaoqi Liu, Chuan Shi||Graph neural networks (GNNs) have achieved remarkable performance on graph-structured data. However, GNNs may inherit prejudice from the training data and make discriminatory predictions based on sensitive attributes, such as gender and race. Recently, there has been an increasing interest in ensuring fairness on GNNs, but all of them are under the assumption that the training and testing data are under the same distribution, i.e., training data and testing data are from the same graph. Will graph fairness performance decrease under distribution shifts? How does distribution shifts affect graph fairness learning? All these open questions are largely unexplored from a theoretical perspective. To answer these questions, we first theoretically identify the factors that determine bias on a graph. Subsequently, we explore the factors influencing fairness on testing graphs, with a noteworthy factor being the representation distances of certain groups between the training and testing graph. Motivated by our theoretical analysis, we propose our framework FatraGNN. Specifically, to guarantee fairness performance on unknown testing graphs, we propose a graph generator to produce numerous graphs with significant bias and under different distributions. Then we minimize the representation distances for each certain group between the training graph and generated graphs. This empowers our model to achieve high classification and fairness performance even on generated graphs with significant bias, thereby effectively handling unknown testing graphs. Experiments on real-world and semi-synthetic datasets demonstrate the effectiveness of our model in terms of both accuracy and fairness.|图神经网络(GNN)在处理图结构化数据方面取得了显著的性能。然而，GNN 可能从训练数据中继承偏见，并基于敏感属性(如性别和种族)作出歧视性预测。最近，人们越来越关注确保 GNN 的公平性，但所有这些网络都假定训练和测试数据处于同一分布状态，即训练数据和测试数据来自同一图表。图的公平性能在分布变化下是否会下降？分布移位如何影响图的公平性学习？所有这些悬而未决的问题在很大程度上都没有从理论的角度进行探讨。为了回答这些问题，我们首先从理论上确定决定图上偏差的因素。随后，我们探讨了影响测试图公平性的因素，其中一个值得注意的因素是训练图和测试图之间某些群体的表示距离。在理论分析的基础上，提出了我们的 FatraGNN 框架。具体来说，为了保证未知测试图的公平性，我们提出了一个图生成器来生成多个具有显著偏差且在不同分布下的图。然后将训练图与生成图之间的表示距离最小化。这使我们的模型能够实现高分类和公平性能，甚至在生成的图有显着的偏差，从而有效地处理未知的测试图。在真实世界和半合成数据集上的实验证明了该模型在准确性和公平性方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Fairness+Learning+under+Distribution+Shifts)|0|
|[Self-Guided Robust Graph Structure Refinement](https://doi.org/10.1145/3589334.3645522)|Yeonjun In, Kanghoon Yoon, Kibum Kim, Kijung Shin, Chanyoung Park||Recent studies have revealed that GNNs are vulnerable to adversarial attacks. To defend against such attacks, robust graph structure refinement (GSR) methods aim at minimizing the effect of adversarial edges based on node features, graph structure, or external information. However, we have discovered that existing GSR methods are limited by narrowassumptions, such as assuming clean node features, moderate structural attacks, and the availability of external clean graphs, resulting in the restricted applicability in real-world scenarios. In this paper, we propose a self-guided GSR framework (SG-GSR), which utilizes a clean sub-graph found within the given attacked graph itself. Furthermore, we propose a novel graph augmentation and a group-training strategy to handle the two technical challenges in the clean sub-graph extraction: 1) loss of structural information, and 2) imbalanced node degree distribution. Extensive experiments demonstrate the effectiveness of SG-GSR under various scenarios including non-targeted attacks, targeted attacks, feature attacks, e-commerce fraud, and noisy node labels. Our code is available at https://github.com/yeonjun-in/torch-SG-GSR.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Guided+Robust+Graph+Structure+Refinement)|0|
|[DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning](https://doi.org/10.1145/3589334.3645561)|Seungyoon Choi, Wonjoong Kim, Sungwon Kim, Yeonjun In, Sein Kim, Chanyoung Park||We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD) approach to consider both the class representativeness and the diversity within each class of the replayed nodes. Moreover, we adopt graph structure learning (GSL) to ensure that the replayed nodes are connected to truly informative neighbors. Extensive experimental results demonstrate the effectiveness and efficiency of DSLR.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DSLR:+Diversity+Enhancement+and+Structure+Learning+for+Rehearsal-based+Graph+Continual+Learning)|0|
|[Calibrating Graph Neural Networks from a Data-centric Perspective](https://doi.org/10.1145/3589334.3645562)|Cheng Yang, Chengdong Yang, Chuan Shi, Yawen Li, Zhiqiang Zhang, Jun Zhou||Graph neural networks (GNNs) have gained popularity in modeling various complex networks, e.g., social network and webpage network. Despite the promising accuracy, the confidences of GNNs are shown to be miscalibrated, indicating limited awareness of prediction uncertainty and harming the reliability of model decisions. Existing calibration methods primarily focus on improving GNN models, e.g., adding regularization during training or introducing temperature scaling after training. In this paper, we argue that the miscalibration of GNNs may stem from the graph data and can be alleviated through topology modification. To support this motivation, we conduct data observations by examining the impacts ofdecisive andhomophilic edges on calibration performance, where decisive edges play a critical role in GNN predictions and homophilic edges connect nodes of the same class. By assigning larger weights to these edges in the adjacency matrix, we observe an improvement in calibration performance without sacrificing classification accuracy. This suggests the potential of a data-centric approach for calibrating GNNs. Motivated by our observations, we propose Data-centric Graph Calibration (DCGC), which uses two edge weighting modules to adjust the input graph for GNN calibration. The first module learns the weights of decisive edges by parameterizing the adjacency matrix and enabling backpropagation of the prediction loss to edge weights. This emphasizes critical edges that fit the prediction needs. The second module computes weights for homophilic edges based on predicted label distributions, assigning larger weights to edges with stronger homophily. These modifications operate at the data level and can be easily integrated with temperature scaling-based methods for better calibration. Experimental results on 8 benchmark datasets demonstrate that DCGC achieves state-of-the-art calibration performance, with an average relative improvement of 36.4% in ECE, while maintaining or even slightly improving classification accuracy. Ablation studies and hyper-parameter analysis further validate the effectiveness and robustness of our proposed method DCGC. Code and data are available at https://github.com/BUPT-GAMMA/DCGC.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Calibrating+Graph+Neural+Networks+from+a+Data-centric+Perspective)|0|
|[EXGC: Bridging Efficiency and Explainability in Graph Condensation](https://doi.org/10.1145/3589334.3645551)|Junfeng Fang, Xinglin Li, Yongduo Sui, Yuan Gao, Guibin Zhang, Kun Wang, Xiang Wang, Xiangnan He||Graph representation learning on vast datasets, like web data, has made significant strides. However, the associated computational and storage overheads raise concerns. In sight of this, Graph condensation (GCond) has been introduced to distill these large real datasets into a more concise yet information-rich synthetic graph. Despite acceleration efforts, existing GCond methods mainly grapple with efficiency, especially on expansive web data graphs. Hence, in this work, we pinpoint two major inefficiencies of current paradigms: (1) the concurrent updating of a vast parameter set, and (2) pronounced parameter redundancy. To counteract these two limitations correspondingly, we first (1) employ the Mean-Field variational approximation for convergence acceleration, and then (2) propose the objective of Gradient Information Bottleneck (GDIB) to prune redundancy. By incorporating the leading explanation techniques (e.g., GNNExplainer and GSAT) to instantiate the GDIB, our EXGC, the Efficient and eXplainable Graph Condensation method is proposed, which can markedly boost efficiency and inject explainability. Our extensive evaluations across eight datasets underscore EXGC's superiority and relevance. Code is available at https://github.com/MangoKiller/EXGC.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EXGC:+Bridging+Efficiency+and+Explainability+in+Graph+Condensation)|0|
|[Fast and Accurate Fair k-Center Clustering in Doubling Metrics](https://doi.org/10.1145/3589334.3645568)|Matteo Ceccarello, Andrea Pietracaprina, Geppino Pucci||We study the classic k-center clustering problem under the additional constraint that each cluster should be fair. In this setting, each point is marked with one or more colors, which can be used to model protected attributes (e.g., gender or ethnicity). A cluster is deemed fair if, for every color, the fraction of its points marked with that color is within some prespecified range. We present a coreset-based approach to fair k-center clustering for general metric spaces which attains almost the best approximation quality of the current state of the art solutions, while featuring running times which can be orders of magnitude faster for large datasets of low doubling dimension. We devise sequential, streaming and MapReduce implementations of our approach and conduct a thorough experimental analysis to provide evidence of their practicality, scalability, and effectiveness.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+and+Accurate+Fair+k-Center+Clustering+in+Doubling+Metrics)|0|
|[Graph Principal Flow Network for Conditional Graph Generation](https://doi.org/10.1145/3589334.3645570)|Zhanfeng Mo, Tianze Luo, Sinno Jialin Pan||Conditional graph generation is crucial and challenging since the conditional distribution of graph topology and feature is complicated and the semantic information is hard to capture by the generative model. In this work, we propose a novel graph conditional generative model, Graph Principal Flow Network (GPrinFlowNet), which enables us to progressively generate high-quality graphs from low- to high-frequency components for a given graph label. We show that GPrinFlowNet follows a coarse-to-fine resolution generation curriculum, which enables it to capture subtle semantic information by generating intermediate graphs with high mutual information relative to the graph label. Extensive experiments and ablation studies showcase that our model achieves state-of-the-art performance compared to existing conditional graph generation models.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Principal+Flow+Network+for+Conditional+Graph+Generation)|0|
|[Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem](https://doi.org/10.1145/3589334.3645583)|Chen Huang, Haoyang Li, Yifan Zhang, Wenqiang Lei, Jiancheng Lv||The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to extract low-frequency signals from graph topology, which may lead to the over-smoothing problem when GCN goes deep. To this end, various methods have been proposed to create an adaptive filter by incorporating an extra filter (e.g., a high-pass filter) extracted from the graph topology. However, these methods heavily rely on topological information and ignore the node attribute space, which severely sacrifices the expressive power of the deep GCNs, especially when dealing with disassortative graphs. In this paper, we propose a cross-space adaptive filter, called CSF, to produce the adaptive-frequency information extracted from both the topology and attribute spaces. Specifically, we first derive a tailored attribute-based high-pass filter that can be interpreted theoretically as a minimizer for semi-supervised kernel ridge regression. Then, we cast the topology-based low-pass filter as a Mercer's kernel within the context of GCNs. This serves as a foundation for combining it with the attribute-based filter to capture the adaptive-frequency information. Finally, we derive the cross-space filter via an effective multiple-kernel learning strategy, which unifies the attribute-based high-pass filter and the topology-based low-pass filter. This helps to address the over-smoothing problem while maintaining effectiveness. Extensive experiments demonstrate that CSF not only successfully alleviates the over-smoothing problem but also promotes the effectiveness of the node classification task.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-Space+Adaptive+Filter:+Integrating+Graph+Topology+and+Node+Attributes+for+Alleviating+the+Over-smoothing+Problem)|0|
|[A Quasi-Wasserstein Loss for Learning Graph Neural Networks](https://doi.org/10.1145/3589334.3645586)|Minjie Cheng, Hongteng Xu||When learning graph neural networks (GNNs) in node-level prediction tasks, most existing loss functions are applied for each node independently, even if node embeddings and their labels are non-i.i.d. because of their graph structures. To eliminate such inconsistency, in this study we propose a novel Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on graphs, leading to new learning and prediction paradigms of GNNs. In particular, we design a "Quasi-Wasserstein" distance between the observed multi-dimensional node labels and their estimations, optimizing the label transport defined on graph edges. The estimations are parameterized by a GNN in which the optimal label transport may determine the graph edge weights optionally. By reformulating the strict constraint of the label transport to a Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein loss associated with two efficient solvers learning the GNN together with optimal label transport. When predicting node labels, our model combines the output of the GNN with the residual component provided by the optimal label transport, leading to a new transductive prediction paradigm. Experiments show that the proposed QW loss applies to various GNNs and helps to improve their performance in node-level classification and regression tasks.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Quasi-Wasserstein+Loss+for+Learning+Graph+Neural+Networks)|0|
|[GNNShap: Scalable and Accurate GNN Explanation using Shapley Values](https://doi.org/10.1145/3589334.3645599)|Selahattin Akkas, Ariful Azad||Graph neural networks (GNNs) are popular machine learning models for graphs with many applications across scientific domains. However, GNNs are considered black box models, and it is challenging to understand how the model makes predictions. Game theoric Shapley value approaches are popular explanation methods in other domains but are not well-studied for graphs. Some studies have proposed Shapley value based GNN explanations, yet they have several limitations: they consider limited samples to approximate Shapley values; some mainly focus on small and large coalition sizes, and they are an order of magnitude slower than other explanation methods, making them inapplicable to even moderate-size graphs. In this work, we propose GNNShap, which provides explanations for edges since they provide more natural explanations for graphs and more fine-grained explanations. We overcome the limitations by sampling from all coalition sizes, parallelizing the sampling on GPUs, and speeding up model predictions by batching. GNNShap gives better fidelity scores and faster explanations than baselines on real-world datasets. The code is available at https://github.com/HipGraph/GNNShap.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GNNShap:+Scalable+and+Accurate+GNN+Explanation+using+Shapley+Values)|0|
|[Graph Out-of-Distribution Generalization via Causal Intervention](https://doi.org/10.1145/3589334.3645604)|Qitian Wu, Fan Nie, Chenxiao Yang, Tianyi Bao, Junchi Yan||Out-of-distribution (OOD) generalization has gained increasing attentions for learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation with distribution shifts. The challenge is that distribution shifts on graphs involve intricate interconnections between nodes, and the environment labels are often absent in data. In this paper, we adopt a bottom-up data-generative perspective and reveal a key observation through causal analysis: the crux of GNNs' failure in OOD generalization lies in the latent confounding bias from the environment. The latter misguides the model to leverage environment-sensitive correlations between ego-graph features and target nodes' labels, resulting in undesirable generalization on new unseen nodes. Built upon this analysis, we introduce a conceptually simple yet principled approach for training robust GNNs under node-level distribution shifts, without prior knowledge of environment labels. Our method resorts to a new learning objective derived from causal inference that coordinates an environment estimator and a mixture-of-expert GNN predictor. The new approach can counteract the confounding bias in training data and facilitate learning generalizable predictive relations. Extensive experiment demonstrates that our model can effectively enhance generalization with various types of distribution shifts and yield up to 27.4% accuracy improvement over state-of-the-arts on graph OOD generalization benchmarks. Source codes are available at https://github.com/fannie1208/CaNet.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Out-of-Distribution+Generalization+via+Causal+Intervention)|0|
|[Adversarial Mask Explainer for Graph Neural Networks](https://doi.org/10.1145/3589334.3645608)|Wei Zhang, Xiaofan Li, Wolfgang Nejdl||The Graph Neural Networks (GNNs) model is a powerful tool for integrating node information with graph topology to learn representations and make predictions. However, the complex graph structure of GNNs has led to a lack of clear explainability in the decision-making process. Recently, there has been a growing interest in seeking instance-level explanations of the GNNs model, which aims to uncover the decision-making process of the GNNs model and provide insights into how it arrives at its final output. Previous works have focused on finding a set of weights (masks) for edges/nodes/node features to determine their importance. These works have adopted a regularization term and a hyperparameter K to control the explanation size during the training process and keep only the top-K weights as the explanation set. However, the true size of the explanation is typically unknown to users, making it difficult to provide reasonable values for the regularization term and K. In this work, we propose a novel framework AMExplainer which leverages the concept of adversarial networks to achieve a dual optimization objective in the target function. This approach ensures both accurate prediction of the mask and sparsity of the explanation set. In addition, we devise a novel scaling function to automatically sense and amplify the weights of the informative part of the graph, which filters out insignificant edges/nodes/node features for expediting the convergence of the solution during training. Our extensive experiments show that AMExplainer yields a more compelling explanation by generating a sparse set of masks while simultaneously maintaining fidelity.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adversarial+Mask+Explainer+for+Graph+Neural+Networks)|0|
|[On the Feasibility of Simple Transformer for Dynamic Graph Modeling](https://doi.org/10.1145/3589334.3645622)|Yuxia Wu, Yuan Fang, Lizi Liao||Dynamic graph modeling is crucial for understanding complex structures in web graphs, spanning applications in social networks, recommender systems, and more. Most existing methods primarily emphasize structural dependencies and their temporal changes. However, these approaches often overlook detailed temporal aspects or struggle with long-term dependencies. Furthermore, many solutions overly complicate the process by emphasizing intricate module designs to capture dynamic evolutions. In this work, we harness the strength of the Transformer's self-attention mechanism, known for adeptly handling long-range dependencies in sequence modeling. Our approach offers a simple Transformer model tailored for dynamic graph modeling without complex modifications. We re-conceptualize dynamic graphs as a sequence modeling challenge and introduce an innovative temporal alignment technique. This technique not only captures the inherent temporal evolution patterns within dynamic graphs but also streamlines the modeling process of their evolution. As a result, our method becomes versatile, catering to an array of applications. Our model's effectiveness is underscored through rigorous experiments on four real-world datasets from various sectors, solidifying its potential in dynamic graph modeling.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Feasibility+of+Simple+Transformer+for+Dynamic+Graph+Modeling)|0|
|[Can GNN be Good Adapter for LLMs?](https://doi.org/10.1145/3589334.3645627)|Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei Chai, Qi Zhu||Recently, large language models (LLMs) have demonstrated superior capabilities in understanding and zero-shot learning on textual data, promising significant advances for many text-related domains. In the graph domain, various real-world scenarios also involve textual data, where tasks and node features can be described by text. These text-attributed graphs (TAGs) have broad applications in social media, recommendation systems, etc. Thus, this paper explores how to utilize LLMs to model TAGs. Previous methods for TAG modeling are based on million-scale LMs. When scaled up to billion-scale LLMs, they face huge challenges in computational costs. Additionally, they also ignore the zero-shot inference capabilities of LLMs. Therefore, we propose GraphAdapter, which uses a graph neural network (GNN) as an efficient adapter in collaboration with LLMs to tackle TAGs. In terms of efficiency, the GNN adapter introduces only a few trainable parameters and can be trained with low computation costs. The entire framework is trained using auto-regression on node text (next token prediction). Once trained, GraphAdapter can be seamlessly fine-tuned with task-specific prompts for various downstream tasks. Through extensive experiments across multiple real-world TAGs, GraphAdapter based on Llama 2 gains an average improvement of approximately 5% in terms of node classification. Furthermore, GraphAdapter can also adapt to other language models, including RoBERTa, GPT-2. The promising results demonstrate that GNNs can serve as effective adapters for LLMs in TAG modeling.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+GNN+be+Good+Adapter+for+LLMs?)|0|
|[When Imbalance Meets Imbalance: Structure-driven Learning for Imbalanced Graph Classification](https://doi.org/10.1145/3589334.3645629)|Wei Xu, Pengkun Wang, Zhe Zhao, Binwu Wang, Xu Wang, Yang Wang||Graph Neural Networks (GNNs) can learn representative graph-level features to achieve efficient graph classification. But GNNs usually assume an environment where both class and structure distribution are balanced. Although previous works have considered the graph classification problem under the scenario of class imbalance or structure imbalance, they habitually ignored the obvious fact that class imbalance and structural imbalance are often intertwined in the real world. In this paper, we propose a carefully designed structure-driven learning framework called ImbGNN to address the potential intertwined class imbalance and structural imbalance in graph classification. Specifically, we find that feature-oriented augmentation (e.g., feature masking) and structure-oriented augmentation (e.g., edge perturbation) will have differential impacts when applied to different graphs. Therefore, we design optional augmentation based on the average degree distribution to alleviate structural imbalance. Furthermore, based on the imbalance of graph size distribution, we utilize a similarity-friendly graph random walk to extract a core subgraph to improve the accuracy of graph kernel similarity calculation, and then construct a more reasonable kernel-based graph of graphs, thereby alleviating the class imbalance and size imbalance. Extensive experiments on multiple benchmark datasets demonstrate that our proposed ImbGNN framework outperforms previous baselines on imbalanced graph classification tasks. The code of ImbGNN is available in~https://github.com/Xiaovy/ImbGNN.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+Imbalance+Meets+Imbalance:+Structure-driven+Learning+for+Imbalanced+Graph+Classification)|0|
|[Disambiguated Node Classification with Graph Neural Networks](https://doi.org/10.1145/3589334.3645637)|Tianxiang Zhao, Xiang Zhang, Suhang Wang||Graph Neural Networks (GNNs) have demonstrated significant success in learning from graph-structured data across various domains. Despite their great successful, one critical challenge is often overlooked by existing works, i.e., the learning of message propagation that can generalize effectively to underrepresented graph regions. These minority regions often exhibit irregular homophily/heterophily patterns and diverse neighborhood class distributions, resulting in ambiguity. In this work, we investigate the ambiguity problem within GNNs, its impact on representation learning, and the development of richer supervision signals to fight against this problem. We conduct a fine-grained evaluation of GNN, analyzing the existence of ambiguity in different graph regions and its relation with node positions. To disambiguate node embeddings, we propose a novel method, , which exploits additional optimization guidance to enhance representation learning, particularly for nodes in ambiguous regions. identifies ambiguous nodes based on temporal inconsistency of predictions and introduces a disambiguation regularization by employing contrastive learning in a topology-aware manner. promotes discriminativity of node representations and can alleviating semantic mixing caused by message propagation, effectively addressing the ambiguity problem. Empirical results validate the efficiency of and highlight its potential to improve GNN performance in underrepresented graph regions.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disambiguated+Node+Classification+with+Graph+Neural+Networks)|0|
|[Finding Densest Subgraphs with Edge-Color Constraints](https://doi.org/10.1145/3589334.3645647)|Lutz Oettershagen, Honglian Wang, Aristides Gionis||We consider a variant of the densest subgraph problem in networks with single or multiple edge attributes. For example, in a social network, the edge attributes may describe the type of relationship between users, such as friends, family, or acquaintances, or different types of communication. For conceptual simplicity, we view the attributes as edge colors. The new problem we address is to find a diverse densest subgraph that fulfills given requirements on the numbers of edges of specific colors. When searching for a dense social network community, our problem will enforce the requirement that the community is diverse according to criteria specified by the edge attributes. We show that the decision versions for finding exactly, at most, and at least h colored edges densest subgraph, where h is a vector of color requirements, are NP-complete, for already two colors. For the problem of finding a densest subgraph with at least h colored edges, we provide a linear-time constant-factor approximation algorithm when the input graph is sparse. On the way, we introduce the related at least h (non-colored) edges densest subgraph problem, show its hardness, and also provide a linear-time constant-factor approximation. In our experiments, we demonstrate the efficacy and efficiency of our new algorithms.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Finding+Densest+Subgraphs+with+Edge-Color+Constraints)|0|
|[Low Mileage, High Fidelity: Evaluating Hypergraph Expansion Methods by Quantifying the Information Loss](https://doi.org/10.1145/3589334.3645657)|David Y. Kang, Qiaozhu Mei, SangWook Kim||In this paper, we first define information loss that occurs in the hypergraph expansion and then propose a novel framework, named MILEAGE, to evaluate hypergraph expansion methods by measuring their degree of information loss. MILEAGE employs the following four steps: (1) expanding a hypergraph; (2) performing the unsupervised representation learning on the expanded graph; (3) reconstructing a hypergraph based on vector representations obtained; and (4) measuring MILEAGE-score (i.e., mileage) by comparing the reconstructed and the original hypergraphs. To demonstrate the usefulness of MILEAGE, we conduct experiments via downstream tasks on three levels (i.e., node, hyperedge, and hypergraph): node classification, hyperedge prediction, and hypergraph classification on eight real-world hypergraph datasets. Through the extensive experiments, we observe that information loss through hypergraph expansion has a negative impact on downstream tasks and MILEAGE can effectively evaluate hypergraph expansion methods through the information loss and recommend a new method that resolves the problems of existing ones.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Low+Mileage,+High+Fidelity:+Evaluating+Hypergraph+Expansion+Methods+by+Quantifying+the+Information+Loss)|0|
|[Learning Scalable Structural Representations for Link Prediction with Bloom Signatures](https://doi.org/10.1145/3589334.3645672)|Tianyi Zhang, Haoteng Yin, Rongzhe Wei, Pan Li, Anshumali Shrivastava||Graph neural networks (GNNs) have shown great potential in learning on graphs, but they are known to perform sub-optimally on link prediction tasks. Existing GNNs are primarily designed to learn node-wise representations and usually fail to capture pairwise relations between target nodes, which proves to be crucial for link prediction. Recent works resort to learning more expressive edge-wise representations by enhancing vanilla GNNs with structural features such as labeling tricks and link prediction heuristics, but they suffer from high computational overhead and limited scalability. To tackle this issue, we propose to learn structural link representations by augmenting the message-passing framework of GNNs with Bloom signatures. Bloom signatures are hashing-based compact encodings of node neighborhoods, which can be efficiently merged to recover various types of edge-wise structural features. We further show that any type of neighborhood overlap-based heuristic can be estimated by a neural network that takes Bloom signatures as input. GNNs with Bloom signatures are provably more expressive than vanilla GNNs and also more scalable than existing edge-wise models. Experimental results on five standard link prediction benchmarks show that our proposed model achieves comparable or better performance than existing edge-wise GNN models while being 3-200 × faster and more memory-efficient for online inference.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Scalable+Structural+Representations+for+Link+Prediction+with+Bloom+Signatures)|0|
|[GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks](https://doi.org/10.1145/3589334.3645682)|Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, Chuan Shi||Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse research fields of artificial intelligence, especially for open-ended tasks. While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor. To break this dilemma, we propose to bridge the pretrained GM and LLM by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of LLMs to offer various open-ended tasks for GM. To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node information, neighbor information and model information. By treating the node representation as a type of language, the proposed GraphTranslator empowers an LLM to make predictions based on node representation and language instructions, providing a unified perspective for both pre-defined and open-ended tasks. Extensive results show that the proposed GraphTranslator effectively improves the results of zero-shot node classification. The graph question answering experiments reveal our GraphTranslator potential across a broad spectrum of open-ended applications through language instructions.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphTranslator:+Aligning+Graph+Model+to+Large+Language+Model+for+Open-ended+Tasks)|0|
|[HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks](https://doi.org/10.1145/3589334.3645685)|Yihong Ma, Ning Yan, Jiayu Li, Masood S. Mortazavi, Nitesh V. Chawla||Graphs have emerged as a natural choice to represent and analyze the intricate patterns and rich information of the Web, enabling applications such as online page classification and social recommendation. The prevailing "pre-train, fine-tune" paradigm has been widely adopted in graph machine learning tasks, particularly in scenarios with limited labeled nodes. However, this approach often exhibits a misalignment between the training objectives of pretext tasks and those of downstream tasks. This gap can result in the "negative transfer" problem, wherein the knowledge gained from pre-training adversely affects performance in the downstream tasks. The surge in prompt-based learning within Natural Language Processing (NLP) suggests the potential of adapting a "pre-train, prompt" paradigm to graphs as an alternative. However, existing graph prompting techniques are tailored to homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To bridge this gap, we propose HetGPT, a general post-training prompting framework to improve the predictive performance of pre-trained heterogeneous graph neural networks (HGNNs). The key is the design of a novel prompting function that integrates a virtual class prompt and a heterogeneous feature prompt, with the aim to reformulate downstream tasks to mirror pretext tasks. Moreover, HetGPT introduces a multi-view neighborhood aggregation mechanism, capturing the complex neighborhood structure in heterogeneous graphs. Extensive experiments on three benchmark datasets demonstrate HetGPT's capability to enhance the performance of state-of-the-art HGNNs on semi-supervised node classification.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HetGPT:+Harnessing+the+Power+of+Prompt+Tuning+in+Pre-Trained+Heterogeneous+Graph+Neural+Networks)|0|
|[Graph Contrastive Learning via Interventional View Generation](https://doi.org/10.1145/3589334.3645687)|Zengyi Wo, Minglai Shao, Wenjun Wang, Xuan Guo, Lu Lin||Graph contrastive learning (GCL), as a popular self-supervised learning technique, has demonstrated promising capability in learning discriminative representations for diverse downstream tasks. A large body of GCL frameworks mainly work on graphs formed under homophily effect, i.e., similar nodes tend to connect with each other. In their design, the augmentation and aggregation are usually conducted indiscriminately on edges, ignoring the existence of heterophilic edges that connect dissimilar nodes. Therefore, the efficacy of GCL could greatly deteriorate on heterophilic graphs, verified by our analysis: GCL on a mixture of homophilic and heterophilic edges will generate representations that are indistinguishable across different classes in the embedding space. To address this challenge, we propose a novel GCL framework via interventional view generation. Specifically, we generate homophilic and heterophilic views through counterfactual intervention, which targets on disentangling homophilic and heterophilic structure from the original graph, such that we can capture their corresponding information using separate filters in the contrastive learning process. Since the homophilic view and the heterophilic view present different frequency signals, they are further encoded via a low-pass and a high-pass filter respectively. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our design. Our proposed framework achieves a remarkably improved downstream performance on graphs with high heterophily while maintaining a comparable ability in learning homophilic graphs. A comprehensive study also verifies the necessity of individual designs in our framework.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Contrastive+Learning+via+Interventional+View+Generation)|0|
|[Endowing Pre-trained Graph Models with Provable Fairness](https://doi.org/10.1145/3589334.3645703)|Zhongjian Zhang, Mengmei Zhang, Yue Yu, Cheng Yang, Jiawei Liu, Chuan Shi||Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks. Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications. The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs. However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient. Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario. To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained Graph models with Provable fAiRness (called GraphPAR). GraphPAR freezes the parameters of PGMs and trains a parameter-efficient adapter to flexibly improve the fairness of PGMs in downstream tasks. Specifically, we design a sensitive semantic augmenter on node representations, to extend the node representations with different sensitive attribute semantics for each node. The extended representations will be used to further train an adapter, to prevent the propagation of sensitive attribute semantics from PGMs to task predictions. Furthermore, with GraphPAR, we quantify whether the fairness of each node is provable, i.e., predictions are always fair within a certain range of sensitive attribute semantics. Experimental evaluations on real-world datasets demonstrate that GraphPAR achieves state-of-the-art prediction performance and fairness on node classification task. Furthermore, based on our GraphPAR, around 90% nodes have provable fairness.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Endowing+Pre-trained+Graph+Models+with+Provable+Fairness)|0|
|[Unveiling Delay Effects in Traffic Forecasting: A Perspective from Spatial-Temporal Delay Differential Equations](https://doi.org/10.1145/3589334.3645688)|Qingqing Long, Zheng Fang, Chen Fang, Chong Chen, Pengfei Wang, Yuanchun Zhou||Traffic flow forecasting is a fundamental research issue for transportation planning and management, which serves as a canonical and typical example of spatial-temporal predictions. In recent years, Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) have achieved great success in capturing spatial-temporal correlations for traffic flow forecasting. Yet, two non-ignorable issues haven't been well solved: 1) The message passing in GNNs is immediate, while in reality the spatial message interactions among neighboring nodes can be delayed. The change of traffic flow at one node will take several minutes, i.e., time delay, to influence its connected neighbors. 2) Traffic conditions undergo continuous changes. The prediction frequency for traffic flow forecasting may vary based on specific scenario requirements. Most existing discretized models require retraining for each prediction horizon, restricting their applicability. To tackle the above issues, we propose a neural Spatial-Temporal Delay Differential Equation model, namely STDDE. It includes both delay effects and continuity into a unified delay differential equation framework, which explicitly models the time delay in spatial information propagation. Furthermore, theoretical proofs are provided to show its stability. Then we design a learnable traffic-graph time-delay estimator, which utilizes the continuity of the hidden states to achieve the gradient backward process. Finally, we propose a continuous output module, allowing us to accurately predict traffic flow at various frequencies, which provides more flexibility and adaptability to different scenarios. Extensive experiments show the superiority of STDDE. Both quantitative and qualitative experiments are conducted to validate the concept of a delay-aware module. Also, the flexibility validation shows the effectiveness of the continuous output module.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+Delay+Effects+in+Traffic+Forecasting:+A+Perspective+from+Spatial-Temporal+Delay+Differential+Equations)|0|
|[Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace Approach](https://doi.org/10.1145/3589334.3645705)|Keke Huang, Wencai Cao, Hoang Ta, Xiaokui Xiao, Pietro Liò||Graph Neural Networks (GNNs), known as spectral graph filters, find a wide range of applications in web networks. To bypass eigendecomposition, polynomial graph filters are proposed to approximate graph filters by leveraging various polynomial bases for filter training. However, no existing studies have explored the diverse polynomial graph filters from a unified perspective for optimization. In this paper, we first unify polynomial graph filters, as well as the optimal filters of identical degrees into the Krylov subspace of the same order, thus providing equivalent expressive power theoretically. Next, we investigate the asymptotic convergence property of polynomials from the unified Krylov subspace perspective, revealing their limited adaptability in graphs with varying heterophily degrees. Inspired by those facts, we design a novel adaptive Krylov subspace approach to optimize polynomial bases with provable controllability over the graph spectrum so as to adapt various heterophily graphs. Subsequently, we propose AdaptKry, an optimized polynomial graph filter utilizing bases from the adaptive Krylov subspaces. Meanwhile, in light of the diverse spectral properties of complex graphs, we extend AdaptKry by leveraging multiple adaptive Krylov bases without incurring extra training costs. As a consequence, extended AdaptKry is able to capture the intricate characteristics of graphs and provide insights into their inherent complexity. We conduct extensive experiments across a series of real-world datasets. The experimental results demonstrate the superior filtering capability of AdaptKry, as well as the optimized efficacy of the adaptive Krylov basis.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Polynomial+Graph+Filters:+A+Novel+Adaptive+Krylov+Subspace+Approach)|0|
|[Full-Attention Driven Graph Contrastive Learning: with Effective Mutual Information Insight](https://doi.org/10.1145/3589334.3645717)|Long Li, Zemin Liu, Chenghao Liu, Jianling Sun||Graph contrastive learning often faces challenges when data augmentations compromise the graph's critical attributes, introducing the risk of generating noise-positive pairs. Although recent methods have attempted to address these issues, they either fall short of ensuring effective data augmentation or suffer from excessive computational demands. The advent of full-attention graph Transformers, with their enhanced capacity for graph representation learning, has sparked significant interest. Despite their potential, employing full-attention graph Transformers for contrastive learning can introduce issues such as noisy redundancies. In this work, we propose the Graph Attention Contrastive Learning (GACL) model, which innovatively combines a full-attention transformer with a message-passing graph neural network as its encoder. To mitigate the noise associated with full-attention mechanisms, we apply a denoising modification. Our GACL model effectively tackles the challenges associated with full-attention mechanisms and introduces a novel approach for data augmentation. Moreover, we propose the concept of effective mutual information to theoretically underpin our methodology. Utilizing this framework, we explore the impact of the denoising matrix within GACL's contrastive learning process and delve into comprehensive discussions on its implications. Empirical assessments underscore GACL's exceptional performance, establishing it as a state-of-the-art solution in graph contrastive learning.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Full-Attention+Driven+Graph+Contrastive+Learning:+with+Effective+Mutual+Information+Insight)|0|
|[Understanding GDPR Non-Compliance in Privacy Policies of Alexa Skills in European Marketplaces](https://doi.org/10.1145/3589334.3645409)|Song Liao, Mohammed Aldeen, Jingwen Yan, Long Cheng, Xiapu Luo, Haipeng Cai, Hongxin Hu||Amazon Alexa is one of the largest Voice Personal Assistant (VPA) platforms and it allows third-party developers to publish their voice apps, named skills, to the Alexa skill store. To satisfy the needs of European users, Amazon Alexa has established multiple skill marketplaces in Europe and allows developers to publish skills in their native languages. Skills in European marketplaces are required to comply with GDPR (General Data Protection Regulation), which imposes strict obligations on data collection and processing. Skills that involve data collection should provide a privacy policy to disclose the data practice to users and meet GDPR requirements. In this work, we analyze the privacy policies of skills in European marketplaces, focusing on whether skills' privacy policies and data collection behaviors comply with GDPR. We collect a large-scale dataset that includes skills in all European marketplaces with privacy policies. To classify whether a sentence in a privacy policy provides GDPR information, we gather a labeled dataset including skills' privacy policy sentences and use it to train a BERT model. Then, we analyze the GDPR compliance of European skills. Using a dynamic testing tool based on ChatGPT, we check whether skills' privacy policies comply with GDPR and are consistent with the actual data collection behaviors. Surprisingly, we find that 67% of the privacy policies fail to comply with GDPR and don't provide necessary GDPR-related information. For 1,187 skills with data collection behaviors, we observe that 603 skills (50.8%) don't provide a complete privacy policy and 1,128 skills (95%) have GDPR non-compliance issues in their privacy policies. Meanwhile, we find that the GDPR has a positive influence on European privacy policies.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+GDPR+Non-Compliance+in+Privacy+Policies+of+Alexa+Skills+in+European+Marketplaces)|0|
|[Differentially Private Selection from Secure Distributed Computing](https://doi.org/10.1145/3589334.3645435)|Ivan Damgård, Hannah Keller, Boel Nelson, Claudio Orlandi, Rasmus Pagh||Given a collection of vectors $x^{(1)},\dots,x^{(n)} \in \{0,1\}^d$, the selection problem asks to report the index of an "approximately largest" entry in $x=\sum_{j=1}^n x^{(j)}$. Selection abstracts a host of problems--in machine learning it can be used for hyperparameter tuning, feature selection, or to model empirical risk minimization. We study selection under differential privacy, where a released index guarantees privacy for each vectors. Though selection can be solved with an excellent utility guarantee in the central model of differential privacy, the distributed setting lacks solutions. Specifically, strong privacy guarantees with high utility are offered in high trust settings, but not in low trust settings. For example, in the popular shuffle model of distributed differential privacy, there are strong lower bounds suggesting that the utility of the central model cannot be obtained. In this paper we design a protocol for differentially private selection in a trust setting similar to the shuffle model--with the crucial difference that our protocol tolerates corrupted servers while maintaining privacy. Our protocol uses techniques from secure multi-party computation (MPC) to implement a protocol that: (i) has utility on par with the best mechanisms in the central model, (ii) scales to large, distributed collections of high-dimensional vectors, and (iii) uses $k\geq 3$ servers that collaborate to compute the result, where the differential privacy holds assuming an honest majority. Since general-purpose MPC techniques are not sufficiently scalable, we propose a novel application of integer secret sharing, and evaluate the utility and efficiency of our protocol theoretically and empirically. Our protocol is the first to demonstrate that large-scale differentially private selection is possible in a distributed setting.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Differentially+Private+Selection+from+Secure+Distributed+Computing)|0|
|[The Dynamics of (Not) Unfollowing Misinformation Spreaders](https://doi.org/10.1145/3589334.3645445)|Joshua Ashkinaze, Eric Gilbert, Ceren Budak||Many studies explore how people 'come into' misinformation exposure. But much less is known about how people 'come out of' misinformation exposure. Do people organically sever ties to misinformation spreaders? And what predicts doing so? Over six months, we tracked the frequency and predictors of 1M followers unfollowing 5K health misinformation spreaders on Twitter. We found that misinformation ties are persistent. Monthly unfollowing rates are just 0.52 Users are also 31 they are to unfollow misinformation spreaders. Although generally infrequent, the factors most associated with unfollowing misinformation spreaders are (1) redundancy and (2) ideology. First, users initially following many spreaders, or who follow spreaders that tweet often, are most likely to unfollow later. Second, liberals are more likely to unfollow than conservatives. Overall, we observe strong persistence of misinformation ties. The fact that users rarely unfollow misinformation spreaders suggests a need for external nudges and the importance of preventing exposure from arising in the first place.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Dynamics+of+(Not)+Unfollowing+Misinformation+Spreaders)|0|
|[Federated Learning Vulnerabilities: Privacy Attacks with Denoising Diffusion Probabilistic Models](https://doi.org/10.1145/3589334.3645514)|Hongyan Gu, Xinyi Zhang, Jiang Li, Hui Wei, Baiqi Li, Xinli Huang||Federal Learning (FL) is highly respected for protecting data privacy in a distributed environment. However, the correlation between the updated gradient and the training data opens up the possibility of data reconstruction for malicious attackers, thus threatening the basic privacy requirements of FL. Previous research on such attacks mainly focuses on two main perspectives: one exclusively relies on gradient attacks, which performs well on small-scale data but falter with large-scale data; the other incorporates images prior but faces practical implementation challenges. So far, the effectiveness of privacy leakage attacks in FL is still far from satisfactory. In this paper, we introduce the Gradient Guided Diffusion Model (GGDM), a novel learning-free approach based on a pre-trained unconditional Denoising Diffusion Probabilistic Models (DDPM), aimed at improving the effectiveness and reducing the difficulty of implementing gradient based privacy attacks on complex networks and high-resolution images. To the best of our knowledge, this is the first work to employ the DDPM for privacy leakage attacks of FL. GGDM capitalizes on the unique nature of gradients and guides DDPM to ensure that reconstructed images closely mirror the original data. In addition, in GGDM, we elegantly combine the gradient similarity function with the Stochastic Differential Equation (SDE) to guide the DDPM sampling process based on theoretical analysis, and further reveal the impact of common similarity functions on data reconstruction. Extensive evaluation results demonstrate the excellent generalization ability of GGDM. Specifically, compared with state-of-the-art methods, GGDM shows clear superiority in both quantitative metrics and visualization, significantly enhancing the reconstruction quality of privacy attacks.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Federated+Learning+Vulnerabilities:+Privacy+Attacks+with+Denoising+Diffusion+Probabilistic+Models)|0|
|[Fair Graph Representation Learning via Sensitive Attribute Disentanglement](https://doi.org/10.1145/3589334.3645532)|Yuchang Zhu, Jintang Li, Zibin Zheng, Liang Chen||Group fairness for Graph Neural Networks (GNNs), which emphasizes algorithmic decisions neither favoring nor harming certain groups defined by sensitive attributes (e.g., race and gender), has gained considerable attention. In particular, the objective of group fairness is to ensure that the decisions made by GNNs are independent of the sensitive attribute. To achieve this objective, most existing approaches involve eliminating sensitive attribute information in node representations or algorithmic decisions. However, such ways may also eliminate task-related information due to its inherent correlation with the sensitive attribute, leading to a sacrifice in utility. In this work, we focus on improving the fairness of GNNs while preserving task-related information and propose a fair GNN framework named FairSAD. Instead of eliminating sensitive attribute information, FairSAD enhances the fairness of GNNs via Sensitive Attribute Disentanglement (SAD), which separates the sensitive attribute-related information into an independent component to mitigate its impact. Additionally, FairSAD utilizes a channel masking mechanism to adaptively identify the sensitive attribute-related component and subsequently decorrelates it. Overall, FairSAD minimizes the impact of the sensitive attribute on GNN outcomes rather than eliminating sensitive attributes, thereby preserving task-related information associated with the sensitive attribute. Furthermore, experiments conducted on several real-world datasets demonstrate that FairSAD outperforms other state-of-the-art methods by a significant margin in terms of both fairness and utility performance. Our source code is available at https://github.com/ZzoomD/FairSAD.||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Graph+Representation+Learning+via+Sensitive+Attribute+Disentanglement)|0|
|[DPAR: Decoupled Graph Neural Networks with Node-Level Differential Privacy](https://doi.org/10.1145/3589334.3645531)|Qiuchen Zhang, HongKyu Lee, Jing Ma, Jian Lou, Carl Yang, Li Xiong||Graph Neural Networks (GNNs) have achieved great success in learning with graph-structured data. Privacy concerns have also been raised for the trained models which could expose the sensitive information of graphs including both node features and the structure information. In this paper, we aim to achieve node-level differential privacy (DP) for training GNNs so that a node and its edges are protected. Node DP is inherently difficult for GNNs because all direct and multi-hop neighbors participate in the calculation of gradients for each node via layer-wise message passing and there is no bound on how many direct and multi-hop neighbors a node can have, so existing DP methods will result in high privacy cost or poor utility due to high node sensitivity. We propose a Decoupled GNN with Differentially Private Approximate Personalized PageRank (DPAR) for training GNNs with an enhanced privacy-utility tradeoff. The key idea is to decouple the feature projection and message passing via a DP PageRank algorithm which learns the structure information and uses the top-K neighbors determined by the PageRank for feature aggregation. By capturing the most important neighbors for each node and avoiding the layer-wise message passing, it bounds the node sensitivity and achieves improved privacy-utility tradeoff compared to layer-wise perturbation based methods. We theoretically analyze the node DP guarantee for the two processes combined together and empirically demonstrate better utilities of DPAR with the same level of node DP compared with state-of-the-art methods.|图形神经网络(GNN)在利用图形结构数据进行学习方面取得了巨大的成功。训练后的模型可以同时暴露图的敏感信息，包括节点特征和结构信息，这也引起了人们对隐私的关注。在本文中，我们的目标是实现节点级差分隐私(DP)来训练 GNN，以保护节点及其边缘。由于所有直接和多跳邻居通过分层消息传递参与计算每个节点的梯度，而且节点可以拥有多少直接和多跳邻居没有限制，所以现有的 DP 方法由于节点灵敏度高而导致隐私成本高或效用低。我们提出了一种具有差分私有近似个性化 PageRank (DPAR)的解耦 GNN，用于训练具有增强的隐私-效用折衷的 GNN。其核心思想是通过 DP PageRank 算法解耦特征投影和消息传递，该算法学习结构信息，利用 PageRank 确定的前 K 邻居进行特征聚合。该算法通过捕获每个节点的最重要邻居，避免了分层消息传递，限制了节点的灵敏度，与分层扰动方法相比，实现了更好的隐私效用折衷。从理论上分析了这两种方法结合的节点 DP 保证，并通过实验验证了相同节点 DP 水平的 DPAR 方法比现有的方法具有更好的实用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DPAR:+Decoupled+Graph+Neural+Networks+with+Node-Level+Differential+Privacy)|0|
|[A Worldwide View on the Reachability of Encrypted DNS Services](https://doi.org/10.1145/3589334.3645539)|Ruixuan Li, Baojun Liu, Chaoyi Lu, Haixin Duan, Jun Shao||To protect user DNS privacy, four DNS over Encryption (DoE) protocols have been proposed, including DNS over TLS (DoT), DNS over HTTPS (DoH), DNS over QUIC (DoQ), and DNS over HTTP/3 (DoH3). Ensuring reachability stands as a prominent prerequisite for the proper functionality of these DoE protocols, driving considerable efforts in this domain. However, existing studies predominantly concentrate on a limited number of DoT/DoH domains or employ a restricted subset of vantage points (VPs). In this paper, we present the first comprehensive worldwide view of DoE service reachability. By collecting data from our 15-month-long scan, we elaborately built a list of 1302 operational DoE domains as measurement targets, 448 of which support IPv6. Then we performed 10M DoE over IPv4 (DoEv4) and 570K DoE over IPv6 (DoEv6) queries from 5K VPs over two months, encompassing 102 countries/regions. Our results reveal that the reachability of DoE services is poor in some countries/regions. Specifically, 592K (5.92%) DoEv4 queries and 28K (4.91%) DoEv6 queries are blocked. In countries/regions with strict Internet control, DoEv4 service blocking often occurs during TCP connection and QUIC version negotiation. Compared to DoEv4, the reachability of DoEv6 services is better. In particular, some DoE blocking policies target only specific IP addresses or DoE protocols, providing clients with the opportunity to access blocked DoE domains. Our study highlights the need for the DNS community to pay attention and improve the reachability of DoE services.|为了保护用户的 DNS 隐私，提出了四种 DNS over Encryption (DoE)协议，包括 DNS over TLS (DoT)、 DNS over HTTPS (DoH)、 DNS over QUIC (DoQ)和 DNS over HTTP/3(DoH3)。确保可达性是这些 DoE 协议正确功能的一个突出的先决条件，推动了这个领域的大量工作。然而，现有的研究主要集中在有限数量的 DoT/DoH 领域或采用有限的有利点子集(VP)。在本文中，我们首次提出了 DoE 服务可达性的全球综合观点。通过收集长达15个月的扫描数据，我们精心构建了一个包含1302个可运行 DoE 域的列表作为测量目标，其中448个域支持 IPv6。然后，我们在两个月内对来自5K VP 的 IPv4(DoEv4)执行10M DoE，对 IPv6(DoEv6)执行570K DoE，覆盖102个国家/地区。我们的研究结果表明，在一些国家/地区，能源部服务的可及性较差。具体来说，592K (5.92%) DoEv4查询和28K (4.91%) DoEv6查询被阻塞。在互联网控制严格的国家/地区，在 TCP 连接和 QUIC 版本协商过程中，DoEv4服务阻塞经常发生。与 DoEv4相比，DoEv6服务的可达性更好。特别是，一些 DoE 阻塞策略只针对特定的 IP 地址或 DoE 协议，为客户机提供访问被阻塞的 DoE 域的机会。我们的研究强调了 DNS 社区需要关注并提高 DoE 服务的可达性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Worldwide+View+on+the+Reachability+of+Encrypted+DNS+Services)|0|
|[Contrastive Fingerprinting: A Novel Website Fingerprinting Attack over Few-shot Traces](https://doi.org/10.1145/3589334.3645575)|Yi Xie, Jiahao Feng, Wenju Huang, Yixi Zhang, Xueliang Sun, Xiaochou Chen, Xiapu Luo||Website Fingerprinting (WF) attacks enable passive adversaries to identify the website a user visits over encrypted or anonymized network connections. WF attacks based on deep learning have achieved high accuracy in identifying websites based on abundant training traffic traces per website. However, collecting large-scale and fresh traces is quite cost-consuming and unrealistic. Morevoer, these deep-learning-based WF attacks lack flexibility because they require a long bootstrap time for retraining when facing new traffic traces with different distributions or newly added monitored websites. This paper proposes a high-accuracy WF attack named Contrastive Fingerprinting (CF), which leverages contrastive learning and data augmentation over a few training traces. The results of extensive experiments on challenging datasets over few-shot traces demonstrate the high accuracy of the CF attack and its robustness against WF defenses. For example, when each monitored website only has 20 training traces, CF identifies monitored websites with a high accuracy of 90.4% in the closed-world scenario and distinguishes monitored websites with a high True Positive Rate of 91.2% in the open-world scenario. The experimental results also show that CF outperforms two existing WF attacks with few-shot traces under different network conditions in real-world applications.|网站指纹(WF)攻击使被动对手能够通过加密或匿名的网络连接识别用户访问的网站。基于深度学习的 WF 攻击在基于每个网站丰富的训练流量轨迹识别网站方面取得了很高的准确性。然而，收集大规模和新鲜的痕迹是相当费用和不现实的。此外，这些基于深度学习的 WF 攻击缺乏灵活性，因为它们在面对不同发行版或新增监控网站的新流量跟踪时，需要较长的自举时间进行再训练。提出了一种高精度的 WF 攻击方法——对比指纹(CF) ，该方法利用对比学习和数据增强技术对少量训练轨迹进行攻击。通过大量的实验证明了 CF 攻击的高精度和对抗 WF 攻击的鲁棒性。例如，当每个被监测的网站只有20个训练轨迹时，CF 在封闭世界场景中以90.4% 的高准确度识别被监测的网站，并在开放世界场景中以91.2% 的高真实阳性率区分被监测的网站。实验结果还表明，在实际应用中，CF 在不同网络条件下的性能优于现有的两种具有少镜头跟踪的 WF 攻击。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Fingerprinting:+A+Novel+Website+Fingerprinting+Attack+over+Few-shot+Traces)|0|
|[Analyzing Ad Exposure and Content in Child-Oriented Videos on YouTube](https://doi.org/10.1145/3589334.3645585)|Emaan Bilal Khan, Nida Tanveer, Aima Shahid, Mohammad Jaffer Iqbal, Haashim Ali Mirza, Armish Javed, Ihsan Ayyub Qazi, Zafar Ayyub Qazi||As a popular choice for video and entertainment streaming, YouTube hosts a large audience, including children, who form a growing proportion of its users. Despite separate "made for kids" labelling and stricter moderation of these videos, inappropriate advertising remains a concern as it threatens the safety of YouTube for young viewers. This paper is the first comparative measurement study that explores how advertisement exposure and content vary across child-oriented videos on YouTube. We do this by conducting a cross-regional advertisement analysis on highly viewed "made for kids" labelled content across a total of ten countries with varying regulation. A second front of comparison is carried out between ad patterns on unlabelled and labelled child-oriented videos. Our analysis reveals that the safety of a child's YouTube experience is shaped significantly by their external environment. There also appears to be lax enforcement of YouTube ad and child protection policies, indicated by the presence of unlabelled child-oriented content with weak ad regulation. We discuss the implications of inappropriate exposure on children and suggest policy and implementation measures to mitigate this threat.|作为视频和娱乐流媒体的流行选择，YouTube 拥有包括儿童在内的大量观众，他们在其用户中所占的比例越来越大。尽管这些视频有单独的“儿童专用”标签和更严格的审核，但不恰当的广告仍然是一个问题，因为它威胁到年轻观众在 YouTube 上的安全。这篇论文是第一篇比较测量研究，探讨了 YouTube 上面向儿童的视频的广告曝光和内容是如何变化的。我们通过跨地区的广告分析来做到这一点，这些广告分析涵盖了10个不同国家的广告标签内容，这些国家的广告标签内容受到了高度关注。第二个方面是对无标签和有标签的面向儿童的视频的广告模式进行比较。我们的分析表明，儿童的 YouTube 体验的安全性在很大程度上取决于他们所处的外部环境。YouTube 广告和儿童保护政策的执行似乎也不严格，表现在广告监管薄弱、没有标识的面向儿童的内容。我们讨论了不适当的暴露对儿童的影响，并建议政策和执行措施，以减轻这一威胁。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analyzing+Ad+Exposure+and+Content+in+Child-Oriented+Videos+on+YouTube)|0|
|[A Study of GDPR Compliance under the Transparency and Consent Framework](https://doi.org/10.1145/3589334.3645618)|Michael Smith, Antonio TorresAgüero, Riley Grossman, Pritam Sen, Yi Chen, Cristian Borcea||This paper presents a study of GDPR compliance under the Interactive Advertising Bureau Europe's Transparency and Consent Framework (TCF). This framework provides digital advertising market participants a standard for sharing users' privacy consent choices. TCF is widely used across the Internet, and this paper presents a thorough experimental evaluation of both the compliance of websites with TCF and its impact on user privacy. We reviewed 2,230 websites that use TCF and accepted the automatic decline of user consent by our data collection system. Unlike previous work on GDPR compliance, we found that most websites using TCF properly record the user's consent choice. However, we found that 72.8% of the websites that were TCF compliant claimed legitimate interest as a rationale for overriding the consent choice. While legitimate interest is legal under GDPR, previous studies have shown that most users disagreed with how it is being used to collect data. Additionally, analysis of cookies set to the browsers indicates that TCF may not fully protect user privacy even when websites are compliant. Our research provides regulators and publishers with a data collection and analysis system to monitor compliance, detect non-compliance, and examine questionable practices of circumventing user consent choices using legitimate interest.|本文介绍了在欧洲互动广告局的透明度和同意框架(TCF)下对 GDPR 遵守情况的研究。该框架为数字广告市场参与者提供了一个共享用户隐私同意选择的标准。TCF 在互联网上得到了广泛的应用，本文对网站遵从 TCF 及其对用户隐私的影响进行了全面的实验评估。我们审查了2,230个使用 TCF 的网站，并接受我们的数据收集系统自动拒绝用户同意。与以前的 GDPR 合规工作不同，我们发现大多数使用 TCF 的网站正确地记录了用户的同意选择。然而，我们发现72.8% 的与 TCF 兼容的网站声称合法的利益是推翻同意选择的理由。虽然根据 GDPR，合法利益是合法的，但以前的研究表明，大多数用户不同意如何使用它来收集数据。此外，对浏览器 Cookie 设置的分析表明，即使网站符合要求，TCF 可能也不能完全保护用户隐私。我们的研究为监管机构和出版商提供了一个数据收集和分析系统，以监测遵守情况，发现不遵守情况，并审查利用合法利益规避用户同意选择的可疑做法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Study+of+GDPR+Compliance+under+the+Transparency+and+Consent+Framework)|0|
|[Breaking the Trilemma of Privacy, Utility, and Efficiency via Controllable Machine Unlearning](https://doi.org/10.1145/3589334.3645669)|Zheyuan Liu, Guangyao Dou, Eli Chien, Chunhui Zhang, Yijun Tian, Ziwei Zhu||Machine Unlearning (MU) algorithms have become increasingly critical due to the imperative adherence to data privacy regulations. The primary objective of MU is to erase the influence of specific data samples on a given model without the need to retrain it from scratch. Accordingly, existing methods focus on maximizing user privacy protection. However, there are different degrees of privacy regulations for each real-world web-based application. Exploring the full spectrum of trade-offs between privacy, model utility, and runtime efficiency is critical for practical unlearning scenarios. Furthermore, designing the MU algorithm with simple control of the aforementioned trade-off is desirable but challenging due to the inherent complex interaction. To address the challenges, we present Controllable Machine Unlearning (ConMU), a novel framework designed to facilitate the calibration of MU. The ConMU framework contains three integral modules: an important data selection module that reconciles the runtime efficiency and model generalization, a progressive Gaussian mechanism module that balances privacy and model generalization, and an unlearning proxy that controls the trade-offs between privacy and runtime efficiency. Comprehensive experiments on various benchmark datasets have demonstrated the robust adaptability of our control mechanism and its superiority over established unlearning methods. ConMU explores the full spectrum of the Privacy-Utility-Efficiency trade-off and allows practitioners to account for different real-world regulations. Source code available at: https://github.com/guangyaodou/ConMU|由于必须遵守数据隐私规定，机器学习(MU)算法已经变得越来越重要。MU 的主要目标是消除特定数据样本对给定模型的影响，而不需要从头开始重新训练它。因此，现有的方法侧重于最大限度地保护用户隐私。然而，对于每个现实世界的网络应用程序，隐私规定的程度是不同的。探索隐私、模型实用程序和运行时效率之间的全部权衡对于实际的忘却场景是至关重要的。此外，设计的 MU 算法与简单的控制上述权衡是可取的，但具有挑战性，由于固有的复杂的交互作用。为了应对这些挑战，我们提出了可控机器学习(ConMU) ，一种新的框架，旨在促进校准的 MU。ConMU 框架包含三个整体模块: 一个协调运行效率和模型泛化的重要数据选择模块，一个平衡隐私和模型泛化的渐进高斯机制模块，以及一个控制隐私和运行效率之间权衡的无学习代理。在各种基准数据集上的综合实验表明了我们的控制机制的鲁棒适应性及其相对于已建立的去学习方法的优越性。ConMU 探索了隐私-效用-效率权衡的全部范围，并允许从业者解释不同的现实世界的法规。源代码可在以下 https://github.com/guangyaodou/conmu 获得:|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Breaking+the+Trilemma+of+Privacy,+Utility,+and+Efficiency+via+Controllable+Machine+Unlearning)|0|
|[Heterogeneous Subgraph Transformer for Fake News Detection](https://doi.org/10.1145/3589334.3645680)|Yuchen Zhang, Xiaoxiao Ma, Jia Wu, Jian Yang, Hao Fan||Fake news is pervasive on social media, inflicting substantial harm on public discourse and societal well-being. We investigate the explicit structural information and textual features of news pieces by constructing a heterogeneous graph concerning the relations among news topics, entities, and content. Through our study, we reveal that fake news can be effectively detected in terms of the atypical heterogeneous subgraphs centered on them, which encapsulate the essential semantics and intricate relations between news elements. However, suffering from the heterogeneity, exploring such heterogeneous subgraphs remains an open problem. To bridge the gap, this work proposes a heterogeneous subgraph transformer (HeteroSGT) to exploit subgraphs in our constructed heterogeneous graph. In HeteroSGT, we first employ a pre-trained language model to derive both word-level and sentence-level semantics. Then the random walk with restart (RWR) is applied to extract subgraphs centered on each news, which are further fed to our proposed subgraph Transformer to quantify the authenticity. Extensive experiments on five real-world datasets demonstrate the superior performance of HeteroSGT over five baselines. Further case and ablation studies validate our motivation and demonstrate that performance improvement stems from our specially designed components.|假新闻在社交媒体上十分普遍，对公共话语和社会福祉造成了巨大损害。我们通过构建一个关于新闻主题、实体和内容之间关系的异质图来研究新闻作品的显性结构信息和文本特征。通过本文的研究，我们发现以假新闻为中心的非典型异质子图可以有效地识别假新闻，它包含了新闻要素之间的本质语义和错综复杂的关系。然而，由于受异质性的影响，探索这样的异质子图仍然是一个悬而未决的问题。为了弥补这一缺陷，本文提出了一种异构子图转换器(HeroSGT)来开发我们构造的异构图中的子图。在异构 SGT 中，我们首先使用一个预先训练好的语言模型来推导词层和句子层的语义。然后应用随机重启游动(RWR)提取以每条新闻为中心的子图，并将子图进一步反馈给我们提出的子图变换器，以量化子图的真实性。在五个真实世界数据集上的大量实验证明了异构 SGT 在五个基线上的优越性能。进一步的案例和消融研究验证了我们的动机，并证明性能改进源于我们特别设计的组件。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Heterogeneous+Subgraph+Transformer+for+Fake+News+Detection)|0|
|[Experimental Security Analysis of Sensitive Data Access by Browser Extensions](https://doi.org/10.1145/3589334.3645683)|Asmit Nayak, Rishabh Khandelwal, Earlence Fernandes, Kassem Fawaz||Browser extensions offer a variety of valuable features and functionalities. They also pose a significant security risk if not properly designed or reviewed. Prior works have shown that browser extensions can access and manipulate data fields, including sensitive data such as passwords, credit card numbers, and Social Security numbers. In this paper, we present an empirical study of the security risks posed by browser extensions. Specifically, we first build a proof-of-concept extension that can steal sensitive user information. We find that the extension passes the Chrome Webstore review process. We then perform a measurement study on the top 10K website login pages to check if the extension access to password fields via JS. We find that none of the password fields are actively protected, and can be accessed using JS. Moreover, we found that 1K websites store passwords in plaintext in their page source, including popular websites like Google.com and Cloudflare.com. We also analyzed over 160K Chrome Web Store extensions for malicious behavior, finding that 28K have permission to access sensitive fields and 190 store password fields in variables. To analyze the behavioral workflow of the potentially malicious extensions, we propose an LLM-driven framework, Extension Reviewer. Finally, we discuss two countermeasures to address these risks: a bolt-on JavaScript package for immediate adoption by website developers allowing them to protect sensitive input fields, and a browser-level solution that alerts users when an extension accesses sensitive input fields. Our research highlights the urgent need for improved security measures to protect sensitive user information online.|浏览器扩展提供了各种有价值的特性和功能。如果设计或审查不当，它们也会带来重大的安全风险。以前的工作表明，浏览器扩展可以访问和操作数据字段，包括敏感数据，如密码，信用卡号码和社会安全号码。本文对浏览器扩展带来的安全风险进行了实证研究。具体来说，我们首先构建一个概念验证扩展，它可以窃取敏感的用户信息。我们发现该扩展通过了 ChromeWebstore 的审查过程。然后，我们对前10K 网站登录页面进行测量研究，检查是否通过 JS 扩展访问密码字段。我们发现没有一个密码字段受到主动保护，可以使用 JS 访问。此外，我们发现1K 网站在其页面源代码中以明文形式存储密码，包括像 Google.com 和 Cloudflare.com 这样的流行网站。我们还分析了超过160K 的 Chrome 网上商店扩展的恶意行为，发现28K 有权访问敏感字段和190个存储密码字段的变量。为了分析潜在恶意扩展的行为工作流，我们提出了一个 LLM 驱动的框架，扩展审查者。最后，我们讨论了解决这些风险的两个对策: 一个插件式的 JavaScript 包，网站开发人员可以立即采用，允许他们保护敏感的输入字段，以及一个浏览器级别的解决方案，当扩展访问敏感的输入字段时提醒用户。我们的研究强调，迫切需要改进安全措施，以保护网上敏感的用户信息。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Experimental+Security+Analysis+of+Sensitive+Data+Access+by+Browser+Extensions)|0|
|[Automating Website Registration for Studying GDPR Compliance](https://doi.org/10.1145/3589334.3645709)|Karel Kubicek, Jakob Merane, Ahmed Bouhoula, David A. Basin||Investigating how websites use sensitive user data is an active research area. However, research based on automated measurements has been limited to those websites that do not require user authentication. To overcome this limitation, we developed a crawler that automates website registrations and newsletter subscriptions and detects both security and privacy threats at scale. We demonstrate our crawler's capabilities by running it on 660k websites. We use this to identify security and privacy threats and to contextualize them within EU laws, namely the General Data Protection Regulation and ePrivacy Directive. Our methods detect private data collection over insecure HTTP connections and websites sending emails with user-provided passwords. We are also the first to apply machine learning to web forms, assessing violations of marketing consent collection requirements. Overall, we find that 37.2% of websites send marketing emails without proper user consent. This is mostly caused by websites failing both to verify and store consent adequately. Additionally, 1.8% of websites share users' email addresses with third parties without a transparent disclosure.|调查网站如何使用敏感用户数据是一个活跃的研究领域。然而，基于自动测量的研究仅限于那些不需要用户认证的网站。为了克服这个限制，我们开发了一个爬虫程序，可以自动进行网站注册和新闻订阅，并检测大规模的安全和隐私威胁。我们证明了我们的爬虫的能力，通过运行它在660k 网站。我们利用这一点来识别安全和隐私威胁，并将其置于欧盟法律的背景之下，即《一般数据保护条例》和《电子隐私指令》。我们的方法通过不安全的 HTTP 连接和网站发送用户提供密码的电子邮件来检测私人数据收集。我们也是第一个将机器学习应用于网络表单，评估违反营销许可收集要求的行为。总的来说，我们发现37.2% 的网站在未经用户同意的情况下发送营销电子邮件。这主要是由于网站未能充分核实和存储同意书造成的。此外，1.8% 的网站在没有透明披露的情况下与第三方共享用户的电子邮件地址。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automating+Website+Registration+for+Studying+GDPR+Compliance)|0|
|[Generating Multi-turn Clarification for Web Information Seeking](https://doi.org/10.1145/3589334.3645712)|Ziliang Zhao, Zhicheng Dou||Asking multi-turn clarifying questions has been applied in various conversational search systems to help recommend people, commodities, and images to users. However, its importance is still not emphasized in the Web search. In this paper, we make a step to extend the multi-turn clarification generation to Web search for clarifying users' ambiguous or faceted intents. Compared with other conversational search scenarios, Web search queries are more complicated, so clarification should be generated instead of being selected which is commonly applied in current studies. To this end, we first define the whole process of multi-turn Web search clarification composed of clarification candidate generation, optimal clarification selection, and document retrieval. Due to the lack of multi-turn open-domain clarification data, we first design a simple yet effective rule-based method to fit the above three components. After that, by utilizing the in-context learning and zero-shot instruction ability of large language models (LLMs), we implement clarification generation and selection by prompting LLMs with demonstrations and declarations, further improving the clarification effectiveness. To evaluate our proposed methods, we first measure whether our methods can improve the ability to retrieve documents. We also evaluate the quality of generated candidate facets. Experimental results show that, compared with existing single-turn methods for Web search clarification, our proposed framework is more suitable for open-domain Web search systems in asking multi-turn clarification questions to clarify users' ambiguous or faceted intents.|在各种会话搜索系统中，提出多回合澄清问题可以帮助向用户推荐人物、商品和图片。然而，它的重要性仍然没有被强调在网络搜索。本文将多回合澄清生成算法扩展到网络搜索，以澄清用户的模糊意图或多面意图。与其他会话搜索场景相比，网络搜索查询更加复杂，因此需要对其进行澄清，而不是像目前研究中常用的那样进行选择。为此，我们首先定义了由澄清候选者生成、最佳澄清选择和文献检索组成的多回合网络搜索澄清的整个过程。由于缺乏多回合开放域澄清数据，我们首先设计了一个简单而有效的基于规则的方法来适应上述三个组件。然后，利用大型语言模型的上下文学习和零指令能力，通过演示和声明提示大型语言模型来实现澄清生成和选择，进一步提高澄清的有效性。为了评估我们提出的方法，我们首先衡量我们的方法是否能够提高检索文档的能力。我们还评估了生成的候选方面的质量。实验结果表明，与现有的单回合澄清方法相比，本文提出的框架更适合于开放域 Web 搜索系统提出多回合澄清问题来澄清用户的模糊或多面意图。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generating+Multi-turn+Clarification+for+Web+Information+Seeking)|0|
|[Advancing Web 3.0: Making Smart Contracts Smarter on Blockchain](https://doi.org/10.1145/3589334.3645319)|Junqin Huang, Linghe Kong, Guanjie Cheng, Qiao Xiang, Guihai Chen, Gang Huang, Xue Liu||Blockchain and smart contracts are one of the key technologies promoting Web 3.0. However, due to security considerations and consistency requirements, smart contracts currently only support simple and deterministic programs, which significantly hinders their deployment in intelligent Web 3.0 applications. To enhance smart contracts intelligence on the blockchain, we propose SMART, a plug-in smart contract framework that supports efficient AI model inference while being compatible with existing blockchains. To handle the high complexity of model inference, we propose an on-chain and off-chain joint execution model, which separates the SMART contract into two parts: the deterministic code still runs inside an on-chain virtual machine, while the complex model inference is offloaded to off-chain compute nodes. To solve the non-determinism brought by model inference, we leverage Trusted Execution Environments (TEEs) to endorse the integrity and correctness of the off-chain execution. We also design distributed attestation and secret key provisioning schemes to further enhance the system security and model privacy. We implement a SMART prototype and evaluate it on a popular Ethereum Virtual Machine (EVM)-based blockchain. Theoretical analysis and prototype evaluation show that SMART not only achieves the security goals of correctness, liveness, and model privacy, but also has approximately 5 orders of magnitude faster inference efficiency than existing on-chain solutions.|区块链和智能合同是推动 Web 3.0的关键技术之一。然而，出于安全考虑和一致性要求，智能契约目前只支持简单和确定性的程序，这严重阻碍了它们在智能 Web 3.0应用程序中的部署。为了增强区块链上的智能契约智能，我们提出了 SMART，一个插件式的智能契约框架，支持有效的 AI 模型推理，同时兼容现有的区块链。针对模型推理的高度复杂性，提出了一种链上和链下联合执行模型，该模型将 SMART 契约分为两部分: 确定性代码仍然在链上虚拟机中运行，而复杂的模型推理则被卸载到链外计算节点上。为了解决模型推理带来的不确定性，我们利用可信执行环境(Trusted Execution Environment，TEE)来认可脱链执行的完整性和正确性。设计了分布式认证和密钥供应方案，进一步提高了系统的安全性和模型的隐私性。我们实现了一个 SMART 原型，并在一个流行的基于以太虚拟机(EVM)的区块链上进行了评估。理论分析和原型评估表明，SMART 不仅实现了正确性、活性和模型隐私性的安全目标，而且比现有的数量级上解决方案具有大约5倍的推理效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Advancing+Web+3.0:+Making+Smart+Contracts+Smarter+on+Blockchain)|0|
|[From Promises to Practice: Evaluating the Private Browsing Modes of Android Browser Apps](https://doi.org/10.1145/3589334.3645320)|Xiaoyin Liu, Wenzhi Li, Qinsheng Hou, Shishuai Yang, Lingyun Ying, Wenrui Diao, Yanan Li, Shanqing Guo, Haixin Duan||Private browsing is a common feature of web browsers on desktop platforms. This feature protects the privacy of users browsing the Internet and, therefore, is widely welcomed by users. In recent years, with the popularity of smartphones, the private browsing mode has been introduced into mobile browsers. However, its deployment on mobile platforms has not been well evaluated. To bridge the gap, in this work, we systemically studied the private browsing modes of Android browser apps. Specifically, we proposed six private rules for mobile browsers to follow by combining the mobile browsing features with the previous research on private browsing. Furthermore, we designed an automated analysis framework, BroDroid, to detect whether mobile browsers violate these rules. Also, with BroDroid, we evaluated 49 popular browser apps crawled from Google Play. Finally, BroDroid successfully identified 58 violations, some of which come from the promised capabilities of the browser. We reported our discovered issues to the corresponding developers, and four of them (Yandex Browser, Mint Browser, Web Explorer, and Net Fast Web Browser) have acknowledged our findings. Our observation may be the tip of the iceberg, and more efforts should be put into improving the privacy protections of mobile browsers.|私人浏览是桌面平台上浏览器的一个共同特征。这项功能保护浏览互联网的用户的隐私，因此受到用户的广泛欢迎。近年来，随着智能手机的普及，私人浏览模式被引入到移动浏览器中。然而，它在移动平台上的部署还没有得到很好的评估。为了弥补这一差距，在这项工作中，我们系统地研究了 Android 浏览器应用程序的私人浏览模式。结合移动浏览器的特点，结合前人对私有浏览器的研究，提出了移动浏览器应遵循的六条私有规则。此外，我们还设计了一个自动分析框架 BroDroid 来检测移动浏览器是否违反了这些规则。此外，使用 BroDroid，我们评估了49个从 GooglePlay 爬行而来的流行浏览器应用程序。最后，BroDroid 成功地识别出58个违规行为，其中一些来自于浏览器所承诺的功能。我们向相应的开发人员报告了我们发现的问题，其中四个(Yandex Browser、 Mint Browser、 Web Explorer 和 Net Fast Web Browser)承认了我们的发现。我们的观察可能只是冰山一角，我们应该投入更多的努力来改善移动浏览器的隐私保护。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Promises+to+Practice:+Evaluating+the+Private+Browsing+Modes+of+Android+Browser+Apps)|0|
|[Interface Illusions: Uncovering the Rise of Visual Scams in Cryptocurrency Wallets](https://doi.org/10.1145/3589334.3645348)|Guoyi Ye, Geng Hong, Yuan Zhang, Min Yang||Cryptocurrencies, while revolutionary, have become a magnet for malicious actors. With numerous reports underscoring cyberattacks and scams in this domain, our paper takes the lead in characterizing visual scams associated with cryptocurrency wallets---a fundamental component of Web3. Specifically, scammers capitalize on the omission of vital wallet interface details, such as token symbols, wallet addresses, and smart contract function names, to mislead users, potentially resulting in unintended financial losses. Analyzing Ethereum blockchain transactions from July 2022 to June 2023, we uncovered a total of 24,901,115 visual scam incidents, which include 3,585,493 counterfeit token attacks, 21,281,749 zero-transfer attacks, and 33,873 function name attacks, orchestrated by 6,768 distinct attackers. Shockingly, over 28,414 victims fell prey to these scams, with losses surpassing 27 million USD. This alarming data underscores the pressing need for robust protective measures. By profiling the typical victims and attackers, we are able to propose mitigation strategies informed by our findings.|加密货币虽然具有革命性，但已成为恶意行为者的磁石。由于许多报告强调了这个领域的网络攻击和欺诈，我们的论文在描述与加密货币钱包相关的视觉欺诈方面起到了带头作用——加密货币钱包是 Web3的一个基本组成部分。具体来说，骗子利用遗漏重要的钱包界面细节(如令牌符号、钱包地址和智能合同功能名称)来误导用户，可能导致意想不到的财务损失。分析2022年7月至2023年6月的以太坊区块链交易，我们共发现24,901,115起视觉欺诈事件，其中包括3,585,493次伪造令牌攻击，21,281,749次零转移攻击和33,873次功能名称攻击，由6,768名不同的攻击者精心策划。令人震惊的是，超过28,414名受害者成为这些骗局的牺牲品，损失超过2700万美元。这一令人担忧的数据突出表明，迫切需要采取强有力的保护措施。通过分析典型的受害者和攻击者，我们能够根据我们的发现提出缓解策略。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interface+Illusions:+Uncovering+the+Rise+of+Visual+Scams+in+Cryptocurrency+Wallets)|0|
|[Trident: A Universal Framework for Fine-Grained and Class-Incremental Unknown Traffic Detection](https://doi.org/10.1145/3589334.3645407)|Ziming Zhao, Zhaoxuan Li, Zhuoxue Song, Wenhao Li, Fan Zhang||To detect unknown attack traffic, anomaly-based network intrusion detection systems (NIDSs) are widely used in Internet infrastructure. However, the security communities realize some limitations when they put most existing proposals into practice. The challenges are mainly concerned with (i) fine-grained emerging attack detection and (ii) incremental updates/adaptations. To tackle these problems, we propose to decouple the need for model capabilities by transforming known/new class identification issues into multiple independent one-class learning tasks. Based on the above core ideas, we develop Trident, a universal framework for fine-grained unknown encrypted traffic detection. It consists of three main modules, i.e., tSieve, tScissors, and tMagnifier are used for profiling traffic, determining outlier thresholds, and clustering respectively, each of which supports custom configuration. Using four popular datasets of network traces, we show that Trident significantly outperforms 16 state-of-the-art (SOTA) methods. Furthermore, a series of experiments (concept drift, overhead/parameter evaluation) demonstrate the stability, scalability, and practicality of Trident.|为了检测未知攻击流量，基于异常的网络入侵检测系统(NIDS)在 Internet 基础设施中得到了广泛的应用。然而，当安全社区将大多数现有提议付诸实践时，他们意识到了一些局限性。这些挑战主要涉及(i)细粒度的新出现的攻击检测和(ii)增量更新/适应。为了解决这些问题，我们建议通过将已知/新的类识别问题转换为多个独立的单类学习任务来解耦对模型功能的需求。基于上述核心思想，我们开发了一个通用的细粒度未知加密流量检测框架 Trident。它由三个主要模块组成，即 tSieve、 tScissors 和 tMagnifier，分别用于分析流量、确定离群值阈值和聚类，每个模块都支持自定义配置。使用四个流行的网络跟踪数据集，我们表明 Trident 显著优于16种最先进的(SOTA)方法。此外，一系列的实验(概念漂移、开销/参数评估)证明了三叉戟的稳定性、可扩展性和实用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Trident:+A+Universal+Framework+for+Fine-Grained+and+Class-Incremental+Unknown+Traffic+Detection)|0|
|[SSI, from Specifications to Protocol? Formally Verify Security!](https://doi.org/10.1145/3589334.3645426)|Christoph H.J. Braun, Ross Horne, Tobias Käfer, Sjouke Mauw|University of Strathclyde, Glasgow, United Kingdom; University of Luxemburg, Esch-sur-Alzette, Luxembourg; Karlsruhe Institute of Technology, Karlsruhe, Germany|We evaluate a bundle of specifications from the Self-Sovereign Identity (SSI) paradigm to construct an authentication protocol for the Web. We demonstrate how relevant standards such as W3C Verifiable Credentials (VC), W3C Decentralised Identifiers (DIDs), and components of the Hyperledger Aries Framework are to be assembled methodologically into a protocol. We make those assumptions from standard trust models explicit that underlie the derived protocol, and verify security and privacy properties, notably secrecy, authentication, and unlinkability. This enables us to formally justify the additional precision that we urge these specifications to consider, to ensure that implementors of SSI-based systems do not neglect security-critical controls.|我们评估来自自主身份(SSI)范式的一组规范，以便为 Web 构建身份验证协议。我们展示了如何将 W3C 可验证凭证(VC)、 W3C 分散标识符(DID)和 Hyperledger Aries 框架的组件等相关标准在方法学上组装成一个协议。我们使这些假设从标准信任模型明确的基础上派生的协议，并验证安全和隐私属性，特别是保密性，认证和不可链接性。这使我们能够正式证明我们敦促这些规范考虑的额外精确性，以确保基于 SSI 的系统的实现者不会忽视安全关键控制。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SSI,+from+Specifications+to+Protocol?+Formally+Verify+Security!)|0|
|[GRASP: Hardening Serverless Applications through Graph Reachability Analysis of Security Policies](https://doi.org/10.1145/3589334.3645436)|Isaac Polinsky, Pubali Datta, Adam Bates, William Enck||Serverless computing is supplanting past versions of cloud computing as the easiest way to rapidly prototype and deploy applications. However, the reentrant and ephemeral nature of serverless functions only exacerbates the challenge of correctly specifying security policies. Unfortunately, with role-based access control solutions like Amazon Identity and Access Management (IAM) already suffering from pervasive misconfiguration problems, the likelihood of policy failures in serverless applications is high. In this work, we introduce GRASP, a graph-based analysis framework for modeling serverless access control policies as queryable reachability graphs. GRASP generates reusable models that represent the principals of a serverless application and the interactions between those principals. We implement GRASP for Amazon IAM in Prolog, then deploy it on a corpus of 731 open source Amazon Lambda applications. We find that serverless policies tend to be short and highly permissive, e.g., 92% of surveyed policies are comprised of just 10 statements and 30% exhibit full reachability between all application functions and resources. We then use GRASP to identify potential attack vectors permitted by these policies, including hundreds of sensitive access channels, a dozen publicly-exposed resources, and four channels that may permit an attacker to exfiltrate an application's private resources through one of its public resources. These findings demonstrate GRASP's utility as a means of identifying opportunities for hardening application policies and highlighting potential exfiltration channels.|无服务器计算正在取代过去的云计算版本，成为快速原型化和部署应用程序的最简单方法。然而，无服务器函数的可重入性和临时性只会加剧正确指定安全策略的挑战。不幸的是，由于像亚马逊身份和访问管理(IAM)这样的以角色为基础的存取控制解决方案已经受到普遍的配置错误问题的困扰，无服务器应用程序的策略失败的可能性很高。在这项工作中，我们介绍了 GRASP，这是一个基于图的分析框架，用于将无服务器访问控制策略建模为可查询可达性图。GRASP 生成可重用的模型，这些模型表示无服务器应用程序的主体以及这些主体之间的交互。我们在 Prolog 为亚马逊 IAM 实现了 GRASP，然后将其部署到731个开源的亚马逊 Lambda 应用程序中。我们发现，无服务器策略往往是简短和高度宽容的，例如，92% 的调查策略只包含10个语句，30% 的策略表现出所有应用程序功能和资源之间的完全可达性。然后，我们使用 GRASP 来识别这些策略允许的潜在攻击向量，包括数百个敏感的访问通道、十几个公开的资源，以及四个可能允许攻击者通过应用程序的一个公共资源泄露应用程序的私有资源的通道。这些发现证明了 GRASP 的实用性，它可以用来识别加强应用程序策略的机会，并突出潜在的渗透渠道。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GRASP:+Hardening+Serverless+Applications+through+Graph+Reachability+Analysis+of+Security+Policies)|0|
|[Divide, Conquer, and Coalesce: Meta Parallel Graph Neural Network for IoT Intrusion Detection at Scale](https://doi.org/10.1145/3589334.3645457)|Hua Ding, Lixing Chen, Shenghong Li, Yang Bai, Pan Zhou, Zhe Qu||This paper proposes Meta Parallel Graph Neural Network (MPGNN) to establish a scalable Network Intrusion Detection System (NIDS) for large-scale Internet of Things (IoT) networks. MPGNN leverages a meta-learning framework to optimize the parallelism of GNN-based NIDS. The core of MPGNN is a coalition formation policy that generates meta-knowledge for partitioning a massive graph into multiple coalitions/subgraphs in a way that maximizes the performance and efficiency of parallel coalitional NIDSs. We propose an offline reinforcement learning algorithm, called Graph-Embedded Adversarially Trained Actor-Critic (G-ATAC), to learn a coalition formation policy that jointly optimizes intrusion detection accuracy, communication overheads, and computational complexities of coalitional NIDSs. In particular, G-ATAC learns to capture the temporal dependencies of network states and coalition formation decisions over offline data, eliminating the need for expensive online interactions with large IoT networks. Given generated coalitions, MPGNN employs E-GraphSAGE to establish coalitional NIDSs which then collaborate via ensemble prediction to accomplish intrusion detection for the entire network. We evaluate MPGNN on two real-world datasets. The experimental results demonstrate the superiority of our method with substantial improvements in F1 score, surpassing the state-of-the-art methods by 0.38 and 0.29 for the respective datasets. Compared to the centralized NIDS, MPGNN reduces the training time of NIDS by 41.63% and 22.11%, while maintaining an intrusion detection performance comparable to centralized NIDS.|本文提出元并行图神经网络(MPGNN) ，以建立一个可扩展的网络入侵预防系统(nIDS) ，用于大规模的物联网(IoT)网络。MPGNN 利用元学习框架优化基于 GNN 的 NIDS 的并行性。MPGNN 的核心是一种联盟形成策略，该策略通过生成元知识将海量图划分为多个联盟/子图，从而最大限度地提高并行联盟 NIDS 的性能和效率。我们提出了一个离线强化学习算法，称为图嵌入式对抗训练演员批评(G-ATAC) ，以学习一个联盟形成策略，联合优化入侵检测的准确性，通信开销和计算复杂性的联盟 NIDS。特别是，G-ATAC 学习捕获网络状态的时间依赖性和离线数据的联盟形成决策，消除了与大型物联网进行昂贵的在线交互的需要。给定生成的联盟，MPGNN 利用 E-GraphSAGE 建立联盟 NIDS，然后通过集成预测协作完成整个网络的入侵检测。我们在两个实际数据集上评估 MPGNN。实验结果显示了我们的方法的优越性，F1得分有了很大的提高，在各自的数据集上比最先进的方法分别提高了0.38和0.29。与集中式网络入侵检测系统相比，MPGNN 使网络入侵检测系统的训练时间分别减少了41.63% 和22.11% ，同时保持了与集中式网络入侵检测系统相当的入侵检测性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Divide,+Conquer,+and+Coalesce:+Meta+Parallel+Graph+Neural+Network+for+IoT+Intrusion+Detection+at+Scale)|0|
|[Medusa: Unveil Memory Exhaustion DoS Vulnerabilities in Protocol Implementations](https://doi.org/10.1145/3589334.3645476)|Zhengjie Du, Yuekang Li, Yaowen Zheng, Xiaohan Zhang, Cen Zhang, Yi Liu, Sheikh Mahbub Habib, Xinghua Li, Linzhang Wang, Yang Liu, Bing Mao||Web services have brought great convenience to our daily lives. Meanwhile, they are vulnerable to Denial-of-Service (DoS) attacks. DoS attacks launched via vulnerabilities in the services can cause great harm. The vulnerabilities in protocol implementations are especially important because they are the keystones of web services. One vulnerable protocol implementation can affect all the web services built on top of it. Compared to the vulnerabilities that cause the target service to crash, resource exhaustion vulnerabilities are equally if not more important. This is because such vulnerabilities can deplete the system resources, leading to the unavailability of not only the vulnerable service but also other services running on the same machine. Despite the significance of this type of vulnerability, there has been limited research in this area. In this paper, we propose Medusa, a dynamic analysis framework to detect memory exhaustion vulnerabilities in protocol implementations, which are the most common type of resource exhaustion vulnerabilities. Medusa works in two phases: exploration phase and verification. In the exploration phase, a protocol property graph (PPG) is constructed to embed the states with relevant properties including memory consumption information. In the verification phase, the PPG is used to simulate DoS attacks to verify the vulnerabilities. We implemented Medusa and evaluated its performance on 21 implementations of five protocols. The results demonstrate that Medusa outperforms the state-of-the-art techniques by discovering overall 127× maximum memory consumption. Lastly, Medusa has discovered six 0-day vulnerabilities in six protocol implementations for three protocols. Particularly, one of the vulnerabilities was found in Eclipse Mosquitto, which can affect thousands of services and it has been assigned with a CVE ID.|网络服务给我们的日常生活带来了极大的便利。同时，他们也很容易受到拒绝服务(DoS)攻击。通过服务中的漏洞发起的 DoS 攻击会造成很大的危害。协议实现中的漏洞尤其重要，因为它们是 Web 服务的关键。一个易受攻击的协议实现可能会影响构建在其上的所有 Web 服务。与导致目标服务崩溃的漏洞相比，资源耗尽的漏洞同样重要(如果不是更重要的话)。这是因为这样的漏洞可能会耗尽系统资源，导致不仅易受攻击的服务不可用，而且还会导致运行在同一台计算机上的其他服务不可用。尽管这种类型的脆弱性具有重要意义，但在这一领域的研究却很有限。在本文中，我们提出了一个动态分析框架 Medusa 来检测协议实现中的内存耗尽漏洞，这是最常见的资源耗尽漏洞类型。美杜莎的工作分为两个阶段: 探索阶段和验证阶段。在探索阶段，构造一个协议属性图(PPG)来嵌入具有相关属性(包括内存消耗信息)的状态。在验证阶段，使用 PPG 模拟 DoS 攻击来验证漏洞。我们实现了 Medusa，并在5个协议的21个实现上对其性能进行了评估。结果表明，美杜莎优于国家的最先进的技术，发现总体127倍的最大记忆消耗。最后，美杜莎在三个协议的六个协议实现中发现了六个0天漏洞。特别是，在 Eclipse Mosquitto 中发现了一个漏洞，这个漏洞可以影响成千上万的服务，它已经被分配了 CVE ID。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Medusa:+Unveil+Memory+Exhaustion+DoS+Vulnerabilities+in+Protocol+Implementations)|0|
|[ContraMTD: An Unsupervised Malicious Network Traffic Detection Method based on Contrastive Learning](https://doi.org/10.1145/3589334.3645479)|Xueying Han, Susu Cui, Jian Qin, Song Liu, Bo Jiang, Cong Dong, Zhigang Lu, Baoxu Liu||Malicious traffic detection has been a focal point in the field of network security, and deep learning-based approaches are emerging as a new paradigm. However, most of them are supervised methods, which highly depend on well-labeled data, and fail to handle unknown or continuously evolving attacks. Unsupervised methods alleviate the need for labeled data, but existing methods are often limited to detecting anomalies either in vertical perspective through historical comparisons or in horizontal perspective by comparing with concurrent entities. Relying on data from a single perspective is unreliable, and it limits the model's accuracy and generalizability. In this paper, we propose a novel method ContraMTD based on contrastive learning, which comprehensively considers both vertical and horizontal perspectives. ContraMTD extracts local behavior features and global interaction features from normal network traffic by proposed SEC and DE-GAT respectively, then employs contrastive learning to learn the relationship, especially consistency between them, and finally detects malicious traffic through a multi-round scoring approach. We conduct extensive experiments on three datasets, including a self-collected dataset, and the results demonstrate that our method outperforms many state-of-the-art methods in the domain of unsupervised malicious traffic detection.|恶意流量检测已经成为网络安全领域的一个焦点，基于深度学习的方法正在成为一种新的范式。然而，它们大多是监督方法，高度依赖于标记良好的数据，不能处理未知的或不断发展的攻击。无监督方法减少了对标记数据的需求，但现有方法往往局限于通过历史比较从纵向角度检测异常，或者通过与并发实体的比较从横向角度检测异常。依赖于单一角度的数据是不可靠的，这限制了模型的准确性和推广性。本文提出了一种新的基于对比学习的 ContraMTD 方法，该方法综合考虑了纵向和横向视角。ContraMTD 通过提出的 SEC 和 DE-GAT 分别从正常网络流量中提取本地行为特征和全局交互特征，然后利用对比学习来学习这些特征之间的关系，特别是它们之间的一致性，最后通过多轮评分的方法来检测恶意流量。我们在三个数据集上进行了广泛的实验，包括一个自收集的数据集，实验结果表明我们的方法在无监督的恶意流量检测领域优于许多最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ContraMTD:+An+Unsupervised+Malicious+Network+Traffic+Detection+Method+based+on+Contrastive+Learning)|0|
|[Unfiltered: Measuring Cloud-based Email Filtering Bypasses](https://doi.org/10.1145/3589334.3645499)|Sumanth Rao, Enze Liu, Grant Ho, Geoffrey M. Voelker, Stefan Savage||Email service has increasingly been outsourced to cloud-based providers and so too has the task of filtering such messages for potential threats. Thus, customers will commonly direct that their incoming email is first sent to a third-party email filtering service (e.g., Proofpoint or Barracuda) and only the "clean" messages are then sent on to their email hosting provider (e.g., Gmail or Microsoft Exchange Online). However, this loosely coupled approach can, in theory, be bypassed if the email hosting provider is not configured to only accept messages that arrive from the email filtering service. In this paper we demonstrate that such bypasses are commonly possible. We document a multi-step methodology to infer if an organization has correctly configured its email hosting provider to guard against such scenarios. Then, using an empirical measurement of edu and com domains as a case study, we show that 80% of such organizations making use of popular cloud-based email filtering services can be bypassed in this manner. We also discuss reasons that lead to such misconfigurations and outline challenges in hardening the binding between email filtering and hosting providers.|电子邮件服务越来越多地外包给基于云计算的供应商，过滤此类邮件以防范潜在威胁的任务也越来越多。因此，客户通常会指示，他们收到的电子邮件首先发送给第三方电子邮件过滤服务(如 Proofpoint 或 Barracuda) ，然后只有“干净”的邮件被发送到他们的电子邮件托管服务提供商(如 Gmail 或微软在线交换)。然而，理论上，如果电子邮件主机提供商没有配置为只接受来自电子邮件过滤服务的消息，这种松散耦合的方法可以被绕过。在本文中，我们证明这样的旁路是普遍可能的。我们记录了一个多步骤的方法来推断一个组织是否已经正确地配置了它的电子邮件主机提供商来防范这种情况。然后，通过对 edu 和 com 领域的实证测量作为案例研究，我们发现80% 使用流行的基于云的电子邮件过滤服务的组织可以以这种方式绕过。我们还讨论了导致这种错误配置的原因，以及在加强电子邮件过滤和托管服务提供商之间的绑定方面的挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unfiltered:+Measuring+Cloud-based+Email+Filtering+Bypasses)|0|
|[RecurScan: Detecting Recurring Vulnerabilities in PHP Web Applications](https://doi.org/10.1145/3589334.3645530)|Youkun Shi, Yuan Zhang, Tianhao Bai, Lei Zhang, Xin Tan, Min Yang||Detecting recurring vulnerabilities has become a popular means of static vulnerability detection in recent years because they do not require labor-intensive vulnerability modeling. Recently, a body of work, with HiddenCPG as a representative, has redefined the problem of statically identifying recurring vulnerabilities as the subgraph isomorphism problem. More specifically, these approaches represent known vulnerable code as graph-based structures (e.g., PDG or CPG), and then identify subgraphs within target applications that match the vulnerable graphs. However, since these methods are highly sensitive to changes in the code graph, they may miss a significant number of recurring vulnerabilities with slight code differences from known vulnerabilities. In this paper, we propose a novel approach, namely RecurScan, which can accurately detect recurring vulnerabilities with resilience to code differences. To achieve this goal, RecurScan works around security patches and symbolic tracking techniques, detecting recurring vulnerabilities by comparing symbolic expressions and selective constraints between the target applications and known vulnerabilities. Benefiting from this design, RecurScan can tolerate the code differences arising from complex data or control flows within the applications. We evaluated RecurScan on 200 popular PHP web applications using 184 known vulnerability patches. The results demonstrate that RecurScan discovered 232 previously unknown vulnerabilities, 174 of which were assigned CVE identifiers, outperforming state-of-the-art approach (i.e., HiddenCPG) by 25.98% in precision and 87.09% in recall.|近年来，检测重复出现的漏洞已经成为一种流行的静态漏洞检测手段，因为它们不需要劳动密集型的漏洞模型。最近，以 HiddenCPG 为代表的一系列工作将静态识别重现漏洞的问题重新定义为子图同构问题。更具体地说，这些方法将已知的脆弱代码表示为基于图的结构(例如，PDG 或 CPG) ，然后在目标应用程序中识别与脆弱图匹配的子图。但是，由于这些方法对代码图中的更改高度敏感，它们可能会遗漏大量重复出现的漏洞，而且与已知的漏洞的代码略有不同。在本文中，我们提出了一种新的方法，即 RecurScan，它可以准确地检测出重复出现的漏洞，并具有对代码差异的恢复能力。为了实现这一目标，RecurScan 围绕安全补丁和符号跟踪技术开展工作，通过比较目标应用程序和已知漏洞之间的符号表达式和选择性约束，检测重复出现的漏洞。受益于这种设计，RecurScan 可以容忍由应用程序中的复杂数据或控制流引起的代码差异。我们使用184个已知的漏洞补丁对200个流行的 PHP web 应用程序进行了 RecurScan 评估。结果表明，RecurScan 发现了232个以前未知的漏洞，其中174个被分配了 CVE 标识符，在精度上比最先进的方法(即 HiddenCPG)高出25.98% ，在召回方面高出87.09% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RecurScan:+Detecting+Recurring+Vulnerabilities+in+PHP+Web+Applications)|0|
|[Phishing Vs. Legit: Comparative Analysis of Client-Side Resources of Phishing and Target Brand Websites](https://doi.org/10.1145/3589334.3645535)|Kyungchan Lim, Jaehwan Park, Doowon Kim||Phishing attacks have persistently remained a prevalent and widespread cybersecurity threat for several years. This leads to numerous endeavors aimed at comprehensively understanding the phishing attack ecosystem, with a specific focus on presenting new attack tactics and defense mechanisms against phishing attacks. Unfortunately, little is known about how client-side resources (e.g., JavaScript libraries) are used in phishing websites, compared to those in their corresponding legitimate target brand websites. This understanding can help us gain insights into the construction and techniques of phishing websites and phishing attackers' behaviors when building phishing websites. In this paper, we gain a deeper understanding of how client-side resources (especially, JavaScript libraries) are used in phishing websites by comparing them with the resources used in the legitimate target websites. For our study, we collect both client-side resources from phishing websites and their corresponding legitimate target brand websites for 25 months: 3.4M phishing websites (1.1M distinct phishing domains). Our study reveals that phishing websites tend to employ more diverse JavaScript libraries than their legitimate websites do. However, these libraries in phishing websites are older (nearly 21.2 months) and distinct in comparison. For example, Socket.IO is uniquely used in phishing websites to send victims' information to an external server in real time. Furthermore, we find that a considerable portion of them still maintain a basic and simplistic structure (e.g., simply displaying a login form or image), while phishing websites have significantly evolved to bypass anti-phishing measures. Finally, through HTML structure and style similarities, we can identify specific target webpages of legitimate brands that phishing attackers reference and use to mimic for their phishing attacks.|多年来，网络钓鱼攻击一直是普遍和广泛的网络安全威胁。这导致了许多旨在全面理解网络钓鱼攻击生态系统的努力，特别侧重于提出针对网络钓鱼攻击的新的攻击策略和防御机制。不幸的是，与那些相应的合法目标品牌网站相比，很少有人知道客户端资源(例如 JavaScript 库)是如何在钓鱼网站中使用的。这种认识有助于我们深入了解网络钓鱼网站的构建和技术，以及网络钓鱼攻击者在构建网络钓鱼网站时的行为。本文通过比较客户端资源(特别是 JavaScript 库)与合法目标网站的资源，对钓鱼网站中客户端资源的使用情况有了更深入的了解。在我们的研究中，我们从钓鱼网站及其相应的合法目标品牌网站收集了25个月的客户端资源: 340万个钓鱼网站(110万个不同的钓鱼域名)。我们的研究表明，钓鱼网站往往采用更多样化的 JavaScript 库比他们的合法网站。然而，这些钓鱼网站的图书馆年代更久远(将近21.2个月) ，相比之下也有所不同。例如，Socket.IO 在钓鱼网站中被独特地使用，可以将受害者的信息实时发送到外部服务器。此外，我们发现其中相当一部分网站仍然保持基本和简单的结构(例如，只显示一个登录表单或图像) ，而钓鱼网站已经大大发展，绕过反钓鱼措施。最后，通过 HTML 结构和风格的相似性，我们可以识别出具体的目标网页的合法品牌，仿冒网络钓鱼攻击者参考和使用模仿他们的网络钓鱼攻击。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Phishing+Vs.+Legit:+Comparative+Analysis+of+Client-Side+Resources+of+Phishing+and+Target+Brand+Websites)|0|
|[Detecting and Understanding Self-Deleting JavaScript Code](https://doi.org/10.1145/3589334.3645540)|Xinzhe Wang, Zeyang Zhuang, Wei Meng, James Cheng||Self-deletion is a well-known strategy frequently utilized by malware to evade detection. Recently, this technique has found its way into client-side JavaScript code, significantly raising the complexity of JavaScript analysis. In this work, we systematically study the emerging client-side JavaScript self-deletion behavior on the web. We tackle various technical challenges associated with JavaScript dynamic analysis and introduce JSRay, a browser-based JavaScript runtime monitoring system designed to comprehensively study client-side script deletion. We conduct a large-scale measurement of one million popular websites, revealing that script self-deletion is prevalent in the real world. While our findings indicate that most developers employ self-deletion for legitimate purposes, we also discover that self-deletion has already been employed together with other anti-analysis techniques for cloaking suspicious operations in client-side JavaScript.|自我删除是一种众所周知的策略，经常被恶意软件用来逃避检测。最近，这种技术已经进入了客户端 JavaScript 代码，大大提高了 JavaScript 分析的复杂性。在这项工作中，我们系统地研究了新兴的客户端 JavaScript 自删除行为在网络上。我们解决了与 JavaScript 动态分析相关的各种技术挑战，并介绍了 JSRay，一个基于浏览器的 JavaScript 运行时监控系统，旨在全面研究客户端脚本删除。我们对一百万个热门网站进行了大规模测试，发现在现实世界中删除脚本的现象非常普遍。虽然我们的研究结果表明，大多数开发人员使用自我删除的合法目的，我们也发现，自我删除已经与其他反分析技术一起使用，以掩盖客户端 JavaScript 中的可疑操作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+and+Understanding+Self-Deleting+JavaScript+Code)|0|
|[Malicious Package Detection using Metadata Information](https://doi.org/10.1145/3589334.3645543)|Sajal Halder, Michael Bewong, Arash Mahboubi, Yinhao Jiang, Md Rafiqul Islam, Md Zahidul Islam, Ryan HL Ip, Muhammad Ejaz Ahmed, Gowri Sankar Ramachandran, Muhammad Ali Babar||Protecting software supply chains from malicious packages is paramount in the evolving landscape of software development. Attacks on the software supply chain involve attackers injecting harmful software into commonly used packages or libraries in a software repository. For instance, JavaScript uses Node Package Manager (NPM), and Python uses Python Package Index (PyPi) as their respective package repositories. In the past, NPM has had vulnerabilities such as the event-stream incident, where a malicious package was introduced into a popular NPM package, potentially impacting a wide range of projects. As the integration of third-party packages becomes increasingly ubiquitous in modern software development, accelerating the creation and deployment of applications, the need for a robust detection mechanism has become critical. On the other hand, due to the sheer volume of new packages being released daily, the task of identifying malicious packages presents a significant challenge. To address this issue, in this paper, we introduce a metadata-based malicious package detection model, MeMPtec. This model extracts a set of features from package metadata information. These extracted features are classified as either easy-to-manipulate (ETM) or difficult-to-manipulate (DTM) features based on monotonicity and restricted control properties. By utilising these metadata features, not only do we improve the effectiveness of detecting malicious packages, but also we demonstrate its resistance to adversarial attacks in comparison with existing state-of-the-art. Our experiments indicate a significant reduction in both false positives (up to 97.56 negatives (up to 91.86|保护软件供应链免受恶意软件包的侵害在软件开发的不断发展的环境中是至关重要的。对软件供应链的攻击包括攻击者将有害软件注入常用软件包或软件资源库库中。例如，JavaScript 使用 Node Package Manager (NPM) ，Python 使用 Python Package Index (PyPi)作为各自的包存储库。在过去，NPM 存在一些漏洞，比如事件流事件，其中一个恶意软件包被引入到流行的 NPM 软件包中，这可能会影响到很多项目。随着第三方软件包的集成在现代软件开发中变得越来越普遍，加速了应用程序的创建和部署，对健壮的检测机制的需求已经变得至关重要。另一方面，由于每天都有大量的新软件包发布，识别恶意软件包的任务带来了巨大的挑战。为了解决这个问题，本文介绍了一个基于元数据的恶意包检测模型 MeMPtec。该模型从包元数据信息中提取一组特征。这些提取的特征根据单调性和受限制的控制特性分为易操作(ETM)特征和难操作(DTM)特征。通过利用这些元数据特性，我们不仅提高了检测恶意软件包的有效性，而且与现有的最先进技术相比，我们展示了它对敌对攻击的抵抗力。我们的实验表明，这两个假阳性(高达97.56负(高达91.86)显着降低|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Malicious+Package+Detection+using+Metadata+Information)|0|
|[Unveiling the Invisible: Detection and Evaluation of Prototype Pollution Gadgets with Dynamic Taint Analysis](https://doi.org/10.1145/3589334.3645579)|Mikhail Shcherbakov, Paul Moosbrugger, Musard Balliu||For better or worse, JavaScript is the cornerstone of modern Web. Prototype-based languages like JavaScript are susceptible to prototype pollution vulnerabilities, enabling an attacker to inject arbitrary properties into an object's prototype. The attacker can subsequently capitalize on the injected properties by executing otherwise benign pieces of code, so-called gadgets, that perform security-sensitive operations. The success of an attack largely depends on the presence of gadgets, leading to high-profile exploits such as privilege escalation and arbitrary code execution (ACE). This paper proposes Dasty, the first semi-automated pipeline to help developers identify gadgets in their applications' software supply chain. Dasty targets server-side Node.js applications and relies on an enhancement of dynamic taint analysis which we implement with the dynamic AST-level instrumentation. Moreover, Dasty provides support for visualization of code flows with an IDE, thus facilitating the subsequent manual analysis for building proof-of-concept exploits. To illustrate the danger of gadgets, we use Dasty in a study of the most dependent-upon NPM packages to analyze the presence of gadgets leading to ACE. Dasty identifies 1,269 server-side packages, of which 631 have code flows that may reach dangerous sinks. We manually prioritize and verify the candidate flows to build proof-of-concept exploits for 49 NPM packages, including popular packages such as ejs, nodemailer and workerpool. To investigate how Dasty integrates with existing tools to find end-to-end exploits, we conduct an in-depth analysis of a popular data visualization dashboard to find one high-severity CVE-2023-31415 leading to remote code execution. For the first time, our results systematically demonstrate the dangers of server-side gadgets and call for further research to solve the problem.|无论好坏，JavaScript 都是现代 Web 的基石。像 JavaScript 这样的基于原型的语言容易受到原型污染漏洞的影响，这使得攻击者能够向对象的原型中注入任意属性。随后，攻击者可以通过执行其他良性代码片段(即所谓的 gadget)来利用注入的属性，这些代码片段执行安全敏感的操作。攻击的成功很大程度上取决于小工具的存在，导致高调的攻击，如权限提升和任意代码执行(ACE)。本文提出了 Dasty，这是第一个半自动化的流水线，可以帮助开发人员在他们的应用软件供应链中识别小工具。Dasty 的目标是服务器端 Node.js 应用程序，它依赖于动态污染分析的增强，我们使用动态 AST 级别的工具来实现这一点。此外，Dasty 还提供了 IDE 对代码流可视化的支持，从而为构建概念验证利用的后续手工分析提供了便利。为了说明小工具的危险性，我们在一个研究中使用 Dasty 来分析导致 ACE 的小工具的存在。Dasty 确定了1,269个服务器端包，其中631个包含可能到达危险接收器的代码流。我们手动对候选流进行优先级排序和验证，以便为49个 NPM 包(包括 ejs、 nodemailer 和 workerpool 等流行包)构建概念验证利用。为了研究达斯蒂如何与现有工具集成以发现端到端的漏洞，我们对一个流行的数据可视化仪表板进行了深入分析，以发现一个高严重性的 CVE-2023-31415导致远程代码执行。我们的研究结果第一次系统地证明了服务器端小工具的危险性，并呼吁进一步研究来解决这个问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+the+Invisible:+Detection+and+Evaluation+of+Prototype+Pollution+Gadgets+with+Dynamic+Taint+Analysis)|0|
|[ARTEMIS: Detecting Airdrop Hunters in NFT Markets with a Graph Learning System](https://doi.org/10.1145/3589334.3645597)|Chenyu Zhou, Hongzhou Chen, Hao Wu, Junyu Zhang, Wei Cai||As Web3 projects leverage airdrops to incentivize participation, airdrop hunters tactically amass wallet addresses to capitalize on token giveaways. This poses challenges to the decentralization goal. Current detection approaches tailored for cryptocurrencies overlook non-fungible tokens (NFTs) nuances. We introduce ARTEMIS, an optimized graph neural network system for identifying airdrop hunters in NFT transactions. ARTEMIS captures NFT airdrop hunters through: (1) a multimodal module extracting visual and textual insights from NFT metadata using Transformer models; (2) a tailored node aggregation function chaining NFT transaction sequences, retaining behavioral insights; (3) engineered features based on market manipulation theories detecting anomalous trading. Evaluated on decentralized exchange Blur's data, ARTEMIS significantly outperforms baselines in pinpointing hunters. This pioneering computational solution for an emergent Web3 phenomenon has broad applicability for blockchain anomaly detection. The data and code for the paper are accessible at the following link: \hrefhttps://doi.org/10.5281/zenodo.10676801 doi.org/10.5281/zenodo.10676801.|随着 Web3项目利用空投来鼓励参与，空投猎人战术性地积累钱包地址来利用象征性赠品。这对地方分权目标构成了挑战。当前为加密货币量身定制的检测方法忽略了不可替换令牌(NFT)的细微差别。我们介绍了 ARTEMIS，一个优化的图形神经网络系统，用于在 NFT 交易中识别空投猎人。ARTEMIS 捕获 NFT 空投猎人通过: (1)一个多模式模块提取视觉和文本洞察 NFT 元数据使用变压器模型; (2)一个定制的节点聚合功能链接 NFT 交易序列，保留行为洞察; (3)工程特征基于市场操纵理论检测异常交易。根据分散交换模糊数据评估，ARTEMIS 在确定猎人方面明显优于基线。这个针对 Web3现象的开创性计算解决方案对区块链异常检测有着广泛的适用性。该文件的数据和代码可在以下链接获得: hrefhttps:// doi.org/10.5281/zenodo.10676801 doi.org/10.5281/zenodo.10676801。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ARTEMIS:+Detecting+Airdrop+Hunters+in+NFT+Markets+with+a+Graph+Learning+System)|0|
|[HSDirSniper: A New Attack Exploiting Vulnerabilities in Tor's Hidden Service Directories](https://doi.org/10.1145/3589334.3645591)|Qingfeng Zhang, Zhiyang Teng, Xuebin Wang, Yue Gao, Qingyun Liu, Jinqiao Shi||Tor hidden services (HSs) are used to provide anonymous services to users on the Internet without revealing the location of the servers. However, existing approaches have proven ineffective in mitigating the misuse of hidden services. Our investigation reveals that the latest iteration of Tor hidden services still exhibits vulnerabilities related to Hidden Service Directories (HSDirs). Building upon this identified weakness, we introduce the HSDirSniper attack, which leverages a substantial volume of descriptors to inundate the HSDir's descriptor cache. This results in the HSDir purging all stored descriptors, thereby blocking arbitrary hidden services. Notably, our attack represents the most practical means of blocking hidden services within the current high-adversarial context. The advantage of the HSDirSniper attack lies in its covert nature, as the targeted hidden service remains unaware of the attack. Additionally, the successful execution of this attack does not require the introduction of a colluding routing node within the Tor Network. We conducted comprehensive experiments in the real-world Tor Network, and the experimental results show that an attacker equipped with a certain quantity of hidden servers can render arbitrary hidden services inaccessible up to 90% of the time. To ascertain the potential scope of damage that the HSDirSniper attack can inflict upon hidden services, we provide a formal analytical framework for quantifying the cost of the HSDirSniper attack. Finally, we discuss the ethical concerns and countermeasures.|Tor 隐藏服务(HSs)用于向 Internet 上的用户提供匿名服务，而不暴露服务器的位置。然而，事实证明，现有的方法在减少对隐藏服务的滥用方面是无效的。我们的调查显示，Tor 隐藏服务的最新迭代仍然存在与隐藏服务目录(HSDir)相关的漏洞。基于这个已识别的弱点，我们引入了 HSDirSniper 攻击，它利用大量描述符淹没 HSDir 的描述符缓存。这导致 HSDir 清除所有存储的描述符，从而阻塞任意的隐藏服务。值得注意的是，我们的攻击代表了在当前高度敌对的环境下阻止隐藏服务的最实用的手段。HSDirSniper 攻击的优势在于它的隐蔽性，因为目标隐藏服务仍然不知道攻击。此外，这种攻击的成功执行并不需要在 Tor 网络中引入一个串通的路由节点。我们在现实的 Tor 网络中进行了全面的实验，实验结果表明，一个装备了一定数量隐藏服务器的攻击者可以使任意隐藏服务在90% 的时间内无法访问。为了确定 HSDirSniper 攻击对隐藏服务的潜在破坏范围，我们提供了一个正式的分析框架来量化 HSDirSniper 攻击的成本。最后，探讨了当前的伦理问题及对策。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HSDirSniper:+A+New+Attack+Exploiting+Vulnerabilities+in+Tor's+Hidden+Service+Directories)|0|
|[The Matter of Captchas: An Analysis of a Brittle Security Feature on the Modern Web](https://doi.org/10.1145/3589334.3645619)|Behzad Ousat, Esteban Schafir, Duc C. Hoang, Mohammad Ali Tofighi, Cuong V. Nguyen, Sajjad Arshad, A. Selcuk Uluagac, Amin Kharraz||The web ecosystem is a fast-paced environment. In this dynamic landscape, new security features are offered one after another to enhance the security and robustness of web applications and the operations they handle. This paper focuses on a fragile but still in-use security feature, text-based CAPTCHAs, that had been wildly used by web applications in the past to protect against automated attacks such as credential stuffing and account hijacking. The paper first investigates what it takes to develop automated scanners that can solve previously unseen text-based CAPTCHAs. We evaluated the possibility of developing and integrating a pre-trained CAPTCHA solver in the automated web scanning process without using a significantly large training dataset. We also perform an analysis of the impact of such autonomous scanners on CAPTCHA-enabled websites. Our analysis shows that solvable text-based CAPTCHAs on login, contact, and comment pages of websites are not uncommon. In particular, we identified over 3,100 text-based CAPTCHA websites in critical sectors such as finance, government, and health with hundreds of thousands of users. We showed that a web scanner with a pre-trained solver could solve more than 20% of previously unseen CAPTCHAs in just one single attempt. This result is worrisome considering the substantial potential to autonomously run the operation across thousands of websites on a daily basis with minimal training. The findings suggest that the integration of autonomous scanning with pre-training and local optimization of models can significantly increase adversaries' asymmetric power to launch their attacks cheaper and faster.|网络生态系统是一个快节奏的环境。在这个动态的环境中，新的保安功能一个接一个地提供，以加强网上应用程式及其处理的操作的安全性和稳健性。本文主要关注一个脆弱但仍在使用的安全特性，基于文本的 CAPTCHA，这个特性在过去被 web 应用广泛用于防止诸如证书填充和帐户劫持等自动攻击。本文首先研究了如何开发能够解决以前未见的基于文本的 CAPTCHA 的自动扫描仪。我们评估了开发和集成一个预先训练的 CAPTCHA 解决方案在自动化网页扫描过程中的可能性，而不使用大量的训练数据集。我们还对这种自动扫描器对支持验证码的网站的影响进行了分析。我们的分析表明，可解决的文本为基础的 CAPTCHA 的登录，联系人和评论网站的网页并不少见。特别是，我们确定了3100多个基于文本的 CAPTCHA 网站，涉及金融、政府和卫生等关键领域，拥有数十万用户。我们展示了一个网页扫描器与一个预先训练的解决方案可以解决超过20% 以前未见的验证码在一个单一的尝试。这个结果是令人担忧的，考虑到巨大的潜力，自主运行的操作，每天在数以千计的网站，最低限度的培训。研究结果表明，自主扫描与预训练和模型局部优化相结合可以显著提高对手发动攻击的不对称力量，使其发动攻击的成本更低、速度更快。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Matter+of+Captchas:+An+Analysis+of+a+Brittle+Security+Feature+on+the+Modern+Web)|0|
|[Characterizing Ethereum Upgradable Smart Contracts and Their Security Implications](https://doi.org/10.1145/3589334.3645640)|Xiaofan Li, Jin Yang, Jiaqi Chen, Yuzhe Tang, Xing Gao||Upgradeable smart contracts (USCs) have been widely adopted to enable modifying deployed smart contracts. While USCs bring great flexibility to developers, improper usage might introduce new security issues, potentially allowing attackers to hijack USCs and their users. In this paper, we conduct a large-scale measurement study to characterize USCs and their security implications in the wild. We summarize six commonly used USC patterns and develop a tool, USCDetector, to identify USCs without needing source code. Particularly, USCDetector collects various information such as bytecode and transaction information to construct upgrade chains for USCs and disclose potentially vulnerable ones. We evaluate USCDetector using verified smart contracts (i.e., with source code) as ground truth and show that USCDetector can achieve high accuracy with a precision of 96.26 to conduct a large-scale study on Ethereum, covering a total of 60,251,064 smart contracts. USCDetecor constructs 10,218 upgrade chains and discloses multiple real-world USCs with potential security issues.|可升级智能契约(USC)已被广泛采用，以便修改已部署的智能契约。虽然 USC 为开发人员带来了极大的灵活性，但是不当的使用可能会引入新的安全问题，潜在地允许攻击者劫持 USC 及其用户。在本文中，我们进行了一个大规模的测量研究，以表征 USC 及其在野外的安全意义。我们总结了六种常用的 USC 模式，并开发了一个工具 USCDetector，它可以在不需要源代码的情况下识别 USC。特别是，USCDetector 收集各种信息，如字节码和事务信息，为 USC 构建升级链，并披露潜在的脆弱信息。我们使用经过验证的智能合同(即源代码)来评估 USC 探测器，结果表明 USC 探测器可以达到96.26的高精度，对以太坊进行大规模的研究，总共涉及60,251,064个智能合同。USCDetecor 构建了10,218个升级链，并公开了存在潜在安全问题的多个现实世界的 USC。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Characterizing+Ethereum+Upgradable+Smart+Contracts+and+Their+Security+Implications)|0|
|[IDEA-DAC: Integrity-Driven Editing for Accountable Decentralized Anonymous Credentials via ZK-JSON](https://doi.org/10.1145/3589334.3645658)|Shuhao Zheng, Zonglun Li, Junliang Luo, Ziyue Xin, Xue Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IDEA-DAC:+Integrity-Driven+Editing+for+Accountable+Decentralized+Anonymous+Credentials+via+ZK-JSON)|0|
|[Is It Safe to Share Your Files? An Empirical Security Analysis of Google Workspace](https://doi.org/10.1145/3589334.3645697)|Liuhuo Wan, Kailong Wang, Haoyu Wang, Guangdong Bai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+It+Safe+to+Share+Your+Files?+An+Empirical+Security+Analysis+of+Google+Workspace)|0|
|[PanoptiChrome: A Modern In-browser Taint Analysis Framework](https://doi.org/10.1145/3589334.3645699)|Rahul Kanyal, Smruti R. Sarangi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PanoptiChrome:+A+Modern+In-browser+Taint+Analysis+Framework)|0|
|[PhishinWebView: Analysis of Anti-Phishing Entities in Mobile Apps with WebView Targeted Phishing](https://doi.org/10.1145/3589334.3645708)|Yoonjung Choi, Woonghee Lee, Junbeom Hur||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PhishinWebView:+Analysis+of+Anti-Phishing+Entities+in+Mobile+Apps+with+WebView+Targeted+Phishing)|0|
|[Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models](https://doi.org/10.1145/3589334.3645376)|Chenhan Yuan, Qianqian Xie, Jimin Huang, Sophia Ananiadou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Back+to+the+Future:+Towards+Explainable+Temporal+Reasoning+with+Large+Language+Models)|0|
|[A Knowledge-Injected Curriculum Pretraining Framework for Question Answering](https://doi.org/10.1145/3589334.3645406)|Xin Lin, Tianhuang Su, Zhenya Huang, Shangzi Xue, Haifeng Liu, Enhong Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Knowledge-Injected+Curriculum+Pretraining+Framework+for+Question+Answering)|0|
|[Using Model Calibration to Evaluate Link Prediction in Knowledge Graphs](https://doi.org/10.1145/3589334.3645506)|Aishwarya Rao, Narayanan Asuri Krishnan, Carlos R. Rivero||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Using+Model+Calibration+to+Evaluate+Link+Prediction+in+Knowledge+Graphs)|0|
|[Zero-shot Image Classification with Logic Adapter and Rule Prompt](https://doi.org/10.1145/3589334.3645554)|Dongran Yu, Xueyan Liu, Bo Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Zero-shot+Image+Classification+with+Logic+Adapter+and+Rule+Prompt)|0|
|[NPCS: Native Provenance Computation for SPARQL](https://doi.org/10.1145/3589334.3645557)|Zubaria Asma, Daniel Hernández, Luis Galárraga, Giorgos Flouris, Irini Fundulaki, Katja Hose||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NPCS:+Native+Provenance+Computation+for+SPARQL)|0|
|[Taxonomy Completion via Implicit Concept Insertion](https://doi.org/10.1145/3589334.3645584)|Jingchuan Shi, Hang Dong, Jiaoyan Chen, Zhe Wu, Ian Horrocks||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Taxonomy+Completion+via+Implicit+Concept+Insertion)|0|
|[UniLP: Unified Topology-aware Generative Framework for Link Prediction in Knowledge Graph](https://doi.org/10.1145/3589334.3645592)|Ben Liu, Miao Peng, Wenjie Xu, Xu Jia, Min Peng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UniLP:+Unified+Topology-aware+Generative+Framework+for+Link+Prediction+in+Knowledge+Graph)|0|
|[A Symbolic Rule Integration Framework with Logic Transformer for Inductive Relation Prediction](https://doi.org/10.1145/3589334.3645594)|Yudai Pan, Jun Liu, Tianzhe Zhao, Lingling Zhang, Yun Lin, Jin Song Dong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Symbolic+Rule+Integration+Framework+with+Logic+Transformer+for+Inductive+Relation+Prediction)|0|
|[Causal Question Answering with Reinforcement Learning](https://doi.org/10.1145/3589334.3645610)|Lukas Blübaum, Stefan Heindorf||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Question+Answering+with+Reinforcement+Learning)|0|
|[DRAM-like Architecture with Asynchronous Refreshing for Continual Relation Extraction](https://doi.org/10.1145/3589334.3645621)|Tianci Bu, Kang Yang, Wenchuan Yang, Jiawei Feng, Xiaoyu Zhang, Xin Lu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DRAM-like+Architecture+with+Asynchronous+Refreshing+for+Continual+Relation+Extraction)|0|
|[KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large Language Models](https://doi.org/10.1145/3589334.3645623)|Yuyang Bai, Shangbin Feng, Vidhisha Balachandran, Zhaoxuan Tan, Shiqi Lou, Tianxing He, Yulia Tsvetkov||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KGQuiz:+Evaluating+the+Generalization+of+Encoded+Knowledge+in+Large+Language+Models)|0|
|[Robust Link Prediction over Noisy Hyper-Relational Knowledge Graphs via Active Learning](https://doi.org/10.1145/3589334.3645686)|Weijian Yu, Jie Yang, Dingqi Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Link+Prediction+over+Noisy+Hyper-Relational+Knowledge+Graphs+via+Active+Learning)|0|
|[OODREB: Benchmarking State-of-the-Art Methods for Out-Of-Distribution Generalization on Relation Extraction](https://doi.org/10.1145/3589334.3645695)|Haotian Chen, Houjing Guo, Bingsheng Chen, Xiangdong Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OODREB:+Benchmarking+State-of-the-Art+Methods+for+Out-Of-Distribution+Generalization+on+Relation+Extraction)|0|
|[Toward Practical Entity Alignment Method Design: Insights from New Highly Heterogeneous Knowledge Graph Datasets](https://doi.org/10.1145/3589334.3645720)|Xuhui Jiang, Chengjin Xu, Yinghan Shen, Yuanzhuo Wang, Fenglong Su, Zhichao Shi, Fei Sun, Zixuan Li, Jian Guo, Huawei Shen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Toward+Practical+Entity+Alignment+Method+Design:+Insights+from+New+Highly+Heterogeneous+Knowledge+Graph+Datasets)|0|
|[Social Media Discourses on Interracial Intimacy: Tracking Racism and Sexism through Chinese Geo-located Social Media Data](https://doi.org/10.1145/3589334.3645334)|Zheng Wei, Yixuan Xie, Danyun Xiao, Simin Zhang, Pan Hui, Muzhi Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Social+Media+Discourses+on+Interracial+Intimacy:+Tracking+Racism+and+Sexism+through+Chinese+Geo-located+Social+Media+Data)|0|
|[Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models](https://doi.org/10.1145/3589334.3645381)|Hongzhan Lin, Ziyang Luo, Wei Gao, Jing Ma, Bo Wang, Ruichao Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Explainable+Harmful+Meme+Detection+through+Multimodal+Debate+between+Large+Language+Models)|0|
|[What News Do People Get on Social Media? Analyzing Exposure and Consumption of News through Data Donations](https://doi.org/10.1145/3589334.3645399)|Salim Chouaki, Abhijnan Chakraborty, Oana Goga, Savvas Zannettou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What+News+Do+People+Get+on+Social+Media?+Analyzing+Exposure+and+Consumption+of+News+through+Data+Donations)|0|
|[Euphemism Identification via Feature Fusion and Individualization](https://doi.org/10.1145/3589334.3645433)|Yuxue Hu, Mingmin Wu, Zhongqiang Huang, Junsong Li, Xing Ge, Ying Sha||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Euphemism+Identification+via+Feature+Fusion+and+Individualization)|0|
|[Team Formation amidst Conflicts](https://doi.org/10.1145/3589334.3645444)|Iasonas Nikolaou, Evimaria Terzi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Team+Formation+amidst+Conflicts)|0|
|[T3RD: Test-Time Training for Rumor Detection on Social Media](https://doi.org/10.1145/3589334.3645443)|Huaiwen Zhang, Xinxin Liu, Qing Yang, Yang Yang, Fan Qi, Shengsheng Qian, Changsheng Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=T3RD:+Test-Time+Training+for+Rumor+Detection+on+Social+Media)|0|
|[ESCNet: Entity-enhanced and Stance Checking Network for Multi-modal Fact-Checking](https://doi.org/10.1145/3589334.3645455)|Fanrui Zhang, Jiawei Liu, Jingyi Xie, Qiang Zhang, Yongchao Xu, ZhengJun Zha||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ESCNet:+Entity-enhanced+and+Stance+Checking+Network+for+Multi-modal+Fact-Checking)|0|
|[Labor Space: A Unifying Representation of the Labor Market via Large Language Models](https://doi.org/10.1145/3589334.3645464)|Seongwoon Kim, YongYeol Ahn, Jaehyuk Park||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Labor+Space:+A+Unifying+Representation+of+the+Labor+Market+via+Large+Language+Models)|0|
|[Explainable Fake News Detection with Large Language Model via Defense Among Competing Wisdom](https://doi.org/10.1145/3589334.3645471)|Bo Wang, Jing Ma, Hongzhan Lin, Zhiwei Yang, Ruichao Yang, Yuan Tian, Yi Chang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+Fake+News+Detection+with+Large+Language+Model+via+Defense+Among+Competing+Wisdom)|0|
|[Navigating the Post-API Dilemma](https://doi.org/10.1145/3589334.3645503)|Amrit Poudel, Tim Weninger||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Navigating+the+Post-API+Dilemma)|0|
|[Unraveling the Dynamics of Stable and Curious Audiences in Web Systems](https://doi.org/10.1145/3589334.3645473)|Rodrigo Alves, Antoine Ledent, Renato Assunção, Pedro O. S. Vaz de Melo, Marius Kloft||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unraveling+the+Dynamics+of+Stable+and+Curious+Audiences+in+Web+Systems)|0|
|[SymLearn: A Symbiotic Crowd-AI Collective Learning Framework to Web-based Healthcare Policy Adherence Assessment](https://doi.org/10.1145/3589334.3645519)|Yang Zhang, Ruohan Zong, Lanyu Shang, Huimin Zeng, Zhenrui Yue, Dong Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SymLearn:+A+Symbiotic+Crowd-AI+Collective+Learning+Framework+to+Web-based+Healthcare+Policy+Adherence+Assessment)|0|
|[An Efficient Automatic Meta-Path Selection for Social Event Detection via Hyperbolic Space](https://doi.org/10.1145/3589334.3645526)|Zitai Qiu, Congbo Ma, Jia Wu, Jian Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Efficient+Automatic+Meta-Path+Selection+for+Social+Event+Detection+via+Hyperbolic+Space)|0|
|[NETEVOLVE: Social Network Forecasting using Multi-Agent Reinforcement Learning with Interpretable Features](https://doi.org/10.1145/3589334.3647982)|Kentaro Miyake, Hiroyoshi Ito, Christos Faloutsos, Hirotomo Matsumoto, Atsuyuki Morishima||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NETEVOLVE:+Social+Network+Forecasting+using+Multi-Agent+Reinforcement+Learning+with+Interpretable+Features)|0|
|[Analysis and Detection of "Pink Slime" Websites in Social Media Posts](https://doi.org/10.1145/3589334.3645588)|Abdullah Aljebreen, Weiyi Meng, Eduard C. Dragut||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analysis+and+Detection+of+"Pink+Slime"+Websites+in+Social+Media+Posts)|0|
|[Navigating Multidimensional Ideologies with Reddit's Political Compass: Economic Conflict and Social Affinity](https://doi.org/10.1145/3589334.3645606)|Ernesto Colacrai, Federico Cinus, Gianmarco De Francisci Morales, Michele Starnini||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Navigating+Multidimensional+Ideologies+with+Reddit's+Political+Compass:+Economic+Conflict+and+Social+Affinity)|0|
|[Unifying Local and Global Knowledge: Empowering Large Language Models as Political Experts with Knowledge Graphs](https://doi.org/10.1145/3589334.3645616)|Xinyi Mou, Zejun Li, Hanjia Lyu, Jiebo Luo, Zhongyu Wei||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unifying+Local+and+Global+Knowledge:+Empowering+Large+Language+Models+as+Political+Experts+with+Knowledge+Graphs)|0|
|[Not All Asians are the Same: A Disaggregated Approach to Identifying Anti-Asian Racism in Social Media](https://doi.org/10.1145/3589334.3645630)|Fan Wu, Sanyam Lakhanpal, Qian Li, Kookjin Lee, Doowon Kim, Heewon Chae, Kyounghee Hazel Kwon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Not+All+Asians+are+the+Same:+A+Disaggregated+Approach+to+Identifying+Anti-Asian+Racism+in+Social+Media)|0|
|[Global News Synchrony and Diversity During the Start of the COVID-19 Pandemic](https://doi.org/10.1145/3589334.3645645)|Xi Chen, Scott A. Hale, David Jurgens, Mattia Samory, Ethan Zuckerman, Przemyslaw A. Grabowicz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Global+News+Synchrony+and+Diversity+During+the+Start+of+the+COVID-19+Pandemic)|0|
|[Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries](https://doi.org/10.1145/3589334.3645643)|Yiqiao Jin, Mohit Chandra, Gaurav Verma, Yibo Hu, Munmun De Choudhury, Srijan Kumar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Better+to+Ask+in+English:+Cross-Lingual+Evaluation+of+Large+Language+Models+for+Healthcare+Queries)|0|
|[Bridging or Breaking: Impact of Intergroup Interactions on Religious Polarization](https://doi.org/10.1145/3589334.3645675)|Rochana Chaturvedi, Sugat Chaturvedi, Elena Zheleva||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bridging+or+Breaking:+Impact+of+Intergroup+Interactions+on+Religious+Polarization)|0|
|[ARES: Predictable Traffic Engineering under Controller Failures in SD-WANs](https://doi.org/10.1145/3589334.3645321)|Songshi Dou, Li Qi, Zehua Guo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ARES:+Predictable+Traffic+Engineering+under+Controller+Failures+in+SD-WANs)|0|
|[QUIC is not Quick Enough over Fast Internet](https://doi.org/10.1145/3589334.3645323)|Xumiao Zhang, Shuowei Jin, Yi He, Ahmad Hassan, Z. Morley Mao, Feng Qian, ZhiLi Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QUIC+is+not+Quick+Enough+over+Fast+Internet)|0|
|[GEES: Enabling Location Privacy-Preserving Energy Saving in Multi-Access Edge Computing](https://doi.org/10.1145/3589334.3645329)|Ziqi Wang, Xiaoyu Xia, Minhui Xue, Ibrahim Khalil, Minghui Liwang, Xun Yi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GEES:+Enabling+Location+Privacy-Preserving+Energy+Saving+in+Multi-Access+Edge+Computing)|0|
|[DirectFaaS: A Clean-Slate Network Architecture for Efficient Serverless Chain Communications](https://doi.org/10.1145/3589334.3645333)|Qingyang Zeng, Kaiyu Hou, Xue Leng, Yan Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DirectFaaS:+A+Clean-Slate+Network+Architecture+for+Efficient+Serverless+Chain+Communications)|0|
|[Meet Challenges of RTT Jitter, A Hybrid Internet Congestion Control Algorithm](https://doi.org/10.1145/3589334.3645338)|Lianchen Jia, Chao Zhou, Tianchi Huang, Chaoyang Li, Lifeng Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Meet+Challenges+of+RTT+Jitter,+A+Hybrid+Internet+Congestion+Control+Algorithm)|0|
|[Towards Energy-efficient Federated Learning via INT8-based Training on Mobile DSPs](https://doi.org/10.1145/3589334.3645341)|Jinliang Yuan, Shangguang Wang, Hongyu Li, Daliang Xu, Yuanchun Li, Mengwei Xu, Xuanzhe Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Energy-efficient+Federated+Learning+via+INT8-based+Training+on+Mobile+DSPs)|0|
|[FreqMAE: Frequency-Aware Masked Autoencoder for Multi-Modal IoT Sensing](https://doi.org/10.1145/3589334.3645346)|Denizhan Kara, Tomoyoshi Kimura, Shengzhong Liu, Jinyang Li, Dongxin Liu, Tianshi Wang, Ruijie Wang, Yizhuo Chen, Yigong Hu, Tarek F. Abdelzaher||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FreqMAE:+Frequency-Aware+Masked+Autoencoder+for+Multi-Modal+IoT+Sensing)|0|
|[Air-CAD: Edge-Assisted Multi-Drone Network for Real-time Crowd Anomaly Detection](https://doi.org/10.1145/3589334.3645362)|Yuanzheng Tan, Qing Li, Junkun Peng, Zhenhui Yuan, Yong Jiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Air-CAD:+Edge-Assisted+Multi-Drone+Network+for+Real-time+Crowd+Anomaly+Detection)|0|
|[λGrapher: A Resource-Efficient Serverless System for GNN Serving through Graph Sharing](https://doi.org/10.1145/3589334.3645383)|Haichuan Hu, Fangming Liu, Qiangyu Pei, Yongjie Yuan, Zichen Xu, Lin Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=λGrapher:+A+Resource-Efficient+Serverless+System+for+GNN+Serving+through+Graph+Sharing)|0|
|[SPRING: Improving the Throughput of Sharding Blockchain via Deep Reinforcement Learning Based State Placement](https://doi.org/10.1145/3589334.3645386)|Pengze Li, Mingxuan Song, Mingzhe Xing, Zhen Xiao, Qiuyu Ding, Shengjie Guan, Jieyi Long||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SPRING:+Improving+the+Throughput+of+Sharding+Blockchain+via+Deep+Reinforcement+Learning+Based+State+Placement)|0|
|[Supervised Fine-Tuning for Unsupervised KPI Anomaly Detection for Mobile Web Systems](https://doi.org/10.1145/3589334.3645392)|Zhaoyang Yu, Shenglin Zhang, Mingze Sun, Yingke Li, Yankai Zhao, Xiaolei Hua, Lin Zhu, Xidao Wen, Dan Pei||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Supervised+Fine-Tuning+for+Unsupervised+KPI+Anomaly+Detection+for+Mobile+Web+Systems)|0|
|[NCTM: A Novel Coded Transmission Mechanism for Short Video Deliveries](https://doi.org/10.1145/3589334.3645387)|Zhenge Xu, Qing Li, Wanxin Shi, Yong Jiang, Zhenhui Yuan, Peng Zhang, GabrielMiro Muntean||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NCTM:+A+Novel+Coded+Transmission+Mechanism+for+Short+Video+Deliveries)|0|
|[InArt: In-Network Aggregation with Route Selection for Accelerating Distributed Training](https://doi.org/10.1145/3589334.3645394)|Jiawei Liu, Yutong Zhai, Gongming Zhao, Hongli Xu, Jin Fang, Zhen Zeng, Ying Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=InArt:+In-Network+Aggregation+with+Route+Selection+for+Accelerating+Distributed+Training)|0|
|[FusionRender: Harnessing WebGPU's Power for Enhanced Graphics Performance on Web Browsers](https://doi.org/10.1145/3589334.3645395)|Weichen Bi, Yun Ma, Yudong Han, Yifan Chen, Deyu Tian, Jiaqi Du||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FusionRender:+Harnessing+WebGPU's+Power+for+Enhanced+Graphics+Performance+on+Web+Browsers)|0|
|[FedDSE: Distribution-aware Sub-model Extraction for Federated Learning over Resource-constrained Devices](https://doi.org/10.1145/3589334.3645416)|Haozhao Wang, Yabo Jia, Meng Zhang, Qinghao Hu, Hao Ren, Peng Sun, Yonggang Wen, Tianwei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedDSE:+Distribution-aware+Sub-model+Extraction+for+Federated+Learning+over+Resource-constrained+Devices)|0|
|[BlockDFL: A Blockchain-based Fully Decentralized Peer-to-Peer Federated Learning Framework](https://doi.org/10.1145/3589334.3645425)|Zhen Qin, Xueqiang Yan, Mengchu Zhou, Shuiguang Deng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BlockDFL:+A+Blockchain-based+Fully+Decentralized+Peer-to-Peer+Federated+Learning+Framework)|0|
|[Incentive and Dynamic Client Selection for Federated Unlearning](https://doi.org/10.1145/3589334.3645462)|Yijing Lin, Zhipeng Gao, Hongyang Du, Dusit Niyato, Jiawen Kang, Xiaoyuan Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Incentive+and+Dynamic+Client+Selection+for+Federated+Unlearning)|0|
|[Accelerating the Decentralized Federated Learning via Manipulating Edges](https://doi.org/10.1145/3589334.3645509)|Mingyang Zhou, Gang Liu, Kezhong Lu, Rui Mao, Hao Liao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Accelerating+the+Decentralized+Federated+Learning+via+Manipulating+Edges)|0|
|[How Few Davids Improve One Goliath: Federated Learning in Resource-Skewed Edge Computing Environments](https://doi.org/10.1145/3589334.3645544)|Jiayun Zhang, Shuheng Li, Haiyu Huang, Zihan Wang, Xiaohan Fu, Dezhi Hong, Rajesh K. Gupta, Jingbo Shang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Few+Davids+Improve+One+Goliath:+Federated+Learning+in+Resource-Skewed+Edge+Computing+Environments)|0|
|[Privacy-Preserving and Fairness-Aware Federated Learning for Critical Infrastructure Protection and Resilience](https://doi.org/10.1145/3589334.3645545)|Yanjun Zhang, Ruoxi Sun, Liyue Shen, Guangdong Bai, Minhui Xue, Mark Huasong Meng, Xue Li, Ryan K. L. Ko, Surya Nepal||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privacy-Preserving+and+Fairness-Aware+Federated+Learning+for+Critical+Infrastructure+Protection+and+Resilience)|0|
|[Making Cloud Spot Instance Interruption Events Visible](https://doi.org/10.1145/3589334.3645548)|Kyunghwan Kim, Kyungyong Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Making+Cloud+Spot+Instance+Interruption+Events+Visible)|0|
|[E2Usd: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series](https://doi.org/10.1145/3589334.3645593)|Zhichen Lai, Huan Li, Dalin Zhang, Yan Zhao, Weizhu Qian, Christian S. Jensen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=E2Usd:+Efficient-yet-effective+Unsupervised+State+Detection+for+Multivariate+Time+Series)|0|
|[Robust Route Planning under Uncertain Pickup Requests for Last-mile Delivery](https://doi.org/10.1145/3589334.3645595)|Hua Yan, Heng Tan, Haotian Wang, Desheng Zhang, Yu Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Route+Planning+under+Uncertain+Pickup+Requests+for+Last-mile+Delivery)|0|
|[Unity is Strength? Benchmarking the Robustness of Fusion-based 3D Object Detection against Physical Sensor Attack](https://doi.org/10.1145/3589334.3645612)|Zizhi Jin, Xuancun Lu, Bo Yang, Yushi Cheng, Chen Yan, Xiaoyu Ji, Wenyuan Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unity+is+Strength?+Benchmarking+the+Robustness+of+Fusion-based+3D+Object+Detection+against+Physical+Sensor+Attack)|0|
|[WEFix: Intelligent Automatic Generation of Explicit Waits for Efficient Web End-to-End Flaky Tests](https://doi.org/10.1145/3589334.3645628)|Xinyue Liu, Zihe Song, Weike Fang, Wei Yang, Weihang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=WEFix:+Intelligent+Automatic+Generation+of+Explicit+Waits+for+Efficient+Web+End-to-End+Flaky+Tests)|0|
|[SatGuard: Concealing Endless and Bursty Packet Losses in LEO Satellite Networks for Delay-Sensitive Web Applications](https://doi.org/10.1145/3589334.3645639)|Jihao Li, Hewu Li, Zeqi Lai, Qian Wu, Yijie Liu, Qi Zhang, Yuanjie Li, Jun Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SatGuard:+Concealing+Endless+and+Bursty+Packet+Losses+in+LEO+Satellite+Networks+for+Delay-Sensitive+Web+Applications)|0|
|[More Than Routing: Joint GPS and Route Modeling for Refine Trajectory Representation Learning](https://doi.org/10.1145/3589334.3645644)|Zhipeng Ma, Zheyan Tu, Xinhai Chen, Yan Zhang, Deguo Xia, Guyue Zhou, Yilun Chen, Yu Zheng, Jiangtao Gong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=More+Than+Routing:+Joint+GPS+and+Route+Modeling+for+Refine+Trajectory+Representation+Learning)|0|
|[Cardinality Counting in "Alcatraz": A Privacy-aware Federated Learning Approach](https://doi.org/10.1145/3589334.3645655)|Nan Wu, Xin Yuan, Shuo Wang, Hongsheng Hu, Minhui Xue||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cardinality+Counting+in+"Alcatraz":+A+Privacy-aware+Federated+Learning+Approach)|0|
|[GAMMA: Graph Neural Network-Based Multi-Bottleneck Localization for Microservices Applications](https://doi.org/10.1145/3589334.3645665)|Gagan Somashekar, Anurag Dutt, Mainak Adak, Tania LoridoBotran, Anshul Gandhi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GAMMA:+Graph+Neural+Network-Based+Multi-Bottleneck+Localization+for+Microservices+Applications)|0|
|[Revisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective](https://doi.org/10.1145/3589334.3645710)|Zexin Wang, Changhua Pei, Minghua Ma, Xin Wang, Zhihan Li, Dan Pei, Saravan Rajmohan, Dongmei Zhang, Qingwei Lin, Haiming Zhang, Jianhui Li, Gaogang Xie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+VAE+for+Unsupervised+Time+Series+Anomaly+Detection:+A+Frequency+Perspective)|0|
|[Interpretable Knowledge Tracing with Multiscale State Representation](https://doi.org/10.1145/3589334.3645373)|Jianwen Sun, Fenghua Yu, Qian Wan, Qing Li, Sannyuya Liu, Xiaoxuan Shen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interpretable+Knowledge+Tracing+with+Multiscale+State+Representation)|0|
|[COLA: Cross-city Mobility Transformer for Human Trajectory Simulation](https://doi.org/10.1145/3589334.3645469)|Yu Wang, Tongya Zheng, Yuxuan Liang, Shunyu Liu, Mingli Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=COLA:+Cross-city+Mobility+Transformer+for+Human+Trajectory+Simulation)|0|
|[Off-Policy Evaluation for Large Action Spaces via Policy Convolution](https://doi.org/10.1145/3589334.3645501)|Noveen Sachdeva, Lequn Wang, Dawen Liang, Nathan Kallus, Julian J. McAuley||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Off-Policy+Evaluation+for+Large+Action+Spaces+via+Policy+Convolution)|0|
|[LFDe: A Lighter, Faster and More Data-Efficient Pre-training Framework for Event Extraction](https://doi.org/10.1145/3589334.3645318)|Zhigang Kan, Liwen Peng, Yifu Gao, Ning Liu, Linbo Qiao, Dongsheng Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LFDe:+A+Lighter,+Faster+and+More+Data-Efficient+Pre-training+Framework+for+Event+Extraction)|0|
|[Multi-Scenario Pricing for Hotel Revenue Management](https://doi.org/10.1145/3589334.3645350)|Wendong Xiao, Shuqi Zhang, Zhiyi Huang, Yao Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Scenario+Pricing+for+Hotel+Revenue+Management)|0|
|[Collaboration-Aware Hybrid Learning for Knowledge Development Prediction](https://doi.org/10.1145/3589334.3645326)|Liyi Chen, Chuan Qin, Ying Sun, Xin Song, Tong Xu, Hengshu Zhu, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaboration-Aware+Hybrid+Learning+for+Knowledge+Development+Prediction)|0|
|[Span-Pair Interaction and Tagging for Dialogue-Level Aspect-Based Sentiment Quadruple Analysis](https://doi.org/10.1145/3589334.3645355)|Changzhi Zhou, Zhijing Wu, Dandan Song, Linmei Hu, Yuhang Tian, Jing Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Span-Pair+Interaction+and+Tagging+for+Dialogue-Level+Aspect-Based+Sentiment+Quadruple+Analysis)|0|
|[UrbanCLIP: Learning Text-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining from the Web](https://doi.org/10.1145/3589334.3645378)|Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen, Roger Zimmermann, Yuxuan Liang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UrbanCLIP:+Learning+Text-enhanced+Urban+Region+Profiling+with+Contrastive+Language-Image+Pretraining+from+the+Web)|0|
|[MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection](https://doi.org/10.1145/3589334.3645385)|Yupeng Li, Haorui He, Jin Bai, Dacheng Wen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MCFEND:+A+Multi-source+Benchmark+Dataset+for+Chinese+Fake+News+Detection)|0|
|[LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty](https://doi.org/10.1145/3589334.3645414)|Zhen Zhang, Yuhua Zhao, Hang Gao, Mengting Hu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LinkNER:+Linking+Local+Named+Entity+Recognition+Models+to+Large+Language+Models+using+Uncertainty)|0|
|[RicciNet: Deep Clustering via A Riemannian Generative Model](https://doi.org/10.1145/3589334.3645428)|Li Sun, Jingbin Hu, Suyang Zhou, Zhenhao Huang, Junda Ye, Hao Peng, Zhengtao Yu, Philip S. Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RicciNet:+Deep+Clustering+via+A+Riemannian+Generative+Model)|0|
|[Weakly Supervised Anomaly Detection via Knowledge-Data Alignment](https://doi.org/10.1145/3589334.3645429)|Haihong Zhao, Chenyi Zi, Yang Liu, Chen Zhang, Yan Zhou, Jia Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weakly+Supervised+Anomaly+Detection+via+Knowledge-Data+Alignment)|0|
|[MULAN: Multi-modal Causal Structure Learning and Root Cause Analysis for Microservice Systems](https://doi.org/10.1145/3589334.3645442)|Lecheng Zheng, Zhengzhang Chen, Jingrui He, Haifeng Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MULAN:+Multi-modal+Causal+Structure+Learning+and+Root+Cause+Analysis+for+Microservice+Systems)|0|
|[Dynamic Multi-Network Mining of Tensor Time Series](https://doi.org/10.1145/3589334.3645461)|Kohei Obata, Koki Kawabata, Yasuko Matsubara, Yasushi Sakurai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Multi-Network+Mining+of+Tensor+Time+Series)|0|
|[MSynFD: Multi-hop Syntax Aware Fake News Detection](https://doi.org/10.1145/3589334.3645468)|Liang Xiao, Qi Zhang, Chongyang Shi, Shoujin Wang, Usman Naseem, Liang Hu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MSynFD:+Multi-hop+Syntax+Aware+Fake+News+Detection)|0|
|[LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Time Series Anomaly Detection](https://doi.org/10.1145/3589334.3645472)|Feiyi Chen, Zhen Qin, Mengchu Zhou, Yingying Zhang, Shuiguang Deng, Lunting Fan, Guansong Pang, Qingsong Wen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LARA:+A+Light+and+Anti-overfitting+Retraining+Approach+for+Unsupervised+Time+Series+Anomaly+Detection)|0|
|[Markovletics: Methods and A Novel Application for Learning Continuous-Time Markov Chain Mixtures](https://doi.org/10.1145/3589334.3645491)|Fabian Spaeh, Charalampos E. Tsourakakis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Markovletics:+Methods+and+A+Novel+Application+for+Learning+Continuous-Time+Markov+Chain+Mixtures)|0|
|[NAT4AT: Using Non-Autoregressive Translation Makes Autoregressive Translation Faster and Better](https://doi.org/10.1145/3589334.3645527)|Huanran Zheng, Wei Zhu, Xiaoling Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NAT4AT:+Using+Non-Autoregressive+Translation+Makes+Autoregressive+Translation+Faster+and+Better)|0|
|[Breaking the Time-Frequency Granularity Discrepancy in Time-Series Anomaly Detection](https://doi.org/10.1145/3589334.3645556)|Youngeun Nam, Susik Yoon, Yooju Shin, Minyoung Bae, Hwanjun Song, JaeGil Lee, Byung Suk Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Breaking+the+Time-Frequency+Granularity+Discrepancy+in+Time-Series+Anomaly+Detection)|0|
|[Question Difficulty Consistent Knowledge Tracing](https://doi.org/10.1145/3589334.3645582)|Guimei Liu, Huijing Zhan, Jungjae Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Question+Difficulty+Consistent+Knowledge+Tracing)|0|
|[A Simple but Effective Approach for Unsupervised Few-Shot Graph Classification](https://doi.org/10.1145/3589334.3645587)|Yonghao Liu, Lan Huang, Bowen Cao, Ximing Li, Fausto Giunchiglia, Xiaoyue Feng, Renchu Guan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Simple+but+Effective+Approach+for+Unsupervised+Few-Shot+Graph+Classification)|0|
|[Inductive Cognitive Diagnosis for Fast Student Learning in Web-Based Intelligent Education Systems](https://doi.org/10.1145/3589334.3645589)|Shuo Liu, Junhao Shen, Hong Qian, Aimin Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Inductive+Cognitive+Diagnosis+for+Fast+Student+Learning+in+Web-Based+Intelligent+Education+Systems)|0|
|[RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules](https://doi.org/10.1145/3589334.3645602)|Miaomiao Li, Jiaqi Zhu, Yang Wang, Yi Yang, Yilin Li, Hongan Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RulePrompt:+Weakly+Supervised+Text+Classification+with+Prompting+PLMs+and+Self-Iterative+Logical+Rules)|0|
|[Multimodal Relation Extraction via a Mixture of Hierarchical Visual Context Learners](https://doi.org/10.1145/3589334.3645603)|Xiyang Liu, Chunming Hu, Richong Zhang, Kai Sun, Samuel Mensah, Yongyi Mao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Relation+Extraction+via+a+Mixture+of+Hierarchical+Visual+Context+Learners)|0|
|[Diagrammatic Reasoning for ALC Visualization with Logic Graphs](https://doi.org/10.1145/3589334.3645607)|Ildar Baimuratov||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diagrammatic+Reasoning+for+ALC+Visualization+with+Logic+Graphs)|0|
|[Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models](https://doi.org/10.1145/3589334.3645611)|Kelvin J. L. Koa, Yunshan Ma, Ritchie Ng, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Generate+Explainable+Stock+Predictions+using+Self-Reflective+Large+Language+Models)|0|
|[High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text Attributed Graphs](https://doi.org/10.1145/3589334.3645614)|Peiyan Zhang, Chaozhuo Li, Liying Kang, Feiran Huang, Senzhang Wang, Xing Xie, Sunghun Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=High-Frequency-aware+Hierarchical+Contrastive+Selective+Coding+for+Representation+Learning+on+Text+Attributed+Graphs)|0|
|[Distributed Data Placement and Content Delivery in Web Caches with Non-Metric Access Costs](https://doi.org/10.1145/3589334.3645654)|||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributed+Data+Placement+and+Content+Delivery+in+Web+Caches+with+Non-Metric+Access+Costs)|0|
|[DualCL: Principled Supervised Contrastive Learning as Mutual Information Maximization for Text Classification](https://doi.org/10.1145/3589334.3645668)|Junfan Chen, Richong Zhang, Yaowei Zheng, Qianben Chen, Chunming Hu, Yongyi Mao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DualCL:+Principled+Supervised+Contrastive+Learning+as+Mutual+Information+Maximization+for+Text+Classification)|0|
|[Graph Anomaly Detection with Bi-level Optimization](https://doi.org/10.1145/3589334.3645673)|Yuan Gao, Junfeng Fang, Yongduo Sui, Yangyang Li, Xiang Wang, Huamin Feng, Yongdong Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Anomaly+Detection+with+Bi-level+Optimization)|0|
|[AN-Net: an Anti-Noise Network for Anonymous Traffic Classification](https://doi.org/10.1145/3589334.3645691)|Xianwen Deng, Yijun Wang, Zhi Xue||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AN-Net:+an+Anti-Noise+Network+for+Anonymous+Traffic+Classification)|0|
|[DenseFlow: Spotting Cryptocurrency Money Laundering in Ethereum Transaction Graphs](https://doi.org/10.1145/3589334.3645692)|Dan Lin, Jiajing Wu, Yunmei Yu, Qishuang Fu, Zibin Zheng, Changlin Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DenseFlow:+Spotting+Cryptocurrency+Money+Laundering+in+Ethereum+Transaction+Graphs)|0|
|[Towards Cross-Table Masked Pretraining for Web Data Mining](https://doi.org/10.1145/3589334.3645707)|Chao Ye, Guoshan Lu, Haobo Wang, Liyao Li, Sai Wu, Gang Chen, Junbo Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Cross-Table+Masked+Pretraining+for+Web+Data+Mining)|0|
|[Beyond Labels and Topics: Discovering Causal Relationships in Neural Topic Modeling](https://doi.org/10.1145/3589334.3645715)|YiKun Tang, Heyan Huang, Xuewen Shi, XianLing Mao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beyond+Labels+and+Topics:+Discovering+Causal+Relationships+in+Neural+Topic+Modeling)|0|
|[HD-KT: Advancing Robust Knowledge Tracing via Anomalous Learning Interaction Detection](https://doi.org/10.1145/3589334.3645718)|Haiping Ma, Yong Yang, Chuan Qin, Xiaoshan Yu, Shangshang Yang, Xingyi Zhang, Hengshu Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HD-KT:+Advancing+Robust+Knowledge+Tracing+via+Anomalous+Learning+Interaction+Detection)|0|
|[MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models](https://doi.org/10.1145/3589334.3648137)|Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, Sophia Ananiadou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MentaLLaMA:+Interpretable+Mental+Health+Analysis+on+Social+Media+with+Large+Language+Models)|0|
|[Message Injection Attack on Rumor Detection under the Black-Box Evasion Setting Using Large Language Model](https://doi.org/10.1145/3589334.3648139)|Yifeng Luo, Yupeng Li, Dacheng Wen, Liang Lan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Message+Injection+Attack+on+Rumor+Detection+under+the+Black-Box+Evasion+Setting+Using+Large+Language+Model)|0|
|[Human vs ChatGPT: Effect of Data Annotation in Interpretable Crisis-Related Microblog Classification](https://doi.org/10.1145/3589334.3648141)|Thi Huyen Nguyen, Koustav Rudra||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Human+vs+ChatGPT:+Effect+of+Data+Annotation+in+Interpretable+Crisis-Related+Microblog+Classification)|0|
|[Contrastive Learning for Multimodal Classification of Crisis related Tweets](https://doi.org/10.1145/3589334.3648143)|Bishwas Mandal, Sarthak Khanal, Doina Caragea||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Learning+for+Multimodal+Classification+of+Crisis+related+Tweets)|0|
|[Modularized Networks for Few-shot Hateful Meme Detection](https://doi.org/10.1145/3589334.3648145)|Rui Cao, Roy KaWei Lee, Jing Jiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modularized+Networks+for+Few-shot+Hateful+Meme+Detection)|0|
|[CapAlign: Improving Cross Modal Alignment via Informative Captioning for Harmful Meme Detection](https://doi.org/10.1145/3589334.3648146)|Junhui Ji, Xuanrui Lin, Usman Naseem||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CapAlign:+Improving+Cross+Modal+Alignment+via+Informative+Captioning+for+Harmful+Meme+Detection)|0|
|[Unveiling Climate Drivers via Feature Importance Shift Analysis in New Zealand](https://doi.org/10.1145/3589334.3648147)|Bowen Chen, Gillian Dobbie, Neelesh Rampal, Yun Sing Koh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+Climate+Drivers+via+Feature+Importance+Shift+Analysis+in+New+Zealand)|0|
|[Causal Graph ODE: Continuous Treatment Effect Modeling in Multi-agent Dynamical Systems](https://doi.org/10.1145/3589334.3648148)|Zijie Huang, Jeehyun Hwang, Junkai Zhang, Jinwoo Baik, Weitong Zhang, Dominik Wodarz, Yizhou Sun, Quanquan Gu, Wei Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Graph+ODE:+Continuous+Treatment+Effect+Modeling+in+Multi-agent+Dynamical+Systems)|0|
|[SceneDAPR: A Scene-Level Free-Hand Drawing Dataset for Web-based Psychological Drawing Assessment](https://doi.org/10.1145/3589334.3648150)|Jiwon Kang, Jiwon Kim, Migyeong Yang, Chaehee Park, Taeeun Kim, Hayeon Song, Jinyoung Han||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SceneDAPR:+A+Scene-Level+Free-Hand+Drawing+Dataset+for+Web-based+Psychological+Drawing+Assessment)|0|
|[MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation](https://doi.org/10.1145/3589334.3648151)|Han Wang, Roy KaWei Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MemeCraft:+Contextual+and+Stance-Driven+Multimodal+Meme+Generation)|0|
|[Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response](https://doi.org/10.1145/3589334.3648153)|Md Towhidul Absar Chowdhury, Soumyajit Datta, Naveen Sharma, Ashiqur R. KhudaBukhsh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Infrastructure+Ombudsman:+Mining+Future+Failure+Concerns+from+Structural+Disaster+Response)|0|
|[Predicting and Presenting Task Difficulty for Crowdsourcing Food Rescue Platforms](https://doi.org/10.1145/3589334.3648155)|Zheyuan Ryan Shi, Jiayin Zhi, Siqi Zeng, Zhicheng Zhang, Ameesh Kapoor, Sean Hudson, Hong Shen, Fei Fang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Predicting+and+Presenting+Task+Difficulty+for+Crowdsourcing+Food+Rescue+Platforms)|0|
|[GraphLeak: Patient Record Leakage through Gradients with Knowledge Graph](https://doi.org/10.1145/3589334.3648157)|Xi Sheryl Zhang, Weifan Guan, Jiahao Lu, Zhaopeng Qiu, Jian Cheng, Xian Wu, Yefeng Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphLeak:+Patient+Record+Leakage+through+Gradients+with+Knowledge+Graph)|0|
